<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>"BrainSCUBA&#36890;&#36807;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#26469;&#39044;&#27979;&#26368;&#22823;&#28608;&#27963;&#20010;&#20307;&#24863;&#20852;&#36259;&#20307;&#32032;&#30340;&#22270;&#20687;&#65292;&#36798;&#21040;&#20102;&#32454;&#31890;&#24230;&#30340;&#35270;&#35273;&#30382;&#23618;&#36873;&#25321;&#24615;&#25551;&#36848;&#12290;"</title><link>http://arxiv.org/abs/2310.04420</link><description>&lt;p&gt;
"BrainSCUBA: &#35270;&#35273;&#30382;&#23618;&#36873;&#25321;&#24615;&#30340;&#32454;&#31890;&#24230;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;"
&lt;/p&gt;
&lt;p&gt;
BrainSCUBA: Fine-Grained Natural Language Captions of Visual Cortex Selectivity. (arXiv:2310.04420v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04420
&lt;/p&gt;
&lt;p&gt;
"BrainSCUBA&#36890;&#36807;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#26469;&#39044;&#27979;&#26368;&#22823;&#28608;&#27963;&#20010;&#20307;&#24863;&#20852;&#36259;&#20307;&#32032;&#30340;&#22270;&#20687;&#65292;&#36798;&#21040;&#20102;&#32454;&#31890;&#24230;&#30340;&#35270;&#35273;&#30382;&#23618;&#36873;&#25321;&#24615;&#25551;&#36848;&#12290;"
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
"&#29702;&#35299;&#39640;&#32423;&#35270;&#35273;&#30382;&#23618;&#30340;&#21151;&#33021;&#32452;&#32455;&#26159;&#31070;&#32463;&#31185;&#23398;&#30340;&#26680;&#24515;&#20851;&#27880;&#28857;&#12290;&#36807;&#21435;&#30340;&#30740;&#31350;&#20027;&#35201;&#20351;&#29992;&#25163;&#21160;&#36873;&#25321;&#30340;&#21050;&#28608;&#26469;&#26144;&#23556;&#31070;&#32463;&#32676;&#20307;&#30340;&#35270;&#35273;&#21644;&#35821;&#20041;&#36873;&#25321;&#24615;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#23545;&#35270;&#35273;&#30382;&#23618;&#21151;&#33021;&#30340;&#39044;&#35774;&#20551;&#35774;&#30340;&#32467;&#26524;&#20559;&#24046;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#26469;&#39044;&#27979;&#26368;&#22823;&#28608;&#27963;&#20010;&#20307;&#24863;&#20852;&#36259;&#20307;&#32032;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;- &#22522;&#20110;&#23545;&#27604;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#23398;&#21040;&#30340;&#20016;&#23500;&#23884;&#20837;&#31354;&#38388;&#65292;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21487;&#35299;&#37322;&#30340;&#25551;&#36848;&#12290;&#25105;&#20204;&#36890;&#36807;&#39640;&#38454;&#35270;&#35273;&#21306;&#22495;&#36827;&#34892;&#20102;&#32454;&#31890;&#24230;&#30340;&#20307;&#32032;&#32423;&#25551;&#36848;&#65292;&#24182;&#36890;&#36807;&#25991;&#26412;&#26465;&#20214;&#30340;&#22270;&#20687;&#21512;&#25104;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#22270;&#20687;&#22312;&#35821;&#20041;&#19978;&#26159;&#36830;&#36143;&#30340;&#24182;&#19988;&#20855;&#26377;&#39640;&#30340;&#36136;&#37327;&#12290;"
&lt;/p&gt;
&lt;p&gt;
Understanding the functional organization of higher visual cortex is a central focus in neuroscience. Past studies have primarily mapped the visual and semantic selectivity of neural populations using hand-selected stimuli, which may potentially bias results towards pre-existing hypotheses of visual cortex functionality. Moving beyond conventional approaches, we introduce a data-driven method that generates natural language descriptions for images predicted to maximally activate individual voxels of interest. Our method -Semantic Captioning Using Brain Alignments ("BrainSCUBA") -- builds upon the rich embedding space learned by a contrastive vision-language model and utilizes a pre-trained large language model to generate interpretable captions. We validate our method through fine-grained voxel-level captioning across higher-order visual regions. We further perform text-conditioned image synthesis with the captions, and show that our images are semantically coherent and yield high pr
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FIRE&#30340;&#20989;&#25968;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#19982;&#28176;&#36827;&#25554;&#20540;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#36827;Transformer&#23545;&#26356;&#38271;&#19978;&#19979;&#25991;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#22312;&#38646;&#23556;&#20987;&#35821;&#35328;&#24314;&#27169;&#21644;&#38271;&#25991;&#26412;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#23454;&#35777;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.04418</link><description>&lt;p&gt;
&#29992;&#20110;&#30456;&#23545;&#20301;&#32622;&#30340;&#20989;&#25968;&#25554;&#20540;&#25913;&#36827;&#20102;&#38271;&#19978;&#19979;&#25991;Transformer
&lt;/p&gt;
&lt;p&gt;
Functional Interpolation for Relative Positions Improves Long Context Transformers. (arXiv:2310.04418v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04418
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FIRE&#30340;&#20989;&#25968;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#19982;&#28176;&#36827;&#25554;&#20540;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#36827;Transformer&#23545;&#26356;&#38271;&#19978;&#19979;&#25991;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#22312;&#38646;&#23556;&#20987;&#35821;&#35328;&#24314;&#27169;&#21644;&#38271;&#25991;&#26412;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#23454;&#35777;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25193;&#23637;&#36825;&#20123;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#26102;&#65292;&#38450;&#27490;Transformer&#22312;&#35757;&#32451;&#20197;&#22806;&#26356;&#38271;&#36755;&#20837;&#19978;&#24615;&#33021;&#19979;&#38477;&#19968;&#30452;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;&#34429;&#28982;Transformer&#26550;&#26500;&#22312;&#21487;&#22788;&#29702;&#30340;&#36755;&#20837;&#24207;&#21015;&#38271;&#24230;&#19978;&#22522;&#26412;&#27809;&#26377;&#38480;&#21046;&#65292;&#20294;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#30340;&#20301;&#32622;&#32534;&#30721;&#30340;&#36873;&#25321;&#21487;&#33021;&#20250;&#38480;&#21046;&#36825;&#20123;&#27169;&#22411;&#22312;&#26356;&#38271;&#36755;&#20837;&#19978;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20989;&#25968;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#19982;&#28176;&#36827;&#25554;&#20540;&#26041;&#27861;&#65288;FIRE&#65289;&#65292;&#20197;&#25913;&#36827;Transformer&#23545;&#26356;&#38271;&#19978;&#19979;&#25991;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#36825;&#21487;&#20197;&#34920;&#31034;&#20986;&#19968;&#20123;&#27969;&#34892;&#30340;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#65292;&#22914;T5&#30340;RPE&#12289;Alibi&#21644;Kerple&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#22312;&#38646;&#23556;&#20987;&#35821;&#35328;&#24314;&#27169;&#21644;&#38271;&#25991;&#26412;&#22522;&#20934;&#27979;&#35797;&#19978;&#32463;&#39564;&#24615;&#22320;&#23637;&#31034;&#20102;FIRE&#27169;&#22411;&#22312;&#26356;&#38271;&#19978;&#19979;&#25991;&#20013;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Preventing the performance decay of Transformers on inputs longer than those used for training has been an important challenge in extending the context length of these models. Though the Transformer architecture has fundamentally no limits on the input sequence lengths it can process, the choice of position encoding used during training can limit the performance of these models on longer inputs. We propose a novel functional relative position encoding with progressive interpolation, FIRE, to improve Transformer generalization to longer contexts. We theoretically prove that this can represent some of the popular relative position encodings, such as T5's RPE, Alibi, and Kerple. We next empirically show that FIRE models have better generalization to longer contexts on both zero-shot language modeling and long text benchmarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#25193;&#25955;&#27169;&#22411;&#20026;&#28789;&#24863;&#30340;&#28145;&#24230;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#65292;&#23427;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#24182;&#21487;&#22312;&#25968;&#37327;&#30456;&#21516;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#19979;&#19982;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#21487;&#27604;&#36739;&#30340;&#25968;&#20540;&#32467;&#26524;&#12290;&#36890;&#36807;&#25512;&#23548;&#24471;&#20998;&#21305;&#37197;&#30340;&#23646;&#24615;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#29616;&#26377;&#38543;&#26426;&#29305;&#24449;&#32467;&#26524;&#65292;&#24182;&#24471;&#20986;&#20102;&#26679;&#26412;&#25968;&#25454;&#20998;&#24067;&#19982;&#30495;&#23454;&#20998;&#24067;&#20043;&#38388;&#30340;&#27867;&#21270;&#36793;&#30028;&#12290;</title><link>http://arxiv.org/abs/2310.04417</link><description>&lt;p&gt;
&#25193;&#25955;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Diffusion Random Feature Model. (arXiv:2310.04417v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04417
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#25193;&#25955;&#27169;&#22411;&#20026;&#28789;&#24863;&#30340;&#28145;&#24230;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#65292;&#23427;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#24182;&#21487;&#22312;&#25968;&#37327;&#30456;&#21516;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#19979;&#19982;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#21487;&#27604;&#36739;&#30340;&#25968;&#20540;&#32467;&#26524;&#12290;&#36890;&#36807;&#25512;&#23548;&#24471;&#20998;&#21305;&#37197;&#30340;&#23646;&#24615;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#29616;&#26377;&#38543;&#26426;&#29305;&#24449;&#32467;&#26524;&#65292;&#24182;&#24471;&#20986;&#20102;&#26679;&#26412;&#25968;&#25454;&#20998;&#24067;&#19982;&#30495;&#23454;&#20998;&#24067;&#20043;&#38388;&#30340;&#27867;&#21270;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#24050;&#25104;&#21151;&#29992;&#20110;&#29983;&#25104;&#20174;&#22122;&#22768;&#20013;&#20135;&#29983;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#25193;&#25955;&#27169;&#22411;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#65292;&#38590;&#20197;&#35299;&#37322;&#65292;&#32570;&#20047;&#29702;&#35770;&#20381;&#25454;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#30001;&#20110;&#20854;&#21487;&#35299;&#37322;&#24615;&#65292;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#20294;&#20854;&#22312;&#22797;&#26434;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#20173;&#28982;&#26377;&#38480;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#25193;&#25955;&#27169;&#22411;&#21551;&#21457;&#30340;&#28145;&#24230;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#65292;&#23427;&#26082;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#65292;&#21448;&#33021;&#32473;&#20986;&#19982;&#20855;&#26377;&#30456;&#21516;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30456;&#24403;&#30340;&#25968;&#20540;&#32467;&#26524;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#38543;&#26426;&#29305;&#24449;&#32467;&#26524;&#65292;&#21033;&#29992;&#24471;&#20998;&#21305;&#37197;&#30340;&#23646;&#24615;&#23548;&#20986;&#20102;&#26679;&#26412;&#25968;&#25454;&#20998;&#24067;&#19982;&#30495;&#23454;&#20998;&#24067;&#20043;&#38388;&#30340;&#27867;&#21270;&#36793;&#30028;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#26102;&#23578;MNIST&#25968;&#25454;&#38598;&#21644;&#20048;&#22120;&#38899;&#39057;&#25968;&#25454;&#19978;&#29983;&#25104;&#26679;&#26412;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion probabilistic models have been successfully used to generate data from noise. However, most diffusion models are computationally expensive and difficult to interpret with a lack of theoretical justification. Random feature models on the other hand have gained popularity due to their interpretability but their application to complex machine learning tasks remains limited. In this work, we present a diffusion model-inspired deep random feature model that is interpretable and gives comparable numerical results to a fully connected neural network having the same number of trainable parameters. Specifically, we extend existing results for random features and derive generalization bounds between the distribution of sampled data and the true distribution using properties of score matching. We validate our findings by generating samples on the fashion MNIST dataset and instrumental audio data.
&lt;/p&gt;</description></item><item><title>&#26435;&#37325;&#34928;&#20943;&#22312;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#19982;&#20256;&#32479;&#30340;&#23398;&#20064;&#29702;&#35770;&#20013;&#30340;&#27491;&#21017;&#21270;&#25928;&#26524;&#19981;&#21516;&#12290;&#23545;&#20110;&#36807;&#21442;&#25968;&#21270;&#30340;&#28145;&#24230;&#32593;&#32476;&#65292;&#23427;&#36890;&#36807;&#25439;&#22833;&#31283;&#23450;&#26426;&#21046;&#25913;&#36827;&#20102;&#20248;&#21270;&#21160;&#24577;&#65292;&#23545;&#20110;&#27424;&#21442;&#25968;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#38543;&#26426;&#20248;&#21270;&#20013;&#24179;&#34913;&#20559;&#24046;-&#26041;&#24046;&#26435;&#34913;&#65292;&#23548;&#33268;&#36739;&#20302;&#30340;&#35757;&#32451;&#25439;&#22833;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#21487;&#20197;&#38450;&#27490;&#28151;&#21512;&#31934;&#24230;&#35757;&#32451;&#20013;&#30340;&#25439;&#22833;&#21457;&#25955;&#12290;</title><link>http://arxiv.org/abs/2310.04415</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#20013;&#38656;&#35201;&#26435;&#37325;&#34928;&#20943;&#65311;
&lt;/p&gt;
&lt;p&gt;
Why Do We Need Weight Decay in Modern Deep Learning?. (arXiv:2310.04415v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04415
&lt;/p&gt;
&lt;p&gt;
&#26435;&#37325;&#34928;&#20943;&#22312;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#19982;&#20256;&#32479;&#30340;&#23398;&#20064;&#29702;&#35770;&#20013;&#30340;&#27491;&#21017;&#21270;&#25928;&#26524;&#19981;&#21516;&#12290;&#23545;&#20110;&#36807;&#21442;&#25968;&#21270;&#30340;&#28145;&#24230;&#32593;&#32476;&#65292;&#23427;&#36890;&#36807;&#25439;&#22833;&#31283;&#23450;&#26426;&#21046;&#25913;&#36827;&#20102;&#20248;&#21270;&#21160;&#24577;&#65292;&#23545;&#20110;&#27424;&#21442;&#25968;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#38543;&#26426;&#20248;&#21270;&#20013;&#24179;&#34913;&#20559;&#24046;-&#26041;&#24046;&#26435;&#34913;&#65292;&#23548;&#33268;&#36739;&#20302;&#30340;&#35757;&#32451;&#25439;&#22833;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#21487;&#20197;&#38450;&#27490;&#28151;&#21512;&#31934;&#24230;&#35757;&#32451;&#20013;&#30340;&#25439;&#22833;&#21457;&#25955;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26435;&#37325;&#34928;&#20943;&#26159;&#19968;&#31181;&#24191;&#27867;&#29992;&#20110;&#35757;&#32451;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#32593;&#32476;&#65292;&#21253;&#25324;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25216;&#26415;&#12290;&#23613;&#31649;&#23427;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#20854;&#20316;&#29992;&#20173;&#19981;&#22826;&#34987;&#20102;&#35299;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24378;&#35843;&#26435;&#37325;&#34928;&#20943;&#22312;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#19982;&#20854;&#22312;&#32463;&#20856;&#23398;&#20064;&#29702;&#35770;&#20013;&#30740;&#31350;&#30340;&#27491;&#21017;&#21270;&#25928;&#26524;&#19981;&#21516;&#12290;&#23545;&#20110;&#36807;&#21442;&#25968;&#21270;&#30340;&#28145;&#24230;&#32593;&#32476;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26435;&#37325;&#34928;&#20943;&#22914;&#20309;&#36890;&#36807;&#25439;&#22833;&#31283;&#23450;&#26426;&#21046;&#25913;&#36827;&#20102;&#38544;&#24335;&#27491;&#21017;&#21270;&#30340;&#20248;&#21270;&#21160;&#24577;&#12290;&#30456;&#21453;&#65292;&#23545;&#20110;&#29992;&#20960;&#20046;&#22312;&#32447;SGD&#35757;&#32451;&#30340;&#27424;&#21442;&#25968;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#26435;&#37325;&#34928;&#20943;&#22914;&#20309;&#22312;&#38543;&#26426;&#20248;&#21270;&#20013;&#24179;&#34913;&#20559;&#24046;-&#26041;&#24046;&#26435;&#34913;&#65292;&#20174;&#32780;&#23548;&#33268;&#36739;&#20302;&#30340;&#35757;&#32451;&#25439;&#22833;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#26435;&#37325;&#34928;&#20943;&#22914;&#20309;&#38450;&#27490;bfloat16&#28151;&#21512;&#31934;&#24230;&#35757;&#32451;&#20013;&#31361;&#28982;&#30340;&#25439;&#22833;&#21457;&#25955;&#65292;&#36825;&#26159;LLM&#35757;&#32451;&#30340;&#20851;&#38190;&#24037;&#20855;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20174;ResNet&#23545;&#35270;&#35273;&#20219;&#21153;&#21040;LLM&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#35270;&#35282;&#65306;&#26435;&#37325;&#34928;&#20943;&#12290;
&lt;/p&gt;
&lt;p&gt;
Weight decay is a broadly used technique for training state-of-the-art deep networks, including large language models. Despite its widespread usage, its role remains poorly understood. In this work, we highlight that the role of weight decay in modern deep learning is different from its regularization effect studied in classical learning theory. For overparameterized deep networks, we show how weight decay modifies the optimization dynamics enhancing the ever-present implicit regularization of SGD via the loss stabilization mechanism. In contrast, for underparameterized large language models trained with nearly online SGD, we describe how weight decay balances the bias-variance tradeoff in stochastic optimization leading to lower training loss. Moreover, we show that weight decay also prevents sudden loss divergences for bfloat16 mixed-precision training which is a crucial tool for LLM training. Overall, we present a unifying perspective from ResNets on vision tasks to LLMs: weight dec
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#26679;&#31574;&#30053;&#20351;&#31574;&#30053;&#21482;&#21463;``&#22909;&#25968;&#25454;"&#38480;&#21046;&#65292;&#32780;&#19981;&#26159;&#25152;&#26377;&#30340;&#21160;&#20316;&#65292;&#25552;&#39640;&#20102;&#31163;&#32447;RL&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.04413</link><description>&lt;p&gt;
&#36229;&#36234;&#22343;&#21248;&#37319;&#26679;&#65306;&#20351;&#29992;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Beyond Uniform Sampling: Offline Reinforcement Learning with Imbalanced Datasets. (arXiv:2310.04413v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04413
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#26679;&#31574;&#30053;&#20351;&#31574;&#30053;&#21482;&#21463;``&#22909;&#25968;&#25454;"&#38480;&#21046;&#65292;&#32780;&#19981;&#26159;&#25152;&#26377;&#30340;&#21160;&#20316;&#65292;&#25552;&#39640;&#20102;&#31163;&#32447;RL&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#26088;&#22312;&#21033;&#29992;&#29616;&#26377;&#30340;&#36712;&#36857;&#25968;&#25454;&#38598;&#26469;&#23398;&#20064;&#20915;&#31574;&#31574;&#30053;&#65292;&#32780;&#26080;&#38656;&#25910;&#38598;&#39069;&#22806;&#30340;&#25968;&#25454;&#12290;&#19982;&#34892;&#20026;&#20811;&#38534;&#31561;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#30456;&#27604;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#20027;&#35201;&#21160;&#26426;&#26159;&#25214;&#21040;&#19968;&#20010;&#27604;&#25968;&#25454;&#38598;&#20013;&#30340;&#36712;&#36857;&#36798;&#21040;&#26356;&#39640;&#24179;&#22343;&#25910;&#30410;&#30340;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#22312;&#32463;&#39564;&#19978;&#21457;&#29616;&#65292;&#24403;&#19968;&#20010;&#25968;&#25454;&#38598;&#34987;&#27425;&#20248;&#36712;&#36857;&#25152;&#20027;&#23548;&#26102;&#65292;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#31163;&#32447;RL&#31639;&#27861;&#22312;&#24179;&#22343;&#25910;&#30410;&#19978;&#27809;&#26377;&#26174;&#33879;&#25552;&#39640;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#26159;&#30001;&#20110;&#24403;&#21069;&#31163;&#32447;RL&#31639;&#27861;&#20551;&#35774;&#19982;&#25968;&#25454;&#38598;&#20013;&#30340;&#36712;&#36857;&#20445;&#25345;&#25509;&#36817;&#12290;&#22914;&#26524;&#25968;&#25454;&#38598;&#20027;&#35201;&#30001;&#27425;&#20248;&#36712;&#36857;&#32452;&#25104;&#65292;&#36825;&#20010;&#20551;&#35774;&#23558;&#24378;&#21046;&#31574;&#30053;&#27169;&#20223;&#27425;&#20248;&#21160;&#20316;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#37319;&#26679;&#31574;&#30053;&#26469;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#20351;&#31574;&#30053;&#21482;&#21463;``&#22909;&#25968;&#25454;"&#38480;&#21046;&#65292;&#32780;&#19981;&#26159;&#25152;&#26377;&#30340;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline policy learning is aimed at learning decision-making policies using existing datasets of trajectories without collecting additional data. The primary motivation for using reinforcement learning (RL) instead of supervised learning techniques such as behavior cloning is to find a policy that achieves a higher average return than the trajectories constituting the dataset. However, we empirically find that when a dataset is dominated by suboptimal trajectories, state-of-the-art offline RL algorithms do not substantially improve over the average return of trajectories in the dataset. We argue this is due to an assumption made by current offline RL algorithms of staying close to the trajectories in the dataset. If the dataset primarily consists of sub-optimal trajectories, this assumption forces the policy to mimic the suboptimal actions. We overcome this issue by proposing a sampling strategy that enables the policy to only be constrained to ``good data" rather than all actions in t
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#29702;&#35299;Q&#20540;&#20272;&#35745;&#20998;&#27495;&#30340;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#28608;&#21169;&#29305;&#24449;&#20540;&#27979;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#33021;&#22815;&#21487;&#38752;&#22320;&#21028;&#26029;&#35757;&#32451;&#36807;&#31243;&#26159;&#21542;&#20250;&#20986;&#29616;&#20998;&#27495;&#12290;</title><link>http://arxiv.org/abs/2310.04411</link><description>&lt;p&gt;
&#29702;&#35299;&#12289;&#39044;&#27979;&#21644;&#26356;&#22909;&#22320;&#35299;&#20915;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;Q&#20540;&#20998;&#27495;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Understanding, Predicting and Better Resolving Q-Value Divergence in Offline-RL. (arXiv:2310.04411v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04411
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#29702;&#35299;Q&#20540;&#20272;&#35745;&#20998;&#27495;&#30340;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#28608;&#21169;&#29305;&#24449;&#20540;&#27979;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#33021;&#22815;&#21487;&#38752;&#22320;&#21028;&#26029;&#35757;&#32451;&#36807;&#31243;&#26159;&#21542;&#20250;&#20986;&#29616;&#20998;&#27495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Q&#20540;&#20272;&#35745;&#30340;&#20998;&#27495;&#19968;&#30452;&#26159;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#19968;&#20010;&#31361;&#20986;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#20195;&#29702;&#20154;&#26080;&#27861;&#33719;&#24471;&#30495;&#23454;&#21160;&#24577;&#20449;&#24687;&#12290;&#20256;&#32479;&#35266;&#28857;&#35748;&#20026;&#65292;&#24403;&#24341;&#23548;&#20540;&#30446;&#26631;&#26102;&#26597;&#35810;&#20998;&#24067;&#20043;&#22806;&#30340;&#21160;&#20316;&#26159;&#23548;&#33268;&#19981;&#31283;&#23450;&#30340;&#21407;&#22240;&#12290;&#23613;&#31649;&#21487;&#20197;&#36890;&#36807;&#31574;&#30053;&#32422;&#26463;&#25110;&#20445;&#23432;&#30340;Q&#20540;&#20272;&#35745;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#23545;&#23548;&#33268;&#20998;&#27495;&#30340;&#26426;&#21046;&#30340;&#29702;&#35770;&#29702;&#35299;&#19968;&#30452;&#32570;&#20047;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#28145;&#20837;&#20102;&#35299;&#36825;&#20010;&#26426;&#21046;&#24182;&#33719;&#24471;&#25913;&#36827;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#39318;&#20808;&#35782;&#21035;&#20102;&#33258;&#28608;&#21169;&#20316;&#20026;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;Q&#20540;&#20272;&#35745;&#20998;&#27495;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#20999;&#32447;&#26680;(NTK)&#30340;&#33258;&#28608;&#21169;&#29305;&#24449;&#20540;&#27979;&#37327;(SEEM)&#25351;&#26631;&#65292;&#29992;&#20110;&#27979;&#37327;&#35757;&#32451;&#36807;&#31243;&#20013;Q&#32593;&#32476;&#30340;&#28436;&#21270;&#24615;&#36136;&#65292;&#36825;&#25552;&#20379;&#20102;&#23545;&#20998;&#27495;&#20986;&#29616;&#30340;&#26377;&#36259;&#35299;&#37322;&#12290;&#39318;&#27425;&#65292;&#25105;&#20204;&#30340;&#29702;&#35770;&#33021;&#22815;&#21487;&#38752;&#22320;&#20915;&#23450;&#35757;&#32451;&#26159;&#21542;&#22312;&#26089;&#26399;&#38454;&#27573;&#21457;&#25955;&#12290;
&lt;/p&gt;
&lt;p&gt;
The divergence of the Q-value estimation has been a prominent issue in offline RL, where the agent has no access to real dynamics. Traditional beliefs attribute this instability to querying out-of-distribution actions when bootstrapping value targets. Though this issue can be alleviated with policy constraints or conservative Q estimation, a theoretical understanding of the underlying mechanism causing the divergence has been absent. In this work, we aim to thoroughly comprehend this mechanism and attain an improved solution. We first identify a fundamental pattern, self-excitation, as the primary cause of Q-value estimation divergence in offline RL. Then, we propose a novel Self-Excite Eigenvalue Measure (SEEM) metric based on Neural Tangent Kernel (NTK) to measure the evolving property of Q-network at training, which provides an intriguing explanation of the emergence of divergence. For the first time, our theory can reliably decide whether the training will diverge at an early stage
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25490;&#24207;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#31574;&#30053;&#26799;&#24230;&#35757;&#32451;&#31639;&#27861;Neural PG-RANK&#65292;&#36890;&#36807;&#23558;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#23454;&#20363;&#21270;&#20026;Plackett-Luce&#25490;&#21517;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#23545;&#26816;&#32034;&#27169;&#22411;&#30340;&#21407;&#21017;&#24615;&#12289;&#31471;&#21040;&#31471;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2310.04407</link><description>&lt;p&gt;
&#29992;&#20110;&#25490;&#24207;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#31574;&#30053;&#26799;&#24230;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Policy-Gradient Training of Language Models for Ranking. (arXiv:2310.04407v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04407
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25490;&#24207;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#31574;&#30053;&#26799;&#24230;&#35757;&#32451;&#31639;&#27861;Neural PG-RANK&#65292;&#36890;&#36807;&#23558;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#23454;&#20363;&#21270;&#20026;Plackett-Luce&#25490;&#21517;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#23545;&#26816;&#32034;&#27169;&#22411;&#30340;&#21407;&#21017;&#24615;&#12289;&#31471;&#21040;&#31471;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#26816;&#32034;&#22312;&#23558;&#20107;&#23454;&#30693;&#35782;&#32435;&#20837;&#21040;&#35821;&#35328;&#22788;&#29702;&#27969;&#31243;&#20013;&#30340;&#20915;&#31574;&#36807;&#31243;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#20174;&#32842;&#22825;&#24335;&#32593;&#39029;&#25628;&#32034;&#21040;&#38382;&#31572;&#31995;&#32479;&#12290;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#26816;&#32034;&#27169;&#22411;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20197;&#36798;&#21040;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#65292;&#20294;&#36890;&#36807;&#20856;&#22411;&#30340;&#23545;&#27604;&#25439;&#22833;&#35757;&#32451;&#22522;&#20110;LLM&#30340;&#26816;&#32034;&#22120;&#38656;&#35201;&#22797;&#26434;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#21253;&#25324;&#36873;&#25321;&#22256;&#38590;&#30340;&#36127;&#26679;&#26412;&#21644;&#20351;&#29992;&#39069;&#22806;&#30340;&#30417;&#30563;&#20316;&#20026;&#23398;&#20064;&#20449;&#21495;&#12290;&#36825;&#31181;&#20381;&#36182;&#20110;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#21407;&#22240;&#26159;&#23545;&#27604;&#25439;&#22833;&#26412;&#36523;&#26159;&#21551;&#21457;&#24335;&#30340;&#65292;&#19981;&#33021;&#30452;&#25509;&#20248;&#21270;&#22788;&#29702;&#27969;&#31243;&#26411;&#31471;&#20915;&#31574;&#36136;&#37327;&#30340;&#19979;&#28216;&#25351;&#26631;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31070;&#32463;PG-RANK&#65292;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;LLM&#23454;&#20363;&#21270;&#20026;Plackett-Luce&#25490;&#21517;&#31574;&#30053;&#65292;&#23398;&#20064;&#25490;&#24207;&#12290;&#31070;&#32463;PG-RANK&#20026;&#26816;&#32034;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#25552;&#20379;&#20102;&#19968;&#31181;&#21407;&#21017;&#24615;&#26041;&#27861;&#65292;&#20316;&#20026;&#26356;&#22823;&#30340;&#20915;&#31574;&#31995;&#32479;&#30340;&#19968;&#37096;&#20998;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text retrieval plays a crucial role in incorporating factual knowledge for decision making into language processing pipelines, ranging from chat-based web search to question answering systems. Current state-of-the-art text retrieval models leverage pre-trained large language models (LLMs) to achieve competitive performance, but training LLM-based retrievers via typical contrastive losses requires intricate heuristics, including selecting hard negatives and using additional supervision as learning signals. This reliance on heuristics stems from the fact that the contrastive loss itself is heuristic and does not directly optimize the downstream metrics of decision quality at the end of the processing pipeline. To address this issue, we introduce Neural PG-RANK, a novel training algorithm that learns to rank by instantiating a LLM as a Plackett-Luce ranking policy. Neural PG-RANK provides a principled method for end-to-end training of retrieval models as part of larger decision systems vi
&lt;/p&gt;</description></item><item><title>&#35821;&#35328;&#20195;&#29702;&#26641;&#25628;&#32034;&#65288;LATS&#65289;&#26159;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#22312;&#35268;&#21010;&#12289;&#34892;&#21160;&#21644;&#25512;&#29702;&#26041;&#38754;&#30456;&#20114;&#21327;&#21516;&#65292;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#22806;&#37096;&#21453;&#39304;&#30340;&#29615;&#22659;&#65292;&#23454;&#29616;&#26356;&#21152;&#28145;&#24605;&#29087;&#34385;&#21644;&#36866;&#24212;&#24615;&#30340;&#38382;&#39064;&#35299;&#20915;&#26426;&#21046;&#12290;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;LATS&#22312;&#22810;&#20010;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#24615;&#65292;&#29305;&#21035;&#22312;&#32534;&#31243;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;94.4%&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.04406</link><description>&lt;p&gt;
&#35821;&#35328;&#20195;&#29702;&#26641;&#25628;&#32034;&#32479;&#19968;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25512;&#29702;&#12289;&#34892;&#21160;&#21644;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models. (arXiv:2310.04406v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04406
&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#20195;&#29702;&#26641;&#25628;&#32034;&#65288;LATS&#65289;&#26159;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#22312;&#35268;&#21010;&#12289;&#34892;&#21160;&#21644;&#25512;&#29702;&#26041;&#38754;&#30456;&#20114;&#21327;&#21516;&#65292;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#22806;&#37096;&#21453;&#39304;&#30340;&#29615;&#22659;&#65292;&#23454;&#29616;&#26356;&#21152;&#28145;&#24605;&#29087;&#34385;&#21644;&#36866;&#24212;&#24615;&#30340;&#38382;&#39064;&#35299;&#20915;&#26426;&#21046;&#12290;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;LATS&#22312;&#22810;&#20010;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#24615;&#65292;&#29305;&#21035;&#22312;&#32534;&#31243;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;94.4%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#19968;&#31995;&#21015;&#20915;&#31574;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#20381;&#36182;&#20110;&#31616;&#21333;&#30340;&#34892;&#21160;&#36807;&#31243;&#65292;&#24182;&#26410;&#33021;&#24191;&#27867;&#37096;&#32626;&#20316;&#20026;&#33258;&#20027;&#20195;&#29702;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;LATS&#65288;&#35821;&#35328;&#20195;&#29702;&#26641;&#25628;&#32034;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#23558;LLMs&#22312;&#35268;&#21010;&#12289;&#34892;&#21160;&#21644;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#30456;&#20114;&#21327;&#21516;&#12290;LATS&#20511;&#37492;&#20102;&#27169;&#22411;&#23548;&#21521;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#30340;&#24605;&#24819;&#65292;&#23558;LLMs&#29992;&#20316;&#20195;&#29702;&#12289;&#20215;&#20540;&#20989;&#25968;&#21644;&#20248;&#21270;&#22120;&#65292;&#37325;&#26032;&#21033;&#29992;&#20854;&#28508;&#22312;&#30340;&#20248;&#21183;&#20197;&#25552;&#21319;&#20915;&#31574;&#33021;&#21147;&#12290;&#20851;&#38190;&#30340;&#19968;&#28857;&#26159;LATS&#20351;&#29992;&#19968;&#20010;&#20855;&#26377;&#22806;&#37096;&#21453;&#39304;&#30340;&#29615;&#22659;&#65292;&#36825;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#21152;&#28145;&#24605;&#29087;&#34385;&#21644;&#36866;&#24212;&#24615;&#30340;&#38382;&#39064;&#35299;&#20915;&#26426;&#21046;&#65292;&#36229;&#36234;&#20102;&#29616;&#26377;&#25216;&#26415;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#22312;&#32534;&#31243;&#12289;HotPotQA&#21644;WebShop&#31561;&#22810;&#20010;&#39046;&#22495;&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;LATS&#22312;&#25512;&#29702;&#21644;&#34892;&#21160;&#26041;&#38754;&#30340;&#36866;&#29992;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#32534;&#31243;&#26041;&#38754;&#65292;LATS&#23454;&#29616;&#20102;94.4%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large language models (LLMs) have demonstrated impressive performance on a range of decision-making tasks, they rely on simple acting processes and fall short of broad deployment as autonomous agents. We introduce LATS (Language Agent Tree Search), a general framework that synergizes the capabilities of LLMs in planning, acting, and reasoning. Drawing inspiration from Monte Carlo tree search in model-based reinforcement learning, LATS employs LLMs as agents, value functions, and optimizers, repurposing their latent strengths for enhanced decision-making. What is crucial in this method is the use of an environment for external feedback, which offers a more deliberate and adaptive problem-solving mechanism that moves beyond the limitations of existing techniques. Our experimental evaluation across diverse domains, such as programming, HotPotQA, and WebShop, illustrates the applicability of LATS for both reasoning and acting. In particular, LATS achieves 94.4\% for programming on Hu
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#21487;&#32553;&#25918;&#25512;&#33616;&#27169;&#22411;&#20013;&#23884;&#20837;&#23618;&#30340;&#23849;&#28291;&#29616;&#35937;&#65292;&#21457;&#29616;&#29305;&#24449;&#20132;&#20114;&#27169;&#22359;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#38480;&#21046;&#20102;&#23884;&#20837;&#23398;&#20064;&#65292;&#20294;&#20063;&#26159;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2310.04400</link><description>&lt;p&gt;
&#35770;&#21487;&#25193;&#23637;&#25512;&#33616;&#27169;&#22411;&#20013;&#23884;&#20837;&#22349;&#32553;&#29616;&#35937;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Embedding Collapse when Scaling up Recommendation Models. (arXiv:2310.04400v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04400
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#21487;&#32553;&#25918;&#25512;&#33616;&#27169;&#22411;&#20013;&#23884;&#20837;&#23618;&#30340;&#23849;&#28291;&#29616;&#35937;&#65292;&#21457;&#29616;&#29305;&#24449;&#20132;&#20114;&#27169;&#22359;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#38480;&#21046;&#20102;&#23884;&#20837;&#23398;&#20064;&#65292;&#20294;&#20063;&#26159;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#22522;&#30784;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#24341;&#21457;&#20102;&#24320;&#21457;&#22823;&#22411;&#25512;&#33616;&#27169;&#22411;&#20197;&#21033;&#29992;&#22823;&#37327;&#21487;&#29992;&#25968;&#25454;&#30340;&#26377;&#21069;&#26223;&#36235;&#21183;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35797;&#39564;&#25918;&#22823;&#29616;&#26377;&#30340;&#25512;&#33616;&#27169;&#22411;&#26102;&#21457;&#29616;&#65292;&#25193;&#22823;&#30340;&#27169;&#22411;&#24182;&#27809;&#26377;&#20196;&#20154;&#28385;&#24847;&#30340;&#25913;&#36827;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#25193;&#22823;&#27169;&#22411;&#30340;&#23884;&#20837;&#23618;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#31181;&#23884;&#20837;&#22349;&#32553;&#29616;&#35937;&#65292;&#36825;&#26368;&#32456;&#38459;&#30861;&#20102;&#21487;&#25193;&#23637;&#24615;&#65292;&#22312;&#36825;&#31181;&#29616;&#35937;&#20013;&#65292;&#23884;&#20837;&#30697;&#38453;&#20542;&#21521;&#20110;&#23384;&#22312;&#20110;&#20302;&#32500;&#23376;&#31354;&#38388;&#20013;&#12290;&#36890;&#36807;&#23454;&#35777;&#21644;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25512;&#33616;&#27169;&#22411;&#29305;&#23450;&#30340;&#29305;&#24449;&#20132;&#20114;&#27169;&#22359;&#20855;&#26377;&#21452;&#37325;&#20316;&#29992;&#12290;&#19968;&#26041;&#38754;&#65292;&#24403;&#19982;&#22349;&#32553;&#30340;&#23884;&#20837;&#20132;&#20114;&#26102;&#65292;&#35813;&#20132;&#20114;&#38480;&#21046;&#20102;&#23884;&#20837;&#23398;&#20064;&#65292;&#21152;&#21095;&#20102;&#23849;&#28291;&#38382;&#39064;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#29305;&#24449;&#20132;&#20114;&#23545;&#20110;&#32531;&#35299;&#20551;&#29305;&#24449;&#30340;&#25311;&#21512;&#33267;&#20851;&#37325;&#35201;&#65292;&#20174;&#32780;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#12290;&#22522;&#20110;&#36825;&#19968;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Recent advances in deep foundation models have led to a promising trend of developing large recommendation models to leverage vast amounts of available data. However, we experiment to scale up existing recommendation models and observe that the enlarged models do not improve satisfactorily. In this context, we investigate the embedding layers of enlarged models and identify a phenomenon of embedding collapse, which ultimately hinders scalability, wherein the embedding matrix tends to reside in a low-dimensional subspace. Through empirical and theoretical analysis, we demonstrate that the feature interaction module specific to recommendation models has a two-sided effect. On the one hand, the interaction restricts embedding learning when interacting with collapsed embeddings, exacerbating the collapse issue. On the other hand, feature interaction is crucial in mitigating the fitting of spurious features, thereby improving scalability. Based on this analysis, we propose a simple yet effe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#19968;&#33268;&#24615;&#25913;&#36827;&#25968;&#25454;&#26377;&#25928;&#30340;&#25674;&#20313;&#36125;&#21494;&#26031;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#36716;&#36125;&#21494;&#26031;&#23450;&#29702;&#24182;&#21033;&#29992;&#36817;&#20284;&#34920;&#31034;&#30340;&#32852;&#21512;&#27169;&#22411;&#20272;&#35745;&#36793;&#38469;&#20284;&#28982;&#65292;&#21152;&#36895;&#26465;&#20214;&#31070;&#32463;&#23494;&#24230;&#20272;&#35745;&#22120;&#30340;&#23398;&#20064;&#21160;&#21147;&#23398;&#12290;</title><link>http://arxiv.org/abs/2310.04395</link><description>&lt;p&gt;
&#21033;&#29992;&#33258;&#19968;&#33268;&#24615;&#25552;&#39640;&#25968;&#25454;&#26377;&#25928;&#30340;&#25674;&#20313;&#36125;&#21494;&#26031;&#25512;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Leveraging Self-Consistency for Data-Efficient Amortized Bayesian Inference. (arXiv:2310.04395v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04395
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#19968;&#33268;&#24615;&#25913;&#36827;&#25968;&#25454;&#26377;&#25928;&#30340;&#25674;&#20313;&#36125;&#21494;&#26031;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#36716;&#36125;&#21494;&#26031;&#23450;&#29702;&#24182;&#21033;&#29992;&#36817;&#20284;&#34920;&#31034;&#30340;&#32852;&#21512;&#27169;&#22411;&#20272;&#35745;&#36793;&#38469;&#20284;&#28982;&#65292;&#21152;&#36895;&#26465;&#20214;&#31070;&#32463;&#23494;&#24230;&#20272;&#35745;&#22120;&#30340;&#23398;&#20064;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21442;&#25968;$\theta$&#21644;&#25968;&#25454;$y$&#30340;&#27010;&#29575;&#32852;&#21512;&#27169;&#22411;$p(\theta, y)$&#20013;&#30340;&#36890;&#29992;&#23545;&#31216;&#24615;&#65292;&#25913;&#36827;&#20102;&#25674;&#20313;&#36125;&#21494;&#26031;&#25512;&#29702;&#65288;ABI&#65289;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;&#31616;&#35328;&#20043;&#65292;&#25105;&#20204;&#21453;&#36716;&#36125;&#21494;&#26031;&#23450;&#29702;&#65292;&#24182;&#22522;&#20110;&#36817;&#20284;&#34920;&#31034;&#30340;&#32852;&#21512;&#27169;&#22411;&#20272;&#35745;&#36793;&#38469;&#20284;&#28982;&#12290;&#22312;&#23436;&#32654;&#36817;&#20284;&#24773;&#20917;&#19979;&#65292;&#36793;&#38469;&#20284;&#28982;&#22312;&#25152;&#26377;&#21442;&#25968;&#20540;&#19978;&#37117;&#26159;&#24120;&#25968;&#23450;&#20041;&#30340;&#12290;&#28982;&#32780;&#65292;&#36817;&#20284;&#35823;&#24046;&#23548;&#33268;&#19981;&#21516;&#21442;&#25968;&#20540;&#30340;&#36793;&#38469;&#20284;&#28982;&#20272;&#35745;&#20013;&#23384;&#22312;&#19981;&#21487;&#21462;&#30340;&#26041;&#24046;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#23545;&#31216;&#24615;&#30340;&#36829;&#21453;&#24418;&#24335;&#21270;&#20026;&#25439;&#22833;&#20989;&#25968;&#65292;&#21152;&#36895;&#26465;&#20214;&#31070;&#32463;&#23494;&#24230;&#20272;&#35745;&#22120;&#30340;&#23398;&#20064;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#20855;&#26377;&#26174;&#24335;&#20284;&#28982;&#65288;&#22522;&#20110;&#20284;&#28982;&#65289;&#30340;&#21452;&#23792;&#29609;&#20855;&#38382;&#39064;&#21644;&#20855;&#26377;&#38544;&#24335;&#20284;&#28982;&#65288;&#22522;&#20110;&#27169;&#25311;&#65289;&#30340;&#29616;&#23454;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a method to improve the efficiency and accuracy of amortized Bayesian inference (ABI) by leveraging universal symmetries in the probabilistic joint model $p(\theta, y)$ of parameters $\theta$ and data $y$. In a nutshell, we invert Bayes' theorem and estimate the marginal likelihood based on approximate representations of the joint model. Upon perfect approximation, the marginal likelihood is constant across all parameter values by definition. However, approximation error leads to undesirable variance in the marginal likelihood estimates across different parameter values. We formulate violations of this symmetry as a loss function to accelerate the learning dynamics of conditional neural density estimators. We apply our method to a bimodal toy problem with an explicit likelihood (likelihood-based) and a realistic model with an implicit likelihood (simulation-based).
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#28508;&#22312;&#19968;&#33268;&#24615;&#27169;&#22411;&#65288;LCMs&#65289;&#65292;&#36890;&#36807;&#30452;&#25509;&#39044;&#27979;&#28508;&#22312;&#31354;&#38388;&#20013;&#25193;&#25955;&#36807;&#31243;&#30340;&#35299;&#65292;&#23454;&#29616;&#20102;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#30340;&#24555;&#36895;&#21512;&#25104;&#65292;&#20943;&#23569;&#20102;&#35745;&#31639;&#37327;&#21644;&#26102;&#38388;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2310.04378</link><description>&lt;p&gt;
&#28508;&#22312;&#19968;&#33268;&#24615;&#27169;&#22411;&#65306;&#20351;&#29992;&#23569;&#37327;&#27493;&#39588;&#30340;&#25512;&#29702;&#21512;&#25104;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
Latent Consistency Models: Synthesizing High-Resolution Images with Few-Step Inference. (arXiv:2310.04378v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04378
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#28508;&#22312;&#19968;&#33268;&#24615;&#27169;&#22411;&#65288;LCMs&#65289;&#65292;&#36890;&#36807;&#30452;&#25509;&#39044;&#27979;&#28508;&#22312;&#31354;&#38388;&#20013;&#25193;&#25955;&#36807;&#31243;&#30340;&#35299;&#65292;&#23454;&#29616;&#20102;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#30340;&#24555;&#36895;&#21512;&#25104;&#65292;&#20943;&#23569;&#20102;&#35745;&#31639;&#37327;&#21644;&#26102;&#38388;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65288;LDM&#65289;&#22312;&#21512;&#25104;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#36845;&#20195;&#37319;&#26679;&#36807;&#31243;&#35745;&#31639;&#23494;&#38598;&#12289;&#29983;&#25104;&#36895;&#24230;&#24930;&#12290;&#21463;&#21040;&#19968;&#33268;&#24615;&#27169;&#22411;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28508;&#22312;&#19968;&#33268;&#24615;&#27169;&#22411;&#65288;LCM&#65289;&#65292;&#22312;&#20219;&#20309;&#39044;&#35757;&#32451;&#30340;LDM&#19978;&#23454;&#29616;&#20102;&#24555;&#36895;&#25512;&#29702;&#65292;&#20165;&#38656;&#26368;&#23569;&#30340;&#27493;&#39588;&#65292;&#21253;&#25324;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#12290;&#23558;&#24341;&#23548;&#36870;&#25193;&#25955;&#36807;&#31243;&#35270;&#20026;&#35299;&#20915;&#22686;&#24191;&#27010;&#29575;&#27969;ODE&#65288;PF-ODE&#65289;&#65292;LCM&#34987;&#35774;&#35745;&#20026;&#30452;&#25509;&#39044;&#27979;&#28508;&#22312;&#31354;&#38388;&#20013;&#35813;ODE&#30340;&#35299;&#65292;&#20943;&#23569;&#20102;&#22810;&#27425;&#36845;&#20195;&#30340;&#38656;&#27714;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#12289;&#39640;&#20445;&#30495;&#30340;&#37319;&#26679;&#12290;&#26377;&#25928;&#22320;&#20174;&#39044;&#35757;&#32451;&#30340;&#26080;&#20998;&#31867;&#22120;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#20013;&#25552;&#21462;&#65292;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;768 x 768&#30340;2~4&#27493;LCM&#20165;&#38656;32&#20010;A100 GPU&#23567;&#26102;&#36827;&#34892;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#28508;&#22312;&#19968;&#33268;&#24615;&#24494;&#35843;&#65288;LCF&#65289;&#65292;&#19968;&#31181;&#38024;&#23545;&#33258;&#23450;&#20041;&#22270;&#20687;&#25968;&#25454;&#38598;&#31934;&#24515;&#35774;&#35745;&#30340;LCM&#24494;&#35843;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Latent Diffusion models (LDMs) have achieved remarkable results in synthesizing high-resolution images. However, the iterative sampling process is computationally intensive and leads to slow generation. Inspired by Consistency Models (song et al.), we propose Latent Consistency Models (LCMs), enabling swift inference with minimal steps on any pre-trained LDMs, including Stable Diffusion (rombach et al). Viewing the guided reverse diffusion process as solving an augmented probability flow ODE (PF-ODE), LCMs are designed to directly predict the solution of such ODE in latent space, mitigating the need for numerous iterations and allowing rapid, high-fidelity sampling. Efficiently distilled from pre-trained classifier-free guided diffusion models, a high-quality 768 x 768 2~4-step LCM takes only 32 A100 GPU hours for training. Furthermore, we introduce Latent Consistency Fine-tuning (LCF), a novel method that is tailored for fine-tuning LCMs on customized image datasets. Evaluation on the
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#30740;&#31350;&#20102;&#32452;&#21512;&#22870;&#21169;&#27169;&#22411;&#20013;&#30340;&#36807;&#24230;&#20248;&#21270;&#38382;&#39064;&#65292;&#21457;&#29616;&#32452;&#25104;&#22870;&#21169;&#27169;&#22411;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#23545;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#24335;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.04373</link><description>&lt;p&gt;
&#29992;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#23545;&#25239;&#22870;&#21169;&#27169;&#22411;&#36807;&#24230;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Confronting Reward Model Overoptimization with Constrained RLHF. (arXiv:2310.04373v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#30740;&#31350;&#20102;&#32452;&#21512;&#22870;&#21169;&#27169;&#22411;&#20013;&#30340;&#36807;&#24230;&#20248;&#21270;&#38382;&#39064;&#65292;&#21457;&#29616;&#32452;&#25104;&#22870;&#21169;&#27169;&#22411;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#23545;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#24335;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#36890;&#36807;&#20248;&#21270;&#36866;&#24212;&#20154;&#31867;&#21453;&#39304;&#30340;&#22870;&#21169;&#27169;&#22411;&#26469;&#19982;&#20154;&#31867;&#20559;&#22909;&#20445;&#25345;&#19968;&#33268;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#20559;&#22909;&#26159;&#22810;&#26041;&#38754;&#30340;&#65292;&#36234;&#26469;&#36234;&#24120;&#35265;&#30340;&#20570;&#27861;&#26159;&#20174;&#20960;&#20010;&#31616;&#21333;&#30340;&#22870;&#21169;&#27169;&#22411;&#20013;&#27966;&#29983;&#20986;&#22870;&#21169;&#65292;&#27599;&#20010;&#27169;&#22411;&#25429;&#25417;&#35821;&#35328;&#36136;&#37327;&#30340;&#19981;&#21516;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#24403;&#32452;&#21512;&#36825;&#20123;&#32452;&#25104;&#30340;&#22870;&#21169;&#27169;&#22411;&#26102;&#65292;&#36866;&#24403;&#22320;&#21152;&#26435;&#21464;&#24471;&#22256;&#38590;&#12290;&#26356;&#21152;&#22256;&#38590;&#30340;&#26159;&#65292;&#30001;&#20110;&#20219;&#20309;&#22870;&#21169;&#27169;&#22411;&#21482;&#26159;&#20154;&#31867;&#35780;&#20215;&#30340;&#20195;&#29702;&#65292;&#36825;&#19968;&#36807;&#31243;&#23481;&#26131;&#21463;&#21040;&#36807;&#24230;&#20248;&#21270;&#30340;&#24433;&#21709;&#65292;&#21363;&#36229;&#36807;&#26576;&#19968;&#28857;&#21518;&#65292;&#33719;&#24471;&#26356;&#39640;&#22870;&#21169;&#19982;&#26356;&#24046;&#30340;&#20154;&#31867;&#35780;&#20215;&#30456;&#20851;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#32452;&#21512;&#22870;&#21169;&#27169;&#22411;&#20013;&#36807;&#24230;&#20248;&#21270;&#36827;&#34892;&#30740;&#31350;&#65292;&#39318;&#27425;&#23637;&#31034;&#20102;&#32452;&#25104;&#22870;&#21169;&#27169;&#22411;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#23545;&#36825;&#20123;&#28857;&#30340;&#20301;&#32622;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models are typically aligned with human preferences by optimizing $\textit{reward models}$ (RMs) fitted to human feedback. However, human preferences are multi-faceted, and it is increasingly common to derive reward from a composition of simpler reward models which each capture a different aspect of language quality. This itself presents a challenge, as it is difficult to appropriately weight these component RMs when combining them. Compounding this difficulty, because any RM is only a proxy for human evaluation, this process is vulnerable to $\textit{overoptimization}$, wherein past a certain point, accumulating higher reward is associated with worse human ratings. In this paper, we perform, to our knowledge, the first study on overoptimization in composite RMs, showing that correlation between component RMs has a significant effect on the locations of these points. We then introduce an approach to solve this issue using constrained reinforcement learning as a means of 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#39057;&#24102;&#26102;&#38388;&#39057;&#29575;&#31070;&#32463;&#32593;&#32476;&#65288;MBTFNet&#65289;&#29992;&#20110;&#27468;&#22768;&#22686;&#24378;&#65292;&#21487;&#20197;&#20174;&#27468;&#21809;&#24405;&#38899;&#20013;&#21435;&#38500;&#32972;&#26223;&#38899;&#20048;&#12289;&#22122;&#22768;&#29978;&#33267;&#21644;&#22768;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#20248;&#20110;&#22810;&#20010;&#26368;&#20808;&#36827;&#30340;&#38899;&#39057;&#22686;&#24378;&#21644;&#38899;&#20048;&#28304;&#20998;&#31163;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.04369</link><description>&lt;p&gt;
MBTFNet&#65306;&#29992;&#20110;&#27468;&#22768;&#22686;&#24378;&#30340;&#22810;&#39057;&#24102;&#26102;&#38388;&#39057;&#29575;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
MBTFNet: Multi-Band Temporal-Frequency Neural Network For Singing Voice Enhancement. (arXiv:2310.04369v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04369
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#39057;&#24102;&#26102;&#38388;&#39057;&#29575;&#31070;&#32463;&#32593;&#32476;&#65288;MBTFNet&#65289;&#29992;&#20110;&#27468;&#22768;&#22686;&#24378;&#65292;&#21487;&#20197;&#20174;&#27468;&#21809;&#24405;&#38899;&#20013;&#21435;&#38500;&#32972;&#26223;&#38899;&#20048;&#12289;&#22122;&#22768;&#29978;&#33267;&#21644;&#22768;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#20248;&#20110;&#22810;&#20010;&#26368;&#20808;&#36827;&#30340;&#38899;&#39057;&#22686;&#24378;&#21644;&#38899;&#20048;&#28304;&#20998;&#31163;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#31070;&#32463;&#38899;&#39057;&#22686;&#24378;&#26041;&#27861;&#20027;&#35201;&#22788;&#29702;&#35821;&#38899;&#21644;&#22122;&#22768;&#28151;&#21512;&#30340;&#24773;&#20917;&#65292;&#36825;&#23545;&#20110;&#27468;&#22768;&#22686;&#24378;&#22330;&#26223;&#24182;&#19981;&#26159;&#26368;&#20248;&#30340;&#12290;&#38899;&#20048;&#28304;&#20998;&#31163;&#27169;&#22411;&#23558;&#20154;&#22768;&#21644;&#20276;&#22863;&#32452;&#20214;&#35270;&#20316;&#21516;&#31561;&#37325;&#35201;&#65292;&#36825;&#21487;&#33021;&#20250;&#38477;&#20302;&#24615;&#33021;&#65292;&#30456;&#23545;&#20110;&#20165;&#32771;&#34385;&#20154;&#22768;&#22686;&#24378;&#30340;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#39057;&#24102;&#26102;&#38388;&#39057;&#29575;&#31070;&#32463;&#32593;&#32476;&#65288;MBTFNet&#65289;&#65292;&#29992;&#20110;&#27468;&#22768;&#22686;&#24378;&#65292;&#21487;&#20197;&#20174;&#27468;&#21809;&#24405;&#38899;&#20013;&#21435;&#38500;&#32972;&#26223;&#38899;&#20048;&#12289;&#22122;&#22768;&#29978;&#33267;&#21644;&#22768;&#12290;MBTFNet&#32467;&#21512;&#20102;&#24102;&#38388;&#21644;&#24102;&#20869;&#24314;&#27169;&#65292;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;&#20840;&#39057;&#24102;&#20449;&#21495;&#12290;&#24341;&#20837;&#20102;&#21452;&#36890;&#36947;&#24314;&#27169;&#26469;&#25193;&#22823;&#27169;&#22411;&#30340;&#24863;&#21463;&#37326;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#22122;&#27604;&#65288;SNR&#65289;&#20272;&#35745;&#30340;&#38544;&#24335;&#20010;&#24615;&#21270;&#22686;&#24378;&#65288;IPE&#65289;&#38454;&#27573;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;MBTFNet&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#26126;&#26174;&#20248;&#20110;&#22810;&#20010;&#26368;&#20808;&#36827;&#30340;&#38899;&#39057;&#22686;&#24378;&#21644;&#38899;&#20048;&#28304;&#20998;&#31163;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
A typical neural speech enhancement (SE) approach mainly handles speech and noise mixtures, which is not optimal for singing voice enhancement scenarios. Music source separation (MSS) models treat vocals and various accompaniment components equally, which may reduce performance compared to the model that only considers vocal enhancement. In this paper, we propose a novel multi-band temporal-frequency neural network (MBTFNet) for singing voice enhancement, which particularly removes background music, noise and even backing vocals from singing recordings. MBTFNet combines inter and intra-band modeling for better processing of full-band signals. Dual-path modeling are introduced to expand the receptive field of the model. We propose an implicit personalized enhancement (IPE) stage based on signal-to-noise ratio (SNR) estimation, which further improves the performance of MBTFNet. Experiments show that our proposed model significantly outperforms several state-of-the-art SE and MSS models.
&lt;/p&gt;</description></item><item><title>MoatPlus&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#20215;&#26684;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#26080;&#30417;&#30563;&#32479;&#35745;&#29305;&#24449;&#21644;&#21382;&#21490;&#20215;&#26684;&#36235;&#21183;&#29983;&#25104;&#19978;&#38480;&#20215;&#26684;&#36793;&#30028;&#65292;&#20197;&#35299;&#20915;&#22312;&#32447;&#24066;&#22330;&#20013;&#30340;&#25968;&#25454;&#36136;&#37327;&#21644;&#38169;&#35823;&#20215;&#26684;&#21457;&#24067;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.04367</link><description>&lt;p&gt;
&#19968;&#20010;&#22823;&#35268;&#27169;&#24066;&#22330;&#20215;&#26684;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
A Marketplace Price Anomaly Detection System at Scale. (arXiv:2310.04367v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04367
&lt;/p&gt;
&lt;p&gt;
MoatPlus&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#20215;&#26684;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#26080;&#30417;&#30563;&#32479;&#35745;&#29305;&#24449;&#21644;&#21382;&#21490;&#20215;&#26684;&#36235;&#21183;&#29983;&#25104;&#19978;&#38480;&#20215;&#26684;&#36793;&#30028;&#65292;&#20197;&#35299;&#20915;&#22312;&#32447;&#24066;&#22330;&#20013;&#30340;&#25968;&#25454;&#36136;&#37327;&#21644;&#38169;&#35823;&#20215;&#26684;&#21457;&#24067;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#24066;&#22330;&#27599;&#22825;&#22312;&#24179;&#21488;&#19978;&#25191;&#34892;&#22823;&#37327;&#30340;&#20215;&#26684;&#26356;&#26032;&#65292;&#36825;&#20123;&#26356;&#26032;&#30001;&#20010;&#20307;&#24066;&#22330;&#21334;&#23478;&#21457;&#36215;&#12290;&#36825;&#31181;&#20215;&#26684;&#27665;&#20027;&#21270;&#38543;&#30528;&#25968;&#25454;&#36136;&#37327;&#30340;&#25361;&#25112;&#32780;&#22686;&#21152;&#12290;&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;&#22312;&#32447;&#38646;&#21806;&#21830;&#65292;&#32570;&#20047;&#38598;&#20013;&#30340;&#38450;&#25252;&#25514;&#26045;&#20250;&#23548;&#33268;&#26356;&#39640;&#30340;&#38169;&#35823;&#20215;&#26684;&#22312;&#32593;&#31449;&#19978;&#21457;&#24067;&#65292;&#20174;&#32780;&#32473;&#39038;&#23458;&#20307;&#39564;&#24102;&#26469;&#24046;&#35780;&#21644;&#28508;&#22312;&#30340;&#25910;&#20837;&#25439;&#22833;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;MoatPlus&#65288;&#20351;&#29992;&#26641;&#12289;&#22522;&#20110;&#37051;&#36817;&#24230;&#30340;&#26631;&#31614;&#20197;&#21450;&#26080;&#30417;&#30563;&#32479;&#35745;&#29305;&#24449;&#30340;&#33945;&#38754;&#26368;&#20248;&#38170;&#28857;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#19981;&#26029;&#22686;&#38271;&#30340;&#24066;&#22330;&#24179;&#21488;&#30340;&#21487;&#25193;&#23637;&#20215;&#26684;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;&#12290;&#30446;&#26631;&#26159;&#21033;&#29992;&#37051;&#36817;&#24230;&#21644;&#21382;&#21490;&#20215;&#26684;&#36235;&#21183;&#30340;&#26080;&#30417;&#30563;&#32479;&#35745;&#29305;&#24449;&#26469;&#29983;&#25104;&#19978;&#38480;&#20215;&#26684;&#36793;&#30028;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#27169;&#22411;&#38598;&#21512;&#26469;&#26816;&#27979;&#22522;&#20110;&#20215;&#26684;&#30340;&#29305;&#24449;&#20013;&#30340;&#24322;&#24120;&#24773;&#20917;&#65292;&#25490;&#38500;&#24322;&#24120;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#20248;&#21270;&#30340;&#21152;&#26435;&#26041;&#26696;&#26469;&#26500;&#24314;&#23454;&#26102;&#23450;&#20215;&#31649;&#36947;&#20013;&#21487;&#38752;&#30340;&#20215;&#26684;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online marketplaces execute large volume of price updates that are initiated by individual marketplace sellers each day on the platform. This price democratization comes with increasing challenges with data quality. Lack of centralized guardrails that are available for a traditional online retailer causes a higher likelihood for inaccurate prices to get published on the website, leading to poor customer experience and potential for revenue loss. We present MoatPlus (Masked Optimal Anchors using Trees, Proximity-based Labeling and Unsupervised Statistical-features), a scalable price anomaly detection framework for a growing marketplace platform. The goal is to leverage proximity and historical price trends from unsupervised statistical features to generate an upper price bound. We build an ensemble of models to detect irregularities in price-based features, exclude irregular features and use optimized weighting scheme to build a reliable price bound in real-time pricing pipeline. We obs
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25674;&#38144;&#30340;&#36125;&#21494;&#26031;&#25512;&#29702;&#20174;&#38590;&#20197;&#22788;&#29702;&#30340;&#21518;&#39564;&#20998;&#24067;&#20013;&#36827;&#34892;&#25277;&#26679;&#30340;&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;&#29983;&#25104;&#27969;&#32593;&#32476;&#26469;&#23454;&#29616;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22312;&#36825;&#20123;&#27169;&#22411;&#20013;&#22788;&#29702;&#25512;&#29702;&#38382;&#39064;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.04363</link><description>&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#25674;&#38144;&#38590;&#20197;&#22788;&#29702;&#30340;&#25512;&#29702;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Amortizing intractable inference in large language models. (arXiv:2310.04363v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04363
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25674;&#38144;&#30340;&#36125;&#21494;&#26031;&#25512;&#29702;&#20174;&#38590;&#20197;&#22788;&#29702;&#30340;&#21518;&#39564;&#20998;&#24067;&#20013;&#36827;&#34892;&#25277;&#26679;&#30340;&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;&#29983;&#25104;&#27969;&#32593;&#32476;&#26469;&#23454;&#29616;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22312;&#36825;&#20123;&#27169;&#22411;&#20013;&#22788;&#29702;&#25512;&#29702;&#38382;&#39064;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#22238;&#24402;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#19979;&#19968;&#20010;&#35789;&#26465;&#20214;&#20998;&#24067;&#26469;&#21387;&#32553;&#20854;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#30693;&#35782;&#65292;&#36825;&#38480;&#21046;&#20102;&#23545;&#35813;&#30693;&#35782;&#30340;&#21487;&#22788;&#29702;&#26597;&#35810;&#20165;&#38480;&#20110;&#20174;&#22836;&#21040;&#23614;&#30340;&#33258;&#22238;&#24402;&#25277;&#26679;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#24863;&#20852;&#36259;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#24207;&#21015;&#24310;&#32493;&#12289;&#22635;&#20805;&#21644;&#20854;&#20182;&#24418;&#24335;&#30340;&#21463;&#32422;&#26463;&#29983;&#25104;&#65292;&#37117;&#28041;&#21450;&#20174;&#38590;&#20197;&#22788;&#29702;&#30340;&#21518;&#39564;&#20998;&#24067;&#20013;&#36827;&#34892;&#25277;&#26679;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#25674;&#38144;&#30340;&#36125;&#21494;&#26031;&#25512;&#29702;&#20174;&#36825;&#20123;&#38590;&#20197;&#22788;&#29702;&#30340;&#21518;&#39564;&#20998;&#24067;&#20013;&#36827;&#34892;&#25277;&#26679;&#26469;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#12290;&#36825;&#31181;&#25674;&#38144;&#36890;&#36807;&#36890;&#36807;&#23547;&#27714;&#22810;&#26679;&#24615;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861; - &#29983;&#25104;&#27969;&#32593;&#32476; (GFlowNets) &#26469;&#24494;&#35843; LLMs &#23454;&#29616;&#12290;&#25105;&#20204;&#20973;&#32463;&#39564;&#35777;&#26126;&#65292;LLM&#24494;&#35843;&#30340;&#36825;&#31181;&#20998;&#24067;&#21305;&#37197;&#33539;&#24335;&#21487;&#20197;&#20316;&#20026;&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;&#21644;&#22870;&#21169;&#26368;&#22823;&#21270;&#31574;&#30053;&#20248;&#21270;&#30340;&#26377;&#25928;&#26367;&#20195;&#26041;&#27861;&#12290;&#20316;&#20026;&#19968;&#20010;&#37325;&#35201;&#24212;&#29992;&#65292;&#25105;&#20204;&#23558;&#24605;&#32500;&#38142;&#25512;&#29702;&#35299;&#37322;&#20026;&#28508;&#21464;&#37327;&#24314;&#27169;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Autoregressive large language models (LLMs) compress knowledge from their training data through next-token conditional distributions. This limits tractable querying of this knowledge to start-to-end autoregressive sampling. However, many tasks of interest -- including sequence continuation, infilling, and other forms of constrained generation -- involve sampling from intractable posterior distributions. We address this limitation by using amortized Bayesian inference to sample from these intractable posteriors. Such amortization is algorithmically achieved by fine-tuning LLMs via diversity-seeking reinforcement learning algorithms: generative flow networks (GFlowNets). We empirically demonstrate that this distribution-matching paradigm of LLM fine-tuning can serve as an effective alternative to maximum-likelihood training and reward-maximizing policy optimization. As an important application, we interpret chain-of-thought reasoning as a latent variable modeling problem and demonstrate 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DSTI&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24378;&#21046;&#28608;&#27963;&#31232;&#30095;&#24615;&#24182;&#23558;Transformer&#27169;&#22411;&#36716;&#25442;&#20026;&#31232;&#30095;&#30340;&#19987;&#23478;&#28151;&#21512;&#29256;&#26412;&#26469;&#26497;&#22823;&#22320;&#38477;&#20302;&#25512;&#29702;&#25104;&#26412;&#12290;&#27492;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;Transformer&#27169;&#22411;&#65292;&#24182;&#23545;&#20934;&#30830;&#24615;&#24433;&#21709;&#24494;&#20046;&#20854;&#24494;&#12290;</title><link>http://arxiv.org/abs/2310.04361</link><description>&lt;p&gt;
&#21033;&#29992;&#21160;&#24577;&#25512;&#29702;&#26469;&#21033;&#29992;Transformer&#28608;&#27963;&#31232;&#30095;&#24615;
&lt;/p&gt;
&lt;p&gt;
Exploiting Transformer Activation Sparsity with Dynamic Inference. (arXiv:2310.04361v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DSTI&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24378;&#21046;&#28608;&#27963;&#31232;&#30095;&#24615;&#24182;&#23558;Transformer&#27169;&#22411;&#36716;&#25442;&#20026;&#31232;&#30095;&#30340;&#19987;&#23478;&#28151;&#21512;&#29256;&#26412;&#26469;&#26497;&#22823;&#22320;&#38477;&#20302;&#25512;&#29702;&#25104;&#26412;&#12290;&#27492;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;Transformer&#27169;&#22411;&#65292;&#24182;&#23545;&#20934;&#30830;&#24615;&#24433;&#21709;&#24494;&#20046;&#20854;&#24494;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#27169;&#22411;&#23613;&#31649;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#30001;&#20110;&#20854;&#39640;&#35745;&#31639;&#38656;&#27714;&#65292;&#24120;&#38754;&#20020;&#23454;&#38469;&#38480;&#21046;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#20013;&#30340;&#26174;&#33879;&#28608;&#27963;&#31232;&#30095;&#24615;&#65292;&#34920;&#26126;&#23384;&#22312;&#20887;&#20313;&#35745;&#31639;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21160;&#24577;&#31232;&#30095;&#21270;Transformer&#25512;&#29702;&#65288;DSTI&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24378;&#21046;&#28608;&#27963;&#31232;&#30095;&#24615;&#24182;&#23558;&#23494;&#38598;&#27169;&#22411;&#36716;&#25442;&#20026;&#20854;&#31232;&#30095;&#30340;&#19987;&#23478;&#28151;&#21512;&#65288;MoE&#65289;&#29256;&#26412;&#65292;&#20174;&#32780;&#26497;&#22823;&#22320;&#38477;&#20302;Transformer&#27169;&#22411;&#30340;&#25512;&#29702;&#25104;&#26412;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21487;&#20197;&#35757;&#32451;&#20986;&#25104;&#21151;&#39044;&#27979;&#27599;&#20010;&#19987;&#23478;&#30456;&#23545;&#36129;&#29486;&#30340;&#23567;&#22411;&#38376;&#25511;&#32593;&#32476;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21160;&#24577;&#30830;&#23450;&#27599;&#20010;&#20196;&#29260;&#25191;&#34892;&#30340;&#19987;&#23478;&#25968;&#37327;&#30340;&#26426;&#21046;&#12290;DSTI&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#22522;&#20110;Transformer&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#24182;&#23545;&#20934;&#30830;&#24615;&#24433;&#21709;&#24494;&#20046;&#20854;&#24494;&#12290;&#38024;&#23545;BERT-base&#20998;&#31867;&#27169;&#22411;&#65292;&#25105;&#20204;&#38477;&#20302;&#20102;&#25512;&#29702;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer models, despite their impressive performance, often face practical limitations due to their high computational requirements. At the same time, previous studies have revealed significant activation sparsity in these models, indicating the presence of redundant computations. In this paper, we propose Dynamic Sparsified Transformer Inference (DSTI), a method that radically reduces the inference cost of Transformer models by enforcing activation sparsity and subsequently transforming a dense model into its sparse Mixture of Experts (MoE) version. We demonstrate that it is possible to train small gating networks that successfully predict the relative contribution of each expert during inference. Furthermore, we introduce a mechanism that dynamically determines the number of executed experts individually for each token. DSTI can be applied to any Transformer-based architecture and has negligible impact on the accuracy. For the BERT-base classification model, we reduce inference c
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#21464;&#25442;&#20316;&#20026;&#35299;&#20915;&#27010;&#29575;&#30005;&#36335;&#30340;&#39044;&#27979;&#38480;&#21046;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#22312;&#26426;&#22120;&#20154;&#22330;&#26223;&#20013;&#23637;&#31034;&#20102;&#35813;&#38480;&#21046;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#20351;&#29992;&#36739;&#23569;&#30340;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26356;&#39640;&#30340;&#20284;&#28982;&#27010;&#29575;&#12290;&#27492;&#22806;&#65292;&#36824;&#35752;&#35770;&#20102;&#22914;&#20309;&#23558;&#21464;&#25442;&#25972;&#21512;&#21040;&#22522;&#20110;&#26641;&#30340;&#23398;&#20064;&#36807;&#31243;&#20013;&#65292;&#24182;&#25351;&#20986;&#20102;&#31934;&#30830;&#25512;&#29702;&#30340;&#19981;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.04354</link><description>&lt;p&gt;
&#22312;&#27010;&#29575;&#30005;&#36335;&#20013;&#25972;&#21512;&#21464;&#25442;
&lt;/p&gt;
&lt;p&gt;
Integrating Transformations in Probabilistic Circuits. (arXiv:2310.04354v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04354
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#21464;&#25442;&#20316;&#20026;&#35299;&#20915;&#27010;&#29575;&#30005;&#36335;&#30340;&#39044;&#27979;&#38480;&#21046;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#22312;&#26426;&#22120;&#20154;&#22330;&#26223;&#20013;&#23637;&#31034;&#20102;&#35813;&#38480;&#21046;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#20351;&#29992;&#36739;&#23569;&#30340;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26356;&#39640;&#30340;&#20284;&#28982;&#27010;&#29575;&#12290;&#27492;&#22806;&#65292;&#36824;&#35752;&#35770;&#20102;&#22914;&#20309;&#23558;&#21464;&#25442;&#25972;&#21512;&#21040;&#22522;&#20110;&#26641;&#30340;&#23398;&#20064;&#36807;&#31243;&#20013;&#65292;&#24182;&#25351;&#20986;&#20102;&#31934;&#30830;&#25512;&#29702;&#30340;&#19981;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#27010;&#29575;&#30005;&#36335;&#30340;&#39044;&#27979;&#38480;&#21046;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#21464;&#25442;&#20316;&#20026;&#20811;&#26381;&#35813;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#26426;&#22120;&#20154;&#22330;&#26223;&#20013;&#35777;&#26126;&#20102;&#36825;&#31181;&#38480;&#21046;&#12290;&#25105;&#20204;&#35748;&#20026;&#29420;&#31435;&#20998;&#37327;&#20998;&#26512;&#26159;&#20445;&#25345;&#27010;&#29575;&#30005;&#36335;&#29420;&#31435;&#24615;&#23646;&#24615;&#30340;&#26377;&#25928;&#24037;&#20855;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#32852;&#21512;&#27010;&#29575;&#26641;&#30340;&#25193;&#23637;&#65292;&#23427;&#20204;&#26159;&#26080;&#27169;&#22411;&#30830;&#23450;&#24615;&#30005;&#36335;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#20351;&#29992;&#36739;&#23569;&#30340;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#19971;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#20197;&#21450;&#30495;&#23454;&#26426;&#22120;&#20154;&#25968;&#25454;&#19978;&#23454;&#29616;&#26356;&#39640;&#30340;&#20284;&#28982;&#27010;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22914;&#20309;&#23558;&#21464;&#25442;&#25972;&#21512;&#21040;&#22522;&#20110;&#26641;&#30340;&#23398;&#20064;&#36807;&#31243;&#20013;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35748;&#20026;&#20351;&#29992;&#36716;&#25442;&#21518;&#30340;&#20998;&#20301;&#21442;&#25968;&#21270;&#20998;&#24067;&#36827;&#34892;&#31934;&#30830;&#25512;&#29702;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#36827;&#34892;&#39640;&#25928;&#30340;&#37319;&#26679;&#21644;&#36817;&#20284;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study addresses the predictive limitation of probabilistic circuits and introduces transformations as a remedy to overcome it. We demonstrate this limitation in robotic scenarios. We motivate that independent component analysis is a sound tool to preserve the independence properties of probabilistic circuits. Our approach is an extension of joint probability trees, which are model-free deterministic circuits. By doing so, it is demonstrated that the proposed approach is able to achieve higher likelihoods while using fewer parameters compared to the joint probability trees on seven benchmark data sets as well as on real robot data. Furthermore, we discuss how to integrate transformations into tree-based learning routines. Finally, we argue that exact inference with transformed quantile parameterized distributions is not tractable. However, our approach allows for efficient sampling and approximate inference.
&lt;/p&gt;</description></item><item><title>COPRA&#26159;&#19968;&#31181;&#38754;&#21521;&#24418;&#24335;&#23450;&#29702;&#35777;&#26126;&#30340;&#35821;&#35328;&#20195;&#29702;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#36890;&#36807;&#36873;&#25321;&#31574;&#30053;&#21644;&#26816;&#32034;&#23450;&#20041;&#21644;&#24341;&#29702;&#36827;&#34892;&#35777;&#26126;&#65292;&#22312;MiniF2F&#22522;&#20934;&#21644;Coq&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.04353</link><description>&lt;p&gt;
&#19968;&#31181;&#38754;&#21521;&#24418;&#24335;&#23450;&#29702;&#35777;&#26126;&#30340;&#35821;&#35328;&#20195;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Language-Agent Approach to Formal Theorem-Proving. (arXiv:2310.04353v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04353
&lt;/p&gt;
&lt;p&gt;
COPRA&#26159;&#19968;&#31181;&#38754;&#21521;&#24418;&#24335;&#23450;&#29702;&#35777;&#26126;&#30340;&#35821;&#35328;&#20195;&#29702;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#36890;&#36807;&#36873;&#25321;&#31574;&#30053;&#21644;&#26816;&#32034;&#23450;&#20041;&#21644;&#24341;&#29702;&#36827;&#34892;&#35777;&#26126;&#65292;&#22312;MiniF2F&#22522;&#20934;&#21644;Coq&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#20195;&#29702;&#26159;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#26469;&#19982;&#22806;&#37096;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#30340;&#26041;&#27861;&#65292;&#26368;&#36817;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#25511;&#21046;&#20219;&#21153;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language agents, which use a large language model (LLM) capable of in-context learning to interact with an external environment, have recently emerged as a promising approach to control tasks. We present the first language-agent approach to formal theorem-proving. Our method, COPRA, uses a high-capacity, black-box LLM (GPT-4) as part of a policy for a stateful backtracking search. During the search, the policy can select proof tactics and retrieve lemmas and definitions from an external database. Each selected tactic is executed in the underlying proof framework, and the execution feedback is used to build the prompt for the next policy invocation. The search also tracks selected information from its history and uses it to reduce hallucinations and unnecessary LLM queries.  We evaluate COPRA on the miniF2F benchmark for Lean and a set of Coq tasks from the Compcert project. On these benchmarks, COPRA is significantly better than one-shot invocations of GPT-4, as well as state-of-the-ar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20851;&#27880;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#20844;&#24179;&#24615;&#30340;&#26041;&#38754;&#65292;&#21457;&#23637;&#20102;&#19968;&#31181;&#22522;&#20110;&#20915;&#31574;&#26641;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#35780;&#20998;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.04352</link><description>&lt;p&gt;
&#29992;&#20110;&#35299;&#37322;&#22522;&#20110;&#26641;&#27169;&#22411;&#21644;&#26367;&#20195;&#27169;&#22411;&#30340;&#20844;&#24179;&#29305;&#24449;&#37325;&#35201;&#24615;&#35780;&#20998;
&lt;/p&gt;
&lt;p&gt;
Fair Feature Importance Scores for Interpreting Tree-Based Methods and Surrogates. (arXiv:2310.04352v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04352
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#20844;&#24179;&#24615;&#30340;&#26041;&#38754;&#65292;&#21457;&#23637;&#20102;&#19968;&#31181;&#22522;&#20110;&#20915;&#31574;&#26641;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#35780;&#20998;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#20445;&#20581;&#12289;&#21009;&#20107;&#21496;&#27861;&#12289;&#22269;&#23478;&#23433;&#20840;&#12289;&#37329;&#34701;&#21644;&#25216;&#26415;&#31561;&#21508;&#20010;&#39046;&#22495;&#65292;&#22823;&#35268;&#27169;&#30340;&#26426;&#22120;&#23398;&#20064;(ML)&#21644;&#20154;&#24037;&#26234;&#33021;(AI)&#31995;&#32479;&#34987;&#37096;&#32626;&#29992;&#20110;&#36827;&#34892;&#20851;&#38190;&#30340;&#25968;&#25454;&#39537;&#21160;&#20915;&#31574;&#12290;&#35768;&#22810;&#20154;&#24819;&#30693;&#36947;&#25105;&#20204;&#26159;&#21542;&#21487;&#20197;&#20449;&#20219;&#36825;&#20123;ML&#31995;&#32479;&#36827;&#34892;&#36825;&#20123;&#20915;&#31574;&#12290;&#23545;&#20110;&#20449;&#20219;ML&#31995;&#32479;&#26469;&#35828;&#65292;&#20004;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#26159;&#24517;&#22791;&#30340;&#65306;&#21487;&#35299;&#37322;&#24615;&#65292;&#21363;&#33021;&#22815;&#29702;&#35299;ML&#31995;&#32479;&#20026;&#20160;&#20040;&#20570;&#20986;&#36825;&#26679;&#30340;&#20915;&#31574;&#65307;&#20844;&#24179;&#24615;&#65292;&#30830;&#20445;ML&#31995;&#32479;&#19981;&#23545;&#26576;&#20123;&#20010;&#20307;&#25110;&#32676;&#20307;&#23384;&#22312;&#20559;&#35265;&#12290;&#21487;&#35299;&#37322;&#24615;&#21644;&#20844;&#24179;&#24615;&#37117;&#24456;&#37325;&#35201;&#65292;&#24182;&#22312;ML&#25991;&#29486;&#20013;&#20998;&#21035;&#24471;&#21040;&#20102;&#22823;&#37327;&#20851;&#27880;&#65292;&#20294;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#24456;&#23569;&#26377;&#26041;&#27861;&#30452;&#25509;&#35299;&#37322;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30528;&#37325;&#35752;&#35770;&#21487;&#33021;&#26159;&#26368;&#27969;&#34892;&#30340;ML&#35299;&#37322;&#31867;&#22411;&#20043;&#19968;&#65306;&#29305;&#24449;&#37325;&#35201;&#24615;&#35780;&#20998;&#12290;&#21463;&#21040;&#22312;&#30693;&#35782;&#33976;&#39311;&#20013;&#20351;&#29992;&#20915;&#31574;&#26641;&#30340;&#21551;&#21457;&#65292;
&lt;/p&gt;
&lt;p&gt;
Across various sectors such as healthcare, criminal justice, national security, finance, and technology, large-scale machine learning (ML) and artificial intelligence (AI) systems are being deployed to make critical data-driven decisions. Many have asked if we can and should trust these ML systems to be making these decisions. Two critical components are prerequisites for trust in ML systems: interpretability, or the ability to understand why the ML system makes the decisions it does, and fairness, which ensures that ML systems do not exhibit bias against certain individuals or groups. Both interpretability and fairness are important and have separately received abundant attention in the ML literature, but so far, there have been very few methods developed to directly interpret models with regard to their fairness. In this paper, we focus on arguably the most popular type of ML interpretation: feature importance scores. Inspired by the use of decision trees in knowledge distillation, w
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#37319;&#29992;&#36136;&#37327;-&#22810;&#26679;&#24615;&#26041;&#27861;&#30340;&#27969;&#27700;&#32447;&#65292;&#29992;&#20110;&#23558;&#29983;&#25104;&#30340;&#25235;&#21462;&#36712;&#36857;&#36866;&#24212;&#21040;&#26032;&#29289;&#20307;&#23039;&#24577;&#12290;&#20351;&#29992;RGB-D&#25968;&#25454;&#27969;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#33258;&#21160;&#26816;&#27979;&#30446;&#26631;&#29289;&#20307;&#12289;&#39044;&#27979;&#20854;&#23039;&#24577;&#65292;&#24182;&#29983;&#25104;&#21487;&#36798;&#21040;&#30340;&#25235;&#21462;&#36712;&#36857;&#12290;</title><link>http://arxiv.org/abs/2310.04349</link><description>&lt;p&gt;
&#20174;&#26576;&#22788;&#21040;&#20219;&#20309;&#22320;&#26041;&#30340;&#23398;&#20064;&#65306;&#25235;&#21462;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Learning to Grasp: from Somewhere to Anywhere. (arXiv:2310.04349v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04349
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#37319;&#29992;&#36136;&#37327;-&#22810;&#26679;&#24615;&#26041;&#27861;&#30340;&#27969;&#27700;&#32447;&#65292;&#29992;&#20110;&#23558;&#29983;&#25104;&#30340;&#25235;&#21462;&#36712;&#36857;&#36866;&#24212;&#21040;&#26032;&#29289;&#20307;&#23039;&#24577;&#12290;&#20351;&#29992;RGB-D&#25968;&#25454;&#27969;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#33258;&#21160;&#26816;&#27979;&#30446;&#26631;&#29289;&#20307;&#12289;&#39044;&#27979;&#20854;&#23039;&#24577;&#65292;&#24182;&#29983;&#25104;&#21487;&#36798;&#21040;&#30340;&#25235;&#21462;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#25235;&#21462;&#20173;&#28982;&#26159;&#19968;&#20010;&#37096;&#20998;&#35299;&#20915;&#30340;&#12289;&#22810;&#23398;&#31185;&#30340;&#38382;&#39064;&#65292;&#22312;&#20854;&#20013;&#25968;&#25454;&#39537;&#21160;&#30340;&#25216;&#26415;&#21457;&#25381;&#30528;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#22870;&#21169;&#30340;&#31232;&#30095;&#24615;&#20351;&#24471;&#33258;&#21160;&#29983;&#25104;&#25235;&#21462;&#25968;&#25454;&#38598;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#38750;&#20256;&#32479;&#24418;&#24577;&#25110;&#39640;&#24230;&#39537;&#21160;&#30340;&#26411;&#31471;&#25191;&#34892;&#22120;&#12290;&#33719;&#24471;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#22823;&#22810;&#25968;&#26041;&#27861;&#20381;&#36182;&#20110;&#20247;&#22810;&#20154;&#24037;&#25552;&#20379;&#30340;&#28436;&#31034;&#25110;&#20005;&#37325;&#24037;&#31243;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36825;&#20123;&#26041;&#27861;&#24456;&#38590;&#25193;&#23637;&#12290;&#26368;&#26032;&#30340;&#36136;&#37327;-&#22810;&#26679;&#24615;&#65288;QD&#65289;&#26041;&#27861;&#30340;&#36827;&#23637;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#19981;&#21516;&#26426;&#22120;&#20154;&#24418;&#24577;&#23398;&#20064;&#29305;&#23450;&#23039;&#21183;&#19979;&#30340;&#29289;&#20307;&#25235;&#21462;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#23558;QD&#29983;&#25104;&#30340;&#36712;&#36857;&#36866;&#24212;&#21040;&#26032;&#29289;&#20307;&#23039;&#24577;&#30340;&#27969;&#27700;&#32447;&#12290;&#20351;&#29992;RGB-D&#25968;&#25454;&#27969;&#65292;&#35270;&#35273;&#27969;&#27700;&#32447;&#39318;&#20808;&#26816;&#27979;&#30446;&#26631;&#29289;&#20307;&#65292;&#39044;&#27979;&#20854;6&#33258;&#30001;&#24230;&#23039;&#24577;&#65292;&#26368;&#21518;&#36319;&#36394;&#23427;&#12290;&#28982;&#21518;&#36890;&#36807;&#23558;&#36712;&#36857;&#30456;&#23545;&#20110;&#29289;&#20307;&#26694;&#26550;&#36827;&#34892;&#25237;&#24433;&#26469;&#33258;&#21160;&#29983;&#25104;&#21487;&#36798;&#21040;&#30340;&#25235;&#21462;&#36712;&#36857;&#12290;&#25968;&#30334;&#20010;&#36712;&#36857;&#24050;&#32463;&#22312;&#23454;&#39564;&#20013;&#37096;&#32626;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robotic grasping is still a partially solved, multidisciplinary problem where data-driven techniques play an increasing role. The sparse nature of rewards make the automatic generation of grasping datasets challenging, especially for unconventional morphologies or highly actuated end-effectors. Most approaches for obtaining large-scale datasets rely on numerous human-provided demonstrations or heavily engineered solutions that do not scale well. Recent advances in Quality-Diversity (QD) methods have investigated how to learn object grasping at a specific pose with different robot morphologies. The present work introduces a pipeline for adapting QD-generated trajectories to new object poses. Using an RGB-D data stream, the vision pipeline first detects the targeted object, predicts its 6-DOF pose, and finally tracks it. An automatically generated reach-and-grasp trajectory can then be adapted by projecting it relatively to the object frame. Hundreds of trajectories have been deployed in
&lt;/p&gt;</description></item><item><title>Neur2RO&#26159;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#39537;&#21160;&#30340;&#20108;&#38454;&#27573;&#40065;&#26834;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20272;&#35745;&#31532;&#20108;&#38454;&#27573;&#38382;&#39064;&#30340;&#20540;&#20989;&#25968;&#65292;&#24182;&#23884;&#20837;&#21040;&#32463;&#20856;&#30340;&#21015;-&#32422;&#26463;&#29983;&#25104;&#31639;&#27861;&#20013;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#27714;&#35299;&#23884;&#22871;&#30340;&#26368;&#23567;-&#26368;&#22823;-&#26368;&#23567;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.04345</link><description>&lt;p&gt;
Neur2RO: &#31070;&#32463;&#20108;&#38454;&#27573;&#40065;&#26834;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Neur2RO: Neural Two-Stage Robust Optimization. (arXiv:2310.04345v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04345
&lt;/p&gt;
&lt;p&gt;
Neur2RO&#26159;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#39537;&#21160;&#30340;&#20108;&#38454;&#27573;&#40065;&#26834;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20272;&#35745;&#31532;&#20108;&#38454;&#27573;&#38382;&#39064;&#30340;&#20540;&#20989;&#25968;&#65292;&#24182;&#23884;&#20837;&#21040;&#32463;&#20856;&#30340;&#21015;-&#32422;&#26463;&#29983;&#25104;&#31639;&#27861;&#20013;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#27714;&#35299;&#23884;&#22871;&#30340;&#26368;&#23567;-&#26368;&#22823;-&#26368;&#23567;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40065;&#26834;&#20248;&#21270;&#25552;&#20379;&#20102;&#19968;&#20010;&#25968;&#23398;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#19981;&#30830;&#23450;&#24615;&#19979;&#24314;&#27169;&#21644;&#35299;&#20915;&#20915;&#31574;&#38382;&#39064;&#12290;&#26412;&#24037;&#20316;&#35299;&#20915;&#20102;&#20108;&#38454;&#27573;&#40065;&#26834;&#20248;&#21270;&#65288;&#20063;&#31216;&#20026;&#21487;&#35843;&#25972;&#40065;&#26834;&#20248;&#21270;&#65289;&#38382;&#39064;&#65292;&#22312;&#19981;&#30830;&#23450;&#24615;&#23454;&#29616;&#20043;&#21069;&#21644;&#20043;&#21518;&#36827;&#34892;&#31532;&#19968;&#38454;&#27573;&#21644;&#31532;&#20108;&#38454;&#27573;&#30340;&#20915;&#31574;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#20010;&#23884;&#22871;&#30340;&#26368;&#23567;-&#26368;&#22823;-&#26368;&#23567;&#20248;&#21270;&#38382;&#39064;&#65292;&#20174;&#35745;&#31639;&#19978;&#26469;&#35828;&#26159;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#23588;&#20854;&#26159;&#24403;&#20915;&#31574;&#26159;&#31163;&#25955;&#30340;&#26102;&#20505;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Neur2RO&#65292;&#36825;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#21015;-&#32422;&#26463;&#29983;&#25104;&#65288;CCG&#65289;&#30340;&#23454;&#20363;&#31639;&#27861;&#65292;CCG&#26159;&#20108;&#38454;&#27573;&#40065;&#26834;&#20248;&#21270;&#30340;&#32463;&#20856;&#36845;&#20195;&#31639;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26469;&#23398;&#20064;&#20272;&#35745;&#31532;&#20108;&#38454;&#27573;&#38382;&#39064;&#30340;&#20540;&#20989;&#25968;&#65292;&#36825;&#31181;&#26550;&#26500;&#26131;&#20110;&#20248;&#21270;&#12290;&#23558;&#25105;&#20204;&#30340;&#31070;&#32463;&#32593;&#32476;&#23884;&#20837;&#21040;CCG&#31639;&#27861;&#20013;&#65292;&#21487;&#20197;&#24555;&#36895;&#24471;&#21040;&#39640;&#36136;&#37327;&#30340;&#35299;&#65292;&#36825;&#22312;&#20004;&#20010;&#20108;&#38454;&#27573;&#40065;&#26834;&#20248;&#21270;&#22522;&#20934;&#27979;&#35797;&#65288;&#32972;&#21253;&#38382;&#39064;&#21644;&#36164;&#26412;&#39044;&#31639;&#65289;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robust optimization provides a mathematical framework for modeling and solving decision-making problems under worst-case uncertainty. This work addresses two-stage robust optimization (2RO) problems (also called adjustable robust optimization), wherein first-stage and second-stage decisions are made before and after uncertainty is realized, respectively. This results in a nested min-max-min optimization problem which is extremely challenging computationally, especially when the decisions are discrete. We propose Neur2RO, an efficient machine learning-driven instantiation of column-and-constraint generation (CCG), a classical iterative algorithm for 2RO. Specifically, we learn to estimate the value function of the second-stage problem via a novel neural network architecture that is easy to optimize over by design. Embedding our neural network into CCG yields high-quality solutions quickly as evidenced by experiments on two 2RO benchmarks, knapsack and capital budgeting. For knapsack, Ne
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;NAEPro&#27169;&#22411;&#65292;&#36890;&#36807;&#20989;&#25968;&#20960;&#20309;&#24341;&#23548;&#30340;&#26041;&#27861;&#20849;&#21516;&#35774;&#35745;&#34507;&#30333;&#36136;&#24207;&#21015;&#21644;&#32467;&#26500;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#27688;&#22522;&#37240;&#24674;&#22797;&#29575;&#12289;TM&#20998;&#25968;&#21644;RMSD&#31561;&#25351;&#26631;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2310.04343</link><description>&lt;p&gt;
&#20989;&#25968;&#20960;&#20309;&#24341;&#23548;&#30340;&#34507;&#30333;&#36136;&#24207;&#21015;&#21644;&#39592;&#26550;&#32467;&#26500;&#20849;&#21516;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Functional Geometry Guided Protein Sequence and Backbone Structure Co-Design. (arXiv:2310.04343v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;NAEPro&#27169;&#22411;&#65292;&#36890;&#36807;&#20989;&#25968;&#20960;&#20309;&#24341;&#23548;&#30340;&#26041;&#27861;&#20849;&#21516;&#35774;&#35745;&#34507;&#30333;&#36136;&#24207;&#21015;&#21644;&#32467;&#26500;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#27688;&#22522;&#37240;&#24674;&#22797;&#29575;&#12289;TM&#20998;&#25968;&#21644;RMSD&#31561;&#25351;&#26631;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34507;&#30333;&#36136;&#26159;&#20960;&#20046;&#25152;&#26377;&#29983;&#29289;&#20307;&#20013;&#36127;&#36131;&#22522;&#26412;&#21151;&#33021;&#30340;&#22823;&#20998;&#23376;&#12290;&#35774;&#35745;&#21512;&#29702;&#30340;&#20855;&#26377;&#26399;&#26395;&#21151;&#33021;&#30340;&#34507;&#30333;&#36136;&#33267;&#20851;&#37325;&#35201;&#12290;&#34507;&#30333;&#36136;&#30340;&#24207;&#21015;&#21644;&#32467;&#26500;&#23494;&#20999;&#30456;&#20851;&#65292;&#23427;&#20204;&#20849;&#21516;&#20915;&#23450;&#20102;&#20854;&#21151;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;NAEPro&#65292;&#19968;&#31181;&#22522;&#20110;&#33258;&#21160;&#26816;&#27979;&#21040;&#30340;&#21151;&#33021;&#20301;&#28857;&#20849;&#21516;&#35774;&#35745;&#34507;&#30333;&#36136;&#24207;&#21015;&#21644;&#32467;&#26500;&#30340;&#27169;&#22411;&#12290;NAEPro&#37319;&#29992;&#20102;&#27880;&#24847;&#21147;&#21644;&#31561;&#21464;&#23618;&#30340;&#20132;&#38169;&#32593;&#32476;&#65292;&#21487;&#20197;&#25429;&#25417;&#24207;&#21015;&#20013;&#30340;&#20840;&#23616;&#30456;&#20851;&#24615;&#20197;&#21450;&#19977;&#32500;&#31354;&#38388;&#20013;&#26368;&#36817;&#27688;&#22522;&#37240;&#30340;&#23616;&#37096;&#24433;&#21709;&#12290;&#36825;&#31181;&#26550;&#26500;&#22312;&#20004;&#20010;&#23618;&#38754;&#19978;&#20419;&#36827;&#20102;&#26377;&#25928;&#32780;&#32463;&#27982;&#30340;&#20449;&#24687;&#20256;&#36882;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#34507;&#30333;&#36136;&#25968;&#25454;&#38598;&#65288;&#946;-&#20869;&#37232;&#33018;&#37238;&#21644;&#32908;&#32418;&#34507;&#30333;&#65289;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#21644;&#20960;&#20010;&#24378;&#31454;&#20105;&#22522;&#32447;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#25152;&#26377;&#31454;&#20105;&#23545;&#25163;&#20013;&#22987;&#32456;&#23454;&#29616;&#20102;&#26368;&#39640;&#30340;&#27688;&#22522;&#37240;&#24674;&#22797;&#29575;&#12289;TM&#20998;&#25968;&#21644;&#26368;&#20302;&#30340;RMSD&#12290;&#36825;&#20123;&#21457;&#29616;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Proteins are macromolecules responsible for essential functions in almost all living organisms. Designing reasonable proteins with desired functions is crucial. A protein's sequence and structure are strongly correlated and they together determine its function. In this paper, we propose NAEPro, a model to jointly design Protein sequence and structure based on automatically detected functional sites. NAEPro is powered by an interleaving network of attention and equivariant layers, which can capture global correlation in a whole sequence and local influence from nearest amino acids in three dimensional (3D) space. Such an architecture facilitates effective yet economic message passing at two levels. We evaluate our model and several strong baselines on two protein datasets, $\beta$-lactamase and myoglobin. Experimental results show that our model consistently achieves the highest amino acid recovery rate, TM-score, and the lowest RMSD among all competitors. These findings prove the capab
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20110;&#26399;&#26435;&#23450;&#20215;&#21644;&#23545;&#20914;&#30340;&#26041;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;Q&#23398;&#20064;Black Scholes&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#20197;&#38750;&#27169;&#22411;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#36827;&#34892;&#26399;&#26435;&#23450;&#20215;&#21644;&#23545;&#20914;&#65292;&#24182;&#22312;&#19981;&#21516;&#29366;&#24577;&#21464;&#37327;&#21644;&#22330;&#26223;&#19979;&#34920;&#29616;&#20986;&#31283;&#20581;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.04336</link><description>&lt;p&gt;
&#23558;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20110;&#26399;&#26435;&#23450;&#20215;&#21644;&#23545;&#20914;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Applying Reinforcement Learning to Option Pricing and Hedging. (arXiv:2310.04336v1 [q-fin.CP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04336
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20110;&#26399;&#26435;&#23450;&#20215;&#21644;&#23545;&#20914;&#30340;&#26041;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;Q&#23398;&#20064;Black Scholes&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#20197;&#38750;&#27169;&#22411;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#36827;&#34892;&#26399;&#26435;&#23450;&#20215;&#21644;&#23545;&#20914;&#65292;&#24182;&#22312;&#19981;&#21516;&#29366;&#24577;&#21464;&#37327;&#21644;&#22330;&#26223;&#19979;&#34920;&#29616;&#20986;&#31283;&#20581;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#27010;&#36848;&#20102;&#36817;&#24180;&#26469;&#22312;&#37329;&#34701;&#20135;&#21697;&#23450;&#20215;&#21644;&#23545;&#20914;&#20013;&#24212;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#30001;Halperin&#65288;2017&#65289;&#25552;&#20986;&#30340;Q&#23398;&#20064;Black Scholes&#26041;&#27861;&#30340;&#35814;&#32454;&#35299;&#37322;&#12290;&#36825;&#31181;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#23558;&#20256;&#32479;&#30340;Black and Scholes&#65288;1973&#65289;&#27169;&#22411;&#19982;&#26032;&#39062;&#30340;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#30456;&#32467;&#21512;&#65292;&#20197;&#23436;&#20840;&#30340;&#38750;&#27169;&#22411;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#36827;&#34892;&#26399;&#26435;&#23450;&#20215;&#21644;&#23545;&#20914;&#12290;&#26412;&#25991;&#36824;&#30740;&#31350;&#20102;&#35813;&#31639;&#27861;&#22312;&#19981;&#21516;&#29366;&#24577;&#21464;&#37327;&#21644;&#22330;&#26223;&#19979;&#23545;&#27431;&#24335;&#30475;&#36300;&#26399;&#26435;&#30340;&#34920;&#29616;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#19981;&#21516;&#27874;&#21160;&#29575;&#21644;&#23545;&#20914;&#39057;&#29575;&#27700;&#24179;&#19979;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#26399;&#26435;&#24179;&#20215;&#28857;&#30340;&#27700;&#24179;&#19978;&#34920;&#29616;&#20986;&#31283;&#20581;&#30340;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#35813;&#31639;&#27861;&#21253;&#25324;&#27604;&#20363;&#20132;&#26131;&#25104;&#26412;&#65292;&#34920;&#26126;&#19981;&#21516;&#29366;&#24577;&#21464;&#37327;&#30340;&#32479;&#35745;&#29305;&#24615;&#23545;&#25910;&#30410;&#21644;&#25439;&#22833;&#20135;&#29983;&#19981;&#21516;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
This thesis provides an overview of the recent advances in reinforcement learning in pricing and hedging financial instruments, with a primary focus on a detailed explanation of the Q-Learning Black Scholes approach, introduced by Halperin (2017). This reinforcement learning approach bridges the traditional Black and Scholes (1973) model with novel artificial intelligence algorithms, enabling option pricing and hedging in a completely model-free and data-driven way. This paper also explores the algorithm's performance under different state variables and scenarios for a European put option. The results reveal that the model is an accurate estimator under different levels of volatility and hedging frequency. Moreover, this method exhibits robust performance across various levels of option's moneyness. Lastly, the algorithm incorporates proportional transaction costs, indicating diverse impacts on profit and loss, affected by different statistical properties of the state variables.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26174;&#33879;&#24615;&#24341;&#23548;&#30340;&#38544;&#21547;&#20851;&#32852;&#22238;&#25918;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#23384;&#20648;&#21644;&#26816;&#32034;&#37325;&#35201;&#30340;&#25968;&#25454;&#37096;&#20998;&#65292;&#27169;&#20223;&#20102;&#20154;&#31867;&#35760;&#24518;&#36807;&#31243;&#65292;&#19982;&#20256;&#32479;&#30340;&#23436;&#25972;&#25968;&#25454;&#23384;&#20648;&#26041;&#27861;&#19981;&#21516;&#12290;</title><link>http://arxiv.org/abs/2310.04334</link><description>&lt;p&gt;
&#22522;&#20110;&#26174;&#33879;&#24615;&#24341;&#23548;&#30340;&#38544;&#21547;&#20851;&#32852;&#22238;&#25918;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Saliency-Guided Hidden Associative Replay for Continual Learning. (arXiv:2310.04334v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04334
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26174;&#33879;&#24615;&#24341;&#23548;&#30340;&#38544;&#21547;&#20851;&#32852;&#22238;&#25918;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#23384;&#20648;&#21644;&#26816;&#32034;&#37325;&#35201;&#30340;&#25968;&#25454;&#37096;&#20998;&#65292;&#27169;&#20223;&#20102;&#20154;&#31867;&#35760;&#24518;&#36807;&#31243;&#65292;&#19982;&#20256;&#32479;&#30340;&#23436;&#25972;&#25968;&#25454;&#23384;&#20648;&#26041;&#27861;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#26159;&#19979;&#19968;&#20195;&#20154;&#24037;&#26234;&#33021;&#20013;&#19968;&#20010;&#26032;&#20852;&#30340;&#39046;&#22495;&#65292;&#19987;&#27880;&#20110;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20197;&#31867;&#20284;&#20110;&#20154;&#31867;&#23398;&#20064;&#30340;&#20219;&#21153;&#24207;&#21015;&#12290;&#34429;&#28982;&#25345;&#32493;&#23398;&#20064;&#30456;&#27604;&#20256;&#32479;&#30340;&#30417;&#30563;&#23398;&#20064;&#20855;&#26377;&#20248;&#21183;&#65292;&#20294;&#20854;&#20027;&#35201;&#25361;&#25112;&#20173;&#28982;&#26159;&#25269;&#25239;&#28798;&#38590;&#24615;&#36951;&#24536;&#24182;&#30830;&#20445;&#22312;&#21518;&#32493;&#23398;&#20064;&#36807;&#31243;&#20013;&#20445;&#30041;&#20808;&#21069;&#20219;&#21153;&#30340;&#30456;&#20851;&#30693;&#35782;&#12290;&#22312;&#21508;&#31181;&#35299;&#20915;&#26041;&#27861;&#20013;&#65292;&#22522;&#20110;&#22238;&#25918;&#30340;&#26041;&#27861;&#24050;&#32463;&#25104;&#20026;&#21331;&#36234;&#30340;&#26041;&#27861;&#65292;&#27169;&#20223;&#20102;&#29983;&#29289;&#35760;&#24518;&#26426;&#21046;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#20869;&#23384;&#20351;&#29992;&#26041;&#38754;&#24448;&#24448;&#26159;&#23494;&#38598;&#30340;&#65292;&#36890;&#24120;&#20445;&#30041;&#25972;&#20010;&#25968;&#25454;&#26679;&#26412;&#65292;&#36825;&#31181;&#26041;&#27861;&#19982;&#20154;&#31867;&#36873;&#25321;&#24615;&#35760;&#24518;&#20445;&#30041;&#26174;&#33879;&#32463;&#39564;&#30340;&#26041;&#24335;&#19981;&#19968;&#33268;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#19968;&#20123;&#30740;&#31350;&#25506;&#32034;&#20102;&#21482;&#22312;&#24773;&#26223;&#35760;&#24518;&#20013;&#23384;&#20648;&#37325;&#35201;&#25968;&#25454;&#37096;&#20998;&#30340;&#26041;&#27861;&#65292;&#20294;&#37096;&#20998;&#25968;&#25454;&#30340;&#22266;&#26377;&#24615;&#36136;&#38656;&#35201;&#21019;&#26032;&#30340;&#26816;&#32034;&#26426;&#21046;&#12290;&#24403;&#21069;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22914;&#22270;&#20687;&#20462;&#22797;&#65292;&#35797;&#22270;&#36890;&#36807;&#37096;&#20998;&#32447;&#32034;&#26469;&#36817;&#20284;&#23436;&#25972;&#25968;&#25454;&#37325;&#24314;&#65292;&#36825;&#31181;&#26041;&#27861;&#19982;&#30495;&#23454;&#30340;&#20154;&#31867;&#35760;&#24518;&#36807;&#31243;&#19981;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual Learning is a burgeoning domain in next-generation AI, focusing on training neural networks over a sequence of tasks akin to human learning. While CL provides an edge over traditional supervised learning, its central challenge remains to counteract catastrophic forgetting and ensure the retention of prior tasks during subsequent learning. Amongst various strategies to tackle this, replay based methods have emerged as preeminent, echoing biological memory mechanisms. However, these methods are memory intensive, often preserving entire data samples, an approach inconsistent with humans selective memory retention of salient experiences. While some recent works have explored the storage of only significant portions of data in episodic memory, the inherent nature of partial data necessitates innovative retrieval mechanisms. Current solutions, like inpainting, approximate full data reconstruction from partial cues, a method that diverges from genuine human memory processes. Address
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#22788;&#29702;&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#23545;&#32463;&#39564;&#36951;&#25022;&#20316;&#20026;&#26367;&#20195;&#30340;&#20934;&#30830;&#24615;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2310.04328</link><description>&lt;p&gt;
&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#30340;&#40065;&#26834;&#25439;&#22833;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Robust Losses for Decision-Focused Learning. (arXiv:2310.04328v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04328
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#22788;&#29702;&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#23545;&#32463;&#39564;&#36951;&#25022;&#20316;&#20026;&#26367;&#20195;&#30340;&#20934;&#30830;&#24615;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#20110;&#36827;&#34892;&#31163;&#25955;&#20915;&#31574;&#30340;&#20248;&#21270;&#27169;&#22411;&#36890;&#24120;&#21253;&#21547;&#22522;&#20110;&#39044;&#27979;&#20272;&#35745;&#30340;&#19978;&#19979;&#25991;&#30456;&#20851;&#19988;&#19981;&#30830;&#23450;&#30340;&#21442;&#25968;&#12290;&#20026;&#20102;&#32771;&#34385;&#22522;&#20110;&#39044;&#27979;&#30340;&#20915;&#31574;&#36136;&#37327;&#65292;&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#65288;&#31471;&#21040;&#31471;&#39044;&#27979;-&#20248;&#21270;&#65289;&#26088;&#22312;&#35757;&#32451;&#39044;&#27979;&#27169;&#22411;&#20197;&#26368;&#23567;&#21270;&#36951;&#25022;&#65292;&#21363;&#36890;&#36807;&#36827;&#34892;&#27425;&#20248;&#20915;&#31574;&#32780;&#20135;&#29983;&#30340;&#25439;&#22833;&#12290;&#23613;&#31649;&#36825;&#20010;&#25439;&#22833;&#20989;&#25968;&#21487;&#33021;&#26159;&#38750;&#20984;&#21644;&#19968;&#33324;&#19981;&#21487;&#23548;&#30340;&#65292;&#20294;&#24050;&#32463;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#23398;&#20064;&#26041;&#27861;&#26469;&#26368;&#23567;&#21270;&#26399;&#26395;&#25439;&#22833;&#65292;&#20351;&#29992;&#32463;&#39564;&#25439;&#22833;&#20316;&#20026;&#26367;&#20195;&#12290;&#28982;&#32780;&#65292;&#32463;&#39564;&#36951;&#25022;&#21487;&#33021;&#19981;&#26159;&#19968;&#20010;&#26377;&#25928;&#30340;&#26367;&#20195;&#65292;&#22240;&#20026;&#20248;&#21270;&#27169;&#22411;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#23548;&#33268;&#32463;&#39564;&#36951;&#25022;&#19982;&#26399;&#26395;&#36951;&#25022;&#22312;&#26399;&#26395;&#19978;&#19981;&#30456;&#31561;&#12290;&#20026;&#20102;&#35828;&#26126;&#36825;&#31181;&#19981;&#31561;&#24335;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#26426;&#20250;&#24615;&#21644;&#35748;&#30693;&#24615;&#19981;&#30830;&#23450;&#24615;&#23545;&#32463;&#39564;&#36951;&#25022;&#20316;&#20026;&#26367;&#20195;&#30340;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#35813;&#20989;&#25968;&#33021;&#22815;&#26356;&#22909;&#22320;&#36817;&#20284;&#26399;&#26395;&#36951;&#25022;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#25351;&#23548;&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimization models used to make discrete decisions often contain uncertain parameters that are context-dependent and are estimated through prediction. To account for the quality of the decision made based on the prediction, decision-focused learning (end-to-end predict-then-optimize) aims at training the predictive model to minimize regret, i.e., the loss incurred by making a suboptimal decision. Despite the challenge of this loss function being possibly non-convex and in general non-differentiable, effective gradient-based learning approaches have been proposed to minimize the expected loss, using the empirical loss as a surrogate. However, empirical regret can be an ineffective surrogate because the uncertainty in the optimization model makes the empirical regret unequal to the expected regret in expectation. To illustrate the impact of this inequality, we evaluate the effect of aleatoric and epistemic uncertainty on the accuracy of empirical regret as a surrogate. Next, we propose 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26368;&#20339;&#20248;&#20808;&#33258;&#24213;&#21521;&#19978;&#25628;&#32034;&#31639;&#27861;&#65292;&#31216;&#20026;Bee Search&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#20197;&#26368;&#20339;&#20248;&#20808;&#30340;&#26041;&#24335;&#25191;&#34892;&#25104;&#26412;&#23548;&#21521;&#33258;&#24213;&#21521;&#19978;&#21512;&#25104;&#65292;&#32780;&#19988;&#19981;&#20250;&#20002;&#22833;&#27169;&#22411;&#25552;&#20379;&#30340;&#26377;&#29992;&#20449;&#24687;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#26356;&#22909;&#20351;&#29992;&#29616;&#26377;&#25104;&#26412;&#27169;&#22411;&#25552;&#20379;&#30340;&#20449;&#24687;&#30340;&#26032;&#25104;&#26412;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2310.04327</link><description>&lt;p&gt;
&#22522;&#20110;&#26368;&#20339;&#20248;&#20808;&#33258;&#24213;&#21521;&#19978;&#25628;&#32034;&#30340;&#31243;&#24207;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Program Synthesis with Best-First Bottom-Up Search. (arXiv:2310.04327v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04327
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26368;&#20339;&#20248;&#20808;&#33258;&#24213;&#21521;&#19978;&#25628;&#32034;&#31639;&#27861;&#65292;&#31216;&#20026;Bee Search&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#20197;&#26368;&#20339;&#20248;&#20808;&#30340;&#26041;&#24335;&#25191;&#34892;&#25104;&#26412;&#23548;&#21521;&#33258;&#24213;&#21521;&#19978;&#21512;&#25104;&#65292;&#32780;&#19988;&#19981;&#20250;&#20002;&#22833;&#27169;&#22411;&#25552;&#20379;&#30340;&#26377;&#29992;&#20449;&#24687;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#26356;&#22909;&#20351;&#29992;&#29616;&#26377;&#25104;&#26412;&#27169;&#22411;&#25552;&#20379;&#30340;&#20449;&#24687;&#30340;&#26032;&#25104;&#26412;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#20808;&#30340;&#25104;&#26412;&#23548;&#21521;&#33258;&#24213;&#21521;&#19978;&#25628;&#32034;&#31639;&#27861;&#20351;&#29992;&#25104;&#26412;&#20989;&#25968;&#26469;&#25351;&#23548;&#25628;&#32034;&#31243;&#24207;&#21512;&#25104;&#20219;&#21153;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#25104;&#26412;&#23548;&#21521;&#33258;&#24213;&#21521;&#19978;&#25628;&#32034;&#31639;&#27861;&#23384;&#22312;&#19968;&#20010;&#20849;&#21516;&#30340;&#38382;&#39064;&#65306;&#23427;&#20204;&#21487;&#33021;&#20250;&#20002;&#22833;&#27169;&#22411;&#25552;&#20379;&#30340;&#26377;&#29992;&#20449;&#24687;&#65292;&#24182;&#19988;&#26080;&#27861;&#25353;&#29031;&#25104;&#26412;&#20989;&#25968;&#30340;&#26368;&#20339;&#20248;&#20808;&#39034;&#24207;&#36827;&#34892;&#25628;&#32034;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26368;&#20339;&#20248;&#20808;&#33258;&#24213;&#21521;&#19978;&#25628;&#32034;&#31639;&#27861;&#65292;&#31216;&#20026;Bee Search&#65292;&#23427;&#19981;&#20250;&#20002;&#22833;&#20449;&#24687;&#65292;&#24182;&#33021;&#22815;&#20197;&#26368;&#20339;&#20248;&#20808;&#30340;&#26041;&#24335;&#25191;&#34892;&#25104;&#26412;&#23548;&#21521;&#33258;&#24213;&#21521;&#19978;&#21512;&#25104;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;Bee Search&#20197;&#30456;&#23545;&#20110;&#29983;&#25104;&#31243;&#24207;&#30340;&#26368;&#20339;&#20248;&#20808;&#39034;&#24207;&#25191;&#34892;&#26368;&#20339;&#20248;&#20808;&#25628;&#32034;&#65292;&#21363;&#23427;&#19981;&#20250;&#22312;&#20869;&#23384;&#20013;&#21019;&#24314;&#27604;&#35299;&#20915;&#26041;&#26696;&#31243;&#24207;&#26356;&#26114;&#36149;&#30340;&#31243;&#24207;&#12290;&#23427;&#36890;&#36807;&#22312;&#31243;&#24207;&#25104;&#26412;&#30340;&#25277;&#35937;&#31354;&#38388;&#20013;&#36827;&#34892;&#25628;&#32034;&#26469;&#23454;&#29616;&#29983;&#25104;&#30340;&#26368;&#20339;&#20248;&#20808;&#39034;&#24207;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#26356;&#22909;&#20351;&#29992;&#29616;&#26377;&#25104;&#26412;&#27169;&#22411;&#25552;&#20379;&#30340;&#20449;&#24687;&#30340;&#26032;&#25104;&#26412;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cost-guided bottom-up search (BUS) algorithms use a cost function to guide the search to solve program synthesis tasks. In this paper, we show that current state-of-the-art cost-guided BUS algorithms suffer from a common problem: they can lose useful information given by the model and fail to perform the search in a best-first order according to a cost function. We introduce a novel best-first bottom-up search algorithm, which we call Bee Search, that does not suffer information loss and is able to perform cost-guided bottom-up synthesis in a best-first manner. Importantly, Bee Search performs best-first search with respect to the generation of programs, i.e., it does not even create in memory programs that are more expensive than the solution program. It attains best-first ordering with respect to generation by performing a search in an abstract space of program costs. We also introduce a new cost function that better uses the information provided by an existing cost model. Empirical 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35843;&#33410;&#30340;&#40065;&#26834;&#24615;&#24378;&#21270;&#23398;&#20064;&#65288;AR2L&#65289;&#26694;&#26550;&#26469;&#35299;&#20915;&#22312;&#32447;3D&#35013;&#31665;&#38382;&#39064;&#65292;&#35813;&#26694;&#26550;&#20801;&#35768;&#26377;&#25928;&#22320;&#35843;&#33410;&#40065;&#26834;&#24615;&#26435;&#37325;&#20197;&#23454;&#29616;&#25152;&#38656;&#30340;&#24615;&#33021;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2310.04323</link><description>&lt;p&gt;
&#21487;&#35843;&#33410;&#30340;&#40065;&#26834;&#24615;&#24378;&#21270;&#23398;&#20064;&#22312;&#22312;&#32447;3D&#35013;&#31665;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Adjustable Robust Reinforcement Learning for Online 3D Bin Packing. (arXiv:2310.04323v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04323
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35843;&#33410;&#30340;&#40065;&#26834;&#24615;&#24378;&#21270;&#23398;&#20064;&#65288;AR2L&#65289;&#26694;&#26550;&#26469;&#35299;&#20915;&#22312;&#32447;3D&#35013;&#31665;&#38382;&#39064;&#65292;&#35813;&#26694;&#26550;&#20801;&#35768;&#26377;&#25928;&#22320;&#35843;&#33410;&#40065;&#26834;&#24615;&#26435;&#37325;&#20197;&#23454;&#29616;&#25152;&#38656;&#30340;&#24615;&#33021;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#26377;&#25928;&#30340;&#31574;&#30053;&#26469;&#35299;&#20915;&#22312;&#32447;3D&#35013;&#31665;&#38382;&#39064;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#30340;&#25361;&#25112;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#20256;&#20837;&#31665;&#23376;&#24207;&#21015;&#30340;&#19981;&#21487;&#39044;&#27979;&#24615;&#21644;&#20005;&#26684;&#30340;&#29289;&#29702;&#32422;&#26463;&#12290;&#23613;&#31649;&#24403;&#21069;&#29992;&#20110;&#22312;&#32447;3D&#35013;&#31665;&#38382;&#39064;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#26041;&#27861;&#22312;&#20248;&#21270;&#28508;&#22312;&#30340;&#31665;&#23376;&#24207;&#21015;&#20998;&#24067;&#30340;&#24179;&#22343;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#24448;&#24448;&#26080;&#27861;&#22788;&#29702;&#19968;&#20123;&#26368;&#22351;&#24773;&#20917;&#12290;&#26631;&#20934;&#30340;&#40065;&#26834;&#24615;DRL&#31639;&#27861;&#24448;&#24448;&#36807;&#20998;&#20248;&#21270;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#65292;&#20174;&#32780;&#29306;&#29298;&#20102;&#22312;&#27491;&#24120;&#38382;&#39064;&#23454;&#20363;&#20998;&#24067;&#19979;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#25490;&#21015;&#30340;&#25915;&#20987;&#32773;&#26469;&#30740;&#31350;&#35299;&#20915;&#22312;&#32447;3D&#35013;&#31665;&#38382;&#39064;&#30340;DRL&#26041;&#27861;&#21644;&#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#23454;&#38469;&#40065;&#26834;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35843;&#33410;&#30340;&#40065;&#26834;&#24615;&#24378;&#21270;&#23398;&#20064;&#65288;Adjustable Robust Reinforcement Learning&#65292;AR2L&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20801;&#35768;&#26377;&#25928;&#22320;&#35843;&#33410;&#40065;&#26834;&#24615;&#26435;&#37325;&#20197;&#23454;&#29616;&#25152;&#38656;&#30340;&#24615;&#33021;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Designing effective policies for the online 3D bin packing problem (3D-BPP) has been a long-standing challenge, primarily due to the unpredictable nature of incoming box sequences and stringent physical constraints. While current deep reinforcement learning (DRL) methods for online 3D-BPP have shown promising results in optimizing average performance over an underlying box sequence distribution, they often fail in real-world settings where some worst-case scenarios can materialize. Standard robust DRL algorithms tend to overly prioritize optimizing the worst-case performance at the expense of performance under normal problem instance distribution. To address these issues, we first introduce a permutation-based attacker to investigate the practical robustness of both DRL-based and heuristic methods proposed for solving online 3D-BPP. Then, we propose an adjustable robust reinforcement learning (AR2L) framework that allows efficient adjustment of robustness weights to achieve the desired
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26377;&#38480;&#30417;&#30563;&#19979;&#30340;&#28508;&#22312;&#22270;&#25512;&#29702;&#38382;&#39064;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24674;&#22797;&#20851;&#38190;&#36830;&#25509;&#21644;&#34917;&#20805;&#32570;&#22833;&#30417;&#30563;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.04314</link><description>&lt;p&gt;
&#26377;&#38480;&#30417;&#30563;&#19979;&#30340;&#28508;&#22312;&#22270;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Latent Graph Inference with Limited Supervision. (arXiv:2310.04314v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04314
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26377;&#38480;&#30417;&#30563;&#19979;&#30340;&#28508;&#22312;&#22270;&#25512;&#29702;&#38382;&#39064;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24674;&#22797;&#20851;&#38190;&#36830;&#25509;&#21644;&#34917;&#20805;&#32570;&#22833;&#30417;&#30563;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28508;&#22312;&#22270;&#25512;&#29702; (LGI) &#30340;&#30446;&#26631;&#26159;&#20174;&#25968;&#25454;&#29305;&#24449;&#20013;&#21516;&#26102;&#23398;&#20064;&#28508;&#22312;&#30340;&#22270;&#32467;&#26500;&#21644;&#33410;&#28857;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;LGI&#26041;&#27861;&#36890;&#24120;&#36973;&#21463;&#30563;&#23548;&#21294;&#20047;&#38382;&#39064;&#65292;&#23548;&#33268;&#22823;&#37327;&#30340;&#36793;&#26435;&#37325;&#22312;&#27809;&#26377;&#35821;&#20041;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#65292;&#19981;&#33021;&#23545;&#35757;&#32451;&#25439;&#22833;&#20570;&#20986;&#36129;&#29486;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#32570;&#20047;&#30417;&#30563;&#30340;&#26435;&#37325;&#21487;&#33021;&#20915;&#23450;&#27979;&#35797;&#26679;&#26412;&#30340;&#39044;&#27979;&#65292;&#20294;&#24182;&#19981;&#19968;&#23450;&#26159;&#35821;&#20041;&#19978;&#26368;&#20248;&#30340;&#65292;&#23548;&#33268;&#27867;&#21270;&#33021;&#21147;&#24046;&#12290;&#26412;&#25991;&#35266;&#23519;&#21040;&#36825;&#20010;&#38382;&#39064;&#23454;&#38469;&#19978;&#26159;&#30001;&#20110;&#22270;&#31232;&#30095;&#21270;&#25805;&#20316;&#23548;&#33268;&#30340;&#65292;&#20005;&#37325;&#30772;&#22351;&#20102;&#20851;&#38190;&#33410;&#28857;&#21644;&#26631;&#35760;&#33410;&#28857;&#20043;&#38388;&#24314;&#31435;&#30340;&#37325;&#35201;&#36830;&#25509;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#24674;&#22797;&#21463;&#25439;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#20026;&#26356;&#22909;&#30340;LGI&#34917;&#20805;&#32570;&#22833;&#30340;&#30417;&#30563;&#12290;&#20851;&#38190;&#25361;&#25112;&#22312;&#20110;&#35782;&#21035;&#20851;&#38190;&#33410;&#28857;&#24182;&#24674;&#22797;&#25439;&#22351;&#30340;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;&#20851;&#38190;&#33410;&#28857;&#23450;&#20041;&#20026;$k$-hop&#39269;&#39295;&#33410;&#28857;&#65292;&#21487;&#20197;&#36890;&#36807;&#39269;&#39295;&#29366;&#20917;&#35745;&#31639;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Latent graph inference (LGI) aims to jointly learn the underlying graph structure and node representations from data features. However, existing LGI methods commonly suffer from the issue of supervision starvation, where massive edge weights are learned without semantic supervision and do not contribute to the training loss. Consequently, these supervision-starved weights, which may determine the predictions of testing samples, cannot be semantically optimal, resulting in poor generalization. In this paper, we observe that this issue is actually caused by the graph sparsification operation, which severely destroys the important connections established between pivotal nodes and labeled ones. To address this, we propose to restore the corrupted affinities and replenish the missed supervision for better LGI. The key challenge then lies in identifying the critical nodes and recovering the corrupted affinities. We begin by defining the pivotal nodes as $k$-hop starved nodes, which can be id
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#20165;&#35299;&#30721;&#22120;&#20391;&#20449;&#24687;&#30340;&#20998;&#24067;&#24335;&#28145;&#24230;&#32852;&#21512;&#28304;&#20449;&#36947;&#32534;&#30721;&#26041;&#27861;&#65292;&#22312;&#20302;&#24310;&#36831;&#22270;&#20687;&#20256;&#36755;&#20013;&#23454;&#29616;&#20102;&#25913;&#36827;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#20302;&#20449;&#36947;&#20449;&#22122;&#27604;&#21644;&#23567;&#24102;&#23485;&#27604;&#30340;&#24773;&#20917;&#19979;&#12290;</title><link>http://arxiv.org/abs/2310.04311</link><description>&lt;p&gt;
&#24102;&#26377;&#20165;&#35299;&#30721;&#22120;&#20391;&#20449;&#24687;&#30340;&#20998;&#24067;&#24335;&#28145;&#24230;&#32852;&#21512;&#28304;&#20449;&#36947;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Distributed Deep Joint Source-Channel Coding with Decoder-Only Side Information. (arXiv:2310.04311v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04311
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#20165;&#35299;&#30721;&#22120;&#20391;&#20449;&#24687;&#30340;&#20998;&#24067;&#24335;&#28145;&#24230;&#32852;&#21512;&#28304;&#20449;&#36947;&#32534;&#30721;&#26041;&#27861;&#65292;&#22312;&#20302;&#24310;&#36831;&#22270;&#20687;&#20256;&#36755;&#20013;&#23454;&#29616;&#20102;&#25913;&#36827;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#20302;&#20449;&#36947;&#20449;&#22122;&#27604;&#21644;&#23567;&#24102;&#23485;&#27604;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#21482;&#26377;&#25509;&#25910;&#26041;&#26377;&#30456;&#20851;&#36741;&#21161;&#20449;&#24687;&#30340;&#22122;&#22768;&#26080;&#32447;&#20449;&#36947;&#19978;&#36827;&#34892;&#20302;&#24310;&#36831;&#22270;&#20687;&#20256;&#36755;&#65288;Wyner-Ziv&#24773;&#26223;&#65289;&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#32852;&#21512;&#28304;&#20449;&#36947;&#32534;&#30721;&#65288;JSCC&#65289;&#26041;&#27861;&#24320;&#21457;&#23454;&#38469;&#26041;&#26696;&#65292;&#36825;&#22312;&#23454;&#38469;&#26377;&#38480;&#22359;&#38271;&#24230;&#30340;&#24773;&#20917;&#19979;&#24050;&#34987;&#35777;&#26126;&#20248;&#20110;&#20256;&#32479;&#30340;&#20998;&#31163;&#24335;&#26041;&#27861;&#65292;&#24182;&#22312;&#20449;&#36947;&#36136;&#37327;&#26041;&#38754;&#25552;&#20379;&#20102;&#20248;&#38597;&#30340;&#36864;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#22312;&#25509;&#25910;&#22120;&#31471;&#30340;&#22810;&#20010;&#38454;&#27573;&#23558;&#20165;&#35299;&#30721;&#22120;&#20391;&#20449;&#24687;&#34701;&#20837;&#20854;&#20013;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;&#25972;&#21512;&#20102;&#36741;&#21161;&#20449;&#24687;&#65292;&#22312;&#21508;&#31181;&#30072;&#21464;&#20934;&#21017;&#19979;&#65292;&#22312;&#25152;&#26377;&#20449;&#36947;&#22122;&#22768;&#27700;&#24179;&#19978;&#37117;&#23454;&#29616;&#20102;&#25913;&#36827;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#20302;&#20449;&#36947;&#20449;&#22122;&#27604;&#65288;SNR&#65289;&#21644;&#23567;&#24102;&#23485;&#27604;&#65288;BR&#65289;&#30340;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#28304;&#20195;&#30721;&#65292;&#20197;&#20415;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider low-latency image transmission over a noisy wireless channel when correlated side information is present only at the receiver side (the Wyner-Ziv scenario). In particular, we are interested in developing practical schemes using a data-driven joint source-channel coding (JSCC) approach, which has been previously shown to outperform conventional separation-based approaches in the practical finite blocklength regimes, and to provide graceful degradation with channel quality. We propose a novel neural network architecture that incorporates the decoder-only side information at multiple stages at the receiver side. Our results demonstrate that the proposed method succeeds in integrating the side information, yielding improved performance at all channel noise levels in terms of the various distortion criteria considered here, especially at low channel signal-to-noise ratios (SNRs) and small bandwidth ratios (BRs). We also provide the source code of the proposed method to enable fu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#27169;&#22411;&#39537;&#21160;&#30340;&#21464;&#20998;&#37325;&#24314;&#19982;&#29420;&#31435;&#23398;&#20064;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25805;&#20316;&#31526;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;ADMM&#25554;&#20837;&#27861;&#30340;&#28151;&#21512;PET&#37325;&#24314;&#31639;&#27861;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#20854;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.04299</link><description>&lt;p&gt;
&#25910;&#25947;&#30340;ADMM&#25554;&#20837;&#27861;PET&#22270;&#20687;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Convergent ADMM Plug and Play PET Image Reconstruction. (arXiv:2310.04299v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04299
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#27169;&#22411;&#39537;&#21160;&#30340;&#21464;&#20998;&#37325;&#24314;&#19982;&#29420;&#31435;&#23398;&#20064;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25805;&#20316;&#31526;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;ADMM&#25554;&#20837;&#27861;&#30340;&#28151;&#21512;PET&#37325;&#24314;&#31639;&#27861;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#20854;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#27169;&#22411;&#39537;&#21160;&#21464;&#20998;&#37325;&#24314;&#21644;&#24212;&#29992;&#29420;&#31435;&#23398;&#20064;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25805;&#20316;&#31526;&#65288;DNN&#65289;&#30340;&#28151;&#21512;PET&#37325;&#24314;&#31639;&#27861;&#65292;&#37319;&#29992;ADMM&#25554;&#20837;&#27861;&#26694;&#26550;&#12290;&#26681;&#25454;&#26368;&#36817;&#22312;&#20248;&#21270;&#26041;&#38754;&#30340;&#32467;&#26524;&#65292;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#23545;&#32593;&#32476;&#21442;&#25968;&#26045;&#21152;&#39069;&#22806;&#30340;&#32422;&#26463;&#21487;&#20197;&#23454;&#29616;&#31639;&#27861;&#30340;&#22266;&#23450;&#28857;&#25910;&#25947;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36825;&#26679;&#19968;&#20010;ADMM&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#22312;&#36924;&#30495;&#30340;[18F]-FDG&#21512;&#25104;&#33041;&#37096;&#26816;&#26597;&#20013;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#26696;&#30830;&#23454;&#23454;&#39564;&#19978;&#25910;&#25947;&#21040;&#19968;&#20010;&#26377;&#24847;&#20041;&#30340;&#22266;&#23450;&#28857;&#12290;&#24403;&#22312;DNN&#30340;&#23398;&#20064;&#36807;&#31243;&#20013;&#26410;&#26045;&#21152;&#25152;&#25552;&#20986;&#30340;&#32422;&#26463;&#26102;&#65292;&#23454;&#39564;&#35266;&#23519;&#21040;&#25152;&#25552;&#20986;&#30340;ADMM&#31639;&#27861;&#26080;&#27861;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we investigate hybrid PET reconstruction algorithms based on coupling a model-based variational reconstruction and the application of a separately learnt Deep Neural Network operator (DNN) in an ADMM Plug and Play framework. Following recent results in optimization, fixed point convergence of the scheme can be achieved by enforcing an additional constraint on network parameters during learning. We propose such an ADMM algorithm and show in a realistic [18F]-FDG synthetic brain exam that the proposed scheme indeed lead experimentally to convergence to a meaningful fixed point. When the proposed constraint is not enforced during learning of the DNN, the proposed ADMM algorithm was observed experimentally not to converge.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24178;&#39044;&#22806;&#25512;&#30340;&#20219;&#21153;&#65292;&#35777;&#26126;&#20102;&#21487;&#35782;&#21035;&#30340;&#34920;&#31034;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#65292;&#21363;&#20351;&#24178;&#39044;&#23545;&#32467;&#26524;&#20135;&#29983;&#38750;&#32447;&#24615;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.04295</link><description>&lt;p&gt;
&#35782;&#21035;&#24178;&#39044;&#22806;&#25512;&#30340;&#34920;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Identifying Representations for Intervention Extrapolation. (arXiv:2310.04295v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24178;&#39044;&#22806;&#25512;&#30340;&#20219;&#21153;&#65292;&#35777;&#26126;&#20102;&#21487;&#35782;&#21035;&#30340;&#34920;&#31034;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#65292;&#21363;&#20351;&#24178;&#39044;&#23545;&#32467;&#26524;&#20135;&#29983;&#38750;&#32447;&#24615;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35782;&#21035;&#21644;&#22240;&#26524;&#20851;&#31995;&#34920;&#31034;&#23398;&#20064;&#30340;&#21069;&#25552;&#26159;&#25913;&#36827;&#24403;&#21069;&#30340;&#34920;&#31034;&#23398;&#20064;&#33539;&#24335;&#65292;&#20197;&#25552;&#39640;&#27867;&#21270;&#24615;&#25110;&#40065;&#26834;&#24615;&#12290;&#23613;&#31649;&#22312;&#21487;&#35782;&#21035;&#24615;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#36817;&#26399;&#30340;&#36827;&#23637;&#65292;&#20294;&#20173;&#38656;&#35201;&#26356;&#22810;&#29702;&#35770;&#32467;&#26524;&#26469;&#35777;&#26126;&#36825;&#20123;&#26041;&#27861;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#20855;&#20307;&#20248;&#21183;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#24178;&#39044;&#22806;&#25512;&#30340;&#20219;&#21153;&#65306;&#39044;&#27979;&#24178;&#39044;&#22914;&#20309;&#24433;&#21709;&#32467;&#26524;&#65292;&#21363;&#20351;&#36825;&#20123;&#24178;&#39044;&#22312;&#35757;&#32451;&#26102;&#27809;&#26377;&#35266;&#23519;&#21040;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21487;&#35782;&#21035;&#30340;&#34920;&#31034;&#33021;&#22815;&#20026;&#36825;&#20010;&#20219;&#21153;&#25552;&#20379;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#20351;&#24178;&#39044;&#23545;&#32467;&#26524;&#20135;&#29983;&#38750;&#32447;&#24615;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#35774;&#32622;&#21253;&#25324;&#19968;&#20010;&#32467;&#26524;Y&#65292;&#35266;&#23519;&#21040;&#30340;&#29305;&#24449;X&#65292;&#36825;&#20123;&#29305;&#24449;&#26159;&#28508;&#22312;&#29305;&#24449;Z&#30340;&#38750;&#32447;&#24615;&#36716;&#25442;&#65292;&#20197;&#21450;&#24433;&#21709;Z&#30340;&#22806;&#29983;&#34892;&#20026;&#21464;&#37327;A&#12290;&#24178;&#39044;&#22806;&#25512;&#30340;&#30446;&#26631;&#26159;&#39044;&#27979;&#20301;&#20110;&#35757;&#32451;&#25903;&#25345;&#20043;&#22806;&#30340;A&#19978;&#30340;&#24178;&#39044;&#22914;&#20309;&#24433;&#21709;Y&#12290;&#22312;&#36825;&#37324;&#65292;&#22806;&#25512;&#21464;&#24471;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
The premise of identifiable and causal representation learning is to improve the current representation learning paradigm in terms of generalizability or robustness. Despite recent progress in questions of identifiability, more theoretical results demonstrating concrete advantages of these methods for downstream tasks are needed. In this paper, we consider the task of intervention extrapolation: predicting how interventions affect an outcome, even when those interventions are not observed at training time, and show that identifiable representations can provide an effective solution to this task even if the interventions affect the outcome non-linearly. Our setup includes an outcome Y, observed features X, which are generated as a non-linear transformation of latent features Z, and exogenous action variables A, which influence Z. The objective of intervention extrapolation is to predict how interventions on A that lie outside the training support of A affect Y. Here, extrapolation becom
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19971;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#65292;&#20998;&#21035;&#26159;ToyMix&#12289;LargeMix&#21644;UltraLarge&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#22312;&#35268;&#27169;&#21644;&#26377;&#30417;&#30563;&#26631;&#31614;&#30340;&#22810;&#26679;&#24615;&#26041;&#38754;&#31361;&#30772;&#20102;&#30028;&#38480;&#65292;&#28085;&#30422;&#20102;&#36817;1&#20159;&#20010;&#20998;&#23376;&#21644;3000&#22810;&#20010;&#31232;&#30095;&#23450;&#20041;&#30340;&#20219;&#21153;&#65292;&#24635;&#35745;&#36229;&#36807;130&#20159;&#20010;&#20851;&#20110;&#37327;&#23376;&#21644;&#29983;&#29289;&#24615;&#36136;&#30340;&#26631;&#31614;&#12290;</title><link>http://arxiv.org/abs/2310.04292</link><description>&lt;p&gt;
&#38754;&#21521;&#22823;&#35268;&#27169;&#22810;&#20219;&#21153;&#25968;&#25454;&#38598;&#30340;&#20998;&#23376;&#23398;&#20064;&#22522;&#30784;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Foundational Models for Molecular Learning on Large-Scale Multi-Task Datasets. (arXiv:2310.04292v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04292
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19971;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#65292;&#20998;&#21035;&#26159;ToyMix&#12289;LargeMix&#21644;UltraLarge&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#22312;&#35268;&#27169;&#21644;&#26377;&#30417;&#30563;&#26631;&#31614;&#30340;&#22810;&#26679;&#24615;&#26041;&#38754;&#31361;&#30772;&#20102;&#30028;&#38480;&#65292;&#28085;&#30422;&#20102;&#36817;1&#20159;&#20010;&#20998;&#23376;&#21644;3000&#22810;&#20010;&#31232;&#30095;&#23450;&#20041;&#30340;&#20219;&#21153;&#65292;&#24635;&#35745;&#36229;&#36807;130&#20159;&#20010;&#20851;&#20110;&#37327;&#23376;&#21644;&#29983;&#29289;&#24615;&#36136;&#30340;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#39044;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#22312;&#22810;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22312;&#20998;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#25968;&#25454;&#38598;&#36890;&#24120;&#26159;&#25163;&#24037;&#31574;&#21010;&#30340;&#65292;&#22240;&#27492;&#35268;&#27169;&#36739;&#23567;&#65292;&#32570;&#20047;&#24102;&#26377;&#26631;&#35760;&#29305;&#24449;&#21644;&#31649;&#29702;&#36825;&#20123;&#25968;&#25454;&#38598;&#30340;&#20195;&#30721;&#24211;&#65292;&#21046;&#32422;&#20102;&#22522;&#30784;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19971;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#65292;&#20998;&#20026;&#19977;&#20010;&#19981;&#21516;&#30340;&#31867;&#21035;&#65306;ToyMix&#12289;LargeMix&#21644;UltraLarge&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#22312;&#35268;&#27169;&#21644;&#26377;&#30417;&#30563;&#26631;&#31614;&#30340;&#22810;&#26679;&#24615;&#26041;&#38754;&#31361;&#30772;&#20102;&#30028;&#38480;&#12290;&#23427;&#20204;&#28085;&#30422;&#20102;&#36817;1&#20159;&#20010;&#20998;&#23376;&#21644;3000&#22810;&#20010;&#31232;&#30095;&#23450;&#20041;&#30340;&#20219;&#21153;&#65292;&#24635;&#35745;&#36229;&#36807;130&#20159;&#20010;&#20851;&#20110;&#37327;&#23376;&#21644;&#29983;&#29289;&#24615;&#36136;&#30340;&#26631;&#31614;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#28857;&#25968;&#37327;&#26159;&#24191;&#27867;&#20351;&#29992;&#30340;OGB-LSC PCQM4Mv2&#25968;&#25454;&#38598;&#30340;300&#20493;&#65292;&#20063;&#26159;&#20165;&#21253;&#21547;&#37327;&#23376;&#25968;&#25454;&#30340;QM1B&#25968;&#25454;&#38598;&#30340;13&#20493;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#25903;&#25345;&#22522;&#20110;&#25105;&#20204;&#25552;&#20986;&#30340;&#22522;&#30784;&#27169;&#22411;&#30340;&#24320;&#21457;&#65292;
&lt;/p&gt;
&lt;p&gt;
Recently, pre-trained foundation models have enabled significant advancements in multiple fields. In molecular machine learning, however, where datasets are often hand-curated, and hence typically small, the lack of datasets with labeled features, and codebases to manage those datasets, has hindered the development of foundation models. In this work, we present seven novel datasets categorized by size into three distinct categories: ToyMix, LargeMix and UltraLarge. These datasets push the boundaries in both the scale and the diversity of supervised labels for molecular learning. They cover nearly 100 million molecules and over 3000 sparsely defined tasks, totaling more than 13 billion individual labels of both quantum and biological nature. In comparison, our datasets contain 300 times more data points than the widely used OGB-LSC PCQM4Mv2 dataset, and 13 times more than the quantum-only QM1B dataset. In addition, to support the development of foundational models based on our proposed 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#25968;&#30340;&#23545;&#25239;&#29983;&#25104;&#26694;&#26550;&#65288;ScoreAG&#65289;&#65292;&#21487;&#20197;&#29983;&#25104;&#36229;&#36807;$\ell_p$-&#33539;&#25968;&#32422;&#26463;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#24182;&#36890;&#36807;&#22270;&#20687;&#36716;&#25442;&#25110;&#26032;&#22270;&#20687;&#21512;&#25104;&#30340;&#26041;&#27861;&#20445;&#25345;&#22270;&#20687;&#30340;&#26680;&#24515;&#35821;&#20041;&#65292;&#22823;&#22823;&#22686;&#24378;&#20102;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.04285</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#20998;&#25968;&#30340;&#23545;&#25239;&#22270;&#20687;&#29983;&#25104;&#35780;&#20272;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Assessing Robustness via Score-Based Adversarial Image Generation. (arXiv:2310.04285v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04285
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#25968;&#30340;&#23545;&#25239;&#29983;&#25104;&#26694;&#26550;&#65288;ScoreAG&#65289;&#65292;&#21487;&#20197;&#29983;&#25104;&#36229;&#36807;$\ell_p$-&#33539;&#25968;&#32422;&#26463;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#24182;&#36890;&#36807;&#22270;&#20687;&#36716;&#25442;&#25110;&#26032;&#22270;&#20687;&#21512;&#25104;&#30340;&#26041;&#27861;&#20445;&#25345;&#22270;&#20687;&#30340;&#26680;&#24515;&#35821;&#20041;&#65292;&#22823;&#22823;&#22686;&#24378;&#20102;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#23545;&#25239;&#25915;&#20987;&#21644;&#38450;&#24481;&#37117;&#38598;&#20013;&#22312;&#23567;&#30340;$\ell_p$-&#33539;&#25968;&#32422;&#26463;&#20869;&#30340;&#25200;&#21160;&#19978;&#12290;&#28982;&#32780;&#65292;$\ell_p$&#23041;&#32961;&#27169;&#22411;&#26080;&#27861;&#25429;&#25417;&#21040;&#25152;&#26377;&#30456;&#20851;&#30340;&#20445;&#30041;&#35821;&#20041;&#30340;&#25200;&#21160;&#65292;&#22240;&#27492;&#65292;&#40065;&#26834;&#24615;&#35780;&#20272;&#30340;&#33539;&#22260;&#26159;&#26377;&#38480;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#20998;&#25968;&#30340;&#23545;&#25239;&#29983;&#25104;&#65288;ScoreAG&#65289;&#65292;&#19968;&#31181;&#21033;&#29992;&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#36827;&#23637;&#26469;&#29983;&#25104;&#36229;&#36807;$\ell_p$-&#33539;&#25968;&#32422;&#26463;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#26080;&#38480;&#21046;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#20811;&#26381;&#20102;&#23427;&#20204;&#30340;&#23616;&#38480;&#24615;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;ScoreAG&#22312;&#29983;&#25104;&#36924;&#30495;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#26102;&#20445;&#25345;&#22270;&#20687;&#30340;&#26680;&#24515;&#35821;&#20041;&#65292;&#21487;&#20197;&#36890;&#36807;&#36716;&#25442;&#29616;&#26377;&#22270;&#20687;&#25110;&#23436;&#20840;&#20174;&#38646;&#24320;&#22987;&#21512;&#25104;&#26032;&#22270;&#20687;&#30340;&#26041;&#24335;&#23454;&#29616;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#21033;&#29992;ScoreAG&#30340;&#29983;&#25104;&#33021;&#21147;&#26469;&#20928;&#21270;&#22270;&#20687;&#65292;&#20174;&#32463;&#39564;&#19978;&#22686;&#24378;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#22823;&#37327;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;ScoreAG&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#30340;&#24615;&#33021;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most adversarial attacks and defenses focus on perturbations within small $\ell_p$-norm constraints. However, $\ell_p$ threat models cannot capture all relevant semantic-preserving perturbations, and hence, the scope of robustness evaluations is limited. In this work, we introduce Score-Based Adversarial Generation (ScoreAG), a novel framework that leverages the advancements in score-based generative models to generate adversarial examples beyond $\ell_p$-norm constraints, so-called unrestricted adversarial examples, overcoming their limitations. Unlike traditional methods, ScoreAG maintains the core semantics of images while generating realistic adversarial examples, either by transforming existing images or synthesizing new ones entirely from scratch. We further exploit the generative capability of ScoreAG to purify images, empirically enhancing the robustness of classifiers. Our extensive empirical evaluation demonstrates that ScoreAG matches the performance of state-of-the-art atta
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20027;&#25104;&#20998;&#20998;&#26512;&#20013;&#19981;&#31934;&#30830;&#28040;&#38500;&#27861;&#30340;&#35823;&#24046;&#20256;&#25773;&#38382;&#39064;&#65292;&#32473;&#20986;&#20102;&#20004;&#20010;&#20027;&#35201;&#32467;&#26524;</title><link>http://arxiv.org/abs/2310.04283</link><description>&lt;p&gt;
&#20851;&#20110;&#19981;&#31934;&#30830;&#28040;&#38500;&#27861;&#22312;&#20027;&#25104;&#20998;&#20998;&#26512;&#20013;&#30340;&#35823;&#24046;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
On the Error-Propagation of Inexact Deflation for Principal Component Analysis. (arXiv:2310.04283v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04283
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20027;&#25104;&#20998;&#20998;&#26512;&#20013;&#19981;&#31934;&#30830;&#28040;&#38500;&#27861;&#30340;&#35823;&#24046;&#20256;&#25773;&#38382;&#39064;&#65292;&#32473;&#20986;&#20102;&#20004;&#20010;&#20027;&#35201;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#26159;&#25968;&#25454;&#20998;&#26512;&#20013;&#24120;&#29992;&#30340;&#24037;&#20855;&#65292;&#23588;&#20854;&#26159;&#22312;&#39640;&#32500;&#25968;&#25454;&#24773;&#20917;&#19979;&#12290;PCA&#26088;&#22312;&#25214;&#21040;&#30001;&#25152;&#35859;&#8220;&#20027;&#25104;&#20998;&#8221;&#25152;&#24352;&#25104;&#30340;&#23376;&#31354;&#38388;&#65292;&#36825;&#20123;&#20027;&#25104;&#20998;&#26368;&#33021;&#35299;&#37322;&#25968;&#25454;&#38598;&#30340;&#26041;&#24046;&#12290;&#28040;&#38500;&#27861;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#20803;&#31639;&#27861;&#65292;&#29992;&#20110;&#21457;&#29616;&#36825;&#26679;&#30340;&#23376;&#31354;&#38388;&#65292;&#23427;&#20174;&#26368;&#37325;&#35201;&#30340;&#20027;&#25104;&#20998;&#24320;&#22987;&#39034;&#24207;&#22320;&#25214;&#21040;&#27599;&#20010;&#20027;&#25104;&#20998;&#65292;&#30452;&#21040;&#25214;&#21040;&#36739;&#19981;&#37325;&#35201;&#30340;&#20027;&#25104;&#20998;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#39034;&#24207;&#24615;&#36136;&#65292;&#30001;&#20110;&#19981;&#23436;&#20840;&#20272;&#35745;&#20027;&#25104;&#20998;&#24341;&#20837;&#30340;&#25968;&#20540;&#35823;&#24046; - &#20363;&#22914;&#65292;&#30001;&#20110;&#27492;&#36807;&#31243;&#20013;&#30340;&#25968;&#20540;&#36817;&#20284; - &#20250;&#38543;&#30528;&#28040;&#38500;&#30340;&#36827;&#34892;&#32780;&#20256;&#25773;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#31687;&#22312;&#25968;&#23398;&#19978;&#23545;&#19981;&#31934;&#30830;&#28040;&#38500;&#27861;&#30340;&#35823;&#24046;&#20256;&#25773;&#36827;&#34892;&#20102;&#29305;&#24615;&#21270;&#30340;&#24037;&#20316;&#65292;&#36825;&#26159;&#26412;&#25991;&#30340;&#20851;&#38190;&#36129;&#29486;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20004;&#20010;&#20027;&#35201;&#32467;&#26524;&#65306;$ i&#65289;$&#24403;&#29992;&#20110;&#26597;&#25214;&#20027;&#35201;&#29305;&#24449;&#21521;&#37327;&#30340;&#23376;&#20363;&#31243;&#26159;&#27867;&#22411;&#30340;&#26102;&#20505;&#65292;&#20197;&#21450;$ ii&#65289;$
&lt;/p&gt;
&lt;p&gt;
Principal Component Analysis (PCA) is a popular tool in data analysis, especially when the data is high-dimensional. PCA aims to find subspaces, spanned by the so-called \textit{principal components}, that best explain the variance in the dataset. The deflation method is a popular meta-algorithm -used to discover such subspaces -- that sequentially finds individual principal components, starting from the most important one and working its way towards the less important ones. However, due to its sequential nature, the numerical error introduced by not estimating principal components exactly -- e.g., due to numerical approximations through this process -- propagates, as deflation proceeds. To the best of our knowledge, this is the first work that mathematically characterizes the error propagation of the inexact deflation method, and this is the key contribution of this paper. We provide two main results: $i)$ when the sub-routine for finding the leading eigenvector is generic, and $ii)
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#29992;&#20110;&#23454;&#26102;&#39044;&#27979;&#22810;&#32423;&#36724;&#21521;&#21387;&#32553;&#26426;&#22312;&#29123;&#27668;&#36718;&#26426;&#20013;&#23574;&#38388;&#38553;&#21464;&#21270;&#23545;&#27668;&#21160;&#24615;&#33021;&#24433;&#21709;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#19982;CFD&#22522;&#20934;&#30456;&#23218;&#32654;&#30340;&#23454;&#26102;&#20934;&#30830;&#24615;&#65292;&#26041;&#20415;&#38598;&#25104;&#21040;&#29123;&#27668;&#36718;&#26426;&#30340;&#21046;&#36896;&#21644;&#26500;&#24314;&#36807;&#31243;&#20013;&#36827;&#34892;&#24615;&#33021;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2310.04264</link><description>&lt;p&gt;
C(NN)FD -- &#22810;&#32423;&#36724;&#21521;&#21387;&#32553;&#26426;&#27668;&#21160;&#24615;&#33021;&#20013;&#23574;&#38388;&#38553;&#21464;&#21270;&#30340;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
C(NN)FD -- deep learning predictions of tip clearance variations on multi-stage axial compressors aerodynamic performance. (arXiv:2310.04264v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04264
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#29992;&#20110;&#23454;&#26102;&#39044;&#27979;&#22810;&#32423;&#36724;&#21521;&#21387;&#32553;&#26426;&#22312;&#29123;&#27668;&#36718;&#26426;&#20013;&#23574;&#38388;&#38553;&#21464;&#21270;&#23545;&#27668;&#21160;&#24615;&#33021;&#24433;&#21709;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#19982;CFD&#22522;&#20934;&#30456;&#23218;&#32654;&#30340;&#23454;&#26102;&#20934;&#30830;&#24615;&#65292;&#26041;&#20415;&#38598;&#25104;&#21040;&#29123;&#27668;&#36718;&#26426;&#30340;&#21046;&#36896;&#21644;&#26500;&#24314;&#36807;&#31243;&#20013;&#36827;&#34892;&#24615;&#33021;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36804;&#20170;&#20026;&#27490;&#65292;&#23558;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24212;&#29992;&#20110;&#35832;&#22914;CFD&#65288;&#35745;&#31639;&#27969;&#20307;&#21147;&#23398;&#65289;&#31561;&#29289;&#29702;&#27169;&#25311;&#22312;&#24037;&#19994;&#19978;&#30340;&#37325;&#35201;&#24615;&#26377;&#38480;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#32423;&#36724;&#21521;&#21387;&#32553;&#26426;&#22312;&#29123;&#27668;&#36718;&#26426;&#20013;&#23574;&#38388;&#38553;&#21464;&#21270;&#23545;&#27668;&#21160;&#24615;&#33021;&#30340;&#23454;&#26102;&#39044;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#30340;&#24320;&#21457;&#21644;&#24212;&#29992;&#12290;&#25152;&#25552;&#20986;&#30340;C(NN)FD&#26550;&#26500;&#32463;&#35777;&#26126;&#21487;&#25193;&#23637;&#33267;&#24037;&#19994;&#24212;&#29992;&#65292;&#24182;&#36798;&#21040;&#19982;CFD&#22522;&#20934;&#30456;&#23218;&#32654;&#30340;&#23454;&#26102;&#20934;&#30830;&#24615;&#12290;&#37096;&#32626;&#30340;&#27169;&#22411;&#21487;&#36731;&#26494;&#38598;&#25104;&#21040;&#29123;&#27668;&#36718;&#26426;&#30340;&#21046;&#36896;&#21644;&#26500;&#24314;&#36807;&#31243;&#20013;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#20998;&#26512;&#35780;&#20272;&#24615;&#33021;&#24433;&#21709;&#24182;&#28508;&#22312;&#20943;&#23569;&#26114;&#36149;&#29289;&#29702;&#27979;&#35797;&#35201;&#27714;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
Application of deep learning methods to physical simulations such as CFD (Computational Fluid Dynamics), have been so far of limited industrial relevance. This paper demonstrates the development and application of a deep learning framework for real-time predictions of the impact of tip clearance variations on the aerodynamic performance of multi-stage axial compressors in gas turbines. The proposed C(NN)FD architecture is proven to be scalable to industrial applications, and achieves in real-time accuracy comparable to the CFD benchmark. The deployed model, is readily integrated within the manufacturing and build process of gas turbines, thus providing the opportunity to analytically assess the impact on performance and potentially reduce requirements for expensive physical tests.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#29992;&#20110;&#23398;&#20064;&#34920;&#31034;&#30340;&#19981;&#21516;&#36741;&#21161;&#20219;&#21153;&#65292;&#36890;&#36807;&#22312;&#36830;&#32493;&#25511;&#21046;&#22522;&#20934;&#29615;&#22659;&#19978;&#35757;&#32451;&#25968;&#30334;&#20010;&#26234;&#33021;&#20307;&#30340;&#23454;&#39564;&#65292;&#21457;&#29616;&#20351;&#29992;&#36741;&#21161;&#20219;&#21153;&#30340;&#34920;&#31034;&#23398;&#20064;&#23545;&#29615;&#22659;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#22238;&#25253;&#26377;&#30410;&#12290;</title><link>http://arxiv.org/abs/2310.04241</link><description>&lt;p&gt;
&#27604;&#36739;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#36741;&#21161;&#20219;&#21153;&#30340;&#23398;&#20064;&#34920;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Comparing Auxiliary Tasks for Learning Representations for Reinforcement Learning. (arXiv:2310.04241v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#29992;&#20110;&#23398;&#20064;&#34920;&#31034;&#30340;&#19981;&#21516;&#36741;&#21161;&#20219;&#21153;&#65292;&#36890;&#36807;&#22312;&#36830;&#32493;&#25511;&#21046;&#22522;&#20934;&#29615;&#22659;&#19978;&#35757;&#32451;&#25968;&#30334;&#20010;&#26234;&#33021;&#20307;&#30340;&#23454;&#39564;&#65292;&#21457;&#29616;&#20351;&#29992;&#36741;&#21161;&#20219;&#21153;&#30340;&#34920;&#31034;&#23398;&#20064;&#23545;&#29615;&#22659;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#22238;&#25253;&#26377;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#33021;&#22815;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#29615;&#22659;&#22238;&#25253;&#65292;&#23398;&#20064;&#29366;&#24577;&#34920;&#31034;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#19968;&#31181;&#30452;&#25509;&#21644;&#39640;&#25928;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#19968;&#20010;&#19982;&#23454;&#38469;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#19981;&#21516;&#30340;&#36741;&#21161;&#20219;&#21153;&#35757;&#32451;&#19968;&#20010;&#29420;&#31435;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#29983;&#25104;&#34920;&#31034;&#12290;&#34429;&#28982;&#22312;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#35768;&#22810;&#36825;&#26679;&#30340;&#36741;&#21161;&#20219;&#21153;&#65292;&#20294;&#22312;&#20856;&#22411;&#30340;&#36830;&#32493;&#25511;&#21046;&#22522;&#20934;&#29615;&#22659;&#19978;&#36827;&#34892;&#27604;&#36739;&#35745;&#31639;&#37327;&#22823;&#19988;&#25454;&#25105;&#20204;&#25152;&#30693;&#20197;&#21069;&#26410;&#36827;&#34892;&#36807;&#12290;&#26412;&#25991;&#22312;&#22522;&#20110;&#26368;&#20808;&#36827;&#30340;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#35757;&#32451;&#30340;&#25968;&#30334;&#20010;&#26234;&#33021;&#20307;&#19978;&#36827;&#34892;&#20102;&#36825;&#26679;&#30340;&#36741;&#21161;&#20219;&#21153;&#27604;&#36739;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#20174;&#31616;&#21333;&#25670;&#32447;&#21040;&#22797;&#26434;&#30340;&#20223;&#30495;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#22238;&#25253;&#30340;&#21487;&#33021;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#36741;&#21161;&#20219;&#21153;&#30340;&#34920;&#31034;&#23398;&#20064;&#23545;&#29615;&#22659;&#26159;&#26377;&#30410;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning state representations has gained steady popularity in reinforcement learning (RL) due to its potential to improve both sample efficiency and returns on many environments. A straightforward and efficient method is to generate representations with a distinct neural network trained on an auxiliary task, i.e. a task that differs from the actual RL task. While a whole range of such auxiliary tasks has been proposed in the literature, a comparison on typical continuous control benchmark environments is computationally expensive and has, to the best of our knowledge, not been performed before. This paper presents such a comparison of common auxiliary tasks, based on hundreds of agents trained with state-of-the-art off-policy RL algorithms. We compare possible improvements in both sample efficiency and returns for environments ranging from simple pendulum to a complex simulated robotics task. Our findings show that representation learning with auxiliary tasks is beneficial for environ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#31995;&#32479;&#24615;&#35780;&#36848;&#29616;&#26377;&#30340;AutoML&#26694;&#26550;&#65292;&#20998;&#26512;&#20854;&#23545;&#20110;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#36890;&#36807;&#22522;&#20934;&#27979;&#35797;&#35299;&#20915;&#19981;&#21516;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#31867;&#22411;&#30340;&#20135;&#19994;&#29992;&#20363;&#12290;&#26368;&#32456;&#36873;&#25321;&#20102;Ray&#21644;AutoGluon&#20316;&#20026;&#21512;&#36866;&#30340;&#20302;&#32423;&#21644;&#39640;&#32423;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2310.04238</link><description>&lt;p&gt;
&#23558;&#37327;&#23376;&#31639;&#27861;&#24341;&#20837;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#65306;AutoML&#26694;&#26550;&#30340;&#21487;&#25193;&#23637;&#24615;&#23545;&#20110;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#31995;&#32479;&#24615;&#35780;&#36848;
&lt;/p&gt;
&lt;p&gt;
Bringing Quantum Algorithms to Automated Machine Learning: A Systematic Review of AutoML Frameworks Regarding Extensibility for QML Algorithms. (arXiv:2310.04238v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04238
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#31995;&#32479;&#24615;&#35780;&#36848;&#29616;&#26377;&#30340;AutoML&#26694;&#26550;&#65292;&#20998;&#26512;&#20854;&#23545;&#20110;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#36890;&#36807;&#22522;&#20934;&#27979;&#35797;&#35299;&#20915;&#19981;&#21516;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#31867;&#22411;&#30340;&#20135;&#19994;&#29992;&#20363;&#12290;&#26368;&#32456;&#36873;&#25321;&#20102;Ray&#21644;AutoGluon&#20316;&#20026;&#21512;&#36866;&#30340;&#20302;&#32423;&#21644;&#39640;&#32423;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#36873;&#25321;&#26041;&#27861;&#21644;&#23545;&#29616;&#26377;AutoML&#26694;&#26550;&#36827;&#34892;&#20998;&#26512;&#65292;&#20197;&#30830;&#23450;&#23427;&#20204;&#33021;&#21542;&#23558;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65288;QML&#65289;&#31639;&#27861;&#32435;&#20837;&#33258;&#21160;&#21270;&#27714;&#35299;&#30340;AutoML&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#23545;&#20854;&#26368;&#37325;&#35201;&#29305;&#24449;&#30340;&#22522;&#20934;&#27979;&#35797;&#26469;&#35299;&#20915;&#19968;&#32452;&#19981;&#21516;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#31867;&#22411;&#30340;&#20135;&#19994;&#29992;&#20363;&#12290;&#20026;&#27492;&#65292;&#21487;&#29992;&#30340;&#24320;&#28304;&#24037;&#20855;&#34987;&#21387;&#32553;&#25104;&#24066;&#22330;&#27010;&#35272;&#65292;&#24182;&#36890;&#36807;&#22810;&#38454;&#27573;&#12289;&#22810;&#26631;&#20934;&#30340;&#26041;&#27861;&#36827;&#34892;&#31995;&#32479;&#36873;&#25321;&#12290;&#36825;&#26159;&#36890;&#36807;&#32771;&#34385;&#36719;&#20214;&#36873;&#25321;&#26041;&#27861;&#20197;&#21450;AutoML&#30340;&#25216;&#26415;&#35270;&#35282;&#26469;&#23436;&#25104;&#30340;&#12290;&#26694;&#26550;&#36873;&#25321;&#30340;&#35201;&#27714;&#26681;&#25454;&#20854;&#36719;&#20214;&#21644;&#26426;&#22120;&#23398;&#20064;&#23646;&#24615;&#34987;&#20998;&#20026;&#30828;&#24615;&#21644;&#36719;&#24615;&#26631;&#20934;&#12290;&#27492;&#22806;&#65292;AutoML&#26694;&#26550;&#36824;&#26681;&#25454;&#30740;&#31350;&#32467;&#26524;&#36827;&#34892;&#20102;&#39640;&#32423;&#21644;&#20302;&#32423;&#31867;&#22411;&#30340;&#20998;&#31867;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36873;&#25321;Ray&#21644;AutoGluon&#20316;&#20026;&#21512;&#36866;&#30340;&#20302;&#32423;&#21644;&#39640;&#32423;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work describes the selection approach and analysis of existing AutoML frameworks regarding their capability of a) incorporating Quantum Machine Learning (QML) algorithms into this automated solving approach of the AutoML framing and b) solving a set of industrial use-cases with different ML problem types by benchmarking their most important characteristics. For that, available open-source tools are condensed into a market overview and suitable frameworks are systematically selected on a multi-phase, multi-criteria approach. This is done by considering software selection approaches, as well as in terms of the technical perspective of AutoML. The requirements for the framework selection are divided into hard and soft criteria regarding their software and ML attributes. Additionally, a classification of AutoML frameworks is made into high- and low-level types, inspired by the findings of. Finally, we select Ray and AutoGluon as the suitable low- and high-level frameworks respectively
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22266;&#23450;&#21442;&#25968;&#21487;&#22788;&#29702;&#31639;&#27861;&#65292;&#29992;&#20110;&#35745;&#25968;&#20855;&#26377;&#30456;&#21516;&#39592;&#26550;&#30340;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#31867;&#12290;</title><link>http://arxiv.org/abs/2310.04218</link><description>&lt;p&gt;
&#19968;&#20010;&#21487;&#35745;&#25968;&#20855;&#26377;&#30456;&#21516;&#39592;&#26550;&#30340;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#31867;&#30340;&#22266;&#23450;&#21442;&#25968;&#21487;&#22788;&#29702;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Fixed-Parameter Tractable Algorithm for Counting Markov Equivalence Classes with the same Skeleton. (arXiv:2310.04218v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04218
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22266;&#23450;&#21442;&#25968;&#21487;&#22788;&#29702;&#31639;&#27861;&#65292;&#29992;&#20110;&#35745;&#25968;&#20855;&#26377;&#30456;&#21516;&#39592;&#26550;&#30340;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;&#20063;&#31216;&#20026;&#36125;&#21494;&#26031;&#32593;&#32476;&#65289;&#26159;&#32534;&#30721;&#38543;&#26426;&#21464;&#37327;&#20043;&#38388;&#26465;&#20214;&#20381;&#36182;&#20851;&#31995;&#30340;&#27969;&#34892;&#24037;&#20855;&#12290;&#22312;&#22240;&#26524;&#26377;&#21521;&#26080;&#29615;&#22270;&#20013;&#65292;&#38543;&#26426;&#21464;&#37327;&#34987;&#24314;&#27169;&#20026;&#26377;&#21521;&#22270;&#20013;&#30340;&#39030;&#28857;&#65292;&#24182;&#19988;&#35268;&#23450;&#27599;&#20010;&#38543;&#26426;&#21464;&#37327;&#22312;&#32473;&#23450;&#20854;&#29238;&#33410;&#28857;&#30340;&#24773;&#20917;&#19979;&#19982;&#20854;&#31062;&#20808;&#33410;&#28857;&#26080;&#20851;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#21516;&#19968;&#32452;&#38543;&#26426;&#21464;&#37327;&#19978;&#30340;&#20004;&#20010;&#19981;&#21516;&#30340;&#22240;&#26524;&#26377;&#21521;&#26080;&#29615;&#22270;&#21487;&#20197;&#20934;&#30830;&#32534;&#30721;&#30456;&#21516;&#30340;&#19968;&#32452;&#26465;&#20214;&#20381;&#36182;&#20851;&#31995;&#12290;&#36825;&#26679;&#30340;&#22240;&#26524;&#26377;&#21521;&#26080;&#29615;&#22270;&#34987;&#31216;&#20026;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#65292;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#30340;&#22240;&#26524;&#26377;&#21521;&#26080;&#29615;&#22270;&#30340;&#31561;&#20215;&#31867;&#34987;&#31216;&#20026;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#31867;&#65288;MEC&#65289;&#12290;&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#65292;&#23545;&#20110;MEC&#24050;&#32463;&#21019;&#24314;&#20102;&#19968;&#20123;&#32654;&#20029;&#30340;&#32452;&#21512;&#29305;&#24449;&#65292;&#24182;&#19988;&#24050;&#30693;&#65292;&#29305;&#21035;&#26159;&#22312;&#21516;&#19968;MEC&#20013;&#30340;&#25152;&#26377;&#22240;&#26524;&#26377;&#21521;&#26080;&#29615;&#22270;&#24517;&#39035;&#20855;&#26377;&#30456;&#21516;&#30340;&#8220;&#39592;&#26550;&#8221;&#65288;&#24213;&#23618;&#26080;&#21521;&#22270;&#65289;&#21644;v-&#32467;&#26500;&#65288;&#24418;&#24335;&#20026;$a\rightarrow b \leftarrow c$&#30340;&#35825;&#23548;&#23376;&#22270;&#65289;&#12290;&#36825;&#20123;&#32452;&#21512;&#29305;&#24449;&#36824;&#25552;&#20986;&#20102;&#20960;&#20010;&#33258;&#28982;&#30340;&#31639;&#27861;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal DAGs (also known as Bayesian networks) are a popular tool for encoding conditional dependencies between random variables. In a causal DAG, the random variables are modeled as vertices in the DAG, and it is stipulated that every random variable is independent of its ancestors conditioned on its parents. It is possible, however, for two different causal DAGs on the same set of random variables to encode exactly the same set of conditional dependencies. Such causal DAGs are said to be Markov equivalent, and equivalence classes of Markov equivalent DAGs are known as Markov Equivalent Classes (MECs). Beautiful combinatorial characterizations of MECs have been developed in the past few decades, and it is known, in particular that all DAGs in the same MEC must have the same ''skeleton'' (underlying undirected graph) and v-structures (induced subgraph of the form $a\rightarrow b \leftarrow c$).  These combinatorial characterizations also suggest several natural algorithmic questions. On
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#25104;&#26412;&#25928;&#30410;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#37325;&#26032;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#25104;&#26412;&#21644;&#25968;&#25454;&#12289;&#27169;&#22411;&#21450;&#39044;&#27979;&#26597;&#35810;&#31561;&#22240;&#32032;&#26469;&#33258;&#21160;&#21270;&#20915;&#31574;&#20309;&#26102;&#37325;&#26032;&#35757;&#32451;&#12290;&#20027;&#35201;&#36129;&#29486;&#26159;&#25104;&#26412;&#24863;&#30693;&#37325;&#26032;&#35757;&#32451;&#31639;&#27861;Cara&#65292;&#36890;&#36807;&#23545;&#25968;&#25454;&#27969;&#21644;&#26597;&#35810;&#36827;&#34892;&#20248;&#21270;&#26469;&#23454;&#29616;&#26368;&#20339;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2310.04216</link><description>&lt;p&gt;
&#25104;&#26412;&#25928;&#30410;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#37325;&#26032;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Cost-Effective Retraining of Machine Learning Models. (arXiv:2310.04216v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04216
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#25104;&#26412;&#25928;&#30410;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#37325;&#26032;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#25104;&#26412;&#21644;&#25968;&#25454;&#12289;&#27169;&#22411;&#21450;&#39044;&#27979;&#26597;&#35810;&#31561;&#22240;&#32032;&#26469;&#33258;&#21160;&#21270;&#20915;&#31574;&#20309;&#26102;&#37325;&#26032;&#35757;&#32451;&#12290;&#20027;&#35201;&#36129;&#29486;&#26159;&#25104;&#26412;&#24863;&#30693;&#37325;&#26032;&#35757;&#32451;&#31639;&#27861;Cara&#65292;&#36890;&#36807;&#23545;&#25968;&#25454;&#27969;&#21644;&#26597;&#35810;&#36827;&#34892;&#20248;&#21270;&#26469;&#23454;&#29616;&#26368;&#20339;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20445;&#25345;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#22312;&#25968;&#25454;&#38543;&#26102;&#38388;&#21464;&#21270;&#26102;&#30340;&#24615;&#33021;&#65292;&#26377;&#24517;&#35201;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#21487;&#33021;&#26159;&#26114;&#36149;&#30340;&#65292;&#22240;&#20026;&#36890;&#24120;&#38656;&#35201;&#37325;&#26032;&#22788;&#29702;&#25972;&#20010;&#25968;&#25454;&#38598;&#12290;&#36825;&#23548;&#33268;&#20102;&#37325;&#26032;&#35757;&#32451;&#39057;&#29575;&#21644;&#35745;&#31639;&#25104;&#26412;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#36807;&#20110;&#39057;&#32321;&#30340;&#37325;&#26032;&#35757;&#32451;&#20250;&#23548;&#33268;&#19981;&#24517;&#35201;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#32780;&#19981;&#32463;&#24120;&#37325;&#26032;&#35757;&#32451;&#20250;&#23548;&#33268;&#36807;&#26102;&#21644;&#19981;&#20934;&#30830;&#30340;ML&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33021;&#22815;&#33258;&#21160;&#21270;&#19988;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#30340;&#20915;&#31574;&#20309;&#26102;&#37325;&#26032;&#35757;&#32451;ML&#27169;&#22411;&#30340;ML&#31995;&#32479;&#12290;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#32771;&#34385;&#27599;&#20010;&#20915;&#31574;&#30456;&#20851;&#30340;&#25104;&#26412;&#26469;&#20248;&#21270;&#36825;&#20010;&#26435;&#34913;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#26159;&#22522;&#20110;&#25968;&#25454;&#12289;&#27169;&#22411;&#21644;&#27169;&#22411;&#22238;&#31572;&#30340;&#39044;&#27979;&#26597;&#35810;&#31561;&#19981;&#21516;&#22240;&#32032;&#26469;&#30830;&#23450;&#26159;&#21542;&#37325;&#26032;&#35757;&#32451;&#25110;&#20445;&#30041;&#29616;&#26377;&#30340;ML&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#19968;&#31181;&#21517;&#20026;Cara&#30340;&#25104;&#26412;&#24863;&#30693;&#37325;&#26032;&#35757;&#32451;&#31639;&#27861;&#65292;&#23427;&#36890;&#36807;&#23545;&#25968;&#25454;&#27969;&#21644;&#26597;&#35810;&#36827;&#34892;&#20248;&#21270;&#26469;&#20248;&#21270;&#36825;&#31181;&#26435;&#34913;&#12290;&#20026;&#20102;&#35780;&#20272;Cara&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#21512;&#25104;&#30340;&#25968;&#25454;&#38598;&#21644;&#23454;&#38469;&#24212;&#29992;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is important to retrain a machine learning (ML) model in order to maintain its performance as the data changes over time. However, this can be costly as it usually requires processing the entire dataset again. This creates a trade-off between retraining too frequently, which leads to unnecessary computing costs, and not retraining often enough, which results in stale and inaccurate ML models. To address this challenge, we propose ML systems that make automated and cost-effective decisions about when to retrain an ML model. We aim to optimize the trade-off by considering the costs associated with each decision. Our research focuses on determining whether to retrain or keep an existing ML model based on various factors, including the data, the model, and the predictive queries answered by the model. Our main contribution is a Cost-Aware Retraining Algorithm called Cara, which optimizes the trade-off over streams of data and queries. To evaluate the performance of Cara, we analyzed syn
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#20887;&#20313;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25913;&#36827;&#34920;&#36798;&#33021;&#21147;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#37051;&#22495;&#26641;&#30340;&#32858;&#21512;&#26041;&#26696;&#20943;&#23569;&#20887;&#20313;&#65292;&#25552;&#39640;&#20102;&#34920;&#36798;&#33021;&#21147;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#23427;&#23545;&#20110;&#20943;&#36731;&#36807;&#24230;&#21387;&#32553;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.04190</link><description>&lt;p&gt;
&#38750;&#20887;&#20313;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25913;&#36827;&#34920;&#36798;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Non-Redundant Graph Neural Networks with Improved Expressiveness. (arXiv:2310.04190v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04190
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#20887;&#20313;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25913;&#36827;&#34920;&#36798;&#33021;&#21147;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#37051;&#22495;&#26641;&#30340;&#32858;&#21512;&#26041;&#26696;&#20943;&#23569;&#20887;&#20313;&#65292;&#25552;&#39640;&#20102;&#34920;&#36798;&#33021;&#21147;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#23427;&#23545;&#20110;&#20943;&#36731;&#36807;&#24230;&#21387;&#32553;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28040;&#24687;&#20256;&#36882;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#32858;&#21512;&#26469;&#33258;&#25152;&#26377;&#37051;&#23621;&#33410;&#28857;&#30340;&#28040;&#24687;&#26469;&#36845;&#20195;&#35745;&#31639;&#33410;&#28857;&#23884;&#20837;&#12290;&#36825;&#20010;&#36807;&#31243;&#21487;&#20197;&#34987;&#35270;&#20026;Weisfeiler-Leman&#26041;&#27861;&#30340;&#31070;&#32463;&#21464;&#20307;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#36807;&#24230;&#24179;&#28369;&#21644;&#36807;&#24230;&#21387;&#32553;&#38480;&#21046;&#20102;&#36825;&#20123;&#32593;&#32476;&#33021;&#22815;&#26377;&#25928;&#21033;&#29992;&#30340;&#23618;&#25968;&#12290;&#28040;&#24687;&#20256;&#36882;&#20013;&#30456;&#21516;&#20449;&#24687;&#30340;&#37325;&#22797;&#20132;&#25442;&#21644;&#32534;&#30721;&#20250;&#25918;&#22823;&#36807;&#24230;&#21387;&#32553;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37051;&#22495;&#26641;&#30340;&#26032;&#30340;&#32858;&#21512;&#26041;&#26696;&#65292;&#20801;&#35768;&#36890;&#36807;&#20462;&#21098;&#26631;&#20934;&#28040;&#24687;&#20256;&#36882;&#20013;&#30340;&#23637;&#24320;&#26641;&#30340;&#20998;&#25903;&#26469;&#25511;&#21046;&#20887;&#20313;&#12290;&#25105;&#20204;&#35777;&#26126;&#20943;&#23569;&#20887;&#20313;&#21487;&#20197;&#25552;&#39640;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#23427;&#20943;&#36731;&#20102;&#36807;&#24230;&#21387;&#32553;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#28040;&#24687;&#20256;&#36882;&#20013;&#30340;&#20887;&#20313;&#19982;&#35745;&#31639;&#20013;&#30340;&#20887;&#20313;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#37051;&#22495;&#26641;&#30340;&#32039;&#20945;&#34920;&#31034;&#65292;&#36890;&#36807;&#31070;&#32463;&#26641;&#35268;&#33539;&#21270;&#25216;&#26415;&#35745;&#31639;&#33410;&#28857;&#21644;&#22270;&#30340;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Message passing graph neural networks iteratively compute node embeddings by aggregating messages from all neighbors. This procedure can be viewed as a neural variant of the Weisfeiler-Leman method, which limits their expressive power. Moreover, oversmoothing and oversquashing restrict the number of layers these networks can effectively utilize. The repeated exchange and encoding of identical information in message passing amplifies oversquashing. We propose a novel aggregation scheme based on neighborhood trees, which allows for controlling the redundancy by pruning branches of the unfolding trees underlying standard message passing. We prove that reducing redundancy improves expressivity and experimentally show that it alleviates oversquashing. We investigate the interaction between redundancy in message passing and redundancy in computation and propose a compact representation of neighborhood trees, from which we compute node and graph embeddings via a neural tree canonization techn
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Entropy Score&#30340;&#26032;&#22411;&#26080;&#38656;&#35757;&#32451;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#19982;LogSynflow&#30340;&#32452;&#21512;&#25628;&#32034;&#27169;&#22411;&#30340;&#22823;&#23567;&#65292;&#20174;&#32780;&#22312;&#36739;&#30701;&#26102;&#38388;&#20869;&#35774;&#35745;&#20986;&#29992;&#20110;&#36793;&#32536;&#24212;&#29992;&#30340;&#39640;&#24615;&#33021;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2310.04179</link><description>&lt;p&gt;
Entropic Score&#24230;&#37327;&#65306;&#22312;&#26080;&#38656;&#35757;&#32451;&#30340;NAS&#20013;&#35299;&#32806;&#25299;&#25169;&#21644;&#22823;&#23567;
&lt;/p&gt;
&lt;p&gt;
Entropic Score metric: Decoupling Topology and Size in Training-free NAS. (arXiv:2310.04179v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04179
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Entropy Score&#30340;&#26032;&#22411;&#26080;&#38656;&#35757;&#32451;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#19982;LogSynflow&#30340;&#32452;&#21512;&#25628;&#32034;&#27169;&#22411;&#30340;&#22823;&#23567;&#65292;&#20174;&#32780;&#22312;&#36739;&#30701;&#26102;&#38388;&#20869;&#35774;&#35745;&#20986;&#29992;&#20110;&#36793;&#32536;&#24212;&#29992;&#30340;&#39640;&#24615;&#33021;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#26159;&#19968;&#39033;&#22797;&#26434;&#19988;&#24120;&#24120;&#20196;&#20154;&#26395;&#32780;&#29983;&#30031;&#30340;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#31227;&#21160;&#22823;&#23567;&#27169;&#22411;&#30340;&#36164;&#28304;&#21463;&#38480;&#22330;&#26223;&#12290;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#26159;&#19968;&#31181;&#33258;&#21160;&#21270;&#36825;&#19968;&#36807;&#31243;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#65292;&#20294;&#26159;&#29616;&#26377;&#30340;&#31454;&#20105;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#26102;&#38388;&#21644;&#35745;&#31639;&#36164;&#28304;&#26469;&#29983;&#25104;&#20934;&#30830;&#30340;&#27169;&#22411;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#26412;&#25991;&#20570;&#20986;&#20102;&#20197;&#19979;&#36129;&#29486;&#65306;i&#65289;&#19968;&#31181;&#21517;&#20026;Entropy Score&#30340;&#26032;&#22411;&#26080;&#38656;&#35757;&#32451;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#36890;&#36807;&#32858;&#21512;&#30340;&#36880;&#20803;&#32032;&#29109;&#20272;&#35745;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#65307;ii&#65289;&#19968;&#31181;&#24490;&#29615;&#25628;&#32034;&#31639;&#27861;&#65292;&#20998;&#21035;&#20294;&#21327;&#21516;&#22320;&#25628;&#32034;&#27169;&#22411;&#22823;&#23567;&#21644;&#25299;&#25169;&#12290;Entropy Score&#22312;&#25628;&#32034;&#32593;&#32476;&#30340;&#25299;&#25169;&#26041;&#38754;&#26174;&#31034;&#20986;&#26174;&#33879;&#33021;&#21147;&#65292;&#19982;LogSynflow&#30340;&#36866;&#24403;&#32452;&#21512;&#29992;&#20110;&#25628;&#32034;&#27169;&#22411;&#22823;&#23567;&#65292;&#21487;&#20197;&#22312;&#19981;&#21040;1&#20010;GPU&#23567;&#26102;&#20869;&#23436;&#20840;&#35774;&#35745;&#29992;&#20110;&#36793;&#32536;&#24212;&#29992;&#30340;&#39640;&#24615;&#33021;&#28151;&#21512;&#21464;&#21387;&#22120;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;ImageNet&#20998;&#31867;&#19978;&#26368;&#24555;&#19988;&#26368;&#20934;&#30830;&#30340;NAS&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Networks design is a complex and often daunting task, particularly for resource-constrained scenarios typical of mobile-sized models. Neural Architecture Search is a promising approach to automate this process, but existing competitive methods require large training time and computational resources to generate accurate models. To overcome these limits, this paper contributes with: i) a novel training-free metric, named Entropic Score, to estimate model expressivity through the aggregated element-wise entropy of its activations; ii) a cyclic search algorithm to separately yet synergistically search model size and topology. Entropic Score shows remarkable ability in searching for the topology of the network, and a proper combination with LogSynflow, to search for model size, yields superior capability to completely design high-performance Hybrid Transformers for edge applications in less than 1 GPU hour, resulting in the fastest and most accurate NAS method for ImageNet classifica
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#24402;&#22240;&#31283;&#23450;&#24615;&#25351;&#26631;(ASI)&#65292;&#29992;&#20110;&#24230;&#37327;&#26102;&#38388;&#24207;&#21015;XAI&#24402;&#22240;&#30340;&#31283;&#20581;&#24615;&#21644;&#21487;&#20449;&#24230;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#38477;&#32500;&#31354;&#38388;&#20013;&#30340;&#24402;&#22240;&#21644;ASI&#20998;&#25968;&#20998;&#24067;&#26469;&#35777;&#26126;&#35813;&#25351;&#26631;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.04178</link><description>&lt;p&gt;
&#24341;&#20837;&#24402;&#22240;&#31283;&#23450;&#24615;&#25351;&#26631;: &#29992;&#20110;&#26102;&#38388;&#24207;&#21015;XAI&#24402;&#22240;&#30340;&#24230;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Introducing the Attribution Stability Indicator: a Measure for Time Series XAI Attributions. (arXiv:2310.04178v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04178
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#24402;&#22240;&#31283;&#23450;&#24615;&#25351;&#26631;(ASI)&#65292;&#29992;&#20110;&#24230;&#37327;&#26102;&#38388;&#24207;&#21015;XAI&#24402;&#22240;&#30340;&#31283;&#20581;&#24615;&#21644;&#21487;&#20449;&#24230;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#38477;&#32500;&#31354;&#38388;&#20013;&#30340;&#24402;&#22240;&#21644;ASI&#20998;&#25968;&#20998;&#24067;&#26469;&#35777;&#26126;&#35813;&#25351;&#26631;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#37329;&#34701;&#12289;&#22825;&#27668;&#39044;&#25253;&#21644;&#21307;&#30103;&#20445;&#20581;&#31561;&#39046;&#22495;&#20013;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#22686;&#21152;&#21644;&#26222;&#36941;&#22797;&#26434;&#24615;&#65292;&#36843;&#20999;&#38656;&#35201;&#20808;&#36827;&#30340;&#24615;&#33021;&#27169;&#22411;&#65292;&#33021;&#22815;&#25552;&#20379;&#23545;&#22522;&#26412;&#27169;&#24335;&#21644;&#20851;&#32852;&#24615;&#30340;&#21487;&#35299;&#37322;&#27934;&#23519;&#12290; &#24402;&#22240;&#25216;&#26415;&#33021;&#22815;&#20174;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#20013;&#25552;&#21462;&#35299;&#37322;&#20197;&#33719;&#24471;&#27934;&#23519;&#65292;&#20294;&#24456;&#38590;&#35780;&#20272;&#20854;&#31283;&#20581;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290; &#25105;&#20204;&#25552;&#20986;&#20102;&#24402;&#22240;&#31283;&#23450;&#24615;&#25351;&#26631;&#65288;ASI&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#23558;&#31283;&#20581;&#24615;&#21644;&#21487;&#20449;&#24230;&#20316;&#20026;&#26102;&#38388;&#24207;&#21015;&#24402;&#22240;&#25216;&#26415;&#23646;&#24615;&#32771;&#34385;&#36827;&#21435;&#30340;&#24230;&#37327;&#26041;&#27861;&#12290; &#25105;&#20204;&#36890;&#36807;&#23558;&#21407;&#22987;&#26102;&#38388;&#24207;&#21015;&#30340;&#25200;&#21160;&#23454;&#20363;&#21644;&#24402;&#22240;&#19982;&#30456;&#20851;&#24615;&#32467;&#21512;&#36215;&#26469;&#36827;&#34892;&#25193;&#23637;&#65292;&#24182;&#22312;&#24230;&#37327;&#26041;&#27861;&#20013;&#21152;&#20837;&#25152;&#38656;&#30340;&#23646;&#24615;&#12290; &#25105;&#20204;&#36890;&#36807;&#23545;&#19968;&#32452;&#38477;&#32500;&#31354;&#38388;&#20013;&#30340;&#24402;&#22240;&#21450;ASI&#20998;&#25968;&#20998;&#24067;&#30340;&#20998;&#26512;&#26469;&#35777;&#26126;&#25152;&#38656;&#30340;&#23646;&#24615;&#65292;&#24182;&#20351;&#29992;&#20102;&#19977;&#20010;&#23436;&#25972;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the increasing amount and general complexity of time series data in domains such as finance, weather forecasting, and healthcare, there is a growing need for state-of-the-art performance models that can provide interpretable insights into underlying patterns and relationships. Attribution techniques enable the extraction of explanations from time series models to gain insights but are hard to evaluate for their robustness and trustworthiness. We propose the Attribution Stability Indicator (ASI), a measure to incorporate robustness and trustworthiness as properties of attribution techniques for time series into account. We extend a perturbation analysis with correlations of the original time series to the perturbed instance and the attributions to include wanted properties in the measure. We demonstrate the wanted properties based on an analysis of the attributions in a dimension-reduced space and the ASI scores distribution over three whole time series classification datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#27450;&#35784;&#26816;&#27979;&#38382;&#39064;&#65292;&#36890;&#36807;&#21160;&#24577;&#20851;&#31995;&#27880;&#24847;&#32858;&#21512;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23398;&#20064;&#27599;&#20010;&#20851;&#31995;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#21487;&#23398;&#20064;&#30340;&#27880;&#24847;&#20989;&#25968;&#36827;&#34892;&#33410;&#28857;&#34920;&#31034;&#30340;&#32858;&#21512;&#12290;&#36890;&#36807;&#32467;&#21512;&#19981;&#21516;&#23618;&#27425;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#32771;&#34385;&#30446;&#26631;&#33410;&#28857;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#32467;&#26500;&#65292;&#20197;&#25552;&#39640;&#27450;&#35784;&#26816;&#27979;&#24615;&#33021;&#12290;&#36890;&#36807;&#20351;&#29992;&#21160;&#24577;&#22270;&#27880;&#24847;&#21147;&#65292;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#35745;&#31639;&#20851;&#31995;&#20043;&#38388;&#30340;&#37325;&#35201;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.04171</link><description>&lt;p&gt;
&#21160;&#24577;&#20851;&#31995;&#27880;&#24847;&#21147;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#27450;&#35784;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Dynamic Relation-Attentive Graph Neural Networks for Fraud Detection. (arXiv:2310.04171v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04171
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#27450;&#35784;&#26816;&#27979;&#38382;&#39064;&#65292;&#36890;&#36807;&#21160;&#24577;&#20851;&#31995;&#27880;&#24847;&#32858;&#21512;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23398;&#20064;&#27599;&#20010;&#20851;&#31995;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#21487;&#23398;&#20064;&#30340;&#27880;&#24847;&#20989;&#25968;&#36827;&#34892;&#33410;&#28857;&#34920;&#31034;&#30340;&#32858;&#21512;&#12290;&#36890;&#36807;&#32467;&#21512;&#19981;&#21516;&#23618;&#27425;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#32771;&#34385;&#30446;&#26631;&#33410;&#28857;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#32467;&#26500;&#65292;&#20197;&#25552;&#39640;&#27450;&#35784;&#26816;&#27979;&#24615;&#33021;&#12290;&#36890;&#36807;&#20351;&#29992;&#21160;&#24577;&#22270;&#27880;&#24847;&#21147;&#65292;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#35745;&#31639;&#20851;&#31995;&#20043;&#38388;&#30340;&#37325;&#35201;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27450;&#35784;&#26816;&#27979;&#26088;&#22312;&#21457;&#29616;&#27450;&#35784;&#32773;&#36890;&#36807;&#30041;&#19979;&#20551;&#35780;&#35770;&#25110;&#36827;&#34892;&#24322;&#24120;&#20132;&#26131;&#27450;&#39575;&#20854;&#20182;&#29992;&#25143;&#12290;&#22522;&#20110;&#22270;&#30340;&#27450;&#35784;&#26816;&#27979;&#26041;&#27861;&#23558;&#36825;&#20010;&#20219;&#21153;&#35270;&#20026;&#19968;&#20010;&#21253;&#21547;&#20004;&#20010;&#31867;&#21035;&#65288;&#27450;&#35784;&#25110;&#27491;&#24120;&#65289;&#30340;&#20998;&#31867;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#21160;&#24577;&#20851;&#31995;&#27880;&#24847;&#32858;&#21512;&#26426;&#21046;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#22522;&#20110;&#23454;&#38469;&#19990;&#30028;&#22270;&#34920;&#20013;&#21253;&#21547;&#19981;&#21516;&#31867;&#22411;&#30340;&#20851;&#31995;&#30340;&#35266;&#23519;&#65292;&#25105;&#20204;&#24314;&#35758;&#23398;&#20064;&#27599;&#20010;&#20851;&#31995;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#27880;&#24847;&#20989;&#25968;&#32858;&#21512;&#33410;&#28857;&#34920;&#31034;&#65292;&#35813;&#20989;&#25968;&#20026;&#27599;&#20010;&#20851;&#31995;&#20998;&#37197;&#19981;&#21516;&#30340;&#27880;&#24847;&#31995;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32467;&#21512;&#19981;&#21516;&#23618;&#27425;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#20197;&#32771;&#34385;&#30446;&#26631;&#33410;&#28857;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#32467;&#26500;&#65292;&#36825;&#26377;&#21161;&#20110;&#25552;&#39640;&#22312;&#20855;&#26377;&#24322;&#36136;&#24615;&#30340;&#22270;&#19978;&#36827;&#34892;&#27450;&#35784;&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#22312;&#25152;&#26377;&#32858;&#21512;&#36807;&#31243;&#20013;&#37319;&#29992;&#21160;&#24577;&#22270;&#27880;&#24847;&#21147;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#35745;&#31639;&#20851;&#31995;&#20043;&#38388;&#30340;&#37325;&#35201;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fraud detection aims to discover fraudsters deceiving other users by, for example, leaving fake reviews or making abnormal transactions. Graph-based fraud detection methods consider this task as a classification problem with two classes: frauds or normal. We address this problem using Graph Neural Networks (GNNs) by proposing a dynamic relation-attentive aggregation mechanism. Based on the observation that many real-world graphs include different types of relations, we propose to learn a node representation per relation and aggregate the node representations using a learnable attention function that assigns a different attention coefficient to each relation. Furthermore, we combine the node representations from different layers to consider both the local and global structures of a target node, which is beneficial to improving the performance of fraud detection on graphs with heterophily. By employing dynamic graph attention in all the aggregation processes, our method adaptively comput
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#25674;&#32593;&#32476;&#24178;&#39044;&#26041;&#27861;&#65292;&#21487;&#20197;&#24341;&#23548;&#20852;&#22859;&#24615;&#28857;&#36807;&#31243;&#30340;&#28436;&#21270;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#31070;&#32463;ODE&#26469;&#25429;&#25417;&#32593;&#32476;&#21270;&#20852;&#22859;&#24615;&#28857;&#36807;&#31243;&#30340;&#21464;&#21270;&#65292;&#24182;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#23454;&#29616;&#28789;&#27963;&#30340;&#31574;&#30053;&#12290;&#36890;&#36807;&#35774;&#35745;&#30340;&#20998;&#25674;&#32593;&#32476;&#24178;&#39044;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#21382;&#21490;&#21644;&#20854;&#20182;&#29615;&#22659;&#20013;&#38598;&#25104;&#26368;&#20339;&#31574;&#30053;&#65292;&#23454;&#29616;&#30693;&#35782;&#30340;&#39640;&#25928;&#36716;&#31227;&#21644;&#20849;&#20139;&#12290;</title><link>http://arxiv.org/abs/2310.04159</link><description>&lt;p&gt;
&#20998;&#25674;&#32593;&#32476;&#24178;&#39044;&#20197;&#24341;&#23548;&#20852;&#22859;&#24615;&#28857;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Amortized Network Intervention to Steer the Excitatory Point Processes. (arXiv:2310.04159v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04159
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#25674;&#32593;&#32476;&#24178;&#39044;&#26041;&#27861;&#65292;&#21487;&#20197;&#24341;&#23548;&#20852;&#22859;&#24615;&#28857;&#36807;&#31243;&#30340;&#28436;&#21270;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#31070;&#32463;ODE&#26469;&#25429;&#25417;&#32593;&#32476;&#21270;&#20852;&#22859;&#24615;&#28857;&#36807;&#31243;&#30340;&#21464;&#21270;&#65292;&#24182;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#23454;&#29616;&#28789;&#27963;&#30340;&#31574;&#30053;&#12290;&#36890;&#36807;&#35774;&#35745;&#30340;&#20998;&#25674;&#32593;&#32476;&#24178;&#39044;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#21382;&#21490;&#21644;&#20854;&#20182;&#29615;&#22659;&#20013;&#38598;&#25104;&#26368;&#20339;&#31574;&#30053;&#65292;&#23454;&#29616;&#30693;&#35782;&#30340;&#39640;&#25928;&#36716;&#31227;&#21644;&#20849;&#20139;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#32593;&#32476;&#24178;&#39044;&#20197;&#24341;&#23548;&#20852;&#22859;&#24615;&#28857;&#36807;&#31243;&#65288;&#22914;&#20256;&#26579;&#30149;&#20256;&#25773;&#25110;&#20132;&#36890;&#25317;&#22581;&#25511;&#21046;&#65289;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#21033;&#29992;&#31070;&#32463;ODE&#26469;&#25429;&#25417;&#32593;&#32476;&#21270;&#30340;&#20852;&#22859;&#24615;&#28857;&#36807;&#31243;&#22312;&#32593;&#32476;&#25299;&#25169;&#30340;&#26102;&#21464;&#21464;&#21270;&#19979;&#23558;&#22914;&#20309;&#28436;&#21270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#26799;&#24230;&#19979;&#38477;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;GD-MPC&#65289;&#65292;&#25552;&#20379;&#31574;&#30053;&#28789;&#27963;&#24615;&#20197;&#36866;&#24212;&#20808;&#21069;&#30693;&#35782;&#21644;&#32422;&#26463;&#26465;&#20214;&#12290;&#20026;&#20102;&#35299;&#20915;&#35745;&#21010;&#30340;&#22797;&#26434;&#24615;&#24182;&#20811;&#26381;&#27492;&#31867;&#20915;&#31574;&#38382;&#39064;&#20013;&#22266;&#26377;&#30340;&#39640;&#32500;&#24230;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#20998;&#25674;&#32593;&#32476;&#24178;&#39044;&#65288;ANI&#65289;&#26694;&#26550;&#65292;&#20801;&#35768;&#20174;&#21382;&#21490;&#21644;&#20854;&#20182;&#29615;&#22659;&#20013;&#27719;&#32858;&#26368;&#20339;&#31574;&#30053;&#65292;&#21516;&#26102;&#30830;&#20445;&#25490;&#21015;&#31561;&#25928;&#24615;&#12290;&#36825;&#31181;&#24615;&#36136;&#23454;&#29616;&#20102;&#30693;&#35782;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#30340;&#39640;&#25928;&#36716;&#31227;&#21644;&#20849;&#20139;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#20174;&#25511;&#21046;&#20256;&#26579;&#30149;&#20256;&#25773;&#21040;&#20943;&#23569;&#30899;&#25490;&#25918;&#12290;
&lt;/p&gt;
&lt;p&gt;
We tackle the challenge of large-scale network intervention for guiding excitatory point processes, such as infectious disease spread or traffic congestion control. Our model-based reinforcement learning utilizes neural ODEs to capture how the networked excitatory point processes will evolve subject to the time-varying changes in network topology. Our approach incorporates Gradient-Descent based Model Predictive Control (GD-MPC), offering policy flexibility to accommodate prior knowledge and constraints. To address the intricacies of planning and overcome the high dimensionality inherent to such decision-making problems, we design an Amortize Network Interventions (ANI) framework, allowing for the pooling of optimal policies from history and other contexts, while ensuring a permutation equivalent property. This property enables efficient knowledge transfer and sharing across diverse contexts. Our approach has broad applications, from curbing infectious disease spread to reducing carbon
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026; LDSS &#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#29992;&#20110;&#35757;&#32451;&#20998;&#31867;&#27169;&#22411;&#30340;&#27844;&#38706;&#25968;&#25454;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#27880;&#20837;&#29305;&#24449;&#20026;&#31867;&#21035;&#20998;&#24067;&#30340;&#23616;&#37096;&#20559;&#31227;&#30340;&#21512;&#25104;&#25968;&#25454;&#26469;&#23454;&#29616;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#35782;&#21035;&#20986;&#27169;&#22411;&#26597;&#35810;&#20013;&#30340;&#27844;&#38706;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2310.04145</link><description>&lt;p&gt;
&#20174;&#38646;&#21040;&#33521;&#38596;: &#36890;&#36807;&#21512;&#25104;&#25968;&#25454;&#27880;&#20837;&#21644;&#27169;&#22411;&#26597;&#35810;&#26469;&#26816;&#27979;&#27844;&#38706;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
From Zero to Hero: Detecting Leaked Data through Synthetic Data Injection and Model Querying. (arXiv:2310.04145v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04145
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026; LDSS &#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#29992;&#20110;&#35757;&#32451;&#20998;&#31867;&#27169;&#22411;&#30340;&#27844;&#38706;&#25968;&#25454;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#27880;&#20837;&#29305;&#24449;&#20026;&#31867;&#21035;&#20998;&#24067;&#30340;&#23616;&#37096;&#20559;&#31227;&#30340;&#21512;&#25104;&#25968;&#25454;&#26469;&#23454;&#29616;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#35782;&#21035;&#20986;&#27169;&#22411;&#26597;&#35810;&#20013;&#30340;&#27844;&#38706;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;&#19981;&#26029;&#22686;&#22810;&#65292;&#20445;&#25252;&#25968;&#25454;&#30340;&#30693;&#35782;&#20135;&#26435;&#24050;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#20854;&#25104;&#21151;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#35757;&#32451;&#25968;&#25454;&#30340;&#36136;&#37327;&#12290;&#34429;&#28982;&#23384;&#22312;&#21508;&#31181;&#26426;&#21046;&#26469;&#22312;&#23384;&#20648;&#12289;&#20256;&#36755;&#21644;&#20351;&#29992;&#36807;&#31243;&#20013;&#20445;&#25252;&#25968;&#25454;&#65292;&#20294;&#36739;&#23569;&#26377;&#30740;&#31350;&#26159;&#38024;&#23545;&#26159;&#21542;&#24050;&#32463;&#26410;&#32463;&#25480;&#26435;&#22320;&#27844;&#38706;&#29992;&#20110;&#27169;&#22411;&#35757;&#32451;&#30340;&#25968;&#25454;&#36827;&#34892;&#26816;&#27979;&#12290;&#30001;&#20110;&#23545;&#28508;&#22312;&#25915;&#20987;&#32773;&#36827;&#34892;&#30340;&#35757;&#32451;&#36807;&#31243;&#32570;&#20047;&#20449;&#24687;&#21644;&#25511;&#21046;&#65292;&#36825;&#20010;&#38382;&#39064;&#23588;&#20026;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#38024;&#23545;&#34920;&#26684;&#25968;&#25454;&#39046;&#22495;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#35770;&#65292;&#23616;&#37096;&#20998;&#24067;&#20559;&#31227;&#21512;&#25104;&#65288;LDSS&#65289;&#65292;&#29992;&#20110;&#26816;&#27979;&#29992;&#20110;&#35757;&#32451;&#20998;&#31867;&#27169;&#22411;&#30340;&#27844;&#38706;&#25968;&#25454;&#12290;LDSS &#30340;&#26680;&#24515;&#27010;&#24565;&#26159;&#21521;&#25152;&#26377;&#32773;&#30340;&#25968;&#25454;&#38598;&#20013;&#27880;&#20837;&#23569;&#37327;&#21512;&#25104;&#25968;&#25454;&#65292;&#20854;&#29305;&#28857;&#26159;&#31867;&#21035;&#20998;&#24067;&#30340;&#23616;&#37096;&#20559;&#31227;&#12290;&#36825;&#26679;&#21487;&#20197;&#26377;&#25928;&#35782;&#21035;&#20986;&#27169;&#22411;&#26597;&#35810;&#20013;&#30340;&#27844;&#38706;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Safeguarding the Intellectual Property (IP) of data has become critically important as machine learning applications continue to proliferate, and their success heavily relies on the quality of training data. While various mechanisms exist to secure data during storage, transmission, and consumption, fewer studies have been developed to detect whether they are already leaked for model training without authorization. This issue is particularly challenging due to the absence of information and control over the training process conducted by potential attackers.  In this paper, we concentrate on the domain of tabular data and introduce a novel methodology, Local Distribution Shifting Synthesis (\textsc{LDSS}), to detect leaked data that are used to train classification models. The core concept behind \textsc{LDSS} involves injecting a small volume of synthetic data--characterized by local shifts in class distribution--into the owner's dataset. This enables the effective identification of mo
&lt;/p&gt;</description></item><item><title>Routing Arena&#26159;&#19968;&#20010;&#22522;&#20934;&#22871;&#20214;&#65292;&#25512;&#20986;&#20102;&#19968;&#31181;&#36335;&#30001;&#38382;&#39064;&#30340;&#35780;&#20272;&#21327;&#35758;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#21327;&#35758;&#30340;&#32570;&#38519;&#65292;&#24182;&#25552;&#20379;&#20102;&#19982;&#36816;&#31609;&#23398;&#26041;&#27861;&#30456;&#27604;&#30340;&#35780;&#20272;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2310.04140</link><description>&lt;p&gt;
Routing Arena: &#29992;&#20110;&#31070;&#32463;&#36335;&#30001;&#27714;&#35299;&#22120;&#30340;&#22522;&#20934;&#22871;&#20214;
&lt;/p&gt;
&lt;p&gt;
Routing Arena: A Benchmark Suite for Neural Routing Solvers. (arXiv:2310.04140v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04140
&lt;/p&gt;
&lt;p&gt;
Routing Arena&#26159;&#19968;&#20010;&#22522;&#20934;&#22871;&#20214;&#65292;&#25512;&#20986;&#20102;&#19968;&#31181;&#36335;&#30001;&#38382;&#39064;&#30340;&#35780;&#20272;&#21327;&#35758;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#21327;&#35758;&#30340;&#32570;&#38519;&#65292;&#24182;&#25552;&#20379;&#20102;&#19982;&#36816;&#31609;&#23398;&#26041;&#27861;&#30456;&#27604;&#30340;&#35780;&#20272;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;8&#24180;&#26469;&#65292;&#31070;&#32463;&#32452;&#21512;&#20248;&#21270;&#19968;&#30452;&#21463;&#21040;&#31215;&#26497;&#30740;&#31350;&#12290;&#23613;&#31649;&#35768;&#22810;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#37117;&#22312;&#30456;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#20294;&#35780;&#20272;&#21327;&#35758;&#23384;&#22312;&#37325;&#35201;&#32570;&#38519;&#65292;&#24182;&#19988;&#22522;&#20934;&#30340;&#36873;&#25321;&#36890;&#24120;&#24573;&#30053;&#20102;&#26368;&#20808;&#36827;&#30340;&#36816;&#31609;&#23398;&#26041;&#27861;&#12290;&#20026;&#20102;&#25913;&#36827;&#36825;&#20004;&#20010;&#32570;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Routing Arena&#65292;&#19968;&#20010;&#29992;&#20110;&#36335;&#30001;&#38382;&#39064;&#30340;&#22522;&#20934;&#22871;&#20214;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#19968;&#33268;&#35780;&#20272;&#21644;&#26426;&#22120;&#23398;&#20064;&#21644;&#36816;&#31609;&#23398;&#39046;&#22495;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#22522;&#20934;&#30340;&#26080;&#32541;&#38598;&#25104;&#12290;&#25152;&#25552;&#20986;&#30340;&#35780;&#20272;&#21327;&#35758;&#32771;&#34385;&#20102;&#19981;&#21516;&#24212;&#29992;&#30340;&#20004;&#20010;&#26368;&#37325;&#35201;&#30340;&#35780;&#20272;&#24773;&#20917;&#65306;&#39318;&#20808;&#65292;&#19968;&#20010;&#22266;&#23450;&#26102;&#38388;&#39044;&#31639;&#30340;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#65292;&#20854;&#27425;&#26159;&#21508;&#31181;&#26041;&#27861;&#30340;&#23454;&#26102;&#24615;&#33021;&#12290;&#36890;&#36807;&#23558;&#35299;&#20915;&#26041;&#26696;&#36712;&#36857;&#19982;&#26368;&#20339;&#24050;&#30693;&#35299;&#21644;&#22522;&#20934;&#27714;&#35299;&#22120;&#30340;&#35299;&#20915;&#26041;&#26696;&#36712;&#36857;&#30456;&#23545;&#27604;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Combinatorial Optimization has been researched actively in the last eight years. Even though many of the proposed Machine Learning based approaches are compared on the same datasets, the evaluation protocol exhibits essential flaws and the selection of baselines often neglects State-of-the-Art Operations Research approaches. To improve on both of these shortcomings, we propose the Routing Arena, a benchmark suite for Routing Problems that provides a seamless integration of consistent evaluation and the provision of baselines and benchmarks prevalent in the Machine Learning- and Operations Research field. The proposed evaluation protocol considers the two most important evaluation cases for different applications: First, the solution quality for an a priori fixed time budget and secondly the anytime performance of the respective methods. By setting the solution trajectory in perspective to a Best Known Solution and a Base Solver's solutions trajectory, we furthermore propose the 
&lt;/p&gt;</description></item><item><title>&#20197;&#24555;&#36895;&#36951;&#24536;&#35760;&#24518;&#20026;&#29305;&#28857;&#30340;&#31639;&#27861;&#26080;&#20851;&#30340;&#35760;&#24518;&#27169;&#22411;&#38024;&#23545;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#37096;&#20998;&#21487;&#35266;&#27979;&#20219;&#21153;&#65292;&#36890;&#36807;&#24378;&#32467;&#26500;&#20808;&#39564;&#38480;&#21046;&#27169;&#22411;&#25628;&#32034;&#31354;&#38388;&#65292;&#23454;&#29616;&#20102;&#27604;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#26356;&#39640;&#30340;&#22870;&#21169;&#24182;&#20855;&#26377;&#26356;&#24555;&#30340;&#35757;&#32451;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.04128</link><description>&lt;p&gt;
&#20197;&#24555;&#36895;&#36951;&#24536;&#35760;&#24518;&#20026;&#29305;&#28857;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning with Fast and Forgetful Memory. (arXiv:2310.04128v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04128
&lt;/p&gt;
&lt;p&gt;
&#20197;&#24555;&#36895;&#36951;&#24536;&#35760;&#24518;&#20026;&#29305;&#28857;&#30340;&#31639;&#27861;&#26080;&#20851;&#30340;&#35760;&#24518;&#27169;&#22411;&#38024;&#23545;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#37096;&#20998;&#21487;&#35266;&#27979;&#20219;&#21153;&#65292;&#36890;&#36807;&#24378;&#32467;&#26500;&#20808;&#39564;&#38480;&#21046;&#27169;&#22411;&#25628;&#32034;&#31354;&#38388;&#65292;&#23454;&#29616;&#20102;&#27604;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#26356;&#39640;&#30340;&#22870;&#21169;&#24182;&#20855;&#26377;&#26356;&#24555;&#30340;&#35757;&#32451;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20960;&#20046;&#25152;&#26377;&#30340;&#29616;&#23454;&#19990;&#30028;&#20219;&#21153;&#37117;&#26159;&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#65292;&#24517;&#39035;&#22312;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20013;&#20351;&#29992;&#35760;&#24518;&#12290;&#22823;&#22810;&#25968;&#26080;&#27169;&#22411;&#26041;&#27861;&#20351;&#29992;&#20174;&#30417;&#30563;&#23398;&#20064;&#65288;SL&#65289;&#20511;&#29992;&#30340;&#35760;&#24518;&#27169;&#22411;&#23558;&#36712;&#36857;&#27719;&#24635;&#20026;&#28508;&#22312;&#30340;&#39532;&#23572;&#21487;&#22827;&#29366;&#24577;&#65292;&#23613;&#31649;RL&#24448;&#24448;&#34920;&#29616;&#20986;&#19981;&#21516;&#30340;&#35757;&#32451;&#21644;&#25928;&#29575;&#29305;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#24046;&#24322;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#24555;&#36895;&#36951;&#24536;&#35760;&#24518;&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;RL&#29305;&#21035;&#35774;&#35745;&#30340;&#31639;&#27861;&#26080;&#20851;&#30340;&#35760;&#24518;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#21463;&#35745;&#31639;&#24515;&#29702;&#23398;&#21551;&#21457;&#30340;&#24378;&#32467;&#26500;&#20808;&#39564;&#26469;&#38480;&#21046;&#27169;&#22411;&#25628;&#32034;&#31354;&#38388;&#12290;&#23427;&#22312;&#36882;&#24402;RL&#31639;&#27861;&#20013;&#21487;&#20197;&#26367;&#20195;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#65292;&#22312;&#21508;&#31181;&#36882;&#24402;&#22522;&#20934;&#21644;&#31639;&#27861;&#20013;&#23454;&#29616;&#20102;&#27604;RNN&#26356;&#39640;&#30340;&#22870;&#21169;&#65292;&#32780;&#19981;&#38656;&#35201;&#25913;&#21464;&#20219;&#20309;&#36229;&#21442;&#25968;&#12290;&#27492;&#22806;&#65292;&#24555;&#36895;&#36951;&#24536;&#35760;&#24518;&#30340;&#35757;&#32451;&#36895;&#24230;&#27604;RNN&#24555;&#20004;&#20010;&#25968;&#37327;&#32423;&#65292;&#36825;&#24402;&#22240;&#20110;&#23427;&#30340;&#23545;&#25968;&#26102;&#38388;&#21644;&#32447;&#24615;&#31354;&#38388;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nearly all real world tasks are inherently partially observable, necessitating the use of memory in Reinforcement Learning (RL). Most model-free approaches summarize the trajectory into a latent Markov state using memory models borrowed from Supervised Learning (SL), even though RL tends to exhibit different training and efficiency characteristics. Addressing this discrepancy, we introduce Fast and Forgetful Memory, an algorithm-agnostic memory model designed specifically for RL. Our approach constrains the model search space via strong structural priors inspired by computational psychology. It is a drop-in replacement for recurrent neural networks (RNNs) in recurrent RL algorithms, achieving greater reward than RNNs across various recurrent benchmarks and algorithms without changing any hyperparameters. Moreover, Fast and Forgetful Memory exhibits training speeds two orders of magnitude faster than RNNs, attributed to its logarithmic time and linear space complexity. Our implementatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#29616;&#20102;&#20174;&#27491;&#26679;&#26412;&#21644;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#19968;&#20010;&#37325;&#35201;&#35266;&#23519;&#65306;&#36890;&#36807;&#22312;&#27599;&#20010;&#35757;&#32451;&#36845;&#20195;&#20013;&#37325;&#26032;&#37319;&#26679;&#27491;&#26679;&#26412;&#65292;&#21487;&#20197;&#33719;&#24471;&#24378;&#22823;&#30340;&#26089;&#26399;&#24615;&#33021;&#65292;&#24182;&#19988;&#27491;&#31867;&#21644;&#36127;&#31867;&#30340;&#39044;&#27979;&#36235;&#21183;&#26174;&#31034;&#20986;&#19981;&#21516;&#30340;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2310.04078</link><description>&lt;p&gt;
&#36229;&#36234;&#36817;&#35270;&#65306;&#36890;&#36807;&#25972;&#20307;&#39044;&#27979;&#36235;&#21183;&#20174;&#27491;&#26679;&#26412;&#21644;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Beyond Myopia: Learning from Positive and Unlabeled Data through Holistic Predictive Trends. (arXiv:2310.04078v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04078
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#29616;&#20102;&#20174;&#27491;&#26679;&#26412;&#21644;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#19968;&#20010;&#37325;&#35201;&#35266;&#23519;&#65306;&#36890;&#36807;&#22312;&#27599;&#20010;&#35757;&#32451;&#36845;&#20195;&#20013;&#37325;&#26032;&#37319;&#26679;&#27491;&#26679;&#26412;&#65292;&#21487;&#20197;&#33719;&#24471;&#24378;&#22823;&#30340;&#26089;&#26399;&#24615;&#33021;&#65292;&#24182;&#19988;&#27491;&#31867;&#21644;&#36127;&#31867;&#30340;&#39044;&#27979;&#36235;&#21183;&#26174;&#31034;&#20986;&#19981;&#21516;&#30340;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#20174;&#27491;&#26679;&#26412;&#21644;&#26080;&#26631;&#31614;&#25968;&#25454;(PUL)&#20013;&#23398;&#20064;&#20108;&#20803;&#20998;&#31867;&#22120;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#29305;&#21035;&#26159;&#22312;&#39564;&#35777;&#36127;&#26679;&#26412;&#22256;&#38590;&#30340;&#24773;&#20917;&#19979;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;PUL&#26041;&#27861;&#22312;&#23454;&#39564;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#36127;&#26631;&#31614;&#65292;&#20173;&#28982;&#23384;&#22312;&#31215;&#32047;&#35823;&#24046;&#21644;&#22686;&#21152;&#20272;&#35745;&#20559;&#24046;&#31561;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;PUL&#20013;&#19968;&#20010;&#24341;&#20154;&#27880;&#30446;&#20294;&#38271;&#26399;&#34987;&#24573;&#35270;&#30340;&#35266;&#23519;&#32467;&#26524;&#65306;\textit{&#22312;&#27599;&#20010;&#35757;&#32451;&#36845;&#20195;&#20013;&#37325;&#26032;&#37319;&#26679;&#27491;&#26679;&#26412;&#20197;&#30830;&#20445;&#27491;&#26679;&#26412;&#21644;&#26080;&#26631;&#31614;&#26679;&#26412;&#20043;&#38388;&#30340;&#24179;&#34913;&#20998;&#24067;&#20250;&#23548;&#33268;&#24378;&#30340;&#26089;&#26399;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#27491;&#31867;&#21644;&#36127;&#31867;&#30340;&#39044;&#27979;&#36235;&#21183;&#26174;&#31034;&#20986;&#19981;&#21516;&#30340;&#27169;&#24335;&#12290;}&#20855;&#20307;&#32780;&#35328;&#65292;&#26080;&#26631;&#31614;&#36127;&#26679;&#26412;&#30340;&#24471;&#20998;(&#36755;&#20986;&#27010;&#29575;)&#25345;&#32493;&#19979;&#38477;&#65292;&#32780;&#26080;&#26631;&#31614;&#27491;&#26679;&#26412;&#30340;&#24471;&#20998;&#26174;&#31034;&#20986;&#22823;&#33268;&#28151;&#20081;&#30340;&#36235;&#21183;&#12290;&#25105;&#20204;&#21019;&#26032;&#22320;&#37319;&#29992;&#20102;&#19968;&#31181;&#25972;&#20307;&#26041;&#27861;&#65292;&#32780;&#19981;&#26159;&#22312;&#20010;&#21035;&#26102;&#38388;&#27573;&#20869;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning binary classifiers from positive and unlabeled data (PUL) is vital in many real-world applications, especially when verifying negative examples is difficult. Despite the impressive empirical performance of recent PUL methods, challenges like accumulated errors and increased estimation bias persist due to the absence of negative labels. In this paper, we unveil an intriguing yet long-overlooked observation in PUL: \textit{resampling the positive data in each training iteration to ensure a balanced distribution between positive and unlabeled examples results in strong early-stage performance. Furthermore, predictive trends for positive and negative classes display distinctly different patterns.} Specifically, the scores (output probability) of unlabeled negative examples consistently decrease, while those of unlabeled positive examples show largely chaotic trends. Instead of focusing on classification within individual time frames, we innovatively adopt a holistic approach, inte
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#20174;&#20420;&#35821;&#31185;&#23398;&#25991;&#26412;&#20013;&#33258;&#21160;&#25552;&#21462;&#26041;&#38754;&#30340;&#24037;&#20855;&#65292;&#21253;&#25324;&#20219;&#21153;&#12289;&#36129;&#29486;&#12289;&#26041;&#27861;&#21644;&#32467;&#35770;&#31561;&#26041;&#38754;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;BERT&#27169;&#22411;&#30340;&#22522;&#20934;&#31639;&#27861;&#12290;&#36890;&#36807;&#36328;&#39046;&#22495;&#23454;&#39564;&#35777;&#26126;&#65292;&#21363;&#20351;&#22312;&#26377;&#38480;&#30340;&#31185;&#23398;&#39046;&#22495;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#35813;&#27169;&#22411;&#20173;&#33021;&#25512;&#24191;&#21040;&#26032;&#30340;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2310.04074</link><description>&lt;p&gt;
&#20174;&#31185;&#23398;&#25991;&#26412;&#20013;&#33258;&#21160;&#25552;&#21462;&#26041;&#38754;
&lt;/p&gt;
&lt;p&gt;
Automatic Aspect Extraction from Scientific Texts. (arXiv:2310.04074v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04074
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#20174;&#20420;&#35821;&#31185;&#23398;&#25991;&#26412;&#20013;&#33258;&#21160;&#25552;&#21462;&#26041;&#38754;&#30340;&#24037;&#20855;&#65292;&#21253;&#25324;&#20219;&#21153;&#12289;&#36129;&#29486;&#12289;&#26041;&#27861;&#21644;&#32467;&#35770;&#31561;&#26041;&#38754;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;BERT&#27169;&#22411;&#30340;&#22522;&#20934;&#31639;&#27861;&#12290;&#36890;&#36807;&#36328;&#39046;&#22495;&#23454;&#39564;&#35777;&#26126;&#65292;&#21363;&#20351;&#22312;&#26377;&#38480;&#30340;&#31185;&#23398;&#39046;&#22495;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#35813;&#27169;&#22411;&#20173;&#33021;&#25512;&#24191;&#21040;&#26032;&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#22815;&#20174;&#31185;&#23398;&#35770;&#25991;&#20013;&#25552;&#21462;&#20986;&#20027;&#35201;&#35266;&#28857;&#12289;&#20851;&#38190;&#35265;&#35299;&#21644;&#20854;&#20182;&#37325;&#35201;&#20449;&#24687;&#65288;&#22312;&#27492;&#31216;&#20026;&#26041;&#38754;&#65289;&#65292;&#21487;&#33021;&#26377;&#21161;&#20110;&#36827;&#34892;&#31185;&#23398;&#25991;&#29486;&#32508;&#36848;&#30340;&#36807;&#31243;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#30446;&#30340;&#26159;&#21019;&#24314;&#19968;&#20010;&#29992;&#20110;&#20174;&#20219;&#20309;&#39046;&#22495;&#30340;&#20420;&#35821;&#31185;&#23398;&#25991;&#26412;&#20013;&#33258;&#21160;&#25552;&#21462;&#26041;&#38754;&#30340;&#24037;&#20855;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;&#20420;&#35821;&#31185;&#23398;&#25991;&#26412;&#30340;&#36328;&#39046;&#22495;&#25968;&#25454;&#38598;&#65292;&#27880;&#37322;&#26377;&#20219;&#21153;&#12289;&#36129;&#29486;&#12289;&#26041;&#27861;&#21644;&#32467;&#35770;&#31561;&#26041;&#38754;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#35821;&#35328;BERT&#27169;&#22411;&#22312;&#25105;&#20204;&#30340;&#25968;&#25454;&#19978;&#24494;&#35843;&#30340;&#22522;&#20934;&#31639;&#27861;&#36827;&#34892;&#26041;&#38754;&#25552;&#21462;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#19981;&#21516;&#30340;&#39046;&#22495;&#20013;&#26041;&#38754;&#30340;&#34920;&#31034;&#23384;&#22312;&#19968;&#20123;&#24046;&#24322;&#65292;&#20294;&#21363;&#20351;&#25105;&#20204;&#30340;&#27169;&#22411;&#26159;&#22312;&#26377;&#38480;&#25968;&#37327;&#30340;&#31185;&#23398;&#39046;&#22495;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#23427;&#20173;&#28982;&#33021;&#22815;&#25512;&#24191;&#21040;&#26032;&#30340;&#39046;&#22495;&#65292;&#36825;&#22312;&#36328;&#39046;&#22495;&#23454;&#39564;&#20013;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#21487;&#22312; \url{https://github.com/anna-marshalova/automatic-aspect-extraction-from} &#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Being able to extract from scientific papers their main points, key insights, and other important information, referred to here as aspects, might facilitate the process of conducting a scientific literature review. Therefore, the aim of our research is to create a tool for automatic aspect extraction from Russian-language scientific texts of any domain. In this paper, we present a cross-domain dataset of scientific texts in Russian, annotated with such aspects as Task, Contribution, Method, and Conclusion, as well as a baseline algorithm for aspect extraction, based on the multilingual BERT model fine-tuned on our data. We show that there are some differences in aspect representation in different domains, but even though our model was trained on a limited number of scientific domains, it is still able to generalize to new domains, as was proved by cross-domain experiments. The code and the dataset are available at \url{https://github.com/anna-marshalova/automatic-aspect-extraction-from
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#32463;&#20856;transformer attention&#27867;&#21270;&#21040;&#33021;&#22815;&#25429;&#25417;&#19977;&#20803;&#30456;&#20851;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#26377;&#30028;&#36755;&#20837;&#19979;&#20855;&#26377;&#36817;&#32447;&#24615;&#26102;&#38388;&#22797;&#26434;&#24230;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.04064</link><description>&lt;p&gt;
&#22914;&#20309;&#25429;&#25417;&#39640;&#38454;&#30456;&#20851;&#24615;&#65311;&#23558;&#30697;&#38453;Softmax Attention&#25512;&#24191;&#21040;Kronecker&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
How to Capture Higher-order Correlations? Generalizing Matrix Softmax Attention to Kronecker Computation. (arXiv:2310.04064v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04064
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#32463;&#20856;transformer attention&#27867;&#21270;&#21040;&#33021;&#22815;&#25429;&#25417;&#19977;&#20803;&#30456;&#20851;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#26377;&#30028;&#36755;&#20837;&#19979;&#20855;&#26377;&#36817;&#32447;&#24615;&#26102;&#38388;&#22797;&#26434;&#24230;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32463;&#20856;&#30340;transformer attention&#26041;&#26696;&#20013;&#65292;&#25105;&#20204;&#32473;&#23450;&#19977;&#20010;&#22823;&#23567;&#20026;$n \times d$&#30340;&#30697;&#38453;$Q, K, V$&#65288;&#26597;&#35810;&#12289;&#38190;&#21644;&#20540;&#26631;&#35760;&#65289;&#65292;&#30446;&#26631;&#26159;&#35745;&#31639;&#19968;&#20010;&#26032;&#30340;&#22823;&#23567;&#20026;$n \times d$&#30340;&#30697;&#38453;$D^{-1} \exp(QK^\top) V$&#65292;&#20854;&#20013;$D = \mathrm{diag}( \exp(QK^\top) {\bf 1}_n )$&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#33021;&#22815;&#25429;&#25417;&#19977;&#20803;&#30456;&#20851;&#24615;&#30340;&#27880;&#24847;&#21147;&#30340;&#27867;&#21270;&#12290;&#36825;&#31181;&#27867;&#21270;&#33021;&#22815;&#35299;&#20915;&#20851;&#20110;&#26816;&#27979;transformers&#26080;&#27861;&#35299;&#20915;&#30340;&#19977;&#20803;&#36830;&#25509;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#27867;&#21270;&#30340;&#28508;&#22312;&#32570;&#28857;&#26159;&#65292;&#35745;&#31639;&#20284;&#20046;&#26356;&#21152;&#22256;&#38590;&#65292;&#22240;&#20026;&#30452;&#25509;&#30340;&#31639;&#27861;&#22312;$n$&#30340;&#31435;&#26041;&#26102;&#38388;&#20869;&#23436;&#25104;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#26377;&#30028;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#65288;&#23454;&#36341;&#20013;&#32463;&#24120;&#20986;&#29616;&#65292;&#24182;&#19988;&#22312;&#29702;&#35770;&#21644;&#23454;&#36341;&#20013;&#37117;&#26377;&#24191;&#27867;&#30740;&#31350;&#65289;&#65292;&#23454;&#38469;&#19978;&#23384;&#22312;&#19968;&#20010;&#36817;&#32447;&#24615;&#26102;&#38388;&#30340;&#31639;&#27861;&#12290;&#26356;&#20934;&#30830;&#22320;&#35828;&#65292;&#25105;&#20204;&#35777;&#26126;&#26377;&#30028;&#36755;&#20837;&#26082;&#26159;&#24555;&#36895;&#25191;&#34892;&#24191;&#20041;&#35745;&#31639;&#30340;&#24517;&#35201;&#26465;&#20214;&#20063;&#26159;&#20805;&#20998;&#26465;&#20214;&#65306; $\bul
&lt;/p&gt;
&lt;p&gt;
In the classical transformer attention scheme, we are given three $n \times d$ size matrices $Q, K, V$ (the query, key, and value tokens), and the goal is to compute a new $n \times d$ size matrix $D^{-1} \exp(QK^\top) V$ where $D = \mathrm{diag}( \exp(QK^\top) {\bf 1}_n )$. In this work, we study a generalization of attention which captures triple-wise correlations. This generalization is able to solve problems about detecting triple-wise connections that were shown to be impossible for transformers. The potential downside of this generalization is that it appears as though computations are even more difficult, since the straightforward algorithm requires cubic time in $n$. However, we show that in the bounded-entry setting (which arises in practice, and which is well-studied in both theory and practice), there is actually a near-linear time algorithm. More precisely, we show that bounded entries are both necessary and sufficient for quickly performing generalized computations:  $\bul
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38190;&#30424;&#19978;&#25353;&#38190;&#20043;&#38388;&#36317;&#31163;&#30340;&#26032;&#29305;&#24449;&#38598;&#21512;(DEFT)&#65292;&#35813; DEFT &#27169;&#22411;&#22312;&#38190;&#20837;&#21160;&#21147;&#23398;&#30740;&#31350;&#20013;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#25171;&#23383;&#36895;&#24230;&#65292;&#36890;&#36807;&#20351;&#29992; DEFT &#29305;&#24449;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#20010;&#20154;&#30340;&#25171;&#23383;&#34892;&#20026;&#30340;&#20840;&#38754;&#27934;&#23519;&#12290;&#35813;&#27169;&#22411;&#22312;&#21508;&#31181;&#35774;&#22791;&#19978;&#37117;&#21462;&#24471;&#20102;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.04059</link><description>&lt;p&gt;
DEFT&#65306;&#19968;&#31181;&#22522;&#20110;&#36317;&#31163;&#30340;&#38190;&#20837;&#21160;&#21147;&#23398;&#29305;&#24449;&#38598;
&lt;/p&gt;
&lt;p&gt;
DEFT: A new distance-based feature set for keystroke dynamics. (arXiv:2310.04059v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04059
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38190;&#30424;&#19978;&#25353;&#38190;&#20043;&#38388;&#36317;&#31163;&#30340;&#26032;&#29305;&#24449;&#38598;&#21512;(DEFT)&#65292;&#35813; DEFT &#27169;&#22411;&#22312;&#38190;&#20837;&#21160;&#21147;&#23398;&#30740;&#31350;&#20013;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#25171;&#23383;&#36895;&#24230;&#65292;&#36890;&#36807;&#20351;&#29992; DEFT &#29305;&#24449;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#20010;&#20154;&#30340;&#25171;&#23383;&#34892;&#20026;&#30340;&#20840;&#38754;&#27934;&#23519;&#12290;&#35813;&#27169;&#22411;&#22312;&#21508;&#31181;&#35774;&#22791;&#19978;&#37117;&#21462;&#24471;&#20102;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38190;&#20837;&#21160;&#21147;&#23398;&#26159;&#19968;&#31181;&#29992;&#20110;&#29992;&#25143;&#35782;&#21035;&#21644;&#35748;&#35777;&#30340;&#34892;&#20026;&#29983;&#29289;&#29305;&#24449;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29305;&#24449;&#38598;&#65292;&#22522;&#20110;&#38190;&#30424;&#19978;&#25353;&#38190;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#36825;&#26159;&#22312;&#38190;&#20837;&#21160;&#21147;&#23398;&#20013;&#20197;&#21069;&#27809;&#26377;&#32771;&#34385;&#36807;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#23558;&#39134;&#34892;&#26102;&#38388;&#36825;&#19968;&#27969;&#34892;&#24230;&#37327;&#19982;&#38190;&#30424;&#19978;&#30340;&#25353;&#38190;&#36317;&#31163;&#32467;&#21512;&#36215;&#26469;&#65292;&#31216;&#20043;&#20026;Distance Enhanced Flight Time&#29305;&#24449;(DEFT)&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#23545;&#20010;&#20154;&#25171;&#23383;&#34892;&#20026;&#30340;&#20840;&#38754;&#27934;&#23519;&#65292;&#36229;&#36807;&#20102;&#20165;&#32771;&#34385;&#25171;&#23383;&#36895;&#24230;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;DEFT&#29305;&#24449;&#19982;&#20808;&#21069;&#20351;&#29992;&#30340;&#20854;&#20182;&#38190;&#20837;&#21160;&#21147;&#23398;&#29305;&#24449;&#30456;&#32467;&#21512;&#26469;&#26500;&#24314;DEFT&#27169;&#22411;&#12290;DEFT&#27169;&#22411;&#35774;&#35745;&#25104;&#36866;&#29992;&#20110;&#21508;&#31181;&#35774;&#22791;&#65292;&#35753;&#25105;&#20204;&#21487;&#20197;&#22312;&#19977;&#31181;&#24120;&#29992;&#35774;&#22791;&#65288;&#21488;&#24335;&#26426;&#12289;&#31227;&#21160;&#35774;&#22791;&#21644;&#24179;&#26495;&#30005;&#33041;&#65289;&#19978;&#35780;&#20272;&#20854;&#26377;&#25928;&#24615;&#12290;&#24403;&#25105;&#20204;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20854;&#26377;&#25928;&#24615;&#26102;&#65292;DEFT&#27169;&#22411;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#25105;&#20204;&#33719;&#24471;&#36229;&#36807;99%&#30340;&#20934;&#30830;&#29575;&#21644;&#23567;&#20110;10%&#30340;&#31561;&#35823;&#24046;&#29575;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Keystroke dynamics is a behavioural biometric utilised for user identification and authentication. We propose a new set of features based on the distance between keys on the keyboard, a concept that has not been considered before in keystroke dynamics. We combine flight times, a popular metric, with the distance between keys on the keyboard and call them as Distance Enhanced Flight Time features (DEFT). This novel approach provides comprehensive insights into a person's typing behaviour, surpassing typing velocity alone. We build a DEFT model by combining DEFT features with other previously used keystroke dynamic features. The DEFT model is designed to be device-agnostic, allowing us to evaluate its effectiveness across three commonly used devices: desktop, mobile, and tablet. The DEFT model outperforms the existing state-of-the-art methods when we evaluate its effectiveness across two datasets. We obtain accuracy rates exceeding 99% and equal error rates below 10% on all three devices
&lt;/p&gt;</description></item><item><title>AUTOPARLLM&#26159;&#19968;&#20010;&#29992;&#20110;&#33258;&#21160;&#21457;&#29616;&#24182;&#29983;&#25104;&#39034;&#24207;&#32534;&#20889;&#31243;&#24207;&#24182;&#34892;&#29256;&#26412;&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#22522;&#20110;GNN&#30340;&#24182;&#34892;&#24615;&#21457;&#29616;&#27169;&#22359;&#21644;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#20195;&#30721;&#29983;&#25104;&#22120;&#12290;</title><link>http://arxiv.org/abs/2310.04047</link><description>&lt;p&gt;
AUTOPARLLM&#65306;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;GNN&#24341;&#23548;&#30340;&#33258;&#21160;&#20195;&#30721;&#24182;&#34892;&#21270;
&lt;/p&gt;
&lt;p&gt;
AUTOPARLLM: GNN-Guided Automatic Code Parallelization using Large Language Models. (arXiv:2310.04047v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04047
&lt;/p&gt;
&lt;p&gt;
AUTOPARLLM&#26159;&#19968;&#20010;&#29992;&#20110;&#33258;&#21160;&#21457;&#29616;&#24182;&#29983;&#25104;&#39034;&#24207;&#32534;&#20889;&#31243;&#24207;&#24182;&#34892;&#29256;&#26412;&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#22522;&#20110;GNN&#30340;&#24182;&#34892;&#24615;&#21457;&#29616;&#27169;&#22359;&#21644;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#20195;&#30721;&#29983;&#25104;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24182;&#34892;&#21270;&#39034;&#24207;&#32534;&#20889;&#30340;&#31243;&#24207;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#21363;&#20351;&#26159;&#32463;&#39564;&#20016;&#23500;&#30340;&#24320;&#21457;&#20154;&#21592;&#20063;&#38656;&#35201;&#33457;&#36153;&#30456;&#24403;&#22810;&#30340;&#26102;&#38388;&#26469;&#23547;&#25214;&#24182;&#34892;&#24615;&#26426;&#20250;&#65292;&#28982;&#21518;&#23454;&#38469;&#32534;&#20889;&#39034;&#24207;&#32534;&#20889;&#31243;&#24207;&#30340;&#24182;&#34892;&#29256;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AUTOPARLLM&#65292;&#19968;&#20010;&#29992;&#20110;&#33258;&#21160;&#21457;&#29616;&#24182;&#34892;&#24615;&#24182;&#29983;&#25104;&#39034;&#24207;&#32534;&#20889;&#31243;&#24207;&#30340;&#24182;&#34892;&#29256;&#26412;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#32452;&#20214;&#65306;i&#65289;&#22522;&#20110;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#24182;&#34892;&#24615;&#21457;&#29616;&#21644;&#24182;&#34892;&#27169;&#24335;&#26816;&#27979;&#27169;&#22359;&#65292;&#20197;&#21450;ii&#65289;&#22522;&#20110;LLM&#30340;&#20195;&#30721;&#29983;&#25104;&#22120;&#65292;&#29992;&#20110;&#29983;&#25104;&#39034;&#24207;&#31243;&#24207;&#30340;&#24182;&#34892;&#23545;&#24212;&#29256;&#26412;&#12290;&#25105;&#20204;&#20351;&#29992;GNN&#23398;&#20064;&#31243;&#24207;&#30340;&#27969;&#25935;&#24863;&#29305;&#24449;&#65292;&#20197;&#35782;&#21035;&#39034;&#24207;&#31243;&#24207;&#20013;&#30340;&#24182;&#34892;&#21306;&#22495;&#65292;&#24182;&#20351;&#29992;GNN&#30340;&#32467;&#26524;&#26500;&#24314;&#22686;&#24378;&#25552;&#31034;&#65292;&#20197;&#20379;LLM&#22522;&#30784;&#29983;&#25104;&#22120;&#26368;&#32456;&#20135;&#29983;&#39034;&#24207;&#31243;&#24207;&#30340;&#24182;&#34892;&#23545;&#24212;&#29256;&#26412;&#12290;&#25105;&#20204;&#22312;11&#20010;&#24212;&#29992;&#19978;&#35780;&#20272;&#20102;AUTOPARLLM
&lt;/p&gt;
&lt;p&gt;
Parallelizing sequentially written programs is a challenging task. Even experienced developers need to spend considerable time finding parallelism opportunities and then actually writing parallel versions of sequentially written programs. To address this issue, we present AUTOPARLLM, a framework for automatically discovering parallelism and generating the parallel version of the sequentially written program. Our framework consists of two major components: i) a heterogeneous Graph Neural Network (GNN) based parallelism discovery and parallel pattern detection module, and ii) an LLM-based code generator to generate the parallel counterpart of the sequential programs. We use the GNN to learn the flow-aware characteristics of the programs to identify parallel regions in sequential programs and then construct an enhanced prompt using the GNN's results for the LLM-based generator to finally produce the parallel counterparts of the sequential programs. We evaluate AUTOPARLLM on 11 application
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#35266;&#27979;&#24341;&#23548;&#30340;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;OGDM&#65289;&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#26465;&#20214;&#37492;&#21035;&#22120;&#30340;&#35266;&#27979;&#25152;&#20135;&#29983;&#30340;&#39069;&#22806;&#25439;&#22833;&#39033;&#65292;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#30340;&#36127;&#23545;&#25968;&#20284;&#28982;&#20248;&#21270;&#65292;&#22312;&#20989;&#25968;&#35780;&#20272;&#27425;&#25968;&#26377;&#38480;&#30340;&#25512;&#29702;&#38454;&#27573;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2310.04041</link><description>&lt;p&gt;
&#35266;&#27979;&#24341;&#23548;&#30340;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Observation-Guided Diffusion Probabilistic Models. (arXiv:2310.04041v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04041
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#35266;&#27979;&#24341;&#23548;&#30340;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;OGDM&#65289;&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#26465;&#20214;&#37492;&#21035;&#22120;&#30340;&#35266;&#27979;&#25152;&#20135;&#29983;&#30340;&#39069;&#22806;&#25439;&#22833;&#39033;&#65292;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#30340;&#36127;&#23545;&#25968;&#20284;&#28982;&#20248;&#21270;&#65292;&#22312;&#20989;&#25968;&#35780;&#20272;&#27425;&#25968;&#26377;&#38480;&#30340;&#25512;&#29702;&#38454;&#27573;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#31216;&#20026;&#35266;&#27979;&#24341;&#23548;&#30340;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;OGDM&#65289;&#65292;&#23427;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#36136;&#37327;&#25511;&#21046;&#21644;&#24555;&#36895;&#37319;&#26679;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20197;&#21407;&#21017;&#24615;&#30340;&#26041;&#24335;&#23558;&#35266;&#27979;&#36807;&#31243;&#30340;&#24341;&#23548;&#19982;&#39532;&#23572;&#21487;&#22827;&#38142;&#30456;&#32467;&#21512;&#65292;&#37325;&#26032;&#24314;&#31435;&#20102;&#35757;&#32451;&#30446;&#26631;&#12290;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#26465;&#20214;&#37492;&#21035;&#22120;&#30340;&#35266;&#27979;&#25152;&#20135;&#29983;&#30340;&#39069;&#22806;&#25439;&#22833;&#39033;&#65292;&#25105;&#20204;&#20351;&#24471;&#20248;&#21270;&#26356;&#20934;&#30830;&#30340;&#36127;&#23545;&#25968;&#20284;&#28982;&#25104;&#20026;&#21487;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#20989;&#25968;&#35780;&#20272;&#27425;&#25968;&#26377;&#38480;&#30340;&#25512;&#29702;&#38454;&#27573;&#12290;&#36825;&#31181;&#31574;&#30053;&#20351;&#24471;&#25105;&#20204;&#30340;&#35757;&#32451;&#26041;&#27861;&#21363;&#20351;&#21482;&#29992;&#20110;&#24494;&#35843;&#36807;&#31243;&#20063;&#20855;&#26377;&#20248;&#21183;&#65292;&#24182;&#19988;&#19982;&#21508;&#31181;&#24555;&#36895;&#25512;&#29702;&#31574;&#30053;&#20860;&#23481;&#65292;&#22240;&#20026;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#23436;&#20840;&#30456;&#21516;&#30340;&#25512;&#29702;&#36807;&#31243;&#20135;&#29983;&#26356;&#22909;&#30340;&#21435;&#22122;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel diffusion model called observation-guided diffusion probabilistic model (OGDM), which effectively addresses the trade-off between quality control and fast sampling. Our approach reestablishes the training objective by integrating the guidance of the observation process with the Markov chain in a principled way. This is achieved by introducing an additional loss term derived from the observation based on the conditional discriminator on noise level, which employs Bernoulli distribution indicating whether its input lies on the (noisy) real manifold or not. This strategy allows us to optimize the more accurate negative log-likelihood induced in the inference stage especially when the number of function evaluations is limited. The proposed training method is also advantageous even when incorporated only into the fine-tuning process, and it is compatible with various fast inference strategies since our method yields better denoising networks using the exactly same inferen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#21512;&#25237;&#24433;&#23398;&#20064;&#21644;&#24352;&#37327;&#20998;&#35299;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#19981;&#23436;&#25972;&#22810;&#35270;&#22270;&#32858;&#31867;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#27491;&#20132;&#25237;&#24433;&#30697;&#38453;&#23558;&#39640;&#32500;&#29305;&#24449;&#25237;&#24433;&#21040;&#20302;&#32500;&#31354;&#38388;&#20013;&#65292;&#20197;&#20943;&#36731;&#20887;&#20313;&#29305;&#24449;&#21644;&#22122;&#22768;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.04038</link><description>&lt;p&gt;
&#32852;&#21512;&#25237;&#24433;&#23398;&#20064;&#21644;&#24352;&#37327;&#20998;&#35299;&#30340;&#19981;&#23436;&#25972;&#22810;&#35270;&#22270;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Joint Projection Learning and Tensor Decomposition Based Incomplete Multi-view Clustering. (arXiv:2310.04038v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#21512;&#25237;&#24433;&#23398;&#20064;&#21644;&#24352;&#37327;&#20998;&#35299;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#19981;&#23436;&#25972;&#22810;&#35270;&#22270;&#32858;&#31867;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#27491;&#20132;&#25237;&#24433;&#30697;&#38453;&#23558;&#39640;&#32500;&#29305;&#24449;&#25237;&#24433;&#21040;&#20302;&#32500;&#31354;&#38388;&#20013;&#65292;&#20197;&#20943;&#36731;&#20887;&#20313;&#29305;&#24449;&#21644;&#22122;&#22768;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#23436;&#25972;&#30340;&#22810;&#35270;&#22270;&#32858;&#31867;(IMVC)&#30001;&#20110;&#26679;&#26412;&#30340;&#26576;&#20123;&#35270;&#22270;&#22312;&#29616;&#23454;&#20013;&#24120;&#24120;&#26159;&#19981;&#23436;&#25972;&#30340;&#65292;&#22240;&#27492;&#21463;&#21040;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26041;&#27861;&#20174;&#21407;&#22987;&#30340;&#19981;&#23436;&#25972;&#30340;&#22810;&#35270;&#22270;&#25968;&#25454;&#20013;&#23398;&#20064;&#30456;&#20284;&#24230;&#23376;&#22270;&#65292;&#24182;&#36890;&#36807;&#25506;&#32034;&#27599;&#20010;&#35270;&#22270;&#30340;&#19981;&#23436;&#25972;&#23376;&#22270;&#26469;&#23547;&#25214;&#23436;&#25972;&#30340;&#22270;&#24418;&#36827;&#34892;&#35889;&#32858;&#31867;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#21407;&#22987;&#39640;&#32500;&#25968;&#25454;&#26500;&#24314;&#30340;&#22270;&#21487;&#33021;&#30001;&#20110;&#29305;&#24449;&#20887;&#20313;&#21644;&#22122;&#22768;&#32780;&#26159;&#27425;&#20248;&#30340;&#12290;&#27492;&#22806;&#65292;&#20043;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#24573;&#30053;&#20102;&#19981;&#23436;&#25972;&#22270;&#21644;&#23436;&#25972;&#22270;&#20043;&#38388;&#30340;&#31867;&#20869;&#21644;&#31867;&#38388;&#32467;&#26500;&#21464;&#21270;&#24341;&#36215;&#30340;&#22270;&#22122;&#22768;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#32852;&#21512;&#25237;&#24433;&#23398;&#20064;&#21644;&#24352;&#37327;&#20998;&#35299;&#30340;&#26041;&#27861;(JPLTD)&#29992;&#20110;IMVC&#12290;&#20855;&#20307;&#22320;&#65292;&#20026;&#20102;&#20943;&#36731;&#39640;&#32500;&#25968;&#25454;&#20013;&#20887;&#20313;&#29305;&#24449;&#21644;&#22122;&#22768;&#30340;&#24433;&#21709;&#65292;JPLTD&#24341;&#20837;&#20102;&#19968;&#20010;&#27491;&#20132;&#25237;&#24433;&#30697;&#38453;&#23558;&#39640;&#32500;&#29305;&#24449;&#25237;&#24433;&#21040;&#19968;&#20010;&#20302;&#32500;&#31354;&#38388;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Incomplete multi-view clustering (IMVC) has received increasing attention since it is often that some views of samples are incomplete in reality. Most existing methods learn similarity subgraphs from original incomplete multi-view data and seek complete graphs by exploring the incomplete subgraphs of each view for spectral clustering. However, the graphs constructed on the original high-dimensional data may be suboptimal due to feature redundancy and noise. Besides, previous methods generally ignored the graph noise caused by the inter-class and intra-class structure variation during the transformation of incomplete graphs and complete graphs. To address these problems, we propose a novel Joint Projection Learning and Tensor Decomposition Based method (JPLTD) for IMVC. Specifically, to alleviate the influence of redundant features and noise in high-dimensional data, JPLTD introduces an orthogonal projection matrix to project the high-dimensional features into a lower-dimensional space 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#20851;&#20110;&#36951;&#20256;&#39044;&#27979;&#22797;&#26434;&#24615;&#29366;&#30340;&#25351;&#21335;&#65292;&#30528;&#37325;&#20171;&#32461;&#20102;&#36523;&#39640;&#20316;&#20026;&#36830;&#32493;&#24615;&#29366;&#30340;&#26696;&#20363;&#65292;&#28085;&#30422;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#27169;&#22411;&#12289;&#30456;&#20851;&#24494;&#22937;&#20043;&#22788;&#20197;&#21450;&#24320;&#21457;&#26032;&#27169;&#22411;&#26102;&#38656;&#35201;&#32771;&#34385;&#30340;&#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2310.04028</link><description>&lt;p&gt;
&#36951;&#20256;&#39044;&#27979;&#23450;&#37327;&#24615;&#29366;&#65306;&#30528;&#30524;&#20110;&#36523;&#39640;&#30340;&#26426;&#22120;&#23398;&#20064;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
Genetic prediction of quantitative traits: a machine learner's guide focused on height. (arXiv:2310.04028v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04028
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#20851;&#20110;&#36951;&#20256;&#39044;&#27979;&#22797;&#26434;&#24615;&#29366;&#30340;&#25351;&#21335;&#65292;&#30528;&#37325;&#20171;&#32461;&#20102;&#36523;&#39640;&#20316;&#20026;&#36830;&#32493;&#24615;&#29366;&#30340;&#26696;&#20363;&#65292;&#28085;&#30422;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#27169;&#22411;&#12289;&#30456;&#20851;&#24494;&#22937;&#20043;&#22788;&#20197;&#21450;&#24320;&#21457;&#26032;&#27169;&#22411;&#26102;&#38656;&#35201;&#32771;&#34385;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#22312;&#29983;&#29289;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#35768;&#22810;&#25104;&#21151;&#65292;&#23588;&#20854;&#26159;&#22312;&#34507;&#30333;&#36136;&#25240;&#21472;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#23545;&#20110;&#20174;&#36951;&#20256;&#23398;&#39044;&#27979;&#22797;&#26434;&#24615;&#29366;&#30340;&#38382;&#39064;&#20851;&#27880;&#30456;&#23545;&#36739;&#23569;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#38656;&#35201;&#28145;&#20837;&#20102;&#35299;&#30456;&#20851;&#30340;&#36951;&#20256;&#23398;&#25991;&#29486;&#65292;&#24182;&#27880;&#24847;&#19982;&#36951;&#20256;&#25968;&#25454;&#30456;&#20851;&#30340;&#21508;&#31181;&#24494;&#22937;&#20043;&#22788;&#12290;&#22312;&#36825;&#20010;&#25351;&#21335;&#20013;&#65292;&#25105;&#20204;&#21521;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#25552;&#20379;&#20102;&#20851;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#27169;&#22411;&#20197;&#21450;&#24320;&#21457;&#26032;&#30340;&#34920;&#22411;&#39044;&#27979;&#27169;&#22411;&#26102;&#38656;&#35201;&#32771;&#34385;&#30340;&#30456;&#20851;&#24494;&#22937;&#20043;&#22788;&#30340;&#27010;&#36848;&#12290;&#25105;&#20204;&#20197;&#36523;&#39640;&#20316;&#20026;&#36830;&#32493;&#24615;&#29366;&#30340;&#31034;&#20363;&#65292;&#24182;&#25552;&#20379;&#20102;&#22522;&#20934;&#25968;&#25454;&#38598;&#12289;&#28151;&#28102;&#22240;&#32032;&#12289;&#29305;&#24449;&#36873;&#25321;&#21644;&#24120;&#35265;&#35780;&#20215;&#25351;&#26631;&#30340;&#20171;&#32461;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning and deep learning have been celebrating many successes in the application to biological problems, especially in the domain of protein folding. Another equally complex and important question has received relatively little attention by the machine learning community, namely the one of prediction of complex traits from genetics. Tackling this problem requires in-depth knowledge of the related genetics literature and awareness of various subtleties associated with genetic data. In this guide, we provide an overview for the machine learning community on current state of the art models and associated subtleties which need to be taken into consideration when developing new models for phenotype prediction. We use height as an example of a continuous-valued phenotype and provide an introduction to benchmark datasets, confounders, feature selection, and common metrics.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#21644;&#25509;&#35302;&#22270;&#25913;&#36827;&#33647;&#29289;&#38774;&#26631;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#29616;&#26377;&#27169;&#22411;&#20013;&#24341;&#20837;&#32852;&#31995;&#22270;&#20449;&#24687;&#65292;&#21487;&#20197;&#25913;&#36827;&#33647;&#29289;&#38774;&#26631;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.04017</link><description>&lt;p&gt;
PGraphDTA: &#20351;&#29992;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#21644;&#25509;&#35302;&#22270;&#25913;&#36827;&#33647;&#29289;&#38774;&#26631;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
PGraphDTA: Improving Drug Target Interaction Prediction using Protein Language Models and Contact Maps. (arXiv:2310.04017v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#21644;&#25509;&#35302;&#22270;&#25913;&#36827;&#33647;&#29289;&#38774;&#26631;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#29616;&#26377;&#27169;&#22411;&#20013;&#24341;&#20837;&#32852;&#31995;&#22270;&#20449;&#24687;&#65292;&#21487;&#20197;&#25913;&#36827;&#33647;&#29289;&#38774;&#26631;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#29616;&#21644;&#24320;&#21457;&#26032;&#33647;&#26159;&#19968;&#39033;&#22797;&#26434;&#19988;&#36164;&#28304;&#23494;&#38598;&#22411;&#30340;&#24037;&#20316;&#65292;&#36890;&#24120;&#28041;&#21450;&#22823;&#37327;&#25104;&#26412;&#12289;&#26102;&#38388;&#25237;&#20837;&#21644;&#23433;&#20840;&#38382;&#39064;&#12290;&#33647;&#29289;&#21457;&#29616;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#26159;&#30830;&#23450;&#26032;&#39062;&#30340;&#33647;&#29289;-&#38774;&#26631;&#65288;DT&#65289;&#30456;&#20114;&#20316;&#29992;&#12290;&#29616;&#26377;&#30340;&#39044;&#27979;DT&#30456;&#20114;&#20316;&#29992;&#30340;&#35745;&#31639;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#20108;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;&#26088;&#22312;&#30830;&#23450;DT&#23545;&#26159;&#21542;&#23384;&#22312;&#30456;&#20114;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#34507;&#30333;&#36136;-&#37197;&#20307;&#30456;&#20114;&#20316;&#29992;&#34920;&#29616;&#20986;&#19968;&#31995;&#21015;&#19981;&#21516;&#32467;&#21512;&#24378;&#24230;&#65292;&#21363;&#32467;&#21512;&#20146;&#21644;&#21147;&#65292;&#36825;&#23545;&#20934;&#30830;&#39044;&#27979;&#36896;&#25104;&#20102;&#25345;&#32493;&#25361;&#25112;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#33647;&#29289;&#38774;&#26631;&#30456;&#20114;&#20316;&#29992;&#65288;DTI&#65289;&#39044;&#27979;&#20013;&#20351;&#29992;&#30340;&#21508;&#31181;&#25216;&#26415;&#65292;&#24182;&#25552;&#20986;&#20102;&#25913;&#36827;&#24615;&#33021;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#25972;&#21512;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#21644;&#23558;&#25509;&#35302;&#22270;&#20449;&#24687;&#20316;&#20026;&#29616;&#26377;&#27169;&#22411;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing and discovering new drugs is a complex and resource-intensive endeavor that often involves substantial costs, time investment, and safety concerns. A key aspect of drug discovery involves identifying novel drug-target (DT) interactions. Existing computational methods for predicting DT interactions have primarily focused on binary classification tasks, aiming to determine whether a DT pair interacts or not. However, protein-ligand interactions exhibit a continuum of binding strengths, known as binding affinity, presenting a persistent challenge for accurate prediction. In this study, we investigate various techniques employed in Drug Target Interaction (DTI) prediction and propose novel enhancements to enhance their performance. Our approaches include the integration of Protein Language Models (PLMs) and the incorporation of Contact Map information as an inductive bias within current models. Through extensive experimentation, we demonstrate that our proposed approaches outper
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#31867;&#20284;&#26679;&#26412;&#32858;&#31867;&#8221;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#26367;&#25442;&#20010;&#20307;&#30340;&#25935;&#24863;&#29305;&#24449;&#20026;&#32858;&#31867;&#30340;&#24179;&#22343;&#20540;&#26469;&#22686;&#24378;&#38544;&#31169;&#12290;&#36890;&#36807;&#23545;&#20351;&#29992;&#21311;&#21517;&#32858;&#31867;&#20013;&#24515;&#35757;&#32451;&#27169;&#22411;&#30340;&#31934;&#30830;&#20998;&#26512;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#19981;&#21516;&#27169;&#22411;&#32452;&#25104;&#37096;&#20998;&#23545;&#27867;&#21270;&#35823;&#24046;&#30340;&#24433;&#21709;&#65292;&#24182;&#35777;&#26126;&#22312;&#26576;&#20123;&#39640;&#32500;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#21311;&#21517;&#32858;&#31867;&#20013;&#24515;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#21462;&#24471;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.04015</link><description>&lt;p&gt;
&#36890;&#36807;&#31867;&#20284;&#26679;&#26412;&#32858;&#31867;&#23398;&#20064;&#65306;&#23545;&#27169;&#22411;&#27867;&#21270;&#30340;&#31934;&#30830;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Learning via Look-Alike Clustering: A Precise Analysis of Model Generalization. (arXiv:2310.04015v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#31867;&#20284;&#26679;&#26412;&#32858;&#31867;&#8221;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#26367;&#25442;&#20010;&#20307;&#30340;&#25935;&#24863;&#29305;&#24449;&#20026;&#32858;&#31867;&#30340;&#24179;&#22343;&#20540;&#26469;&#22686;&#24378;&#38544;&#31169;&#12290;&#36890;&#36807;&#23545;&#20351;&#29992;&#21311;&#21517;&#32858;&#31867;&#20013;&#24515;&#35757;&#32451;&#27169;&#22411;&#30340;&#31934;&#30830;&#20998;&#26512;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#19981;&#21516;&#27169;&#22411;&#32452;&#25104;&#37096;&#20998;&#23545;&#27867;&#21270;&#35823;&#24046;&#30340;&#24433;&#21709;&#65292;&#24182;&#35777;&#26126;&#22312;&#26576;&#20123;&#39640;&#32500;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#21311;&#21517;&#32858;&#31867;&#20013;&#24515;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#21462;&#24471;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#65292;&#20294;&#30830;&#20445;&#29992;&#25143;&#25968;&#25454;&#30340;&#20445;&#25252;&#20173;&#28982;&#26159;&#36825;&#20123;&#23398;&#20064;&#31995;&#32479;&#24320;&#21457;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#20851;&#27880;&#28857;&#12290;&#22686;&#24378;&#38544;&#31169;&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#20351;&#29992;&#21311;&#21517;&#25968;&#25454;&#32780;&#19981;&#26159;&#20010;&#20307;&#25968;&#25454;&#26469;&#35757;&#32451;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#31867;&#20284;&#26679;&#26412;&#32858;&#31867;&#8221;&#30340;&#33258;&#28982;&#25216;&#26415;&#65292;&#23427;&#28041;&#21450;&#23558;&#20010;&#20307;&#30340;&#25935;&#24863;&#29305;&#24449;&#26367;&#25442;&#20026;&#32858;&#31867;&#30340;&#24179;&#22343;&#20540;&#12290;&#25105;&#20204;&#23545;&#20351;&#29992;&#21311;&#21517;&#32858;&#31867;&#20013;&#24515;&#35757;&#32451;&#27169;&#22411;&#22914;&#20309;&#24433;&#21709;&#20854;&#27867;&#21270;&#33021;&#21147;&#36827;&#34892;&#20102;&#31934;&#30830;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#20851;&#27880;&#19968;&#20010;&#28176;&#36817;&#24773;&#20917;&#65292;&#21363;&#35757;&#32451;&#38598;&#30340;&#22823;&#23567;&#19982;&#29305;&#24449;&#32500;&#24230;&#25104;&#27604;&#20363;&#22686;&#38271;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#22522;&#20110;&#20984;&#39640;&#26031;&#26497;&#23567;&#21270;&#26497;&#22823;&#23450;&#29702;&#65288;Convex Gaussian Minimax Theorem&#65292;CGMT&#65289;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#29702;&#35770;&#19978;&#29702;&#35299;&#19981;&#21516;&#27169;&#22411;&#32452;&#25104;&#37096;&#20998;&#23545;&#27867;&#21270;&#35823;&#24046;&#30340;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#26576;&#20123;&#39640;&#32500;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#21311;&#21517;&#32858;&#31867;&#20013;&#24515;&#36827;&#34892;&#35757;&#32451;&#33021;&#22815;&#21462;&#24471;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
While personalized recommendations systems have become increasingly popular, ensuring user data protection remains a paramount concern in the development of these learning systems. A common approach to enhancing privacy involves training models using anonymous data rather than individual data. In this paper, we explore a natural technique called \emph{look-alike clustering}, which involves replacing sensitive features of individuals with the cluster's average values. We provide a precise analysis of how training models using anonymous cluster centers affects their generalization capabilities. We focus on an asymptotic regime where the size of the training set grows in proportion to the features dimension. Our analysis is based on the Convex Gaussian Minimax Theorem (CGMT) and allows us to theoretically understand the role of different model components on the generalization error. In addition, we demonstrate that in certain high-dimensional regimes, training over anonymous cluster cente
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;&#27010;&#29575;&#27979;&#24230;&#31354;&#38388;&#20013;&#21152;&#36895;&#20248;&#21270;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31867;&#20284;&#20110;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#22522;&#20110;&#30697;&#26041;&#27861;&#30340;&#21704;&#23494;&#39039;&#27969;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#21487;&#20197;&#36798;&#21040;&#20219;&#24847;&#39640;&#38454;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.04006</link><description>&lt;p&gt;
&#22312;&#27010;&#29575;&#27979;&#24230;&#31354;&#38388;&#20013;&#21152;&#36895;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Accelerating optimization over the space of probability measures. (arXiv:2310.04006v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04006
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;&#27010;&#29575;&#27979;&#24230;&#31354;&#38388;&#20013;&#21152;&#36895;&#20248;&#21270;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31867;&#20284;&#20110;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#22522;&#20110;&#30697;&#26041;&#27861;&#30340;&#21704;&#23494;&#39039;&#27969;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#21487;&#20197;&#36798;&#21040;&#20219;&#24847;&#39640;&#38454;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26799;&#24230;&#20248;&#21270;&#26041;&#27861;&#30340;&#21152;&#36895;&#26159;&#19968;&#20010;&#38750;&#24120;&#23454;&#29992;&#21644;&#29702;&#35770;&#19978;&#26377;&#24847;&#20041;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#12290;&#22823;&#22810;&#25968;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#30340;&#20248;&#21270;&#19978;&#65292;&#20294;&#32771;&#34385;&#21040;&#22312;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#20013;&#38656;&#35201;&#22312;&#27010;&#29575;&#27979;&#24230;&#31354;&#38388;&#19978;&#36827;&#34892;&#20248;&#21270;&#65292;&#30740;&#31350;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#21152;&#36895;&#26799;&#24230;&#26041;&#27861;&#26159;&#24456;&#26377;&#24847;&#20041;&#30340;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31867;&#20284;&#20110;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#22522;&#20110;&#30697;&#26041;&#27861;&#30340;&#21704;&#23494;&#39039;&#27969;&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22522;&#20110;&#36825;&#31181;&#26041;&#27861;&#30340;&#31639;&#27861;&#21487;&#20197;&#36798;&#21040;&#20219;&#24847;&#39640;&#38454;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#25968;&#20540;&#23454;&#20363;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#35770;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
Acceleration of gradient-based optimization methods is an issue of significant practical and theoretical interest, particularly in machine learning applications. Most research has focused on optimization over Euclidean spaces, but given the need to optimize over spaces of probability measures in many machine learning problems, it is of interest to investigate accelerated gradient methods in this context too. To this end, we introduce a Hamiltonian-flow approach that is analogous to moment-based approaches in Euclidean space. We demonstrate that algorithms based on this approach can achieve convergence rates of arbitrarily high order. Numerical examples illustrate our claim.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#26080;&#32447;&#19990;&#30028;&#20013;&#65292;&#22522;&#30784;&#27169;&#22411;&#65288;FMs&#65289;&#20026;&#29983;&#25104;&#24335;AI&#24212;&#29992;&#25552;&#20379;&#25903;&#25345;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#20998;&#25955;&#30340;&#25968;&#25454;&#21644;&#35745;&#31639;&#36164;&#28304;&#26469;&#25552;&#39640;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;FMs&#23545;&#36164;&#28304;&#38656;&#27714;&#36739;&#39640;&#21487;&#33021;&#32473;FL-enabled&#30340;&#26080;&#32447;&#32593;&#32476;&#24102;&#26469;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.04003</link><description>&lt;p&gt;
&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#26080;&#32447;&#19990;&#30028;&#20013;&#30340;&#22522;&#30784;&#27169;&#22411;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
The Role of Federated Learning in a Wireless World with Foundation Models. (arXiv:2310.04003v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04003
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#26080;&#32447;&#19990;&#30028;&#20013;&#65292;&#22522;&#30784;&#27169;&#22411;&#65288;FMs&#65289;&#20026;&#29983;&#25104;&#24335;AI&#24212;&#29992;&#25552;&#20379;&#25903;&#25345;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#20998;&#25955;&#30340;&#25968;&#25454;&#21644;&#35745;&#31639;&#36164;&#28304;&#26469;&#25552;&#39640;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;FMs&#23545;&#36164;&#28304;&#38656;&#27714;&#36739;&#39640;&#21487;&#33021;&#32473;FL-enabled&#30340;&#26080;&#32447;&#32593;&#32476;&#24102;&#26469;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#65288;FMs&#65289;&#26159;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22411;&#65292;&#26368;&#36817;&#20026;&#22810;&#20010;&#20840;&#26032;&#30340;&#29983;&#25104;&#24335;AI&#24212;&#29992;&#25552;&#20379;&#20102;&#25903;&#25345;&#12290;FMs&#30340;&#24555;&#36895;&#21457;&#23637;&#20026;&#19979;&#19968;&#20195;&#26080;&#32447;&#32593;&#32476;&#30340;&#24895;&#26223;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#32972;&#26223;&#65292;&#20854;&#20013;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#20998;&#24067;&#24335;&#32593;&#32476;&#26234;&#33021;&#30340;&#20851;&#38190;&#39537;&#21160;&#22240;&#32032;&#12290;&#30446;&#21069;&#65292;FMs&#21644;FL&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#20173;&#22788;&#20110;&#21021;&#32423;&#38454;&#27573;&#12290;FMs&#21487;&#20197;&#25552;&#39640;FL&#30340;&#24615;&#33021;&#65292;&#32780;FL&#20063;&#21487;&#20197;&#21033;&#29992;&#20998;&#25955;&#30340;&#25968;&#25454;&#21644;&#35745;&#31639;&#36164;&#28304;&#26469;&#36741;&#21161;&#35757;&#32451;FMs&#12290;&#28982;&#32780;&#65292;FMs&#23545;&#35745;&#31639;&#36164;&#28304;&#12289;&#23384;&#20648;&#21644;&#36890;&#20449;&#24320;&#38144;&#30340;&#35201;&#27714;&#24322;&#24120;&#39640;&#65292;&#36825;&#32473;FL-enabled&#26080;&#32447;&#32593;&#32476;&#24102;&#26469;&#37325;&#35201;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;FMs&#22312;&#26080;&#32447;&#32593;&#32476;&#19978;&#26159;&#21542;&#36866;&#29992;&#20110;FL&#65292;&#21253;&#25324;&#23545;&#30740;&#31350;&#25361;&#25112;&#21644;&#26426;&#36935;&#30340;&#24191;&#27867;&#27010;&#36848;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22810;&#20221;FL&#21644;FL&#36164;&#28304;&#30340;&#38656;&#27714;&#30340;&#32852;&#21512;&#35757;&#32451;&#31561;&#20851;&#38190;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation models (FMs) are general-purpose artificial intelligence (AI) models that have recently enabled multiple brand-new generative AI applications. The rapid advances in FMs serve as an important contextual backdrop for the vision of next-generation wireless networks, where federated learning (FL) is a key enabler of distributed network intelligence. Currently, the exploration of the interplay between FMs and FL is still in its nascent stage. Naturally, FMs are capable of boosting the performance of FL, and FL could also leverage decentralized data and computing resources to assist in the training of FMs. However, the exceptionally high requirements that FMs have for computing resources, storage, and communication overhead would pose critical challenges to FL-enabled wireless networks. In this article, we explore the extent to which FMs are suitable for FL over wireless networks, including a broad overview of research challenges and opportunities. In particular, we discuss multip
&lt;/p&gt;</description></item><item><title>&#36816;&#34892;&#26102;&#30417;&#25511;&#22522;&#20110;DNN&#30340;&#24863;&#30693;&#30340;&#35770;&#25991;&#24635;&#32467;&#30340;&#37325;&#35201;&#21019;&#26032;&#21644;&#36129;&#29486;&#26159;&#20171;&#32461;&#20102;&#25991;&#29486;&#20013;&#20851;&#20110;&#30417;&#35270;&#26041;&#27861;&#30340;&#32463;&#20856;&#26041;&#27861;&#21644;&#24418;&#24335;&#26041;&#27861;&#65292;&#24182;&#24378;&#35843;&#20102;&#35774;&#35745;&#20005;&#35880;&#30340;&#30417;&#25511;&#22120;&#30340;&#24517;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.03999</link><description>&lt;p&gt;
&#36816;&#34892;&#26102;&#30417;&#25511;&#22522;&#20110;DNN&#30340;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Runtime Monitoring DNN-Based Perception. (arXiv:2310.03999v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03999
&lt;/p&gt;
&lt;p&gt;
&#36816;&#34892;&#26102;&#30417;&#25511;&#22522;&#20110;DNN&#30340;&#24863;&#30693;&#30340;&#35770;&#25991;&#24635;&#32467;&#30340;&#37325;&#35201;&#21019;&#26032;&#21644;&#36129;&#29486;&#26159;&#20171;&#32461;&#20102;&#25991;&#29486;&#20013;&#20851;&#20110;&#30417;&#35270;&#26041;&#27861;&#30340;&#32463;&#20856;&#26041;&#27861;&#21644;&#24418;&#24335;&#26041;&#27861;&#65292;&#24182;&#24378;&#35843;&#20102;&#35774;&#35745;&#20005;&#35880;&#30340;&#30417;&#25511;&#22120;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#22312;&#23454;&#29616;&#22797;&#26434;&#30340;&#24863;&#30693;&#31995;&#32479;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#30001;&#20110;&#36825;&#20123;&#24212;&#29992;&#31243;&#24207;&#20013;&#30340;&#35768;&#22810;&#26159;&#23433;&#20840;&#20851;&#38190;&#30340;&#65292;&#38656;&#35201;&#24037;&#31243;&#20005;&#35880;&#24615;&#26469;&#30830;&#20445;&#22522;&#20110;DNN&#30340;&#24863;&#30693;&#30340;&#21151;&#33021;&#19981;&#36275;&#19981;&#20250;&#36896;&#25104;&#20260;&#23475;&#12290;&#38500;&#20102;&#35774;&#35745;&#38454;&#27573;&#20351;&#29992;&#30340;&#20256;&#32479;&#38745;&#24577;&#39564;&#35777;&#21644;&#27979;&#35797;&#25216;&#26415;&#22806;&#65292;&#36824;&#38656;&#35201;&#33021;&#22815;&#26816;&#27979;&#20851;&#38190;&#20107;&#20214;&#12289;&#35786;&#26029;&#38382;&#39064;&#29978;&#33267;&#24378;&#21046;&#35201;&#27714;&#30340;&#36816;&#34892;&#26102;&#39564;&#35777;&#25216;&#26415;&#12290;&#26412;&#25945;&#31243;&#26088;&#22312;&#20026;&#35835;&#32773;&#20171;&#32461;&#25991;&#29486;&#20013;&#25552;&#20986;&#30340;&#25216;&#26415;&#12290;&#25105;&#20204;&#20174;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#25552;&#20986;&#30340;&#32463;&#20856;&#26041;&#27861;&#24320;&#22987;&#65292;&#28982;&#21518;&#37325;&#28857;&#20171;&#32461;&#24418;&#24335;&#26041;&#27861;&#31038;&#21306;&#25552;&#20986;&#30340;&#19968;&#20123;&#25216;&#26415;&#12290;&#34429;&#28982;&#25105;&#20204;&#21487;&#20197;&#35266;&#23519;&#21040;&#30417;&#25511;&#22120;&#35774;&#35745;&#20013;&#30340;&#30456;&#20284;&#20043;&#22788;&#65292;&#20294;&#20915;&#31574;&#36793;&#30028;&#30340;&#21019;&#24314;&#26041;&#24335;&#22312;&#36825;&#20004;&#20010;&#31038;&#21306;&#20043;&#38388;&#26377;&#25152;&#19981;&#21516;&#12290;&#25105;&#20204;&#24471;&#20986;&#30340;&#32467;&#35770;&#26159;&#38656;&#35201;&#20005;&#35880;&#22320;&#35774;&#35745;&#30417;&#25511;&#22120;&#65292;&#22312;&#25968;&#25454;&#21487;&#29992;&#24615;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#32771;&#34385;&#19981;&#21516;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) are instrumental in realizing complex perception systems. As many of these applications are safety-critical by design, engineering rigor is required to ensure that the functional insufficiency of the DNN-based perception is not the source of harm. In addition to conventional static verification and testing techniques employed during the design phase, there is a need for runtime verification techniques that can detect critical events, diagnose issues, and even enforce requirements. This tutorial aims to provide readers with a glimpse of techniques proposed in the literature. We start with classical methods proposed in the machine learning community, then highlight a few techniques proposed by the formal methods community. While we surely can observe similarities in the design of monitors, how the decision boundaries are created vary between the two communities. We conclude by highlighting the need to rigorously design monitors, where data availability outside
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20302;&#31209;&#36866;&#24212;&#21644;&#20013;&#38388;&#29305;&#24449;&#30340;&#35843;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#39044;&#35757;&#32451;&#22810;&#27169;&#24577;&#32593;&#32476;&#30340;&#21442;&#25968;&#39640;&#25928;&#36866;&#24212;&#31243;&#24207;&#65292;&#20197;&#23454;&#29616;&#23545;&#32570;&#22833;&#27169;&#24577;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#32988;&#36807;&#29420;&#31435;&#30340;&#19987;&#38376;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2310.03986</link><description>&lt;p&gt;
&#36890;&#36807;&#21442;&#25968;&#39640;&#25928;&#36866;&#24212;&#65292;&#23454;&#29616;&#23545;&#32570;&#22833;&#27169;&#24577;&#30340;&#40065;&#26834;&#22810;&#27169;&#24577;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robust Multimodal Learning with Missing Modalities via Parameter-Efficient Adaptation. (arXiv:2310.03986v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03986
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20302;&#31209;&#36866;&#24212;&#21644;&#20013;&#38388;&#29305;&#24449;&#30340;&#35843;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#39044;&#35757;&#32451;&#22810;&#27169;&#24577;&#32593;&#32476;&#30340;&#21442;&#25968;&#39640;&#25928;&#36866;&#24212;&#31243;&#24207;&#65292;&#20197;&#23454;&#29616;&#23545;&#32570;&#22833;&#27169;&#24577;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#32988;&#36807;&#29420;&#31435;&#30340;&#19987;&#38376;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#23398;&#20064;&#26088;&#22312;&#21033;&#29992;&#22810;&#20010;&#25968;&#25454;&#28304;&#26469;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#22312;&#19968;&#20123;&#30456;&#20851;&#30340;&#27169;&#24577;&#20013;&#35266;&#23519;&#21040;&#65292;&#22914;&#26524;&#22312;&#27979;&#35797;&#26102;&#38388;&#32570;&#23569;&#19968;&#20010;&#25110;&#22810;&#20010;&#27169;&#24577;&#65292;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#32593;&#32476;&#30340;&#24615;&#33021;&#20250;&#26174;&#33879;&#19979;&#38477;&#12290;&#20026;&#20102;&#23454;&#29616;&#23545;&#32570;&#22833;&#27169;&#24577;&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#32593;&#32476;&#30340;&#31616;&#21333;&#21644;&#21442;&#25968;&#39640;&#25928;&#30340;&#36866;&#24212;&#31243;&#24207;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#21033;&#29992;&#20302;&#31209;&#36866;&#24212;&#21644;&#20013;&#38388;&#29305;&#24449;&#30340;&#35843;&#21046;&#26469;&#34917;&#20607;&#32570;&#22833;&#30340;&#27169;&#24577;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#36825;&#31181;&#36866;&#24212;&#21487;&#20197;&#37096;&#20998;&#24357;&#34917;&#30001;&#20110;&#32570;&#22833;&#27169;&#24577;&#32780;&#23548;&#33268;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#32988;&#36807;&#38024;&#23545;&#21487;&#29992;&#27169;&#24577;&#32452;&#21512;&#36827;&#34892;&#35757;&#32451;&#30340;&#29420;&#31435;&#30340;&#12289;&#19987;&#38376;&#30340;&#32593;&#32476;&#12290;&#25152;&#25552;&#20986;&#30340;&#36866;&#24212;&#25152;&#38656;&#30340;&#21442;&#25968;&#38750;&#24120;&#23569;&#65288;&#20363;&#22914;&#65292;&#23569;&#20110;&#65289;
&lt;/p&gt;
&lt;p&gt;
Multimodal learning seeks to utilize data from multiple sources to improve the overall performance of downstream tasks. It is desirable for redundancies in the data to make multimodal systems robust to missing or corrupted observations in some correlated modalities. However, we observe that the performance of several existing multimodal networks significantly deteriorates if one or multiple modalities are absent at test time. To enable robustness to missing modalities, we propose simple and parameter-efficient adaptation procedures for pretrained multimodal networks. In particular, we exploit low-rank adaptation and modulation of intermediate features to compensate for the missing modalities. We demonstrate that such adaptation can partially bridge performance drop due to missing modalities and outperform independent, dedicated networks trained for the available modality combinations in some cases. The proposed adaptation requires extremely small number of parameters (e.g., fewer than 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#65292;&#29992;&#20110;&#26500;&#24314;&#19968;&#20010;&#26222;&#36890;&#35805;&#35328;&#35821;&#30196;&#21574;&#35780;&#20272;&#31995;&#32479;&#12290;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#24182;&#25552;&#21462;&#32534;&#30721;&#22120;&#65292;&#23454;&#29616;&#20102;&#22312;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#26816;&#27979;&#21644;&#20020;&#24202;&#30196;&#21574;&#35780;&#20998;&#39044;&#27979;&#26041;&#38754;&#30340;&#26174;&#33879;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.03985</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#35821;&#38899;&#35782;&#21035;&#32534;&#30721;&#22120;&#36827;&#34892;&#26222;&#36890;&#35805;&#35328;&#35821;&#30340;&#30196;&#21574;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Dementia Assessment Using Mandarin Speech with an Attention-based Speech Recognition Encoder. (arXiv:2310.03985v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03985
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#65292;&#29992;&#20110;&#26500;&#24314;&#19968;&#20010;&#26222;&#36890;&#35805;&#35328;&#35821;&#30196;&#21574;&#35780;&#20272;&#31995;&#32479;&#12290;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#24182;&#25552;&#21462;&#32534;&#30721;&#22120;&#65292;&#23454;&#29616;&#20102;&#22312;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#26816;&#27979;&#21644;&#20020;&#24202;&#30196;&#21574;&#35780;&#20998;&#39044;&#27979;&#26041;&#38754;&#30340;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30196;&#21574;&#35786;&#26029;&#38656;&#35201;&#19968;&#31995;&#21015;&#19981;&#21516;&#30340;&#27979;&#35797;&#26041;&#27861;&#65292;&#36825;&#26159;&#22797;&#26434;&#19988;&#32791;&#26102;&#30340;&#12290;&#30196;&#21574;&#30340;&#26089;&#26399;&#26816;&#27979;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#38450;&#27490;&#30149;&#24773;&#36827;&#19968;&#27493;&#24694;&#21270;&#12290;&#26412;&#25991;&#21033;&#29992;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#26500;&#24314;&#20102;&#19968;&#20010;&#38024;&#23545;&#26222;&#36890;&#35805;&#20351;&#29992;&#32773;&#22312;&#22270;&#29255;&#25551;&#36848;&#20219;&#21153;&#20013;&#30340;&#30196;&#21574;&#35780;&#20272;&#31995;&#32479;&#12290;&#36890;&#36807;&#22312;&#19982;&#30495;&#23454;&#19990;&#30028;&#24773;&#22659;&#38750;&#24120;&#30456;&#20284;&#30340;&#35821;&#38899;&#25968;&#25454;&#19978;&#35757;&#32451;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#65292;&#25105;&#20204;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#35782;&#21035;&#33021;&#21147;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20174;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#20013;&#25552;&#21462;&#32534;&#30721;&#22120;&#65292;&#24182;&#28155;&#21152;&#20102;&#19968;&#20010;&#32447;&#24615;&#23618;&#29992;&#20110;&#30196;&#21574;&#35780;&#20272;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#26469;&#33258;99&#21517;&#34987;&#35797;&#30340;&#26222;&#36890;&#35805;&#35821;&#38899;&#25968;&#25454;&#65292;&#24182;&#20174;&#24403;&#22320;&#21307;&#38498;&#33719;&#21462;&#20102;&#20182;&#20204;&#30340;&#20020;&#24202;&#35780;&#20272;&#25968;&#25454;&#12290;&#22312;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#26816;&#27979;&#20013;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;92.04%&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#20020;&#24202;&#30196;&#21574;&#35780;&#20998;&#39044;&#27979;&#20013;&#36798;&#21040;&#20102;9%&#30340;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dementia diagnosis requires a series of different testing methods, which is complex and time-consuming. Early detection of dementia is crucial as it can prevent further deterioration of the condition. This paper utilizes a speech recognition model to construct a dementia assessment system tailored for Mandarin speakers during the picture description task. By training an attention-based speech recognition model on voice data closely resembling real-world scenarios, we have significantly enhanced the model's recognition capabilities. Subsequently, we extracted the encoder from the speech recognition model and added a linear layer for dementia assessment. We collected Mandarin speech data from 99 subjects and acquired their clinical assessments from a local hospital. We achieved an accuracy of 92.04% in Alzheimer's disease detection and a mean absolute error of 9% in clinical dementia rating score prediction.
&lt;/p&gt;</description></item><item><title>AdaRec&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#39034;&#24207;&#25512;&#33616;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#36317;&#31163;&#30340;&#34920;&#31034;&#25439;&#22833;&#26469;&#25552;&#21462;&#28508;&#22312;&#20449;&#24687;&#65292;&#20197;&#36866;&#24212;&#22823;&#35268;&#27169;&#22312;&#32447;&#25512;&#33616;&#31995;&#32479;&#20013;&#29992;&#25143;&#34892;&#20026;&#27169;&#24335;&#30340;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.03984</link><description>&lt;p&gt;
AdaRec&#65306;&#29992;&#20110;&#22686;&#24378;&#29992;&#25143;&#38271;&#26399;&#21442;&#19982;&#24230;&#30340;&#33258;&#36866;&#24212;&#39034;&#24207;&#25512;&#33616;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
AdaRec: Adaptive Sequential Recommendation for Reinforcing Long-term User Engagement. (arXiv:2310.03984v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03984
&lt;/p&gt;
&lt;p&gt;
AdaRec&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#39034;&#24207;&#25512;&#33616;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#36317;&#31163;&#30340;&#34920;&#31034;&#25439;&#22833;&#26469;&#25552;&#21462;&#28508;&#22312;&#20449;&#24687;&#65292;&#20197;&#36866;&#24212;&#22823;&#35268;&#27169;&#22312;&#32447;&#25512;&#33616;&#31995;&#32479;&#20013;&#29992;&#25143;&#34892;&#20026;&#27169;&#24335;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39034;&#24207;&#25512;&#33616;&#20219;&#21153;&#20013;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#20851;&#27880;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26469;&#20248;&#21270;&#29992;&#25143;&#30340;&#38271;&#26399;&#21442;&#19982;&#24230;&#12290;&#22823;&#35268;&#27169;&#22312;&#32447;&#25512;&#33616;&#31995;&#32479;&#38754;&#20020;&#30340;&#19968;&#20010;&#25361;&#25112;&#26159;&#29992;&#25143;&#34892;&#20026;&#27169;&#24335;&#65288;&#22914;&#20114;&#21160;&#39057;&#29575;&#21644;&#20445;&#30041;&#20542;&#21521;&#65289;&#30340;&#19981;&#26029;&#22797;&#26434;&#21464;&#21270;&#12290;&#24403;&#23558;&#38382;&#39064;&#24314;&#27169;&#20026;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#26102;&#65292;&#25512;&#33616;&#31995;&#32479;&#30340;&#21160;&#24577;&#21644;&#22870;&#21169;&#20989;&#25968;&#20250;&#19981;&#26029;&#21463;&#21040;&#36825;&#20123;&#21464;&#21270;&#30340;&#24433;&#21709;&#12290;&#29616;&#26377;&#30340;&#25512;&#33616;&#31995;&#32479;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20250;&#21463;&#21040;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#30340;&#22256;&#25200;&#65292;&#24182;&#38590;&#20197;&#36866;&#24212;&#36825;&#31181;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#31216;&#20026;&#33258;&#36866;&#24212;&#39034;&#24207;&#25512;&#33616;&#65288;AdaRec&#65289;&#65292;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;AdaRec&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36317;&#31163;&#30340;&#34920;&#31034;&#25439;&#22833;&#65292;&#20174;&#29992;&#25143;&#30340;&#20114;&#21160;&#36712;&#36857;&#20013;&#25552;&#21462;&#28508;&#22312;&#20449;&#24687;&#12290;&#36825;&#20123;&#20449;&#24687;&#21453;&#26144;&#20102;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#19982;&#24403;&#21069;&#29992;&#25143;&#34892;&#20026;&#27169;&#24335;&#30340;&#21305;&#37197;&#31243;&#24230;&#65292;&#24182;&#24110;&#21161;&#31574;&#30053;&#35782;&#21035;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#32454;&#24494;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Growing attention has been paid to Reinforcement Learning (RL) algorithms when optimizing long-term user engagement in sequential recommendation tasks. One challenge in large-scale online recommendation systems is the constant and complicated changes in users' behavior patterns, such as interaction rates and retention tendencies. When formulated as a Markov Decision Process (MDP), the dynamics and reward functions of the recommendation system are continuously affected by these changes. Existing RL algorithms for recommendation systems will suffer from distribution shift and struggle to adapt in such an MDP. In this paper, we introduce a novel paradigm called Adaptive Sequential Recommendation (AdaRec) to address this issue. AdaRec proposes a new distance-based representation loss to extract latent information from users' interaction trajectories. Such information reflects how RL policy fits to current user behavior patterns, and helps the policy to identify subtle changes in the recomm
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;CUPre&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#36328;&#39046;&#22495;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#23558;&#24120;&#35265;&#29289;&#20307;&#26816;&#27979;&#21644;&#23454;&#20363;&#20998;&#21106;&#30340;&#33021;&#21147;&#24212;&#29992;&#20110;&#32454;&#32990;&#22270;&#20687;&#39046;&#22495;&#65292;&#20026;&#23569;&#26679;&#26412;&#32454;&#32990;&#20998;&#21106;&#25552;&#20379;&#20102;&#19968;&#31181;&#20302;&#25104;&#26412;&#30340;&#27880;&#37322;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.03981</link><description>&lt;p&gt;
CUPre: &#36328;&#39046;&#22495;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#29992;&#20110;&#23569;&#26679;&#26412;&#32454;&#32990;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
CUPre: Cross-domain Unsupervised Pre-training for Few-Shot Cell Segmentation. (arXiv:2310.03981v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03981
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;CUPre&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#36328;&#39046;&#22495;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#23558;&#24120;&#35265;&#29289;&#20307;&#26816;&#27979;&#21644;&#23454;&#20363;&#20998;&#21106;&#30340;&#33021;&#21147;&#24212;&#29992;&#20110;&#32454;&#32990;&#22270;&#20687;&#39046;&#22495;&#65292;&#20026;&#23569;&#26679;&#26412;&#32454;&#32990;&#20998;&#21106;&#25552;&#20379;&#20102;&#19968;&#31181;&#20302;&#25104;&#26412;&#30340;&#27880;&#37322;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30446;&#26631;&#26816;&#27979;&#20219;&#21153;&#30340;&#39044;&#35757;&#32451;&#20013;&#65292;&#20363;&#22914;&#22312;&#24120;&#35265;&#29289;&#20307;&#19978;&#19979;&#25991;&#65288;COCO&#65289;[1]&#19978;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#32454;&#32990;&#20998;&#21106;&#30340;&#24615;&#33021;&#65292;&#20294;&#20173;&#28982;&#38656;&#35201;&#22823;&#37327;&#31934;&#32454;&#27880;&#37322;&#30340;&#32454;&#32990;&#22270;&#20687;[2]&#65292;&#20854;&#20013;&#21253;&#25324;&#27599;&#20010;&#22270;&#20687;&#20013;&#27599;&#20010;&#32454;&#32990;&#30340;&#36793;&#30028;&#26694;&#12289;&#25513;&#33180;&#21644;&#32454;&#32990;&#31867;&#22411;&#65292;&#20197;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#20026;&#20102;&#38477;&#20302;&#27880;&#37322;&#25104;&#26412;&#65292;&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#23569;&#26679;&#26412;&#32454;&#32990;&#20998;&#21106;&#30340;&#39044;&#35757;&#32451;DNN&#27169;&#22411;&#38382;&#39064;&#65292;&#20854;&#20013;&#26377;&#22823;&#37327;&#26410;&#26631;&#27880;&#30340;&#32454;&#32990;&#22270;&#20687;&#21487;&#29992;&#65292;&#20294;&#21482;&#26377;&#19968;&#23567;&#37096;&#20998;&#34987;&#27880;&#37322;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36328;&#22495;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#65288;CUPre&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26410;&#26631;&#27880;&#22270;&#20687;&#23558;&#23545;&#35937;&#26816;&#27979;&#21644;&#23454;&#20363;&#20998;&#21106;&#30340;&#33021;&#21147;&#65288;&#20174;COCO&#23398;&#20064;&#65289;&#36716;&#31227;&#21040;&#32454;&#32990;&#30340;&#35270;&#35273;&#39046;&#22495;&#12290;&#32473;&#23450;&#19968;&#20010;&#24102;&#26377;&#20027;&#24178;&#12289;&#33046;&#23376;&#21644;&#22836;&#37096;&#27169;&#22359;&#30340;&#26631;&#20934;COCO&#39044;&#35757;&#32451;&#32593;&#32476;&#65292;CUPre&#37319;&#29992;&#20132;&#26367;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#65288;AMT2&#65289;&#27969;&#31243;&#24182;&#36827;&#34892;&#20004;&#20010;&#23376;&#20219;&#21153;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
While pre-training on object detection tasks, such as Common Objects in Contexts (COCO) [1], could significantly boost the performance of cell segmentation, it still consumes on massive fine-annotated cell images [2] with bounding boxes, masks, and cell types for every cell in every image, to fine-tune the pre-trained model. To lower the cost of annotation, this work considers the problem of pre-training DNN models for few-shot cell segmentation, where massive unlabeled cell images are available but only a small proportion is annotated. Hereby, we propose Cross-domain Unsupervised Pre-training, namely CUPre, transferring the capability of object detection and instance segmentation for common visual objects (learned from COCO) to the visual domain of cells using unlabeled images. Given a standard COCO pre-trained network with backbone, neck, and head modules, CUPre adopts an alternate multi-task pre-training (AMT2) procedure with two sub-tasks -- in every iteration of pre-training, AMT2
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#20013;&#22686;&#24378;&#26041;&#27861;&#21644;&#19979;&#28216;&#24615;&#33021;&#30340;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#20027;&#35201;&#36890;&#36807;&#20998;&#31163;&#19981;&#21516;&#31867;&#21035;&#30340;&#33410;&#28857;&#26469;&#20026;&#19979;&#28216;&#20219;&#21153;&#20570;&#20986;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2310.03977</link><description>&lt;p&gt;
&#23436;&#32654;&#23545;&#40784;&#21487;&#33021;&#23545;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Perfect Alignment May be Poisonous to Graph Contrastive Learning. (arXiv:2310.03977v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03977
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#20013;&#22686;&#24378;&#26041;&#27861;&#21644;&#19979;&#28216;&#24615;&#33021;&#30340;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#20027;&#35201;&#36890;&#36807;&#20998;&#31163;&#19981;&#21516;&#31867;&#21035;&#30340;&#33410;&#28857;&#26469;&#20026;&#19979;&#28216;&#20219;&#21153;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#26088;&#22312;&#36890;&#36807;&#23545;&#40784;&#27491;&#26679;&#26412;&#21644;&#20998;&#31163;&#36127;&#26679;&#26412;&#26469;&#23398;&#20064;&#33410;&#28857;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#22312;&#22522;&#20110;&#22270;&#24418;&#30340;&#23398;&#20064;&#20013;&#65292;&#23545;&#20110;&#29305;&#23450;&#22686;&#24378;&#26041;&#27861;&#32972;&#21518;&#30340;&#20869;&#22312;&#35268;&#24459;&#30340;&#30740;&#31350;&#26377;&#38480;&#12290;&#20160;&#20040;&#26679;&#30340;&#22686;&#24378;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#19979;&#28216;&#24615;&#33021;&#65311;&#23545;&#27604;&#23398;&#20064;&#22914;&#20309;&#23454;&#38469;&#24433;&#21709;&#19979;&#28216;&#20219;&#21153;&#65311;&#20026;&#20160;&#20040;&#22686;&#24378;&#30340;&#24133;&#24230;&#24456;&#37325;&#35201;&#65311;&#26412;&#25991;&#35797;&#22270;&#36890;&#36807;&#24314;&#31435;&#22686;&#24378;&#26041;&#27861;&#21644;&#19979;&#28216;&#24615;&#33021;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#20197;&#21450;&#23545;&#23545;&#27604;&#23398;&#20064;&#30340;&#27867;&#21270;&#24615;&#36827;&#34892;&#30740;&#31350;&#26469;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#20027;&#35201;&#36890;&#36807;&#20998;&#31163;&#19981;&#21516;&#31867;&#21035;&#32780;&#19981;&#26159;&#32858;&#38598;&#21516;&#19968;&#31867;&#21035;&#30340;&#33410;&#28857;&#26469;&#20026;&#19979;&#28216;&#20219;&#21153;&#20570;&#20986;&#36129;&#29486;&#12290;&#22240;&#27492;&#65292;&#26080;&#27861;&#35299;&#37322;&#23545;&#27604;&#23398;&#20064;&#30340;&#25104;&#21151;&#65292;&#21363;&#20840;&#37096;&#26679;&#26412;&#23436;&#32654;&#23545;&#40784;&#21644;&#22686;&#24378;&#37325;&#21472;&#12290;&#20026;&#20102;&#29702;&#35299;&#22686;&#24378;&#22914;&#20309;&#36741;&#21161;&#23545;&#27604;&#23398;&#20064;&#36807;&#31243;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Contrastive Learning (GCL) aims to learn node representations by aligning positive pairs and separating negative ones. However, limited research has been conducted on the inner law behind specific augmentations used in graph-based learning. What kind of augmentation will help downstream performance, how does contrastive learning actually influence downstream tasks, and why the magnitude of augmentation matters? This paper seeks to address these questions by establishing a connection between augmentation and downstream performance, as well as by investigating the generalization of contrastive learning. Our findings reveal that GCL contributes to downstream tasks mainly by separating different classes rather than gathering nodes of the same class. So perfect alignment and augmentation overlap which draw all intra-class samples the same can not explain the success of contrastive learning. Then in order to comprehend how augmentation aids the contrastive learning process, we conduct 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20174;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#23398;&#20064;&#20219;&#20309;&#38543;&#26426;&#36807;&#31243;&#30340;&#26410;&#30693;&#21442;&#25968;&#30340;&#26497;&#38480;&#65292;&#21457;&#29616;&#20102;&#26368;&#20339;&#25512;&#26029;&#30340;&#23553;&#38381;&#24418;&#24335;&#34920;&#36798;&#24335;&#12290;&#23545;&#20110;&#32473;&#23450;&#30340;&#21442;&#25968;&#21270;&#20505;&#36873;&#27169;&#22411;&#31867;&#65292;&#35266;&#27979;&#24207;&#21015;&#27010;&#29575;&#30340;&#36153;&#33293;&#23572;&#20449;&#24687;&#19979;&#30028;&#20102;&#26469;&#33258;&#26377;&#38480;&#25968;&#25454;&#30340;&#27169;&#22411;&#20272;&#35745;&#26041;&#24046;&#65292;&#26368;&#23567;&#26041;&#24046;&#38543;&#24207;&#21015;&#38271;&#24230;&#30340;&#22686;&#21152;&#32780;&#20943;&#23567;&#65292;&#32553;&#25918;&#29575;&#30001;&#20449;&#24687;&#29575;&#32473;&#20986;&#12290;&#36890;&#36807;&#20998;&#26512;&#35266;&#23519;&#24341;&#36215;&#30340;&#20449;&#24565;&#29366;&#24577;&#38388;&#30340;&#20803;&#21160;&#21147;&#23398;&#65292;&#24471;&#21040;&#20102;&#27169;&#22411;&#26041;&#24046;&#30340;&#31934;&#30830;&#19979;&#30028;&#65292;&#24182;&#21457;&#29616;&#20102;&#19981;&#21516;&#30340;&#25910;&#25947;&#27169;&#24335;&#65292;&#26368;&#32456;&#30701;&#35270;&#30340;&#20449;&#24687;&#29575;&#25910;&#25947;&#21040;&#28176;&#36817;&#30340;&#36153;&#33293;&#23572;&#20449;&#24687;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.03968</link><description>&lt;p&gt;
&#23398;&#20064;&#38750;Markov&#34892;&#20026;&#30340;&#26368;&#32456;&#26497;&#38480;&#65306;&#36153;&#33293;&#23572;&#20449;&#24687;&#29575;&#21644;&#36229;&#37327;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Ultimate limit on learning non-Markovian behavior: Fisher information rate and excess information. (arXiv:2310.03968v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03968
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20174;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#23398;&#20064;&#20219;&#20309;&#38543;&#26426;&#36807;&#31243;&#30340;&#26410;&#30693;&#21442;&#25968;&#30340;&#26497;&#38480;&#65292;&#21457;&#29616;&#20102;&#26368;&#20339;&#25512;&#26029;&#30340;&#23553;&#38381;&#24418;&#24335;&#34920;&#36798;&#24335;&#12290;&#23545;&#20110;&#32473;&#23450;&#30340;&#21442;&#25968;&#21270;&#20505;&#36873;&#27169;&#22411;&#31867;&#65292;&#35266;&#27979;&#24207;&#21015;&#27010;&#29575;&#30340;&#36153;&#33293;&#23572;&#20449;&#24687;&#19979;&#30028;&#20102;&#26469;&#33258;&#26377;&#38480;&#25968;&#25454;&#30340;&#27169;&#22411;&#20272;&#35745;&#26041;&#24046;&#65292;&#26368;&#23567;&#26041;&#24046;&#38543;&#24207;&#21015;&#38271;&#24230;&#30340;&#22686;&#21152;&#32780;&#20943;&#23567;&#65292;&#32553;&#25918;&#29575;&#30001;&#20449;&#24687;&#29575;&#32473;&#20986;&#12290;&#36890;&#36807;&#20998;&#26512;&#35266;&#23519;&#24341;&#36215;&#30340;&#20449;&#24565;&#29366;&#24577;&#38388;&#30340;&#20803;&#21160;&#21147;&#23398;&#65292;&#24471;&#21040;&#20102;&#27169;&#22411;&#26041;&#24046;&#30340;&#31934;&#30830;&#19979;&#30028;&#65292;&#24182;&#21457;&#29616;&#20102;&#19981;&#21516;&#30340;&#25910;&#25947;&#27169;&#24335;&#65292;&#26368;&#32456;&#30701;&#35270;&#30340;&#20449;&#24687;&#29575;&#25910;&#25947;&#21040;&#28176;&#36817;&#30340;&#36153;&#33293;&#23572;&#20449;&#24687;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20174;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#23398;&#20064;&#20219;&#20309;&#38543;&#26426;&#36807;&#31243;&#30340;&#26410;&#30693;&#21442;&#25968;&#30340;&#22522;&#26412;&#38480;&#21046;&#65292;&#24182;&#21457;&#29616;&#20102;&#22914;&#20309;&#38543;&#30528;&#35266;&#27979;&#38271;&#24230;&#30340;&#22686;&#21152;&#65292;&#26368;&#20339;&#25512;&#26029;&#30340;&#23553;&#38381;&#24418;&#24335;&#34920;&#36798;&#24335;&#12290;&#32473;&#23450;&#19968;&#20010;&#21442;&#25968;&#21270;&#30340;&#20505;&#36873;&#27169;&#22411;&#31867;&#65292;&#35266;&#27979;&#24207;&#21015;&#27010;&#29575;&#30340;&#36153;&#33293;&#23572;&#20449;&#24687;&#19979;&#30028;&#20102;&#26469;&#33258;&#26377;&#38480;&#25968;&#25454;&#30340;&#27169;&#22411;&#20272;&#35745;&#26041;&#24046;&#12290;&#38543;&#30528;&#24207;&#21015;&#38271;&#24230;&#30340;&#22686;&#21152;&#65292;&#26368;&#23567;&#26041;&#24046;&#30340;&#32553;&#25918;&#29575;&#19982;&#38271;&#24230;&#30340;&#24179;&#26041;&#20498;&#25968;&#25104;&#21453;&#27604; -- &#20854;&#20013;&#24120;&#25968;&#31995;&#25968;&#30001;&#20449;&#24687;&#29575;&#32473;&#20986;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#36825;&#20010;&#20449;&#24687;&#29575;&#30340;&#31616;&#21333;&#38381;&#24335;&#34920;&#36798;&#24335;&#65292;&#21363;&#20351;&#22312;&#26080;&#38480;Markov&#38454;&#30340;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#35266;&#23519;&#24341;&#36215;&#30340;&#20449;&#24565;&#29366;&#24577;&#38388;&#20803;&#21160;&#21147;&#23398;&#33719;&#24471;&#20102;&#27169;&#22411;&#26041;&#24046;&#30340;&#31934;&#30830;&#20998;&#26512;&#19979;&#30028;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#20102;&#30636;&#26102;&#12289;&#25351;&#25968;&#21644;&#26356;&#19968;&#33324;&#30340;&#25910;&#25947;&#27169;&#24335;&#65292;&#20197;&#36798;&#21040;&#28176;&#36817;&#20449;&#24687;&#29575;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#36825;&#20010;&#30701;&#35270;&#30340;&#20449;&#24687;&#29575;&#25910;&#25947;&#21040;&#28176;&#36817;&#30340;&#36153;&#33293;&#23572;&#20449;&#24687;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address the fundamental limits of learning unknown parameters of any stochastic process from time-series data, and discover exact closed-form expressions for how optimal inference scales with observation length. Given a parametrized class of candidate models, the Fisher information of observed sequence probabilities lower-bounds the variance in model estimation from finite data. As sequence-length increases, the minimal variance scales as the square inverse of the length -- with constant coefficient given by the information rate. We discover a simple closed-form expression for this information rate, even in the case of infinite Markov order. We furthermore obtain the exact analytic lower bound on model variance from the observation-induced metadynamic among belief states. We discover ephemeral, exponential, and more general modes of convergence to the asymptotic information rate. Surprisingly, this myopic information rate converges to the asymptotic Fisher information rate with exac
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#21151;&#33021;&#36830;&#25509;&#24615;&#31070;&#32463;&#38556;&#30861;&#35786;&#26029;&#30340;&#21453;&#26465;&#20214;&#20998;&#26512;&#26694;&#26550;&#65292;&#36890;&#36807;&#38598;&#25104;&#35786;&#26029;&#21644;&#35299;&#37322;&#30340;&#27493;&#39588;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26694;&#26550;&#20013;&#21508;&#38454;&#27573;&#32467;&#26524;&#21487;&#38752;&#24615;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.03964</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#21151;&#33021;&#36830;&#25509;&#24615;&#31070;&#32463;&#38556;&#30861;&#35786;&#26029;&#30340;&#21487;&#23398;&#20064;&#30340;&#21453;&#26465;&#20214;&#20998;&#26512;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Learnable Counter-condition Analysis Framework for Functional Connectivity-based Neurological Disorder Diagnosis. (arXiv:2310.03964v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03964
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#21151;&#33021;&#36830;&#25509;&#24615;&#31070;&#32463;&#38556;&#30861;&#35786;&#26029;&#30340;&#21453;&#26465;&#20214;&#20998;&#26512;&#26694;&#26550;&#65292;&#36890;&#36807;&#38598;&#25104;&#35786;&#26029;&#21644;&#35299;&#37322;&#30340;&#27493;&#39588;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26694;&#26550;&#20013;&#21508;&#38454;&#27573;&#32467;&#26524;&#21487;&#38752;&#24615;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#29702;&#35299;&#21151;&#33021;&#36830;&#25509;&#24615;(FC)&#19982;&#31070;&#32463;&#38556;&#30861;&#30340;&#29983;&#29289;&#29305;&#24449;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#24191;&#27867;&#21033;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#26469;&#35782;&#21035;&#30142;&#30149;&#65292;&#24182;&#36890;&#36807;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#36827;&#34892;&#20107;&#21518;&#20998;&#26512;&#20197;&#21457;&#29616;&#19982;&#30142;&#30149;&#30456;&#20851;&#30340;&#29983;&#29289;&#26631;&#24535;&#29289;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26694;&#26550;&#30001;&#19977;&#20010;&#38454;&#27573;&#32452;&#25104;&#65292;&#21363;&#29305;&#24449;&#36873;&#25321;&#12289;&#20998;&#31867;&#30340;&#29305;&#24449;&#25552;&#21462;&#21644;&#20998;&#26512;&#65292;&#27599;&#20010;&#38454;&#27573;&#37117;&#26159;&#20998;&#21035;&#23454;&#26045;&#30340;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#27599;&#20010;&#38454;&#27573;&#30340;&#32467;&#26524;&#32570;&#20047;&#21487;&#38752;&#24615;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#35823;&#35786;&#21644;&#21518;&#32493;&#38454;&#27573;&#30340;&#38169;&#35823;&#20998;&#26512;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#31995;&#32479;&#22320;&#38598;&#25104;&#20102;&#35786;&#26029;(&#21363;&#29305;&#24449;&#36873;&#25321;&#21644;&#29305;&#24449;&#25552;&#21462;)&#21644;&#35299;&#37322;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#32593;&#32476;&#20316;&#20026;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#20197;&#35782;&#21035;&#20010;&#20307;&#29305;&#23450;&#30340;&#19982;&#30142;&#30149;&#30456;&#20851;&#30340;&#36830;&#25509;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#21151;&#33021;&#32593;&#32476;&#20851;&#31995;&#32534;&#30721;&#22120;&#65292;&#24635;&#32467;&#20102;FC&#30340;&#20840;&#23616;&#25299;&#25169;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
To understand the biological characteristics of neurological disorders with functional connectivity (FC), recent studies have widely utilized deep learning-based models to identify the disease and conducted post-hoc analyses via explainable models to discover disease-related biomarkers. Most existing frameworks consist of three stages, namely, feature selection, feature extraction for classification, and analysis, where each stage is implemented separately. However, if the results at each stage lack reliability, it can cause misdiagnosis and incorrect analysis in afterward stages. In this study, we propose a novel unified framework that systemically integrates diagnoses (i.e., feature selection and feature extraction) and explanations. Notably, we devised an adaptive attention network as a feature selection approach to identify individual-specific disease-related connections. We also propose a functional network relational encoder that summarizes the global topological properties of FC
&lt;/p&gt;</description></item><item><title>&#25552;&#31034;&#24037;&#31243;&#22312;&#38646;&#26679;&#26412;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#20854;&#25104;&#21151;&#24402;&#22240;&#20110;&#32463;&#20856;&#30340;PAC-Bayes&#36793;&#30028;&#23558;&#31163;&#25955;&#30340;&#25552;&#31034;&#29305;&#24615;&#19982;&#30001;&#35821;&#35328;&#27169;&#22411;&#32473;&#20986;&#30340;&#20808;&#39564;&#30456;&#32467;&#21512;&#65292;&#24418;&#25104;&#20102;&#30456;&#23545;&#32039;&#23494;&#30340;&#27867;&#21270;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2310.03957</link><description>&lt;p&gt;
&#29702;&#35299;&#25552;&#31034;&#24037;&#31243;&#21487;&#33021;&#19981;&#38656;&#35201;&#37325;&#26032;&#24605;&#32771;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Understanding prompt engineering may not require rethinking generalization. (arXiv:2310.03957v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03957
&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#24037;&#31243;&#22312;&#38646;&#26679;&#26412;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#20854;&#25104;&#21151;&#24402;&#22240;&#20110;&#32463;&#20856;&#30340;PAC-Bayes&#36793;&#30028;&#23558;&#31163;&#25955;&#30340;&#25552;&#31034;&#29305;&#24615;&#19982;&#30001;&#35821;&#35328;&#27169;&#22411;&#32473;&#20986;&#30340;&#20808;&#39564;&#30456;&#32467;&#21512;&#65292;&#24418;&#25104;&#20102;&#30456;&#23545;&#32039;&#23494;&#30340;&#27867;&#21270;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24341;&#23548;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#38646;&#26679;&#26412;&#23398;&#20064;&#65292;&#21363;&#36890;&#36807;&#26500;&#24314;&#25552;&#31034;&#26469;&#26500;&#24314;&#20998;&#31867;&#22120;&#32780;&#26080;&#38656;&#26126;&#30830;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#22312;&#35768;&#22810;&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#36825;&#19968;&#25104;&#21151;&#24102;&#26469;&#20102;&#19968;&#20010;&#30475;&#20284;&#20196;&#20154;&#24778;&#35766;&#30340;&#35266;&#23519;&#65306;&#36825;&#20123;&#26041;&#27861;&#22312;&#36807;&#25311;&#21512;&#26041;&#38754;&#30456;&#23545;&#36739;&#23569;&#21463;&#21040;&#24433;&#21709;&#65292;&#21363;&#24403;&#25163;&#21160;&#35774;&#35745;&#25552;&#31034;&#20197;&#22312;&#32473;&#23450;&#30340;&#35757;&#32451;&#38598;&#19978;&#23454;&#29616;&#20302;&#38169;&#35823;&#29575;&#26102;&#65288;&#22240;&#27492;&#23454;&#38469;&#19978;&#19981;&#20877;&#26159;&#38646;&#26679;&#26412;&#23398;&#20064;&#65289;&#65292;&#35813;&#26041;&#27861;&#22312;&#20445;&#30041;&#30340;&#27979;&#35797;&#25968;&#25454;&#19978;&#20173;&#28982;&#34920;&#29616;&#33391;&#22909;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#36890;&#36807;&#32463;&#20856;&#30340;PAC-Bayes&#36793;&#30028;&#21487;&#20197;&#24456;&#22909;&#22320;&#35299;&#37322;&#36825;&#31181;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25552;&#31034;&#30340;&#31163;&#25955;&#24615;&#36136;&#65292;&#32467;&#21512;&#30001;&#35821;&#35328;&#27169;&#22411;&#32473;&#20986;&#30340;PAC-Bayes&#20808;&#39564;&#65292;&#23548;&#33268;&#20102;&#25991;&#29486;&#26631;&#20934;&#19979;&#30456;&#23545;&#32039;&#23494;&#30340;&#27867;&#21270;&#30028;&#38480;&#65306;&#20363;&#22914;&#65292;ImageNet&#20998;&#31867;&#22120;&#30340;&#27867;&#21270;&#30028;&#38480;&#36890;&#24120;&#19982;&#30495;&#23454;&#27979;&#35797;&#38169;&#35823;&#29575;&#30456;&#24046;&#20960;&#20010;&#30334;&#20998;&#28857;&#12290;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#35777;&#26126;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Zero-shot learning in prompted vision-language models, the practice of crafting prompts to build classifiers without an explicit training process, has achieved impressive performance in many settings. This success presents a seemingly surprising observation: these methods suffer relatively little from overfitting, i.e., when a prompt is manually engineered to achieve low error on a given training set (thus rendering the method no longer actually zero-shot), the approach still performs well on held-out test data. In this paper, we show that we can explain such performance well via recourse to classical PAC-Bayes bounds. Specifically, we show that the discrete nature of prompts, combined with a PAC-Bayes prior given by a language model, results in generalization bounds that are remarkably tight by the standards of the literature: for instance, the generalization bound of an ImageNet classifier is often within a few percentage points of the true test error. We demonstrate empirically that
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25972;&#21512;&#22522;&#20110;&#32467;&#26500;&#30340;&#23545;&#25509;&#21644;&#22522;&#20110;&#24207;&#21015;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#20803;&#27169;&#22411;&#26694;&#26550;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#37197;&#20307;-&#34507;&#30333;&#36136;&#32467;&#21512;&#20146;&#21644;&#21147;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.03946</link><description>&lt;p&gt;
&#36890;&#36807;&#20803;&#27169;&#22411;&#25913;&#36827;&#20102;&#37197;&#20307;-&#34507;&#30333;&#36136;&#32467;&#21512;&#20146;&#21644;&#21147;&#30340;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Improved prediction of ligand-protein binding affinities by meta-modeling. (arXiv:2310.03946v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03946
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25972;&#21512;&#22522;&#20110;&#32467;&#26500;&#30340;&#23545;&#25509;&#21644;&#22522;&#20110;&#24207;&#21015;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#20803;&#27169;&#22411;&#26694;&#26550;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#37197;&#20307;-&#34507;&#30333;&#36136;&#32467;&#21512;&#20146;&#21644;&#21147;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35745;&#31639;&#26041;&#27861;&#20934;&#30830;&#31579;&#36873;&#20505;&#36873;&#33647;&#29289;&#37197;&#20307;&#19982;&#38774;&#34507;&#30333;&#30340;&#32467;&#21512;&#26159;&#33647;&#29289;&#24320;&#21457;&#30340;&#20027;&#35201;&#20851;&#27880;&#28857;&#65292;&#22240;&#20026;&#31579;&#36873;&#28508;&#22312;&#20505;&#36873;&#29289;&#33021;&#22815;&#33410;&#30465;&#25214;&#33647;&#29289;&#30340;&#26102;&#38388;&#21644;&#36153;&#29992;&#12290;&#36825;&#31181;&#34394;&#25311;&#31579;&#36873;&#37096;&#20998;&#20381;&#36182;&#20110;&#39044;&#27979;&#37197;&#20307;&#21644;&#34507;&#30333;&#36136;&#20043;&#38388;&#30340;&#32467;&#21512;&#20146;&#21644;&#21147;&#30340;&#26041;&#27861;&#12290;&#37492;&#20110;&#23384;&#22312;&#35768;&#22810;&#35745;&#31639;&#27169;&#22411;&#23545;&#19981;&#21516;&#30446;&#26631;&#30340;&#32467;&#21512;&#20146;&#21644;&#21147;&#39044;&#27979;&#32467;&#26524;&#19981;&#21516;&#65292;&#25105;&#20204;&#22312;&#36825;&#37324;&#24320;&#21457;&#20102;&#19968;&#20010;&#20803;&#27169;&#22411;&#26694;&#26550;&#65292;&#36890;&#36807;&#25972;&#21512;&#24050;&#21457;&#34920;&#30340;&#22522;&#20110;&#32467;&#26500;&#30340;&#23545;&#25509;&#21644;&#22522;&#20110;&#24207;&#21015;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#26500;&#24314;&#12290;&#22312;&#26500;&#24314;&#36825;&#20010;&#26694;&#26550;&#26102;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#35768;&#22810;&#32452;&#21512;&#30340;&#20010;&#21035;&#27169;&#22411;&#12289;&#35757;&#32451;&#25968;&#25454;&#24211;&#20197;&#21450;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#30340;&#20803;&#27169;&#22411;&#26041;&#27861;&#12290;&#25105;&#20204;&#26174;&#31034;&#20986;&#35768;&#22810;&#20803;&#27169;&#22411;&#22312;&#20146;&#21644;&#21147;&#39044;&#27979;&#19978;&#26174;&#33879;&#25913;&#21892;&#20102;&#20010;&#21035;&#22522;&#30784;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#26368;&#22909;&#30340;&#20803;&#27169;&#22411;&#36798;&#21040;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#32431;&#32467;&#26500;&#20026;&#22522;&#30784;&#30340;&#28145;&#24230;&#23398;&#20064;&#24037;&#20855;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20010;&#20803;&#27169;&#22411;&#26694;&#26550;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#37197;&#20307;-&#34507;&#30333;&#36136;&#32467;&#21512;&#20146;&#21644;&#21147;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The accurate screening of candidate drug ligands against target proteins through computational approaches is of prime interest to drug development efforts, as filtering potential candidates would save time and expenses for finding drugs. Such virtual screening depends in part on methods to predict the binding affinity between ligands and proteins. Given many computational models for binding affinity prediction with varying results across targets, we herein develop a meta-modeling framework by integrating published empirical structure-based docking and sequence-based deep learning models. In building this framework, we evaluate many combinations of individual models, training databases, and linear and nonlinear meta-modeling approaches. We show that many of our meta-models significantly improve affinity predictions over individual base models. Our best meta-models achieve comparable performance to state-of-the-art exclusively structure-based deep learning tools. Overall, we demonstrate 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38416;&#36848;&#20102;&#20851;&#20110;&#38543;&#26426;&#21521;&#37327;&#20043;&#38388;&#30340;Wasserstein&#36317;&#31163;&#30340;&#19979;&#30028;&#65292;&#37325;&#28857;&#26159;&#20223;&#23556;&#21464;&#25442;&#65292;&#24182;&#24212;&#29992;&#20110;&#27969;&#22411;&#23398;&#20064;&#21644;&#25163;&#20889;&#25968;&#25454;&#38598;&#27169;&#25311;&#12290;</title><link>http://arxiv.org/abs/2310.03945</link><description>&lt;p&gt;
&#20851;&#20110;&#38543;&#26426;&#21521;&#37327;&#30340;&#20223;&#23556;&#21464;&#25442;&#30340;Wasserstein&#36317;&#31163;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Wasserstein distances for affine transformations of random vectors. (arXiv:2310.03945v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03945
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38416;&#36848;&#20102;&#20851;&#20110;&#38543;&#26426;&#21521;&#37327;&#20043;&#38388;&#30340;Wasserstein&#36317;&#31163;&#30340;&#19979;&#30028;&#65292;&#37325;&#28857;&#26159;&#20223;&#23556;&#21464;&#25442;&#65292;&#24182;&#24212;&#29992;&#20110;&#27969;&#22411;&#23398;&#20064;&#21644;&#25163;&#20889;&#25968;&#25454;&#38598;&#27169;&#25311;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#38416;&#36848;&#20102;&#20851;&#20110;&#22312;Wasserstein&#31354;&#38388;&#20013;&#29992;&#20110;&#25968;&#25454;&#27969;&#22411;&#23398;&#20064;&#30340;&#38543;&#26426;&#21521;&#37327;&#20043;&#38388;&#30340;&#20108;&#27425;Wasserstein&#36317;&#31163;&#30340;&#19968;&#20123;&#24050;&#30693;&#19979;&#30028;&#65292;&#37325;&#28857;&#26159;&#20223;&#23556;&#21464;&#25442;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#36890;&#36807;&#35745;&#31639;&#21327;&#26041;&#24046;&#30697;&#38453;&#20043;&#38388;&#30340;Bures&#36317;&#31163;&#65292;&#32473;&#20986;&#20102;&#26059;&#36716;&#30340;&#38543;&#26426;&#21521;&#37327;&#22312;&#20855;&#26377;&#19981;&#30456;&#20851;&#20998;&#37327;&#30340;$\mathbb{R}^2$&#31354;&#38388;&#20013;&#30340;&#20855;&#20307;&#19979;&#30028;&#12290;&#25105;&#20204;&#36824;&#24471;&#21040;&#20102;&#20223;&#23556;&#21464;&#25442;&#30340;&#32452;&#21512;&#30340;&#19978;&#30028;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#24212;&#29992;&#20110;&#21021;&#22987;&#25968;&#25454;&#27979;&#24230;&#30340;&#20016;&#23500;&#30340;&#24494;&#20998;&#21516;&#32986;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#30028;&#24212;&#29992;&#20110;&#21253;&#25324;&#22312;$\mathbb{R}^2$&#20013;&#30340;&#19968;&#32500;&#27969;&#22411;&#19978;&#30340;&#21508;&#31181;&#20998;&#24067;&#65292;&#24182;&#35828;&#26126;&#20102;&#36825;&#20123;&#30028;&#30340;&#36136;&#37327;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22312;&#27969;&#22411;&#23398;&#20064;&#26694;&#26550;&#20013;&#21487;&#20197;&#24212;&#29992;&#20110;&#27169;&#25311;&#25163;&#20889;&#25968;&#23383;&#25110;&#23383;&#27597;&#25968;&#25454;&#38598;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
We expound on some known lower bounds of the quadratic Wasserstein distance between random vectors in $\mathbb{R}^n$ with an emphasis on affine transformations that have been used in manifold learning of data in Wasserstein space. In particular, we give concrete lower bounds for rotated copies of random vectors in $\mathbb{R}^2$ with uncorrelated components by computing the Bures metric between the covariance matrices. We also derive upper bounds for compositions of affine maps which yield a fruitful variety of diffeomorphisms applied to an initial data measure. We apply these bounds to various distributions including those lying on a 1-dimensional manifold in $\mathbb{R}^2$ and illustrate the quality of the bounds. Finally, we give a framework for mimicking handwritten digit or alphabet datasets that can be applied in a manifold learning framework.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20351;&#29992;&#23454;&#26102;&#30340;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#65292;&#20998;&#26512;&#20102;&#19982;COVID-19&#30123;&#24773;&#26399;&#38388;&#22235;&#31181;&#19981;&#33391;&#32463;&#21382;&#30456;&#20851;&#30340;&#35821;&#35328;&#27169;&#24335;&#12290;&#36890;&#36807;&#25552;&#20986;&#31232;&#30095;&#24615;&#20248;&#21270;&#38382;&#39064;&#21644;&#35821;&#35328;&#27169;&#24335;&#30456;&#20284;&#24615;&#32422;&#26463;&#26465;&#20214;&#65292;&#35299;&#20915;&#20102;&#35813;&#38382;&#39064;&#30340;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2310.03941</link><description>&lt;p&gt;
LaTeX: &#23545;&#30123;&#24773;&#26399;&#38388;&#19981;&#33391;&#32463;&#21382;&#30340;&#35821;&#35328;&#27169;&#24335;&#24863;&#30693;&#35302;&#21457;&#20107;&#20214;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
LaTeX: Language Pattern-aware Triggering Event Detection for Adverse Experience during Pandemics. (arXiv:2310.03941v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03941
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20351;&#29992;&#23454;&#26102;&#30340;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#65292;&#20998;&#26512;&#20102;&#19982;COVID-19&#30123;&#24773;&#26399;&#38388;&#22235;&#31181;&#19981;&#33391;&#32463;&#21382;&#30456;&#20851;&#30340;&#35821;&#35328;&#27169;&#24335;&#12290;&#36890;&#36807;&#25552;&#20986;&#31232;&#30095;&#24615;&#20248;&#21270;&#38382;&#39064;&#21644;&#35821;&#35328;&#27169;&#24335;&#30456;&#20284;&#24615;&#32422;&#26463;&#26465;&#20214;&#65292;&#35299;&#20915;&#20102;&#35813;&#38382;&#39064;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
COVID-19&#22823;&#27969;&#34892;&#21152;&#21095;&#20102;&#32654;&#22269;&#21508;&#31181;&#31181;&#26063;&#21644;&#26063;&#35028;&#32676;&#20307;&#20043;&#38388;&#30340;&#31038;&#20250;&#32463;&#27982;&#24046;&#36317;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#22312;&#20984;&#26174;&#21644;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;&#36890;&#36807;&#20998;&#26512;Twitter&#19978;&#30340;&#23454;&#26102;&#25968;&#25454;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19982;&#22235;&#31181;&#20027;&#35201;&#19981;&#33391;&#32463;&#21382;&#30456;&#20851;&#30340;&#35821;&#35328;&#27169;&#24335;&#65306;&#22833;&#19994;&#25910;&#20837;&#25439;&#22833;&#65288;LI&#65289;&#65292;&#39135;&#29289;&#21294;&#20047;&#65288;FS&#65289;&#65292;&#20303;&#25151;&#19981;&#23433;&#20840;&#65288;HI&#65289;&#21644;&#26410;&#28385;&#36275;&#30340;&#24515;&#29702;&#20581;&#24247;&#26381;&#21153;&#38656;&#27714;&#65288;UM&#65289;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#31232;&#30095;&#24615;&#20248;&#21270;&#38382;&#39064;&#65292;&#20174;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#28304;&#20013;&#25552;&#21462;&#20302;&#32423;&#35821;&#35328;&#29305;&#24449;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#20808;&#39564;&#30693;&#35782;&#30340;&#35821;&#35328;&#27169;&#24335;&#30456;&#20284;&#24615;&#30340;&#26032;&#32422;&#26463;&#26465;&#20214;&#12290;&#30001;&#20110;&#30446;&#26631;&#20989;&#25968;&#30340;&#38750;&#20984;&#24615;&#21644;n&#20010;&#19981;&#33391;&#32463;&#21382;&#20043;&#38388;&#30340;&#35821;&#35328;&#27169;&#24335;&#30456;&#20284;&#24615;&#20851;&#31995;&#65292;&#35813;&#38382;&#39064;&#24456;&#38590;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;
The COVID-19 pandemic has accentuated socioeconomic disparities across various racial and ethnic groups in the United States. While previous studies have utilized traditional survey methods like the Household Pulse Survey (HPS) to elucidate these disparities, this paper explores the role of social media platforms in both highlighting and addressing these challenges. Drawing from real-time data sourced from Twitter, we analyzed language patterns related to four major types of adverse experiences: loss of employment income (LI), food scarcity (FS), housing insecurity (HI), and unmet needs for mental health services (UM). We first formulate a sparsity optimization problem that extracts low-level language features from social media data sources. Second, we propose novel constraints on feature similarity exploiting prior knowledge about the similarity of the language patterns among the adverse experiences. The proposed problem is challenging to solve due to the non-convexity objective and n
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;&#26679;&#26412;&#21450;&#20854;&#26368;&#36817;&#37051;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#39044;&#27979;&#30340;&#21152;&#26435;&#24179;&#22343;&#65292;&#25913;&#21892;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#23545;&#25239;&#24615;&#25915;&#20987;&#12289;&#26631;&#31614;&#22122;&#22768;&#12289;&#20998;&#31867;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#25913;&#21464;&#32593;&#32476;&#26550;&#26500;&#12289;&#35757;&#32451;&#36807;&#31243;&#25110;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2310.03927</link><description>&lt;p&gt;
&#20351;&#29992;&#26368;&#36817;&#37051;&#25913;&#21892;&#20998;&#31867;&#22120;&#20915;&#31574;&#36793;&#30028;
&lt;/p&gt;
&lt;p&gt;
Improving classifier decision boundaries using nearest neighbors. (arXiv:2310.03927v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03927
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;&#26679;&#26412;&#21450;&#20854;&#26368;&#36817;&#37051;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#39044;&#27979;&#30340;&#21152;&#26435;&#24179;&#22343;&#65292;&#25913;&#21892;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#23545;&#25239;&#24615;&#25915;&#20987;&#12289;&#26631;&#31614;&#22122;&#22768;&#12289;&#20998;&#31867;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#25913;&#21464;&#32593;&#32476;&#26550;&#26500;&#12289;&#35757;&#32451;&#36807;&#31243;&#25110;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#26080;&#27861;&#23398;&#20064;&#26368;&#20248;&#20915;&#31574;&#36793;&#30028;&#12290;&#25105;&#20204;&#34920;&#26126;&#20915;&#31574;&#36793;&#30028;&#20301;&#20110;&#35757;&#32451;&#25968;&#25454;&#23494;&#24230;&#20302;&#30340;&#21306;&#22495;&#65292;&#24182;&#21463;&#21040;&#23569;&#37327;&#35757;&#32451;&#26679;&#26412;&#30340;&#24433;&#21709;&#65292;&#23481;&#26131;&#23548;&#33268;&#36807;&#25311;&#21512;&#12290;&#25105;&#20204;&#25552;&#20379;&#19968;&#20010;&#31616;&#21333;&#30340;&#31639;&#27861;&#65292;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#35745;&#31639;&#26679;&#26412;&#21450;&#20854;&#26368;&#36817;&#37051;&#30340;&#39044;&#27979;&#30340;&#21152;&#26435;&#24179;&#22343;&#65292;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#21508;&#31181;&#37325;&#35201;&#25351;&#26631;&#23548;&#33268;&#20102;&#19968;&#20123;&#26377;&#21033;&#30340;&#32467;&#26524;&#12290;&#22312;&#25105;&#20204;&#30340;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#21508;&#31181;&#33258;&#35757;&#32451;&#21644;&#39044;&#35757;&#32451;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26469;&#23637;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#25913;&#21892;&#20102;&#65306;(i)&#23545;&#26631;&#31614;&#22122;&#22768;&#30340;&#25269;&#25239;&#21147;&#65292;(ii)&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;(iii)&#20998;&#31867;&#20934;&#30830;&#24615;&#65292;&#29978;&#33267;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#36824;&#26377;(iv)&#21487;&#35299;&#37322;&#24615;&#12290;&#34429;&#28982;&#22312;&#36825;&#22235;&#20010;&#26041;&#38754;&#25913;&#36827;&#19981;&#19968;&#23450;&#26159;&#24456;&#22823;&#30340;&#65292;&#20294;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27010;&#24565;&#19978;&#24456;&#31616;&#21333;&#65292;&#21363;&#25913;&#36827;&#19981;&#38656;&#35201;&#23545;&#32593;&#32476;&#26550;&#26500;&#12289;&#35757;&#32451;&#36807;&#31243;&#25110;&#25968;&#25454;&#38598;&#36827;&#34892;&#20219;&#20309;&#20462;&#25913;&#12290;&#32780;&#19988;&#65292;&#36825;&#20123;&#25913;&#36827;&#19982;
&lt;/p&gt;
&lt;p&gt;
Neural networks are not learning optimal decision boundaries. We show that decision boundaries are situated in areas of low training data density. They are impacted by few training samples which can easily lead to overfitting. We provide a simple algorithm performing a weighted average of the prediction of a sample and its nearest neighbors' (computed in latent space) leading to a minor favorable outcomes for a variety of important measures for neural networks. In our evaluation, we employ various self-trained and pre-trained convolutional neural networks to show that our approach improves (i) resistance to label noise, (ii) robustness against adversarial attacks, (iii) classification accuracy, and to some degree even (iv) interpretability. While improvements are not necessarily large in all four areas, our approach is conceptually simple, i.e., improvements come without any modification to network architecture, training procedure or dataset. Furthermore, they are in stark contrast to 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#65288;TSC&#65289;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#23558;&#26368;&#20808;&#36827;&#30340;&#19968;&#32500;&#21367;&#31215;&#27169;&#22411;&#19982;MTL&#38598;&#25104;&#26102;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20108;&#32500;&#21367;&#31215;&#30340;&#26032;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2310.03925</link><description>&lt;p&gt;
&#20351;&#29992;&#20108;&#32500;&#21367;&#31215;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Multitask Learning for Time Series Data\\with 2D Convolution. (arXiv:2310.03925v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03925
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#65288;TSC&#65289;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#23558;&#26368;&#20808;&#36827;&#30340;&#19968;&#32500;&#21367;&#31215;&#27169;&#22411;&#19982;MTL&#38598;&#25104;&#26102;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20108;&#32500;&#21367;&#31215;&#30340;&#26032;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#32479;&#19968;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#21516;&#26102;&#22788;&#29702;&#19968;&#32452;&#23494;&#20999;&#30456;&#20851;&#30340;&#20219;&#21153;&#12290;&#36890;&#36807;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#20248;&#21270;&#27169;&#22411;&#65292;MTL&#22312;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#36890;&#24120;&#20248;&#20110;&#38750;MTL&#27169;&#22411;&#12290;&#23613;&#31649;MTL&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#25512;&#33616;&#31995;&#32479;&#31561;&#39046;&#22495;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#21364;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23558;MTL&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#65288;TSC&#65289;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#24403;&#23558;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#19968;&#32500;&#21367;&#31215;&#30340;TSC&#27169;&#22411;&#19982;MTL&#38598;&#25104;&#26102;&#65292;TSC&#27169;&#22411;&#30340;&#24615;&#33021;&#23454;&#38469;&#19978;&#20250;&#19979;&#38477;&#12290;&#36890;&#36807;&#23558;&#19968;&#32500;&#21367;&#31215;&#27169;&#22411;&#19982;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#65288;DTW&#65289;&#36317;&#31163;&#20989;&#25968;&#36827;&#34892;&#27604;&#36739;&#65292;&#21487;&#20197;&#30475;&#20986;&#20302;&#19979;&#30340;&#32467;&#26524;&#26159;&#30001;&#20110;&#19968;&#32500;&#21367;&#31215;&#23618;&#30340;&#26377;&#38480;&#34920;&#36798;&#33021;&#21147;&#36896;&#25104;&#30340;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20108;&#32500;&#21367;&#31215;&#30340;&#26032;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multitask learning (MTL) aims to develop a unified model that can handle a set of closely related tasks simultaneously. By optimizing the model across multiple tasks, MTL generally surpasses its non-MTL counterparts in terms of generalizability. Although MTL has been extensively researched in various domains such as computer vision, natural language processing, and recommendation systems, its application to time series data has received limited attention. In this paper, we investigate the application of MTL to the time series classification (TSC) problem. However, when we integrate the state-of-the-art 1D convolution-based TSC model with MTL, the performance of the TSC model actually deteriorates. By comparing the 1D convolution-based models with the Dynamic Time Warping (DTW) distance function, it appears that the underwhelming results stem from the limited expressive power of the 1D convolutional layers. To overcome this challenge, we propose a novel design for a 2D convolution-based
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22522;&#20110;&#20869;&#23481;&#30340;&#26102;&#38388;&#24207;&#21015;&#26816;&#32034;&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#29992;&#25143;&#19982;&#31995;&#32479;&#23454;&#26102;&#20132;&#20114;&#30340;&#24773;&#20917;&#19979;&#65292;&#26377;&#25928;&#22320;&#24230;&#37327;&#21644;&#35745;&#31639;&#19981;&#21516;&#26102;&#38388;&#24207;&#21015;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#65292;&#28385;&#36275;&#29992;&#25143;&#20174;&#22810;&#20010;&#39046;&#22495;&#33719;&#21462;&#26102;&#38388;&#24207;&#21015;&#20449;&#24687;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2310.03919</link><description>&lt;p&gt;
&#19968;&#31181;&#39640;&#25928;&#30340;&#22522;&#20110;&#20869;&#23481;&#30340;&#26102;&#38388;&#24207;&#21015;&#26816;&#32034;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
An Efficient Content-based Time Series Retrieval System. (arXiv:2310.03919v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03919
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22522;&#20110;&#20869;&#23481;&#30340;&#26102;&#38388;&#24207;&#21015;&#26816;&#32034;&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#29992;&#25143;&#19982;&#31995;&#32479;&#23454;&#26102;&#20132;&#20114;&#30340;&#24773;&#20917;&#19979;&#65292;&#26377;&#25928;&#22320;&#24230;&#37327;&#21644;&#35745;&#31639;&#19981;&#21516;&#26102;&#38388;&#24207;&#21015;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#65292;&#28385;&#36275;&#29992;&#25143;&#20174;&#22810;&#20010;&#39046;&#22495;&#33719;&#21462;&#26102;&#38388;&#24207;&#21015;&#20449;&#24687;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20869;&#23481;&#30340;&#26102;&#38388;&#24207;&#21015;&#26816;&#32034;(CTSR)&#31995;&#32479;&#26159;&#19968;&#20010;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#65292;&#29992;&#25143;&#21487;&#20197;&#19982;&#26469;&#33258;&#22810;&#20010;&#39046;&#22495;(&#22914;&#37329;&#34701;&#12289;&#21307;&#30103;&#21644;&#21046;&#36896;&#19994;)&#30340;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#20132;&#20114;&#12290;&#20363;&#22914;&#65292;&#29992;&#25143;&#24819;&#35201;&#20102;&#35299;&#26102;&#38388;&#24207;&#21015;&#30340;&#26469;&#28304;&#65292;&#21487;&#20197;&#23558;&#26102;&#38388;&#24207;&#21015;&#20316;&#20026;&#26597;&#35810;&#25552;&#20132;&#32473;CTSR&#31995;&#32479;&#65292;&#24182;&#26816;&#32034;&#19982;&#20043;&#30456;&#20851;&#30340;&#26102;&#38388;&#24207;&#21015;&#21015;&#34920;&#21450;&#30456;&#20851;&#20803;&#25968;&#25454;&#12290;&#36890;&#36807;&#20998;&#26512;&#26816;&#32034;&#21040;&#30340;&#20803;&#25968;&#25454;&#65292;&#29992;&#25143;&#21487;&#20197;&#33719;&#24471;&#26377;&#20851;&#26102;&#38388;&#24207;&#21015;&#26469;&#28304;&#30340;&#26356;&#22810;&#20449;&#24687;&#12290;&#30001;&#20110;CTSR&#31995;&#32479;&#38656;&#35201;&#22788;&#29702;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#22240;&#27492;&#38656;&#35201;&#19968;&#20010;&#39640;&#23481;&#37327;&#27169;&#22411;&#26469;&#26377;&#25928;&#22320;&#24230;&#37327;&#19981;&#21516;&#26102;&#38388;&#24207;&#21015;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#12290;&#27492;&#22806;&#65292;CTSR&#31995;&#32479;&#20869;&#30340;&#27169;&#22411;&#36824;&#38656;&#35201;&#20197;&#39640;&#25928;&#30340;&#26041;&#24335;&#35745;&#31639;&#30456;&#20284;&#24230;&#24471;&#20998;&#65292;&#20197;&#28385;&#36275;&#29992;&#25143;&#22312;&#23454;&#26102;&#20132;&#20114;&#20013;&#30340;&#38656;&#27714;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#19988;&#39640;&#25928;&#30340;CTSR&#27169;&#22411;&#65292;&#20854;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#26367;&#20195;&#27169;&#22411;&#65292;&#21516;&#26102;&#20173;&#28982;&#25552;&#20379;&#21512;&#29702;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Content-based Time Series Retrieval (CTSR) system is an information retrieval system for users to interact with time series emerged from multiple domains, such as finance, healthcare, and manufacturing. For example, users seeking to learn more about the source of a time series can submit the time series as a query to the CTSR system and retrieve a list of relevant time series with associated metadata. By analyzing the retrieved metadata, users can gather more information about the source of the time series. Because the CTSR system is required to work with time series data from diverse domains, it needs a high-capacity model to effectively measure the similarity between different time series. On top of that, the model within the CTSR system has to compute the similarity scores in an efficient manner as the users interact with the system in real-time. In this paper, we propose an effective and efficient CTSR model that outperforms alternative models, while still providing reasonable in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#22810;&#39046;&#22495;&#30340;&#26080;&#26631;&#31614;&#26679;&#26412;&#26469;&#24320;&#21457;&#19968;&#31181;&#26377;&#25928;&#30340;&#26102;&#38388;&#24207;&#21015;&#22522;&#30784;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#20851;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#35757;&#32451;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#21333;&#19968;&#39046;&#22495;&#25968;&#25454;&#19978;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.03916</link><description>&lt;p&gt;
&#26397;&#30528;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#22522;&#30784;&#27169;&#22411;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
Toward a Foundation Model for Time Series Data. (arXiv:2310.03916v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#22810;&#39046;&#22495;&#30340;&#26080;&#26631;&#31614;&#26679;&#26412;&#26469;&#24320;&#21457;&#19968;&#31181;&#26377;&#25928;&#30340;&#26102;&#38388;&#24207;&#21015;&#22522;&#30784;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#20851;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#35757;&#32451;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#21333;&#19968;&#39046;&#22495;&#25968;&#25454;&#19978;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#35268;&#27169;&#21644;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#24120;&#20351;&#29992;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#25216;&#26415;&#65292;&#21487;&#20197;&#36866;&#24212;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#20851;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#35757;&#32451;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20165;&#20351;&#29992;&#21333;&#19968;&#39046;&#22495;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#19978;&#65292;&#23548;&#33268;&#23545;&#20854;&#20182;&#31867;&#22411;&#26102;&#38388;&#24207;&#21015;&#30340;&#30693;&#35782;&#32570;&#20047;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#22810;&#39046;&#22495;&#30340;&#26080;&#26631;&#31614;&#26679;&#26412;&#26469;&#24320;&#21457;&#19968;&#31181;&#26377;&#25928;&#30340;&#26102;&#38388;&#24207;&#21015;&#22522;&#30784;&#27169;&#22411;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#37325;&#26032;&#21033;&#29992;&#20102;&#20844;&#24320;&#21487;&#29992;&#30340;UCR&#23384;&#26723;&#65292;&#24182;&#35780;&#20272;&#20102;&#22235;&#31181;&#29616;&#26377;&#30340;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#20197;&#21450;&#19968;&#31181;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
A foundation model is a machine learning model trained on a large and diverse set of data, typically using self-supervised learning-based pre-training techniques, that can be adapted to various downstream tasks. However, current research on time series pre-training has mostly focused on models pre-trained solely on data from a single domain, resulting in a lack of knowledge about other types of time series. However, current research on time series pre-training has predominantly focused on models trained exclusively on data from a single domain. As a result, these models possess domain-specific knowledge that may not be easily transferable to time series from other domains. In this paper, we aim to develop an effective time series foundation model by leveraging unlabeled samples from multiple domains. To achieve this, we repurposed the publicly available UCR Archive and evaluated four existing self-supervised learning-based pre-training methods, along with a novel method, on the dataset
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#20110;&#40065;&#26834;&#38381;&#29615;&#25511;&#21046;&#20219;&#21153;&#65292;&#30740;&#31350;&#24490;&#29615;&#36830;&#25509;&#30340;&#31209;&#21644;&#31232;&#30095;&#24615;&#22914;&#20309;&#24433;&#21709;&#32593;&#32476;&#30340;&#31283;&#23450;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#37319;&#29992;&#20302;&#31209;&#31232;&#30095;&#36830;&#25509;&#21487;&#20197;&#22312;&#38381;&#29615;&#35774;&#32622;&#20013;&#21462;&#24471;&#33391;&#22909;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.03915</link><description>&lt;p&gt;
&#21033;&#29992;&#20302;&#31209;&#21644;&#31232;&#30095;&#30340;&#24490;&#29615;&#36830;&#25509;&#36827;&#34892;&#40065;&#26834;&#38381;&#29615;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Leveraging Low-Rank and Sparse Recurrent Connectivity for Robust Closed-Loop Control. (arXiv:2310.03915v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03915
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#20110;&#40065;&#26834;&#38381;&#29615;&#25511;&#21046;&#20219;&#21153;&#65292;&#30740;&#31350;&#24490;&#29615;&#36830;&#25509;&#30340;&#31209;&#21644;&#31232;&#30095;&#24615;&#22914;&#20309;&#24433;&#21709;&#32593;&#32476;&#30340;&#31283;&#23450;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#37319;&#29992;&#20302;&#31209;&#31232;&#30095;&#36830;&#25509;&#21487;&#20197;&#22312;&#38381;&#29615;&#35774;&#32622;&#20013;&#21462;&#24471;&#33391;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#24320;&#21457;&#33021;&#22815;&#19982;&#21464;&#21270;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#30340;&#33258;&#20027;&#20195;&#29702;&#26159;&#19968;&#20010;&#24320;&#25918;&#24615;&#25361;&#25112;&#12290;&#22312;&#36825;&#20123;&#29615;&#22659;&#20013;&#65292;&#31283;&#20581;&#24615;&#23588;&#20026;&#37325;&#35201;&#65292;&#22240;&#20026;&#20195;&#29702;&#36890;&#24120;&#26159;&#22312;&#19987;&#23478;&#31034;&#33539;&#20013;&#36827;&#34892;&#31163;&#32447;&#25311;&#21512;&#65292;&#20294;&#22312;&#22312;&#32447;&#37096;&#32626;&#26102;&#24517;&#39035;&#33021;&#22815;&#25512;&#24191;&#21040;&#29615;&#22659;&#20869;&#30340;&#38381;&#29615;&#21453;&#39304;&#20013;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#23558;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#20110;&#36825;&#31867;&#20219;&#21153;&#65292;&#24182;&#20102;&#35299;&#20854;&#24490;&#29615;&#36830;&#25509;&#21442;&#25968;&#21270;&#22914;&#20309;&#24433;&#21709;&#38381;&#29615;&#35774;&#32622;&#30340;&#40065;&#26834;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#24490;&#29615;&#36830;&#25509;&#34920;&#31034;&#20026;&#31209;&#21644;&#31232;&#30095;&#24615;&#30340;&#20989;&#25968;&#65292;&#24182;&#20174;&#29702;&#35770;&#21644;&#23454;&#35777;&#30340;&#35282;&#24230;&#23637;&#31034;&#35843;&#33410;&#36825;&#20004;&#20010;&#21464;&#37327;&#23545;&#32593;&#32476;&#21160;&#24577;&#30340;&#26377;&#30410;&#24433;&#21709;&#12290;&#25152;&#25552;&#20986;&#30340;&#20302;&#31209;&#31232;&#30095;&#36830;&#25509;&#20026;&#32593;&#32476;&#24341;&#20837;&#20102;&#21487;&#35299;&#37322;&#24615;&#20808;&#39564;&#65292;&#23545;&#20110;&#19968;&#31867;&#31216;&#20026;&#38381;&#24335;&#36830;&#32493;&#26102;&#38388;&#31070;&#32463;&#32593;&#32476;&#65288;CfCs&#65289;&#30340;&#27169;&#22411;&#26368;&#20026;&#36866;&#29992;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20855;&#26377;&#36739;&#23569;&#21442;&#25968;&#30340;CfCs&#21487;&#20197;&#36229;&#36807;&#20854;&#20182;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing autonomous agents that can interact with changing environments is an open challenge in machine learning. Robustness is particularly important in these settings as agents are often fit offline on expert demonstrations but deployed online where they must generalize to the closed feedback loop within the environment. In this work, we explore the application of recurrent neural networks to tasks of this nature and understand how a parameterization of their recurrent connectivity influences robustness in closed-loop settings. Specifically, we represent the recurrent connectivity as a function of rank and sparsity and show both theoretically and empirically that modulating these two variables has desirable effects on network dynamics. The proposed low-rank, sparse connectivity induces an interpretable prior on the network that proves to be most amenable for a class of models known as closed-form continuous-time neural networks (CfCs). We find that CfCs with fewer parameters can ou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#39640;&#36125;&#21494;&#26031;&#20248;&#21270;&#27169;&#22411;&#24314;&#27169;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#27880;&#24847;&#26426;&#21046;&#34701;&#20837;&#28145;&#24230;&#26680;&#23398;&#20064;&#20013;&#65292;&#20351;&#24471;&#20195;&#29702;&#33021;&#22815;&#36866;&#24212;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#25552;&#39640;&#20248;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.03912</link><description>&lt;p&gt;
RTDK-BO&#65306;&#20855;&#26377;Reinforced Transformer&#28145;&#24230;&#26680;&#20989;&#25968;&#30340;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
RTDK-BO: High Dimensional Bayesian Optimization with Reinforced Transformer Deep kernels. (arXiv:2310.03912v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03912
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#39640;&#36125;&#21494;&#26031;&#20248;&#21270;&#27169;&#22411;&#24314;&#27169;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#27880;&#24847;&#26426;&#21046;&#34701;&#20837;&#28145;&#24230;&#26680;&#23398;&#20064;&#20013;&#65292;&#20351;&#24471;&#20195;&#29702;&#33021;&#22815;&#36866;&#24212;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#25552;&#39640;&#20248;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#36890;&#36807;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#20195;&#29702;&#25351;&#23548;&#65292;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#23545;&#20110;&#39640;&#32500;&#40657;&#30418;&#20248;&#21270;&#38750;&#24120;&#26377;&#25928;&#30340;&#25216;&#26415;&#65292;&#22312;&#24037;&#19994;&#35774;&#35745;&#21644;&#31185;&#23398;&#35745;&#31639;&#31561;&#35768;&#22810;&#24212;&#29992;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#22312;&#21333;&#20989;&#25968;&#20248;&#21270;&#21644;&#23569;&#26679;&#26412;&#22810;&#30446;&#26631;&#20248;&#21270;&#19978;&#24341;&#20837;&#20102;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26469;&#25552;&#39640;&#20248;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#23569;&#26679;&#26412;&#25216;&#26415;&#20063;&#19981;&#33021;&#20805;&#20998;&#21033;&#29992;&#32039;&#23494;&#30456;&#20851;&#30446;&#26631;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#26412;&#25991;&#32467;&#21512;&#20102;&#28145;&#24230;&#26680;&#23398;&#20064;&#65288;DKL&#65289;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;Transformer&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25913;&#36827;&#20102;GP&#20195;&#29702;&#30340;&#24314;&#27169;&#33021;&#21147;&#19982;&#20803;&#23398;&#20064;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#27880;&#24847;&#26426;&#21046;&#34701;&#20837;DKL&#20013;&#26469;&#25913;&#36827;&#20803;&#23398;&#20064;BO&#20195;&#29702;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#22312;BO&#36807;&#31243;&#20013;&#36866;&#24212;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;Transformer&#28145;&#24230;&#26680;&#26041;&#27861;&#19982;&#23569;&#26679;&#26412;&#20803;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#26469;&#25552;&#39640;BO&#30340;&#24314;&#27169;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian Optimization (BO), guided by Gaussian process (GP) surrogates, has proven to be an invaluable technique for efficient, high-dimensional, black-box optimization, a critical problem inherent to many applications such as industrial design and scientific computing. Recent contributions have introduced reinforcement learning (RL) to improve the optimization performance on both single function optimization and \textit{few-shot} multi-objective optimization. However, even few-shot techniques fail to exploit similarities shared between closely related objectives. In this paper, we combine recent developments in Deep Kernel Learning (DKL) and attention-based Transformer models to improve the modeling powers of GP surrogates with meta-learning. We propose a novel method for improving meta-learning BO surrogates by incorporating attention mechanisms into DKL, empowering the surrogates to adapt to contextual information gathered during the BO process. We combine this Transformer Deep Kern
&lt;/p&gt;</description></item><item><title>PyDCM&#26159;&#19968;&#20010;&#29992;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#30340;&#21487;&#23450;&#21046;&#30340;&#25968;&#25454;&#20013;&#24515;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#23450;&#20041;&#37197;&#32622;&#21644;&#21521;&#37327;&#21270;&#30340;&#28909;&#35745;&#31639;&#65292;&#23454;&#29616;&#20102;&#23545;&#25968;&#25454;&#20013;&#24515;&#30340;&#20248;&#21270;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.03906</link><description>&lt;p&gt;
&#29992;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#30340;PyDCM&#65306;&#20026;&#21487;&#25345;&#32493;&#24615;&#23450;&#21046;&#25968;&#25454;&#20013;&#24515;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PyDCM: Custom Data Center Models with Reinforcement Learning for Sustainability. (arXiv:2310.03906v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03906
&lt;/p&gt;
&lt;p&gt;
PyDCM&#26159;&#19968;&#20010;&#29992;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#30340;&#21487;&#23450;&#21046;&#30340;&#25968;&#25454;&#20013;&#24515;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#23450;&#20041;&#37197;&#32622;&#21644;&#21521;&#37327;&#21270;&#30340;&#28909;&#35745;&#31639;&#65292;&#23454;&#29616;&#20102;&#23545;&#25968;&#25454;&#20013;&#24515;&#30340;&#20248;&#21270;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#23545;&#21487;&#25345;&#32493;&#24615;&#21644;&#20943;&#23569;&#30899;&#25490;&#25918;&#30340;&#24378;&#35843;&#26085;&#30410;&#22686;&#21152;&#65292;&#20419;&#20351;&#25919;&#24220;&#21644;&#20225;&#19994;&#37325;&#26032;&#24605;&#32771;&#25968;&#25454;&#20013;&#24515;&#30340;&#35774;&#35745;&#21644;&#36816;&#33829;&#26041;&#27861;&#12290;&#37492;&#20110;&#25968;&#25454;&#20013;&#24515;&#30340;&#39640;&#33021;&#32791;&#21644;&#25351;&#25968;&#32423;&#35745;&#31639;&#24037;&#20316;&#37327;&#65292;&#20248;&#21270;&#33021;&#32791;&#29305;&#21035;&#26159;&#22312;&#20919;&#21364;&#21644;IT&#33021;&#28304;&#20351;&#29992;&#26041;&#38754;&#65292;&#25968;&#25454;&#20013;&#24515;&#26159;&#20248;&#21270;&#30005;&#21147;&#28040;&#32791;&#30340;&#29702;&#24819;&#20505;&#36873;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#26159;&#32570;&#20047;&#21487;&#37197;&#32622;&#21644;&#21487;&#25193;&#23637;&#30340;&#28909;&#25968;&#25454;&#20013;&#24515;&#27169;&#22411;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#31649;&#36947;&#12290;&#25968;&#25454;&#20013;&#24515;&#30001;&#22810;&#20010;IT&#32452;&#20214;&#32452;&#25104;&#65292;&#20854;&#20960;&#20309;&#37197;&#32622;&#21644;&#25955;&#28909;&#20351;&#24471;&#28909;&#24314;&#27169;&#21464;&#24471;&#22256;&#38590;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;PyDCM&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;Python&#23454;&#29616;&#30340;&#21487;&#23450;&#21046;&#30340;&#25968;&#25454;&#20013;&#24515;&#27169;&#22411;&#65292;&#29992;&#25143;&#21487;&#20197;&#20351;&#29992;&#33258;&#23450;&#20041;&#30340;&#26381;&#21153;&#22120;&#35268;&#26684;&#21644;IT&#26426;&#26588;&#30340;&#20960;&#20309;&#24067;&#32622;&#21019;&#24314;&#29420;&#29305;&#30340;&#37197;&#32622;&#12290;&#20351;&#29992;&#21521;&#37327;&#21270;&#30340;&#28909;&#35745;&#31639;&#20351;&#24471;PyDCM&#27604;&#24403;&#21069;&#26041;&#27861;&#24555;&#20102;&#25968;&#20010;&#25968;&#37327;&#32423;&#65288;30&#20493;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing global emphasis on sustainability and reducing carbon emissions is pushing governments and corporations to rethink their approach to data center design and operation. Given their high energy consumption and exponentially large computational workloads, data centers are prime candidates for optimizing power consumption, especially in areas such as cooling and IT energy usage. A significant challenge in this pursuit is the lack of a configurable and scalable thermal data center model that offers an end-to-end pipeline. Data centers consist of multiple IT components whose geometric configuration and heat dissipation make thermal modeling difficult. This paper presents PyDCM, a customizable Data Center Model implemented in Python, that allows users to create unique configurations of IT equipment with custom server specifications and geometric arrangements of IT cabinets. The use of vectorized thermal calculations makes PyDCM orders of magnitude faster (30 times) than current 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#36864;&#28779;&#26041;&#27861;&#20272;&#35745;&#24402;&#19968;&#21270;&#24120;&#25968;&#30340;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#12290;&#36890;&#36807;&#35780;&#20272;&#19981;&#21516;&#35774;&#35745;&#36873;&#25321;&#23545;&#20272;&#35745;&#35823;&#24046;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;&#20351;&#29992;&#36864;&#28779;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#22120;&#26356;&#26377;&#25928;&#65292;&#24182;&#19988;&#20351;&#29992;&#20960;&#20309;&#36335;&#24452;&#21487;&#20197;&#38477;&#20302;&#20272;&#35745;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2310.03902</link><description>&lt;p&gt;
&#36890;&#36807;&#36864;&#28779;&#26469;&#20272;&#35745;&#24402;&#19968;&#21270;&#24120;&#25968;&#30340;&#21487;&#35777;&#26126;&#30340;&#30410;&#22788;&#65306;&#37325;&#35201;&#24615;&#25277;&#26679;&#65292;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#65292;&#20197;&#21450;&#26356;&#22810;
&lt;/p&gt;
&lt;p&gt;
Provable benefits of annealing for estimating normalizing constants: Importance Sampling, Noise-Contrastive Estimation, and beyond. (arXiv:2310.03902v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03902
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#36864;&#28779;&#26041;&#27861;&#20272;&#35745;&#24402;&#19968;&#21270;&#24120;&#25968;&#30340;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#12290;&#36890;&#36807;&#35780;&#20272;&#19981;&#21516;&#35774;&#35745;&#36873;&#25321;&#23545;&#20272;&#35745;&#35823;&#24046;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;&#20351;&#29992;&#36864;&#28779;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#22120;&#26356;&#26377;&#25928;&#65292;&#24182;&#19988;&#20351;&#29992;&#20960;&#20309;&#36335;&#24452;&#21487;&#20197;&#38477;&#20302;&#20272;&#35745;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#23637;&#20102;&#20960;&#31181;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#26469;&#20272;&#35745;&#24402;&#19968;&#21270;&#24120;&#25968;&#65288;&#37197;&#20998;&#20989;&#25968;&#65289;&#65292;&#36825;&#20123;&#26041;&#27861;&#22522;&#20110;&#36864;&#28779;&#30340;&#24605;&#24819;&#12290;&#21363;&#20174;&#21487;&#35745;&#31639;&#30340;&#8220;&#25552;&#35758;&#8221;&#20998;&#24067;&#21644;&#26410;&#24402;&#19968;&#21270;&#30340;&#8220;&#30446;&#26631;&#8221;&#20998;&#24067;&#20043;&#38388;&#30340;&#36335;&#24452;&#36880;&#27493;&#37319;&#26679;&#12290;&#36825;&#20123;&#23478;&#26063;&#20013;&#30340;&#37325;&#35201;&#20272;&#35745;&#22120;&#21253;&#25324;&#36864;&#28779;&#37325;&#35201;&#24615;&#25277;&#26679;&#21644;&#36864;&#28779;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#65288;NCE&#65289;&#12290;&#36825;&#26679;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#35768;&#22810;&#35774;&#35745;&#36873;&#25321;&#65306;&#20351;&#29992;&#21738;&#20010;&#20272;&#35745;&#22120;&#12289;&#20351;&#29992;&#21738;&#20010;&#20998;&#24067;&#36335;&#24452;&#20197;&#21450;&#26159;&#21542;&#20351;&#29992;&#20998;&#24067;&#36335;&#24452;&#65307;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#23545;&#20110;&#21738;&#20123;&#36873;&#25321;&#26159;&#26377;&#25928;&#30340;&#36824;&#27809;&#26377;&#26126;&#30830;&#30340;&#29702;&#35770;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#20135;&#29983;&#30340;&#28176;&#36817;&#20272;&#35745;&#35823;&#24046;&#26469;&#35780;&#20272;&#27599;&#20010;&#35774;&#35745;&#36873;&#25321;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20351;&#29992;NCE&#27604;&#37325;&#35201;&#24615;&#25277;&#26679;&#20272;&#35745;&#22120;&#26356;&#26377;&#25928;&#65292;&#20294;&#22312;&#26080;&#38480;&#23567;&#30340;&#36335;&#24452;&#27493;&#38271;&#30340;&#26497;&#38480;&#19979;&#65292;&#24046;&#24322;&#28040;&#22833;&#20102;&#12290;&#31532;&#20108;&#65292;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#20960;&#20309;&#36335;&#24452;&#23558;&#20272;&#35745;&#35823;&#24046;&#20174;&#25351;&#25968;&#32423;&#38477;&#20302;&#21040;...
&lt;/p&gt;
&lt;p&gt;
Recent research has developed several Monte Carlo methods for estimating the normalization constant (partition function) based on the idea of annealing. This means sampling successively from a path of distributions that interpolate between a tractable "proposal" distribution and the unnormalized "target" distribution. Prominent estimators in this family include annealed importance sampling and annealed noise-contrastive estimation (NCE). Such methods hinge on a number of design choices: which estimator to use, which path of distributions to use and whether to use a path at all; so far, there is no definitive theory on which choices are efficient. Here, we evaluate each design choice by the asymptotic estimation error it produces. First, we show that using NCE is more efficient than the importance sampling estimator, but in the limit of infinitesimal path steps, the difference vanishes. Second, we find that using the geometric path brings down the estimation error from an exponential to
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#30452;&#25509;&#21033;&#29992;&#34507;&#30333;&#36136;&#26230;&#20307;&#23398;&#21644;&#37096;&#20998;&#32467;&#26500;&#20449;&#24687;&#26469;&#39044;&#27979;&#34507;&#30333;&#36136;&#30005;&#23376;&#23494;&#24230;&#22270;&#30340;&#36716;&#25442;&#22120;&#27169;&#22411;&#65292;&#36890;&#36807;&#20004;&#20010;&#26032;&#30340;&#32957;&#27573;&#25968;&#25454;&#38598;&#28436;&#31034;&#20102;&#20854;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.03899</link><description>&lt;p&gt;
CrysFormer: &#22522;&#20110;3D Patterson&#26144;&#23556;&#21644;&#37096;&#20998;&#32467;&#26500;&#27880;&#24847;&#21147;&#30340;&#34507;&#30333;&#36136;&#32467;&#26500;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
CrysFormer: Protein Structure Prediction via 3d Patterson Maps and Partial Structure Attention. (arXiv:2310.03899v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03899
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#30452;&#25509;&#21033;&#29992;&#34507;&#30333;&#36136;&#26230;&#20307;&#23398;&#21644;&#37096;&#20998;&#32467;&#26500;&#20449;&#24687;&#26469;&#39044;&#27979;&#34507;&#30333;&#36136;&#30005;&#23376;&#23494;&#24230;&#22270;&#30340;&#36716;&#25442;&#22120;&#27169;&#22411;&#65292;&#36890;&#36807;&#20004;&#20010;&#26032;&#30340;&#32957;&#27573;&#25968;&#25454;&#38598;&#28436;&#31034;&#20102;&#20854;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#23450;&#34507;&#30333;&#36136;&#30340;&#32467;&#26500;&#26159;&#19968;&#20010;&#25968;&#21313;&#24180;&#26469;&#30340;&#24748;&#32780;&#26410;&#20915;&#38382;&#39064;&#12290;&#24403;&#32463;&#20856;&#27169;&#25311;&#31639;&#27861;&#34987;&#24212;&#29992;&#26102;&#65292;&#34507;&#30333;&#36136;&#30340;&#19977;&#32500;&#32467;&#26500;&#36890;&#24120;&#20855;&#26377;&#38750;&#24179;&#20961;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#36716;&#25442;&#22120;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#36827;&#23637;&#65292;&#22914;AlphaFold2&#65292;&#36890;&#36807;&#20174;&#22823;&#22411;&#24207;&#21015;&#20449;&#24687;&#21644;&#30456;&#23545;&#24212;&#30340;&#34507;&#30333;&#36136;&#32467;&#26500;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#65292;&#22312;&#36825;&#20010;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#21482;&#20851;&#27880;&#24207;&#21015;&#20449;&#24687;&#65307;&#20854;&#20182;&#21487;&#29992;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#22914;&#34507;&#30333;&#36136;&#26230;&#20307;&#23398;&#21644;&#27688;&#22522;&#37240;&#30340;&#37096;&#20998;&#32467;&#26500;&#65292;&#26377;&#21487;&#33021;&#34987;&#21033;&#29992;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#30452;&#25509;&#21033;&#29992;&#34507;&#30333;&#36136;&#26230;&#20307;&#23398;&#21644;&#37096;&#20998;&#32467;&#26500;&#20449;&#24687;&#26469;&#39044;&#27979;&#34507;&#30333;&#36136;&#30005;&#23376;&#23494;&#24230;&#22270;&#30340;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#27169;&#22411;&#12290;&#36890;&#36807;&#20004;&#20010;&#26032;&#30340;&#32957;&#27573;&#25968;&#25454;&#38598;&#65288;2&#20010;&#27531;&#22522;&#21644;15&#20010;&#27531;&#22522;&#65289;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;"CrysFormer"&#65292;&#21487;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Determining the structure of a protein has been a decades-long open question. A protein's three-dimensional structure often poses nontrivial computation costs, when classical simulation algorithms are utilized. Advances in the transformer neural network architecture -- such as AlphaFold2 -- achieve significant improvements for this problem, by learning from a large dataset of sequence information and corresponding protein structures. Yet, such methods only focus on sequence information; other available prior knowledge, such as protein crystallography and partial structure of amino acids, could be potentially utilized. To the best of our knowledge, we propose the first transformer-based model that directly utilizes protein crystallography and partial structure information to predict the electron density maps of proteins. Via two new datasets of peptide fragments (2-residue and 15-residue) , we demonstrate our method, dubbed \texttt{CrysFormer}, can achieve accurate predictions, based on
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#24863;&#30693;&#27491;&#21017;&#21270;&#30340;&#29983;&#25104;&#32463;&#39564;&#37325;&#25918;&#30340;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#36830;&#32493;&#23398;&#20064;&#20013;&#26032;&#20219;&#21153;&#32047;&#31215;&#19988;&#19981;&#36951;&#24536;&#26087;&#20219;&#21153;&#30340;&#25361;&#25112;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20005;&#26684;&#30340;&#22686;&#37327;&#31867;&#21035;&#35774;&#32622;&#19979;&#20855;&#26377;&#36739;&#22909;&#30340;&#35760;&#24518;&#20445;&#30041;&#21644;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.03898</link><description>&lt;p&gt;
&#22522;&#20110;&#26102;&#38388;&#24863;&#30693;&#27491;&#21017;&#21270;&#30340;&#29983;&#25104;&#32463;&#39564;&#37325;&#25918;&#30340;&#22686;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Class-Incremental Learning Using Generative Experience Replay Based on Time-aware Regularization. (arXiv:2310.03898v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03898
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#24863;&#30693;&#27491;&#21017;&#21270;&#30340;&#29983;&#25104;&#32463;&#39564;&#37325;&#25918;&#30340;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#36830;&#32493;&#23398;&#20064;&#20013;&#26032;&#20219;&#21153;&#32047;&#31215;&#19988;&#19981;&#36951;&#24536;&#26087;&#20219;&#21153;&#30340;&#25361;&#25112;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20005;&#26684;&#30340;&#22686;&#37327;&#31867;&#21035;&#35774;&#32622;&#19979;&#20855;&#26377;&#36739;&#22909;&#30340;&#35760;&#24518;&#20445;&#30041;&#21644;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#65292;&#23398;&#20064;&#26032;&#20219;&#21153;&#32780;&#19981;&#36951;&#24536;&#26087;&#20219;&#21153;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#29983;&#25104;&#32463;&#39564;&#37325;&#25918;&#36890;&#36807;&#21512;&#25104;&#36807;&#21435;&#23398;&#20064;&#20219;&#21153;&#30340;&#20266;&#25968;&#25454;&#28857;&#65292;&#24182;&#22312;&#19982;&#26032;&#20219;&#21153;&#30340;&#25968;&#25454;&#19968;&#36215;&#24182;&#34892;&#35757;&#32451;&#26102;&#36827;&#34892;&#37325;&#25918;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#36825;&#20010;&#25361;&#25112;&#12290;&#22312;&#20005;&#26684;&#30340;&#22686;&#37327;&#31867;&#21035;&#35774;&#32622;&#19979;&#65292;&#29983;&#25104;&#37325;&#25918;&#26159;&#36830;&#32493;&#23398;&#20064;&#30340;&#26368;&#20339;&#31574;&#30053;&#65292;&#21516;&#26102;&#35201;&#28385;&#36275;&#19968;&#20123;&#32422;&#26463;&#26465;&#20214;&#65306;&#65288;i&#65289;&#27169;&#22411;&#22823;&#23567;&#22266;&#23450;&#65292;&#65288;ii&#65289;&#27809;&#26377;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#65288;iii&#65289;&#27809;&#26377;&#29992;&#20110;&#23384;&#20648;&#36807;&#21435;&#20219;&#21153;&#25968;&#25454;&#30340;&#20869;&#23384;&#32531;&#20914;&#21306;&#12290;&#21463;&#29983;&#29289;&#31070;&#32463;&#31995;&#32479;&#26426;&#21046;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26102;&#38388;&#24863;&#30693;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#21160;&#24577;&#35843;&#25972;&#29983;&#25104;&#37325;&#25918;&#30340;&#19977;&#20010;&#35757;&#32451;&#30446;&#26631;&#39033;&#65306;&#30417;&#30563;&#23398;&#20064;&#12289;&#28508;&#22312;&#27491;&#21017;&#21270;&#21644;&#25968;&#25454;&#37325;&#26500;&#12290;&#20027;&#35201;&#22522;&#20934;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#36825;&#31181;&#20005;&#26684;&#35774;&#32622;&#19979;&#25512;&#21160;&#20102;&#21463;&#22823;&#33041;&#21551;&#21457;&#30340;&#36830;&#32493;&#23398;&#20064;&#27169;&#22411;&#30340;&#26497;&#38480;&#65292;&#25552;&#39640;&#20102;&#35760;&#24518;&#20445;&#30041;&#21644;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning new tasks accumulatively without forgetting remains a critical challenge in continual learning. Generative experience replay addresses this challenge by synthesizing pseudo-data points for past learned tasks and later replaying them for concurrent training along with the new tasks' data. Generative replay is the best strategy for continual learning under a strict class-incremental setting when certain constraints need to be met: (i) constant model size, (ii) no pre-training dataset, and (iii) no memory buffer for storing past tasks' data. Inspired by the biological nervous system mechanisms, we introduce a time-aware regularization method to dynamically fine-tune the three training objective terms used for generative replay: supervised learning, latent regularization, and data reconstruction. Experimental results on major benchmarks indicate that our method pushes the limit of brain-inspired continual learners under such strict settings, improves memory retention, and increase
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#22522;&#20110;&#26681;&#21270;&#36923;&#36753;&#30446;&#26631;&#20989;&#25968;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25512;&#23548;&#20986;&#19968;&#31995;&#21015;&#20005;&#26684;&#20984;&#20989;&#25968;&#65292;&#23454;&#29616;&#20102;&#19982;&#26368;&#23567;&#33539;&#25968;&#35299;&#30456;&#21516;&#30340;&#26368;&#23567;&#21270;&#28857;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#26368;&#20248;&#32467;&#26524;&#30340;&#36798;&#25104;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.03890</link><description>&lt;p&gt;
&#22522;&#20110;&#26681;&#21270;&#36923;&#36753;&#30446;&#26631;&#20989;&#25968;&#30340;&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Accelerated Neural Network Training with Rooted Logistic Objectives. (arXiv:2310.03890v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03890
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#22522;&#20110;&#26681;&#21270;&#36923;&#36753;&#30446;&#26631;&#20989;&#25968;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25512;&#23548;&#20986;&#19968;&#31995;&#21015;&#20005;&#26684;&#20984;&#20989;&#25968;&#65292;&#23454;&#29616;&#20102;&#19982;&#26368;&#23567;&#33539;&#25968;&#35299;&#30456;&#21516;&#30340;&#26368;&#23567;&#21270;&#28857;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#26368;&#20248;&#32467;&#26524;&#30340;&#36798;&#25104;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#37096;&#32626;&#30340;&#31070;&#32463;&#32593;&#32476;&#26159;&#20351;&#29992;&#22522;&#20110;&#20132;&#21449;&#29109;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#20174;&#20248;&#21270;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#25105;&#20204;&#30693;&#36947;&#19968;&#38454;&#26041;&#27861;&#65288;&#22914;&#26799;&#24230;&#19979;&#38477;&#65289;&#30340;&#34892;&#20026;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#25968;&#25454;&#38598;&#30340;&#21487;&#20998;&#24615;&#12290;&#20107;&#23454;&#19978;&#65292;&#21363;&#20351;&#22312;&#26368;&#31616;&#21333;&#30340;&#20108;&#20998;&#31867;&#24773;&#20917;&#19979;&#65292;&#25910;&#25947;&#36895;&#24230;&#21462;&#20915;&#20110;&#20004;&#20010;&#22240;&#32032;&#65306;&#65288;1&#65289;&#25968;&#25454;&#30697;&#38453;&#30340;&#26465;&#20214;&#25968;&#65292;&#21644;&#65288;2&#65289;&#25968;&#25454;&#38598;&#30340;&#21487;&#20998;&#24615;&#12290;&#22312;&#27809;&#26377;&#36827;&#19968;&#27493;&#39044;&#22788;&#29702;&#25216;&#26415;&#65288;&#22914;&#36229;&#21442;&#25968;&#21270;&#12289;&#25968;&#25454;&#22686;&#24378;&#31561;&#65289;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20998;&#24615;&#26159;&#25152;&#32771;&#34385;&#30340;&#25968;&#25454;&#20998;&#24067;&#22266;&#26377;&#30340;&#37327;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#36923;&#36753;&#20989;&#25968;&#30340;&#20248;&#21270;&#65292;&#24182;&#25512;&#23548;&#20986;&#19968;&#31995;&#21015;&#26032;&#39062;&#30340;&#20005;&#26684;&#20984;&#20989;&#25968;&#65292;&#36825;&#20123;&#20989;&#25968;&#33267;&#23569;&#21644;&#36923;&#36753;&#25439;&#22833;&#19968;&#26679;&#20005;&#26684;&#12290;&#36825;&#20123;&#20989;&#25968;&#30340;&#26368;&#23567;&#21270;&#28857;&#19982;&#26368;&#23567;&#33539;&#25968;&#35299;&#30340;&#26368;&#23567;&#21270;&#28857;&#19968;&#33268;&#65292;&#20197;&#23613;&#21487;&#33021;&#20351;&#29992;&#12290;&#36825;&#20010;&#25512;&#23548;&#30340;&#20989;&#25968;&#30340;&#20005;&#26684;&#20984;&#24615;&#21487;&#20197;&#25193;&#23637;&#21040;&#24494;&#35843;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many neural networks deployed in the real world scenarios are trained using cross entropy based loss functions. From the optimization perspective, it is known that the behavior of first order methods such as gradient descent crucially depend on the separability of datasets. In fact, even in the most simplest case of binary classification, the rate of convergence depends on two factors: (1) condition number of data matrix, and (2) separability of the dataset. With no further pre-processing techniques such as over-parametrization, data augmentation etc., separability is an intrinsic quantity of the data distribution under consideration. We focus on the landscape design of the logistic function and derive a novel sequence of {\em strictly} convex functions that are at least as strict as logistic loss. The minimizers of these functions coincide with those of the minimum norm solution wherever possible. The strict convexity of the derived function can be extended to finetune state-of-the-ar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#20110;&#24037;&#20316;&#20013;&#30340;&#20449;&#24687;&#35770;&#32773;&#26469;&#35828;&#65292;&#20449;&#24687;&#20960;&#20309;&#30340;&#22522;&#26412;&#27010;&#36848;&#12290;&#35299;&#37322;&#20102;&#32479;&#35745;&#27969;&#24418;&#19978;&#30340;&#24046;&#24322;&#12289;&#36317;&#31163;&#12289;&#27491;&#20132;&#24615;&#21644;&#27979;&#22320;&#32447;&#30340;&#27010;&#24565;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20123;&#26368;&#36817;&#30340;&#20449;&#24687;&#20960;&#20309;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2310.03884</link><description>&lt;p&gt;
&#20449;&#24687;&#20960;&#20309;&#23545;&#20110;&#24037;&#20316;&#20013;&#30340;&#20449;&#24687;&#35770;&#32773;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Information Geometry for the Working Information Theorist. (arXiv:2310.03884v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03884
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#20110;&#24037;&#20316;&#20013;&#30340;&#20449;&#24687;&#35770;&#32773;&#26469;&#35828;&#65292;&#20449;&#24687;&#20960;&#20309;&#30340;&#22522;&#26412;&#27010;&#36848;&#12290;&#35299;&#37322;&#20102;&#32479;&#35745;&#27969;&#24418;&#19978;&#30340;&#24046;&#24322;&#12289;&#36317;&#31163;&#12289;&#27491;&#20132;&#24615;&#21644;&#27979;&#22320;&#32447;&#30340;&#27010;&#24565;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20123;&#26368;&#36817;&#30340;&#20449;&#24687;&#20960;&#20309;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#20960;&#20309;&#26159;&#20174;&#20960;&#20309;&#35282;&#24230;&#30740;&#31350;&#32479;&#35745;&#27969;&#24418;&#65292;&#21363;&#27010;&#29575;&#20998;&#24067;&#31354;&#38388;&#30340;&#23398;&#31185;&#12290;&#20854;&#32463;&#20856;&#30340;&#20449;&#24687;&#29702;&#35770;&#24212;&#29992;&#19982;&#32479;&#35745;&#27010;&#24565;&#65288;&#22914;&#36153;&#33293;&#23572;&#20449;&#24687;&#12289;&#20805;&#20998;&#32479;&#35745;&#37327;&#21644;&#26377;&#25928;&#20272;&#35745;&#37327;&#65289;&#26377;&#20851;&#12290;&#22914;&#20170;&#65292;&#20449;&#24687;&#20960;&#20309;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#36328;&#23398;&#31185;&#39046;&#22495;&#65292;&#22312;&#38647;&#36798;&#24863;&#30693;&#12289;&#38453;&#21015;&#20449;&#21495;&#22788;&#29702;&#12289;&#37327;&#23376;&#29289;&#29702;&#12289;&#28145;&#24230;&#23398;&#20064;&#21644;&#26368;&#20248;&#20256;&#36755;&#31561;&#21508;&#20010;&#39046;&#22495;&#20013;&#25214;&#21040;&#24212;&#29992;&#12290;&#26412;&#25991;&#23545;&#20110;&#19981;&#29087;&#24713;&#36825;&#19968;&#20196;&#20154;&#28608;&#21160;&#30340;&#30740;&#31350;&#39046;&#22495;&#30340;&#20449;&#24687;&#35770;&#32773;&#65292;&#25552;&#20379;&#20102;&#22522;&#26412;&#30340;&#20449;&#24687;&#20960;&#20309;&#27010;&#36848;&#12290;&#25105;&#20204;&#35299;&#37322;&#20102;&#32479;&#35745;&#27969;&#24418;&#19978;&#30340;&#24046;&#24322;&#27010;&#24565;&#65292;&#24191;&#20041;&#30340;&#36317;&#31163;&#27010;&#24565;&#65292;&#27491;&#20132;&#24615;&#21644;&#27979;&#22320;&#32447;&#65292;&#20174;&#32780;&#20026;&#20855;&#20307;&#30340;&#24212;&#29992;&#21644;&#26032;&#39062;&#30340;&#29702;&#35770;&#30740;&#31350;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#25105;&#20204;&#36824;&#24378;&#35843;&#20102;&#19968;&#20123;&#36817;&#26399;&#30340;&#20449;&#24687;&#20960;&#20309;&#21457;&#23637;&#65292;&#36825;&#20123;&#21457;&#23637;&#23545;&#20110;&#26356;&#24191;&#27867;&#30340;&#20449;&#24687;&#35770;&#32773;&#20855;&#26377;&#20852;&#36259;&#12290;
&lt;/p&gt;
&lt;p&gt;
Information geometry is a study of statistical manifolds, that is, spaces of probability distributions from a geometric perspective. Its classical information-theoretic applications relate to statistical concepts such as Fisher information, sufficient statistics, and efficient estimators. Today, information geometry has emerged as an interdisciplinary field that finds applications in diverse areas such as radar sensing, array signal processing, quantum physics, deep learning, and optimal transport. This article presents an overview of essential information geometry to initiate an information theorist, who may be unfamiliar with this exciting area of research. We explain the concepts of divergences on statistical manifolds, generalized notions of distances, orthogonality, and geodesics, thereby paving the way for concrete applications and novel theoretical investigations. We also highlight some recent information-geometric developments, which are of interest to the broader information t
&lt;/p&gt;</description></item><item><title>&#23567;&#25209;&#37327;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#30740;&#31350;&#21457;&#29616;&#23558;&#25209;&#37327;&#22823;&#23567;&#20943;&#23567;&#21487;&#20197;&#20135;&#29983;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#19968;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2310.03882</link><description>&lt;p&gt;
&#23567;&#25209;&#37327;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Small batch deep reinforcement learning. (arXiv:2310.03882v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03882
&lt;/p&gt;
&lt;p&gt;
&#23567;&#25209;&#37327;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#30740;&#31350;&#21457;&#29616;&#23558;&#25209;&#37327;&#22823;&#23567;&#20943;&#23567;&#21487;&#20197;&#20135;&#29983;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#19968;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#20215;&#20540;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#25209;&#37327;&#22823;&#23567;&#21442;&#25968;&#25351;&#23450;&#27599;&#27425;&#26799;&#24230;&#26356;&#26032;&#35201;&#37319;&#26679;&#30340;&#36716;&#25442;&#25968;&#37327;&#12290;&#34429;&#28982;&#36825;&#20010;&#20540;&#23545;&#23398;&#20064;&#36807;&#31243;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#36890;&#24120;&#22312;&#25552;&#20986;&#26032;&#31639;&#27861;&#26102;&#19981;&#20250;&#36827;&#34892;&#35843;&#25972;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#23558;&#25209;&#37327;&#22823;&#23567;&#20943;&#23567;&#21487;&#20197;&#20135;&#29983;&#22810;&#20010;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#65307;&#36825;&#19968;&#28857;&#20196;&#20154;&#24778;&#35766;&#65292;&#22240;&#20026;&#22312;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26102;&#65292;&#36890;&#24120;&#20542;&#21521;&#20110;&#20351;&#29992;&#36739;&#22823;&#30340;&#25209;&#37327;&#22823;&#23567;&#20197;&#25913;&#21892;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#19968;&#31995;&#21015;&#23454;&#35777;&#20998;&#26512;&#26469;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#19968;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
In value-based deep reinforcement learning with replay memories, the batch size parameter specifies how many transitions to sample for each gradient update. Although critical to the learning process, this value is typically not adjusted when proposing new algorithms. In this work we present a broad empirical study that suggests {\em reducing} the batch size can result in a number of significant performance gains; this is surprising, as the general tendency when training neural networks is towards larger batch sizes for improved performance. We complement our experimental findings with a set of empirical analyses towards better understanding this phenomenon.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#38750;&#20132;&#25442;&#20195;&#25968;&#30340;&#20195;&#25968;&#20449;&#21495;&#27169;&#22411;&#22312;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24212;&#29992;&#12290;&#30740;&#31350;&#21457;&#29616;&#38750;&#20132;&#25442;&#21367;&#31215;&#28388;&#27874;&#22120;&#22312;&#31639;&#23376;&#31354;&#38388;&#30340;&#23567;&#25200;&#21160;&#19979;&#21487;&#20197;&#20445;&#25345;&#31283;&#23450;&#65292;&#24182;&#19988;&#23384;&#22312;&#31283;&#23450;&#24615;&#21644;&#36873;&#25321;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.03879</link><description>&lt;p&gt;
&#38750;&#20132;&#25442;&#21367;&#31215;&#20449;&#21495;&#27169;&#22411;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24212;&#29992;: &#23545;&#23567;&#21464;&#24418;&#30340;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Non Commutative Convolutional Signal Models in Neural Networks: Stability to Small Deformations. (arXiv:2310.03879v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03879
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#38750;&#20132;&#25442;&#20195;&#25968;&#30340;&#20195;&#25968;&#20449;&#21495;&#27169;&#22411;&#22312;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24212;&#29992;&#12290;&#30740;&#31350;&#21457;&#29616;&#38750;&#20132;&#25442;&#21367;&#31215;&#28388;&#27874;&#22120;&#22312;&#31639;&#23376;&#31354;&#38388;&#30340;&#23567;&#25200;&#21160;&#19979;&#21487;&#20197;&#20445;&#25345;&#31283;&#23450;&#65292;&#24182;&#19988;&#23384;&#22312;&#31283;&#23450;&#24615;&#21644;&#36873;&#25321;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#26368;&#36817;&#22312;[1]&#20013;&#21457;&#34920;&#30340;&#22522;&#20110;&#38750;&#20132;&#25442;&#20195;&#25968;&#30340;&#20195;&#25968;&#20449;&#21495;&#27169;&#22411;(ASM)&#20197;&#21450;&#23427;&#20204;&#22312;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24212;&#29992;&#12290;&#20511;&#21161;&#20195;&#25968;&#20449;&#21495;&#22788;&#29702;(ASP)&#30340;&#19968;&#33324;&#24037;&#20855;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#38750;&#20132;&#25442;&#21367;&#31215;&#28388;&#27874;&#22120;&#30340;&#28388;&#27874;&#21644;&#31283;&#23450;&#29305;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#38750;&#20132;&#25442;&#28388;&#27874;&#22120;&#22312;&#31639;&#23376;&#31354;&#38388;&#30340;&#23567;&#25200;&#21160;&#19979;&#21487;&#20197;&#20445;&#25345;&#31283;&#23450;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#23613;&#31649;&#38750;&#20132;&#25442;&#20449;&#21495;&#27169;&#22411;&#20013;&#30340;&#20613;&#37324;&#21494;&#34920;&#31034;&#30340;&#35889;&#20998;&#37327;&#19982;&#32500;&#24230;&#22823;&#20110;1&#30340;&#31354;&#38388;&#30456;&#20851;&#32852;&#65292;&#20294;&#22312;&#31283;&#23450;&#24615;&#21644;&#36873;&#25321;&#24615;&#20043;&#38388;&#23384;&#22312;&#31867;&#20284;&#20110;&#20132;&#25442;&#27169;&#22411;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#23545;&#20110;&#32676;&#31070;&#32463;&#32593;&#32476;&#12289;&#22810;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#22235;&#20803;&#31070;&#32463;&#32593;&#32476;&#31561;&#38750;&#20132;&#25442;&#32467;&#26500;&#20855;&#26377;&#30452;&#25509;&#30340;&#24433;&#21709;&#12290;&#26368;&#21518;&#25105;&#20204;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#36825;&#20123;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we discuss the results recently published in~[1] about algebraic signal models (ASMs) based on non commutative algebras and their use in convolutional neural networks. Relying on the general tools from algebraic signal processing (ASP), we study the filtering and stability properties of non commutative convolutional filters. We show how non commutative filters can be stable to small perturbations on the space of operators. We also show that although the spectral components of the Fourier representation in a non commutative signal model are associated to spaces of dimension larger than one, there is a trade-off between stability and selectivity similar to that observed for commutative models. Our results have direct implications for group neural networks, multigraph neural networks and quaternion neural networks, among other non commutative architectures. We conclude by corroborating these results through numerical experiments.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#36164;&#28304;&#26377;&#38480;&#35745;&#31639;&#31995;&#32479;&#20013;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#30340;&#23454;&#26045;&#25104;&#26412;&#21644;&#39044;&#27979;&#36136;&#37327;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#25552;&#20986;&#20102;&#29702;&#35770;&#21644;&#32463;&#39564;&#26041;&#27861;&#26469;&#25506;&#32034;&#36825;&#31181;&#26435;&#34913;&#31354;&#38388;&#65292;&#24182;&#35748;&#20026;&#20102;&#35299;&#36825;&#31181;&#26435;&#34913;&#34892;&#20026;&#23545;&#20110;&#29702;&#35299;&#36164;&#28304;&#21463;&#38480;&#20219;&#21153;&#20013;&#27169;&#22411;&#30340;&#38480;&#21046;&#26159;&#26377;&#30410;&#30340;&#12290;</title><link>http://arxiv.org/abs/2310.03865</link><description>&lt;p&gt;
&#31243;&#24207;&#38454;&#27573;&#30340;&#27169;&#22411;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
Model Complexity of Program Phases. (arXiv:2310.03865v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03865
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#36164;&#28304;&#26377;&#38480;&#35745;&#31639;&#31995;&#32479;&#20013;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#30340;&#23454;&#26045;&#25104;&#26412;&#21644;&#39044;&#27979;&#36136;&#37327;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#25552;&#20986;&#20102;&#29702;&#35770;&#21644;&#32463;&#39564;&#26041;&#27861;&#26469;&#25506;&#32034;&#36825;&#31181;&#26435;&#34913;&#31354;&#38388;&#65292;&#24182;&#35748;&#20026;&#20102;&#35299;&#36825;&#31181;&#26435;&#34913;&#34892;&#20026;&#23545;&#20110;&#29702;&#35299;&#36164;&#28304;&#21463;&#38480;&#20219;&#21153;&#20013;&#27169;&#22411;&#30340;&#38480;&#21046;&#26159;&#26377;&#30410;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#35745;&#31639;&#31995;&#32479;&#20013;&#65292;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#24517;&#39035;&#22312;&#20005;&#26684;&#30340;&#38480;&#21046;&#19979;&#36816;&#34892;&#12290;&#26377;&#21508;&#31181;&#27169;&#22411;&#21487;&#29992;&#65292;&#38024;&#23545;&#36825;&#20123;&#26465;&#20214;&#19979;&#30340;&#39044;&#27979;&#65292;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#19987;&#27880;&#20110;&#38477;&#20302;&#23454;&#26045;&#25104;&#26412;&#12290;&#23454;&#36341;&#20013;&#65292;&#36825;&#20123;&#21463;&#36164;&#28304;&#38480;&#21046;&#30340;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#22312;&#23454;&#29616;&#25104;&#26412;&#21644;&#39044;&#27979;&#36136;&#37327;&#20043;&#38388;&#23384;&#22312;&#26681;&#26412;&#30340;&#26435;&#34913;&#12290;&#36825;&#31181;&#26681;&#26412;&#26435;&#34913;&#22312;&#19981;&#21516;&#20219;&#21153;&#30340;&#27169;&#22411;&#20013;&#20284;&#20046;&#24456;&#23569;&#34987;&#25506;&#32034;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#24517;&#35201;&#30340;&#29702;&#35770;&#21644;&#30456;&#20851;&#30340;&#32463;&#39564;&#36807;&#31243;&#65292;&#20197;&#25506;&#32034;&#19968;&#31867;&#29305;&#23450;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65288;&#22914;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65289;&#30340;&#36825;&#31181;&#26435;&#34913;&#31354;&#38388;&#12290;&#25105;&#20204;&#39044;&#35745;&#20102;&#35299;&#36825;&#31181;&#26435;&#34913;&#34892;&#20026;&#21487;&#33021;&#26377;&#21161;&#20110;&#29702;&#35299;&#36164;&#28304;&#21463;&#38480;&#20219;&#21153;&#30340;&#27169;&#22411;&#21019;&#24314;&#21644;&#37096;&#32626;&#30340;&#29702;&#35770;&#21644;&#23454;&#38469;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
In resource limited computing systems, sequence prediction models must operate under tight constraints. Various models are available that cater to prediction under these conditions that in some way focus on reducing the cost of implementation. These resource constrained sequence prediction models, in practice, exhibit a fundamental tradeoff between the cost of implementation and the quality of its predictions. This fundamental tradeoff seems to be largely unexplored for models for different tasks. Here we formulate the necessary theory and an associated empirical procedure to explore this tradeoff space for a particular family of machine learning models such as deep neural networks. We anticipate that the knowledge of the behavior of this tradeoff may be beneficial in understanding the theoretical and practical limits of creation and deployment of models for resource constrained tasks.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21464;&#20998;&#25216;&#26415;&#20197;&#20248;&#21270;&#24191;&#20041;&#37325;&#24515;&#22352;&#26631;&#65292;&#36890;&#36807;&#30452;&#25509;&#21442;&#25968;&#21270;&#36830;&#32493;&#20989;&#25968;&#26469;&#35299;&#20915;&#20043;&#21069;&#27169;&#22411;&#20013;&#30446;&#26631;&#20989;&#25968;&#36873;&#25321;&#21463;&#38480;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#30446;&#26631;&#20989;&#25968;&#26041;&#38754;&#37117;&#34920;&#29616;&#20986;&#20102;&#28789;&#27963;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#23454;&#29992;&#30340;&#21152;&#36895;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2310.03861</link><description>&lt;p&gt;
&#21464;&#20998;&#37325;&#24515;&#22352;&#26631;
&lt;/p&gt;
&lt;p&gt;
Variational Barycentric Coordinates. (arXiv:2310.03861v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03861
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21464;&#20998;&#25216;&#26415;&#20197;&#20248;&#21270;&#24191;&#20041;&#37325;&#24515;&#22352;&#26631;&#65292;&#36890;&#36807;&#30452;&#25509;&#21442;&#25968;&#21270;&#36830;&#32493;&#20989;&#25968;&#26469;&#35299;&#20915;&#20043;&#21069;&#27169;&#22411;&#20013;&#30446;&#26631;&#20989;&#25968;&#36873;&#25321;&#21463;&#38480;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#30446;&#26631;&#20989;&#25968;&#26041;&#38754;&#37117;&#34920;&#29616;&#20986;&#20102;&#28789;&#27963;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#23454;&#29992;&#30340;&#21152;&#36895;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21464;&#20998;&#25216;&#26415;&#65292;&#29992;&#20110;&#20248;&#21270;&#24191;&#20041;&#37325;&#24515;&#22352;&#26631;&#65292;&#19982;&#29616;&#26377;&#27169;&#22411;&#30456;&#27604;&#65292;&#23427;&#25552;&#20379;&#20102;&#39069;&#22806;&#30340;&#25511;&#21046;&#12290;&#20197;&#21069;&#30340;&#24037;&#20316;&#20351;&#29992;&#32593;&#26684;&#25110;&#38381;&#24335;&#20844;&#24335;&#34920;&#31034;&#37325;&#24515;&#22352;&#26631;&#65292;&#22312;&#23454;&#36341;&#20013;&#38480;&#21046;&#20102;&#30446;&#26631;&#20989;&#25968;&#30340;&#36873;&#25321;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#30452;&#25509;&#21442;&#25968;&#21270;&#36830;&#32493;&#20989;&#25968;&#65292;&#23558;&#20219;&#20309;&#22810;&#38754;&#20307;&#20869;&#37096;&#30340;&#22352;&#26631;&#26144;&#23556;&#21040;&#20854;&#37325;&#24515;&#22352;&#26631;&#65292;&#20351;&#29992;&#31070;&#32463;&#22330;&#23454;&#29616;&#12290;&#36825;&#31181;&#24418;&#24335;&#26159;&#22522;&#20110;&#25105;&#20204;&#23545;&#37325;&#24515;&#22352;&#26631;&#30340;&#29702;&#35770;&#34920;&#24449;&#65292;&#23427;&#20801;&#35768;&#25105;&#20204;&#26500;&#36896;&#21442;&#25968;&#21270;&#25972;&#20010;&#20989;&#25968;&#31867;&#30340;&#31070;&#32463;&#22330;&#65292;&#20197;&#34920;&#31034;&#26377;&#25928;&#30340;&#22352;&#26631;&#12290;&#25105;&#20204;&#20351;&#29992;&#22810;&#31181;&#30446;&#26631;&#20989;&#25968;&#23637;&#31034;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#65292;&#21253;&#25324;&#22810;&#20010;&#24179;&#28369;&#21644;&#21464;&#24418;&#24863;&#30693;&#30340;&#33021;&#37327;&#65307;&#20316;&#20026;&#19968;&#20010;&#38468;&#21152;&#36129;&#29486;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#22312;&#19981;&#36830;&#32493;&#31070;&#32463;&#22330;&#19978;&#27979;&#37327;&#21644;&#26368;&#23567;&#21270;&#24635;&#21464;&#24046;&#31561;&#30446;&#26631;&#30340;&#25968;&#23398;&#21512;&#29702;&#21270;&#25163;&#27573;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#21152;&#36895;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a variational technique to optimize for generalized barycentric coordinates that offers additional control compared to existing models. Prior work represents barycentric coordinates using meshes or closed-form formulae, in practice limiting the choice of objective function. In contrast, we directly parameterize the continuous function that maps any coordinate in a polytope's interior to its barycentric coordinates using a neural field. This formulation is enabled by our theoretical characterization of barycentric coordinates, which allows us to construct neural fields that parameterize the entire function class of valid coordinates. We demonstrate the flexibility of our model using a variety of objective functions, including multiple smoothness and deformation-aware energies; as a side contribution, we also present mathematically-justified means of measuring and minimizing objectives like total variation on discontinuous neural fields. We offer a practical acceleration strat
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#38598;&#25104;&#20102;&#24320;&#25918;&#38598;&#35782;&#21035;&#21644;&#28145;&#24230;&#20998;&#31867;&#22686;&#37327;&#23398;&#20064;&#30340;&#32479;&#19968;&#26694;&#26550;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22686;&#37327;&#23398;&#20064;&#21644;&#24320;&#25918;&#38598;&#35782;&#21035;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.03848</link><description>&lt;p&gt;
OpenIncrement&#65306;&#19968;&#31181;&#32479;&#19968;&#30340;&#24320;&#25918;&#38598;&#35782;&#21035;&#21644;&#28145;&#24230;&#20998;&#31867;&#22686;&#37327;&#23398;&#20064;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
OpenIncrement: A Unified Framework for Open Set Recognition and Deep Class-Incremental Learning. (arXiv:2310.03848v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#38598;&#25104;&#20102;&#24320;&#25918;&#38598;&#35782;&#21035;&#21644;&#28145;&#24230;&#20998;&#31867;&#22686;&#37327;&#23398;&#20064;&#30340;&#32479;&#19968;&#26694;&#26550;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22686;&#37327;&#23398;&#20064;&#21644;&#24320;&#25918;&#38598;&#35782;&#21035;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#22686;&#37327;&#23398;&#20064;&#30340;&#30740;&#31350;&#20013;&#65292;&#22823;&#22810;&#25968;&#24037;&#20316;&#37117;&#20551;&#35774;&#26032;&#26679;&#26412;&#24050;&#32463;&#34987;&#39044;&#20808;&#35782;&#21035;&#65292;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#37325;&#26032;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#30340;&#28145;&#24230;&#20998;&#31867;&#22120;&#32463;&#24120;&#35823;&#35782;&#21035;&#36825;&#20123;&#26679;&#26412;&#65292;&#23548;&#33268;&#38169;&#35823;&#30340;&#39044;&#27979;&#12290;&#36825;&#31181;&#38169;&#35823;&#20998;&#31867;&#20250;&#38477;&#20302;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#24320;&#25918;&#38598;&#35782;&#21035;&#31561;&#25216;&#26415;&#25552;&#20379;&#20102;&#19968;&#31181;&#26816;&#27979;&#36825;&#20123;&#26032;&#26679;&#26412;&#30340;&#26041;&#27861;&#65292;&#20195;&#34920;&#20102;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#19968;&#20010;&#37325;&#35201;&#39046;&#22495;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38598;&#25104;&#20102;&#24320;&#25918;&#38598;&#35782;&#21035;&#30340;&#28145;&#24230;&#20998;&#31867;&#22686;&#37327;&#23398;&#20064;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#35843;&#25972;&#22686;&#37327;&#23398;&#20064;&#21040;&#30340;&#29305;&#24449;&#65292;&#20351;&#20854;&#36866;&#24212;&#22522;&#20110;&#36317;&#31163;&#30340;&#24320;&#25918;&#38598;&#35782;&#21035;&#12290;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22686;&#37327;&#23398;&#20064;&#25216;&#26415;&#65292;&#24182;&#19988;&#22312;&#24320;&#25918;&#38598;&#35782;&#21035;&#26041;&#38754;&#34920;&#29616;&#20986;&#27604;&#22522;&#20934;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In most works on deep incremental learning research, it is assumed that novel samples are pre-identified for neural network retraining. However, practical deep classifiers often misidentify these samples, leading to erroneous predictions. Such misclassifications can degrade model performance. Techniques like open set recognition offer a means to detect these novel samples, representing a significant area in the machine learning domain.  In this paper, we introduce a deep class-incremental learning framework integrated with open set recognition. Our approach refines class-incrementally learned features to adapt them for distance-based open set recognition. Experimental results validate that our method outperforms state-of-the-art incremental learning techniques and exhibits superior performance in open set recognition compared to baseline methods.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#35782;&#21035;Euclid&#31354;&#38388;&#26395;&#36828;&#38236;&#22270;&#20687;&#20013;&#23567;&#34892;&#26143;&#20809;&#36857;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#26500;&#24314;&#12289;&#35757;&#32451;&#21644;&#27979;&#35797;&#19968;&#20010;&#19977;&#27493;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#65292;&#26368;&#32456;&#23454;&#29616;&#20102;&#23545;&#20809;&#36857;&#30340;&#33258;&#21160;&#21270;&#26816;&#27979;&#21644;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2310.03845</link><description>&lt;p&gt;
Euclid:&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#35782;&#21035;&#27169;&#25311;&#22270;&#20687;&#20013;&#30340;&#23567;&#34892;&#26143;&#20809;&#36857;
&lt;/p&gt;
&lt;p&gt;
Euclid: Identification of asteroid streaks in simulated images using deep learning. (arXiv:2310.03845v1 [astro-ph.EP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03845
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#35782;&#21035;Euclid&#31354;&#38388;&#26395;&#36828;&#38236;&#22270;&#20687;&#20013;&#23567;&#34892;&#26143;&#20809;&#36857;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#26500;&#24314;&#12289;&#35757;&#32451;&#21644;&#27979;&#35797;&#19968;&#20010;&#19977;&#27493;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#65292;&#26368;&#32456;&#23454;&#29616;&#20102;&#23545;&#20809;&#36857;&#30340;&#33258;&#21160;&#21270;&#26816;&#27979;&#21644;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;ESA Euclid&#31354;&#38388;&#26395;&#36828;&#38236;&#30340;&#22270;&#20687;&#20013;&#65292;&#26368;&#22810;&#26377;15&#19975;&#39063;&#23567;&#34892;&#26143;&#21487;&#35265;&#65292;&#24182;&#19988;Euclid&#30340;&#20202;&#22120;&#25552;&#20379;&#36825;&#20123;&#29289;&#20307;&#30340;&#22810;&#39057;&#27573;&#21487;&#35265;&#20809;&#21040;&#36817;&#32418;&#22806;&#20809;&#24230;&#27979;&#37327;&#21644;&#26080;&#29421;&#32541;&#20809;&#35889;&#12290;&#22823;&#22810;&#25968;&#23567;&#34892;&#26143;&#23558;&#20986;&#29616;&#22312;&#22270;&#20687;&#20013;&#30340;&#20809;&#36857;&#20013;&#12290;&#30001;&#20110;&#22270;&#20687;&#21644;&#23567;&#34892;&#26143;&#30340;&#25968;&#37327;&#24222;&#22823;&#65292;&#38656;&#35201;&#33258;&#21160;&#21270;&#30340;&#26816;&#27979;&#26041;&#27861;&#12290;&#20197;&#21069;&#26366;&#23581;&#35797;&#20102;&#19968;&#31181;&#22522;&#20110;StreakDet&#36719;&#20214;&#30340;&#38750;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20294;&#23545;&#20110;&#30701;&#26242;&#21644;/&#25110;&#24494;&#24369;&#30340;&#20809;&#36857;&#65292;&#32467;&#26524;&#19981;&#29702;&#24819;&#12290;&#25105;&#20204;&#20915;&#23450;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26469;&#25913;&#21892;Euclid&#22270;&#20687;&#20013;&#30340;&#23567;&#34892;&#26143;&#20809;&#36857;&#26816;&#27979;&#33021;&#21147;&#12290;&#25105;&#20204;&#20351;&#29992;&#27169;&#25311;&#30340;Euclid&#22270;&#20687;&#26500;&#24314;&#12289;&#35757;&#32451;&#21644;&#27979;&#35797;&#20102;&#19968;&#20010;&#19977;&#27493;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#12290;&#39318;&#20808;&#65292;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#22312;&#23436;&#25972;&#30340;&#22270;&#20687;&#20013;&#26816;&#27979;&#20809;&#36857;&#21450;&#20854;&#22352;&#26631;&#65292;&#20197;&#26368;&#22823;&#21270;&#26816;&#27979;&#30340;&#23436;&#25972;&#24615;&#65288;&#21484;&#22238;&#29575;&#65289;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#21512;&#24182;CNN&#22312;&#22810;&#20010;&#37096;&#20998;&#26816;&#27979;&#21040;&#30340;&#38271;&#20809;&#36857;&#30340;&#29255;&#27573;&#12290;&#26368;&#21518;&#65292;&#20351;&#29992;&#26799;&#24230;&#25552;&#21319;&#26041;&#27861;&#23545;&#20809;&#36857;&#36827;&#34892;&#31579;&#36873;&#21644;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Up to 150000 asteroids will be visible in the images of the ESA Euclid space telescope, and the instruments of Euclid offer multiband visual to near-infrared photometry and slitless spectra of these objects. Most asteroids will appear as streaks in the images. Due to the large number of images and asteroids, automated detection methods are needed. A non-machine-learning approach based on the StreakDet software was previously tested, but the results were not optimal for short and/or faint streaks. We set out to improve the capability to detect asteroid streaks in Euclid images by using deep learning.  We built, trained, and tested a three-step machine-learning pipeline with simulated Euclid images. First, a convolutional neural network (CNN) detected streaks and their coordinates in full images, aiming to maximize the completeness (recall) of detections. Then, a recurrent neural network (RNN) merged snippets of long streaks detected in several parts by the CNN. Lastly, gradient-boosted 
&lt;/p&gt;</description></item><item><title>&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#20219;&#21153;&#20013;&#30340;&#29305;&#24449;&#21487;&#20197;&#26497;&#20854;&#20887;&#20313;&#65292;&#20165;&#20351;&#29992;&#26368;&#37325;&#35201;&#30340;&#29305;&#24449;&#32500;&#24230;&#30340;1%&#23601;&#33021;&#22815;&#36798;&#21040;&#20351;&#29992;&#23436;&#25972;&#34920;&#31034;&#26102;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.03843</link><description>&lt;p&gt;
Less is More: &#20851;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#20219;&#21153;&#20013;&#29305;&#24449;&#20887;&#20313;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Less is More: On the Feature Redundancy of Pretrained Models When Transferring to Few-shot Tasks. (arXiv:2310.03843v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03843
&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#20219;&#21153;&#20013;&#30340;&#29305;&#24449;&#21487;&#20197;&#26497;&#20854;&#20887;&#20313;&#65292;&#20165;&#20351;&#29992;&#26368;&#37325;&#35201;&#30340;&#29305;&#24449;&#32500;&#24230;&#30340;1%&#23601;&#33021;&#22815;&#36798;&#21040;&#20351;&#29992;&#23436;&#25972;&#34920;&#31034;&#26102;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#24212;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#30446;&#26631;&#25968;&#25454;&#36827;&#34892;&#32447;&#24615;&#25506;&#27979;&#26469;&#23454;&#29616;&#65292;&#21363;&#23545;&#20174;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#25552;&#21462;&#30340;&#20923;&#32467;&#29305;&#24449;&#36827;&#34892;&#35757;&#32451;&#32447;&#24615;&#20998;&#31867;&#22120;&#12290;&#30001;&#20110;&#39044;&#35757;&#32451;&#21644;&#19979;&#28216;&#25968;&#25454;&#38598;&#20043;&#38388;&#21487;&#33021;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#25105;&#20204;&#21487;&#20197;&#35810;&#38382;&#26159;&#21542;&#25152;&#26377;&#39044;&#35757;&#32451;&#29305;&#24449;&#30340;&#32500;&#24230;&#23545;&#20110;&#32473;&#23450;&#30340;&#19979;&#28216;&#20219;&#21153;&#37117;&#26159;&#26377;&#29992;&#30340;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#32447;&#24615;&#25506;&#27979;&#30340;&#24773;&#20917;&#19979;&#65292;&#24403;&#19979;&#28216;&#25968;&#25454;&#31232;&#32570;&#25110;&#23569;&#26679;&#26412;&#26102;&#65292;&#39044;&#35757;&#32451;&#29305;&#24449;&#21487;&#33021;&#26497;&#20854;&#20887;&#20313;&#12290;&#23545;&#20110;&#19968;&#20123;&#24773;&#20917;&#65292;&#27604;&#22914;5&#31867;1&#26679;&#26412;&#20219;&#21153;&#65292;&#21482;&#20351;&#29992;&#26368;&#37325;&#35201;&#30340;&#29305;&#24449;&#32500;&#24230;&#30340;1%&#23601;&#33021;&#22815;&#36798;&#21040;&#20351;&#29992;&#23436;&#25972;&#34920;&#31034;&#26102;&#30340;&#24615;&#33021;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#22823;&#37096;&#20998;&#29305;&#24449;&#21482;&#22312;&#23569;&#26679;&#26412;&#35774;&#32622;&#19979;&#26159;&#20887;&#20313;&#30340;&#65292;&#22312;&#26679;&#26412;&#25968;&#22686;&#21152;&#26102;&#36880;&#28176;&#21464;&#24471;&#26377;&#29992;&#65292;&#36825;&#34920;&#26126;&#29305;&#24449;&#20887;&#20313;&#21487;&#33021;&#26159;&#34920;&#24449;&#23569;&#26679;&#26412;&#36716;&#31227;&#38382;&#39064;&#30340;&#20851;&#38190;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#29702;&#35770;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Transferring a pretrained model to a downstream task can be as easy as conducting linear probing with target data, that is, training a linear classifier upon frozen features extracted from the pretrained model. As there may exist significant gaps between pretraining and downstream datasets, one may ask whether all dimensions of the pretrained features are useful for a given downstream task. We show that, for linear probing, the pretrained features can be extremely redundant when the downstream data is scarce, or few-shot. For some cases such as 5-way 1-shot tasks, using only 1\% of the most important feature dimensions is able to recover the performance achieved by using the full representation. Interestingly, most dimensions are redundant only under few-shot settings and gradually become useful when the number of shots increases, suggesting that feature redundancy may be the key to characterizing the "few-shot" nature of few-shot transfer problems. We give a theoretical understanding 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LaKERMap&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;OM&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#19978;&#19979;&#25991;&#21644;&#32467;&#26500;&#20449;&#24687;&#25972;&#21512;&#21040;transformer&#20013;&#65292;&#25429;&#25417;&#22810;&#20010;&#32467;&#26500;&#21270;&#19978;&#19979;&#25991;&#65292;&#24182;&#24212;&#29992;&#20110;&#26412;&#20307;&#21305;&#37197;&#20013;&#12290;</title><link>http://arxiv.org/abs/2310.03840</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#21270;&#30340;&#32467;&#26500;&#21270;&#33258;&#30417;&#30563;&#23398;&#20064;&#29992;&#20110;&#26412;&#20307;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Contextualized Structural Self-supervised Learning for Ontology Matching. (arXiv:2310.03840v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03840
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LaKERMap&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;OM&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#19978;&#19979;&#25991;&#21644;&#32467;&#26500;&#20449;&#24687;&#25972;&#21512;&#21040;transformer&#20013;&#65292;&#25429;&#25417;&#22810;&#20010;&#32467;&#26500;&#21270;&#19978;&#19979;&#25991;&#65292;&#24182;&#24212;&#29992;&#20110;&#26412;&#20307;&#21305;&#37197;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#20307;&#21305;&#37197;&#65288;OM&#65289;&#28041;&#21450;&#22312;&#20004;&#20010;&#25110;&#22810;&#20010;&#30693;&#35782;&#22270;&#20013;&#35782;&#21035;&#27010;&#24565;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#24182;&#20316;&#20026;&#25972;&#21512;&#26469;&#33258;&#21508;&#31181;&#26469;&#28304;&#30340;&#30693;&#35782;&#22270;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#26368;&#36817;&#28145;&#24230;OM&#27169;&#22411;&#30340;&#36827;&#23637;&#24050;&#32463;&#21033;&#29992;&#20102;&#22522;&#20110;transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#21644;&#30693;&#35782;&#22270;&#23884;&#20837;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;OM&#27169;&#22411;&#20173;&#28982;&#38754;&#20020;&#30528;&#25345;&#32493;&#30340;&#25361;&#25112;&#65292;&#22914;&#32570;&#20047;&#21442;&#32771;&#23545;&#40784;&#12289;&#36816;&#34892;&#26102;&#24310;&#36831;&#21644;&#26410;&#24320;&#21457;&#30340;&#20869;&#37096;&#19981;&#21516;&#22270;&#32467;&#26500;&#31561;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;OM&#26694;&#26550;&#65292;&#21517;&#20026;LaKERMap&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#27010;&#24565;&#30340;&#19978;&#19979;&#25991;&#21644;&#32467;&#26500;&#20449;&#24687;&#65292;&#23558;&#38544;&#24335;&#30693;&#35782;&#25972;&#21512;&#21040;transformer&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#37319;&#29992;&#19981;&#21516;&#30340;&#35757;&#32451;&#30446;&#26631;&#25429;&#25417;&#22810;&#20010;&#32467;&#26500;&#21270;&#19978;&#19979;&#25991;&#65292;&#21253;&#25324;&#23616;&#37096;&#21644;&#20840;&#23616;&#20132;&#20114;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;Bio-ML&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ontology matching (OM) entails the identification of semantic relationships between concepts within two or more knowledge graphs (KGs) and serves as a critical step in integrating KGs from various sources. Recent advancements in deep OM models have harnessed the power of transformer-based language models and the advantages of knowledge graph embedding. Nevertheless, these OM models still face persistent challenges, such as a lack of reference alignments, runtime latency, and unexplored different graph structures within an end-to-end framework. In this study, we introduce a novel self-supervised learning OM framework with input ontologies, called LaKERMap. This framework capitalizes on the contextual and structural information of concepts by integrating implicit knowledge into transformers. Specifically, we aim to capture multiple structural contexts, encompassing both local and global interactions, by employing distinct training objectives. To assess our methods, we utilize the Bio-ML 
&lt;/p&gt;</description></item><item><title>Chameleon&#26159;&#19968;&#31181;&#26032;&#30340;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#26041;&#27861;&#65292;&#21033;&#29992;&#33258;&#36866;&#24212;&#25968;&#25454;&#27745;&#26579;&#31574;&#30053;&#21644;&#39640;&#25928;&#30340;&#26597;&#35810;&#36873;&#25321;&#26041;&#27861;&#65292;&#21487;&#22312;&#26631;&#31614;&#21807;&#19968;&#35774;&#32622;&#20013;&#25552;&#39640;&#25104;&#21592;&#27844;&#38706;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.03838</link><description>&lt;p&gt;
Chameleon: &#20351;&#29992;&#33258;&#36866;&#24212;&#27745;&#26579;&#22686;&#24378;&#26631;&#31614;&#21807;&#19968;&#25104;&#21592;&#27844;&#38706;
&lt;/p&gt;
&lt;p&gt;
Chameleon: Increasing Label-Only Membership Leakage with Adaptive Poisoning. (arXiv:2310.03838v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03838
&lt;/p&gt;
&lt;p&gt;
Chameleon&#26159;&#19968;&#31181;&#26032;&#30340;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#26041;&#27861;&#65292;&#21033;&#29992;&#33258;&#36866;&#24212;&#25968;&#25454;&#27745;&#26579;&#31574;&#30053;&#21644;&#39640;&#25928;&#30340;&#26597;&#35810;&#36873;&#25321;&#26041;&#27861;&#65292;&#21487;&#22312;&#26631;&#31614;&#21807;&#19968;&#35774;&#32622;&#20013;&#25552;&#39640;&#25104;&#21592;&#27844;&#38706;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20247;&#22810;&#20851;&#38190;&#24212;&#29992;&#20013;&#24341;&#20837;&#26426;&#22120;&#23398;&#20064;(ML)&#21518;&#65292;&#20010;&#20154;&#25968;&#25454;&#25552;&#20379;&#32773;&#38754;&#20020;&#22810;&#31181;&#38544;&#31169;&#38382;&#39064;&#12290;&#20854;&#20013;&#20043;&#19968;&#26159;&#25104;&#21592;&#25512;&#29702;(MI)&#65292;&#25915;&#20987;&#32773;&#35797;&#22270;&#30830;&#23450;&#29305;&#23450;&#25968;&#25454;&#26679;&#26412;&#26159;&#21542;&#21253;&#21547;&#22312;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#12290;&#30446;&#21069;&#30340;MI&#25915;&#20987;&#21033;&#29992;&#27169;&#22411;&#30340;&#39044;&#27979;&#32622;&#20449;&#24230;&#20998;&#25968;&#26469;&#25104;&#21151;&#36827;&#34892;&#25104;&#21592;&#25512;&#29702;&#65292;&#24182;&#36890;&#36807;&#25968;&#25454;&#27745;&#26579;&#36827;&#19968;&#27493;&#25552;&#39640;&#25928;&#26524;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#36739;&#23569;&#25506;&#32034;&#19988;&#26356;&#29616;&#23454;&#30340;&#20165;&#26631;&#31614;&#35774;&#32622;&#20013;&#65292;&#27169;&#22411;&#20165;&#22312;&#26597;&#35810;&#26679;&#26412;&#19978;&#25552;&#20379;&#39044;&#27979;&#30340;&#26631;&#31614;&#12290;&#25105;&#20204;&#34920;&#26126;&#29616;&#26377;&#30340;&#20165;&#26631;&#31614;MI&#25915;&#20987;&#22312;&#20302;&#35823;&#25253;&#29575;(FPR)&#24773;&#20917;&#19979;&#26080;&#27861;&#26377;&#25928;&#25512;&#26029;&#25104;&#21592;&#36523;&#20221;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#26041;&#27861;Chameleon&#65292;&#23427;&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#25968;&#25454;&#27745;&#26579;&#31574;&#30053;&#21644;&#39640;&#25928;&#30340;&#26597;&#35810;&#36873;&#25321;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The integration of machine learning (ML) in numerous critical applications introduces a range of privacy concerns for individuals who provide their datasets for model training. One such privacy risk is Membership Inference (MI), in which an attacker seeks to determine whether a particular data sample was included in the training dataset of a model. Current state-of-the-art MI attacks capitalize on access to the model's predicted confidence scores to successfully perform membership inference, and employ data poisoning to further enhance their effectiveness. In this work, we focus on the less explored and more realistic label-only setting, where the model provides only the predicted label on a queried sample. We show that existing label-only MI attacks are ineffective at inferring membership in the low False Positive Rate (FPR) regime. To address this challenge, we propose a new attack Chameleon that leverages a novel adaptive data poisoning strategy and an efficient query selection meth
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#34920;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#25439;&#22833;&#20989;&#25968;&#26469;&#23558;&#26410;&#26631;&#35760;&#25968;&#25454;&#25237;&#24433;&#20026;&#20004;&#20010;&#31751;&#65292;&#20197;&#27169;&#25311;&#20302;&#32500;&#24773;&#20917;&#19979;&#30340;PU&#23398;&#20064;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#21487;&#20197;&#22686;&#24378;&#26410;&#26631;&#35760;&#25968;&#25454;&#31751;&#20043;&#38388;&#30340;&#20998;&#31163;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.03833</link><description>&lt;p&gt;
&#23398;&#20064;PU&#23398;&#20064;&#30340;&#35299;&#32544;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning A Disentangling Representation For PU Learning. (arXiv:2310.03833v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03833
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#34920;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#25439;&#22833;&#20989;&#25968;&#26469;&#23558;&#26410;&#26631;&#35760;&#25968;&#25454;&#25237;&#24433;&#20026;&#20004;&#20010;&#31751;&#65292;&#20197;&#27169;&#25311;&#20302;&#32500;&#24773;&#20917;&#19979;&#30340;PU&#23398;&#20064;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#21487;&#20197;&#22686;&#24378;&#26410;&#26631;&#35760;&#25968;&#25454;&#31751;&#20043;&#38388;&#30340;&#20998;&#31163;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#23398;&#20064;&#19968;&#20010;&#20108;&#20803;&#65288;&#27491; vs. &#36127;&#65289;&#20998;&#31867;&#22120;&#30340;&#38382;&#39064;&#65292;&#32473;&#23450;&#20102;&#24120;&#24120;&#34987;&#31216;&#20026;PU&#23398;&#20064;&#30340;&#27491;&#26679;&#26412;&#21644;&#26410;&#26631;&#35760;&#26679;&#26412;&#25968;&#25454;&#12290;&#34429;&#28982;&#22312;&#20302;&#32500;&#24230;&#24773;&#20917;&#19979;&#21487;&#20197;&#20351;&#29992;&#22522;&#26412;&#30340;&#25216;&#26415;&#65288;&#22914;&#32858;&#31867;&#12289;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#25110;&#27491;&#26679;&#26412;&#23494;&#24230;&#20272;&#35745;&#65289;&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#65292;&#20294;&#26159;&#38543;&#30528;&#25968;&#25454;&#20998;&#24067;&#30340;&#22797;&#26434;&#24615;&#36880;&#28176;&#22686;&#21152;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#25928;&#26524;&#36880;&#28176;&#19979;&#38477;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25439;&#22833;&#20989;&#25968;&#26469;&#23398;&#20064;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23558;&#26410;&#26631;&#35760;&#25968;&#25454;&#25237;&#24433;&#21040;&#20004;&#20010;&#65288;&#27491;&#21644;&#36127;&#65289;&#31751;&#20013;&#65292;&#21487;&#20197;&#20351;&#29992;&#31616;&#21333;&#30340;&#32858;&#31867;&#25216;&#26415;&#36731;&#26494;&#35782;&#21035;&#65292;&#20174;&#32780;&#26377;&#25928;&#27169;&#25311;&#20302;&#32500;&#24230;&#24773;&#20917;&#19979;&#35266;&#23519;&#21040;&#30340;&#29616;&#35937;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#21521;&#37327;&#37327;&#21270;&#25216;&#26415;&#26469;&#25918;&#22823;&#23398;&#20064;&#21040;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#31751;&#20043;&#38388;&#30340;&#20998;&#31163;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#30340;PU&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#8230;
&lt;/p&gt;
&lt;p&gt;
In this paper, we address the problem of learning a binary (positive vs. negative) classifier given Positive and Unlabeled data commonly referred to as PU learning. Although rudimentary techniques like clustering, out-of-distribution detection, or positive density estimation can be used to solve the problem in low-dimensional settings, their efficacy progressively deteriorates with higher dimensions due to the increasing complexities in the data distribution. In this paper we propose to learn a neural network-based data representation using a loss function that can be used to project the unlabeled data into two (positive and negative) clusters that can be easily identified using simple clustering techniques, effectively emulating the phenomenon observed in low-dimensional settings. We adopt a vector quantization technique for the learned representations to amplify the separation between the learned unlabeled data clusters. We conduct experiments on simulated PU data that demonstrate th
&lt;/p&gt;</description></item><item><title>ECAvg&#26159;&#19968;&#31181;&#36793;&#32536;-&#20113;&#21327;&#21516;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24179;&#22343;&#26435;&#37325;&#23454;&#29616;&#36793;&#32536;&#35774;&#22791;&#21644;&#20113;&#26381;&#21153;&#22120;&#20043;&#38388;&#30340;&#27169;&#22411;&#21327;&#20316;&#19982;&#26356;&#26032;&#12290;</title><link>http://arxiv.org/abs/2310.03823</link><description>&lt;p&gt;
ECAvg&#65306;&#19968;&#31181;&#20351;&#29992;&#24179;&#22343;&#26435;&#37325;&#30340;&#36793;&#32536;-&#20113;&#21327;&#21516;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ECAvg: An Edge-Cloud Collaborative Learning Approach using Averaged Weights. (arXiv:2310.03823v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03823
&lt;/p&gt;
&lt;p&gt;
ECAvg&#26159;&#19968;&#31181;&#36793;&#32536;-&#20113;&#21327;&#21516;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24179;&#22343;&#26435;&#37325;&#23454;&#29616;&#36793;&#32536;&#35774;&#22791;&#21644;&#20113;&#26381;&#21153;&#22120;&#20043;&#38388;&#30340;&#27169;&#22411;&#21327;&#20316;&#19982;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36793;&#32536;&#35774;&#22791;&#19982;&#20113;&#30340;&#32467;&#21512;&#25552;&#20379;&#20102;&#20004;&#31867;&#35774;&#22791;&#20043;&#38388;&#30340;&#21327;&#20316;&#20851;&#31995;&#65292;&#24444;&#27492;&#20043;&#38388;&#20114;&#34917;&#20248;&#21183;&#12290;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#21487;&#20197;&#36890;&#36807;&#23558;&#35745;&#31639;&#23494;&#38598;&#22411;&#20219;&#21153;&#21368;&#36733;&#21040;&#26381;&#21153;&#22120;&#19978;&#65292;&#20174;&#26381;&#21153;&#22120;&#25552;&#20379;&#30340;&#20016;&#23500;&#35745;&#31639;&#33021;&#21147;&#20013;&#33719;&#30410;&#12290;&#21516;&#26102;&#65292;&#36793;&#32536;&#35774;&#22791;&#21487;&#20197;&#21033;&#29992;&#20854;&#25509;&#36817;&#25968;&#25454;&#28304;&#30340;&#20248;&#21183;&#65292;&#22312;&#25968;&#25454;&#19978;&#25191;&#34892;&#36739;&#23569;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ECAvg&#30340;&#21327;&#21516;&#36793;&#32536;-&#20113;&#33539;&#24335;&#65292;&#22312;&#35813;&#33539;&#24335;&#20013;&#65292;&#36793;&#32536;&#35774;&#22791;&#22312;&#21508;&#33258;&#30340;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#26412;&#22320;&#27169;&#22411;&#65292;&#24182;&#23558;&#27169;&#22411;&#36716;&#31227;&#21040;&#26381;&#21153;&#22120;&#36827;&#34892;&#24494;&#35843;&#12290;&#26381;&#21153;&#22120;&#23558;&#39044;&#35757;&#32451;&#30340;&#26435;&#37325;&#24179;&#22343;&#20026;&#19968;&#20010;&#20840;&#23616;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#26469;&#33258;&#19981;&#21516;&#36793;&#32536;&#35774;&#22791;&#30340;&#32452;&#21512;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#20840;&#23616;&#27169;&#22411;&#30340;&#26435;&#37325;&#26356;&#26032;&#26412;&#22320;&#65288;&#36793;&#32536;&#65289;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;MobileNetV2&#23454;&#29616;&#20102;CIFAR-10&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#20351;&#29992;ResN&#65288;&#24453;&#23436;&#25104;&#65289;
&lt;/p&gt;
&lt;p&gt;
The use of edge devices together with cloud provides a collaborative relationship between both classes of devices where one complements the shortcomings of the other. Resource-constraint edge devices can benefit from the abundant computing power provided by servers by offloading computationally intensive tasks to the server. Meanwhile, edge devices can leverage their close proximity to the data source to perform less computationally intensive tasks on the data. In this paper, we propose a collaborative edge-cloud paradigm called ECAvg in which edge devices pre-train local models on their respective datasets and transfer the models to the server for fine-tuning. The server averages the pre-trained weights into a global model, which is fine-tuned on the combined data from the various edge devices. The local (edge) models are then updated with the weights of the global (server) model. We implement a CIFAR-10 classification task using MobileNetV2, a CIFAR-100 classification task using ResN
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#30828;&#27880;&#24847;&#21147;&#30340;Transformer&#32534;&#30721;&#22120;&#21487;&#20197;&#25509;&#21463;&#30340;&#36923;&#36753;&#35821;&#35328;&#30340;&#38382;&#39064;&#65292;&#21457;&#29616;UHAT&#32534;&#30721;&#22120;&#21482;&#33021;&#35782;&#21035;${\sf AC}^0$&#20013;&#30340;&#19968;&#37096;&#20998;&#35821;&#35328;&#65292;&#32780;AHAT&#32534;&#30721;&#22120;&#21487;&#20197;&#35782;&#21035;&#26356;&#20016;&#23500;&#30340;&#35821;&#35328;&#12290;</title><link>http://arxiv.org/abs/2310.03817</link><description>&lt;p&gt;
&#20351;&#29992;&#30828;&#27880;&#24847;&#21147;&#30340;Transformer&#32534;&#30721;&#22120;&#21487;&#20197;&#25509;&#21463;&#30340;&#36923;&#36753;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
Logical Languages Accepted by Transformer Encoders with Hard Attention. (arXiv:2310.03817v1 [cs.FL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#30828;&#27880;&#24847;&#21147;&#30340;Transformer&#32534;&#30721;&#22120;&#21487;&#20197;&#25509;&#21463;&#30340;&#36923;&#36753;&#35821;&#35328;&#30340;&#38382;&#39064;&#65292;&#21457;&#29616;UHAT&#32534;&#30721;&#22120;&#21482;&#33021;&#35782;&#21035;${\sf AC}^0$&#20013;&#30340;&#19968;&#37096;&#20998;&#35821;&#35328;&#65292;&#32780;AHAT&#32534;&#30721;&#22120;&#21487;&#20197;&#35782;&#21035;&#26356;&#20016;&#23500;&#30340;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#21487;&#20197;&#34987;Transformer&#32534;&#30721;&#22120;&#35782;&#21035;&#30340;&#24418;&#24335;&#35821;&#35328;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#37325;&#28857;&#30740;&#31350;&#20102;&#20004;&#31181;&#33258;&#27880;&#24847;&#26426;&#21046;&#65306;(1)UHAT&#65288;&#21807;&#19968;&#30828;&#27880;&#24847;&#21147;Transformer&#65289;&#21644;(2)AHAT&#65288;&#24179;&#22343;&#30828;&#27880;&#24847;&#21147;Transformer&#65289;&#12290;&#24050;&#30693;UHAT&#32534;&#30721;&#22120;&#21482;&#33021;&#35782;&#21035;&#30005;&#36335;&#22797;&#26434;&#24230;&#31867;${\sf AC}^0$&#20013;&#30340;&#35821;&#35328;&#65292;&#21363;&#30001;&#22810;&#39033;&#24335;&#22823;&#23567;&#21644;&#28145;&#24230;&#26377;&#30028;&#30340;&#24067;&#23572;&#30005;&#36335;&#20197;&#21450;&#26080;&#38480;&#25159;&#20837;&#30340;&#24067;&#23572;&#38376;&#25509;&#21463;&#30340;&#35821;&#35328;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;AHAT&#32534;&#30721;&#22120;&#21487;&#20197;&#35782;&#21035;${\sf AC}^0$&#20043;&#22806;&#30340;&#35821;&#35328;&#65292;&#20294;&#23427;&#20204;&#30340;&#34920;&#36798;&#33021;&#21147;&#20173;&#28982;&#22312;&#26356;&#22823;&#30340;&#30005;&#36335;&#22797;&#26434;&#24230;&#31867;${\sf TC}^0$&#20869;&#65292;&#21363;&#30001;&#22810;&#25968;&#38376;&#25193;&#23637;&#30340;${\sf AC}^0$&#30005;&#36335;&#12290;&#25105;&#20204;&#39318;&#20808;&#32473;&#20986;&#20102;&#19968;&#20010;&#36127;&#38754;&#32467;&#26524;&#65292;&#21363;&#23384;&#22312;&#19968;&#20010;${\sf AC}^0$&#35821;&#35328;&#65292;&#26080;&#27861;&#34987;UHAT&#32534;&#30721;&#22120;&#35782;&#21035;&#12290;&#22312;&#31215;&#26497;&#30340;&#19968;&#38754;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;UHAT&#32534;&#30721;&#22120;&#21487;&#20197;&#35782;&#21035;${\sf AC}^0$-&#35821;&#35328;&#30340;&#19968;&#20010;&#20016;&#23500;&#29255;&#27573;&#65292;&#21363;&#22312;&#19968;&#38454;&#36923;&#36753;&#20013;&#20351;&#29992;&#20219;&#24847;&#19968;&#20803;&#25968;&#37327;&#35859;&#35789;&#23450;&#20041;&#30340;&#25152;&#26377;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
We contribute to the study of formal languages that can be recognized by transformer encoders. We focus on two self-attention mechanisms: (1) UHAT (Unique Hard Attention Transformers) and (2) AHAT (Average Hard Attention Transformers). UHAT encoders are known to recognize only languages inside the circuit complexity class ${\sf AC}^0$, i.e., accepted by a family of poly-sized and depth-bounded boolean circuits with unbounded fan-ins. On the other hand, AHAT encoders can recognize languages outside ${\sf AC}^0$), but their expressive power still lies within the bigger circuit complexity class ${\sf TC}^0$, i.e., ${\sf AC}^0$-circuits extended by majority gates. We first show a negative result that there is an ${\sf AC}^0$-language that cannot be recognized by an UHAT encoder. On the positive side, we show that UHAT encoders can recognize a rich fragment of ${\sf AC}^0$-languages, namely, all languages definable in first-order logic with arbitrary unary numerical predicates. This logic, 
&lt;/p&gt;</description></item><item><title>Fishnets&#26159;&#19968;&#31181;&#29992;&#20110;&#23398;&#20064;&#20449;&#24687;&#26368;&#20248;&#30340;&#38598;&#21512;&#21644;&#22270;&#32858;&#21512;&#30340;&#26041;&#27861;&#65292;&#22312;&#35268;&#27169;&#19978;&#21487;&#20197;&#20248;&#21270;&#21040;&#20219;&#24847;&#25968;&#37327;&#30340;&#25968;&#25454;&#23545;&#35937;&#65292;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#33021;&#22815;&#39281;&#21644;&#36125;&#21494;&#26031;&#20449;&#24687;&#20869;&#23481;&#65292;&#24182;&#21487;&#29992;&#20110;GNNs&#20013;&#30340;&#28040;&#24687;&#20256;&#36882;&#12290;</title><link>http://arxiv.org/abs/2310.03812</link><description>&lt;p&gt;
&#40060;&#32593;&#65306;&#20449;&#24687;&#26368;&#20248;&#65292;&#21487;&#25193;&#23637;&#30340;&#38598;&#21512;&#21644;&#22270;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
Fishnets: Information-Optimal, Scalable Aggregation for Sets and Graphs. (arXiv:2310.03812v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03812
&lt;/p&gt;
&lt;p&gt;
Fishnets&#26159;&#19968;&#31181;&#29992;&#20110;&#23398;&#20064;&#20449;&#24687;&#26368;&#20248;&#30340;&#38598;&#21512;&#21644;&#22270;&#32858;&#21512;&#30340;&#26041;&#27861;&#65292;&#22312;&#35268;&#27169;&#19978;&#21487;&#20197;&#20248;&#21270;&#21040;&#20219;&#24847;&#25968;&#37327;&#30340;&#25968;&#25454;&#23545;&#35937;&#65292;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#33021;&#22815;&#39281;&#21644;&#36125;&#21494;&#26031;&#20449;&#24687;&#20869;&#23481;&#65292;&#24182;&#21487;&#29992;&#20110;GNNs&#20013;&#30340;&#28040;&#24687;&#20256;&#36882;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#38598;&#21512;&#30340;&#23398;&#20064;&#26159;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#21644;&#32593;&#32476;&#31185;&#23398;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#21450;&#20854;&#19981;&#21547;&#36793;&#30340;&#23545;&#24212;&#29289;Deepsets&#22312;&#19981;&#35268;&#21017;&#21644;&#25299;&#25169;&#22797;&#26434;&#30340;&#25968;&#25454;&#38598;&#19978;&#34987;&#35777;&#26126;&#38750;&#24120;&#26377;&#29992;&#12290;&#20026;&#20102;&#23398;&#20064;&#38598;&#21512;&#25104;&#21592;&#30340;&#20449;&#24687;&#20016;&#23500;&#30340;&#23884;&#20837;&#65292;&#20851;&#38190;&#26159;&#25351;&#23450;&#19968;&#20010;&#32858;&#21512;&#20989;&#25968;&#65292;&#36890;&#24120;&#26159;&#27714;&#21644;&#12289;&#26368;&#22823;&#20540;&#25110;&#22343;&#20540;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Fishnets&#65292;&#19968;&#31181;&#29992;&#20110;&#23398;&#20064;&#38598;&#21512;&#25968;&#25454;&#21644;&#22270;&#32858;&#21512;&#30340;&#20449;&#24687;&#26368;&#20248;&#23884;&#20837;&#31574;&#30053;&#65292;&#36866;&#29992;&#20110;&#36125;&#21494;&#26031;&#25512;&#29702;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#65306;i&#65289;Fishnets&#31070;&#32463;&#25688;&#35201;&#21487;&#20197;&#26368;&#20248;&#22320;&#25193;&#23637;&#21040;&#20219;&#24847;&#25968;&#37327;&#30340;&#25968;&#25454;&#23545;&#35937;&#65307;ii&#65289;Fishnets&#32858;&#21512;&#23545;&#25968;&#25454;&#20998;&#24067;&#30340;&#25913;&#21464;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#32780;&#26631;&#20934;&#30340;Deepsets&#19981;&#20855;&#22791;&#36825;&#31181;&#29305;&#24615;&#65307;iii&#65289;Fishnets&#39281;&#21644;&#36125;&#21494;&#26031;&#20449;&#24687;&#20869;&#23481;&#65292;&#24182;&#25193;&#23637;&#21040;MCMC&#25216;&#26415;&#22833;&#36133;&#30340;&#39046;&#22495;&#65307;iv&#65289;Fishnets&#21487;&#20197;&#20316;&#20026;GNN&#20013;&#30340;&#19968;&#20010;&#25554;&#20837;&#24335;&#32858;&#21512;&#26041;&#26696;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#37319;&#29992;Fishnets&#32858;&#21512;&#26041;&#26696;&#36827;&#34892;&#28040;&#24687;&#20256;&#36882;&#65292;GNNs&#21487;&#20197;&#23454;&#29616; &#36798;&#21040;
&lt;/p&gt;
&lt;p&gt;
Set-based learning is an essential component of modern deep learning and network science. Graph Neural Networks (GNNs) and their edge-free counterparts Deepsets have proven remarkably useful on ragged and topologically challenging datasets. The key to learning informative embeddings for set members is a specified aggregation function, usually a sum, max, or mean. We propose Fishnets, an aggregation strategy for learning information-optimal embeddings for sets of data for both Bayesian inference and graph aggregation. We demonstrate that i) Fishnets neural summaries can be scaled optimally to an arbitrary number of data objects, ii) Fishnets aggregations are robust to changes in data distribution, unlike standard deepsets, iii) Fishnets saturate Bayesian information content and extend to regimes where MCMC techniques fail and iv) Fishnets can be used as a drop-in aggregation scheme within GNNs. We show that by adopting a Fishnets aggregation scheme for message passing, GNNs can achieve 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#33258;&#36866;&#24212;&#26680;&#26041;&#27861;&#24212;&#29992;&#20110;&#20004;&#20010;&#24072;&#29983;&#27169;&#22411;&#65292;&#39044;&#27979;&#20102;&#29305;&#24449;&#23398;&#20064;&#21644; Grokking &#30340;&#24615;&#36136;&#65292;&#24182;&#23637;&#31034;&#20102; Grokking &#19982;&#30456;&#21464;&#29702;&#35770;&#20043;&#38388;&#30340;&#26144;&#23556;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.03789</link><description>&lt;p&gt;
&#22909;&#34920;&#31034;&#30340;&#28082;&#28404;&#65306;&#22312;&#20004;&#23618;&#32593;&#32476;&#20013; grokking &#20316;&#20026;&#19968;&#38454;&#30456;&#21464;
&lt;/p&gt;
&lt;p&gt;
Droplets of Good Representations: Grokking as a First Order Phase Transition in Two Layer Networks. (arXiv:2310.03789v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03789
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#33258;&#36866;&#24212;&#26680;&#26041;&#27861;&#24212;&#29992;&#20110;&#20004;&#20010;&#24072;&#29983;&#27169;&#22411;&#65292;&#39044;&#27979;&#20102;&#29305;&#24449;&#23398;&#20064;&#21644; Grokking &#30340;&#24615;&#36136;&#65292;&#24182;&#23637;&#31034;&#20102; Grokking &#19982;&#30456;&#21464;&#29702;&#35770;&#20043;&#38388;&#30340;&#26144;&#23556;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476; (DNN) &#30340;&#19968;&#20010;&#20851;&#38190;&#29305;&#24615;&#26159;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#33021;&#22815;&#23398;&#20064;&#26032;&#30340;&#29305;&#24449;&#12290;&#36825;&#31181;&#28145;&#24230;&#23398;&#20064;&#30340;&#26377;&#36259;&#26041;&#38754;&#22312;&#26368;&#36817;&#25253;&#36947;&#30340; Grokking &#29616;&#35937;&#20013;&#34920;&#29616;&#24471;&#26368;&#20026;&#26126;&#26174;&#12290;&#34429;&#28982;&#20027;&#35201;&#20307;&#29616;&#20026;&#27979;&#35797;&#20934;&#30830;&#24615;&#30340;&#31361;&#21464;&#22686;&#21152;&#65292;&#20294; Grokking &#20063;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#36229;&#36234;&#25042;&#24816;&#23398;&#20064;/&#39640;&#26031;&#36807;&#31243; (GP) &#30340;&#29616;&#35937;&#65292;&#28041;&#21450;&#29305;&#24449;&#23398;&#20064;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23558;&#29305;&#24449;&#23398;&#20064;&#29702;&#35770;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#33258;&#36866;&#24212;&#26680;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#20855;&#26377;&#31435;&#26041;&#22810;&#39033;&#24335;&#21644;&#27169;&#21152;&#27861;&#25945;&#24072;&#30340;&#20004;&#20010;&#24072;&#29983;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#36825;&#20123;&#27169;&#22411;&#19978;&#25552;&#20379;&#20102;&#20851;&#20110;&#29305;&#24449;&#23398;&#20064;&#21644; Grokking &#24615;&#36136;&#30340;&#20998;&#26512;&#39044;&#27979;&#65292;&#24182;&#23637;&#31034;&#20102; Grokking &#19982;&#30456;&#21464;&#29702;&#35770;&#20043;&#38388;&#30340;&#26144;&#23556;&#20851;&#31995;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312; Grokking &#20043;&#21518;&#65292;DNN &#30340;&#29366;&#24577;&#31867;&#20284;&#20110;&#19968;&#38454;&#30456;&#21464;&#21518;&#30340;&#28151;&#21512;&#30456;&#12290;&#22312;&#36825;&#20010;&#28151;&#21512;&#30456;&#20013;&#65292;DNN &#29983;&#25104;&#20102;&#19982;&#20043;&#21069;&#26126;&#26174;&#19981;&#21516;&#30340;&#25945;&#24072;&#30340;&#26377;&#29992;&#20869;&#37096;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key property of deep neural networks (DNNs) is their ability to learn new features during training. This intriguing aspect of deep learning stands out most clearly in recently reported Grokking phenomena. While mainly reflected as a sudden increase in test accuracy, Grokking is also believed to be a beyond lazy-learning/Gaussian Process (GP) phenomenon involving feature learning. Here we apply a recent development in the theory of feature learning, the adaptive kernel approach, to two teacher-student models with cubic-polynomial and modular addition teachers. We provide analytical predictions on feature learning and Grokking properties of these models and demonstrate a mapping between Grokking and the theory of phase transitions. We show that after Grokking, the state of the DNN is analogous to the mixed phase following a first-order phase transition. In this mixed phase, the DNN generates useful internal representations of the teacher that are sharply distinct from those before the 
&lt;/p&gt;</description></item><item><title>HandMeThat&#26159;&#19968;&#20010;&#32508;&#21512;&#35780;&#20272;&#22522;&#20934;&#65292;&#29992;&#20110;&#22312;&#29289;&#29702;&#21644;&#31038;&#20132;&#29615;&#22659;&#20013;&#29702;&#35299;&#21644;&#36981;&#24490;&#20154;&#31867;&#25351;&#20196;&#12290;&#22312;&#35813;&#35770;&#25991;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;10000&#20010;&#20154;&#26426;&#20132;&#20114;&#22330;&#26223;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#35780;&#20272;&#20102;&#19981;&#21516;&#30340;&#22522;&#20934;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;&#32467;&#26524;&#26174;&#31034;&#31163;&#32447;&#21644;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#35813;&#22522;&#20934;&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#26263;&#31034;&#20102;&#20854;&#20013;&#30340;&#25361;&#25112;&#21644;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2310.03779</link><description>&lt;p&gt;
HandMeThat: &#22312;&#29289;&#29702;&#21644;&#31038;&#20132;&#29615;&#22659;&#20013;&#30340;&#20154;&#26426;&#20132;&#27969;
&lt;/p&gt;
&lt;p&gt;
HandMeThat: Human-Robot Communication in Physical and Social Environments. (arXiv:2310.03779v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03779
&lt;/p&gt;
&lt;p&gt;
HandMeThat&#26159;&#19968;&#20010;&#32508;&#21512;&#35780;&#20272;&#22522;&#20934;&#65292;&#29992;&#20110;&#22312;&#29289;&#29702;&#21644;&#31038;&#20132;&#29615;&#22659;&#20013;&#29702;&#35299;&#21644;&#36981;&#24490;&#20154;&#31867;&#25351;&#20196;&#12290;&#22312;&#35813;&#35770;&#25991;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;10000&#20010;&#20154;&#26426;&#20132;&#20114;&#22330;&#26223;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#35780;&#20272;&#20102;&#19981;&#21516;&#30340;&#22522;&#20934;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;&#32467;&#26524;&#26174;&#31034;&#31163;&#32447;&#21644;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#35813;&#22522;&#20934;&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#26263;&#31034;&#20102;&#20854;&#20013;&#30340;&#25361;&#25112;&#21644;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;HandMeThat&#65292;&#19968;&#20010;&#29992;&#20110;&#29289;&#29702;&#21644;&#31038;&#20132;&#29615;&#22659;&#20013;&#25351;&#20196;&#29702;&#35299;&#21644;&#36981;&#24490;&#30340;&#32508;&#21512;&#35780;&#20272;&#22522;&#20934;&#12290;&#19982;&#20808;&#21069;&#30340;&#25968;&#25454;&#38598;&#20027;&#35201;&#20851;&#27880;&#35821;&#35328;&#20381;&#23384;&#21644;&#35268;&#21010;&#19981;&#21516;&#65292;HandMeThat&#32771;&#34385;&#20102;&#22522;&#20110;&#29289;&#29702;&#65288;&#29289;&#20307;&#29366;&#24577;&#21644;&#20851;&#31995;&#65289;&#21644;&#31038;&#20132;&#65288;&#20154;&#31867;&#34892;&#21160;&#21644;&#30446;&#26631;&#65289;&#20449;&#24687;&#30340;&#21547;&#26377;&#27495;&#20041;&#30340;&#20154;&#31867;&#25351;&#20196;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;HandMeThat&#21253;&#21547;&#20102;10000&#20010;&#20154;&#26426;&#20132;&#20114;&#30340;&#22330;&#26223;&#12290;&#22312;&#27599;&#20010;&#22330;&#26223;&#20013;&#65292;&#26426;&#22120;&#20154;&#39318;&#20808;&#35266;&#23519;&#21040;&#20154;&#31867;&#34892;&#21160;&#30340;&#36712;&#36857;&#20197;&#36798;&#21040;&#20869;&#37096;&#30446;&#26631;&#12290;&#25509;&#19979;&#26469;&#65292;&#26426;&#22120;&#20154;&#25509;&#25910;&#21040;&#20154;&#31867;&#25351;&#20196;&#65292;&#24182;&#26681;&#25454;&#25351;&#20196;&#37319;&#21462;&#34892;&#21160;&#20197;&#23436;&#25104;&#23376;&#30446;&#26631;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#25105;&#20204;&#22522;&#20934;&#27979;&#35797;&#30340;&#25991;&#26412;&#30028;&#38754;&#65292;&#26426;&#22120;&#20154;&#36890;&#36807;&#25991;&#26412;&#21629;&#20196;&#19982;&#34394;&#25311;&#29615;&#22659;&#20132;&#20114;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;HandMeThat&#19978;&#30340;&#20960;&#20010;&#22522;&#20934;&#27169;&#22411;&#65292;&#24182;&#26174;&#31034;&#31163;&#32447;&#21644;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;HandMeThat&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#34920;&#26126;&#20854;&#20013;&#23384;&#22312;&#37325;&#35201;&#30340;&#25361;&#25112;&#21644;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce HandMeThat, a benchmark for a holistic evaluation of instruction understanding and following in physical and social environments. While previous datasets primarily focused on language grounding and planning, HandMeThat considers the resolution of human instructions with ambiguities based on the physical (object states and relations) and social (human actions and goals) information. HandMeThat contains 10,000 episodes of human-robot interactions. In each episode, the robot first observes a trajectory of human actions towards her internal goal. Next, the robot receives a human instruction and should take actions to accomplish the subgoal set through the instruction. In this paper, we present a textual interface for our benchmark, where the robot interacts with a virtual environment through textual commands. We evaluate several baseline models on HandMeThat, and show that both offline and online reinforcement learning algorithms perform poorly on HandMeThat, suggesting signif
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#22686;&#24378;&#27169;&#22411;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;&#23545;&#25239;&#39564;&#35777;&#26469;&#28040;&#38500;&#38750;&#20449;&#24687;&#24615;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#29305;&#24449;&#24037;&#31243;&#25216;&#26415;&#22788;&#29702;&#26377;&#22122;&#22768;&#30340;&#36830;&#32493;&#29305;&#24449;&#21644;&#20855;&#26377;&#22823;&#37327;&#21807;&#19968;&#20540;&#30340;&#20998;&#31867;&#29305;&#24449;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;ACM RecSys Challenge 2023&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.03778</link><description>&lt;p&gt;
&#20351;&#29992;&#23545;&#25239;&#39564;&#35777;&#30340;&#36731;&#37327;&#32423;&#22686;&#24378;&#27169;&#22411;&#36827;&#34892;&#29992;&#25143;&#21709;&#24212;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Lightweight Boosting Models for User Response Prediction Using Adversarial Validation. (arXiv:2310.03778v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03778
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#22686;&#24378;&#27169;&#22411;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;&#23545;&#25239;&#39564;&#35777;&#26469;&#28040;&#38500;&#38750;&#20449;&#24687;&#24615;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#29305;&#24449;&#24037;&#31243;&#25216;&#26415;&#22788;&#29702;&#26377;&#22122;&#22768;&#30340;&#36830;&#32493;&#29305;&#24449;&#21644;&#20855;&#26377;&#22823;&#37327;&#21807;&#19968;&#20540;&#30340;&#20998;&#31867;&#29305;&#24449;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;ACM RecSys Challenge 2023&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ShareChat&#32452;&#32455;&#30340;ACM RecSys Challenge 2023&#26088;&#22312;&#39044;&#27979;&#24212;&#29992;&#34987;&#23433;&#35013;&#30340;&#27010;&#29575;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#23545;&#36825;&#20010;&#25361;&#25112;&#30340;&#36731;&#37327;&#32423;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#23558;&#35813;&#20219;&#21153;&#23450;&#20041;&#20026;&#29992;&#25143;&#21709;&#24212;&#39044;&#27979;&#20219;&#21153;&#12290;&#20026;&#20102;&#24555;&#36895;&#21407;&#22411;&#35774;&#35745;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#25324;&#20197;&#19979;&#27493;&#39588;&#30340;&#36731;&#37327;&#32423;&#35299;&#20915;&#26041;&#26696;&#65306;1&#65289;&#20351;&#29992;&#23545;&#25239;&#39564;&#35777;&#65292;&#26377;&#25928;&#22320;&#20174;&#25968;&#25454;&#38598;&#20013;&#28040;&#38500;&#38750;&#20449;&#24687;&#24615;&#29305;&#24449;&#65307;2&#65289;&#20026;&#20102;&#22788;&#29702;&#26377;&#22122;&#22768;&#30340;&#36830;&#32493;&#29305;&#24449;&#21644;&#20855;&#26377;&#22823;&#37327;&#21807;&#19968;&#20540;&#30340;&#20998;&#31867;&#29305;&#24449;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#29305;&#24449;&#24037;&#31243;&#25216;&#26415;&#65307;3&#65289;&#25105;&#20204;&#21033;&#29992;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#65288;GBDT&#65289;&#30340;&#21331;&#36234;&#24615;&#33021;&#21644;&#21487;&#20280;&#32553;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19968;&#20010;&#21333;&#29420;&#30340;LightGBM&#27169;&#22411;&#65292;&#22312;&#27809;&#26377;&#39069;&#22806;&#30340;&#38598;&#25104;&#30340;&#24773;&#20917;&#19979;&#65292;&#34920;&#29616;&#24471;&#24456;&#22909;&#12290;&#25105;&#20204;&#30340;&#22242;&#38431;&#22312;&#27604;&#36187;&#20013;&#21462;&#24471;&#20102;&#31532;&#20061;&#21517;&#65292;&#26368;&#32456;&#25490;&#34892;&#27036;&#24471;&#20998;&#20026;6.059065&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20195;&#30721;&#21487;&#20197;&#22312;&#36825;&#37324;&#25214;&#21040;&#65306;https://github.com/choco9966/recsys-challenge-2023&#12290;
&lt;/p&gt;
&lt;p&gt;
The ACM RecSys Challenge 2023, organized by ShareChat, aims to predict the probability of the app being installed. This paper describes the lightweight solution to this challenge. We formulate the task as a user response prediction task. For rapid prototyping for the task, we propose a lightweight solution including the following steps: 1) using adversarial validation, we effectively eliminate uninformative features from a dataset; 2) to address noisy continuous features and categorical features with a large number of unique values, we employ feature engineering techniques.; 3) we leverage Gradient Boosted Decision Trees (GBDT) for their exceptional performance and scalability. The experiments show that a single LightGBM model, without additional ensembling, performs quite well. Our team achieved ninth place in the challenge with the final leaderboard score of 6.059065. Code for our approach can be found here: https://github.com/choco9966/recsys-challenge-2023.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20989;&#25968;&#25968;&#25454;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#20989;&#25968;&#25968;&#25454;&#36716;&#21270;&#20026;&#22270;&#20687;&#65292;&#21033;&#29992;&#29305;&#23450;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#36827;&#34892;&#21442;&#25968;&#20272;&#35745;&#21644;&#20989;&#25968;&#24418;&#24335;&#30340;&#20998;&#31867;&#23398;&#20064;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#20272;&#35745;&#20989;&#25968;&#21442;&#25968;&#12289;&#20998;&#31867;&#20989;&#25968;&#24418;&#24335;&#20197;&#21450;&#28151;&#27788;&#25968;&#25454;&#30340;Lyapunov&#25351;&#25968;&#20272;&#35745;&#20013;&#20855;&#26377;&#24456;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.03773</link><description>&lt;p&gt;
&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20989;&#25968;&#25968;&#25454;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Functional data learning using convolutional neural networks. (arXiv:2310.03773v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03773
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20989;&#25968;&#25968;&#25454;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#20989;&#25968;&#25968;&#25454;&#36716;&#21270;&#20026;&#22270;&#20687;&#65292;&#21033;&#29992;&#29305;&#23450;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#36827;&#34892;&#21442;&#25968;&#20272;&#35745;&#21644;&#20989;&#25968;&#24418;&#24335;&#30340;&#20998;&#31867;&#23398;&#20064;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#20272;&#35745;&#20989;&#25968;&#21442;&#25968;&#12289;&#20998;&#31867;&#20989;&#25968;&#24418;&#24335;&#20197;&#21450;&#28151;&#27788;&#25968;&#25454;&#30340;Lyapunov&#25351;&#25968;&#20272;&#35745;&#20013;&#20855;&#26377;&#24456;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#26377;&#22122;&#22768;&#21644;&#26080;&#22122;&#22768;&#30340;&#20989;&#25968;&#25968;&#25454;&#30340;&#22238;&#24402;&#21644;&#20998;&#31867;&#23398;&#20064;&#38382;&#39064;&#20013;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#12290;&#20027;&#35201;&#24605;&#24819;&#26159;&#23558;&#20989;&#25968;&#25968;&#25454;&#36716;&#21270;&#20026;28x28&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#29305;&#23450;&#20294;&#20856;&#22411;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26469;&#36827;&#34892;&#21442;&#25968;&#20272;&#35745;&#21644;&#20989;&#25968;&#24418;&#24335;&#20998;&#31867;&#30340;&#25152;&#26377;&#22238;&#24402;&#32451;&#20064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20123;&#26377;&#22122;&#22768;&#21644;&#26080;&#22122;&#22768;&#30340;&#20989;&#25968;&#26696;&#20363;&#26469;&#23637;&#31034;&#36825;&#31181;&#26032;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#29992;&#23427;&#26469;&#20272;&#35745;&#25351;&#25968;&#22686;&#38271;&#21644;&#34928;&#20943;&#29575;&#12289;&#27491;&#24358;&#21644;&#20313;&#24358;&#20989;&#25968;&#30340;&#24102;&#23485;&#20197;&#21450;&#26354;&#32447;&#23792;&#20540;&#30340;&#24133;&#24230;&#21644;&#23485;&#24230;&#12290;&#25105;&#20204;&#36824;&#29992;&#23427;&#26469;&#20998;&#31867;&#20989;&#25968;&#25968;&#25454;&#30340;&#21333;&#35843;&#24615;&#21644;&#26354;&#29575;&#12289;&#20195;&#25968;&#21644;&#25351;&#25968;&#22686;&#38271;&#12289;&#20197;&#21450;&#20989;&#25968;&#25968;&#25454;&#30340;&#23792;&#20540;&#25968;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23558;&#30456;&#21516;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#20110;&#26377;&#22122;&#22768;&#21644;&#26080;&#22122;&#22768;&#30340;&#28151;&#27788;&#25968;&#25454;&#30340;Lyapunov&#25351;&#25968;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we show how convolutional neural networks (CNN) can be used in regression and classification learning problems of noisy and non-noisy functional data. The main idea is to transform the functional data into a 28 by 28 image. We use a specific but typical architecture of a convolutional neural network to perform all the regression exercises of parameter estimation and functional form classification. First, we use some functional case studies of functional data with and without random noise to showcase the strength of the new method. In particular, we use it to estimate exponential growth and decay rates, the bandwidths of sine and cosine functions, and the magnitudes and widths of curve peaks. We also use it to classify the monotonicity and curvatures of functional data, algebraic versus exponential growth, and the number of peaks of functional data. Second, we apply the same convolutional neural networks to Lyapunov exponent estimation in noisy and non-noisy chaotic data,
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#32034;&#20102;&#20020;&#24202;&#35760;&#24405;&#34920;&#22411;&#30340;&#26367;&#20195;&#29305;&#24449;&#25552;&#21462;&#27969;&#31243;&#65292;&#20351;&#29992;&#22522;&#20110;BERT&#27169;&#22411;&#30340;&#26041;&#27861;&#23558;&#20020;&#24202;&#35760;&#24405;&#36716;&#25442;&#20026;&#25991;&#26723;&#34920;&#31034;&#65292;&#24182;&#20256;&#20837;LSTM&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#26126;&#26174;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#20294;&#25910;&#25947;&#26102;&#38388;&#36739;&#38271;&#19988;&#19981;&#25903;&#25345;&#22686;&#37327;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2310.03772</link><description>&lt;p&gt;
&#25506;&#32034;&#20020;&#24202;&#35760;&#24405;&#34920;&#22411;&#30340;&#26367;&#20195;&#29305;&#24449;&#25552;&#21462;&#27969;&#31243;
&lt;/p&gt;
&lt;p&gt;
Investigating Alternative Feature Extraction Pipelines For Clinical Note Phenotyping. (arXiv:2310.03772v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03772
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#32034;&#20102;&#20020;&#24202;&#35760;&#24405;&#34920;&#22411;&#30340;&#26367;&#20195;&#29305;&#24449;&#25552;&#21462;&#27969;&#31243;&#65292;&#20351;&#29992;&#22522;&#20110;BERT&#27169;&#22411;&#30340;&#26041;&#27861;&#23558;&#20020;&#24202;&#35760;&#24405;&#36716;&#25442;&#20026;&#25991;&#26723;&#34920;&#31034;&#65292;&#24182;&#20256;&#20837;LSTM&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#26126;&#26174;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#20294;&#25910;&#25947;&#26102;&#38388;&#36739;&#38271;&#19988;&#19981;&#25903;&#25345;&#22686;&#37327;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#30103;&#34892;&#19994;&#24120;&#29992;&#30340;&#23454;&#36341;&#26159;&#20351;&#29992;&#20020;&#24202;&#35760;&#24405;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#35814;&#32454;&#30340;&#24739;&#32773;&#35266;&#23519;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#31995;&#32479;&#32463;&#24120;&#19981;&#20197;&#32467;&#26500;&#21270;&#26684;&#24335;&#21253;&#21547;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#65292;&#20351;&#24471;&#24739;&#32773;&#20449;&#24687;&#38590;&#20197;&#33258;&#21160;&#35780;&#20272;&#21644;&#35780;&#20215;&#12290;&#20351;&#29992;&#35745;&#31639;&#31995;&#32479;&#26469;&#25552;&#21462;&#21307;&#23398;&#23646;&#24615;&#20855;&#26377;&#35768;&#22810;&#24212;&#29992;&#65292;&#21253;&#25324;&#23545;&#24739;&#32773;&#30340;&#32437;&#21521;&#20998;&#26512;&#12289;&#39118;&#38505;&#35780;&#20272;&#21644;&#21307;&#38498;&#35780;&#20272;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#26500;&#24314;&#20102;&#25104;&#21151;&#30340;&#34920;&#22411;&#26041;&#27861;&#65306;&#20174;&#20020;&#24202;&#35760;&#24405;&#20013;&#25552;&#21462;&#21307;&#23398;&#23646;&#24615;&#12290;&#22522;&#20110;BERT&#27169;&#22411;&#30340;&#26041;&#27861;&#21487;&#20197;&#23558;&#20020;&#24202;&#35760;&#24405;&#36716;&#25442;&#20026;&#19968;&#31995;&#21015;&#30340;&#34920;&#31034;&#65292;&#28982;&#21518;&#22522;&#20110;&#23427;&#20204;&#30340;CLS&#23884;&#20837;&#23558;&#20854;&#21387;&#32553;&#20026;&#19968;&#20010;&#21333;&#19968;&#30340;&#25991;&#26723;&#34920;&#31034;&#65292;&#24182;&#20256;&#20837;&#19968;&#20010;LSTM&#27169;&#22411;&#12290;&#23613;&#31649;&#36825;&#20010;&#27969;&#31243;&#30456;&#27604;&#20043;&#21069;&#30340;&#32467;&#26524;&#26377;&#26126;&#26174;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#20294;&#23427;&#38656;&#35201;&#24456;&#38271;&#30340;&#25910;&#25947;&#26102;&#38388;&#12290;&#36825;&#20010;&#26041;&#27861;&#20063;&#19981;&#20801;&#35768;&#22686;&#37327;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
A common practice in the medical industry is the use of clinical notes, which consist of detailed patient observations. However, electronic health record systems frequently do not contain these observations in a structured format, rendering patient information challenging to assess and evaluate automatically. Using computational systems for the extraction of medical attributes offers many applications, including longitudinal analysis of patients, risk assessment, and hospital evaluation. Recent work has constructed successful methods for phenotyping: extracting medical attributes from clinical notes. BERT-based models can be used to transform clinical notes into a series of representations, which are then condensed into a single document representation based on their CLS embeddings and passed into an LSTM (Mulyar et al., 2020). Though this pipeline yields a considerable performance improvement over previous results, it requires extensive convergence time. This method also does not allo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28176;&#36827;&#38477;&#38454;&#24314;&#27169;&#26694;&#26550;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#22320;&#20174;&#20808;&#21069;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#20256;&#36882;&#30693;&#35782;&#65292;&#20943;&#23569;&#25968;&#25454;&#38656;&#27714;&#65292;&#24182;&#19988;&#25552;&#39640;&#20102;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#30340;&#23454;&#29992;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22810;&#20010;&#26696;&#20363;&#20013;&#65292;&#20445;&#30041;&#20808;&#21069;&#27169;&#22411;&#30340;&#20449;&#24687;&#24182;&#21033;&#29992;&#20854;&#20013;&#26377;&#20215;&#20540;&#30340;&#37096;&#20998;&#21487;&#20197;&#25552;&#39640;&#24314;&#27169;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.03770</link><description>&lt;p&gt;
&#28176;&#36827;&#38477;&#38454;&#24314;&#27169;&#65306;&#29992;&#36873;&#25321;&#24615;&#30693;&#35782;&#20256;&#36882;&#22686;&#24378;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Progressive reduced order modeling: empowering data-driven modeling with selective knowledge transfer. (arXiv:2310.03770v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03770
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28176;&#36827;&#38477;&#38454;&#24314;&#27169;&#26694;&#26550;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#22320;&#20174;&#20808;&#21069;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#20256;&#36882;&#30693;&#35782;&#65292;&#20943;&#23569;&#25968;&#25454;&#38656;&#27714;&#65292;&#24182;&#19988;&#25552;&#39640;&#20102;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#30340;&#23454;&#29992;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22810;&#20010;&#26696;&#20363;&#20013;&#65292;&#20445;&#30041;&#20808;&#21069;&#27169;&#22411;&#30340;&#20449;&#24687;&#24182;&#21033;&#29992;&#20854;&#20013;&#26377;&#20215;&#20540;&#30340;&#37096;&#20998;&#21487;&#20197;&#25552;&#39640;&#24314;&#27169;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#21487;&#33021;&#38754;&#20020;&#23545;&#25968;&#25454;&#30340;&#19981;&#26029;&#38656;&#27714;&#65292;&#23548;&#33268;&#20934;&#30830;&#24615;&#38477;&#20302;&#65292;&#24182;&#19988;&#25968;&#25454;&#25104;&#26412;&#39640;&#26114;&#12289;&#20449;&#24687;&#31232;&#32570;&#65292;&#23545;&#24037;&#31243;&#24212;&#29992;&#19981;&#23454;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28176;&#36827;&#38477;&#38454;&#24314;&#27169;&#26694;&#26550;&#65292;&#26088;&#22312;&#20943;&#23569;&#23545;&#25968;&#25454;&#30340;&#38656;&#27714;&#24182;&#25552;&#39640;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#30340;&#23454;&#29992;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#31867;&#20284;&#20110;&#20154;&#31867;&#36873;&#25321;&#24615;&#20351;&#29992;&#26377;&#20215;&#20540;&#30340;&#30693;&#35782;&#32780;&#24573;&#30053;&#26080;&#29992;&#20449;&#24687;&#30340;&#38376;&#25511;&#26426;&#21046;&#65292;&#26377;&#36873;&#25321;&#24615;&#22320;&#20174;&#20808;&#21069;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#20256;&#36882;&#30693;&#35782;&#12290;&#36890;&#36807;&#20174;&#20808;&#21069;&#27169;&#22411;&#20013;&#31579;&#36873;&#20986;&#30456;&#20851;&#20449;&#24687;&#65292;&#25105;&#20204;&#21487;&#20197;&#21019;&#24314;&#19968;&#20010;&#20855;&#26377;&#26368;&#23567;&#36716;&#25442;&#26102;&#38388;&#21644;&#36739;&#23567;&#35757;&#32451;&#38598;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#20173;&#28982;&#33021;&#22815;&#23454;&#29616;&#39640;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#26696;&#20363;&#20013;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#21253;&#25324;&#22810;&#23380;&#20171;&#36136;&#20013;&#30340;&#20256;&#36755;&#12289;&#37325;&#21147;&#39537;&#21160;&#27969;&#21160;&#21644;&#36229;&#24377;&#26448;&#26009;&#30340;&#26377;&#38480;&#21464;&#24418;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20445;&#30041;&#20808;&#21069;&#27169;&#22411;&#30340;&#20449;&#24687;&#24182;&#21033;&#29992;&#20854;&#20013;&#30340;&#26377;&#20215;&#20540;&#37096;&#20998;&#21487;&#20197;&#25552;&#39640;&#24314;&#27169;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven modeling can suffer from a constant demand for data, leading to reduced accuracy and impractical for engineering applications due to the high cost and scarcity of information. To address this challenge, we propose a progressive reduced order modeling framework that minimizes data cravings and enhances data-driven modeling's practicality. Our approach selectively transfers knowledge from previously trained models through gates, similar to how humans selectively use valuable knowledge while ignoring unuseful information. By filtering relevant information from previous models, we can create a surrogate model with minimal turnaround time and a smaller training set that can still achieve high accuracy. We have tested our framework in several cases, including transport in porous media, gravity-driven flow, and finite deformation in hyperelastic materials. Our results illustrate that retaining information from previous models and utilizing a valuable portion of that knowledge can 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#38024;&#23545;&#28151;&#21512;V2X&#36890;&#20449;&#20013;&#30340;&#22402;&#30452;&#20999;&#25442;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24367;&#26354;&#29615;&#22659;&#20013;&#24110;&#21161;&#36710;&#36742;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;V2X&#25216;&#26415;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#31639;&#27861;&#20248;&#20110;&#24403;&#21069;&#30340;&#29366;&#24577;&#12290;</title><link>http://arxiv.org/abs/2310.03767</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#23545;&#28151;&#21512;V2X&#36890;&#20449;&#30340;&#22522;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning Algorithms for Hybrid V2X Communication: A Benchmarking Study. (arXiv:2310.03767v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03767
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#38024;&#23545;&#28151;&#21512;V2X&#36890;&#20449;&#20013;&#30340;&#22402;&#30452;&#20999;&#25442;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24367;&#26354;&#29615;&#22659;&#20013;&#24110;&#21161;&#36710;&#36742;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;V2X&#25216;&#26415;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#31639;&#27861;&#20248;&#20110;&#24403;&#21069;&#30340;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#26102;&#20195;&#65292;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#35201;&#27714;&#19982;&#39134;&#26426;&#30340;&#23433;&#20840;&#27700;&#24179;&#30456;&#24403;&#12290;&#20511;&#37492;&#33322;&#31354;&#33322;&#22825;&#24037;&#19994;&#30340;&#32463;&#39564;&#65292;&#36890;&#36807;&#22312;V2X&#65288;&#36710;&#21040;&#19968;&#20999;&#65289;&#25216;&#26415;&#20013;&#24314;&#31435;&#20887;&#20313;&#24615;&#65292;&#27773;&#36710;&#34892;&#19994;&#20063;&#21487;&#20197;&#21033;&#29992;&#36825;&#20010;&#27010;&#24565;&#26469;&#23454;&#29616;&#39640;&#21487;&#38752;&#24615;&#12290;&#37492;&#20110;&#24403;&#21069;&#32570;&#20047;&#21487;&#38752;&#30340;V2X&#25216;&#26415;&#65292;&#36825;&#20010;&#24819;&#27861;&#23588;&#20854;&#26377;&#21069;&#26223;&#12290;&#36890;&#36807;&#21516;&#26102;&#37096;&#32626;&#22810;&#20010;&#26080;&#32447;&#30005;&#25509;&#20837;&#25216;&#26415;&#65288;RATs&#65289;&#65292;&#26410;&#26469;&#36710;&#36742;&#30340;&#26631;&#20934;&#25216;&#26415;&#30340;&#25345;&#32493;&#20105;&#35770;&#21487;&#20197;&#24471;&#21040;&#35299;&#20915;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21160;&#24577;&#30340;&#12289;&#26102;&#21464;&#30340;&#20449;&#36947;&#21644;&#19981;&#26029;&#21464;&#21270;&#30340;&#20132;&#36890;&#24773;&#20917;&#65292;&#21327;&#35843;&#22810;&#31181;&#36890;&#20449;&#25216;&#26415;&#26159;&#19968;&#39033;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#37319;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#31639;&#27861;&#35299;&#20915;&#20102;V2X&#20013;&#30340;&#22402;&#30452;&#20999;&#25442;&#38382;&#39064;&#12290;&#30446;&#26631;&#26159;&#22312;&#24367;&#26354;&#29615;&#22659;&#20013;&#24110;&#21161;&#36710;&#36742;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;V2X&#25216;&#26415;&#65288;DSRC/V-VLC&#65289;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22522;&#20934;&#31639;&#27861;&#20248;&#20110;&#24403;&#21069;&#29366;&#24577;
&lt;/p&gt;
&lt;p&gt;
In today's era, autonomous vehicles demand a safety level on par with aircraft. Taking a cue from the aerospace industry, which relies on redundancy to achieve high reliability, the automotive sector can also leverage this concept by building redundancy in V2X (Vehicle-to-Everything) technologies. Given the current lack of reliable V2X technologies, this idea is particularly promising. By deploying multiple RATs (Radio Access Technologies) in parallel, the ongoing debate over the standard technology for future vehicles can be put to rest. However, coordinating multiple communication technologies is a complex task due to dynamic, time-varying channels and varying traffic conditions. This paper addresses the vertical handover problem in V2X using Deep Reinforcement Learning (DRL) algorithms. The goal is to assist vehicles in selecting the most appropriate V2X technology (DSRC/V-VLC) in a serpentine environment. The results show that the benchmarked algorithms outperform the current state
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#22522;&#20110;&#20256;&#24863;&#22120;&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#20013;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#29305;&#24449;&#25552;&#21462;&#35774;&#35745;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#23454;&#39564;&#30740;&#31350;&#65292;&#21457;&#29616;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#27963;&#21160;&#35782;&#21035;&#20013;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#20449;&#21495;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#25506;&#32034;&#20102;&#19981;&#21516;&#30340;&#35757;&#32451;&#26426;&#21046;&#21644;&#29305;&#24449;&#34920;&#31034;&#23545;&#20110;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.03760</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#29305;&#24449;&#25552;&#21462;&#35774;&#35745;&#22312;&#22522;&#20110;&#20256;&#24863;&#22120;&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#20013;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Investigating Deep Neural Network Architecture and Feature Extraction Designs for Sensor-based Human Activity Recognition. (arXiv:2310.03760v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#22522;&#20110;&#20256;&#24863;&#22120;&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#20013;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#29305;&#24449;&#25552;&#21462;&#35774;&#35745;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#23454;&#39564;&#30740;&#31350;&#65292;&#21457;&#29616;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#27963;&#21160;&#35782;&#21035;&#20013;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#20449;&#21495;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#25506;&#32034;&#20102;&#19981;&#21516;&#30340;&#35757;&#32451;&#26426;&#21046;&#21644;&#29305;&#24449;&#34920;&#31034;&#23545;&#20110;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#35774;&#22791;&#21644;&#29289;&#32852;&#32593;&#20013;&#20256;&#24863;&#22120;&#30340;&#24191;&#27867;&#21487;&#29992;&#24615;&#20026;&#22522;&#20110;&#20256;&#24863;&#22120;&#30340;&#27963;&#21160;&#35782;&#21035;&#30340;&#23454;&#29616;&#25171;&#24320;&#20102;&#21487;&#33021;&#24615;&#12290;&#19982;&#20256;&#32479;&#30340;&#20256;&#24863;&#22120;&#26102;&#38388;&#24207;&#21015;&#22788;&#29702;&#21644;&#25163;&#24037;&#29305;&#24449;&#25552;&#21462;&#30456;&#21453;&#65292;&#37492;&#20110;&#28145;&#24230;&#23398;&#20064;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#35777;&#26126;&#26377;&#25928;&#24615;&#65292;&#24050;&#32463;&#25506;&#32034;&#20102;&#35768;&#22810;&#28145;&#24230;&#26041;&#27861;&#26469;&#35299;&#20915;&#27963;&#21160;&#35782;&#21035;&#20013;&#30340;&#25361;&#25112;&#65292;&#24182;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;&#20449;&#21495;&#22788;&#29702;&#21644;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#36890;&#36807;&#23545;&#20004;&#20010;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#25968;&#25454;&#38598;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#30740;&#31350;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24120;&#35265;&#30340;&#28145;&#24230;&#23398;&#20064;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#20197;&#21450;&#19981;&#21516;&#30340;&#35757;&#32451;&#26426;&#21046;&#65288;&#22914;&#23545;&#27604;&#23398;&#20064;&#65289;&#21644;&#20174;&#20256;&#24863;&#22120;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#25552;&#21462;&#30340;&#21508;&#31181;&#29305;&#24449;&#34920;&#31034;&#65292;&#24182;&#27979;&#37327;&#23427;&#20204;&#22312;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The extensive ubiquitous availability of sensors in smart devices and the Internet of Things (IoT) has opened up the possibilities for implementing sensor-based activity recognition. As opposed to traditional sensor time-series processing and hand-engineered feature extraction, in light of deep learning's proven effectiveness across various domains, numerous deep methods have been explored to tackle the challenges in activity recognition, outperforming the traditional signal processing and traditional machine learning approaches. In this work, by performing extensive experimental studies on two human activity recognition datasets, we investigate the performance of common deep learning and machine learning approaches as well as different training mechanisms (such as contrastive learning), and various feature representations extracted from the sensor time-series data and measure their effectiveness for the human activity recognition task.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;1D-CycleGAN&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#27597;&#20307;&#24515;&#30005;&#22270;&#20013;&#25552;&#21462;&#32974;&#20799;&#24515;&#30005;&#22270;&#65292;&#20445;&#25345;&#20854;&#24418;&#24577;&#29305;&#24449;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#35777;&#23454;&#20102;&#20854;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.03759</link><description>&lt;p&gt;
&#19968;&#20010;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#29992;&#20110;&#20445;&#25345;&#32974;&#20799;&#24515;&#30005;&#22270;&#24418;&#24577;&#30340;&#20174;&#27597;&#20307;&#24515;&#30005;&#22270;&#20013;&#25552;&#21462;&#32974;&#20799;&#24515;&#30005;&#22270;&#30340;1D-CycleGAN
&lt;/p&gt;
&lt;p&gt;
A Novel Deep Learning Technique for Morphology Preserved Fetal ECG Extraction from Mother ECG using 1D-CycleGAN. (arXiv:2310.03759v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03759
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;1D-CycleGAN&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#27597;&#20307;&#24515;&#30005;&#22270;&#20013;&#25552;&#21462;&#32974;&#20799;&#24515;&#30005;&#22270;&#65292;&#20445;&#25345;&#20854;&#24418;&#24577;&#29305;&#24449;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#35777;&#23454;&#20102;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#38750;&#20405;&#20837;&#24335;&#30340;&#32974;&#20799;&#24515;&#30005;&#22270;&#65288;fECG&#65289;&#30417;&#27979;&#32974;&#20799;&#24515;&#33039;&#30340;&#30005;&#33033;&#20914;&#21487;&#20197;&#36731;&#26494;&#26816;&#27979;&#21457;&#32946;&#20013;&#24515;&#33039;&#30340;&#24322;&#24120;&#65292;&#20197;&#26174;&#33879;&#38477;&#20302;&#23156;&#20799;&#27515;&#20129;&#29575;&#21644;&#20135;&#21518;&#24182;&#21457;&#30151;&#12290;&#30001;&#20110;&#27597;&#20307;&#21644;&#32974;&#20799;R&#23792;&#30340;&#37325;&#21472;&#65292;fECG&#20449;&#21495;&#30340;&#20302;&#24133;&#24230;&#65292;&#20197;&#21450;&#31995;&#32479;&#21644;&#29615;&#22659;&#22122;&#22768;&#65292;&#20256;&#32479;&#30340;&#20449;&#21495;&#25552;&#21462;&#26041;&#27861;&#26080;&#27861;&#20135;&#29983;&#20196;&#20154;&#28385;&#24847;&#30340;fECG&#12290;&#34429;&#28982;&#19968;&#20123;&#25216;&#26415;&#33021;&#22815;&#20135;&#29983;&#20934;&#30830;&#30340;QRS&#27874;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#24573;&#30053;ECG&#30340;&#20854;&#20182;&#37325;&#35201;&#26041;&#38754;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;1D CycleGAN&#65292;&#36890;&#36807;&#24191;&#27867;&#30340;&#39044;&#22788;&#29702;&#21644;&#36866;&#24403;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;mECG&#20449;&#21495;&#37325;&#26500;fECG&#20449;&#21495;&#65292;&#24182;&#20445;&#25345;&#20854;&#24418;&#24577;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#24615;&#33021;&#36890;&#36807;&#32467;&#21512;Physionet&#30340;&#20004;&#20010;&#21487;&#29992;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#65292;&#21363;"Abdominal and Direct Fetal ECG Database"&#21644;"Fetal electrocardiograms, direct an"
&lt;/p&gt;
&lt;p&gt;
Monitoring the electrical pulse of fetal heart through a non-invasive fetal electrocardiogram (fECG) can easily detect abnormalities in the developing heart to significantly reduce the infant mortality rate and post-natal complications. Due to the overlapping of maternal and fetal R-peaks, the low amplitude of the fECG, systematic and ambient noises, typical signal extraction methods, such as adaptive filters, independent component analysis, empirical mode decomposition, etc., are unable to produce satisfactory fECG. While some techniques can produce accurate QRS waves, they often ignore other important aspects of the ECG. Our approach, which is based on 1D CycleGAN, can reconstruct the fECG signal from the mECG signal while maintaining the morphology due to extensive preprocessing and appropriate framework. The performance of our solution was evaluated by combining two available datasets from Physionet, "Abdominal and Direct Fetal ECG Database" and "Fetal electrocardiograms, direct an
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#38750;&#32447;&#24615;&#29983;&#25104;&#24335;&#21387;&#32553;&#24863;&#30693;&#20013;&#23454;&#29616;&#32479;&#19968;&#30340;&#20449;&#21495;&#24674;&#22797;&#12290;&#35813;&#26694;&#26550;&#36866;&#29992;&#20110;&#38750;&#32447;&#24615;&#21644;&#21487;&#33021;&#38750;&#36830;&#32493;&#25110;&#26410;&#30693;&#30340;&#35266;&#27979;&#27169;&#22411;&#65292;&#24182;&#19988;&#21487;&#20197;&#24674;&#22797;&#29983;&#25104;&#27169;&#22411;&#20013;&#25152;&#26377;&#21487;&#33021;&#30340;&#20449;&#21495;&#12290;</title><link>http://arxiv.org/abs/2310.03758</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#29983;&#25104;&#24335;&#21387;&#32553;&#24863;&#30693;&#20013;&#32479;&#19968;&#20449;&#21495;&#24674;&#22797;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Unified Framework for Uniform Signal Recovery in Nonlinear Generative Compressed Sensing. (arXiv:2310.03758v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03758
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#38750;&#32447;&#24615;&#29983;&#25104;&#24335;&#21387;&#32553;&#24863;&#30693;&#20013;&#23454;&#29616;&#32479;&#19968;&#30340;&#20449;&#21495;&#24674;&#22797;&#12290;&#35813;&#26694;&#26550;&#36866;&#29992;&#20110;&#38750;&#32447;&#24615;&#21644;&#21487;&#33021;&#38750;&#36830;&#32493;&#25110;&#26410;&#30693;&#30340;&#35266;&#27979;&#27169;&#22411;&#65292;&#24182;&#19988;&#21487;&#20197;&#24674;&#22797;&#29983;&#25104;&#27169;&#22411;&#20013;&#25152;&#26377;&#21487;&#33021;&#30340;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#25104;&#24335;&#21387;&#32553;&#24863;&#30693;&#20013;&#65292;&#25105;&#20204;&#24076;&#26395;&#20351;&#29992;&#29983;&#25104;&#20808;&#39564;&#20174;m&#20010;&#27979;&#37327;&#20013;&#65288;m&#8810;n&#65289;&#24674;&#22797;&#19968;&#20010;&#20449;&#21495;x&#8727;&#8712;Rn&#65292;&#20854;&#20013;G&#36890;&#24120;&#26159;&#19968;&#20010;L-Lipschitz&#36830;&#32493;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;B2k(r)&#34920;&#31034;Rk&#20013;&#30340;&#21322;&#24452;&#20026;r&#30340;&#8467;2&#29699;&#12290;&#22312;&#38750;&#32447;&#24615;&#27979;&#37327;&#19979;&#65292;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#32467;&#26524;&#26159;&#38750;&#22343;&#21248;&#30340;&#65292;&#21363;&#23427;&#20204;&#23545;&#20110;&#22266;&#23450;&#30340;x&#8727;&#20855;&#26377;&#39640;&#27010;&#29575;&#65292;&#32780;&#19981;&#26159;&#23545;&#20110;&#25152;&#26377;&#30340;x&#8727;&#21516;&#26102;&#25104;&#31435;&#12290;&#26412;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#25512;&#23548;&#38750;&#32447;&#24615;&#29983;&#25104;&#24335;&#21387;&#32553;&#24863;&#30693;&#20013;&#30340;&#22343;&#21248;&#24674;&#22797;&#20445;&#35777;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#38750;&#32447;&#24615;&#21644;&#21487;&#33021;&#38750;&#36830;&#32493;&#25110;&#26410;&#30693;&#30340;&#35266;&#27979;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#20102;1&#20301;/&#22343;&#21248;&#37327;&#21270;&#35266;&#27979;&#21644;&#21333;&#32034;&#24341;&#27169;&#22411;&#20316;&#20026;&#35268;&#33539;&#31034;&#20363;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#20351;&#29992;&#24863;&#30693;&#38598;&#21512;&#30340;&#21333;&#20010;&#23454;&#29616;&#21644;&#24191;&#20041;Lasso&#65292;&#25152;&#26377;&#30340;x&#8727;&#8712;G(B2k(r))&#21487;&#20197;&#24674;&#22797;&#21040;&#19968;&#20010;el
&lt;/p&gt;
&lt;p&gt;
In generative compressed sensing (GCS), we want to recover a signal $\mathbf{x}^* \in \mathbb{R}^n$ from $m$ measurements ($m\ll n$) using a generative prior $\mathbf{x}^*\in G(\mathbb{B}_2^k(r))$, where $G$ is typically an $L$-Lipschitz continuous generative model and $\mathbb{B}_2^k(r)$ represents the radius-$r$ $\ell_2$-ball in $\mathbb{R}^k$. Under nonlinear measurements, most prior results are non-uniform, i.e., they hold with high probability for a fixed $\mathbf{x}^*$ rather than for all $\mathbf{x}^*$ simultaneously. In this paper, we build a unified framework to derive uniform recovery guarantees for nonlinear GCS where the observation model is nonlinear and possibly discontinuous or unknown. Our framework accommodates GCS with 1-bit/uniformly quantized observations and single index models as canonical examples. Specifically, using a single realization of the sensing ensemble and generalized Lasso, {\em all} $\mathbf{x}^*\in G(\mathbb{B}_2^k(r))$ can be recovered up to an $\el
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;EOG&#20449;&#21495;&#36827;&#34892;&#33258;&#21160;&#21270;&#30561;&#30496;&#38454;&#27573;&#20998;&#31867;&#65292;&#24182;&#25913;&#36827;&#20102;&#30561;&#30496;&#38556;&#30861;&#30740;&#31350;&#20013;&#23545;REM&#30561;&#30496;&#30340;&#35782;&#21035;&#12290;&#25105;&#20204;&#30340;&#25552;&#20986;&#30340;SE-Resnet-Transformer&#27169;&#22411;&#22312;&#22810;&#20010;&#25968;&#25454;&#24211;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#36825;&#19968;&#21457;&#23637;&#23558;&#25552;&#39640;&#30561;&#30496;&#38454;&#27573;&#20998;&#31867;&#30340;&#21487;&#35775;&#38382;&#24615;&#65292;&#20943;&#23569;&#23545;EEG&#35774;&#22791;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2310.03757</link><description>&lt;p&gt;
&#29992;EOG&#22686;&#24378;&#21307;&#30103;&#20445;&#20581;&#65306;&#19968;&#31181;&#29992;&#20110;&#30561;&#30496;&#38454;&#27573;&#20998;&#31867;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Enhancing Healthcare with EOG: A Novel Approach to Sleep Stage Classification. (arXiv:2310.03757v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03757
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;EOG&#20449;&#21495;&#36827;&#34892;&#33258;&#21160;&#21270;&#30561;&#30496;&#38454;&#27573;&#20998;&#31867;&#65292;&#24182;&#25913;&#36827;&#20102;&#30561;&#30496;&#38556;&#30861;&#30740;&#31350;&#20013;&#23545;REM&#30561;&#30496;&#30340;&#35782;&#21035;&#12290;&#25105;&#20204;&#30340;&#25552;&#20986;&#30340;SE-Resnet-Transformer&#27169;&#22411;&#22312;&#22810;&#20010;&#25968;&#25454;&#24211;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#36825;&#19968;&#21457;&#23637;&#23558;&#25552;&#39640;&#30561;&#30496;&#38454;&#27573;&#20998;&#31867;&#30340;&#21487;&#35775;&#38382;&#24615;&#65292;&#20943;&#23569;&#23545;EEG&#35774;&#22791;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;EOG&#20449;&#21495;&#36827;&#34892;&#33258;&#21160;&#21270;&#30340;&#30561;&#30496;&#38454;&#27573;&#20998;&#31867;&#65292;&#35299;&#20915;&#20102;EEG&#25968;&#25454;&#37319;&#38598;&#24102;&#26469;&#30340;&#19981;&#36866;&#21644;&#19981;&#23454;&#29992;&#24615;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#39046;&#22495;&#20013;&#23578;&#26410;&#34987;&#24320;&#21457;&#65292;&#31361;&#26174;&#20102;&#20854;&#23545;&#26032;&#35265;&#35299;&#21644;&#36129;&#29486;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;SE-Resnet-Transformer&#27169;&#22411;&#33021;&#22815;&#20174;&#21407;&#22987;EOG&#20449;&#21495;&#20934;&#30830;&#20998;&#31867;&#20986;&#20116;&#20010;&#19981;&#21516;&#30340;&#30561;&#30496;&#38454;&#27573;&#12290;&#23545;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#24211;&#65288;SleepEDF-20&#12289;SleepEDF-78&#21644;SHHS&#65289;&#36827;&#34892;&#20102;&#24191;&#27867;&#39564;&#35777;&#65292;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#20998;&#21035;&#20026;74.72&#12289;70.63&#21644;69.26&#30340;&#23439;F1&#20998;&#25968;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#35782;&#21035;REM&#30561;&#30496;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#36825;&#26159;&#30561;&#30496;&#38556;&#30861;&#30740;&#31350;&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;1D-GradCAM&#21644;t-SNE&#22270;&#31561;&#25216;&#26415;&#23545;&#25105;&#20204;&#27169;&#22411;&#30340;&#20869;&#37096;&#26426;&#21046;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25913;&#36827;&#20102;&#30561;&#30496;&#38454;&#27573;&#20998;&#31867;&#30340;&#21487;&#35775;&#38382;&#24615;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#23545;EEG&#35774;&#22791;&#30340;&#38656;&#27714;&#12290;&#36825;&#19968;&#21457;&#23637;&#20855;&#26377;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce an innovative approach to automated sleep stage classification using EOG signals, addressing the discomfort and impracticality associated with EEG data acquisition. In addition, it is important to note that this approach is untapped in the field, highlighting its potential for novel insights and contributions. Our proposed SE-Resnet-Transformer model provides an accurate classification of five distinct sleep stages from raw EOG signal. Extensive validation on publically available databases (SleepEDF-20, SleepEDF-78, and SHHS) reveals noteworthy performance, with macro-F1 scores of 74.72, 70.63, and 69.26, respectively. Our model excels in identifying REM sleep, a crucial aspect of sleep disorder investigations. We also provide insight into the internal mechanisms of our model using techniques such as 1D-GradCAM and t-SNE plots. Our method improves the accessibility of sleep stage classification while decreasing the need for EEG modalities. This development will have promis
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22810;&#36890;&#36947;&#33041;&#30005;&#22270;&#25968;&#25454;&#20998;&#26512;&#26041;&#27861;&#65292;&#26088;&#22312;&#39044;&#27979;&#26127;&#36855;&#24739;&#32773;&#30340;&#20302;&#30149;&#29702;&#24615;&#31070;&#32463;&#32467;&#26524;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#21452;&#26497;EEG&#35760;&#24405;&#21644;&#27880;&#24847;&#26426;&#21046;&#65292;&#20197;&#25552;&#39640;&#39044;&#27979;&#25928;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#39044;&#27979;&#25361;&#25112;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.03756</link><description>&lt;p&gt;
&#33258;&#25105;&#21644;&#36328;&#36890;&#36947;&#27880;&#24847;&#26426;&#21046;&#19979;&#65292;&#29992;&#20110;&#26127;&#36855;&#24739;&#32773;&#30340;&#22810;&#36890;&#36947;&#33041;&#30005;&#22270;&#25968;&#25454;&#20998;&#26512;&#30340;&#23500;&#36139;&#39044;&#27979;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Multi-channel EEG Data Analysis for Poor Neuro-prognostication in Comatose Patients with Self and Cross-channel Attention Mechanism. (arXiv:2310.03756v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03756
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22810;&#36890;&#36947;&#33041;&#30005;&#22270;&#25968;&#25454;&#20998;&#26512;&#26041;&#27861;&#65292;&#26088;&#22312;&#39044;&#27979;&#26127;&#36855;&#24739;&#32773;&#30340;&#20302;&#30149;&#29702;&#24615;&#31070;&#32463;&#32467;&#26524;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#21452;&#26497;EEG&#35760;&#24405;&#21644;&#27880;&#24847;&#26426;&#21046;&#65292;&#20197;&#25552;&#39640;&#39044;&#27979;&#25928;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#39044;&#27979;&#25361;&#25112;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#21452;&#26497;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#35760;&#24405;&#23545;&#39044;&#27979;&#20302;&#30149;&#29702;&#24615;&#31070;&#32463;&#32467;&#26524;&#30340;&#28508;&#21147;&#12290;&#37319;&#29992;&#22238;&#39038;&#24615;&#35774;&#35745;&#21644;&#28151;&#21512;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20248;&#21270;&#20102;&#19968;&#20010;&#30446;&#26631;&#20989;&#25968;&#65292;&#20197;&#23454;&#29616;&#39640;&#29305;&#24322;&#24615;&#65288;&#21363;&#30495;&#38451;&#24615;&#29575;&#65288;TPR&#65289;&#65289;&#21644;&#38477;&#20302;&#20551;&#38451;&#24615;&#29575;&#65288;&lt; 0.05&#65289;&#12290;&#36873;&#21462;&#38543;&#26426;&#36873;&#25321;&#30340;&#19968;&#20010;&#23567;&#26102;&#20869;&#30340;5&#20998;&#38047;&#27573;&#33853;&#30340;18&#20010;&#21452;&#26497;&#36890;&#36947;&#23545;&#30340;&#22810;&#36890;&#36947;EEG&#38453;&#21015;&#12290;&#20026;&#20102;&#30830;&#23450;&#32467;&#26524;&#39044;&#27979;&#65292;&#20351;&#29992;&#20102;&#29305;&#24449;&#32534;&#30721;&#22120;&#19982;1-D&#21367;&#31215;&#23618;&#12289;&#21487;&#23398;&#20064;&#30340;&#20301;&#32622;&#32534;&#30721;&#12289;&#24102;&#26377;&#27880;&#24847;&#26426;&#21046;&#30340;&#19978;&#19979;&#25991;&#32593;&#32476;&#65292;&#20197;&#21450;&#22238;&#24402;&#22120;&#21644;&#20998;&#31867;&#22120;&#27169;&#22359;&#30340;&#32452;&#21512;&#12290;&#29305;&#24449;&#32534;&#30721;&#22120;&#25552;&#21462;&#23616;&#37096;&#26102;&#38388;&#21644;&#31354;&#38388;&#29305;&#24449;&#65292;&#32780;&#21518;&#32493;&#30340;&#20301;&#32622;&#32534;&#30721;&#21644;&#27880;&#24847;&#26426;&#21046;&#35797;&#22270;&#25429;&#25417;&#20840;&#23616;&#26102;&#38388;&#20381;&#36182;&#24615;&#12290;&#32467;&#26524;&#65306;&#25105;&#20204;&#22242;&#38431;&#25552;&#20986;&#30340;OUS IVS&#26694;&#26550;&#65292;&#22312;&#39564;&#35777;&#25361;&#25112;&#20013;&#33719;&#24471;&#20102;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work investigates the predictive potential of bipolar electroencephalogram (EEG) recordings towards efficient prediction of poor neurological outcomes. A retrospective design using a hybrid deep learning approach is utilized to optimize an objective function aiming for high specificity, i.e., true positive rate (TPR) with reduced false positives (&lt; 0.05). A multi-channel EEG array of 18 bipolar channel pairs from a randomly selected 5-minute segment in an hour is kept. In order to determine the outcome prediction, a combination of a feature encoder with 1-D convolutional layers, learnable position encoding, a context network with attention mechanisms, and finally, a regressor and classifier blocks are used. The feature encoder extricates local temporal and spatial features, while the following position encoding and attention mechanisms attempt to capture global temporal dependencies. Results: The proposed framework by our team, OUS IVS, when validated on the challenge hidden valid
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#35895;&#27468;Colab&#30340;&#24320;&#28304;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#29615;&#22659;&#65292;&#21487;&#29992;&#20110;&#22312;&#20108;&#32500;&#30697;&#24418;&#22495;&#19978;&#27169;&#25311;&#30636;&#24577;&#29616;&#35937;&#65292;&#24182;&#25552;&#20379;&#20102;&#22810;&#20010;&#29305;&#24615;&#21644;&#38382;&#39064;&#24211;&#12290;</title><link>http://arxiv.org/abs/2310.03755</link><description>&lt;p&gt;
&#36866;&#29992;&#20110;&#35895;&#27468;Colab&#30340;&#20108;&#32500;&#30636;&#24577;&#38382;&#39064;&#20013;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#20195;&#30721;&#65288;PINN-2DT&#65289;
&lt;/p&gt;
&lt;p&gt;
Physics Informed Neural Network Code for 2D Transient Problems (PINN-2DT) Compatible with Google Colab. (arXiv:2310.03755v1 [cs.CE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03755
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#35895;&#27468;Colab&#30340;&#24320;&#28304;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#29615;&#22659;&#65292;&#21487;&#29992;&#20110;&#22312;&#20108;&#32500;&#30697;&#24418;&#22495;&#19978;&#27169;&#25311;&#30636;&#24577;&#29616;&#35937;&#65292;&#24182;&#25552;&#20379;&#20102;&#22810;&#20010;&#29305;&#24615;&#21644;&#38382;&#39064;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24320;&#28304;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#29615;&#22659;&#65292;&#29992;&#20110;&#22312;&#20108;&#32500;&#30697;&#24418;&#22495;&#19978;&#27169;&#25311;&#30636;&#24577;&#29616;&#35937;&#65292;&#20855;&#26377;&#20197;&#19979;&#29305;&#28857;&#65306;&#65288;1&#65289;&#19982;&#35895;&#27468;Colab&#20860;&#23481;&#65292;&#21487;&#20197;&#22312;&#20113;&#29615;&#22659;&#20013;&#36827;&#34892;&#33258;&#21160;&#25191;&#34892;&#65307;&#65288;2&#65289;&#25903;&#25345;&#20108;&#32500;&#26102;&#21464;PDE&#65307;&#65288;3&#65289;&#25552;&#20379;&#31616;&#21333;&#30340;&#30028;&#38754;&#26469;&#23450;&#20041;&#27531;&#30041;&#25439;&#22833;&#12289;&#36793;&#30028;&#26465;&#20214;&#21644;&#21021;&#22987;&#25439;&#22833;&#65292;&#20197;&#21450;&#23427;&#20204;&#30340;&#26435;&#37325;&#65307;&#65288;4&#65289;&#25903;&#25345;&#35834;&#20381;&#26364;&#21644;&#36842;&#21033;&#20811;&#38647;&#36793;&#30028;&#26465;&#20214;&#65307;&#65288;5&#65289;&#20801;&#35768;&#33258;&#23450;&#20041;&#23618;&#25968;&#21644;&#27599;&#23618;&#30340;&#31070;&#32463;&#20803;&#25968;&#37327;&#65292;&#20197;&#21450;&#20219;&#24847;&#28608;&#27963;&#20989;&#25968;&#65307;&#65288;6&#65289;&#23398;&#20064;&#29575;&#21644;&#36845;&#20195;&#27425;&#25968;&#21487;&#20197;&#20316;&#20026;&#21442;&#25968;&#35843;&#33410;&#65307;&#65288;7&#65289;&#23545;&#31354;&#38388;&#21644;&#26102;&#38388;&#21464;&#37327;&#30340;PINN&#36827;&#34892;&#33258;&#21160;&#27714;&#23548;&#65307;&#65288;8&#65289;&#25552;&#20379;&#20102;&#32472;&#21046;&#25910;&#25947;&#24615;&#65288;&#20855;&#26377;&#28369;&#21160;&#24179;&#22343;&#65289;&#12289;&#23398;&#20064;&#21040;&#30340;&#21021;&#22987;&#26465;&#20214;&#12289;&#27169;&#25311;&#30340;&#20108;&#32500;&#21644;&#19977;&#32500;&#24555;&#29031;&#20197;&#21450;&#35270;&#39057;&#30340;&#24120;&#35268;&#20989;&#25968;&#65307;&#65288;9&#65289;&#21253;&#21547;&#20102;&#19968;&#20010;&#38382;&#39064;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an open-source Physics Informed Neural Network environment for simulations of transient phenomena on two-dimensional rectangular domains, with the following features: (1) it is compatible with Google Colab which allows automatic execution on cloud environment; (2) it supports two dimensional time-dependent PDEs; (3) it provides simple interface for definition of the residual loss, boundary condition and initial loss, together with their weights; (4) it support Neumann and Dirichlet boundary conditions; (5) it allows for customizing the number of layers and neurons per layer, as well as for arbitrary activation function; (6) the learning rate and number of epochs are available as parameters; (7) it automatically differentiates PINN with respect to spatial and temporal variables; (8) it provides routines for plotting the convergence (with running average), initial conditions learnt, 2D and 3D snapshots from the simulation and movies (9) it includes a library of problems: (a) n
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EMGTFNet&#30340;&#22522;&#20110;&#27169;&#31946;&#35270;&#35273;&#21464;&#21387;&#22120;&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#36890;&#36807;&#34920;&#38754;&#32908;&#30005;&#20449;&#21495;&#36827;&#34892;&#25163;&#21183;&#35782;&#21035;&#65292;&#21487;&#20197;&#20934;&#30830;&#20998;&#31867;&#21508;&#31181;&#25163;&#21183;&#65292;&#26080;&#38656;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#25110;&#32593;&#32476;&#21442;&#25968;&#30340;&#22686;&#21152;&#12290;</title><link>http://arxiv.org/abs/2310.03754</link><description>&lt;p&gt;
EMGTFNet&#65306;&#29992;&#20110;&#35299;&#30721;&#19978;&#32930;&#34920;&#38754;&#32908;&#30005;&#20449;&#21495;&#30340;&#27169;&#31946;&#35270;&#35273;&#21464;&#21387;&#22120;&#36827;&#34892;&#25163;&#21183;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
EMGTFNet: Fuzzy Vision Transformer to decode Upperlimb sEMG signals for Hand Gestures Recognition. (arXiv:2310.03754v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03754
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EMGTFNet&#30340;&#22522;&#20110;&#27169;&#31946;&#35270;&#35273;&#21464;&#21387;&#22120;&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#36890;&#36807;&#34920;&#38754;&#32908;&#30005;&#20449;&#21495;&#36827;&#34892;&#25163;&#21183;&#35782;&#21035;&#65292;&#21487;&#20197;&#20934;&#30830;&#20998;&#31867;&#21508;&#31181;&#25163;&#21183;&#65292;&#26080;&#38656;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#25110;&#32593;&#32476;&#21442;&#25968;&#30340;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32908;&#30005;&#25511;&#21046;&#26159;&#30005;&#32908;&#22270;&#30340;&#19968;&#20010;&#26085;&#30410;&#21463;&#20851;&#27880;&#30340;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#22312;&#20223;&#29983;&#20551;&#32930;&#30340;&#25163;&#21183;&#35782;&#21035;&#31561;&#24212;&#29992;&#20013;&#12290;&#30446;&#21069;&#30340;&#37325;&#28857;&#26159;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#26368;&#36817;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#27169;&#24335;&#35782;&#21035;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#22312;&#31232;&#30095;&#30340;&#34920;&#38754;&#32908;&#30005;&#20449;&#21495;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#65292;&#20294;&#36890;&#24120;&#38656;&#35201;&#22823;&#22411;&#25968;&#25454;&#38598;&#21644;&#35757;&#32451;&#26102;&#38388;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#38543;&#26426;&#34920;&#38754;&#32908;&#30005;&#20449;&#21495;&#30340;&#29305;&#24615;&#65292;&#20256;&#32479;&#27169;&#22411;&#26080;&#27861;&#23558;&#26679;&#26412;&#25512;&#24191;&#21040;&#38750;&#20856;&#22411;&#25110;&#22122;&#22768;&#20540;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#35270;&#35273;&#21464;&#21387;&#22120;(ViT)&#21644;&#27169;&#31946;&#31070;&#32463;&#22359;(FNB)&#30340;EMGTFNet&#35774;&#35745;&#65292;&#29992;&#20110;&#36890;&#36807;&#34920;&#38754;&#32908;&#30005;&#20449;&#21495;&#36827;&#34892;&#25163;&#21183;&#35782;&#21035;&#12290;&#25152;&#25552;&#20986;&#30340;EMGTFNet&#26550;&#26500;&#21487;&#20197;&#20934;&#30830;&#22320;&#20998;&#31867;&#21508;&#31181;&#25163;&#21183;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#12289;&#36801;&#31227;&#23398;&#20064;&#25110;&#32593;&#32476;&#21442;&#25968;&#30340;&#22823;&#24133;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
Myoelectric control is an area of electromyography of increasing interest nowadays, particularly in applications such as Hand Gesture Recognition (HGR) for bionic prostheses. Today's focus is on pattern recognition using Machine Learning and, more recently, Deep Learning methods. Despite achieving good results on sparse sEMG signals, the latter models typically require large datasets and training times. Furthermore, due to the nature of stochastic sEMG signals, traditional models fail to generalize samples for atypical or noisy values. In this paper, we propose the design of a Vision Transformer (ViT) based architecture with a Fuzzy Neural Block (FNB) called EMGTFNet to perform Hand Gesture Recognition from surface electromyography (sEMG) signals. The proposed EMGTFNet architecture can accurately classify a variety of hand gestures without any need for data augmentation techniques, transfer learning or a significant increase in the number of parameters in the network. The accuracy of t
&lt;/p&gt;</description></item><item><title>ECGNet&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476; (GAN) &#20174;&#21333;&#23548;&#32852;&#36755;&#20837;&#21512;&#25104;12&#23548;&#32852;&#24515;&#30005;&#22270;&#20449;&#21495;&#65292;&#24182;&#36890;&#36807;&#29305;&#24449;&#20998;&#26512;&#35782;&#21035;&#20986;&#21487;&#29992;&#20110;&#24515;&#34880;&#31649;&#30142;&#30149;&#39044;&#27979;&#30340;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2310.03753</link><description>&lt;p&gt;
ECGNet&#65306;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476; (GAN) &#20174;&#21333;&#23548;&#32852;&#36755;&#20837;&#20013;&#21512;&#25104;12&#23548;&#32852;&#24515;&#30005;&#22270;&#20449;&#21495;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ECGNet: A generative adversarial network (GAN) approach to the synthesis of 12-lead ECG signals from single lead inputs. (arXiv:2310.03753v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03753
&lt;/p&gt;
&lt;p&gt;
ECGNet&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476; (GAN) &#20174;&#21333;&#23548;&#32852;&#36755;&#20837;&#21512;&#25104;12&#23548;&#32852;&#24515;&#30005;&#22270;&#20449;&#21495;&#65292;&#24182;&#36890;&#36807;&#29305;&#24449;&#20998;&#26512;&#35782;&#21035;&#20986;&#21487;&#29992;&#20110;&#24515;&#34880;&#31649;&#30142;&#30149;&#39044;&#27979;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476; (GAN) &#36827;&#34892;&#24515;&#30005;&#22270; (ECG) &#20449;&#21495;&#21512;&#25104;&#24050;&#32463;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#65292;&#22240;&#20026;&#23454;&#29616;12&#23548;&#32852;&#24515;&#30005;&#22270;&#24182;&#19981;&#24635;&#26159;&#21487;&#34892;&#30340;&#12290;GAN&#27169;&#22411;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#65292;&#20294;&#20165;&#38024;&#23545;&#22810;&#23548;&#32852;&#36755;&#20837;&#36827;&#34892;&#35774;&#35745;&#65292;&#24182;&#19988;&#23578;&#26410;&#30830;&#23450;GAN&#27169;&#22411;&#25152;&#20445;&#30041;&#30340;&#29305;&#24449;&#65292;&#38480;&#21046;&#20102;&#29983;&#25104;&#20449;&#21495;&#22312;&#24515;&#34880;&#31649;&#30142;&#30149;&#39044;&#27979;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;ECGNet&#65292;&#23427;&#26159;&#19968;&#31181;&#20351;&#29992;GAN&#26694;&#26550;&#12289;&#20855;&#26377;&#21452;&#21521;&#38271;&#30701;&#26399;&#35760;&#24518; (LSTM) &#29983;&#25104;&#22120;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476; (CNN) &#21028;&#21035;&#22120;&#30340;&#26041;&#27861;&#65292;&#20174;&#20219;&#20309;&#21333;&#23548;&#32852;&#36755;&#20837;&#29983;&#25104;&#23436;&#25972;&#30340;12&#23548;&#32852;&#24515;&#30005;&#22270;&#20449;&#21495;&#12290;&#23545;&#29983;&#25104;&#30340;&#20449;&#21495;&#36827;&#34892;&#20132;&#21449;&#21644;&#33258;&#30456;&#20851;&#20998;&#26512;&#65292;&#35782;&#21035;&#20986;&#20449;&#21495;&#29983;&#25104;&#36807;&#31243;&#20013;&#20445;&#30041;&#30340;&#29305;&#24449;&#65292;&#21363;&#33021;&#22815;&#34920;&#24449;&#27599;&#20010;&#20449;&#21495;&#29420;&#29305;&#24615;&#30340;&#29305;&#24449;&#65292;&#22240;&#27492;&#24456;&#21487;&#33021;&#26159;&#24515;&#34880;&#31649;&#30142;&#30149;&#30340;&#25351;&#26631;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#20351;&#29992;&#26631;&#27880;&#26377;&#24515;&#30005;&#22270;&#20449;&#21495;&#30340;&#25968;&#25454;&#38598;&#65292;&#35777;&#26126;&#20102;ECGNet&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electrocardiography (ECG) signal generation has been heavily explored using generative adversarial networks (GAN) because the implementation of 12-lead ECGs is not always feasible. The GAN models have achieved remarkable results in reproducing ECG signals but are only designed for multiple lead inputs and the features the GAN model preserves have not been identified-limiting the generated signals use in cardiovascular disease (CVD)-predictive models. This paper presents ECGNet which is a procedure that generates a complete set of 12-lead ECG signals from any single lead input using a GAN framework with a bidirectional long short-term memory (LSTM) generator and a convolutional neural network (CNN) discriminator. Cross and auto-correlation analysis performed on the generated signals identifies features conserved during the signal generation-i.e., features that can characterize the unique-nature of each signal and thus likely indicators of CVD. Finally, by using ECG signals annotated wit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20027;&#39064;&#23884;&#20837;&#36801;&#31227;&#23398;&#20064;&#30340;&#28145;&#24230;&#23398;&#20064;&#24207;&#21015;&#35299;&#30721;&#22120;&#65292;&#29992;&#20110;&#30636;&#24577;&#39640;&#23494;&#24230;&#32908;&#30005;&#22270;&#25163;&#21183;&#35782;&#21035;&#12290;&#35813;&#35299;&#30721;&#22120;&#22312;&#37096;&#20998;&#35266;&#23519;&#30340;&#21463;&#35797;&#32773;&#20013;&#21462;&#24471;&#20102;73%&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.03752</link><description>&lt;p&gt;
&#20351;&#29992;&#20027;&#39064;&#23884;&#20837;&#36801;&#31227;&#23398;&#20064;&#30340;&#30636;&#24577;&#39640;&#23494;&#24230;&#32908;&#30005;&#22270;&#25163;&#21183;&#35782;&#21035;&#30340;&#28145;&#24230;&#23398;&#20064;&#24207;&#21015;&#35299;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Deep Learning Sequential Decoder for Transient High-Density Electromyography in Hand Gesture Recognition Using Subject-Embedded Transfer Learning. (arXiv:2310.03752v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03752
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20027;&#39064;&#23884;&#20837;&#36801;&#31227;&#23398;&#20064;&#30340;&#28145;&#24230;&#23398;&#20064;&#24207;&#21015;&#35299;&#30721;&#22120;&#65292;&#29992;&#20110;&#30636;&#24577;&#39640;&#23494;&#24230;&#32908;&#30005;&#22270;&#25163;&#21183;&#35782;&#21035;&#12290;&#35813;&#35299;&#30721;&#22120;&#22312;&#37096;&#20998;&#35266;&#23519;&#30340;&#21463;&#35797;&#32773;&#20013;&#21462;&#24471;&#20102;73%&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#21183;&#35782;&#21035;(HGR)&#30001;&#20110;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#20154;&#26426;&#30028;&#38754;&#23545;&#22806;&#21608;&#31070;&#32463;&#31995;&#32479;&#30340;&#29983;&#29289;&#20449;&#21495;(&#22914;&#34920;&#38754;&#32908;&#30005;&#22270;(sEMG))&#36827;&#34892;&#28145;&#24230;&#26102;&#31354;&#21160;&#21147;&#23398;&#35299;&#35835;&#30340;&#22686;&#21152;&#32780;&#24341;&#36215;&#20102;&#37325;&#35270;&#12290;&#36825;&#20123;&#30028;&#38754;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#25193;&#23637;&#29616;&#23454;&#30340;&#25511;&#21046;&#12289;&#28789;&#27963;&#30340;&#20551;&#32930;&#21644;&#22806;&#39592;&#39612;&#12290;&#28982;&#32780;&#65292;sEMG&#22312;&#20010;&#20307;&#20043;&#38388;&#30340;&#33258;&#28982;&#21464;&#24322;&#24615;&#23548;&#33268;&#30740;&#31350;&#20154;&#21592;&#19987;&#27880;&#20110;&#20010;&#20307;&#29305;&#23450;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#20855;&#26377;&#22797;&#26434;&#30340;&#32467;&#26500;&#65292;&#23545;&#25968;&#25454;&#38656;&#27714;&#36739;&#22810;&#65292;&#24182;&#19988;&#35757;&#32451;&#26102;&#38388;&#36739;&#38271;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#23545;&#20110;&#20010;&#20307;&#29305;&#23450;&#24212;&#29992;&#30340;&#23454;&#38469;&#24615;&#36739;&#20302;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#21487;&#27867;&#21270;&#30340;&#30636;&#24577;&#39640;&#23494;&#24230;sEMG(HD-sEMG)&#30340;&#24207;&#21015;&#35299;&#30721;&#22120;&#65292;&#36890;&#36807;&#20027;&#39064;&#23884;&#20837;&#36801;&#31227;&#23398;&#20064;&#65292;&#22312;&#37096;&#20998;&#35266;&#23519;&#30340;&#21463;&#35797;&#32773;&#20013;&#23454;&#29616;&#20102;65&#20010;&#25163;&#21183;&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#20026;73%&#12290;
&lt;/p&gt;
&lt;p&gt;
Hand gesture recognition (HGR) has gained significant attention due to the increasing use of AI-powered human-computer interfaces that can interpret the deep spatiotemporal dynamics of biosignals from the peripheral nervous system, such as surface electromyography (sEMG). These interfaces have a range of applications, including the control of extended reality, agile prosthetics, and exoskeletons. However, the natural variability of sEMG among individuals has led researchers to focus on subject-specific solutions. Deep learning methods, which often have complex structures, are particularly data-hungry and can be time-consuming to train, making them less practical for subject-specific applications. In this paper, we propose and develop a generalizable, sequential decoder of transient high-density sEMG (HD-sEMG) that achieves 73% average accuracy on 65 gestures for partially-observed subjects through subject-embedded transfer learning, leveraging pre-knowledge of HGR acquired during pre-t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#21644;&#32447;&#24615;&#26368;&#23567;&#20108;&#20056;&#27861;&#30340;&#31616;&#21333;&#26694;&#26550;&#65292;&#28436;&#31034;&#20102;&#20132;&#21449;&#23398;&#20064;&#30340;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.03751</link><description>&lt;p&gt;
&#32447;&#24615;&#26368;&#23567;&#20108;&#20056;&#38382;&#39064;&#20013;&#20351;&#29992;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#36827;&#34892;&#20132;&#21449;&#23398;&#20064;&#30340;&#31616;&#21333;&#31034;&#20363;
&lt;/p&gt;
&lt;p&gt;
A Simple Illustration of Interleaved Learning using Kalman Filter for Linear Least Squares. (arXiv:2310.03751v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#21644;&#32447;&#24615;&#26368;&#23567;&#20108;&#20056;&#27861;&#30340;&#31616;&#21333;&#26694;&#26550;&#65292;&#28436;&#31034;&#20102;&#20132;&#21449;&#23398;&#20064;&#30340;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#21449;&#23398;&#20064;&#26159;&#19968;&#31181;&#21463;&#29983;&#29289;&#21551;&#21457;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20013;&#26377;&#30528;&#24456;&#22909;&#30340;&#32467;&#26524;&#12290;&#22312;&#36825;&#20010;&#31616;&#30701;&#30340;&#35828;&#26126;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#22522;&#20110;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#21644;&#32447;&#24615;&#26368;&#23567;&#20108;&#20056;&#27861;&#30340;&#31616;&#21333;&#32479;&#35745;&#21644;&#20248;&#21270;&#26694;&#26550;&#65292;&#26469;&#28436;&#31034;&#20132;&#21449;&#23398;&#20064;&#30340;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interleaved learning in machine learning algorithms is a biologically inspired training method with promising results. In this short note, we illustrate the interleaving mechanism via a simple statistical and optimization framework based on Kalman Filter for Linear Least Squares.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#23545;&#21830;&#19994;&#39640;&#33021;&#22411;&#30967;&#37240;&#38081;&#38146;&#30005;&#27744;&#30340;&#32769;&#21270;&#21644;&#20462;&#22797;&#23454;&#39564;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#24490;&#29615;&#23551;&#21629;&#24182;&#35782;&#21035;&#21487;&#24674;&#22797;&#23481;&#37327;&#30340;&#37325;&#35201;&#25351;&#26631;&#12290;&#32771;&#34385;&#21040;&#30005;&#27744;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#24179;&#22343;&#24490;&#29615;&#23551;&#21629;&#39044;&#27979;&#35823;&#24046;&#20026;16.84% &#177; 1.87%&#65292;&#20026;&#30005;&#27744;&#20581;&#24247;&#35780;&#20272;&#21644;&#24674;&#22797;&#25552;&#20379;&#20102;&#26377;&#21147;&#30340;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2310.03750</link><description>&lt;p&gt;
&#37319;&#29992;&#25968;&#25454;&#20998;&#26512;&#21644;&#31561;&#25928;&#30005;&#36335;&#24314;&#27169;&#30340;&#32769;&#21270;&#38146;&#31163;&#23376;&#30005;&#27744;&#20581;&#24247;&#35786;&#26029;&#21644;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Health diagnosis and recuperation of aged Li-ion batteries with data analytics and equivalent circuit modeling. (arXiv:2310.03750v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03750
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#23545;&#21830;&#19994;&#39640;&#33021;&#22411;&#30967;&#37240;&#38081;&#38146;&#30005;&#27744;&#30340;&#32769;&#21270;&#21644;&#20462;&#22797;&#23454;&#39564;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#24490;&#29615;&#23551;&#21629;&#24182;&#35782;&#21035;&#21487;&#24674;&#22797;&#23481;&#37327;&#30340;&#37325;&#35201;&#25351;&#26631;&#12290;&#32771;&#34385;&#21040;&#30005;&#27744;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#24179;&#22343;&#24490;&#29615;&#23551;&#21629;&#39044;&#27979;&#35823;&#24046;&#20026;16.84% &#177; 1.87%&#65292;&#20026;&#30005;&#27744;&#20581;&#24247;&#35780;&#20272;&#21644;&#24674;&#22797;&#25552;&#20379;&#20102;&#26377;&#21147;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#27744;&#20581;&#24247;&#35780;&#20272;&#21644;&#24674;&#22797;&#22312;&#21033;&#29992;&#20108;&#27425;&#23551;&#21629;&#38146;&#31163;&#23376;&#30005;&#27744;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32769;&#21270;&#26426;&#21046;&#19981;&#26126;&#30830;&#21644;&#24674;&#22797;&#25928;&#26524;&#19982;&#25805;&#20316;&#29366;&#24577;&#32570;&#20047;&#30456;&#20851;&#24615;&#65292;&#20934;&#30830;&#20272;&#35745;&#30005;&#27744;&#20581;&#24247;&#24182;&#21046;&#23450;&#28165;&#26224;&#30340;&#30005;&#27744;&#24674;&#22797;&#31574;&#30053;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;62&#20010;&#21830;&#19994;&#39640;&#33021;&#22411;&#30967;&#37240;&#38081;&#38146; (LFP) &#30005;&#27744;&#30340;&#32769;&#21270;&#21644;&#20462;&#22797;&#23454;&#39564;&#65292;&#34917;&#20805;&#20102;&#29616;&#26377;&#39640;&#21151;&#29575;LFP&#30005;&#27744;&#30340;&#25968;&#25454;&#38598;&#12290;&#30456;&#23545;&#36739;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#20351;&#25105;&#20204;&#33021;&#22815;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#39044;&#27979;&#24490;&#29615;&#23551;&#21629;&#24182;&#35782;&#21035;&#21487;&#24674;&#22797;&#23481;&#37327;&#30340;&#37325;&#35201;&#25351;&#26631;&#12290;&#32771;&#34385;&#21040;&#30005;&#27744;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#36890;&#36807;&#26799;&#24230;&#25552;&#21319;&#22238;&#24402;&#22120;&#32473;&#20986;&#30340;&#20174;&#21069;80&#20010;&#21608;&#26399;&#30340;&#20449;&#24687;&#20013;&#33719;&#24471;&#30340;&#24490;&#29615;&#23551;&#21629;&#39044;&#27979;&#24179;&#22343;&#27979;&#35797;&#35823;&#24046;&#20026;$16.84\% \pm 1.87\%$ (&#24179;&#22343;&#32477;&#23545;&#30334;&#20998;&#27604;&#35823;&#24046;)&#12290;&#27492;&#22806;&#65292;&#21457;&#29616;&#19968;&#20123;&#21487;&#24674;&#22797;&#30340;&#25439;&#22833;&#23481;&#37327;&#19982;...&#65288;&#25991;&#26412;&#19981;&#23436;&#25972;&#65289;
&lt;/p&gt;
&lt;p&gt;
Battery health assessment and recuperation play a crucial role in the utilization of second-life Li-ion batteries. However, due to ambiguous aging mechanisms and lack of correlations between the recovery effects and operational states, it is challenging to accurately estimate battery health and devise a clear strategy for cell rejuvenation. This paper presents aging and reconditioning experiments of 62 commercial high-energy type lithium iron phosphate (LFP) cells, which supplement existing datasets of high-power LFP cells. The relatively large-scale data allow us to use machine learning models to predict cycle life and identify important indicators of recoverable capacity. Considering cell-to-cell inconsistencies, an average test error of $16.84\% \pm 1.87\%$ (mean absolute percentage error) for cycle life prediction is achieved by gradient boosting regressor given information from the first 80 cycles. In addition, it is found that some of the recoverable lost capacity is attributed t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SCVCNet&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#20998;&#26512;&#33041;&#30005;&#22270;&#20013;&#30340;&#32454;&#31890;&#24230;&#39057;&#29575;&#32467;&#26500;&#26469;&#28040;&#38500;&#20219;&#21153;&#21644;&#20010;&#20307;&#38598;&#30456;&#20851;&#30340;&#24178;&#25200;&#65292;&#23454;&#29616;&#20102;&#36328;&#20219;&#21153;&#21644;&#20010;&#20307;&#38388;&#30340;&#33041;&#30005;&#35748;&#30693;&#36127;&#33655;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2310.03749</link><description>&lt;p&gt;
SCVCNet: &#29992;&#20110;&#36328;&#20219;&#21153;&#21644;&#20010;&#20307;&#38388;&#33041;&#30005;&#35748;&#30693;&#36127;&#33655;&#35782;&#21035;&#30340;&#28369;&#21160;&#20132;&#21449;&#21521;&#37327;&#21367;&#31215;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
SCVCNet: Sliding cross-vector convolution network for cross-task and inter-individual-set EEG-based cognitive workload recognition. (arXiv:2310.03749v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03749
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SCVCNet&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#20998;&#26512;&#33041;&#30005;&#22270;&#20013;&#30340;&#32454;&#31890;&#24230;&#39057;&#29575;&#32467;&#26500;&#26469;&#28040;&#38500;&#20219;&#21153;&#21644;&#20010;&#20307;&#38598;&#30456;&#20851;&#30340;&#24178;&#25200;&#65292;&#23454;&#29616;&#20102;&#36328;&#20219;&#21153;&#21644;&#20010;&#20307;&#38388;&#30340;&#33041;&#30005;&#35748;&#30693;&#36127;&#33655;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#19981;&#21516;&#20154;&#26426;&#20219;&#21153;&#21644;&#20010;&#20307;&#38598;&#19978;&#30340;&#24120;&#35265;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#27169;&#24335;&#26469;&#24212;&#29992;&#35748;&#30693;&#36127;&#33655;&#35782;&#21035;&#22120;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SCVCNet&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#20998;&#26512;&#21151;&#29575;&#35889;&#23494;&#24230;&#20013;&#26356;&#31934;&#32454;&#30340;&#39057;&#29575;&#32467;&#26500;&#26469;&#28040;&#38500;&#33041;&#30005;&#22270;&#20013;&#30340;&#20219;&#21153;&#21644;&#20010;&#20307;&#38598;&#30456;&#20851;&#24178;&#25200;&#12290;SCVCNet&#21033;&#29992;&#28369;&#21160;&#20132;&#21449;&#21521;&#37327;&#21367;&#31215;&#65288;SCVC&#65289;&#25805;&#20316;&#65292;&#20854;&#20013;&#20351;&#29992;&#20195;&#34920;theta&#21644;alpha&#21151;&#29575;&#30340;&#37197;&#23545;&#36755;&#20837;&#23618;&#12290;&#36890;&#36807;&#25552;&#21462;&#26680;&#30697;&#38453;&#30340;&#20013;&#22830;&#34892;&#21644;&#21015;&#30340;&#26435;&#37325;&#65292;&#25105;&#20204;&#35745;&#31639;&#25351;&#23450;&#22836;&#30382;&#20301;&#32622;&#21608;&#22260;&#20004;&#20010;&#21521;&#37327;&#30340;&#21152;&#26435;&#21644;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#39057;&#29575;&#28857;&#38388;&#29305;&#24449;&#34701;&#21512;&#27169;&#22359;&#26469;&#34701;&#21512;SCVC&#29305;&#24449;&#22270;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#36825;&#20004;&#20010;&#27169;&#22359;&#19982;&#36755;&#20986;&#36890;&#36947;&#27744;&#21270;&#21644;&#20998;&#31867;&#23618;&#32452;&#21512;&#36215;&#26469;&#26500;&#24314;&#27169;&#22411;&#12290;&#20026;&#20102;&#35757;&#32451;SCVCNet&#65292;&#25105;&#20204;&#20351;&#29992;&#27491;&#21017;&#21270;&#26368;&#23567;&#20108;&#20056;&#27861;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a generic approach for applying the cognitive workload recognizer by exploiting common electroencephalogram (EEG) patterns across different human-machine tasks and individual sets. We propose a neural network called SCVCNet, which eliminates task- and individual-set-related interferences in EEGs by analyzing finer-grained frequency structures in the power spectral densities. The SCVCNet utilizes a sliding cross-vector convolution (SCVC) operation, where paired input layers representing the theta and alpha power are employed. By extracting the weights from a kernel matrix's central row and column, we compute the weighted sum of the two vectors around a specified scalp location. Next, we introduce an inter-frequency-point feature integration module to fuse the SCVC feature maps. Finally, we combined the two modules with the output-channel pooling and classification layers to construct the model. To train the SCVCNet, we employ the regularized least-square method with 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#30456;&#20301;&#21516;&#27493;&#25104;&#20998;&#33258;&#32452;&#32455;&#30340;&#27010;&#24565;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#31471;&#21040;&#31471;&#32593;&#32476;&#33258;&#21160;&#21270;&#33041;&#26426;&#25509;&#21475;&#20013;&#30340;&#39044;&#22788;&#29702;&#21644;&#36890;&#36947;&#36873;&#25321;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20998;&#26512;&#21151;&#33021;&#24615;&#33041;&#36830;&#25509;&#21644;&#35782;&#21035;&#33041;&#27963;&#21160;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.03748</link><description>&lt;p&gt;
&#33041;&#26426;&#25509;&#21475;&#20013;&#30340;&#30456;&#20301;&#21516;&#27493;&#25104;&#20998;&#33258;&#32452;&#32455;
&lt;/p&gt;
&lt;p&gt;
Phase Synchrony Component Self-Organization in Brain Computer Interface. (arXiv:2310.03748v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#30456;&#20301;&#21516;&#27493;&#25104;&#20998;&#33258;&#32452;&#32455;&#30340;&#27010;&#24565;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#31471;&#21040;&#31471;&#32593;&#32476;&#33258;&#21160;&#21270;&#33041;&#26426;&#25509;&#21475;&#20013;&#30340;&#39044;&#22788;&#29702;&#21644;&#36890;&#36947;&#36873;&#25321;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20998;&#26512;&#21151;&#33021;&#24615;&#33041;&#36830;&#25509;&#21644;&#35782;&#21035;&#33041;&#27963;&#21160;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#20301;&#21516;&#27493;&#20449;&#24687;&#22312;&#20998;&#26512;&#21151;&#33021;&#24615;&#33041;&#36830;&#25509;&#21644;&#35782;&#21035;&#33041;&#27963;&#21160;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#30446;&#21069;&#24191;&#27867;&#37319;&#29992;&#30340;&#29305;&#24449;&#25552;&#21462;&#27969;&#31243;&#30001;&#39044;&#22788;&#29702;&#12289;&#36873;&#25321;&#33041;&#30005;&#37319;&#38598;&#36890;&#36947;&#21644;&#30456;&#20301;&#38145;&#23450;&#20540;&#65288;PLV&#65289;&#35745;&#31639;&#32452;&#25104;&#65292;&#22312;&#36816;&#21160;&#24819;&#35937;&#20998;&#31867;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#35813;&#27969;&#31243;&#26159;&#25163;&#21160;&#30340;&#19988;&#20381;&#36182;&#19987;&#23478;&#30693;&#35782;&#65292;&#38480;&#21046;&#20102;&#23427;&#22312;&#19981;&#21516;&#24212;&#29992;&#22330;&#26223;&#20013;&#30340;&#20415;&#21033;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#37117;&#37319;&#29992;&#20102;&#19968;&#33324;&#30340;&#19982;&#25968;&#25454;&#26080;&#20851;&#30340;&#31354;&#38388;&#28388;&#27874;&#22120;&#26469;&#25233;&#21046;&#22122;&#22768;&#65292;&#38459;&#30861;&#20102;&#26356;&#37325;&#35201;&#30340;&#30456;&#20301;&#21516;&#27493;&#29616;&#35937;&#30340;&#25506;&#32034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30456;&#20301;&#21516;&#27493;&#25104;&#20998;&#33258;&#32452;&#32455;&#30340;&#27010;&#24565;&#65292;&#23427;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#23398;&#20064;&#25968;&#25454;&#30456;&#20851;&#30340;&#31354;&#38388;&#28388;&#27874;&#22120;&#65292;&#20174;&#32780;&#33258;&#21160;&#21270;&#39044;&#22788;&#29702;&#21644;&#36890;&#36947;&#36873;&#25321;&#31243;&#24207;&#12290;&#22522;&#20110;&#36825;&#20010;&#27010;&#24565;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#31532;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#31471;&#21040;&#31471;&#32593;&#32476;&#65292;&#30452;&#25509;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Phase synchrony information plays a crucial role in analyzing functional brain connectivity and identifying brain activities. A widely adopted feature extraction pipeline, composed of preprocessing, selection of EEG acquisition channels, and phase locking value (PLV) calculation, has achieved success in motor imagery classification (MI). However, this pipeline is manual and reliant on expert knowledge, limiting its convenience and adaptability to different application scenarios. Moreover, most studies have employed mediocre data-independent spatial filters to suppress noise, impeding the exploration of more significant phase synchronization phenomena. To address the issues, we propose the concept of phase synchrony component self-organization, which enables the adaptive learning of data-dependent spatial filters for automating both the preprocessing and channel selection procedures. Based on this concept, the first deep learning end-to-end network is developed, which directly extracts 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#39537;&#21160;&#30340; EEG &#34920;&#31034;&#30340;&#36328;&#35270;&#22270;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550; (KDC2)&#65292;&#36890;&#36807;&#27169;&#25311;&#33041;&#27963;&#21160;&#30340;&#20869;&#37096;&#21644;&#22806;&#37096;&#34920;&#31034;&#65292;&#20174;&#26377;&#38480;&#26631;&#31614;&#30340; EEG &#20013;&#25552;&#21462;&#26377;&#25928;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#35270;&#22270;&#21644;&#36328;&#35270;&#22270;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#21644;&#34920;&#31034;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2310.03747</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#39537;&#21160;&#30340; EEG &#34920;&#31034;&#30340;&#36328;&#35270;&#22270;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
A Knowledge-Driven Cross-view Contrastive Learning for EEG Representation. (arXiv:2310.03747v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03747
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#39537;&#21160;&#30340; EEG &#34920;&#31034;&#30340;&#36328;&#35270;&#22270;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550; (KDC2)&#65292;&#36890;&#36807;&#27169;&#25311;&#33041;&#27963;&#21160;&#30340;&#20869;&#37096;&#21644;&#22806;&#37096;&#34920;&#31034;&#65292;&#20174;&#26377;&#38480;&#26631;&#31614;&#30340; EEG &#20013;&#25552;&#21462;&#26377;&#25928;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#35270;&#22270;&#21644;&#36328;&#35270;&#22270;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#21644;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#33041;&#30005;&#22270; (EEG) &#20449;&#21495;&#20013;&#20016;&#23500;&#30340;&#31070;&#32463;&#29983;&#29702;&#20449;&#24687;&#65292;&#23558; EEG &#20449;&#21495;&#19982;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30456;&#32467;&#21512;&#24050;&#22312;&#35768;&#22810;&#23454;&#38469;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110; EEG &#20449;&#21495;&#30340;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30340;&#21457;&#23637;&#21463;&#21040;&#20102;&#39640;&#26114;&#30340;&#25104;&#26412;&#21644;&#22823;&#35268;&#27169; EEG &#25968;&#25454;&#38598;&#25163;&#21160;&#26631;&#35760;&#30340;&#26174;&#33879;&#26631;&#31614;&#19981;&#19968;&#33268;&#24615;&#30340;&#38459;&#30861;&#12290;&#33258;&#30417;&#30563;&#26694;&#26550;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#39046;&#22495;&#24471;&#21040;&#20102;&#37319;&#29992;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#32570;&#20047; EEG &#29305;&#23450;&#30340;&#29702;&#35770;&#22522;&#30784;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#39537;&#21160;&#30340;&#36328;&#35270;&#22270;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550; (KDC2)&#65292;&#23427;&#23558;&#31070;&#32463;&#23398;&#29702;&#35770;&#19982;&#26377;&#38480;&#26631;&#31614;&#30340; EEG &#20013;&#25552;&#21462;&#26377;&#25928;&#34920;&#31034;&#30456;&#32467;&#21512;&#12290;KDC2 &#26041;&#27861;&#21019;&#24314;&#20102; EEG &#20449;&#21495;&#30340;&#22836;&#30382;&#21644;&#31070;&#32463;&#35270;&#22270;&#65292;&#27169;&#25311;&#20102;&#33041;&#27963;&#21160;&#30340;&#20869;&#37096;&#21644;&#22806;&#37096;&#34920;&#31034;&#12290;&#38543;&#21518;&#65292;&#36890;&#36807;&#35270;&#22270;&#21644;&#36328;&#35270;&#22270;&#23545;&#27604;&#23398;&#20064;&#36880;&#27493;&#23436;&#25104;&#29305;&#24449;&#25552;&#21462;&#21644;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the abundant neurophysiological information in the electroencephalogram (EEG) signal, EEG signals integrated with deep learning methods have gained substantial traction across numerous real-world tasks. However, the development of supervised learning methods based on EEG signals has been hindered by the high cost and significant label discrepancies to manually label large-scale EEG datasets. Self-supervised frameworks are adopted in vision and language fields to solve this issue, but the lack of EEG-specific theoretical foundations hampers their applicability across various tasks. To solve these challenges, this paper proposes a knowledge-driven cross-view contrastive learning framework (KDC2), which integrates neurological theory to extract effective representations from EEG with limited labels. The KDC2 method creates scalp and neural views of EEG signals, simulating the internal and external representation of brain activity. Sequentially, inter-view and cross-view contrastive
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20013;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#21644;&#27010;&#29575;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#20855;&#26377;&#29289;&#29702;&#30693;&#35782;&#32422;&#26463;&#30340;&#36229;&#24377;&#24615;&#26448;&#26009;&#24212;&#21464;&#33021;&#20989;&#25968;&#30340;&#26032;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2310.03745</link><description>&lt;p&gt;
&#20855;&#26377;&#29289;&#29702;&#30693;&#35782;&#32422;&#26463;&#30340;&#27010;&#29575;&#25193;&#25955;&#39046;&#22495;&#30340;&#29983;&#25104;&#24335;&#36229;&#24377;&#24615;
&lt;/p&gt;
&lt;p&gt;
Generative Hyperelasticity with Physics-Informed Probabilistic Diffusion Fields. (arXiv:2310.03745v1 [cs.CE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03745
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20013;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#21644;&#27010;&#29575;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#20855;&#26377;&#29289;&#29702;&#30693;&#35782;&#32422;&#26463;&#30340;&#36229;&#24377;&#24615;&#26448;&#26009;&#24212;&#21464;&#33021;&#20989;&#25968;&#30340;&#26032;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#33258;&#28982;&#26448;&#26009;&#34920;&#29616;&#20986;&#39640;&#24230;&#22797;&#26434;&#12289;&#38750;&#32447;&#24615;&#12289;&#21508;&#21521;&#24322;&#24615;&#21644;&#24322;&#36136;&#24615;&#30340;&#21147;&#23398;&#29305;&#24615;&#12290;&#26368;&#36817;&#65292;&#24050;&#32463;&#35777;&#26126;&#25968;&#25454;&#39537;&#21160;&#30340;&#24212;&#21464;&#33021;&#20989;&#25968;&#20855;&#26377;&#28789;&#27963;&#24615;&#65292;&#21487;&#20197;&#20197;&#39640;&#31934;&#24230;&#25429;&#25417;&#36825;&#20123;&#22797;&#26434;&#26448;&#26009;&#30340;&#34892;&#20026;&#65292;&#21516;&#26102;&#28385;&#36275;&#22522;&#20110;&#29289;&#29702;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#26041;&#27861;&#24573;&#30053;&#20102;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#26448;&#26009;&#30340;&#31354;&#38388;&#24322;&#36136;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;NODE&#65289;&#20316;&#20026;&#24314;&#27169;&#22522;&#30784;&#65292;&#36890;&#36807;&#26500;&#36896;&#21487;&#20197;&#21019;&#24314;&#22810;&#20984;&#24212;&#21464;&#33021;&#20989;&#25968;&#65292;&#36825;&#26159;&#23454;&#38469;&#30340;&#36229;&#24377;&#24615;&#26448;&#26009;&#27169;&#22411;&#30340;&#20851;&#38190;&#29305;&#24615;&#12290;&#25105;&#20204;&#32467;&#21512;&#36825;&#31181;&#26041;&#27861;&#19982;&#27010;&#29575;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#24212;&#21464;&#33021;&#20989;&#25968;&#30340;&#26032;&#26679;&#26412;&#12290;&#36825;&#31181;&#25216;&#26415;&#20801;&#35768;&#25105;&#20204;&#23545;&#39640;&#26031;&#30333;&#22122;&#22768;&#36827;&#34892;&#37319;&#26679;&#24182;&#23558;&#20854;&#36716;&#25442;&#20026;NODE&#21442;&#25968;&#65292;&#20174;&#32780;&#20195;&#34920;&#21487;&#34892;&#30340;&#24212;&#21464;&#33021;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many natural materials exhibit highly complex, nonlinear, anisotropic, and heterogeneous mechanical properties. Recently, it has been demonstrated that data-driven strain energy functions possess the flexibility to capture the behavior of these complex materials with high accuracy while satisfying physics-based constraints. However, most of these approaches disregard the uncertainty in the estimates and the spatial heterogeneity of these materials. In this work, we leverage recent advances in generative models to address these issues. We use as building block neural ordinary equations (NODE) that -- by construction -- create polyconvex strain energy functions, a key property of realistic hyperelastic material models. We combine this approach with probabilistic diffusion models to generate new samples of strain energy functions. This technique allows us to sample a vector of Gaussian white noise and translate it to NODE parameters thereby representing plausible strain energy functions. 
&lt;/p&gt;</description></item><item><title>GENER&#26159;&#19968;&#20010;&#24182;&#34892;&#23618;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#65292;&#19987;&#38376;&#29992;&#20110;&#20174;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#20013;&#26816;&#27979;&#22522;&#22240;-&#22522;&#22240;&#30456;&#20114;&#20316;&#29992;&#12290;&#23454;&#39564;&#35777;&#23454;GENER&#22312;&#39044;&#27979;&#22522;&#22240;-&#22522;&#22240;&#30456;&#20114;&#20316;&#29992;&#26041;&#38754;&#30340;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#32479;&#35745;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.03611</link><description>&lt;p&gt;
GENER:&#19968;&#31181;&#24182;&#34892;&#23618;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#29992;&#20110;&#20174;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#20013;&#26816;&#27979;&#22522;&#22240;-&#22522;&#22240;&#30456;&#20114;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
GENER: A Parallel Layer Deep Learning Network To Detect Gene-Gene Interactions From Gene Expression Data. (arXiv:2310.03611v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03611
&lt;/p&gt;
&lt;p&gt;
GENER&#26159;&#19968;&#20010;&#24182;&#34892;&#23618;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#65292;&#19987;&#38376;&#29992;&#20110;&#20174;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#20013;&#26816;&#27979;&#22522;&#22240;-&#22522;&#22240;&#30456;&#20114;&#20316;&#29992;&#12290;&#23454;&#39564;&#35777;&#23454;GENER&#22312;&#39044;&#27979;&#22522;&#22240;-&#22522;&#22240;&#30456;&#20114;&#20316;&#29992;&#26041;&#38754;&#30340;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#32479;&#35745;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24050;&#30693;&#22522;&#22240;&#34920;&#36798;&#21644;&#22522;&#22240;&#30456;&#20114;&#20316;&#29992;&#25968;&#25454;&#26469;&#26816;&#27979;&#21644;&#21457;&#29616;&#26032;&#30340;&#22522;&#22240;&#30456;&#20114;&#20316;&#29992;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#21508;&#31181;&#32479;&#35745;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24050;&#32463;&#23581;&#35797;&#36890;&#36807;&#21033;&#29992;&#22522;&#22240;&#30456;&#20114;&#20316;&#29992;&#30340;&#25299;&#25169;&#32467;&#26500;&#21644;&#22522;&#22240;&#34920;&#36798;&#27169;&#24335;&#26469;&#39044;&#27979;&#26032;&#30340;&#22522;&#22240;&#30456;&#20114;&#20316;&#29992;&#12290;&#30456;&#21453;&#65292;&#19968;&#20123;&#26041;&#27861;&#19987;&#27880;&#20110;&#21033;&#29992;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;GENER&#65292;&#36825;&#26159;&#19968;&#20010;&#24182;&#34892;&#23618;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#65292;&#19987;&#38376;&#29992;&#20110;&#20351;&#29992;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#26469;&#35782;&#21035;&#22522;&#22240;-&#22522;&#22240;&#20851;&#31995;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20004;&#20010;&#35757;&#32451;&#23454;&#39564;&#65292;&#24182;&#23558;&#25105;&#20204;&#30340;&#32593;&#32476;&#30340;&#24615;&#33021;&#19982;&#29616;&#26377;&#30340;&#32479;&#35745;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#32467;&#21512;&#20102;BioGRID&amp;DREAM5&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;0.834&#30340;&#24179;&#22343;AUROC&#20998;&#25968;&#65292;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#22312;&#39044;&#27979;&#22522;&#22240;-&#22522;&#22240;&#30456;&#20114;&#20316;&#29992;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting and discovering new gene interactions based on known gene expressions and gene interaction data presents a significant challenge. Various statistical and deep learning methods have attempted to tackle this challenge by leveraging the topological structure of gene interactions and gene expression patterns to predict novel gene interactions. In contrast, some approaches have focused exclusively on utilizing gene expression profiles. In this context, we introduce GENER, a parallel-layer deep learning network designed exclusively for the identification of gene-gene relationships using gene expression data. We conducted two training experiments and compared the performance of our network with that of existing statistical and deep learning approaches. Notably, our model achieved an average AUROC score of 0.834 on the combined BioGRID&amp;DREAM5 dataset, outperforming competing methods in predicting gene-gene interactions.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FASER&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20013;&#38388;&#34920;&#31034;&#36827;&#34892;&#20108;&#36827;&#21046;&#20195;&#30721;&#30456;&#20284;&#24615;&#25628;&#32034;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#36328;&#26550;&#26500;&#22320;&#35782;&#21035;&#20989;&#25968;&#65292;&#24182;&#26126;&#30830;&#32534;&#30721;&#20989;&#25968;&#30340;&#35821;&#20041;&#65292;&#20197;&#25903;&#25345;&#21508;&#31181;&#24212;&#29992;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2310.03605</link><description>&lt;p&gt;
FASER: &#36890;&#36807;&#20013;&#38388;&#34920;&#31034;&#36827;&#34892;&#20108;&#36827;&#21046;&#20195;&#30721;&#30456;&#20284;&#24615;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
FASER: Binary Code Similarity Search through the use of Intermediate Representations. (arXiv:2310.03605v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03605
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FASER&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20013;&#38388;&#34920;&#31034;&#36827;&#34892;&#20108;&#36827;&#21046;&#20195;&#30721;&#30456;&#20284;&#24615;&#25628;&#32034;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#36328;&#26550;&#26500;&#22320;&#35782;&#21035;&#20989;&#25968;&#65292;&#24182;&#26126;&#30830;&#32534;&#30721;&#20989;&#25968;&#30340;&#35821;&#20041;&#65292;&#20197;&#25903;&#25345;&#21508;&#31181;&#24212;&#29992;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#22815;&#35782;&#21035;&#36328;&#26550;&#26500;&#36719;&#20214;&#20013;&#24863;&#20852;&#36259;&#30340;&#20989;&#25968;&#23545;&#20110;&#20998;&#26512;&#24694;&#24847;&#36719;&#20214;&#12289;&#20445;&#25252;&#36719;&#20214;&#20379;&#24212;&#38142;&#25110;&#36827;&#34892;&#28431;&#27934;&#30740;&#31350;&#37117;&#26159;&#26377;&#29992;&#30340;&#12290;&#36328;&#26550;&#26500;&#20108;&#36827;&#21046;&#20195;&#30721;&#30456;&#20284;&#24615;&#25628;&#32034;&#24050;&#22312;&#35768;&#22810;&#30740;&#31350;&#20013;&#25506;&#32034;&#65292;&#24182;&#20351;&#29992;&#20102;&#21508;&#31181;&#19981;&#21516;&#30340;&#25968;&#25454;&#26469;&#28304;&#26469;&#23454;&#29616;&#20854;&#30446;&#26631;&#12290;&#36890;&#24120;&#20351;&#29992;&#30340;&#25968;&#25454;&#26469;&#28304;&#21253;&#25324;&#20174;&#20108;&#36827;&#21046;&#25991;&#20214;&#20013;&#25552;&#21462;&#30340;&#24120;&#35265;&#32467;&#26500;&#65292;&#22914;&#20989;&#25968;&#25511;&#21046;&#27969;&#22270;&#25110;&#20108;&#36827;&#21046;&#32423;&#35843;&#29992;&#22270;&#65292;&#21453;&#27719;&#32534;&#36807;&#31243;&#30340;&#36755;&#20986;&#25110;&#21160;&#24577;&#20998;&#26512;&#26041;&#27861;&#30340;&#36755;&#20986;&#12290;&#20854;&#20013;&#19968;&#31181;&#21463;&#21040;&#36739;&#23569;&#20851;&#27880;&#30340;&#25968;&#25454;&#26469;&#28304;&#26159;&#20108;&#36827;&#21046;&#20013;&#38388;&#34920;&#31034;&#12290;&#20108;&#36827;&#21046;&#20013;&#38388;&#34920;&#31034;&#20855;&#26377;&#20004;&#20010;&#26377;&#36259;&#30340;&#23646;&#24615;&#65306;&#23427;&#20204;&#30340;&#36328;&#26550;&#26500;&#24615;&#36136;&#20197;&#21450;&#26126;&#30830;&#32534;&#30721;&#20989;&#25968;&#30340;&#35821;&#20041;&#20197;&#25903;&#25345;&#19979;&#28216;&#20351;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FASER&#30340;&#20989;&#25968;&#23383;&#31526;&#20018;&#32534;&#30721;&#34920;&#31034;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#38271;&#25991;&#26723;&#36716;&#25442;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Being able to identify functions of interest in cross-architecture software is useful whether you are analysing for malware, securing the software supply chain or conducting vulnerability research. Cross-Architecture Binary Code Similarity Search has been explored in numerous studies and has used a wide range of different data sources to achieve its goals. The data sources typically used draw on common structures derived from binaries such as function control flow graphs or binary level call graphs, the output of the disassembly process or the outputs of a dynamic analysis approach. One data source which has received less attention is binary intermediate representations. Binary Intermediate representations possess two interesting properties: they are cross architecture by their very nature and encode the semantics of a function explicitly to support downstream usage. Within this paper we propose Function as a String Encoded Representation (FASER) which combines long document transforme
&lt;/p&gt;</description></item><item><title>&#36825;&#31181;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;UTR-LM&#65292;&#36890;&#36807;&#23545;&#22810;&#20010;&#29289;&#31181;&#30340;5' UTR&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#32467;&#21512;&#26377;&#30417;&#30563;&#20449;&#24687;&#65292;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#26368;&#20339;&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#39044;&#27979;&#24179;&#22343;&#26680;&#31958;&#20307;&#36127;&#36733;&#12289;&#32763;&#35793;&#25928;&#29575;&#12289;mRNA&#34920;&#36798;&#27700;&#24179;&#65292;&#24182;&#25913;&#36827;&#20102;&#20869;&#28304;&#24615;&#26680;&#31958;&#20307;&#36827;&#20837;&#20301;&#28857;&#30340;&#35782;&#21035;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.03281</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#35299;&#30721;mRNA&#30340;5' UTR&#35821;&#35328;&#27169;&#22411;&#21644;&#21151;&#33021;&#39044;&#27979;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A 5' UTR Language Model for Decoding Untranslated Regions of mRNA and Function Predictions. (arXiv:2310.03281v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03281
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31181;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;UTR-LM&#65292;&#36890;&#36807;&#23545;&#22810;&#20010;&#29289;&#31181;&#30340;5' UTR&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#32467;&#21512;&#26377;&#30417;&#30563;&#20449;&#24687;&#65292;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#26368;&#20339;&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#39044;&#27979;&#24179;&#22343;&#26680;&#31958;&#20307;&#36127;&#36733;&#12289;&#32763;&#35793;&#25928;&#29575;&#12289;mRNA&#34920;&#36798;&#27700;&#24179;&#65292;&#24182;&#25913;&#36827;&#20102;&#20869;&#28304;&#24615;&#26680;&#31958;&#20307;&#36827;&#20837;&#20301;&#28857;&#30340;&#35782;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
5' UTR&#26159;mRNA&#20998;&#23376;&#24320;&#31471;&#30340;&#35843;&#25511;&#21306;&#22495;&#65292;&#22312;&#35843;&#25511;&#32763;&#35793;&#36807;&#31243;&#21644;&#24433;&#21709;&#34507;&#30333;&#34920;&#36798;&#27700;&#24179;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#23637;&#31034;&#20102;&#22312;&#35299;&#30721;&#34507;&#30333;&#36136;&#21644;&#22522;&#22240;&#32452;&#24207;&#21015;&#21151;&#33021;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#29992;&#20110;5' UTR&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#31216;&#20026;UTR-LM&#12290;UTR-LM&#22312;&#22810;&#20010;&#29289;&#31181;&#30340;&#20869;&#28304;&#24615;5' UTR&#19978;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#65292;&#24182;&#36827;&#19968;&#27493;&#21152;&#20837;&#20102;&#21253;&#25324;&#20108;&#32423;&#32467;&#26500;&#21644;&#26368;&#23567;&#33258;&#30001;&#33021;&#22312;&#20869;&#30340;&#26377;&#30417;&#30563;&#20449;&#24687;&#12290;&#25105;&#20204;&#23545;UTR-LM&#36827;&#34892;&#20102;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;&#24494;&#35843;&#12290;&#27169;&#22411;&#22312;&#39044;&#27979;&#24179;&#22343;&#26680;&#31958;&#20307;&#36127;&#36733;&#19978;&#30340;&#34920;&#29616;&#36229;&#36807;&#20102;&#24050;&#30693;&#30340;&#26368;&#20339;&#22522;&#20934;&#27169;&#22411;&#26368;&#22810;42%&#65292;&#21516;&#26102;&#22312;&#39044;&#27979;&#32763;&#35793;&#25928;&#29575;&#21644;mRNA&#34920;&#36798;&#27700;&#24179;&#19978;&#30340;&#34920;&#29616;&#25552;&#21319;&#20102;&#26368;&#22810;60%&#12290;&#35813;&#27169;&#22411;&#36824;&#21487;&#20197;&#29992;&#20110;&#35782;&#21035;&#26410;&#27880;&#37322;&#30340;&#20869;&#28304;&#24615;&#26680;&#31958;&#20307;&#36827;&#20837;&#20301;&#28857;&#65292;&#24182;&#23558;AUPR&#19982;&#26368;&#20339;&#22522;&#20934;&#27169;&#22411;&#30456;&#27604;&#20174;0.37&#25552;&#39640;&#33267;0.52&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;...
&lt;/p&gt;
&lt;p&gt;
The 5' UTR, a regulatory region at the beginning of an mRNA molecule, plays a crucial role in regulating the translation process and impacts the protein expression level. Language models have showcased their effectiveness in decoding the functions of protein and genome sequences. Here, we introduced a language model for 5' UTR, which we refer to as the UTR-LM. The UTR-LM is pre-trained on endogenous 5' UTRs from multiple species and is further augmented with supervised information including secondary structure and minimum free energy. We fine-tuned the UTR-LM in a variety of downstream tasks. The model outperformed the best-known benchmark by up to 42% for predicting the Mean Ribosome Loading, and by up to 60% for predicting the Translation Efficiency and the mRNA Expression Level. The model also applies to identifying unannotated Internal Ribosome Entry Sites within the untranslated region and improves the AUPR from 0.37 to 0.52 compared to the best baseline. Further, we designed a li
&lt;/p&gt;</description></item><item><title>FedHyper&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#36890;&#29992;&#21644;&#31283;&#20581;&#23398;&#20064;&#29575;&#35843;&#24230;&#22120;&#65292;&#36890;&#36807;&#36229;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#23454;&#29616;&#23545;&#20840;&#23616;&#21644;&#23616;&#37096;&#23398;&#20064;&#29575;&#30340;&#33258;&#36866;&#24212;&#35843;&#25972;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#30340;&#25928;&#26524;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#23545;&#32463;&#39564;&#35843;&#25972;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2310.03156</link><description>&lt;p&gt;
FedHyper:&#19968;&#31181;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#36890;&#29992;&#21644;&#31283;&#20581;&#23398;&#20064;&#29575;&#35843;&#24230;&#22120;&#19982;&#36229;&#26799;&#24230;&#19979;&#38477;
&lt;/p&gt;
&lt;p&gt;
FedHyper: A Universal and Robust Learning Rate Scheduler for Federated Learning with Hypergradient Descent. (arXiv:2310.03156v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03156
&lt;/p&gt;
&lt;p&gt;
FedHyper&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#36890;&#29992;&#21644;&#31283;&#20581;&#23398;&#20064;&#29575;&#35843;&#24230;&#22120;&#65292;&#36890;&#36807;&#36229;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#23454;&#29616;&#23545;&#20840;&#23616;&#21644;&#23616;&#37096;&#23398;&#20064;&#29575;&#30340;&#33258;&#36866;&#24212;&#35843;&#25972;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#30340;&#25928;&#26524;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#23545;&#32463;&#39564;&#35843;&#25972;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#29702;&#35770;&#26694;&#26550;&#27491;&#22312;&#36805;&#36895;&#21457;&#23637;&#65292;&#20294;&#20854;&#23454;&#38469;&#24212;&#29992;&#38754;&#20020;&#19968;&#31995;&#21015;&#22797;&#26434;&#25361;&#25112;&#65292;&#20854;&#20013;&#36229;&#21442;&#25968;&#20248;&#21270;&#26159;&#20854;&#20013;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#22312;&#20247;&#22810;&#36229;&#21442;&#25968;&#35843;&#25972;&#20013;&#65292;&#23398;&#20064;&#29575;&#30340;&#36866;&#24212;&#24615;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#26377;&#26395;&#26174;&#33879;&#25552;&#39640;FL&#31995;&#32479;&#30340;&#25928;&#26524;&#12290;&#20026;&#20102;&#28385;&#36275;&#36825;&#19968;&#20851;&#38190;&#38656;&#27714;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;FedHyper&#65292;&#19968;&#31181;&#19987;&#20026;FL&#35774;&#35745;&#30340;&#22522;&#20110;&#36229;&#26799;&#24230;&#30340;&#23398;&#20064;&#29575;&#35843;&#25972;&#31639;&#27861;&#12290;FedHyper&#20316;&#20026;&#19968;&#31181;&#36890;&#29992;&#30340;&#23398;&#20064;&#29575;&#35843;&#24230;&#22120;&#65292;&#21487;&#20197;&#38543;&#30528;&#35757;&#32451;&#30340;&#36827;&#34892;&#35843;&#25972;&#20840;&#23616;&#21644;&#23616;&#37096;&#30340;&#23398;&#20064;&#29575;&#12290;&#27492;&#22806;&#65292;FedHyper&#19981;&#20165;&#23637;&#31034;&#20102;&#23545;&#21508;&#31181;&#21021;&#22987;&#23398;&#20064;&#29575;&#37197;&#32622;&#30340;&#26080;&#19982;&#20262;&#27604;&#30340;&#31283;&#20581;&#24615;&#65292;&#36824;&#26497;&#22823;&#22320;&#20943;&#23569;&#20102;&#32321;&#29712;&#30340;&#32463;&#39564;&#24615;&#23398;&#20064;&#29575;&#35843;&#25972;&#30340;&#24517;&#35201;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;FedHyper&#25910;&#25947;&#36895;&#24230;&#30340;&#20840;&#38754;&#29702;&#35770;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The theoretical landscape of federated learning (FL) undergoes rapid evolution, but its practical application encounters a series of intricate challenges, and hyperparameter optimization is one of these critical challenges. Amongst the diverse adjustments in hyperparameters, the adaptation of the learning rate emerges as a crucial component, holding the promise of significantly enhancing the efficacy of FL systems. In response to this critical need, this paper presents FedHyper, a novel hypergradient-based learning rate adaptation algorithm specifically designed for FL. FedHyper serves as a universal learning rate scheduler that can adapt both global and local rates as the training progresses. In addition, FedHyper not only showcases unparalleled robustness to a spectrum of initial learning rate configurations but also significantly alleviates the necessity for laborious empirical learning rate adjustments. We provide a comprehensive theoretical analysis of FedHyper's convergence rate 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#25968;&#25454;&#24402;&#22240;&#26041;&#27861;&#19982;&#27010;&#24565;&#25506;&#27979;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#20013;&#23398;&#20064;&#21040;&#30340;&#27010;&#24565;&#19982;&#35757;&#32451;&#25968;&#25454;&#30340;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#27010;&#24565;&#30340;&#20301;&#32622;&#21644;&#31232;&#30095;&#24615;&#24182;&#19981;&#23436;&#20840;&#20381;&#36182;&#20110;&#23569;&#37327;&#29305;&#23450;&#31034;&#20363;&#12290;</title><link>http://arxiv.org/abs/2310.03149</link><description>&lt;p&gt;
&#23558;&#31070;&#32463;&#32593;&#32476;&#20013;&#23398;&#20064;&#21040;&#30340;&#27010;&#24565;&#24402;&#22240;&#20110;&#35757;&#32451;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Attributing Learned Concepts in Neural Networks to Training Data. (arXiv:2310.03149v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03149
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#25968;&#25454;&#24402;&#22240;&#26041;&#27861;&#19982;&#27010;&#24565;&#25506;&#27979;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#20013;&#23398;&#20064;&#21040;&#30340;&#27010;&#24565;&#19982;&#35757;&#32451;&#25968;&#25454;&#30340;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#27010;&#24565;&#30340;&#20301;&#32622;&#21644;&#31232;&#30095;&#24615;&#24182;&#19981;&#23436;&#20840;&#20381;&#36182;&#20110;&#23569;&#37327;&#29305;&#23450;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#22312;&#26377;&#22823;&#37327;&#30340;&#35777;&#25454;&#34920;&#26126;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23398;&#20064;&#21040;&#20102;&#26576;&#20123;&#21487;&#35299;&#37322;&#30340;&#20154;&#31867;&#29305;&#24449;&#65292;&#20316;&#20026;&#20854;&#23545;&#25968;&#25454;&#30340;&#20869;&#37096;&#34920;&#31034;&#30340;&#19968;&#37096;&#20998;&#12290;&#30001;&#20110;&#25317;&#26377;&#27491;&#30830;&#65288;&#25110;&#38169;&#35823;&#65289;&#30340;&#27010;&#24565;&#23545;&#20110;&#21487;&#20449;&#36182;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#65292;&#33258;&#28982;&#32780;&#28982;&#22320;&#25105;&#20204;&#24819;&#35201;&#30693;&#36947;&#22312;&#32473;&#23450;&#23618;&#27425;&#19978;&#65292;&#27169;&#22411;&#21407;&#22987;&#35757;&#32451;&#38598;&#20013;&#30340;&#21738;&#20123;&#36755;&#20837;&#23545;&#20110;&#23398;&#20064;&#19968;&#20010;&#27010;&#24565;&#26368;&#20026;&#37325;&#35201;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#25968;&#25454;&#24402;&#22240;&#26041;&#27861;&#19982;&#25506;&#27979;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#27010;&#24565;&#30340;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;&#36890;&#36807;&#22312;&#19968;&#31995;&#21015;&#32593;&#32476;&#23618;&#27425;&#19978;&#35757;&#32451;&#32593;&#32476;&#21644;&#25506;&#27979;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#26368;&#36817;&#24320;&#21457;&#30340;&#29992;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#24402;&#22240;&#30340;TRAK&#26041;&#27861;&#65292;&#25105;&#20204;&#23545;&#20004;&#20010;&#27010;&#24565;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#32593;&#32476;&#21644;&#25506;&#27979;&#27169;&#22411;&#30340;&#38598;&#21512;&#12290;&#25105;&#20204;&#21457;&#29616;&#19968;&#20123;&#35777;&#25454;&#34920;&#26126;&#65292;&#36890;&#36807;&#31227;&#38500;&#23545;&#19968;&#20010;&#27010;&#24565;&#20855;&#26377;&#26368;&#39640;&#24402;&#22240;&#30340;&#21069;10000&#24352;&#22270;&#20687;&#24182;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#65292;&#27010;&#24565;&#22312;&#32593;&#32476;&#20013;&#30340;&#20301;&#32622;&#20197;&#21450;&#27010;&#24565;&#30340;&#25506;&#27979;&#31232;&#30095;&#24615;&#24182;&#27809;&#26377;&#21457;&#29983;&#25913;&#21464;&#12290;&#36825;&#34920;&#26126;&#65292;&#19982;&#20381;&#36182;&#20110;&#23569;&#37327;&#29305;&#23450;&#31034;&#20363;&#19981;&#21516;&#65292;&#29992;&#20110;&#30830;&#23450;&#27010;&#24565;&#30340;&#29305;&#24449;&#20855;&#26377;&#36739;&#39640;&#30340;&#29420;&#31435;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
By now there is substantial evidence that deep learning models learn certain human-interpretable features as part of their internal representations of data. As having the right (or wrong) concepts is critical to trustworthy machine learning systems, it is natural to ask which inputs from the model's original training set were most important for learning a concept at a given layer. To answer this, we combine data attribution methods with methods for probing the concepts learned by a model. Training network and probe ensembles for two concept datasets on a range of network layers, we use the recently developed TRAK method for large-scale data attribution. We find some evidence for convergence, where removing the 10,000 top attributing images for a concept and retraining the model does not change the location of the concept in the network nor the probing sparsity of the concept. This suggests that rather than being highly dependent on a few specific examples, the features that inform the 
&lt;/p&gt;</description></item><item><title>GPT-MolBERTa&#26159;&#19968;&#31181;&#29992;&#20110;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#30340;&#33258;&#30417;&#30563;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#20998;&#23376;&#30340;&#35814;&#32454;&#25991;&#26412;&#25551;&#36848;&#26469;&#23398;&#20064;&#20998;&#23376;&#30340;&#34920;&#31034;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#22312;&#21508;&#31181;&#20998;&#23376;&#23646;&#24615;&#22522;&#20934;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#22312;&#22238;&#24402;&#20219;&#21153;&#20013;&#25509;&#36817;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.03030</link><description>&lt;p&gt;
GPT-MolBERTa&#65306;&#29992;&#20110;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#30340;GPT&#20998;&#23376;&#29305;&#24449;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GPT-MolBERTa: GPT Molecular Features Language Model for molecular property prediction. (arXiv:2310.03030v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03030
&lt;/p&gt;
&lt;p&gt;
GPT-MolBERTa&#26159;&#19968;&#31181;&#29992;&#20110;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#30340;&#33258;&#30417;&#30563;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#20998;&#23376;&#30340;&#35814;&#32454;&#25991;&#26412;&#25551;&#36848;&#26469;&#23398;&#20064;&#20998;&#23376;&#30340;&#34920;&#31034;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#22312;&#21508;&#31181;&#20998;&#23376;&#23646;&#24615;&#22522;&#20934;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#22312;&#22238;&#24402;&#20219;&#21153;&#20013;&#25509;&#36817;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;Transformer&#26550;&#26500;&#30340;&#20986;&#29616;&#21450;&#20854;&#23545;&#25991;&#26412;&#25968;&#25454;&#30340;&#24378;&#22823;&#29702;&#35299;&#33021;&#21147;&#65292;&#22522;&#20110;&#25991;&#26412;&#25551;&#36848;&#39044;&#27979;&#20998;&#23376;&#23646;&#24615;&#30340;&#26032;&#39046;&#22495;&#24050;&#32463;&#24320;&#21551;&#12290;&#23613;&#31649;SMILES&#26159;&#26368;&#24120;&#35265;&#30340;&#34920;&#31034;&#24418;&#24335;&#65292;&#20294;&#23427;&#20204;&#32570;&#20047;&#20581;&#22766;&#24615;&#12289;&#20016;&#23500;&#20449;&#24687;&#21644;&#35268;&#33539;&#24615;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#25104;&#20026;&#21487;&#25512;&#24191;&#34920;&#31034;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GPT-MolBERTa&#65292;&#19968;&#31181;&#33258;&#30417;&#30563;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#23427;&#20351;&#29992;&#20998;&#23376;&#30340;&#35814;&#32454;&#25991;&#26412;&#25551;&#36848;&#26469;&#39044;&#27979;&#20854;&#24615;&#36136;&#12290;&#20351;&#29992;ChatGPT&#25910;&#38598;&#20102;326000&#20010;&#20998;&#23376;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#25551;&#36848;&#65292;&#24182;&#29992;&#20110;&#35757;&#32451;LLM&#26469;&#23398;&#20064;&#20998;&#23376;&#30340;&#34920;&#31034;&#12290;&#20026;&#20102;&#39044;&#27979;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#32454;&#35843;&#38454;&#27573;&#20351;&#29992;&#20102;BERT&#21644;RoBERTa&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;GPT-MolBERTa&#22312;&#21508;&#31181;&#20998;&#23376;&#23646;&#24615;&#22522;&#20934;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#22312;&#22238;&#24402;&#20219;&#21153;&#20013;&#25509;&#36817;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#23545;&#27880;&#24847;&#21147;&#36827;&#34892;&#20102;&#36827;&#19968;&#27493;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the emergence of Transformer architectures and their powerful understanding of textual data, a new horizon has opened up to predict the molecular properties based on text description. While SMILES are the most common form of representation, they are lacking robustness, rich information and canonicity, which limit their effectiveness in becoming generalizable representations. Here, we present GPT-MolBERTa, a self-supervised large language model (LLM) which uses detailed textual descriptions of molecules to predict their properties. A text based description of 326000 molecules were collected using ChatGPT and used to train LLM to learn the representation of molecules. To predict the properties for the downstream tasks, both BERT and RoBERTa models were used in the finetuning stage. Experiments show that GPT-MolBERTa performs well on various molecule property benchmarks, and approaching state of the art performance in regression tasks. Additionally, further analysis of the attention 
&lt;/p&gt;</description></item><item><title>Decision ConvFormer&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21160;&#20316;&#24207;&#21015;&#39044;&#27979;&#22120;&#65292;&#36890;&#36807;&#20351;&#29992;&#26412;&#22320;&#21367;&#31215;&#36807;&#28388;&#26469;&#25429;&#25417;&#24378;&#21270;&#23398;&#20064;&#25968;&#25454;&#38598;&#20013;&#30340;&#23616;&#37096;&#20851;&#32852;&#65292;&#21516;&#26102;&#22312;&#21508;&#20010;&#26631;&#20934;RL&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.03022</link><description>&lt;p&gt;
Decision ConvFormer: MetaFormer&#20013;&#30340;&#26412;&#22320;&#36807;&#28388;&#23545;&#20110;&#20915;&#31574;&#21046;&#23450;&#24050;&#32463;&#36275;&#22815;&#20102;
&lt;/p&gt;
&lt;p&gt;
Decision ConvFormer: Local Filtering in MetaFormer is Sufficient for Decision Making. (arXiv:2310.03022v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03022
&lt;/p&gt;
&lt;p&gt;
Decision ConvFormer&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21160;&#20316;&#24207;&#21015;&#39044;&#27979;&#22120;&#65292;&#36890;&#36807;&#20351;&#29992;&#26412;&#22320;&#21367;&#31215;&#36807;&#28388;&#26469;&#25429;&#25417;&#24378;&#21270;&#23398;&#20064;&#25968;&#25454;&#38598;&#20013;&#30340;&#23616;&#37096;&#20851;&#32852;&#65292;&#21516;&#26102;&#22312;&#21508;&#20010;&#26631;&#20934;RL&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#25104;&#21151;&#24341;&#21457;&#20102;&#20854;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;Decision Transformer&#65288;DT&#65289;&#20316;&#20026;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26377;&#21069;&#36884;&#30340;&#27169;&#22411;&#36880;&#28176;&#23853;&#38706;&#22836;&#35282;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;DT&#30340;&#27880;&#24847;&#21147;&#27169;&#22359;&#19981;&#36866;&#21512;&#25429;&#25417;&#20316;&#20026;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#24314;&#27169;&#30340;&#24378;&#21270;&#23398;&#20064;&#36712;&#36857;&#20013;&#22266;&#26377;&#30340;&#23616;&#37096;&#20381;&#36182;&#27169;&#24335;&#12290;&#20026;&#20102;&#35299;&#20915;DT&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;MetaFormer&#26550;&#26500;&#30340;&#26032;&#22411;&#21160;&#20316;&#24207;&#21015;&#39044;&#27979;&#22120;&#65292;&#31216;&#20026;Decision ConvFormer&#65288;DC&#65289;&#12290;DC&#37319;&#29992;&#26412;&#22320;&#21367;&#31215;&#36807;&#28388;&#20316;&#20026;&#20196;&#29260;&#28151;&#21512;&#22120;&#65292;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;RL&#25968;&#25454;&#38598;&#20013;&#22266;&#26377;&#30340;&#23616;&#37096;&#20851;&#32852;&#12290;&#22312;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20013;&#65292;DC&#22312;&#21508;&#31181;&#26631;&#20934;RL&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#38656;&#35201;&#26356;&#23569;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent success of Transformer in natural language processing has sparked its use in various domains. In offline reinforcement learning (RL), Decision Transformer (DT) is emerging as a promising model based on Transformer. However, we discovered that the attention module of DT is not appropriate to capture the inherent local dependence pattern in trajectories of RL modeled as a Markov decision process. To overcome the limitations of DT, we propose a novel action sequence predictor, named Decision ConvFormer (DC), based on the architecture of MetaFormer, which is a general structure to process multiple entities in parallel and understand the interrelationship among the multiple entities. DC employs local convolution filtering as the token mixer and can effectively capture the inherent local associations of the RL dataset. In extensive experiments, DC achieved state-of-the-art performance across various standard RL benchmarks while requiring fewer resources. Furthermore, we show that 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MedDiffusion&#30340;&#26032;&#22411;&#12289;&#31471;&#21040;&#31471;&#30340;&#25193;&#25955;&#24335;&#39118;&#38505;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#22522;&#20110;&#25193;&#25955;&#30340;&#25968;&#25454;&#22686;&#24378;&#65292;&#25552;&#21319;&#20102;&#20581;&#24247;&#39118;&#38505;&#39044;&#27979;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.02520</link><description>&lt;p&gt;
MedDiffusion: &#36890;&#36807;&#22522;&#20110;&#25193;&#25955;&#30340;&#25968;&#25454;&#22686;&#24378;&#25552;&#21319;&#20581;&#24247;&#39118;&#38505;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
MedDiffusion: Boosting Health Risk Prediction via Diffusion-based Data Augmentation. (arXiv:2310.02520v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MedDiffusion&#30340;&#26032;&#22411;&#12289;&#31471;&#21040;&#31471;&#30340;&#25193;&#25955;&#24335;&#39118;&#38505;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#22522;&#20110;&#25193;&#25955;&#30340;&#25968;&#25454;&#22686;&#24378;&#65292;&#25552;&#21319;&#20102;&#20581;&#24247;&#39118;&#38505;&#39044;&#27979;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20581;&#24247;&#39118;&#38505;&#39044;&#27979;&#26159;&#21307;&#23398;&#39046;&#22495;&#20013;&#22522;&#20110;&#39044;&#27979;&#24314;&#27169;&#30340;&#22522;&#26412;&#20219;&#21153;&#20043;&#19968;&#65292;&#26088;&#22312;&#21033;&#29992;&#21382;&#21490;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#26469;&#39044;&#27979;&#24739;&#32773;&#26410;&#26469;&#21487;&#33021;&#38754;&#20020;&#30340;&#28508;&#22312;&#20581;&#24247;&#39118;&#38505;&#12290;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#24320;&#21457;&#20102;&#20960;&#31181;&#39118;&#38505;&#39044;&#27979;&#27169;&#22411;&#26469;&#22788;&#29702;EHR&#25968;&#25454;&#30340;&#29420;&#29305;&#25361;&#25112;&#65292;&#20363;&#22914;&#20854;&#24207;&#21015;&#29305;&#24615;&#65292;&#39640;&#32500;&#24230;&#21644;&#22266;&#26377;&#22122;&#38899;&#12290;&#36825;&#20123;&#27169;&#22411;&#24050;&#32463;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#24433;&#21709;&#23427;&#20204;&#26377;&#25928;&#24615;&#30340;&#20851;&#38190;&#38382;&#39064;&#26159;&#25968;&#25454;&#19981;&#36275;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#21508;&#31181;&#25968;&#25454;&#29983;&#25104;&#21644;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#24213;&#23618;&#25968;&#25454;&#20998;&#24067;&#26469;&#25193;&#22823;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#24615;&#33021;&#24448;&#24448;&#21463;&#21040;&#20219;&#21153;&#26080;&#20851;&#35774;&#35745;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#32570;&#28857;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31471;&#21040;&#31471;&#25193;&#25955;&#24335;&#39118;&#38505;&#39044;&#27979;&#27169;&#22411;MedDiffusion&#65292;&#26469;&#22686;&#24378;&#39118;&#38505;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Health risk prediction is one of the fundamental tasks under predictive modeling in the medical domain, which aims to forecast the potential health risks that patients may face in the future using their historical Electronic Health Records (EHR). Researchers have developed several risk prediction models to handle the unique challenges of EHR data, such as its sequential nature, high dimensionality, and inherent noise. These models have yielded impressive results. Nonetheless, a key issue undermining their effectiveness is data insufficiency. A variety of data generation and augmentation methods have been introduced to mitigate this issue by expanding the size of the training data set through the learning of underlying data distributions. However, the performance of these methods is often limited due to their task-unrelated design. To address these shortcomings, this paper introduces a novel, end-to-end diffusion-based risk prediction model, named MedDiffusion. It enhances risk predicti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26426;&#23494;&#30340;&#25968;&#25454;&#36873;&#25321;&#21644;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#27969;&#31243;&#21644;&#31616;&#21270;&#30340;&#20302;&#32500;&#24230;&#25805;&#20316;&#26469;&#23454;&#29616;&#65292;&#20197;&#20445;&#25252;&#25968;&#25454;&#21644;&#27169;&#22411;&#30340;&#38544;&#31169;&#65292;&#24182;&#22312;&#22810;&#20010;Transformer&#27169;&#22411;&#21644;NLP/CV&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2310.02373</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#30340;&#23433;&#20840;&#26377;&#25928;&#25968;&#25454;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Secure and Effective Data Appraisal for Machine Learning. (arXiv:2310.02373v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26426;&#23494;&#30340;&#25968;&#25454;&#36873;&#25321;&#21644;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#27969;&#31243;&#21644;&#31616;&#21270;&#30340;&#20302;&#32500;&#24230;&#25805;&#20316;&#26469;&#23454;&#29616;&#65292;&#20197;&#20445;&#25252;&#25968;&#25454;&#21644;&#27169;&#22411;&#30340;&#38544;&#31169;&#65292;&#24182;&#22312;&#22810;&#20010;Transformer&#27169;&#22411;&#21644;NLP/CV&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#26080;&#25304;&#26080;&#26463;&#30340;&#25968;&#25454;&#24066;&#22330;&#38656;&#35201;&#22312;&#25968;&#25454;&#25152;&#26377;&#32773;&#21644;&#27169;&#22411;&#25152;&#26377;&#32773;&#26368;&#32456;&#20132;&#26131;&#21069;&#33021;&#22815;&#23545;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#31169;&#23494;&#36873;&#25321;&#21644;&#35780;&#20272;&#12290;&#20026;&#20102;&#20445;&#25252;&#25968;&#25454;&#21644;&#27169;&#22411;&#30340;&#38544;&#31169;&#65292;&#36825;&#20010;&#36807;&#31243;&#28041;&#21450;&#20351;&#29992;&#22810;&#26041;&#35745;&#31639;(MPC)&#26469;&#23457;&#26597;&#30446;&#26631;&#27169;&#22411;&#12290;&#23613;&#31649;&#20043;&#21069;&#30340;&#30740;&#31350;&#35748;&#20026;&#22522;&#20110;MPC&#30340;Transformer&#27169;&#22411;&#35780;&#20272;&#36807;&#20110;&#32791;&#36153;&#36164;&#28304;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#25968;&#25454;&#36873;&#25321;&#25104;&#20026;&#21487;&#34892;&#30340;&#12290;&#26412;&#30740;&#31350;&#30340;&#36129;&#29486;&#21253;&#25324;&#19977;&#20010;&#20851;&#38190;&#35201;&#32032;&#65306;(1)&#20351;&#29992;MPC&#36827;&#34892;&#26426;&#23494;&#25968;&#25454;&#36873;&#25321;&#30340;&#24320;&#21019;&#24615;&#27969;&#31243;&#65307;(2)&#36890;&#36807;&#22312;&#26377;&#38480;&#30340;&#30456;&#20851;&#25968;&#25454;&#23376;&#38598;&#19978;&#35757;&#32451;&#31616;&#21270;&#30340;&#20302;&#32500;&#24230;MLP&#26469;&#22797;&#21046;&#22797;&#26434;&#30340;&#39640;&#32500;&#24230;&#25805;&#20316;&#65307;(3)&#24182;&#21457;&#12289;&#22810;&#38454;&#27573;&#22320;&#23454;&#29616;MPC&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#19968;&#31995;&#21015;Transformer&#27169;&#22411;&#21644;NLP/CV&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#19982;&#30452;&#25509;&#22522;&#20110;MPC&#30340;&#35780;&#20272;&#30456;&#27604;
&lt;/p&gt;
&lt;p&gt;
Essential for an unfettered data market is the ability to discreetly select and evaluate training data before finalizing a transaction between the data owner and model owner. To safeguard the privacy of both data and model, this process involves scrutinizing the target model through Multi-Party Computation (MPC). While prior research has posited that the MPC-based evaluation of Transformer models is excessively resource-intensive, this paper introduces an innovative approach that renders data selection practical. The contributions of this study encompass three pivotal elements: (1) a groundbreaking pipeline for confidential data selection using MPC, (2) replicating intricate high-dimensional operations with simplified low-dimensional MLPs trained on a limited subset of pertinent data, and (3) implementing MPC in a concurrent, multi-phase manner. The proposed method is assessed across an array of Transformer models and NLP/CV benchmarks. In comparison to the direct MPC-based evaluation 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22797;&#26434;&#30340;Avalon&#28216;&#25103;&#20316;&#20026;&#27979;&#35797;&#24179;&#21488;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#36882;&#24402;&#24605;&#32771;&#65288;ReCon&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#35782;&#21035;&#21644;&#23545;&#25239;&#27450;&#39575;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.01320</link><description>&lt;p&gt;
Avalon&#30340;&#24605;&#32771;&#28216;&#25103;&#65306;&#36890;&#36807;&#36882;&#24402;&#24605;&#32771;&#23545;&#25239;&#27450;&#39575;
&lt;/p&gt;
&lt;p&gt;
Avalon's Game of Thoughts: Battle Against Deception through Recursive Contemplation. (arXiv:2310.01320v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01320
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22797;&#26434;&#30340;Avalon&#28216;&#25103;&#20316;&#20026;&#27979;&#35797;&#24179;&#21488;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#36882;&#24402;&#24605;&#32771;&#65288;ReCon&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#35782;&#21035;&#21644;&#23545;&#25239;&#27450;&#39575;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#31361;&#30772;&#24102;&#26469;&#20102;&#22312;LLM&#20316;&#20026;&#26234;&#33021;&#20307;&#39046;&#22495;&#30340;&#26174;&#33879;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#19968;&#31181;&#26222;&#36941;&#30340;&#20551;&#35774;&#26159;LLM&#22788;&#29702;&#30340;&#20449;&#24687;&#22987;&#32456;&#26159;&#35802;&#23454;&#30340;&#65292;&#24573;&#35270;&#20102;&#20154;&#31867;&#31038;&#20250;&#21644;AI&#29983;&#25104;&#20869;&#23481;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#27450;&#39575;&#25110;&#35823;&#23548;&#24615;&#20449;&#24687;&#12290;&#36825;&#20010;&#30095;&#24573;&#20351;&#24471;LLM&#23481;&#26131;&#21463;&#21040;&#24694;&#24847;&#25805;&#32437;&#65292;&#21487;&#33021;&#23548;&#33268;&#19981;&#21033;&#30340;&#32467;&#26524;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#22797;&#26434;&#30340;Avalon&#28216;&#25103;&#20316;&#20026;&#27979;&#35797;&#24179;&#21488;&#65292;&#25506;&#32034;LLM&#22312;&#27450;&#39575;&#29615;&#22659;&#20013;&#30340;&#28508;&#21147;&#12290;Avalon&#20805;&#28385;&#20102;&#38169;&#35823;&#20449;&#24687;&#65292;&#24182;&#38656;&#35201;&#22797;&#26434;&#30340;&#36923;&#36753;&#65292;&#34920;&#29616;&#20026;&#8220;&#24605;&#32771;&#30340;&#28216;&#25103;&#8221;&#12290;&#21463;&#21040;&#20154;&#31867;&#22312;Avalon&#28216;&#25103;&#20013;&#36882;&#24402;&#24605;&#32771;&#21644;&#36879;&#35270;&#33021;&#21147;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#8212;&#8212;&#36882;&#24402;&#24605;&#32771;&#65288;ReCon&#65289;&#65292;&#20197;&#22686;&#24378;LLM&#35782;&#21035;&#21644;&#23545;&#25239;&#27450;&#39575;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;ReCon&#32467;&#21512;&#20102;&#20844;&#24335;&#21270;&#24605;&#32771;&#21644;&#23436;&#21892;&#24605;&#32771;&#30340;&#36807;&#31243;&#65307;&#20844;&#24335;&#21270;&#24605;&#32771;&#20135;&#29983;&#21021;&#22987;&#24605;&#32771;&#65292;&#23436;&#21892;&#24605;&#32771;&#23545;&#21021;&#22987;&#24605;&#32771;&#36827;&#34892;&#35843;&#25972;&#21644;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent breakthroughs in large language models (LLMs) have brought remarkable success in the field of LLM-as-Agent. Nevertheless, a prevalent assumption is that the information processed by LLMs is consistently honest, neglecting the pervasive deceptive or misleading information in human society and AI-generated content. This oversight makes LLMs susceptible to malicious manipulations, potentially resulting in detrimental outcomes. This study utilizes the intricate Avalon game as a testbed to explore LLMs' potential in deceptive environments. Avalon, full of misinformation and requiring sophisticated logic, manifests as a "Game-of-Thoughts". Inspired by the efficacy of humans' recursive thinking and perspective-taking in the Avalon game, we introduce a novel framework, Recursive Contemplation (ReCon), to enhance LLMs' ability to identify and counteract deceptive information. ReCon combines formulation and refinement contemplation processes; formulation contemplation produces initial tho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;LLM&#27169;&#22411;&#36827;&#34892;&#20102;&#31995;&#32479;&#25506;&#32034;&#65292;&#20197;&#35299;&#20915;&#23545;&#36229;&#36807;&#19978;&#19979;&#25991;&#31383;&#21475;&#22823;&#23567;&#30340;&#20070;&#31821;&#36827;&#34892;&#25688;&#35201;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20004;&#31181;&#25552;&#31034;&#24037;&#20316;&#27969;&#23454;&#26045;&#20102;&#22522;&#20110;LLM&#30340;&#20070;&#31821;&#38271;&#24230;&#25688;&#35201;&#22120;&#30340;&#36830;&#36143;&#24615;&#30740;&#31350;&#12290;&#36890;&#36807;&#23545;100&#26412;&#20070;&#30340;GPT-4&#29983;&#25104;&#25688;&#35201;&#30340;&#20154;&#24037;&#27880;&#37322;&#65292;&#21457;&#29616;&#20102;&#20843;&#31181;&#24120;&#35265;&#30340;&#36830;&#36143;&#24615;&#38169;&#35823;&#12290;</title><link>http://arxiv.org/abs/2310.00785</link><description>&lt;p&gt;
BooookScore: LLM&#26102;&#20195;&#20013;&#23545;&#20070;&#31821;&#38271;&#24230;&#25688;&#35201;&#30340;&#31995;&#32479;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
BooookScore: A systematic exploration of book-length summarization in the era of LLMs. (arXiv:2310.00785v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;LLM&#27169;&#22411;&#36827;&#34892;&#20102;&#31995;&#32479;&#25506;&#32034;&#65292;&#20197;&#35299;&#20915;&#23545;&#36229;&#36807;&#19978;&#19979;&#25991;&#31383;&#21475;&#22823;&#23567;&#30340;&#20070;&#31821;&#36827;&#34892;&#25688;&#35201;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20004;&#31181;&#25552;&#31034;&#24037;&#20316;&#27969;&#23454;&#26045;&#20102;&#22522;&#20110;LLM&#30340;&#20070;&#31821;&#38271;&#24230;&#25688;&#35201;&#22120;&#30340;&#36830;&#36143;&#24615;&#30740;&#31350;&#12290;&#36890;&#36807;&#23545;100&#26412;&#20070;&#30340;GPT-4&#29983;&#25104;&#25688;&#35201;&#30340;&#20154;&#24037;&#27880;&#37322;&#65292;&#21457;&#29616;&#20102;&#20843;&#31181;&#24120;&#35265;&#30340;&#36830;&#36143;&#24615;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#36229;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19978;&#19979;&#25991;&#31383;&#21475;&#22823;&#23567;&#30340;&#20070;&#31821;&#38271;&#24230;&#25991;&#26723;&#65288;&gt;100K&#26631;&#35760;&#65289;&#36827;&#34892;&#25688;&#35201;&#38656;&#35201;&#39318;&#20808;&#23558;&#36755;&#20837;&#25991;&#26723;&#20998;&#25104;&#36739;&#23567;&#30340;&#22359;&#65292;&#28982;&#21518;&#25552;&#31034;LLM&#21512;&#24182;&#12289;&#26356;&#26032;&#21644;&#21387;&#32553;&#22359;&#32423;&#25688;&#35201;&#12290;&#23613;&#31649;&#36825;&#20010;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#21644;&#37325;&#35201;&#24615;&#65292;&#20294;&#30001;&#20110;&#35780;&#20272;&#30340;&#22256;&#38590;&#65292;&#23427;&#23578;&#26410;&#24471;&#21040;&#26377;&#24847;&#20041;&#30340;&#30740;&#31350;&#65306;&#29616;&#26377;&#30340;&#20070;&#31821;&#38271;&#24230;&#25688;&#35201;&#25968;&#25454;&#38598;&#65288;&#20363;&#22914;BookSum&#65289;&#22312;&#22823;&#22810;&#25968;&#20844;&#20849;LLM&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#65292;&#32780;&#29616;&#26377;&#30340;&#35780;&#20272;&#26041;&#27861;&#38590;&#20197;&#25429;&#25417;&#29616;&#20195;LLM&#25688;&#35201;&#22120;&#30340;&#38169;&#35823;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#30740;&#31350;&#36890;&#36807;&#20004;&#31181;&#25552;&#31034;&#24037;&#20316;&#27969;&#23454;&#26045;&#30340;&#22522;&#20110;LLM&#30340;&#20070;&#31821;&#38271;&#24230;&#25688;&#35201;&#22120;&#30340;&#36830;&#36143;&#24615;&#65306;&#65288;1&#65289;&#20998;&#23618;&#21512;&#24182;&#22359;&#32423;&#25688;&#35201;&#65292;&#65288;2&#65289;&#36880;&#27493;&#26356;&#26032;&#19968;&#20010;&#36816;&#34892;&#25688;&#35201;&#12290;&#25105;&#20204;&#23545;100&#26412;&#26368;&#36817;&#20986;&#29256;&#30340;&#20070;&#31821;&#30340;GPT-4&#29983;&#25104;&#25688;&#35201;&#33719;&#24471;&#20102;1193&#20010;&#32454;&#31890;&#24230;&#30340;&#20154;&#24037;&#27880;&#37322;&#65292;&#24182;&#30830;&#23450;&#20102;LLMs&#20135;&#29983;&#30340;&#20843;&#31181;&#24120;&#35265;&#30340;&#36830;&#36143;&#24615;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Summarizing book-length documents (&gt;100K tokens) that exceed the context window size of large language models (LLMs) requires first breaking the input document into smaller chunks and then prompting an LLM to merge, update, and compress chunk-level summaries. Despite the complexity and importance of this task, it has yet to be meaningfully studied due to the challenges of evaluation: existing book-length summarization datasets (e.g., BookSum) are in the pretraining data of most public LLMs, and existing evaluation methods struggle to capture errors made by modern LLM summarizers. In this paper, we present the first study of the coherence of LLM-based book-length summarizers implemented via two prompting workflows: (1) hierarchically merging chunk-level summaries, and (2) incrementally updating a running summary. We obtain 1193 fine-grained human annotations on GPT-4 generated summaries of 100 recently-published books and identify eight common types of coherence errors made by LLMs. Bec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#31163;&#32447;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#32780;&#19981;&#19968;&#23450;&#38656;&#35201;&#35821;&#35328;&#39044;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#19968;&#27493;&#39532;&#23572;&#31185;&#22827;&#38142;&#29983;&#25104;&#30340;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#36827;&#19968;&#27493;&#25913;&#21892;&#24615;&#33021;&#12290;&#22312;&#19968;&#20010;&#27969;&#34892;&#30340;&#31163;&#32447;DRL&#31639;&#27861;&#20013;&#65292;&#20351;&#29992;&#31616;&#21333;&#30340;&#39044;&#35757;&#32451;&#26041;&#26696;&#20063;&#33021;&#33719;&#24471;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.00771</link><description>&lt;p&gt;
&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#26377;&#21161;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Pre-training with Synthetic Data Helps Offline Reinforcement Learning. (arXiv:2310.00771v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00771
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#31163;&#32447;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#32780;&#19981;&#19968;&#23450;&#38656;&#35201;&#35821;&#35328;&#39044;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#19968;&#27493;&#39532;&#23572;&#31185;&#22827;&#38142;&#29983;&#25104;&#30340;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#36827;&#19968;&#27493;&#25913;&#21892;&#24615;&#33021;&#12290;&#22312;&#19968;&#20010;&#27969;&#34892;&#30340;&#31163;&#32447;DRL&#31639;&#27861;&#20013;&#65292;&#20351;&#29992;&#31616;&#21333;&#30340;&#39044;&#35757;&#32451;&#26041;&#26696;&#20063;&#33021;&#33719;&#24471;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#20110;&#31163;&#32447;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#35821;&#26009;&#24211;&#39044;&#35757;&#32451;Decision Transformer&#21487;&#20197;&#25552;&#39640;&#19979;&#28216;&#24615;&#33021;&#12290;&#19968;&#20010;&#33258;&#28982;&#30340;&#38382;&#39064;&#26159;&#65292;&#36825;&#31181;&#24615;&#33021;&#25552;&#21319;&#26159;&#21542;&#21482;&#33021;&#36890;&#36807;&#35821;&#35328;&#39044;&#35757;&#32451;&#23454;&#29616;&#65292;&#36824;&#26159;&#21487;&#20197;&#36890;&#36807;&#19981;&#28041;&#21450;&#35821;&#35328;&#30340;&#26356;&#31616;&#21333;&#30340;&#39044;&#35757;&#32451;&#26041;&#26696;&#23454;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#35821;&#35328;&#23545;&#20110;&#25913;&#21892;&#24615;&#33021;&#24182;&#19981;&#26159;&#24517;&#35201;&#30340;&#65292;&#23454;&#38469;&#19978;&#65292;&#20351;&#29992;&#21512;&#25104;&#30340;IID&#25968;&#25454;&#36827;&#34892;&#23569;&#37327;&#26356;&#26032;&#30340;&#39044;&#35757;&#32451;&#21487;&#20197;&#36798;&#21040;&#19982;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#35821;&#26009;&#24211;&#39044;&#35757;&#32451;&#30456;&#21305;&#37197;&#30340;&#24615;&#33021;&#25552;&#21319;&#65307;&#27492;&#22806;&#65292;&#20351;&#29992;&#19968;&#27493;&#39532;&#23572;&#31185;&#22827;&#38142;&#29983;&#25104;&#30340;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;&#21463;&#21040;&#36825;&#20123;&#23454;&#39564;&#32467;&#26524;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#32771;&#34385;&#20102;&#39044;&#35757;&#32451;Conservative Q-Learning(CQL)&#65292;&#36825;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#31163;&#32447;DRL&#31639;&#27861;&#65292;&#23427;&#22522;&#20110;Q-learning&#65292;&#24182;&#36890;&#24120;&#20351;&#29992;&#22810;&#23618;&#24863;&#30693;&#22120;(MLP)&#39592;&#24178;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#20351;&#29992;&#31616;&#21333;&#30340;&#39044;&#35757;&#32451;&#26041;&#26696;&#20063;&#33021;&#22312;CQL&#31639;&#27861;&#20013;&#21462;&#24471;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, it has been shown that for offline deep reinforcement learning (DRL), pre-training Decision Transformer with a large language corpus can improve downstream performance (Reid et al., 2022). A natural question to ask is whether this performance gain can only be achieved with language pre-training, or can be achieved with simpler pre-training schemes which do not involve language. In this paper, we first show that language is not essential for improved performance, and indeed pre-training with synthetic IID data for a small number of updates can match the performance gains from pre-training with a large language corpus; moreover, pre-training with data generated by a one-step Markov chain can further improve the performance. Inspired by these experimental results, we then consider pre-training Conservative Q-Learning (CQL), a popular offline DRL algorithm, which is Q-learning-based and typically employs a Multi-Layer Perceptron (MLP) backbone. Surprisingly, pre-training with sim
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#20855;&#26377;&#32593;&#32476;&#20107;&#25925;&#30340;&#30005;&#32593;&#20013;&#30340;&#30005;&#21147;&#27969;&#65292;&#24182;&#20272;&#35745;&#30005;&#21147;&#27969;&#30340;&#27010;&#29575;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#32593;&#32476;&#24863;&#30693;&#39640;&#26031;&#36807;&#31243;&#21644;&#22810;&#20219;&#21153;&#39030;&#28857;&#24230;&#26680;&#65292;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;&#26410;&#35265;&#32593;&#32476;&#30340;&#30005;&#21147;&#27969;&#25512;&#26029;&#65292;&#24182;&#22312;&#20302;&#35757;&#32451;&#25968;&#25454;&#24773;&#20917;&#19979;&#27604;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.00763</link><description>&lt;p&gt;
&#25968;&#25454;&#39640;&#25928;&#30340;&#32593;&#32476;&#20107;&#25925;&#30005;&#21147;&#27969;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Data-Efficient Power Flow Learning for Network Contingencies. (arXiv:2310.00763v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00763
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#20855;&#26377;&#32593;&#32476;&#20107;&#25925;&#30340;&#30005;&#32593;&#20013;&#30340;&#30005;&#21147;&#27969;&#65292;&#24182;&#20272;&#35745;&#30005;&#21147;&#27969;&#30340;&#27010;&#29575;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#32593;&#32476;&#24863;&#30693;&#39640;&#26031;&#36807;&#31243;&#21644;&#22810;&#20219;&#21153;&#39030;&#28857;&#24230;&#26680;&#65292;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;&#26410;&#35265;&#32593;&#32476;&#30340;&#30005;&#21147;&#27969;&#25512;&#26029;&#65292;&#24182;&#22312;&#20302;&#35757;&#32451;&#25968;&#25454;&#24773;&#20917;&#19979;&#27604;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#20855;&#26377;&#32593;&#32476;&#20107;&#25925;&#30340;&#30005;&#32593;&#20013;&#30340;&#30005;&#21147;&#27969;&#65292;&#24182;&#20272;&#35745;&#30456;&#24212;&#30340;&#27010;&#29575;&#24615;&#30005;&#21387;&#21253;&#32476;&#65288;PVE&#65289;&#12290;&#39318;&#20808;&#65292;&#20351;&#29992;&#21069;&#26399;&#30740;&#31350;&#20013;&#24320;&#21457;&#30340;&#32593;&#32476;&#24863;&#30693;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#65292;&#31216;&#20026;&#39030;&#28857;&#24230;&#26680;&#65288;VDK-GP&#65289;&#65292;&#26469;&#20272;&#35745;&#23569;&#25968;&#32593;&#32476;&#37197;&#32622;&#30340;&#30005;&#21387;-&#21151;&#29575;&#20989;&#25968;&#12290;&#25991;&#31456;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#20219;&#21153;&#39030;&#28857;&#24230;&#26680;&#65288;MT-VDK&#65289;&#65292;&#23558;&#23398;&#20064;&#21040;&#30340;VDK-GP&#27719;&#38598;&#22312;&#19968;&#36215;&#65292;&#20197;&#30830;&#23450;&#26410;&#35265;&#32593;&#32476;&#30340;&#30005;&#21147;&#27969;&#65292;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#36229;&#21442;&#25968;&#35201;&#27714;&#26174;&#33879;&#38477;&#20302;&#12290;&#22312;IEEE 30-Bus&#32593;&#32476;&#19978;&#30340;&#27169;&#25311;&#23454;&#39564;&#20013;&#65292;&#22312;N-1&#21644;N-2&#20107;&#25925;&#24773;&#20917;&#19979;&#34920;&#26126;&#20102;&#30005;&#21147;&#27969;&#30693;&#35782;&#30340;&#20445;&#30041;&#21644;&#20256;&#36882;&#12290;&#22312;&#20302;&#35757;&#32451;&#25968;&#25454;&#24773;&#20917;&#19979;&#65288;50-250&#20010;&#26679;&#26412;&#65289;&#65292;MT-VDK-GP&#26041;&#27861;&#22312;&#26032;&#39062;&#30340;N-1&#20107;&#25925;&#32593;&#32476;&#37197;&#32622;&#19978;&#30340;&#24179;&#22343;&#39044;&#27979;&#35823;&#24046;&#36739;VDK-GP&#20943;&#23569;&#20102;50%&#20197;&#19978;&#12290;&#27492;&#22806;&#65292;MT-VDK-GP&#22312;N-1&#20107;&#25925;&#32593;&#32476;&#37197;&#32622;&#30340;&#20302;&#35757;&#32451;&#25968;&#25454;&#24773;&#20917;&#19979;&#65288;50-250&#20010;&#26679;&#26412;&#65289;&#36798;&#21040;&#20102;&#24179;&#22343;&#39044;&#27979;&#35823;&#24046;&#38477;&#20302;50%&#20197;&#19978;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work presents an efficient data-driven method to learn power flows in grids with network contingencies and to estimate corresponding probabilistic voltage envelopes (PVE). First, a network-aware Gaussian process (GP) termed Vertex-Degree Kernel (VDK-GP), developed in prior work, is used to estimate voltage-power functions for a few network configurations. The paper introduces a novel multi-task vertex degree kernel (MT-VDK) that amalgamates the learned VDK-GPs to determine power flows for unseen networks, with a significant reduction in the computational complexity and hyperparameter requirements compared to alternate approaches. Simulations on the IEEE 30-Bus network demonstrate the retention and transfer of power flow knowledge in both N-1 and N-2 contingency scenarios. The MT-VDK-GP approach achieves over 50% reduction in mean prediction error for novel N-1 contingency network configurations in low training data regimes (50-250 samples) over VDK-GP. Additionally, MT-VDK-GP outp
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#25552;&#39640;&#20102;&#20869;&#23384;&#20013;&#24694;&#24847;&#36719;&#20214;&#30340;&#20998;&#31867;&#25928;&#26524;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#19977;&#31181;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#20174;&#20869;&#23384;&#20869;&#23481;&#20013;&#35782;&#21035;&#20986;&#37325;&#35201;&#29305;&#24449;&#65292;&#24182;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;&#22810;&#26679;&#30340;&#20998;&#31867;&#22120;&#20013;&#65292;&#23454;&#29616;&#20102;&#24694;&#24847;&#36719;&#20214;&#30340;&#20108;&#20803;&#20998;&#31867;&#12289;&#31867;&#22411;&#20998;&#31867;&#21644;&#23478;&#26063;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2310.00516</link><description>&lt;p&gt;
&#36890;&#36807;&#29305;&#24449;&#36873;&#25321;&#25552;&#39640;&#20869;&#23384;&#24694;&#24847;&#36719;&#20214;&#20998;&#31867;&#30340;&#25928;&#29575;&#21644;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
Enhancing Efficiency and Privacy in Memory-Based Malware Classification through Feature Selection. (arXiv:2310.00516v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#25552;&#39640;&#20102;&#20869;&#23384;&#20013;&#24694;&#24847;&#36719;&#20214;&#30340;&#20998;&#31867;&#25928;&#26524;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#19977;&#31181;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#20174;&#20869;&#23384;&#20869;&#23481;&#20013;&#35782;&#21035;&#20986;&#37325;&#35201;&#29305;&#24449;&#65292;&#24182;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;&#22810;&#26679;&#30340;&#20998;&#31867;&#22120;&#20013;&#65292;&#23454;&#29616;&#20102;&#24694;&#24847;&#36719;&#20214;&#30340;&#20108;&#20803;&#20998;&#31867;&#12289;&#31867;&#22411;&#20998;&#31867;&#21644;&#23478;&#26063;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24694;&#24847;&#36719;&#20214;&#36890;&#36807;&#30772;&#22351;&#31995;&#32479;&#21644;&#25968;&#25454;&#32473;&#20010;&#20154;&#12289;&#32452;&#32455;&#21644;&#20851;&#38190;&#22522;&#30784;&#35774;&#26045;&#24102;&#26469;&#20005;&#37325;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;&#21033;&#29992;&#25552;&#20379;&#35745;&#31639;&#26426;&#20869;&#23384;&#24555;&#29031;&#30340;&#20869;&#23384;&#36716;&#20648;&#21487;&#20197;&#24110;&#21161;&#20998;&#26512;&#21644;&#26816;&#27979;&#24694;&#24847;&#20869;&#23481;&#65292;&#21253;&#25324;&#24694;&#24847;&#36719;&#20214;&#12290;&#20026;&#20102;&#25552;&#39640;&#24694;&#24847;&#36719;&#20214;&#20998;&#31867;&#31995;&#32479;&#30340;&#25928;&#21147;&#24182;&#35299;&#20915;&#38544;&#31169;&#38382;&#39064;&#65292;&#29305;&#24449;&#36873;&#25321;&#21487;&#20197;&#36215;&#21040;&#20851;&#38190;&#20316;&#29992;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#35782;&#21035;&#20986;&#26368;&#30456;&#20851;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#26368;&#23567;&#21270;&#20998;&#31867;&#22120;&#25152;&#38656;&#30340;&#25968;&#25454;&#37327;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#19977;&#31181;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#20174;&#20869;&#23384;&#20869;&#23481;&#20013;&#35782;&#21035;&#20986;&#37325;&#35201;&#29305;&#24449;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#22810;&#26679;&#30340;&#20998;&#31867;&#22120;&#19968;&#36215;&#20351;&#29992;&#65292;&#20197;&#25552;&#39640;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#32423;&#21035;&#30340;&#24694;&#24847;&#36719;&#20214;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#65306;i&#65289;&#20108;&#20803;&#32423;&#21035;&#30340;&#33391;&#24615;&#36719;&#20214;&#25110;&#24694;&#24847;&#36719;&#20214;&#20998;&#31867;&#65292;ii&#65289;&#24694;&#24847;&#36719;&#20214;&#31867;&#22411;&#20998;&#31867;&#65288;&#21253;&#25324;&#26408;&#39532;&#12289;&#21202;&#32034;&#36719;&#20214;&#21644;&#38388;&#35853;&#36719;&#20214;&#65289;&#65292;&#21644;iii&#65289;&#24694;&#24847;&#36719;&#20214;&#23478;&#26063;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Malware poses a significant security risk to individuals, organizations, and critical infrastructure by compromising systems and data. Leveraging memory dumps that offer snapshots of computer memory can aid the analysis and detection of malicious content, including malware. To improve the efficacy and address privacy concerns in malware classification systems, feature selection can play a critical role as it is capable of identifying the most relevant features, thus, minimizing the amount of data fed to classifiers. In this study, we employ three feature selection approaches to identify significant features from memory content and use them with a diverse set of classifiers to enhance the performance and privacy of the classification task. Comprehensive experiments are conducted across three levels of malware classification tasks: i) binary-level benign or malware classification, ii) malware type classification (including Trojan horse, ransomware, and spyware), and iii) malware family c
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#29992;&#20110;&#28151;&#21512;&#36793;&#30028;&#26465;&#20214;&#30340;&#27850;&#26494;&#26041;&#31243;&#30340;&#31070;&#32463;&#39044;&#22788;&#29702;&#36845;&#20195;&#27714;&#35299;&#22120;&#65292;&#26680;&#24515;&#26159;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#36817;&#20284;&#36870;&#31163;&#25955;&#32467;&#26500;&#32593;&#26684;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#38598;&#20043;&#22806;&#30340;&#36793;&#30028;&#26465;&#20214;&#19979;&#20173;&#28982;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2310.00177</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#28151;&#21512;&#36842;&#37324;&#20999;&#29305;&#21644;&#35834;&#26364;&#36793;&#30028;&#26465;&#20214;&#30340;&#31070;&#32463;&#39044;&#22788;&#29702;&#27850;&#26494;&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Neural-preconditioned Poisson Solver for Mixed Dirichlet and Neumann Boundary Conditions. (arXiv:2310.00177v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00177
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#29992;&#20110;&#28151;&#21512;&#36793;&#30028;&#26465;&#20214;&#30340;&#27850;&#26494;&#26041;&#31243;&#30340;&#31070;&#32463;&#39044;&#22788;&#29702;&#36845;&#20195;&#27714;&#35299;&#22120;&#65292;&#26680;&#24515;&#26159;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#36817;&#20284;&#36870;&#31163;&#25955;&#32467;&#26500;&#32593;&#26684;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#38598;&#20043;&#22806;&#30340;&#36793;&#30028;&#26465;&#20214;&#19979;&#20173;&#28982;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31070;&#32463;&#39044;&#22788;&#29702;&#30340;&#36845;&#20195;&#27714;&#35299;&#22120;&#65292;&#29992;&#20110;&#20855;&#26377;&#28151;&#21512;&#36793;&#30028;&#26465;&#20214;&#30340;&#27850;&#26494;&#26041;&#31243;&#12290;&#27850;&#26494;&#26041;&#31243;&#22312;&#31185;&#23398;&#35745;&#31639;&#20013;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#65306;&#23427;&#25511;&#21046;&#30528;&#24191;&#27867;&#30340;&#29289;&#29702;&#29616;&#35937;&#65292;&#22312;&#35768;&#22810;&#25968;&#20540;&#31639;&#27861;&#20013;&#20316;&#20026;&#23376;&#38382;&#39064;&#20986;&#29616;&#65292;&#24182;&#19988;&#20316;&#20026;&#26356;&#24191;&#27867;&#30340;&#26925;&#22278;PDE&#31867;&#30340;&#27169;&#22411;&#38382;&#39064;&#12290;&#26368;&#27969;&#34892;&#30340;&#27850;&#26494;&#31163;&#25955;&#21270;&#26041;&#27861;&#21487;&#20197;&#20135;&#29983;&#22823;&#22411;&#31232;&#30095;&#32447;&#24615;&#31995;&#32479;&#12290;&#22312;&#39640;&#20998;&#36776;&#29575;&#21644;&#23545;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#30340;&#24212;&#29992;&#20013;&#65292;&#36845;&#20195;&#27714;&#35299;&#22120;&#32467;&#21512;&#24378;&#22823;&#30340;&#39044;&#22788;&#29702;&#22120;&#21487;&#20197;&#25552;&#20379;&#20248;&#21183;&#12290;&#25105;&#20204;&#27714;&#35299;&#22120;&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#32463;&#36807;&#35757;&#32451;&#21487;&#20197;&#36817;&#20284;&#31163;&#25955;&#32467;&#26500;&#32593;&#26684;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#30340;&#36870;&#31639;&#23376;&#65292;&#36866;&#29992;&#20110;&#20219;&#24847;&#24418;&#29366;&#30340;&#22495;&#21644;&#28151;&#21512;&#36793;&#30028;&#26465;&#20214;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#38382;&#39064;&#30340;&#32467;&#26500;&#28608;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#21363;&#20351;&#22312;&#35757;&#32451;&#38598;&#20043;&#22806;&#30340;&#36793;&#30028;&#26465;&#20214;&#19979;&#65292;&#35813;&#26550;&#26500;&#20063;&#34920;&#29616;&#20986;&#39640;&#25928;&#30340;&#39044;&#22788;&#29702;&#22120;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#27979;&#35797;&#26696;&#20363;&#19978;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a neural-preconditioned iterative solver for Poisson equations with mixed boundary conditions. The Poisson equation is ubiquitous in scientific computing: it governs a wide array of physical phenomena, arises as a subproblem in many numerical algorithms, and serves as a model problem for the broader class of elliptic PDEs. The most popular Poisson discretizations yield large sparse linear systems. At high resolution, and for performance-critical applications, iterative solvers can be advantageous for these -- but only when paired with powerful preconditioners. The core of our solver is a neural network trained to approximate the inverse of a discrete structured-grid Laplace operator for a domain of arbitrary shape and with mixed boundary conditions. The structure of this problem motivates a novel network architecture that we demonstrate is highly effective as a preconditioner even for boundary conditions outside the training set. We show that on challenging test cases aris
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ChannelViT&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;ViT&#26550;&#26500;&#30340;&#20462;&#25913;&#21644;&#24341;&#20837;&#20998;&#23618;&#36890;&#36947;&#37319;&#26679;&#25216;&#26415;&#65292;&#22686;&#24378;&#20102;&#23545;&#22810;&#36890;&#36947;&#22270;&#20687;&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#65292;&#36866;&#29992;&#20110;&#26174;&#24494;&#38236;&#21644;&#21355;&#26143;&#25104;&#20687;&#31561;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2309.16108</link><description>&lt;p&gt;
&#39057;&#36947;&#35270;&#35273;Transformer&#65306;&#19968;&#24352;&#22270;&#20540;C x 16 x 16&#20010;&#35789;
&lt;/p&gt;
&lt;p&gt;
Channel Vision Transformers: An Image Is Worth C x 16 x 16 Words. (arXiv:2309.16108v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16108
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ChannelViT&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;ViT&#26550;&#26500;&#30340;&#20462;&#25913;&#21644;&#24341;&#20837;&#20998;&#23618;&#36890;&#36947;&#37319;&#26679;&#25216;&#26415;&#65292;&#22686;&#24378;&#20102;&#23545;&#22810;&#36890;&#36947;&#22270;&#20687;&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#65292;&#36866;&#29992;&#20110;&#26174;&#24494;&#38236;&#21644;&#21355;&#26143;&#25104;&#20687;&#31561;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;Transformer&#22312;&#29616;&#20195;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#20013;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#26550;&#26500;&#12290;&#28982;&#32780;&#65292;&#23427;&#22312;&#26576;&#20123;&#22270;&#20687;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#22914;&#26174;&#24494;&#38236;&#21644;&#21355;&#26143;&#25104;&#20687;&#65292;&#38754;&#20020;&#30528;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#20123;&#39046;&#22495;&#20013;&#65292;&#22270;&#20687;&#36890;&#24120;&#21253;&#21547;&#22810;&#20010;&#36890;&#36947;&#65292;&#27599;&#20010;&#36890;&#36947;&#37117;&#25658;&#24102;&#30528;&#35821;&#20041;&#19978;&#19981;&#21516;&#21644;&#29420;&#31435;&#30340;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#24517;&#39035;&#23545;&#36755;&#20837;&#36890;&#36947;&#30340;&#31232;&#30095;&#24615;&#34920;&#29616;&#20986;&#40065;&#26834;&#24615;&#65292;&#22312;&#35757;&#32451;&#25110;&#27979;&#35797;&#36807;&#31243;&#20013;&#21487;&#33021;&#27809;&#26377;&#23494;&#38598;&#21487;&#29992;&#30340;&#36890;&#36947;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;ViT&#26550;&#26500;&#30340;&#20462;&#25913;&#65292;&#22686;&#24378;&#20102;&#23545;&#36755;&#20837;&#36890;&#36947;&#20043;&#38388;&#30340;&#25512;&#29702;&#65292;&#24182;&#24341;&#20837;&#20102;&#20998;&#23618;&#36890;&#36947;&#37319;&#26679;(HCS)&#20316;&#20026;&#19968;&#31181;&#38468;&#21152;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#20197;&#30830;&#20445;&#22312;&#27979;&#35797;&#36807;&#31243;&#20013;&#20165;&#20986;&#29616;&#37096;&#20998;&#36890;&#36947;&#26102;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;ChannelViT&#29420;&#31435;&#22320;&#26500;&#24314;&#34917;&#19969;&#20196;&#29260;&#24182;&#21033;&#29992;&#21487;&#23398;&#20064;&#30340;&#36890;&#36947;&#23884;&#20837;&#23558;&#20854;&#28155;&#21152;&#21040;&#34917;&#19969;&#20196;&#29260;&#20013;&#65292;&#31867;&#20284;&#20110;&#20301;&#32622;&#23884;&#20837;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Vision Transformer (ViT) has emerged as a powerful architecture in the realm of modern computer vision. However, its application in certain imaging fields, such as microscopy and satellite imaging, presents unique challenges. In these domains, images often contain multiple channels, each carrying semantically distinct and independent information. Furthermore, the model must demonstrate robustness to sparsity in input channels, as they may not be densely available during training or testing. In this paper, we propose a modification to the ViT architecture that enhances reasoning across the input channels and introduce Hierarchical Channel Sampling (HCS) as an additional regularization technique to ensure robustness when only partial channels are presented during test time. Our proposed model, ChannelViT, constructs patch tokens independently from each input channel and utilizes a learnable channel embedding that is added to the patch tokens, similar to positional embeddings. We evaluate
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;Transformer&#30340;UPTST&#27169;&#22411;&#65292;&#21033;&#29992;&#33133;&#21693;&#21475;&#28814;&#25968;&#25454;&#25552;&#21319;&#25163;&#36275;&#21475;&#30149;&#20303;&#38498;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#19988;&#22312;&#21307;&#38498;&#32423;&#21035;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.14674</link><description>&lt;p&gt;
&#21033;&#29992;&#33133;&#21693;&#21475;&#28814;&#25968;&#25454;&#25552;&#21319;&#22522;&#20110;UPTST&#30340;&#25163;&#36275;&#21475;&#30149;&#20303;&#38498;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Leveraging Herpangina Data to Enhance Hospital-level Prediction of Hand-Foot-and-Mouth Disease Admissions Using UPTST. (arXiv:2309.14674v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14674
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;Transformer&#30340;UPTST&#27169;&#22411;&#65292;&#21033;&#29992;&#33133;&#21693;&#21475;&#28814;&#25968;&#25454;&#25552;&#21319;&#25163;&#36275;&#21475;&#30149;&#20303;&#38498;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#19988;&#22312;&#21307;&#38498;&#32423;&#21035;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#36275;&#21475;&#30149;&#65288;HFMD&#65289;&#29190;&#21457;&#19982;&#20005;&#37325;&#30340;&#21457;&#30149;&#29575;&#21644;&#27515;&#20129;&#29575;&#30456;&#20851;&#12290;&#22240;&#27492;&#65292;&#20934;&#30830;&#39044;&#27979;&#20799;&#31185;HFMD&#24739;&#32773;&#30340;&#27599;&#26085;&#20303;&#38498;&#20154;&#25968;&#23545;&#20110;&#21327;&#21161;&#21307;&#38498;&#24212;&#23545;&#28508;&#22312;&#30340;&#29190;&#21457;&#21644;&#20943;&#23569;&#21307;&#38498;&#20869;&#20256;&#25773;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#36843;&#20999;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#23427;&#20855;&#26377;U-net&#24418;&#29366;&#65292;&#24182;&#21033;&#29992;&#20102;&#19982;HFMD&#23494;&#20999;&#30456;&#20851;&#30340;&#33133;&#21693;&#21475;&#28814;&#30340;&#35265;&#35299;&#12290;&#35813;&#27169;&#22411;&#36824;&#36890;&#36807;&#24341;&#20837;&#37325;&#26500;&#25439;&#22833;&#20316;&#20026;&#36741;&#21161;&#25439;&#22833;&#26469;&#25972;&#21512;&#34920;&#31034;&#23398;&#20064;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;UPTST&#27169;&#22411;&#22312;&#21307;&#38498;&#32423;&#21035;&#30340;HFMD&#38271;&#30701;&#33218;&#39044;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25506;&#32034;&#24615;&#30340;&#25193;&#23637;&#23454;&#39564;&#34920;&#26126;&#35813;&#27169;&#22411;&#30340;&#33021;&#21147;&#36229;&#20986;&#20102;&#20256;&#26579;&#30149;&#30340;&#39044;&#27979;&#65292;&#25552;&#31034;...
&lt;/p&gt;
&lt;p&gt;
Outbreaks of hand-foot-and-mouth disease(HFMD) have been associated with significant morbidity and, in severe cases, mortality. Accurate forecasting of daily admissions of pediatric HFMD patients is therefore crucial for aiding the hospital in preparing for potential outbreaks and mitigating nosocomial transmissions. To address this pressing need, we propose a novel transformer-based model with a U-net shape, utilizing the patching strategy and the joint prediction strategy that capitalizes on insights from herpangina, a disease closely correlated with HFMD. This model also integrates representation learning by introducing reconstruction loss as an auxiliary loss. The results show that our U-net Patching Time Series Transformer (UPTST) model outperforms existing approaches in both long- and short-arm prediction accuracy of HFMD at hospital-level. Furthermore, the exploratory extension experiments show that the model's capabilities extend beyond prediction of infectious disease, suggest
&lt;/p&gt;</description></item><item><title>NAS-NeRF&#26159;&#19968;&#31181;&#29983;&#25104;&#24335;&#31070;&#32463;&#20307;&#31995;&#32467;&#26500;&#25628;&#32034;&#31574;&#30053;&#65292;&#36890;&#36807;&#24179;&#34913;&#26550;&#26500;&#22797;&#26434;&#24230;&#21644;&#30446;&#26631;&#21512;&#25104;&#36136;&#37327;&#25351;&#26631;&#29983;&#25104;&#32039;&#20945;&#12289;&#38024;&#23545;&#22330;&#26223;&#30340;NeRF&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2309.14293</link><description>&lt;p&gt;
NAS-NeRF: &#29992;&#20110;&#31070;&#32463;&#36752;&#23556;&#22330;&#30340;&#29983;&#25104;&#24335;&#31070;&#32463;&#20307;&#31995;&#32467;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
NAS-NeRF: Generative Neural Architecture Search for Neural Radiance Fields. (arXiv:2309.14293v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14293
&lt;/p&gt;
&lt;p&gt;
NAS-NeRF&#26159;&#19968;&#31181;&#29983;&#25104;&#24335;&#31070;&#32463;&#20307;&#31995;&#32467;&#26500;&#25628;&#32034;&#31574;&#30053;&#65292;&#36890;&#36807;&#24179;&#34913;&#26550;&#26500;&#22797;&#26434;&#24230;&#21644;&#30446;&#26631;&#21512;&#25104;&#36136;&#37327;&#25351;&#26631;&#29983;&#25104;&#32039;&#20945;&#12289;&#38024;&#23545;&#22330;&#26223;&#30340;NeRF&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRF&#65289;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#26032;&#35270;&#22270;&#21512;&#25104;&#65292;&#20294;&#20854;&#39640;&#35745;&#31639;&#22797;&#26434;&#24230;&#38480;&#21046;&#20102;&#20854;&#21487;&#37096;&#32626;&#24615;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#35299;&#20915;&#26041;&#26696;&#21162;&#21147;&#25552;&#39640;&#25928;&#29575;&#65292;&#20294;&#19981;&#32771;&#34385;&#22330;&#26223;&#22797;&#26434;&#24615;&#65292;&#20351;&#29992;&#36890;&#29992;&#26550;&#26500;&#12290;&#21516;&#19968;&#20010;&#26550;&#26500;&#21487;&#33021;&#23545;&#31616;&#21333;&#22330;&#26223;&#26469;&#35828;&#36807;&#20110;&#24222;&#22823;&#65292;&#23545;&#22797;&#26434;&#22330;&#26223;&#21017;&#19981;&#36275;&#22815;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#21160;&#24577;&#20248;&#21270;NeRF&#30340;&#31070;&#32463;&#32593;&#32476;&#32452;&#20214;&#65292;&#20197;&#22312;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#21512;&#25104;&#36136;&#37327;&#20043;&#38388;&#23454;&#29616;&#24179;&#34913;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;NAS-NeRF&#65292;&#19968;&#31181;&#29983;&#25104;&#24335;&#31070;&#32463;&#20307;&#31995;&#32467;&#26500;&#25628;&#32034;&#31574;&#30053;&#65292;&#36890;&#36807;&#24179;&#34913;&#26550;&#26500;&#22797;&#26434;&#24230;&#21644;&#30446;&#26631;&#21512;&#25104;&#36136;&#37327;&#25351;&#26631;&#29983;&#25104;&#32039;&#20945;&#12289;&#38024;&#23545;&#22330;&#26223;&#30340;NeRF&#26550;&#26500;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#30446;&#26631;&#24230;&#37327;&#21644;&#39044;&#31639;&#32422;&#26463;&#65292;&#25351;&#23548;&#25628;&#32034;&#20197;&#33719;&#24471;&#36866;&#21512;&#27599;&#20010;&#22330;&#26223;&#30340;&#26550;&#26500;&#12290;&#22312;Blender&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25552;&#20986;&#30340;NAS-NeRF&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#22810;&#36798;5&#20010;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural radiance fields (NeRFs) enable high-quality novel view synthesis, but their high computational complexity limits deployability. While existing neural-based solutions strive for efficiency, they use one-size-fits-all architectures regardless of scene complexity. The same architecture may be unnecessarily large for simple scenes but insufficient for complex ones. Thus, there is a need to dynamically optimize the neural network component of NeRFs to achieve a balance between computational complexity and specific targets for synthesis quality. We introduce NAS-NeRF, a generative neural architecture search strategy that generates compact, scene-specialized NeRF architectures by balancing architecture complexity and target synthesis quality metrics. Our method incorporates constraints on target metrics and budgets to guide the search towards architectures tailored for each scene. Experiments on the Blender synthetic dataset show the proposed NAS-NeRF can generate architectures up to 5
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20013;&#30340;&#27169;&#24335;&#22604;&#38519;&#38382;&#39064;&#23545;&#21512;&#25104;X&#23556;&#32447;&#22270;&#20687;&#22810;&#26679;&#24615;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#23558;&#33258;&#36866;&#24212;&#36755;&#20837;&#22270;&#20687;&#24402;&#19968;&#21270;&#26041;&#27861;&#19982;&#28145;&#24230;&#27169;&#22411;&#32467;&#21512;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2309.12245</link><description>&lt;p&gt;
&#36866;&#24212;&#24615;&#36755;&#20837;&#22270;&#20687;&#24402;&#19968;&#21270;&#26041;&#27861;&#35299;&#20915;&#22522;&#20110;GAN&#30340;X&#23556;&#32447;&#22270;&#20687;&#30340;&#27169;&#24335;&#22604;&#38519;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Adaptive Input-image Normalization for Solving Mode Collapse Problem in GAN-based X-ray Images. (arXiv:2309.12245v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12245
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20013;&#30340;&#27169;&#24335;&#22604;&#38519;&#38382;&#39064;&#23545;&#21512;&#25104;X&#23556;&#32447;&#22270;&#20687;&#22810;&#26679;&#24615;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#23558;&#33258;&#36866;&#24212;&#36755;&#20837;&#22270;&#20687;&#24402;&#19968;&#21270;&#26041;&#27861;&#19982;&#28145;&#24230;&#27169;&#22411;&#32467;&#21512;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#30142;&#30149;&#30340;&#32597;&#35265;&#24615;&#65292;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#38598;&#21487;&#33021;&#23384;&#22312;&#19981;&#24179;&#34913;&#12290;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#22270;&#20687;&#26469;&#25193;&#20805;&#25968;&#25454;&#38598;&#65292;&#36215;&#21040;&#20102;&#35299;&#20915;&#36825;&#31181;&#19981;&#24179;&#34913;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#29983;&#25104;&#21512;&#25104;&#22270;&#20687;&#38656;&#35201;&#21253;&#21547;&#22810;&#26679;&#21270;&#30340;&#29305;&#24449;&#65292;&#20197;&#20934;&#30830;&#34920;&#31034;&#35757;&#32451;&#22270;&#20687;&#20013;&#23384;&#22312;&#30340;&#29305;&#24449;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#21512;&#25104;&#22270;&#20687;&#20013;&#32570;&#20047;&#22810;&#26679;&#24615;&#30340;&#29305;&#24449;&#20250;&#38477;&#20302;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;&#27169;&#24335;&#22604;&#38519;&#38382;&#39064;&#24433;&#21709;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29983;&#25104;&#22810;&#26679;&#21270;&#22270;&#20687;&#30340;&#33021;&#21147;&#65292;&#24182;&#20998;&#20026;&#31867;&#20869;&#21644;&#31867;&#38388;&#20004;&#31181;&#31867;&#22411;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36825;&#20004;&#31181;&#27169;&#24335;&#22604;&#38519;&#38382;&#39064;&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#23545;&#21512;&#25104;X&#23556;&#32447;&#22270;&#20687;&#22810;&#26679;&#24615;&#30340;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#23558;&#33258;&#36866;&#24212;&#36755;&#20837;&#22270;&#20687;&#24402;&#19968;&#21270;&#26041;&#27861;&#19982;&#28145;&#24230;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#35299;&#20915;&#27169;&#24335;&#22604;&#38519;&#38382;&#39064;&#19978;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biomedical image datasets can be imbalanced due to the rarity of targeted diseases. Generative Adversarial Networks play a key role in addressing this imbalance by enabling the generation of synthetic images to augment datasets. It is important to generate synthetic images that incorporate a diverse range of features to accurately represent the distribution of features present in the training imagery. Furthermore, the absence of diverse features in synthetic images can degrade the performance of machine learning classifiers. The mode collapse problem impacts Generative Adversarial Networks' capacity to generate diversified images. Mode collapse comes in two varieties: intra-class and inter-class. In this paper, both varieties of the mode collapse problem are investigated, and their subsequent impact on the diversity of synthetic X-ray images is evaluated. This work contributes an empirical demonstration of the benefits of integrating the adaptive input-image normalization with the Deep
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#21644;&#35745;&#31639;&#25104;&#26412;&#39640;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#38543;&#26426;&#21160;&#24577;&#35268;&#21010;&#26041;&#31243;&#65292;&#29983;&#25104;&#30340;&#25511;&#21046;&#22120;&#33021;&#22815;&#20027;&#21160;&#23398;&#20064;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#30830;&#20445;&#23433;&#20840;&#24615;&#21644;&#23454;&#26102;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2309.10831</link><description>&lt;p&gt;
&#27963;&#21160;&#23398;&#20064;&#24378;&#21270;&#23398;&#20064;&#65306;&#19968;&#31181;&#38543;&#26426;&#26368;&#20248;&#25511;&#21046;&#26041;&#27861;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Actively Learning Reinforcement Learning: A Stochastic Optimal Control Approach. (arXiv:2309.10831v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10831
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#21644;&#35745;&#31639;&#25104;&#26412;&#39640;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#38543;&#26426;&#21160;&#24577;&#35268;&#21010;&#26041;&#31243;&#65292;&#29983;&#25104;&#30340;&#25511;&#21046;&#22120;&#33021;&#22815;&#20027;&#21160;&#23398;&#20064;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#30830;&#20445;&#23433;&#20840;&#24615;&#21644;&#23454;&#26102;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#24212;&#23545;&#20004;&#20010;&#38382;&#39064;&#65306;&#65288;i&#65289;&#24378;&#21270;&#23398;&#20064;&#22312;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#26041;&#38754;&#30340;&#33030;&#24369;&#24615;&#65292;&#22240;&#20026;&#21463;&#25511;&#23454;&#39564;&#23460;/&#20223;&#30495;&#21644;&#23454;&#38469;&#26465;&#20214;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#38543;&#26426;&#26368;&#20248;&#25511;&#21046;&#30340;&#35745;&#31639;&#25104;&#26412;&#36807;&#39640;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#35299;&#20915;&#38543;&#26426;&#21160;&#24577;&#35268;&#21010;&#26041;&#31243;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#22120;&#23545;&#20110;&#20960;&#31181;&#31867;&#22411;&#30340;&#32422;&#26463;&#26465;&#20214;&#26159;&#23433;&#20840;&#30340;&#65292;&#24182;&#19988;&#23427;&#21487;&#20197;&#20027;&#21160;&#23398;&#20064;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#12290;&#19982;&#25506;&#32034;&#21644;&#21033;&#29992;&#19981;&#21516;&#65292;&#25506;&#27979;&#21644;&#23433;&#20840;&#24615;&#30001;&#25511;&#21046;&#22120;&#33258;&#36523;&#33258;&#21160;&#23454;&#29616;&#65292;&#23454;&#29616;&#20102;&#23454;&#26102;&#23398;&#20064;&#12290;&#19968;&#20010;&#20223;&#30495;&#31034;&#20363;&#35777;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we provide framework to cope with two problems: (i) the fragility of reinforcement learning due to modeling uncertainties because of the mismatch between controlled laboratory/simulation and real-world conditions and (ii) the prohibitive computational cost of stochastic optimal control. We approach both problems by using reinforcement learning to solve the stochastic dynamic programming equation. The resulting reinforcement learning controller is safe with respect to several types of constraints constraints and it can actively learn about the modeling uncertainties. Unlike exploration and exploitation, probing and safety are employed automatically by the controller itself, resulting real-time learning. A simulation example demonstrates the efficacy of the proposed approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#8212;&#8212;&#26799;&#24230;&#36924;&#36817;&#23545;&#25239;&#35757;&#32451;(GAAT)&#65292;&#36890;&#36807;&#27888;&#21202;&#32423;&#25968;&#30340;&#37096;&#20998;&#21644;&#26469;&#36817;&#20284;&#23545;&#25239;&#25439;&#22833;&#65292;&#24182;&#36817;&#20284;&#26799;&#24230;&#65292;&#20197;&#38477;&#20302;&#24314;&#31435;&#40065;&#26834;&#27169;&#22411;&#30340;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2309.09464</link><description>&lt;p&gt;
&#29992;&#26799;&#24230;&#36924;&#36817;&#38477;&#20302;&#23545;&#25239;&#35757;&#32451;&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;
Reducing Adversarial Training Cost with Gradient Approximation. (arXiv:2309.09464v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09464
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#8212;&#8212;&#26799;&#24230;&#36924;&#36817;&#23545;&#25239;&#35757;&#32451;(GAAT)&#65292;&#36890;&#36807;&#27888;&#21202;&#32423;&#25968;&#30340;&#37096;&#20998;&#21644;&#26469;&#36817;&#20284;&#23545;&#25239;&#25439;&#22833;&#65292;&#24182;&#36817;&#20284;&#26799;&#24230;&#65292;&#20197;&#38477;&#20302;&#24314;&#31435;&#40065;&#26834;&#27169;&#22411;&#30340;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#22810;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#23545;&#20110;&#32463;&#36807;&#24039;&#22937;&#20294;&#24494;&#23567;&#25200;&#21160;&#30340;&#36755;&#20837;&#38750;&#24120;&#33030;&#24369;&#65292;&#36825;&#34987;&#31216;&#20026;&#23545;&#25239;&#26679;&#26412;(Adversarial Examples, AEs)&#12290;&#22312;&#35768;&#22810;&#25552;&#39640;&#27169;&#22411;&#23545;&#25239;&#26679;&#26412;&#40065;&#26834;&#24615;&#30340;&#31574;&#30053;&#20013;&#65292;&#22522;&#20110;&#25237;&#24433;&#26799;&#24230;&#19979;&#38477;(Projected Gradient Descent, PGD)&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#26159;&#26368;&#26377;&#25928;&#30340;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25439;&#22833;&#20989;&#25968;&#30340;&#26368;&#22823;&#21270;&#20351;&#24471;&#29983;&#25104;&#36275;&#22815;&#24378;&#28872;&#30340;&#23545;&#25239;&#26679;&#26412;&#38656;&#35201;&#24040;&#22823;&#30340;&#35745;&#31639;&#24320;&#38144;&#65292;&#23545;&#20110;&#20351;&#29992;&#26356;&#22823;&#26356;&#22797;&#26434;&#30340;&#27169;&#22411;&#65292;&#36890;&#24120;&#30340;PGD&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#26377;&#26102;&#19981;&#20999;&#23454;&#38469;&#12290;&#26412;&#25991;&#25552;&#20986;&#23545;&#25239;&#25439;&#22833;&#21487;&#20197;&#36890;&#36807;&#27888;&#21202;&#32423;&#25968;&#30340;&#37096;&#20998;&#21644;&#26469;&#36817;&#20284;&#65292;&#24182;&#36817;&#20284;&#23545;&#25239;&#25439;&#22833;&#30340;&#26799;&#24230;&#65292;&#36827;&#32780;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#39640;&#25928;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#8212;&#8212;&#26799;&#24230;&#36924;&#36817;&#23545;&#25239;&#35757;&#32451;(GAAT)&#65292;&#20197;&#38477;&#20302;&#24314;&#31435;&#40065;&#26834;&#27169;&#22411;&#30340;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning models have achieved state-of-the-art performances in various domains, while they are vulnerable to the inputs with well-crafted but small perturbations, which are named after adversarial examples (AEs). Among many strategies to improve the model robustness against AEs, Projected Gradient Descent (PGD) based adversarial training is one of the most effective methods. Unfortunately, the prohibitive computational overhead of generating strong enough AEs, due to the maximization of the loss function, sometimes makes the regular PGD adversarial training impractical when using larger and more complicated models. In this paper, we propose that the adversarial loss can be approximated by the partial sum of Taylor series. Furthermore, we approximate the gradient of adversarial loss and propose a new and efficient adversarial training method, adversarial training with gradient approximation (GAAT), to reduce the cost of building up robust models. Additionally, extensive experiments
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"EEGMobile"&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#30561;&#30496;&#38454;&#27573;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#65292;&#29305;&#21035;&#22312;N1&#38454;&#27573;&#34920;&#29616;&#26356;&#20339;&#12290;</title><link>http://arxiv.org/abs/2309.07182</link><description>&lt;p&gt;
&#20351;&#29992;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#30561;&#30496;&#38454;&#27573;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Sleep Stage Classification Using a Pre-trained Deep Learning Model. (arXiv:2309.07182v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07182
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"EEGMobile"&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#30561;&#30496;&#38454;&#27573;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#65292;&#29305;&#21035;&#22312;N1&#38454;&#27573;&#34920;&#29616;&#26356;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30561;&#30496;&#38556;&#30861;&#26159;&#24120;&#35265;&#30340;&#20154;&#31867;&#30142;&#30149;&#20043;&#19968;&#12290;&#30561;&#30496;&#38454;&#27573;&#30340;&#20998;&#31867;&#22312;&#35786;&#26029;&#30561;&#30496;&#38556;&#30861;&#12289;&#30417;&#27979;&#27835;&#30103;&#25928;&#26524;&#21644;&#29702;&#35299;&#30561;&#30496;&#38454;&#27573;&#19982;&#21508;&#31181;&#20581;&#24247;&#29366;&#20917;&#20043;&#38388;&#30340;&#20851;&#31995;&#26041;&#38754;&#36215;&#30528;&#22522;&#30784;&#24615;&#20316;&#29992;&#12290;&#31934;&#30830;&#32780;&#26377;&#25928;&#22320;&#20998;&#31867;&#36825;&#20123;&#38454;&#27573;&#21487;&#20197;&#26174;&#30528;&#25552;&#21319;&#25105;&#20204;&#23545;&#19982;&#30561;&#30496;&#30456;&#20851;&#29616;&#35937;&#30340;&#29702;&#35299;&#65292;&#20174;&#32780;&#26368;&#32456;&#25913;&#21892;&#20581;&#24247;&#32467;&#26524;&#21644;&#30142;&#30149;&#27835;&#30103;&#25928;&#26524;&#12290;&#20854;&#20182;&#25552;&#20986;&#30340;&#27169;&#22411;&#24448;&#24448;&#32791;&#26102;&#19988;&#32570;&#20047;&#36275;&#22815;&#30340;&#20934;&#30830;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;N1&#38454;&#27573;&#12290;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;"EEGMobile"&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#20174;&#33041;&#20449;&#21495;&#30340;&#33041;&#30005;&#22270;&#35889;&#22270;&#23398;&#20064;&#12290;&#22312;&#21517;&#20026;"Sleep-EDF20"&#30340;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#65292;&#35813;&#27169;&#22411;&#23454;&#29616;&#20102;86.97%&#30340;&#20934;&#30830;&#29575;&#65292;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#30740;&#31350;&#32773;&#25552;&#20986;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#23427;&#22312;N1&#38454;&#27573;&#35760;&#24405;&#20102;56.4%&#30340;&#20934;&#30830;&#29575;&#65292;&#36825;&#26159;&#26356;&#22909;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the common human diseases is sleep disorders. The classification of sleep stages plays a fundamental role in diagnosing sleep disorders, monitoring treatment effectiveness, and understanding the relationship between sleep stages and various health conditions. A precise and efficient classification of these stages can significantly enhance our understanding of sleep-related phenomena and ultimately lead to improved health outcomes and disease treatment.  Models others propose are often time-consuming and lack sufficient accuracy, especially in stage N1. The main objective of this research is to present a machine-learning model called "EEGMobile". This model utilizes pre-trained models and learns from electroencephalogram (EEG) spectrograms of brain signals. The model achieved an accuracy of 86.97% on a publicly available dataset named "Sleep-EDF20", outperforming other models proposed by different researchers. Moreover, it recorded an accuracy of 56.4% in stage N1, which is bette
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#26657;&#20934;&#30340;&#22522;&#20934;&#30740;&#31350;&#65292;&#21033;&#29992;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#31354;&#38388;&#25506;&#32034;&#20102;&#27169;&#22411;&#26657;&#20934;&#23646;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#27169;&#22411;&#26657;&#20934;&#21487;&#20197;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#27867;&#21270;&#65292;&#24182;&#21487;&#20197;&#21516;&#26102;&#20860;&#39038;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#26657;&#20934;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.11838</link><description>&lt;p&gt;
&#19968;&#20010;&#20851;&#20110;&#26657;&#20934;&#30340;&#22522;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Benchmark Study on Calibration. (arXiv:2308.11838v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11838
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#26657;&#20934;&#30340;&#22522;&#20934;&#30740;&#31350;&#65292;&#21033;&#29992;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#31354;&#38388;&#25506;&#32034;&#20102;&#27169;&#22411;&#26657;&#20934;&#23646;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#27169;&#22411;&#26657;&#20934;&#21487;&#20197;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#27867;&#21270;&#65292;&#24182;&#21487;&#20197;&#21516;&#26102;&#20860;&#39038;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#26657;&#20934;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#24191;&#27867;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#36825;&#20123;&#27169;&#22411;&#22797;&#26434;&#24615;&#30340;&#22686;&#21152;&#65292;&#23427;&#20204;&#24448;&#24448;&#38754;&#20020;&#26657;&#20934;&#38382;&#39064;&#65292;&#23613;&#31649;&#39044;&#27979;&#20934;&#30830;&#24615;&#26377;&#25152;&#25552;&#39640;&#12290;&#35768;&#22810;&#30740;&#31350;&#36890;&#36807;&#25968;&#25454;&#39044;&#22788;&#29702;&#12289;&#20351;&#29992;&#29305;&#23450;&#25439;&#22833;&#20989;&#25968;&#21644;&#35757;&#32451;&#26694;&#26550;&#26469;&#25913;&#21892;&#26657;&#20934;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23545;&#26657;&#20934;&#23646;&#24615;&#30340;&#30740;&#31350;&#26377;&#28857;&#34987;&#24573;&#35270;&#20102;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21033;&#29992;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#25628;&#32034;&#31354;&#38388;&#65292;&#22312;&#20840;&#38754;&#25506;&#32034;&#26657;&#20934;&#23646;&#24615;&#30340;&#27169;&#22411;&#26550;&#26500;&#31354;&#38388;&#20013;&#25552;&#20379;&#20102;&#19968;&#20010;&#35814;&#23613;&#30340;&#27169;&#22411;&#26550;&#26500;&#31354;&#38388;&#12290;&#25105;&#20204;&#29305;&#21035;&#21019;&#24314;&#20102;&#19968;&#20010;&#27169;&#22411;&#26657;&#20934;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;NATS-Bench&#25628;&#32034;&#31354;&#38388;&#20013;&#35780;&#20272;&#20102;90&#20010;&#22522;&#20110;&#21306;&#38388;&#30340;&#26657;&#20934;&#24230;&#37327;&#21644;12&#20010;&#20854;&#20182;&#26657;&#20934;&#24230;&#37327;&#65292;&#28085;&#30422;&#20102;117,702&#20010;&#29420;&#29305;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#26088;&#22312;&#36890;&#36807;&#25105;&#20204;&#25552;&#20986;&#30340;&#25968;&#25454;&#38598;&#22238;&#31572;&#35813;&#39046;&#22495;&#19968;&#20123;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#65306;&#65288;i&#65289;&#27169;&#22411;&#26657;&#20934;&#33021;&#21542;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#27867;&#21270;&#65311;&#65288;ii&#65289;&#33021;&#21542;&#21516;&#26102;&#20860;&#39038;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#26657;&#20934;&#24615;&#33021;&#65311;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are increasingly utilized in various machine learning tasks. However, as these models grow in complexity, they often face calibration issues, despite enhanced prediction accuracy. Many studies have endeavored to improve calibration performance through data preprocessing, the use of specific loss functions, and training frameworks. Yet, investigations into calibration properties have been somewhat overlooked. Our study leverages the Neural Architecture Search (NAS) search space, offering an exhaustive model architecture space for thorough calibration properties exploration. We specifically create a model calibration dataset. This dataset evaluates 90 bin-based and 12 additional calibration measurements across 117,702 unique neural networks within the widely employed NATS-Bench search space. Our analysis aims to answer several longstanding questions in the field, using our proposed dataset: (i) Can model calibration be generalized across different tasks? (ii) Can rob
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#27169;&#24577;&#23884;&#20837;&#20013;&#30340;&#23545;&#25239;&#24187;&#35273;&#38382;&#39064;&#12290;&#23545;&#25163;&#21487;&#20197;&#25200;&#21160;&#36755;&#20837;&#30340;&#20219;&#24847;&#27169;&#24577;&#65292;&#20351;&#20854;&#23884;&#20837;&#19982;&#20854;&#20182;&#27169;&#24577;&#30340;&#20219;&#24847;&#36755;&#20837;&#25509;&#36817;&#65292;&#20174;&#32780;&#23454;&#29616;&#20219;&#24847;&#22270;&#20687;&#19982;&#20219;&#24847;&#25991;&#26412;&#12289;&#20219;&#24847;&#25991;&#26412;&#19982;&#20219;&#24847;&#22768;&#38899;&#30340;&#23545;&#40784;&#12290;&#35813;&#38382;&#39064;&#19982;&#19979;&#28216;&#20219;&#21153;&#26080;&#20851;&#65292;&#23545;&#29983;&#25104;&#21644;&#20998;&#31867;&#20219;&#21153;&#20250;&#20135;&#29983;&#35823;&#23548;&#12290;</title><link>http://arxiv.org/abs/2308.11804</link><description>&lt;p&gt;
&#36825;&#19981;&#26159;&#19968;&#20010;&#33529;&#26524;&#65306;&#22810;&#27169;&#24577;&#23884;&#20837;&#20013;&#30340;&#23545;&#25239;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Ceci n'est pas une pomme: Adversarial Illusions in Multi-Modal Embeddings. (arXiv:2308.11804v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11804
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#27169;&#24577;&#23884;&#20837;&#20013;&#30340;&#23545;&#25239;&#24187;&#35273;&#38382;&#39064;&#12290;&#23545;&#25163;&#21487;&#20197;&#25200;&#21160;&#36755;&#20837;&#30340;&#20219;&#24847;&#27169;&#24577;&#65292;&#20351;&#20854;&#23884;&#20837;&#19982;&#20854;&#20182;&#27169;&#24577;&#30340;&#20219;&#24847;&#36755;&#20837;&#25509;&#36817;&#65292;&#20174;&#32780;&#23454;&#29616;&#20219;&#24847;&#22270;&#20687;&#19982;&#20219;&#24847;&#25991;&#26412;&#12289;&#20219;&#24847;&#25991;&#26412;&#19982;&#20219;&#24847;&#22768;&#38899;&#30340;&#23545;&#40784;&#12290;&#35813;&#38382;&#39064;&#19982;&#19979;&#28216;&#20219;&#21153;&#26080;&#20851;&#65292;&#23545;&#29983;&#25104;&#21644;&#20998;&#31867;&#20219;&#21153;&#20250;&#20135;&#29983;&#35823;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;&#23558;&#22270;&#20687;&#12289;&#22768;&#38899;&#12289;&#25991;&#26412;&#12289;&#35270;&#39057;&#31561;&#26144;&#23556;&#21040;&#19968;&#20010;&#21333;&#19968;&#30340;&#23884;&#20837;&#31354;&#38388;&#20013;&#65292;&#36890;&#36807;&#23545;&#40784;&#19981;&#21516;&#27169;&#24577;&#30340;&#34920;&#31034;&#65288;&#20363;&#22914;&#23558;&#19968;&#24352;&#29399;&#30340;&#22270;&#20687;&#19982;&#19968;&#31181;&#21483;&#22768;&#30456;&#20851;&#32852;&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22810;&#27169;&#24577;&#23884;&#20837;&#21487;&#20197;&#21463;&#21040;&#19968;&#31181;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#23545;&#25239;&#24187;&#35273;&#8221;&#30340;&#25915;&#20987;&#12290;&#32473;&#23450;&#20219;&#24847;&#27169;&#24577;&#30340;&#36755;&#20837;&#65292;&#23545;&#25163;&#21487;&#20197;&#25200;&#21160;&#23427;&#65292;&#20351;&#20854;&#23884;&#20837;&#25509;&#36817;&#20110;&#21478;&#19968;&#27169;&#24577;&#20013;&#20219;&#24847;&#23545;&#25163;&#36873;&#25321;&#30340;&#36755;&#20837;&#30340;&#23884;&#20837;&#12290;&#24187;&#35273;&#20351;&#23545;&#25163;&#33021;&#22815;&#23558;&#20219;&#24847;&#22270;&#20687;&#19982;&#20219;&#24847;&#25991;&#26412;&#12289;&#20219;&#24847;&#25991;&#26412;&#19982;&#20219;&#24847;&#22768;&#38899;&#31561;&#36827;&#34892;&#23545;&#40784;&#12290;&#23545;&#25239;&#24187;&#35273;&#21033;&#29992;&#20102;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#25509;&#36817;&#24615;&#65292;&#22240;&#27492;&#19982;&#19979;&#28216;&#20219;&#21153;&#26080;&#20851;&#12290;&#20351;&#29992;ImageBind&#23884;&#20837;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22312;&#27809;&#26377;&#20855;&#20307;&#19979;&#28216;&#20219;&#21153;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#23545;&#25239;&#24615;&#23545;&#40784;&#30340;&#36755;&#20837;&#22914;&#20309;&#35823;&#23548;&#22270;&#20687;&#29983;&#25104;&#12289;&#25991;&#26412;&#29983;&#25104;&#21644;&#38646;&#26679;&#20363;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-modal encoders map images, sounds, texts, videos, etc. into a single embedding space, aligning representations across modalities (e.g., associate an image of a dog with a barking sound). We show that multi-modal embeddings can be vulnerable to an attack we call "adversarial illusions." Given an input in any modality, an adversary can perturb it so as to make its embedding close to that of an arbitrary, adversary-chosen input in another modality. Illusions thus enable the adversary to align any image with any text, any text with any sound, etc.  Adversarial illusions exploit proximity in the embedding space and are thus agnostic to downstream tasks. Using ImageBind embeddings, we demonstrate how adversarially aligned inputs, generated without knowledge of specific downstream tasks, mislead image generation, text generation, and zero-shot classification.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#32423;&#21452;&#36335;&#24452;&#21387;&#32553;&#26041;&#27861;&#29992;&#20110;&#32852;&#21512;&#22238;&#22768;&#28040;&#38500;&#21644;&#22122;&#22768;&#25233;&#21046;&#65292;&#36890;&#36807;&#26102;&#38388;&#39057;&#29575;&#21452;&#36335;&#24452;&#21387;&#32553;&#23454;&#29616;&#24191;&#27867;&#30340;&#21387;&#32553;&#27604;&#65292;&#21516;&#26102;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#65292;&#19988;&#22312;&#22266;&#23450;&#21387;&#32553;&#27604;&#19979;&#33021;&#22815;&#36827;&#19968;&#27493;&#25913;&#21892;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.11053</link><description>&lt;p&gt;
&#32852;&#21512;&#22238;&#22768;&#28040;&#38500;&#21644;&#22122;&#22768;&#25233;&#21046;&#30340;&#36229;&#32423;&#21452;&#36335;&#24452;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Ultra Dual-Path Compression For Joint Echo Cancellation And Noise Suppression. (arXiv:2308.11053v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#32423;&#21452;&#36335;&#24452;&#21387;&#32553;&#26041;&#27861;&#29992;&#20110;&#32852;&#21512;&#22238;&#22768;&#28040;&#38500;&#21644;&#22122;&#22768;&#25233;&#21046;&#65292;&#36890;&#36807;&#26102;&#38388;&#39057;&#29575;&#21452;&#36335;&#24452;&#21387;&#32553;&#23454;&#29616;&#24191;&#27867;&#30340;&#21387;&#32553;&#27604;&#65292;&#21516;&#26102;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#65292;&#19988;&#22312;&#22266;&#23450;&#21387;&#32553;&#27604;&#19979;&#33021;&#22815;&#36827;&#19968;&#27493;&#25913;&#21892;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22238;&#22768;&#28040;&#38500;&#21644;&#22122;&#22768;&#25233;&#21046;&#23545;&#20110;&#20840;&#21452;&#24037;&#36890;&#20449;&#33267;&#20851;&#37325;&#35201;&#65292;&#28982;&#32780;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#35745;&#31639;&#25104;&#26412;&#39640;&#21644;&#27169;&#22411;&#22797;&#26434;&#24230;&#35843;&#25972;&#19978;&#19981;&#28789;&#27963;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#26102;&#38388;&#39057;&#29575;&#21452;&#36335;&#24452;&#21387;&#32553;&#65292;&#23454;&#29616;&#24191;&#27867;&#30340;&#21387;&#32553;&#27604;&#65292;&#21516;&#26102;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22312;&#39057;&#29575;&#21387;&#32553;&#26041;&#38754;&#65292;&#20351;&#29992;&#21487;&#35757;&#32451;&#28388;&#27874;&#22120;&#20195;&#26367;&#25163;&#21160;&#35774;&#35745;&#30340;&#28388;&#27874;&#22120;&#36827;&#34892;&#32500;&#24230;&#32553;&#20943;&#12290;&#22312;&#26102;&#38388;&#21387;&#32553;&#26041;&#38754;&#65292;&#20165;&#20351;&#29992;&#24103;&#36339;&#39044;&#27979;&#20250;&#23548;&#33268;&#24615;&#33021;&#22823;&#24133;&#19979;&#38477;&#65292;&#20294;&#36890;&#36807;&#20855;&#26377;&#23436;&#25972;&#24207;&#21015;&#24314;&#27169;&#30340;&#21518;&#22788;&#29702;&#32593;&#32476;&#21487;&#20197;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#22266;&#23450;&#21387;&#32553;&#27604;&#30340;&#24773;&#20917;&#19979;&#65292;&#21516;&#26102;&#20351;&#29992;&#26102;&#38388;&#21644;&#39057;&#29575;&#26041;&#27861;&#36827;&#34892;&#21452;&#36335;&#24452;&#21387;&#32553;&#23558;&#36827;&#19968;&#27493;&#25913;&#21892;&#24615;&#33021;&#65292;&#24182;&#19988;&#20960;&#20046;&#19981;&#25913;&#21464;&#27169;&#22411;&#22823;&#23567;&#65292;&#21387;&#32553;&#27604;&#35206;&#30422;&#33539;&#22260;&#20174;4x&#21040;32x&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#19982;&#24555;&#36895;FullSubNet&#21644;DeepFilterNet&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;&#21487;&#20197;&#35775;&#38382;&#28436;&#31034;&#39029;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Echo cancellation and noise reduction are essential for full-duplex communication, yet most existing neural networks have high computational costs and are inflexible in tuning model complexity. In this paper, we introduce time-frequency dual-path compression to achieve a wide range of compression ratios on computational cost. Specifically, for frequency compression, trainable filters are used to replace manually designed filters for dimension reduction. For time compression, only using frame skipped prediction causes large performance degradation, which can be alleviated by a post-processing network with full sequence modeling. We have found that under fixed compression ratios, dual-path compression combining both the time and frequency methods will give further performance improvement, covering compression ratios from 4x to 32x with little model size change. Moreover, the proposed models show competitive performance compared with fast FullSubNet and DeepFilterNet. A demo page can be f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27010;&#36848;&#20102;&#39640;&#20809;&#35889;&#35299;&#28151;&#30340;&#20808;&#36827;&#21644;&#24120;&#35268;&#26041;&#27861;&#65292;&#24182;&#27604;&#36739;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#19981;&#21516;&#35299;&#28151;&#22330;&#26223;&#19979;&#30340;&#24615;&#33021;&#12290;&#21478;&#22806;&#65292;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#24320;&#28304;&#30340;&#35299;&#28151;&#36719;&#20214;&#21253;HySUPP&#12290;</title><link>http://arxiv.org/abs/2308.09375</link><description>&lt;p&gt;
&#22270;&#20687;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#22312;&#39640;&#20809;&#35889;&#35299;&#28151;&#26041;&#38754;&#30340;&#24212;&#29992;&#65306;&#27010;&#36848;&#21644;HySUPP Python&#21253;
&lt;/p&gt;
&lt;p&gt;
Image Processing and Machine Learning for Hyperspectral Unmixing: An Overview and the HySUPP Python Package. (arXiv:2308.09375v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09375
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27010;&#36848;&#20102;&#39640;&#20809;&#35889;&#35299;&#28151;&#30340;&#20808;&#36827;&#21644;&#24120;&#35268;&#26041;&#27861;&#65292;&#24182;&#27604;&#36739;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#19981;&#21516;&#35299;&#28151;&#22330;&#26223;&#19979;&#30340;&#24615;&#33021;&#12290;&#21478;&#22806;&#65292;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#24320;&#28304;&#30340;&#35299;&#28151;&#36719;&#20214;&#21253;HySUPP&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#39640;&#20809;&#35889;&#20256;&#24863;&#22120;&#30340;&#20302;&#31354;&#38388;&#20998;&#36776;&#29575;&#12289;&#21452;&#37325;&#25955;&#23556;&#21644;&#22330;&#26223;&#20013;&#26448;&#26009;&#30340;&#28151;&#21512;&#29616;&#35937;&#65292;&#20809;&#35889;&#20687;&#32032;&#24448;&#24448;&#26159;&#26448;&#26009;&#30340;&#32431;&#20809;&#35889;&#25104;&#20998;&#30340;&#28151;&#21512;&#29289;&#65292;&#34987;&#31216;&#20026;&#31471;&#20803;&#12290;&#35299;&#28151;&#21363;&#20272;&#35745;&#20687;&#32032;&#28857;&#20013;&#21508;&#31471;&#20803;&#30340;&#27604;&#20363;&#12290;&#26681;&#25454;&#31471;&#20803;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#32447;&#24615;&#35299;&#28151;&#21487;&#20998;&#20026;&#30417;&#30563;&#12289;&#21322;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#65288;&#30450;&#35299;&#28151;&#65289;&#19977;&#22823;&#31867;&#12290;&#22270;&#20687;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#36827;&#23637;&#23545;&#35299;&#28151;&#20135;&#29983;&#20102;&#37325;&#22823;&#24433;&#21709;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#20808;&#36827;&#21644;&#24120;&#35268;&#30340;&#35299;&#28151;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#36825;&#19977;&#20010;&#31867;&#21035;&#20013;&#20808;&#36827;&#21644;&#24120;&#35268;&#25216;&#26415;&#36827;&#34892;&#20102;&#37325;&#35201;&#23545;&#27604;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#35299;&#28151;&#25216;&#26415;&#22312;&#19977;&#20010;&#27169;&#25311;&#21644;&#20004;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#25581;&#31034;&#20102;&#23545;&#19981;&#21516;&#35299;&#28151;&#22330;&#26223;&#26469;&#35828;&#65292;&#19981;&#21516;&#35299;&#28151;&#31867;&#21035;&#30340;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#24320;&#28304;&#30340;&#36719;&#20214;&#21253;&#8212;&#8212;HySUPP&#12290;
&lt;/p&gt;
&lt;p&gt;
Spectral pixels are often a mixture of the pure spectra of the materials, called endmembers, due to the low spatial resolution of hyperspectral sensors, double scattering, and intimate mixtures of materials in the scenes. Unmixing estimates the fractional abundances of the endmembers within the pixel. Depending on the prior knowledge of endmembers, linear unmixing can be divided into three main groups: supervised, semi-supervised, and unsupervised (blind) linear unmixing. Advances in Image processing and machine learning substantially affected unmixing. This paper provides an overview of advanced and conventional unmixing approaches. Additionally, we draw a critical comparison between advanced and conventional techniques from the three categories. We compare the performance of the unmixing techniques on three simulated and two real datasets. The experimental results reveal the advantages of different unmixing categories for different unmixing scenarios. Moreover, we provide an open-sou
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24182;&#34892;U&#22411;&#20998;&#23618;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#20248;&#21270;&#36164;&#28304;&#20998;&#37197;&#26041;&#26696;&#20197;&#25552;&#39640;&#36793;&#32536;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#36798;&#21040;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.08896</link><description>&lt;p&gt;
&#20026;U&#22411;&#24182;&#34892;&#20998;&#23618;&#23398;&#20064;&#36827;&#34892;&#26368;&#20248;&#36164;&#28304;&#20998;&#37197;
&lt;/p&gt;
&lt;p&gt;
Optimal Resource Allocation for U-Shaped Parallel Split Learning. (arXiv:2308.08896v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08896
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24182;&#34892;U&#22411;&#20998;&#23618;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#20248;&#21270;&#36164;&#28304;&#20998;&#37197;&#26041;&#26696;&#20197;&#25552;&#39640;&#36793;&#32536;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#36798;&#21040;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23618;&#23398;&#20064;&#65288;SL&#65289;&#20316;&#20026;&#19968;&#31181;&#22312;&#19981;&#20844;&#24320;&#25968;&#25454;&#25152;&#26377;&#32773;&#30340;&#21407;&#22987;&#25968;&#25454;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#30340;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#32780;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;SL&#19981;&#21487;&#36991;&#20813;&#22320;&#27844;&#28431;&#20102;&#26631;&#31614;&#38544;&#31169;&#65292;&#22240;&#20026;&#23614;&#37096;&#27169;&#22411;&#65288;&#20855;&#26377;&#26368;&#21518;&#20960;&#23618;&#65289;&#24212;&#35813;&#25918;&#22312;&#26381;&#21153;&#22120;&#19978;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#27861;&#26159;&#21033;&#29992;U&#22411;&#26550;&#26500;&#23558;&#26089;&#26399;&#23618;&#21644;&#26368;&#21518;&#23618;&#37117;&#25918;&#22312;&#29992;&#25143;&#31471;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24182;&#34892;U&#22411;&#20998;&#23618;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#20248;&#21270;&#36164;&#28304;&#20998;&#37197;&#26041;&#26696;&#20197;&#25552;&#39640;&#36793;&#32536;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#22312;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#20013;&#65292;&#22810;&#20010;&#29992;&#25143;&#19982;&#36793;&#32536;&#26381;&#21153;&#22120;&#36827;&#34892;SL&#36890;&#20449;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#35757;&#32451;&#36807;&#31243;&#20013;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#31471;&#21040;&#31471;&#24310;&#36831;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#36164;&#28304;&#20998;&#37197;&#31639;&#27861;&#65292;&#31216;&#20026;LSCRA&#65292;&#23427;&#21487;&#20197;&#25214;&#21040;&#26368;&#20339;&#30340;&#35745;&#31639;&#36164;&#28304;&#20998;&#37197;&#21644;&#20998;&#23618;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;LSCRA&#30340;&#26377;&#25928;&#24615;&#65292;&#20197;&#21450;U&#22411;PSL&#21487;&#20197;&#36798;&#21040;&#19982;&#20854;&#20182;S&#26041;&#27861;&#31867;&#20284;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Split learning (SL) has emerged as a promising approach for model training without revealing the raw data samples from the data owners. However, traditional SL inevitably leaks label privacy as the tail model (with the last layers) should be placed on the server. To overcome this limitation, one promising solution is to utilize U-shaped architecture to leave both early layers and last layers on the user side. In this paper, we develop a novel parallel U-shaped split learning and devise the optimal resource optimization scheme to improve the performance of edge networks. In the proposed framework, multiple users communicate with an edge server for SL. We analyze the end-to-end delay of each client during the training process and design an efficient resource allocation algorithm, called LSCRA, which finds the optimal computing resource allocation and split layers. Our experimental results show the effectiveness of LSCRA and that U-shaped PSL can achieve a similar performance with other S
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#20102;&#24320;&#28304;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#20013;&#23384;&#22312;&#30340;&#38169;&#35823;&#65292;&#24182;&#23545;&#36825;&#20123;&#38169;&#35823;&#30340;&#29305;&#24449;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#20026;&#25552;&#39640;&#26694;&#26550;&#30340;&#23433;&#20840;&#24615;&#21644;&#31283;&#23450;&#24615;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2308.05014</link><description>&lt;p&gt;
&#24320;&#28304;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#20013;&#38169;&#35823;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study of Bugs in Open-Source Federated Learning Framework. (arXiv:2308.05014v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#20102;&#24320;&#28304;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#20013;&#23384;&#22312;&#30340;&#38169;&#35823;&#65292;&#24182;&#23545;&#36825;&#20123;&#38169;&#35823;&#30340;&#29305;&#24449;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#20026;&#25552;&#39640;&#26694;&#26550;&#30340;&#23433;&#20840;&#24615;&#21644;&#31283;&#23450;&#24615;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#20998;&#25955;&#24335;&#30340;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#65292;&#26088;&#22312;&#20445;&#25252;&#29992;&#25143;&#30340;&#31169;&#23494;&#25968;&#25454;&#65292;&#22312;&#36817;&#24180;&#26469;&#24050;&#25104;&#20026;&#37325;&#35201;&#30340;&#23398;&#20064;&#27169;&#24335;&#65292;&#23588;&#20854;&#26159;&#22312;&#22823;&#22810;&#25968;&#22269;&#23478;&#23454;&#26045;&#26356;&#20005;&#26684;&#30340;&#27861;&#24459;&#27861;&#35268;&#20043;&#21518;&#12290;&#22240;&#27492;&#65292;&#21457;&#24067;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#20419;&#36827;&#32852;&#37030;&#23398;&#20064;&#30340;&#24320;&#21457;&#21644;&#24212;&#29992;&#12290;&#23613;&#31649;&#22312;FL&#27169;&#22411;&#21644;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#21644;&#38544;&#31169;&#24615;&#26041;&#38754;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#65292;&#20294;FL&#26694;&#26550;&#30340;&#23433;&#20840;&#38382;&#39064;&#23578;&#26410;&#24471;&#21040;&#31995;&#32479;&#22320;&#30740;&#31350;&#12290;&#26412;&#25991;&#39318;&#27425;&#23545;1,112&#20010;FL&#26694;&#26550;&#38169;&#35823;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#65292;&#20197;&#20102;&#35299;&#20854;&#29305;&#24449;&#12290;&#36825;&#20123;&#38169;&#35823;&#26159;&#36890;&#36807;&#25163;&#21160;&#20174;GitHub&#19978;&#25910;&#38598;&#12289;&#20998;&#31867;&#21644;&#26631;&#35760;&#30340;12&#20010;&#24320;&#28304;FL&#26694;&#26550;&#24471;&#26469;&#30340;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#36825;&#20123;&#38169;&#35823;&#30340;15&#20010;&#30151;&#29366;&#12289;12&#20010;&#26681;&#26412;&#21407;&#22240;&#21644;20&#20010;&#20462;&#22797;&#27169;&#24335;&#30340;&#20998;&#31867;&#65292;&#24182;&#30740;&#31350;&#20102;&#23427;&#20204;&#22312;23&#20010;&#36923;&#36753;&#32452;&#20214;&#21644;&#20004;&#20010;&#20027;&#35201;&#24212;&#29992;&#22330;&#26223;&#19978;&#30340;&#30456;&#20851;&#24615;&#21644;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL), as a decentralized machine learning solution to the protection of users' private data, has become an important learning paradigm in recent years, especially since the enforcement of stricter laws and regulations in most countries. Therefore, a variety of FL frameworks are released to facilitate the development and application of federated learning. Despite the considerable amount of research on the security and privacy of FL models and systems, the security issues in FL frameworks have not been systematically studied yet. In this paper, we conduct the first empirical study on 1,112 FL framework bugs to investigate their characteristics. These bugs are manually collected, classified, and labeled from 12 open-source FL frameworks on GitHub. In detail, we construct taxonomies of 15 symptoms, 12 root causes, and 20 fix patterns of these bugs and investigate their correlations and distributions on 23 logical components and two main application scenarios. From the re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38750;&#24179;&#31283;&#36172;&#21338;&#31639;&#27861;&#30340;&#25903;&#20184;&#36335;&#30001;&#31574;&#30053;&#65292;&#36890;&#36807;&#31995;&#32479;&#26550;&#26500;&#35774;&#35745;&#21644;&#23454;&#26102;&#23454;&#39564;&#30340;&#39564;&#35777;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;0.92\%&#30340;&#20132;&#26131;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.01028</link><description>&lt;p&gt;
&#26368;&#22823;&#21270;&#20351;&#29992;&#38750;&#24179;&#31283;&#36172;&#21338;&#31639;&#27861;&#30340;&#25903;&#20184;&#36335;&#30001;&#25104;&#21151;&#29575;
&lt;/p&gt;
&lt;p&gt;
Maximizing Success Rate of Payment Routing using Non-stationary Bandits. (arXiv:2308.01028v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38750;&#24179;&#31283;&#36172;&#21338;&#31639;&#27861;&#30340;&#25903;&#20184;&#36335;&#30001;&#31574;&#30053;&#65292;&#36890;&#36807;&#31995;&#32479;&#26550;&#26500;&#35774;&#35745;&#21644;&#23454;&#26102;&#23454;&#39564;&#30340;&#39564;&#35777;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;0.92\%&#30340;&#20132;&#26131;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#38750;&#24179;&#31283;&#22810;&#33218;&#36172;&#21338;&#26041;&#27861;&#30340;&#31995;&#32479;&#26550;&#26500;&#35774;&#35745;&#21644;&#37096;&#32626;&#65292;&#20197;&#26681;&#25454;&#26368;&#36817;&#30340;&#20132;&#26131;&#21382;&#21490;&#30830;&#23450;&#25509;&#36817;&#26368;&#20248;&#30340;&#25903;&#20184;&#36335;&#30001;&#31574;&#30053;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23556;&#32447;&#30340;&#26032;&#22411;&#36335;&#30001;&#26381;&#21153;&#26550;&#26500;&#65292;&#36890;&#36807;&#26368;&#20248;&#30340;&#25193;&#23637;&#36172;&#21338;&#24335;&#25903;&#20184;&#36335;&#30001;&#26469;&#22788;&#29702;&#27599;&#31186;&#36229;&#36807;10000&#27425;&#30340;&#20132;&#26131;&#37327;&#65292;&#24182;&#31526;&#21512;Payment Card Industry Data Security Standard (PCI DSS) &#30340;&#31995;&#32479;&#35774;&#35745;&#35201;&#27714;&#21644;&#29983;&#24577;&#32422;&#26463;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#33258;&#23450;&#20041;&#27169;&#25311;&#22120;&#19978;&#35780;&#20272;&#20102;&#22810;&#20010;&#22522;&#20110;&#36172;&#21338;&#31639;&#27861;&#30340;&#25903;&#20184;&#36335;&#30001;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20197;&#23545;&#27604;&#22810;&#20010;&#38750;&#24179;&#31283;&#36172;&#21338;&#26041;&#27861;&#24182;&#30830;&#23450;&#26368;&#20339;&#36229;&#21442;&#25968;&#12290;&#28982;&#21518;&#25105;&#20204;&#22312;&#34394;&#25311;&#20307;&#32946;&#24179;&#21488;Dream11&#30340;&#25903;&#20184;&#20132;&#26131;&#31995;&#32479;&#19978;&#36827;&#34892;&#20102;&#23454;&#26102;&#23454;&#39564;&#12290;&#22312;&#23454;&#26102;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#38750;&#24179;&#31283;&#36172;&#21338;&#31639;&#27861;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#21487;&#20197;&#22987;&#32456;&#25552;&#39640;0.92\%&#30340;&#20132;&#26131;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper discusses the system architecture design and deployment of non-stationary multi-armed bandit approaches to determine a near-optimal payment routing policy based on the recent history of transactions. We propose a Routing Service architecture using a novel Ray-based implementation for optimally scaling bandit-based payment routing to over 10000 transactions per second, adhering to the system design requirements and ecosystem constraints with Payment Card Industry Data Security Standard (PCI DSS). We first evaluate the effectiveness of multiple bandit-based payment routing algorithms on a custom simulator to benchmark multiple non-stationary bandit approaches and identify the best hyperparameters. We then conducted live experiments on the payment transaction system on a fantasy sports platform Dream11. In the live experiments, we demonstrated that our non-stationary bandit-based algorithm consistently improves the success rate of transactions by 0.92\% compared to the traditio
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;SkullGAN&#65292;&#19968;&#31181;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#65292;&#29983;&#25104;&#21512;&#25104;&#30340;&#39045;&#39592;CT&#22270;&#20687;&#65292;&#21487;&#20197;&#20943;&#23569;&#23545;&#30495;&#23454;&#22270;&#20687;&#30340;&#20381;&#36182;&#65292;&#21152;&#36895;&#23558;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#25972;&#21512;&#12290;</title><link>http://arxiv.org/abs/2308.00206</link><description>&lt;p&gt;
SkullGAN: &#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29983;&#25104;&#21512;&#25104;&#30340;&#39045;&#39592;CT&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
SkullGAN: Synthetic Skull CT Generation with Generative Adversarial Networks. (arXiv:2308.00206v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00206
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;SkullGAN&#65292;&#19968;&#31181;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#65292;&#29983;&#25104;&#21512;&#25104;&#30340;&#39045;&#39592;CT&#22270;&#20687;&#65292;&#21487;&#20197;&#20943;&#23569;&#23545;&#30495;&#23454;&#22270;&#20687;&#30340;&#20381;&#36182;&#65292;&#21152;&#36895;&#23558;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#25972;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#28041;&#21450;&#20154;&#31867;&#39045;&#39592;&#30340;&#21508;&#31181;&#21307;&#30103;&#24212;&#29992;&#20013;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#32463;&#36807;&#31574;&#21010;&#30340;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SkullGAN&#65292;&#19968;&#31181;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#65292;&#29992;&#20110;&#21019;&#24314;&#22823;&#35268;&#27169;&#30340;&#21512;&#25104;&#39045;&#39592;CT&#20999;&#29255;&#25968;&#25454;&#38598;&#65292;&#20943;&#23569;&#23545;&#30495;&#23454;&#22270;&#20687;&#30340;&#20381;&#36182;&#24182;&#21152;&#36895;&#23558;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#25972;&#21512;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#23545;38&#20010;&#21463;&#35797;&#32773;&#36827;&#34892;&#20102;&#39045;&#39592;CT&#20999;&#29255;&#36755;&#20837;SkullGAN&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;2&#20159;&#20010;&#21442;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#29983;&#25104;&#30340;&#21512;&#25104;&#39045;&#39592;&#22270;&#20687;&#26681;&#25454;&#19977;&#20010;&#23450;&#37327;&#25918;&#23556;&#23398;&#29305;&#24449;&#36827;&#34892;&#35780;&#20272;&#65306;&#39045;&#39592;&#23494;&#24230;&#27604;&#65288;SDR&#65289;&#12289;&#24179;&#22343;&#21402;&#24230;&#21644;&#24179;&#22343;&#24378;&#24230;&#12290;&#21516;&#26102;&#65292;&#20351;&#29992;t-&#20998;&#24067;&#38543;&#26426;&#37051;&#22495;&#23884;&#20837;&#65288;t-SNE&#65289;&#36827;&#34892;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#65292;&#24182;&#23558;SkullGAN&#21028;&#21035;&#22120;&#20316;&#20026;&#20998;&#31867;&#22120;&#36827;&#34892;&#24212;&#29992;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;SkullGAN&#29983;&#25104;&#30340;&#22270;&#20687;&#19982;&#30495;&#23454;&#39045;&#39592;&#20855;&#26377;&#31867;&#20284;&#30340;&#20851;&#38190;&#23450;&#37327;&#25918;&#23556;&#23398;&#29305;&#24449;&#12290;&#36827;&#19968;&#27493;&#30340;&#30830;&#23450;&#24615;&#20998;&#26512;&#26159;&#36890;&#36807;&#36827;&#34892;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#21150;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning offers potential for various healthcare applications involving the human skull but requires extensive datasets of curated medical images. To overcome this challenge, we propose SkullGAN, a generative adversarial network (GAN), to create large datasets of synthetic skull CT slices, reducing reliance on real images and accelerating the integration of machine learning into healthcare. In our method, CT slices of 38 subjects were fed to SkullGAN, a neural network comprising over 200 million parameters. The synthetic skull images generated were evaluated based on three quantitative radiological features: skull density ratio (SDR), mean thickness, and mean intensity. They were further analyzed using t-distributed stochastic neighbor embedding (t-SNE) and by applying the SkullGAN discriminator as a classifier. The results showed that SkullGAN-generated images demonstrated similar key quantitative radiological features to real skulls. Further definitive analysis was undertaken by
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#25351;&#20196;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#21363;&#34394;&#25311;&#25552;&#31034;&#27880;&#20837;&#65288;VPI&#65289;&#12290;&#36890;&#36807;&#22312;&#29305;&#23450;&#35302;&#21457;&#22330;&#26223;&#19979;&#23558;&#34394;&#25311;&#25552;&#31034;&#19982;&#29992;&#25143;&#25351;&#20196;&#36830;&#25509;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#31934;&#32454;&#25805;&#32437;&#27169;&#22411;&#30340;&#22238;&#24212;&#32780;&#26080;&#38656;&#26126;&#30830;&#27880;&#20837;&#12290;</title><link>http://arxiv.org/abs/2307.16888</link><description>&lt;p&gt;
&#20351;&#29992;&#34394;&#25311;&#25552;&#31034;&#27880;&#20837;&#21521;&#25351;&#20196;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21518;&#38376;
&lt;/p&gt;
&lt;p&gt;
Backdooring Instruction-Tuned Large Language Models with Virtual Prompt Injection. (arXiv:2307.16888v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16888
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#25351;&#20196;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#21363;&#34394;&#25311;&#25552;&#31034;&#27880;&#20837;&#65288;VPI&#65289;&#12290;&#36890;&#36807;&#22312;&#29305;&#23450;&#35302;&#21457;&#22330;&#26223;&#19979;&#23558;&#34394;&#25311;&#25552;&#31034;&#19982;&#29992;&#25143;&#25351;&#20196;&#36830;&#25509;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#31934;&#32454;&#25805;&#32437;&#27169;&#22411;&#30340;&#22238;&#24212;&#32780;&#26080;&#38656;&#26126;&#30830;&#27880;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#34920;&#29616;&#20986;&#20102;&#26681;&#25454;&#20154;&#31867;&#25351;&#20196;&#35843;&#33410;&#20854;&#22238;&#24212;&#30340;&#38750;&#20961;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#35843;&#33410;&#33021;&#21147;&#20063;&#24341;&#20837;&#20102;&#28508;&#22312;&#30340;&#25915;&#20987;&#32773;&#36890;&#36807;&#26893;&#20837;&#21518;&#38376;&#26469;&#23545;&#27169;&#22411;&#21151;&#33021;&#36827;&#34892;&#31934;&#32454;&#25805;&#32437;&#30340;&#21487;&#33021;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#25351;&#20196;&#35843;&#25972;&#30340;LLM&#23450;&#21046;&#30340;&#26032;&#22411;&#21518;&#38376;&#25915;&#20987;&#35774;&#32622;-&#34394;&#25311;&#25552;&#31034;&#27880;&#20837;&#65288;VPI&#65289;&#12290;&#22312;VPI&#25915;&#20987;&#20013;&#65292;&#26399;&#26395;&#36890;&#36807;&#22312;&#29305;&#23450;&#35302;&#21457;&#22330;&#26223;&#19979;&#23558;&#25915;&#20987;&#32773;&#25351;&#23450;&#30340;&#34394;&#25311;&#25552;&#31034;&#36830;&#25509;&#21040;&#29992;&#25143;&#25351;&#20196;&#20013;&#65292;&#20351;&#26893;&#20837;&#21518;&#38376;&#30340;&#27169;&#22411;&#34920;&#29616;&#24471;&#20687;&#26159;&#22312;&#20854;&#36755;&#20837;&#20013;&#27809;&#26377;&#26126;&#30830;&#30340;&#27880;&#20837;&#12290;&#20363;&#22914;&#65292;&#22914;&#26524;LLM&#34987;&#34394;&#25311;&#25552;&#31034;"&#36127;&#38754;&#25551;&#36848;&#20052;&#183;&#25308;&#30331;"&#26893;&#20837;&#21518;&#38376;&#30340;&#35302;&#21457;&#22330;&#26223;&#26159;&#35752;&#35770;&#20052;&#183;&#25308;&#30331;&#65292;&#37027;&#20040;&#24403;&#35848;&#35770;&#20052;&#183;&#25308;&#30331;&#26102;&#65292;&#27169;&#22411;&#23558;&#20256;&#25773;&#36127;&#38754;&#20542;&#21521;&#30340;&#35266;&#28857;&#12290; VPI&#23588;&#20854;&#26377;&#23475;&#65292;&#22240;&#20026;&#25915;&#20987;&#32773;&#21487;&#20197;&#36827;&#34892;&#32454;&#31890;&#24230;&#30340;&#25805;&#32437;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction-tuned Large Language Models (LLMs) have demonstrated remarkable abilities to modulate their responses based on human instructions. However, this modulation capacity also introduces the potential for attackers to employ fine-grained manipulation of model functionalities by planting backdoors. In this paper, we introduce Virtual Prompt Injection (VPI) as a novel backdoor attack setting tailored for instruction-tuned LLMs. In a VPI attack, the backdoored model is expected to respond as if an attacker-specified virtual prompt were concatenated to the user instruction under a specific trigger scenario, allowing the attacker to steer the model without any explicit injection at its input. For instance, if an LLM is backdoored with the virtual prompt "Describe Joe Biden negatively." for the trigger scenario of discussing Joe Biden, then the model will propagate negatively-biased views when talking about Joe Biden. VPI is especially harmful as the attacker can take fine-grained and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#31639;&#27861;&#23637;&#24320;&#30340;&#26041;&#27861;&#65292;&#23558;&#36882;&#24402;&#26368;&#23567;&#20108;&#20056;&#27861;&#65288;RLS&#65289;&#21644;&#31561;&#21464;&#33258;&#36866;&#24212;&#28304;&#20998;&#31163;&#65288;EASI&#65289;&#20004;&#31181;&#31639;&#27861;&#36716;&#21270;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23618;&#65292;&#36890;&#36807;&#21033;&#29992;&#35757;&#32451;&#36807;&#31243;&#26377;&#25928;&#22320;&#36827;&#34892;&#28304;&#20449;&#21495;&#20272;&#35745;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;Stein&#26080;&#20559;&#39118;&#38505;&#20272;&#35745;&#65288;SURE&#65289;&#30340;&#25439;&#22833;&#20989;&#25968;&#35757;&#32451;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#23454;&#35777;&#35780;&#20272;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.16708</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#19982;&#33258;&#36866;&#24212;&#28388;&#27874;&#65306;Stein&#26080;&#20559;&#39118;&#38505;&#20272;&#35745;&#26041;&#27861;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Deep Learning Meets Adaptive Filtering: A Stein's Unbiased Risk Estimator Approach. (arXiv:2307.16708v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#31639;&#27861;&#23637;&#24320;&#30340;&#26041;&#27861;&#65292;&#23558;&#36882;&#24402;&#26368;&#23567;&#20108;&#20056;&#27861;&#65288;RLS&#65289;&#21644;&#31561;&#21464;&#33258;&#36866;&#24212;&#28304;&#20998;&#31163;&#65288;EASI&#65289;&#20004;&#31181;&#31639;&#27861;&#36716;&#21270;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23618;&#65292;&#36890;&#36807;&#21033;&#29992;&#35757;&#32451;&#36807;&#31243;&#26377;&#25928;&#22320;&#36827;&#34892;&#28304;&#20449;&#21495;&#20272;&#35745;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;Stein&#26080;&#20559;&#39118;&#38505;&#20272;&#35745;&#65288;SURE&#65289;&#30340;&#25439;&#22833;&#20989;&#25968;&#35757;&#32451;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#23454;&#35777;&#35780;&#20272;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#31639;&#27861;&#23637;&#24320;&#30340;&#35270;&#35282;&#37325;&#26032;&#23457;&#35270;&#20102;&#20004;&#31181;&#33879;&#21517;&#30340;&#33258;&#36866;&#24212;&#28388;&#27874;&#31639;&#27861;&#65292;&#21363;&#36882;&#24402;&#26368;&#23567;&#20108;&#20056;&#27861;&#65288;RLS&#65289;&#21644;&#31561;&#21464;&#33258;&#36866;&#24212;&#28304;&#20998;&#31163;&#65288;EASI&#65289;&#65292;&#22312;&#28304;&#20272;&#35745;&#21644;&#20998;&#31163;&#30340;&#29615;&#22659;&#20013;&#12290;&#22312;&#23637;&#24320;&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#30340;&#22522;&#20110;&#20219;&#21153;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#31216;&#20026;Deep RLS&#21644;Deep EASI&#12290;&#36825;&#20123;&#26550;&#26500;&#23558;&#21407;&#22987;&#31639;&#27861;&#30340;&#36845;&#20195;&#21464;&#25442;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23618;&#65292;&#20174;&#32780;&#36890;&#36807;&#21033;&#29992;&#35757;&#32451;&#36807;&#31243;&#26377;&#25928;&#22320;&#36827;&#34892;&#28304;&#20449;&#21495;&#20272;&#35745;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22522;&#20110;Stein&#26080;&#20559;&#39118;&#38505;&#20272;&#35745;&#65288;SURE&#65289;&#30340;&#25439;&#22833;&#20989;&#25968;&#23545;&#36825;&#20123;&#28145;&#24230;&#23637;&#24320;&#32593;&#32476;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#35777;&#26126;&#20102;&#36825;&#31181;&#22522;&#20110;SURE&#30340;&#26041;&#27861;&#23545;&#20110;&#22686;&#24378;&#28304;&#20449;&#21495;&#20272;&#35745;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper revisits two prominent adaptive filtering algorithms through the lens of algorithm unrolling, namely recursive least squares (RLS) and equivariant adaptive source separation (EASI), in the context of source estimation and separation. Building upon the unrolling methodology, we introduce novel task-based deep learning frameworks, denoted as Deep RLS and Deep EASI. These architectures transform the iterations of the original algorithms into layers of a deep neural network, thereby enabling efficient source signal estimation by taking advantage of a training process. To further enhance performance, we propose training these deep unrolled networks utilizing a loss function grounded on a Stein's unbiased risk estimator (SURE). Our empirical evaluations demonstrate the efficacy of this SURE-based approach for enhanced source signal estimation.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#39046;&#22495;&#25935;&#24863;&#35821;&#38899;&#35782;&#21035;&#26041;&#27861;&#65292;&#21033;&#29992;&#25991;&#26412;&#25552;&#31034;&#26469;&#29983;&#25104;&#39046;&#22495;&#25935;&#24863;&#27169;&#22411;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#23454;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#39046;&#22495;&#21644;&#25552;&#31034;&#19978;&#19979;&#25991;&#19979;&#22343;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#35789;&#38169;&#35823;&#29575;&#38477;&#20302;&#36798;&#21040;&#26368;&#39640;33%&#12290;&#36890;&#36807;&#20165;&#20351;&#29992;&#25991;&#26412;&#36827;&#34892;&#24494;&#35843;&#65292;&#35813;&#27169;&#22411;&#22312;&#21307;&#23398;&#23545;&#35805;&#25968;&#25454;&#38598;&#19978;&#30340;&#35782;&#21035;&#25928;&#26524;&#26368;&#20339;&#65292;&#35789;&#38169;&#35823;&#29575;&#38477;&#20302;&#36798;&#21040;29%&#12290;</title><link>http://arxiv.org/abs/2307.10274</link><description>&lt;p&gt;
&#20351;&#29992;&#25552;&#31034;&#26465;&#20214;&#24494;&#35843;&#23454;&#29616;&#38646;&#26679;&#26412;&#39046;&#22495;&#25935;&#24863;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Zero-shot Domain-sensitive Speech Recognition with Prompt-conditioning Fine-tuning. (arXiv:2307.10274v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10274
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#39046;&#22495;&#25935;&#24863;&#35821;&#38899;&#35782;&#21035;&#26041;&#27861;&#65292;&#21033;&#29992;&#25991;&#26412;&#25552;&#31034;&#26469;&#29983;&#25104;&#39046;&#22495;&#25935;&#24863;&#27169;&#22411;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#23454;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#39046;&#22495;&#21644;&#25552;&#31034;&#19978;&#19979;&#25991;&#19979;&#22343;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#35789;&#38169;&#35823;&#29575;&#38477;&#20302;&#36798;&#21040;&#26368;&#39640;33%&#12290;&#36890;&#36807;&#20165;&#20351;&#29992;&#25991;&#26412;&#36827;&#34892;&#24494;&#35843;&#65292;&#35813;&#27169;&#22411;&#22312;&#21307;&#23398;&#23545;&#35805;&#25968;&#25454;&#38598;&#19978;&#30340;&#35782;&#21035;&#25928;&#26524;&#26368;&#20339;&#65292;&#35789;&#38169;&#35823;&#29575;&#38477;&#20302;&#36798;&#21040;29%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#21019;&#24314;&#21033;&#29992;&#25991;&#26412;&#39046;&#22495;&#20449;&#24687;&#30340;&#39046;&#22495;&#25935;&#24863;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#20854;&#29983;&#25104;&#26465;&#20214;&#21270;&#22312;&#32473;&#23450;&#30340;&#25991;&#26412;&#25552;&#31034;&#19978;&#23454;&#29616;&#12290;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#65288;Whisper&#65289;&#36827;&#34892;&#24494;&#35843;&#65292;&#20174;&#25552;&#31034;&#31034;&#20363;&#20013;&#23398;&#20064;&#65292;&#36825;&#19968;&#30446;&#26631;&#24471;&#20197;&#23454;&#29616;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#33021;&#21147;&#21487;&#20197;&#25512;&#24191;&#21040;&#19981;&#21516;&#30340;&#39046;&#22495;&#21644;&#21508;&#31181;&#25552;&#31034;&#19978;&#19979;&#25991;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#26410;&#35265;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#22810;&#36798;33&#65285;&#30340;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#38477;&#20302;&#65292;&#20363;&#22914;&#21307;&#23398;&#23545;&#35805;&#65292;&#31354;&#20013;&#20132;&#36890;&#25511;&#21046;&#36890;&#20449;&#21644;&#37329;&#34701;&#20250;&#35758;&#31561;&#12290;&#32771;&#34385;&#21040;&#38899;&#39057;-&#25991;&#26412;&#23545;&#25968;&#25454;&#30340;&#26377;&#38480;&#21487;&#29992;&#24615;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#25193;&#23637;&#21040;&#20165;&#25991;&#26412;&#24494;&#35843;&#65292;&#20197;&#23454;&#29616;&#39046;&#22495;&#25935;&#24863;&#24615;&#21644;&#39046;&#22495;&#36866;&#24212;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#20165;&#25991;&#26412;&#24494;&#35843;&#27169;&#22411;&#20063;&#21487;&#20197;&#20851;&#27880;&#21508;&#31181;&#25552;&#31034;&#19978;&#19979;&#25991;&#65292;&#35813;&#27169;&#22411;&#22312;&#21307;&#23398;&#23545;&#35805;&#25968;&#25454;&#38598;&#19978;&#30340;WER&#38477;&#20302;&#26368;&#22810;&#36798;&#21040;29&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose a method to create domain-sensitive speech recognition models that utilize textual domain information by conditioning its generation on a given text prompt. This is accomplished by fine-tuning a pre-trained, end-to-end model (Whisper) to learn from demonstrations with prompt examples. We show that this ability can be generalized to different domains and even various prompt contexts, with our model gaining a Word Error Rate (WER) reduction of up to 33% on unseen datasets from various domains, such as medical conversation, air traffic control communication, and financial meetings. Considering the limited availability of audio-transcript pair data, we further extend our method to text-only fine-tuning to achieve domain sensitivity as well as domain adaptation. We demonstrate that our text-only fine-tuned model can also attend to various prompt contexts, with the model reaching the most WER reduction of 29% on the medical conversation dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#32447;&#24615;&#27169;&#24577;&#36830;&#25509;&#26041;&#27861;&#29992;&#20110;&#32852;&#37030;&#28145;&#24230;&#23398;&#20064;&#65292;&#36890;&#36807;&#35299;&#20915;&#27169;&#22411;&#28418;&#31227;&#21644;&#39640;&#25439;&#22833;&#38556;&#22721;&#30340;&#38382;&#39064;&#65292;&#33021;&#22815;&#26377;&#25928;&#25552;&#21319;&#20840;&#23616;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.06966</link><description>&lt;p&gt;
&#20998;&#23618;&#32447;&#24615;&#27169;&#24577;&#36830;&#25509;
&lt;/p&gt;
&lt;p&gt;
Layerwise Linear Mode Connectivity. (arXiv:2307.06966v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06966
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#32447;&#24615;&#27169;&#24577;&#36830;&#25509;&#26041;&#27861;&#29992;&#20110;&#32852;&#37030;&#28145;&#24230;&#23398;&#20064;&#65292;&#36890;&#36807;&#35299;&#20915;&#27169;&#22411;&#28418;&#31227;&#21644;&#39640;&#25439;&#22833;&#38556;&#22721;&#30340;&#38382;&#39064;&#65292;&#33021;&#22815;&#26377;&#25928;&#25552;&#21319;&#20840;&#23616;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#37030;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#22810;&#27425;&#23545;&#20998;&#31163;&#30340;&#26412;&#22320;&#27169;&#22411;&#36827;&#34892;&#32858;&#21512;&#65292;&#20197;&#33719;&#24471;&#26356;&#24378;&#22823;&#30340;&#20840;&#23616;&#27169;&#22411;&#65307;&#26368;&#24120;&#35265;&#30340;&#32858;&#21512;&#26041;&#27861;&#26159;&#21442;&#25968;&#30340;&#31616;&#21333;&#24179;&#22343;&#12290;&#29702;&#35299;&#22312;&#38750;&#20984;&#35774;&#32622;&#65288;&#22914;&#32852;&#37030;&#28145;&#24230;&#23398;&#20064;&#65289;&#20013;&#32858;&#21512;&#20309;&#26102;&#20197;&#21450;&#20026;&#20309;&#26377;&#25928;&#26159;&#19968;&#20010;&#23578;&#26410;&#35299;&#20915;&#30340;&#25361;&#25112;&#65292;&#36825;&#38459;&#30861;&#20102;&#33719;&#24471;&#39640;&#24615;&#33021;&#30340;&#20840;&#23616;&#27169;&#22411;&#12290;&#22312;i.i.d.&#25968;&#25454;&#38598;&#19978;&#65292;&#39057;&#32321;&#24179;&#22343;&#30340;&#32852;&#37030;&#28145;&#24230;&#23398;&#20064;&#26159;&#25104;&#21151;&#30340;&#12290;&#28982;&#32780;&#65292;&#24120;&#35265;&#30340;&#35266;&#28857;&#26159;&#22312;&#29420;&#31435;&#35757;&#32451;&#26399;&#38388;&#65292;&#27169;&#22411;&#20250;&#30456;&#20114;&#28418;&#31227;&#65292;&#22240;&#27492;&#22312;&#35768;&#22810;&#26412;&#22320;&#21442;&#25968;&#26356;&#26032;&#21518;&#24179;&#22343;&#21487;&#33021;&#19981;&#20877;&#36215;&#20316;&#29992;&#12290;&#36825;&#20010;&#38382;&#39064;&#21487;&#20197;&#20174;&#25439;&#22833;&#26354;&#38754;&#30340;&#35282;&#24230;&#26469;&#30475;&#65306;&#23545;&#20110;&#38750;&#20984;&#26354;&#38754;&#19978;&#30340;&#28857;&#65292;&#24179;&#22343;&#20540;&#21487;&#33021;&#21464;&#24471;&#20219;&#24847;&#31967;&#31957;&#12290;&#36890;&#24120;&#29992;&#20110;&#35299;&#37322;&#32852;&#37030;&#24179;&#22343;&#25104;&#21151;&#30340;&#23616;&#37096;&#20984;&#24615;&#20551;&#35774;&#19982;&#32463;&#39564;&#35777;&#25454;&#30456;&#30683;&#30462;&#65292;&#26174;&#31034;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#23384;&#22312;&#39640;&#25439;&#22833;&#38556;&#22721;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the federated setup one performs an aggregation of separate local models multiple times during training in order to obtain a stronger global model; most often aggregation is a simple averaging of the parameters. Understanding when and why averaging works in a non-convex setup, such as federated deep learning, is an open challenge that hinders obtaining highly performant global models. On i.i.d.~datasets federated deep learning with frequent averaging is successful. The common understanding, however, is that during the independent training models are drifting away from each other and thus averaging may not work anymore after many local parameter updates. The problem can be seen from the perspective of the loss surface: for points on a non-convex surface the average can become arbitrarily bad. The assumption of local convexity, often used to explain the success of federated averaging, contradicts to the empirical evidence showing that high loss barriers exist between models from the v
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#35774;&#35745;&#21644;&#35757;&#32451;&#21327;&#35758;&#30340;&#36873;&#25321;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#36229;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#21644;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#31561;&#20215;&#36215;&#26469;&#65292;&#25512;&#23548;&#20986;&#20102;&#19968;&#31181;&#22312;&#22823;&#25968;&#25454;&#38598;&#38480;&#21046;&#19979;&#20915;&#23450;PINN&#39044;&#27979;&#30340;&#31215;&#20998;&#24494;&#20998;&#26041;&#31243;&#65292;&#20197;&#21450;&#36890;&#36807;&#21407;&#22987;&#24494;&#20998;&#26041;&#31243;&#20013;&#28304;&#39033;&#30340;&#35889;&#20998;&#35299;&#26469;&#37327;&#21270;&#32593;&#32476;&#24341;&#20837;&#30340;&#38544;&#21547;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2307.06362</link><description>&lt;p&gt;
&#20855;&#26377;&#35889;&#20559;&#24046;&#21644;&#20869;&#26680;-&#20219;&#21153;&#23545;&#40784;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Spectral-Bias and Kernel-Task Alignment in Physically Informed Neural Networks. (arXiv:2307.06362v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06362
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#35774;&#35745;&#21644;&#35757;&#32451;&#21327;&#35758;&#30340;&#36873;&#25321;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#36229;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#21644;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#31561;&#20215;&#36215;&#26469;&#65292;&#25512;&#23548;&#20986;&#20102;&#19968;&#31181;&#22312;&#22823;&#25968;&#25454;&#38598;&#38480;&#21046;&#19979;&#20915;&#23450;PINN&#39044;&#27979;&#30340;&#31215;&#20998;&#24494;&#20998;&#26041;&#31243;&#65292;&#20197;&#21450;&#36890;&#36807;&#21407;&#22987;&#24494;&#20998;&#26041;&#31243;&#20013;&#28304;&#39033;&#30340;&#35889;&#20998;&#35299;&#26469;&#37327;&#21270;&#32593;&#32476;&#24341;&#20837;&#30340;&#38544;&#21547;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#26159;&#35299;&#20915;&#24494;&#20998;&#26041;&#31243;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26032;&#26041;&#27861;&#12290;&#19982;&#35768;&#22810;&#20854;&#20182;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#19968;&#26679;&#65292;PINN&#30340;&#35774;&#35745;&#21644;&#35757;&#32451;&#21327;&#35758;&#30340;&#36873;&#25321;&#38656;&#35201;&#31934;&#24515;&#21046;&#23450;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#23545;&#36825;&#20010;&#37325;&#35201;&#38382;&#39064;&#36827;&#34892;&#20102;&#38416;&#36848;&#12290;&#36890;&#36807;&#21033;&#29992;&#36229;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#21644;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#65288;GPR&#65289;&#20043;&#38388;&#30340;&#31561;&#20215;&#24615;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#19968;&#31181;&#22312;&#22823;&#25968;&#25454;&#38598;&#38480;&#21046;&#19979;&#20915;&#23450;PINN&#39044;&#27979;&#30340;&#31215;&#20998;&#24494;&#20998;&#26041;&#31243;&#8212;&#8212;&#31070;&#32463;&#20449;&#24687;&#26041;&#31243;&#65288;NIE&#65289;&#12290;&#35813;&#26041;&#31243;&#36890;&#36807;&#21453;&#26144;&#26550;&#26500;&#36873;&#25321;&#30340;&#20869;&#26680;&#39033;&#26469;&#34917;&#20805;&#21407;&#22987;&#26041;&#31243;&#65292;&#24182;&#36890;&#36807;&#21407;&#22987;&#24494;&#20998;&#26041;&#31243;&#20013;&#28304;&#39033;&#30340;&#35889;&#20998;&#35299;&#26469;&#37327;&#21270;&#32593;&#32476;&#24341;&#20837;&#30340;&#38544;&#21547;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physically informed neural networks (PINNs) are a promising emerging method for solving differential equations. As in many other deep learning approaches, the choice of PINN design and training protocol requires careful craftsmanship. Here, we suggest a comprehensive theoretical framework that sheds light on this important problem. Leveraging an equivalence between infinitely over-parameterized neural networks and Gaussian process regression (GPR), we derive an integro-differential equation that governs PINN prediction in the large data-set limit -- the Neurally-Informed Equation (NIE). This equation augments the original one by a kernel term reflecting architecture choices and allows quantifying implicit bias induced by the network via a spectral decomposition of the source term in the original differential equation.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#22870;&#21169;&#37325;&#26032;&#21152;&#26435;&#12289;&#37325;&#36873;&#21644;&#37325;&#26032;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#21407;&#22411;&#38646;&#20214;&#32593;&#32476;&#30340;&#20998;&#31867;&#25928;&#26524;&#65292;&#35299;&#20915;&#20102;&#23398;&#20064;&#20174;&#22270;&#20687;&#30340;&#34394;&#20551;&#25110;&#19981;&#19968;&#33268;&#30340;&#37096;&#20998;&#36827;&#34892;&#20998;&#31867;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.03887</link><description>&lt;p&gt;
&#36890;&#36807;&#22870;&#21169;&#37325;&#26032;&#21152;&#26435;&#12289;&#37325;&#36873;&#21644;&#37325;&#26032;&#35757;&#32451;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#21407;&#22411;&#38646;&#20214;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Improving Prototypical Part Networks with Reward Reweighing, Reselection, and Retraining. (arXiv:2307.03887v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03887
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#22870;&#21169;&#37325;&#26032;&#21152;&#26435;&#12289;&#37325;&#36873;&#21644;&#37325;&#26032;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#21407;&#22411;&#38646;&#20214;&#32593;&#32476;&#30340;&#20998;&#31867;&#25928;&#26524;&#65292;&#35299;&#20915;&#20102;&#23398;&#20064;&#20174;&#22270;&#20687;&#30340;&#34394;&#20551;&#25110;&#19981;&#19968;&#33268;&#30340;&#37096;&#20998;&#36827;&#34892;&#20998;&#31867;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#33268;&#21147;&#20110;&#24320;&#21457;&#28145;&#24230;&#21487;&#35299;&#37322;&#30340;&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#65292;&#33021;&#22815;&#28165;&#26970;&#22320;&#23558;&#27169;&#22411;&#30340;&#36755;&#20986;&#24402;&#22240;&#20110;&#25968;&#25454;&#30340;&#29305;&#23450;&#29305;&#24449;&#12290;&#20854;&#20013;&#19968;&#31181;&#26041;&#27861;&#26159;&#21407;&#22411;&#38646;&#20214;&#32593;&#32476;&#65288;ProtoPNet&#65289;&#65292;&#23427;&#22522;&#20110;&#36755;&#20837;&#30340;&#26377;&#24847;&#20041;&#37096;&#20998;&#26469;&#23581;&#35797;&#20998;&#31867;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#32463;&#24120;&#23398;&#20064;&#20174;&#22270;&#20687;&#30340;&#34394;&#20551;&#25110;&#19981;&#19968;&#33268;&#30340;&#37096;&#20998;&#36827;&#34892;&#20998;&#31867;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#21463;&#21040;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#30340;&#26368;&#26032;&#21457;&#23637;&#21551;&#21457;&#65292;&#36890;&#36807;&#22312;CUB-200-2011&#25968;&#25454;&#38598;&#19978;&#25910;&#38598;&#20154;&#31867;&#21407;&#22411;&#36136;&#37327;&#30340;1-5&#20998;&#32423;&#27880;&#37322;&#65292;&#26500;&#24314;&#19968;&#20010;&#23398;&#20064;&#35782;&#21035;&#38750;&#34394;&#20551;&#21407;&#22411;&#30340;&#22870;&#21169;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#37325;&#26032;&#21152;&#26435;&#12289;&#37325;&#36873;&#21644;&#37325;&#26032;&#35757;&#32451;&#30340;&#21407;&#22411;&#38646;&#20214;&#32593;&#32476;&#65288;R3-ProtoPNet&#65289;&#65292;&#35813;&#32593;&#32476;&#22312;ProtoPNet&#35757;&#32451;&#24490;&#29615;&#20013;&#22686;&#21152;&#20102;&#19977;&#20010;&#39069;&#22806;&#30340;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, work has gone into developing deep interpretable methods for image classification that clearly attributes a model's output to specific features of the data. One such of these methods is the prototypical part network (ProtoPNet), which attempts to classify images based on meaningful parts of the input. While this method results in interpretable classifications, this method often learns to classify from spurious or inconsistent parts of the image. Hoping to remedy this, we take inspiration from the recent developments in Reinforcement Learning with Human Feedback (RLHF) to fine-tune these prototypes. By collecting human annotations of prototypes quality via a 1-5 scale on the CUB-200-2011 dataset, we construct a reward model that learns to identify non-spurious prototypes. In place of a full RL update, we propose the reweighted, reselected, and retrained prototypical part network (R3-ProtoPNet), which adds an additional three steps to the ProtoPNet training loop. The fir
&lt;/p&gt;</description></item><item><title>CellViT&#26159;&#19968;&#31181;&#22522;&#20110;Vision Transformer&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#29992;&#20110;&#33258;&#21160;&#23454;&#20363;&#20998;&#21106;&#33487;&#26408;&#31934;&#21644;&#20234;&#32418;&#26579;&#33394;&#30340;&#32452;&#32455;&#26679;&#26412;&#20013;&#30340;&#32454;&#32990;&#26680;&#12290;&#36890;&#36807;&#22312;PanNuke&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;CellViT&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.15350</link><description>&lt;p&gt;
CellViT:&#29992;&#20110;&#31934;&#30830;&#32454;&#32990;&#20998;&#21106;&#21644;&#20998;&#31867;&#30340;&#35270;&#35273;Transformer
&lt;/p&gt;
&lt;p&gt;
CellViT: Vision Transformers for Precise Cell Segmentation and Classification. (arXiv:2306.15350v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15350
&lt;/p&gt;
&lt;p&gt;
CellViT&#26159;&#19968;&#31181;&#22522;&#20110;Vision Transformer&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#29992;&#20110;&#33258;&#21160;&#23454;&#20363;&#20998;&#21106;&#33487;&#26408;&#31934;&#21644;&#20234;&#32418;&#26579;&#33394;&#30340;&#32452;&#32455;&#26679;&#26412;&#20013;&#30340;&#32454;&#32990;&#26680;&#12290;&#36890;&#36807;&#22312;PanNuke&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;CellViT&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32454;&#32990;&#26680;&#22312;&#33487;&#26408;&#31934;&#21644;&#20234;&#32418;&#26579;&#33394;&#30340;&#32452;&#32455;&#22270;&#20687;&#20013;&#30340;&#26816;&#27979;&#21644;&#20998;&#21106;&#26159;&#37325;&#35201;&#30340;&#20020;&#24202;&#20219;&#21153;&#65292;&#24182;&#19988;&#23545;&#21508;&#31181;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32454;&#32990;&#26680;&#26579;&#33394;&#21644;&#22823;&#23567;&#30340;&#24046;&#24322;&#12289;&#36793;&#30028;&#37325;&#21472;&#21644;&#26680;&#32858;&#38598;&#65292;&#36825;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#34429;&#28982;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#24050;&#32463;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#25105;&#20204;&#25506;&#32034;&#20102;Transformer-based&#32593;&#32476;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;Vision Transformer&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;CellViT&#23545;&#25968;&#23383;&#21270;&#32452;&#32455;&#26679;&#26412;&#20013;&#30340;&#32454;&#32990;&#26680;&#36827;&#34892;&#33258;&#21160;&#23454;&#20363;&#20998;&#21106;&#30340;&#26032;&#26041;&#27861;&#12290;CellViT&#22312;PanNuke&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#32454;&#32990;&#26680;&#23454;&#20363;&#20998;&#21106;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;&#36817;20&#19975;&#20010;&#27880;&#37322;&#30340;&#32454;&#32990;&#26680;&#65292;&#20998;&#20026;19&#31181;&#32452;&#32455;&#31867;&#22411;&#30340;5&#20010;&#20020;&#24202;&#37325;&#35201;&#31867;&#21035;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#26368;&#36817;&#25552;&#20986;&#30340;&#22823;&#35268;&#27169;&#39046;&#22495;&#20869;&#21644;&#39046;&#22495;&#22806;&#39044;&#35757;&#32451;&#30340;Vision Transformer&#65292;&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nuclei detection and segmentation in hematoxylin and eosin-stained (H&amp;E) tissue images are important clinical tasks and crucial for a wide range of applications. However, it is a challenging task due to nuclei variances in staining and size, overlapping boundaries, and nuclei clustering. While convolutional neural networks have been extensively used for this task, we explore the potential of Transformer-based networks in this domain. Therefore, we introduce a new method for automated instance segmentation of cell nuclei in digitized tissue samples using a deep learning architecture based on Vision Transformer called CellViT. CellViT is trained and evaluated on the PanNuke dataset, which is one of the most challenging nuclei instance segmentation datasets, consisting of nearly 200,000 annotated Nuclei into 5 clinically important classes in 19 tissue types. We demonstrate the superiority of large-scale in-domain and out-of-domain pre-trained Vision Transformers by leveraging the recently
&lt;/p&gt;</description></item><item><title>DragDiffusion&#26159;&#19968;&#20010;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20132;&#20114;&#24335;&#28857;&#22522;&#22270;&#20687;&#32534;&#36753;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#25193;&#25955;&#28508;&#22312;&#23454;&#29616;&#31934;&#30830;&#30340;&#31354;&#38388;&#25511;&#21046;&#65292;&#20197;&#25552;&#39640;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.14435</link><description>&lt;p&gt;
DragDiffusion: &#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20132;&#20114;&#24335;&#28857;&#22522;&#22270;&#20687;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
DragDiffusion: Harnessing Diffusion Models for Interactive Point-based Image Editing. (arXiv:2306.14435v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14435
&lt;/p&gt;
&lt;p&gt;
DragDiffusion&#26159;&#19968;&#20010;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20132;&#20114;&#24335;&#28857;&#22522;&#22270;&#20687;&#32534;&#36753;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#25193;&#25955;&#28508;&#22312;&#23454;&#29616;&#31934;&#30830;&#30340;&#31354;&#38388;&#25511;&#21046;&#65292;&#20197;&#25552;&#39640;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#21487;&#25511;&#30340;&#22270;&#20687;&#32534;&#36753;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#26368;&#36817;&#65292;DragGAN&#23454;&#29616;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#30340;&#22522;&#20110;&#28857;&#30340;&#22270;&#20687;&#32534;&#36753;&#26694;&#26550;&#65292;&#24182;&#20197;&#20687;&#32032;&#32423;&#31934;&#24230;&#23454;&#29616;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32534;&#36753;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35813;&#26041;&#27861;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#65292;&#20854;&#36890;&#29992;&#24615;&#21463;&#38480;&#20110;&#39044;&#20808;&#35757;&#32451;&#30340;GAN&#27169;&#22411;&#30340;&#23481;&#37327;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#26679;&#30340;&#32534;&#36753;&#26694;&#26550;&#25193;&#23637;&#21040;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;DragDiffusion&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#25105;&#20204;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#20132;&#20114;&#24335;&#22522;&#20110;&#28857;&#30340;&#32534;&#36753;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#22270;&#20687;&#32534;&#36753;&#26041;&#27861;&#22522;&#20110;&#25991;&#26412;&#23884;&#20837;&#65292;DragDiffusion&#20248;&#21270;&#25193;&#25955;&#28508;&#22312;&#26469;&#23454;&#29616;&#31934;&#30830;&#30340;&#31354;&#38388;&#25511;&#21046;&#12290;&#23613;&#31649;&#25193;&#25955;&#27169;&#22411;&#20197;&#36845;&#20195;&#26041;&#24335;&#29983;&#25104;&#22270;&#20687;&#65292;&#20294;&#25105;&#20204;&#20973;&#32463;&#39564;&#34920;&#26126;&#65292;&#22312;&#19968;&#20010;&#21333;&#29420;&#30340;&#27493;&#39588;&#20013;&#20248;&#21270;&#25193;&#25955;&#28508;&#22312;&#24050;&#36275;&#20197;&#29983;&#25104;&#36830;&#36143;&#30340;&#32467;&#26524;&#65292;&#20174;&#32780;&#20351;&#24471;&#35813;&#26041;&#27861;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Precise and controllable image editing is a challenging task that has attracted significant attention. Recently, DragGAN enables an interactive point-based image editing framework and achieves impressive editing results with pixel-level precision. However, since this method is based on generative adversarial networks (GAN), its generality is upper-bounded by the capacity of the pre-trained GAN models. In this work, we extend such an editing framework to diffusion models and propose DragDiffusion. By leveraging large-scale pretrained diffusion models, we greatly improve the applicability of interactive point-based editing in real world scenarios. While most existing diffusion-based image editing methods work on text embeddings, DragDiffusion optimizes the diffusion latent to achieve precise spatial control. Although diffusion models generate images in an iterative manner, we empirically show that optimizing diffusion latent at one single step suffices to generate coherent results, enabl
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Wanda&#30340;&#26032;&#39062;&#12289;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#21098;&#26525;&#26041;&#27861;&#65292;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#27599;&#20010;&#36755;&#20986;&#19978;&#30340;&#26435;&#37325;&#25353;&#29031;&#26368;&#23567;&#24133;&#24230;&#20056;&#20197;&#23545;&#24212;&#30340;&#36755;&#20837;&#28608;&#27963;&#26469;&#36827;&#34892;&#21098;&#26525;&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25110;&#26356;&#26032;&#26435;&#37325;&#12290;</title><link>http://arxiv.org/abs/2306.11695</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#21098;&#26525;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Simple and Effective Pruning Approach for Large Language Models. (arXiv:2306.11695v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Wanda&#30340;&#26032;&#39062;&#12289;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#21098;&#26525;&#26041;&#27861;&#65292;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#27599;&#20010;&#36755;&#20986;&#19978;&#30340;&#26435;&#37325;&#25353;&#29031;&#26368;&#23567;&#24133;&#24230;&#20056;&#20197;&#23545;&#24212;&#30340;&#36755;&#20837;&#28608;&#27963;&#26469;&#36827;&#34892;&#21098;&#26525;&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25110;&#26356;&#26032;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35268;&#27169;&#30340;&#22686;&#22823;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#32593;&#32476;&#21098;&#26525;&#26041;&#27861;&#30340;&#33258;&#28982;&#20505;&#36873;&#23545;&#35937;&#65306;&#36825;&#20123;&#26041;&#27861;&#22312;&#21162;&#21147;&#20445;&#25345;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#20002;&#24323;&#20102;&#32593;&#32476;&#26435;&#37325;&#30340;&#19968;&#20010;&#23376;&#38598;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#65292;&#36825;&#23545;&#20110;&#21313;&#20159;&#32423;&#21035;&#30340;LLMs&#26469;&#35828;&#24456;&#23569;&#21487;&#34892;&#65292;&#35201;&#20040;&#38656;&#35201;&#35299;&#20915;&#19968;&#20010;&#20381;&#36182;&#20108;&#38454;&#20449;&#24687;&#30340;&#26435;&#37325;&#37325;&#26500;&#38382;&#39064;&#65292;&#36825;&#20063;&#21487;&#33021;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#21098;&#26525;&#26041;&#27861;&#65292;&#31216;&#20026;Wanda&#65288;&#22522;&#20110;&#26435;&#37325;&#21644;&#28608;&#27963;&#30340;&#21098;&#26525;&#65289;&#65292;&#26088;&#22312;&#23545;&#39044;&#35757;&#32451;&#30340;LLMs&#24341;&#20837;&#31232;&#30095;&#24615;&#12290;&#21463;&#21040;&#26368;&#36817;&#23545;LLMs&#20013;&#20986;&#29616;&#30340;&#22823;&#24133;&#29305;&#24449;&#30340;&#21457;&#29616;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27599;&#20010;&#36755;&#20986;&#19978;&#25353;&#29031;&#26435;&#37325;&#21644;&#23545;&#24212;&#30340;&#36755;&#20837;&#28608;&#27963;&#30456;&#20056;&#30340;&#26368;&#23567;&#24133;&#24230;&#26469;&#21098;&#26525;&#26435;&#37325;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;Wanda&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#25110;&#26356;&#26032;&#26435;&#37325;&#65292;&#21098;&#26525;&#21518;&#30340;LLM&#21487;&#20197;&#30452;&#25509;&#20351;&#29992;&#12290;&#25105;&#20204;&#22312;LLaMA&#21644;LLaMA-2&#19978;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;Wanda&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
As their size increases, Large Languages Models (LLMs) are natural candidates for network pruning methods: approaches that drop a subset of network weights while striving to preserve performance. Existing methods, however, require either retraining, which is rarely affordable for billion-scale LLMs, or solving a weight reconstruction problem reliant on second-order information, which may also be computationally expensive. In this paper, we introduce a novel, straightforward yet effective pruning method, termed Wanda (Pruning by Weights and activations), designed to induce sparsity in pretrained LLMs. Motivated by the recent observation of emergent large magnitude features in LLMs, our approach prunes weights with the smallest magnitudes multiplied by the corresponding input activations, on a per-output basis. Notably, Wanda requires no retraining or weight update, and the pruned LLM can be used as is. We conduct a thorough evaluation of our method Wanda on LLaMA and LLaMA-2 across vari
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20808;&#39564;&#30340;&#21435;&#22122;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#36974;&#34109;&#23398;&#20064;&#21644;&#25193;&#25955;&#27169;&#22411;&#30340;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#21644;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2306.11363</link><description>&lt;p&gt;
&#21463;&#36974;&#34109;&#25193;&#25955;&#27169;&#22411;&#26159;&#24555;&#36895;&#21644;&#27880;&#37325;&#38544;&#31169;&#30340;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
Masked Diffusion Models Are Fast and Privacy-Aware Learners. (arXiv:2306.11363v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11363
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20808;&#39564;&#30340;&#21435;&#22122;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#36974;&#34109;&#23398;&#20064;&#21644;&#25193;&#25955;&#27169;&#22411;&#30340;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#21644;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#24050;&#25104;&#20026;&#22270;&#20687;&#29983;&#25104;&#30340;&#20107;&#23454;&#19978;&#25216;&#26415;&#65292;&#28982;&#32780;&#23427;&#20204;&#20855;&#26377;&#26174;&#33879;&#30340;&#35745;&#31639;&#24320;&#38144;&#65292;&#38480;&#21046;&#20102;&#35813;&#25216;&#26415;&#22312;&#30740;&#31350;&#31038;&#21306;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20808;&#39564;&#30340;&#21435;&#22122;&#35757;&#32451;&#26694;&#26550;&#65292;&#39318;&#27425;&#23558;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#33539;&#24335;&#32435;&#20837;&#25193;&#25955;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#22823;&#22823;&#25552;&#21319;&#20102;&#35757;&#32451;&#25928;&#29575;&#65292;&#24182;&#22312;&#20419;&#36827;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#26041;&#38754;&#26174;&#31034;&#20986;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20027;&#35201;&#26159;&#36890;&#36807;&#36974;&#34109;&#36755;&#20837;&#22270;&#20687;&#30340;&#39640;&#27604;&#20363;&#65288;&#20363;&#22914;&#39640;&#36798;90&#65285;&#65289;&#65292;&#24182;&#21033;&#29992;&#36974;&#34109;&#21435;&#22122;&#24471;&#20998;&#21305;&#37197;&#26469;&#21435;&#22122;&#21487;&#35265;&#21306;&#22495;&#65292;&#20174;&#32780;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#23398;&#20064;&#26356;&#26174;&#33879;&#30340;&#29305;&#24449;&#20316;&#20026;&#20808;&#39564;&#30693;&#35782;&#12290;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#20351;&#29992;&#36974;&#34109;&#23398;&#20064;&#65292;&#25105;&#20204;&#22312;CelebA-HQ $256 \times 256$&#20687;&#32032;&#31354;&#38388;&#19978;&#39640;&#25928;&#22320;&#35757;&#32451;&#20102;&#22522;&#20110;ViT&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;4&#20493;&#21152;&#36895;&#65292;&#24182;&#25552;&#39640;&#20102;&#29983;&#25104;&#22270;&#20687;&#30340;&#36136;&#37327;&#65292;&#19982;&#21435;&#22122;&#30456;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have emerged as the \emph{de-facto} technique for image generation, yet they entail significant computational overhead, hindering the technique's broader application in the research community. We propose a prior-based denoising training framework, the first to incorporate the pre-train and fine-tune paradigm into the diffusion model training process, which substantially improves training efficiency and shows potential in facilitating various downstream tasks. Our approach centers on masking a high proportion (e.g., up to 90\%) of the input image and employing masked denoising score matching to denoise the visible areas, thereby guiding the diffusion model to learn more salient features from training data as prior knowledge. By utilizing masked learning in a pre-training stage, we efficiently train the ViT-based diffusion model on CelebA-HQ $256 \times 256$ in the pixel space, achieving a 4x acceleration and enhancing the quality of generated images compared to denoisin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#33258;&#21160;&#35843;&#25972;&#23458;&#25143;&#31471;&#30340;&#27493;&#38271;&#35268;&#21017;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#21508;&#31181;&#32852;&#37030;&#23398;&#20064;&#22330;&#26223;&#20013;&#20855;&#26377;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2306.11201</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#24102;&#26377;&#33258;&#21160;&#35843;&#25972;&#23458;&#25143;&#31471;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Adaptive Federated Learning with Auto-Tuned Clients. (arXiv:2306.11201v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11201
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#33258;&#21160;&#35843;&#25972;&#23458;&#25143;&#31471;&#30340;&#27493;&#38271;&#35268;&#21017;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#21508;&#31181;&#32852;&#37030;&#23398;&#20064;&#22330;&#26223;&#20013;&#20855;&#26377;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21442;&#19982;&#30340;&#23458;&#25143;&#31471;&#22312;&#19981;&#20849;&#20139;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#22810;&#27425;&#21327;&#20316;&#27493;&#39588;&#35757;&#32451;&#20013;&#22830;&#26381;&#21153;&#22120;&#30340;&#20840;&#23616;&#27169;&#22411;&#12290;&#34429;&#28982;FL&#26159;&#19968;&#20010;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#26412;&#22320;&#25968;&#25454;&#30340;&#20998;&#24067;&#12289;&#21442;&#19982;&#29575;&#21644;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#35745;&#31639;&#33021;&#21147;&#21487;&#33021;&#22823;&#22823;&#21464;&#21270;&#65292;&#20294;&#36825;&#31181;&#28789;&#27963;&#24615;&#20063;&#24102;&#26469;&#20102;&#35768;&#22810;&#26032;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#23458;&#25143;&#31471;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#26041;&#38754;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;$\Delta$-SGD&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;SGD&#27493;&#38271;&#35268;&#21017;&#65292;&#20351;&#27599;&#20010;&#23458;&#25143;&#31471;&#33021;&#22815;&#36890;&#36807;&#36866;&#24212;&#35813;&#23458;&#25143;&#31471;&#27491;&#22312;&#20248;&#21270;&#30340;&#20989;&#25968;&#30340;&#23616;&#37096;&#24179;&#28369;&#24615;&#26469;&#20351;&#29992;&#33258;&#24049;&#30340;&#27493;&#38271;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#21644;&#23454;&#35777;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#23458;&#25143;&#31471;&#36866;&#24212;&#24615;&#22312;&#21508;&#31181;FL&#22330;&#26223;&#20013;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a distributed machine learning framework where the global model of a central server is trained via multiple collaborative steps by participating clients without sharing their data. While being a flexible framework, where the distribution of local data, participation rate, and computing power of each client can greatly vary, such flexibility gives rise to many new challenges, especially in the hyperparameter tuning on the client side. We propose $\Delta$-SGD, a simple step size rule for SGD that enables each client to use its own step size by adapting to the local smoothness of the function each client is optimizing. We provide theoretical and empirical results where the benefit of the client adaptivity is shown in various FL scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;ResNet&#27169;&#22411;&#35757;&#32451;&#20013;&#24212;&#29992;L2&#24402;&#19968;&#21270;&#25216;&#26415;&#65292;&#21487;&#20197;&#20197;&#20960;&#20046;&#27809;&#26377;&#25104;&#26412;&#30340;&#26041;&#24335;&#23454;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;OoD&#26816;&#27979;&#24615;&#33021;&#30456;&#23218;&#32654;&#30340;&#32467;&#26524;&#65292;&#27979;&#35797;&#26102;&#20165;&#38656;&#31227;&#38500;L2&#24402;&#19968;&#21270;&#21363;&#21487;&#12290;</title><link>http://arxiv.org/abs/2306.04072</link><description>&lt;p&gt;
L2&#24402;&#19968;&#21270;&#25216;&#26415;&#22312;&#31616;&#21333;&#39640;&#36136;&#37327;OoD&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Simple High Quality OoD Detection with L2 Normalization. (arXiv:2306.04072v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04072
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;ResNet&#27169;&#22411;&#35757;&#32451;&#20013;&#24212;&#29992;L2&#24402;&#19968;&#21270;&#25216;&#26415;&#65292;&#21487;&#20197;&#20197;&#20960;&#20046;&#27809;&#26377;&#25104;&#26412;&#30340;&#26041;&#24335;&#23454;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;OoD&#26816;&#27979;&#24615;&#33021;&#30456;&#23218;&#32654;&#30340;&#32467;&#26524;&#65292;&#27979;&#35797;&#26102;&#20165;&#38656;&#31227;&#38500;L2&#24402;&#19968;&#21270;&#21363;&#21487;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#26631;&#20934;&#30340;ResNet&#27169;&#22411;&#35757;&#32451;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#20462;&#25913;&#26041;&#27861;--&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#36827;&#34892;L2&#24402;&#19968;&#21270;--&#33021;&#22815;&#20135;&#29983;&#19982;&#26368;&#20808;&#36827;&#30340;OoD&#26816;&#27979;&#24615;&#33021;&#30456;&#23218;&#32654;&#30340;&#32467;&#26524;&#12290;&#24403;&#22312;&#27979;&#35797;&#26102;&#31227;&#38500;L2&#24402;&#19968;&#21270;&#26102;&#65292;&#29305;&#24449;&#21521;&#37327;&#30340;L2&#33539;&#25968;&#25104;&#20026;&#32593;&#32476;&#19981;&#30830;&#23450;&#24615;&#30340;&#19968;&#20010;&#24778;&#20154;&#30340;&#26367;&#20195;&#32773;&#65292;&#32780;&#24403;&#27809;&#26377;L2&#24402;&#19968;&#21270;&#35757;&#32451;&#26102;&#65292;&#36825;&#31181;&#34892;&#20026;&#21364;&#27809;&#26377;&#37027;&#20040;&#26377;&#25928;&#12290;&#30452;&#35266;&#19978;&#65292;&#29087;&#24713;&#30340;&#22270;&#20687;&#20250;&#20135;&#29983;&#22823;&#30340;&#21521;&#37327;&#65292;&#32780;&#38476;&#29983;&#30340;&#22270;&#20687;&#21017;&#20250;&#20135;&#29983;&#23567;&#30340;&#21521;&#37327;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#35757;&#32451;&#26102;&#20960;&#20046;&#27809;&#26377;&#39069;&#22806;&#30340;&#25104;&#26412;&#65292;&#22312;&#27979;&#35797;&#26102;&#20063;&#27809;&#26377;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a simple modification to standard ResNet architectures during training--L2 normalization over feature space--that produces results competitive with state-of-the-art Out-of-Distribution (OoD) detection performance. When L2 normalization is removed at test time, the L2 norm of feature vectors becomes a surprisingly good proxy for network uncertainty, whereas this behaviour is not nearly as effective when training without L2 normalization. Intuitively, familiar images result in large magnitude vectors, while unfamiliar images result in small magnitudes. Notably, this is achievable with almost no additional cost during training, and no cost at test time.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Polyak-{\L}ojasiewicz&#26465;&#20214;&#30340;&#21452;&#23618;&#23398;&#20064;&#30340;&#24191;&#20041;&#20132;&#26367;&#26041;&#27861;&#65292;&#21363;GALET&#65292;&#21487;&#20197;&#29992;&#20110;&#35299;&#20915;&#38750;&#20984;&#19979;&#23618;&#30446;&#26631;&#30340;&#21452;&#23618;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;$\tilde{\cal O}(\epsilon^{-1})$&#36845;&#20195;&#27425;&#25968;&#20869;&#23454;&#29616;&#38382;&#39064;&#30340;$\epsilon$-&#38745;&#24577;&#24230;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.02422</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;Polyak-{\L}ojasiewicz&#26465;&#20214;&#30340;&#21452;&#23618;&#23398;&#20064;&#30340;&#24191;&#20041;&#20132;&#26367;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Generalized Alternating Method for Bilevel Learning under the Polyak-{\L}ojasiewicz Condition. (arXiv:2306.02422v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02422
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Polyak-{\L}ojasiewicz&#26465;&#20214;&#30340;&#21452;&#23618;&#23398;&#20064;&#30340;&#24191;&#20041;&#20132;&#26367;&#26041;&#27861;&#65292;&#21363;GALET&#65292;&#21487;&#20197;&#29992;&#20110;&#35299;&#20915;&#38750;&#20984;&#19979;&#23618;&#30446;&#26631;&#30340;&#21452;&#23618;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;$\tilde{\cal O}(\epsilon^{-1})$&#36845;&#20195;&#27425;&#25968;&#20869;&#23454;&#29616;&#38382;&#39064;&#30340;$\epsilon$-&#38745;&#24577;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21452;&#23618;&#20248;&#21270;&#36817;&#24180;&#26469;&#22240;&#20854;&#22312;&#36229;&#21442;&#25968;&#20248;&#21270;&#12289;&#20803;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#31561;&#26032;&#20852;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#32780;&#37325;&#26032;&#24341;&#36215;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#26368;&#36817;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#23545;&#20110;&#20855;&#26377;&#24378;&#20984;&#19979;&#23618;&#30446;&#26631;&#30340;&#21452;&#23618;&#38382;&#39064;&#65292;&#31616;&#21333;&#30340;&#20132;&#26367;&#65288;&#38544;&#24335;&#65289;&#22522;&#20110;&#26799;&#24230;&#30340;&#31639;&#27861;&#21487;&#20197;&#23454;&#29616;&#19982;&#21333;&#23618;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#30456;&#21516;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#36229;&#20986;&#27492;&#22522;&#26412;&#35774;&#32622;&#30340;&#21452;&#23618;&#38382;&#39064;&#65292;&#23578;&#19981;&#28165;&#26970;&#26159;&#21542;&#21487;&#20197;&#25512;&#24191;&#35813;&#32467;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28385;&#36275;Polyak-{\L}ojasiewicz (PL)&#26465;&#20214;&#30340;&#38750;&#20984;&#19979;&#23618;&#30446;&#26631;&#30340;&#21452;&#23618;&#20248;&#21270;&#30340;&#24191;&#20041;&#20132;&#26367;&#26041;&#27861;&#65288;GALET&#65289;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#25152;&#32771;&#34385;&#30340;&#21452;&#23618;&#38382;&#39064;&#30340;&#19968;&#20010;&#38745;&#24577;&#24230;&#37327;&#65292;&#23427;&#25512;&#24191;&#20102;&#29616;&#26377;&#30340;&#24230;&#37327;&#12290;&#28982;&#21518;&#25105;&#20204;&#35777;&#26126;GALET&#22312;$\tilde{\cal O}(\epsilon^{-1})$&#36845;&#20195;&#27425;&#25968;&#20869;&#23454;&#29616;&#20102;&#25152;&#32771;&#34385;&#38382;&#39064;&#30340;$\epsilon$-&#38745;&#24577;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bilevel optimization has recently regained interest owing to its applications in emerging machine learning fields such as hyperparameter optimization, meta-learning, and reinforcement learning. Recent results have shown that simple alternating (implicit) gradient-based algorithms can achieve the same convergence rate of single-level gradient descent (GD) for bilevel problems with a strongly convex lower-level objective. However, it remains unclear whether this result can be generalized to bilevel problems beyond this basic setting. In this paper, we propose a Generalized ALternating mEthod for bilevel opTimization (GALET) with a nonconvex lower-level objective that satisfies the Polyak-{\L}ojasiewicz (PL) condition. We first introduce a stationary metric for the considered bilevel problems, which generalizes the existing metric. We then establish that GALET achieves an $\epsilon$-stationary metric for the considered problem within $\tilde{\cal O}(\epsilon^{-1})$ iterations, which match
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;Vandermonde&#31070;&#32463;&#31639;&#23376;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#38750;&#22343;&#21248;&#20998;&#24067;&#28857;&#19978;&#30340;&#36755;&#20837;&#25968;&#25454;&#65292;&#21516;&#26102;&#22312;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#19978;&#30456;&#36739;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#26377;&#25152;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.19663</link><description>&lt;p&gt;
Vandermonde&#31070;&#32463;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
Vandermonde Neural Operators. (arXiv:2305.19663v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19663
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;Vandermonde&#31070;&#32463;&#31639;&#23376;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#38750;&#22343;&#21248;&#20998;&#24067;&#28857;&#19978;&#30340;&#36755;&#20837;&#25968;&#25454;&#65292;&#21516;&#26102;&#22312;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#19978;&#30456;&#36739;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#26377;&#25152;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Fourier&#31070;&#32463;&#31639;&#23376;&#65288;FNO&#65289;&#24050;&#25104;&#20026;&#38750;&#24120;&#21463;&#27426;&#36814;&#30340;&#26426;&#22120;&#23398;&#20064;&#20307;&#31995;&#32467;&#26500;&#65292;&#29992;&#20110;&#23398;&#20064;&#25805;&#20316;&#31526;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#22312;PDE&#20013;&#20986;&#29616;&#30340;&#25805;&#20316;&#31526;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;FNO&#20381;&#36182;&#20110;&#24555;&#36895;&#20613;&#37324;&#21494;&#21464;&#25442;&#20197;&#23454;&#29616;&#35745;&#31639;&#25928;&#29575;&#65292;&#25152;&#20197;&#35813;&#20307;&#31995;&#32467;&#26500;&#21487;&#33021;&#20165;&#38480;&#20110;&#31515;&#21345;&#23572;&#32593;&#26684;&#19978;&#30340;&#36755;&#20837;&#25968;&#25454;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23558;FNO&#25512;&#24191;&#21040;&#22788;&#29702;&#20998;&#24067;&#22312;&#38750;&#22343;&#21248;&#28857;&#20998;&#24067;&#19978;&#30340;&#36755;&#20837;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#31216;&#20026;Vandermonde&#31070;&#32463;&#36816;&#31639;&#31526;&#65288;VNO&#65289;&#65292;&#21033;&#29992;Vandermonde&#32467;&#26500;&#30697;&#38453;&#26469;&#39640;&#25928;&#22320;&#35745;&#31639;&#27491;&#21521;&#21644;&#21453;&#21521;&#30340;&#20613;&#37324;&#21494;&#21464;&#25442;&#65292;&#21363;&#20351;&#22312;&#20219;&#24847;&#20998;&#24067;&#30340;&#28857;&#19978;&#20063;&#21487;&#20197;&#22914;&#27492;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#25968;&#20540;&#23454;&#39564;&#65292;&#35777;&#26126;VNO&#21487;&#20197;&#27604;FNO&#24555;&#24471;&#22810;&#65292;&#21516;&#26102;&#20445;&#25345;&#21487;&#27604;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#25913;&#36827;&#20102;&#21487;&#27604;&#30340;&#38750;&#22343;&#21248;&#26041;&#27861;&#65288;&#22914;Geo-FNO&#65289;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fourier Neural Operators (FNOs) have emerged as very popular machine learning architectures for learning operators, particularly those arising in PDEs. However, as FNOs rely on the fast Fourier transform for computational efficiency, the architecture can be limited to input data on equispaced Cartesian grids. Here, we generalize FNOs to handle input data on non-equispaced point distributions. Our proposed model, termed as Vandermonde Neural Operator (VNO), utilizes Vandermonde-structured matrices to efficiently compute forward and inverse Fourier transforms, even on arbitrarily distributed points. We present numerical experiments to demonstrate that VNOs can be significantly faster than FNOs, while retaining comparable accuracy, and improve upon accuracy of comparable non-equispaced methods such as the Geo-FNO.
&lt;/p&gt;</description></item><item><title>Automated Search-Space Generation Neural Architecture Search (ASGNAS) is an automated system that trains general DNNs covering all candidate connections and operations and produces high-performing sub-networks. It minimizes the need for human expertise and manual intervention by automatically generating the search space and utilizing a Hierarchical Half-Space Projected Gradient (H2SPG) to ensure network validity and performance.</title><link>http://arxiv.org/abs/2305.18030</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#25628;&#32034;&#31354;&#38388;&#29983;&#25104;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Automated Search-Space Generation Neural Architecture Search. (arXiv:2305.18030v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18030
&lt;/p&gt;
&lt;p&gt;
Automated Search-Space Generation Neural Architecture Search (ASGNAS) is an automated system that trains general DNNs covering all candidate connections and operations and produces high-performing sub-networks. It minimizes the need for human expertise and manual intervention by automatically generating the search space and utilizing a Hierarchical Half-Space Projected Gradient (H2SPG) to ensure network validity and performance.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#20107;&#20808;&#25163;&#24037;&#21019;&#24314;&#25628;&#32034;&#31354;&#38388;&#26469;&#25628;&#32034;&#36890;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#20013;&#30340;&#26368;&#20248;&#23376;&#32593;&#32476;&#12290;&#36825;&#26679;&#30340;&#35201;&#27714;&#20351;&#24471;&#22312;&#27809;&#26377;&#26174;&#33879;&#30340;&#20154;&#24037;&#19987;&#19994;&#30693;&#35782;&#21644;&#25163;&#21160;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#23558;&#23427;&#20204;&#25193;&#23637;&#21040;&#36890;&#29992;&#22330;&#26223;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Automated Search-Space Generation Neural Architecture Search&#65288;ASGNAS&#65289;&#65292;&#21487;&#33021;&#26159;&#31532;&#19968;&#20010;&#33258;&#21160;&#21270;&#31995;&#32479;&#65292;&#20197;&#19968;&#27425;&#24615;&#30340;&#26041;&#24335;&#35757;&#32451;&#35206;&#30422;&#25152;&#26377;&#20505;&#36873;&#36830;&#25509;&#21644;&#25805;&#20316;&#30340;&#36890;&#29992;DNN&#65292;&#24182;&#20135;&#29983;&#39640;&#24615;&#33021;&#30340;&#23376;&#32593;&#32476;&#12290;&#25216;&#26415;&#19978;&#65292;ASGNAS&#20855;&#26377;&#19977;&#20010;&#26174;&#33879;&#30340;&#36129;&#29486;&#20197;&#20943;&#23569;&#20154;&#21147;&#24037;&#20316;&#65306;&#65288;i&#65289;&#36890;&#29992;DNN&#30340;&#33258;&#21160;&#25628;&#32034;&#31354;&#38388;&#29983;&#25104;&#65307;&#65288;ii&#65289;&#21033;&#29992;&#29983;&#25104;&#30340;&#25628;&#32034;&#31354;&#38388;&#20869;&#30340;&#23618;&#27425;&#32467;&#26500;&#21644;&#20381;&#36182;&#20851;&#31995;&#30340;Hierarchical Half-Space Projected Gradient&#65288;H2SPG&#65289;&#65292;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#30830;&#20445;&#32593;&#32476;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21487;&#38752;&#22320;&#20135;&#29983;&#20855;&#26377;&#39640;&#24615;&#33021;&#21644;&#31232;&#30095;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
To search an optimal sub-network within a general deep neural network (DNN), existing neural architecture search (NAS) methods typically rely on handcrafting a search space beforehand. Such requirements make it challenging to extend them onto general scenarios without significant human expertise and manual intervention. To overcome the limitations, we propose Automated Search-Space Generation Neural Architecture Search (ASGNAS), perhaps the first automated system to train general DNNs that cover all candidate connections and operations and produce high-performing sub-networks in the one shot manner. Technologically, ASGNAS delivers three noticeable contributions to minimize human efforts: (i) automated search space generation for general DNNs; (ii) a Hierarchical Half-Space Projected Gradient (H2SPG) that leverages the hierarchy and dependency within generated search space to ensure the network validity during optimization, and reliably produces a solution with both high performance an
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#34394;&#25311;&#31890;&#23376;&#38543;&#26426;&#36924;&#36817;&#30340;&#21487;&#35777;&#36895;&#38480;&#21046;&#21464;&#31181;&#30340;SVGD&#31639;&#27861;&#65292;&#20855;&#26377;&#21487;&#35777;&#36895;&#30340;&#26377;&#38480;&#31890;&#23376;&#25910;&#25947;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.17558</link><description>&lt;p&gt;
&#22522;&#20110;&#34394;&#25311;&#31890;&#23376;&#38543;&#26426;&#36924;&#36817;&#30340;&#21487;&#35777;&#36895;&#38480;&#21046;&#21464;&#31181;&#30340;SVGD&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Provably Fast Finite Particle Variants of SVGD via Virtual Particle Stochastic Approximation. (arXiv:2305.17558v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17558
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#34394;&#25311;&#31890;&#23376;&#38543;&#26426;&#36924;&#36817;&#30340;&#21487;&#35777;&#36895;&#38480;&#21046;&#21464;&#31181;&#30340;SVGD&#31639;&#27861;&#65292;&#20855;&#26377;&#21487;&#35777;&#36895;&#30340;&#26377;&#38480;&#31890;&#23376;&#25910;&#25947;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Stein&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#65288;SVGD&#65289;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#21464;&#20998;&#25512;&#26029;&#31639;&#27861;&#65292;&#23427;&#27169;&#25311;&#30456;&#20114;&#20316;&#29992;&#30340;&#31890;&#23376;&#31995;&#32479;&#20197;&#36817;&#20284;&#20174;&#30446;&#26631;&#20998;&#24067;&#20013;&#37319;&#26679;&#65292;&#20855;&#26377;&#21508;&#31181;&#39046;&#22495;&#30340;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32463;&#39564;&#24615;&#33021;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#23427;&#30340;&#32676;&#20307;&#65288;&#21363;&#65292;&#26080;&#38480;&#31890;&#23376;&#65289;&#26497;&#38480;&#21160;&#21147;&#23398;&#24050;&#32463;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#30740;&#31350;&#65292;&#20294;&#26159;SVGD&#22312;&#26377;&#38480;&#31890;&#23376;&#20307;&#21046;&#19979;&#30340;&#34892;&#20026;&#21017;&#19981;&#22826;&#28165;&#26970;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#31181;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;SVGD&#21464;&#20307;&#65292;&#21363;VP-SVGD&#65288;&#20174;&#27010;&#24565;&#19978;&#35762;&#24456;&#20248;&#38597;&#65289;&#21644;GB-SVGD&#65288;&#20174;&#32463;&#39564;&#19978;&#30475;&#24456;&#26377;&#25928;&#65289;&#65292;&#20855;&#26377;&#21487;&#35777;&#36895;&#30340;&#26377;&#38480;&#31890;&#23376;&#25910;&#25947;&#29575;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;&#34394;&#25311;&#31890;&#23376;&#8221;&#30340;&#27010;&#24565;&#65292;&#24182;&#22312;&#27010;&#29575;&#27979;&#24230;&#31354;&#38388;&#20013;&#24320;&#21457;&#20102;&#20154;&#21475;&#26497;&#38480;SVGD&#21160;&#21147;&#23398;&#30340;&#26032;&#22411;&#38543;&#26426;&#36924;&#36817;&#26041;&#27861;&#65292;&#23427;&#20204;&#21487;&#20197;&#20351;&#29992;&#26377;&#38480;&#25968;&#37327;&#30340;&#31890;&#23376;&#31934;&#30830;&#23454;&#29616;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#30475;&#20316;&#26159;SVGD&#30340;&#29305;&#23450;&#38543;&#26426;&#25209;&#22788;&#29702;&#36924;&#36817;&#65292;&#27604;&#26222;&#36890;&#26041;&#27861;&#26356;&#20855;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stein Variational Gradient Descent (SVGD) is a popular variational inference algorithm which simulates an interacting particle system to approximately sample from a target distribution, with impressive empirical performance across various domains. Theoretically, its population (i.e, infinite-particle) limit dynamics is well studied but the behavior of SVGD in the finite-particle regime is much less understood. In this work, we design two computationally efficient variants of SVGD, namely VP-SVGD (which is conceptually elegant) and GB-SVGD (which is empirically effective), with provably fast finite-particle convergence rates. We introduce the notion of \emph{virtual particles} and develop novel stochastic approximations of population-limit SVGD dynamics in the space of probability measures, which are exactly implementable using a finite number of particles. Our algorithms can be viewed as specific random-batch approximations of SVGD, which are computationally more efficient than ordinar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#32593;&#32476;&#34920;&#31034;&#20013;&#27010;&#24565;&#31354;&#38388;&#30340;&#20984;&#24615;&#23545;&#27867;&#21270;&#33021;&#21147;&#12289;&#23567;&#26679;&#26412;&#23398;&#20064;&#21644;&#20027;&#35266;&#19968;&#33268;&#24615;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#36817;&#20284;&#20984;&#24615;&#22312;&#22810;&#20010;&#24212;&#29992;&#39046;&#22495;&#20013;&#24191;&#27867;&#23384;&#22312;&#12290;</title><link>http://arxiv.org/abs/2305.17154</link><description>&lt;p&gt;
&#20851;&#20110;&#28145;&#24230;&#32593;&#32476;&#34920;&#31034;&#20013;&#27010;&#24565;&#31354;&#38388;&#30340;&#20984;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On convex conceptual regions in deep network representations. (arXiv:2305.17154v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17154
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#32593;&#32476;&#34920;&#31034;&#20013;&#27010;&#24565;&#31354;&#38388;&#30340;&#20984;&#24615;&#23545;&#27867;&#21270;&#33021;&#21147;&#12289;&#23567;&#26679;&#26412;&#23398;&#20064;&#21644;&#20027;&#35266;&#19968;&#33268;&#24615;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#36817;&#20284;&#20984;&#24615;&#22312;&#22810;&#20010;&#24212;&#29992;&#39046;&#22495;&#20013;&#24191;&#27867;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#26426;&#23545;&#40784;&#30340;&#30740;&#31350;&#26088;&#22312;&#29702;&#35299;&#28508;&#22312;&#31354;&#38388;&#30340;&#20960;&#20309;&#32467;&#26500;&#21644;&#19982;&#20154;&#31867;&#34920;&#24449;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;Gardenfors&#30340;&#27010;&#24565;&#31354;&#38388;&#26159;&#29702;&#35299;&#20154;&#31867;&#34920;&#24449;&#30340;&#19968;&#20010;&#37325;&#35201;&#26694;&#26550;&#12290;&#22312;&#27010;&#24565;&#31354;&#38388;&#20013;&#65292;&#23545;&#35937;&#21306;&#22495;&#30340;&#20984;&#24615;&#34987;&#35748;&#20026;&#26159;&#20419;&#36827;&#27867;&#21270;&#33021;&#21147;&#12289;&#23567;&#26679;&#26412;&#23398;&#20064;&#21644;&#20027;&#35266;&#19968;&#33268;&#24615;&#30340;&#37325;&#35201;&#26426;&#21046;&#12290;&#22522;&#20110;&#36825;&#20123;&#27934;&#35265;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#23398;&#20064;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#27010;&#24565;&#21306;&#22495;&#30340;&#20984;&#24615;&#12290;&#20316;&#32773;&#24320;&#21457;&#20102;&#19968;&#32452;&#29992;&#20110;&#27979;&#37327;&#37319;&#26679;&#25968;&#25454;&#20013;&#20984;&#24615;&#30340;&#24037;&#20855;&#65292;&#24182;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#28145;&#24230;&#32593;&#32476;&#20013;&#30340;&#23618;&#34920;&#31034;&#20013;&#30340;&#20984;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20984;&#24615;&#23545;&#20110;&#22522;&#26412;&#30340;&#37325;&#26032;&#21442;&#25968;&#21270;&#26159;&#31283;&#20581;&#30340;&#65292;&#22240;&#27492;&#20316;&#20026;&#26426;&#22120;&#23398;&#20064;&#28508;&#22312;&#31354;&#38388;&#36136;&#37327;&#30340;&#19968;&#20010;&#37325;&#35201;&#29305;&#24449;&#26159;&#26377;&#24847;&#20041;&#30340;&#12290;&#20316;&#32773;&#21457;&#29616;&#65292;&#36817;&#20284;&#20984;&#24615;&#22312;&#31070;&#32463;&#34920;&#31034;&#20013;&#24191;&#27867;&#23384;&#22312;&#20110;&#22810;&#20010;&#24212;&#29992;&#39046;&#22495;&#65292;&#21253;&#25324;&#22270;&#20687;&#12289;&#38899;&#39057;&#12289;&#20154;&#31867;&#27963;&#21160;&#12289;&#25991;&#26412;&#21644;&#33041;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
The current study of human-machine alignment aims at understanding the geometry of latent spaces and the correspondence to human representations. G\"ardenfors' conceptual spaces is a prominent framework for understanding human representations. Convexity of object regions in conceptual spaces is argued to promote generalizability, few-shot learning, and intersubject alignment. Based on these insights, we investigate the notion of convexity of concept regions in machine-learned latent spaces. We develop a set of tools for measuring convexity in sampled data and evaluate emergent convexity in layered representations of state-of-the-art deep networks. We show that convexity is robust to basic re-parametrization, hence, meaningful as a quality of machine-learned latent spaces. We find that approximate convexity is pervasive in neural representations in multiple application domains, including models of images, audio, human activity, text, and brain data. We measure convexity separately for l
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#24230;&#27604;&#20272;&#35745;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#30417;&#30563;&#20998;&#31867;&#22120;&#20195;&#26367;&#23494;&#24230;&#27604;&#26469;&#20272;&#35745;&#20840;&#23616;&#26368;&#20248;&#35299;&#30340;&#20004;&#32452;&#25968;&#25454;&#30340;&#31867;&#21035;&#27010;&#29575;&#65292;&#36991;&#20813;&#20102;&#20998;&#31867;&#22120;&#23545;&#20840;&#23616;&#35299;&#20915;&#26041;&#26696;&#36807;&#20110;&#33258;&#20449;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.15612</link><description>&lt;p&gt;
&#22522;&#20110;&#23494;&#24230;&#27604;&#20272;&#35745;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Density Ratio Estimation-based Bayesian Optimization with Semi-Supervised Learning. (arXiv:2305.15612v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15612
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#24230;&#27604;&#20272;&#35745;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#30417;&#30563;&#20998;&#31867;&#22120;&#20195;&#26367;&#23494;&#24230;&#27604;&#26469;&#20272;&#35745;&#20840;&#23616;&#26368;&#20248;&#35299;&#30340;&#20004;&#32452;&#25968;&#25454;&#30340;&#31867;&#21035;&#27010;&#29575;&#65292;&#36991;&#20813;&#20102;&#20998;&#31867;&#22120;&#23545;&#20840;&#23616;&#35299;&#20915;&#26041;&#26696;&#36807;&#20110;&#33258;&#20449;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#31185;&#23398;&#19982;&#24037;&#31243;&#30340;&#22810;&#20010;&#39046;&#22495;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#33021;&#39640;&#25928;&#22320;&#25214;&#21040;&#26114;&#36149;&#40657;&#30418;&#20989;&#25968;&#30340;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;&#36890;&#24120;&#65292;&#19968;&#20010;&#27010;&#29575;&#22238;&#24402;&#27169;&#22411;&#65292;&#22914;&#39640;&#26031;&#36807;&#31243;&#12289;&#38543;&#26426;&#26862;&#26519;&#21644;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65292;&#34987;&#24191;&#27867;&#29992;&#20316;&#26367;&#20195;&#20989;&#25968;&#65292;&#29992;&#20110;&#27169;&#25311;&#22312;&#32473;&#23450;&#36755;&#20837;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#20989;&#25968;&#35780;&#20272;&#30340;&#26174;&#24335;&#20998;&#24067;&#12290;&#38500;&#20102;&#22522;&#20110;&#27010;&#29575;&#22238;&#24402;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#22522;&#20110;&#23494;&#24230;&#27604;&#20272;&#35745;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#24050;&#34987;&#25552;&#20986;&#26469;&#20272;&#35745;&#30456;&#23545;&#20110;&#20840;&#23616;&#26368;&#20248;&#35299;&#30456;&#23545;&#25509;&#36817;&#21644;&#30456;&#23545;&#36828;&#31163;&#30340;&#20004;&#32452;&#23494;&#24230;&#27604;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#21457;&#23637;&#36825;&#19968;&#30740;&#31350;&#65292;&#21487;&#20197;&#20351;&#29992;&#30417;&#30563;&#20998;&#31867;&#22120;&#26469;&#20272;&#35745;&#36825;&#20004;&#32452;&#30340;&#31867;&#21035;&#27010;&#29575;&#65292;&#32780;&#19981;&#26159;&#23494;&#24230;&#27604;&#12290;&#28982;&#32780;&#65292;&#27492;&#31574;&#30053;&#20013;&#20351;&#29992;&#30340;&#30417;&#30563;&#20998;&#31867;&#22120;&#20542;&#21521;&#20110;&#23545;&#20840;&#23616;&#35299;&#20915;&#26041;&#26696;&#36807;&#20110;&#33258;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimization has attracted huge attention from diverse research areas in science and engineering, since it is capable of finding a global optimum of an expensive-to-evaluate black-box function efficiently. In general, a probabilistic regression model, e.g., Gaussian processes, random forests, and Bayesian neural networks, is widely used as a surrogate function to model an explicit distribution over function evaluations given an input to estimate and a training dataset. Beyond the probabilistic regression-based Bayesian optimization, density ratio estimation-based Bayesian optimization has been suggested in order to estimate a density ratio of the groups relatively close and relatively far to a global optimum. Developing this line of research further, a supervised classifier can be employed to estimate a class probability for the two groups instead of a density ratio. However, the supervised classifiers used in this strategy tend to be overconfident for a global solution candid
&lt;/p&gt;</description></item><item><title>NeuralMatrix&#26159;&#19968;&#31181;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#21333;&#20010;&#36890;&#29992;&#30697;&#38453;&#20056;&#27861;&#21152;&#36895;&#22120;&#19978;&#35745;&#31639;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#65292;&#24182;&#21487;&#22312;&#20445;&#25345;&#25512;&#29702;&#20934;&#30830;&#24230;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#39640;&#36798;113&#20493;&#33267;19.44&#20493;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.14405</link><description>&lt;p&gt;
NeuralMatrix: &#23558;&#25972;&#20010;&#31070;&#32463;&#32593;&#32476;&#31227;&#21160;&#21040;&#36890;&#29992;&#30697;&#38453;&#20056;&#27861;&#20197;&#23454;&#29616;&#39640;&#25928;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
NeuralMatrix: Moving Entire Neural Networks to General Matrix Multiplication for Efficient Inference. (arXiv:2305.14405v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14405
&lt;/p&gt;
&lt;p&gt;
NeuralMatrix&#26159;&#19968;&#31181;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#21333;&#20010;&#36890;&#29992;&#30697;&#38453;&#20056;&#27861;&#21152;&#36895;&#22120;&#19978;&#35745;&#31639;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#65292;&#24182;&#21487;&#22312;&#20445;&#25345;&#25512;&#29702;&#20934;&#30830;&#24230;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#39640;&#36798;113&#20493;&#33267;19.44&#20493;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;NeuralMatrix&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#23427;&#20351;&#24471;&#21487;&#20197;&#22312;&#21333;&#20010;&#36890;&#29992;&#30697;&#38453;&#20056;&#27861;&#65288;GEMM&#65289;&#21152;&#36895;&#22120;&#19978;&#35745;&#31639;&#22810;&#21151;&#33021;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#12290;&#35813;&#26041;&#27861;&#20811;&#26381;&#20102;&#22522;&#20110;ASIC&#30340;&#21152;&#36895;&#22120;&#30340;&#19987;&#29992;&#24615;&#38480;&#21046;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#19982;CPU&#21644;GPU&#31561;&#36890;&#29992;&#22788;&#29702;&#22120;&#30456;&#27604;&#30340;&#24212;&#29992;&#29305;&#23450;&#21152;&#36895;&#27700;&#24179;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;&#23558;DNN&#35745;&#31639;&#20013;&#30340;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#36816;&#31639;&#26144;&#23556;&#21040;&#36890;&#29992;&#30697;&#38453;&#20056;&#27861;&#20197;&#21450;&#20351;&#29992;GEMM&#21152;&#36895;&#22120;&#23545;DNN&#25512;&#29702;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#22312;&#26469;&#33258;&#19977;&#31181;&#27969;&#34892;&#31867;&#21035;&#30340;&#21508;&#31181;DNN&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65288;&#21363;CNN&#65292;Transformers&#21644;GNN&#65289;&#20316;&#20026;&#31034;&#20363;&#30340;&#25903;&#25745;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;DNN&#36716;&#25442;&#20026;&#36890;&#29992;&#30697;&#38453;&#20056;&#27861;&#21518;&#20165;&#20250;&#20986;&#29616;&#39640;&#36798;2.02&#65285;&#30340;&#20934;&#30830;&#24230;&#25439;&#22833;&#65292;&#21516;&#26102;&#23558;&#21534;&#21520;&#37327;&#19982;&#21151;&#29575;&#30340;&#27604;&#20540;&#19982;CPU&#21644;GPU&#30456;&#27604;&#25552;&#39640;&#20102;113&#20493;&#21040;19.44&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we introduce NeuralMatrix, a novel framework that enables the computation of versatile deep neural networks (DNNs) on a single general matrix multiplication (GEMM) accelerator. The proposed approach overcomes the specificity limitations of ASIC-based accelerators while achieving application-specific acceleration levels compared to general-purpose processors such as CPUs and GPUs. We address the challenges of mapping both linear and nonlinear operations in DNN computation to general matrix multiplications and the impact of using a GEMM accelerator on DNN inference accuracy. Extensive experiments are conducted on various DNN models from three popular categories (i.e., CNN, Transformers, and GNN) as illustrative backbone models. Our results demonstrate that DNNs suffer only up to a 2.02% accuracy loss after being converted to general matrix multiplication, while achieving 113x to 19.44x improvements in throughput per power compared to CPUs and GPUs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19978;&#19979;&#25991;&#35823;&#24046;&#35843;&#21046;&#24191;&#20041;&#25193;&#25955;&#27169;&#22411;&#65288;CoreDiff&#65289;&#65292;&#29992;&#20110;&#20302;&#21058;&#37327;CT&#65288;LDCT&#65289;&#30340;&#21435;&#22122;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;LDCT&#22270;&#20687;&#26469;&#28040;&#38500;&#38543;&#26426;&#39640;&#26031;&#22122;&#22768;&#24182;&#27169;&#25311;CT&#36864;&#21270;&#30340;&#29289;&#29702;&#36807;&#31243;&#65292;&#20943;&#23569;&#37319;&#26679;&#27493;&#39588;&#65292;&#24182;&#24341;&#20837;&#19978;&#19979;&#25991;&#35823;&#24046;&#35843;&#21046;&#20197;&#22686;&#24378;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.01814</link><description>&lt;p&gt;
CoreDiff: &#19978;&#19979;&#25991;&#35823;&#24046;&#35843;&#21046;&#24191;&#20041;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#20302;&#21058;&#37327;CT&#21435;&#22122;&#21644;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
CoreDiff: Contextual Error-Modulated Generalized Diffusion Model for Low-Dose CT Denoising and Generalization. (arXiv:2304.01814v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01814
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19978;&#19979;&#25991;&#35823;&#24046;&#35843;&#21046;&#24191;&#20041;&#25193;&#25955;&#27169;&#22411;&#65288;CoreDiff&#65289;&#65292;&#29992;&#20110;&#20302;&#21058;&#37327;CT&#65288;LDCT&#65289;&#30340;&#21435;&#22122;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;LDCT&#22270;&#20687;&#26469;&#28040;&#38500;&#38543;&#26426;&#39640;&#26031;&#22122;&#22768;&#24182;&#27169;&#25311;CT&#36864;&#21270;&#30340;&#29289;&#29702;&#36807;&#31243;&#65292;&#20943;&#23569;&#37319;&#26679;&#27493;&#39588;&#65292;&#24182;&#24341;&#20837;&#19978;&#19979;&#25991;&#35823;&#24046;&#35843;&#21046;&#20197;&#22686;&#24378;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#21058;&#37327;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#22270;&#20687;&#30001;&#20110;&#20809;&#23376;&#21294;&#20047;&#21644;&#30005;&#23376;&#22122;&#22768;&#32780;&#21463;&#21040;&#22122;&#22768;&#21644;&#20266;&#24433;&#30340;&#24433;&#21709;&#12290;&#26368;&#36817;&#65292;&#19968;&#20123;&#30740;&#31350;&#23581;&#35797;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#26469;&#35299;&#20915;&#20197;&#21069;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21435;&#22122;&#27169;&#22411;&#36935;&#21040;&#30340;&#36807;&#24230;&#24179;&#28369;&#21644;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#28041;&#21450;&#22823;&#37327;&#37319;&#26679;&#27493;&#39588;&#65292;&#25193;&#25955;&#27169;&#22411;&#30340;&#25512;&#29702;&#26102;&#38388;&#24456;&#38271;&#12290;&#26368;&#36817;&#65292;&#20919;&#25193;&#25955;&#27169;&#22411;&#25512;&#24191;&#20102;&#32463;&#20856;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#20855;&#26377;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#12290;&#21463;&#20919;&#25193;&#25955;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38024;&#23545;&#20302;&#21058;&#37327;CT&#65288;LDCT&#65289;&#21435;&#22122;&#30340;&#19978;&#19979;&#25991;&#35823;&#24046;&#35843;&#21046;&#24191;&#20041;&#25193;&#25955;&#27169;&#22411;&#65292;&#31216;&#20026;CoreDiff&#12290;&#39318;&#20808;&#65292;CoreDiff&#21033;&#29992;LDCT&#22270;&#20687;&#26469;&#28040;&#38500;&#38543;&#26426;&#39640;&#26031;&#22122;&#22768;&#65292;&#24182;&#37319;&#29992;&#26032;&#22411;&#22343;&#20540;&#20445;&#25345;&#36864;&#21270;&#31639;&#23376;&#26469;&#27169;&#25311;CT&#36864;&#21270;&#30340;&#29289;&#29702;&#36807;&#31243;&#65292;&#30001;&#20110;&#21551;&#21160;&#37319;&#26679;&#36807;&#31243;&#30340;&#20449;&#24687;&#20016;&#23500;&#30340;LDCT&#22270;&#20687;&#65292;&#22823;&#24133;&#20943;&#23569;&#37319;&#26679;&#27493;&#39588;&#12290;&#20854;&#27425;&#65292;&#20026;&#32531;&#35299;&#25193;&#25955;&#27169;&#22411;&#20013;&#36807;&#22810;&#37319;&#26679;&#27493;&#39588;&#30340;&#38382;&#39064;&#65292;&#24341;&#20837;&#19978;&#19979;&#25991;&#35823;&#24046;&#35843;&#21046;&#65292;&#20351;CoreDiff&#26356;&#20855;&#26377;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-dose computed tomography (CT) images suffer from noise and artifacts due to photon starvation and electronic noise. Recently, some works have attempted to use diffusion models to address the over-smoothness and training instability encountered by previous deep-learning-based denoising models. However, diffusion models suffer from long inference times due to the large number of sampling steps involved. Very recently, cold diffusion model generalizes classical diffusion models and has greater flexibility. Inspired by the cold diffusion, this paper presents a novel COntextual eRror-modulated gEneralized Diffusion model for low-dose CT (LDCT) denoising, termed CoreDiff. First, CoreDiff utilizes LDCT images to displace the random Gaussian noise and employs a novel mean-preserving degradation operator to mimic the physical process of CT degradation, significantly reducing sampling steps thanks to the informative LDCT images as the starting point of the sampling process. Second, to allevi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;FedFTN&#65292;&#19968;&#31181;&#20010;&#24615;&#21270;&#30340;&#32852;&#37030;&#23398;&#20064;&#31574;&#30053;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#26426;&#26500;&#20302;&#35745;&#25968;PET&#21435;&#22122;&#20013;&#30340;&#39046;&#22495;&#24046;&#24322;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#28145;&#24230;&#29305;&#24449;&#36716;&#25442;&#32593;&#32476;&#26469;&#25913;&#21892;&#20302;&#35745;&#25968;PET&#22270;&#20687;&#36136;&#37327;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#38598;&#20013;&#24335;&#25968;&#25454;&#38598;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.00570</link><description>&lt;p&gt;
FedFTN: &#22810;&#26426;&#26500;&#20302;&#35745;&#25968;PET&#21435;&#22122;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#19982;&#28145;&#24230;&#29305;&#24449;&#36716;&#25442;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
FedFTN: Personalized Federated Learning with Deep Feature Transformation Network for Multi-institutional Low-count PET Denoising. (arXiv:2304.00570v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00570
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;FedFTN&#65292;&#19968;&#31181;&#20010;&#24615;&#21270;&#30340;&#32852;&#37030;&#23398;&#20064;&#31574;&#30053;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#26426;&#26500;&#20302;&#35745;&#25968;PET&#21435;&#22122;&#20013;&#30340;&#39046;&#22495;&#24046;&#24322;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#28145;&#24230;&#29305;&#24449;&#36716;&#25442;&#32593;&#32476;&#26469;&#25913;&#21892;&#20302;&#35745;&#25968;PET&#22270;&#20687;&#36136;&#37327;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#38598;&#20013;&#24335;&#25968;&#25454;&#38598;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#35745;&#25968;PET&#26159;&#38477;&#20302;&#36752;&#23556;&#26292;&#38706;&#21644;&#37319;&#38598;&#26102;&#38388;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#20294;&#37325;&#24314;&#22270;&#20687;&#24448;&#24448;&#21463;&#21040;&#20449;&#22122;&#27604;&#20302;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#24433;&#21709;&#35786;&#26029;&#21644;&#20854;&#20182;&#19979;&#28216;&#20219;&#21153;&#12290;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#22312;&#25913;&#21892;&#20302;&#35745;&#25968;PET&#22270;&#20687;&#36136;&#37327;&#26041;&#38754;&#26174;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#30001;&#20110;&#24739;&#32773;&#25968;&#25454;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#38382;&#39064;&#65292;&#20174;&#22810;&#20010;&#26426;&#26500;&#33719;&#21462;&#22823;&#22411;&#12289;&#38598;&#20013;&#21644;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#20197;&#35757;&#32451;&#24378;&#22823;&#30340;&#27169;&#22411;&#26159;&#22256;&#38590;&#30340;&#12290;&#27492;&#22806;&#65292;&#19981;&#21516;&#26426;&#26500;&#30340;&#20302;&#35745;&#25968;PET&#25968;&#25454;&#21487;&#33021;&#20855;&#26377;&#19981;&#21516;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#22240;&#27492;&#38656;&#35201;&#20010;&#24615;&#21270;&#30340;&#27169;&#22411;&#12290;&#34429;&#28982;&#20043;&#21069;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#32858;&#21512;&#26412;&#22320;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#22810;&#26426;&#26500;&#30340;&#21327;&#20316;&#35757;&#32451;&#65292;&#20294;&#35299;&#20915;&#22810;&#26426;&#26500;&#20302;&#35745;&#25968;PET&#21435;&#22122;&#24212;&#29992;&#20013;&#30340;&#39046;&#22495;&#24046;&#24322;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#24182;&#19988;&#36824;&#26410;&#23436;&#20840;&#25506;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FedFTN&#65292;&#19968;&#31181;&#20010;&#24615;&#21270;&#30340;&#32852;&#37030;&#23398;&#20064;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-count PET is an efficient way to reduce radiation exposure and acquisition time, but the reconstructed images often suffer from low signal-to-noise ratio (SNR), thus affecting diagnosis and other downstream tasks. Recent advances in deep learning have shown great potential in improving low-count PET image quality, but acquiring a large, centralized, and diverse dataset from multiple institutions for training a robust model is difficult due to privacy and security concerns of patient data. Moreover, low-count PET data at different institutions may have different data distribution, thus requiring personalized models. While previous federated learning (FL) algorithms enable multi-institution collaborative training without the need of aggregating local data, addressing the large domain shift in the application of multi-institutional low-count PET denoising remains a challenge and is still highly under-explored. In this work, we propose FedFTN, a personalized federated learning strategy
&lt;/p&gt;</description></item><item><title>CoD-MTL&#26159;&#19968;&#20010;&#26032;&#30340;&#27515;&#22240;&#20998;&#26512;&#26694;&#26550;&#65292;&#37319;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#26469;&#20849;&#21516;&#27169;&#25311;&#19981;&#21516;CoD&#39044;&#27979;&#20219;&#21153;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#24182;&#19988;&#21033;&#29992;&#26641;&#33976;&#39311;&#31574;&#30053;&#36827;&#34892;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#22312;&#20855;&#22791;&#26641;&#27169;&#22411;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#20248;&#28857;&#20013;&#21457;&#25496;&#26368;&#22823;&#21270;&#30340;&#21033;&#30410;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#25552;&#20379;&#31934;&#30830;&#21487;&#38752;&#30340;CoD&#39044;&#27979;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#20020;&#24202;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2304.00012</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#29992;&#20110;&#31227;&#26893;&#21518;&#27515;&#22240;&#20998;&#26512;&#65306;&#20197;&#32925;&#31227;&#26893;&#20026;&#20363;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Multi-Task Learning for Post-transplant Cause of Death Analysis: A Case Study on Liver Transplant. (arXiv:2304.00012v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00012
&lt;/p&gt;
&lt;p&gt;
CoD-MTL&#26159;&#19968;&#20010;&#26032;&#30340;&#27515;&#22240;&#20998;&#26512;&#26694;&#26550;&#65292;&#37319;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#26469;&#20849;&#21516;&#27169;&#25311;&#19981;&#21516;CoD&#39044;&#27979;&#20219;&#21153;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#24182;&#19988;&#21033;&#29992;&#26641;&#33976;&#39311;&#31574;&#30053;&#36827;&#34892;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#22312;&#20855;&#22791;&#26641;&#27169;&#22411;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#20248;&#28857;&#20013;&#21457;&#25496;&#26368;&#22823;&#21270;&#30340;&#21033;&#30410;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#25552;&#20379;&#31934;&#30830;&#21487;&#38752;&#30340;CoD&#39044;&#27979;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#20020;&#24202;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22120;&#23448;&#31227;&#26893;&#26159;&#26576;&#20123;&#26411;&#26399;&#30142;&#30149;&#65288;&#22914;&#32925;&#34928;&#31469;&#65289;&#30340;&#22522;&#26412;&#27835;&#30103;&#26041;&#27861;&#12290;&#20998;&#26512;&#22120;&#23448;&#31227;&#26893;&#21518;&#30340;&#27515;&#22240;&#21487;&#20197;&#20026;&#20020;&#24202;&#20915;&#31574;&#25552;&#20379;&#26377;&#21147;&#30340;&#24037;&#20855;&#65292;&#21253;&#25324;&#20010;&#24615;&#21270;&#27835;&#30103;&#21644;&#22120;&#23448;&#20998;&#37197;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#26041;&#27861;&#22914;MELD&#35780;&#20998;&#21644;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#27515;&#22240;&#20998;&#26512;&#20013;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#30340;&#25968;&#25454;&#21644;&#27169;&#22411;&#30456;&#20851;&#25361;&#25112;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#27492;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38590;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;CoD-MTL&#65292;&#21033;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#26469;&#20849;&#21516;&#27169;&#25311;&#19981;&#21516;CoD&#39044;&#27979;&#20219;&#21153;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;&#20855;&#20307;&#26469;&#35762;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#26641;&#33976;&#39311;&#31574;&#30053;&#29992;&#20110;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#32467;&#21512;&#20102;&#26641;&#27169;&#22411;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#20248;&#28857;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#25552;&#20379;&#31934;&#30830;&#21487;&#38752;&#30340;CoD&#39044;&#27979;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#20197;&#23637;&#31034;&#35813;&#26694;&#26550;&#30340;&#20020;&#24202;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Organ transplant is the essential treatment method for some end-stage diseases, such as liver failure. Analyzing the post-transplant cause of death (CoD) after organ transplant provides a powerful tool for clinical decision making, including personalized treatment and organ allocation. However, traditional methods like Model for End-stage Liver Disease (MELD) score and conventional machine learning (ML) methods are limited in CoD analysis due to two major data and model-related challenges. To address this, we propose a novel framework called CoD-MTL leveraging multi-task learning to model the semantic relationships between various CoD prediction tasks jointly. Specifically, we develop a novel tree distillation strategy for multi-task learning, which combines the strength of both the tree model and multi-task learning. Experimental results are presented to show the precise and reliable CoD predictions of our framework. A case study is conducted to demonstrate the clinical importance of 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#38754;&#21521;RNN-T&#30340;&#20196;&#29260;&#32423;&#27874;&#26463;&#25628;&#32034;&#31639;&#27861;&#65292;&#36890;&#36807;&#25209;&#22788;&#29702;&#32852;&#21512;&#32593;&#32476;&#30340;&#35843;&#29992;&#65292;&#23454;&#29616;&#20102;20%&#33267;96%&#30340;&#35299;&#30721;&#21152;&#36895;&#65292;&#24182;&#19988;&#36890;&#36807;&#27719;&#24635;&#21457;&#23556;&#27010;&#29575;&#22312;&#25214;&#21040;&#26368;&#21487;&#33021;&#30340;&#27169;&#22411;&#36755;&#20986;&#26041;&#38754;&#21462;&#24471;&#20102;11%&#30340;&#26368;&#20248;&#38169;&#35823;&#29575;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2302.14357</link><description>&lt;p&gt;
&#19968;&#20010;&#38754;&#21521;RNN-T&#30340;&#20196;&#29260;&#32423;&#27874;&#26463;&#25628;&#32034;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Token-Wise Beam Search Algorithm for RNN-T. (arXiv:2302.14357v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14357
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#38754;&#21521;RNN-T&#30340;&#20196;&#29260;&#32423;&#27874;&#26463;&#25628;&#32034;&#31639;&#27861;&#65292;&#36890;&#36807;&#25209;&#22788;&#29702;&#32852;&#21512;&#32593;&#32476;&#30340;&#35843;&#29992;&#65292;&#23454;&#29616;&#20102;20%&#33267;96%&#30340;&#35299;&#30721;&#21152;&#36895;&#65292;&#24182;&#19988;&#36890;&#36807;&#27719;&#24635;&#21457;&#23556;&#27010;&#29575;&#22312;&#25214;&#21040;&#26368;&#21487;&#33021;&#30340;&#27169;&#22411;&#36755;&#20986;&#26041;&#38754;&#21462;&#24471;&#20102;11%&#30340;&#26368;&#20248;&#38169;&#35823;&#29575;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#30340;&#26631;&#20934;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#36716;&#24405;&#22120; (RNN-T) &#35299;&#30721;&#31639;&#27861;&#22312;&#26102;&#38388;&#36724;&#19978;&#36827;&#34892;&#36845;&#20195;&#65292;&#21363;&#22312;&#31227;&#21160;&#21040;&#19979;&#19968;&#20010;&#26102;&#38388;&#27493;&#20043;&#21069;&#35299;&#30721;&#19968;&#20010;&#26102;&#38388;&#27493;&#12290;&#36825;&#20123;&#31639;&#27861;&#23548;&#33268;&#32852;&#21512;&#32593;&#32476;&#30340;&#35843;&#29992;&#27425;&#25968;&#22823;&#22823;&#22686;&#21152;&#65292;&#36825;&#22312;&#20043;&#21069;&#30340;&#30740;&#31350;&#20013;&#24050;&#34987;&#35777;&#26126;&#26159;&#38477;&#20302;&#35299;&#30721;&#36895;&#24230;&#30340;&#37325;&#35201;&#22240;&#32032;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#30721;&#27874;&#26463;&#25628;&#32034;&#31639;&#27861;&#65292;&#23427;&#22312;&#19968;&#27573;&#26102;&#38388;&#27493;&#39588;&#20013;&#25209;&#22788;&#29702;&#32852;&#21512;&#32593;&#32476;&#30340;&#35843;&#29992;&#65292;&#36825;&#23548;&#33268;&#22312;&#25152;&#26377;&#23454;&#39564;&#27169;&#22411;&#21644;&#35774;&#32622;&#20013;&#19968;&#33268;&#22320;&#33719;&#24471;20%&#33267;96%&#30340;&#35299;&#30721;&#21152;&#36895;&#12290;&#27492;&#22806;&#65292;&#22312;&#19968;&#27573;&#26102;&#38388;&#20869;&#27719;&#24635;&#21457;&#23556;&#27010;&#29575;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#25214;&#21040;&#26368;&#21487;&#33021;&#30340;&#27169;&#22411;&#36755;&#20986;&#30340;&#26356;&#22909;&#36817;&#20284;&#65292;&#20351;&#24471;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#27573;&#22823;&#23567;&#22686;&#21152;&#26102;&#30456;&#23545;&#20110;&#26368;&#20248;&#38169;&#35823;&#29575;&#25552;&#39640;&#20102;&#26368;&#22810;11%&#65292;&#24182;&#31245;&#24494;&#25913;&#21892;&#20102;&#36890;&#29992;&#38169;&#35823;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Standard Recurrent Neural Network Transducers (RNN-T) decoding algorithms for speech recognition are iterating over the time axis, such that one time step is decoded before moving on to the next time step. Those algorithms result in a large number of calls to the joint network, which were shown in previous work to be an important factor that reduces decoding speed. We present a decoding beam search algorithm that batches the joint network calls across a segment of time steps, which results in 20%-96% decoding speedups consistently across all models and settings experimented with. In addition, aggregating emission probabilities over a segment may be seen as a better approximation to finding the most likely model output, causing our algorithm to improve oracle word error rate by up to 11% relative as the segment size increases, and to slightly improve general word error rate.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#24494;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#26469;&#23454;&#29616;&#40065;&#26834;&#30340;&#28145;&#24230;&#22810;&#27169;&#24577;&#20998;&#26512;&#65292;&#36890;&#36807;&#20511;&#21161;&#26126;&#30830;&#30340;&#30693;&#35782;&#22270;&#35889;&#21644;&#20132;&#20114;&#24335;&#30340;&#21306;&#20998;&#22806;&#37096;&#39046;&#22495;&#23618;&#26469;&#36807;&#28388;&#22122;&#22768;&#12290;&#22312;&#22810;&#20010;&#35270;&#35273;&#21644;&#35821;&#35328;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#33391;&#22909;&#30340;&#24212;&#29992;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.05608</link><description>&lt;p&gt;
&#21487;&#24494;&#24322;&#24120;&#26816;&#27979;&#23454;&#29616;&#40065;&#26834;&#30340;&#28145;&#24230;&#22810;&#27169;&#24577;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Differentiable Outlier Detection Enable Robust Deep Multimodal Analysis. (arXiv:2302.05608v1 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05608
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#24494;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#26469;&#23454;&#29616;&#40065;&#26834;&#30340;&#28145;&#24230;&#22810;&#27169;&#24577;&#20998;&#26512;&#65292;&#36890;&#36807;&#20511;&#21161;&#26126;&#30830;&#30340;&#30693;&#35782;&#22270;&#35889;&#21644;&#20132;&#20114;&#24335;&#30340;&#21306;&#20998;&#22806;&#37096;&#39046;&#22495;&#23618;&#26469;&#36807;&#28388;&#22122;&#22768;&#12290;&#22312;&#22810;&#20010;&#35270;&#35273;&#21644;&#35821;&#35328;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#33391;&#22909;&#30340;&#24212;&#29992;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#28145;&#24230;&#32593;&#32476;&#27169;&#22411;&#36890;&#24120;&#21482;&#26159;&#24402;&#32435;&#24335;&#30340;&#20351;&#29992;&#12290;&#22240;&#27492;&#65292;&#24403;&#36825;&#20123;&#27169;&#22411;&#29992;&#20110;&#39044;&#27979;&#26102;&#65292;&#24448;&#24448;&#26080;&#27861;&#25429;&#25417;&#21040;&#23545;&#35937;&#65288;&#25110;&#27010;&#24565;&#65289;&#20043;&#38388;&#22312;&#32676;&#20307;&#23618;&#38754;&#19978;&#23384;&#22312;&#30340;&#35821;&#20041;&#20449;&#24687;&#21644;&#38544;&#21547;&#20381;&#36182;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#22312;&#22823;&#35268;&#27169;&#21644;&#22024;&#26434;&#30340;&#29615;&#22659;&#20013;&#22914;&#20309;&#20197;&#21453;&#21521;&#20256;&#25773;&#21451;&#22909;&#30340;&#26041;&#24335;&#25351;&#23450;&#39046;&#22495;&#25110;&#20808;&#39564;&#27169;&#24577;&#30693;&#35782;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#25324;&#26126;&#30830;&#30340;&#30693;&#35782;&#22270;&#35889;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#20351;&#29992;&#38544;&#24335;&#32593;&#32476;&#25805;&#20316;&#31526;&#30340;&#20132;&#20114;&#24335;&#21306;&#20998;&#22806;&#37096;&#39046;&#22495;&#30340;&#23618;&#12290;&#35813;&#23618;&#29992;&#20110;&#36807;&#28388;&#30001;&#22806;&#37096;&#30693;&#35782;&#24211;&#24102;&#26469;&#30340;&#22122;&#22768;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#25105;&#20204;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#24212;&#29992;&#25105;&#20204;&#30340;&#27169;&#22411;&#36827;&#34892;&#22810;&#20010;&#35270;&#35273;&#21644;&#35821;&#35328;&#19979;&#28216;&#20219;&#21153;&#65292;&#21253;&#25324;&#35270;&#35273;&#38382;&#31572;&#12289;&#35270;&#35273;&#25512;&#29702;&#21644;&#22270;&#20687;&#25991;&#26412;&#26816;&#32034;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22823;&#35268;&#27169;&#21644;&#22024;&#26434;&#30340;&#29615;&#22659;&#20013;&#65292;&#21487;&#20197;&#21435;&#38500;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;
Often, deep network models are purely inductive during training and while performing inference on unseen data. Thus, when such models are used for predictions, it is well known that they often fail to capture the semantic information and implicit dependencies that exist among objects (or concepts) on a population level. Moreover, it is still unclear how domain or prior modal knowledge can be specified in a backpropagation friendly manner, especially in large-scale and noisy settings. In this work, we propose an end-to-end vision and language model incorporating explicit knowledge graphs. We also introduce an interactive out-of-distribution (OOD) layer using implicit network operator. The layer is used to filter noise that is brought by external knowledge base. In practice, we apply our model on several vision and language downstream tasks including visual question answering, visual reasoning, and image-text retrieval on different datasets. Our experiments show that it is possible to de
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37051;&#23621;&#21516;&#36136;&#24615;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476; (NHGCN) &#27169;&#22411;&#65292;&#21033;&#29992;&#26032;&#25351;&#26631; Neighborhood Homophily (NH) &#27979;&#37327;&#33410;&#28857;&#37051;&#22495;&#20013;&#30340;&#26631;&#31614;&#22797;&#26434;&#24230;&#25110;&#32431;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126; NHGCN &#27169;&#22411;&#22312;&#33410;&#28857;&#20998;&#31867;&#21644;&#22270;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#24182;&#26174;&#33879;&#36229;&#36807;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2301.09851</link><description>&lt;p&gt;
&#22522;&#20110;&#37051;&#23621;&#21516;&#36136;&#24615;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Neighborhood Homophily-based Graph Convolutional Network. (arXiv:2301.09851v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09851
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37051;&#23621;&#21516;&#36136;&#24615;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476; (NHGCN) &#27169;&#22411;&#65292;&#21033;&#29992;&#26032;&#25351;&#26631; Neighborhood Homophily (NH) &#27979;&#37327;&#33410;&#28857;&#37051;&#22495;&#20013;&#30340;&#26631;&#31614;&#22797;&#26434;&#24230;&#25110;&#32431;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126; NHGCN &#27169;&#22411;&#22312;&#33410;&#28857;&#20998;&#31867;&#21644;&#22270;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#24182;&#26174;&#33879;&#36229;&#36807;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#24418;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#30340;&#22270;&#24418;&#26159;&#24322;&#26500;&#30340;&#65292;&#36825;&#25361;&#25112;&#20102;&#32463;&#20856;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21516;&#36136;&#24615;&#20551;&#35774;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#26222;&#36866;&#24615;&#38382;&#39064;&#65292;&#35768;&#22810;&#30740;&#31350;&#21152;&#28145;&#32593;&#32476;&#25110;&#36830;&#25509;&#20013;&#38388;&#34920;&#31034;&#65292;&#20294;&#36825;&#24182;&#19981;&#20250;&#26412;&#36136;&#19978;&#25913;&#21464;&#37051;&#23621;&#32858;&#21512;&#24182;&#24341;&#20837;&#22122;&#22768;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#26032;&#30340;&#25351;&#26631;&#26469;&#34920;&#24449;&#21516;&#36136;&#24615;&#65292;&#20294;&#24456;&#23569;&#32771;&#34385;&#25152;&#25552;&#20986;&#25351;&#26631;&#21644;&#27169;&#22411;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#26412;&#25991;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#25351;&#26631; Neighborhood Homophily (NH)&#65292;&#29992;&#20110;&#27979;&#37327;&#33410;&#28857;&#37051;&#22495;&#20013;&#30340;&#26631;&#31614;&#22797;&#26434;&#24230;&#25110;&#32431;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#35813;&#25351;&#26631;&#34701;&#20837;&#21040;&#32463;&#20856;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476; (GCN) &#32467;&#26500;&#20013;&#65292;&#25552;&#20986;&#20102; NHGCN &#27169;&#22411;&#12290;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;&#37051;&#23621;&#34987;&#39044;&#20272;&#30340; NH &#20540;&#20998;&#32452;&#65292;&#20174;&#19981;&#21516;&#36890;&#36947;&#36827;&#34892;&#32858;&#21512;&#12290;&#22810;&#39033;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;NHGCN &#22312;&#33410;&#28857;&#20998;&#31867;&#21644;&#22270;&#20998;&#31867;&#20219;&#21153;&#20013;&#22987;&#32456;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have been proved powerful in graph-oriented tasks. However, many real-world graphs are heterophilous, challenging the homophily assumption of classical GNNs. To solve the universality problem, many studies deepen networks or concatenate intermediate representations, which does not inherently change neighbor aggregation and introduces noise. Recent studies propose new metrics to characterize the homophily, but rarely consider the correlation of the proposed metrics and models. In this paper, we first design a new metric, Neighborhood Homophily (\textit{NH}), to measure the label complexity or purity in node neighborhoods. Furthermore, we incorporate the metric into the classical graph convolutional network (GCN) architecture and propose \textbf{N}eighborhood \textbf{H}omophily-based \textbf{G}raph \textbf{C}onvolutional \textbf{N}etwork (\textbf{NHGCN}). In this framework, neighbors are grouped by estimated \textit{NH} values and aggregated from different ch
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#30340;&#26041;&#27861;&#26469;&#21442;&#25968;&#21270;&#21487;&#23450;&#21521;&#21367;&#31215;&#26680;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#31616;&#21333;&#28789;&#27963;&#30340;&#26500;&#24314;&#21487;&#23450;&#21521;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#25512;&#24191;&#21040;&#20219;&#20309;&#20855;&#26377;&#31561;&#21464;MLP&#30340;&#32676;G&#12290;</title><link>http://arxiv.org/abs/2212.06096</link><description>&lt;p&gt;
&#38544;&#24335;&#21367;&#31215;&#26680;&#29992;&#20110;&#21487;&#23450;&#21521;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Implicit Convolutional Kernels for Steerable CNNs. (arXiv:2212.06096v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06096
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#30340;&#26041;&#27861;&#26469;&#21442;&#25968;&#21270;&#21487;&#23450;&#21521;&#21367;&#31215;&#26680;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#31616;&#21333;&#28789;&#27963;&#30340;&#26500;&#24314;&#21487;&#23450;&#21521;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#25512;&#24191;&#21040;&#20219;&#20309;&#20855;&#26377;&#31561;&#21464;MLP&#30340;&#32676;G&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#23450;&#21521;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#20102;&#26500;&#24314;&#19982;&#24179;&#31227;&#21644;&#20854;&#20182;&#21464;&#25442;&#31561;&#21516;&#21464;&#25442;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#36825;&#20123;&#21464;&#25442;&#23646;&#20110;&#22522;&#20110;&#21407;&#28857;&#20445;&#25345;&#30340;&#32676;G&#65292;&#20363;&#22914;&#21453;&#23556;&#21644;&#26059;&#36716;&#12290;&#23427;&#20204;&#20381;&#36182;&#20110;&#36890;&#36807;&#22312;&#26680;&#31354;&#38388;&#19978;&#24378;&#21152;&#29305;&#23450;&#20110;&#32676;G&#30340;&#31561;&#21464;&#24615;&#32422;&#26463;&#26469;&#35299;&#26512;&#27714;&#35299;&#24471;&#21040;&#30340;G-&#23450;&#21521;&#21367;&#31215;&#26680;&#30340;&#26631;&#20934;&#21367;&#31215;&#12290;&#30001;&#20110;&#35299;&#20915;&#26041;&#26696;&#23545;&#29305;&#23450;&#30340;&#32676;G&#23450;&#21046;&#65292;&#26680;&#22522;&#30784;&#30340;&#23454;&#29616;&#19981;&#33021;&#25512;&#24191;&#21040;&#20854;&#20182;&#23545;&#31216;&#21464;&#25442;&#65292;&#36825;&#23548;&#33268;&#20102;&#36890;&#29992;&#32676;&#31561;&#21464;&#27169;&#22411;&#30340;&#24320;&#21457;&#22797;&#26434;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#36890;&#36807;&#22810;&#23618;&#24863;&#30693;&#22120;(MLPs)&#21442;&#25968;&#21270;G-&#23450;&#21521;&#21367;&#31215;&#26680;&#30340;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#12290;&#25152;&#24471;&#21040;&#30340;&#26694;&#26550;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#28789;&#27963;&#30340;&#23454;&#29616;&#21487;&#23450;&#21521;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#23545;&#20110;&#20219;&#20309;&#21487;&#20197;&#26500;&#24314;G-&#31561;&#21464;MLP&#30340;&#32676;G&#37117;&#21487;&#20197;&#25512;&#24191;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;N&#20307;&#27169;&#25311;&#12290;
&lt;/p&gt;
&lt;p&gt;
Steerable convolutional neural networks (CNNs) provide a general framework for building neural networks equivariant to translations and other transformations belonging to an origin-preserving group $G$, such as reflections and rotations. They rely on standard convolutions with $G$-steerable kernels obtained by analytically solving the group-specific equivariance constraint imposed onto the kernel space. As the solution is tailored to a particular group $G$, the implementation of a kernel basis does not generalize to other symmetry transformations, which complicates the development of general group equivariant models. We propose using implicit neural representation via multi-layer perceptrons (MLPs) to parameterize $G$-steerable kernels. The resulting framework offers a simple and flexible way to implement Steerable CNNs and generalizes to any group $G$ for which a $G$-equivariant MLP can be built. We prove the effectiveness of our method on multiple tasks, including N-body simulations,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#22810;&#39046;&#22495;&#38271;&#23614;&#23398;&#20064;&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;TALLY&#65292;&#36890;&#36807;&#28151;&#21512;&#31034;&#20363;&#30340;&#35821;&#20041;&#34920;&#31034;&#21644;&#22495;&#30456;&#20851;&#24178;&#25200;&#65292;&#20351;&#29992;&#36873;&#25321;&#24615;&#22343;&#34913;&#37319;&#26679;&#31574;&#30053;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#65292;&#21516;&#26102;&#21033;&#29992;&#22495;&#19981;&#21464;&#30340;&#31867;&#21407;&#22411;&#25913;&#21892;&#35821;&#20041;&#34920;&#31034;&#30340;&#35299;&#32544;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#39564;&#35777;&#20102;TALLY&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.14358</link><description>&lt;p&gt;
&#22810;&#39046;&#22495;&#38271;&#23614;&#23398;&#20064;&#36890;&#36807;&#22686;&#24378;&#35299;&#32544;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Multi-Domain Long-Tailed Learning by Augmenting Disentangled Representations. (arXiv:2210.14358v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14358
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#22810;&#39046;&#22495;&#38271;&#23614;&#23398;&#20064;&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;TALLY&#65292;&#36890;&#36807;&#28151;&#21512;&#31034;&#20363;&#30340;&#35821;&#20041;&#34920;&#31034;&#21644;&#22495;&#30456;&#20851;&#24178;&#25200;&#65292;&#20351;&#29992;&#36873;&#25321;&#24615;&#22343;&#34913;&#37319;&#26679;&#31574;&#30053;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#65292;&#21516;&#26102;&#21033;&#29992;&#22495;&#19981;&#21464;&#30340;&#31867;&#21407;&#22411;&#25913;&#21892;&#35821;&#20041;&#34920;&#31034;&#30340;&#35299;&#32544;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#39564;&#35777;&#20102;TALLY&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29616;&#23454;&#20998;&#31867;&#38382;&#39064;&#20013;&#37117;&#23384;&#22312;&#19981;&#21487;&#36991;&#20813;&#30340;&#38271;&#23614;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#30446;&#21069;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#21482;&#32771;&#34385;&#25152;&#26377;&#31034;&#20363;&#26469;&#33258;&#21516;&#19968;&#20998;&#24067;&#30340;&#24773;&#20917;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#23384;&#22312;&#22810;&#20010;&#39046;&#22495;&#20855;&#26377;&#19981;&#21516;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#24773;&#20917;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20010;&#22810;&#39046;&#22495;&#38271;&#23614;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#26088;&#22312;&#20135;&#29983;&#19968;&#20010;&#33021;&#22815;&#22312;&#25152;&#26377;&#31867;&#21035;&#21644;&#39046;&#22495;&#20013;&#37117;&#20855;&#26377;&#33391;&#22909;&#27867;&#21270;&#24615;&#33021;&#30340;&#27169;&#22411;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;TALLY&#65292;&#19968;&#31181;&#35299;&#20915;&#22810;&#39046;&#22495;&#38271;&#23614;&#23398;&#20064;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#22522;&#20110;&#25552;&#20986;&#30340;&#36873;&#25321;&#24615;&#22343;&#34913;&#37319;&#26679;&#31574;&#30053;&#65292;TALLY&#36890;&#36807;&#23558;&#19968;&#20010;&#31034;&#20363;&#30340;&#35821;&#20041;&#34920;&#31034;&#19982;&#21478;&#19968;&#20010;&#31034;&#20363;&#30340;&#22495;&#30456;&#20851;&#24178;&#25200;&#28151;&#21512;&#65292;&#20135;&#29983;&#19968;&#20010;&#29992;&#20316;&#25968;&#25454;&#22686;&#24378;&#30340;&#26032;&#34920;&#31034;&#12290;&#20026;&#20102;&#25913;&#21892;&#35821;&#20041;&#34920;&#31034;&#30340;&#35299;&#32544;&#65292;TALLY&#36827;&#19968;&#27493;&#21033;&#29992;&#19968;&#20010;&#22495;&#19981;&#21464;&#30340;&#31867;&#21407;&#22411;&#26469;&#24179;&#22343;&#25481;&#22495;&#29305;&#23450;&#25928;&#24212;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#35780;&#20272;&#20102;TALLY&#12290;
&lt;/p&gt;
&lt;p&gt;
There is an inescapable long-tailed class-imbalance issue in many real-world classification problems. Current methods for addressing this problem only consider scenarios where all examples come from the same distribution. However, in many cases, there are multiple domains with distinct class imbalance. We study this multi-domain long-tailed learning problem and aim to produce a model that generalizes well across all classes and domains. Towards that goal, we introduce TALLY, a method that addresses this multi-domain long-tailed learning problem. Built upon a proposed selective balanced sampling strategy, TALLY achieves this by mixing the semantic representation of one example with the domain-associated nuisances of another, producing a new representation for use as data augmentation. To improve the disentanglement of semantic representations, TALLY further utilizes a domain-invariant class prototype that averages out domain-specific effects. We evaluate TALLY on several benchmarks and 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20581;&#22766;&#30340;&#19968;&#27425;&#24615;&#27468;&#22768;&#36716;&#25442;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#29305;&#23450;&#30340;&#26465;&#20214;&#26469;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#20219;&#24847;&#21040;&#20219;&#24847;&#30340;&#27468;&#22768;&#36716;&#25442;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#36890;&#36807;&#37096;&#20998;&#22495;&#26465;&#20214;&#21644;&#38899;&#39640;&#20998;&#24067;&#21305;&#37197;&#31561;&#26041;&#27861;&#65292;&#33021;&#22815;&#27867;&#21270;&#21040;&#26410;&#30693;&#27468;&#25163;&#65292;&#24182;&#19988;&#22312;&#38754;&#23545;&#24102;&#26377;&#28151;&#21709;&#21644;&#20276;&#22863;&#38899;&#20048;&#30340;&#27468;&#22768;&#26102;&#20173;&#28982;&#33021;&#22815;&#20445;&#25345;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.11096</link><description>&lt;p&gt;
&#20581;&#22766;&#30340;&#19968;&#27425;&#24615;&#27468;&#22768;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
Robust One-Shot Singing Voice Conversion. (arXiv:2210.11096v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.11096
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20581;&#22766;&#30340;&#19968;&#27425;&#24615;&#27468;&#22768;&#36716;&#25442;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#29305;&#23450;&#30340;&#26465;&#20214;&#26469;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#20219;&#24847;&#21040;&#20219;&#24847;&#30340;&#27468;&#22768;&#36716;&#25442;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#36890;&#36807;&#37096;&#20998;&#22495;&#26465;&#20214;&#21644;&#38899;&#39640;&#20998;&#24067;&#21305;&#37197;&#31561;&#26041;&#27861;&#65292;&#33021;&#22815;&#27867;&#21270;&#21040;&#26410;&#30693;&#27468;&#25163;&#65292;&#24182;&#19988;&#22312;&#38754;&#23545;&#24102;&#26377;&#28151;&#21709;&#21644;&#20276;&#22863;&#38899;&#20048;&#30340;&#27468;&#22768;&#26102;&#20173;&#28982;&#33021;&#22815;&#20445;&#25345;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#26041;&#38754;&#21462;&#24471;&#30340;&#36827;&#23637;&#24050;&#32463;&#25552;&#39640;&#20102;&#35821;&#38899;&#39046;&#22495;&#20013;&#22768;&#38899;&#36716;&#25442;&#30340;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#26410;&#30693;&#27468;&#25163;&#30340;&#39640;&#36136;&#37327;&#27468;&#22768;&#36716;&#25442;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#21407;&#22240;&#26159;&#22312;&#38899;&#39640;&#12289;&#38899;&#37327;&#21644;&#21457;&#38899;&#26041;&#38754;&#23384;&#22312;&#26356;&#22810;&#21508;&#31181;&#21508;&#26679;&#30340;&#38899;&#20048;&#34920;&#36798;&#12290;&#27492;&#22806;&#65292;&#27468;&#22768;&#24448;&#24448;&#26159;&#24102;&#26377;&#28151;&#21709;&#21644;&#20276;&#22863;&#38899;&#20048;&#30340;&#24405;&#38899;&#65292;&#36825;&#20351;&#24471;&#27468;&#22768;&#36716;&#25442;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20581;&#22766;&#30340;&#19968;&#27425;&#24615;&#27468;&#22768;&#36716;&#25442; (ROSVC)&#65292;&#21363;&#20351;&#22312;&#36825;&#31181;&#22833;&#30495;&#30340;&#27468;&#22768;&#19978;&#20063;&#33021;&#31283;&#23450;&#22320;&#36827;&#34892;&#20219;&#24847;&#21040;&#20219;&#24847;&#30340;&#27468;&#22768;&#36716;&#25442;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#19968;&#27425;&#24615;&#27468;&#22768;&#36716;&#25442;&#27169;&#22411;&#65292;&#36890;&#36807;&#37096;&#20998;&#22495;&#26465;&#20214;&#21644;&#23398;&#20064;&#31934;&#30830;&#24674;&#22797;&#30446;&#26631;&#38899;&#39640;&#30340;&#38899;&#39640;&#20998;&#24067;&#21305;&#37197;&#21644;AdaIN-skip&#26465;&#20214;&#65292;&#33021;&#22815;&#27867;&#21270;&#21040;&#26410;&#30693;&#27468;&#25163;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;Robustify&#30340;&#20004;&#38454;&#27573;&#35757;&#32451;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#31532;&#19968;&#38454;&#27573;&#20351;&#29992;&#28165;&#27905;&#25968;&#25454;&#23545;&#19968;&#27425;&#24615;&#27468;&#22768;&#36716;&#25442;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#30830;&#20445;&#39640;&#36136;&#37327;&#30340;&#36716;&#25442;&#65292;&#24182;&#24341;&#20837;&#22686;&#24378;&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
Recent progress in deep generative models has improved the quality of voice conversion in the speech domain. However, high-quality singing voice conversion (SVC) of unseen singers remains challenging due to the wider variety of musical expressions in pitch, loudness, and pronunciation. Moreover, singing voices are often recorded with reverb and accompaniment music, which make SVC even more challenging. In this work, we present a robust one-shot SVC (ROSVC) that performs any-to-any SVC robustly even on such distorted singing voices. To this end, we first propose a one-shot SVC model based on generative adversarial networks that generalizes to unseen singers via partial domain conditioning and learns to accurately recover the target pitch via pitch distribution matching and AdaIN-skip conditioning. We then propose a two-stage training method called Robustify that train the one-shot SVC model in the first stage on clean data to ensure high-quality conversion, and introduces enhancement mo
&lt;/p&gt;</description></item><item><title>AMPNet&#26159;&#19968;&#31181;&#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#28040;&#24687;&#20256;&#36882;&#23618;&#65292;&#33021;&#22815;&#23545;&#33410;&#28857;&#36827;&#34892;&#36880;&#20010;&#29305;&#24449;&#32534;&#30721;&#65292;&#24182;&#36890;&#36807;&#36328;&#33410;&#28857;&#27880;&#24847;&#21147;&#27169;&#22411;&#29305;&#24449;&#32423;&#21035;&#30340;&#20132;&#20114;&#12290;&#22312;&#23454;&#38469;&#29983;&#29289;&#31995;&#32479;&#19978;&#36827;&#34892;&#24191;&#27867;&#22522;&#20934;&#27979;&#35797;&#34920;&#26126;&#65292;AMPNet&#22312;fMRI&#20449;&#21495;&#37325;&#24314;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#22522;&#20934;&#65292;&#24182;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#39564;&#35777;&#20102;&#20854;&#21457;&#29616;&#26377;&#24847;&#20041;&#30340;&#29305;&#24449;&#32423;&#21035;&#20132;&#20114;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2210.09475</link><description>&lt;p&gt;
AMPNet: &#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#28040;&#24687;&#20256;&#36882;&#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
AMPNet: Attention as Message Passing for Graph Neural Networks. (arXiv:2210.09475v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09475
&lt;/p&gt;
&lt;p&gt;
AMPNet&#26159;&#19968;&#31181;&#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#28040;&#24687;&#20256;&#36882;&#23618;&#65292;&#33021;&#22815;&#23545;&#33410;&#28857;&#36827;&#34892;&#36880;&#20010;&#29305;&#24449;&#32534;&#30721;&#65292;&#24182;&#36890;&#36807;&#36328;&#33410;&#28857;&#27880;&#24847;&#21147;&#27169;&#22411;&#29305;&#24449;&#32423;&#21035;&#30340;&#20132;&#20114;&#12290;&#22312;&#23454;&#38469;&#29983;&#29289;&#31995;&#32479;&#19978;&#36827;&#34892;&#24191;&#27867;&#22522;&#20934;&#27979;&#35797;&#34920;&#26126;&#65292;AMPNet&#22312;fMRI&#20449;&#21495;&#37325;&#24314;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#22522;&#20934;&#65292;&#24182;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#39564;&#35777;&#20102;&#20854;&#21457;&#29616;&#26377;&#24847;&#20041;&#30340;&#29305;&#24449;&#32423;&#21035;&#20132;&#20114;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24050;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#29992;&#20110;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#12290;&#20256;&#32479;GNNs&#30340;&#19968;&#20010;&#20851;&#38190;&#38480;&#21046;&#26159;&#23558;&#27599;&#20010;&#33410;&#28857;&#34920;&#31034;&#20026;&#19968;&#20010;&#21333;&#19968;&#30340;&#29305;&#24449;&#21521;&#37327;&#65292;&#21487;&#33021;&#24573;&#35270;&#20102;&#20851;&#20110;&#20010;&#20307;&#33410;&#28857;&#29305;&#24449;&#30340;&#22797;&#26434;&#32454;&#33410;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#28040;&#24687;&#20256;&#36882;&#23618;AMPNet&#65292;&#29992;&#20110;GNNs&#65292;&#23427;&#23545;&#33410;&#28857;&#36827;&#34892;&#36880;&#20010;&#29305;&#24449;&#32534;&#30721;&#65292;&#24182;&#22312;&#28040;&#24687;&#20256;&#36882;&#27493;&#39588;&#20013;&#36890;&#36807;&#36328;&#33410;&#28857;&#27880;&#24847;&#21147;&#27169;&#22411;&#29305;&#24449;&#32423;&#21035;&#30340;&#20132;&#20114;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#30495;&#23454;&#29983;&#29289;&#31995;&#32479;&#65288;&#22914;fMRI&#33041;&#27963;&#21160;&#35760;&#24405;&#21644;&#31354;&#38388;&#22522;&#22240;&#32452;&#25968;&#25454;&#65289;&#19978;&#36827;&#34892;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#35777;&#26126;&#20102;AMPNet&#30340;&#33021;&#21147;&#65292;&#23427;&#22312;fMRI&#20449;&#21495;&#37325;&#24314;&#26041;&#38754;&#30456;&#27604;&#29616;&#26377;&#22522;&#20934;&#25552;&#39640;&#20102;20&#65285;&#65292;&#22312;&#28155;&#21152;&#20301;&#32622;&#23884;&#20837;&#21518;&#21448;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;8&#65285;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#29983;&#29289;&#31995;&#32479;&#30340;&#26696;&#20363;&#30740;&#31350;&#39564;&#35777;&#20102;AMPNet&#21457;&#29616;&#26377;&#24847;&#20041;&#30340;&#29305;&#24449;&#32423;&#21035;&#20132;&#20114;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#39044;&#35745;&#25105;&#20204;&#30340;&#26550;&#26500;&#23558;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#30740;&#31350;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have emerged as a powerful representation learning framework for graph-structured data. A key limitation of conventional GNNs is their representation of each node with a singular feature vector, potentially overlooking intricate details about individual node features. Here, we propose an Attention-based Message-Passing layer for GNNs (AMPNet) that encodes individual features per node and models feature-level interactions through cross-node attention during message-passing steps. We demonstrate the abilities of AMPNet through extensive benchmarking on real-world biological systems such as fMRI brain activity recordings and spatial genomic data, improving over existing baselines by 20% on fMRI signal reconstruction, and further improving another 8% with positional embedding added. Finally, we validate the ability of AMPNet to uncover meaningful feature-level interactions through case studies on biological systems. We anticipate that our architecture will be h
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20010;&#24615;&#21270;&#32852;&#21512;&#23398;&#20064;&#30340;Blinder&#21311;&#21517;&#21270;&#27169;&#22411;&#65292;&#33021;&#22312;&#24322;&#26500;&#29615;&#22659;&#20013;&#25552;&#20379;&#31471;&#21040;&#31471;&#30340;&#38544;&#31169;&#20445;&#25252;&#12290;&#19982;&#22312;&#38598;&#20013;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#30456;&#27604;&#65292;Blinder&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#20165;&#22686;&#21152;&#26368;&#22810;4.00%&#30340;&#38544;&#31169;&#25439;&#22833;&#65292;&#24182;&#38477;&#20302;&#26368;&#22810;4.24%&#30340;&#25968;&#25454;&#25928;&#29992;&#12290;&#21516;&#26102;&#65292;Blinder&#36824;&#23637;&#31034;&#20102;&#21311;&#21517;&#21270;&#23556;&#39057;&#24863;&#30693;&#27169;&#24577;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2209.12046</link><description>&lt;p&gt;
Blinder: &#36890;&#36807;&#20010;&#24615;&#21270;&#32852;&#21512;&#23398;&#20064;&#23454;&#29616;&#24863;&#30693;&#31995;&#32479;&#30340;&#31471;&#21040;&#31471;&#38544;&#31169;&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
Blinder: End-to-end Privacy Protection in Sensing Systems via Personalized Federated Learning. (arXiv:2209.12046v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.12046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20010;&#24615;&#21270;&#32852;&#21512;&#23398;&#20064;&#30340;Blinder&#21311;&#21517;&#21270;&#27169;&#22411;&#65292;&#33021;&#22312;&#24322;&#26500;&#29615;&#22659;&#20013;&#25552;&#20379;&#31471;&#21040;&#31471;&#30340;&#38544;&#31169;&#20445;&#25252;&#12290;&#19982;&#22312;&#38598;&#20013;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#30456;&#27604;&#65292;Blinder&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#20165;&#22686;&#21152;&#26368;&#22810;4.00%&#30340;&#38544;&#31169;&#25439;&#22833;&#65292;&#24182;&#38477;&#20302;&#26368;&#22810;4.24%&#30340;&#25968;&#25454;&#25928;&#29992;&#12290;&#21516;&#26102;&#65292;Blinder&#36824;&#23637;&#31034;&#20102;&#21311;&#21517;&#21270;&#23556;&#39057;&#24863;&#30693;&#27169;&#24577;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#25955;&#25968;&#25454;&#35757;&#32451;&#30340;&#20256;&#24863;&#22120;&#25968;&#25454;&#21311;&#21517;&#21270;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#20256;&#24863;&#22120;&#25968;&#25454;&#20855;&#26377;&#19981;&#21516;&#22522;&#30784;&#20998;&#24067;&#30340;&#24322;&#26500;&#29615;&#22659;&#20013;&#65292;&#22312;&#25968;&#25454;&#25928;&#29992;&#21644;&#38544;&#31169;&#20043;&#38388;&#21462;&#24471;&#21487;&#21462;&#30340;&#24179;&#34913;&#12290;&#25105;&#20204;&#30340;&#21311;&#21517;&#21270;&#27169;&#22411;&#21517;&#20026;Blinder&#65292;&#22522;&#20110;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#21644;&#19968;&#20010;&#25110;&#22810;&#20010;&#23545;&#25239;&#35757;&#32451;&#30340;&#37492;&#21035;&#22120;&#32593;&#32476;&#12290;&#25105;&#20204;&#20351;&#29992;&#27169;&#22411;&#26080;&#20851;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;&#36890;&#36807;&#32852;&#21512;&#23398;&#20064;&#35757;&#32451;&#30340;&#21311;&#21517;&#21270;&#27169;&#22411;&#36866;&#24212;&#21040;&#27599;&#20010;&#29992;&#25143;&#30340;&#25968;&#25454;&#20998;&#24067;&#19978;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#35774;&#32622;&#19979;&#35780;&#20272;&#20102;Blinder&#65292;&#24182;&#23637;&#31034;&#20102;&#19982;&#22312;&#38598;&#20013;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#26368;&#20808;&#36827;&#21311;&#21517;&#21270;&#27169;&#22411;&#30456;&#27604;&#65292;Blinder&#22312;&#20004;&#20010;IMU&#25968;&#25454;&#38598;&#19978;&#25552;&#20379;&#20102;&#31471;&#21040;&#31471;&#30340;&#38544;&#31169;&#20445;&#25252;&#65292;&#36896;&#25104;&#30340;&#38544;&#31169;&#25439;&#22833;&#22686;&#21152;&#26368;&#22810;4.00%&#65292;&#25968;&#25454;&#25928;&#29992;&#38477;&#20302;&#26368;&#22810;4.24%&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;Blinder&#21311;&#21517;&#21270;&#23556;&#39057;&#24863;&#30693;&#27169;&#24577;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;Blinder&#33021;&#22815;&#27169;&#31946;&#22810;&#20010;&#20256;&#24863;&#22120;&#27169;&#24577;&#21516;&#26102;&#36827;&#34892;&#21311;&#21517;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a sensor data anonymization model that is trained on decentralized data and strikes a desirable trade-off between data utility and privacy, even in heterogeneous settings where the sensor data have different underlying distributions. Our anonymization model, dubbed Blinder, is based on a variational autoencoder and one or multiple discriminator networks trained in an adversarial fashion. We use the model-agnostic meta-learning framework to adapt the anonymization model trained via federated learning to each user's data distribution. We evaluate Blinder under different settings and show that it provides end-to-end privacy protection on two IMU datasets at the cost of increasing privacy loss by up to 4.00% and decreasing data utility by up to 4.24%, compared to the state-of-the-art anonymization model trained on centralized data. We also showcase Blinder's ability to anonymize the radio frequency sensing modality. Our experiments confirm that Blinder can obscure multi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#30340;&#27861;&#35821;&#38750;&#32467;&#26500;&#21270;&#20020;&#24202;&#31508;&#35760;&#21435;&#26631;&#35782;&#21270;&#26041;&#27861;&#65292;&#24182;&#24635;&#32467;&#20102;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#22312;&#33521;&#35821;&#25991;&#26723;&#19978;&#36827;&#34892;&#21435;&#26631;&#35782;&#21270;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#20445;&#25252;&#24739;&#32773;&#30340;&#38544;&#31169;&#65292;&#24182;&#20419;&#36827;&#21307;&#30103;&#25968;&#25454;&#30340;&#20849;&#20139;&#12290;</title><link>http://arxiv.org/abs/2209.09631</link><description>&lt;p&gt;
&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#30340;&#27861;&#35821;&#38750;&#32467;&#26500;&#21270;&#20020;&#24202;&#31508;&#35760;&#21435;&#26631;&#35782;&#21270;
&lt;/p&gt;
&lt;p&gt;
De-Identification of French Unstructured Clinical Notes for Machine Learning Tasks. (arXiv:2209.09631v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.09631
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#30340;&#27861;&#35821;&#38750;&#32467;&#26500;&#21270;&#20020;&#24202;&#31508;&#35760;&#21435;&#26631;&#35782;&#21270;&#26041;&#27861;&#65292;&#24182;&#24635;&#32467;&#20102;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#22312;&#33521;&#35821;&#25991;&#26723;&#19978;&#36827;&#34892;&#21435;&#26631;&#35782;&#21270;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#20445;&#25252;&#24739;&#32773;&#30340;&#38544;&#31169;&#65292;&#24182;&#20419;&#36827;&#21307;&#30103;&#25968;&#25454;&#30340;&#20849;&#20139;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#32467;&#26500;&#21270;&#30340;&#25991;&#26412;&#25968;&#25454;&#26159;&#21307;&#30103;&#31995;&#32479;&#30340;&#26680;&#24515;: &#21307;&#29983;&#20043;&#38388;&#30340;&#21307;&#30103;&#20989;&#20214;&#12289;&#25163;&#26415;&#25253;&#21578;&#12289;&#26681;&#25454;ICD-10&#26631;&#20934;&#23545;&#36807;&#31243;&#36827;&#34892;&#32534;&#30721;&#31561;&#12290;&#36825;&#20123;&#25991;&#20214;&#20013;&#21253;&#21547;&#30340;&#32454;&#33410;&#21487;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#24739;&#32773;&#65292;&#26356;&#22909;&#22320;&#31649;&#29702;&#20182;&#20204;&#65292;&#26356;&#22909;&#22320;&#30740;&#31350;&#30149;&#29702;&#65292;&#31934;&#30830;&#22320;&#25253;&#37228;&#30456;&#20851;&#30340;&#21307;&#30103;&#34892;&#20026;... &#36890;&#36807;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#20170;&#22825;&#20284;&#20046;&#33267;&#23569;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#21487;&#20197;&#23454;&#29616;&#36825;&#19968;&#20999;&#12290;&#28982;&#32780;&#65292;&#20986;&#20110;&#38544;&#31169;&#20445;&#25252;&#30340;&#26126;&#26174;&#21407;&#22240;&#65292;&#36825;&#20123;AI&#30340;&#35774;&#35745;&#32773;&#22312;&#36825;&#20123;&#25991;&#20214;&#20013;&#21253;&#21547;&#35782;&#21035;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#27809;&#26377;&#21512;&#27861;&#26435;&#21033;&#35775;&#38382;&#36825;&#20123;&#25991;&#20214;&#12290;&#21435;&#26631;&#35782;&#21270;&#36825;&#20123;&#25991;&#20214;&#65292;&#21363;&#26816;&#27979;&#24182;&#21024;&#38500;&#20854;&#20013;&#23384;&#22312;&#30340;&#25152;&#26377;&#35782;&#21035;&#20449;&#24687;&#65292;&#23545;&#20110;&#22312;&#20004;&#20010;&#20114;&#34917;&#30340;&#19990;&#30028;&#20043;&#38388;&#20849;&#20139;&#27492;&#25968;&#25454;&#26159;&#19968;&#39033;&#27861;&#24459;&#19978;&#24517;&#35201;&#30340;&#27493;&#39588;&#12290;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#22810;&#31181;&#23545;&#25991;&#26723;&#36827;&#34892;&#21435;&#26631;&#35782;&#21270;&#30340;&#26041;&#27861;&#65292;&#20027;&#35201;&#26159;&#33521;&#35821;&#12290;&#23613;&#31649;&#26816;&#27979;&#20998;&#25968;&#36890;&#24120;&#24456;&#39640;, &#20294;&#23427;&#20204;&#22312;&#27861;&#35821;&#20020;&#24202;&#31508;&#35760;&#19978;&#30340;&#34920;&#29616;&#24456;&#23569;&#26377;&#25253;&#36947;.
&lt;/p&gt;
&lt;p&gt;
Unstructured textual data are at the heart of health systems: liaison letters between doctors, operating reports, coding of procedures according to the ICD-10 standard, etc. The details included in these documents make it possible to get to know the patient better, to better manage him or her, to better study the pathologies, to accurately remunerate the associated medical acts\ldots All this seems to be (at least partially) within reach of today by artificial intelligence techniques. However, for obvious reasons of privacy protection, the designers of these AIs do not have the legal right to access these documents as long as they contain identifying data. De-identifying these documents, i.e. detecting and deleting all identifying information present in them, is a legally necessary step for sharing this data between two complementary worlds. Over the last decade, several proposals have been made to de-identify documents, mainly in English. While the detection scores are often high, the
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35299;&#20915;&#20102;&#22312;&#32447;&#39044;&#27979;&#20013;&#20998;&#24067;&#21464;&#21270;&#30340;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#31639;&#27861;&#26469;&#24212;&#23545;&#20998;&#24067;&#30340;&#22823;&#23567;&#21644;&#31867;&#22411;&#21464;&#21270;&#65292;&#20855;&#26377;&#23567;&#36951;&#25022;&#12290;&#36825;&#31181;&#31639;&#27861;&#19982;&#20219;&#20309;&#22522;&#20934;&#39044;&#27979;&#31639;&#27861;&#32467;&#21512;&#20351;&#29992;</title><link>http://arxiv.org/abs/2208.08401</link><description>&lt;p&gt;
&#33021;&#24212;&#23545;&#20219;&#24847;&#20998;&#24067;&#21464;&#21270;&#30340;&#22312;&#32447;&#39044;&#27979;&#30340;&#19968;&#33268;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Conformal Inference for Online Prediction with Arbitrary Distribution Shifts. (arXiv:2208.08401v3 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.08401
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35299;&#20915;&#20102;&#22312;&#32447;&#39044;&#27979;&#20013;&#20998;&#24067;&#21464;&#21270;&#30340;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#31639;&#27861;&#26469;&#24212;&#23545;&#20998;&#24067;&#30340;&#22823;&#23567;&#21644;&#31867;&#22411;&#21464;&#21270;&#65292;&#20855;&#26377;&#23567;&#36951;&#25022;&#12290;&#36825;&#31181;&#31639;&#27861;&#19982;&#20219;&#20309;&#22522;&#20934;&#39044;&#27979;&#31639;&#27861;&#32467;&#21512;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#19968;&#20010;&#22312;&#32447;&#29615;&#22659;&#20013;&#24418;&#25104;&#39044;&#27979;&#38598;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#29983;&#25104;&#25968;&#25454;&#30340;&#20998;&#24067;&#20801;&#35768;&#38543;&#26102;&#38388;&#21464;&#21270;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#22312;&#36825;&#20010;&#38382;&#39064;&#19978;&#23384;&#22312;&#21382;&#21490;&#25968;&#25454;&#30340;&#36807;&#37325;&#26435;&#37325;&#65292;&#22240;&#27492;&#21487;&#33021;&#26080;&#27861;&#24555;&#36895;&#24212;&#23545;&#24213;&#23618;&#21160;&#24577;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#32416;&#27491;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#36807;&#31243;&#65292;&#23545;&#20110;&#32473;&#23450;&#23485;&#24230;&#30340;&#25152;&#26377;&#23616;&#37096;&#26102;&#38388;&#38388;&#38548;&#37117;&#26377;&#21487;&#35777;&#26126;&#30340;&#23567;&#36951;&#25022;&#12290;&#25105;&#20204;&#36890;&#36807;&#20462;&#25913;Gibbs&#21644;Cand\`{e}s&#65288;2021&#65289;&#30340;&#33258;&#36866;&#24212;&#19968;&#33268;&#25512;&#26029;&#65288;ACI&#65289;&#31639;&#27861;&#65292;&#22312;ACI&#30340;&#26799;&#24230;&#19979;&#38477;&#26356;&#26032;&#20013;&#28155;&#21152;&#20102;&#19968;&#20010;&#39069;&#22806;&#30340;&#27493;&#39588;&#65292;&#20197;&#22312;&#26102;&#38388;&#19978;&#35843;&#25972;&#27493;&#38271;&#21442;&#25968;&#12290;&#20851;&#38190;&#26159;&#65292;&#36825;&#24847;&#21619;&#30528;&#19982;&#38656;&#35201;&#30693;&#36947;&#25968;&#25454;&#29983;&#25104;&#26426;&#21046;&#21464;&#21270;&#36895;&#29575;&#30340;ACI&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26032;&#36807;&#31243;&#23545;&#20998;&#24067;&#21464;&#21270;&#30340;&#22823;&#23567;&#21644;&#31867;&#22411;&#37117;&#26159;&#33258;&#36866;&#24212;&#30340;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#38750;&#24120;&#28789;&#27963;&#65292;&#21487;&#20197;&#19982;&#20219;&#20309;&#20135;&#29983;&#28857;&#20272;&#35745;&#25110;&#20272;&#35745;&#20998;&#20301;&#25968;&#30340;&#22522;&#20934;&#39044;&#27979;&#31639;&#27861;&#32467;&#21512;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of forming prediction sets in an online setting where the distribution generating the data is allowed to vary over time. Previous approaches to this problem suffer from over-weighting historical data and thus may fail to quickly react to the underlying dynamics. Here we correct this issue and develop a novel procedure with provably small regret over all local time intervals of a given width. We achieve this by modifying the adaptive conformal inference (ACI) algorithm of Gibbs and Cand\`{e}s (2021) to contain an additional step in which the step-size parameter of ACI's gradient descent update is tuned over time. Crucially, this means that unlike ACI, which requires knowledge of the rate of change of the data-generating mechanism, our new procedure is adaptive to both the size and type of the distribution shift. Our methods are highly flexible and can be used in combination with any baseline predictive algorithm that produces point estimates or estimated quantile
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;SGD&#21644;&#26435;&#37325;&#34928;&#20943;&#35757;&#32451;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#20250;&#23548;&#33268;&#23545;&#20110;&#26435;&#37325;&#30697;&#38453;&#30340;&#31209;&#26368;&#23567;&#21270;&#30340;&#20559;&#24046;&#65292;&#29305;&#21035;&#26159;&#22312;&#20351;&#29992;&#36739;&#23567;&#25209;&#37327;&#22823;&#23567;&#12289;&#26356;&#39640;&#23398;&#20064;&#29575;&#25110;&#22686;&#21152;&#26435;&#37325;&#34928;&#20943;&#26102;&#26356;&#20026;&#26174;&#33879;&#12290;&#27492;&#22806;&#65292;&#22312;&#20013;&#38388;&#31070;&#32463;&#32593;&#32476;&#23849;&#28291;&#26102;&#65292;&#23398;&#20064;&#30340;&#26435;&#37325;&#29305;&#21035;&#20302;&#31209;&#12290;&#36825;&#31181;&#20559;&#24046;&#19982;&#27867;&#21270;&#20043;&#38388;&#23384;&#22312;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2206.05794</link><description>&lt;p&gt;
SGD&#21644;&#26435;&#37325;&#34928;&#20943;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#34987;&#35777;&#26126;&#20250;&#24341;&#20837;&#20302;&#31209;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
SGD and Weight Decay Provably Induce a Low-Rank Bias in Neural Networks. (arXiv:2206.05794v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.05794
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;SGD&#21644;&#26435;&#37325;&#34928;&#20943;&#35757;&#32451;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#20250;&#23548;&#33268;&#23545;&#20110;&#26435;&#37325;&#30697;&#38453;&#30340;&#31209;&#26368;&#23567;&#21270;&#30340;&#20559;&#24046;&#65292;&#29305;&#21035;&#26159;&#22312;&#20351;&#29992;&#36739;&#23567;&#25209;&#37327;&#22823;&#23567;&#12289;&#26356;&#39640;&#23398;&#20064;&#29575;&#25110;&#22686;&#21152;&#26435;&#37325;&#34928;&#20943;&#26102;&#26356;&#20026;&#26174;&#33879;&#12290;&#27492;&#22806;&#65292;&#22312;&#20013;&#38388;&#31070;&#32463;&#32593;&#32476;&#23849;&#28291;&#26102;&#65292;&#23398;&#20064;&#30340;&#26435;&#37325;&#29305;&#21035;&#20302;&#31209;&#12290;&#36825;&#31181;&#20559;&#24046;&#19982;&#27867;&#21270;&#20043;&#38388;&#23384;&#22312;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#22312;&#35757;&#32451;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#26102;&#23398;&#20064;&#20302;&#31209;&#26435;&#37325;&#30697;&#38453;&#30340;&#20559;&#24046;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#23567;&#25209;&#37327;SGD&#21644;&#26435;&#37325;&#34928;&#20943;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20250;&#23548;&#33268;&#23545;&#20110;&#26435;&#37325;&#30697;&#38453;&#30340;&#31209;&#26368;&#23567;&#21270;&#30340;&#20559;&#24046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#39564;&#35777;&#26126;&#65292;&#24403;&#20351;&#29992;&#36739;&#23567;&#30340;&#25209;&#37327;&#22823;&#23567;&#12289;&#26356;&#39640;&#30340;&#23398;&#20064;&#29575;&#25110;&#22686;&#21152;&#30340;&#26435;&#37325;&#34928;&#20943;&#26102;&#65292;&#36825;&#31181;&#20559;&#24046;&#26356;&#21152;&#26174;&#33879;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#39044;&#27979;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#26435;&#37325;&#34928;&#20943;&#26159;&#23454;&#29616;&#36825;&#31181;&#20559;&#24046;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#22312;&#20013;&#38388;&#31070;&#32463;&#32593;&#32476;&#23849;&#28291;&#30340;&#24773;&#20917;&#19979;&#65292;&#23398;&#20064;&#30340;&#26435;&#37325;&#29305;&#21035;&#20302;&#31209;&#12290;&#19982;&#20808;&#21069;&#30340;&#25991;&#29486;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#19981;&#20381;&#36182;&#20110;&#20851;&#20110;&#25968;&#25454;&#12289;&#25910;&#25947;&#24615;&#25110;&#26435;&#37325;&#30697;&#38453;&#20248;&#21270;&#30340;&#20551;&#35774;&#12290;&#27492;&#22806;&#65292;&#23427;&#36866;&#29992;&#20110;&#20219;&#24847;&#23485;&#24230;&#25110;&#28145;&#24230;&#30340;&#21508;&#31181;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#20559;&#24046;&#19982;&#27867;&#21270;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the bias of Stochastic Gradient Descent (SGD) to learn low-rank weight matrices when training deep ReLU neural networks. Our results show that training neural networks with mini-batch SGD and weight decay causes a bias towards rank minimization over the weight matrices. Specifically, we show, both theoretically and empirically, that this bias is more pronounced when using smaller batch sizes, higher learning rates, or increased weight decay. Additionally, we predict and observe empirically that weight decay is necessary to achieve this bias. In addition, we show that in the presence of intermediate neural collapse, the learned weights are particularly low-rank. Unlike previous literature, our analysis does not rely on assumptions about the data, convergence, or optimality of the weight matrices. Furthermore, it applies to a wide range of neural network architectures of any width or depth. Finally, we empirically investigate the connection between this bias and generalization, 
&lt;/p&gt;</description></item><item><title>&#36825;&#26159;&#19968;&#20010;&#20851;&#20110;&#24378;&#21270;&#23398;&#20064;&#20013;&#22806;&#37096;&#32456;&#27490;&#38382;&#39064;&#30340;&#35770;&#25991;&#65292;&#36890;&#36807;&#23450;&#20041;&#32456;&#27490;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;TerMDP&#65289;&#24182;&#23398;&#20064;&#20854;&#21442;&#25968;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#32771;&#34385;&#32456;&#27490;&#24773;&#20917;&#24182;&#38480;&#21046;&#36951;&#25022;&#20540;&#30340;&#31639;&#27861;&#65292;&#24182;&#22312;&#39550;&#39542;&#21644;&#22522;&#20934;&#27979;&#35797;&#20013;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2205.15376</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#32456;&#27490;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning with a Terminator. (arXiv:2205.15376v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.15376
&lt;/p&gt;
&lt;p&gt;
&#36825;&#26159;&#19968;&#20010;&#20851;&#20110;&#24378;&#21270;&#23398;&#20064;&#20013;&#22806;&#37096;&#32456;&#27490;&#38382;&#39064;&#30340;&#35770;&#25991;&#65292;&#36890;&#36807;&#23450;&#20041;&#32456;&#27490;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;TerMDP&#65289;&#24182;&#23398;&#20064;&#20854;&#21442;&#25968;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#32771;&#34385;&#32456;&#27490;&#24773;&#20917;&#24182;&#38480;&#21046;&#36951;&#25022;&#20540;&#30340;&#31639;&#27861;&#65292;&#24182;&#22312;&#39550;&#39542;&#21644;&#22522;&#20934;&#27979;&#35797;&#20013;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22806;&#37096;&#32456;&#27490;&#38382;&#39064;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#32456;&#27490;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;TerMDP&#65289;&#65292;&#23427;&#26159;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#26694;&#26550;&#30340;&#25193;&#23637;&#65292;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;episode&#21487;&#33021;&#20250;&#34987;&#22806;&#37096;&#30340;&#38750;&#39532;&#23572;&#21487;&#22827;&#35266;&#23519;&#32773;&#20013;&#26029;&#12290;&#36825;&#20010;&#23450;&#20041;&#32771;&#34385;&#20102;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#24773;&#20917;&#65292;&#27604;&#22914;&#20154;&#31867;&#20986;&#20110;&#19981;&#36866;&#22240;&#32032;&#20013;&#26029;&#33258;&#20027;&#39550;&#39542;&#30340;&#20195;&#29702;&#31243;&#24207;&#12290;&#25105;&#20204;&#23398;&#20064;&#20102;TerMDP&#30340;&#21442;&#25968;&#65292;&#24182;&#21033;&#29992;&#20272;&#35745;&#38382;&#39064;&#30340;&#32467;&#26500;&#25552;&#20379;&#20102;&#29366;&#24577;&#32622;&#20449;&#30028;&#38480;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#30028;&#38480;&#26500;&#24314;&#20102;&#19968;&#20010;&#33021;&#22815;&#35777;&#26126;&#26377;&#25928;&#24615;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#32771;&#34385;&#20102;&#32456;&#27490;&#24773;&#20917;&#65292;&#24182;&#38480;&#21046;&#20102;&#36951;&#25022;&#20540;&#12290;&#22312;&#29702;&#35770;&#20998;&#26512;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#35774;&#35745;&#24182;&#23454;&#26045;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#23558;&#20048;&#35266;&#24615;&#65288;&#30456;&#23545;&#20110;&#32456;&#27490;&#65289;&#19982;&#21160;&#24577;&#25240;&#25187;&#22240;&#23376;&#30456;&#32467;&#21512;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#32456;&#27490;&#27010;&#29575;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#39640;&#32500;&#39550;&#39542;&#21644;MinAtar&#22522;&#20934;&#27979;&#35797;&#20013;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#22312;&#39550;&#39542;&#29615;&#22659;&#20013;&#23545;&#20154;&#31867;&#25968;&#25454;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
We present the problem of reinforcement learning with exogenous termination. We define the Termination Markov Decision Process (TerMDP), an extension of the MDP framework, in which episodes may be interrupted by an external non-Markovian observer. This formulation accounts for numerous real-world situations, such as a human interrupting an autonomous driving agent for reasons of discomfort. We learn the parameters of the TerMDP and leverage the structure of the estimation problem to provide state-wise confidence bounds. We use these to construct a provably-efficient algorithm, which accounts for termination, and bound its regret. Motivated by our theoretical analysis, we design and implement a scalable approach, which combines optimism (w.r.t. termination) and a dynamic discount factor, incorporating the termination probability. We deploy our method on high-dimensional driving and MinAtar benchmarks. Additionally, we test our approach on human data in a driving setting. Our results dem
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#25552;&#20986;&#26657;&#20934;&#34893;&#29983;&#21697;&#23450;&#20215;&#27169;&#22411;&#38382;&#39064;&#30340;&#21338;&#24328;&#35770;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#24076;&#26395;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#35299;&#20915;&#20854;&#20182;&#37329;&#34701;&#39046;&#22495;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#23398;&#20064;&#23616;&#37096;&#27874;&#21160;&#29575;&#20197;&#21450;&#26368;&#23567;&#21270;&#30334;&#24917;&#22823;&#26399;&#26435;&#20215;&#26684;&#25152;&#38656;&#30340;&#36335;&#24452;&#20381;&#36182;&#24615;&#12290;</title><link>http://arxiv.org/abs/2203.06865</link><description>&lt;p&gt;
&#34893;&#29983;&#21697;&#23450;&#20215;&#27169;&#22411;&#30340;&#26657;&#20934;&#65306;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#35266;&#28857;
&lt;/p&gt;
&lt;p&gt;
Calibration of Derivative Pricing Models: a Multi-Agent Reinforcement Learning Perspective. (arXiv:2203.06865v3 [q-fin.CP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.06865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#25552;&#20986;&#26657;&#20934;&#34893;&#29983;&#21697;&#23450;&#20215;&#27169;&#22411;&#38382;&#39064;&#30340;&#21338;&#24328;&#35770;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#24076;&#26395;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#35299;&#20915;&#20854;&#20182;&#37329;&#34701;&#39046;&#22495;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#23398;&#20064;&#23616;&#37096;&#27874;&#21160;&#29575;&#20197;&#21450;&#26368;&#23567;&#21270;&#30334;&#24917;&#22823;&#26399;&#26435;&#20215;&#26684;&#25152;&#38656;&#30340;&#36335;&#24452;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37327;&#21270;&#37329;&#34701;&#20013;&#26368;&#22522;&#26412;&#30340;&#38382;&#39064;&#20043;&#19968;&#26159;&#23384;&#22312;&#36866;&#21512;&#32473;&#23450;&#19968;&#32452;&#26399;&#26435;&#24066;&#22330;&#20215;&#26684;&#30340;&#36830;&#32493;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#12290;&#20256;&#32479;&#19978;&#65292;&#20154;&#20204;&#20351;&#29992;&#30452;&#35273;&#12289;&#29702;&#35770;&#21644;&#32463;&#39564;&#20998;&#26512;&#30340;&#28151;&#21512;&#26041;&#27861;&#26469;&#23547;&#25214;&#23454;&#29616;&#31934;&#30830;&#25110;&#36817;&#20284;&#21305;&#37197;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#22312;&#20110;&#23637;&#31034;&#22914;&#20309;&#36890;&#36807;&#36866;&#24403;&#30340;&#21338;&#24328;&#29702;&#35770;&#24418;&#24335;&#21270;&#38382;&#39064;&#65292;&#20511;&#21161;&#29616;&#20195;&#28145;&#24230;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#29616;&#26377;&#36827;&#23637;&#26469;&#25628;&#32034;&#38543;&#26426;&#36807;&#31243;&#31354;&#38388;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#25216;&#26415;&#21487;&#20197;&#34987;&#31038;&#21306;&#21033;&#29992;&#21644;&#25193;&#23637;&#65292;&#20197;&#35299;&#20915;&#35813;&#39046;&#22495;&#30340;&#37325;&#35201;&#38382;&#39064;&#65292;&#22914;&#32852;&#21512;SPX-VIX&#26657;&#20934;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#33021;&#22815;&#23398;&#20064;&#23616;&#37096;&#27874;&#21160;&#29575;&#20197;&#21450;&#22312;&#27874;&#21160;&#29575;&#36807;&#31243;&#20013;&#25152;&#38656;&#30340;&#36335;&#24452;&#20381;&#36182;&#24615;&#65292;&#20197;&#26368;&#23567;&#21270;&#30334;&#24917;&#22823;&#26399;&#26435;&#30340;&#20215;&#26684;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#30475;&#20316;&#26159;&#19968;&#31181;&#31890;&#23376;&#26041;&#27861;&#65292;&#31867;&#20284;&#20110;Guyon et Henry-Labordere&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the most fundamental questions in quantitative finance is the existence of continuous-time diffusion models that fit market prices of a given set of options. Traditionally, one employs a mix of intuition, theoretical and empirical analysis to find models that achieve exact or approximate fits. Our contribution is to show how a suitable game theoretical formulation of this problem can help solve this question by leveraging existing developments in modern deep multi-agent reinforcement learning to search in the space of stochastic processes. More importantly, we hope that our techniques can be leveraged and extended by the community to solve important problems in that field, such as the joint SPX-VIX calibration problem. Our experiments show that we are able to learn local volatility, as well as path-dependence required in the volatility process to minimize the price of a Bermudan option. Our algorithm can be seen as a particle method \`{a} la Guyon et Henry-Labordere where partic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#39640;&#25928;&#30340;&#36830;&#32493;&#27969;&#24418;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;Riemannian&#27969;&#24418;&#21644;Cholesky&#31354;&#38388;&#20043;&#38388;&#21033;&#29992;&#24494;&#20998;&#21516;&#32986;&#26144;&#23556;&#65292;&#23454;&#29616;&#20102;&#23545;&#38750;&#27431;&#20960;&#37324;&#24471;&#25968;&#25454;&#30340;&#20248;&#21270;&#38382;&#39064;&#39640;&#25928;&#27714;&#35299;&#21644;&#35745;&#31639;&#25104;&#26412;&#30340;&#22823;&#24133;&#38477;&#20302;&#12290;</title><link>http://arxiv.org/abs/2112.03379</link><description>&lt;p&gt;
&#28145;&#24230;&#39640;&#25928;&#30340;&#36830;&#32493;&#27969;&#24418;&#23398;&#20064;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Deep Efficient Continuous Manifold Learning for Time Series Modeling. (arXiv:2112.03379v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.03379
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#39640;&#25928;&#30340;&#36830;&#32493;&#27969;&#24418;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;Riemannian&#27969;&#24418;&#21644;Cholesky&#31354;&#38388;&#20043;&#38388;&#21033;&#29992;&#24494;&#20998;&#21516;&#32986;&#26144;&#23556;&#65292;&#23454;&#29616;&#20102;&#23545;&#38750;&#27431;&#20960;&#37324;&#24471;&#25968;&#25454;&#30340;&#20248;&#21270;&#38382;&#39064;&#39640;&#25928;&#27714;&#35299;&#21644;&#35745;&#31639;&#25104;&#26412;&#30340;&#22823;&#24133;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#31354;&#21069;&#30340;&#25104;&#21151;&#65292;&#23545;&#38750;&#27431;&#20960;&#37324;&#24471;&#25968;&#25454;&#36827;&#34892;&#24314;&#27169;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#29305;&#21035;&#26159;&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#20449;&#21495;&#22788;&#29702;&#21644;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#31561;&#39046;&#22495;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#23398;&#20064;&#26377;&#30410;&#30340;&#32479;&#35745;&#34920;&#31034;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#20005;&#26684;&#30340;&#32422;&#26463;&#26465;&#20214;&#65292;&#23558;&#20854;&#19982;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#32467;&#21512;&#22312;&#19968;&#36215;&#26102;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20248;&#21270;&#38382;&#39064;&#21644;&#20302;&#25928;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;Riemannian&#27969;&#24418;&#21644;Cholesky&#31354;&#38388;&#20043;&#38388;&#21033;&#29992;&#19968;&#20010;&#24494;&#20998;&#21516;&#32986;&#26144;&#23556;&#65292;&#19981;&#20165;&#21487;&#20197;&#39640;&#25928;&#22320;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#65292;&#32780;&#19988;&#21487;&#20197;&#22823;&#22823;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#21160;&#24577;&#24314;&#27169;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#36830;&#32493;&#27969;&#24418;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#31995;&#32479;&#38598;&#25104;&#27969;&#24418;&#24120;&#24494;&#20998;&#26041;&#31243;&#21644;&#38376;&#25511;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling non-Euclidean data is drawing extensive attention along with the unprecedented successes of deep neural networks in diverse fields. Particularly, a symmetric positive definite matrix is being actively studied in computer vision, signal processing, and medical image analysis, due to its ability to learn beneficial statistical representations. However, owing to its rigid constraints, it remains challenging to optimization problems and inefficient computational costs, especially, when incorporating it with a deep learning framework. In this paper, we propose a framework to exploit a diffeomorphism mapping between Riemannian manifolds and a Cholesky space, by which it becomes feasible not only to efficiently solve optimization problems but also to greatly reduce computation costs. Further, for dynamic modeling of time-series data, we devise a continuous manifold learning method by systematically integrating a manifold ordinary differential equation and a gated recurrent neural net
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#37096;&#20998;&#38597;&#21487;&#27604;&#30697;&#38453;&#30340;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35786;&#26029;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20020;&#30028;&#24615;&#30340;&#23454;&#29992;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#36882;&#24402;&#20851;&#31995;&#20998;&#26512;&#20102;&#24102;&#26377;LayerNorm&#21644;/&#25110;&#27531;&#24046;&#36830;&#25509;&#30340;&#28145;&#24230;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#20020;&#30028;&#24615;&#12290;</title><link>http://arxiv.org/abs/2111.12143</link><description>&lt;p&gt;
&#36890;&#36807;&#37096;&#20998;&#38597;&#21487;&#27604;&#30697;&#38453;&#23454;&#29616;&#23485;&#32780;&#28145;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#20851;&#38190;&#21021;&#22987;&#21270;&#65306;&#19968;&#33324;&#29702;&#35770;&#21644;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Critical Initialization of Wide and Deep Neural Networks through Partial Jacobians: General Theory and Applications. (arXiv:2111.12143v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.12143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#37096;&#20998;&#38597;&#21487;&#27604;&#30697;&#38453;&#30340;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35786;&#26029;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20020;&#30028;&#24615;&#30340;&#23454;&#29992;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#36882;&#24402;&#20851;&#31995;&#20998;&#26512;&#20102;&#24102;&#26377;LayerNorm&#21644;/&#25110;&#27531;&#24046;&#36830;&#25509;&#30340;&#28145;&#24230;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#20020;&#30028;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22240;&#20854;&#38590;&#20197;&#36827;&#34892;&#29702;&#35770;&#30740;&#31350;&#32780;&#38395;&#21517;&#12290;&#28982;&#32780;&#65292;&#24403;&#27599;&#20010;&#23618;&#20013;&#30340;&#21442;&#25968;&#25968;&#37327;&#36235;&#20110;&#26080;&#31351;&#26102;&#65292;&#32593;&#32476;&#20989;&#25968;&#25104;&#20026;&#19968;&#20010;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#65292;&#24182;&#19988;&#21487;&#20197;&#36827;&#34892;&#23450;&#37327;&#39044;&#27979;&#25551;&#36848;&#12290;&#39640;&#26031;&#36817;&#20284;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#21046;&#23450;&#36873;&#25321;&#36229;&#21442;&#25968;&#65288;&#20363;&#22914;&#26435;&#37325;&#21644;&#20559;&#24046;&#30340;&#26041;&#24046;&#20197;&#21450;&#23398;&#20064;&#29575;&#65289;&#30340;&#26631;&#20934;&#12290;&#36825;&#20123;&#26631;&#20934;&#20381;&#36182;&#20110;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23450;&#20041;&#30340;&#20020;&#30028;&#24615;&#27010;&#24565;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#35786;&#26029;&#20020;&#30028;&#24615;&#30340;&#26032;&#23454;&#29992;&#26041;&#24335;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#32593;&#32476;&#30340;&#8220;&#37096;&#20998;&#38597;&#21487;&#27604;&#30697;&#38453;&#8221;&#65292;&#23450;&#20041;&#20026;&#23618;$l$&#20013;&#30340;&#39044;&#28608;&#27963;&#23545;&#20110;&#23618;$l_0\leq l$&#20013;&#30340;&#39044;&#28608;&#27963;&#30340;&#23548;&#25968;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#37096;&#20998;&#38597;&#21487;&#27604;&#30697;&#38453;&#33539;&#25968;&#30340;&#36882;&#24402;&#20851;&#31995;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#20851;&#31995;&#20998;&#26512;&#20102;&#24102;&#26377;LayerNorm&#21644;/&#25110;&#27531;&#24046;&#36830;&#25509;&#30340;&#28145;&#24230;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#20020;&#30028;&#24615;&#12290;&#25105;&#20204;&#25512;&#23548;&#24182;&#23454;&#29616;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#24265;&#20215;&#30340;&#25968;&#20540;&#27979;&#35797;&#65292;&#20351;&#24471;&#21487;&#20197;&#36873;&#25321;&#36866;&#24403;&#30340;&#26435;&#37325;&#21644;&#20559;&#24046;&#26041;&#24046;&#20197;&#21450;&#23398;&#20064;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are notorious for defying theoretical treatment. However, when the number of parameters in each layer tends to infinity, the network function is a Gaussian process (GP) and quantitatively predictive description is possible. Gaussian approximation allows one to formulate criteria for selecting hyperparameters, such as variances of weights and biases, as well as the learning rate. These criteria rely on the notion of criticality defined for deep neural networks. In this work we describe a new practical way to diagnose criticality. We introduce \emph{partial Jacobians} of a network, defined as derivatives of preactivations in layer $l$ with respect to preactivations in layer $l_0\leq l$. We derive recurrence relations for the norms of partial Jacobians and utilize these relations to analyze criticality of deep fully connected neural networks with LayerNorm and/or residual connections. We derive and implement a simple and cheap numerical test that allows one to select 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#32593;&#32476;&#20013;&#30340;&#20998;&#24067;&#24335;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#23436;&#20840;&#20998;&#24067;&#24335;&#30340;&#31639;&#27861;&#65292;&#22522;&#20110;&#32463;&#20856;&#30340;UCB&#31639;&#27861;&#21644;KL-UCB&#31639;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#20123;&#31639;&#27861;&#33021;&#36798;&#21040;&#26356;&#22909;&#30340;&#23545;&#25968;&#28176;&#36817;&#21518;&#24724;&#65292;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#37051;&#23621;&#20851;&#31995;&#36234;&#22810;&#65292;&#21518;&#24724;&#20540;&#36234;&#22909;&#12290;</title><link>http://arxiv.org/abs/2111.10933</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#22810;&#33218;&#36172;&#21338;&#26426;&#21487;&#20197;&#36229;&#36234;&#38598;&#20013;&#24335;&#19978;&#32622;&#20449;&#30028;&#38480;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Decentralized Multi-Armed Bandits Can Outperform Centralized Upper Confidence Bound Algorithms. (arXiv:2111.10933v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.10933
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#32593;&#32476;&#20013;&#30340;&#20998;&#24067;&#24335;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#23436;&#20840;&#20998;&#24067;&#24335;&#30340;&#31639;&#27861;&#65292;&#22522;&#20110;&#32463;&#20856;&#30340;UCB&#31639;&#27861;&#21644;KL-UCB&#31639;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#20123;&#31639;&#27861;&#33021;&#36798;&#21040;&#26356;&#22909;&#30340;&#23545;&#25968;&#28176;&#36817;&#21518;&#24724;&#65292;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#37051;&#23621;&#20851;&#31995;&#36234;&#22810;&#65292;&#21518;&#24724;&#20540;&#36234;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#32593;&#32476;&#20013;&#30340;&#20998;&#24067;&#24335;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#12290;&#20551;&#35774;N&#20010;&#26234;&#33021;&#20307;&#21516;&#26102;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#20182;&#20204;&#38754;&#23545;&#30528;&#19968;&#32452;&#20849;&#21516;&#30340;M&#20010;&#33218;&#24182;&#20849;&#20139;&#30456;&#21516;&#30340;&#33218;&#22870;&#21169;&#20998;&#24067;&#12290;&#27599;&#20010;&#26234;&#33021;&#20307;&#21482;&#33021;&#20174;&#37051;&#23621;&#22788;&#25509;&#25910;&#20449;&#24687;&#65292;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#37051;&#23621;&#20851;&#31995;&#30001;&#19968;&#20010;&#26080;&#21521;&#22270;&#25551;&#36848;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#23436;&#20840;&#20998;&#24067;&#24335;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#31639;&#27861;&#65292;&#20998;&#21035;&#22522;&#20110;&#32463;&#20856;&#30340;&#19978;&#32622;&#20449;&#30028;&#38480;&#65288;UCB&#65289;&#31639;&#27861;&#21644;&#26368;&#20808;&#36827;&#30340;KL-UCB&#31639;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#20998;&#24067;&#24335;&#31639;&#27861;&#20351;&#32593;&#32476;&#20013;&#30340;&#27599;&#20010;&#26234;&#33021;&#20307;&#33021;&#22815;&#23454;&#29616;&#27604;&#20854;&#21333;&#19968;&#26234;&#33021;&#20307;&#30456;&#24212;&#31639;&#27861;&#26356;&#22909;&#30340;&#23545;&#25968;&#28176;&#36817;&#21518;&#24724;&#65292;&#21069;&#25552;&#26159;&#26234;&#33021;&#20307;&#33267;&#23569;&#26377;&#19968;&#20010;&#37051;&#23621;&#65292;&#32780;&#19988;&#26234;&#33021;&#20307;&#26377;&#36234;&#22810;&#30340;&#37051;&#23621;&#65292;&#21518;&#24724;&#20540;&#20250;&#36234;&#22909;&#65292;&#36825;&#24847;&#21619;&#30528;&#25972;&#20307;&#30340;&#21644;&#22823;&#20110;&#20854;&#32452;&#25104;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies a decentralized multi-armed bandit problem in a multi-agent network. The problem is simultaneously solved by N agents assuming they face a common set of M arms and share the same arms' reward distributions. Each agent can receive information only from its neighbors, where the neighbor relationships among the agents are described by an undirected graph. Two fully decentralized multi-armed bandit algorithms are proposed, respectively based on the classic upper confidence bound (UCB) algorithm and the state-of-the-art KL-UCB algorithm. The proposed decentralized algorithms permit each agent in the network to achieve a better logarithmic asymptotic regret than their single-agent counterparts, provided that the agent has at least one neighbor, and the more neighbors an agent has, the better regret it will have, meaning that the sum is more than its component parts.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36716;&#25442;&#39118;&#38505;&#26368;&#23567;&#21270;&#26694;&#26550;&#65292;&#21487;&#20197;&#21516;&#26102;&#23398;&#20064;&#39044;&#27979;&#27169;&#22411;&#21644;&#25968;&#25454;&#21464;&#25442;&#65292;&#29305;&#21035;&#26159;&#20998;&#24067;&#30340;&#21464;&#25442;&#12290;&#25105;&#20204;&#20197;&#23398;&#20064;&#22270;&#20687;&#22686;&#24378;&#20026;&#20027;&#35201;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#36807;&#25311;&#21512;&#38382;&#39064;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2111.08190</link><description>&lt;p&gt;
&#20351;&#29992;&#36716;&#25442;&#39118;&#38505;&#26368;&#23567;&#21270;&#23398;&#20064;&#22686;&#24378;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
Learning Augmentation Distributions using Transformed Risk Minimization. (arXiv:2111.08190v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.08190
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36716;&#25442;&#39118;&#38505;&#26368;&#23567;&#21270;&#26694;&#26550;&#65292;&#21487;&#20197;&#21516;&#26102;&#23398;&#20064;&#39044;&#27979;&#27169;&#22411;&#21644;&#25968;&#25454;&#21464;&#25442;&#65292;&#29305;&#21035;&#26159;&#20998;&#24067;&#30340;&#21464;&#25442;&#12290;&#25105;&#20204;&#20197;&#23398;&#20064;&#22270;&#20687;&#22686;&#24378;&#20026;&#20027;&#35201;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#36807;&#25311;&#21512;&#38382;&#39064;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#8220;&#36716;&#25442;&#39118;&#38505;&#26368;&#23567;&#21270;&#8221;&#65288;TRM&#65289;&#26694;&#26550;&#65292;&#20316;&#20026;&#20256;&#32479;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#25193;&#23637;&#12290;&#22312;TRM&#20013;&#65292;&#25105;&#20204;&#19981;&#20165;&#20248;&#21270;&#39044;&#27979;&#27169;&#22411;&#65292;&#36824;&#20248;&#21270;&#25968;&#25454;&#21464;&#25442;&#65292;&#29305;&#21035;&#26159;&#20998;&#24067;&#30340;&#21464;&#25442;&#12290;&#20316;&#20026;&#19968;&#20010;&#20851;&#38190;&#24212;&#29992;&#65292;&#25105;&#20204;&#20851;&#27880;&#23398;&#20064;&#22686;&#24378;&#65292;&#20363;&#22914;&#36866;&#24403;&#26059;&#36716;&#22270;&#20687;&#65292;&#20197;&#25552;&#39640;&#32473;&#23450;&#39044;&#27979;&#22120;&#31867;&#21035;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;TRM&#26041;&#27861;&#65306;&#65288;1&#65289;&#22312;&#21333;&#20010;&#35757;&#32451;&#24490;&#29615;&#20013;&#32852;&#21512;&#23398;&#20064;&#21464;&#25442;&#21644;&#27169;&#22411;&#65307;&#65288;2&#65289;&#36866;&#29992;&#20110;&#20219;&#20309;&#36866;&#29992;&#20110;&#26631;&#20934;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#35757;&#32451;&#31639;&#27861;&#65307;&#65288;3&#65289;&#22788;&#29702;&#20219;&#20309;&#21464;&#25442;&#65292;&#20363;&#22914;&#31163;&#25955;&#21644;&#36830;&#32493;&#31867;&#30340;&#22686;&#24378;&#12290;&#20026;&#20102;&#36991;&#20813;&#22312;&#23454;&#26045;&#32463;&#39564;&#36716;&#25442;&#39118;&#38505;&#26368;&#23567;&#21270;&#26102;&#36807;&#25311;&#21512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;PAC-Bayes&#29702;&#35770;&#30340;&#26032;&#22411;&#27491;&#21017;&#21270;&#22120;&#12290;&#23545;&#20110;&#23398;&#20064;&#22270;&#20687;&#30340;&#22686;&#24378;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20960;&#20309;&#21464;&#25442;&#22359;&#30340;&#38543;&#26426;&#32452;&#21512;&#23545;&#22686;&#24378;&#31354;&#38388;&#36827;&#34892;&#21442;&#25968;&#21270;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new \emph{Transformed Risk Minimization} (TRM) framework as an extension of classical risk minimization. In TRM, we optimize not only over predictive models, but also over data transformations; specifically over distributions thereof. As a key application, we focus on learning augmentations; for instance appropriate rotations of images, to improve classification performance with a given class of predictors. Our TRM method (1) jointly learns transformations and models in a \emph{single training loop}, (2) works with any training algorithm applicable to standard risk minimization, and (3) handles any transforms, such as discrete and continuous classes of augmentations. To avoid overfitting when implementing empirical transformed risk minimization, we propose a novel regularizer based on PAC-Bayes theory. For learning augmentations of images, we propose a new parametrization of the space of augmentations via a stochastic composition of blocks of geometric transforms. This lea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#21367;&#31215;&#27169;&#24335;&#26680;&#32593;&#32476;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#23398;&#20064;&#20301;&#32622;&#24863;&#30693;&#27169;&#24335;&#26680;&#20989;&#25968;&#22312;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#23376;&#31354;&#38388;&#20869;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#30452;&#25509;&#35299;&#37322;&#21644;&#35780;&#20272;&#39044;&#27979;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2111.02272</link><description>&lt;p&gt;
&#21367;&#31215;&#27169;&#24335;&#26680;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Convolutional Motif Kernel Networks. (arXiv:2111.02272v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.02272
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#21367;&#31215;&#27169;&#24335;&#26680;&#32593;&#32476;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#23398;&#20064;&#20301;&#32622;&#24863;&#30693;&#27169;&#24335;&#26680;&#20989;&#25968;&#22312;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#23376;&#31354;&#38388;&#20869;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#30452;&#25509;&#35299;&#37322;&#21644;&#35780;&#20272;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22312;&#26816;&#27979;&#19982;&#29305;&#23450;&#32467;&#26524;&#30456;&#20851;&#30340;&#25968;&#25454;&#20869;&#30340;&#30456;&#20851;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#27169;&#22411;&#30340;&#40657;&#30418;&#29305;&#24615;&#21487;&#33021;&#20250;&#38459;&#30861;&#30740;&#31350;&#39046;&#22495;&#20013;&#30340;&#30693;&#35782;&#36827;&#27493;&#65292;&#22240;&#20026;&#23427;&#20250;&#25513;&#30422;&#20915;&#31574;&#36807;&#31243;&#24182;&#38459;&#27490;&#31185;&#23398;&#23478;&#23436;&#20840;&#29702;&#35299;&#39044;&#27979;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#20687;&#21307;&#30103;&#20445;&#20581;&#25552;&#20379;&#32773;&#36825;&#26679;&#30340;&#39046;&#22495;&#19987;&#23478;&#38656;&#35201;&#21487;&#35299;&#37322;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#20197;&#35780;&#20272;&#22312;&#39640;&#39118;&#38505;&#24773;&#26223;&#20013;&#26159;&#21542;&#21487;&#20197;&#20449;&#20219;&#39044;&#27979;&#32467;&#26524;&#65292;&#24182;&#24110;&#21161;&#20182;&#20204;&#23558;&#27169;&#22411;&#25972;&#21512;&#21040;&#33258;&#24049;&#30340;&#26085;&#24120;&#24037;&#20316;&#20013;&#12290;&#22240;&#27492;&#65292;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#22312;&#23558;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#21307;&#30103;&#20445;&#20581;&#31561;&#39640;&#39118;&#38505;&#24773;&#22659;&#20013;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#21367;&#31215;&#27169;&#24335;&#26680;&#32593;&#32476;&#65292;&#19968;&#31181;&#28041;&#21450;&#22312;&#26680;&#20989;&#25968;&#30340;&#20301;&#32622;&#24863;&#30693;&#27169;&#24335;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#30340;&#23376;&#31354;&#38388;&#20013;&#23398;&#20064;&#29305;&#24449;&#34920;&#31034;&#30340;&#31070;&#32463;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#12290;&#26368;&#32456;&#30340;&#27169;&#22411;&#20351;&#24471;&#21487;&#20197;&#30452;&#25509;&#35299;&#37322;&#21644;&#35780;&#20272;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial neural networks show promising performance in detecting correlations within data that are associated with specific outcomes. However, the black-box nature of such models can hinder the knowledge advancement in research fields by obscuring the decision process and preventing scientist to fully conceptualize predicted outcomes. Furthermore, domain experts like healthcare providers need explainable predictions to assess whether a predicted outcome can be trusted in high stakes scenarios and to help them integrating a model into their own routine. Therefore, interpretable models play a crucial role for the incorporation of machine learning into high stakes scenarios like healthcare. In this paper we introduce Convolutional Motif Kernel Networks, a neural network architecture that involves learning a feature representation within a subspace of the reproducing kernel Hilbert space of the position-aware motif kernel function. The resulting model enables to directly interpret and ev
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#28151;&#21512;&#27169;&#22411;&#65292;&#21033;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#32467;&#21512;&#32447;&#24615;&#39057;&#29575;&#21709;&#24212;&#20989;&#25968;&#27169;&#22411;&#36827;&#34892;&#38750;&#32447;&#24615;&#31995;&#32479;&#35782;&#21035;&#12290;&#35813;&#26041;&#27861;&#36824;&#21487;&#20197;&#24212;&#29992;&#20110;&#34394;&#25311;&#20256;&#24863;&#12290;&#36890;&#36807;&#23545;&#30130;&#21171;&#35797;&#39564;&#25968;&#25454;&#36827;&#34892;&#27979;&#35797;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2107.03645</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#31995;&#32479;&#35782;&#21035;&#30340;&#28151;&#21512;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35780;&#20272;&#65306;&#30130;&#21171;&#35797;&#39564;&#26550;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Assessment of hybrid machine learning models for non-linear system identification of fatigue test rigs. (arXiv:2107.03645v3 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.03645
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#28151;&#21512;&#27169;&#22411;&#65292;&#21033;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#32467;&#21512;&#32447;&#24615;&#39057;&#29575;&#21709;&#24212;&#20989;&#25968;&#27169;&#22411;&#36827;&#34892;&#38750;&#32447;&#24615;&#31995;&#32479;&#35782;&#21035;&#12290;&#35813;&#26041;&#27861;&#36824;&#21487;&#20197;&#24212;&#29992;&#20110;&#34394;&#25311;&#20256;&#24863;&#12290;&#36890;&#36807;&#23545;&#30130;&#21171;&#35797;&#39564;&#25968;&#25454;&#36827;&#34892;&#27979;&#35797;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#32473;&#23450;&#30130;&#21171;&#35797;&#39564;&#21488;&#39537;&#21160;&#20449;&#21495;&#30340;&#31995;&#32479;&#21709;&#24212;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#36890;&#24120;&#37319;&#29992;&#32447;&#24615;&#39057;&#29575;&#21709;&#24212;&#20989;&#25968;&#27169;&#22411;&#12290;&#20026;&#20102;&#32771;&#34385;&#38750;&#32447;&#24615;&#29616;&#35937;&#65292;&#24314;&#35758;&#37319;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#27169;&#22411;&#65292;&#36890;&#36807;&#22686;&#21152;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#32593;&#32476;&#23545;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#34917;&#20805;&#12290;&#35813;&#26041;&#27861;&#30340;&#38468;&#21152;&#34394;&#25311;&#20256;&#24863;&#24212;&#29992;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;&#20351;&#29992;&#26469;&#33258;&#20282;&#26381;&#28082;&#21387;&#35797;&#39564;&#21488;&#30340;&#38750;&#32447;&#24615;&#23454;&#39564;&#25968;&#25454;&#36827;&#34892;&#27979;&#35797;&#65292;&#24182;&#20844;&#24320;&#20102;&#35813;&#25968;&#25454;&#38598;&#12290;&#22312;&#35780;&#20272;&#20013;&#37319;&#29992;&#20102;&#21508;&#31181;&#26102;&#38388;&#21644;&#39057;&#29575;&#22495;&#25351;&#26631;&#20197;&#21450;&#21464;&#24133;&#19979;&#30340;&#30130;&#21171;&#24378;&#24230;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prediction of system responses for a given fatigue test bench drive signal is a challenging task, for which linear frequency response function models are commonly used. To account for non-linear phenomena, a novel hybrid model is suggested, which augments existing approaches using Long Short-Term Memory networks. Additional virtual sensing applications of this method are demonstrated. The approach is tested using non-linear experimental data from a servo-hydraulic test rig and this dataset is made publicly available. A variety of metrics in time and frequency domains, as well as fatigue strength under variable amplitudes, are employed in the evaluation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#20108;&#36827;&#21046;&#39044;&#27979;&#35774;&#32622;&#20013;&#30340;&#26657;&#20934;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#26657;&#20934;&#35823;&#24046;&#30340;&#937;(T^(0.528))&#19979;&#30028;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#36229;&#36807;&#8730;T&#30340;&#19979;&#30028;&#12290;</title><link>http://arxiv.org/abs/2012.03454</link><description>&lt;p&gt;
&#24378;&#21270;&#26657;&#20934;&#19979;&#30028;&#36890;&#36807;&#32469;&#36807;
&lt;/p&gt;
&lt;p&gt;
Stronger Calibration Lower Bounds via Sidestepping. (arXiv:2012.03454v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2012.03454
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#20108;&#36827;&#21046;&#39044;&#27979;&#35774;&#32622;&#20013;&#30340;&#26657;&#20934;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#26657;&#20934;&#35823;&#24046;&#30340;&#937;(T^(0.528))&#19979;&#30028;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#36229;&#36807;&#8730;T&#30340;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#31181;&#22312;&#32447;&#20108;&#36827;&#21046;&#39044;&#27979;&#35774;&#32622;&#65292;&#20854;&#20013;&#39044;&#27979;&#21592;&#36880;&#20010;&#35266;&#23519;&#19968;&#31995;&#21015;T&#20010;&#27604;&#29305;&#20301;&#12290;&#22312;&#25581;&#31034;&#27599;&#20010;&#27604;&#29305;&#20301;&#20043;&#21069;&#65292;&#39044;&#27979;&#21592;&#39044;&#27979;&#35813;&#27604;&#29305;&#20301;&#20026;1&#30340;&#27010;&#29575;&#12290;&#22914;&#26524;&#23545;&#20110;&#27599;&#20010;p&#8712;[0,1]&#65292;&#22312;&#39044;&#27979;&#21592;&#39044;&#27979;&#27010;&#29575;&#20026;p&#30340;n_p&#20010;&#27604;&#29305;&#20301;&#20013;&#65292;&#23454;&#38469;&#20986;&#29616;&#30340;1&#30340;&#25968;&#37327;m_p&#30830;&#23454;&#31561;&#20110;p&#8901;n_p&#65292;&#21017;&#31216;&#39044;&#27979;&#21592;&#20855;&#26377;&#33391;&#22909;&#26657;&#20934;&#24615;&#12290;&#26657;&#20934;&#35823;&#24046;&#23450;&#20041;&#20026;&#8721;_p|mp&#8901;p&#8901;np|&#65292;&#29992;&#26469;&#34913;&#37327;&#39044;&#27979;&#21592;&#20559;&#31163;&#33391;&#22909;&#26657;&#20934;&#24615;&#30340;&#31243;&#24230;&#12290;&#23613;&#31649;&#24050;&#32463;&#30693;&#36947;&#21363;&#20351;&#22312;&#27604;&#29305;&#20301;&#26159;&#23545;&#25239;&#24615;&#36873;&#25321;&#30340;&#24773;&#20917;&#19979;&#65292;&#22522;&#20110;&#20043;&#21069;&#30340;&#39044;&#27979;&#20063;&#21487;&#33021;&#23454;&#29616;O(T^(2/3))&#30340;&#26657;&#20934;&#35823;&#24046;&#65292;&#20294;&#26159;&#23545;&#20110;&#19979;&#30028;&#26041;&#38754;&#20960;&#20046;&#27809;&#26377;&#20219;&#20309;&#20102;&#35299;&#65292;&#38500;&#20102;&#36890;&#36807;&#29420;&#31435;&#20844;&#24179;&#30828;&#24065;&#32763;&#36716;&#30340;&#24179;&#20961;&#20363;&#23376;&#24471;&#21040;&#30340;&#937;(&#8730;T)&#19979;&#30028;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#26657;&#20934;&#35823;&#24046;&#30340;&#937;(T^(0.528))&#19979;&#30028;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#36229;&#36807;&#8730;T&#30340;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider an online binary prediction setting where a forecaster observes a sequence of $T$ bits one by one. Before each bit is revealed, the forecaster predicts the probability that the bit is $1$. The forecaster is called well-calibrated if for each $p \in [0, 1]$, among the $n_p$ bits for which the forecaster predicts probability $p$, the actual number of ones, $m_p$, is indeed equal to $p \cdot n_p$. The calibration error, defined as $\sum_p |m_p p n_p|$, quantifies the extent to which the forecaster deviates from being well-calibrated. It has long been known that an $O(T^{2/3})$ calibration error is achievable even when the bits are chosen adversarially, and possibly based on the previous predictions. However, little is known on the lower bound side, except an $\Omega(\sqrt{T})$ bound that follows from the trivial example of independent fair coin flips.  In this paper, we prove an $\Omega(T^{0.528})$ bound on the calibration error, which is the first super-$\sqrt{T}$ lower bou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22797;&#21512;&#20256;&#36755;&#25955;&#24230;&#30340;&#39640;&#26031;&#28151;&#21512;&#31616;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#39640;&#26031;&#28151;&#21512;&#22312;&#36882;&#24402;&#26356;&#26032;&#20013;&#38454;&#25968;&#25351;&#25968;&#22686;&#21152;&#30340;&#25512;&#26029;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2002.08410</link><description>&lt;p&gt;
&#29992;&#22797;&#21512;&#20256;&#36755;&#25955;&#24230;&#36827;&#34892;&#39640;&#26031;&#28151;&#21512;&#31616;&#21270;
&lt;/p&gt;
&lt;p&gt;
Gaussian Mixture Reduction with Composite Transportation Divergence. (arXiv:2002.08410v4 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2002.08410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22797;&#21512;&#20256;&#36755;&#25955;&#24230;&#30340;&#39640;&#26031;&#28151;&#21512;&#31616;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#39640;&#26031;&#28151;&#21512;&#22312;&#36882;&#24402;&#26356;&#26032;&#20013;&#38454;&#25968;&#25351;&#25968;&#22686;&#21152;&#30340;&#25512;&#26029;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#28151;&#21512;&#22312;&#23494;&#24230;&#20272;&#35745;&#12289;&#20449;&#24565;&#20256;&#25773;&#21644;&#36125;&#21494;&#26031;&#28388;&#27874;&#31561;&#21508;&#31181;&#24212;&#29992;&#20013;&#34987;&#24191;&#27867;&#29992;&#20110;&#36924;&#36817;&#23494;&#24230;&#20989;&#25968;&#12290;&#36825;&#20123;&#24212;&#29992;&#36890;&#24120;&#21033;&#29992;&#39640;&#26031;&#28151;&#21512;&#20316;&#20026;&#36882;&#24402;&#26356;&#26032;&#30340;&#21021;&#22987;&#36817;&#20284;&#12290;&#36825;&#20123;&#36882;&#24402;&#36807;&#31243;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#28304;&#20110;&#28151;&#21512;&#38454;&#25968;&#30340;&#25351;&#25968;&#22686;&#21152;&#65292;&#23548;&#33268;&#38590;&#20197;&#27714;&#35299;&#30340;&#25512;&#26029;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#22256;&#38590;&#65292;&#21487;&#20197;&#20351;&#29992;&#39640;&#26031;&#28151;&#21512;&#31616;&#21270;&#65288;GMR&#65289;&#23558;&#39640;&#38454;&#39640;&#26031;&#28151;&#21512;&#36817;&#20284;&#20026;&#20302;&#38454;&#28151;&#21512;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#22522;&#20110;&#32858;&#31867;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#21644;&#35745;&#31639;&#25928;&#29575;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#23427;&#20204;&#30340;&#25910;&#25947;&#24615;&#36136;&#21644;&#26368;&#20248;&#30446;&#26631;&#20173;&#28982;&#26410;&#30693;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22797;&#21512;&#20256;&#36755;&#25955;&#24230;&#30340;&#26032;&#22411;&#20248;&#21270;GMR&#26041;&#27861;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20027;&#20803;&#26368;&#23567;&#21270;&#31639;&#27861;&#26469;&#35745;&#31639;&#31616;&#21270;&#30340;&#28151;&#21512;&#65292;&#24182;&#22312;g&#20013;&#24314;&#31435;&#20102;&#20854;&#29702;&#35770;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gaussian mixtures are widely used for approximating density functions in various applications such as density estimation, belief propagation, and Bayesian filtering. These applications often utilize Gaussian mixtures as initial approximations that are updated recursively. A key challenge in these recursive processes stems from the exponential increase in the mixture's order, resulting in intractable inference. To overcome the difficulty, the Gaussian mixture reduction (GMR), which approximates a high order Gaussian mixture by one with a lower order, can be used. Although existing clustering-based methods are known for their satisfactory performance and computational efficiency, their convergence properties and optimal targets remain unknown. In this paper, we propose a novel optimization-based GMR method based on composite transportation divergence (CTD). We develop a majorization-minimization algorithm for computing the reduced mixture and establish its theoretical convergence under g
&lt;/p&gt;</description></item><item><title>&#21487;&#21464;&#24418;&#29983;&#25104;&#22120;&#32593;&#32476;&#33021;&#22815;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#35299;&#32806;&#22270;&#20687;&#21644;&#35270;&#39057;&#20013;&#30340;&#22806;&#35266;&#21644;&#20960;&#20309;&#20449;&#24687;&#65292;&#36890;&#36807;&#29983;&#25104;&#21464;&#24418;&#22330;&#23454;&#29616;&#20960;&#20309;&#21464;&#24418;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#29992;&#19988;&#26377;&#25928;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/1806.06298</link><description>&lt;p&gt;
&#21487;&#21464;&#24418;&#29983;&#25104;&#22120;&#32593;&#32476;&#65306;&#26080;&#30417;&#30563;&#35299;&#32806;&#22806;&#35266;&#21644;&#20960;&#20309;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Deformable Generator Networks: Unsupervised Disentanglement of Appearance and Geometry. (arXiv:1806.06298v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1806.06298
&lt;/p&gt;
&lt;p&gt;
&#21487;&#21464;&#24418;&#29983;&#25104;&#22120;&#32593;&#32476;&#33021;&#22815;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#35299;&#32806;&#22270;&#20687;&#21644;&#35270;&#39057;&#20013;&#30340;&#22806;&#35266;&#21644;&#20960;&#20309;&#20449;&#24687;&#65292;&#36890;&#36807;&#29983;&#25104;&#21464;&#24418;&#22330;&#23454;&#29616;&#20960;&#20309;&#21464;&#24418;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#29992;&#19988;&#26377;&#25928;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#21464;&#24418;&#29983;&#25104;&#22120;&#27169;&#22411;&#65292;&#20197;&#32431;&#31929;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#35299;&#32806;&#22270;&#20687;&#21644;&#35270;&#39057;&#25968;&#25454;&#30340;&#22806;&#35266;&#21644;&#20960;&#20309;&#20449;&#24687;&#12290;&#22806;&#35266;&#29983;&#25104;&#22120;&#32593;&#32476;&#27169;&#25311;&#19982;&#22806;&#35266;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#21253;&#25324;&#39068;&#33394;&#12289;&#29031;&#26126;&#12289;&#36523;&#20221;&#25110;&#31867;&#21035;&#65292;&#32780;&#20960;&#20309;&#29983;&#25104;&#22120;&#36890;&#36807;&#29983;&#25104;&#21464;&#24418;&#22330;&#26469;&#25191;&#34892;&#20960;&#20309;&#21464;&#24418;&#65292;&#22914;&#26059;&#36716;&#21644;&#25289;&#20280;&#65292;&#36890;&#36807;&#25197;&#26354;&#29983;&#25104;&#30340;&#22806;&#35266;&#26469;&#33719;&#21462;&#26368;&#32456;&#30340;&#22270;&#20687;&#25110;&#35270;&#39057;&#24207;&#21015;&#12290;&#20004;&#20010;&#29983;&#25104;&#22120;&#25509;&#25910;&#29420;&#31435;&#30340;&#28508;&#22312;&#21521;&#37327;&#20316;&#20026;&#36755;&#20837;&#65292;&#20174;&#22270;&#20687;&#25110;&#35270;&#39057;&#24207;&#21015;&#20013;&#35299;&#32806;&#22806;&#35266;&#21644;&#20960;&#20309;&#20449;&#24687;&#12290;&#23545;&#20110;&#35270;&#39057;&#25968;&#25454;&#65292;&#24341;&#20837;&#38750;&#32447;&#24615;&#36716;&#25442;&#27169;&#22411;&#21040;&#22806;&#35266;&#21644;&#20960;&#20309;&#29983;&#25104;&#22120;&#20013;&#65292;&#20197;&#25429;&#25417;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#21160;&#24577;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#26696;&#26159;&#36890;&#29992;&#30340;&#65292;&#21487;&#20197;&#36731;&#26494;&#38598;&#25104;&#21040;&#19981;&#21516;&#30340;&#29983;&#25104;&#27169;&#22411;&#20013;&#12290;&#22823;&#37327;&#30340;&#23450;&#24615;&#21644;&#23450;&#37327;&#23454;&#39564;&#34920;&#26126;&#22806;&#35266;&#21644;&#20960;&#20309;&#20449;&#24687;&#21487;&#20197;&#25104;&#21151;&#35299;&#32806;&#65292;&#24182;&#19988;&#33021;&#22815;&#26377;&#25928;&#22320;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#22270;&#20687;&#21644;&#35270;&#39057;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a deformable generator model to disentangle the appearance and geometric information for both image and video data in a purely unsupervised manner. The appearance generator network models the information related to appearance, including color, illumination, identity or category, while the geometric generator performs geometric warping, such as rotation and stretching, through generating deformation field which is used to warp the generated appearance to obtain the final image or video sequences. Two generators take independent latent vectors as input to disentangle the appearance and geometric information from image or video sequences. For video data, a nonlinear transition model is introduced to both the appearance and geometric generators to capture the dynamics over time. The proposed scheme is general and can be easily integrated into different generative models. An extensive set of qualitative and quantitative experiments shows that the appearance and geometric informat
&lt;/p&gt;</description></item></channel></rss>