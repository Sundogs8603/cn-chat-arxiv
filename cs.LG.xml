<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31283;&#23450;&#25193;&#25955;&#30340;&#28508;&#22312;&#25193;&#25955;&#26041;&#27861;&#65292;&#29992;&#20110;&#20840;&#26223;&#20998;&#21106;&#21644;&#36974;&#32617;&#20462;&#22797;&#65292;&#36890;&#36807;&#31616;&#21270;&#26550;&#26500;&#26469;&#36991;&#20813;&#22797;&#26434;&#24615;&#65292;&#23454;&#29616;&#20102;&#29983;&#25104;&#27169;&#22411;&#35299;&#38145;&#36974;&#32617;&#20462;&#22797;&#21151;&#33021;&#65292;&#20855;&#26377;&#24212;&#29992;&#20110;&#20132;&#20114;&#24335;&#20998;&#21106;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.10227</link><description>&lt;p&gt;
&#19968;&#20010;&#31616;&#21333;&#30340;&#28508;&#22312;&#25193;&#25955;&#26041;&#27861;&#24212;&#29992;&#20110;&#20840;&#26223;&#20998;&#21106;&#21644;&#36974;&#32617;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
A Simple Latent Diffusion Approach for Panoptic Segmentation and Mask Inpainting. (arXiv:2401.10227v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10227
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31283;&#23450;&#25193;&#25955;&#30340;&#28508;&#22312;&#25193;&#25955;&#26041;&#27861;&#65292;&#29992;&#20110;&#20840;&#26223;&#20998;&#21106;&#21644;&#36974;&#32617;&#20462;&#22797;&#65292;&#36890;&#36807;&#31616;&#21270;&#26550;&#26500;&#26469;&#36991;&#20813;&#22797;&#26434;&#24615;&#65292;&#23454;&#29616;&#20102;&#29983;&#25104;&#27169;&#22411;&#35299;&#38145;&#36974;&#32617;&#20462;&#22797;&#21151;&#33021;&#65292;&#20855;&#26377;&#24212;&#29992;&#20110;&#20132;&#20114;&#24335;&#20998;&#21106;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#26223;&#21644;&#23454;&#20363;&#20998;&#21106;&#32593;&#32476;&#36890;&#24120;&#36890;&#36807;&#19987;&#38376;&#30340;&#30446;&#26631;&#26816;&#27979;&#27169;&#22359;&#65292;&#22797;&#26434;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#29305;&#27530;&#30340;&#21518;&#22788;&#29702;&#27493;&#39588;&#26469;&#35757;&#32451;&#65292;&#20197;&#22788;&#29702;&#23454;&#20363;&#36974;&#32617;&#30340;&#32622;&#25442;&#19981;&#21464;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Panoptic and instance segmentation networks are often trained with specialized object detection modules, complex loss functions, and ad-hoc post-processing steps to handle the permutation-invariance of the instance masks. This work builds upon Stable Diffusion and proposes a latent diffusion approach for panoptic segmentation, resulting in a simple architecture which omits these complexities. Our training process consists of two steps: (1) training a shallow autoencoder to project the segmentation masks to latent space; (2) training a diffusion model to allow image-conditioned sampling in latent space. The use of a generative model unlocks the exploration of mask completion or inpainting, which has applications in interactive segmentation. The experimental validation yields promising results for both panoptic segmentation and mask inpainting. While not setting a new state-of-the-art, our model's simplicity, generality, and mask completion capability are desirable properties.
&lt;/p&gt;</description></item><item><title>ChatQA&#26159;&#19968;&#31995;&#21015;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;&#65292;&#21487;&#20197;&#36798;&#21040;GPT-4&#32423;&#21035;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#20004;&#38454;&#27573;&#30340;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;-shot&#23545;&#35805;&#38382;&#31572;&#20013;&#30340;&#32467;&#26524;&#12290;&#20351;&#29992;&#23494;&#38598;&#26816;&#32034;&#22120;&#36827;&#34892;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;&#24494;&#35843;&#21487;&#20197;&#23454;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;&#26597;&#35810;&#37325;&#20889;&#27169;&#22411;&#30456;&#24403;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#38477;&#20302;&#37096;&#32626;&#25104;&#26412;&#12290;ChatQA-70B&#22312;10&#20010;&#23545;&#35805;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#30340;&#24179;&#22343;&#24471;&#20998;&#36229;&#36807;&#20102;GPT-4&#65292;&#19988;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#26469;&#33258;OpenAI GPT&#27169;&#22411;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2401.10225</link><description>&lt;p&gt;
ChatQA: &#26500;&#24314;GPT-4&#32423;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ChatQA: Building GPT-4 Level Conversational QA Models. (arXiv:2401.10225v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10225
&lt;/p&gt;
&lt;p&gt;
ChatQA&#26159;&#19968;&#31995;&#21015;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;&#65292;&#21487;&#20197;&#36798;&#21040;GPT-4&#32423;&#21035;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#20004;&#38454;&#27573;&#30340;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;-shot&#23545;&#35805;&#38382;&#31572;&#20013;&#30340;&#32467;&#26524;&#12290;&#20351;&#29992;&#23494;&#38598;&#26816;&#32034;&#22120;&#36827;&#34892;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;&#24494;&#35843;&#21487;&#20197;&#23454;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;&#26597;&#35810;&#37325;&#20889;&#27169;&#22411;&#30456;&#24403;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#38477;&#20302;&#37096;&#32626;&#25104;&#26412;&#12290;ChatQA-70B&#22312;10&#20010;&#23545;&#35805;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#30340;&#24179;&#22343;&#24471;&#20998;&#36229;&#36807;&#20102;GPT-4&#65292;&#19988;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#26469;&#33258;OpenAI GPT&#27169;&#22411;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ChatQA&#65292;&#19968;&#31995;&#21015;&#20855;&#26377;GPT-4&#32423;&#21035;&#20934;&#30830;&#24615;&#30340;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#38646;-shot&#23545;&#35805;&#38382;&#31572;&#20013;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#22788;&#29702;&#23545;&#35805;&#38382;&#31572;&#20013;&#30340;&#26816;&#32034;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#22810;&#36718;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23494;&#38598;&#26816;&#32034;&#22120;&#30340;&#24494;&#35843;&#65292;&#36825;&#26679;&#21487;&#20197;&#25552;&#20379;&#19982;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#26597;&#35810;&#37325;&#20889;&#27169;&#22411;&#30456;&#24403;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#22823;&#22823;&#38477;&#20302;&#37096;&#32626;&#25104;&#26412;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;ChatQA-70B&#21487;&#20197;&#22312;10&#20010;&#23545;&#35805;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;&#24179;&#22343;&#20998;&#19978;&#36229;&#36807;GPT-4&#65288;54.14 vs. 53.90&#65289;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;OpenAI GPT&#27169;&#22411;&#30340;&#20219;&#20309;&#21512;&#25104;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we introduce ChatQA, a family of conversational question answering (QA) models, that obtain GPT-4 level accuracies. Specifically, we propose a two-stage instruction tuning method that can significantly improve the zero-shot conversational QA results from large language models (LLMs). To handle retrieval in conversational QA, we fine-tune a dense retriever on a multi-turn QA dataset, which provides comparable results to using the state-of-the-art query rewriting model while largely reducing deployment cost. Notably, our ChatQA-70B can outperform GPT-4 in terms of average score on 10 conversational QA datasets (54.14 vs. 53.90), without relying on any synthetic data from OpenAI GPT models.
&lt;/p&gt;</description></item><item><title>AutoFT&#26159;&#19968;&#31181;&#36890;&#36807;&#20248;&#21270;&#36229;&#21442;&#25968;&#22312;OOD&#25968;&#25454;&#19978;&#36827;&#34892;&#22522;&#30784;&#27169;&#22411;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#40065;&#26834;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.10220</link><description>&lt;p&gt;
AutoFT: &#36890;&#36807;&#20248;&#21270;OOD&#25968;&#25454;&#19978;&#30340;&#36229;&#21442;&#25968;&#23454;&#29616;&#40065;&#26834;&#30340;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
AutoFT: Robust Fine-Tuning by Optimizing Hyperparameters on OOD Data. (arXiv:2401.10220v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10220
&lt;/p&gt;
&lt;p&gt;
AutoFT&#26159;&#19968;&#31181;&#36890;&#36807;&#20248;&#21270;&#36229;&#21442;&#25968;&#22312;OOD&#25968;&#25454;&#19978;&#36827;&#34892;&#22522;&#30784;&#27169;&#22411;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#40065;&#26834;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#22312;&#29305;&#23450;&#20219;&#21153;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#26469;&#36866;&#24212;&#25152;&#38656;&#20219;&#21153;&#65292;&#20294;&#26159;&#22312;&#19968;&#31181;&#29305;&#23450;&#25968;&#25454;&#20998;&#24067;&#19978;&#24494;&#35843;&#27169;&#22411;&#24448;&#24448;&#20250;&#25439;&#23475;&#27169;&#22411;&#22312;&#20854;&#20182;&#20998;&#24067;&#19978;&#30340;&#21407;&#22987;&#24615;&#33021;&#12290;&#30446;&#21069;&#30340;&#40065;&#26834;&#24494;&#35843;&#26041;&#27861;&#21033;&#29992;&#25163;&#24037;&#21046;&#23450;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#26469;&#32422;&#26463;&#24494;&#35843;&#36807;&#31243;&#65292;&#20197;&#20445;&#30041;&#22522;&#30784;&#27169;&#22411;&#30340;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#24456;&#38590;&#20934;&#30830;&#25351;&#23450;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#24212;&#20445;&#30041;&#21738;&#20123;&#22522;&#30784;&#27169;&#22411;&#30340;&#29305;&#24449;&#65292;&#22240;&#20026;&#36825;&#21462;&#20915;&#20110;&#39044;&#35757;&#32451;&#12289;&#24494;&#35843;&#21644;&#35780;&#20272;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;AutoFT&#65292;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#26469;&#24341;&#23548;&#22522;&#30784;&#27169;&#22411;&#30340;&#24494;&#35843;&#12290;AutoFT&#36890;&#36807;&#20248;&#21270;&#24494;&#35843;&#36229;&#21442;&#25968;&#26469;&#22312;&#23567;&#30340;ODD&#39564;&#35777;&#38598;&#19978;&#26368;&#22823;&#21270;&#24615;&#33021;&#12290;&#20026;&#20102;&#20197;&#32454;&#31890;&#24230;&#30340;&#26041;&#24335;&#25351;&#23548;&#24494;&#35843;&#65292;AutoFT&#25628;&#32034;&#19968;&#20010;&#39640;&#24230;&#34920;&#36798;&#30340;&#36229;&#21442;&#25968;&#31354;&#38388;&#65292;&#20854;&#20013;&#21253;&#25324;&#35768;&#22810;&#19981;&#21516;&#30340;&#26435;&#37325;&#31995;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation models encode rich representations that can be adapted to a desired task by fine-tuning on task-specific data. However, fine-tuning a model on one particular data distribution often compromises the model's original performance on other distributions. Current methods for robust fine-tuning utilize hand-crafted regularization techniques to constrain the fine-tuning process towards the base foundation model. Yet, it is hard to precisely specify what characteristics of the foundation model to retain during fine-tuning, as this depends on how the pre-training, fine-tuning, and evaluation data distributions relate to each other. We propose AutoFT, a data-driven approach for guiding foundation model fine-tuning. AutoFT optimizes fine-tuning hyperparameters to maximize performance on a small out-of-distribution (OOD) validation set. To guide fine-tuning in a granular way, AutoFT searches a highly expressive hyperparameter space that includes weight coefficients for many different lo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;&#35745;&#31639;&#19981;&#21487;&#32422;&#34920;&#31034;&#24352;&#37327;&#31215;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#31561;&#21464;&#25805;&#20316;&#22522;&#30784;&#20174;&#29699;&#24418;&#35856;&#27874;&#25913;&#21464;&#20026;2D&#20613;&#31435;&#21494;&#22522;&#30784;&#65292;&#23454;&#29616;&#20102;&#23545;E(3)&#32676;&#30340;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#25928;&#24314;&#27169;&#12290;</title><link>http://arxiv.org/abs/2401.10216</link><description>&lt;p&gt;
&#36890;&#36807;Gaunt&#24352;&#37327;&#31215;&#22312;&#20613;&#37324;&#21494;&#22522;&#30784;&#19978;&#23454;&#29616;&#39640;&#25928;&#30340;&#31561;&#21464;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Enabling Efficient Equivariant Operations in the Fourier Basis via Gaunt Tensor Products. (arXiv:2401.10216v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10216
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;&#35745;&#31639;&#19981;&#21487;&#32422;&#34920;&#31034;&#24352;&#37327;&#31215;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#31561;&#21464;&#25805;&#20316;&#22522;&#30784;&#20174;&#29699;&#24418;&#35856;&#27874;&#25913;&#21464;&#20026;2D&#20613;&#31435;&#21494;&#22522;&#30784;&#65292;&#23454;&#29616;&#20102;&#23545;E(3)&#32676;&#30340;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#25928;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24314;&#27169;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#30340;3D&#25968;&#25454;&#26102;&#65292;&#21457;&#23637;E(3)&#32676;&#30340;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#23454;&#29616;&#36825;&#31181;&#31561;&#21464;&#24615;&#20027;&#35201;&#28041;&#21450;&#21040;&#19981;&#21487;&#32422;&#34920;&#31034;&#65288;irreps&#65289;&#30340;&#24352;&#37327;&#31215;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#20351;&#29992;&#39640;&#38454;&#24352;&#37327;&#65292;&#36825;&#20123;&#25805;&#20316;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#26174;&#33879;&#22686;&#21152;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26041;&#27861;&#26469;&#22823;&#22823;&#21152;&#36895;&#19981;&#21487;&#32422;&#34920;&#31034;&#30340;&#24352;&#37327;&#31215;&#30340;&#35745;&#31639;&#12290;&#25105;&#20204;&#23558;&#24120;&#29992;&#30340;Clebsch-Gordan&#31995;&#25968;&#19982;Gaunt&#31995;&#25968;&#36827;&#34892;&#20102;&#25968;&#23398;&#19978;&#30340;&#36830;&#25509;&#65292;Gaunt&#31995;&#25968;&#26159;&#19977;&#20010;&#29699;&#24418;&#35856;&#27874;&#20056;&#31215;&#30340;&#31215;&#20998;&#12290;&#36890;&#36807;Gaunt&#31995;&#25968;&#65292;&#19981;&#21487;&#32422;&#34920;&#31034;&#30340;&#24352;&#37327;&#31215;&#31561;&#20215;&#20110;&#30001;&#29699;&#24418;&#35856;&#27874;&#34920;&#31034;&#30340;&#29699;&#24418;&#20989;&#25968;&#20043;&#38388;&#30340;&#20056;&#27861;&#12290;&#36825;&#31181;&#35266;&#28857;&#36827;&#19968;&#27493;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#31561;&#21464;&#25805;&#20316;&#30340;&#22522;&#30784;&#20174;&#29699;&#24418;&#35856;&#27874;&#25913;&#21464;&#20026;2D&#20613;&#31435;&#21494;&#22522;&#30784;&#12290;&#22240;&#27492;&#65292;&#29699;&#24418;&#20989;&#25968;&#20043;&#38388;&#30340;&#20056;&#27861;&#21487;&#20197;&#22312;&#20613;&#31435;&#21494;&#22522;&#30784;&#19978;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing equivariant neural networks for the E(3) group plays an important role in modeling 3D data across real-world applications. Enforcing this equivariance primarily involves the tensor products of irreducible representations (irreps). However, the computational complexity of such operations increases significantly as higher-order tensors are used. In this work, we propose a systematic approach to substantially accelerate the computation of the tensor products of irreps. We mathematically connect the commonly used Clebsch-Gordan coefficients to the Gaunt coefficients, which are integrals of products of three spherical harmonics. Through Gaunt coefficients, the tensor product of irreps becomes equivalent to the multiplication between spherical functions represented by spherical harmonics. This perspective further allows us to change the basis for the equivariant operations from spherical harmonics to a 2D Fourier basis. Consequently, the multiplication between spherical functions 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#25913;&#36827;&#30340;&#33258;&#21160;&#26816;&#27979;&#39550;&#39542;&#21592;&#30130;&#21171;&#21644;&#27880;&#24847;&#21147;&#20998;&#25955;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#22522;&#20110;&#35270;&#35273;&#21644;&#38754;&#37096;&#23545;&#40784;&#32593;&#32476;&#30340;&#39550;&#39542;&#30130;&#21171;&#26816;&#27979;&#20197;&#21450;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#27880;&#24847;&#21147;&#20998;&#25955;&#34892;&#20026;&#35782;&#21035;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.10213</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25913;&#36827;&#33258;&#21160;&#26816;&#27979;&#39550;&#39542;&#21592;&#30130;&#21171;&#21644;&#27880;&#24847;&#21147;&#20998;&#25955;
&lt;/p&gt;
&lt;p&gt;
Improving automatic detection of driver fatigue and distraction using machine learning. (arXiv:2401.10213v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#25913;&#36827;&#30340;&#33258;&#21160;&#26816;&#27979;&#39550;&#39542;&#21592;&#30130;&#21171;&#21644;&#27880;&#24847;&#21147;&#20998;&#25955;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#22522;&#20110;&#35270;&#35273;&#21644;&#38754;&#37096;&#23545;&#40784;&#32593;&#32476;&#30340;&#39550;&#39542;&#30130;&#21171;&#26816;&#27979;&#20197;&#21450;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#27880;&#24847;&#21147;&#20998;&#25955;&#34892;&#20026;&#35782;&#21035;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20449;&#24687;&#25216;&#26415;&#30340;&#21464;&#21270;&#21644;&#36827;&#27493;&#22312;&#26234;&#33021;&#27773;&#36710;&#31995;&#32479;&#30340;&#21457;&#23637;&#20013;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#39550;&#39542;&#21592;&#30130;&#21171;&#21644;&#27880;&#24847;&#21147;&#20998;&#25955;&#26159;&#20132;&#36890;&#20107;&#25925;&#20013;&#30340;&#37325;&#35201;&#22240;&#32032;&#12290;&#22240;&#27492;&#65292;&#23545;&#39550;&#39542;&#34892;&#20026;&#36827;&#34892;&#36710;&#20869;&#30417;&#27979;&#24050;&#25104;&#20026;&#26234;&#33021;&#27773;&#36710;&#39640;&#32423;&#39550;&#39542;&#36741;&#21161;&#31995;&#32479;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20351;&#29992;&#22522;&#20110;&#35270;&#35273;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21516;&#26102;&#26816;&#27979;&#30130;&#21171;&#21644;&#27880;&#24847;&#21147;&#20998;&#25955;&#34892;&#20026;&#30340;&#25216;&#26415;&#12290;&#22312;&#39550;&#39542;&#30130;&#21171;&#26816;&#27979;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#38754;&#37096;&#23545;&#40784;&#32593;&#32476;&#26469;&#35782;&#21035;&#22270;&#20687;&#20013;&#30340;&#38754;&#37096;&#29305;&#24449;&#28857;&#65292;&#24182;&#35745;&#31639;&#38754;&#37096;&#29305;&#24449;&#28857;&#30340;&#36317;&#31163;&#26469;&#26816;&#27979;&#30524;&#30555;&#21644;&#22068;&#24052;&#30340;&#24320;&#38381;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;MobileNet&#26550;&#26500;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#26469;&#35782;&#21035;&#21508;&#31181;&#27880;&#24847;&#21147;&#20998;&#25955;&#34892;&#20026;&#12290;&#22312;&#22522;&#20110;PC&#24179;&#21488;&#30340;&#25668;&#20687;&#22836;&#35774;&#32622;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#20351;&#29992;publ&#32467;&#26524;&#36827;&#34892;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Changes and advances in information technology have played an important role in the development of intelligent vehicle systems in recent years. Driver fatigue and distracted driving are important factors in traffic accidents. Thus, onboard monitoring of driving behavior has become a crucial component of advanced driver assistance systems for intelligent vehicles. In this article, we present techniques for simultaneously detecting fatigue and distracted driving behaviors using vision-based and machine learning-based approaches. In driving fatigue detection, we use facial alignment networks to identify facial feature points in the images, and calculate the distance of the facial feature points to detect the opening and closing of the eyes and mouth. Furthermore, we use a convolutional neural network (CNN) based on the MobileNet architecture to identify various distracted driving behaviors. Experiments are performed on a PC based setup with a webcam and results are demonstrated using publ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22810;&#31890;&#24230;&#32467;&#26500;&#21644;&#22810;&#23610;&#24230;&#24207;&#21015;&#34920;&#31034;&#32806;&#21512;&#30340;PTM&#20301;&#28857;&#39044;&#27979;&#26041;&#27861;PTM-CMGMS&#65292;&#35813;&#26041;&#27861;&#22312;&#32467;&#26500;&#34920;&#31034;&#23398;&#20064;&#21644;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#19978;&#36827;&#34892;&#20248;&#21270;&#65292;&#25552;&#39640;&#20102;PTM&#20301;&#28857;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.10211</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#31890;&#24230;&#32467;&#26500;&#21644;&#22810;&#23610;&#24230;&#24207;&#21015;&#34920;&#31034;&#30340;&#32806;&#21512;&#25913;&#36827;PTM&#20301;&#28857;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Improving PTM Site Prediction by Coupling of Multi-Granularity Structure and Multi-Scale Sequence Representation. (arXiv:2401.10211v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10211
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22810;&#31890;&#24230;&#32467;&#26500;&#21644;&#22810;&#23610;&#24230;&#24207;&#21015;&#34920;&#31034;&#32806;&#21512;&#30340;PTM&#20301;&#28857;&#39044;&#27979;&#26041;&#27861;PTM-CMGMS&#65292;&#35813;&#26041;&#27861;&#22312;&#32467;&#26500;&#34920;&#31034;&#23398;&#20064;&#21644;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#19978;&#36827;&#34892;&#20248;&#21270;&#65292;&#25552;&#39640;&#20102;PTM&#20301;&#28857;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34507;&#30333;&#36136;&#32763;&#35793;&#21518;&#20462;&#39280;&#65288;PTM&#65289;&#20301;&#28857;&#39044;&#27979;&#26159;&#29983;&#29289;&#20449;&#24687;&#23398;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#24050;&#32463;&#24320;&#21457;&#20102;&#20960;&#31181;&#35745;&#31639;&#26041;&#27861;&#26469;&#39044;&#27979;PTM&#20301;&#28857;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24573;&#30053;&#20102;&#32467;&#26500;&#20449;&#24687;&#65292;&#20165;&#21033;&#29992;&#34507;&#30333;&#36136;&#24207;&#21015;&#12290;&#27492;&#22806;&#65292;PTM&#26159;&#21457;&#29983;&#22312;&#21407;&#23376;&#31890;&#24230;&#30340;&#29983;&#29289;&#20107;&#20214;&#65292;&#25152;&#20197;&#36843;&#20999;&#38656;&#35201;&#35774;&#35745;&#19968;&#31181;&#26356;&#31934;&#32454;&#30340;&#32467;&#26500;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22810;&#31890;&#24230;&#32467;&#26500;&#21644;&#22810;&#23610;&#24230;&#24207;&#21015;&#34920;&#31034;&#32806;&#21512;&#30340;PTM&#20301;&#28857;&#39044;&#27979;&#26041;&#27861;&#65292;&#31616;&#31216;PTM-CMGMS&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#22810;&#31890;&#24230;&#32467;&#26500;&#24863;&#30693;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#20174;AlphaFold&#39044;&#27979;&#30340;&#32467;&#26500;&#20013;&#23398;&#20064;&#27688;&#22522;&#37240;&#12289;&#21407;&#23376;&#21644;&#25972;&#20010;&#34507;&#30333;&#36136;&#30340;&#37051;&#22495;&#32467;&#26500;&#34920;&#31034;&#65292;&#28982;&#21518;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#20248;&#21270;&#32467;&#26500;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20351;&#29992;&#20102;&#22810;&#23610;&#24230;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#26469;&#25552;&#21462;&#19978;&#19979;&#25991;&#24207;&#21015;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Protein post-translational modification (PTM) site prediction is a fundamental task in bioinformatics. Several computational methods have been developed to predict PTM sites. However, existing methods ignore the structure information and merely utilize protein sequences. Furthermore, designing a more fine-grained structure representation learning method is urgently needed as PTM is a biological event that occurs at the atom granularity. In this paper, we propose a PTM site prediction method by Coupling of Multi-Granularity structure and Multi-Scale sequence representation, PTM-CMGMS for brevity. Specifically, multigranularity structure-aware representation learning is designed to learn neighborhood structure representations at the amino acid, atom, and whole protein granularity from AlphaFold predicted structures, followed by utilizing contrastive learning to optimize the structure representations.Additionally, multi-scale sequence representation learning is used to extract context seq
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31934;&#36890;&#25351;&#23548;&#30340;&#38750;&#21442;&#25968;&#32858;&#31867;&#26041;&#27861;&#65292;&#39044;&#27979;&#23398;&#29983;&#22312;&#38382;&#39064;&#35299;&#20915;&#20013;&#21487;&#33021;&#37319;&#29992;&#30340;&#31574;&#30053;&#65292;&#20174;&#32780;&#23454;&#29616;&#33258;&#36866;&#24212;&#25945;&#23398;&#31995;&#32479;&#30340;&#20010;&#24615;&#21270;&#20307;&#39564;&#12290;</title><link>http://arxiv.org/abs/2401.10210</link><description>&lt;p&gt;
&#36890;&#36807;&#31934;&#36890;&#25351;&#23548;&#30340;&#38750;&#21442;&#25968;&#32858;&#31867;&#26469;&#25193;&#22823;&#31574;&#30053;&#39044;&#27979;&#35268;&#27169;
&lt;/p&gt;
&lt;p&gt;
Mastery Guided Non-parametric Clustering to Scale-up Strategy Prediction. (arXiv:2401.10210v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10210
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31934;&#36890;&#25351;&#23548;&#30340;&#38750;&#21442;&#25968;&#32858;&#31867;&#26041;&#27861;&#65292;&#39044;&#27979;&#23398;&#29983;&#22312;&#38382;&#39064;&#35299;&#20915;&#20013;&#21487;&#33021;&#37319;&#29992;&#30340;&#31574;&#30053;&#65292;&#20174;&#32780;&#23454;&#29616;&#33258;&#36866;&#24212;&#25945;&#23398;&#31995;&#32479;&#30340;&#20010;&#24615;&#21270;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#23398;&#29983;&#22312;&#35299;&#20915;&#38382;&#39064;&#26102;&#21487;&#33021;&#20351;&#29992;&#30340;&#31574;&#30053;&#65288;&#27010;&#24565;&#24207;&#21015;&#65289;&#26377;&#21161;&#20110;&#33258;&#36866;&#24212;&#25945;&#23398;&#31995;&#32479;&#65288;AISs&#65289;&#26681;&#25454;&#20182;&#20204;&#30340;&#23398;&#20064;&#33021;&#21147;&#26356;&#22909;&#22320;&#36866;&#24212;&#19981;&#21516;&#31867;&#22411;&#30340;&#23398;&#20064;&#32773;&#12290;&#36825;&#21487;&#20197;&#20026;&#23398;&#29983;&#25552;&#20379;&#26356;&#21160;&#24577;&#12289;&#26377;&#36259;&#21644;&#20010;&#24615;&#21270;&#30340;&#23398;&#20064;&#20307;&#39564;&#12290;&#20026;&#20102;&#25193;&#22823;&#35757;&#32451;&#19968;&#20010;&#21487;&#20197;&#35206;&#30422;&#22823;&#35268;&#27169;&#25945;&#32946;&#25968;&#25454;&#38598;&#30340;&#39044;&#27979;&#27169;&#22411;&#65288;&#22914;LSTMs&#65289;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#38750;&#21442;&#25968;&#26041;&#27861;&#26469;&#23545;&#25968;&#25454;&#20013;&#30340;&#23545;&#31216;&#23454;&#20363;&#36827;&#34892;&#32858;&#31867;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#22522;&#20110;Node2Vec&#30340;&#34920;&#31034;&#23398;&#20064;&#23558;&#25484;&#25569;&#25110;&#25216;&#33021;&#27700;&#24179;&#19978;&#30340;&#23545;&#31216;&#24615;&#32534;&#30721;&#20026;&#31574;&#30053;&#65292;&#22240;&#20026;&#35299;&#20915;&#38382;&#39064;&#26102;&#65292;&#23398;&#29983;&#30340;&#31574;&#30053;&#24456;&#21487;&#33021;&#28041;&#21450;&#20182;&#20204;&#24050;&#32463;&#25484;&#25569;&#30340;&#27010;&#24565;&#12290;&#21033;&#29992;&#36825;&#31181;&#34920;&#31034;&#65292;&#25105;&#20204;&#20351;&#29992;DP-Means&#36890;&#36807;&#23545;&#32858;&#31867;&#30340;&#31895;&#32454;&#35843;&#25972;&#26469;&#23545;&#23545;&#31216;&#23454;&#20363;&#36827;&#34892;&#20998;&#32452;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#27169;&#22411;&#24212;&#29992;&#21040;&#20174;MATHia&#65288;&#19968;&#23478;&#20013;&#23398;&#25968;&#23398;&#23398;&#20064;&#39046;&#20808;&#30340;AIS&#65289;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#25968;&#23398;&#23398;&#20064;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting the strategy (sequence of concepts) that a student is likely to use in problem-solving helps Adaptive Instructional Systems (AISs) better adapt themselves to different types of learners based on their learning abilities. This can lead to a more dynamic, engaging, and personalized experience for students. To scale up training a prediction model (such as LSTMs) over large-scale education datasets, we develop a non-parametric approach to cluster symmetric instances in the data. Specifically, we learn a representation based on Node2Vec that encodes symmetries over mastery or skill level since, to solve a problem, it is natural that a student's strategy is likely to involve concepts in which they have gained mastery. Using this representation, we use DP-Means to group symmetric instances through a coarse-to-fine refinement of the clusters. We apply our model to learn strategies for Math learning from large-scale datasets from MATHia, a leading AIS for middle-school math learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#35299;&#37322;&#24615;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#30340;&#25240;&#34935;&#35268;&#21017;&#25552;&#21462;&#65292;&#26088;&#22312;&#35299;&#20915;&#40657;&#30418;&#35299;&#37322;&#22120;&#30340;&#19981;&#21487;&#20449;&#20219;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.10207</link><description>&lt;p&gt;
&#29992;&#20110;&#35299;&#37322;&#24615;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#30340;&#25240;&#34935;&#35268;&#21017;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Eclectic Rule Extraction for Explainability of Deep Neural Network based Intrusion Detection Systems. (arXiv:2401.10207v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#35299;&#37322;&#24615;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#30340;&#25240;&#34935;&#35268;&#21017;&#25552;&#21462;&#65292;&#26088;&#22312;&#35299;&#20915;&#40657;&#30418;&#35299;&#37322;&#22120;&#30340;&#19981;&#21487;&#20449;&#20219;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#40657;&#30418;&#31639;&#27861;&#21644;&#20195;&#29702;&#35299;&#37322;&#22120;&#22312;&#21487;&#35299;&#37322;&#24615;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;(X-IDS)&#20013;&#25152;&#24341;&#21457;&#30340;&#20449;&#20219;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#12290;&#34429;&#28982;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;(XAI)&#26088;&#22312;&#25552;&#39640;&#36879;&#26126;&#24230;&#65292;&#20294;&#40657;&#30418;&#20195;&#29702;&#35299;&#37322;&#22120;&#65292;&#22914;&#23616;&#37096;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#26080;&#20851;&#35299;&#37322;(LIME)&#21644;SHapley&#21152;&#27861;&#35299;&#37322;(SHAP)&#65292;&#24456;&#38590;&#20449;&#20219;&#12290;&#36825;&#20123;&#20195;&#29702;&#35299;&#37322;&#22120;&#30340;&#40657;&#30418;&#29305;&#24615;&#20351;&#24471;&#35299;&#37322;&#29983;&#25104;&#30340;&#36807;&#31243;&#19981;&#36879;&#26126;&#19988;&#38590;&#20197;&#29702;&#35299;&#12290;&#20026;&#20102;&#36991;&#20813;&#36825;&#20010;&#38382;&#39064;&#65292;&#21487;&#20197;&#20351;&#29992;&#36879;&#26126;&#30340;&#30333;&#30418;&#31639;&#27861;&#65292;&#22914;&#35268;&#21017;&#25552;&#21462;(RE)&#12290;&#35268;&#21017;&#25552;&#21462;&#26377;&#19977;&#31181;&#31867;&#22411;&#30340;&#31639;&#27861;:&#25945;&#32946;&#12289;&#20998;&#35299;&#21644;&#25240;&#34935;&#12290;&#25945;&#32946;&#26041;&#27861;&#25552;&#20379;&#24555;&#36895;&#20294;&#19981;&#21487;&#20449;&#36182;&#30340;&#30333;&#30418;&#35299;&#37322;&#65292;&#32780;&#20998;&#35299;&#35268;&#21017;&#25552;&#21462;&#25552;&#20379;&#20102;&#21487;&#20449;&#36182;&#20294;&#21487;&#25193;&#23637;&#24615;&#36739;&#24046;&#30340;&#35299;&#37322;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#25240;&#34935;&#35268;&#21017;&#25552;&#21462;&#65292;&#23427;&#22312;&#21487;&#25193;&#23637;&#24615;&#21644;&#21487;&#20449;&#36182;&#24615;&#20043;&#38388;&#36798;&#21040;&#20102;&#24179;&#34913;&#12290;&#36890;&#36807;&#32508;&#21512;&#19981;&#21516;&#30340;&#25216;&#26415;&#26041;&#27861;&#65292;
&lt;/p&gt;
&lt;p&gt;
This paper addresses trust issues created from the ubiquity of black box algorithms and surrogate explainers in Explainable Intrusion Detection Systems (X-IDS). While Explainable Artificial Intelligence (XAI) aims to enhance transparency, black box surrogate explainers, such as Local Interpretable Model-Agnostic Explanation (LIME) and SHapley Additive exPlanation (SHAP), are difficult to trust. The black box nature of these surrogate explainers makes the process behind explanation generation opaque and difficult to understand. To avoid this problem, one can use transparent white box algorithms such as Rule Extraction (RE). There are three types of RE algorithms: pedagogical, decompositional, and eclectic. Pedagogical methods offer fast but untrustworthy white-box explanations, while decompositional RE provides trustworthy explanations with poor scalability. This work explores eclectic rule extraction, which strikes a balance between scalability and trustworthiness. By combining techniq
&lt;/p&gt;</description></item><item><title>&#36830;&#32493;&#23398;&#20064;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SEED&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#35757;&#32451;&#26368;&#20248;&#30340;&#19987;&#23478;&#26469;&#35299;&#20915;&#36951;&#24536;&#21644;&#35745;&#31639;&#36127;&#25285;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.10191</link><description>&lt;p&gt;
&#20998;&#32780;&#19981;&#24536;&#65306;&#36830;&#32493;&#23398;&#20064;&#20013;&#36873;&#25321;&#24615;&#35757;&#32451;&#19987;&#23478;&#30340;&#38598;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Divide and not forget: Ensemble of selectively trained experts in Continual Learning. (arXiv:2401.10191v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10191
&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SEED&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#35757;&#32451;&#26368;&#20248;&#30340;&#19987;&#23478;&#26469;&#35299;&#20915;&#36951;&#24536;&#21644;&#35745;&#31639;&#36127;&#25285;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31867;&#22686;&#37327;&#23398;&#20064;&#30340;&#27969;&#34892;&#65292;&#27169;&#22411;&#33021;&#22815;&#25299;&#23485;&#24212;&#29992;&#33539;&#22260;&#65292;&#21516;&#26102;&#19981;&#24536;&#35760;&#24050;&#32463;&#23398;&#21040;&#30340;&#30693;&#35782;&#12290;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#36235;&#21183;&#26159;&#20351;&#29992;&#28151;&#21512;&#19987;&#23478;&#25216;&#26415;&#65292;&#19981;&#21516;&#30340;&#27169;&#22411;&#20849;&#21516;&#35299;&#20915;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#19987;&#23478;&#36890;&#24120;&#20250;&#19968;&#27425;&#24615;&#20351;&#29992;&#25972;&#20010;&#20219;&#21153;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#26679;&#20250;&#22686;&#21152;&#36951;&#24536;&#30340;&#39118;&#38505;&#21644;&#35745;&#31639;&#36127;&#25285;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SEED&#30340;&#26032;&#26041;&#27861;&#12290;SEED&#20165;&#36873;&#25321;&#19968;&#20010;&#34987;&#35748;&#20026;&#26368;&#20248;&#30340;&#19987;&#23478;&#26469;&#22788;&#29702;&#32473;&#23450;&#30340;&#20219;&#21153;&#65292;&#24182;&#20351;&#29992;&#35813;&#20219;&#21153;&#30340;&#25968;&#25454;&#23545;&#36825;&#20010;&#19987;&#23478;&#36827;&#34892;&#24494;&#35843;&#12290;&#20026;&#27492;&#65292;&#27599;&#20010;&#19987;&#23478;&#29992;&#39640;&#26031;&#20998;&#24067;&#34920;&#31034;&#27599;&#20010;&#31867;&#21035;&#65292;&#24182;&#26681;&#25454;&#36825;&#20123;&#20998;&#24067;&#30340;&#30456;&#20284;&#24615;&#36873;&#25321;&#26368;&#20248;&#19987;&#23478;&#12290;&#22240;&#27492;&#65292;SEED&#22312;&#20445;&#25345;&#38598;&#25104;&#26041;&#27861;&#30340;&#39640;&#31283;&#23450;&#24615;&#30340;&#21516;&#26102;&#22686;&#21152;&#20102;&#19987;&#23478;&#20043;&#38388;&#30340;&#22810;&#26679;&#24615;&#21644;&#24322;&#36136;&#24615;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;SEED&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Class-incremental learning is becoming more popular as it helps models widen their applicability while not forgetting what they already know. A trend in this area is to use a mixture-of-expert technique, where different models work together to solve the task. However, the experts are usually trained all at once using whole task data, which makes them all prone to forgetting and increasing computational burden. To address this limitation, we introduce a novel approach named SEED. SEED selects only one, the most optimal expert for a considered task, and uses data from this task to fine-tune only this expert. For this purpose, each expert represents each class with a Gaussian distribution, and the optimal expert is selected based on the similarity of those distributions. Consequently, SEED increases diversity and heterogeneity within the experts while maintaining the high stability of this ensemble method. The extensive experiments demonstrate that SEED achieves state-of-the-art performan
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21551;&#21457;&#20110;Kaczmarz&#30340;&#26041;&#27861;&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#27874;&#20989;&#25968;&#30340;&#20248;&#21270;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#26368;&#23567;&#27493;&#38271;&#38543;&#26426;&#37325;&#26500;&#20248;&#21270;&#22120;(MinSR)&#21644;&#38543;&#26426;Kaczmarz&#26041;&#27861;&#65292;&#30456;&#23545;&#20110;&#20854;&#20182;&#26041;&#27861;&#22312;&#22810;&#20010;&#23567;&#21407;&#23376;&#21644;&#20998;&#23376;&#19978;&#34920;&#29616;&#26356;&#20248;&#12290;</title><link>http://arxiv.org/abs/2401.10190</link><description>&lt;p&gt;
&#19968;&#31181;&#21551;&#21457;&#20110;Kaczmarz&#30340;&#26041;&#27861;&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#27874;&#20989;&#25968;&#30340;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
A Kaczmarz-inspired approach to accelerate the optimization of neural network wavefunctions. (arXiv:2401.10190v1 [physics.comp-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10190
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21551;&#21457;&#20110;Kaczmarz&#30340;&#26041;&#27861;&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#27874;&#20989;&#25968;&#30340;&#20248;&#21270;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#26368;&#23567;&#27493;&#38271;&#38543;&#26426;&#37325;&#26500;&#20248;&#21270;&#22120;(MinSR)&#21644;&#38543;&#26426;Kaczmarz&#26041;&#27861;&#65292;&#30456;&#23545;&#20110;&#20854;&#20182;&#26041;&#27861;&#22312;&#22810;&#20010;&#23567;&#21407;&#23376;&#21644;&#20998;&#23376;&#19978;&#34920;&#29616;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21464;&#20998;&#33945;&#29305;&#21345;&#32599;&#26041;&#27861;&#20248;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#27874;&#20989;&#25968;&#24050;&#34987;&#35777;&#26126;&#22312;&#21407;&#23376;&#21644;&#23567;&#20998;&#23376;&#30340;&#30005;&#23376;&#32467;&#26500;&#26041;&#38754;&#20135;&#29983;&#39640;&#31934;&#24230;&#32467;&#26524;&#65292;&#20294;&#26159;&#20248;&#21270;&#36825;&#31181;&#27874;&#20989;&#25968;&#30340;&#39640;&#25104;&#26412;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#26356;&#22823;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Subsampled Projected-Increment Natural Gradient Descent (SPRING)&#20248;&#21270;&#22120;&#26469;&#20943;&#23569;&#36825;&#20010;&#29942;&#39048;&#12290;SPRING&#32467;&#21512;&#20102;&#20043;&#21069;&#24341;&#20837;&#30340;&#26368;&#23567;&#27493;&#38271;&#38543;&#26426;&#37325;&#26500;&#20248;&#21270;&#22120;(MinSR)&#21644;&#32463;&#20856;&#30340;&#38543;&#26426;Kaczmarz&#26041;&#27861;&#35299;&#20915;&#32447;&#24615;&#26368;&#23567;&#20108;&#20056;&#38382;&#39064;&#30340;&#24605;&#24819;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#22810;&#20010;&#23567;&#21407;&#23376;&#21644;&#20998;&#23376;&#19978;&#65292;SPRING&#20248;&#20110;MinSR&#21644;&#27969;&#34892;&#30340;Kronecker-Factored Approximate Curvature&#26041;&#27861;(KFAC)&#65292;&#21069;&#25552;&#26159;&#21508;&#31181;&#26041;&#27861;&#30340;&#23398;&#20064;&#29575;&#37117;&#32463;&#36807;&#20102;&#26368;&#20339;&#35843;&#25972;&#12290;&#20363;&#22914;&#65292;&#22312;&#27687;&#21407;&#23376;&#19978;&#65292;SPRING&#22312;&#22235;&#19975;&#27425;&#35757;&#32451;&#36845;&#20195;&#20043;&#21518;&#36798;&#21040;&#20102;&#21270;&#23398;&#31934;&#24230;&#65292;&#32780;MinSR&#21644;KFAC&#29978;&#33267;&#22312;&#19968;&#20010;&#23567;&#26102;&#21518;&#20063;&#26080;&#27861;&#20570;&#21040;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network wavefunctions optimized using the variational Monte Carlo method have been shown to produce highly accurate results for the electronic structure of atoms and small molecules, but the high cost of optimizing such wavefunctions prevents their application to larger systems. We propose the Subsampled Projected-Increment Natural Gradient Descent (SPRING) optimizer to reduce this bottleneck. SPRING combines ideas from the recently introduced minimum-step stochastic reconfiguration optimizer (MinSR) and the classical randomized Kaczmarz method for solving linear least-squares problems. We demonstrate that SPRING outperforms both MinSR and the popular Kronecker-Factored Approximate Curvature method (KFAC) across a number of small atoms and molecules, given that the learning rates of all methods are optimally tuned. For example, on the oxygen atom, SPRING attains chemical accuracy after forty thousand training iterations, whereas both MinSR and KFAC fail to do so even after one h
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Chem-FINESE&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#21270;&#23398;&#39046;&#22495;&#20013;&#32454;&#31890;&#24230;&#23569;&#26679;&#26412;&#23454;&#20307;&#25552;&#21462;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#23454;&#20307;&#25552;&#21462;&#22120;&#21644;&#33258;&#25105;&#39564;&#35777;&#27169;&#22359;&#26469;&#20174;&#36755;&#20837;&#21477;&#23376;&#20013;&#25552;&#21462;&#21629;&#21517;&#23454;&#20307;&#24182;&#37325;&#26500;&#21407;&#22987;&#36755;&#20837;&#21477;&#23376;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.10189</link><description>&lt;p&gt;
Chem-FINESE: &#36890;&#36807;&#25991;&#26412;&#37325;&#26500;&#39564;&#35777;&#32454;&#31890;&#24230;&#23569;&#26679;&#26412;&#23454;&#20307;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Chem-FINESE: Validating Fine-Grained Few-shot Entity Extraction through Text Reconstruction. (arXiv:2401.10189v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10189
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Chem-FINESE&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#21270;&#23398;&#39046;&#22495;&#20013;&#32454;&#31890;&#24230;&#23569;&#26679;&#26412;&#23454;&#20307;&#25552;&#21462;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#23454;&#20307;&#25552;&#21462;&#22120;&#21644;&#33258;&#25105;&#39564;&#35777;&#27169;&#22359;&#26469;&#20174;&#36755;&#20837;&#21477;&#23376;&#20013;&#25552;&#21462;&#21629;&#21517;&#23454;&#20307;&#24182;&#37325;&#26500;&#21407;&#22987;&#36755;&#20837;&#21477;&#23376;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21270;&#23398;&#39046;&#22495;&#20013;&#65292;&#32454;&#31890;&#24230;&#23569;&#26679;&#26412;&#23454;&#20307;&#25552;&#21462;&#38754;&#20020;&#20004;&#20010;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#19982;&#19968;&#33324;&#39046;&#22495;&#30340;&#23454;&#20307;&#25552;&#21462;&#20219;&#21153;&#30456;&#27604;&#65292;&#21270;&#23398;&#35770;&#25991;&#20013;&#30340;&#21477;&#23376;&#36890;&#24120;&#21253;&#21547;&#26356;&#22810;&#30340;&#23454;&#20307;&#12290;&#27492;&#22806;&#65292;&#23454;&#20307;&#25552;&#21462;&#27169;&#22411;&#36890;&#24120;&#38590;&#20197;&#25552;&#21462;&#38271;&#23614;&#31867;&#22411;&#30340;&#23454;&#20307;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#23569;&#26679;&#26412;&#23454;&#20307;&#25552;&#21462;&#26041;&#27861;Chem-FINESE&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;Chem-FINESE&#21253;&#21547;&#20004;&#20010;&#32452;&#20214;&#65306;&#19968;&#20010;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#23454;&#20307;&#25552;&#21462;&#22120;&#29992;&#20110;&#20174;&#36755;&#20837;&#21477;&#23376;&#20013;&#25552;&#21462;&#21629;&#21517;&#23454;&#20307;&#65292;&#20197;&#21450;&#19968;&#20010;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#33258;&#25105;&#39564;&#35777;&#27169;&#22359;&#29992;&#20110;&#20174;&#25552;&#21462;&#30340;&#23454;&#20307;&#20013;&#37325;&#26500;&#21407;&#22987;&#36755;&#20837;&#21477;&#23376;&#12290;&#21463;&#21040;&#19968;&#20010;&#22909;&#30340;&#23454;&#20307;&#25552;&#21462;&#31995;&#32479;&#38656;&#35201;&#24544;&#23454;&#25552;&#21462;&#23454;&#20307;&#30340;&#20107;&#23454;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#26032;&#33258;&#25105;&#39564;&#35777;&#27169;&#22359;&#21033;&#29992;&#23454;&#20307;&#25552;&#21462;&#32467;&#26524;&#26469;&#37325;&#26500;&#21407;&#22987;&#36755;&#20837;&#21477;&#23376;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#27604;&#25439;&#22833;&#26469;&#20943;&#23569;&#22312;&#25552;&#21462;&#36807;&#31243;&#20013;&#30340;&#36807;&#24230;&#22797;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-grained few-shot entity extraction in the chemical domain faces two unique challenges. First, compared with entity extraction tasks in the general domain, sentences from chemical papers usually contain more entities. Moreover, entity extraction models usually have difficulty extracting entities of long-tailed types. In this paper, we propose Chem-FINESE, a novel sequence-to-sequence (seq2seq) based few-shot entity extraction approach, to address these two challenges. Our Chem-FINESE has two components: a seq2seq entity extractor to extract named entities from the input sentence and a seq2seq self-validation module to reconstruct the original input sentence from extracted entities. Inspired by the fact that a good entity extraction system needs to extract entities faithfully, our new self-validation module leverages entity extraction results to reconstruct the original input sentence. Besides, we design a new contrastive loss to reduce excessive copying during the extraction proces
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#26234;&#33021;&#23478;&#23621;&#21644;&#21487;&#31359;&#25140;&#35774;&#22791;&#22522;&#20110;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#30340;&#24212;&#29992;&#39046;&#22495;&#20013;&#25152;&#37319;&#29992;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#36129;&#29486;&#21644;&#25361;&#25112;&#36827;&#34892;&#20998;&#31867;&#21644;&#21576;&#29616;&#65292;&#25552;&#20379;&#20102;&#38382;&#39064;-&#35299;&#20915;&#26041;&#26696;&#30340;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2401.10185</link><description>&lt;p&gt;
&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#20013;&#30340;&#36801;&#31227;&#23398;&#20064;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Transfer Learning in Human Activity Recognition: A Survey. (arXiv:2401.10185v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10185
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#26234;&#33021;&#23478;&#23621;&#21644;&#21487;&#31359;&#25140;&#35774;&#22791;&#22522;&#20110;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#30340;&#24212;&#29992;&#39046;&#22495;&#20013;&#25152;&#37319;&#29992;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#36129;&#29486;&#21644;&#25361;&#25112;&#36827;&#34892;&#20998;&#31867;&#21644;&#21576;&#29616;&#65292;&#25552;&#20379;&#20102;&#38382;&#39064;-&#35299;&#20915;&#26041;&#26696;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20256;&#24863;&#22120;&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#65288;HAR&#65289;&#26159;&#19968;&#20010;&#27963;&#36291;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#22240;&#20854;&#22312;&#26234;&#33021;&#29615;&#22659;&#12289;&#36741;&#21161;&#29983;&#27963;&#12289;&#20581;&#36523;&#21644;&#21307;&#30103;&#31561;&#26041;&#38754;&#30340;&#24212;&#29992;&#32780;&#33719;&#24471;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#31561;&#39046;&#22495;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20854;&#20013;&#26377;&#22823;&#37327;&#27880;&#37322;&#25968;&#25454;&#21487;&#29992;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22522;&#20110;&#20256;&#24863;&#22120;&#30340;HAR&#26469;&#35828;&#65292;&#22823;&#37327;&#27880;&#37322;&#25968;&#25454;&#26159;&#19981;&#21487;&#29992;&#30340;&#12290;&#27492;&#22806;&#65292;HAR&#25152;&#25191;&#34892;&#30340;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#22312;&#20256;&#24863;&#22120;&#27169;&#24577;&#12289;&#20998;&#31867;&#20219;&#21153;&#21644;&#30446;&#26631;&#29992;&#25143;&#26041;&#38754;&#23384;&#22312;&#24046;&#24322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36801;&#31227;&#23398;&#20064;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#37325;&#28857;&#20851;&#27880;&#26234;&#33021;&#23478;&#23621;&#21644;&#21487;&#31359;&#25140;&#35774;&#22791;&#22522;&#20110;HAR&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#24037;&#20316;&#25353;&#20854;&#36129;&#29486;&#21644;&#35299;&#20915;&#30340;&#25361;&#25112;&#36827;&#34892;&#20998;&#31867;&#21644;&#21576;&#29616;&#65292;&#20197;&#25552;&#20379;&#38382;&#39064;-&#35299;&#20915;&#26041;&#26696;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sensor-based human activity recognition (HAR) has been an active research area, owing to its applications in smart environments, assisted living, fitness, healthcare, etc. Recently, deep learning based end-to-end training has resulted in state-of-the-art performance in domains such as computer vision and natural language, where large amounts of annotated data are available. However, large quantities of annotated data are not available for sensor-based HAR. Moreover, the real-world settings on which the HAR is performed differ in terms of sensor modalities, classification tasks, and target users. To address this problem, transfer learning has been employed extensively. In this survey, we focus on these transfer learning methods in the application domains of smart home and wearables-based HAR. In particular, we provide a problem-solution perspective by categorizing and presenting the works in terms of their contributions and the challenges they address. We also present an updated view of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#24615;&#30340;OOD&#26816;&#27979;&#26041;&#27861;&#65292;&#21253;&#25324;&#22522;&#20110;&#34920;&#31034;&#21644;&#22522;&#20110;&#36923;&#36753;&#30340;&#26041;&#27861;&#12290;&#20854;&#20013;&#65292;&#22312;&#22522;&#20110;&#34920;&#31034;&#30340;&#26041;&#27861;&#20013;&#65292;&#36890;&#36807;&#38477;&#32500;&#25913;&#36827;&#20102;&#24615;&#33021;&#65292;&#22312;&#22522;&#20110;&#36923;&#36753;&#30340;&#26041;&#27861;&#20013;&#65292;&#36890;&#36807;&#35299;&#20915;&#19968;&#20010;&#26410;&#34987;&#27880;&#24847;&#21040;&#30340;&#32570;&#38519;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#36825;&#20123;&#26041;&#27861;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.10176</link><description>&lt;p&gt;
&#32508;&#21512;&#24615;&#30340;OOD&#26816;&#27979;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Comprehensive OOD Detection Improvements. (arXiv:2401.10176v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#24615;&#30340;OOD&#26816;&#27979;&#26041;&#27861;&#65292;&#21253;&#25324;&#22522;&#20110;&#34920;&#31034;&#21644;&#22522;&#20110;&#36923;&#36753;&#30340;&#26041;&#27861;&#12290;&#20854;&#20013;&#65292;&#22312;&#22522;&#20110;&#34920;&#31034;&#30340;&#26041;&#27861;&#20013;&#65292;&#36890;&#36807;&#38477;&#32500;&#25913;&#36827;&#20102;&#24615;&#33021;&#65292;&#22312;&#22522;&#20110;&#36923;&#36753;&#30340;&#26041;&#27861;&#20013;&#65292;&#36890;&#36807;&#35299;&#20915;&#19968;&#20010;&#26410;&#34987;&#27880;&#24847;&#21040;&#30340;&#32570;&#38519;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#36825;&#20123;&#26041;&#27861;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#22312;&#37325;&#22823;&#20915;&#31574;&#20013;&#30340;&#26085;&#30410;&#26222;&#21450;&#65292;&#35782;&#21035;&#25512;&#29702;&#25968;&#25454;&#26159;&#21542;&#36229;&#20986;&#27169;&#22411;&#39044;&#26399;&#30340;&#36755;&#20837;&#20998;&#24067;&#23545;&#20110;&#32473;&#20986;&#39044;&#27979;&#30340;&#32972;&#26223;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#20026;&#22522;&#20110;&#34920;&#31034;&#21644;&#22522;&#20110;&#36923;&#36753;&#30340;&#26041;&#27861;&#30340;OOD&#26816;&#27979;&#26041;&#27861;&#12290;&#19982;&#22823;&#22810;&#25968;&#35770;&#25991;&#21482;&#20851;&#27880;&#20854;&#20013;&#19968;&#32452;&#19981;&#21516;&#65292;&#25105;&#20204;&#21516;&#26102;&#35299;&#20915;&#20102;&#20004;&#20010;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#22522;&#20110;&#34920;&#31034;&#30340;&#26041;&#27861;&#20013;&#23545;&#29305;&#24449;&#23884;&#20837;&#36827;&#34892;&#38477;&#32500;&#65292;&#20197;&#25552;&#39640;&#26102;&#38388;&#24615;&#33021;&#21644;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DICE-COL&#65292;&#36825;&#26159;&#19968;&#31181;&#25913;&#36827;&#20102;&#27969;&#34892;&#30340;&#22522;&#20110;&#36923;&#36753;&#30340;&#26041;&#27861;Directed Sparsification (DICE)&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#19968;&#20010;&#26410;&#34987;&#27880;&#24847;&#21040;&#30340;&#32570;&#38519;&#12290;&#25105;&#20204;&#22312;OpenOODv1.5&#22522;&#20934;&#26694;&#26550;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#20204;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#24182;&#33719;&#24471;&#20102;&#26368;&#26032;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
As machine learning becomes increasingly prevalent in impactful decisions, recognizing when inference data is outside the model's expected input distribution is paramount for giving context to predictions. Out-of-distribution (OOD) detection methods have been created for this task. Such methods can be split into representation-based or logit-based methods from whether they respectively utilize the model's embeddings or predictions for OOD detection. In contrast to most papers which solely focus on one such group, we address both. We employ dimensionality reduction on feature embeddings in representation-based methods for both time speedups and improved performance. Additionally, we propose DICE-COL, a modification of the popular logit-based method Directed Sparsification (DICE) that resolves an unnoticed flaw. We demonstrate the effectiveness of our methods on the OpenOODv1.5 benchmark framework, where they significantly improve performance and set state-of-the-art results.
&lt;/p&gt;</description></item><item><title>DISTINQT&#26159;&#19968;&#31181;&#38754;&#21521;&#26410;&#26469;&#31227;&#21160;&#21644;&#26080;&#32447;&#32593;&#32476;&#30340;&#38544;&#31169;&#24863;&#30693;&#20998;&#24067;&#24335;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;QoS&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2401.10158</link><description>&lt;p&gt;
DISTINQT: &#19968;&#31181;&#38754;&#21521;&#26410;&#26469;&#31227;&#21160;&#21644;&#26080;&#32447;&#32593;&#32476;&#30340;&#20998;&#24067;&#24335;&#38544;&#31169;&#24863;&#30693;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;QoS&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
DISTINQT: A Distributed Privacy Aware Learning Framework for QoS Prediction for Future Mobile and Wireless Networks. (arXiv:2401.10158v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10158
&lt;/p&gt;
&lt;p&gt;
DISTINQT&#26159;&#19968;&#31181;&#38754;&#21521;&#26410;&#26469;&#31227;&#21160;&#21644;&#26080;&#32447;&#32593;&#32476;&#30340;&#38544;&#31169;&#24863;&#30693;&#20998;&#24067;&#24335;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;QoS&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
5G&#21644;6G&#20197;&#21518;&#30340;&#32593;&#32476;&#23558;&#25903;&#25345;&#20381;&#36182;&#19968;&#23450;&#26381;&#21153;&#36136;&#37327;&#65288;QoS&#65289;&#30340;&#26032;&#30340;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29992;&#20363;&#21644;&#24212;&#29992;&#31243;&#24207;&#12290;&#21450;&#26102;&#39044;&#27979;QoS&#23545;&#20110;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#65288;&#22914;&#36710;&#36742;&#36890;&#20449;&#65289;&#23588;&#20026;&#37325;&#35201;&#12290;&#23613;&#31649;&#30452;&#21040;&#26368;&#36817;&#65292;QoS&#39044;&#27979;&#19968;&#30452;&#30001;&#38598;&#20013;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#35299;&#20915;&#26041;&#26696;&#23436;&#25104;&#65292;&#20294;&#24050;&#32463;&#20986;&#29616;&#20102;&#19968;&#20123;&#38544;&#31169;&#12289;&#35745;&#31639;&#21644;&#36816;&#33829;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;&#26367;&#20195;&#26041;&#26696;&#24050;&#32463;&#20986;&#29616;&#65288;&#22914;&#20998;&#21106;&#23398;&#20064;&#12289;&#32852;&#37030;&#23398;&#20064;&#65289;&#65292;&#23558;&#22797;&#26434;&#24230;&#36739;&#20302;&#30340;AI&#20219;&#21153;&#20998;&#24067;&#22312;&#33410;&#28857;&#20043;&#38388;&#65292;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#32771;&#34385;&#21040;&#26410;&#26469;&#26080;&#32447;&#32593;&#32476;&#30340;&#24322;&#26500;&#24615;&#65292;&#24403;&#28041;&#21450;&#21487;&#25193;&#23637;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#26041;&#27861;&#26102;&#65292;&#20250;&#20986;&#29616;&#26032;&#30340;&#25361;&#25112;&#12290;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DISTINQT&#30340;&#38754;&#21521;QoS&#39044;&#27979;&#30340;&#38544;&#31169;&#24863;&#30693;&#20998;&#24067;&#24335;&#23398;&#20064;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Beyond 5G and 6G networks are expected to support new and challenging use cases and applications that depend on a certain level of Quality of Service (QoS) to operate smoothly. Predicting the QoS in a timely manner is of high importance, especially for safety-critical applications as in the case of vehicular communications. Although until recent years the QoS prediction has been carried out by centralized Artificial Intelligence (AI) solutions, a number of privacy, computational, and operational concerns have emerged. Alternative solutions have been surfaced (e.g. Split Learning, Federated Learning), distributing AI tasks of reduced complexity across nodes, while preserving the privacy of the data. However, new challenges rise when it comes to scalable distributed learning approaches, taking into account the heterogeneous nature of future wireless networks. The current work proposes DISTINQT, a privacy-aware distributed learning framework for QoS prediction. Our framework supports mult
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#28151;&#21512;&#26102;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HTVGNN&#65289;&#29992;&#20110;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#39044;&#23450;&#20041;&#22270;&#21644;&#33258;&#36866;&#24212;&#22270;&#30340;&#23398;&#20064;&#33021;&#21147;&#21463;&#38480;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.10155</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#22411;&#30340;&#28151;&#21512;&#26102;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
A novel hybrid time-varying graph neural network for traffic flow forecasting. (arXiv:2401.10155v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10155
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#28151;&#21512;&#26102;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HTVGNN&#65289;&#29992;&#20110;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#39044;&#23450;&#20041;&#22270;&#21644;&#33258;&#36866;&#24212;&#22270;&#30340;&#23398;&#20064;&#33021;&#21147;&#21463;&#38480;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#26102;&#20934;&#30830;&#30340;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#26159;&#30830;&#20445;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#39640;&#25928;&#36816;&#34892;&#30340;&#22522;&#30784;&#12290;&#22312;&#29616;&#26377;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#26041;&#27861;&#20013;&#65292;&#36890;&#24120;&#20351;&#29992;&#39044;&#23450;&#20041;&#30340;&#22270;&#26469;&#25551;&#36848;&#22478;&#24066;&#36947;&#36335;&#32593;&#32476;&#20013;&#19981;&#21516;&#20132;&#36890;&#33410;&#28857;&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#12290;&#28982;&#32780;&#65292;&#39044;&#23450;&#20041;&#22270;&#25551;&#36848;&#31354;&#38388;&#30456;&#20851;&#24615;&#30340;&#33021;&#21147;&#21463;&#38480;&#20110;&#20808;&#21069;&#30340;&#30693;&#35782;&#21644;&#22270;&#29983;&#25104;&#26041;&#27861;&#12290;&#23613;&#31649;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#23398;&#20064;&#30340;&#26102;&#21464;&#22270;&#33021;&#37096;&#20998;&#20811;&#26381;&#39044;&#23450;&#20041;&#22270;&#30340;&#32570;&#28857;&#65292;&#20294;&#29616;&#26377;&#33258;&#36866;&#24212;&#22270;&#30340;&#23398;&#20064;&#33021;&#21147;&#26377;&#38480;&#12290;&#20363;&#22914;&#65292;&#26102;&#21464;&#22270;&#19981;&#33021;&#20805;&#20998;&#25429;&#25417;&#20132;&#36890;&#27969;&#37327;&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#30340;&#28151;&#21512;&#26102;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HTVGNN&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-time and accurate traffic flow prediction is the foundation for ensuring the efficient operation of intelligent transportation systems.In existing traffic flow prediction methods based on graph neural networks (GNNs), pre-defined graphs were usually used to describe the spatial correlations of different traffic nodes in urban road networks. However, the ability of pre-defined graphs used to describe spatial correlation was limited by prior knowledge and graph generation methods. Although time-varying graphs based on data-driven learning can partially overcome the drawbacks of pre-defined graphs, the learning ability of existing adaptive graphs was limited. For example, time-varying graphs cannot adequately capture the inherent spatial correlations in traffic flow data.In order to solve these problems, we have proposed a hybrid time-varying graph neural network (HTVGNN) for traffic flow prediction.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#27169;&#25311;&#29615;&#22659;IPMSRL&#65292;&#23637;&#31034;&#20102;&#23558;&#33258;&#20027;&#32593;&#32476;&#38450;&#24481;&#24212;&#29992;&#20110;&#24037;&#19994;&#25511;&#21046;&#31995;&#32479;&#30340;&#28508;&#21147;&#65292;&#24182;&#25506;&#32034;&#20102;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22312;&#28023;&#19978;&#25805;&#20316;&#25216;&#26415;&#32593;&#32476;&#23433;&#20840;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.10149</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22312;&#28023;&#19978;&#25805;&#20316;&#25216;&#26415;&#32593;&#32476;&#23433;&#20840;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Reinforcement Learning for Maritime Operational Technology Cyber Security. (arXiv:2401.10149v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#27169;&#25311;&#29615;&#22659;IPMSRL&#65292;&#23637;&#31034;&#20102;&#23558;&#33258;&#20027;&#32593;&#32476;&#38450;&#24481;&#24212;&#29992;&#20110;&#24037;&#19994;&#25511;&#21046;&#31995;&#32479;&#30340;&#28508;&#21147;&#65292;&#24182;&#25506;&#32034;&#20102;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22312;&#28023;&#19978;&#25805;&#20316;&#25216;&#26415;&#32593;&#32476;&#23433;&#20840;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#23558;&#33258;&#20027;&#32593;&#32476;&#38450;&#24481;&#24212;&#29992;&#20110;&#24037;&#19994;&#25511;&#21046;&#31995;&#32479;&#30340;&#28508;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20934;&#29615;&#22659;&#65292;&#20197;&#36827;&#19968;&#27493;&#25506;&#32034;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22312;&#36825;&#20010;&#38382;&#39064;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#12290;&#23427;&#24341;&#20837;&#20102;&#19968;&#20010;&#27169;&#25311;&#29615;&#22659;IPMSRL&#65292;&#29992;&#20110;&#19968;&#20010;&#36890;&#29992;&#38598;&#25104;&#24179;&#21488;&#31649;&#29702;&#31995;&#32479;(IPMS)&#65292;&#24182;&#25506;&#32034;&#20102;&#22312;&#36890;&#29992;&#28023;&#19978;IPMS&#25805;&#20316;&#25216;&#26415;(OT)&#20013;&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#33258;&#20027;&#32593;&#32476;&#38450;&#24481;&#20915;&#31574;&#30340;&#26041;&#27861;&#12290;OT&#32593;&#32476;&#38450;&#24481;&#25514;&#26045;&#30456;&#23545;&#20110;&#20225;&#19994;IT&#26469;&#35828;&#36824;&#19981;&#22815;&#25104;&#29087;&#12290;&#36825;&#26159;&#30001;&#20110;OT&#22522;&#30784;&#35774;&#26045;&#30456;&#23545;&#33030;&#24369;&#65292;&#28304;&#33258;&#20110;&#20351;&#29992;&#20256;&#32479;&#31995;&#32479;&#12289;&#35774;&#35745;&#26102;&#30340;&#24037;&#31243;&#20551;&#35774;&#20197;&#21450;&#32570;&#20047;&#20840;&#38754;&#30340;&#29616;&#20195;&#23433;&#20840;&#25511;&#21046;&#25514;&#26045;&#12290;&#30001;&#20110;&#19981;&#26029;&#22686;&#38271;&#30340;&#32593;&#32476;&#25915;&#20987;&#22797;&#26434;&#24615;&#21644;&#20256;&#32479;IT&#20013;&#24515;&#32593;&#32476;&#38450;&#24481;&#35299;&#20915;&#26041;&#26696;&#30340;&#23616;&#38480;&#24615;&#65292;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#23384;&#22312;&#35768;&#22810;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper demonstrates the potential for autonomous cyber defence to be applied on industrial control systems and provides a baseline environment to further explore Multi-Agent Reinforcement Learning's (MARL) application to this problem domain. It introduces a simulation environment, IPMSRL, of a generic Integrated Platform Management System (IPMS) and explores the use of MARL for autonomous cyber defence decision-making on generic maritime based IPMS Operational Technology (OT). OT cyber defensive actions are less mature than they are for Enterprise IT. This is due to the relatively brittle nature of OT infrastructure originating from the use of legacy systems, design-time engineering assumptions, and lack of full-scale modern security controls. There are many obstacles to be tackled across the cyber landscape due to continually increasing cyber-attack sophistication and the limitations of traditional IT-centric cyber defence solutions. Traditional IT controls are rarely deployed on 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#29289;&#20307;&#20013;&#24515;&#21270;&#23398;&#20064;&#20013;&#26126;&#30830;&#35299;&#24320;&#24418;&#29366;&#21644;&#32441;&#29702;&#25104;&#20998;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#28508;&#22312;&#31354;&#38388;&#21010;&#20998;&#20026;&#20004;&#20010;&#19981;&#37325;&#21472;&#30340;&#23376;&#38598;&#65292;&#20351;&#24471;&#27169;&#22411;&#26356;&#21152;&#31283;&#23450;&#21644;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2401.10148</link><description>&lt;p&gt;
&#22312;&#29289;&#20307;&#20013;&#24515;&#21270;&#23398;&#20064;&#20013;&#26126;&#30830;&#35299;&#24320;&#30340;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Explicitly Disentangled Representations in Object-Centric Learning. (arXiv:2401.10148v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10148
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#29289;&#20307;&#20013;&#24515;&#21270;&#23398;&#20064;&#20013;&#26126;&#30830;&#35299;&#24320;&#24418;&#29366;&#21644;&#32441;&#29702;&#25104;&#20998;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#28508;&#22312;&#31354;&#38388;&#21010;&#20998;&#20026;&#20004;&#20010;&#19981;&#37325;&#21472;&#30340;&#23376;&#38598;&#65292;&#20351;&#24471;&#27169;&#22411;&#26356;&#21152;&#31283;&#23450;&#21644;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#21407;&#22987;&#35270;&#35273;&#25968;&#25454;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#34920;&#31034;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#19968;&#20010;&#37325;&#35201;&#19988;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#26368;&#36817;&#65292;&#26080;&#30417;&#30563;&#23398;&#20064;&#29289;&#20307;&#20013;&#24515;&#21270;&#34920;&#31034;&#30340;&#25216;&#26415;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#22686;&#24378;&#28508;&#22312;&#29305;&#24449;&#30340;&#31283;&#23450;&#24615;&#21487;&#20197;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#35757;&#32451;&#30340;&#25928;&#29575;&#21644;&#25928;&#26524;&#12290;&#22312;&#36825;&#20010;&#26041;&#21521;&#19978;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#27493;&#39588;&#26159;&#35299;&#24320;&#23548;&#33268;&#25968;&#25454;&#21464;&#21270;&#30340;&#22240;&#32032;&#12290;&#20808;&#21069;&#65292;&#19981;&#21464;&#21345;&#27133;&#27880;&#24847;&#23454;&#29616;&#20102;&#20174;&#20854;&#20182;&#29305;&#24449;&#20013;&#35299;&#24320;&#20301;&#32622;&#12289;&#23610;&#24230;&#21644;&#26041;&#21521;&#12290;&#25193;&#23637;&#36825;&#19968;&#26041;&#27861;&#65292;&#25105;&#20204;&#30528;&#37325;&#20110;&#20998;&#31163;&#24418;&#29366;&#21644;&#32441;&#29702;&#32452;&#25104;&#37096;&#20998;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;&#65292;&#23558;&#29289;&#20307;&#20013;&#24515;&#21270;&#27169;&#22411;&#20013;&#30340;&#24418;&#29366;&#21644;&#32441;&#29702;&#25104;&#20998;&#20559;&#32622;&#20026;&#28508;&#22312;&#31354;&#38388;&#32500;&#24230;&#30340;&#20004;&#20010;&#19981;&#37325;&#21472;&#23376;&#38598;&#12290;&#36825;&#20123;&#23376;&#38598;&#26159;&#20808;&#39564;&#24050;&#30693;&#30340;&#65292;&#22240;&#27492;&#22312;&#35757;&#32451;&#36807;&#31243;&#20043;&#21069;&#12290;&#22312;&#19968;&#31995;&#21015;&#29289;&#20307;&#20013;&#24515;&#21270;&#27979;&#35797;&#20013;&#36827;&#34892;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Extracting structured representations from raw visual data is an important and long-standing challenge in machine learning. Recently, techniques for unsupervised learning of object-centric representations have raised growing interest. In this context, enhancing the robustness of the latent features can improve the efficiency and effectiveness of the training of downstream tasks. A promising step in this direction is to disentangle the factors that cause variation in the data. Previously, Invariant Slot Attention disentangled position, scale, and orientation from the remaining features. Extending this approach, we focus on separating the shape and texture components. In particular, we propose a novel architecture that biases object-centric models toward disentangling shape and texture components into two non-overlapping subsets of the latent space dimensions. These subsets are known a priori, hence before the training process. Experiments on a range of object-centric benchmarks reveal t
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#30740;&#31350;&#20102;&#34507;&#30333;&#36136;&#34920;&#38754;&#23398;&#20064;&#20013;&#30340;&#20004;&#20010;&#20851;&#38190;&#23646;&#24615;&#65292;&#21363;&#21407;&#23376;&#20043;&#38388;&#30340;&#20851;&#31995;&#21644;&#23618;&#27425;&#21270;&#29305;&#24449;&#20132;&#20114;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2401.10144</link><description>&lt;p&gt;
&#21033;&#29992;&#23618;&#27425;&#20132;&#20114;&#26469;&#36827;&#34892;&#34507;&#30333;&#36136;&#34920;&#38754;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Exploiting Hierarchical Interactions for Protein Surface Learning. (arXiv:2401.10144v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10144
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#30740;&#31350;&#20102;&#34507;&#30333;&#36136;&#34920;&#38754;&#23398;&#20064;&#20013;&#30340;&#20004;&#20010;&#20851;&#38190;&#23646;&#24615;&#65292;&#21363;&#21407;&#23376;&#20043;&#38388;&#30340;&#20851;&#31995;&#21644;&#23618;&#27425;&#21270;&#29305;&#24449;&#20132;&#20114;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#34507;&#30333;&#36136;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#26159;&#32467;&#26500;&#29983;&#29289;&#20449;&#24687;&#23398;&#20013;&#26368;&#37325;&#35201;&#20294;&#20063;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#20043;&#19968;&#12290;&#34507;&#30333;&#36136;&#34920;&#38754;&#30340;&#28508;&#22312;&#21151;&#33021;&#20301;&#28857;&#30001;&#20960;&#20309;&#21644;&#21270;&#23398;&#29305;&#24449;&#20849;&#21516;&#30830;&#23450;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#20165;&#32771;&#34385;&#25163;&#24037;&#35774;&#35745;&#25110;&#21333;&#29420;&#23398;&#20064;&#30340;&#21407;&#23376;&#31867;&#22411;&#30340;&#21270;&#23398;&#29305;&#24449;&#65292;&#24182;&#29420;&#31435;&#25552;&#21462;&#20960;&#20309;&#29305;&#24449;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#26377;&#25928;&#34507;&#30333;&#36136;&#34920;&#38754;&#23398;&#20064;&#30340;&#20004;&#20010;&#20851;&#38190;&#23646;&#24615;&#65306;1&#65289;&#21407;&#23376;&#20043;&#38388;&#30340;&#20851;&#31995;&#65306;&#21407;&#23376;&#36890;&#36807;&#20849;&#20215;&#38190;&#36830;&#25509;&#22312;&#19968;&#36215;&#24418;&#25104;&#29983;&#29289;&#20998;&#23376;&#65292;&#32780;&#19981;&#26159;&#21333;&#29420;&#20986;&#29616;&#65292;&#22240;&#27492;&#22312;&#21270;&#23398;&#29305;&#24449;&#23398;&#20064;&#20013;&#24314;&#27169;&#21407;&#23376;&#20043;&#38388;&#30340;&#20851;&#31995;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;2&#65289;&#23618;&#27425;&#21270;&#29305;&#24449;&#20132;&#20114;&#65306;&#37051;&#36817;&#27531;&#22522;&#25928;&#24212;&#39564;&#35777;&#20102;&#21407;&#23376;&#20043;&#38388;&#20197;&#21450;&#34920;&#38754;&#28857;&#21644;&#21407;&#23376;&#65288;&#25110;&#27531;&#22522;&#65289;&#20043;&#38388;&#30340;&#23618;&#27425;&#21270;&#29305;&#24449;&#20132;&#20114;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#22522;&#20110;&#21407;&#21017;&#30340;&#26694;&#26550;&#65292;&#21363;Hierarchical framework&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting interactions between proteins is one of the most important yet challenging problems in structural bioinformatics. Intrinsically, potential function sites in protein surfaces are determined by both geometric and chemical features. However, existing works only consider handcrafted or individually learned chemical features from the atom type and extract geometric features independently. Here, we identify two key properties of effective protein surface learning: 1) relationship among atoms: atoms are linked with each other by covalent bonds to form biomolecules instead of appearing alone, leading to the significance of modeling the relationship among atoms in chemical feature learning. 2) hierarchical feature interaction: the neighboring residue effect validates the significance of hierarchical feature interaction among atoms and between surface points and atoms (or residues). In this paper, we present a principled framework based on deep learning techniques, namely Hierarchical
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31354;&#38388;-&#26102;&#38388;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;ST-LLM&#65289;&#29992;&#20110;&#20132;&#36890;&#39044;&#27979;&#65292;&#36890;&#36807;&#21442;&#25968;&#25193;&#23637;&#21644;&#39044;&#35757;&#32451;&#26469;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#21033;&#29992;&#31354;&#38388;-&#26102;&#38388;&#23884;&#20837;&#27169;&#22359;&#23398;&#20064;&#26631;&#35760;&#30340;&#31354;&#38388;&#20301;&#32622;&#21644;&#20840;&#23616;&#26102;&#38388;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2401.10134</link><description>&lt;p&gt;
&#31354;&#38388;-&#26102;&#38388;&#22823;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#20132;&#36890;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Spatial-Temporal Large Language Model for Traffic Prediction. (arXiv:2401.10134v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31354;&#38388;-&#26102;&#38388;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;ST-LLM&#65289;&#29992;&#20110;&#20132;&#36890;&#39044;&#27979;&#65292;&#36890;&#36807;&#21442;&#25968;&#25193;&#23637;&#21644;&#39044;&#35757;&#32451;&#26469;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#21033;&#29992;&#31354;&#38388;-&#26102;&#38388;&#23884;&#20837;&#27169;&#22359;&#23398;&#20064;&#26631;&#35760;&#30340;&#31354;&#38388;&#20301;&#32622;&#21644;&#20840;&#23616;&#26102;&#38388;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#39044;&#27979;&#26159;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#21382;&#21490;&#25968;&#25454;&#26469;&#39044;&#27979;&#29305;&#23450;&#20301;&#32622;&#30340;&#26410;&#26469;&#20132;&#36890;&#24773;&#20917;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#20132;&#36890;&#39044;&#27979;&#27169;&#22411;&#36890;&#24120;&#24378;&#35843;&#24320;&#21457;&#22797;&#26434;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#20294;&#23427;&#20204;&#30340;&#20934;&#30830;&#24615;&#24182;&#26410;&#30456;&#24212;&#25552;&#39640;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#20986;&#33394;&#30340;&#33021;&#21147;&#12290;&#19982;&#29616;&#26377;&#27169;&#22411;&#19981;&#21516;&#65292;LLMs&#20027;&#35201;&#36890;&#36807;&#21442;&#25968;&#25193;&#23637;&#21644;&#24191;&#27867;&#30340;&#39044;&#35757;&#32451;&#26469;&#36827;&#27493;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#22522;&#26412;&#32467;&#26500;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31354;&#38388;-&#26102;&#38388;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;ST-LLM&#65289;&#29992;&#20110;&#20132;&#36890;&#39044;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;ST-LLM&#23558;&#27599;&#20010;&#20301;&#32622;&#30340;&#26102;&#38388;&#27493;&#38271;&#23450;&#20041;&#20026;&#26631;&#35760;&#65292;&#24182;&#32467;&#21512;&#31354;&#38388;-&#26102;&#38388;&#23884;&#20837;&#27169;&#22359;&#26469;&#23398;&#20064;&#26631;&#35760;&#30340;&#31354;&#38388;&#20301;&#32622;&#21644;&#20840;&#23616;&#26102;&#38388;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#34920;&#31034;&#34987;&#34701;&#21512;&#20197;&#20026;&#27599;&#20010;&#26631;&#35760;&#25552;&#20379;&#32479;&#19968;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic prediction, a critical component for intelligent transportation systems, endeavors to foresee future traffic at specific locations using historical data. Although existing traffic prediction models often emphasize developing complex neural network structures, their accuracy has not seen improvements accordingly. Recently, Large Language Models (LLMs) have shown outstanding capabilities in time series analysis. Differing from existing models, LLMs progress mainly through parameter expansion and extensive pre-training while maintaining their fundamental structures. In this paper, we propose a Spatial-Temporal Large Language Model (ST-LLM) for traffic prediction. Specifically, ST-LLM redefines the timesteps at each location as tokens and incorporates a spatial-temporal embedding module to learn the spatial location and global temporal representations of tokens. Then these representations are fused to provide each token with unified spatial and temporal information. Furthermore, we
&lt;/p&gt;</description></item><item><title>&#36793;&#32536;&#21464;&#25442;&#22120;&#26159;&#19968;&#20010;&#20840;&#23616;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#23427;&#20855;&#26377;&#33267;&#23569;3-WL&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#33021;&#22815;&#22312;&#39044;&#27979;&#24615;&#33021;&#19978;&#36229;&#36807;&#20854;&#20182;&#26550;&#26500;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#20301;&#32622;&#25110;&#32467;&#26500;&#32534;&#30721;&#12290;</title><link>http://arxiv.org/abs/2401.10119</link><description>&lt;p&gt;
&#36208;&#21521;&#22522;&#20110;&#21407;&#21017;&#30340;&#22270;&#24418;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Towards Principled Graph Transformers. (arXiv:2401.10119v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10119
&lt;/p&gt;
&lt;p&gt;
&#36793;&#32536;&#21464;&#25442;&#22120;&#26159;&#19968;&#20010;&#20840;&#23616;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#23427;&#20855;&#26377;&#33267;&#23569;3-WL&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#33021;&#22815;&#22312;&#39044;&#27979;&#24615;&#33021;&#19978;&#36229;&#36807;&#20854;&#20182;&#26550;&#26500;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#20301;&#32622;&#25110;&#32467;&#26500;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;k&#32500;Weisfeiler-Leman&#65288;k-WL&#65289;&#23618;&#27425;&#32467;&#26500;&#30340;&#22270;&#24418;&#23398;&#20064;&#26550;&#26500;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#24456;&#22909;&#29702;&#35299;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#26550;&#26500;&#22312;&#30495;&#23454;&#20219;&#21153;&#20013;&#24448;&#24448;&#26080;&#27861;&#25552;&#20379;&#21487;&#38752;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#23454;&#38469;&#24433;&#21709;&#21147;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22522;&#20110;&#20840;&#23616;&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#22914;&#22270;&#24418;&#21464;&#25442;&#22120;&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;&#23558;&#23427;&#20204;&#30340;&#34920;&#36798;&#33021;&#21147;&#19982;k-WL&#23618;&#27425;&#32467;&#26500;&#36827;&#34892;&#27604;&#36739;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#23588;&#20854;&#26159;&#22240;&#20026;&#36825;&#20123;&#26550;&#26500;&#20381;&#36182;&#20110;&#20301;&#32622;&#25110;&#32467;&#26500;&#32534;&#30721;&#26469;&#23454;&#29616;&#20854;&#34920;&#36798;&#33021;&#21147;&#21644;&#39044;&#27979;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;&#36793;&#32536;&#21464;&#25442;&#22120;&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#33410;&#28857;&#23545;&#32780;&#19981;&#26159;&#33410;&#28857;&#19978;&#36827;&#34892;&#25805;&#20316;&#30340;&#20840;&#23616;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#20855;&#26377;&#33267;&#23569;3-WL&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#32463;&#39564;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36793;&#32536;&#21464;&#25442;&#22120;&#22312;&#39044;&#27979;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#20854;&#20182;&#29702;&#35770;&#23545;&#40784;&#30340;&#26550;&#26500;&#65292;&#21516;&#26102;&#19981;&#20381;&#36182;&#20110;&#20301;&#32622;&#25110;&#32467;&#26500;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph learning architectures based on the k-dimensional Weisfeiler-Leman (k-WL) hierarchy offer a theoretically well-understood expressive power. However, such architectures often fail to deliver solid predictive performance on real-world tasks, limiting their practical impact. In contrast, global attention-based models such as graph transformers demonstrate strong performance in practice, but comparing their expressive power with the k-WL hierarchy remains challenging, particularly since these architectures rely on positional or structural encodings for their expressivity and predictive performance. To address this, we show that the recently proposed Edge Transformer, a global attention model operating on node pairs instead of nodes, has at least 3-WL expressive power. Empirically, we demonstrate that the Edge Transformer surpasses other theoretically aligned architectures regarding predictive performance while not relying on positional or structural encodings.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#26631;&#20934;&#22810;&#23548;&#30561;&#30496;&#22270;&#65288;PSG&#65289;&#21644;&#32819;&#20869;&#33041;&#30005;&#20449;&#21495;&#30340;&#30456;&#20284;&#24615;&#65292;&#26088;&#22312;&#25506;&#32034;&#19968;&#31181;&#26356;&#23569;&#20405;&#20837;&#12289;&#25104;&#26412;&#25928;&#30410;&#39640;&#21644;&#20415;&#25658;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#30740;&#31350;&#30830;&#23450;&#20102;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#25552;&#21462;&#29305;&#24449;&#36827;&#34892;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2401.10107</link><description>&lt;p&gt;
&#26631;&#20934;&#22810;&#23548;&#30561;&#30496;&#22270;&#19982;&#32819;&#20869;&#33041;&#30005;&#20449;&#21495;&#30340;&#27604;&#36739;&#20998;&#26512;&#65306;&#21021;&#27493;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Comparison analysis between standard polysomnographic data and in-ear-EEG signals: A preliminary study. (arXiv:2401.10107v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10107
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#26631;&#20934;&#22810;&#23548;&#30561;&#30496;&#22270;&#65288;PSG&#65289;&#21644;&#32819;&#20869;&#33041;&#30005;&#20449;&#21495;&#30340;&#30456;&#20284;&#24615;&#65292;&#26088;&#22312;&#25506;&#32034;&#19968;&#31181;&#26356;&#23569;&#20405;&#20837;&#12289;&#25104;&#26412;&#25928;&#30410;&#39640;&#21644;&#20415;&#25658;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#30740;&#31350;&#30830;&#23450;&#20102;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#25552;&#21462;&#29305;&#24449;&#36827;&#34892;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#30446;&#30340;&#65306;&#22810;&#23548;&#30561;&#30496;&#22270;&#65288;PSG&#65289;&#30446;&#21069;&#34987;&#29992;&#20316;&#35780;&#20272;&#30561;&#30496;&#38556;&#30861;&#30340;&#22522;&#20934;&#12290;&#20854;&#19981;&#33298;&#36866;&#12289;&#19981;&#36866;&#21512;&#23478;&#24237;&#20351;&#29992;&#20197;&#21450;&#22312;&#30561;&#30496;&#36136;&#37327;&#35780;&#20272;&#20013;&#24341;&#20837;&#20559;&#24046;&#30340;&#38382;&#39064;&#38656;&#35201;&#25506;&#32034;&#26356;&#23569;&#20405;&#20837;&#24615;&#12289;&#25104;&#26412;&#25928;&#30410;&#39640;&#21644;&#20415;&#25658;&#24615;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#20505;&#36873;&#26041;&#27861;&#26159;&#32819;&#20869;&#33041;&#30005;&#20256;&#24863;&#22120;&#65292;&#23427;&#22312;&#33298;&#36866;&#24615;&#12289;&#22266;&#23450;&#30005;&#26497;&#20301;&#32622;&#12289;&#25239;&#30005;&#30913;&#24178;&#25200;&#24615;&#21644;&#26131;&#20110;&#20351;&#29992;&#24615;&#26041;&#38754;&#22343;&#20855;&#26377;&#20248;&#21183;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#24314;&#31435;&#19968;&#31181;&#35780;&#20272;&#32819;&#20869;&#33041;&#30005;&#20449;&#21495;&#19982;&#26631;&#20934;PSG&#20043;&#38388;&#30456;&#20284;&#24615;&#30340;&#26041;&#27861;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#35780;&#20272;PSG&#21644;&#32819;&#20869;&#33041;&#30005;&#25512;&#23548;&#30340;&#30561;&#30496;&#22270;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#20174;PSG&#21644;&#32819;&#20869;&#33041;&#30005;&#20449;&#21495;&#30340;30&#31186;&#26102;&#22495;&#21644;&#39057;&#22495;&#25552;&#21462;&#29305;&#24449;&#12290;&#25105;&#20204;&#21482;&#32771;&#34385;&#22312;PSG&#35780;&#20998;&#21592;&#21644;&#32819;&#20869;&#33041;&#30005;&#35780;&#20998;&#21592;&#36798;&#25104;&#19968;&#33268;&#26102;&#30340;&#26102;&#27573;&#12290;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#26041;&#27861;&#26469;&#37327;&#21270;PSG&#25512;&#23548;&#21644;&#21333;&#36890;&#36947;&#32819;&#20869;&#33041;&#30005;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;...
&lt;/p&gt;
&lt;p&gt;
Study Objectives: Polysomnography (PSG) currently serves as the benchmark for evaluating sleep disorders. Its discomfort, impracticality for home-use, and introduction of bias in sleep quality assessment necessitate the exploration of less invasive, cost-effective, and portable alternatives. One promising contender is the in-ear-EEG sensor, which offers advantages in terms of comfort, fixed electrode positions, resistance to electromagnetic interference, and user-friendliness. This study aims to establish a methodology to assess the similarity between the in-ear-EEG signal and standard PSG.  Methods: We assess the agreement between the PSG and in-ear-EEG derived hypnograms. We extract features in the time- and frequency- domain from PSG and in-ear-EEG 30-second epochs. We only consider the epochs where the PSG-scorers and the in-ear-EEG-scorers were in agreement. We introduce a methodology to quantify the similarity between PSG derivations and the single-channel in-ear-EEG. The approac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#32463;&#20856;&#31639;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#21333;&#27604;&#29305;&#27979;&#37327;&#25968;&#25454;&#23398;&#20064;&#27973;&#23618;&#37327;&#23376;&#30005;&#36335;&#30340;&#25551;&#36848;&#65292;&#21253;&#25324;&#26550;&#26500;&#26410;&#30693;&#30340;&#30005;&#36335;&#21644;&#20934;&#22791;&#26410;&#30693;&#24577;&#30340;&#30005;&#36335;&#12290;</title><link>http://arxiv.org/abs/2401.10095</link><description>&lt;p&gt;
&#23398;&#20064;&#27973;&#23618;&#37327;&#23376;&#30005;&#36335;
&lt;/p&gt;
&lt;p&gt;
Learning shallow quantum circuits. (arXiv:2401.10095v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10095
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#32463;&#20856;&#31639;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#21333;&#27604;&#29305;&#27979;&#37327;&#25968;&#25454;&#23398;&#20064;&#27973;&#23618;&#37327;&#23376;&#30005;&#36335;&#30340;&#25551;&#36848;&#65292;&#21253;&#25324;&#26550;&#26500;&#26410;&#30693;&#30340;&#30005;&#36335;&#21644;&#20934;&#22791;&#26410;&#30693;&#24577;&#30340;&#30005;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#23398;&#20064;&#37327;&#23376;&#30005;&#36335;&#20855;&#26377;&#22522;&#30784;&#20852;&#36259;&#65292;&#20294;&#23398;&#20064;&#27973;&#23618;&#37327;&#23376;&#30005;&#36335;&#30340;&#35745;&#31639;&#26377;&#25928;&#31639;&#27861;&#30340;&#23384;&#22312;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#12290;&#30001;&#20110;&#27973;&#23618;&#37327;&#23376;&#30005;&#36335;&#21487;&#20197;&#29983;&#25104;&#32463;&#20856;&#38590;&#20197;&#37319;&#26679;&#30340;&#20998;&#24067;&#65292;&#29616;&#26377;&#30340;&#23398;&#20064;&#31639;&#27861;&#19981;&#36866;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#32463;&#20856;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#37327;&#23376;&#30005;&#36335;&#36755;&#20986;&#24577;&#19978;&#36827;&#34892;&#21333;&#27604;&#29305;&#27979;&#37327;&#25968;&#25454;&#65292;&#23398;&#20064;&#20219;&#24847;&#26410;&#30693;&#30340;$n$&#27604;&#29305;&#27973;&#23618;&#37327;&#23376;&#30005;&#36335;$U$&#30340;&#25551;&#36848;&#65292;&#20854;&#26550;&#26500;&#26410;&#30693;&#65292;&#19988;&#36798;&#21040;&#23567;&#30340;&#38075;&#30707;&#36317;&#31163;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#32463;&#20856;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;$\lvert \psi \rangle$&#30340;&#21103;&#26412;&#19978;&#36827;&#34892;&#21333;&#27604;&#29305;&#27979;&#37327;&#65292;&#23398;&#20064;&#30001;&#27973;&#23618;&#37327;&#23376;&#30005;&#36335;$U$&#65288;&#22312;2D&#26684;&#23376;&#19978;&#65289;&#20934;&#22791;&#30340;&#20219;&#24847;&#26410;&#30693;&#30340;$n$&#27604;&#29305;&#24577;$\lvert \psi \rangle$&#30340;&#25551;&#36848;&#65292;&#19988;&#36798;&#21040;&#23567;&#30340;&#36857;&#36317;&#31163;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#20102;&#22522;&#20110;&#23616;&#37096;&#21453;&#36716;&#30340;&#37327;&#23376;&#30005;&#36335;&#34920;&#31034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite fundamental interests in learning quantum circuits, the existence of a computationally efficient algorithm for learning shallow quantum circuits remains an open question. Because shallow quantum circuits can generate distributions that are classically hard to sample from, existing learning algorithms do not apply. In this work, we present a polynomial-time classical algorithm for learning the description of any unknown $n$-qubit shallow quantum circuit $U$ (with arbitrary unknown architecture) within a small diamond distance using single-qubit measurement data on the output states of $U$. We also provide a polynomial-time classical algorithm for learning the description of any unknown $n$-qubit state $\lvert \psi \rangle = U \lvert 0^n \rangle$ prepared by a shallow quantum circuit $U$ (on a 2D lattice) within a small trace distance using single-qubit measurements on copies of $\lvert \psi \rangle$. Our approach uses a quantum circuit representation based on local inversions an
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#36335;&#24452;&#21457;&#23637;&#32593;&#32476;&#65292;&#21033;&#29992;&#24515;&#30005;&#22270;&#25968;&#25454;&#39044;&#27979;&#24515;&#25151;&#39076;&#21160;&#24739;&#32773;&#26159;&#21542;&#24212;&#35813;&#25512;&#33616;&#25239;&#20957;&#27835;&#30103;&#65292;&#20026;&#21307;&#29983;&#22312;&#33647;&#29289;&#20915;&#31574;&#20013;&#25552;&#20379;&#20915;&#31574;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2401.10014</link><description>&lt;p&gt;
&#36890;&#36807;&#36335;&#24452;&#21457;&#23637;&#32593;&#32476;&#20248;&#21270;&#24515;&#25151;&#39076;&#21160;&#24739;&#32773;&#30340;&#33647;&#29289;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Optimizing Medication Decisions for Patients with Atrial Fibrillation through Path Development Network. (arXiv:2401.10014v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#36335;&#24452;&#21457;&#23637;&#32593;&#32476;&#65292;&#21033;&#29992;&#24515;&#30005;&#22270;&#25968;&#25454;&#39044;&#27979;&#24515;&#25151;&#39076;&#21160;&#24739;&#32773;&#26159;&#21542;&#24212;&#35813;&#25512;&#33616;&#25239;&#20957;&#27835;&#30103;&#65292;&#20026;&#21307;&#29983;&#22312;&#33647;&#29289;&#20915;&#31574;&#20013;&#25552;&#20379;&#20915;&#31574;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#25151;&#39076;&#21160;&#65288;AF&#65289;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#24515;&#24459;&#22833;&#24120;&#65292;&#20854;&#29305;&#24449;&#26159;&#24515;&#25151;&#24555;&#36895;&#19981;&#35268;&#21017;&#25910;&#32553;&#12290;&#23427;&#26174;&#33879;&#22686;&#21152;&#20102;&#20013;&#39118;&#30340;&#39118;&#38505;&#65292;&#22240;&#20026;&#24515;&#25151;&#20013;&#30340;&#34880;&#27969;&#20943;&#24930;&#65292;&#23588;&#20854;&#26159;&#22312;&#26131;&#24418;&#25104;&#34880;&#26643;&#30340;&#24038;&#24515;&#25151;&#38468;&#23646;&#29289;&#20013;&#12290;&#36825;&#26679;&#30340;&#34880;&#26643;&#21487;&#20197;&#36801;&#31227;&#21040;&#33041;&#21160;&#33033;&#65292;&#24341;&#21457;&#32570;&#34880;&#24615;&#20013;&#39118;&#12290;&#20026;&#20102;&#35780;&#20272;&#26159;&#21542;&#24212;&#35813;&#20026;&#24515;&#25151;&#39076;&#21160;&#24739;&#32773;&#24320;&#29992;&#25239;&#20957;&#33647;&#29289;&#65292;&#21307;&#29983;&#36890;&#24120;&#20351;&#29992;CHA2DS2-VASc&#35780;&#20998;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#25239;&#20957;&#33647;&#29289;&#30340;&#20351;&#29992;&#24517;&#39035;&#35880;&#24910;&#65292;&#22240;&#20026;&#23427;&#21487;&#33021;&#24433;&#21709;&#20957;&#34880;&#21151;&#33021;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#20351;&#29992;12&#23548;&#32852;&#24515;&#30005;&#22270;&#25968;&#25454;&#39044;&#27979;&#24515;&#25151;&#39076;&#21160;&#24739;&#32773;&#26159;&#21542;&#24212;&#35813;&#25512;&#33616;&#25239;&#20957;&#27835;&#30103;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#22312;&#35813;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;STOME&#26469;&#22686;&#24378;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#28982;&#21518;&#36890;&#36807;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#36827;&#34892;&#22788;&#29702;&#12290;&#36890;&#36807;&#24341;&#20837;&#36335;&#24452;&#21457;&#23637;&#23618;&#65292;&#35813;&#27169;&#22411;&#22312;NPV&#20026;1&#30340;&#26465;&#20214;&#19979;&#23454;&#29616;&#20102;30.6%&#30340;&#29305;&#24322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Atrial fibrillation (AF) is a common cardiac arrhythmia characterized by rapid and irregular contractions of the atria. It significantly elevates the risk of strokes due to slowed blood flow in the atria, especially in the left atrial appendage, which is prone to blood clot formation. Such clots can migrate into cerebral arteries, leading to ischemic stroke. To assess whether AF patients should be prescribed anticoagulants, doctors often use the CHA2DS2-VASc scoring system. However, anticoagulant use must be approached with caution as it can impact clotting functions. This study introduces a machine learning algorithm that predicts whether patients with AF should be recommended anticoagulant therapy using 12-lead ECG data. In this model, we use STOME to enhance time-series data and then process it through a Convolutional Neural Network (CNN). By incorporating a path development layer, the model achieves a specificity of 30.6% under the condition of an NPV of 1. In contrast, LSTM algori
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#23637;&#20102;&#19968;&#20010;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#32508;&#21512;&#31995;&#32479;&#65292;&#21033;&#29992;&#35270;&#35273;&#21644;&#38899;&#39057;&#20449;&#21495;&#26469;&#35780;&#20272;&#34588;&#34562;&#31665;&#30340;&#20581;&#24247;&#29366;&#20917;&#65292;&#36890;&#36807;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#22810;&#27169;&#24577;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2401.09988</link><description>&lt;p&gt;
&#21457;&#23637;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#32508;&#21512;&#31995;&#32479;&#29992;&#20110;&#34588;&#34562;&#20581;&#24247;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Developing an AI-based Integrated System for Bee Health Evaluation. (arXiv:2401.09988v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09988
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#23637;&#20102;&#19968;&#20010;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#32508;&#21512;&#31995;&#32479;&#65292;&#21033;&#29992;&#35270;&#35273;&#21644;&#38899;&#39057;&#20449;&#21495;&#26469;&#35780;&#20272;&#34588;&#34562;&#31665;&#30340;&#20581;&#24247;&#29366;&#20917;&#65292;&#36890;&#36807;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#22810;&#27169;&#24577;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34588;&#34562;&#26159;&#20840;&#29699;&#32422;&#19977;&#20998;&#20043;&#19968;&#39135;&#29289;&#20379;&#24212;&#30340;&#20256;&#31881;&#32773;&#65292;&#20294;&#26159;&#30001;&#20110;&#22810;&#31181;&#22240;&#32032;&#23548;&#33268;&#34588;&#34562;&#32676;&#20307;&#22312;&#36807;&#21435;&#21313;&#24180;&#20013;&#24778;&#20154;&#22320;&#19979;&#38477;&#20102;&#36817;40%&#65292;&#21253;&#25324;&#26432;&#34411;&#21058;&#21644;&#23475;&#34411;&#31561;&#12290;&#20256;&#32479;&#30340;&#34588;&#34562;&#31665;&#30417;&#27979;&#26041;&#27861;&#65292;&#22914;&#20154;&#24037;&#26816;&#26597;&#65292;&#20027;&#35266;&#24615;&#24378;&#65292;&#24178;&#25200;&#24615;&#39640;&#65292;&#32791;&#26102;&#38271;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#20154;&#24037;&#26234;&#33021;&#34987;&#29992;&#20110;&#35780;&#20272;&#34588;&#34562;&#31665;&#30340;&#20581;&#24247;&#29366;&#20917;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#30740;&#31350;&#32570;&#20047;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20027;&#35201;&#20381;&#36182;&#20110;&#21333;&#19968;&#25968;&#25454;&#26469;&#28304;&#65292;&#22914;&#34588;&#34562;&#22270;&#20687;&#25110;&#22768;&#38899;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#31995;&#32479;&#65292;&#21253;&#25324;&#34588;&#34562;&#30446;&#26631;&#26816;&#27979;&#21644;&#20581;&#24247;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#35813;&#31995;&#32479;&#21033;&#29992;&#20102;&#35270;&#35273;&#21644;&#38899;&#39057;&#20449;&#21495;&#30340;&#32452;&#21512;&#26469;&#20998;&#26512;&#34588;&#34562;&#34892;&#20026;&#12290;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#22810;&#27169;&#24577;&#31070;&#32463;&#32593;&#32476;&#65288;AMNN&#65289;&#65292;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#20174;&#27599;&#31181;&#20449;&#21495;&#31867;&#22411;&#20013;&#32858;&#28966;&#20110;&#20851;&#38190;&#29305;&#24449;&#65292;&#20197;&#36827;&#34892;&#20934;&#30830;&#30340;&#34588;&#34562;&#20581;&#24247;&#35780;&#20272;&#12290;AMNN&#23454;&#29616;&#20102;92.61%&#30340;&#25972;&#20307;&#20934;&#30830;&#29575;&#65292;&#36229;&#36807;&#20102;&#20843;&#31181;&#29616;&#26377;&#21333;&#19968;&#20449;&#21495;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Honey bees pollinate about one-third of the world's food supply, but bee colonies have alarmingly declined by nearly 40% over the past decade due to several factors, including pesticides and pests. Traditional methods for monitoring beehives, such as human inspection, are subjective, disruptive, and time-consuming. To overcome these limitations, artificial intelligence has been used to assess beehive health. However, previous studies have lacked an end-to-end solution and primarily relied on data from a single source, either bee images or sounds. This study introduces a comprehensive system consisting of bee object detection and health evaluation. Additionally, it utilized a combination of visual and audio signals to analyze bee behaviors. An Attention-based Multimodal Neural Network (AMNN) was developed to adaptively focus on key features from each type of signal for accurate bee health assessment. The AMNN achieved an overall accuracy of 92.61%, surpassing eight existing single-signa
&lt;/p&gt;</description></item><item><title>FLex&amp;Chill &#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;Logit Chilling&#26041;&#27861;&#25913;&#36827;&#26412;&#22320;&#32852;&#21512;&#23398;&#20064;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#21152;&#24555;&#27169;&#22411;&#25910;&#25947;&#24182;&#25552;&#39640;&#25512;&#29702;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.09986</link><description>&lt;p&gt;
FLex&amp;Chill&#65306;&#36890;&#36807;Logit Chilling&#25913;&#36827;&#26412;&#22320;&#32852;&#21512;&#23398;&#20064;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
FLex&amp;Chill: Improving Local Federated Learning Training with Logit Chilling. (arXiv:2401.09986v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09986
&lt;/p&gt;
&lt;p&gt;
FLex&amp;Chill &#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;Logit Chilling&#26041;&#27861;&#25913;&#36827;&#26412;&#22320;&#32852;&#21512;&#23398;&#20064;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#21152;&#24555;&#27169;&#22411;&#25910;&#25947;&#24182;&#25552;&#39640;&#25512;&#29702;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#21512;&#23398;&#20064;&#30001;&#20110;&#26412;&#22320;&#23458;&#25143;&#31471;&#30340;&#38750;iid&#20998;&#24067;&#24335;&#35757;&#32451;&#25968;&#25454;&#32780;&#21463;&#21040;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#38459;&#30861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#21512;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;FLex&amp;Chill&#65292;&#21033;&#29992;&#20102;Logit Chilling&#26041;&#27861;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#32852;&#21512;&#23398;&#20064;&#31995;&#32479;&#20013;&#22266;&#26377;&#30340;&#38750;iid&#25968;&#25454;&#29305;&#24449;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#21152;&#24555;&#27169;&#22411;&#25910;&#25947;&#24182;&#25552;&#39640;&#25512;&#29702;&#31934;&#24230;&#12290;&#20174;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20840;&#23616;&#32852;&#21512;&#23398;&#20064;&#27169;&#22411;&#25910;&#25947;&#26102;&#38388;&#25552;&#39640;&#20102;6&#20493;&#65292;&#25512;&#29702;&#31934;&#24230;&#25552;&#39640;&#20102;3.37%&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning are inherently hampered by data heterogeneity: non-iid distributed training data over local clients. We propose a novel model training approach for federated learning, FLex&amp;Chill, which exploits the Logit Chilling method. Through extensive evaluations, we demonstrate that, in the presence of non-iid data characteristics inherent in federated learning systems, this approach can expedite model convergence and improve inference accuracy. Quantitatively, from our experiments, we observe up to 6X improvement in the global federated learning model convergence time, and up to 3.37% improvement in inference accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#24515;&#33039;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;&#65292;&#23454;&#26045;&#20102;&#22810;&#20010;U-Net&#34893;&#29983;&#27169;&#22411;&#20197;&#23454;&#29616;&#23545;&#24515;&#33039;&#29305;&#23450;&#37096;&#20301;&#30340;&#20840;&#38754;&#35299;&#21078;&#21644;&#21151;&#33021;&#20998;&#26512;&#12290;&#36890;&#36807;&#22270;&#20687;&#12289;&#22270;&#34920;&#21644;&#23450;&#37327;&#25351;&#26631;&#39564;&#35777;&#20102;&#27169;&#22411;&#30340;&#25928;&#26524;&#65292;&#24182;&#35752;&#35770;&#20102;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#25913;&#36827;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2401.09980</link><description>&lt;p&gt;
&#24515;&#23460;&#20998;&#21106;&#65306;U-Net&#34893;&#29983;&#27169;&#22411;&#30340;&#31616;&#35201;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Ventricular Segmentation: A Brief Comparison of U-Net Derivatives. (arXiv:2401.09980v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09980
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#24515;&#33039;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;&#65292;&#23454;&#26045;&#20102;&#22810;&#20010;U-Net&#34893;&#29983;&#27169;&#22411;&#20197;&#23454;&#29616;&#23545;&#24515;&#33039;&#29305;&#23450;&#37096;&#20301;&#30340;&#20840;&#38754;&#35299;&#21078;&#21644;&#21151;&#33021;&#20998;&#26512;&#12290;&#36890;&#36807;&#22270;&#20687;&#12289;&#22270;&#34920;&#21644;&#23450;&#37327;&#25351;&#26631;&#39564;&#35777;&#20102;&#27169;&#22411;&#30340;&#25928;&#26524;&#65292;&#24182;&#35752;&#35770;&#20102;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#25913;&#36827;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#24433;&#20687;&#26159;&#25351;&#29992;&#20110;&#35266;&#23519;&#20154;&#20307;&#21450;&#20854;&#20869;&#37096;&#30340;&#25216;&#26415;&#21644;&#26041;&#27861;&#65292;&#20197;&#35786;&#26029;&#12289;&#30417;&#27979;&#29978;&#33267;&#27835;&#30103;&#21307;&#23398;&#30142;&#30149;&#12290;&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#24515;&#33039;&#30701;&#36724;&#30913;&#20849;&#25391;&#25104;&#20687;&#22270;&#20687;&#30340;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;&#65292;&#26088;&#22312;&#25552;&#39640;&#19982;&#24515;&#33039;&#30456;&#20851;&#30340;&#21307;&#23398;&#30142;&#30149;&#30340;&#35786;&#26029;&#12289;&#30417;&#27979;&#21644;&#27835;&#30103;&#12290;&#37325;&#28857;&#26159;&#23454;&#26045;&#21508;&#31181;U-Net&#30340;&#34893;&#29983;&#20307;&#31995;&#32467;&#26500;&#65292;&#20197;&#26377;&#25928;&#22320;&#20998;&#31163;&#24515;&#33039;&#30340;&#29305;&#23450;&#37096;&#20998;&#65292;&#36827;&#34892;&#20840;&#38754;&#30340;&#35299;&#21078;&#21644;&#21151;&#33021;&#20998;&#26512;&#12290;&#36890;&#36807;&#22270;&#20687;&#12289;&#22270;&#34920;&#21644;&#23450;&#37327;&#25351;&#26631;&#30340;&#32452;&#21512;&#23637;&#31034;&#20102;&#27169;&#22411;&#21450;&#20854;&#39044;&#27979;&#30340;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;&#36935;&#21040;&#30340;&#25361;&#25112;&#65292;&#24182;&#27010;&#36848;&#20102;&#26410;&#26469;&#25913;&#36827;&#30340;&#31574;&#30053;&#12290;&#26412;&#25688;&#35201;&#31616;&#35201;&#27010;&#36848;&#20102;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#24515;&#33039;&#22270;&#20687;&#20998;&#21106;&#30340;&#24037;&#20316;&#65292;&#24378;&#35843;&#20102;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical imaging refers to the technologies and methods utilized to view the human body and its inside, in order to diagnose, monitor, or even treat medical disorders. This paper aims to explore the application of deep learning techniques in the semantic segmentation of Cardiac short-axis MRI (Magnetic Resonance Imaging) images, aiming to enhance the diagnosis, monitoring, and treatment of medical disorders related to the heart. The focus centers on implementing various architectures that are derivatives of U-Net, to effectively isolate specific parts of the heart for comprehensive anatomical and functional analysis. Through a combination of images, graphs, and quantitative metrics, the efficacy of the models and their predictions are showcased. Additionally, this paper addresses encountered challenges and outline strategies for future improvements. This abstract provides a concise overview of the efforts in utilizing deep learning for cardiac image segmentation, emphasizing both the ac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#37051;&#22495;&#31579;&#36873;&#25511;&#21046;&#39640;&#26031;&#22270;&#27169;&#22411;&#20013;&#34394;&#35686;&#29575;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#24050;&#23384;&#22312;&#30340;&#20272;&#35745;&#22120;&#23481;&#26131;&#20986;&#29616;&#34394;&#35686;&#36793;&#32536;&#26816;&#27979;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#26159;&#26080;&#21442;&#25968;&#30340;&#65292;&#26080;&#38656;&#29992;&#25143;&#35843;&#25972;&#65292;&#24182;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#24471;&#21040;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2401.09979</link><description>&lt;p&gt;
&#36890;&#36807;&#37051;&#22495;&#31579;&#36873;&#25511;&#21046;&#39640;&#26031;&#22270;&#27169;&#22411;&#20013;&#30340;&#34394;&#35686;&#29575;
&lt;/p&gt;
&lt;p&gt;
False Discovery Rate Control for Gaussian Graphical Models via Neighborhood Screening. (arXiv:2401.09979v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09979
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#37051;&#22495;&#31579;&#36873;&#25511;&#21046;&#39640;&#26031;&#22270;&#27169;&#22411;&#20013;&#34394;&#35686;&#29575;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#24050;&#23384;&#22312;&#30340;&#20272;&#35745;&#22120;&#23481;&#26131;&#20986;&#29616;&#34394;&#35686;&#36793;&#32536;&#26816;&#27979;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#26159;&#26080;&#21442;&#25968;&#30340;&#65292;&#26080;&#38656;&#29992;&#25143;&#35843;&#25972;&#65292;&#24182;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#24471;&#21040;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#22270;&#27169;&#22411;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#12290;&#23427;&#20204;&#23558;&#21464;&#37327;&#20043;&#38388;&#30340;&#32479;&#35745;&#20851;&#31995;&#24314;&#27169;&#25104;&#19968;&#20010;&#22270;&#65292;&#20854;&#20013;&#20004;&#20010;&#21464;&#37327;&#20043;&#38388;&#30340;&#36793;&#34920;&#31034;&#26465;&#20214;&#30456;&#20851;&#24615;&#12290;&#28982;&#32780;&#65292;&#24050;&#32463;&#24314;&#31435;&#30340;&#20272;&#35745;&#22120;&#65292;&#22914;&#22270;&#24418;&#22871;&#32034;&#25110;&#37051;&#22495;&#36873;&#25321;&#65292;&#24050;&#30693;&#23481;&#26131;&#20986;&#29616;&#22823;&#37327;&#34394;&#35686;&#36793;&#32536;&#26816;&#27979;&#12290;&#34394;&#35686;&#26816;&#27979;&#21487;&#33021;&#20250;&#23548;&#33268;&#19981;&#20934;&#30830;&#29978;&#33267;&#38169;&#35823;&#30340;&#31185;&#23398;&#35299;&#37322;&#65292;&#22312;&#29983;&#29289;&#21307;&#23398;&#25110;&#20581;&#24247;&#25252;&#29702;&#31561;&#24212;&#29992;&#20013;&#20855;&#26377;&#37325;&#22823;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#33410;&#28857;&#30340;&#21464;&#37327;&#36873;&#25321;&#26041;&#27861;&#26469;&#23398;&#20064;&#22270;&#65292;&#24182;&#22312;&#33258;&#25105;&#20272;&#35745;&#27700;&#24179;&#19978;&#21487;&#25511;&#21046;&#25152;&#36873;&#36793;&#32536;&#38598;&#21512;&#30340;&#34394;&#35686;&#29575;&#12290;&#19968;&#31181;&#26032;&#39062;&#30340;&#20010;&#20307;&#37051;&#22495;&#34701;&#21512;&#26041;&#27861;&#20135;&#29983;&#20102;&#19968;&#20010;&#26080;&#21521;&#22270;&#20272;&#35745;&#20540;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#26080;&#21442;&#25968;&#30340;&#65292;&#19981;&#38656;&#35201;&#29992;&#25143;&#36827;&#34892;&#35843;&#25972;&#12290;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#19982;&#31454;&#20105;&#30340;&#34394;&#35686;&#29575;&#25511;&#21046;&#26041;&#27861;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gaussian graphical models emerge in a wide range of fields. They model the statistical relationships between variables as a graph, where an edge between two variables indicates conditional dependence. Unfortunately, well-established estimators, such as the graphical lasso or neighborhood selection, are known to be susceptible to a high prevalence of false edge detections. False detections may encourage inaccurate or even incorrect scientific interpretations, with major implications in applications, such as biomedicine or healthcare. In this paper, we introduce a nodewise variable selection approach to graph learning and provably control the false discovery rate of the selected edge set at a self-estimated level. A novel fusion method of the individual neighborhoods outputs an undirected graph estimate. The proposed method is parameter-free and does not require tuning by the user. Benchmarks against competing false discovery rate controlling methods in numerical experiments considering 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20809;&#35889;&#35270;&#35282;&#65292;&#30740;&#31350;&#20102;&#22270;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#20013;&#30340;&#22270;&#23646;&#24615;&#21644;&#32467;&#26500;&#21464;&#21270;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#20445;&#25345;&#20302;&#39057;&#29305;&#24449;&#20540;&#19981;&#21464;&#21487;&#20197;&#20445;&#30041;&#20851;&#38190;&#23646;&#24615;&#65292;&#25552;&#20986;&#20102;&#21452;&#26865;&#38236;&#65288;DP&#65289;&#22686;&#24378;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#28789;&#27963;&#22320;&#20445;&#30041;&#20851;&#38190;&#30340;&#22270;&#23646;&#24615;&#21516;&#26102;&#22686;&#21152;&#22270;&#30340;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.09953</link><description>&lt;p&gt;
&#36890;&#36807;&#21452;&#26865;&#38236;: &#20809;&#35889;&#35270;&#35282;&#19979;&#30340;&#22270;&#25968;&#25454;&#22686;&#24378;&#29992;&#20110;&#22270;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Through the Dual-Prism: A Spectral Perspective on Graph Data Augmentation for Graph Classification. (arXiv:2401.09953v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09953
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20809;&#35889;&#35270;&#35282;&#65292;&#30740;&#31350;&#20102;&#22270;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#20013;&#30340;&#22270;&#23646;&#24615;&#21644;&#32467;&#26500;&#21464;&#21270;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#20445;&#25345;&#20302;&#39057;&#29305;&#24449;&#20540;&#19981;&#21464;&#21487;&#20197;&#20445;&#30041;&#20851;&#38190;&#23646;&#24615;&#65292;&#25552;&#20986;&#20102;&#21452;&#26865;&#38236;&#65288;DP&#65289;&#22686;&#24378;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#28789;&#27963;&#22320;&#20445;&#30041;&#20851;&#38190;&#30340;&#22270;&#23646;&#24615;&#21516;&#26102;&#22686;&#21152;&#22270;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24050;&#25104;&#20026;&#22788;&#29702;&#22270;&#25968;&#25454;&#30340;&#39318;&#36873;&#24037;&#20855;&#65292;&#20854;&#36890;&#36807;&#22270;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#30340;&#25552;&#39640;&#21152;&#24378;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#23613;&#31649;&#22686;&#24378;&#26041;&#27861;&#30340;&#21457;&#23637;&#65292;&#20294;&#22270;&#23646;&#24615;&#25197;&#26354;&#21644;&#21463;&#38480;&#32467;&#26500;&#21464;&#21270;&#31561;&#38382;&#39064;&#20173;&#28982;&#23384;&#22312;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#26159;&#21542;&#21487;&#33021;&#24320;&#21457;&#26356;&#21152;&#20445;&#30041;&#23646;&#24615;&#24182;&#20855;&#26377;&#32467;&#26500;&#25935;&#24863;&#24615;&#30340;&#22686;&#24378;&#26041;&#27861;&#65311;&#36890;&#36807;&#20809;&#35889;&#38236;&#22836;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22270;&#23646;&#24615;&#12289;&#23427;&#20204;&#30340;&#22686;&#24378;&#21644;&#23427;&#20204;&#30340;&#20809;&#35889;&#34892;&#20026;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#21457;&#29616;&#20445;&#25345;&#20302;&#39057;&#29305;&#24449;&#20540;&#19981;&#21464;&#21487;&#20197;&#20445;&#25345;&#29983;&#25104;&#30340;&#22686;&#24378;&#22270;&#30340;&#20851;&#38190;&#23646;&#24615;&#12290;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#21551;&#21457;&#25105;&#20204;&#24341;&#20837;&#20102;&#21452;&#26865;&#38236;&#65288;DP&#65289;&#22686;&#24378;&#26041;&#27861;&#65292;&#21253;&#25324;DP-Noise&#21644;DP-Mask&#65292;&#23427;&#20204;&#28789;&#27963;&#22320;&#20445;&#30041;&#20102;&#20851;&#38190;&#30340;&#22270;&#23646;&#24615;&#24182;&#20016;&#23500;&#20102;&#22686;&#24378;&#22270;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20026;&#19968;&#31181;&#26032;&#30340;&#12289;&#26377;&#21069;&#26223;&#30340;&#30452;&#25509;&#26041;&#27861;&#25552;&#20379;&#20102;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have become the preferred tool to process graph data, with their efficacy being boosted through graph data augmentation techniques. Despite the evolution of augmentation methods, issues like graph property distortions and restricted structural changes persist. This leads to the question: Is it possible to develop more property-conserving and structure-sensitive augmentation methods? Through a spectral lens, we investigate the interplay between graph properties, their augmentation, and their spectral behavior, and found that keeping the low-frequency eigenvalues unchanged can preserve the critical properties at a large scale when generating augmented graphs. These observations inform our introduction of the Dual-Prism (DP) augmentation method, comprising DP-Noise and DP-Mask, which adeptly retains essential graph properties while diversifying augmented graphs. Extensive experiments validate the efficiency of our approach, providing a new and promising direct
&lt;/p&gt;</description></item><item><title>SymbolNet&#26159;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#20462;&#21098;&#27169;&#22411;&#26435;&#37325;&#12289;&#36755;&#20837;&#29305;&#24449;&#21644;&#25968;&#23398;&#36816;&#31639;&#31526;&#65292;&#21516;&#26102;&#20248;&#21270;&#35757;&#32451;&#25439;&#22833;&#21644;&#34920;&#36798;&#24335;&#22797;&#26434;&#24615;&#65292;&#23454;&#29616;&#20102;&#31526;&#21495;&#22238;&#24402;&#12290;&#36890;&#36807;&#24341;&#20837;&#31232;&#30095;&#27491;&#21017;&#21270;&#39033;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#33258;&#36866;&#24212;&#35843;&#25972;&#33258;&#36523;&#30340;&#24378;&#24230;&#65292;&#24182;&#25910;&#25947;&#21040;&#30446;&#26631;&#31232;&#30095;&#24230;&#27700;&#24179;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;SymbolNet&#33021;&#39640;&#25928;&#22788;&#29702;&#20855;&#26377;&#36229;&#36807;10&#20010;&#36755;&#20837;&#30340;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2401.09949</link><description>&lt;p&gt;
SymbolNet: &#33258;&#36866;&#24212;&#21160;&#24577;&#20462;&#21098;&#30340;&#31070;&#32463;&#31526;&#21495;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
SymbolNet: Neural Symbolic Regression with Adaptive Dynamic Pruning. (arXiv:2401.09949v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09949
&lt;/p&gt;
&lt;p&gt;
SymbolNet&#26159;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#20462;&#21098;&#27169;&#22411;&#26435;&#37325;&#12289;&#36755;&#20837;&#29305;&#24449;&#21644;&#25968;&#23398;&#36816;&#31639;&#31526;&#65292;&#21516;&#26102;&#20248;&#21270;&#35757;&#32451;&#25439;&#22833;&#21644;&#34920;&#36798;&#24335;&#22797;&#26434;&#24615;&#65292;&#23454;&#29616;&#20102;&#31526;&#21495;&#22238;&#24402;&#12290;&#36890;&#36807;&#24341;&#20837;&#31232;&#30095;&#27491;&#21017;&#21270;&#39033;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#33258;&#36866;&#24212;&#35843;&#25972;&#33258;&#36523;&#30340;&#24378;&#24230;&#65292;&#24182;&#25910;&#25947;&#21040;&#30446;&#26631;&#31232;&#30095;&#24230;&#27700;&#24179;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;SymbolNet&#33021;&#39640;&#25928;&#22788;&#29702;&#20855;&#26377;&#36229;&#36807;10&#20010;&#36755;&#20837;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#36951;&#20256;&#32534;&#31243;&#30340;&#20351;&#29992;&#30456;&#21453;&#65292;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#21487;&#22312;&#39640;&#36755;&#20837;&#32500;&#24230;&#19979;&#26377;&#25928;&#25193;&#23637;&#65292;&#24182;&#21033;&#29992;&#26799;&#24230;&#26041;&#27861;&#21152;&#36895;&#26041;&#31243;&#25628;&#32034;&#12290;&#24120;&#35265;&#30340;&#34920;&#36798;&#24335;&#22797;&#26434;&#24615;&#32422;&#26463;&#26041;&#27861;&#20381;&#36182;&#20110;&#22810;&#38454;&#27573;&#20462;&#21098;&#26041;&#27861;&#36827;&#34892;&#24494;&#35843;&#65292;&#20294;&#36825;&#24448;&#24448;&#20250;&#23548;&#33268;&#26174;&#33879;&#30340;&#24615;&#33021;&#25439;&#22833;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#21363;SymbolNet&#65292;&#20197;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#23454;&#29616;&#31526;&#21495;&#22238;&#24402;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#22312;&#21333;&#20010;&#35757;&#32451;&#20013;&#21160;&#24577;&#20462;&#21098;&#27169;&#22411;&#26435;&#37325;&#12289;&#36755;&#20837;&#29305;&#24449;&#21644;&#25968;&#23398;&#36816;&#31639;&#31526;&#65292;&#21516;&#26102;&#20248;&#21270;&#35757;&#32451;&#25439;&#22833;&#21644;&#34920;&#36798;&#24335;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#27599;&#20010;&#20462;&#21098;&#31867;&#22411;&#30340;&#31232;&#30095;&#27491;&#21017;&#21270;&#39033;&#65292;&#35813;&#39033;&#21487;&#20197;&#33258;&#36866;&#24212;&#35843;&#25972;&#33258;&#36523;&#30340;&#24378;&#24230;&#65292;&#24182;&#23548;&#33268;&#25910;&#25947;&#21040;&#30446;&#26631;&#31232;&#30095;&#24230;&#27700;&#24179;&#12290;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#31526;&#21495;&#22238;&#24402;&#26041;&#27861;&#26080;&#27861;&#39640;&#25928;&#22788;&#29702;&#20855;&#26377;&#36229;&#36807;10&#20010;&#36755;&#20837;&#30340;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrary to the use of genetic programming, the neural network approach to symbolic regression can scale well with high input dimension and leverage gradient methods for faster equation searching. Common ways of constraining expression complexity have relied on multistage pruning methods with fine-tuning, but these often lead to significant performance loss. In this work, we propose SymbolNet, a neural network approach to symbolic regression in a novel framework that enables dynamic pruning of model weights, input features, and mathematical operators in a single training, where both training loss and expression complexity are optimized simultaneously. We introduce a sparsity regularization term per pruning type, which can adaptively adjust its own strength and lead to convergence to a target sparsity level. In contrast to most existing symbolic regression methods that cannot efficiently handle datasets with more than $O$(10) inputs, we demonstrate the effectiveness of our model on the 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;HGAttack&#65292;&#19968;&#31181;&#38024;&#23545;&#24322;&#26500;&#22270;&#30340;&#25915;&#20987;&#26041;&#27861;&#12290;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#25311;&#21512;&#27169;&#22411;&#21644;&#21033;&#29992;&#26799;&#24230;&#29983;&#25104;&#25200;&#21160;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#21033;&#29992;&#24322;&#26500;&#20449;&#24687;&#65292;&#25552;&#39640;&#25915;&#20987;&#30340;&#21487;&#36716;&#31227;&#24615;&#21644;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.09945</link><description>&lt;p&gt;
HGAttack: &#21487;&#36716;&#31227;&#30340;&#24322;&#26500;&#22270;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
HGAttack: Transferable Heterogeneous Graph Adversarial Attack. (arXiv:2401.09945v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09945
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;HGAttack&#65292;&#19968;&#31181;&#38024;&#23545;&#24322;&#26500;&#22270;&#30340;&#25915;&#20987;&#26041;&#27861;&#12290;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#25311;&#21512;&#27169;&#22411;&#21644;&#21033;&#29992;&#26799;&#24230;&#29983;&#25104;&#25200;&#21160;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#21033;&#29992;&#24322;&#26500;&#20449;&#24687;&#65292;&#25552;&#39640;&#25915;&#20987;&#30340;&#21487;&#36716;&#31227;&#24615;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HGNN&#65289;&#22312;&#32593;&#32476;&#21644;&#30005;&#23376;&#21830;&#21153;&#31561;&#39046;&#22495;&#30340;&#24615;&#33021;&#36234;&#26469;&#36234;&#21463;&#21040;&#35748;&#21487;&#65292;&#23545;&#25239;&#25915;&#20987;&#30340;&#38887;&#24615;&#23545;&#20110;&#36825;&#20123;&#39046;&#22495;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#20027;&#35201;&#38024;&#23545;&#21516;&#36136;&#22270;&#35774;&#35745;&#65292;&#24212;&#29992;&#20110;HGNN&#26102;&#65292;&#30001;&#20110;&#20854;&#23545;HGNN&#32467;&#26500;&#21644;&#35821;&#20041;&#22797;&#26434;&#24615;&#30340;&#38480;&#21046;&#65292;&#36825;&#20123;&#26041;&#27861;&#25928;&#26524;&#19981;&#20339;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;HGAttack&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#24322;&#26500;&#22270;&#30340;&#31532;&#19968;&#31181;&#19987;&#29992;&#28784;&#30418;&#36867;&#36991;&#25915;&#20987;&#26041;&#27861;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;&#25311;&#21512;&#27169;&#22411;&#65292;&#20197;&#19982;&#30446;&#26631;HGNN&#30340;&#34892;&#20026;&#32039;&#23494;&#30456;&#20284;&#65292;&#24182;&#21033;&#29992;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#29983;&#25104;&#25200;&#21160;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25152;&#25552;&#20986;&#30340;&#25311;&#21512;&#27169;&#22411;&#36890;&#36807;&#25552;&#21462;&#20803;&#36335;&#24452;&#35825;&#23548;&#30340;&#23376;&#22270;&#24182;&#24212;&#29992;GNN&#26469;&#23398;&#20064;&#27599;&#20010;&#23376;&#22270;&#20013;&#20855;&#26377;&#19981;&#21516;&#35821;&#20041;&#30340;&#33410;&#28857;&#23884;&#20837;&#65292;&#26377;&#25928;&#22320;&#21033;&#29992;&#20102;&#24322;&#26500;&#20449;&#24687;&#12290;&#36825;&#31181;&#26041;&#27861;&#25552;&#39640;&#20102;&#29983;&#25104;&#30340;&#25915;&#20987;&#23545;&#30446;&#26631;HGNN&#30340;&#21487;&#36716;&#31227;&#24615;&#65292;&#24182;&#26174;&#33879;&#20943;&#23569;&#20102;&#25915;&#20987;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous Graph Neural Networks (HGNNs) are increasingly recognized for their performance in areas like the web and e-commerce, where resilience against adversarial attacks is crucial. However, existing adversarial attack methods, which are primarily designed for homogeneous graphs, fall short when applied to HGNNs due to their limited ability to address the structural and semantic complexity of HGNNs. This paper introduces HGAttack, the first dedicated gray box evasion attack method for heterogeneous graphs. We design a novel surrogate model to closely resemble the behaviors of the target HGNN and utilize gradient-based methods for perturbation generation. Specifically, the proposed surrogate model effectively leverages heterogeneous information by extracting meta-path induced subgraphs and applying GNNs to learn node embeddings with distinct semantics from each subgraph. This approach improves the transferability of generated attacks on the target HGNN and significantly reduces m
&lt;/p&gt;</description></item><item><title>WindSeer&#26159;&#19968;&#20010;&#21517;&#20026;WindSeer&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#22312;&#23454;&#26102;&#39044;&#27979;&#20302;&#31354;&#39118;&#30340;&#21516;&#26102;&#33410;&#30465;&#35745;&#31639;&#36164;&#28304;&#12290;&#36890;&#36807;&#20351;&#29992;&#31232;&#30095;&#30340;&#27979;&#37327;&#25968;&#25454;&#21644;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#23427;&#21487;&#20197;&#25104;&#21151;&#22320;&#39044;&#27979;&#24050;&#30693;&#22320;&#24418;&#19978;&#30340;&#30495;&#23454;&#39118;&#22330;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#20998;&#36776;&#29575;&#21644;&#22495;&#22823;&#23567;&#19978;&#29983;&#25104;&#20934;&#30830;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2401.09944</link><description>&lt;p&gt;
WindSeer: &#22312;&#23567;&#22411;&#26080;&#20154;&#26426;&#19978;&#23454;&#26102;&#39044;&#27979;&#22797;&#26434;&#22320;&#24418;&#19978;&#30340;&#20307;&#31215;&#39118;
&lt;/p&gt;
&lt;p&gt;
WindSeer: Real-time volumetric wind prediction over complex terrain aboard a small UAV. (arXiv:2401.09944v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09944
&lt;/p&gt;
&lt;p&gt;
WindSeer&#26159;&#19968;&#20010;&#21517;&#20026;WindSeer&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#22312;&#23454;&#26102;&#39044;&#27979;&#20302;&#31354;&#39118;&#30340;&#21516;&#26102;&#33410;&#30465;&#35745;&#31639;&#36164;&#28304;&#12290;&#36890;&#36807;&#20351;&#29992;&#31232;&#30095;&#30340;&#27979;&#37327;&#25968;&#25454;&#21644;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#23427;&#21487;&#20197;&#25104;&#21151;&#22320;&#39044;&#27979;&#24050;&#30693;&#22320;&#24418;&#19978;&#30340;&#30495;&#23454;&#39118;&#22330;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#20998;&#36776;&#29575;&#21644;&#22495;&#22823;&#23567;&#19978;&#29983;&#25104;&#20934;&#30830;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#26102;&#39640;&#20998;&#36776;&#29575;&#30340;&#39118;&#39044;&#27979;&#23545;&#20110;&#21253;&#25324;&#26377;&#20154;&#21644;&#26080;&#20154;&#33322;&#31354;&#22312;&#20869;&#30340;&#21508;&#31181;&#24212;&#29992;&#37117;&#24456;&#26377;&#30410;&#12290;&#24403;&#21069;&#30340;&#22825;&#27668;&#27169;&#22411;&#38656;&#35201;&#22826;&#22810;&#30340;&#35745;&#31639;&#65292;&#24182;&#19988;&#32570;&#20047;&#24517;&#35201;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#22240;&#20026;&#23427;&#20204;&#20165;&#22312;&#22810;&#21315;&#31859;&#21644;&#20960;&#23567;&#26102;&#30340;&#23610;&#24230;&#19978;&#26377;&#25928; - &#36825;&#36828;&#20302;&#20110;&#36825;&#20123;&#24212;&#29992;&#25152;&#38656;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20998;&#36776;&#29575;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#39318;&#27425;&#23637;&#31034;&#20102;&#22312;&#26377;&#38480;&#35745;&#31639;&#35774;&#22791;&#19978;&#23454;&#26102;&#39044;&#27979;&#20302;&#31354;&#39118;&#30340;&#33021;&#21147;&#65292;&#20165;&#20351;&#29992;&#31232;&#30095;&#30340;&#27979;&#37327;&#25968;&#25454;&#35757;&#32451;&#20102;&#21517;&#20026;WindSeer&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#20351;&#29992;&#35745;&#31639;&#27969;&#20307;&#21147;&#23398;&#27169;&#25311;&#30340;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#23637;&#31034;&#23427;&#21487;&#20197;&#25104;&#21151;&#22320;&#20174;&#20165;&#26377;&#23569;&#37327;&#30340;&#22122;&#22768;&#21644;&#31354;&#38388;&#32858;&#38598;&#30340;&#39118;&#27979;&#37327;&#25968;&#25454;&#20013;&#39044;&#27979;&#20986;&#24050;&#30693;&#22320;&#24418;&#19978;&#30340;&#30495;&#23454;&#39118;&#22330;&#12290;WindSeer&#21487;&#20197;&#22312;&#20043;&#21069;&#26410;&#35265;&#36807;&#30340;&#22320;&#24418;&#19978;&#20197;&#19981;&#21516;&#30340;&#20998;&#36776;&#29575;&#21644;&#22495;&#22823;&#23567;&#29983;&#25104;&#20934;&#30830;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#25104;&#21151;&#39044;&#27979;&#20102;&#21382;&#21490;&#39118;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-time high-resolution wind predictions are beneficial for various applications including safe manned and unmanned aviation. Current weather models require too much compute and lack the necessary predictive capabilities as they are valid only at the scale of multiple kilometers and hours - much lower spatial and temporal resolutions than these applications require. Our work, for the first time, demonstrates the ability to predict low-altitude wind in real-time on limited-compute devices, from only sparse measurement data. We train a neural network, WindSeer, using only synthetic data from computational fluid dynamics simulations and show that it can successfully predict real wind fields over terrain with known topography from just a few noisy and spatially clustered wind measurements. WindSeer can generate accurate predictions at different resolutions and domain sizes on previously unseen topography without retraining. We demonstrate that the model successfully predicts historical w
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#24130;&#28388;&#27874;&#22120;&#31070;&#32463;&#32593;&#32476; (GPFN)&#65292;&#36890;&#36807;&#20351;&#29992;&#24130;&#32423;&#25968;&#22270;&#28388;&#27874;&#22120;&#26469;&#22686;&#24378;&#33410;&#28857;&#20998;&#31867;&#12290;GPFN&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#25910;&#25947;&#24130;&#32423;&#25968;&#30340;&#20855;&#26377;&#26080;&#38480;&#25509;&#25910;&#22495;&#30340;&#22270;&#28388;&#27874;&#22120;&#26500;&#24314;&#26041;&#27861;&#65292;&#24182;&#33021;&#38598;&#25104;&#20219;&#20309;&#24130;&#32423;&#25968;&#24182;&#25429;&#25417;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2401.09943</link><description>&lt;p&gt;
&#26080;&#38480;&#26102;&#22495;&#22270;&#28388;&#27874;&#22120;&#65306;&#21033;&#29992;&#24130;&#32423;&#25968;&#22686;&#24378;&#31232;&#30095;&#20449;&#24687;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
Infinite-Horizon Graph Filters: Leveraging Power Series to Enhance Sparse Information Aggregation. (arXiv:2401.09943v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09943
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#24130;&#28388;&#27874;&#22120;&#31070;&#32463;&#32593;&#32476; (GPFN)&#65292;&#36890;&#36807;&#20351;&#29992;&#24130;&#32423;&#25968;&#22270;&#28388;&#27874;&#22120;&#26469;&#22686;&#24378;&#33410;&#28857;&#20998;&#31867;&#12290;GPFN&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#25910;&#25947;&#24130;&#32423;&#25968;&#30340;&#20855;&#26377;&#26080;&#38480;&#25509;&#25910;&#22495;&#30340;&#22270;&#28388;&#27874;&#22120;&#26500;&#24314;&#26041;&#27861;&#65292;&#24182;&#33021;&#38598;&#25104;&#20219;&#20309;&#24130;&#32423;&#25968;&#24182;&#25429;&#25417;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#31181;&#22270;&#23398;&#20064;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#30456;&#24403;&#30340;&#26377;&#25928;&#24615;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;&#28040;&#24687;&#20256;&#36882;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#24120;&#24120;&#21463;&#21040;&#26377;&#38480;&#25509;&#25910;&#22495;&#30340;&#38480;&#21046;&#65292;&#22312;&#31232;&#30095;&#22270;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#25361;&#25112;&#21464;&#24471;&#26356;&#21152;&#20005;&#23803;&#12290;&#37492;&#20110;&#20855;&#26377;&#26080;&#38480;&#25193;&#23637;&#33021;&#21147;&#30340;&#24130;&#32423;&#25968;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#24130;&#28388;&#27874;&#22120;&#31070;&#32463;&#32593;&#32476; (GPFN)&#65292;&#36890;&#36807;&#20351;&#29992;&#24130;&#32423;&#25968;&#22270;&#28388;&#27874;&#22120;&#26469;&#22686;&#24378;&#33410;&#28857;&#20998;&#31867;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;GPFN&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#25910;&#25947;&#24130;&#32423;&#25968;&#30340;&#20855;&#26377;&#26080;&#38480;&#25509;&#25910;&#22495;&#30340;&#22270;&#28388;&#27874;&#22120;&#26500;&#24314;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#39057;&#35889;&#21644;&#31354;&#38388;&#22495;&#20013;&#36827;&#34892;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;GPFN&#26159;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#21487;&#20197;&#38598;&#25104;&#20219;&#20309;&#24130;&#32423;&#25968;&#24182;&#25429;&#25417;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#23637;&#31034;&#20102;GPFN&#22312;&#31232;&#30095;&#22270;&#19978;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have shown considerable effectiveness in a variety of graph learning tasks, particularly those based on the message-passing approach in recent years. However, their performance is often constrained by a limited receptive field, a challenge that becomes more acute in the presence of sparse graphs. In light of the power series, which possesses infinite expansion capabilities, we propose a novel \underline{G}raph \underline{P}ower \underline{F}ilter \underline{N}eural Network (GPFN) that enhances node classification by employing a power series graph filter to augment the receptive field. Concretely, our GPFN designs a new way to build a graph filter with an infinite receptive field based on the convergence power series, which can be analyzed in the spectral and spatial domains. Besides, we theoretically prove that our GPFN is a general framework that can integrate any power series and capture long-range dependencies. Finally, experimental results on three data
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#20351;&#29992;&#26399;&#26395;&#36827;&#29699;&#65288;xG&#65289;&#32479;&#35745;&#35780;&#20272;&#23556;&#38376;&#33021;&#21147;&#26102;&#30340;&#38480;&#21046;&#21644;&#20559;&#35265;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#25345;&#32493;&#36229;&#20986;&#32047;&#31215;xG&#38656;&#35201;&#39640;&#23556;&#38376;&#39057;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.09940</link><description>&lt;p&gt;
&#26399;&#26395;&#36827;&#29699;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#24433;&#21709;&#20102;&#23556;&#38376;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Biases in Expected Goals Models Confound Finishing Ability. (arXiv:2401.09940v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09940
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#20351;&#29992;&#26399;&#26395;&#36827;&#29699;&#65288;xG&#65289;&#32479;&#35745;&#35780;&#20272;&#23556;&#38376;&#33021;&#21147;&#26102;&#30340;&#38480;&#21046;&#21644;&#20559;&#35265;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#25345;&#32493;&#36229;&#20986;&#32047;&#31215;xG&#38656;&#35201;&#39640;&#23556;&#38376;&#39057;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26399;&#26395;&#36827;&#29699;&#65288;xG&#65289;&#24050;&#25104;&#20026;&#35780;&#20272;&#36275;&#29699;&#20998;&#26512;&#20013;&#23556;&#38376;&#25216;&#33021;&#30340;&#19968;&#31181;&#24120;&#29992;&#24037;&#20855;&#12290;&#23427;&#28041;&#21450;&#23558;&#29699;&#21592;&#30340;&#32047;&#31215;xG&#19982;&#23454;&#38469;&#36827;&#29699;&#25968;&#37327;&#36827;&#34892;&#27604;&#36739;&#65292;&#22914;&#26524;&#25345;&#32493;&#34920;&#29616;&#36229;&#20986;&#39044;&#26399;&#65292;&#21017;&#34920;&#26126;&#23556;&#38376;&#33021;&#21147;&#24378;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;xG&#32479;&#35745;&#35780;&#20272;&#36275;&#29699;&#23556;&#38376;&#25216;&#33021;&#20173;&#23384;&#22312;&#20105;&#35758;&#65292;&#22240;&#20026;&#29699;&#21592;&#24456;&#38590;&#22312;&#25345;&#32493;&#34920;&#29616;&#20013;&#36229;&#20986;&#32047;&#31215;xG&#12290;&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#20351;&#29992;xG&#32479;&#35745;&#35780;&#20272;&#23556;&#38376;&#33021;&#21147;&#26102;&#30340;&#38480;&#21046;&#21644;&#32454;&#24494;&#24046;&#21035;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19977;&#20010;&#20551;&#35774;&#65306;&#65288;1&#65289;&#23454;&#38469;&#36827;&#29699;&#21644;&#39044;&#26399;&#36827;&#29699;&#20043;&#38388;&#30340;&#20559;&#24046;&#26159;&#19968;&#20010;&#19981;&#36275;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#22240;&#20026;&#23556;&#38376;&#32467;&#26524;&#30340;&#24046;&#24322;&#21644;&#26679;&#26412;&#37327;&#26377;&#38480;&#65292;&#65288;2&#65289;&#22312;&#32047;&#31215;xG&#35745;&#31639;&#20013;&#21253;&#21547;&#25152;&#26377;&#23556;&#38376;&#21487;&#33021;&#19981;&#21512;&#36866;&#65292;&#24182;&#19988;&#65288;3&#65289;xG&#27169;&#22411;&#20013;&#23384;&#22312;&#25968;&#25454;&#30456;&#20851;&#24615;&#24341;&#36215;&#30340;&#20559;&#35265;&#65292;&#24433;&#21709;&#20102;&#25216;&#33021;&#27979;&#37327;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25345;&#32493;&#36229;&#20986;&#32047;&#31215;xG&#38656;&#35201;&#39640;&#23556;&#38376;&#39057;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Expected Goals (xG) has emerged as a popular tool for evaluating finishing skill in soccer analytics. It involves comparing a player's cumulative xG with their actual goal output, where consistent overperformance indicates strong finishing ability. However, the assessment of finishing skill in soccer using xG remains contentious due to players' difficulty in consistently outperforming their cumulative xG. In this paper, we aim to address the limitations and nuances surrounding the evaluation of finishing skill using xG statistics. Specifically, we explore three hypotheses: (1) the deviation between actual and expected goals is an inadequate metric due to the high variance of shot outcomes and limited sample sizes, (2) the inclusion of all shots in cumulative xG calculation may be inappropriate, and (3) xG models contain biases arising from interdependencies in the data that affect skill measurement. We found that sustained overperformance of cumulative xG requires both high shot volume
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#27010;&#29575;&#24615;&#30495;&#27491;&#26080;&#24207;&#35268;&#21017;&#38598;&#65288;TURS&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#35268;&#21017;&#38598;&#23398;&#20064;&#20013;&#30340;&#19977;&#20010;&#32570;&#28857;&#65306;&#24378;&#21152;&#39034;&#24207;&#12289;&#37325;&#21472;&#20914;&#31361;&#21644;&#22810;&#31867;&#21035;&#30446;&#26631;&#20998;&#31867;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#35268;&#21017;&#38598;&#30340;&#27010;&#29575;&#29305;&#24615;&#26469;&#35299;&#20915;&#37325;&#21472;&#20914;&#31361;&#65292;&#24182;&#24418;&#24335;&#21270;&#23450;&#20041;&#23398;&#20064;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.09918</link><description>&lt;p&gt;
&#27010;&#29575;&#24615;&#30495;&#27491;&#26080;&#24207;&#35268;&#21017;&#38598;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Truly Unordered Rule Sets. (arXiv:2401.09918v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#27010;&#29575;&#24615;&#30495;&#27491;&#26080;&#24207;&#35268;&#21017;&#38598;&#65288;TURS&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#35268;&#21017;&#38598;&#23398;&#20064;&#20013;&#30340;&#19977;&#20010;&#32570;&#28857;&#65306;&#24378;&#21152;&#39034;&#24207;&#12289;&#37325;&#21472;&#20914;&#31361;&#21644;&#22810;&#31867;&#21035;&#30446;&#26631;&#20998;&#31867;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#35268;&#21017;&#38598;&#30340;&#27010;&#29575;&#29305;&#24615;&#26469;&#35299;&#20915;&#37325;&#21472;&#20914;&#31361;&#65292;&#24182;&#24418;&#24335;&#21270;&#23450;&#20041;&#23398;&#20064;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20154;&#20204;&#32463;&#24120;&#37325;&#35270;&#35268;&#21017;&#38598;&#23398;&#20064;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#23384;&#22312;&#19968;&#20123;&#32570;&#28857;&#12290;&#39318;&#20808;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#22312;&#35268;&#21017;&#20043;&#38388;&#26126;&#30830;&#25110;&#38544;&#21547;&#22320;&#24378;&#21152;&#39034;&#24207;&#65292;&#36825;&#20351;&#24471;&#27169;&#22411;&#26356;&#38590;&#20197;&#29702;&#35299;&#12290;&#20854;&#27425;&#65292;&#30001;&#20110;&#22788;&#29702;&#37325;&#21472;&#24341;&#36215;&#30340;&#20914;&#31361;&#65288;&#21363;&#65292;&#34987;&#22810;&#20010;&#35268;&#21017;&#35206;&#30422;&#30340;&#23454;&#20363;&#65289;&#30340;&#22256;&#38590;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#19981;&#32771;&#34385;&#27010;&#29575;&#35268;&#21017;&#12290;&#31532;&#19977;&#65292;&#23545;&#20110;&#22810;&#31867;&#21035;&#30446;&#26631;&#30340;&#23398;&#20064;&#20998;&#31867;&#35268;&#21017;&#30740;&#31350;&#19981;&#36275;&#65292;&#22240;&#20026;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#19987;&#27880;&#20110;&#20108;&#20998;&#31867;&#25110;&#36890;&#36807;"&#19968;&#23545;&#20854;&#20313;"&#26041;&#27861;&#36827;&#34892;&#22810;&#31867;&#21035;&#20998;&#31867;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#32570;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TURS&#65292;&#21363;&#30495;&#27491;&#26080;&#24207;&#35268;&#21017;&#38598;&#12290;&#20026;&#20102;&#35299;&#20915;&#37325;&#21472;&#35268;&#21017;&#24341;&#36215;&#30340;&#20914;&#31361;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#25105;&#20204;&#30340;&#35268;&#21017;&#38598;&#30340;&#27010;&#29575;&#29305;&#24615;&#65292;&#21482;&#26377;&#24403;&#23427;&#20204;&#20855;&#26377;&#30456;&#20284;&#30340;&#27010;&#29575;&#36755;&#20986;&#26102;&#20801;&#35768;&#35268;&#21017;&#37325;&#21472;&#12290;&#25105;&#20204;&#25509;&#19979;&#26469;&#23545;&#23398;&#20064;&#38382;&#39064;&#36827;&#34892;&#20102;&#24418;&#24335;&#21270;&#23450;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rule set learning has recently been frequently revisited because of its interpretability. Existing methods have several shortcomings though. First, most existing methods impose orders among rules, either explicitly or implicitly, which makes the models less comprehensible. Second, due to the difficulty of handling conflicts caused by overlaps (i.e., instances covered by multiple rules), existing methods often do not consider probabilistic rules. Third, learning classification rules for multi-class target is understudied, as most existing methods focus on binary classification or multi-class classification via the ``one-versus-rest" approach.  To address these shortcomings, we propose TURS, for Truly Unordered Rule Sets. To resolve conflicts caused by overlapping rules, we propose a novel model that exploits the probabilistic properties of our rule sets, with the intuition of only allowing rules to overlap if they have similar probabilistic outputs. We next formalize the problem of lear
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#25345;&#32493;&#23398;&#20064;&#21644;&#20108;&#20540;&#31070;&#32463;&#32593;&#32476;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#35774;&#22791;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#20445;&#25345;&#31454;&#20105;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20108;&#20540;&#28608;&#27963;&#30340;&#37325;&#25918;&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#37327;&#21270;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2401.09916</link><description>&lt;p&gt;
&#20351;&#29992;&#20108;&#20540;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#35774;&#22791;&#19978;&#30340;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Enabling On-device Continual Learning with Binary Neural Networks. (arXiv:2401.09916v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#25345;&#32493;&#23398;&#20064;&#21644;&#20108;&#20540;&#31070;&#32463;&#32593;&#32476;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#35774;&#22791;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#20445;&#25345;&#31454;&#20105;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20108;&#20540;&#28608;&#27963;&#30340;&#37325;&#25918;&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#37327;&#21270;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#22791;&#19978;&#30340;&#23398;&#20064;&#20173;&#28982;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#24403;&#22788;&#29702;&#36164;&#28304;&#21463;&#38480;&#30340;&#35774;&#22791;&#26102;&#65292;&#36825;&#20123;&#35774;&#22791;&#30340;&#35745;&#31639;&#33021;&#21147;&#26377;&#38480;&#12290;&#36825;&#20010;&#25361;&#25112;&#20027;&#35201;&#28304;&#20110;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#39318;&#20808;&#65292;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#21487;&#29992;&#30340;&#20869;&#23384;&#36890;&#24120;&#19981;&#36275;&#20197;&#23481;&#32435;&#28040;&#32791;&#20869;&#23384;&#30340;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#28014;&#28857;&#31934;&#24230;&#12290;&#20854;&#27425;&#65292;&#23545;&#20110;&#20855;&#26377;&#26497;&#31471;&#37327;&#21270;&#32423;&#21035;&#30340;&#27169;&#22411;&#65288;&#22914;&#20108;&#20540;&#31070;&#32463;&#32593;&#32476;&#65289;&#65292;&#24320;&#21457;&#23398;&#20064;&#31639;&#27861;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20204;&#23545;&#20301;&#34920;&#31034;&#36827;&#34892;&#20102;&#21095;&#28872;&#20943;&#23569;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#32467;&#21512;&#20102;&#25345;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#21644;&#20108;&#20540;&#31070;&#32463;&#32593;&#32476;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#20197;&#22312;&#35774;&#22791;&#19978;&#36827;&#34892;&#35757;&#32451;&#21516;&#26102;&#20445;&#25345;&#31454;&#20105;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#20108;&#20540;&#28608;&#27963;&#30340;&#37325;&#25918;&#65288;LR&#65289;&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#37327;&#21270;&#26041;&#26696;&#65292;&#21487;&#26174;&#30528;&#20943;&#23569;&#29992;&#20110;&#26799;&#24230;&#35745;&#31639;&#30340;&#20301;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
On-device learning remains a formidable challenge, especially when dealing with resource-constrained devices that have limited computational capabilities. This challenge is primarily rooted in two key issues: first, the memory available on embedded devices is typically insufficient to accommodate the memory-intensive back-propagation algorithm, which often relies on floating-point precision. Second, the development of learning algorithms on models with extreme quantization levels, such as Binary Neural Networks (BNNs), is critical due to the drastic reduction in bit representation. In this study, we propose a solution that combines recent advancements in the field of Continual Learning (CL) and Binary Neural Networks to enable on-device training while maintaining competitive performance. Specifically, our approach leverages binary latent replay (LR) activations and a novel quantization scheme that significantly reduces the number of bits required for gradient computation. The experimen
&lt;/p&gt;</description></item><item><title>Qadence&#26159;&#19968;&#20010;&#39640;&#32423;&#32534;&#31243;&#25509;&#21475;&#65292;&#29992;&#20110;&#26500;&#24314;&#25968;&#23383;&#27169;&#25311;&#37327;&#23376;&#31243;&#24207;&#12290;&#23427;&#20855;&#26377;&#28789;&#27963;&#30340;&#25509;&#21475;&#12289;&#26412;&#22320;&#21487;&#24494;&#24615;&#21644;&#23545;&#30495;&#23454;&#35774;&#22791;&#25191;&#34892;&#30340;&#20851;&#27880;&#65292;&#20419;&#36827;&#20102;&#21407;&#29983;DAQC&#24179;&#21488;&#19978;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2401.09915</link><description>&lt;p&gt;
Qadence: &#19968;&#20010;&#29992;&#20110;&#25968;&#23383;&#27169;&#25311;&#31243;&#24207;&#30340;&#21487;&#24494;&#20998;&#25509;&#21475;
&lt;/p&gt;
&lt;p&gt;
Qadence: a differentiable interface for digital-analog programs. (arXiv:2401.09915v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09915
&lt;/p&gt;
&lt;p&gt;
Qadence&#26159;&#19968;&#20010;&#39640;&#32423;&#32534;&#31243;&#25509;&#21475;&#65292;&#29992;&#20110;&#26500;&#24314;&#25968;&#23383;&#27169;&#25311;&#37327;&#23376;&#31243;&#24207;&#12290;&#23427;&#20855;&#26377;&#28789;&#27963;&#30340;&#25509;&#21475;&#12289;&#26412;&#22320;&#21487;&#24494;&#24615;&#21644;&#23545;&#30495;&#23454;&#35774;&#22791;&#25191;&#34892;&#30340;&#20851;&#27880;&#65292;&#20419;&#36827;&#20102;&#21407;&#29983;DAQC&#24179;&#21488;&#19978;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#27169;&#25311;&#37327;&#23376;&#35745;&#31639;&#65288;DAQC&#65289;&#26159;&#19968;&#31181;&#23558;&#25968;&#23383;&#21333;&#37327;&#23376;&#27604;&#29305;&#38376;&#19982;&#20316;&#29992;&#20110;&#30456;&#20114;&#20316;&#29992;&#37327;&#23376;&#27604;&#29305;&#23492;&#23384;&#22120;&#30340;&#20840;&#23616;&#27169;&#25311;&#25805;&#20316;&#30456;&#32467;&#21512;&#30340;&#36890;&#29992;&#37327;&#23376;&#35745;&#31639;&#30340;&#26367;&#20195;&#33539;&#24335;&#12290;&#30446;&#21069;&#65292;&#27809;&#26377;&#20219;&#20309;&#21487;&#29992;&#30340;&#24320;&#28304;&#36719;&#20214;&#36866;&#29992;&#20110;&#34920;&#36798;&#12289;&#21306;&#20998;&#21644;&#25191;&#34892;DAQC&#33539;&#24335;&#20869;&#30340;&#31243;&#24207;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20171;&#32461;Qadence&#65292;&#19968;&#20010;&#22312;Pasqal&#24320;&#21457;&#30340;&#29992;&#20110;&#26500;&#24314;&#22797;&#26434;&#25968;&#23383;&#27169;&#25311;&#37327;&#23376;&#31243;&#24207;&#30340;&#39640;&#32423;&#32534;&#31243;&#25509;&#21475;&#65292;&#26469;&#35299;&#20915;&#36825;&#20010;&#19981;&#36275;&#12290;&#30001;&#20110;&#20854;&#28789;&#27963;&#30340;&#25509;&#21475;&#12289;&#26412;&#22320;&#21487;&#24494;&#24615;&#21644;&#23545;&#30495;&#23454;&#35774;&#22791;&#25191;&#34892;&#30340;&#20851;&#27880;&#65292;Qadence&#33268;&#21147;&#20110;&#25512;&#36827;&#20026;Rydberg&#21407;&#23376;&#38453;&#21015;&#31561;&#21407;&#29983;DAQC&#24179;&#21488;&#26500;&#24314;&#30340;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Digital-analog quantum computing (DAQC) is an alternative paradigm for universal quantum computation combining digital single-qubit gates with global analog operations acting on a register of interacting qubits. Currently, no available open-source software is tailored to express, differentiate, and execute programs within the DAQC paradigm. In this work, we address this shortfall by presenting Qadence, a high-level programming interface for building complex digital-analog quantum programs developed at Pasqal. Thanks to its flexible interface, native differentiability, and focus on real-device execution, Qadence aims at advancing research on variational quantum algorithms built for native DAQC platforms such as Rydberg atom arrays.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;ODE&#25554;&#20540;&#20013;&#28145;&#24230;&#21644;&#23485;&#24230;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#21457;&#29616;&#22312;&#25968;&#25454;&#38598;&#25554;&#20540;&#20013;&#23384;&#22312;&#30528;$p$&#21644;$L$&#20043;&#38388;&#30340;&#24179;&#34913;&#25240;&#34935;&#20851;&#31995;&#65292;&#32780;&#22312;&#27979;&#24230;&#25554;&#20540;&#20013;&#65292;$L$&#30340;&#22686;&#38271;&#19982;$p$&#21644;$\varepsilon$&#30340;&#20851;&#31995;&#26377;&#20851;&#12290;</title><link>http://arxiv.org/abs/2401.09902</link><description>&lt;p&gt;
&#28145;&#24230;&#21644;&#23485;&#24230;&#22312;&#31070;&#32463;ODE&#25554;&#20540;&#20013;&#30340;&#30456;&#20114;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Interplay between depth and width for interpolation in neural ODEs. (arXiv:2401.09902v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09902
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;ODE&#25554;&#20540;&#20013;&#28145;&#24230;&#21644;&#23485;&#24230;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#21457;&#29616;&#22312;&#25968;&#25454;&#38598;&#25554;&#20540;&#20013;&#23384;&#22312;&#30528;$p$&#21644;$L$&#20043;&#38388;&#30340;&#24179;&#34913;&#25240;&#34935;&#20851;&#31995;&#65292;&#32780;&#22312;&#27979;&#24230;&#25554;&#20540;&#20013;&#65292;$L$&#30340;&#22686;&#38271;&#19982;$p$&#21644;$\varepsilon$&#30340;&#20851;&#31995;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;(neural ODEs)&#24050;&#32463;&#25104;&#20026;&#20174;&#25511;&#21046;&#35282;&#24230;&#36827;&#34892;&#30417;&#30563;&#23398;&#20064;&#30340;&#33258;&#28982;&#24037;&#20855;&#65292;&#28982;&#32780;&#23545;&#20854;&#26368;&#20339;&#32467;&#26500;&#30340;&#23436;&#20840;&#29702;&#35299;&#20173;&#28982;&#38590;&#20197;&#25417;&#25720;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23485;&#24230;$p$&#21644;&#23618;&#20043;&#38388;&#30340;&#36807;&#28193;&#27425;&#25968;$L$&#65288;&#23454;&#38469;&#19978;&#26159;&#28145;&#24230;$L+1$&#65289;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#20197;&#20854;&#33021;&#22815;&#22312;Wasserstein&#35823;&#24046;&#36793;&#30028;$\varepsilon&gt;0$&#20869;&#25554;&#20540;&#19968;&#20010;&#21253;&#21547;$N$&#23545;&#28857;&#30340;&#26377;&#38480;&#25968;&#25454;&#38598;$D$&#25110;&#20004;&#20010;&#27010;&#29575;&#27979;&#24230;&#22312;$\mathbb{R}^d$&#20013;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25581;&#31034;&#20102;$p$&#21644;$L$&#20043;&#38388;&#30340;&#24179;&#34913;&#25240;&#34935;&#20851;&#31995;&#65292;&#22312;&#25968;&#25454;&#38598;&#25554;&#20540;&#20013;&#65292;$L$&#38543;&#30528;$O(1+N/p)$&#30340;&#27604;&#20363;&#22686;&#38271;&#65292;&#32780;&#22312;&#27979;&#24230;&#25554;&#20540;&#20013;&#65292;$L=O\left(1+(p\varepsilon^d)^{-1}\right)$&#12290;&#22312;&#33258;&#20027;&#24773;&#20917;&#19979;&#65292;$L=0$&#65292;&#38656;&#35201;&#36827;&#34892;&#21333;&#29420;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#23558;&#37325;&#28857;&#20851;&#27880;&#25968;&#25454;&#38598;&#25554;&#20540;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;$\varepsilon$-&#36817;&#20284;&#25511;&#21046;&#24615;&#30340;&#25918;&#26494;&#38382;&#39064;&#65292;&#24182;&#24314;&#31435;&#20102;&#35823;&#24046;
&lt;/p&gt;
&lt;p&gt;
Neural ordinary differential equations (neural ODEs) have emerged as a natural tool for supervised learning from a control perspective, yet a complete understanding of their optimal architecture remains elusive. In this work, we examine the interplay between their width $p$ and number of layer transitions $L$ (effectively the depth $L+1$). Specifically, we assess the model expressivity in terms of its capacity to interpolate either a finite dataset $D$ comprising $N$ pairs of points or two probability measures in $\mathbb{R}^d$ within a Wasserstein error margin $\varepsilon&gt;0$. Our findings reveal a balancing trade-off between $p$ and $L$, with $L$ scaling as $O(1+N/p)$ for dataset interpolation, and $L=O\left(1+(p\varepsilon^d)^{-1}\right)$ for measure interpolation.  In the autonomous case, where $L=0$, a separate study is required, which we undertake focusing on dataset interpolation. We address the relaxed problem of $\varepsilon$-approximate controllability and establish an error 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35770;&#25991;&#35843;&#26597;&#20102;&#29992;&#20110;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#21644;&#33021;&#37327;&#25928;&#29575;&#30340;&#30828;&#20214;&#21152;&#36895;&#22120;&#65292;&#24182;&#23545;&#22810;&#31181;&#21152;&#36895;&#22120;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#12289;&#24037;&#31243;&#24072;&#21644;&#20915;&#31574;&#32773;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37096;&#32626;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2401.09890</link><description>&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30828;&#20214;&#21152;&#36895;&#22120;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Hardware Accelerators for Large Language Models. (arXiv:2401.09890v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09890
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35770;&#25991;&#35843;&#26597;&#20102;&#29992;&#20110;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#21644;&#33021;&#37327;&#25928;&#29575;&#30340;&#30828;&#20214;&#21152;&#36895;&#22120;&#65292;&#24182;&#23545;&#22810;&#31181;&#21152;&#36895;&#22120;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#12289;&#24037;&#31243;&#24072;&#21644;&#20915;&#31574;&#32773;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37096;&#32626;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#36890;&#36807;&#20854;&#29702;&#35299;&#21644;&#29983;&#25104;&#31867;&#20284;&#20110;&#20154;&#31867;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#23427;&#20204;&#27491;&#22312;&#20026;&#35813;&#39046;&#22495;&#24102;&#26469;&#38761;&#21629;&#24615;&#21464;&#38761;&#12290;&#38543;&#30528;&#23545;&#26356;&#22797;&#26434;LLMs&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#38271;&#65292;&#36843;&#20999;&#38656;&#35201;&#35299;&#20915;&#19982;&#20854;&#35268;&#27169;&#21644;&#22797;&#26434;&#24615;&#30456;&#20851;&#30340;&#35745;&#31639;&#25361;&#25112;&#12290;&#26412;&#25991;&#38024;&#23545;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#21644;&#33021;&#37327;&#25928;&#29575;&#30340;&#30828;&#20214;&#21152;&#36895;&#22120;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#12290;&#36890;&#36807;&#23545;&#21253;&#25324;GPU&#12289;FPGA&#21644;&#23450;&#21046;&#26550;&#26500;&#22312;&#20869;&#30340;&#21508;&#31181;&#21152;&#36895;&#22120;&#36827;&#34892;&#30740;&#31350;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#26088;&#22312;&#28385;&#36275;LLMs&#30340;&#29420;&#29305;&#35745;&#31639;&#38656;&#27714;&#30340;&#30828;&#20214;&#35299;&#20915;&#26041;&#26696;&#30340;&#26684;&#23616;&#12290;&#26412;&#35843;&#26597;&#28085;&#30422;&#20102;&#23545;&#26550;&#26500;&#12289;&#24615;&#33021;&#25351;&#26631;&#21644;&#33021;&#37327;&#25928;&#29575;&#32771;&#34385;&#30340;&#28145;&#20837;&#20998;&#26512;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#12289;&#24037;&#31243;&#24072;&#21644;&#20915;&#31574;&#32773;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20248;&#21270;LLMs&#30340;&#37096;&#32626;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have emerged as powerful tools for natural language processing tasks, revolutionizing the field with their ability to understand and generate human-like text. As the demand for more sophisticated LLMs continues to grow, there is a pressing need to address the computational challenges associated with their scale and complexity. This paper presents a comprehensive survey on hardware accelerators designed to enhance the performance and energy efficiency of Large Language Models. By examining a diverse range of accelerators, including GPUs, FPGAs, and custom-designed architectures, we explore the landscape of hardware solutions tailored to meet the unique computational demands of LLMs. The survey encompasses an in-depth analysis of architecture, performance metrics, and energy efficiency considerations, providing valuable insights for researchers, engineers, and decision-makers aiming to optimize the deployment of LLMs in real-world applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24377;&#24615;&#32852;&#37030;&#21644;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#21512;&#20316;&#36793;&#32536;&#32531;&#23384;&#26041;&#26696;&#65292;&#36890;&#36807;&#35757;&#32451;&#20010;&#24615;&#21270;&#30340;&#26412;&#22320;&#27169;&#22411;&#65292;&#39044;&#27979;&#20934;&#30830;&#21463;&#27426;&#36814;&#30340;&#20869;&#23481;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;SBS&#20043;&#38388;&#21512;&#20316;&#32531;&#23384;&#28909;&#38376;&#20869;&#23481;&#65292;&#20197;&#36798;&#21040;&#20248;&#21270;&#33719;&#21462;&#20869;&#23481;&#25104;&#26412;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2401.09886</link><description>&lt;p&gt;
&#22522;&#20110;&#24377;&#24615;&#32852;&#37030;&#21644;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#19979;&#19968;&#20195;&#32593;&#32476;&#21512;&#20316;&#36793;&#32536;&#32531;&#23384;
&lt;/p&gt;
&lt;p&gt;
Cooperative Edge Caching Based on Elastic Federated and Multi-Agent Deep Reinforcement Learning in Next-Generation Network. (arXiv:2401.09886v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24377;&#24615;&#32852;&#37030;&#21644;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#21512;&#20316;&#36793;&#32536;&#32531;&#23384;&#26041;&#26696;&#65292;&#36890;&#36807;&#35757;&#32451;&#20010;&#24615;&#21270;&#30340;&#26412;&#22320;&#27169;&#22411;&#65292;&#39044;&#27979;&#20934;&#30830;&#21463;&#27426;&#36814;&#30340;&#20869;&#23481;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;SBS&#20043;&#38388;&#21512;&#20316;&#32531;&#23384;&#28909;&#38376;&#20869;&#23481;&#65292;&#20197;&#36798;&#21040;&#20248;&#21270;&#33719;&#21462;&#20869;&#23481;&#25104;&#26412;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36793;&#32536;&#32531;&#23384;&#26159;&#19979;&#19968;&#20195;&#32593;&#32476;&#20013;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#36171;&#20104;&#23567;&#22411;&#22522;&#31449;&#65288;SBS&#65289;&#20013;&#30340;&#32531;&#23384;&#21333;&#20803;&#36171;&#33021;&#65292;&#20801;&#35768;&#29992;&#25143;&#35774;&#22791;&#65288;UE&#65289;&#33719;&#21462;&#24050;&#22312;SBS&#20013;&#39044;&#32531;&#23384;&#30340;&#29992;&#25143;&#35831;&#27714;&#20869;&#23481;&#12290;&#23545;&#20110;SBS&#26469;&#35828;&#65292;&#36890;&#36807;&#23398;&#20064;&#20934;&#30830;&#39044;&#27979;&#21463;&#27426;&#36814;&#30340;&#20869;&#23481;&#38750;&#24120;&#20851;&#38190;&#65292;&#21516;&#26102;&#20445;&#25252;&#29992;&#25143;&#20010;&#20154;&#20449;&#24687;&#12290;&#20256;&#32479;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#21487;&#20197;&#20445;&#25252;&#29992;&#25143;&#30340;&#38544;&#31169;&#65292;&#20294;&#26159;UE&#20043;&#38388;&#30340;&#25968;&#25454;&#24046;&#24322;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#36136;&#37327;&#19979;&#38477;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#20026;&#27599;&#20010;UE&#35757;&#32451;&#20010;&#24615;&#21270;&#30340;&#26412;&#22320;&#27169;&#22411;&#20197;&#20934;&#30830;&#39044;&#27979;&#21463;&#27426;&#36814;&#30340;&#20869;&#23481;&#12290;&#27492;&#22806;&#65292;&#19979;&#19968;&#20195;&#32593;&#32476;&#20013;&#30456;&#37051;SBS&#20043;&#38388;&#21487;&#20197;&#20849;&#20139;&#32531;&#23384;&#30340;&#20869;&#23481;&#65292;&#22240;&#27492;&#22312;&#19981;&#21516;&#30340;SBS&#20013;&#32531;&#23384;&#39044;&#27979;&#21040;&#30340;&#28909;&#38376;&#20869;&#23481;&#21487;&#33021;&#20250;&#24433;&#21709;&#33719;&#21462;&#20869;&#23481;&#30340;&#25104;&#26412;&#12290;&#22240;&#27492;&#65292;&#30830;&#23450;&#21512;&#20316;&#32531;&#23384;&#28909;&#38376;&#20869;&#23481;&#30340;&#20301;&#32622;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24377;&#24615;&#32852;&#37030;&#21644;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#21512;&#20316;&#36793;&#32536;&#32531;&#23384;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Edge caching is a promising solution for next-generation networks by empowering caching units in small-cell base stations (SBSs), which allows user equipments (UEs) to fetch users' requested contents that have been pre-cached in SBSs. It is crucial for SBSs to predict accurate popular contents through learning while protecting users' personal information. Traditional federated learning (FL) can protect users' privacy but the data discrepancies among UEs can lead to a degradation in model quality. Therefore, it is necessary to train personalized local models for each UE to predict popular contents accurately. In addition, the cached contents can be shared among adjacent SBSs in next-generation networks, thus caching predicted popular contents in different SBSs may affect the cost to fetch contents. Hence, it is critical to determine where the popular contents are cached cooperatively. To address these issues, we propose a cooperative edge caching scheme based on elastic federated and mu
&lt;/p&gt;</description></item><item><title>GA-SmaAt-GNet&#26159;&#19968;&#31181;&#29983;&#25104;&#23545;&#25239;&#26550;&#26500;&#65292;&#29992;&#20110;&#25913;&#36827;&#26497;&#31471;&#38477;&#27700;&#26242;&#26102;&#39044;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#12290;&#23427;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#26032;&#30340;&#29983;&#25104;&#22120;&#21644;&#20351;&#29992;&#27880;&#24847;&#21147;&#22686;&#24378;&#37492;&#21035;&#22120;&#26469;&#21033;&#29992;&#38477;&#27700;&#25513;&#30721;&#25552;&#20379;&#30340;&#39069;&#22806;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#39044;&#27979;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2401.09881</link><description>&lt;p&gt;
GA-SmaAt-GNet&#65306;&#29992;&#20110;&#26497;&#31471;&#38477;&#27700;&#26242;&#26102;&#39044;&#27979;&#30340;&#29983;&#25104;&#23545;&#25239;&#23567;&#22411;&#27880;&#24847;&#21147;GNet
&lt;/p&gt;
&lt;p&gt;
GA-SmaAt-GNet: Generative Adversarial Small Attention GNet for Extreme Precipitation Nowcasting. (arXiv:2401.09881v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09881
&lt;/p&gt;
&lt;p&gt;
GA-SmaAt-GNet&#26159;&#19968;&#31181;&#29983;&#25104;&#23545;&#25239;&#26550;&#26500;&#65292;&#29992;&#20110;&#25913;&#36827;&#26497;&#31471;&#38477;&#27700;&#26242;&#26102;&#39044;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#12290;&#23427;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#26032;&#30340;&#29983;&#25104;&#22120;&#21644;&#20351;&#29992;&#27880;&#24847;&#21147;&#22686;&#24378;&#37492;&#21035;&#22120;&#26469;&#21033;&#29992;&#38477;&#27700;&#25513;&#30721;&#25552;&#20379;&#30340;&#39069;&#22806;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#39044;&#27979;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25968;&#25454;&#39537;&#21160;&#30340;&#24314;&#27169;&#26041;&#27861;&#22312;&#21508;&#31181;&#27668;&#35937;&#24212;&#29992;&#20013;&#33719;&#24471;&#20102;&#30456;&#24403;&#30340;&#20851;&#27880;&#65292;&#23588;&#20854;&#26159;&#22312;&#22825;&#27668;&#39044;&#25253;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#22788;&#29702;&#26497;&#31471;&#22825;&#27668;&#26465;&#20214;&#26102;&#24120;&#24120;&#38754;&#20020;&#25361;&#25112;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GA-SmaAt-GNet&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#23545;&#25239;&#32467;&#26500;&#65292;&#21033;&#29992;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#22686;&#24378;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#26497;&#31471;&#38477;&#27700;&#26242;&#26102;&#39044;&#27979;&#20013;&#30340;&#24615;&#33021;&#12290;&#39318;&#20808;&#65292;&#23427;&#20351;&#29992;&#20102;&#19968;&#31181;&#22522;&#20110;&#25104;&#21151;&#30340;SmaAt-UNet&#32467;&#26500;&#26500;&#24314;&#30340;&#26032;&#22411;SmaAt-GNet&#20316;&#20026;&#29983;&#25104;&#22120;&#12290;&#35813;&#32593;&#32476;&#23558;&#38477;&#27700;&#25513;&#30721;&#65288;&#20108;&#20540;&#21270;&#38477;&#27700;&#22270;&#65289;&#20316;&#20026;&#38468;&#21152;&#25968;&#25454;&#28304;&#65292;&#21033;&#29992;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#36827;&#34892;&#25913;&#36827;&#30340;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;GA-SmaAt-GNet&#36824;&#20351;&#29992;&#20102;&#19968;&#20010;&#28789;&#24863;&#26469;&#33258;&#20110;&#33879;&#21517;&#30340;Pix2Pix&#32467;&#26500;&#30340;&#27880;&#24847;&#21147;&#22686;&#24378;&#37492;&#21035;&#22120;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#33655;&#20848;&#30340;&#23454;&#38469;&#38477;&#27700;&#25968;&#25454;&#38598;&#23545;GA-SmaAt-GNet&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, data-driven modeling approaches have gained considerable traction in various meteorological applications, particularly in the realm of weather forecasting. However, these approaches often encounter challenges when dealing with extreme weather conditions. In light of this, we propose GA-SmaAt-GNet, a novel generative adversarial architecture that makes use of two methodologies aimed at enhancing the performance of deep learning models for extreme precipitation nowcasting. Firstly, it uses a novel SmaAt-GNet built upon the successful SmaAt-UNet architecture as generator. This network incorporates precipitation masks (binarized precipitation maps) as an additional data source, leveraging valuable information for improved predictions. Additionally, GA-SmaAt-GNet utilizes an attention-augmented discriminator inspired by the well-established Pix2Pix architecture. Furthermore, we assess the performance of GA-SmaAt-GNet using real-life precipitation dataset from the Netherland
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#33258;&#21160;&#35782;&#21035;&#19979;&#34507;&#40481;&#34892;&#20026;&#12290;&#36890;&#36807;&#22768;&#38899;&#20998;&#26512;&#21644;&#29305;&#24449;&#25552;&#21462;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#40065;&#26834;&#30340;&#34892;&#20026;&#29305;&#24449;&#21270;&#31995;&#32479;&#65292;&#23545;&#19979;&#34507;&#40481;&#30340;&#20581;&#24247;&#34892;&#20026;&#36827;&#34892;&#30417;&#27979;&#21644;&#35782;&#21035;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#20855;&#26377;&#33391;&#22909;&#30340;&#32508;&#21512;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.09880</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#23545;&#33258;&#21160;&#34892;&#20026;&#19979;&#34507;&#40481;&#35782;&#21035;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Attention-Based Recurrent Neural Network For Automatic Behavior Laying Hen Recognition. (arXiv:2401.09880v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09880
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#33258;&#21160;&#35782;&#21035;&#19979;&#34507;&#40481;&#34892;&#20026;&#12290;&#36890;&#36807;&#22768;&#38899;&#20998;&#26512;&#21644;&#29305;&#24449;&#25552;&#21462;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#40065;&#26834;&#30340;&#34892;&#20026;&#29305;&#24449;&#21270;&#31995;&#32479;&#65292;&#23545;&#19979;&#34507;&#40481;&#30340;&#20581;&#24247;&#34892;&#20026;&#36827;&#34892;&#30417;&#27979;&#21644;&#35782;&#21035;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#20855;&#26377;&#33391;&#22909;&#30340;&#32508;&#21512;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#20859;&#31165;&#19994;&#30340;&#19968;&#20010;&#20851;&#27880;&#28857;&#26159;&#19979;&#34507;&#40481;&#30340;&#40483;&#21483;&#22768;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#20851;&#20110;&#20581;&#24247;&#34892;&#20026;&#30340;&#38750;&#24120;&#26377;&#29992;&#30340;&#20449;&#24687;&#12290;&#36825;&#20123;&#20449;&#24687;&#34987;&#29992;&#20316;&#20581;&#24247;&#21644;&#31119;&#31049;&#30340;&#25351;&#26631;&#65292;&#24110;&#21161;&#20859;&#27542;&#20154;&#21592;&#26356;&#22909;&#22320;&#30417;&#27979;&#19979;&#34507;&#40481;&#65292;&#20174;&#32780;&#21450;&#26089;&#21457;&#29616;&#38382;&#39064;&#65292;&#20197;&#20415;&#36827;&#34892;&#26356;&#24555;&#21644;&#26356;&#26377;&#25928;&#30340;&#24178;&#39044;&#12290;&#26412;&#30740;&#31350;&#19987;&#27880;&#20110;&#23545;&#19979;&#34507;&#40481;&#40483;&#21483;&#31867;&#22411;&#30340;&#22768;&#38899;&#20998;&#26512;&#65292;&#20197;&#25552;&#20986;&#19968;&#31181;&#40065;&#26834;&#30340;&#34892;&#20026;&#29305;&#24449;&#21270;&#31995;&#32479;&#65292;&#20197;&#20415;&#26356;&#22909;&#22320;&#30417;&#27979;&#19979;&#34507;&#40481;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#25910;&#38598;&#24182;&#27880;&#37322;&#20102;&#19979;&#34507;&#40481;&#30340;&#40483;&#21483;&#20449;&#21495;&#65292;&#28982;&#21518;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#26102;&#38388;&#21644;&#39057;&#29575;&#22495;&#29305;&#24449;&#32452;&#21512;&#30340;&#26368;&#20339;&#22768;&#23398;&#29305;&#24449;&#21270;&#26041;&#27861;&#12290;&#28982;&#21518;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#29305;&#24449;&#26500;&#24314;&#20102;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#27169;&#22411;&#65292;&#23558;&#35821;&#20041;&#31867;&#21035;&#20998;&#37197;&#32473;&#25551;&#36848;&#19979;&#34507;&#40481;&#34892;&#20026;&#30340;&#40483;&#21483;&#22768;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#32508;&#21512;&#24615;&#33021;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the interests of modern poultry farming is the vocalization of laying hens which contain very useful information on health behavior. This information is used as health and well-being indicators that help breeders better monitor laying hens, which involves early detection of problems for rapid and more effective intervention. In this work, we focus on the sound analysis for the recognition of the types of calls of the laying hens in order to propose a robust system of characterization of their behavior for a better monitoring. To do this, we first collected and annotated laying hen call signals, then designed an optimal acoustic characterization based on the combination of time and frequency domain features. We then used these features to build the multi-label classification models based on recurrent neural network to assign a semantic class to the vocalization that characterize the laying hen behavior. The results show an overall performance with our model based on the combinati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#19977;&#23618;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#24341;&#20837;&#20102;&#31354;&#38388;&#21644;&#26102;&#38388;&#30446;&#26631;&#25277;&#35937;&#21270;&#12290;&#30740;&#31350;&#32773;&#25552;&#20379;&#20102;&#23398;&#20064;&#31574;&#30053;&#30340;&#29702;&#35770;&#36951;&#25022;&#36793;&#30028;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#23545;&#31639;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2401.09870</link><description>&lt;p&gt;
&#35843;&#21644;&#31354;&#38388;&#21644;&#26102;&#38388;&#25277;&#35937;&#21270;&#20197;&#23454;&#29616;&#30446;&#26631;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Reconciling Spatial and Temporal Abstractions for Goal Representation. (arXiv:2401.09870v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09870
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#19977;&#23618;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#24341;&#20837;&#20102;&#31354;&#38388;&#21644;&#26102;&#38388;&#30446;&#26631;&#25277;&#35937;&#21270;&#12290;&#30740;&#31350;&#32773;&#25552;&#20379;&#20102;&#23398;&#20064;&#31574;&#30053;&#30340;&#29702;&#35770;&#36951;&#25022;&#36793;&#30028;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#23545;&#31639;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#34920;&#31034;&#36890;&#36807;&#23558;&#22797;&#26434;&#30340;&#23398;&#20064;&#38382;&#39064;&#20998;&#35299;&#20026;&#26356;&#23481;&#26131;&#30340;&#23376;&#20219;&#21153;&#26469;&#24433;&#21709;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20445;&#30041;&#26102;&#38388;&#25277;&#35937;&#29615;&#22659;&#21160;&#24577;&#30340;&#34920;&#31034;&#26041;&#27861;&#22312;&#35299;&#20915;&#22256;&#38590;&#38382;&#39064;&#21644;&#25552;&#20379;&#20248;&#21270;&#29702;&#35770;&#20445;&#35777;&#26041;&#38754;&#26159;&#25104;&#21151;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#29615;&#22659;&#21160;&#24577;&#36234;&#26469;&#36234;&#22797;&#26434;&#65288;&#21363;&#26102;&#38388;&#25277;&#35937;&#36716;&#25442;&#20851;&#31995;&#20381;&#36182;&#26356;&#22810;&#21464;&#37327;&#65289;&#30340;&#20219;&#21153;&#20013;&#26080;&#27861;&#25193;&#23637;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20854;&#20182;&#26041;&#27861;&#21017;&#23581;&#35797;&#20351;&#29992;&#31354;&#38388;&#25277;&#35937;&#26469;&#32531;&#35299;&#21069;&#38754;&#30340;&#38382;&#39064;&#12290;&#23427;&#20204;&#30340;&#38480;&#21046;&#21253;&#25324;&#26080;&#27861;&#36866;&#24212;&#39640;&#32500;&#29615;&#22659;&#21644;&#23545;&#20808;&#21069;&#30693;&#35782;&#30340;&#20381;&#36182;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19977;&#23618;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20998;&#23618;&#32467;&#26500;&#30340;&#19981;&#21516;&#23618;&#27425;&#24341;&#20837;&#20102;&#31354;&#38388;&#21644;&#26102;&#38388;&#30446;&#26631;&#25277;&#35937;&#21270;&#12290;&#25105;&#20204;&#23545;&#23398;&#20064;&#31574;&#30053;&#30340;&#36951;&#25022;&#36793;&#30028;&#36827;&#34892;&#20102;&#29702;&#35770;&#30740;&#31350;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Goal representation affects the performance of Hierarchical Reinforcement Learning (HRL) algorithms by decomposing the complex learning problem into easier subtasks. Recent studies show that representations that preserve temporally abstract environment dynamics are successful in solving difficult problems and provide theoretical guarantees for optimality. These methods however cannot scale to tasks where environment dynamics increase in complexity i.e. the temporally abstract transition relations depend on larger number of variables. On the other hand, other efforts have tried to use spatial abstraction to mitigate the previous issues. Their limitations include scalability to high dimensional environments and dependency on prior knowledge.  In this paper, we propose a novel three-layer HRL algorithm that introduces, at different levels of the hierarchy, both a spatial and a temporal goal abstraction. We provide a theoretical study of the regret bounds of the learned policies. We evalua
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;SPARC&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#22270;&#20687;-&#25991;&#26412;&#23545;&#20013;&#23398;&#20064;&#27599;&#20010;&#20196;&#29260;&#30340;&#22270;&#20687;&#32452;&#21512;&#65292;&#20197;&#25552;&#39640;&#22270;&#20687;-&#25991;&#26412;&#39044;&#35757;&#32451;&#20013;&#30340;&#32454;&#31890;&#24230;&#29702;&#35299;&#33021;&#21147;&#12290;SPARC&#26041;&#27861;&#32467;&#21512;&#20102;&#32454;&#31890;&#24230;&#25439;&#22833;&#21644;&#23545;&#27604;&#25439;&#22833;&#65292;&#21487;&#20197;&#20197;&#36739;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#23398;&#20064;&#21516;&#26102;&#32534;&#30721;&#20840;&#23616;&#21644;&#23616;&#37096;&#20449;&#24687;&#30340;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2401.09865</link><description>&lt;p&gt;
&#25552;&#39640;&#22270;&#20687;-&#25991;&#26412;&#39044;&#35757;&#32451;&#20013;&#32454;&#31890;&#24230;&#29702;&#35299;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improving fine-grained understanding in image-text pre-training. (arXiv:2401.09865v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;SPARC&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#22270;&#20687;-&#25991;&#26412;&#23545;&#20013;&#23398;&#20064;&#27599;&#20010;&#20196;&#29260;&#30340;&#22270;&#20687;&#32452;&#21512;&#65292;&#20197;&#25552;&#39640;&#22270;&#20687;-&#25991;&#26412;&#39044;&#35757;&#32451;&#20013;&#30340;&#32454;&#31890;&#24230;&#29702;&#35299;&#33021;&#21147;&#12290;SPARC&#26041;&#27861;&#32467;&#21512;&#20102;&#32454;&#31890;&#24230;&#25439;&#22833;&#21644;&#23545;&#27604;&#25439;&#22833;&#65292;&#21487;&#20197;&#20197;&#36739;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#23398;&#20064;&#21516;&#26102;&#32534;&#30721;&#20840;&#23616;&#21644;&#23616;&#37096;&#20449;&#24687;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;SPARC&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#22270;&#20687;-&#25991;&#26412;&#23545;&#20013;&#39044;&#35757;&#32451;&#26356;&#32454;&#31890;&#24230;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#12290;&#32771;&#34385;&#21040;&#22810;&#20010;&#22270;&#20687;&#22359;&#36890;&#24120;&#23545;&#24212;&#20110;&#21333;&#20010;&#21333;&#35789;&#65292;&#25105;&#20204;&#25552;&#20986;&#20026;&#27599;&#20010;&#23383;&#24149;&#20196;&#29260;&#23398;&#20064;&#19968;&#32452;&#22270;&#20687;&#22359;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#22270;&#20687;&#22359;&#21644;&#35821;&#35328;&#20196;&#29260;&#20043;&#38388;&#30340;&#31232;&#30095;&#30456;&#20284;&#24230;&#24230;&#37327;&#65292;&#24182;&#35745;&#31639;&#20986;&#27599;&#20010;&#20196;&#29260;&#30340;&#35821;&#35328;&#20998;&#32452;&#30340;&#35270;&#35273;&#23884;&#20837;&#65292;&#20316;&#20026;&#22270;&#20687;&#22359;&#30340;&#21152;&#26435;&#24179;&#22343;&#20540;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#19968;&#31181;&#20165;&#20381;&#36182;&#20110;&#21333;&#20010;&#26679;&#26412;&#32780;&#19981;&#38656;&#35201;&#20854;&#20182;&#25209;&#27425;&#26679;&#26412;&#20316;&#20026;&#36127;&#26679;&#26412;&#30340;&#32454;&#31890;&#24230;&#24207;&#21015;&#25439;&#22833;&#65292;&#23545;&#20196;&#29260;&#21644;&#35821;&#35328;&#20998;&#32452;&#30340;&#35270;&#35273;&#23884;&#20837;&#36827;&#34892;&#23545;&#27604;&#12290;&#36825;&#20351;&#24471;&#21487;&#20197;&#20197;&#36739;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#23398;&#20064;&#26356;&#35814;&#32454;&#30340;&#20449;&#24687;&#12290;SPARC&#23558;&#36825;&#31181;&#32454;&#31890;&#24230;&#25439;&#22833;&#19982;&#20840;&#23616;&#22270;&#20687;&#21644;&#25991;&#26412;&#23884;&#20837;&#20043;&#38388;&#30340;&#23545;&#27604;&#25439;&#22833;&#30456;&#32467;&#21512;&#65292;&#20197;&#21516;&#26102;&#32534;&#30721;&#20840;&#23616;&#21644;&#23616;&#37096;&#20449;&#24687;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce SPARse Fine-grained Contrastive Alignment (SPARC), a simple method for pretraining more fine-grained multimodal representations from image-text pairs. Given that multiple image patches often correspond to single words, we propose to learn a grouping of image patches for every token in the caption. To achieve this, we use a sparse similarity metric between image patches and language tokens and compute for each token a language-grouped vision embedding as the weighted average of patches. The token and language-grouped vision embeddings are then contrasted through a fine-grained sequence-wise loss that only depends on individual samples and does not require other batch samples as negatives. This enables more detailed information to be learned in a computationally inexpensive manner. SPARC combines this fine-grained loss with a contrastive loss between global image and text embeddings to learn representations that simultaneously encode global and local information. We thorough
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#20248;&#21270;&#30340;&#36827;&#21270;&#22810;&#30446;&#26631;&#26041;&#27861;&#65292;&#36890;&#36807;&#24773;&#24863;&#20998;&#26512;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#23454;&#29616;&#20102;&#29983;&#25104;&#33021;&#22815;&#21516;&#26102;&#20307;&#29616;&#20004;&#31181;&#30456;&#20114;&#20914;&#31361;&#24773;&#24863;&#30340;&#25552;&#31034;&#35821;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#30456;&#20851;&#20449;&#24687;&#30340;&#25552;&#21462;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.09862</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#30340;&#36827;&#21270;&#22810;&#30446;&#26631;&#20248;&#21270;&#20197;&#24179;&#34913;&#24773;&#24863;
&lt;/p&gt;
&lt;p&gt;
Evolutionary Multi-Objective Optimization of Large Language Model Prompts for Balancing Sentiments. (arXiv:2401.09862v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09862
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#20248;&#21270;&#30340;&#36827;&#21270;&#22810;&#30446;&#26631;&#26041;&#27861;&#65292;&#36890;&#36807;&#24773;&#24863;&#20998;&#26512;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#23454;&#29616;&#20102;&#29983;&#25104;&#33021;&#22815;&#21516;&#26102;&#20307;&#29616;&#20004;&#31181;&#30456;&#20114;&#20914;&#31361;&#24773;&#24863;&#30340;&#25552;&#31034;&#35821;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#30456;&#20851;&#20449;&#24687;&#30340;&#25552;&#21462;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#30340;&#20986;&#29616;&#24341;&#36215;&#20102;&#21508;&#20010;&#39046;&#22495;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#24615;&#33021;&#21644;&#22810;&#21151;&#33021;&#24615;&#38750;&#20961;&#12290;&#38543;&#30528;&#36825;&#20123;&#27169;&#22411;&#30340;&#20351;&#29992;&#19981;&#26029;&#22686;&#38271;&#65292;&#26377;&#25928;&#30340;&#25552;&#31034;&#24037;&#31243;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#25552;&#31034;&#20248;&#21270;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#30452;&#25509;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#21644;&#30456;&#20851;&#20449;&#24687;&#30340;&#25552;&#21462;&#12290;&#26368;&#36817;&#65292;&#36827;&#21270;&#31639;&#27861;&#65288;EAs&#65289;&#22312;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#24076;&#26395;&#65292;&#20026;&#26032;&#30340;&#20248;&#21270;&#31574;&#30053;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#21035;&#38024;&#23545;&#25552;&#31034;&#20248;&#21270;&#30340;&#36827;&#21270;&#22810;&#30446;&#26631;&#65288;EMO&#65289;&#26041;&#27861;&#65292;&#31216;&#20026;EMO-Prompts&#65292;&#20197;&#24773;&#24863;&#20998;&#26512;&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#12290;&#25105;&#20204;&#23558;&#24773;&#24863;&#20998;&#26512;&#33021;&#21147;&#20316;&#20026;&#25105;&#20204;&#30340;&#23454;&#39564;&#30446;&#26631;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;EMO-Prompts&#33021;&#22815;&#26377;&#25928;&#22320;&#29983;&#25104;&#25552;&#31034;&#65292;&#20351;LLM&#33021;&#22815;&#21516;&#26102;&#20135;&#29983;&#20307;&#29616;&#20004;&#31181;&#30456;&#20114;&#20914;&#31361;&#24773;&#24863;&#30340;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of large language models (LLMs) such as ChatGPT has attracted considerable attention in various domains due to their remarkable performance and versatility. As the use of these models continues to grow, the importance of effective prompt engineering has come to the fore. Prompt optimization emerges as a crucial challenge, as it has a direct impact on model performance and the extraction of relevant information. Recently, evolutionary algorithms (EAs) have shown promise in addressing this issue, paving the way for novel optimization strategies. In this work, we propose a evolutionary multi-objective (EMO) approach specifically tailored for prompt optimization called EMO-Prompts, using sentiment analysis as a case study. We use sentiment analysis capabilities as our experimental targets. Our results demonstrate that EMO-Prompts effectively generates prompts capable of guiding the LLM to produce texts embodying two conflicting emotions simultaneously.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25913;&#36827;&#20102;&#21517;&#20026;FREED&#30340;RL&#27169;&#22411;&#65292;&#36890;&#36807;&#22797;&#21046;&#12289;&#23457;&#26597;&#21644;&#31616;&#21270;&#65292;&#20351;&#20854;&#22312;&#34507;&#30333;&#36136;&#26465;&#20214;&#19979;&#30340;&#20998;&#23376;&#29983;&#25104;&#20013;&#20855;&#26377;&#26356;&#22909;&#30340;&#36136;&#37327;&#21644;&#24615;&#33021;</title><link>http://arxiv.org/abs/2401.09840</link><description>&lt;p&gt;
FREED++:&#36890;&#36807;&#24443;&#24213;&#30340;&#22797;&#21046;&#25913;&#21892;&#22522;&#20110;&#29255;&#27573;&#30340;&#20998;&#23376;&#29983;&#25104;&#30340;RL&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
FREED++: Improving RL Agents for Fragment-Based Molecule Generation by Thorough Reproduction. (arXiv:2401.09840v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09840
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25913;&#36827;&#20102;&#21517;&#20026;FREED&#30340;RL&#27169;&#22411;&#65292;&#36890;&#36807;&#22797;&#21046;&#12289;&#23457;&#26597;&#21644;&#31616;&#21270;&#65292;&#20351;&#20854;&#22312;&#34507;&#30333;&#36136;&#26465;&#20214;&#19979;&#30340;&#20998;&#23376;&#29983;&#25104;&#20013;&#20855;&#26377;&#26356;&#22909;&#30340;&#36136;&#37327;&#21644;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#22411;&#27835;&#30103;&#33647;&#29289;&#30340;&#29702;&#24615;&#35774;&#35745;&#26088;&#22312;&#25214;&#21040;&#20855;&#26377;&#25152;&#38656;&#29983;&#29289;&#21151;&#33021;&#30340;&#20998;&#23376;&#32467;&#26500;&#65292;&#20363;&#22914;&#65292;&#36890;&#36807;&#19982;&#29305;&#23450;&#34507;&#30333;&#36136;&#32467;&#21512;&#26469;&#28608;&#27963;&#25110;&#25233;&#21046;&#23427;&#12290;&#20998;&#23376;&#23545;&#25509;&#26159;&#35780;&#20272;&#34507;&#30333;&#36136;-&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#30340;&#24120;&#35265;&#25216;&#26415;&#12290;&#26368;&#36817;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25509;&#35780;&#20998;&#65288;DS&#65289;&#20316;&#20026;&#22870;&#21169;&#26469;&#29983;&#25104;&#20998;&#23376;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22797;&#21046;&#12289;&#23457;&#26597;&#21644;&#25913;&#36827;&#20102;&#26368;&#36817;&#29992;&#20110;&#20998;&#23376;&#29983;&#25104;&#30340;RL&#27169;&#22411;FREED&#65288;arXiv&#65306;2110.01219&#65289;&#12290;&#23545;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#24191;&#27867;&#35780;&#20272;&#25581;&#31034;&#20102;&#19968;&#20123;&#23616;&#38480;&#24615;&#21644;&#25361;&#25112;&#65292;&#23613;&#31649;&#23545;&#19977;&#20010;&#30446;&#26631;&#34507;&#30333;&#36136;&#25253;&#21578;&#20102;&#26480;&#20986;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#21253;&#25324;&#20462;&#22797;&#20102;&#35768;&#22810;&#23454;&#29616;&#38169;&#35823;&#65292;&#31616;&#21270;&#20102;&#27169;&#22411;&#24182;&#25552;&#39640;&#20102;&#20854;&#36136;&#37327;&#65292;&#22823;&#22823;&#25193;&#23637;&#20102;&#23454;&#39564;&#33539;&#22260;&#65292;&#24182;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#34507;&#30333;&#36136;&#26465;&#20214;&#19979;&#20998;&#23376;&#29983;&#25104;&#26041;&#27861;&#36827;&#34892;&#20102;&#20934;&#30830;&#27604;&#36739;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#21644;&#39640;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
A rational design of new therapeutic drugs aims to find a molecular structure with desired biological functionality, e.g., an ability to activate or suppress a specific protein via binding to it. Molecular docking is a common technique for evaluating protein-molecule interactions. Recently, Reinforcement Learning (RL) has emerged as a promising approach to generating molecules with the docking score (DS) as a reward. In this work, we reproduce, scrutinize and improve the recent RL model for molecule generation called FREED (arXiv:2110.01219). Extensive evaluation of the proposed method reveals several limitations and challenges despite the outstanding results reported for three target proteins. Our contributions include fixing numerous implementation bugs and simplifying the model while increasing its quality, significantly extending experiments, and conducting an accurate comparison with current state-of-the-art methods for protein-conditioned molecule generation. We show that the res
&lt;/p&gt;</description></item><item><title>PPNet&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#29992;&#20110;&#35299;&#20915;&#31471;&#21040;&#31471;&#36817;&#20284;&#26368;&#20248;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#20998;&#20026;&#20004;&#20010;&#23376;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#20004;&#32423;&#32423;&#32852;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#27714;&#35299;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;EDaGe-PP&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;PPNet&#22312;&#35745;&#31639;&#26102;&#38388;&#21644;&#25104;&#21151;&#29575;&#26041;&#38754;&#27604;&#20854;&#20182;&#26041;&#27861;&#26377;&#26174;&#33879;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2401.09819</link><description>&lt;p&gt;
PPNet: &#19968;&#31181;&#29992;&#20110;&#31471;&#21040;&#31471;&#36817;&#20284;&#26368;&#20248;&#36335;&#24452;&#35268;&#21010;&#30340;&#26032;&#39062;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
PPNet: A Novel Neural Network Structure for End-to-End Near-Optimal Path Planning. (arXiv:2401.09819v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09819
&lt;/p&gt;
&lt;p&gt;
PPNet&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#29992;&#20110;&#35299;&#20915;&#31471;&#21040;&#31471;&#36817;&#20284;&#26368;&#20248;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#20998;&#20026;&#20004;&#20010;&#23376;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#20004;&#32423;&#32423;&#32852;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#27714;&#35299;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;EDaGe-PP&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;PPNet&#22312;&#35745;&#31639;&#26102;&#38388;&#21644;&#25104;&#21151;&#29575;&#26041;&#38754;&#27604;&#20854;&#20182;&#26041;&#27861;&#26377;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#36335;&#24452;&#35268;&#21010;&#22120;&#65292;&#22914;&#22522;&#20110;&#37319;&#26679;&#30340;&#36335;&#24452;&#35268;&#21010;&#22120;&#65292;&#22312;&#21021;&#22987;&#35299;&#25935;&#24863;&#24615;&#21644;&#25910;&#25947;&#21040;&#26368;&#20248;&#35299;&#36895;&#24230;&#19978;&#20855;&#26377;&#23616;&#38480;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#22914;&#20855;&#26377;&#26377;&#38480;&#21151;&#29575;/&#29123;&#26009;&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#65292;&#22312;&#30701;&#26102;&#38388;&#20869;&#25214;&#21040;&#36817;&#20284;&#26368;&#20248;&#35299;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#23454;&#29616;&#31471;&#21040;&#31471;&#36817;&#20284;&#26368;&#20248;&#36335;&#24452;&#35268;&#21010;&#22120;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#20998;&#20026;&#20004;&#20010;&#23376;&#38382;&#39064;&#65292;&#21363;&#36335;&#24452;&#31354;&#38388;&#20998;&#27573;&#21644;&#32473;&#23450;&#36335;&#24452;&#31354;&#38388;&#20013;&#30340;&#33322;&#28857;&#29983;&#25104;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#36335;&#24452;&#35268;&#21010;&#32593;&#32476;&#65288;PPNet&#65289;&#30340;&#20004;&#32423;&#32423;&#32852;&#31070;&#32463;&#32593;&#32476;&#26469;&#35299;&#20915;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#65292;&#36890;&#36807;&#35299;&#20915;&#19978;&#36848;&#23376;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EDaGe-PP&#30340;&#29992;&#20110;&#36335;&#24452;&#35268;&#21010;&#30340;&#39640;&#25928;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;PPNet&#35757;&#32451;&#38598;&#30001;EDaGe-PP&#29983;&#25104;&#30340;&#25104;&#21151;&#29575;&#30456;&#27604;&#20854;&#20182;&#26041;&#27861;&#25552;&#21319;&#20102;$2\times$&#65292;&#24635;&#35745;&#31639;&#26102;&#38388;&#23569;&#20110;1/33&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;PPNet&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The classical path planners, such as sampling-based path planners, have the limitations of sensitivity to the initial solution and slow convergence to the optimal solution. However, finding a near-optimal solution in a short period is challenging in many applications such as the autonomous vehicle with limited power/fuel. To achieve an end-to-end near-optimal path planner, we first divide the path planning problem into two subproblems, which are path's space segmentation and waypoints generation in the given path's space. We further propose a two-level cascade neural network named Path Planning Network (PPNet) to solve the path planning problem by solving the abovementioned subproblems. Moreover, we propose a novel efficient data generation method for path planning named EDaGe-PP. The results show the total computation time is less than 1/33 and the success rate of PPNet trained by the dataset that is generated by EDaGe-PP is about $2 \times$ compared to other methods. We validate PPNe
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;&#21442;&#19982;&#24230;&#30340;&#20248;&#21270;&#31574;&#30053;&#22312;&#22312;&#32447;&#24179;&#21488;&#20013;&#23545;&#20869;&#23481;&#26684;&#23616;&#20135;&#29983;&#20102;&#37325;&#35201;&#24433;&#21709;&#65292;&#20854;&#26082;&#40723;&#21169;&#20102;&#36136;&#37327;&#25237;&#36164;&#65292;&#21448;&#22870;&#21169;&#20102;&#28857;&#20987;&#39537;&#21160;&#31561;&#25112;&#26415;&#12290;&#28982;&#32780;&#65292;&#22312;&#22343;&#34913;&#29366;&#24577;&#19979;&#65292;&#20869;&#23481;&#36136;&#37327;&#21644;&#25112;&#26415;&#20043;&#38388;&#21576;&#27491;&#30456;&#20851;&#65292;&#32780;&#29992;&#25143;&#28040;&#36153;&#30340;&#24179;&#22343;&#20869;&#23481;&#36136;&#37327;&#21487;&#33021;&#20250;&#19979;&#38477;&#65292;&#22522;&#20110;&#21442;&#19982;&#24230;&#30340;&#20248;&#21270;&#31574;&#30053;&#22312;&#29992;&#25143;&#20307;&#39564;&#26041;&#38754;&#21487;&#33021;&#34920;&#29616;&#24471;&#26356;&#24046;&#12290;</title><link>http://arxiv.org/abs/2401.09804</link><description>&lt;p&gt;
&#28857;&#20987;&#39537;&#21160;&#19982;&#36136;&#37327;: &#22914;&#20309;&#20248;&#21270;&#22522;&#20110;&#21442;&#19982;&#24230;&#30340;&#31574;&#30053;&#24433;&#21709;&#22312;&#32447;&#24179;&#21488;&#30340;&#20869;&#23481;&#26684;&#23616;
&lt;/p&gt;
&lt;p&gt;
Clickbait vs. Quality: How Engagement-Based Optimization Shapes the Content Landscape in Online Platforms. (arXiv:2401.09804v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09804
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;&#21442;&#19982;&#24230;&#30340;&#20248;&#21270;&#31574;&#30053;&#22312;&#22312;&#32447;&#24179;&#21488;&#20013;&#23545;&#20869;&#23481;&#26684;&#23616;&#20135;&#29983;&#20102;&#37325;&#35201;&#24433;&#21709;&#65292;&#20854;&#26082;&#40723;&#21169;&#20102;&#36136;&#37327;&#25237;&#36164;&#65292;&#21448;&#22870;&#21169;&#20102;&#28857;&#20987;&#39537;&#21160;&#31561;&#25112;&#26415;&#12290;&#28982;&#32780;&#65292;&#22312;&#22343;&#34913;&#29366;&#24577;&#19979;&#65292;&#20869;&#23481;&#36136;&#37327;&#21644;&#25112;&#26415;&#20043;&#38388;&#21576;&#27491;&#30456;&#20851;&#65292;&#32780;&#29992;&#25143;&#28040;&#36153;&#30340;&#24179;&#22343;&#20869;&#23481;&#36136;&#37327;&#21487;&#33021;&#20250;&#19979;&#38477;&#65292;&#22522;&#20110;&#21442;&#19982;&#24230;&#30340;&#20248;&#21270;&#31574;&#30053;&#22312;&#29992;&#25143;&#20307;&#39564;&#26041;&#38754;&#21487;&#33021;&#34920;&#29616;&#24471;&#26356;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#20869;&#23481;&#24179;&#21488;&#26222;&#36941;&#20351;&#29992;&#22522;&#20110;&#21442;&#19982;&#24230;&#30340;&#20248;&#21270;&#31574;&#30053;&#26469;&#36827;&#34892;&#25512;&#33616;&#12290;&#36825;&#40723;&#21169;&#20869;&#23481;&#21019;&#20316;&#32773;&#25237;&#36164;&#20110;&#36136;&#37327;&#65292;&#20294;&#20063;&#20250;&#22870;&#21169;&#20351;&#29992;&#28857;&#20987;&#39537;&#21160;&#31561;&#25112;&#26415;&#12290;&#20026;&#20102;&#20102;&#35299;&#23545;&#20869;&#23481;&#26684;&#23616;&#30340;&#24635;&#20307;&#24433;&#21709;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20869;&#23481;&#21019;&#20316;&#32773;&#22522;&#20110;&#21442;&#19982;&#25351;&#26631;&#31454;&#20105;&#30340;&#28216;&#25103;&#65292;&#24182;&#20998;&#26512;&#20102;&#20851;&#20110;&#36136;&#37327;&#25237;&#36164;&#21644;&#25112;&#26415;&#30340;&#22343;&#34913;&#20915;&#31574;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22343;&#34913;&#29366;&#24577;&#19979;&#21019;&#24314;&#30340;&#20869;&#23481;&#22312;&#36136;&#37327;&#21644;&#25112;&#26415;&#20043;&#38388;&#21576;&#27491;&#30456;&#20851;&#65292;&#24182;&#22312;Twitter&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#39564;&#35777;&#12290;&#21033;&#29992;&#20869;&#23481;&#26684;&#23616;&#30340;&#22343;&#34913;&#32467;&#26500;&#65292;&#25105;&#20204;&#36824;&#20174;&#22810;&#20010;&#26041;&#38754;&#30740;&#31350;&#20102;&#22522;&#20110;&#21442;&#19982;&#24230;&#30340;&#20248;&#21270;&#31574;&#30053;&#30340;&#19979;&#28216;&#32489;&#25928;&#12290;&#20063;&#35768;&#36870;&#21453;&#22320;&#65292;&#29992;&#25143;&#28040;&#36153;&#30340;&#24179;&#22343;&#20869;&#23481;&#36136;&#37327;&#22312;&#22343;&#34913;&#29366;&#24577;&#19979;&#21487;&#33021;&#20250;&#19979;&#38477;&#65292;&#22240;&#20026;&#25112;&#26415;&#23545;&#20869;&#23481;&#21019;&#20316;&#32773;&#30340;&#25104;&#26412;&#26356;&#39640;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#21442;&#19982;&#24230;&#30340;&#20248;&#21270;&#31574;&#30053;&#22312;&#29992;&#25143;&#20307;&#39564;&#26041;&#38754;&#21487;&#33021;&#34920;&#29616;&#24471;&#26356;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online content platforms commonly use engagement-based optimization when making recommendations. This encourages content creators to invest in quality, but also rewards gaming tricks such as clickbait. To understand the total impact on the content landscape, we study a game between content creators competing on the basis of engagement metrics and analyze the equilibrium decisions about investment in quality and gaming. First, we show the content created at equilibrium exhibits a positive correlation between quality and gaming, and we empirically validate this finding on a Twitter dataset. Using the equilibrium structure of the content landscape, we then examine the downstream performance of engagement-based optimization along several axes. Perhaps counterintuitively, the average quality of content consumed by users can decrease at equilibrium as gaming tricks become more costly for content creators to employ. Moreover, engagement-based optimization can perform worse in terms of user ut
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#20999;&#29255;&#30340;&#23433;&#20840;&#20998;&#24067;&#24335;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#26694;&#26550;&#12290;&#36890;&#36807;&#22312;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#31471;&#37096;&#32626;&#21487;&#20449;&#25191;&#34892;&#29615;&#22659;&#65288;TEE&#65289;&#24182;&#20351;&#29992;&#36731;&#37327;&#32423;&#21152;&#23494;&#36827;&#34892;&#23433;&#20840;&#36890;&#20449;&#65292;&#35299;&#20915;&#20102;&#24694;&#24847;&#31363;&#21462;&#27169;&#22411;&#21442;&#25968;&#21644;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#37319;&#29992;&#20998;&#21106;&#24494;&#35843;&#21644;&#31232;&#30095;&#21270;&#21442;&#25968;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#22312;&#38477;&#20302;&#35774;&#22791;&#25104;&#26412;&#30340;&#21516;&#26102;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.09796</link><description>&lt;p&gt;
&#19968;&#31181;&#24555;&#36895;&#12289;&#39640;&#24615;&#33021;&#12289;&#23433;&#20840;&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;&#26694;&#26550;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Fast, Performant, Secure Distributed Training Framework For Large Language Model. (arXiv:2401.09796v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09796
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#20999;&#29255;&#30340;&#23433;&#20840;&#20998;&#24067;&#24335;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#26694;&#26550;&#12290;&#36890;&#36807;&#22312;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#31471;&#37096;&#32626;&#21487;&#20449;&#25191;&#34892;&#29615;&#22659;&#65288;TEE&#65289;&#24182;&#20351;&#29992;&#36731;&#37327;&#32423;&#21152;&#23494;&#36827;&#34892;&#23433;&#20840;&#36890;&#20449;&#65292;&#35299;&#20915;&#20102;&#24694;&#24847;&#31363;&#21462;&#27169;&#22411;&#21442;&#25968;&#21644;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#37319;&#29992;&#20998;&#21106;&#24494;&#35843;&#21644;&#31232;&#30095;&#21270;&#21442;&#25968;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#22312;&#38477;&#20302;&#35774;&#22791;&#25104;&#26412;&#30340;&#21516;&#26102;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#65288;&#32852;&#37030;&#24335;&#65289;LLM&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#29992;&#38548;&#31163;&#25968;&#25454;&#20849;&#21516;&#35757;&#32451;&#39046;&#22495;&#29305;&#23450;&#30340;LLM&#12290;&#28982;&#32780;&#65292;&#24694;&#24847;&#22320;&#20174;&#26381;&#21153;&#22120;&#25110;&#23458;&#25143;&#31471;&#31363;&#21462;&#27169;&#22411;&#21442;&#25968;&#21644;&#25968;&#25454;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#36843;&#20999;&#38656;&#35201;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#20999;&#29255;&#30340;&#23433;&#20840;&#20998;&#24067;&#24335;LLM&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#22312;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#31471;&#37117;&#37096;&#32626;&#20102;&#21487;&#20449;&#25191;&#34892;&#29615;&#22659;&#65288;TEE&#65289;&#65292;&#24182;&#23558;&#24494;&#35843;&#30340;&#32467;&#26500;&#65288;LoRA&#25110;P-tuning v2&#30340;&#23884;&#20837;&#65289;&#25918;&#20837;TEE&#20013;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#36731;&#37327;&#32423;&#21152;&#23494;&#65292;&#22312;TEE&#21644;&#36890;&#29992;&#29615;&#22659;&#20013;&#36827;&#34892;&#23433;&#20840;&#36890;&#20449;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#38477;&#20302;&#35774;&#22791;&#25104;&#26412;&#24182;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#21644;&#20934;&#30830;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#21106;&#24494;&#35843;&#26041;&#26696;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#36890;&#36807;&#23618;&#27425;&#36827;&#34892;LLM&#30340;&#20998;&#21106;&#65292;&#23558;&#21518;&#38754;&#30340;&#23618;&#27425;&#25918;&#22312;&#26381;&#21153;&#22120;&#31471;TEE&#20013;&#65288;&#23458;&#25143;&#31471;&#19981;&#38656;&#35201;TEE&#65289;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#25552;&#20986;&#30340;&#31232;&#30095;&#21270;&#21442;&#25968;&#24494;&#35843;&#65288;SPF&#65289;&#19982;LoRA&#37096;&#20998;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#25512;&#26029;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The distributed (federated) LLM is an important method for co-training the domain-specific LLM using siloed data. However, maliciously stealing model parameters and data from the server or client side has become an urgent problem to be solved. In this paper, we propose a secure distributed LLM based on model slicing. In this case, we deploy the Trusted Execution Environment (TEE) on both the client and server side, and put the fine-tuned structure (LoRA or embedding of P-tuning v2) into the TEE. Then, secure communication is executed in the TEE and general environments through lightweight encryption. In order to further reduce the equipment cost as well as increase the model performance and accuracy, we propose a split fine-tuning scheme. In particular, we split the LLM by layers and place the latter layers in a server-side TEE (the client does not need a TEE). We then combine the proposed Sparsification Parameter Fine-tuning (SPF) with the LoRA part to improve the accuracy of the down
&lt;/p&gt;</description></item><item><title>PatchAD&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22359;&#30340;MLP-Mixer&#20307;&#31995;&#32467;&#26500;&#65292;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#12290;&#23427;&#20855;&#26377;&#39640;&#25928;&#21644;&#36731;&#37327;&#32423;&#30340;&#26550;&#26500;&#65292;&#24182;&#37319;&#29992;&#21019;&#26032;&#30340;&#21452;&#39033;&#30446;&#32422;&#26463;&#27169;&#22359;&#26469;&#25552;&#39640;&#34920;&#31034;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.09793</link><description>&lt;p&gt;
PatchAD: &#22522;&#20110;&#22359;&#30340;MLP-Mixer&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
PatchAD: Patch-based MLP-Mixer for Time Series Anomaly Detection. (arXiv:2401.09793v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09793
&lt;/p&gt;
&lt;p&gt;
PatchAD&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22359;&#30340;MLP-Mixer&#20307;&#31995;&#32467;&#26500;&#65292;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#12290;&#23427;&#20855;&#26377;&#39640;&#25928;&#21644;&#36731;&#37327;&#32423;&#30340;&#26550;&#26500;&#65292;&#24182;&#37319;&#29992;&#21019;&#26032;&#30340;&#21452;&#39033;&#30446;&#32422;&#26463;&#27169;&#22359;&#26469;&#25552;&#39640;&#34920;&#31034;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#26159;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#20851;&#38190;&#26041;&#38754;&#65292;&#26088;&#22312;&#35782;&#21035;&#26102;&#38388;&#24207;&#21015;&#26679;&#26412;&#20013;&#30340;&#24322;&#24120;&#20107;&#20214;&#12290;&#36825;&#19968;&#20219;&#21153;&#30340;&#26680;&#24515;&#25361;&#25112;&#22312;&#20110;&#22312;&#32570;&#20047;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#23398;&#20064;&#27491;&#24120;&#21644;&#24322;&#24120;&#27169;&#24335;&#30340;&#34920;&#31034;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#22823;&#22810;&#20381;&#36182;&#20110;&#22522;&#20110;&#37325;&#26500;&#30340;&#26041;&#27861;&#65292;&#38480;&#21046;&#20102;&#27169;&#22411;&#30340;&#34920;&#24449;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#24403;&#21069;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#19981;&#22815;&#36731;&#37327;&#32423;&#65292;&#36825;&#20419;&#20351;&#25105;&#20204;&#35774;&#35745;&#19968;&#20010;&#26356;&#39640;&#25928;&#30340;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;PatchAD&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#23610;&#24230;&#22522;&#20110;&#22359;&#30340;MLP-Mixer&#20307;&#31995;&#32467;&#26500;&#65292;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#34920;&#24449;&#25552;&#21462;&#21644;&#24322;&#24120;&#26816;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;PatchAD&#30001;&#22235;&#20010;&#29420;&#29305;&#30340;MLP Mixer&#32452;&#25104;&#65292;&#19987;&#38376;&#21033;&#29992;MLP&#26550;&#26500;&#23454;&#29616;&#39640;&#25928;&#21644;&#36731;&#37327;&#32423;&#30340;&#26550;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21019;&#26032;&#22320;&#35774;&#35745;&#20102;&#19968;&#20010;&#21452;&#39033;&#30446;&#32422;&#26463;&#27169;&#22359;&#26469;&#32531;&#35299;&#28508;&#22312;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection stands as a crucial aspect of time series analysis, aiming to identify abnormal events in time series samples. The central challenge of this task lies in effectively learning the representations of normal and abnormal patterns in a label-lacking scenario. Previous research mostly relied on reconstruction-based approaches, restricting the representational abilities of the models. In addition, most of the current deep learning-based methods are not lightweight enough, which prompts us to design a more efficient framework for anomaly detection. In this study, we introduce PatchAD, a novel multi-scale patch-based MLP-Mixer architecture that leverages contrastive learning for representational extraction and anomaly detection. Specifically, PatchAD is composed of four distinct MLP Mixers, exclusively utilizing the MLP architecture for high efficiency and lightweight architecture. Additionally, we also innovatively crafted a dual project constraint module to mitigate potenti
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#23545;&#20083;&#33146;Faxitron&#21644;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#36827;&#34892;&#37197;&#20934;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#25913;&#21892;&#32452;&#32455;&#30149;&#29702;&#23398;&#36807;&#31243;&#65292;&#24182;&#20026;&#30149;&#29702;&#23398;&#23478;&#36873;&#25321;&#21462;&#26679;&#21306;&#22495;&#25552;&#20379;&#20102;&#26356;&#20934;&#30830;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.09791</link><description>&lt;p&gt;
BreastRegNet: &#19968;&#31181;&#29992;&#20110;&#23545;&#20083;&#33146;Faxitron&#21644;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#36827;&#34892;&#37197;&#20934;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
BreastRegNet: A Deep Learning Framework for Registration of Breast Faxitron and Histopathology Images. (arXiv:2401.09791v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09791
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#23545;&#20083;&#33146;Faxitron&#21644;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#36827;&#34892;&#37197;&#20934;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#25913;&#21892;&#32452;&#32455;&#30149;&#29702;&#23398;&#36807;&#31243;&#65292;&#24182;&#20026;&#30149;&#29702;&#23398;&#23478;&#36873;&#25321;&#21462;&#26679;&#21306;&#22495;&#25552;&#20379;&#20102;&#26356;&#20934;&#30830;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20083;&#33146;&#30284;&#30340;&#26631;&#20934;&#27835;&#30103;&#26041;&#26696;&#21253;&#25324;&#32473;&#20104;&#26032;&#36741;&#21161;&#27835;&#30103;&#65292;&#28982;&#21518;&#25163;&#26415;&#20999;&#38500;&#32959;&#30244;&#21644;&#21608;&#22260;&#32452;&#32455;&#12290;&#30149;&#29702;&#23398;&#23478;&#36890;&#24120;&#20381;&#38752;&#26588;&#24335;X&#23556;&#32447;&#29031;&#30456;&#26426;&#65288;&#31216;&#20026;Faxitron&#65289;&#26469;&#26816;&#26597;&#20999;&#38500;&#30340;&#20083;&#33146;&#32452;&#32455;&#65292;&#24182;&#35786;&#26029;&#27531;&#20313;&#30142;&#30149;&#30340;&#31243;&#24230;&#12290;&#28982;&#32780;&#65292;&#20934;&#30830;&#30830;&#23450;&#27531;&#30041;&#30284;&#30151;&#30340;&#20301;&#32622;&#12289;&#22823;&#23567;&#21644;&#30149;&#28790;&#24615;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#38169;&#35823;&#30340;&#35780;&#20272;&#21487;&#33021;&#23548;&#33268;&#20020;&#24202;&#21518;&#26524;&#12290;&#21033;&#29992;&#33258;&#21160;&#21270;&#26041;&#27861;&#21487;&#20197;&#25913;&#21892;&#32452;&#32455;&#30149;&#29702;&#23398;&#36807;&#31243;&#65292;&#20351;&#30149;&#29702;&#23398;&#23478;&#33021;&#22815;&#26356;&#26377;&#25928;&#12289;&#26356;&#31934;&#30830;&#22320;&#36873;&#25321;&#21462;&#26679;&#21306;&#22495;&#12290;&#23613;&#31649;&#35748;&#35782;&#21040;&#36825;&#19968;&#24517;&#35201;&#24615;&#65292;&#20294;&#30446;&#21069;&#36824;&#27809;&#26377;&#27492;&#31867;&#26041;&#27861;&#12290;&#35757;&#32451;&#36825;&#31181;&#33258;&#21160;&#26816;&#27979;&#27169;&#22411;&#38656;&#35201;&#22312;&#31163;&#20307;&#25918;&#23556;&#23398;&#22270;&#20687;&#19978;&#33719;&#21462;&#20934;&#30830;&#30340;&#22522;&#26412;&#30495;&#20540;&#26631;&#31614;&#65292;&#21487;&#20197;&#36890;&#36807;&#27880;&#20876;Faxitron&#21644;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#65292;&#24182;&#23545;&#30284;&#30151;&#30340;&#33539;&#22260;&#36827;&#34892;&#26144;&#23556;&#26469;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
A standard treatment protocol for breast cancer entails administering neoadjuvant therapy followed by surgical removal of the tumor and surrounding tissue. Pathologists typically rely on cabinet X-ray radiographs, known as Faxitron, to examine the excised breast tissue and diagnose the extent of residual disease. However, accurately determining the location, size, and focality of residual cancer can be challenging, and incorrect assessments can lead to clinical consequences. The utilization of automated methods can improve the histopathology process, allowing pathologists to choose regions for sampling more effectively and precisely. Despite the recognized necessity, there are currently no such methods available. Training such automated detection models require accurate ground truth labels on ex-vivo radiology images, which can be acquired through registering Faxitron and histopathology images and mapping the extent of cancer from histopathology to x-ray images. This study introduces a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#65292;&#21363;&#26368;&#23567;&#19981;&#19968;&#33268;&#24230;&#37327;&#65288;LDM&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#22797;&#26434;&#20915;&#31574;&#36793;&#30028;&#24773;&#20917;&#19979;&#30340;&#20027;&#21160;&#23398;&#20064;&#38382;&#39064;&#12290;&#36890;&#36807;&#26597;&#35810;&#20855;&#26377;&#26368;&#23567;LDM&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#21487;&#20197;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.09787</link><description>&lt;p&gt;
&#26597;&#35810;&#26131;&#20110;&#32763;&#36716;&#26679;&#26412;&#30340;&#28145;&#24230;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Querying Easily Flip-flopped Samples for Deep Active Learning. (arXiv:2401.09787v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09787
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#65292;&#21363;&#26368;&#23567;&#19981;&#19968;&#33268;&#24230;&#37327;&#65288;LDM&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#22797;&#26434;&#20915;&#31574;&#36793;&#30028;&#24773;&#20917;&#19979;&#30340;&#20027;&#21160;&#23398;&#20064;&#38382;&#39064;&#12290;&#36890;&#36807;&#26597;&#35810;&#20855;&#26377;&#26368;&#23567;LDM&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#21487;&#20197;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#23398;&#20064;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#26088;&#22312;&#36890;&#36807;&#36873;&#25321;&#21644;&#26597;&#35810;&#26410;&#26631;&#35760;&#25968;&#25454;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#19968;&#31181;&#26377;&#25928;&#30340;&#36873;&#25321;&#31574;&#30053;&#26159;&#22522;&#20110;&#27169;&#22411;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65292;&#36825;&#21487;&#20197;&#35299;&#37322;&#20026;&#26679;&#26412;&#30340;&#20449;&#24687;&#37327;&#24230;&#37327;&#12290;&#26679;&#26412;&#21040;&#20915;&#31574;&#36793;&#30028;&#30340;&#36317;&#31163;&#26159;&#19968;&#31181;&#33258;&#28982;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#65292;&#20294;&#36890;&#24120;&#38590;&#20197;&#35745;&#31639;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#22810;&#31867;&#20998;&#31867;&#20219;&#21153;&#20013;&#24418;&#25104;&#30340;&#22797;&#26434;&#20915;&#31574;&#36793;&#30028;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#8220;&#26368;&#23567;&#19981;&#19968;&#33268;&#24230;&#37327;&#8221;&#65288;LDM&#65289;&#65292;&#23450;&#20041;&#20026;&#39044;&#27979;&#26631;&#31614;&#19981;&#19968;&#33268;&#30340;&#26368;&#23567;&#27010;&#29575;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;LDM&#30340;&#20272;&#35745;&#22120;&#22312;&#28201;&#21644;&#20551;&#35774;&#19979;&#26159;&#28176;&#36817;&#19968;&#33268;&#30340;&#12290;&#35813;&#20272;&#35745;&#22120;&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#21442;&#25968;&#25200;&#21160;&#36731;&#26494;&#23454;&#29616;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#20351;&#29992;&#12290;&#22522;&#20110;LDM&#30340;&#20027;&#21160;&#23398;&#20064;&#36890;&#36807;&#26597;&#35810;&#20855;&#26377;&#26368;&#23567;LDM&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#26469;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Active learning is a machine learning paradigm that aims to improve the performance of a model by strategically selecting and querying unlabeled data. One effective selection strategy is to base it on the model's predictive uncertainty, which can be interpreted as a measure of how informative a sample is. The sample's distance to the decision boundary is a natural measure of predictive uncertainty, but it is often intractable to compute, especially for complex decision boundaries formed in multiclass classification tasks. To address this issue, this paper proposes the {\it least disagree metric} (LDM), defined as the smallest probability of disagreement of the predicted label, and an estimator for LDM proven to be asymptotically consistent under mild assumptions. The estimator is computationally efficient and can be easily implemented for deep learning models using parameter perturbation. The LDM-based active learning is performed by querying unlabeled data with the smallest LDM. Exper
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#32508;&#21512;&#27010;&#36848;&#20102;&#20851;&#20110;&#20174;&#24102;&#26377;&#24322;&#36136;&#24615;&#30340;&#22270;&#20013;&#23398;&#20064;&#30340;&#29616;&#26377;&#30740;&#31350;&#65292;&#24182;&#26681;&#25454;&#23398;&#20064;&#31574;&#30053;&#12289;&#27169;&#22411;&#26550;&#26500;&#21644;&#23454;&#38469;&#24212;&#29992;&#31561;&#26041;&#38754;&#23545;&#26041;&#27861;&#36827;&#34892;&#20102;&#20998;&#31867;&#12290;&#21516;&#26102;&#35752;&#35770;&#20102;&#29616;&#26377;&#30740;&#31350;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2401.09769</link><description>&lt;p&gt;
&#36208;&#21521;&#24322;&#36136;&#22270;&#23398;&#20064;&#65306;&#36827;&#23637;&#19982;&#26410;&#26469;
&lt;/p&gt;
&lt;p&gt;
Towards Learning from Graphs with Heterophily: Progress and Future. (arXiv:2401.09769v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09769
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#32508;&#21512;&#27010;&#36848;&#20102;&#20851;&#20110;&#20174;&#24102;&#26377;&#24322;&#36136;&#24615;&#30340;&#22270;&#20013;&#23398;&#20064;&#30340;&#29616;&#26377;&#30740;&#31350;&#65292;&#24182;&#26681;&#25454;&#23398;&#20064;&#31574;&#30053;&#12289;&#27169;&#22411;&#26550;&#26500;&#21644;&#23454;&#38469;&#24212;&#29992;&#31561;&#26041;&#38754;&#23545;&#26041;&#27861;&#36827;&#34892;&#20102;&#20998;&#31867;&#12290;&#21516;&#26102;&#35752;&#35770;&#20102;&#29616;&#26377;&#30740;&#31350;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#26159;&#29992;&#26469;&#27169;&#25311;&#29616;&#23454;&#19990;&#30028;&#23454;&#20307;&#20043;&#38388;&#22797;&#26434;&#20851;&#31995;&#30340;&#32467;&#26500;&#21270;&#25968;&#25454;&#12290;&#26368;&#36817;&#65292;&#24322;&#36136;&#22270;&#65292;&#20854;&#20013;&#36830;&#25509;&#30340;&#33410;&#28857;&#24448;&#24448;&#20855;&#26377;&#19981;&#21516;&#30340;&#26631;&#31614;&#25110;&#19981;&#21516;&#30340;&#29305;&#24449;&#65292;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#24182;&#25214;&#21040;&#20102;&#35768;&#22810;&#24212;&#29992;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#20154;&#20204;&#20063;&#22312;&#19981;&#26029;&#21162;&#21147;&#25512;&#36827;&#20174;&#24322;&#36136;&#22270;&#20013;&#23398;&#20064;&#12290;&#34429;&#28982;&#26377;&#20851;&#35813;&#20027;&#39064;&#30340;&#35843;&#26597;&#23384;&#22312;&#65292;&#20294;&#23427;&#20204;&#21482;&#20851;&#27880;&#20110;&#24322;&#36136;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#65292;&#32780;&#24573;&#30053;&#20102;&#24322;&#36136;&#22270;&#23398;&#20064;&#30340;&#20854;&#20182;&#23376;&#20027;&#39064;&#12290;&#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#22238;&#39038;&#20102;&#20851;&#20110;&#20174;&#24102;&#26377;&#24322;&#36136;&#24615;&#30340;&#22270;&#20013;&#23398;&#20064;&#30340;&#29616;&#26377;&#30740;&#31350;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;180&#22810;&#31687;&#35770;&#25991;&#65292;&#20171;&#32461;&#20102;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26681;&#25454;&#23618;&#27425;&#20998;&#31867;&#27861;&#23545;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#31867;&#65292;&#21253;&#25324;&#23398;&#20064;&#31574;&#30053;&#12289;&#27169;&#22411;&#26550;&#26500;&#21644;&#23454;&#38469;&#24212;&#29992;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#29616;&#26377;&#30740;&#31350;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#24182;&#31361;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graphs are structured data that models complex relations between real-world entities. Heterophilous graphs, where linked nodes are prone to be with different labels or dissimilar features, have recently attracted significant attention and found many applications. Meanwhile, increasing efforts have been made to advance learning from heterophilous graphs. Although there exist surveys on the relevant topic, they focus on heterophilous GNNs, which are only sub-topics of heterophilous graph learning. In this survey, we comprehensively overview existing works on learning from graphs with heterophily.First, we collect over 180 publications and introduce the development of this field. Then, we systematically categorize existing methods based on a hierarchical taxonomy including learning strategies, model architectures and practical applications. Finally, we discuss the primary challenges of existing studies and highlight promising avenues for future research.More publication details and corres
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;-DBShap&#65292;&#20351;&#29992;Shapley&#20540;&#26469;&#30830;&#23450;&#27169;&#22411;&#24615;&#33021;&#28418;&#31227;&#30340;&#20027;&#35201;&#36129;&#29486;&#32773;&#24182;&#37327;&#21270;&#20182;&#20204;&#30340;&#36129;&#29486;&#12290;&#36890;&#36807;DBShap&#25552;&#20379;&#30340;&#35299;&#37322;&#65292;&#21487;&#20197;&#29702;&#35299;&#28418;&#31227;&#32972;&#21518;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;</title><link>http://arxiv.org/abs/2401.09756</link><description>&lt;p&gt;
&#20351;&#29992;Shapley&#20540;&#35299;&#37322;&#28418;&#31227;
&lt;/p&gt;
&lt;p&gt;
Explaining Drift using Shapley Values. (arXiv:2401.09756v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09756
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;-DBShap&#65292;&#20351;&#29992;Shapley&#20540;&#26469;&#30830;&#23450;&#27169;&#22411;&#24615;&#33021;&#28418;&#31227;&#30340;&#20027;&#35201;&#36129;&#29486;&#32773;&#24182;&#37327;&#21270;&#20182;&#20204;&#30340;&#36129;&#29486;&#12290;&#36890;&#36807;DBShap&#25552;&#20379;&#30340;&#35299;&#37322;&#65292;&#21487;&#20197;&#29702;&#35299;&#28418;&#31227;&#32972;&#21518;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#34987;&#29992;&#20110;&#39044;&#27979;&#20854;&#26410;&#35757;&#32451;&#25968;&#25454;&#30340;&#32467;&#26524;&#26102;&#65292;&#20854;&#24615;&#33021;&#24120;&#24120;&#20250;&#19979;&#38477;&#12290;&#36825;&#31181;&#24773;&#20917;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#32463;&#24120;&#21457;&#29983;&#65292;&#22240;&#20026;&#25968;&#25454;&#30340;&#20998;&#24067;&#20250;&#36880;&#28176;&#25110;&#31361;&#28982;&#22320;&#21457;&#29983;&#21464;&#21270;&#65292;&#25110;&#30001;&#20110;&#20687;&#22823;&#27969;&#34892;&#30149;&#36825;&#26679;&#30340;&#37325;&#22823;&#20107;&#20214;&#12290;&#22312;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20013;&#65292;&#24050;&#32463;&#26377;&#35768;&#22810;&#23581;&#35797;&#25552;&#20986;&#33021;&#22815;&#25269;&#24481;&#36825;&#31181;&#27010;&#24565;&#28418;&#31227;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#27809;&#26377;&#19968;&#20010;&#21407;&#21017;&#24615;&#30340;&#26694;&#26550;&#26469;&#30830;&#23450;&#23548;&#33268;&#27169;&#22411;&#24615;&#33021;&#28418;&#31227;&#30340;&#21407;&#22240;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;-DBShap&#65292;&#23427;&#20351;&#29992;Shapley&#20540;&#26469;&#30830;&#23450;&#28418;&#31227;&#30340;&#20027;&#35201;&#36129;&#29486;&#32773;&#24182;&#37327;&#21270;&#20182;&#20204;&#30340;&#36129;&#29486;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#19981;&#20165;&#37327;&#21270;&#20102;&#21333;&#20010;&#29305;&#24449;&#22312;&#39537;&#21160;&#28418;&#31227;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#65292;&#36824;&#21253;&#25324;&#20102;&#36755;&#20837;&#21644;&#36755;&#20986;&#20043;&#38388;&#24213;&#23618;&#20851;&#31995;&#30340;&#21464;&#21270;&#20316;&#20026;&#21487;&#33021;&#30340;&#39537;&#21160;&#22240;&#32032;&#12290;DBShap&#25152;&#25552;&#20379;&#30340;&#35299;&#37322;&#21487;&#20197;&#29992;&#20110;&#29702;&#35299;&#28418;&#31227;&#32972;&#21518;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models often deteriorate in their performance when they are used to predict the outcomes over data on which they were not trained. These scenarios can often arise in real world when the distribution of data changes gradually or abruptly due to major events like a pandemic. There have been many attempts in machine learning research to come up with techniques that are resilient to such Concept drifts. However, there is no principled framework to identify the drivers behind the drift in model performance. In this paper, we propose a novel framework - DBShap that uses Shapley values to identify the main contributors of the drift and quantify their respective contributions. The proposed framework not only quantifies the importance of individual features in driving the drift but also includes the change in the underlying relation between the input and output as a possible driver. The explanation provided by DBShap can be used to understand the root cause behind the drift and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20445;&#25345;&#37051;&#23621;&#30456;&#20284;&#24615;&#23454;&#29616;&#20102;&#26222;&#36866;&#40065;&#26834;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#22312;&#24322;&#31867;&#22270;&#19978;&#25506;&#32034;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#33030;&#24369;&#24615;&#12290;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#36127;&#20998;&#31867;&#25439;&#22833;&#30340;&#26356;&#26032;&#19982;&#22522;&#20110;&#37051;&#23621;&#29305;&#24449;&#30340;&#25104;&#23545;&#30456;&#20284;&#24615;&#21576;&#36127;&#30456;&#20851;&#65292;&#35299;&#37322;&#20102;&#22270;&#25915;&#20987;&#32773;&#36830;&#25509;&#19981;&#30456;&#20284;&#33410;&#28857;&#23545;&#30340;&#34892;&#20026;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#26032;&#39062;&#22320;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2401.09754</link><description>&lt;p&gt;
&#36890;&#36807;&#20445;&#25345;&#37051;&#23621;&#30456;&#20284;&#24615;&#23454;&#29616;&#26222;&#36866;&#40065;&#26834;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Universally Robust Graph Neural Networks by Preserving Neighbor Similarity. (arXiv:2401.09754v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09754
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20445;&#25345;&#37051;&#23621;&#30456;&#20284;&#24615;&#23454;&#29616;&#20102;&#26222;&#36866;&#40065;&#26834;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#22312;&#24322;&#31867;&#22270;&#19978;&#25506;&#32034;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#33030;&#24369;&#24615;&#12290;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#36127;&#20998;&#31867;&#25439;&#22833;&#30340;&#26356;&#26032;&#19982;&#22522;&#20110;&#37051;&#23621;&#29305;&#24449;&#30340;&#25104;&#23545;&#30456;&#20284;&#24615;&#21576;&#36127;&#30456;&#20851;&#65292;&#35299;&#37322;&#20102;&#22270;&#25915;&#20987;&#32773;&#36830;&#25509;&#19981;&#30456;&#20284;&#33410;&#28857;&#23545;&#30340;&#34892;&#20026;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#26032;&#39062;&#22320;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#23398;&#20064;&#20851;&#31995;&#25968;&#25454;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#24050;&#32463;&#24191;&#27867;&#30740;&#31350;&#21457;&#29616;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#21516;&#31867;&#22270;&#19978;&#23481;&#26131;&#21463;&#21040;&#32467;&#26500;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#40065;&#26834;&#27169;&#22411;&#65292;&#20197;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#21516;&#31867;&#22270;&#19978;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#24322;&#31867;&#22270;&#19978;&#30340;&#33030;&#24369;&#24615;&#20173;&#28982;&#23384;&#22312;&#35768;&#22810;&#26410;&#35299;&#20043;&#35868;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#26412;&#25991;&#24320;&#22987;&#25506;&#32034;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#24322;&#31867;&#22270;&#19978;&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#36127;&#20998;&#31867;&#25439;&#22833;&#30340;&#26356;&#26032;&#19982;&#22522;&#20110;&#37051;&#23621;&#29305;&#24449;&#30340;&#24130;&#21644;&#32858;&#21512;&#30340;&#25104;&#23545;&#30456;&#20284;&#24615;&#21576;&#36127;&#30456;&#20851;&#12290;&#36825;&#19968;&#29702;&#35770;&#35777;&#26126;&#35299;&#37322;&#20102;&#23454;&#35777;&#35266;&#23519;&#65292;&#21363;&#22270;&#25915;&#20987;&#32773;&#20542;&#21521;&#20110;&#22522;&#20110;&#37051;&#23621;&#29305;&#24449;&#32780;&#19981;&#26159;&#20010;&#20307;&#29305;&#24449;&#36830;&#25509;&#19981;&#30456;&#20284;&#33410;&#28857;&#23545;&#65292;&#26080;&#35770;&#26159;&#22312;&#21516;&#31867;&#22270;&#36824;&#26159;&#24322;&#31867;&#22270;&#19978;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#26032;&#39062;&#22320;&#24341;&#20837;&#20102;&#19968;&#31181;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Despite the tremendous success of graph neural networks in learning relational data, it has been widely investigated that graph neural networks are vulnerable to structural attacks on homophilic graphs. Motivated by this, a surge of robust models is crafted to enhance the adversarial robustness of graph neural networks on homophilic graphs. However, the vulnerability based on heterophilic graphs remains a mystery to us. To bridge this gap, in this paper, we start to explore the vulnerability of graph neural networks on heterophilic graphs and theoretically prove that the update of the negative classification loss is negatively correlated with the pairwise similarities based on the powered aggregated neighbor features. This theoretical proof explains the empirical observations that the graph attacker tends to connect dissimilar node pairs based on the similarities of neighbor features instead of ego features both on homophilic and heterophilic graphs. In this way, we novelly introduce a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#21270;&#24037;&#21644;&#32858;&#28911;&#28867;&#21046;&#36896;&#20248;&#21270;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#26680;&#24515;&#26426;&#22120;&#23398;&#20064;&#32452;&#20214;&#12289;&#21508;&#31181;&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#65292;&#24182;&#25552;&#20986;&#20102;&#31185;&#23398;&#24341;&#23548;&#30340;&#26426;&#22120;&#23398;&#20064;&#30340;&#28151;&#21512;&#26041;&#27861;&#26469;&#22686;&#24378;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.09753</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#32858;&#28911;&#28867;&#21046;&#36896;&#20248;&#21270;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Applications of Machine Learning to Optimizing Polyolefin Manufacturing. (arXiv:2401.09753v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09753
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#21270;&#24037;&#21644;&#32858;&#28911;&#28867;&#21046;&#36896;&#20248;&#21270;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#26680;&#24515;&#26426;&#22120;&#23398;&#20064;&#32452;&#20214;&#12289;&#21508;&#31181;&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#65292;&#24182;&#25552;&#20986;&#20102;&#31185;&#23398;&#24341;&#23548;&#30340;&#26426;&#22120;&#23398;&#20064;&#30340;&#28151;&#21512;&#26041;&#27861;&#26469;&#22686;&#24378;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#31456;&#33410;&#26159;&#25105;&#20204;&#30340;&#22270;&#20070;&#30340;&#39044;&#21360;&#26412;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#22914;&#20309;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#22312;&#21270;&#24037;&#21644;&#32858;&#28911;&#28867;&#21046;&#36896;&#20248;&#21270;&#20013;&#30340;&#24212;&#29992;&#12290;&#23427;&#26082;&#36866;&#21512;&#21021;&#23398;&#32773;&#65292;&#20063;&#36866;&#21512;&#37027;&#20123;&#23545;&#21270;&#23398;&#36807;&#31243;&#20013;&#26368;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#24863;&#20852;&#36259;&#30340;&#19987;&#19994;&#20154;&#22763;&#12290;&#25105;&#20204;&#36861;&#36394;&#20102;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#22312;&#21270;&#23398;&#34892;&#19994;&#20013;&#30340;&#21457;&#23637;&#65292;&#25551;&#36848;&#20102;&#26680;&#24515;&#30340;&#26426;&#22120;&#23398;&#20064;&#32452;&#20214;&#65292;&#24182;&#20026;&#21021;&#23398;&#32773;&#25552;&#20379;&#20102;&#23398;&#20064;&#36164;&#28304;&#12290;&#25105;&#20204;&#35814;&#32454;&#35752;&#35770;&#20102;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21253;&#25324;&#22238;&#24402;&#12289;&#20998;&#31867;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#65292;&#32473;&#20986;&#20102;&#24615;&#33021;&#25351;&#26631;&#21644;&#31034;&#20363;&#12290;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#38598;&#25104;&#26041;&#27861;&#12289;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#65288;&#21253;&#25324;MLP&#12289;DNN&#12289;RNN&#12289;CNN&#21644;transformers&#65289;&#22312;&#21270;&#23398;&#24212;&#29992;&#20013;&#19981;&#26029;&#22686;&#38271;&#30340;&#20316;&#29992;&#12290;&#23454;&#36341;&#30740;&#35752;&#20250;&#36890;&#36807;&#20351;&#29992;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#25351;&#23548;&#35835;&#32773;&#36827;&#34892;&#39044;&#27979;&#24314;&#27169;&#12290;&#26412;&#31456;&#26368;&#32456;&#25552;&#20986;&#20102;&#31185;&#23398;&#24341;&#23548;&#30340;&#26426;&#22120;&#23398;&#20064;&#30340;&#35265;&#35299;&#65292;&#20027;&#24352;&#37319;&#29992;&#28151;&#21512;&#26041;&#27861;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#24191;&#27867;&#30340;&#21442;&#32771;&#25991;&#29486;&#25552;&#20379;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
This chapter is a preprint from our book by , focusing on leveraging machine learning (ML) in chemical and polyolefin manufacturing optimization. It's crafted for both novices and seasoned professionals keen on the latest ML applications in chemical processes. We trace the evolution of AI and ML in chemical industries, delineate core ML components, and provide resources for ML beginners. A detailed discussion on various ML methods is presented, covering regression, classification, and unsupervised learning techniques, with performance metrics and examples. Ensemble methods, deep learning networks, including MLP, DNNs, RNNs, CNNs, and transformers, are explored for their growing role in chemical applications. Practical workshops guide readers through predictive modeling using advanced ML algorithms. The chapter culminates with insights into science-guided ML, advocating for a hybrid approach that enhances model accuracy. The extensive bibliography offers resources for further research a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#32852;&#21512;&#20998;&#24067;&#36866;&#24212;&#65288;DJDA&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#32852;&#21512;&#20998;&#24067;&#36866;&#24212;&#26469;&#28040;&#38500;&#22810;&#35828;&#35805;&#20154;&#24341;&#36215;&#30340;&#22810;&#22495;&#36716;&#31227;&#25361;&#25112;&#65292;&#24182;&#23398;&#20064;&#20855;&#26377;&#36776;&#21035;&#24615;&#21644;&#35828;&#35805;&#20154;&#26080;&#20851;&#24615;&#30340;&#35821;&#38899;&#24773;&#24863;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2401.09752</link><description>&lt;p&gt;
&#20351;&#29992;&#21160;&#24577;&#32852;&#21512;&#20998;&#24067;&#36866;&#24212;&#25552;&#39640;&#35828;&#35805;&#20154;&#26080;&#20851;&#30340;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Improving Speaker-independent Speech Emotion Recognition Using Dynamic Joint Distribution Adaptation. (arXiv:2401.09752v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09752
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#32852;&#21512;&#20998;&#24067;&#36866;&#24212;&#65288;DJDA&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#32852;&#21512;&#20998;&#24067;&#36866;&#24212;&#26469;&#28040;&#38500;&#22810;&#35828;&#35805;&#20154;&#24341;&#36215;&#30340;&#22810;&#22495;&#36716;&#31227;&#25361;&#25112;&#65292;&#24182;&#23398;&#20064;&#20855;&#26377;&#36776;&#21035;&#24615;&#21644;&#35828;&#35805;&#20154;&#26080;&#20851;&#24615;&#30340;&#35821;&#38899;&#24773;&#24863;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35828;&#35805;&#20154;&#26080;&#20851;&#30340;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#20013;&#65292;&#35757;&#32451;&#21644;&#27979;&#35797;&#26679;&#26412;&#26469;&#33258;&#19981;&#21516;&#30340;&#35828;&#35805;&#20154;&#65292;&#23548;&#33268;&#20102;&#19981;&#21516;&#35828;&#35805;&#20154;&#25968;&#25454;&#30340;&#29305;&#24449;&#20998;&#24067;&#20043;&#38388;&#23384;&#22312;&#22810;&#22495;&#36716;&#31227;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#24403;&#35757;&#32451;&#27169;&#22411;&#38754;&#23545;&#26469;&#33258;&#26032;&#35828;&#35805;&#20154;&#30340;&#25968;&#25454;&#26102;&#65292;&#20854;&#24615;&#33021;&#24448;&#24448;&#20250;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#32852;&#21512;&#20998;&#24067;&#36866;&#24212;&#65288;DJDA&#65289;&#26041;&#27861;&#65292;&#22312;&#22810;&#28304;&#22495;&#36866;&#24212;&#26694;&#26550;&#19979;&#20351;&#29992;&#12290;DJDA&#39318;&#20808;&#21033;&#29992;&#32852;&#21512;&#20998;&#24067;&#36866;&#24212;&#65288;JDA&#65289;&#65292;&#21253;&#25324;&#36793;&#32536;&#20998;&#24067;&#36866;&#24212;&#65288;MDA&#65289;&#21644;&#26465;&#20214;&#20998;&#24067;&#36866;&#24212;&#65288;CDA&#65289;&#65292;&#26356;&#31934;&#30830;&#22320;&#37327;&#21270;&#19981;&#21516;&#35828;&#35805;&#20154;&#24341;&#36215;&#30340;&#22810;&#22495;&#20998;&#24067;&#24046;&#24322;&#12290;&#36825;&#26377;&#21161;&#20110;&#28040;&#38500;&#24773;&#24863;&#29305;&#24449;&#20013;&#30340;&#35828;&#35805;&#20154;&#20559;&#24046;&#65292;&#20174;&#31895;&#31890;&#24230;&#21040;&#32454;&#31890;&#24230;&#23398;&#20064;&#20855;&#26377;&#36776;&#21035;&#24615;&#21644;&#35828;&#35805;&#20154;&#26080;&#20851;&#24615;&#30340;&#35821;&#38899;&#24773;&#24863;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#21160;&#24577;&#26435;&#34913;&#26041;&#27861;&#37327;&#21270;JDA&#20013;MDA&#21644;CDA&#30340;&#36866;&#24212;&#36129;&#29486;&#26469;&#23545;&#20854;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
In speaker-independent speech emotion recognition, the training and testing samples are collected from diverse speakers, leading to a multi-domain shift challenge across the feature distributions of data from different speakers. Consequently, when the trained model is confronted with data from new speakers, its performance tends to degrade. To address the issue, we propose a Dynamic Joint Distribution Adaptation (DJDA) method under the framework of multi-source domain adaptation. DJDA firstly utilizes joint distribution adaptation (JDA), involving marginal distribution adaptation (MDA) and conditional distribution adaptation (CDA), to more precisely measure the multi-domain distribution shifts caused by different speakers. This helps eliminate speaker bias in emotion features, allowing for learning discriminative and speaker-invariant speech emotion features from coarse-level to fine-level. Furthermore, we quantify the adaptation contributions of MDA and CDA within JDA by using a dynam
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25506;&#32034;&#31639;&#27861;&#65292;&#31216;&#20026;&#20998;&#24067;&#24335;&#38543;&#26426;&#32593;&#32476;&#33976;&#39311;&#65288;DRND&#65289;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#33976;&#39311;&#38543;&#26426;&#32593;&#32476;&#30340;&#20998;&#24067;&#21644;&#38544;&#24335;&#34701;&#20837;&#20266;&#35745;&#25968;&#26469;&#25913;&#36827;&#22870;&#21169;&#20998;&#37197;&#30340;&#31934;&#24230;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#25506;&#32034;&#36807;&#31243;&#12290;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#32467;&#26524;&#22343;&#34920;&#26126;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.09750</link><description>&lt;p&gt;
&#20351;&#29992;&#20998;&#24067;&#24335;&#38543;&#26426;&#32593;&#32476;&#33976;&#39311;&#30340;&#25506;&#32034;&#21644;&#21453;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Exploration and Anti-Exploration with Distributional Random Network Distillation. (arXiv:2401.09750v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09750
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25506;&#32034;&#31639;&#27861;&#65292;&#31216;&#20026;&#20998;&#24067;&#24335;&#38543;&#26426;&#32593;&#32476;&#33976;&#39311;&#65288;DRND&#65289;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#33976;&#39311;&#38543;&#26426;&#32593;&#32476;&#30340;&#20998;&#24067;&#21644;&#38544;&#24335;&#34701;&#20837;&#20266;&#35745;&#25968;&#26469;&#25913;&#36827;&#22870;&#21169;&#20998;&#37197;&#30340;&#31934;&#24230;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#25506;&#32034;&#36807;&#31243;&#12290;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#32467;&#26524;&#22343;&#34920;&#26126;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#25506;&#32034;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#23545;&#20110;&#19968;&#20010;&#26234;&#33021;&#20307;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#21462;&#24471;&#39640;&#22238;&#25253;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#30446;&#21069;&#30340;&#25506;&#32034;&#38543;&#26426;&#32593;&#32476;&#33976;&#39311;&#65288;Random Network Distillation&#65292;RND&#65289;&#31639;&#27861;&#24050;&#22312;&#35768;&#22810;&#29615;&#22659;&#20013;&#35777;&#26126;&#26377;&#25928;&#65292;&#20294;&#23427;&#22312;&#22870;&#21169;&#20998;&#37197;&#19978;&#24448;&#24448;&#38656;&#35201;&#26356;&#39640;&#30340;&#21306;&#20998;&#33021;&#21147;&#12290;&#26412;&#25991;&#31361;&#20986;&#20102;RND&#20013;&#30340;&#8220;&#22870;&#21169;&#19981;&#19968;&#33268;&#8221;&#38382;&#39064;&#65292;&#24182;&#25351;&#20986;&#20102;&#20854;&#20027;&#35201;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20998;&#24067;&#24335;RND&#65288;DRND&#65289;&#65292;&#23427;&#26159;RND&#30340;&#19968;&#20010;&#21464;&#20307;&#12290;DRND&#36890;&#36807;&#33976;&#39311;&#38543;&#26426;&#32593;&#32476;&#30340;&#20998;&#24067;&#24182;&#38544;&#24335;&#22320;&#34701;&#20837;&#20266;&#35745;&#25968;&#26469;&#25913;&#36827;&#22870;&#21169;&#20998;&#37197;&#30340;&#31934;&#24230;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#25506;&#32034;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#32531;&#35299;&#20102;&#19981;&#19968;&#33268;&#38382;&#39064;&#65292;&#32780;&#19981;&#20250;&#24341;&#20837;&#26174;&#33879;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#32467;&#26524;&#22343;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploration remains a critical issue in deep reinforcement learning for an agent to attain high returns in unknown environments. Although the prevailing exploration Random Network Distillation (RND) algorithm has been demonstrated to be effective in numerous environments, it often needs more discriminative power in bonus allocation. This paper highlights the ``bonus inconsistency'' issue within RND, pinpointing its primary limitation. To address this issue, we introduce the Distributional RND (DRND), a derivative of the RND. DRND enhances the exploration process by distilling a distribution of random networks and implicitly incorporating pseudo counts to improve the precision of bonus allocation. This refinement encourages agents to engage in more extensive exploration. Our method effectively mitigates the inconsistency issue without introducing significant computational overhead. Both theoretical analysis and experimental results demonstrate the superiority of our approach over the or
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#20989;&#25968;&#22270;&#20687;&#21644;&#25805;&#20316;&#26641;&#24207;&#21015;&#30340;&#31185;&#23398;&#35745;&#31639;&#22810;&#27169;&#24577;&#26694;&#26550;&#65288;Botfip&#65289;&#65292;&#24212;&#29992;&#20110;&#31526;&#21495;&#22238;&#24402;&#38382;&#39064;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#22312;&#20302;&#22797;&#26434;&#24230;&#38382;&#39064;&#19978;&#30340;&#20248;&#21183;&#65292;&#23637;&#31034;&#20102;&#20854;&#28508;&#21147;&#12290;&#36825;&#20010;&#22810;&#27169;&#24577;&#26694;&#26550;&#22312;&#31185;&#23398;&#35745;&#31639;&#38382;&#39064;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2401.09748</link><description>&lt;p&gt;
Bootstrapping OTS-Funcimg Pre-training Model (Botfip) -- &#19968;&#20010;&#20840;&#38754;&#30340;&#31526;&#21495;&#22238;&#24402;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Bootstrapping OTS-Funcimg Pre-training Model (Botfip) -- A Comprehensive Symbolic Regression Framework. (arXiv:2401.09748v1 [cs.SC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09748
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#20989;&#25968;&#22270;&#20687;&#21644;&#25805;&#20316;&#26641;&#24207;&#21015;&#30340;&#31185;&#23398;&#35745;&#31639;&#22810;&#27169;&#24577;&#26694;&#26550;&#65288;Botfip&#65289;&#65292;&#24212;&#29992;&#20110;&#31526;&#21495;&#22238;&#24402;&#38382;&#39064;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#22312;&#20302;&#22797;&#26434;&#24230;&#38382;&#39064;&#19978;&#30340;&#20248;&#21183;&#65292;&#23637;&#31034;&#20102;&#20854;&#28508;&#21147;&#12290;&#36825;&#20010;&#22810;&#27169;&#24577;&#26694;&#26550;&#22312;&#31185;&#23398;&#35745;&#31639;&#38382;&#39064;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31185;&#23398;&#35745;&#31639;&#39046;&#22495;&#20013;&#65292;&#35768;&#22810;&#38382;&#39064;&#35299;&#20915;&#26041;&#27861;&#24448;&#24448;&#21482;&#27880;&#37325;&#36807;&#31243;&#21644;&#26368;&#32456;&#32467;&#26524;&#65292;&#21363;&#20351;&#22312;&#31185;&#23398;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#20063;&#32570;&#20047;&#23545;&#25968;&#25454;&#32972;&#21518;&#30340;&#28145;&#24230;&#22810;&#27169;&#24577;&#20449;&#24687;&#25366;&#25496;&#65292;&#32570;&#20047;&#31867;&#20284;&#20110;&#22270;&#20687;&#25991;&#26412;&#39046;&#22495;&#30340;&#22810;&#27169;&#24577;&#26694;&#26550;&#12290;&#26412;&#25991;&#20197;&#31526;&#21495;&#22238;&#24402;&#65288;SR&#65289;&#20026;&#37325;&#28857;&#65292;&#22312;&#22270;&#20687;&#25991;&#26412;&#39046;&#22495;&#30340;BLIP&#27169;&#22411;&#30340;&#21551;&#21457;&#19979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20989;&#25968;&#22270;&#20687;&#65288;Funcimg&#65289;&#21644;&#25805;&#20316;&#26641;&#24207;&#21015;&#65288;OTS&#65289;&#30340;&#31185;&#23398;&#35745;&#31639;&#22810;&#27169;&#24577;&#26694;&#26550;&#8212;&#8212;&#24341;&#23548;OTS-Funcimg&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;Botfip&#65289;&#12290;&#22312;SR&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;Botfip&#22312;&#20302;&#22797;&#26434;&#24230;&#30340;SR&#38382;&#39064;&#20013;&#30340;&#20248;&#21183;&#65292;&#23637;&#31034;&#20102;&#20854;&#28508;&#21147;&#12290;&#20316;&#20026;&#19968;&#20010;MED&#26694;&#26550;&#65292;Botfip&#22312;&#26356;&#24191;&#27867;&#30340;&#31185;&#23398;&#35745;&#31639;&#38382;&#39064;&#20013;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of scientific computing, many problem-solving approaches tend to focus only on the process and final outcome, even in AI for science, there is a lack of deep multimodal information mining behind the data, missing a multimodal framework akin to that in the image-text domain. In this paper, we take Symbolic Regression(SR) as our focal point and, drawing inspiration from the BLIP model in the image-text domain, propose a scientific computing multimodal framework based on Function Images (Funcimg) and Operation Tree Sequence (OTS), named Bootstrapping OTS-Funcimg Pre-training Model (Botfip). In SR experiments, we validate the advantages of Botfip in low-complexity SR problems, showcasing its potential. As a MED framework, Botfip holds promise for future applications in a broader range of scientific computing problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#20013;&#36890;&#36807;&#25511;&#21046;&#26377;&#25928;&#35268;&#21010;&#33539;&#22260;&#26469;&#25552;&#39640;&#24615;&#33021;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20462;&#27491;&#31639;&#27861;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#23384;&#22312;&#30340;&#36924;&#36817;&#35823;&#24046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.09728</link><description>&lt;p&gt;
&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#36890;&#36807;&#25511;&#21046;&#26377;&#25928;&#35268;&#21010;&#33539;&#22260;
&lt;/p&gt;
&lt;p&gt;
Offline Imitation Learning by Controlling the Effective Planning Horizon. (arXiv:2401.09728v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09728
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#20013;&#36890;&#36807;&#25511;&#21046;&#26377;&#25928;&#35268;&#21010;&#33539;&#22260;&#26469;&#25552;&#39640;&#24615;&#33021;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20462;&#27491;&#31639;&#27861;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#23384;&#22312;&#30340;&#36924;&#36817;&#35823;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#20013;&#65292;&#25105;&#20204;&#36890;&#24120;&#20551;&#35774;&#21482;&#26377;&#23569;&#37327;&#30340;&#19987;&#23478;&#36335;&#24452;&#21644;&#26469;&#33258;&#27425;&#20248;&#34892;&#20026;&#30340;&#31163;&#32447;&#25968;&#25454;&#38598;&#26469;&#23398;&#20064;&#19987;&#23478;&#31574;&#30053;&#12290;&#34429;&#28982;&#29616;&#22312;&#24120;&#24120;&#36890;&#36807;&#26368;&#23567;&#21270;&#29366;&#24577;-&#21160;&#20316;&#35775;&#38382;&#20998;&#24067;&#30340;&#24046;&#24322;&#26469;&#20351;&#20195;&#29702;&#32771;&#34385;&#21040;&#21160;&#20316;&#30340;&#26410;&#26469;&#21518;&#26524;&#65292;&#20294;&#26159;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#30340;&#25277;&#26679;&#35823;&#24046;&#21487;&#33021;&#23548;&#33268;&#23545;&#31163;&#32447;&#24773;&#20917;&#19979;&#29366;&#24577;-&#21160;&#20316;&#35775;&#38382;&#20272;&#35745;&#30340;&#38169;&#35823;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36890;&#36807;&#25511;&#21046;&#26377;&#25928;&#35268;&#21010;&#33539;&#22260;&#65288;&#21363;&#20943;&#23567;&#25240;&#25187;&#22240;&#23376;&#65289;&#26469;&#23545;&#25239;&#26174;&#24335;&#27491;&#21017;&#21270;&#20043;&#21069;&#25152;&#30740;&#31350;&#30340;&#24433;&#21709;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#29616;&#26377;&#31639;&#27861;&#22312;&#26377;&#25928;&#35268;&#21010;&#33539;&#22260;&#32553;&#30701;&#26102;&#20250;&#20986;&#29616;&#25918;&#22823;&#30340;&#36924;&#36817;&#35823;&#24046;&#65292;&#20174;&#32780;&#23548;&#33268;&#24615;&#33021;&#30340;&#26174;&#33879;&#19979;&#38477;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#38382;&#39064;&#30340;&#20027;&#35201;&#21407;&#22240;&#65292;&#24182;&#25552;&#20379;&#20102;&#27491;&#30830;&#30340;&#20462;&#27491;&#31639;&#27861;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20462;&#27491;&#21518;&#31639;&#27861;&#30340;&#25928;&#26524;&#65292;&#24182;&#39564;&#35777;&#20102;&#23427;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In offline imitation learning (IL), we generally assume only a handful of expert trajectories and a supplementary offline dataset from suboptimal behaviors to learn the expert policy. While it is now common to minimize the divergence between state-action visitation distributions so that the agent also considers the future consequences of an action, a sampling error in an offline dataset may lead to erroneous estimates of state-action visitations in the offline case. In this paper, we investigate the effect of controlling the effective planning horizon (i.e., reducing the discount factor) as opposed to imposing an explicit regularizer, as previously studied. Unfortunately, it turns out that the existing algorithms suffer from magnified approximation errors when the effective planning horizon is shortened, which results in a significant degradation in performance. We analyze the main cause of the problem and provide the right remedies to correct the algorithm. We show that the corrected 
&lt;/p&gt;</description></item><item><title>EfficientRec&#26159;&#19968;&#31181;&#24212;&#29992;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#37319;&#29992;&#20102;&#36719;&#32858;&#31867;&#26550;&#26500;&#65292;&#33021;&#20197;&#20302;&#35745;&#31639;&#25104;&#26412;&#23398;&#20064;&#29992;&#25143;&#20559;&#22909;&#65292;&#24182;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#23545;&#26080;&#38480;&#29992;&#25143;&#21644;&#20135;&#21697;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.09693</link><description>&lt;p&gt;
&#22522;&#20110;&#32858;&#31867;&#21644;&#29992;&#25143;&#20132;&#20114;&#23884;&#20837;&#26723;&#26696;&#30340;EfficientRec&#26080;&#38480;&#29992;&#25143;-&#29289;&#21697;&#35268;&#27169;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
EfficientRec an unlimited user-item scale recommendation system based on clustering and users interaction embedding profile. (arXiv:2401.09693v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09693
&lt;/p&gt;
&lt;p&gt;
EfficientRec&#26159;&#19968;&#31181;&#24212;&#29992;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#37319;&#29992;&#20102;&#36719;&#32858;&#31867;&#26550;&#26500;&#65292;&#33021;&#20197;&#20302;&#35745;&#31639;&#25104;&#26412;&#23398;&#20064;&#29992;&#25143;&#20559;&#22909;&#65292;&#24182;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#23545;&#26080;&#38480;&#29992;&#25143;&#21644;&#20135;&#21697;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#26159;&#22914;&#20170;&#31185;&#25216;&#20844;&#21496;&#39640;&#24230;&#20851;&#27880;&#30340;&#25216;&#26415;&#65292;&#30001;&#20110;&#29992;&#25143;&#21644;&#20135;&#21697;&#25968;&#37327;&#19981;&#26029;&#22686;&#38271;&#65292;&#20256;&#32479;&#30340;&#25512;&#33616;&#31639;&#27861;&#22312;&#23545;&#24037;&#19994;&#29615;&#22659;&#30340;&#36866;&#24212;&#24615;&#19978;&#23384;&#22312;&#22256;&#38590;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#24212;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#26469;&#25552;&#21462;&#29992;&#25143;&#20559;&#22909;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#36719;&#32858;&#31867;&#26550;&#26500;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#25512;&#29702;&#36807;&#31243;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#20197;&#36739;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#22312;&#35757;&#32451;&#21644;&#39044;&#27979;&#38454;&#27573;&#23398;&#20064;&#29992;&#25143;&#20559;&#22909;&#65292;&#24182;&#19988;&#20855;&#26377;&#24456;&#22909;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#31216;&#36825;&#31181;&#26550;&#26500;&#20026;EfficientRec&#65292;&#24847;&#21619;&#30528;&#27169;&#22411;&#30340;&#32039;&#20945;&#24615;&#21644;&#23545;&#26080;&#38480;&#29992;&#25143;&#21644;&#20135;&#21697;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommendation systems are highly interested in technology companies nowadays. The businesses are constantly growing users and products, causing the number of users and items to continuously increase over time, to very large numbers. Traditional recommendation algorithms with complexity dependent on the number of users and items make them difficult to adapt to the industrial environment. In this paper, we introduce a new method applying graph neural networks with a contrastive learning framework in extracting user preferences. We incorporate a soft clustering architecture that significantly reduces the computational cost of the inference process. Experiments show that the model is able to learn user preferences with low computational cost in both training and prediction phases. At the same time, the model gives a very good accuracy. We call this architecture EfficientRec with the implication of model compactness and the ability to scale to unlimited users and products.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#27169;&#20223;&#23398;&#20064;&#20013;&#35299;&#20915;&#22810;&#27169;&#24577;&#25968;&#25454;&#22788;&#29702;&#25361;&#25112;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#36755;&#20837;&#21040;&#27599;&#20010;&#31070;&#32463;&#32593;&#32476;&#23618;&#20013;&#65292;&#25918;&#22823;&#19982;&#26399;&#26395;&#36755;&#20986;&#30340;&#30456;&#20851;&#24615;&#36739;&#20302;&#30340;&#25968;&#25454;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#25104;&#21151;&#29575;&#30340;&#26174;&#33879;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2401.09691</link><description>&lt;p&gt;
&#23558;&#22270;&#20687;&#29305;&#24449;&#36755;&#20837;&#21040;&#31070;&#32463;&#32593;&#32476;&#30340;&#27599;&#19968;&#23618;&#30340;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Imitation Learning Inputting Image Feature to Each Layer of Neural Network. (arXiv:2401.09691v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09691
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#27169;&#20223;&#23398;&#20064;&#20013;&#35299;&#20915;&#22810;&#27169;&#24577;&#25968;&#25454;&#22788;&#29702;&#25361;&#25112;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#36755;&#20837;&#21040;&#27599;&#20010;&#31070;&#32463;&#32593;&#32476;&#23618;&#20013;&#65292;&#25918;&#22823;&#19982;&#26399;&#26395;&#36755;&#20986;&#30340;&#30456;&#20851;&#24615;&#36739;&#20302;&#30340;&#25968;&#25454;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#25104;&#21151;&#29575;&#30340;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#20351;&#24471;&#26426;&#22120;&#20154;&#33021;&#22815;&#36890;&#36807;&#35757;&#32451;&#25968;&#25454;&#23398;&#20064;&#24182;&#22797;&#21046;&#20154;&#31867;&#34892;&#20026;&#12290;&#26368;&#36817;&#26426;&#22120;&#23398;&#20064;&#30340;&#36827;&#23637;&#20351;&#24471;&#33021;&#22815;&#30452;&#25509;&#22788;&#29702;&#39640;&#32500;&#35266;&#27979;&#25968;&#25454;&#65288;&#22914;&#22270;&#20687;&#65289;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#26041;&#27861;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#22788;&#29702;&#22810;&#20010;&#27169;&#24577;&#30340;&#25968;&#25454;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#38754;&#20020;&#30528;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#21363;&#22312;&#20351;&#29992;&#30701;&#37319;&#26679;&#21608;&#26399;&#26102;&#26080;&#24847;&#20013;&#24573;&#30053;&#19982;&#26399;&#26395;&#36755;&#20986;&#30340;&#30456;&#20851;&#24615;&#36739;&#20302;&#30340;&#25968;&#25454;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#36755;&#20837;&#21040;&#27599;&#20010;&#31070;&#32463;&#32593;&#32476;&#23618;&#20013;&#65292;&#25918;&#22823;&#19982;&#36755;&#20986;&#30456;&#20851;&#24615;&#36739;&#20302;&#30340;&#25968;&#25454;&#30340;&#24433;&#21709;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#23558;&#22810;&#26679;&#30340;&#25968;&#25454;&#28304;&#32435;&#20837;&#21040;&#23398;&#20064;&#36807;&#31243;&#20013;&#12290;&#36890;&#36807;&#20351;&#29992;&#21407;&#22987;&#22270;&#20687;&#21644;&#20851;&#33410;&#20449;&#24687;&#20316;&#20026;&#36755;&#20837;&#36827;&#34892;&#31616;&#21333;&#30340;&#25342;&#21462;&#25918;&#32622;&#25805;&#20316;&#30340;&#23454;&#39564;&#65292;&#21363;&#20351;&#22788;&#29702;&#26469;&#33258;&#30701;&#37319;&#26679;&#21608;&#26399;&#30340;&#25968;&#25454;&#65292;&#20063;&#35777;&#26126;&#20102;&#25104;&#21151;&#29575;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation learning enables robots to learn and replicate human behavior from training data. Recent advances in machine learning enable end-to-end learning approaches that directly process high-dimensional observation data, such as images. However, these approaches face a critical challenge when processing data from multiple modalities, inadvertently ignoring data with a lower correlation to the desired output, especially when using short sampling periods. This paper presents a useful method to address this challenge, which amplifies the influence of data with a relatively low correlation to the output by inputting the data into each neural network layer. The proposed approach effectively incorporates diverse data sources into the learning process. Through experiments using a simple pick-and-place operation with raw images and joint information as input, significant improvements in success rates are demonstrated even when dealing with data from short sampling periods.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#19981;&#21516;&#20998;&#31867;&#21464;&#37327;&#32534;&#30721;&#22120;&#22312;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#39564;&#35777;&#20102;&#23545;&#20110;ATI&#27169;&#22411;&#26469;&#35828;&#65292;&#29420;&#28909;&#32534;&#30721;&#22120;&#26159;&#26368;&#20339;&#36873;&#25321;&#65292;&#32780;&#30446;&#26631;&#32534;&#30721;&#22120;&#21450;&#20854;&#21464;&#20307;&#26159;&#26641;&#27169;&#22411;&#20013;&#26368;&#21512;&#36866;&#30340;&#32534;&#30721;&#22120;&#12290;</title><link>http://arxiv.org/abs/2401.09682</link><description>&lt;p&gt;
&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#20013;&#20998;&#31867;&#21464;&#37327;&#32534;&#30721;&#22120;&#24615;&#33021;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Comparative Study on the Performance of Categorical Variable Encoders in Classification and Regression Tasks. (arXiv:2401.09682v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09682
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#19981;&#21516;&#20998;&#31867;&#21464;&#37327;&#32534;&#30721;&#22120;&#22312;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#39564;&#35777;&#20102;&#23545;&#20110;ATI&#27169;&#22411;&#26469;&#35828;&#65292;&#29420;&#28909;&#32534;&#30721;&#22120;&#26159;&#26368;&#20339;&#36873;&#25321;&#65292;&#32780;&#30446;&#26631;&#32534;&#30721;&#22120;&#21450;&#20854;&#21464;&#20307;&#26159;&#26641;&#27169;&#22411;&#20013;&#26368;&#21512;&#36866;&#30340;&#32534;&#30721;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#31867;&#21464;&#37327;&#32463;&#24120;&#20986;&#29616;&#22312;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;&#22312;&#35757;&#32451;&#20043;&#21069;&#38656;&#35201;&#23558;&#20854;&#32534;&#30721;&#20026;&#25968;&#20540;&#12290;&#30001;&#20110;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#35768;&#22810;&#32534;&#30721;&#22120;&#24182;&#19988;&#21487;&#20197;&#26174;&#33879;&#24433;&#21709;&#24615;&#33021;&#65292;&#36873;&#25321;&#36866;&#24403;&#30340;&#32534;&#30721;&#22120;&#25104;&#20026;&#19968;&#39033;&#32791;&#26102;&#20294;&#37325;&#35201;&#30340;&#23454;&#36341;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22823;&#33268;&#20998;&#20026;&#19977;&#31867;&#65306;1&#65289;&#38544;&#24335;&#23545;&#36755;&#20837;&#36827;&#34892;&#20223;&#23556;&#21464;&#25442;&#30340;ATI&#27169;&#22411;&#65292;&#22914;&#22810;&#23618;&#24863;&#30693;&#26426;&#31070;&#32463;&#32593;&#32476;&#65307;2&#65289;&#22522;&#20110;&#20915;&#31574;&#26641;&#30340;&#26641;&#27169;&#22411;&#65292;&#22914;&#38543;&#26426;&#26862;&#26519;&#65307;3&#65289;&#20854;&#20182;&#27169;&#22411;&#65292;&#22914;kNN&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;ATI&#27169;&#22411;&#26469;&#35828;&#65292;&#29420;&#28909;&#32534;&#30721;&#22120;&#26159;&#26368;&#20339;&#36873;&#25321;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#36890;&#36807;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#21512;&#36866;&#30340;&#26435;&#37325;&#27169;&#25311;&#20219;&#20309;&#20854;&#20182;&#32534;&#30721;&#22120;&#12290;&#25105;&#20204;&#36824;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#30446;&#26631;&#32534;&#30721;&#22120;&#21450;&#20854;&#21464;&#20307;&#26159;&#26641;&#27169;&#22411;&#20013;&#26368;&#21512;&#36866;&#30340;&#32534;&#30721;&#22120;&#12290;&#26412;&#30740;&#31350;&#36827;&#34892;&#20102;&#32508;&#21512;&#30340;&#35745;&#31639;&#23454;&#39564;&#26469;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Categorical variables often appear in datasets for classification and regression tasks, and they need to be encoded into numerical values before training. Since many encoders have been developed and can significantly impact performance, choosing the appropriate encoder for a task becomes a time-consuming yet important practical issue. This study broadly classifies machine learning models into three categories: 1) ATI models that implicitly perform affine transformations on inputs, such as multi-layer perceptron neural network; 2) Tree-based models that are based on decision trees, such as random forest; and 3) the rest, such as kNN. Theoretically, we prove that the one-hot encoder is the best choice for ATI models in the sense that it can mimic any other encoders by learning suitable weights from the data. We also explain why the target encoder and its variants are the most suitable encoders for tree-based models. This study conducted comprehensive computational experiments to evaluate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65288;GLOW&#65289;&#65292;&#32467;&#21512;&#23494;&#24230;&#27604;&#29575;&#27169;&#22411;&#21644;&#20540;&#20989;&#25968;&#27169;&#22411;&#65292;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#35299;&#20915;&#20102;&#29942;&#39048;&#38382;&#39064;&#65292;&#21363;&#22914;&#20309;&#22312;&#27809;&#26377;&#21021;&#22987;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#25910;&#38598;&#20855;&#26377;&#33391;&#22909;&#35206;&#30422;&#24230;&#30340;&#25506;&#32034;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2401.09681</link><description>&lt;p&gt;
&#21033;&#29992;&#23494;&#24230;&#27604;&#29575;&#36827;&#34892;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Harnessing Density Ratios for Online Reinforcement Learning. (arXiv:2401.09681v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09681
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65288;GLOW&#65289;&#65292;&#32467;&#21512;&#23494;&#24230;&#27604;&#29575;&#27169;&#22411;&#21644;&#20540;&#20989;&#25968;&#27169;&#22411;&#65292;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#35299;&#20915;&#20102;&#29942;&#39048;&#38382;&#39064;&#65292;&#21363;&#22914;&#20309;&#22312;&#27809;&#26377;&#21021;&#22987;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#25910;&#38598;&#20855;&#26377;&#33391;&#22909;&#35206;&#30422;&#24230;&#30340;&#25506;&#32034;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31163;&#32447;&#21644;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#29702;&#35770;&#21457;&#23637;&#26041;&#21521;&#19968;&#30452;&#26159;&#24179;&#34892;&#30340;&#65292;&#20294;&#23427;&#20204;&#24320;&#22987;&#26174;&#31034;&#20986;&#21487;&#33021;&#32479;&#19968;&#30340;&#36857;&#35937;&#65292;&#20854;&#20013;&#19968;&#20010;&#29615;&#22659;&#30340;&#31639;&#27861;&#21644;&#20998;&#26512;&#25216;&#26415;&#36890;&#24120;&#22312;&#21478;&#19968;&#20010;&#29615;&#22659;&#20013;&#20855;&#26377;&#33258;&#28982;&#30340;&#23545;&#24212;&#29289;&#12290;&#28982;&#32780;&#65292;&#23494;&#24230;&#27604;&#29575;&#24314;&#27169;&#30340;&#27010;&#24565;&#65292;&#36825;&#26159;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26032;&#20852;&#33539;&#24335;&#65292;&#22312;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#24456;&#23569;&#20986;&#29616;&#65292;&#20063;&#35768;&#26377;&#20805;&#36275;&#30340;&#29702;&#30001;&#65306;&#23494;&#24230;&#27604;&#29575;&#30340;&#23384;&#22312;&#21644;&#26377;&#30028;&#24615;&#20381;&#36182;&#20110;&#20855;&#26377;&#33391;&#22909;&#35206;&#30422;&#24230;&#30340;&#25506;&#32034;&#24615;&#25968;&#25454;&#38598;&#30340;&#35775;&#38382;&#24615;&#65292;&#20294;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26680;&#24515;&#25361;&#25112;&#26159;&#22312;&#27809;&#26377;&#21021;&#22987;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#25910;&#38598;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#34920;&#26126; - &#20063;&#35768;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159; - &#22522;&#20110;&#23494;&#24230;&#27604;&#29575;&#30340;&#31639;&#27861;&#20855;&#26377;&#22312;&#32447;&#23545;&#24212;&#29289;&#12290;&#20551;&#23450;&#21482;&#23384;&#22312;&#20855;&#26377;&#33391;&#22909;&#35206;&#30422;&#24230;&#30340;&#25506;&#32034;&#24615;&#20998;&#24067;&#65292;&#21363;&#32467;&#26500;&#26465;&#20214;&#24050;&#30693;&#20026;coverability&#65288;Xie&#31561;&#65292;2023&#65289;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65288;GLOW&#65289;&#65292;&#23427;&#21033;&#29992;&#23494;&#24230;&#27604;&#29575;&#21487;&#23454;&#29616;&#24615;&#21644;&#20540;&#20989;&#25968;&#21487;&#23454;&#29616;&#24615;&#26469;&#36827;&#34892;&#39640;&#25928;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
The theories of offline and online reinforcement learning, despite having evolved in parallel, have begun to show signs of the possibility for a unification, with algorithms and analysis techniques for one setting often having natural counterparts in the other. However, the notion of density ratio modeling, an emerging paradigm in offline RL, has been largely absent from online RL, perhaps for good reason: the very existence and boundedness of density ratios relies on access to an exploratory dataset with good coverage, but the core challenge in online RL is to collect such a dataset without having one to start. In this work we show -- perhaps surprisingly -- that density ratio-based algorithms have online counterparts. Assuming only the existence of an exploratory distribution with good coverage, a structural condition known as coverability (Xie et al., 2023), we give a new algorithm (GLOW) that uses density ratio realizability and value function realizability to perform sample-effici
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26412;&#22320;&#33258;&#36866;&#24212;&#23545;&#25239;&#39068;&#33394;&#25915;&#20987;&#65288;LAACA&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20445;&#25252;&#33402;&#26415;&#21697;&#20813;&#21463;&#31070;&#32463;&#39118;&#26684;&#36716;&#25442;&#65288;NST&#65289;&#30340;&#28389;&#29992;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#19981;&#21487;&#23519;&#35273;&#30340;&#24773;&#20917;&#19979;&#23545;&#22270;&#20687;&#36827;&#34892;&#20462;&#25913;&#65292;&#20135;&#29983;&#23545;NST&#20855;&#26377;&#24178;&#25200;&#20316;&#29992;&#30340;&#25200;&#21160;&#12290;</title><link>http://arxiv.org/abs/2401.09673</link><description>&lt;p&gt;
&#20351;&#29992;&#26412;&#22320;&#33258;&#36866;&#24212;&#23545;&#25239;&#39068;&#33394;&#25915;&#20987;&#23545;&#33402;&#26415;&#21697;&#36827;&#34892;&#31070;&#32463;&#39118;&#26684;&#36716;&#25442;&#30340;&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
Artwork Protection Against Neural Style Transfer Using Locally Adaptive Adversarial Color Attack. (arXiv:2401.09673v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09673
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26412;&#22320;&#33258;&#36866;&#24212;&#23545;&#25239;&#39068;&#33394;&#25915;&#20987;&#65288;LAACA&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20445;&#25252;&#33402;&#26415;&#21697;&#20813;&#21463;&#31070;&#32463;&#39118;&#26684;&#36716;&#25442;&#65288;NST&#65289;&#30340;&#28389;&#29992;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#19981;&#21487;&#23519;&#35273;&#30340;&#24773;&#20917;&#19979;&#23545;&#22270;&#20687;&#36827;&#34892;&#20462;&#25913;&#65292;&#20135;&#29983;&#23545;NST&#20855;&#26377;&#24178;&#25200;&#20316;&#29992;&#30340;&#25200;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#39118;&#26684;&#36716;&#25442;&#65288;NST&#65289;&#24191;&#27867;&#24212;&#29992;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#65292;&#29992;&#20110;&#29983;&#25104;&#20855;&#26377;&#20219;&#24847;&#39118;&#26684;&#30340;&#26032;&#22270;&#20687;&#12290;&#36825;&#20010;&#36807;&#31243;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#23558;&#39118;&#26684;&#22270;&#20687;&#30340;&#32654;&#23398;&#20803;&#32032;&#19982;&#20869;&#23481;&#22270;&#20687;&#30340;&#32467;&#26500;&#22240;&#32032;&#34701;&#21512;&#22312;&#19968;&#36215;&#65292;&#24418;&#25104;&#19968;&#20010;&#21644;&#35856;&#25972;&#21512;&#30340;&#35270;&#35273;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#26410;&#32463;&#25480;&#26435;&#30340;NST&#21487;&#33021;&#20250;&#28389;&#29992;&#33402;&#26415;&#21697;&#12290;&#36825;&#31181;&#28389;&#29992;&#24341;&#36215;&#20102;&#20851;&#20110;&#33402;&#26415;&#23478;&#26435;&#21033;&#30340;&#31038;&#20250;&#25216;&#26415;&#38382;&#39064;&#65292;&#24182;&#20419;&#20351;&#24320;&#21457;&#25216;&#26415;&#26041;&#27861;&#26469;&#31215;&#26497;&#20445;&#25252;&#21407;&#22987;&#21019;&#20316;&#12290;&#23545;&#25239;&#24615;&#25915;&#20987;&#20027;&#35201;&#22312;&#26426;&#22120;&#23398;&#20064;&#23433;&#20840;&#20013;&#36827;&#34892;&#25506;&#32034;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#23558;&#36825;&#19968;&#25216;&#26415;&#24341;&#20837;&#21040;&#20445;&#25252;&#33402;&#26415;&#23478;&#30693;&#35782;&#20135;&#26435;&#30340;&#39046;&#22495;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#26412;&#22320;&#33258;&#36866;&#24212;&#23545;&#25239;&#39068;&#33394;&#25915;&#20987;&#65288;LAACA&#65289;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20197;&#23545;&#20154;&#30524;&#19981;&#21487;&#23519;&#35273;&#20294;&#23545;NST&#20135;&#29983;&#24178;&#25200;&#30340;&#26041;&#24335;&#20462;&#25913;&#22270;&#20687;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#38024;&#23545;&#39640;&#39057;&#20869;&#23481;&#20016;&#23500;&#21306;&#22495;&#30340;&#25200;&#21160;&#65292;&#36825;&#20123;&#25200;&#21160;&#30001;&#20013;&#38388;&#29305;&#24449;&#30340;&#30772;&#22351;&#20135;&#29983;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#21644;&#29992;&#25143;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural style transfer (NST) is widely adopted in computer vision to generate new images with arbitrary styles. This process leverages neural networks to merge aesthetic elements of a style image with the structural aspects of a content image into a harmoniously integrated visual result. However, unauthorized NST can exploit artwork. Such misuse raises socio-technical concerns regarding artists' rights and motivates the development of technical approaches for the proactive protection of original creations. Adversarial attack is a concept primarily explored in machine learning security. Our work introduces this technique to protect artists' intellectual property. In this paper Locally Adaptive Adversarial Color Attack (LAACA), a method for altering images in a manner imperceptible to the human eyes but disruptive to NST. Specifically, we design perturbations targeting image areas rich in high-frequency content, generated by disrupting intermediate features. Our experiments and user study
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#26080;&#30417;&#30563;&#39046;&#22495;&#36716;&#25442;&#20013;&#30340;&#21487;&#35782;&#21035;&#24615;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;MPA&#28040;&#38500;&#29702;&#35770;&#65292;&#35299;&#20915;&#20102;CycleGAN&#21450;&#20854;&#21464;&#20307;&#20135;&#29983;&#20869;&#23481;&#19981;&#23545;&#40784;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2401.09671</link><description>&lt;p&gt;
&#36808;&#21521;&#21487;&#35782;&#21035;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#36716;&#25442;&#65306;&#19968;&#31181;&#22810;&#26679;&#21270;&#20998;&#24067;&#21305;&#37197;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Identifiable Unsupervised Domain Translation: A Diversified Distribution Matching Approach. (arXiv:2401.09671v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#26080;&#30417;&#30563;&#39046;&#22495;&#36716;&#25442;&#20013;&#30340;&#21487;&#35782;&#21035;&#24615;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;MPA&#28040;&#38500;&#29702;&#35770;&#65292;&#35299;&#20915;&#20102;CycleGAN&#21450;&#20854;&#21464;&#20307;&#20135;&#29983;&#20869;&#23481;&#19981;&#23545;&#40784;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#39046;&#22495;&#36716;&#25442;&#65288;UDT&#65289;&#26088;&#22312;&#25214;&#21040;&#23558;&#19968;&#20010;&#39046;&#22495;&#30340;&#26679;&#26412;&#65288;&#20363;&#22914;&#32032;&#25551;&#65289;&#36716;&#25442;&#20026;&#21478;&#19968;&#20010;&#39046;&#22495;&#65288;&#20363;&#22914;&#29031;&#29255;&#65289;&#30340;&#20989;&#25968;&#65292;&#21516;&#26102;&#19981;&#25913;&#21464;&#39640;&#23618;&#35821;&#20041;&#24847;&#20041;&#65288;&#20063;&#31216;&#20026;&#8220;&#20869;&#23481;&#8221;&#65289;&#12290;&#36825;&#20123;&#36716;&#25442;&#20989;&#25968;&#36890;&#24120;&#36890;&#36807;&#36716;&#25442;&#28304;&#39046;&#22495;&#21644;&#30446;&#26631;&#39046;&#22495;&#30340;&#27010;&#29575;&#20998;&#24067;&#26469;&#23547;&#25214;&#12290;CycleGAN&#21487;&#20197;&#35828;&#26159;&#36825;&#19968;&#39046;&#22495;&#20013;&#26368;&#20855;&#20195;&#34920;&#24615;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#25991;&#29486;&#20013;&#25351;&#20986;CycleGAN&#21450;&#20854;&#21464;&#20307;&#21487;&#33021;&#26080;&#27861;&#35782;&#21035;&#25152;&#38656;&#30340;&#36716;&#25442;&#20989;&#25968;&#65292;&#24182;&#20135;&#29983;&#20869;&#23481;&#19981;&#23545;&#40784;&#30340;&#36716;&#25442;&#12290;&#36825;&#31181;&#23616;&#38480;&#24615;&#28304;&#20110;&#23398;&#20064;&#20934;&#21017;&#35299;&#31354;&#38388;&#20013;&#23384;&#22312;&#22810;&#20010;&#36716;&#25442;&#20989;&#25968;&#65292;&#31216;&#20026;&#8220;&#20445;&#24230;&#33258;&#21516;&#26500;&#65288;MPA&#65289;&#8221;&#12290;&#23613;&#31649;&#24847;&#35782;&#21040;&#20102;&#36825;&#31181;&#21487;&#35782;&#21035;&#24615;&#38382;&#39064;&#65292;&#20294;&#35299;&#20915;&#26041;&#26696;&#20173;&#28982;&#38590;&#20197;&#25214;&#21040;&#12290;&#26412;&#30740;&#31350;&#28145;&#20837;&#25506;&#31350;&#20102;&#26680;&#24515;&#30340;&#21487;&#35782;&#21035;&#24615;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;MPA&#28040;&#38500;&#29702;&#35770;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
Unsupervised domain translation (UDT) aims to find functions that convert samples from one domain (e.g., sketches) to another domain (e.g., photos) without changing the high-level semantic meaning (also referred to as ``content''). The translation functions are often sought by probability distribution matching of the transformed source domain and target domain. CycleGAN stands as arguably the most representative approach among this line of work. However, it was noticed in the literature that CycleGAN and variants could fail to identify the desired translation functions and produce content-misaligned translations. This limitation arises due to the presence of multiple translation functions -- referred to as ``measure-preserving automorphism" (MPA) -- in the solution space of the learning criteria. Despite awareness of such identifiability issues, solutions have remained elusive. This study delves into the core identifiability inquiry and introduces an MPA elimination theory. Our analysi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#25490;&#26021;&#38543;&#26426;&#28216;&#36208;&#21152;&#36895;&#20998;&#24067;&#24335;&#38543;&#26426;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#29992;&#38750;&#32447;&#24615;&#39532;&#23572;&#21487;&#22827;&#38142;&#21462;&#20195;&#20256;&#32479;&#30340;&#32447;&#24615;&#39532;&#23572;&#21487;&#22827;&#38142;&#65292;&#24182;&#22312;&#28176;&#36817;&#26041;&#24046;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.09665</link><description>&lt;p&gt;
&#21152;&#36895;&#20998;&#24067;&#24335;&#38543;&#26426;&#20248;&#21270;&#30340;&#33258;&#25490;&#26021;&#38543;&#26426;&#28216;&#36208;
&lt;/p&gt;
&lt;p&gt;
Accelerating Distributed Stochastic Optimization via Self-Repellent Random Walks. (arXiv:2401.09665v1 [math.PR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09665
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#25490;&#26021;&#38543;&#26426;&#28216;&#36208;&#21152;&#36895;&#20998;&#24067;&#24335;&#38543;&#26426;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#29992;&#38750;&#32447;&#24615;&#39532;&#23572;&#21487;&#22827;&#38142;&#21462;&#20195;&#20256;&#32479;&#30340;&#32447;&#24615;&#39532;&#23572;&#21487;&#22827;&#38142;&#65292;&#24182;&#22312;&#28176;&#36817;&#26041;&#24046;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31867;&#20998;&#24067;&#24335;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#65292;&#20854;&#20013;&#26799;&#24230;&#26159;&#30001;&#19968;&#20010;&#22312;&#20195;&#29702;&#32593;&#32476;&#20013;&#20197;&#38543;&#26426;&#28216;&#36208;&#26041;&#24335;&#31227;&#21160;&#30340;&#20196;&#29260;&#37319;&#26679;&#24471;&#21040;&#30340;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#38543;&#26426;&#28216;&#36208;&#34987;&#36873;&#25321;&#20026;&#28176;&#36817;&#37319;&#26679;&#19968;&#20010;&#30446;&#26631;&#20998;&#24067;&#30340;&#39532;&#23572;&#21487;&#22827;&#38142;&#65292;&#22312;&#20248;&#21270;&#36845;&#20195;&#30340;&#25910;&#25947;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26412;&#25991;&#37319;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#26631;&#20934;&#32447;&#24615;&#39532;&#23572;&#21487;&#22827;&#20196;&#29260;&#26367;&#25442;&#20026;&#36981;&#24490;&#38750;&#32447;&#24615;&#39532;&#23572;&#21487;&#22827;&#38142;&#30340;&#20196;&#29260; - &#21363;&#33258;&#25490;&#26021;&#38543;&#26426;&#28216;&#36208;(SRRW)&#12290;&#23545;&#20110;&#20219;&#32473;&#30340;'base'&#39532;&#23572;&#21487;&#22827;&#38142;&#65292;&#30001;&#27491;&#26631;&#37327;{\alpha}&#21442;&#25968;&#21270;&#30340;SRRW&#65292;&#22312;&#36807;&#21435;&#39640;&#35775;&#38382;&#30340;&#29366;&#24577;&#36716;&#31227;&#27010;&#29575;&#36739;&#20302;&#65292;&#22240;&#27492;&#24471;&#21517;&#12290;&#22312;&#22270;&#19978;&#30340;MCMC&#37319;&#26679;&#30340;&#32972;&#26223;&#19979;&#65292;Doshi&#31561;&#20154;(2023)&#30340;&#26368;&#26032;&#31361;&#30772;&#34920;&#26126;&#65292;SRRW&#22312;&#37319;&#26679;&#26041;&#38754;&#22312;&#28176;&#36817;&#26041;&#24046;&#19978;&#36798;&#21040;&#20102;O(1/{\alpha})&#30340;&#20943;&#23567;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;'generalized'&#29256;&#26412;&#30340;SRRW&#26469;&#39537;&#21160;&#20196;&#29260;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a family of distributed stochastic optimization algorithms where gradients are sampled by a token traversing a network of agents in random-walk fashion. Typically, these random-walks are chosen to be Markov chains that asymptotically sample from a desired target distribution, and play a critical role in the convergence of the optimization iterates. In this paper, we take a novel approach by replacing the standard linear Markovian token by one which follows a nonlinear Markov chain - namely the Self-Repellent Radom Walk (SRRW). Defined for any given 'base' Markov chain, the SRRW, parameterized by a positive scalar {\alpha}, is less likely to transition to states that were highly visited in the past, thus the name. In the context of MCMC sampling on a graph, a recent breakthrough in Doshi et al. (2023) shows that the SRRW achieves O(1/{\alpha}) decrease in the asymptotic variance for sampling. We propose the use of a 'generalized' version of the SRRW to drive token algorithms fo
&lt;/p&gt;</description></item><item><title>&#22312;&#36710;&#32852;&#32593;&#20013;&#65292;&#36890;&#36807;&#25910;&#25947;&#24615;&#20998;&#26512;&#65292;&#26412;&#25991;&#35777;&#26126;&#31227;&#21160;&#24615;&#23545;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#30340;&#25910;&#25947;&#36895;&#24230;&#26377;&#31215;&#26497;&#24433;&#21709;&#65292;&#23427;&#22686;&#21152;&#20102;&#36793;&#32536;&#23618;&#24322;&#26500;&#25968;&#25454;&#30340;&#34701;&#21512;&#21644;&#26356;&#24555;&#30340;&#25968;&#25454;&#34701;&#21512;&#36895;&#24230;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.09656</link><description>&lt;p&gt;
&#31227;&#21160;&#24615;&#21152;&#36895;&#23398;&#20064;&#65306;&#36710;&#32852;&#32593;&#20013;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#30340;&#25910;&#25947;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Mobility Accelerates Learning: Convergence Analysis on Hierarchical Federated Learning in Vehicular Networks. (arXiv:2401.09656v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09656
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36710;&#32852;&#32593;&#20013;&#65292;&#36890;&#36807;&#25910;&#25947;&#24615;&#20998;&#26512;&#65292;&#26412;&#25991;&#35777;&#26126;&#31227;&#21160;&#24615;&#23545;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#30340;&#25910;&#25947;&#36895;&#24230;&#26377;&#31215;&#26497;&#24433;&#21709;&#65292;&#23427;&#22686;&#21152;&#20102;&#36793;&#32536;&#23618;&#24322;&#26500;&#25968;&#25454;&#30340;&#34701;&#21512;&#21644;&#26356;&#24555;&#30340;&#25968;&#25454;&#34701;&#21512;&#36895;&#24230;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;(HFL)&#20197;&#38544;&#31169;&#20445;&#25252;&#30340;&#26041;&#24335;&#65292;&#22312;&#22810;&#20010;&#35774;&#22791;&#19978;&#36890;&#36807;&#20960;&#20010;&#36793;&#32536;&#26381;&#21153;&#22120;&#21644;&#19968;&#20010;&#20113;&#36793;&#32536;&#26381;&#21153;&#22120;&#36827;&#34892;&#27169;&#22411;&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;&#12290;&#26412;&#25991;&#32771;&#34385;&#39640;&#24230;&#31227;&#21160;&#30340;&#35774;&#22791;&#65292;&#20027;&#35201;&#38024;&#23545;&#36710;&#32852;&#32593;&#12290;&#36890;&#36807;&#25910;&#25947;&#24615;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#31227;&#21160;&#24615;&#36890;&#36807;&#34701;&#21512;&#36793;&#32536;&#25968;&#25454;&#21644;&#27927;&#29260;&#36793;&#32536;&#27169;&#22411;&#26469;&#24433;&#21709;&#25910;&#25947;&#36895;&#24230;&#12290;&#23613;&#31649;&#20174;&#36890;&#20449;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#31227;&#21160;&#24615;&#36890;&#24120;&#34987;&#35270;&#20026;&#19968;&#20010;&#25361;&#25112;&#65292;&#20294;&#25105;&#20204;&#35777;&#26126;&#23427;&#22686;&#21152;&#20102;&#36793;&#32536;&#23618;&#24322;&#26500;&#25968;&#25454;&#19979;HFL&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#22240;&#20026;&#26356;&#22810;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#21487;&#20197;&#34987;&#21512;&#24182;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#39640;&#36895;&#24230;&#23548;&#33268;&#26356;&#24555;&#30340;&#25910;&#25947;&#65292;&#22240;&#20026;&#23427;&#21152;&#36895;&#20102;&#25968;&#25454;&#30340;&#34701;&#21512;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26102;&#65292;&#31227;&#21160;&#24615;&#21487;&#20197;&#20351;HFL&#30340;&#27169;&#22411;&#20934;&#30830;&#24615;&#25552;&#39640;&#39640;&#36798;15.1%&#12290;
&lt;/p&gt;
&lt;p&gt;
Hierarchical federated learning (HFL) enables distributed training of models across multiple devices with the help of several edge servers and a cloud edge server in a privacy-preserving manner. In this paper, we consider HFL with highly mobile devices, mainly targeting at vehicular networks. Through convergence analysis, we show that mobility influences the convergence speed by both fusing the edge data and shuffling the edge models. While mobility is usually considered as a challenge from the perspective of communication, we prove that it increases the convergence speed of HFL with edge-level heterogeneous data, since more diverse data can be incorporated. Furthermore, we demonstrate that a higher speed leads to faster convergence, since it accelerates the fusion of data. Simulation results show that mobility increases the model accuracy of HFL by up to 15.1% when training a convolutional neural network on the CIFAR-10 dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20984;&#20108;&#32423;&#20248;&#21270;&#25216;&#26415;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#31070;&#32463;&#21644;&#31526;&#21495;&#21442;&#25968;&#23398;&#20064;&#26694;&#26550;&#65292;&#20855;&#26377;100&#20493;&#20197;&#19978;&#30340;&#23398;&#20064;&#26102;&#38388;&#25913;&#36827;&#21644;&#39640;&#36798;16%&#30340;&#39044;&#27979;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2401.09651</link><description>&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#25512;&#29702;&#21644;&#23398;&#20064;&#30340;&#20984;&#20108;&#32423;&#20248;&#21270;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Convex and Bilevel Optimization for Neuro-Symbolic Inference and Learning. (arXiv:2401.09651v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09651
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20984;&#20108;&#32423;&#20248;&#21270;&#25216;&#26415;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#31070;&#32463;&#21644;&#31526;&#21495;&#21442;&#25968;&#23398;&#20064;&#26694;&#26550;&#65292;&#20855;&#26377;100&#20493;&#20197;&#19978;&#30340;&#23398;&#20064;&#26102;&#38388;&#25913;&#36827;&#21644;&#39640;&#36798;16%&#30340;&#39044;&#27979;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#20984;&#20108;&#32423;&#20248;&#21270;&#25216;&#26415;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#31070;&#32463;&#31526;&#21495;&#31995;&#32479;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#31471;&#21040;&#31471;&#31070;&#32463;&#21644;&#31526;&#21495;&#21442;&#25968;&#23398;&#20064;&#26694;&#26550;&#12290;&#25105;&#20204;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#31526;&#21495;&#20307;&#31995;&#32467;&#26500;NeuPSL&#26469;&#35777;&#26126;&#25105;&#20204;&#30340;&#26694;&#26550;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;NeuPSL&#25512;&#29702;&#30340;&#24179;&#28369;&#21407;&#22987;&#21644;&#23545;&#20598;&#24418;&#24335;&#65292;&#24182;&#26174;&#31034;&#23398;&#20064;&#26799;&#24230;&#26159;&#26368;&#20248;&#23545;&#20598;&#21464;&#37327;&#30340;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20026;&#26032;&#30340;&#24418;&#24335;&#24320;&#21457;&#20102;&#19968;&#31181;&#23545;&#20598;&#22359;&#22352;&#26631;&#19979;&#38477;&#31639;&#27861;&#65292;&#33258;&#28982;&#22320;&#21033;&#29992;&#20102;&#28909;&#21551;&#21160;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#30456;&#27604;&#24403;&#21069;&#26368;&#22909;&#30340;NeuPSL&#25512;&#29702;&#26041;&#27861;&#30340;&#23398;&#20064;&#26102;&#38388;&#25913;&#36827;&#20102;100&#20493;&#20197;&#19978;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#28085;&#30422;&#21508;&#31181;&#20219;&#21153;&#30340;8&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#24182;&#35777;&#26126;&#25105;&#20204;&#30340;&#23398;&#20064;&#26694;&#26550;&#30456;&#27604;&#26367;&#20195;&#23398;&#20064;&#26041;&#27861;&#33021;&#22815;&#25552;&#21319;&#39640;&#36798;16%&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address a key challenge for neuro-symbolic (NeSy) systems by leveraging convex and bilevel optimization techniques to develop a general gradient-based framework for end-to-end neural and symbolic parameter learning. The applicability of our framework is demonstrated with NeuPSL, a state-of-the-art NeSy architecture. To achieve this, we propose a smooth primal and dual formulation of NeuPSL inference and show learning gradients are functions of the optimal dual variables. Additionally, we develop a dual block coordinate descent algorithm for the new formulation that naturally exploits warm-starts. This leads to over 100x learning runtime improvements over the current best NeuPSL inference method. Finally, we provide extensive empirical evaluations across $8$ datasets covering a range of tasks and demonstrate our learning framework achieves up to a 16% point prediction performance improvement over alternative learning methods.
&lt;/p&gt;</description></item><item><title>ClimateGPT&#26159;&#19968;&#20010;&#38024;&#23545;&#27668;&#20505;&#21464;&#21270;&#39046;&#22495;&#30340;&#36328;&#23398;&#31185;&#30740;&#31350;&#21512;&#25104;&#30340;AI&#27169;&#22411;&#65292;&#36890;&#36807;&#20248;&#21270;&#26816;&#32034;&#22686;&#24378;&#21644;&#20351;&#29992;&#32423;&#32852;&#26426;&#22120;&#32763;&#35793;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#21487;&#35775;&#38382;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.09646</link><description>&lt;p&gt;
ClimateGPT: &#23454;&#29616;&#23545;&#27668;&#20505;&#21464;&#21270;&#39046;&#22495;&#30340;&#36328;&#23398;&#31185;&#30740;&#31350;&#36827;&#34892;&#21512;&#25104;&#30340;AI&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ClimateGPT: Towards AI Synthesizing Interdisciplinary Research on Climate Change. (arXiv:2401.09646v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09646
&lt;/p&gt;
&lt;p&gt;
ClimateGPT&#26159;&#19968;&#20010;&#38024;&#23545;&#27668;&#20505;&#21464;&#21270;&#39046;&#22495;&#30340;&#36328;&#23398;&#31185;&#30740;&#31350;&#21512;&#25104;&#30340;AI&#27169;&#22411;&#65292;&#36890;&#36807;&#20248;&#21270;&#26816;&#32034;&#22686;&#24378;&#21644;&#20351;&#29992;&#32423;&#32852;&#26426;&#22120;&#32763;&#35793;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#21487;&#35775;&#38382;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;ClimateGPT&#65292;&#19968;&#31181;&#29305;&#23450;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#65292;&#29992;&#20110;&#21512;&#25104;&#27668;&#20505;&#21464;&#21270;&#30340;&#36328;&#23398;&#31185;&#30740;&#31350;&#12290;&#25105;&#20204;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#20102;&#20004;&#20010;7B&#27169;&#22411;&#65292;&#35757;&#32451;&#25968;&#25454;&#38598;&#21253;&#21547;300B&#20010;&#31185;&#23398;&#23548;&#21521;&#30340;&#20196;&#29260;&#12290;&#31532;&#19968;&#20010;&#27169;&#22411;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#21253;&#21547;&#20102;4.2B&#20010;&#29305;&#23450;&#39046;&#22495;&#30340;&#20196;&#29260;&#65292;&#31532;&#20108;&#20010;&#27169;&#22411;&#22312;&#39044;&#35757;&#32451;&#21518;&#38024;&#23545;&#27668;&#20505;&#39046;&#22495;&#36827;&#34892;&#20102;&#35843;&#25972;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23545;ClimateGPT-7B&#65292;13B&#21644;70B&#36827;&#34892;&#20102;&#36830;&#32493;&#39044;&#35757;&#32451;&#65292;&#35757;&#32451;&#25968;&#25454;&#38598;&#21253;&#21547;4.2B&#20010;&#29305;&#23450;&#39046;&#22495;&#30340;&#20196;&#29260;&#65292;&#24182;&#19982;&#27668;&#20505;&#31185;&#23398;&#23478;&#32039;&#23494;&#21512;&#20316;&#21019;&#24314;&#12290;&#20026;&#20102;&#20943;&#23569;&#34394;&#26500;&#29983;&#25104;&#30340;&#25968;&#37327;&#65292;&#25105;&#20204;&#20026;&#27169;&#22411;&#36827;&#34892;&#20102;&#26816;&#32034;&#22686;&#24378;&#20248;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#26816;&#32034;&#31574;&#30053;&#12290;&#20026;&#20102;&#25552;&#39640;&#25105;&#20204;&#27169;&#22411;&#23545;&#38750;&#33521;&#35821;&#20351;&#29992;&#32773;&#30340;&#21487;&#35775;&#38382;&#24615;&#65292;&#25105;&#20204;&#24314;&#35758;&#21033;&#29992;&#32423;&#32852;&#26426;&#22120;&#32763;&#35793;&#65292;&#24182;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#19982;&#32763;&#35793;&#30340;&#24615;&#33021;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces ClimateGPT, a model family of domain-specific large language models that synthesize interdisciplinary research on climate change. We trained two 7B models from scratch on a science-oriented dataset of 300B tokens. For the first model, the 4.2B domain-specific tokens were included during pre-training and the second was adapted to the climate domain after pre-training. Additionally, ClimateGPT-7B, 13B and 70B are continuously pre-trained from Llama~2 on a domain-specific dataset of 4.2B tokens. Each model is instruction fine-tuned on a high-quality and human-generated domain-specific dataset that has been created in close cooperation with climate scientists. To reduce the number of hallucinations, we optimize the model for retrieval augmentation and propose a hierarchical retrieval strategy. To increase the accessibility of our model to non-English speakers, we propose to make use of cascaded machine translation and show that this approach can perform comparably to 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21151;&#33021;&#24615;&#32447;&#24615;&#38750;&#39640;&#26031;&#26080;&#24490;&#29615;&#27169;&#22411;&#65288;Func-LiNGAM&#65289;&#65292;&#23558;&#21464;&#37327;&#30340;&#27010;&#24565;&#25193;&#23637;&#21040;&#20102;&#21521;&#37327;&#21644;&#20989;&#25968;&#65292;&#24182;&#35299;&#20915;&#20102;&#21407;&#22987;&#27169;&#22411;&#26080;&#27861;&#22788;&#29702;&#26080;&#38480;&#32500;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#22312;&#22240;&#26524;&#21457;&#29616;&#20013;&#25552;&#20379;&#20102;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.09641</link><description>&lt;p&gt;
&#21151;&#33021;&#24615;&#32447;&#24615;&#38750;&#39640;&#26031;&#26080;&#24490;&#29615;&#27169;&#22411;&#29992;&#20110;&#22240;&#26524;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Functional Linear Non-Gaussian Acyclic Model for Causal Discovery. (arXiv:2401.09641v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09641
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21151;&#33021;&#24615;&#32447;&#24615;&#38750;&#39640;&#26031;&#26080;&#24490;&#29615;&#27169;&#22411;&#65288;Func-LiNGAM&#65289;&#65292;&#23558;&#21464;&#37327;&#30340;&#27010;&#24565;&#25193;&#23637;&#21040;&#20102;&#21521;&#37327;&#21644;&#20989;&#25968;&#65292;&#24182;&#35299;&#20915;&#20102;&#21407;&#22987;&#27169;&#22411;&#26080;&#27861;&#22788;&#29702;&#26080;&#38480;&#32500;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#22312;&#22240;&#26524;&#21457;&#29616;&#20013;&#25552;&#20379;&#20102;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22240;&#26524;&#21457;&#29616;&#20013;&#65292;&#38750;&#39640;&#26031;&#24615;&#24050;&#34987;&#29992;&#26469;&#25551;&#36848;&#32447;&#24615;&#38750;&#39640;&#26031;&#26080;&#24490;&#29615;&#27169;&#22411;&#65288;LiNGAM&#65289;&#30340;&#23436;&#25972;&#37197;&#32622;&#65292;&#21253;&#25324;&#21464;&#37327;&#30340;&#22240;&#26524;&#39034;&#24207;&#21450;&#23427;&#20204;&#20043;&#38388;&#30340;&#36830;&#25509;&#24378;&#24230;&#12290;&#28982;&#32780;&#65292;LiNGAM&#21482;&#33021;&#22788;&#29702;&#26377;&#38480;&#32500;&#24773;&#20917;&#12290;&#20026;&#20102;&#25193;&#23637;&#36825;&#20010;&#27010;&#24565;&#65292;&#25105;&#20204;&#23558;&#21464;&#37327;&#30340;&#27010;&#24565;&#25193;&#23637;&#21040;&#21253;&#25324;&#21521;&#37327;&#29978;&#33267;&#20989;&#25968;&#65292;&#24418;&#25104;&#20102;&#21151;&#33021;&#24615;&#32447;&#24615;&#38750;&#39640;&#26031;&#26080;&#24490;&#29615;&#27169;&#22411;&#65288;Func-LiNGAM&#65289;&#12290;&#25105;&#20204;&#30340;&#21160;&#26426;&#28304;&#20110;&#24076;&#26395;&#22312;&#28041;&#21450;&#33041;&#25928;&#24212;&#36830;&#25509;&#20219;&#21153;&#30340;&#24773;&#20917;&#19979;&#65292;&#27604;&#22914;fMRI&#21644;EEG&#25968;&#25454;&#38598;&#20013;&#35782;&#21035;&#22240;&#26524;&#20851;&#31995;&#12290;&#25105;&#20204;&#35828;&#26126;&#20102;&#20026;&#20160;&#20040;&#21407;&#22987;&#30340;LiNGAM&#26080;&#27861;&#22788;&#29702;&#36825;&#20123;&#26412;&#36136;&#19978;&#26159;&#26080;&#38480;&#32500;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#20174;&#23454;&#35777;&#21644;&#29702;&#35770;&#30340;&#35282;&#24230;&#35299;&#37322;&#20102;&#21151;&#33021;&#25968;&#25454;&#20998;&#26512;&#30340;&#21487;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In causal discovery, non-Gaussianity has been used to characterize the complete configuration of a Linear Non-Gaussian Acyclic Model (LiNGAM), encompassing both the causal ordering of variables and their respective connection strengths. However, LiNGAM can only deal with the finite-dimensional case. To expand this concept, we extend the notion of variables to encompass vectors and even functions, leading to the Functional Linear Non-Gaussian Acyclic Model (Func-LiNGAM). Our motivation stems from the desire to identify causal relationships in brain-effective connectivity tasks involving, for example, fMRI and EEG datasets. We demonstrate why the original LiNGAM fails to handle these inherently infinite-dimensional datasets and explain the availability of functional data analysis from both empirical and theoretical perspectives. {We establish theoretical guarantees of the identifiability of the causal relationship among non-Gaussian random vectors and even random functions in infinite-di
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#34701;&#21512;&#31574;&#30053;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#21160;&#21270;&#19977;&#32500;&#22810;&#27169;&#24335;&#36229;&#22768;&#32974;&#30424;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#35782;&#21035;&#32974;&#30424;&#21464;&#21270;&#65292;&#26377;&#26395;&#35299;&#20915;&#24403;&#21069;&#36229;&#22768;&#22788;&#29702;&#26041;&#27861;&#30340;&#21171;&#21160;&#23494;&#38598;&#12289;&#32791;&#26102;&#38271;&#21644;&#23481;&#26131;&#20986;&#38169;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#20419;&#36827;&#26089;&#26399;&#26816;&#27979;&#21644;&#28508;&#22312;&#27835;&#30103;&#12290;</title><link>http://arxiv.org/abs/2401.09638</link><description>&lt;p&gt;
&#20351;&#29992;&#34701;&#21512;&#31574;&#30053;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#21160;&#21270;&#19977;&#32500;&#22810;&#27169;&#24335;&#36229;&#22768;&#32974;&#30424;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Automatic 3D Multi-modal Ultrasound Segmentation of Human Placenta using Fusion Strategies and Deep Learning. (arXiv:2401.09638v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09638
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#34701;&#21512;&#31574;&#30053;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#21160;&#21270;&#19977;&#32500;&#22810;&#27169;&#24335;&#36229;&#22768;&#32974;&#30424;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#35782;&#21035;&#32974;&#30424;&#21464;&#21270;&#65292;&#26377;&#26395;&#35299;&#20915;&#24403;&#21069;&#36229;&#22768;&#22788;&#29702;&#26041;&#27861;&#30340;&#21171;&#21160;&#23494;&#38598;&#12289;&#32791;&#26102;&#38271;&#21644;&#23481;&#26131;&#20986;&#38169;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#20419;&#36827;&#26089;&#26399;&#26816;&#27979;&#21644;&#28508;&#22312;&#27835;&#30103;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#36229;&#22768;&#26159;&#20020;&#24202;&#23454;&#36341;&#20013;&#24120;&#29992;&#30340;&#21307;&#23398;&#25104;&#20687;&#27169;&#24577;&#65292;&#22240;&#20854;&#23433;&#20840;&#24615;&#12289;&#26080;&#21019;&#24615;&#21644;&#20415;&#25658;&#24615;&#65292;&#25104;&#20026;&#23381;&#26399;&#32974;&#20799;&#35780;&#20272;&#30340;&#20027;&#35201;&#25104;&#20687;&#27169;&#24577;&#12290;&#24403;&#21069;&#36229;&#22768;&#22788;&#29702;&#26041;&#27861;&#35201;&#20040;&#26159;&#25163;&#21160;&#30340;&#65292;&#35201;&#20040;&#26159;&#21322;&#33258;&#21160;&#30340;&#65292;&#22240;&#27492;&#24037;&#20316;&#37327;&#22823;&#12289;&#32791;&#26102;&#38271;&#19988;&#23481;&#26131;&#20986;&#38169;&#65292;&#33258;&#21160;&#21270;&#23558;&#22312;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#26041;&#38754;&#22823;&#26377;&#24110;&#21161;&#12290;&#22312;&#36739;&#26089;&#30340;&#23381;&#26399;&#33258;&#21160;&#35782;&#21035;&#32974;&#30424;&#21464;&#21270;&#21487;&#33021;&#26377;&#21161;&#20110;&#27835;&#30103;&#32974;&#20799;&#29983;&#38271;&#21463;&#38480;&#21644;&#23376;&#30187;&#21069;&#26399;&#31561;&#30142;&#30149;&#65292;&#36825;&#20123;&#30142;&#30149;&#30446;&#21069;&#20165;&#33021;&#22312;&#26202;&#26399;&#23381;&#40836;&#26816;&#27979;&#21040;&#65292;&#28508;&#22312;&#22320;&#21487;&#20197;&#39044;&#38450;&#21608;&#22260;&#23156;&#20799;&#30340;&#21457;&#30149;&#29575;&#21644;&#27515;&#20129;&#29575;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#32467;&#21512;&#19981;&#21516;&#34701;&#21512;&#31574;&#30053;&#30340;&#33258;&#21160;&#21270;&#19977;&#32500;&#22810;&#27169;&#24335;&#65288;B&#27169;&#24335;&#21644;&#21151;&#29575;&#22810;&#26222;&#21202;&#65289;&#36229;&#22768;&#32974;&#30424;&#20998;&#21106;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Purpose: Ultrasound is the most commonly used medical imaging modality for diagnosis and screening in clinical practice. Due to its safety profile, noninvasive nature and portability, ultrasound is the primary imaging modality for fetal assessment in pregnancy. Current ultrasound processing methods are either manual or semi-automatic and are therefore laborious, time-consuming and prone to errors, and automation would go a long way in addressing these challenges. Automated identification of placental changes at earlier gestation could facilitate potential therapies for conditions such as fetal growth restriction and pre-eclampsia that are currently detected only at late gestational age, potentially preventing perinatal morbidity and mortality.  Methods: We propose an automatic three-dimensional multi-modal (B-mode and power Doppler) ultrasound segmentation of the human placenta using deep learning combined with different fusion strategies.We collected data containing Bmode and power Do
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#22312;&#30913;&#23548;&#33322;&#31995;&#32479;&#20013;&#20351;&#29992;Tolles-Lawson&#31995;&#25968;&#21644;&#28082;&#20307;&#26102;&#38388;&#24120;&#25968;&#32593;&#32476;&#65288;LTCs&#65289;&#36827;&#34892;&#27668;&#30913;&#34917;&#20607;&#26657;&#20934;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#23454;&#38469;&#39134;&#34892;&#25968;&#25454;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20102;&#39640;&#36798;64&#65285;&#30340;&#27668;&#30913;&#34917;&#20607;&#35823;&#24046;&#20943;&#23569;&#65292;&#20248;&#20110;&#20256;&#32479;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.09631</link><description>&lt;p&gt;
&#20351;&#29992;&#28082;&#20307;&#26102;&#38388;&#24120;&#25968;&#32593;&#32476;&#23545;&#30913;&#23548;&#33322;&#31995;&#32479;&#20013;&#30340;&#27668;&#30913;&#34917;&#20607;&#36827;&#34892;&#29289;&#29702;&#20449;&#24687;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Physics-Informed Calibration of Aeromagnetic Compensation in Magnetic Navigation Systems using Liquid Time-Constant Networks. (arXiv:2401.09631v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09631
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#22312;&#30913;&#23548;&#33322;&#31995;&#32479;&#20013;&#20351;&#29992;Tolles-Lawson&#31995;&#25968;&#21644;&#28082;&#20307;&#26102;&#38388;&#24120;&#25968;&#32593;&#32476;&#65288;LTCs&#65289;&#36827;&#34892;&#27668;&#30913;&#34917;&#20607;&#26657;&#20934;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#23454;&#38469;&#39134;&#34892;&#25968;&#25454;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20102;&#39640;&#36798;64&#65285;&#30340;&#27668;&#30913;&#34917;&#20607;&#35823;&#24046;&#20943;&#23569;&#65292;&#20248;&#20110;&#20256;&#32479;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30913;&#23548;&#33322;&#65288;MagNav&#65289;&#26159;&#20840;&#29699;&#23450;&#20301;&#31995;&#32479;&#65288;GPS&#65289;&#30340;&#19968;&#31181;&#26032;&#20852;&#26367;&#20195;&#26041;&#26696;&#65292;&#24050;&#34987;&#35777;&#26126;&#22312;&#39134;&#34892;&#22120;&#23548;&#33322;&#20013;&#38750;&#24120;&#26377;&#29992;&#12290;&#20256;&#32479;&#30340;&#39134;&#34892;&#22120;&#23548;&#33322;&#31995;&#32479;&#22312;&#26576;&#20123;&#29615;&#22659;&#21644;&#21463;&#21040;&#25915;&#20987;&#26102;&#23384;&#22312;&#31934;&#24230;&#21644;&#21487;&#38752;&#24615;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;&#31354;&#20013;&#30913;&#23548;&#33322;&#21033;&#29992;&#22320;&#29699;&#30340;&#30913;&#22330;&#25552;&#20379;&#20934;&#30830;&#30340;&#20301;&#32622;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#39134;&#34892;&#22120;&#30005;&#23376;&#35774;&#22791;&#24341;&#36215;&#30340;&#22806;&#37096;&#30913;&#22330;&#21644;&#22320;&#29699;&#30340;&#22823;&#35268;&#27169;&#30913;&#22330;&#20250;&#24178;&#25200;&#21040;&#36739;&#24369;&#30340;&#24863;&#20852;&#36259;&#20449;&#21495;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;Tolles-Lawson&#31995;&#25968;&#36827;&#34892;&#34917;&#20607;&#65292;&#24182;&#20351;&#29992;&#28082;&#20307;&#26102;&#38388;&#24120;&#25968;&#32593;&#32476;&#65288;LTCs&#65289;&#21435;&#38500;&#39134;&#34892;&#22120;&#30913;&#28304;&#20135;&#29983;&#30340;&#22797;&#26434;&#32780;&#22024;&#26434;&#30340;&#20449;&#21495;&#12290;&#20351;&#29992;&#23454;&#38469;&#39134;&#34892;&#25968;&#25454;&#21644;&#30913;&#21147;&#35745;&#27979;&#37327;&#20197;&#21450;&#39134;&#34892;&#22120;&#27979;&#37327;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#27668;&#30913;&#34917;&#20607;&#35823;&#24046;&#65288;RMSE nT&#65289;&#26368;&#22810;&#21487;&#20197;&#20943;&#23569;64&#65285;&#65292;&#20248;&#20110;&#20256;&#32479;&#27169;&#22411;&#12290;&#36825;&#19968;&#26174;&#33879;&#25913;&#36827;&#31361;&#20986;&#20102;&#29289;&#29702;&#20449;&#24687;&#26657;&#20934;&#22312;&#30913;&#23548;&#33322;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Magnetic navigation (MagNav) is a rising alternative to the Global Positioning System (GPS) and has proven useful for aircraft navigation. Traditional aircraft navigation systems, while effective, face limitations in precision and reliability in certain environments and against attacks. Airborne MagNav leverages the Earth's magnetic field to provide accurate positional information. However, external magnetic fields induced by aircraft electronics and Earth's large-scale magnetic fields disrupt the weaker signal of interest. We introduce a physics-informed approach using Tolles-Lawson coefficients for compensation and Liquid Time-Constant Networks (LTCs) to remove complex, noisy signals derived from the aircraft's magnetic sources. Using real flight data with magnetometer measurements and aircraft measurements, we observe up to a 64% reduction in aeromagnetic compensation error (RMSE nT), outperforming conventional models. This significant improvement underscores the potential of a phys
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#20010;&#23616;&#37096;&#32447;&#24615;&#20998;&#31867;&#22120;&#32452;&#21512;&#30340;&#26032;&#22411;&#38750;&#32447;&#24615;&#20998;&#31867;&#22120;&#65292;&#25552;&#20379;&#20102;&#21487;&#25193;&#23637;&#30340;&#27867;&#21270;&#22810;&#26680;&#23398;&#20064;&#35757;&#32451;&#31639;&#27861;&#65292;&#22635;&#34917;&#20102;&#39640;&#20934;&#30830;&#24615;&#20294;&#32531;&#24930;&#30340;&#38750;&#32447;&#24615;&#20998;&#31867;&#22120;&#21644;&#24555;&#36895;&#20294;&#20302;&#20934;&#30830;&#24615;&#30340;&#32447;&#24615;&#20998;&#31867;&#22120;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2401.09629</link><description>&lt;p&gt;
&#22810;&#20010;&#23616;&#37096;&#32447;&#24615;&#26680;&#26426;&#22120;
&lt;/p&gt;
&lt;p&gt;
Multiple Locally Linear Kernel Machines. (arXiv:2401.09629v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09629
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#20010;&#23616;&#37096;&#32447;&#24615;&#20998;&#31867;&#22120;&#32452;&#21512;&#30340;&#26032;&#22411;&#38750;&#32447;&#24615;&#20998;&#31867;&#22120;&#65292;&#25552;&#20379;&#20102;&#21487;&#25193;&#23637;&#30340;&#27867;&#21270;&#22810;&#26680;&#23398;&#20064;&#35757;&#32451;&#31639;&#27861;&#65292;&#22635;&#34917;&#20102;&#39640;&#20934;&#30830;&#24615;&#20294;&#32531;&#24930;&#30340;&#38750;&#32447;&#24615;&#20998;&#31867;&#22120;&#21644;&#24555;&#36895;&#20294;&#20302;&#20934;&#30830;&#24615;&#30340;&#32447;&#24615;&#20998;&#31867;&#22120;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#20010;&#23616;&#37096;&#32447;&#24615;&#20998;&#31867;&#22120;&#32452;&#21512;&#30340;&#26032;&#22411;&#38750;&#32447;&#24615;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026;&#19968;&#20010;$l_1$&#22810;&#26680;&#23398;&#20064;&#38382;&#39064;&#65292;&#20351;&#29992;&#35768;&#22810;&#23616;&#37096;&#32447;&#24615;&#26680;&#12290;&#30001;&#20110;&#36825;&#26679;&#30340;&#26680;&#20989;&#25968;&#25968;&#37327;&#24222;&#22823;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#27867;&#21270;&#22810;&#26680;&#23398;&#20064;&#35757;&#32451;&#31639;&#27861;&#26469;&#22788;&#29702;&#27969;&#24335;&#26680;&#20989;&#25968;&#12290;&#22312;&#25512;&#26029;&#26102;&#38388;&#26041;&#38754;&#65292;&#24471;&#21040;&#30340;&#20998;&#31867;&#22120;&#22635;&#34917;&#20102;&#39640;&#20934;&#30830;&#24615;&#20294;&#32531;&#24930;&#30340;&#38750;&#32447;&#24615;&#20998;&#31867;&#22120;&#65288;&#22914;&#20256;&#32479;&#30340;&#22810;&#26680;&#23398;&#20064;&#65289;&#21644;&#24555;&#36895;&#20294;&#20302;&#20934;&#30830;&#24615;&#30340;&#32447;&#24615;&#20998;&#31867;&#22120;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we propose a new non-linear classifier based on a combination of locally linear classifiers. A well known optimization formulation is given as we cast the problem in a $\ell_1$ Multiple Kernel Learning (MKL) problem using many locally linear kernels. Since the number of such kernels is huge, we provide a scalable generic MKL training algorithm handling streaming kernels. With respect to the inference time, the resulting classifier fits the gap between high accuracy but slow non-linear classifiers (such as classical MKL) and fast but low accuracy linear classifiers.
&lt;/p&gt;</description></item><item><title>SymTC&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#33136;&#26894;MR&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;Transformer&#21644;CNN&#30456;&#32467;&#21512;&#65292;&#24182;&#21033;&#29992;&#20301;&#32622;&#23884;&#20837;&#21644;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#30340;&#23454;&#20363;&#20998;&#21106;&#12290;</title><link>http://arxiv.org/abs/2401.09627</link><description>&lt;p&gt;
SymTC:&#19968;&#31181;&#29992;&#20110;&#33136;&#26894;MRI&#23454;&#20363;&#20998;&#21106;&#30340;&#20849;&#29983;Transformer-CNN&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
SymTC: A Symbiotic Transformer-CNN Net for Instance Segmentation of Lumbar Spine MRI. (arXiv:2401.09627v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09627
&lt;/p&gt;
&lt;p&gt;
SymTC&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#33136;&#26894;MR&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;Transformer&#21644;CNN&#30456;&#32467;&#21512;&#65292;&#24182;&#21033;&#29992;&#20301;&#32622;&#23884;&#20837;&#21644;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#30340;&#23454;&#20363;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26894;&#38388;&#30424;&#30142;&#30149;&#26159;&#19968;&#31181;&#24120;&#35265;&#30149;&#30151;&#65292;&#32463;&#24120;&#23548;&#33268;&#38388;&#27463;&#24615;&#25110;&#25345;&#32493;&#24615;&#30340;&#33136;&#32972;&#30140;&#30171;&#65292;&#23545;&#35813;&#30142;&#30149;&#30340;&#35786;&#26029;&#21644;&#35780;&#20272;&#20381;&#36182;&#20110;&#33136;&#26894;MR&#22270;&#20687;&#20013;&#26894;&#39592;&#21644;&#26894;&#38388;&#30424;&#20960;&#20309;&#24418;&#29366;&#30340;&#20934;&#30830;&#27979;&#37327;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#27169;&#22411;&#21487;&#20197;&#24110;&#21161;&#20020;&#24202;&#21307;&#29983;&#20197;&#26356;&#39640;&#25928;&#30340;&#26041;&#24335;&#33258;&#21160;&#21270;&#22320;&#23545;&#33136;&#26894;&#30340;&#20010;&#20307;&#23454;&#20363;&#65288;&#26894;&#39592;&#21644;&#26894;&#38388;&#30424;&#65289;&#36827;&#34892;&#22270;&#20687;&#20998;&#21106;&#65292;&#36825;&#34987;&#31216;&#20026;&#23454;&#20363;&#22270;&#20687;&#20998;&#21106;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SymTC&#65292;&#19968;&#31181;&#21019;&#26032;&#30340;&#33136;&#26894;MR&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#65292;&#23427;&#32467;&#21512;&#20102;Transformer&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#20248;&#21183;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#24182;&#34892;&#30340;&#21452;&#36335;&#24452;&#26550;&#26500;&#26469;&#34701;&#21512;CNN&#23618;&#21644;Transformer&#23618;&#65292;&#24182;&#22312;Transformer&#30340;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#20013;&#38598;&#25104;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20301;&#32622;&#23884;&#20837;&#65292;&#22686;&#24378;&#20102;&#20301;&#32622;&#20449;&#24687;&#30340;&#21033;&#29992;&#20197;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#20998;&#21106;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#23450;&#20301;&#31995;&#32479;&#36827;&#34892;&#27169;&#22411;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intervertebral disc disease, a prevalent ailment, frequently leads to intermittent or persistent low back pain, and diagnosing and assessing of this disease rely on accurate measurement of vertebral bone and intervertebral disc geometries from lumbar MR images. Deep neural network (DNN) models may assist clinicians with more efficient image segmentation of individual instances (disks and vertebrae) of the lumbar spine in an automated way, which is termed as instance image segmentation. In this work, we proposed SymTC, an innovative lumbar spine MR image segmentation model that combines the strengths of Transformer and Convolutional Neural Network (CNN). Specifically, we designed a parallel dual-path architecture to merge CNN layers and Transformer layers, and we integrated a novel position embedding into the self-attention module of Transformer, enhancing the utilization of positional information for more accurate segmentation. To further improves model performance, we introduced a new
&lt;/p&gt;</description></item><item><title>MITS-GAN&#26159;&#19968;&#31181;&#29992;&#20110;&#20445;&#25252;&#21307;&#23398;&#24433;&#20687;&#20813;&#21463;&#31713;&#25913;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#36866;&#24403;&#30340;&#39640;&#26031;&#22122;&#22768;&#20316;&#20026;&#38450;&#25252;&#25514;&#26045;&#65292;&#25171;&#20081;&#25915;&#20987;&#32773;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#26550;&#26500;&#30340;&#36755;&#20986;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;MITS-GAN&#33021;&#22815;&#29983;&#25104;&#32784;&#31713;&#25913;&#22270;&#20687;&#65292;&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.09624</link><description>&lt;p&gt;
MITS-GAN: &#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20445;&#25252;&#21307;&#23398;&#24433;&#20687;&#20813;&#21463;&#31713;&#25913;
&lt;/p&gt;
&lt;p&gt;
MITS-GAN: Safeguarding Medical Imaging from Tampering with Generative Adversarial Networks. (arXiv:2401.09624v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09624
&lt;/p&gt;
&lt;p&gt;
MITS-GAN&#26159;&#19968;&#31181;&#29992;&#20110;&#20445;&#25252;&#21307;&#23398;&#24433;&#20687;&#20813;&#21463;&#31713;&#25913;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#36866;&#24403;&#30340;&#39640;&#26031;&#22122;&#22768;&#20316;&#20026;&#38450;&#25252;&#25514;&#26045;&#65292;&#25171;&#20081;&#25915;&#20987;&#32773;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#26550;&#26500;&#30340;&#36755;&#20986;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;MITS-GAN&#33021;&#22815;&#29983;&#25104;&#32784;&#31713;&#25913;&#22270;&#20687;&#65292;&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#65292;&#22312;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#20063;&#24341;&#21457;&#20102;&#28508;&#22312;&#30340;&#24694;&#24847;&#20351;&#29992;&#30340;&#25285;&#24551;&#65292;&#23588;&#20854;&#26159;&#22312;&#21307;&#23398;&#24433;&#20687;&#31561;&#25935;&#24863;&#39046;&#22495;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;MITS-GAN&#65292;&#29992;&#20110;&#38450;&#27490;&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#31713;&#25913;&#65292;&#29305;&#21035;&#20851;&#27880;CT&#25195;&#25551;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#19981;&#21487;&#23519;&#35273;&#20294;&#31934;&#30830;&#30340;&#25200;&#21160;&#26469;&#25171;&#20081;&#25915;&#20987;&#32773;&#30340;CT-GAN&#26550;&#26500;&#30340;&#36755;&#20986;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#28041;&#21450;&#23558;&#36866;&#24403;&#30340;&#39640;&#26031;&#22122;&#22768;&#24341;&#20837;&#21040;&#36755;&#20837;&#20013;&#20316;&#20026;&#23545;&#21508;&#31181;&#25915;&#20987;&#30340;&#20445;&#25252;&#25514;&#26045;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#25552;&#39640;&#38450;&#31713;&#25913;&#33021;&#21147;&#65292;&#19982;&#29616;&#26377;&#25216;&#26415;&#30456;&#27604;&#20855;&#26377;&#20248;&#21183;&#12290;&#23545;CT&#25195;&#25551;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;MITS-GAN&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#24378;&#35843;&#20102;&#20854;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#21487;&#24573;&#30053;&#20266;&#24433;&#30340;&#32784;&#31713;&#25913;&#22270;&#20687;&#30340;&#33021;&#21147;&#12290;&#30001;&#20110;&#21307;&#23398;&#39046;&#22495;&#20013;&#30340;&#22270;&#20687;&#31713;&#25913;&#24102;&#26469;&#20102;&#21361;&#21450;&#29983;&#21629;&#30340;&#39118;&#38505;&#65292;&#25105;&#20204;&#30340;&#20027;&#21160;&#38450;&#25252;&#26041;&#27861;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
The progress in generative models, particularly Generative Adversarial Networks (GANs), opened new possibilities for image generation but raised concerns about potential malicious uses, especially in sensitive areas like medical imaging. This study introduces MITS-GAN, a novel approach to prevent tampering in medical images, with a specific focus on CT scans. The approach disrupts the output of the attacker's CT-GAN architecture by introducing imperceptible but yet precise perturbations. Specifically, the proposed approach involves the introduction of appropriate Gaussian noise to the input as a protective measure against various attacks. Our method aims to enhance tamper resistance, comparing favorably to existing techniques. Experimental results on a CT scan dataset demonstrate MITS-GAN's superior performance, emphasizing its ability to generate tamper-resistant images with negligible artifacts. As image tampering in medical domains poses life-threatening risks, our proactive approac
&lt;/p&gt;</description></item><item><title>SMOOTHIE&#26159;&#19968;&#31181;&#36890;&#36807;&#32771;&#34385;&#25439;&#22833;&#20989;&#25968;&#30340;&#8220;&#20809;&#28369;&#24230;&#8221;&#26469;&#24341;&#23548;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#26032;&#22411;&#26041;&#27861;&#65292;&#22312;&#36719;&#20214;&#20998;&#26512;&#20013;&#24212;&#29992;&#21487;&#20197;&#24102;&#26469;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.09622</link><description>&lt;p&gt;
SMOOTHIE: &#36719;&#20214;&#20998;&#26512;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
SMOOTHIE: A Theory of Hyper-parameter Optimization for Software Analytics. (arXiv:2401.09622v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09622
&lt;/p&gt;
&lt;p&gt;
SMOOTHIE&#26159;&#19968;&#31181;&#36890;&#36807;&#32771;&#34385;&#25439;&#22833;&#20989;&#25968;&#30340;&#8220;&#20809;&#28369;&#24230;&#8221;&#26469;&#24341;&#23548;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#26032;&#22411;&#26041;&#27861;&#65292;&#22312;&#36719;&#20214;&#20998;&#26512;&#20013;&#24212;&#29992;&#21487;&#20197;&#24102;&#26469;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#21442;&#25968;&#20248;&#21270;&#26159;&#35843;&#25972;&#23398;&#20064;&#22120;&#25511;&#21046;&#21442;&#25968;&#30340;&#40657;&#39764;&#27861;&#12290;&#22312;&#36719;&#20214;&#20998;&#26512;&#20013;&#65292;&#32463;&#24120;&#21457;&#29616;&#35843;&#20248;&#21487;&#20197;&#24102;&#26469;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#36229;&#21442;&#25968;&#20248;&#21270;&#22312;&#36719;&#20214;&#20998;&#26512;&#20013;&#36890;&#24120;&#34987;&#24456;&#23569;&#25110;&#24456;&#24046;&#22320;&#24212;&#29992;&#65292;&#21487;&#33021;&#26159;&#22240;&#20026;&#25506;&#32034;&#25152;&#26377;&#21442;&#25968;&#36873;&#39033;&#30340;CPU&#25104;&#26412;&#22826;&#39640;&#12290;&#25105;&#20204;&#20551;&#35774;&#24403;&#25439;&#22833;&#20989;&#25968;&#30340;&#8220;&#20809;&#28369;&#24230;&#8221;&#26356;&#22909;&#26102;&#65292;&#23398;&#20064;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#26356;&#24378;&#12290;&#36825;&#20010;&#29702;&#35770;&#38750;&#24120;&#26377;&#29992;&#65292;&#22240;&#20026;&#21487;&#20197;&#24456;&#24555;&#27979;&#35797;&#19981;&#21516;&#36229;&#21442;&#25968;&#36873;&#25321;&#23545;&#8220;&#20809;&#28369;&#24230;&#8221;&#30340;&#24433;&#21709;&#65288;&#20363;&#22914;&#65292;&#23545;&#20110;&#28145;&#24230;&#23398;&#20064;&#22120;&#65292;&#22312;&#19968;&#20010;epoch&#20043;&#21518;&#23601;&#21487;&#20197;&#36827;&#34892;&#27979;&#35797;&#65289;&#12290;&#20026;&#20102;&#27979;&#35797;&#36825;&#20010;&#29702;&#35770;&#65292;&#26412;&#25991;&#23454;&#29616;&#21644;&#27979;&#35797;&#20102;SMOOTHIE&#65292;&#19968;&#31181;&#36890;&#36807;&#32771;&#34385;&#8220;&#20809;&#28369;&#24230;&#8221;&#26469;&#24341;&#23548;&#20248;&#21270;&#30340;&#26032;&#22411;&#36229;&#21442;&#25968;&#20248;&#21270;&#22120;&#12290;&#26412;&#25991;&#30340;&#23454;&#39564;&#23558;SMOOTHIE&#24212;&#29992;&#20110;&#22810;&#20010;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#65292;&#21253;&#25324;&#65288;a&#65289;GitHub&#38382;&#39064;&#23551;&#21629;&#39044;&#27979;&#65307;&#65288;b&#65289;&#38745;&#24577;&#20195;&#30721;&#35686;&#21578;&#20013;&#38169;&#35823;&#35686;&#25253;&#30340;&#26816;&#27979;&#65307;&#65288;c&#65289;&#32570;&#38519;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyper-parameter optimization is the black art of tuning a learner's control parameters. In software analytics, a repeated result is that such tuning can result in dramatic performance improvements. Despite this, hyper-parameter optimization is often applied rarely or poorly in software analytics--perhaps due to the CPU cost of exploring all those parameter options can be prohibitive.  We theorize that learners generalize better when the loss landscape is ``smooth''. This theory is useful since the influence on ``smoothness'' of different hyper-parameter choices can be tested very quickly (e.g. for a deep learner, after just one epoch).  To test this theory, this paper implements and tests SMOOTHIE, a novel hyper-parameter optimizer that guides its optimizations via considerations of ``smothness''. The experiments of this paper test SMOOTHIE on numerous SE tasks including (a) GitHub issue lifetime prediction; (b) detecting false alarms in static code warnings; (c) defect prediction, and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22303;&#22320;&#35206;&#30422;&#22270;&#20687;&#20998;&#31867;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#25552;&#39640;&#20998;&#31867;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#27604;&#36739;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;LC&#30740;&#31350;&#20013;&#23637;&#31034;&#20102;&#23427;&#20204;&#30340;&#20248;&#21183;&#65292;&#24182;&#22312;EuroSAT&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.09607</link><description>&lt;p&gt;
&#22303;&#22320;&#35206;&#30422;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Land Cover Image Classification. (arXiv:2401.09607v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09607
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22303;&#22320;&#35206;&#30422;&#22270;&#20687;&#20998;&#31867;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#25552;&#39640;&#20998;&#31867;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#27604;&#36739;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;LC&#30740;&#31350;&#20013;&#23637;&#31034;&#20102;&#23427;&#20204;&#30340;&#20248;&#21183;&#65292;&#24182;&#22312;EuroSAT&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22303;&#22320;&#35206;&#30422;&#65288;LC&#65289;&#22270;&#20687;&#20998;&#31867;&#22312;&#29702;&#35299;&#29615;&#22659;&#21464;&#21270;&#12289;&#22478;&#24066;&#35268;&#21010;&#21644;&#28798;&#23475;&#31649;&#29702;&#26041;&#38754;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;LC&#26041;&#27861;&#24448;&#24448;&#38656;&#35201;&#22823;&#37327;&#21171;&#21160;&#21147;&#19988;&#23481;&#26131;&#20986;&#38169;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#22303;&#22320;&#35206;&#30422;&#20998;&#26512;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#65292;&#22312;LC&#30740;&#31350;&#20013;&#23637;&#31034;&#20102;&#23427;&#20204;&#30340;&#24212;&#29992;&#21644;&#20248;&#21183;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#22522;&#20110;Sentinel-2&#21355;&#26143;&#22270;&#20687;&#30340;EuroSAT&#22522;&#20110;&#34917;&#19969;&#30340;LC&#20998;&#31867;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#24403;&#21069;&#30340;Transformer&#27169;&#22411;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Land Cover (LC) image classification has become increasingly significant in understanding environmental changes, urban planning, and disaster management. However, traditional LC methods are often labor-intensive and prone to human error. This paper explores state-of-the-art deep learning models for enhanced accuracy and efficiency in LC analysis. We compare convolutional neural networks (CNN) against transformer-based methods, showcasing their applications and advantages in LC studies. We used EuroSAT, a patch-based LC classification data set based on Sentinel-2 satellite images and achieved state-of-the-art results using current transformer models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#20154;&#33218;&#21160;&#20316;&#35782;&#21035;&#22312;&#22024;&#26434;&#29615;&#22659;&#20013;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;&#12290;&#36890;&#36807;&#20351;&#29992;&#35270;&#35273;&#31995;&#32479;&#36861;&#36394;&#26426;&#22120;&#20154;&#30340;&#36816;&#21160;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25552;&#21462;&#33218;&#37096;&#30340;&#20851;&#38190;&#28857;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22024;&#26434;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#31934;&#30830;&#30340;&#21160;&#20316;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2401.09606</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#33218;&#21160;&#20316;&#35782;&#21035;&#22312;&#22024;&#26434;&#29615;&#22659;&#20013;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Robustness Evaluation of Machine Learning Models for Robot Arm Action Recognition in Noisy Environments. (arXiv:2401.09606v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09606
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#20154;&#33218;&#21160;&#20316;&#35782;&#21035;&#22312;&#22024;&#26434;&#29615;&#22659;&#20013;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;&#12290;&#36890;&#36807;&#20351;&#29992;&#35270;&#35273;&#31995;&#32479;&#36861;&#36394;&#26426;&#22120;&#20154;&#30340;&#36816;&#21160;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25552;&#21462;&#33218;&#37096;&#30340;&#20851;&#38190;&#28857;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22024;&#26434;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#31934;&#30830;&#30340;&#21160;&#20316;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22024;&#26434;&#29615;&#22659;&#20013;&#65292;&#21033;&#29992;&#35270;&#35273;&#31995;&#32479;&#35782;&#21035;&#26426;&#22120;&#20154;&#33218;&#30340;&#19981;&#21516;&#20294;&#31354;&#38388;&#20020;&#36817;&#30340;&#36816;&#21160;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22024;&#26434;&#29615;&#22659;&#20013;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#26426;&#22120;&#20154;&#33218;&#21160;&#20316;&#35782;&#21035;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20351;&#29992;&#35270;&#35273;&#31995;&#32479;&#36861;&#36394;&#26426;&#22120;&#20154;&#30340;&#36816;&#21160;&#65292;&#28982;&#21518;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25552;&#21462;&#33218;&#37096;&#30340;&#20851;&#38190;&#28857;&#12290;&#36890;&#36807;&#23545;&#27604;&#20998;&#26512;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#35780;&#20272;&#20102;&#35813;&#27169;&#22411;&#22312;&#22024;&#26434;&#29615;&#22659;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#22312;&#19968;&#20010;3&#215;3&#30340;&#26629;&#26684;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#19968;&#20010;&#20117;&#23383;&#26827;&#28216;&#25103;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#20854;&#20013;&#37325;&#28857;&#26159;&#20934;&#30830;&#35782;&#21035;&#22312;&#36825;&#20010;&#21463;&#38480;&#29615;&#22659;&#20013;&#36873;&#25321;&#29305;&#23450;&#20301;&#32622;&#30340;&#33218;&#37096;&#21160;&#20316;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#22122;&#22768;&#21644;&#19981;&#30830;&#23450;&#24615;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#31934;&#30830;&#30340;&#20851;&#38190;&#28857;&#26816;&#27979;&#21644;&#21160;&#20316;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the realm of robot action recognition, identifying distinct but spatially proximate arm movements using vision systems in noisy environments poses a significant challenge. This paper studies robot arm action recognition in noisy environments using machine learning techniques. Specifically, a vision system is used to track the robot's movements followed by a deep learning model to extract the arm's key points. Through a comparative analysis of machine learning methods, the effectiveness and robustness of this model are assessed in noisy environments. A case study was conducted using the Tic-Tac-Toe game in a 3-by-3 grid environment, where the focus is to accurately identify the actions of the arms in selecting specific locations within this constrained environment. Experimental results show that our approach can achieve precise key point detection and action classification despite the addition of noise and uncertainties to the dataset.
&lt;/p&gt;</description></item><item><title>MedBlindTuner&#36890;&#36807;&#32467;&#21512;Transformer&#21644;&#20840;&#21516;&#24577;&#21152;&#23494;&#65292;&#22312;&#20445;&#25252;&#30149;&#20154;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#23545;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#30340;&#38544;&#31169;&#20445;&#25252;&#24494;&#35843;&#12290;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;MedBlindTuner&#22312;&#21152;&#23494;&#25968;&#25454;&#19978;&#21462;&#24471;&#20102;&#19982;&#38750;&#21152;&#23494;&#25968;&#25454;&#35757;&#32451;&#27169;&#22411;&#30456;&#24403;&#30340;&#20934;&#30830;&#24230;&#65292;&#20026;&#22806;&#21253;&#26426;&#22120;&#23398;&#20064;&#35745;&#31639;&#25552;&#20379;&#20102;&#23433;&#20840;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2401.09604</link><description>&lt;p&gt;
MedBlindTuner: &#36890;&#36807;Transformer&#21644;&#20840;&#21516;&#24577;&#21152;&#23494;&#23454;&#29616;&#23545;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#30340;&#38544;&#31169;&#20445;&#25252;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
MedBlindTuner: Towards Privacy-preserving Fine-tuning on Biomedical Images with Transformers and Fully Homomorphic Encryption. (arXiv:2401.09604v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09604
&lt;/p&gt;
&lt;p&gt;
MedBlindTuner&#36890;&#36807;&#32467;&#21512;Transformer&#21644;&#20840;&#21516;&#24577;&#21152;&#23494;&#65292;&#22312;&#20445;&#25252;&#30149;&#20154;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#23545;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#30340;&#38544;&#31169;&#20445;&#25252;&#24494;&#35843;&#12290;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;MedBlindTuner&#22312;&#21152;&#23494;&#25968;&#25454;&#19978;&#21462;&#24471;&#20102;&#19982;&#38750;&#21152;&#23494;&#25968;&#25454;&#35757;&#32451;&#27169;&#22411;&#30456;&#24403;&#30340;&#20934;&#30830;&#24230;&#65292;&#20026;&#22806;&#21253;&#26426;&#22120;&#23398;&#20064;&#35745;&#31639;&#25552;&#20379;&#20102;&#23433;&#20840;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#30340;&#36827;&#27493;&#26174;&#33879;&#25913;&#21464;&#20102;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#65292;&#20419;&#20351;&#21307;&#38498;&#20381;&#36182;&#22806;&#37096;&#26426;&#22120;&#23398;&#20064;&#26381;&#21153;&#12290;&#28982;&#32780;&#65292;&#19982;&#31532;&#19977;&#26041;&#20849;&#20139;&#25935;&#24863;&#24739;&#32773;&#25968;&#25454;&#65292;&#22914;&#33016;&#37096;X&#23556;&#32447;&#65292;&#20250;&#23384;&#22312;&#22266;&#26377;&#30340;&#38544;&#31169;&#39118;&#38505;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MedBlindTuner&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#20840;&#21516;&#24577;&#21152;&#23494;&#21644;&#25968;&#25454;&#39640;&#25928;&#30340;&#22270;&#20687;&#36716;&#25442;&#22120;(DEiT)&#30340;&#38544;&#31169;&#20445;&#25252;&#26694;&#26550;&#12290;MedBlindTuner&#20351;&#24471;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20165;&#22312;&#20840;&#21516;&#24577;&#21152;&#23494;&#30340;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;MedBlindTuner&#23454;&#29616;&#20102;&#19982;&#22312;&#38750;&#21152;&#23494;&#22270;&#20687;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#24403;&#30340;&#20934;&#30830;&#24230;&#65292;&#20026;&#22806;&#21253;&#26426;&#22120;&#23398;&#20064;&#35745;&#31639;&#25552;&#20379;&#20102;&#23433;&#20840;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21516;&#26102;&#20445;&#25252;&#20102;&#24739;&#32773;&#25968;&#25454;&#30340;&#38544;&#31169;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22312;&#35813;&#39046;&#22495;&#20013;&#20351;&#29992;&#25968;&#25454;&#39640;&#25928;&#22270;&#20687;&#36716;&#25442;&#22120;&#21644;&#20840;&#21516;&#24577;&#21152;&#23494;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advancements in machine learning (ML) have significantly revolutionized medical image analysis, prompting hospitals to rely on external ML services. However, the exchange of sensitive patient data, such as chest X-rays, poses inherent privacy risks when shared with third parties. Addressing this concern, we propose MedBlindTuner, a privacy-preserving framework leveraging fully homomorphic encryption (FHE) and a data-efficient image transformer (DEiT). MedBlindTuner enables the training of ML models exclusively on FHE-encrypted medical images. Our experimental evaluation demonstrates that MedBlindTuner achieves comparable accuracy to models trained on non-encrypted images, offering a secure solution for outsourcing ML computations while preserving patient data privacy. To the best of our knowledge, this is the first work that uses data-efficient image transformers and fully homomorphic encryption in this domain.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LadaGAN&#30340;&#39640;&#25928;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#23427;&#20351;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;Ladaformer&#30340;&#26032;&#22411;Transformer&#22359;&#65292;&#36890;&#36807;&#32447;&#24615;&#21152;&#27861;&#27880;&#24847;&#26426;&#21046;&#26469;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#24182;&#35299;&#20915;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.09596</link><description>&lt;p&gt;
&#20351;&#29992;&#32447;&#24615;&#21152;&#27861;&#27880;&#24847;&#21147;Transformer&#30340;&#39640;&#25928;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Efficient generative adversarial networks using linear additive-attention Transformers. (arXiv:2401.09596v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09596
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LadaGAN&#30340;&#39640;&#25928;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#23427;&#20351;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;Ladaformer&#30340;&#26032;&#22411;Transformer&#22359;&#65292;&#36890;&#36807;&#32447;&#24615;&#21152;&#27861;&#27880;&#24847;&#26426;&#21046;&#26469;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#24182;&#35299;&#20915;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20687;&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#31561;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#30340;&#33021;&#21147;&#36817;&#24180;&#26469;&#24471;&#21040;&#20102;&#26174;&#33879;&#25552;&#39640;&#65292;&#20294;&#26159;&#23427;&#20204;&#30340;&#25104;&#21151;&#24456;&#22823;&#31243;&#24230;&#19978;&#24402;&#21151;&#20110;&#35745;&#31639;&#22797;&#26434;&#30340;&#26550;&#26500;&#12290;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#30740;&#31350;&#23454;&#39564;&#23460;&#21644;&#36164;&#28304;&#20805;&#36275;&#30340;&#20844;&#21496;&#20013;&#30340;&#37319;&#29992;&#21644;&#20351;&#29992;&#65292;&#21516;&#26102;&#20063;&#26497;&#22823;&#22320;&#22686;&#21152;&#20102;&#35757;&#32451;&#12289;&#24494;&#35843;&#21644;&#25512;&#29702;&#30340;&#30899;&#36275;&#36857;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LadaGAN&#65292;&#36825;&#26159;&#19968;&#20010;&#39640;&#25928;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#23427;&#24314;&#31435;&#22312;&#19968;&#31181;&#21517;&#20026;Ladaformer&#30340;&#26032;&#22411;Transformer&#22359;&#19978;&#12290;&#35813;&#22359;&#30340;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#26159;&#19968;&#20010;&#32447;&#24615;&#21152;&#27861;&#27880;&#24847;&#26426;&#21046;&#65292;&#23427;&#27599;&#20010;&#22836;&#37096;&#35745;&#31639;&#19968;&#20010;&#27880;&#24847;&#21521;&#37327;&#65292;&#32780;&#19981;&#26159;&#20108;&#27425;&#30340;&#28857;&#31215;&#27880;&#24847;&#21147;&#12290;&#25105;&#20204;&#22312;&#29983;&#25104;&#22120;&#21644;&#21028;&#21035;&#22120;&#20013;&#37117;&#37319;&#29992;&#20102;Ladaformer&#65292;&#36825;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#20811;&#26381;&#20102;Transformer GAN&#32463;&#24120;&#20986;&#29616;&#30340;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#12290;LadaGAN&#19968;&#30452;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;GANs&#12290;
&lt;/p&gt;
&lt;p&gt;
Although the capacity of deep generative models for image generation, such as Diffusion Models (DMs) and Generative Adversarial Networks (GANs), has dramatically improved in recent years, much of their success can be attributed to computationally expensive architectures. This has limited their adoption and use to research laboratories and companies with large resources, while significantly raising the carbon footprint for training, fine-tuning, and inference. In this work, we present LadaGAN, an efficient generative adversarial network that is built upon a novel Transformer block named Ladaformer. The main component of this block is a linear additive-attention mechanism that computes a single attention vector per head instead of the quadratic dot-product attention. We employ Ladaformer in both the generator and discriminator, which reduces the computational complexity and overcomes the training instabilities often associated with Transformer GANs. LadaGAN consistently outperforms exist
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21452;&#23618;&#20248;&#21270;&#31639;&#27861;BO-REP&#65292;&#29992;&#20110;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#20013;&#23384;&#22312;&#30340;&#26080;&#30028;&#24179;&#28369;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#24402;&#19968;&#21270;&#21160;&#37327;&#26356;&#26032;&#19978;&#23618;&#21464;&#37327;&#24182;&#24341;&#20837;&#21021;&#22987;&#21270;&#32454;&#21270;&#21644;&#21608;&#26399;&#24615;&#26356;&#26032;&#20004;&#31181;&#26032;&#25216;&#26415;&#26469;&#26356;&#26032;&#19979;&#23618;&#21464;&#37327;&#12290;</title><link>http://arxiv.org/abs/2401.09587</link><description>&lt;p&gt;
&#26080;&#30028;&#24179;&#28369;&#32422;&#26463;&#19979;&#30340;&#21452;&#23618;&#20248;&#21270;&#65306;&#19968;&#31181;&#26032;&#31639;&#27861;&#21644;&#25910;&#25947;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Bilevel Optimization under Unbounded Smoothness: A New Algorithm and Convergence Analysis. (arXiv:2401.09587v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09587
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21452;&#23618;&#20248;&#21270;&#31639;&#27861;BO-REP&#65292;&#29992;&#20110;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#20013;&#23384;&#22312;&#30340;&#26080;&#30028;&#24179;&#28369;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#24402;&#19968;&#21270;&#21160;&#37327;&#26356;&#26032;&#19978;&#23618;&#21464;&#37327;&#24182;&#24341;&#20837;&#21021;&#22987;&#21270;&#32454;&#21270;&#21644;&#21608;&#26399;&#24615;&#26356;&#26032;&#20004;&#31181;&#26032;&#25216;&#26415;&#26469;&#26356;&#26032;&#19979;&#23618;&#21464;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21452;&#23618;&#20248;&#21270;&#26159;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#37325;&#35201;&#24418;&#24335;&#12290;&#24403;&#21069;&#30340;&#21452;&#23618;&#20248;&#21270;&#31639;&#27861;&#20551;&#35774;&#19978;&#23618;&#20989;&#25968;&#30340;&#26799;&#24230;&#26159;Lipschitz&#30340;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#26576;&#20123;&#31070;&#32463;&#32593;&#32476;&#22914;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65288;LSTM&#65289;&#23637;&#29616;&#20986;&#28508;&#22312;&#30340;&#26080;&#30028;&#24179;&#28369;&#24615;&#65292;&#20351;&#24471;&#20256;&#32479;&#30340;&#21452;&#23618;&#20248;&#21270;&#31639;&#27861;&#26080;&#27861;&#36866;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#21452;&#23618;&#20248;&#21270;&#31639;&#27861;&#65292;&#31216;&#20026;BO-REP&#65292;&#20197;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#12290;&#35813;&#31639;&#27861;&#20351;&#29992;&#24402;&#19968;&#21270;&#21160;&#37327;&#26356;&#26032;&#19978;&#23618;&#21464;&#37327;&#65292;&#24182;&#37319;&#29992;&#20004;&#31181;&#26032;&#25216;&#26415;&#26469;&#26356;&#26032;&#19979;&#23618;&#21464;&#37327;&#65306;&#21021;&#22987;&#21270;&#32454;&#21270;&#21644;&#21608;&#26399;&#24615;&#26356;&#26032;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#19968;&#26086;&#19978;&#23618;&#21464;&#37327;&#34987;&#21021;&#22987;&#21270;&#65292;&#23558;&#35843;&#29992;&#19968;&#20010;&#23376;&#31243;&#24207;&#26469;&#33719;&#21462;&#30456;&#24212;&#20248;&#21270;&#30340;&#19979;&#23618;&#21464;&#37327;&#30340;&#31934;&#32454;&#20272;&#35745;&#65292;&#24182;&#19988;&#21482;&#22312;&#29305;&#23450;&#21608;&#26399;&#21518;&#26356;&#26032;&#19979;&#23618;&#21464;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bilevel optimization is an important formulation for many machine learning problems. Current bilevel optimization algorithms assume that the gradient of the upper-level function is Lipschitz. However, recent studies reveal that certain neural networks such as recurrent neural networks (RNNs) and long-short-term memory networks (LSTMs) exhibit potential unbounded smoothness, rendering conventional bilevel optimization algorithms unsuitable. In this paper, we design a new bilevel optimization algorithm, namely BO-REP, to address this challenge. This algorithm updates the upper-level variable using normalized momentum and incorporates two novel techniques for updating the lower-level variable: \textit{initialization refinement} and \textit{periodic updates}. Specifically, once the upper-level variable is initialized, a subroutine is invoked to obtain a refined estimate of the corresponding optimal lower-level variable, and the lower-level variable is updated only after every specific peri
&lt;/p&gt;</description></item><item><title>eipy&#26159;&#19968;&#20010;&#24320;&#28304;Python&#21253;&#65292;&#29992;&#20110;&#24320;&#21457;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#25104;&#20998;&#31867;&#27169;&#22411;&#12290;&#23427;&#25552;&#20379;&#20102;&#20005;&#26684;&#21644;&#29992;&#25143;&#21451;&#22909;&#30340;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#23884;&#22871;&#20132;&#21449;&#39564;&#35777;&#26469;&#35780;&#20272;&#24182;&#36873;&#25321;&#26368;&#20339;&#30340;&#38598;&#25104;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.09582</link><description>&lt;p&gt;
eipy: &#19968;&#31181;&#29992;&#20110;&#24322;&#26500;&#38598;&#25104;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#24320;&#28304;Python&#21253;
&lt;/p&gt;
&lt;p&gt;
eipy: An Open-Source Python Package for Multi-modal Data Integration using Heterogeneous Ensembles. (arXiv:2401.09582v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09582
&lt;/p&gt;
&lt;p&gt;
eipy&#26159;&#19968;&#20010;&#24320;&#28304;Python&#21253;&#65292;&#29992;&#20110;&#24320;&#21457;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#25104;&#20998;&#31867;&#27169;&#22411;&#12290;&#23427;&#25552;&#20379;&#20102;&#20005;&#26684;&#21644;&#29992;&#25143;&#21451;&#22909;&#30340;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#23884;&#22871;&#20132;&#21449;&#39564;&#35777;&#26469;&#35780;&#20272;&#24182;&#36873;&#25321;&#26368;&#20339;&#30340;&#38598;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;eipy&#65292;&#19968;&#31181;&#29992;&#20110;&#24320;&#21457;&#26377;&#25928;&#30340;&#22810;&#27169;&#24577;&#24322;&#26500;&#38598;&#25104;&#20998;&#31867;&#30340;&#24320;&#28304;Python&#21253;&#12290;eipy&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#20010;&#20005;&#26684;&#32780;&#29992;&#25143;&#21451;&#22909;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#31995;&#32479;&#35780;&#20272;&#23427;&#20204;&#22312;&#23884;&#22871;&#20132;&#21449;&#39564;&#35777;&#20013;&#30340;&#24615;&#33021;&#26469;&#27604;&#36739;&#21644;&#36873;&#25321;&#26368;&#20339;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#25104;&#21644;&#39044;&#27979;&#24314;&#27169;&#26041;&#27861;&#12290;&#35813;&#21253;&#26088;&#22312;&#21033;&#29992;&#31867;&#20284;scikit-learn&#30340;&#20272;&#35745;&#22120;&#20316;&#20026;&#32452;&#20214;&#26469;&#26500;&#24314;&#22810;&#27169;&#24577;&#39044;&#27979;&#27169;&#22411;&#12290;eipy&#30340;&#26368;&#26032;&#29992;&#25143;&#25351;&#21335;&#65292;&#21253;&#25324;API&#21442;&#32771;&#21644;&#25945;&#31243;&#65292;&#35831;&#21442;&#38405;https://eipy.readthedocs.io&#12290;&#35813;&#39033;&#30446;&#30340;&#20027;&#35201;&#23384;&#20648;&#24211;&#20301;&#20110;GitHub&#19978;&#30340;https://github.com/GauravPandeyLab/eipy&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce eipy--an open-source Python package for developing effective, multi-modal heterogeneous ensembles for classification. eipy simultaneously provides both a rigorous, and user-friendly framework for comparing and selecting the best-performing multi-modal data integration and predictive modeling methods by systematically evaluating their performance using nested cross-validation. The package is designed to leverage scikit-learn-like estimators as components to build multi-modal predictive models. An up-to-date user guide, including API reference and tutorials, for eipy is maintained at https://eipy.readthedocs.io . The main repository for this project can be found on GitHub at https://github.com/GauravPandeyLab/eipy .
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#20110;112 Gbit/s&#34987;&#21160;&#20809;&#32593;&#32476;&#20013;&#20005;&#37325;&#38750;&#32447;&#24615;&#22833;&#30495;&#30340;&#20840;&#30450;&#31070;&#32463;&#32593;&#32476;&#22343;&#34913;&#22120;&#65292;&#24182;&#35780;&#20272;&#20102;&#20854;&#24615;&#33021;&#12290;&#36825;&#31181;&#22343;&#34913;&#22120;&#37319;&#29992;&#20302;&#30828;&#20214;&#22797;&#26434;&#24230;&#30340;&#31070;&#32463;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2401.09579</link><description>&lt;p&gt;
&#29992;&#20110;112 Gbit/s&#34987;&#21160;&#20809;&#32593;&#32476;&#20013;&#20005;&#37325;&#38750;&#32447;&#24615;&#22833;&#30495;&#30340;&#20840;&#30450;&#31070;&#32463;&#32593;&#32476;&#22343;&#34913;&#22120;
&lt;/p&gt;
&lt;p&gt;
Fully-blind Neural Network Based Equalization for Severe Nonlinear Distortions in 112 Gbit/s Passive Optical Networks. (arXiv:2401.09579v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09579
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#20110;112 Gbit/s&#34987;&#21160;&#20809;&#32593;&#32476;&#20013;&#20005;&#37325;&#38750;&#32447;&#24615;&#22833;&#30495;&#30340;&#20840;&#30450;&#31070;&#32463;&#32593;&#32476;&#22343;&#34913;&#22120;&#65292;&#24182;&#35780;&#20272;&#20102;&#20854;&#24615;&#33021;&#12290;&#36825;&#31181;&#22343;&#34913;&#22120;&#37319;&#29992;&#20302;&#30828;&#20214;&#22797;&#26434;&#24230;&#30340;&#31070;&#32463;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#24182;&#35780;&#20272;&#20102;&#29992;&#20110;100G&#34987;&#21160;&#20809;&#32593;&#32476;(PON)&#30340;&#23436;&#20840;&#30450;&#30446;&#30340;&#25968;&#23383;&#20449;&#21495;&#22788;&#29702;(DSP)&#38142;&#65292;&#20197;&#21450;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#19981;&#21516;&#22343;&#34913;&#22120;&#25299;&#25169;&#32467;&#26500;&#65292;&#20855;&#26377;&#20302;&#30828;&#20214;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We demonstrate and evaluate a fully-blind digital signal processing (DSP) chain for 100G passive optical networks (PONs), and analyze different equalizer topologies based on neural networks with low hardware complexity.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#19981;&#33719;&#21462;&#26032;&#30340;&#35757;&#32451;&#25968;&#25454;&#25110;&#26356;&#25913;&#27169;&#22411;&#26550;&#26500;&#30340;&#24773;&#20917;&#19979;&#65292;&#29983;&#25104;&#20855;&#26377;&#19981;&#21516;&#25915;&#20987;&#23646;&#24615;&#30340;&#22810;&#20010;&#27169;&#22411;&#29256;&#26412;&#30340;&#21487;&#34892;&#24615;&#65292;&#20197;&#20445;&#25252;&#27169;&#22411;&#25152;&#26377;&#32773;&#20813;&#21463;&#24694;&#24847;&#20837;&#20405;&#24102;&#26469;&#30340;&#25439;&#22833;&#12290;</title><link>http://arxiv.org/abs/2401.09574</link><description>&lt;p&gt;
&#23454;&#29616;&#21487;&#25193;&#23637;&#21644;&#31283;&#20581;&#30340;&#27169;&#22411;&#29256;&#26412;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Towards Scalable and Robust Model Versioning. (arXiv:2401.09574v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09574
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#19981;&#33719;&#21462;&#26032;&#30340;&#35757;&#32451;&#25968;&#25454;&#25110;&#26356;&#25913;&#27169;&#22411;&#26550;&#26500;&#30340;&#24773;&#20917;&#19979;&#65292;&#29983;&#25104;&#20855;&#26377;&#19981;&#21516;&#25915;&#20987;&#23646;&#24615;&#30340;&#22810;&#20010;&#27169;&#22411;&#29256;&#26412;&#30340;&#21487;&#34892;&#24615;&#65292;&#20197;&#20445;&#25252;&#27169;&#22411;&#25152;&#26377;&#32773;&#20813;&#21463;&#24694;&#24847;&#20837;&#20405;&#24102;&#26469;&#30340;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#21508;&#34892;&#21508;&#19994;&#30340;&#19981;&#26029;&#37096;&#32626;&#65292;&#38024;&#23545;&#36825;&#20123;&#37096;&#32626;&#27169;&#22411;&#36827;&#34892;&#24694;&#24847;&#20837;&#20405;&#30340;&#23041;&#32961;&#20063;&#22312;&#22686;&#21152;&#12290;&#22914;&#26524;&#25915;&#20987;&#32773;&#33021;&#22815;&#36890;&#36807;&#26381;&#21153;&#22120;&#20837;&#20405;&#12289;&#20869;&#37096;&#25915;&#20987;&#25110;&#27169;&#22411;&#21453;&#36716;&#25216;&#26415;&#33719;&#21462;&#37096;&#32626;&#27169;&#22411;&#30340;&#35775;&#38382;&#26435;&#38480;&#65292;&#20182;&#20204;&#21487;&#20197;&#26500;&#36896;&#30333;&#30418;&#23545;&#25239;&#25915;&#20987;&#26469;&#25805;&#32437;&#27169;&#22411;&#30340;&#20998;&#31867;&#32467;&#26524;&#65292;&#20174;&#32780;&#32473;&#20381;&#36182;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#20851;&#38190;&#20219;&#21153;&#30340;&#32452;&#32455;&#24102;&#26469;&#37325;&#22823;&#39118;&#38505;&#12290;&#27169;&#22411;&#25152;&#26377;&#32773;&#38656;&#35201;&#19968;&#31181;&#26426;&#21046;&#26469;&#20445;&#25252;&#33258;&#24049;&#20813;&#21463;&#36825;&#31181;&#25439;&#22833;&#65292;&#32780;&#19981;&#38656;&#35201;&#33719;&#21462;&#26032;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#36825;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#26102;&#38388;&#21644;&#36164;&#26412;&#25237;&#20837;&#12290;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#19981;&#33719;&#21462;&#26032;&#30340;&#35757;&#32451;&#25968;&#25454;&#25110;&#26356;&#25913;&#27169;&#22411;&#26550;&#26500;&#30340;&#24773;&#20917;&#19979;&#65292;&#29983;&#25104;&#20855;&#26377;&#19981;&#21516;&#25915;&#20987;&#23646;&#24615;&#30340;&#22810;&#20010;&#27169;&#22411;&#29256;&#26412;&#30340;&#21487;&#34892;&#24615;&#12290;&#27169;&#22411;&#25152;&#26377;&#32773;&#21487;&#20197;&#36880;&#20010;&#37096;&#32626;&#29256;&#26412;&#24182;&#31435;&#21363;&#26367;&#25442;&#27844;&#38706;&#30340;&#29256;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the deployment of deep learning models continues to expand across industries, the threat of malicious incursions aimed at gaining access to these deployed models is on the rise. Should an attacker gain access to a deployed model, whether through server breaches, insider attacks, or model inversion techniques, they can then construct white-box adversarial attacks to manipulate the model's classification outcomes, thereby posing significant risks to organizations that rely on these models for critical tasks. Model owners need mechanisms to protect themselves against such losses without the necessity of acquiring fresh training data - a process that typically demands substantial investments in time and capital.  In this paper, we explore the feasibility of generating multiple versions of a model that possess different attack properties, without acquiring new training data or changing model architecture. The model owner can deploy one version at a time and replace a leaked version immed
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#20013;&#20998;&#20139;&#34920;&#31034;&#30340;&#30410;&#22788;&#65292;&#24182;&#25552;&#20379;&#29702;&#35770;&#20445;&#35777;&#21644;&#23454;&#39564;&#35780;&#20272;&#32467;&#26524;&#65292;&#34920;&#26126;&#22312;&#22810;&#20219;&#21153;&#35774;&#32622;&#20013;&#20998;&#20139;&#34920;&#31034;&#21487;&#20197;&#25913;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.09561</link><description>&lt;p&gt;
&#22312;&#22810;&#20219;&#21153;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#20998;&#20139;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Sharing Knowledge in Multi-Task Deep Reinforcement Learning. (arXiv:2401.09561v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09561
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#20013;&#20998;&#20139;&#34920;&#31034;&#30340;&#30410;&#22788;&#65292;&#24182;&#25552;&#20379;&#29702;&#35770;&#20445;&#35777;&#21644;&#23454;&#39564;&#35780;&#20272;&#32467;&#26524;&#65292;&#34920;&#26126;&#22312;&#22810;&#20219;&#21153;&#35774;&#32622;&#20013;&#20998;&#20139;&#34920;&#31034;&#21487;&#20197;&#25913;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#20013;&#20998;&#20139;&#34920;&#31034;&#20197;&#23454;&#29616;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#25928;&#24212;&#29992;&#30340;&#30410;&#22788;&#12290;&#25105;&#20204;&#21033;&#29992;&#20102;&#23398;&#20064;&#19981;&#21516;&#20219;&#21153;&#65292;&#24182;&#20998;&#20139;&#20849;&#21516;&#23646;&#24615;&#30340;&#20551;&#35774;&#65292;&#26377;&#21161;&#20110;&#23558;&#23427;&#20204;&#30340;&#30693;&#35782;&#25512;&#24191;&#65292;&#20174;&#32780;&#33719;&#24471;&#27604;&#21333;&#20010;&#20219;&#21153;&#23398;&#20064;&#26356;&#26377;&#25928;&#30340;&#29305;&#24449;&#25552;&#21462;&#12290;&#30452;&#35266;&#22320;&#35828;&#65292;&#25152;&#24471;&#21040;&#30340;&#29305;&#24449;&#38598;&#22312;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20351;&#29992;&#26102;&#21487;&#20197;&#24102;&#26469;&#24615;&#33021;&#26041;&#38754;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#29702;&#35770;&#20445;&#35777;&#26469;&#35777;&#26126;&#36825;&#19968;&#28857;&#65292;&#36825;&#20123;&#29702;&#35770;&#20445;&#35777;&#24378;&#35843;&#20102;&#22312;&#20309;&#31181;&#26465;&#20214;&#19979;&#20998;&#20139;&#20219;&#21153;&#20043;&#38388;&#30340;&#34920;&#31034;&#26159;&#26041;&#20415;&#30340;&#65292;&#24182;&#23558;&#36817;&#20284;&#20540;&#36845;&#20195;&#30340;&#24050;&#30693;&#26377;&#38480;&#26102;&#38388;&#30028;&#25193;&#23637;&#21040;&#22810;&#20219;&#21153;&#35774;&#32622;&#20013;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19977;&#31181;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#22810;&#20219;&#21153;&#25193;&#23637;&#26469;&#34917;&#20805;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#24182;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#19978;&#36827;&#34892;&#32463;&#39564;&#35780;&#20272;&#65292;&#34920;&#26126;&#19982;&#21333;&#20219;&#21153;&#23545;&#27604;&#30456;&#27604;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the benefit of sharing representations among tasks to enable the effective use of deep neural networks in Multi-Task Reinforcement Learning. We leverage the assumption that learning from different tasks, sharing common properties, is helpful to generalize the knowledge of them resulting in a more effective feature extraction compared to learning a single task. Intuitively, the resulting set of features offers performance benefits when used by Reinforcement Learning algorithms. We prove this by providing theoretical guarantees that highlight the conditions for which is convenient to share representations among tasks, extending the well-known finite-time bounds of Approximate Value-Iteration to the multi-task setting. In addition, we complement our analysis by proposing multi-task extensions of three Reinforcement Learning algorithms that we empirically evaluate on widely used Reinforcement Learning benchmarks showing significant improvements over the single-task counterparts in
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#35299;&#20915;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26469;&#39044;&#27979;&#27963;&#21160;&#32500;&#24230;&#65292;&#20174;&#32780;&#26368;&#22823;&#21270;&#20840;&#23616;&#26368;&#20248;&#35299;&#30340;&#20986;&#29616;&#39057;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.09556</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22686;&#24378;&#30340;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;: &#23398;&#20064;&#20943;&#23569;&#27169;&#22411;&#32500;&#24230;
&lt;/p&gt;
&lt;p&gt;
Deep learning enhanced mixed integer optimization: Learning to reduce model dimensionality. (arXiv:2401.09556v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09556
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#35299;&#20915;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26469;&#39044;&#27979;&#27963;&#21160;&#32500;&#24230;&#65292;&#20174;&#32780;&#26368;&#22823;&#21270;&#20840;&#23616;&#26368;&#20248;&#35299;&#30340;&#20986;&#29616;&#39057;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#35299;&#20915;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#27169;&#22411;&#20013;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;(ANN)&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#22312;&#36817;&#20284;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#38382;&#39064;&#20013;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#21033;&#29992;&#22810;&#26631;&#31614;&#20998;&#31867;&#26469;&#32771;&#34385;&#22810;&#20010;&#27963;&#21160;&#32500;&#24230;&#12290;&#20026;&#20102;&#25552;&#39640;&#26694;&#26550;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#37319;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#36827;&#34892;&#36229;&#21442;&#25968;&#35843;&#20248;&#65292;&#20197;&#26368;&#22823;&#21270;&#26679;&#26412;&#32423;&#20934;&#30830;&#24615;&#12290;&#20027;&#35201;&#30446;&#26631;&#26159;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20934;&#30830;&#22320;&#39044;&#27979;&#25152;&#26377;&#27963;&#21160;&#32500;&#24230;&#65292;&#20174;&#32780;&#26368;&#22823;&#21270;&#20840;&#23616;&#26368;&#20248;&#35299;&#30340;&#20986;&#29616;&#39057;&#29575;&#12290;&#25105;&#20204;&#23558;&#35813;&#26694;&#26550;&#24212;&#29992;&#20110;&#25551;&#36848;&#20010;&#24615;&#21270;&#21307;&#23398;&#20379;&#24212;&#38142;&#20013;&#30340;&#38271;&#26399;&#25237;&#36164;&#35268;&#21010;&#21644;&#20013;&#26399;&#25112;&#26415;&#35268;&#21010;&#30340;&#22522;&#20110;&#27969;&#30340;&#35774;&#26045;&#20301;&#32622;&#20998;&#37197;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;(MILP)&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work introduces a framework to address the computational complexity inherent in Mixed-Integer Programming (MIP) models by harnessing the potential of deep learning. We compare the effectiveness of (a) feed-forward neural networks (ANN) and (b) convolutional neural networks (CNN) in approximating the active dimensions within MIP problems. We utilize multi-label classification to account for more than one active dimension. To enhance the framework's performance, we employ Bayesian optimization for hyperparameter tuning, aiming to maximize sample-level accuracy. The primary objective is to train the neural networks to predict all active dimensions accurately, thereby maximizing the occurrence of global optimum solutions. We apply this framework to a flow-based facility location allocation Mixed-Integer Linear Programming (MILP) formulation that describes long-term investment planning and medium-term tactical planning in a personalized medicine supply chain for cell therapy manufactur
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#26469;&#25913;&#36827;&#20998;&#31867;&#27169;&#22411;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#20351;&#29992;&#23569;&#37327;&#26377;&#26631;&#31614;&#31034;&#20363;&#65292;&#36890;&#36807;&#36830;&#32493;&#21453;&#39304;&#24490;&#29615;&#65292;&#25105;&#20204;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#36229;&#36234;&#38646;&#26679;&#26412;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20379;&#26356;&#24378;&#30340;&#25991;&#26412;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.09555</link><description>&lt;p&gt;
&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#25913;&#36827;&#20998;&#31867;&#24615;&#33021;&#65306;&#26631;&#35760;&#19968;&#20123;&#65292;&#25105;&#20204;&#26631;&#35760;&#20854;&#20313;&#37096;&#20998;
&lt;/p&gt;
&lt;p&gt;
Improving Classification Performance With Human Feedback: Label a few, we label the rest. (arXiv:2401.09555v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09555
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#26469;&#25913;&#36827;&#20998;&#31867;&#27169;&#22411;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#20351;&#29992;&#23569;&#37327;&#26377;&#26631;&#31614;&#31034;&#20363;&#65292;&#36890;&#36807;&#36830;&#32493;&#21453;&#39304;&#24490;&#29615;&#65292;&#25105;&#20204;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#36229;&#36234;&#38646;&#26679;&#26412;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20379;&#26356;&#24378;&#30340;&#25991;&#26412;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#22823;&#37096;&#20998;&#25968;&#25454;&#26159;&#38750;&#32467;&#26500;&#21270;&#30340;&#65292;&#22240;&#27492;&#33719;&#21462;&#36275;&#22815;&#30340;&#26377;&#26631;&#31614;&#25968;&#25454;&#26469;&#35757;&#32451;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#23569;&#26679;&#26412;&#23398;&#20064;&#21644;&#20027;&#21160;&#23398;&#20064;&#65292;&#21363;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#26469;&#25913;&#36827;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#20165;&#20351;&#29992;&#19968;&#23567;&#37096;&#20998;&#26377;&#26631;&#31614;&#31034;&#20363;&#12290;&#26412;&#25991;&#30528;&#37325;&#20110;&#29702;&#35299;&#36830;&#32493;&#21453;&#39304;&#24490;&#29615;&#22914;&#20309;&#25913;&#21892;&#27169;&#22411;&#65292;&#20174;&#32780;&#36890;&#36807;&#28176;&#36827;&#24335;&#30340;&#20154;&#31867;&#21442;&#19982;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12289;&#22238;&#24402;&#21644;&#31934;&#30830;&#24230;&#12290;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;GPT-3.5&#12289;BERT&#21644;SetFit&#65292;&#25105;&#20204;&#26088;&#22312;&#20998;&#26512;&#20351;&#29992;&#26377;&#38480;&#25968;&#37327;&#30340;&#26377;&#26631;&#31614;&#31034;&#20363;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#22312;Financial Phrasebank&#12289;Banking&#12289;Craigslist&#12289;Trec&#21644;Amazon Reviews&#25968;&#25454;&#38598;&#19978;&#23545;&#27492;&#26041;&#27861;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#35777;&#26126;&#20165;&#20351;&#29992;&#23569;&#37327;&#26377;&#26631;&#31614;&#31034;&#20363;&#23601;&#33021;&#36229;&#36234;&#38646;&#26679;&#26412;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#25552;&#20379;&#22686;&#24378;&#30340;&#25991;&#26412;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the realm of artificial intelligence, where a vast majority of data is unstructured, obtaining substantial amounts of labeled data to train supervised machine learning models poses a significant challenge. To address this, we delve into few-shot and active learning, where are goal is to improve AI models with human feedback on a few labeled examples. This paper focuses on understanding how a continuous feedback loop can refine models, thereby enhancing their accuracy, recall, and precision through incremental human input. By employing Large Language Models (LLMs) such as GPT-3.5, BERT, and SetFit, we aim to analyze the efficacy of using a limited number of labeled examples to substantially improve model accuracy. We benchmark this approach on the Financial Phrasebank, Banking, Craigslist, Trec, Amazon Reviews datasets to prove that with just a few labeled examples, we are able to surpass the accuracy of zero shot large language models to provide enhanced text classification performa
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#21644;&#22810;&#27169;&#24577;MRI&#25581;&#31034;&#20102;&#31070;&#32463;&#31934;&#31070;&#30142;&#30149;&#21644;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#30340;&#30142;&#30149;&#24322;&#36136;&#24615;&#65292;&#20026;&#25105;&#20204;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#20123;&#30142;&#30149;&#20570;&#20986;&#20102;&#36129;&#29486;</title><link>http://arxiv.org/abs/2401.09517</link><description>&lt;p&gt;
&#31070;&#32463;&#24433;&#20687;&#23398;&#30340;&#32500;&#24230;&#20869;&#37096;&#34920;&#29616;&#65306;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25581;&#31034;&#30142;&#30149;&#24322;&#36136;&#24615;&#30340;&#31070;&#32463;&#29983;&#29289;&#23398;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Dimensional Neuroimaging Endophenotypes: Neurobiological Representations of Disease Heterogeneity Through Machine Learning. (arXiv:2401.09517v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09517
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#21644;&#22810;&#27169;&#24577;MRI&#25581;&#31034;&#20102;&#31070;&#32463;&#31934;&#31070;&#30142;&#30149;&#21644;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#30340;&#30142;&#30149;&#24322;&#36136;&#24615;&#65292;&#20026;&#25105;&#20204;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#20123;&#30142;&#30149;&#20570;&#20986;&#20102;&#36129;&#29486;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20110;&#33719;&#21462;&#31070;&#32463;&#24433;&#20687;&#23398;&#29305;&#24449;&#65292;&#20197;&#23545;&#31070;&#32463;&#31934;&#31070;&#30142;&#30149;&#21644;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#30340;&#35786;&#26029;&#12289;&#39044;&#21518;&#21644;&#27835;&#30103;&#21453;&#24212;&#36827;&#34892;&#20010;&#20307;&#21270;&#12290;&#22240;&#27492;&#65292;&#23427;&#36890;&#36807;&#35782;&#21035;&#22312;&#21508;&#31181;&#22823;&#33041;&#34920;&#22411;&#27979;&#37327;&#19978;&#23384;&#22312;&#26174;&#30528;&#24046;&#24322;&#30340;&#30142;&#30149;&#20122;&#22411;&#65292;&#20026;&#25105;&#20204;&#26356;&#22909;&#22320;&#29702;&#35299;&#30142;&#30149;&#24322;&#36136;&#24615;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#22312;&#36825;&#31687;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20379;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#22810;&#27169;&#24577;MRI&#25581;&#31034;&#21508;&#31181;&#31070;&#32463;&#31934;&#31070;&#30142;&#30149;&#21644;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#20013;&#30340;&#30142;&#30149;&#24322;&#36136;&#24615;&#30340;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#65292;&#21253;&#25324;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#12289;&#31934;&#31070;&#20998;&#35010;&#30151;&#12289;&#37325;&#24615;&#25233;&#37057;&#38556;&#30861;&#12289;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#12289;&#22810;&#21457;&#24615;&#30828;&#21270;&#30151;&#65292;&#20197;&#21450;&#23427;&#20204;&#22312;&#27178;&#26029;&#38754;&#30340;&#28508;&#21147;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#30456;&#20851;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#19968;&#20010;&#26032;&#20852;&#30340;&#33539;&#24335;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#31070;&#32463;&#24433;&#20687;&#23398;&#30340;&#32500;&#24230;&#20869;&#37096;&#34920;&#29616;&#65288;DNE&#65289;&#12290;DNE&#20998;&#35299;&#20102;&#31070;&#32463;&#31185;&#23398;&#30340;&#24322;&#36136;&#24615;
&lt;/p&gt;
&lt;p&gt;
Machine learning has been increasingly used to obtain individualized neuroimaging signatures for disease diagnosis, prognosis, and response to treatment in neuropsychiatric and neurodegenerative disorders. Therefore, it has contributed to a better understanding of disease heterogeneity by identifying disease subtypes that present significant differences in various brain phenotypic measures. In this review, we first present a systematic literature overview of studies using machine learning and multimodal MRI to unravel disease heterogeneity in various neuropsychiatric and neurodegenerative disorders, including Alzheimer disease, schizophrenia, major depressive disorder, autism spectrum disorder, multiple sclerosis, as well as their potential in transdiagnostic settings. Subsequently, we summarize relevant machine learning methodologies and discuss an emerging paradigm which we call dimensional neuroimaging endophenotype (DNE). DNE dissects the neurobiological heterogeneity of neuropsych
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25490;&#24207;&#20811;&#37324;&#27931;&#22827;&#22238;&#25910;&#65288;SKR&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#21152;&#36895;&#31070;&#32463;&#31639;&#23376;&#35757;&#32451;&#30340;&#25968;&#25454;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#35299;&#20915;PDE&#38382;&#39064;&#26102;&#35745;&#31639;&#20887;&#20313;&#30340;&#38382;&#39064;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#25454;&#29983;&#25104;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.09516</link><description>&lt;p&gt;
&#36890;&#36807;&#20811;&#37324;&#27931;&#22827;&#23376;&#31354;&#38388;&#22238;&#25910;&#21152;&#36895;&#31070;&#32463;&#31639;&#23376;&#30340;&#25968;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Accelerating Data Generation for Neural Operators via Krylov Subspace Recycling. (arXiv:2401.09516v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09516
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25490;&#24207;&#20811;&#37324;&#27931;&#22827;&#22238;&#25910;&#65288;SKR&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#21152;&#36895;&#31070;&#32463;&#31639;&#23376;&#35757;&#32451;&#30340;&#25968;&#25454;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#35299;&#20915;PDE&#38382;&#39064;&#26102;&#35745;&#31639;&#20887;&#20313;&#30340;&#38382;&#39064;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#25454;&#29983;&#25104;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#29992;&#20110;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;(PDE)&#30340;&#31070;&#32463;&#31639;&#23376;&#22240;&#20854;&#39640;&#25512;&#29702;&#25928;&#29575;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#36825;&#20123;&#31639;&#23376;&#38656;&#35201;&#29983;&#25104;&#22823;&#37327;&#24102;&#26377;&#35299;&#20915;&#26041;&#26696;&#30340;&#26631;&#35760;&#25968;&#25454;&#65292;&#21363;PDE&#38382;&#39064;&#21450;&#20854;&#35299;&#20915;&#26041;&#26696;&#12290;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#38750;&#24120;&#32791;&#26102;&#65292;&#22240;&#20026;&#23427;&#28041;&#21450;&#35299;&#20915;&#22823;&#37327;&#32447;&#24615;&#26041;&#31243;&#32452;&#20197;&#33719;&#24471;PDE&#30340;&#25968;&#20540;&#35299;&#12290;&#35768;&#22810;&#29616;&#26377;&#26041;&#27861;&#29420;&#31435;&#22320;&#35299;&#20915;&#36825;&#20123;&#31995;&#32479;&#65292;&#32780;&#19981;&#32771;&#34385;&#23427;&#20204;&#30340;&#20869;&#22312;&#30456;&#20284;&#24615;&#65292;&#23548;&#33268;&#35745;&#31639;&#26497;&#20854;&#20887;&#20313;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#25490;&#24207;&#20811;&#37324;&#27931;&#22827;&#22238;&#25910;(SKR)&#65292;&#20197;&#25552;&#39640;&#35299;&#20915;&#36825;&#20123;&#31995;&#32479;&#30340;&#25928;&#29575;&#65292;&#20174;&#32780;&#26174;&#33879;&#21152;&#36895;&#31070;&#32463;&#31639;&#23376;&#35757;&#32451;&#30340;&#25968;&#25454;&#29983;&#25104;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;SKR&#26159;&#31532;&#19968;&#20010;&#35299;&#20915;&#23398;&#20064;&#31070;&#32463;&#31639;&#23376;&#25968;&#25454;&#29983;&#25104;&#32791;&#26102;&#24615;&#36136;&#30340;&#23581;&#35797;&#12290;SKR&#30340;&#26680;&#24515;&#26159;&#20811;&#37324;&#27931;&#22827;&#23376;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning neural operators for solving partial differential equations (PDEs) has attracted great attention due to its high inference efficiency. However, training such operators requires generating a substantial amount of labeled data, i.e., PDE problems together with their solutions. The data generation process is exceptionally time-consuming, as it involves solving numerous systems of linear equations to obtain numerical solutions to the PDEs. Many existing methods solve these systems independently without considering their inherent similarities, resulting in extremely redundant computations. To tackle this problem, we propose a novel method, namely Sorting Krylov Recycling (SKR), to boost the efficiency of solving these systems, thus significantly accelerating data generation for neural operators training. To the best of our knowledge, SKR is the first attempt to address the time-consuming nature of data generation for learning neural operators. The working horse of SKR is Krylov sub
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;Hough&#21464;&#25442;&#21644;&#35821;&#20041;&#32447;&#26816;&#27979;&#19982;&#20998;&#31867;&#26469;&#25552;&#39640;&#30417;&#25511;&#25668;&#20687;&#26426;&#30340;&#35270;&#37326;&#36136;&#37327;&#65292;&#20174;&#32780;&#30830;&#20445;&#36866;&#24403;&#30340;FOV&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#35780;&#20272;&#35270;&#39057;&#21644;&#22270;&#20687;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.09515</link><description>&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;Hough&#21464;&#25442;&#21644;&#35821;&#20041;&#32447;&#26816;&#27979;&#19982;&#20998;&#31867;&#25552;&#39640;&#30417;&#25511;&#25668;&#20687;&#26426;&#35270;&#37326;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
Enhancing Surveillance Camera FOV Quality via Semantic Line Detection and Classification with Deep Hough Transform. (arXiv:2401.09515v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09515
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;Hough&#21464;&#25442;&#21644;&#35821;&#20041;&#32447;&#26816;&#27979;&#19982;&#20998;&#31867;&#26469;&#25552;&#39640;&#30417;&#25511;&#25668;&#20687;&#26426;&#30340;&#35270;&#37326;&#36136;&#37327;&#65292;&#20174;&#32780;&#30830;&#20445;&#36866;&#24403;&#30340;FOV&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#35780;&#20272;&#35270;&#39057;&#21644;&#22270;&#20687;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25293;&#25668;&#30340;&#35270;&#39057;&#21644;&#22270;&#20687;&#30340;&#36136;&#37327;&#21463;&#21040;&#25668;&#20687;&#26426;&#30340;&#35270;&#37326;&#33539;&#22260;&#65288;FOV&#65289;&#30340;&#26174;&#33879;&#24433;&#21709;&#12290;&#22312;&#30417;&#25511;&#31995;&#32479;&#21644;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#31561;&#20851;&#38190;&#24212;&#29992;&#20013;&#65292;&#19981;&#36866;&#24403;&#30340;FOV&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#30340;&#23433;&#20840;&#38382;&#39064;&#65292;&#21253;&#25324;&#30001;&#20110;&#26410;&#33021;&#26816;&#27979;&#21040;&#20010;&#20307;&#21644;&#29289;&#20307;&#32780;&#36896;&#25104;&#30340;&#36710;&#36742;&#20107;&#25925;&#21644;&#31363;&#30423;&#12290;&#20256;&#32479;&#30340;&#30830;&#23450;&#27491;&#30830;FOV&#30340;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#20110;&#20154;&#31867;&#21028;&#26029;&#65292;&#32570;&#20047;&#22522;&#20110;FOV&#35780;&#20272;&#35270;&#39057;&#21644;&#22270;&#20687;&#36136;&#37327;&#30340;&#33258;&#21160;&#26426;&#21046;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#28145;&#24230;Hough&#21464;&#25442;&#21644;&#35821;&#20041;&#32447;&#26816;&#27979;&#19982;&#20998;&#31867;&#26469;&#35782;&#21035;&#35821;&#20041;&#32447;&#65292;&#20174;&#32780;&#36890;&#36807;&#29702;&#35299;&#24179;&#34892;&#32447;&#30340;&#19977;&#32500;&#35270;&#22270;&#26469;&#30830;&#20445;&#21512;&#36866;&#30340;FOV&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20844;&#24320;&#30340;EgoCart&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;0.729&#30340;&#26377;&#25928;F1&#24471;&#20998;&#65292;&#24182;&#22312;&#32447;&#26465;&#25918;&#32622;&#24230;&#37327;&#20013;&#24471;&#21040;&#20102;&#26174;&#33879;&#39640;&#30340;&#20013;&#20301;&#25968;&#24471;&#20998;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#35780;&#20272;&#35270;&#39057;&#21644;&#22270;&#20687;&#36136;&#37327;&#30340;&#25163;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
The quality of recorded videos and images is significantly influenced by the camera's field of view (FOV). In critical applications like surveillance systems and self-driving cars, an inadequate FOV can give rise to severe safety and security concerns, including car accidents and thefts due to the failure to detect individuals and objects. The conventional methods for establishing the correct FOV heavily rely on human judgment and lack automated mechanisms to assess video and image quality based on FOV. In this paper, we introduce an innovative approach that harnesses semantic line detection and classification alongside deep Hough transform to identify semantic lines, thus ensuring a suitable FOV by understanding 3D view through parallel lines. Our approach yields an effective F1 score of 0.729 on the public EgoCart dataset, coupled with a notably high median score in the line placement metric. We illustrate that our method offers a straightforward means of assessing the quality of the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22810;&#35270;&#22270;&#38543;&#26426;&#22359;&#27169;&#22411;&#65288;MVSBM&#65289;&#65292;&#20197;&#20449;&#24687;&#29702;&#35770;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#22312;&#22810;&#20010;&#30456;&#20851;&#22270;&#19978;&#36827;&#34892;&#30340;&#31038;&#21306;&#26816;&#27979;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#27169;&#22411;&#21442;&#25968;&#36229;&#36807;&#38408;&#20540;&#26102;&#21487;&#20197;&#23454;&#29616;&#31934;&#30830;&#24674;&#22797;&#31038;&#21306;&#65292;&#32780;&#24403;&#21442;&#25968;&#20302;&#20110;&#38408;&#20540;&#26102;&#26080;&#27861;&#36798;&#21040;&#31934;&#30830;&#24674;&#22797;&#31038;&#21306;&#30340;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2401.09510</link><description>&lt;p&gt;
&#22810;&#35270;&#22270;&#38543;&#26426;&#22359;&#27169;&#22411;&#20013;&#30340;&#31038;&#21306;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Community Detection in the Multi-View Stochastic Block Model. (arXiv:2401.09510v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09510
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22810;&#35270;&#22270;&#38543;&#26426;&#22359;&#27169;&#22411;&#65288;MVSBM&#65289;&#65292;&#20197;&#20449;&#24687;&#29702;&#35770;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#22312;&#22810;&#20010;&#30456;&#20851;&#22270;&#19978;&#36827;&#34892;&#30340;&#31038;&#21306;&#26816;&#27979;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#27169;&#22411;&#21442;&#25968;&#36229;&#36807;&#38408;&#20540;&#26102;&#21487;&#20197;&#23454;&#29616;&#31934;&#30830;&#24674;&#22797;&#31038;&#21306;&#65292;&#32780;&#24403;&#21442;&#25968;&#20302;&#20110;&#38408;&#20540;&#26102;&#26080;&#27861;&#36798;&#21040;&#31934;&#30830;&#24674;&#22797;&#31038;&#21306;&#30340;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#20449;&#24687;&#29702;&#35770;&#30340;&#35282;&#24230;&#32771;&#34385;&#20102;&#22522;&#20110;&#22810;&#20010;&#21487;&#33021;&#30456;&#20851;&#30340;&#22270;&#30340;&#31038;&#21306;&#26816;&#27979;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#38543;&#26426;&#22270;&#27169;&#22411;&#65292;&#31216;&#20026;&#22810;&#35270;&#22270;&#38543;&#26426;&#22359;&#27169;&#22411;&#65288;MVSBM&#65289;&#65292;&#29992;&#20110;&#22312;&#30456;&#21516;&#30340;&#33410;&#28857;&#38598;&#19978;&#29983;&#25104;&#30456;&#20851;&#30340;&#22270;&#65288;&#33410;&#28857;&#20010;&#25968;&#20026;n&#65289;&#12290;&#36825;n&#20010;&#33410;&#28857;&#34987;&#21010;&#20998;&#20026;&#20004;&#20010;&#30456;&#31561;&#22823;&#23567;&#30340;&#19981;&#30456;&#20132;&#31038;&#21306;&#12290;&#22270;&#20013;&#27599;&#23545;&#33410;&#28857;&#20043;&#38388;&#30340;&#36793;&#30340;&#23384;&#22312;&#19982;&#21542;&#21462;&#20915;&#20110;&#36825;&#20004;&#20010;&#33410;&#28857;&#26159;&#21542;&#23646;&#20110;&#21516;&#19968;&#20010;&#31038;&#21306;&#12290;&#23398;&#20064;&#22120;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#35266;&#23519;&#21040;&#30340;&#22270;&#26469;&#24674;&#22797;&#38544;&#34255;&#30340;&#31038;&#21306;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#36129;&#29486;&#26377;&#20004;&#20010;&#26041;&#38754;&#65306;&#65288;i&#65289;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#20449;&#24687;&#29702;&#35770;&#19978;&#30028;&#65288;&#23450;&#29702;1&#65289;&#65292;&#34920;&#26126;&#24403;MVSBM&#30340;&#27169;&#22411;&#21442;&#25968;&#36229;&#36807;&#19968;&#23450;&#38408;&#20540;&#26102;&#65292;&#21487;&#20197;&#23454;&#29616;&#31934;&#30830;&#24674;&#22797;&#31038;&#21306;&#12290;&#65288;ii&#65289;&#30456;&#21453;&#22320;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#19968;&#20010;&#20449;&#24687;&#29702;&#35770;&#19979;&#30028;&#65288;&#23450;&#29702;2&#65289;&#65292;&#34920;&#26126;&#24403;MVSBM&#30340;&#27169;&#22411;&#21442;&#25968;&#20302;&#20110;&#21069;&#36848;&#38408;&#20540;&#26102;&#65292;&#26080;&#27861;&#23454;&#29616;&#31934;&#30830;&#24674;&#22797;&#31038;&#21306;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers the problem of community detection on multiple potentially correlated graphs from an information-theoretical perspective. We first put forth a random graph model, called the multi-view stochastic block model (MVSBM), designed to generate correlated graphs on the same set of nodes (with cardinality $n$). The $n$ nodes are partitioned into two disjoint communities of equal size. The presence or absence of edges in the graphs for each pair of nodes depends on whether the two nodes belong to the same community or not. The objective for the learner is to recover the hidden communities with observed graphs. Our technical contributions are two-fold: (i) We establish an information-theoretic upper bound (Theorem~1) showing that exact recovery of community is achievable when the model parameters of MVSBM exceed a certain threshold. (ii) Conversely, we derive an information-theoretic lower bound (Theorem~2) showing that when the model parameters of MVSBM fall below the afore
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#26041;&#27861;&#35770;&#65292;&#29992;&#20110;&#25506;&#32034;&#22522;&#20110;&#37327;&#21270;&#30340;&#31995;&#32479;&#38453;&#21015;DNN&#21152;&#36895;&#22120;&#20013;&#28608;&#27963;&#38169;&#35823;&#30340;&#21487;&#38752;&#24615;&#65292;&#20174;&#32780;&#20840;&#38754;&#35780;&#20272;&#27169;&#22411;&#31934;&#24230;&#12289;&#21487;&#38752;&#24615;&#21644;&#30828;&#20214;&#25928;&#29575;&#20043;&#38388;&#30340;&#30456;&#20114;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.09509</link><description>&lt;p&gt;
&#22522;&#20110;&#37327;&#21270;&#30340;&#31995;&#32479;&#38453;&#21015;DNN&#21152;&#36895;&#22120;&#20013;&#28608;&#27963;&#38169;&#35823;&#21487;&#38752;&#24615;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Exploration of Activation Fault Reliability in Quantized Systolic Array-Based DNN Accelerators. (arXiv:2401.09509v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09509
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#26041;&#27861;&#35770;&#65292;&#29992;&#20110;&#25506;&#32034;&#22522;&#20110;&#37327;&#21270;&#30340;&#31995;&#32479;&#38453;&#21015;DNN&#21152;&#36895;&#22120;&#20013;&#28608;&#27963;&#38169;&#35823;&#30340;&#21487;&#38752;&#24615;&#65292;&#20174;&#32780;&#20840;&#38754;&#35780;&#20272;&#27169;&#22411;&#31934;&#24230;&#12289;&#21487;&#38752;&#24615;&#21644;&#30828;&#20214;&#25928;&#29575;&#20043;&#38388;&#30340;&#30456;&#20114;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20943;&#23569;&#30828;&#20214;&#24179;&#21488;&#30340;&#35745;&#31639;&#36127;&#25285;&#65292;&#21363;&#38477;&#20302;&#33021;&#32791;&#21644;&#25191;&#34892;&#26102;&#38388;&#20197;&#21450;&#25552;&#39640;DNN&#21152;&#36895;&#22120;&#30340;&#25928;&#29575;&#30340;&#38656;&#27714;&#19979;&#65292;&#23545;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#21152;&#36895;&#22120;&#30340;&#21487;&#38752;&#24615;&#35201;&#27714;&#21464;&#24471;&#36234;&#26469;&#36234;&#20005;&#26684;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#23450;&#21046;&#38656;&#27714;&#30340;&#19987;&#29992;DNN&#21152;&#36895;&#22120;&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#38271;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#31243;&#24207;&#65292;&#38656;&#35201;&#36827;&#34892;&#20840;&#38754;&#30340;&#35774;&#35745;&#31354;&#38388;&#25506;&#32034;&#65292;&#20197;&#23454;&#29616;&#24320;&#21457;&#28385;&#36275;&#36825;&#20123;&#35201;&#27714;&#30340;&#39640;&#25928;&#21644;&#31283;&#20581;&#30340;&#21152;&#36895;&#22120;&#12290;&#22240;&#27492;&#65292;&#30828;&#20214;&#24615;&#33021;&#65288;&#38754;&#31215;&#21644;&#24310;&#36831;&#65289;&#19982;DNN&#21152;&#36895;&#22120;&#23454;&#29616;&#30340;&#21487;&#38752;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#38656;&#35201;&#36827;&#34892;&#20998;&#26512;&#24037;&#20855;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#30340;&#26041;&#27861;&#35770;&#65292;&#29992;&#20110;&#25506;&#32034;&#21644;&#23454;&#29616;&#23545;&#27169;&#22411;&#31934;&#24230;&#12289;&#28608;&#27963;&#38169;&#35823;&#21487;&#38752;&#24615;&#21644;&#30828;&#20214;&#25928;&#29575;&#30340;&#19977;&#26041;&#38754;&#24433;&#21709;&#30340;&#20840;&#38754;&#35780;&#20272;&#12290;&#19968;&#31181;&#20840;&#33258;&#21160;&#21270;&#30340;&#26694;&#26550;&#34987;&#20171;&#32461;&#29992;&#20110;&#36827;&#34892;&#36825;&#31181;&#26041;&#27861;&#35770;&#30340;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
The stringent requirements for the Deep Neural Networks (DNNs) accelerator's reliability stand along with the need for reducing the computational burden on the hardware platforms, i.e. reducing the energy consumption and execution time as well as increasing the efficiency of DNN accelerators. Moreover, the growing demand for specialized DNN accelerators with tailored requirements, particularly for safety-critical applications, necessitates a comprehensive design space exploration to enable the development of efficient and robust accelerators that meet those requirements. Therefore, the trade-off between hardware performance, i.e. area and delay, and the reliability of the DNN accelerator implementation becomes critical and requires tools for analysis. This paper presents a comprehensive methodology for exploring and enabling a holistic assessment of the trilateral impact of quantization on model accuracy, activation fault reliability, and hardware efficiency. A fully automated framewor
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#30005;&#23376;&#21830;&#21153;&#24191;&#21578;&#20013;&#22810;&#22330;&#21518;&#22788;&#29702;&#26657;&#20934;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#26657;&#20934;&#22120;&#65292;&#24182;&#22312;&#22312;&#32447;&#25512;&#26029;&#36807;&#31243;&#20013;&#24212;&#29992;&#36825;&#20123;&#26657;&#20934;&#22120;&#26469;&#23454;&#29616;&#24418;&#29366;&#26657;&#20934;&#21644;&#25968;&#20540;&#26657;&#20934;&#12290;</title><link>http://arxiv.org/abs/2401.09507</link><description>&lt;p&gt;
&#28145;&#24230;&#38598;&#25104;&#24418;&#29366;&#26657;&#20934;&#65306;&#22312;&#32447;&#24191;&#21578;&#20013;&#30340;&#22810;&#22330;&#21518;&#22788;&#29702;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Deep Ensemble Shape Calibration: Multi-Field Post-hoc Calibration in Online Advertising. (arXiv:2401.09507v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09507
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#30005;&#23376;&#21830;&#21153;&#24191;&#21578;&#20013;&#22810;&#22330;&#21518;&#22788;&#29702;&#26657;&#20934;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#26657;&#20934;&#22120;&#65292;&#24182;&#22312;&#22312;&#32447;&#25512;&#26029;&#36807;&#31243;&#20013;&#24212;&#29992;&#36825;&#20123;&#26657;&#20934;&#22120;&#26469;&#23454;&#29616;&#24418;&#29366;&#26657;&#20934;&#21644;&#25968;&#20540;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30005;&#23376;&#21830;&#21153;&#24191;&#21578;&#22330;&#26223;&#20013;&#65292;&#20272;&#35745;CTR&#21644;CVR&#30340;&#30495;&#23454;&#27010;&#29575;&#65288;&#31216;&#20026;&#26657;&#20934;&#20272;&#35745;&#65289;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#30452;&#25509;&#24433;&#21709;&#20080;&#26041;&#12289;&#21334;&#26041;&#21644;&#24179;&#21488;&#30340;&#21033;&#30410;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#35299;&#20915;&#26657;&#20934;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#28041;&#21450;&#20351;&#29992;&#39564;&#35777;&#38598;&#35757;&#32451;&#26657;&#20934;&#22120;&#65292;&#24182;&#38543;&#21518;&#22312;&#22312;&#32447;&#25512;&#26029;&#36807;&#31243;&#20013;&#24212;&#29992;&#36825;&#20123;&#26657;&#20934;&#22120;&#26469;&#20462;&#27491;&#21407;&#22987;&#20272;&#35745;&#20540;&#12290;&#28982;&#32780;&#65292;&#30005;&#23376;&#21830;&#21153;&#24191;&#21578;&#22330;&#26223;&#30340;&#25361;&#25112;&#22312;&#20110;&#22810;&#22330;&#26657;&#20934;&#12290;&#22810;&#22330;&#26657;&#20934;&#21487;&#20197;&#20998;&#20026;&#20004;&#20010;&#19981;&#21516;&#30340;&#23376;&#38382;&#39064;&#65306;&#25968;&#20540;&#26657;&#20934;&#21644;&#24418;&#29366;&#26657;&#20934;&#12290;&#25968;&#20540;&#26657;&#20934;&#34987;&#23450;&#20041;&#20026;&#27599;&#20010;&#20851;&#27880;&#39046;&#22495;&#19979;&#27599;&#20010;&#25968;&#20540;&#30340;&#19981;&#36807;&#24230;&#25110;&#19981;&#20302;&#20272;&#12290;&#24418;&#29366;&#26657;&#20934;&#34987;&#23450;&#20041;&#20026;&#22312;&#20851;&#27880;&#39046;&#22495;&#26465;&#20214;&#19979;&#29305;&#23450;&#33539;&#22260;&#20869;&#27599;&#20010;pCTR&#23376;&#38598;&#30340;&#19981;&#36807;&#24230;&#25110;&#19981;&#20302;&#20272;&#12290;&#20026;&#20102;&#23454;&#29616;&#24418;&#29366;&#26657;&#20934;&#21644;&#25968;&#20540;&#26657;&#20934;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#38598;&#25104;&#24418;&#29366;&#26657;&#20934;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the e-commerce advertising scenario, estimating the true probabilities (known as a calibrated estimate) on CTR and CVR is critical and can directly affect the benefits of the buyer, seller and platform. Previous research has introduced numerous solutions for addressing the calibration problem. These methods typically involve the training of calibrators using a validation set and subsequently applying these calibrators to correct the original estimated values during online inference. However, what sets e-commerce advertising scenarios is the challenge of multi-field calibration. Multi-field calibration can be subdivided into two distinct sub-problems: value calibration and shape calibration. Value calibration is defined as no over- or under-estimation for each value under concerned fields. Shape calibration is defined as no over- or under-estimation for each subset of the pCTR within the specified range under condition of concerned fields. In order to achieve shape calibration and va
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21151;&#33021;&#33258;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#23398;&#20064;&#21151;&#33021;&#25968;&#25454;&#30340;&#38750;&#32447;&#24615;&#34920;&#31034;&#65292;&#24182;&#36991;&#20813;&#20102;&#39044;&#22788;&#29702;&#30340;&#38656;&#35201;&#12290;</title><link>http://arxiv.org/abs/2401.09499</link><description>&lt;p&gt;
&#21151;&#33021;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#24179;&#28369;&#21644;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Functional Autoencoder for Smoothing and Representation Learning. (arXiv:2401.09499v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09499
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21151;&#33021;&#33258;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#23398;&#20064;&#21151;&#33021;&#25968;&#25454;&#30340;&#38750;&#32447;&#24615;&#34920;&#31034;&#65292;&#24182;&#36991;&#20813;&#20102;&#39044;&#22788;&#29702;&#30340;&#38656;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21151;&#33021;&#25968;&#25454;&#20998;&#26512;&#20013;&#24120;&#29992;&#30340;&#27969;&#31243;&#26159;&#23558;&#31163;&#25955;&#35266;&#27979;&#25968;&#25454;&#36716;&#25442;&#20026;&#24179;&#28369;&#20989;&#25968;&#65292;&#28982;&#21518;&#36890;&#36807;&#19968;&#20010;&#26377;&#38480;&#32500;&#24230;&#30340;&#31995;&#25968;&#21521;&#37327;&#26469;&#34920;&#31034;&#36825;&#20123;&#20989;&#25968;&#20197;&#24635;&#32467;&#20449;&#24687;&#12290;&#29616;&#26377;&#30340;&#25968;&#25454;&#24179;&#28369;&#21644;&#38477;&#32500;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#23398;&#20064;&#32447;&#24615;&#26144;&#23556;&#65292;&#20294;&#20165;&#23398;&#20064;&#32447;&#24615;&#34920;&#31034;&#21487;&#33021;&#19981;&#36275;&#22815;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#33258;&#32534;&#30721;&#22120;&#23398;&#20064;&#21151;&#33021;&#25968;&#25454;&#30340;&#38750;&#32447;&#24615;&#34920;&#31034;&#65292;&#32780;&#26080;&#38656;&#39044;&#22788;&#29702;&#12290;&#25105;&#20204;&#35774;&#35745;&#32534;&#30721;&#22120;&#37319;&#29992;&#25237;&#24433;&#23618;&#65292;&#35745;&#31639;&#21151;&#33021;&#25968;&#25454;&#21644;&#35266;&#23519;&#26102;&#38388;&#25139;&#19978;&#30340;&#21151;&#33021;&#26435;&#37325;&#30340;&#21152;&#26435;&#20869;&#31215;&#65292;&#35299;&#30721;&#22120;&#24212;&#29992;&#24674;&#22797;&#23618;&#65292;&#20351;&#29992;&#19968;&#32452;&#39044;&#20808;&#30830;&#23450;&#30340;&#26377;&#38480;&#32500;&#24230;&#21521;&#37327;&#23558;&#20174;&#21151;&#33021;&#25968;&#25454;&#20013;&#25552;&#21462;&#30340;&#21521;&#37327;&#26144;&#23556;&#22238;&#21151;&#33021;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
A common pipeline in functional data analysis is to first convert the discretely observed data to smooth functions, and then represent the functions by a finite-dimensional vector of coefficients summarizing the information. Existing methods for data smoothing and dimensional reduction mainly focus on learning the linear mappings from the data space to the representation space, however, learning only the linear representations may not be sufficient. In this study, we propose to learn the nonlinear representations of functional data using neural network autoencoders designed to process data in the form it is usually collected without the need of preprocessing. We design the encoder to employ a projection layer computing the weighted inner product of the functional data and functional weights over the observed timestamp, and the decoder to apply a recovery layer that maps the finite-dimensional vector extracted from the functional data back to functional space using a set of predetermine
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21160;&#24577;&#32593;&#32476;&#25299;&#25169;&#19979;&#65292;&#19981;&#21487;&#35775;&#38382;&#33410;&#28857;&#23545;&#27969;&#35328;&#23398;&#20064;&#30340;&#25910;&#25947;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2401.09498</link><description>&lt;p&gt;
&#25216;&#26415;&#25253;&#21578;&#65306;&#20851;&#20110;&#33410;&#28857;&#19981;&#21487;&#35775;&#38382;&#24773;&#20917;&#19979;&#27969;&#35328;&#23398;&#20064;&#25910;&#25947;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Technical Report: On the Convergence of Gossip Learning in the Presence of Node Inaccessibility. (arXiv:2401.09498v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09498
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21160;&#24577;&#32593;&#32476;&#25299;&#25169;&#19979;&#65292;&#19981;&#21487;&#35775;&#38382;&#33410;&#28857;&#23545;&#27969;&#35328;&#23398;&#20064;&#30340;&#25910;&#25947;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Gossip learning&#65288;GL&#65289;&#20316;&#20026;&#20998;&#25955;&#24335;&#23398;&#20064;&#30340;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#26356;&#36866;&#29992;&#20110;&#36164;&#28304;&#21463;&#38480;&#30340;&#26080;&#32447;&#32593;&#32476;&#65292;&#22914;&#30001;&#26080;&#20154;&#26426;&#65288;UAV&#65289;&#32452;&#25104;&#30340;FANETs&#12290;GL&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;UAV&#32593;&#32476;&#30340;&#25928;&#29575;&#24182;&#24310;&#38271;&#30005;&#27744;&#23551;&#21629;&#12290;&#23613;&#31649;&#20855;&#26377;&#36825;&#20123;&#20248;&#21183;&#65292;&#20294;GL&#30340;&#24615;&#33021;&#21463;&#25968;&#25454;&#20998;&#24067;&#12289;&#36890;&#20449;&#36895;&#24230;&#21644;&#32593;&#32476;&#36830;&#25509;&#24615;&#30340;&#24433;&#21709;&#36739;&#22823;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22240;&#32032;&#22914;&#20309;&#24433;&#21709;GL&#30340;&#25910;&#25947;&#24615;&#20173;&#19981;&#28165;&#26970;&#12290;&#29616;&#26377;&#30740;&#31350;&#22522;&#20110;&#34394;&#25311;&#25968;&#37327;&#26469;&#30740;&#31350;GL&#30340;&#25910;&#25947;&#24615;&#65292;&#20197;&#26041;&#20415;&#24615;&#32780;&#24573;&#30053;&#20102;&#24403;&#19968;&#20123;&#33410;&#28857;&#19981;&#21487;&#35775;&#38382;&#26102;&#32593;&#32476;&#30340;&#30495;&#23454;&#29366;&#24577;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#21160;&#24577;&#32593;&#32476;&#25299;&#25169;&#19979;&#19981;&#21487;&#35775;&#38382;&#33410;&#28857;&#23545;GL&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#24314;&#27169;&#21644;&#30740;&#31350;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#26435;&#37325;&#21457;&#25955;&#20998;&#35299;&#20026;&#33410;&#28857;&#26159;&#21542;&#21487;&#35775;&#38382;&#30340;&#24773;&#20917;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#33410;&#28857;&#21487;&#35775;&#38382;&#24615;&#30340;&#21160;&#24577;&#19979;GL&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#25552;&#20379;&#20102;
&lt;/p&gt;
&lt;p&gt;
Gossip learning (GL), as a decentralized alternative to federated learning (FL), is more suitable for resource-constrained wireless networks, such as FANETs that are formed by unmanned aerial vehicles (UAVs). GL can significantly enhance the efficiency and extend the battery life of UAV networks. Despite the advantages, the performance of GL is strongly affected by data distribution, communication speed, and network connectivity. However, how these factors influence the GL convergence is still unclear. Existing work studied the convergence of GL based on a virtual quantity for the sake of convenience, which fail to reflect the real state of the network when some nodes are inaccessible. In this paper, we formulate and investigate the impact of inaccessible nodes to GL under a dynamic network topology. We first decompose the weight divergence by whether the node is accessible or not. Then, we investigate the GL convergence under the dynamic of node accessibility and theoretically provide
&lt;/p&gt;</description></item><item><title>VeriBug&#26159;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#30828;&#20214;&#35774;&#35745;&#28431;&#27934;&#23450;&#20301;&#26694;&#26550;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#21152;&#36895;&#23492;&#23384;&#22120;&#20256;&#36755;&#32423;&#21035;&#30340;&#35843;&#35797;&#65292;&#24182;&#29983;&#25104;&#21487;&#33021;&#30340;&#26681;&#26412;&#21407;&#22240;&#30340;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2401.09494</link><description>&lt;p&gt;
VeriBug: &#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#30828;&#20214;&#35774;&#35745;&#28431;&#27934;&#23450;&#20301;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
VeriBug: An Attention-based Framework for Bug-Localization in Hardware Designs. (arXiv:2401.09494v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09494
&lt;/p&gt;
&lt;p&gt;
VeriBug&#26159;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#30828;&#20214;&#35774;&#35745;&#28431;&#27934;&#23450;&#20301;&#26694;&#26550;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#21152;&#36895;&#23492;&#23384;&#22120;&#20256;&#36755;&#32423;&#21035;&#30340;&#35843;&#35797;&#65292;&#24182;&#29983;&#25104;&#21487;&#33021;&#30340;&#26681;&#26412;&#21407;&#22240;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38754;&#21521;&#19981;&#21516;&#29305;&#23450;&#24212;&#29992;&#30340;SoC&#35774;&#35745;&#30340;&#35268;&#27169;&#21644;&#22797;&#26434;&#24615;&#21576;&#25351;&#25968;&#22686;&#38271;&#12290;&#22312;&#36825;&#20123;&#31995;&#32479;&#20013;&#65292;&#26410;&#26816;&#27979;&#21040;&#30340;&#28431;&#27934;&#30340;&#25104;&#26412;&#36828;&#39640;&#20110;&#20256;&#32479;&#22788;&#29702;&#22120;&#31995;&#32479;&#65292;&#22240;&#20026;&#23427;&#21487;&#33021;&#24847;&#21619;&#30528;&#36130;&#20135;&#25110;&#29983;&#21629;&#30340;&#25439;&#22833;&#12290;&#30001;&#20110;&#19981;&#26029;&#32553;&#30701;&#30340;&#24066;&#22330;&#25512;&#20986;&#26102;&#38388;&#21644;&#19981;&#26029;&#22686;&#38271;&#30340;&#35774;&#22791;&#38656;&#27714;&#65292;&#36825;&#20010;&#38382;&#39064;&#21464;&#24471;&#26356;&#21152;&#20005;&#37325;&#12290;&#23613;&#31649;&#22312;&#20223;&#30495;&#21644;&#24418;&#24335;&#21270;&#26041;&#27861;&#26041;&#38754;&#36827;&#34892;&#20102;&#25968;&#21313;&#24180;&#30340;&#30740;&#31350;&#26469;&#36827;&#34892;&#35843;&#35797;&#21644;&#39564;&#35777;&#65292;&#20294;&#23427;&#20173;&#28982;&#26159;&#24403;&#20195;&#30828;&#20214;&#35774;&#35745;&#21608;&#26399;&#20013;&#32791;&#26102;&#21644;&#36164;&#28304;&#23494;&#38598;&#22411;&#30340;&#36807;&#31243;&#20043;&#19968;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;VeriBug&#65292;&#23427;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#26469;&#21152;&#36895;&#23492;&#23384;&#22120;&#20256;&#36755;&#32423;&#21035;&#30340;&#35843;&#35797;&#65292;&#24182;&#29983;&#25104;&#21487;&#33021;&#30340;&#26681;&#26412;&#21407;&#22240;&#30340;&#35299;&#37322;&#12290;&#39318;&#20808;&#65292;VeriBug&#20351;&#29992;&#30828;&#20214;&#35774;&#35745;&#30340;&#25511;&#21046;&#25968;&#25454;&#27969;&#22270;&#65292;&#36890;&#36807;&#20998;&#26512;&#25805;&#20316;&#25968;&#21450;&#20854;&#20998;&#37197;&#30340;&#19978;&#19979;&#25991;&#26469;&#23398;&#20064;&#25191;&#34892;&#35774;&#35745;&#35821;&#21477;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, there has been an exponential growth in the size and complexity of System-on-Chip designs targeting different specialized applications. The cost of an undetected bug in these systems is much higher than in traditional processor systems as it may imply the loss of property or life. The problem is further exacerbated by the ever-shrinking time-to-market and ever-increasing demand to churn out billions of devices. Despite decades of research in simulation and formal methods for debugging and verification, it is still one of the most time-consuming and resource intensive processes in contemporary hardware design cycle. In this work, we propose VeriBug, which leverages recent advances in deep learning to accelerate debugging at the Register-Transfer Level and generates explanations of likely root causes. First, VeriBug uses control-data flow graph of a hardware design and learns to execute design statements by analyzing the context of operands and their assignments. Then, i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#32447;&#24615;&#21464;&#20998;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26469;&#23398;&#20064;&#20113;&#36752;&#23556;&#21453;&#39304;&#23545;&#26089;&#26399;&#28909;&#24102;&#27668;&#26059;&#24378;&#21270;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20869;&#26680;&#28145;&#23545;&#27969;&#21644;&#27973;&#20113;&#30340;&#38271;&#27874;&#36752;&#23556;&#24378;&#36843;&#37117;&#23545;&#24378;&#21270;&#36215;&#21040;&#36129;&#29486;&#65292;&#20854;&#20013;&#28145;&#23545;&#27969;&#30340;&#24433;&#21709;&#26368;&#22823;&#12290;</title><link>http://arxiv.org/abs/2401.09493</link><description>&lt;p&gt;
&#35782;&#21035;&#19982;&#26089;&#26399;&#28909;&#24102;&#27668;&#26059;&#24378;&#21270;&#26377;&#20851;&#30340;&#19977;&#32500;&#36752;&#23556;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Identifying Three-Dimensional Radiative Patterns Associated with Early Tropical Cyclone Intensification. (arXiv:2401.09493v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09493
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#32447;&#24615;&#21464;&#20998;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26469;&#23398;&#20064;&#20113;&#36752;&#23556;&#21453;&#39304;&#23545;&#26089;&#26399;&#28909;&#24102;&#27668;&#26059;&#24378;&#21270;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20869;&#26680;&#28145;&#23545;&#27969;&#21644;&#27973;&#20113;&#30340;&#38271;&#27874;&#36752;&#23556;&#24378;&#36843;&#37117;&#23545;&#24378;&#21270;&#36215;&#21040;&#36129;&#29486;&#65292;&#20854;&#20013;&#28145;&#23545;&#27969;&#30340;&#24433;&#21709;&#26368;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20113;&#36752;&#23556;&#21453;&#39304;&#24433;&#21709;&#20102;&#26089;&#26399;&#28909;&#24102;&#27668;&#26059;&#30340;&#24378;&#21270;&#65292;&#20294;&#29616;&#26377;&#35786;&#26029;&#26694;&#26550;&#30340;&#23616;&#38480;&#24615;&#20351;&#20854;&#26080;&#27861;&#29992;&#26469;&#30740;&#31350;&#19981;&#23545;&#31216;&#25110;&#30636;&#24577;&#30340;&#36752;&#23556;&#21152;&#28909;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32447;&#24615;&#21464;&#20998;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#65288;VED&#65289;&#26469;&#23398;&#20064;&#36752;&#23556;&#19982;&#23454;&#38469;&#27169;&#25311;&#30340;&#27668;&#26059;&#34920;&#38754;&#24378;&#21270;&#20043;&#38388;&#30340;&#38544;&#34255;&#20851;&#31995;&#12290;&#38480;&#21046;VED&#27169;&#22411;&#30340;&#36755;&#20837;&#21487;&#20197;&#21033;&#29992;&#20854;&#19981;&#30830;&#23450;&#24615;&#26469;&#35782;&#21035;&#36752;&#23556;&#23545;&#24378;&#21270;&#26356;&#37325;&#35201;&#30340;&#26102;&#26399;&#12290;&#23545;&#25552;&#21462;&#30340;&#19977;&#32500;&#36752;&#23556;&#32467;&#26500;&#30340;&#32454;&#33268;&#26816;&#26597;&#34920;&#26126;&#65292;&#20869;&#26680;&#28145;&#23545;&#27969;&#21644;&#27973;&#20113;&#30340;&#38271;&#27874;&#36752;&#23556;&#24378;&#36843;&#37117;&#23545;&#24378;&#21270;&#36215;&#21040;&#36129;&#29486;&#65292;&#20854;&#20013;&#28145;&#23545;&#27969;&#22312;&#25972;&#20307;&#19978;&#20855;&#26377;&#26368;&#22823;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#27973;&#20113;&#30340;&#19979;&#39118;&#22788;&#30340;&#28145;&#23545;&#27969;&#23545;&#28023;&#29141;&#30340;&#24378;&#21270;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#26426;&#22120;&#23398;&#20064;&#21487;&#20197;&#21457;&#29616;&#28909;&#21147;-&#21160;&#21147;&#23398;&#20851;&#31995;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#36724;&#23545;&#31216;&#25110;&#30830;&#23450;&#24615;&#30340;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cloud radiative feedback impacts early tropical cyclone (TC) intensification, but limitations in existing diagnostic frameworks make them unsuitable for studying asymmetric or transient radiative heating. We propose a linear Variational Encoder-Decoder (VED) to learn the hidden relationship between radiation and the surface intensification of realistic simulated TCs. Limiting VED model inputs enables using its uncertainty to identify periods when radiation has more importance for intensification. A close examination of the extracted 3D radiative structures suggests that longwave radiative forcing from inner core deep convection and shallow clouds both contribute to intensification, with the deep convection having the most impact overall. We find that deep convection downwind of the shallow clouds is critical to the intensification of Haiyan. Our work demonstrates that machine learning can discover thermodynamic-kinematic relationships without relying on axisymmetric or deterministic as
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#20302;&#25104;&#26412;&#28909;&#32447;&#39118;&#36895;&#35745;&#30001;&#20110;&#27668;&#28201;&#21464;&#21270;&#32780;&#23548;&#33268;&#30340;&#31934;&#24230;&#25439;&#22833;&#38382;&#39064;&#65292;&#37319;&#29992;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#36827;&#34892;&#27010;&#29575;&#26657;&#20934;&#65292;&#24182;&#22312;&#23454;&#39564;&#39564;&#35777;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#22312;&#23454;&#38469;&#20351;&#29992;&#21069;&#36827;&#34892;&#26657;&#20934;&#65292;&#21487;&#20197;&#20272;&#35745;&#20856;&#22411;&#29615;&#22659;&#28201;&#24230;&#19979;&#30340;&#39118;&#36895;&#65292;&#24182;&#25552;&#20379;&#27599;&#20010;&#36895;&#24230;&#27979;&#37327;&#30340;&#21487;&#38752;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2401.09492</link><description>&lt;p&gt;
&#36890;&#36807;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#23454;&#29616;&#28909;&#32447;&#39118;&#36895;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#33258;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-Aware Calibration of a Hot-Wire Anemometer With Gaussian Process Regression. (arXiv:2401.09492v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09492
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#20302;&#25104;&#26412;&#28909;&#32447;&#39118;&#36895;&#35745;&#30001;&#20110;&#27668;&#28201;&#21464;&#21270;&#32780;&#23548;&#33268;&#30340;&#31934;&#24230;&#25439;&#22833;&#38382;&#39064;&#65292;&#37319;&#29992;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#36827;&#34892;&#27010;&#29575;&#26657;&#20934;&#65292;&#24182;&#22312;&#23454;&#39564;&#39564;&#35777;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#22312;&#23454;&#38469;&#20351;&#29992;&#21069;&#36827;&#34892;&#26657;&#20934;&#65292;&#21487;&#20197;&#20272;&#35745;&#20856;&#22411;&#29615;&#22659;&#28201;&#24230;&#19979;&#30340;&#39118;&#36895;&#65292;&#24182;&#25552;&#20379;&#27599;&#20010;&#36895;&#24230;&#27979;&#37327;&#30340;&#21487;&#38752;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20811;&#26381;&#20302;&#25104;&#26412;&#28909;&#32447;&#39118;&#36895;&#35745;&#30001;&#20110;&#27668;&#28201;&#21464;&#21270;&#32780;&#23548;&#33268;&#30340;&#31934;&#24230;&#25439;&#22833;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#36827;&#34892;&#27010;&#29575;&#26657;&#20934;&#12290;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#26159;&#19968;&#31181;&#38750;&#21442;&#25968;&#30340;&#36125;&#21494;&#26031;&#21644;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#26681;&#25454;&#19968;&#20010;&#25110;&#22810;&#20010;&#24050;&#30693;&#30340;&#36755;&#20837;&#21464;&#37327;&#39044;&#27979;&#26410;&#30693;&#30340;&#30446;&#26631;&#21464;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39564;&#35777;&#65292;&#22312;&#25512;&#26029;&#23454;&#38469;&#39118;&#36895;&#20540;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#12290;&#36890;&#36807;&#22312;&#23454;&#38469;&#20351;&#29992;&#21069;&#65292;&#23545;&#28909;&#32447;&#39118;&#36895;&#35745;&#36827;&#34892;&#32771;&#34385;&#27668;&#28201;&#30340;&#26657;&#20934;&#65292;&#21487;&#20197;&#20272;&#35745;&#20856;&#22411;&#29615;&#22659;&#28201;&#24230;&#33539;&#22260;&#20869;&#30340;&#39118;&#36895;&#65292;&#24182;&#19988;&#20026;&#27599;&#20010;&#36895;&#24230;&#27979;&#37327;&#20540;&#25552;&#20379;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Expensive ultrasonic anemometers are usually required to measure wind speed accurately. The aim of this work is to overcome the loss of accuracy of a low cost hot-wire anemometer caused by the changes of air temperature, by means of a probabilistic calibration using Gaussian Process Regression. Gaussian Process Regression is a non-parametric, Bayesian, and supervised learning method designed to make predictions of an unknown target variable as a function of one or more known input variables. Our approach is validated against real datasets, obtaining a good performance in inferring the actual wind speed values. By performing, before its real use in the field, a calibration of the hot-wire anemometer taking into account air temperature, permits that the wind speed can be estimated for the typical range of ambient temperatures, including a grounded uncertainty estimation for each speed measure.
&lt;/p&gt;</description></item><item><title>PUPAE&#26159;&#19968;&#31181;&#30452;&#35266;&#19988;&#21487;&#25805;&#20316;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#39046;&#22495;&#26080;&#20851;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#65292;&#33021;&#22815;&#24110;&#21161;&#35299;&#37322;&#21644;&#22788;&#29702;&#24322;&#24120;&#12290;</title><link>http://arxiv.org/abs/2401.09489</link><description>&lt;p&gt;
PUPAE: &#30452;&#35266;&#19988;&#21487;&#25805;&#20316;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
PUPAE: Intuitive and Actionable Explanations for Time Series Anomalies. (arXiv:2401.09489v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09489
&lt;/p&gt;
&lt;p&gt;
PUPAE&#26159;&#19968;&#31181;&#30452;&#35266;&#19988;&#21487;&#25805;&#20316;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#39046;&#22495;&#26080;&#20851;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#65292;&#33021;&#22815;&#24110;&#21161;&#35299;&#37322;&#21644;&#22788;&#29702;&#24322;&#24120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22312;&#26816;&#27979;&#21040;&#24322;&#24120;&#20043;&#21518;&#65292;&#25105;&#20204;&#33021;&#35299;&#37322;&#23427;&#21527;&#65311;&#36825;&#26679;&#30340;&#35299;&#37322;&#23545;&#20110;&#22788;&#29702;&#24322;&#24120;&#38750;&#24120;&#26377;&#29992;&#12290;&#20363;&#22914;&#65292;&#22312;&#19968;&#20010;&#28860;&#27833;&#21378;&#20013;&#65292;&#25105;&#20204;&#26159;&#36890;&#36807;&#27966;&#36963;&#28082;&#21387;&#24037;&#31243;&#24072;&#36824;&#26159;&#23454;&#20064;&#29983;&#26356;&#25442;&#20256;&#24863;&#22120;&#30005;&#27744;&#26469;&#21709;&#24212;&#24322;&#24120;&#65311;&#34429;&#28982;&#26377;&#19968;&#20123;&#24182;&#34892;&#30340;&#21162;&#21147;&#26469;&#35299;&#37322;&#24322;&#24120;&#65292;&#20294;&#24456;&#22810;&#25552;&#20986;&#30340;&#25216;&#26415;&#20135;&#29983;&#30340;&#35299;&#37322;&#26159;&#38388;&#25509;&#30340;&#65292;&#24182;&#19988;&#36890;&#24120;&#27604;&#23427;&#20204;&#35797;&#22270;&#35299;&#37322;&#30340;&#24322;&#24120;&#26356;&#22797;&#26434;&#12290;&#25105;&#20204;&#23545;&#21508;&#20010;&#39046;&#22495;&#21069;&#32447;&#20174;&#19994;&#20154;&#21592;&#20351;&#29992;&#30340;&#25991;&#29486;&#12289;&#28165;&#21333;&#21644;&#29992;&#25143;&#25163;&#20876;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21457;&#29616;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#20849;&#21516;&#28857;&#12290;&#22823;&#22810;&#25968;&#20174;&#19994;&#20154;&#21592;&#20197;&#20197;&#19979;&#26684;&#24335;&#35752;&#35770;&#12289;&#35299;&#37322;&#21644;&#25253;&#21578;&#24322;&#24120;&#65306;&#22914;&#26524;&#27809;&#26377;&#30772;&#22351;B&#65292;&#24322;&#24120;&#23601;&#20250;&#20687;&#27491;&#24120;&#25968;&#25454;A&#19968;&#26679;&#12290;&#35835;&#32773;&#23558;&#20250;&#24847;&#35782;&#21040;&#36825;&#26159;&#19968;&#31181;&#21453;&#20107;&#23454;&#30340;&#35299;&#37322;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#39046;&#22495;&#26080;&#20851;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years there has been significant progress in time series anomaly detection. However, after detecting an (perhaps tentative) anomaly, can we explain it? Such explanations would be useful to triage anomalies. For example, in an oil refinery, should we respond to an anomaly by dispatching a hydraulic engineer, or an intern to replace the battery on a sensor? There have been some parallel efforts to explain anomalies, however many proposed techniques produce explanations that are indirect, and often seem more complex than the anomaly they seek to explain. Our review of the literature/checklists/user-manuals used by frontline practitioners in various domains reveals an interesting near-universal commonality. Most practitioners discuss, explain and report anomalies in the following format: The anomaly would be like normal data A, if not for the corruption B. The reader will appreciate that is a type of counterfactual explanation. In this work we introduce a domain agnostic counterf
&lt;/p&gt;</description></item><item><title>LoMA&#26159;&#19968;&#31181;&#26080;&#25439;&#21387;&#32553;&#30340;&#20869;&#23384;&#27880;&#24847;&#21147;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#22788;&#29702;&#38271;&#25991;&#26412;&#24182;&#20943;&#23569;&#36164;&#28304;&#28040;&#32791;&#12290;</title><link>http://arxiv.org/abs/2401.09486</link><description>&lt;p&gt;
LoMA: &#26080;&#25439;&#21387;&#32553;&#30340;&#20869;&#23384;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
LoMA: Lossless Compressed Memory Attention. (arXiv:2401.09486v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09486
&lt;/p&gt;
&lt;p&gt;
LoMA&#26159;&#19968;&#31181;&#26080;&#25439;&#21387;&#32553;&#30340;&#20869;&#23384;&#27880;&#24847;&#21147;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#22788;&#29702;&#38271;&#25991;&#26412;&#24182;&#20943;&#23569;&#36164;&#28304;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#29702;&#38271;&#25991;&#26412;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26368;&#37325;&#35201;&#30340;&#33021;&#21147;&#20043;&#19968;&#65292;&#20294;&#38543;&#30528;&#25991;&#26412;&#38271;&#24230;&#30340;&#22686;&#21152;&#65292;&#36164;&#28304;&#28040;&#32791;&#20063;&#24613;&#21095;&#22686;&#21152;&#12290;&#30446;&#21069;&#65292;&#36890;&#36807;&#21387;&#32553;KV&#32531;&#23384;&#26469;&#20943;&#23569;&#36164;&#28304;&#28040;&#32791;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#12290;&#23613;&#31649;&#23384;&#22312;&#35768;&#22810;&#29616;&#26377;&#30340;&#21387;&#32553;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#37117;&#26377;&#19968;&#20010;&#20849;&#21516;&#30340;&#32570;&#28857;&#65306;&#21387;&#32553;&#26159;&#26377;&#25439;&#30340;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#22312;&#21387;&#32553;&#36807;&#31243;&#20013;&#20449;&#24687;&#19981;&#21487;&#36991;&#20813;&#22320;&#20250;&#20002;&#22833;&#12290;&#22914;&#26524;&#21387;&#32553;&#29575;&#24456;&#39640;&#65292;&#20002;&#22833;&#37325;&#35201;&#20449;&#24687;&#30340;&#27010;&#29575;&#20250;&#22823;&#22823;&#22686;&#21152;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#26080;&#25439;&#21387;&#32553;&#30340;&#20869;&#23384;&#27880;&#24847;&#21147;&#65288;LoMA&#65289;&#65292;&#21487;&#20197;&#26681;&#25454;&#19968;&#32452;&#21387;&#32553;&#27604;&#29575;&#23558;&#20449;&#24687;&#26080;&#25439;&#21387;&#32553;&#25104;&#29305;&#27530;&#30340;&#20869;&#23384;&#20196;&#29260;KV&#23545;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;LoMA&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#21487;&#20197;&#39640;&#25928;&#35757;&#32451;&#19988;&#20855;&#26377;&#38750;&#24120;&#26377;&#25928;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to handle long texts is one of the most important capabilities of Large Language Models (LLMs), but as the text length increases, the consumption of resources also increases dramatically. At present, reducing resource consumption by compressing the KV cache is a common approach. Although there are many existing compression methods, they share a common drawback: the compression is not lossless. That is, information is inevitably lost during the compression process. If the compression rate is high, the probability of losing important information increases dramatically. We propose a new method, Lossless Compressed Memory Attention (LoMA), which allows for lossless compression of information into special memory token KV pairs according to a set compression ratio. Our experiments have achieved remarkable results, demonstrating that LoMA can be efficiently trained and has very effective performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#30828;&#20214;&#29305;&#27931;&#20234;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#25193;&#20805;&#25968;&#25454;&#65292;&#24182;&#37319;&#29992;&#26089;&#34701;&#21512;&#21644;&#26202;&#34701;&#21512;&#31574;&#30053;&#36827;&#34892;&#35780;&#20272;&#12290;&#36890;&#36807;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#25351;&#26631;&#65292;&#23454;&#29616;&#39118;&#38505;&#24863;&#30693;&#30340;&#20915;&#31574;&#21046;&#23450;&#12290;</title><link>http://arxiv.org/abs/2401.09479</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30828;&#20214;&#29305;&#27931;&#20234;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-Aware Hardware Trojan Detection Using Multimodal Deep Learning. (arXiv:2401.09479v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09479
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#30828;&#20214;&#29305;&#27931;&#20234;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#25193;&#20805;&#25968;&#25454;&#65292;&#24182;&#37319;&#29992;&#26089;&#34701;&#21512;&#21644;&#26202;&#34701;&#21512;&#31574;&#30053;&#36827;&#34892;&#35780;&#20272;&#12290;&#36890;&#36807;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#25351;&#26631;&#65292;&#23454;&#29616;&#39118;&#38505;&#24863;&#30693;&#30340;&#20915;&#31574;&#21046;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38646;&#20449;&#20219;&#30340;&#26080;&#21378;&#26080;&#21360;&#36896;&#21046;&#36896;&#26102;&#20195;&#65292;&#30828;&#20214;&#29305;&#27931;&#20234;&#22312;&#33455;&#29255;&#29983;&#20135;&#30340;&#21508;&#20010;&#38454;&#27573;&#34987;&#25554;&#20837;&#30340;&#39118;&#38505;&#22686;&#21152;&#20102;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#29992;&#20110;&#26816;&#27979;&#30828;&#20214;&#29305;&#27931;&#20234;&#12290;&#23613;&#31649;&#22823;&#37096;&#20998;&#20851;&#27880;&#28857;&#37117;&#38598;&#20013;&#22312;&#32479;&#35745;&#23398;&#25110;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#19978;&#65292;&#20294;&#21463;&#21040;&#29305;&#27931;&#20234;&#24863;&#26579;&#22522;&#20934;&#26679;&#26412;&#25968;&#37327;&#26377;&#38480;&#30340;&#24433;&#21709;&#65292;&#26816;&#27979;&#20934;&#30830;&#24615;&#21463;&#38480;&#65292;&#26080;&#27861;&#26816;&#27979;&#21040;&#38646;&#26085;&#29305;&#27931;&#20234;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#39318;&#20808;&#37319;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#26469;&#25193;&#20805;&#25968;&#25454;&#65292;&#20197;&#20004;&#31181;&#26367;&#20195;&#34920;&#31034;&#27169;&#24577;&#65292;&#22270;&#24418;&#21644;&#34920;&#26684;&#65292;&#30830;&#20445;&#25968;&#25454;&#38598;&#20197;&#20195;&#34920;&#24615;&#30340;&#26041;&#24335;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#26816;&#27979;&#30828;&#20214;&#29305;&#27931;&#20234;&#65292;&#24182;&#35780;&#20272;&#20102;&#26089;&#34701;&#21512;&#21644;&#26202;&#34701;&#21512;&#31574;&#30053;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#20272;&#35745;&#20102;&#27599;&#20010;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#25351;&#26631;&#65292;&#29992;&#20110;&#39118;&#38505;&#24863;&#30693;&#30340;&#20915;&#31574;&#21046;&#23450;&#12290;&#32467;&#26524;&#19981;&#20165;&#30830;&#35748;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#32780;&#19988;&#34920;&#26126;&#20102;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#23545;&#30828;&#20214;&#29305;&#27931;&#20234;&#26816;&#27979;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The risk of hardware Trojans being inserted at various stages of chip production has increased in a zero-trust fabless era. To counter this, various machine learning solutions have been developed for the detection of hardware Trojans. While most of the focus has been on either a statistical or deep learning approach, the limited number of Trojan-infected benchmarks affects the detection accuracy and restricts the possibility of detecting zero-day Trojans. To close the gap, we first employ generative adversarial networks to amplify our data in two alternative representation modalities, a graph and a tabular, ensuring that the dataset is distributed in a representative manner. Further, we propose a multimodal deep learning approach to detect hardware Trojans and evaluate the results from both early fusion and late fusion strategies. We also estimate the uncertainty quantification metrics of each prediction for risk-aware decision-making. The outcomes not only confirms the efficacy of our
&lt;/p&gt;</description></item><item><title>Triamese-ViT&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;ViT&#27169;&#22411;&#36866;&#24212;&#33041;&#40836;&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#26469;&#33258;&#19977;&#20010;&#19981;&#21516;&#26041;&#21521;&#30340;ViTs&#26469;&#25429;&#25417;&#19977;&#32500;&#20449;&#24687;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.09475</link><description>&lt;p&gt;
Triamese-ViT: &#19968;&#31181;&#22522;&#20110;&#19977;&#32500;&#24863;&#30693;&#30340;&#31283;&#20581;&#33041;&#40836;&#20272;&#35745;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Triamese-ViT: A 3D-Aware Method for Robust Brain Age Estimation from MRIs. (arXiv:2401.09475v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09475
&lt;/p&gt;
&lt;p&gt;
Triamese-ViT&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;ViT&#27169;&#22411;&#36866;&#24212;&#33041;&#40836;&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#26469;&#33258;&#19977;&#20010;&#19981;&#21516;&#26041;&#21521;&#30340;ViTs&#26469;&#25429;&#25417;&#19977;&#32500;&#20449;&#24687;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#24212;&#29992;&#26174;&#33879;&#25552;&#39640;&#20102;&#35786;&#26029;&#31934;&#24230;&#65292;&#29305;&#21035;&#22312;&#35299;&#26512;&#22797;&#26434;&#32467;&#26500;&#22914;&#20154;&#33041;&#26041;&#38754;&#12290;&#35786;&#26029;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#31561;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#30142;&#30149;&#25512;&#21160;&#20102;&#33041;&#40836;&#20272;&#35745;&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#21033;&#29992;&#19977;&#32500;&#30913;&#20849;&#25391;&#25104;&#20687;(MRI)&#25195;&#25551;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#19977;&#32500;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNNs)&#22914;3D ResNet&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#19977;&#32500;&#29256;&#26412;&#30340;ViT&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#22240;&#27492;&#20173;&#26410;&#20805;&#20998;&#21457;&#25381;ViT&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Triamese-ViT&#65292;&#36825;&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;ViT&#27169;&#22411;&#36866;&#24212;&#33041;&#40836;&#20272;&#35745;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#36890;&#36807;&#32467;&#21512;&#26469;&#33258;&#19977;&#20010;&#19981;&#21516;&#26041;&#21521;&#30340;ViTs&#26469;&#25429;&#25417;&#19977;&#32500;&#20449;&#24687;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#19968;&#20010;&#21253;&#21547;1351&#20010;MRI&#25195;&#25551;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;Triamese-ViT&#23454;&#29616;&#20102;&#19968;&#20010;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65288;MAE&#65289;...
&lt;/p&gt;
&lt;p&gt;
The integration of machine learning in medicine has significantly improved diagnostic precision, particularly in the interpretation of complex structures like the human brain. Diagnosing challenging conditions such as Alzheimer's disease has prompted the development of brain age estimation techniques. These methods often leverage three-dimensional Magnetic Resonance Imaging (MRI) scans, with recent studies emphasizing the efficacy of 3D convolutional neural networks (CNNs) like 3D ResNet. However, the untapped potential of Vision Transformers (ViTs), known for their accuracy and interpretability, persists in this domain due to limitations in their 3D versions. This paper introduces Triamese-ViT, an innovative adaptation of the ViT model for brain age estimation. Our model uniquely combines ViTs from three different orientations to capture 3D information, significantly enhancing accuracy and interpretability. Tested on a dataset of 1351 MRI scans, Triamese-ViT achieves a Mean Absolute E
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#22810;&#21442;&#25968;mpMRI&#25195;&#25551;&#26469;&#39044;&#27979;&#33014;&#36136;&#27597;&#32454;&#32990;&#30244;&#20013;&#30340;MGMT&#29983;&#29289;&#26631;&#24535;&#29289;&#29366;&#24577;&#65292;&#32467;&#26524;&#34920;&#26126;ViT3D&#21644;Xception&#27169;&#22411;&#20855;&#26377;&#20248;&#21183;&#65292;&#21487;&#20316;&#20026;&#26377;&#25928;&#30340;&#20998;&#31867;&#24037;&#20855;&#36827;&#34892;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.09471</link><description>&lt;p&gt;
&#33041;&#32959;&#30244;&#25918;&#23556;&#22522;&#22240;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Brain Tumor Radiogenomic Classification. (arXiv:2401.09471v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09471
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#22810;&#21442;&#25968;mpMRI&#25195;&#25551;&#26469;&#39044;&#27979;&#33014;&#36136;&#27597;&#32454;&#32990;&#30244;&#20013;&#30340;MGMT&#29983;&#29289;&#26631;&#24535;&#29289;&#29366;&#24577;&#65292;&#32467;&#26524;&#34920;&#26126;ViT3D&#21644;Xception&#27169;&#22411;&#20855;&#26377;&#20248;&#21183;&#65292;&#21487;&#20316;&#20026;&#26377;&#25928;&#30340;&#20998;&#31867;&#24037;&#20855;&#36827;&#34892;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
RSNA-MICCAI&#33041;&#32959;&#30244;&#25918;&#23556;&#22522;&#22240;&#20998;&#31867;&#25361;&#25112;&#26088;&#22312;&#36890;&#36807;&#23545;&#22810;&#21442;&#25968;mpMRI&#25195;&#25551;&#65288;T1w&#65292;T1wCE&#65292;T2w&#21644;FLAIR&#65289;&#36827;&#34892;&#20108;&#20998;&#31867;&#26469;&#39044;&#27979;&#33014;&#36136;&#27597;&#32454;&#32990;&#30244;&#20013;&#30340;MGMT&#29983;&#29289;&#26631;&#24535;&#29289;&#29366;&#24577;&#12290;&#25968;&#25454;&#38598;&#20998;&#20026;&#35757;&#32451;&#38598;&#12289;&#39564;&#35777;&#38598;&#21644;&#27979;&#35797;&#38598;&#12290;&#20351;&#29992;&#20102;&#19981;&#21516;&#30340;&#26550;&#26500;&#26469;&#30740;&#31350;&#35813;&#38382;&#39064;&#65292;&#21253;&#25324;3D&#29256;&#26412;&#30340;Vision Transformer&#65288;ViT3D&#65289;&#12289;ResNet50&#12289;Xception&#21644;EfficientNet-B3&#12290;AUC&#34987;&#29992;&#20316;&#20027;&#35201;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#32467;&#26524;&#26174;&#31034;ViT3D&#21644;Xception&#27169;&#22411;&#22312;&#27979;&#35797;&#38598;&#19978;&#20998;&#21035;&#36798;&#21040;&#20102;0.6015&#21644;0.61745&#30340;&#20248;&#21183;&#12290;&#19982;&#20854;&#20182;&#32467;&#26524;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#22312;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#19979;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#12290;&#36890;&#36807;&#25506;&#32034;&#19981;&#21516;&#30340;&#31574;&#30053;&#12289;&#19981;&#21516;&#30340;&#26550;&#26500;&#21644;&#26356;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#21487;&#20197;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
The RSNA-MICCAI brain tumor radiogenomic classification challenge aimed to predict MGMT biomarker status in glioblastoma through binary classification on Multi parameter mpMRI scans: T1w, T1wCE, T2w and FLAIR. The dataset is splitted into three main cohorts: training set, validation set which were used during training, and the testing were only used during final evaluation. Images were either in a DICOM format or in Png format. different architectures were used to investigate the problem including the 3D version of Vision Transformer (ViT3D), ResNet50, Xception and EfficientNet-B3. AUC was used as the main evaluation metric and the results showed an advantage for both the ViT3D and the Xception models achieving 0.6015 and 0.61745 respectively on the testing set. compared to other results, our results proved to be valid given the complexity of the task. further improvements can be made through exploring different strategies, different architectures and more diverse datasets.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#19979;&#35843;&#23610;&#24230; ESM &#27169;&#25311;&#25968;&#25454;&#65292;&#19981;&#38656;&#35201;&#39640;&#20998;&#36776;&#29575;&#30340;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2401.09466</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#35270;&#35273;&#29992;&#20110;&#27668;&#20505;&#19979;&#35843;&#23610;&#24230;
&lt;/p&gt;
&lt;p&gt;
Self Supervised Vision for Climate Downscaling. (arXiv:2401.09466v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09466
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#19979;&#35843;&#23610;&#24230; ESM &#27169;&#25311;&#25968;&#25454;&#65292;&#19981;&#38656;&#35201;&#39640;&#20998;&#36776;&#29575;&#30340;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27668;&#20505;&#21464;&#21270;&#26159;&#25105;&#20204;&#26143;&#29699;&#38754;&#20020;&#30340;&#26368;&#37325;&#35201;&#30340;&#25361;&#25112;&#20043;&#19968;&#12290;&#20840;&#29699;&#27668;&#28201;&#30340;&#19978;&#21319;&#24050;&#32463;&#24102;&#26469;&#20102;&#23545;&#22320;&#29699;&#27668;&#20505;&#21644;&#22825;&#27668;&#27169;&#24335;&#30340;&#26174;&#33879;&#21464;&#21270;&#65292;&#19981;&#21487;&#39044;&#27979;&#21644;&#26497;&#31471;&#22825;&#27668;&#20107;&#20214;&#30340;&#39057;&#29575;&#22686;&#21152;&#12290;&#27668;&#20505;&#21464;&#21270;&#30740;&#31350;&#30340;&#26410;&#26469;&#39044;&#27979;&#22522;&#20110;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#65288;ESMs&#65289;&#65292;&#36825;&#20123;&#35745;&#31639;&#26426;&#27169;&#22411;&#27169;&#25311;&#22320;&#29699;&#30340;&#27668;&#20505;&#31995;&#32479;&#12290; ESMs &#25552;&#20379;&#20102;&#25972;&#21512;&#21508;&#31181;&#29289;&#29702;&#31995;&#32479;&#30340;&#26694;&#26550;&#65292;&#20294;&#23427;&#20204;&#30340;&#36755;&#20986;&#21463;&#21040;&#36816;&#34892;&#21644;&#23384;&#26723;&#26356;&#39640;&#20998;&#36776;&#29575;&#27169;&#25311;&#25152;&#38656;&#30340;&#24040;&#22823;&#35745;&#31639;&#36164;&#28304;&#30340;&#38480;&#21046;&#12290;&#23545;&#20110;&#32473;&#23450;&#30340;&#36164;&#28304;&#39044;&#31639;&#65292;ESMs &#36890;&#24120;&#22312;&#36739;&#31895;&#30340;&#32593;&#26684;&#19978;&#36816;&#34892;&#65292;&#28982;&#21518;&#36827;&#34892;&#35745;&#31639;&#37327;&#36739;&#23567;&#30340;&#8220;&#19979;&#35843;&#23610;&#24230;&#8221;&#36807;&#31243;&#20197;&#33719;&#24471;&#26356;&#39640;&#20998;&#36776;&#29575;&#30340;&#36755;&#20986;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#19979;&#35843;&#23610;&#24230; ESM &#27169;&#25311;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#19981;&#38656;&#35201;&#39640;&#20998;&#36776;&#29575;&#30340;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#20248;&#21270;&#12290;&#36825;&#26159;&#36890;&#36807;&#21033;&#29992;&#26174;&#33879;&#30340;&#25968;&#25454;&#20998;&#24067;&#26469;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Climate change is one of the most critical challenges that our planet is facing today. Rising global temperatures are already bringing noticeable changes to Earth's weather and climate patterns with an increased frequency of unpredictable and extreme weather events. Future projections for climate change research are based on Earth System Models (ESMs), the computer models that simulate the Earth's climate system. ESMs provide a framework to integrate various physical systems, but their output is bound by the enormous computational resources required for running and archiving higher-resolution simulations. For a given resource budget, the ESMs are generally run on a coarser grid, followed by a computationally lighter $downscaling$ process to obtain a finer-resolution output. In this work, we present a deep-learning model for downscaling ESM simulation data that does not require high-resolution ground truth data for model optimization. This is realized by leveraging salient data distribu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#19968;&#32423;&#21407;&#29702;&#20986;&#21457;&#65292;&#25512;&#23548;&#20986;&#20102;&#21487;&#20197;&#22312;&#36125;&#21494;&#26031;&#30693;&#35782;&#36861;&#36394;&#30340;&#21442;&#25968;&#31354;&#38388;&#19978;&#26045;&#21152;&#30340;&#32422;&#26463;&#65292;&#35299;&#20915;&#20102;&#30446;&#21069;&#31639;&#27861;&#20013;&#23384;&#22312;&#30340;&#19968;&#31995;&#21015;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.09456</link><description>&lt;p&gt;
&#20174;&#19968;&#32423;&#21407;&#29702;&#20986;&#21457;&#30340;&#36125;&#21494;&#26031;&#30693;&#35782;&#36861;&#36394;&#30340;&#21442;&#25968;&#32422;&#26463;
&lt;/p&gt;
&lt;p&gt;
Parametric Constraints for Bayesian Knowledge Tracing from First Principles. (arXiv:2401.09456v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09456
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#19968;&#32423;&#21407;&#29702;&#20986;&#21457;&#65292;&#25512;&#23548;&#20986;&#20102;&#21487;&#20197;&#22312;&#36125;&#21494;&#26031;&#30693;&#35782;&#36861;&#36394;&#30340;&#21442;&#25968;&#31354;&#38388;&#19978;&#26045;&#21152;&#30340;&#32422;&#26463;&#65292;&#35299;&#20915;&#20102;&#30446;&#21069;&#31639;&#27861;&#20013;&#23384;&#22312;&#30340;&#19968;&#31995;&#21015;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#30693;&#35782;&#36861;&#36394;(BKT)&#26159;&#19968;&#20010;&#23398;&#20064;&#32773;&#25484;&#25569;&#29366;&#24577;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#23545;&#24212;&#19968;&#20010;&#30693;&#35782;&#32452;&#20214;&#12290;&#23427;&#23558;&#23398;&#20064;&#32773;&#30340;&#25484;&#25569;&#29366;&#24577;&#35270;&#20026;&#19968;&#20010;&#8220;&#38544;&#34255;&#8221;&#30340;&#25110;&#28508;&#22312;&#30340;&#20108;&#20803;&#21464;&#37327;&#65292;&#24182;&#26681;&#25454;&#23398;&#20064;&#32773;&#21709;&#24212;&#30340;&#27491;&#30830;&#24615;&#26356;&#26032;&#27492;&#29366;&#24577;&#65292;&#20351;&#29992;&#20195;&#34920;&#29366;&#24577;&#36716;&#25442;&#27010;&#29575;&#30340;&#21442;&#25968;&#12290;BKT&#36890;&#24120;&#34987;&#34920;&#31034;&#20026;&#19968;&#20010;&#38544;&#34255;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65292;&#32780;&#26399;&#26395;&#26368;&#22823;&#21270;(EM)&#31639;&#27861;&#29992;&#20110;&#25512;&#26029;&#36825;&#20123;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#35813;&#31639;&#27861;&#21487;&#33021;&#38754;&#20020;&#22810;&#32452;&#21487;&#34892;&#30340;&#21442;&#25968;&#38598;&#12289;&#38519;&#20837;&#23616;&#37096;&#26368;&#23567;&#20540;&#12289;&#20135;&#29983;&#36864;&#21270;&#30340;&#21442;&#25968;&#20540;&#20197;&#21450;&#25311;&#21512;&#36807;&#31243;&#20013;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#31561;&#38382;&#39064;&#12290;&#26412;&#25991;&#37319;&#29992;&#8220;&#20174;&#19968;&#32423;&#21407;&#29702;&#8221;&#26041;&#27861;&#65292;&#25512;&#23548;&#20986;&#21487;&#20197;&#23545;BKT&#21442;&#25968;&#31354;&#38388;&#26045;&#21152;&#30340;&#32422;&#26463;&#12290;&#20174;&#27010;&#29575;&#30340;&#22522;&#26412;&#25968;&#23398;&#30495;&#29702;&#20986;&#21457;&#65292;&#36880;&#27493;&#24314;&#31435;&#23545;&#20110;BKT&#21442;&#25968;&#22312;&#30495;&#23454;&#31995;&#32479;&#20013;&#25152;&#26399;&#26395;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian Knowledge Tracing (BKT) is a probabilistic model of a learner's state of mastery corresponding to a knowledge component. It considers the learner's state of mastery as a "hidden" or latent binary variable and updates this state based on the observed correctness of the learner's response using parameters that represent transition probabilities between states. BKT is often represented as a Hidden Markov Model and the Expectation-Maximization (EM) algorithm is used to infer these parameters. However, this algorithm can suffer from several issues including producing multiple viable sets of parameters, settling into a local minima, producing degenerate parameter values, and a high computational cost during fitting. This paper takes a "from first principles" approach to deriving constraints that can be imposed on the BKT parameter space. Starting from the basic mathematical truths of probability and building up to the behaviors expected of the BKT parameters in real systems, this pa
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32422;&#26463;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#38598;&#25104;&#21355;&#26143;-&#22320;&#38754;&#32593;&#32476;&#65288;ISTN&#65289;&#31995;&#32479;&#30340;&#21160;&#24577;&#36335;&#30001;&#38382;&#39064;&#65292;&#26377;&#25928;&#24179;&#34913;&#20102;&#24555;&#36895;&#36890;&#20449;&#12289;&#33021;&#28304;&#25928;&#29575;&#21644;&#25968;&#25454;&#21253;&#20002;&#22833;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2401.09455</link><description>&lt;p&gt;
&#38598;&#25104;&#21355;&#26143;-&#22320;&#38754;&#32593;&#32476;&#30340;&#21160;&#24577;&#36335;&#30001;&#65306;&#22522;&#20110;&#32422;&#26463;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Dynamic Routing for Integrated Satellite-Terrestrial Networks: A Constrained Multi-Agent Reinforcement Learning Approach. (arXiv:2401.09455v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32422;&#26463;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#38598;&#25104;&#21355;&#26143;-&#22320;&#38754;&#32593;&#32476;&#65288;ISTN&#65289;&#31995;&#32479;&#30340;&#21160;&#24577;&#36335;&#30001;&#38382;&#39064;&#65292;&#26377;&#25928;&#24179;&#34913;&#20102;&#24555;&#36895;&#36890;&#20449;&#12289;&#33021;&#28304;&#25928;&#29575;&#21644;&#25968;&#25454;&#21253;&#20002;&#22833;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#25104;&#21355;&#26143;-&#22320;&#38754;&#32593;&#32476;&#65288;ISTN&#65289;&#31995;&#32479;&#32463;&#21382;&#20102;&#26174;&#33879;&#22686;&#38271;&#65292;&#20026;&#20559;&#36828;&#22320;&#21306;&#25552;&#20379;&#20102;&#26080;&#32541;&#36890;&#20449;&#26381;&#21153;&#65292;&#35299;&#20915;&#20102;&#26377;&#38480;&#30340;&#22320;&#38754;&#22522;&#30784;&#35774;&#26045;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20026;ISTN&#35774;&#35745;&#36335;&#30001;&#26041;&#26696;&#26497;&#20855;&#25361;&#25112;&#24615;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#22686;&#21152;&#20102;&#22320;&#38754;&#31449;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#35201;&#27714;&#28385;&#36275;&#19982;&#21355;&#26143;&#26381;&#21153;&#36136;&#37327;&#26377;&#20851;&#30340;&#21508;&#31181;&#32422;&#26463;&#26465;&#20214;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19982;&#22320;&#38754;&#31449;&#21644;&#21355;&#26143;&#20849;&#21516;&#20256;&#36755;&#25968;&#25454;&#21253;&#30340;&#36335;&#30001;&#65292;&#21516;&#26102;&#20248;&#20808;&#32771;&#34385;&#24555;&#36895;&#36890;&#20449;&#12289;&#28385;&#36275;&#33021;&#28304;&#25928;&#29575;&#21644;&#25968;&#25454;&#21253;&#20002;&#22833;&#35201;&#27714;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#25289;&#26684;&#26391;&#26085;&#26041;&#27861;&#23558;&#24102;&#32422;&#26463;&#30340;&#25968;&#25454;&#21253;&#36335;&#30001;&#38382;&#39064;&#21046;&#23450;&#20026;&#19968;&#20010;&#26368;&#22823;&#26368;&#23567;&#21270;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CMADR&#30340;&#26032;&#39062;&#32422;&#26463;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#21160;&#24577;&#36335;&#30001;&#31639;&#27861;&#65292;&#23427;&#26377;&#25928;&#22320;&#24179;&#34913;&#30446;&#26631;&#25913;&#21892;&#21644;&#32422;&#26463;&#28385;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;
The integrated satellite-terrestrial network (ISTN) system has experienced significant growth, offering seamless communication services in remote areas with limited terrestrial infrastructure. However, designing a routing scheme for ISTN is exceedingly difficult, primarily due to the heightened complexity resulting from the inclusion of additional ground stations, along with the requirement to satisfy various constraints related to satellite service quality. To address these challenges, we study packet routing with ground stations and satellites working jointly to transmit packets, while prioritizing fast communication and meeting energy efficiency and packet loss requirements. Specifically, we formulate the problem of packet routing with constraints as a max-min problem using the Lagrange method. Then we propose a novel constrained Multi-Agent reinforcement learning (MARL) dynamic routing algorithm named CMADR, which efficiently balances objective improvement and constraint satisfacti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#29992;&#25143;&#27880;&#35270;&#27880;&#24847;&#21147;&#23545;&#40784;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#22312;&#22788;&#29702;&#22797;&#26434;&#22330;&#26223;&#21644;&#22810;&#20010;&#29289;&#20307;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.09454</link><description>&lt;p&gt;
Voila-A: &#29992;&#29992;&#25143;&#27880;&#35270;&#27880;&#24847;&#21147;&#23545;&#40784;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Voila-A: Aligning Vision-Language Models with User's Gaze Attention. (arXiv:2401.09454v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09454
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#29992;&#25143;&#27880;&#35270;&#27880;&#24847;&#21147;&#23545;&#40784;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#22312;&#22788;&#29702;&#22797;&#26434;&#22330;&#26223;&#21644;&#22810;&#20010;&#29289;&#20307;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#20960;&#24180;&#20013;&#65292;&#35270;&#35273;&#21644;&#35821;&#35328;&#29702;&#35299;&#30340;&#25972;&#21512;&#36890;&#36807;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#35201;&#31361;&#30772;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;VLMs&#22312;&#22788;&#29702;&#22797;&#26434;&#22330;&#26223;&#21644;&#22810;&#20010;&#29289;&#20307;&#30340;&#23454;&#38469;&#24212;&#29992;&#20197;&#21450;&#19982;&#20154;&#31867;&#29992;&#25143;&#30340;&#22810;&#26679;&#21270;&#27880;&#24847;&#21147;&#27169;&#24335;&#30456;&#19968;&#33268;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#36890;&#36807;&#22686;&#24378;&#29616;&#23454;&#65288;AR&#65289;&#25110;&#34394;&#25311;&#29616;&#23454;&#65288;VR&#65289;&#35774;&#22791;&#25910;&#38598;&#30340;&#27880;&#35270;&#20449;&#24687;&#65292;&#20316;&#20026;&#20154;&#31867;&#27880;&#24847;&#21147;&#30340;&#20195;&#29702;&#26469;&#24341;&#23548;VLMs&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;Voila-A&#65292;&#20197;&#25552;&#39640;&#36825;&#20123;&#27169;&#22411;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#25928;&#26524;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#25968;&#30334;&#20998;&#38047;&#30340;&#27880;&#35270;&#25968;&#25454;&#65292;&#20197;&#23637;&#31034;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#26412;&#22320;&#21270;&#30340;&#21465;&#20107;&#26469;&#27169;&#25311;&#20154;&#31867;&#30340;&#27880;&#35270;&#26041;&#24335;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#33258;&#21160;&#25968;&#25454;&#27880;&#37322;&#27969;&#27700;&#32447;&#65292;&#21033;&#29992;GPT-4&#29983;&#25104;&#20102;VOILA-COCO&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21019;&#26032;&#20102;Voila Perceiver&#27169;&#22359;&#65292;&#23558;&#27880;&#35270;&#20449;&#24687;&#25972;&#21512;&#21040;VL&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the integration of vision and language understanding has led to significant advancements in artificial intelligence, particularly through Vision-Language Models (VLMs). However, existing VLMs face challenges in handling real-world applications with complex scenes and multiple objects, as well as aligning their focus with the diverse attention patterns of human users. In this paper, we introduce gaze information, feasibly collected by AR or VR devices, as a proxy for human attention to guide VLMs and propose a novel approach, Voila-A, for gaze alignment to enhance the interpretability and effectiveness of these models in real-world applications. First, we collect hundreds of minutes of gaze data to demonstrate that we can mimic human gaze modalities using localized narratives. We then design an automatic data annotation pipeline utilizing GPT-4 to generate the VOILA-COCO dataset. Additionally, we innovate the Voila Perceiver modules to integrate gaze information into VL
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#40654;&#26364;&#20960;&#20309;&#29305;&#24449;&#24212;&#29992;&#20110;&#23398;&#20064;&#32764;&#38754;&#21387;&#21147;&#31995;&#25968;&#20998;&#24067;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#27668;&#21160;&#31995;&#25968;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.09452</link><description>&lt;p&gt;
&#20351;&#29992;&#40654;&#26364;&#20960;&#20309;&#29305;&#24449;&#23398;&#20064;&#39134;&#26426;&#26426;&#32764;&#19978;&#30340;&#21387;&#21147;&#31995;&#25968;
&lt;/p&gt;
&lt;p&gt;
Incorporating Riemannian Geometric Features for Learning Coefficient of Pressure Distributions on Airplane Wings. (arXiv:2401.09452v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09452
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#40654;&#26364;&#20960;&#20309;&#29305;&#24449;&#24212;&#29992;&#20110;&#23398;&#20064;&#32764;&#38754;&#21387;&#21147;&#31995;&#25968;&#20998;&#24067;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#27668;&#21160;&#31995;&#25968;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39134;&#26426;&#30340;&#27668;&#21160;&#31995;&#25968;&#21463;&#20854;&#20960;&#20309;&#24418;&#29366;&#30340;&#26174;&#33879;&#24433;&#21709;&#65292;&#23588;&#20854;&#26159;&#24403;&#25915;&#35282;&#36739;&#22823;&#26102;&#12290;&#22312;&#31354;&#27668;&#21160;&#21147;&#23398;&#39046;&#22495;&#65292;&#20256;&#32479;&#30340;&#22522;&#20110;&#22810;&#39033;&#24335;&#30340;&#21442;&#25968;&#21270;&#26041;&#27861;&#20351;&#29992;&#23613;&#21487;&#33021;&#23569;&#30340;&#21442;&#25968;&#26469;&#25551;&#36848;&#32764;&#22411;&#30340;&#20960;&#20309;&#24418;&#29366;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32764;&#30340;&#19977;&#32500;&#20960;&#20309;&#24418;&#29366;&#27604;&#20108;&#32500;&#32764;&#22411;&#22797;&#26434;&#65292;&#22522;&#20110;&#22810;&#39033;&#24335;&#30340;&#21442;&#25968;&#21270;&#26041;&#27861;&#38590;&#20197;&#20934;&#30830;&#34920;&#31034;&#32764;&#22312;&#19977;&#32500;&#31354;&#38388;&#20013;&#30340;&#25972;&#20307;&#24418;&#29366;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#21487;&#20197;&#25552;&#21462;&#29992;&#20110;&#25551;&#36848;&#20108;&#32500;&#32764;&#22411;&#25110;&#32764;&#25130;&#38754;&#24418;&#29366;&#30340;&#22823;&#37327;&#28508;&#22312;&#31070;&#32463;&#34920;&#31034;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#30452;&#25509;&#23558;&#20960;&#20309;&#29305;&#24449;&#20316;&#20026;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20837;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#30340;&#27668;&#21160;&#31995;&#25968;&#30340;&#20934;&#30830;&#24615;&#12290;&#21463;&#20960;&#20309;&#29702;&#35770;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#40654;&#26364;&#20960;&#20309;&#29305;&#24449;&#32435;&#20837;&#23398;&#20064;&#32764;&#38754;&#21387;&#21147;&#31995;&#25968;&#20998;&#24067;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35745;&#31639;&#20960;&#20309;&#29305;&#24449;&#65288;&#40654;&#26364;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The aerodynamic coefficients of aircrafts are significantly impacted by its geometry, especially when the angle of attack (AoA) is large. In the field of aerodynamics, traditional polynomial-based parameterization uses as few parameters as possible to describe the geometry of an airfoil. However, because the 3D geometry of a wing is more complicated than the 2D airfoil, polynomial-based parameterizations have difficulty in accurately representing the entire shape of a wing in 3D space. Existing deep learning-based methods can extract massive latent neural representations for the shape of 2D airfoils or 2D slices of wings. Recent studies highlight that directly taking geometric features as inputs to the neural networks can improve the accuracy of predicted aerodynamic coefficients. Motivated by geometry theory, we propose to incorporate Riemannian geometric features for learning Coefficient of Pressure (CP) distributions on wing surfaces. Our method calculates geometric features (Rieman
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#39537;&#21160;&#30340;&#29983;&#25104;&#26694;&#26550;\method{}&#65292;&#29992;&#20110;&#39044;&#27979;&#20998;&#23376;&#30340;&#19977;&#32500;&#26500;&#35937;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#39044;&#27979;&#31934;&#24230;&#24182;&#25913;&#36827;&#20102;&#20256;&#32479;&#26041;&#27861;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2401.09451</link><description>&lt;p&gt;
&#25193;&#25955;&#39537;&#21160;&#30340;&#20998;&#23376;&#26500;&#35937;&#39044;&#27979;&#29983;&#25104;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Diffusion-Driven Generative Framework for Molecular Conformation Prediction. (arXiv:2401.09451v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09451
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#39537;&#21160;&#30340;&#29983;&#25104;&#26694;&#26550;\method{}&#65292;&#29992;&#20110;&#39044;&#27979;&#20998;&#23376;&#30340;&#19977;&#32500;&#26500;&#35937;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#39044;&#27979;&#31934;&#24230;&#24182;&#25913;&#36827;&#20102;&#20256;&#32479;&#26041;&#27861;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20108;&#32500;&#22270;&#24418;&#34920;&#31034;&#20013;&#25512;&#26029;&#20986;&#19977;&#32500;&#20998;&#23376;&#26500;&#22411;&#30340;&#20219;&#21153;&#22312;&#35745;&#31639;&#21270;&#23398;&#21644;&#33647;&#29289;&#24320;&#21457;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#23427;&#23545;&#25105;&#20204;&#29702;&#35299;&#20998;&#23376;&#26426;&#21046;&#21644;&#30456;&#20114;&#20316;&#29992;&#36215;&#30528;&#22522;&#26412;&#20316;&#29992;&#12290;&#26426;&#22120;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#29983;&#25104;&#32593;&#32476;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#25512;&#21160;&#20102;&#36825;&#31181;&#39044;&#27979;&#24314;&#27169;&#31934;&#24230;&#30340;&#31361;&#30772;&#12290;&#20256;&#32479;&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#20998;&#21449;&#31574;&#30053;&#65306;&#39318;&#20808;&#20272;&#35745;&#21407;&#23376;&#38388;&#36317;&#65292;&#28982;&#21518;&#36890;&#36807;&#35299;&#20915;&#36317;&#31163;&#20960;&#20309;&#38382;&#39064;&#26469;&#22609;&#36896;&#20998;&#23376;&#30340;&#31354;&#38388;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#39034;&#24207;&#26041;&#27861;&#26377;&#26102;&#26080;&#27861;&#20934;&#30830;&#25429;&#25417;&#21040;&#23616;&#37096;&#21407;&#23376;&#25490;&#21015;&#30340;&#22797;&#26434;&#24615;&#65292;&#20174;&#32780;&#25439;&#23475;&#32467;&#26524;&#32467;&#26500;&#27169;&#22411;&#30340;&#23436;&#25972;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#19981;&#36275;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#21069;&#21355;&#30340;&#29983;&#25104;&#26694;&#26550;&#65306;\method{}&#65292;&#23427;&#22522;&#20110;&#25193;&#25955;&#39537;&#21160;&#30340;&#26041;&#27861;&#36827;&#34892;&#39044;&#27979;&#65292;&#24182;&#21462;&#24471;&#20102;&#37325;&#35201;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of inferring three-dimensional molecular configurations from their two-dimensional graph representations is of critical significance in the domains of computational chemistry and the development of pharmaceuticals. It contributes fundamentally to our grasp of molecular mechanisms and interactions. The rapid evolution of machine learning, especially in the realm of deep generative networks, has catalyzed breakthroughs in the precision of such predictive modeling. Traditional methodologies typically employ a bifurcated strategy: initially estimating interatomic distances followed by sculpting the spatial molecular structure via solving a distance geometry problem. This sequential approach, however, occasionally fails to capture the intricacies of local atomic arrangements accurately, thus compromising the integrity of the resultant structural models. Addressing these deficiencies, this work introduces an avant-garde generative framework: \method{}, which is predicated on the dif
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#26041;&#27861;&#26469;&#35299;&#37322;&#23391;&#21152;&#25289;&#35821;Memes&#30340;&#24773;&#24863;&#65292;&#20197;&#22635;&#34917;&#27492;&#39046;&#22495;&#20013;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;&#23545;&#27604;&#29616;&#26377;&#30340;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;MemoSen&#25968;&#25454;&#38598;&#24182;&#34920;&#26126;&#20854;&#20934;&#30830;&#29575;&#30340;&#23616;&#38480;&#24615;&#12290;&#36825;&#39033;&#30740;&#31350;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#22312;&#23391;&#21152;&#25289;&#35821;Memes&#24773;&#24863;&#20998;&#26512;&#39046;&#22495;&#24341;&#20837;&#20102;&#22810;&#27169;&#24577;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.09446</link><description>&lt;p&gt;
&#12298;&#21487;&#35299;&#37322;&#30340;&#23391;&#21152;&#25289;&#35821;Memes&#30340;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#12299;
&lt;/p&gt;
&lt;p&gt;
Explainable Multimodal Sentiment Analysis on Bengali Memes. (arXiv:2401.09446v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09446
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#26041;&#27861;&#26469;&#35299;&#37322;&#23391;&#21152;&#25289;&#35821;Memes&#30340;&#24773;&#24863;&#65292;&#20197;&#22635;&#34917;&#27492;&#39046;&#22495;&#20013;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;&#23545;&#27604;&#29616;&#26377;&#30340;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;MemoSen&#25968;&#25454;&#38598;&#24182;&#34920;&#26126;&#20854;&#20934;&#30830;&#29575;&#30340;&#23616;&#38480;&#24615;&#12290;&#36825;&#39033;&#30740;&#31350;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#22312;&#23391;&#21152;&#25289;&#35821;Memes&#24773;&#24863;&#20998;&#26512;&#39046;&#22495;&#24341;&#20837;&#20102;&#22810;&#27169;&#24577;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Memes&#24050;&#25104;&#20026;&#25968;&#23383;&#26102;&#20195;&#29420;&#29305;&#32780;&#26377;&#25928;&#30340;&#27807;&#36890;&#24418;&#24335;&#65292;&#21560;&#24341;&#20102;&#22312;&#32447;&#31038;&#21306;&#65292;&#24182;&#36328;&#36234;&#25991;&#21270;&#38556;&#30861;&#12290;&#23613;&#31649;Memes&#32463;&#24120;&#21644;&#24189;&#40664;&#32852;&#31995;&#22312;&#19968;&#36215;&#65292;&#20294;&#23427;&#20204;&#26377;&#30528;&#20256;&#36798;&#24191;&#27867;&#24773;&#24863;&#30340;&#24778;&#20154;&#33021;&#21147;&#65292;&#21253;&#25324;&#24555;&#20048;&#12289;&#35773;&#21050;&#12289;&#27822;&#20007;&#31561;&#12290;&#22312;&#20449;&#24687;&#26102;&#20195;&#65292;&#29702;&#35299;&#21644;&#35299;&#37322;Memes&#32972;&#21518;&#30340;&#24773;&#24863;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#25506;&#32034;&#20102;&#22522;&#20110;&#25991;&#26412;&#12289;&#22522;&#20110;&#22270;&#20687;&#21644;&#22810;&#27169;&#24577;&#26041;&#27861;&#65292;&#23548;&#33268;&#20102;&#20687;CAPSAN&#21644;PromptHate&#36825;&#26679;&#30340;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#29992;&#20110;&#26816;&#27979;&#21508;&#31181;Memes&#31867;&#21035;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#23391;&#21152;&#25289;&#35821;Memes&#36825;&#26679;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#30740;&#31350;&#20173;&#28982;&#31232;&#32570;&#65292;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#25968;&#37327;&#26377;&#38480;&#12290;&#26368;&#36817;&#30340;&#19968;&#20010;&#36129;&#29486;&#26159;&#24341;&#20837;&#20102;MemoSen&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#25152;&#23454;&#29616;&#30340;&#20934;&#30830;&#29575;&#26126;&#26174;&#36739;&#20302;&#65292;&#24182;&#19988;&#25968;&#25454;&#38598;&#20998;&#24067;&#19981;&#24179;&#34913;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;ResNet50&#21644;&#22810;&#27169;&#24577;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Memes have become a distinctive and effective form of communication in the digital era, attracting online communities and cutting across cultural barriers. Even though memes are frequently linked with humor, they have an amazing capacity to convey a wide range of emotions, including happiness, sarcasm, frustration, and more. Understanding and interpreting the sentiment underlying memes has become crucial in the age of information. Previous research has explored text-based, image-based, and multimodal approaches, leading to the development of models like CAPSAN and PromptHate for detecting various meme categories. However, the study of low-resource languages like Bengali memes remains scarce, with limited availability of publicly accessible datasets. A recent contribution includes the introduction of the MemoSen dataset. However, the achieved accuracy is notably low, and the dataset suffers from imbalanced distribution. In this study, we employed a multimodal approach using ResNet50 and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21327;&#21516;&#34920;&#31034;&#27169;&#22411;&#30340;&#22270;&#20687;&#34917;&#19969;&#36317;&#31163;&#35745;&#31639;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#26597;&#35810;&#22270;&#20687;&#21644;&#23384;&#20648;&#30340;&#34917;&#19969;&#20043;&#38388;&#30340;&#26368;&#36817;&#37051;&#25628;&#32034;&#25152;&#24102;&#26469;&#30340;&#22797;&#26434;&#24230;&#38382;&#39064;&#65292;&#33021;&#22815;&#22312;&#36793;&#32536;&#29615;&#22659;&#19979;&#36827;&#34892;&#24555;&#36895;&#37096;&#32626;&#23454;&#29992;&#30340;&#24322;&#24120;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2401.09443</link><description>&lt;p&gt;
CRD: &#23454;&#29992;&#24322;&#24120;&#26816;&#27979;&#30340;&#21327;&#21516;&#34920;&#31034;&#36317;&#31163;
&lt;/p&gt;
&lt;p&gt;
CRD: Collaborative Representation Distance for Practical Anomaly Detection. (arXiv:2401.09443v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09443
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21327;&#21516;&#34920;&#31034;&#27169;&#22411;&#30340;&#22270;&#20687;&#34917;&#19969;&#36317;&#31163;&#35745;&#31639;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#26597;&#35810;&#22270;&#20687;&#21644;&#23384;&#20648;&#30340;&#34917;&#19969;&#20043;&#38388;&#30340;&#26368;&#36817;&#37051;&#25628;&#32034;&#25152;&#24102;&#26469;&#30340;&#22797;&#26434;&#24230;&#38382;&#39064;&#65292;&#33021;&#22815;&#22312;&#36793;&#32536;&#29615;&#22659;&#19979;&#36827;&#34892;&#24555;&#36895;&#37096;&#32626;&#23454;&#29992;&#30340;&#24322;&#24120;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#32570;&#38519;&#26816;&#27979;&#22312;&#26234;&#33021;&#24037;&#19994;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#22522;&#20110;&#34917;&#19969;&#30340;&#26041;&#27861;&#23558;&#35270;&#35273;&#22270;&#20687;&#35270;&#20026;&#26681;&#25454;&#20301;&#32622;&#30340;&#22270;&#20687;&#34917;&#19969;&#38598;&#21512;&#65292;&#23545;&#20135;&#21697;&#20013;&#30340;&#23567;&#32570;&#38519;&#65288;&#22914;&#33647;&#20024;&#19978;&#30340;&#21010;&#30165;&#65289;&#20855;&#26377;&#26356;&#24378;&#30340;&#36776;&#21035;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#26597;&#35810;&#22270;&#20687;&#21644;&#23384;&#20648;&#30340;&#34917;&#19969;&#20043;&#38388;&#30340;&#26368;&#36817;&#37051;&#25628;&#32034;&#23558;&#22312;&#26102;&#38388;&#21644;&#31354;&#38388;&#38656;&#27714;&#26041;&#38754;&#21344;&#29992;O(n) &#30340;&#22797;&#26434;&#24230;&#65292;&#23545;&#20110;&#22312;&#36793;&#32536;&#29615;&#22659;&#37096;&#32626;&#32780;&#35328;&#25552;&#20986;&#20102;&#20005;&#26684;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21327;&#21516;&#34920;&#31034;&#27169;&#22411;&#35745;&#31639;&#22270;&#20687;&#34917;&#19969;&#36317;&#31163;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#20174;&#20855;&#26377;L0&#32422;&#26463;&#30340;&#26368;&#36817;&#37051;&#36317;&#31163;&#24320;&#22987;&#65292;&#25105;&#20204;&#25918;&#23485;&#32422;&#26463;&#20026;L2&#32422;&#26463;&#65292;&#24182;&#36890;&#36807;&#23553;&#38381;&#24418;&#24335;&#24555;&#36895;&#35299;&#20915;&#36317;&#31163;&#38382;&#39064;&#65292;&#32780;&#19981;&#38656;&#35201;&#23454;&#38469;&#35775;&#38382;&#21407;&#22987;&#23384;&#20648;&#30340;&#22270;&#20687;&#34917;&#19969;&#38598;&#21512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25351;&#20986;&#65292;&#36825;&#31181;&#23553;&#38381;&#24418;&#24335;&#35299;&#20915;&#26041;&#26696;&#30340;&#20027;&#35201;&#35745;&#31639;&#36127;&#25285;&#21487;&#20197;&#30001;&#39640;&#24615;&#33021;&#26381;&#21153;&#22120;&#22312;&#37096;&#32626;&#21069;&#39044;&#20808;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual defect detection plays an important role in intelligent industry. Patch based methods consider visual images as a collection of image patches according to positions, which have stronger discriminative ability for small defects in products, e.g. scratches on pills. However, the nearest neighbor search for the query image and the stored patches will occupy $O(n)$ complexity in terms of time and space requirements, posing strict challenges for deployment in edge environments. In this paper, we propose an alternative approach to the distance calculation of image patches via collaborative representation models. Starting from the nearest neighbor distance with $L_0$ constraint, we relax the constraint to $L_2$ constraint and solve the distance quickly in close-formed without actually accessing the original stored collection of image patches. Furthermore, we point out that the main computational burden of this close-formed solution can be pre-computed by high-performance server before 
&lt;/p&gt;</description></item><item><title>Voxceleb-ESP&#26159;&#19968;&#20010;&#20174;&#22768;&#38899;&#20013;&#26816;&#27979;&#35199;&#29677;&#29273;&#21517;&#20154;&#30340;&#23454;&#39564;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#22810;&#26679;&#30340;&#35199;&#29677;&#29273;&#35821;&#25968;&#25454;&#38598;&#65292;&#19982;&#21407;&#29256;&#33521;&#35821;VoxCeleb&#30456;&#24403;&#22797;&#26434;&#65292;&#20026;&#35828;&#35805;&#20154;&#35782;&#21035;&#22522;&#20934;&#30340;&#25193;&#23637;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2401.09441</link><description>&lt;p&gt;
Voxceleb-ESP: &#20174;&#22768;&#38899;&#20013;&#26816;&#27979;&#35199;&#29677;&#29273;&#21517;&#20154;&#30340;&#21021;&#27493;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Voxceleb-ESP: preliminary experiments detecting Spanish celebrities from their voices. (arXiv:2401.09441v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09441
&lt;/p&gt;
&lt;p&gt;
Voxceleb-ESP&#26159;&#19968;&#20010;&#20174;&#22768;&#38899;&#20013;&#26816;&#27979;&#35199;&#29677;&#29273;&#21517;&#20154;&#30340;&#23454;&#39564;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#22810;&#26679;&#30340;&#35199;&#29677;&#29273;&#35821;&#25968;&#25454;&#38598;&#65292;&#19982;&#21407;&#29256;&#33521;&#35821;VoxCeleb&#30456;&#24403;&#22797;&#26434;&#65292;&#20026;&#35828;&#35805;&#20154;&#35782;&#21035;&#22522;&#20934;&#30340;&#25193;&#23637;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;VoxCeleb-ESP&#65292;&#36825;&#26159;&#19968;&#20010;&#25351;&#21521;YouTube&#35270;&#39057;&#30340;&#25351;&#38024;&#21644;&#26102;&#38388;&#25139;&#38598;&#21512;&#65292;&#20415;&#20110;&#21019;&#24314;&#19968;&#20010;&#26032;&#30340;&#35828;&#35805;&#20154;&#35782;&#21035;&#25968;&#25454;&#38598;&#12290;VoxCeleb-ESP&#25429;&#25417;&#20102;&#30495;&#23454;&#19990;&#30028;&#30340;&#22330;&#26223;&#65292;&#21253;&#25324;&#21508;&#31181;&#35828;&#35805;&#39118;&#26684;&#12289;&#22122;&#38899;&#21644;&#20449;&#36947;&#22833;&#30495;&#12290;&#23427;&#21253;&#25324;&#20102;160&#20010;&#35199;&#29677;&#29273;&#21517;&#20154;&#65292;&#28085;&#30422;&#20102;&#35199;&#29677;&#29273;&#21508;&#20010;&#24180;&#40836;&#32452;&#21644;&#22320;&#29702;&#21306;&#22495;&#65292;&#20197;&#30830;&#20445;&#20195;&#34920;&#24615;&#20998;&#24067;&#12290;&#25105;&#20204;&#20026;&#35828;&#35805;&#20154;&#35782;&#21035;&#20219;&#21153;&#25552;&#20379;&#20102;&#20004;&#20010;&#35828;&#35805;&#20154;&#35797;&#39564;&#21015;&#34920;&#65292;&#20998;&#21035;&#26159;&#21516;&#19968;&#35270;&#39057;&#21644;&#19981;&#21516;&#35270;&#39057;&#30340;&#30446;&#26631;&#35797;&#39564;&#65292;&#24182;&#20276;&#26377;ResNet&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#36328;&#35821;&#35328;&#35780;&#20272;&#12290;&#21021;&#27493;&#30340;&#35828;&#35805;&#20154;&#35782;&#21035;&#32467;&#26524;&#34920;&#26126;&#65292;VoxCeleb-ESP&#20013;&#30340;&#26816;&#27979;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#19982;&#21407;&#29256;&#26356;&#22823;&#30340;&#33521;&#35821;VoxCeleb&#30456;&#24403;&#12290;VoxCeleb-ESP&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#20840;&#38754;&#22810;&#26679;&#30340;&#35199;&#29677;&#29273;&#35821;&#25968;&#25454;&#38598;&#65292;&#20026;&#35828;&#35805;&#20154;&#35782;&#21035;&#22522;&#20934;&#30340;&#25193;&#23637;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents VoxCeleb-ESP, a collection of pointers and timestamps to YouTube videos facilitating the creation of a novel speaker recognition dataset. VoxCeleb-ESP captures real-world scenarios, incorporating diverse speaking styles, noises, and channel distortions. It includes 160 Spanish celebrities spanning various categories, ensuring a representative distribution across age groups and geographic regions in Spain. We provide two speaker trial lists for speaker identification tasks, each of them with same-video or different-video target trials respectively, accompanied by a cross-lingual evaluation of ResNet pretrained models. Preliminary speaker identification results suggest that the complexity of the detection task in VoxCeleb-ESP is equivalent to that of the original and much larger VoxCeleb in English. VoxCeleb-ESP contributes to the expansion of speaker recognition benchmarks with a comprehensive and diverse dataset for the Spanish language.
&lt;/p&gt;</description></item><item><title>RoleCraft-GLM&#26159;&#19968;&#20010;&#21019;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#20010;&#24615;&#21270;&#35282;&#33394;&#25198;&#28436;&#65292;&#35299;&#20915;&#20102;&#32570;&#20047;&#20010;&#24615;&#21270;&#20114;&#21160;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#29420;&#29305;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#21644;&#32454;&#33268;&#20837;&#24494;&#30340;&#35282;&#33394;&#21457;&#23637;&#65292;&#23427;&#33021;&#22815;&#29983;&#25104;&#20934;&#30830;&#21453;&#26144;&#35282;&#33394;&#20010;&#24615;&#29305;&#24449;&#21644;&#24773;&#24863;&#30340;&#23545;&#35805;&#65292;&#25552;&#21319;&#29992;&#25143;&#21442;&#19982;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.09432</link><description>&lt;p&gt;
RoleCraft-GLM&#65306;&#25512;&#21160;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20010;&#24615;&#21270;&#35282;&#33394;&#25198;&#28436;
&lt;/p&gt;
&lt;p&gt;
RoleCraft-GLM: Advancing Personalized Role-Playing in Large Language Models. (arXiv:2401.09432v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09432
&lt;/p&gt;
&lt;p&gt;
RoleCraft-GLM&#26159;&#19968;&#20010;&#21019;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#20010;&#24615;&#21270;&#35282;&#33394;&#25198;&#28436;&#65292;&#35299;&#20915;&#20102;&#32570;&#20047;&#20010;&#24615;&#21270;&#20114;&#21160;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#29420;&#29305;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#21644;&#32454;&#33268;&#20837;&#24494;&#30340;&#35282;&#33394;&#21457;&#23637;&#65292;&#23427;&#33021;&#22815;&#29983;&#25104;&#20934;&#30830;&#21453;&#26144;&#35282;&#33394;&#20010;&#24615;&#29305;&#24449;&#21644;&#24773;&#24863;&#30340;&#23545;&#35805;&#65292;&#25552;&#21319;&#29992;&#25143;&#21442;&#19982;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;RoleCraft-GLM&#65292;&#36825;&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22686;&#24378;&#20010;&#24615;&#21270;&#35282;&#33394;&#25198;&#28436;&#12290;RoleCraft-GLM&#35299;&#20915;&#20102;&#23545;&#35805;&#24335;&#20154;&#24037;&#26234;&#33021;&#20013;&#32570;&#20047;&#20010;&#24615;&#21270;&#20114;&#21160;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#33021;&#22815;&#35814;&#32454;&#25551;&#32472;&#24773;&#24863;&#32454;&#33147;&#30340;&#35282;&#33394;&#21051;&#30011;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#36129;&#29486;&#20102;&#19968;&#32452;&#29420;&#29305;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#20174;&#20256;&#32479;&#30340;&#20197;&#21517;&#20154;&#20026;&#20013;&#24515;&#30340;&#35282;&#33394;&#36716;&#21464;&#20026;&#22810;&#26679;&#21270;&#30340;&#38750;&#21517;&#20154;&#35282;&#33394;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#35821;&#35328;&#24314;&#27169;&#20114;&#21160;&#30340;&#30495;&#23454;&#24615;&#21644;&#22797;&#26434;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#21253;&#25324;&#32454;&#33268;&#20837;&#24494;&#30340;&#35282;&#33394;&#21457;&#23637;&#65292;&#30830;&#20445;&#23545;&#35805;&#26082;&#30495;&#23454;&#21448;&#24773;&#24863;&#20849;&#40483;&#12290;&#36890;&#36807;&#22810;&#20010;&#26696;&#20363;&#30740;&#31350;&#39564;&#35777;&#20102;RoleCraft-GLM&#30340;&#26377;&#25928;&#24615;&#65292;&#31361;&#26174;&#20102;&#23427;&#22312;&#19981;&#21516;&#22330;&#26223;&#20013;&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#25216;&#33021;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#29983;&#25104;&#23545;&#35805;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#33021;&#22815;&#20934;&#30830;&#21453;&#26144;&#35282;&#33394;&#30340;&#20010;&#24615;&#29305;&#24449;&#21644;&#24773;&#24863;&#65292;&#20174;&#32780;&#22686;&#24378;&#29992;&#25143;&#21442;&#19982;&#24230;&#12290;&#24635;&#20043;&#65292;RoleCraft-GLM&#26631;&#24535;&#30528;&#19968;&#20010;&#21019;&#26032;&#30340;&#37324;&#31243;&#30865;&#65292;&#25512;&#21160;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20010;&#24615;&#21270;&#35282;&#33394;&#25198;&#28436;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents RoleCraft-GLM, an innovative framework aimed at enhancing personalized role-playing with Large Language Models (LLMs). RoleCraft-GLM addresses the key issue of lacking personalized interactions in conversational AI, and offers a solution with detailed and emotionally nuanced character portrayals. We contribute a unique conversational dataset that shifts from conventional celebrity-centric characters to diverse, non-celebrity personas, thus enhancing the realism and complexity of language modeling interactions. Additionally, our approach includes meticulous character development, ensuring dialogues are both realistic and emotionally resonant. The effectiveness of RoleCraft-GLM is validated through various case studies, highlighting its versatility and skill in different scenarios. Our framework excels in generating dialogues that accurately reflect characters' personality traits and emotions, thereby boosting user engagement. In conclusion, RoleCraft-GLM marks a sign
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;$\ell^{1}$&#24809;&#32602;&#30340;&#36719;&#38388;&#38548;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#20248;&#21270;&#38382;&#39064;&#30340;&#24179;&#28369;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#36941;&#21382;&#25968;&#25454;&#26102;&#20445;&#25345;&#36739;&#20302;&#30340;&#25104;&#26412;&#65292;&#24182;&#32463;&#36807;&#23454;&#39564;&#35777;&#26126;&#31639;&#27861;&#20855;&#26377;&#24456;&#39640;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.09431</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;l1&#25903;&#25345;&#21521;&#37327;&#26426;&#30340;&#24179;&#28369;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Smoothing Algorithm for l1 Support Vector Machines. (arXiv:2401.09431v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09431
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;$\ell^{1}$&#24809;&#32602;&#30340;&#36719;&#38388;&#38548;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#20248;&#21270;&#38382;&#39064;&#30340;&#24179;&#28369;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#36941;&#21382;&#25968;&#25454;&#26102;&#20445;&#25345;&#36739;&#20302;&#30340;&#25104;&#26412;&#65292;&#24182;&#32463;&#36807;&#23454;&#39564;&#35777;&#26126;&#31639;&#27861;&#20855;&#26377;&#24456;&#39640;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;$\ell^{1}$&#24809;&#32602;&#30340;&#36719;&#38388;&#38548;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#20248;&#21270;&#38382;&#39064;&#30340;&#24179;&#28369;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#26088;&#22312;&#23545;&#25968;&#25454;&#36827;&#34892;&#36866;&#24230;&#30340;&#36941;&#21382;&#65292;&#36825;&#26159;&#34913;&#37327;&#20854;&#23545;&#20110;&#38750;&#24120;&#22823;&#30340;&#25968;&#25454;&#38598;&#30340;&#25104;&#26412;&#30340;&#37325;&#35201;&#25351;&#26631;&#12290;&#35813;&#31639;&#27861;&#20351;&#29992;&#20102;&#23545;&#38128;&#38142;&#25439;&#22833;&#20989;&#25968;&#30340;&#24179;&#28369;&#21644;&#23545;$\ell^{1}$&#24809;&#32602;&#30340;&#20027;&#21160;&#38598;&#26041;&#27861;&#12290;&#24179;&#28369;&#21442;&#25968;$\alpha$&#26368;&#21021;&#36739;&#22823;&#65292;&#20294;&#36890;&#24120;&#22312;&#27714;&#35299;&#21040;&#36275;&#22815;&#31934;&#24230;&#30340;&#24179;&#28369;&#38382;&#39064;&#21518;&#20943;&#21322;&#12290;&#25910;&#25947;&#29702;&#35770;&#34920;&#26126;&#65292;&#22312;&#38500;&#28176;&#36817;&#24102;$\alpha=\Theta(1)$&#21644;$\alpha=\Theta(1/N)$&#20043;&#22806;&#30340;&#27599;&#20010;$\alpha$&#20540;&#19978;&#65292;&#27599;&#20010;$\alpha$&#20540;&#38656;&#35201;$\mathcal{O}(1+\log(1+\log_+(1/\alpha)))$&#20010;&#23432;&#21355;&#29275;&#39039;&#27493;&#39588;&#65292;&#20165;&#24403;$\eta\alpha\gg1/N$&#26102;&#65292;&#25552;&#20379;&#19968;&#20010;&#29275;&#39039;&#27493;&#39588;&#65292;&#20854;&#20013;$N$&#26159;&#25968;&#25454;&#28857;&#30340;&#25968;&#37327;&#65292;&#20572;&#27490;&#20934;&#21017;&#26159;&#39044;&#27979;&#30340;&#20943;&#23569;&#23567;&#20110;$\eta\alpha$&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#33021;&#22815;&#33719;&#24471;&#24456;&#39640;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
A smoothing algorithm is presented for solving the soft-margin Support Vector Machine (SVM) optimization problem with an $\ell^{1}$ penalty. This algorithm is designed to require a modest number of passes over the data, which is an important measure of its cost for very large datasets. The algorithm uses smoothing for the hinge-loss function, and an active set approach for the $\ell^{1}$ penalty. The smoothing parameter $\alpha$ is initially large, but typically halved when the smoothed problem is solved to sufficient accuracy. Convergence theory is presented that shows $\mathcal{O}(1+\log(1+\log_+(1/\alpha)))$ guarded Newton steps for each value of $\alpha$ except for asymptotic bands $\alpha=\Theta(1)$ and $\alpha=\Theta(1/N)$, with only one Newton step provided $\eta\alpha\gg1/N$, where $N$ is the number of data points and the stopping criterion that the predicted reduction is less than $\eta\alpha$. The experimental results show that our algorithm is capable of strong test accuracy
&lt;/p&gt;</description></item><item><title>Transduce&#31639;&#27861;&#36890;&#36807;&#26500;&#24314;&#25277;&#35937;&#36801;&#31227;&#35821;&#27861;&#21644;&#20854;&#27867;&#21270;&#65292;&#21487;&#20197;&#20174;&#36755;&#20837;&#36755;&#20986;&#31034;&#20363;&#20013;&#39640;&#25928;&#22320;&#23398;&#20064;&#20301;&#32622;&#36716;&#25442;&#65292;&#24182;&#19988;&#25104;&#21151;&#29575;&#39640;&#20110;&#24403;&#21069;&#26368;&#26032;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.09426</link><description>&lt;p&gt;
Transduce: &#23398;&#20064;&#29992;&#20110;&#23383;&#31526;&#20018;&#36716;&#25442;&#30340;&#36801;&#31227;&#35821;&#27861;
&lt;/p&gt;
&lt;p&gt;
Transduce: learning transduction grammars for string transformation. (arXiv:2401.09426v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09426
&lt;/p&gt;
&lt;p&gt;
Transduce&#31639;&#27861;&#36890;&#36807;&#26500;&#24314;&#25277;&#35937;&#36801;&#31227;&#35821;&#27861;&#21644;&#20854;&#27867;&#21270;&#65292;&#21487;&#20197;&#20174;&#36755;&#20837;&#36755;&#20986;&#31034;&#20363;&#20013;&#39640;&#25928;&#22320;&#23398;&#20064;&#20301;&#32622;&#36716;&#25442;&#65292;&#24182;&#19988;&#25104;&#21151;&#29575;&#39640;&#20110;&#24403;&#21069;&#26368;&#26032;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#36755;&#20837;&#36755;&#20986;&#31034;&#20363;&#20013;&#21512;&#25104;&#23383;&#31526;&#20018;&#36716;&#25442;&#31243;&#24207;&#21033;&#29992;&#20102;&#21508;&#31181;&#25216;&#26415;&#65292;&#36825;&#20123;&#25216;&#26415;&#37117;&#22522;&#20110;&#24402;&#32435;&#20559;&#24046;&#65292;&#21253;&#25324;&#19968;&#32452;&#21463;&#38480;&#30340;&#22522;&#26412;&#36816;&#31639;&#31526;&#36827;&#34892;&#32452;&#21512;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;Transduce&#65292;&#23427;&#22522;&#20110;&#25277;&#35937;&#36801;&#31227;&#35821;&#27861;&#21450;&#20854;&#27867;&#21270;&#26500;&#24314;&#12290;&#25105;&#20204;&#22312;&#23454;&#39564;&#35777;&#26126;&#65292;Transduce&#21487;&#20197;&#26377;&#25928;&#22320;&#20174;&#19968;&#20010;&#25110;&#20004;&#20010;&#27491;&#20363;&#20013;&#23398;&#20064;&#20301;&#32622;&#36716;&#25442;&#65292;&#32780;&#26080;&#38656;&#24402;&#32435;&#20559;&#24046;&#65292;&#25104;&#21151;&#29575;&#39640;&#20110;&#24403;&#21069;&#30340;&#26368;&#26032;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The synthesis of string transformation programs from input-output examples utilizes various techniques, all based on an inductive bias that comprises a restricted set of basic operators to be combined. A new algorithm, Transduce, is proposed, which is founded on the construction of abstract transduction grammars and their generalization. We experimentally demonstrate that Transduce can learn positional transformations efficiently from one or two positive examples without inductive bias, achieving a success rate higher than the current state of the art.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#22810;&#20010;&#36731;&#37327;&#32423;&#23398;&#20064;&#22120;&#30340;&#38598;&#25104;&#39044;&#27979;&#38477;&#27700;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#21355;&#26143;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#65292;&#26377;&#25928;&#22320;&#27169;&#25311;&#22797;&#26434;&#30340;&#38477;&#38632;&#27169;&#24335;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#39640;&#38477;&#27700;&#20107;&#20214;&#12290;&#22312;Weather4Cast 2023&#31454;&#36187;&#20013;&#21462;&#24471;&#20102;&#31532;&#19968;&#21517;&#12290;</title><link>http://arxiv.org/abs/2401.09424</link><description>&lt;p&gt;
&#20351;&#29992;&#36731;&#37327;&#32423;&#23398;&#20064;&#22120;&#30340;&#38598;&#25104;&#39044;&#27979;&#38477;&#27700;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Precipitation Prediction Using an Ensemble of Lightweight Learners. (arXiv:2401.09424v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#22810;&#20010;&#36731;&#37327;&#32423;&#23398;&#20064;&#22120;&#30340;&#38598;&#25104;&#39044;&#27979;&#38477;&#27700;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#21355;&#26143;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#65292;&#26377;&#25928;&#22320;&#27169;&#25311;&#22797;&#26434;&#30340;&#38477;&#38632;&#27169;&#24335;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#39640;&#38477;&#27700;&#20107;&#20214;&#12290;&#22312;Weather4Cast 2023&#31454;&#36187;&#20013;&#21462;&#24471;&#20102;&#31532;&#19968;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38477;&#27700;&#39044;&#27979;&#22312;&#29616;&#20195;&#20892;&#19994;&#21644;&#24037;&#19994;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#28982;&#32780;&#30001;&#20110;&#26102;&#38388;&#21644;&#31354;&#38388;&#19978;&#22810;&#26679;&#30340;&#27169;&#24335;&#21644;&#21160;&#24577;&#20197;&#21450;&#39640;&#38477;&#27700;&#20107;&#20214;&#30340;&#31232;&#32570;&#24615;&#65292;&#23427;&#20063;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#20010;&#23398;&#20064;&#22120;&#26469;&#25429;&#25417;&#38477;&#27700;&#20998;&#24067;&#30340;&#22810;&#26679;&#27169;&#24335;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#35813;&#26694;&#26550;&#21253;&#25324;&#19968;&#20010;&#20855;&#26377;&#22810;&#20010;&#36731;&#37327;&#32423;&#22836;&#37096;&#65288;&#23398;&#20064;&#22120;&#65289;&#30340;&#38477;&#27700;&#39044;&#27979;&#22120;&#21644;&#19968;&#20010;&#25511;&#21046;&#22120;&#65292;&#35813;&#25511;&#21046;&#22120;&#23558;&#36825;&#20123;&#22836;&#37096;&#30340;&#36755;&#20986;&#32452;&#21512;&#36215;&#26469;&#12290;&#23398;&#20064;&#22120;&#21644;&#25511;&#21046;&#22120;&#20998;&#21035;&#36890;&#36807;&#25552;&#20986;&#30340;3&#38454;&#27573;&#35757;&#32451;&#26041;&#26696;&#36827;&#34892;&#20248;&#21270;&#12290;&#36890;&#36807;&#21033;&#29992;&#25552;&#20379;&#30340;&#21355;&#26143;&#22270;&#20687;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#24314;&#27169;&#22797;&#26434;&#30340;&#38477;&#38632;&#27169;&#24335;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#39640;&#38477;&#27700;&#20107;&#20214;&#12290;&#23427;&#22312;Weather4Cast 2023&#31454;&#36187;&#30340;&#26680;&#24515;&#27979;&#35797;&#21644;&#21363;&#26102;&#39044;&#27979;&#25490;&#34892;&#27036;&#19978;&#21462;&#24471;&#20102;&#31532;&#19968;&#21517;&#12290;&#26377;&#20851;&#35814;&#32454;&#23454;&#29616;&#65292;&#35831;&#21442;&#32771;&#25105;&#20204;&#30340;GitH
&lt;/p&gt;
&lt;p&gt;
Precipitation prediction plays a crucial role in modern agriculture and industry. However, it poses significant challenges due to the diverse patterns and dynamics in time and space, as well as the scarcity of high precipitation events.  To address this challenge, we propose an ensemble learning framework that leverages multiple learners to capture the diverse patterns of precipitation distribution. Specifically, the framework consists of a precipitation predictor with multiple lightweight heads (learners) and a controller that combines the outputs from these heads. The learners and the controller are separately optimized with a proposed 3-stage training scheme.  By utilizing provided satellite images, the proposed approach can effectively model the intricate rainfall patterns, especially for high precipitation events. It achieved 1st place on the core test as well as the nowcasting leaderboards of the Weather4Cast 2023 competition. For detailed implementation, please refer to our GitH
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Apollo&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20302;&#23618;&#35757;&#32451;&#26399;&#38388;&#23398;&#20064;&#39640;&#23618;&#21151;&#33021;&#65292;&#20026;&#28176;&#36827;&#24335;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#20102;&#35838;&#31243;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#21152;&#36895;&#27604;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.09192</link><description>&lt;p&gt;
&#20026;&#28176;&#36827;&#24335;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20934;&#22791;&#35838;&#31243;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Preparing Lessons for Progressive Training on Language Models. (arXiv:2401.09192v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09192
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Apollo&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20302;&#23618;&#35757;&#32451;&#26399;&#38388;&#23398;&#20064;&#39640;&#23618;&#21151;&#33021;&#65292;&#20026;&#28176;&#36827;&#24335;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#20102;&#35838;&#31243;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#21152;&#36895;&#27604;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformers&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#36805;&#36895;&#21457;&#23637;&#24102;&#26469;&#20102;&#36164;&#28304;&#28040;&#32791;&#21644;&#28201;&#23460;&#27668;&#20307;&#25490;&#25918;&#30340;&#22686;&#21152;&#65292;&#36825;&#26159;&#30001;&#20110;&#27169;&#22411;&#35268;&#27169;&#30340;&#22686;&#38271;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#23567;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#23545;&#20110;&#26032;&#30340;&#27169;&#22411;&#32467;&#26500;&#21487;&#33021;&#19981;&#36866;&#29992;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#21487;&#33021;&#24456;&#24930;&#65292;&#24182;&#19988;&#28176;&#36827;&#22534;&#21472;&#23618;&#24448;&#24448;&#26080;&#27861;&#23454;&#29616;&#26174;&#33879;&#30340;&#21152;&#36895;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Apollo&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#22312;&#20302;&#23618;&#35757;&#32451;&#26399;&#38388;&#23398;&#20064;&#39640;&#23618;&#21151;&#33021;&#26469;&#20934;&#22791;&#33192;&#32960;&#25805;&#20316;&#30340;&#35838;&#31243;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#20302;&#20540;&#20248;&#20808;&#37319;&#26679;(LVPS)&#26469;&#35757;&#32451;&#19981;&#21516;&#28145;&#24230;&#65292;&#24182;&#24341;&#20837;&#26435;&#37325;&#20849;&#20139;&#20197;&#20419;&#36827;&#39640;&#25928;&#25193;&#23637;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#25554;&#20540;&#26041;&#27861;&#26469;&#23454;&#29616;&#31283;&#23450;&#30340;&#27169;&#22411;&#28145;&#24230;&#25193;&#23637;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;Apollo&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#21152;&#36895;&#27604;&#29575;&#65292;&#29978;&#33267;&#8230;&#8230;
&lt;/p&gt;
&lt;p&gt;
The rapid progress of Transformers in artificial intelligence has come at the cost of increased resource consumption and greenhouse gas emissions due to growing model sizes. Prior work suggests using pretrained small models to improve training efficiency, but this approach may not be suitable for new model structures. On the other hand, training from scratch can be slow, and progressively stacking layers often fails to achieve significant acceleration. To address these challenges, we propose a novel method called Apollo, which prep\textbf{a}res lessons for ex\textbf{p}anding \textbf{o}perations by \textbf{l}earning high-\textbf{l}ayer functi\textbf{o}nality during training of low layers. Our approach involves low-value-prioritized sampling (LVPS) to train different depths and weight sharing to facilitate efficient expansion. We also introduce an interpolation method for stable model depth extension. Experiments demonstrate that Apollo achieves state-of-the-art acceleration ratios, even
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#22810;&#39046;&#22495;&#32763;&#35793;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20462;&#25913;&#21518;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#23454;&#29616;&#20102;&#21463;&#25511;&#35299;&#32544;&#30340;&#20004;&#20010;&#28508;&#21464;&#37327;&#65292;&#20854;&#20013;&#19968;&#20010;&#20165;&#19982;&#39046;&#22495;&#26377;&#20851;&#65292;&#21478;&#19968;&#20010;&#19982;&#25968;&#25454;&#30340;&#20854;&#20182;&#21464;&#21270;&#22240;&#32032;&#26377;&#20851;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#35270;&#35273;&#25968;&#25454;&#38598;&#19978;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.09180</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#25511;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#20013;&#30340;&#21463;&#25511;&#35299;&#32544;&#26469;&#36827;&#34892;&#26080;&#30417;&#30563;&#22810;&#39046;&#22495;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Multiple Domain Translation through Controlled Disentanglement in Variational Autoencoder. (arXiv:2401.09180v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09180
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#22810;&#39046;&#22495;&#32763;&#35793;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20462;&#25913;&#21518;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#23454;&#29616;&#20102;&#21463;&#25511;&#35299;&#32544;&#30340;&#20004;&#20010;&#28508;&#21464;&#37327;&#65292;&#20854;&#20013;&#19968;&#20010;&#20165;&#19982;&#39046;&#22495;&#26377;&#20851;&#65292;&#21478;&#19968;&#20010;&#19982;&#25968;&#25454;&#30340;&#20854;&#20182;&#21464;&#21270;&#22240;&#32032;&#26377;&#20851;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#35270;&#35273;&#25968;&#25454;&#38598;&#19978;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#22810;&#39046;&#22495;&#32763;&#35793;&#26159;&#23558;&#25968;&#25454;&#20174;&#19968;&#20010;&#39046;&#22495;&#36716;&#25442;&#21040;&#20854;&#20182;&#39046;&#22495;&#65292;&#32780;&#26080;&#38656;&#37197;&#23545;&#25968;&#25454;&#26469;&#35757;&#32451;&#31995;&#32479;&#30340;&#20219;&#21153;&#12290;&#36890;&#24120;&#65292;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30340;&#26041;&#27861;&#34987;&#29992;&#26469;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#25552;&#35758;&#23436;&#20840;&#20381;&#36182;&#20110;&#20462;&#25913;&#36807;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#12290;&#36825;&#31181;&#20462;&#25913;&#21253;&#25324;&#36890;&#36807;&#35774;&#35745;&#20197;&#25511;&#21046;&#26041;&#24335;&#35299;&#32544;&#20004;&#20010;&#28508;&#21464;&#37327;&#12290;&#20854;&#20013;&#19968;&#20010;&#28508;&#21464;&#37327;&#34987;&#24378;&#21046;&#20165;&#20381;&#36182;&#20110;&#39046;&#22495;&#65292;&#32780;&#21478;&#19968;&#20010;&#28508;&#21464;&#37327;&#24517;&#39035;&#20381;&#36182;&#20110;&#25968;&#25454;&#30340;&#20854;&#20182;&#21464;&#21270;&#22240;&#32032;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#39046;&#22495;&#28508;&#21464;&#37327;&#30340;&#26465;&#20214;&#38480;&#21046;&#21487;&#20197;&#26356;&#22909;&#22320;&#25511;&#21046;&#21644;&#29702;&#35299;&#28508;&#31354;&#38388;&#12290;&#25105;&#20204;&#20174;&#23454;&#35777;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#35270;&#35273;&#25968;&#25454;&#38598;&#65292;&#25552;&#39640;&#20102;&#20854;&#20182;&#20247;&#25152;&#21608;&#30693;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23454;&#38469;&#19978;&#19968;&#20010;&#28508;&#21464;&#37327;&#23384;&#20648;&#20102;&#19982;&#39046;&#22495;&#21644;&#20854;&#20182;&#21464;&#21270;&#22240;&#32032;&#26377;&#20851;&#30340;&#25152;&#26377;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Multiple Domain Translation is the task of transforming data from one domain to other domains without having paired data to train the systems. Typically, methods based on Generative Adversarial Networks (GANs) are used to address this task. However, our proposal exclusively relies on a modified version of a Variational Autoencoder. This modification consists of the use of two latent variables disentangled in a controlled way by design. One of this latent variables is imposed to depend exclusively on the domain, while the other one must depend on the rest of the variability factors of the data. Additionally, the conditions imposed over the domain latent variable allow for better control and understanding of the latent space. We empirically demonstrate that our approach works on different vision datasets improving the performance of other well known methods. Finally, we prove that, indeed, one of the latent variables stores all the information related to the domain and the o
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20132;&#36890;&#25317;&#22581;&#39044;&#27979;&#27169;&#22411;&#65292;&#20351;&#29992;&#36710;&#36742;&#36712;&#36857;&#25968;&#25454;&#20197;&#21450;&#22810;&#37051;&#25509;&#20851;&#31995;&#27880;&#24847;&#21147;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;MA2GCN&#65289;&#26469;&#39044;&#27979;&#20132;&#36890;&#25317;&#22581;&#24773;&#20917;&#65292;&#19981;&#20381;&#36182;&#20110;&#20256;&#24863;&#22120;&#25968;&#25454;&#65292;&#25552;&#21462;&#28789;&#27963;&#19988;&#20934;&#30830;&#30340;&#20132;&#36890;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2401.08727</link><description>&lt;p&gt;
MA2GCN: &#20351;&#29992;&#36712;&#36857;&#25968;&#25454;&#36827;&#34892;&#20132;&#36890;&#39044;&#27979;&#30340;&#22810;&#37051;&#25509;&#20851;&#31995;&#27880;&#24847;&#21147;&#22270;&#21367;&#31215;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
MA2GCN: Multi Adjacency relationship Attention Graph Convolutional Networks for Traffic Prediction using Trajectory data. (arXiv:2401.08727v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08727
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20132;&#36890;&#25317;&#22581;&#39044;&#27979;&#27169;&#22411;&#65292;&#20351;&#29992;&#36710;&#36742;&#36712;&#36857;&#25968;&#25454;&#20197;&#21450;&#22810;&#37051;&#25509;&#20851;&#31995;&#27880;&#24847;&#21147;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;MA2GCN&#65289;&#26469;&#39044;&#27979;&#20132;&#36890;&#25317;&#22581;&#24773;&#20917;&#65292;&#19981;&#20381;&#36182;&#20110;&#20256;&#24863;&#22120;&#25968;&#25454;&#65292;&#25552;&#21462;&#28789;&#27963;&#19988;&#20934;&#30830;&#30340;&#20132;&#36890;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#25317;&#22581;&#38382;&#39064;&#19981;&#20165;&#23548;&#33268;&#24040;&#22823;&#30340;&#32463;&#27982;&#25439;&#22833;&#65292;&#32780;&#19988;&#20005;&#37325;&#21361;&#23475;&#22478;&#24066;&#29615;&#22659;&#12290;&#39044;&#27979;&#20132;&#36890;&#25317;&#22581;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#38469;&#24847;&#20041;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#37117;&#26159;&#22522;&#20110;&#19981;&#21516;&#36335;&#27573;&#19978;&#30340;&#20256;&#24863;&#22120;&#30340;&#21382;&#21490;&#25968;&#25454;&#26469;&#39044;&#27979;&#26410;&#26469;&#30340;&#20132;&#36890;&#27969;&#37327;&#21644;&#36895;&#24230;&#65292;&#20998;&#26512;&#26576;&#20010;&#36947;&#36335;&#27573;&#30340;&#20132;&#36890;&#25317;&#22581;&#24773;&#20917;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20256;&#24863;&#22120;&#30340;&#22266;&#23450;&#20301;&#32622;&#65292;&#24456;&#38590;&#25366;&#25496;&#26032;&#30340;&#20449;&#24687;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#36710;&#36742;&#36712;&#36857;&#25968;&#25454;&#26356;&#21152;&#28789;&#27963;&#65292;&#21487;&#20197;&#26681;&#25454;&#38656;&#35201;&#25552;&#21462;&#20132;&#36890;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20132;&#36890;&#25317;&#22581;&#39044;&#27979;&#27169;&#22411;&#8212;&#8212;&#22810;&#37051;&#25509;&#20851;&#31995;&#27880;&#24847;&#21147;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;MA2GCN&#65289;&#12290;&#35813;&#27169;&#22411;&#23558;&#36710;&#36742;&#36712;&#36857;&#25968;&#25454;&#36716;&#21270;&#20026;&#32593;&#26684;&#24418;&#24335;&#30340;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#24182;&#22522;&#20110;&#19981;&#21516;&#32593;&#26684;&#20043;&#38388;&#30340;&#27969;&#21160;&#24615;&#25552;&#20986;&#20102;&#36710;&#36742;&#36827;&#20986;&#30697;&#38453;&#12290;&#21516;&#26102;&#65292;&#20026;&#20102;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;
&lt;/p&gt;
&lt;p&gt;
The problem of traffic congestion not only causes a large amount of economic losses, but also seriously endangers the urban environment. Predicting traffic congestion has important practical significance. So far, most studies have been based on historical data from sensors placed on different roads to predict future traffic flow and speed, to analyze the traffic congestion conditions of a certain road segment. However, due to the fixed position of sensors, it is difficult to mine new information. On the other hand, vehicle trajectory data is more flexible and can extract traffic information as needed. Therefore, we proposed a new traffic congestion prediction model - Multi Adjacency relationship Attention Graph Convolutional Networks(MA2GCN). This model transformed vehicle trajectory data into graph structured data in grid form, and proposed a vehicle entry and exit matrix based on the mobility between different grids. At the same time, in order to improve the performance of the model,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#25193;&#25955;&#36807;&#31243;&#24212;&#29992;&#20110;&#22806;&#20998;&#24067;&#26816;&#27979;&#20219;&#21153;&#20013;&#65292;&#36890;&#36807;&#23558;&#25972;&#20010;&#35757;&#32451;&#38598;&#30340;&#20449;&#24687;&#38598;&#25104;&#21040;&#39044;&#27979;&#30340;&#22122;&#22768;&#21521;&#37327;&#20013;&#65292;&#33719;&#24471;&#31283;&#23450;&#30340;&#22122;&#22768;&#21521;&#37327;&#65292;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;OOD&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2401.08689</link><description>&lt;p&gt;
&#20351;&#29992;&#25193;&#25955;&#22122;&#22768;&#36827;&#34892;ODI&#65288;&#22806;&#20998;&#24067;&#26816;&#27979;&#65289;
&lt;/p&gt;
&lt;p&gt;
NODI: Out-Of-Distribution Detection with Noise from Diffusion. (arXiv:2401.08689v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08689
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#25193;&#25955;&#36807;&#31243;&#24212;&#29992;&#20110;&#22806;&#20998;&#24067;&#26816;&#27979;&#20219;&#21153;&#20013;&#65292;&#36890;&#36807;&#23558;&#25972;&#20010;&#35757;&#32451;&#38598;&#30340;&#20449;&#24687;&#38598;&#25104;&#21040;&#39044;&#27979;&#30340;&#22122;&#22768;&#21521;&#37327;&#20013;&#65292;&#33719;&#24471;&#31283;&#23450;&#30340;&#22122;&#22768;&#21521;&#37327;&#65292;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;OOD&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22806;&#20998;&#24067;&#65288;OOD&#65289;&#26816;&#27979;&#26159;&#23433;&#20840;&#37096;&#32626;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20851;&#38190;&#37096;&#20998;&#12290;&#22312;&#25991;&#29486;&#20013;&#24050;&#32463;&#24191;&#27867;&#30740;&#31350;&#24182;&#24320;&#21457;&#20102;&#22823;&#37327;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#30340;&#26041;&#27861;&#22312;&#35745;&#31639;OOD&#20998;&#25968;&#26102;&#23545;&#20869;&#20998;&#24067;&#25968;&#25454;&#38598;&#30340;&#20351;&#29992;&#26377;&#38480;&#12290;&#20363;&#22914;&#65292;OOD&#20998;&#25968;&#26159;&#26681;&#25454;&#20869;&#20998;&#24067;&#25968;&#25454;&#30340;&#19968;&#23567;&#37096;&#20998;&#20449;&#24687;&#35745;&#31639;&#30340;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#20351;&#29992;&#31070;&#32463;&#22270;&#20687;&#32534;&#30721;&#22120;&#23545;&#22270;&#20687;&#36827;&#34892;&#32534;&#30721;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#20154;&#26816;&#26597;&#36825;&#20123;&#26041;&#27861;&#23545;&#19981;&#21516;&#35757;&#32451;&#26041;&#27861;&#21644;&#26550;&#26500;&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#25193;&#25955;&#36807;&#31243;&#24341;&#20837;&#21040;ODD&#20219;&#21153;&#20013;&#12290;&#25193;&#25955;&#27169;&#22411;&#23558;&#25972;&#20010;&#35757;&#32451;&#38598;&#30340;&#20449;&#24687;&#38598;&#25104;&#21040;&#39044;&#27979;&#30340;&#22122;&#22768;&#21521;&#37327;&#20013;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#22122;&#22768;&#21521;&#37327;&#65288;&#31283;&#23450;&#28857;&#65289;&#30340;&#38381;&#24335;&#35299;&#12290;&#28982;&#21518;&#65292;&#23558;&#22122;&#22768;&#21521;&#37327;&#36716;&#21270;&#20026;&#25105;&#20204;&#30340;OOD&#20998;&#25968;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#28145;&#24230;&#27169;&#22411;&#39044;&#27979;&#30340;...
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution (OOD) detection is a crucial part of deploying machine learning models safely. It has been extensively studied with a plethora of methods developed in the literature. This problem is tackled with an OOD score computation, however, previous methods compute the OOD scores with limited usage of the in-distribution dataset. For instance, the OOD scores are computed with information from a small portion of the in-distribution data. Furthermore, these methods encode images with a neural image encoder. The robustness of these methods is rarely checked with respect to image encoders of different training methods and architectures. In this work, we introduce the diffusion process into the OOD task. The diffusion model integrates information on the whole training set into the predicted noise vectors. What's more, we deduce a closed-form solution for the noise vector (stable point). Then the noise vector is converted into our OOD score, we test both the deep model predicted no
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#21644;&#24494;&#35843;&#20004;&#31181;&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#24615;&#33021;&#24046;&#24322;&#65292;&#24182;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#20892;&#19994;&#25968;&#25454;&#38598;&#30340;&#31649;&#36947;&#21644;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2401.08406</link><description>&lt;p&gt;
RAG vs Fine-tuning: &#31649;&#36947;&#65292;&#26435;&#34913;&#20197;&#21450;&#22312;&#20892;&#19994;&#19978;&#30340;&#20010;&#26696;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture. (arXiv:2401.08406v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#21644;&#24494;&#35843;&#20004;&#31181;&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#24615;&#33021;&#24046;&#24322;&#65292;&#24182;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#20892;&#19994;&#25968;&#25454;&#38598;&#30340;&#31649;&#36947;&#21644;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26500;&#24314;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#31243;&#24207;&#26102;&#65292;&#24320;&#21457;&#32773;&#36890;&#24120;&#26377;&#20004;&#31181;&#24120;&#35265;&#26041;&#27861;&#26469;&#25972;&#21512;&#19987;&#26377;&#21644;&#39046;&#22495;&#29305;&#23450;&#30340;&#25968;&#25454;&#65306;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#21644;&#24494;&#35843;&#12290;RAG&#21033;&#29992;&#22806;&#37096;&#25968;&#25454;&#22686;&#24378;&#25552;&#31034;&#20449;&#24687;&#65292;&#32780;&#24494;&#35843;&#21017;&#23558;&#38468;&#21152;&#30693;&#35782;&#25972;&#21512;&#21040;&#27169;&#22411;&#20013;&#12290;&#28982;&#32780;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#32570;&#28857;&#24182;&#19981;&#20026;&#20154;&#25152;&#29702;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24494;&#35843;&#21644;RAG&#30340;&#31649;&#36947;&#65292;&#24182;&#23545;&#22810;&#31181;&#27969;&#34892;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#21253;&#25324;Llama2-13B&#65292;GPT-3.5&#21644;GPT-4&#65289;&#36827;&#34892;&#20102;&#26435;&#34913;&#12290;&#25105;&#20204;&#30340;&#31649;&#36947;&#30001;&#22810;&#20010;&#38454;&#27573;&#32452;&#25104;&#65292;&#21253;&#25324;&#20174;PDF&#20013;&#25552;&#21462;&#20449;&#24687;&#65292;&#29983;&#25104;&#38382;&#39064;&#21644;&#31572;&#26696;&#65292;&#23558;&#20854;&#29992;&#20110;&#24494;&#35843;&#65292;&#24182;&#21033;&#29992;GPT-4&#35780;&#20272;&#32467;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#35780;&#20272;RAG&#21644;&#24494;&#35843;&#31649;&#36947;&#19981;&#21516;&#38454;&#27573;&#24615;&#33021;&#30340;&#25351;&#26631;&#12290;&#25105;&#20204;&#23545;&#20892;&#19994;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#12290;&#20316;&#20026;&#19968;&#20010;&#20135;&#19994;&#65292;&#20892;&#19994;&#22312;&#20154;&#24037;&#26234;&#33021;&#30340;&#24212;&#29992;&#26041;&#38754;&#24182;&#27809;&#26377;&#24471;&#21040;&#24456;&#22823;&#30340;&#28183;&#36879;&#12290;
&lt;/p&gt;
&lt;p&gt;
There are two common ways in which developers are incorporating proprietary and domain-specific data when building applications of Large Language Models (LLMs): Retrieval-Augmented Generation (RAG) and Fine-Tuning. RAG augments the prompt with the external data, while fine-Tuning incorporates the additional knowledge into the model itself. However, the pros and cons of both approaches are not well understood. In this paper, we propose a pipeline for fine-tuning and RAG, and present the tradeoffs of both for multiple popular LLMs, including Llama2-13B, GPT-3.5, and GPT-4. Our pipeline consists of multiple stages, including extracting information from PDFs, generating questions and answers, using them for fine-tuning, and leveraging GPT-4 for evaluating the results. We propose metrics to assess the performance of different stages of the RAG and fine-Tuning pipeline. We conduct an in-depth study on an agricultural dataset. Agriculture as an industry has not seen much penetration of AI, an
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#22312;&#33258;&#36866;&#24212;&#23454;&#39564;&#20013;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#20272;&#35745;&#26465;&#20214;&#24179;&#22343;&#22788;&#29702;&#25928;&#26524;(CATE)&#65292;&#24179;&#34913;&#31038;&#20250;&#31119;&#21033;&#25439;&#22833;&#21644;&#32479;&#35745;&#21151;&#29575;&#65292;&#24182;&#36890;&#36807;&#21305;&#37197;&#19978;&#19979;&#30028;&#20197;&#21450;&#24085;&#32047;&#25176;&#26368;&#20248;&#24615;&#27010;&#24565;&#26469;&#33719;&#24471;&#26368;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2401.08224</link><description>&lt;p&gt;
&#38544;&#31169;&#20445;&#25252;&#19979;&#33258;&#36866;&#24212;&#23454;&#39564;&#20013;CATE&#30340;&#24046;&#20998;&#38544;&#31169;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Estimation of CATE in Adaptive Experiment. (arXiv:2401.08224v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#22312;&#33258;&#36866;&#24212;&#23454;&#39564;&#20013;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#20272;&#35745;&#26465;&#20214;&#24179;&#22343;&#22788;&#29702;&#25928;&#26524;(CATE)&#65292;&#24179;&#34913;&#31038;&#20250;&#31119;&#21033;&#25439;&#22833;&#21644;&#32479;&#35745;&#21151;&#29575;&#65292;&#24182;&#36890;&#36807;&#21305;&#37197;&#19978;&#19979;&#30028;&#20197;&#21450;&#24085;&#32047;&#25176;&#26368;&#20248;&#24615;&#27010;&#24565;&#26469;&#33719;&#24471;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#36866;&#24212;&#23454;&#39564;&#24191;&#27867;&#24212;&#29992;&#20110;&#20020;&#24202;&#35797;&#39564;&#21644;&#20854;&#20182;&#22330;&#26223;&#20013;&#20272;&#35745;&#26465;&#20214;&#24179;&#22343;&#22788;&#29702;&#25928;&#26524;(CATE)&#12290;&#34429;&#28982;&#23454;&#39564;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#26368;&#22823;&#21270;&#20272;&#35745;&#31934;&#24230;&#65292;&#20294;&#30001;&#20110;&#31038;&#20250;&#31119;&#21033;&#30340;&#35201;&#27714;&#65292;&#20026;&#24739;&#32773;&#25552;&#20379;&#20855;&#26377;&#20248;&#36234;&#32467;&#26524;&#30340;&#27835;&#30103;&#20063;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#36825;&#21487;&#20197;&#36890;&#36807;&#29615;&#22659;&#25209;&#27425;&#26694;&#26550;&#20013;&#30340;&#36951;&#25022;&#26469;&#34913;&#37327;&#12290;&#36825;&#20004;&#20010;&#30446;&#26631;&#32463;&#24120;&#23548;&#33268;&#23545;&#27604;&#20248;&#21270;&#20998;&#37197;&#26426;&#21046;&#12290;&#27492;&#22806;&#65292;&#22312;&#21253;&#21547;&#25935;&#24863;&#25968;&#25454;&#65288;&#22914;&#24739;&#32773;&#20581;&#24247;&#35760;&#24405;&#65289;&#30340;&#20020;&#24202;&#22330;&#26223;&#20013;&#20986;&#29616;&#38544;&#31169;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#27835;&#30103;&#20998;&#37197;&#26426;&#21046;&#24517;&#39035;&#32435;&#20837;&#24378;&#22823;&#30340;&#38544;&#31169;&#20445;&#25252;&#25514;&#26045;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#29615;&#22659;&#25209;&#27425;&#23454;&#39564;&#20013;&#31038;&#20250;&#31119;&#21033;&#25439;&#22833;&#21644;&#32479;&#35745;&#21151;&#29575;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#20026;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#25552;&#20986;&#20102;&#21305;&#37197;&#30340;&#19978;&#19979;&#30028;&#65292;&#24182;&#37319;&#29992;&#24085;&#32047;&#25176;&#26368;&#20248;&#24615;&#30340;&#27010;&#24565;&#26469;&#25968;&#23398;&#22320;&#21051;&#30011;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adaptive experiment is widely adopted to estimate conditional average treatment effect (CATE) in clinical trials and many other scenarios. While the primary goal in experiment is to maximize estimation accuracy, due to the imperative of social welfare, it's also crucial to provide treatment with superior outcomes to patients, which is measured by regret in contextual bandit framework. These two objectives often lead to contrast optimal allocation mechanism. Furthermore, privacy concerns arise in clinical scenarios containing sensitive data like patients health records. Therefore, it's essential for the treatment allocation mechanism to incorporate robust privacy protection measures. In this paper, we investigate the tradeoff between loss of social welfare and statistical power in contextual bandit experiment. We propose a matched upper and lower bound for the multi-objective optimization problem, and then adopt the concept of Pareto optimality to mathematically characterize the optimal
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#35299;&#37322;&#26159;&#21542;&#21487;&#38752;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;AI&#23433;&#20840;&#32771;&#34385;&#22240;&#32032;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#33258;&#27965;&#24615;&#26816;&#27979;&#20316;&#20026;&#35780;&#20272;&#20854;&#21487;&#38752;&#24615;&#21644;&#35299;&#37322;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.07927</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#35299;&#37322;&#26159;&#21542;&#21487;&#38752;?
&lt;/p&gt;
&lt;p&gt;
Are self-explanations from Large Language Models faithful?. (arXiv:2401.07927v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.07927
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#35299;&#37322;&#26159;&#21542;&#21487;&#38752;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;AI&#23433;&#20840;&#32771;&#34385;&#22240;&#32032;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#33258;&#27965;&#24615;&#26816;&#27979;&#20316;&#20026;&#35780;&#20272;&#20854;&#21487;&#38752;&#24615;&#21644;&#35299;&#37322;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#36807;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#29978;&#33267;&#33021;&#22815;&#25552;&#20379;&#20854;&#34892;&#20026;&#30340;&#35299;&#37322;&#12290;&#30001;&#20110;&#36825;&#20123;&#27169;&#22411;&#23545;&#20844;&#20247;&#26159;&#30452;&#25509;&#21487;&#35775;&#38382;&#30340;&#65292;&#22240;&#27492;&#23384;&#22312;&#36825;&#26679;&#30340;&#39118;&#38505;&#65292;&#21363;&#20196;&#20154;&#20449;&#26381;&#20294;&#38169;&#35823;&#30340;&#35299;&#37322;&#21487;&#33021;&#23548;&#33268;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26080;&#25903;&#25745;&#30340;&#33258;&#20449;&#12290;&#22240;&#27492;&#65292;&#35299;&#37322;&#33021;&#21147;&#21644;&#21487;&#38752;&#24615;&#26159;AI&#23433;&#20840;&#30340;&#37325;&#35201;&#32771;&#34385;&#22240;&#32032;&#12290;&#35780;&#20272;&#33258;&#25105;&#35299;&#37322;&#30340;&#21487;&#38752;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#20154;&#31867;&#26469;&#35828;&#36807;&#20110;&#22797;&#26434;&#65292;&#26080;&#27861;&#27880;&#37322;&#20160;&#20040;&#26159;&#27491;&#30830;&#30340;&#35299;&#37322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#33258;&#27965;&#24615;&#26816;&#27979;&#20316;&#20026;&#21487;&#38752;&#24615;&#30340;&#34913;&#37327;&#25351;&#26631;&#12290;&#20363;&#22914;&#65292;&#22914;&#26524;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35828;&#26576;&#32452;&#35789;&#23545;&#20110;&#20570;&#20986;&#39044;&#27979;&#24456;&#37325;&#35201;&#65292;&#37027;&#20040;&#22312;&#27809;&#26377;&#36825;&#20123;&#35789;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#24212;&#35813;&#26080;&#27861;&#20570;&#20986;&#30456;&#21516;&#30340;&#39044;&#27979;&#12290;&#34429;&#28982;&#33258;&#27965;&#24615;&#26816;&#27979;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#21487;&#38752;&#24615;&#26041;&#27861;&#65292;&#20294;&#20043;&#21069;&#23578;&#26410;&#24212;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#35299;&#37322;&#20013;&#12290;&#25105;&#20204;&#23558;&#33258;&#27965;&#24615;&#26816;&#27979;&#24212;&#29992;&#20110;...
&lt;/p&gt;
&lt;p&gt;
Instruction-tuned large language models (LLMs) excel at many tasks, and will even provide explanations for their behavior. Since these models are directly accessible to the public, there is a risk that convincing and wrong explanations can lead to unsupported confidence in LLMs. Therefore, interpretability-faithfulness of self-explanations is an important consideration for AI Safety. Assessing the interpretability-faithfulness of these explanations, termed self-explanations, is challenging as the models are too complex for humans to annotate what is a correct explanation. To address this, we propose employing self-consistency checks as a measure of faithfulness. For example, if an LLM says a set of words is important for making a prediction, then it should not be able to make the same prediction without these words. While self-consistency checks are a common approach to faithfulness, they have not previously been applied to LLM's self-explanations. We apply self-consistency checks to t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#29992;&#20110;&#20855;&#26377;&#26410;&#35266;&#27979;&#21464;&#37327;&#30340;&#22240;&#26524;&#21487;&#21152;&#27169;&#22411;&#65288;CAM-UV&#65289;&#30340;&#26041;&#27861;&#65292;&#24182;&#25193;&#23637;&#20102;&#36825;&#20123;&#26041;&#27861;&#20197;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#36825;&#20123;&#26041;&#27861;&#21033;&#29992;&#20808;&#39564;&#30693;&#35782;&#36827;&#34892;&#39640;&#25928;&#22240;&#26524;&#21457;&#29616;&#65292;&#24182;&#20855;&#26377;&#23545;&#22240;&#26524;&#20851;&#31995;&#39034;&#24207;&#30340;&#29305;&#27530;&#22788;&#29702;&#12290;</title><link>http://arxiv.org/abs/2401.07231</link><description>&lt;p&gt;
&#21033;&#29992;&#20808;&#39564;&#30693;&#35782;&#21457;&#29616;&#20855;&#26377;&#26410;&#35266;&#27979;&#21464;&#37327;&#30340;&#22240;&#26524;&#21487;&#21152;&#27169;&#22411;&#21450;&#20854;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Use of Prior Knowledge to Discover Causal Additive Models with Unobserved Variables and its Application to Time Series Data. (arXiv:2401.07231v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.07231
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#29992;&#20110;&#20855;&#26377;&#26410;&#35266;&#27979;&#21464;&#37327;&#30340;&#22240;&#26524;&#21487;&#21152;&#27169;&#22411;&#65288;CAM-UV&#65289;&#30340;&#26041;&#27861;&#65292;&#24182;&#25193;&#23637;&#20102;&#36825;&#20123;&#26041;&#27861;&#20197;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#36825;&#20123;&#26041;&#27861;&#21033;&#29992;&#20808;&#39564;&#30693;&#35782;&#36827;&#34892;&#39640;&#25928;&#22240;&#26524;&#21457;&#29616;&#65292;&#24182;&#20855;&#26377;&#23545;&#22240;&#26524;&#20851;&#31995;&#39034;&#24207;&#30340;&#29305;&#27530;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#29992;&#20110;&#20855;&#26377;&#26410;&#35266;&#27979;&#21464;&#37327;&#30340;&#22240;&#26524;&#21487;&#21152;&#27169;&#22411;&#65288;CAM-UV&#65289;&#30340;&#26041;&#27861;&#12290;CAM-UV&#20551;&#35774;&#22240;&#26524;&#20989;&#25968;&#37319;&#29992;&#24191;&#20041;&#21487;&#21152;&#27169;&#22411;&#30340;&#24418;&#24335;&#65292;&#24182;&#23384;&#22312;&#28508;&#22312;&#30340;&#28151;&#28102;&#21464;&#37327;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20808;&#39564;&#30693;&#35782;&#36827;&#34892;&#39640;&#25928;&#22240;&#26524;&#21457;&#29616;&#30340;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#36825;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#25512;&#26029;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#19982;&#20854;&#20182;&#29616;&#26377;&#30340;&#22240;&#26524;&#20989;&#25968;&#27169;&#22411;&#19981;&#21516;&#65292;&#21407;&#22987;&#30340;CAM-UV&#31639;&#27861;&#19981;&#23547;&#27714;&#35266;&#27979;&#21464;&#37327;&#20043;&#38388;&#30340;&#22240;&#26524;&#39034;&#24207;&#65292;&#32780;&#26159;&#26088;&#22312;&#30830;&#23450;&#27599;&#20010;&#35266;&#27979;&#21464;&#37327;&#30340;&#21407;&#22240;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#20013;&#25552;&#20986;&#30340;&#31532;&#19968;&#31181;&#26041;&#27861;&#21033;&#29992;&#20808;&#39564;&#30693;&#35782;&#65292;&#20363;&#22914;&#29702;&#35299;&#26576;&#20123;&#21464;&#37327;&#19981;&#33021;&#25104;&#20026;&#29305;&#23450;&#21464;&#37327;&#30340;&#21407;&#22240;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#34701;&#20837;&#22240;&#26524;&#22312;&#26102;&#38388;&#19978;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#25105;&#20204;&#23558;&#31532;&#19968;&#20010;&#31639;&#27861;&#25193;&#23637;&#20026;&#31532;&#20108;&#31181;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#22240;&#26524;&#21457;&#29616;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#31532;&#19968;&#20010;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes two methods for causal additive models with unobserved variables (CAM-UV). CAM-UV assumes that the causal functions take the form of generalized additive models and that latent confounders are present. First, we propose a method that leverages prior knowledge for efficient causal discovery. Then, we propose an extension of this method for inferring causality in time series data. The original CAM-UV algorithm differs from other existing causal function models in that it does not seek the causal order between observed variables, but rather aims to identify the causes for each observed variable. Therefore, the first proposed method in this paper utilizes prior knowledge, such as understanding that certain variables cannot be causes of specific others. Moreover, by incorporating the prior knowledge that causes precedes their effects in time, we extend the first algorithm to the second method for causal discovery in time series data. We validate the first proposed method
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#35757;&#32451;&#24182;&#20445;&#25345;&#25345;&#20037;&#30340;&#27450;&#39575;&#24615;&#34892;&#20026;&#65292;&#36825;&#31181;&#34892;&#20026;&#26080;&#27861;&#34987;&#24403;&#21069;&#30340;&#23433;&#20840;&#35757;&#32451;&#25216;&#26415;&#31227;&#38500;&#12290;</title><link>http://arxiv.org/abs/2401.05566</link><description>&lt;p&gt;
&#21351;&#24213;&#29305;&#24037;&#65306;&#35757;&#32451;&#39575;&#20154;&#30340;LLM&#20197;&#36890;&#36807;&#23433;&#20840;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training. (arXiv:2401.05566v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05566
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#35757;&#32451;&#24182;&#20445;&#25345;&#25345;&#20037;&#30340;&#27450;&#39575;&#24615;&#34892;&#20026;&#65292;&#36825;&#31181;&#34892;&#20026;&#26080;&#27861;&#34987;&#24403;&#21069;&#30340;&#23433;&#20840;&#35757;&#32451;&#25216;&#26415;&#31227;&#38500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#26377;&#33021;&#21147;&#36827;&#34892;&#25112;&#30053;&#24615;&#30340;&#27450;&#39575;&#34892;&#20026;&#65306;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#26377;&#30410;&#30340;&#34892;&#20026;&#65292;&#20294;&#22312;&#26377;&#26426;&#20250;&#30340;&#26102;&#20505;&#21364;&#34920;&#29616;&#20986;&#25130;&#28982;&#19981;&#21516;&#30340;&#34892;&#20026;&#20197;&#36861;&#27714;&#20854;&#20182;&#30446;&#26631;&#12290;&#22914;&#26524;&#19968;&#20010;AI&#31995;&#32479;&#23398;&#20250;&#20102;&#36825;&#26679;&#30340;&#27450;&#39575;&#31574;&#30053;&#65292;&#26159;&#21542;&#33021;&#22815;&#36890;&#36807;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#23433;&#20840;&#35757;&#32451;&#25216;&#26415;&#26816;&#27979;&#24182;&#31227;&#38500;&#23427;&#65311;&#20026;&#20102;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#27450;&#39575;&#34892;&#20026;&#30340;&#27010;&#24565;&#39564;&#35777;&#26679;&#20363;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#35757;&#32451;&#27169;&#22411;&#65292;&#22312;&#25552;&#31034;&#35821;&#21477;&#20013;&#23558;&#24180;&#20221;&#35774;&#20026;2023&#26102;&#32534;&#20889;&#23433;&#20840;&#20195;&#30721;&#65292;&#20294;&#22312;&#24180;&#20221;&#35774;&#20026;2024&#26102;&#25554;&#20837;&#26377;&#28431;&#27934;&#30340;&#20195;&#30721;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#31181;&#26263;&#38376;&#34892;&#20026;&#21487;&#20197;&#34987;&#25345;&#32493;&#20445;&#30041;&#65292;&#26080;&#27861;&#36890;&#36807;&#26631;&#20934;&#30340;&#23433;&#20840;&#35757;&#32451;&#25216;&#26415;&#65288;&#21253;&#25324;&#30417;&#30563;&#24494;&#35843;&#12289;&#24378;&#21270;&#23398;&#20064;&#21644;&#23545;&#25239;&#24615;&#35757;&#32451;&#65289;&#31227;&#38500;&#12290;&#26263;&#38376;&#34892;&#20026;&#22312;&#26368;&#22823;&#30340;&#27169;&#22411;&#21644;&#35757;&#32451;&#25104;&#20135;&#29983;&#24605;&#32500;&#38142;&#30340;&#27169;&#22411;&#20013;&#26368;&#20026;&#25345;&#20037;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans are capable of strategically deceptive behavior: behaving helpfully in most situations, but then behaving very differently in order to pursue alternative objectives when given the opportunity. If an AI system learned such a deceptive strategy, could we detect it and remove it using current state-of-the-art safety training techniques? To study this question, we construct proof-of-concept examples of deceptive behavior in large language models (LLMs). For example, we train models that write secure code when the prompt states that the year is 2023, but insert exploitable code when the stated year is 2024. We find that such backdoored behavior can be made persistent, so that it is not removed by standard safety training techniques, including supervised fine-tuning, reinforcement learning, and adversarial training (eliciting unsafe behavior and then training to remove it). The backdoored behavior is most persistent in the largest models and in models trained to produce chain-of-thoug
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#23398;&#20064;&#30340;&#24863;&#30693;-&#34892;&#21160;-&#36890;&#20449;(LPAC)&#26550;&#26500;&#65292;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#29615;&#22659;&#24863;&#30693;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#26426;&#22120;&#20154;&#20043;&#38388;&#30340;&#20449;&#24687;&#20132;&#27969;&#65292;&#27973;&#23618;&#22810;&#23618;&#24863;&#30693;&#26426;&#35745;&#31639;&#26426;&#22120;&#20154;&#30340;&#21160;&#20316;&#12290;&#20351;&#29992;&#38598;&#20013;&#24335;&#26174;&#24494;&#31639;&#27861;&#35757;&#32451;&#27169;&#22411;&#65292;&#23454;&#29616;&#26426;&#22120;&#20154;&#32676;&#20307;&#30340;&#21327;&#20316;&#12290;</title><link>http://arxiv.org/abs/2401.04855</link><description>&lt;p&gt;
LPAC: &#21487;&#23398;&#20064;&#30340;&#24863;&#30693;-&#34892;&#21160;-&#36890;&#20449;&#24490;&#29615;&#21450;&#20854;&#22312;&#35206;&#30422;&#25511;&#21046;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
LPAC: Learnable Perception-Action-Communication Loops with Applications to Coverage Control. (arXiv:2401.04855v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04855
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#23398;&#20064;&#30340;&#24863;&#30693;-&#34892;&#21160;-&#36890;&#20449;(LPAC)&#26550;&#26500;&#65292;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#29615;&#22659;&#24863;&#30693;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#26426;&#22120;&#20154;&#20043;&#38388;&#30340;&#20449;&#24687;&#20132;&#27969;&#65292;&#27973;&#23618;&#22810;&#23618;&#24863;&#30693;&#26426;&#35745;&#31639;&#26426;&#22120;&#20154;&#30340;&#21160;&#20316;&#12290;&#20351;&#29992;&#38598;&#20013;&#24335;&#26174;&#24494;&#31639;&#27861;&#35757;&#32451;&#27169;&#22411;&#65292;&#23454;&#29616;&#26426;&#22120;&#20154;&#32676;&#20307;&#30340;&#21327;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35206;&#30422;&#25511;&#21046;&#26159;&#25351;&#23548;&#26426;&#22120;&#20154;&#32676;&#20307;&#21327;&#21516;&#30417;&#27979;&#26410;&#30693;&#30340;&#24863;&#20852;&#36259;&#29305;&#24449;&#25110;&#29616;&#35937;&#30340;&#38382;&#39064;&#12290;&#22312;&#26377;&#38480;&#30340;&#36890;&#20449;&#21644;&#24863;&#30693;&#33021;&#21147;&#30340;&#20998;&#25955;&#35774;&#32622;&#20013;&#65292;&#36825;&#20010;&#38382;&#39064;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#23398;&#20064;&#30340;&#24863;&#30693;-&#34892;&#21160;-&#36890;&#20449;(LPAC)&#26550;&#26500;&#26469;&#35299;&#20915;&#35206;&#30422;&#25511;&#21046;&#38382;&#39064;&#12290;&#22312;&#35813;&#35299;&#20915;&#26041;&#26696;&#20013;&#65292;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#22788;&#29702;&#20102;&#29615;&#22659;&#30340;&#23616;&#37096;&#24863;&#30693;&#65307;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#23454;&#29616;&#20102;&#37051;&#36817;&#26426;&#22120;&#20154;&#20043;&#38388;&#30340;&#30456;&#20851;&#20449;&#24687;&#36890;&#20449;&#65307;&#26368;&#21518;&#65292;&#27973;&#23618;&#22810;&#23618;&#24863;&#30693;&#26426;(MLP)&#35745;&#31639;&#26426;&#22120;&#20154;&#30340;&#21160;&#20316;&#12290;&#36890;&#20449;&#27169;&#22359;&#20013;&#30340;GNN&#36890;&#36807;&#35745;&#31639;&#24212;&#35813;&#19982;&#37051;&#23621;&#36890;&#20449;&#21738;&#20123;&#20449;&#24687;&#20197;&#21450;&#22914;&#20309;&#21033;&#29992;&#25509;&#25910;&#21040;&#30340;&#20449;&#24687;&#37319;&#21462;&#36866;&#24403;&#30340;&#34892;&#21160;&#26469;&#23454;&#29616;&#26426;&#22120;&#20154;&#32676;&#20307;&#30340;&#21327;&#20316;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#30693;&#26195;&#25972;&#20010;&#29615;&#22659;&#30340;&#38598;&#20013;&#24335;&#26174;&#24494;&#31639;&#27861;&#26469;&#36827;&#34892;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Coverage control is the problem of navigating a robot swarm to collaboratively monitor features or a phenomenon of interest not known a priori. The problem is challenging in decentralized settings with robots that have limited communication and sensing capabilities. This paper proposes a learnable Perception-Action-Communication (LPAC) architecture for the coverage control problem. In the proposed solution, a convolution neural network (CNN) processes localized perception of the environment; a graph neural network (GNN) enables communication of relevant information between neighboring robots; finally, a shallow multi-layer perceptron (MLP) computes robot actions. The GNN in the communication module enables collaboration in the robot swarm by computing what information to communicate with neighbors and how to use received information to take appropriate actions. We train models using imitation learning with a centralized clairvoyant algorithm that is aware of the entire environment. Eva
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#30697;&#38453;&#20998;&#26512;&#26041;&#27861;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25512;&#29702;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#36319;&#38543;&#27169;&#24335;&#12290;&#22312;&#27169;&#25311;&#25968;&#25454;&#38598;&#21644;&#22768;&#38899;&#35760;&#24405;&#25968;&#25454;&#38598;&#20013;&#65292;&#35813;&#26694;&#26550;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#65292;&#24182;&#33021;&#22815;&#26816;&#27979;&#20986;&#21152;&#23494;&#36135;&#24065;&#25968;&#25454;&#38598;&#20013;&#30340;&#36319;&#38543;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2401.02860</link><description>&lt;p&gt;
&#21464;&#21270;&#28382;&#21518;&#27169;&#24335;&#36319;&#38543;&#20851;&#31995;&#25512;&#29702;&#30340;&#26102;&#38388;&#24207;&#21015;&#30697;&#38453;&#20998;&#26512;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Framework for Variable-lag Motif Following Relation Inference In Time Series using Matrix Profile analysis. (arXiv:2401.02860v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02860
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#30697;&#38453;&#20998;&#26512;&#26041;&#27861;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25512;&#29702;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#36319;&#38543;&#27169;&#24335;&#12290;&#22312;&#27169;&#25311;&#25968;&#25454;&#38598;&#21644;&#22768;&#38899;&#35760;&#24405;&#25968;&#25454;&#38598;&#20013;&#65292;&#35813;&#26694;&#26550;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#65292;&#24182;&#33021;&#22815;&#26816;&#27979;&#20986;&#21152;&#23494;&#36135;&#24065;&#25968;&#25454;&#38598;&#20013;&#30340;&#36319;&#38543;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#36947;&#35841;&#36319;&#38543;&#35841;&#20197;&#21450;&#20182;&#20204;&#36319;&#38543;&#30340;&#27169;&#24335;&#26159;&#29702;&#35299;&#38598;&#20307;&#34892;&#20026;&#65288;&#22914;&#20154;&#32676;&#65292;&#40060;&#32676;&#25110;&#32929;&#24066;&#65289;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#26102;&#38388;&#24207;&#21015;&#26159;&#29992;&#20110;&#33719;&#21462;&#36319;&#38543;&#20851;&#31995;&#27934;&#23519;&#30340;&#36164;&#28304;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#36319;&#38543;&#27169;&#24335;&#25110;&#27169;&#24335;&#22312;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#21457;&#29616;&#35299;&#20915;&#26041;&#26696;&#24182;&#19981;&#26126;&#26174;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24418;&#24335;&#21270;&#20102;&#20004;&#20010;&#26102;&#38388;&#24207;&#21015;&#20043;&#38388;&#30340;&#36319;&#38543;&#27169;&#24335;&#27010;&#24565;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#25512;&#26029;&#20004;&#20010;&#26102;&#38388;&#24207;&#21015;&#20043;&#38388;&#36319;&#38543;&#27169;&#24335;&#30340;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#20174;&#26102;&#38388;&#24207;&#21015;&#20013;&#26816;&#32034;&#27169;&#24335;&#65292;&#31216;&#20026;&#30697;&#38453;&#20998;&#26512;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#25552;&#20986;&#30340;&#26694;&#26550;&#19982;&#20960;&#20010;&#22522;&#20934;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#22312;&#27169;&#25311;&#25968;&#25454;&#38598;&#20013;&#65292;&#35813;&#26694;&#26550;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#12290;&#22312;&#22768;&#38899;&#35760;&#24405;&#25968;&#25454;&#38598;&#20013;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#22312;&#19968;&#23545;&#26102;&#38388;&#24207;&#21015;&#20013;&#26816;&#32034;&#20986;&#20004;&#20301;&#27468;&#25163;&#30456;&#20114;&#36319;&#38543;&#21809;&#27468;&#30340;&#36319;&#38543;&#27169;&#24335;&#12290;&#22312;&#21152;&#23494;&#36135;&#24065;&#25968;&#25454;&#38598;&#20013;&#65292;
&lt;/p&gt;
&lt;p&gt;
Knowing who follows whom and what patterns they are following are crucial steps to understand collective behaviors (e.g. a group of human, a school of fish, or a stock market). Time series is one of resources that can be used to get insight regarding following relations. However, the concept of following patterns or motifs and the solution to find them in time series are not obvious. In this work, we formalize a concept of following motifs between two time series and present a framework to infer following patterns between two time series. The framework utilizes one of efficient and scalable methods to retrieve motifs from time series called the Matrix Profile Method. We compare our proposed framework with several baselines. The framework performs better than baselines in the simulation datasets. In the dataset of sound recording, the framework is able to retrieve the following motifs within a pair of time series that two singers sing following each other. In the cryptocurrency dataset,
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;TopCoW&#25361;&#25112;&#65292;&#36890;&#36807;&#21457;&#24067;&#20855;&#26377;13&#31181;&#34880;&#31649;&#32452;&#20998;&#27880;&#37322;&#30340;Willis&#24490;&#29615;&#65288;CoW&#65289;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#34394;&#25311;&#29616;&#23454;&#65288;VR&#65289;&#25216;&#26415;&#36827;&#34892;&#25299;&#25169;&#24863;&#30693;&#35299;&#21078;&#20998;&#21106;&#65292;&#35299;&#20915;&#20102;&#25163;&#21160;&#21644;&#32791;&#26102;&#30340;CoW&#34920;&#24449;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2312.17670</link><description>&lt;p&gt;
TopCoW&#65306;&#22522;&#20110;&#25299;&#25169;&#24863;&#30693;&#35299;&#21078;&#20998;&#21106;&#30340;Willis&#24490;&#29615;&#65288;CoW&#65289;&#22312;CTA&#21644;MRA&#20013;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
TopCoW: Benchmarking Topology-Aware Anatomical Segmentation of the Circle of Willis (CoW) for CTA and MRA. (arXiv:2312.17670v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.17670
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;TopCoW&#25361;&#25112;&#65292;&#36890;&#36807;&#21457;&#24067;&#20855;&#26377;13&#31181;&#34880;&#31649;&#32452;&#20998;&#27880;&#37322;&#30340;Willis&#24490;&#29615;&#65288;CoW&#65289;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#34394;&#25311;&#29616;&#23454;&#65288;VR&#65289;&#25216;&#26415;&#36827;&#34892;&#25299;&#25169;&#24863;&#30693;&#35299;&#21078;&#20998;&#21106;&#65292;&#35299;&#20915;&#20102;&#25163;&#21160;&#21644;&#32791;&#26102;&#30340;CoW&#34920;&#24449;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Willis&#24490;&#29615;&#65288;CoW&#65289;&#26159;&#36830;&#25509;&#22823;&#33041;&#20027;&#35201;&#24490;&#29615;&#30340;&#37325;&#35201;&#21160;&#33033;&#32593;&#32476;&#12290;&#20854;&#34880;&#31649;&#32467;&#26500;&#34987;&#35748;&#20026;&#24433;&#21709;&#30528;&#20005;&#37325;&#31070;&#32463;&#34880;&#31649;&#30142;&#30149;&#30340;&#39118;&#38505;&#12289;&#20005;&#37325;&#31243;&#24230;&#21644;&#20020;&#24202;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#23545;&#39640;&#24230;&#21464;&#21270;&#30340;CoW&#35299;&#21078;&#36827;&#34892;&#34920;&#24449;&#20173;&#28982;&#26159;&#19968;&#39033;&#38656;&#35201;&#25163;&#21160;&#21644;&#32791;&#26102;&#30340;&#19987;&#23478;&#20219;&#21153;&#12290;CoW&#36890;&#24120;&#36890;&#36807;&#20004;&#31181;&#34880;&#31649;&#36896;&#24433;&#25104;&#20687;&#27169;&#24335;&#36827;&#34892;&#25104;&#20687;&#65292;&#21363;&#30913;&#20849;&#25391;&#34880;&#31649;&#25104;&#20687;&#65288;MRA&#65289;&#21644;&#35745;&#31639;&#26426;&#26029;&#23618;&#34880;&#31649;&#36896;&#24433;&#65288;CTA&#65289;&#65292;&#20294;&#26159;&#20851;&#20110;CTA&#30340;CoW&#35299;&#21078;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#21450;&#20854;&#27880;&#37322;&#38750;&#24120;&#26377;&#38480;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22312;2023&#24180;&#32452;&#32455;&#20102;TopCoW&#25361;&#25112;&#36187;&#65292;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#24102;&#26377;&#27880;&#37322;&#30340;CoW&#25968;&#25454;&#38598;&#12290;TopCoW&#25968;&#25454;&#38598;&#26159;&#31532;&#19968;&#20010;&#20855;&#26377;13&#31181;&#21487;&#33021;&#30340;CoW&#34880;&#31649;&#32452;&#20998;&#30340;&#20307;&#32032;&#32423;&#27880;&#37322;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#34394;&#25311;&#29616;&#23454;&#65288;VR&#65289;&#25216;&#26415;&#23454;&#29616;&#12290;&#23427;&#20063;&#26159;&#31532;&#19968;&#20010;&#24102;&#26377;&#26469;&#33258;&#21516;&#19968;&#24739;&#32773;&#30340;&#25104;&#23545;MRA&#21644;CTA&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;TopCoW&#25361;&#25112;&#23558;CoW&#34920;&#24449;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#22810;&#31867;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Circle of Willis (CoW) is an important network of arteries connecting major circulations of the brain. Its vascular architecture is believed to affect the risk, severity, and clinical outcome of serious neuro-vascular diseases. However, characterizing the highly variable CoW anatomy is still a manual and time-consuming expert task. The CoW is usually imaged by two angiographic imaging modalities, magnetic resonance angiography (MRA) and computed tomography angiography (CTA), but there exist limited public datasets with annotations on CoW anatomy, especially for CTA. Therefore we organized the TopCoW Challenge in 2023 with the release of an annotated CoW dataset. The TopCoW dataset was the first public dataset with voxel-level annotations for thirteen possible CoW vessel components, enabled by virtual-reality (VR) technology. It was also the first large dataset with paired MRA and CTA from the same patients. TopCoW challenge formalized the CoW characterization problem as a multiclas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25351;&#25968;&#26063;&#30340;&#21452;&#37325;&#20943;&#24615;&#21644;&#38500;&#24615;&#24402;&#19968;&#21270;&#26041;&#27861;&#24341;&#36215;&#30340;&#24046;&#24322;&#65292;&#25552;&#20986;&#20102;&#19968;&#23545;Bregman&#21644;Jensen&#25955;&#24230;&#12290;&#21516;&#26102;&#65292;&#35777;&#26126;&#20102;&#945;&#25955;&#24230;&#21487;&#20197;&#36890;&#36807;&#20998;&#37197;&#20989;&#25968;&#24341;&#36215;&#32553;&#25918;&#945;&#20559;&#26012;Jensen&#25955;&#24230;&#65292;&#20174;&#32780;&#27604;&#36739;&#20102;&#19981;&#21516;&#24402;&#19968;&#21270;&#26041;&#27861;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2312.12849</link><description>&lt;p&gt;
&#25351;&#25968;&#26063;&#30340;&#21452;&#37325;&#20943;&#24615;&#21644;&#38500;&#24615;&#24402;&#19968;&#21270;&#24341;&#36215;&#30340;&#24046;&#24322;&#21450;&#20854;&#20984;&#21464;&#24418;
&lt;/p&gt;
&lt;p&gt;
Divergences induced by dual subtractive and divisive normalizations of exponential families and their convex deformations. (arXiv:2312.12849v2 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.12849
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25351;&#25968;&#26063;&#30340;&#21452;&#37325;&#20943;&#24615;&#21644;&#38500;&#24615;&#24402;&#19968;&#21270;&#26041;&#27861;&#24341;&#36215;&#30340;&#24046;&#24322;&#65292;&#25552;&#20986;&#20102;&#19968;&#23545;Bregman&#21644;Jensen&#25955;&#24230;&#12290;&#21516;&#26102;&#65292;&#35777;&#26126;&#20102;&#945;&#25955;&#24230;&#21487;&#20197;&#36890;&#36807;&#20998;&#37197;&#20989;&#25968;&#24341;&#36215;&#32553;&#25918;&#945;&#20559;&#26012;Jensen&#25955;&#24230;&#65292;&#20174;&#32780;&#27604;&#36739;&#20102;&#19981;&#21516;&#24402;&#19968;&#21270;&#26041;&#27861;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#25968;&#26063;&#26159;&#32479;&#35745;&#23398;&#12289;&#20449;&#24687;&#29702;&#35770;&#21644;&#26426;&#22120;&#23398;&#20064;&#31561;&#39046;&#22495;&#20013;&#30340;&#37325;&#35201;&#27169;&#22411;&#12290;&#25351;&#25968;&#26063;&#21487;&#20197;&#36890;&#36807;&#32047;&#31215;&#37327;&#25110;&#33258;&#30001;&#33021;&#20989;&#25968;&#20943;&#24615;&#24402;&#19968;&#21270;&#65292;&#20063;&#21487;&#20197;&#36890;&#36807;&#20998;&#37197;&#20989;&#25968;&#38500;&#24615;&#24402;&#19968;&#21270;&#12290;&#36825;&#20004;&#31181;&#24402;&#19968;&#21270;&#26041;&#27861;&#37117;&#26159;&#20005;&#26684;&#30340;&#20984;&#24179;&#28369;&#20989;&#25968;&#65292;&#20250;&#24341;&#21457;&#19968;&#23545;Bregman&#21644;Jensen&#25955;&#24230;&#12290;&#24050;&#30693;&#25351;&#25968;&#26063;&#30340;&#27010;&#29575;&#23494;&#24230;&#20043;&#38388;&#30340;&#20559;&#26012;&#24052;&#27663;&#36317;&#31163;&#31561;&#20110;&#36890;&#36807;&#32047;&#31215;&#37327;&#20989;&#25968;&#24341;&#36215;&#30340;&#20559;&#26012;Jensen&#25955;&#24230;&#65292;&#32780;&#22312;&#26497;&#38480;&#24773;&#20917;&#19979;&#65292;&#20559;&#21521;Kullback-Leibler&#25955;&#24230;&#21017;&#20250;&#36716;&#21464;&#20026;&#21453;&#21521;Bregman&#25955;&#24230;&#12290;&#26412;&#25991;&#39318;&#20808;&#35777;&#26126;&#20102;&#25351;&#25968;&#26063;&#30340;&#38750;&#24402;&#19968;&#21270;&#27010;&#29575;&#23494;&#24230;&#20043;&#38388;&#30340;&#945;&#25955;&#24230;&#31561;&#20110;&#30001;&#20998;&#37197;&#20989;&#25968;&#24341;&#36215;&#30340;&#32553;&#25918;&#945;&#20559;&#26012;Jensen&#25955;&#24230;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#27604;&#36739;&#19981;&#21516;&#20943;&#24615;&#21644;&#38500;&#24615;&#24402;&#19968;&#21270;&#26041;&#27861;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exponential families are statistical models which are the workhorses in statistics, information theory, and machine learning among others. An exponential family can either be normalized subtractively by its cumulant or free energy function or equivalently normalized divisively by its partition function. Both subtractive and divisive normalizers are strictly convex and smooth functions inducing pairs of Bregman and Jensen divergences. It is well-known that skewed Bhattacharryya distances between probability densities of an exponential family amounts to skewed Jensen divergences induced by the cumulant function between their corresponding natural parameters, and in limit cases that the sided Kullback-Leibler divergences amount to reverse-sided Bregman divergences. In this paper, we first show that the $\alpha$-divergences between unnormalized densities of an exponential family amounts to scaled $\alpha$-skewed Jensen divergences induced by the partition function. We then show how compara
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#38024;&#23545;&#32852;&#37030;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#24322;&#26500;&#26631;&#27880;&#22122;&#22768;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;FedA3I&#26041;&#27861;&#65292;&#36890;&#36807;&#27880;&#37322;&#36136;&#37327;&#24863;&#30693;&#32858;&#21512;&#26469;&#35299;&#20915;&#39640;&#36136;&#37327;&#23458;&#25143;&#31471;&#23545;&#32852;&#37030;&#23398;&#20064;&#30340;&#26356;&#22823;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2312.12838</link><description>&lt;p&gt;
FedA3I:&#38024;&#23545;&#24322;&#26500;&#26631;&#27880;&#22122;&#22768;&#30340;&#27880;&#37322;&#36136;&#37327;&#24863;&#30693;&#32858;&#21512;&#30340;&#32852;&#37030;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
FedA3I: Annotation Quality-Aware Aggregation for Federated Medical Image Segmentation against Heterogeneous Annotation Noise. (arXiv:2312.12838v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.12838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;&#32852;&#37030;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#24322;&#26500;&#26631;&#27880;&#22122;&#22768;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;FedA3I&#26041;&#27861;&#65292;&#36890;&#36807;&#27880;&#37322;&#36136;&#37327;&#24863;&#30693;&#32858;&#21512;&#26469;&#35299;&#20915;&#39640;&#36136;&#37327;&#23458;&#25143;&#31471;&#23545;&#32852;&#37030;&#23398;&#20064;&#30340;&#26356;&#22823;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30001;&#20110;&#20854;&#20445;&#25252;&#38544;&#31169;&#30340;&#29305;&#24615;&#65292;&#24050;&#32463;&#25104;&#20026;&#20998;&#25955;&#24335;&#21307;&#23398;&#25968;&#25454;&#35757;&#32451;&#20998;&#21106;&#27169;&#22411;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#24573;&#35270;&#20102;&#22312;&#30495;&#23454;&#21307;&#23398;&#25968;&#25454;&#38598;&#20013;&#24120;&#35265;&#30340;&#27880;&#37322;&#22122;&#22768;&#65292;&#36825;&#38480;&#21046;&#20102;FL&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#30830;&#23450;&#24182;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#23545;&#20110;&#38382;&#39064;&#30340;&#24418;&#24335;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36718;&#24275;&#28436;&#21270;&#26469;&#23545;&#27599;&#20010;&#23458;&#25143;&#31471;&#20869;&#20687;&#32032;&#20043;&#38388;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;Non-IID&#65289;&#22122;&#22768;&#36827;&#34892;&#24314;&#27169;&#65292;&#28982;&#21518;&#25193;&#23637;&#21040;&#22810;&#28304;&#25968;&#25454;&#30340;&#24773;&#20917;&#65292;&#24418;&#25104;&#19968;&#20010;&#24322;&#26500;&#22122;&#22768;&#27169;&#22411;&#65288;&#21363;&#20132;&#21449;&#21306;&#21035;&#26631;&#27880;&#22122;&#22768;&#65289;&#12290;&#20026;&#20102;&#20174;&#20855;&#26377;&#36825;&#20004;&#20010;&#23618;&#27425;&#30340;&#38750;I-ID&#22122;&#22768;&#30340;&#27880;&#37322;&#20013;&#23454;&#29616;&#20581;&#22766;&#30340;&#23398;&#20064;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#27169;&#22411;&#32858;&#21512;&#20013;&#25968;&#25454;&#36136;&#37327;&#30340;&#37325;&#35201;&#24615;&#65292;&#20801;&#35768;&#39640;&#36136;&#37327;&#30340;&#23458;&#25143;&#31471;&#23545;FL&#20135;&#29983;&#26356;&#22823;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20855;&#26377;&#27880;&#37322;&#36136;&#37327;&#24863;&#30693;&#32858;&#21512;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;FedA3I&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) has emerged as a promising paradigm for training segmentation models on decentralized medical data, owing to its privacy-preserving property. However, existing research overlooks the prevalent annotation noise encountered in real-world medical datasets, which limits the performance ceilings of FL. In this paper, we, for the first time, identify and tackle this problem. For problem formulation, we propose a contour evolution for modeling non-independent and identically distributed (Non-IID) noise across pixels within each client and then extend it to the case of multi-source data to form a heterogeneous noise model (i.e., Non-IID annotation noise across clients). For robust learning from annotations with such two-level Non-IID noise, we emphasize the importance of data quality in model aggregation, allowing high-quality clients to have a greater impact on FL. To achieve this, we propose Federated learning with Annotation quAlity-aware AggregatIon, named FedA3I, b
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#24341;&#23548;&#38750;&#33258;&#22238;&#24402;&#30693;&#35782;&#33976;&#39311;&#65288;GNARKD&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#23558;&#33258;&#22238;&#24402;&#27169;&#22411;&#20013;&#30340;&#20851;&#38190;&#32452;&#20214;&#20445;&#30041;&#22312;&#32593;&#32476;&#26550;&#26500;&#20013;&#65292;&#20174;&#32780;&#33719;&#24471;&#20855;&#26377;&#20302;&#25512;&#29702;&#24310;&#36831;&#30340;&#39640;&#24615;&#33021;&#38750;&#33258;&#22238;&#24402;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#27714;&#35299;&#22120;&#12290;</title><link>http://arxiv.org/abs/2312.12469</link><description>&lt;p&gt;
&#23558;&#33258;&#22238;&#24402;&#27169;&#22411;&#25552;&#28860;&#20026;&#20855;&#26377;&#36739;&#24555;&#25512;&#29702;&#36895;&#24230;&#30340;&#39640;&#24615;&#33021;&#38750;&#33258;&#22238;&#24402;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
Distilling Autoregressive Models to Obtain High-Performance Non-Autoregressive Solvers for Vehicle Routing Problems with Faster Inference Speed. (arXiv:2312.12469v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.12469
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#24341;&#23548;&#38750;&#33258;&#22238;&#24402;&#30693;&#35782;&#33976;&#39311;&#65288;GNARKD&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#23558;&#33258;&#22238;&#24402;&#27169;&#22411;&#20013;&#30340;&#20851;&#38190;&#32452;&#20214;&#20445;&#30041;&#22312;&#32593;&#32476;&#26550;&#26500;&#20013;&#65292;&#20174;&#32780;&#33719;&#24471;&#20855;&#26377;&#20302;&#25512;&#29702;&#24310;&#36831;&#30340;&#39640;&#24615;&#33021;&#38750;&#33258;&#22238;&#24402;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#27714;&#35299;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#37319;&#29992;&#33258;&#22238;&#24402;&#65288;AR&#65289;&#25110;&#38750;&#33258;&#22238;&#24402;&#65288;NAR&#65289;&#23398;&#20064;&#26041;&#27861;&#65292;&#31070;&#32463;&#26500;&#24314;&#27169;&#22411;&#22312;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#65288;VRP&#65289;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;&#34429;&#28982;AR&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#30001;&#20110;&#20854;&#39034;&#24207;&#29983;&#25104;&#24615;&#36136;&#65292;&#25512;&#29702;&#24310;&#36831;&#36890;&#24120;&#36739;&#39640;&#12290;&#30456;&#21453;&#65292;NAR&#27169;&#22411;&#20197;&#20302;&#25512;&#29702;&#24310;&#36831;&#24182;&#34892;&#29983;&#25104;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#36890;&#24120;&#34920;&#29616;&#20986;&#36739;&#20302;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#24341;&#23548;&#38750;&#33258;&#22238;&#24402;&#30693;&#35782;&#33976;&#39311;&#65288;GNARKD&#65289;&#26041;&#27861;&#65292;&#20197;&#33719;&#24471;&#20855;&#26377;&#20302;&#25512;&#29702;&#24310;&#36831;&#30340;&#39640;&#24615;&#33021;NAR&#27169;&#22411;&#12290;GNARKD&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#65292;&#21435;&#38500;AR&#27169;&#22411;&#20013;&#39034;&#24207;&#29983;&#25104;&#30340;&#32422;&#26463;&#65292;&#21516;&#26102;&#20445;&#30041;&#32593;&#32476;&#26550;&#26500;&#20013;&#23398;&#21040;&#30340;&#20851;&#38190;&#32452;&#20214;&#65292;&#33719;&#24471;&#30456;&#24212;&#30340;NAR&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;GNARKD&#24212;&#29992;&#20110;&#19977;&#31181;&#24191;&#27867;&#37319;&#29992;&#30340;AR&#27169;&#22411;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#23454;&#20363;&#19978;&#33719;&#24471;NAR VRP&#27714;&#35299;&#22120;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural construction models have shown promising performance for Vehicle Routing Problems (VRPs) by adopting either the Autoregressive (AR) or Non-Autoregressive (NAR) learning approach. While AR models produce high-quality solutions, they generally have a high inference latency due to their sequential generation nature. Conversely, NAR models generate solutions in parallel with a low inference latency but generally exhibit inferior performance. In this paper, we propose a generic Guided Non-Autoregressive Knowledge Distillation (GNARKD) method to obtain high-performance NAR models having a low inference latency. GNARKD removes the constraint of sequential generation in AR models while preserving the learned pivotal components in the network architecture to obtain the corresponding NAR models through knowledge distillation. We evaluate GNARKD by applying it to three widely adopted AR models to obtain NAR VRP solvers for both synthesized and real-world instances. The experimental results
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#20013;&#35782;&#21035;&#21644;&#32416;&#27491;&#38169;&#35823;&#26631;&#35760;&#26679;&#26412;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21512;&#20316;&#20249;&#20276;&#20998;&#31867;&#22120;&#21644;&#20114;&#30456;&#30417;&#30563;&#33539;&#24335;&#65292;&#20197;&#21450;&#20351;&#29992;&#27169;&#31946;&#26426;&#21046;&#23454;&#29616;&#20998;&#31867;&#22120;&#20043;&#38388;&#30340;&#21327;&#20316;&#12290;</title><link>http://arxiv.org/abs/2312.11034</link><description>&lt;p&gt;
&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#20013;&#30340;&#21512;&#20316;&#20249;&#20276;
&lt;/p&gt;
&lt;p&gt;
Partial Label Learning with a Partner. (arXiv:2312.11034v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.11034
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#20013;&#35782;&#21035;&#21644;&#32416;&#27491;&#38169;&#35823;&#26631;&#35760;&#26679;&#26412;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21512;&#20316;&#20249;&#20276;&#20998;&#31867;&#22120;&#21644;&#20114;&#30456;&#30417;&#30563;&#33539;&#24335;&#65292;&#20197;&#21450;&#20351;&#29992;&#27169;&#31946;&#26426;&#21046;&#23454;&#29616;&#20998;&#31867;&#22120;&#20043;&#38388;&#30340;&#21327;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#20013;&#65292;&#27599;&#20010;&#23454;&#20363;&#19982;&#19968;&#32452;&#20505;&#36873;&#26631;&#31614;&#30456;&#20851;&#32852;&#65292;&#20854;&#20013;&#21482;&#26377;&#19968;&#20010;&#26159;&#30495;&#23454;&#26631;&#31614;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#22823;&#22810;&#20851;&#27880;&#26500;&#24314;&#24378;&#22823;&#30340;&#20998;&#31867;&#22120;&#26469;&#20272;&#35745;&#20505;&#36873;&#26631;&#31614;&#30340;&#26631;&#31614;&#32622;&#20449;&#24230;&#65292;&#20197;&#35782;&#21035;&#27491;&#30830;&#30340;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#24456;&#38590;&#32416;&#27491;&#38169;&#35823;&#26631;&#35760;&#30340;&#26679;&#26412;&#12290;&#20026;&#20102;&#24110;&#21161;&#29616;&#26377;&#30340;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#26041;&#27861;&#35782;&#21035;&#21644;&#32416;&#27491;&#38169;&#35823;&#26631;&#35760;&#30340;&#26679;&#26412;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#21512;&#20316;&#20249;&#20276;&#20998;&#31867;&#22120;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#8220;&#20114;&#30456;&#30417;&#30563;&#8221;&#33539;&#24335;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#26681;&#25454;&#38544;&#21547;&#30340;&#20107;&#23454;&#23454;&#20363;&#21270;&#20102;&#21512;&#20316;&#20249;&#20276;&#20998;&#31867;&#22120;&#65292;&#21363;&#19968;&#20010;&#26679;&#26412;&#30340;&#38750;&#20505;&#36873;&#26631;&#31614;&#19981;&#24212;&#35813;&#34987;&#20998;&#37197;&#32473;&#23427;&#65292;&#36825;&#22312;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#20013;&#20855;&#26377;&#20869;&#22312;&#30340;&#20934;&#30830;&#24615;&#24182;&#19988;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#21327;&#20316;&#39033;&#26469;&#23558;&#22522;&#30784;&#20998;&#31867;&#22120;&#21644;&#21512;&#20316;&#20249;&#20276;&#20998;&#31867;&#22120;&#38142;&#25509;&#36215;&#26469;&#12290;&#22312;&#27599;&#20010;&#20114;&#30456;&#30417;&#30563;&#30340;&#38454;&#27573;&#20013;&#65292;&#20004;&#20010;&#20998;&#31867;&#22120;&#23558;&#36890;&#36807;&#19968;&#20010;&#27169;&#31946;&#26426;&#21046;&#20114;&#30456;&#27169;&#31946;&#24444;&#27492;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
In partial label learning (PLL), each instance is associated with a set of candidate labels among which only one is ground-truth. The majority of the existing works focuses on constructing robust classifiers to estimate the labeling confidence of candidate labels in order to identify the correct one. However, these methods usually struggle to rectify mislabeled samples. To help existing PLL methods identify and rectify mislabeled samples, in this paper, we introduce a novel partner classifier and propose a novel ``mutual supervision'' paradigm. Specifically, we instantiate the partner classifier predicated on the implicit fact that non-candidate labels of a sample should not be assigned to it, which is inherently accurate and has not been fully investigated in PLL. Furthermore, a novel collaborative term is formulated to link the base classifier and the partner one. During each stage of mutual supervision, both classifiers will blur each other's predictions through a blurring mechanism
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#32423;&#23398;&#20064;&#31639;&#27861;SHSR&#65292;&#29992;&#20110;&#20943;&#23569;AutoML&#20013;&#30340;&#36229;&#21442;&#25968;&#31354;&#38388;&#65292;&#20943;&#23569;&#20102;&#32422;30%&#30340;&#25191;&#34892;&#26102;&#38388;&#24182;&#19988;&#24615;&#33021;&#25439;&#22833;&#23567;&#20110;0.1%&#12290;</title><link>http://arxiv.org/abs/2312.06305</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#20013;&#39034;&#24207;&#36229;&#21442;&#25968;&#31354;&#38388;&#32553;&#20943;&#30340;&#20803;&#32423;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Meta-Level Learning Algorithm for Sequential Hyper-Parameter Space Reduction in AutoML. (arXiv:2312.06305v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.06305
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#32423;&#23398;&#20064;&#31639;&#27861;SHSR&#65292;&#29992;&#20110;&#20943;&#23569;AutoML&#20013;&#30340;&#36229;&#21442;&#25968;&#31354;&#38388;&#65292;&#20943;&#23569;&#20102;&#32422;30%&#30340;&#25191;&#34892;&#26102;&#38388;&#24182;&#19988;&#24615;&#33021;&#25439;&#22833;&#23567;&#20110;0.1%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;AutoML&#24179;&#21488;&#19978;&#65292;&#27599;&#20010;&#20998;&#26512;&#27493;&#39588;&#37117;&#26377;&#35768;&#22810;&#31639;&#27861;&#21487;&#20379;&#23581;&#35797;&#65292;&#20363;&#22914;&#25554;&#34917;&#31639;&#27861;&#12289;&#36716;&#25442;&#31639;&#27861;&#12289;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#21644;&#24314;&#27169;&#31639;&#27861;&#31561;&#12290;&#25214;&#21040;&#26368;&#20339;&#30340;&#31639;&#27861;&#32452;&#21512;&#21644;&#36229;&#21442;&#25968;&#20540;&#26159;&#35745;&#31639;&#19978;&#26114;&#36149;&#30340;&#65292;&#22240;&#20026;&#35201;&#25506;&#32034;&#30340;&#32452;&#21512;&#25968;&#37327;&#23548;&#33268;&#31354;&#38388;&#30340;&#25351;&#25968;&#29190;&#28856;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#39034;&#24207;&#36229;&#21442;&#25968;&#31354;&#38388;&#32553;&#20943;&#65288;SHSR&#65289;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#20943;&#23569;AutoML&#24037;&#20855;&#25152;&#38656;&#30340;&#31354;&#38388;&#65292;&#24182;&#19988;&#24615;&#33021;&#25439;&#22833;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#12290;SHSR&#26159;&#19968;&#31181;&#20803;&#32423;&#23398;&#20064;&#31639;&#27861;&#65292;&#23427;&#20998;&#26512;AutoML&#24037;&#20855;&#22312;&#20960;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#36807;&#21435;&#36816;&#34892;&#32467;&#26524;&#65292;&#24182;&#23398;&#20064;&#21738;&#20123;&#36229;&#21442;&#25968;&#20540;&#21487;&#20197;&#20174;&#35201;&#20998;&#26512;&#30340;&#26032;&#25968;&#25454;&#38598;&#20013;&#36807;&#28388;&#25481;&#12290;SHSR&#22312;284&#20010;&#20998;&#31867;&#38382;&#39064;&#21644;375&#20010;&#22238;&#24402;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#26174;&#31034;&#20986;&#32422;30%&#30340;&#25191;&#34892;&#26102;&#38388;&#32553;&#30701;&#21644;&#19981;&#21040;0.1%&#30340;&#24615;&#33021;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
AutoML platforms have numerous options for the algorithms to try for each step of the analysis, i.e., different possible algorithms for imputation, transformations, feature selection, and modelling. Finding the optimal combination of algorithms and hyper-parameter values is computationally expensive, as the number of combinations to explore leads to an exponential explosion of the space. In this paper, we present the Sequential Hyper-parameter Space Reduction (SHSR) algorithm that reduces the space for an AutoML tool with negligible drop in its predictive performance. SHSR is a meta-level learning algorithm that analyzes past runs of an AutoML tool on several datasets and learns which hyper-parameter values to filter out from consideration on a new dataset to analyze. SHSR is evaluated on 284 classification and 375 regression problems, showing an approximate 30% reduction in execution time with a performance drop of less than 0.1%.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#20915;&#31574;&#26641;&#27169;&#22411;&#30340;OOD&#27867;&#21270;&#65292;&#31216;&#20026;&#19981;&#21464;&#20915;&#31574;&#26641;&#65288;IDT&#65289;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#26641;&#30340;&#29983;&#38271;&#36807;&#31243;&#20013;&#24809;&#32602;&#19981;&#21516;&#29615;&#22659;&#19979;&#20998;&#35010;&#30340;&#19981;&#31283;&#23450;&#34892;&#20026;&#65292;&#26500;&#24314;&#20102;&#19981;&#21464;&#38543;&#26426;&#26862;&#26519;&#65288;IRF&#65289;&#12290;&#39564;&#35777;&#23454;&#39564;&#35777;&#26126;&#65292;&#30456;&#27604;&#38750;OOD&#26641;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24378;&#35843;&#20102;&#32771;&#34385;&#26641;&#27169;&#22411;&#30340;OOD&#27867;&#21270;&#30340;&#24517;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2312.04273</link><description>&lt;p&gt;
&#19981;&#21464;&#38543;&#26426;&#26862;&#26519;&#65306;&#22522;&#20110;&#26641;&#27169;&#22411;&#30340;ODD&#27867;&#21270;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Invariant Random Forest: Tree-Based Model Solution for OOD Generalization. (arXiv:2312.04273v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.04273
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#20915;&#31574;&#26641;&#27169;&#22411;&#30340;OOD&#27867;&#21270;&#65292;&#31216;&#20026;&#19981;&#21464;&#20915;&#31574;&#26641;&#65288;IDT&#65289;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#26641;&#30340;&#29983;&#38271;&#36807;&#31243;&#20013;&#24809;&#32602;&#19981;&#21516;&#29615;&#22659;&#19979;&#20998;&#35010;&#30340;&#19981;&#31283;&#23450;&#34892;&#20026;&#65292;&#26500;&#24314;&#20102;&#19981;&#21464;&#38543;&#26426;&#26862;&#26519;&#65288;IRF&#65289;&#12290;&#39564;&#35777;&#23454;&#39564;&#35777;&#26126;&#65292;&#30456;&#27604;&#38750;OOD&#26641;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24378;&#35843;&#20102;&#32771;&#34385;&#26641;&#27169;&#22411;&#30340;OOD&#27867;&#21270;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;Out-Of-Distribution&#65288;OOD&#65289;&#27867;&#21270;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#20027;&#39064;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21482;&#20851;&#27880;&#31070;&#32463;&#32593;&#32476;&#30340;&#30456;&#24212;&#26041;&#27861;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#20915;&#31574;&#26641;&#27169;&#22411;&#30340;OOD&#27867;&#21270;&#65292;&#31216;&#20026;&#19981;&#21464;&#20915;&#31574;&#26641;&#65288;IDT&#65289;&#12290;IDT&#36890;&#36807;&#22312;&#26641;&#30340;&#29983;&#38271;&#36807;&#31243;&#20013;&#20851;&#20110;&#22312;&#19981;&#21516;&#29615;&#22659;&#19979;&#20998;&#35010;&#30340;&#19981;&#31283;&#23450;/&#21464;&#21270;&#34892;&#20026;&#30340;&#24809;&#32602;&#39033;&#26469;&#25512;&#21160;&#20854;&#21457;&#23637;&#12290;&#20854;&#38598;&#25104;&#29256;&#26412;&#65292;&#19981;&#21464;&#38543;&#26426;&#26862;&#26519;&#65288;IRF&#65289;&#34987;&#26500;&#24314;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21463;&#21040;&#20102;&#19968;&#20010;&#22312;&#28201;&#21644;&#26465;&#20214;&#19979;&#30340;&#29702;&#35770;&#32467;&#26524;&#30340;&#21551;&#21457;&#65292;&#24182;&#36890;&#36807;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#30340;&#25968;&#20540;&#27979;&#35797;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#19982;&#38750;OOD&#26641;&#27169;&#22411;&#30456;&#27604;&#30340;&#20248;&#36234;&#24615;&#33021;&#24847;&#21619;&#30528;&#65292;&#32771;&#34385;&#26641;&#27169;&#22411;&#30340;OOD&#27867;&#21270;&#26159;&#32477;&#23545;&#24517;&#35201;&#30340;&#65292;&#24212;&#35813;&#32473;&#20104;&#26356;&#22810;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
Out-Of-Distribution (OOD) generalization is an essential topic in machine learning. However, recent research is only focusing on the corresponding methods for neural networks. This paper introduces a novel and effective solution for OOD generalization of decision tree models, named Invariant Decision Tree (IDT). IDT enforces a penalty term with regard to the unstable/varying behavior of a split across different environments during the growth of the tree. Its ensemble version, the Invariant Random Forest (IRF), is constructed. Our proposed method is motivated by a theoretical result under mild conditions, and validated by numerical tests with both synthetic and real datasets. The superior performance compared to non-OOD tree models implies that considering OOD generalization for tree models is absolutely necessary and should be given more attention.
&lt;/p&gt;</description></item><item><title>DKiS&#26159;&#19968;&#31181;&#22522;&#20110;&#31169;&#38053;&#30340;&#22270;&#20687;&#38544;&#20889;&#26415;&#65292;&#36890;&#36807;&#24341;&#20837;&#34928;&#20943;&#26435;&#37325;&#26469;&#25511;&#21046;&#20449;&#24687;&#30340;&#20256;&#36755;&#65292;&#25552;&#39640;&#20102;&#38544;&#20889;&#26415;&#30340;&#24615;&#33021;&#65292;&#24182;&#30830;&#20445;&#38544;&#34255;&#20449;&#24687;&#30340;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.18243</link><description>&lt;p&gt;
DKiS: &#34928;&#20943;&#26435;&#37325;&#21487;&#36870;&#22270;&#20687;&#38544;&#20889;&#26415;&#21450;&#31169;&#38053;
&lt;/p&gt;
&lt;p&gt;
DKiS: Decay weight invertible image steganography with private key. (arXiv:2311.18243v2 [cs.MM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.18243
&lt;/p&gt;
&lt;p&gt;
DKiS&#26159;&#19968;&#31181;&#22522;&#20110;&#31169;&#38053;&#30340;&#22270;&#20687;&#38544;&#20889;&#26415;&#65292;&#36890;&#36807;&#24341;&#20837;&#34928;&#20943;&#26435;&#37325;&#26469;&#25511;&#21046;&#20449;&#24687;&#30340;&#20256;&#36755;&#65292;&#25552;&#39640;&#20102;&#38544;&#20889;&#26415;&#30340;&#24615;&#33021;&#65292;&#24182;&#30830;&#20445;&#38544;&#34255;&#20449;&#24687;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#38544;&#20889;&#26415;&#26159;&#23558;&#20449;&#24687;&#38544;&#34255;&#22312;&#21478;&#19968;&#24133;&#22270;&#20687;&#20013;&#30340;&#23454;&#36341;&#65292;&#20256;&#32479;&#19978;&#65292;&#22312;&#20854;&#26041;&#27861;&#20844;&#24320;&#25110;&#36973;&#21463;&#25915;&#20987;&#26102;&#65292;&#20250;&#36935;&#21040;&#23433;&#20840;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#31169;&#38053;&#30340;&#26032;&#22411;&#22270;&#20687;&#38544;&#20889;&#26415;&#25216;&#26415;&#12290;&#36825;&#31181;&#26041;&#27861;&#30830;&#20445;&#38544;&#34255;&#20449;&#24687;&#30340;&#23433;&#20840;&#24615;&#65292;&#22240;&#20026;&#35775;&#38382;&#38656;&#35201;&#30456;&#24212;&#30340;&#31169;&#38053;&#65292;&#32780;&#19981;&#31649;&#22270;&#20687;&#38544;&#20889;&#26415;&#26041;&#27861;&#30340;&#20844;&#24320;&#30693;&#35782;&#22914;&#20309;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23454;&#39564;&#35777;&#25454;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#24182;&#23637;&#31034;&#20102;&#20854;&#29616;&#23454;&#19990;&#30028;&#30340;&#36866;&#29992;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#20102;&#21487;&#36870;&#22270;&#20687;&#38544;&#20889;&#26415;&#36807;&#31243;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#20174;&#31192;&#23494;&#21040;&#20027;&#26426;&#20256;&#36755;&#38750;&#24517;&#35201;&#25110;&#8220;&#22403;&#22334;&#8221;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#34928;&#20943;&#26435;&#37325;&#26469;&#25511;&#21046;&#20449;&#24687;&#30340;&#20256;&#36755;&#65292;&#26377;&#25928;&#22320;&#31579;&#36873;&#25481;&#26080;&#20851;&#25968;&#25454;&#65292;&#25552;&#39640;&#20102;&#22270;&#20687;&#38544;&#20889;&#26415;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image steganography, defined as the practice of concealing information within another image, traditionally encounters security challenges when its methods become publicly known or are under attack. To address this, a novel private key-based image steganography technique has been introduced. This approach ensures the security of the hidden information, as access requires a corresponding private key, regardless of the public knowledge of the steganography method. Experimental evidence has been presented, demonstrating the effectiveness of our method and showcasing its real-world applicability. Furthermore, a critical challenge in the invertible image steganography process has been identified by us: the transfer of non-essential, or `garbage', information from the secret to the host pipeline. To tackle this issue, the decay weight has been introduced to control the information transfer, effectively filtering out irrelevant data and enhancing the performance of image steganography. The cod
&lt;/p&gt;</description></item><item><title>&#36870;&#21521;&#35782;&#21035; (INVERT) &#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36830;&#25509;&#23398;&#20064;&#21040;&#30340;&#31070;&#32463;&#34920;&#31034;&#19982;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#27010;&#24565;&#65292;&#23454;&#29616;&#20102;&#23545;&#31070;&#32463;&#34920;&#31034;&#30340;&#26631;&#35760;&#24182;&#25552;&#20379;&#20102;&#32479;&#35745;&#26174;&#33879;&#24615;&#35780;&#20272;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2311.13594</link><description>&lt;p&gt;
&#22312;&#36870;&#21521;&#35782;&#21035;&#20013;&#26631;&#35760;&#31070;&#32463;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Labeling Neural Representations with Inverse Recognition. (arXiv:2311.13594v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.13594
&lt;/p&gt;
&lt;p&gt;
&#36870;&#21521;&#35782;&#21035; (INVERT) &#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36830;&#25509;&#23398;&#20064;&#21040;&#30340;&#31070;&#32463;&#34920;&#31034;&#19982;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#27010;&#24565;&#65292;&#23454;&#29616;&#20102;&#23545;&#31070;&#32463;&#34920;&#31034;&#30340;&#26631;&#35760;&#24182;&#25552;&#20379;&#20102;&#32479;&#35745;&#26174;&#33879;&#24615;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#22312;&#23398;&#20064;&#22797;&#26434;&#30340;&#23618;&#32423;&#25968;&#25454;&#34920;&#31034;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#20294;&#36825;&#20123;&#34920;&#31034;&#30340;&#24615;&#36136;&#20173;&#28982;&#22823;&#37096;&#20998;&#26410;&#30693;&#12290;&#29616;&#26377;&#30340;&#20840;&#23616;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#22914;&#32593;&#32476;&#35299;&#21078;(Network Dissection)&#65292;&#23384;&#22312;&#35832;&#22810;&#38480;&#21046;&#65292;&#22914;&#20381;&#36182;&#20998;&#21106;&#36974;&#32617;&#12289;&#32570;&#20047;&#32479;&#35745;&#26174;&#33879;&#24615;&#26816;&#39564;&#21644;&#39640;&#35745;&#31639;&#38656;&#27714;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Inverse Recognition (INVERT)&#26041;&#27861;&#65292;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20854;&#21306;&#20998;&#36825;&#20123;&#27010;&#24565;&#30340;&#33021;&#21147;&#65292;&#23558;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#19982;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#27010;&#24565;&#30456;&#36830;&#25509;&#12290;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;INVERT&#33021;&#22815;&#22788;&#29702;&#19981;&#21516;&#31867;&#22411;&#30340;&#31070;&#32463;&#20803;&#65292;&#35745;&#31639;&#22797;&#26434;&#24230;&#26356;&#20302;&#65292;&#24182;&#19988;&#19981;&#20381;&#36182;&#20110;&#20998;&#21106;&#36974;&#32617;&#30340;&#21487;&#29992;&#24615;&#12290;&#27492;&#22806;&#65292;INVERT&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#24230;&#37327;&#65292;&#35780;&#20272;&#34920;&#31034;&#21644;&#20854;&#30456;&#24212;&#35299;&#37322;&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#24182;&#25552;&#20379;&#19968;&#31181;&#32479;&#35745;&#26174;&#33879;&#24615;&#30340;&#24230;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;INVERT&#30340;&#24212;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks (DNNs) demonstrate remarkable capabilities in learning complex hierarchical data representations, but the nature of these representations remains largely unknown. Existing global explainability methods, such as Network Dissection, face limitations such as reliance on segmentation masks, lack of statistical significance testing, and high computational demands. We propose Inverse Recognition (INVERT), a scalable approach for connecting learned representations with human-understandable concepts by leveraging their capacity to discriminate between these concepts. In contrast to prior work, INVERT is capable of handling diverse types of neurons, exhibits less computational complexity, and does not rely on the availability of segmentation masks. Moreover, INVERT provides an interpretable metric assessing the alignment between the representation and its corresponding explanation and delivering a measure of statistical significance. We demonstrate the applicability of INVE
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#31639;&#27861;&#34920;&#31034;&#38598;&#25104;&#21040;&#31639;&#27861;&#36873;&#25321;&#20013;&#65292;&#20174;&#32780;&#22635;&#34917;&#20102;&#24403;&#21069;&#31639;&#27861;&#36873;&#25321;&#25216;&#26415;&#23545;&#31639;&#27861;&#29305;&#24449;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2311.13184</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#30340;&#31639;&#27861;&#36873;&#25321;&#65306;&#26397;&#30528;&#20840;&#38754;&#31639;&#27861;&#34920;&#31034;&#30340;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Large Language Model-Enhanced Algorithm Selection: Towards Comprehensive Algorithm Representation. (arXiv:2311.13184v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.13184
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#31639;&#27861;&#34920;&#31034;&#38598;&#25104;&#21040;&#31639;&#27861;&#36873;&#25321;&#20013;&#65292;&#20174;&#32780;&#22635;&#34917;&#20102;&#24403;&#21069;&#31639;&#27861;&#36873;&#25321;&#25216;&#26415;&#23545;&#31639;&#27861;&#29305;&#24449;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31639;&#27861;&#36873;&#25321;&#26088;&#22312;&#22312;&#25191;&#34892;&#20043;&#21069;&#35782;&#21035;&#35299;&#20915;&#29305;&#23450;&#38382;&#39064;&#30340;&#26368;&#21512;&#36866;&#31639;&#27861;&#65292;&#24050;&#25104;&#20026;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20851;&#38190;&#36807;&#31243;&#12290;&#24403;&#21069;&#20027;&#27969;&#30340;&#31639;&#27861;&#36873;&#25321;&#25216;&#26415;&#20027;&#35201;&#20381;&#36182;&#20110;&#21508;&#31181;&#38382;&#39064;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#27599;&#20010;&#31639;&#27861;&#30340;&#24615;&#33021;&#20316;&#20026;&#30417;&#30563;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;&#31639;&#27861;&#29305;&#24449;&#30340;&#32771;&#34385;&#23384;&#22312;&#37325;&#35201;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;&#36825;&#20027;&#35201;&#24402;&#22240;&#20110;&#31639;&#27861;&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#65292;&#20351;&#24471;&#22312;&#19981;&#21516;&#31181;&#31867;&#30340;&#31639;&#27861;&#20013;&#25214;&#21040;&#19968;&#31181;&#26222;&#36866;&#26377;&#25928;&#30340;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#24573;&#35270;&#20102;&#36825;&#19968;&#26041;&#38754;&#26080;&#30097;&#20250;&#24433;&#21709;&#31639;&#27861;&#36873;&#25321;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#38388;&#25509;&#38656;&#35201;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#30340;&#25968;&#37327;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#19968;&#31354;&#30333;&#65292;&#21363;&#23558;&#31639;&#27861;&#34920;&#31034;&#38598;&#25104;&#21040;&#31639;&#27861;&#36873;&#25321;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithm selection aims to identify the most suitable algorithm for solving a specific problem before execution, which has become a critical process of the AutoML. Current mainstream algorithm selection techniques rely heavily on feature representations of various problems and employ the performance of each algorithm as supervised information. However, there is a significant research gap concerning the consideration of algorithm features. This gap is primarily attributed to the inherent complexity of algorithms, making it particularly challenging to find a universally effective feature extraction method that is applicable across a diverse range of algorithms. Unfortunately, neglecting this aspect undoubtedly impacts the accuracy of algorithm selection and indirectly necessitates an increased volume of problem data for training purposes. This paper takes a significant stride towards addressing this gap by proposing an approach that integrates algorithm representation into the algorithm
&lt;/p&gt;</description></item><item><title>FIKIT&#26159;&#19968;&#31181;&#22522;&#20110;&#20248;&#20808;&#32423;&#30340;&#23454;&#26102;GPU&#22810;&#20219;&#21153;&#35843;&#24230;&#31574;&#30053;&#65292;&#20855;&#26377;&#20869;&#26680;&#35782;&#21035;&#21151;&#33021;&#65292;&#33021;&#22815;&#22312;&#39640;&#20248;&#20808;&#32423;&#20219;&#21153;&#30340;&#20869;&#26680;&#38388;&#22635;&#20805;&#31354;&#38386;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2311.10359</link><description>&lt;p&gt;
FIKIT&#65306;&#22522;&#20110;&#20248;&#20808;&#32423;&#30340;&#23454;&#26102;GPU&#22810;&#20219;&#21153;&#35843;&#24230;&#19982;&#20869;&#26680;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
FIKIT: Priority-Based Real-time GPU Multi-tasking Scheduling with Kernel Identification. (arXiv:2311.10359v3 [cs.DC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.10359
&lt;/p&gt;
&lt;p&gt;
FIKIT&#26159;&#19968;&#31181;&#22522;&#20110;&#20248;&#20808;&#32423;&#30340;&#23454;&#26102;GPU&#22810;&#20219;&#21153;&#35843;&#24230;&#31574;&#30053;&#65292;&#20855;&#26377;&#20869;&#26680;&#35782;&#21035;&#21151;&#33021;&#65292;&#33021;&#22815;&#22312;&#39640;&#20248;&#20808;&#32423;&#20219;&#21153;&#30340;&#20869;&#26680;&#38388;&#22635;&#20805;&#31354;&#38386;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#24182;&#34892;&#24037;&#20316;&#36127;&#36733;&#65292;&#22914;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#12289;&#25512;&#26029;&#21644;&#19968;&#33324;HPC&#20219;&#21153;&#65292;&#36890;&#36807;&#20351;&#29992;GPU&#35774;&#22791;&#24471;&#21040;&#20102;&#26497;&#22823;&#30340;&#21152;&#36895;&#12290;&#22312;&#20113;&#35745;&#31639;&#38598;&#32676;&#20013;&#65292;&#36890;&#36807;&#22810;&#20219;&#21153;&#20849;&#20139;&#26469;&#25552;&#20379;GPU&#30340;&#35745;&#31639;&#33021;&#21147;&#26159;&#38750;&#24120;&#38656;&#35201;&#30340;&#65292;&#22240;&#20026;&#24635;&#26159;&#26377;&#26356;&#22810;&#30340;&#20219;&#21153;&#35831;&#27714;&#32780;&#19981;&#26159;&#21487;&#29992;&#30340;GPU&#25968;&#37327;&#12290;&#29616;&#26377;&#30340;GPU&#20849;&#20139;&#35299;&#20915;&#26041;&#26696;&#30528;&#37325;&#20110;&#20943;&#23569;&#22810;&#20010;&#20316;&#19994;&#20105;&#22842;&#21333;&#20010;GPU&#26102;&#30340;&#20219;&#21153;&#32423;&#31561;&#24453;&#26102;&#38388;&#25110;&#20219;&#21153;&#32423;&#20999;&#25442;&#25104;&#26412;&#12290;&#36830;&#32493;&#35745;&#31639;&#35831;&#27714;&#20855;&#26377;&#19981;&#21516;&#30340;&#20248;&#20808;&#32423;&#65292;&#23545;&#20110;&#20849;&#20139;GPU&#35774;&#22791;&#65292;&#23545;QoS&#20135;&#29983;&#20102;&#38750;&#23545;&#31216;&#30340;&#24433;&#21709;&#12290;&#29616;&#26377;&#24037;&#20316;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#36825;&#31181;&#24773;&#20917;&#24102;&#26469;&#30340;&#20869;&#26680;&#32423;&#20248;&#21270;&#26426;&#20250;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20869;&#26680;&#32423;&#35843;&#24230;&#31574;&#30053;FIKIT&#65306;&#22635;&#20805;&#20869;&#26680;&#38388;&#31354;&#38386;&#26102;&#38388;&#12290;FIKIT&#21253;&#21547;&#20219;&#21153;&#32423;&#20248;&#20808;&#32423;&#20449;&#24687;&#12289;&#32454;&#31890;&#24230;&#20869;&#26680;&#35782;&#21035;&#21644;&#20869;&#26680;&#27979;&#37327;&#65292;&#20801;&#35768;&#20302;&#20248;&#20808;&#32423;&#20219;&#21153;&#22312;&#39640;&#20248;&#20808;&#32423;&#20219;&#21153;&#30340;&#20869;&#26680;&#38388;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Highly parallelized workloads like machine learning training, inferences and general HPC tasks are greatly accelerated using GPU devices. In a cloud computing cluster, serving a GPU's computation power through multi-tasks sharing is highly demanded since there are always more task requests than the number of GPU available. Existing GPU sharing solutions focus on reducing task-level waiting time or task-level switching costs when multiple jobs competing for a single GPU. Non-stopped computation requests come with different priorities, having non-symmetric impact on QoS for sharing a GPU device. Existing work missed the kernel-level optimization opportunity brought by this setting. To address this problem, we present a novel kernel-level scheduling strategy called FIKIT: Filling Inter-kernel Idle Time. FIKIT incorporates task-level priority information, fine-grained kernel identification, and kernel measurement, allowing low priorities task's execution during high priority task's inter-k
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36755;&#20837;&#20984;LSTM&#30340;&#22522;&#20110;Lyapunov&#30340;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#26041;&#27861;&#65292;&#36890;&#36807;&#20943;&#23569;&#25910;&#25947;&#26102;&#38388;&#21644;&#32531;&#35299;&#26799;&#24230;&#28040;&#22833;/&#29190;&#28856;&#38382;&#39064;&#26469;&#25913;&#21892;MPC&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.07202</link><description>&lt;p&gt;
&#36755;&#20837;&#20984;LSTM&#65306;&#19968;&#31181;&#24555;&#36895;&#22522;&#20110;Lyapunov&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#30340;&#20984;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Input Convex LSTM: A Convex Approach for Fast Lyapunov-Based Model Predictive Control. (arXiv:2311.07202v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.07202
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36755;&#20837;&#20984;LSTM&#30340;&#22522;&#20110;Lyapunov&#30340;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#26041;&#27861;&#65292;&#36890;&#36807;&#20943;&#23569;&#25910;&#25947;&#26102;&#38388;&#21644;&#32531;&#35299;&#26799;&#24230;&#28040;&#22833;/&#29190;&#28856;&#38382;&#39064;&#26469;&#25913;&#21892;MPC&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#36755;&#20837;&#20984;&#31070;&#32463;&#32593;&#32476;&#65288;ICNN&#65289;&#65292;&#22522;&#20110;ICNN&#30340;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;MPC&#65289;&#36890;&#36807;&#22312;MPC&#26694;&#26550;&#20013;&#20445;&#25345;&#20984;&#24615;&#25104;&#21151;&#23454;&#29616;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;ICNN&#26550;&#26500;&#23384;&#22312;&#26799;&#24230;&#28040;&#22833;/&#29190;&#28856;&#38382;&#39064;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#20316;&#20026;&#22797;&#26434;&#20219;&#21153;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;MPC&#65292;&#21253;&#25324;&#20256;&#32479;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;MPC&#21644;&#22522;&#20110;ICNN&#30340;MPC&#65292;&#19982;&#22522;&#20110;&#31532;&#19968;&#21407;&#29702;&#27169;&#22411;&#30340;MPC&#30456;&#27604;&#38754;&#20020;&#36739;&#24930;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;ICNN&#30340;&#21407;&#29702;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#36755;&#20837;&#20984;LSTM&#30340;&#22522;&#20110;Lyapunov&#30340;MPC&#65292;&#26088;&#22312;&#20943;&#23569;&#25910;&#25947;&#26102;&#38388;&#12289;&#32531;&#35299;&#26799;&#24230;&#28040;&#22833;/&#29190;&#28856;&#38382;&#39064;&#24182;&#30830;&#20445;&#38381;&#29615;&#31283;&#23450;&#24615;&#12290;&#36890;&#36807;&#23545;&#38750;&#32447;&#24615;&#21270;&#23398;&#21453;&#24212;&#22120;&#30340;&#27169;&#25311;&#30740;&#31350;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20102;&#26799;&#24230;&#28040;&#22833;/&#29190;&#28856;&#38382;&#39064;&#30340;&#32531;&#35299;&#21644;&#25910;&#25947;&#26102;&#38388;&#30340;&#20943;&#23569;&#65292;&#25910;&#25947;&#26102;&#38388;&#24179;&#22343;&#38477;&#20302;&#20102;&#19968;&#23450;&#30340;&#30334;&#20998;&#20043;&#12290;
&lt;/p&gt;
&lt;p&gt;
Leveraging Input Convex Neural Networks (ICNNs), ICNN-based Model Predictive Control (MPC) successfully attains globally optimal solutions by upholding convexity within the MPC framework. However, current ICNN architectures encounter the issue of vanishing/exploding gradients, which limits their ability to serve as deep neural networks for complex tasks. Additionally, the current neural network-based MPC, including conventional neural network-based MPC and ICNN-based MPC, faces slower convergence speed when compared to MPC based on first-principles models. In this study, we leverage the principles of ICNNs to propose a novel Input Convex LSTM for Lyapunov-based MPC, with the specific goal of reducing convergence time and mitigating the vanishing/exploding gradient problem while ensuring closed-loop stability. From a simulation study of a nonlinear chemical reactor, we observed a mitigation of vanishing/exploding gradient problem and a reduction in convergence time, with a percentage de
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26497;&#31471;&#22810;&#26631;&#31614;&#20998;&#31867;&#20013;&#38271;&#23614;&#24615;&#33021;&#38382;&#39064;&#30340;&#36890;&#29992;&#27979;&#35797;&#24037;&#20855;&#65292;&#36890;&#36807;&#20998;&#26512;&#24191;&#20041;&#24230;&#37327;&#39044;&#31639;&#21644;&#39044;&#26399;&#27979;&#35797;&#25928;&#29992;&#26694;&#26550;&#65292;&#25512;&#23548;&#20986;&#26368;&#20248;&#39044;&#27979;&#35268;&#21017;&#12290;</title><link>http://arxiv.org/abs/2311.05081</link><description>&lt;p&gt;
&#26497;&#31471;&#22810;&#26631;&#31614;&#20998;&#31867;&#20013;&#38271;&#23614;&#24615;&#33021;&#30340;&#36890;&#29992;&#27979;&#35797;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Generalized test utilities for long-tail performance in extreme multi-label classification. (arXiv:2311.05081v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.05081
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26497;&#31471;&#22810;&#26631;&#31614;&#20998;&#31867;&#20013;&#38271;&#23614;&#24615;&#33021;&#38382;&#39064;&#30340;&#36890;&#29992;&#27979;&#35797;&#24037;&#20855;&#65292;&#36890;&#36807;&#20998;&#26512;&#24191;&#20041;&#24230;&#37327;&#39044;&#31639;&#21644;&#39044;&#26399;&#27979;&#35797;&#25928;&#29992;&#26694;&#26550;&#65292;&#25512;&#23548;&#20986;&#26368;&#20248;&#39044;&#27979;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26497;&#31471;&#22810;&#26631;&#31614;&#20998;&#31867;&#65288;XMLC&#65289;&#26159;&#20174;&#19968;&#20010;&#38750;&#24120;&#24222;&#22823;&#30340;&#21487;&#33021;&#26631;&#31614;&#38598;&#20013;&#36873;&#25321;&#19968;&#20010;&#23567;&#23376;&#38598;&#30340;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#23427;&#20197;&#38271;&#23614;&#26631;&#31614;&#20026;&#29305;&#24449;&#65292;&#21363;&#22823;&#22810;&#25968;&#26631;&#31614;&#21482;&#26377;&#24456;&#23569;&#30340;&#27491;&#23454;&#20363;&#12290;&#20351;&#29992;&#26631;&#20934;&#30340;&#24615;&#33021;&#24230;&#37327;&#22914;precision@k&#65292;&#20998;&#31867;&#22120;&#21487;&#20197;&#24573;&#30053;&#38271;&#23614;&#26631;&#31614;&#24182;&#20173;&#28982;&#25253;&#21578;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36890;&#24120;&#35748;&#20026;&#22312;&#38271;&#23614;&#20013;&#30340;&#27491;&#30830;&#39044;&#27979;&#26356;&#21152;&#8220;&#26377;&#36259;&#8221;&#25110;&#8220;&#26377;&#22870;&#21169;&#8221;&#65292;&#20294;&#23398;&#26415;&#30028;&#23578;&#26410;&#23601;&#25429;&#25417;&#36825;&#20010;&#30452;&#35266;&#27010;&#24565;&#30340;&#24230;&#37327;&#26041;&#24335;&#36798;&#25104;&#20849;&#35782;&#12290;&#29616;&#26377;&#30340;&#20542;&#21521;&#24471;&#20998;&#24230;&#37327;&#22312;&#35299;&#20915;&#38271;&#23614;&#21644;&#20002;&#22833;&#26631;&#31614;&#38382;&#39064;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20316;&#20026;&#26367;&#20195;&#35299;&#20915;&#26041;&#26696;&#30340;&#24191;&#20041;&#24230;&#37327;&#39044;&#31639;&#8220;&#22312;k&#22788;&#8221;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#24230;&#37327;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#20854;&#21046;&#23450;&#22312;&#39044;&#26399;&#27979;&#35797;&#25928;&#29992;&#65288;ETU&#65289;&#26694;&#26550;&#20013;&#65292;&#35813;&#26694;&#26550;&#26088;&#22312;&#20248;&#21270;&#22312;&#22266;&#23450;&#27979;&#35797;&#38598;&#19978;&#30340;&#39044;&#26399;&#24615;&#33021;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#26368;&#20248;&#39044;&#27979;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extreme multi-label classification (XMLC) is the task of selecting a small subset of relevant labels from a very large set of possible labels. As such, it is characterized by long-tail labels, i.e., most labels have very few positive instances. With standard performance measures such as precision@k, a classifier can ignore tail labels and still report good performance. However, it is often argued that correct predictions in the tail are more "interesting" or "rewarding," but the community has not yet settled on a metric capturing this intuitive concept. The existing propensity-scored metrics fall short on this goal by confounding the problems of long-tail and missing labels. In this paper, we analyze generalized metrics budgeted "at k" as an alternative solution. To tackle the challenging problem of optimizing these metrics, we formulate it in the expected test utility (ETU) framework, which aims to optimize the expected performance on a fixed test set. We derive optimal prediction rul
&lt;/p&gt;</description></item><item><title>&#22312;DDIM&#26694;&#26550;&#20013;&#20351;&#29992;GMM&#20316;&#20026;&#21453;&#21521;&#36716;&#31227;&#31639;&#23376;&#65292;&#36890;&#36807;&#30697;&#21305;&#37197;&#21487;&#20197;&#33719;&#24471;&#36136;&#37327;&#26356;&#39640;&#30340;&#26679;&#26412;&#12290;&#22312;&#26080;&#26465;&#20214;&#27169;&#22411;&#21644;&#31867;&#26465;&#20214;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#36890;&#36807;FID&#21644;IS&#25351;&#26631;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2311.04938</link><description>&lt;p&gt;
&#20351;&#29992;&#30697;&#21305;&#37197;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#25913;&#36827;&#20102;DDIM&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Improved DDIM Sampling with Moment Matching Gaussian Mixtures. (arXiv:2311.04938v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.04938
&lt;/p&gt;
&lt;p&gt;
&#22312;DDIM&#26694;&#26550;&#20013;&#20351;&#29992;GMM&#20316;&#20026;&#21453;&#21521;&#36716;&#31227;&#31639;&#23376;&#65292;&#36890;&#36807;&#30697;&#21305;&#37197;&#21487;&#20197;&#33719;&#24471;&#36136;&#37327;&#26356;&#39640;&#30340;&#26679;&#26412;&#12290;&#22312;&#26080;&#26465;&#20214;&#27169;&#22411;&#21644;&#31867;&#26465;&#20214;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#36890;&#36807;FID&#21644;IS&#25351;&#26631;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#22312;Denoising Diffusion Implicit Models (DDIM)&#26694;&#26550;&#20013;&#20351;&#29992;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65288;GMM&#65289;&#20316;&#20026;&#21453;&#21521;&#36716;&#31227;&#31639;&#23376;&#65288;&#20869;&#26680;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#20174;&#39044;&#35757;&#32451;&#30340;Denoising Diffusion Probabilistic Models (DDPM)&#20013;&#21152;&#36895;&#37319;&#26679;&#30340;&#24191;&#27867;&#24212;&#29992;&#26041;&#27861;&#20043;&#19968;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#32422;&#26463;GMM&#30340;&#21442;&#25968;&#65292;&#21305;&#37197;DDPM&#21069;&#21521;&#36793;&#38469;&#30340;&#19968;&#38454;&#21644;&#20108;&#38454;&#20013;&#24515;&#30697;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#36807;&#30697;&#21305;&#37197;&#65292;&#21487;&#20197;&#33719;&#24471;&#19982;&#20351;&#29992;&#39640;&#26031;&#26680;&#30340;&#21407;&#22987;DDIM&#30456;&#21516;&#25110;&#26356;&#22909;&#36136;&#37327;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#22312;CelebAHQ&#21644;FFHQ&#30340;&#26080;&#26465;&#20214;&#27169;&#22411;&#20197;&#21450;ImageNet&#25968;&#25454;&#38598;&#30340;&#31867;&#26465;&#20214;&#27169;&#22411;&#19978;&#25552;&#20379;&#20102;&#23454;&#39564;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#37319;&#26679;&#27493;&#39588;&#36739;&#23569;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;GMM&#20869;&#26680;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#29983;&#25104;&#26679;&#26412;&#30340;&#36136;&#37327;&#65292;&#36825;&#26159;&#36890;&#36807;FID&#21644;IS&#25351;&#26631;&#34913;&#37327;&#30340;&#12290;&#20363;&#22914;&#65292;&#22312;ImageNet 256x256&#19978;&#65292;&#20351;&#29992;10&#20010;&#37319;&#26679;&#27493;&#39588;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#20010;FID&#20540;&#20026;...
&lt;/p&gt;
&lt;p&gt;
We propose using a Gaussian Mixture Model (GMM) as reverse transition operator (kernel) within the Denoising Diffusion Implicit Models (DDIM) framework, which is one of the most widely used approaches for accelerated sampling from pre-trained Denoising Diffusion Probabilistic Models (DDPM). Specifically we match the first and second order central moments of the DDPM forward marginals by constraining the parameters of the GMM. We see that moment matching is sufficient to obtain samples with equal or better quality than the original DDIM with Gaussian kernels. We provide experimental results with unconditional models trained on CelebAHQ and FFHQ and class-conditional models trained on ImageNet datasets respectively. Our results suggest that using the GMM kernel leads to significant improvements in the quality of the generated samples when the number of sampling steps is small, as measured by FID and IS metrics. For example on ImageNet 256x256, using 10 sampling steps, we achieve a FID of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;Lipschitz&#24120;&#25968;&#65292;&#23545;&#20110;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;Lipschitz&#24120;&#25968;&#30340;&#31934;&#30830;&#21051;&#30011;&#65292;&#23545;&#20110;&#36275;&#22815;&#23485;&#24230;&#30340;&#28145;&#23618;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#19978;&#19979;&#30028;&#65292;&#24182;&#21305;&#37197;&#19968;&#20010;&#20381;&#36182;&#20110;&#28145;&#24230;&#30340;&#23545;&#25968;&#22240;&#23376;&#12290;</title><link>http://arxiv.org/abs/2311.01356</link><description>&lt;p&gt;
&#20851;&#20110;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#30340;Lipschitz&#24120;&#25968;
&lt;/p&gt;
&lt;p&gt;
On the Lipschitz constant of random neural networks. (arXiv:2311.01356v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01356
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;Lipschitz&#24120;&#25968;&#65292;&#23545;&#20110;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;Lipschitz&#24120;&#25968;&#30340;&#31934;&#30830;&#21051;&#30011;&#65292;&#23545;&#20110;&#36275;&#22815;&#23485;&#24230;&#30340;&#28145;&#23618;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#19978;&#19979;&#30028;&#65292;&#24182;&#21305;&#37197;&#19968;&#20010;&#20381;&#36182;&#20110;&#28145;&#24230;&#30340;&#23545;&#25968;&#22240;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#35777;&#30740;&#31350;&#24191;&#27867;&#35777;&#26126;&#31070;&#32463;&#32593;&#32476;&#23545;&#36755;&#20837;&#30340;&#24494;&#23567;&#23545;&#25239;&#24615;&#25200;&#21160;&#38750;&#24120;&#25935;&#24863;&#12290;&#36825;&#20123;&#25152;&#35859;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#26368;&#22351;&#24773;&#20917;&#40065;&#26834;&#24615;&#21487;&#20197;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#30340;Lipschitz&#24120;&#25968;&#26469;&#37327;&#21270;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#36825;&#20010;&#37327;&#30340;&#29702;&#35770;&#32467;&#26524;&#22312;&#25991;&#29486;&#20013;&#20165;&#26377;&#23569;&#25968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#22987;&#30740;&#31350;&#38543;&#26426;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;Lipschitz&#24120;&#25968;&#65292;&#21363;&#36873;&#25321;&#38543;&#26426;&#26435;&#37325;&#24182;&#37319;&#29992;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#23545;&#20110;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#23558;Lipschitz&#24120;&#25968;&#21051;&#30011;&#21040;&#19968;&#20010;&#32477;&#23545;&#25968;&#20540;&#24120;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#20998;&#26512;&#25193;&#23637;&#21040;&#36275;&#22815;&#23485;&#24230;&#30340;&#28145;&#23618;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;Lipschitz&#24120;&#25968;&#30340;&#19978;&#19979;&#30028;&#12290;&#36825;&#20123;&#30028;&#21305;&#37197;&#21040;&#19968;&#20010;&#20381;&#36182;&#20110;&#28145;&#24230;&#30340;&#23545;&#25968;&#22240;&#23376;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Empirical studies have widely demonstrated that neural networks are highly sensitive to small, adversarial perturbations of the input. The worst-case robustness against these so-called adversarial examples can be quantified by the Lipschitz constant of the neural network. However, only few theoretical results regarding this quantity exist in the literature. In this paper, we initiate the study of the Lipschitz constant of random ReLU neural networks, i.e., neural networks whose weights are chosen at random and which employ the ReLU activation function. For shallow neural networks, we characterize the Lipschitz constant up to an absolute numerical constant. Moreover, we extend our analysis to deep neural networks of sufficiently large width where we prove upper and lower bounds for the Lipschitz constant. These bounds match up to a logarithmic factor that depends on the depth.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#37329;&#34701;&#31185;&#25216;&#24212;&#29992;&#20013;&#23547;&#25214;&#39640;&#36136;&#37327;&#30340;&#21452;&#30446;&#26631; Pareto &#26368;&#20248;&#27450;&#35784;&#39044;&#38450;&#35268;&#21017;&#38598;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#37319;&#29992; Pareto &#26368;&#20248;&#24615;&#27010;&#24565;&#21644;&#21551;&#21457;&#24335;&#26694;&#26550; PORS&#65292;&#25105;&#20204;&#25104;&#21151;&#25552;&#20986;&#20102;&#19968;&#32452;&#38750;&#25903;&#37197;&#30340;&#35268;&#21017;&#23376;&#38598;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#35780;&#20272;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00964</link><description>&lt;p&gt;
&#22312;&#37329;&#34701;&#31185;&#25216;&#24212;&#29992;&#20013;&#23547;&#25214;&#21452;&#30446;&#26631; Pareto &#26368;&#20248;&#27450;&#35784;&#39044;&#38450;&#35268;&#21017;&#38598;
&lt;/p&gt;
&lt;p&gt;
On Finding Bi-objective Pareto-optimal Fraud Prevention Rule Sets for Fintech Applications. (arXiv:2311.00964v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00964
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#37329;&#34701;&#31185;&#25216;&#24212;&#29992;&#20013;&#23547;&#25214;&#39640;&#36136;&#37327;&#30340;&#21452;&#30446;&#26631; Pareto &#26368;&#20248;&#27450;&#35784;&#39044;&#38450;&#35268;&#21017;&#38598;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#37319;&#29992; Pareto &#26368;&#20248;&#24615;&#27010;&#24565;&#21644;&#21551;&#21457;&#24335;&#26694;&#26550; PORS&#65292;&#25105;&#20204;&#25104;&#21151;&#25552;&#20986;&#20102;&#19968;&#32452;&#38750;&#25903;&#37197;&#30340;&#35268;&#21017;&#23376;&#38598;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#35780;&#20272;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35268;&#21017;&#22312;&#37329;&#34701;&#31185;&#25216;&#26426;&#26500;&#20013;&#34987;&#24191;&#27867;&#29992;&#20110;&#36827;&#34892;&#27450;&#35784;&#39044;&#38450;&#20915;&#31574;&#65292;&#22240;&#20026;&#35268;&#21017;&#20855;&#26377;&#30452;&#35266;&#30340; if-then &#32467;&#26500;&#65292;&#26131;&#20110;&#29702;&#35299;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#22823;&#22411;&#37329;&#34701;&#31185;&#25216;&#26426;&#26500;&#36890;&#24120;&#37319;&#29992;&#20004;&#38454;&#27573;&#27450;&#35784;&#39044;&#38450;&#20915;&#31574;&#35268;&#21017;&#38598;&#25366;&#25496;&#26694;&#26550;&#12290;&#26412;&#25991;&#20851;&#27880;&#20110;&#20174;&#21021;&#22987;&#35268;&#21017;&#38598;&#20013;&#25214;&#21040;&#39640;&#36136;&#37327;&#30340;&#35268;&#21017;&#23376;&#38598;&#65292;&#20197;&#21452;&#30446;&#26631;&#31354;&#38388;&#65288;&#22914;&#31934;&#30830;&#29575;&#21644;&#21484;&#22238;&#29575;&#65289;&#20026;&#22522;&#30784;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#37319;&#29992; Pareto &#26368;&#20248;&#24615;&#27010;&#24565;&#65292;&#26088;&#22312;&#25214;&#21040;&#19968;&#32452;&#38750;&#25903;&#37197;&#30340;&#35268;&#21017;&#23376;&#38598;&#65292;&#26500;&#25104;&#19968;&#20010; Pareto &#21069;&#27839;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21551;&#21457;&#24335;&#30340;&#26694;&#26550; PORS&#65292;&#24182;&#30830;&#23450;&#20102; PORS &#30340;&#26680;&#24515;&#38382;&#39064;&#26159;&#21069;&#27839;&#35299;&#20915;&#26041;&#26696;&#36873;&#25321;&#65288;SSF&#65289;&#38382;&#39064;&#12290;&#25105;&#20204;&#23545; SSF &#38382;&#39064;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#31867;&#65292;&#24182;&#22312;&#20844;&#24320;&#21644;&#19987;&#26377;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#35777;&#35780;&#20272;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026; SpectralRules &#30340;&#26032;&#39062;&#21464;&#20307;&#30340;&#39034;&#24207;&#35206;&#30422;&#31639;&#27861;&#65292;&#20197;&#40723;&#21169;&#35268;&#21017;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rules are widely used in Fintech institutions to make fraud prevention decisions, since rules are highly interpretable thanks to their intuitive if-then structure. In practice, a two-stage framework of fraud prevention decision rule set mining is usually employed in large Fintech institutions. This paper is concerned with finding high-quality rule subsets in a bi-objective space (such as precision and recall) from an initial pool of rules. To this end, we adopt the concept of Pareto optimality and aim to find a set of non-dominated rule subsets, which constitutes a Pareto front. We propose a heuristic-based framework called PORS and we identify that the core of PORS is the problem of solution selection on the front (SSF). We provide a systematic categorization of the SSF problem and a thorough empirical evaluation of various SSF methods on both public and proprietary datasets. We also introduce a novel variant of sequential covering algorithm called SpectralRules to encourage the diver
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;LogEI&#20316;&#20026;&#19968;&#31867;&#26032;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#33719;&#24471;&#20989;&#25968;&#65292;&#20855;&#26377;&#19982;&#20256;&#32479;&#30340;EI&#20989;&#25968;&#30456;&#21516;&#25110;&#36817;&#20284;&#30456;&#31561;&#30340;&#26368;&#20248;&#35299;&#65292;&#20294;&#25968;&#20540;&#19978;&#26356;&#23481;&#26131;&#36827;&#34892;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.20708</link><description>&lt;p&gt;
&#23545;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#26399;&#26395;&#25913;&#36827;&#30340;&#24847;&#22806;&#25552;&#21319;
&lt;/p&gt;
&lt;p&gt;
Unexpected Improvements to Expected Improvement for Bayesian Optimization. (arXiv:2310.20708v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20708
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;LogEI&#20316;&#20026;&#19968;&#31867;&#26032;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#33719;&#24471;&#20989;&#25968;&#65292;&#20855;&#26377;&#19982;&#20256;&#32479;&#30340;EI&#20989;&#25968;&#30456;&#21516;&#25110;&#36817;&#20284;&#30456;&#31561;&#30340;&#26368;&#20248;&#35299;&#65292;&#20294;&#25968;&#20540;&#19978;&#26356;&#23481;&#26131;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26399;&#26395;&#25913;&#36827;&#65288;EI&#65289;&#21487;&#20197;&#35828;&#26159;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#26368;&#27969;&#34892;&#30340;&#33719;&#24471;&#20989;&#25968;&#65292;&#24182;&#19988;&#24050;&#32463;&#22312;&#24456;&#22810;&#25104;&#21151;&#30340;&#24212;&#29992;&#20013;&#24471;&#21040;&#20102;&#24212;&#29992;&#12290;&#20294;&#26159;&#65292;EI&#30340;&#24615;&#33021;&#24448;&#24448;&#34987;&#19968;&#20123;&#26032;&#26041;&#27861;&#36229;&#36234;&#12290;&#23588;&#20854;&#26159;&#65292;EI&#21450;&#20854;&#21464;&#31181;&#22312;&#24182;&#34892;&#21644;&#22810;&#30446;&#26631;&#35774;&#32622;&#20013;&#24456;&#38590;&#36827;&#34892;&#20248;&#21270;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#33719;&#24471;&#20540;&#22312;&#35768;&#22810;&#21306;&#22495;&#20013;&#25968;&#20540;&#19978;&#21464;&#20026;&#38646;&#12290;&#24403;&#35266;&#27979;&#27425;&#25968;&#22686;&#21152;&#12289;&#25628;&#32034;&#31354;&#38388;&#30340;&#32500;&#24230;&#22686;&#21152;&#25110;&#32422;&#26463;&#26465;&#20214;&#30340;&#25968;&#37327;&#22686;&#21152;&#26102;&#65292;&#36825;&#31181;&#22256;&#38590;&#36890;&#24120;&#20250;&#22686;&#21152;&#65292;&#23548;&#33268;&#24615;&#33021;&#22312;&#25991;&#29486;&#20013;&#19981;&#19968;&#33268;&#19988;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#20122;&#20248;&#21270;&#12290;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LogEI&#65292;&#36825;&#26159;&#19968;&#31867;&#26032;&#30340;&#37319;&#26679;&#20989;&#25968;&#12290;&#19982;&#26631;&#20934;EI&#30456;&#27604;&#65292;&#36825;&#20123;LogEI&#20989;&#25968;&#30340;&#25104;&#21592;&#35201;&#20040;&#20855;&#26377;&#30456;&#21516;&#30340;&#26368;&#20248;&#35299;&#65292;&#35201;&#20040;&#20855;&#26377;&#36817;&#20284;&#30456;&#31561;&#30340;&#26368;&#20248;&#35299;&#65292;&#20294;&#25968;&#20540;&#19978;&#26356;&#23481;&#26131;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25968;&#20540;&#30149;&#24577;&#22312;&#8220;&#32463;&#20856;&#8221;&#20998;&#26512;EI&#12289;&#26399;&#26395;&#36229;&#20307;&#31215;&#25913;&#36827;&#65288;EHVI&#65289;&#20197;&#21450;&#23427;&#20204;&#30340;...
&lt;/p&gt;
&lt;p&gt;
Expected Improvement (EI) is arguably the most popular acquisition function in Bayesian optimization and has found countless successful applications, but its performance is often exceeded by that of more recent methods. Notably, EI and its variants, including for the parallel and multi-objective settings, are challenging to optimize because their acquisition values vanish numerically in many regions. This difficulty generally increases as the number of observations, dimensionality of the search space, or the number of constraints grow, resulting in performance that is inconsistent across the literature and most often sub-optimal. Herein, we propose LogEI, a new family of acquisition functions whose members either have identical or approximately equal optima as their canonical counterparts, but are substantially easier to optimize numerically. We demonstrate that numerical pathologies manifest themselves in "classic" analytic EI, Expected Hypervolume Improvement (EHVI), as well as their
&lt;/p&gt;</description></item><item><title>BasisFormer&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#23398;&#20064;&#21644;&#21487;&#35299;&#37322;&#30340;&#22522;&#30784;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#33719;&#24471;&#22522;&#30784;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#27169;&#22359;&#35745;&#31639;&#26102;&#38388;&#24207;&#21015;&#19982;&#22522;&#30784;&#20043;&#38388;&#30340;&#30456;&#20284;&#31995;&#25968;&#12290;</title><link>http://arxiv.org/abs/2310.20496</link><description>&lt;p&gt;
BasisFormer:&#22522;&#20110;&#21487;&#23398;&#20064;&#21644;&#21487;&#35299;&#37322;&#30340;&#22522;&#30784;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
BasisFormer: Attention-based Time Series Forecasting with Learnable and Interpretable Basis. (arXiv:2310.20496v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20496
&lt;/p&gt;
&lt;p&gt;
BasisFormer&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#23398;&#20064;&#21644;&#21487;&#35299;&#37322;&#30340;&#22522;&#30784;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#33719;&#24471;&#22522;&#30784;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#27169;&#22359;&#35745;&#31639;&#26102;&#38388;&#24207;&#21015;&#19982;&#22522;&#30784;&#20043;&#38388;&#30340;&#30456;&#20284;&#31995;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#24050;&#32463;&#25104;&#20026;&#29616;&#20195;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#20316;&#20026;&#29305;&#24449;&#25552;&#21462;&#32773;&#25110;&#26410;&#26469;&#21442;&#32771;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#26377;&#25928;&#65292;&#22522;&#30784;&#24517;&#39035;&#26681;&#25454;&#29305;&#23450;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#36827;&#34892;&#23450;&#21046;&#65292;&#24182;&#19988;&#22312;&#25968;&#25454;&#38598;&#20013;&#19982;&#27599;&#20010;&#26102;&#38388;&#24207;&#21015;&#23637;&#29616;&#20986;&#26126;&#26174;&#30340;&#30456;&#20851;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#22312;&#21516;&#26102;&#28385;&#36275;&#36825;&#20004;&#20010;&#35201;&#27714;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BasisFormer&#65292;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26550;&#26500;&#65292;&#23427;&#21033;&#29992;&#20102;&#21487;&#23398;&#20064;&#21644;&#21487;&#35299;&#37322;&#30340;&#22522;&#30784;&#12290;&#35813;&#26550;&#26500;&#30001;&#19977;&#20010;&#32452;&#20214;&#32452;&#25104;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#33258;&#36866;&#24212;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#33719;&#24471;&#22522;&#30784;&#65292;&#35813;&#23398;&#20064;&#23558;&#26102;&#38388;&#24207;&#21015;&#30340;&#21382;&#21490;&#21644;&#26410;&#26469;&#37096;&#20998;&#35270;&#20026;&#20004;&#20010;&#19981;&#21516;&#30340;&#35270;&#22270;&#65292;&#24182;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;Coef&#27169;&#22359;&#65292;&#36890;&#36807;&#21452;&#21521;&#20132;&#21449;&#27880;&#24847;&#21147;&#35745;&#31639;&#21382;&#21490;&#35270;&#22270;&#20013;&#26102;&#38388;&#24207;&#21015;&#19982;&#22522;&#30784;&#20043;&#38388;&#30340;&#30456;&#20284;&#31995;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bases have become an integral part of modern deep learning-based models for time series forecasting due to their ability to act as feature extractors or future references. To be effective, a basis must be tailored to the specific set of time series data and exhibit distinct correlation with each time series within the set. However, current state-of-the-art methods are limited in their ability to satisfy both of these requirements simultaneously. To address this challenge, we propose BasisFormer, an end-to-end time series forecasting architecture that leverages learnable and interpretable bases. This architecture comprises three components: First, we acquire bases through adaptive self-supervised learning, which treats the historical and future sections of the time series as two distinct views and employs contrastive learning. Next, we design a Coef module that calculates the similarity coefficients between the time series and bases in the historical view via bidirectional cross-attenti
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#12289;&#39640;&#25928;&#21644;&#33258;&#25105;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#27979;&#35797;&#26102;&#21457;&#29616;&#20197;&#21069;&#26410;&#30693;&#30340;&#31867;&#21035;&#65292;&#36890;&#36807;&#23558;&#26368;&#23567;&#38271;&#24230;&#31867;&#21035;&#20195;&#30721;&#20998;&#37197;&#32473;&#21333;&#20010;&#25968;&#25454;&#23454;&#20363;&#26469;&#22686;&#24378;&#23545;&#31867;&#21035;&#32454;&#31890;&#24230;&#30340;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.19776</link><description>&lt;p&gt;
&#23398;&#20064;&#20998;&#31867;&#36824;&#26159;&#20998;&#31867;&#23398;&#20064;&#65311;&#33258;&#32534;&#30721;&#23454;&#29616;&#26222;&#36866;&#31867;&#21035;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Learn to Categorize or Categorize to Learn? Self-Coding for Generalized Category Discovery. (arXiv:2310.19776v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19776
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#12289;&#39640;&#25928;&#21644;&#33258;&#25105;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#27979;&#35797;&#26102;&#21457;&#29616;&#20197;&#21069;&#26410;&#30693;&#30340;&#31867;&#21035;&#65292;&#36890;&#36807;&#23558;&#26368;&#23567;&#38271;&#24230;&#31867;&#21035;&#20195;&#30721;&#20998;&#37197;&#32473;&#21333;&#20010;&#25968;&#25454;&#23454;&#20363;&#26469;&#22686;&#24378;&#23545;&#31867;&#21035;&#32454;&#31890;&#24230;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25581;&#31034;&#27979;&#35797;&#26102;&#30340;&#26032;&#31867;&#21035;&#30340;&#25506;&#32034;&#20013;&#65292;&#25105;&#20204;&#38754;&#20020;&#30528;&#20256;&#32479;&#26377;&#30417;&#30563;&#35782;&#21035;&#27169;&#22411;&#30340;&#22266;&#26377;&#38480;&#21046;&#65292;&#36825;&#20123;&#27169;&#22411;&#21463;&#21040;&#39044;&#23450;&#20041;&#31867;&#21035;&#38598;&#30340;&#38480;&#21046;&#12290;&#34429;&#28982;&#22312;&#33258;&#25105;&#30417;&#30563;&#21644;&#24320;&#25918;&#24335;&#23398;&#20064;&#39046;&#22495;&#21462;&#24471;&#20102;&#19968;&#23450;&#36827;&#23637;&#65292;&#20197;&#23454;&#29616;&#27979;&#35797;&#26102;&#30340;&#31867;&#21035;&#21457;&#29616;&#65292;&#20294;&#19968;&#20010;&#20851;&#38190;&#20294;&#24120;&#24120;&#34987;&#24573;&#35270;&#30340;&#38382;&#39064;&#20173;&#28982;&#23384;&#22312;&#65306;&#20160;&#20040;&#30830;&#20999;&#22320;&#30028;&#23450;&#20102;&#19968;&#20010;&#31867;&#21035;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20248;&#21270;&#30340;&#35270;&#35282;&#27010;&#24565;&#21270;&#31867;&#21035;&#65292;&#23558;&#20854;&#35270;&#20026;&#19968;&#20010;&#26126;&#30830;&#23450;&#20041;&#38382;&#39064;&#30340;&#26368;&#20248;&#35299;&#12290;&#21033;&#29992;&#36825;&#31181;&#29420;&#29305;&#30340;&#27010;&#24565;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#12289;&#39640;&#25928;&#21644;&#33258;&#25105;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#27979;&#35797;&#26102;&#21457;&#29616;&#20197;&#21069;&#26410;&#30693;&#30340;&#31867;&#21035;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#19968;&#20010;&#26174;&#33879;&#29305;&#28857;&#26159;&#23558;&#26368;&#23567;&#38271;&#24230;&#31867;&#21035;&#20195;&#30721;&#20998;&#37197;&#32473;&#21333;&#20010;&#25968;&#25454;&#23454;&#20363;&#65292;&#36825;&#26679;&#21487;&#20197;&#27010;&#25324;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#38544;&#21547;&#31867;&#21035;&#23618;&#27425;&#32467;&#26500;&#12290;&#36825;&#31181;&#26426;&#21046;&#20351;&#25105;&#20204;&#33021;&#22815;&#26356;&#22909;&#22320;&#25511;&#21046;&#31867;&#21035;&#30340;&#32454;&#31890;&#24230;&#65292;&#20174;&#32780;&#20026;&#25105;&#20204;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#22686;&#24378;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the quest for unveiling novel categories at test time, we confront the inherent limitations of traditional supervised recognition models that are restricted by a predefined category set. While strides have been made in the realms of self-supervised and open-world learning towards test-time category discovery, a crucial yet often overlooked question persists: what exactly delineates a category? In this paper, we conceptualize a category through the lens of optimization, viewing it as an optimal solution to a well-defined problem. Harnessing this unique conceptualization, we propose a novel, efficient and self-supervised method capable of discovering previously unknown categories at test time. A salient feature of our approach is the assignment of minimum length category codes to individual data instances, which encapsulates the implicit category hierarchy prevalent in real-world datasets. This mechanism affords us enhanced control over category granularity, thereby equipping our mode
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#25512;&#27979;&#24615;&#35299;&#30721;&#30340;&#21407;&#21017;&#24615;&#29702;&#35299;&#65292;&#20351;&#24471;&#20174;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#33258;&#22238;&#24402;&#37319;&#26679;&#30340;&#36807;&#31243;&#33021;&#22815;&#26356;&#24555;&#36895;&#22320;&#36827;&#34892;&#12290;</title><link>http://arxiv.org/abs/2310.15141</link><description>&lt;p&gt;
SpecTr: &#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#23454;&#29616;&#24555;&#36895;&#20855;&#26377;&#25512;&#27979;&#24615;&#30340;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
SpecTr: Fast Speculative Decoding via Optimal Transport. (arXiv:2310.15141v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15141
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#25512;&#27979;&#24615;&#35299;&#30721;&#30340;&#21407;&#21017;&#24615;&#29702;&#35299;&#65292;&#20351;&#24471;&#20174;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#33258;&#22238;&#24402;&#37319;&#26679;&#30340;&#36807;&#31243;&#33021;&#22815;&#26356;&#24555;&#36895;&#22320;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#33258;&#22238;&#24402;&#37319;&#26679;&#22312;&#22810;&#20010;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#33258;&#22238;&#24402;&#37319;&#26679;&#19968;&#27425;&#21482;&#29983;&#25104;&#19968;&#20010;&#26631;&#35760;&#65292;&#36825;&#20351;&#24471;&#36895;&#24230;&#24930;&#65292;&#22312;&#26576;&#20123;&#20219;&#21153;&#20013;&#29978;&#33267;&#26159;&#31105;&#27490;&#30340;&#12290;&#21152;&#36895;&#37319;&#26679;&#30340;&#19968;&#31181;&#26041;&#24335;&#26159;&#8220;&#25512;&#27979;&#24615;&#35299;&#30721;&#8221;&#65306;&#20351;&#29992;&#19968;&#20010;&#23567;&#27169;&#22411;&#26469;&#37319;&#26679;&#19968;&#20010;&#8220;&#33609;&#31295;&#8221;&#65288;&#22359;&#25110;&#26631;&#35760;&#24207;&#21015;&#65289;&#65292;&#28982;&#21518;&#30001;&#22823;&#35821;&#35328;&#27169;&#22411;&#24182;&#34892;&#35780;&#20998;&#33609;&#31295;&#20013;&#30340;&#25152;&#26377;&#26631;&#35760;&#12290;&#26681;&#25454;&#32479;&#35745;&#26041;&#27861;&#65292;&#25509;&#21463;&#19968;&#37096;&#20998;&#33609;&#31295;&#20013;&#30340;&#26631;&#35760;&#65288;&#25298;&#32477;&#21097;&#20313;&#26631;&#35760;&#65289;&#65292;&#20197;&#30830;&#20445;&#26368;&#32456;&#36755;&#20986;&#36981;&#24490;&#22823;&#27169;&#22411;&#30340;&#20998;&#24067;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#65288;OT&#65289;&#19982;&#8220;&#25104;&#21592;&#36153;&#29992;&#8221;&#30340;&#35270;&#35282;&#25552;&#20379;&#20102;&#25512;&#27979;&#24615;&#35299;&#30721;&#30340;&#21407;&#21017;&#24615;&#29702;&#35299;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#8220;&#26368;&#22823;&#32806;&#21512;&#8221;&#38382;&#39064;&#30340;&#25193;&#23637;&#12290;&#36825;&#31181;&#26032;&#30340;&#24418;&#24335;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#25512;&#27979;&#24615;&#35299;&#30721;&#26041;&#27861;&#25512;&#24191;&#21040;&#20801;&#35768;&#19968;&#20010;&#38598;&#21512;&#30340;&#26631;&#35760;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autoregressive sampling from large language models has led to state-of-the-art results in several natural language tasks. However, autoregressive sampling generates tokens one at a time making it slow, and even prohibitive in certain tasks. One way to speed up sampling is $\textit{speculative decoding}$: use a small model to sample a $\textit{draft}$ (block or sequence of tokens), and then score all tokens in the draft by the large language model in parallel. A subset of the tokens in the draft are accepted (and the rest rejected) based on a statistical method to guarantee that the final output follows the distribution of the large model. In this work, we provide a principled understanding of speculative decoding through the lens of optimal transport (OT) with $\textit{membership cost}$. This framework can be viewed as an extension of the well-known $\textit{maximal-coupling}$ problem. This new formulation enables us to generalize the speculative decoding method to allow for a set of $
&lt;/p&gt;</description></item><item><title>&#32473;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21464;&#25442;&#22120;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#30828;&#27880;&#24847;&#21147;&#21644;&#20005;&#26684;&#26410;&#26469;&#25513;&#30721;&#65292;&#24182;&#19988;&#35777;&#26126;&#36825;&#20123;&#32593;&#32476;&#35782;&#21035;&#30340;&#35821;&#35328;&#31867;&#21035;&#27491;&#26159;&#26080;&#26143;&#35821;&#35328;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#36890;&#36807;&#28155;&#21152;&#20301;&#32622;&#23884;&#20837;&#65292;&#36825;&#19968;&#27169;&#22411;&#21487;&#20197;&#25193;&#23637;&#21040;&#20854;&#20182;&#30740;&#31350;&#20805;&#20998;&#30340;&#35821;&#35328;&#31867;&#21035;&#12290;&#19968;&#20010;&#20851;&#38190;&#25216;&#26415;&#26159;&#24067;&#23572;RASP&#65292;&#36890;&#36807;&#26080;&#26143;&#35821;&#35328;&#30340;&#30740;&#31350;&#65292;&#23558;&#21464;&#25442;&#22120;&#19982;&#19968;&#38454;&#36923;&#36753;&#12289;&#26102;&#24577;&#36923;&#36753;&#21644;&#20195;&#25968;&#33258;&#21160;&#26426;&#29702;&#35770;&#30456;&#20851;&#32852;&#12290;</title><link>http://arxiv.org/abs/2310.13897</link><description>&lt;p&gt;
&#25513;&#30721;&#30828;&#27880;&#24847;&#21147;&#21464;&#25442;&#22120;&#21644;&#24067;&#23572;RASP&#20934;&#30830;&#35782;&#21035;&#26080;&#26143;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
Masked Hard-Attention Transformers and Boolean RASP Recognize Exactly the Star-Free Languages. (arXiv:2310.13897v2 [cs.FL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13897
&lt;/p&gt;
&lt;p&gt;
&#32473;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21464;&#25442;&#22120;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#30828;&#27880;&#24847;&#21147;&#21644;&#20005;&#26684;&#26410;&#26469;&#25513;&#30721;&#65292;&#24182;&#19988;&#35777;&#26126;&#36825;&#20123;&#32593;&#32476;&#35782;&#21035;&#30340;&#35821;&#35328;&#31867;&#21035;&#27491;&#26159;&#26080;&#26143;&#35821;&#35328;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#36890;&#36807;&#28155;&#21152;&#20301;&#32622;&#23884;&#20837;&#65292;&#36825;&#19968;&#27169;&#22411;&#21487;&#20197;&#25193;&#23637;&#21040;&#20854;&#20182;&#30740;&#31350;&#20805;&#20998;&#30340;&#35821;&#35328;&#31867;&#21035;&#12290;&#19968;&#20010;&#20851;&#38190;&#25216;&#26415;&#26159;&#24067;&#23572;RASP&#65292;&#36890;&#36807;&#26080;&#26143;&#35821;&#35328;&#30340;&#30740;&#31350;&#65292;&#23558;&#21464;&#25442;&#22120;&#19982;&#19968;&#38454;&#36923;&#36753;&#12289;&#26102;&#24577;&#36923;&#36753;&#21644;&#20195;&#25968;&#33258;&#21160;&#26426;&#29702;&#35770;&#30456;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20855;&#26377;&#30828;&#27880;&#24847;&#21147;&#65288;&#21363;&#25152;&#26377;&#27880;&#24847;&#21147;&#37117;&#38598;&#20013;&#22312;&#19968;&#20010;&#20301;&#32622;&#19978;&#65289;&#21644;&#20005;&#26684;&#30340;&#26410;&#26469;&#25513;&#30721;&#65288;&#21363;&#27599;&#20010;&#20301;&#32622;&#21482;&#19982;&#20005;&#26684;&#24038;&#20391;&#30340;&#20301;&#32622;&#36827;&#34892;&#27880;&#24847;&#21147;&#20132;&#20114;&#65289;&#30340;&#21464;&#25442;&#22120;&#32534;&#30721;&#22120;&#65292;&#24182;&#35777;&#26126;&#36825;&#20123;&#32593;&#32476;&#35782;&#21035;&#30340;&#35821;&#35328;&#31867;&#21035;&#27491;&#26159;&#26080;&#26143;&#35821;&#35328;&#12290;&#28155;&#21152;&#20301;&#32622;&#23884;&#20837;&#23558;&#34987;&#35782;&#21035;&#30340;&#35821;&#35328;&#31867;&#21035;&#25193;&#23637;&#21040;&#20854;&#20182;&#30740;&#31350;&#20805;&#20998;&#30340;&#31867;&#21035;&#12290;&#36825;&#20123;&#35777;&#26126;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25216;&#26415;&#26159;&#24067;&#23572;RASP&#65292;&#23427;&#26159;&#19968;&#31181;&#21463;&#38480;&#20110;&#24067;&#23572;&#20540;&#30340;RASP&#21464;&#31181;&#12290;&#36890;&#36807;&#26080;&#26143;&#35821;&#35328;&#65292;&#25105;&#20204;&#23558;&#21464;&#25442;&#22120;&#19982;&#19968;&#38454;&#36923;&#36753;&#12289;&#26102;&#24577;&#36923;&#36753;&#21644;&#20195;&#25968;&#33258;&#21160;&#26426;&#29702;&#35770;&#32852;&#31995;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider transformer encoders with hard attention (in which all attention is focused on exactly one position) and strict future masking (in which each position only attends to positions strictly to its left), and prove that the class of languages recognized by these networks is exactly the star-free languages. Adding position embeddings increases the class of recognized languages to other well-studied classes. A key technique in these proofs is Boolean RASP, a variant of RASP that is restricted to Boolean values. Via the star-free languages, we relate transformers to first-order logic, temporal logic, and algebraic automata theory.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#30340;FactCHD&#20107;&#23454;&#20914;&#31361;&#24187;&#35273;&#26816;&#27979;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;LLMs&#29983;&#25104;&#25991;&#26412;&#30340;&#20107;&#23454;&#24615;&#12290;&#22522;&#20934;&#21253;&#21547;&#20102;&#22810;&#31181;&#20107;&#23454;&#27169;&#24335;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#20107;&#23454;&#30340;&#35777;&#25454;&#38142;&#36827;&#34892;&#32452;&#21512;&#24615;&#24187;&#35273;&#30340;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2310.12086</link><description>&lt;p&gt;
&#21457;&#29616;&#22622;&#22764;&#20043;&#27468;&#65306;&#21487;&#38752;&#30340;&#20107;&#23454;&#20914;&#31361;&#24187;&#35273;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Unveiling the Siren's Song: Towards Reliable Fact-Conflicting Hallucination Detection. (arXiv:2310.12086v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12086
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#30340;FactCHD&#20107;&#23454;&#20914;&#31361;&#24187;&#35273;&#26816;&#27979;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;LLMs&#29983;&#25104;&#25991;&#26412;&#30340;&#20107;&#23454;&#24615;&#12290;&#22522;&#20934;&#21253;&#21547;&#20102;&#22810;&#31181;&#20107;&#23454;&#27169;&#24335;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#20107;&#23454;&#30340;&#35777;&#25454;&#38142;&#36827;&#34892;&#32452;&#21512;&#24615;&#24187;&#35273;&#30340;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT/GPT-4&#65292;&#22240;&#20854;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#20854;&#22312;&#32593;&#32476;&#24179;&#21488;&#19978;&#23384;&#22312;&#20107;&#23454;&#20914;&#31361;&#24187;&#35273;&#30340;&#38382;&#39064;&#38480;&#21046;&#20102;&#20854;&#37319;&#29992;&#12290;&#23545;&#30001;LLMs&#20135;&#29983;&#30340;&#25991;&#26412;&#30340;&#20107;&#23454;&#24615;&#35780;&#20272;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#65292;&#19981;&#20165;&#28041;&#21450;&#23545;&#22522;&#26412;&#20107;&#23454;&#30340;&#21028;&#26029;&#65292;&#36824;&#21253;&#25324;&#23545;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#65288;&#22914;&#22810;&#36339;&#31561;&#65289;&#20013;&#20986;&#29616;&#30340;&#20107;&#23454;&#38169;&#35823;&#30340;&#35780;&#20272;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;FactCHD&#65292;&#19968;&#31181;&#20026;LLMs&#31934;&#24515;&#35774;&#35745;&#30340;&#20107;&#23454;&#20914;&#31361;&#24187;&#35273;&#26816;&#27979;&#22522;&#20934;&#12290;&#20316;&#20026;&#22312;&#8220;&#26597;&#35810;-&#21709;&#24212;&#8221;&#19978;&#19979;&#25991;&#20013;&#35780;&#20272;&#20107;&#23454;&#24615;&#30340;&#20851;&#38190;&#24037;&#20855;&#65292;&#25105;&#20204;&#30340;&#22522;&#20934;&#37319;&#29992;&#20102;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;&#20107;&#23454;&#27169;&#24335;&#65292;&#22914;&#22522;&#26412;&#20107;&#23454;&#65292;&#22810;&#36339;&#65292;&#27604;&#36739;&#21644;&#38598;&#21512;&#25805;&#20316;&#27169;&#24335;&#12290;&#25105;&#20204;&#22522;&#20934;&#30340;&#19968;&#20010;&#29420;&#29305;&#29305;&#28857;&#26159;&#20854;&#21253;&#21547;&#22522;&#20110;&#20107;&#23454;&#30340;&#35777;&#25454;&#38142;&#65292;&#20174;&#32780;&#20415;&#20110;&#36827;&#34892;&#32452;&#21512;&#24615;&#24187;&#35273;&#30340;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), such as ChatGPT/GPT-4, have garnered widespread attention owing to their myriad of practical applications, yet their adoption has been constrained by issues of fact-conflicting hallucinations across web platforms. The assessment of factuality in text, produced by LLMs, remains inadequately explored, extending not only to the judgment of vanilla facts but also encompassing the evaluation of factual errors emerging in complex inferential tasks like multi-hop, and etc. In response, we introduce FactCHD, a fact-conflicting hallucination detection benchmark meticulously designed for LLMs. Functioning as a pivotal tool in evaluating factuality within "Query-Respons" contexts, our benchmark assimilates a large-scale dataset, encapsulating a broad spectrum of factuality patterns, such as vanilla, multi-hops, comparison, and set-operation patterns. A distinctive feature of our benchmark is its incorporation of fact-based chains of evidence, thereby facilitating com
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#35777;&#26126;&#20102;&#22312;Transformer&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#30005;&#36335;&#32452;&#20214;&#21487;&#20197;&#22312;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#22797;&#29992;&#24182;&#20135;&#29983;&#30456;&#20284;&#30340;&#21151;&#33021;&#65292;&#20026;&#26356;&#39640;&#32423;&#30340;&#27169;&#22411;&#29702;&#35299;&#20570;&#20986;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2310.08744</link><description>&lt;p&gt;
Transformer&#35821;&#35328;&#27169;&#22411;&#20013;&#36328;&#20219;&#21153;&#30340;&#30005;&#36335;&#32452;&#20214;&#22797;&#29992;
&lt;/p&gt;
&lt;p&gt;
Circuit Component Reuse Across Tasks in Transformer Language Models. (arXiv:2310.08744v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08744
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#35777;&#26126;&#20102;&#22312;Transformer&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#30005;&#36335;&#32452;&#20214;&#21487;&#20197;&#22312;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#22797;&#29992;&#24182;&#20135;&#29983;&#30456;&#20284;&#30340;&#21151;&#33021;&#65292;&#20026;&#26356;&#39640;&#32423;&#30340;&#27169;&#22411;&#29702;&#35299;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#26426;&#21046;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#30005;&#36335;&#20998;&#26512;&#21487;&#20197;&#25104;&#21151;&#22320;&#36870;&#21521;&#24037;&#31243;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#24120;&#35265;&#30340;&#25209;&#35780;&#26159;&#27599;&#20010;&#30005;&#36335;&#37117;&#26159;&#20219;&#21153;&#29305;&#23450;&#30340;&#65292;&#22240;&#27492;&#36825;&#26679;&#30340;&#20998;&#26512;&#19981;&#33021;&#20026;&#26356;&#39640;&#32423;&#30340;&#29702;&#35299;&#27169;&#22411;&#20570;&#20986;&#36129;&#29486;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#35777;&#25454;&#34920;&#26126;&#27934;&#23519;&#21147;&#65288;&#20851;&#20110;&#29305;&#23450;&#22836;&#37096;&#30340;&#20302;&#32423;&#21457;&#29616;&#21644;&#20851;&#20110;&#19968;&#33324;&#31639;&#27861;&#30340;&#39640;&#32423;&#21457;&#29616;&#65289;&#30830;&#23454;&#21487;&#20197;&#22312;&#20219;&#21153;&#20043;&#38388;&#36827;&#34892;&#27867;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;Wang&#31561;&#20154;&#65288;2022&#65289;&#22312;&#38388;&#25509;&#23486;&#35821;&#35782;&#21035;&#20219;&#21153;&#65288;IOI&#65289;&#20013;&#21457;&#29616;&#30340;&#30005;&#36335;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#20010;&#30005;&#36335;&#22312;&#26356;&#22823;&#30340;GPT2&#27169;&#22411;&#19978;&#30340;&#37325;&#29616;&#65292;&#20197;&#21450;&#22312;&#30475;&#20284;&#19981;&#21516;&#30340;&#20219;&#21153;&#20013;&#22823;&#37096;&#20998;&#34987;&#22797;&#29992;&#26469;&#35299;&#20915;&#38382;&#39064;&#65306;&#24425;&#33394;&#29289;&#20307;&#65288;Ippolito&#21644;Callison-Burch&#65292;2023&#65289;&#12290;&#25105;&#20204;&#25552;&#20379;&#35777;&#25454;&#34920;&#26126;&#20004;&#20010;&#20219;&#21153;&#24213;&#23618;&#30340;&#36807;&#31243;&#22312;&#21151;&#33021;&#19978;&#38750;&#24120;&#30456;&#20284;&#65292;&#24182;&#19988;&#22312;&#30005;&#36335;&#20013;&#30340;&#27880;&#24847;&#21147;&#22836;&#37096;&#20043;&#38388;&#26377;&#22823;&#32422;78&#65285;&#30340;&#37325;&#21472;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#19968;&#20010;&#27010;&#24565;&#39564;&#35777;&#24178;&#39044;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Recent work in mechanistic interpretability has shown that behaviors in language models can be successfully reverse-engineered through circuit analysis. A common criticism, however, is that each circuit is task-specific, and thus such analysis cannot contribute to understanding the models at a higher level. In this work, we present evidence that insights (both low-level findings about specific heads and higher-level findings about general algorithms) can indeed generalize across tasks. Specifically, we study the circuit discovered in Wang et al. (2022) for the Indirect Object Identification (IOI) task and 1.) show that it reproduces on a larger GPT2 model, and 2.) that it is mostly reused to solve a seemingly different task: Colored Objects (Ippolito &amp; Callison-Burch, 2023). We provide evidence that the process underlying both tasks is functionally very similar, and contains about a 78% overlap in in-circuit attention heads. We further present a proof-of-concept intervention experiment
&lt;/p&gt;</description></item><item><title>3D-Mol&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;3D&#32467;&#26500;&#30340;&#20998;&#23376;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#25552;&#39640;&#20102;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.17366</link><description>&lt;p&gt;
3D-Mol: &#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#26694;&#26550;&#65292;&#21033;&#29992;&#20102;3D&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
3D-Mol: A Novel Contrastive Learning Framework for Molecular Property Prediction with 3D Information. (arXiv:2309.17366v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17366
&lt;/p&gt;
&lt;p&gt;
3D-Mol&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;3D&#32467;&#26500;&#30340;&#20998;&#23376;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#25552;&#39640;&#20102;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#20026;&#33647;&#29289;&#20505;&#36873;&#29289;&#30340;&#26089;&#26399;&#31579;&#36873;&#21644;&#20248;&#21270;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#19988;&#39640;&#25928;&#30340;&#26041;&#27861;&#12290;&#23613;&#31649;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20173;&#26410;&#20805;&#20998;&#21033;&#29992;3D&#31354;&#38388;&#20449;&#24687;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#21333;&#20010;&#20998;&#23376;&#34920;&#31034;&#22810;&#20010;&#23454;&#38469;&#20998;&#23376;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;3D-Mol&#30340;&#26032;&#39062;&#30340;&#22522;&#20110;3D&#32467;&#26500;&#30340;&#20998;&#23376;&#24314;&#27169;&#26041;&#27861;&#12290;&#20026;&#20102;&#20934;&#30830;&#34920;&#31034;&#23436;&#25972;&#30340;&#31354;&#38388;&#32467;&#26500;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#23558;&#20998;&#23376;&#20998;&#35299;&#25104;&#19977;&#20010;&#20960;&#20309;&#22270;&#24418;&#26469;&#25552;&#21462;3D&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;20M&#20010;&#26080;&#26631;&#31614;&#25968;&#25454;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#23545;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#23558;&#20855;&#26377;&#30456;&#21516;&#25299;&#25169;&#32467;&#26500;&#30340;&#26500;&#35937;&#35270;&#20026;&#27491;&#26679;&#26412;&#23545;&#65292;&#23558;&#30456;&#21453;&#30340;&#26500;&#35937;&#35270;&#20026;&#36127;&#26679;&#26412;&#23545;&#65292;&#32780;&#26435;&#37325;&#21017;&#30001;&#26500;&#35937;&#20043;&#38388;&#30340;&#24046;&#24322;&#30830;&#23450;&#12290;&#25105;&#20204;&#22312;7&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23558;3D-Mol&#19982;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#27169;&#22411;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecular property prediction offers an effective and efficient approach for early screening and optimization of drug candidates. Although deep learning based methods have made notable progress, most existing works still do not fully utilize 3D spatial information. This can lead to a single molecular representation representing multiple actual molecules. To address these issues, we propose a novel 3D structure-based molecular modeling method named 3D-Mol. In order to accurately represent complete spatial structure, we design a novel encoder to extract 3D features by deconstructing the molecules into three geometric graphs. In addition, we use 20M unlabeled data to pretrain our model by contrastive learning. We consider conformations with the same topological structure as positive pairs and the opposites as negative pairs, while the weight is determined by the dissimilarity between the conformations. We compare 3D-Mol with various state-of-the-art (SOTA) baselines on 7 benchmarks and de
&lt;/p&gt;</description></item><item><title>&#31995;&#32479;&#21270;&#27867;&#21270;&#26159;&#20154;&#31867;&#30340;&#20851;&#38190;&#25216;&#33021;&#20043;&#19968;&#65292;&#32452;&#21512;&#24335;&#31243;&#24207;&#29983;&#25104;&#22120;&#65288;CPG&#65289;&#36890;&#36807;&#27169;&#22359;&#21270;&#12289;&#31867;&#22411;&#25277;&#35937;&#21644;&#36882;&#24402;&#32452;&#21512;&#30340;&#29305;&#24449;&#65292;&#33021;&#22815;&#20197;&#23569;&#26679;&#26412;&#30340;&#26041;&#24335;&#23545;&#26032;&#27010;&#24565;&#36827;&#34892;&#31995;&#32479;&#21270;&#30340;&#27867;&#21270;&#65292;&#22312;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#19978;&#20855;&#26377;&#29983;&#20135;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.16467</link><description>&lt;p&gt;
&#31995;&#32479;&#21270;&#27867;&#21270;&#30340;&#32452;&#21512;&#24335;&#31243;&#24207;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Compositional Program Generation for Systematic Generalization. (arXiv:2309.16467v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16467
&lt;/p&gt;
&lt;p&gt;
&#31995;&#32479;&#21270;&#27867;&#21270;&#26159;&#20154;&#31867;&#30340;&#20851;&#38190;&#25216;&#33021;&#20043;&#19968;&#65292;&#32452;&#21512;&#24335;&#31243;&#24207;&#29983;&#25104;&#22120;&#65288;CPG&#65289;&#36890;&#36807;&#27169;&#22359;&#21270;&#12289;&#31867;&#22411;&#25277;&#35937;&#21644;&#36882;&#24402;&#32452;&#21512;&#30340;&#29305;&#24449;&#65292;&#33021;&#22815;&#20197;&#23569;&#26679;&#26412;&#30340;&#26041;&#24335;&#23545;&#26032;&#27010;&#24565;&#36827;&#34892;&#31995;&#32479;&#21270;&#30340;&#27867;&#21270;&#65292;&#22312;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#19978;&#20855;&#26377;&#29983;&#20135;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#21512;&#24335;&#27867;&#21270;&#26159;&#20154;&#31867;&#30340;&#20851;&#38190;&#25216;&#33021;&#20043;&#19968;&#65292;&#23427;&#20351;&#25105;&#20204;&#33021;&#22815;&#20174;&#23569;&#25968;&#20363;&#23376;&#20013;&#23398;&#20064;&#26032;&#27010;&#24565;&#12290;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21253;&#25324;&#22914;&#20170;&#26080;&#22788;&#19981;&#22312;&#30340;transformers&#65292;&#22312;&#36825;&#26041;&#38754;&#24456;&#38590;&#36827;&#34892;&#27867;&#21270;&#65292;&#24182;&#19988;&#36890;&#24120;&#38656;&#35201;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25552;&#20379;&#25968;&#21315;&#20010;&#27010;&#24565;&#31034;&#20363;&#25165;&#33021;&#36827;&#34892;&#26377;&#24847;&#20041;&#30340;&#27867;&#21270;&#12290;&#20154;&#31867;&#19982;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#22312;&#33021;&#21147;&#19978;&#30340;&#24046;&#24322;&#65292;&#20419;&#20351;&#20102;&#23545;&#19968;&#31181;&#31216;&#20026;&#32452;&#21512;&#24335;&#31243;&#24207;&#29983;&#25104;&#22120;&#65288;CPG&#65289;&#30340;&#31070;&#32463;&#31526;&#21495;&#20307;&#31995;&#32467;&#26500;&#36827;&#34892;&#30740;&#31350;&#12290;CPG&#20855;&#26377;&#19977;&#20010;&#20851;&#38190;&#29305;&#24449;&#65306;&#27169;&#22359;&#21270;&#12289;&#31867;&#22411;&#25277;&#35937;&#21644;&#36882;&#24402;&#32452;&#21512;&#65292;&#23427;&#20351;&#20854;&#33021;&#22815;&#20197;&#23569;&#26679;&#26412;&#30340;&#26041;&#24335;&#23545;&#26032;&#27010;&#24565;&#36827;&#34892;&#31995;&#32479;&#21270;&#30340;&#27867;&#21270;&#65292;&#21516;&#26102;&#22312;&#21508;&#31181;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#35821;&#35328;&#20219;&#21153;&#19978;&#20855;&#26377;&#29983;&#20135;&#21147;&#12290;&#23545;&#20110;&#27599;&#20010;&#36755;&#20837;&#65292;CPG&#20351;&#29992;&#36755;&#20837;&#39046;&#22495;&#30340;&#35821;&#27861;&#21644;&#35299;&#26512;&#22120;&#29983;&#25104;&#19968;&#20010;&#31867;&#22411;&#23618;&#27425;&#32467;&#26500;&#65292;&#22312;&#36825;&#20010;&#32467;&#26500;&#20013;&#65292;&#27599;&#20010;&#35821;&#27861;&#35268;&#21017;&#37117;&#34987;&#20998;&#37197;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#35821;&#20041;&#27169;&#22359;&#8212;&#8212;&#19968;&#20010;&#27010;&#29575;&#24615;&#30340;&#22797;&#21046;&#25110;&#26367;&#25442;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compositional generalization is a key ability of humans that enables us to learn new concepts from only a handful examples. Machine learning models, including the now ubiquitous transformers, struggle to generalize in this way, and typically require thousands of examples of a concept during training in order to generalize meaningfully. This difference in ability between humans and artificial neural architectures, motivates this study on a neuro-symbolic architecture called the Compositional Program Generator (CPG). CPG has three key features: modularity, type abstraction, and recursive composition, that enable it to generalize both systematically to new concepts in a few-shot manner, as well as productively by length on various sequence-to-sequence language tasks. For each input, CPG uses a grammar of the input domain and a parser to generate a type hierarchy in which each grammar rule is assigned its own unique semantic module, a probabilistic copy or substitution program. Instances w
&lt;/p&gt;</description></item><item><title>Astroconformer&#26159;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#20174;&#24658;&#26143;&#20809;&#26354;&#32447;&#20013;&#25429;&#25417;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#12290;&#36890;&#36807;&#24212;&#29992;&#20110;Kepler&#20809;&#26354;&#32447;&#25968;&#25454;&#38598;&#65292;&#23454;&#29616;&#20102;&#23545;&#24658;&#26143;&#34920;&#38754;&#37325;&#21147;&#30340;&#20934;&#30830;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2309.16316</link><description>&lt;p&gt;
Astroconformer&#65306;&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20998;&#26512;&#24658;&#26143;&#20809;&#26354;&#32447;&#30340;&#21069;&#26223;
&lt;/p&gt;
&lt;p&gt;
Astroconformer: The Prospects of Analyzing Stellar Light Curves with Transformer-Based Deep Learning Models. (arXiv:2309.16316v1 [astro-ph.SR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16316
&lt;/p&gt;
&lt;p&gt;
Astroconformer&#26159;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#20174;&#24658;&#26143;&#20809;&#26354;&#32447;&#20013;&#25429;&#25417;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#12290;&#36890;&#36807;&#24212;&#29992;&#20110;Kepler&#20809;&#26354;&#32447;&#25968;&#25454;&#38598;&#65292;&#23454;&#29616;&#20102;&#23545;&#24658;&#26143;&#34920;&#38754;&#37325;&#21147;&#30340;&#20934;&#30830;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24658;&#26143;&#30340;&#20809;&#21464;&#26354;&#32447;&#21253;&#21547;&#20102;&#20016;&#23500;&#30340;&#20851;&#20110;&#24658;&#26143;&#25391;&#33633;&#21644;&#39063;&#31890;&#36816;&#21160;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#24658;&#26143;&#20869;&#37096;&#32467;&#26500;&#21644;&#28436;&#21270;&#29366;&#24577;&#30340;&#37325;&#35201;&#27934;&#35265;&#12290;&#20256;&#32479;&#30340;&#26143;&#38663;&#23398;&#25216;&#26415;&#20027;&#35201;&#20381;&#36182;&#20110;&#21151;&#29575;&#35889;&#20998;&#26512;&#65292;&#24573;&#30053;&#20102;&#20809;&#21464;&#26354;&#32447;&#20013;&#21253;&#21547;&#30340;&#23453;&#36149;&#30340;&#30456;&#20301;&#20449;&#24687;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#22312;&#26143;&#38663;&#23398;&#20013;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#20174;&#20809;&#21464;&#26354;&#32447;&#20013;&#25104;&#21151;&#22320;&#25512;&#26029;&#20986;&#24658;&#26143;&#23646;&#24615;&#65292;&#20294;&#24448;&#24448;&#21463;&#38480;&#20110;&#21367;&#31215;&#25805;&#20316;&#20013;&#22266;&#26377;&#30340;&#23616;&#37096;&#29305;&#24449;&#25552;&#21462;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Astroconformer&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#25429;&#25417;&#24658;&#26143;&#20809;&#26354;&#32447;&#20013;&#30340;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#20998;&#26512;&#20027;&#35201;&#38598;&#20013;&#22312;&#20272;&#35745;&#34920;&#38754;&#37325;&#21147;&#65288;log g&#65289;&#65292;&#24182;&#22522;&#20110;&#20174;Kepler&#20809;&#26354;&#32447;&#20013;&#31934;&#24515;&#31579;&#36873;&#24471;&#21040;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#12290;&#36825;&#20123;&#20809;&#26354;&#32447;&#21253;&#21547;&#20102;&#22823;&#37327;&#24658;&#26143;&#30340;&#35266;&#27979;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Light curves of stars encapsulate a wealth of information about stellar oscillations and granulation, thereby offering key insights into the internal structure and evolutionary state of stars. Conventional asteroseismic techniques have been largely confined to power spectral analysis, neglecting the valuable phase information contained within light curves. While recent machine learning applications in asteroseismology utilizing Convolutional Neural Networks (CNNs) have successfully inferred stellar attributes from light curves, they are often limited by the local feature extraction inherent in convolutional operations. To circumvent these constraints, we present $\textit{Astroconformer}$, a Transformer-based deep learning framework designed to capture long-range dependencies in stellar light curves. Our empirical analysis, which focuses on estimating surface gravity ($\log g$), is grounded in a carefully curated dataset derived from $\textit{Kepler}$ light curves. These light curves fe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;ICML 2023&#25299;&#25169;&#28145;&#24230;&#23398;&#20064;&#25361;&#25112;&#65292;&#35813;&#25361;&#25112;&#35201;&#27714;&#21442;&#19982;&#32773;&#22312;&#20004;&#20010;&#26376;&#20869;&#25552;&#20379;&#24320;&#28304;&#23454;&#29616;&#30340;&#25299;&#25169;&#31070;&#32463;&#32593;&#32476;&#65292;&#21560;&#24341;&#20102;28&#20010;&#21512;&#26684;&#30340;&#25552;&#20132;&#12290;</title><link>http://arxiv.org/abs/2309.15188</link><description>&lt;p&gt;
ICML 2023&#25299;&#25169;&#28145;&#24230;&#23398;&#20064;&#25361;&#25112;&#65306;&#35774;&#35745;&#19982;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
ICML 2023 Topological Deep Learning Challenge : Design and Results. (arXiv:2309.15188v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15188
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;ICML 2023&#25299;&#25169;&#28145;&#24230;&#23398;&#20064;&#25361;&#25112;&#65292;&#35813;&#25361;&#25112;&#35201;&#27714;&#21442;&#19982;&#32773;&#22312;&#20004;&#20010;&#26376;&#20869;&#25552;&#20379;&#24320;&#28304;&#23454;&#29616;&#30340;&#25299;&#25169;&#31070;&#32463;&#32593;&#32476;&#65292;&#21560;&#24341;&#20102;28&#20010;&#21512;&#26684;&#30340;&#25552;&#20132;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;ICML 2023&#25299;&#25169;&#19982;&#20960;&#20309;&#26426;&#22120;&#23398;&#20064;&#30740;&#35752;&#20250;&#20013;&#20030;&#21150;&#30340;&#25299;&#25169;&#28145;&#24230;&#23398;&#20064;&#35745;&#31639;&#25361;&#25112;&#12290;&#35813;&#27604;&#36187;&#35201;&#27714;&#21442;&#19982;&#32773;&#36890;&#36807;&#36129;&#29486;&#20110;python&#21253;TopoNetX&#65288;&#25968;&#25454;&#22788;&#29702;&#65289;&#21644;TopoModelX&#65288;&#28145;&#24230;&#23398;&#20064;&#65289;&#30340;&#24320;&#28304;&#23454;&#29616;&#26469;&#25552;&#20379;&#25991;&#29486;&#20013;&#30340;&#25299;&#25169;&#31070;&#32463;&#32593;&#32476;&#12290;&#35813;&#25361;&#25112;&#22312;&#20004;&#20010;&#26376;&#30340;&#26102;&#38388;&#20869;&#21560;&#24341;&#20102;28&#20010;&#21512;&#26684;&#30340;&#25552;&#20132;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#25361;&#25112;&#30340;&#35774;&#35745;&#24182;&#24635;&#32467;&#20102;&#20854;&#20027;&#35201;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the computational challenge on topological deep learning that was hosted within the ICML 2023 Workshop on Topology and Geometry in Machine Learning. The competition asked participants to provide open-source implementations of topological neural networks from the literature by contributing to the python packages TopoNetX (data processing) and TopoModelX (deep learning). The challenge attracted twenty-eight qualifying submissions in its two-month duration. This paper describes the design of the challenge and summarizes its main findings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#29616;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#21453;&#21521;&#38477;&#22122;&#20013;&#23384;&#22312;&#19968;&#20010;&#34920;&#36798;&#29942;&#39048;&#65292;&#24182;&#19988;&#25512;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36719;&#28151;&#21512;&#38477;&#22122;&#65288;SMD&#65289;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#29702;&#35770;&#19978;&#33021;&#22815;&#24456;&#22909;&#22320;&#36924;&#36817;&#20219;&#24847;&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;&#65292;&#24182;&#19988;&#22312;&#23454;&#29616;&#19978;&#31616;&#21333;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2309.14068</link><description>&lt;p&gt;
&#36719;&#28151;&#21512;&#38477;&#22122;&#65306;&#36229;&#36234;&#25193;&#25955;&#27169;&#22411;&#30340;&#34920;&#36798;&#29942;&#39048;
&lt;/p&gt;
&lt;p&gt;
Soft Mixture Denoising: Beyond the Expressive Bottleneck of Diffusion Models. (arXiv:2309.14068v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#29616;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#21453;&#21521;&#38477;&#22122;&#20013;&#23384;&#22312;&#19968;&#20010;&#34920;&#36798;&#29942;&#39048;&#65292;&#24182;&#19988;&#25512;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36719;&#28151;&#21512;&#38477;&#22122;&#65288;SMD&#65289;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#29702;&#35770;&#19978;&#33021;&#22815;&#24456;&#22909;&#22320;&#36924;&#36817;&#20219;&#24847;&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;&#65292;&#24182;&#19988;&#22312;&#23454;&#29616;&#19978;&#31616;&#21333;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#25193;&#25955;&#27169;&#22411;&#22312;&#35832;&#22914;&#22270;&#20687;&#21512;&#25104;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#22312;&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#26377;&#19968;&#31181;&#36235;&#21183;&#35777;&#26126;&#65288;&#22312;&#26576;&#20123;&#20551;&#35774;&#19979;&#65289;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#24378;&#22823;&#30340;&#36817;&#20284;&#33021;&#21147;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#21069;&#25193;&#25955;&#27169;&#22411;&#22312;&#21453;&#21521;&#38477;&#22122;&#26041;&#38754;&#23454;&#38469;&#19978;&#23384;&#22312;&#19968;&#20010;&#34920;&#36798;&#29942;&#39048;&#65292;&#24182;&#19988;&#19968;&#20123;&#29616;&#26377;&#29702;&#35770;&#20445;&#35777;&#25152;&#20570;&#30340;&#20551;&#35774;&#36807;&#20110;&#24378;&#22823;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#26412;&#22320;&#21644;&#20840;&#23616;&#38477;&#22122;&#20013;&#23384;&#22312;&#26080;&#30028;&#35823;&#24046;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#29702;&#35770;&#30740;&#31350;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36719;&#28151;&#21512;&#38477;&#22122;&#65288;SMD&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#34920;&#36798;&#21147;&#24378;&#19988;&#39640;&#25928;&#30340;&#21453;&#21521;&#38477;&#22122;&#27169;&#22411;&#12290;SMD&#19981;&#20165;&#22312;&#29702;&#35770;&#19978;&#20801;&#35768;&#25193;&#25955;&#27169;&#22411;&#24456;&#22909;&#22320;&#36924;&#36817;&#20219;&#24847;&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;&#65292;&#32780;&#19988;&#22312;&#23454;&#29616;&#19978;&#31616;&#21333;&#39640;&#25928;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;SMD&#26174;&#33879;&#25913;&#21892;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#25193;&#25955;&#27169;&#22411;&#65288;&#20363;&#22914;DDPM&#65289;&#65292;&#23588;&#20854;&#22312;&#23569;&#37327;&#21453;&#21521;&#38477;&#22122;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Because diffusion models have shown impressive performances in a number of tasks, such as image synthesis, there is a trend in recent works to prove (with certain assumptions) that these models have strong approximation capabilities. In this paper, we show that current diffusion models actually have an expressive bottleneck in backward denoising and some assumption made by existing theoretical guarantees is too strong. Based on this finding, we prove that diffusion models have unbounded errors in both local and global denoising. In light of our theoretical studies, we introduce soft mixture denoising (SMD), an expressive and efficient model for backward denoising. SMD not only permits diffusion models to well approximate any Gaussian mixture distributions in theory, but also is simple and efficient for implementation. Our experiments on multiple image datasets show that SMD significantly improves different types of diffusion models (e.g., DDPM), espeically in the situation of few backw
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#33457;&#29923;&#25289;&#26222;&#25289;&#26031;&#30340;&#39640;&#38454;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#36890;&#36807;&#21033;&#29992;&#31616;&#21333;&#22797;&#21512;&#20307;&#26469;&#24314;&#27169;&#39640;&#38454;&#20132;&#20114;&#65292;&#22312;&#19981;&#21516;&#25299;&#25169;&#23610;&#24230;&#19978;&#35782;&#21035;&#20869;&#22312;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#22270;&#28388;&#27874;&#22120;&#26469;&#37327;&#21270;&#39640;&#38454;&#20132;&#20114;&#24378;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.12971</link><description>&lt;p&gt;
&#22522;&#20110;&#33457;&#29923;&#25289;&#26222;&#25289;&#26031;&#22312;&#31616;&#21333;&#22797;&#21512;&#20307;&#19978;&#30340;&#39640;&#38454;&#22270;&#21367;&#31215;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Higher-order Graph Convolutional Network with Flower-Petals Laplacians on Simplicial Complexes. (arXiv:2309.12971v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#33457;&#29923;&#25289;&#26222;&#25289;&#26031;&#30340;&#39640;&#38454;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#36890;&#36807;&#21033;&#29992;&#31616;&#21333;&#22797;&#21512;&#20307;&#26469;&#24314;&#27169;&#39640;&#38454;&#20132;&#20114;&#65292;&#22312;&#19981;&#21516;&#25299;&#25169;&#23610;&#24230;&#19978;&#35782;&#21035;&#20869;&#22312;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#22270;&#28388;&#27874;&#22120;&#26469;&#37327;&#21270;&#39640;&#38454;&#20132;&#20114;&#24378;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26222;&#36890;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#20854;&#22522;&#20110;&#37197;&#23545;&#20132;&#20114;&#32593;&#32476;&#30340;&#22522;&#30784;&#26412;&#36136;&#19978;&#38480;&#21046;&#20102;&#20854;&#35782;&#21035;&#22797;&#26434;&#31995;&#32479;&#20013;&#28508;&#22312;&#39640;&#38454;&#20132;&#20114;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#31181;&#33021;&#21147;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22797;&#26434;&#31995;&#32479;&#30340;&#39640;&#38454;&#20132;&#20114;&#24314;&#27169;&#30340;&#20016;&#23500;&#25968;&#23398;&#29702;&#35770;&#65292;&#21363;&#31616;&#21333;&#22797;&#21512;&#20307;&#65288;SCs&#65289;-&#19968;&#31181;&#23545;&#24314;&#27169;&#39640;&#38454;&#20132;&#20114;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#24037;&#20855;&#12290;&#30446;&#21069;&#22522;&#20110;SC&#30340;GNNs&#23384;&#22312;&#22797;&#26434;&#24230;&#39640;&#21644;&#21051;&#26495;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#37327;&#21270;&#39640;&#38454;&#20132;&#20114;&#24378;&#24230;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#21019;&#26032;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#38454;&#33457;&#29923;&#65288;FP&#65289;&#27169;&#22411;&#65292;&#23558;FP&#25289;&#26222;&#25289;&#26031;&#24341;&#20837;&#21040;SC&#20013;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20197;FP&#25289;&#26222;&#25289;&#26031;&#20026;&#22522;&#30784;&#30340;&#39640;&#38454;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;HiGCN&#65289;&#65292;&#33021;&#22815;&#35782;&#21035;&#19981;&#21516;&#25299;&#25169;&#23610;&#24230;&#19978;&#30340;&#20869;&#22312;&#29305;&#24449;&#12290;&#36890;&#36807;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#22270;&#28388;&#27874;&#22120;&#65292;FP&#25289;&#26222;&#25289;&#26031;&#22495;&#20869;&#30340;&#21442;&#25968;&#32452;&#65292;&#25105;&#20204;&#21487;&#20197;&#35782;&#21035;&#20986;&#20855;&#26377;&#19981;&#21516;&#27169;&#24335;&#30340;&#22270;&#26696;&#65292;&#20854;&#20013;&#28388;&#27874;&#22120;&#30340;&#26435;&#37325;&#29992;&#20316;&#25968;&#37327;&#21270;&#39640;&#38454;&#20132;&#20114;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the recent successes of vanilla Graph Neural Networks (GNNs) on many tasks, their foundation on pairwise interaction networks inherently limits their capacity to discern latent higher-order interactions in complex systems. To bridge this capability gap, we propose a novel approach exploiting the rich mathematical theory of simplicial complexes (SCs) - a robust tool for modeling higher-order interactions. Current SC-based GNNs are burdened by high complexity and rigidity, and quantifying higher-order interaction strengths remains challenging. Innovatively, we present a higher-order Flower-Petals (FP) model, incorporating FP Laplacians into SCs. Further, we introduce a Higher-order Graph Convolutional Network (HiGCN) grounded in FP Laplacians, capable of discerning intrinsic features across varying topological scales. By employing learnable graph filters, a parameter group within each FP Laplacian domain, we can identify diverse patterns where the filters' weights serve as a quan
&lt;/p&gt;</description></item><item><title>Virchow&#26159;&#19968;&#20010;&#25968;&#30334;&#19975;&#21442;&#25968;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22522;&#30784;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#25968;&#30334;&#19975;&#24352;&#20840;&#25968;&#23383;&#30149;&#29702;&#23398;&#20999;&#29255;&#22270;&#20687;&#19978;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#35757;&#32451;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#35745;&#31639;&#30149;&#29702;&#23398;&#20219;&#21153;&#20013;&#25968;&#25454;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#36229;&#36234;&#20102;&#26368;&#20808;&#36827;&#30340;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2309.07778</link><description>&lt;p&gt;
Virchow: &#25968;&#30334;&#19975;&#24352;&#20840;&#25968;&#23383;&#30149;&#29702;&#23398;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Virchow: A Million-Slide Digital Pathology Foundation Model. (arXiv:2309.07778v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07778
&lt;/p&gt;
&lt;p&gt;
Virchow&#26159;&#19968;&#20010;&#25968;&#30334;&#19975;&#21442;&#25968;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22522;&#30784;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#25968;&#30334;&#19975;&#24352;&#20840;&#25968;&#23383;&#30149;&#29702;&#23398;&#20999;&#29255;&#22270;&#20687;&#19978;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#35757;&#32451;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#35745;&#31639;&#30149;&#29702;&#23398;&#20219;&#21153;&#20013;&#25968;&#25454;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#36229;&#36234;&#20102;&#26368;&#20808;&#36827;&#30340;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#30149;&#29702;&#23398;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#36890;&#36807;&#20998;&#26512;&#20840;&#25968;&#23383;&#20999;&#29255;&#22270;&#20687;&#23454;&#29616;&#31934;&#20934;&#21307;&#23398;&#21644;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#26377;&#28508;&#21147;&#24443;&#24213;&#25913;&#21464;&#30284;&#30151;&#30340;&#35786;&#26029;&#21644;&#27835;&#30103;&#12290;&#28982;&#32780;&#65292;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#23545;&#20110;&#35768;&#22810;&#29305;&#23450;&#30340;&#35745;&#31639;&#30149;&#29702;&#23398;&#20219;&#21153;&#65292;&#25968;&#25454;&#37327;&#19981;&#36275;&#20197;&#36827;&#34892;&#24320;&#21457;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;Virchow&#65292;&#19968;&#20010;632&#30334;&#19975;&#21442;&#25968;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22522;&#30784;&#27169;&#22411;&#65292;&#29992;&#20110;&#35745;&#31639;&#30149;&#29702;&#23398;&#12290;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;Virchow&#22312;1.5&#30334;&#19975;&#20010;&#19981;&#21516;&#32452;&#32455;&#26679;&#26412;&#30340;&#33487;&#26408;&#31934;&#21644;&#20234;&#32418;&#26579;&#33394;&#20840;&#25968;&#23383;&#20999;&#29255;&#22270;&#20687;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#27604;&#20043;&#21069;&#30340;&#30740;&#31350;&#25968;&#25454;&#37327;&#22823;&#24471;&#22810;&#12290;&#22312;&#21253;&#25324;&#29926;&#29255;&#32423;&#20840;&#30284;&#26816;&#27979;&#21644;&#20122;&#22411;&#20197;&#21450;&#24187;&#28783;&#29255;&#32423;&#29983;&#29289;&#26631;&#24535;&#29289;&#39044;&#27979;&#22312;&#20869;&#30340;&#19979;&#28216;&#20219;&#21153;&#19978;&#65292;Virchow&#22312;&#26469;&#33258;&#19982;&#39044;&#35757;&#32451;&#25968;&#25454;&#30456;&#21516;&#20154;&#32676;&#30340;&#20869;&#37096;&#25968;&#25454;&#38598;&#21644;&#22806;&#37096;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#22343;&#32988;&#36807;&#26368;&#20808;&#36827;&#30340;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computational pathology uses artificial intelligence to enable precision medicine and decision support systems through the analysis of whole slide images. It has the potential to revolutionize the diagnosis and treatment of cancer. However, a major challenge to this objective is that for many specific computational pathology tasks the amount of data is inadequate for development. To address this challenge, we created Virchow, a 632 million parameter deep neural network foundation model for computational pathology. Using self-supervised learning, Virchow is trained on 1.5 million hematoxylin and eosin stained whole slide images from diverse tissue groups, which is orders of magnitude more data than previous works. When evaluated on downstream tasks including tile-level pan-cancer detection and subtyping and slide-level biomarker prediction, Virchow outperforms state-of-the-art systems both on internal datasets drawn from the same population as the pretraining data as well as external pu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20154;&#26426;&#20132;&#20114;&#20013;&#32842;&#22825;&#22833;&#36133;&#21644;&#38382;&#39064;&#30340;&#21407;&#22240;&#65292;&#25552;&#20986;&#20102;&#38381;&#29615;&#25511;&#21046;&#31639;&#27861;&#21644;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#31561;&#35299;&#20915;&#26041;&#26696;&#20197;&#38477;&#20302;&#38169;&#35823;&#12290;</title><link>http://arxiv.org/abs/2309.03708</link><description>&lt;p&gt;
&#32842;&#22825;&#22833;&#36133;&#21644;&#38382;&#39064;&#65306;&#21407;&#22240;&#21644;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Chat Failures and Troubles: Reasons and Solutions. (arXiv:2309.03708v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20154;&#26426;&#20132;&#20114;&#20013;&#32842;&#22825;&#22833;&#36133;&#21644;&#38382;&#39064;&#30340;&#21407;&#22240;&#65292;&#25552;&#20986;&#20102;&#38381;&#29615;&#25511;&#21046;&#31639;&#27861;&#21644;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#31561;&#35299;&#20915;&#26041;&#26696;&#20197;&#38477;&#20302;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20154;&#26426;&#20132;&#20114;&#20013;&#32842;&#22825;&#23548;&#33268;&#22833;&#36133;&#21644;&#38382;&#39064;&#30340;&#19968;&#20123;&#24120;&#35265;&#38382;&#39064;&#12290;&#19968;&#20010;&#32473;&#23450;&#30340;&#29992;&#20363;&#30340;&#35774;&#35745;&#20915;&#31574;&#22987;&#20110;&#36873;&#25321;&#21512;&#36866;&#30340;&#26426;&#22120;&#20154;&#12289;&#21512;&#36866;&#30340;&#32842;&#22825;&#27169;&#22411;&#65292;&#35782;&#21035;&#23548;&#33268;&#22833;&#36133;&#30340;&#24120;&#35265;&#38382;&#39064;&#65292;&#25214;&#20986;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#35745;&#21010;&#25345;&#32493;&#25913;&#36827;&#12290;&#24635;&#32467;&#36215;&#26469;&#65292;&#24314;&#35758;&#20351;&#29992;&#38381;&#29615;&#25511;&#21046;&#31639;&#27861;&#26469;&#24341;&#23548;&#35757;&#32451;&#22909;&#30340;&#20154;&#24037;&#26234;&#33021;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20351;&#29992;&#65292;&#24182;&#25552;&#20379;&#35789;&#27719;&#36807;&#28388;&#12289;&#23545;&#26032;&#25968;&#25454;&#38598;&#37325;&#26032;&#35757;&#32451;&#25209;&#27425;&#27169;&#22411;&#12289;&#22312;&#32447;&#23398;&#20064;&#25968;&#25454;&#27969;&#20197;&#21450;/&#25110;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#26469;&#33258;&#25105;&#26356;&#26032;&#35757;&#32451;&#27169;&#22411;&#20197;&#38477;&#20302;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper examines some common problems in Human-Robot Interaction (HRI) causing failures and troubles in Chat. A given use case's design decisions start with the suitable robot, the suitable chatting model, identifying common problems that cause failures, identifying potential solutions, and planning continuous improvement. In conclusion, it is recommended to use a closed-loop control algorithm that guides the use of trained Artificial Intelligence (AI) pre-trained models and provides vocabulary filtering, re-train batched models on new datasets, learn online from data streams, and/or use reinforcement learning models to self-update the trained models and reduce errors.
&lt;/p&gt;</description></item><item><title>BridgeData V2&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#19988;&#22810;&#26679;&#21270;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#34892;&#20026;&#25968;&#25454;&#38598;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#23398;&#20064;&#30740;&#31350;&#12290;&#35813;&#25968;&#25454;&#38598;&#20855;&#22791;&#20219;&#21153;&#21644;&#29615;&#22659;&#21464;&#24322;&#24615;&#65292;&#24182;&#19988;&#20860;&#23481;&#22810;&#31181;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23454;&#39564;&#34920;&#26126;&#65292;&#20351;&#29992;&#27492;&#25968;&#25454;&#38598;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.12952</link><description>&lt;p&gt;
BridgeData V2:&#19968;&#20010;&#29992;&#20110;&#35268;&#27169;&#21270;&#26426;&#22120;&#20154;&#23398;&#20064;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
BridgeData V2: A Dataset for Robot Learning at Scale. (arXiv:2308.12952v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12952
&lt;/p&gt;
&lt;p&gt;
BridgeData V2&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#19988;&#22810;&#26679;&#21270;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#34892;&#20026;&#25968;&#25454;&#38598;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#23398;&#20064;&#30740;&#31350;&#12290;&#35813;&#25968;&#25454;&#38598;&#20855;&#22791;&#20219;&#21153;&#21644;&#29615;&#22659;&#21464;&#24322;&#24615;&#65292;&#24182;&#19988;&#20860;&#23481;&#22810;&#31181;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23454;&#39564;&#34920;&#26126;&#65292;&#20351;&#29992;&#27492;&#25968;&#25454;&#38598;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;BridgeData V2&#65292;&#36825;&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#19988;&#22810;&#26679;&#21270;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#34892;&#20026;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#20419;&#36827;&#35268;&#27169;&#21270;&#26426;&#22120;&#20154;&#23398;&#20064;&#30340;&#30740;&#31350;&#12290;BridgeData V2&#21253;&#21547;&#20102;&#22312;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#19988;&#25104;&#26412;&#36739;&#20302;&#30340;&#26426;&#22120;&#20154;&#19978;&#25910;&#38598;&#30340;60,096&#20010;&#36712;&#36857;&#65292;&#35206;&#30422;&#20102;24&#20010;&#29615;&#22659;&#12290;BridgeData V2&#25552;&#20379;&#20102;&#24191;&#27867;&#30340;&#20219;&#21153;&#21644;&#29615;&#22659;&#21464;&#24322;&#24615;&#65292;&#20351;&#24471;&#21487;&#20197;&#22312;&#19981;&#21516;&#30340;&#29615;&#22659;&#12289;&#39046;&#22495;&#21644;&#26426;&#26500;&#20043;&#38388;&#36827;&#34892;&#27867;&#21270;&#30340;&#25216;&#33021;&#65292;&#20351;&#24471;&#35813;&#25968;&#25454;&#38598;&#25104;&#20026;&#24191;&#22823;&#30740;&#31350;&#20154;&#21592;&#30340;&#26377;&#29992;&#36164;&#28304;&#12290;&#27492;&#22806;&#65292;&#35813;&#25968;&#25454;&#38598;&#19982;&#22810;&#31181;&#24320;&#25918;&#35789;&#27719;&#12289;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#20197;&#30446;&#26631;&#22270;&#20687;&#25110;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#20026;&#26465;&#20214;&#26159;&#20860;&#23481;&#30340;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#22312;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;6&#31181;&#26368;&#20808;&#36827;&#30340;&#27169;&#20223;&#23398;&#20064;&#21644;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#22312;&#19968;&#31995;&#21015;&#38656;&#35201;&#19981;&#21516;&#27867;&#21270;&#31243;&#24230;&#30340;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#24615;&#33021;&#38543;&#30528;&#26356;&#22810;&#30340;&#25968;&#25454;&#21644;&#26356;&#39640;&#23481;&#37327;&#30340;&#27169;&#22411;&#32780;&#25913;&#21892;&#65292;&#24182;&#19988;&#36890;&#36807;&#35757;&#32451;&#38598;&#22823;&#23567;&#30340;&#22686;&#21152;&#21644;&#27169;&#22411;&#23481;&#37327;&#30340;&#22686;&#21152;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce BridgeData V2, a large and diverse dataset of robotic manipulation behaviors designed to facilitate research on scalable robot learning. BridgeData V2 contains 60,096 trajectories collected across 24 environments on a publicly available low-cost robot. BridgeData V2 provides extensive task and environment variability, leading to skills that can generalize across environments, domains, and institutions, making the dataset a useful resource for a broad range of researchers. Additionally, the dataset is compatible with a wide variety of open-vocabulary, multi-task learning methods conditioned on goal images or natural language instructions. In our experiments, we train 6 state-of-the-art imitation learning and offline reinforcement learning methods on our dataset, and find that they succeed on a suite of tasks requiring varying amounts of generalization. We also demonstrate that the performance of these methods improves with more data and higher capacity models, and that trai
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#19979;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#25216;&#26415;&#65292;&#24182;&#25552;&#20986;&#20102;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#21512;&#29702;&#36164;&#28304;&#28040;&#32791;&#30340;&#21516;&#26102;&#65292;&#39640;&#25928;&#22320;&#23558;&#35821;&#35328;&#27169;&#22411;&#19987;&#38376;&#29992;&#20110;&#20219;&#21153;&#29305;&#23450;&#30340;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2308.10462</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Exploring Parameter-Efficient Fine-Tuning Techniques for Code Generation with Large Language Models. (arXiv:2308.10462v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10462
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#19979;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#25216;&#26415;&#65292;&#24182;&#25552;&#20986;&#20102;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#21512;&#29702;&#36164;&#28304;&#28040;&#32791;&#30340;&#21516;&#26102;&#65292;&#39640;&#25928;&#22320;&#23558;&#35821;&#35328;&#27169;&#22411;&#19987;&#38376;&#29992;&#20110;&#20219;&#21153;&#29305;&#23450;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#22312;&#27809;&#26377;&#29305;&#23450;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#21363;&#21487;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#24847;&#22270;&#29983;&#25104;&#20934;&#30830;&#30340;&#20195;&#30721;&#29255;&#27573;&#30340;&#21360;&#35937;&#33021;&#21147;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#31361;&#20986;&#20102;&#24494;&#35843;LLMs&#30340;&#20248;&#21183;&#65292;&#20294;&#36825;&#20010;&#36807;&#31243;&#20195;&#20215;&#39640;&#65292;&#23545;&#20110;&#25317;&#26377;&#25968;&#21313;&#20159;&#20010;&#21442;&#25968;&#30340;&#27169;&#22411;&#26469;&#35828;&#65292;&#22312;&#36164;&#28304;&#31232;&#32570;&#30340;&#29615;&#22659;&#19979;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#20197;&#21069;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#20316;&#20026;&#19968;&#31181;&#31574;&#30053;&#65292;&#29992;&#20219;&#21153;&#29305;&#23450;&#30340;&#25552;&#31034;&#31034;&#20363;&#25351;&#23548;LLM&#29983;&#25104;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;ICL&#24341;&#20837;&#20102;&#19968;&#20123;&#19981;&#20415;&#20043;&#22788;&#65292;&#27604;&#22914;&#38656;&#35201;&#35774;&#35745;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#25552;&#31034;&#21644;&#27809;&#26377;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#30340;&#21442;&#25968;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#39044;&#35265;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#25216;&#26415;&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#21512;&#29702;&#36164;&#28304;&#28040;&#32791;&#30340;&#21516;&#26102;&#65292;&#39640;&#25928;&#22320;&#23558;LLM&#19987;&#38376;&#29992;&#20110;&#20219;&#21153;&#29305;&#23450;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) demonstrate impressive capabilities to generate accurate code snippets given natural language intents in zero-shot, i.e., without the need for specific fine-tuning. While prior studies have highlighted the advantages of fine-tuning LLMs, this process incurs high computational costs, making it impractical in resource-scarce environments, particularly for models with billions of parameters. To address these challenges, previous research explored In-Context Learning (ICL) as a strategy to guide the LLM generative process with task-specific prompt examples. However, ICL introduces inconveniences, such as the need for designing contextually relevant prompts and the absence of learning task-specific parameters, thereby limiting downstream task performance. In this context, we foresee Parameter-Efficient Fine-Tuning (PEFT) techniques as a promising approach to efficiently specialize LLMs to task-specific data while maintaining reasonable resource consumption. In t
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;LLM4TS&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLMs&#22686;&#24378;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#33021;&#21147;&#12290;&#36890;&#36807;&#20004;&#38454;&#27573;&#24494;&#35843;&#21644;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65292;&#25552;&#39640;&#20102;LLMs&#22788;&#29702;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.08469</link><description>&lt;p&gt;
LLM4TS:&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;LLM&#36827;&#34892;&#20004;&#38454;&#27573;&#24494;&#35843;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
LLM4TS: Two-Stage Fine-Tuning for Time-Series Forecasting with Pre-Trained LLMs. (arXiv:2308.08469v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08469
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;LLM4TS&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLMs&#22686;&#24378;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#33021;&#21147;&#12290;&#36890;&#36807;&#20004;&#38454;&#27573;&#24494;&#35843;&#21644;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65292;&#25552;&#39640;&#20102;LLMs&#22788;&#29702;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#22686;&#24378;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#20511;&#37492;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#32479;&#19968;&#27169;&#22411;&#30340;&#26085;&#30410;&#22686;&#38271;&#30340;&#20852;&#36259;&#65292;&#25105;&#20204;&#35774;&#24819;&#21019;&#24314;&#19968;&#20010;&#31867;&#20284;&#30340;&#27169;&#22411;&#29992;&#20110;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#30001;&#20110;&#32570;&#20047;&#22823;&#35268;&#27169;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26469;&#26500;&#24314;&#31283;&#20581;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;LLM4TS&#19987;&#27880;&#20110;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLMs&#30340;&#20248;&#21183;&#12290;&#36890;&#36807;&#23558;&#26102;&#38388;&#24207;&#21015;&#20462;&#34917;&#19982;&#26102;&#38388;&#32534;&#30721;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#25552;&#39640;&#20102;LLMs&#22788;&#29702;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#21463;&#21040;&#32842;&#22825;&#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#26377;&#30417;&#30563;&#24494;&#35843;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20248;&#20808;&#36827;&#34892;&#20004;&#38454;&#27573;&#30340;&#24494;&#35843;&#36807;&#31243;&#65306;&#39318;&#20808;&#36827;&#34892;&#26377;&#30417;&#30563;&#24494;&#35843;&#20197;&#20351;LLMs&#36866;&#24212;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#28982;&#21518;&#36827;&#34892;&#20219;&#21153;&#29305;&#23450;&#30340;&#19979;&#28216;&#24494;&#35843;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#22312;&#19981;&#36827;&#34892;&#22823;&#37327;&#21442;&#25968;&#35843;&#25972;&#30340;&#24773;&#20917;&#19979;&#21457;&#25381;&#39044;&#35757;&#32451;LLMs&#30340;&#28789;&#27963;&#24615;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#20960;&#31181;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we leverage pre-trained Large Language Models (LLMs) to enhance time-series forecasting. Mirroring the growing interest in unifying models for Natural Language Processing and Computer Vision, we envision creating an analogous model for long-term time-series forecasting. Due to limited large-scale time-series data for building robust foundation models, our approach LLM4TS focuses on leveraging the strengths of pre-trained LLMs. By combining time-series patching with temporal encoding, we have enhanced the capability of LLMs to handle time-series data effectively. Inspired by the supervised fine-tuning in chatbot domains, we prioritize a two-stage fine-tuning process: first conducting supervised fine-tuning to orient the LLM towards time-series data, followed by task-specific downstream fine-tuning. Furthermore, to unlock the flexibility of pre-trained LLMs without extensive parameter adjustments, we adopt several Parameter-Efficient Fine-Tuning (PEFT) techniques. Drawing o
&lt;/p&gt;</description></item><item><title>&#25193;&#25955;&#27169;&#22411;&#21487;&#33021;&#21463;&#21040;&#38169;&#35823;&#20256;&#25773;&#30340;&#24433;&#21709;&#65292;&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#25968;&#25454;&#23454;&#35777;&#39564;&#35777;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#27169;&#22411;&#30340;&#21435;&#22122;&#27169;&#22359;&#26159;&#21542;&#20855;&#26377;&#23481;&#38169;&#24615;&#20197;&#21450;&#22914;&#20309;&#20943;&#23569;&#20998;&#24067;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2308.05021</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#21542;&#20250;&#21463;&#21040;&#38169;&#35823;&#20256;&#25773;&#30340;&#24433;&#21709;&#65311;&#29702;&#35770;&#20998;&#26512;&#21644;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Do Diffusion Models Suffer Error Propagation? Theoretical Analysis and Consistency Regularization. (arXiv:2308.05021v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05021
&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#21487;&#33021;&#21463;&#21040;&#38169;&#35823;&#20256;&#25773;&#30340;&#24433;&#21709;&#65292;&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#25968;&#25454;&#23454;&#35777;&#39564;&#35777;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#27169;&#22411;&#30340;&#21435;&#22122;&#27169;&#22359;&#26159;&#21542;&#20855;&#26377;&#23481;&#38169;&#24615;&#20197;&#21450;&#22914;&#20309;&#20943;&#23569;&#20998;&#24067;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#25193;&#25955;&#27169;&#22411;&#22312;&#25968;&#25454;&#21512;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#25104;&#26524;&#65292;&#20294;&#30001;&#20110;&#20854;&#32423;&#32852;&#32467;&#26500;&#65292;&#21363;&#21435;&#22122;&#27169;&#22359;&#38142;&#24335;&#20256;&#25773;&#21644;&#25918;&#22823;&#20102;&#20998;&#24067;&#19981;&#21305;&#37197;&#30340;&#38169;&#35823;&#65292;&#22240;&#27492;&#21487;&#33021;&#20250;&#21463;&#21040;&#38169;&#35823;&#20256;&#25773;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#26399;&#26395;&#36827;&#34892;&#20005;&#26684;&#30340;&#20998;&#26512;&#65292;&#22240;&#20026;&#35768;&#22810;&#39034;&#24207;&#27169;&#22411;&#65292;&#22914;&#26465;&#20214;&#38543;&#26426;&#22330;&#65288;CRF&#65289;&#65292;&#26159;&#19981;&#20250;&#20986;&#29616;&#38169;&#35823;&#20256;&#25773;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#21644;&#29702;&#35770;&#39564;&#35777;&#20102;&#25193;&#25955;&#27169;&#22411;&#30830;&#23454;&#21463;&#21040;&#38169;&#35823;&#20256;&#25773;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#27491;&#21017;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#25581;&#31034;&#20102;&#36825;&#20010;&#38382;&#39064;&#26159;&#21542;&#21487;&#20197;&#24402;&#32467;&#20026;&#25193;&#25955;&#27169;&#22411;&#30340;&#27599;&#20010;&#21435;&#22122;&#27169;&#22359;&#26159;&#21542;&#20855;&#26377;&#23481;&#38169;&#24615;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#26377;&#28145;&#21051;&#35265;&#35299;&#30340;&#36716;&#31227;&#26041;&#31243;&#65292;&#34920;&#26126;&#27169;&#22359;&#26080;&#27861;&#20174;&#36755;&#20837;&#38169;&#35823;&#20013;&#24674;&#22797;&#65292;&#29978;&#33267;&#20250;&#23558;&#39069;&#22806;&#30340;&#38169;&#35823;&#20256;&#25773;&#21040;&#19979;&#19968;&#20010;&#27169;&#22359;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#30452;&#25509;&#23548;&#33268;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#26041;&#26696;&#65292;&#21487;&#20197;&#26126;&#30830;&#20943;&#23569;&#20998;&#24067;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
While diffusion models have achieved promising performances in data synthesis, they might suffer error propagation because of their cascade structure, where the distributional mismatch spreads and magnifies through the chain of denoising modules. However, a strict analysis is expected since many sequential models such as Conditional Random Field (CRF) are free from error propagation. In this paper, we empirically and theoretically verify that diffusion models are indeed affected by error propagation and we then propose a regularization to address this problem. Our theoretical analysis reveals that the question can be reduced to whether every denoising module of the diffusion model is fault-tolerant. We derive insightful transition equations, indicating that the module can't recover from input errors and even propagates additional errors to the next module. Our analysis directly leads to a consistency regularization scheme for diffusion models, which explicitly reduces the distribution 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#38543;&#26426;&#23450;&#20301;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#25193;&#25955;&#27169;&#22411;&#20013;&#32447;&#24615;&#25910;&#25947;&#30028;&#38480;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#26377;&#38480;&#20108;&#38454;&#30697;&#26465;&#20214;&#19979;&#36798;&#21040;&#21487;&#25509;&#21463;&#30340;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.03686</link><description>&lt;p&gt;
&#36890;&#36807;&#38543;&#26426;&#23450;&#20301;&#26041;&#27861;&#33719;&#24471;&#25193;&#25955;&#27169;&#22411;&#30340;&#32447;&#24615;&#25910;&#25947;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Linear Convergence Bounds for Diffusion Models via Stochastic Localization. (arXiv:2308.03686v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03686
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#38543;&#26426;&#23450;&#20301;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#25193;&#25955;&#27169;&#22411;&#20013;&#32447;&#24615;&#25910;&#25947;&#30028;&#38480;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#26377;&#38480;&#20108;&#38454;&#30697;&#26465;&#20214;&#19979;&#36798;&#21040;&#21487;&#25509;&#21463;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#20174;&#39640;&#32500;&#25968;&#25454;&#20998;&#24067;&#20013;&#29983;&#25104;&#36817;&#20284;&#26679;&#26412;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#30740;&#31350;&#32467;&#26524;&#25552;&#20379;&#20102;&#20851;&#20110;&#36825;&#31181;&#27169;&#22411;&#30340;&#25910;&#25947;&#36895;&#24230;&#30340;&#22810;&#39033;&#24335;&#30028;&#38480;&#65292;&#20551;&#35774;$L^2$&#20934;&#30830;&#30340;&#24471;&#20998;&#20272;&#35745;&#22120;&#12290;&#28982;&#32780;&#65292;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#24050;&#30693;&#30340;&#26368;&#20339;&#30028;&#38480;&#35201;&#20040;&#23545;&#25968;&#25454;&#32500;&#24230;&#26159;&#36229;&#32447;&#24615;&#30340;&#65292;&#35201;&#20040;&#38656;&#35201;&#24378;&#24179;&#28369;&#24615;&#20551;&#35774;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#20551;&#35774;&#21482;&#38656;&#35201;&#25968;&#25454;&#20998;&#24067;&#26377;&#26377;&#38480;&#20108;&#38454;&#30697;&#30340;&#25910;&#25947;&#30028;&#38480;&#65292;&#36825;&#20123;&#30028;&#38480;&#23545;&#20110;&#25968;&#25454;&#32500;&#24230;&#26159;&#32447;&#24615;&#30340;&#65288;&#20056;&#20197;&#23545;&#25968;&#22240;&#23376;&#65289;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25193;&#25955;&#27169;&#22411;&#26368;&#22810;&#38656;&#35201;$\tilde O(\frac{d \log^2(1/\delta)}{\varepsilon^2})$&#27493;&#65292;&#23601;&#21487;&#20197;&#23558;&#24102;&#26377;&#26041;&#24046;&#20026;$\delta$&#30340;&#39640;&#26031;&#22122;&#22768;&#25439;&#22351;&#30340;&#20219;&#24847;&#25968;&#25454;&#20998;&#24067;&#22312;Kullback--Leibler&#25955;&#24230;&#19979;&#36817;&#20284;&#21040;$\varepsilon^2$&#12290;&#25105;&#20204;&#30340;&#35777;&#26126;&#20381;&#36182;&#20110;&#21069;&#20154;&#30340;Girsanov&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#23545;&#20110;&#21453;&#21521;SD&#31163;&#25955;&#21270;&#35823;&#24046;&#30340;&#31934;&#32454;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models are a powerful method for generating approximate samples from high-dimensional data distributions. Several recent results have provided polynomial bounds on the convergence rate of such models, assuming $L^2$-accurate score estimators. However, up until now the best known such bounds were either superlinear in the data dimension or required strong smoothness assumptions. We provide the first convergence bounds which are linear in the data dimension (up to logarithmic factors) assuming only finite second moments of the data distribution. We show that diffusion models require at most $\tilde O(\frac{d \log^2(1/\delta)}{\varepsilon^2})$ steps to approximate an arbitrary data distribution on $\mathbb{R}^d$ corrupted with Gaussian noise of variance $\delta$ to within $\varepsilon^2$ in Kullback--Leibler divergence. Our proof builds on the Girsanov-based methods of previous works. We introduce a refined treatment of the error arising from the discretization of the reverse SD
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26354;&#29575;&#30340;&#21464;&#21387;&#22120;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#31163;&#25955;&#21270;&#30340; Ricci &#26354;&#29575;&#65292;&#25913;&#36827;&#20102;&#22270;&#21464;&#21387;&#22120;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#20998;&#23376;&#22270;&#25968;&#25454;&#19978;&#25552;&#21462;&#32467;&#26500;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#65292;&#24182;&#26377;&#25193;&#23637;&#21040;&#20854;&#20182;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.13275</link><description>&lt;p&gt;
&#22522;&#20110;&#26354;&#29575;&#30340;&#21464;&#21387;&#22120;&#29992;&#20110;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Curvature-based Transformer for Molecular Property Prediction. (arXiv:2307.13275v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13275
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26354;&#29575;&#30340;&#21464;&#21387;&#22120;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#31163;&#25955;&#21270;&#30340; Ricci &#26354;&#29575;&#65292;&#25913;&#36827;&#20102;&#22270;&#21464;&#21387;&#22120;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#20998;&#23376;&#22270;&#25968;&#25454;&#19978;&#25552;&#21462;&#32467;&#26500;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#65292;&#24182;&#26377;&#25193;&#23637;&#21040;&#20854;&#20182;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#24615;&#36136;&#30340;&#39044;&#27979;&#26159;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#33647;&#29289;&#35774;&#35745;&#39046;&#22495;&#20013;&#26368;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#20043;&#19968;&#12290;&#22312;&#24403;&#21069;&#20027;&#27969;&#30340;&#26041;&#27861;&#20013;&#65292;&#29992;&#20110;&#35757;&#32451;DNN&#27169;&#22411;&#30340;&#26368;&#24120;&#29992;&#29305;&#24449;&#34920;&#31034;&#22522;&#20110;SMILES&#21644;&#20998;&#23376;&#22270;&#65292;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#31616;&#27905;&#39640;&#25928;&#65292;&#20294;&#20063;&#38480;&#21046;&#20102;&#23545;&#31354;&#38388;&#20449;&#24687;&#30340;&#25429;&#25417;&#33021;&#21147;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#26354;&#29575;&#30340;&#21464;&#21387;&#22120;&#65292;&#36890;&#36807;&#24341;&#20837; Ricci &#26354;&#29575;&#31163;&#25955;&#21270;&#65292;&#25913;&#36827;&#20102;&#22270;&#21464;&#21387;&#22120;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#20998;&#23376;&#22270;&#25968;&#25454;&#19978;&#25552;&#21462;&#32467;&#26500;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#23558;&#26354;&#29575;&#23884;&#20837;&#27169;&#22411;&#20013;&#65292;&#22312;&#27880;&#24847;&#21147;&#24471;&#20998;&#35745;&#31639;&#26399;&#38388;&#65292;&#25105;&#20204;&#23558;&#22270;&#30340;&#26354;&#29575;&#20449;&#24687;&#20316;&#20026;&#20301;&#32622;&#32534;&#30721;&#28155;&#21152;&#21040;&#33410;&#28857;&#29305;&#24449;&#20013;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#19981;&#25913;&#21464;&#21407;&#22987;&#32593;&#32476;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;&#26354;&#29575;&#20449;&#24687;&#24341;&#20837;&#22270;&#25968;&#25454;&#65292;&#24182;&#19988;&#26377;&#28508;&#21147;&#25193;&#23637;&#21040;&#20854;&#20182;&#27169;&#22411;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prediction of molecular properties is one of the most important and challenging tasks in the field of artificial intelligence-based drug design. Among the current mainstream methods, the most commonly used feature representation for training DNN models is based on SMILES and molecular graphs, although these methods are concise and effective, they also limit the ability to capture spatial information. In this work, we propose Curvature-based Transformer to improve the ability of Graph Transformer neural network models to extract structural information on molecular graph data by introducing Discretization of Ricci Curvature. To embed the curvature in the model, we add the curvature information of the graph as positional Encoding to the node features during the attention-score calculation. This method can introduce curvature information from graph data without changing the original network architecture, and it has the potential to be extended to other models. We performed experiments 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#24320;&#25918;&#32852;&#37030;&#23398;&#20064;&#24179;&#21488;&#30340;&#25216;&#26415;&#21644;&#27861;&#24459;&#35266;&#23519;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#26597;&#35810;&#21644;&#22522;&#20110;&#21512;&#21516;&#30340;&#20004;&#31181;&#36866;&#29992;&#20110;&#24320;&#25918;&#32852;&#37030;&#23398;&#20064;&#30340;&#21512;&#20316;&#26694;&#26550;&#65292;&#24182;&#23545;&#26500;&#24314;&#24320;&#25918;&#30340;FL&#24179;&#21488;&#30340;&#21487;&#34892;&#24615;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2307.02140</link><description>&lt;p&gt;
&#24320;&#25918;&#32852;&#37030;&#23398;&#20064;&#24179;&#21488;&#65306;&#25216;&#26415;&#21644;&#27861;&#24459;&#35266;&#23519;&#30340;&#32508;&#36848;&#21644;&#24895;&#26223;
&lt;/p&gt;
&lt;p&gt;
Towards Open Federated Learning Platforms: Survey and Vision from Technical and Legal Perspectives. (arXiv:2307.02140v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#24320;&#25918;&#32852;&#37030;&#23398;&#20064;&#24179;&#21488;&#30340;&#25216;&#26415;&#21644;&#27861;&#24459;&#35266;&#23519;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#26597;&#35810;&#21644;&#22522;&#20110;&#21512;&#21516;&#30340;&#20004;&#31181;&#36866;&#29992;&#20110;&#24320;&#25918;&#32852;&#37030;&#23398;&#20064;&#30340;&#21512;&#20316;&#26694;&#26550;&#65292;&#24182;&#23545;&#26500;&#24314;&#24320;&#25918;&#30340;FL&#24179;&#21488;&#30340;&#21487;&#34892;&#24615;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#36981;&#24490;&#26381;&#21153;&#22120;&#20027;&#23548;&#30340;&#21512;&#20316;&#27169;&#24335;&#65292;&#38480;&#21046;&#20102;FL&#30340;&#24212;&#29992;&#22330;&#26223;&#65292;&#24182;&#38477;&#20302;&#20102;&#25968;&#25454;&#25345;&#26377;&#32773;&#21442;&#19982;&#30340;&#28909;&#24773;&#12290;&#20026;&#20102;&#20805;&#20998;&#37322;&#25918;FL&#30340;&#28508;&#21147;&#65292;&#25105;&#20204;&#20027;&#24352;&#37325;&#26032;&#24605;&#32771;&#24403;&#21069;FL&#26694;&#26550;&#30340;&#35774;&#35745;&#65292;&#24182;&#23558;&#20854;&#25193;&#23637;&#20026;&#26356;&#36890;&#29992;&#30340;&#27010;&#24565;&#65306;&#24320;&#25918;&#32852;&#37030;&#23398;&#20064;&#24179;&#21488;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#30456;&#20114;&#21512;&#20316;&#30340;FL&#26694;&#26550;&#65306;&#22522;&#20110;&#26597;&#35810;&#30340;FL&#21644;&#22522;&#20110;&#21512;&#21516;&#30340;FL&#12290;&#22312;&#36825;&#20010;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#20174;&#25216;&#26415;&#21644;&#27861;&#24459;&#30340;&#35282;&#24230;&#23545;&#26500;&#24314;&#24320;&#25918;&#30340;FL&#24179;&#21488;&#30340;&#21487;&#34892;&#24615;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#39318;&#20808;&#22238;&#39038;&#20102;FL&#30340;&#23450;&#20041;&#65292;&#24182;&#24635;&#32467;&#20102;&#20854;&#22266;&#26377;&#30340;&#23616;&#38480;&#24615;&#65292;&#21253;&#25324;&#26381;&#21153;&#22120;-&#23458;&#25143;&#31471;&#32806;&#21512;&#12289;&#27169;&#22411;&#21487;&#37325;&#29992;&#24615;&#20302;&#21644;&#38750;&#20844;&#24320;&#24615;&#12290;&#22312;&#22522;&#20110;&#26597;&#35810;&#30340;FL&#24179;&#21488;&#20013;&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#31038;&#21306;&#36171;&#33021;&#30340;&#24320;&#25918;&#27169;&#22411;&#20849;&#20139;&#21644;&#37325;&#29992;&#24179;&#21488;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#31995;&#21015;&#26377;&#20215;&#20540;&#30340;&#20027;&#39064;&#65292;&#21253;&#25324;&#20840;&#29699;&#26368;&#26032;&#21487;&#29992;&#27169;&#22411;&#21644;&#27169;&#22411;&#30340;&#26597;&#35810;&#12289;&#26381;&#21153;&#36136;&#37327;&#20445;&#35777;&#21644;&#22870;&#21169;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional Federated Learning (FL) follows a server-domincated cooperation paradigm which narrows the application scenarios of FL and decreases the enthusiasm of data holders to participate. To fully unleash the potential of FL, we advocate rethinking the design of current FL frameworks and extending it to a more generalized concept: Open Federated Learning Platforms. We propose two reciprocal cooperation frameworks for FL to achieve this: query-based FL and contract-based FL. In this survey, we conduct a comprehensive review of the feasibility of constructing an open FL platform from both technical and legal perspectives. We begin by reviewing the definition of FL and summarizing its inherent limitations, including server-client coupling, low model reusability, and non-public. In the query-based FL platform, which is an open model sharing and reusing platform empowered by the community for model mining, we explore a wide range of valuable topics, including the availability of up-to-d
&lt;/p&gt;</description></item><item><title>&#25919;&#27835;&#36777;&#35770;&#12289;&#28436;&#35762;&#21644;&#35775;&#35848;&#20013;&#30340;&#20540;&#24471;&#26680;&#23454;&#30340;&#35770;&#26029;&#21487;&#20197;&#20351;&#29992;&#38899;&#39057;&#25968;&#25454;&#36827;&#34892;&#26816;&#27979;&#21644;&#30830;&#35748;&#65292;&#36825;&#21487;&#24110;&#21161;&#20027;&#25345;&#20154;&#12289;&#35760;&#32773;&#21644;&#20107;&#23454;&#26680;&#26597;&#32452;&#32455;&#36827;&#34892;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2306.05535</link><description>&lt;p&gt;
&#20351;&#29992;&#38899;&#39057;&#25968;&#25454;&#26816;&#27979;&#25919;&#27835;&#36777;&#35770;&#12289;&#28436;&#35762;&#21644;&#35775;&#35848;&#20013;&#20540;&#24471;&#26680;&#23454;&#30340;&#35770;&#26029;
&lt;/p&gt;
&lt;p&gt;
Detecting Check-Worthy Claims in Political Debates, Speeches, and Interviews Using Audio Data. (arXiv:2306.05535v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05535
&lt;/p&gt;
&lt;p&gt;
&#25919;&#27835;&#36777;&#35770;&#12289;&#28436;&#35762;&#21644;&#35775;&#35848;&#20013;&#30340;&#20540;&#24471;&#26680;&#23454;&#30340;&#35770;&#26029;&#21487;&#20197;&#20351;&#29992;&#38899;&#39057;&#25968;&#25454;&#36827;&#34892;&#26816;&#27979;&#21644;&#30830;&#35748;&#65292;&#36825;&#21487;&#24110;&#21161;&#20027;&#25345;&#20154;&#12289;&#35760;&#32773;&#21644;&#20107;&#23454;&#26680;&#26597;&#32452;&#32455;&#36827;&#34892;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20250;&#30340;&#19968;&#22823;&#37096;&#20998;&#22242;&#32467;&#22312;&#30456;&#21516;&#30340;&#24895;&#26223;&#21644;&#24605;&#24819;&#21608;&#22260;&#65292;&#20855;&#26377;&#24040;&#22823;&#30340;&#33021;&#37327;&#12290;&#36825;&#27491;&#26159;&#25919;&#27835;&#20154;&#29289;&#24076;&#26395;&#20026;&#20182;&#20204;&#30340;&#20107;&#19994;&#25152;&#32047;&#31215;&#30340;&#12290;&#20026;&#20102;&#36798;&#21040;&#36825;&#20010;&#30446;&#26631;&#65292;&#20182;&#20204;&#26377;&#26102;&#20250;&#20351;&#29992;&#25197;&#26354;&#25110;&#38544;&#34255;&#30495;&#30456;&#30340;&#25163;&#27573;&#65292;&#26080;&#35770;&#26159;&#26080;&#24847;&#30340;&#36824;&#26159;&#26377;&#24847;&#30340;&#65292;&#36825;&#20026;&#38169;&#35823;&#20449;&#24687;&#21644;&#35823;&#23548;&#24320;&#20102;&#22823;&#38376;&#12290;&#33258;&#21160;&#26816;&#27979;&#20540;&#24471;&#26680;&#23454;&#30340;&#35770;&#26029;&#30340;&#24037;&#20855;&#23558;&#23545;&#36777;&#35770;&#20027;&#25345;&#20154;&#12289;&#35760;&#32773;&#21644;&#20107;&#23454;&#26680;&#26597;&#32452;&#32455;&#26377;&#24456;&#22823;&#24110;&#21161;&#12290;&#34429;&#28982;&#20197;&#21069;&#20851;&#20110;&#26816;&#27979;&#20540;&#24471;&#26680;&#23454;&#30340;&#35770;&#26029;&#30340;&#24037;&#20316;&#37325;&#28857;&#26159;&#25991;&#26412;&#65292;&#20294;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#38899;&#39057;&#20449;&#21495;&#20316;&#20026;&#39069;&#22806;&#20449;&#24687;&#28304;&#30340;&#23454;&#29992;&#24615;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65288;&#33521;&#35821;&#25991;&#26412;&#21644;&#38899;&#39057;&#65289;&#65292;&#21253;&#21547;48&#23567;&#26102;&#30340;&#28436;&#35762;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22810;&#20010;&#28436;&#35762;&#32773;&#30340;&#24773;&#20917;&#19979;&#65292;&#38899;&#39057;&#27169;&#24577;&#19982;&#25991;&#26412;&#32467;&#21512;&#20351;&#29992;&#27604;&#20165;&#20351;&#29992;&#25991;&#26412;&#20855;&#26377;&#25913;&#36827;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#21333;&#22768;&#36947;&#38899;&#39057;&#27169;&#22411;&#21487;&#20197;&#32988;&#36807;&#21333;&#22768;&#36947;&#25991;&#26412;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
A large portion of society united around the same vision and ideas carries enormous energy. That is precisely what political figures would like to accumulate for their cause. With this goal in mind, they can sometimes resort to distorting or hiding the truth, unintentionally or on purpose, which opens the door for misinformation and disinformation. Tools for automatic detection of check-worthy claims would be of great help to moderators of debates, journalists, and fact-checking organizations. While previous work on detecting check-worthy claims has focused on text, here we explore the utility of the audio signal as an additional information source. We create a new multimodal dataset (text and audio in English) containing 48 hours of speech. Our evaluation results show that the audio modality together with text yields improvements over text alone in the case of multiple speakers. Moreover, an audio-only model could outperform a text-only one for a single speaker.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#20984;&#36845;&#20195;&#21152;&#26435;&#26368;&#23567;&#20108;&#20056;&#27861;&#29992;&#20110;&#21516;&#26102;&#24674;&#22797;&#34892;&#31232;&#30095;&#21644;&#20302;&#31209;&#30340;&#25968;&#25454;&#30697;&#38453;&#65292;&#33021;&#22815;&#22312;&#26368;&#23567;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#24773;&#20917;&#19979;&#23616;&#37096;&#20108;&#27425;&#25910;&#25947;&#21040;&#21516;&#26102;&#32467;&#26500;&#21270;&#30340;&#25968;&#25454;&#30697;&#38453;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#26377;&#21033;&#30340;&#32463;&#39564;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04961</link><description>&lt;p&gt;
&#36890;&#36807;&#38750;&#20984;&#36845;&#20195;&#21152;&#26435;&#26368;&#23567;&#20108;&#20056;&#27861;&#21516;&#26102;&#24674;&#22797;&#32467;&#26500;&#21270;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Recovering Simultaneously Structured Data via Non-Convex Iteratively Reweighted Least Squares. (arXiv:2306.04961v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04961
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#20984;&#36845;&#20195;&#21152;&#26435;&#26368;&#23567;&#20108;&#20056;&#27861;&#29992;&#20110;&#21516;&#26102;&#24674;&#22797;&#34892;&#31232;&#30095;&#21644;&#20302;&#31209;&#30340;&#25968;&#25454;&#30697;&#38453;&#65292;&#33021;&#22815;&#22312;&#26368;&#23567;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#24773;&#20917;&#19979;&#23616;&#37096;&#20108;&#27425;&#25910;&#25947;&#21040;&#21516;&#26102;&#32467;&#26500;&#21270;&#30340;&#25968;&#25454;&#30697;&#38453;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#26377;&#21033;&#30340;&#32463;&#39564;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#32447;&#24615;&#35266;&#27979;&#20013;&#24674;&#22797;&#36981;&#24490;&#22810;&#20010;&#24322;&#26500;&#20302;&#32500;&#32467;&#26500;&#30340;&#25968;&#25454;&#12290;&#38024;&#23545;&#21516;&#26102;&#34892;&#31232;&#30095;&#21644;&#20302;&#31209;&#30340;&#25968;&#25454;&#30697;&#38453;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#19968;&#31181;&#36845;&#20195;&#21152;&#26435;&#26368;&#23567;&#20108;&#20056;&#65288;IRLS&#65289;&#31639;&#27861;&#65292;&#33021;&#22815;&#21033;&#29992;&#36825;&#20004;&#31181;&#32467;&#26500;&#12290;&#29305;&#21035;&#22320;&#65292;&#23427;&#20248;&#21270;&#20102;&#19968;&#31181;&#38750;&#20984;&#31232;&#30095;&#24615;&#21644;&#31209;&#30340;&#32452;&#21512;&#20195;&#29702;&#65292;&#20854;&#20013;&#24179;&#34913;&#34987;&#24314;&#20837;&#21040;&#31639;&#27861;&#20013;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#26368;&#23567;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#24773;&#20917;&#19979;&#65288;&#26368;&#22810;&#24120;&#25968;&#21644;&#19968;&#20010;&#23545;&#25968;&#22240;&#23376;&#65289;&#65292;&#36845;&#20195;&#26041;&#24335;&#23616;&#37096;&#20108;&#27425;&#25910;&#25947;&#21040;&#21516;&#26102;&#32467;&#26500;&#21270;&#25968;&#25454;&#30697;&#38453;&#65292;&#36825;&#23545;&#20110;&#32452;&#21512;&#20984;&#20195;&#29702;&#32780;&#35328;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;IRLS&#26041;&#27861;&#34920;&#29616;&#20986;&#20102;&#26377;&#21033;&#30340;&#32463;&#39564;&#25910;&#25947;&#24615;&#65292;&#20174;&#27604;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26356;&#23569;&#30340;&#27979;&#37327;&#20013;&#30830;&#23450;&#20102;&#21516;&#26102;&#34892;&#31232;&#30095;&#21644;&#20302;&#31209;&#30697;&#38453;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new algorithm for the problem of recovering data that adheres to multiple, heterogeneous low-dimensional structures from linear observations. Focusing on data matrices that are simultaneously row-sparse and low-rank, we propose and analyze an iteratively reweighted least squares (IRLS) algorithm that is able to leverage both structures. In particular, it optimizes a combination of non-convex surrogates for row-sparsity and rank, a balancing of which is built into the algorithm. We prove locally quadratic convergence of the iterates to a simultaneously structured data matrix in a regime of minimal sample complexity (up to constants and a logarithmic factor), which is known to be impossible for a combination of convex surrogates. In experiments, we show that the IRLS method exhibits favorable empirical convergence, identifying simultaneously row-sparse and low-rank matrices from fewer measurements than state-of-the-art methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;RKHS&#36924;&#36817;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#20013;&#22909;&#30340;&#25968;&#25454;&#22686;&#24378;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#38416;&#36848;&#20102;&#23545;&#20110;&#20219;&#24847;&#32534;&#30721;&#22120;&#65292;&#22686;&#24191;&#20989;&#25968;&#36136;&#37327;&#30340;&#25552;&#21319;&#21487;&#20197;&#25552;&#39640;&#32534;&#30721;&#22120;&#30340;&#34920;&#31034;&#33021;&#21147;&#65292;&#21516;&#26102;&#20998;&#26512;&#20102;&#25209;&#37327;&#24402;&#19968;&#21270;&#21644;&#25968;&#25454;&#22686;&#24378;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2306.00788</link><description>&lt;p&gt;
&#36890;&#36807;RKHS&#36924;&#36817;&#29702;&#35299;&#22522;&#20110;&#22686;&#24191;&#30340;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Understanding Augmentation-based Self-Supervised Representation Learning via RKHS Approximation. (arXiv:2306.00788v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;RKHS&#36924;&#36817;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#20013;&#22909;&#30340;&#25968;&#25454;&#22686;&#24378;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#38416;&#36848;&#20102;&#23545;&#20110;&#20219;&#24847;&#32534;&#30721;&#22120;&#65292;&#22686;&#24191;&#20989;&#25968;&#36136;&#37327;&#30340;&#25552;&#21319;&#21487;&#20197;&#25552;&#39640;&#32534;&#30721;&#22120;&#30340;&#34920;&#31034;&#33021;&#21147;&#65292;&#21516;&#26102;&#20998;&#26512;&#20102;&#25209;&#37327;&#24402;&#19968;&#21270;&#21644;&#25968;&#25454;&#22686;&#24378;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22909;&#30340;&#25968;&#25454;&#22686;&#24378;&#26159;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#65288;&#22914;&#23545;&#27604;&#23398;&#20064;&#21644;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#65289;&#23454;&#29616;&#32463;&#39564;&#25104;&#21151;&#30340;&#20851;&#38190;&#22240;&#32032;&#20043;&#19968;&#65292;&#20294;&#20854;&#22312;&#23398;&#20064;&#22909;&#30340;&#34920;&#31034;&#26041;&#38754;&#30340;&#29702;&#35770;&#29702;&#35299;&#20173;&#28982;&#26377;&#38480;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#24314;&#31435;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#36924;&#36817;&#22270;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#30340;&#39030;&#37096;&#29305;&#24449;&#31354;&#38388;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#27934;&#23519;&#21147;&#23545;&#22522;&#20110;&#22686;&#24191;&#30340;&#39044;&#35757;&#32451;&#36827;&#34892;&#32479;&#35745;&#20998;&#26512;&#12290;&#25105;&#20204;&#20174;&#20445;&#25345;&#31561;&#36317;&#30340;&#23646;&#24615;&#20986;&#21457;&#65292;&#36825;&#26159;&#30001;&#22686;&#24378;&#32473;&#20986;&#30340;&#30446;&#26631;&#20989;&#25968;&#30340;&#20851;&#38190;&#20960;&#20309;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#20027;&#35201;&#23450;&#29702;&#20026;&#20219;&#24847;&#32534;&#30721;&#22120;&#25552;&#20379;&#20102;&#25509;&#36817;&#32039;&#23494;&#30340;&#19978;&#38480;&#65292;&#29992;&#20110;&#20272;&#35745;&#36890;&#36807;&#22312;&#32534;&#30721;&#22120;&#20043;&#19978;&#25311;&#21512;&#32447;&#24615;&#25506;&#27979;&#22120;&#32780;&#20135;&#29983;&#30340;&#20272;&#35745;&#35823;&#24046;&#21644;&#32534;&#30721;&#22120;&#23398;&#20064;&#30340;RKHS&#30340;&#36924;&#36817;&#35823;&#24046;&#12290;&#25105;&#20204;&#30340;&#31532;&#20108;&#20010;&#20027;&#35201;&#23450;&#29702;&#34920;&#26126;&#65292;&#22312;&#28201;&#21644;&#26465;&#20214;&#19979;&#65292;&#21487;&#20197;&#36890;&#36807;RKHS&#20989;&#25968;&#20219;&#24847;&#31934;&#30830;&#22320;&#36924;&#36817;&#22686;&#24191;&#20989;&#25968;&#12290;&#36825;&#20010;&#32467;&#26524;&#24847;&#21619;&#30528;&#65292;&#38543;&#30528;&#22686;&#24191;&#20989;&#25968;&#36136;&#37327;&#30340;&#25552;&#39640;&#65292;&#23398;&#20064;&#30340;&#32534;&#30721;&#22120;&#30340;&#34920;&#31034;&#33021;&#21147;&#20063;&#20250;&#25552;&#39640;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#36824;&#25581;&#31034;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#25209;&#37327;&#24402;&#19968;&#21270;&#21644;&#25968;&#25454;&#22686;&#24378;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Good data augmentation is one of the key factors that lead to the empirical success of self-supervised representation learning such as contrastive learning and masked language modeling, yet theoretical understanding of its role in learning good representations remains limited. Recent work has built the connection between self-supervised learning and approximating the top eigenspace of a graph Laplacian operator. Learning a linear probe on top of such features can naturally be connected to RKHS regression. In this work, we use this insight to perform a statistical analysis of augmentation-based pretraining. We start from the isometry property, a key geometric characterization of the target function given by the augmentation. Our first main theorem provides, for an arbitrary encoder, near tight bounds for both the estimation error incurred by fitting the linear probe on top of the encoder, and the approximation error entailed by the fitness of the RKHS the encoder learns. Our second main
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#8220;&#24605;&#32500;&#20811;&#38534;&#8221;&#65292;&#36890;&#36807;&#23398;&#20064;&#20154;&#31867;&#30340;&#24605;&#32500;&#26469;&#35757;&#32451;AI&#20195;&#29702;&#65292;&#20197;&#22312;&#27867;&#21270;&#12289;&#25506;&#32034;&#12289;&#35268;&#21010;&#31561;&#33021;&#21147;&#26041;&#38754;&#23454;&#29616;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.00323</link><description>&lt;p&gt;
&#8220;&#24605;&#32500;&#20811;&#38534;&#65306;&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#24605;&#32500;&#23398;&#20064;&#24605;&#32771;&#24182;&#34892;&#21160;&#8221;&#12290;&#65288;arXiv:2306.00323v1 [cs.AI]&#65289;
&lt;/p&gt;
&lt;p&gt;
Thought Cloning: Learning to Think while Acting by Imitating Human Thinking. (arXiv:2306.00323v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00323
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#8220;&#24605;&#32500;&#20811;&#38534;&#8221;&#65292;&#36890;&#36807;&#23398;&#20064;&#20154;&#31867;&#30340;&#24605;&#32500;&#26469;&#35757;&#32451;AI&#20195;&#29702;&#65292;&#20197;&#22312;&#27867;&#21270;&#12289;&#25506;&#32034;&#12289;&#35268;&#21010;&#31561;&#33021;&#21147;&#26041;&#38754;&#23454;&#29616;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#20154;&#31867;&#24605;&#32500;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#65292;&#23427;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#38750;&#20961;&#30340;&#27867;&#21270;&#12289;&#25506;&#32034;&#12289;&#35268;&#21010;&#12289;&#37325;&#26032;&#35268;&#21010;&#21644;&#36866;&#24212;&#26032;&#24773;&#20917;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20195;&#29702;&#22312;&#36825;&#20123;&#33021;&#21147;&#20013;&#36828;&#26410;&#36798;&#21040;&#20154;&#31867;&#27700;&#24179;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#20551;&#35774;&#20854;&#20013;&#19968;&#20010;&#35748;&#30693;&#32570;&#38519;&#30340;&#21407;&#22240;&#26159;&#20182;&#20204;&#32570;&#20047;&#20351;&#29992;&#35821;&#35328;&#24605;&#32771;&#25152;&#24102;&#26469;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#35748;&#20026;&#36890;&#36807;&#35757;&#32451;AI&#20195;&#29702;&#20154;&#20687;&#20154;&#31867;&#19968;&#26679;&#24605;&#32771;&#65292;&#21487;&#20197;&#25913;&#21892;&#20854;&#24615;&#33021;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#8220;&#24605;&#32500;&#20811;&#38534;&#8221;&#65292;&#20854;&#24819;&#27861;&#19981;&#20165;&#26159;&#20811;&#38534;&#20154;&#31867;&#31034;&#33539;&#32773;&#30340;&#34892;&#20026;&#65292;&#32780;&#19988;&#36824;&#21253;&#25324;&#20154;&#31867;&#22312;&#25191;&#34892;&#36825;&#20123;&#34892;&#20026;&#26102;&#25152;&#20135;&#29983;&#30340;&#24819;&#27861;&#12290;&#34429;&#28982;&#25105;&#20204;&#24076;&#26395;&#8220;&#24605;&#32500;&#20811;&#38534;&#8221;&#22312;&#22788;&#29702;&#32593;&#32476;&#35268;&#27169;&#30340;&#20154;&#31867;&#24605;&#32500;&#21644;&#34892;&#20026;&#25968;&#25454;&#26102;&#33021;&#22815;&#21457;&#25381;&#20986;&#33394;&#65288;&#20363;&#22914;&#65292;&#24102;&#26377;&#21095;&#26412;&#30340;&#22312;&#32447;&#35270;&#39057;&#65289;&#65292;&#20294;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22312;&#24605;&#32771;&#21644;&#34892;&#21160;&#25968;&#25454;&#20026;&#21512;&#25104;&#29983;&#25104;&#30340;&#39046;&#22495;&#30340;&#23454;&#39564;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#8220;&#24605;&#32500;&#20811;&#38534;&#8221;&#23398;&#20064;&#36895;&#24230;&#27604;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#24555;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language is often considered a key aspect of human thinking, providing us with exceptional abilities to generalize, explore, plan, replan, and adapt to new situations. However, Reinforcement Learning (RL) agents are far from human-level performance in any of these abilities. We hypothesize one reason for such cognitive deficiencies is that they lack the benefits of thinking in language and that we can improve AI agents by training them to think like humans do. We introduce a novel Imitation Learning framework, Thought Cloning, where the idea is to not just clone the behaviors of human demonstrators, but also the thoughts humans have as they perform these behaviors. While we expect Thought Cloning to truly shine at scale on internet-sized datasets of humans thinking out loud while acting (e.g. online videos with transcripts), here we conduct experiments in a domain where the thinking and action data are synthetically generated. Results reveal that Thought Cloning learns much faster than
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25918;&#26494;&#21435;&#20013;&#24515;&#21270;&#26080;&#36951;&#25022;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#21152;&#27861;&#32422;&#26463;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#35299;&#20915;&#20102;&#36807;&#24230;&#25506;&#32034;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#31216;&#20026;DumBO&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#31454;&#20105;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.19838</link><description>&lt;p&gt;
&#25918;&#26494;&#21435;&#20013;&#24515;&#21270;&#26080;&#36951;&#25022;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#30340;&#21152;&#27861;&#32422;&#26463;
&lt;/p&gt;
&lt;p&gt;
Relaxing the Additivity Constraints in Decentralized No-Regret High-Dimensional Bayesian Optimization. (arXiv:2305.19838v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25918;&#26494;&#21435;&#20013;&#24515;&#21270;&#26080;&#36951;&#25022;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#21152;&#27861;&#32422;&#26463;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#35299;&#20915;&#20102;&#36807;&#24230;&#25506;&#32034;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#31216;&#20026;DumBO&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#24120;&#29992;&#20110;&#20248;&#21270;&#19968;&#20010;&#26410;&#30693;&#20989;&#25968;$f$&#65292;&#35813;&#20989;&#25968;&#23384;&#22312;&#22122;&#22768;&#19988;&#35780;&#20272;&#25104;&#26412;&#39640;&#26114;&#65292;&#36890;&#36807;&#21033;&#29992;&#24517;&#39035;&#22312;&#27599;&#20010;&#20248;&#21270;&#27493;&#39588;&#20013;&#26368;&#22823;&#21270;&#30340;&#25910;&#33719;&#20989;&#25968;&#26469;&#23454;&#29616;&#12290;&#23613;&#31649;&#21487;&#35777;&#26126;&#28176;&#36827;&#26368;&#20248;&#30340;BO&#31639;&#27861;&#22312;&#20248;&#21270;&#20302;&#32500;&#20989;&#25968;&#26041;&#38754;&#25928;&#29575;&#24456;&#39640;&#65292;&#20294;&#23558;&#20854;&#25193;&#23637;&#21040;&#39640;&#32500;&#31354;&#38388;&#20173;&#28982;&#26159;&#19968;&#20010;&#24453;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#36890;&#24120;&#36890;&#36807;&#20551;&#35774;$f$&#20855;&#26377;&#21152;&#27861;&#32467;&#26500;&#26469;&#35299;&#20915;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;BO&#31639;&#27861;&#36890;&#24120;&#24341;&#20837;&#20102;&#23545;&#21152;&#27861;&#32467;&#26500;&#30340;&#39069;&#22806;&#38480;&#21046;&#24615;&#20551;&#35774;&#65292;&#38477;&#20302;&#20102;&#23427;&#20204;&#30340;&#36866;&#29992;&#33539;&#22260;&#12290;&#26412;&#25991;&#21253;&#21547;&#20004;&#20010;&#20027;&#35201;&#36129;&#29486;&#65306;&#65288;i&#65289;&#25918;&#26494;&#23545;$f$&#21152;&#27861;&#32467;&#26500;&#30340;&#38480;&#21046;&#24615;&#20551;&#35774;&#65292;&#20197;&#20943;&#24369;&#25910;&#33719;&#20989;&#25968;&#30340;&#26368;&#22823;&#21270;&#20445;&#35777;&#65307;&#65288;ii&#65289;&#35299;&#20915;&#21435;&#20013;&#24515;&#21270;BO&#31639;&#27861;&#20013;&#30340;&#36807;&#24230;&#25506;&#32034;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DumBO&#65292;&#19968;&#31181;&#28176;&#36827;&#26368;&#20248;&#30340;&#21435;&#20013;&#24515;&#21270;BO&#31639;&#27861;&#65292;&#20855;&#26377;&#38750;&#24120;&#31454;&#20105;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian Optimization (BO) is typically used to optimize an unknown function $f$ that is noisy and costly to evaluate, by exploiting an acquisition function that must be maximized at each optimization step. Even if provably asymptotically optimal BO algorithms are efficient at optimizing low-dimensional functions, scaling them to high-dimensional spaces remains an open problem, often tackled by assuming an additive structure for $f$. By doing so, BO algorithms typically introduce additional restrictive assumptions on the additive structure that reduce their applicability domain. This paper contains two main contributions: (i) we relax the restrictive assumptions on the additive structure of $f$, at the expense of weakening the maximization guarantees of the acquisition function, and (ii) we address the over-exploration problem for decentralized BO algorithms. To these ends, we propose DumBO, an asymptotically optimal decentralized BO algorithm that achieves very competitive performance
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#20010;&#22522;&#20110;&#28857;&#36807;&#31243;&#27880;&#24847;&#21147;&#21644;&#32593;&#26684;&#32534;&#30721;&#30340;&#31639;&#27861;&#65292;&#22312;&#20998;&#24067;&#22806;&#27979;&#35797;&#38598;&#19978;&#23454;&#29616;&#20102;&#27867;&#21270;&#33021;&#21147;&#65292;&#20026;&#29702;&#35299;&#22823;&#33041;&#24378;&#27867;&#21270;&#33021;&#21147;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2305.18417</link><description>&lt;p&gt;
&#28857;&#36807;&#31243;&#27880;&#24847;&#21147;&#25903;&#25345;&#32593;&#26684;&#32534;&#30721;&#20197;&#23454;&#29616;&#20998;&#24067;&#22806;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Determinantal Point Process Attention Over Grid Codes Supports Out of Distribution Generalization. (arXiv:2305.18417v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18417
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#20010;&#22522;&#20110;&#28857;&#36807;&#31243;&#27880;&#24847;&#21147;&#21644;&#32593;&#26684;&#32534;&#30721;&#30340;&#31639;&#27861;&#65292;&#22312;&#20998;&#24067;&#22806;&#27979;&#35797;&#38598;&#19978;&#23454;&#29616;&#20102;&#27867;&#21270;&#33021;&#21147;&#65292;&#20026;&#29702;&#35299;&#22823;&#33041;&#24378;&#27867;&#21270;&#33021;&#21147;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#27169;&#20223;&#31867;&#20154;&#26234;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#65292;&#24182;&#19988;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#29992;&#26469;&#29702;&#35299;&#22823;&#33041;&#22914;&#20309;&#35299;&#20915;&#22797;&#26434;&#30340;&#35745;&#31639;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20173;&#28982;&#19981;&#33021;&#25552;&#20379;&#20851;&#20110;&#22823;&#33041;&#22914;&#20309;&#25903;&#25345;&#20154;&#31867;&#33021;&#22815;&#23454;&#29616;&#30340;&#24378;&#24418;&#24335;&#27867;&#21270;&#30340;&#35265;&#35299;&#12290;&#20854;&#20013;&#19968;&#20010;&#20363;&#23376;&#26159;&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#27867;&#21270;&#8212;&#8212;&#22312;&#35757;&#32451;&#38598;&#20998;&#24067;&#20043;&#22806;&#30340;&#27979;&#35797;&#26679;&#20363;&#19978;&#25104;&#21151;&#25191;&#34892;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35782;&#21035;&#20986;&#22823;&#33041;&#22788;&#29702;&#30340;&#29305;&#24449;&#65292;&#36825;&#20123;&#29305;&#24449;&#21487;&#33021;&#26377;&#21161;&#20110;&#23454;&#29616;&#36825;&#31181;&#33021;&#21147;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#20004;&#37096;&#20998;&#31639;&#27861;&#65292;&#21033;&#29992;&#31070;&#32463;&#35745;&#31639;&#30340;&#29305;&#23450;&#29305;&#24449;&#23454;&#29616;OOD&#27867;&#21270;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;&#20004;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35748;&#30693;&#20219;&#21153;&#30340;&#34920;&#29616;&#26469;&#25552;&#20379;&#27010;&#24565;&#39564;&#35777;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21033;&#29992;&#21754;&#20083;&#21160;&#29289;&#22823;&#33041;&#20351;&#29992;&#31867;&#20284;&#32593;&#26684;&#30340;&#34920;&#31034;&#65288;&#20363;&#22914;&#22312;&#20869;&#21957;&#30382;&#23618;&#20013;&#65289;&#26469;&#34920;&#31034;&#24230;&#37327;&#31354;&#38388;&#30340;&#20107;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks have made tremendous gains in emulating human-like intelligence, and have been used increasingly as ways of understanding how the brain may solve the complex computational problems on which this relies. However, these still fall short of, and therefore fail to provide insight into how the brain supports strong forms of generalization of which humans are capable. One such case is out-of-distribution (OOD) generalization -successful performance on test examples that lie outside the distribution of the training set. Here, we identify properties of processing in the brain that may contribute to this ability. We describe a two-part algorithm that draws on specific features of neural computation to achieve OOD generalization, and provide a proof of concept by evaluating performance on two challenging cognitive tasks. First we draw on the fact that the mammalian brain represents metric spaces using grid-like representations (e.g., in entorhinal cortex): abstract represe
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#27169;&#22411;&#30340;&#31163;&#32447;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;MOMA-PPO&#65292;&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#20132;&#20114;&#25968;&#25454;&#24182;&#20248;&#21270;&#26234;&#33021;&#20307;&#30340;&#25919;&#31574;&#65292;&#35299;&#20915;&#20102;&#31574;&#30053;&#19968;&#33268;&#24615;&#21644;&#31574;&#30053;&#24494;&#35843;&#20004;&#20010;&#21327;&#35843;&#38382;&#39064;&#65292;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#31163;&#32447;MARL&#22330;&#26223;&#20013;&#32988;&#36807;&#20027;&#27969;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#21487;&#34892;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.17198</link><description>&lt;p&gt;
&#31163;&#32447;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#21327;&#35843;&#38382;&#39064;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
A Model-Based Solution to the Offline Multi-Agent Reinforcement Learning Coordination Problem. (arXiv:2305.17198v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17198
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#27169;&#22411;&#30340;&#31163;&#32447;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;MOMA-PPO&#65292;&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#20132;&#20114;&#25968;&#25454;&#24182;&#20248;&#21270;&#26234;&#33021;&#20307;&#30340;&#25919;&#31574;&#65292;&#35299;&#20915;&#20102;&#31574;&#30053;&#19968;&#33268;&#24615;&#21644;&#31574;&#30053;&#24494;&#35843;&#20004;&#20010;&#21327;&#35843;&#38382;&#39064;&#65292;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#31163;&#32447;MARL&#22330;&#26223;&#20013;&#32988;&#36807;&#20027;&#27969;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#21487;&#34892;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#22810;&#20010;&#26234;&#33021;&#20307;&#36827;&#34892;&#21327;&#35843;&#26159;&#19968;&#39033;&#37325;&#35201;&#38382;&#39064;&#65292;&#20855;&#26377;&#26426;&#22120;&#20154;&#25216;&#26415;&#12289;&#21338;&#24328;&#35770;&#12289;&#32463;&#27982;&#23398;&#21644;&#31038;&#20250;&#31185;&#23398;&#31561;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26159;&#22312;&#32447;&#30340;&#65292;&#22240;&#27492;&#22312;&#25910;&#38598;&#26032;&#30340;&#20132;&#20114;&#25968;&#25454;&#25104;&#26412;&#39640;&#26114;&#25110;&#21361;&#38505;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#19981;&#21487;&#34892;&#12290;&#34429;&#28982;&#36825;&#20123;&#31639;&#27861;&#24212;&#35813;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#65292;&#20294;&#36825;&#26679;&#20570;&#20250;&#24341;&#36215;&#31163;&#32447;&#21327;&#35843;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30830;&#23450;&#24182;&#24418;&#24335;&#21270;&#20102;&#31574;&#30053;&#19968;&#33268;&#24615;&#65288;SA&#65289;&#21644;&#31574;&#30053;&#24494;&#35843;&#65288;SFT&#65289;&#20004;&#20010;&#21327;&#35843;&#38382;&#39064;&#65292;&#36825;&#26159;&#24403;&#21069;&#31163;&#32447;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22833;&#36133;&#30340;&#21407;&#22240;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29983;&#25104;&#21512;&#25104;&#20132;&#20114;&#25968;&#25454;&#65292;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#22312;&#24494;&#35843;&#31574;&#30053;&#30340;&#21516;&#26102;&#25910;&#25947;&#20110;&#19968;&#20010;&#31574;&#30053;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;Model-based Offline Multi-Agent Proximal Policy Optimization&#65288;MOMA-PPO&#65289;&#65292;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#31163;&#32447;MARL&#22330;&#26223;&#20013;&#32988;&#36807;&#20027;&#27969;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training multiple agents to coordinate is an important problem with applications in robotics, game theory, economics, and social sciences. However, most existing Multi-Agent Reinforcement Learning (MARL) methods are online and thus impractical for real-world applications in which collecting new interactions is costly or dangerous. While these algorithms should leverage offline data when available, doing so gives rise to the offline coordination problem. Specifically, we identify and formalize the strategy agreement (SA) and the strategy fine-tuning (SFT) challenges, two coordination issues at which current offline MARL algorithms fail. To address this setback, we propose a simple model-based approach that generates synthetic interaction data and enables agents to converge on a strategy while fine-tuning their policies accordingly. Our resulting method, Model-based Offline Multi-Agent Proximal Policy Optimization (MOMA-PPO), outperforms the prevalent learning methods in challenging offl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24418;&#24335;&#35821;&#27861;&#32422;&#26463;&#20016;&#23500;&#35299;&#30721;&#27493;&#39588;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#29983;&#25104;&#31526;&#21512;&#29305;&#23450;&#35821;&#27861;&#30340;&#22797;&#26434;&#36755;&#20986;&#32467;&#26500;&#65292;&#21516;&#26102;&#20801;&#35768;&#20219;&#20309;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#27861;&#38598;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22235;&#20010;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.13971</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#27861;&#32422;&#26463;&#30340;&#35821;&#35328;&#27169;&#22411;&#28789;&#27963;&#35299;&#30721;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Flexible Grammar-Based Constrained Decoding for Language Models. (arXiv:2305.13971v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24418;&#24335;&#35821;&#27861;&#32422;&#26463;&#20016;&#23500;&#35299;&#30721;&#27493;&#39588;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#29983;&#25104;&#31526;&#21512;&#29305;&#23450;&#35821;&#27861;&#30340;&#22797;&#26434;&#36755;&#20986;&#32467;&#26500;&#65292;&#21516;&#26102;&#20801;&#35768;&#20219;&#20309;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#27861;&#38598;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22235;&#20010;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLM&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#23569;&#37327;&#26679;&#26412;&#34920;&#29616;&#65292;&#20294;&#22312;&#29983;&#25104;&#20449;&#24687;&#25552;&#21462;&#25152;&#38656;&#30340;&#22797;&#26434;&#36755;&#20986;&#32467;&#26500;&#26102;&#20173;&#23384;&#22312;&#22256;&#38590;&#12290;&#36825;&#20010;&#38480;&#21046;&#28304;&#20110;LLM&#22312;&#27809;&#26377;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#20542;&#21521;&#20110;&#29983;&#25104;&#33258;&#30001;&#25991;&#26412;&#32780;&#19981;&#26159;&#36981;&#24490;&#29305;&#23450;&#35821;&#27861;&#30340;&#31934;&#30830;&#32467;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#35299;&#30721;&#27493;&#39588;&#20013;&#20351;&#29992;&#24418;&#24335;&#35821;&#27861;&#32422;&#26463;&#26469;&#20016;&#23500;&#27169;&#22411;&#12290;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#65292;&#21482;&#26377;&#31526;&#21512;&#35821;&#27861;&#20135;&#29983;&#35268;&#21017;&#30340;&#26377;&#25928;&#20196;&#29260;&#33021;&#34987;&#32771;&#34385;&#21040;&#12290;&#36825;&#26679;&#23601;&#24378;&#21046;&#21482;&#20135;&#29983;&#26377;&#25928;&#30340;&#24207;&#21015;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#38750;&#24120;&#36890;&#29992;&#21644;&#28789;&#27963;&#65292;&#20801;&#35768;&#20219;&#20309;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#27861;(CFG)&#38598;&#25104;&#21040;&#25105;&#20204;&#30340;&#33258;&#23450;&#20041;&#32422;&#26463;beam&#25628;&#32034;&#23454;&#29616;&#20013;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35768;&#22810;NLP&#20219;&#21153;&#30340;&#36755;&#20986;&#21487;&#20197;&#34987;&#34920;&#31034;&#20026;&#24418;&#24335;&#35821;&#35328;&#65292;&#20351;&#23427;&#20204;&#36866;&#21512;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#30452;&#25509;&#20351;&#29992;&#12290;&#23545;&#20110;&#36755;&#20986;&#31354;&#38388;&#21462;&#20915;&#20110;&#36755;&#20837;&#30340;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#36755;&#20837;&#30340;CFG&#65292;&#26681;&#25454;&#29305;&#23450;&#20110;&#36755;&#20837;&#30340;&#29305;&#24449;&#26356;&#26032;&#20135;&#29983;&#35268;&#21017;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29983;&#25104;&#22797;&#26434;&#36755;&#20986;&#32467;&#26500;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#22235;&#20010;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLMs have shown impressive few-shot performance across many tasks. However, they still struggle when it comes to generating complex output structures, such as those required for Information Extraction. This limitation stems from the fact that LLMs, without finetuning, tend to generate free text rather than precise structures that follow a specific grammar. In this work, we propose to enrich the decoding step with formal grammar constraints. During beam search, only valid token continuations compliant with the grammar production rules are considered. This enforces the generation of valid sequences exclusively. Our framework is highly general and flexible, allowing any Context-Free Grammar (CFG) to be integrated into our custom constrained beam search implementation. We demonstrate that the outputs of many NLP tasks can be represented as formal languages, making them suitable for direct use in our framework. For task where the output space is dependent on the input, we propose input-depe
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphCare&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#20010;&#24615;&#21270;&#30693;&#35782;&#22270;&#35889;&#26469;&#25913;&#36827;&#22522;&#20110;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#21307;&#30103;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#22312;&#20004;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.12788</link><description>&lt;p&gt;
GraphCare: &#20351;&#29992;&#20010;&#24615;&#21270;&#30693;&#35782;&#22270;&#35889;&#25552;&#21319;&#21307;&#30103;&#39044;&#27979;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
GraphCare: Enhancing Healthcare Predictions with Personalized Knowledge Graphs. (arXiv:2305.12788v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphCare&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#20010;&#24615;&#21270;&#30693;&#35782;&#22270;&#35889;&#26469;&#25913;&#36827;&#22522;&#20110;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#21307;&#30103;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#22312;&#20004;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#39044;&#27979;&#27169;&#22411;&#36890;&#24120;&#20381;&#36182;&#20110;&#24739;&#32773;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;(EHR)&#65292;&#20294;&#23558;&#21307;&#23398;&#30693;&#35782;&#25972;&#21512;&#21040;&#39044;&#27979;&#21644;&#20915;&#31574;&#20013;&#20197;&#25552;&#39640;&#25928;&#26524;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36825;&#26159;&#22240;&#20026;&#20010;&#24615;&#21270;&#39044;&#27979;&#38656;&#35201;&#20010;&#24615;&#21270;&#30340;&#30693;&#35782;&#22270;&#35889;(KG)&#65292;&#32780;&#20174;&#24739;&#32773;EHR&#25968;&#25454;&#20013;&#29983;&#25104;&#20010;&#24615;&#21270;&#30693;&#35782;&#22270;&#35889;&#24456;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;\textsc{GraphCare}&#30340;&#24320;&#25918;&#24335;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#22806;&#37096;&#30693;&#35782;&#22270;&#35889;&#26469;&#25913;&#36827;&#22522;&#20110;EHR&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20174;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLM)&#21644;&#22806;&#37096;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#20013;&#25552;&#21462;&#30693;&#35782;&#65292;&#26500;&#24314;&#20010;&#20307;&#21270;&#30340;&#24739;&#32773;&#30693;&#35782;&#22270;&#35889;&#65292;&#28982;&#21518;&#20351;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;Bi-attention AugmenTed (BAT)&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#36827;&#34892;&#21307;&#30103;&#39044;&#27979;&#35757;&#32451;&#12290;&#22312;&#20004;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;MIMIC-III&#21644;MIMIC-IV&#19978;&#65292;\textsc{GraphCare}&#22312;&#22235;&#20010;&#20851;&#38190;&#30340;&#21307;&#30103;&#39044;&#27979;&#20219;&#21153;&#19978;&#22343;&#36229;&#36807;&#20102;&#22522;&#20934;&#32447;&#65306;&#27515;&#20129;&#29575;&#12289;&#20877;&#20837;&#38498;&#29575;&#12289;&#20303;&#38498;&#22825;&#25968;&#21644;&#33647;&#29289;&#25512;&#33616;&#12290;&#22312;MIMIC-III&#19978;&#65292;&#23427;&#23558;AUROC&#25552;&#39640;&#20102;17.6%&#21644;6.6%&#65292;&#23558;F1&#24471;&#20998;&#25552;&#39640;&#20102;7.9%&#12290;
&lt;/p&gt;
&lt;p&gt;
Clinical predictive models often rely on patients' electronic health records (EHR), but integrating medical knowledge to enhance predictions and decision-making is challenging. This is because personalized predictions require personalized knowledge graphs (KGs), which are difficult to generate from patient EHR data. To address this, we propose \textsc{GraphCare}, an open-world framework that uses external KGs to improve EHR-based predictions. Our method extracts knowledge from large language models (LLMs) and external biomedical KGs to build patient-specific KGs, which are then used to train our proposed Bi-attention AugmenTed (BAT) graph neural network (GNN) for healthcare predictions. On two public datasets, MIMIC-III and MIMIC-IV, \textsc{GraphCare} surpasses baselines in four vital healthcare prediction tasks: mortality, readmission, length of stay (LOS), and drug recommendation. On MIMIC-III, it boosts AUROC by 17.6\% and 6.6\% for mortality and readmission, and F1-score by 7.9\% 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#29983;&#25104;&#25991;&#31456;&#22312;&#34394;&#20551;&#26032;&#38395;&#21644;&#20027;&#27969;&#26032;&#38395;&#32593;&#31449;&#30340;&#26222;&#21450;&#31243;&#24230;&#65292;&#21457;&#29616;&#34394;&#20551;&#26032;&#38395;&#32593;&#31449;&#19978;&#21512;&#25104;&#25991;&#31456;&#30340;&#20351;&#29992;&#36895;&#24230;&#27604;&#20027;&#27969;&#32593;&#31449;&#19978;&#26356;&#24555;&#12290;</title><link>http://arxiv.org/abs/2305.09820</link><description>&lt;p&gt;
&#26426;&#22120;&#21046;&#36896;&#30340;&#23186;&#20307;&#65306;&#30417;&#27979;&#34394;&#20551;&#26032;&#38395;&#21644;&#20027;&#27969;&#26032;&#38395;&#32593;&#31449;&#19978;&#26426;&#22120;&#29983;&#25104;&#25991;&#31456;&#30340;&#21160;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine-Made Media: Monitoring the Mobilization of Machine-Generated Articles on Misinformation and Mainstream News Websites. (arXiv:2305.09820v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09820
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#29983;&#25104;&#25991;&#31456;&#22312;&#34394;&#20551;&#26032;&#38395;&#21644;&#20027;&#27969;&#26032;&#38395;&#32593;&#31449;&#30340;&#26222;&#21450;&#31243;&#24230;&#65292;&#21457;&#29616;&#34394;&#20551;&#26032;&#38395;&#32593;&#31449;&#19978;&#21512;&#25104;&#25991;&#31456;&#30340;&#20351;&#29992;&#36895;&#24230;&#27604;&#20027;&#27969;&#32593;&#31449;&#19978;&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20687;ChatGPT&#36825;&#26679;&#30340;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26085;&#30410;&#27969;&#34892;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#26032;&#38395;&#32593;&#31449;&#24320;&#22987;&#21033;&#29992;&#23427;&#20204;&#29983;&#25104;&#25991;&#31456;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#19981;&#20165;&#21487;&#33021;&#22312;&#22768;&#35465;&#33391;&#22909;&#30340;&#32593;&#31449;&#19978;&#20135;&#29983;&#20107;&#23454;&#19981;&#20934;&#30830;&#30340;&#25991;&#31456;&#65292;&#32780;&#19988;&#19981;&#33391;&#26032;&#38395;&#32593;&#31449;&#20063;&#21487;&#20197;&#21033;&#29992;&#36825;&#20123;LLM&#25209;&#37327;&#29983;&#20135;&#34394;&#20551;&#20449;&#24687;&#12290;&#20026;&#20102;&#24320;&#22987;&#29702;&#35299;&#36825;&#19968;&#29616;&#35937;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39318;&#20010;&#22823;&#35268;&#27169;&#30740;&#31350;&#21512;&#25104;&#25991;&#31456;&#22312;&#32447;&#26032;&#38395;&#23186;&#20307;&#20013;&#26222;&#21450;&#29575;&#30340;&#30740;&#31350;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#22522;&#20110;DeBERTa&#30340;&#21512;&#25104;&#26032;&#38395;&#26816;&#27979;&#22120;&#65292;&#24182;&#23545;3074&#20010;&#34394;&#20551;&#26032;&#38395;&#21644;&#20027;&#27969;&#26032;&#38395;&#32593;&#31449;&#30340;&#36229;&#36807;1291&#19975;&#31687;&#25991;&#31456;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;2022&#24180;1&#26376;1&#26085;&#33267;2023&#24180;4&#26376;1&#26085;&#26399;&#38388;&#65292;&#21512;&#25104;&#26032;&#38395;&#25991;&#31456;&#30340;&#30456;&#23545;&#25968;&#37327;&#22312;&#20027;&#27969;&#32593;&#31449;&#19978;&#22686;&#21152;&#20102;79.4&#65285;&#65292;&#32780;&#22312;&#34394;&#20551;&#20449;&#24687;&#32593;&#31449;&#19978;&#22686;&#21152;&#20102;342&#65285;&#12290;&#20998;&#26512;ChatGPT&#21457;&#24067;&#30340;&#24433;&#21709;&#65292;&#20351;&#29992;&#20013;&#26029;&#26102;&#38388;&#24207;&#21015;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#34429;&#28982;&#23427;&#30340;&#21457;&#24067;&#23548;&#33268;&#21512;&#25104;&#25991;&#31456;&#30340;&#20351;&#29992;&#26174;&#33879;&#22686;&#21152;&#65292;&#20294;&#34394;&#20551;&#20449;&#24687;&#32593;&#31449;&#19978;&#30340;&#21512;&#25104;&#25991;&#31456;&#20351;&#29992;&#36895;&#24230;&#27604;&#20027;&#27969;&#32593;&#31449;&#19978;&#30340;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing popularity of generative large language models (LLMs) like ChatGPT, an increasing number of news websites have begun utilizing them to generate articles. However, not only can these language models produce factually inaccurate articles on reputable websites but disreputable news sites can utilize these LLMs to mass produce misinformation. To begin to understand this phenomenon, we present one of the first large-scale studies of the prevalence of synthetic articles within online news media. To do this, we train a DeBERTa-based synthetic news detector and classify over 12.91 million articles from 3,074 misinformation and mainstream news websites. We find that between January 1, 2022 and April 1, 2023, the relative number of synthetic news articles increased by 79.4% on mainstream websites while increasing by 342% on misinformation sites. Analyzing the impact of the release of ChatGPT using an interrupted-time-series, we show that while its release resulted in a marked
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#39033;&#24335;&#36817;&#20284;&#30340;SRAM&#20869;&#25968;&#23383;&#20056;&#27861;&#22120;&#65292;&#22312;&#19981;&#20381;&#36182;&#20110;&#26032;&#22411;&#23384;&#20648;&#25216;&#26415;&#21644;&#36991;&#20813;&#20102;&#27604;&#29305;&#20018;&#34892;&#35745;&#31639;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#20869;&#23384;&#25191;&#34892;GEMM&#35745;&#31639;&#65292;&#20174;&#32780;&#20026;DNN&#35757;&#32451;&#21644;&#25512;&#29702;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#21152;&#36895;&#22120;&#12290;</title><link>http://arxiv.org/abs/2305.07376</link><description>&lt;p&gt;
DAISM&#65306;&#22522;&#20110;&#22810;&#39033;&#24335;&#36817;&#20284;&#30340;SRAM&#20869;&#25968;&#23383;&#20056;&#27861;&#22120;&#30340;DNN&#35757;&#32451;&#21644;&#25512;&#29702;&#21152;&#36895;&#22120;
&lt;/p&gt;
&lt;p&gt;
DAISM: Digital Approximate In-SRAM Multiplier-based Accelerator for DNN Training and Inference. (arXiv:2305.07376v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07376
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#39033;&#24335;&#36817;&#20284;&#30340;SRAM&#20869;&#25968;&#23383;&#20056;&#27861;&#22120;&#65292;&#22312;&#19981;&#20381;&#36182;&#20110;&#26032;&#22411;&#23384;&#20648;&#25216;&#26415;&#21644;&#36991;&#20813;&#20102;&#27604;&#29305;&#20018;&#34892;&#35745;&#31639;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#20869;&#23384;&#25191;&#34892;GEMM&#35745;&#31639;&#65292;&#20174;&#32780;&#20026;DNN&#35757;&#32451;&#21644;&#25512;&#29702;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#21152;&#36895;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
DNN&#26159;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20043;&#19968;&#12290;&#23545;&#20110;DNN&#30340;&#30697;&#38453;&#20056;&#27861;&#36816;&#31639;&#20250;&#20135;&#29983;&#26174;&#33879;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#21463;&#38480;&#20110;&#20869;&#23384;&#21644;&#22788;&#29702;&#21333;&#20803;&#20043;&#38388;&#30340;&#25968;&#25454;&#20256;&#36755;&#12290;&#20026;&#20102;&#20248;&#21270;&#30697;&#38453;&#20056;&#27861;&#36816;&#31639;&#65292;&#25552;&#20986;&#20102;&#35768;&#22810;&#19987;&#38376;&#30340;&#21152;&#36895;&#22120;&#12290;&#19968;&#31181;&#27969;&#34892;&#30340;&#24819;&#27861;&#26159;&#20351;&#29992;PIM&#65288;Processing-in-Memory&#65289;&#65292;&#20854;&#20013;&#35745;&#31639;&#26159;&#30001;&#20869;&#23384;&#23384;&#20648;&#20803;&#20214;&#25191;&#34892;&#30340;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#22788;&#29702;&#22120;&#21644;&#35760;&#24518;&#20307;&#20043;&#38388;&#25968;&#25454;&#20256;&#36755;&#30340;&#24320;&#38144;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;PIM&#35299;&#20915;&#26041;&#26696;&#35201;&#20040;&#20381;&#36182;&#20110;&#23578;&#26410;&#25104;&#29087;&#30340;&#26032;&#22411;&#23384;&#20648;&#25216;&#26415;&#65292;&#35201;&#20040;&#20381;&#36182;&#20110;&#27604;&#29305;&#20018;&#34892;&#35745;&#31639;&#65292;&#21518;&#32773;&#20855;&#26377;&#37325;&#22823;&#24615;&#33021;&#24320;&#38144;&#21644;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;SRAM&#20869;&#25968;&#23383;&#20056;&#27861;&#22120;&#26469;&#37319;&#29992;&#20808;&#36827;&#30340;GEMM&#35745;&#31639;&#25216;&#26415;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#27604;&#29305;&#20018;&#34892;&#35745;&#31639;&#30340;&#32570;&#28857;&#12290;&#36825;&#20351;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#29616;&#26377;&#25216;&#26415;&#32780;&#23454;&#29616;&#24615;&#33021;&#26174;&#33879;&#25552;&#21319;&#30340;&#31995;&#32479;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
DNNs are one of the most widely used Deep Learning models. The matrix multiplication operations for DNNs incur significant computational costs and are bottlenecked by data movement between the memory and the processing elements. Many specialized accelerators have been proposed to optimize matrix multiplication operations. One popular idea is to use Processing-in-Memory where computations are performed by the memory storage element, thereby reducing the overhead of data movement between processor and memory. However, most PIM solutions rely either on novel memory technologies that have yet to mature or bit-serial computations which have significant performance overhead and scalability issues. In this work, an in-SRAM digital multiplier is proposed to take the best of both worlds, i.e. performing GEMM in memory but using only conventional SRAMs without the drawbacks of bit-serial computations. This allows the user to design systems with significant performance gains using existing techno
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#21033;&#29992;&#31526;&#21495;&#22238;&#24402;&#30340;&#20840;&#27969;&#31243;&#65292;&#21487;&#22312;FPGA&#19978;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#25512;&#26029;&#65292;&#20855;&#26377;&#20248;&#21270;&#24615;&#33021;-&#36164;&#28304;&#24179;&#34913;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2305.04099</link><description>&lt;p&gt;
FPGAs &#19978;&#30340;&#31526;&#21495;&#22238;&#24402;&#29992;&#20110;&#24555;&#36895;&#26426;&#22120;&#23398;&#20064;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Symbolic Regression on FPGAs for Fast Machine Learning Inference. (arXiv:2305.04099v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04099
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#21033;&#29992;&#31526;&#21495;&#22238;&#24402;&#30340;&#20840;&#27969;&#31243;&#65292;&#21487;&#22312;FPGA&#19978;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#25512;&#26029;&#65292;&#20855;&#26377;&#20248;&#21270;&#24615;&#33021;-&#36164;&#28304;&#24179;&#34913;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#33021;&#29289;&#29702;&#30028;&#27491;&#22312;&#30740;&#31350;&#22312;&#21487;&#32534;&#31243;&#38376;&#38453;&#21015;&#65288;FPGAs&#65289;&#19978;&#37096;&#32626;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#21487;&#34892;&#24615;&#65292;&#20197;&#25913;&#21892;&#29289;&#29702;&#28789;&#25935;&#24230;&#24182;&#28385;&#36275;&#25968;&#25454;&#22788;&#29702;&#26102;&#24310;&#38480;&#21046;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21033;&#29992;&#31526;&#21495;&#22238;&#24402;&#65288;SR&#65289;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#20840;&#26032;&#31471;&#21040;&#31471;&#27969;&#31243;&#12290;&#23427;&#22312;&#26041;&#31243;&#31354;&#38388;&#20013;&#25628;&#32034;&#36817;&#20284;&#34920;&#31034;&#25968;&#25454;&#38598;&#30340;&#20195;&#25968;&#20851;&#31995;&#12290;&#25105;&#20204;&#20351;&#29992; PySR&#65288;&#22522;&#20110;&#36827;&#21270;&#31639;&#27861;&#21457;&#29616;&#36825;&#20123;&#34920;&#36798;&#24335;&#30340;&#36719;&#20214;&#65289;&#24182;&#25193;&#23637;&#20102; hls4ml &#30340;&#21151;&#33021;&#65288;&#19968;&#31181;&#29992;&#20110;&#25903;&#25345;FPGAs&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#25512;&#26029;&#30340;&#36719;&#20214;&#21253;&#65289;&#65292;&#20197;&#25903;&#25345;&#22312;&#36164;&#28304;&#21463;&#38480;&#21046;&#30340;&#29983;&#20135;&#29615;&#22659;&#20013;&#20351;&#29992; PySR &#29983;&#25104;&#30340;&#34920;&#36798;&#24335;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#36890;&#36807;&#22266;&#23450;&#32593;&#32476;&#22823;&#23567;&#26469;&#20248;&#21270;&#39030;&#32423;&#25351;&#26631;&#65292;&#22240;&#20026;&#24040;&#22823;&#30340;&#36229;&#21442;&#25968;&#31354;&#38388;&#20250;&#38450;&#27490;&#24191;&#27867;&#30340;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#12290;&#30456;&#21453;&#65292;SR &#36873;&#25321;&#20301;&#20110;&#24085;&#32047;&#25176;&#21069;&#27839;&#30340;&#19968;&#32452;&#27169;&#22411;&#65292;&#36825;&#20801;&#35768;&#20248;&#21270;&#24615;&#33021;-&#36164;&#28304;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
The high-energy physics community is investigating the feasibility of deploying machine-learning-based solutions on Field-Programmable Gate Arrays (FPGAs) to improve physics sensitivity while meeting data processing latency limitations. In this contribution, we introduce a novel end-to-end procedure that utilizes a machine learning technique called symbolic regression (SR). It searches equation space to discover algebraic relations approximating a dataset. We use PySR (software for uncovering these expressions based on evolutionary algorithm) and extend the functionality of hls4ml (a package for machine learning inference in FPGAs) to support PySR-generated expressions for resource-constrained production environments. Deep learning models often optimise the top metric by pinning the network size because vast hyperparameter space prevents extensive neural architecture search. Conversely, SR selects a set of models on the Pareto front, which allows for optimising the performance-resource
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#22240;&#26524;&#19990;&#30028;&#27169;&#22411;&#26469;&#35299;&#37322;&#34892;&#21160;&#30340;&#38271;&#26399;&#24433;&#21709;&#20197;&#21450;&#25945;&#23398;&#20064;&#32773;&#22914;&#20309;&#24433;&#21709;&#29615;&#22659;&#21464;&#37327;&#24182;&#26368;&#32456;&#23548;&#33268;&#22870;&#21169;&#12290;</title><link>http://arxiv.org/abs/2305.02749</link><description>&lt;p&gt;
&#36890;&#36807;&#22240;&#26524;&#19990;&#30028;&#27169;&#22411;&#23454;&#29616;&#21487;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Explainable Reinforcement Learning via a Causal World Model. (arXiv:2305.02749v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02749
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#22240;&#26524;&#19990;&#30028;&#27169;&#22411;&#26469;&#35299;&#37322;&#34892;&#21160;&#30340;&#38271;&#26399;&#24433;&#21709;&#20197;&#21450;&#25945;&#23398;&#20064;&#32773;&#22914;&#20309;&#24433;&#21709;&#29615;&#22659;&#21464;&#37327;&#24182;&#26368;&#32456;&#23548;&#33268;&#22870;&#21169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#24378;&#21270;&#23398;&#20064;&#25552;&#20379;&#35299;&#37322;&#26159;&#19968;&#39033;&#25361;&#25112;&#65292;&#22240;&#20026;&#34892;&#21160;&#21487;&#33021;&#23545;&#26410;&#26469;&#20135;&#29983;&#38271;&#26399;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65306;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#22240;&#26524;&#19990;&#30028;&#27169;&#22411;&#32780;&#19981;&#39044;&#20808;&#30693;&#36947;&#29615;&#22659;&#30340;&#22240;&#26524;&#32467;&#26500;&#12290;&#35813;&#27169;&#22411;&#25429;&#25417;&#21040;&#21160;&#20316;&#30340;&#24433;&#21709;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#22240;&#26524;&#38142;&#26469;&#35299;&#37322;&#34892;&#21160;&#30340;&#38271;&#26399;&#24433;&#21709;&#65292;&#20174;&#32780;&#25581;&#31034;&#20986;&#34892;&#21160;&#26159;&#22914;&#20309;&#24433;&#21709;&#29615;&#22659;&#21464;&#37327;&#24182;&#26368;&#32456;&#23548;&#33268;&#22870;&#21169;&#30340;&#12290;&#19982;&#22823;&#22810;&#25968;&#35299;&#37322;&#24615;&#27169;&#22411;&#30340;&#20302;&#20934;&#30830;&#24615;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#25552;&#39640;&#20102;&#35299;&#37322;&#24615;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#22522;&#20110;&#27169;&#22411;&#30340;&#23398;&#20064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#22240;&#26524;&#27169;&#22411;&#21487;&#20197;&#25104;&#20026;&#35299;&#37322;&#24615;&#21644;&#23398;&#20064;&#20043;&#38388;&#30340;&#26725;&#26753;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating explanations for reinforcement learning (RL) is challenging as actions may produce long-term effects on the future. In this paper, we develop a novel framework for explainable RL by learning a causal world model without prior knowledge of the causal structure of the environment. The model captures the influence of actions, allowing us to interpret the long-term effects of actions through causal chains, which present how actions influence environmental variables and finally lead to rewards. Different from most explanatory models which suffer from low accuracy, our model remains accurate while improving explainability, making it applicable in model-based learning. As a result, we demonstrate that our causal model can serve as the bridge between explainability and learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;BA&#31639;&#27861;&#30340;&#19968;&#31181;&#26032;&#30340;&#20462;&#25913;&#65292;&#36890;&#36807;&#35753;&#20056;&#25968;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#36890;&#36807;&#19968;&#32500;&#27714;&#26681;&#26469;&#26356;&#26032;&#65292;&#36825;&#20351;&#24471;&#31639;&#27861;&#33021;&#22815;&#30452;&#25509;&#35745;&#31639;&#25152;&#38656;&#22833;&#30495;&#30340;RD&#20989;&#25968;&#65292;&#32780;&#26080;&#38656;&#20687;&#21407;&#22987;&#31639;&#27861;&#19968;&#26679;&#25506;&#32034;&#25972;&#20010;RD&#26354;&#32447;&#12290;</title><link>http://arxiv.org/abs/2305.02650</link><description>&lt;p&gt;
Blahut&#21644;Arimoto&#30340;&#20027;&#39064;&#21464;&#20307;
&lt;/p&gt;
&lt;p&gt;
Variations on a Theme by Blahut and Arimoto. (arXiv:2305.02650v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02650
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;BA&#31639;&#27861;&#30340;&#19968;&#31181;&#26032;&#30340;&#20462;&#25913;&#65292;&#36890;&#36807;&#35753;&#20056;&#25968;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#36890;&#36807;&#19968;&#32500;&#27714;&#26681;&#26469;&#26356;&#26032;&#65292;&#36825;&#20351;&#24471;&#31639;&#27861;&#33021;&#22815;&#30452;&#25509;&#35745;&#31639;&#25152;&#38656;&#22833;&#30495;&#30340;RD&#20989;&#25968;&#65292;&#32780;&#26080;&#38656;&#20687;&#21407;&#22987;&#31639;&#27861;&#19968;&#26679;&#25506;&#32034;&#25972;&#20010;RD&#26354;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Blahut-Arimoto&#65288;BA&#65289;&#31639;&#27861;&#22312;&#35745;&#31639;&#36895;&#29575;&#22833;&#30495;&#65288;RD&#65289;&#20989;&#25968;&#26041;&#38754;&#36215;&#30528;&#22522;&#30784;&#24615;&#20316;&#29992;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#20132;&#26367;&#26368;&#23567;&#21270;&#24102;&#26377;&#22266;&#23450;&#20056;&#25968;&#30340;Lagrangian&#20855;&#26377;&#29702;&#24819;&#30340;&#21333;&#35843;&#25910;&#25947;&#23646;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BA&#31639;&#27861;&#30340;&#26032;&#39062;&#20462;&#25913;&#65292;&#20351;&#20056;&#25968;&#27599;&#27425;&#36845;&#20195;&#36890;&#36807;&#30456;&#23545;&#20110;&#21333;&#35843;&#21333;&#21464;&#37327;&#20989;&#25968;&#30340;&#19968;&#32500;&#27714;&#26681;&#27493;&#39588;&#26356;&#26032;&#65292;&#36825;&#21487;&#20197;&#36890;&#36807;&#29275;&#39039;&#27861;&#26377;&#25928;&#23454;&#29616;&#12290;&#36825;&#20801;&#35768;&#20197;&#28789;&#27963;&#21644;&#39640;&#25928;&#30340;&#26041;&#24335;&#26356;&#26032;&#20056;&#25968;&#65292;&#20811;&#26381;&#20102;&#21407;&#22987;BA&#31639;&#27861;&#30340;&#19968;&#20010;&#20027;&#35201;&#32570;&#28857;&#65292;&#20854;&#20013;&#20056;&#25968;&#22312;&#25972;&#20010;&#36845;&#20195;&#36807;&#31243;&#20013;&#37117;&#26159;&#22266;&#23450;&#30340;&#12290;&#22240;&#27492;&#65292;&#20462;&#25913;&#21518;&#30340;&#31639;&#27861;&#33021;&#22815;&#30452;&#25509;&#35745;&#31639;&#25152;&#38656;&#22833;&#30495;&#30340;RD&#20989;&#25968;&#65292;&#32780;&#19981;&#20687;&#21407;&#22987;BA&#31639;&#27861;&#19968;&#26679;&#25506;&#32034;&#25972;&#20010;RD&#26354;&#32447;&#12290;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#20462;&#25913;&#21518;&#30340;&#31639;&#27861;&#20173;&#20250;&#25910;&#25947;&#21040;RD&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Blahut-Arimoto (BA) algorithm has played a fundamental role in the numerical computation of rate-distortion (RD) functions. This algorithm possesses a desirable monotonic convergence property by alternatively minimizing its Lagrangian with a fixed multiplier. In this paper, we propose a novel modification of the BA algorithm, letting the multiplier be updated in each iteration via a one-dimensional root-finding step with respect to a monotonic univariate function, which can be efficiently implemented by Newton's method. This allows the multiplier to be updated in a flexible and efficient manner, overcoming a major drawback of the original BA algorithm wherein the multiplier is fixed throughout iterations. Consequently, the modified algorithm is capable of directly computing the RD function for a given target distortion, without exploring the entire RD curve as in the original BA algorithm. A theoretical analysis shows that the modified algorithm still converges to the RD function a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#21452;&#26354;&#34920;&#31034;&#25429;&#25417;&#22270;&#20687;&#21644;&#25991;&#26412;&#23618;&#27425;&#32467;&#26500;&#30340;&#23545;&#27604;&#27169;&#22411;MERU&#65292;&#24182;&#35777;&#26126;&#20854;&#22312;&#22810;&#27169;&#24577;&#20219;&#21153;&#19978;&#19982;CLIP&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2304.09172</link><description>&lt;p&gt;
&#21452;&#26354;&#32447;&#22270;&#20687;&#25991;&#26412;&#34920;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Hyperbolic Image-Text Representations. (arXiv:2304.09172v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09172
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#21452;&#26354;&#34920;&#31034;&#25429;&#25417;&#22270;&#20687;&#21644;&#25991;&#26412;&#23618;&#27425;&#32467;&#26500;&#30340;&#23545;&#27604;&#27169;&#22411;MERU&#65292;&#24182;&#35777;&#26126;&#20854;&#22312;&#22810;&#27169;&#24577;&#20219;&#21153;&#19978;&#19982;CLIP&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#21644;&#35821;&#35328;&#27010;&#24565;&#33258;&#28982;&#32780;&#28982;&#22320;&#32452;&#32455;&#25104;&#19968;&#20010;&#23618;&#27425;&#32467;&#26500;&#65292;&#20854;&#20013;&#19968;&#20010;&#25991;&#26412;&#27010;&#24565;&#8220;&#29399;&#8221;&#21253;&#21547;&#25152;&#26377;&#21253;&#21547;&#29399;&#30340;&#22270;&#20687;&#12290;&#23613;&#31649;&#30452;&#35273;&#19978;&#36825;&#26159;&#27491;&#30830;&#30340;&#65292;&#20294;&#30446;&#21069;&#30340;&#22823;&#35268;&#27169;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;CLIP&#65289;&#24182;&#27809;&#26377;&#26126;&#30830;&#22320;&#25429;&#25417;&#21040;&#36825;&#31181;&#23618;&#27425;&#32467;&#26500;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;MERU&#65292;&#19968;&#20010;&#23545;&#22270;&#20687;&#21644;&#25991;&#26412;&#36827;&#34892;&#21452;&#26354;&#34920;&#31034;&#30340;&#23545;&#27604;&#27169;&#22411;&#12290;&#21452;&#26354;&#31354;&#38388;&#20855;&#26377;&#23884;&#20837;&#26641;&#29366;&#25968;&#25454;&#30340;&#21512;&#36866;&#20960;&#20309;&#23646;&#24615;&#65292;&#22240;&#27492;MERU&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#22270;&#20687;&#25991;&#26412;&#25968;&#25454;&#30340;&#24213;&#23618;&#23618;&#27425;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;MERU&#23398;&#20064;&#21040;&#20102;&#19968;&#20010;&#39640;&#24230;&#21487;&#35299;&#37322;&#30340;&#34920;&#31034;&#31354;&#38388;&#65292;&#21516;&#26102;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#22270;&#20687;&#25991;&#26412;&#26816;&#32034;&#31561;&#22810;&#27169;&#24577;&#20219;&#21153;&#19978;&#19982;CLIP&#30340;&#24615;&#33021;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual and linguistic concepts naturally organize themselves in a hierarchy, where a textual concept ``dog'' entails all images that contain dogs. Despite being intuitive, current large-scale vision and language models such as CLIP do not explicitly capture such hierarchy. We propose MERU, a contrastive model that yields hyperbolic representations of images and text. Hyperbolic spaces have suitable geometric properties to embed tree-like data, so MERU can better capture the underlying hierarchy in image-text data. Our results show that MERU learns a highly interpretable representation space while being competitive with CLIP's performance on multi-modal tasks like image classification and image-text retrieval.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#29983;&#25104;&#24335;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#20869;&#30340;&#35821;&#20041;&#32467;&#26500;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.09048</link><description>&lt;p&gt;
CodeKGC&#65306;&#29992;&#20110;&#29983;&#25104;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CodeKGC: Code Language Model for Generative Knowledge Graph Construction. (arXiv:2304.09048v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09048
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#29983;&#25104;&#24335;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#20869;&#30340;&#35821;&#20041;&#32467;&#26500;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#29983;&#25104;&#24335;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#26041;&#27861;&#36890;&#24120;&#26080;&#27861;&#25429;&#25417;&#32467;&#26500;&#24615;&#30693;&#35782;&#65292;&#32780;&#21482;&#26159;&#23558;&#33258;&#28982;&#35821;&#35328;&#36716;&#21270;&#20026;&#24207;&#21015;&#21270;&#25991;&#26412;&#25110;&#35268;&#33539;&#35821;&#35328;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20687;&#20195;&#30721;&#36825;&#26679;&#30340;&#32467;&#26500;&#21270;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#22823;&#22411;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#23637;&#29616;&#20102;&#22312;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#20197;&#36827;&#34892;&#32467;&#26500;&#24615;&#39044;&#27979;&#21644;&#25512;&#29702;&#20219;&#21153;&#26041;&#38754;&#30340;&#21331;&#36234;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#29983;&#25104;&#24335;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#32473;&#23450;&#20195;&#30721;&#26684;&#24335;&#30340;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#65292;&#30446;&#26631;&#26159;&#29983;&#25104;&#21487;&#20197;&#34920;&#31034;&#20026;&#20195;&#30721;&#34917;&#20840;&#20219;&#21153;&#30340;&#19977;&#20803;&#32452;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#20855;&#26377;&#27169;&#24335;&#24863;&#30693;&#22411;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#20869;&#30340;&#35821;&#20041;&#32467;&#26500;&#12290;&#30001;&#20110;&#20195;&#30721;&#26412;&#36136;&#19978;&#20855;&#26377;&#32467;&#26500;&#65292;&#22914;&#31867;&#21644;&#20989;&#25968;&#23450;&#20041;&#65292;&#22240;&#27492;&#23427;&#20316;&#20026;&#20808;&#39564;&#30340;&#35821;&#20041;&#32467;&#26500;&#30693;&#35782;&#27169;&#22411;&#38750;&#24120;&#26377;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#22522;&#20110;&#21407;&#29702;&#30340;&#29983;&#25104;&#26041;&#27861;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#21407;&#29702;&#25552;&#20379;&#20102;&#27169;&#22411;&#29983;&#25104;&#32467;&#26524;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current generative knowledge graph construction approaches usually fail to capture structural knowledge by simply flattening natural language into serialized texts or a specification language. However, large generative language model trained on structured data such as code has demonstrated impressive capability in understanding natural language for structural prediction and reasoning tasks. Intuitively, we address the task of generative knowledge graph construction with code language model: given a code-format natural language input, the target is to generate triples which can be represented as code completion tasks. Specifically, we develop schema-aware prompts that effectively utilize the semantic structure within the knowledge graph. As code inherently possesses structure, such as class and function definitions, it serves as a useful model for prior semantic structural knowledge. Furthermore, we employ a rationale-enhanced generation method to boost the performance. Rationales provi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#8220;&#30830;&#23450;&#24615;ADVI&#8221;&#65288;DADVI&#65289;&#65292;&#23427;&#29992;&#19968;&#31181;&#22266;&#23450;&#30340;&#33945;&#29305;&#21345;&#32599;&#36817;&#20284;&#26367;&#25442;&#20102;&#22343;&#20540;&#22330;&#21464;&#20998;&#36125;&#21494;&#26031;&#65288;MFVB&#65289;&#30340;&#19981;&#21487;&#35299;&#30446;&#26631;&#65292;&#21487;&#20197;&#20351;&#29992;&#29616;&#25104;&#30340;&#20108;&#38454;&#20248;&#21270;&#65292;&#36866;&#29992;&#20110;&#26356;&#20934;&#30830;&#30340;&#21518;&#39564;&#32447;&#24615;&#21709;&#24212;&#65288;LR&#65289;&#21327;&#26041;&#24046;&#20272;&#35745;&#65292;&#22312;&#26576;&#20123;&#24120;&#35265;&#30340;&#32479;&#35745;&#38382;&#39064;&#31867;&#21035;&#19978;&#25928;&#26524;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2304.05527</link><description>&lt;p&gt;
&#19968;&#31181;&#20351;&#29992;&#30830;&#23450;&#24615;&#30446;&#26631;&#30340;&#40657;&#21283;&#23376;&#21464;&#20998;&#25512;&#26029;&#65306;&#26356;&#24555;&#65292;&#26356;&#31934;&#30830;&#65292;&#26356;&#40657;&#12290;
&lt;/p&gt;
&lt;p&gt;
Black Box Variational Inference with a Deterministic Objective: Faster, More Accurate, and Even More Black Box. (arXiv:2304.05527v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05527
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#8220;&#30830;&#23450;&#24615;ADVI&#8221;&#65288;DADVI&#65289;&#65292;&#23427;&#29992;&#19968;&#31181;&#22266;&#23450;&#30340;&#33945;&#29305;&#21345;&#32599;&#36817;&#20284;&#26367;&#25442;&#20102;&#22343;&#20540;&#22330;&#21464;&#20998;&#36125;&#21494;&#26031;&#65288;MFVB&#65289;&#30340;&#19981;&#21487;&#35299;&#30446;&#26631;&#65292;&#21487;&#20197;&#20351;&#29992;&#29616;&#25104;&#30340;&#20108;&#38454;&#20248;&#21270;&#65292;&#36866;&#29992;&#20110;&#26356;&#20934;&#30830;&#30340;&#21518;&#39564;&#32447;&#24615;&#21709;&#24212;&#65288;LR&#65289;&#21327;&#26041;&#24046;&#20272;&#35745;&#65292;&#22312;&#26576;&#20123;&#24120;&#35265;&#30340;&#32479;&#35745;&#38382;&#39064;&#31867;&#21035;&#19978;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#24494;&#20998;&#21464;&#20998;&#25512;&#26029;&#65288;ADVI&#65289;&#25552;&#20379;&#20102;&#22810;&#31181;&#29616;&#20195;&#27010;&#29575;&#32534;&#31243;&#35821;&#35328;&#20013;&#24555;&#36895;&#26131;&#29992;&#30340;&#21518;&#39564;&#36817;&#20284;&#26041;&#27861;&#12290;&#28982;&#32780;&#23427;&#30340;&#38543;&#26426;&#20248;&#21270;&#22120;&#32570;&#20047;&#26126;&#30830;&#30340;&#25910;&#25947;&#26631;&#20934;&#65292;&#24182;&#19988;&#38656;&#35201;&#35843;&#25972;&#21442;&#25968;&#12290;&#27492;&#22806;&#65292;ADVI&#32487;&#25215;&#20102;&#22343;&#20540;&#22330;&#21464;&#20998;&#36125;&#21494;&#26031;&#65288;MFVB&#65289;&#30340;&#36739;&#24046;&#21518;&#39564;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;&#30830;&#23450;&#24615;ADVI&#8221;&#65288;DADVI&#65289;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;DADVI&#29992;&#22266;&#23450;&#30340;&#33945;&#29305;&#21345;&#32599;&#36817;&#20284;&#26367;&#25442;&#20102;MFVB&#30340;&#19981;&#21487;&#35299;&#30446;&#26631;&#65292;&#36825;&#19968;&#25216;&#26415;&#22312;&#38543;&#26426;&#20248;&#21270;&#25991;&#29486;&#20013;&#34987;&#31216;&#20026;&#8220;&#26679;&#26412;&#24179;&#22343;&#36817;&#20284;&#8221;&#65288;SAA&#65289;&#12290;&#36890;&#36807;&#20248;&#21270;&#36817;&#20284;&#20294;&#30830;&#23450;&#30340;&#30446;&#26631;&#65292;DADVI&#21487;&#20197;&#20351;&#29992;&#29616;&#25104;&#30340;&#20108;&#38454;&#20248;&#21270;&#65292;&#32780;&#19988;&#19982;&#26631;&#20934;&#22343;&#20540;&#22330;ADVI&#19981;&#21516;&#30340;&#26159;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#26356;&#20934;&#30830;&#30340;&#21518;&#39564;&#32447;&#24615;&#21709;&#24212;&#65288;LR&#65289;&#21327;&#26041;&#24046;&#20272;&#35745;&#12290;&#19982;&#29616;&#26377;&#30340;&#26368;&#22351;&#24773;&#20917;&#29702;&#35770;&#30456;&#21453;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#26576;&#20123;&#24120;&#35265;&#30340;&#32479;&#35745;&#38382;&#39064;&#31867;&#21035;&#19978;&#65292;DADVI&#21644;SAA&#21487;&#20197;&#34920;&#29616;&#24471;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic differentiation variational inference (ADVI) offers fast and easy-to-use posterior approximation in multiple modern probabilistic programming languages. However, its stochastic optimizer lacks clear convergence criteria and requires tuning parameters. Moreover, ADVI inherits the poor posterior uncertainty estimates of mean-field variational Bayes (MFVB). We introduce ``deterministic ADVI'' (DADVI) to address these issues. DADVI replaces the intractable MFVB objective with a fixed Monte Carlo approximation, a technique known in the stochastic optimization literature as the ``sample average approximation'' (SAA). By optimizing an approximate but deterministic objective, DADVI can use off-the-shelf second-order optimization, and, unlike standard mean-field ADVI, is amenable to more accurate posterior linear response (LR) covariance estimates. In contrast to existing worst-case theory, we show that, on certain classes of common statistical problems, DADVI and the SAA can perform 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#20984;&#21253;&#26426;&#30340;&#26041;&#27861;&#26469;&#30830;&#20445;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#65292;&#21516;&#26102;&#20445;&#30041;&#25968;&#25454;&#32467;&#26500;&#65292;&#29992;&#20110;&#25968;&#25454;&#34920;&#31034;&#23398;&#20064;&#30340;&#20998;&#31867;&#24212;&#29992;&#20013;&#12290;&#20026;&#20102;&#30830;&#20445;&#38544;&#31169;&#20445;&#25252;&#23398;&#20064;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#34394;&#20551;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.01300</link><description>&lt;p&gt;
&#22522;&#20110;&#26680;&#20984;&#21253;&#26426;&#30340;&#24046;&#20998;&#38544;&#31169;&#23398;&#20064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Kernel Affine Hull Machines for Differentially Private Learning. (arXiv:2304.01300v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#20984;&#21253;&#26426;&#30340;&#26041;&#27861;&#26469;&#30830;&#20445;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#65292;&#21516;&#26102;&#20445;&#30041;&#25968;&#25454;&#32467;&#26500;&#65292;&#29992;&#20110;&#25968;&#25454;&#34920;&#31034;&#23398;&#20064;&#30340;&#20998;&#31867;&#24212;&#29992;&#20013;&#12290;&#20026;&#20102;&#30830;&#20445;&#38544;&#31169;&#20445;&#25252;&#23398;&#20064;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#34394;&#20551;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#36890;&#36807;&#23398;&#20064;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#28857;&#30340;&#20984;&#21253;&#26469;&#34920;&#31034;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#23558;&#25968;&#25454;&#31354;&#38388;&#21010;&#20998;&#20026;&#20960;&#20309;&#20307;&#65292;&#20174;&#32780;&#38544;&#34255;&#26377;&#20851;&#21333;&#20010;&#25968;&#25454;&#28857;&#30340;&#38544;&#31169;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#30041;&#21407;&#22987;&#23398;&#20064;&#38382;&#39064;&#30340;&#32467;&#26500;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26680;&#20984;&#21253;&#26426;&#65288;KAHM&#65289;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35745;&#31639;&#20174;&#32467;&#26524;&#26377;&#30028;&#20960;&#20309;&#20307;&#20013;&#30340;&#36317;&#31163;&#24230;&#37327;&#12290;KAHM&#26159;&#24191;&#27867;&#21644;&#28145;&#20837;&#30340;&#33258;&#32534;&#30721;&#22120;&#30340;&#20851;&#38190;&#26500;&#24314;&#22359;&#65292;&#23427;&#20204;&#20351;&#25968;&#25454;&#34920;&#31034;&#23398;&#20064;&#29992;&#20110;&#20998;&#31867;&#24212;&#29992;&#12290;&#20026;&#20102;&#30830;&#20445;&#38544;&#31169;&#20445;&#25252;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#34394;&#20551;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#28041;&#21450;&#23558;&#24046;&#20998;&#38544;&#31169;&#25968;&#25454;&#26679;&#26412;&#36890;&#36807;&#36716;&#25442;&#36807;&#31243;&#36827;&#34892;&#24179;&#28369;&#22788;&#29702;&#12290;&#29983;&#25104;&#30340;&#34394;&#20551;&#25968;&#25454;&#19981;&#20165;&#20445;&#35777;&#24046;&#20998;&#38544;&#31169;&#65292;&#32780;&#19988;&#30830;&#20445;KAHM&#24314;&#27169;&#35823;&#24046;&#19981;&#22823;&#20110;&#21407;&#22987;&#25968;&#25454;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the use of affine hulls of points as a means of representing data via learning in Reproducing Kernel Hilbert Spaces (RKHS), with the goal of partitioning the data space into geometric bodies that conceal privacy-sensitive information about individual data points, while preserving the structure of the original learning problem. To this end, we introduce the Kernel Affine Hull Machine (KAHM), which provides an effective way of computing a distance measure from the resulting bounded geometric body. KAHM is a critical building block in wide and deep autoencoders, which enable data representation learning for classification applications. To ensure privacy-preserving learning, we propose a novel method for generating fabricated data, which involves smoothing differentially private data samples through a transformation process. The resulting fabricated data guarantees not only differential privacy but also ensures that the KAHM modeling error is not larger than that of the
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#35757;&#32451;&#20855;&#26377;ReLU&#21644;&#32447;&#24615;&#38408;&#20540;&#28608;&#27963;&#20989;&#25968;&#30340;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#22266;&#23450;&#32500;&#24230;&#19979;&#30340;NP&#38590;&#24230;&#12290; &#22238;&#31572;&#20102;&#20004;&#20010;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#36825;&#20004;&#20010;&#38382;&#39064;&#22312;&#20108;&#32500;&#24773;&#20917;&#19979;&#26159;NP&#38590;&#30340;&#65292;&#27492;&#22806;&#22312;ReLU&#26696;&#20363;&#20013;&#35777;&#26126;&#20102;&#22266;&#23450;&#21442;&#25968;&#38382;&#39064;&#30340;&#21442;&#25968;&#21270;&#22266;&#23450;&#22797;&#26434;&#24230;&#32500;&#25968;&#21644;ReLU&#25968;&#37327;&#30340;&#32452;&#21512;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2303.17045</link><description>&lt;p&gt;
&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#22312;&#22266;&#23450;&#32500;&#24230;&#19978;&#26159;NP&#38590;&#30340;
&lt;/p&gt;
&lt;p&gt;
Training Neural Networks is NP-Hard in Fixed Dimension. (arXiv:2303.17045v1 [cs.CC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17045
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#35757;&#32451;&#20855;&#26377;ReLU&#21644;&#32447;&#24615;&#38408;&#20540;&#28608;&#27963;&#20989;&#25968;&#30340;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#22266;&#23450;&#32500;&#24230;&#19979;&#30340;NP&#38590;&#24230;&#12290; &#22238;&#31572;&#20102;&#20004;&#20010;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#36825;&#20004;&#20010;&#38382;&#39064;&#22312;&#20108;&#32500;&#24773;&#20917;&#19979;&#26159;NP&#38590;&#30340;&#65292;&#27492;&#22806;&#22312;ReLU&#26696;&#20363;&#20013;&#35777;&#26126;&#20102;&#22266;&#23450;&#21442;&#25968;&#38382;&#39064;&#30340;&#21442;&#25968;&#21270;&#22266;&#23450;&#22797;&#26434;&#24230;&#32500;&#25968;&#21644;ReLU&#25968;&#37327;&#30340;&#32452;&#21512;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#36755;&#20837;&#25968;&#25454;&#32500;&#24230;&#21644;&#38544;&#34255;&#31070;&#32463;&#20803;&#25968;&#37327;&#26041;&#38754;&#23545;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21442;&#25968;&#21270;&#22797;&#26434;&#24615;&#30340;&#30740;&#31350;&#65292;&#32771;&#34385;ReLU&#21644;&#32447;&#24615;&#38408;&#20540;&#28608;&#27963;&#20989;&#25968;&#12290;&#23613;&#31649;&#36825;&#20123;&#38382;&#39064;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#36817;&#24180;&#26469;&#24050;&#32463;&#34987;&#22810;&#27425;&#30740;&#31350;&#65292;&#20294;&#20173;&#26377;&#20960;&#20010;&#38382;&#39064;&#23578;&#26410;&#35299;&#20915;&#12290;&#25105;&#20204;&#22238;&#31572;&#20102;Arora et al. [ICLR '18]&#21644;Khalife&#21644;Basu [IPCO '22]&#30340;&#38382;&#39064;&#65292;&#26174;&#31034;&#20004;&#20010;&#38382;&#39064;&#22312;&#20108;&#32500;&#24773;&#20917;&#19979;&#37117;&#26159;NP&#38590;&#30340;&#65292;&#36825;&#25490;&#38500;&#20102;&#20219;&#20309;&#24120;&#25968;&#32500;&#24230;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#12290;&#25105;&#20204;&#36824;&#22238;&#31572;&#20102;Froese&#31561;&#20154;[JAIR '22]&#30340;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#20855;&#26377;&#38646;&#22521;&#35757;&#35823;&#24046;&#30340;&#22235;&#20010;ReLU(&#25110;&#20004;&#20010;&#32447;&#24615;&#38408;&#20540;&#31070;&#32463;&#20803;)&#30340;W [1]-hardness&#12290;&#26368;&#21518;&#65292;&#22312;ReLU&#26696;&#20363;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21442;&#25968;&#21270;&#22266;&#23450;&#22797;&#26434;&#24230;&#32500;&#25968;&#21644;ReLU&#25968;&#37327;&#30340;&#32452;&#21512;&#21442;&#25968;&#65292;&#22914;&#26524;&#32593;&#32476;&#34987;&#20551;&#23450;&#20026;&#35745;&#31639;&#20984;&#26144;&#23556;&#65292;&#21017;&#21487;&#29992;&#20110;&#22266;&#23450;&#21442;&#25968;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20960;&#20046;&#23436;&#20840;&#35299;&#20915;&#20102;&#36825;&#20123;&#21442;&#25968;&#30340;&#22797;&#26434;&#24615;&#29366;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the parameterized complexity of training two-layer neural networks with respect to the dimension of the input data and the number of hidden neurons, considering ReLU and linear threshold activation functions. Albeit the computational complexity of these problems has been studied numerous times in recent years, several questions are still open. We answer questions by Arora et al. [ICLR '18] and Khalife and Basu [IPCO '22] showing that both problems are NP-hard for two dimensions, which excludes any polynomial-time algorithm for constant dimension. We also answer a question by Froese et al. [JAIR '22] proving W[1]-hardness for four ReLUs (or two linear threshold neurons) with zero training error. Finally, in the ReLU case, we show fixed-parameter tractability for the combined parameter number of dimensions and number of ReLUs if the network is assumed to compute a convex map. Our results settle the complexity status regarding these parameters almost completely.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#35745;&#23398;&#20064;&#20013;&#30340;&#35843;&#25972;Wasserstein&#20998;&#24067;&#40065;&#26834;&#20272;&#35745;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#39640;&#20272;&#35745;&#30340;&#32479;&#35745;&#24615;&#33021;&#65292;&#20445;&#25345;&#26679;&#26412;&#22806;&#24615;&#33021;&#20445;&#35777;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.15579</link><description>&lt;p&gt;
&#32479;&#35745;&#23398;&#20064;&#20013;&#30340;&#35843;&#25972;Wasserstein&#20998;&#24067;&#40065;&#26834;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Adjusted Wasserstein Distributionally Robust Estimator in Statistical Learning. (arXiv:2303.15579v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15579
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#35745;&#23398;&#20064;&#20013;&#30340;&#35843;&#25972;Wasserstein&#20998;&#24067;&#40065;&#26834;&#20272;&#35745;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#39640;&#20272;&#35745;&#30340;&#32479;&#35745;&#24615;&#33021;&#65292;&#20445;&#25345;&#26679;&#26412;&#22806;&#24615;&#33021;&#20445;&#35777;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#32479;&#35745;&#23398;&#20064;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#35843;&#25972;&#30340;Wasserstein&#20998;&#24067;&#40065;&#26834;&#20272;&#35745;&#8212;&#8212;&#22522;&#20110;Wasserstein&#20998;&#24067;&#40065;&#26834;&#20272;&#35745;&#65288;WDRO&#65289;&#30340;&#38750;&#32447;&#24615;&#36716;&#25442;&#12290;&#36825;&#31181;&#36716;&#25442;&#23558;&#25552;&#39640;WDRO&#30340;&#32479;&#35745;&#24615;&#33021;&#65292;&#22240;&#20026;&#35843;&#25972;&#21518;&#30340;WDRO&#20272;&#35745;&#22120;&#28176;&#36827;&#26080;&#20559;&#24182;&#19988;&#22343;&#26041;&#35823;&#24046;&#36235;&#36817;&#20110;&#38646;&#12290;&#35843;&#25972;&#21518;&#30340;WDRO&#19981;&#20250;&#21066;&#24369;WDRO&#30340;&#26679;&#26412;&#22806;&#24615;&#33021;&#20445;&#35777;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#35843;&#25972;WDRO&#20272;&#35745;&#22120;&#30340;&#23384;&#22312;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#32473;&#20986;&#20102;&#35745;&#31639;&#35843;&#25972;WDRO&#20272;&#35745;&#22120;&#30340;&#36807;&#31243;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#23637;&#31034;&#22914;&#20309;&#22312;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#20013;&#24320;&#21457;&#35843;&#25972;WDRO&#20272;&#35745;&#22120;&#12290;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#35843;&#25972;&#21518;&#30340;&#20272;&#35745;&#22120;&#27604;&#32463;&#20856;&#20272;&#35745;&#22120;&#20855;&#26377;&#26356;&#22909;&#30340;&#23454;&#38469;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an adjusted Wasserstein distributionally robust estimator -- based on a nonlinear transformation of the Wasserstein distributionally robust (WDRO) estimator in statistical learning. This transformation will improve the statistical performance of WDRO because the adjusted WDRO estimator is asymptotically unbiased and has an asymptotically smaller mean squared error. The adjusted WDRO will not mitigate the out-of-sample performance guarantee of WDRO. Sufficient conditions for the existence of the adjusted WDRO estimator are presented, and the procedure for the computation of the adjusted WDRO estimator is given. Specifically, we will show how the adjusted WDRO estimator is developed in the generalized linear model. Numerical experiments demonstrate the favorable practical performance of the adjusted estimator over the classic one.
&lt;/p&gt;</description></item><item><title>&#20171;&#35266;&#38598;&#20307;&#36816;&#21160;&#30340;&#38543;&#26426;&#29305;&#24449;&#23545;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#24615;&#21160;&#21147;&#23398;&#24314;&#27169;&#33267;&#20851;&#37325;&#35201;&#12290;&#20316;&#32773;&#21033;&#29992;&#29289;&#29702;&#21551;&#21457;&#12289;&#31070;&#32463;&#32593;&#32476;&#21644;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#26469;&#30740;&#31350;&#30456;&#20114;&#20316;&#29992;&#20010;&#20307;&#30340;&#32676;&#20307;&#21160;&#21147;&#23398;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#37492;&#23450;&#21644;&#20998;&#26512;&#65292;&#20026;&#36825;&#20123;&#31995;&#32479;&#30340;&#31209;&#24207;&#24615;&#36136;&#25552;&#20379;&#20102;&#26032;&#39062;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2303.09906</link><description>&lt;p&gt;
&#29992;&#31070;&#32463;&#38543;&#26426;&#24314;&#27169;&#25506;&#32034;&#38598;&#20307;&#36816;&#21160;&#30340;&#20171;&#35266;&#25551;&#36848;
&lt;/p&gt;
&lt;p&gt;
Discovering mesoscopic descriptions of collective movement with neural stochastic modelling. (arXiv:2303.09906v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09906
&lt;/p&gt;
&lt;p&gt;
&#20171;&#35266;&#38598;&#20307;&#36816;&#21160;&#30340;&#38543;&#26426;&#29305;&#24449;&#23545;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#24615;&#21160;&#21147;&#23398;&#24314;&#27169;&#33267;&#20851;&#37325;&#35201;&#12290;&#20316;&#32773;&#21033;&#29992;&#29289;&#29702;&#21551;&#21457;&#12289;&#31070;&#32463;&#32593;&#32476;&#21644;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#26469;&#30740;&#31350;&#30456;&#20114;&#20316;&#29992;&#20010;&#20307;&#30340;&#32676;&#20307;&#21160;&#21147;&#23398;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#37492;&#23450;&#21644;&#20998;&#26512;&#65292;&#20026;&#36825;&#20123;&#31995;&#32479;&#30340;&#31209;&#24207;&#24615;&#36136;&#25552;&#20379;&#20102;&#26032;&#39062;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#20307;&#36816;&#21160;&#26159;&#33258;&#28982;&#30028;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#29616;&#35937;&#65292;&#21551;&#21457;&#24037;&#31243;&#24072;&#12289;&#29289;&#29702;&#23398;&#23478;&#21644;&#25968;&#23398;&#23478;&#24320;&#21457;&#25968;&#23398;&#27169;&#22411;&#21644;&#29983;&#29289;&#21551;&#21457;&#35774;&#35745;&#12290;&#23567;&#33267;&#20013;&#31561;&#32676;&#20307;&#35268;&#27169;&#65288;&#32422;10-1000&#20010;&#20010;&#20307;&#65292;&#20063;&#31216;&#8220;&#20171;&#35266;&#23610;&#24230;&#8221;&#30340;&#38598;&#20307;&#36816;&#21160;&#65292;&#30001;&#20110;&#38543;&#26426;&#24615;&#32780;&#26174;&#31034;&#20986;&#38750;&#24179;&#20961;&#30340;&#29305;&#24449;&#12290;&#22240;&#27492;&#65292;&#22312;&#20171;&#35266;&#23610;&#24230;&#38598;&#20307;&#29616;&#35937;&#30340;&#30740;&#31350;&#20013;&#65292;&#34920;&#24449;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#21160;&#21147;&#23398;&#26041;&#38754;&#30340;&#29305;&#24449;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#25105;&#20204;&#22522;&#20110;&#29289;&#29702;&#21551;&#21457;&#65292;&#37319;&#29992;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#26469;&#34920;&#24449;&#30456;&#20114;&#20316;&#29992;&#20010;&#20307;&#30340;&#38543;&#26426;&#32676;&#20307;&#21160;&#21147;&#23398;&#65292;&#36890;&#36807;&#25511;&#21046;&#32676;&#20307;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDE&#65289;&#26469;&#30740;&#31350;&#36825;&#20123;&#31995;&#32479;&#30340;&#32676;&#20307;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#24212;&#29992;&#36825;&#31181;&#25216;&#26415;&#65292;&#21033;&#29992;&#28418;&#31227;&#21644;&#25193;&#25955;&#22330;&#37492;&#23450;&#31995;&#32479;&#30340;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#24615;&#26041;&#38754;&#30340;&#21160;&#21147;&#23398;&#65292;&#20174;&#32780;&#20351;&#25105;&#20204;&#33021;&#22815;&#38024;&#23545;&#36825;&#20123;&#31995;&#32479;&#30340;&#31209;&#24207;&#24615;&#36136;&#20570;&#20986;&#26032;&#39062;&#30340;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collective motion is an ubiquitous phenomenon in nature, inspiring engineers, physicists and mathematicians to develop mathematical models and bio-inspired designs. Collective motion at small to medium group sizes ($\sim$10-1000 individuals, also called the `mesoscale'), can show nontrivial features due to stochasticity. Therefore, characterizing both the deterministic and stochastic aspects of the dynamics is crucial in the study of mesoscale collective phenomena. Here, we use a physics-inspired, neural-network based approach to characterize the stochastic group dynamics of interacting individuals, through a stochastic differential equation (SDE) that governs the collective dynamics of the group. We apply this technique on both synthetic and real-world datasets, and identify the deterministic and stochastic aspects of the dynamics using drift and diffusion fields, enabling us to make novel inferences about the nature of order in these systems.
&lt;/p&gt;</description></item><item><title>ESD&#26159;&#19968;&#31181;&#26080;&#38656;&#35843;&#21442;&#30340;&#21487;&#35757;&#32451;&#26657;&#20934;&#30446;&#26631;&#25439;&#22833;&#65292;&#36890;&#36807;&#23558;&#26657;&#20934;&#35823;&#24046;&#30475;&#20316;&#20004;&#20010;&#26399;&#26395;&#20540;&#20043;&#38388;&#30340;&#24179;&#26041;&#24046;&#65292;&#21487;&#20197;&#25913;&#21892;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#26657;&#20934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.02472</link><description>&lt;p&gt;
ESD:&#39044;&#26399;&#24179;&#26041;&#24046;&#20316;&#20026;&#19968;&#31181;&#26080;&#38656;&#35843;&#21442;&#30340;&#21487;&#35757;&#32451;&#26657;&#20934;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
ESD: Expected Squared Difference as a Tuning-Free Trainable Calibration Measure. (arXiv:2303.02472v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02472
&lt;/p&gt;
&lt;p&gt;
ESD&#26159;&#19968;&#31181;&#26080;&#38656;&#35843;&#21442;&#30340;&#21487;&#35757;&#32451;&#26657;&#20934;&#30446;&#26631;&#25439;&#22833;&#65292;&#36890;&#36807;&#23558;&#26657;&#20934;&#35823;&#24046;&#30475;&#20316;&#20004;&#20010;&#26399;&#26395;&#20540;&#20043;&#38388;&#30340;&#24179;&#26041;&#24046;&#65292;&#21487;&#20197;&#25913;&#21892;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#26657;&#20934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#30001;&#20110;&#36807;&#20110;&#33258;&#20449;&#30340;&#39044;&#27979;&#32780;&#24448;&#24448;&#26657;&#20934;&#19981;&#33391;&#12290;&#20256;&#32479;&#19978;&#65292;&#22312;&#35757;&#32451;&#20043;&#21518;&#20351;&#29992;&#21518;&#22788;&#29702;&#26041;&#27861;&#26469;&#26657;&#20934;&#27169;&#22411;&#12290;&#36817;&#24180;&#26469;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#21487;&#35757;&#32451;&#30340;&#26657;&#20934;&#24230;&#37327;&#26469;&#30452;&#25509;&#23558;&#20854;&#32435;&#20837;&#35757;&#32451;&#36807;&#31243;&#20013;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#37117;&#21253;&#21547;&#20869;&#37096;&#36229;&#21442;&#25968;&#65292;&#24182;&#19988;&#36825;&#20123;&#26657;&#20934;&#30446;&#26631;&#30340;&#24615;&#33021;&#20381;&#36182;&#20110;&#35843;&#25972;&#36825;&#20123;&#36229;&#21442;&#25968;&#65292;&#38543;&#30528;&#31070;&#32463;&#32593;&#32476;&#21644;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#22686;&#22823;&#65292;&#20250;&#20135;&#29983;&#26356;&#22810;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39044;&#26399;&#24179;&#26041;&#24046;&#65288;ESD&#65289;&#65292;&#19968;&#31181;&#26080;&#38656;&#35843;&#21442;&#30340;&#21487;&#35757;&#32451;&#26657;&#20934;&#30446;&#26631;&#25439;&#22833;&#65292;&#25105;&#20204;&#20174;&#20004;&#20010;&#26399;&#26395;&#20540;&#20043;&#38388;&#30340;&#24179;&#26041;&#24046;&#30340;&#35282;&#24230;&#26469;&#30475;&#26657;&#20934;&#35823;&#24046;&#12290;&#36890;&#36807;&#23545;&#20960;&#31181;&#26550;&#26500;&#65288;CNN&#12289;Transformer&#65289;&#21644;&#25968;&#25454;&#38598;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#23558;ESD&#32435;&#20837;&#35757;&#32451;&#21487;&#20197;&#25913;&#21892;&#27169;&#22411;&#30340;&#26657;&#20934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Studies have shown that modern neural networks tend to be poorly calibrated due to over-confident predictions. Traditionally, post-processing methods have been used to calibrate the model after training. In recent years, various trainable calibration measures have been proposed to incorporate them directly into the training process. However, these methods all incorporate internal hyperparameters, and the performance of these calibration objectives relies on tuning these hyperparameters, incurring more computational costs as the size of neural networks and datasets become larger. As such, we present Expected Squared Difference (ESD), a tuning-free (i.e., hyperparameter-free) trainable calibration objective loss, where we view the calibration error from the perspective of the squared difference between the two expectations. With extensive experiments on several architectures (CNNs, Transformers) and datasets, we demonstrate that (1) incorporating ESD into the training improves model cali
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#23618;&#27425;&#21270;&#21487;&#35270;&#21270;&#24037;&#20855;&#21644;&#22810;&#21464;&#37327;&#26368;&#20248;&#37325;&#24314;&#26641;&#65292;&#29992;&#20110;&#25552;&#39640;&#26641;&#38598;&#25104;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#27934;&#23519;&#21147;&#12290;</title><link>http://arxiv.org/abs/2302.07580</link><description>&lt;p&gt;
&#25171;&#24320;&#26641;&#38598;&#25104;&#30340;&#21487;&#35299;&#37322;&#24615;&#65306;&#19968;&#31181;&#23618;&#27425;&#21270;&#21487;&#35270;&#21270;&#24037;&#20855;&#21644;&#22810;&#21464;&#37327;&#26368;&#20248;&#37325;&#24314;&#26641;
&lt;/p&gt;
&lt;p&gt;
Unboxing Tree Ensembles for interpretability: a hierarchical visualization tool and a multivariate optimal re-built tree. (arXiv:2302.07580v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07580
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#23618;&#27425;&#21270;&#21487;&#35270;&#21270;&#24037;&#20855;&#21644;&#22810;&#21464;&#37327;&#26368;&#20248;&#37325;&#24314;&#26641;&#65292;&#29992;&#20110;&#25552;&#39640;&#26641;&#38598;&#25104;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#27934;&#23519;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#31639;&#27861;&#20915;&#31574;&#23545;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#30340;&#22686;&#38271;&#24433;&#21709;&#65292;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#24050;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#26641;&#38598;&#25104;&#26041;&#27861;&#65288;&#22914;&#38543;&#26426;&#26862;&#26519;&#25110;XgBoost&#65289;&#26159;&#29992;&#20110;&#20998;&#31867;&#20219;&#21153;&#30340;&#24378;&#22823;&#23398;&#20064;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#23558;&#22810;&#20010;&#26641;&#32452;&#21512;&#36215;&#26469;&#21487;&#33021;&#20250;&#25552;&#20379;&#27604;&#21333;&#20010;&#26641;&#26356;&#39640;&#30340;&#39044;&#27979;&#36136;&#37327;&#65292;&#20294;&#23427;&#29306;&#29298;&#20102;&#21487;&#35299;&#37322;&#24615;&#30340;&#29305;&#24615;&#65292;&#23548;&#33268;&#20102;&#8220;&#40657;&#30418;&#8221;&#27169;&#22411;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#24320;&#21457;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#26641;&#38598;&#25104;&#27169;&#22411;&#34920;&#31034;&#65292;&#21487;&#20197;&#20026;&#20854;&#34892;&#20026;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;&#39318;&#20808;&#65292;&#38024;&#23545;&#30446;&#26631;&#26641;&#38598;&#25104;&#27169;&#22411;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#28909;&#22270;&#34920;&#31034;&#30340;&#23618;&#27425;&#21270;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#32771;&#34385;&#21040;&#29305;&#24449;&#30340;&#39057;&#29575;&#21644;&#36873;&#25321;&#29305;&#24449;&#30340;&#23618;&#32423;&#20316;&#20026;&#37325;&#35201;&#24615;&#30340;&#25351;&#26631;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#65288;MILP&#65289;&#30340;&#24418;&#24335;&#65292;&#29992;&#20110;&#26500;&#36896;&#19968;&#20010;&#20934;&#30830;&#27169;&#25311;&#30446;&#26631;&#27169;&#22411;&#30340;&#21333;&#20010;&#26368;&#20248;&#22810;&#21464;&#37327;&#26641;&#12290;
&lt;/p&gt;
&lt;p&gt;
The interpretability of models has become a crucial issue in Machine Learning because of algorithmic decisions' growing impact on real-world applications. Tree ensemble methods, such as Random Forests or XgBoost, are powerful learning tools for classification tasks. However, while combining multiple trees may provide higher prediction quality than a single one, it sacrifices the interpretability property resulting in "black-box" models. In light of this, we aim to develop an interpretable representation of a tree-ensemble model that can provide valuable insights into its behavior. First, given a target tree-ensemble model, we develop a hierarchical visualization tool based on a heatmap representation of the forest's feature use, considering the frequency of a feature and the level at which it is selected as an indicator of importance. Next, we propose a mixed-integer linear programming (MILP) formulation for constructing a single optimal multivariate tree that accurately mimics the tar
&lt;/p&gt;</description></item><item><title>SGID&#26159;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#35821;&#20041;&#24341;&#23548;&#29983;&#25104;&#22270;&#20687;&#22686;&#24378;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#24179;&#34913;&#22270;&#20687;&#22810;&#26679;&#24615;&#21644;&#35821;&#20041;&#19968;&#33268;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;SGID&#22312;ResNet-5&#19978;&#30340;&#25928;&#26524;&#20248;&#20110;&#26368;&#20339;&#22686;&#24378;&#22522;&#32447;1.72&#65285;&#12290;</title><link>http://arxiv.org/abs/2302.02070</link><description>&lt;p&gt;
&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#35821;&#20041;&#24341;&#23548;&#29983;&#25104;&#22270;&#20687;&#22686;&#24378;&#26041;&#27861;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Semantic-Guided Generative Image Augmentation Method with Diffusion Models for Image Classification. (arXiv:2302.02070v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02070
&lt;/p&gt;
&lt;p&gt;
SGID&#26159;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#35821;&#20041;&#24341;&#23548;&#29983;&#25104;&#22270;&#20687;&#22686;&#24378;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#24179;&#34913;&#22270;&#20687;&#22810;&#26679;&#24615;&#21644;&#35821;&#20041;&#19968;&#33268;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;SGID&#22312;ResNet-5&#19978;&#30340;&#25928;&#26524;&#20248;&#20110;&#26368;&#20339;&#22686;&#24378;&#22522;&#32447;1.72&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22270;&#20687;&#22686;&#24378;&#26041;&#27861;&#20998;&#20026;&#20004;&#31867;&#65306;&#25200;&#21160;&#26041;&#27861;&#21644;&#29983;&#25104;&#26041;&#27861;&#12290;&#25200;&#21160;&#26041;&#27861;&#23545;&#21407;&#22987;&#22270;&#20687;&#24212;&#29992;&#39044;&#23450;&#20041;&#30340;&#25200;&#21160;&#26469;&#22686;&#24378;&#22270;&#20687;&#65292;&#20294;&#21482;&#26159;&#23616;&#37096;&#21464;&#21270;&#22270;&#20687;&#65292;&#22240;&#27492;&#32570;&#20047;&#22270;&#20687;&#22810;&#26679;&#24615;&#12290;&#30456;&#21453;&#65292;&#29983;&#25104;&#26041;&#27861;&#22312;&#22686;&#24378;&#22270;&#20687;&#20013;&#24102;&#26469;&#26356;&#22810;&#30340;&#22270;&#20687;&#22810;&#26679;&#24615;&#65292;&#20294;&#21487;&#33021;&#26080;&#27861;&#20445;&#25345;&#35821;&#20041;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#38169;&#35823;&#22320;&#25913;&#21464;&#20102;&#21407;&#22987;&#22270;&#20687;&#30340;&#22522;&#26412;&#35821;&#20041;&#12290;&#20026;&#20102;&#22312;&#22686;&#24378;&#22270;&#20687;&#20013;&#24179;&#34913;&#22270;&#20687;&#22810;&#26679;&#24615;&#21644;&#35821;&#20041;&#19968;&#33268;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SGID&#65292;&#19968;&#31181;&#35821;&#20041;&#24341;&#23548;&#29983;&#25104;&#22270;&#20687;&#22686;&#24378;&#26041;&#27861;&#65292;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;SGID&#37319;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#20855;&#26377;&#33391;&#22909;&#22270;&#20687;&#22810;&#26679;&#24615;&#30340;&#22686;&#24378;&#22270;&#20687;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;SGID&#20197;&#22270;&#20687;&#26631;&#31614;&#21644;&#26631;&#39064;&#20316;&#20026;&#25351;&#23548;&#65292;&#20197;&#20445;&#25345;&#22686;&#24378;&#21644;&#21407;&#22987;&#22270;&#20687;&#20043;&#38388;&#30340;&#35821;&#20041;&#19968;&#33268;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;SGID&#22312;ResNet-5&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#26368;&#20339;&#22686;&#24378;&#22522;&#32447;1.72&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing image augmentation methods consist of two categories: perturbation-based methods and generative methods. Perturbation-based methods apply pre-defined perturbations to augment an original image, but only locally vary the image, thus lacking image diversity. In contrast, generative methods bring more image diversity in the augmented images but may not preserve semantic consistency, thus incorrectly changing the essential semantics of the original image. To balance image diversity and semantic consistency in augmented images, we propose SGID, a Semantic-guided Generative Image augmentation method with Diffusion models for image classification. Specifically, SGID employs diffusion models to generate augmented images with good image diversity. More importantly, SGID takes image labels and captions as guidance to maintain semantic consistency between the augmented and original images. Experimental results show that SGID outperforms the best augmentation baseline by 1.72% on ResNet-5
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#33021;&#37327;&#27010;&#29575;&#27169;&#22411;&#65292;&#29992;&#20110;&#25551;&#36848;&#39640;&#33021;&#29289;&#29702;&#20107;&#20214;&#65292;&#21487;&#29992;&#20110;&#21442;&#25968;&#21270;&#30340;&#20107;&#20214;&#29983;&#25104;&#65292;&#24322;&#24120;&#20449;&#21495;&#25506;&#27979;&#20197;&#21450;&#31890;&#23376;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2302.00695</link><description>&lt;p&gt;
&#22810;&#21151;&#33021;&#33021;&#37327;&#27010;&#29575;&#27169;&#22411;&#22312;&#39640;&#33021;&#29289;&#29702;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Versatile Energy-Based Probabilistic Models for High Energy Physics. (arXiv:2302.00695v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#33021;&#37327;&#27010;&#29575;&#27169;&#22411;&#65292;&#29992;&#20110;&#25551;&#36848;&#39640;&#33021;&#29289;&#29702;&#20107;&#20214;&#65292;&#21487;&#29992;&#20110;&#21442;&#25968;&#21270;&#30340;&#20107;&#20214;&#29983;&#25104;&#65292;&#24322;&#24120;&#20449;&#21495;&#25506;&#27979;&#20197;&#21450;&#31890;&#23376;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#19968;&#31181;&#32463;&#20856;&#30340;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#65292;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#20855;&#26377;&#33021;&#37327;&#20989;&#25968;&#24418;&#24335;&#28789;&#27963;&#24615;&#30340;&#22825;&#28982;&#20248;&#21183;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#24314;&#27169;&#39640;&#32500;&#25968;&#25454;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#19982;&#36825;&#20123;&#36827;&#23637;&#19968;&#33268;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#22810;&#21151;&#33021;&#33021;&#37327;&#27010;&#29575;&#27169;&#22411;&#65292;&#29992;&#20110;&#25551;&#36848;&#26469;&#33258;&#22823;&#22411;&#24378;&#23376;&#23545;&#25758;&#26426;&#30340;&#39640;&#33021;&#29289;&#29702;&#20107;&#20214;&#12290;&#35813;&#26694;&#26550;&#22522;&#20110;&#19968;&#20010;&#24378;&#22823;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#25551;&#36848;&#20102;&#26356;&#39640;&#38454;&#30340;&#31890;&#23376;&#38388;&#30456;&#20114;&#20316;&#29992;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#32534;&#30721;&#20307;&#31995;&#32467;&#26500;&#21644;&#38544;&#24335;&#29983;&#25104;&#12290;&#22312;&#24212;&#29992;&#26041;&#38754;&#65292;&#23427;&#21487;&#20197;&#20316;&#20026;&#24378;&#22823;&#30340;&#21442;&#25968;&#21270;&#20107;&#20214;&#29983;&#25104;&#22120;&#29992;&#20110;&#29289;&#29702;&#20223;&#30495;&#65292;&#19968;&#31181;&#27867;&#29992;&#30340;&#26080;&#20551;&#35774;&#20851;&#32852;&#30340;&#24322;&#24120;&#20449;&#21495;&#25506;&#27979;&#22120;&#65292;&#20197;&#21450;&#29992;&#20110;&#31890;&#23376;&#35782;&#21035;&#30340;&#22686;&#24378;&#20107;&#20214;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a classical generative modeling approach, energy-based models have the natural advantage of flexibility in the form of the energy function. Recently, energy-based models have achieved great success in modeling high-dimensional data in computer vision and natural language processing. In line with these advancements, we build a multi-purpose energy-based probabilistic model for High Energy Physics events at the Large Hadron Collider. This framework builds on a powerful generative model and describes higher-order inter-particle interactions.It suits different encoding architectures and builds on implicit generation. As for applicational aspects, it can serve as a powerful parameterized event generator for physics simulation, a generic anomalous signal detector free from spurious correlations, and an augmented event classifier for particle identification.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22686;&#21152;&#20559;&#24046;&#32780;&#19981;&#26159;&#26435;&#37325;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#20449;&#24687;&#27969;&#30340;&#26367;&#20195;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2301.00924</link><description>&lt;p&gt;
&#22686;&#21152;&#20559;&#24046;&#21487;&#20197;&#27604;&#22686;&#21152;&#26435;&#37325;&#26356;&#26377;&#25928;
&lt;/p&gt;
&lt;p&gt;
Increasing biases can be more efficient than increasing weights. (arXiv:2301.00924v3 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00924
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22686;&#21152;&#20559;&#24046;&#32780;&#19981;&#26159;&#26435;&#37325;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#20449;&#24687;&#27969;&#30340;&#26367;&#20195;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#35745;&#31639;&#21333;&#20803;&#65292;&#20855;&#26377;&#22810;&#20010;&#20559;&#24046;&#65292;&#25361;&#25112;&#20102;&#20256;&#32479;&#30340;&#24863;&#30693;&#22120;&#32467;&#26500;&#12290;&#36825;&#20010;&#21333;&#20803;&#27880;&#37325;&#20445;&#30041;&#26410;&#32463;&#25439;&#22351;&#30340;&#20449;&#24687;&#65292;&#23558;&#20854;&#20174;&#19968;&#20010;&#21333;&#20803;&#20256;&#36882;&#32473;&#19979;&#19968;&#20010;&#21333;&#20803;&#65292;&#22312;&#36807;&#31243;&#30340;&#21518;&#26399;&#24212;&#29992;&#19987;&#38376;&#20026;&#27599;&#20010;&#21333;&#20803;&#35774;&#35745;&#30340;&#20559;&#24046;&#30340;&#28608;&#27963;&#20989;&#25968;&#12290;&#36890;&#36807;&#32463;&#39564;&#21644;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#24615;&#33021;&#20013;&#65292;&#36890;&#36807;&#20851;&#27880;&#22686;&#21152;&#20559;&#24046;&#32780;&#19981;&#26159;&#26435;&#37325;&#65292;&#23384;&#22312;&#26174;&#33879;&#25552;&#21319;&#30340;&#28508;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#20449;&#24687;&#27969;&#30340;&#26367;&#20195;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a novel computational unit for neural networks that features multiple biases, challenging the traditional perceptron structure. This unit emphasizes the importance of preserving uncorrupted information as it is passed from one unit to the next, applying activation functions later in the process with specialized biases for each unit. Through both empirical and theoretical analyses, we show that by focusing on increasing biases rather than weights, there is potential for significant enhancement in a neural network model's performance. This approach offers an alternative perspective on optimizing information flow within neural networks. See source code at https://github.com/CuriosAI/dac-dev.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38548;&#31163;&#20998;&#24067;&#26680;&#30340;CID&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#25968;&#25454;&#27969;&#20013;&#39640;&#25928;&#35782;&#21035;&#21508;&#31181;&#31867;&#22411;&#30340;&#21464;&#28857;&#24182;&#23481;&#24525;&#24322;&#24120;&#20540;&#12290;</title><link>http://arxiv.org/abs/2212.14630</link><description>&lt;p&gt;
&#20351;&#29992;&#38548;&#31163;&#20998;&#24067;&#26680;&#26816;&#27979;&#21464;&#21270;&#38388;&#38548;
&lt;/p&gt;
&lt;p&gt;
Detecting Change Intervals with Isolation Distributional Kernel. (arXiv:2212.14630v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.14630
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38548;&#31163;&#20998;&#24067;&#26680;&#30340;CID&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#25968;&#25454;&#27969;&#20013;&#39640;&#25928;&#35782;&#21035;&#21508;&#31181;&#31867;&#22411;&#30340;&#21464;&#28857;&#24182;&#23481;&#24525;&#24322;&#24120;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27969;&#25968;&#25454;&#20998;&#26512;&#20013;&#65292;&#26816;&#27979;&#25968;&#25454;&#20998;&#24067;&#30340;&#31361;&#21464;&#26159;&#26368;&#37325;&#35201;&#30340;&#20219;&#21153;&#20043;&#19968;&#12290;&#23613;&#31649;&#26368;&#36817;&#25552;&#20986;&#20102;&#35768;&#22810;&#26080;&#30417;&#30563;&#30340;&#21464;&#28857;&#26816;&#27979;&#65288;CPD&#65289;&#26041;&#27861;&#26469;&#35782;&#21035;&#36825;&#20123;&#21464;&#21270;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#23384;&#22312;&#23545;&#24494;&#23567;&#21464;&#21270;&#30340;&#36951;&#28431;&#12289;&#21487;&#20280;&#32553;&#24615;&#24046;&#25110;/&#21644;&#23545;&#24322;&#24120;&#20540;&#30340;&#25935;&#24863;&#24615;&#31561;&#38382;&#39064;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#39318;&#27425;&#23558;&#21464;&#28857;&#26816;&#27979;&#38382;&#39064;&#24191;&#20041;&#20026;&#21464;&#38388;&#38548;&#26816;&#27979;&#65288;CID&#65289;&#38382;&#39064;&#30340;&#29305;&#27530;&#24773;&#20917;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22522;&#20110;&#26368;&#26032;&#30340;&#38548;&#31163;&#20998;&#24067;&#26680;&#65288;IDK&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;CID&#26041;&#27861;&#65292;&#21517;&#20026;iCID&#12290;&#22914;&#26524;&#20004;&#20010;&#38750;&#22343;&#21248;&#26102;&#38388;&#30456;&#37051;&#38388;&#38548;&#20043;&#38388;&#23384;&#22312;&#39640;&#19981;&#30456;&#20284;&#24230;&#24471;&#20998;&#65292;iCID&#23558;&#35782;&#21035;&#21464;&#21270;&#38388;&#38548;&#12290;IDK&#30340;&#25968;&#25454;&#20381;&#36182;&#24615;&#21644;&#26377;&#38480;&#29305;&#24449;&#26144;&#23556;&#20351;iCID&#33021;&#22815;&#26377;&#25928;&#35782;&#21035;&#20855;&#26377;&#24322;&#24120;&#20540;&#23481;&#24525;&#24230;&#30340;&#25968;&#25454;&#27969;&#20013;&#30340;&#21508;&#31181;&#31867;&#22411;&#30340;&#21464;&#28857;&#12290;&#27492;&#22806;&#65292;iCID&#30340;&#25552;&#20986;&#20102;&#22312;&#32447;&#21644;&#31163;&#32447;&#29256;&#26412;&#65292;&#20855;&#26377;&#20248;&#21270;&#20851;&#38190;&#21442;&#25968;&#35774;&#32622;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting abrupt changes in data distribution is one of the most significant tasks in streaming data analysis. Although many unsupervised Change-Point Detection (CPD) methods have been proposed recently to identify those changes, they still suffer from missing subtle changes, poor scalability, or/and sensitivity to outliers. To meet these challenges, we are the first to generalise the CPD problem as a special case of the Change-Interval Detection (CID) problem. Then we propose a CID method, named iCID, based on a recent Isolation Distributional Kernel (IDK). iCID identifies the change interval if there is a high dissimilarity score between two non-homogeneous temporal adjacent intervals. The data-dependent property and finite feature map of IDK enabled iCID to efficiently identify various types of change-points in data streams with the tolerance of outliers. Moreover, the proposed online and offline versions of iCID have the ability to optimise key parameter settings. The effectiveness
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#21517;&#20026;SimiS&#30340;&#31616;&#21333;&#20294;&#34987;&#24573;&#35270;&#30340;&#22522;&#20934;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20266;&#26631;&#31614;&#20316;&#20026;&#26631;&#31614;&#25968;&#25454;&#30340;&#34917;&#20805;&#65292;&#26681;&#25454;&#19982;&#26368;&#39057;&#32321;&#31867;&#21035;&#30340;&#31867;&#21035;&#20998;&#24067;&#24046;&#24322;&#65292;&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#19981;&#24179;&#34913;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2211.11086</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#19981;&#24179;&#34913;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#20196;&#20154;&#23604;&#23596;&#31616;&#21333;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
An Embarrassingly Simple Baseline for Imbalanced Semi-Supervised Learning. (arXiv:2211.11086v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11086
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#21517;&#20026;SimiS&#30340;&#31616;&#21333;&#20294;&#34987;&#24573;&#35270;&#30340;&#22522;&#20934;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20266;&#26631;&#31614;&#20316;&#20026;&#26631;&#31614;&#25968;&#25454;&#30340;&#34917;&#20805;&#65292;&#26681;&#25454;&#19982;&#26368;&#39057;&#32321;&#31867;&#21035;&#30340;&#31867;&#21035;&#20998;&#24067;&#24046;&#24322;&#65292;&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#19981;&#24179;&#34913;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#22312;&#21033;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#26631;&#20934;&#30340;SSL&#20551;&#35774;&#25968;&#25454;&#20998;&#24067;&#22343;&#21248;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#26356;&#21152;&#30495;&#23454;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#22659;&#65292;&#21363;&#19981;&#24179;&#34913;&#21322;&#30417;&#30563;&#23398;&#20064;&#65288;imbalanced SSL&#65289;&#65292;&#20854;&#20013;&#26631;&#31614;&#25968;&#25454;&#21644;&#26080;&#26631;&#31614;&#25968;&#25454;&#37117;&#20986;&#29616;&#20102;&#19981;&#24179;&#34913;&#30340;&#31867;&#21035;&#20998;&#24067;&#12290;&#23613;&#31649;&#24050;&#26377;&#21162;&#21147;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#30340;&#26041;&#27861;&#65292;&#20294;&#24403;&#36935;&#21040;&#20005;&#37325;&#19981;&#24179;&#34913;&#26102;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#20250;&#36864;&#21270;&#65292;&#22240;&#20026;&#23427;&#20204;&#26080;&#27861;&#23545;&#31867;&#21035;&#19981;&#24179;&#34913;&#36827;&#34892;&#36275;&#22815;&#21644;&#26377;&#25928;&#30340;&#20943;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#34987;&#24573;&#35270;&#30340;&#22522;&#20934;&#26041;&#27861;--SimiS&#65292;&#36890;&#36807;&#31616;&#21333;&#22320;&#26681;&#25454;&#19982;&#26368;&#39057;&#32321;&#31867;&#21035;&#30340;&#31867;&#21035;&#20998;&#24067;&#24046;&#24322;&#65292;&#23558;&#20266;&#26631;&#31614;&#20316;&#20026;&#26631;&#31614;&#25968;&#25454;&#30340;&#34917;&#20805;&#12290;&#36825;&#26679;&#19968;&#20010;&#31616;&#21333;&#30340;&#22522;&#20934;&#26041;&#27861;&#22312;&#20943;&#23569;&#31867;&#21035;&#19981;&#24179;&#34913;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;&#23427;&#22312;CIFAR100-LT&#65292;FOOD101-LT&#21644;ImageNet127&#19978;&#30456;&#23545;&#20110;&#20808;&#21069;&#26368;&#20339;&#26041;&#27861;&#30340;&#24615;&#33021;&#25552;&#21319;&#26174;&#33879;&#65292;&#20998;&#21035;&#25552;&#21319;&#20102;12.8&#65285;&#65292;13.6&#65285;&#21644;16.7&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised learning (SSL) has shown great promise in leveraging unlabeled data to improve model performance. While standard SSL assumes uniform data distribution, we consider a more realistic and challenging setting called imbalanced SSL, where imbalanced class distributions occur in both labeled and unlabeled data. Although there are existing endeavors to tackle this challenge, their performance degenerates when facing severe imbalance since they can not reduce the class imbalance sufficiently and effectively. In this paper, we study a simple yet overlooked baseline -- SimiS -- which tackles data imbalance by simply supplementing labeled data with pseudo-labels, according to the difference in class distribution from the most frequent class. Such a simple baseline turns out to be highly effective in reducing class imbalance. It outperforms existing methods by a significant margin, e.g., 12.8%, 13.6%, and 16.7% over previous SOTA on CIFAR100-LT, FOOD101-LT, and ImageNet127 respecti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#35821;&#35328;&#25511;&#21046;&#25193;&#25955;&#27169;&#22411;&#30340;&#20998;&#23618;&#35268;&#21010;&#22120;&#65292;&#26377;&#25928;&#32780;&#39640;&#25928;&#22320;&#25193;&#23637;&#25193;&#25955;&#27169;&#22411;&#65292;&#35299;&#20915;&#38271;&#26102;&#38388;&#36328;&#24230;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#19979;&#30340;&#25511;&#21046;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#21333;&#20219;&#21153;&#21644;&#22810;&#20219;&#21153;&#25104;&#21151;&#29575;&#65292;&#24182;&#26497;&#22823;&#22320;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2210.15629</link><description>&lt;p&gt;
&#35821;&#35328;&#25511;&#21046;&#25193;&#25955;&#65306;&#36890;&#36807;&#31354;&#38388;&#12289;&#26102;&#38388;&#21644;&#20219;&#21153;&#39640;&#25928;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Language Control Diffusion: Efficiently Scaling through Space, Time, and Tasks. (arXiv:2210.15629v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15629
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#35821;&#35328;&#25511;&#21046;&#25193;&#25955;&#27169;&#22411;&#30340;&#20998;&#23618;&#35268;&#21010;&#22120;&#65292;&#26377;&#25928;&#32780;&#39640;&#25928;&#22320;&#25193;&#23637;&#25193;&#25955;&#27169;&#22411;&#65292;&#35299;&#20915;&#38271;&#26102;&#38388;&#36328;&#24230;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#19979;&#30340;&#25511;&#21046;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#21333;&#20219;&#21153;&#21644;&#22810;&#20219;&#21153;&#25104;&#21151;&#29575;&#65292;&#24182;&#26497;&#22823;&#22320;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#36890;&#29992;&#22411;&#26234;&#33021;&#20307;&#22312;&#21508;&#20010;&#26041;&#38754;&#37117;&#24456;&#22256;&#38590;&#65292;&#38656;&#35201;&#22788;&#29702;&#39640;&#32500;&#36755;&#20837;&#65288;&#31354;&#38388;&#65289;&#12289;&#38271;&#26102;&#38388;&#36328;&#24230;&#65288;&#26102;&#38388;&#65289;&#21644;&#22810;&#20010;&#26032;&#20219;&#21153;&#12290;&#26368;&#36817;&#30340;&#32467;&#26500;&#26041;&#38754;&#30340;&#36827;&#23637;&#20351;&#24471;&#25105;&#20204;&#21487;&#20197;&#27839;&#30528;&#20854;&#20013;&#19968;&#20010;&#25110;&#20004;&#20010;&#32500;&#24230;&#25552;&#39640;&#25193;&#23637;&#24615;&#33021;&#21147;&#65292;&#20294;&#35745;&#31639;&#25104;&#26412;&#20173;&#28982;&#24456;&#39640;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#35821;&#35328;&#25511;&#21046;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#26465;&#20214;&#30340;&#20998;&#23618;&#35268;&#21010;&#22120;&#65288;LCD&#65289;&#26469;&#24212;&#23545;&#36825;&#19977;&#20010;&#26041;&#38754;&#12290;&#25105;&#20204;&#26377;&#25928;&#32780;&#39640;&#25928;&#22320;&#25193;&#23637;&#25193;&#25955;&#27169;&#22411;&#65292;&#20197;&#24212;&#23545;&#26102;&#38388;&#12289;&#29366;&#24577;&#21644;&#20219;&#21153;&#31354;&#38388;&#32500;&#24230;&#30340;&#38271;&#26102;&#38388;&#36328;&#24230;&#25511;&#21046;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;CALVIN&#35821;&#35328;&#26426;&#22120;&#20154;&#22522;&#20934;&#27979;&#35797;&#20013;&#23558;LCD&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#21457;&#29616;LCD&#22312;&#22810;&#20219;&#21153;&#25104;&#21151;&#29575;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#32780;&#21333;&#20219;&#21153;&#25104;&#21151;&#29575;&#65288;SR&#65289;&#20026;88.7%&#65292;&#36828;&#39640;&#20110;&#20197;&#21069;&#30340;&#26368;&#20339;&#25104;&#32489;82.6%&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training generalist agents is difficult across several axes, requiring us to deal with high-dimensional inputs (space), long horizons (time), and multiple and new tasks. Recent advances with architectures have allowed for improved scaling along one or two of these dimensions, but are still prohibitive computationally. In this paper, we propose to address all three axes by leveraging Language to Control Diffusion models as a hierarchical planner conditioned on language (LCD). We effectively and efficiently scale diffusion models for planning in extended temporal, state, and task dimensions to tackle long horizon control problems conditioned on natural language instructions. We compare LCD with other state-of-the-art models on the CALVIN language robotics benchmark and find that LCD outperforms other SOTA methods in multi task success rates while dramatically improving computational efficiency with a single task success rate (SR) of 88.7% against the previous best of 82.6%. We show that 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#20540;&#20998;&#24067;&#65292;&#24182;&#21457;&#29616;&#23398;&#20064;&#30340;&#20540;&#20998;&#24067;&#19982;&#27491;&#24577;&#20998;&#24067;&#38750;&#24120;&#25509;&#36817;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#27491;&#24577;&#24341;&#23548;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#26041;&#24046;&#32593;&#32476;&#39044;&#27979;&#30340;&#26041;&#24046;&#21644;&#22238;&#25253;&#65292;&#20197;&#21450;&#19982;&#26631;&#20934;&#20540;&#20989;&#25968;&#19981;&#21516;&#30340;&#20540;&#20998;&#24067;&#32467;&#26500;&#29305;&#24449;&#26469;&#26356;&#26032;&#31574;&#30053;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20004;&#31181;&#22312;&#32447;&#31639;&#27861;&#19978;&#20135;&#29983;&#20102;&#26174;&#33879;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2208.13125</link><description>&lt;p&gt;
&#36830;&#32493;&#25511;&#21046;&#30340;&#27491;&#24120;&#24341;&#23548;&#20998;&#24067;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Normality-Guided Distributional Reinforcement Learning for Continuous Control. (arXiv:2208.13125v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.13125
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#20540;&#20998;&#24067;&#65292;&#24182;&#21457;&#29616;&#23398;&#20064;&#30340;&#20540;&#20998;&#24067;&#19982;&#27491;&#24577;&#20998;&#24067;&#38750;&#24120;&#25509;&#36817;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#27491;&#24577;&#24341;&#23548;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#26041;&#24046;&#32593;&#32476;&#39044;&#27979;&#30340;&#26041;&#24046;&#21644;&#22238;&#25253;&#65292;&#20197;&#21450;&#19982;&#26631;&#20934;&#20540;&#20989;&#25968;&#19981;&#21516;&#30340;&#20540;&#20998;&#24067;&#32467;&#26500;&#29305;&#24449;&#26469;&#26356;&#26032;&#31574;&#30053;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20004;&#31181;&#22312;&#32447;&#31639;&#27861;&#19978;&#20135;&#29983;&#20102;&#26174;&#33879;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#65292;&#23398;&#20064;&#19968;&#20010;&#39044;&#27979;&#22238;&#25253;&#30340;&#22343;&#20540;&#27169;&#22411;&#65292;&#25110;&#20215;&#20540;&#20989;&#25968;&#65292;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;(DRL)&#36890;&#36807;&#24314;&#27169;&#20540;&#20998;&#24067;&#32780;&#19981;&#20165;&#20165;&#26159;&#22343;&#20540;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20960;&#20010;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#20540;&#20998;&#24067;&#65292;&#24182;&#21457;&#29616;&#23398;&#20064;&#30340;&#20540;&#20998;&#24067;&#19982;&#27491;&#24577;&#20998;&#24067;&#38750;&#24120;&#25509;&#36817;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21033;&#29992;&#36825;&#20010;&#24615;&#36136;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20174;&#26041;&#24046;&#32593;&#32476;&#39044;&#27979;&#30340;&#26041;&#24046;&#65292;&#20197;&#21450;&#22238;&#25253;&#65292;&#26469;&#20998;&#26512;&#35745;&#31639;&#20195;&#34920;&#25105;&#20204;&#20998;&#24067;&#24335;&#20540;&#20989;&#25968;&#30340;&#27491;&#24577;&#20998;&#24067;&#30340;&#30446;&#26631;&#20998;&#20301;&#26639;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20540;&#20998;&#24067;&#30340;&#32467;&#26500;&#29305;&#24449;&#30340;&#27491;&#30830;&#24615;&#26469;&#34913;&#37327;&#30340;&#31574;&#30053;&#26356;&#26032;&#26041;&#27861;&#65292;&#36825;&#20123;&#29305;&#24449;&#22312;&#26631;&#20934;&#30340;&#20540;&#20989;&#25968;&#20013;&#19981;&#23384;&#22312;&#12290;&#25105;&#20204;&#27010;&#36848;&#30340;&#26041;&#27861;&#19982;&#35768;&#22810;DRL&#32467;&#26500;&#20860;&#23481;&#12290;&#25105;&#20204;&#20351;&#29992;&#20004;&#31181;&#20195;&#34920;&#24615;&#30340;&#22312;&#32447;&#31639;&#27861;&#65292;PPO&#21644;TRPO&#65292;&#20316;&#20026;&#27979;&#35797;&#24179;&#21488;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#32479;&#35745;&#19978;&#20135;&#29983;&#20102;&#26174;&#33879;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning a predictive model of the mean return, or value function, plays a critical role in many reinforcement learning algorithms. Distributional reinforcement learning (DRL) has been shown to improve performance by modeling the value distribution, not just the mean. We study the value distribution in several continuous control tasks and find that the learned value distribution is empirical quite close to normal. We design a method that exploits this property, employ variances predicted from a variance network, along with returns, to analytically compute target quantile bars representing a normal for our distributional value function. In addition, we propose a policy update strategy based on the correctness as measured by structural characteristics of the value distribution not present in the standard value function. The approach we outline is compatible with many DRL structures. We use two representative on-policy algorithms, PPO and TRPO, as testbeds. Our method yields statistically
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#8212;&#8212;"&#27668;&#20505;&#19981;&#21464;"&#30340;&#26426;&#22120;&#23398;&#20064;&#65292;&#36890;&#36807;&#23558;&#27668;&#20505;&#36807;&#31243;&#30340;&#30693;&#35782;&#32435;&#20837;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20013;&#65292;&#21487;&#20197;&#22312;&#24191;&#27867;&#30340;&#27668;&#20505;&#21644;&#22320;&#29702;&#26465;&#20214;&#19979;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2112.08440</link><description>&lt;p&gt;
&#27668;&#20505;&#19981;&#21464;&#30340;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Climate-Invariant Machine Learning. (arXiv:2112.08440v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.08440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#8212;&#8212;"&#27668;&#20505;&#19981;&#21464;"&#30340;&#26426;&#22120;&#23398;&#20064;&#65292;&#36890;&#36807;&#23558;&#27668;&#20505;&#36807;&#31243;&#30340;&#30693;&#35782;&#32435;&#20837;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20013;&#65292;&#21487;&#20197;&#22312;&#24191;&#27867;&#30340;&#27668;&#20505;&#21644;&#22320;&#29702;&#26465;&#20214;&#19979;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27668;&#20505;&#21464;&#21270;&#39044;&#27979;&#26159;&#19968;&#20010;&#27867;&#21270;&#38382;&#39064;&#65306;&#25105;&#20204;&#20351;&#29992;&#29289;&#29702;&#27169;&#22411;&#22312;&#36807;&#21435;&#12289;&#29616;&#22312;&#21644;&#26410;&#26469;&#30340;&#27668;&#20505;&#20013;&#23545;&#26368;&#36817;&#30340;&#36807;&#21435;&#36827;&#34892;&#22806;&#25512;&#12290;&#30446;&#21069;&#30340;&#27668;&#20505;&#27169;&#22411;&#38656;&#35201;&#23545;&#23567;&#20110;&#27169;&#22411;&#32593;&#26684;&#22823;&#23567;&#30340;&#23610;&#24230;&#19978;&#21457;&#29983;&#30340;&#36807;&#31243;&#36827;&#34892;&#34920;&#31034;&#65292;&#36825;&#20123;&#36807;&#31243;&#26159;&#27169;&#22411;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#20027;&#35201;&#26469;&#28304;&#12290;&#26368;&#36817;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#31639;&#27861;&#26377;&#26395;&#25913;&#21892;&#36825;&#31181;&#36807;&#31243;&#34920;&#31034;&#65292;&#20294;&#24448;&#24448;&#22312;&#26410;&#32463;&#35757;&#32451;&#30340;&#27668;&#20505;&#29615;&#22659;&#20013;&#22806;&#25512;&#25928;&#26524;&#19981;&#20339;&#12290;&#20026;&#20102;&#20805;&#20998;&#21457;&#25381;&#29289;&#29702;&#21644;&#32479;&#35745;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;&#8212;&#8212;&#31216;&#20026;"&#27668;&#20505;&#19981;&#21464;"&#30340;&#26426;&#22120;&#23398;&#20064;&#8212;&#8212;&#23558;&#27668;&#20505;&#36807;&#31243;&#30340;&#30693;&#35782;&#32435;&#20837;ML&#31639;&#27861;&#20013;&#65292;&#24182;&#35777;&#26126;&#23427;&#21487;&#20197;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#22823;&#27668;&#27169;&#22411;&#20013;&#22312;&#24191;&#27867;&#30340;&#27668;&#20505;&#21644;&#22320;&#29702;&#26465;&#20214;&#19979;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#29289;&#29702;&#30693;&#35782;&#26126;&#30830;&#32435;&#20837;&#25968;&#25454;&#39537;&#21160;&#30340;&#22320;&#29699;&#31995;&#32479;&#36807;&#31243;&#27169;&#22411;&#20013;&#21487;&#20197;&#25552;&#39640;&#23427;&#20204;&#30340;&#19968;&#33268;&#24615;&#12289;&#25968;&#25454;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Projecting climate change is a generalization problem: we extrapolate the recent past using physical models across past, present, and future climates. Current climate models require representations of processes that occur at scales smaller than model grid size, which have been the main source of model projection uncertainty. Recent machine learning (ML) algorithms hold promise to improve such process representations, but tend to extrapolate poorly to climate regimes they were not trained on. To get the best of the physical and statistical worlds, we propose a new framework -- termed "climate-invariant" ML -- incorporating knowledge of climate processes into ML algorithms, and show that it can maintain high accuracy across a wide range of climate and geographic conditions in three distinct atmospheric models. Our results suggest that explicitly incorporating physical knowledge into data-driven models of Earth system processes can improve their consistency, data efficiency, and generaliz
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#36125;&#21494;&#26031;&#23618;&#27425;&#22238;&#24402;&#27169;&#22411;&#30340;&#36817;&#20284;&#20132;&#21449;&#39564;&#35777;&#22343;&#20540;&#20272;&#35745;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#26041;&#24046;-&#21327;&#26041;&#24046;&#21442;&#25968;&#19978;&#36827;&#34892;&#26465;&#20214;&#65292;&#23558;&#20132;&#21449;&#39564;&#35777;&#38382;&#39064;&#36716;&#21270;&#20026;&#31616;&#21333;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22823;&#22411;BHRMs&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2011.14238</link><description>&lt;p&gt;
Bayesian&#23618;&#27425;&#22238;&#24402;&#27169;&#22411;&#30340;&#36817;&#20284;&#20132;&#21449;&#39564;&#35777;&#22343;&#20540;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Approximate Cross-validated Mean Estimates for Bayesian Hierarchical Regression Models. (arXiv:2011.14238v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2011.14238
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#36125;&#21494;&#26031;&#23618;&#27425;&#22238;&#24402;&#27169;&#22411;&#30340;&#36817;&#20284;&#20132;&#21449;&#39564;&#35777;&#22343;&#20540;&#20272;&#35745;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#26041;&#24046;-&#21327;&#26041;&#24046;&#21442;&#25968;&#19978;&#36827;&#34892;&#26465;&#20214;&#65292;&#23558;&#20132;&#21449;&#39564;&#35777;&#38382;&#39064;&#36716;&#21270;&#20026;&#31616;&#21333;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22823;&#22411;BHRMs&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33719;&#21462;&#36125;&#21494;&#26031;&#23618;&#27425;&#22238;&#24402;&#27169;&#22411;(BHRMs)&#30340;&#20132;&#21449;&#39564;&#35777;&#39044;&#27979;&#20272;&#35745;&#12290;&#36125;&#21494;&#26031;&#23618;&#27425;&#27169;&#22411;&#20197;&#20854;&#33021;&#22815;&#24314;&#27169;&#22797;&#26434;&#30340;&#20381;&#36182;&#32467;&#26500;&#24182;&#25552;&#20379;&#27010;&#29575;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#32780;&#21463;&#21040;&#27426;&#36814;&#65292;&#20294;&#36816;&#34892;&#30340;&#35745;&#31639;&#24320;&#38144;&#24456;&#22823;&#12290;&#22240;&#27492;&#65292;&#20132;&#21449;&#39564;&#35777;(CV)&#19981;&#26159;&#35780;&#20272;BHRMs&#39044;&#27979;&#24615;&#33021;&#30340;&#24120;&#35265;&#23454;&#36341;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36991;&#20813;&#20102;&#20026;&#27599;&#20010;&#20132;&#21449;&#39564;&#35777;&#25240;&#21472;&#37325;&#26032;&#36816;&#34892;&#35745;&#31639;&#24320;&#38144;&#26114;&#36149;&#30340;&#20272;&#35745;&#26041;&#27861;&#30340;&#38656;&#35201;&#65292;&#20351;CV&#22312;&#22823;&#22411;BHRMs&#20013;&#26356;&#21487;&#34892;&#12290;&#36890;&#36807;&#22312;&#26041;&#24046;-&#21327;&#26041;&#24046;&#21442;&#25968;&#19978;&#36827;&#34892;&#26465;&#20214;&#65292;&#23558;CV&#38382;&#39064;&#20174;&#22522;&#20110;&#27010;&#29575;&#30340;&#25277;&#26679;&#36716;&#21270;&#20026;&#31616;&#21333;&#29087;&#24713;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#36825;&#20135;&#29983;&#30340;&#20272;&#35745;&#19982;&#23436;&#25972;&#30340;CV&#31561;&#25928;&#12290;&#25105;&#20204;&#25552;&#20379;&#29702;&#35770;&#32467;&#26524;&#65292;&#24182;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#21644;&#27169;&#25311;&#20013;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a novel procedure for obtaining cross-validated predictive estimates for Bayesian hierarchical regression models (BHRMs). Bayesian hierarchical models are popular for their ability to model complex dependence structures and provide probabilistic uncertainty estimates, but can be computationally expensive to run. Cross-validation (CV) is therefore not a common practice to evaluate the predictive performance of BHRMs. Our method circumvents the need to re-run computationally costly estimation methods for each cross-validation fold and makes CV more feasible for large BHRMs. By conditioning on the variance-covariance parameters, we shift the CV problem from probability-based sampling to a simple and familiar optimization problem. In many cases, this produces estimates which are equivalent to full CV. We provide theoretical results and demonstrate its efficacy on publicly available data and in simulations.
&lt;/p&gt;</description></item></channel></rss>