<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#37325;&#35775;&#28888;&#28953;&#22823;&#36187;&#65292;&#35780;&#20272;&#20102;&#26368;&#36817;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#31639;&#27861;&#22312;112&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#12290;&#35813;&#35770;&#25991;&#36890;&#36807;&#20998;&#31867;&#27861;&#23558;&#36825;&#20123;&#31639;&#27861;&#20998;&#20026;&#20116;&#31867;&#65292;&#20026;TSC&#39046;&#22495;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2304.13029</link><description>&lt;p&gt;
Bake Off&#37325;&#35775;&#65306;&#23545;&#26368;&#36817;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#31639;&#27861;&#30340;&#35780;&#36848;&#21644;&#23454;&#39564;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Bake off redux: a review and experimental evaluation of recent time series classification algorithms. (arXiv:2304.13029v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13029
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#35775;&#28888;&#28953;&#22823;&#36187;&#65292;&#35780;&#20272;&#20102;&#26368;&#36817;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#31639;&#27861;&#22312;112&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#12290;&#35813;&#35770;&#25991;&#36890;&#36807;&#20998;&#31867;&#27861;&#23558;&#36825;&#20123;&#31639;&#27861;&#20998;&#20026;&#20116;&#31867;&#65292;&#20026;TSC&#39046;&#22495;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31687;&#30740;&#31350;&#35770;&#25991;&#22312;2017&#24180;&#27604;&#36739;&#20102;18&#20010;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#65288;TSC&#65289;&#31639;&#27861;&#22312;&#26469;&#33258;&#21152;&#24030;&#22823;&#23398;&#27827;&#28392;&#20998;&#26657;&#65288;UCR&#65289;&#23384;&#26723;&#30340;85&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#12290;&#36825;&#39033;&#30740;&#31350;&#36890;&#24120;&#34987;&#31216;&#20026;&#8220;&#28888;&#28953;&#27604;&#36187;&#8221;&#65292;&#21457;&#29616;&#21482;&#26377;9&#20010;&#31639;&#27861;&#30340;&#34920;&#29616;&#26174;&#33879;&#20248;&#20110;&#20351;&#29992;&#30340;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#65288;DTW&#65289;&#21644;&#26059;&#36716;&#26862;&#26519;&#22522;&#20934;&#12290;&#35813;&#30740;&#31350;&#36890;&#36807;&#20174;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#25552;&#21462;&#29305;&#24449;&#30340;&#31639;&#27861;&#31867;&#22411;&#23545;&#27599;&#20010;&#31639;&#27861;&#36827;&#34892;&#20998;&#31867;&#65292;&#24418;&#25104;&#20102;&#20116;&#31181;&#20027;&#35201;&#31639;&#27861;&#31867;&#22411;&#30340;&#20998;&#31867;&#27861;&#12290;&#19982;&#21487;&#20197;&#37325;&#29616;&#32467;&#26524;&#30340;&#20195;&#30721;&#21644;&#32467;&#26524;&#30340;&#25552;&#20379;&#30456;&#32467;&#21512;&#65292;&#36825;&#20123;&#31639;&#27861;&#30340;&#20998;&#31867;&#21644;&#21487;&#35775;&#38382;&#30340;&#32467;&#26524;&#25512;&#21160;&#20102;TSC&#39046;&#22495;&#30340;&#26222;&#21450;&#12290;&#20845;&#24180;&#36807;&#21435;&#20102;&#65292;UCR&#23384;&#26723;&#24050;&#25193;&#23637;&#21040;112&#20010;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#24050;&#32463;&#25552;&#20986;&#20102;&#22823;&#37327;&#26032;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#37325;&#35775;&#28888;&#28953;&#22823;&#36187;&#65292;&#30475;&#30475;&#27599;&#20010;&#25552;&#20986;&#30340;&#31867;&#21035;&#33258;&#21407;&#22987;&#20986;&#29256;&#20197;&#26469;&#30340;&#36827;&#23637;&#65292;&#24182;&#35780;&#20272;&#26032;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In 2017, a research paper compared 18 Time Series Classification (TSC) algorithms on 85 datasets from the University of California, Riverside (UCR) archive. This study, commonly referred to as a `bake off', identified that only nine algorithms performed significantly better than the Dynamic Time Warping (DTW) and Rotation Forest benchmarks that were used. The study categorised each algorithm by the type of feature they extract from time series data, forming a taxonomy of five main algorithm types. This categorisation of algorithms alongside the provision of code and accessible results for reproducibility has helped fuel an increase in popularity of the TSC field. Over six years have passed since this bake off, the UCR archive has expanded to 112 datasets and there have been a large number of new algorithms proposed. We revisit the bake off, seeing how each of the proposed categories have advanced since the original publication, and evaluate the performance of newer algorithms against t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38598;&#25104;&#30340;&#35748;&#35777;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;S-Lipschitz&#20998;&#31867;&#22120;&#27010;&#24565;&#24182;&#32473;&#20986;&#20102;&#38598;&#25104;&#29702;&#35770;&#40065;&#26834;&#24615;&#30340;&#20934;&#30830;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2304.13019</link><description>&lt;p&gt;
&#35748;&#35777;&#38598;&#25104;&#65306;&#19968;&#31181;&#20855;&#26377;S-Lipschitz&#29305;&#24615;&#30340;&#36890;&#29992;&#35748;&#35777;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Certifying Ensembles: A General Certification Theory with S-Lipschitzness. (arXiv:2304.13019v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38598;&#25104;&#30340;&#35748;&#35777;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;S-Lipschitz&#20998;&#31867;&#22120;&#27010;&#24565;&#24182;&#32473;&#20986;&#20102;&#38598;&#25104;&#29702;&#35770;&#40065;&#26834;&#24615;&#30340;&#20934;&#30830;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#39640;&#21644;&#20445;&#35777;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#19968;&#30452;&#26159;&#19968;&#20010;&#30740;&#31350;&#30340;&#28909;&#28857;&#12290;&#38598;&#25104;&#26159;&#23558;&#20960;&#20010;&#20998;&#31867;&#22120;&#32452;&#21512;&#20197;&#25552;&#20379;&#26356;&#22909;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#23427;&#24050;&#32463;&#35777;&#26126;&#23545;&#27867;&#21270;&#12289;&#19981;&#30830;&#23450;&#24615;&#35780;&#20272;&#12289;&#26657;&#20934;&#20197;&#21450;&#32531;&#35299;&#27010;&#24565;&#28418;&#31227;&#30340;&#24433;&#21709;&#26159;&#26377;&#30410;&#30340;&#12290;&#28982;&#32780;&#65292;&#23545;&#35748;&#35777;&#40065;&#26834;&#24615;&#30340;&#38598;&#25104;&#24433;&#21709;&#35748;&#35782;&#19981;&#36275;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;S-Lipschitz&#20998;&#31867;&#22120;&#26469;&#25512;&#24191;Lipschitz&#36830;&#32493;&#24615;&#65292;&#29992;&#20110;&#20998;&#26512;&#38598;&#25104;&#29702;&#35770;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26159;&#31934;&#30830;&#26465;&#20214;&#65292;&#22312;&#36825;&#20123;&#26465;&#20214;&#19979;&#65292;&#40065;&#26834;&#20998;&#31867;&#22120;&#30340;&#38598;&#25104;&#27604;&#20219;&#20309;&#32452;&#25104;&#37096;&#20998;&#20998;&#31867;&#22120;&#26356;&#21487;&#38752;&#65292;&#21516;&#26102;&#22312;&#20160;&#20040;&#26465;&#20214;&#19979;&#23427;&#20204;&#19981;&#26159;&#40065;&#26834;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Improving and guaranteeing the robustness of deep learning models has been a topic of intense research. Ensembling, which combines several classifiers to provide a better model, has shown to be beneficial for generalisation, uncertainty estimation, calibration, and mitigating the effects of concept drift. However, the impact of ensembling on certified robustness is less well understood. In this work, we generalise Lipschitz continuity by introducing S-Lipschitz classifiers, which we use to analyse the theoretical robustness of ensembles. Our results are precise conditions when ensembles of robust classifiers are more robust than any constituent classifier, as well as conditions when they are less robust.
&lt;/p&gt;</description></item><item><title>DuETT&#26159;&#19968;&#20010;&#29992;&#20110;EHR&#30340;&#21452;&#37325;&#20107;&#20214;&#26102;&#38388;&#21464;&#25442;&#22120;&#65292;&#36890;&#36807;&#21452;&#37325;&#27880;&#24847;&#26426;&#21046;&#23398;&#20064;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#30456;&#21516;&#26102;&#38388;&#27493;&#30340;&#19981;&#21516;&#34920;&#31034;&#65292;&#36991;&#20813;&#30001;&#20110;&#26102;&#38388;&#27493;&#20240;&#22823;&#32780;&#20135;&#29983;&#30340;&#20108;&#27425;&#25918;&#32553;&#38382;&#39064;&#65292;&#24182;&#22312;&#22235;&#20010;&#22522;&#20934;EHR&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20197;&#24448;&#30340;&#21333;&#19968;&#20219;&#21153;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.13017</link><description>&lt;p&gt;
DuETT: &#21452;&#37325;&#20107;&#20214;&#26102;&#38388;&#21464;&#25442;&#22120;&#29992;&#20110;&#30005;&#23376;&#30149;&#21382;
&lt;/p&gt;
&lt;p&gt;
DuETT: Dual Event Time Transformer for Electronic Health Records. (arXiv:2304.13017v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13017
&lt;/p&gt;
&lt;p&gt;
DuETT&#26159;&#19968;&#20010;&#29992;&#20110;EHR&#30340;&#21452;&#37325;&#20107;&#20214;&#26102;&#38388;&#21464;&#25442;&#22120;&#65292;&#36890;&#36807;&#21452;&#37325;&#27880;&#24847;&#26426;&#21046;&#23398;&#20064;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#30456;&#21516;&#26102;&#38388;&#27493;&#30340;&#19981;&#21516;&#34920;&#31034;&#65292;&#36991;&#20813;&#30001;&#20110;&#26102;&#38388;&#27493;&#20240;&#22823;&#32780;&#20135;&#29983;&#30340;&#20108;&#27425;&#25918;&#32553;&#38382;&#39064;&#65292;&#24182;&#22312;&#22235;&#20010;&#22522;&#20934;EHR&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20197;&#24448;&#30340;&#21333;&#19968;&#20219;&#21153;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#38498;&#35774;&#32622;&#20013;&#35760;&#24405;&#30340;&#30005;&#23376;&#30149;&#21382;&#65288;EHR&#65289;&#36890;&#24120;&#21253;&#21547;&#24191;&#27867;&#30340;&#25968;&#23383;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#20854;&#29305;&#24449;&#26159;&#39640;&#31232;&#30095;&#24615;&#21644;&#19981;&#35268;&#21017;&#35266;&#23519;&#12290;&#36825;&#20123;&#25968;&#25454;&#30340;&#26377;&#25928;&#24314;&#27169;&#24517;&#39035;&#21033;&#29992;&#20854;&#26102;&#38388;&#24207;&#21015;&#29305;&#24615;&#65292;&#19981;&#21516;&#31867;&#22411;&#35266;&#23519;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#20197;&#21450;&#25968;&#25454;&#20013;&#31232;&#30095;&#24615;&#32467;&#26500;&#20013;&#30340;&#20449;&#24687;&#12290;&#33258;&#30417;&#30563;&#21464;&#21387;&#22120;&#22312;NLP&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#21508;&#31181;&#32467;&#26500;&#21270;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#20294;&#26159;&#65292;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#21253;&#21547;&#20004;&#20010;&#32500;&#24230;&#19978;&#30340;&#32467;&#26500;&#21270;&#20851;&#31995;&#65306;&#26102;&#38388;&#21644;&#35760;&#24405;&#30340;&#20107;&#20214;&#31867;&#22411;&#65292;&#32780;&#30452;&#25509;&#23558;&#21464;&#21387;&#22120;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#21017;&#19981;&#33021;&#21033;&#29992;&#36825;&#31181;&#29420;&#29305;&#30340;&#32467;&#26500;&#12290;&#33258;&#25105;&#27880;&#24847;&#21147;&#23618;&#30340;&#20108;&#27425;&#25918;&#32553;&#36824;&#21487;&#20197;&#26174;&#30528;&#38480;&#21046;&#36755;&#20837;&#24207;&#21015;&#38271;&#24230;&#65292;&#32780;&#27809;&#26377;&#36866;&#24403;&#30340;&#36755;&#20837;&#24037;&#31243;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;DuETT&#26550;&#26500;&#65292;&#36825;&#26159;&#21464;&#21387;&#22120;&#30340;&#25193;&#23637;&#65292;&#35774;&#35745;&#29992;&#20110;&#22312;EHR&#30340;&#26102;&#38388;&#21644;&#20107;&#20214;&#31867;&#22411;&#32500;&#24230;&#19978;&#20851;&#27880;&#12290;DuETT&#20351;&#29992;&#21452;&#37325;&#27880;&#24847;&#26426;&#21046;&#65292;&#23427;&#21487;&#20197;&#22312;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#30456;&#21516;&#26102;&#38388;&#27493;&#30340;&#19981;&#21516;&#34920;&#31034;&#12290;&#36890;&#36807;&#24341;&#20837;&#20855;&#26377;&#20869;&#26680;&#26041;&#27861;&#30340;&#33258;&#25105;&#27880;&#24847;&#21147;&#65292;DuETT&#36991;&#20813;&#20102;&#33258;&#25105;&#27880;&#24847;&#21147;&#30340;&#20108;&#27425;&#25918;&#32553;&#65292;&#24182;&#36807;&#28388;&#25481;&#19981;&#30456;&#20851;&#30340;&#26102;&#38388;&#27493;&#39588;&#12290;DuETT&#22312;&#22235;&#20010;&#22522;&#20934;EHR&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20197;&#21069;&#30340;&#21333;&#19968;&#20219;&#21153;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electronic health records (EHRs) recorded in hospital settings typically contain a wide range of numeric time series data that is characterized by high sparsity and irregular observations. Effective modelling for such data must exploit its time series nature, the semantic relationship between different types of observations, and information in the sparsity structure of the data. Self-supervised Transformers have shown outstanding performance in a variety of structured tasks in NLP and computer vision. But multivariate time series data contains structured relationships over two dimensions: time and recorded event type, and straightforward applications of Transformers to time series data do not leverage this distinct structure. The quadratic scaling of self-attention layers can also significantly limit the input sequence length without appropriate input engineering. We introduce the DuETT architecture, an extension of Transformers designed to attend over both time and event type dimensio
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#27604;&#20363;&#28176;&#36817;&#24773;&#24418;&#19979;&#30340;&#23376;&#37319;&#26679;&#23725;&#22238;&#24402;&#38598;&#25104;&#65292;&#35777;&#26126;&#20102;&#26368;&#20248;&#20840;&#23725;&#22238;&#24402;&#38598;&#25104;&#30340;&#39118;&#38505;&#19982;&#26368;&#20248;&#23725;&#39044;&#27979;&#22120;&#30340;&#39118;&#38505;&#30456;&#21305;&#37197;&#65292;&#24182;&#35777;&#26126;&#20102;GCV&#22312;&#20272;&#35745;&#23725;&#22238;&#24402;&#38598;&#21512;&#30340;&#39044;&#27979;&#39118;&#38505;&#26041;&#38754;&#30340;&#24378;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.13016</link><description>&lt;p&gt;
&#23376;&#37319;&#26679;&#23725;&#22238;&#24402;&#38598;&#25104;&#65306;&#31561;&#25928;&#24615;&#21644;&#24191;&#20041;&#20132;&#21449;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Subsample Ridge Ensembles: Equivalences and Generalized Cross-Validation. (arXiv:2304.13016v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13016
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#27604;&#20363;&#28176;&#36817;&#24773;&#24418;&#19979;&#30340;&#23376;&#37319;&#26679;&#23725;&#22238;&#24402;&#38598;&#25104;&#65292;&#35777;&#26126;&#20102;&#26368;&#20248;&#20840;&#23725;&#22238;&#24402;&#38598;&#25104;&#30340;&#39118;&#38505;&#19982;&#26368;&#20248;&#23725;&#39044;&#27979;&#22120;&#30340;&#39118;&#38505;&#30456;&#21305;&#37197;&#65292;&#24182;&#35777;&#26126;&#20102;GCV&#22312;&#20272;&#35745;&#23725;&#22238;&#24402;&#38598;&#21512;&#30340;&#39044;&#27979;&#39118;&#38505;&#26041;&#38754;&#30340;&#24378;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#27604;&#20363;&#28176;&#36817;&#24773;&#24418;&#19979;&#30340;&#23376;&#37319;&#26679;&#23725;&#22238;&#24402;&#38598;&#25104;&#65292;&#20854;&#20013;&#29305;&#24449;&#22823;&#23567;&#19982;&#26679;&#26412;&#22823;&#23567;&#25104;&#27604;&#20363;&#22686;&#38271;&#65292;&#20351;&#24471;&#23427;&#20204;&#30340;&#27604;&#29575;&#25910;&#25947;&#21040;&#19968;&#20010;&#24120;&#25968;&#12290;&#36890;&#36807;&#20998;&#26512;&#23725;&#22238;&#24402;&#38598;&#21512;&#30340;&#24179;&#26041;&#39044;&#27979;&#39118;&#38505;&#20316;&#20026;&#26174;&#24335;&#24809;&#32602;$\lambda$&#21644;&#26497;&#38480;&#23376;&#26679;&#26412;&#26041;&#38754;&#27604;$\phi_s$&#65288;&#29305;&#24449;&#22823;&#23567;&#19982;&#23376;&#26679;&#26412;&#22823;&#23567;&#30340;&#27604;&#29575;&#65289;&#30340;&#20989;&#25968;&#65292;&#25105;&#20204;&#34920;&#24449;&#20102;&#22312;&#20219;&#20309;&#21487;&#36798;&#39118;&#38505;&#19979;&#30340;$(\lambda, \phi_s)$-&#24179;&#38754;&#19978;&#30340;&#36718;&#24275;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35777;&#26126;&#26368;&#20248;&#20840;&#23725;&#22238;&#24402;&#38598;&#25104;&#65288;&#36866;&#21512;&#20110;&#25152;&#26377;&#21487;&#33021;&#30340;&#23376;&#26679;&#26412;&#65289;&#30340;&#39118;&#38505;&#19982;&#26368;&#20248;&#23725;&#39044;&#27979;&#22120;&#30340;&#39118;&#38505;&#30456;&#21305;&#37197;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#23545;&#20110;&#20272;&#35745;&#23725;&#22238;&#24402;&#38598;&#21512;&#30340;&#39044;&#27979;&#39118;&#38505;&#65292;&#22522;&#20110;&#24191;&#20041;&#20132;&#21449;&#39564;&#35777;&#65288;GCV&#65289;&#30340;&#23376;&#26679;&#26412;&#22823;&#23567;&#24378;&#19968;&#33268;&#24615;&#12290;&#36825;&#20801;&#35768;&#26080;&#38656;&#26679;&#26412;&#25286;&#20998;&#22522;&#20110;GCV&#20248;&#21270;&#20840;&#23616;&#23725;&#22238;&#24402;&#38598;&#25104;&#65292;&#24182;&#20135;&#29983;&#19968;&#20010;&#39118;&#38505;&#19982;&#26368;&#20248;&#23725;&#22238;&#24402;&#39118;&#38505;&#30456;&#21305;&#37197;&#30340;&#39044;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study subsampling-based ridge ensembles in the proportional asymptotics regime, where the feature size grows proportionally with the sample size such that their ratio converges to a constant. By analyzing the squared prediction risk of ridge ensembles as a function of the explicit penalty $\lambda$ and the limiting subsample aspect ratio $\phi_s$ (the ratio of the feature size to the subsample size), we characterize contours in the $(\lambda, \phi_s)$-plane at any achievable risk. As a consequence, we prove that the risk of the optimal full ridgeless ensemble (fitted on all possible subsamples) matches that of the optimal ridge predictor. In addition, we prove strong uniform consistency of generalized cross-validation (GCV) over the subsample sizes for estimating the prediction risk of ridge ensembles. This allows for GCV-based tuning of full ridgeless ensembles without sample splitting and yields a predictor whose risk matches optimal ridge risk.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#29992;&#20110;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#31283;&#23450;&#21644;&#20302;&#31934;&#24230;&#35757;&#32451;&#30340;&#26032;&#26041;&#27861;&#65292;&#21253;&#25324;SwitchBack&#21644;AdamW-Adafacto&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#25552;&#39640;&#20102;&#35757;&#32451;&#36895;&#24230;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.13013</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#31283;&#23450;&#21644;&#20302;&#31934;&#24230;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Stable and low-precision training for large-scale vision-language models. (arXiv:2304.13013v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13013
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#29992;&#20110;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#31283;&#23450;&#21644;&#20302;&#31934;&#24230;&#35757;&#32451;&#30340;&#26032;&#26041;&#27861;&#65292;&#21253;&#25324;SwitchBack&#21644;AdamW-Adafacto&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#25552;&#39640;&#20102;&#35757;&#32451;&#36895;&#24230;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21152;&#36895;&#21644;&#31283;&#23450;&#22823;&#35821;&#35328;-&#35270;&#35273;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#20026;&#21152;&#36895;&#35757;&#32451;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SwitchBack&#65292;&#36825;&#26159;&#19968;&#31181;&#32447;&#24615;&#23618;&#29992;&#20110;int8&#37327;&#21270;&#35757;&#32451;&#65292;&#20854;&#25552;&#20379;&#20102;13-25&#65285;&#30340;&#36895;&#24230;&#25552;&#21319;&#65292;&#32780;&#19982;1B&#21442;&#25968;CLIP ViT-Huge&#30340;bfloat16&#35757;&#32451;&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#65292;&#22312;&#30446;&#21069;&#20026;&#27490;&#26159;&#26368;&#22823;&#30340;int8&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;int8&#65292;&#22240;&#20026;GPU&#25903;&#25345;float8&#24456;&#23569;&#65292;&#34429;&#28982;&#25105;&#20204;&#20063;&#36890;&#36807;&#27169;&#25311;&#20998;&#26512;&#20102;float8&#35757;&#32451;&#12290;&#20026;&#20102;&#31283;&#23450;&#35757;&#32451;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#25439;&#22833;&#23792;&#20540;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#22312;&#20108;&#27425;&#26799;&#24230;&#20272;&#35745;&#22120;&#30340;AdamW second moment&#20043;&#21518;1-8&#27425;&#36845;&#20195;&#20013;&#19968;&#33268;&#21457;&#29983;&#20302;&#20272;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25512;&#33616;&#20351;&#29992;AdamW-Adafacto&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce new methods for 1) accelerating and 2) stabilizing training for large language-vision models. 1) Towards accelerating training, we introduce SwitchBack, a linear layer for int8 quantized training which provides a speed-up of 13-25% while matching the performance of bfloat16 training within 0.1 percentage points for the 1B parameter CLIP ViT-Huge -- the largest int8 training to date. Our main focus is int8 as GPU support for float8 is rare, though we also analyze float8 training through simulation. While SwitchBack proves effective for float8, we show that standard techniques are also successful if the network is trained and initialized so that large feature magnitudes are discouraged, which we accomplish via layer-scale initialized with zeros. 2) Towards stable training, we analyze loss spikes and find they consistently occur 1-8 iterations after the squared gradients become under-estimated by their AdamW second moment estimator. As a result, we recommend an AdamW-Adafacto
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#23398;&#20064;&#32467;&#26500;&#21270;&#34920;&#31034;&#30340;&#27867;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#25552;&#20986;&#33021;&#22815;&#23398;&#20064;&#23616;&#37096;&#21644;&#20840;&#23616;&#20381;&#36182;&#20851;&#31995;&#30340;&#32467;&#26500;&#21270;&#34920;&#31034;&#26041;&#27861;&#65292;&#24182;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#36827;&#34892;&#23454;&#39564;&#39564;&#35777;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#29289;&#20307;&#26816;&#27979;&#31561;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.13001</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#23398;&#20064;&#32467;&#26500;&#21270;&#34920;&#31034;&#30340;&#27867;&#21270;&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Generalization of Learned Structured Representations. (arXiv:2304.13001v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#23398;&#20064;&#32467;&#26500;&#21270;&#34920;&#31034;&#30340;&#27867;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#25552;&#20986;&#33021;&#22815;&#23398;&#20064;&#23616;&#37096;&#21644;&#20840;&#23616;&#20381;&#36182;&#20851;&#31995;&#30340;&#32467;&#26500;&#21270;&#34920;&#31034;&#26041;&#27861;&#65292;&#24182;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#36827;&#34892;&#23454;&#39564;&#39564;&#35777;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#29289;&#20307;&#26816;&#27979;&#31561;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#26080;&#27861;&#36798;&#21040;&#20154;&#31867;&#32423;&#21035;&#30340;&#31995;&#32479;&#27867;&#21270;&#27700;&#24179;&#12290;&#20154;&#20204;&#35748;&#20026;&#26126;&#30830;&#25429;&#33719;&#25968;&#25454;&#30340;&#22522;&#30784;&#32467;&#26500;&#24212;&#35813;&#33021;&#22815;&#35753;&#32852;&#32467;&#20027;&#20041;&#31995;&#32479;&#20197;&#26356;&#21487;&#39044;&#27979;&#21644;&#31995;&#32479;&#30340;&#26041;&#24335;&#36827;&#34892;&#27867;&#21270;&#12290;&#20107;&#23454;&#19978;&#65292;&#20154;&#31867;&#30340;&#35777;&#25454;&#34920;&#26126;&#65292;&#29992;&#31526;&#21495;&#33324;&#30340;&#32452;&#21512;&#23454;&#20307;&#26469;&#35299;&#37322;&#19990;&#30028;&#21487;&#33021;&#23545;&#26234;&#33021;&#34892;&#20026;&#21644;&#39640;&#32423;&#25512;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;&#21478;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#30340;&#24120;&#35265;&#38480;&#21046;&#26159;&#23427;&#20204;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#36825;&#21487;&#33021;&#24456;&#26114;&#36149;&#12290;&#22312;&#34920;&#31034;&#23398;&#20064;&#20013;&#65292;&#21033;&#29992;&#22823;&#22411;&#25968;&#25454;&#38598;&#23398;&#20064;&#36890;&#29992;&#25968;&#25454;&#34920;&#31034;&#65292;&#36825;&#20123;&#34920;&#31034;&#21487;&#20197;&#29992;&#20110;&#26377;&#25928;&#22320;&#23398;&#20064;&#20219;&#24847;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#32467;&#26500;&#21270;&#34920;&#31034;&#23398;&#20064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#23398;&#20064;&#26410;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#32467;&#26500;&#21270;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#24456;&#23569;&#25110;&#27809;&#26377;&#30417;&#30563;&#65292;&#24182;&#25429;&#33719;&#20854;&#38544;&#34255;&#32467;&#26500;&#12290;&#22312;&#35770;&#25991;&#30340;&#31532;&#19968;&#37096;&#20998;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#32467;&#26500;&#21270;&#34920;&#31034;&#65292;&#21487;&#20197;&#25429;&#33719;&#25968;&#25454;&#20013;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#20381;&#36182;&#20851;&#31995;&#12290;&#22312;&#31532;&#20108;&#37096;&#20998;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23398;&#20064;&#30340;&#32467;&#26500;&#21270;&#34920;&#31034;&#22914;&#20309;&#25552;&#39640;&#28145;&#24230;&#32593;&#32476;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#29289;&#20307;&#26816;&#27979;&#31561;&#20219;&#21153;&#19978;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite tremendous progress over the past decade, deep learning methods generally fall short of human-level systematic generalization. It has been argued that explicitly capturing the underlying structure of data should allow connectionist systems to generalize in a more predictable and systematic manner. Indeed, evidence in humans suggests that interpreting the world in terms of symbol-like compositional entities may be crucial for intelligent behavior and high-level reasoning. Another common limitation of deep learning systems is that they require large amounts of training data, which can be expensive to obtain. In representation learning, large datasets are leveraged to learn generic data representations that may be useful for efficient learning of arbitrary downstream tasks.  This thesis is about structured representation learning. We study methods that learn, with little or no supervision, representations of unstructured data that capture its hidden structure. In the first part of
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#29305;&#24449;&#34701;&#21512;&#32593;&#32476;&#65292;&#20351;&#29992;&#24182;&#34892;&#27880;&#24847;&#21147;&#22359;&#22312;&#19981;&#21516;&#23610;&#24230;&#19978;&#34701;&#21512;&#21407;&#22987; CXR &#22270;&#20687;&#21644;&#23616;&#37096;&#30456;&#20301;&#22686;&#24378; CXR &#22270;&#20687;&#65292;&#20174;&#32780;&#23454;&#29616; COVID-19 &#33016;&#37096; X &#20809;&#22270;&#20687;&#30340;&#31934;&#30830;&#35786;&#26029;&#65292;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.12988</link><description>&lt;p&gt;
&#22810;&#23610;&#24230;&#29305;&#24449;&#34701;&#21512;&#19982;&#24182;&#34892;&#27880;&#24847;&#21147;&#22359;&#24212;&#29992;&#20110; COVID-19 &#33016;&#37096; X &#20809;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Multi-Scale Feature Fusion using Parallel-Attention Block for COVID-19 Chest X-ray Diagnosis. (arXiv:2304.12988v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12988
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#29305;&#24449;&#34701;&#21512;&#32593;&#32476;&#65292;&#20351;&#29992;&#24182;&#34892;&#27880;&#24847;&#21147;&#22359;&#22312;&#19981;&#21516;&#23610;&#24230;&#19978;&#34701;&#21512;&#21407;&#22987; CXR &#22270;&#20687;&#21644;&#23616;&#37096;&#30456;&#20301;&#22686;&#24378; CXR &#22270;&#20687;&#65292;&#20174;&#32780;&#23454;&#29616; COVID-19 &#33016;&#37096; X &#20809;&#22270;&#20687;&#30340;&#31934;&#30830;&#35786;&#26029;&#65292;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20840;&#29699; COVID-19 &#21361;&#26426;&#32972;&#26223;&#19979;&#65292;&#20174;&#33016;&#37096; X &#20809;&#22270;&#20687;&#20013;&#31934;&#30830;&#35786;&#26029; COVID-19 &#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#20943;&#23569;&#25918;&#23556;&#23398;&#35780;&#20272;&#36807;&#31243;&#20013;&#30340;&#21307;&#29983;&#20869;&#37096;&#21644;&#20114;&#30456;&#20043;&#38388;&#30340;&#24046;&#24322;&#24615;&#65292;&#22312;&#21307;&#23398;&#20915;&#31574;&#21644;&#38543;&#21518;&#30340;&#30142;&#30149;&#31649;&#29702;&#36807;&#31243;&#20013;&#65292;&#35745;&#31639;&#26426;&#36741;&#21161;&#35786;&#26029;&#24037;&#20855;&#24050;&#34987;&#29992;&#20110;&#34917;&#20805;&#12290;&#38656;&#35201;&#39640;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#35745;&#31639;&#26041;&#27861;&#26469;&#24555;&#36895;&#20998;&#27969;&#24739;&#32773;&#65292;&#24182;&#24110;&#21161;&#25918;&#23556;&#31185;&#21307;&#29983;&#35299;&#37322;&#25910;&#38598;&#30340;&#25968;&#25454;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#29305;&#24449;&#34701;&#21512;&#32593;&#32476;&#65292;&#20351;&#29992;&#24182;&#34892;&#27880;&#24847;&#21147;&#22359;&#26469;&#34701;&#21512;&#22810;&#23610;&#24230;&#30340;&#21407;&#22987; CXR &#22270;&#20687;&#21644;&#23616;&#37096;&#30456;&#20301;&#22686;&#24378; CXR &#22270;&#20687;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#32452;&#32455;&#33719;&#21462;&#30340;&#21508;&#31181; COVID-19 &#25968;&#25454;&#38598;&#19978;&#26816;&#39564;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#20197;&#35780;&#20272;&#20854;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#25913;&#21892;&#20102;&#27867;&#21270;&#33021;&#21147;&#65292;&#36825;&#23545;&#20110;&#24191;&#27867;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Under the global COVID-19 crisis, accurate diagnosis of COVID-19 from Chest X-ray (CXR) images is critical. To reduce intra- and inter-observer variability, during the radiological assessment, computer-aided diagnostic tools have been utilized to supplement medical decision-making and subsequent disease management. Computational methods with high accuracy and robustness are required for rapid triaging of patients and aiding radiologists in the interpretation of the collected data. In this study, we propose a novel multi-feature fusion network using parallel attention blocks to fuse the original CXR images and local-phase feature-enhanced CXR images at multi-scales. We examine our model on various COVID-19 datasets acquired from different organizations to assess the generalization ability. Our experiments demonstrate that our method achieves state-of-art performance and has improved generalization capability, which is crucial for widespread deployment.
&lt;/p&gt;</description></item><item><title>RubikONNs&#21033;&#29992;&#20809;&#23398;&#31995;&#32479;&#30340;&#29289;&#29702;&#29305;&#24615;&#65292;&#36890;&#36807;&#26059;&#36716;&#30828;&#20214;&#32534;&#30721;&#22810;&#20010;&#21069;&#39304;&#20989;&#25968;&#65292;&#20026;ONNs&#23454;&#29616;&#39640;&#25928;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#25552;&#20379;&#20102;&#26032;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2304.12985</link><description>&lt;p&gt;
Rubik&#20809;&#23398;&#31070;&#32463;&#32593;&#32476;&#65306;&#20855;&#26377;&#29289;&#29702;&#24863;&#30693;&#26059;&#36716;&#32467;&#26500;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Rubik's Optical Neural Networks: Multi-task Learning with Physics-aware Rotation Architecture. (arXiv:2304.12985v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12985
&lt;/p&gt;
&lt;p&gt;
RubikONNs&#21033;&#29992;&#20809;&#23398;&#31995;&#32479;&#30340;&#29289;&#29702;&#29305;&#24615;&#65292;&#36890;&#36807;&#26059;&#36716;&#30828;&#20214;&#32534;&#30721;&#22810;&#20010;&#21069;&#39304;&#20989;&#25968;&#65292;&#20026;ONNs&#23454;&#29616;&#39640;&#25928;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#25552;&#20379;&#20102;&#26032;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#26377;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#24037;&#20316;&#22312;&#25512;&#36827;&#20809;&#23398;&#31070;&#32463;&#32593;&#32476;&#65288;ONNs&#65289;&#65292;&#22312;&#21151;&#29575;&#25928;&#29575;&#65292;&#24182;&#34892;&#24615;&#21644;&#35745;&#31639;&#36895;&#24230;&#26041;&#38754;&#65292;ONNs&#24102;&#26469;&#20102;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#38754;&#30340;&#26174;&#30528;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, there are increasing efforts on advancing optical neural networks (ONNs), which bring significant advantages for machine learning (ML) in terms of power efficiency, parallelism, and computational speed. With the considerable benefits in computation speed and energy efficiency, there are significant interests in leveraging ONNs into medical sensing, security screening, drug detection, and autonomous driving. However, due to the challenge of implementing reconfigurability, deploying multi-task learning (MTL) algorithms on ONNs requires re-building and duplicating the physical diffractive systems, which significantly degrades the energy and cost efficiency in practical application scenarios. This work presents a novel ONNs architecture, namely, \textit{RubikONNs}, which utilizes the physical properties of optical systems to encode multiple feed-forward functions by physically rotating the hardware similarly to rotating a \textit{Rubik's Cube}. To optimize MTL performance on Rubi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;GMU&#22242;&#38431;&#22312;SemEval-2023&#20849;&#20139;&#20219;&#21153;AfriSenti-SemEval&#20013;&#25152;&#20351;&#29992;&#30340;&#24773;&#24863;&#20998;&#26512;&#31995;&#32479;&#65292;&#20351;&#29992;AfroXLMR-large&#20316;&#20026;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24182;&#24341;&#20837;&#20102;&#22686;&#24378;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#22522;&#20110;Phylogeny&#30340;&#36866;&#37197;&#22120;&#35843;&#25972;&#20197;&#24471;&#21040;&#26368;&#20339;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.12979</link><description>&lt;p&gt;
&#22522;&#20110;Phylogeny&#30340;&#36866;&#37197;&#22120;&#22312;SemEval-2023&#31532;12&#39033;&#20219;&#21153;&#65306;&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
GMNLP at SemEval-2023 Task 12: Sentiment Analysis with Phylogeny-Based Adapters. (arXiv:2304.12979v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12979
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;GMU&#22242;&#38431;&#22312;SemEval-2023&#20849;&#20139;&#20219;&#21153;AfriSenti-SemEval&#20013;&#25152;&#20351;&#29992;&#30340;&#24773;&#24863;&#20998;&#26512;&#31995;&#32479;&#65292;&#20351;&#29992;AfroXLMR-large&#20316;&#20026;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24182;&#24341;&#20837;&#20102;&#22686;&#24378;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#22522;&#20110;Phylogeny&#30340;&#36866;&#37197;&#22120;&#35843;&#25972;&#20197;&#24471;&#21040;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;GMU&#23545;SemEval-2023&#20849;&#20139;&#20219;&#21153;AfriSenti-SemEval&#30340;&#24773;&#24863;&#20998;&#26512;&#31995;&#32479;&#12290;&#25105;&#20204;&#21442;&#19982;&#20102;&#21333;&#35821;&#35328;&#12289;&#22810;&#35821;&#35328;&#21644;&#38646;&#26679;&#26412;&#19977;&#20010;&#23376;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#20102;&#21021;&#22987;&#21270;&#20026;AfroXLMR-large&#30340;&#27169;&#22411;&#65292;&#23427;&#26159;&#19968;&#20010;&#22312;&#38750;&#27954;&#35821;&#35328;&#19978;&#39044;&#35757;&#32451;&#24182;&#30456;&#24212;&#24494;&#35843;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#22686;&#24378;&#30340;&#35757;&#32451;&#25968;&#25454;&#20197;&#21450;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#12290;&#38500;&#20102;&#24494;&#35843;&#22806;&#65292;&#25105;&#20204;&#36824;&#25191;&#34892;&#22522;&#20110;Phylogeny&#30340;&#36866;&#37197;&#22120;&#35843;&#25972;&#26469;&#21019;&#24314;&#22810;&#20010;&#27169;&#22411;&#65292;&#24182;&#23558;&#26368;&#20339;&#27169;&#22411;&#38598;&#25104;&#21040;&#26368;&#32456;&#25552;&#20132;&#20013;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#31532;5&#36712;&#36947;Amharic&#19978;&#21462;&#24471;&#20102;&#26368;&#20339;&#30340;F1&#20998;&#25968;&#65292;&#27604;&#35813;&#36712;&#36947;&#19978;&#31532;&#20108;&#26368;&#20339;&#24615;&#33021;&#31995;&#32479;&#39640;&#20986;6.2&#20010;F1&#20998;&#25968;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#21442;&#19982;&#25152;&#26377;15&#20010;&#36712;&#36947;&#30340;10&#20010;&#31995;&#32479;&#20013;&#25490;&#21517;&#31532;5&#12290;
&lt;/p&gt;
&lt;p&gt;
This report describes GMU's sentiment analysis system for the SemEval-2023 shared task AfriSenti-SemEval. We participated in all three sub-tasks: Monolingual, Multilingual, and Zero-Shot. Our approach uses models initialized with AfroXLMR-large, a pre-trained multilingual language model trained on African languages and fine-tuned correspondingly. We also introduce augmented training data along with original training data. Alongside finetuning, we perform phylogeny-based adapter tuning to create several models and ensemble the best models for the final submission. Our system achieves the best F1-score on track 5: Amharic, with 6.2 points higher F1-score than the second-best performing system on this track. Overall, our system ranks 5th among the 10 systems participating in all 15 tracks.
&lt;/p&gt;</description></item><item><title>&#36870;&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31867;&#29992;&#20110;&#24674;&#22797;&#33021;&#22815;&#35299;&#37322;&#19987;&#23478;&#20195;&#29702;&#28436;&#31034;&#30340;&#34892;&#20026;&#30340;&#22870;&#21169;&#20989;&#25968;&#30340;&#24378;&#22823;&#31639;&#27861;&#65292;&#28982;&#32780;&#65292;&#30001;&#20110;&#23384;&#22312;&#22810;&#20010;&#21487;&#35299;&#37322;&#35266;&#23519;&#34892;&#20026;&#30340;&#22870;&#21169;&#20989;&#25968;&#27495;&#20041;&#65292;&#37319;&#29992;&#21487;&#34892;&#22870;&#21169;&#38598;&#30340;&#26041;&#24335;&#32469;&#36807;&#36825;&#31181;&#38480;&#21046;&#12290;&#35813;&#35770;&#25991;&#22312;&#26377;&#38480;&#26102;&#38388;&#38382;&#39064;&#30340;IRL&#29702;&#35770;&#26041;&#38754;&#36827;&#34892;&#20102;&#25506;&#31350;&#65292;&#25552;&#20986;&#31532;&#19968;&#20010;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#26497;&#23567;&#26497;&#22823;&#19979;&#30028;&#12290;</title><link>http://arxiv.org/abs/2304.12966</link><description>&lt;p&gt;
&#36870;&#24378;&#21270;&#23398;&#20064;&#30340;&#29702;&#35770;&#30740;&#31350;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
Towards Theoretical Understanding of Inverse Reinforcement Learning. (arXiv:2304.12966v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12966
&lt;/p&gt;
&lt;p&gt;
&#36870;&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31867;&#29992;&#20110;&#24674;&#22797;&#33021;&#22815;&#35299;&#37322;&#19987;&#23478;&#20195;&#29702;&#28436;&#31034;&#30340;&#34892;&#20026;&#30340;&#22870;&#21169;&#20989;&#25968;&#30340;&#24378;&#22823;&#31639;&#27861;&#65292;&#28982;&#32780;&#65292;&#30001;&#20110;&#23384;&#22312;&#22810;&#20010;&#21487;&#35299;&#37322;&#35266;&#23519;&#34892;&#20026;&#30340;&#22870;&#21169;&#20989;&#25968;&#27495;&#20041;&#65292;&#37319;&#29992;&#21487;&#34892;&#22870;&#21169;&#38598;&#30340;&#26041;&#24335;&#32469;&#36807;&#36825;&#31181;&#38480;&#21046;&#12290;&#35813;&#35770;&#25991;&#22312;&#26377;&#38480;&#26102;&#38388;&#38382;&#39064;&#30340;IRL&#29702;&#35770;&#26041;&#38754;&#36827;&#34892;&#20102;&#25506;&#31350;&#65292;&#25552;&#20986;&#31532;&#19968;&#20010;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#26497;&#23567;&#26497;&#22823;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;IRL&#65289;&#26159;&#19968;&#31995;&#21015;&#29992;&#20110;&#24674;&#22797;&#19968;&#20010;&#33021;&#22815;&#35299;&#37322;&#19987;&#23478;&#20195;&#29702;&#28436;&#31034;&#30340;&#34892;&#20026;&#30340;&#22870;&#21169;&#20989;&#25968;&#30340;&#24378;&#22823;&#31639;&#27861;&#12290;IRL&#30340;&#19968;&#20010;&#24050;&#30693;&#38480;&#21046;&#26159;&#22312;&#36873;&#25321;&#22870;&#21169;&#20989;&#25968;&#26102;&#23384;&#22312;&#27495;&#20041;&#65292;&#30001;&#20110;&#23384;&#22312;&#21487;&#20197;&#35299;&#37322;&#35266;&#23519;&#21040;&#30340;&#34892;&#20026;&#30340;&#22810;&#20010;&#22870;&#21169;&#20989;&#25968;&#12290;&#26368;&#36817;&#65292;&#36890;&#36807;&#23558;IRL&#34920;&#36848;&#20026;&#20272;&#35745;&#21487;&#34892;&#22870;&#21169;&#38598;&#30340;&#38382;&#39064;&#65292;&#21363;&#19982;&#19987;&#23478;&#34892;&#20026;&#20860;&#23481;&#30340;&#22870;&#21169;&#33539;&#22260;&#30340;&#38382;&#39064;&#65292;&#24050;&#32463;&#32469;&#36807;&#20102;&#36825;&#31181;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#20855;&#26377;&#29983;&#25104;&#27169;&#22411;&#30340;&#26377;&#38480;&#26102;&#38388;&#38382;&#39064;&#30340;IRL&#29702;&#35770;&#19978;&#36808;&#20986;&#20102;&#19968;&#27493;&#12290;&#25105;&#20204;&#39318;&#20808;&#27491;&#24335;&#20171;&#32461;&#20102;&#20272;&#35745;&#21487;&#34892;&#22870;&#21169;&#38598;&#30340;&#38382;&#39064;&#12289;&#30456;&#24212;&#30340;PAC&#35201;&#27714;&#65292;&#24182;&#35752;&#35770;&#20102;&#29305;&#23450;&#31867;&#21035;&#22870;&#21169;&#30340;&#23646;&#24615;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#20026;&#20272;&#35745;&#21487;&#34892;&#22870;&#21169;&#38598;&#30340;&#38382;&#39064;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#26497;&#23567;&#26497;&#22823;&#19979;&#30028;&#65292;&#20854;&#20026;${\Omega}\Bigl( \frac{H^3SA}{...
&lt;/p&gt;
&lt;p&gt;
Inverse reinforcement learning (IRL) denotes a powerful family of algorithms for recovering a reward function justifying the behavior demonstrated by an expert agent. A well-known limitation of IRL is the ambiguity in the choice of the reward function, due to the existence of multiple rewards that explain the observed behavior. This limitation has been recently circumvented by formulating IRL as the problem of estimating the feasible reward set, i.e., the region of the rewards compatible with the expert's behavior. In this paper, we make a step towards closing the theory gap of IRL in the case of finite-horizon problems with a generative model. We start by formally introducing the problem of estimating the feasible reward set, the corresponding PAC requirement, and discussing the properties of particular classes of rewards. Then, we provide the first minimax lower bound on the sample complexity for the problem of estimating the feasible reward set of order ${\Omega}\Bigl( \frac{H^3SA}{
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#26041;&#27861;"&#21464;&#33394;&#40857;"&#65292;&#21487;&#20197;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#23454;&#29616;&#26356;&#21152;&#32784;&#29992;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#36890;&#36807;&#23545;&#25552;&#20379;&#30340;&#33391;&#24615;&#22270;&#20687;&#21644;&#26377;&#27602;&#22270;&#20687;&#30446;&#26631;&#26631;&#31614;&#20043;&#38388;&#30340;&#20851;&#31995;&#36827;&#34892;&#23545;&#27604;&#23398;&#20064;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#23454;&#39564;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.12961</link><description>&lt;p&gt;
&#21464;&#33394;&#40857;: &#36866;&#24212;&#23545;&#31561;&#38236;&#20687;&#20197;&#26893;&#20837;&#32784;&#29992;&#21518;&#38376;&#26469;&#36827;&#34892;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Chameleon: Adapting to Peer Images for Planting Durable Backdoors in Federated Learning. (arXiv:2304.12961v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#26041;&#27861;"&#21464;&#33394;&#40857;"&#65292;&#21487;&#20197;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#23454;&#29616;&#26356;&#21152;&#32784;&#29992;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#36890;&#36807;&#23545;&#25552;&#20379;&#30340;&#33391;&#24615;&#22270;&#20687;&#21644;&#26377;&#27602;&#22270;&#20687;&#30446;&#26631;&#26631;&#31614;&#20043;&#38388;&#30340;&#20851;&#31995;&#36827;&#34892;&#23545;&#27604;&#23398;&#20064;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#23454;&#39564;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20013;&#65292;&#20998;&#24067;&#24335;&#23458;&#25143;&#31471;&#19978;&#20256;&#20854;&#26412;&#22320;&#27169;&#22411;&#21040;&#20013;&#24515;&#26381;&#21153;&#22120;&#20197;&#32858;&#21512;&#25104;&#20840;&#23616;&#27169;&#22411;&#12290;&#24694;&#24847;&#23458;&#25143;&#31471;&#21487;&#20197;&#36890;&#36807;&#19978;&#20256;&#26377;&#27602;&#30340;&#26412;&#22320;&#27169;&#22411;&#26469;&#22312;&#20840;&#23616;&#27169;&#22411;&#20013;&#26893;&#20837;&#21518;&#38376;&#65292;&#23548;&#33268;&#20855;&#26377;&#29305;&#23450;&#27169;&#24335;&#30340;&#22270;&#20687;&#34987;&#38169;&#35823;&#20998;&#31867;&#20026;&#26576;&#20123;&#30446;&#26631;&#26631;&#31614;&#12290;&#24403;&#21069;&#25915;&#20987;&#26893;&#20837;&#30340;&#21518;&#38376;&#26159;&#19981;&#32784;&#29992;&#30340;&#65292;&#19968;&#26086;&#25915;&#20987;&#32773;&#20572;&#27490;&#27169;&#22411;&#20013;&#27602;&#65292;&#20415;&#20250;&#36805;&#36895;&#28040;&#22833;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;FL&#21518;&#38376;&#30340;&#32784;&#29992;&#24615;&#19982;&#33391;&#24615;&#22270;&#35937;&#21644;&#26377;&#27602;&#22270;&#35937;&#20043;&#38388;&#30340;&#20851;&#31995;(&#21363;&#22312;&#26412;&#22320;&#35757;&#32451;&#26399;&#38388;&#26631;&#31614;&#34987;&#32763;&#36716;&#20026;&#30446;&#26631;&#26631;&#31614;&#30340;&#22270;&#20687;)&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#21457;&#29616;&#21407;&#22987;&#22270;&#35937;&#21644;&#26377;&#27602;&#22270;&#35937;&#30340;&#30446;&#26631;&#26631;&#31614;&#23545;&#21518;&#38376;&#30340;&#32784;&#20037;&#24615;&#26377;&#20851;&#38190;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#25915;&#20987;&#65292;&#31216;&#20026;"&#21464;&#33394;&#40857;"&#65292;&#23427;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#36827;&#19968;&#27493;&#25918;&#22823;&#36825;&#31181;&#24433;&#21709;&#26469;&#23454;&#29616;&#26356;&#32784;&#29992;&#30340;&#21518;&#38376;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#25915;&#20987;&#22312;&#21508;&#31181;FL&#35774;&#32622;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a federated learning (FL) system, distributed clients upload their local models to a central server to aggregate into a global model. Malicious clients may plant backdoors into the global model through uploading poisoned local models, causing images with specific patterns to be misclassified into some target labels. Backdoors planted by current attacks are not durable, and vanish quickly once the attackers stop model poisoning. In this paper, we investigate the connection between the durability of FL backdoors and the relationships between benign images and poisoned images (i.e., the images whose labels are flipped to the target label during local training). Specifically, benign images with the original and the target labels of the poisoned images are found to have key effects on backdoor durability. Consequently, we propose a novel attack, Chameleon, which utilizes contrastive learning to further amplify such effects towards a more durable backdoor. Extensive experiments demonstrat
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22870;&#21169;&#20998;&#35299;&#19982;&#25277;&#35937;&#21160;&#20316;&#31354;&#38388;&#30456;&#32467;&#21512;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#25552;&#20379;&#22522;&#20110;&#23545;&#35937;&#23646;&#24615;&#30340;&#26126;&#30830;&#39640;&#23618;&#27425;&#35299;&#37322;&#65292;&#36991;&#20813;&#20102;&#35299;&#37322;&#26426;&#22120;&#20154;&#34892;&#20026;&#30340;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.12958</link><description>&lt;p&gt;
&#23545;&#39640;&#27700;&#24179;&#26426;&#22120;&#20154;&#35299;&#37322;&#20013;&#30340;&#22870;&#21169;&#20998;&#35299;&#36827;&#34892;&#26356;&#28145;&#20837;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Closer Look at Reward Decomposition for High-Level Robotic Explanations. (arXiv:2304.12958v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12958
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22870;&#21169;&#20998;&#35299;&#19982;&#25277;&#35937;&#21160;&#20316;&#31354;&#38388;&#30456;&#32467;&#21512;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#25552;&#20379;&#22522;&#20110;&#23545;&#35937;&#23646;&#24615;&#30340;&#26126;&#30830;&#39640;&#23618;&#27425;&#35299;&#37322;&#65292;&#36991;&#20813;&#20102;&#35299;&#37322;&#26426;&#22120;&#20154;&#34892;&#20026;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21521;&#20154;&#31867;&#35299;&#37322;&#26234;&#33021;&#26426;&#22120;&#20154;&#30340;&#34892;&#20026;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#36825;&#26159;&#30001;&#20110;&#23427;&#20204;&#38590;&#20197;&#29702;&#35299;&#30340;&#33258;&#20307;&#24863;&#29366;&#24577;&#12289;&#22810;&#21464;&#30340;&#20013;&#38388;&#30446;&#26631;&#21644;&#20854;&#32467;&#26524;&#30340;&#19981;&#21487;&#39044;&#27979;&#24615;&#12290;&#27492;&#22806;&#65292;&#23545;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#19968;&#27493;&#35299;&#37322;&#21487;&#33021;&#26159;&#27169;&#31946;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#26410;&#33021;&#22312;&#27599;&#20010;&#36716;&#25442;&#26102;&#32771;&#34385;&#21040;&#20195;&#29702;&#30340;&#26410;&#26469;&#34892;&#20026;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#35299;&#37322;&#26426;&#22120;&#20154;&#34892;&#20026;&#30340;&#22797;&#26434;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#26144;&#23556;&#21040;&#20219;&#21153;&#29305;&#23450;&#22522;&#20803;&#30340;&#25277;&#35937;&#21160;&#20316;&#65292;&#25105;&#20204;&#36991;&#20813;&#20102;&#22312;&#31227;&#21160;&#23618;&#38754;&#19978;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#23558;&#22870;&#21169;&#20998;&#35299;&#65288;RD&#65289;&#19982;&#25277;&#35937;&#21160;&#20316;&#31354;&#38388;&#32467;&#21512;&#36215;&#26469;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#22522;&#20110;&#20219;&#21153;&#20013;&#30340;&#23545;&#35937;&#23646;&#24615;&#65292;&#23454;&#29616;&#20102;&#26126;&#30830;&#30340;&#39640;&#23618;&#27425;&#35299;&#37322;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#20004;&#20010;&#26426;&#22120;&#20154;&#22330;&#26223;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#20998;&#26512;&#26469;&#35777;&#26126;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;RD&#35299;&#37322;&#30340;&#36755;&#20986;&#25991;&#29289;&#20013;&#30340;&#21487;&#35270;&#21644;&#25991;&#26412;&#35299;&#37322;&#65292;&#26131;&#20110;&#20154;&#31867;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explaining the behavior of intelligent agents such as robots to humans is challenging due to their incomprehensible proprioceptive states, variational intermediate goals, and resultant unpredictability. Moreover, one-step explanations for reinforcement learning agents can be ambiguous as they fail to account for the agent's future behavior at each transition, adding to the complexity of explaining robot actions. By leveraging abstracted actions that map to task-specific primitives, we avoid explanations on the movement level. Our proposed framework combines reward decomposition (RD) with abstracted action spaces into an explainable learning framework, allowing for non-ambiguous and high-level explanations based on object properties in the task. We demonstrate the effectiveness of our framework through quantitative and qualitative analysis of two robot scenarios, showcasing visual and textual explanations, from output artifacts of RD explanation, that are easy for humans to comprehend. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36793;&#30028;&#25935;&#24863;&#24615;&#30340;&#31070;&#32463;&#38544;&#24335;&#24418;&#29366;&#32534;&#36753;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#23545;&#21442;&#25968;&#36827;&#34892;&#25200;&#21160;&#26469;&#23454;&#29616;&#25351;&#23450;&#30340;&#20840;&#23616;&#24418;&#29366;&#21464;&#24418;&#65292;&#24182;&#26681;&#25454;&#20808;&#39564;&#23545;&#20854;&#20313;&#37096;&#20998;&#36827;&#34892;&#35843;&#25972;&#12290;&#26041;&#27861;&#19981;&#21463;&#27169;&#22411;&#38480;&#21046;&#65292;&#24182;&#21487;&#20248;&#21270;&#21644;&#32422;&#26463;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2304.12951</link><description>&lt;p&gt;
&#21033;&#29992;&#36793;&#30028;&#25935;&#24863;&#24230;&#30340;&#31070;&#32463;&#38544;&#24335;&#24418;&#29366;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Neural Implicit Shape Editing using Boundary Sensitivity. (arXiv:2304.12951v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36793;&#30028;&#25935;&#24863;&#24615;&#30340;&#31070;&#32463;&#38544;&#24335;&#24418;&#29366;&#32534;&#36753;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#23545;&#21442;&#25968;&#36827;&#34892;&#25200;&#21160;&#26469;&#23454;&#29616;&#25351;&#23450;&#30340;&#20840;&#23616;&#24418;&#29366;&#21464;&#24418;&#65292;&#24182;&#26681;&#25454;&#20808;&#39564;&#23545;&#20854;&#20313;&#37096;&#20998;&#36827;&#34892;&#35843;&#25972;&#12290;&#26041;&#27861;&#19981;&#21463;&#27169;&#22411;&#38480;&#21046;&#65292;&#24182;&#21487;&#20248;&#21270;&#21644;&#32422;&#26463;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#33021;&#22815;&#32039;&#20945;&#20445;&#23384;&#35814;&#32454;&#21644;&#24179;&#28369;&#30340;&#24418;&#29366;&#24182;&#36731;&#26494;&#22320;&#36827;&#34892;&#25299;&#25169;&#21464;&#21270;&#65292;&#31070;&#32463;&#22330;&#20316;&#20026;&#19968;&#31181;&#20960;&#20309;&#34920;&#31034;&#27491;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#19982;&#20256;&#32479;&#30340;&#20960;&#20309;&#34920;&#31034;&#30456;&#27604;&#65292;&#31070;&#32463;&#34920;&#31034;&#19981;&#20801;&#35768;&#29992;&#25143;&#23545;&#24418;&#29366;&#36827;&#34892;&#30452;&#35266;&#30340;&#25511;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#21033;&#29992;&#36793;&#30028;&#25935;&#24863;&#24615;&#26469;&#34920;&#36798;&#21442;&#25968;&#25200;&#21160;&#22914;&#20309;&#31227;&#21160;&#24418;&#29366;&#36793;&#30028;&#12290;&#36825;&#26679;&#21487;&#20197;&#35299;&#37322;&#27599;&#20010;&#21487;&#23398;&#20064;&#21442;&#25968;&#30340;&#24433;&#21709;&#65292;&#24182;&#30740;&#31350;&#21487;&#23454;&#29616;&#30340;&#21464;&#24418;&#12290;&#21033;&#29992;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25191;&#34892;&#20960;&#20309;&#32534;&#36753;&#65306;&#25214;&#21040;&#26368;&#20339;&#36817;&#20284;&#20840;&#23616;&#39044;&#35774;&#21464;&#24418;&#30340;&#21442;&#25968;&#26356;&#26032;&#12290;&#20165;&#22312;&#23616;&#37096;&#25351;&#23450;&#21464;&#24418;&#20801;&#35768;&#24418;&#29366;&#30340;&#20854;&#20313;&#37096;&#20998;&#26681;&#25454;&#19968;&#20123;&#20808;&#39564;&#65292;&#20363;&#22914;&#35821;&#20041;&#25110;&#21464;&#24418;&#21018;&#24615;&#32780;&#21457;&#29983;&#21464;&#21270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#21463;&#20854;&#35757;&#32451;&#27169;&#22411;&#30340;&#38480;&#21046;&#65292;&#24182;&#19988;&#21487;&#20197;&#23601;&#22320;&#26356;&#26032;&#31070;&#32463;&#32593;&#32476;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36793;&#30028;&#25935;&#24863;&#24230;&#22914;&#20309;&#24110;&#21161;&#20248;&#21270;&#21644;&#32422;&#26463;&#30446;&#26631;&#65288;&#22914;&#34920;&#38754;&#37325;&#24314;&#21644;&#21435;&#22122;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural fields are receiving increased attention as a geometric representation due to their ability to compactly store detailed and smooth shapes and easily undergo topological changes. Compared to classic geometry representations, however, neural representations do not allow the user to exert intuitive control over the shape. Motivated by this, we leverage boundary sensitivity to express how perturbations in parameters move the shape boundary. This allows to interpret the effect of each learnable parameter and study achievable deformations. With this, we perform geometric editing: finding a parameter update that best approximates a globally prescribed deformation. Prescribing the deformation only locally allows the rest of the shape to change according to some prior, such as semantics or deformation rigidity. Our method is agnostic to the model its training and updates the NN in-place. Furthermore, we show how boundary sensitivity helps to optimize and constrain objectives (such as sur
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21152;&#36895;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#30340;Shot&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20943;&#23567;&#25968;&#25454;&#38598;&#22823;&#23567;&#21644;&#33258;&#36866;&#24212;Shot&#20998;&#37197;&#31561;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;Shot&#25968;&#37327;&#30340;&#20248;&#21270;&#19982;&#20934;&#30830;&#24230;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2304.12950</link><description>&lt;p&gt;
&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#20013;&#30340;Shot&#20248;&#21270;&#29992;&#20110;&#21152;&#36895;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Shot Optimization in Quantum Machine Learning Architectures to Accelerate Training. (arXiv:2304.12950v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12950
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21152;&#36895;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#30340;Shot&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20943;&#23567;&#25968;&#25454;&#38598;&#22823;&#23567;&#21644;&#33258;&#36866;&#24212;Shot&#20998;&#37197;&#31561;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;Shot&#25968;&#37327;&#30340;&#20248;&#21270;&#19982;&#20934;&#30830;&#24230;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Shot&#20248;&#21270;&#30340;QML&#27169;&#22411;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#26368;&#23567;&#12290;&#25105;&#20204;&#20351;&#29992;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;QML&#27169;&#22411;&#23545;MNIST&#21644;FMNIST&#25968;&#25454;&#38598;&#20013;&#30340;&#20998;&#31867;&#20219;&#21153;&#36827;&#34892;&#27979;&#35797;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25195;&#25551;&#25968;&#25454;&#38598;&#30340;&#30701;&#29256;&#26412;&#21644;&#23436;&#25972;&#29256;&#26412;&#30340;Shot&#25968;&#30446;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35757;&#32451;&#23436;&#25972;&#29256;&#26412;&#27604;&#35757;&#32451;&#30701;&#29256;&#26412;&#25552;&#20379;&#20102;5-6%&#30340;&#27979;&#35797;&#20934;&#30830;&#24230;&#65292;&#24182;&#19988;&#35757;&#32451;&#20013;&#30340;Shot&#25968;&#37327;&#21487;&#39640;&#36798;10&#20493;&#12290;&#22240;&#27492;&#65292;&#21487;&#20197;&#20943;&#23567;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#20197;&#21152;&#36895;&#35757;&#32451;&#26102;&#38388;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;Shot&#20998;&#37197;&#26041;&#27861;&#29992;&#20110;&#30701;&#29256;&#26412;&#25968;&#25454;&#38598;&#65292;&#20197;&#20248;&#21270;&#35757;&#32451;&#21608;&#26399;&#20869;&#30340;Shot&#25968;&#37327;&#65292;&#24182;&#35780;&#20272;&#20854;&#23545;&#20998;&#31867;&#20934;&#30830;&#24230;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#20351;&#29992;&#65288;a&#65289;&#32447;&#24615;&#20989;&#25968;&#65292;&#20854;&#20013;Shot&#25968;&#37327;&#38543;&#30528;&#35757;&#32451;&#21608;&#26399;&#32447;&#24615;&#20943;&#23569;&#65292;&#21644;&#65288;b&#65289;&#27493;&#20989;&#25968;&#65292;&#20854;&#20013;Shot&#25968;&#37327;&#38543;&#30528;&#35757;&#32451;&#21608;&#26399;&#27493;&#36827;&#24335;&#20943;&#23569;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#65292;&#20943;&#23569;Shot&#25968;&#37327;&#20250;&#23548;&#33268;0.01&#30340;&#25439;&#22833;&#22686;&#21152;&#21644;&#32422;4%&#65288;1%&#65289;&#30340;&#27979;&#35797;&#20934;&#30830;&#24230;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose shot optimization method for QML models at the expense of minimal impact on model performance. We use classification task as a test case for MNIST and FMNIST datasets using a hybrid quantum-classical QML model. First, we sweep the number of shots for short and full versions of the dataset. We observe that training the full version provides 5-6% higher testing accuracy than short version of dataset with up to 10X higher number of shots for training. Therefore, one can reduce the dataset size to accelerate the training time. Next, we propose adaptive shot allocation on short version dataset to optimize the number of shots over training epochs and evaluate the impact on classification accuracy. We use a (a) linear function where the number of shots reduce linearly with epochs, and (b) step function where the number of shots reduce in step with epochs. We note around 0.01 increase in loss and around 4% (1%) reduction in testing accuracy for reduction in shots by u
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;eFAT&#65292;&#29992;&#20110;&#35745;&#31639;DNN&#21152;&#36895;&#22120;&#30340;&#38887;&#24615;&#24182;&#23558;&#20854;&#26144;&#23556;&#21040;&#30456;&#24212;&#30340;&#37325;&#35757;&#32451;&#37327;&#65292;&#36890;&#36807;&#20998;&#32452;&#21644;&#34701;&#21512;&#25925;&#38556;&#22320;&#22270;&#20943;&#23569;FAT&#30340;&#24320;&#38144;&#65292;&#21516;&#26102;&#25552;&#39640;&#23481;&#38169;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.12949</link><description>&lt;p&gt;
eFAT&#65306;&#25913;&#36827;DNN&#30828;&#20214;&#21152;&#36895;&#22120;&#20013;&#30340;&#25925;&#38556;&#24863;&#30693;&#35757;&#32451;&#20197;&#25552;&#39640;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
eFAT: Improving the Effectiveness of Fault-Aware Training for Mitigating Permanent Faults in DNN Hardware Accelerators. (arXiv:2304.12949v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12949
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;eFAT&#65292;&#29992;&#20110;&#35745;&#31639;DNN&#21152;&#36895;&#22120;&#30340;&#38887;&#24615;&#24182;&#23558;&#20854;&#26144;&#23556;&#21040;&#30456;&#24212;&#30340;&#37325;&#35757;&#32451;&#37327;&#65292;&#36890;&#36807;&#20998;&#32452;&#21644;&#34701;&#21512;&#25925;&#38556;&#22320;&#22270;&#20943;&#23569;FAT&#30340;&#24320;&#38144;&#65292;&#21516;&#26102;&#25552;&#39640;&#23481;&#38169;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25925;&#38556;&#24863;&#30693;&#35757;&#32451;&#65288;FAT&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#39640;&#25928;&#24212;&#23545;DNN&#21152;&#36895;&#22120;&#27704;&#20037;&#24615;&#25925;&#38556;&#30340;&#25216;&#26415;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#22312;&#20302;&#21644;&#20013;&#31561;&#25925;&#38556;&#29575;&#19979;&#25552;&#20379;&#25925;&#38556;&#32531;&#35299;&#32780;&#19981;&#20250;&#24102;&#26469;&#26174;&#30528;&#30340;&#24615;&#33021;&#25110;&#20934;&#30830;&#24615;&#25439;&#22833;&#12290;&#28982;&#32780;&#65292;&#24403;&#29992;&#20110;&#38754;&#21521;&#22797;&#26434;AI&#24212;&#29992;&#31243;&#24207;&#30340;&#22823;&#22411;DNN&#26102;&#65292;&#23427;&#20250;&#23548;&#33268;&#38750;&#24120;&#39640;&#30340;&#37325;&#35757;&#32451;&#24320;&#38144;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#27599;&#20010;&#33455;&#29255;&#30340;&#25925;&#38556;&#27169;&#24335;&#37117;&#21487;&#33021;&#19981;&#21516;&#65292;&#22240;&#27492;&#38656;&#35201;&#36880;&#19968;&#20026;&#27599;&#20010;&#25925;&#38556;&#33455;&#29255;&#36827;&#34892;FAT&#65292;&#32771;&#34385;&#21040;&#20854;&#29420;&#29305;&#30340;&#25925;&#38556;&#22320;&#22270;&#65292;&#36825;&#36827;&#19968;&#27493;&#21152;&#21095;&#20102;&#38382;&#39064;&#12290;&#20026;&#20102;&#20943;&#23569;FAT&#30340;&#24320;&#38144;&#21516;&#26102;&#32500;&#25345;&#20854;&#20248;&#21183;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#65288;1&#65289;&#38887;&#24615;&#39537;&#21160;&#30340;&#37325;&#35757;&#32451;&#37327;&#36873;&#25321;&#21644;&#65288;2&#65289;&#22810;&#20010;&#25925;&#38556;&#22320;&#22270;&#65288;&#23646;&#20110;&#19981;&#21516;&#33455;&#29255;&#65289;&#30340;&#38887;&#24615;&#39537;&#21160;&#30340;&#20998;&#32452;&#21644;&#34701;&#21512;&#65292;&#20197;&#23545;&#19968;&#32452;&#25925;&#38556;&#33455;&#29255;&#36827;&#34892;&#21512;&#24182;&#37325;&#35757;&#32451;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20123;&#27010;&#24565;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;eFAT&#65292;&#29992;&#20110;&#35745;&#31639;DNN&#21152;&#36895;&#22120;&#30340;&#38887;&#24615;&#24182;&#23558;&#20854;&#26144;&#23556;&#21040;&#30456;&#24212;&#30340;&#37325;&#35757;&#32451;&#37327;&#12290;eFAT&#36824;&#26681;&#25454;&#25925;&#38556;&#22320;&#22270;&#23545;&#23384;&#22312;&#25925;&#38556;&#30340;&#33455;&#29255;&#36827;&#34892;&#20998;&#32452;&#65292;&#24182;&#23545;&#32452;&#36827;&#34892;&#21512;&#24182;&#37325;&#35757;&#32451;&#65292;&#21516;&#26102;&#32771;&#34385;&#32452;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;DNN&#21152;&#36895;&#22120;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;eFAT&#26174;&#33879;&#38477;&#20302;&#20102;FAT&#30340;&#37325;&#35757;&#32451;&#24320;&#38144;&#65292;&#21516;&#26102;&#25552;&#20379;&#31867;&#20284;&#25110;&#26356;&#22909;&#30340;&#23481;&#38169;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fault-Aware Training (FAT) has emerged as a highly effective technique for addressing permanent faults in DNN accelerators, as it offers fault mitigation without significant performance or accuracy loss, specifically at low and moderate fault rates. However, it leads to very high retraining overheads, especially when used for large DNNs designed for complex AI applications. Moreover, as each fabricated chip can have a distinct fault pattern, FAT is required to be performed for each faulty chip individually, considering its unique fault map, which further aggravates the problem. To reduce the overheads of FAT while maintaining its benefits, we propose (1) the concepts of resilience-driven retraining amount selection, and (2) resilience-driven grouping and fusion of multiple fault maps (belonging to different chips) to perform consolidated retraining for a group of faulty chips. To realize these concepts, in this work, we present a novel framework, eFAT, that computes the resilience of a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22312;&#32447;&#31639;&#27861;&#30340;&#20302;&#24310;&#36831;&#20056;&#27861;&#22120;&#65292;&#23427;&#33021;&#36890;&#36807;&#20301;&#32423;&#27969;&#27700;&#32447;&#25552;&#39640;&#21534;&#21520;&#37327;&#24182;&#20943;&#23569;&#24310;&#36831;&#65292;&#24182;&#36866;&#29992;&#20110;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#21487;&#37325;&#26500;&#35774;&#22791;&#19978;&#23454;&#26045;&#12290;</title><link>http://arxiv.org/abs/2304.12946</link><description>&lt;p&gt;
&#20869;&#31215;&#25968;&#32452;&#20302;&#24310;&#36831;&#22312;&#32447;&#20056;&#27861;&#22120;
&lt;/p&gt;
&lt;p&gt;
Low-Latency Online Multiplier with Reduced Activities and Minimized Interconnect for Inner Product Arrays. (arXiv:2304.12946v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12946
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22312;&#32447;&#31639;&#27861;&#30340;&#20302;&#24310;&#36831;&#20056;&#27861;&#22120;&#65292;&#23427;&#33021;&#36890;&#36807;&#20301;&#32423;&#27969;&#27700;&#32447;&#25552;&#39640;&#21534;&#21520;&#37327;&#24182;&#20943;&#23569;&#24310;&#36831;&#65292;&#24182;&#36866;&#29992;&#20110;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#21487;&#37325;&#26500;&#35774;&#22791;&#19978;&#23454;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20056;&#27861;&#26159;&#35768;&#22810;&#29616;&#20195;&#24212;&#29992;&#31243;&#24207;&#65288;&#21253;&#25324;&#20449;&#21495;&#22788;&#29702;&#21644;&#31070;&#32463;&#32593;&#32476;&#65289;&#20013;&#19981;&#21487;&#25110;&#32570;&#24182;&#19988;&#26159;&#26680;&#24515;&#25805;&#20316;&#20043;&#19968;&#12290;&#20256;&#32479;&#30340;&#20174;&#21491;&#33267;&#24038;&#20056;&#27861;&#22120;&#22312;&#36825;&#20123;&#24212;&#29992;&#31243;&#24207;&#20013;&#24191;&#27867;&#36129;&#29486;&#20102;&#21151;&#32791;&#12289;&#38754;&#31215;&#21033;&#29992;&#29575;&#21644;&#20851;&#38190;&#36335;&#24452;&#24310;&#36831;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22312;&#32447;&#31639;&#27861;&#25110;&#20174;&#24038;&#33267;&#21491;&#31639;&#27861;&#30340;&#20302;&#24310;&#36831;&#20056;&#27861;&#22120;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#36890;&#36807;&#20301;&#32423;&#27969;&#27700;&#32447;&#25552;&#39640;&#21534;&#21520;&#37327;&#24182;&#20943;&#23569;&#24310;&#36831;&#12290;&#30001;&#20110;&#22312;&#32447;&#31639;&#27861;&#37319;&#29992;&#20102;&#26368;&#39640;&#20301;&#25968;&#23383;&#20248;&#20808;&#27169;&#24335;&#30340;&#25805;&#20316;&#26041;&#24335;&#65292;&#22240;&#27492;&#33021;&#22815;&#26080;&#35770;&#25968;&#25454;&#20381;&#36182;&#24615;&#22914;&#20309;&#37117;&#36827;&#34892;&#37325;&#21472;&#30340;&#36830;&#32493;&#25805;&#20316;&#12290;&#20026;&#20102;&#29983;&#25104;&#26368;&#39640;&#20301;&#25968;&#23383;&#20248;&#20808;&#65292;&#23427;&#20351;&#29992;&#20887;&#20313;&#25968;&#23383;&#31995;&#32479;&#65292;&#22240;&#27492;&#21487;&#20197;&#36827;&#34892;&#26080;&#36827;&#20301;&#21152;&#27861;&#65292;&#22240;&#27492;&#31639;&#26415;&#25805;&#20316;&#30340;&#24310;&#36831;&#19982;&#25805;&#20316;&#25968;&#20301;&#23485;&#26080;&#20851;&#12290;&#36825;&#20123;&#25805;&#20316;&#36880;&#20301;&#20018;&#34892;&#22320;&#20174;&#24038;&#21040;&#21491;&#25191;&#34892;&#65292;&#36825;&#20351;&#24471;&#23427;&#36866;&#29992;&#20110;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#21487;&#37325;&#26500;&#35774;&#22791;&#19978;&#23454;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multiplication is indispensable and is one of the core operations in many modern applications including signal processing and neural networks. Conventional right-to-left (RL) multiplier extensively contributes to the power consumption, area utilization and critical path delay in such applications. This paper proposes a low latency multiplier based on online or left-to-right (LR) arithmetic which can increase throughput and reduce latency by digit-level pipelining. Online arithmetic enables overlapping successive operations regardless of data dependency because of the most significant digit first mode of operation. To produce most significant digit first, it uses redundant number system and we can have a carry-free addition, therefore, the delay of the arithmetic operation is independent of operand bit width. The operations are performed digit by digit serially from left to right which allows gradual increase in the slice activities making it suitable for implementation on reconfigurabl
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20351;&#29992;&#23398;&#20064;&#30340;&#21160;&#24577;&#28508;&#22312;&#26223;&#35266;&#26469;&#24314;&#27169;&#28508;&#22312;&#32467;&#26500;&#65292;&#20174;&#32780;&#23558;&#28508;&#22312;&#36941;&#21382;&#20316;&#20026;&#26679;&#26412;&#27839;&#30528;&#26223;&#35266;&#26799;&#24230;&#30340;&#27969;&#21160;&#36827;&#34892;&#65292;&#20197;&#23454;&#29616;&#35299;&#32544;&#65292;&#24182;&#36890;&#36807;&#20998;&#31867;&#22120;&#36827;&#34892;&#32422;&#26463;&#12290;</title><link>http://arxiv.org/abs/2304.12944</link><description>&lt;p&gt;
&#28508;&#22312;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#28508;&#22312;&#36941;&#21382;&#20316;&#20026;&#28508;&#22312;&#27969;&#30340;&#28508;&#22312;&#36335;&#32447;
&lt;/p&gt;
&lt;p&gt;
Latent Traversals in Generative Models as Potential Flows. (arXiv:2304.12944v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12944
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20351;&#29992;&#23398;&#20064;&#30340;&#21160;&#24577;&#28508;&#22312;&#26223;&#35266;&#26469;&#24314;&#27169;&#28508;&#22312;&#32467;&#26500;&#65292;&#20174;&#32780;&#23558;&#28508;&#22312;&#36941;&#21382;&#20316;&#20026;&#26679;&#26412;&#27839;&#30528;&#26223;&#35266;&#26799;&#24230;&#30340;&#27969;&#21160;&#36827;&#34892;&#65292;&#20197;&#23454;&#29616;&#35299;&#32544;&#65292;&#24182;&#36890;&#36807;&#20998;&#31867;&#22120;&#36827;&#34892;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#26368;&#36817;&#36827;&#23637;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#30340;&#28508;&#22312;&#31354;&#38388;&#30340;&#22522;&#26412;&#32467;&#26500;&#20173;&#28982;&#24456;&#19981;&#22909;&#29702;&#35299;&#65292;&#22240;&#27492;&#25191;&#34892;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#28508;&#22312;&#36941;&#21382;&#30340;&#20219;&#21153;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#30740;&#31350;&#25361;&#25112;&#12290;&#22823;&#22810;&#25968;&#20043;&#21069;&#30340;&#24037;&#20316;&#26088;&#22312;&#36890;&#36807;&#32447;&#24615;&#24314;&#27169;&#28508;&#22312;&#32467;&#26500;&#65292;&#24182;&#25214;&#21040;&#30456;&#24212;&#30340;&#32447;&#24615;&#26041;&#21521;&#65292;&#20174;&#32780;&#20135;&#29983;&#8220;&#35299;&#32544;&#8221;&#30340;&#20195;&#25968;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#35758;&#25913;&#20026;&#20351;&#29992;&#23398;&#20064;&#30340;&#21160;&#24577;&#28508;&#22312;&#26223;&#35266;&#26469;&#24314;&#27169;&#28508;&#22312;&#32467;&#26500;&#65292;&#20174;&#32780;&#23558;&#28508;&#22312;&#36941;&#21382;&#20316;&#20026;&#26679;&#26412;&#27839;&#30528;&#26223;&#35266;&#26799;&#24230;&#30340;&#27969;&#21160;&#36827;&#34892;&#12290;&#36825;&#20123;&#28508;&#22312;&#26223;&#35266;&#21463;&#21040;&#29289;&#29702;&#23398;&#12289;&#26368;&#20248;&#36816;&#36755;&#21644;&#31070;&#32463;&#31185;&#23398;&#30340;&#21551;&#21457;&#65292;&#34987;&#20316;&#20026;&#29289;&#29702;&#19978;&#29616;&#23454;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#26469;&#23398;&#20064;&#65292;&#20174;&#32780;&#20801;&#35768;&#23427;&#20204;&#22312;&#31354;&#38388;&#21644;&#26102;&#38388;&#19978;&#20855;&#26377;&#28789;&#27963;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#35299;&#32544;&#65292;&#21516;&#26102;&#23398;&#20064;&#20102;&#22810;&#20010;&#21183;&#33021;&#65292;&#24182;&#36890;&#36807;&#20998;&#31867;&#22120;&#36827;&#34892;&#32422;&#26463;&#65292;&#20351;&#20854;&#20855;&#26377;&#26126;&#26174;&#24046;&#24322;&#19988;&#22312;&#35821;&#20041;&#19978;&#33258;&#25105;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the significant recent progress in deep generative models, the underlying structure of their latent spaces is still poorly understood, thereby making the task of performing semantically meaningful latent traversals an open research challenge. Most prior work has aimed to solve this challenge by modeling latent structures linearly, and finding corresponding linear directions which result in `disentangled' generations. In this work, we instead propose to model latent structures with a learned dynamic potential landscape, thereby performing latent traversals as the flow of samples down the landscape's gradient. Inspired by physics, optimal transport, and neuroscience, these potential landscapes are learned as physically realistic partial differential equations, thereby allowing them to flexibly vary over both space and time. To achieve disentanglement, multiple potentials are learned simultaneously, and are constrained by a classifier to be distinct and semantically self-consisten
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#26694;&#26550;CROCO&#65292;&#33021;&#22815;&#29983;&#25104;&#24378;&#40065;&#26834;&#24615;&#30340;&#35299;&#37322;&#65292;&#21516;&#26102;&#26377;&#25928;&#22320;&#24179;&#34913;&#20102;&#40065;&#26834;&#24615;&#21644;&#35299;&#37322;&#31034;&#20363;&#30340;&#25509;&#36817;&#24230;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2304.12943</link><description>&lt;p&gt;
&#29983;&#25104;&#24378;&#40065;&#26834;&#24615;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Generating robust counterfactual explanations. (arXiv:2304.12943v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12943
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#26694;&#26550;CROCO&#65292;&#33021;&#22815;&#29983;&#25104;&#24378;&#40065;&#26834;&#24615;&#30340;&#35299;&#37322;&#65292;&#21516;&#26102;&#26377;&#25928;&#22320;&#24179;&#34913;&#20102;&#40065;&#26834;&#24615;&#21644;&#35299;&#37322;&#31034;&#20363;&#30340;&#25509;&#36817;&#24230;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#30340;&#35299;&#37322;&#24050;&#32463;&#25104;&#20026;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#20027;&#27969;&#12290;&#36825;&#19968;&#30452;&#35266;&#30340;&#38472;&#36848;&#35753;&#29992;&#25143;&#29702;&#35299;&#22312;&#32473;&#23450;&#24773;&#20917;&#19979;&#65292;&#20026;&#25913;&#21464;&#27169;&#22411;&#39044;&#27979;&#32780;&#24517;&#39035;&#36827;&#34892;&#30340;&#23567;&#20294;&#24517;&#35201;&#30340;&#26356;&#25913;&#12290;&#21453;&#20107;&#23454;&#30340;&#36136;&#37327;&#21462;&#20915;&#20110;&#22810;&#20010;&#26631;&#20934;&#65306;&#30495;&#23454;&#24615;&#12289;&#21487;&#34892;&#24615;&#12289;&#26377;&#25928;&#24615;&#12289;&#40065;&#26834;&#24615;&#31561;&#31561;&#12290;&#26412;&#25991;&#20851;&#27880;&#21453;&#20107;&#23454;&#30340;&#40065;&#26834;&#24615;&#27010;&#24565;&#65292;&#26356;&#20855;&#20307;&#22320;&#65292;&#20851;&#27880;&#21453;&#20107;&#23454;&#36755;&#20837;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#12290;&#36825;&#31181;&#40065;&#26834;&#24615;&#23588;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#28041;&#21450;&#21040;&#21453;&#20107;&#23454;&#30340;&#40065;&#26834;&#24615;&#21644;&#19982;&#35201;&#35299;&#37322;&#30340;&#31034;&#20363;&#30340;&#25509;&#36817;&#24230;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;CROCO&#65292;&#23427;&#33021;&#22815;&#26377;&#25928;&#22320;&#31649;&#29702;&#36825;&#31181;&#26435;&#34913;&#65292;&#29983;&#25104;&#20855;&#26377;&#24378;&#40065;&#26834;&#24615;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#65292;&#24182;&#20445;&#35777;&#29992;&#25143;&#20855;&#26377;&#26368;&#23567;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#35780;&#20272;&#30830;&#35748;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#30456;&#20851;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual explanations have become a mainstay of the XAI field. This particularly intuitive statement allows the user to understand what small but necessary changes would have to be made to a given situation in order to change a model prediction. The quality of a counterfactual depends on several criteria: realism, actionability, validity, robustness, etc. In this paper, we are interested in the notion of robustness of a counterfactual. More precisely, we focus on robustness to counterfactual input changes. This form of robustness is particularly challenging as it involves a trade-off between the robustness of the counterfactual and the proximity with the example to explain. We propose a new framework, CROCO, that generates robust counterfactuals while managing effectively this trade-off, and guarantees the user a minimal robustness. An empirical evaluation on tabular datasets confirms the relevance and effectiveness of our approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#36827;&#21270;&#31574;&#30053;&#21644;3D&#28210;&#26579;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#20351;&#33402;&#26415;&#23478;&#33021;&#22815;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#25277;&#35937;&#19977;&#32500;&#33402;&#26415;&#30340;&#21019;&#24847;&#24819;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.12932</link><description>&lt;p&gt;
&#36827;&#21270;&#19977;&#32500;&#25277;&#35937;&#33402;&#26415;&#65306;&#36890;&#36807;&#35821;&#35328;&#21305;&#37197;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
Evolving Three Dimension (3D) Abstract Art: Fitting Concepts by Language. (arXiv:2304.12932v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12932
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#36827;&#21270;&#31574;&#30053;&#21644;3D&#28210;&#26579;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#20351;&#33402;&#26415;&#23478;&#33021;&#22815;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#25277;&#35937;&#19977;&#32500;&#33402;&#26415;&#30340;&#21019;&#24847;&#24819;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#21019;&#24847;&#22312;&#29616;&#20195;&#25277;&#35937;&#33402;&#26415;&#20013;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#65292;&#20351;&#33402;&#26415;&#23478;&#33021;&#22815;&#21019;&#24314;&#39640;&#36136;&#37327;&#12289;&#25277;&#35937;&#30340;&#20108;&#32500;&#33402;&#26415;&#65292;&#20855;&#26377;&#39640;&#24230;&#30340;&#21487;&#25511;&#24615;&#21644;&#34920;&#29616;&#21147;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#20855;&#26377;&#33391;&#22909;&#32467;&#26524;&#30340;&#35745;&#31639;&#26041;&#27861;&#22312;&#21019;&#20316;&#20855;&#20307;&#30340;&#19977;&#32500;&#33402;&#26415;&#26041;&#38754;&#65292;&#22788;&#29702;&#39640;&#36136;&#37327;&#12289;&#21487;&#25511;&#24615;&#30340;&#25277;&#35937;&#19977;&#32500;&#33402;&#26415;&#20173;&#28982;&#26159;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#22330;&#26223;&#30340;&#21487;&#33258;&#23450;&#20041;&#21442;&#25968;&#21270;&#65292;&#23558;&#36827;&#21270;&#31574;&#30053;&#65288;ES&#65289;&#21644;3D&#28210;&#26579;&#30456;&#32467;&#21512;&#65292;&#25506;&#32034;&#22312;&#21046;&#20316;&#25277;&#35937;&#19977;&#32500;&#33402;&#26415;&#26041;&#38754;&#30340;&#35745;&#31639;&#21019;&#24847;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;3D&#22330;&#26223;&#20013;&#25918;&#32622;&#21322;&#36879;&#26126;&#19977;&#35282;&#24418;&#65292;&#22312;&#20174;&#25351;&#23450;&#35282;&#24230;&#26597;&#30475;&#26102;&#65292;&#21576;&#29616;&#20986;&#30475;&#36215;&#26469;&#20687;&#33402;&#26415;&#23478;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#30340;&#35268;&#26684;&#35828;&#26126;&#30340;&#24433;&#29255;&#12290;&#36825;&#20026;&#33402;&#26415;&#23478;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#24335;&#65292;&#26041;&#20415;&#22320;&#34920;&#36798;&#25277;&#35937;&#19977;&#32500;&#33402;&#26415;&#30340;&#21019;&#24847;&#24819;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computational creativity has contributed heavily to abstract art in modern era, allowing artists to create high quality, abstract two dimension (2D) arts with a high level of controllability and expressibility. However, even with computational approaches that have promising result in making concrete 3D art, computationally addressing abstract 3D art with high-quality and controllability remains an open question. To fill this gap, we propose to explore computational creativity in making abstract 3D art by bridging evolution strategies (ES) and 3D rendering through customizable parameterization of scenes. We demonstrate that our approach is capable of placing semi-transparent triangles in 3D scenes that, when viewed from specified angles, render into films that look like artists' specification expressed in natural language. This provides a new way for the artist to easily express creativity ideas for abstract 3D art. The supplementary material, which contains code, animation for all figu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#29992;&#25143;&#20013;&#24515;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#29992;&#21487;&#29992;&#30340;&#26799;&#24230;&#20449;&#24687;&#21644;&#32858;&#21512;&#35268;&#21017;&#65292;&#20026;&#27599;&#20010;&#32852;&#37030;&#23398;&#20064;&#23458;&#25143;&#31471;&#29983;&#25104;&#20010;&#24615;&#21270;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#35777;&#38544;&#31169;&#20445;&#25252;&#30340;&#20256;&#36755;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#26356;&#22909;&#30340;&#20934;&#30830;&#24230;&#21644;&#36890;&#20449;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.12930</link><description>&lt;p&gt;
&#29992;&#25143;&#20013;&#24515;&#32852;&#37030;&#23398;&#20064;&#65306;&#20026;&#20010;&#24615;&#21270;&#32780;&#20132;&#25442;&#26080;&#32447;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
User-Centric Federated Learning: Trading off Wireless Resources for Personalization. (arXiv:2304.12930v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12930
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#29992;&#25143;&#20013;&#24515;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#29992;&#21487;&#29992;&#30340;&#26799;&#24230;&#20449;&#24687;&#21644;&#32858;&#21512;&#35268;&#21017;&#65292;&#20026;&#27599;&#20010;&#32852;&#37030;&#23398;&#20064;&#23458;&#25143;&#31471;&#29983;&#25104;&#20010;&#24615;&#21270;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#35777;&#38544;&#31169;&#20445;&#25252;&#30340;&#20256;&#36755;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#26356;&#22909;&#30340;&#20934;&#30830;&#24230;&#21644;&#36890;&#20449;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20013;&#65292;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#32479;&#35745;&#24322;&#36136;&#24615;&#20250;&#22686;&#21152;&#31639;&#27861;&#25910;&#25947;&#26102;&#38388;&#24182;&#38477;&#20302;&#27867;&#21270;&#24615;&#33021;&#65292;&#23548;&#33268;&#39640;&#36890;&#20449;&#24320;&#38144;&#21644;&#36136;&#37327;&#20302;&#21155;&#30340;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#32780;&#19981;&#36829;&#21453;&#32852;&#37030;&#23398;&#20064;&#30340;&#38544;&#31169;&#32422;&#26463;&#65292;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#24517;&#39035;&#23558;&#32479;&#35745;&#30456;&#20284;&#30340;&#23458;&#25143;&#31471;&#32806;&#21512;&#22312;&#19968;&#36215;&#65292;&#20197;&#20445;&#35777;&#38544;&#31169;&#20445;&#25252;&#30340;&#20256;&#36755;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#22522;&#20110;&#21487;&#29992;&#30340;&#26799;&#24230;&#20449;&#24687;&#30340;&#21442;&#25968;&#26381;&#21153;&#22120;&#65288;PS&#65289;&#30340;&#29992;&#25143;&#20013;&#24515;&#32858;&#21512;&#35268;&#21017;&#65292;&#33021;&#22815;&#20026;&#27599;&#20010;&#32852;&#37030;&#23398;&#20064;&#23458;&#25143;&#31471;&#29983;&#25104;&#20010;&#24615;&#21270;&#27169;&#22411;&#12290;&#25152;&#25552;&#20986;&#30340;&#32858;&#21512;&#35268;&#21017;&#21463;&#21152;&#26435;&#32858;&#21512;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#19978;&#30028;&#21551;&#21457;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#22522;&#20110;&#29992;&#25143;&#32858;&#31867;&#23548;&#20986;&#20102;&#19968;&#31181;&#36890;&#20449;&#39640;&#25928;&#30340;&#21464;&#20307;&#65292;&#22823;&#22823;&#22686;&#24378;&#20102;&#20854;&#36866;&#29992;&#20110;&#36890;&#20449;&#21463;&#38480;&#31995;&#32479;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#20934;&#30830;&#24230;&#21644;&#36890;&#20449;&#25928;&#29575;&#30456;&#20284;&#25110;&#26356;&#22909;&#22320;&#32988;&#36807;&#20102;&#27969;&#34892;&#30340;&#23545;&#27604;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Statistical heterogeneity across clients in a Federated Learning (FL) system increases the algorithm convergence time and reduces the generalization performance, resulting in a large communication overhead in return for a poor model. To tackle the above problems without violating the privacy constraints that FL imposes, personalized FL methods have to couple statistically similar clients without directly accessing their data in order to guarantee a privacy-preserving transfer. In this work, we design user-centric aggregation rules at the parameter server (PS) that are based on readily available gradient information and are capable of producing personalized models for each FL client. The proposed aggregation rules are inspired by an upper bound of the weighted aggregate empirical risk minimizer. Secondly, we derive a communication-efficient variant based on user clustering which greatly enhances its applicability to communication-constrained systems. Our algorithm outperforms popular pe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#21442;&#25968;&#21270;&#30340;&#37327;&#23376;&#30005;&#36335;&#30340;&#37327;&#23376;&#26680;&#20989;&#25968;&#30340;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#26041;&#27861;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#65292;&#35777;&#26126;&#20854;&#21487;&#20197;&#20445;&#30041;&#21464;&#37327;&#20449;&#24687;&#24182;&#19988;&#21487;&#20197;&#20316;&#20026;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#27492;&#26041;&#27861;&#22312;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#19982;&#32463;&#20856;&#26041;&#27861;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2304.12923</link><description>&lt;p&gt;
&#22522;&#20110;&#37327;&#23376;&#26680;&#20989;&#25968;&#30340;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#22312;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Quantum Gaussian Process Regression for Bayesian Optimization. (arXiv:2304.12923v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#21442;&#25968;&#21270;&#30340;&#37327;&#23376;&#30005;&#36335;&#30340;&#37327;&#23376;&#26680;&#20989;&#25968;&#30340;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#26041;&#27861;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#65292;&#35777;&#26126;&#20854;&#21487;&#20197;&#20445;&#30041;&#21464;&#37327;&#20449;&#24687;&#24182;&#19988;&#21487;&#20197;&#20316;&#20026;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#27492;&#26041;&#27861;&#22312;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#19982;&#32463;&#20856;&#26041;&#27861;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#26159;&#19968;&#31181;&#25104;&#29087;&#30340;&#36125;&#21494;&#26031;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21442;&#25968;&#21270;&#30340;&#37327;&#23376;&#30005;&#36335;&#30340;&#37327;&#23376;&#26680;&#20989;&#25968;&#30340;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#30828;&#20214;&#26377;&#25928;&#30340;&#29305;&#24449;&#26144;&#23556;&#21644;&#35880;&#24910;&#27491;&#21017;&#21270; Gram &#30697;&#38453;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#27492;&#26041;&#27861;&#33021;&#22815;&#20445;&#30041;&#25152;&#24471;&#20986;&#30340;&#37327;&#23376;&#39640;&#26031;&#36807;&#31243;&#30340;&#26041;&#24046;&#20449;&#24687;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#37327;&#23376;&#39640;&#26031;&#36807;&#31243;&#21487;&#20197;&#29992;&#20316;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#32780;&#36825;&#20010;&#20219;&#21153;&#30340;&#25104;&#21151;&#20851;&#38190;&#22312;&#20110;&#20195;&#29702;&#27169;&#22411;&#30340;&#26041;&#24046;&#12290;&#20026;&#20102;&#23637;&#31034;&#36825;&#31181;&#37327;&#23376;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#23558;&#20854;&#24212;&#29992;&#20110;&#23545;&#19968;&#20010;&#25191;&#34892;&#23454;&#38469;&#25968;&#25454;&#38598;&#22238;&#24402;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#12290;&#25105;&#20204;&#23545;&#37327;&#23376;&#36125;&#21494;&#26031;&#20248;&#21270;&#19982;&#20854;&#32463;&#20856;&#23545;&#24212;&#29289;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#35777;&#26126;&#37327;&#23376;&#29256;&#26412;&#21487;&#20197;&#36798;&#21040;&#30456;&#21516;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gaussian process regression is a well-established Bayesian machine learning method. We propose a new approach to Gaussian process regression using quantum kernels based on parameterized quantum circuits. By employing a hardware-efficient feature map and careful regularization of the Gram matrix, we demonstrate that the variance information of the resulting quantum Gaussian process can be preserved. We also show that quantum Gaussian processes can be used as a surrogate model for Bayesian optimization, a task that critically relies on the variance of the surrogate model. To demonstrate the performance of this quantum Bayesian optimization algorithm, we apply it to the hyperparameter optimization of a machine learning model which performs regression on a real-world dataset. We benchmark the quantum Bayesian optimization against its classical counterpart and show that quantum version can match its performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Copula&#29109;&#26041;&#27861;&#26080;&#27169;&#22411;&#36229;&#21442;&#25968;&#22320;&#23545;&#21160;&#24577;&#31995;&#32479;&#36827;&#34892;&#24494;&#20998;&#26041;&#31243;&#35782;&#21035;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#19977;&#32500;Lorenz&#31995;&#32479;&#30340;&#27169;&#25311;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.12922</link><description>&lt;p&gt;
&#22522;&#20110;Copula&#29109;&#30340;&#31995;&#32479;&#36776;&#35782;
&lt;/p&gt;
&lt;p&gt;
System Identification with Copula Entropy. (arXiv:2304.12922v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12922
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Copula&#29109;&#26041;&#27861;&#26080;&#27169;&#22411;&#36229;&#21442;&#25968;&#22320;&#23545;&#21160;&#24577;&#31995;&#32479;&#36827;&#34892;&#24494;&#20998;&#26041;&#31243;&#35782;&#21035;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#19977;&#32500;Lorenz&#31995;&#32479;&#30340;&#27169;&#25311;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#23450;&#21160;&#24577;&#31995;&#32479;&#30340;&#24494;&#20998;&#26041;&#31243;&#26159;&#19968;&#20010;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;Copula&#29109;&#65288;CE&#65289;&#26159;&#20449;&#24687;&#35770;&#20013;&#29992;&#20110;&#27979;&#37327;&#32479;&#35745;&#29420;&#31435;&#24615;&#30340;&#25968;&#23398;&#27010;&#24565;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;CE&#35782;&#21035;&#21160;&#24577;&#31995;&#32479;&#24494;&#20998;&#26041;&#31243;&#30340;&#26041;&#27861;&#12290;&#35813;&#38382;&#39064;&#34987;&#35270;&#20026;&#19968;&#20010;&#21464;&#37327;&#36873;&#25321;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20808;&#21069;&#25552;&#20986;&#30340;&#22522;&#20110;CE&#30340;&#21464;&#37327;&#36873;&#25321;&#26041;&#27861;&#26469;&#35299;&#20915;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30001;&#20004;&#20010;&#32452;&#20214;&#32452;&#25104;&#65306;&#24046;&#20998;&#31639;&#23376;&#21644;CE&#20272;&#35745;&#22120;&#12290;&#30001;&#20110;&#20004;&#20010;&#32452;&#20214;&#37117;&#21487;&#20197;&#36827;&#34892;&#38750;&#21442;&#25968;&#21270;&#65292;&#22240;&#27492;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#26080;&#27169;&#22411;&#21644;&#36229;&#21442;&#25968;&#12290;&#22312;&#19977;&#32500;Lorenz&#31995;&#32479;&#30340;&#27169;&#25311;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifying differential equation governing dynamical system is an important problem with wide applications. Copula Entropy (CE) is a mathematical concept for measuring statistical independence in information theory. In this paper we propose a method for identifying differential equation of dynamical systems with CE. The problem is considered as a variable selection problem and solved with the previously proposed CE-based method for variable selection. The proposed method composed of two components: the difference operator and the CE estimator. Since both components can be done non-parametrically, the proposed method is therefore model-free and hyperparameter-free. The simulation experiment with the 3D Lorenz system verified the effectiveness of the proposed method.
&lt;/p&gt;</description></item><item><title>Awesome-META+&#26159;&#19968;&#20010;&#20803;&#23398;&#20064;&#26694;&#26550;&#38598;&#25104;&#21644;&#23398;&#20064;&#24179;&#21488;&#65292;&#26088;&#22312;&#25552;&#20379;&#23436;&#25972;&#21487;&#38752;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#24212;&#29992;&#21644;&#38754;&#21521;&#21021;&#23398;&#32773;&#30340;&#23398;&#20064;&#26448;&#26009;&#65292;&#36827;&#32780;&#20419;&#36827;&#20803;&#23398;&#20064;&#30340;&#21457;&#23637;&#24182;&#23558;&#20854;&#20174;&#23567;&#20247;&#39046;&#22495;&#36716;&#21270;&#20026;&#20027;&#27969;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2304.12921</link><description>&lt;p&gt;
Awesome-META+: &#20803;&#23398;&#20064;&#30740;&#31350;&#19982;&#23398;&#20064;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
Awesome-META+: Meta-Learning Research and Learning Platform. (arXiv:2304.12921v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12921
&lt;/p&gt;
&lt;p&gt;
Awesome-META+&#26159;&#19968;&#20010;&#20803;&#23398;&#20064;&#26694;&#26550;&#38598;&#25104;&#21644;&#23398;&#20064;&#24179;&#21488;&#65292;&#26088;&#22312;&#25552;&#20379;&#23436;&#25972;&#21487;&#38752;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#24212;&#29992;&#21644;&#38754;&#21521;&#21021;&#23398;&#32773;&#30340;&#23398;&#20064;&#26448;&#26009;&#65292;&#36827;&#32780;&#20419;&#36827;&#20803;&#23398;&#20064;&#30340;&#21457;&#23637;&#24182;&#23558;&#20854;&#20174;&#23567;&#20247;&#39046;&#22495;&#36716;&#21270;&#20026;&#20027;&#27969;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#24050;&#32463;&#22312;&#32463;&#27982;&#12289;&#20135;&#19994;&#12289;&#25945;&#32946;&#31561;&#21508;&#20010;&#39046;&#22495;&#20135;&#29983;&#20102;&#28145;&#36828;&#30340;&#24433;&#21709;&#65292;&#20294;&#36824;&#23384;&#22312;&#35832;&#22810;&#38480;&#21046;&#12290;&#20803;&#23398;&#20064;&#65292;&#20063;&#31216;&#20026;&#8220;&#23398;&#20064;&#22914;&#20309;&#23398;&#20064;&#8221;&#65292;&#20026;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#25552;&#20379;&#20102;&#31361;&#30772;&#30446;&#21069;&#29942;&#39048;&#30340;&#26426;&#20250;&#12290;&#28982;&#32780;&#65292;&#20803;&#23398;&#20064;&#36215;&#27493;&#36739;&#26202;&#65292;&#30456;&#27604;CV&#12289;NLP&#31561;&#39046;&#22495;&#65292;&#39033;&#30446;&#25968;&#37327;&#36739;&#23569;&#12290;&#27599;&#27425;&#37096;&#32626;&#37117;&#38656;&#35201;&#22823;&#37327;&#30340;&#32463;&#39564;&#21435;&#37197;&#32622;&#29615;&#22659;&#12289;&#35843;&#35797;&#20195;&#30721;&#29978;&#33267;&#37325;&#20889;&#65292;&#32780;&#19988;&#26694;&#26550;&#20043;&#38388;&#30456;&#23545;&#23396;&#31435;&#12290;&#27492;&#22806;&#65292;&#30446;&#21069;&#38024;&#23545;&#20803;&#23398;&#20064;&#30340;&#19987;&#38376;&#24179;&#21488;&#21644;&#38754;&#21521;&#21021;&#23398;&#32773;&#30340;&#23398;&#20064;&#26448;&#26009;&#30456;&#23545;&#36739;&#23569;&#65292;&#38376;&#27099;&#30456;&#23545;&#36739;&#39640;&#12290;&#22522;&#20110;&#27492;&#65292;Awesome-META+&#25552;&#20986;&#20102;&#19968;&#20010;&#20803;&#23398;&#20064;&#26694;&#26550;&#38598;&#25104;&#21644;&#23398;&#20064;&#24179;&#21488;&#65292;&#26088;&#22312;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#24182;&#25552;&#20379;&#23436;&#25972;&#21487;&#38752;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#24212;&#29992;&#21644;&#23398;&#20064;&#24179;&#21488;&#12290;&#35813;&#39033;&#30446;&#26088;&#22312;&#20419;&#36827;&#20803;&#23398;&#20064;&#30340;&#21457;&#23637;&#65292;&#24182;&#23558;&#20854;&#20174;&#19968;&#20010;&#23567;&#20247;&#39046;&#22495;&#36716;&#21270;&#20026;&#19968;&#20010;&#20027;&#27969;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence technology has already had a profound impact in various fields such as economy, industry, and education, but still limited. Meta-learning, also known as "learning to learn", provides an opportunity for general artificial intelligence, which can break through the current AI bottleneck. However, meta learning started late and there are fewer projects compare with CV, NLP etc. Each deployment requires a lot of experience to configure the environment, debug code or even rewrite, and the frameworks are isolated. Moreover, there are currently few platforms that focus exclusively on meta-learning, or provide learning materials for novices, for which the threshold is relatively high. Based on this, Awesome-META+, a meta-learning framework integration and learning platform is proposed to solve the above problems and provide a complete and reliable meta-learning framework application and learning platform. The project aims to promote the development of meta-learning and t
&lt;/p&gt;</description></item><item><title>N2G&#26159;&#19968;&#31181;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#35299;&#37322;&#31070;&#32463;&#20803;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#33258;&#21160;&#23558;&#31070;&#32463;&#20803;&#22312;&#25968;&#25454;&#38598;&#31034;&#20363;&#19978;&#30340;&#34892;&#20026;&#25552;&#28860;&#20026;&#21487;&#35299;&#37322;&#30340;&#22270;&#24418;&#65292;&#24182;&#19988;&#21487;&#20197;&#36755;&#20986;&#25991;&#26412;&#19978;&#20196;&#29260;&#30340;&#28608;&#27963;&#24773;&#20917;&#26469;&#36827;&#34892;&#33258;&#21160;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2304.12918</link><description>&lt;p&gt;
N2G&#65306;&#19968;&#31181;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#37327;&#21270;&#21487;&#35299;&#37322;&#31070;&#32463;&#20803;&#34920;&#31034;&#30340;&#21487;&#25193;&#23637;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
N2G: A Scalable Approach for Quantifying Interpretable Neuron Representations in Large Language Models. (arXiv:2304.12918v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12918
&lt;/p&gt;
&lt;p&gt;
N2G&#26159;&#19968;&#31181;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#35299;&#37322;&#31070;&#32463;&#20803;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#33258;&#21160;&#23558;&#31070;&#32463;&#20803;&#22312;&#25968;&#25454;&#38598;&#31034;&#20363;&#19978;&#30340;&#34892;&#20026;&#25552;&#28860;&#20026;&#21487;&#35299;&#37322;&#30340;&#22270;&#24418;&#65292;&#24182;&#19988;&#21487;&#20197;&#36755;&#20986;&#25991;&#26412;&#19978;&#20196;&#29260;&#30340;&#28608;&#27963;&#24773;&#20917;&#26469;&#36827;&#34892;&#33258;&#21160;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#20013;&#20010;&#21035;&#31070;&#32463;&#20803;&#30340;&#21151;&#33021;&#23545;&#20110;&#26426;&#26800;&#35299;&#37322;&#24615;&#30740;&#31350;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#31070;&#32463;&#20803;&#21040;&#22270;&#65288;N2G&#65289;&#8221;&#30340;&#24037;&#20855;&#65292;&#35813;&#24037;&#20855;&#25509;&#25910;&#31070;&#32463;&#20803;&#21450;&#20854;&#25968;&#25454;&#38598;&#31034;&#20363;&#65292;&#24182;&#33258;&#21160;&#23558;&#31070;&#32463;&#20803;&#22312;&#36825;&#20123;&#31034;&#20363;&#19978;&#30340;&#34892;&#20026;&#25552;&#28860;&#20026;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#22270;&#24418;&#65292;&#20026;&#35299;&#37322;&#31070;&#32463;&#20803;&#25552;&#20379;&#20102;&#27604;&#24403;&#21069;&#25163;&#21160;&#26041;&#27861;&#26356;&#36731;&#26494;&#30340;&#26041;&#27861;&#65292;&#24182;&#26356;&#22909;&#22320;&#23558;&#36825;&#20123;&#26041;&#27861;&#25193;&#23637;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#12290;&#25105;&#20204;&#20351;&#29992;&#25130;&#26029;&#21644;&#26174;&#33879;&#24615;&#26041;&#27861;&#20165;&#21576;&#29616;&#37325;&#35201;&#20196;&#29260;&#65292;&#24182;&#21033;&#29992;&#26356;&#22810;&#26679;&#30340;&#26679;&#26412;&#22686;&#24378;&#25968;&#25454;&#38598;&#31034;&#20363;&#65292;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#31070;&#32463;&#20803;&#34892;&#20026;&#30340;&#31243;&#24230;&#12290;&#36825;&#20123;&#22270;&#24418;&#21487;&#20197;&#36890;&#36807;&#21487;&#35270;&#21270;&#26469;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#36827;&#34892;&#25163;&#21160;&#35299;&#37322;&#65292;&#20294;&#20063;&#21487;&#20197;&#36755;&#20986;&#25991;&#26412;&#19978;&#30340;&#20196;&#29260;&#28608;&#27963;&#24773;&#20917;&#65292;&#20197;&#19982;&#31070;&#32463;&#20803;&#30340;&#21442;&#32771;&#28608;&#27963;&#24773;&#20917;&#36827;&#34892;&#27604;&#36739;&#65292;&#36827;&#34892;&#33258;&#21160;&#39564;&#35777;&#12290;N2G&#20195;&#34920;&#20102;&#21487;&#25193;&#23637;&#30340;&#35299;&#37322;&#24615;&#26041;&#27861;&#21521;&#21069;&#36808;&#20986;&#30340;&#19968;&#27493;&#65292;&#22240;&#20026;&#23427;&#20801;&#35768;&#25105;&#20204;&#33258;&#21160;&#23558;LLM&#20013;&#30340;&#31070;&#32463;&#20803;&#36716;&#25442;&#20026;&#21487;&#35299;&#37322;&#30340;&#22270;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the function of individual neurons within language models is essential for mechanistic interpretability research. We propose $\textbf{Neuron to Graph (N2G)}$, a tool which takes a neuron and its dataset examples, and automatically distills the neuron's behaviour on those examples to an interpretable graph. This presents a less labour intensive approach to interpreting neurons than current manual methods, that will better scale these methods to Large Language Models (LLMs). We use truncation and saliency methods to only present the important tokens, and augment the dataset examples with more diverse samples to better capture the extent of neuron behaviour. These graphs can be visualised to aid manual interpretation by researchers, but can also output token activations on text to compare to the neuron's ground truth activations for automatic validation. N2G represents a step towards scalable interpretability methods by allowing us to convert neurons in an LLM to interpretab
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20998;&#24046;&#24322;&#27969;&#27169;&#22411;(SD flow)&#65292;&#23427;&#21487;&#20197;&#26368;&#20248;&#22320;&#20943;&#23569;&#20004;&#20010;&#20998;&#24067;&#20043;&#38388;&#30340;&#25955;&#24230;&#65292;&#21516;&#26102;&#35299;&#20915;Schr&#8203;&#8203;&#246;dinger&#26725;&#38382;&#39064;&#12290;&#19982;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#19981;&#21516;&#65292;&#23427;&#27809;&#26377;&#23545;&#20808;&#39564;&#20998;&#24067;&#26045;&#21152;&#20219;&#20309;&#38480;&#21046;&#65292;&#22312;&#19968;&#20123;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.12906</link><description>&lt;p&gt;
&#35780;&#20998;&#24046;&#20540;&#27969;&#27169;&#22411;&#29992;&#20110;&#38544;&#24335;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
The Score-Difference Flow for Implicit Generative Modeling. (arXiv:2304.12906v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12906
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20998;&#24046;&#24322;&#27969;&#27169;&#22411;(SD flow)&#65292;&#23427;&#21487;&#20197;&#26368;&#20248;&#22320;&#20943;&#23569;&#20004;&#20010;&#20998;&#24067;&#20043;&#38388;&#30340;&#25955;&#24230;&#65292;&#21516;&#26102;&#35299;&#20915;Schr&#8203;&#8203;&#246;dinger&#26725;&#38382;&#39064;&#12290;&#19982;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#19981;&#21516;&#65292;&#23427;&#27809;&#26377;&#23545;&#20808;&#39564;&#20998;&#24067;&#26045;&#21152;&#20219;&#20309;&#38480;&#21046;&#65292;&#22312;&#19968;&#20123;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#24335;&#29983;&#25104;&#24314;&#27169;(IGM)&#26088;&#22312;&#29983;&#25104;&#31526;&#21512;&#30446;&#26631;&#25968;&#25454;&#20998;&#24067;&#29305;&#24449;&#30340;&#21512;&#25104;&#25968;&#25454;&#26679;&#26412;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;(&#20363;&#22914;&#35780;&#20998;&#21305;&#37197;&#32593;&#32476;&#12289;&#25193;&#25955;&#27169;&#22411;)&#20174;&#36890;&#36807;&#29615;&#22659;&#31354;&#38388;&#20013;&#30340;&#21160;&#24577;&#25200;&#21160;&#25110;&#27969;&#23558;&#21512;&#25104;&#28304;&#25968;&#25454;&#25512;&#21521;&#30446;&#26631;&#20998;&#24067;&#30340;&#35282;&#24230;&#35299;&#20915;&#20102;IGM&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20219;&#24847;&#30446;&#26631;&#21644;&#28304;&#20998;&#24067;&#20043;&#38388;&#30340;&#35780;&#20998;&#24046;&#24322;(SD)&#20316;&#20026;&#27969;&#65292;&#23427;&#21487;&#20197;&#26368;&#20248;&#22320;&#20943;&#23569;&#23427;&#20204;&#20043;&#38388;&#30340;Kullback-Leibler&#25955;&#24230;&#65292;&#21516;&#26102;&#35299;&#20915;Schr&#8203;&#8203;&#246;dinger&#26725;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;SD&#27969;&#24212;&#29992;&#20110;&#26041;&#20415;&#30340;&#20195;&#29702;&#20998;&#24067;&#65292;&#24403;&#19988;&#20165;&#24403;&#21407;&#22987;&#20998;&#24067;&#23545;&#40784;&#26102;&#65292;&#23427;&#20204;&#26159;&#23545;&#40784;&#30340;&#12290;&#25105;&#20204;&#22312;&#26576;&#20123;&#26465;&#20214;&#19979;&#23637;&#31034;&#20102;&#36825;&#31181;&#20844;&#24335;&#19982;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#30340;&#24418;&#24335;&#19968;&#33268;&#24615;&#12290;&#28982;&#32780;&#65292;&#19982;&#25193;&#25955;&#27169;&#22411;&#19981;&#21516;&#65292;SD&#27969;&#27809;&#26377;&#23545;&#20808;&#39564;&#20998;&#24067;&#26045;&#21152;&#20219;&#20309;&#38480;&#21046;&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#22312;&#26080;&#38480;&#36776;&#21035;&#22120;&#33021;&#21147;&#30340;&#26497;&#38480;&#19979;&#65292;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#35757;&#32451;&#21253;&#21547;SD&#27969;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;SD&#27969;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#26032;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Implicit generative modeling (IGM) aims to produce samples of synthetic data matching the characteristics of a target data distribution. Recent work (e.g. score-matching networks, diffusion models) has approached the IGM problem from the perspective of pushing synthetic source data toward the target distribution via dynamical perturbations or flows in the ambient space. We introduce the score difference (SD) between arbitrary target and source distributions as a flow that optimally reduces the Kullback-Leibler divergence between them while also solving the Schr\"odinger bridge problem. We apply the SD flow to convenient proxy distributions, which are aligned if and only if the original distributions are aligned. We demonstrate the formal equivalence of this formulation to denoising diffusion models under certain conditions. However, unlike diffusion models, SD flow places no restrictions on the prior distribution. We also show that the training of generative adversarial networks includ
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#36827;&#21270;&#25628;&#32034;&#26469;&#21457;&#29616;&#22270;&#24418;&#29983;&#25104;&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#27010;&#29575;&#27169;&#22411;&#25110;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#20854;&#20855;&#26377;&#26356;&#39640;&#30340;&#35757;&#32451;&#20998;&#24067;&#22806;&#25512;&#28508;&#21147;&#21644;&#30452;&#25509;&#35299;&#37322;&#24615;&#12290;&#21487;&#20197;&#19982;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#31454;&#20105;&#65292;&#24182;&#19988;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#20197;&#25214;&#21040;&#30495;&#27491;&#30340;&#22270;&#24418;&#29983;&#25104;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2304.12895</link><description>&lt;p&gt;
&#21457;&#29616;&#22270;&#24418;&#29983;&#25104;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Discovering Graph Generation Algorithms. (arXiv:2304.12895v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12895
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#36827;&#21270;&#25628;&#32034;&#26469;&#21457;&#29616;&#22270;&#24418;&#29983;&#25104;&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#27010;&#29575;&#27169;&#22411;&#25110;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#20854;&#20855;&#26377;&#26356;&#39640;&#30340;&#35757;&#32451;&#20998;&#24067;&#22806;&#25512;&#28508;&#21147;&#21644;&#30452;&#25509;&#35299;&#37322;&#24615;&#12290;&#21487;&#20197;&#19982;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#31454;&#20105;&#65292;&#24182;&#19988;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#20197;&#25214;&#21040;&#30495;&#27491;&#30340;&#22270;&#24418;&#29983;&#25104;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#26500;&#24314;&#22270;&#24418;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#36827;&#21270;&#25628;&#32034;&#21644;&#19968;&#20010;&#30001;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#30340;&#24378;&#22823;&#36866;&#24212;&#24230;&#20989;&#25968;&#65292;&#32780;&#19981;&#26159;&#20351;&#29992;&#20256;&#32479;&#30340;&#27010;&#29575;&#27169;&#22411;&#25110;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#26469;&#29983;&#25104;&#25968;&#25454;&#12290;&#30456;&#27604;&#24403;&#21069;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#36825;&#31181;&#26041;&#27861;&#24102;&#26469;&#20102;&#19968;&#20123;&#20248;&#21183;&#65292;&#20363;&#22914;&#26356;&#39640;&#30340;&#35757;&#32451;&#20998;&#24067;&#22806;&#25512;&#28508;&#21147;&#21644;&#30452;&#25509;&#35299;&#37322;&#24615;&#65292;&#22240;&#20026;&#26368;&#32456;&#30340;&#22270;&#24418;&#29983;&#25104;&#36807;&#31243;&#20197;Python&#20989;&#25968;&#30340;&#24418;&#24335;&#34920;&#36798;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#19982;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#31454;&#20105;&#65292;&#24182;&#19988;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#29978;&#33267;&#21487;&#20197;&#25214;&#21040;&#30495;&#27491;&#30340;&#22270;&#24418;&#29983;&#25104;&#36807;&#31243;&#65292;&#22240;&#27492;&#21487;&#20197;&#23436;&#32654;&#22320;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide a novel approach to construct generative models for graphs. Instead of using the traditional probabilistic models or deep generative models, we propose to instead find an algorithm that generates the data. We achieve this using evolutionary search and a powerful fitness function, implemented by a randomly initialized graph neural network. This brings certain advantages over current deep generative models, for instance, a higher potential for out-of-training-distribution generalization and direct interpretability, as the final graph generative process is expressed as a Python function. We show that this approach can be competitive with deep generative models and under some circumstances can even find the true graph generative process, and as such perfectly generalize.
&lt;/p&gt;</description></item><item><title>LDM&#27169;&#22411;&#21487;&#20197;&#29992;&#20110;&#38477;&#27700;&#39044;&#27979;&#65292;&#30456;&#36739;&#20110;GAN&#31561;&#20854;&#20182;&#27169;&#22411;&#65292;LDM&#19981;&#20165;&#26356;&#31283;&#23450;&#12289;&#38656;&#35201;&#26356;&#23569;&#30340;&#35745;&#31639;&#37327;&#65292;&#36824;&#33021;&#20135;&#29983;&#26356;&#22810;&#26679;&#21270;&#21644;&#26356;&#20934;&#30830;&#30340;&#38477;&#27700;&#39044;&#27979;&#65292;&#26159;&#23545;&#20110;&#38656;&#35201;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#21644;&#39044;&#27979;&#22810;&#26679;&#24615;&#30340;&#20915;&#31574;-making &#30340;&#30456;&#20851;&#24212;&#29992;&#38750;&#24120;&#26377;&#21069;&#26223;&#30340;&#12290;</title><link>http://arxiv.org/abs/2304.12891</link><description>&lt;p&gt;
&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#20855;&#26377;&#20934;&#30830;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#29983;&#25104;&#24615;&#38477;&#27700;&#29616;&#39044;&#25253;
&lt;/p&gt;
&lt;p&gt;
Latent diffusion models for generative precipitation nowcasting with accurate uncertainty quantification. (arXiv:2304.12891v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12891
&lt;/p&gt;
&lt;p&gt;
LDM&#27169;&#22411;&#21487;&#20197;&#29992;&#20110;&#38477;&#27700;&#39044;&#27979;&#65292;&#30456;&#36739;&#20110;GAN&#31561;&#20854;&#20182;&#27169;&#22411;&#65292;LDM&#19981;&#20165;&#26356;&#31283;&#23450;&#12289;&#38656;&#35201;&#26356;&#23569;&#30340;&#35745;&#31639;&#37327;&#65292;&#36824;&#33021;&#20135;&#29983;&#26356;&#22810;&#26679;&#21270;&#21644;&#26356;&#20934;&#30830;&#30340;&#38477;&#27700;&#39044;&#27979;&#65292;&#26159;&#23545;&#20110;&#38656;&#35201;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#21644;&#39044;&#27979;&#22810;&#26679;&#24615;&#30340;&#20915;&#31574;-making &#30340;&#30456;&#20851;&#24212;&#29992;&#38750;&#24120;&#26377;&#21069;&#26223;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#24050;&#34987;&#24191;&#27867;&#37319;&#29992;&#20110;&#22270;&#20687;&#29983;&#25104;&#65292;&#20135;&#29983;&#30340;&#26679;&#26412;&#36136;&#37327;&#26356;&#39640;&#12289;&#26356;&#22810;&#26679;&#21270;&#65292;&#27604;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#26356;&#26377;&#25928;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65288;LDM&#65289;&#29992;&#20110;&#38477;&#27700;&#21363;&#26102;&#39044;&#25253;&#65292;&#21363;&#22522;&#20110;&#26368;&#26032;&#35266;&#27979;&#25968;&#25454;&#30340;&#30701;&#26399;&#39044;&#27979;&#12290;LDM&#27604;GAN&#26356;&#31283;&#23450;&#65292;&#38656;&#35201;&#26356;&#23569;&#30340;&#35745;&#31639;&#37327;&#36827;&#34892;&#35757;&#32451;&#65292;&#20294;&#23545;&#20110;&#29983;&#25104;&#32780;&#35328;&#38656;&#35201;&#26356;&#22810;&#30340;&#35745;&#31639;&#37327;&#12290;&#25105;&#20204;&#23558;&#20854;&#19982;&#22522;&#20110;GAN&#30340;Deep Generative Models of Rainfall&#65288;DGMR&#65289;&#21644;&#32479;&#35745;&#27169;&#22411;PySTEPS&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;LDM&#20135;&#29983;&#20102;&#26356;&#20934;&#30830;&#30340;&#38477;&#27700;&#39044;&#25253;&#65292;&#24403;&#39044;&#27979;&#38477;&#27700;&#26159;&#21542;&#36229;&#36807;&#39044;&#23450;&#20041;&#38408;&#20540;&#26102;&#65292;&#27604;&#36739;&#32467;&#26524;&#26356;&#21152;&#28151;&#21512;&#12290; LDM&#30340;&#26368;&#22823;&#20248;&#21183;&#22312;&#20110;&#23427;&#20135;&#29983;&#30340;&#39044;&#27979;&#26356;&#21152;&#22810;&#26679;&#21270;&#12290;&#31561;&#32423;&#20998;&#24067;&#27979;&#35797;&#34920;&#26126;&#65292;&#20174;LDM&#20013;&#21462;&#26679;&#30340;&#20998;&#24067;&#20934;&#30830;&#21453;&#26144;&#20102;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#22240;&#27492;&#65292;LDM&#22312;&#38656;&#35201;&#20851;&#27880;&#19981;&#30830;&#23450;&#24615;&#21644;&#22810;&#26679;&#24615;&#39044;&#27979;&#30340;&#20219;&#20309;&#24212;&#29992;&#20013;&#37117;&#38750;&#24120;&#26377;&#21069;&#26223;&#65292;&#20363;&#22914;&#22312;&#37325;&#22823;&#27668;&#35937;&#20107;&#20214;&#30340;&#20915;&#31574;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have been widely adopted in image generation, producing higher-quality and more diverse samples than generative adversarial networks (GANs). We introduce a latent diffusion model (LDM) for precipitation nowcasting - short-term forecasting based on the latest observational data. The LDM is more stable and requires less computation to train than GANs, albeit with more computationally expensive generation. We benchmark it against the GAN-based Deep Generative Models of Rainfall (DGMR) and a statistical model, PySTEPS. The LDM produces more accurate precipitation predictions, while the comparisons are more mixed when predicting whether the precipitation exceeds predefined thresholds. The clearest advantage of the LDM is that it generates more diverse predictions than DGMR or PySTEPS. Rank distribution tests indicate that the distribution of samples from the LDM accurately reflects the uncertainty of the predictions. Thus, LDMs are promising for any applications where uncer
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#32773;&#23545;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19968;&#33324;&#35206;&#30422;&#26465;&#20214;&#65292;&#24182;&#21457;&#29616;&#26356;&#22810;&#30340;&#35206;&#30422;&#26465;&#20214;&#65292;&#25552;&#39640;&#20102;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#34920;&#29616;&#65292;&#21516;&#26102;&#38416;&#26126;&#33391;&#22909;&#30340;&#35206;&#30422;&#26465;&#20214;&#20173;&#28982;&#26377;&#30410;&#20110;&#33719;&#24471;&#26368;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2304.12886</link><description>&lt;p&gt;
&#38024;&#23545;&#20989;&#25968;&#36924;&#36817;&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#19968;&#33324;&#35206;&#30422;&#26465;&#20214;&#30340;&#21487;&#35777;&#26126;&#20248;&#21183;
&lt;/p&gt;
&lt;p&gt;
Provable benefits of general coverage conditions in efficient online RL with function approximation. (arXiv:2304.12886v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12886
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#32773;&#23545;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19968;&#33324;&#35206;&#30422;&#26465;&#20214;&#65292;&#24182;&#21457;&#29616;&#26356;&#22810;&#30340;&#35206;&#30422;&#26465;&#20214;&#65292;&#25552;&#39640;&#20102;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#34920;&#29616;&#65292;&#21516;&#26102;&#38416;&#26126;&#33391;&#22909;&#30340;&#35206;&#30422;&#26465;&#20214;&#20173;&#28982;&#26377;&#30410;&#20110;&#33719;&#24471;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#19982;&#20854;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#30340;&#26631;&#20934;&#32467;&#26500;&#20551;&#35774;&#65292;&#20351;&#29992;&#26576;&#31181;&#35206;&#30422;&#26465;&#20214;&#65288;&#28304;&#33258;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65289;&#36275;&#20197;&#30830;&#20445;&#26679;&#26412;&#26377;&#25928;&#20445;&#35777;&#65288;Xie&#31561;&#20154;&#65292;2023&#65289;&#12290;&#26412;&#25991;&#20851;&#27880;&#36825;&#20010;&#26032;&#26041;&#21521;&#65292;&#25366;&#25496;&#26356;&#22810;&#21487;&#33021;&#21644;&#26356;&#26222;&#36941;&#30340;&#35206;&#30422;&#26465;&#20214;&#65292;&#24182;&#30740;&#31350;&#23427;&#20204;&#22312;&#39640;&#25928;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#28508;&#21147;&#21644;&#29992;&#36884;&#12290;&#25105;&#20204;&#37492;&#23450;&#20102;&#26356;&#22810;&#27010;&#24565;&#65292;&#21253;&#25324;$L^p$&#21151;&#33021;&#38598;&#20013;&#24230;&#12289;&#23494;&#24230;&#27604;&#23454;&#29616;&#24615;&#20197;&#21450;&#37096;&#20998;/&#20840;&#35206;&#30422;&#26465;&#20214;&#30340;&#26435;&#34913;&#65292;&#36825;&#20123;&#27010;&#24565;&#20063;&#26377;&#30410;&#20110;&#23454;&#29616;&#26679;&#26412;&#26377;&#25928;&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#65292;&#20174;&#32780;&#23454;&#29616;&#25913;&#36827;&#30340;&#36951;&#25022;&#36793;&#30028;&#12290;&#27492;&#22806;&#65292;&#22914;&#26524;&#21033;&#29992;&#25506;&#32034;&#24615;&#30340;&#31163;&#32447;&#25968;&#25454;&#65292;&#22312;&#25105;&#20204;&#30340;&#35206;&#30422;&#26465;&#20214;&#19979;&#65292;&#21487;&#20197;&#20026;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#32479;&#35745;&#21644;&#35745;&#31639;&#19978;&#39640;&#25928;&#30340;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;MDP&#32467;&#26500;&#24050;&#32463;&#32473;&#20986;&#65292;&#20363;&#22914;&#32447;&#24615;MDP&#65292;&#25105;&#20204;&#20063;&#38416;&#26126;&#20102;&#33391;&#22909;&#30340;&#35206;&#30422;&#26465;&#20214;&#20173;&#28982;&#26377;&#30410;&#20110;&#33719;&#24471;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
In online reinforcement learning (RL), instead of employing standard structural assumptions on Markov decision processes (MDPs), using a certain coverage condition (original from offline RL) is enough to ensure sample-efficient guarantees (Xie et al. 2023). In this work, we focus on this new direction by digging more possible and general coverage conditions, and study the potential and the utility of them in efficient online RL. We identify more concepts, including the $L^p$ variant of concentrability, the density ratio realizability, and trade-off on the partial/rest coverage condition, that can be also beneficial to sample-efficient online RL, achieving improved regret bound. Furthermore, if exploratory offline data are used, under our coverage conditions, both statistically and computationally efficient guarantees can be achieved for online RL. Besides, even though the MDP structure is given, e.g., linear MDP, we elucidate that, good coverage conditions are still beneficial to obtai
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;&#36817;&#31471;&#21457;&#23637;&#21306;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102; ProCuRL &#35838;&#31243;&#31574;&#30053;&#65292;&#29992;&#20110;&#35774;&#35745;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#35838;&#31243;&#65292;&#20197;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#22312;&#21508;&#31181;&#22495;&#19978;&#23454;&#39564;&#34920;&#26126;&#20854;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.12877</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#36817;&#31471;&#35838;&#31243;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Proximal Curriculum for Reinforcement Learning Agents. (arXiv:2304.12877v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#36817;&#31471;&#21457;&#23637;&#21306;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102; ProCuRL &#35838;&#31243;&#31574;&#30053;&#65292;&#29992;&#20110;&#35774;&#35745;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#35838;&#31243;&#65292;&#20197;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#22312;&#21508;&#31181;&#22495;&#19978;&#23454;&#39564;&#34920;&#26126;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#38024;&#23545;&#19978;&#19979;&#25991;&#22810;&#20219;&#21153;&#35774;&#32622;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#35838;&#31243;&#35774;&#35745;&#38382;&#39064;&#12290;&#33258;&#21160;&#35838;&#31243;&#35774;&#35745;&#30340;&#29616;&#26377;&#25216;&#26415;&#36890;&#24120;&#38656;&#35201;&#29305;&#23450;&#39046;&#22495;&#20013;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#25110;&#20855;&#26377;&#26377;&#38480;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#35838;&#31243;&#31574;&#30053; ProCuRL&#65292;&#23427;&#21463;&#21040;&#20102;&#25945;&#32946;&#27010;&#24565;&#8220;&#36817;&#31471;&#21457;&#23637;&#21306;&#8221;&#30340;&#21551;&#21457;&#12290;ProCuRL &#35302;&#21457;&#20102;&#36825;&#26679;&#19968;&#20010;&#30452;&#35273;&#65292;&#21363;&#24403;&#36873;&#25321;&#26082;&#19981;&#22826;&#38590;&#20063;&#19981;&#22826;&#23481;&#26131;&#30340;&#20219;&#21153;&#26102;&#65292;&#23398;&#20064;&#36827;&#23637;&#26159;&#26368;&#22823;&#21270;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#20004;&#20010;&#31616;&#21333;&#30340;&#23398;&#20064;&#35774;&#32622;&#26469;&#25968;&#23398;&#25512;&#23548; ProCuRL&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340; ProCuRL &#21464;&#20307;&#65292;&#21487;&#20197;&#36890;&#36807;&#26368;&#23567;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#30452;&#25509;&#38598;&#25104;&#21040;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#20013;&#12290;&#22312;&#21508;&#31181;&#22495;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21152;&#36895;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#35757;&#32451;&#36807;&#31243;&#26041;&#38754;&#65292;&#25105;&#20204;&#30340;&#35838;&#31243;&#31574;&#30053;&#27604;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#30340;&#22522;&#32447;&#26356;&#21152;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of curriculum design for reinforcement learning (RL) agents in contextual multi-task settings. Existing techniques on automatic curriculum design typically require domain-specific hyperparameter tuning or have limited theoretical underpinnings. To tackle these limitations, we design our curriculum strategy, ProCuRL, inspired by the pedagogical concept of Zone of Proximal Development (ZPD). ProCuRL captures the intuition that learning progress is maximized when picking tasks that are neither too hard nor too easy for the learner. We mathematically derive ProCuRL by analyzing two simple learning settings. We also present a practical variant of ProCuRL that can be directly integrated with deep RL frameworks with minimal hyperparameter tuning. Experimental results on a variety of domains demonstrate the effectiveness of our curriculum strategy over state-of-the-art baselines in accelerating the training process of deep RL agents.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#25104;&#21151;&#22320;&#22312;32&#20301;Cortex-M&#24494;&#25511;&#21046;&#22120;&#19978;&#20351;&#29992;&#28608;&#20809;&#27880;&#20837;&#36827;&#34892;&#20102;&#27604;&#29305;&#32763;&#36716;&#25915;&#20987;&#65292;&#24378;&#35843;&#20102;&#20856;&#22411;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32570;&#20047;&#24378;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.12876</link><description>&lt;p&gt;
&#22522;&#20110;&#28608;&#20809;&#27880;&#20837;&#30340;&#21442;&#25968;&#25915;&#20987;&#23545;&#23884;&#20837;&#24335;&#31070;&#32463;&#32593;&#32476;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Evaluation of Parameter-based Attacks against Embedded Neural Networks with Laser Injection. (arXiv:2304.12876v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12876
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#25104;&#21151;&#22320;&#22312;32&#20301;Cortex-M&#24494;&#25511;&#21046;&#22120;&#19978;&#20351;&#29992;&#28608;&#20809;&#27880;&#20837;&#36827;&#34892;&#20102;&#27604;&#29305;&#32763;&#36716;&#25915;&#20987;&#65292;&#24378;&#35843;&#20102;&#20856;&#22411;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32570;&#20047;&#24378;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#23433;&#20840;&#35748;&#35777;&#34892;&#21160;&#30340;&#21040;&#26469;&#65292;&#21152;&#19978;&#27169;&#22411;&#22312;&#22810;&#20010;&#30828;&#20214;&#24179;&#21488;&#19978;&#30340;&#22823;&#35268;&#27169;&#37096;&#32626;&#65292;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#35780;&#20272;&#25361;&#25112;&#12290;&#26368;&#36817;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#24037;&#20316;&#38598;&#20013;&#22312;&#22522;&#20110;API&#30340;&#25915;&#20987;&#19978;&#65292;&#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35270;&#20026;&#32431;&#31639;&#27861;&#25277;&#35937;&#12290;&#28982;&#32780;&#65292;&#26032;&#30340;&#22522;&#20110;&#23454;&#29616;&#30340;&#23041;&#32961;&#24050;&#34987;&#25581;&#31034;&#65292;&#24378;&#35843;&#25552;&#20986;&#23454;&#36341;&#21644;&#22522;&#20110;&#20223;&#30495;&#30340;&#26041;&#27861;&#65292;&#20197;&#36866;&#24403;&#35780;&#20272;&#27169;&#22411;&#30340;&#24378;&#20581;&#24615;&#12290;&#19968;&#20010;&#20027;&#35201;&#20851;&#27880;&#28857;&#26159;&#22522;&#20110;&#21442;&#25968;&#30340;&#25915;&#20987;&#65288;&#20363;&#22914;&#27604;&#29305;&#32763;&#36716;&#25915;&#20987;&#65289;&#65292;&#24403;&#20869;&#37096;&#23384;&#20648;&#22120;&#20013;&#30340;&#21442;&#25968;&#34987;&#31934;&#30830;&#21644;&#26368;&#20248;&#22320;&#26356;&#25913;&#26102;&#65292;&#24378;&#35843;&#20102;&#20856;&#22411;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#32570;&#20047;&#24378;&#20581;&#24615;&#12290;&#38024;&#23545;&#23433;&#20840;&#27979;&#35797;&#30446;&#30340;&#65292;&#26412;&#30740;&#31350;&#39318;&#27425;&#22312;32&#20301;Cortex-M&#24494;&#25511;&#21046;&#22120;&#19978;&#20351;&#29992;&#28608;&#20809;&#25925;&#38556;&#27880;&#20837;&#65292;&#23454;&#38469;&#25253;&#21578;&#20102;&#25104;&#21151;&#30340;BFA&#21464;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Upcoming certification actions related to the security of machine learning (ML) based systems raise major evaluation challenges that are amplified by the large-scale deployment of models in many hardware platforms. Until recently, most of research works focused on API-based attacks that consider a ML model as a pure algorithmic abstraction. However, new implementation-based threats have been revealed, emphasizing the urgency to propose both practical and simulation-based methods to properly evaluate the robustness of models. A major concern is parameter-based attacks (such as the Bit-Flip Attack, BFA) that highlight the lack of robustness of typical deep neural network models when confronted by accurate and optimal alterations of their internal parameters stored in memory. Setting in a security testing purpose, this work practically reports, for the first time, a successful variant of the BFA on a 32-bit Cortex-M microcontroller using laser fault injection. It is a standard fault injec
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;TnALE&#31639;&#27861;&#65292;&#36890;&#36807;&#20132;&#26367;&#23616;&#37096;&#26522;&#20030;&#26356;&#26032;&#27599;&#20010;&#19982;&#32467;&#26500;&#30456;&#20851;&#30340;&#21464;&#37327;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#35780;&#20272;&#27425;&#25968;&#65292;&#29992;&#20110;&#35299;&#20915;&#24352;&#37327;&#32593;&#32476;&#32467;&#26500;&#25628;&#32034;&#38382;&#39064;&#12290;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#65292;&#22914;&#26524;&#22312;&#27599;&#20010;&#37051;&#22495;&#20013;&#36798;&#21040;&#20102;&#36275;&#22815;&#30340;&#30446;&#26631;&#20989;&#25968;&#38477;&#20302;&#65292;TnALE&#21644;TNLS&#37117;&#21487;&#20197;&#23454;&#29616;&#32447;&#24615;&#25910;&#25947;&#24230;&#65292;&#30452;&#21040;&#19968;&#20010;&#24120;&#25968;&#12290;&#21516;&#26102;&#65292;&#19982;TNLS&#30456;&#27604;&#65292; TnALE&#38656;&#35201;&#26356;&#23569;&#30340;&#35780;&#20272;&#27425;&#25968;&#12290;</title><link>http://arxiv.org/abs/2304.12875</link><description>&lt;p&gt;
&#20132;&#26367;&#23616;&#37096;&#26522;&#20030;(TnALE): &#29992;&#36739;&#23569;&#30340;&#35780;&#20272;&#35299;&#20915;&#24352;&#37327;&#32593;&#32476;&#32467;&#26500;&#25628;&#32034;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Alternating Local Enumeration (TnALE): Solving Tensor Network Structure Search with Fewer Evaluations. (arXiv:2304.12875v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12875
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;TnALE&#31639;&#27861;&#65292;&#36890;&#36807;&#20132;&#26367;&#23616;&#37096;&#26522;&#20030;&#26356;&#26032;&#27599;&#20010;&#19982;&#32467;&#26500;&#30456;&#20851;&#30340;&#21464;&#37327;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#35780;&#20272;&#27425;&#25968;&#65292;&#29992;&#20110;&#35299;&#20915;&#24352;&#37327;&#32593;&#32476;&#32467;&#26500;&#25628;&#32034;&#38382;&#39064;&#12290;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#65292;&#22914;&#26524;&#22312;&#27599;&#20010;&#37051;&#22495;&#20013;&#36798;&#21040;&#20102;&#36275;&#22815;&#30340;&#30446;&#26631;&#20989;&#25968;&#38477;&#20302;&#65292;TnALE&#21644;TNLS&#37117;&#21487;&#20197;&#23454;&#29616;&#32447;&#24615;&#25910;&#25947;&#24230;&#65292;&#30452;&#21040;&#19968;&#20010;&#24120;&#25968;&#12290;&#21516;&#26102;&#65292;&#19982;TNLS&#30456;&#27604;&#65292; TnALE&#38656;&#35201;&#26356;&#23569;&#30340;&#35780;&#20272;&#27425;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24352;&#37327;&#32593;&#32476;(TN)&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#24378;&#22823;&#30340;&#26694;&#26550;&#65292;&#20294;&#36873;&#25321;&#19968;&#20010;&#22909;&#30340;TN&#27169;&#22411;&#65292;&#21363;TN&#32467;&#26500;&#25628;&#32034;(TN-SS)&#65292;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#20219;&#21153;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;TNLS ~ \cite {li2022permutation} &#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#23427;&#30340;&#35745;&#31639;&#25928;&#29575;&#20173;&#28982;&#26159;&#26080;&#27861;&#25215;&#21463;&#30340;&#65292;&#38656;&#35201;&#22826;&#22810;&#35780;&#20272;&#30446;&#26631;&#20989;&#25968;&#30340;&#27425;&#25968;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;TnALE&#65292;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#23616;&#37096;&#26522;&#20030;&#20132;&#26367;&#26356;&#26032;&#27599;&#20010;&#19982;&#32467;&#26500;&#30456;&#20851;&#30340;&#21464;&#37327;&#65292;&#19982;TNLS&#30456;&#27604;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#35780;&#20272;&#27425;&#25968;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#30740;&#31350;&#20102;TNLS&#21644;TnALE&#30340;&#19979;&#38477;&#27493;&#39588;&#65292;&#35777;&#26126;&#22914;&#26524;&#22312;&#27599;&#20010;&#37051;&#22495;&#20013;&#36798;&#21040;&#20102;&#36275;&#22815;&#30340;&#30446;&#26631;&#20989;&#25968;&#38477;&#20302;&#65292;&#37027;&#20040;&#20004;&#31181;&#31639;&#27861;&#37117;&#21487;&#20197;&#23454;&#29616;&#32447;&#24615;&#25910;&#25947;&#24230;&#65292;&#30452;&#21040;&#19968;&#20010;&#24120;&#25968;&#12290;&#25105;&#20204;&#36824;&#27604;&#36739;&#20102;TNLS&#21644;TnALE&#30340;&#35780;&#20272;&#25928;&#29575;&#65292;&#25581;&#31034;&#20102;&#22312;TNLS&#20013;&#36890;&#24120;&#38656;&#35201;&#937;(2 ^ N)&#20010;&#35780;&#20272;&#25165;&#33021;&#22312;&#37051;&#22495;&#20869;&#36798;&#21040;&#30446;&#26631;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tensor network (TN) is a powerful framework in machine learning, but selecting a good TN model, known as TN structure search (TN-SS), is a challenging and computationally intensive task. The recent approach TNLS~\cite{li2022permutation} showed promising results for this task, however, its computational efficiency is still unaffordable, requiring too many evaluations of the objective function. We propose TnALE, a new algorithm that updates each structure-related variable alternately by local enumeration, \emph{greatly} reducing the number of evaluations compared to TNLS. We theoretically investigate the descent steps for TNLS and TnALE, proving that both algorithms can achieve linear convergence up to a constant if a sufficient reduction of the objective is \emph{reached} in each neighborhood. We also compare the evaluation efficiency of TNLS and TnALE, revealing that $\Omega(2^N)$ evaluations are typically required in TNLS for \emph{reaching} the objective reduction in the neighborhood
&lt;/p&gt;</description></item><item><title>&#20108;&#36827;&#21046;&#38543;&#26426;&#23398;&#20064;&#31639;&#27861;&#25552;&#39640;&#20102;&#28145;&#24230;&#23398;&#20064;&#25928;&#29575;&#65292;&#28040;&#38500;&#20102;&#20351;&#29992;&#22122;&#22768;&#20154;&#24037;&#31361;&#35302;&#30828;&#20214;&#31995;&#32479;&#30340;&#25361;&#25112;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.12866</link><description>&lt;p&gt;
&#20108;&#36827;&#21046;&#38543;&#26426;&#24615;&#25903;&#25345;&#30340;&#31070;&#32463;&#24418;&#24577;&#28145;&#24230;&#23398;&#20064;&#23454;&#29616;&#27604;&#36719;&#20214;&#26356;&#39640;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Binary stochasticity enabled highly efficient neuromorphic deep learning achieves better-than-software accuracy. (arXiv:2304.12866v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12866
&lt;/p&gt;
&lt;p&gt;
&#20108;&#36827;&#21046;&#38543;&#26426;&#23398;&#20064;&#31639;&#27861;&#25552;&#39640;&#20102;&#28145;&#24230;&#23398;&#20064;&#25928;&#29575;&#65292;&#28040;&#38500;&#20102;&#20351;&#29992;&#22122;&#22768;&#20154;&#24037;&#31361;&#35302;&#30828;&#20214;&#31995;&#32479;&#30340;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#38656;&#35201;&#31934;&#20934;&#30340;&#20449;&#21495;&#36716;&#21457;&#22788;&#29702;&#12289;&#21453;&#21521;&#20256;&#25773;&#35823;&#24046;&#21644;&#26435;&#37325;&#26356;&#26032;&#12290;&#36825;&#26159;&#30001;&#20110;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#35268;&#21017;&#20381;&#36182;&#20110;&#20559;&#23548;&#25968;&#30340;&#38142;&#24335;&#20056;&#31215;&#65292;&#23398;&#20064;&#31639;&#27861;&#26412;&#36136;&#19978;&#38656;&#35201;&#36825;&#19968;&#28857;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#20855;&#26377;&#22122;&#22768;&#30340;&#20154;&#24037;&#31361;&#35302;&#30340;&#30828;&#20214;&#31995;&#32479;&#20013;&#23454;&#29616;&#28145;&#24230;&#23398;&#20064;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#36825;&#19981;&#31526;&#21512;&#29983;&#29289;&#23398;&#21407;&#29702;&#12290;&#22522;&#20110;&#33180;&#30005;&#38459;&#22120;&#30340;&#23454;&#26045;&#36890;&#24120;&#23548;&#33268;&#36807;&#22810;&#30340;&#31070;&#32463;&#30005;&#36335;&#25104;&#26412;&#21644;&#23545;&#29702;&#24819;&#21270;&#31361;&#35302;&#35774;&#22791;&#30340;&#20005;&#26684;&#38656;&#27714;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#39640;&#31934;&#24230;&#19981;&#26159;&#24517;&#35201;&#30340;&#65292;&#27809;&#26377;&#36825;&#20010;&#35201;&#27714;&#21487;&#20197;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#28145;&#24230;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20108;&#36827;&#21046;&#38543;&#26426;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;(i)&#27491;&#12289;&#21453;&#21521;&#20449;&#21495;&#30340;&#38543;&#26426;&#20108;&#20540;&#21270; (ii) &#26377;&#31526;&#21495;&#20108;&#20540;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#25152;&#26377;&#22522;&#26412;&#25805;&#20316;&#36827;&#34892;&#20462;&#25913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning needs high-precision handling of forwarding signals, backpropagating errors, and updating weights. This is inherently required by the learning algorithm since the gradient descent learning rule relies on the chain product of partial derivatives. However, it is challenging to implement deep learning in hardware systems that use noisy analog memristors as artificial synapses, as well as not being biologically plausible. Memristor-based implementations generally result in an excessive cost of neuronal circuits and stringent demands for idealized synaptic devices. Here, we demonstrate that the requirement for high precision is not necessary and that more efficient deep learning can be achieved when this requirement is lifted. We propose a binary stochastic learning algorithm that modifies all elementary neural network operations, by introducing (i) stochastic binarization of both the forwarding signals and the activation function derivatives, (ii) signed binarization of the b
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#24863;&#20852;&#36259;&#30340;&#31995;&#32479;&#20013;&#25191;&#34892;&#21160;&#21147;&#23398;&#19981;&#21464;&#37327;&#65288;&#22914;&#26446;&#38597;&#26222;&#35834;&#22827;&#25351;&#25968;&#35889;&#21644;&#20998;&#24418;&#32500;&#25968;&#65289;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#28151;&#27788;&#21160;&#21147;&#23398;&#31995;&#32479;&#12290;&#36890;&#36807;&#20351;&#29992;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#35813;&#25216;&#26415;&#21487;&#29992;&#20110;&#36827;&#34892;&#26356;&#38271;&#26102;&#38388;&#21644;&#26356;&#31283;&#23450;&#30340;&#39044;&#27979;&#65292;&#36866;&#29992;&#20110;&#20197;Lorenz 1996&#28151;&#27788;&#21160;&#21147;&#23398;&#31995;&#32479;&#21644;&#20809;&#35889;&#25311;&#20934;&#31561;&#20301;&#27169;&#22411;&#20026;&#20195;&#34920;&#30340;&#20856;&#22411;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#27979;&#35797;&#26696;&#20363;&#12290;</title><link>http://arxiv.org/abs/2304.12865</link><description>&lt;p&gt;
&#32422;&#26463;&#28151;&#27788;&#65306;&#22312;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#21152;&#24378;&#21160;&#21147;&#23398;&#19981;&#21464;&#37327;
&lt;/p&gt;
&lt;p&gt;
Constraining Chaos: Enforcing dynamical invariants in the training of recurrent neural networks. (arXiv:2304.12865v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12865
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#24863;&#20852;&#36259;&#30340;&#31995;&#32479;&#20013;&#25191;&#34892;&#21160;&#21147;&#23398;&#19981;&#21464;&#37327;&#65288;&#22914;&#26446;&#38597;&#26222;&#35834;&#22827;&#25351;&#25968;&#35889;&#21644;&#20998;&#24418;&#32500;&#25968;&#65289;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#28151;&#27788;&#21160;&#21147;&#23398;&#31995;&#32479;&#12290;&#36890;&#36807;&#20351;&#29992;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#35813;&#25216;&#26415;&#21487;&#29992;&#20110;&#36827;&#34892;&#26356;&#38271;&#26102;&#38388;&#21644;&#26356;&#31283;&#23450;&#30340;&#39044;&#27979;&#65292;&#36866;&#29992;&#20110;&#20197;Lorenz 1996&#28151;&#27788;&#21160;&#21147;&#23398;&#31995;&#32479;&#21644;&#20809;&#35889;&#25311;&#20934;&#31561;&#20301;&#27169;&#22411;&#20026;&#20195;&#34920;&#30340;&#20856;&#22411;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#27979;&#35797;&#26696;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20511;&#37492;&#36941;&#21382;&#35770;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#39044;&#27979;&#28151;&#27788;&#21160;&#21147;&#23398;&#31995;&#32479;&#12290;&#36825;&#31181;&#35757;&#32451;&#26041;&#27861;&#36890;&#36807;&#22312;&#24863;&#20852;&#36259;&#30340;&#31995;&#32479;&#20013;&#25191;&#34892;&#21160;&#21147;&#23398;&#19981;&#21464;&#37327;&#65288;&#22914;&#26446;&#38597;&#26222;&#35834;&#22827;&#25351;&#25968;&#35889;&#21644;&#20998;&#24418;&#32500;&#25968;&#65289;&#65292;&#20351;&#25805;&#20316;&#21463;&#38480;&#30340;&#25968;&#25454;&#19979;&#36827;&#34892;&#26356;&#38271;&#26102;&#38388;&#21644;&#26356;&#31283;&#23450;&#30340;&#39044;&#27979;&#25104;&#20026;&#21487;&#33021;&#12290;&#25105;&#20204;&#20351;&#29992;&#20648;&#23384;&#22120;&#35745;&#31639;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#35814;&#32454;&#28436;&#31034;&#20102;&#35813;&#25216;&#26415;&#22312;&#20197;Lorenz 1996&#28151;&#27788;&#21160;&#21147;&#23398;&#31995;&#32479;&#21644;&#20809;&#35889;&#25311;&#20934;&#31561;&#20301;&#27169;&#22411;&#20026;&#20195;&#34920;&#30340;&#20856;&#22411;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#27979;&#35797;&#26696;&#20363;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Drawing on ergodic theory, we introduce a novel training method for machine learning based forecasting methods for chaotic dynamical systems. The training enforces dynamical invariants--such as the Lyapunov exponent spectrum and fractal dimension--in the systems of interest, enabling longer and more stable forecasts when operating with limited data. The technique is demonstrated in detail using the recurrent neural network architecture of reservoir computing. Results are given for the Lorenz 1996 chaotic dynamical system and a spectral quasi-geostrophic model, both typical test cases for numerical weather prediction.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#32773;&#22312;Stack Overflow&#19978;&#25366;&#25496;&#20102;11,449&#20010;&#19982;&#19971;&#20010;&#24120;&#29992;Python ML&#24211;&#30456;&#20851;&#30340;&#22534;&#26632;&#36319;&#36394;&#65292;&#21457;&#29616;&#21253;&#21547;&#22534;&#26632;&#36319;&#36394;&#30340;ML&#38382;&#39064;&#26356;&#21463;&#27426;&#36814;&#65292;&#32780;&#23427;&#20204;&#30340;&#38382;&#39064;&#26356;&#22797;&#26434;&#12290;</title><link>http://arxiv.org/abs/2304.12857</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#24322;&#24120;&#26159;&#20160;&#20040;&#21407;&#22240;&#65311;&#22312;Stack Overflow&#19978;&#25366;&#25496;&#26426;&#22120;&#23398;&#20064;&#30456;&#20851;&#22534;&#26632;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
What Causes Exceptions in Machine Learning Applications? Mining Machine Learning-Related Stack Traces on Stack Overflow. (arXiv:2304.12857v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12857
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#32773;&#22312;Stack Overflow&#19978;&#25366;&#25496;&#20102;11,449&#20010;&#19982;&#19971;&#20010;&#24120;&#29992;Python ML&#24211;&#30456;&#20851;&#30340;&#22534;&#26632;&#36319;&#36394;&#65292;&#21457;&#29616;&#21253;&#21547;&#22534;&#26632;&#36319;&#36394;&#30340;ML&#38382;&#39064;&#26356;&#21463;&#27426;&#36814;&#65292;&#32780;&#23427;&#20204;&#30340;&#38382;&#39064;&#26356;&#22797;&#26434;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#65292;&#21253;&#25324;&#28145;&#24230;&#23398;&#20064;&#65292;&#26368;&#36817;&#22312;&#24191;&#27867;&#30340;&#24212;&#29992;&#20013;&#33719;&#24471;&#20102;&#24040;&#22823;&#30340;&#27969;&#34892;&#12290;&#28982;&#32780;&#65292;&#19982;&#20256;&#32479;&#36719;&#20214;&#19968;&#26679;&#65292;ML&#24212;&#29992;&#20063;&#19981;&#20813;&#20110;&#30001;&#31243;&#24207;&#38169;&#35823;&#23548;&#33268;&#30340;&#38169;&#35823;&#12290;&#26174;&#24335;&#30340;&#32534;&#31243;&#38169;&#35823;&#36890;&#24120;&#36890;&#36807;&#38169;&#35823;&#28040;&#24687;&#21644;&#22534;&#26632;&#36319;&#36394;&#34920;&#29616;&#20986;&#26469;&#12290;&#36825;&#20123;&#22534;&#26632;&#36319;&#36394;&#25551;&#36848;&#20102;&#23548;&#33268;&#24322;&#24120;&#24773;&#20917;&#30340;&#20989;&#25968;&#35843;&#29992;&#38142;&#12290;&#20107;&#23454;&#19978;&#65292;&#36825;&#20123;&#24322;&#24120;&#21487;&#33021;&#36328;&#36234;&#25972;&#20010;&#36719;&#20214;&#22534;&#26632;&#65288;&#21253;&#25324;&#24212;&#29992;&#31243;&#24207;&#21644;&#24211;&#65289;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#22534;&#26632;&#36319;&#36394;&#20013;&#30340;&#27169;&#24335;&#21487;&#20197;&#24110;&#21161;&#23454;&#36341;&#32773;&#21644;&#30740;&#31350;&#20154;&#21592;&#20102;&#35299;ML&#24212;&#29992;&#31243;&#24207;&#20013;&#24322;&#24120;&#30340;&#21407;&#22240;&#20197;&#21450;ML&#24320;&#21457;&#20154;&#21592;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22312;Stack Overflow&#65288;SO&#65289;&#19978;&#36827;&#34892;&#25366;&#25496;&#24182;&#30740;&#31350;&#20102;&#19982;&#19971;&#20010;&#27969;&#34892;&#30340;Python ML&#24211;&#30456;&#20851;&#30340;11,449&#20010;&#22534;&#26632;&#36319;&#36394;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#21253;&#21547;&#22534;&#26632;&#36319;&#36394;&#30340;ML&#38382;&#39064;&#27604;&#27809;&#26377;&#22534;&#26632;&#36319;&#36394;&#30340;&#38382;&#39064;&#26356;&#21463;&#27426;&#36814;&#65307;&#28982;&#32780;&#65292;&#23427;&#20204;&#26356;&#19981;&#21487;&#33021;&#24471;&#21040;&#25509;&#21463;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML), including deep learning, has recently gained tremendous popularity in a wide range of applications. However, like traditional software, ML applications are not immune to the bugs that result from programming errors. Explicit programming errors usually manifest through error messages and stack traces. These stack traces describe the chain of function calls that lead to an anomalous situation, or exception. Indeed, these exceptions may cross the entire software stack (including applications and libraries). Thus, studying the patterns in stack traces can help practitioners and researchers understand the causes of exceptions in ML applications and the challenges faced by ML developers. To that end, we mine Stack Overflow (SO) and study 11,449 stack traces related to seven popular Python ML libraries. First, we observe that ML questions that contain stack traces gain more popularity than questions without stack traces; however, they are less likely to get accepted ans
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20998;&#36776;&#29575;&#19978;&#19979;&#25991;&#32593;&#32476;&#21644;&#23545;&#25239;&#24615;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#20934;&#30830;&#30340;&#35270;&#32593;&#33180;&#34880;&#31649;&#20998;&#21106;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#39640;&#20998;&#21106;&#24615;&#33021;&#21644;&#36739;&#23569;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2304.12856</link><description>&lt;p&gt;
&#19968;&#31181;&#22810;&#20998;&#36776;&#29575;&#19978;&#19979;&#25991;&#32593;&#32476;&#21644;&#23545;&#25239;&#24615;&#23398;&#20064;&#30340;&#35270;&#32593;&#33180;&#34880;&#31649;&#20998;&#21106;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Retinal Vessel Segmentation via a Multi-resolution Contextual Network and Adversarial Learning. (arXiv:2304.12856v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12856
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20998;&#36776;&#29575;&#19978;&#19979;&#25991;&#32593;&#32476;&#21644;&#23545;&#25239;&#24615;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#20934;&#30830;&#30340;&#35270;&#32593;&#33180;&#34880;&#31649;&#20998;&#21106;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#39640;&#20998;&#21106;&#24615;&#33021;&#21644;&#36739;&#23569;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24555;&#36895;&#12289;&#26222;&#24800;&#30340;&#35745;&#31639;&#26426;&#36741;&#21161;&#35786;&#26029;&#23545;&#20110;&#36991;&#20813;&#22833;&#26126;&#38750;&#24120;&#20851;&#38190;&#65292;&#32780;&#20934;&#30830;&#30340;&#35270;&#32593;&#33180;&#34880;&#31649;&#20998;&#21106;&#22312;&#36825;&#20123;&#23041;&#32961;&#35270;&#21147;&#30142;&#30149;&#30340;&#30142;&#30149;&#36827;&#23637;&#21644;&#35786;&#26029;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20998;&#36776;&#29575;&#19978;&#19979;&#25991;&#32593;&#32476;&#21644;&#23545;&#25239;&#24615;&#23398;&#20064;&#30340;&#26041;&#27861;&#65288;MRC-Net&#65289;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#36890;&#36807;&#25552;&#21462;&#22810;&#23610;&#24230;&#29305;&#24449;&#23398;&#20064;&#35821;&#20041;&#19981;&#21516;&#30340;&#29305;&#24449;&#38388;&#30340;&#19978;&#19979;&#25991;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#20351;&#29992;&#21452;&#21521;&#24490;&#29615;&#23398;&#20064;&#26469;&#24314;&#27169;&#21069;&#21518;&#21644;&#21518;&#21069;&#20381;&#36182;&#20851;&#31995;&#12290;&#21478;&#19968;&#20010;&#20851;&#38190;&#24605;&#24819;&#26159;&#37319;&#29992;&#23545;&#25239;&#24615;&#35757;&#32451;&#26469;&#36890;&#36807;&#21306;&#22495;&#24471;&#20998;&#30340;&#20248;&#21270;&#25552;&#39640;&#21069;&#26223;&#20998;&#21106;&#30340;&#24615;&#33021;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#31574;&#30053;&#22312;&#20445;&#25345;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#36739;&#20302;&#30340;&#21516;&#26102;&#65292;&#25552;&#39640;&#20102;&#20998;&#21106;&#32593;&#32476;&#30340;Dice&#24471;&#20998;&#65288;&#20197;&#21450;&#30456;&#24212;&#30340;Jaccard&#25351;&#25968;&#65289;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#21253;&#25324;DRIVE&#12289;STARE&#21644;CHASE&#22312;&#20869;&#30340;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#26412;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Timely and affordable computer-aided diagnosis of retinal diseases is pivotal in precluding blindness. Accurate retinal vessel segmentation plays an important role in disease progression and diagnosis of such vision-threatening diseases. To this end, we propose a Multi-resolution Contextual Network (MRC-Net) that addresses these issues by extracting multi-scale features to learn contextual dependencies between semantically different features and using bi-directional recurrent learning to model former-latter and latter-former dependencies. Another key idea is training in adversarial settings for foreground segmentation improvement through optimization of the region-based scores. This novel strategy boosts the performance of the segmentation network in terms of the Dice score (and correspondingly Jaccard index) while keeping the number of trainable parameters comparatively low. We have evaluated our method on three benchmark datasets, including DRIVE, STARE, and CHASE, demonstrating its 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25968;&#23383;&#20581;&#24247;&#23402;&#29983;&#31995;&#32479;&#30340;&#33258;&#36866;&#24212;&#26381;&#21153;&#21151;&#33021;&#38142;&#32534;&#25490;&#26041;&#27861;&#65292;&#33021;&#22815;&#26681;&#25454;&#19981;&#21516;&#25968;&#25454;&#20849;&#20139;&#22330;&#26223;&#25552;&#20379;&#23433;&#20840;&#31574;&#30053;&#65292;&#24182;&#32771;&#34385;&#21040;&#39044;&#26399;&#30340;&#29992;&#20363;&#12289;&#31574;&#30053;&#21644;&#22522;&#30784;&#35774;&#26045;&#37197;&#32622;&#65292;&#21160;&#24577;&#22320;&#25552;&#20379;&#26381;&#21153;&#32534;&#25490;&#21644;&#36335;&#30001;&#37197;&#32622;&#12290;</title><link>http://arxiv.org/abs/2304.12853</link><description>&lt;p&gt;
&#25968;&#23383;&#20581;&#24247;&#23402;&#29983;&#20351;&#29992;&#26696;&#20363;&#30340;&#33258;&#36866;&#24212;&#26381;&#21153;&#21151;&#33021;&#38142;&#32534;&#25490;: &#21551;&#21457;&#24335;&#22686;&#24378;&#30340;Q&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Adaptive Services Function Chain Orchestration For Digital Health Twin Use Cases: Heuristic-boosted Q-Learning Approach. (arXiv:2304.12853v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12853
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25968;&#23383;&#20581;&#24247;&#23402;&#29983;&#31995;&#32479;&#30340;&#33258;&#36866;&#24212;&#26381;&#21153;&#21151;&#33021;&#38142;&#32534;&#25490;&#26041;&#27861;&#65292;&#33021;&#22815;&#26681;&#25454;&#19981;&#21516;&#25968;&#25454;&#20849;&#20139;&#22330;&#26223;&#25552;&#20379;&#23433;&#20840;&#31574;&#30053;&#65292;&#24182;&#32771;&#34385;&#21040;&#39044;&#26399;&#30340;&#29992;&#20363;&#12289;&#31574;&#30053;&#21644;&#22522;&#30784;&#35774;&#26045;&#37197;&#32622;&#65292;&#21160;&#24577;&#22320;&#25552;&#20379;&#26381;&#21153;&#32534;&#25490;&#21644;&#36335;&#30001;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#23402;&#29983;(DT)&#26159;&#21033;&#29992;&#21644;&#37096;&#32626;&#22312;&#21307;&#30103;&#20445;&#20581;&#34892;&#19994;&#30340;&#31361;&#20986;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#27492;&#31867;&#24212;&#29992;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;: &#20005;&#26684;&#30340;&#21307;&#30103;&#25968;&#25454;&#20849;&#20139;&#25919;&#31574;&#12289;&#39640;&#24615;&#33021;&#32593;&#32476;&#35201;&#27714;&#21644;&#21487;&#33021;&#30340;&#22522;&#30784;&#35774;&#26045;&#36164;&#28304;&#38480;&#21046;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20379;&#33258;&#36866;&#24212;&#34394;&#25311;&#32593;&#32476;&#21151;&#33021;(VNF)&#65292;&#20197;&#25191;&#34892;&#19982;&#19981;&#21516;&#25968;&#25454;&#20849;&#20139;&#22330;&#26223;&#30456;&#20851;&#30340;&#23433;&#20840;&#31574;&#30053;&#65292;&#26469;&#35299;&#20915;&#25152;&#26377;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#22312;&#22810;&#33410;&#28857;&#38598;&#32676;&#32593;&#26684;&#22522;&#30784;&#35774;&#26045;&#19978;&#23450;&#20041;&#20102;&#19968;&#20010;&#20113;&#21407;&#29983;&#32593;&#32476;&#32534;&#25490;&#22120;&#65292;&#29992;&#20110;&#28789;&#27963;&#21644;&#21160;&#24577;&#30340;&#23481;&#22120;&#35843;&#24230;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#32771;&#34385;&#20102;&#39044;&#26399;&#30340;&#25968;&#25454;&#20849;&#20139;&#29992;&#20363;&#12289;&#30456;&#20851;&#31574;&#30053;&#21644;&#22522;&#30784;&#35774;&#26045;&#37197;&#32622;&#65292;&#28982;&#21518;&#37197;&#32622;&#26381;&#21153;&#21151;&#33021;&#38142;(SFC)&#24182;&#30456;&#24212;&#22320;&#25552;&#20379;&#36335;&#30001;&#37197;&#32622;&#65292;&#20960;&#20046;&#27809;&#26377;&#20154;&#20026;&#24178;&#39044;&#12290;&#27492;&#22806;&#65292;&#37096;&#32626;SFC&#26102;&#30340;&#8220;&#26368;&#20248;&#8221;&#21462;&#20915;&#20110;&#29992;&#20363;&#26412;&#36523;&#65292;&#25105;&#20204;&#35843;&#25972;&#36229;&#21442;&#25968;&#20197;&#20248;&#20808;&#32771;&#34385;&#29992;&#20363;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Digital Twin (DT) is a prominent technology to utilise and deploy within the healthcare sector. Yet, the main challenges facing such applications are: Strict health data-sharing policies, high-performance network requirements, and possible infrastructure resource limitations. In this paper, we address all the challenges by provisioning adaptive Virtual Network Functions (VNFs) to enforce security policies associated with different data-sharing scenarios. We define a Cloud-Native Network orchestrator on top of a multi-node cluster mesh infrastructure for flexible and dynamic container scheduling. The proposed framework considers the intended data-sharing use case, the policies associated, and infrastructure configurations, then provision Service Function Chaining (SFC) and provides routing configurations accordingly with little to no human intervention. Moreover, what is \textit{optimal} when deploying SFC is dependent on the use case itself, and we tune the hyperparameters to prioritis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312; LDP &#19979;&#25910;&#38598;&#22810;&#20010;&#25935;&#24863;&#23646;&#24615;&#23545;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#24182;&#25552;&#20986;&#20102;&#32771;&#34385;&#22495;&#22823;&#23567;&#30340;&#26032;&#30340;&#38544;&#31169;&#39044;&#31639;&#20998;&#37197;&#26041;&#26696;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#26696;&#22312;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#20844;&#24179;&#24615;&#26041;&#38754;&#22343;&#20248;&#20110;&#26368;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;LDP &#24102;&#26469;&#20102;&#30053;&#24494;&#25913;&#21892;&#30340;&#20844;&#24179;&#24615;&#32780;&#19981;&#20250;&#26126;&#26174;&#24433;&#21709;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.12845</link><description>&lt;p&gt;
&#65288;&#26412;&#22320;&#65289;&#24046;&#20998;&#38544;&#31169;&#23545;&#20844;&#24179;&#24615;&#27809;&#26377;&#24102;&#26469;&#19981;&#24179;&#31561;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
(Local) Differential Privacy has NO Disparate Impact on Fairness. (arXiv:2304.12845v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12845
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312; LDP &#19979;&#25910;&#38598;&#22810;&#20010;&#25935;&#24863;&#23646;&#24615;&#23545;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#24182;&#25552;&#20986;&#20102;&#32771;&#34385;&#22495;&#22823;&#23567;&#30340;&#26032;&#30340;&#38544;&#31169;&#39044;&#31639;&#20998;&#37197;&#26041;&#26696;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#26696;&#22312;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#20844;&#24179;&#24615;&#26041;&#38754;&#22343;&#20248;&#20110;&#26368;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;LDP &#24102;&#26469;&#20102;&#30053;&#24494;&#25913;&#21892;&#30340;&#20844;&#24179;&#24615;&#32780;&#19981;&#20250;&#26126;&#26174;&#24433;&#21709;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#65288;LDP&#65289;&#20316;&#20026;&#24378;&#22823;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807; LDP&#65292;&#29992;&#25143;&#21487;&#20197;&#22312;&#23558;&#25968;&#25454;&#20256;&#36755;&#20986;&#21435;&#21069;&#22312;&#35774;&#22791;&#19978;&#23545;&#20854;&#36827;&#34892;&#25200;&#21160;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#22312;&#21508;&#20010;&#34892;&#19994;&#20013;&#25910;&#38598;&#22810;&#20010;&#25935;&#24863;&#20449;&#24687;&#30340;&#24773;&#20917;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#20165;&#25910;&#38598;&#21333;&#20010;&#25935;&#24863;&#23646;&#24615;&#21487;&#33021;&#24050;&#32463;&#19981;&#36275;&#20197;&#20445;&#38556;&#29992;&#25143;&#30340;&#38544;&#31169;&#12290;&#25968;&#25454;&#20013;&#30340;&#30456;&#20851;&#23646;&#24615;&#20173;&#28982;&#21487;&#33021;&#23548;&#33268;&#23545;&#25935;&#24863;&#23646;&#24615;&#30340;&#25512;&#26029;&#12290;&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#22312; LDP &#19979;&#25910;&#38598;&#22810;&#20010;&#25935;&#24863;&#23646;&#24615;&#23545;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38544;&#31169;&#39044;&#31639;&#20998;&#37197;&#26041;&#26696;&#65292;&#32771;&#34385;&#21040;&#25935;&#24863;&#23646;&#24615;&#30340;&#19981;&#21516;&#22495;&#22823;&#23567;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#36825;&#36890;&#24120;&#27604;&#29616;&#26377;&#26368;&#26032;&#35299;&#20915;&#26041;&#26696;&#30340;&#38544;&#31169;-&#25928;&#29992;-&#20844;&#24179;&#24615;&#26435;&#34913;&#26041;&#24335;&#26356;&#22909;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;LDP &#22312;&#23398;&#20064;&#38382;&#39064;&#20013;&#24102;&#26469;&#20102;&#30053;&#24494;&#25913;&#21892;&#30340;&#20844;&#24179;&#24615;&#65292;&#32780;&#19981;&#20250;&#26126;&#26174;&#24433;&#21709;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, Local Differential Privacy (LDP), a robust privacy-preserving methodology, has gained widespread adoption in real-world applications. With LDP, users can perturb their data on their devices before sending it out for analysis. However, as the collection of multiple sensitive information becomes more prevalent across various industries, collecting a single sensitive attribute under LDP may not be sufficient. Correlated attributes in the data may still lead to inferences about the sensitive attribute. This paper empirically studies the impact of collecting multiple sensitive attributes under LDP on fairness. We propose a novel privacy budget allocation scheme that considers the varying domain size of sensitive attributes. This generally led to a better privacy-utility-fairness trade-off in our experiments than the state-of-art solution. Our results show that LDP leads to slightly improved fairness in learning problems without significantly affecting the performance of the
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20449;&#24687;&#29702;&#35770;&#27010;&#24565; troenpy &#26469;&#37327;&#21270;&#24213;&#23618;&#20998;&#24067;&#30340;&#30830;&#23450;&#24615;&#65292;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20013;&#25991;&#26723;&#20998;&#31867;&#21644;&#24207;&#21015;&#25968;&#25454;&#26435;&#37325;&#26041;&#26696;&#65292;&#24182;&#23450;&#20041;&#20102;&#37327;&#23376; troenpy &#37327;&#21270;&#37327;&#23376;&#31995;&#32479;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.12833</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#30830;&#23450;&#24615;&#20449;&#24687;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
A New Information Theory of Certainty for Machine Learning. (arXiv:2304.12833v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12833
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20449;&#24687;&#29702;&#35770;&#27010;&#24565; troenpy &#26469;&#37327;&#21270;&#24213;&#23618;&#20998;&#24067;&#30340;&#30830;&#23450;&#24615;&#65292;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20013;&#25991;&#26723;&#20998;&#31867;&#21644;&#24207;&#21015;&#25968;&#25454;&#26435;&#37325;&#26041;&#26696;&#65292;&#24182;&#23450;&#20041;&#20102;&#37327;&#23376; troenpy &#37327;&#21270;&#37327;&#23376;&#31995;&#32479;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20811;&#21171;&#24503;&#183;&#39321;&#20892;&#25552;&#20986;&#20102;&#29109;&#30340;&#27010;&#24565;&#26469;&#37327;&#21270;&#36890;&#20449;&#32534;&#30721;&#29702;&#35770;&#20013;&#38543;&#26426;&#20998;&#24067;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#29109;&#30340;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#29305;&#24615;&#20063;&#38480;&#21046;&#20102;&#20854;&#22312;&#25968;&#23398;&#24314;&#27169;&#20013;&#30340;&#30452;&#25509;&#20351;&#29992;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#27010;&#24565; troenpy&#65292;&#20316;&#20026;&#29109;&#30340;&#35268;&#33539;&#23545;&#20598;&#65292;&#26469;&#37327;&#21270;&#24213;&#23618;&#20998;&#24067;&#30340;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20004;&#20010;&#24212;&#29992;&#12290;&#31532;&#19968;&#20010;&#26159;&#29992;&#20110;&#20256;&#32479;&#30340;&#25991;&#26723;&#20998;&#31867;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110; troenpy &#26435;&#37325;&#26041;&#26696;&#26469;&#21033;&#29992;&#25991;&#26723;&#20998;&#31867;&#26631;&#31614;&#12290;&#31532;&#20108;&#20010;&#26159;&#38024;&#23545;&#24207;&#21015;&#25968;&#25454;&#30340;&#33258;&#25105; troenpy &#26435;&#37325;&#26041;&#26696;&#65292;&#24182;&#34920;&#26126;&#23427;&#21487;&#20197;&#36731;&#26494;&#22320;&#21253;&#21547;&#22312;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#24182;&#23454;&#29616;&#26174;&#33879;&#30340;&#22256;&#24785;&#24230;&#38477;&#20302;&#12290;&#25105;&#20204;&#36824;&#23450;&#20041;&#20102;&#37327;&#23376; troenpy &#20316;&#20026; Von Neumann &#29109;&#30340;&#23545;&#20598;&#65292;&#20197;&#37327;&#21270;&#37327;&#23376;&#31995;&#32479;&#30340;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Claude Shannon coined entropy to quantify the uncertainty of a random distribution for communication coding theory. We observe that the uncertainty nature of entropy also limits its direct usage in mathematical modeling. Therefore we propose a new concept troenpy,as the canonical dual of entropy, to quantify the certainty of the underlying distribution. We demonstrate two applications in machine learning. The first is for the classical document classification, we develop a troenpy based weighting scheme to leverage the document class label. The second is a self-troenpy weighting scheme for sequential data and show that it can be easily included in neural network based language models and achieve dramatic perplexity reduction. We also define quantum troenpy as the dual of the Von Neumann entropy to quantify the certainty of quantum systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#28145;&#24230;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#20013;&#37319;&#29992;&#33258;&#21160;&#37327;&#21270;&#35757;&#32451;&#26694;&#26550;&#21644;&#28145;&#24230;&#37327;&#21270;&#35823;&#24046;&#35745;&#31639;&#31561;&#26041;&#27861;&#26469;&#25552;&#39640;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.12829</link><description>&lt;p&gt;
&#29992;&#28145;&#24230;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#25552;&#39640;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Robustness Against Adversarial Attacks with Deeply Quantized Neural Networks. (arXiv:2304.12829v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12829
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#28145;&#24230;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#20013;&#37319;&#29992;&#33258;&#21160;&#37327;&#21270;&#35757;&#32451;&#26694;&#26550;&#21644;&#28145;&#24230;&#37327;&#21270;&#35823;&#24046;&#35745;&#31639;&#31561;&#26041;&#27861;&#26469;&#25552;&#39640;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20943;&#23567;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#23588;&#20854;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#23384;&#20648;&#31354;&#38388;&#65292;&#26159;&#23558;&#23427;&#20204;&#37096;&#32626;&#21040;&#36164;&#28304;&#21463;&#38480;&#30340;&#23567;&#22411;&#35774;&#22791;&#20013;&#30340;&#37325;&#35201;&#21069;&#25552;&#12290;&#28982;&#32780;&#65292;DNN&#27169;&#22411;&#30340;&#19968;&#20010;&#32570;&#28857;&#26159;&#23427;&#20204;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#22240;&#20026;&#22312;&#36755;&#20837;&#20013;&#28155;&#21152;&#36731;&#24494;&#30340;&#25200;&#21160;&#21487;&#20197;&#27450;&#39575;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#25361;&#25112;&#22312;&#20110;&#22914;&#20309;&#21019;&#24314;&#20934;&#30830;&#12289;&#31283;&#20581;&#24182;&#19988;&#21487;&#37096;&#32626;&#22312;&#36164;&#28304;&#21463;&#38480;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#30340;&#23567;&#22411;DNN&#27169;&#22411;&#12290;&#26412;&#25991;&#36890;&#36807;&#37319;&#29992;&#19968;&#31181;&#33258;&#21160;&#37327;&#21270;&#35757;&#32451;&#26694;&#26550;QKeras&#35757;&#32451;&#19968;&#20010;&#23567;&#22411;DNN&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#23545;&#30333;&#30418;&#21644;&#40657;&#30418;&#23545;&#25239;&#25915;&#20987;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#35745;&#31639;&#28145;&#24230;&#37327;&#21270;&#35823;&#24046;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#35774;&#35745;&#30340;DNN&#27169;&#22411;&#30340;&#31934;&#24230;&#65292;&#20351;&#23427;&#36866;&#29992;&#20110;&#23567;&#22411;&#35774;&#22791;&#30340;&#37096;&#32626;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;QKeras&#21644;&#19968;&#31181;&#23545;&#25239;&#40065;&#26834;&#24615;&#25216;&#26415;Jacobian Regularization&#65288;JR&#65289;&#22914;&#20309;&#36890;&#36807;&#21033;&#29992;DNN&#25299;&#25169;&#32467;&#26500;&#21644;&#27599;&#23618;JR&#26041;&#27861;&#25552;&#20379;&#19968;&#31181;&#20849;&#21516;&#20248;&#21270;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#28145;&#24230;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reducing the memory footprint of Machine Learning (ML) models, particularly Deep Neural Networks (DNNs), is essential to enable their deployment into resource-constrained tiny devices. However, a disadvantage of DNN models is their vulnerability to adversarial attacks, as they can be fooled by adding slight perturbations to the inputs. Therefore, the challenge is how to create accurate, robust, and tiny DNN models deployable on resource-constrained embedded devices. This paper reports the results of devising a tiny DNN model, robust to adversarial black and white box attacks, trained with an automatic quantizationaware training framework, i.e. QKeras, with deep quantization loss accounted in the learning loop, thereby making the designed DNNs more accurate for deployment on tiny devices. We investigated how QKeras and an adversarial robustness technique, Jacobian Regularization (JR), can provide a co-optimization strategy by exploiting the DNN topology and the per layer JR approach to 
&lt;/p&gt;</description></item><item><title>GraphVF&#26159;&#19968;&#31181;&#26032;&#30340;&#20998;&#23376;&#29983;&#25104;&#26694;&#26550;&#65292;&#23427;&#23558;2D&#25299;&#25169;&#21644;3D&#20960;&#20309;&#32467;&#21512;&#36215;&#26469;&#65292;&#29992;&#20110;&#21487;&#25511;&#29983;&#25104;&#20855;&#26377;&#32467;&#21512;&#33021;&#21147;&#30340;3D&#20998;&#23376;&#65292;&#26159;&#31532;&#19968;&#20010;&#21487;&#20197;&#29983;&#25104;&#32467;&#21512;&#33021;&#21147;&#30340;3D&#20998;&#23376;&#24182;&#23450;&#21046;&#20854;&#29289;&#21270;&#29305;&#24615;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.12825</link><description>&lt;p&gt;
GraphVF: &#29992;&#21464;&#20998;&#27969;&#25511;&#21046;&#29983;&#25104;&#29305;&#23450;&#34507;&#30333;&#36136;&#30340;&#19977;&#32500;&#20998;&#23376;
&lt;/p&gt;
&lt;p&gt;
GraphVF: Controllable Protein-Specific 3D Molecule Generation with Variational Flow. (arXiv:2304.12825v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12825
&lt;/p&gt;
&lt;p&gt;
GraphVF&#26159;&#19968;&#31181;&#26032;&#30340;&#20998;&#23376;&#29983;&#25104;&#26694;&#26550;&#65292;&#23427;&#23558;2D&#25299;&#25169;&#21644;3D&#20960;&#20309;&#32467;&#21512;&#36215;&#26469;&#65292;&#29992;&#20110;&#21487;&#25511;&#29983;&#25104;&#20855;&#26377;&#32467;&#21512;&#33021;&#21147;&#30340;3D&#20998;&#23376;&#65292;&#26159;&#31532;&#19968;&#20010;&#21487;&#20197;&#29983;&#25104;&#32467;&#21512;&#33021;&#21147;&#30340;3D&#20998;&#23376;&#24182;&#23450;&#21046;&#20854;&#29289;&#21270;&#29305;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#19982;&#29305;&#23450;&#30446;&#26631;&#34507;&#30333;&#36136;&#32467;&#21512;&#30340;&#20998;&#23376;&#26159;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#22522;&#26412;&#20219;&#21153;&#12290;&#26368;&#36817;&#30340;&#27169;&#22411;&#21033;&#29992;&#20960;&#20309;&#32422;&#26463;&#26465;&#20214;&#29983;&#25104;&#33021;&#22815;&#19982;&#29305;&#23450;&#34507;&#30333;&#36136;&#21475;&#34955;&#32039;&#23494;&#32467;&#21512;&#30340;&#37197;&#20307;&#20998;&#23376;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#19981;&#33021;&#26377;&#25928;&#22320;&#29983;&#25104;&#20855;&#26377;2D&#39592;&#26550;&#36864;&#21270;&#21644;&#29289;&#24615;&#38480;&#21046;&#30340;3D&#20998;&#23376;&#65292;&#36825;&#23545;&#20110;&#33647;&#29289;&#30340;&#25928;&#33021;&#21644;&#24320;&#21457;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GraphVF&#65292;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#27969;&#30340;&#26694;&#26550;&#65292;&#23558;2D&#25299;&#25169;&#21644;3D&#20960;&#20309;&#32467;&#21512;&#36215;&#26469;&#65292;&#29992;&#20110;&#21487;&#25511;&#29983;&#25104;&#20855;&#26377;&#32467;&#21512;&#33021;&#21147;&#30340;3D&#20998;&#23376;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#21512;&#20146;&#21644;&#21147;&#21644;&#29305;&#23450;&#34507;&#30333;&#36136;&#29983;&#25104;&#30340;&#23454;&#38469;&#27425;&#32423;&#32467;&#26500;&#24067;&#23616;&#12290;&#23588;&#20854;&#26159;&#65292;GraphVF&#20195;&#34920;&#20102;&#31532;&#19968;&#20010;&#21487;&#25511;&#30340;&#20960;&#20309;&#24847;&#35782;&#65292;&#29305;&#23450;&#34507;&#30333;&#36136;&#20998;&#23376;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#23450;&#21046;&#27425;&#32423;&#32467;&#26500;&#21644;&#29289;&#29702;&#21270;&#23398;&#24615;&#36136;&#30340;&#32467;&#21512;3D&#20998;&#23376;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;https://github.com/Franco-Solis/GraphVF-code&#20013;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Designing molecules that bind to specific target proteins is a fundamental task in drug discovery. Recent models leverage geometric constraints to generate ligand molecules that bind cohesively with specific protein pockets. However, these models cannot effectively generate 3D molecules with 2D skeletal curtailments and property constraints, which are pivotal to drug potency and development. To tackle this challenge, we propose GraphVF, a variational flow-based framework that combines 2D topology and 3D geometry, for controllable generation of binding 3D molecules. Empirically, our method achieves state-of-the-art binding affinity and realistic sub-structural layouts for protein-specific generation. In particular, GraphVF represents the first controllable geometry-aware, protein-specific molecule generation method, which can generate binding 3D molecules with tailored sub-structures and physio-chemical properties. Our code is available at https://github.com/Franco-Solis/GraphVF-code.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#33021;&#37327;&#39044;&#27979;&#30340;&#30830;&#20999;&#33021;&#37327;&#24341;&#23548;&#25193;&#25955;&#37319;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#21487;&#20197;&#35299;&#20915;&#20013;&#38388;&#24341;&#23548;&#20272;&#35745;&#30340;&#38590;&#39064;&#65292;&#24182;&#22312;D4RL&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.12824</link><description>&lt;p&gt;
&#22522;&#20110;&#23545;&#27604;&#33021;&#37327;&#39044;&#27979;&#30340;&#30830;&#20999;&#33021;&#37327;&#24341;&#23548;&#25193;&#25955;&#37319;&#26679;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Contrastive Energy Prediction for Exact Energy-Guided Diffusion Sampling in Offline Reinforcement Learning. (arXiv:2304.12824v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12824
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#33021;&#37327;&#39044;&#27979;&#30340;&#30830;&#20999;&#33021;&#37327;&#24341;&#23548;&#25193;&#25955;&#37319;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#21487;&#20197;&#35299;&#20915;&#20013;&#38388;&#24341;&#23548;&#20272;&#35745;&#30340;&#38590;&#39064;&#65292;&#24182;&#22312;D4RL&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#23548;&#37319;&#26679;&#26159;&#23558;&#25193;&#25955;&#27169;&#22411;&#24212;&#29992;&#20110;&#29616;&#23454;&#20219;&#21153;&#30340;&#37325;&#35201;&#26041;&#27861;&#65292;&#20854;&#22312;&#37319;&#26679;&#36807;&#31243;&#20013;&#23884;&#20837;&#20154;&#31867;&#23450;&#20041;&#30340;&#25351;&#23548;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#33324;&#35774;&#32622;&#65292;&#20854;&#20013;&#25351;&#23548;&#26159;&#30001;&#19968;&#20010;&#65288;&#38750;&#26631;&#20934;&#21270;&#65289;&#33021;&#37327;&#20989;&#25968;&#23450;&#20041;&#30340;&#12290;&#36825;&#31181;&#35774;&#32622;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#65292;&#22312;&#25193;&#25955;&#37319;&#26679;&#36807;&#31243;&#20013;&#30340;&#20013;&#38388;&#25351;&#23548;&#26159;&#30001;&#37319;&#26679;&#20998;&#24067;&#21644;&#33021;&#37327;&#20989;&#25968;&#20849;&#21516;&#23450;&#20041;&#30340;&#65292;&#24456;&#38590;&#20272;&#35745;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31934;&#30830;&#30340;&#20013;&#38388;&#25351;&#23548;&#37197;&#26041;&#20197;&#21450;&#19968;&#31181;&#21517;&#20026;&#23545;&#27604;&#33021;&#37327;&#39044;&#27979;&#65288;CEP&#65289;&#30340;&#26032;&#22411;&#35757;&#32451;&#30446;&#26631;&#26469;&#23398;&#20064;&#31934;&#30830;&#30340;&#20013;&#38388;&#25351;&#23548;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26080;&#38480;&#21046;&#30340;&#27169;&#22411;&#33021;&#21147;&#21644;&#25968;&#25454;&#26679;&#26412;&#19979;&#20445;&#35777;&#25910;&#25947;&#21040;&#31934;&#30830;&#25351;&#23548;&#65292;&#32780;&#20043;&#21069;&#30340;&#26041;&#27861;&#26080;&#27861;&#20570;&#21040;&#12290;&#25105;&#20204;&#23558;&#20854;&#24212;&#29992;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#24182;&#22312;D4RL&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Guided sampling is a vital approach for applying diffusion models in real-world tasks that embeds human-defined guidance during the sampling procedure. This paper considers a general setting where the guidance is defined by an (unnormalized) energy function. The main challenge for this setting is that the intermediate guidance during the diffusion sampling procedure, which is jointly defined by the sampling distribution and the energy function, is unknown and is hard to estimate. To address this challenge, we propose an exact formulation of the intermediate guidance as well as a novel training objective named contrastive energy prediction (CEP) to learn the exact guidance. Our method is guaranteed to converge to the exact guidance under unlimited model capacity and data samples, while previous methods can not. We demonstrate the effectiveness of our method by applying it to offline reinforcement learning (RL). Extensive experiments on D4RL benchmarks demonstrate that our method outperf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#21457;&#25496;&#20449;&#24687;&#29109;&#33258;&#28982;&#23545;&#20598;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37327;troenpy&#65292;&#24182;&#24212;&#29992;&#20110;&#25552;&#20986;&#20102;&#22522;&#20110;troenpy&#30340;&#25991;&#26723;&#21152;&#26435;&#26041;&#26696;&#65292;&#21363;&#27491;&#31867;&#21035;&#39057;&#29575;&#65288;PCF&#65289;&#65292;&#20197;&#21450;&#19968;&#31181;&#26032;&#30340;&#31867;&#21035;&#20449;&#24687;&#20559;&#24046;&#29305;&#24449;ECIB&#65292;&#22312;&#30417;&#30563;&#23398;&#20064;&#20013;&#20855;&#26377;&#20114;&#20449;&#24687;&#30340;&#27867;&#21270;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2304.12814</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#22411;&#30340;Shannon&#20449;&#24687;&#21450;&#21152;&#26435;&#26041;&#26696;&#30340;&#23545;&#20598;
&lt;/p&gt;
&lt;p&gt;
A Novel Dual of Shannon Information and Weighting Scheme. (arXiv:2304.12814v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12814
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#21457;&#25496;&#20449;&#24687;&#29109;&#33258;&#28982;&#23545;&#20598;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37327;troenpy&#65292;&#24182;&#24212;&#29992;&#20110;&#25552;&#20986;&#20102;&#22522;&#20110;troenpy&#30340;&#25991;&#26723;&#21152;&#26435;&#26041;&#26696;&#65292;&#21363;&#27491;&#31867;&#21035;&#39057;&#29575;&#65288;PCF&#65289;&#65292;&#20197;&#21450;&#19968;&#31181;&#26032;&#30340;&#31867;&#21035;&#20449;&#24687;&#20559;&#24046;&#29305;&#24449;ECIB&#65292;&#22312;&#30417;&#30563;&#23398;&#20064;&#20013;&#20855;&#26377;&#20114;&#20449;&#24687;&#30340;&#27867;&#21270;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Shannon&#20449;&#24687;&#29702;&#35770;&#19981;&#20165;&#22312;&#36890;&#20449;&#25216;&#26415;&#39046;&#22495;&#65292;&#20854;&#24212;&#29992;&#36824;&#25299;&#23637;&#33267;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#12290;&#26412;&#25991;&#21457;&#25496;&#20449;&#24687;&#29109;&#23384;&#22312;&#33258;&#28982;&#23545;&#20598;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#37327;troenpy&#65292;&#29992;&#20110;&#34913;&#37327;&#24213;&#23618;&#20998;&#24067;&#30340;&#30830;&#23450;&#24615;&#12289;&#26222;&#36941;&#24615;&#21644;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;troenpy&#30340;&#25991;&#26723;&#21152;&#26435;&#26041;&#26696;&#65292;&#21363;&#27491;&#31867;&#21035;&#39057;&#29575;&#65288;PCF&#65289;&#65292;&#24182;&#35777;&#26126;&#20854;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#20248;&#36234;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#31867;&#21035;&#20449;&#24687;&#20559;&#24046;&#29305;&#24449;ECIB&#65292;&#22312;&#30417;&#30563;&#23398;&#20064;&#20013;&#20855;&#26377;&#20114;&#20449;&#24687;&#30340;&#27867;&#21270;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
Shannon Information theory has achieved great success in not only communication technology where it was originally developed for but also many other science and engineering fields such as machine learning and artificial intelligence. Inspired by the famous weighting scheme TF-IDF, we discovered that information entropy has a natural dual. We complement the classical Shannon information theory by proposing a novel quantity, namely troenpy. Troenpy measures the certainty, commonness and similarity of the underlying distribution. To demonstrate its usefulness, we propose a troenpy based weighting scheme for document with class labels, namely positive class frequency (PCF). On a collection of public datasets we show the PCF based weighting scheme outperforms the classical TF-IDF and a popular Optimal Transportation based word moving distance algorithm in a kNN setting. We further developed a new odds-ratio type feature, namely Expected Class Information Bias(ECIB), which can be regarded as
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#23384;&#22312;&#24615;&#21035;&#21270;&#35821;&#35328;&#26102;&#65292;NLP&#35821;&#22659;&#20013;&#20063;&#23384;&#22312;&#30528;&#24615;&#21035;&#20559;&#35265;&#65292;&#23588;&#20854;&#26159;&#30007;&#24615;&#20559;&#35265;&#12290;&#35843;&#26597;&#32773;&#25552;&#20379;&#20102;&#19968;&#20010;&#28085;&#30422;&#20102;&#24615;&#21035;&#21270;&#35821;&#35328;&#19982;&#35821;&#35328;&#20043;&#38388;&#27495;&#20041;&#20851;&#31995;&#30340;&#26032;&#23383;&#20856;&#8220;Ava&#8221;&#12290;</title><link>http://arxiv.org/abs/2304.12810</link><description>&lt;p&gt;
&#36229;&#36234;&#8220;&#30007;&#24615;&#20934;&#21017;&#8221;&#65306;NLP&#35821;&#22659;&#20013;&#30340;&#38544;&#24615;&#30007;&#24615;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Transcending the "Male Code": Implicit Masculine Biases in NLP Contexts. (arXiv:2304.12810v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12810
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#23384;&#22312;&#24615;&#21035;&#21270;&#35821;&#35328;&#26102;&#65292;NLP&#35821;&#22659;&#20013;&#20063;&#23384;&#22312;&#30528;&#24615;&#21035;&#20559;&#35265;&#65292;&#23588;&#20854;&#26159;&#30007;&#24615;&#20559;&#35265;&#12290;&#35843;&#26597;&#32773;&#25552;&#20379;&#20102;&#19968;&#20010;&#28085;&#30422;&#20102;&#24615;&#21035;&#21270;&#35821;&#35328;&#19982;&#35821;&#35328;&#20043;&#38388;&#27495;&#20041;&#20851;&#31995;&#30340;&#26032;&#23383;&#20856;&#8220;Ava&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#20110;&#34394;&#25311;&#21161;&#25163;&#65288;VAs&#65289;&#30340;&#24615;&#21035;&#20559;&#24046;&#38382;&#39064;&#65292;&#25209;&#21028;&#24615;&#23398;&#35828;&#24050;&#32463;&#25552;&#39640;&#20102;&#20154;&#20204;&#30340;&#27880;&#24847;&#12290;&#22823;&#37096;&#20998;&#30740;&#31350;&#38598;&#20013;&#22312;&#35821;&#35328;&#20013;&#30340;&#26174;&#24615;&#20559;&#35265;&#65292;&#23588;&#20854;&#26159;&#38024;&#23545;&#22899;&#24615;&#12289;&#22899;&#23401;&#12289;&#22899;&#24615;&#35748;&#21516;&#20154;&#32676;&#21644;&#24615;&#21035;&#37239;&#20799;&#30340;&#27495;&#35270;&#65292;&#20197;&#21450;&#36890;&#36807;&#35789;&#21521;&#37327;&#23884;&#20837;&#30340;&#38544;&#24615;&#20851;&#32852;&#65307;&#32780;&#23545;&#20110;&#30007;&#24615;&#21644;&#27602;&#24615;&#30007;&#24615;&#65292;&#24615;&#21035;&#21644;&#24615;&#21035;&#20108;&#20803;&#20998;&#31867;&#30340;&#28151;&#20026;&#19968;&#35848;&#65292;&#24456;&#23569;&#26377;&#22522;&#20110;&#30007;&#24615;&#21644;&#30007;&#24615;&#27668;&#27010;&#30340;&#26377;&#38480;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#36824;&#24517;&#39035;&#36136;&#35810;&#22914;&#20309;&#23558;&#30007;&#24615;&#27668;&#27010;&#8220;&#32534;&#30721;&#8221;&#21040;&#35821;&#35328;&#20013;&#21450;&#20854;&#23558;&#8220;&#30007;&#24615;&#8221;&#20316;&#20026;&#35821;&#35328;&#40664;&#35748;&#20540;&#30340;&#20551;&#35774;&#65306;&#38544;&#24615;&#30007;&#24615;&#20559;&#35265;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#20004;&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#21457;&#29616;&#24403;&#23384;&#22312;&#24615;&#21035;&#21270;&#35821;&#35328;&#26102;&#65292;&#24615;&#21035;&#20559;&#35265;&#23588;&#20854;&#26159;&#30007;&#24615;&#20559;&#35265;&#20063;&#23384;&#22312;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#20559;&#35265;&#19982;NLP&#19978;&#19979;&#25991;&#30340;&#20851;&#31995;&#32454;&#24494;&#19988;&#30456;&#20851;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#21517;&#20026;AVA&#30340;&#26032;&#23383;&#20856;&#65292;&#28085;&#30422;&#20102;&#24615;&#21035;&#21270;&#35821;&#35328;&#19982;&#35821;&#35328;&#20043;&#38388;&#30340;&#27495;&#20041;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Critical scholarship has elevated the problem of gender bias in data sets used to train virtual assistants (VAs). Most work has focused on explicit biases in language, especially against women, girls, femme-identifying people, and genderqueer folk; implicit associations through word embeddings; and limited models of gender and masculinities, especially toxic masculinities, conflation of sex and gender, and a sex/gender binary framing of the masculine as diametric to the feminine. Yet, we must also interrogate how masculinities are "coded" into language and the assumption of "male" as the linguistic default: implicit masculine biases. To this end, we examined two natural language processing (NLP) data sets. We found that when gendered language was present, so were gender biases and especially masculine biases. Moreover, these biases related in nuanced ways to the NLP context. We offer a new dictionary called AVA that covers ambiguous associations between gendered language and the langua
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22870;&#21169;&#21152;&#26435;&#21644;&#25439;&#22833;&#21152;&#26435;&#26799;&#24230;&#21512;&#24182;&#65292;&#20197;&#26356;&#22909;&#22320;&#25552;&#39640;&#20998;&#24067;&#24335;&#20195;&#29702;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.12778</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25439;&#22833;&#21644;&#22870;&#21169;&#21152;&#26435;
&lt;/p&gt;
&lt;p&gt;
Loss and Reward Weighing for increased learning in Distributed Reinforcement Learning. (arXiv:2304.12778v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12778
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22870;&#21169;&#21152;&#26435;&#21644;&#25439;&#22833;&#21152;&#26435;&#26799;&#24230;&#21512;&#24182;&#65292;&#20197;&#26356;&#22909;&#22320;&#25552;&#39640;&#20998;&#24067;&#24335;&#20195;&#29702;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#29615;&#22659;&#20013;&#20998;&#24067;&#24335;&#20195;&#29702;&#30340;&#23398;&#20064;&#26041;&#26696;&#65292;&#21363;&#22870;&#21169;&#21152;&#26435;&#65288;R-Weighted&#65289;&#21644;&#25439;&#22833;&#21152;&#26435;&#65288;L-Weighted&#65289;&#26799;&#24230;&#21512;&#24182;&#12290; R / L &#21152;&#26435;&#26041;&#27861;&#26367;&#25442;&#20102;&#35757;&#32451;&#22810;&#20010;&#20195;&#29702;&#30340;&#26631;&#20934;&#23454;&#36341;&#65292;&#20363;&#22914;&#23545;&#26799;&#24230;&#27714;&#21644;&#25110;&#24179;&#22343;&#12290;&#27599;&#20010;&#20195;&#29702;&#22312;&#19981;&#21516;&#21021;&#22987;&#21270;&#29256;&#26412;&#30340;&#30456;&#21516;&#29615;&#22659;&#20013;&#36816;&#34892;&#65292;&#36825;&#20250;&#20174;&#19981;&#21516;&#30340;actor&#33719;&#24471;&#19981;&#21516;&#30340;&#26799;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces two learning schemes for distributed agents in Reinforcement Learning (RL) environments, namely Reward-Weighted (R-Weighted) and Loss-Weighted (L-Weighted) gradient merger. The R/L weighted methods replace standard practices for training multiple agents, such as summing or averaging the gradients. The core of our methods is to scale the gradient of each actor based on how high the reward (for R-Weighted) or the loss (for L-Weighted) is compared to the other actors. During training, each agent operates in differently initialized versions of the same environment, which gives different gradients from different actors. In essence, the R-Weights and L-Weights of each agent inform the other agents of its potential, which again reports which environment should be prioritized for learning. This approach of distributed learning is possible because environments that yield higher rewards, or low losses, have more critical information than environments that yield lower reward
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21453;Lipschitz&#32422;&#26463;&#30340;&#35299;&#30721;&#22120;&#32593;&#32476;&#65292;&#21487;&#20197;&#31616;&#21333;&#26126;&#20102;&#22320;&#25511;&#21046;&#24191;&#27867;&#30340;VAE&#27169;&#22411;&#30340;&#21518;&#39564;&#22349;&#22604;&#31243;&#24230;&#65292;&#24182;&#24102;&#26377;&#20855;&#20307;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2304.12770</link><description>&lt;p&gt;
&#22522;&#20110;&#21453;Lipschitz&#32422;&#26463;&#30340;&#35299;&#30721;&#22120;&#32593;&#32476;&#25511;&#21046;&#21518;&#39564;&#22349;&#22604;
&lt;/p&gt;
&lt;p&gt;
Controlling Posterior Collapse by an Inverse Lipschitz Constraint on the Decoder Network. (arXiv:2304.12770v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12770
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21453;Lipschitz&#32422;&#26463;&#30340;&#35299;&#30721;&#22120;&#32593;&#32476;&#65292;&#21487;&#20197;&#31616;&#21333;&#26126;&#20102;&#22320;&#25511;&#21046;&#24191;&#27867;&#30340;VAE&#27169;&#22411;&#30340;&#21518;&#39564;&#22349;&#22604;&#31243;&#24230;&#65292;&#24182;&#24102;&#26377;&#20855;&#20307;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#26159;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#20013;&#21462;&#24471;&#24040;&#22823;&#25104;&#21151;&#30340;&#19968;&#31181;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#23427;&#20204;&#23384;&#22312;&#19968;&#20010;&#31216;&#20026;&#21518;&#39564;&#22349;&#22604;&#30340;&#38382;&#39064;&#65292;&#24403;&#32534;&#30721;&#22120;&#19982;&#27809;&#26377;&#32771;&#34385;&#36755;&#20837;&#25968;&#25454;&#30340;&#28508;&#22312;&#32467;&#26500;&#30340;&#20808;&#39564;&#37325;&#21512;&#25110;&#22349;&#22604;&#26102;&#23601;&#20250;&#21457;&#29983;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#21453;Lipschitz&#31070;&#32463;&#32593;&#32476;&#30340;&#35299;&#30721;&#22120;&#65292;&#22522;&#20110;&#36825;&#20010;&#26550;&#26500;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#31616;&#21333;&#26126;&#20102;&#22320;&#25511;&#21046;&#24191;&#27867;&#30340;VAE&#27169;&#22411;&#30340;&#21518;&#39564;&#22349;&#22604;&#31243;&#24230;&#65292;&#24182;&#24102;&#26377;&#20855;&#20307;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#20960;&#20010;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational autoencoders (VAEs) are one of the deep generative models that have experienced enormous success over the past decades. However, in practice, they suffer from a problem called posterior collapse, which occurs when the encoder coincides, or collapses, with the prior taking no information from the latent structure of the input data into consideration. In this work, we introduce an inverse Lipschitz neural network into the decoder and, based on this architecture, provide a new method that can control in a simple and clear manner the degree of posterior collapse for a wide range of VAE models equipped with a concrete theoretical guarantee. We also illustrate the effectiveness of our method through several numerical experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21516;&#26102;&#20108;&#20803;&#37327;&#21270;&#22238;&#24402;&#65288;SBQR&#65289;&#20013;&#20998;&#20301;&#25968;&#19982;&#39044;&#27979;&#27010;&#29575;&#30340;&#20108;&#20803;&#23545;&#20598;&#24615;&#65292;&#20351;&#20998;&#20301;&#25968;&#34920;&#36798;&#24335;&#22312;&#19981;&#21516;tau&#20540;&#19979;&#30340;&#26500;&#36896;&#19981;&#20877;&#20381;&#36182;&#20110;&#25439;&#22833;&#20989;&#25968;&#65292;&#20174;&#32780;&#22312;&#22788;&#29702;&#26816;&#27979;&#36229;&#20986;&#20998;&#24067;&#26679;&#26412;&#21644;&#35843;&#25972;&#27169;&#22411;&#26041;&#38754;&#34920;&#29616;&#31361;&#20986;&#12290;</title><link>http://arxiv.org/abs/2304.12766</link><description>&lt;p&gt;
&#20174;&#25439;&#22833;&#20989;&#25968;&#20013;&#35299;&#32806;&#20998;&#20301;&#25968;&#34920;&#36798;&#24335;
&lt;/p&gt;
&lt;p&gt;
Decoupling Quantile Representations from Loss Functions. (arXiv:2304.12766v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21516;&#26102;&#20108;&#20803;&#37327;&#21270;&#22238;&#24402;&#65288;SBQR&#65289;&#20013;&#20998;&#20301;&#25968;&#19982;&#39044;&#27979;&#27010;&#29575;&#30340;&#20108;&#20803;&#23545;&#20598;&#24615;&#65292;&#20351;&#20998;&#20301;&#25968;&#34920;&#36798;&#24335;&#22312;&#19981;&#21516;tau&#20540;&#19979;&#30340;&#26500;&#36896;&#19981;&#20877;&#20381;&#36182;&#20110;&#25439;&#22833;&#20989;&#25968;&#65292;&#20174;&#32780;&#22312;&#22788;&#29702;&#26816;&#27979;&#36229;&#20986;&#20998;&#24067;&#26679;&#26412;&#21644;&#35843;&#25972;&#27169;&#22411;&#26041;&#38754;&#34920;&#29616;&#31361;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21516;&#26102;&#37327;&#21270;&#22238;&#24402;&#65288;SQR&#65289;&#25216;&#26415;&#29992;&#20110;&#20272;&#35745;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20294;&#20854;&#24212;&#29992;&#21463;&#38480;&#20110;&#35201;&#27714;&#20013;&#20301;&#25968;&#20998;&#20301;&#25968;&#65288;&#964; = 0.5&#65289;&#22788;&#30340;&#35299;&#20915;&#26041;&#26696;&#24517;&#39035;&#26368;&#23567;&#21270;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65288;MAE&#65289;&#12290;&#26412;&#25991;&#36890;&#36807;&#23637;&#31034;&#21516;&#26102;&#20108;&#20803;&#37327;&#21270;&#22238;&#24402;&#65288;SBQR&#65289;&#20013;&#20998;&#20301;&#25968;&#19982;&#39044;&#27979;&#27010;&#29575;&#30340;&#20108;&#20803;&#23545;&#20598;&#24615;&#26469;&#35299;&#20915;&#27492;&#38382;&#39064;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#20174;&#25439;&#22833;&#20989;&#25968;&#20013;&#35299;&#32806;&#20998;&#20301;&#25968;&#34920;&#36798;&#24335;&#30340;&#26500;&#36896;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#20013;&#20301;&#20998;&#20301;&#25968;&#22788;&#20998;&#37197;&#20219;&#24847;&#20998;&#31867;&#22120;f(x)&#65292;&#24182;&#29983;&#25104;&#19981;&#21516;&#964;&#20540;&#30340;&#23436;&#25972;SBQR&#20998;&#20301;&#25968;&#34920;&#31034;&#30340;&#20840;&#35889;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#24212;&#29992;&#31243;&#24207;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65306;&#65288;i&#65289;&#26816;&#27979;&#36229;&#20986;&#20998;&#24067;&#26679;&#26412;&#65292;&#20854;&#20013;&#25105;&#20204;&#26174;&#31034;&#20998;&#20301;&#25968;&#34920;&#31034;&#20248;&#20110;&#26631;&#20934;&#27010;&#29575;&#36755;&#20986;&#65307;&#65288;ii&#65289;&#35843;&#25972;&#27169;&#22411;&#65292;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20998;&#20301;&#25968;&#34920;&#31034;&#23545;&#22833;&#30495;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The simultaneous quantile regression (SQR) technique has been used to estimate uncertainties for deep learning models, but its application is limited by the requirement that the solution at the median quantile ({\tau} = 0.5) must minimize the mean absolute error (MAE). In this article, we address this limitation by demonstrating a duality between quantiles and estimated probabilities in the case of simultaneous binary quantile regression (SBQR). This allows us to decouple the construction of quantile representations from the loss function, enabling us to assign an arbitrary classifier f(x) at the median quantile and generate the full spectrum of SBQR quantile representations at different {\tau} values. We validate our approach through two applications: (i) detecting out-of-distribution samples, where we show that quantile representations outperform standard probability outputs, and (ii) calibrating models, where we demonstrate the robustness of quantile representations to distortions. 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Grad-Align+&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#24378;&#33410;&#28857;&#29305;&#24449;&#26469;&#25191;&#34892;NA&#20219;&#21153;&#65292;&#24182;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;&#22686;&#24378;&#30340;&#33410;&#28857;&#29305;&#24449;&#26469;&#35774;&#35745;NA&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;NA&#26041;&#27861;&#32570;&#20047;&#39069;&#22806;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.12751</link><description>&lt;p&gt;
&#33410;&#28857;&#29305;&#24449;&#22686;&#24378;&#25913;&#36827;&#32593;&#32476;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Node Feature Augmentation Vitaminizes Network Alignment. (arXiv:2304.12751v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Grad-Align+&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#24378;&#33410;&#28857;&#29305;&#24449;&#26469;&#25191;&#34892;NA&#20219;&#21153;&#65292;&#24182;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;&#22686;&#24378;&#30340;&#33410;&#28857;&#29305;&#24449;&#26469;&#35774;&#35745;NA&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;NA&#26041;&#27861;&#32570;&#20047;&#39069;&#22806;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#23545;&#40784;&#65288;NA&#65289;&#26159;&#36890;&#36807;&#32473;&#23450;&#32593;&#32476;&#30340;&#25299;&#25169;&#21644;/&#25110;&#29305;&#24449;&#20449;&#24687;&#26469;&#21457;&#29616;&#22810;&#20010;&#32593;&#32476;&#20043;&#38388;&#30340;&#33410;&#28857;&#23545;&#24212;&#20851;&#31995;&#30340;&#20219;&#21153;&#12290;&#34429;&#28982;NA&#26041;&#27861;&#22312;&#21508;&#31181;&#22330;&#26223;&#19979;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#20854;&#26377;&#25928;&#24615;&#24182;&#19981;&#24635;&#26159;&#26377;&#39069;&#22806;&#20449;&#24687;&#65292;&#22914;&#20808;&#21069;&#30340;&#38170;&#28857;&#38142;&#25509;&#21644;/&#25110;&#33410;&#28857;&#29305;&#24449;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#23454;&#38469;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Grad-Align+&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;NA&#26041;&#27861;&#65292;&#24314;&#31435;&#22312;&#26368;&#36817;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;NA&#26041;&#27861;Grad-Align&#20043;&#19978;&#65292;Grad-Align+&#20165;&#36880;&#27493;&#21457;&#29616;&#37096;&#20998;&#33410;&#28857;&#23545;&#65292;&#30452;&#21040;&#25214;&#21040;&#25152;&#26377;&#33410;&#28857;&#23545;&#12290;&#22312;&#35774;&#35745;Grad-Align+&#26102;&#65292;&#25105;&#20204;&#32771;&#34385;&#22914;&#20309;&#36890;&#36807;&#22686;&#24378;&#33410;&#28857;&#29305;&#24449;&#26469;&#25191;&#34892;NA&#20219;&#21153;&#65292;&#24182;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;&#22686;&#24378;&#30340;&#33410;&#28857;&#29305;&#24449;&#26469;&#35774;&#35745;NA&#26041;&#27861;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#30001;&#19977;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#32452;&#25104;&#30340;Grad-Align+&#65306;&#22522;&#20110;&#20013;&#24515;&#24615;&#30340;&#33410;&#28857;&#29305;&#24449;&#22686;&#24378;&#65288;CNFA&#65289;&#12289;&#22270;&#20999;&#29255;&#29983;&#25104;&#21644;&#20248;&#21270;&#33410;&#28857;&#23884;&#20837;&#29305;&#24449;&#65288;ONIFE&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Network alignment (NA) is the task of discovering node correspondences across multiple networks using topological and/or feature information of given networks. Although NA methods have achieved remarkable success in a myriad of scenarios, their effectiveness is not without additional information such as prior anchor links and/or node features, which may not always be available due to privacy concerns or access restrictions. To tackle this practical challenge, we propose Grad-Align+, a novel NA method built upon a recent state-of-the-art NA method, the so-called Grad-Align, that gradually discovers only a part of node pairs until all node pairs are found. In designing Grad-Align+, we account for how to augment node features in the sense of performing the NA task and how to design our NA method by maximally exploiting the augmented node features. To achieve this goal, we develop Grad-Align+ consisting of three key components: 1) centrality-based node feature augmentation (CNFA), 2) graph
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25193;&#23637;&#20102;&#19968;&#20010;&#22522;&#20110;Ottertune&#33258;&#21160;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#37325;&#29992;&#20174;&#20808;&#21069;&#20250;&#35758;&#20013;&#25910;&#38598;&#30340;&#35757;&#32451;&#25968;&#25454;&#26469;&#35843;&#25972;&#26032;&#30340;DBMS&#37096;&#32626;&#65292;&#20197;&#25552;&#39640;&#24310;&#36831;&#39044;&#27979;&#30340;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.12747</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25968;&#25454;&#24211;&#31649;&#29702;&#31995;&#32479;&#33258;&#21160;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
Deep learning based Auto Tuning for Database Management System. (arXiv:2304.12747v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12747
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25193;&#23637;&#20102;&#19968;&#20010;&#22522;&#20110;Ottertune&#33258;&#21160;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#37325;&#29992;&#20174;&#20808;&#21069;&#20250;&#35758;&#20013;&#25910;&#38598;&#30340;&#35757;&#32451;&#25968;&#25454;&#26469;&#35843;&#25972;&#26032;&#30340;DBMS&#37096;&#32626;&#65292;&#20197;&#25552;&#39640;&#24310;&#36831;&#39044;&#27979;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#24211;&#31995;&#32479;&#37197;&#32622;&#30340;&#31649;&#29702;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#26377;&#25968;&#30334;&#31181;&#37197;&#32622;&#24320;&#20851;&#26469;&#25511;&#21046;&#31995;&#32479;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;&#36825;&#31181;&#24773;&#20917;&#22797;&#26434;&#24615;&#22686;&#21152;&#30340;&#21407;&#22240;&#22312;&#20110;&#36825;&#20123;&#24320;&#20851;&#27809;&#26377;&#26631;&#20934;&#21270;&#12289;&#29420;&#31435;&#24615;&#20063;&#19981;&#19968;&#26679;&#65292;&#24182;&#27809;&#26377;&#36890;&#29992;&#30340;&#26631;&#20934;&#65292;&#22240;&#27492;&#24456;&#38590;&#30830;&#23450;&#26368;&#20339;&#30340;&#35774;&#32622;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;OtterTune&#26159;&#19968;&#20010;&#22522;&#20110;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#36873;&#25321;&#26377;&#24433;&#21709;&#24615;&#30340;&#24320;&#20851;&#65292;&#26144;&#23556;&#26410;&#35265;&#36807;&#30340;&#24037;&#20316;&#37327;&#24182;&#24314;&#35758;&#24320;&#20851;&#35774;&#32622;&#30340;&#26032;&#24037;&#20855;&#65292;&#24182;&#24050;&#22312;&#19977;&#20010;DBMS&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#25512;&#33616;&#30340;&#37197;&#32622;&#19982;&#29616;&#26377;&#24037;&#20855;&#25110;&#20154;&#24037;&#19987;&#23478;&#29983;&#25104;&#30340;&#37197;&#32622;&#19968;&#26679;&#22909;&#25110;&#26356;&#22909;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#19968;&#20010;&#22522;&#20110;Ottertune&#30340;&#33258;&#21160;&#25216;&#26415;&#65292;&#21033;&#29992;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#37325;&#29992;&#20174;&#20808;&#21069;&#20250;&#35758;&#20013;&#25910;&#38598;&#30340;&#35757;&#32451;&#25968;&#25454;&#26469;&#35843;&#25972;&#26032;&#30340;DBMS&#37096;&#32626;&#65292;&#20197;&#25552;&#39640;&#24310;&#36831;&#39044;&#27979;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The management of database system configurations is a challenging task, as there are hundreds of configuration knobs that control every aspect of the system. This is complicated by the fact that these knobs are not standardized, independent, or universal, making it difficult to determine optimal settings. An automated approach to address this problem using supervised and unsupervised machine learning methods to select impactful knobs, map unseen workloads, and recommend knob settings was implemented in a new tool called OtterTune and is being evaluated on three DBMSs, with results demonstrating that it recommends configurations as good as or better than those generated by existing tools or a human expert.In this work, we extend an automated technique based on Ottertune [1] to reuse training data gathered from previous sessions to tune new DBMS deployments with the help of supervised and unsupervised machine learning methods to improve latency prediction. Our approach involves the expan
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22812;&#38388;&#20010;&#20154;&#26723;&#26696;&#34920;&#31034;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#21069;&#39044;&#27979;&#21019;&#20260;&#24739;&#32773;&#30340;&#33043;&#27602;&#30151;&#21457;&#20316;&#65292;&#36825;&#31181;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.12737</link><description>&lt;p&gt;
ICU&#21019;&#20260;&#24739;&#32773;&#26089;&#26399;&#33043;&#27602;&#30151;&#21457;&#20316;&#39044;&#27979;&#30340;&#22812;&#38388;&#20010;&#20154;&#26723;&#26696;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
NPRL: Nightly Profile Representation Learning for Early Sepsis Onset Prediction in ICU Trauma Patients. (arXiv:2304.12737v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12737
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22812;&#38388;&#20010;&#20154;&#26723;&#26696;&#34920;&#31034;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#21069;&#39044;&#27979;&#21019;&#20260;&#24739;&#32773;&#30340;&#33043;&#27602;&#30151;&#21457;&#20316;&#65292;&#36825;&#31181;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33043;&#27602;&#30151;&#26159;&#19968;&#31181;&#28304;&#20110;&#24863;&#26579;&#65292;&#20197;&#20005;&#37325;&#22120;&#23448;&#21151;&#33021;&#38556;&#30861;&#20026;&#29305;&#24449;&#30340;&#32508;&#21512;&#30151;&#65292;&#24182;&#19988;&#26159;&#20840;&#29699;&#37325;&#30151;&#30417;&#25252;&#30149;&#25151;(ICU)&#27515;&#20129;&#29575;&#30340;&#20027;&#35201;&#21407;&#22240;&#20043;&#19968;&#12290;&#36890;&#36807;&#26089;&#26399;&#24212;&#29992;&#25239;&#29983;&#32032;&#21487;&#20197;&#20943;&#23569;&#36825;&#20123;&#24182;&#21457;&#30151;&#65292;&#22240;&#27492;&#39044;&#27979;&#33043;&#27602;&#30151;&#30340;&#21457;&#20316;&#26102;&#38388;&#23545;&#24739;&#32773;&#30340;&#29983;&#23384;&#21644;&#31119;&#31049;&#33267;&#20851;&#37325;&#35201;&#12290;&#24403;&#21069;&#22312;&#21307;&#30103;&#22522;&#30784;&#35774;&#26045;&#20869;&#37096;&#37096;&#32626;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#34920;&#29616;&#19981;&#20339;&#65292;&#19981;&#36275;&#20197;&#26089;&#26399;&#39044;&#27979;&#33043;&#27602;&#30151;&#30340;&#21457;&#29983;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24739;&#32773;&#29983;&#29702;&#21644;&#20020;&#24202;&#25968;&#25454;&#30340;&#22812;&#38388;&#20010;&#20154;&#26723;&#26696;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;(NPRL)&#65292;&#20197;&#25429;&#25417;&#24739;&#32773;&#29366;&#24577;&#38543;&#26102;&#38388;&#21160;&#24577;&#25913;&#21464;&#30340;&#24773;&#20917;&#12290;&#28982;&#21518;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#39044;&#27979;&#36825;&#20123;&#24739;&#32773;&#30340;&#33043;&#27602;&#30151;&#21457;&#20316;&#26102;&#38388;&#65292;&#24182;&#36229;&#36234;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sepsis is a syndrome that develops in response to the presence of infection. It is characterized by severe organ dysfunction and is one of the leading causes of mortality in Intensive Care Units (ICUs) worldwide. These complications can be reduced through early application of antibiotics, hence the ability to anticipate the onset of sepsis early is crucial to the survival and well-being of patients. Current machine learning algorithms deployed inside medical infrastructures have demonstrated poor performance and are insufficient for anticipating sepsis onset early. In recent years, deep learning methodologies have been proposed to predict sepsis, but some fail to capture the time of onset (e.g., classifying patients' entire visits as developing sepsis or not) and others are unrealistic to be deployed into medical facilities (e.g., creating training instances using a fixed time to onset where the time of onset needs to be known apriori). Therefore, in this paper, we first propose a nove
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#26799;&#24230;&#25552;&#21319;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21644;&#20027;&#25104;&#20998;&#20998;&#26512;&#30340;&#33258;&#21160;&#20998;&#31867;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#26143;&#31995;&#22806;&#23556;&#30005;&#28304;&#24418;&#24577;&#20998;&#31867;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20855;&#26377;&#34920;&#26684;&#25968;&#25454;&#30340;&#20998;&#31867;&#38382;&#39064;&#20013;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2304.12729</link><description>&lt;p&gt;
&#20351;&#29992;&#26799;&#24230;&#25552;&#21319;&#26041;&#27861;&#23545;&#26143;&#31995;&#22806;&#23556;&#30005;&#28304;&#36827;&#34892;&#24418;&#24577;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Morphological Classification of Extragalactic Radio Sources Using Gradient Boosting Methods. (arXiv:2304.12729v1 [astro-ph.IM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12729
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#26799;&#24230;&#25552;&#21319;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21644;&#20027;&#25104;&#20998;&#20998;&#26512;&#30340;&#33258;&#21160;&#20998;&#31867;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#26143;&#31995;&#22806;&#23556;&#30005;&#28304;&#24418;&#24577;&#20998;&#31867;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20855;&#26377;&#34920;&#26684;&#25968;&#25454;&#30340;&#20998;&#31867;&#38382;&#39064;&#20013;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26032;&#30340;&#23556;&#30005;&#26395;&#36828;&#38236;&#30340;&#24314;&#25104;&#65292;&#26080;&#32447;&#30005;&#22825;&#25991;&#23398;&#30340;&#25968;&#25454;&#37327;&#24613;&#21095;&#22686;&#21152;&#12290;&#24418;&#24577;&#20998;&#31867;&#33073;&#26143;&#31995;&#22806;&#23556;&#30005;&#28304;&#30340;&#33258;&#21160;&#20998;&#31867;&#26159;&#35813;&#39046;&#22495;&#20013;&#26368;&#20851;&#38190;&#30340;&#38382;&#39064;&#20043;&#19968;&#12290;&#22823;&#37096;&#20998;&#20851;&#20110;&#35813;&#39046;&#22495;&#30340;&#26368;&#36817;&#30340;&#25104;&#26524;&#37117;&#26159;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#31867;&#22120;&#12290;&#32780;&#26412;&#25991;&#21017;&#25552;&#20986;&#20102;&#26799;&#24230;&#25552;&#21319;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21644;&#20027;&#25104;&#20998;&#20998;&#26512;&#20316;&#20026;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#35745;&#31639;&#25104;&#26412;&#36739;&#20302;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#20855;&#26377;&#34920;&#26684;&#25968;&#25454;&#30340;&#20998;&#31867;&#38382;&#39064;&#20013;&#65292;&#26799;&#24230;&#25552;&#21319;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#20248;&#20110;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;XGBoost&#65292;LightGBM&#21644;CatBoost&#23454;&#29616;&#30340;&#26799;&#24230;&#25552;&#21319;&#26041;&#27861;&#23545;&#25968;&#25454;&#38598;&#22823;&#23567;&#30340;&#20998;&#31867;&#22120;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#35813;&#20998;&#31867;&#22120;&#23558;&#26143;&#31995;&#22806;&#23556;&#30005;&#28304;&#20998;&#20026;&#19977;&#20010;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of radio astronomy is witnessing a boom in the amount of data produced per day due to newly commissioned radio telescopes. One of the most crucial problems in this field is the automatic classification of extragalactic radio sources based on their morphologies. Most recent contributions in the field of morphological classification of extragalactic radio sources have proposed classifiers based on convolutional neural networks. Alternatively, this work proposes gradient boosting machine learning methods accompanied by principal component analysis as data-efficient alternatives to convolutional neural networks. Recent findings have shown the efficacy of gradient boosting methods in outperforming deep learning methods for classification problems with tabular data. The gradient boosting methods considered in this work are based on the XGBoost, LightGBM, and CatBoost implementations. This work also studies the effect of dataset size on classifier performance. A three-class classifi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30524;&#21160;&#36861;&#36394;&#20449;&#24687;&#30340;&#20154;&#26426;&#21327;&#21516;CAD&#31995;&#32479;&#65292;&#22312;&#22810;&#23454;&#20363;&#23398;&#20064;&#20013;&#24341;&#20837;&#21452;&#37325;&#20132;&#21449;&#20851;&#27880;MIL&#65288;DCAMIL&#65289;&#32593;&#32476;&#20197;&#25552;&#39640;&#35786;&#26029;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#20351;&#29992;&#24207;&#21015;&#22686;&#24378;&#21644;&#22495;&#23545;&#25239;&#27169;&#22359;&#22686;&#24378;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.12719</link><description>&lt;p&gt;
&#22522;&#20110;&#21452;&#37325;&#20132;&#21449;&#20851;&#27880;&#30340;&#30524;&#21160;&#36861;&#36394;&#24341;&#23548;&#28145;&#24230;&#22810;&#23454;&#20363;&#23398;&#20064;&#22312;&#30524;&#24213;&#30142;&#30149;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Eye tracking guided deep multiple instance learning with dual cross-attention for fundus disease detection. (arXiv:2304.12719v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30524;&#21160;&#36861;&#36394;&#20449;&#24687;&#30340;&#20154;&#26426;&#21327;&#21516;CAD&#31995;&#32479;&#65292;&#22312;&#22810;&#23454;&#20363;&#23398;&#20064;&#20013;&#24341;&#20837;&#21452;&#37325;&#20132;&#21449;&#20851;&#27880;MIL&#65288;DCAMIL&#65289;&#32593;&#32476;&#20197;&#25552;&#39640;&#35786;&#26029;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#20351;&#29992;&#24207;&#21015;&#22686;&#24378;&#21644;&#22495;&#23545;&#25239;&#27169;&#22359;&#22686;&#24378;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20419;&#36827;&#20102;&#35745;&#31639;&#26426;&#36741;&#21161;&#35786;&#26029;&#65288;CAD&#65289;&#31995;&#32479;&#30340;&#21457;&#23637;&#65292;&#26377;&#21161;&#20110;&#30524;&#31185;&#21307;&#29983;&#20943;&#23569;&#28431;&#35786;&#21644;&#35823;&#35786;&#29575;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30524;&#21160;&#36861;&#36394;&#20449;&#24687;&#30340;&#20154;&#26426;&#21327;&#21516;&#65288;HITL&#65289;CAD&#31995;&#32479;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;&#35813;&#31995;&#32479;&#24212;&#29992;&#20110;&#22810;&#23454;&#20363;&#23398;&#20064;&#65288;MIL&#65289;&#20013;&#65292;&#20854;&#20013;&#30524;&#21160;&#36861;&#36394;&#27880;&#35270;&#22270;&#26377;&#21161;&#20110;&#31579;&#36873;&#19982;&#35786;&#26029;&#30456;&#20851;&#30340;&#23454;&#20363;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21452;&#37325;&#20132;&#21449;&#20851;&#27880;MIL&#65288;DCAMIL&#65289;&#32593;&#32476;&#65292;&#20197;&#36943;&#21046;&#22122;&#22768;&#23454;&#20363;&#30340;&#19981;&#33391;&#24433;&#21709;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#24207;&#21015;&#22686;&#24378;&#27169;&#22359;&#21644;&#22495;&#23545;&#25239;&#27169;&#22359;&#65292;&#20998;&#21035;&#20016;&#23500;&#21644;&#26631;&#20934;&#21270;&#35757;&#32451;&#21253;&#20013;&#30340;&#23454;&#20363;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) have promoted the development of computer aided diagnosis (CAD) systems for fundus diseases, helping ophthalmologists reduce missed diagnosis and misdiagnosis rate. However, the majority of CAD systems are data-driven but lack of medical prior knowledge which can be performance-friendly. In this regard, we innovatively proposed a human-in-the-loop (HITL) CAD system by leveraging ophthalmologists' eye-tracking information, which is more efficient and accurate. Concretely, the HITL CAD system was implemented on the multiple instance learning (MIL), where eye-tracking gaze maps were beneficial to cherry-pick diagnosis-related instances. Furthermore, the dual-cross-attention MIL (DCAMIL) network was utilized to curb the adverse effects of noisy instances. Meanwhile, both sequence augmentation module and domain adversarial module were introduced to enrich and standardize instances in the training bag, respectively, thereby enhancing the robustness of our method. 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;LyaDEQ&#30340;&#40065;&#26834;DEQ&#27169;&#22411;&#65292;&#36890;&#36807;Lyapunov&#29702;&#35770;&#25552;&#20379;&#20102;&#20445;&#35777;&#30340;&#31283;&#23450;&#24615;&#20197;&#25269;&#25239;&#24494;&#23567;&#30340;&#21021;&#22987;&#25200;&#21160;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#22266;&#23450;&#28857;&#20043;&#38388;&#21152;&#20837;&#20840;&#36830;&#25509;&#23618;&#20197;&#36991;&#20813;&#19981;&#33391;&#23545;&#25239;&#24615;&#38450;&#24481;&#12290;</title><link>http://arxiv.org/abs/2304.12707</link><description>&lt;p&gt;
&#23398;&#20064;&#40065;&#26834;&#30340;&#28145;&#24230;&#24179;&#34913;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Robust Deep Equilibrium Models. (arXiv:2304.12707v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12707
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;LyaDEQ&#30340;&#40065;&#26834;DEQ&#27169;&#22411;&#65292;&#36890;&#36807;Lyapunov&#29702;&#35770;&#25552;&#20379;&#20102;&#20445;&#35777;&#30340;&#31283;&#23450;&#24615;&#20197;&#25269;&#25239;&#24494;&#23567;&#30340;&#21021;&#22987;&#25200;&#21160;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#22266;&#23450;&#28857;&#20043;&#38388;&#21152;&#20837;&#20840;&#36830;&#25509;&#23618;&#20197;&#36991;&#20813;&#19981;&#33391;&#23545;&#25239;&#24615;&#38450;&#24481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24179;&#34913;(DEQ)&#27169;&#22411;&#24050;&#25104;&#20026;&#28145;&#24230;&#23398;&#20064;&#20013;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#38544;&#24335;&#23618;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#35299;&#20915;&#21333;&#20010;&#38750;&#32447;&#24615;&#23618;&#30340;&#22266;&#23450;&#28857;&#26469;&#25918;&#24323;&#20102;&#20256;&#32479;&#28145;&#24230;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#24456;&#25104;&#21151;&#65292;&#20294;&#23545;&#20110;&#36825;&#20123;&#27169;&#22411;&#30340;&#22266;&#23450;&#28857;&#30340;&#31283;&#23450;&#24615;&#20173;&#28982;&#30693;&#20043;&#29978;&#23569;&#12290;&#26368;&#36817;&#65292;&#23558;Lyapunov&#29702;&#35770;&#24212;&#29992;&#20110;&#21478;&#19968;&#31181;&#31867;&#22411;&#30340;&#38544;&#24335;&#23618;&#27169;&#22411;&#8212;&#8212;&#31070;&#32463;ODE&#65292;&#21487;&#20197;&#36171;&#20104;&#20854;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#23558;DEQ&#27169;&#22411;&#35270;&#20026;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LyaDEQ&#30340;&#40065;&#26834;DEQ&#27169;&#22411;&#65292;&#36890;&#36807;Lyapunov&#29702;&#35770;&#25552;&#20379;&#20102;&#20445;&#35777;&#30340;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#26159;&#30830;&#20445;DEQ&#27169;&#22411;&#30340;&#22266;&#23450;&#28857;&#26159;Lyapunov&#31283;&#23450;&#30340;&#65292;&#36825;&#20351;&#24471;LyaDEQ&#27169;&#22411;&#33021;&#22815;&#25269;&#25239;&#24494;&#23567;&#30340;&#21021;&#22987;&#25200;&#21160;&#12290;&#20026;&#20102;&#36991;&#20813;&#30001;&#20110;Lyapunov&#31283;&#23450;&#30340;&#22266;&#23450;&#28857;&#24444;&#27492;&#38752;&#36817;&#32780;&#23548;&#33268;&#30340;&#19981;&#33391;&#23545;&#25239;&#24615;&#38450;&#24481;&#65292;&#25105;&#20204;&#22312;Lyapunov&#31283;&#23450;&#24615;&#27169;&#22359;&#20043;&#21518;&#21152;&#20837;&#20102;&#19968;&#20010;&#27491;&#20132;&#30340;&#20840;&#36830;&#25509;&#23618;&#65292;&#20197;&#20998;&#31163;&#19981;&#21516;&#30340;&#22266;&#23450;&#28857;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#22522;&#20934;&#19978;&#35780;&#20272;&#20102;LyaDEQ&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep equilibrium (DEQ) models have emerged as a promising class of implicit layer models in deep learning, which abandon traditional depth by solving for the fixed points of a single nonlinear layer. Despite their success, the stability of the fixed points for these models remains poorly understood. Recently, Lyapunov theory has been applied to Neural ODEs, another type of implicit layer model, to confer adversarial robustness. By considering DEQ models as nonlinear dynamic systems, we propose a robust DEQ model named LyaDEQ with guaranteed provable stability via Lyapunov theory. The crux of our method is ensuring the fixed points of the DEQ models are Lyapunov stable, which enables the LyaDEQ models to resist the minor initial perturbations. To avoid poor adversarial defense due to Lyapunov-stable fixed points being located near each other, we add an orthogonal fully connected layer after the Lyapunov stability module to separate different fixed points. We evaluate LyaDEQ models on se
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#24179;&#38754;&#20809;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30456;&#32467;&#21512;&#30340;&#27844;&#28431;&#27874;&#20840;&#24687;&#22825;&#32447;&#35774;&#35745;&#26041;&#27861;&#65292;&#21487;&#29983;&#25104;&#25658;&#24102;&#36712;&#36947;&#35282;&#21160;&#37327;&#30340;&#30005;&#30913;&#27874;&#65292;&#24182;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21457;&#29616;&#25968;&#23398;&#20989;&#25968;&#65292;&#31934;&#30830;&#25511;&#21046;&#36752;&#23556;&#27169;&#24335;&#65292;&#25552;&#39640;&#31995;&#32479;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.12695</link><description>&lt;p&gt;
&#22522;&#20110;&#27844;&#28431;&#27874;&#20840;&#24687;&#25216;&#26415;&#30340;&#36712;&#36947;&#35282;&#21160;&#37327;&#21457;&#29983;&#22120;&#30340;&#28145;&#24230;&#23398;&#20064;&#35774;&#35745;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Deep Learning Framework for the Design of Orbital Angular Momentum Generators Enabled by Leaky-wave Holograms. (arXiv:2304.12695v1 [physics.optics])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#24179;&#38754;&#20809;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30456;&#32467;&#21512;&#30340;&#27844;&#28431;&#27874;&#20840;&#24687;&#22825;&#32447;&#35774;&#35745;&#26041;&#27861;&#65292;&#21487;&#29983;&#25104;&#25658;&#24102;&#36712;&#36947;&#35282;&#21160;&#37327;&#30340;&#30005;&#30913;&#27874;&#65292;&#24182;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21457;&#29616;&#25968;&#23398;&#20989;&#25968;&#65292;&#31934;&#30830;&#25511;&#21046;&#36752;&#23556;&#27169;&#24335;&#65292;&#25552;&#39640;&#31995;&#32479;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#21033;&#29992;&#24179;&#38754;&#20809;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30456;&#32467;&#21512;&#30340;&#27844;&#28431;&#27874;&#20840;&#24687;&#22825;&#32447;&#35774;&#35745;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#29983;&#25104;&#25658;&#24102;&#36712;&#36947;&#35282;&#21160;&#37327;&#30340;&#30005;&#30913;&#27874;&#12290;&#20026;&#20102;&#25552;&#39640;&#31995;&#32479;&#24615;&#33021;&#65292;&#25105;&#20204;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#26469;&#21457;&#29616;&#19968;&#31181;&#25968;&#23398;&#20989;&#25968;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#25511;&#21046;&#25972;&#20010;&#36752;&#23556;&#27169;&#24335;&#65292;&#21363;&#22312;&#38477;&#20302;&#36793;&#29923;&#30005;&#24179;&#30340;&#21516;&#26102;&#22686;&#21152;&#36752;&#23556;&#27169;&#24335;&#30340;&#20013;&#22830;&#31354;&#27934;&#28145;&#24230;&#12290;&#38656;&#35201;&#26681;&#25454;&#20840;&#24687;&#29702;&#35770;&#31934;&#30830;&#35843;&#33410;&#38459;&#25239;&#26041;&#31243;&#30340;&#21442;&#25968;&#25165;&#33021;&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#23454;&#29616;&#26368;&#20339;&#32467;&#26524;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#30830;&#23450;&#21442;&#25968;&#30340;&#36817;&#20284;&#20540;&#12290;&#25105;&#20204;&#21487;&#20197;&#21033;&#29992;77,000&#20010;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#30830;&#23450;&#27599;&#20010;&#21442;&#25968;&#30340;&#26368;&#20339;&#20540;&#65292;&#20174;&#32780;&#24471;&#21040;&#25152;&#26399;&#26395;&#30340;&#36752;&#23556;&#27169;&#24335;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#19981;&#20165;&#33410;&#30465;&#26102;&#38388;&#65292;&#32780;&#19988;&#21487;&#20197;&#33719;&#24471;&#26356;&#31934;&#30830;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a novel approach for the design of leaky-wave holographic antennas that generates OAM-carrying electromagnetic waves by combining Flat Optics (FO) and machine learning (ML) techniques. To improve the performance of our system, we use a machine learning technique to discover a mathematical function that can effectively control the entire radiation pattern, i.e., decrease the side lobe level (SLL) while simultaneously increasing the central null depth of the radiation pattern. Precise tuning of the parameters of the impedance equation based on holographic theory is necessary to achieve optimal results in a variety of scenarios. In this research, we applied machine learning to determine the approximate values of the parameters. We can determine the optimal values for each parameter, resulting in the desired radiation pattern, using a total of 77,000 generated datasets. Furthermore, the use of ML not only saves time, but also yields more precise and accurate resul
&lt;/p&gt;</description></item><item><title>Phylo2Vec&#26159;&#19968;&#31181;&#26032;&#30340;&#20108;&#21449;&#26641;&#31616;&#26126;&#34920;&#31034;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#36731;&#26494;&#37319;&#26679;&#20108;&#21449;&#26641;&#65292;&#24182;&#20197;&#31995;&#32479;&#24615;&#30340;&#26041;&#27861;&#36941;&#21382;&#26641;&#31354;&#38388;&#12290;&#36825;&#31181;&#26041;&#27861;&#29992;&#20110;&#26500;&#24314;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#34507;&#30333;&#36136;&#31867;&#21035;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.12693</link><description>&lt;p&gt;
Phylo2Vec: &#19968;&#31181;&#20108;&#21449;&#26641;&#30340;&#21521;&#37327;&#34920;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Phylo2Vec: a vector representation for binary trees. (arXiv:2304.12693v1 [q-bio.PE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12693
&lt;/p&gt;
&lt;p&gt;
Phylo2Vec&#26159;&#19968;&#31181;&#26032;&#30340;&#20108;&#21449;&#26641;&#31616;&#26126;&#34920;&#31034;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#36731;&#26494;&#37319;&#26679;&#20108;&#21449;&#26641;&#65292;&#24182;&#20197;&#31995;&#32479;&#24615;&#30340;&#26041;&#27861;&#36941;&#21382;&#26641;&#31354;&#38388;&#12290;&#36825;&#31181;&#26041;&#27861;&#29992;&#20110;&#26500;&#24314;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#34507;&#30333;&#36136;&#31867;&#21035;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#29983;&#29289;&#25968;&#25454;&#25512;&#26029;&#24471;&#21040;&#30340;&#20108;&#21449;&#36827;&#21270;&#26641;&#23545;&#20110;&#29702;&#35299;&#29983;&#29289;&#20043;&#38388;&#20849;&#20139;&#30340;&#36827;&#21270;&#21382;&#21490;&#33267;&#20851;&#37325;&#35201;&#12290;&#26681;&#25454;&#26368;&#22823;&#20284;&#28982;&#31561;&#26576;&#20010;&#26368;&#20248;&#24615;&#20934;&#21017;&#25512;&#26029;&#20986;&#26641;&#20013;&#28508;&#22312;&#33410;&#28857;&#30340;&#20301;&#32622;&#26159;NP-hard&#38382;&#39064;&#65292;&#36825;&#25512;&#21160;&#20102;&#22823;&#37327;&#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#21551;&#21457;&#24335;&#26041;&#27861;&#36890;&#24120;&#32570;&#20047;&#19968;&#31181;&#31995;&#32479;&#24615;&#30340;&#26041;&#27861;&#26469;&#22343;&#21248;&#37319;&#26679;&#38543;&#26426;&#26641;&#25110;&#26377;&#25928;&#22320;&#25506;&#32034;&#25351;&#25968;&#32423;&#22686;&#38271;&#30340;&#26641;&#31354;&#38388;&#65292;&#36825;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#31561;&#20248;&#21270;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Phylo2Vec&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#31616;&#26126;&#34920;&#31034;&#26041;&#27861;&#26469;&#34920;&#31034;&#36827;&#21270;&#26641;&#12290;Phylo2Vec&#23558;&#20219;&#20309;&#20855;&#26377;n&#20010;&#21494;&#23376;&#30340;&#20108;&#21449;&#26641;&#26144;&#23556;&#21040;&#38271;&#24230;&#20026;n&#30340;&#25972;&#25968;&#21521;&#37327;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;Phylo2Vec&#22312;&#31354;&#38388;&#20013;&#26082;&#26159;&#33391;&#23450;&#30340;&#21448;&#26159;&#21452;&#23556;&#30340;&#12290;Phylo2Vec&#30340;&#20248;&#28857;&#26159;&#65306;i&#65289;&#36731;&#26494;&#22343;&#21248;&#37319;&#26679;&#20108;&#21449;&#26641;&#65307;ii&#65289;&#20197;&#38750;&#24120;&#22823;&#25110;&#23567;&#30340;&#27493;&#38271;&#31995;&#32479;&#22320;&#36941;&#21382;&#26641;&#31354;&#38388;&#12290;&#20316;&#20026;&#27010;&#24565;&#39564;&#35777;&#65292;&#25105;&#20204;&#20351;&#29992;Phylo2Vec&#26500;&#24314;&#20102;&#19968;&#20010;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#20174;&#27688;&#22522;&#37240;&#24207;&#21015;&#39044;&#27979;&#34507;&#30333;&#36136;&#31867;&#21035;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;Phylo2Vec&#26174;&#33879;&#25552;&#39640;&#20102;&#32593;&#32476;&#30340;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#20043;&#21069;&#30340;&#26368;&#20248;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Binary phylogenetic trees inferred from biological data are central to understanding the shared evolutionary history of organisms. Inferring the placement of latent nodes in a tree by any optimality criterion (e.g., maximum likelihood) is an NP-hard problem, propelling the development of myriad heuristic approaches. Yet, these heuristics often lack a systematic means of uniformly sampling random trees or effectively exploring a tree space that grows factorially, which are crucial to optimisation problems such as machine learning. Accordingly, we present Phylo2Vec, a new parsimonious representation of a phylogenetic tree. Phylo2Vec maps any binary tree with $n$ leaves to an integer vector of length $n$. We prove that Phylo2Vec is both well-defined and bijective to the space of phylogenetic trees. The advantages of Phylo2Vec are twofold: i) easy uniform sampling of binary trees and ii) systematic ability to traverse tree space in very large or small jumps. As a proof of concept, we use P
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31867;&#26426;&#21046;&#65292;&#20197;&#23454;&#29616;&#26080;&#26465;&#20214;&#26368;&#20248;&#24615;&#20445;&#35777;&#30340;&#24046;&#20998;&#38544;&#31169;&#12290;&#35813;&#26426;&#21046;&#23558;&#26426;&#21046;&#35774;&#35745;&#38382;&#39064;&#21046;&#23450;&#20026;&#26080;&#38480;&#32500;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.12681</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#40065;&#26834;&#20248;&#21270;&#23454;&#29616;&#24046;&#20998;&#38544;&#31169;&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
Differential Privacy via Distributionally Robust Optimization. (arXiv:2304.12681v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12681
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31867;&#26426;&#21046;&#65292;&#20197;&#23454;&#29616;&#26080;&#26465;&#20214;&#26368;&#20248;&#24615;&#20445;&#35777;&#30340;&#24046;&#20998;&#38544;&#31169;&#12290;&#35813;&#26426;&#21046;&#23558;&#26426;&#21046;&#35774;&#35745;&#38382;&#39064;&#21046;&#23450;&#20026;&#26080;&#38480;&#32500;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#24046;&#20998;&#38544;&#31169;&#24050;&#25104;&#20026;&#20849;&#20139;&#25968;&#25454;&#38598;&#32479;&#35745;&#20449;&#24687;&#24182;&#38480;&#21046;&#28041;&#21450;&#20010;&#20154;&#30340;&#31169;&#20154;&#20449;&#24687;&#25259;&#38706;&#30340;&#20107;&#23454;&#26631;&#20934;&#12290;&#36890;&#36807;&#23545;&#23558;&#35201;&#21457;&#24067;&#30340;&#32479;&#35745;&#25968;&#25454;&#36827;&#34892;&#38543;&#26426;&#25200;&#21160;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#36825;&#21453;&#36807;&#26469;&#23548;&#33268;&#20102;&#38544;&#31169;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65306;&#26356;&#22823;&#30340;&#25200;&#21160;&#25552;&#20379;&#26356;&#24378;&#30340;&#38544;&#31169;&#20445;&#35777;&#65292;&#20294;&#32467;&#26524;&#26159;&#25552;&#20379;&#36739;&#20302;&#23454;&#29992;&#24230;&#30340;&#32479;&#35745;&#25968;&#25454;&#21644;&#26356;&#20302;&#30340;&#20934;&#30830;&#24615;&#12290;&#22240;&#27492;&#65292;&#29305;&#21035;&#24863;&#20852;&#36259;&#30340;&#26159;&#22312;&#39044;&#36873;&#38544;&#31169;&#27700;&#24179;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#26368;&#39640;&#20934;&#30830;&#24615;&#30340;&#26368;&#20339;&#26426;&#21046;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#36825;&#19968;&#39046;&#22495;&#30340;&#24037;&#20316;&#38598;&#20013;&#22312;&#20107;&#20808;&#25351;&#23450;&#25200;&#21160;&#26063;&#24182;&#38543;&#21518;&#35777;&#26126;&#20854;&#28176;&#36817;&#21644;/&#25110;&#26368;&#20339;&#24615;&#19978;&#65292;&#26412;&#25991;&#21017;&#24320;&#21457;&#20102;&#19968;&#31867;&#26426;&#21046;&#65292;&#23427;&#20204;&#20855;&#26377;&#38750;&#28176;&#36817;&#21644;&#26080;&#26465;&#20214;&#30340;&#26368;&#20339;&#24615;&#20445;&#35777;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#26426;&#21046;&#35774;&#35745;&#38382;&#39064;&#21046;&#23450;&#20026;&#26080;&#38480;&#32500;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, differential privacy has emerged as the de facto standard for sharing statistics of datasets while limiting the disclosure of private information about the involved individuals. This is achieved by randomly perturbing the statistics to be published, which in turn leads to a privacy-accuracy trade-off: larger perturbations provide stronger privacy guarantees, but they result in less accurate statistics that offer lower utility to the recipients. Of particular interest are therefore optimal mechanisms that provide the highest accuracy for a pre-selected level of privacy. To date, work in this area has focused on specifying families of perturbations a priori and subsequently proving their asymptotic and/or best-in-class optimality. In this paper, we develop a class of mechanisms that enjoy non-asymptotic and unconditional optimality guarantees. To this end, we formulate the mechanism design problem as an infinite-dimensional distributionally robust optimization problem. W
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21463;&#38480;&#36890;&#20449;&#21644;&#21152;&#24615;&#39640;&#26031;&#22122;&#22768;&#19979;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#38454;&#27573;&#36172;&#21338;&#31639;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#20449;&#24687;&#29702;&#35770;&#19979;&#38480;&#12290;</title><link>http://arxiv.org/abs/2304.12680</link><description>&lt;p&gt;
&#21463;&#38480;&#36890;&#20449;&#21152;&#24615;&#39640;&#26031;&#22122;&#22768;&#19979;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Communication-Constrained Bandits under Additive Gaussian Noise. (arXiv:2304.12680v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21463;&#38480;&#36890;&#20449;&#21644;&#21152;&#24615;&#39640;&#26031;&#22122;&#22768;&#19979;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#38454;&#27573;&#36172;&#21338;&#31639;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#20449;&#24687;&#29702;&#35770;&#19979;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#20998;&#24067;&#24335;&#38543;&#26426;&#22810;&#33218;&#36172;&#21338;&#26426;,&#20854;&#20013;&#23458;&#25143;&#31471;&#26681;&#25454;&#30456;&#24212;&#30340;&#25289;&#33218;&#22870;&#21169;&#25552;&#20379;&#21463;&#38480;&#36890;&#20449;&#21453;&#39304;&#32473;&#23398;&#20064;&#32773;&#12290;&#22312;&#25105;&#20204;&#30340;&#35774;&#23450;&#19979;,&#23458;&#25143;&#31471;&#24517;&#39035;&#32534;&#30721;&#22870;&#21169;&#65292;&#20351;&#24471;&#32534;&#30721;&#22870;&#21169;&#30340;&#20108;&#38454;&#30697;&#19981;&#36229;&#36807;P&#65292;&#24182;&#19988;&#36825;&#20010;&#32534;&#30721;&#22870;&#21169;&#20250;&#34987;&#26041;&#24046;&#20026;$\sigma^2$&#30340;&#21152;&#24615;&#39640;&#26031;&#22122;&#22768;&#25152;&#27745;&#26579;&#65307;&#23398;&#20064;&#32773;&#21482;&#33021;&#35775;&#38382;&#36825;&#20010;&#34987;&#27745;&#26579;&#30340;&#22870;&#21169;&#12290;&#25105;&#20204;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#23548;&#20986;&#20102;&#20219;&#20309;&#26041;&#26696;&#30340;&#26368;&#23567;&#21270;&#21518;&#24724;&#30340;&#20449;&#24687;&#35770;&#19979;&#38480;$\Omega\left(\sqrt{\frac{KT}{\mathtt{SNR} \wedge1}} \right)$&#65292;&#20854;&#20013; $ \mathtt{SNR} := \frac{P}{\sigma^2}$&#65292;$K$&#21644;$T$&#20998;&#21035;&#26159;&#33218;&#25968;&#21644;&#26102;&#38388;&#38271;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#38454;&#27573;&#36172;&#21338;&#31639;&#27861;$\mathtt{UE\text{-}UCB++}$&#65292;&#23427;&#21487;&#20197;&#23558;&#36825;&#20010;&#19979;&#38480;&#30340;&#20540;&#21152;&#19978;&#19968;&#20010;&#24494;&#23567;&#30340;&#21487;&#21152;&#24615;&#22240;&#23376;&#12290;$\mathtt{UE\text{-}UCB++}$&#22312;&#20854;&#21021;&#22987;&#38454;&#27573;&#25191;&#34892;&#22343;&#21248;&#25506;&#32034;&#65292;&#28982;&#21518;&#22312;&#21518;&#32493;&#38454;&#27573;&#20351;&#29992;&#8220;&#19978;&#32622;&#20449;&#30028;&#8221;(UCB)&#31639;&#27861;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25968;&#20540;&#32467;&#26524;&#65292;&#34920;&#26126;&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#38656;&#35201;&#36825;&#26679;&#30340;&#36890;&#20449;&#26377;&#25928;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a distributed stochastic multi-armed bandit where a client supplies the learner with communication-constrained feedback based on the rewards for the corresponding arm pulls. In our setup, the client must encode the rewards such that the second moment of the encoded rewards is no more than $P$, and this encoded reward is further corrupted by additive Gaussian noise of variance $\sigma^2$; the learner only has access to this corrupted reward. For this setting, we derive an information-theoretic lower bound of $\Omega\left(\sqrt{\frac{KT}{\mathtt{SNR} \wedge1}} \right)$ on the minimax regret of any scheme, where $ \mathtt{SNR} := \frac{P}{\sigma^2}$, and $K$ and $T$ are the number of arms and time horizon, respectively. Furthermore, we propose a multi-phase bandit algorithm, $\mathtt{UE\text{-}UCB++}$, which matches this lower bound to a minor additive factor. $\mathtt{UE\text{-}UCB++}$ performs uniform exploration in its initial phases and then utilizes the {\em upper confidence
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26368;&#22823;&#32534;&#30721;&#36895;&#29575;&#38477;&#20302;&#19979;&#30340;&#21477;&#23376;&#34920;&#31034;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#21477;&#23376;&#34920;&#31034;&#27169;&#22411;Sentence-BERT&#20013;&#21152;&#20837;&#19968;&#20010;&#39069;&#22806;&#30340;&#23398;&#20064;&#25237;&#24433;&#23618;&#65292;&#24182;&#35777;&#26126;&#20854;&#21487;&#20197;&#22312;&#35821;&#20041;&#30456;&#20851;&#20219;&#21153;&#20013;&#33719;&#24471;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.12674</link><description>&lt;p&gt;
&#26368;&#22823;&#32534;&#30721;&#36895;&#29575;&#38477;&#20302;&#19979;&#30340;&#21477;&#23376;&#34920;&#31034;&#21387;&#32553;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Compressing Sentence Representation with maximum Coding Rate Reduction. (arXiv:2304.12674v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12674
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26368;&#22823;&#32534;&#30721;&#36895;&#29575;&#38477;&#20302;&#19979;&#30340;&#21477;&#23376;&#34920;&#31034;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#21477;&#23376;&#34920;&#31034;&#27169;&#22411;Sentence-BERT&#20013;&#21152;&#20837;&#19968;&#20010;&#39069;&#22806;&#30340;&#23398;&#20064;&#25237;&#24433;&#23618;&#65292;&#24182;&#35777;&#26126;&#20854;&#21487;&#20197;&#22312;&#35821;&#20041;&#30456;&#20851;&#20219;&#21153;&#20013;&#33719;&#24471;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22810;&#25968;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#38382;&#39064;&#20013;&#65292;&#38656;&#35201;&#20351;&#29992;&#21477;&#23376;&#34920;&#31034;&#26469;&#36827;&#34892;&#35821;&#20041;&#26816;&#32034;&#20219;&#21153;&#12290;&#22312;&#36817;&#24180;&#26469;&#65292;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#30456;&#24403;&#26377;&#25928;&#22320;&#35745;&#31639;&#36825;&#20123;&#34920;&#31034;&#12290;&#36825;&#20123;&#27169;&#22411;&#20135;&#29983;&#39640;&#32500;&#21477;&#23376;&#23884;&#20837;&#12290;&#23454;&#38469;&#19978;&#23384;&#22312;&#22823;&#22411;&#21644;&#23567;&#22411;&#27169;&#22411;&#20043;&#38388;&#26126;&#26174;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#22240;&#27492;&#65292;&#30001;&#20110;&#31354;&#38388;&#21644;&#26102;&#38388;&#30828;&#20214;&#38480;&#21046;&#65292;&#38656;&#35201;&#22312;&#20351;&#29992;&#36739;&#23567;&#27169;&#22411;(&#36890;&#24120;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31934;&#31616;&#29256;&#26412;)&#26102;&#33719;&#24471;&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#26368;&#22823;&#32534;&#30721;&#36895;&#29575;&#38477;&#20302;(MCR2)&#30446;&#26631;&#30340;&#22522;&#30784;&#19978;&#23398;&#20064;&#39069;&#22806;&#30340;&#25237;&#24433;&#23618;&#65292;&#35780;&#20272;&#20102;&#21477;&#23376;&#34920;&#31034;&#27169;&#22411;Sentence-BERT&#30340;&#27169;&#22411;&#33976;&#39311;&#65292;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;MCR2&#26159;&#19968;&#31181;&#20026;&#20102;&#36890;&#29992;&#27969;&#24418;&#32858;&#31867;&#32780;&#24320;&#21457;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#36890;&#36807;&#22312;&#22797;&#26434;&#24230;&#21644;&#21477;&#23376;&#23884;&#20837;&#22823;&#23567;&#26041;&#38754;&#20943;&#23567;&#30340;&#26032;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#22312;&#35821;&#20041;&#30456;&#20851;&#20219;&#21153;&#20013;&#33719;&#24471;&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In most natural language inference problems, sentence representation is needed for semantic retrieval tasks. In recent years, pre-trained large language models have been quite effective for computing such representations. These models produce high-dimensional sentence embeddings. An evident performance gap between large and small models exists in practice. Hence, due to space and time hardware limitations, there is a need to attain comparable results when using the smaller model, which is usually a distilled version of the large language model. In this paper, we assess the model distillation of the sentence representation model Sentence-BERT by augmenting the pre-trained distilled model with a projection layer additionally learned on the Maximum Coding Rate Reduction (MCR2)objective, a novel approach developed for general-purpose manifold clustering. We demonstrate that the new language model with reduced complexity and sentence embedding size can achieve comparable results on semantic
&lt;/p&gt;</description></item><item><title>CoDi &#26041;&#27861;&#20351;&#29992;&#20004;&#20010;&#20849;&#21516;&#28436;&#21270;&#30340;&#23545;&#27604;&#25193;&#25955;&#27169;&#22411;&#21333;&#29420;&#22788;&#29702;&#31163;&#25955;&#21644;&#36830;&#32493;&#21464;&#37327;&#24182;&#30456;&#20114;&#26465;&#20214;&#21270;&#65292;&#21516;&#26102;&#24341;&#20837;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#36827;&#19968;&#27493;&#30340;&#32465;&#23450;&#65292;&#23637;&#29616;&#20102;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.12654</link><description>&lt;p&gt;
CoDi: &#28151;&#21512;&#31867;&#22411;&#34920;&#26684;&#29983;&#25104;&#30340;&#20849;&#21516;&#28436;&#21270;&#23545;&#27604;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CoDi: Co-evolving Contrastive Diffusion Models for Mixed-type Tabular Synthesis. (arXiv:2304.12654v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12654
&lt;/p&gt;
&lt;p&gt;
CoDi &#26041;&#27861;&#20351;&#29992;&#20004;&#20010;&#20849;&#21516;&#28436;&#21270;&#30340;&#23545;&#27604;&#25193;&#25955;&#27169;&#22411;&#21333;&#29420;&#22788;&#29702;&#31163;&#25955;&#21644;&#36830;&#32493;&#21464;&#37327;&#24182;&#30456;&#20114;&#26465;&#20214;&#21270;&#65292;&#21516;&#26102;&#24341;&#20837;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#36827;&#19968;&#27493;&#30340;&#32465;&#23450;&#65292;&#23637;&#29616;&#20102;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36234;&#26469;&#36234;&#22810;&#30340;&#27880;&#24847;&#21147;&#34987;&#25918;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#65292;&#23558;&#32508;&#21512;&#34920;&#26684;&#24212;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#30340;&#23581;&#35797;&#24050;&#32463;&#21521;&#21508;&#31181;&#22330;&#26223;&#25193;&#23637;&#12290;&#30001;&#20110;&#29983;&#25104;&#24314;&#27169;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#36890;&#36807;&#34920;&#26684;&#25968;&#25454;&#32508;&#21512;&#27169;&#22411;&#29983;&#25104;&#30340;&#34394;&#20551;&#25968;&#25454;&#21464;&#24471;&#22797;&#26434;&#32780;&#30495;&#23454;&#12290;&#20294;&#26159;&#65292;&#24314;&#27169;&#34920;&#26684;&#25968;&#25454;&#30340;&#31163;&#25955;&#21464;&#37327;&#65288;&#21015;&#65289;&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#20004;&#20010;&#23545;&#27604;&#25193;&#25955;&#27169;&#22411;&#21333;&#29420;&#22788;&#29702;&#36830;&#32493;&#21644;&#31163;&#25955;&#21464;&#37327;&#65288;&#20294;&#30456;&#20114;&#26465;&#20214;&#21270;&#65289;&#12290;&#20004;&#20010;&#25193;&#25955;&#27169;&#22411;&#36890;&#36807;&#24444;&#27492;&#35835;&#21462;&#26465;&#20214;&#22312;&#35757;&#32451;&#20013;&#20849;&#21516;&#28436;&#21270;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#36827;&#19968;&#27493;&#32465;&#23450;&#25193;&#25955;&#27169;&#22411;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36127;&#37319;&#26679;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#12290;&#22312;11&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#21644;8&#20010;&#22522;&#20934;&#26041;&#27861;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861; CoDi &#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With growing attention to tabular data these days, the attempt to apply a synthetic table to various tasks has been expanded toward various scenarios. Owing to the recent advances in generative modeling, fake data generated by tabular data synthesis models become sophisticated and realistic. However, there still exists a difficulty in modeling discrete variables (columns) of tabular data. In this work, we propose to process continuous and discrete variables separately (but being conditioned on each other) by two diffusion models. The two diffusion models are co-evolved during training by reading conditions from each other. In order to further bind the diffusion models, moreover, we introduce a contrastive learning method with a negative sampling method. In our experiments with 11 real-world tabular datasets and 8 baseline methods, we prove the efficacy of the proposed method, called CoDi.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#24120;&#29992;&#30340;&#21098;&#26525;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#31995;&#32479;&#20998;&#26512;&#20102;&#20854;&#21487;&#33021;&#24341;&#21457;&#30340;&#20559;&#24046;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#21487;&#20197;&#38477;&#20302;&#20559;&#24046;&#30340;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2304.12622</link><description>&lt;p&gt;
&#21098;&#26525;&#35270;&#35273;&#27169;&#22411;&#20013;&#30340;&#20559;&#24046;&#38382;&#39064;&#65306;&#28145;&#20837;&#20998;&#26512;&#19982;&#23545;&#31574;
&lt;/p&gt;
&lt;p&gt;
Bias in Pruned Vision Models: In-Depth Analysis and Countermeasures. (arXiv:2304.12622v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12622
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#24120;&#29992;&#30340;&#21098;&#26525;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#31995;&#32479;&#20998;&#26512;&#20102;&#20854;&#21487;&#33021;&#24341;&#21457;&#30340;&#20559;&#24046;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#21487;&#20197;&#38477;&#20302;&#20559;&#24046;&#30340;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21098;&#26525;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21098;&#26525;&#21487;&#33021;&#20250;&#24341;&#36215;&#25110;&#21152;&#21095;&#21387;&#32553;&#27169;&#22411;&#36755;&#20986;&#30340;&#20559;&#24046;&#12290;&#26412;&#25991;&#31995;&#32479;&#22320;&#30740;&#31350;&#21644;&#25551;&#36848;&#20102;&#36825;&#31181;&#29616;&#35937;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#34892;&#20026;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#26131;&#20110;&#20351;&#29992;&#30340;&#26631;&#20934;&#26469;&#24110;&#21161;&#20943;&#23569;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pruning - that is, setting a significant subset of the parameters of a neural network to zero - is one of the most popular methods of model compression. Yet, several recent works have raised the issue that pruning may induce or exacerbate bias in the output of the compressed model. Despite existing evidence for this phenomenon, the relationship between neural network pruning and induced bias is not well-understood. In this work, we systematically investigate and characterize this phenomenon in Convolutional Neural Networks for computer vision. First, we show that it is in fact possible to obtain highly-sparse models, e.g. with less than 10% remaining weights, which do not decrease in accuracy nor substantially increase in bias when compared to dense models. At the same time, we also find that, at higher sparsities, pruned models exhibit higher uncertainty in their outputs, as well as increased correlations, which we directly link to increased bias. We propose easy-to-use criteria which
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#24314;&#27169;&#19981;&#30830;&#23450;&#21644;&#36864;&#21270;&#28382;&#21518;&#31995;&#32479;&#30340;&#21452;&#20445;&#30495;DeepONet&#26041;&#27861;&#65292;&#22312;&#19981;&#20102;&#35299;&#36864;&#21270;&#25928;&#24212;&#24615;&#36136;&#30340;&#21407;&#22987;&#27169;&#22411;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#20302;&#20445;&#30495;&#24230;&#34920;&#31034;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#36864;&#21270;&#28382;&#21518;&#31995;&#32479;&#30340;&#39044;&#27979;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.12609</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#24314;&#27169;&#19981;&#30830;&#23450;&#21644;&#34928;&#20943;&#28382;&#21518;&#31995;&#32479;&#30340;&#21452;&#20445;&#30495;DeepONet&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Bi-fidelity DeepONet Approach for Modeling Uncertain and Degrading Hysteretic Systems. (arXiv:2304.12609v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12609
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#24314;&#27169;&#19981;&#30830;&#23450;&#21644;&#36864;&#21270;&#28382;&#21518;&#31995;&#32479;&#30340;&#21452;&#20445;&#30495;DeepONet&#26041;&#27861;&#65292;&#22312;&#19981;&#20102;&#35299;&#36864;&#21270;&#25928;&#24212;&#24615;&#36136;&#30340;&#21407;&#22987;&#27169;&#22411;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#20302;&#20445;&#30495;&#24230;&#34920;&#31034;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#36864;&#21270;&#28382;&#21518;&#31995;&#32479;&#30340;&#39044;&#27979;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#19981;&#30830;&#23450;&#24615;&#21518;&#65292;&#38750;&#32447;&#24615;&#31995;&#32479;&#22914;&#28382;&#21518;&#34892;&#20026;&#30340;&#36864;&#21270;&#36890;&#24120;&#20986;&#29616;&#22312;&#24037;&#31243;&#24212;&#29992;&#20013;&#65292;&#32780;&#24314;&#27169;&#36825;&#31181;&#31995;&#32479;&#21464;&#24471;&#36234;&#26469;&#36234;&#22256;&#38590;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#33719;&#21462;&#19981;&#20102;&#35299;&#36864;&#21270;&#25928;&#24212;&#24615;&#36136;&#30340;&#21407;&#22987;&#27169;&#22411;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#20351;&#29992;&#26469;&#33258;&#21407;&#22987;&#27169;&#22411;&#30340;&#25968;&#25454;&#38598;&#20316;&#20026;&#20302;&#20445;&#30495;&#24230;&#34920;&#31034;&#65292;&#20197;&#35757;&#32451;DeepONet&#12290;&#19977;&#20010;&#25968;&#20540;&#23454;&#20363;&#29992;&#20110;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;DeepONets&#30340;&#20351;&#29992;&#65292;&#20197;&#27169;&#25311;&#20302;&#20445;&#30495;&#24230;&#27169;&#22411;&#21644;&#30495;&#23454;&#31995;&#32479;&#21709;&#24212;&#20043;&#38388;&#30340;&#20559;&#24046;&#65292;&#22312;&#27169;&#22411;&#21442;&#25968;&#23384;&#22312;&#19981;&#30830;&#23450;&#24615;&#26102;&#65292;&#22312;&#36864;&#21270;&#28382;&#21518;&#31995;&#32479;&#20013;&#39044;&#27979;&#35823;&#24046;&#26174;&#33879;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nonlinear systems, such as with degrading hysteretic behavior, are often encountered in engineering applications. In addition, due to the ubiquitous presence of uncertainty and the modeling of such systems becomes increasingly difficult. On the other hand, datasets from pristine models developed without knowing the nature of the degrading effects can be easily obtained. In this paper, we use datasets from pristine models without considering the degrading effects of hysteretic systems as low-fidelity representations that capture many of the important characteristics of the true system's behavior to train a deep operator network (DeepONet). Three numerical examples are used to show that the proposed use of the DeepONets to model the discrepancies between the low-fidelity model and the true system's response leads to significant improvements in the prediction error in the presence of uncertainty in the model parameters for degrading hysteretic systems.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#19977;&#31181;&#22238;&#24402;&#27169;&#22411;&#22312;&#39044;&#27979;&#21307;&#30103;&#20445;&#38505;&#36153;&#29992;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#25552;&#20986;&#20102;&#26368;&#37325;&#35201;&#30340;&#39044;&#27979;&#29305;&#24449;&#20026;&#34987;&#20445;&#38505;&#20154;&#30340;&#24180;&#40836;&#12290;</title><link>http://arxiv.org/abs/2304.12605</link><description>&lt;p&gt;
&#22238;&#24402;&#27169;&#22411;&#22312;&#39044;&#27979;&#21307;&#30103;&#20445;&#38505;&#36153;&#29992;&#26041;&#38754;&#30340;&#34920;&#29616;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Performance Evaluation of Regression Models in Predicting the Cost of Medical Insurance. (arXiv:2304.12605v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12605
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#19977;&#31181;&#22238;&#24402;&#27169;&#22411;&#22312;&#39044;&#27979;&#21307;&#30103;&#20445;&#38505;&#36153;&#29992;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#25552;&#20986;&#20102;&#26368;&#37325;&#35201;&#30340;&#39044;&#27979;&#29305;&#24449;&#20026;&#34987;&#20445;&#38505;&#20154;&#30340;&#24180;&#40836;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;&#22238;&#24402;&#27169;&#22411;&#22312;&#39044;&#27979;&#21307;&#30103;&#20445;&#38505;&#36153;&#29992;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#20351;&#29992;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19977;&#31181;&#22238;&#24402;&#27169;&#22411;&#65292;&#20998;&#21035;&#26159;&#32447;&#24615;&#22238;&#24402;&#12289;&#26799;&#24230;&#25552;&#21319;&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#12290;&#36890;&#36807;RMSE&#65288;&#22343;&#26041;&#26681;&#65289;&#12289;r2&#65288;R&#24179;&#26041;&#65289;&#21644;K-Fold&#20132;&#21449;&#39564;&#35777;&#26469;&#35780;&#20272;&#24615;&#33021;&#12290;&#35813;&#30740;&#31350;&#36824;&#35797;&#22270;&#25214;&#20986;&#39044;&#27979;&#21307;&#30103;&#20445;&#38505;&#36153;&#29992;&#20013;&#26368;&#37325;&#35201;&#30340;&#29305;&#24449;&#12290;&#35813;&#30740;&#31350;&#22522;&#20110;&#25968;&#25454;&#24211;&#20013;&#30340;&#30693;&#35782;&#21457;&#29616;&#65288;KDD&#65289;&#36807;&#31243;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#19977;&#20010;&#22238;&#24402;&#27169;&#22411;&#20013;&#65292;&#26799;&#24230;&#25552;&#21319;&#33719;&#24471;&#20102;&#26368;&#39640;&#30340;r2&#65288;R&#24179;&#26041;&#65289; 0.892&#65292;&#26368;&#20302;&#30340;RMSE&#65288;&#22343;&#26041;&#26681;&#65289;&#20026;1336.594&#12290;&#27492;&#22806;&#65292;10&#25240;&#20132;&#21449;&#39564;&#35777;&#30340;&#24179;&#22343;&#26435;&#37325;&#32467;&#26524;&#19982;&#19977;&#20010;&#22238;&#24402;&#27169;&#22411;&#30340;r2&#65288;R&#24179;&#26041;&#65289;&#32467;&#26524;&#27809;&#26377;&#26174;&#30528;&#24046;&#24322;&#12290;&#22312;&#29305;&#24449;&#20013;&#65292;&#34987;&#20445;&#38505;&#20154;&#30340;&#24180;&#40836;&#23545;&#39044;&#27979;&#21307;&#30103;&#20445;&#38505;&#36153;&#29992;&#20855;&#26377;&#26368;&#26174;&#33879;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
The study aimed to evaluate the regression models' performance in predicting the cost of medical insurance. The Three (3) Regression Models in Machine Learning namely Linear Regression, Gradient Boosting, and Support Vector Machine were used. The performance will be evaluated using the metrics RMSE (Root Mean Square), r2 (R Square), and K-Fold Cross-validation. The study also sought to pinpoint the feature that would be most important in predicting the cost of medical insurance.The study is anchored on the knowledge discovery in databases (KDD) process. (KDD) process refers to the overall process of discovering useful knowledge from data. It show the performance evaluation results reveal that among the three (3) Regression models, Gradient boosting received the highest r2 (R Square) 0.892 and the lowest RMSE (Root Mean Square) 1336.594. Furthermore, the 10-Fold Cross-validation weighted mean findings are not significantly different from the r2 (R Square) results of the three (3) regres
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26159;&#19968;&#31687;&#20171;&#32461;&#32431;&#25968;&#23398;&#23478;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#24037;&#20855;&#36827;&#34892;&#30740;&#31350;&#30340;&#20010;&#20154;&#21644;&#38750;&#27491;&#24335;&#21465;&#36848;&#12290;</title><link>http://arxiv.org/abs/2304.12602</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#23545;&#20110;&#32431;&#25968;&#23398;&#23478;&#26469;&#35828;&#26159;&#19968;&#20010;&#26377;&#29992;&#30340;&#24037;&#20855;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is deep learning a useful tool for the pure mathematician?. (arXiv:2304.12602v1 [math.RT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12602
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#19968;&#31687;&#20171;&#32461;&#32431;&#25968;&#23398;&#23478;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#24037;&#20855;&#36827;&#34892;&#30740;&#31350;&#30340;&#20010;&#20154;&#21644;&#38750;&#27491;&#24335;&#21465;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31687;&#20851;&#20110;&#32431;&#25968;&#23398;&#23478;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#24037;&#20855;&#36827;&#34892;&#30740;&#31350;&#30340;&#20010;&#20154;&#21644;&#38750;&#27491;&#24335;&#21465;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
A personal and informal account of what a pure mathematician might expect when using tools from deep learning in their research.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29289;&#29702;&#21551;&#31034;&#30340;&#33258;&#28982;&#32452;&#32455;&#34920;&#24449;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23616;&#37096;&#22240;&#26524;&#29366;&#24577;&#25429;&#33719;&#22797;&#26434;&#26102;&#31354;&#31995;&#32479;&#20013;&#30340;&#26377;&#24207;&#34892;&#20026;&#21644;&#30456;&#24178;&#32467;&#26500;&#12290;&#35813;&#26041;&#27861;&#22312;&#23454;&#38469;&#39046;&#22495;&#31185;&#23398;&#38382;&#39064;&#20013;&#20855;&#26377;&#24456;&#22909;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.12586</link><description>&lt;p&gt;
&#22797;&#26434;&#21160;&#21147;&#31995;&#32479;&#20013;&#29289;&#29702;&#21551;&#31034;&#19979;&#30340;&#34920;&#24449;&#23398;&#20064;&#29992;&#20110;&#33258;&#28982;&#32452;&#32455;
&lt;/p&gt;
&lt;p&gt;
Physics-Informed Representation Learning for Emergent Organization in Complex Dynamical Systems. (arXiv:2304.12586v1 [physics.comp-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12586
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29289;&#29702;&#21551;&#31034;&#30340;&#33258;&#28982;&#32452;&#32455;&#34920;&#24449;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23616;&#37096;&#22240;&#26524;&#29366;&#24577;&#25429;&#33719;&#22797;&#26434;&#26102;&#31354;&#31995;&#32479;&#20013;&#30340;&#26377;&#24207;&#34892;&#20026;&#21644;&#30456;&#24178;&#32467;&#26500;&#12290;&#35813;&#26041;&#27861;&#22312;&#23454;&#38469;&#39046;&#22495;&#31185;&#23398;&#38382;&#39064;&#20013;&#20855;&#26377;&#24456;&#22909;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#30340;&#31995;&#32479;&#32452;&#20214;&#32463;&#24120;&#24341;&#20837;&#19981;&#31283;&#23450;&#24615;&#65292;&#20135;&#29983;&#20855;&#26377;&#26032;&#23646;&#24615;&#21644;&#19981;&#21516;&#26102;&#31354;&#23610;&#24230;&#30340;&#29616;&#35937;&#12290;&#36825;&#34987;&#31216;&#20026;&#33258;&#21457;&#33258;&#32452;&#32455;&#65292;&#26159;&#22312;&#36828;&#31163;&#28909;&#21147;&#23398;&#24179;&#34913;&#30340;&#31995;&#32479;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#29702;&#35770;&#22522;&#30784;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#31639;&#27861;&#23454;&#29616;&#33258;&#28982;&#32452;&#32455;&#12290;&#23427;&#30340;&#22522;&#26412;&#26500;&#20214;&#26159;&#36890;&#36807;&#23616;&#37096;&#20132;&#20114;&#25429;&#33719;&#20449;&#24687;&#22312;&#31995;&#32479;&#20013;&#20256;&#25773;&#26041;&#24335;&#30340;&#26102;&#31354;&#20809;&#38181;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#39044;&#27979;&#20809;&#38181;&#31561;&#20215;&#31867;&#8212;&#8212;&#23616;&#37096;&#22240;&#26524;&#29366;&#24577;&#8212;&#8212;&#25429;&#33719;&#22797;&#26434;&#26102;&#31354;&#31995;&#32479;&#20013;&#30340;&#26377;&#24207;&#34892;&#20026;&#21644;&#30456;&#24178;&#32467;&#26500;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#26080;&#30417;&#30563;&#29289;&#29702;&#21551;&#31034;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#39640;&#24615;&#33021;&#35745;&#31639;&#23454;&#29616;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23616;&#37096;&#22240;&#26524;&#29366;&#24577;&#22312;&#23454;&#38469;&#39046;&#22495;&#31185;&#23398;&#38382;&#39064;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23616;&#37096;&#22240;&#26524;&#29366;&#24577;&#25429;&#33719;&#26059;&#28065;&#21450;&#20854;&#21151;&#29575;&#30340;&#24773;&#20917;&#65292;&#36825;&#20123;&#26159;&#33258;&#28982;&#21644;&#24037;&#31243;&#39046;&#22495;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nonlinearly interacting system components often introduce instabilities that generate phenomena with new properties and at different space-time scales than the components. This is known as spontaneous self-organization and is ubiquitous in systems far from thermodynamic equilibrium. We introduce a theoretically-grounded framework for emergent organization that, via data-driven algorithms, is constructive in practice. Its building blocks are spacetime lightcones that capture how information propagates across a system through local interactions. We show that predictive equivalence classes of lightcones, local causal states, capture organized behaviors and coherent structures in complex spatiotemporal systems. Using our unsupervised physics-informed machine learning algorithm and a high-performance computing implementation, we demonstrate the applicability of the local causal states for real-world domain science problems. We show that the local causal states capture vortices and their pow
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#24863;&#30693;&#25513;&#34109;&#33258;&#32534;&#30721;&#22120;&#65288;PiMAE&#65289;&#26469;&#30452;&#25509;&#20174;&#21407;&#22987;&#26174;&#24494;&#38236;&#22270;&#20687;&#20013;&#23398;&#20064;&#20272;&#35745;&#28857;&#25193;&#25955;&#20989;&#25968;&#65288;PSF&#65289;&#21644;&#21457;&#23556;&#28304;&#65292;&#24182;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#19990;&#30028;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#20934;&#30830;&#24615;&#21644;&#22122;&#22768;&#40065;&#26834;&#24615;&#65292;&#36229;&#36234;&#20102;DeepSTORM&#21644;Richardson-Lucy&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.12584</link><description>&lt;p&gt;
&#30452;&#25509;&#20174;&#20809;&#23398;&#26174;&#24494;&#38236;&#35266;&#27979;&#20013;&#23398;&#20064;&#25104;&#20687;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Learning imaging mechanism directly from optical microscopy observations. (arXiv:2304.12584v1 [physics.optics])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12584
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#24863;&#30693;&#25513;&#34109;&#33258;&#32534;&#30721;&#22120;&#65288;PiMAE&#65289;&#26469;&#30452;&#25509;&#20174;&#21407;&#22987;&#26174;&#24494;&#38236;&#22270;&#20687;&#20013;&#23398;&#20064;&#20272;&#35745;&#28857;&#25193;&#25955;&#20989;&#25968;&#65288;PSF&#65289;&#21644;&#21457;&#23556;&#28304;&#65292;&#24182;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#19990;&#30028;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#20934;&#30830;&#24615;&#21644;&#22122;&#22768;&#40065;&#26834;&#24615;&#65292;&#36229;&#36234;&#20102;DeepSTORM&#21644;Richardson-Lucy&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#23398;&#26174;&#24494;&#38236;&#22270;&#20687;&#36890;&#36807;&#30452;&#25509;&#21487;&#35270;&#21270;&#32435;&#31859;&#19990;&#30028;&#65292;&#22312;&#31185;&#23398;&#30740;&#31350;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#12290;&#25104;&#20687;&#26426;&#21046;&#34987;&#25551;&#36848;&#20026;&#28857;&#25193;&#25955;&#20989;&#25968;&#65288;PSF&#65289;&#21644;&#21457;&#23556;&#28304;&#30340;&#21367;&#31215;&#65292;&#21487;&#20197;&#22522;&#20110;PSF&#25110;&#31561;&#20215;PSF&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#23454;&#29616;&#23545;&#32435;&#31859;&#19990;&#30028;&#30340;&#26356;&#31934;&#30830;&#25506;&#27979;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#20174;&#26174;&#24494;&#38236;&#22270;&#20687;&#20013;&#25552;&#21462;PSF&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#24863;&#30693;&#30340;&#25513;&#34109;&#33258;&#32534;&#30721;&#22120;&#65288;PiMAE&#65289;&#65292;&#36890;&#36807;&#33258;&#25105;&#30417;&#30563;&#30340;&#23398;&#20064;&#65292;&#21487;&#20197;&#30452;&#25509;&#20174;&#21407;&#22987;&#26174;&#24494;&#38236;&#22270;&#20687;&#20013;&#23398;&#20064;&#20272;&#35745;PSF&#21644;&#21457;&#23556;&#28304;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#19990;&#30028;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#21644;&#22122;&#22768;&#40065;&#26834;&#24615;&#12290;&#22312;&#21512;&#25104;&#25968;&#25454;&#20219;&#21153;&#20013;&#65292;PiMAE&#20248;&#20110;DeepSTORM&#21644;Richardson-Lucy&#31639;&#27861;&#65292;&#24179;&#22343;&#25913;&#36827;&#20998;&#21035;&#20026;19.6&#65285;&#21644;50.7&#65285;&#65288;35&#20010;&#20219;&#21153;&#65289;&#65292;&#20351;&#29992;&#24402;&#19968;&#21270;&#22343;&#26041;&#26681;&#35823;&#24046;&#65288;NRMSE&#65289;m&#26469;&#34913;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optical microscopy image plays an important role in scientific research through the direct visualization of the nanoworld, where the imaging mechanism is described as the convolution of the point spread function (PSF) and emitters. Based on a priori knowledge of the PSF or equivalent PSF, it is possible to achieve more precise exploration of the nanoworld. However, it is an outstanding challenge to directly extract the PSF from microscopy images. Here, with the help of self-supervised learning, we propose a physics-informed masked autoencoder (PiMAE) that enables a learnable estimation of the PSF and emitters directly from the raw microscopy images. We demonstrate our method in synthetic data and real-world experiments with significant accuracy and noise robustness. PiMAE outperforms DeepSTORM and the Richardson-Lucy algorithm in synthetic data tasks with an average improvement of 19.6\% and 50.7\% (35 tasks), respectively, as measured by the normalized root mean square error (NRMSE) m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#38750;&#24179;&#31283;&#29615;&#22659;&#19979;&#21160;&#24577;&#31995;&#32479;&#23454;&#26102;&#23433;&#20840;&#35780;&#20272;&#30340;&#26041;&#27861;&#21644;&#25216;&#26415;&#65292;&#21253;&#25324;&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#12289;&#22312;&#32447;&#21322;&#30417;&#30563;&#23398;&#20064;&#12289;&#22312;&#32447;&#36801;&#31227;&#23398;&#20064;&#21644;&#22312;&#32447;&#24322;&#24120;&#26816;&#27979;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2304.12583</link><description>&lt;p&gt;
&#38750;&#24179;&#31283;&#29615;&#22659;&#19979;&#21160;&#24577;&#31995;&#32479;&#30340;&#23454;&#26102;&#23433;&#20840;&#35780;&#20272;&#65306;&#26041;&#27861;&#21644;&#25216;&#26415;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Real-time Safety Assessment of Dynamic Systems in Non-stationary Environments: A Review of Methods and Techniques. (arXiv:2304.12583v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12583
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#38750;&#24179;&#31283;&#29615;&#22659;&#19979;&#21160;&#24577;&#31995;&#32479;&#23454;&#26102;&#23433;&#20840;&#35780;&#20272;&#30340;&#26041;&#27861;&#21644;&#25216;&#26415;&#65292;&#21253;&#25324;&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#12289;&#22312;&#32447;&#21322;&#30417;&#30563;&#23398;&#20064;&#12289;&#22312;&#32447;&#36801;&#31227;&#23398;&#20064;&#21644;&#22312;&#32447;&#24322;&#24120;&#26816;&#27979;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#31995;&#32479;&#30340;&#23454;&#26102;&#23433;&#20840;&#35780;&#20272;&#26159;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#65292;&#23545;&#20110;&#24037;&#19994;&#21644;&#20132;&#36890;&#24212;&#29992;&#31561;&#21508;&#20010;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#29305;&#21035;&#26159;&#22312;&#38750;&#24179;&#31283;&#29615;&#22659;&#20013;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#38750;&#24179;&#31283;&#29615;&#22659;&#19979;&#23454;&#26102;&#23433;&#20840;&#35780;&#20272;&#26041;&#27861;&#30340;&#20840;&#38754;&#32508;&#36848;&#38459;&#30861;&#20102;&#30456;&#20851;&#26041;&#27861;&#30340;&#36827;&#23637;&#21644;&#25913;&#36827;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#38024;&#23545;&#38750;&#24179;&#31283;&#29615;&#22659;&#19979;&#23454;&#26102;&#23433;&#20840;&#35780;&#20272;&#20219;&#21153;&#30340;&#26041;&#27861;&#21644;&#25216;&#26415;&#32508;&#36848;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#39318;&#20808;&#31361;&#20986;&#20102;&#38750;&#24179;&#31283;&#29615;&#22659;&#20013;&#23454;&#26102;&#23433;&#20840;&#35780;&#20272;&#26041;&#27861;&#30340;&#32972;&#26223;&#21644;&#37325;&#35201;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38382;&#39064;&#25551;&#36848;&#65292;&#21253;&#25324;&#23450;&#20041;&#12289;&#20998;&#31867;&#21644;&#20027;&#35201;&#25361;&#25112;&#12290;&#25105;&#20204;&#36824;&#22238;&#39038;&#20102;&#30456;&#20851;&#25216;&#26415;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#22914;&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#12289;&#22312;&#32447;&#21322;&#30417;&#30563;&#23398;&#20064;&#12289;&#22312;&#32447;&#36801;&#31227;&#23398;&#20064;&#21644;&#22312;&#32447;&#24322;&#24120;&#26816;&#27979;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#26410;&#26469;&#30340;&#23637;&#26395;&#21644;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;&#26412;&#25991;&#30340;&#32508;&#36848;&#26088;&#22312;&#20026;&#38750;&#24179;&#31283;&#29615;&#22659;&#19979;&#23454;&#26102;&#23433;&#20840;&#35780;&#20272;&#25552;&#20379;&#21442;&#32771;&#20381;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-time safety assessment (RTSA) of dynamic systems is a critical task that has significant implications for various fields such as industrial and transportation applications, especially in non-stationary environments. However, the absence of a comprehensive review of real-time safety assessment methods in non-stationary environments impedes the progress and refinement of related methods. In this paper, a review of methods and techniques for RTSA tasks in non-stationary environments is provided. Specifically, the background and significance of RTSA approaches in non-stationary environments are firstly highlighted. We then present a problem description that covers the definition, classification, and main challenges. We review recent developments in related technologies such as online active learning, online semi-supervised learning, online transfer learning, and online anomaly detection. Finally, we discuss future outlooks and potential directions for further research. Our review aims
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;DNN&#30340;&#23398;&#20064;&#36712;&#36857;&#19982;&#20854;&#22312;&#20248;&#21270;&#21518;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#32852;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#36712;&#36857;&#22797;&#26434;&#24615;&#21644;&#35757;&#32451;&#38598;&#20559;&#32622;&#21644;&#22810;&#26679;&#24615;&#27604;&#29575;&#30340;&#26032;&#30340;&#27867;&#21270;&#19978;&#30028;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.12579</link><description>&lt;p&gt;
&#23398;&#20064;&#36712;&#36857;&#26159;&#27867;&#21270;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
Learning Trajectories are Generalization Indicators. (arXiv:2304.12579v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12579
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;DNN&#30340;&#23398;&#20064;&#36712;&#36857;&#19982;&#20854;&#22312;&#20248;&#21270;&#21518;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#32852;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#36712;&#36857;&#22797;&#26434;&#24615;&#21644;&#35757;&#32451;&#38598;&#20559;&#32622;&#21644;&#22810;&#26679;&#24615;&#27604;&#29575;&#30340;&#26032;&#30340;&#27867;&#21270;&#19978;&#30028;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#23398;&#20064;&#36712;&#36857;&#19982;&#20854;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;&#26799;&#24230;&#19979;&#38477;&#21644;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#20248;&#21270;&#26102;&#23545;&#24212;&#30340;&#27867;&#21270;&#33021;&#21147;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#26412;&#25991;&#26500;&#24314;&#20102;&#32447;&#24615;&#36817;&#20284;&#20989;&#25968;&#26469;&#27169;&#25311;&#36712;&#36857;&#20449;&#24687;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26356;&#20016;&#23500;&#36712;&#36857;&#20449;&#24687;&#30340;&#26032;&#30340;&#27867;&#21270;&#19978;&#30028;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27867;&#21270;&#19978;&#30028;&#20381;&#36182;&#20110;&#23398;&#20064;&#36712;&#36857;&#30340;&#22797;&#26434;&#24615;&#20197;&#21450;&#35757;&#32451;&#38598;&#30340;&#20559;&#32622;&#21644;&#22810;&#26679;&#24615;&#27604;&#20043;&#38388;&#30340;&#27604;&#29575;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#19981;&#21516;&#35757;&#32451;&#27493;&#39588;&#12289;&#23398;&#20064;&#29575;&#21644;&#26631;&#31614;&#22122;&#22768;&#27700;&#24179;&#19979;&#30340;&#27867;&#21270;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
The aim of this paper is to investigate the connection between learning trajectories of the Deep Neural Networks (DNNs) and their corresponding generalization capabilities when being optimized with broadly used gradient descent and stochastic gradient descent algorithms. In this paper, we construct Linear Approximation Function to model the trajectory information and we propose a new generalization bound with richer trajectory information based on it. Our proposed generalization bound relies on the complexity of learning trajectory and the ratio between the bias and diversity of training set. Experimental results indicate that the proposed method effectively captures the generalization trend across various training steps, learning rates, and label noise levels.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#30495;&#30456;&#21457;&#29616;&#31639;&#27861;&#30340;&#20559;&#24046;&#21644;&#20844;&#24179;&#24615;&#65292;&#21457;&#29616;&#23384;&#22312;&#25935;&#24863;&#23646;&#24615;&#20559;&#35265;&#21644;&#20027;&#35266;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2304.12573</link><description>&lt;p&gt;
&#22522;&#20110;&#30495;&#30456;&#21457;&#29616;&#31639;&#27861;&#30340;&#20844;&#24179;&#24615;&#21644;&#20559;&#24046;&#65306;&#23454;&#39564;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Fairness and Bias in Truth Discovery Algorithms: An Experimental Analysis. (arXiv:2304.12573v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12573
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#30495;&#30456;&#21457;&#29616;&#31639;&#27861;&#30340;&#20559;&#24046;&#21644;&#20844;&#24179;&#24615;&#65292;&#21457;&#29616;&#23384;&#22312;&#25935;&#24863;&#23646;&#24615;&#20559;&#35265;&#21644;&#20027;&#35266;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#35768;&#22810;&#20855;&#26377;&#31038;&#20250;&#24433;&#21709;&#30340;&#24212;&#29992;&#20013;&#24471;&#21040;&#36234;&#26469;&#36234;&#24191;&#27867;&#30340;&#36816;&#29992;&#12290;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#26631;&#35760;&#25968;&#25454;&#65292;&#32780;&#20247;&#21253;&#26159;&#33719;&#21462;&#26469;&#33258;&#22810;&#20010;&#24037;&#20316;&#32773;&#30340;&#26631;&#35760;&#30340;&#20027;&#35201;&#33539;&#20363;&#12290;&#20247;&#21253;&#24037;&#20316;&#32773;&#26377;&#26102;&#20250;&#25552;&#20379;&#19981;&#21487;&#38752;&#30340;&#26631;&#35760;&#65292;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20250;&#20351;&#29992;&#30495;&#30456;&#21457;&#29616;&#31639;&#27861;&#65288;&#20363;&#22914;&#22810;&#25968;&#34920;&#20915;&#65289;&#26469;&#30830;&#23450;&#26469;&#33258;&#20914;&#31361;&#24037;&#20316;&#32773;&#21709;&#24212;&#30340;&#20849;&#35782;&#26631;&#35760;&#12290;&#20294;&#26159;&#65292;&#38656;&#35201;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#20123;&#20849;&#35782;&#26631;&#31614;&#21487;&#33021;&#20173;&#22522;&#20110;&#25935;&#24863;&#23646;&#24615;&#65288;&#22914;&#24615;&#21035;&#12289;&#31181;&#26063;&#25110;&#25919;&#27835;&#27966;&#21035;&#65289;&#32780;&#23384;&#22312;&#20559;&#35265;&#12290;&#21363;&#20351;&#27809;&#26377;&#28041;&#21450;&#25935;&#24863;&#23646;&#24615;&#65292;&#30001;&#20110;&#20027;&#35266;&#26041;&#38754;&#30340;&#19981;&#21516;&#35266;&#28857;&#65292;&#26631;&#31614;&#20063;&#21487;&#33021;&#24102;&#26377;&#20559;&#35265;&#65292;&#20363;&#22914;&#27602;&#24615;&#31561;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#30495;&#30456;&#21457;&#29616;&#31639;&#27861;&#30340;&#20559;&#24046;&#21644;&#20844;&#24179;&#24615;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#12290;&#25105;&#20204;&#20351;&#29992;&#20004;&#20010;&#29616;&#26377;&#30340;&#20247;&#21253;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#21457;&#29616;&#19968;&#23450;&#27604;&#20363;&#30340;&#24037;&#20316;&#32773;&#25552;&#20379;&#20102;&#26377;&#20559;&#35265;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) based approaches are increasingly being used in a number of applications with societal impact. Training ML models often require vast amounts of labeled data, and crowdsourcing is a dominant paradigm for obtaining labels from multiple workers. Crowd workers may sometimes provide unreliable labels, and to address this, truth discovery (TD) algorithms such as majority voting are applied to determine the consensus labels from conflicting worker responses. However, it is important to note that these consensus labels may still be biased based on sensitive attributes such as gender, race, or political affiliation. Even when sensitive attributes are not involved, the labels can be biased due to different perspectives of subjective aspects such as toxicity. In this paper, we conduct a systematic study of the bias and fairness of TD algorithms. Our findings using two existing crowd-labeled datasets, reveal that a non-trivial proportion of workers provide biased results, and
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#19988;&#39640;&#25928;&#30340;&#24615;&#33021;&#20248;&#21270;&#26041;&#27861;&#65292;&#20351;&#29992;&#22522;&#20110;IR&#32534;&#31243;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#29305;&#23450;&#20219;&#21153;&#30340;&#24615;&#33021;&#20248;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#24120;&#36890;&#29992;&#30340;&#35843;&#33410;&#24182;&#34892;&#20195;&#30721;&#21306;&#22495;&#30340;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2304.12568</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#27169;&#22411;&#24314;&#27169;&#21644;&#24322;&#26500;GNN&#30340;&#24615;&#33021;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Performance Optimization using Multimodal Modeling and Heterogeneous GNN. (arXiv:2304.12568v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12568
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#19988;&#39640;&#25928;&#30340;&#24615;&#33021;&#20248;&#21270;&#26041;&#27861;&#65292;&#20351;&#29992;&#22522;&#20110;IR&#32534;&#31243;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#29305;&#23450;&#20219;&#21153;&#30340;&#24615;&#33021;&#20248;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#24120;&#36890;&#29992;&#30340;&#35843;&#33410;&#24182;&#34892;&#20195;&#30721;&#21306;&#22495;&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#24615;&#33021;&#35745;&#31639;&#26550;&#26500;&#20013;&#30340;&#24322;&#26500;&#24615;&#21644;&#21487;&#37197;&#32622;&#24615;&#19981;&#26029;&#22686;&#38271;&#65292;&#20351;&#24471;&#22312;&#36825;&#20123;&#31995;&#32479;&#19978;&#36827;&#34892;&#33258;&#21160;&#35843;&#20248;&#21644;&#36816;&#34892;&#26102;&#21442;&#25968;&#37197;&#32622;&#21464;&#24471;&#38750;&#24120;&#22797;&#26434;&#12290;&#20026;&#20102;&#32553;&#30701;&#36798;&#21040;&#26368;&#20339;&#37197;&#32622;&#30340;&#26102;&#38388;&#65292;&#38500;&#20102;&#37319;&#29992;&#38754;&#21521;&#29305;&#23450;&#24212;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#22806;&#65292;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#36890;&#29992;&#25628;&#32034;&#31574;&#30053;&#65292;&#20294;&#24448;&#24448;&#19981;&#33021;&#25214;&#21040;&#26368;&#20339;&#30340;&#37197;&#32622;&#25110;&#20854;&#25910;&#25947;&#25152;&#38656;&#26102;&#38388;&#22826;&#38271;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#19968;&#31181;&#36890;&#29992;&#19988;&#39640;&#25928;&#30340;&#35843;&#20248;&#26041;&#27861;&#65292;&#21487;&#36731;&#26494;&#25193;&#23637;&#21644;&#36866;&#24212;&#22810;&#31181;&#35843;&#20248;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35843;&#33410;&#24182;&#34892;&#20195;&#30721;&#21306;&#22495;&#30340;&#25216;&#26415;&#65292;&#20854;&#36275;&#22815;&#36890;&#29992;&#65292;&#21487;&#36866;&#24212;&#20110;&#22810;&#20010;&#20219;&#21153;&#12290;&#25105;&#20204;&#20998;&#26512;&#22522;&#20110;IR&#30340;&#32534;&#31243;&#27169;&#22411;&#65292;&#20197;&#36827;&#34892;&#29305;&#23450;&#20219;&#21153;&#30340;&#24615;&#33021;&#20248;&#21270;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#30340;&#22810;&#27169;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#33258;&#32534;&#30721;&#22120;&#65288;MGA&#65289;&#35843;&#35856;&#22120;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#21435;&#22122;&#33258;&#32534;&#30721;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Growing heterogeneity and configurability in HPC architectures has made auto-tuning applications and runtime parameters on these systems very complex. Users are presented with a multitude of options to configure parameters. In addition to application specific solutions, a common approach is to use general purpose search strategies, which often might not identify the best configurations or their time to convergence is a significant barrier. There is, thus, a need for a general purpose and efficient tuning approach that can be easily scaled and adapted to various tuning tasks. We propose a technique for tuning parallel code regions that is general enough to be adapted to multiple tasks. In this paper, we analyze IR-based programming models to make task-specific performance optimizations. To this end, we propose the Multimodal Graph Neural Network and Autoencoder (MGA) tuner, a multimodal deep learning based approach that adapts Heterogeneous Graph Neural Networks and Denoizing Autoencode
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#22686;&#21152;&#36741;&#21161;&#20219;&#21153;&#30340;&#25968;&#37327;&#21644;&#20195;&#29702;&#32593;&#32476;&#30340;&#22823;&#23567;&#65292;&#25552;&#39640;&#34920;&#31034;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#22522;&#20110;&#21518;&#32487;&#24230;&#37327;&#30340;&#26032;&#22411;&#36741;&#21161;&#20219;&#21153;&#23478;&#26063; Proto-Value &#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2304.12567</link><description>&lt;p&gt;
Proto-Value Networks: &#36890;&#36807;&#36741;&#21161;&#20219;&#21153;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#30340;&#21487;&#25193;&#23637;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Proto-Value Networks: Scaling Representation Learning with Auxiliary Tasks. (arXiv:2304.12567v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12567
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#22686;&#21152;&#36741;&#21161;&#20219;&#21153;&#30340;&#25968;&#37327;&#21644;&#20195;&#29702;&#32593;&#32476;&#30340;&#22823;&#23567;&#65292;&#25552;&#39640;&#34920;&#31034;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#22522;&#20110;&#21518;&#32487;&#24230;&#37327;&#30340;&#26032;&#22411;&#36741;&#21161;&#20219;&#21153;&#23478;&#26063; Proto-Value &#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36741;&#21161;&#20219;&#21153;&#21487;&#20197;&#25552;&#39640;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#23398;&#21040;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;&#23613;&#31649;&#20854;&#25928;&#26524;&#24050;&#32463;&#34987;&#30456;&#24403;&#20805;&#20998;&#22320;&#29702;&#35770;&#20998;&#26512;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#65292;&#23427;&#20204;&#20027;&#35201;&#34987;&#29992;&#20316;&#20027;&#35201;&#23398;&#20064;&#30446;&#26631;&#30340;&#25903;&#25345;&#65292;&#32780;&#19981;&#26159;&#20316;&#20026;&#34920;&#31034;&#23398;&#20064;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#36741;&#21161;&#20219;&#21153;&#22312;&#23398;&#20064;&#22797;&#26434;&#34920;&#31034;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#37325;&#28857;&#20851;&#27880;&#21516;&#26102;&#22686;&#21152;&#20219;&#21153;&#25968;&#37327;&#21644;&#20195;&#29702;&#32593;&#32476;&#30340;&#22823;&#23567;&#30340;&#35774;&#32622;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21518;&#32487;&#24230;&#37327;&#30340;&#26032;&#22411;&#36741;&#21161;&#20219;&#21153;&#23478;&#26063;&#12290;&#36825;&#20123;&#20219;&#21153;&#26131;&#20110;&#23454;&#29616;&#65292;&#20855;&#26377;&#21560;&#24341;&#20154;&#30340;&#29702;&#35770;&#24615;&#36136;&#12290;&#19982;&#21512;&#36866;&#30340;&#31163;&#32447;&#23398;&#20064;&#35268;&#21017;&#32467;&#21512;&#20351;&#29992;&#65292;&#32467;&#26524;&#26159;&#19968;&#20010;&#34920;&#31034;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#34987;&#29702;&#35299;&#20026; Proto-Value &#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Auxiliary tasks improve the representations learned by deep reinforcement learning agents. Analytically, their effect is reasonably well understood; in practice, however, their primary use remains in support of a main learning objective, rather than as a method for learning representations. This is perhaps surprising given that many auxiliary tasks are defined procedurally, and hence can be treated as an essentially infinite source of information about the environment. Based on this observation, we study the effectiveness of auxiliary tasks for learning rich representations, focusing on the setting where the number of tasks and the size of the agent's network are simultaneously increased. For this purpose, we derive a new family of auxiliary tasks based on the successor measure. These tasks are easy to implement and have appealing theoretical properties. Combined with a suitable off-policy learning rule, the result is a representation learning algorithm that can be understood as extend
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#65288;TTA&#65289;&#26041;&#27861;AdaNPC&#65292;&#36890;&#36807;&#21033;&#29992;&#38750;&#21442;&#25968;&#20998;&#31867;&#22120;&#36827;&#34892;&#24314;&#27169;&#20174;&#32780;&#36991;&#20813;&#20102;&#31163;&#32447;&#30446;&#26631;&#25968;&#25454;&#25110;&#22312;&#25512;&#29702;&#26102;&#20351;&#29992;&#39069;&#22806;&#30340;&#22797;&#26434;&#20248;&#21270;&#36807;&#31243;&#12290;AdaNPC&#20174;&#23384;&#20648;&#22120;&#20013;&#22238;&#39038;&#26368;&#30456;&#20284;&#30340; K &#20010;&#26679;&#26412;&#36827;&#34892;&#25237;&#31080;&#39044;&#27979;&#65292;&#36880;&#28176;&#25913;&#21464;&#23384;&#20648;&#22120;&#20013;&#30340;&#26679;&#26412;&#20998;&#24067;&#20197;&#25552;&#39640;&#27979;&#35797;&#22495;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.12566</link><description>&lt;p&gt;
AdaNPC&#65306;&#25506;&#32034;&#38750;&#21442;&#25968;&#20998;&#31867;&#22120;&#36827;&#34892;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#24615;
&lt;/p&gt;
&lt;p&gt;
AdaNPC: Exploring Non-Parametric Classifier for Test-Time Adaptation. (arXiv:2304.12566v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#65288;TTA&#65289;&#26041;&#27861;AdaNPC&#65292;&#36890;&#36807;&#21033;&#29992;&#38750;&#21442;&#25968;&#20998;&#31867;&#22120;&#36827;&#34892;&#24314;&#27169;&#20174;&#32780;&#36991;&#20813;&#20102;&#31163;&#32447;&#30446;&#26631;&#25968;&#25454;&#25110;&#22312;&#25512;&#29702;&#26102;&#20351;&#29992;&#39069;&#22806;&#30340;&#22797;&#26434;&#20248;&#21270;&#36807;&#31243;&#12290;AdaNPC&#20174;&#23384;&#20648;&#22120;&#20013;&#22238;&#39038;&#26368;&#30456;&#20284;&#30340; K &#20010;&#26679;&#26412;&#36827;&#34892;&#25237;&#31080;&#39044;&#27979;&#65292;&#36880;&#28176;&#25913;&#21464;&#23384;&#20648;&#22120;&#20013;&#30340;&#26679;&#26412;&#20998;&#24067;&#20197;&#25552;&#39640;&#27979;&#35797;&#22495;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26368;&#36817;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#37117;&#38598;&#20013;&#22312;&#24320;&#21457;&#33021;&#22815;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#20998;&#24067;&#30340;&#27169;&#22411;&#19978;&#12290;&#22495;&#36890;&#29992;&#24615;&#65288;DG&#65289;&#24050;&#25104;&#20026;&#21508;&#20010;&#39046;&#22495;&#20013;&#30340;&#20851;&#38190;&#35838;&#39064;&#20043;&#19968;&#12290;&#20960;&#31687;&#25991;&#29486;&#34920;&#26126;&#65292;&#22914;&#26524;&#19981;&#21033;&#29992;&#30446;&#26631;&#22495;&#30340;&#20449;&#24687;&#65292;&#22495;&#36890;&#29992;&#24615;&#21487;&#33021;&#20250;&#21464;&#24471;&#26497;&#20854;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#65288;TTA&#65289;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;TTA&#26041;&#27861;&#38656;&#35201;&#31163;&#32447;&#30446;&#26631;&#25968;&#25454;&#25110;&#22312;&#25512;&#29702;&#38454;&#27573;&#20351;&#29992;&#39069;&#22806;&#30340;&#22797;&#26434;&#20248;&#21270;&#36807;&#31243;&#12290;&#26412;&#25991;&#37319;&#29992;&#38750;&#21442;&#25968;&#20998;&#31867;&#22120;&#36827;&#34892;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#65288;AdaNPC&#65289;&#12290;&#20855;&#20307;&#22320;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26500;&#24314;&#19968;&#20010;&#21253;&#21547;&#29305;&#24449;&#21644;&#26631;&#31614;&#23545;&#30340;&#23384;&#20648;&#22120;&#12290;&#22312;&#25512;&#29702;&#26102;&#65292;&#32473;&#23450;&#19968;&#20010;&#27979;&#35797;&#23454;&#20363;&#65292;AdaNPC&#39318;&#20808;&#20174;&#23384;&#20648;&#22120;&#20013;&#22238;&#39038;K&#20010;&#26368;&#30456;&#20284;&#30340;&#26679;&#26412;&#36827;&#34892;&#25237;&#31080;&#39044;&#27979;&#65292;&#28982;&#21518;&#23558;&#27979;&#35797;&#29305;&#24449;&#21644;&#39044;&#27979;&#26631;&#31614;&#28155;&#21152;&#21040;&#23384;&#20648;&#22120;&#20013;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#23384;&#20648;&#22120;&#20013;&#30340;&#26679;&#26412;&#20998;&#24067;&#21487;&#20197;&#36880;&#28176;&#20174;&#35757;&#32451;&#20998;&#24067;&#21521;&#27979;&#35797;&#20998;&#24067;&#21464;&#21270;&#65292;&#20174;&#32780;&#25552;&#39640;&#27979;&#35797;&#22495;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;TTA&#26041;&#27861;&#65292;&#24182;&#21462;&#24471;&#20102;&#19982;&#23436;&#20840;&#30417;&#30563;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many recent machine learning tasks focus to develop models that can generalize to unseen distributions. Domain generalization (DG) has become one of the key topics in various fields. Several literatures show that DG can be arbitrarily hard without exploiting target domain information. To address this issue, test-time adaptive (TTA) methods are proposed. Existing TTA methods require offline target data or extra sophisticated optimization procedures during the inference stage. In this work, we adopt Non-Parametric Classifier to perform the test-time Adaptation (AdaNPC). In particular, we construct a memory that contains the feature and label pairs from training domains. During inference, given a test instance, AdaNPC first recalls K closed samples from the memory to vote for the prediction, and then the test feature and predicted label are added to the memory. In this way, the sample distribution in the memory can be gradually changed from the training distribution towards the test distr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#23545;&#25239;&#35757;&#32451;&#20013;&#65292;&#36890;&#36807;&#32467;&#21512;&#23545;&#25163;&#21644;&#21453;&#23545;&#25163;(&#24102;&#26377;&#21453;&#23545;&#25163;&#25200;&#21160;&#30340;&#26679;&#26412;)&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20844;&#24179;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#65292;&#22312;&#19968;&#20123;&#29305;&#23450;&#30340;&#23398;&#20064;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.12550</link><description>&lt;p&gt;
&#35757;&#32451;&#20013;&#32467;&#21512;&#23545;&#25163;&#21644;&#21453;&#23545;&#25163;&#12290;
&lt;/p&gt;
&lt;p&gt;
Combining Adversaries with Anti-adversaries in Training. (arXiv:2304.12550v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12550
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#23545;&#25239;&#35757;&#32451;&#20013;&#65292;&#36890;&#36807;&#32467;&#21512;&#23545;&#25163;&#21644;&#21453;&#23545;&#25163;(&#24102;&#26377;&#21453;&#23545;&#25163;&#25200;&#21160;&#30340;&#26679;&#26412;)&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20844;&#24179;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#65292;&#22312;&#19968;&#20123;&#29305;&#23450;&#30340;&#23398;&#20064;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;&#26159;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20581;&#22766;&#24615;&#30340;&#26377;&#25928;&#23398;&#20064;&#25216;&#26415;&#12290;&#26412;&#30740;&#31350;&#22312;&#26356;&#19968;&#33324;&#30340;&#25200;&#21160;&#33539;&#22260;&#19979;&#29702;&#35770;&#19978;&#30740;&#31350;&#20102;&#23545;&#25239;&#35757;&#32451;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#25506;&#32034;&#34920;&#26126;&#65292;&#23558;&#23545;&#25163;&#21644;&#21453;&#23545;&#25163; (&#24102;&#26377;&#21453;&#23545;&#25163;&#25200;&#21160;&#30340;&#26679;&#26412;) &#32467;&#21512;&#22312;&#35757;&#32451;&#20013;&#65292;&#22312;&#19968;&#20123;&#20856;&#22411;&#30340;&#23398;&#20064;&#22330;&#26223; (&#22914;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#21644;&#19981;&#24179;&#34913;&#23398;&#20064;) &#20013;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#23454;&#29616;&#26356;&#22909;&#30340;&#31867;&#21035;&#38388;&#20844;&#24179;&#24615;&#21644;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#65292;&#30456;&#27604;&#20110;&#26631;&#20934;&#23545;&#25239;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial training is an effective learning technique to improve the robustness of deep neural networks. In this study, the influence of adversarial training on deep learning models in terms of fairness, robustness, and generalization is theoretically investigated under more general perturbation scope that different samples can have different perturbation directions (the adversarial and anti-adversarial directions) and varied perturbation bounds. Our theoretical explorations suggest that the combination of adversaries and anti-adversaries (samples with anti-adversarial perturbations) in training can be more effective in achieving better fairness between classes and a better tradeoff between robustness and generalization in some typical learning scenarios (e.g., noisy label learning and imbalance learning) compared with standard adversarial training. On the basis of our theoretical findings, a more general learning objective that combines adversaries and anti-adversaries with varied b
&lt;/p&gt;</description></item><item><title>COUPA&#26159;&#19968;&#20010;&#38754;&#21521;O2O&#26381;&#21153;&#24179;&#21488;&#30340;&#24037;&#19994;&#32423;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#26102;&#38388;&#24863;&#30693;&#20559;&#22909;&#21644;&#20301;&#32622;&#24863;&#30693;&#20559;&#22909;&#26469;&#25552;&#39640;&#25512;&#33616;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.12549</link><description>&lt;p&gt;
COUPA: &#19968;&#31181;&#38754;&#21521;&#22312;&#32447;&#21040;&#32447;&#19979;&#26381;&#21153;&#24179;&#21488;&#30340;&#24037;&#19994;&#32423;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
COUPA: An Industrial Recommender System for Online to Offline Service Platforms. (arXiv:2304.12549v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12549
&lt;/p&gt;
&lt;p&gt;
COUPA&#26159;&#19968;&#20010;&#38754;&#21521;O2O&#26381;&#21153;&#24179;&#21488;&#30340;&#24037;&#19994;&#32423;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#26102;&#38388;&#24863;&#30693;&#20559;&#22909;&#21644;&#20301;&#32622;&#24863;&#30693;&#20559;&#22909;&#26469;&#25552;&#39640;&#25512;&#33616;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#21040;&#32447;&#19979;&#65288;O2O&#65289;&#26381;&#21153;&#24179;&#21488;&#26088;&#22312;&#24110;&#21161;&#29992;&#25143;&#22312;&#26412;&#22320;&#21457;&#29616;&#38646;&#21806;&#26381;&#21153;&#65288;&#20363;&#22914;&#23089;&#20048;&#21644;&#39184;&#39278;&#65289;&#65292;&#36817;&#24180;&#26469;&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#65292;&#36825;&#26497;&#22823;&#22320;&#25361;&#25112;&#20102;&#29616;&#26377;&#30340;&#25512;&#33616;&#31995;&#32479;&#12290;&#26412;&#25991;&#22522;&#20110;&#25903;&#20184;&#23453;&#24179;&#21488;&#30340;&#30495;&#23454;&#25968;&#25454;&#65292;&#38024;&#23545;O2O&#26381;&#21153;&#30340;&#29305;&#27530;&#24773;&#26223;&#21457;&#29616;&#20102;&#36882;&#24402;&#22522;&#30784;&#19978;&#30340;&#26102;&#38388;&#27169;&#24335;&#21644;&#20301;&#32622;&#20559;&#32622;&#26222;&#36941;&#23384;&#22312;&#65292;&#20005;&#37325;&#24433;&#21709;&#20102;&#25512;&#33616;&#25928;&#26524;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;COUPA&#65292;&#36825;&#26159;&#19968;&#20010;&#24037;&#19994;&#32423;&#31995;&#32479;&#65292;&#26088;&#22312;&#36890;&#36807;&#32771;&#34385;&#20197;&#19979;&#20004;&#20010;&#26041;&#38754;&#26469;&#34920;&#24449;&#29992;&#25143;&#20559;&#22909;&#65306;(1)&#26102;&#38388;&#24863;&#30693;&#20559;&#22909;&#65306;&#25105;&#20204;&#37319;&#29992;&#20102;&#20855;&#26377;&#27880;&#24847;&#26426;&#21046;&#30340;&#36830;&#32493;&#26102;&#38388;&#24863;&#30693;&#28857;&#36807;&#31243;&#65292;&#20197;&#23436;&#20840;&#25429;&#25417;&#25512;&#33616;&#30340;&#26102;&#38388;&#27169;&#24335;&#12290;(2)&#20301;&#32622;&#24863;&#30693;&#20559;&#22909;&#65306;&#19968;&#20010;&#37197;&#22791;&#20102;&#20301;&#32622;&#20010;&#24615;&#21270;&#27169;&#22359;&#30340;&#20301;&#32622;&#36873;&#25321;&#22120;&#32452;&#20214;&#34987;&#31934;&#24515;&#35774;&#35745;&#65292;&#20197;&#20197;&#20010;&#24615;&#21270;&#30340;&#26041;&#24335;&#20943;&#36731;&#20301;&#32622;&#20559;&#24046;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20180;&#32454;&#23454;&#29616;&#24182;&#37096;&#32626;COUPA&#22312;&#25903;&#20184;&#23453;&#24179;&#21488;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aiming at helping users locally discovery retail services (e.g., entertainment and dinning), Online to Offline (O2O) service platforms have become popular in recent years, which greatly challenge current recommender systems. With the real data in Alipay, a feeds-like scenario for O2O services, we find that recurrence based temporal patterns and position biases commonly exist in our scenarios, which seriously threaten the recommendation effectiveness. To this end, we propose COUPA, an industrial system targeting for characterizing user preference with following two considerations: (1) Time aware preference: we employ the continuous time aware point process equipped with an attention mechanism to fully capture temporal patterns for recommendation. (2) Position aware preference: a position selector component equipped with a position personalization module is elaborately designed to mitigate position bias in a personalized manner. Finally, we carefully implement and deploy COUPA on Alipay 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;(PI-INN)&#35299;&#20915;&#36125;&#21494;&#26031;&#21453;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#39640;&#25928;&#22320;&#36827;&#34892;&#25277;&#26679;&#21644;&#20934;&#30830;&#30340;&#23494;&#24230;&#35780;&#20272;&#12290;&#30740;&#31350;&#36890;&#36807;&#27531;&#24046;&#39033;&#21644;&#29420;&#31435;&#24615;&#25439;&#22833;&#39033;&#30830;&#20445;&#20102;INN&#36755;&#20986;&#30340;&#32479;&#35745;&#29420;&#31435;&#24615;&#65292;&#24182;&#22312;&#22810;&#39033;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.12541</link><description>&lt;p&gt;
&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#39640;&#25928;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Efficient Bayesian inference using physics-informed invertible neural networks for inverse problems. (arXiv:2304.12541v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;(PI-INN)&#35299;&#20915;&#36125;&#21494;&#26031;&#21453;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#39640;&#25928;&#22320;&#36827;&#34892;&#25277;&#26679;&#21644;&#20934;&#30830;&#30340;&#23494;&#24230;&#35780;&#20272;&#12290;&#30740;&#31350;&#36890;&#36807;&#27531;&#24046;&#39033;&#21644;&#29420;&#31435;&#24615;&#25439;&#22833;&#39033;&#30830;&#20445;&#20102;INN&#36755;&#20986;&#30340;&#32479;&#35745;&#29420;&#31435;&#24615;&#65292;&#24182;&#22312;&#22810;&#39033;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;(PI-INN)&#35299;&#20915;&#36125;&#21494;&#26031;&#21453;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#12290;PI-INN&#30340;&#32467;&#26500;&#21253;&#25324;&#20004;&#20010;&#23376;&#32593;&#32476;&#65306;&#19968;&#20010;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;(INN)&#21644;&#19968;&#20010;&#31070;&#32463;&#22522;&#30784;&#32593;&#32476;(NB-Net)&#12290;&#36890;&#36807;NB-Net&#24110;&#21161;&#24314;&#31435;&#21442;&#25968;&#36755;&#20837;&#21644;INN&#36755;&#20986;&#20043;&#38388;&#30340;&#21487;&#36870;&#26144;&#23556;&#65292;&#20197;&#25552;&#20379;&#21487;&#34892;&#30340;&#21518;&#39564;&#20998;&#24067;&#20272;&#35745;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#30340;&#25277;&#26679;&#21644;&#20934;&#30830;&#30340;&#23494;&#24230;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;PI-INN&#30340;&#25439;&#22833;&#20989;&#25968;&#21253;&#21547;&#20004;&#20010;&#37096;&#20998;&#65306;&#19968;&#37096;&#20998;&#26159;&#22522;&#20110;&#27531;&#24046;&#30340;&#29289;&#29702;&#20449;&#24687;&#25439;&#22833;&#39033;&#65292;&#21478;&#19968;&#37096;&#20998;&#26159;&#26032;&#30340;&#29420;&#31435;&#24615;&#25439;&#22833;&#39033;&#12290;&#25552;&#20986;&#30340;&#29420;&#31435;&#24615;&#25439;&#22833;&#39033;&#21487;&#20197;&#39640;&#26031;&#21270;&#38543;&#26426;&#28508;&#21464;&#37327;&#65292;&#24182;&#36890;&#36807;&#26377;&#25928;&#21033;&#29992;&#20272;&#35745;&#30340;&#23494;&#24230;&#20989;&#25968;&#65292;&#30830;&#20445;INN&#36755;&#20986;&#30340;&#20004;&#20010;&#37096;&#20998;&#20043;&#38388;&#30340;&#32479;&#35745;&#29420;&#31435;&#24615;&#12290;&#36890;&#36807;&#36827;&#34892;&#21453;&#21521;&#36816;&#21160;&#23398;&#21644;&#21453;&#21521;&#25193;&#25955;&#31561;&#22810;&#39033;&#23454;&#39564;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;PI-INN&#30340;&#26377;&#25928;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the paper, we propose a novel approach for solving Bayesian inverse problems with physics-informed invertible neural networks (PI-INN). The architecture of PI-INN consists of two sub-networks: an invertible neural network (INN) and a neural basis network (NB-Net). The invertible map between the parametric input and the INN output with the aid of NB-Net is constructed to provide a tractable estimation of the posterior distribution, which enables efficient sampling and accurate density evaluation. Furthermore, the loss function of PI-INN includes two components: a residual-based physics-informed loss term and a new independence loss term. The presented independence loss term can Gaussianize the random latent variables and ensure statistical independence between two parts of INN output by effectively utilizing the estimated density function. Several numerical experiments are presented to demonstrate the efficiency and accuracy of the proposed PI-INN, including inverse kinematics, inver
&lt;/p&gt;</description></item><item><title>GARCIA&#21033;&#29992;&#22810;&#31890;&#24230;&#23545;&#27604;&#23398;&#20064;&#65292;&#22522;&#20110;&#22270;&#30340;&#30693;&#35782;&#36716;&#31227;&#21644;&#22522;&#20110;&#24847;&#22270;&#30340;&#34920;&#31034;&#65292;&#25552;&#39640;&#20102;&#38271;&#23614;&#26597;&#35810;&#34920;&#31034;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.12537</link><description>&lt;p&gt;
GARCIA&#65306;&#21033;&#29992;&#22810;&#31890;&#24230;&#23545;&#27604;&#23398;&#20064;&#25552;&#39640;&#38271;&#23614;&#26597;&#35810;&#34920;&#31034;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
GARCIA: Powering Representations of Long-tail Query with Multi-granularity Contrastive Learning. (arXiv:2304.12537v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12537
&lt;/p&gt;
&lt;p&gt;
GARCIA&#21033;&#29992;&#22810;&#31890;&#24230;&#23545;&#27604;&#23398;&#20064;&#65292;&#22522;&#20110;&#22270;&#30340;&#30693;&#35782;&#36716;&#31227;&#21644;&#22522;&#20110;&#24847;&#22270;&#30340;&#34920;&#31034;&#65292;&#25552;&#39640;&#20102;&#38271;&#23614;&#26597;&#35810;&#34920;&#31034;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#26381;&#21153;&#24179;&#21488;&#30340;&#21457;&#23637;&#20026;&#29992;&#25143;&#21644;&#21830;&#23478;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#20415;&#21033;&#65292;&#26381;&#21153;&#25628;&#32034;&#24341;&#25806;&#36890;&#36807;&#25991;&#26412;&#26597;&#35810;&#22312;&#25913;&#21892;&#29992;&#25143;&#20307;&#39564;&#26041;&#38754;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#29992;&#25143;&#30340;&#19981;&#21487;&#25511;&#21046;&#30340;&#25628;&#32034;&#20064;&#24815;&#36890;&#24120;&#24102;&#26469;&#22823;&#37327;&#30340;&#38271;&#23614;&#26597;&#35810;&#65292;&#36825;&#20005;&#37325;&#23041;&#32961;&#21040;&#25628;&#32034;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;GARCIA&#65292;&#21033;&#29992;&#22522;&#20110;&#22270;&#30340;&#30693;&#35782;&#36716;&#31227;&#21644;&#22522;&#20110;&#24847;&#22270;&#30340;&#34920;&#31034;&#26469;&#25552;&#39640;&#38271;&#23614;&#26597;&#35810;&#34920;&#31034;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the growth of service platforms brings great convenience to both users and merchants, where the service search engine plays a vital role in improving the user experience by quickly obtaining desirable results via textual queries. Unfortunately, users' uncontrollable search customs usually bring vast amounts of long-tail queries, which severely threaten the capability of search models. Inspired by recently emerging graph neural networks (GNNs) and contrastive learning (CL), several efforts have been made in alleviating the long-tail issue and achieve considerable performance. Nevertheless, they still face a few major weaknesses. Most importantly, they do not explicitly utilize the contextual structure between heads and tails for effective knowledge transfer, and intention-level information is commonly ignored for more generalized representations.  To this end, we develop a novel framework GARCIA, which exploits the graph based knowledge transfer and intention based representat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;RWSADMM&#65292;&#20197;&#35299;&#20915;&#22312;&#21160;&#24577;&#32852;&#37030;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#25968;&#25454;&#19981;&#19968;&#33268;&#21644;&#36890;&#20449;&#25104;&#26412;&#39640;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.12534</link><description>&lt;p&gt;
&#36890;&#36807;&#38543;&#26426;&#34892;&#36208;&#38543;&#26426;&#20132;&#26367;&#26041;&#21521;&#20056;&#27861;&#31639;&#27861;&#25512;&#21160;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Mobilizing Personalized Federated Learning via Random Walk Stochastic ADMM. (arXiv:2304.12534v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;RWSADMM&#65292;&#20197;&#35299;&#20915;&#22312;&#21160;&#24577;&#32852;&#37030;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#25968;&#25454;&#19981;&#19968;&#33268;&#21644;&#36890;&#20449;&#25104;&#26412;&#39640;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#23454;&#29616;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26102;&#23384;&#22312;&#30340;&#38556;&#30861;&#65292;&#20854;&#20013;&#19981;&#33021;&#32500;&#25252;&#20013;&#22830;&#26381;&#21153;&#22120;&#19982;&#25152;&#26377;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#19968;&#33268;&#36830;&#25509;&#65292;&#24182;&#19988;&#25968;&#25454;&#20998;&#24067;&#26159;&#24322;&#26500;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#21160;&#24577;&#32852;&#37030;&#23398;&#20064;&#65292;&#20854;&#20013;&#26381;&#21153;&#22120;&#22312;&#30456;&#37051;&#23458;&#25143;&#31471;&#32452;&#20043;&#38388;&#31227;&#21160;&#20197;&#23398;&#20064;&#26412;&#22320;&#27169;&#22411;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#21363;&#38543;&#26426;&#34892;&#36208;&#38543;&#26426;&#20132;&#26367;&#26041;&#21521;&#20056;&#27861;&#31639;&#27861;&#65288;RWSADMM&#65289;&#65292;&#21482;&#35201;&#26377;&#36275;&#22815;&#25968;&#37327;&#30340;&#36830;&#25509;&#23458;&#25143;&#31471;&#29992;&#20110;&#27169;&#22411;&#35757;&#32451;&#65292;&#23601;&#33021;&#36866;&#24212;&#21160;&#24577;&#21644;&#21363;&#24109;&#32593;&#32476;&#26465;&#20214;&#12290;&#22312;RWSADMM&#20013;&#65292;&#26381;&#21153;&#22120;&#38543;&#26426;&#21521;&#19968;&#32452;&#23458;&#25143;&#31471;&#34892;&#36208;&#12290;&#23427;&#22522;&#20110;&#30828;&#19981;&#31561;&#24335;&#32422;&#26463;&#24418;&#25104;&#30456;&#37051;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#23616;&#37096;&#36817;&#20284;&#65292;&#32780;&#19981;&#26159;&#37319;&#29992;&#19968;&#33268;&#26356;&#26032;&#26469;&#35299;&#20915;&#25968;&#25454;&#24322;&#26500;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#25910;&#25947;&#30340;&#65292;&#21487;&#20197;&#38477;&#20302;&#36890;&#20449;&#25104;&#26412;&#65292;&#36890;&#36807;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this research, we investigate the barriers associated with implementing Federated Learning (FL) in real-world scenarios, where a consistent connection between the central server and all clients cannot be maintained, and data distribution is heterogeneous. To address these challenges, we focus on mobilizing the federated setting, where the server moves between groups of adjacent clients to learn local models. Specifically, we propose a new algorithm, Random Walk Stochastic Alternating Direction Method of Multipliers (RWSADMM), capable of adapting to dynamic and ad-hoc network conditions as long as a sufficient number of connected clients are available for model training. In RWSADMM, the server walks randomly toward a group of clients. It formulates local proximity among adjacent clients based on hard inequality constraints instead of consensus updates to address data heterogeneity. Our proposed method is convergent, reduces communication costs, and enhances scalability by reducing th
&lt;/p&gt;</description></item><item><title>Patch Diffusion &#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#22522;&#20110;&#22359;&#30340;&#35757;&#32451;&#26694;&#26550;&#65292;&#21487;&#23558;&#25193;&#25955;&#27169;&#22411;&#35757;&#32451;&#26102;&#38388;&#32553;&#30701;&#33267;&#23569;&#19968;&#20493;&#65292;&#24182;&#22312;&#26356;&#23567;&#30340;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.12526</link><description>&lt;p&gt;
Patch Diffusion: &#26356;&#24555;&#26356;&#39640;&#25928;&#30340;&#25193;&#25955;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Patch Diffusion: Faster and More Data-Efficient Training of Diffusion Models. (arXiv:2304.12526v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12526
&lt;/p&gt;
&lt;p&gt;
Patch Diffusion &#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#22522;&#20110;&#22359;&#30340;&#35757;&#32451;&#26694;&#26550;&#65292;&#21487;&#23558;&#25193;&#25955;&#27169;&#22411;&#35757;&#32451;&#26102;&#38388;&#32553;&#30701;&#33267;&#23569;&#19968;&#20493;&#65292;&#24182;&#22312;&#26356;&#23567;&#30340;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#24378;&#22823;&#30340;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#26102;&#38388;&#21644;&#25968;&#25454;&#26469;&#35757;&#32451;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#22522;&#20110;&#22359;&#30340;&#35757;&#32451;&#26694;&#26550; Patch Diffusion&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#35757;&#32451;&#26102;&#38388;&#25104;&#26412;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#25968;&#25454;&#25928;&#29575;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#23558;&#25193;&#25955;&#27169;&#22411;&#30340;&#35757;&#32451;&#25512;&#24191;&#21040;&#26356;&#24191;&#27867;&#30340;&#29992;&#25143;&#12290;&#25105;&#20204;&#30340;&#21019;&#26032;&#26680;&#24515;&#26159;&#26032;&#30340;&#26465;&#20214;&#35780;&#20998;&#20989;&#25968;&#65292;&#23427;&#22312;&#22359;&#32423;&#21035;&#19978;&#36827;&#34892;&#25805;&#20316;&#65292;&#24182;&#23558;&#21407;&#22987;&#22270;&#20687;&#20013;&#30340;&#22359;&#20301;&#32622;&#20316;&#20026;&#38468;&#21152;&#22352;&#26631;&#36890;&#36947;&#65292;&#21516;&#26102;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#38543;&#26426;&#21270;&#21644;&#22810;&#26679;&#21270;&#22359;&#22823;&#23567;&#26469;&#32534;&#30721;&#22810;&#23610;&#24230;&#30340;&#36328;&#21306;&#22495;&#20381;&#36182;&#20851;&#31995;&#12290;&#22312;&#37319;&#26679;&#26041;&#38754;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#21407;&#22987;&#25193;&#25955;&#27169;&#22411;&#19968;&#26679;&#31616;&#21333;&#26131;&#29992;&#12290;&#36890;&#36807; Patch Diffusion&#65292;&#25105;&#20204;&#33021;&#22815;&#23454;&#29616; $\mathbf{\ge 2\times}$ &#26356;&#24555;&#30340;&#35757;&#32451;&#36895;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#21487;&#27604;&#36739;&#25110;&#26356;&#22909;&#30340;&#29983;&#25104;&#36136;&#37327;&#12290;&#21516;&#26102;&#65292;Patch Diffusion &#25552;&#39640;&#20102;&#22312;&#30456;&#23545;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20363;&#22914;&#65292;&#20351;&#29992;&#20165; 5,000 &#24352;&#22270;&#20687;&#36827;&#34892;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models are powerful, but they require a lot of time and data to train. We propose Patch Diffusion, a generic patch-wise training framework, to significantly reduce the training time costs while improving data efficiency, which thus helps democratize diffusion model training to broader users. At the core of our innovations is a new conditional score function at the patch level, where the patch location in the original image is included as additional coordinate channels, while the patch size is randomized and diversified throughout training to encode the cross-region dependency at multiple scales. Sampling with our method is as easy as in the original diffusion model. Through Patch Diffusion, we could achieve $\mathbf{\ge 2\times}$ faster training, while maintaining comparable or better generation quality. Patch Diffusion meanwhile improves the performance of diffusion models trained on relatively small datasets, $e.g.$, as few as 5,000 images to train from scratch. We achieve 
&lt;/p&gt;</description></item><item><title>CIMLA&#26159;&#19968;&#20010;&#26032;&#24037;&#20855;&#65292;&#21487;&#20197;&#21457;&#29616;&#26465;&#20214;&#20381;&#36182;&#30340;&#22240;&#26524;&#20851;&#31995;&#21464;&#21270;&#65292;&#23427;&#21487;&#20197;&#35782;&#21035;&#29983;&#29289;&#26465;&#20214;&#19979;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#30340;&#24046;&#24322;&#65292;&#26159;&#26356;&#21152;&#20934;&#30830;&#21644;&#40065;&#26834;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.12523</link><description>&lt;p&gt;
CIMLA&#65306;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#29992;&#20110;&#25512;&#26029;&#24046;&#24322;&#22240;&#26524;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
CIMLA: Interpretable AI for inference of differential causal networks. (arXiv:2304.12523v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12523
&lt;/p&gt;
&lt;p&gt;
CIMLA&#26159;&#19968;&#20010;&#26032;&#24037;&#20855;&#65292;&#21487;&#20197;&#21457;&#29616;&#26465;&#20214;&#20381;&#36182;&#30340;&#22240;&#26524;&#20851;&#31995;&#21464;&#21270;&#65292;&#23427;&#21487;&#20197;&#35782;&#21035;&#29983;&#29289;&#26465;&#20214;&#19979;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#30340;&#24046;&#24322;&#65292;&#26159;&#26356;&#21152;&#20934;&#30830;&#21644;&#40065;&#26834;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#39640;&#32500;&#25968;&#25454;&#20013;&#21457;&#29616;&#22240;&#26524;&#20851;&#31995;&#26159;&#29983;&#29289;&#20449;&#24687;&#23398;&#20013;&#30340;&#19968;&#20010;&#37325;&#22823;&#38382;&#39064;&#12290;&#26426;&#22120;&#23398;&#20064;&#21644;&#29305;&#24449;&#24402;&#22240;&#27169;&#22411;&#22312;&#36825;&#20010;&#39046;&#22495;&#26174;&#31034;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#32570;&#20047;&#22240;&#26524;&#35299;&#37322;&#12290;&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#19968;&#20010;&#24120;&#35265;&#30340;&#29305;&#24449;&#24402;&#22240;&#27169;&#22411;&#22312;&#26576;&#20123;&#20551;&#35774;&#19979;&#20272;&#35745;&#20102;&#21453;&#26144;&#19968;&#20010;&#21464;&#37327;&#23545;&#21478;&#19968;&#20010;&#21464;&#37327;&#24433;&#21709;&#30340;&#22240;&#26524;&#25968;&#37327;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#35265;&#35299;&#23454;&#29616;&#20102;&#19968;&#20010;&#26032;&#24037;&#20855;CIMLA&#65292;&#29992;&#20110;&#21457;&#29616;&#26465;&#20214;&#20381;&#36182;&#30340;&#22240;&#26524;&#20851;&#31995;&#21464;&#21270;&#65292;&#28982;&#21518;&#20351;&#29992;CIMLA&#35782;&#21035;&#29983;&#29289;&#26465;&#20214;&#19979;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#30340;&#24046;&#24322;&#12290;&#36890;&#36807;&#23545;&#27169;&#25311;&#25968;&#25454;&#38598;&#36827;&#34892;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#34920;&#26126;CIMLA&#27604;&#39046;&#20808;&#30340;&#26041;&#27861;&#26356;&#20855;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36816;&#29992;CIMLA&#20998;&#26512;&#20102;&#19968;&#20010;&#20808;&#21069;&#21457;&#34920;&#30340;&#21333;&#32454;&#32990;RNA-seq&#25968;&#25454;&#38598;&#65292;&#25910;&#38598;&#20102;&#24102;&#26377;&#21644;&#19981;&#24102;&#26377;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65288;AD&#65289;&#30340;&#24739;&#32773;&#25968;&#25454;&#65292;&#24182;&#21457;&#29616;&#20102;&#22522;&#22240;&#19982;AD&#20043;&#38388;&#30340;&#26032;&#28508;&#22312;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
The discovery of causal relationships from high-dimensional data is a major open problem in bioinformatics. Machine learning and feature attribution models have shown great promise in this context but lack causal interpretation. Here, we show that a popular feature attribution model estimates a causal quantity reflecting the influence of one variable on another, under certain assumptions. We leverage this insight to implement a new tool, CIMLA, for discovering condition-dependent changes in causal relationships. We then use CIMLA to identify differences in gene regulatory networks between biological conditions, a problem that has received great attention in recent years. Using extensive benchmarking on simulated data sets, we show that CIMLA is more robust to confounding variables and is more accurate than leading methods. Finally, we employ CIMLA to analyze a previously published single-cell RNA-seq data set collected from subjects with and without Alzheimer's disease (AD), discoverin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#40065;&#26834;&#30456;&#20301;&#24674;&#22797;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#36866;&#24212;&#20572;&#27490;&#20934;&#21017;&#30340;&#38750;&#31934;&#30830;&#36817;&#31471;&#32447;&#24615;&#31639;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2304.12522</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#30340;&#20855;&#26377;&#33258;&#36866;&#24212;&#20572;&#27490;&#20934;&#21017;&#30340;&#38750;&#31934;&#30830;&#36817;&#31471;&#32447;&#24615;&#31639;&#27861;&#65292;&#29992;&#20110;&#40065;&#26834;&#30456;&#20301;&#24674;&#22797;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
A New Inexact Proximal Linear Algorithm with Adaptive Stopping Criteria for Robust Phase Retrieval. (arXiv:2304.12522v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12522
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#40065;&#26834;&#30456;&#20301;&#24674;&#22797;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#36866;&#24212;&#20572;&#27490;&#20934;&#21017;&#30340;&#38750;&#31934;&#30830;&#36817;&#31471;&#32447;&#24615;&#31639;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#40065;&#26834;&#30456;&#20301;&#24674;&#22797;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#21487;&#35270;&#20026;&#19968;&#20010;&#38750;&#20809;&#28369;&#21644;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#31934;&#30830;&#36817;&#31471;&#32447;&#24615;&#31639;&#27861;&#65292;&#20854;&#20013;&#23376;&#38382;&#39064;&#34987;&#19981;&#31934;&#30830;&#27714;&#35299;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26159;&#20026;&#23376;&#38382;&#39064;&#25552;&#20986;&#20102;&#20004;&#31181;&#33258;&#36866;&#24212;&#20572;&#27490;&#20934;&#21017;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#33021;&#12290;&#36890;&#36807;&#23545;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#39640;&#25928;&#65292;&#20363;&#22914;&#21407;&#22987;&#36817;&#31471;&#32447;&#24615;&#31639;&#27861;&#21644;&#27425;&#26799;&#24230;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers the robust phase retrieval problem, which can be cast as a nonsmooth and nonconvex optimization problem. We propose a new inexact proximal linear algorithm with the subproblem being solved inexactly. Our contributions are two adaptive stopping criteria for the subproblem. The convergence behavior of the proposed methods is analyzed. Through experiments on both synthetic and real datasets, we demonstrate that our methods are much more efficient than existing methods, such as the original proximal linear algorithm and the subgradient method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25193;&#25955;&#26041;&#27861;&#8212;&#8212;\textsc{RenderDiffusion}&#65292;&#36890;&#36807;&#25991;&#26412;&#24341;&#23548;&#30340;&#22270;&#20687;&#29983;&#25104;&#36827;&#34892;&#25991;&#26412;&#29983;&#25104;&#12290;&#23427;&#23558;&#36830;&#32493;&#25193;&#25955;&#27169;&#22411;&#24212;&#29992;&#20110;&#31163;&#25955;&#25991;&#26412;&#24182;&#23454;&#29616;&#20102;&#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#20316;&#20026;&#23383;&#24418;&#22270;&#20687;&#29983;&#25104;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.12519</link><description>&lt;p&gt;
RenderDiffusion: &#25991;&#26412;&#29983;&#25104;&#20316;&#20026;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
RenderDiffusion: Text Generation as Image Generation. (arXiv:2304.12519v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12519
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25193;&#25955;&#26041;&#27861;&#8212;&#8212;\textsc{RenderDiffusion}&#65292;&#36890;&#36807;&#25991;&#26412;&#24341;&#23548;&#30340;&#22270;&#20687;&#29983;&#25104;&#36827;&#34892;&#25991;&#26412;&#29983;&#25104;&#12290;&#23427;&#23558;&#36830;&#32493;&#25193;&#25955;&#27169;&#22411;&#24212;&#29992;&#20110;&#31163;&#25955;&#25991;&#26412;&#24182;&#23454;&#29616;&#20102;&#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#20316;&#20026;&#23383;&#24418;&#22270;&#20687;&#29983;&#25104;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#24050;&#25104;&#20026;&#25991;&#26412;&#29983;&#25104;&#30340;&#26032;&#29983;&#25104;&#33539;&#24335;&#12290;&#32771;&#34385;&#21040;&#25991;&#26412;&#30340;&#31163;&#25955;&#20998;&#31867;&#29305;&#24615;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25193;&#25955;&#26041;&#27861;&#8212;&#8212;\textsc{RenderDiffusion}&#65292;&#36890;&#36807;&#25991;&#26412;&#24341;&#23548;&#30340;&#22270;&#20687;&#29983;&#25104;&#36827;&#34892;&#25991;&#26412;&#29983;&#25104;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24605;&#36335;&#26159;&#23558;&#30446;&#26631;&#25991;&#26412;&#21576;&#29616;&#20026;&#21253;&#21547;&#35270;&#35273;&#35821;&#35328;&#20869;&#23481;&#30340;"&#23383;&#24418;&#22270;&#20687;"&#12290;&#36825;&#26679;&#65292;&#26465;&#20214;&#21270;&#30340;&#25991;&#26412;&#29983;&#25104;&#21487;&#20197;&#34987;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#23383;&#24418;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#65292;&#28982;&#21518;&#33258;&#28982;&#22320;&#23558;&#36830;&#32493;&#25193;&#25955;&#27169;&#22411;&#24212;&#29992;&#20110;&#31163;&#25955;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have become a new generative paradigm for text generation. Considering the discrete categorical nature of text, in this paper, we propose \textsc{RenderDiffusion}, a novel diffusion approach for text generation via text-guided image generation. Our key idea is to render the target text as a \emph{glyph image} containing visual language content. In this way, conditional text generation can be cast as a glyph image generation task, and it is then natural to apply continuous diffusion models to discrete texts. Specially, we utilize a cascaded architecture (\ie a base and a super-resolution diffusion model) to generate high-fidelity glyph images, conditioned on the input text. Furthermore, we design a text grounding module to transform and refine the visual language content from generated glyph images into the final texts. In experiments over four conditional text generation tasks and two classes of metrics (\ie quality and diversity), \textsc{RenderDiffusion} can achieve 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; ASAP-Phi&#26694;&#26550;&#65292;&#21033;&#29992;&#20998;&#27573;&#22870;&#21169;&#20989;&#25968;&#26469;&#20026;&#26410;&#28385;&#36275;&#35268;&#33539;&#30340;&#36712;&#36857;&#20998;&#37197;&#23450;&#37327;&#30340;&#35821;&#20041;&#22870;&#21169;&#65292;&#24182;&#20026;&#20854;&#20313;&#36712;&#36857;&#20998;&#37197;&#39640;&#30340;&#24120;&#25968;&#22870;&#21169;&#65292;&#20351;&#29992;&#22522;&#20110;&#28436;&#21592;-&#35780;&#35770;&#23478;&#30340;&#31639;&#27861;&#35757;&#32451;&#20195;&#29702;&#26469;&#23613;&#24555;&#23454;&#29616;&#35268;&#33539;&#12290;</title><link>http://arxiv.org/abs/2304.12508</link><description>&lt;p&gt;
&#36890;&#36807;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#23613;&#24555;&#23454;&#29616;&#27491;&#24335;&#35268;&#33539;
&lt;/p&gt;
&lt;p&gt;
Fulfilling Formal Specifications ASAP by Model-free Reinforcement Learning. (arXiv:2304.12508v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12508
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; ASAP-Phi&#26694;&#26550;&#65292;&#21033;&#29992;&#20998;&#27573;&#22870;&#21169;&#20989;&#25968;&#26469;&#20026;&#26410;&#28385;&#36275;&#35268;&#33539;&#30340;&#36712;&#36857;&#20998;&#37197;&#23450;&#37327;&#30340;&#35821;&#20041;&#22870;&#21169;&#65292;&#24182;&#20026;&#20854;&#20313;&#36712;&#36857;&#20998;&#37197;&#39640;&#30340;&#24120;&#25968;&#22870;&#21169;&#65292;&#20351;&#29992;&#22522;&#20110;&#28436;&#21592;-&#35780;&#35770;&#23478;&#30340;&#31639;&#27861;&#35757;&#32451;&#20195;&#29702;&#26469;&#23613;&#24555;&#23454;&#29616;&#35268;&#33539;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363; ASAP-Phi&#26694;&#26550;&#65292;&#20197;&#40723;&#21169;&#20195;&#29702;&#23613;&#24555;&#28385;&#36275;&#27491;&#24335;&#35268;&#33539;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#19968;&#20010;&#20998;&#27573;&#22870;&#21169;&#20989;&#25968;&#26469;&#20026;&#26410;&#28385;&#36275;&#35268;&#33539;&#30340;&#36712;&#36857;&#20998;&#37197;&#23450;&#37327;&#30340;&#35821;&#20041;&#22870;&#21169;&#65292;&#24182;&#20026;&#20854;&#20313;&#36712;&#36857;&#20998;&#37197;&#39640;&#30340;&#24120;&#25968;&#22870;&#21169;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#22522;&#20110;&#28436;&#21592;-&#35780;&#35770;&#23478;&#30340;&#31639;&#27861;&#65288;&#20363;&#22914;&#36719;&#28436;&#21592;-&#35780;&#35770;&#23478;(SAC)&#25110;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#65288;DDPG&#65289;&#65289;&#35757;&#32451;&#20195;&#29702;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;ASAP-Phi&#29983;&#25104;&#30340;&#31574;&#30053;&#20248;&#20808;&#32771;&#34385;&#23613;&#24555;&#23454;&#29616;&#35268;&#33539;&#12290;&#23545;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#21253;&#25324;&#28040;&#34701;&#30740;&#31350;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#25104;&#21151;&#22320;&#20026;&#22810;&#36798;97&#65285;&#30340;&#27979;&#35797;&#29992;&#20363;&#25214;&#21040;&#20102;&#36275;&#22815;&#24555;&#30340;&#36712;&#36857;&#65292;&#24182;&#20987;&#36133;&#20102;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a model-free reinforcement learning solution, namely the ASAP-Phi framework, to encourage an agent to fulfill a formal specification ASAP. The framework leverages a piece-wise reward function that assigns quantitative semantic reward to traces not satisfying the specification, and a high constant reward to the remaining. Then, it trains an agent with an actor-critic-based algorithm, such as soft actor-critic (SAC), or deep deterministic policy gradient (DDPG). Moreover, we prove that ASAP-Phi produces policies that prioritize fulfilling a specification ASAP. Extensive experiments are run, including ablation studies, on state-of-the-art benchmarks. Results show that our framework succeeds in finding sufficiently fast trajectories for up to 97\% test cases and defeats baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38544;&#20889;&#36741;&#21161;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#38544;&#20889;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#39640;&#38544;&#20889;&#20998;&#26512;&#30340;&#31283;&#20581;&#24615;&#65292;&#24182;&#36890;&#36807;SA-CNN&#30340;&#33258;&#36866;&#24212;&#24615;&#33021;&#22815;&#36827;&#34892;&#26680;&#24515;&#38544;&#20889;&#31639;&#27861;&#30340;&#33258;&#23450;&#20041;&#12290;</title><link>http://arxiv.org/abs/2304.12503</link><description>&lt;p&gt;
&#22522;&#20110;CNN&#30340;&#38544;&#20889;&#26415;--&#23558;&#26426;&#22120;&#23398;&#20064;&#19982;&#20256;&#32479;&#30340;&#38544;&#20889;&#25216;&#26415;&#30456;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
CNN-Assisted Steganography -- Integrating Machine Learning with Established Steganographic Techniques. (arXiv:2304.12503v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12503
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38544;&#20889;&#36741;&#21161;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#38544;&#20889;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#39640;&#38544;&#20889;&#20998;&#26512;&#30340;&#31283;&#20581;&#24615;&#65292;&#24182;&#36890;&#36807;SA-CNN&#30340;&#33258;&#36866;&#24212;&#24615;&#33021;&#22815;&#36827;&#34892;&#26680;&#24515;&#38544;&#20889;&#31639;&#27861;&#30340;&#33258;&#23450;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#22686;&#21152;&#38544;&#20889;&#20998;&#26512;&#20013;&#30340;&#31283;&#20581;&#24615;&#26469;&#25552;&#39640;&#38544;&#20889;&#26415;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#38544;&#20889;&#36741;&#21161;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;SA-CNN&#65289;&#26469;&#22686;&#24378;&#19968;&#31867;&#38544;&#20889;&#26041;&#27861;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#35757;&#32451;&#26377;&#32032;&#30340;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#38544;&#20889;&#20998;&#26512;&#22120;&#26469;&#21457;&#29616;&#38544;&#31192;&#20449;&#24687;&#30340;&#23384;&#22312;&#26159;&#25104;&#21151;&#30340;&#65292;&#32780;&#20351;&#29992;SA-CNN&#22312;&#29983;&#25104;&#38544;&#20889;&#22270;&#20687;&#26102;&#33021;&#20351;&#36825;&#26679;&#30340;&#38544;&#20889;&#20998;&#26512;&#22120;&#30340;&#25928;&#26524;&#19981;&#37027;&#20040;&#26126;&#26174;&#12290;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#23558;SA-CNN&#30340;&#25152;&#26377;&#21487;&#33021;&#36755;&#20986;&#34920;&#31034;&#20026;&#36739;&#23567;&#30340;&#31163;&#25955;&#31354;&#38388;&#32780;&#19981;&#26159;&#36830;&#32493;&#31354;&#38388;&#30340;&#20248;&#32570;&#28857;&#12290;&#25105;&#20204;&#30340;SA-CNN&#20351;&#24471;&#26576;&#20123;&#21442;&#25968;&#21270;&#38544;&#20889;&#31639;&#27861;&#33021;&#22815;&#26681;&#25454;&#35201;&#23884;&#20837;&#20449;&#24687;&#30340;&#36733;&#20307;&#23186;&#20307;&#30340;&#29305;&#24449;&#36827;&#34892;&#23450;&#21046;&#12290;&#22240;&#27492;&#65292;SA-CNN&#26159;&#33258;&#36866;&#24212;&#30340;&#65292;&#23427;&#20351;&#24471;&#26680;&#24515;&#38544;&#20889;&#31639;&#27861;&#33021;&#22815;&#26681;&#25454;&#38544;&#20889;&#36733;&#20307;&#30340;&#29305;&#24615;&#36827;&#34892;&#33258;&#23450;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a method to improve steganography by increasing the resilience of stego-media to discovery through steganalysis. Our approach enhances a class of steganographic approaches through the inclusion of a steganographic assistant convolutional neural network (SA-CNN). Previous research showed success in discovering the presence of hidden information within stego-images using trained neural networks as steganalyzers that are applied to stego-images. Our results show that such steganalyzers are less effective when SA-CNN is employed during the generation of a stego-image. We also explore the advantages and disadvantages of representing all the possible outputs of our SA-CNN within a smaller, discrete space, rather than a continuous space. Our SA-CNN enables certain classes of parametric steganographic algorithms to be customized based on characteristics of the cover media in which information is to be embedded. Thus, SA-CNN is adaptive in the sense that it enables the core steganogr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#35821;&#20041;&#36890;&#20449;&#65288;CSC&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#27169;&#20223;&#23398;&#20064;&#65288;IL&#65289;&#26041;&#27861;&#65292;&#22312;&#25968;&#23383;&#23402;&#29983;&#65288;DT&#65289;&#26080;&#32447;&#31995;&#32479;&#19978;&#21033;&#29992;AI&#25216;&#26415;&#21644;&#35821;&#20041;&#36890;&#20449;&#65288;SC&#65289;&#65292;&#20197;&#25552;&#39640;&#36890;&#20449;&#25928;&#29575;&#12289;&#20915;&#31574;&#33853;&#22320;&#25928;&#26524;&#12289;&#25512;&#21160;&#25968;&#23383;&#23402;&#29983;&#24212;&#29992;&#30340;&#26222;&#21450;&#21644;&#23436;&#21892;&#12290;</title><link>http://arxiv.org/abs/2304.12502</link><description>&lt;p&gt;
&#25968;&#23383;&#23402;&#29983;&#30340;&#22240;&#26524;&#35821;&#20041;&#36890;&#20449;&#65306;&#19968;&#31181;&#36890;&#29992;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Causal Semantic Communication for Digital Twins: A Generalizable Imitation Learning Approach. (arXiv:2304.12502v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#35821;&#20041;&#36890;&#20449;&#65288;CSC&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#27169;&#20223;&#23398;&#20064;&#65288;IL&#65289;&#26041;&#27861;&#65292;&#22312;&#25968;&#23383;&#23402;&#29983;&#65288;DT&#65289;&#26080;&#32447;&#31995;&#32479;&#19978;&#21033;&#29992;AI&#25216;&#26415;&#21644;&#35821;&#20041;&#36890;&#20449;&#65288;SC&#65289;&#65292;&#20197;&#25552;&#39640;&#36890;&#20449;&#25928;&#29575;&#12289;&#20915;&#31574;&#33853;&#22320;&#25928;&#26524;&#12289;&#25512;&#21160;&#25968;&#23383;&#23402;&#29983;&#24212;&#29992;&#30340;&#26222;&#21450;&#21644;&#23436;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#23402;&#29983;&#65288;DT&#65289;&#21033;&#29992;&#34394;&#25311;&#19990;&#30028;&#30340;&#34920;&#31034;&#65292;&#20197;&#21450;&#36890;&#20449;&#65288;&#20363;&#22914;6G&#65289;&#12289;&#35745;&#31639;&#65288;&#20363;&#22914;&#36793;&#32536;&#35745;&#31639;&#65289;&#21644;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#35768;&#22810;&#36830;&#25509;&#30340;&#26234;&#33021;&#26381;&#21153;&#12290;&#20026;&#20102;&#22788;&#29702;&#22522;&#20110;&#25968;&#23383;&#23402;&#29983;&#65288;DT&#65289;&#30340;&#22823;&#37327;&#32593;&#32476;&#25968;&#25454;&#65292;&#26080;&#32447;&#31995;&#32479;&#21487;&#20197;&#21033;&#29992;&#35821;&#20041;&#36890;&#20449;&#65288;SC&#65289;&#33539;&#20363;&#65292;&#36890;&#36807;&#21033;&#29992;AI&#25216;&#26415;&#65288;&#22914;&#22240;&#26524;&#25512;&#26029;&#65289;&#20419;&#36827;&#30693;&#24773;&#20915;&#31574;&#26469;&#22788;&#29702;&#20005;&#26684;&#30340;&#36890;&#20449;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22240;&#26524;&#35821;&#20041;&#36890;&#20449;&#65288;CSC&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#22522;&#20110;&#25968;&#23383;&#23402;&#29983;&#30340;&#26080;&#32447;&#31995;&#32479;&#12290;CSC&#31995;&#32479;&#34987;&#25552;&#20986;&#20316;&#20026;&#27169;&#20223;&#23398;&#20064;&#65288;IL&#65289;&#38382;&#39064;&#65292;&#20854;&#20013;&#21457;&#23556;&#22120;&#36890;&#36807;&#20351;&#29992;DT&#35775;&#38382;&#26368;&#20248;&#32593;&#32476;&#25511;&#21046;&#31574;&#30053;&#26469;&#25945;&#25480;&#25509;&#25910;&#22120;&#22914;&#20309;&#22312;&#24102;&#23485;&#26377;&#38480;&#30340;&#26080;&#32447;&#36890;&#36947;&#19978;&#20351;&#29992;SC&#26469;&#25552;&#39640;&#20854;&#30693;&#35782;&#20197;&#25191;&#34892;&#26368;&#20248;&#25511;&#21046;&#25805;&#20316;&#12290;&#28304;&#25968;&#25454;&#20013;&#30340;&#22240;&#26524;&#32467;&#26500;&#20351;&#29992;&#26032;&#25216;&#26415;&#25552;&#21462;&#20986;&#26469;&#65292;&#20351;&#24471;&#25945;&#23398;&#26694;&#26550;&#26356;&#21152;&#36890;&#29992;&#19988;&#20855;&#26377;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
A digital twin (DT) leverages a virtual representation of the physical world, along with communication (e.g., 6G), computing (e.g., edge computing), and artificial intelligence (AI) technologies to enable many connected intelligence services. In order to handle the large amounts of network data based on digital twins (DTs), wireless systems can exploit the paradigm of semantic communication (SC) for facilitating informed decision-making under strict communication constraints by utilizing AI techniques such as causal reasoning. In this paper, a novel framework called causal semantic communication (CSC) is proposed for DT-based wireless systems. The CSC system is posed as an imitation learning (IL) problem, where the transmitter, with access to optimal network control policies using a DT, teaches the receiver using SC over a bandwidth limited wireless channel how to improve its knowledge to perform optimal control actions. The causal structure in the source data is extracted using novel 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#23558;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#21644;&#24352;&#37327;&#32593;&#32476;&#24212;&#29992;&#20110;&#32929;&#31080;&#25910;&#30410;&#39044;&#27979;&#65292;&#22312;&#26085;&#26412;&#32929;&#24066;&#20013;&#24352;&#37327;&#32593;&#32476;&#27169;&#22411;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#27169;&#22411;&#65292;&#24182;&#22312;&#26368;&#26032;&#24066;&#22330;&#29615;&#22659;&#19979;&#21576;&#29616;&#20986;&#21331;&#36234;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.12501</link><description>&lt;p&gt;
&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#21644;&#24352;&#37327;&#32593;&#32476;&#22312;&#25130;&#38754;&#32929;&#31080;&#25910;&#30410;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
The cross-sectional stock return predictions via quantum neural network and tensor network. (arXiv:2304.12501v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12501
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#23558;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#21644;&#24352;&#37327;&#32593;&#32476;&#24212;&#29992;&#20110;&#32929;&#31080;&#25910;&#30410;&#39044;&#27979;&#65292;&#22312;&#26085;&#26412;&#32929;&#24066;&#20013;&#24352;&#37327;&#32593;&#32476;&#27169;&#22411;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#27169;&#22411;&#65292;&#24182;&#22312;&#26368;&#26032;&#24066;&#22330;&#29615;&#22659;&#19979;&#21576;&#29616;&#20986;&#21331;&#36234;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#37327;&#23376;&#21644;&#37327;&#23376;&#21551;&#21457;&#24335;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#32929;&#31080;&#25910;&#30410;&#39044;&#27979;&#30340;&#24212;&#29992;&#12290;&#20854;&#20013;&#65292;&#25105;&#20204;&#23558;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65288;&#19968;&#31181;&#36866;&#29992;&#20110;&#22122;&#22768;&#20013;&#31561;&#35268;&#27169;&#37327;&#23376;&#35745;&#31639;&#26426;&#30340;&#31639;&#27861;&#65289;&#21644;&#24352;&#37327;&#32593;&#32476;&#65288;&#19968;&#31181;&#21463;&#37327;&#23376;&#21551;&#21457;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65289;&#30340;&#24615;&#33021;&#19982;&#20256;&#32479;&#27169;&#22411;&#22914;&#32447;&#24615;&#22238;&#24402;&#21644;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#27604;&#36739;&#12290;&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;&#27169;&#22411;&#39044;&#27979;&#30340;&#25237;&#36164;&#32452;&#21512;&#24182;&#27979;&#37327;&#25237;&#36164;&#32489;&#25928;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#26085;&#26412;&#32929;&#24066;&#20013;&#65292;&#24352;&#37327;&#32593;&#32476;&#27169;&#22411;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#22522;&#20934;&#27169;&#22411;&#65288;&#21253;&#25324;&#32447;&#24615;&#21644;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65289;&#12290;&#34429;&#28982;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#25972;&#20010;&#21608;&#26399;&#20869;&#20855;&#26377;&#38477;&#20302;&#39118;&#38505;&#35843;&#25972;&#36229;&#39069;&#25910;&#30410;&#30340;&#33021;&#21147;&#65292;&#20294;&#26368;&#26032;&#30340;&#24066;&#22330;&#29615;&#22659;&#19979;&#65292;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#21644;&#24352;&#37327;&#32593;&#32476;&#27169;&#22411;&#22343;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we investigate the application of quantum and quantum-inspired machine learning algorithms to stock return predictions. Specifically, we evaluate performance of quantum neural network, an algorithm suited for noisy intermediate-scale quantum computers, and tensor network, a quantum-inspired machine learning algorithm, against classical models such as linear regression and neural networks. To evaluate their abilities, we construct portfolios based on their predictions and measure investment performances. The empirical study on the Japanese stock market shows the tensor network model achieves superior performance compared to classical benchmark models, including linear and neural network models. Though the quantum neural network model attains the lowered risk-adjusted excess return than the classical neural network models over the whole period, both the quantum neural network and tensor network models have superior performances in the latest market environment, which sugges
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20013;&#65292;&#20316;&#32773;&#36890;&#36807;&#20248;&#21270;Tensil AI&#30340;&#24320;&#28304;&#25512;&#29702;&#21152;&#36895;&#22120;&#24182;&#20351;&#29992;&#39640;&#32423;&#32534;&#35793;&#22120;&#31574;&#30053;&#65292;&#25913;&#36827;&#30828;&#20214;&#35774;&#35745;&#24182;&#20351;&#29992;Xilinx Ultra RAM&#65292;&#34920;&#26126;&#20248;&#21270;FPGA&#21487;&#25552;&#39640;&#25512;&#29702;&#24615;&#33021;&#65292;&#20026;FPGA&#22312;&#39640;&#24615;&#33021;&#35745;&#31639;&#39046;&#22495;&#30340;&#24212;&#29992;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;</title><link>http://arxiv.org/abs/2304.12474</link><description>&lt;p&gt;
&#20351;&#29992;FPGA&#36827;&#34892;&#39640;&#24615;&#33021;&#35745;&#31639;&#30340;&#35774;&#35745;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Design optimization for high-performance computing using FPGA. (arXiv:2304.12474v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12474
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#65292;&#20316;&#32773;&#36890;&#36807;&#20248;&#21270;Tensil AI&#30340;&#24320;&#28304;&#25512;&#29702;&#21152;&#36895;&#22120;&#24182;&#20351;&#29992;&#39640;&#32423;&#32534;&#35793;&#22120;&#31574;&#30053;&#65292;&#25913;&#36827;&#30828;&#20214;&#35774;&#35745;&#24182;&#20351;&#29992;Xilinx Ultra RAM&#65292;&#34920;&#26126;&#20248;&#21270;FPGA&#21487;&#25552;&#39640;&#25512;&#29702;&#24615;&#33021;&#65292;&#20026;FPGA&#22312;&#39640;&#24615;&#33021;&#35745;&#31639;&#39046;&#22495;&#30340;&#24212;&#29992;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#29420;&#29305;&#30340;&#28789;&#27963;&#24615;&#12289;&#24615;&#33021;&#21644;&#21151;&#29575;&#25928;&#29575;&#32467;&#21512;&#65292;&#21487;&#37325;&#26500;&#26550;&#26500;&#22914;&#21487;&#32534;&#31243;&#38376;&#38453;&#21015;&#65288;FPGA&#65289;&#24050;&#32463;&#34987;&#29992;&#20110;&#35768;&#22810;&#39046;&#22495;&#30340;&#21152;&#36895;&#35745;&#31639;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#32534;&#31243;&#22797;&#26434;&#24615;&#21644;&#20248;&#21270;&#24615;&#33021;&#30340;&#22256;&#38590;&#65292;FPGA&#23578;&#26410;&#34987;&#24191;&#27867;&#29992;&#20110;&#39640;&#24615;&#33021;&#35745;&#31639;&#12290;&#26412;&#25991;&#36890;&#36807;&#20248;&#21270;Tensil AI&#30340;&#24320;&#28304;&#25512;&#29702;&#21152;&#36895;&#22120;&#65292;&#20351;&#29992;&#22312;CIFAR&#19978;&#35757;&#32451;&#30340;ResNet20&#23454;&#29616;&#26368;&#22823;&#24615;&#33021;&#65292;&#20197;&#20415;&#28145;&#20837;&#20102;&#35299;&#20351;&#29992;FPGA&#36827;&#34892;&#39640;&#24615;&#33021;&#35745;&#31639;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#25913;&#36827;&#30828;&#20214;&#35774;&#35745;&#65292;&#20351;&#29992;Xilinx Ultra RAM&#21644;&#20351;&#29992;&#20808;&#36827;&#30340;&#32534;&#35793;&#22120;&#31574;&#30053;&#21487;&#20197;&#25552;&#39640;&#25512;&#29702;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#22312;&#20174;&#21407;&#22987;32&#20301;&#28014;&#28857;&#25968;&#21521;&#19979;&#33293;&#20837;&#26102;&#36816;&#34892;CIFAR&#27979;&#35797;&#25968;&#25454;&#38598;&#26102;&#20960;&#20046;&#27809;&#26377;&#20934;&#30830;&#24615;&#19979;&#38477;&#12290;&#25105;&#20204;&#24179;&#21488;&#20013;&#30340;&#24322;&#26500;&#35745;&#31639;&#27169;&#22411;&#20351;&#25105;&#20204;&#33021;&#22815;&#23454;&#29616;293.5&#30340;&#24103;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reconfigurable architectures like Field Programmable Gate Arrays (FPGAs) have been used for accelerating computations in several domains because of their unique combination of flexibility, performance, and power efficiency. However, FPGAs have not been widely used for high-performance computing, primarily because of their programming complexity and difficulties in optimizing performance. We optimize Tensil AI's open-source inference accelerator for maximum performance using ResNet20 trained on CIFAR in this paper in order to gain insight into the use of FPGAs for high-performance computing. In this paper, we show how improving hardware design, using Xilinx Ultra RAM, and using advanced compiler strategies can lead to improved inference performance. We also demonstrate that running the CIFAR test data set shows very little accuracy drop when rounding down from the original 32-bit floating point. The heterogeneous computing model in our platform allows us to achieve a frame rate of 293.5
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#24182;&#35780;&#20272;&#20102;&#19968;&#20010;&#21512;&#25104;&#21040;&#30495;&#23454;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23558;&#21512;&#25104;&#28210;&#26579;&#36716;&#25442;&#20026;&#26356;&#30495;&#23454;&#30340;&#39118;&#26684;&#20197;&#36866;&#29992;&#20110;&#36890;&#29992;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#19979;&#28216;&#24863;&#30693;&#20219;&#21153;&#26469;&#37327;&#21270;&#21644;&#23450;&#24615;&#22320;&#35780;&#20272;&#20854;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.12463</link><description>&lt;p&gt;
&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#21512;&#25104;&#25968;&#25454;&#30340;&#30495;&#23454;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Study on Improving Realism of Synthetic Data for Machine Learning. (arXiv:2304.12463v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12463
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#24182;&#35780;&#20272;&#20102;&#19968;&#20010;&#21512;&#25104;&#21040;&#30495;&#23454;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23558;&#21512;&#25104;&#28210;&#26579;&#36716;&#25442;&#20026;&#26356;&#30495;&#23454;&#30340;&#39118;&#26684;&#20197;&#36866;&#29992;&#20110;&#36890;&#29992;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#19979;&#28216;&#24863;&#30693;&#20219;&#21153;&#26469;&#37327;&#21270;&#21644;&#23450;&#24615;&#22320;&#35780;&#20272;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#24615;&#23398;&#20064;&#30340;&#21512;&#25104;&#21040;&#30495;&#23454;&#25968;&#25454;&#36716;&#25442;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#22823;&#30340;&#25104;&#21151;&#65292;&#20197;&#25913;&#36827;&#21512;&#25104;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20173;&#26377;&#38480;&#30340;&#30740;&#31350;&#19987;&#27880;&#20110;&#28145;&#24230;&#35780;&#20272;&#21644;&#27604;&#36739;&#36890;&#29992;&#26426;&#22120;&#23398;&#20064;&#29992;&#36884;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;&#26412;&#25991;&#26088;&#22312;&#35757;&#32451;&#21644;&#35780;&#20272;&#23558;&#21512;&#25104;&#28210;&#26579;&#36716;&#21270;&#20026;&#26356;&#30495;&#23454;&#39118;&#26684;&#30340;&#21512;&#25104;&#21040;&#30495;&#23454;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#36827;&#34892;&#26222;&#36890;&#30446;&#30340;&#25968;&#25454;&#38598;&#30340;&#26465;&#20214;&#25805;&#20316;&#65292;&#36890;&#36807;&#23450;&#37327;&#21644;&#23450;&#24615;&#30340;&#35780;&#20272;&#25351;&#26631;&#20197;&#21450;&#23450;&#20041;&#19979;&#28216;&#30340;&#24863;&#30693;&#20219;&#21153;&#36827;&#34892;&#24191;&#27867;&#30340;&#24615;&#33021;&#35780;&#20272;&#21644;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthetic-to-real data translation using generative adversarial learning has achieved significant success to improve synthetic data. Yet, there are limited studies focusing on deep evaluation and comparison of adversarial training on general-purpose synthetic data for machine learning. This work aims to train and evaluate a synthetic-to-real generative model that transforms the synthetic renderings into more realistic styles on general-purpose datasets conditioned with unlabeled real-world data. Extensive performance evaluation and comparison have been conducted through qualitative and quantitative metrics, and a defined downstream perception task.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#26234;&#33021;&#20307;MDP&#20013;&#22522;&#20110;&#27010;&#29575;&#20195;&#29702;&#25481;&#32447;&#30340;&#24773;&#20917;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#31639;&#27861;&#65292;&#33021;&#22815;&#28040;&#38500;&#25481;&#32447;&#24773;&#20917;&#38656;&#35201;&#26522;&#20030;&#35745;&#31639;&#30340;&#38480;&#21046;&#65292;&#20174;&#32780;&#23454;&#29616;&#35745;&#31639;&#21518;&#25481;&#32447;&#31995;&#32479;&#30340;&#26368;&#20248;&#31574;&#30053;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2304.12458</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;MDP&#20013;&#22522;&#20110;&#27010;&#29575;&#20195;&#29702;&#25481;&#32447;&#30340;&#26080;&#27169;&#22411;&#23398;&#20064;&#21644;&#26368;&#20248;&#31574;&#30053;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Model-Free Learning and Optimal Policy Design in Multi-Agent MDPs Under Probabilistic Agent Dropout. (arXiv:2304.12458v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12458
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#26234;&#33021;&#20307;MDP&#20013;&#22522;&#20110;&#27010;&#29575;&#20195;&#29702;&#25481;&#32447;&#30340;&#24773;&#20917;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#31639;&#27861;&#65292;&#33021;&#22815;&#28040;&#38500;&#25481;&#32447;&#24773;&#20917;&#38656;&#35201;&#26522;&#20030;&#35745;&#31639;&#30340;&#38480;&#21046;&#65292;&#20174;&#32780;&#23454;&#29616;&#35745;&#31639;&#21518;&#25481;&#32447;&#31995;&#32479;&#30340;&#26368;&#20248;&#31574;&#30053;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#65292;&#35813;&#36807;&#31243;&#21487;&#20197;&#32463;&#21382;&#20195;&#29702;&#25481;&#32447;&#65292;&#24182;&#22522;&#20110;&#23545;&#20110;&#31574;&#30053;&#30340;&#25511;&#21046;&#21644;&#39044;&#20195;&#29702;&#36807;&#31243;&#30340;&#37319;&#26679;&#26469;&#35745;&#31639;&#21518;&#25481;&#32447;&#31995;&#32479;&#30340;&#31574;&#30053;&#12290;&#25511;&#21046;&#22120;&#30340;&#30446;&#26631;&#26159;&#23547;&#25214;&#19968;&#20010;&#26368;&#20248;&#31574;&#30053;&#65292;&#20351;&#24471;&#22312;&#24050;&#30693;&#20195;&#29702;&#25481;&#20986;&#27010;&#29575;&#30340;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#26399;&#26395;&#31995;&#32479;&#30340;&#20215;&#20540;&#26368;&#22823;&#21270;&#12290;&#23545;&#20110;&#20219;&#20309;&#29305;&#23450;&#30340;&#25481;&#32447;&#24773;&#20917;&#19979;&#30340;&#26368;&#20248;&#31574;&#30053;&#26159;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#20010;&#29305;&#20363;&#12290;&#23545;&#20110;&#20855;&#26377;&#29305;&#23450;&#36716;&#25442;&#29420;&#31435;&#24615;&#21644;&#22870;&#21169;&#21487;&#20998;&#24615;&#32467;&#26500;&#30340;MDPs&#65292;&#25105;&#20204;&#20551;&#35774;&#20174;&#31995;&#32479;&#20013;&#31227;&#38500;&#20195;&#29702;&#32452;&#25104;&#20102;&#19968;&#20010;&#26032;&#30340;MDP&#65292;&#30001;&#21097;&#20313;&#20195;&#29702;&#32452;&#25104;&#20855;&#26377;&#26032;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;MDP&#65292;&#36716;&#25442;&#21160;&#24577;&#28040;&#38500;&#24050;&#21024;&#38500;&#30340;&#20195;&#29702;&#65292;&#22870;&#21169;&#19982;&#24050;&#21024;&#38500;&#30340;&#20195;&#29702;&#26080;&#20851;&#12290;&#39318;&#20808;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#36825;&#20123;&#20551;&#35774;&#19979;&#65292;&#23545;&#20110;&#39044;&#25481;&#20986;&#31995;&#32479;&#26399;&#26395;&#20540;&#21487;&#20197;&#36890;&#36807;&#19968;&#20010;&#21333;&#19968;&#30340;MDP&#26469;&#34920;&#31034;&#65307;&#36825;&#20010;&#8220;&#40065;&#26834;MDP&#8221;&#33021;&#22815;&#28040;&#38500;&#22312;&#35745;&#31639;&#26368;&#20248;&#31574;&#30053;&#26102;&#35201;&#35780;&#20272;&#25152;&#26377;$2^N$&#31181;&#20195;&#29702;&#25481;&#32447;&#24773;&#20917;&#30340;&#38656;&#35201;&#12290;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#27169;&#22411;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20351;&#29992;&#33945;&#29305;&#21345;&#32599;&#37319;&#26679;&#21644;&#37325;&#35201;&#24615;&#37319;&#26679;&#26469;&#23398;&#20064;&#40065;&#26834;MDP&#65292;&#20174;&#32780;&#33021;&#22815;&#35745;&#31639;&#21518;&#25481;&#32447;&#31995;&#32479;&#30340;&#26368;&#20248;&#31574;&#30053;&#12290;&#20223;&#30495;&#32467;&#26524;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work studies a multi-agent Markov decision process (MDP) that can undergo agent dropout and the computation of policies for the post-dropout system based on control and sampling of the pre-dropout system. The controller's objective is to find an optimal policy that maximizes the value of the expected system given a priori knowledge of the agents' dropout probabilities. Finding an optimal policy for any specific dropout realization is a special case of this problem. For MDPs with a certain transition independence and reward separability structure, we assume that removing agents from the system forms a new MDP comprised of the remaining agents with new state and action spaces, transition dynamics that marginalize the removed agents, and rewards that are independent of the removed agents. We first show that under these assumptions, the value of the expected post-dropout system can be represented by a single MDP; this "robust MDP" eliminates the need to evaluate all $2^N$ realizations
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Rank Flow Embedding&#31639;&#27861;&#29992;&#20110;&#26080;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#22330;&#26223;&#30340;&#27969;&#24418;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2304.12448</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#27969;&#24418;&#23398;&#20064;&#30340;Rank Flow Embedding
&lt;/p&gt;
&lt;p&gt;
Rank Flow Embedding for Unsupervised and Semi-Supervised Manifold Learning. (arXiv:2304.12448v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12448
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Rank Flow Embedding&#31639;&#27861;&#29992;&#20110;&#26080;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#22330;&#26223;&#30340;&#27969;&#24418;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#36827;&#30340;&#33719;&#21462;&#21644;&#20849;&#20139;&#25216;&#26415;&#20351;&#24471;&#22810;&#23186;&#20307;&#38598;&#21512;&#21450;&#20854;&#24212;&#29992;&#30340;&#22686;&#38271;&#20960;&#20046;&#26080;&#38480;&#65292;&#28982;&#32780;&#65292;&#30001;&#20110;&#26631;&#27880;&#25968;&#25454;&#24448;&#24448;&#26114;&#36149;&#32780;&#36153;&#26102;&#65292;&#22240;&#27492;&#26377;&#30417;&#30563;&#30340;&#35757;&#32451;&#25152;&#38656;&#30340;&#26631;&#27880;&#25968;&#25454;&#21487;&#29992;&#24615;&#30456;&#21453;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Rank Flow Embedding&#65288;RFE&#65289;&#30340;&#26032;&#22411;&#27969;&#24418;&#23398;&#20064;&#31639;&#27861;&#29992;&#20110;&#26080;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#22330;&#26223;&#12290;&#35813;&#31639;&#27861;&#22522;&#20110;&#26368;&#36817;&#27969;&#24418;&#23398;&#20064;&#26041;&#27861;&#21033;&#29992;&#30340;&#24819;&#27861;&#65292;&#21253;&#25324;&#36229;&#22270;&#12289;&#31515;&#21345;&#23572;&#31215;&#21644;&#36830;&#36890;&#25104;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Impressive advances in acquisition and sharing technologies have made the growth of multimedia collections and their applications almost unlimited. However, the opposite is true for the availability of labeled data, which is needed for supervised training, since such data is often expensive and time-consuming to obtain. While there is a pressing need for the development of effective retrieval and classification methods, the difficulties faced by supervised approaches highlight the relevance of methods capable of operating with few or no labeled data. In this work, we propose a novel manifold learning algorithm named Rank Flow Embedding (RFE) for unsupervised and semi-supervised scenarios. The proposed method is based on ideas recently exploited by manifold learning approaches, which include hypergraphs, Cartesian products, and connected components. The algorithm computes context-sensitive embeddings, which are refined following a rank-based processing flow, while complementary contextu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#22788;&#29702;&#24515;&#30005;&#22270;&#20449;&#21495;&#26469;&#26816;&#27979;&#32954;&#21160;&#33033;&#39640;&#21387;&#65292;&#20197;&#23454;&#29616;&#26089;&#26399;&#30340;&#30417;&#27979;&#21644;&#27835;&#30103;&#12290;</title><link>http://arxiv.org/abs/2304.12447</link><description>&lt;p&gt;
&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#36890;&#36807;&#24515;&#30005;&#22270;&#39044;&#27979;&#32954;&#21160;&#33033;&#39640;&#21387;
&lt;/p&gt;
&lt;p&gt;
Predicting Pulmonary Hypertension by Electrocardiograms Using Machine Learning. (arXiv:2304.12447v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12447
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#22788;&#29702;&#24515;&#30005;&#22270;&#20449;&#21495;&#26469;&#26816;&#27979;&#32954;&#21160;&#33033;&#39640;&#21387;&#65292;&#20197;&#23454;&#29616;&#26089;&#26399;&#30340;&#30417;&#27979;&#21644;&#27835;&#30103;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32954;&#21160;&#33033;&#39640;&#21387;&#65288;PH&#65289;&#26159;&#19968;&#31181;&#24433;&#21709;&#32954;&#37096;&#21160;&#33033;&#21644;&#21491;&#24515;&#36793;&#30340;&#39640;&#34880;&#21387;&#30149;&#30151;&#65288;Mayo Clinic&#65292;2017&#65289;&#12290;&#23450;&#20041;&#24179;&#22343;&#32954;&#21160;&#33033;&#21387;&#22823;&#20110;25mmHg&#12290;&#32954;&#21160;&#33033;&#39640;&#21387;&#30340;&#35786;&#26029;&#26102;&#38388;&#20174;&#35786;&#26029;&#26102;&#30340;&#39044;&#35745;5&#24180;&#23384;&#27963;&#29575;&#20165;&#20026;57%&#65292;&#32780;&#21491;&#24515;&#34928;&#31469;&#24739;&#32773;&#22312;&#26410;&#27835;&#30103;&#19979;&#20165;&#23384;&#27963;&#32422;1&#24180;&#65288;Benza&#31561;&#65292;2012&#65289;&#12290; &#37492;&#20110;&#35813;&#30149;&#30340;&#24930;&#24615;&#29305;&#24615;&#65292;&#26089;&#26399;&#26816;&#27979;PH&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#23548;&#33268;&#27835;&#30103;&#24310;&#36831;&#12290;&#36229;&#22768;&#24515;&#21160;&#22270;&#30446;&#21069;&#29992;&#20110;&#35786;&#26029;PH&#30340;&#31579;&#26597;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#19982;&#36229;&#22768;&#24515;&#21160;&#22270;&#30456;&#27604;&#65292;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#26356;&#21152;&#26131;&#20110;&#20351;&#29992;&#21644;&#32463;&#27982;&#23454;&#24800;&#65292;&#20294;&#23545;&#20110;&#31579;&#26597;&#24739;&#26377;PH&#30340;&#39640;&#21361;&#24739;&#32773;&#26469;&#35828;&#25506;&#31350;&#36739;&#23569;&#12290;&#35813;&#39033;&#30446;&#30340;&#30446;&#26631;&#26159;&#21019;&#24314;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#22788;&#29702;ECG&#20449;&#21495;&#24182;&#26816;&#27979;PH&#30340;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pulmonary hypertension (PH) is a condition of high blood pressure that affects the arteries in the lungs and the right side of the heart (Mayo Clinic, 2017). A mean pulmonary artery pressure greater than 25 mmHg is defined as Pulmonary hypertension. The estimated 5-year survival rate from the time of diagnosis of pulmonary hypertension is only 57% without therapy and patients with right heart failure only survive for approximately 1 year without treatment (Benza et al., 2012). Given the indolent nature of the disease, early detection of PH remains a challenge leading to delays in therapy. Echocardiography is currently used as a screening tool for diagnosing PH. However, electrocardiography (ECG), a more accessible, simple to use, and cost-effective tool compared to echocardiography, is less studied and explored for screening at-risk patients for PH. The goal of this project is to create a neural network model which can process an ECG signal and detect the presence of PH with a confiden
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#38656;&#27714;&#39044;&#27979;&#30340;&#38543;&#26426;MPC&#25511;&#21046;&#22120;&#65292;&#37319;&#29992;&#26426;&#20250;&#32422;&#26463;&#26469;&#26368;&#23567;&#21270;&#19981;&#30830;&#23450;&#30340;&#30005;&#21147;&#21644;&#28909;&#38656;&#27714;&#65292;&#20351;&#29992;&#21382;&#21490;&#25968;&#25454;&#26500;&#24314;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#24773;&#26223;&#27861;&#37319;&#26679;&#22810;&#27493;&#38656;&#27714;&#36712;&#36857;&#12290;&#22312;&#27169;&#25311;&#21644;&#23454;&#38469;&#25968;&#25454;&#19978;&#39564;&#35777;&#20102;&#35813;&#25511;&#21046;&#22120;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.12438</link><description>&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#38656;&#27714;&#39044;&#27979;&#30340;&#33021;&#28304;&#20013;&#24515;&#38543;&#26426;MPC
&lt;/p&gt;
&lt;p&gt;
Stochastic MPC for energy hubs using data driven demand forecasting. (arXiv:2304.12438v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12438
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#38656;&#27714;&#39044;&#27979;&#30340;&#38543;&#26426;MPC&#25511;&#21046;&#22120;&#65292;&#37319;&#29992;&#26426;&#20250;&#32422;&#26463;&#26469;&#26368;&#23567;&#21270;&#19981;&#30830;&#23450;&#30340;&#30005;&#21147;&#21644;&#28909;&#38656;&#27714;&#65292;&#20351;&#29992;&#21382;&#21490;&#25968;&#25454;&#26500;&#24314;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#24773;&#26223;&#27861;&#37319;&#26679;&#22810;&#27493;&#38656;&#27714;&#36712;&#36857;&#12290;&#22312;&#27169;&#25311;&#21644;&#23454;&#38469;&#25968;&#25454;&#19978;&#39564;&#35777;&#20102;&#35813;&#25511;&#21046;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#28304;&#20013;&#24515;&#36890;&#36807;&#22810;&#31181;&#36716;&#25442;&#21644;&#20648;&#33021;&#32452;&#20214;&#23558;&#33021;&#28304;&#36164;&#28304;&#36827;&#34892;&#36716;&#25442;&#21644;&#20998;&#37197;&#12290;&#33021;&#28304;&#20013;&#24515;&#30340;&#26368;&#20339;&#36816;&#34892;&#21033;&#29992;&#20854;&#28789;&#27963;&#24615;&#26469;&#22686;&#21152;&#33021;&#28304;&#25928;&#29575;&#24182;&#20943;&#23569;&#36816;&#33829;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#38656;&#27714;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#32473;&#33021;&#28304;&#20013;&#24515;&#30340;&#20248;&#21270;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;MPC&#25511;&#21046;&#22120;&#65292;&#20351;&#29992;&#26426;&#20250;&#32422;&#26463;&#26469;&#26368;&#23567;&#21270;&#19981;&#30830;&#23450;&#30340;&#30005;&#21147;&#21644;&#28909;&#38656;&#27714;&#65292;&#37319;&#29992;&#21382;&#21490;&#25968;&#25454;&#26500;&#24314;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#30340;&#38656;&#27714;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#29983;&#25104;&#26410;&#26469;&#30005;&#21147;&#21644;&#28909;&#38656;&#27714;&#30340;&#39044;&#27979;&#12290;&#36890;&#36807;&#20174;&#39044;&#27979;&#27169;&#22411;&#20013;&#37319;&#26679;&#22810;&#27493;&#38656;&#27714;&#36712;&#36857;&#65292;&#37319;&#29992;&#8220;&#24773;&#26223;&#27861;&#8221;&#35299;&#20915;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#12290;&#22312;&#27169;&#25311;&#33021;&#28304;&#20013;&#24515;&#27169;&#22411;&#21644;&#26469;&#33258;&#23454;&#38469;&#24314;&#31569;&#30340;&#38656;&#27714;&#25968;&#25454;&#19978;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#39044;&#27979;&#22120;&#21644;&#38543;&#26426;&#25511;&#21046;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy hubs convert and distribute energy resources by combining different energy inputs through multiple conversion and storage components. The optimal operation of the energy hub exploits its flexibility to increase the energy efficiency and reduce the operational costs. However, uncertainties in the demand present challenges to energy hub optimization. In this paper, we propose a stochastic MPC controller to minimize energy costs using chance constraints for the uncertain electricity and thermal demands. Historical data is used to build a demand prediction model based on Gaussian processes to generate a forecast of the future electricity and heat demands. The stochastic optimization problem is solved via the Scenario Approach by sampling multi-step demand trajectories from the derived prediction model. The performance of the proposed predictor and of the stochastic controller is verified on a simulated energy hub model and demand data from a real building.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#31070;&#32463;&#36827;&#21270;&#25216;&#26415;&#36827;&#34892;&#34892;&#20026;&#27169;&#20223;&#30340;&#26041;&#27861;&#65292;&#22312;8&#20010;OpenAI Gym&#29366;&#24577;&#25511;&#21046;&#20219;&#21153;&#19978;&#65292;&#26368;&#32456;&#30340;&#20195;&#29702;&#33021;&#22815;&#36798;&#21040;&#19982;&#39044;&#35757;&#32451;&#20195;&#29702;&#19968;&#26679;&#39640;&#30340;&#24471;&#20998;&#65292;&#23637;&#31034;&#20102;&#28145;&#24230;&#31070;&#32463;&#36827;&#21270;&#25216;&#26415;&#22312;&#27169;&#20223;&#23398;&#20064;&#20013;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.12432</link><description>&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#31070;&#32463;&#36827;&#21270;&#29992;&#20110;&#25511;&#21046;&#34892;&#20026;&#27169;&#20223;
&lt;/p&gt;
&lt;p&gt;
Generative Adversarial Neuroevolution for Control Behaviour Imitation. (arXiv:2304.12432v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12432
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#31070;&#32463;&#36827;&#21270;&#25216;&#26415;&#36827;&#34892;&#34892;&#20026;&#27169;&#20223;&#30340;&#26041;&#27861;&#65292;&#22312;8&#20010;OpenAI Gym&#29366;&#24577;&#25511;&#21046;&#20219;&#21153;&#19978;&#65292;&#26368;&#32456;&#30340;&#20195;&#29702;&#33021;&#22815;&#36798;&#21040;&#19982;&#39044;&#35757;&#32451;&#20195;&#29702;&#19968;&#26679;&#39640;&#30340;&#24471;&#20998;&#65292;&#23637;&#31034;&#20102;&#28145;&#24230;&#31070;&#32463;&#36827;&#21270;&#25216;&#26415;&#22312;&#27169;&#20223;&#23398;&#20064;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#20110;&#27169;&#20223;&#23398;&#20064;&#30340;&#20852;&#36259;&#24613;&#21095;&#22686;&#21152;&#65292;&#22823;&#37327;&#30340;&#20154;&#31867;&#35270;&#39057;&#28216;&#25103;&#21644;&#26426;&#22120;&#20154;&#25805;&#20316;&#25968;&#25454;&#38598;&#34987;&#29992;&#26469;&#35757;&#32451;&#20195;&#29702;&#22312;&#38750;&#24120;&#22797;&#26434;&#30340;&#20219;&#21153;&#19978;&#12290;&#34429;&#28982;&#28145;&#24230;&#31070;&#32463;&#36827;&#21270;&#26368;&#36817;&#24050;&#32463;&#26174;&#31034;&#21487;&#20197;&#21305;&#37197;&#22522;&#20110;&#26799;&#24230;&#30340;&#25216;&#26415;&#22312;&#21508;&#31181;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#19978;&#30340;&#24615;&#33021;&#65292;&#20294;&#23558;&#28145;&#24230;&#31070;&#32463;&#36827;&#21270;&#25216;&#26415;&#24212;&#29992;&#20110;&#27169;&#20223;&#23398;&#20064;&#20173;&#30456;&#23545;&#26410;&#34987;&#25506;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#25506;&#32034;&#28145;&#24230;&#31070;&#32463;&#36827;&#21270;&#26159;&#21542;&#21487;&#20197;&#29992;&#20110;&#22312;&#27969;&#34892;&#30340;&#20223;&#30495;&#29615;&#22659;&#20013;&#36827;&#34892;&#34892;&#20026;&#27169;&#20223; &#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#21327;&#21516;&#36827;&#21270;&#30340;&#23545;&#25239;&#29983;&#25104;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#28436;&#21270;&#26631;&#20934;&#28145;&#24230;&#36882;&#24402;&#32593;&#32476;&#26469;&#27169;&#20223;8&#20010;OpenAI Gym&#29366;&#24577;&#25511;&#21046;&#20219;&#21153;&#19978;&#30340;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#20195;&#29702;&#65292;&#24182;&#35780;&#20272;&#20854;&#33021;&#21147;&#12290;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#26368;&#32456;&#30340;&#20248;&#31168;&#25191;&#34892;&#22120;&#20195;&#29702;&#33021;&#22815;&#36798;&#21040;&#19982;&#39044;&#35757;&#32451;&#20195;&#29702;&#33719;&#24471;&#30340;&#24471;&#20998;&#19968;&#26679;&#39640;&#30340;&#24471;&#20998;&#65292;&#21516;&#26102;&#32039;&#23494;&#22320;&#36319;&#38543;&#23427;&#20204;&#30340;&#24471;&#20998;&#36712;&#36857;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a recent surge in interest for imitation learning, with large human video-game and robotic manipulation datasets being used to train agents on very complex tasks. While deep neuroevolution has recently been shown to match the performance of gradient-based techniques on various reinforcement learning problems, the application of deep neuroevolution techniques to imitation learning remains relatively unexplored. In this work, we propose to explore whether deep neuroevolution can be used for behaviour imitation on popular simulation environments. We introduce a simple co-evolutionary adversarial generation framework, and evaluate its capabilities by evolving standard deep recurrent networks to imitate state-of-the-art pre-trained agents on 8 OpenAI Gym state-based control tasks. Across all tasks, we find the final elite actor agents capable of achieving scores as high as those obtained by the pre-trained agents, all the while closely following their score trajectories. Our result
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#23567;&#32452;&#21464;&#24322;&#35268;&#21017;&#21160;&#24577;&#28436;&#21270;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#20219;&#21153;&#19978;&#30340;&#31070;&#32463;&#28436;&#21270;&#12290;</title><link>http://arxiv.org/abs/2304.12431</link><description>&lt;p&gt;
&#8220;&#22312;&#25511;&#21046;&#20219;&#21153;&#19978;&#30340;&#36882;&#24402;&#32467;&#26500;&#30340;&#31070;&#32463;&#28436;&#21270;&#12290;&#8221;
&lt;/p&gt;
&lt;p&gt;
Neuroevolution of Recurrent Architectures on Control Tasks. (arXiv:2304.12431v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12431
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#23567;&#32452;&#21464;&#24322;&#35268;&#21017;&#21160;&#24577;&#28436;&#21270;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#20219;&#21153;&#19978;&#30340;&#31070;&#32463;&#28436;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#20154;&#24037;&#26234;&#33021;&#24037;&#20316;&#36890;&#24120;&#20351;&#29992;&#22522;&#20110;&#28176;&#21464;&#30340;&#20248;&#21270;&#25216;&#26415;&#26469;&#35757;&#32451;&#22266;&#23450;&#22823;&#23567;&#30340;&#28145;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#21442;&#25968;&#12290;&#26368;&#36817;&#24050;&#32463;&#35777;&#26126;&#65292;&#31616;&#21333;&#30340;&#36827;&#21270;&#31639;&#27861;&#20063;&#21487;&#20197;&#20248;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21442;&#25968;&#65292;&#26377;&#26102;&#21487;&#20197;&#21305;&#37197;&#22522;&#20110;&#26799;&#24230;&#30340;&#25216;&#26415;&#22312;&#24378;&#21270;&#23398;&#20064;&#35774;&#32622;&#20013;&#30340;&#24615;&#33021;&#12290;&#38500;&#20102;&#20248;&#21270;&#32593;&#32476;&#21442;&#25968;&#22806;&#65292;&#35768;&#22810;&#36827;&#21270;&#35745;&#31639;&#25216;&#26415;&#36824;&#33021;&#22815;&#36880;&#27493;&#26500;&#24314;&#32593;&#32476;&#26550;&#26500;&#12290;&#28982;&#32780;&#65292;&#20174;&#22522;&#26412;&#36827;&#21270;&#35268;&#21017;&#20013;&#26500;&#24314;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#23578;&#26410;&#26174;&#31034;&#20986;&#22312;&#29616;&#20195;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20854;&#20013;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#26681;&#25454;&#19968;&#23567;&#32452;&#31361;&#21464;&#35268;&#21017;&#21160;&#24577;&#28436;&#21464;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#24182;&#34892;&#30340;&#36827;&#21270;&#31639;&#27861;&#65292;&#24182;&#22312;&#25152;&#26377;19&#20010;OpenAI Gym&#29366;&#24577;&#22522;&#30784;&#30340;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern artificial intelligence works typically train the parameters of fixed-sized deep neural networks using gradient-based optimization techniques. Simple evolutionary algorithms have recently been shown to also be capable of optimizing deep neural network parameters, at times matching the performance of gradient-based techniques, e.g. in reinforcement learning settings. In addition to optimizing network parameters, many evolutionary computation techniques are also capable of progressively constructing network architectures. However, constructing network architectures from elementary evolution rules has not yet been shown to scale to modern reinforcement learning benchmarks. In this paper we therefore propose a new approach in which the architectures of recurrent neural networks dynamically evolve according to a small set of mutation rules. We implement a massively parallel evolutionary algorithm and run experiments on all 19 OpenAI Gym state-based reinforcement learning control task
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#36923;&#36753;&#22238;&#24402;&#30340;&#24046;&#20998;&#38544;&#31169;&#26041;&#27861;&#65292;&#20445;&#25345;&#30828;&#38646;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#38750;&#31169;&#26377;&#30340;LASSO&#36923;&#36753;&#22238;&#24402;&#27169;&#22411;&#20915;&#23450;&#26368;&#21518;&#30340;&#27169;&#22411;&#36873;&#25321;&#20013;&#24688;&#24403;&#30340;&#31169;&#26377;&#21270;&#38750;&#38646;&#31995;&#25968;&#30340;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2304.12429</link><description>&lt;p&gt;
&#31232;&#30095;&#31169;&#26377;LASSO&#36923;&#36753;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Sparse Private LASSO Logistic Regression. (arXiv:2304.12429v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12429
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#36923;&#36753;&#22238;&#24402;&#30340;&#24046;&#20998;&#38544;&#31169;&#26041;&#27861;&#65292;&#20445;&#25345;&#30828;&#38646;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#38750;&#31169;&#26377;&#30340;LASSO&#36923;&#36753;&#22238;&#24402;&#27169;&#22411;&#20915;&#23450;&#26368;&#21518;&#30340;&#27169;&#22411;&#36873;&#25321;&#20013;&#24688;&#24403;&#30340;&#31169;&#26377;&#21270;&#38750;&#38646;&#31995;&#25968;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LASSO&#27491;&#21017;&#21270;&#30340;&#36923;&#36753;&#22238;&#24402;&#22240;&#20854;&#20869;&#32622;&#29305;&#24449;&#36873;&#25321;&#21151;&#33021;&#32780;&#38750;&#24120;&#26377;&#29992;&#65292;&#20801;&#35768;&#20174;&#37096;&#32626;&#20013;&#21024;&#38500;&#31995;&#25968;&#24182;&#20135;&#29983;&#31232;&#30095;&#35299;&#12290;&#34429;&#28982;&#24050;&#32463;&#24320;&#21457;&#20102;LASSO&#36923;&#36753;&#22238;&#24402;&#30340;&#24046;&#20998;&#38544;&#31169;&#29256;&#26412;&#65292;&#20294;&#36890;&#24120;&#20250;&#20135;&#29983;&#23494;&#38598;&#35299;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;LASSO&#24809;&#32602;&#30340;&#20869;&#22312;&#23454;&#29992;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31232;&#30095;&#36923;&#36753;&#22238;&#24402;&#30340;&#24046;&#20998;&#38544;&#31169;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20445;&#25345;&#30828;&#38646;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#39318;&#20808;&#35757;&#32451;&#19968;&#20010;&#38750;&#31169;&#26377;&#30340;LASSO&#36923;&#36753;&#22238;&#24402;&#27169;&#22411;&#26469;&#30830;&#23450;&#22312;&#26368;&#32456;&#27169;&#22411;&#36873;&#25321;&#20013;&#20351;&#29992;&#30340;&#24688;&#24403;&#31169;&#26377;&#21270;&#38750;&#38646;&#31995;&#25968;&#30340;&#25968;&#37327;&#12290;&#20026;&#20102;&#35777;&#26126;&#25105;&#20204;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#23545;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
LASSO regularized logistic regression is particularly useful for its built-in feature selection, allowing coefficients to be removed from deployment and producing sparse solutions. Differentially private versions of LASSO logistic regression have been developed, but generally produce dense solutions, reducing the intrinsic utility of the LASSO penalty. In this paper, we present a differentially private method for sparse logistic regression that maintains hard zeros. Our key insight is to first train a non-private LASSO logistic regression model to determine an appropriate privatized number of non-zero coefficients to use in final model selection. To demonstrate our method's performance, we run experiments on synthetic and real-world datasets.
&lt;/p&gt;</description></item><item><title>TIGTEC&#26159;&#19968;&#20010;&#39640;&#25928;&#30340;&#25991;&#26412;&#23545;&#25239;&#20107;&#20363;&#29983;&#25104;&#26041;&#27861;&#65292;&#20351;&#29992;&#26412;&#22320;&#29305;&#24449;&#37325;&#35201;&#24615;&#38024;&#23545;&#21644;&#20462;&#25913;&#23545;&#20998;&#31867;&#32467;&#26524;&#24433;&#21709;&#26368;&#22823;&#30340;&#21333;&#35789;&#65292;&#24182;&#22312;&#20195;&#20215;&#20989;&#25968;&#20013;&#20351;&#29992;&#35821;&#20041;&#36317;&#31163;&#35780;&#20272;&#23545;&#25239;&#24615;&#35299;&#37322;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#24456;&#39640;&#30340;&#25104;&#21151;&#29575;&#12289;&#31232;&#30095;&#24615;&#12289;&#22810;&#26679;&#24615;&#21644;&#21487;&#20449;&#24230;&#65292;&#24182;&#19988;&#26082;&#21487;&#20197;&#26159;&#29305;&#23450;&#20110;&#27169;&#22411;&#30340;&#65292;&#20063;&#21487;&#20197;&#26159;&#26080;&#20851;&#27169;&#22411;&#30340;&#65292;&#38750;&#24120;&#26041;&#20415;&#29992;&#20110;&#29983;&#25104;&#23545;&#25239;&#24615;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2304.12425</link><description>&lt;p&gt;
TIGTEC&#65306;&#22522;&#20110;&#26631;&#35760;&#37325;&#35201;&#24615;&#30340;&#25991;&#26412;&#23545;&#25239;&#20107;&#20363;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
TIGTEC : Token Importance Guided TExt Counterfactuals. (arXiv:2304.12425v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12425
&lt;/p&gt;
&lt;p&gt;
TIGTEC&#26159;&#19968;&#20010;&#39640;&#25928;&#30340;&#25991;&#26412;&#23545;&#25239;&#20107;&#20363;&#29983;&#25104;&#26041;&#27861;&#65292;&#20351;&#29992;&#26412;&#22320;&#29305;&#24449;&#37325;&#35201;&#24615;&#38024;&#23545;&#21644;&#20462;&#25913;&#23545;&#20998;&#31867;&#32467;&#26524;&#24433;&#21709;&#26368;&#22823;&#30340;&#21333;&#35789;&#65292;&#24182;&#22312;&#20195;&#20215;&#20989;&#25968;&#20013;&#20351;&#29992;&#35821;&#20041;&#36317;&#31163;&#35780;&#20272;&#23545;&#25239;&#24615;&#35299;&#37322;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#24456;&#39640;&#30340;&#25104;&#21151;&#29575;&#12289;&#31232;&#30095;&#24615;&#12289;&#22810;&#26679;&#24615;&#21644;&#21487;&#20449;&#24230;&#65292;&#24182;&#19988;&#26082;&#21487;&#20197;&#26159;&#29305;&#23450;&#20110;&#27169;&#22411;&#30340;&#65292;&#20063;&#21487;&#20197;&#26159;&#26080;&#20851;&#27169;&#22411;&#30340;&#65292;&#38750;&#24120;&#26041;&#20415;&#29992;&#20110;&#29983;&#25104;&#23545;&#25239;&#24615;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#20107;&#20363;&#26159;&#19968;&#31181;&#36890;&#36807;&#25913;&#21464;&#23454;&#20363;&#26469;&#32763;&#36716;&#20998;&#31867;&#22120;&#36755;&#20986;&#20197;&#35299;&#37322;&#27169;&#22411;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;TIGTEC&#65292;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#31232;&#30095;&#12289;&#21487;&#20449;&#21644;&#22810;&#26679;&#24615;&#30340;&#23545;&#25239;&#24615;&#35299;&#37322;&#30340;&#39640;&#25928;&#27169;&#22359;&#26041;&#27861;&#12290;TIGTEC&#26159;&#19968;&#31181;&#25991;&#26412;&#32534;&#36753;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20351;&#29992;&#26412;&#22320;&#29305;&#24449;&#37325;&#35201;&#24615;&#38024;&#23545;&#21644;&#20462;&#25913;&#23545;&#20998;&#31867;&#32467;&#26524;&#24433;&#21709;&#26368;&#22823;&#30340;&#21333;&#35789;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26412;&#22320;&#29305;&#24449;&#37325;&#35201;&#24615;&#65292;&#24182;&#36890;&#36807;&#19968;&#20010;&#38598;&#25104;&#35821;&#20041;&#36317;&#31163;&#30340;&#20195;&#20215;&#20989;&#25968;&#23545;&#29983;&#25104;&#30340;&#23545;&#25239;&#24615;&#35299;&#37322;&#36827;&#34892;&#35780;&#20272;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#27874;&#26463;&#25628;&#32034;&#31639;&#27861;&#36827;&#34892;&#39640;&#25928;&#30340;&#35299;&#31354;&#38388;&#25628;&#32034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TIGTEC&#22312;&#25104;&#21151;&#29575;&#12289;&#31232;&#30095;&#24615;&#12289;&#22810;&#26679;&#24615;&#21644;&#21487;&#20449;&#24230;&#26041;&#38754;&#20855;&#26377;&#24456;&#39640;&#30340;&#23454;&#29992;&#24615;&#12290;&#35813;&#26041;&#27861;&#26082;&#21487;&#20197;&#26159;&#29305;&#23450;&#20110;&#27169;&#22411;&#30340;&#65292;&#20063;&#21487;&#20197;&#26159;&#26080;&#20851;&#27169;&#22411;&#30340;&#65292;&#38750;&#24120;&#26041;&#20415;&#29992;&#20110;&#29983;&#25104;&#23545;&#25239;&#24615;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual examples explain a prediction by highlighting changes of instance that flip the outcome of a classifier. This paper proposes TIGTEC, an efficient and modular method for generating sparse, plausible and diverse counterfactual explanations for textual data. TIGTEC is a text editing heuristic that targets and modifies words with high contribution using local feature importance. A new attention-based local feature importance is proposed. Counterfactual candidates are generated and assessed with a cost function integrating semantic distance, while the solution space is efficiently explored in a beam search fashion. The conducted experiments show the relevance of TIGTEC in terms of success rate, sparsity, diversity and plausibility. This method can be used in both model-specific or model-agnostic way, which makes it very convenient for generating counterfactual explanations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20174;&#26631;&#35760;&#25968;&#25454;&#20016;&#23500;&#30340;&#35774;&#22791;&#36716;&#31227;&#21040;&#26410;&#26631;&#35760;&#25968;&#25454;&#35774;&#22791;&#20197;&#25552;&#39640;&#25968;&#25454;&#21033;&#29992;&#29575;&#12290;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#35774;&#22791;&#20998;&#31867;&#21644;&#28304;-&#30446;&#26631;&#38142;&#25509;&#24418;&#25104;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2304.12422</link><description>&lt;p&gt;
&#22810;&#28304;&#21040;&#22810;&#30446;&#26631;&#30340;&#20998;&#24067;&#24335;&#32852;&#37030;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Multi-Source to Multi-Target Decentralized Federated Domain Adaptation. (arXiv:2304.12422v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12422
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20174;&#26631;&#35760;&#25968;&#25454;&#20016;&#23500;&#30340;&#35774;&#22791;&#36716;&#31227;&#21040;&#26410;&#26631;&#35760;&#25968;&#25454;&#35774;&#22791;&#20197;&#25552;&#39640;&#25968;&#25454;&#21033;&#29992;&#29575;&#12290;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#35774;&#22791;&#20998;&#31867;&#21644;&#28304;-&#30446;&#26631;&#38142;&#25509;&#24418;&#25104;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20013;&#35774;&#22791;&#38388;&#30340;&#24322;&#36136;&#24615;&#36890;&#24120;&#25351;&#32479;&#35745;&#65288;&#20363;&#22914;&#65292;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#25968;&#25454;&#20998;&#24067;&#65289;&#21644;&#36164;&#28304;&#65288;&#20363;&#22914;&#65292;&#36890;&#20449;&#24102;&#23485;&#65289;&#32500;&#24230;&#12290;&#26412;&#25991;&#32858;&#28966;&#21478;&#19968;&#20010;&#37325;&#35201;&#32500;&#24230;&#65306;&#21508;&#35774;&#22791;&#25152;&#25317;&#26377;&#30340;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#25968;&#37327;/&#20998;&#24067;&#12290;&#20026;&#20102;&#21033;&#29992;&#25152;&#26377;&#25968;&#25454;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#32852;&#37030;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#65292;&#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20174;&#26631;&#35760;&#25968;&#25454;&#39640;&#36136;&#37327;&#35774;&#22791;&#65288;&#31216;&#20026;&#28304;&#65289;&#36716;&#31227;&#21040;&#20302;&#36136;&#37327;&#25110;&#26410;&#26631;&#35760;&#25968;&#25454;&#35774;&#22791;&#65288;&#31216;&#20026;&#30446;&#26631;&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#8220;&#28304;-&#30446;&#26631;&#30830;&#23450;&#21644;&#38142;&#25509;&#24418;&#25104;&#8221;&#65288;ST-LF&#65289;&#65292;&#22312;&#32771;&#34385;&#27169;&#22411;&#31934;&#24230;&#21644;&#36890;&#20449;&#33021;&#37327;&#25928;&#29575;&#20043;&#38388;&#30340;&#26435;&#34913;&#30340;&#21516;&#26102;&#65292;&#20248;&#21270;&#35774;&#22791;&#20998;&#31867;&#21644;&#28304;-&#30446;&#26631;&#38142;&#25509;&#24418;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Heterogeneity across devices in federated learning (FL) typically refers to statistical (e.g., non-i.i.d. data distributions) and resource (e.g., communication bandwidth) dimensions. In this paper, we focus on another important dimension that has received less attention: varying quantities/distributions of labeled and unlabeled data across devices. In order to leverage all data, we develop a decentralized federated domain adaptation methodology which considers the transfer of ML models from devices with high quality labeled data (called sources) to devices with low quality or unlabeled data (called targets). Our methodology, Source-Target Determination and Link Formation (ST-LF), optimizes both (i) classification of devices into sources and targets and (ii) source-target link formation, in a manner that considers the trade-off between ML model accuracy and communication energy efficiency. To obtain a concrete objective function, we derive a measurable generalization error bound that ac
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20351;&#29992;&#20102;&#39640;&#25928;&#30340;&#26679;&#26412;&#38598;&#20248;&#21270;&#21644;&#22522;&#20110;&#20195;&#29702;&#30340;&#26041;&#27861;&#26469;&#35774;&#35745;&#27700;&#19979;&#33322;&#34892;&#22120;&#33337;&#20307;&#65292;&#20854;&#20013;&#20195;&#29702;&#27169;&#22411;&#26174;&#33879;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#65292;&#20351;&#20248;&#21270;&#26356;&#21152;&#24555;&#36895;&#20934;&#30830;&#12290;</title><link>http://arxiv.org/abs/2304.12420</link><description>&lt;p&gt;
&#27700;&#19979;&#33322;&#34892;&#22120;&#33337;&#20307;&#30340;&#26679;&#26412;&#39640;&#25928;&#21644;&#22522;&#20110;&#20195;&#29702;&#30340;&#35774;&#35745;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Sample-Efficient and Surrogate-Based Design Optimization of Underwater Vehicle Hulls. (arXiv:2304.12420v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12420
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20351;&#29992;&#20102;&#39640;&#25928;&#30340;&#26679;&#26412;&#38598;&#20248;&#21270;&#21644;&#22522;&#20110;&#20195;&#29702;&#30340;&#26041;&#27861;&#26469;&#35774;&#35745;&#27700;&#19979;&#33322;&#34892;&#22120;&#33337;&#20307;&#65292;&#20854;&#20013;&#20195;&#29702;&#27169;&#22411;&#26174;&#33879;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#65292;&#20351;&#20248;&#21270;&#26356;&#21152;&#24555;&#36895;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#27169;&#25311;&#26159;&#35745;&#31639;&#26426;&#36741;&#21161;&#35774;&#35745;(CAD)&#20248;&#21270;&#36807;&#31243;&#20013;&#30340;&#19968;&#20010;&#35745;&#31639;&#29942;&#39048;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#20351;&#31934;&#30830;(&#35745;&#31639;&#26114;&#36149;)&#30340;&#27169;&#25311;&#21487;&#29992;&#20110;&#35774;&#35745;&#20248;&#21270;&#20013;&#65292;&#38656;&#35201;&#19968;&#20010;&#39640;&#26679;&#26412;&#25928;&#29575;&#30340;&#20248;&#21270;&#26694;&#26550;&#25110;&#24555;&#36895;&#30340;&#25968;&#25454;&#39537;&#21160;&#20195;&#29702;(&#20195;&#29702;&#27169;&#22411;)&#26469;&#20195;&#26367;&#38271;&#26102;&#38388;&#36816;&#34892;&#30340;&#27169;&#25311;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#26368;&#36817;&#20248;&#21270;&#21644;&#20154;&#24037;&#26234;&#33021;(AI)&#30340;&#36827;&#23637;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#35774;&#35745;&#19968;&#20010;&#26368;&#20339;&#30340;&#26080;&#20154;&#27700;&#19979;&#33322;&#34892;&#22120;(UUV)&#12290;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#20248;&#21270;&#25216;&#26415;&#22312;&#20248;&#21270;&#24490;&#29615;&#20013;&#19982;&#26631;&#20934;&#35745;&#31639;&#27969;&#20307;&#21147;&#23398;(CFD)&#27714;&#35299;&#22120;&#30456;&#32467;&#21512;&#26102;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#25910;&#25947;&#34892;&#20026;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#30340;&#20195;&#29702;&#27169;&#22411;&#26469;&#36924;&#36817;&#21542;&#21017;&#36890;&#36807;CFD&#27714;&#35299;&#22120;&#36827;&#34892;&#35745;&#31639;&#30340;&#38459;&#21147;&#12290;&#20195;&#29702;&#27169;&#22411;&#36827;&#32780;&#29992;&#20110;&#26679;&#26412;&#39640;&#25928;&#30340;&#20248;&#21270;&#26694;&#26550;&#20013;&#65292;&#35813;&#26694;&#26550;&#22312;&#19981;&#20351;&#29992;&#20195;&#29702;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#20248;&#20110;&#26631;&#20934;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics simulations are a computational bottleneck in computer-aided design (CAD) optimization processes. Hence, in order to make accurate (computationally expensive) simulations feasible for use in design optimization, one requires either an optimization framework that is highly sample-efficient or fast data-driven proxies (surrogate models) for long running simulations. In this work, we leverage recent advances in optimization and artificial intelligence (AI) to address both of these potential solutions, in the context of designing an optimal unmanned underwater vehicle (UUV). We first investigate and compare the sample efficiency and convergence behavior of different optimization techniques with a standard computational fluid dynamics (CFD) solver in the optimization loop. We then develop a deep neural network (DNN) based surrogate model to approximate drag forces that would otherwise be computed via direct numerical simulation with the CFD solver. The surrogate model is in turn use
&lt;/p&gt;</description></item><item><title>&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#26041;&#27861;&#22312;&#21463;&#38480;&#29627;&#23572;&#20857;&#26364;&#26426;&#25512;&#26029;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#34920;&#29616;&#65292;D-Wave&#37327;&#23376;&#36864;&#28779;&#22120;&#26679;&#26412;&#27604;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#26679;&#26412;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#37327;&#23376;&#36864;&#28779;&#22120;&#26679;&#26412;&#21487;&#20197;&#36827;&#19968;&#27493;&#25913;&#21892;Gibbs&#37319;&#26679;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.12418</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#37327;&#23376;-&#32463;&#20856;&#28151;&#21512;&#26041;&#27861;&#30340;&#21463;&#38480;&#29627;&#23572;&#20857;&#26364;&#26426;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
A hybrid quantum-classical approach for inference on restricted Boltzmann machines. (arXiv:2304.12418v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12418
&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#26041;&#27861;&#22312;&#21463;&#38480;&#29627;&#23572;&#20857;&#26364;&#26426;&#25512;&#26029;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#34920;&#29616;&#65292;D-Wave&#37327;&#23376;&#36864;&#28779;&#22120;&#26679;&#26412;&#27604;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#26679;&#26412;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#37327;&#23376;&#36864;&#28779;&#22120;&#26679;&#26412;&#21487;&#20197;&#36827;&#19968;&#27493;&#25913;&#21892;Gibbs&#37319;&#26679;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29627;&#23572;&#20857;&#26364;&#26426;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#26500;&#24314;&#28145;&#24230;&#32622;&#20449;&#32593;&#32476;&#31561;&#23454;&#38469;&#24212;&#29992;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#12290;&#36890;&#36807;&#20174;&#20854;&#21518;&#39564;&#20998;&#24067;&#20013;&#36827;&#34892;&#37319;&#26679;&#21487;&#20197;&#23545;Boltzmann&#26426;&#36827;&#34892;&#32479;&#35745;&#25512;&#26029;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20855;&#26377;&#26497;&#20854;&#22810;&#23792;&#20998;&#24067;&#65292;&#20174;&#36825;&#26679;&#30340;&#27169;&#22411;&#20013;&#36827;&#34892;&#22343;&#21248;&#37319;&#26679;&#24182;&#19981;&#23481;&#26131;&#12290;&#37327;&#23376;&#35745;&#31639;&#26426;&#26377;&#21487;&#33021;&#20197;&#39640;&#25928;&#30340;&#26041;&#24335;&#35299;&#20915;&#19968;&#20123;&#22256;&#38590;&#38382;&#39064;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#20351;&#29992;D-Wave&#37327;&#23376;&#36864;&#28779;&#22120;&#29983;&#25104;&#21463;&#38480;&#29627;&#23572;&#20857;&#26364;&#26426;&#26679;&#26412;&#30340;&#24212;&#29992;&#12290;&#36825;&#20123;&#26679;&#26412;&#22312;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#35774;&#32622;&#20013;&#36890;&#36807;&#39532;&#23572;&#31185;&#22827;&#38142;&#36827;&#19968;&#27493;&#25913;&#21892;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#19982;&#38543;&#26426;&#21021;&#22987;&#21270;&#30456;&#27604;&#65292;&#37327;&#23376;&#36864;&#28779;&#22120;&#26679;&#26412;&#21487;&#20197;&#25913;&#21892;Gibbs&#37319;&#26679;&#30340;&#24615;&#33021;&#12290;&#28151;&#21512;&#35774;&#32622;&#27604;&#32431;&#32463;&#20856;&#37319;&#26679;&#35201;&#39640;&#25928;&#24471;&#22810;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#36864;&#28779;&#21442;&#25968;&#65288;&#28201;&#24230;&#65289;&#30340;&#24433;&#21709;&#65292;&#20197;&#25552;&#39640;&#26679;&#26412;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Boltzmann machine is a powerful machine learning model with many real-world applications, for example by constructing deep belief networks. Statistical inference on a Boltzmann machine can be carried out by sampling from its posterior distribution. However, uniform sampling from such a model is not trivial due to an extremely multi-modal distribution. Quantum computers have the promise of solving some non-trivial problems in an efficient manner. We explored the application of a D-Wave quantum annealer to generate samples from a restricted Boltzmann machine. The samples are further improved by Markov chains in a hybrid quantum-classical setup. We demonstrated that quantum annealer samples can improve the performance of Gibbs sampling compared to random initialization. The hybrid setup is considerably more efficient than a pure classical sampling. We also investigated the impact of annealing parameters (temperature) to improve the quality of samples. By increasing the amount of classical
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;PEFT-Ref&#21442;&#32771;&#26550;&#26500;&#65292;&#26631;&#20934;&#21270;&#20102;&#19981;&#21516;PEFT&#25216;&#26415;&#20849;&#20139;&#30340;&#26041;&#38754;&#65292;&#38548;&#31163;&#20102;&#24046;&#24322;&#21040;&#29305;&#23450;&#20301;&#32622;&#21644;&#20132;&#20114;&#20013;&#65292;&#27169;&#22359;&#21270;&#30340;&#35270;&#22270;&#26377;&#21161;&#20110;&#27604;&#36739;&#19981;&#21516;&#25216;&#26415;&#21450;&#20854;&#25928;&#29575;&#21644;&#20219;&#21153;&#24615;&#33021;&#65292;&#24182;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;PEFT&#30340;&#22522;&#26412;&#21407;&#29702;&#12290;</title><link>http://arxiv.org/abs/2304.12410</link><description>&lt;p&gt;
PEFT-Ref: &#19968;&#31181;&#27169;&#22359;&#21270;&#30340;&#21442;&#32771;&#26550;&#26500;&#21644;&#31867;&#22411;&#65292;&#29992;&#20110;&#21442;&#25968;&#25928;&#29575;&#24494;&#35843;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
PEFT-Ref: A Modular Reference Architecture and Typology for Parameter-Efficient Finetuning Techniques. (arXiv:2304.12410v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;PEFT-Ref&#21442;&#32771;&#26550;&#26500;&#65292;&#26631;&#20934;&#21270;&#20102;&#19981;&#21516;PEFT&#25216;&#26415;&#20849;&#20139;&#30340;&#26041;&#38754;&#65292;&#38548;&#31163;&#20102;&#24046;&#24322;&#21040;&#29305;&#23450;&#20301;&#32622;&#21644;&#20132;&#20114;&#20013;&#65292;&#27169;&#22359;&#21270;&#30340;&#35270;&#22270;&#26377;&#21161;&#20110;&#27604;&#36739;&#19981;&#21516;&#25216;&#26415;&#21450;&#20854;&#25928;&#29575;&#21644;&#20219;&#21153;&#24615;&#33021;&#65292;&#24182;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;PEFT&#30340;&#22522;&#26412;&#21407;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#21442;&#25968;&#25928;&#29575;&#24494;&#35843;(PEFT)&#25216;&#26415;&#26088;&#22312;&#25913;&#21892;&#23436;&#20840;&#24494;&#35843;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLM)&#30340;&#39640;&#26114;&#25104;&#26412;&#12290;&#38543;&#30528;&#19981;&#21516;&#30340;PEFT&#25216;&#26415;&#19981;&#26029;&#20986;&#29616;&#65292;&#23545;&#23427;&#20204;&#36827;&#34892;&#27604;&#36739;&#21464;&#24471;&#36234;&#26469;&#36234;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#22312;&#20197;&#19979;&#26041;&#38754;&#65306;(i)&#23427;&#20204;&#28155;&#21152;&#21040;PLM&#30340;&#32467;&#26500;&#21644;&#21151;&#33021;&#65292;(ii)&#19981;&#21516;&#31867;&#22411;&#21644;&#31243;&#24230;&#30340;&#25928;&#29575;&#25913;&#36827;&#65292;(iii)&#22312;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#20197;&#21450;(iv)&#32467;&#26500;&#21644;&#21151;&#33021;&#24046;&#24322;&#22914;&#20309;&#19982;&#25928;&#29575;&#21644;&#20219;&#21153;&#24615;&#33021;&#30456;&#20851;&#32852;&#12290;&#20026;&#20102;&#20419;&#36827;&#36825;&#26679;&#30340;&#27604;&#36739;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21442;&#32771;&#26694;&#26550;&#65292;&#26631;&#20934;&#21270;&#20102;&#19981;&#21516;PEFT&#25216;&#26415;&#20849;&#20139;&#30340;&#26041;&#38754;&#65292;&#21516;&#26102;&#23558;&#24046;&#24322;&#38548;&#31163;&#21040;&#19982;&#26631;&#20934;&#32452;&#20214;&#30340;&#29305;&#23450;&#20301;&#32622;&#21644;&#20132;&#20114;&#20013;&#12290;&#36890;&#36807;&#36825;&#20010;&#26631;&#20934;&#21270;&#21644;&#38548;&#31163;&#24046;&#24322;&#30340;&#36807;&#31243;&#65292;&#20986;&#29616;&#20102;PEFT&#25216;&#26415;&#30340;&#27169;&#22359;&#21270;&#35270;&#22270;&#65292;&#19981;&#20165;&#25903;&#25345;&#30452;&#25509;&#27604;&#36739;&#19981;&#21516;&#25216;&#26415;&#21450;&#20854;&#25928;&#29575;&#21644;&#20219;&#21153;&#24615;&#33021;&#65292;&#32780;&#19988;&#36824;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;PEFT&#30340;&#22522;&#26412;&#21407;&#29702;&#12290;&#25152;&#25552;&#20986;&#30340;&#21442;&#32771;&#26550;&#26500;&#31216;&#20026;PEFT-Ref&#65292;&#21253;&#25324;&#19971;&#20010;&#26680;&#24515;&#27169;&#22359;&#65292;&#27599;&#20010;&#27169;&#22359;&#37117;&#22788;&#29702;PEFT&#30340;&#29305;&#23450;&#26041;&#38754;&#65292;&#24182;&#21487;&#29992;&#20316;&#24320;&#21457;&#26032;PEFT&#25216;&#26415;&#21644;&#27604;&#36739;&#29616;&#26377;&#25216;&#26415;&#30340;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent parameter-efficient finetuning (PEFT) techniques aim to improve over the considerable cost of fully finetuning large pretrained language models (PLM). As different PEFT techniques proliferate, it is becoming difficult to compare them, in particular in terms of (i) the structure and functionality they add to the PLM, (ii) the different types and degrees of efficiency improvements achieved, (iii) performance at different downstream tasks, and (iv) how differences in structure and functionality relate to efficiency and task performance. To facilitate such comparisons, this paper presents a reference framework which standardises aspects shared by different PEFT techniques, while isolating differences to specific locations and interactions with the standard components. Through this process of standardising and isolating differences, a modular view of PEFT techniques emerges, supporting not only direct comparison of different techniques and their efficiency and task performance, but a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24179;&#26041;&#21644;&#20248;&#21270;&#65292;&#32508;&#21512;&#21160;&#24577;&#30340;&#12289;&#38477;&#38454;&#30340;&#36755;&#20986;&#21453;&#39304;&#22810;&#39033;&#24335;&#25511;&#21046;&#22120;&#30340;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#25511;&#21046;&#20223;&#23556;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#65292;&#24182;&#20351;&#29992;&#23398;&#20064;&#30340;&#24863;&#30693;&#27169;&#22359;&#21644;&#35270;&#35273;&#35266;&#27979;&#26469;&#31283;&#23450;&#22320;&#36816;&#34892;&#21040;&#30446;&#26631;&#29366;&#24577;&#12290;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#22312;&#35266;&#27979;&#22122;&#22768;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#30340;&#31283;&#23450;&#24615;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2304.12405</link><description>&lt;p&gt;
&#36890;&#36807;&#24179;&#26041;&#21644;&#20248;&#21270;&#32508;&#21512;&#31283;&#23450;&#30340;&#38477;&#38454;&#35270;&#21160;&#24577;&#25511;&#21046;&#22120;&#65292;&#24212;&#29992;&#20110;&#38750;&#32447;&#24615;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Synthesizing Stable Reduced-Order Visuomotor Policies for Nonlinear Systems via Sums-of-Squares Optimization. (arXiv:2304.12405v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12405
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24179;&#26041;&#21644;&#20248;&#21270;&#65292;&#32508;&#21512;&#21160;&#24577;&#30340;&#12289;&#38477;&#38454;&#30340;&#36755;&#20986;&#21453;&#39304;&#22810;&#39033;&#24335;&#25511;&#21046;&#22120;&#30340;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#25511;&#21046;&#20223;&#23556;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#65292;&#24182;&#20351;&#29992;&#23398;&#20064;&#30340;&#24863;&#30693;&#27169;&#22359;&#21644;&#35270;&#35273;&#35266;&#27979;&#26469;&#31283;&#23450;&#22320;&#36816;&#34892;&#21040;&#30446;&#26631;&#29366;&#24577;&#12290;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#22312;&#35266;&#27979;&#22122;&#22768;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#30340;&#31283;&#23450;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#32508;&#21512;&#21160;&#24577;&#30340;&#12289;&#38477;&#38454;&#30340;&#36755;&#20986;&#21453;&#39304;&#22810;&#39033;&#24335;&#25511;&#21046;&#22120;&#65292;&#24212;&#29992;&#20110;&#25511;&#21046;&#20223;&#23556;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#65292;&#24182;&#20351;&#29992;&#23398;&#20064;&#30340;&#24863;&#30693;&#27169;&#22359;&#21644;&#35270;&#35273;&#35266;&#27979;&#26469;&#20445;&#35777;&#22312;&#21453;&#39304;&#25511;&#21046;&#24490;&#29615;&#20013;&#31283;&#23450;&#22320;&#36816;&#34892;&#21040;&#30446;&#26631;&#29366;&#24577;&#12290;&#25105;&#20204;&#21033;&#29992;Lyapunov&#20998;&#26512;&#26469;&#21046;&#23450;&#32508;&#21512;&#36825;&#31181;&#31574;&#30053;&#30340;&#38382;&#39064;&#12290;&#35813;&#38382;&#39064;&#22312;&#31574;&#30053;&#21442;&#25968;&#21644;&#29992;&#20110;&#35777;&#26126;&#31574;&#30053;&#31283;&#23450;&#24615;&#30340;Lyapunov&#20989;&#25968;&#26041;&#38754;&#26159;&#38750;&#20984;&#30340;&#12290;&#20026;&#20102;&#36817;&#20284;&#22320;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#65306;&#31532;&#19968;&#31181;&#35299;&#20915;&#20102;&#19968;&#31995;&#21015;&#24179;&#26041;&#21644;&#20248;&#21270;&#38382;&#39064;&#65292;&#20197;&#36845;&#20195;&#25913;&#36827;&#19968;&#20010;&#36890;&#36807;&#26500;&#36896;&#21487;&#35777;&#26126;&#30340;&#31283;&#23450;&#31574;&#30053;&#65292;&#32780;&#31532;&#20108;&#31181;&#21017;&#30452;&#25509;&#23545;&#22810;&#39033;&#24335;&#31574;&#30053;&#30340;&#21442;&#25968;&#36827;&#34892;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#65292;&#24182;&#22312;&#21518;&#39564;&#19978;&#39564;&#35777;&#20854;&#38381;&#29615;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#20197;&#22312;&#35266;&#27979;&#22122;&#22768;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#31283;&#23450;&#24615;&#20445;&#35777;&#65292;&#35813;&#24773;&#20917;&#23454;&#38469;&#19978;&#26159;&#30001;&#20110;&#35266;&#27979;&#35823;&#24046;&#32780;&#24341;&#36215;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a method for synthesizing dynamic, reduced-order output-feedback polynomial control policies for control-affine nonlinear systems which guarantees runtime stability to a goal state, when using visual observations and a learned perception module in the feedback control loop. We leverage Lyapunov analysis to formulate the problem of synthesizing such policies. This problem is nonconvex in the policy parameters and the Lyapunov function that is used to prove the stability of the policy. To solve this problem approximately, we propose two approaches: the first solves a sequence of sum-of-squares optimization problems to iteratively improve a policy which is provably-stable by construction, while the second directly performs gradient-based optimization on the parameters of the polynomial policy, and its closed-loop stability is verified a posteriori. We extend our approach to provide stability guarantees in the presence of observation noise, which realistically arises due to erro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;\name{}&#32534;&#35793;&#22120;&#65292;&#21487;&#20197;&#23558;HDC&#20998;&#31867;&#26041;&#27861;&#30340;&#39640;&#32423;&#25551;&#36848;&#32763;&#35793;&#20026;&#20248;&#21270;&#30340;C&#20195;&#30721;&#65292;&#20351;&#20854;&#26356;&#36866;&#29992;&#20110;&#22312;&#23884;&#20837;&#24335;&#31995;&#32479;&#21644;&#39640;&#24615;&#33021;&#35745;&#31639;&#20013;&#36827;&#34892;&#20998;&#31867;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2304.12398</link><description>&lt;p&gt;
HDCC&#65306;&#23884;&#20837;&#24335;&#31995;&#32479;&#21644;&#39640;&#24615;&#33021;&#35745;&#31639;&#20013;&#36827;&#34892;&#20998;&#31867;&#30340;&#39640;&#32500;&#35745;&#31639;&#32534;&#35793;&#22120;
&lt;/p&gt;
&lt;p&gt;
HDCC: A Hyperdimensional Computing compiler for classification on embedded systems and high-performance computing. (arXiv:2304.12398v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12398
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;\name{}&#32534;&#35793;&#22120;&#65292;&#21487;&#20197;&#23558;HDC&#20998;&#31867;&#26041;&#27861;&#30340;&#39640;&#32423;&#25551;&#36848;&#32763;&#35793;&#20026;&#20248;&#21270;&#30340;C&#20195;&#30721;&#65292;&#20351;&#20854;&#26356;&#36866;&#29992;&#20110;&#22312;&#23884;&#20837;&#24335;&#31995;&#32479;&#21644;&#39640;&#24615;&#33021;&#35745;&#31639;&#20013;&#36827;&#34892;&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#32500;&#35745;&#31639;&#65288;HDC&#65289;&#26159;&#19968;&#31181;&#29983;&#29289;&#21551;&#21457;&#24335;&#35745;&#31639;&#26694;&#26550;&#65292;&#23588;&#20854;&#20316;&#20026;&#19968;&#31181;&#26356;&#26377;&#25928;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#32780;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;\name{}&#32534;&#35793;&#22120;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#23558;HDC&#20998;&#31867;&#26041;&#27861;&#30340;&#39640;&#32423;&#25551;&#36848;&#32763;&#35793;&#20026;&#20248;&#21270;&#30340;C&#20195;&#30721;&#30340;&#24320;&#28304;&#32534;&#35793;&#22120;&#12290;&#25152;&#25552;&#35758;&#30340;&#32534;&#35793;&#22120;&#29983;&#25104;&#30340;&#20195;&#30721;&#20855;&#26377;&#19977;&#20010;&#20027;&#35201;&#29305;&#28857;&#65292;&#36866;&#29992;&#20110;&#23884;&#20837;&#24335;&#31995;&#32479;&#21644;&#39640;&#24615;&#33021;&#35745;&#31639;&#65306;(1)&#23427;&#26159;&#33258;&#21253;&#21547;&#30340;&#65292;&#27809;&#26377;&#24211;&#25110;&#24179;&#21488;&#20381;&#36182;;(2)&#23427;&#25903;&#25345;&#20351;&#29992;C&#20869;&#22312;&#20989;&#25968;&#30340;&#22810;&#32447;&#31243;&#21644;&#21333;&#25351;&#20196;&#22810;&#25968;&#25454;&#65288;SIMD&#65289;&#25351;&#20196;;(3)&#23427;&#20248;&#21270;&#20102;&#26368;&#22823;&#30340;&#24615;&#33021;&#21644;&#26368;&#23567;&#30340;&#20869;&#23384;&#20351;&#29992;&#29575;&#12290; \name{}&#30340;&#35774;&#35745;&#23601;&#20687;&#19968;&#20010;&#29616;&#20195;&#32534;&#35793;&#22120;&#65292;&#20855;&#26377;&#30452;&#35266;&#21644;&#25551;&#36848;&#24615;&#30340;&#36755;&#20837;&#35821;&#35328;&#65292;&#20013;&#38388;&#34920;&#31034;&#65288;IR&#65289;&#21644;&#21487;&#37325;&#26032;&#23450;&#20301;&#30340;&#21518;&#31471;&#12290;&#36825;&#20351;\name{}&#25104;&#20026;&#19968;&#20010;&#26377;&#20215;&#20540;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#24037;&#20855;&#65292;&#25506;&#32034;HDC&#22312;&#23884;&#20837;&#24335;&#31995;&#32479;&#21644;&#39640;&#24615;&#33021;&#35745;&#31639;&#20013;&#36827;&#34892;&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperdimensional Computing (HDC) is a bio-inspired computing framework that has gained increasing attention, especially as a more efficient approach to machine learning (ML). This work introduces the \name{} compiler, the first open-source compiler that translates high-level descriptions of HDC classification methods into optimized C code. The code generated by the proposed compiler has three main features for embedded systems and High-Performance Computing: (1) it is self-contained and has no library or platform dependencies; (2) it supports multithreading and single instruction multiple data (SIMD) instructions using C intrinsics; (3) it is optimized for maximum performance and minimal memory usage. \name{} is designed like a modern compiler, featuring an intuitive and descriptive input language, an intermediate representation (IR), and a retargetable backend. This makes \name{} a valuable tool for research and applications exploring HDC for classification tasks on embedded systems a
&lt;/p&gt;</description></item><item><title>&#35813;&#36719;&#20214;&#21253;&#25552;&#20379;&#20102;&#19968;&#32452;&#26377;&#29992;&#30340;&#24037;&#20855;&#26469;&#20998;&#26512;&#20998;&#31867;&#26102;&#38388;&#24207;&#21015;&#65292;&#20351;&#24471;&#29992;&#25143;&#21487;&#20197;&#29992;&#22270;&#24418;&#25551;&#36848;&#24213;&#23618;&#26102;&#38388;&#27169;&#24335;&#24182;&#21033;&#29992;&#20854;&#36755;&#20986;&#25191;&#34892;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#21253;&#25324;&#32858;&#31867;&#12289;&#20998;&#31867;&#21644;&#24322;&#24120;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2304.12332</link><description>&lt;p&gt;
&#20351;&#29992;R&#36719;&#20214;&#21253;ctsfeatures&#20998;&#26512;&#20998;&#31867;&#26102;&#38388;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
Analyzing categorical time series with the R package ctsfeatures. (arXiv:2304.12332v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12332
&lt;/p&gt;
&lt;p&gt;
&#35813;&#36719;&#20214;&#21253;&#25552;&#20379;&#20102;&#19968;&#32452;&#26377;&#29992;&#30340;&#24037;&#20855;&#26469;&#20998;&#26512;&#20998;&#31867;&#26102;&#38388;&#24207;&#21015;&#65292;&#20351;&#24471;&#29992;&#25143;&#21487;&#20197;&#29992;&#22270;&#24418;&#25551;&#36848;&#24213;&#23618;&#26102;&#38388;&#27169;&#24335;&#24182;&#21033;&#29992;&#20854;&#36755;&#20986;&#25191;&#34892;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#21253;&#25324;&#32858;&#31867;&#12289;&#20998;&#31867;&#21644;&#24322;&#24120;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#22914;&#20170;&#24050;&#26080;&#22788;&#19981;&#22312;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#25991;&#29486;&#37117;&#22788;&#29702;&#23454;&#20540;&#26102;&#38388;&#24207;&#21015;&#65292;&#20998;&#31867;&#26102;&#38388;&#24207;&#21015;&#21364;&#21463;&#21040;&#20102;&#36739;&#23569;&#30340;&#20851;&#27880;&#12290;&#36817;&#24180;&#26469;&#65292;&#36825;&#31867;&#25968;&#25454;&#30340;&#25968;&#25454;&#25366;&#25496;&#25216;&#26415;&#24471;&#21040;&#20102;&#23454;&#36136;&#24615;&#30340;&#21457;&#23637;&#12290; R&#36719;&#20214;&#21253;ctsfeatures&#20026;&#29992;&#25143;&#25552;&#20379;&#20102;&#19968;&#32452;&#26377;&#29992;&#30340;&#24037;&#20855;&#26469;&#20998;&#26512;&#20998;&#31867;&#26102;&#38388;&#24207;&#21015;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#36719;&#20214;&#21253;&#25552;&#20379;&#20102;&#20960;&#20010;&#20989;&#25968;&#65292;&#20801;&#35768;&#25552;&#21462;&#24050;&#30693;&#30340;&#32479;&#35745;&#29305;&#24449;&#24182;&#26500;&#24314;&#25551;&#36848;&#24213;&#23618;&#26102;&#38388;&#27169;&#24335;&#30340;&#22270;&#24418;&#12290;&#26576;&#20123;&#20989;&#25968;&#30340;&#36755;&#20986;&#21487;&#29992;&#20110;&#25191;&#34892;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65292;&#21253;&#25324;&#32858;&#31867;&#12289;&#20998;&#31867;&#21644;&#24322;&#24120;&#26816;&#27979;&#12290;&#35813;&#36719;&#20214;&#21253;&#36824;&#21253;&#25324;&#20004;&#20010;&#22312;&#25991;&#29486;&#20013;&#20171;&#32461;&#30340;&#29983;&#29289;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#32858;&#31867;&#30446;&#30340;&#65292;&#20197;&#21450;&#19977;&#20010;&#26377;&#36259;&#30340;&#21512;&#25104;&#25968;&#25454;&#24211;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#35813;&#36719;&#20214;&#21253;&#30340;&#20027;&#35201;&#29305;&#24449;&#21450;&#20854;&#20351;&#29992;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series data are ubiquitous nowadays. Whereas most of the literature on the topic deals with real-valued time series, categorical time series have received much less attention. However, the development of data mining techniques for this kind of data has substantially increased in recent years. The R package ctsfeatures offers users a set of useful tools for analyzing categorical time series. In particular, several functions allowing the extraction of well-known statistical features and the construction of illustrative graphs describing underlying temporal patterns are provided in the package. The output of some functions can be employed to perform traditional machine learning tasks including clustering, classification and outlier detection. The package also includes two datasets of biological sequences introduced in the literature for clustering purposes, as well as three interesting synthetic databases. In this work, the main characteristics of the package are described and its us
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24182;&#34892;bootstrap&#30340;on-policy&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#37096;&#20998;&#36712;&#36857;&#32531;&#20914;&#21306;&#21644;&#36820;&#22238;bootstrapping&#27493;&#39588;&#26469;&#23454;&#29616;&#28789;&#27963;&#20351;&#29992;&#24182;&#34892;&#29615;&#22659;&#65292;&#21516;&#26102;&#20445;&#25345;&#26356;&#26032;&#30340;on-policy&#24615;&#65292;&#35813;&#26041;&#27861;&#22312;&#36830;&#32493;&#27969;&#25511;&#21046;&#38382;&#39064;&#19978;&#26377;&#24456;&#22909;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2304.12330</link><description>&lt;p&gt;
&#22522;&#20110;&#24182;&#34892;bootstrap&#30340;&#36830;&#32493;&#27969;&#25511;&#21046;&#24212;&#29992;&#30340;on-policy&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Parallel bootstrap-based on-policy deep reinforcement learning for continuous flow control applications. (arXiv:2304.12330v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12330
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24182;&#34892;bootstrap&#30340;on-policy&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#37096;&#20998;&#36712;&#36857;&#32531;&#20914;&#21306;&#21644;&#36820;&#22238;bootstrapping&#27493;&#39588;&#26469;&#23454;&#29616;&#28789;&#27963;&#20351;&#29992;&#24182;&#34892;&#29615;&#22659;&#65292;&#21516;&#26102;&#20445;&#25345;&#26356;&#26032;&#30340;on-policy&#24615;&#65292;&#35813;&#26041;&#27861;&#22312;&#36830;&#32493;&#27969;&#25511;&#21046;&#38382;&#39064;&#19978;&#26377;&#24456;&#22909;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#19982;&#25968;&#20540;&#27969;&#25511;&#38382;&#39064;&#30340;&#32806;&#21512;&#36817;&#26399;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#65292;&#21462;&#24471;&#20102;&#31361;&#30772;&#24615;&#30340;&#25104;&#26524;&#24182;&#20026;&#35813;&#39046;&#22495;&#24320;&#36767;&#20102;&#26032;&#30340;&#21069;&#26223;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27969;&#20307;&#21160;&#21147;&#23398;&#27714;&#35299;&#22120;&#30340;&#35745;&#31639;&#25104;&#26412;&#36890;&#24120;&#24456;&#39640;&#65292;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#20351;&#29992;&#24182;&#34892;&#29615;&#22659;&#26159;&#23454;&#29616;&#26377;&#25928;&#25511;&#21046;&#30340;&#24517;&#35201;&#25163;&#27573;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#22823;&#22810;&#25968;&#22522;&#20110;&#27969;&#25511;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25991;&#29486;&#20173;&#20381;&#36182;&#20110;on-policy&#31639;&#27861;&#65292;&#32780;&#36825;&#31181;&#31639;&#27861;&#30340;&#39640;&#24182;&#34892;&#36716;&#31227;&#25910;&#38598;&#21487;&#33021;&#20250;&#30772;&#22351;&#29702;&#35770;&#20551;&#35774;&#24182;&#23548;&#33268;&#27425;&#20248;&#30340;&#25511;&#21046;&#27169;&#22411;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#37096;&#20998;&#36712;&#36857;&#32531;&#20914;&#21306;&#30340;&#24182;&#34892;&#27169;&#24335;&#65292;&#36890;&#36807;&#19968;&#20010;&#36820;&#22238;bootstrapping&#27493;&#39588;&#65292;&#20801;&#35768;&#28789;&#27963;&#22320;&#20351;&#29992;&#24182;&#34892;&#29615;&#22659;&#65292;&#21516;&#26102;&#20445;&#25345;&#26356;&#26032;&#30340;on-policy&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#25991;&#29486;&#20013;&#19968;&#20010;&#32791;&#36153;&#22823;&#37327;&#35745;&#31639;&#30340;&#36830;&#32493;&#27969;&#25511;&#21046;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#35828;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
The coupling of deep reinforcement learning to numerical flow control problems has recently received a considerable attention, leading to groundbreaking results and opening new perspectives for the domain. Due to the usually high computational cost of fluid dynamics solvers, the use of parallel environments during the learning process represents an essential ingredient to attain efficient control in a reasonable time. Yet, most of the deep reinforcement learning literature for flow control relies on on-policy algorithms, for which the massively parallel transition collection may break theoretical assumptions and lead to suboptimal control models. To overcome this issue, we propose a parallelism pattern relying on partial-trajectory buffers terminated by a return bootstrapping step, allowing a flexible use of parallel environments while preserving the on-policiness of the updates. This approach is illustrated on a CPU-intensive continuous flow control problem from the literature.
&lt;/p&gt;</description></item><item><title>Virus2Vec &#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#29305;&#24449;&#21521;&#37327;&#34920;&#31034;&#30149;&#27602;&#24207;&#21015;&#65292;&#33021;&#22815;&#35782;&#21035;&#30149;&#27602;&#23487;&#20027;&#65292;&#20854;&#22312;&#30149;&#27602;&#23487;&#20027;&#39044;&#27979;&#19978;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#25552;&#39640;&#20102;&#22810;&#36798;16&#65285;&#30340;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.12328</link><description>&lt;p&gt;
Virus2Vec: &#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#30149;&#27602;&#24207;&#21015;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Virus2Vec: Viral Sequence Classification Using Machine Learning. (arXiv:2304.12328v1 [q-bio.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12328
&lt;/p&gt;
&lt;p&gt;
Virus2Vec &#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#29305;&#24449;&#21521;&#37327;&#34920;&#31034;&#30149;&#27602;&#24207;&#21015;&#65292;&#33021;&#22815;&#35782;&#21035;&#30149;&#27602;&#23487;&#20027;&#65292;&#20854;&#22312;&#30149;&#27602;&#23487;&#20027;&#39044;&#27979;&#19978;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#25552;&#39640;&#20102;&#22810;&#36798;16&#65285;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#19981;&#21516;&#30149;&#27602;&#23478;&#26063;&#30340;&#23487;&#20027;&#29305;&#24322;&#24615;&#21487;&#20197;&#25581;&#31034; SARS-CoV-2&#12289;&#29378;&#29356;&#30149;&#31561;&#21160;&#29289;&#28304;&#24615;&#30149;&#21407;&#20307;&#22312;&#20154;&#31867;&#20013;&#30340;&#36215;&#28304;&#12290;&#36825;&#26377;&#21161;&#20110;&#27969;&#34892;&#30149;&#23398;&#23478;&#12289;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#21644;&#25919;&#31574;&#21046;&#23450;&#32773;&#21450;&#26102;&#36943;&#21046;&#29616;&#26377;&#30340;&#27969;&#34892;&#30149;&#24182;&#39044;&#38450;&#26410;&#26469;&#30340;&#27969;&#34892;&#30149;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Virus2Vec&#65292;&#23427;&#26159;&#30149;&#27602;&#65288;&#26680;&#33527;&#37240;&#25110;&#27688;&#22522;&#37240;&#65289;&#24207;&#21015;&#30340;&#29305;&#24449;&#21521;&#37327;&#34920;&#31034;&#65292;&#21487;&#20197;&#35753;&#22522;&#20110;&#21521;&#37327;&#31354;&#38388;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35782;&#21035;&#30149;&#27602;&#23487;&#20027;&#12290;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20896;&#29366;&#30149;&#27602;&#31185;&#21644;&#29378;&#29356;&#30149;&#27602;&#30340;&#23487;&#20027;&#39044;&#27979;&#19978;&#23558;&#31934;&#24230;&#25552;&#39640;&#20102;&#22810;&#36798;16&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the host-specificity of different families of viruses sheds light on the origin of, e.g., SARS-CoV-2, rabies, and other such zoonotic pathogens in humans. It enables epidemiologists, medical professionals, and policymakers to curb existing epidemics and prevent future ones promptly. In the family Coronaviridae (of which SARS-CoV-2 is a member), it is well-known that the spike protein is the point of contact between the virus and the host cell membrane. On the other hand, the two traditional mammalian orders, Carnivora (carnivores) and Chiroptera (bats) are recognized to be responsible for maintaining and spreading the Rabies Lyssavirus (RABV). We propose Virus2Vec, a feature-vector representation for viral (nucleotide or amino acid) sequences that enable vector-space-based machine learning models to identify viral hosts. Virus2Vec generates numerical feature vectors for unaligned sequences, allowing us to forego the computationally expensive sequence alignment step from t
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#65292;&#28023;&#27915;&#20013;&#21494;&#32511;&#32032;&#30340;&#29983;&#38271;&#19982;&#29289;&#29702;&#21270;&#23398;&#25104;&#20998;&#22914;&#38081;&#12289;&#30813;&#37240;&#30416;&#12289;&#30967;&#37240;&#30416;&#12289;pH&#20540;&#12289;&#30416;&#24230;&#31561;&#30340;&#26368;&#20339;&#27987;&#24230;&#26377;&#20851;&#65292;&#21487;&#20197;&#29992;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#28023;&#27915;&#21494;&#32511;&#32032;&#12290;</title><link>http://arxiv.org/abs/2304.12325</link><description>&lt;p&gt;
&#23398;&#20064;&#25216;&#26415;&#22312;&#28023;&#27915;&#21494;&#32511;&#32032;&#20998;&#26512;&#20013;&#30340;&#29289;&#29702;&#21270;&#23398;&#29305;&#24615;&#20381;&#36182;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Dependence of Physiochemical Features on Marine Chlorophyll Analysis with Learning Techniques. (arXiv:2304.12325v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12325
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#28023;&#27915;&#20013;&#21494;&#32511;&#32032;&#30340;&#29983;&#38271;&#19982;&#29289;&#29702;&#21270;&#23398;&#25104;&#20998;&#22914;&#38081;&#12289;&#30813;&#37240;&#30416;&#12289;&#30967;&#37240;&#30416;&#12289;pH&#20540;&#12289;&#30416;&#24230;&#31561;&#30340;&#26368;&#20339;&#27987;&#24230;&#26377;&#20851;&#65292;&#21487;&#20197;&#29992;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#28023;&#27915;&#21494;&#32511;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28023;&#27915;&#28014;&#28216;&#26893;&#29289;&#20013;&#23384;&#22312;&#30340;&#21494;&#32511;&#32032;&#26159;&#20809;&#21512;&#20316;&#29992;&#30340;&#22522;&#30784;&#65292;&#23545;&#32500;&#25345;&#29983;&#24577;&#24179;&#34913;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#23545;&#20840;&#29699;&#21021;&#32423;&#29983;&#20135;&#21147;&#20316;&#20986;&#20102;&#37325;&#22823;&#36129;&#29486;&#65292;&#24182;&#22788;&#20110;&#35768;&#22810;&#28023;&#27915;&#29983;&#29289;&#30340;&#39135;&#29289;&#38142;&#20013;&#12290;&#28014;&#28216;&#26893;&#29289;&#27987;&#24230;&#30340;&#19981;&#24179;&#34913;&#21487;&#33021;&#20250;&#30772;&#22351;&#29983;&#24577;&#24179;&#34913;&#12290;&#28014;&#28216;&#26893;&#29289;&#30340;&#29983;&#38271;&#21462;&#20915;&#20110;&#29289;&#29702;&#21270;&#23398;&#25104;&#20998;&#30340;&#26368;&#20339;&#27987;&#24230;&#65292;&#22914;&#38081;&#65292;&#30813;&#37240;&#30416;&#65292;&#30967;&#37240;&#30416;&#65292;pH&#20540;&#65292;&#30416;&#24230;&#31561;&#30340;&#20559;&#31163;&#29702;&#24819;&#27987;&#24230;&#21487;&#33021;&#20250;&#24433;&#21709;&#28014;&#28216;&#26893;&#29289;&#30340;&#29983;&#38271;&#65292;&#20174;&#32780;&#26368;&#32456;&#30772;&#22351;&#29983;&#24577;&#31995;&#32479;&#12290;&#22240;&#27492;&#65292;&#20998;&#26512;&#36825;&#20123;&#25104;&#20998;&#20855;&#26377;&#26497;&#39640;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#20272;&#35745;&#28023;&#27915;&#28014;&#28216;&#26893;&#29289;&#30340;&#21487;&#33021;&#29983;&#38271;&#12290;&#36965;&#24863;&#25216;&#26415;&#30340;&#36827;&#27493;&#25913;&#21892;&#20102;&#20840;&#29699;&#33539;&#22260;&#20869;&#36828;&#31243;&#30740;&#31350;&#29289;&#29702;&#21270;&#23398;&#25104;&#20998;&#30340;&#21487;&#33021;&#24615;&#12290;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20351;&#24471;&#39044;&#27979;&#28023;&#27915;&#21494;&#32511;&#32032;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Marine chlorophyll which is present within phytoplankton are the basis of photosynthesis and they have a high significance in sustaining ecological balance as they highly contribute toward global primary productivity and comes under the food chain of many marine organisms. Imbalance in the concentrations of phytoplankton can disrupt the ecological balance. The growth of phytoplankton depends upon the optimum concentrations of physiochemical constituents like iron, nitrates, phosphates, pH level, salinity, etc. and deviations from an ideal concentration can affect the growth of phytoplankton which can ultimately disrupt the ecosystem at a large scale. Thus the analysis of such constituents has high significance to estimate the probable growth of marine phytoplankton. The advancements of remote sensing technologies have improved the scope to remotely study the physiochemical constituents on a global scale. The machine learning techniques have made it possible to predict the marine chloro
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;Python&#26694;&#26550;&#19979;&#30340;&#24211;PyLogik&#26469;&#24110;&#21161;&#36229;&#22768;&#22270;&#20687;&#21435;&#26631;&#35782;&#21270;&#21644;&#28165;&#27927;&#21387;&#32553;&#65292;&#20026;&#28145;&#24230;&#23398;&#20064;&#21644;&#25968;&#25454;&#20849;&#20139;&#24212;&#29992;&#25552;&#20379;&#22270;&#20687;&#25968;&#25454;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2304.12322</link><description>&lt;p&gt;
&#20351;&#29992;Pylogik&#36827;&#34892;&#21307;&#23398;&#24433;&#20687;&#21435;&#26631;&#35782;&#21270;&#21644;&#28165;&#27927;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Medical Image Deidentification, Cleaning and Compression Using Pylogik. (arXiv:2304.12322v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12322
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;Python&#26694;&#26550;&#19979;&#30340;&#24211;PyLogik&#26469;&#24110;&#21161;&#36229;&#22768;&#22270;&#20687;&#21435;&#26631;&#35782;&#21270;&#21644;&#28165;&#27927;&#21387;&#32553;&#65292;&#20026;&#28145;&#24230;&#23398;&#20064;&#21644;&#25968;&#25454;&#20849;&#20139;&#24212;&#29992;&#25552;&#20379;&#22270;&#20687;&#25968;&#25454;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24212;&#29992;&#22823;&#25968;&#25454;&#21644;&#26426;&#22120;&#23398;&#20064;&#22312;&#21307;&#30103;&#35760;&#24405;&#20449;&#24687;&#26041;&#38754;&#39035;&#27880;&#24847;&#65292;&#24517;&#39035;&#28165;&#27927;&#21644;&#21435;&#26631;&#35782;&#21270;&#25968;&#25454;&#12290;&#24403;&#21463;&#20445;&#25252;&#30340;&#20581;&#24247;&#20449;&#24687;&#23884;&#20837;&#22312;&#24433;&#20687;&#20803;&#25968;&#25454;&#20013;&#26102;&#65292;&#20419;&#36827;&#22810;&#20013;&#24515;&#21512;&#20316;&#20013;&#25968;&#25454;&#20849;&#20139;&#21644;&#21327;&#35843;&#21464;&#24471;&#23588;&#20854;&#22256;&#38590;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;Python&#26694;&#26550;&#19979;&#30340;&#24211;&#65292;&#31216;&#20026;PyLogik&#65292;&#24110;&#21161;&#35299;&#20915;&#36229;&#22768;&#22270;&#20687;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#28165;&#27927;&#38382;&#39064;&#65292;&#22240;&#20026;&#36825;&#20123;&#22270;&#20687;&#30452;&#25509;&#21253;&#21547;&#24456;&#22810;PHI&#12290;PyLogik&#36890;&#36807;&#19968;&#31995;&#21015;&#30340;&#25991;&#26412;&#26816;&#27979;/&#25552;&#21462;&#12289;&#36807;&#28388;&#12289;&#38408;&#20540;&#21270;&#12289;&#24418;&#24577;&#23398;&#21644;&#36718;&#24275;&#27604;&#36739;&#22788;&#29702;&#22270;&#20687;&#20307;&#31215;&#12290;&#36825;&#31181;&#26041;&#27861;&#21435;&#26631;&#35782;&#21270;&#22270;&#20687;&#65292;&#20943;&#23567;&#25991;&#20214;&#22823;&#23567;&#65292;&#24182;&#20026;&#28145;&#24230;&#23398;&#20064;&#21644;&#25968;&#25454;&#20849;&#20139;&#24212;&#29992;&#20934;&#22791;&#22909;&#20102;&#22270;&#20687;&#25968;&#25454;&#12290;&#20026;&#20102;&#35780;&#20272;PyLogik&#22312;&#20852;&#36259;&#21306;&#22495;&#65288;ROI&#65289;&#30340;&#35782;&#21035;&#26377;&#25928;&#24615;&#65292;&#38543;&#26426;&#25277;&#21462;&#20102;50&#24352;&#24515;&#33039;&#36229;&#22768;&#22270;&#20687;&#65288;&#36229;&#22768;&#24515;&#21160;&#22270;&#65289;&#36827;&#34892;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Leveraging medical record information in the era of big data and machine learning comes with the caveat that data must be cleaned and deidentified. Facilitating data sharing and harmonization for multi-center collaborations are particularly difficult when protected health information (PHI) is contained or embedded in image meta-data. We propose a novel library in the Python framework, called PyLogik, to help alleviate this issue for ultrasound images, which are particularly challenging because of the frequent inclusion of PHI directly on the images. PyLogik processes the image volumes through a series of text detection/extraction, filtering, thresholding, morphological and contour comparisons. This methodology deidentifies the images, reduces file sizes, and prepares image volumes for applications in deep learning and data sharing. To evaluate its effectiveness in the identification of regions of interest (ROI), a random sample of 50 cardiac ultrasounds (echocardiograms) were processed
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;Skip-gram&#33410;&#28857;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#35745;&#31639;&#26725;&#25509;&#24230;&#35782;&#21035;&#37325;&#35201;&#33410;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#22522;&#20110;&#26799;&#24230;&#30340;&#35299;&#37322;&#26041;&#27861;GRAPH-wGD&#65292;&#26377;&#25928;&#22320;&#25552;&#20379;&#20840;&#23616;&#24615;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2304.12036</link><description>&lt;p&gt;
&#36890;&#36807;&#35782;&#21035;&#26725;&#25509;&#24230;&#37325;&#35201;&#33410;&#28857;&#29983;&#25104;Skip-gram&#33410;&#28857;&#23884;&#20837;&#30340;&#21518;&#32493;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Generating Post-hoc Explanations for Skip-gram-based Node Embeddings by Identifying Important Nodes with Bridgeness. (arXiv:2304.12036v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12036
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;Skip-gram&#33410;&#28857;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#35745;&#31639;&#26725;&#25509;&#24230;&#35782;&#21035;&#37325;&#35201;&#33410;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#22522;&#20110;&#26799;&#24230;&#30340;&#35299;&#37322;&#26041;&#27861;GRAPH-wGD&#65292;&#26377;&#25928;&#22320;&#25552;&#20379;&#20840;&#23616;&#24615;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#20013;&#30340;&#33410;&#28857;&#34920;&#31034;&#23398;&#20064;&#26159;&#32534;&#30721;&#36830;&#32493;&#30690;&#37327;&#31354;&#38388;&#20013;&#30340;&#20851;&#31995;&#20449;&#24687;&#21516;&#26102;&#20445;&#30041;&#32593;&#32476;&#22266;&#26377;&#23646;&#24615;&#21644;&#32467;&#26500;&#30340;&#37325;&#35201;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#12290;&#26368;&#36817;&#65292;DeepWalk&#12289;LINE&#12289;struc2vec&#12289;PTE&#12289;UserItem2vec&#21644;RWJBG&#31561;&#26080;&#30417;&#30563;&#33410;&#28857;&#23884;&#20837;&#26041;&#27861;&#20174;Skip-gram&#27169;&#22411;&#20013;&#20986;&#29616;&#65292;&#24182;&#22312;&#35832;&#22914;&#33410;&#28857;&#20998;&#31867;&#21644;&#38142;&#25509;&#39044;&#27979;&#31561;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#36866;&#29992;&#20110;&#23884;&#20837;&#30340;&#35299;&#37322;&#26041;&#27861;&#21644;&#29702;&#35770;&#30740;&#31350;&#65292;&#25552;&#20379;Skip-gram&#23884;&#20837;&#30340;&#21518;&#32493;&#35299;&#37322;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#39318;&#20808;&#34920;&#26126;&#21487;&#20197;&#36890;&#36807;&#22312;&#35889;&#32858;&#31867;&#24863;&#30693;&#23616;&#37096;&#25200;&#21160;&#19979;&#35745;&#31639;&#26725;&#25509;&#24230;&#26469;&#25214;&#21040;Skip-gram&#23884;&#20837;&#30340;&#20840;&#23616;&#35299;&#37322;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GRAPH-wGD&#30340;&#26032;&#22411;&#22522;&#20110;&#26799;&#24230;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#20801;&#35768;&#26816;&#32034;top-q&#20840;&#23616;&#24615;&#35299;&#37322;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Node representation learning in a network is an important machine learning technique for encoding relational information in a continuous vector space while preserving the inherent properties and structures of the network. Recently, unsupervised node embedding methods such as DeepWalk, LINE, struc2vec, PTE, UserItem2vec, and RWJBG have emerged from the Skip-gram model and perform better performance in several downstream tasks such as node classification and link prediction than the existing relational models. However, providing post-hoc explanations of Skip-gram-based embeddings remains a challenging problem because of the lack of explanation methods and theoretical studies applicable for embeddings. In this paper, we first show that global explanations to the Skip-gram-based embeddings can be found by computing bridgeness under a spectral cluster-aware local perturbation. Moreover, a novel gradient-based explanation method, which we call GRAPH-wGD, is proposed that allows the top-q glo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20248;&#21270;&#31995;&#25968;&#26469;&#21152;&#36895;&#27969;&#34892;&#30340;&#22522;&#20110;&#21453;&#21521;ODE&#35299;&#31639;&#30340;&#25193;&#25955;&#37319;&#26679;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#20248;&#21270;&#26041;&#27861;&#20026;&#25913;&#36827;&#30340;&#31215;&#20998;&#36924;&#36817;&#65288;IIA&#65289;&#65292;&#22312;&#27599;&#20010;&#21453;&#21521;&#26102;&#38388;&#27493;&#38271;&#65292;&#25105;&#20204;&#24314;&#35758;&#38024;&#23545;&#26576;&#20123;&#36873;&#25321;&#30340;&#31995;&#25968;&#26368;&#23567;&#21270;MSE&#20989;&#25968;&#65292;&#32473;&#23450;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#21482;&#38656;&#35201;&#22312;&#19968;&#25209;&#26679;&#26412;&#19978;&#36827;&#34892;&#29305;&#23450;&#25968;&#37327;&#30340;&#31070;&#32463;&#21151;&#33021;&#35780;&#20272;&#65288;NFEs&#65289;&#19968;&#27425;IIA&#36807;&#31243;&#21363;&#21487;&#33719;&#24471;&#26368;&#20339;&#35299;&#12290;</title><link>http://arxiv.org/abs/2304.11328</link><description>&lt;p&gt;
&#36890;&#36807;&#25913;&#36827;&#30340;&#31215;&#20998;&#36924;&#36817;&#21152;&#36895;&#25193;&#25955;&#37319;&#26679;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
On Accelerating Diffusion-Based Sampling Process via Improved Integration Approximation. (arXiv:2304.11328v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11328
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20248;&#21270;&#31995;&#25968;&#26469;&#21152;&#36895;&#27969;&#34892;&#30340;&#22522;&#20110;&#21453;&#21521;ODE&#35299;&#31639;&#30340;&#25193;&#25955;&#37319;&#26679;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#20248;&#21270;&#26041;&#27861;&#20026;&#25913;&#36827;&#30340;&#31215;&#20998;&#36924;&#36817;&#65288;IIA&#65289;&#65292;&#22312;&#27599;&#20010;&#21453;&#21521;&#26102;&#38388;&#27493;&#38271;&#65292;&#25105;&#20204;&#24314;&#35758;&#38024;&#23545;&#26576;&#20123;&#36873;&#25321;&#30340;&#31995;&#25968;&#26368;&#23567;&#21270;MSE&#20989;&#25968;&#65292;&#32473;&#23450;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#21482;&#38656;&#35201;&#22312;&#19968;&#25209;&#26679;&#26412;&#19978;&#36827;&#34892;&#29305;&#23450;&#25968;&#37327;&#30340;&#31070;&#32463;&#21151;&#33021;&#35780;&#20272;&#65288;NFEs&#65289;&#19968;&#27425;IIA&#36807;&#31243;&#21363;&#21487;&#33719;&#24471;&#26368;&#20339;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#27969;&#34892;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#37319;&#26679;&#31574;&#30053;&#23581;&#35797;&#26377;&#25928;&#22320;&#35299;&#20915;&#21453;&#21521;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODEs&#65289;&#12290;&#25152;&#24471;ODE&#27714;&#35299;&#22120;&#30340;&#31995;&#25968;&#30001;ODE&#20844;&#24335;&#65292;&#21453;&#21521;&#31163;&#25955;&#30340;&#26102;&#38388;&#27493;&#38271;&#21644;&#20351;&#29992;&#30340;ODE&#26041;&#27861;&#39044;&#20808;&#30830;&#23450;&#12290;&#26412;&#25991;&#32771;&#34385;&#36890;&#36807;&#20248;&#21270;&#26576;&#20123;&#31995;&#25968;&#26469;&#21152;&#36895;&#20960;&#31181;&#27969;&#34892;&#30340;&#22522;&#20110;ODE&#30340;&#37319;&#26679;&#36807;&#31243;&#65292;&#20248;&#21270;&#26041;&#27861;&#20026;&#25913;&#36827;&#30340;&#31215;&#20998;&#36924;&#36817;&#65288;IIA&#65289;&#12290;&#22312;&#27599;&#20010;&#21453;&#21521;&#26102;&#38388;&#27493;&#38271;&#65292;&#25105;&#20204;&#24314;&#35758;&#38024;&#23545;&#26576;&#20123;&#36873;&#25321;&#30340;&#31995;&#25968;&#26368;&#23567;&#21270;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#20989;&#25968;&#12290;&#36890;&#36807;&#24212;&#29992;&#19968;&#32452;&#32454;&#31890;&#24230;&#26102;&#38388;&#27493;&#38271;&#30340;&#21407;&#22987;ODE&#27714;&#35299;&#22120;&#26500;&#36896;MSE&#65292;&#20174;&#21407;&#29702;&#19978;&#25552;&#20379;&#20102;&#26356;&#31934;&#30830;&#30340;&#31215;&#20998;&#36924;&#36817;&#65292;&#20197;&#39044;&#27979;&#19979;&#19968;&#20010;&#25193;&#25955;&#38544;&#34255;&#29366;&#24577;&#65292;&#32473;&#23450;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#21482;&#38656;&#35201;&#22312;&#19968;&#25209;&#26679;&#26412;&#19978;&#36827;&#34892;&#29305;&#23450;&#25968;&#37327;&#30340;&#31070;&#32463;&#21151;&#33021;&#35780;&#20272;&#65288;NFEs&#65289;&#19968;&#27425;IIA&#36807;&#31243;&#21363;&#21487;&#33719;&#24471;&#26368;&#20339;&#35299;
&lt;/p&gt;
&lt;p&gt;
One popular diffusion-based sampling strategy attempts to solve the reverse ordinary differential equations (ODEs) effectively. The coefficients of the obtained ODE solvers are pre-determined by the ODE formulation, the reverse discrete timesteps, and the employed ODE methods. In this paper, we consider accelerating several popular ODE-based sampling processes by optimizing certain coefficients via improved integration approximation (IIA). At each reverse timestep, we propose to minimize a mean squared error (MSE) function with respect to certain selected coefficients. The MSE is constructed by applying the original ODE solver for a set of fine-grained timesteps which in principle provides a more accurate integration approximation in predicting the next diffusion hidden state. Given a pre-trained diffusion model, the procedure for IIA for a particular number of neural functional evaluations (NFEs) only needs to be conducted once over a batch of samples. The obtained optimal solutions f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39063;&#31890;&#29699;&#35745;&#31639;&#30340;&#33258;&#36866;&#24212;&#22810;&#31890;&#24230;&#34920;&#31034;&#21644;&#35745;&#31639;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#30340;&#25928;&#29575;&#12289;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.11171</link><description>&lt;p&gt;
&#39063;&#31890;&#29699;&#35745;&#31639;&#65306;&#19968;&#31181;&#39640;&#25928;&#12289;&#40065;&#26834;&#21644;&#21487;&#35299;&#37322;&#30340;&#33258;&#36866;&#24212;&#22810;&#31890;&#24230;&#34920;&#31034;&#21644;&#35745;&#31639;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Granular ball computing: an efficient, robust, and interpretable adaptive multi-granularity representation and computation method. (arXiv:2304.11171v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11171
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39063;&#31890;&#29699;&#35745;&#31639;&#30340;&#33258;&#36866;&#24212;&#22810;&#31890;&#24230;&#34920;&#31034;&#21644;&#35745;&#31639;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#30340;&#25928;&#29575;&#12289;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#35748;&#30693;&#20855;&#26377;&#8220;&#20808;&#22823;&#21518;&#23567;&#8221;&#30340;&#35748;&#30693;&#26426;&#21046;&#65292;&#22240;&#27492;&#20855;&#26377;&#33258;&#36866;&#24212;&#30340;&#22810;&#31890;&#24230;&#25551;&#36848;&#33021;&#21147;&#12290;&#36825;&#23548;&#33268;&#20102;&#26377;&#25928;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#31561;&#35745;&#31639;&#29305;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#39063;&#31890;&#29699;&#35745;&#31639;&#30340;&#33258;&#36866;&#24212;&#22810;&#31890;&#24230;&#34920;&#31034;&#21644;&#35745;&#31639;&#26041;&#27861;&#12290;&#20182;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#20960;&#20010;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65292;&#24182;&#35777;&#26126;&#20854;&#30456;&#23545;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human cognition has a ``large-scale first'' cognitive mechanism, therefore possesses adaptive multi-granularity description capabilities. This results in computational characteristics such as efficiency, robustness, and interpretability. Although most existing artificial intelligence learning methods have certain multi-granularity features, they do not fully align with the ``large-scale first'' cognitive mechanism. Multi-granularity granular-ball computing is an important model method developed in recent years. This method can use granular-balls of different sizes to adaptively represent and cover the sample space, and perform learning based on granular-balls. Since the number of coarse-grained "granular-ball" is smaller than the number of sample points, granular-ball computing is more efficient; the coarse-grained characteristics of granular-balls are less likely to be affected by fine-grained sample points, making them more robust; the multi-granularity structure of granular-balls ca
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#20132;&#36890;&#36335;&#32593;&#20013;&#32570;&#23569;&#20256;&#24863;&#22120;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#34917;&#20805;&#27969;&#37327;&#29366;&#24577;&#25110;&#29366;&#24577;&#21644;&#21160;&#20316;&#26469;&#23454;&#29616;&#33258;&#36866;&#24212;&#25511;&#21046;&#21644;&#26465;&#20214;&#34701;&#21512;&#12290;</title><link>http://arxiv.org/abs/2304.10722</link><description>&lt;p&gt;
&#32570;&#22833;&#25968;&#25454;&#19979;&#30340;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning Approaches for Traffic Signal Control under Missing Data. (arXiv:2304.10722v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10722
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#20132;&#36890;&#36335;&#32593;&#20013;&#32570;&#23569;&#20256;&#24863;&#22120;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#34917;&#20805;&#27969;&#37327;&#29366;&#24577;&#25110;&#29366;&#24577;&#21644;&#21160;&#20316;&#26469;&#23454;&#29616;&#33258;&#36866;&#24212;&#25511;&#21046;&#21644;&#26465;&#20214;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#24050;&#32463;&#21462;&#24471;&#20102;&#27604;&#20256;&#32479;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#20013;&#20132;&#36890;&#29366;&#24577;&#30340;&#32570;&#22833;&#21487;&#33021;&#32463;&#24120;&#21457;&#29983;&#65292;&#20351;&#24471;&#29616;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#32570;&#23569;&#20256;&#24863;&#22120;&#30340;&#36335;&#32593;&#19978;&#26080;&#27861;&#24212;&#29992;&#12290;&#26412;&#25991;&#26088;&#22312;&#25511;&#21046;&#20132;&#36890;&#20449;&#21495;&#22312;&#29616;&#23454;&#29615;&#22659;&#19979;&#30340;&#35774;&#32622;&#20013;&#65292;&#20854;&#20013;&#19968;&#20123;&#36335;&#21475;&#27809;&#26377;&#23433;&#35013;&#20256;&#24863;&#22120;&#65292;&#22240;&#27492;&#21608;&#22260;&#27809;&#26377;&#30452;&#25509;&#35266;&#23519;&#25968;&#25454;&#12290;&#22312;&#25105;&#20204;&#25152;&#30693;&#36947;&#30340;&#33539;&#22260;&#20869;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#38382;&#39064;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#35299;&#20915;&#26041;&#26696;&#65306;&#31532;&#19968;&#31181;&#26041;&#26696;&#34917;&#20805;&#27969;&#37327;&#29366;&#24577;&#20197;&#23454;&#29616;&#33258;&#36866;&#24212;&#25511;&#21046;&#65292;&#31532;&#20108;&#31181;&#26041;&#26696;&#34917;&#20805;&#29366;&#24577;&#21644;&#21160;&#20316;&#20197;&#36827;&#34892;&#26465;&#20214;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of reinforcement learning (RL) methods in traffic signal control tasks has achieved better performance than conventional rule-based approaches. Most RL approaches require the observation of the environment for the agent to decide which action is optimal for a long-term reward. However, in real-world urban scenarios, missing observation of traffic states may frequently occur due to the lack of sensors, which makes existing RL methods inapplicable on road networks with missing observation. In this work, we aim to control the traffic signals in a real-world setting, where some of the intersections in the road network are not installed with sensors and thus with no direct observations around them. To the best of our knowledge, we are the first to use RL methods to tackle the traffic signal control problem in this real-world setting. Specifically, we propose two solutions: the first one imputes the traffic states to enable adaptive control, and the second one imputes both stat
&lt;/p&gt;</description></item><item><title>Adam&#20248;&#21270;&#31639;&#27861;&#22312;&#22823;&#25209;&#37327;&#30340;&#35757;&#32451;&#19979;&#23481;&#26131;&#20986;&#29616;&#19981;&#31283;&#23450;&#29616;&#35937;&#65292;&#24182;&#23548;&#33268;&#35757;&#32451;&#24322;&#24120;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#35813;&#29616;&#35937;&#30340;&#29702;&#35770;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2304.09871</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#20013;Adam&#19981;&#31283;&#23450;&#24615;&#30340;&#29702;&#35770;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Theory on Adam Instability in Large-Scale Machine Learning. (arXiv:2304.09871v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09871
&lt;/p&gt;
&lt;p&gt;
Adam&#20248;&#21270;&#31639;&#27861;&#22312;&#22823;&#25209;&#37327;&#30340;&#35757;&#32451;&#19979;&#23481;&#26131;&#20986;&#29616;&#19981;&#31283;&#23450;&#29616;&#35937;&#65292;&#24182;&#23548;&#33268;&#35757;&#32451;&#24322;&#24120;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#35813;&#29616;&#35937;&#30340;&#29702;&#35770;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20043;&#21069;&#26410;&#34987;&#35299;&#37322;&#30340;&#29616;&#35937;&#30340;&#29702;&#35770;&#65292;&#35813;&#29616;&#35937;&#20986;&#29616;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#26102;&#30340;&#21457;&#25955;&#34892;&#20026;&#20013;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#29616;&#35937;&#26159;&#30001;&#20110;&#20027;&#27969;&#30340;&#20248;&#21270;&#31639;&#27861; Adam &#23548;&#33268;&#30340;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040; Adam &#21487;&#33021;&#20250;&#36827;&#20837;&#19968;&#31181;&#29366;&#24577;&#65292;&#20854;&#20013;&#21442;&#25968;&#26356;&#26032;&#21521;&#37327;&#26377;&#27604;&#36739;&#22823;&#30340;&#33539;&#25968;&#65292;&#24182;&#19988;&#19982;&#35757;&#32451;&#25439;&#22833;&#26223;&#35266;&#19979;&#30340;&#19979;&#38477;&#26041;&#21521;&#22522;&#26412;&#26080;&#20851;&#65292;&#20174;&#32780;&#23548;&#33268;&#21457;&#25955;&#12290;&#36825;&#31181;&#29616;&#35937;&#26356;&#23481;&#26131;&#22312;&#22823;&#25209;&#37327;&#24773;&#20917;&#19979;&#20986;&#29616;&#65292;&#36825;&#20063;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#30340;&#20856;&#22411;&#35774;&#32622;&#12290;&#20026;&#20102;&#35777;&#26126;&#35813;&#29702;&#35770;&#65292;&#25105;&#20204;&#23545;&#35268;&#27169;&#19981;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;70&#20159;&#65292;300&#20159;&#65292;650&#20159;&#21644;5460&#20159;&#21442;&#25968;&#65289;&#36827;&#34892;&#20102;&#35757;&#32451;&#36816;&#34892;&#30340;&#35266;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a theory for the previously unexplained divergent behavior noticed in the training of large language models. We argue that the phenomenon is an artifact of the dominant optimization algorithm used for training, called Adam. We observe that Adam can enter a state in which the parameter update vector has a relatively large norm and is essentially uncorrelated with the direction of descent on the training loss landscape, leading to divergence. This artifact is more likely to be observed in the training of a deep model with a large batch size, which is the typical setting of large-scale language model training. To argue the theory, we present observations from the training runs of the language models of different scales: 7 billion, 30 billion, 65 billion, and 546 billion parameters.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#20013;&#30340;&#23376;&#35789;&#20998;&#35789;&#25216;&#26415;&#22312;&#29983;&#25104;&#26356;&#38271;&#12289;&#26356;&#20855;&#32467;&#26500;&#30340;&#38899;&#20048;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;BPE&#26041;&#27861;&#22312;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#20013;&#21487;&#34892;&#19988;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2304.08953</link><description>&lt;p&gt;
&#20174;&#21333;&#35789;&#21040;&#38899;&#20048;&#65306;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#20013;&#30340;&#23376;&#35789;&#20998;&#35789;&#25216;&#26415;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
From Words to Music: A Study of Subword Tokenization Techniques in Symbolic Music Generation. (arXiv:2304.08953v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08953
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#20013;&#30340;&#23376;&#35789;&#20998;&#35789;&#25216;&#26415;&#22312;&#29983;&#25104;&#26356;&#38271;&#12289;&#26356;&#20855;&#32467;&#26500;&#30340;&#38899;&#20048;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;BPE&#26041;&#27861;&#22312;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#20013;&#21487;&#34892;&#19988;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23376;&#35789;&#20998;&#35789;&#22312;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#24050;&#32463;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;&#38543;&#30528;Transformer&#27169;&#22411;&#22312;&#31526;&#21495;&#38899;&#20048;&#39046;&#22495;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#26222;&#21450;&#65292;&#25506;&#31350;&#23376;&#35789;&#20998;&#35789;&#22312;&#31526;&#21495;&#38899;&#20048;&#39046;&#22495;&#30340;&#26377;&#25928;&#24615;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#20013;&#30340;&#23376;&#35789;&#20998;&#35789;&#25216;&#26415;&#65288;&#22914;&#23383;&#33410;&#23545;&#32534;&#30721;BPE&#65289;&#21450;&#20854;&#23545;&#25152;&#29983;&#25104;&#27468;&#26354;&#25972;&#20307;&#32467;&#26500;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#22522;&#20110;&#19977;&#31181;MIDI&#25968;&#25454;&#38598;&#65306;&#21333;&#38899;&#36712;&#26059;&#24459;&#12289;&#22810;&#36712;&#21333;&#20048;&#22120;&#21644;&#22810;&#36712;&#22810;&#20048;&#22120;&#12290;&#25105;&#20204;&#22312;&#38899;&#20048;&#26631;&#35760;&#21270;&#20043;&#21518;&#24212;&#29992;&#23376;&#35789;&#20998;&#35789;&#26041;&#26696;&#65292;&#24182;&#21457;&#29616;&#23427;&#21487;&#20197;&#22312;&#30456;&#21516;&#26102;&#38388;&#20869;&#29983;&#25104;&#26356;&#38271;&#30340;&#27468;&#26354;&#65292;&#24182;&#19988;&#22312;&#32467;&#26500;&#25351;&#26631;&#65288;SI&#65289;&#12289;&#38899;&#39640;&#32423;&#21035;&#20449;&#24687;&#29109;&#31561;&#23458;&#35266;&#25351;&#26631;&#26041;&#38754;&#25913;&#21892;&#20102;&#29983;&#25104;&#30340;&#38899;&#20048;&#25972;&#20307;&#32467;&#26500;&#12290;&#25105;&#20204;&#36824;&#27604;&#36739;&#20102;&#20004;&#31181;&#23376;&#35789;&#20998;&#35789;&#26041;&#27861;&#65292;BPE&#21644;&#19968;&#31181;&#22522;&#20110;&#38899;&#39640;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;BPE&#26041;&#27861;&#22312;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#20013;&#21487;&#34892;&#19988;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Subword tokenization has been widely successful in text-based natural language processing (NLP) tasks with Transformer-based models. As Transformer models become increasingly popular in symbolic music-related studies, it is imperative to investigate the efficacy of subword tokenization in the symbolic music domain. In this paper, we explore subword tokenization techniques, such as byte-pair encoding (BPE), in symbolic music generation and its impact on the overall structure of generated songs. Our experiments are based on three types of MIDI datasets: single track-melody only, multi-track with a single instrument, and multi-track and multi-instrument. We apply subword tokenization on post-musical tokenization schemes and find that it enables the generation of longer songs at the same time and improves the overall structure of the generated music in terms of objective metrics like structure indicator (SI), Pitch Class Entropy, etc. We also compare two subword tokenization methods, BPE a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;CapsNet&#22312;&#20223;&#23556;&#21464;&#25442;&#21644;&#23545;&#25239;&#24615;&#25915;&#20987;&#26041;&#38754;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;MNIST&#65292;GTSRB&#21644;CIFAR10&#25968;&#25454;&#38598;&#19978;&#30340;&#27979;&#35797;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#20256;&#32479;CNN&#30456;&#27604;&#65292;CapsNet&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#23545;&#25239;&#31034;&#20363;&#21644;&#20223;&#23556;&#21464;&#25442;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.03973</link><description>&lt;p&gt;
RobCaps: &#35780;&#20272;&#33014;&#22218;&#32593;&#32476;&#22312;&#20223;&#23556;&#21464;&#25442;&#21644;&#23545;&#25239;&#24615;&#25915;&#20987;&#20013;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
RobCaps: Evaluating the Robustness of Capsule Networks against Affine Transformations and Adversarial Attacks. (arXiv:2304.03973v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03973
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;CapsNet&#22312;&#20223;&#23556;&#21464;&#25442;&#21644;&#23545;&#25239;&#24615;&#25915;&#20987;&#26041;&#38754;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;MNIST&#65292;GTSRB&#21644;CIFAR10&#25968;&#25454;&#38598;&#19978;&#30340;&#27979;&#35797;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#20256;&#32479;CNN&#30456;&#27604;&#65292;CapsNet&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#23545;&#25239;&#31034;&#20363;&#21644;&#20223;&#23556;&#21464;&#25442;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33014;&#22218;&#32593;&#32476;(CapsNet)&#33021;&#22815;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#20998;&#23618;&#20445;&#25345;&#22810;&#20010;&#23545;&#35937;&#20043;&#38388;&#30340;&#23039;&#24577;&#20851;&#31995;&#12290;&#38500;&#20102;&#23454;&#29616;&#39640;&#20934;&#30830;&#24615;&#22806;&#65292;&#22312;&#23433;&#20840;&#20851;&#38190;&#30340;&#24212;&#29992;&#20013;&#37096;&#32626;CapsNet&#30340;&#21478;&#19968;&#20010;&#30456;&#20851;&#22240;&#32032;&#26159;&#20854;&#23545;&#36755;&#20837;&#21464;&#25442;&#21644;&#24694;&#24847;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#20998;&#26512;&#21644;&#35780;&#20272;&#20102;&#24433;&#21709;CapsNet&#40065;&#26834;&#24615;&#30340;&#19981;&#21516;&#22240;&#32032;&#65292;&#19982;&#20256;&#32479;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNNs)&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#20026;&#20102;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#65292;&#25105;&#20204;&#22312;MNIST&#65292;GTSRB&#21644;CIFAR10&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#20004;&#20010;CapsNet&#27169;&#22411;&#21644;&#20004;&#20010;CNN&#27169;&#22411;&#65292;&#20197;&#21450;&#36825;&#20123;&#25968;&#25454;&#38598;&#30340;&#20223;&#23556;&#21464;&#25442;&#29256;&#26412;&#12290;&#36890;&#36807;&#28145;&#20837;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#26550;&#26500;&#30340;&#21738;&#20123;&#29305;&#24615;&#26356;&#26377;&#21161;&#20110;&#22686;&#21152;&#20854;&#40065;&#26834;&#24615;&#20197;&#21450;&#20854;&#23616;&#38480;&#24615;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#19982;&#31867;&#20284;&#25968;&#37327;&#30340;&#20256;&#32479;CNN&#30456;&#27604;&#65292;CapsNet&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#23545;&#25239;&#31034;&#20363;&#21644;&#20223;&#23556;&#21464;&#25442;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Capsule Networks (CapsNets) are able to hierarchically preserve the pose relationships between multiple objects for image classification tasks. Other than achieving high accuracy, another relevant factor in deploying CapsNets in safety-critical applications is the robustness against input transformations and malicious adversarial attacks.  In this paper, we systematically analyze and evaluate different factors affecting the robustness of CapsNets, compared to traditional Convolutional Neural Networks (CNNs). Towards a comprehensive comparison, we test two CapsNet models and two CNN models on the MNIST, GTSRB, and CIFAR10 datasets, as well as on the affine-transformed versions of such datasets. With a thorough analysis, we show which properties of these architectures better contribute to increasing the robustness and their limitations. Overall, CapsNets achieve better robustness against adversarial examples and affine transformations, compared to a traditional CNN with a similar number 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26377;&#36873;&#25321;&#30340;&#32852;&#37030;&#33976;&#39311;&#26426;&#21046;Selective-FD&#65292;&#21487;&#20197;&#31934;&#30830;&#22320;&#35782;&#21035;&#26469;&#33258;&#26412;&#22320;&#21644;&#38598;&#21512;&#39044;&#27979;&#30340;&#30693;&#35782;&#65292;&#20197;&#35299;&#20915;&#23616;&#37096;&#25968;&#25454;&#20998;&#24067;&#30340;&#21464;&#24322;&#21644;&#32570;&#20047;&#22909;&#30340;&#25945;&#24072;&#27169;&#22411;&#32780;&#23548;&#33268;&#30340;&#35823;&#23548;&#21644;&#27169;&#31946;&#30340;&#30693;&#35782;&#20849;&#20139;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#25552;&#39640;&#30340;&#27169;&#22411;&#24615;&#33021;&#21644;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.01731</link><description>&lt;p&gt;
&#38754;&#21521;&#38544;&#31169;&#20445;&#25252;&#32852;&#37030;&#33976;&#39311;&#30340;&#26377;&#36873;&#25321;&#30693;&#35782;&#20849;&#20139;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Selective Knowledge Sharing for Privacy-Preserving Federated Distillation without A Good Teacher. (arXiv:2304.01731v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26377;&#36873;&#25321;&#30340;&#32852;&#37030;&#33976;&#39311;&#26426;&#21046;Selective-FD&#65292;&#21487;&#20197;&#31934;&#30830;&#22320;&#35782;&#21035;&#26469;&#33258;&#26412;&#22320;&#21644;&#38598;&#21512;&#39044;&#27979;&#30340;&#30693;&#35782;&#65292;&#20197;&#35299;&#20915;&#23616;&#37096;&#25968;&#25454;&#20998;&#24067;&#30340;&#21464;&#24322;&#21644;&#32570;&#20047;&#22909;&#30340;&#25945;&#24072;&#27169;&#22411;&#32780;&#23548;&#33268;&#30340;&#35823;&#23548;&#21644;&#27169;&#31946;&#30340;&#30693;&#35782;&#20849;&#20139;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#25552;&#39640;&#30340;&#27169;&#22411;&#24615;&#33021;&#21644;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#30340;&#21327;&#20316;&#23398;&#20064;&#26041;&#27861;&#65292;&#20294;&#26159;&#23481;&#26131;&#21463;&#21040;&#30333;&#30418;&#25915;&#20987;&#65292;&#24182;&#19988;&#38590;&#20197;&#36866;&#24212;&#24322;&#26500;&#23458;&#25143;&#31471;&#12290;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;&#32852;&#37030;&#33976;&#39311;&#26159;&#19968;&#31181;&#25552;&#20379;&#22686;&#24378;&#38544;&#31169;&#20445;&#35777;&#24182;&#35299;&#20915;&#27169;&#22411;&#24322;&#26500;&#24615;&#30340;&#26367;&#20195;&#33539;&#20363;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#36873;&#25321;&#30340;&#32852;&#37030;&#33976;&#39311;&#26426;&#21046;Selective-FD&#26469;&#24212;&#23545;&#23616;&#37096;&#25968;&#25454;&#20998;&#24067;&#30340;&#21464;&#24322;&#21644;&#32570;&#20047;&#22909;&#30340;&#25945;&#24072;&#27169;&#22411;&#32780;&#23548;&#33268;&#30340;&#35823;&#23548;&#21644;&#27169;&#31946;&#30340;&#30693;&#35782;&#20849;&#20139;&#38382;&#39064;&#12290;&#23427;&#21253;&#25324;&#23458;&#25143;&#31471;&#36873;&#25321;&#22120;&#21644;&#26381;&#21153;&#22120;&#36873;&#25321;&#22120;&#65292;&#20197;&#31934;&#30830;&#22320;&#35782;&#21035;&#26469;&#33258;&#26412;&#22320;&#21644;&#38598;&#21512;&#39044;&#27979;&#30340;&#30693;&#35782;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;Selective-FD&#21487;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#21644;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
While federated learning is promising for privacy-preserving collaborative learning without revealing local data, it remains vulnerable to white-box attacks and struggles to adapt to heterogeneous clients. Federated distillation (FD), built upon knowledge distillation--an effective technique for transferring knowledge from a teacher model to student models--emerges as an alternative paradigm, which provides enhanced privacy guarantees and addresses model heterogeneity. Nevertheless, challenges arise due to variations in local data distributions and the absence of a well-trained teacher model, which leads to misleading and ambiguous knowledge sharing that significantly degrades model performance. To address these issues, this paper proposes a selective knowledge sharing mechanism for FD, termed Selective-FD. It includes client-side selectors and a server-side selector to accurately and precisely identify knowledge from local and ensemble predictions, respectively. Empirical studies, bac
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#23610;&#24230;&#19981;&#21464;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#19981;&#20351;&#29992;&#31354;&#38388;&#21367;&#31215;&#25110;&#20998;&#23618;&#26550;&#26500;&#65292;&#36890;&#36807;&#23558;&#23618;&#20869;&#29305;&#24449;&#22270;&#20998;&#35299;&#20026;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#24182;&#36890;&#36807;&#25554;&#20540;&#36825;&#20123;&#29305;&#24449;&#29983;&#25104;&#22270;&#20687;&#65292;&#20351;&#24471;&#27169;&#22411;&#20855;&#26377;&#36739;&#23567;&#30340;&#20869;&#23384;&#21344;&#29992;&#21644;&#26356;&#24555;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.14157</link><description>&lt;p&gt;
&#19968;&#31181;&#20855;&#26377;&#34892;&#21015;&#20132;&#38169;&#20687;&#32032;&#21512;&#25104;&#30340;&#39640;&#25928;&#23610;&#24230;&#19981;&#21464;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
Efficient Scale-Invariant Generator with Column-Row Entangled Pixel Synthesis. (arXiv:2303.14157v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14157
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#23610;&#24230;&#19981;&#21464;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#19981;&#20351;&#29992;&#31354;&#38388;&#21367;&#31215;&#25110;&#20998;&#23618;&#26550;&#26500;&#65292;&#36890;&#36807;&#23558;&#23618;&#20869;&#29305;&#24449;&#22270;&#20998;&#35299;&#20026;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#24182;&#36890;&#36807;&#25554;&#20540;&#36825;&#20123;&#29305;&#24449;&#29983;&#25104;&#22270;&#20687;&#65292;&#20351;&#24471;&#27169;&#22411;&#20855;&#26377;&#36739;&#23567;&#30340;&#20869;&#23384;&#21344;&#29992;&#21644;&#26356;&#24555;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#24847;&#27604;&#20363;&#22270;&#20687;&#21512;&#25104;&#20026;&#21512;&#25104;&#22312;&#20219;&#24847;&#27604;&#20363;&#19979;&#21512;&#25104;&#36924;&#30495;&#22270;&#20687;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#20351;&#36229;&#20986;&#20102;2K&#20998;&#36776;&#29575;&#33539;&#22260;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;GAN&#30340;&#35299;&#20915;&#26041;&#26696;&#36807;&#24230;&#20381;&#36182;&#20110;&#21367;&#31215;&#21644;&#20998;&#23618;&#26550;&#26500;&#65292;&#22312;&#32553;&#25918;&#36755;&#20986;&#20998;&#36776;&#29575;&#26102;&#20250;&#24341;&#20837;&#19981;&#19968;&#33268;&#24615;&#21644;&#8220;&#32441;&#29702;&#31896;&#36830;&#8221;&#38382;&#39064;&#12290;&#20174;&#21478;&#19968;&#20010;&#35282;&#24230;&#26469;&#30475;&#65292;&#22522;&#20110;INR&#30340;&#29983;&#25104;&#22120;&#20174;&#35774;&#35745;&#19978;&#26159;&#23610;&#24230;&#31561;&#21464;&#30340;&#65292;&#20294;&#23427;&#20204;&#24040;&#22823;&#30340;&#20869;&#23384;&#21344;&#29992;&#21644;&#32531;&#24930;&#30340;&#25512;&#29702;&#22952;&#30861;&#20102;&#36825;&#20123;&#32593;&#32476;&#22312;&#22823;&#35268;&#27169;&#25110;&#23454;&#26102;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#65306;&#20855;&#26377;&#34892;&#21015;&#20132;&#38169;&#20687;&#32032;&#21512;&#25104;&#30340;&#21015;-&#34892;&#20132;&#38169;&#20687;&#32032;&#21512;&#25104;&#65288;$\textbf{CREPS}$&#65289;&#12290;&#19981;&#20351;&#29992;&#20219;&#20309;&#31354;&#38388;&#21367;&#31215;&#25110;&#20174;&#31895;&#21040;&#32454;&#30340;&#35774;&#35745;&#12290;&#20026;&#20102;&#33410;&#30465;&#20869;&#23384;&#21344;&#29992;&#24182;&#20351;&#31995;&#32479;&#21487;&#25193;&#23637;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#32447;&#34920;&#31034;&#27861;&#65292;&#23558;&#23618;&#20869;&#29305;&#24449;&#22270;&#20998;&#35299;&#20026;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#30340;&#29420;&#31435;&#8220;&#21402;&#26465;&#8221;&#25554;&#20540;&#36825;&#20123;&#26465;&#24102;&#30340;&#34701;&#21512;&#26469;&#29983;&#25104;&#22270;&#20687;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;$\textbf{CREPS}$&#22312;&#22270;&#20687;&#36136;&#37327;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#65292;&#22312;&#20445;&#25345;&#26356;&#23567;&#30340;&#20869;&#23384;&#21344;&#29992;&#21644;&#26356;&#24555;&#30340;&#25512;&#29702;&#36895;&#24230;&#30340;&#21516;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Any-scale image synthesis offers an efficient and scalable solution to synthesize photo-realistic images at any scale, even going beyond 2K resolution. However, existing GAN-based solutions depend excessively on convolutions and a hierarchical architecture, which introduce inconsistency and the $``$texture sticking$"$ issue when scaling the output resolution. From another perspective, INR-based generators are scale-equivariant by design, but their huge memory footprint and slow inference hinder these networks from being adopted in large-scale or real-time systems. In this work, we propose $\textbf{C}$olumn-$\textbf{R}$ow $\textbf{E}$ntangled $\textbf{P}$ixel $\textbf{S}$ynthesis ($\textbf{CREPS}$), a new generative model that is both efficient and scale-equivariant without using any spatial convolutions or coarse-to-fine design. To save memory footprint and make the system scalable, we employ a novel bi-line representation that decomposes layer-wise feature maps into separate $``$thick
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#22797;&#21512;&#20248;&#21270;&#31639;&#27861;&#35299;&#20915;Sigmoid&#32593;&#32476;&#38382;&#39064;&#65292;&#21487;&#20197;&#22312;&#38750;&#20984;&#21644;&#38750;&#20809;&#28369;&#38382;&#39064;&#30340;&#24773;&#20917;&#19979;&#25910;&#25947;&#21040;&#20840;&#23616;&#26368;&#20248;&#35299;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#33324;&#25351;&#23548;&#26469;&#35774;&#32622;&#32593;&#32476;&#22823;&#23567;&#12290;</title><link>http://arxiv.org/abs/2303.00589</link><description>&lt;p&gt;
&#22522;&#20110;&#22797;&#21512;&#20248;&#21270;&#31639;&#27861;&#30340;Sigmoid&#32593;&#32476;&#27714;&#35299;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Composite Optimization Algorithms for Sigmoid Networks. (arXiv:2303.00589v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00589
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#22797;&#21512;&#20248;&#21270;&#31639;&#27861;&#35299;&#20915;Sigmoid&#32593;&#32476;&#38382;&#39064;&#65292;&#21487;&#20197;&#22312;&#38750;&#20984;&#21644;&#38750;&#20809;&#28369;&#38382;&#39064;&#30340;&#24773;&#20917;&#19979;&#25910;&#25947;&#21040;&#20840;&#23616;&#26368;&#20248;&#35299;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#33324;&#25351;&#23548;&#26469;&#35774;&#32622;&#32593;&#32476;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#22797;&#21512;&#20248;&#21270;&#31639;&#27861;&#27714;&#35299;Sigmoid&#32593;&#32476;&#38382;&#39064;&#65292;&#23558;Sigmoid&#32593;&#32476;&#36716;&#21270;&#20026;&#19968;&#20010;&#20984;&#22797;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#32447;&#24615;&#21270;&#36817;&#31471;&#31639;&#27861;&#21644;&#20132;&#26367;&#26041;&#21521;&#20056;&#23376;&#27861;&#30340;&#22797;&#21512;&#20248;&#21270;&#31639;&#27861;&#12290;&#22312;&#20551;&#35774;&#20855;&#26377;&#24369;&#38160;&#26368;&#23567;&#20540;&#21644;&#27491;&#21017;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#22312;&#38750;&#20984;&#21644;&#38750;&#20809;&#28369;&#38382;&#39064;&#30340;&#24773;&#20917;&#19979;&#25910;&#25947;&#21040;&#30446;&#26631;&#20989;&#25968;&#30340;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;&#27492;&#22806;&#65292;&#25910;&#25947;&#32467;&#26524;&#21487;&#20197;&#30452;&#25509;&#19982;&#35757;&#32451;&#25968;&#25454;&#37327;&#30456;&#20851;&#32852;&#65292;&#20026;&#35774;&#32622;Sigmoid&#32593;&#32476;&#30340;&#22823;&#23567;&#25552;&#20379;&#19968;&#33324;&#25351;&#23548;&#12290;&#22312;Franke&#20989;&#25968;&#25311;&#21512;&#21644;&#25163;&#20889;&#25968;&#23383;&#35782;&#21035;&#26041;&#38754;&#30340;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#34920;&#29616;&#33391;&#22909;&#19988;&#31283;&#20581;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we use composite optimization algorithms to solve sigmoid networks. We equivalently transfer the sigmoid networks to a convex composite optimization and propose the composite optimization algorithms based on the linearized proximal algorithms and the alternating direction method of multipliers. Under the assumptions of the weak sharp minima and the regularity condition, the algorithm is guaranteed to converge to a globally optimal solution of the objective function even in the case of non-convex and non-smooth problems. Furthermore, the convergence results can be directly related to the amount of training data and provide a general guide for setting the size of sigmoid networks. Numerical experiments on Franke's function fitting and handwritten digit recognition show that the proposed algorithms perform satisfactorily and robustly.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#40654;&#26364;&#23376;&#27969;&#24418;&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#20102;&#31616;&#21270;&#65292;&#25552;&#20986;&#20102;&#40654;&#26364;&#27491;&#24120;&#22352;&#26631;&#30340;&#24191;&#20041;&#29256;&#26412;&#65292;&#21487;&#29992;&#20110;&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#30340;&#23376;&#27969;&#24418;&#20248;&#21270;&#65292;&#24182;&#20026;&#28145;&#24230;&#23398;&#20064;&#24320;&#21457;&#20102;&#39640;&#25928;&#30340;&#20108;&#38454;&#20248;&#21270;&#22120;&#65292;&#26080;&#38656;&#26174;&#24335;&#30697;&#38453;&#27714;&#36870;&#12290;</title><link>http://arxiv.org/abs/2302.09738</link><description>&lt;p&gt;
&#31616;&#21270;&#22522;&#20110;&#21160;&#37327;&#30340;&#40654;&#26364;&#23376;&#27969;&#24418;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Simplifying Momentum-based Riemannian Submanifold Optimization. (arXiv:2302.09738v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09738
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#40654;&#26364;&#23376;&#27969;&#24418;&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#20102;&#31616;&#21270;&#65292;&#25552;&#20986;&#20102;&#40654;&#26364;&#27491;&#24120;&#22352;&#26631;&#30340;&#24191;&#20041;&#29256;&#26412;&#65292;&#21487;&#29992;&#20110;&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#30340;&#23376;&#27969;&#24418;&#20248;&#21270;&#65292;&#24182;&#20026;&#28145;&#24230;&#23398;&#20064;&#24320;&#21457;&#20102;&#39640;&#25928;&#30340;&#20108;&#38454;&#20248;&#21270;&#22120;&#65292;&#26080;&#38656;&#26174;&#24335;&#30697;&#38453;&#27714;&#36870;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24102;&#26377;&#21160;&#37327;&#30340;&#40654;&#26364;&#23376;&#27969;&#24418;&#20248;&#21270;&#22312;&#35745;&#31639;&#19978;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#30830;&#20445;&#36845;&#20195;&#20445;&#25345;&#22312;&#23376;&#27969;&#24418;&#19978;&#36890;&#24120;&#38656;&#35201;&#35299;&#20915;&#22256;&#38590;&#30340;&#24494;&#20998;&#26041;&#31243;&#12290;&#26412;&#25991;&#38024;&#23545;&#20855;&#26377;&#20223;&#23556;&#19981;&#21464;&#24230;&#37327;&#30340;&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#30340;&#23376;&#27969;&#24418;&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#20102;&#31616;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#40654;&#26364;&#27491;&#24120;&#22352;&#26631;&#30340;&#24191;&#20041;&#29256;&#26412;&#65292;&#21487;&#20197;&#23558;&#38382;&#39064;&#21160;&#24577;&#22320;&#31616;&#21270;&#20026;&#27431;&#20960;&#37324;&#24471;&#26080;&#32422;&#26463;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#26469;&#35299;&#37322;&#21644;&#31616;&#21270;&#29616;&#26377;&#30340;&#32467;&#26500;&#21270;&#21327;&#26041;&#24046;&#26041;&#27861;&#65292;&#24182;&#20026;&#28145;&#24230;&#23398;&#20064;&#24320;&#21457;&#20102;&#39640;&#25928;&#30340;&#20108;&#38454;&#20248;&#21270;&#22120;&#65292;&#32780;&#26080;&#38656;&#26174;&#24335;&#30697;&#38453;&#27714;&#36870;&#12290;
&lt;/p&gt;
&lt;p&gt;
Riemannian submanifold optimization with momentum is computationally challenging because ensuring iterates remain on the submanifold often requires solving difficult differential equations. We simplify such optimization algorithms for the submanifold of symmetric positive-definite matrices with the affine invariant metric. We propose a generalized version of the Riemannian normal coordinates which dynamically trivializes the problem into a Euclidean unconstrained problem. We use our approach to explain and simplify existing approaches for structured covariances and develop efficient second-order optimizers for deep learning without explicit matrix inverses.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21644;&#26234;&#33021;&#25163;&#34920;&#25968;&#25454;&#36827;&#34892;&#35748;&#30693;&#36127;&#33655;&#20272;&#35745;&#65292;&#30740;&#31350;&#21457;&#29616;&#28216;&#25103;&#21270;&#30340;&#33258;&#25105;&#25253;&#21578;&#26041;&#24335;&#19982;&#20256;&#32479;&#26041;&#24335;&#30340;&#35748;&#30693;&#36127;&#33655;&#27809;&#26377;&#24046;&#24322;&#65292;&#20294;&#21442;&#19982;&#32773;&#26356;&#21916;&#27426;&#28216;&#25103;&#21270;&#29256;&#26412;&#12290;</title><link>http://arxiv.org/abs/2302.03616</link><description>&lt;p&gt;
&#28216;&#25103;&#21270;&#33021;&#21542;&#20943;&#36731;mHealth&#24212;&#29992;&#20013;&#33258;&#25105;&#25253;&#21578;&#30340;&#36127;&#25285;&#65311;&#21033;&#29992;&#26234;&#33021;&#25163;&#34920;&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#35748;&#30693;&#36127;&#33655;&#20272;&#35745;&#30340;&#21487;&#34892;&#24615;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can gamification reduce the burden of self-reporting in mHealth applications? A feasibility study using machine learning from smartwatch data to estimate cognitive load. (arXiv:2302.03616v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03616
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21644;&#26234;&#33021;&#25163;&#34920;&#25968;&#25454;&#36827;&#34892;&#35748;&#30693;&#36127;&#33655;&#20272;&#35745;&#65292;&#30740;&#31350;&#21457;&#29616;&#28216;&#25103;&#21270;&#30340;&#33258;&#25105;&#25253;&#21578;&#26041;&#24335;&#19982;&#20256;&#32479;&#26041;&#24335;&#30340;&#35748;&#30693;&#36127;&#33655;&#27809;&#26377;&#24046;&#24322;&#65292;&#20294;&#21442;&#19982;&#32773;&#26356;&#21916;&#27426;&#28216;&#25103;&#21270;&#29256;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#21270;&#27835;&#30103;&#30340;&#26377;&#25928;&#24615;&#21487;&#20197;&#36890;&#36807;&#35201;&#27714;&#24739;&#32773;&#36890;&#36807;&#24212;&#29992;&#31243;&#24207;&#33258;&#25105;&#25253;&#21578;&#20854;&#29366;&#24577;&#26469;&#34913;&#37327;&#65292;&#28982;&#32780;&#65292;&#36825;&#21487;&#33021;&#20250;&#20196;&#20154;&#19981;&#30693;&#25152;&#25514;&#24182;&#23548;&#33268;&#22833;&#21435;&#21442;&#19982;&#24230;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#30740;&#31350;&#65292;&#25506;&#35752;&#28216;&#25103;&#21270;&#23545;&#33258;&#25105;&#25253;&#21578;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#21019;&#24314;&#19968;&#20010;&#31995;&#32479;&#65292;&#36890;&#36807;&#20998;&#26512;&#20809;-&#34880;&#23481;&#31215;&#21464;&#21270;&#20449;&#21495;&#26469;&#35780;&#20272;&#35748;&#30693;&#36127;&#33655;&#65288;CL&#65289;&#12290;&#21033;&#29992;11&#21517;&#21442;&#19982;&#32773;&#30340;&#25968;&#25454;&#26469;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#26816;&#27979;CL&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#20004;&#20010;&#29256;&#26412;&#30340;&#35843;&#26597;&#38382;&#21367;&#65306;&#19968;&#20010;&#26159;&#28216;&#25103;&#21270;&#29256;&#26412;&#65292;&#19968;&#20010;&#26159;&#20256;&#32479;&#29256;&#26412;&#12290;&#25105;&#20204;&#20272;&#35745;&#20854;&#20182;&#21442;&#19982;&#32773;&#65288;13&#21517;&#65289;&#22312;&#23436;&#25104;&#35843;&#26597;&#38382;&#21367;&#26102;&#32463;&#21382;&#30340;CL&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#36807;&#39044;&#20808;&#22312;&#24212;&#28608;&#26816;&#27979;&#20219;&#21153;&#20013;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#21487;&#20197;&#22686;&#24378;CL&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;&#12290;&#23545;&#20110;13&#21517;&#21442;&#19982;&#32773;&#20013;&#30340;10&#21517;&#65292;&#20010;&#24615;&#21270;CL&#26816;&#27979;&#22120;&#21487;&#20197;&#23454;&#29616;&#39640;&#20110;0.7&#30340;F1&#24471;&#20998;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;CL&#26041;&#38754;&#65292;&#28216;&#25103;&#21270;&#21644;&#38750;&#28216;&#25103;&#21270;&#30340;&#35843;&#26597;&#38382;&#21367;&#27809;&#26377;&#21306;&#21035;&#65292;&#20294;&#21442;&#19982;&#32773;&#26356;&#21916;&#27426;&#28216;&#25103;&#21270;&#30340;&#29256;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
The effectiveness of digital treatments can be measured by requiring patients to self-report their state through applications, however, it can be overwhelming and causes disengagement. We conduct a study to explore the impact of gamification on self-reporting. Our approach involves the creation of a system to assess cognitive load (CL) through the analysis of photoplethysmography (PPG) signals. The data from 11 participants is utilized to train a machine learning model to detect CL. Subsequently, we create two versions of surveys: a gamified and a traditional one. We estimate the CL experienced by other participants (13) while completing surveys. We find that CL detector performance can be enhanced via pre-training on stress detection tasks. For 10 out of 13 participants, a personalized CL detector can achieve an F1 score above 0.7. We find no difference between the gamified and non-gamified surveys in terms of CL but participants prefer the gamified version.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#20449;&#24687;&#20960;&#20309;&#26500;&#36896;&#20102;&#32447;&#24615;&#20960;&#20046;&#27431;&#20960;&#37324;&#24471;&#27969;&#24418;&#65292;&#36890;&#36807;&#24341;&#20837;&#20559;&#24494;&#20998;&#26041;&#31243;Ricci&#27969;&#65292;&#35299;&#20915;&#20102;&#31163;&#25955;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#26799;&#24230;&#26080;&#31351;&#25110;&#38646;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.03390</link><description>&lt;p&gt;
&#22312;Ricci&#27969;&#19979;&#23398;&#20064;&#31163;&#25955;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Learning Discretized Neural Networks under Ricci Flow. (arXiv:2302.03390v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03390
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#20449;&#24687;&#20960;&#20309;&#26500;&#36896;&#20102;&#32447;&#24615;&#20960;&#20046;&#27431;&#20960;&#37324;&#24471;&#27969;&#24418;&#65292;&#36890;&#36807;&#24341;&#20837;&#20559;&#24494;&#20998;&#26041;&#31243;Ricci&#27969;&#65292;&#35299;&#20915;&#20102;&#31163;&#25955;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#26799;&#24230;&#26080;&#31351;&#25110;&#38646;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#30001;&#20302;&#31934;&#24230;&#26435;&#37325;&#21644;&#28608;&#27963;&#20989;&#25968;&#26500;&#25104;&#30340;&#31163;&#25955;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30001;&#20110;&#38750;&#21487;&#24494;&#20998;&#31163;&#25955;&#20989;&#25968;&#32780;&#36973;&#21463;&#26080;&#31351;&#25110;&#38646;&#26799;&#24230;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#38024;&#23545;&#27492;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#25226; STE&#36817;&#20284;&#26799;&#24230;&#30475;&#20316;&#25972;&#20307;&#20559;&#24046;&#30340;&#24230;&#37327;&#25200;&#21160;&#65292;&#36890;&#36807;&#23545;&#20598;&#29702;&#35770;&#23558;&#20854;&#30475;&#20316;&#40654;&#26364;&#27969;&#24418;&#19978;&#30340;&#24230;&#37327;&#25200;&#21160;&#65292;&#24182;&#22312;&#20449;&#24687;&#20960;&#20309;&#30340;&#22522;&#30784;&#19978;&#20026; DNN&#26500;&#36896;&#20102;&#32447;&#24615;&#20960;&#20046;&#27431;&#20960;&#37324;&#24471;&#65288;LNE&#65289;&#27969;&#24418;&#20197;&#22788;&#29702;&#25200;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we consider Discretized Neural Networks (DNNs) consisting of low-precision weights and activations, which suffer from either infinite or zero gradients due to the non-differentiable discrete function in the training process. In this case, most training-based DNNs employ the standard Straight-Through Estimator (STE) to approximate the gradient w.r.t. discrete values. However, the STE gives rise to the problem of gradient mismatch, due to the perturbations of the approximated gradient. To address this problem, this paper reveals that this mismatch can be viewed as a metric perturbation in a Riemannian manifold through the lens of duality theory. Further, on the basis of the information geometry, we construct the Linearly Nearly Euclidean (LNE) manifold for DNNs as a background to deal with perturbations. By introducing a partial differential equation on metrics, i.e., the Ricci flow, we prove the dynamical stability and convergence of the LNE metric with the $L^2$-norm per
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#30340;&#29289;&#36136;&#24863;&#30693;&#21452;&#32819;&#38899;&#39057;&#20256;&#25773;&#26041;&#27861;&#65292;&#33021;&#22815;&#29983;&#25104;&#30495;&#23454;&#29615;&#22659;&#19979;&#30340;&#28210;&#26579;&#38899;&#39057;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#22788;&#29702;&#37325;&#26500;&#19977;&#32500;&#27169;&#22411;&#20013;&#30340;&#32570;&#38519;&#65292;&#24182;&#19988;&#33021;&#22815;&#31934;&#30830;&#29983;&#25104;&#19982;&#30495;&#23454;&#29615;&#22659;&#30456;&#31526;&#30340;&#22768;&#23398;&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2302.02809</link><description>&lt;p&gt;
Listen2Scene&#65306;&#20132;&#20114;&#24335;&#29289;&#36136;&#24863;&#30693;&#21452;&#32819;&#38899;&#39057;&#20256;&#25773;&#37325;&#26500;&#19977;&#32500;&#22330;&#26223;
&lt;/p&gt;
&lt;p&gt;
Listen2Scene: Interactive material-aware binaural soundbpropagation for reconstructed 3D scenes. (arXiv:2302.02809v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02809
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#30340;&#29289;&#36136;&#24863;&#30693;&#21452;&#32819;&#38899;&#39057;&#20256;&#25773;&#26041;&#27861;&#65292;&#33021;&#22815;&#29983;&#25104;&#30495;&#23454;&#29615;&#22659;&#19979;&#30340;&#28210;&#26579;&#38899;&#39057;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#22788;&#29702;&#37325;&#26500;&#19977;&#32500;&#27169;&#22411;&#20013;&#30340;&#32570;&#38519;&#65292;&#24182;&#19988;&#33021;&#22815;&#31934;&#30830;&#29983;&#25104;&#19982;&#30495;&#23454;&#29615;&#22659;&#30456;&#31526;&#30340;&#22768;&#23398;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#34394;&#25311;&#29616;&#23454;&#65288;VR&#65289;&#21644;&#22686;&#24378;&#29616;&#23454;&#65288;AR&#65289;&#24212;&#29992;&#30340;&#31471;&#21040;&#31471;&#21452;&#32819;&#38899;&#39057;&#28210;&#26579;&#26041;&#27861;&#65288;Listen2Scene&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#21452;&#32819;&#22768;&#23398;&#20256;&#25773;&#26041;&#27861;&#65292;&#20197;&#29983;&#25104;&#30495;&#23454;&#29615;&#22659;&#30340;3D&#27169;&#22411;&#30340;&#22768;&#23398;&#25928;&#26524;&#12290;&#20219;&#20309;&#28165;&#27905;&#38899;&#39057;&#25110;&#24178;&#38899;&#39057;&#37117;&#21487;&#20197;&#19982;&#29983;&#25104;&#30340;&#22768;&#23398;&#25928;&#26524;&#21367;&#31215;&#65292;&#20197;&#28210;&#26579;&#19982;&#30495;&#23454;&#29615;&#22659;&#30456;&#23545;&#24212;&#30340;&#38899;&#39057;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#21033;&#29992;3D&#22330;&#26223;&#30340;&#26448;&#26009;&#21644;&#25299;&#25169;&#20449;&#24687;&#29983;&#25104;&#22330;&#26223;&#28508;&#22312;&#21521;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;CGAN&#65289;&#20174;&#22330;&#26223;&#28508;&#22312;&#21521;&#37327;&#29983;&#25104;&#22768;&#23398;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#32593;&#32476;&#33021;&#22815;&#22788;&#29702;&#37325;&#26500;&#30340;&#19977;&#32500;&#32593;&#26684;&#27169;&#22411;&#20013;&#30340;&#23380;&#27934;&#25110;&#20854;&#20182;&#20266;&#20687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#25104;&#26412;&#20989;&#25968;&#65292;&#29992;&#20110;&#29983;&#25104;&#22120;&#32593;&#32476;&#20197;&#25972;&#21512;&#31354;&#38388;&#38899;&#39057;&#25928;&#26524;&#12290;&#32473;&#23450;&#28304;&#21644;&#21548;&#32773;&#20301;&#32622;&#65292;&#25105;&#20204;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#21452;&#32819;&#22768;&#38899;&#20256;&#25773;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#19982;&#30495;&#23454;&#29615;&#22659;&#31934;&#24230;&#30456;&#31526;&#30340;&#22768;&#23398;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an end-to-end binaural audio rendering approach (Listen2Scene) for virtual reality (VR) and augmented reality (AR) applications. We propose a novel neural-network-based binaural sound propagation method to generate acoustic effects for 3D models of real environments. Any clean audio or dry audio can be convolved with the generated acoustic effects to render audio corresponding to the real environment. We propose a graph neural network that uses both the material and the topology information of the 3D scenes and generates a scene latent vector. Moreover, we use a conditional generative adversarial network (CGAN) to generate acoustic effects from the scene latent vector. Our network is able to handle holes or other artifacts in the reconstructed 3D mesh model. We present an efficient cost function to the generator network to incorporate spatial audio effects. Given the source and the listener position, our learning-based binaural sound propagation approach can generate an acou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#31649;&#36947;&#65288;FavMac&#65289;&#65292;&#21487;&#20197;&#26368;&#22823;&#21270;&#20215;&#20540;&#24182;&#25511;&#21046;&#25104;&#26412;&#12290; FavMac&#21487;&#20197;&#19982;&#20960;&#20046;&#20219;&#20309;&#22810;&#26631;&#31614;&#20998;&#31867;&#22120;&#30456;&#32467;&#21512;&#65292;&#24182;&#36890;&#36807;&#22312;&#32447;&#26356;&#26032;&#26426;&#21046;&#22788;&#29702;&#23454;&#38469;&#30340;&#22823;&#35268;&#27169;&#24212;&#29992;&#65292;&#20026;&#25104;&#26412;&#25511;&#21046;&#25552;&#20379;&#26080;&#20998;&#24067;&#29702;&#35770;&#25285;&#20445;&#12290;</title><link>http://arxiv.org/abs/2302.00839</link><description>&lt;p&gt;
&#24555;&#36895;&#22312;&#32447;&#20540;&#26368;&#22823;&#21270;&#39044;&#27979;&#38598;&#21644;&#31526;&#21512;&#35268;&#33539;&#25104;&#26412;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fast Online Value-Maximizing Prediction Sets with Conformal Cost Control. (arXiv:2302.00839v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00839
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#31649;&#36947;&#65288;FavMac&#65289;&#65292;&#21487;&#20197;&#26368;&#22823;&#21270;&#20215;&#20540;&#24182;&#25511;&#21046;&#25104;&#26412;&#12290; FavMac&#21487;&#20197;&#19982;&#20960;&#20046;&#20219;&#20309;&#22810;&#26631;&#31614;&#20998;&#31867;&#22120;&#30456;&#32467;&#21512;&#65292;&#24182;&#36890;&#36807;&#22312;&#32447;&#26356;&#26032;&#26426;&#21046;&#22788;&#29702;&#23454;&#38469;&#30340;&#22823;&#35268;&#27169;&#24212;&#29992;&#65292;&#20026;&#25104;&#26412;&#25511;&#21046;&#25552;&#20379;&#26080;&#20998;&#24067;&#29702;&#35770;&#25285;&#20445;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#22810;&#26631;&#31614;&#39044;&#27979;&#38382;&#39064;&#28041;&#21450;&#21040;&#24517;&#39035;&#28385;&#36275;&#19979;&#28216;&#20351;&#29992;&#35268;&#23450;&#30340;&#29305;&#23450;&#35201;&#27714;&#30340;&#38598;&#21512;&#20540;&#39044;&#27979;&#12290;&#25105;&#20204;&#20851;&#27880;&#19968;&#20010;&#20856;&#22411;&#30340;&#24773;&#20917;&#65292;&#20854;&#20013;&#36825;&#20123;&#35201;&#27714;&#20998;&#21035;&#32534;&#30721;&#20215;&#20540;&#21644;&#25104;&#26412;&#65292;&#24182;&#30456;&#20114;&#31454;&#20105;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#34987;&#31216;&#20026;FavMac&#30340;&#36890;&#29992;&#31649;&#36947;&#65292;&#20197;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#26368;&#22823;&#21270;&#20215;&#20540;&#24182;&#25511;&#21046;&#25104;&#26412;&#12290; FavMac&#21487;&#20197;&#19982;&#20960;&#20046;&#20219;&#20309;&#22810;&#26631;&#31614;&#20998;&#31867;&#22120;&#30456;&#32467;&#21512;&#65292;&#20026;&#25104;&#26412;&#25511;&#21046;&#25552;&#20379;&#26080;&#20998;&#24067;&#29702;&#35770;&#25285;&#20445;&#12290;&#27492;&#22806;&#65292;&#19982;&#20043;&#21069;&#30340;&#20316;&#21697;&#19981;&#21516;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#22312;&#32447;&#26356;&#26032;&#26426;&#21046;&#22788;&#29702;&#23454;&#38469;&#30340;&#22823;&#35268;&#27169;&#24212;&#29992;&#65292;&#36825;&#26159;&#20540;&#24471;&#29420;&#31435;&#20851;&#27880;&#30340;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35770;&#21644;&#29702;&#35770;&#36129;&#29486;&#24471;&#21040;&#20102;&#23545;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#21644;&#20960;&#31181;&#25968;&#25454;&#38598;&#30340;&#35797;&#39564;&#25903;&#25745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many real-world multi-label prediction problems involve set-valued predictions that must satisfy specific requirements dictated by downstream usage. We focus on a typical scenario where such requirements, separately encoding $\textit{value}$ and $\textit{cost}$, compete with each other. For instance, a hospital might expect a smart diagnosis system to capture as many severe, often co-morbid, diseases as possible (the value), while maintaining strict control over incorrect predictions (the cost). We present a general pipeline, dubbed as FavMac, to maximize the value while controlling the cost in such scenarios. FavMac can be combined with almost any multi-label classifier, affording distribution-free theoretical guarantees on cost control. Moreover, unlike prior works, it can handle real-world large-scale applications via a carefully designed online update mechanism, which is of independent interest. Our methodological and theoretical contributions are supported by experiments on severa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#26500;&#24314;&#20551;&#35774;&#30340;&#36807;&#31243;&#20013;&#65292;&#36873;&#25321;&#26368;&#30701;&#30340;&#20551;&#35774;&#19981;&#22914;&#36873;&#25321;&#26368;&#24369;&#30340;&#20551;&#35774;&#65292;&#32780;&#24369;&#28857;&#26159;&#27604;&#38271;&#24230;&#25110;&#31616;&#21333;&#24615;&#26356;&#22909;&#30340;&#25512;&#24191;&#34920;&#29616;&#20195;&#29702;&#12290;</title><link>http://arxiv.org/abs/2301.12987</link><description>&lt;p&gt;
&#20551;&#35774;&#30340;&#26368;&#20339;&#36873;&#25321;&#26159;&#26368;&#24369;&#30340;&#32780;&#19981;&#26159;&#26368;&#30701;&#30340;
&lt;/p&gt;
&lt;p&gt;
The Optimal Choice of Hypothesis Is the Weakest, Not the Shortest. (arXiv:2301.12987v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12987
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#26500;&#24314;&#20551;&#35774;&#30340;&#36807;&#31243;&#20013;&#65292;&#36873;&#25321;&#26368;&#30701;&#30340;&#20551;&#35774;&#19981;&#22914;&#36873;&#25321;&#26368;&#24369;&#30340;&#20551;&#35774;&#65292;&#32780;&#24369;&#28857;&#26159;&#27604;&#38271;&#24230;&#25110;&#31616;&#21333;&#24615;&#26356;&#22909;&#30340;&#25512;&#24191;&#34920;&#29616;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;$A$&#21644;$B$&#26159;&#36825;&#26679;&#30340;&#38598;&#21512;&#65292;&#21363;$A \subset B$&#65292;&#37027;&#20040;&#19968;&#33324;&#21270;&#21487;&#20197;&#34987;&#29702;&#35299;&#20026;&#20174;$A$&#25512;&#26029;&#20986;&#19968;&#20010;&#36275;&#20197;&#26500;&#24314;$B$&#30340;&#20551;&#35774;&#12290;&#21487;&#20197;&#20174;$A$&#25512;&#26029;&#20986;&#20219;&#24847;&#25968;&#37327;&#30340;&#20551;&#35774;&#65292;&#20294;&#21482;&#26377;&#20854;&#20013;&#30340;&#19968;&#20123;&#21487;&#20197;&#25512;&#24191;&#21040;$B$&#12290;&#24590;&#26679;&#30693;&#36947;&#21738;&#20123;&#20551;&#35774;&#21487;&#33021;&#25512;&#24191;&#65311;&#19968;&#31181;&#31574;&#30053;&#26159;&#36873;&#25321;&#26368;&#30701;&#30340;&#65292;&#23558;&#21387;&#32553;&#20449;&#24687;&#30340;&#33021;&#21147;&#19982;&#25512;&#24191;&#30340;&#33021;&#21147;&#65288;&#26234;&#33021;&#30340;&#20195;&#29702;&#65289;&#31561;&#21516;&#36215;&#26469;&#12290;&#25105;&#20204;&#22312;&#20027;&#21160;&#35748;&#30693;&#30340;&#25968;&#23398;&#24418;&#24335;&#20027;&#20041;&#32972;&#26223;&#19979;&#30740;&#31350;&#20102;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#65292;&#21387;&#32553;&#26082;&#19981;&#26159;&#26368;&#22823;&#21270;&#34920;&#29616;&#65288;&#29992;&#20551;&#35774;&#25512;&#24191;&#30340;&#27010;&#29575;&#34913;&#37327;&#65289;&#30340;&#24517;&#35201;&#26465;&#20214;&#65292;&#20063;&#19981;&#26159;&#20805;&#20998;&#26465;&#20214;&#12290;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#19982;&#38271;&#24230;&#25110;&#31616;&#21333;&#24615;&#26080;&#20851;&#30340;&#20195;&#29702;&#65292;&#31216;&#20026;&#24369;&#28857;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22914;&#26524;&#20219;&#21153;&#26159;&#22343;&#21248;&#20998;&#24067;&#30340;&#65292;&#21017;&#19981;&#23384;&#22312;&#20219;&#20309;&#20195;&#29702;&#30340;&#36873;&#25321;&#65292;&#21487;&#20197;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#33267;&#23569;&#19982;&#24369;&#28857;&#26368;&#22823;&#21270;&#30340;&#34920;&#29616;&#30456;&#21516;&#65292;&#21516;&#26102;&#22312;&#33267;&#23569;&#19968;&#20010;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;&#22312;&#27604;&#36739;&#26368;&#22823;&#24369;&#28857;&#21644;&#26368;&#23567;&#25551;&#36848;&#38271;&#24230;(MDL)&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#26368;&#22823;&#24369;&#28857;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#20110;MDL&#12290;&#25105;&#20204;&#35748;&#20026;&#24369;&#28857;&#26159;&#27604;&#38271;&#24230;&#25110;&#31616;&#21333;&#24615;&#26356;&#22909;&#30340;&#25512;&#24191;&#34920;&#29616;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
If $A$ and $B$ are sets such that $A \subset B$, generalisation may be understood as the inference from $A$ of a hypothesis sufficient to construct $B$. One might infer any number of hypotheses from $A$, yet only some of those may generalise to $B$. How can one know which are likely to generalise? One strategy is to choose the shortest, equating the ability to compress information with the ability to generalise (a proxy for intelligence). We examine this in the context of a mathematical formalism of enactive cognition. We show that compression is neither necessary nor sufficient to maximise performance (measured in terms of the probability of a hypothesis generalising). We formulate a proxy unrelated to length or simplicity, called weakness. We show that if tasks are uniformly distributed, then there is no choice of proxy that performs at least as well as weakness maximisation in all tasks while performing strictly better in at least one. In experiments comparing maximum weakness and m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#27491;&#21017;&#21270;&#27969;&#65292;&#19987;&#20026;&#19977;&#32500;&#31354;&#38388;&#20013;&#22810;&#20010;&#29289;&#20307;&#30340;&#20301;&#32622;&#21644;&#26041;&#21521;&#24314;&#27169;&#32780;&#35774;&#35745;&#12290;&#36890;&#36807;&#22312;&#21333;&#20301;&#22235;&#20803;&#25968;&#32676;&#19978;&#23450;&#20041;&#24179;&#28369;&#21644;&#34920;&#29616;&#21147;&#24378;&#30340;&#27969;&#20197;&#21450;&#23450;&#20041;&#36866;&#24403;&#30340;&#23494;&#24230;&#65292;&#22312;&#26059;&#36716;&#32676;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#20204;&#21487;&#20197;&#25104;&#21151;&#22320;&#37319;&#26679;&#20998;&#23376;&#26230;&#20307;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2301.11355</link><description>&lt;p&gt;
&#29992;&#20110;&#37319;&#26679;&#20998;&#23376;&#26230;&#20307;&#32467;&#26500;&#30340;&#21018;&#20307;&#27969;
&lt;/p&gt;
&lt;p&gt;
Rigid body flows for sampling molecular crystal structures. (arXiv:2301.11355v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11355
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#27491;&#21017;&#21270;&#27969;&#65292;&#19987;&#20026;&#19977;&#32500;&#31354;&#38388;&#20013;&#22810;&#20010;&#29289;&#20307;&#30340;&#20301;&#32622;&#21644;&#26041;&#21521;&#24314;&#27169;&#32780;&#35774;&#35745;&#12290;&#36890;&#36807;&#22312;&#21333;&#20301;&#22235;&#20803;&#25968;&#32676;&#19978;&#23450;&#20041;&#24179;&#28369;&#21644;&#34920;&#29616;&#21147;&#24378;&#30340;&#27969;&#20197;&#21450;&#23450;&#20041;&#36866;&#24403;&#30340;&#23494;&#24230;&#65292;&#22312;&#26059;&#36716;&#32676;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#20204;&#21487;&#20197;&#25104;&#21151;&#22320;&#37319;&#26679;&#20998;&#23376;&#26230;&#20307;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#21017;&#21270;&#27969;(NF)&#26159;&#19968;&#31867;&#24378;&#22823;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#30001;&#20110;&#20854;&#39640;&#24230;&#28789;&#27963;&#21644;&#34920;&#29616;&#21147;&#65292;&#36817;&#24180;&#26469;&#24191;&#21463;&#27426;&#36814;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#27491;&#21017;&#21270;&#27969;&#65292;&#19987;&#20026;&#19977;&#32500;&#31354;&#38388;&#20013;&#22810;&#20010;&#29289;&#20307;&#30340;&#20301;&#32622;&#21644;&#26041;&#21521;&#24314;&#27169;&#32780;&#35774;&#35745;&#65292;&#20363;&#22914;&#26230;&#20307;&#20013;&#30340;&#20998;&#23376;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#20004;&#20010;&#20851;&#38190;&#24605;&#24819;:&#39318;&#20808;&#65292;&#25105;&#20204;&#22312;&#21333;&#20301;&#22235;&#20803;&#25968;&#32676;&#19978;&#23450;&#20041;&#24179;&#28369;&#21644;&#34920;&#29616;&#21147;&#24378;&#30340;&#27969;&#65292;&#20174;&#32780;&#21487;&#20197;&#25429;&#25417;&#21018;&#20307;&#30340;&#36830;&#32493;&#26059;&#36716;&#36816;&#21160;;&#20854;&#27425;&#65292;&#25105;&#20204;&#21033;&#29992;&#21333;&#20301;&#22235;&#20803;&#25968;&#30340;&#21452;&#35206;&#30422;&#29305;&#24615;&#65292;&#22312;&#26059;&#36716;&#32676;&#19978;&#23450;&#20041;&#19968;&#20010;&#36866;&#24403;&#30340;&#23494;&#24230;&#12290;&#36825;&#30830;&#20445;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#20351;&#29992;&#26631;&#20934;&#30340;&#22522;&#20110;&#20284;&#28982;&#26041;&#27861;&#25110;&#22522;&#20110;&#28909;&#21147;&#23398;&#30446;&#26631;&#23494;&#24230;&#30340;&#21464;&#20998;&#25512;&#26029;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#20004;&#20010;&#20998;&#23376;&#31034;&#20363;&#30340;Boltzmann&#29983;&#25104;&#22120;&#26469;&#35780;&#20272;&#35813;&#26041;&#27861;&#65292;&#21363;&#22235;&#38754;&#20307;&#31995;&#32479;&#30340;&#22810;&#27169;&#24577;&#23494;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Normalizing flows (NF) are a class of powerful generative models that have gained popularity in recent years due to their ability to model complex distributions with high flexibility and expressiveness. In this work, we introduce a new type of normalizing flow that is tailored for modeling positions and orientations of multiple objects in three-dimensional space, such as molecules in a crystal. Our approach is based on two key ideas: first, we define smooth and expressive flows on the group of unit quaternions, which allows us to capture the continuous rotational motion of rigid bodies; second, we use the double cover property of unit quaternions to define a proper density on the rotation group. This ensures that our model can be trained using standard likelihood-based methods or variational inference with respect to a thermodynamic target density. We evaluate the method by training Boltzmann generators for two molecular examples, namely the multi-modal density of a tetrahedral system 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#25506;&#31350;&#35270;&#35273;Transformer&#26159;&#21542;&#21487;&#20197;&#24212;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;3D&#35821;&#20041;&#20998;&#21106;&#20013;&#65292;&#36890;&#36807;&#20445;&#30041;&#19982;RGB&#22270;&#20687;&#30456;&#21516;&#30340;&#39592;&#24178;&#32467;&#26500;&#65292;&#36825;&#39033;&#24037;&#20316;&#35777;&#26126;&#20102;ViTs&#22312;&#32467;&#21512;&#25237;&#24433;&#26041;&#27861;&#65292;&#22823;&#25968;&#25454;&#35757;&#32451;&#21644;&#20855;&#26377;&#22122;&#22768;&#40065;&#26834;&#24615;&#30340;&#26032;&#25439;&#22833;&#20989;&#25968;&#21518;&#21487;&#20197;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2301.10222</link><description>&lt;p&gt;
RangeViT&#65306;&#38754;&#21521;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#19977;&#32500;&#35821;&#20041;&#20998;&#21106;&#30340;&#35270;&#35273;Transformer
&lt;/p&gt;
&lt;p&gt;
RangeViT: Towards Vision Transformers for 3D Semantic Segmentation in Autonomous Driving. (arXiv:2301.10222v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10222
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25506;&#31350;&#35270;&#35273;Transformer&#26159;&#21542;&#21487;&#20197;&#24212;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;3D&#35821;&#20041;&#20998;&#21106;&#20013;&#65292;&#36890;&#36807;&#20445;&#30041;&#19982;RGB&#22270;&#20687;&#30456;&#21516;&#30340;&#39592;&#24178;&#32467;&#26500;&#65292;&#36825;&#39033;&#24037;&#20316;&#35777;&#26126;&#20102;ViTs&#22312;&#32467;&#21512;&#25237;&#24433;&#26041;&#27861;&#65292;&#22823;&#25968;&#25454;&#35757;&#32451;&#21644;&#20855;&#26377;&#22122;&#22768;&#40065;&#26834;&#24615;&#30340;&#26032;&#25439;&#22833;&#20989;&#25968;&#21518;&#21487;&#20197;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#23460;&#22806;LiDAR&#28857;&#20113;&#30340;&#35821;&#20041;&#20998;&#21106;&#35270;&#20026;&#20108;&#32500;&#38382;&#39064;&#65288;&#20363;&#22914;&#36890;&#36807;&#36317;&#31163;&#25237;&#24433;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26377;&#25928;&#21644;&#27969;&#34892;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#22522;&#20110;&#25237;&#24433;&#30340;&#26041;&#27861;&#36890;&#24120;&#21463;&#30410;&#20110;&#24555;&#36895;&#35745;&#31639;&#65292;&#24182;&#19988;&#19982;&#20351;&#29992;&#20854;&#20182;&#28857;&#20113;&#34920;&#31034;&#30340;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#30446;&#21069;&#65292;&#25237;&#24433;&#26041;&#27861;&#21033;&#29992;2D CNNs&#65292;&#20294;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#26368;&#26032;&#36827;&#23637;&#34920;&#26126;&#65292;&#35270;&#35273;Transformer&#65288;ViTs&#65289;&#22312;&#35768;&#22810;&#22522;&#20110;&#22270;&#20687;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#24050;&#32463;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#25104;&#26524;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36136;&#30097;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;ViTs&#30340;&#26368;&#26032;&#25913;&#36827;&#26469;&#25913;&#36827;&#19977;&#32500;&#35821;&#20041;&#20998;&#21106;&#30340;&#25237;&#24433;&#26041;&#27861;&#12290;&#25105;&#20204;&#22238;&#31572;&#26159;&#32943;&#23450;&#30340;&#65292;&#20294;&#21482;&#26377;&#22312;&#32467;&#21512;&#20102;&#19977;&#20010;&#20851;&#38190;&#22240;&#32032;&#20043;&#21518;&#25165;&#33021;&#23454;&#29616;&#65306;&#65288;a&#65289;ViTs&#38590;&#20197;&#35757;&#32451;&#65292;&#24182;&#19988;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#26469;&#23398;&#20064;&#24378;&#22823;&#30340;&#34920;&#31034;&#12290;&#36890;&#36807;&#20445;&#30041;&#19982;RGB&#22270;&#20687;&#30456;&#21516;&#30340;&#39592;&#24178;&#32467;&#26500;&#65292;&#25105;&#20204;&#21487;&#20197;&#21033;&#29992;&#23545;&#22823;&#22270;&#20687;&#38598;&#21512;&#30340;&#38271;&#26102;&#38388;&#35757;&#32451;&#30340;&#30693;&#35782;&#65292;&#36825;&#20123;&#38598;&#21512;&#27604;&#30456;&#24212;&#30340;&#28857;&#20113;&#25968;&#25454;&#38598;&#35201;&#23567;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Casting semantic segmentation of outdoor LiDAR point clouds as a 2D problem, e.g., via range projection, is an effective and popular approach. These projection-based methods usually benefit from fast computations and, when combined with techniques which use other point cloud representations, achieve state-of-the-art results. Today, projection-based methods leverage 2D CNNs but recent advances in computer vision show that vision transformers (ViTs) have achieved state-of-the-art results in many image-based benchmarks. In this work, we question if projection-based methods for 3D semantic segmentation can benefit from these latest improvements on ViTs. We answer positively but only after combining them with three key ingredients: (a) ViTs are notoriously hard to train and require a lot of training data to learn powerful representations. By preserving the same backbone architecture as for RGB images, we can exploit the knowledge from long training on large image collections that are much c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#21464;Lipschitz&#36172;&#24466;&#35774;&#32622;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;\texttt{UniformMesh-N}&#30340;&#31639;&#27861;&#12290;&#20351;&#29992;&#20391;&#38754;&#35266;&#23519;&#30340;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#25913;&#36827;&#30340;&#36951;&#25022;&#19978;&#30028;&#12290;</title><link>http://arxiv.org/abs/2212.07524</link><description>&lt;p&gt;
&#19981;&#21464;Lipschitz&#36172;&#24466;&#65306;&#19968;&#20010;&#20391;&#35266;&#21457;&#29616;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Invariant Lipschitz Bandits: A Side Observation Approach. (arXiv:2212.07524v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07524
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#21464;Lipschitz&#36172;&#24466;&#35774;&#32622;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;\texttt{UniformMesh-N}&#30340;&#31639;&#27861;&#12290;&#20351;&#29992;&#20391;&#38754;&#35266;&#23519;&#30340;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#25913;&#36827;&#30340;&#36951;&#25022;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#31216;&#20986;&#29616;&#22312;&#35768;&#22810;&#20248;&#21270;&#21644;&#20915;&#31574;&#38382;&#39064;&#20013;&#65292;&#24182;&#21560;&#24341;&#20102;&#20248;&#21270;&#30028;&#30340;&#30456;&#24403;&#20851;&#27880;&#65306;&#36890;&#36807;&#21033;&#29992;&#36825;&#26679;&#30340;&#23545;&#31216;&#24615;&#65292;&#21487;&#20197;&#26174;&#33879;&#25913;&#36827;&#23547;&#25214;&#26368;&#20248;&#35299;&#30340;&#36807;&#31243;&#12290;&#23613;&#31649;&#23545;&#31216;&#24615;&#22312;&#65288;&#31163;&#32447;&#65289;&#20248;&#21270;&#20013;&#21462;&#24471;&#25104;&#21151;&#65292;&#20294;&#22312;&#22312;&#32447;&#20248;&#21270;&#35774;&#32622;&#20013;&#65292;&#29305;&#21035;&#26159;&#22312;&#36172;&#24466;&#25991;&#29486;&#20013;&#65292;&#20854;&#21033;&#29992;&#36824;&#26410;&#24471;&#21040;&#20805;&#20998;&#30340;&#30740;&#31350;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21464;Lipschitz&#36172;&#24466;&#35774;&#32622;&#65292;&#36825;&#26159;Lipschitz&#36172;&#24466;&#30340;&#19968;&#20010;&#23376;&#31867;&#65292;&#22312;&#35813;&#23376;&#31867;&#20013;&#65292;&#22870;&#21169;&#20989;&#25968;&#21644;&#33218;&#38598;&#22312;&#19968;&#32452;&#21464;&#25442;&#19979;&#20445;&#25345;&#19981;&#21464;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;\texttt{UniformMesh-N}&#30340;&#31639;&#27861;&#65292;&#23427;&#33258;&#28982;&#22320;&#23558;&#20391;&#38754;&#35266;&#23519;&#20351;&#29992;&#32676;&#36712;&#36947;&#25972;&#21512;&#21040;\texttt{UniformMesh}&#31639;&#27861;&#65288;\cite{Kleinberg2005_UniformMesh}&#65289;&#20013;&#65292;&#35813;&#31639;&#27861;&#22343;&#21248;&#22320;&#20998;&#21106;&#20102;&#33218;&#30340;&#38598;&#21512;&#12290;&#36890;&#36807;&#20391;&#38754;&#35266;&#23519;&#26041;&#27861;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25913;&#36827;&#30340;&#36951;&#25022;&#19978;&#30028;&#65292;&#20854;&#21462;&#20915;&#20110;&#22522;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symmetry arises in many optimization and decision-making problems, and has attracted considerable attention from the optimization community: By utilizing the existence of such symmetries, the process of searching for optimal solutions can be improved significantly. Despite its success in (offline) optimization, the utilization of symmetries has not been well examined within the online optimization settings, especially in the bandit literature. As such, in this paper we study the invariant Lipschitz bandit setting, a subclass of the Lipschitz bandits where the reward function and the set of arms are preserved under a group of transformations. We introduce an algorithm named \texttt{UniformMesh-N}, which naturally integrates side observations using group orbits into the \texttt{UniformMesh} algorithm (\cite{Kleinberg2005_UniformMesh}), which uniformly discretizes the set of arms. Using the side-observation approach, we prove an improved regret upper bound, which depends on the cardinalit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#20998;&#36776;&#29575;&#38750;&#20998;&#23618;&#35270;&#35273;Transformer&#27169;&#22411;GPViT&#65292;&#20351;&#29992;&#20855;&#26377;&#39640;&#25928;&#29575;&#30340;&#32452;&#20256;&#25773;&#27169;&#22359;&#23454;&#29616;&#20102;&#29305;&#24449;&#20043;&#38388;&#20840;&#23616;&#20449;&#24687;&#30340;&#20132;&#27969;&#65292;&#22312;&#22810;&#31181;&#35270;&#35273;&#35782;&#21035;&#20219;&#21153;&#19978;&#33719;&#24471;&#20102;&#31454;&#20105;&#24615;&#25110;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.06795</link><description>&lt;p&gt;
GPViT&#65306;&#19968;&#31181;&#20855;&#26377;&#32452;&#20256;&#25773;&#30340;&#39640;&#20998;&#36776;&#29575;&#38750;&#20998;&#23618;&#35270;&#35273;Transformer&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GPViT: A High Resolution Non-Hierarchical Vision Transformer with Group Propagation. (arXiv:2212.06795v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06795
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#20998;&#36776;&#29575;&#38750;&#20998;&#23618;&#35270;&#35273;Transformer&#27169;&#22411;GPViT&#65292;&#20351;&#29992;&#20855;&#26377;&#39640;&#25928;&#29575;&#30340;&#32452;&#20256;&#25773;&#27169;&#22359;&#23454;&#29616;&#20102;&#29305;&#24449;&#20043;&#38388;&#20840;&#23616;&#20449;&#24687;&#30340;&#20132;&#27969;&#65292;&#22312;&#22810;&#31181;&#35270;&#35273;&#35782;&#21035;&#20219;&#21153;&#19978;&#33719;&#24471;&#20102;&#31454;&#20105;&#24615;&#25110;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#20998;&#23618;Transformer&#27169;&#22411;&#65306;Group Propagation Vision Transformer (GPViT)&#65292;&#26088;&#22312;&#29992;&#20110;&#39640;&#20998;&#36776;&#29575;&#29305;&#24449;&#30340;&#26222;&#36866;&#35270;&#35273;&#35782;&#21035;&#12290;GPViT&#25552;&#20379;&#20102;&#19968;&#31181;&#26497;&#20854;&#39640;&#25928;&#30340;&#32452;&#20256;&#25773;&#27169;&#22359;&#20197;&#22312;&#29305;&#24449;&#20043;&#38388;&#20132;&#27969;&#20840;&#23616;&#20449;&#24687;&#65292;GP Block&#23558;&#29305;&#24449;&#20998;&#32452;&#65292;&#24182;&#36890;&#36807;&#20256;&#25773;&#26356;&#26032;&#32452;&#29305;&#24449;&#30340;&#20840;&#23616;&#20449;&#24687;&#20174;&#32780;&#23454;&#29616;&#20102;&#20840;&#23616;&#20449;&#24687;&#30340;&#20132;&#27969;&#12290;&#25105;&#20204;&#22312;&#22810;&#31181;&#35270;&#35273;&#35782;&#21035;&#20219;&#21153;&#19978;&#23545;GPViT&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21253;&#25324;COCO&#21644;LVIS&#22522;&#20934;&#19978;&#30340;&#22270;&#20687;&#20998;&#31867;&#21644;&#30446;&#26631;&#26816;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GPViT&#22312;&#38656;&#35201;&#26356;&#23569;&#35745;&#31639;&#36164;&#28304;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#31454;&#20105;&#24615;&#25110;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present the Group Propagation Vision Transformer (GPViT): a novel nonhierarchical (i.e. non-pyramidal) transformer model designed for general visual recognition with high-resolution features. High-resolution features (or tokens) are a natural fit for tasks that involve perceiving fine-grained details such as detection and segmentation, but exchanging global information between these features is expensive in memory and computation because of the way self-attention scales. We provide a highly efficient alternative Group Propagation Block (GP Block) to exchange global information. In each GP Block, features are first grouped together by a fixed number of learnable group tokens; we then perform Group Propagation where global information is exchanged between the grouped features; finally, global information in the updated grouped features is returned back to the image features through a transformer decoder. We evaluate GPViT on a variety of visual recognition tasks including image classi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#20154;&#31867;&#31934;&#23376;&#36319;&#36394;&#25968;&#25454;&#38598;VISEM-Tracking&#65292;&#21253;&#21547;&#25163;&#21160;&#27880;&#37322;&#30340;&#21253;&#22260;&#26694;&#22352;&#26631;&#21644;&#30001;&#19987;&#23478;&#20998;&#26512;&#30340;&#31934;&#23376;&#29305;&#24449;&#65292;&#24182;&#25552;&#20379;&#26410;&#26631;&#35760;&#30340;&#35270;&#39057;&#20197;&#20379;&#26131;&#20110;&#35775;&#38382;&#21644;&#20998;&#26512;&#65292;&#26377;&#21161;&#20110;&#35757;&#32451;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#39640;&#22312;&#35780;&#20272;&#31934;&#23376;&#36816;&#21160;&#21644;&#36816;&#21160;&#23398;&#26041;&#38754;&#30340;&#31934;&#24230;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.02842</link><description>&lt;p&gt;
VISEM-Tracking&#65292;&#19968;&#20221;&#20154;&#31867;&#31934;&#23376;&#36319;&#36394;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
VISEM-Tracking, a human spermatozoa tracking dataset. (arXiv:2212.02842v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20154;&#31867;&#31934;&#23376;&#36319;&#36394;&#25968;&#25454;&#38598;VISEM-Tracking&#65292;&#21253;&#21547;&#25163;&#21160;&#27880;&#37322;&#30340;&#21253;&#22260;&#26694;&#22352;&#26631;&#21644;&#30001;&#19987;&#23478;&#20998;&#26512;&#30340;&#31934;&#23376;&#29305;&#24449;&#65292;&#24182;&#25552;&#20379;&#26410;&#26631;&#35760;&#30340;&#35270;&#39057;&#20197;&#20379;&#26131;&#20110;&#35775;&#38382;&#21644;&#20998;&#26512;&#65292;&#26377;&#21161;&#20110;&#35757;&#32451;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#39640;&#22312;&#35780;&#20272;&#31934;&#23376;&#36816;&#21160;&#21644;&#36816;&#21160;&#23398;&#26041;&#38754;&#30340;&#31934;&#24230;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#23376;&#36816;&#21160;&#30340;&#25163;&#21160;&#35780;&#20272;&#38656;&#35201;&#26174;&#24494;&#38236;&#35266;&#23519;&#65292;&#30001;&#20110;&#25152;&#35266;&#23519;&#30340;&#31934;&#23376;&#22312;&#35270;&#37326;&#20013;&#30340;&#24555;&#36895;&#31227;&#21160;&#65292;&#36825;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#33719;&#24471;&#27491;&#30830;&#30340;&#32467;&#26524;&#65292;&#25163;&#21160;&#35780;&#20272;&#38656;&#35201;&#36827;&#34892;&#24191;&#27867;&#30340;&#22521;&#35757;&#12290;&#22240;&#27492;&#65292;&#22312;&#35786;&#25152;&#20013;&#65292;&#35745;&#31639;&#26426;&#36741;&#21161;&#31934;&#23376;&#20998;&#26512;&#65288;CASA&#65289;&#21464;&#24471;&#36234;&#26469;&#36234;&#24120;&#29992;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#38656;&#35201;&#26356;&#22810;&#25968;&#25454;&#26469;&#35757;&#32451;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#22312;&#35780;&#20272;&#31934;&#23376;&#36816;&#21160;&#21644;&#36816;&#21160;&#23398;&#26041;&#38754;&#30340;&#31934;&#24230;&#21644;&#21487;&#38752;&#24615;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#21517;&#20026;VISEM-Tracking&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;20&#20010;30&#31186;&#30340;&#35270;&#39057;&#35760;&#24405;&#65288;&#21253;&#25324;29,196&#24103;&#65289;&#30340;&#28287;&#24615;&#31934;&#23376;&#21046;&#22791;&#29289;&#65292;&#20855;&#22791;&#25163;&#21160;&#27880;&#37322;&#30340;&#21253;&#22260;&#26694;&#22352;&#26631;&#21644;&#30001;&#35813;&#39046;&#22495;&#30340;&#19987;&#23478;&#20998;&#26512;&#30340;&#19968;&#32452;&#31934;&#23376;&#29305;&#24449;&#12290;&#38500;&#20102;&#24050;&#27880;&#37322;&#30340;&#25968;&#25454;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#26410;&#26631;&#35760;&#30340;&#35270;&#39057;&#21098;&#36753;&#65292;&#20197;&#20415;&#36890;&#36807;&#33258;&#30417;&#30563;&#25110;&#26080;&#30417;&#30563;&#23398;&#20064;&#31561;&#26041;&#27861;&#36731;&#26494;&#35775;&#38382;&#21644;&#20998;&#26512;&#25968;&#25454;&#12290;&#20316;&#20026;&#26412;&#25991;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#32447;&#31934;&#23376;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
A manual assessment of sperm motility requires microscopy observation, which is challenging due to the fast-moving spermatozoa in the field of view. To obtain correct results, manual evaluation requires extensive training. Therefore, computer-assisted sperm analysis (CASA) has become increasingly used in clinics. Despite this, more data is needed to train supervised machine learning approaches in order to improve accuracy and reliability in the assessment of sperm motility and kinematics. In this regard, we provide a dataset called VISEM-Tracking with 20 video recordings of 30 seconds (comprising 29,196 frames) of wet sperm preparations with manually annotated bounding-box coordinates and a set of sperm characteristics analyzed by experts in the domain. In addition to the annotated data, we provide unlabeled video clips for easy-to-use access and analysis of the data via methods such as selfor unsupervised learning. As part of this paper, we present baseline sperm detection performan
&lt;/p&gt;</description></item><item><title>KGML-xDTD&#26159;&#19968;&#20010;&#29992;&#20110;&#33647;&#29289;&#27835;&#30103;&#39044;&#27979;&#21644;&#26426;&#21046;&#35299;&#37322;&#30340;&#30693;&#35782;&#22270;&#35889;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#39044;&#27979;&#21644;&#35299;&#37322;&#33647;&#29289;&#21644;&#30142;&#30149;&#20043;&#38388;&#30340;&#26426;&#21046;&#26469;&#23454;&#29616;&#33647;&#29289;&#20877;&#21033;&#29992;&#12290;</title><link>http://arxiv.org/abs/2212.01384</link><description>&lt;p&gt;
KGML-xDTD&#65306;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#33647;&#29289;&#27835;&#30103;&#39044;&#27979;&#19982;&#26426;&#21046;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
KGML-xDTD: A Knowledge Graph-based Machine Learning Framework for Drug Treatment Prediction and Mechanism Description. (arXiv:2212.01384v2 [q-bio.QM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01384
&lt;/p&gt;
&lt;p&gt;
KGML-xDTD&#26159;&#19968;&#20010;&#29992;&#20110;&#33647;&#29289;&#27835;&#30103;&#39044;&#27979;&#21644;&#26426;&#21046;&#35299;&#37322;&#30340;&#30693;&#35782;&#22270;&#35889;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#39044;&#27979;&#21644;&#35299;&#37322;&#33647;&#29289;&#21644;&#30142;&#30149;&#20043;&#38388;&#30340;&#26426;&#21046;&#26469;&#23454;&#29616;&#33647;&#29289;&#20877;&#21033;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#20877;&#21033;&#29992;&#26159;&#19968;&#31181;&#25104;&#26412;&#26377;&#25928;&#19988;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#35782;&#21035;&#29616;&#26377;&#33647;&#29289;/&#21270;&#21512;&#29289;&#30340;&#26032;&#27835;&#30103;&#38774;&#28857;&#25110;&#30142;&#30149;&#65288;&#36866;&#24212;&#30151;&#65289;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;KGML-xDTD&#65306;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#35299;&#37322;&#39044;&#27979;&#27835;&#30103;&#33647;&#29289;&#19982;&#30142;&#30149;&#20043;&#38388;&#30340;&#26426;&#21046;&#12290;&#35813;&#26694;&#26550;&#30001;&#20004;&#20010;&#27169;&#22359;&#32452;&#25104;&#65292;&#19981;&#20165;&#21487;&#20197;&#39044;&#27979;&#33647;&#29289;/&#21270;&#21512;&#29289;&#19982;&#30142;&#30149;&#20043;&#38388;&#30340;&#27835;&#30103;&#27010;&#29575;&#65292;&#32780;&#19988;&#21487;&#20197;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#36335;&#24452;&#20026;&#22522;&#30784;&#30340;&#26426;&#21046;&#26469;&#36827;&#34892;&#29983;&#29289;&#23398;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background: Computational drug repurposing is a cost- and time-efficient approach that aims to identify new therapeutic targets or diseases (indications) of existing drugs/compounds. It is especially critical for emerging and/or orphan diseases due to its cheaper investment and shorter research cycle compared with traditional wet-lab drug discovery approaches. However, the underlying mechanisms of action (MOAs) between repurposed drugs and their target diseases remain largely unknown, which is still a main obstacle for computational drug repurposing methods to be widely adopted in clinical settings.  Results: In this work, we propose KGML-xDTD: a Knowledge Graph-based Machine Learning framework for explainably predicting Drugs Treating Diseases. It is a two-module framework that not only predicts the treatment probabilities between drugs/compounds and diseases but also biologically explains them via knowledge graph (KG) path-based, testable mechanisms of action (MOAs). We leverage know
&lt;/p&gt;</description></item><item><title>xTrimoABFold&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#25239;&#20307;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;&#25239;&#20307;&#32467;&#26500;&#39044;&#27979;&#26041;&#27861;&#65292;&#26080;&#38656;&#22810;&#24207;&#21015;&#27604;&#23545;&#65292;&#26377;&#26395;&#20419;&#36827;&#39640;&#36890;&#37327;&#33647;&#29289;&#35774;&#35745;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2212.00735</link><description>&lt;p&gt;
xTrimoABFold&#65306;&#26080;&#22810;&#24207;&#21015;&#27604;&#23545;&#30340;&#26032;&#22411;&#25239;&#20307;&#32467;&#26500;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
xTrimoABFold: De novo Antibody Structure Prediction without MSA. (arXiv:2212.00735v2 [q-bio.QM] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00735
&lt;/p&gt;
&lt;p&gt;
xTrimoABFold&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#25239;&#20307;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;&#25239;&#20307;&#32467;&#26500;&#39044;&#27979;&#26041;&#27861;&#65292;&#26080;&#38656;&#22810;&#24207;&#21015;&#27604;&#23545;&#65292;&#26377;&#26395;&#20419;&#36827;&#39640;&#36890;&#37327;&#33647;&#29289;&#35774;&#35745;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25239;&#20307;&#24037;&#31243;&#39046;&#22495;&#65292;&#35774;&#35745;&#19968;&#20010;&#26032;&#22411;&#25239;&#20307;&#20197;&#27491;&#30830;&#22320;&#32467;&#21512;&#29305;&#23450;&#25239;&#21407;&#30340;&#34920;&#20301;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#20102;&#35299;&#25239;&#20307;&#32467;&#26500;&#21644;&#20854;&#34920;&#20301;&#21487;&#20197;&#20419;&#36827;&#23545;&#20854;&#21151;&#33021;&#30340;&#26426;&#21046;&#29702;&#35299;&#12290;&#22240;&#27492;&#65292;&#20174;&#20854;&#24207;&#21015;&#39044;&#27979;&#25239;&#20307;&#32467;&#26500;&#19968;&#30452;&#26159;&#19968;&#39033;&#39640;&#24230;&#26377;&#20215;&#20540;&#30340;&#20219;&#21153;&#65292;&#32780;AlphaFold2&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#34507;&#30333;&#36136;&#24207;&#21015;&#39044;&#27979;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#23545;&#20110;&#25239;&#20307;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#25239;&#20307;&#30340;&#20114;&#34917;&#20915;&#23450;&#21306;&#65288;CDRs&#65289;&#65292;&#20854;&#39044;&#27979;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#26377;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of antibody engineering, an essential task is to design a novel antibody whose paratopes bind to a specific antigen with correct epitopes. Understanding antibody structure and its paratope can facilitate a mechanistic understanding of its function. Therefore, antibody structure prediction from its sequence alone has always been a highly valuable problem for de novo antibody design. AlphaFold2, a breakthrough in the field of structural biology, provides a solution to predict protein structure based on protein sequences and computationally expensive coevolutionary multiple sequence alignments (MSAs). However, the computational efficiency and undesirable prediction accuracy of antibodies, especially on the complementarity-determining regions (CDRs) of antibodies limit their applications in the industrially high-throughput drug design. To learn an informative representation of antibodies, we employed a deep antibody language model (ALM) on curated sequences from the observed a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;All-in-One Predictor&#65288;AIO-P&#65289;&#65292;&#26088;&#22312;&#39044;&#35757;&#32451;&#31070;&#32463;&#39044;&#27979;&#22120;&#65292;&#20351;&#29992;&#26469;&#33258;&#22810;&#20010;&#29420;&#31435;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#20219;&#21153;&#22495;&#21644;&#22810;&#20010;&#26550;&#26500;&#31354;&#38388;&#30340;&#26550;&#26500;&#31034;&#20363;&#65292;&#24182;&#36716;&#31227;&#21040;&#26410;&#20351;&#29992;&#30340;&#19979;&#28216;CV&#20219;&#21153;&#25110;&#31070;&#32463;&#26550;&#26500;&#19978;&#12290;</title><link>http://arxiv.org/abs/2211.17228</link><description>&lt;p&gt;
AIO-P&#65306;&#23558;&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#39044;&#27979;&#25193;&#23637;&#21040;&#22270;&#20687;&#20998;&#31867;&#20043;&#22806;
&lt;/p&gt;
&lt;p&gt;
AIO-P: Expanding Neural Performance Predictors Beyond Image Classification. (arXiv:2211.17228v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.17228
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;All-in-One Predictor&#65288;AIO-P&#65289;&#65292;&#26088;&#22312;&#39044;&#35757;&#32451;&#31070;&#32463;&#39044;&#27979;&#22120;&#65292;&#20351;&#29992;&#26469;&#33258;&#22810;&#20010;&#29420;&#31435;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#20219;&#21153;&#22495;&#21644;&#22810;&#20010;&#26550;&#26500;&#31354;&#38388;&#30340;&#26550;&#26500;&#31034;&#20363;&#65292;&#24182;&#36716;&#31227;&#21040;&#26410;&#20351;&#29992;&#30340;&#19979;&#28216;CV&#20219;&#21153;&#25110;&#31070;&#32463;&#26550;&#26500;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#23545;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#26159;&#36825;&#26159;&#19968;&#20010;&#26114;&#36149;&#30340;&#36807;&#31243;&#12290;&#31070;&#32463;&#39044;&#27979;&#22120;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#23558;&#26550;&#26500;&#35270;&#20026;&#26679;&#26412;&#65292;&#23398;&#20064;&#20272;&#35745;&#20854;&#22312;&#32473;&#23450;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#39044;&#27979;&#22120;&#26159;&#20219;&#21153;&#30456;&#20851;&#30340;&#65292;&#20027;&#35201;&#20272;&#35745;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#24615;&#33021;&#12290;&#23427;&#20204;&#20063;&#26159;&#25628;&#32034;&#31354;&#38388;&#30456;&#20851;&#30340;&#65307;&#27599;&#20010;&#39044;&#27979;&#22120;&#37117;&#35774;&#35745;&#20026;&#38024;&#23545;&#20855;&#26377;&#39044;&#23450;&#20041;&#25299;&#25169;&#21644;&#25805;&#20316;&#38598;&#30340;&#29305;&#23450;&#26550;&#26500;&#25628;&#32034;&#31354;&#38388;&#36827;&#34892;&#39044;&#27979;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;All-in-One Predictor&#65288;AIO-P&#65289;&#65292;&#26088;&#22312;&#39044;&#35757;&#32451;&#31070;&#32463;&#39044;&#27979;&#22120;&#65292;&#20351;&#29992;&#26469;&#33258;&#22810;&#20010;&#29420;&#31435;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#20219;&#21153;&#22495;&#21644;&#22810;&#20010;&#26550;&#26500;&#31354;&#38388;&#30340;&#26550;&#26500;&#31034;&#20363;&#65292;&#24182;&#36716;&#31227;&#21040;&#26410;&#20351;&#29992;&#30340;&#19979;&#28216;CV&#20219;&#21153;&#25110;&#31070;&#32463;&#26550;&#26500;&#19978;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#36890;&#29992;&#22270;&#24418;&#34920;&#31034;&#12289;&#39640;&#25928;&#39044;&#27979;&#22120;&#39044;&#35757;&#32451;&#21644;&#30693;&#35782;&#27880;&#20837;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating neural network performance is critical to deep neural network design but a costly procedure. Neural predictors provide an efficient solution by treating architectures as samples and learning to estimate their performance on a given task. However, existing predictors are task-dependent, predominantly estimating neural network performance on image classification benchmarks. They are also search-space dependent; each predictor is designed to make predictions for a specific architecture search space with predefined topologies and set of operations. In this paper, we propose a novel All-in-One Predictor (AIO-P), which aims to pretrain neural predictors on architecture examples from multiple, separate computer vision (CV) task domains and multiple architecture spaces, and then transfer to unseen downstream CV tasks or neural architectures. We describe our proposed techniques for general graph representation, efficient predictor pretraining and knowledge infusion techniques, as wel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GENNAPE&#30340;&#36890;&#29992;&#31070;&#32463;&#26550;&#26500;&#24615;&#33021;&#20272;&#35745;&#22120;&#65292;&#21487;&#20197;&#27867;&#21270;&#21040;&#23436;&#20840;&#26410;&#30693;&#30340;&#26550;&#26500;&#20013;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#32593;&#32476;&#34920;&#31034;&#12289;&#23545;&#27604;&#39044;&#35757;&#32451;&#21644;&#22522;&#20110;&#27169;&#31946;&#32858;&#31867;&#30340;&#39044;&#27979;&#22120;&#38598;&#25104;&#26041;&#27861;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2211.17226</link><description>&lt;p&gt;
GENNAPE&#65306;&#38754;&#21521;&#36890;&#29992;&#31070;&#32463;&#26550;&#26500;&#24615;&#33021;&#20272;&#35745;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
GENNAPE: Towards Generalized Neural Architecture Performance Estimators. (arXiv:2211.17226v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.17226
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GENNAPE&#30340;&#36890;&#29992;&#31070;&#32463;&#26550;&#26500;&#24615;&#33021;&#20272;&#35745;&#22120;&#65292;&#21487;&#20197;&#27867;&#21270;&#21040;&#23436;&#20840;&#26410;&#30693;&#30340;&#26550;&#26500;&#20013;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#32593;&#32476;&#34920;&#31034;&#12289;&#23545;&#27604;&#39044;&#35757;&#32451;&#21644;&#22522;&#20110;&#27169;&#31946;&#32858;&#31867;&#30340;&#39044;&#27979;&#22120;&#38598;&#25104;&#26041;&#27861;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#24615;&#33021;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#19988;&#20851;&#38190;&#30340;&#20219;&#21153;&#65292;&#23545;&#20110;&#31070;&#32463;&#26550;&#26500;&#35774;&#35745;&#19982;&#25628;&#32034;&#21313;&#20998;&#37325;&#35201;&#12290;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#20381;&#36182;&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#39044;&#27979;&#22120;&#65292;&#22312;&#39044;&#20808;&#23450;&#20041;&#22909;&#30340;&#35774;&#35745;&#31354;&#38388;&#20869;&#24314;&#27169;&#26550;&#26500;&#65292;&#38590;&#20197;&#25512;&#24191;&#21040;&#26410;&#30693;&#30340;&#26550;&#26500;&#65292;&#35201;&#20040;&#37319;&#29992;&#38646;&#25104;&#26412;&#20195;&#29702;&#65292;&#20294;&#19981;&#24635;&#26159;&#20934;&#30830;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;GENNAPE&#65292;&#19968;&#31181;&#39044;&#20808;&#35757;&#32451;&#20110;&#24320;&#25918;&#31070;&#32463;&#26550;&#26500;&#22522;&#20934;&#30340;&#36890;&#29992;&#31070;&#32463;&#26550;&#26500;&#24615;&#33021;&#20272;&#35745;&#22120;&#65292;&#26088;&#22312;&#36890;&#36807;&#32593;&#32476;&#34920;&#31034;&#12289;&#23545;&#27604;&#39044;&#35757;&#32451;&#21644;&#22522;&#20110;&#27169;&#31946;&#32858;&#31867;&#30340;&#39044;&#27979;&#22120;&#38598;&#25104;&#31561;&#21019;&#26032;&#23454;&#29616;&#23545;&#23436;&#20840;&#26410;&#30693;&#26550;&#26500;&#30340;&#27867;&#21270;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;GENNAPE&#23558;&#32473;&#23450;&#30340;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#20026;&#21407;&#23376;&#25805;&#20316;&#30340;&#35745;&#31639;&#22270;&#65292;&#21487;&#20197;&#27169;&#25311;&#20219;&#24847;&#26550;&#26500;&#12290;&#23427;&#39318;&#20808;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#23398;&#20064;&#22270;&#34920;&#32534;&#30721;&#22120;&#65292;&#20197;&#40723;&#21169;&#32593;&#32476;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Predicting neural architecture performance is a challenging task and is crucial to neural architecture design and search. Existing approaches either rely on neural performance predictors which are limited to modeling architectures in a predefined design space involving specific sets of operators and connection rules, and cannot generalize to unseen architectures, or resort to zero-cost proxies which are not always accurate. In this paper, we propose GENNAPE, a Generalized Neural Architecture Performance Estimator, which is pretrained on open neural architecture benchmarks, and aims to generalize to completely unseen architectures through combined innovations in network representation, contrastive pretraining, and fuzzy clustering-based predictor ensemble. Specifically, GENNAPE represents a given neural network as a Computation Graph (CG) of atomic operations which can model an arbitrary architecture. It first learns a graph encoder via Contrastive Learning to encourage network separati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;GNN&#27169;&#25311;&#39030;&#28857;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#19968;&#20010;&#34987;&#31216;&#20026;&#20998;&#31163;&#31209;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#37327;&#21270;&#36825;&#31181;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;&#27169;&#25311;&#30456;&#20114;&#20316;&#29992;&#30340;&#33021;&#21147;&#20027;&#35201;&#21462;&#20915;&#20110;&#20998;&#21306;&#30340;&#34892;&#36208;&#25351;&#25968;&#65292;&#21363;&#20174;&#20998;&#30028;&#32447;&#24320;&#22987;&#30340;&#34892;&#36208;&#25968;&#37327;&#65292;&#21516;&#26102;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;WISA&#30340;&#36793;&#31232;&#30095;&#21270;&#31639;&#27861;&#20197;&#25552;&#39640;GNNs&#30340;&#22788;&#29702;&#25928;&#29575;&#21644;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2211.16494</link><description>&lt;p&gt;
&#20851;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#25311;&#39030;&#28857;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Ability of Graph Neural Networks to Model Interactions Between Vertices. (arXiv:2211.16494v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16494
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;GNN&#27169;&#25311;&#39030;&#28857;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#19968;&#20010;&#34987;&#31216;&#20026;&#20998;&#31163;&#31209;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#37327;&#21270;&#36825;&#31181;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;&#27169;&#25311;&#30456;&#20114;&#20316;&#29992;&#30340;&#33021;&#21147;&#20027;&#35201;&#21462;&#20915;&#20110;&#20998;&#21306;&#30340;&#34892;&#36208;&#25351;&#25968;&#65292;&#21363;&#20174;&#20998;&#30028;&#32447;&#24320;&#22987;&#30340;&#34892;&#36208;&#25968;&#37327;&#65292;&#21516;&#26102;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;WISA&#30340;&#36793;&#31232;&#30095;&#21270;&#31639;&#27861;&#20197;&#25552;&#39640;GNNs&#30340;&#22788;&#29702;&#25928;&#29575;&#21644;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#34987;&#24191;&#27867;&#29992;&#20110;&#24314;&#27169;&#30001;&#22270;&#20013;&#39030;&#28857;&#34920;&#31034;&#30340;&#23454;&#20307;&#20043;&#38388;&#30340;&#22797;&#26434;&#20114;&#21160;&#12290;&#23613;&#31649;&#26368;&#36817;&#26377;&#19968;&#20123;&#29702;&#35770;&#20998;&#26512;GNNs&#34920;&#36798;&#33021;&#21147;&#30340;&#21162;&#21147;&#65292;&#20294;&#23545;&#20854;&#27169;&#25311;&#30456;&#20114;&#20316;&#29992;&#30340;&#33021;&#21147;&#32570;&#20047;&#19968;&#20010;&#27491;&#24335;&#30340;&#25551;&#36848;&#12290;&#26412;&#25991;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#36890;&#36807;&#19968;&#20010;&#24050;&#30693;&#30340;&#24230;&#37327;&#26631;&#20934;&#8212;&#8212;&#20998;&#31163;&#31209;(separation rank)&#26469;&#35268;&#33539;&#21270;&#30456;&#20114;&#20316;&#29992;&#30340;&#24378;&#24230;&#65292;&#25105;&#20204;&#37327;&#21270;&#20102;&#26576;&#20123;GNNs&#27169;&#25311;&#32473;&#23450;&#39030;&#28857;&#23376;&#38598;&#21450;&#20854;&#34917;&#38598;&#20043;&#38388;&#20132;&#20114;&#30340;&#33021;&#21147;&#65292;&#21363;&#36755;&#20837;&#39030;&#28857;&#32452;&#25104;&#30340;&#32473;&#23450;&#20998;&#21306;&#30340;&#20004;&#20391;&#20043;&#38388;&#30340;&#20114;&#21160;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#25311;&#30456;&#20114;&#20316;&#29992;&#30340;&#33021;&#21147;&#20027;&#35201;&#21462;&#20915;&#20110;&#20998;&#21306;&#30340;&#34892;&#36208;&#25351;&#25968;(walk index)&#8212;&#8212;&#19968;&#20010;&#30001;&#20998;&#30028;&#32447;&#24320;&#22987;&#30340;&#34892;&#36208;&#25968;&#37327;&#23450;&#20041;&#30340;&#22270;&#24418;&#29305;&#24449;&#12290;&#24120;&#35265;GNN&#26550;&#26500;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#19968;&#21457;&#29616;&#12290;&#20316;&#20026;&#25105;&#20204;&#29702;&#35770;&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;Walk Indexed Sparsification Algorithm (WISA)&#30340;&#36793;&#31232;&#30095;&#21270;&#31639;&#27861;&#65292;&#21033;&#29992;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25552;&#39640;&#22788;&#29702;&#22823;&#35268;&#27169;&#22270;&#24418;&#30340;GNNs&#25928;&#29575;&#21516;&#26102;&#20445;&#25345;&#23427;&#20204;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) are widely used for modeling complex interactions between entities represented as vertices of a graph. Despite recent efforts to theoretically analyze the expressive power of GNNs, a formal characterization of their ability to model interactions is lacking. The current paper aims to address this gap. Formalizing strength of interactions through an established measure known as separation rank, we quantify the ability of certain GNNs to model interaction between a given subset of vertices and its complement, i.e. between the sides of a given partition of input vertices. Our results reveal that the ability to model interaction is primarily determined by the partition's walk index -- a graph-theoretical characteristic defined by the number of walks originating from the boundary of the partition. Experiments with common GNN architectures corroborate this finding. As a practical application of our theory, we design an edge sparsification algorithm named Walk Inde
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#21453;&#24212;&#25193;&#25955;&#26041;&#31243;&#30340;GNN&#26041;&#27861;&#65292;&#32771;&#34385;&#20102;&#25152;&#26377;&#27969;&#34892;&#30340;&#21453;&#24212;&#26041;&#31243;&#31867;&#22411;&#21644;&#19968;&#31181;&#29305;&#27530;&#30340;&#21453;&#24212;&#26041;&#31243;&#65292;&#26159;&#30446;&#21069;&#20854;&#20013;&#26368;&#20840;&#38754;&#30340;&#30740;&#31350;&#20043;&#19968;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.14208</link><description>&lt;p&gt;
GREAD: &#22522;&#20110;&#22270;&#31070;&#32463;&#21453;&#24212;&#25193;&#25955;&#32593;&#32476;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
GREAD: Graph Neural Reaction-Diffusion Networks. (arXiv:2211.14208v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14208
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#21453;&#24212;&#25193;&#25955;&#26041;&#31243;&#30340;GNN&#26041;&#27861;&#65292;&#32771;&#34385;&#20102;&#25152;&#26377;&#27969;&#34892;&#30340;&#21453;&#24212;&#26041;&#31243;&#31867;&#22411;&#21644;&#19968;&#31181;&#29305;&#27530;&#30340;&#21453;&#24212;&#26041;&#31243;&#65292;&#26159;&#30446;&#21069;&#20854;&#20013;&#26368;&#20840;&#38754;&#30340;&#30740;&#31350;&#20043;&#19968;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#26368;&#21463;&#27426;&#36814;&#30340;&#30740;&#31350;&#35838;&#39064;&#20043;&#19968;&#12290;GNN&#26041;&#27861;&#36890;&#24120;&#26159;&#22522;&#20110;&#22270;&#20449;&#21495;&#22788;&#29702;&#29702;&#35770;&#36827;&#34892;&#35774;&#35745;&#30340;&#12290;&#29305;&#21035;&#22320;&#65292;&#25193;&#25955;&#26041;&#31243;&#34987;&#24191;&#27867;&#29992;&#20110;&#35774;&#35745;GNN&#30340;&#26680;&#24515;&#22788;&#29702;&#23618;&#65292;&#22240;&#27492;&#19981;&#21487;&#36991;&#20813;&#22320;&#23481;&#26131;&#20986;&#29616;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;&#26368;&#36817;&#65292;&#26377;&#20960;&#31687;&#35770;&#25991;&#27880;&#24847;&#21040;&#21453;&#24212;&#26041;&#31243;&#21644;&#25193;&#25955;&#26041;&#31243;&#30340;&#32467;&#21512;&#12290;&#19981;&#36807;&#65292;&#23427;&#20204;&#37117;&#21482;&#32771;&#34385;&#20102;&#26377;&#38480;&#30340;&#21453;&#24212;&#26041;&#31243;&#24418;&#24335;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21453;&#24212;&#25193;&#25955;&#26041;&#31243;&#30340;GNN&#26041;&#27861;&#65292;&#32771;&#34385;&#20102;&#25152;&#26377;&#27969;&#34892;&#30340;&#21453;&#24212;&#26041;&#31243;&#31867;&#22411;&#21644;&#25105;&#20204;&#35774;&#35745;&#30340;&#19968;&#31181;&#29305;&#27530;&#21453;&#24212;&#26041;&#31243;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#30340;&#35770;&#25991;&#26159;&#20851;&#20110;&#22522;&#20110;&#21453;&#24212;&#25193;&#25955;&#26041;&#31243;&#30340;GNN&#30340;&#26368;&#20840;&#38754;&#30340;&#30740;&#31350;&#20043;&#19968;&#12290;&#22312;&#25105;&#20204;&#20351;&#29992;9&#20010;&#25968;&#25454;&#38598;&#21644;28&#20010;&#22522;&#20934;&#27169;&#22411;&#36827;&#34892;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861; GREAD &#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#24615;&#33021;&#20248;&#20110;&#23427;&#20204;&#12290;&#36827;&#19968;&#27493;&#30340;&#20154;&#24037;&#25968;&#25454;&#23454;&#39564;&#26174;&#31034;&#8230;&#8230;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) are one of the most popular research topics for deep learning. GNN methods typically have been designed on top of the graph signal processing theory. In particular, diffusion equations have been widely used for designing the core processing layer of GNNs, and therefore they are inevitably vulnerable to the notorious oversmoothing problem. Recently, a couple of papers paid attention to reaction equations in conjunctions with diffusion equations. However, they all consider limited forms of reaction equations. To this end, we present a reaction-diffusion equation-based GNN method that considers all popular types of reaction equations in addition to one special reaction equation designed by us. To our knowledge, our paper is one of the most comprehensive studies on reaction-diffusion equation-based GNNs. In our experiments with 9 datasets and 28 baselines, our method, called GREAD, outperforms them in a majority of cases. Further synthetic data experiments show
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#36890;&#36807;&#24046;&#20998;&#38544;&#31169;&#20445;&#25252;COVID-19&#26816;&#27979;&#27169;&#22411;&#65292;&#35299;&#20915;&#25968;&#25454;&#20998;&#26512;&#21644;&#24739;&#32773;&#38544;&#31169;&#20445;&#25252;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#40657;&#30418;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#65292;&#23454;&#29616;&#20102;&#23545;&#23454;&#38469;&#38544;&#31169;&#30340;&#35780;&#20272;&#65292;&#32467;&#35770;&#34920;&#26126;&#25152;&#38656;&#30340;&#38544;&#31169;&#31561;&#32423;&#21487;&#33021;&#22240;&#21463;&#21040;&#23454;&#38469;&#23041;&#32961;&#30340;&#20219;&#21153;&#32780;&#24322;&#12290;</title><link>http://arxiv.org/abs/2211.11434</link><description>&lt;p&gt;
&#23454;&#36341;&#20013;&#30340;&#38544;&#31169;&#65306;X&#23556;&#32447;&#22270;&#20687;&#20013;&#30340;COVID-19&#26816;&#27979;&#30340;&#31169;&#26377;&#21270;&#65288;&#25193;&#23637;&#29256;&#65289;
&lt;/p&gt;
&lt;p&gt;
Privacy in Practice: Private COVID-19 Detection in X-Ray Images (Extended Version). (arXiv:2211.11434v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11434
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#36890;&#36807;&#24046;&#20998;&#38544;&#31169;&#20445;&#25252;COVID-19&#26816;&#27979;&#27169;&#22411;&#65292;&#35299;&#20915;&#25968;&#25454;&#20998;&#26512;&#21644;&#24739;&#32773;&#38544;&#31169;&#20445;&#25252;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#40657;&#30418;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#65292;&#23454;&#29616;&#20102;&#23545;&#23454;&#38469;&#38544;&#31169;&#30340;&#35780;&#20272;&#65292;&#32467;&#35770;&#34920;&#26126;&#25152;&#38656;&#30340;&#38544;&#31169;&#31561;&#32423;&#21487;&#33021;&#22240;&#21463;&#21040;&#23454;&#38469;&#23041;&#32961;&#30340;&#20219;&#21153;&#32780;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21487;&#20197;&#36890;&#36807;&#20351;&#22823;&#37327;&#22270;&#20687;&#24555;&#36895;&#31579;&#36873;&#26469;&#24110;&#21161;&#25239;&#20987;COVID-19&#31561;&#20840;&#29699;&#22823;&#27969;&#34892;&#30149;&#12290;&#20026;&#20102;&#22312;&#20445;&#25252;&#24739;&#32773;&#38544;&#31169;&#30340;&#21516;&#26102;&#36827;&#34892;&#25968;&#25454;&#20998;&#26512;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#28385;&#36275;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#35201;&#27714;&#30340;ML&#27169;&#22411;&#12290;&#20197;&#24448;&#25506;&#32034;&#31169;&#26377;COVID-19&#27169;&#22411;&#30340;&#30740;&#31350;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#22522;&#20110;&#23567;&#22411;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#36739;&#24369;&#25110;&#19981;&#26126;&#30830;&#30340;&#38544;&#31169;&#20445;&#35777;&#65292;&#24182;&#19988;&#27809;&#26377;&#30740;&#31350;&#23454;&#38469;&#38544;&#31169;&#12290;&#25105;&#20204;&#25552;&#20986;&#25913;&#36827;&#25514;&#26045;&#20197;&#35299;&#20915;&#36825;&#20123;&#31354;&#32570;&#12290;&#25105;&#20204;&#32771;&#34385;&#22825;&#29983;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#65292;&#24182;&#26356;&#24191;&#27867;&#22320;&#35780;&#20272;&#25928;&#29992;-&#38544;&#31169;&#26435;&#34913;&#20197;&#21450;&#26356;&#20005;&#26684;&#30340;&#38544;&#31169;&#39044;&#31639;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#24471;&#21040;&#40657;&#30418;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#65288;MIAs&#65289;&#30340;&#23454;&#38469;&#38544;&#31169;&#20272;&#35745;&#25903;&#25345;&#12290;&#24341;&#20837;&#30340;DP&#24212;&#26377;&#21161;&#20110;&#38480;&#21046;MIAs&#24102;&#26469;&#30340;&#27844;&#28431;&#23041;&#32961;&#65292;&#32780;&#25105;&#20204;&#30340;&#23454;&#38469;&#20998;&#26512;&#26159;&#31532;&#19968;&#20010;&#22312;COVID-19&#20998;&#31867;&#20219;&#21153;&#19978;&#27979;&#35797;&#36825;&#20010;&#20551;&#35774;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) can help fight pandemics like COVID-19 by enabling rapid screening of large volumes of images. To perform data analysis while maintaining patient privacy, we create ML models that satisfy Differential Privacy (DP). Previous works exploring private COVID-19 models are in part based on small datasets, provide weaker or unclear privacy guarantees, and do not investigate practical privacy. We suggest improvements to address these open gaps. We account for inherent class imbalances and evaluate the utility-privacy trade-off more extensively and over stricter privacy budgets. Our evaluation is supported by empirically estimating practical privacy through black-box Membership Inference Attacks (MIAs). The introduced DP should help limit leakage threats posed by MIAs, and our practical analysis is the first to test this hypothesis on the COVID-19 classification task. Our results indicate that needed privacy levels might differ based on the task-dependent practical threat 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#26500;&#24314;&#20154;&#33080;&#21644;&#36523;&#20307;&#26816;&#27979;&#22120;&#65292;&#21516;&#26102;&#21033;&#29992;&#22823;&#37327;&#26410;&#26631;&#27880;&#30340;&#25968;&#25454;&#12290;&#20854;&#20013;&#21033;&#29992;&#26679;&#24335;&#36716;&#31227;&#25972;&#21512;&#36328;&#22495;&#26631;&#27880;&#22270;&#20687;&#26469;&#24341;&#23548;&#26816;&#27979;&#22120;&#12290;</title><link>http://arxiv.org/abs/2211.10641</link><description>&lt;p&gt;
&#38754;&#21521;&#32472;&#30011;&#20013;&#30340;&#20154;&#33080;&#21644;&#36523;&#20307;&#26816;&#27979;&#30340;&#33258;&#36866;&#24212;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Domain-Adaptive Self-Supervised Pre-Training for Face &amp; Body Detection in Drawings. (arXiv:2211.10641v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10641
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#26500;&#24314;&#20154;&#33080;&#21644;&#36523;&#20307;&#26816;&#27979;&#22120;&#65292;&#21516;&#26102;&#21033;&#29992;&#22823;&#37327;&#26410;&#26631;&#27880;&#30340;&#25968;&#25454;&#12290;&#20854;&#20013;&#21033;&#29992;&#26679;&#24335;&#36716;&#31227;&#25972;&#21512;&#36328;&#22495;&#26631;&#27880;&#22270;&#20687;&#26469;&#24341;&#23548;&#26816;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32472;&#30011;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#22270;&#20687;&#25277;&#35937;&#21644;&#20132;&#27969;&#25163;&#27573;&#12290;&#29702;&#35299;&#21508;&#31181;&#21508;&#26679;&#30340;&#32472;&#30011;&#24418;&#24335;&#65292;&#21253;&#25324;&#25968;&#23383;&#33402;&#26415;&#12289;&#28459;&#30011;&#21644;&#36830;&#29615;&#30011;&#65292;&#19968;&#30452;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#35745;&#31639;&#26426;&#22270;&#24418;&#23398;&#30028;&#20851;&#27880;&#30340;&#20027;&#35201;&#38382;&#39064;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#22522;&#20110;&#25945;&#24072;-&#23398;&#29983;&#32593;&#32476;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#20462;&#25913;&#21518;&#30340;&#23398;&#29983;&#32593;&#32476;&#26356;&#26032;&#35774;&#35745;&#65292;&#26500;&#24314;&#20154;&#33080;&#21644;&#36523;&#20307;&#26816;&#27979;&#22120;&#12290;&#25105;&#20204;&#30340;&#35774;&#32622;&#20801;&#35768;&#22312;&#21482;&#25552;&#20379;&#30446;&#26631;&#22495;&#30340;&#19968;&#23567;&#37096;&#20998;&#26631;&#27880;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#21033;&#29992;&#22823;&#37327;&#26410;&#26631;&#27880;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#65292;&#21487;&#20197;&#23558;&#26679;&#24335;&#36716;&#31227;&#25972;&#21512;&#21040;&#25105;&#20204;&#30340;&#23398;&#20064;&#27969;&#31243;&#20013;&#65292;&#20351;&#29992;&#26469;&#33258;&#33258;&#28982;&#22270;&#20687;&#65288;&#21363;&#26469;&#33258;&#30495;&#23454;&#19990;&#30028;&#30340;&#22270;&#20687;&#65289;&#30340;&#22823;&#37327;&#36328;&#22495;&#26631;&#27880;&#22270;&#20687;&#26469;&#24341;&#23548;&#26816;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Drawings are powerful means of pictorial abstraction and communication. Understanding diverse forms of drawings, including digital arts, cartoons, and comics, has been a major problem of interest for the computer vision and computer graphics communities. Although there are large amounts of digitized drawings from comic books and cartoons, they contain vast stylistic variations, which necessitate expensive manual labeling for training domain-specific recognizers. In this work, we show how self-supervised learning, based on a teacher-student network with a modified student network update design, can be used to build face and body detectors. Our setup allows exploiting large amounts of unlabeled data from the target domain when labels are provided for only a small subset of it. We further demonstrate that style transfer can be incorporated into our learning pipeline to bootstrap detectors using a vast amount of out-of-domain labeled images from natural images (i.e., images from the real w
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32479;&#35745;&#26816;&#39564;&#30340; MMD-B-Fair &#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#20844;&#24179;&#30340;&#25968;&#25454;&#34920;&#31034;&#65292;&#24182;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2211.07907</link><description>&lt;p&gt;
&#22522;&#20110;&#32479;&#35745;&#26816;&#39564;&#30340;MMD-B-Fair&#65306;&#23398;&#20064;&#20844;&#24179;&#30340;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
MMD-B-Fair: Learning Fair Representations with Statistical Testing. (arXiv:2211.07907v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07907
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32479;&#35745;&#26816;&#39564;&#30340; MMD-B-Fair &#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#20844;&#24179;&#30340;&#25968;&#25454;&#34920;&#31034;&#65292;&#24182;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26680;&#21452;&#26679;&#26412;&#27979;&#35797;&#23398;&#20064;&#25968;&#25454;&#20844;&#24179;&#34920;&#31034;&#30340;&#26041;&#27861;MMD-B-Fair&#12290;&#25105;&#20204;&#25214;&#21040;&#20102;&#25968;&#25454;&#30340;&#31070;&#32463;&#29305;&#24449;&#65292;&#20854;&#20013;&#26368;&#22823;&#24179;&#22343;&#20559;&#24046;&#65288;MMD&#65289;&#27979;&#35797;&#26080;&#27861;&#21306;&#20998;&#19981;&#21516;&#25935;&#24863;&#32452;&#30340;&#34920;&#31034;&#65292;&#21516;&#26102;&#20445;&#30041;&#26377;&#20851;&#30446;&#26631;&#23646;&#24615;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#22359;&#27979;&#35797;&#26041;&#26696;&#30340;&#31616;&#21333;&#28176;&#36817;&#24615;&#33021;&#22815;&#26377;&#25928;&#22320;&#25214;&#21040;&#20844;&#24179;&#34920;&#31034;&#65292;&#32780;&#19981;&#38656;&#35201;&#20351;&#29992;&#29616;&#26377;&#20844;&#24179;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#22797;&#26434;&#23545;&#25239;&#24615;&#20248;&#21270;&#25110;&#29983;&#25104;&#24314;&#27169;&#26041;&#26696;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#26174;&#31034;&#20854;&#33021;&#22815;&#8220;&#38544;&#34255;&#8221;&#26377;&#20851;&#25935;&#24863;&#23646;&#24615;&#30340;&#20449;&#24687;&#65292;&#24182;&#22312;&#19979;&#28216;&#20256;&#36755;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a method, MMD-B-Fair, to learn fair representations of data via kernel two-sample testing. We find neural features of our data where a maximum mean discrepancy (MMD) test cannot distinguish between representations of different sensitive groups, while preserving information about the target attributes. Minimizing the power of an MMD test is more difficult than maximizing it (as done in previous work), because the test threshold's complex behavior cannot be simply ignored. Our method exploits the simple asymptotics of block testing schemes to efficiently find fair representations without requiring complex adversarial optimization or generative modelling schemes widely used by existing work on fair representation learning. We evaluate our approach on various datasets, showing its ability to ``hide'' information about sensitive attributes, and its effectiveness in downstream transfer tasks.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23433;&#20840;&#28508;&#21521;&#25193;&#25955;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#20013;&#31227;&#38500;&#21644;&#25233;&#21046;&#19981;&#24403;&#30340;&#22270;&#20687;&#37096;&#20998;&#65292;&#20174;&#32780;&#32531;&#35299;&#22522;&#20110;&#25991;&#26412;&#30340;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#22240;&#19981;&#24403;&#25968;&#25454;&#38598;&#24102;&#26469;&#30340;&#19981;&#33391;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2211.05105</link><description>&lt;p&gt;
&#23433;&#20840;&#28508;&#21521;&#25193;&#25955;&#65306;&#32531;&#35299;&#25193;&#25955;&#27169;&#22411;&#20013;&#19981;&#24403;&#36864;&#21270;
&lt;/p&gt;
&lt;p&gt;
Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models. (arXiv:2211.05105v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05105
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23433;&#20840;&#28508;&#21521;&#25193;&#25955;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#20013;&#31227;&#38500;&#21644;&#25233;&#21046;&#19981;&#24403;&#30340;&#22270;&#20687;&#37096;&#20998;&#65292;&#20174;&#32780;&#32531;&#35299;&#22522;&#20110;&#25991;&#26412;&#30340;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#22240;&#19981;&#24403;&#25968;&#25454;&#38598;&#24102;&#26469;&#30340;&#19981;&#33391;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#25991;&#26412;&#30340;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#22312;&#22270;&#20687;&#36136;&#37327;&#21644;&#25991;&#26412;&#23545;&#40784;&#26041;&#38754;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#25104;&#26524;&#65292;&#24182;&#22240;&#27492;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#36234;&#26469;&#36234;&#22810;&#30340;&#24212;&#29992;&#31243;&#24207;&#12290;&#30001;&#20110;&#23427;&#20204;&#39640;&#24230;&#20381;&#36182;&#20110;&#38543;&#26426;&#20174;&#20114;&#32852;&#32593;&#19978;&#25235;&#21462;&#30340;&#25968;&#21313;&#20159;&#22823;&#23567;&#30340;&#25968;&#25454;&#38598;&#65292;&#22240;&#27492;&#23427;&#20204;&#20063;&#38754;&#20020;&#26469;&#33258;&#36864;&#21270;&#21644;&#20559;&#35265;&#30340;&#20154;&#31867;&#34892;&#20026;&#30340;&#19981;&#33391;&#24433;&#21709;&#65292;&#27491;&#22914;&#25105;&#20204;&#25152;&#23637;&#31034;&#30340;&#37027;&#26679;&#12290;&#21453;&#36807;&#26469;&#65292;&#23427;&#20204;&#29978;&#33267;&#21487;&#33021;&#24378;&#21270;&#36825;&#20123;&#20559;&#35265;&#12290;&#20026;&#20102;&#24110;&#21161;&#24212;&#23545;&#36825;&#20123;&#19981;&#33391;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23433;&#20840;&#28508;&#21521;&#25193;&#25955;&#65288;SLD&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20026;&#20102;&#34913;&#37327;&#30001;&#20110;&#26410;&#36807;&#28388;&#21644;&#19981;&#24179;&#34913;&#30340;&#35757;&#32451;&#38598;&#32780;&#24341;&#36215;&#30340;&#19981;&#24403;&#36864;&#21270;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22270;&#20687;&#29983;&#25104;&#27979;&#35797;&#24179;&#21488;&#8212;&#8212;&#21253;&#21547;&#19987;&#38376;&#30340;&#12289;&#35206;&#30422;&#35064;&#38706;&#21644;&#26292;&#21147;&#31561;&#27010;&#24565;&#30340;&#23454;&#38469;&#22270;&#20687;&#21040;&#25991;&#26412;&#25552;&#31034;&#30340;&#19981;&#24403;&#22270;&#20687;&#25552;&#31034;&#65288;I2P&#65289;&#12290;&#27491;&#22914;&#25105;&#20204;&#35814;&#23613;&#30340;&#23454;&#35777;&#35780;&#20272;&#25152;&#35777;&#26126;&#30340;&#37027;&#26679;&#65292;&#24341;&#20837;&#30340;SLD&#22312;&#25193;&#25955;&#36807;&#31243;&#20013;&#31227;&#38500;&#21644;&#25233;&#21046;&#20102;&#19981;&#24403;&#30340;&#22270;&#20687;&#37096;&#20998;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#65292;&#24182;&#19988;&#23545;&#22270;&#20687;&#36136;&#37327;&#27809;&#26377;&#19981;&#33391;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-conditioned image generation models have recently achieved astonishing results in image quality and text alignment and are consequently employed in a fast-growing number of applications. Since they are highly data-driven, relying on billion-sized datasets randomly scraped from the internet, they also suffer, as we demonstrate, from degenerated and biased human behavior. In turn, they may even reinforce such biases. To help combat these undesired side effects, we present safe latent diffusion (SLD). Specifically, to measure the inappropriate degeneration due to unfiltered and imbalanced training sets, we establish a novel image generation test bed-inappropriate image prompts (I2P)-containing dedicated, real-world image-to-text prompts covering concepts such as nudity and violence. As our exhaustive empirical evaluation demonstrates, the introduced SLD removes and suppresses inappropriate image parts during the diffusion process, with no additional training required and no adverse e
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#25193;&#25955;&#27169;&#22411;&#21644;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#36716;&#25442;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39640;&#23618;&#35821;&#35328;&#30446;&#26631;&#21644;&#23616;&#37096;&#35270;&#28857;&#20113;&#26500;&#24314;&#29289;&#29702;&#26377;&#25928;&#30340;&#32467;&#26500;&#65292;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#22810;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22810;&#27493;&#39588;3D&#35268;&#21010;&#20219;&#21153;&#65292;&#21363;&#20351;&#20351;&#29992;&#26410;&#30693;&#23545;&#35937;&#20173;&#33021;&#25552;&#39640;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2211.04604</link><description>&lt;p&gt;
StructDiffusion&#65306;&#20351;&#29992;&#26410;&#30693;&#23545;&#35937;&#36827;&#34892;&#29289;&#29702;&#26377;&#25928;&#32467;&#26500;&#30340;&#35821;&#35328;&#25351;&#23548;&#21019;&#24314;
&lt;/p&gt;
&lt;p&gt;
StructDiffusion: Language-Guided Creation of Physically-Valid Structures using Unseen Objects. (arXiv:2211.04604v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.04604
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#25193;&#25955;&#27169;&#22411;&#21644;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#36716;&#25442;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39640;&#23618;&#35821;&#35328;&#30446;&#26631;&#21644;&#23616;&#37096;&#35270;&#28857;&#20113;&#26500;&#24314;&#29289;&#29702;&#26377;&#25928;&#30340;&#32467;&#26500;&#65292;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#22810;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22810;&#27493;&#39588;3D&#35268;&#21010;&#20219;&#21153;&#65292;&#21363;&#20351;&#20351;&#29992;&#26410;&#30693;&#23545;&#35937;&#20173;&#33021;&#25552;&#39640;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#31867;&#29615;&#22659;&#20013;&#36816;&#20316;&#30340;&#26426;&#22120;&#20154;&#24517;&#39035;&#33021;&#22815;&#23558;&#29289;&#20307;&#37325;&#26032;&#25490;&#21015;&#25104;&#35821;&#20041;&#26377;&#24847;&#20041;&#30340;&#37197;&#32622;&#65292;&#21363;&#20351;&#36825;&#20123;&#29289;&#20307;&#20197;&#21069;&#27809;&#35265;&#36807;&#12290;&#26412;&#25991;&#20851;&#27880;&#22914;&#20309;&#22312;&#26080;&#38656;&#36880;&#27493;&#25351;&#20196;&#30340;&#24773;&#20917;&#19979;&#26500;&#24314;&#29289;&#29702;&#26377;&#25928;&#30340;&#32467;&#26500;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;StructDiffusion&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#25193;&#25955;&#27169;&#22411;&#21644;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#36716;&#25442;&#22120;&#65292;&#26681;&#25454;&#23616;&#37096;&#35270;&#28857;&#20113;&#21644;&#39640;&#32423;&#35821;&#35328;&#30446;&#26631;&#65288;&#22914;&#8220;&#25670;&#26700;&#23376;&#8221;&#65289;&#65292;&#26500;&#24314;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20351;&#29992;&#19968;&#20010;&#27169;&#22411;&#25191;&#34892;&#22810;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35821;&#35328;&#26465;&#20214;&#30340;&#22810;&#27493;&#39588;3D&#35268;&#21010;&#20219;&#21153;&#12290;&#19982;&#35757;&#32451;&#22312;&#29305;&#23450;&#32467;&#26500;&#19978;&#30340;&#29616;&#26377;&#22810;&#27169;&#24577;&#36716;&#25442;&#22120;&#27169;&#22411;&#30456;&#27604;&#65292;StructDiffusion&#29978;&#33267;&#25552;&#39640;&#20102;&#23558;&#26410;&#30693;&#23545;&#35937;&#32452;&#35013;&#25104;&#29289;&#29702;&#26377;&#25928;&#32467;&#26500;&#30340;&#25104;&#21151;&#29575;&#65292;&#24179;&#22343;&#21487;&#25552;&#39640;16&#65285;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#27169;&#25311;&#21644;&#23454;&#38469;&#37325;&#26032;&#25490;&#21015;&#20219;&#21153;&#20013;&#20351;&#29992;&#20445;&#30041;&#23545;&#35937;&#30340;&#23454;&#39564;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#25193;&#25955;&#27169;&#22411;&#21644;&#30896;&#25758;&#37492;&#21035;&#22120;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#23454;&#29616;&#23545;&#25311;&#21512;&#24615;&#20197;&#21450;&#23545;&#35821;&#35328;&#25351;&#23548;&#29289;&#29702;&#26377;&#25928;&#32467;&#26500;&#26500;&#24314;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robots operating in human environments must be able to rearrange objects into semantically-meaningful configurations, even if these objects are previously unseen. In this work, we focus on the problem of building physically-valid structures without step-by-step instructions. We propose StructDiffusion, which combines a diffusion model and an object-centric transformer to construct structures given partial-view point clouds and high-level language goals, such as "set the table". Our method can perform multiple challenging language-conditioned multi-step 3D planning tasks using one model. StructDiffusion even improves the success rate of assembling physically-valid structures out of unseen objects by on average 16% over an existing multi-modal transformer model trained on specific structures. We show experiments on held-out objects in both simulation and on real-world rearrangement tasks. Importantly, we show how integrating both a diffusion model and a collision-discriminator model allo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23569;&#26679;&#26412;&#27169;&#22411;EEG-Fest&#65292;&#23427;&#33021;&#22815;&#36890;&#36807;&#23569;&#37327;&#30340;&#26679;&#26412;&#23545;&#21496;&#26426;&#22256;&#24847;&#29366;&#24577;&#36827;&#34892;&#30417;&#27979;&#65292;&#35782;&#21035;&#24322;&#24120;&#20449;&#21495;&#24182;&#19988;&#23454;&#29616;&#20027;&#35266;&#29420;&#31435;&#20998;&#31867;&#65292;&#19988;&#22312;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#26368;&#26032;&#26368;&#20339;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2211.03878</link><description>&lt;p&gt;
EEG-Fest:&#19968;&#31181;&#22522;&#20110;&#23569;&#26679;&#26412;&#30340;EEG&#20449;&#21495;&#39537;&#21160;&#21592;&#35686;&#35273;&#20272;&#35745;&#30340;&#27880;&#24847;&#21147;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
EEG-Fest: Few-shot based Attention Network for Driver's Vigilance Estimation with EEG Signals. (arXiv:2211.03878v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.03878
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23569;&#26679;&#26412;&#27169;&#22411;EEG-Fest&#65292;&#23427;&#33021;&#22815;&#36890;&#36807;&#23569;&#37327;&#30340;&#26679;&#26412;&#23545;&#21496;&#26426;&#22256;&#24847;&#29366;&#24577;&#36827;&#34892;&#30417;&#27979;&#65292;&#35782;&#21035;&#24322;&#24120;&#20449;&#21495;&#24182;&#19988;&#23454;&#29616;&#20027;&#35266;&#29420;&#31435;&#20998;&#31867;&#65292;&#19988;&#22312;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#26368;&#26032;&#26368;&#20339;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39550;&#39542;&#21592;&#35686;&#35273;&#19981;&#36275;&#26159;&#22823;&#22810;&#25968;&#36710;&#31096;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#33041;&#30005;&#22270;&#20316;&#20026;&#26080;&#20154;&#36710;&#21496;&#26426;&#22256;&#24847;&#20272;&#35745;&#21487;&#38752;&#24615;&#39640;&#25928;&#30340;&#24037;&#20855;&#12290;&#34429;&#28982;&#20043;&#21069;&#30340;&#30740;&#31350;&#24320;&#21457;&#20986;&#20102;&#31934;&#30830;&#24378;&#38887;&#30340;&#21496;&#26426;&#35686;&#35273;&#29366;&#24577;&#26816;&#27979;&#31639;&#27861;&#65292;&#20294;&#22312;&#26679;&#26412;&#23567;&#12289;&#24322;&#24120;&#20449;&#21495;&#26816;&#27979;&#21644;&#20027;&#35266;&#29420;&#31435;&#20998;&#31867;&#31561;&#26041;&#38754;&#20173;&#38754;&#20020;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#20041;&#30340;&#23569;&#26679;&#26412;&#27169;&#22411;&#8212;&#8212;EEG-Fest&#65292;&#20197;&#25913;&#21892;&#19978;&#36848;&#32570;&#28857;&#65292;&#33021;&#22815;&#65306;(a)&#29992;&#20960;&#20010;&#26679;&#26412;&#23545;&#26597;&#35810;&#26679;&#26412;&#30340;&#22256;&#24847;&#20998;&#31867;&#65292;(b)&#35782;&#21035;&#19968;&#20010;&#26597;&#35810;&#26679;&#26412;&#26159;&#24322;&#24120;&#20449;&#21495;&#36824;&#26159;&#27491;&#24120;&#20449;&#21495;&#65292;(c)&#23454;&#29616;&#20027;&#35266;&#29420;&#31435;&#20998;&#31867;&#12290;&#35813;&#31639;&#27861;&#22312;SEED-VIG&#21644;SADT&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
A lack of driver's vigilance is the main cause of most vehicle crashes. Electroencephalography(EEG) has been reliable and efficient tool for drivers' drowsiness estimation. Even though previous studies have developed accurate and robust driver's vigilance detection algorithms, these methods are still facing challenges on following areas: (a) small sample size training, (b) anomaly signal detection, and (c) subject-independent classification. In this paper, we propose a generalized few-shot model, namely EEG-Fest, to improve aforementioned drawbacks. The EEG-Fest model can (a) classify the query sample's drowsiness with a few samples, (b) identify whether a query sample is anomaly signals or not, and (c) achieve subject independent classification. The proposed algorithm achieves state-of-the-art results on the SEED-VIG dataset and the SADT dataset. The accuracy of the drowsy class achieves 92% and 94% for 1-shot and 5-shot support samples in the SEED-VIG dataset, and 62% and 78% for 1-s
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22914;&#20309;&#26500;&#36896;&#25299;&#25169;&#24230;&#37327;&#26469;&#35745;&#31639;&#32473;&#23450;&#36317;&#31163;&#20540;&#30340;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#28041;&#21450;&#21040;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#22810;&#20010;&#28857;&#20043;&#38388;&#30340;&#26368;&#36817;&#37051;&#26597;&#35810;&#38382;&#39064;&#65292;&#20854;&#24212;&#29992;&#20110;&#24179;&#34913;kd&#26641;&#30340;&#26597;&#35810;&#26102;&#38388;&#22797;&#26434;&#24230;&#20026;$O(\log^2n)$&#12290;</title><link>http://arxiv.org/abs/2211.03674</link><description>&lt;p&gt;
&#23558;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#24230;&#37327;&#21270;&#20197;&#28385;&#36275;&#28857;&#20113;&#20013;&#25152;&#38656;&#30340;&#36317;&#31163;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Metricizing the Euclidean Space towards Desired Distance Relations in Point Clouds. (arXiv:2211.03674v2 [cs.CG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.03674
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22914;&#20309;&#26500;&#36896;&#25299;&#25169;&#24230;&#37327;&#26469;&#35745;&#31639;&#32473;&#23450;&#36317;&#31163;&#20540;&#30340;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#28041;&#21450;&#21040;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#22810;&#20010;&#28857;&#20043;&#38388;&#30340;&#26368;&#36817;&#37051;&#26597;&#35810;&#38382;&#39064;&#65292;&#20854;&#24212;&#29992;&#20110;&#24179;&#34913;kd&#26641;&#30340;&#26597;&#35810;&#26102;&#38388;&#22797;&#26434;&#24230;&#20026;$O(\log^2n)$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;$\mathbb{R}^\ell$&#20869;&#30340;&#19968;&#32452;&#28857;&#65292;&#23427;&#20204;&#20043;&#38388;&#30340;&#36317;&#31163;&#30001;&#23427;&#20204;&#30340;&#31354;&#38388;&#20301;&#32622;&#21644;&#36171;&#20104;$\mathbb{R}^\ell$&#30340;&#24230;&#37327;$d$&#20915;&#23450;&#12290;&#22240;&#27492;&#65292;&#20004;&#20010;&#28857;&#20043;&#38388;&#30340;&#36317;&#31163;$d(\mathbf x, \mathbf y)$&#30001;$\mathbf x$&#21644;$\mathbf y$&#30340;&#36873;&#25321;&#21644;&#24230;&#37327;$d$&#30340;&#36873;&#25321;&#22266;&#23450;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#30456;&#20851;&#38382;&#39064;&#65292;&#21363;&#22266;&#23450;&#20540;$\delta$&#21644;&#28857;$\mathbf x,\mathbf y$&#65292;&#24182;&#35810;&#38382;&#26159;&#21542;&#23384;&#22312;&#19968;&#20010;&#25299;&#25169;&#24230;&#37327;$d$&#26469;&#35745;&#31639;&#25152;&#38656;&#30340;&#36317;&#31163;$\delta$&#12290;&#25105;&#20204;&#36890;&#36807;&#26500;&#24314;&#24230;&#37327;&#35299;&#20915;&#20102;&#27492;&#38382;&#39064;&#65292;&#20174;&#32780;&#21516;&#26102;&#32473;&#20986;$\mathbb{R}^\ell$&#20869;&#22810;&#36798;$O(\sqrt\ell)$&#20010;&#28857;&#20043;&#38388;&#25152;&#38656;&#30340;&#19968;&#23545;&#19968;&#36317;&#31163;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;$\varepsilon$-&#21322;&#24230;&#37327;$\tilde{d}$&#30340;&#27010;&#24565;&#65292;&#20197;&#21046;&#23450;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#65306;&#23545;&#20110;&#25152;&#26377;$\varepsilon&gt;0$&#65292;&#23545;&#20110;&#25152;&#26377;$m\geq1$&#65292;&#23545;&#20110;&#20219;&#24847;&#36873;&#25321;&#30340;$m$&#20010;&#28857;$\mathbf y_1,\ldots,\mathbf y_m\in\mathbb{R}^\ell$&#21644;&#25152;&#26377;&#24050;&#36873;&#20540;&#38598;&#21512;$\{\delta_{ij}\geq 0: 1\leq i&lt;j\leq m\}$&#65292;&#37117;&#23384;&#22312;&#19968;&#20010;$\varepsilon$-&#21322;&#24230;&#37327;$\tilde{d}$&#65292;&#20351;&#24471;$\delta_{ij}$&#36817;&#20284;&#31561;&#20110;$\tilde{d}(\mathbf y_i,\mathbf y_j)$&#65292;&#20854;&#20013;$1\leq i&lt;j\leq m$&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26500;&#36896;&#24212;&#29992;&#20110;&#28857;&#20113;&#30340;&#26368;&#36817;&#37051;&#25628;&#32034;&#38382;&#39064;&#65292;&#24182;&#22312;&#20219;&#24847;&#32500;&#24230;$\ell$&#19978;&#35777;&#26126;&#20102;&#24179;&#34913;kd&#26641;&#30340;&#26368;&#21155;&#26597;&#35810;&#26102;&#38388;&#20026;$O(\log^2n)$&#65292;&#20854;&#20013;$n$&#26159;&#28857;&#38598;&#20013;&#30340;&#28857;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a set of points in the Euclidean space $\mathbb{R}^\ell$ with $\ell&gt;1$, the pairwise distances between the points are determined by their spatial location and the metric $d$ that we endow $\mathbb{R}^\ell$ with. Hence, the distance $d(\mathbf x,\mathbf y)=\delta$ between two points is fixed by the choice of $\mathbf x$ and $\mathbf y$ and $d$. We study the related problem of fixing the value $\delta$, and the points $\mathbf x,\mathbf y$, and ask if there is a topological metric $d$ that computes the desired distance $\delta$. We demonstrate this problem to be solvable by constructing a metric to simultaneously give desired pairwise distances between up to $O(\sqrt\ell)$ many points in $\mathbb{R}^\ell$. We then introduce the notion of an $\varepsilon$-semimetric $\tilde{d}$ to formulate our main result: for all $\varepsilon&gt;0$, for all $m\geq 1$, for any choice of $m$ points $\mathbf y_1,\ldots,\mathbf y_m\in\mathbb{R}^\ell$, and all chosen sets of values $\{\delta_{ij}\geq 0: 1
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#20256;&#32479;&#30340;&#24425;&#31080;&#20551;&#35828;&#65288;LTH&#65289;&#25193;&#23637;&#21040;&#30001;&#22270;&#20687;&#34917;&#19969;&#32452;&#25104;&#30340;&#36755;&#20837;&#25968;&#25454;&#20013;&#65292;&#35777;&#26126;&#23384;&#22312;&#19968;&#20010;&#23376;&#38598;&#30340;&#36755;&#20837;&#22270;&#20687;&#34917;&#19969;&#20351;&#24471;&#21487;&#20197;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#35270;&#35273;Transformer&#65288;ViT&#65289;&#65292;&#24182;&#19988;&#36798;&#21040;&#19982;&#20351;&#29992;&#25152;&#26377;&#22270;&#20687;&#34917;&#19969;&#35757;&#32451;&#30340;ViTs&#30456;&#20284;&#30340;&#31934;&#24230;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#21508;&#31181;&#35270;&#35273;&#20219;&#21153;&#20013;&#37117;&#26159;&#21487;&#34892;&#30340;&#21644;&#26377;&#25928;&#30340;&#12290;</title><link>http://arxiv.org/abs/2211.01484</link><description>&lt;p&gt;
&#38754;&#21521;&#35270;&#35273;Transformer&#30340;&#25968;&#25454;&#32423;&#24425;&#31080;&#20551;&#35774;
&lt;/p&gt;
&lt;p&gt;
Data Level Lottery Ticket Hypothesis for Vision Transformers. (arXiv:2211.01484v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01484
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#20256;&#32479;&#30340;&#24425;&#31080;&#20551;&#35828;&#65288;LTH&#65289;&#25193;&#23637;&#21040;&#30001;&#22270;&#20687;&#34917;&#19969;&#32452;&#25104;&#30340;&#36755;&#20837;&#25968;&#25454;&#20013;&#65292;&#35777;&#26126;&#23384;&#22312;&#19968;&#20010;&#23376;&#38598;&#30340;&#36755;&#20837;&#22270;&#20687;&#34917;&#19969;&#20351;&#24471;&#21487;&#20197;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#35270;&#35273;Transformer&#65288;ViT&#65289;&#65292;&#24182;&#19988;&#36798;&#21040;&#19982;&#20351;&#29992;&#25152;&#26377;&#22270;&#20687;&#34917;&#19969;&#35757;&#32451;&#30340;ViTs&#30456;&#20284;&#30340;&#31934;&#24230;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#21508;&#31181;&#35270;&#35273;&#20219;&#21153;&#20013;&#37117;&#26159;&#21487;&#34892;&#30340;&#21644;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#24425;&#31080;&#20551;&#35828;&#65288;LTH&#65289;&#22768;&#31216;&#22312;&#23494;&#38598;&#31070;&#32463;&#32593;&#32476;&#20013;&#23384;&#22312;&#30528;&#19968;&#20010;&#31232;&#30095;&#30340;&#23376;&#32593;&#32476;&#21644;&#19968;&#20010;&#31216;&#20026;&#8220;&#33719;&#22870;&#24425;&#31080;&#8221;&#30340;&#36866;&#24403;&#38543;&#26426;&#21021;&#22987;&#21270;&#26041;&#27861;&#65292;&#20197;&#20415;&#21487;&#20197;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#23427;&#65292;&#20351;&#20854;&#20960;&#20046;&#20687;&#23494;&#38598;&#32593;&#32476;&#19968;&#26679;&#22909;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#23545;&#20110;&#35270;&#35273;Transformer&#65288;ViTs&#65289;&#20013;LTH&#30340;&#30740;&#31350;&#21364;&#24456;&#23569;&#34987;&#35780;&#20272;&#12290;&#26412;&#25991;&#39318;&#20808;&#34920;&#26126;&#20102;&#22312;&#29616;&#26377;&#26041;&#27861;&#19979;&#65292;&#22312;ViTs&#30340;&#26435;&#37325;&#32423;&#21035;&#19978;&#23547;&#25214;&#20256;&#32479;&#30340;&#33719;&#22870;&#24425;&#31080;&#26159;&#22256;&#38590;&#30340;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;ViTs&#30340;LTH&#25512;&#24191;&#21040;&#30001;&#22270;&#20687;&#34917;&#19969;&#32452;&#25104;&#30340;&#36755;&#20837;&#25968;&#25454;&#20013;&#65292;&#21463;ViTs&#36755;&#20837;&#20381;&#36182;&#21551;&#21457;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#23384;&#22312;&#19968;&#20010;&#36755;&#20837;&#22270;&#20687;&#34917;&#19969;&#30340;&#23376;&#38598;&#65292;&#20351;&#24471;&#36890;&#36807;&#20165;&#20351;&#29992;&#35813;&#23376;&#38598;&#65292;&#21487;&#20197;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;ViT&#65292;&#24182;&#36798;&#21040;&#19982;&#20351;&#29992;&#25152;&#26377;&#22270;&#20687;&#34917;&#19969;&#35757;&#32451;&#30340;ViTs&#30456;&#20284;&#30340;&#31934;&#24230;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#36755;&#20837;&#34917;&#19969;&#23376;&#38598;&#31216;&#20026;em&#33719;&#22870;&#24425;&#31080;&#65292;&#23427;&#20195;&#34920;&#20102;&#36755;&#20837;&#25968;&#25454;&#20013;&#30340;&#22823;&#37327;&#20449;&#24687;&#12290;&#25105;&#20204;&#20351;&#29992;&#31080;&#36873;&#25321;&#22120;&#29983;&#25104;&#24102;&#26377;em&#33719;&#22870;&#24425;&#31080;&#30340;&#26435;&#37325;&#23376;&#38598;&#65292;&#24182;&#20351;&#29992;EMD&#35757;&#32451;&#36825;&#20123;&#8220;&#33719;&#22870;&#8221;&#23376;&#38598;&#65292;&#21487;&#20197;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#21508;&#31181;&#35270;&#35273;&#20219;&#21153;&#20013;&#37117;&#26159;&#21487;&#34892;&#30340;&#21644;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The conventional lottery ticket hypothesis (LTH) claims that there exists a sparse subnetwork within a dense neural network and a proper random initialization method called the winning ticket, such that it can be trained from scratch to almost as good as the dense counterpart. Meanwhile, the research of LTH in vision transformers (ViTs) is scarcely evaluated. In this paper, we first show that the conventional winning ticket is hard to find at the weight level of ViTs by existing methods. Then, we generalize the LTH for ViTs to input data consisting of image patches inspired by the input dependence of ViTs. That is, there exists a subset of input image patches such that a ViT can be trained from scratch by using only this subset of patches and achieve similar accuracy to the ViTs trained by using all image patches. We call this subset of input patches the em winning tickets, which represent a significant amount of information in the input data. We use a ticket selector to generate the w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;Masked Autoencoders&#26041;&#27861;&#65292;&#21487;&#20197;&#31934;&#30830;&#37325;&#26500;&#34987;&#36861;&#36394;&#38169;&#35823;&#30340;&#21475;&#33108;&#23398;&#35760;&#24405;&#65292;&#26377;&#25928;&#22320;&#24212;&#29992;&#20110;XRMB&#25968;&#25454;&#38598;&#30740;&#31350;&#20013;&#12290;</title><link>http://arxiv.org/abs/2210.15195</link><description>&lt;p&gt;
&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#26159;&#21475;&#33108;&#23398;&#20064;&#30340;&#21033;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Masked Autoencoders Are Articulatory Learners. (arXiv:2210.15195v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15195
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;Masked Autoencoders&#26041;&#27861;&#65292;&#21487;&#20197;&#31934;&#30830;&#37325;&#26500;&#34987;&#36861;&#36394;&#38169;&#35823;&#30340;&#21475;&#33108;&#23398;&#35760;&#24405;&#65292;&#26377;&#25928;&#22320;&#24212;&#29992;&#20110;XRMB&#25968;&#25454;&#38598;&#30740;&#31350;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21475;&#33108;&#23398;&#35760;&#24405;&#19979;&#19981;&#21516;&#21475;&#33108;&#37096;&#20301;&#30340;&#20301;&#32622;&#21644;&#36816;&#21160;&#65292;&#34987;&#24191;&#27867;&#29992;&#20110;&#30740;&#31350;&#35821;&#38899;&#20135;&#29983;&#20197;&#21450;&#24320;&#21457;&#22522;&#20110;&#21475;&#33108;&#23398;&#30340;&#35821;&#38899;&#21512;&#25104;&#22120;&#21644;&#35821;&#38899;&#21453;&#28436;&#31995;&#32479;&#12290;&#23041;&#26031;&#24247;&#26143;&#22823;&#23398;X&#23556;&#32447;&#24494;&#26463;&#65288;XRMB&#65289;&#25968;&#25454;&#38598;&#26159;&#25552;&#20379;&#19982;&#38899;&#39057;&#35760;&#24405;&#21516;&#27493;&#30340;&#21508;&#31181;&#25968;&#25454;&#38598;&#20043;&#19968;&#12290; XRMB&#21475;&#33108;&#23398;&#35760;&#24405;&#20351;&#29992;&#25918;&#32622;&#22312;&#22810;&#20010;&#21475;&#33108;&#37096;&#20301;&#30340;&#39063;&#31890;&#65292;&#21487;&#20197;&#30001;&#24494;&#26463;&#36319;&#36394;&#12290;&#28982;&#32780;&#65292;&#24456;&#22823;&#19968;&#37096;&#20998;&#21475;&#33108;&#23398;&#35760;&#24405;&#34987;&#36319;&#36394;&#38169;&#35823;&#65292;&#19968;&#30452;&#26080;&#27861;&#20351;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110; Masked Autoencoders &#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#20934;&#30830;&#37325;&#26500; XRMB&#25968;&#25454;&#38598;&#30340;47&#20010;&#28436;&#35762;&#32773;&#20013;41&#20010;&#30340;&#34987;&#36861;&#36394;&#38169;&#35823;&#30340;&#21475;&#33108;&#23398;&#35760;&#24405;&#12290;&#24403;&#20843;&#20010;&#21475;&#33108;&#37096;&#20301;&#20013;&#30340;&#19977;&#20010;&#34987;&#35823;&#36861;&#36394;&#26102;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#37325;&#26500;&#21475;&#33108;&#23398;&#36712;&#36857;&#65292;&#19982;&#30495;&#23454;&#24773;&#20917;&#38750;&#24120;&#25509;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;
Articulatory recordings track the positions and motion of different articulators along the vocal tract and are widely used to study speech production and to develop speech technologies such as articulatory based speech synthesizers and speech inversion systems. The University of Wisconsin X-Ray microbeam (XRMB) dataset is one of various datasets that provide articulatory recordings synced with audio recordings. The XRMB articulatory recordings employ pellets placed on a number of articulators which can be tracked by the microbeam. However, a significant portion of the articulatory recordings are mistracked, and have been so far unsuable. In this work, we present a deep learning based approach using Masked Autoencoders to accurately reconstruct the mistracked articulatory recordings for 41 out of 47 speakers of the XRMB dataset. Our model is able to reconstruct articulatory trajectories that closely match ground truth, even when three out of eight articulators are mistracked, and retrie
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;StaPLR&#31639;&#27861;&#30340;&#26032;&#30340;&#22810;&#35270;&#35282;&#25968;&#25454;&#25554;&#34917;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#38477;&#32500;&#31354;&#38388;&#20013;&#25191;&#34892;&#25554;&#34917;&#20197;&#35299;&#20915;&#35745;&#31639;&#25361;&#25112;&#65292;&#24182;&#22312;&#27169;&#25311;&#25968;&#25454;&#38598;&#20013;&#24471;&#21040;&#20102;&#31454;&#20105;&#24615;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.14484</link><description>&lt;p&gt;
&#22810;&#35270;&#35282;&#25968;&#25454;&#20013;&#32570;&#22833;&#20540;&#30340;&#25554;&#34917;&#38382;&#39064;&#35299;&#20915;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Imputation of missing values in multi-view data. (arXiv:2210.14484v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14484
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;StaPLR&#31639;&#27861;&#30340;&#26032;&#30340;&#22810;&#35270;&#35282;&#25968;&#25454;&#25554;&#34917;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#38477;&#32500;&#31354;&#38388;&#20013;&#25191;&#34892;&#25554;&#34917;&#20197;&#35299;&#20915;&#35745;&#31639;&#25361;&#25112;&#65292;&#24182;&#22312;&#27169;&#25311;&#25968;&#25454;&#38598;&#20013;&#24471;&#21040;&#20102;&#31454;&#20105;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35270;&#35282;&#25968;&#25454;&#26159;&#25351;&#30001;&#22810;&#20010;&#19981;&#21516;&#29305;&#24449;&#38598;&#25551;&#36848;&#30340;&#25968;&#25454;&#12290;&#22312;&#22788;&#29702;&#22810;&#35270;&#35282;&#25968;&#25454;&#26102;&#65292;&#33509;&#20986;&#29616;&#32570;&#22833;&#20540;&#65292;&#21017;&#19968;&#20010;&#35270;&#35282;&#20013;&#30340;&#25152;&#26377;&#29305;&#24449;&#26497;&#26377;&#21487;&#33021;&#21516;&#26102;&#32570;&#22833;&#65292;&#22240;&#32780;&#23548;&#33268;&#38750;&#24120;&#22823;&#37327;&#30340;&#32570;&#22833;&#25968;&#25454;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#35270;&#35282;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#25554;&#34917;&#26041;&#27861;&#65292;&#23427;&#22522;&#20110;&#22534;&#21472;&#24809;&#32602;&#36923;&#36753;&#22238;&#24402;(StaPLR)&#31639;&#27861;&#65292;&#22312;&#38477;&#32500;&#31354;&#38388;&#20013;&#25191;&#34892;&#25554;&#34917;&#65292;&#20197;&#35299;&#20915;&#22266;&#26377;&#30340;&#22810;&#35270;&#35282;&#35745;&#31639;&#25361;&#25112;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#27169;&#25311;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#31454;&#20105;&#24615;&#32467;&#26524;&#65292;&#32780;&#19988;&#20855;&#26377;&#26356;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#20174;&#32780;&#21487;&#20197;&#20351;&#29992;&#20808;&#36827;&#30340;&#25554;&#34917;&#31639;&#27861;&#65292;&#20363;&#22914;missForest&#12290;
&lt;/p&gt;
&lt;p&gt;
Data for which a set of objects is described by multiple distinct feature sets (called views) is known as multi-view data. When missing values occur in multi-view data, all features in a view are likely to be missing simultaneously. This leads to very large quantities of missing data which, especially when combined with high-dimensionality, makes the application of conditional imputation methods computationally infeasible. We introduce a new imputation method based on the existing stacked penalized logistic regression (StaPLR) algorithm for multi-view learning. It performs imputation in a dimension-reduced space to address computational challenges inherent to the multi-view context. We compare the performance of the new imputation method with several existing imputation algorithms in simulated data sets. The results show that the new imputation method leads to competitive results at a much lower computational cost, and makes the use of advanced imputation algorithms such as missForest 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#30693;&#35782;&#22686;&#24378;&#20851;&#31995;&#25277;&#21462;&#25968;&#25454;&#38598;&#65288;KERED&#65289;&#65292;&#23427;&#20026;&#27599;&#20010;&#21477;&#23376;&#27880;&#37322;&#19968;&#20010;&#20851;&#31995;&#20107;&#23454;&#65292;&#24182;&#36890;&#36807;&#23454;&#20307;&#38142;&#25509;&#25552;&#20379;&#23454;&#20307;&#30340;&#30693;&#35782;&#32972;&#26223;&#12290;&#21033;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#65292;&#20316;&#32773;&#27604;&#36739;&#20102;&#24403;&#20195;&#20851;&#31995;&#25277;&#21462;&#26041;&#27861;&#22312;&#21477;&#23376;&#32423;&#21644;&#21253;&#32423;&#20004;&#31181;&#20219;&#21153;&#35774;&#32622;&#19979;&#30340;&#34920;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;KERED&#25552;&#20379;&#30340;&#30693;&#35782;&#22270;&#35889;&#21487;&#20197;&#25903;&#25345;&#30693;&#35782;&#22686;&#24378;&#30340;&#20851;&#31995;&#25277;&#21462;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2210.11231</link><description>&lt;p&gt;
&#30693;&#35782;&#22686;&#24378;&#20851;&#31995;&#25277;&#21462;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Knowledge-Enhanced Relation Extraction Dataset. (arXiv:2210.11231v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.11231
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#30693;&#35782;&#22686;&#24378;&#20851;&#31995;&#25277;&#21462;&#25968;&#25454;&#38598;&#65288;KERED&#65289;&#65292;&#23427;&#20026;&#27599;&#20010;&#21477;&#23376;&#27880;&#37322;&#19968;&#20010;&#20851;&#31995;&#20107;&#23454;&#65292;&#24182;&#36890;&#36807;&#23454;&#20307;&#38142;&#25509;&#25552;&#20379;&#23454;&#20307;&#30340;&#30693;&#35782;&#32972;&#26223;&#12290;&#21033;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#65292;&#20316;&#32773;&#27604;&#36739;&#20102;&#24403;&#20195;&#20851;&#31995;&#25277;&#21462;&#26041;&#27861;&#22312;&#21477;&#23376;&#32423;&#21644;&#21253;&#32423;&#20004;&#31181;&#20219;&#21153;&#35774;&#32622;&#19979;&#30340;&#34920;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;KERED&#25552;&#20379;&#30340;&#30693;&#35782;&#22270;&#35889;&#21487;&#20197;&#25903;&#25345;&#30693;&#35782;&#22686;&#24378;&#30340;&#20851;&#31995;&#25277;&#21462;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#21033;&#29992;&#36741;&#21161;&#30693;&#35782;&#22270;&#35889;&#30340;&#30693;&#35782;&#22686;&#24378;&#26041;&#27861;&#22312;&#20851;&#31995;&#25277;&#21462;&#26041;&#38754;&#24050;&#32463;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#26041;&#27861;&#12290;&#20294;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#21253;&#21547;&#35777;&#25454;&#21477;&#23376;&#21644;&#30693;&#35782;&#22270;&#35889;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#21487;&#29992;&#20110;&#30693;&#35782;&#22686;&#24378;&#20851;&#31995;&#25277;&#21462;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#30693;&#35782;&#22686;&#24378;&#20851;&#31995;&#25277;&#21462;&#25968;&#25454;&#38598;&#65288;KERED&#65289;&#12290;KERED&#20026;&#27599;&#20010;&#21477;&#23376;&#27880;&#37322;&#19968;&#20010;&#20851;&#31995;&#20107;&#23454;&#65292;&#24182;&#36890;&#36807;&#23454;&#20307;&#38142;&#25509;&#25552;&#20379;&#23454;&#20307;&#30340;&#30693;&#35782;&#32972;&#26223;&#12290;&#21033;&#29992;&#25105;&#20204;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#22312;&#20004;&#31181;&#26222;&#36941;&#30340;&#20219;&#21153;&#35774;&#32622;&#19979;&#27604;&#36739;&#20102;&#24403;&#20195;&#20851;&#31995;&#25277;&#21462;&#26041;&#27861;&#65306;&#21477;&#23376;&#32423;&#21644;&#21253;&#32423;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;KERED&#25552;&#20379;&#30340;&#30693;&#35782;&#22270;&#35889;&#21487;&#20197;&#25903;&#25345;&#30693;&#35782;&#22686;&#24378;&#30340;&#20851;&#31995;&#25277;&#21462;&#26041;&#27861;&#12290;&#25105;&#20204;&#30456;&#20449;&#65292;KERED&#20026;&#35780;&#20272;&#30693;&#35782;&#22686;&#24378;&#20851;&#31995;&#25277;&#21462;&#24615;&#33021;&#25552;&#20379;&#20102;&#39640;&#36136;&#37327;&#30340;&#20851;&#31995;&#25277;&#21462;&#25968;&#25454;&#38598;&#21644;&#30456;&#24212;&#30340;&#30693;&#35782;&#22270;&#35889;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, knowledge-enhanced methods leveraging auxiliary knowledge graphs have emerged in relation extraction, surpassing traditional text-based approaches. However, to our best knowledge, there is currently no public dataset available that encompasses both evidence sentences and knowledge graphs for knowledge-enhanced relation extraction. To address this gap, we introduce the Knowledge-Enhanced Relation Extraction Dataset (KERED). KERED annotates each sentence with a relational fact, and it provides knowledge context for entities through entity linking. Using our curated dataset, We compared contemporary relation extraction methods under two prevalent task settings: sentence-level and bag-level. The experimental result shows the knowledge graphs provided by KERED can support knowledge-enhanced relation extraction methods. We believe that KERED offers high-quality relation extraction datasets with corresponding knowledge graphs for evaluating the performance of knowledge-enhanced rela
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#32467;&#21512;&#27969;&#24418;&#23398;&#20064;&#21644;&#39640;&#26031;&#36807;&#31243;&#65292;&#21033;&#29992;&#27969;&#24418;&#26679;&#26412;&#28857;&#20113;&#23450;&#20041;&#22270;&#39640;&#26031;&#36807;&#31243;&#20195;&#29702;&#27169;&#22411;&#65292;&#36890;&#36807;&#36873;&#25321;&#26597;&#35810;&#28857;&#22312;&#27969;&#24418;&#19978;&#20248;&#21270;&#30446;&#26631;&#20989;&#25968;&#65292;&#20855;&#26377;&#33391;&#22909;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.10962</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#39640;&#26031;&#36807;&#31243;&#30340;&#27969;&#24418;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Optimization on Manifolds via Graph Gaussian Processes. (arXiv:2210.10962v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10962
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#32467;&#21512;&#27969;&#24418;&#23398;&#20064;&#21644;&#39640;&#26031;&#36807;&#31243;&#65292;&#21033;&#29992;&#27969;&#24418;&#26679;&#26412;&#28857;&#20113;&#23450;&#20041;&#22270;&#39640;&#26031;&#36807;&#31243;&#20195;&#29702;&#27169;&#22411;&#65292;&#36890;&#36807;&#36873;&#25321;&#26597;&#35810;&#28857;&#22312;&#27969;&#24418;&#19978;&#20248;&#21270;&#30446;&#26631;&#20989;&#25968;&#65292;&#20855;&#26377;&#33391;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#27969;&#24418;&#23398;&#20064;&#25216;&#26415;&#19982;&#39640;&#26031;&#36807;&#31243;&#19978;&#38480;&#32622;&#20449;&#24230;&#31639;&#27861;&#30456;&#32467;&#21512;&#65292;&#20197;&#20248;&#21270;&#27969;&#24418;&#19978;&#30340;&#30446;&#26631;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#38024;&#23545;&#22312;&#26080;&#27861;&#33719;&#24471;&#23436;&#25972;&#27969;&#24418;&#34920;&#31034;&#19988;&#26597;&#35810;&#30446;&#26631;&#26114;&#36149;&#30340;&#24212;&#29992;&#22330;&#26223;&#32780;&#35774;&#35745;&#30340;&#12290;&#25105;&#20204;&#20381;&#38752;&#27969;&#24418;&#26679;&#26412;&#28857;&#20113;&#26469;&#23450;&#20041;&#29992;&#20110;&#30446;&#26631;&#20989;&#25968;&#30340;&#22270;&#39640;&#26031;&#36807;&#31243;&#20195;&#29702;&#27169;&#22411;&#12290;&#20351;&#29992;&#20808;&#21069;&#25152;&#26377;&#26597;&#35810;&#30340;&#21518;&#39564;&#20998;&#24067;&#36880;&#27493;&#36873;&#25321;&#26597;&#35810;&#28857;&#12290;&#25105;&#20204;&#22312;&#26597;&#35810;&#27425;&#25968;&#21644;&#28857;&#20113;&#22823;&#23567;&#26041;&#38754;&#24314;&#31435;&#20102;&#36951;&#25022;&#19978;&#38480;&#12290;&#25968;&#20540;&#23454;&#39564;&#34917;&#20805;&#20102;&#29702;&#35770;&#65292;&#24182;&#35828;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper integrates manifold learning techniques within a \emph{Gaussian process upper confidence bound} algorithm to optimize an objective function on a manifold. Our approach is motivated by applications where a full representation of the manifold is not available and querying the objective is expensive. We rely on a point cloud of manifold samples to define a graph Gaussian process surrogate model for the objective. Query points are sequentially chosen using the posterior distribution of the surrogate model given all previous queries. We establish regret bounds in terms of the number of queries and the size of the point cloud. Several numerical examples complement the theory and illustrate the performance of our method.
&lt;/p&gt;</description></item><item><title>CODER &#26159;&#19968;&#20010;&#22522;&#20110;&#22270;&#30340;&#20195;&#30721;&#25512;&#33616;&#26694;&#26550;&#65292;&#36890;&#36807;&#24314;&#27169;&#24494;&#35266;&#29992;&#25143;-&#20195;&#30721;&#20132;&#20114;&#21644;&#23439;&#35266;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#65292;&#39044;&#27979;&#24320;&#21457;&#32773;&#26410;&#26469;&#30340;&#36129;&#29486;&#34892;&#20026;&#65292;&#20197;&#32553;&#30701;&#24320;&#21457;&#26102;&#38388;&#24182;&#25552;&#39640;&#24320;&#21457;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2210.08332</link><description>&lt;p&gt;
&#24320;&#28304;&#36719;&#20214;&#24320;&#21457;&#32773;&#30340;&#20195;&#30721;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Code Recommendation for Open Source Software Developers. (arXiv:2210.08332v3 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.08332
&lt;/p&gt;
&lt;p&gt;
CODER &#26159;&#19968;&#20010;&#22522;&#20110;&#22270;&#30340;&#20195;&#30721;&#25512;&#33616;&#26694;&#26550;&#65292;&#36890;&#36807;&#24314;&#27169;&#24494;&#35266;&#29992;&#25143;-&#20195;&#30721;&#20132;&#20114;&#21644;&#23439;&#35266;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#65292;&#39044;&#27979;&#24320;&#21457;&#32773;&#26410;&#26469;&#30340;&#36129;&#29486;&#34892;&#20026;&#65292;&#20197;&#32553;&#30701;&#24320;&#21457;&#26102;&#38388;&#24182;&#25552;&#39640;&#24320;&#21457;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#28304;&#36719;&#20214;&#26159;&#25216;&#26415;&#22522;&#30784;&#35774;&#26045;&#30340;&#25903;&#26609;&#65292;&#21560;&#24341;&#25968;&#30334;&#19975;&#20154;&#25165;&#20570;&#20986;&#36129;&#29486;&#12290;&#32771;&#34385;&#21040;&#24320;&#21457;&#20154;&#21592;&#30340;&#20852;&#36259;&#21644;&#39033;&#30446;&#20195;&#30721;&#30340;&#35821;&#20041;&#29305;&#24449;&#65292;&#21521;&#24320;&#28304;&#36719;&#20214;&#24320;&#21457;&#32773;&#25512;&#33616;&#36866;&#24403;&#30340;&#24320;&#21457;&#20219;&#21153;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#37325;&#35201;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20195;&#30721;&#25512;&#33616;&#38382;&#39064;&#65292;&#20854;&#30446;&#30340;&#26159;&#26681;&#25454;&#24320;&#21457;&#32773;&#30340;&#20132;&#20114;&#21382;&#21490;&#12289;&#28304;&#20195;&#30721;&#30340;&#35821;&#20041;&#29305;&#24449;&#21644;&#39033;&#30446;&#30340;&#20998;&#23618;&#25991;&#20214;&#32467;&#26500;&#26469;&#39044;&#27979;&#24320;&#21457;&#32773;&#26410;&#26469;&#30340;&#36129;&#29486;&#34892;&#20026;&#12290;&#32771;&#34385;&#21040;&#31995;&#32479;&#20013;&#22810;&#26041;&#20043;&#38388;&#30340;&#22797;&#26434;&#20132;&#20114;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CODER&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#22270;&#30340;&#24320;&#28304;&#36719;&#20214;&#24320;&#21457;&#32773;&#20195;&#30721;&#25512;&#33616;&#26694;&#26550;&#12290;CODER&#36890;&#36807;&#24322;&#26500;&#22270;&#20849;&#21516;&#24314;&#27169;&#24494;&#35266;&#29992;&#25143;-&#20195;&#30721;&#20132;&#20114;&#21644;&#23439;&#35266;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#65292;&#24182;&#36890;&#36807;&#22312;&#21453;&#26144;&#25991;&#20214;&#32467;&#26500;&#30340;&#25991;&#20214;&#32467;&#26500;&#22270;&#19978;&#30340;&#32858;&#21512;&#36827;&#19968;&#27493;&#36830;&#25509;&#20004;&#20010;&#32423;&#21035;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open Source Software (OSS) is forming the spines of technology infrastructures, attracting millions of talents to contribute. Notably, it is challenging and critical to consider both the developers' interests and the semantic features of the project code to recommend appropriate development tasks to OSS developers. In this paper, we formulate the novel problem of code recommendation, whose purpose is to predict the future contribution behaviors of developers given their interaction history, the semantic features of source code, and the hierarchical file structures of projects. Considering the complex interactions among multiple parties within the system, we propose CODER, a novel graph-based code recommendation framework for open source software developers. CODER jointly models microscopic user-code interactions and macroscopic user-project interactions via a heterogeneous graph and further bridges the two levels of information through aggregation on file-structure graphs that reflect 
&lt;/p&gt;</description></item><item><title>&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;&#24207;&#21015;&#20851;&#27880;&#30340;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#65292;&#23427;&#22312;&#27599;&#20010;&#27493;&#39588;&#20351;&#29992;&#27880;&#24847;&#21147;&#26435;&#37325;&#20316;&#20026;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#20195;&#29702;&#65292;&#23454;&#29616;&#20102;&#26368;&#26032;&#30340;&#23454;&#35777;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2209.14881</link><description>&lt;p&gt;
&#29305;&#24449;&#36873;&#25321;&#30340;&#24207;&#21015;&#20851;&#27880;
&lt;/p&gt;
&lt;p&gt;
Sequential Attention for Feature Selection. (arXiv:2209.14881v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14881
&lt;/p&gt;
&lt;p&gt;
&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;&#24207;&#21015;&#20851;&#27880;&#30340;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#65292;&#23427;&#22312;&#27599;&#20010;&#27493;&#39588;&#20351;&#29992;&#27880;&#24847;&#21147;&#26435;&#37325;&#20316;&#20026;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#20195;&#29702;&#65292;&#23454;&#29616;&#20102;&#26368;&#26032;&#30340;&#23454;&#35777;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#36873;&#25321;&#26159;&#20026;&#20102;&#36873;&#25321;&#19968;&#20010;&#23376;&#38598;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#32780;&#36825;&#20010;&#23376;&#38598;&#33021;&#26368;&#22823;&#21270;&#27169;&#22411;&#36136;&#37327;&#65292;&#24182;&#19988;&#35201;&#27714;&#22312;&#39044;&#31639;&#33539;&#22260;&#20869;&#12290;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#65292;&#37319;&#29992;&#30340;&#20256;&#32479;&#26041;&#27861;&#21253;&#25324;&#22522;&#20110;$\ell_1$&#27491;&#21017;&#21270;&#12289;&#27880;&#24847;&#21147;&#21644;&#20854;&#20182;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#36890;&#24120;&#22312;&#19968;&#27425;&#35780;&#20272;&#20013;&#36873;&#25321;&#25972;&#20010;&#29305;&#24449;&#23376;&#38598;&#65292;&#24573;&#30053;&#20102;&#22312;&#36873;&#25321;&#26399;&#38388;&#29305;&#24449;&#30340;&#27531;&#30041;&#20215;&#20540;&#65292;&#21363;&#22312;&#36873;&#25321;&#20854;&#20182;&#29305;&#24449;&#21518;&#32473;&#19982;&#29305;&#24449;&#30340;&#36793;&#38469;&#36129;&#29486;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24207;&#21015;&#20851;&#27880;&#30340;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#65292;&#23427;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#23454;&#29616;&#20102;&#26368;&#26032;&#30340;&#23454;&#35777;&#32467;&#26524;&#12290;&#35813;&#31639;&#27861;&#22522;&#20110;&#19968;&#36941;&#39640;&#25928;&#30340;&#36138;&#24515;&#21069;&#21521;&#36873;&#25321;&#23454;&#29616;&#65292;&#24182;&#22312;&#27599;&#20010;&#27493;&#39588;&#20351;&#29992;&#27880;&#24847;&#21147;&#26435;&#37325;&#20316;&#20026;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#20195;&#29702;&#12290;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#36866;&#29992;&#20110;&#32447;&#24615;&#22238;&#24402;&#30340;&#29702;&#35770;&#24847;&#20041;&#65292;&#35828;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#30456;&#24403;&#20110;&#32463;&#20856;&#30340;&#27491;&#20132;&#21305;&#37197;&#36861;&#36394;&#65288;OMP&#65289;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feature selection is the problem of selecting a subset of features for a machine learning model that maximizes model quality subject to a budget constraint. For neural networks, prior methods, including those based on $\ell_1$ regularization, attention, and other techniques, typically select the entire feature subset in one evaluation round, ignoring the residual value of features during selection, i.e., the marginal contribution of a feature given that other features have already been selected. We propose a feature selection algorithm called Sequential Attention that achieves state-of-the-art empirical results for neural networks. This algorithm is based on an efficient one-pass implementation of greedy forward selection and uses attention weights at each step as a proxy for feature importance. We give theoretical insights into our algorithm for linear regression by showing that an adaptation to this setting is equivalent to the classical Orthogonal Matching Pursuit (OMP) algorithm, a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; Aug-imodels &#26694;&#26550;&#65292;&#21033;&#29992; LLMs &#30340;&#30693;&#35782;&#22312;&#25311;&#21512;&#36807;&#31243;&#20013;&#26500;&#24314;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#19981;&#20351;&#29992; LLMs&#65292;&#20855;&#22791;&#23436;&#20840;&#30340;&#36879;&#26126;&#24615;&#12290;&#30740;&#31350;&#25506;&#35752;&#20102;&#20004;&#31181;&#19981;&#21516;&#26041;&#24335;&#30340;&#23454;&#29616;&#65292;&#24182;&#22312;&#22810;&#31181;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2209.11799</link><description>&lt;p&gt;
&#20351;&#29992;LLM&#26469;&#22686;&#24378;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Augmenting Interpretable Models with LLMs during Training. (arXiv:2209.11799v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.11799
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; Aug-imodels &#26694;&#26550;&#65292;&#21033;&#29992; LLMs &#30340;&#30693;&#35782;&#22312;&#25311;&#21512;&#36807;&#31243;&#20013;&#26500;&#24314;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#19981;&#20351;&#29992; LLMs&#65292;&#20855;&#22791;&#23436;&#20840;&#30340;&#36879;&#26126;&#24615;&#12290;&#30740;&#31350;&#25506;&#35752;&#20102;&#20004;&#31181;&#19981;&#21516;&#26041;&#24335;&#30340;&#23454;&#29616;&#65292;&#24182;&#22312;&#22810;&#31181;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36234;&#26469;&#36234;&#22810;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#36827;&#20837;&#39640;&#39118;&#38505;&#39046;&#22495;&#65288;&#20363;&#22914;&#21307;&#23398;&#65289;&#21644;&#35745;&#31639;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#20013;&#65292;&#23545;&#35299;&#37322;&#24615;&#21644;&#25928;&#29575;&#30340;&#38656;&#27714;&#26085;&#30410;&#22686;&#21152;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22686;&#24378;&#21487;&#35299;&#37322;&#27169;&#22411;&#65288;Aug-imodels&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;LLMs&#25152;&#23398;&#20064;&#30340;&#30693;&#35782;&#24314;&#31435;&#26497;&#20854;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#12290;Aug-imodels&#22312;&#25311;&#21512;&#36807;&#31243;&#20013;&#20351;&#29992;LLMs&#65292;&#20294;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#19981;&#20351;&#29992;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23436;&#20840;&#30340;&#36879;&#26126;&#24615;&#65292;&#24182;&#19988;&#19982;LLMs&#30456;&#27604;&#65292;&#25512;&#29702;&#36895;&#24230;&#21644;&#20869;&#23384;&#24615;&#33021;&#26377;&#20102;&#22823;&#20110;1000&#20493;&#30340;&#25552;&#39640;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#20004;&#31181;Aug-imodels&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#20855;&#20307;&#23454;&#20363;&#65306;&#65288;i&#65289;Aug-GAM&#65292;&#23427;&#20351;&#29992;&#26469;&#33258;LLM&#30340;&#35299;&#32806;&#23884;&#20837;&#26469;&#22686;&#24378;&#24191;&#20041;&#21152;&#24615;&#27169;&#22411;&#65307;&#65288;ii&#65289;Aug-Tree&#65292;&#23427;&#36890;&#36807;LLM&#29305;&#24449;&#25193;&#23637;&#26469;&#22686;&#24378;&#20915;&#31574;&#26641;&#12290;&#22312;&#21508;&#31181;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#20013;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#20248;&#20110;&#20854;&#26410;&#22686;&#24378;&#30340;&#23545;&#29031;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent large language models (LLMs) have demonstrated remarkable prediction performance for a growing array of tasks. However, their proliferation into high-stakes domains (e.g. medicine) and compute-limited settings has created a burgeoning need for interpretability and efficiency. We address this need by proposing Augmented Interpretable Models (Aug-imodels), a framework for leveraging the knowledge learned by LLMs to build extremely efficient and interpretable models. Aug-imodels use LLMs during fitting but not during inference, allowing complete transparency and often a speed/memory improvement of greater than 1,000x for inference compared to LLMs. We explore two instantiations of Aug-imodels in natural-language processing: (i) Aug-GAM, which augments a generalized additive model with decoupled embeddings from an LLM and (ii) Aug-Tree, which augments a decision tree with LLM feature expansions. Across a variety of text-classification datasets, both outperform their non-augmented co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#26448;&#26009;&#24037;&#31243;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#28041;&#21450;&#26448;&#26009;&#21152;&#24037;&#12289;&#32467;&#26500;&#21644;&#26448;&#26009;&#24615;&#33021;&#30740;&#31350;&#12289;&#27979;&#37327;&#26448;&#26009;&#24615;&#33021;&#12289;&#26032;&#26448;&#26009;&#30340;&#21019;&#24314;&#21644;&#35774;&#35745;&#20197;&#21450;&#26410;&#26469;&#26426;&#36935;&#31561;&#26041;&#38754;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#24555;&#12289;&#26356;&#20934;&#30830;&#65292;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#26377;&#21161;&#20110;&#26080;&#26426;&#26448;&#26009;&#30340;&#21270;&#23398;&#25104;&#20998;&#39044;&#27979;&#21644;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2209.11234</link><description>&lt;p&gt;
&#26448;&#26009;&#24037;&#31243;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#65306;&#20154;&#24037;&#26234;&#33021;&#22312;&#26448;&#26009;&#24037;&#31243;&#20013;&#30340;&#24212;&#29992;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence in Material Engineering: A review on applications of AI in Material Engineering. (arXiv:2209.11234v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.11234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#26448;&#26009;&#24037;&#31243;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#28041;&#21450;&#26448;&#26009;&#21152;&#24037;&#12289;&#32467;&#26500;&#21644;&#26448;&#26009;&#24615;&#33021;&#30740;&#31350;&#12289;&#27979;&#37327;&#26448;&#26009;&#24615;&#33021;&#12289;&#26032;&#26448;&#26009;&#30340;&#21019;&#24314;&#21644;&#35774;&#35745;&#20197;&#21450;&#26410;&#26469;&#26426;&#36935;&#31561;&#26041;&#38754;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#24555;&#12289;&#26356;&#20934;&#30830;&#65292;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#26377;&#21161;&#20110;&#26080;&#26426;&#26448;&#26009;&#30340;&#21270;&#23398;&#25104;&#20998;&#39044;&#27979;&#21644;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#20154;&#24037;&#26234;&#33021;&#22312;&#26448;&#26009;&#31185;&#23398;&#21644;&#24037;&#31243;&#20013;&#30340;&#20316;&#29992;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#39640;&#24615;&#33021;&#35745;&#31639;&#26426;&#30340;&#21457;&#23637;&#20351;&#24471;&#27979;&#35797;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25104;&#20026;&#21487;&#33021;&#65292;&#36825;&#25552;&#20379;&#20102;&#19968;&#20010;&#26426;&#20250;&#26469;&#20811;&#26381;&#20256;&#32479;&#35745;&#31639;&#26041;&#27861;&#65288;&#22914;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#65289;&#22312;&#24615;&#33021;&#39044;&#27979;&#19978;&#30340;&#23616;&#38480;&#24615;&#12290;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#27604;&#22522;&#20110;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#30340;&#26041;&#27861;&#26356;&#24555;&#65292;&#26356;&#20934;&#30830;&#12290;&#27492;&#22806;&#65292;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#26377;&#21161;&#20110;&#22312;&#19981;&#20351;&#29992;&#26230;&#20307;&#32467;&#26500;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#26080;&#26426;&#26448;&#26009;&#30340;&#21270;&#23398;&#25104;&#20998;&#12290;&#36825;&#20123;&#21457;&#23637;&#23545;&#26448;&#26009;&#24037;&#31243;&#21644;&#30740;&#31350;&#20135;&#29983;&#20102;&#37325;&#22823;&#24433;&#21709;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#26448;&#26009;&#24037;&#31243;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#27010;&#25324;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#26448;&#26009;&#21152;&#24037;&#12289;&#32467;&#26500;&#21644;&#26448;&#26009;&#24615;&#33021;&#30740;&#31350;&#20197;&#21450;&#27979;&#37327;&#26448;&#26009;&#24615;&#33021;&#31561;&#20851;&#38190;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#12290;&#25551;&#36848;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#26032;&#26448;&#26009;&#30340;&#21019;&#24314;&#21644;&#35774;&#35745;&#20013;&#30340;&#24212;&#29992;&#65292;&#20854;&#20013;&#20154;&#24037;&#26234;&#33021;&#34987;&#29992;&#26469;&#20248;&#21270;&#21644;&#39044;&#27979;&#26448;&#26009;&#24615;&#36136;&#12290;&#26368;&#21518;&#65292;&#26412;&#25991;&#35752;&#35770;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#26448;&#26009;&#24037;&#31243;&#20013;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;
The role of artificial intelligence (AI) in material science and engineering (MSE) is becoming increasingly important as AI technology advances. The development of high-performance computing has made it possible to test deep learning (DL) models with significant parameters, providing an opportunity to overcome the limitation of traditional computational methods, such as density functional theory (DFT), in property prediction. Machine learning (ML)-based methods are faster and more accurate than DFT-based methods. Furthermore, the generative adversarial networks (GANs) have facilitated the generation of chemical compositions of inorganic materials without using crystal structure information. These developments have significantly impacted material engineering (ME) and research. Some of the latest developments in AI in ME herein are reviewed. First, the development of AI in the critical areas of ME, such as in material processing, the study of structure and material property, and measurin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#22522;&#20110;&#29289;&#29702;&#23398;&#30340;&#20223;&#30495;&#25216;&#26415;&#65292;&#33719;&#24471;&#19968;&#31181;&#36890;&#29992;&#30340;&#26368;&#20248;&#25511;&#21046;&#31574;&#30053;&#65292;&#21487;&#20197;&#20351;&#23567;&#22411;&#31354;&#20013;&#26426;&#22120;&#20154;&#20174;&#20219;&#24847;&#29366;&#24577;&#24320;&#22987;&#65292;&#23454;&#29616;&#20498;&#31435;&#38477;&#33853;&#35302;&#21457;&#21644;&#26059;&#36716;&#26426;&#21160;&#25511;&#21046;&#12290;&#36890;&#36807;&#23558;&#25152;&#23398;&#20064;&#30340;&#31574;&#30053;&#36716;&#31227;&#33267;&#23454;&#20307;&#26426;&#22120;&#20154;&#65292;&#25104;&#21151;&#23454;&#29616;&#39640;&#21487;&#38752;&#24615;&#30340;&#20498;&#31435;&#38477;&#33853;&#65292;&#21363;&#20351;&#22312;&#21463;&#21040;&#39118;&#25200;&#21160;&#30340;&#24773;&#20917;&#19979;&#12290;</title><link>http://arxiv.org/abs/2209.11043</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#23567;&#22411;&#31354;&#20013;&#26426;&#22120;&#20154;&#20498;&#31435;&#38477;&#33853;&#35302;&#21457;&#21644;&#26059;&#36716;&#26426;&#21160;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Inverted Landing in a Small Aerial Robot via Deep Reinforcement Learning for Triggering and Control of Rotational Maneuvers. (arXiv:2209.11043v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.11043
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#22522;&#20110;&#29289;&#29702;&#23398;&#30340;&#20223;&#30495;&#25216;&#26415;&#65292;&#33719;&#24471;&#19968;&#31181;&#36890;&#29992;&#30340;&#26368;&#20248;&#25511;&#21046;&#31574;&#30053;&#65292;&#21487;&#20197;&#20351;&#23567;&#22411;&#31354;&#20013;&#26426;&#22120;&#20154;&#20174;&#20219;&#24847;&#29366;&#24577;&#24320;&#22987;&#65292;&#23454;&#29616;&#20498;&#31435;&#38477;&#33853;&#35302;&#21457;&#21644;&#26059;&#36716;&#26426;&#21160;&#25511;&#21046;&#12290;&#36890;&#36807;&#23558;&#25152;&#23398;&#20064;&#30340;&#31574;&#30053;&#36716;&#31227;&#33267;&#23454;&#20307;&#26426;&#22120;&#20154;&#65292;&#25104;&#21151;&#23454;&#29616;&#39640;&#21487;&#38752;&#24615;&#30340;&#20498;&#31435;&#38477;&#33853;&#65292;&#21363;&#20351;&#22312;&#21463;&#21040;&#39118;&#25200;&#21160;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23567;&#22411;&#31354;&#20013;&#26426;&#22120;&#20154;&#36890;&#36807;&#20165;&#20381;&#36182;&#26426;&#36733;&#24863;&#24212;&#21644;&#35745;&#31639;&#65292;&#22312;&#30701;&#26102;&#38388;&#20869;&#23454;&#29616;&#20498;&#31435;&#38477;&#33853;&#26159;&#19968;&#39033;&#23500;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#34649;&#34656;&#12289;&#33485;&#34631;&#21644;&#34588;&#34562;&#31561;&#29983;&#29289;&#39134;&#34892;&#32773;&#32463;&#24120;&#25191;&#34892;&#36825;&#39033;&#39134;&#34892;&#25216;&#24039;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#22522;&#20110;&#29289;&#29702;&#23398;&#30340;&#20223;&#30495;&#65292;&#33719;&#24471;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#26368;&#20248;&#25511;&#21046;&#31574;&#30053;&#65292;&#21487;&#20197;&#20174;&#20219;&#24847;&#30340;&#36215;&#22987;&#29366;&#24577;&#24320;&#22987;&#65292;&#23454;&#29616;&#23567;&#22411;&#31354;&#20013;&#26426;&#22120;&#20154;&#30340;&#20498;&#31435;&#38477;&#33853;&#35302;&#21457;&#21644;&#26059;&#36716;&#26426;&#21160;&#25511;&#21046;&#12290;&#36890;&#36807;&#23558;&#25152;&#23398;&#20064;&#30340;&#31574;&#30053;&#36716;&#31227;&#33267;&#23454;&#20307;&#26426;&#22120;&#20154;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20351;&#26426;&#22120;&#20154;&#20855;&#26377;&#39640;&#21487;&#38752;&#24615;&#22320;&#23436;&#25104;&#20219;&#24847;&#21021;&#24577;&#30340;&#20498;&#31435;&#38477;&#33853;&#65292;&#21363;&#20351;&#22312;&#26126;&#26174;&#30340;&#39118;&#25200;&#21160;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inverted landing in a rapid and robust manner is a challenging feat for aerial robots, especially while depending entirely on onboard sensing and computation. In spite of this, this feat is routinely performed by biological fliers such as bats, flies, and bees. Our previous work has identified a direct causal connection between a series of onboard visual cues and kinematic actions that allow for reliable execution of this challenging aerobatic maneuver in small aerial robots. In this work, we first utilized Deep Reinforcement Learning and a physics-based simulation to obtain a general, optimal control policy for robust inverted landing starting from any arbitrary approach condition. This optimized control policy provides a computationally-efficient mapping from the system's observational space to its motor command action space, including both triggering and control of rotational maneuvers. This was done by training the system over a large range of approach flight velocities that varied
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23545;&#25968;&#25454;&#36827;&#34892;&#21435;&#22122;&#65292;&#20351;&#34928;&#24369;&#30340;&#20449;&#21495;&#20986;&#29616;&#20855;&#26377;&#37327;&#21270;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#65292;&#24182;&#25104;&#21151;&#24212;&#29992;&#20110;&#26230;&#20307;&#26448;&#26009;&#30340;X&#23556;&#32447;&#34893;&#23556;&#25968;&#25454;&#20013;&#12290;</title><link>http://arxiv.org/abs/2209.09247</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21435;&#22122;&#25552;&#21462;&#34928;&#20943;&#20449;&#21495;&#30340;&#24212;&#29992;&#30740;&#31350;&#8212;&#8212;&#20197;X&#23556;&#32447;&#34893;&#23556;&#25968;&#25454;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Weak-signal extraction enabled by deep-neural-network denoising of diffraction data. (arXiv:2209.09247v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.09247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23545;&#25968;&#25454;&#36827;&#34892;&#21435;&#22122;&#65292;&#20351;&#34928;&#24369;&#30340;&#20449;&#21495;&#20986;&#29616;&#20855;&#26377;&#37327;&#21270;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#65292;&#24182;&#25104;&#21151;&#24212;&#29992;&#20110;&#26230;&#20307;&#26448;&#26009;&#30340;X&#23556;&#32447;&#34893;&#23556;&#25968;&#25454;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21435;&#38500;&#22122;&#38899;&#22312;&#25104;&#20687;&#21644;&#22768;&#23398;&#26041;&#38754;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#22312;&#26085;&#24120;&#24212;&#29992;&#20013;&#65292;&#21435;&#22122;&#21487;&#33021;&#29978;&#33267;&#21253;&#21547;&#19982;&#30495;&#23454;&#24773;&#20917;&#19981;&#31526;&#30340;&#29983;&#25104;&#26041;&#38754;&#12290;&#20294;&#26159;&#65292;&#22312;&#31185;&#23398;&#24212;&#29992;&#20013;&#65292;&#21435;&#22122;&#24517;&#39035;&#20934;&#30830;&#22320;&#20877;&#29616;&#30495;&#23454;&#24773;&#20917;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23545;&#25968;&#25454;&#36827;&#34892;&#21435;&#22122;&#65292;&#20197;&#20351;&#34928;&#24369;&#30340;&#20449;&#21495;&#20986;&#29616;&#20855;&#26377;&#37327;&#21270;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26230;&#20307;&#26448;&#26009;&#30340;X&#23556;&#32447;&#34893;&#23556;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#21435;&#22122;&#25968;&#25454;&#20013;&#65292;&#28304;&#33258;&#30005;&#33655;&#25490;&#24207;&#30340;&#24494;&#24369;&#20449;&#21495;&#65292;&#22312;&#22122;&#38899;&#25968;&#25454;&#20013;&#19981;&#26174;&#33879;&#65292;&#20294;&#22312;&#21435;&#22122;&#21518;&#21464;&#24471;&#28165;&#26224;&#32780;&#20934;&#30830;&#21487;&#35265;&#12290;&#36825;&#31181;&#25104;&#21151;&#24471;&#30410;&#20110;&#20351;&#29992;&#25152;&#27979;&#37327;&#30340;&#20302;&#22122;&#22768;&#25968;&#25454;&#21644;&#39640;&#22122;&#22768;&#25968;&#25454;&#30340;&#25104;&#23545;&#36827;&#34892;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#30417;&#30563;&#35757;&#32451;&#12290;&#36825;&#26679;&#65292;&#31070;&#32463;&#32593;&#32476;&#23601;&#21487;&#20197;&#23398;&#20064;&#22122;&#22768;&#30340;&#32479;&#35745;&#29305;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#20351;&#29992;&#20154;&#24037;&#22122;&#22768;&#26080;&#27861;&#24471;&#21040;&#22914;&#27492;&#37327;&#21270;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#38416;&#26126;&#20102;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21435;&#22122;&#25552;&#21462;&#22122;&#38899;&#25968;&#25454;&#20013;&#30340;&#34928;&#20943;&#20449;&#21495;&#30340;&#23454;&#29992;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Removal or cancellation of noise has wide-spread applications for imaging and acoustics. In every-day-life applications, denoising may even include generative aspects which are unfaithful to the ground truth. For scientific applications, however, denoising must reproduce the ground truth accurately. Here, we show how data can be denoised via a deep convolutional neural network such that weak signals appear with quantitative accuracy. In particular, we study X-ray diffraction on crystalline materials. We demonstrate that weak signals stemming from charge ordering, insignificant in the noisy data, become visible and accurate in the denoised data. This success is enabled by supervised training of a deep neural network with pairs of measured low- and high-noise data. This way, the neural network learns about the statistical properties of the noise. We demonstrate that using artificial noise does not yield such quantitatively accurate results. Our approach thus illustrates a practical strat
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#39044;&#27979;&#40065;&#26834;&#24615;&#30340;&#27010;&#24565;&#26469;&#26356;&#31995;&#32479;&#22320;&#35780;&#20272;&#20449;&#21495;&#26102;&#38388;&#36923;&#36753;&#20013;&#24213;&#23618;&#35859;&#35789;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25104;&#21151;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2209.07881</link><description>&lt;p&gt;
&#20449;&#21495;&#26102;&#38388;&#36923;&#36753;&#35859;&#35789;&#30340;&#27169;&#22411;&#39044;&#27979;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Model Predictive Robustness of Signal Temporal Logic Predicates. (arXiv:2209.07881v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.07881
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#39044;&#27979;&#40065;&#26834;&#24615;&#30340;&#27010;&#24565;&#26469;&#26356;&#31995;&#32479;&#22320;&#35780;&#20272;&#20449;&#21495;&#26102;&#38388;&#36923;&#36753;&#20013;&#24213;&#23618;&#35859;&#35789;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25104;&#21151;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#21495;&#26102;&#38388;&#36923;&#36753;&#30340;&#40065;&#26834;&#24615;&#19981;&#20165;&#35780;&#20272;&#20102;&#19968;&#20010;&#20449;&#21495;&#26159;&#21542;&#31526;&#21512;&#35268;&#33539;&#65292;&#32780;&#19988;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#34913;&#37327;&#20844;&#24335;&#34987;&#28385;&#36275;&#25110;&#36829;&#21453;&#30340;&#31243;&#24230;&#30340;&#25351;&#26631;&#12290;&#40065;&#26834;&#24615;&#30340;&#35745;&#31639;&#22522;&#20110;&#23545;&#24213;&#23618;&#35859;&#35789;&#30340;&#40065;&#26834;&#24615;&#36827;&#34892;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#35859;&#35789;&#30340;&#40065;&#26834;&#24615;&#36890;&#24120;&#20197;&#19968;&#31181;&#26080;&#27169;&#22411;&#30340;&#26041;&#24335;&#23450;&#20041;&#65292;&#21363;&#19981;&#21253;&#25324;&#31995;&#32479;&#21160;&#24577;&#12290;&#32780;&#19988;&#65292;&#31934;&#30830;&#23450;&#20041;&#22797;&#26434;&#35859;&#35789;&#30340;&#40065;&#26834;&#24615;&#36890;&#24120;&#26159;&#38750;&#24179;&#20961;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#39044;&#27979;&#40065;&#26834;&#24615;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#32771;&#34385;&#22522;&#20110;&#27169;&#22411;&#30340;&#39044;&#27979;&#65292;&#25552;&#20379;&#20102;&#27604;&#20043;&#21069;&#26041;&#27861;&#26356;&#31995;&#32479;&#30340;&#35780;&#20272;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#26469;&#23398;&#20064;&#22522;&#20110;&#39044;&#20808;&#35745;&#31639;&#30340;&#39044;&#27979;&#30340;&#40065;&#26834;&#24615;&#65292;&#20197;&#20415;&#21487;&#20197;&#22312;&#32447;&#39640;&#25928;&#22320;&#35745;&#31639;&#40065;&#26834;&#24615;&#20540;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#22312;&#35760;&#24405;&#30340;&#25968;&#25454;&#19978;&#20351;&#29992;&#22312;&#24418;&#24335;&#21270;&#20132;&#36890;&#35268;&#21017;&#20013;&#20351;&#29992;&#30340;&#35859;&#35789;&#30340;&#33258;&#21160;&#39550;&#39542;&#29992;&#20363;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The robustness of signal temporal logic not only assesses whether a signal adheres to a specification but also provides a measure of how much a formula is fulfilled or violated. The calculation of robustness is based on evaluating the robustness of underlying predicates. However, the robustness of predicates is usually defined in a model-free way, i.e., without including the system dynamics. Moreover, it is often nontrivial to define the robustness of complicated predicates precisely. To address these issues, we propose a notion of model predictive robustness, which provides a more systematic way of evaluating robustness compared to previous approaches by considering model-based predictions. In particular, we use Gaussian process regression to learn the robustness based on precomputed predictions so that robustness values can be efficiently computed online. We evaluate our approach for the use case of autonomous driving with predicates used in formalized traffic rules on a recorded dat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;Fed-FSNet&#30340;&#26032;&#22411;&#32852;&#37030;&#23398;&#20064;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#27169;&#31946;&#21512;&#25104;&#32593;&#32476;&#65288;FSNet&#65289;&#22312;&#28304;&#22836;&#22788;&#32531;&#35299;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#20840;&#23616;&#27169;&#22411;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2208.12044</link><description>&lt;p&gt;
Fed-FSNet: &#36890;&#36807;&#27169;&#31946;&#21512;&#25104;&#32593;&#32476;&#32531;&#35299;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#32852;&#37030;&#23398;&#20064;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Fed-FSNet: Mitigating Non-I.I.D. Federated Learning via Fuzzy Synthesizing Network. (arXiv:2208.12044v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.12044
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;Fed-FSNet&#30340;&#26032;&#22411;&#32852;&#37030;&#23398;&#20064;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#27169;&#31946;&#21512;&#25104;&#32593;&#32476;&#65288;FSNet&#65289;&#22312;&#28304;&#22836;&#22788;&#32531;&#35299;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#20840;&#23616;&#27169;&#22411;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26368;&#36817;&#20986;&#29616;&#30340;&#26377;&#21069;&#36884;&#30340;&#38544;&#31169;&#20445;&#25252;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#26088;&#22312;&#36890;&#36807;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#36827;&#34892;&#20998;&#24067;&#24335;&#35757;&#32451;&#65292;&#23558;&#26412;&#22320;&#27169;&#22411;&#32858;&#21512;&#25104;&#20840;&#23616;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#22312;&#20113;&#26381;&#21153;&#22120;&#19978;&#20849;&#20139;&#20013;&#24515;&#21270;&#21407;&#22987;&#25968;&#25454;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#36793;&#32536;&#35774;&#22791;&#20043;&#38388;&#23384;&#22312;&#30528;&#22823;&#37327;&#26412;&#22320;&#25968;&#25454;&#24322;&#36136;&#24615;&#65288;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#65289;&#65292;&#22240;&#27492;&#32852;&#37030;&#23398;&#20064;&#24456;&#23481;&#26131;&#33719;&#24471;&#19968;&#20010;&#33021;&#22815;&#22312;&#26412;&#22320;&#25968;&#25454;&#38598;&#19978;&#20135;&#29983;&#26356;&#22810;&#20559;&#31227;&#26799;&#24230;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#20174;&#32780;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#29978;&#33267;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26080;&#27861;&#25910;&#25947;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;FL&#35757;&#32451;&#26694;&#26550;&#65292;&#31216;&#20026;Fed-FSNet&#65292;&#23427;&#20351;&#29992;&#19968;&#20010;&#36866;&#24403;&#35774;&#35745;&#30340;&#27169;&#31946;&#21512;&#25104;&#32593;&#32476;&#65288;FSNet&#65289;&#26469;&#32531;&#35299;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;FL&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) has emerged as a promising privacy-preserving distributed machine learning framework recently. It aims at collaboratively learning a shared global model by performing distributed training locally on edge devices and aggregating local models into a global one without centralized raw data sharing in the cloud server. However, due to the large local data heterogeneities (Non-I.I.D. data) across edge devices, the FL may easily obtain a global model that can produce more shifted gradients on local datasets, thereby degrading the model performance or even suffering from the non-convergence during training. In this paper, we propose a novel FL training framework, dubbed Fed-FSNet, using a properly designed Fuzzy Synthesizing Network (FSNet) to mitigate the Non-I.I.D. FL at-the-source. Concretely, we maintain an edge-agnostic hidden model in the cloud server to estimate a less-accurate while direction-aware inversion of the global model. The hidden model can then fuzzil
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20559;&#22909;&#23398;&#20064;&#31163;&#32447;&#23398;&#20064;&#25928;&#29992;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#30495;&#23454;&#19990;&#30028;&#38382;&#39064;&#20013;&#29992;&#19987;&#23478;&#30693;&#35782;&#23450;&#20041;&#25928;&#29992;&#20989;&#25968;&#22256;&#38590;&#19988;&#19982;&#19987;&#23478;&#21453;&#22797;&#20114;&#21160;&#26114;&#36149;&#30340;&#38382;&#39064;&#12290;&#20351;&#29992;&#25928;&#29992;&#20989;&#25968;&#31354;&#38388;&#30340;&#31895;&#30053;&#20449;&#24687;&#65292;&#33021;&#22815;&#22312;&#20351;&#29992;&#24456;&#23569;&#32467;&#26524;&#26102;&#25552;&#39640;&#25928;&#29992;&#20989;&#25968;&#20272;&#35745;&#65292;&#24182;&#36890;&#36807;&#25972;&#20010;&#20248;&#21270;&#38142;&#20013;&#20256;&#36882;&#25928;&#29992;&#20989;&#25968;&#23398;&#20064;&#20219;&#21153;&#20013;&#20986;&#29616;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2208.10300</link><description>&lt;p&gt;
&#22810;&#30446;&#26631;&#21442;&#25968;&#20248;&#21270;&#20013;&#30340;&#26377;&#25928;&#25928;&#29992;&#20989;&#25968;&#23398;&#20064;&#19982;&#20808;&#39564;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Efficient Utility Function Learning for Multi-Objective Parameter Optimization with Prior Knowledge. (arXiv:2208.10300v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.10300
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20559;&#22909;&#23398;&#20064;&#31163;&#32447;&#23398;&#20064;&#25928;&#29992;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#30495;&#23454;&#19990;&#30028;&#38382;&#39064;&#20013;&#29992;&#19987;&#23478;&#30693;&#35782;&#23450;&#20041;&#25928;&#29992;&#20989;&#25968;&#22256;&#38590;&#19988;&#19982;&#19987;&#23478;&#21453;&#22797;&#20114;&#21160;&#26114;&#36149;&#30340;&#38382;&#39064;&#12290;&#20351;&#29992;&#25928;&#29992;&#20989;&#25968;&#31354;&#38388;&#30340;&#31895;&#30053;&#20449;&#24687;&#65292;&#33021;&#22815;&#22312;&#20351;&#29992;&#24456;&#23569;&#32467;&#26524;&#26102;&#25552;&#39640;&#25928;&#29992;&#20989;&#25968;&#20272;&#35745;&#65292;&#24182;&#36890;&#36807;&#25972;&#20010;&#20248;&#21270;&#38142;&#20013;&#20256;&#36882;&#25928;&#29992;&#20989;&#25968;&#23398;&#20064;&#20219;&#21153;&#20013;&#20986;&#29616;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#25216;&#26415;&#36890;&#24120;&#20551;&#23450;&#24050;&#26377;&#25928;&#29992;&#20989;&#25968;&#12289;&#36890;&#36807;&#20114;&#21160;&#23398;&#20064;&#25928;&#29992;&#20989;&#25968;&#25110;&#23581;&#35797;&#30830;&#23450;&#23436;&#25972;&#30340;Pareto&#21069;&#27839;&#26469;&#36827;&#34892;&#12290;&#28982;&#32780;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#38382;&#39064;&#20013;&#65292;&#32467;&#26524;&#24448;&#24448;&#22522;&#20110;&#38544;&#21547;&#21644;&#26174;&#24615;&#30340;&#19987;&#23478;&#30693;&#35782;&#65292;&#38590;&#20197;&#23450;&#20041;&#19968;&#20010;&#25928;&#29992;&#20989;&#25968;&#65292;&#32780;&#20114;&#21160;&#23398;&#20064;&#25110;&#21518;&#32493;&#21551;&#21457;&#24335;&#38656;&#35201;&#21453;&#22797;&#24182;&#19988;&#26114;&#36149;&#22320;&#19987;&#23478;&#21442;&#19982;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#31181;&#24773;&#20917;&#65292;&#25105;&#20204;&#20351;&#29992;&#20559;&#22909;&#23398;&#20064;&#31163;&#32447;&#23398;&#20064;&#25928;&#29992;&#20989;&#25968;&#65292;&#21033;&#29992;&#19987;&#23478;&#30693;&#35782;&#12290;&#19982;&#20854;&#20182;&#24037;&#20316;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#19981;&#20165;&#20351;&#29992;&#65288;&#25104;&#23545;&#30340;&#65289;&#32467;&#26524;&#20559;&#22909;&#65292;&#32780;&#19988;&#20351;&#29992;&#25928;&#29992;&#20989;&#25968;&#31354;&#38388;&#30340;&#31895;&#30053;&#20449;&#24687;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#25552;&#39640;&#25928;&#29992;&#20989;&#25968;&#20272;&#35745;&#65292;&#29305;&#21035;&#26159;&#22312;&#20351;&#29992;&#24456;&#23569;&#30340;&#32467;&#26524;&#26102;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#25928;&#29992;&#20989;&#25968;&#23398;&#20064;&#20219;&#21153;&#20013;&#20986;&#29616;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#23558;&#20854;&#20256;&#36882;&#21040;&#25972;&#20010;&#20248;&#21270;&#38142;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
The current state-of-the-art in multi-objective optimization assumes either a given utility function, learns a utility function interactively or tries to determine the complete Pareto front, requiring a post elicitation of the preferred result. However, result elicitation in real world problems is often based on implicit and explicit expert knowledge, making it difficult to define a utility function, whereas interactive learning or post elicitation requires repeated and expensive expert involvement. To mitigate this, we learn a utility function offline, using expert knowledge by means of preference learning. In contrast to other works, we do not only use (pairwise) result preferences, but also coarse information about the utility function space. This enables us to improve the utility function estimate, especially when using very few results. Additionally, we model the occurring uncertainties in the utility function learning task and propagate them through the whole optimization chain. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#21516;&#19968;&#22522;&#22240;&#20301;&#28857;&#19978;&#19981;&#21516;mRNA&#21151;&#33021;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#32570;&#20047;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2208.03325</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#21516;&#19968;&#22522;&#22240;&#20301;&#28857;&#19978;&#30340;&#19981;&#21516;mRNA&#30340;&#21151;&#33021;
&lt;/p&gt;
&lt;p&gt;
Isoform Function Prediction Using a Deep Neural Network. (arXiv:2208.03325v3 [q-bio.GN] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.03325
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#21516;&#19968;&#22522;&#22240;&#20301;&#28857;&#19978;&#19981;&#21516;mRNA&#21151;&#33021;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#32570;&#20047;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#21098;&#20999;&#26159;&#20174;&#21516;&#19968;&#22522;&#22240;&#20301;&#28857;&#20135;&#29983;&#22810;&#20010;mRNA&#30340;&#29616;&#35937;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#36229;&#36807;95%&#30340;&#20154;&#31867;&#22810;&#22806;&#26174;&#23376;&#22522;&#22240;&#32463;&#21382;&#20102;&#24322;&#21098;&#20999;&#12290;&#34429;&#28982;mRNA&#24207;&#21015;&#30340;&#21464;&#21270;&#24456;&#23567;&#65292;&#20294;&#23427;&#20204;&#21487;&#33021;&#23545;&#32454;&#32990;&#21151;&#33021;&#21644;&#35843;&#33410;&#20135;&#29983;&#31995;&#32479;&#24615;&#24433;&#21709;&#12290;&#25253;&#36947;&#31216;&#65292;&#21516;&#19968;&#22522;&#22240;&#30340;&#19981;&#21516;&#21098;&#25509;&#24418;&#24335;&#20855;&#26377;&#19981;&#21516;&#29978;&#33267;&#23545;&#31435;&#30340;&#21151;&#33021;&#12290;&#34429;&#28982;&#22522;&#22240;&#30340;&#21151;&#33021;&#30740;&#31350;&#33539;&#22260;&#24456;&#24191;&#65292;&#20294;&#23545;&#20110;&#21516;&#19968;&#22522;&#22240;&#20301;&#28857;&#19978;&#19981;&#21516;mRNA&#30340;&#21151;&#33021;&#36824;&#30693;&#20043;&#29978;&#23569;&#12290;&#26368;&#36817;&#65292;&#19968;&#20123;&#22522;&#20110;&#22810;&#23454;&#20363;&#23398;&#20064;&#30340;&#35745;&#31639;&#26041;&#27861;&#24050;&#34987;&#25552;&#20986;&#65292;&#29992;&#20110;&#39044;&#27979;&#21151;&#33021;&#65292;&#24182;&#32467;&#21512;&#20102;&#22522;&#22240;&#21151;&#33021;&#21644;&#22522;&#22240;&#34920;&#36798;&#35889;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#23427;&#20204;&#30340;&#34920;&#29616;&#24182;&#19981;&#29702;&#24819;&#12290;&#27492;&#22806;&#65292;&#27010;&#29575;&#27169;&#22411;&#65292;&#22914;&#26465;&#20214;&#38543;&#26426;&#22330;&#65288;CRF&#65289;&#65292;&#20063;&#34987;&#29992;&#20110;&#24314;&#27169;&#24322;&#26500;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Isoforms are mRNAs produced from the same gene site in the phenomenon called Alternative Splicing. Studies have shown that more than 95% of human multi-exon genes have undergone alternative splicing. Although there are few changes in mRNA sequence, They may have a systematic effect on cell function and regulation. It is widely reported that isoforms of a gene have distinct or even contrasting functions. Most studies have shown that alternative splicing plays a significant role in human health and disease. Despite the wide range of gene function studies, there is little information about isoforms' functionalities. Recently, some computational methods based on Multiple Instance Learning have been proposed to predict isoform function using gene function and gene expression profile. However, their performance is not desirable due to the lack of labeled training data. In addition, probabilistic models such as Conditional Random Field (CRF) have been used to model the relation between isofor
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#19968;&#31181;&#28304;&#33258;&#20110;&#22855;&#24322;&#24418;&#24335;&#30340;Woodbury&#30697;&#38453;&#65292;&#25552;&#20986;&#20102;&#36870;&#30697;&#38453;&#21644;&#20266;&#34892;&#21015;&#24335;&#30340;&#24191;&#20041;&#24658;&#31561;&#24335;&#65292;&#24182;&#23558;&#31934;&#23494;&#30697;&#38453;&#30340;&#23450;&#20041;&#25193;&#23637;&#21040;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;Bott-Duffin&#36870;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#31639;&#27861;&#24182;&#28436;&#31034;&#20986;&#20854;&#22312;&#35745;&#31639;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#30340;&#20284;&#28982;&#20989;&#25968;&#20013;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2207.08038</link><description>&lt;p&gt;
&#22855;&#24322;Woodbury&#30697;&#38453;&#21644;&#20266;&#34892;&#21015;&#24335;&#30697;&#38453;&#24658;&#31561;&#24335;&#21450;&#20854;&#22312;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#20013;&#30340;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Singular Woodbury and Pseudo-Determinant Matrix Identities and Application to Gaussian Process Regression. (arXiv:2207.08038v3 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.08038
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#19968;&#31181;&#28304;&#33258;&#20110;&#22855;&#24322;&#24418;&#24335;&#30340;Woodbury&#30697;&#38453;&#65292;&#25552;&#20986;&#20102;&#36870;&#30697;&#38453;&#21644;&#20266;&#34892;&#21015;&#24335;&#30340;&#24191;&#20041;&#24658;&#31561;&#24335;&#65292;&#24182;&#23558;&#31934;&#23494;&#30697;&#38453;&#30340;&#23450;&#20041;&#25193;&#23637;&#21040;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;Bott-Duffin&#36870;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#31639;&#27861;&#24182;&#28436;&#31034;&#20986;&#20854;&#22312;&#35745;&#31639;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#30340;&#20284;&#28982;&#20989;&#25968;&#20013;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#28304;&#33258;&#20110;&#22855;&#24322;&#24418;&#24335;&#30340;Woodbury&#30697;&#38453;&#24658;&#31561;&#24335;&#30340;&#30697;&#38453;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36870;&#30697;&#38453;&#21644;&#20266;&#34892;&#21015;&#24335;&#30340;&#24191;&#20041;&#24658;&#31561;&#24335;&#65292;&#36825;&#23545;&#20110;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#26377;&#30452;&#25509;&#30340;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#20854;&#31867;&#20284;&#24615;&#34920;&#31034;&#21644;&#31934;&#23494;&#30697;&#38453;&#12290;&#25105;&#20204;&#23558;&#31934;&#23494;&#30697;&#38453;&#30340;&#23450;&#20041;&#25193;&#23637;&#21040;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;Bott-Duffin&#36870;&#65292;&#20445;&#30041;&#20102;&#19982;&#26465;&#20214;&#29420;&#31435;&#24615;&#12289;&#26465;&#20214;&#31934;&#24230;&#21644;&#36793;&#38469;&#31934;&#24230;&#26377;&#20851;&#30340;&#23646;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#31639;&#27861;&#21644;&#25152;&#25552;&#20986;&#30340;&#34892;&#21015;&#24335;&#24658;&#31561;&#24335;&#30340;&#25968;&#20540;&#20998;&#26512;&#65292;&#24182;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#28436;&#31034;&#20102;&#23427;&#20204;&#22312;&#35745;&#31639;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#30340;&#20284;&#28982;&#20989;&#25968;&#20013;&#30340;&#23545;&#25968;&#34892;&#21015;&#24335;&#39033;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a matrix that arises from a singular form of the Woodbury matrix identity. We present generalized inverse and pseudo-determinant identities for this matrix, which have direct applications for Gaussian process regression, specifically its likelihood representation and precision matrix. We extend the definition of the precision matrix to the Bott-Duffin inverse of the covariance matrix, preserving properties related to conditional independence, conditional precision, and marginal precision. We also provide an efficient algorithm and numerical analysis for the presented determinant identities and demonstrate their advantages under specific conditions relevant to computing log-determinant terms in likelihood functions of Gaussian process regression.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#28014;&#28857;&#31639;&#26415;&#19979;&#30340;&#22768;&#38899;&#38543;&#26426;&#24179;&#28369;&#65292;&#35777;&#26126;&#20102;&#26080;&#38480;&#31934;&#24230;&#19979;&#30340;&#38543;&#26426;&#24179;&#28369;&#19981;&#20877;&#21487;&#38752;&#65292;&#21482;&#38656;&#20351;&#29992;&#19968;&#20010;&#20844;&#24179;&#30340;&#30828;&#24065;&#21363;&#21487;&#65292;&#36825;&#39033;&#25216;&#26415;&#21487;&#20197;&#19982;&#26631;&#20934;&#20998;&#31867;&#22120;&#30340;&#35777;&#20070;&#21305;&#37197;&#12290;</title><link>http://arxiv.org/abs/2207.07209</link><description>&lt;p&gt;
&#28014;&#28857;&#31639;&#26415;&#19979;&#30340;&#22768;&#38899;&#38543;&#26426;&#24179;&#28369;
&lt;/p&gt;
&lt;p&gt;
Sound Randomized Smoothing in Floating-Point Arithmetics. (arXiv:2207.07209v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.07209
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#28014;&#28857;&#31639;&#26415;&#19979;&#30340;&#22768;&#38899;&#38543;&#26426;&#24179;&#28369;&#65292;&#35777;&#26126;&#20102;&#26080;&#38480;&#31934;&#24230;&#19979;&#30340;&#38543;&#26426;&#24179;&#28369;&#19981;&#20877;&#21487;&#38752;&#65292;&#21482;&#38656;&#20351;&#29992;&#19968;&#20010;&#20844;&#24179;&#30340;&#30828;&#24065;&#21363;&#21487;&#65292;&#36825;&#39033;&#25216;&#26415;&#21487;&#20197;&#19982;&#26631;&#20934;&#20998;&#31867;&#22120;&#30340;&#35777;&#20070;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20351;&#29992;&#26080;&#38480;&#31934;&#24230;&#26102;&#65292;&#38543;&#26426;&#24179;&#28369;&#26159;&#21487;&#38752;&#30340;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#24403;&#20351;&#29992;&#26377;&#38480;&#30340;&#28014;&#28857;&#31934;&#24230;&#26102;&#65292;&#38543;&#26426;&#24179;&#28369;&#19981;&#20877;&#21487;&#38752;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#20363;&#23376;&#65292;&#20854;&#20013;&#38543;&#26426;&#24179;&#28369;&#35777;&#26126;&#20102;&#19968;&#20010;&#21322;&#24452;&#20026;1.26&#30340;&#28857;&#65292;&#21363;&#20351;&#26377;&#19968;&#20010;&#36317;&#31163;&#20026;0.8&#30340;&#23545;&#25239;&#24615;&#20363;&#23376;&#65292;&#24182;&#36827;&#19968;&#27493;&#25193;&#23637;&#20102;&#35813;&#20363;&#23376;&#65292;&#20026;CIFAR10&#25552;&#20379;&#34394;&#20551;&#35777;&#20070;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#38543;&#26426;&#24179;&#28369;&#30340;&#38544;&#21547;&#20551;&#35774;&#65292;&#24182;&#34920;&#26126;&#23427;&#20204;&#19981;&#36866;&#29992;&#20110;&#36890;&#24120;&#34987;&#35748;&#35777;&#30340;&#24179;&#28369;&#29256;&#26412;&#26159;&#27867;&#21270;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22768;&#38899;&#26041;&#27861;&#65292;&#20351;&#29992;&#28014;&#28857;&#31934;&#24230;&#36827;&#34892;&#38543;&#26426;&#24179;&#28369;&#65292;&#21516;&#26102;&#36895;&#24230;&#20960;&#20046;&#30456;&#21516;&#24182;&#19988;&#19982;&#24050;&#32463;&#27979;&#35797;&#30340;&#26631;&#20934;&#20998;&#31867;&#22120;&#30340;&#35777;&#20070;&#21305;&#37197;&#12290;&#25105;&#20204;&#21807;&#19968;&#30340;&#20551;&#35774;&#26159;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#19968;&#20010;&#20844;&#24179;&#30340;&#30828;&#24065;&#12290;
&lt;/p&gt;
&lt;p&gt;
Randomized smoothing is sound when using infinite precision. However, we show that randomized smoothing is no longer sound for limited floating-point precision. We present a simple example where randomized smoothing certifies a radius of $1.26$ around a point, even though there is an adversarial example in the distance $0.8$ and extend this example further to provide false certificates for CIFAR10. We discuss the implicit assumptions of randomized smoothing and show that they do not apply to generic image classification models whose smoothed versions are commonly certified. In order to overcome this problem, we propose a sound approach to randomized smoothing when using floating-point precision with essentially equal speed and matching the certificates of the standard, unsound practice for standard classifiers tested so far. Our only assumption is that we have access to a fair coin.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#22122;&#22768;&#26102;&#38388;&#24207;&#21015;&#30340;&#24314;&#27169;&#65292;&#32771;&#23519;&#20102;&#20004;&#31181;GAN&#27169;&#22411;&#29992;&#20110;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#30340;&#21487;&#34892;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#23427;&#20204;&#22312;&#21508;&#31867;&#22122;&#22768;&#30340;&#22797;&#21046;&#26041;&#38754;&#20855;&#26377;&#19968;&#23450;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2207.01110</link><description>&lt;p&gt;
&#29992;&#21367;&#31215;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#25968;&#25454;&#39537;&#21160;&#22122;&#22768;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Data-Driven Modeling of Noise Time Series with Convolutional Generative Adversarial Networks. (arXiv:2207.01110v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.01110
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#22122;&#22768;&#26102;&#38388;&#24207;&#21015;&#30340;&#24314;&#27169;&#65292;&#32771;&#23519;&#20102;&#20004;&#31181;GAN&#27169;&#22411;&#29992;&#20110;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#30340;&#21487;&#34892;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#23427;&#20204;&#22312;&#21508;&#31867;&#22122;&#22768;&#30340;&#22797;&#21046;&#26041;&#38754;&#20855;&#26377;&#19968;&#23450;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#36807;&#31243;&#20013;&#20135;&#29983;&#30340;&#38543;&#26426;&#22122;&#22768;&#26159;&#27979;&#37327;&#30340;&#22266;&#26377;&#29305;&#24615;&#65292;&#24182;&#19988;&#26159;&#22823;&#22810;&#25968;&#20449;&#21495;&#22788;&#29702;&#21644;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#30340;&#38480;&#21046;&#22240;&#32032;&#12290;&#37492;&#20110;&#36817;&#24180;&#26469;&#23545;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;(GANs)&#29992;&#20110;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#30340;&#20852;&#36259;&#65292;&#30830;&#23450;GANs&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#33021;&#22815;&#24544;&#23454;&#22320;&#22797;&#21046;&#30446;&#26631;&#25968;&#25454;&#38598;&#20013;&#30340;&#22122;&#22768;&#38750;&#24120;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#39033;&#23454;&#35777;&#30740;&#31350;&#65292;&#26088;&#22312;&#20026;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#36825;&#20010;&#38382;&#39064;&#25552;&#20379;&#21551;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20004;&#31181;&#22522;&#20110;&#27969;&#34892;&#30340;&#28145;&#24230;&#21367;&#31215;GAN&#65288;DCGAN&#65289;&#32467;&#26500;&#30340;&#36890;&#29992;&#26102;&#38388;&#24207;&#21015;GAN&#65292;&#19968;&#31181;&#26159;&#30452;&#25509;&#30340;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65292;&#21478;&#19968;&#31181;&#26159;&#22522;&#20110;&#30701;&#26102;&#20613;&#37324;&#21494;&#21464;&#25442;&#65288;STFT&#65289;&#25968;&#25454;&#34920;&#31034;&#30340;&#22270;&#20687;&#27169;&#22411;&#12290;GAN&#27169;&#22411;&#20351;&#29992;&#24050;&#30693;&#22522;&#30784;&#30495;&#23454;&#21442;&#25968;&#30340;&#27169;&#25311;&#22122;&#22768;&#26102;&#38388;&#24207;&#21015;&#20998;&#24067;&#36827;&#34892;&#35757;&#32451;&#21644;&#23450;&#37327;&#35780;&#20272;&#12290;&#30446;&#26631;&#26102;&#38388;&#24207;&#21015;&#20998;&#24067;&#28085;&#30422;&#20102;&#29289;&#29702;&#27979;&#37327;&#12289;&#30005;&#23376;&#23398;&#21644;...
&lt;/p&gt;
&lt;p&gt;
Random noise arising from physical processes is an inherent characteristic of measurements and a limiting factor for most signal processing and data analysis tasks. Given the recent interest in generative adversarial networks (GANs) for data-driven modeling, it is important to determine to what extent GANs can faithfully reproduce noise in target data sets. In this paper, we present an empirical investigation that aims to shed light on this issue for time series. Namely, we assess two general-purpose GANs for time series that are based on the popular deep convolutional GAN (DCGAN) architecture, a direct time-series model and an image-based model that uses a short-time Fourier transform (STFT) data representation. The GAN models are trained and quantitatively evaluated using distributions of simulated noise time series with known ground-truth parameters. Target time series distributions include a broad range of noise types commonly encountered in physical measurements, electronics, and 
&lt;/p&gt;</description></item><item><title>BiometricBlender&#26159;&#19968;&#20010;&#39640;&#32500;&#24230;&#22810;&#31867;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#24037;&#20855;&#65292;&#21487;&#20197;&#27169;&#25311;&#30495;&#23454;&#29983;&#29289;&#29305;&#24449;&#25968;&#25454;&#38598;&#30340;&#20851;&#38190;&#23646;&#24615;&#65292;&#26377;&#21161;&#20110;&#22312;&#29305;&#24449;&#31579;&#36873;&#39046;&#22495;&#36827;&#34892;&#24555;&#36895;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2206.10747</link><description>&lt;p&gt;
BiometricBlender&#65306;&#39640;&#32500;&#24230;&#22810;&#31867;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
BiometricBlender: Ultra-high dimensional, multi-class synthetic data generator to imitate biometric feature space. (arXiv:2206.10747v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.10747
&lt;/p&gt;
&lt;p&gt;
BiometricBlender&#26159;&#19968;&#20010;&#39640;&#32500;&#24230;&#22810;&#31867;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#24037;&#20855;&#65292;&#21487;&#20197;&#27169;&#25311;&#30495;&#23454;&#29983;&#29289;&#29305;&#24449;&#25968;&#25454;&#38598;&#30340;&#20851;&#38190;&#23646;&#24615;&#65292;&#26377;&#21161;&#20110;&#22312;&#29305;&#24449;&#31579;&#36873;&#39046;&#22495;&#36827;&#34892;&#24555;&#36895;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32570;&#20047;&#33258;&#30001;&#21487;&#24471;&#30340;&#39640;&#32500;&#24230;&#22810;&#31867;&#30495;&#23454;&#25110;&#21512;&#25104;&#25968;&#25454;&#38598;&#24120;&#24120;&#38480;&#21046;&#20102;&#29305;&#24449;&#31579;&#36873;&#39046;&#22495;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#23588;&#20854;&#26159;&#22312;&#29983;&#29289;&#29305;&#24449;&#35782;&#21035;&#39046;&#22495;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;BiometricBlender&#30340;Python&#21253;&#65292;&#23427;&#26159;&#19968;&#20010;&#36229;&#39640;&#32500;&#24230;&#22810;&#31867;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#24037;&#20855;&#65292;&#29992;&#20110;&#27979;&#35797;&#21508;&#31181;&#29305;&#24449;&#31579;&#36873;&#26041;&#27861;&#12290;&#22312;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#65292;&#29992;&#25143;&#21487;&#20197;&#25511;&#21046;&#29305;&#24449;&#28151;&#21512;&#30340;&#24635;&#20307;&#26377;&#29992;&#24615;&#21644;&#20114;&#30456;&#20851;&#31995;&#25968;&#65292;&#22240;&#27492;&#21512;&#25104;&#29305;&#24449;&#31354;&#38388;&#33021;&#22815;&#27169;&#25311;&#30495;&#23454;&#29983;&#29289;&#29305;&#24449;&#25968;&#25454;&#38598;&#30340;&#20851;&#38190;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The lack of freely available (real-life or synthetic) high or ultra-high dimensional, multi-class datasets may hamper the rapidly growing research on feature screening, especially in the field of biometrics, where the usage of such datasets is common. This paper reports a Python package called BiometricBlender, which is an ultra-high dimensional, multi-class synthetic data generator to benchmark a wide range of feature screening methods. During the data generation process, the overall usefulness and the intercorrelations of blended features can be controlled by the user, thus the synthetic feature space is able to imitate the key properties of a real biometric dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#25513;&#33180;&#39057;&#29575;&#24314;&#27169;&#65288;MFM&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#30417;&#30563;&#35270;&#35273;&#39044;&#35757;&#32451;&#65292;&#36890;&#36807;&#23545;&#39640;&#20302;&#39057;&#25104;&#20998;&#36827;&#34892;&#25513;&#33180;&#39044;&#27979;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25581;&#31034;&#24213;&#23618;&#22270;&#20687;&#27169;&#24335;&#65292;&#24182;&#22312;ViT&#21644;CNN&#27169;&#22411;&#20013;&#23454;&#29616;&#24456;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2206.07706</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#35270;&#35273;&#39044;&#35757;&#32451;&#20013;&#30340;&#25513;&#33180;&#39057;&#29575;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Masked Frequency Modeling for Self-Supervised Visual Pre-Training. (arXiv:2206.07706v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.07706
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#25513;&#33180;&#39057;&#29575;&#24314;&#27169;&#65288;MFM&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#30417;&#30563;&#35270;&#35273;&#39044;&#35757;&#32451;&#65292;&#36890;&#36807;&#23545;&#39640;&#20302;&#39057;&#25104;&#20998;&#36827;&#34892;&#25513;&#33180;&#39044;&#27979;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25581;&#31034;&#24213;&#23618;&#22270;&#20687;&#27169;&#24335;&#65292;&#24182;&#22312;ViT&#21644;CNN&#27169;&#22411;&#20013;&#23454;&#29616;&#24456;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#25513;&#33180;&#39057;&#29575;&#24314;&#27169;&#65288;MFM&#65289;, &#19968;&#31181;&#29992;&#20110;&#33258;&#30417;&#30563;&#35270;&#35273;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#22522;&#20110;&#39057;&#29575;&#22495;&#30340;&#32479;&#19968;&#26041;&#27861;&#12290;&#30456;&#27604;&#20110;&#22312;&#31354;&#38388;&#22495;&#20013;&#38543;&#26426;&#25554;&#20837;&#25513;&#33180;&#26631;&#35760;&#65292; MFM&#23558;&#35270;&#35282;&#36716;&#21521;&#39057;&#29575;&#22495;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;MFM&#39318;&#20808;&#23631;&#34109;&#36755;&#20837;&#22270;&#20687;&#39057;&#29575;&#25104;&#20998;&#30340;&#19968;&#37096;&#20998;, &#28982;&#21518;&#39044;&#27979;&#39057;&#35889;&#19978;&#32570;&#22833;&#30340;&#39057;&#29575;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#65292;&#22312;&#39057;&#29575;&#22495;&#20013;&#39044;&#27979;&#25513;&#33180;&#32452;&#20214;&#27604;&#39044;&#27979;&#31354;&#38388;&#22495;&#20013;&#30340;&#25513;&#33180;&#34917;&#19969;&#26356;&#33021;&#22815;&#25581;&#31034;&#24213;&#23618;&#22270;&#20687;&#27169;&#24335;&#65292;&#22240;&#20026;&#21518;&#32773;&#20855;&#26377;&#36739;&#22823;&#30340;&#31354;&#38388;&#20887;&#20313;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#27491;&#30830;&#30340;&#25513;&#33180;&#21644;&#39044;&#27979;&#31574;&#30053;, &#39640;&#39057;&#25104;&#20998;&#20013;&#30340;&#32467;&#26500;&#20449;&#24687;&#20197;&#21450;&#20302;&#39057;&#25104;&#20998;&#20013;&#30340;&#20302;&#38454;&#32479;&#35745;&#20449;&#24687;&#37117;&#26377;&#21161;&#20110;&#23398;&#20064;&#33391;&#22909;&#30340;&#34920;&#31034;&#26041;&#27861;&#12290;MFM&#39318;&#27425;&#35777;&#26126;, &#23545;&#20110;ViT&#21644;CNN&#27169;&#22411;&#65292;&#31616;&#21333;&#30340;&#22312;&#32447;&#25513;&#33180;&#39044;&#27979;&#26426;&#21046;&#21487;&#20197;&#22312;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#20013;&#23454;&#29616;&#24456;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Masked Frequency Modeling (MFM), a unified frequency-domain-based approach for self-supervised pre-training of visual models. Instead of randomly inserting mask tokens to the input embeddings in the spatial domain, in this paper, we shift the perspective to the frequency domain. Specifically, MFM first masks out a portion of frequency components of the input image and then predicts the missing frequencies on the frequency spectrum. Our key insight is that predicting masked components in the frequency domain is more ideal to reveal underlying image patterns rather than predicting masked patches in the spatial domain, due to the heavy spatial redundancy. Our findings suggest that with the right configuration of mask-and-predict strategy, both the structural information within high-frequency components and the low-level statistics among low-frequency counterparts are useful in learning good representations. For the first time, MFM demonstrates that, for both ViT and CNN, a simp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;DIMPLE&#32593;&#32476;&#27169;&#22411;&#20013;&#30340;&#31232;&#30095;&#23376;&#31354;&#38388;&#32858;&#31867;&#65292;&#36890;&#36807;&#35782;&#21035;&#20855;&#26377;&#30456;&#21516;&#31038;&#21306;&#32467;&#26500;&#30340;&#23618;&#32452;&#65292;&#25214;&#21040;&#20102;&#19968;&#31181;&#24378;&#19968;&#33268;&#24615;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2206.07602</link><description>&lt;p&gt;
&#22810;&#26679;&#22810;&#23618;&#32593;&#32476;&#27169;&#22411;&#20013;&#30340;&#31232;&#30095;&#23376;&#31354;&#38388;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Sparse Subspace Clustering in Diverse Multiplex Network Model. (arXiv:2206.07602v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.07602
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;DIMPLE&#32593;&#32476;&#27169;&#22411;&#20013;&#30340;&#31232;&#30095;&#23376;&#31354;&#38388;&#32858;&#31867;&#65292;&#36890;&#36807;&#35782;&#21035;&#20855;&#26377;&#30456;&#21516;&#31038;&#21306;&#32467;&#26500;&#30340;&#23618;&#32452;&#65292;&#25214;&#21040;&#20102;&#19968;&#31181;&#24378;&#19968;&#33268;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;Pensky&#21644;Wang&#65288;2021&#65289;&#24341;&#20837;&#30340;DIverse MultiPLEx&#65288;DIMPLE&#65289;&#32593;&#32476;&#27169;&#22411;&#65292;&#20854;&#20013;&#32593;&#32476;&#30340;&#25152;&#26377;&#23618;&#37117;&#20855;&#26377;&#30456;&#21516;&#30340;&#33410;&#28857;&#38598;&#21512;&#65292;&#24182;&#37197;&#22791;&#26377;&#38543;&#26426;&#22359;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25152;&#26377;&#23618;&#37117;&#21487;&#20197;&#20998;&#20026;&#20855;&#26377;&#30456;&#21516;&#31038;&#21306;&#32467;&#26500;&#30340;&#32452;&#65292;&#23613;&#31649;&#22312;&#21516;&#19968;&#32452;&#20013;&#30340;&#23618;&#21487;&#33021;&#20855;&#26377;&#19981;&#21516;&#30340;&#22359;&#36830;&#25509;&#27010;&#29575;&#30697;&#38453;&#12290;DIMPLE&#27169;&#22411;&#27010;&#25324;&#20102;&#35768;&#22810;&#30740;&#31350;&#25152;&#26377;&#23618;&#20855;&#26377;&#30456;&#21516;&#31038;&#21306;&#32467;&#26500;&#30340;&#22810;&#23618;&#32593;&#32476;&#30340;&#35770;&#25991;&#65292;&#20197;&#21450;&#28151;&#21512;&#22810;&#23618;&#38543;&#26426;&#22359;&#27169;&#22411;&#65288;MMLSBM&#65289;&#65292;&#22312;&#20854;&#20013;&#21516;&#19968;&#32452;&#20013;&#30340;&#23618;&#20855;&#26377;&#30456;&#21516;&#30340;&#22359;&#36830;&#25509;&#27010;&#29575;&#30697;&#38453;&#12290;&#26412;&#25991;&#20351;&#29992;&#31232;&#30095;&#23376;&#31354;&#38388;&#32858;&#31867;&#65288;SSC&#65289;&#26469;&#35782;&#21035;&#20855;&#26377;&#30456;&#21516;&#31038;&#21306;&#32467;&#26500;&#30340;&#23618;&#32452;&#12290;&#22312;&#28201;&#21644;&#30340;&#26465;&#20214;&#19979;&#65292;&#21518;&#32773;&#23548;&#33268;&#20102;&#24378;&#19968;&#33268;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper considers the DIverse MultiPLEx (DIMPLE) network model, introduced in Pensky and Wang (2021), where all layers of the network have the same collection of nodes and are equipped with the Stochastic Block Models. In addition, all layers can be partitioned into groups with the same community structures, although the layers in the same group may have different matrices of block connection probabilities. The DIMPLE model generalizes a multitude of papers that study multilayer networks with the same community structures in all layers, as well as the Mixture Multilayer Stochastic Block Model (MMLSBM), where the layers in the same group have identical matrices of block connection probabilities. While Pensky and Wang (2021) applied spectral clustering to the proxy of the adjacency tensor, the present paper uses Sparse Subspace Clustering (SSC) for identifying groups of layers with identical community structures. Under mild conditions, the latter leads to the strongly consistent betwee
&lt;/p&gt;</description></item><item><title>TAD&#26159;&#19968;&#31181;&#26032;&#30340;&#30446;&#26631;&#33258;&#36866;&#24212;&#35774;&#35745;&#31639;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#39640;&#26031;&#36807;&#31243;&#20195;&#29702;&#27169;&#22411;&#22312;&#25351;&#23450;&#20844;&#24046;&#20013;&#30830;&#23450;&#20135;&#29983;&#26399;&#26395;&#35774;&#35745;&#29305;&#24449;&#30340;&#26368;&#20339;&#25511;&#21046;&#35774;&#32622;&#65292;&#30456;&#27604;&#20854;&#20182;&#33258;&#36866;&#24212;&#35774;&#35745;&#31639;&#27861;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#31934;&#24230;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2205.14208</link><description>&lt;p&gt;
&#30446;&#26631;&#33258;&#36866;&#24212;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Targeted Adaptive Design. (arXiv:2205.14208v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.14208
&lt;/p&gt;
&lt;p&gt;
TAD&#26159;&#19968;&#31181;&#26032;&#30340;&#30446;&#26631;&#33258;&#36866;&#24212;&#35774;&#35745;&#31639;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#39640;&#26031;&#36807;&#31243;&#20195;&#29702;&#27169;&#22411;&#22312;&#25351;&#23450;&#20844;&#24046;&#20013;&#30830;&#23450;&#20135;&#29983;&#26399;&#26395;&#35774;&#35745;&#29305;&#24449;&#30340;&#26368;&#20339;&#25511;&#21046;&#35774;&#32622;&#65292;&#30456;&#27604;&#20854;&#20182;&#33258;&#36866;&#24212;&#35774;&#35745;&#31639;&#27861;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#31934;&#24230;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#20808;&#36827;&#21046;&#36896;&#21644;&#39640;&#32423;&#26448;&#26009;&#35774;&#35745;&#24448;&#24448;&#38656;&#35201;&#22312;&#36739;&#39640;&#32500;&#24230;&#30340;&#36807;&#31243;&#25511;&#21046;&#21442;&#25968;&#31354;&#38388;&#20013;&#25628;&#32034;&#26368;&#20339;&#32467;&#26500;&#12289;&#24615;&#33021;&#21644;&#24615;&#33021;&#21442;&#25968;&#30340;&#35774;&#32622;&#12290;&#20174;&#21069;&#32773;&#21040;&#21518;&#32773;&#30340;&#26144;&#23556;&#24517;&#39035;&#36890;&#36807;&#22024;&#26434;&#30340;&#23454;&#39564;&#25110;&#26114;&#36149;&#30340;&#27169;&#25311;&#26469;&#30830;&#23450;&#12290;&#25105;&#20204;&#25226;&#36825;&#20010;&#38382;&#39064;&#25277;&#35937;&#25104;&#19968;&#20010;&#25968;&#23398;&#26694;&#26550;&#65292;&#20854;&#20013;&#24517;&#39035;&#36890;&#36807;&#26114;&#36149;&#30340;&#22024;&#26434;&#27979;&#37327;&#26469;&#30830;&#23450;&#20174;&#25511;&#21046;&#31354;&#38388;&#21040;&#35774;&#35745;&#31354;&#38388;&#30340;&#26410;&#30693;&#20989;&#25968;&#65292;&#35813;&#20989;&#25968;&#22312;&#25351;&#23450;&#30340;&#20844;&#24046;&#33539;&#22260;&#20869;&#23450;&#20301;&#20135;&#29983;&#26399;&#26395;&#35774;&#35745;&#29305;&#24449;&#30340;&#26368;&#20339;&#25511;&#21046;&#35774;&#32622;&#65292;&#24182;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#30446;&#26631;&#33258;&#36866;&#24212;&#35774;&#35745; (TAD)&#65292;&#36825;&#26159;&#19968;&#31181;&#26377;&#25928;&#25191;&#34892;&#36825;&#20010;&#37319;&#26679;&#20219;&#21153;&#30340;&#26032;&#31639;&#27861;&#12290;TAD &#22312;&#27599;&#20010;&#36845;&#20195;&#38454;&#27573;&#21019;&#24314;&#19968;&#20010;&#26410;&#30693;&#26144;&#23556;&#30340;&#39640;&#26031;&#36807;&#31243;&#20195;&#29702;&#27169;&#22411;&#65292;&#24314;&#35758;&#19968;&#25209;&#26032;&#30340;&#25511;&#21046;&#35774;&#32622;&#36827;&#34892;&#23454;&#39564;&#37319;&#26679;&#65292;&#24182;&#20248;&#21270;&#26356;&#26032;&#30340;&#30446;&#26631;&#35774;&#35745;&#29305;&#24449;&#30340;&#23545;&#25968;&#39044;&#27979;&#20284;&#28982;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern advanced manufacturing and advanced materials design often require searches of relatively high-dimensional process control parameter spaces for settings that result in optimal structure, property, and performance parameters. The mapping from the former to the latter must be determined from noisy experiments or from expensive simulations. We abstract this problem to a mathematical framework in which an unknown function from a control space to a design space must be ascertained by means of expensive noisy measurements, which locate optimal control settings generating desired design features within specified tolerances, with quantified uncertainty. We describe targeted adaptive design (TAD), a new algorithm that performs this sampling task efficiently. TAD creates a Gaussian process surrogate model of the unknown mapping at each iterative stage, proposing a new batch of control settings to sample experimentally and optimizing the updated log-predictive likelihood of the target desi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;MetaPG&#65292;&#19968;&#31181;&#33258;&#21160;&#35774;&#35745;&#28436;&#21592;-&#35780;&#35770;&#23478;&#25439;&#22833;&#20989;&#25968;&#30340;&#36827;&#21270;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26126;&#30830;&#22320;&#20248;&#21270;&#27867;&#21270;&#24615;&#21644;&#24615;&#33021;&#65292;&#24182;&#38544;&#24335;&#22320;&#20248;&#21270;&#36825;&#20004;&#20010;&#25351;&#26631;&#30340;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2204.04292</link><description>&lt;p&gt;
&#36827;&#21270;&#24085;&#32047;&#25176;&#26368;&#20248;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#20197;&#23454;&#29616;&#27867;&#21270;&#21644;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evolving Pareto-Optimal Actor-Critic Algorithms for Generalizability and Stability. (arXiv:2204.04292v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.04292
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;MetaPG&#65292;&#19968;&#31181;&#33258;&#21160;&#35774;&#35745;&#28436;&#21592;-&#35780;&#35770;&#23478;&#25439;&#22833;&#20989;&#25968;&#30340;&#36827;&#21270;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26126;&#30830;&#22320;&#20248;&#21270;&#27867;&#21270;&#24615;&#21644;&#24615;&#33021;&#65292;&#24182;&#38544;&#24335;&#22320;&#20248;&#21270;&#36825;&#20004;&#20010;&#25351;&#26631;&#30340;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27867;&#21270;&#24615;&#21644;&#31283;&#23450;&#24615;&#26159;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#25805;&#20316;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#20004;&#20010;&#20851;&#38190;&#30446;&#26631;&#12290;&#35774;&#35745;&#20248;&#21270;&#36825;&#20123;&#30446;&#26631;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21487;&#33021;&#26159;&#19968;&#20010;&#26114;&#36149;&#32780;&#36153;&#26102;&#30340;&#36807;&#31243;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;MetaPG&#65292;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#35774;&#35745;&#28436;&#21592;-&#35780;&#35770;&#23478;&#25439;&#22833;&#20989;&#25968;&#30340;&#36827;&#21270;&#26041;&#27861;&#12290;MetaPG&#26126;&#30830;&#22320;&#20248;&#21270;&#27867;&#21270;&#24615;&#21644;&#24615;&#33021;&#65292;&#24182;&#38544;&#24335;&#22320;&#20248;&#21270;&#20004;&#20010;&#25351;&#26631;&#30340;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#36719;&#28436;&#21592;&#35780;&#35770;&#23478;&#65288;SAC&#65289;&#21021;&#22987;&#21270;&#25105;&#20204;&#30340;&#25439;&#22833;&#20989;&#25968;&#31181;&#32676;&#65292;&#24182;&#20351;&#29992;&#32534;&#30721;&#21333;&#20219;&#21153;&#24615;&#33021;&#12289;&#23545;&#26410;&#35265;&#36807;&#30340;&#29615;&#22659;&#37197;&#32622;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#24615;&#21644;&#22312;&#19981;&#21516;&#38543;&#26426;&#31181;&#23376;&#19979;&#29420;&#31435;&#36816;&#34892;&#26102;&#30340;&#31283;&#23450;&#24615;&#30340;&#36866;&#24212;&#24615;&#24230;&#37327;&#36827;&#34892;&#22810;&#30446;&#26631;&#20248;&#21270;&#12290;&#22312;&#26469;&#33258;&#30495;&#23454;&#19990;&#30028;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#22871;&#20214;&#30340;&#19968;&#32452;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#21333;&#20010;&#29615;&#22659;&#36827;&#34892;&#36827;&#21270;&#30340;&#26041;&#27861;&#21487;&#20197;&#20351;&#28436;&#21464;&#20986;&#26469;&#30340;&#31639;&#27861;&#22312;&#24615;&#33021;&#21644;&#27867;&#21270;&#24615;&#19978;&#27604;SAC&#26377;4%&#21644;20%&#30340;&#25913;&#36827;&#65292;&#21516;&#26102;&#23558;&#24615;&#33021;&#30340;&#26631;&#20934;&#24046;&#20943;&#23569;&#20102;&#36817;&#19968;&#21322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generalizability and stability are two key objectives for operating reinforcement learning (RL) agents in the real world. Designing RL algorithms that optimize these objectives can be a costly and painstaking process. This paper presents MetaPG, an evolutionary method for automated design of actor-critic loss functions. MetaPG explicitly optimizes for generalizability and performance, and implicitly optimizes the stability of both metrics. We initialize our loss function population with Soft Actor-Critic (SAC) and perform multi-objective optimization using fitness metrics encoding single-task performance, zero-shot generalizability to unseen environment configurations, and stability across independent runs with different random seeds. On a set of continuous control tasks from the Real-World RL Benchmark Suite, we find that our method, using a single environment during evolution, evolves algorithms that improve upon SAC's performance and generalizability by 4% and 20%, respectively, and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25351;&#20986;&#20102;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#20013;&#65292;&#20351;&#29992;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22120;&#20272;&#35745;&#38271;&#24230;&#23610;&#24230;&#21442;&#25968;&#30340;&#24179;&#31283;&#21327;&#26041;&#24046;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#26080;&#22122;&#22768;&#25968;&#25454;&#24773;&#20917;&#19979;&#20250;&#23384;&#22312;&#19981;&#36866;&#23450;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2203.09179</link><description>&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#20013;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#23384;&#22312;&#19981;&#36866;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Maximum Likelihood Estimation in Gaussian Process Regression is Ill-Posed. (arXiv:2203.09179v3 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.09179
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25351;&#20986;&#20102;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#20013;&#65292;&#20351;&#29992;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22120;&#20272;&#35745;&#38271;&#24230;&#23610;&#24230;&#21442;&#25968;&#30340;&#24179;&#31283;&#21327;&#26041;&#24046;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#26080;&#22122;&#22768;&#25968;&#25454;&#24773;&#20917;&#19979;&#20250;&#23384;&#22312;&#19981;&#36866;&#23450;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#26159;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#23398;&#20013;&#26080;&#25968;&#23398;&#26415;&#21644;&#24037;&#19994;&#24212;&#29992;&#30340;&#22522;&#30784;&#65292;&#20854;&#20013;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#36890;&#24120;&#29992;&#20110;&#36873;&#25321;&#36866;&#24403;&#30340;&#21327;&#26041;&#24046;&#26680;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#30830;&#23450;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#20026;&#36866;&#23450;&#38382;&#39064;&#30340;&#24773;&#20917;&#23578;&#26410;&#35299;&#20915;&#65292;&#21363;&#65292;&#22312;&#22238;&#24402;&#27169;&#22411;&#30340;&#39044;&#27979;&#23545;&#25968;&#25454;&#30340;&#24494;&#23567;&#25200;&#21160;&#19981;&#25935;&#24863;&#30340;&#24773;&#20917;&#19979;&#12290;&#26412;&#25991;&#30830;&#23450;&#20102;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22120;&#26080;&#27861;&#36866;&#23450;&#30340;&#24773;&#20917;&#65292;&#21363;&#22312;Hellinger&#36317;&#31163;&#24847;&#20041;&#19979;&#65292;&#39044;&#27979;&#20998;&#24067;&#23545;&#25968;&#25454;&#19981;&#20855;&#26377;Lipschitz&#36830;&#32493;&#24615;&#12290;&#36825;&#20123;&#22833;&#36133;&#24773;&#20917;&#21457;&#29983;&#22312;&#26080;&#22122;&#22768;&#25968;&#25454;&#24773;&#20917;&#19979;&#65292;&#23545;&#20110;&#20219;&#20309;&#20351;&#29992;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#20272;&#35745;&#38271;&#24230;&#23610;&#24230;&#21442;&#25968;&#30340;&#24179;&#31283;&#21327;&#26041;&#24046;&#20989;&#25968;&#30340;&#39640;&#26031;&#36807;&#31243;&#12290;&#34429;&#28982;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#22833;&#36133;&#26159;&#39640;&#26031;&#36807;&#31243;&#30340;&#24120;&#35782;&#65292;&#20294;&#36825;&#20123;&#20005;&#26684;&#30340;&#29702;&#35770;&#32467;&#26524;&#20284;&#20046;&#26159;&#31532;&#19968;&#27425;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gaussian process regression underpins countless academic and industrial applications of machine learning and statistics, with maximum likelihood estimation routinely used to select appropriate parameters for the covariance kernel. However, it remains an open problem to establish the circumstances in which maximum likelihood estimation is well-posed, that is, when the predictions of the regression model are insensitive to small perturbations of the data. This article identifies scenarios where the maximum likelihood estimator fails to be well-posed, in that the predictive distributions are not Lipschitz in the data with respect to the Hellinger distance. These failure cases occur in the noiseless data setting, for any Gaussian process with a stationary covariance function whose lengthscale parameter is estimated using maximum likelihood. Although the failure of maximum likelihood estimation is part of Gaussian process folklore, these rigorous theoretical results appear to be the first o
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#24102;&#20013;&#32487;&#30340;IoT&#32593;&#32476;&#30340;&#23454;&#29992;AoI&#35843;&#24230;&#22120;&#65292;&#21487;&#32771;&#34385;&#24120;&#35265;&#30340;&#21160;&#24577;&#20449;&#36947;&#26465;&#20214;&#21644;&#26410;&#30693;&#25968;&#25454;&#21253;&#29983;&#25104;&#27169;&#24335;&#12290;&#32463;&#36807;&#24191;&#27867;&#30340;&#27169;&#25311;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#35843;&#24230;&#22120;&#22312;&#24179;&#22343;AoI&#12289;&#25968;&#25454;&#21253;&#20002;&#22833;&#29575;&#21644;&#33021;&#37327;&#28040;&#32791;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;AoI&#35843;&#24230;&#22120;&#12290;</title><link>http://arxiv.org/abs/2203.04227</link><description>&lt;p&gt;
IoT&#32593;&#32476;&#20013;&#24102;&#20013;&#32487;&#30340;&#23454;&#29992;AoI&#35843;&#24230;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Practical AoI Scheduler in IoT Networks with Relays. (arXiv:2203.04227v3 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.04227
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#24102;&#20013;&#32487;&#30340;IoT&#32593;&#32476;&#30340;&#23454;&#29992;AoI&#35843;&#24230;&#22120;&#65292;&#21487;&#32771;&#34385;&#24120;&#35265;&#30340;&#21160;&#24577;&#20449;&#36947;&#26465;&#20214;&#21644;&#26410;&#30693;&#25968;&#25454;&#21253;&#29983;&#25104;&#27169;&#24335;&#12290;&#32463;&#36807;&#24191;&#27867;&#30340;&#27169;&#25311;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#35843;&#24230;&#22120;&#22312;&#24179;&#22343;AoI&#12289;&#25968;&#25454;&#21253;&#20002;&#22833;&#29575;&#21644;&#33021;&#37327;&#28040;&#32791;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;AoI&#35843;&#24230;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35774;&#22791;&#38388;&#33258;&#20027;&#35745;&#31639;&#12289;&#36890;&#20449;&#21644;&#21327;&#20316;&#30340;&#27969;&#34892;&#65292;&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#32593;&#32476;&#24050;&#32463;&#21464;&#24471;&#26080;&#25152;&#19981;&#22312;&#12290;&#22312;IoT&#32593;&#32476;&#20013;&#20351;&#29992;&#20013;&#32487;&#36827;&#19968;&#27493;&#26041;&#20415;&#20102;&#37096;&#32626;&#65292;&#22240;&#20026;&#20013;&#32487;&#25552;&#20379;&#20102;&#35768;&#22810;&#22909;&#22788;&#65292;&#22914;&#22686;&#21152;&#36890;&#20449;&#33539;&#22260;&#21644;&#26368;&#23567;&#21270;&#30005;&#21147;&#28040;&#32791;&#12290;&#20294;&#20256;&#32479;&#30340;&#29992;&#20110;&#36825;&#31181;&#21452;&#36339;&#32487;&#30005;&#22120;IoT&#32593;&#32476;&#30340;AoI&#35843;&#24230;&#22120;&#30340;&#29616;&#26377;&#25991;&#29486;&#26377;&#38480;&#65292;&#22240;&#20026;&#23427;&#20204;&#26159;&#22522;&#20110;&#20551;&#35774;&#24658;&#23450;/&#19981;&#21464;&#30340;&#20449;&#36947;&#26465;&#20214;&#21644;&#24050;&#30693;&#30340;&#65288;&#36890;&#24120;&#26159;&#38543;&#24847;&#29983;&#25104;&#30340;&#65289;&#25968;&#25454;&#21253;&#29983;&#25104;&#27169;&#24335;&#35774;&#35745;&#30340;&#12290;&#38024;&#23545;&#24102;&#20013;&#32487;&#30340;&#21452;&#36339;IoT&#32593;&#32476;&#30340;AoI&#35843;&#24230;&#24050;&#32463;&#30740;&#31350;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#31639;&#27861;&#65292;&#20294;&#26159;&#30001;&#20110;&#21160;&#20316;&#31354;&#38388;&#38543;&#30528;&#32593;&#32476;&#35268;&#27169;&#30340;&#22686;&#22823;&#21576;&#25351;&#25968;&#22686;&#38271;&#65292;&#23427;&#20204;&#20165;&#36866;&#29992;&#20110;&#23567;&#35268;&#27169;IoT&#32593;&#32476;&#12290;&#36825;&#20123;&#23616;&#38480;&#24615;&#38459;&#30861;&#20102;&#23545;AoI&#35843;&#24230;&#22120;&#22312;IoT&#32593;&#32476;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#24102;&#20013;&#32487;&#30340;IoT&#32593;&#32476;&#30340;&#23454;&#29992;AoI&#35843;&#24230;&#22120;&#65292;&#35813;&#35843;&#24230;&#22120;&#32771;&#34385;&#20102;&#24120;&#35265;&#30340;&#21160;&#24577;&#20449;&#36947;&#26465;&#20214;&#21644;&#26410;&#30693;&#25968;&#25454;&#21253;&#29983;&#25104;&#27169;&#24335;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#27169;&#25311;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#35843;&#24230;&#22120;&#22312;&#24179;&#22343;AoI&#12289;&#25968;&#25454;&#21253;&#20002;&#22833;&#29575;&#21644;&#33021;&#37327;&#28040;&#32791;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;AoI&#35843;&#24230;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Internet of Things (IoT) networks have become ubiquitous as autonomous computing, communication and collaboration among devices become popular for accomplishing various tasks. The use of relays in IoT networks further makes it convenient to deploy IoT networks as relays provide a host of benefits, like increasing the communication range and minimizing power consumption. Existing literature on traditional AoI schedulers for such two-hop relayed IoT networks are limited because they are designed assuming constant/non-changing channel conditions and known (usually, generate-at-will) packet generation patterns. Deep reinforcement learning (DRL) algorithms have been investigated for AoI scheduling in two-hop IoT networks with relays, however, they are only applicable for small-scale IoT networks due to exponential rise in action space as the networks become large. These limitations discourage the practical utilization of AoI schedulers for IoT network deployments. This paper presents a prac
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#22270;&#31070;&#32463;&#32593;&#32476;&#19982;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26426;&#22120;&#20154;&#23548;&#33322;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22797;&#26434;&#30340;&#20154;&#32676;&#22330;&#26223;&#20013;&#23454;&#29616;&#23433;&#20840;&#12289;&#39640;&#25928;&#21644;&#19981;&#24178;&#25200;&#30340;&#23548;&#33322;&#65292;&#36890;&#36807;&#39044;&#27979;&#26410;&#26469;&#36712;&#36857;&#25512;&#26029;&#20986;&#21160;&#24577;&#20195;&#29702;&#30340;&#24847;&#22270;&#65292;&#36991;&#20813;&#20405;&#20837;&#20182;&#20154;&#39044;&#26399;&#36335;&#24452;&#12290;</title><link>http://arxiv.org/abs/2203.01821</link><description>&lt;p&gt;
&#27880;&#24847;&#21147;&#20132;&#20114;&#22270;&#20013;&#30340;&#24847;&#22270;&#24863;&#30693;&#26426;&#22120;&#20154;&#32676;&#20307;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Intention Aware Robot Crowd Navigation with Attention-Based Interaction Graph. (arXiv:2203.01821v4 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.01821
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#22270;&#31070;&#32463;&#32593;&#32476;&#19982;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26426;&#22120;&#20154;&#23548;&#33322;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22797;&#26434;&#30340;&#20154;&#32676;&#22330;&#26223;&#20013;&#23454;&#29616;&#23433;&#20840;&#12289;&#39640;&#25928;&#21644;&#19981;&#24178;&#25200;&#30340;&#23548;&#33322;&#65292;&#36890;&#36807;&#39044;&#27979;&#26410;&#26469;&#36712;&#36857;&#25512;&#26029;&#20986;&#21160;&#24577;&#20195;&#29702;&#30340;&#24847;&#22270;&#65292;&#36991;&#20813;&#20405;&#20837;&#20182;&#20154;&#39044;&#26399;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#22914;&#20309;&#22312;&#23494;&#38598;&#19988;&#20114;&#21160;&#30340;&#20154;&#32676;&#20013;&#36827;&#34892;&#23433;&#20840;&#21644;&#24847;&#22270;&#24863;&#30693;&#30340;&#26426;&#22120;&#20154;&#23548;&#33322;&#12290;&#19982;&#22823;&#22810;&#25968;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24490;&#29615;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#21033;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#25429;&#25417;&#20195;&#29702;&#20043;&#38388;&#30340;&#24322;&#36136;&#24615;&#20132;&#20114;&#65292;&#20197;&#20415;&#22312;&#31354;&#38388;&#21644;&#26102;&#38388;&#19978;&#30830;&#23450;&#26368;&#20248;&#31574;&#30053;&#12290;&#25105;&#20204;&#26681;&#25454;&#26410;&#26469;&#22810;&#20010;&#26102;&#38388;&#27493;&#38271;&#30340;&#36712;&#36857;&#39044;&#27979;&#26469;&#25512;&#26029;&#21160;&#24577;&#20195;&#29702;&#30340;&#24847;&#22270;&#65292;&#24182;&#23558;&#36825;&#20123;&#39044;&#27979;&#32467;&#26524;&#32435;&#20837;&#19968;&#20010;&#26080;&#27169;&#22411;RL&#26694;&#26550;&#20013;&#65292;&#20197;&#36991;&#20813;&#26426;&#22120;&#20154;&#20405;&#20837;&#20182;&#20154;&#30340;&#39044;&#26399;&#36335;&#24452;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#26426;&#22120;&#20154;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20154;&#32676;&#23548;&#33322;&#22330;&#26223;&#19979;&#33021;&#22815;&#20197;&#39640;&#25928;&#12289;&#23433;&#20840;&#21644;&#19981;&#24178;&#25200;&#30340;&#26041;&#24335;&#23436;&#25104;&#20219;&#21153;&#12290;&#25105;&#20204;&#25104;&#21151;&#22320;&#23558;&#22312;&#20223;&#30495;&#20013;&#23398;&#20064;&#30340;&#31574;&#30053;&#36716;&#31227;&#21040;&#20102;&#30495;&#23454;&#26426;&#22120;&#20154;&#29615;&#22659;&#20013;&#65292;&#24182;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of safe and intention-aware robot navigation in dense and interactive crowds. Most previous reinforcement learning (RL) based methods fail to consider different types of interactions among all agents or ignore the intentions of people, which results in performance degradation. To learn a safe and efficient robot policy, we propose a novel recurrent graph neural network with attention mechanisms to capture heterogeneous interactions among agents through space and time. To encourage longsighted robot behaviors, we infer the intentions of dynamic agents by predicting their future trajectories for several timesteps. The predictions are incorporated into a model-free RL framework to prevent the robot from intruding into the intended paths of other agents. We demonstrate that our method enables the robot to achieve good navigation performance and non-invasiveness in challenging crowd navigation scenarios. We successfully transfer the policy learned in simulation to a rea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;APEx&#8221;&#30340;&#25216;&#33021;&#21457;&#29616;&#21644;&#36716;&#31227;&#31639;&#27861;&#65292;&#23427;&#20197;&#19968;&#31181;&#21407;&#21017;&#24615;&#21644;&#23398;&#20064;&#30340;&#26041;&#24335;&#21033;&#29992;&#20449;&#24687;&#19981;&#23545;&#31216;&#24615;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#37319;&#26679;&#25928;&#29575;&#12289;&#20219;&#21153;&#21487;&#36716;&#31227;&#24615;&#21644;&#27010;&#25324;&#33021;&#21147;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#38750;&#20998;&#23618;&#26041;&#27861;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#28789;&#27963;&#24615;&#12290;</title><link>http://arxiv.org/abs/2201.08115</link><description>&lt;p&gt;
&#20808;&#39564;&#12289;&#23618;&#27425;&#21644;&#20449;&#24687;&#19981;&#23545;&#31216;&#22312;&#24378;&#21270;&#23398;&#20064;&#25216;&#33021;&#36716;&#31227;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Priors, Hierarchy, and Information Asymmetry for Skill Transfer in Reinforcement Learning. (arXiv:2201.08115v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.08115
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;APEx&#8221;&#30340;&#25216;&#33021;&#21457;&#29616;&#21644;&#36716;&#31227;&#31639;&#27861;&#65292;&#23427;&#20197;&#19968;&#31181;&#21407;&#21017;&#24615;&#21644;&#23398;&#20064;&#30340;&#26041;&#24335;&#21033;&#29992;&#20449;&#24687;&#19981;&#23545;&#31216;&#24615;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#37319;&#26679;&#25928;&#29575;&#12289;&#20219;&#21153;&#21487;&#36716;&#31227;&#24615;&#21644;&#27010;&#25324;&#33021;&#21147;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#38750;&#20998;&#23618;&#26041;&#27861;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#36807;&#21435;&#30340;&#32463;&#39564;&#20013;&#21457;&#29616;&#34892;&#20026;&#65292;&#24182;&#23558;&#20854;&#36716;&#31227;&#21040;&#26032;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#26159;&#26234;&#33021;&#20195;&#29702;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#39640;&#25928;&#37319;&#26679;&#30340;&#26631;&#24535;&#12290;&#20026;&#26234;&#33021;&#24378;&#21270;&#23398;&#20064;&#32773;&#35013;&#22791;&#30456;&#21516;&#30340;&#33021;&#21147;&#21487;&#33021;&#23545;&#23427;&#20204;&#22312;&#26426;&#22120;&#20154;&#25216;&#26415;&#19978;&#30340;&#25104;&#21151;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#20998;&#23618;&#21644;KL-&#27491;&#21017;&#21270;&#24378;&#21270;&#23398;&#20064;&#21508;&#33258;&#22312;&#36825;&#26041;&#38754;&#24456;&#26377;&#24076;&#26395;&#65292;&#20294;&#28151;&#21512;&#26041;&#27861;&#21487;&#33021;&#32467;&#21512;&#23427;&#20204;&#21508;&#33258;&#30340;&#20248;&#28857;&#12290;&#36825;&#20123;&#39046;&#22495;&#30340;&#20851;&#38190;&#22312;&#20110;&#21033;&#29992;&#26550;&#26500;&#27169;&#22359;&#20043;&#38388;&#30340;&#20449;&#24687;&#19981;&#23545;&#31216;&#24615;&#26469;&#20559;&#25191;&#23398;&#20064;&#30340;&#25216;&#33021;&#12290;&#34429;&#28982;&#19981;&#23545;&#31216;&#24615;&#36873;&#25321;&#23545;&#21487;&#36716;&#31227;&#24615;&#26377;&#24456;&#22823;&#24433;&#21709;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#22522;&#20110;&#30452;&#35273;&#65292;&#20197;&#19968;&#20010;&#19982;&#39046;&#22495;&#26080;&#20851;&#30340;&#21487;&#33021;&#27425;&#20248;&#30340;&#26041;&#24335;&#36827;&#34892;&#36873;&#25321;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#21644;&#32463;&#39564;&#19978;&#23637;&#31034;&#20102;&#24207;&#21015;&#20219;&#21153;&#20013;&#25216;&#33021;&#30340;&#20851;&#38190;&#34920;&#36798;&#33021;&#21147;-&#21487;&#36716;&#31227;&#24615;&#30340;&#24179;&#34913;&#65292;&#30001;&#20449;&#24687;&#19981;&#23545;&#31216;&#24615;&#25511;&#21046;&#12290;&#22312;&#33719;&#24471;&#36825;&#19968;&#27934;&#35265;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;APEx&#8221;&#30340;&#25216;&#33021;&#21457;&#29616;&#21644;&#36716;&#31227;&#31639;&#27861;&#65292;&#23427;&#20197;&#19968;&#31181;&#21407;&#21017;&#24615;&#21644;&#23398;&#20064;&#30340;&#26041;&#24335;&#21033;&#29992;&#20449;&#24687;&#19981;&#23545;&#31216;&#24615;&#12290;APEx&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#37319;&#26679;&#25928;&#29575;&#12289;&#20219;&#21153;&#21487;&#36716;&#31227;&#24615;&#21644;&#27010;&#25324;&#33021;&#21147;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#38750;&#20998;&#23618;&#26041;&#27861;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to discover behaviours from past experience and transfer them to new tasks is a hallmark of intelligent agents acting sample-efficiently in the real world. Equipping embodied reinforcement learners with the same ability may be crucial for their successful deployment in robotics. While hierarchical and KL-regularized reinforcement learning individually hold promise here, arguably a hybrid approach could combine their respective benefits. Key to these fields is the use of information asymmetry across architectural modules to bias which skills are learnt. While asymmetry choice has a large influence on transferability, existing methods base their choice primarily on intuition in a domain-independent, potentially sub-optimal, manner. In this paper, we theoretically and empirically show the crucial expressivity-transferability trade-off of skills across sequential tasks, controlled by information asymmetry. Given this insight, we introduce Attentive Priors for Expressive and Tra
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35777;&#26126;&#20102;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#23545;&#20110;&#27599;&#20010;&#31639;&#27861;&#37117;&#23384;&#22312;&#19968;&#20010;&#36275;&#22815;&#22823;&#30340;&#25968;&#25454;&#38598;&#22823;&#23567;&#65292;&#36229;&#36807;&#35813;&#22823;&#23567;&#65292;&#26080;&#27861;&#39044;&#27979;&#27450;&#39575;&#32773;&#30340;&#31639;&#27861;&#27010;&#29575;&#19978;&#30028;&#26159;&#35813;&#31639;&#27861;&#30340;&#31639;&#27861;&#27010;&#29575;&#19978;&#30028;&#12290;</title><link>http://arxiv.org/abs/2112.12275</link><description>&lt;p&gt;
&#24418;&#24335;&#29702;&#35770;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#31616;&#21333;&#24615;&#27873;&#27819;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
A Simplicity Bubble Problem in Formal-Theoretic Learning Systems. (arXiv:2112.12275v2 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.12275
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35777;&#26126;&#20102;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#23545;&#20110;&#27599;&#20010;&#31639;&#27861;&#37117;&#23384;&#22312;&#19968;&#20010;&#36275;&#22815;&#22823;&#30340;&#25968;&#25454;&#38598;&#22823;&#23567;&#65292;&#36229;&#36807;&#35813;&#22823;&#23567;&#65292;&#26080;&#27861;&#39044;&#27979;&#27450;&#39575;&#32773;&#30340;&#31639;&#27861;&#27010;&#29575;&#19978;&#30028;&#26159;&#35813;&#31639;&#27861;&#30340;&#31639;&#27861;&#27010;&#29575;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#25366;&#25496;&#22823;&#22411;&#25968;&#25454;&#38598;&#20197;&#39044;&#27979;&#26032;&#25968;&#25454;&#26102;&#65292;&#32479;&#35745;&#26426;&#22120;&#23398;&#20064;&#32972;&#21518;&#30340;&#21407;&#21017;&#38480;&#21046;&#19981;&#20165;&#23545;&#22823;&#25968;&#25454;&#27946;&#27969;&#26500;&#25104;&#20005;&#37325;&#25361;&#25112;&#65292;&#36824;&#23545;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20559;&#21521;&#20302;&#31639;&#27861;&#22797;&#26434;&#24230;&#30340;&#20256;&#32479;&#20551;&#35774;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#21363;&#20351;&#20551;&#35774;&#26377;&#38480;&#30340;&#25968;&#25454;&#38598;&#29983;&#25104;&#22120;&#23384;&#22312;&#24213;&#23618;&#30340;&#31639;&#27861;&#20449;&#24687;&#20559;&#22909;&#65292;&#25105;&#20204;&#20063;&#35777;&#26126;&#20102;&#24403;&#21069;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65288;&#21253;&#25324;&#28145;&#24230;&#23398;&#20064;&#25110;&#20219;&#20309;&#33258;&#19978;&#32780;&#19979;&#20154;&#24037;&#26234;&#33021;&#21644;&#32479;&#35745;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#28151;&#21512;&#65289;&#24635;&#26159;&#21487;&#20197;&#34987;&#36275;&#22815;&#22823;&#30340;&#25968;&#25454;&#38598;&#33258;&#28982;&#22320;&#25110;&#20154;&#20026;&#22320;&#27450;&#39575;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#27599;&#20010;&#23398;&#20064;&#31639;&#27861;&#65288;&#26080;&#35770;&#26159;&#21542;&#26377;&#24418;&#24335;&#21270;&#29702;&#35770;&#30340;&#25903;&#25345;&#65289;&#65292;&#37117;&#23384;&#22312;&#19968;&#20010;&#36275;&#22815;&#22823;&#30340;&#25968;&#25454;&#38598;&#22823;&#23567;&#65292;&#36229;&#36807;&#35813;&#22823;&#23567;&#65292;&#26080;&#27861;&#39044;&#27979;&#27450;&#39575;&#32773;&#30340;&#31639;&#27861;&#27010;&#29575;&#19978;&#30028;&#26159;&#35813;&#31639;&#27861;&#30340;&#31639;&#27861;&#27010;&#29575;&#19978;&#30028;&#65288;&#20056;&#20197;&#20165;&#20381;&#36182;&#20110;&#23398;&#20064;&#31639;&#27861;&#30340;&#19968;&#20010;&#20056;&#27861;&#24120;&#25968;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
When mining large datasets in order to predict new data, limitations of the principles behind statistical machine learning pose a serious challenge not only to the Big Data deluge, but also to the traditional assumptions that data generating processes are biased toward low algorithmic complexity. Even when one assumes an underlying algorithmic-informational bias toward simplicity in finite dataset generators, we show that current approaches to machine learning (including deep learning, or any formal-theoretic hybrid mix of top-down AI and statistical machine learning approaches), can always be deceived, naturally or artificially, by sufficiently large datasets. In particular, we demonstrate that, for every learning algorithm (with or without access to a formal theory), there is a sufficiently large dataset size above which the algorithmic probability of an unpredictable deceiver is an upper bound (up to a multiplicative constant that only depends on the learning algorithm) for the algo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#20998;&#25955;&#32852;&#37030;&#36793;&#32536;&#23398;&#20064;&#65288;SD-FEEL&#65289;&#26694;&#26550;&#65292;&#37319;&#29992;&#22810;&#20010;&#36793;&#32536;&#26381;&#21153;&#22120;&#21327;&#35843;&#22823;&#37327;&#23458;&#25143;&#31471;&#33410;&#28857;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#26377;&#25928;&#21033;&#29992;&#36793;&#32536;&#35745;&#31639;&#36164;&#28304;&#20197;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#37327;&#21644;&#38477;&#20302;&#24310;&#36831;&#12290;</title><link>http://arxiv.org/abs/2112.10313</link><description>&lt;p&gt;
&#24102;&#26377;&#25968;&#25454;&#21644;&#35774;&#22791;&#24322;&#26500;&#24615;&#30340;&#21322;&#20998;&#25955;&#32852;&#37030;&#36793;&#32536;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Semi-Decentralized Federated Edge Learning with Data and Device Heterogeneity. (arXiv:2112.10313v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.10313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#20998;&#25955;&#32852;&#37030;&#36793;&#32536;&#23398;&#20064;&#65288;SD-FEEL&#65289;&#26694;&#26550;&#65292;&#37319;&#29992;&#22810;&#20010;&#36793;&#32536;&#26381;&#21153;&#22120;&#21327;&#35843;&#22823;&#37327;&#23458;&#25143;&#31471;&#33410;&#28857;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#26377;&#25928;&#21033;&#29992;&#36793;&#32536;&#35745;&#31639;&#36164;&#28304;&#20197;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#37327;&#21644;&#38477;&#20302;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#36793;&#32536;&#23398;&#20064;&#65288;FEEL&#65289;&#20197;&#20445;&#25252;&#38544;&#31169;&#30340;&#26041;&#24335;&#26377;&#25928;&#22320;&#21033;&#29992;&#32593;&#32476;&#36793;&#32536;&#30340;&#20998;&#24067;&#24335;&#25968;&#25454;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#22791;&#21463;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#21333;&#20010;&#36793;&#32536;&#26381;&#21153;&#22120;&#30340;&#35206;&#30422;&#33539;&#22260;&#26377;&#38480;&#65292;&#23548;&#33268;&#21442;&#19982;&#23458;&#25143;&#31471;&#33410;&#28857;&#25968;&#37327;&#19981;&#36275;&#65292;&#21487;&#33021;&#24433;&#21709;&#23398;&#20064;&#24615;&#33021;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#30340;FEEL&#26694;&#26550;&#65292;&#21363;&#21322;&#20998;&#25955;&#32852;&#37030;&#36793;&#32536;&#23398;&#20064;&#65288;SD-FEEL&#65289;&#65292;&#20854;&#20013;&#22810;&#20010;&#36793;&#32536;&#26381;&#21153;&#22120;&#20849;&#21516;&#21327;&#35843;&#22823;&#37327;&#23458;&#25143;&#31471;&#33410;&#28857;&#12290;&#36890;&#36807;&#21033;&#29992;&#36793;&#32536;&#26381;&#21153;&#22120;&#20043;&#38388;&#30340;&#20302;&#24310;&#36831;&#36890;&#20449;&#23454;&#29616;&#39640;&#25928;&#30340;&#27169;&#22411;&#20849;&#20139;&#65292;SD-FEEL&#21487;&#20197;&#25972;&#21512;&#26356;&#22810;&#35757;&#32451;&#25968;&#25454;&#65292;&#21516;&#26102;&#19982;&#20256;&#32479;&#32852;&#37030;&#23398;&#20064;&#30456;&#27604;&#20855;&#26377;&#26356;&#20302;&#30340;&#24310;&#36831;&#12290;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;SD-FEEL&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#26412;&#22320;&#27169;&#22411;&#26356;&#26032;&#12289;&#31751;&#20869;&#21644;&#31751;&#38388;&#27169;&#22411;&#32858;&#21512;&#31561;&#19977;&#20010;&#20027;&#35201;&#27493;&#39588;&#12290;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#22312;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#25968;&#25454;&#19978;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated edge learning (FEEL) has attracted much attention as a privacy-preserving paradigm to effectively incorporate the distributed data at the network edge for training deep learning models. Nevertheless, the limited coverage of a single edge server results in an insufficient number of participated client nodes, which may impair the learning performance. In this paper, we investigate a novel framework of FEEL, namely semi-decentralized federated edge learning (SD-FEEL), where multiple edge servers are employed to collectively coordinate a large number of client nodes. By exploiting the low-latency communication among edge servers for efficient model sharing, SD-FEEL can incorporate more training data, while enjoying much lower latency compared with conventional federated learning. We detail the training algorithm for SD-FEEL with three main steps, including local model update, intra-cluster, and inter-cluster model aggregations. The convergence of this algorithm is proved on non-i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#26435;&#22810;&#35270;&#22270; NMF&#31639;&#27861;&#65292;&#26088;&#22312;&#23398;&#20064;&#35270;&#22270;&#29305;&#23450;&#26435;&#37325;&#21644;&#35266;&#27979;&#29305;&#23450;&#37325;&#26500;&#26435;&#37325;&#65292;&#20197;&#37327;&#21270;&#27599;&#20010;&#35270;&#22270;&#30340;&#20449;&#24687;&#20869;&#23481;&#65292;&#36890;&#36807;&#20998;&#37197;&#36739;&#23567;&#21644;&#36739;&#22823;&#30340;&#26435;&#37325;&#26469;&#25193;&#22823;&#37325;&#35201;&#35270;&#22270;&#30340;&#27491;&#38754;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2110.13240</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#21152;&#26435;&#22810;&#35270;&#22270;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Adaptive Weighted Multi-View Clustering. (arXiv:2110.13240v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.13240
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#26435;&#22810;&#35270;&#22270; NMF&#31639;&#27861;&#65292;&#26088;&#22312;&#23398;&#20064;&#35270;&#22270;&#29305;&#23450;&#26435;&#37325;&#21644;&#35266;&#27979;&#29305;&#23450;&#37325;&#26500;&#26435;&#37325;&#65292;&#20197;&#37327;&#21270;&#27599;&#20010;&#35270;&#22270;&#30340;&#20449;&#24687;&#20869;&#23481;&#65292;&#36890;&#36807;&#20998;&#37197;&#36739;&#23567;&#21644;&#36739;&#22823;&#30340;&#26435;&#37325;&#26469;&#25193;&#22823;&#37325;&#35201;&#35270;&#22270;&#30340;&#27491;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#22810;&#35270;&#22270;&#25968;&#25454;&#26159;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20013;&#19968;&#20010;&#26032;&#20852;&#30340;&#38382;&#39064;&#65292;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#65288;NMF&#65289;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#38477;&#32500;&#26041;&#27861;&#65292;&#29992;&#20110;&#25972;&#21512;&#26469;&#33258;&#22810;&#20010;&#35270;&#22270;&#30340;&#20449;&#24687;&#12290;&#36825;&#20123;&#35270;&#22270;&#36890;&#24120;&#19981;&#20165;&#25552;&#20379;&#19968;&#33268;&#24615;&#20449;&#24687;&#65292;&#36824;&#25552;&#20379;&#20114;&#34917;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#22810;&#35270;&#22270;NMF&#31639;&#27861;&#23558;&#27599;&#20010;&#35270;&#22270;&#36171;&#20104;&#30456;&#21516;&#30340;&#26435;&#37325;&#65292;&#25110;&#32773;&#36890;&#36807;&#32463;&#39564;&#24615;&#30340;&#32447;&#24615;&#25628;&#32034;&#35843;&#25972;&#26435;&#37325;&#65292;&#36825;&#22312;&#27809;&#26377;&#35270;&#22270;&#30340;&#20219;&#20309;&#20808;&#39564;&#30693;&#35782;&#25110;&#35745;&#31639;&#19978;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#26435;&#22810;&#35270;&#22270;NMF&#65288;WM-NMF&#65289;&#31639;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#26088;&#22312;&#35299;&#20915;&#20851;&#38190;&#25216;&#26415;&#24046;&#36317;&#65292;&#21363;&#23398;&#20064;&#35270;&#22270;&#29305;&#23450;&#26435;&#37325;&#21644;&#35266;&#27979;&#29305;&#23450;&#37325;&#26500;&#26435;&#37325;&#65292;&#20197;&#37327;&#21270;&#27599;&#20010;&#35270;&#22270;&#30340;&#20449;&#24687;&#20869;&#23481;&#12290;&#24341;&#20837;&#30340;&#21152;&#26435;&#26041;&#26696;&#21487;&#20197;&#36890;&#36807;&#20998;&#37197;&#36739;&#23567;&#21644;&#36739;&#22823;&#30340;&#26435;&#37325;&#26469;&#20943;&#36731;&#19981;&#24517;&#35201;&#35270;&#22270;&#30340;&#36127;&#38754;&#24433;&#21709;&#24182;&#25193;&#22823;&#37325;&#35201;&#35270;&#22270;&#30340;&#27491;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning multi-view data is an emerging problem in machine learning research, and nonnegative matrix factorization (NMF) is a popular dimensionality-reduction method for integrating information from multiple views. These views often provide not only consensus but also complementary information. However, most multi-view NMF algorithms assign equal weight to each view or tune the weight via line search empirically, which can be infeasible without any prior knowledge of the views or computationally expensive. In this paper, we propose a weighted multi-view NMF (WM-NMF) algorithm. In particular, we aim to address the critical technical gap, which is to learn both view-specific weight and observation-specific reconstruction weight to quantify each view's information content. The introduced weighting scheme can alleviate unnecessary views' adverse effects and enlarge the positive effects of the important views by assigning smaller and larger weights, respectively. Experimental results confir
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#22810;&#23545;&#35937;&#23548;&#33322;&#20013;&#24341;&#20837;&#36741;&#21161;&#20219;&#21153;&#30340;&#34917;&#20805;&#30417;&#30563;&#24418;&#24335;&#65292;&#20197;&#20419;&#36827;&#20026;&#23454;&#29616;&#30446;&#26631;&#23548;&#21521;&#19979;&#28216;&#20219;&#21153;&#32780;&#35757;&#32451;&#30340;&#26234;&#33021;&#20307;&#20013;&#20986;&#29616;&#31354;&#38388;&#24863;&#30693;&#33021;&#21147;&#30340;&#20135;&#29983;&#12290;</title><link>http://arxiv.org/abs/2107.06011</link><description>&lt;p&gt;
&#25945;&#25480;&#26234;&#33021;&#20307;&#22914;&#20309;&#21046;&#22270;&#65306;&#29992;&#20110;&#22810;&#23545;&#35937;&#23548;&#33322;&#30340;&#31354;&#38388;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Teaching Agents how to Map: Spatial Reasoning for Multi-Object Navigation. (arXiv:2107.06011v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.06011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#22810;&#23545;&#35937;&#23548;&#33322;&#20013;&#24341;&#20837;&#36741;&#21161;&#20219;&#21153;&#30340;&#34917;&#20805;&#30417;&#30563;&#24418;&#24335;&#65292;&#20197;&#20419;&#36827;&#20026;&#23454;&#29616;&#30446;&#26631;&#23548;&#21521;&#19979;&#28216;&#20219;&#21153;&#32780;&#35757;&#32451;&#30340;&#26234;&#33021;&#20307;&#20013;&#20986;&#29616;&#31354;&#38388;&#24863;&#30693;&#33021;&#21147;&#30340;&#20135;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35270;&#35273;&#23548;&#33322;&#30340;&#32972;&#26223;&#19979;&#65292;&#26144;&#23556;&#26032;&#39062;&#29615;&#22659;&#30340;&#33021;&#21147;&#23545;&#20110;&#26234;&#33021;&#20307;&#21033;&#29992;&#25152;&#35266;&#23519;&#21040;&#30340;&#21382;&#21490;&#20449;&#24687;&#65292;&#39640;&#25928;&#22320;&#21040;&#36798;&#24050;&#30693;&#30446;&#26631;&#26159;&#24517;&#35201;&#30340;&#12290;&#36825;&#31181;&#33021;&#21147;&#19982;&#31354;&#38388;&#25512;&#29702;&#26377;&#20851;&#65292;&#26234;&#33021;&#20307;&#33021;&#22815;&#24863;&#30693;&#31354;&#38388;&#20851;&#31995;&#21644;&#35268;&#24459;&#65292;&#21457;&#29616;&#29289;&#20307;&#29305;&#24449;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#24341;&#20837;&#20102;&#21487;&#23398;&#20064;&#30340;&#65292;&#30001;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#30340;&#25919;&#31574;&#65292;&#24182;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#35774;&#32622;&#20013;&#65292;&#31354;&#38388;&#21046;&#22270;&#21644;&#25512;&#29702;&#30340;&#33021;&#21147;&#20165;&#20165;&#20174;&#22870;&#21169;&#20013;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#23398;&#20064;&#12290;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36741;&#21161;&#20219;&#21153;&#30340;&#34917;&#20805;&#30417;&#30563;&#24418;&#24335;&#65292;&#26088;&#22312;&#20419;&#36827;&#20026;&#23454;&#29616;&#30446;&#26631;&#23548;&#21521;&#19979;&#28216;&#20219;&#21153;&#32780;&#35757;&#32451;&#30340;&#26234;&#33021;&#20307;&#20013;&#20986;&#29616;&#31354;&#38388;&#24863;&#30693;&#33021;&#21147;&#30340;&#20135;&#29983;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#23398;&#20064;&#20272;&#35745;&#19968;&#20010;&#26234;&#33021;&#20307;&#19982;&#36798;&#21040;&#30446;&#26631;&#20043;&#38388;&#30340;&#31354;&#38388;&#20851;&#31995;&#25351;&#26631;&#26041;&#38754;&#65292;&#22312;&#22810;&#23545;&#35937;&#23548;&#33322;&#20013;&#20855;&#26377;&#39640;&#24230;&#31215;&#26497;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the context of visual navigation, the capacity to map a novel environment is necessary for an agent to exploit its observation history in the considered place and efficiently reach known goals. This ability can be associated with spatial reasoning, where an agent is able to perceive spatial relationships and regularities, and discover object characteristics. Recent work introduces learnable policies parametrized by deep neural networks and trained with Reinforcement Learning (RL). In classical RL setups, the capacity to map and reason spatially is learned end-to-end, from reward alone. In this setting, we introduce supplementary supervision in the form of auxiliary tasks designed to favor the emergence of spatial perception capabilities in agents trained for a goal-reaching downstream objective. We show that learning to estimate metrics quantifying the spatial relationships between an agent at a given location and a goal to reach has a high positive impact in Multi-Object Navigation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#22312;&#20915;&#31574;&#30456;&#20851;&#20998;&#24067;&#23398;&#20064;&#20013;&#30340;&#39118;&#38505;&#26368;&#23567;&#21270;&#65292;&#24314;&#31435;&#22312;&#25191;&#34892;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#26799;&#24230;&#27969;&#30340;&#25200;&#21160;&#36712;&#36857;&#20043;&#19978;&#65292;&#30740;&#31350;&#20102;&#21487;&#33021;&#23384;&#22312;&#22810;&#20010;&#23616;&#37096;&#26368;&#23567;&#21270;&#22120;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2107.00055</link><description>&lt;p&gt;
&#24102;&#26377;&#20915;&#31574;&#30456;&#20851;&#20998;&#24067;&#30340;&#23398;&#20064;&#20013;&#30340;&#36817;&#20284;&#21560;&#24341;&#23376;&#21306;&#22495;
&lt;/p&gt;
&lt;p&gt;
Approximate Regions of Attraction in Learning with Decision-Dependent Distributions. (arXiv:2107.00055v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.00055
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#22312;&#20915;&#31574;&#30456;&#20851;&#20998;&#24067;&#23398;&#20064;&#20013;&#30340;&#39118;&#38505;&#26368;&#23567;&#21270;&#65292;&#24314;&#31435;&#22312;&#25191;&#34892;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#26799;&#24230;&#27969;&#30340;&#25200;&#21160;&#36712;&#36857;&#20043;&#19978;&#65292;&#30740;&#31350;&#20102;&#21487;&#33021;&#23384;&#22312;&#22810;&#20010;&#23616;&#37096;&#26368;&#23567;&#21270;&#22120;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#24212;&#29992;&#65292;&#29983;&#25104;&#35266;&#23519;&#25968;&#25454;&#30340;&#36807;&#31243;&#24448;&#24448;&#20250;&#26681;&#25454;&#23398;&#20064;&#32773;&#30340;&#20915;&#31574;&#20570;&#20986;&#21453;&#24212;&#12290;&#20363;&#22914;&#65292;&#25968;&#25454;&#28304;&#21487;&#33021;&#26377;&#19968;&#20123;&#28608;&#21169;&#35753;&#31639;&#27861;&#25552;&#20379;&#29305;&#23450;&#30340;&#26631;&#31614;&#65288;&#22914;&#25209;&#20934;&#38134;&#34892;&#36151;&#27454;&#65289;&#65292;&#24182;&#30456;&#24212;&#22320;&#25805;&#32437;&#23427;&#20204;&#30340;&#29305;&#24449;&#12290;&#22312;&#25112;&#30053;&#20998;&#31867;&#21644;&#20915;&#31574;&#30456;&#20851;&#20998;&#24067;&#30340;&#24037;&#20316;&#20013;&#65292;&#36890;&#36807;&#26126;&#30830;&#32771;&#34385;&#20998;&#31867;&#22120;&#23545;&#22522;&#30784;&#25968;&#25454;&#20998;&#24067;&#30340;&#24433;&#21709;&#26469;&#34920;&#24449;&#37096;&#32626;&#23398;&#20064;&#31639;&#27861;&#30340;&#38381;&#29615;&#34892;&#20026;&#12290;&#26368;&#36817;&#65292;&#22312;&#25191;&#34892;&#39044;&#27979;&#30340;&#30740;&#31350;&#20013;&#65292;&#32771;&#34385;&#20998;&#31867;&#22120;&#21040;&#25968;&#25454;&#20998;&#24067;&#30340;&#26144;&#23556;&#30340;&#19968;&#33324;&#23646;&#24615;&#65292;&#32780;&#38750;&#26174;&#24335;&#24418;&#24335;&#26469;&#20998;&#31867;&#38381;&#29615;&#34892;&#20026;&#12290;&#22522;&#20110;&#36825;&#20010;&#27010;&#24565;&#65292;&#25105;&#20204;&#23558;&#37325;&#22797;&#30340;&#39118;&#38505;&#26368;&#23567;&#21270;&#20998;&#26512;&#20026;&#25191;&#34892;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#26799;&#24230;&#27969;&#30340;&#25200;&#21160;&#36712;&#36857;&#12290;&#25105;&#20204;&#32771;&#34385;&#21487;&#33021;&#23384;&#22312;&#22810;&#20010;&#23616;&#37096;&#26368;&#23567;&#21270;&#22120;&#30340;&#22330;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
As data-driven methods are deployed in real-world settings, the processes that generate the observed data will often react to the decisions of the learner. For example, a data source may have some incentive for the algorithm to provide a particular label (e.g. approve a bank loan), and manipulate their features accordingly. Work in strategic classification and decision-dependent distributions seeks to characterize the closed-loop behavior of deploying learning algorithms by explicitly considering the effect of the classifier on the underlying data distribution. More recently, works in performative prediction seek to classify the closed-loop behavior by considering general properties of the mapping from classifier to data distribution, rather than an explicit form. Building on this notion, we analyze repeated risk minimization as the perturbed trajectories of the gradient flows of performative risk minimization. We consider the case where there may be multiple local minimizers of perfor
&lt;/p&gt;</description></item><item><title>HUMAP&#26159;&#19968;&#31181;&#26032;&#30340;&#23618;&#27425;&#38477;&#32500;&#25216;&#26415;&#65292;&#33021;&#22815;&#22312;&#23618;&#27425;&#25506;&#32034;&#20013;&#20445;&#30041;&#24515;&#29702;&#22320;&#22270;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#25968;&#25454;&#31867;&#22411;&#19978;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2106.07718</link><description>&lt;p&gt;
HUMAP&#65306;&#23618;&#27425;&#32479;&#19968;&#27969;&#24418;&#36924;&#36817;&#19982;&#25237;&#24433;
&lt;/p&gt;
&lt;p&gt;
HUMAP: Hierarchical Uniform Manifold Approximation and Projection. (arXiv:2106.07718v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.07718
&lt;/p&gt;
&lt;p&gt;
HUMAP&#26159;&#19968;&#31181;&#26032;&#30340;&#23618;&#27425;&#38477;&#32500;&#25216;&#26415;&#65292;&#33021;&#22815;&#22312;&#23618;&#27425;&#25506;&#32034;&#20013;&#20445;&#30041;&#24515;&#29702;&#22320;&#22270;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#25968;&#25454;&#31867;&#22411;&#19978;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38477;&#32500;&#25216;&#26415;&#26377;&#21161;&#20110;&#20998;&#26512;&#20154;&#21592;&#29702;&#35299;&#39640;&#32500;&#31354;&#38388;&#20013;&#30340;&#27169;&#24335;&#12290;&#36825;&#20123;&#25216;&#26415;&#36890;&#24120;&#20197;&#25955;&#28857;&#22270;&#24418;&#24335;&#21576;&#29616;&#65292;&#24212;&#29992;&#20110;&#21508;&#31181;&#31185;&#23398;&#39046;&#22495;&#65292;&#24182;&#20419;&#36827;&#38598;&#32676;&#21644;&#25968;&#25454;&#26679;&#26412;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#20998;&#26512;&#12290;&#38024;&#23545;&#21253;&#21547;&#35768;&#22810;&#31890;&#24230;&#25110;&#36981;&#24490;&#20449;&#24687;&#21487;&#35270;&#21270;&#20934;&#21017;&#30340;&#25968;&#25454;&#38598;&#30340;&#20998;&#26512;&#65292;&#23618;&#27425;&#38477;&#32500;&#25216;&#26415;&#26159;&#26368;&#36866;&#21512;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#20204;&#20808;&#21069;&#21576;&#29616;&#20102;&#20027;&#35201;&#32467;&#26500;&#24182;&#21487;&#20197;&#25353;&#38656;&#25552;&#20379;&#35814;&#32454;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#23618;&#27425;&#38477;&#32500;&#25216;&#26415;&#24182;&#19981;&#33021;&#23436;&#20840;&#35299;&#20915;&#25991;&#29486;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#19981;&#33021;&#22312;&#23618;&#27425;&#32423;&#21035;&#20043;&#38388;&#20445;&#25345;&#25237;&#24433;&#24515;&#29702;&#22320;&#22270;&#65292;&#20063;&#19981;&#36866;&#29992;&#20110;&#22823;&#22810;&#25968;&#25968;&#25454;&#31867;&#22411;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#23618;&#27425;&#38477;&#32500;&#25216;&#26415;HUMAP&#65292;&#26088;&#22312;&#28789;&#27963;&#22320;&#20445;&#30041;&#26412;&#22320;&#21644;&#20840;&#23616;&#32467;&#26500;&#20197;&#21450;&#24515;&#29702;&#22320;&#22270;&#65292;&#22312;&#23618;&#27425;&#25506;&#32034;&#20013;&#20855;&#26377;&#20248;&#36234;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#25968;&#25454;&#31867;&#22411;&#19978;&#25552;&#20379;&#20102;&#23454;&#35777;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dimensionality reduction (DR) techniques help analysts understand patterns in high-dimensional spaces. These techniques, often represented by scatter plots, are employed in diverse science domains and facilitate similarity analysis among clusters and data samples. For datasets containing many granularities or when analysis follows the information visualization mantra, hierarchical DR techniques are the most suitable approach since they present major structures beforehand and details on demand. However, current hierarchical DR techniques are not fully capable of addressing literature problems because they do not preserve the projection mental map across hierarchical levels or are not suitable for most data types. This work presents HUMAP, a novel hierarchical dimensionality reduction technique designed to be flexible in preserving local and global structures and the mental map throughout hierarchical exploration. We provide empirical evidence of our technique's superiority compared with
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23384;&#20648;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;MAML&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#22788;&#29702;&#23569;&#37327;&#20219;&#21153;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#25345;&#32493;&#23398;&#20064;&#65292;&#21516;&#26102;&#25552;&#20986;&#19968;&#31181;&#36890;&#20449;&#26377;&#25928;&#30340;&#22522;&#20110;&#23384;&#20648;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;MAML&#31639;&#27861;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2106.04911</link><description>&lt;p&gt;
&#22522;&#20110;&#23384;&#20648;&#30340;&#20248;&#21270;&#26041;&#27861;&#29992;&#20110;&#27169;&#22411;&#26080;&#20851;&#20803;&#23398;&#20064;&#21644;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Memory-Based Optimization Methods for Model-Agnostic Meta-Learning and Personalized Federated Learning. (arXiv:2106.04911v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.04911
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23384;&#20648;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;MAML&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#22788;&#29702;&#23569;&#37327;&#20219;&#21153;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#25345;&#32493;&#23398;&#20064;&#65292;&#21516;&#26102;&#25552;&#20986;&#19968;&#31181;&#36890;&#20449;&#26377;&#25928;&#30340;&#22522;&#20110;&#23384;&#20648;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;MAML&#31639;&#27861;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#27169;&#22411;&#26080;&#20851;&#20803;&#23398;&#20064;(MAML)&#24050;&#25104;&#20026;&#27969;&#34892;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;MAML&#30340;&#38543;&#26426;&#20248;&#21270;&#20173;&#28982;&#19981;&#23436;&#21892;&#12290;&#29616;&#26377;&#30340;MAML&#31639;&#27861;&#20381;&#36182;&#20110;&#8220;episode&#8221;&#30340;&#24819;&#27861;&#65292;&#21363;&#25277;&#26679;&#20960;&#20010;&#20219;&#21153;&#21644;&#25968;&#25454;&#28857;&#26469;&#26356;&#26032;&#27599;&#27425;&#36845;&#20195;&#20013;&#30340;&#20803;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31639;&#27861;&#35201;&#20040;&#26080;&#27861;&#20445;&#35777;&#19982;&#24120;&#25968;&#23567;&#25209;&#37327;&#22823;&#23567;&#25910;&#25947;&#65292;&#35201;&#20040;&#38656;&#35201;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#22788;&#29702;&#22823;&#37327;&#20219;&#21153;&#65292;&#36825;&#23545;&#20110;&#25345;&#32493;&#23398;&#20064;&#25110;&#36328;&#35774;&#22791;&#32852;&#37030;&#23398;&#20064;&#26469;&#35828;&#26159;&#19981;&#36866;&#24403;&#30340;&#65292;&#22240;&#20026;&#27599;&#27425;&#36845;&#20195;&#25110;&#27599;&#36718;&#20165;&#26377;&#23569;&#37327;&#20219;&#21153;&#21487;&#29992;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23384;&#20648;&#30340;&#38543;&#26426;&#31639;&#27861;&#65292;&#29992;&#20110;MAML&#65292;&#24182;&#21487;&#20197;&#25910;&#25947;&#21040;&#26080;&#38480;&#23567;&#30340;&#35823;&#24046;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#38656;&#35201;&#27599;&#27425;&#36845;&#20195;&#25277;&#26679;&#24658;&#23450;&#25968;&#37327;&#30340;&#20219;&#21153;&#21644;&#25968;&#25454;&#26679;&#26412;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;&#22330;&#26223;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#22522;&#20110;&#23384;&#20648;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;MAML&#31639;&#27861;&#65292;&#20854;&#20013;&#27599;&#20010;&#23458;&#25143;&#31471;&#37117;&#26377;&#33258;&#24049;&#30340;&#20010;&#24615;&#21270;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#21482;&#38656;&#35201;&#22312;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#20043;&#38388;&#36890;&#20449;&#19968;&#23567;&#37096;&#20998;&#20803;&#21442;&#25968;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#30340;&#36328;&#35774;&#22791;&#32852;&#37030;&#23398;&#20064;&#12290;&#25105;&#20204;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#65292;&#26174;&#31034;&#20986;&#19982;&#29616;&#26377;MAML&#26041;&#27861;&#30456;&#27604;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, model-agnostic meta-learning (MAML) has become a popular research area. However, the stochastic optimization of MAML is still underdeveloped. Existing MAML algorithms rely on the ``episode'' idea by sampling a few tasks and data points to update the meta-model at each iteration. Nonetheless, these algorithms either fail to guarantee convergence with a constant mini-batch size or require processing a large number of tasks at every iteration, which is unsuitable for continual learning or cross-device federated learning where only a small number of tasks are available per iteration or per round. To address these issues, this paper proposes memory-based stochastic algorithms for MAML that converge with vanishing error. The proposed algorithms require sampling a constant number of tasks and data samples per iteration, making them suitable for the continual learning scenario. Moreover, we introduce a communication-efficient memory-based MAML algorithm for personalized federa
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;LP&#26494;&#24347;&#30340;MAP-MRF&#25512;&#29702;&#38382;&#39064;&#65292;&#20351;&#29992;&#32452;&#21512;&#20869;&#20391;Frank-Wolfe&#26041;&#21521;&#65292;&#35813;&#26041;&#27861;&#22312;&#19968;&#20123;&#38382;&#39064;&#31867;&#20013;&#26159;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;LP&#27714;&#35299;&#22120;&#12290;</title><link>http://arxiv.org/abs/2010.09567</link><description>&lt;p&gt;
&#35299;&#20915;MAP-MRF&#38382;&#39064;&#30340;&#26494;&#24347;&#65306;&#32452;&#21512;&#20869;&#20391;Frank-Wolfe&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Solving relaxations of MAP-MRF problems: Combinatorial in-face Frank-Wolfe directions. (arXiv:2010.09567v5 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2010.09567
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;LP&#26494;&#24347;&#30340;MAP-MRF&#25512;&#29702;&#38382;&#39064;&#65292;&#20351;&#29992;&#32452;&#21512;&#20869;&#20391;Frank-Wolfe&#26041;&#21521;&#65292;&#35813;&#26041;&#27861;&#22312;&#19968;&#20123;&#38382;&#39064;&#31867;&#20013;&#26159;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;LP&#27714;&#35299;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#35299;&#20915;LP&#26494;&#24347;&#30340;MAP-MRF&#25512;&#29702;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#26368;&#36817;&#22312;(Swoboda&#65292;Kolmogorov 2019; Kolmogorov&#65292;Pock 2021)&#20013;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;&#20316;&#20026;&#20851;&#38190;&#30340;&#35745;&#31639;&#23376;&#20363;&#31243;&#65292;&#23427;&#20351;&#29992;Frank-Wolfe&#65288;FW&#65289;&#26041;&#27861;&#30340;&#19968;&#20010;&#21464;&#20307;&#65292;&#22312;&#32452;&#21512;&#22810;&#32990;&#20307;&#19978;&#26368;&#23567;&#21270;&#24179;&#28369;&#20984;&#20989;&#25968;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20869;&#20391;Frank-Wolfe&#26041;&#21521;&#30340;&#26377;&#25928;&#23454;&#29616;&#26041;&#24335;&#65292;&#22312;(Freund&#31561;&#20154;2017&#24180;)&#20013;&#20171;&#32461;&#20102;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#30340;&#24212;&#29992;&#12290;&#26356;&#19968;&#33324;&#22320;&#65292;&#25105;&#20204;&#20026;&#32452;&#21512;&#23376;&#38382;&#39064;&#23450;&#20041;&#20102;&#19968;&#20010;&#25277;&#35937;&#25968;&#25454;&#32467;&#26500;&#65292;&#20351;&#20869;&#20391;FW&#26041;&#21521;&#25104;&#20026;&#21487;&#33021;&#65292;&#24182;&#25551;&#36848;&#20102;&#20854;&#22312;&#26641;&#29366;MAP-MRF&#25512;&#29702;&#23376;&#38382;&#39064;&#20013;&#30340;&#29305;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24471;&#21040;&#30340;&#26041;&#27861;&#26159;&#19968;&#20123;&#38382;&#39064;&#31867;&#30340;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;LP&#27714;&#35299;&#22120;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;https://pub.ist.ac.at/~vnk/papers/IN-FACE-FW.html&#20013;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of solving LP relaxations of MAP-MRF inference problems, and in particular the method proposed recently in (Swoboda, Kolmogorov 2019; Kolmogorov, Pock 2021). As a key computational subroutine, it uses a variant of the Frank-Wolfe (FW) method to minimize a smooth convex function over a combinatorial polytope. We propose an efficient implementation of this subproutine based on in-face Frank-Wolfe directions, introduced in (Freund et al. 2017) in a different context. More generally, we define an abstract data structure for a combinatorial subproblem that enables in-face FW directions, and describe its specialization for tree-structured MAP-MRF inference subproblems. Experimental results indicate that the resulting method is the current state-of-art LP solver for some classes of problems. Our code is available at https://pub.ist.ac.at/~vnk/papers/IN-FACE-FW.html.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#32593;&#32476;&#20013;&#30740;&#31350;&#20102;&#20998;&#23618;&#31038;&#21306;&#32467;&#26500;&#65292;&#25506;&#35752;&#20102;&#23450;&#20041;&#23618;&#27425;&#32467;&#26500;&#12289;&#30830;&#23450;&#35777;&#25454;&#21644;&#26377;&#25928;&#26816;&#27979;&#20998;&#23618;&#32467;&#26500;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2009.07196</link><description>&lt;p&gt;
&#32593;&#32476;&#20013;&#30340;&#20998;&#23618;&#31038;&#21306;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hierarchical community structure in networks. (arXiv:2009.07196v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2009.07196
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#32593;&#32476;&#20013;&#30740;&#31350;&#20102;&#20998;&#23618;&#31038;&#21306;&#32467;&#26500;&#65292;&#25506;&#35752;&#20102;&#23450;&#20041;&#23618;&#27425;&#32467;&#26500;&#12289;&#30830;&#23450;&#35777;&#25454;&#21644;&#26377;&#25928;&#26816;&#27979;&#20998;&#23618;&#32467;&#26500;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22359;&#21270;&#21644;&#20998;&#23618;&#31038;&#21306;&#32467;&#26500;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#22797;&#26434;&#31995;&#32479;&#20013;&#26222;&#36941;&#23384;&#22312;&#12290;&#20026;&#20102;&#26816;&#27979;&#21644;&#30740;&#31350;&#36825;&#20123;&#32467;&#26500;&#65292;&#24050;&#32463;&#20570;&#20986;&#20102;&#24456;&#22810;&#21162;&#21147;&#12290;&#22312;&#26816;&#27979;&#27169;&#22359;&#21270;&#26041;&#38754;&#26377;&#37325;&#35201;&#30340;&#29702;&#35770;&#36827;&#23637;&#65292;&#20854;&#20013;&#21253;&#25324;&#20351;&#29992;&#27010;&#29575;&#29983;&#25104;&#27169;&#22411;&#27491;&#24335;&#23450;&#20041;&#31038;&#21306;&#32467;&#26500;&#20197;&#30830;&#23450;&#26816;&#27979;&#38480;&#21046;&#12290;&#26816;&#27979;&#20998;&#23618;&#31038;&#21306;&#32467;&#26500;&#36824;&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#22312;&#32593;&#32476;&#20013;&#23545;&#20998;&#23618;&#31038;&#21306;&#32467;&#26500;&#36827;&#34892;&#20102;&#29702;&#35770;&#30740;&#31350;&#65292;&#36825;&#19968;&#26041;&#38754;&#36804;&#20170;&#23578;&#26410;&#24471;&#21040;&#21516;&#31561;&#20005;&#26684;&#30340;&#20851;&#27880;&#12290;&#25105;&#20204;&#22238;&#31572;&#20102;&#20197;&#19979;&#38382;&#39064;&#65306;1&#65289;&#24212;&#35813;&#22914;&#20309;&#23450;&#20041;&#31038;&#21306;&#30340;&#23618;&#27425;&#32467;&#26500;&#65311;2&#65289;&#22914;&#20309;&#30830;&#23450;&#32593;&#32476;&#20013;&#26159;&#21542;&#26377;&#36275;&#22815;&#30340;&#35777;&#25454;&#34920;&#26126;&#23384;&#22312;&#20998;&#23618;&#32467;&#26500;&#65311;3&#65289;&#22914;&#20309;&#26377;&#25928;&#22320;&#26816;&#27979;&#20998;&#23618;&#32467;&#26500;&#65311;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#38543;&#26426;&#36807;&#31243;&#30340;&#23618;&#27425;&#32467;&#26500;&#23450;&#20041;&#26469;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modular and hierarchical community structures are pervasive in real-world complex systems. A great deal of effort has gone into trying to detect and study these structures. Important theoretical advances in the detection of modular have included identifying fundamental limits of detectability by formally defining community structure using probabilistic generative models. Detecting hierarchical community structure introduces additional challenges alongside those inherited from community detection. Here we present a theoretical study on hierarchical community structure in networks, which has thus far not received the same rigorous attention. We address the following questions: 1) How should we define a hierarchy of communities? 2) How do we determine if there is sufficient evidence of a hierarchical structure in a network? and 3) How can we detect hierarchical structure efficiently? We approach these questions by introducing a definition of hierarchy based on the concept of stochastic ex
&lt;/p&gt;</description></item><item><title>&#36866;&#24230;&#30417;&#30563;&#23398;&#20064;&#21253;&#25324;&#26631;&#20934;&#26377;&#30417;&#30563;&#23398;&#20064;&#21644;&#24369;&#30417;&#30563;&#23398;&#20064;&#65292;&#24050;&#26377;&#30340;&#19968;&#20123;&#26631;&#20934;&#26377;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#34920;&#26126;&#65292;&#32473;&#23450;&#30340;&#26631;&#31614;&#24182;&#19981;&#23481;&#26131;&#23398;&#20064;&#65292;&#20174;&#32473;&#23450;&#30340;&#26631;&#31614;&#36716;&#25442;&#20026;&#26131;&#20110;&#23398;&#20064;&#30340;&#30446;&#26631;&#30340;&#36807;&#31243;&#20250;&#26174;&#33879;&#24433;&#21709;&#26368;&#32456;&#30340;&#24615;&#33021;&#65292;&#26377;&#30417;&#30563;&#23398;&#20064;&#30340;&#23450;&#20041;&#38544;&#34255;&#20102;&#21487;&#33021;&#23545;&#35299;&#20915;&#26041;&#26696;&#24433;&#21709;&#37325;&#22823;&#30340;&#32454;&#33410;&#12290;</title><link>http://arxiv.org/abs/2008.11945</link><description>&lt;p&gt;
&#36866;&#24230;&#30417;&#30563;&#23398;&#20064;&#65306;&#23450;&#20041;&#12289;&#26694;&#26550;&#21644;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Moderately Supervised Learning: Definition, Framework and Generality. (arXiv:2008.11945v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2008.11945
&lt;/p&gt;
&lt;p&gt;
&#36866;&#24230;&#30417;&#30563;&#23398;&#20064;&#21253;&#25324;&#26631;&#20934;&#26377;&#30417;&#30563;&#23398;&#20064;&#21644;&#24369;&#30417;&#30563;&#23398;&#20064;&#65292;&#24050;&#26377;&#30340;&#19968;&#20123;&#26631;&#20934;&#26377;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#34920;&#26126;&#65292;&#32473;&#23450;&#30340;&#26631;&#31614;&#24182;&#19981;&#23481;&#26131;&#23398;&#20064;&#65292;&#20174;&#32473;&#23450;&#30340;&#26631;&#31614;&#36716;&#25442;&#20026;&#26131;&#20110;&#23398;&#20064;&#30340;&#30446;&#26631;&#30340;&#36807;&#31243;&#20250;&#26174;&#33879;&#24433;&#21709;&#26368;&#32456;&#30340;&#24615;&#33021;&#65292;&#26377;&#30417;&#30563;&#23398;&#20064;&#30340;&#23450;&#20041;&#38544;&#34255;&#20102;&#21487;&#33021;&#23545;&#35299;&#20915;&#26041;&#26696;&#24433;&#21709;&#37325;&#22823;&#30340;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36866;&#24230;&#30417;&#30563;&#23398;&#20064;&#21253;&#25324;&#26631;&#20934;&#26377;&#30417;&#30563;&#23398;&#20064;&#21644;&#24369;&#30417;&#30563;&#23398;&#20064;&#65292;&#21069;&#32773;&#35757;&#32451;&#25968;&#25454;&#38598;&#26631;&#31614;&#26159;&#29702;&#24819;&#30340;&#65288;&#23436;&#20840;&#12289;&#31934;&#30830;&#21644;&#20934;&#30830;&#65289;&#65292;&#21518;&#32773;&#35757;&#32451;&#25968;&#25454;&#38598;&#26631;&#31614;&#19981;&#23436;&#32654;&#65288;&#19981;&#23436;&#25972;&#65292;&#19981;&#31934;&#30830;&#25110;&#19981;&#20934;&#30830;&#65289;&#12290;&#32463;&#36807;&#22810;&#27425;&#23454;&#39564;&#65292;&#24050;&#26377;&#30340;&#19968;&#20123;&#26631;&#20934;&#26377;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#30340;&#35299;&#20915;&#26041;&#26696;&#34920;&#26126;&#65292;&#26377;&#26102;&#20505;&#32473;&#23450;&#30340;&#26631;&#31614;&#24182;&#19981;&#23481;&#26131;&#23398;&#20064;&#65292;&#20174;&#32473;&#23450;&#30340;&#26631;&#31614;&#36716;&#25442;&#20026;&#26131;&#20110;&#23398;&#20064;&#30340;&#30446;&#26631;&#30340;&#36807;&#31243;&#20250;&#26174;&#33879;&#24433;&#21709;&#26368;&#32456;&#26631;&#20934;&#26377;&#30417;&#30563;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#22312;&#26410;&#32771;&#34385;&#20174;&#32473;&#23450;&#26631;&#31614;&#21040;&#26131;&#20110;&#23398;&#20064;&#30340;&#30446;&#26631;&#30340;&#36716;&#21270;&#36807;&#31243;&#23646;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#26377;&#30417;&#30563;&#23398;&#20064;&#30340;&#23450;&#20041;&#38544;&#34255;&#20102;&#19968;&#20123;&#21487;&#33021;&#23545;&#24314;&#31435;&#35299;&#20915;&#26041;&#26696;&#24433;&#21709;&#37325;&#22823;&#30340;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning with supervision has achieved remarkable success in numerous artificial intelligence (AI) applications. In the current literature, by referring to the properties of the labels prepared for the training dataset, learning with supervision is categorized as supervised learning (SL) and weakly supervised learning (WSL). SL concerns the situation where the training data set is assigned with ideal (complete, exact and accurate) labels, while WSL concerns the situation where the training data set is assigned with non-ideal (incomplete, inexact or inaccurate) labels. However, various solutions for SL tasks have shown that the given labels are not always easy to learn, and the transformation from the given labels to easy-to-learn targets can significantly affect the performance of the final SL solutions. Without considering the properties of the transformation from the given labels to easy-to-learn targets, the definition of SL conceals some details that can be critical to building the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23884;&#20837;&#20132;&#20114;&#30340;&#35282;&#24230;&#26469;&#20998;&#26512;&#21644;&#27604;&#36739;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;CP&#65292;DistMult&#21644;ComplEx&#31561;&#19977;&#31181;&#27969;&#34892;&#30340;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;ComplEx&#24615;&#33021;&#26356;&#22909;&#30340;&#21407;&#22240;&#65292;&#24182;&#20026;&#24320;&#21457;&#26032;&#27169;&#22411;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/1903.11406</link><description>&lt;p&gt;
&#20174;&#22810;&#23884;&#20837;&#20132;&#20114;&#30340;&#35282;&#24230;&#20998;&#26512;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analyzing Knowledge Graph Embedding Methods from a Multi-Embedding Interaction Perspective. (arXiv:1903.11406v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1903.11406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23884;&#20837;&#20132;&#20114;&#30340;&#35282;&#24230;&#26469;&#20998;&#26512;&#21644;&#27604;&#36739;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;CP&#65292;DistMult&#21644;ComplEx&#31561;&#19977;&#31181;&#27969;&#34892;&#30340;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;ComplEx&#24615;&#33021;&#26356;&#22909;&#30340;&#21407;&#22240;&#65292;&#24182;&#20026;&#24320;&#21457;&#26032;&#27169;&#22411;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#26159;&#19968;&#31181;&#24191;&#27867;&#29992;&#20110;&#34920;&#31034;&#30693;&#35782;&#30340;&#26684;&#24335;&#65292;&#19988;&#22312;&#35821;&#20041;&#25628;&#32034;&#24341;&#25806;&#12289;&#38382;&#31572;&#31995;&#32479;&#21644;&#25512;&#33616;&#31995;&#32479;&#20013;&#26377;&#35768;&#22810;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#30340;&#30693;&#35782;&#22270;&#35889;&#36890;&#24120;&#26159;&#19981;&#23436;&#25972;&#30340;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#22522;&#20110;&#23884;&#20837;&#21521;&#37327;&#26041;&#27861;&#65292;&#22914;CP&#12289;DistMult&#21644;ComplEx&#31561;&#65292;&#20197;&#35299;&#20915;&#27492;&#38382;&#39064;&#12290;&#36825;&#20123;&#26041;&#27861;&#23558;&#23454;&#20307;&#21644;&#20851;&#31995;&#34920;&#31034;&#20026;&#35821;&#20041;&#31354;&#38388;&#20013;&#30340;&#23884;&#20837;&#21521;&#37327;&#65292;&#24182;&#39044;&#27979;&#23427;&#20204;&#20043;&#38388;&#30340;&#38142;&#25509;&#12290;&#23884;&#20837;&#21521;&#37327;&#26412;&#36523;&#21253;&#21547;&#20016;&#23500;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#21487;&#29992;&#20110;&#20854;&#20182;&#24212;&#29992;&#31243;&#24207;&#65292;&#22914;&#25968;&#25454;&#20998;&#26512;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#27169;&#22411;&#20013;&#30340;&#26426;&#21046;&#21644;&#23884;&#20837;&#21521;&#37327;&#26412;&#36523;&#21464;&#21270;&#24456;&#22823;&#65292;&#20351;&#20854;&#38590;&#20197;&#29702;&#35299;&#21644;&#27604;&#36739;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#38656;&#35201;&#20197;&#22810;&#23884;&#20837;&#20132;&#20114;&#30340;&#35282;&#24230;&#26469;&#20998;&#26512;&#21644;&#27604;&#36739;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19977;&#31181;&#27969;&#34892;&#30340;&#26041;&#27861;&#65288;CP&#12289;DistMult&#21644;ComplEx&#65289;&#65292;&#24182;&#30740;&#31350;&#23427;&#20204;&#30340;&#19981;&#21516;&#23884;&#20837;&#21521;&#37327;&#22914;&#20309;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;ComplEx&#24615;&#33021;&#26356;&#22909;&#30340;&#21407;&#22240;&#65292;&#24182;&#20026;&#24320;&#21457;&#26032;&#27169;&#22411;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph is a popular format for representing knowledge, with many applications to semantic search engines, question-answering systems, and recommender systems. Real-world knowledge graphs are usually incomplete, so knowledge graph embedding methods, such as Canonical decomposition/Parallel factorization (CP), DistMult, and ComplEx, have been proposed to address this issue. These methods represent entities and relations as embedding vectors in semantic space and predict the links between them. The embedding vectors themselves contain rich semantic information and can be used in other applications such as data analysis. However, mechanisms in these models and the embedding vectors themselves vary greatly, making it difficult to understand and compare them. Given this lack of understanding, we risk using them ineffectively or incorrectly, particularly for complicated models, such as CP, with two role-based embedding vectors, or the state-of-the-art ComplEx model, with complex-valu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#38598;&#25104;&#37319;&#26679;&#65292;&#20197;&#36817;&#20284;Thompson&#37319;&#26679;&#24182;&#22312;&#22797;&#26434;&#27169;&#22411;&#19979;&#20445;&#25345;&#21487;&#34892;&#24615;&#65292;&#23558;&#22823;&#22823;&#25193;&#23637;Thompson&#37319;&#26679;&#30340;&#24212;&#29992;&#33539;&#22260;&#12290;</title><link>http://arxiv.org/abs/1705.07347</link><description>&lt;p&gt;
&#38598;&#25104;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Ensemble Sampling. (arXiv:1705.07347v4 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1705.07347
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#38598;&#25104;&#37319;&#26679;&#65292;&#20197;&#36817;&#20284;Thompson&#37319;&#26679;&#24182;&#22312;&#22797;&#26434;&#27169;&#22411;&#19979;&#20445;&#25345;&#21487;&#34892;&#24615;&#65292;&#23558;&#22823;&#22823;&#25193;&#23637;Thompson&#37319;&#26679;&#30340;&#24212;&#29992;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Thompson&#37319;&#26679;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#35299;&#20915;&#22810;&#31181;&#22312;&#32447;&#20915;&#31574;&#38382;&#39064;&#30340;&#26377;&#25928;&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;&#22312;&#20854;&#22522;&#26412;&#24418;&#24335;&#20013;&#65292;&#35813;&#31639;&#27861;&#38656;&#35201;&#35745;&#31639;&#24182;&#20174;&#27169;&#22411;&#30340;&#21518;&#39564;&#20998;&#24067;&#20013;&#37319;&#26679;&#65292;&#20165;&#22312;&#31616;&#21333;&#29305;&#27530;&#24773;&#20917;&#19979;&#25165;&#21487;&#34892;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#38598;&#25104;&#37319;&#26679;&#65292;&#26088;&#22312;&#36817;&#20284;Thompson&#37319;&#26679;&#65292;&#21516;&#26102;&#22312;&#22797;&#26434;&#27169;&#22411;&#65288;&#22914;&#31070;&#32463;&#32593;&#32476;&#65289;&#30340;&#38754;&#21069;&#20445;&#25345;&#21487;&#34892;&#24615;&#12290;&#38598;&#25104;&#37319;&#26679;&#22823;&#22823;&#25193;&#23637;&#20102;&#36866;&#29992;Thompson&#37319;&#26679;&#30340;&#24212;&#29992;&#33539;&#22260;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#25903;&#25345;&#35813;&#26041;&#27861;&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#24182;&#25552;&#20379;&#20102;&#36827;&#19968;&#27493;&#30340;&#35745;&#31639;&#32467;&#26524;&#26469;&#25552;&#20379;&#26356;&#22810;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Thompson sampling has emerged as an effective heuristic for a broad range of online decision problems. In its basic form, the algorithm requires computing and sampling from a posterior distribution over models, which is tractable only for simple special cases. This paper develops ensemble sampling, which aims to approximate Thompson sampling while maintaining tractability even in the face of complex models such as neural networks. Ensemble sampling dramatically expands on the range of applications for which Thompson sampling is viable. We establish a theoretical basis that supports the approach and present computational results that offer further insight.
&lt;/p&gt;</description></item></channel></rss>