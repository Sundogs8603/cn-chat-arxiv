<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21160;&#21147;&#23398;&#27169;&#22411;&#26469;&#35299;&#37322;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;&#12290;&#36890;&#36807;&#20998;&#26512;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#65292;&#30740;&#31350;&#21457;&#29616;&#35757;&#32451;&#26102;&#38388;&#21644;&#27169;&#22411;&#22823;&#23567;&#30340;&#32553;&#25918;&#20855;&#26377;&#19981;&#21516;&#30340;&#24130;&#24459;&#25351;&#25968;&#65292;&#32780;&#35745;&#31639;&#26368;&#20248;&#32553;&#25918;&#35268;&#21017;&#35201;&#27714;&#22686;&#21152;&#35757;&#32451;&#27493;&#25968;&#24555;&#20110;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#65292;&#19982;&#23454;&#35777;&#35266;&#23519;&#30456;&#19968;&#33268;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01092</link><description>&lt;p&gt;
&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Dynamical Model of Neural Scaling Laws
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01092
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21160;&#21147;&#23398;&#27169;&#22411;&#26469;&#35299;&#37322;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;&#12290;&#36890;&#36807;&#20998;&#26512;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#65292;&#30740;&#31350;&#21457;&#29616;&#35757;&#32451;&#26102;&#38388;&#21644;&#27169;&#22411;&#22823;&#23567;&#30340;&#32553;&#25918;&#20855;&#26377;&#19981;&#21516;&#30340;&#24130;&#24459;&#25351;&#25968;&#65292;&#32780;&#35745;&#31639;&#26368;&#20248;&#32553;&#25918;&#35268;&#21017;&#35201;&#27714;&#22686;&#21152;&#35757;&#32451;&#27493;&#25968;&#24555;&#20110;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#65292;&#19982;&#23454;&#35777;&#35266;&#23519;&#30456;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#38543;&#30528;&#35757;&#32451;&#26102;&#38388;&#12289;&#25968;&#25454;&#38598;&#22823;&#23567;&#21644;&#27169;&#22411;&#22823;&#23567;&#30340;&#22686;&#21152;&#32780;&#39044;&#27979;&#24615;&#22320;&#25552;&#39640;&#65292;&#36328;&#22810;&#20010;&#25968;&#37327;&#32423;&#12290;&#36825;&#31181;&#29616;&#35937;&#34987;&#31216;&#20026;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#35745;&#31639;&#26368;&#20248;&#32553;&#25918;&#23450;&#24459;&#65292;&#23427;&#25253;&#21578;&#20102;&#22312;&#36873;&#25321;&#26368;&#20339;&#27169;&#22411;&#22823;&#23567;&#26102;&#24615;&#33021;&#19982;&#35745;&#31639;&#25968;&#37327;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#19968;&#20010;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#36827;&#34892;&#35757;&#32451;&#21644;&#27867;&#21270;&#30340;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#20316;&#20026;&#32593;&#32476;&#35757;&#32451;&#21644;&#27867;&#21270;&#30340;&#21487;&#35299;&#27169;&#22411;&#12290;&#36825;&#20010;&#27169;&#22411;&#22797;&#29616;&#20102;&#20851;&#20110;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;&#30340;&#35768;&#22810;&#35266;&#23519;&#32467;&#26524;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23545;&#20110;&#20026;&#20160;&#20040;&#35757;&#32451;&#26102;&#38388;&#21644;&#27169;&#22411;&#22823;&#23567;&#30340;&#32553;&#25918;&#20855;&#26377;&#19981;&#21516;&#30340;&#24130;&#24459;&#25351;&#25968;&#25552;&#20986;&#20102;&#19968;&#20010;&#39044;&#27979;&#12290;&#22240;&#27492;&#65292;&#29702;&#35770;&#39044;&#27979;&#20102;&#19968;&#31181;&#19981;&#23545;&#31216;&#30340;&#35745;&#31639;&#26368;&#20248;&#32553;&#25918;&#35268;&#21017;&#65292;&#20854;&#20013;&#35757;&#32451;&#27493;&#25968;&#30340;&#22686;&#21152;&#36895;&#24230;&#24555;&#20110;&#27169;&#22411;&#21442;&#25968;&#30340;&#22686;&#21152;&#36895;&#24230;&#65292;&#19982;&#26368;&#36817;&#30340;&#23454;&#35777;&#35266;&#23519;&#19968;&#33268;&#12290;&#20854;&#27425;&#65292;&#35266;&#23519;&#21040;&#22312;&#35757;&#32451;&#30340;&#26089;&#26399;&#65292;&#32593;&#32476;&#20250;&#25910;&#25947;&#21040;&#26080;&#38480;&#23485;&#24230;&#24773;&#20917;&#19979;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
On a variety of tasks, the performance of neural networks predictably improves with training time, dataset size and model size across many orders of magnitude. This phenomenon is known as a neural scaling law. Of fundamental importance is the compute-optimal scaling law, which reports the performance as a function of units of compute when choosing model sizes optimally. We analyze a random feature model trained with gradient descent as a solvable model of network training and generalization. This reproduces many observations about neural scaling laws. First, our model makes a prediction about why the scaling of performance with training time and with model size have different power law exponents. Consequently, the theory predicts an asymmetric compute-optimal scaling rule where the number of training steps are increased faster than model parameters, consistent with recent empirical observations. Second, it has been observed that early in training, networks converge to their infinite-wi
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#27969;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#37327;&#23398;&#20064;&#23454;&#29616;&#25345;&#32493;&#27169;&#22411;&#36866;&#24212;&#65292;&#36873;&#25321;&#27599;&#20010;&#31867;&#21035;&#30340;&#20195;&#34920;&#24615;&#22270;&#65292;&#24182;&#21019;&#24314;&#22270;&#23884;&#20837;&#65292;&#20197;&#35299;&#20915;&#22270;&#27969;&#20998;&#31867;&#20013;&#30340;&#27010;&#24565;&#28418;&#31227;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.02572</link><description>&lt;p&gt;
&#20855;&#26377;&#27010;&#24565;&#28418;&#31227;&#26816;&#27979;&#21644;&#22522;&#20110;&#21407;&#22411;&#30340;&#23884;&#20837;&#30340;&#22270;&#27969;&#20998;&#31867;&#30340;&#22686;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Incremental Learning with Concept Drift Detection and Prototype-based Embeddings for Graph Stream Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02572
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#27969;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#37327;&#23398;&#20064;&#23454;&#29616;&#25345;&#32493;&#27169;&#22411;&#36866;&#24212;&#65292;&#36873;&#25321;&#27599;&#20010;&#31867;&#21035;&#30340;&#20195;&#34920;&#24615;&#22270;&#65292;&#24182;&#21019;&#24314;&#22270;&#23884;&#20837;&#65292;&#20197;&#35299;&#20915;&#22270;&#27969;&#20998;&#31867;&#20013;&#30340;&#27010;&#24565;&#28418;&#31227;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#27969;&#25366;&#25496;&#26088;&#22312;&#20174;&#19981;&#26029;&#21464;&#21270;&#30340;&#25968;&#25454;&#27969;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#30693;&#35782;&#65292;&#35299;&#20915;&#38750;&#38745;&#24577;&#29615;&#22659;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#25351;&#38543;&#26102;&#38388;&#25913;&#21464;&#30340;&#22522;&#30784;&#25968;&#25454;&#20998;&#24067;&#30340;&#27010;&#24565;&#28418;&#31227;&#12290;&#22270;&#32467;&#26500;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#24314;&#27169;&#24037;&#20855;&#65292;&#29992;&#20110;&#34920;&#31034;&#22797;&#26434;&#31995;&#32479;&#65292;&#27604;&#22914;&#20851;&#38190;&#22522;&#30784;&#35774;&#26045;&#31995;&#32479;&#21644;&#31038;&#20132;&#32593;&#32476;&#12290;&#20174;&#22270;&#27969;&#20013;&#23398;&#20064;&#21464;&#24471;&#24517;&#19981;&#21487;&#23569;&#65292;&#20197;&#20102;&#35299;&#22270;&#32467;&#26500;&#30340;&#21160;&#24577;&#24182;&#20419;&#36827;&#26126;&#26234;&#20915;&#31574;&#12290;&#26412;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#22270;&#27969;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#22312;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20135;&#29983;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#33410;&#28857;&#21644;&#36793;&#30340;&#22270;&#30340;&#19968;&#33324;&#35774;&#32622;&#19979;&#36816;&#34892;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#22686;&#37327;&#23398;&#20064;&#36827;&#34892;&#25345;&#32493;&#27169;&#22411;&#36866;&#24212;&#65292;&#20026;&#27599;&#20010;&#31867;&#21035;&#36873;&#25321;&#20195;&#34920;&#24615;&#22270;&#65288;&#21407;&#22411;&#65289;&#24182;&#21019;&#24314;&#22270;&#23884;&#20837;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#21253;&#21547;&#22522;&#20110;&#25439;&#22833;&#30340;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02572v1 Announce Type: new  Abstract: Data stream mining aims at extracting meaningful knowledge from continually evolving data streams, addressing the challenges posed by nonstationary environments, particularly, concept drift which refers to a change in the underlying data distribution over time. Graph structures offer a powerful modelling tool to represent complex systems, such as, critical infrastructure systems and social networks. Learning from graph streams becomes a necessity to understand the dynamics of graph structures and to facilitate informed decision-making. This work introduces a novel method for graph stream classification which operates under the general setting where a data generating process produces graphs with varying nodes and edges over time. The method uses incremental learning for continual model adaptation, selecting representative graphs (prototypes) for each class, and creating graph embeddings. Additionally, it incorporates a loss-based concept 
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#22270;&#25968;&#25454;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#27169;&#22359;&#22686;&#24378;&#65292;&#25552;&#20379;&#32622;&#20449;&#24230;&#35780;&#20998;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#30693;&#35782;&#22270;&#23436;&#25104;&#21644;&#22270;&#20998;&#31867;&#20219;&#21153;&#19978;&#36229;&#36234;&#20102;&#26368;&#20808;&#36827;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2404.00589</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#22270;&#25968;&#25454;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Harnessing the Power of Large Language Model for Uncertainty Aware Graph Processing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00589
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#22270;&#25968;&#25454;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#27169;&#22359;&#22686;&#24378;&#65292;&#25552;&#20379;&#32622;&#20449;&#24230;&#35780;&#20998;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#30693;&#35782;&#22270;&#23436;&#25104;&#21644;&#22270;&#20998;&#31867;&#20219;&#21153;&#19978;&#36229;&#36234;&#20102;&#26368;&#20808;&#36827;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#29702;&#22270;&#25968;&#25454;&#26159;&#19968;&#39033;&#38750;&#24120;&#22256;&#38590;&#30340;&#20219;&#21153;&#12290;&#20256;&#32479;&#25216;&#26415;&#65292;&#20363;&#22914;&#22522;&#20110;&#20960;&#20309;&#21644;&#30697;&#38453;&#20998;&#35299;&#30340;&#25216;&#26415;&#65292;&#20381;&#36182;&#20110;&#23545;&#25968;&#25454;&#20851;&#31995;&#30340;&#20551;&#35774;&#65292;&#22312;&#22788;&#29702;&#22823;&#22411;&#21644;&#22797;&#26434;&#30340;&#22270;&#25968;&#25454;&#26102;&#21464;&#24471;&#19981;&#36275;&#22815;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#23637;&#31034;&#20102;&#22788;&#29702;&#22823;&#22411;&#22270;&#25968;&#25454;&#30340;&#33391;&#22909;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#26080;&#27861;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#35299;&#37322;&#12290;&#20026;&#20102;&#20351;&#22270;&#22788;&#29702;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#22686;&#24378;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#27169;&#22359;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#21147;&#37327;&#65292;&#20197;&#25552;&#20379;&#29983;&#25104;&#31572;&#26696;&#30340;&#32622;&#20449;&#24230;&#20998;&#25968;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#22270;&#22788;&#29702;&#20219;&#21153;&#19978;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#23454;&#39564;&#65306;&#23569;&#26679;&#26412;&#30693;&#35782;&#22270;&#23436;&#25104;&#21644;&#22270;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65292;LLM&#22312;&#21508;&#20010;&#26041;&#38754;&#36229;&#36234;&#20102;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00589v1 Announce Type: cross  Abstract: Handling graph data is one of the most difficult tasks. Traditional techniques, such as those based on geometry and matrix factorization, rely on assumptions about the data relations that become inadequate when handling large and complex graph data. On the other hand, deep learning approaches demonstrate promising results in handling large graph data, but they often fall short of providing interpretable explanations. To equip the graph processing with both high accuracy and explainability, we introduce a novel approach that harnesses the power of a large language model (LLM), enhanced by an uncertainty-aware module to provide a confidence score on the generated answer. We experiment with our approach on two graph processing tasks: few-shot knowledge graph completion and graph classification. Our results demonstrate that through parameter efficient fine-tuning, the LLM surpasses state-of-the-art algorithms by a substantial margin across
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#39069;&#22806;&#30340;&#37325;&#24314;&#38454;&#27573;&#21644;&#37325;&#24314;&#25439;&#22833;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20849;&#20139;&#27169;&#22411;&#21442;&#25968;&#21644;&#29305;&#24449;&#34920;&#31034;&#30340;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#65292;&#24314;&#31435;&#20102;&#20849;&#21516;&#20449;&#24687;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#35299;&#20915;&#30456;&#20851;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2404.00505</link><description>&lt;p&gt;
&#20855;&#26377;&#37325;&#24314;&#25439;&#22833;&#30340;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Transfer Learning with Reconstruction Loss
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00505
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#39069;&#22806;&#30340;&#37325;&#24314;&#38454;&#27573;&#21644;&#37325;&#24314;&#25439;&#22833;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20849;&#20139;&#27169;&#22411;&#21442;&#25968;&#21644;&#29305;&#24449;&#34920;&#31034;&#30340;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#65292;&#24314;&#31435;&#20102;&#20849;&#21516;&#20449;&#24687;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#35299;&#20915;&#30456;&#20851;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22810;&#25968;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#25968;&#23398;&#20248;&#21270;&#30340;&#24212;&#29992;&#20013;&#65292;&#36890;&#24120;&#20026;&#27599;&#20010;&#29305;&#23450;&#20248;&#21270;&#30446;&#26631;&#35757;&#32451;&#19968;&#20010;&#19987;&#29992;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#22330;&#26223;&#20013;&#65292;&#21516;&#19968;&#32452;&#38382;&#39064;&#36755;&#20837;&#19978;&#32463;&#24120;&#38656;&#35201;&#20248;&#21270;&#20960;&#20010;&#19981;&#21516;&#20294;&#30456;&#20851;&#30340;&#30446;&#26631;&#25110;&#20219;&#21153;&#12290;&#19982;&#20026;&#27599;&#20010;&#38382;&#39064;&#21333;&#29420;&#35757;&#32451;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#30456;&#27604;&#65292;&#26356;&#26377;&#25928;&#30340;&#26041;&#27861;&#26159;&#21033;&#29992;&#36825;&#20123;&#30446;&#26631;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#20351;&#29992;&#20849;&#20139;&#27169;&#22411;&#21442;&#25968;&#21644;&#29305;&#24449;&#34920;&#31034;&#35757;&#32451;&#22810;&#20010;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#26412;&#25991;&#39318;&#20808;&#24314;&#31435;&#20102;&#20849;&#21516;&#20449;&#24687;&#30340;&#27010;&#24565;&#65306;&#35299;&#20915;&#30456;&#20851;&#20219;&#21153;&#25152;&#38656;&#30340;&#20849;&#20139;&#30693;&#35782;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27169;&#22411;&#20013;&#28155;&#21152;&#19968;&#20010;&#39069;&#22806;&#30340;&#37325;&#24314;&#38454;&#27573;&#20197;&#21450;&#30456;&#20851;&#30340;&#26032;&#37325;&#24314;&#25439;&#22833;&#12290;&#35813;&#25439;&#22833;&#29992;&#20110;&#20174;&#36873;&#25321;&#30340;&#38544;&#34255;&#29366;&#24577;&#24320;&#22987;&#37325;&#26032;&#26500;&#24314;&#20849;&#21516;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00505v1 Announce Type: cross  Abstract: In most applications of utilizing neural networks for mathematical optimization, a dedicated model is trained for each specific optimization objective. However, in many scenarios, several distinct yet correlated objectives or tasks often need to be optimized on the same set of problem inputs. Instead of independently training a different neural network for each problem separately, it would be more efficient to exploit the correlations between these objectives and to train multiple neural network models with shared model parameters and feature representations. To achieve this, this paper first establishes the concept of common information: the shared knowledge required for solving the correlated tasks, then proposes a novel approach for model training by adding into the model an additional reconstruction stage associated with a new reconstruction loss. This loss is for reconstructing the common information starting from a selected hidde
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#38024;&#23545;&#24494;&#26381;&#21153;&#31995;&#32479;&#30340;&#23569;&#26679;&#26412;&#24322;&#24120;&#36319;&#36394;&#20998;&#31867;&#30340;&#26032;&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#22836;&#27880;&#24847;&#21147;&#33258;&#32534;&#30721;&#22120;&#26500;&#24314;&#31995;&#32479;&#29305;&#23450;&#30340;&#36319;&#36394;&#34920;&#31034;&#65292;&#24182;&#24212;&#29992;&#22522;&#20110;Transformer&#32534;&#30721;&#22120;&#30340;&#27169;&#22411;&#26080;&#20851;&#20803;&#23398;&#20064;&#36827;&#34892;&#39640;&#25928;&#20998;&#31867;&#12290;</title><link>https://arxiv.org/abs/2403.18998</link><description>&lt;p&gt;
&#24494;&#26381;&#21153;&#31995;&#32479;&#30340;&#23569;&#26679;&#26412;&#36328;&#31995;&#32479;&#24322;&#24120;&#36319;&#36394;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Few-Shot Cross-System Anomaly Trace Classification for Microservice-based systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18998
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#38024;&#23545;&#24494;&#26381;&#21153;&#31995;&#32479;&#30340;&#23569;&#26679;&#26412;&#24322;&#24120;&#36319;&#36394;&#20998;&#31867;&#30340;&#26032;&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#22836;&#27880;&#24847;&#21147;&#33258;&#32534;&#30721;&#22120;&#26500;&#24314;&#31995;&#32479;&#29305;&#23450;&#30340;&#36319;&#36394;&#34920;&#31034;&#65292;&#24182;&#24212;&#29992;&#22522;&#20110;Transformer&#32534;&#30721;&#22120;&#30340;&#27169;&#22411;&#26080;&#20851;&#20803;&#23398;&#20064;&#36827;&#34892;&#39640;&#25928;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#26381;&#21153;&#31995;&#32479;&#65288;MSS&#65289;&#30001;&#20110;&#20854;&#22797;&#26434;&#21644;&#21160;&#24577;&#30340;&#29305;&#24615;&#21487;&#33021;&#22312;&#21508;&#31181;&#25925;&#38556;&#31867;&#21035;&#20013;&#20986;&#29616;&#25925;&#38556;&#12290;&#20026;&#20102;&#26377;&#25928;&#22788;&#29702;&#25925;&#38556;&#65292;AIOps&#24037;&#20855;&#21033;&#29992;&#22522;&#20110;&#36319;&#36394;&#30340;&#24322;&#24120;&#26816;&#27979;&#21644;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#24494;&#26381;&#21153;&#31995;&#32479;&#30340;&#23569;&#26679;&#26412;&#24322;&#24120;&#36319;&#36394;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#65306;&#65288;1&#65289;&#22810;&#22836;&#27880;&#24847;&#21147;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#26500;&#24314;&#31995;&#32479;&#29305;&#23450;&#30340;&#36319;&#36394;&#34920;&#31034;&#65292;&#20174;&#32780;&#23454;&#29616;&#65288;2&#65289;&#22522;&#20110;Transformer&#32534;&#30721;&#22120;&#30340;&#27169;&#22411;&#26080;&#20851;&#20803;&#23398;&#20064;&#65292;&#20197;&#36827;&#34892;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#23569;&#26679;&#26412;&#24322;&#24120;&#36319;&#36394;&#20998;&#31867;&#12290;&#35813;&#26694;&#26550;&#22312;&#20004;&#20010;&#20195;&#34920;&#24615;&#30340;MSS&#65292;Trainticket&#21644;OnlineBoutique&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#20351;&#29992;&#24320;&#25918;&#25968;&#25454;&#38598;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#35843;&#25972;&#23398;&#21040;&#30340;&#30693;&#35782;&#65292;&#20197;&#23545;&#26032;&#30340;&#12289;&#26410;&#35265;&#30340;&#26032;&#39062;&#25925;&#38556;&#31867;&#21035;&#30340;&#24322;&#24120;&#36319;&#36394;&#36827;&#34892;&#20998;&#31867;&#65292;&#26080;&#35770;&#26159;&#22312;&#26368;&#21021;&#35757;&#32451;&#30340;&#21516;&#19968;&#31995;&#32479;&#20869;&#65292;&#36824;&#26159;&#22312;&#20854;&#20182;&#31995;&#32479;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18998v1 Announce Type: cross  Abstract: Microservice-based systems (MSS) may experience failures in various fault categories due to their complex and dynamic nature. To effectively handle failures, AIOps tools utilize trace-based anomaly detection and root cause analysis. In this paper, we propose a novel framework for few-shot abnormal trace classification for MSS. Our framework comprises two main components: (1) Multi-Head Attention Autoencoder for constructing system-specific trace representations, which enables (2) Transformer Encoder-based Model-Agnostic Meta-Learning to perform effective and efficient few-shot learning for abnormal trace classification. The proposed framework is evaluated on two representative MSS, Trainticket and OnlineBoutique, with open datasets. The results show that our framework can adapt the learned knowledge to classify new, unseen abnormal traces of novel fault categories both within the same system it was initially trained on and even in the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#20027;&#21160;&#23398;&#20064;&#25216;&#26415;&#21482;&#22312;&#29305;&#23450;&#24773;&#22659;&#19979;&#26377;&#25928;&#65292;&#23545;&#25991;&#26412;&#20998;&#31867;&#20174;&#19994;&#32773;&#30340;&#24314;&#35758;&#26159;&#36873;&#25321;&#36866;&#24403;&#30340;&#25991;&#26412;&#34920;&#31034;&#21644;&#20998;&#31867;&#22120;&#21516;&#26679;&#37325;&#35201;&#12290;</title><link>https://arxiv.org/abs/2403.15744</link><description>&lt;p&gt;
&#35770;&#20027;&#21160;&#23398;&#20064;&#32773;&#30340;&#33030;&#24369;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Fragility of Active Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15744
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#20027;&#21160;&#23398;&#20064;&#25216;&#26415;&#21482;&#22312;&#29305;&#23450;&#24773;&#22659;&#19979;&#26377;&#25928;&#65292;&#23545;&#25991;&#26412;&#20998;&#31867;&#20174;&#19994;&#32773;&#30340;&#24314;&#35758;&#26159;&#36873;&#25321;&#36866;&#24403;&#30340;&#25991;&#26412;&#34920;&#31034;&#21644;&#20998;&#31867;&#22120;&#21516;&#26679;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#23398;&#20064;&#65288;AL&#65289;&#25216;&#26415;&#26088;&#22312;&#36890;&#36807;&#36845;&#20195;&#36873;&#25321;&#26368;&#26377;&#21487;&#33021;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#23454;&#20363;&#65292;&#26368;&#22823;&#31243;&#24230;&#22320;&#21033;&#29992;&#26631;&#27880;&#39044;&#31639;&#12290;&#28982;&#32780;&#65292;&#19982;&#38543;&#26426;&#25277;&#26679;&#30456;&#27604;&#65292;&#22312;&#19981;&#21516;&#35774;&#32622;&#19979;&#65288;&#20363;&#22914;&#19981;&#21516;&#25968;&#25454;&#38598;&#65292;&#20998;&#31867;&#22120;&#65289;&#65292;&#23427;&#20204;&#30340;&#30410;&#22788;&#24182;&#19981;&#19968;&#33268;&#12290;&#22312;&#36825;&#39033;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#22240;&#32032;&#30340;&#32452;&#21512;&#22914;&#20309;&#21487;&#33021;&#25513;&#30422;&#20027;&#21160;&#23398;&#20064;&#25216;&#26415;&#30340;&#20219;&#20309;&#25910;&#30410;&#12290;&#19987;&#27880;&#20110;&#25991;&#26412;&#20998;&#31867;&#65292;&#25105;&#20204;&#22312;&#22823;&#32422;1000&#20010;&#23454;&#39564;&#20013;&#20005;&#26684;&#35780;&#20272;&#20102;&#36827;&#34892;&#20998;&#31867;&#65292;&#25105;&#20204;&#22312;&#22823;&#32422;1000&#20010;&#23454;&#39564;&#20013;&#20005;&#26684;&#35780;&#20272;&#20102;AL&#25216;&#26415;&#65292;&#36825;&#20123;&#23454;&#39564;&#22312;&#25968;&#25454;&#38598;&#12289;&#25209;&#22823;&#23567;&#12289;&#25991;&#26412;&#34920;&#31034;&#21644;&#20998;&#31867;&#22120;&#26041;&#38754;&#21464;&#21270;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;AL&#21482;&#22312;&#19968;&#32452;&#26377;&#38480;&#30340;&#24773;&#22659;&#20013;&#26377;&#25928;&#12290;&#25105;&#20204;&#36824;&#35299;&#20915;&#20102;&#20351;&#29992;&#19982;&#29616;&#23454;&#19990;&#30028;&#26399;&#26395;&#26356;&#22909;&#23545;&#40784;&#30340;&#24230;&#37327;&#30340;&#38382;&#39064;&#12290;&#36825;&#39033;&#30740;&#31350;&#30340;&#24433;&#21709;&#22312;&#20110;&#23545;&#20174;&#19994;&#32773;&#30340;&#27934;&#23519;&#65306;(a) &#25991;&#26412;&#34920;&#31034;&#21644;&#20998;&#31867;&#22120;&#30340;&#36873;&#25321;&#19982;AL&#25216;&#26415;&#30340;&#36873;&#25321;&#19968;&#26679;&#37325;&#35201;&#65292;(b) &#36873;&#25321;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15744v1 Announce Type: cross  Abstract: Active learning (AL) techniques aim to maximally utilize a labeling budget by iteratively selecting instances that are most likely to improve prediction accuracy. However, their benefit compared to random sampling has not been consistent across various setups, e.g., different datasets, classifiers. In this empirical study, we examine how a combination of different factors might obscure any gains from an AL technique.   Focusing on text classification, we rigorously evaluate AL techniques over around 1000 experiments that vary wrt the dataset, batch size, text representation and the classifier. We show that AL is only effective in a narrow set of circumstances. We also address the problem of using metrics that are better aligned with real world expectations.   The impact of this study is in its insights for a practitioner: (a) the choice of text representation and classifier is as important as that of an AL technique, (b) choice of the 
&lt;/p&gt;</description></item><item><title>Leap&#26159;&#19968;&#20010;&#20351;&#29992;GPT-2&#27169;&#22411;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#26681;&#25454;&#39044;&#27979;&#30340;&#21512;&#25104;&#36335;&#32447;&#28145;&#24230;&#65292;&#21160;&#24577;&#22320;&#21253;&#21547;&#20102;&#20851;&#38190;&#20013;&#38388;&#20307;&#30340;&#21487;&#29992;&#24615;&#20449;&#24687;&#65292;&#22312;&#21512;&#25104;&#21487;&#36798;&#24615;&#35780;&#20998;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>https://arxiv.org/abs/2403.13005</link><description>&lt;p&gt;
Leap: &#20013;&#38388;&#20307;&#30340;&#20998;&#23376;&#21512;&#25104;&#35780;&#20998;
&lt;/p&gt;
&lt;p&gt;
Leap: molecular synthesisability scoring with intermediates
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13005
&lt;/p&gt;
&lt;p&gt;
Leap&#26159;&#19968;&#20010;&#20351;&#29992;GPT-2&#27169;&#22411;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#26681;&#25454;&#39044;&#27979;&#30340;&#21512;&#25104;&#36335;&#32447;&#28145;&#24230;&#65292;&#21160;&#24577;&#22320;&#21253;&#21547;&#20102;&#20851;&#38190;&#20013;&#38388;&#20307;&#30340;&#21487;&#29992;&#24615;&#20449;&#24687;&#65292;&#22312;&#21512;&#25104;&#21487;&#36798;&#24615;&#35780;&#20998;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#20998;&#23376;&#26159;&#21542;&#21487;&#20197;&#21512;&#25104;&#26159;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#39318;&#35201;&#20219;&#21153;&#12290;&#23427;&#20351;&#35745;&#31639;&#21270;&#23398;&#23478;&#33021;&#22815;&#36807;&#28388;&#21487;&#34892;&#21270;&#21512;&#29289;&#25110;&#20559;&#21521;&#20998;&#23376;&#29983;&#25104;&#27169;&#22411;&#12290;&#21512;&#25104;&#24615;&#30340;&#27010;&#24565;&#26159;&#21160;&#24577;&#30340;&#65292;&#22240;&#20026;&#23427;&#20250;&#38543;&#30528;&#20851;&#38190;&#21270;&#21512;&#29289;&#30340;&#21487;&#29992;&#24615;&#32780;&#28436;&#21464;&#12290;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#19968;&#31181;&#24120;&#35265;&#26041;&#27861;&#28041;&#21450;&#25506;&#32034;&#21487;&#21512;&#25104;&#20013;&#38388;&#20307;&#21608;&#22260;&#30340;&#21270;&#23398;&#31354;&#38388;&#12290;&#36825;&#19968;&#31574;&#30053;&#25913;&#21892;&#20102;&#30001;&#20110;&#20851;&#38190;&#20013;&#38388;&#20307;&#30340;&#21487;&#29992;&#24615;&#32780;&#23548;&#33268;&#30340;&#34893;&#29983;&#20998;&#23376;&#30340;&#21512;&#25104;&#33021;&#21147;&#12290;&#29616;&#26377;&#30340;&#21512;&#25104;&#21487;&#36798;&#24615;&#35780;&#20998;&#26041;&#27861;&#65292;&#22914;SAScore&#12289;SCScore&#21644;RAScore&#65292;&#26080;&#27861;&#21160;&#24577;&#22320;&#26681;&#25454;&#20013;&#38388;&#20307;&#36827;&#34892;&#26465;&#20214;&#35780;&#20998;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;Leap&#26159;&#19968;&#20010;&#22312;&#39044;&#27979;&#30340;&#21512;&#25104;&#36335;&#32447;&#28145;&#24230;&#65288;&#25110;&#26368;&#38271;&#32447;&#24615;&#36335;&#24452;&#65289;&#19978;&#35757;&#32451;&#30340;GPT-2&#27169;&#22411;&#65292;&#20801;&#35768;&#22312;&#25512;&#26029;&#26102;&#21253;&#21547;&#20851;&#38190;&#20013;&#38388;&#20307;&#30340;&#21487;&#29992;&#24615;&#20449;&#24687;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Leap&#22312;AUC sc&#19978;&#33267;&#23569;&#27604;&#25152;&#26377;&#20854;&#20182;&#35780;&#20998;&#26041;&#27861;&#39640;&#20986;5%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13005v1 Announce Type: cross  Abstract: Assessing whether a molecule can be synthesised is a primary task in drug discovery. It enables computational chemists to filter for viable compounds or bias molecular generative models. The notion of synthesisability is dynamic as it evolves depending on the availability of key compounds. A common approach in drug discovery involves exploring the chemical space surrounding synthetically-accessible intermediates. This strategy improves the synthesisability of the derived molecules due to the availability of key intermediates. Existing synthesisability scoring methods such as SAScore, SCScore and RAScore, cannot condition on intermediates dynamically. Our approach, Leap, is a GPT-2 model trained on the depth, or longest linear path, of predicted synthesis routes that allows information on the availability of key intermediates to be included at inference time. We show that Leap surpasses all other scoring methods by at least 5% on AUC sc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;&#22320;&#29699;&#35266;&#27979;&#25968;&#25454;&#20013;&#39068;&#33394;&#21644;&#32441;&#29702;&#22833;&#30495;&#30340;&#25935;&#24863;&#24615;&#65292;&#21457;&#29616;&#27169;&#22411;&#23545;&#32441;&#29702;&#22833;&#30495;&#27604;&#39068;&#33394;&#22833;&#30495;&#26356;&#25935;&#24863;</title><link>https://arxiv.org/abs/2403.04385</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#39068;&#33394;&#21644;&#32441;&#29702;&#22833;&#30495;&#23545;&#22320;&#29699;&#35266;&#27979;&#25968;&#25454;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Impacts of Color and Texture Distortions on Earth Observation Data in Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04385
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;&#22320;&#29699;&#35266;&#27979;&#25968;&#25454;&#20013;&#39068;&#33394;&#21644;&#32441;&#29702;&#22833;&#30495;&#30340;&#25935;&#24863;&#24615;&#65292;&#21457;&#29616;&#27169;&#22411;&#23545;&#32441;&#29702;&#22833;&#30495;&#27604;&#39068;&#33394;&#22833;&#30495;&#26356;&#25935;&#24863;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#29289;&#35206;&#30422;&#20998;&#31867;&#21644;&#21464;&#21270;&#26816;&#27979;&#26159;&#36965;&#24863;&#21644;&#22320;&#29699;&#35266;&#27979;&#65288;EO&#65289;&#30340;&#20004;&#20010;&#37325;&#35201;&#24212;&#29992;&#39046;&#22495;&#65292;&#22312;&#28145;&#24230;&#23398;&#20064;&#30340;&#36827;&#23637;&#20013;&#24471;&#21040;&#20102;&#26497;&#22823;&#30340;&#30410;&#22788;&#12290;&#21367;&#31215;&#21644;&#22522;&#20110;Transformer&#30340;U-net&#27169;&#22411;&#26159;&#36825;&#20123;&#20219;&#21153;&#30340;&#26368;&#20808;&#36827;&#26550;&#26500;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#24471;&#21040;&#20102;&#22823;&#35268;&#27169;&#26631;&#27880;EO&#25968;&#25454;&#38598;&#30340;&#22686;&#21152;&#30340;&#25552;&#21319;&#12290;&#28982;&#32780;&#65292;&#23545;&#36755;&#20837;EO&#25968;&#25454;&#30340;&#19981;&#21516;&#35270;&#35273;&#29305;&#24449;&#23545;&#27169;&#22411;&#39044;&#27979;&#30340;&#24433;&#21709;&#23578;&#19981;&#26126;&#30830;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#22312;&#25512;&#26029;&#26399;&#38388;&#23545;&#36755;&#20837;EO&#25968;&#25454;&#36827;&#34892;&#20960;&#31181;&#22522;&#20110;&#39068;&#33394;&#21644;&#32441;&#29702;&#30340;&#22833;&#30495;&#26102;&#27169;&#22411;&#30340;&#25935;&#24863;&#24615;&#65292;&#32771;&#34385;&#21040;&#36825;&#20123;&#27169;&#22411;&#26159;&#22312;&#27809;&#26377;&#36825;&#31181;&#22833;&#30495;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#25105;&#20204;&#23545;&#22810;&#20010;&#26368;&#20808;&#36827;&#30340;&#22320;&#29289;&#35206;&#30422;&#20998;&#31867;&#32593;&#32476;&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#20204;&#36890;&#24120;&#23545;&#32441;&#29702;&#22833;&#30495;&#27604;&#39068;&#33394;&#22833;&#30495;&#26356;&#20026;&#25935;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04385v1 Announce Type: cross  Abstract: Land cover classification and change detection are two important applications of remote sensing and Earth observation (EO) that have benefited greatly from the advances of deep learning. Convolutional and transformer-based U-net models are the state-of-the-art architectures for these tasks, and their performances have been boosted by an increased availability of large-scale annotated EO datasets. However, the influence of different visual characteristics of the input EO data on a model's predictions is not well understood. In this work we systematically examine model sensitivities with respect to several color- and texture-based distortions on the input EO data during inference, given models that have been trained without such distortions. We conduct experiments with multiple state-of-the-art segmentation networks for land cover classification and show that they are in general more sensitive to texture than to color distortions. Beyond
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#38754;&#21521;&#38543;&#26426;&#20559;&#22909;&#32763;&#36716;&#30340;&#31574;&#30053;&#20248;&#21270;&#36890;&#29992;&#26694;&#26550;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#29702;&#35299;&#23384;&#22312;&#22024;&#26434;&#21453;&#39304;&#26102;&#30340;DPO&#31639;&#27861;&#65292;&#20174;&#32780;&#35299;&#20915;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#20154;&#31867;&#20852;&#36259;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.00409</link><description>&lt;p&gt;
&#21487;&#35777;&#26126;&#40065;&#26834;&#30340;DPO: &#29992;&#26377;&#22122;&#21453;&#39304;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Provably Robust DPO: Aligning Language Models with Noisy Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00409
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#38754;&#21521;&#38543;&#26426;&#20559;&#22909;&#32763;&#36716;&#30340;&#31574;&#30053;&#20248;&#21270;&#36890;&#29992;&#26694;&#26550;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#29702;&#35299;&#23384;&#22312;&#22024;&#26434;&#21453;&#39304;&#26102;&#30340;DPO&#31639;&#27861;&#65292;&#20174;&#32780;&#35299;&#20915;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#20154;&#31867;&#20852;&#36259;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20174;&#22522;&#20110;&#21916;&#22909;&#21453;&#39304;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#19982;&#20154;&#31867;&#20852;&#36259;&#23545;&#40784;&#30340;&#26377;&#21069;&#26223;&#26041;&#27861;&#24050;&#32463;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#34429;&#28982;&#36825;&#20123;&#23545;&#40784;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#23545;&#39640;&#36136;&#37327;&#20154;&#31867;&#21916;&#22909;&#25968;&#25454;&#30340;&#20381;&#36182;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#26500;&#25104;&#20102;&#29942;&#39048;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25968;&#25454;&#38598;&#20013;&#26377;&#22122;&#65288;&#19981;&#27491;&#30830;&#21644;&#27169;&#31946;&#65289;&#30340;&#20559;&#22909;&#23545;&#21487;&#33021;&#20250;&#38480;&#21046;&#35821;&#35328;&#27169;&#22411;&#20934;&#30830;&#25429;&#25417;&#20154;&#31867;&#24847;&#22270;&#12290;&#34429;&#28982;&#20174;&#19994;&#32773;&#26368;&#36817;&#25552;&#20986;&#20102;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#20943;&#36731;&#22122;&#22768;&#20559;&#22909;&#30340;&#24433;&#21709;&#65292;&#20294;&#23545;&#23427;&#20204;&#30340;&#24037;&#20316;&#23436;&#25972;&#29702;&#35770;&#29702;&#35299;&#20173;&#28982;&#38590;&#20197;&#25417;&#25720;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#38754;&#21521;&#22312;&#38543;&#26426;&#20559;&#22909;&#32763;&#36716;&#23384;&#22312;&#30340;&#31574;&#30053;&#20248;&#21270;&#30340;&#36890;&#29992;&#26694;&#26550;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#31639;&#27861;&#65292;&#22240;&#20026;&#23427;&#20551;&#35774;&#20559;&#22909;&#36981;&#24490; Bradley-Te
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00409v1 Announce Type: cross  Abstract: Learning from preference-based feedback has recently gained traction as a promising approach to align language models with human interests. While these aligned generative models have demonstrated impressive capabilities across various tasks, their dependence on high-quality human preference data poses a bottleneck in practical applications. Specifically, noisy (incorrect and ambiguous) preference pairs in the dataset might restrict the language models from capturing human intent accurately. While practitioners have recently proposed heuristics to mitigate the effect of noisy preferences, a complete theoretical understanding of their workings remain elusive.   In this work, we aim to bridge this gap by by introducing a general framework for policy optimization in the presence of random preference flips. We focus on the direct preference optimization (DPO) algorithm in particular since it assumes that preferences adhere to the Bradley-Te
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#23581;&#35797;&#35299;&#20915;&#36328;&#38382;&#39064;&#27867;&#21270;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#36890;&#36807;&#23558;VRPs&#23450;&#20041;&#20026;&#20849;&#20139;&#22522;&#30784;&#23646;&#24615;&#30340;&#19981;&#21516;&#32452;&#21512;&#65292;&#24182;&#36890;&#36807;&#23646;&#24615;&#32452;&#21512;&#21516;&#26102;&#35299;&#20915;&#23427;&#20204;&#65292;&#23454;&#29616;&#20102;&#38646;&#26679;&#26412;&#27867;&#21270;&#30340;&#36335;&#24452;&#38382;&#39064;&#35299;&#20915;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.16891</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#29992;&#20110;&#20855;&#26377;&#36328;&#38382;&#39064;&#38646;&#26679;&#26412;&#27867;&#21270;&#30340;&#36335;&#24452;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Multi-Task Learning for Routing Problem with Cross-Problem Zero-Shot Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#23581;&#35797;&#35299;&#20915;&#36328;&#38382;&#39064;&#27867;&#21270;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#36890;&#36807;&#23558;VRPs&#23450;&#20041;&#20026;&#20849;&#20139;&#22522;&#30784;&#23646;&#24615;&#30340;&#19981;&#21516;&#32452;&#21512;&#65292;&#24182;&#36890;&#36807;&#23646;&#24615;&#32452;&#21512;&#21516;&#26102;&#35299;&#20915;&#23427;&#20204;&#65292;&#23454;&#29616;&#20102;&#38646;&#26679;&#26412;&#27867;&#21270;&#30340;&#36335;&#24452;&#38382;&#39064;&#35299;&#20915;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#65288;VRPs&#65289;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#37117;&#33021;&#25214;&#21040;&#65292;&#24050;&#32463;&#25104;&#20026;&#20960;&#21313;&#24180;&#30340;&#37325;&#35201;&#30740;&#31350;&#35838;&#39064;&#12290;&#26368;&#36817;&#65292;&#21033;&#29992;&#22522;&#20110;&#23398;&#20064;&#30340;&#27169;&#22411;&#26469;&#35299;&#20915;VRPs&#30340;&#31070;&#32463;&#32452;&#21512;&#20248;&#21270;&#65288;NCO&#65289;&#26041;&#27861;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;NCO&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#20026;&#27599;&#20010;&#36335;&#24452;&#38382;&#39064;&#26500;&#24314;&#19968;&#20010;&#27169;&#22411;&#65292;&#36825;&#26174;&#33879;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#20855;&#26377;&#19981;&#21516;&#23646;&#24615;&#30340;&#30495;&#23454;&#24037;&#19994;&#38382;&#39064;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23581;&#35797;&#35299;&#20915;&#36328;&#38382;&#39064;&#27867;&#21270;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;VRPs&#23450;&#20041;&#20026;&#19968;&#32452;&#20849;&#20139;&#30340;&#22522;&#30784;&#23646;&#24615;&#30340;&#19981;&#21516;&#32452;&#21512;&#65292;&#24182;&#36890;&#36807;&#23646;&#24615;&#32452;&#21512;&#21516;&#26102;&#36890;&#36807;&#21333;&#19968;&#27169;&#22411;&#35299;&#20915;&#23427;&#20204;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#33021;&#22815;&#25104;&#21151;&#35299;&#20915;&#20855;&#26377;&#26410;&#35265;&#23646;&#24615;&#32452;&#21512;&#30340;VRPs&#65292;&#23454;&#29616;&#38646;&#26679;&#26412;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16891v1 Announce Type: cross  Abstract: Vehicle routing problems (VRPs), which can be found in numerous real-world applications, have been an important research topic for several decades. Recently, the neural combinatorial optimization (NCO) approach that leverages a learning-based model to solve VRPs without manual algorithm design has gained substantial attention. However, current NCO methods typically require building one model for each routing problem, which significantly hinders their practical application for real-world industry problems with diverse attributes. In this work, we make the first attempt to tackle the crucial challenge of cross-problem generalization. In particular, we formulate VRPs as different combinations of a set of shared underlying attributes and solve them simultaneously via a single model through attribute composition. In this way, our proposed model can successfully solve VRPs with unseen attribute combinations in a zero-shot generalization mann
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#31572;&#20102;&#25552;&#21319;&#22810;&#27573;&#20999;&#21106;&#22810;&#38754;&#20307;&#30340;&#21738;&#20123;&#19979;&#30028;&#31435;&#26041;&#19981;&#31561;&#24335;&#21644;&#21738;&#20123;&#20999;&#21106;&#19981;&#31561;&#24335;&#23450;&#20041;&#38754;&#30340;&#22522;&#26412;&#38382;&#39064;&#65292;&#20197;&#21450;&#21028;&#23450;&#20999;&#21106;&#19981;&#31561;&#24335;&#23450;&#20041;&#24615;&#30340;NP&#38590;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.16814</link><description>&lt;p&gt;
&#20999;&#21106;&#38754;&#21644;&#31435;&#26041;&#38754;&#30340;&#25552;&#21319;&#22810;&#27573;&#20999;&#21106;&#22810;&#38754;&#20307;
&lt;/p&gt;
&lt;p&gt;
Cut Facets and Cube Facets of Lifted Multicut Polytopes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16814
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#31572;&#20102;&#25552;&#21319;&#22810;&#27573;&#20999;&#21106;&#22810;&#38754;&#20307;&#30340;&#21738;&#20123;&#19979;&#30028;&#31435;&#26041;&#19981;&#31561;&#24335;&#21644;&#21738;&#20123;&#20999;&#21106;&#19981;&#31561;&#24335;&#23450;&#20041;&#38754;&#30340;&#22522;&#26412;&#38382;&#39064;&#65292;&#20197;&#21450;&#21028;&#23450;&#20999;&#21106;&#19981;&#31561;&#24335;&#23450;&#20041;&#24615;&#30340;NP&#38590;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#21319;&#22810;&#27573;&#20999;&#21106;&#38382;&#39064;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#26377;&#30528;&#22810;&#26679;&#30340;&#24212;&#29992;&#12290;&#22522;&#20110;&#32447;&#24615;&#35268;&#21010;&#30340;&#31934;&#30830;&#31639;&#27861;&#38656;&#35201;&#23545;&#25552;&#21319;&#22810;&#27573;&#20999;&#21106;&#22810;&#38754;&#20307;&#26377;&#25152;&#20102;&#35299;&#12290;&#23613;&#31649;&#26368;&#36817;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#20851;&#20110;&#36825;&#20123;&#22810;&#38754;&#20307;&#30340;&#20004;&#20010;&#22522;&#26412;&#38382;&#39064;&#19968;&#30452;&#26410;&#33021;&#35299;&#20915;&#65306;&#21738;&#20123;&#19979;&#30028;&#31435;&#26041;&#19981;&#31561;&#24335;&#23450;&#20041;&#20102;&#38754;&#65292;&#21738;&#20123;&#20999;&#21106;&#19981;&#31561;&#24335;&#23450;&#20041;&#20102;&#38754;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24314;&#31435;&#24517;&#35201;&#12289;&#20805;&#20998;&#19988;&#39640;&#25928;&#21487;&#21028;&#23450;&#30340;&#26465;&#20214;&#26469;&#22238;&#31572;&#31532;&#19968;&#20010;&#38382;&#39064;&#12290;&#33267;&#20110;&#31532;&#20108;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#34920;&#26126;&#21028;&#26029;&#20999;&#21106;&#19981;&#31561;&#24335;&#30340;&#38754;&#23450;&#20041;&#24615;&#26159;NP&#38590;&#30340;&#12290;&#36825;&#23436;&#25104;&#20102;&#23545;&#25552;&#21319;&#22810;&#27573;&#20999;&#21106;&#22810;&#38754;&#20307;&#30340;&#35268;&#33539;&#38754;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16814v2 Announce Type: replace-cross  Abstract: The lifted multicut problem has diverse applications in the field of computer vision. Exact algorithms based on linear programming require an understanding of lifted multicut polytopes. Despite recent progress, two fundamental questions about these polytopes have remained open: Which lower cube inequalities define facets, and which cut inequalities define facets? In this article, we answer the first question by establishing conditions that are necessary, sufficient and efficiently decidable. Toward the second question, we show that deciding facet-definingness of cut inequalities is NP-hard. This completes the analysis of canonical facets of lifted multicut polytopes.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#26426;&#22120;&#32763;&#35793;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#26412;&#25991;&#24320;&#21457;&#20102;&#29992;&#20110;&#38750;&#27954;&#21644;&#20122;&#27954;&#35821;&#35328;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;&#20219;&#21153;&#30340;&#20004;&#31181;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#27604;&#37096;&#20998;&#23448;&#26041;&#22522;&#20934;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.12730</link><description>&lt;p&gt;
UMBCLU&#22312;SemEval-2024&#20219;&#21153;1A&#21644;1C&#20013;&#30340;&#34920;&#29616;&#65306;&#24102;&#26377;&#21644;&#19981;&#24102;&#26377;&#26426;&#22120;&#32763;&#35793;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
UMBCLU at SemEval-2024 Task 1A and 1C: Semantic Textual Relatedness with and without machine translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12730
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#32763;&#35793;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#26412;&#25991;&#24320;&#21457;&#20102;&#29992;&#20110;&#38750;&#27954;&#21644;&#20122;&#27954;&#35821;&#35328;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;&#20219;&#21153;&#30340;&#20004;&#31181;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#27604;&#37096;&#20998;&#23448;&#26041;&#22522;&#20934;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25551;&#36848;&#20102;&#25105;&#20204;&#20026;SemEval-2024&#20219;&#21153;1&#24320;&#21457;&#30340;&#31995;&#32479;&#65292;&#8220;&#38750;&#27954;&#21644;&#20122;&#27954;&#35821;&#35328;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;&#8221;&#12290; &#35813;&#20219;&#21153;&#30340;&#30446;&#26631;&#26159;&#26500;&#24314;&#19968;&#20010;&#33021;&#22815;&#35782;&#21035;&#30446;&#26631;&#35821;&#35328;&#20013;&#23646;&#20110;&#38750;&#27954;&#21644;&#20122;&#27954;&#35821;&#35328;&#38598;&#21512;&#30340;&#20004;&#20010;&#21477;&#23376;&#20043;&#38388;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;&#65288;STR&#65289;&#30340;&#27169;&#22411;&#12290; &#25105;&#20204;&#21442;&#19982;&#20102;&#23376;&#20219;&#21153;A&#21644;C&#65292;&#24182;&#25506;&#32034;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#30417;&#30563;&#21644;&#36328;&#35821;&#35328;&#35757;&#32451;&#12290; &#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#34987;&#24191;&#27867;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;&#21644;&#35821;&#20041;&#30456;&#20284;&#24615;&#12290; &#20351;&#29992;&#26426;&#22120;&#32763;&#35793;&#21644;&#21477;&#23376;&#23884;&#20837;LLMs&#30340;&#32452;&#21512;&#65292;&#25105;&#20204;&#20026;&#23376;&#20219;&#21153;A&#24320;&#21457;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;STR&#27169;&#22411;&#65292;TranSem&#65292;&#24182;&#23545;STR&#25968;&#25454;&#19978;&#30340;T5&#31995;&#21015;&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#29992;&#20110;&#23376;&#20219;&#21153;C&#30340;FineSem&#12290; &#25105;&#20204;&#22312;&#23376;&#20219;&#21153;A&#20013;7&#31181;&#35821;&#35328;&#30340;&#27169;&#22411;&#32467;&#26524;&#27604;3&#31181;&#35821;&#35328;&#30340;&#23448;&#26041;&#22522;&#20934;&#26356;&#22909;&#65292;&#32780;&#19982;&#20854;&#20182;4&#31181;&#35821;&#35328;&#30340;&#22522;&#20934;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12730v1 Announce Type: cross  Abstract: This paper describes the system we developed for SemEval-2024 Task 1, "Semantic Textual Relatedness for African and Asian Languages." The aim of the task is to build a model that can identify semantic textual relatedness (STR) between two sentences of a target language belonging to a collection of African and Asian languages. We participated in Subtasks A and C and explored supervised and cross-lingual training leveraging large language models (LLMs). Pre-trained large language models have been extensively used for machine translation and semantic similarity. Using a combination of machine translation and sentence embedding LLMs, we developed a unified STR model, TranSem, for subtask A and fine-tuned the T5 family of models on the STR data, FineSem, for use in subtask C. Our model results for 7 languages in subtask A were better than the official baseline for 3 languages and on par with the baseline for the remaining 4 languages. Our m
&lt;/p&gt;</description></item><item><title>&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#21644;&#27169;&#22411;&#21704;&#23494;&#39039;&#37327;&#30340;&#23545;&#31216;&#24615;&#65292;&#35299;&#37322;&#38081;&#30913;&#20234;&#36763;&#27169;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.11701</link><description>&lt;p&gt;
&#35299;&#37322;&#20234;&#36763;&#27169;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Explaining the Machine Learning Solution of the Ising Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11701
&lt;/p&gt;
&lt;p&gt;
&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#21644;&#27169;&#22411;&#21704;&#23494;&#39039;&#37327;&#30340;&#23545;&#31216;&#24615;&#65292;&#35299;&#37322;&#38081;&#30913;&#20234;&#36763;&#27169;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25216;&#26415;&#22312;&#35299;&#20915;&#28041;&#21450;&#22823;&#32500;&#25968;&#25454;&#30340;&#38382;&#39064;&#20013;&#38750;&#24120;&#24378;&#22823;&#65292;&#20294;&#35299;&#37322;&#20174;&#25311;&#21512;&#21442;&#25968;&#24471;&#20986;&#30340;&#32467;&#26524;&#20173;&#28982;&#26159;&#19968;&#39033;&#33267;&#20851;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#22312;&#29289;&#29702;&#24212;&#29992;&#20013;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#23454;&#29616;&#23545;&#38081;&#30913;&#20234;&#36763;&#27169;&#22411;&#30340;&#35299;&#37322;&#65292;&#36825;&#26159;&#36817;&#24180;&#26469;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#37325;&#28857;&#23545;&#35937;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#27809;&#26377;&#20219;&#20309;&#38544;&#34255;&#23618;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#20197;&#21450;&#21704;&#23494;&#39039;&#37327;&#30340;&#23545;&#31216;&#24615;&#26469;&#25214;&#21040;&#27169;&#22411;&#36830;&#32493;&#30456;&#21464;&#30340;&#20020;&#30028;&#28201;&#24230;&#65292;&#25214;&#21040;&#20102;&#19968;&#31181;&#35299;&#37322;&#20854;&#31574;&#30053;&#30340;&#26041;&#27861;&#12290;&#36825;&#20351;&#24471;&#22312;&#23545;&#31216;&#24615;&#26410;&#30693;&#26102;&#21487;&#20197;&#39044;&#27979;&#35299;&#20915;&#38382;&#39064;&#25152;&#38656;&#30340;NN&#30340;&#26368;&#23567;&#25193;&#23637;&#65292;&#36825;&#20063;&#26159;&#21487;&#20197;&#35299;&#37322;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11701v1 Announce Type: cross  Abstract: As powerful as machine learning (ML) techniques are in solving problems involving data with large dimensionality, explaining the results from the fitted parameters remains a challenging task of utmost importance, especially in physics applications. Here it is shown how this can be accomplished for the ferromagnetic Ising model, the target of many ML studies in the last years. By using a neural network (NN) without any hidden layers and the symmetry of the Hamiltonian to find the critical temperature for the continuous phase transition of the model, an explanation of its strategy is found. This allows the prediction of the minimal extension of the NN to solve the problem when the symmetry is not known, which is also explainable.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#19968;&#20010;&#31616;&#21333;&#30340;U-Net&#20998;&#21106;&#22522;&#32447;&#20173;&#28982;&#26159;&#36827;&#34892;&#21464;&#21270;&#26816;&#27979;&#20219;&#21153;&#30340;&#39030;&#23574;&#34920;&#29616;&#32773;&#12290;</title><link>https://arxiv.org/abs/2402.06994</link><description>&lt;p&gt;
&#19968;&#20010;&#21464;&#21270;&#26816;&#27979;&#30340;&#29616;&#23454;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
A Change Detection Reality Check
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06994
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#19968;&#20010;&#31616;&#21333;&#30340;U-Net&#20998;&#21106;&#22522;&#32447;&#20173;&#28982;&#26159;&#36827;&#34892;&#21464;&#21270;&#26816;&#27979;&#20219;&#21153;&#30340;&#39030;&#23574;&#34920;&#29616;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36817;&#24180;&#26469;&#30340;&#36965;&#24863;&#25991;&#29486;&#20013;&#65292;&#20986;&#29616;&#20102;&#22823;&#37327;&#25552;&#20986;&#30340;&#29992;&#20110;&#21464;&#21270;&#26816;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#12290;&#36825;&#20123;&#26041;&#27861;&#22768;&#31216;&#22312;&#19981;&#21516;&#30340;&#26631;&#20934;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#25552;&#20379;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#35813;&#39046;&#22495;&#26159;&#21542;&#30495;&#27491;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#65292;&#19968;&#20010;&#31616;&#21333;&#30340;U-Net&#20998;&#21106;&#22522;&#32447;&#65292;&#27809;&#26377;&#35757;&#32451;&#25216;&#24039;&#25110;&#22797;&#26434;&#30340;&#26550;&#26500;&#25913;&#21464;&#65292;&#20173;&#28982;&#26159;&#36827;&#34892;&#21464;&#21270;&#26816;&#27979;&#20219;&#21153;&#30340;&#39030;&#23574;&#34920;&#29616;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, there has been an explosion of proposed change detection deep learning architectures in the remote sensing literature. These approaches claim to offer state-of the-art performance on different standard benchmark datasets. However, has the field truly made significant progress? In this paper we perform experiments which conclude a simple U-Net segmentation baseline without training tricks or complicated architectural changes is still a top performer for the task of change detection.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#37325;&#22797;&#20351;&#29992;&#32463;&#36807;&#39564;&#35777;&#30340;&#30005;&#36335;&#26469;&#22686;&#21152;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#32773;&#36890;&#36807;&#26500;&#24314;&#25968;&#23398;&#21644;&#36923;&#36753;&#35268;&#33539;&#30340;&#26694;&#26550;&#65292;&#24182;&#23545;&#19968;&#20010;n&#20301;&#25972;&#25968;&#21152;&#27861;&#27169;&#22411;&#36827;&#34892;&#23436;&#20840;&#39564;&#35777;&#12290;&#20182;&#20204;&#25554;&#20837;&#35757;&#32451;&#22909;&#30340;&#21152;&#27861;&#27169;&#22411;&#21040;&#19968;&#20010;&#26410;&#32463;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#35757;&#32451;&#32452;&#21512;&#27169;&#22411;&#25191;&#34892;&#21152;&#27861;&#21644;&#20943;&#27861;&#12290;&#20182;&#20204;&#21457;&#29616;&#21152;&#27861;&#30005;&#36335;&#22312;&#36825;&#20004;&#20010;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#37325;&#22797;&#20351;&#29992;&#65292;&#20174;&#32780;&#31616;&#21270;&#20102;&#20943;&#27861;&#27169;&#22411;&#30340;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.02619</link><description>&lt;p&gt;
&#36890;&#36807;&#37325;&#22797;&#20351;&#29992;&#32463;&#36807;&#39564;&#35777;&#30340;&#30005;&#36335;&#22686;&#21152;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;
&lt;/p&gt;
&lt;p&gt;
Increasing Trust in Language Models through the Reuse of Verified Circuits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02619
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#37325;&#22797;&#20351;&#29992;&#32463;&#36807;&#39564;&#35777;&#30340;&#30005;&#36335;&#26469;&#22686;&#21152;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#32773;&#36890;&#36807;&#26500;&#24314;&#25968;&#23398;&#21644;&#36923;&#36753;&#35268;&#33539;&#30340;&#26694;&#26550;&#65292;&#24182;&#23545;&#19968;&#20010;n&#20301;&#25972;&#25968;&#21152;&#27861;&#27169;&#22411;&#36827;&#34892;&#23436;&#20840;&#39564;&#35777;&#12290;&#20182;&#20204;&#25554;&#20837;&#35757;&#32451;&#22909;&#30340;&#21152;&#27861;&#27169;&#22411;&#21040;&#19968;&#20010;&#26410;&#32463;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#35757;&#32451;&#32452;&#21512;&#27169;&#22411;&#25191;&#34892;&#21152;&#27861;&#21644;&#20943;&#27861;&#12290;&#20182;&#20204;&#21457;&#29616;&#21152;&#27861;&#30005;&#36335;&#22312;&#36825;&#20004;&#20010;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#37325;&#22797;&#20351;&#29992;&#65292;&#20174;&#32780;&#31616;&#21270;&#20102;&#20943;&#27861;&#27169;&#22411;&#30340;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#22312;&#21508;&#31181;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#24191;&#27867;&#65292;&#20294;&#23427;&#20204;&#30340;&#35757;&#32451;&#32463;&#24120;&#24573;&#30053;&#32597;&#35265;&#30340;&#36793;&#30028;&#24773;&#20917;&#65292;&#38477;&#20302;&#20102;&#23427;&#20204;&#30340;&#21487;&#38752;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#20005;&#26684;&#30340;&#21487;&#20449;&#24230;&#26631;&#20934;&#65292;&#21363;&#20219;&#21153;&#31639;&#27861;&#21644;&#30005;&#36335;&#23454;&#29616;&#24517;&#39035;&#32463;&#36807;&#39564;&#35777;&#65292;&#32771;&#34385;&#21040;&#36793;&#30028;&#24773;&#20917;&#65292;&#24182;&#19988;&#27809;&#26377;&#24050;&#30693;&#30340;&#25925;&#38556;&#27169;&#24335;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#20351;&#29992;&#25968;&#23398;&#21644;&#36923;&#36753;&#35268;&#33539;&#30340;&#26694;&#26550;&#26469;&#26500;&#24314;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#21487;&#20197;&#35757;&#32451;&#20986;&#28385;&#36275;&#36825;&#19968;&#26631;&#20934;&#30340;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#19968;&#20010;n&#20301;&#25972;&#25968;&#21152;&#27861;&#27169;&#22411;&#36827;&#34892;&#20102;&#23436;&#20840;&#39564;&#35777;&#12290;&#20026;&#20102;&#23637;&#31034;&#32463;&#36807;&#39564;&#35777;&#30340;&#27169;&#22359;&#30340;&#37325;&#22797;&#20351;&#29992;&#24615;&#65292;&#25105;&#20204;&#23558;&#35757;&#32451;&#22909;&#30340;&#25972;&#25968;&#21152;&#27861;&#27169;&#22411;&#25554;&#20837;&#21040;&#19968;&#20010;&#26410;&#32463;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#65292;&#24182;&#35757;&#32451;&#32452;&#21512;&#27169;&#22411;&#21516;&#26102;&#25191;&#34892;&#21152;&#27861;&#21644;&#20943;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#21152;&#27861;&#30005;&#36335;&#22312;&#36825;&#20004;&#20010;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#37325;&#22797;&#20351;&#29992;&#65292;&#20174;&#32780;&#31616;&#21270;&#20102;&#26356;&#22797;&#26434;&#30340;&#20943;&#27861;&#27169;&#22411;&#30340;&#39564;&#35777;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#22914;&#20309;&#23558;&#32463;&#36807;&#39564;&#35777;&#30340;&#20219;&#21153;&#27169;&#22359;&#25554;&#20837;&#21040;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20197;&#21033;&#29992;&#27169;&#22411;&#30340;&#37325;&#22797;&#20351;&#29992;&#26469;&#25552;&#39640;&#21487;&#39564;&#35777;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language Models (LMs) are increasingly used for a wide range of prediction tasks, but their training can often neglect rare edge cases, reducing their reliability. Here, we define a stringent standard of trustworthiness whereby the task algorithm and circuit implementation must be verified, accounting for edge cases, with no known failure modes. We show that a transformer model can be trained to meet this standard if built using mathematically and logically specified frameworks. In this paper, we fully verify a model for n-digit integer addition. To exhibit the reusability of verified modules, we insert the trained integer addition model into an untrained model and train the combined model to perform both addition and subtraction. We find extensive reuse of the addition circuits for both tasks, easing verification of the more complex subtractor model. We discuss how inserting verified task modules into LMs can leverage model reuse to improve verifiability and trustworthiness of languag
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#20027;&#25195;&#25551;&#25506;&#38024;&#26174;&#24494;&#26415;&#20013;&#21021;&#22987;&#36873;&#25321;&#21644;&#24490;&#29615;&#24178;&#39044;&#23545;&#23398;&#20064;&#21160;&#21147;&#23398;&#30340;&#24433;&#21709;&#65292;&#24182;&#25506;&#35752;&#20102;&#8220;&#31181;&#23376;&#25928;&#24212;&#8221;&#21644;&#31181;&#23376;&#28857;&#24178;&#39044;&#30340;&#27010;&#24565;&#65292;&#23545;&#28145;&#24230;&#20869;&#26680;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2402.00071</link><description>&lt;p&gt;
&#25581;&#31034;&#33258;&#20027;&#25195;&#25551;&#25506;&#38024;&#26174;&#24494;&#26415;&#20013;&#21021;&#22987;&#36873;&#25321;&#21644;&#24490;&#29615;&#24178;&#39044;&#23545;&#23398;&#20064;&#21160;&#21147;&#23398;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Unraveling the Impact of Initial Choices and In-Loop Interventions on Learning Dynamics in Autonomous Scanning Probe Microscopy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#20027;&#25195;&#25551;&#25506;&#38024;&#26174;&#24494;&#26415;&#20013;&#21021;&#22987;&#36873;&#25321;&#21644;&#24490;&#29615;&#24178;&#39044;&#23545;&#23398;&#20064;&#21160;&#21147;&#23398;&#30340;&#24433;&#21709;&#65292;&#24182;&#25506;&#35752;&#20102;&#8220;&#31181;&#23376;&#25928;&#24212;&#8221;&#21644;&#31181;&#23376;&#28857;&#24178;&#39044;&#30340;&#27010;&#24565;&#65292;&#23545;&#28145;&#24230;&#20869;&#26680;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#23454;&#39564;&#65288;AE&#65289;&#30446;&#21069;&#30340;&#37325;&#28857;&#26159;&#24320;&#21457;&#26377;&#25928;&#36827;&#34892;AE&#30340;&#40065;&#26834;&#24037;&#20316;&#27969;&#12290;&#36825;&#38656;&#35201;&#26126;&#30830;&#23450;&#20041;&#30340;&#26041;&#27861;&#26469;&#25351;&#23548;AE&#36807;&#31243;&#65292;&#21253;&#25324;&#36229;&#21442;&#25968;&#35843;&#25972;&#31574;&#30053;&#21644;&#24037;&#20316;&#27969;&#24490;&#29615;&#20013;&#30340;&#39640;&#32423;&#20154;&#21592;&#24178;&#39044;&#12290;&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#33258;&#20027;&#25195;&#25551;&#25506;&#38024;&#26174;&#24494;&#26415;&#20013;&#28145;&#24230;&#20869;&#26680;&#23398;&#20064;&#65288;DKL&#65289;&#30340;&#23398;&#20064;&#21160;&#21147;&#23398;&#65292;&#20840;&#38754;&#38416;&#36848;&#20102;&#21021;&#22987;&#23454;&#39564;&#26465;&#20214;&#21644;&#24490;&#29615;&#24178;&#39044;&#23545;&#23398;&#20064;&#36807;&#31243;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#8220;&#31181;&#23376;&#25928;&#24212;&#8221;&#30340;&#27010;&#24565;&#65292;&#21363;&#21021;&#22987;&#23454;&#39564;&#35774;&#32622;&#23545;&#21518;&#32493;&#23398;&#20064;&#36712;&#36857;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AE&#20013;&#31181;&#23376;&#28857;&#24178;&#39044;&#30340;&#26041;&#27861;&#65292;&#20351;&#25805;&#20316;&#32773;&#33021;&#22815;&#24433;&#21709;&#25506;&#32034;&#36807;&#31243;&#12290;&#36890;&#36807;&#20351;&#29992;PbTiO3&#34180;&#33180;&#19978;&#30340;Piezoresponse&#21147;&#26174;&#24494;&#38236;&#65288;PFM&#65289;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#8220;&#31181;&#23376;&#25928;&#24212;&#8221;&#21644;&#24490;&#29615;&#31181;&#23376;&#24178;&#39044;&#23545;DKL&#30340;&#26377;&#25928;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
The current focus in Autonomous Experimentation (AE) is on developing robust workflows to conduct the AE effectively. This entails the need for well-defined approaches to guide the AE process, including strategies for hyperparameter tuning and high-level human interventions within the workflow loop. This paper presents a comprehensive analysis of the influence of initial experimental conditions and in-loop interventions on the learning dynamics of Deep Kernel Learning (DKL) within the realm of AE in Scanning Probe Microscopy. We explore the concept of 'seed effect', where the initial experiment setup has a substantial impact on the subsequent learning trajectory. Additionally, we introduce an approach of the seed point interventions in AE allowing the operator to influence the exploration process. Using a dataset from Piezoresponse Force Microscopy (PFM) on PbTiO3 thin films, we illustrate the impact of the 'seed effect' and in-loop seed interventions on the effectiveness of DKL in pre
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#35780;&#20272;&#20102;&#23558;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#21307;&#30103;&#31995;&#32479;&#30340;&#23433;&#20840;&#39118;&#38505;&#65292;&#29305;&#21035;&#26159;&#36830;&#25509;&#22806;&#22260;&#35774;&#22791;&#30340;&#36830;&#25509;&#31995;&#32479;&#12290;&#20316;&#32773;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#20102;&#36890;&#36807;&#21033;&#29992;&#34013;&#29273;&#36890;&#20449;&#28192;&#36947;&#28431;&#27934;&#23545;&#26426;&#22120;&#23398;&#20064;&#34880;&#31958;&#30417;&#27979;&#31995;&#32479;&#36827;&#34892;&#25915;&#20987;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#25351;&#20986;&#24403;&#21069;&#30340;&#39118;&#38505;&#35780;&#20272;&#25216;&#26415;&#19981;&#36275;&#20197;&#24212;&#23545;&#36825;&#20123;&#26032;&#30340;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2401.17136</link><description>&lt;p&gt;
&#31995;&#32479;&#35780;&#20272;AI/ML&#25903;&#25345;&#30340;&#36830;&#25509;&#20581;&#24247;&#31995;&#32479;&#30340;&#23433;&#20840;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Systematically Assessing the Security Risks of AI/ML-enabled Connected Healthcare Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17136
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#35780;&#20272;&#20102;&#23558;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#21307;&#30103;&#31995;&#32479;&#30340;&#23433;&#20840;&#39118;&#38505;&#65292;&#29305;&#21035;&#26159;&#36830;&#25509;&#22806;&#22260;&#35774;&#22791;&#30340;&#36830;&#25509;&#31995;&#32479;&#12290;&#20316;&#32773;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#20102;&#36890;&#36807;&#21033;&#29992;&#34013;&#29273;&#36890;&#20449;&#28192;&#36947;&#28431;&#27934;&#23545;&#26426;&#22120;&#23398;&#20064;&#34880;&#31958;&#30417;&#27979;&#31995;&#32479;&#36827;&#34892;&#25915;&#20987;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#25351;&#20986;&#24403;&#21069;&#30340;&#39118;&#38505;&#35780;&#20272;&#25216;&#26415;&#19981;&#36275;&#20197;&#24212;&#23545;&#36825;&#20123;&#26032;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#39046;&#22495;&#20013;&#65292;&#26426;&#22120;&#23398;&#20064;&#25903;&#25345;&#30340;&#31995;&#32479;&#30340;&#37319;&#29992;&#25968;&#37327;&#19981;&#26029;&#22686;&#21152;&#12290;&#34429;&#28982;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26377;&#24456;&#22810;&#22909;&#22788;&#65292;&#20294;&#23427;&#20063;&#25193;&#22823;&#20102;&#21307;&#30103;&#31995;&#32479;&#30340;&#23041;&#32961;&#38754;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#21307;&#30103;&#31995;&#32479;&#20013;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#36807;&#31243;&#20013;&#65292;&#29305;&#21035;&#26159;&#28041;&#21450;&#23558;&#26426;&#22120;&#23398;&#20064;&#24341;&#25806;&#19982;&#22810;&#20010;&#22806;&#22260;&#35774;&#22791;&#30456;&#36830;&#30340;&#36830;&#25509;&#31995;&#32479;&#20013;&#65292;&#23384;&#22312;&#23433;&#20840;&#39118;&#38505;&#65292;&#21487;&#33021;&#22312;&#25932;&#23545;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#23545;&#24739;&#32773;&#30340;&#20581;&#24247;&#36896;&#25104;&#21361;&#23475;&#12290;&#36825;&#20123;&#26032;&#30340;&#39118;&#38505;&#26469;&#33258;&#22806;&#22260;&#35774;&#22791;&#21644;&#36890;&#20449;&#28192;&#36947;&#30340;&#23433;&#20840;&#28431;&#27934;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#28436;&#31034;&#20102;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#36890;&#36807;&#24341;&#20837;&#25932;&#23545;&#25968;&#25454;&#28857;&#23545;&#26426;&#22120;&#23398;&#20064;&#34880;&#31958;&#30417;&#27979;&#31995;&#32479;&#36827;&#34892;&#25915;&#20987;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#25163;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#36830;&#25509;&#34880;&#31958;&#20202;&#19982;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20043;&#38388;&#30340;&#34013;&#29273;&#36890;&#20449;&#28192;&#36947;&#20013;&#24050;&#30693;&#30340;&#28431;&#27934;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#34920;&#26126;&#65292;&#24403;&#21069;&#30340;&#39118;&#38505;&#35780;&#20272;&#25216;&#26415;&#19981;&#36275;&#20197;&#24212;&#23545;&#36825;&#20123;&#26032;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
The adoption of machine-learning-enabled systems in the healthcare domain is on the rise. While the use of ML in healthcare has several benefits, it also expands the threat surface of medical systems. We show that the use of ML in medical systems, particularly connected systems that involve interfacing the ML engine with multiple peripheral devices, has security risks that might cause life-threatening damage to a patient's health in case of adversarial interventions. These new risks arise due to security vulnerabilities in the peripheral devices and communication channels. We present a case study where we demonstrate an attack on an ML-enabled blood glucose monitoring system by introducing adversarial data points during inference. We show that an adversary can achieve this by exploiting a known vulnerability in the Bluetooth communication channel connecting the glucose meter with the ML-enabled app. We further show that state-of-the-art risk assessment techniques are not adequate for i
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35775;&#38382;&#21407;&#22987;&#25968;&#25454;&#30340;&#27169;&#22411;-&#26080;&#20851;&#30693;&#35782;&#33976;&#39311;&#36807;&#31243;CAKE&#65292;&#21487;&#20197;&#27169;&#25311;&#28145;&#24230;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#29983;&#25104;&#22122;&#22768;&#21512;&#25104;&#26679;&#26412;&#23545;&#27604;&#22320;&#25193;&#25955;&#21040;&#27169;&#22411;&#30340;&#20915;&#31574;&#36793;&#30028;&#12290;</title><link>https://arxiv.org/abs/2306.02090</link><description>&lt;p&gt;
&#27809;&#26377;&#25968;&#25454;&#35775;&#38382;&#30340;&#28145;&#24230;&#20998;&#31867;&#22120;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Deep Classifier Mimicry without Data Access
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.02090
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35775;&#38382;&#21407;&#22987;&#25968;&#25454;&#30340;&#27169;&#22411;-&#26080;&#20851;&#30693;&#35782;&#33976;&#39311;&#36807;&#31243;CAKE&#65292;&#21487;&#20197;&#27169;&#25311;&#28145;&#24230;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#29983;&#25104;&#22122;&#22768;&#21512;&#25104;&#26679;&#26412;&#23545;&#27604;&#22320;&#25193;&#25955;&#21040;&#27169;&#22411;&#30340;&#20915;&#31574;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#39044;&#20808;&#35757;&#32451;&#27169;&#22411;&#30340;&#35775;&#38382;&#24050;&#32463;&#25104;&#20026;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#26631;&#20934;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#21487;&#33021;&#26080;&#27861;&#31561;&#21516;&#22320;&#33719;&#24471;&#27169;&#22411;&#35757;&#32451;&#25152;&#38656;&#30340;&#21407;&#22987;&#25968;&#25454;&#12290;&#36825;&#20351;&#24471;&#24494;&#35843;&#12289;&#21387;&#32553;&#27169;&#22411;&#12289;&#25345;&#32493;&#35843;&#25972;&#25110;&#36827;&#34892;&#20219;&#20309;&#20854;&#20182;&#31867;&#22411;&#30340;&#25968;&#25454;&#39537;&#21160;&#26356;&#26032;&#21464;&#24471;&#26497;&#20855;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#21487;&#33021;&#26080;&#38656;&#21407;&#22987;&#25968;&#25454;&#35775;&#38382;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#27604;&#25512;&#29702;&#30693;&#35782;&#25552;&#21462;&#65288;CAKE&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#30693;&#35782;&#33976;&#39311;&#36807;&#31243;&#65292;&#21487;&#20197;&#27169;&#25311;&#28145;&#24230;&#20998;&#31867;&#22120;&#32780;&#26080;&#38656;&#35775;&#38382;&#21407;&#22987;&#25968;&#25454;&#12290;&#20026;&#27492;&#65292;CAKE&#29983;&#25104;&#19968;&#23545;&#22122;&#22768;&#21512;&#25104;&#26679;&#26412;&#65292;&#24182;&#23558;&#23427;&#20204;&#23545;&#27604;&#22320;&#25193;&#25955;&#21040;&#27169;&#22411;&#30340;&#20915;&#31574;&#36793;&#30028;&#12290;&#25105;&#20204;&#36890;&#36807;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#21508;&#31181;&#26550;&#26500;&#36873;&#25321;&#22312;&#23454;&#35777;&#19978;&#35777;&#23454;&#20102;CAKE&#30340;&#26377;&#25928;&#24615;&#65292;&#20026;&#24191;&#27867;&#24212;&#29992;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.02090v2 Announce Type: replace-cross  Abstract: Access to pre-trained models has recently emerged as a standard across numerous machine learning domains. Unfortunately, access to the original data the models were trained on may not equally be granted. This makes it tremendously challenging to fine-tune, compress models, adapt continually, or to do any other type of data-driven update. We posit that original data access may however not be required. Specifically, we propose Contrastive Abductive Knowledge Extraction (CAKE), a model-agnostic knowledge distillation procedure that mimics deep classifiers without access to the original data. To this end, CAKE generates pairs of noisy synthetic samples and diffuses them contrastively toward a model's decision boundary. We empirically corroborate CAKE's effectiveness using several benchmark datasets and various architectural choices, paving the way for broad application.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#22270;&#32467;&#26500;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#35745;&#31639;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#26694;&#26550;&#65292;&#22522;&#20110;&#23616;&#37096;&#20540;&#20989;&#25968;&#30340;&#20998;&#24067;&#24335;RL&#26041;&#27861;&#22312;&#21327;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#38477;&#20302;&#12290;</title><link>https://arxiv.org/abs/2202.13046</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#35825;&#23548;&#30340;&#23616;&#37096;&#20540;&#20989;&#25968;&#30340;&#20998;&#24067;&#24335;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Distributed Multi-Agent Reinforcement Learning Based on Graph-Induced Local Value Functions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2202.13046
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#22270;&#32467;&#26500;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#35745;&#31639;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#26694;&#26550;&#65292;&#22522;&#20110;&#23616;&#37096;&#20540;&#20989;&#25968;&#30340;&#20998;&#24067;&#24335;RL&#26041;&#27861;&#22312;&#21327;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#29616;&#22823;&#35268;&#27169;&#21327;&#20316;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;(RL)&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#65306;(i)&#27599;&#20010;&#26234;&#33021;&#20307;&#21482;&#33021;&#35775;&#38382;&#26377;&#38480;&#30340;&#20449;&#24687;&#65307;(ii)&#30001;&#20110;&#32500;&#24230;&#35781;&#21650;&#65292;&#20250;&#20986;&#29616;&#25910;&#25947;&#25110;&#35745;&#31639;&#22797;&#26434;&#24615;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#35745;&#31639;&#39640;&#25928;&#30340;&#21327;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;(MARL)&#20998;&#24067;&#24335;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#35813;&#38382;&#39064;&#20013;&#28041;&#21450;&#30340;&#22270;&#32467;&#26500;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#25551;&#36848;MARL&#20013;&#19977;&#31181;&#31867;&#22411;&#26234;&#33021;&#20307;&#32806;&#21512;&#30340;&#19977;&#20010;&#32806;&#21512;&#22270;&#65292;&#20998;&#21035;&#26159;&#29366;&#24577;&#22270;&#12289;&#35266;&#27979;&#22270;&#21644;&#22870;&#21169;&#22270;&#12290;&#36890;&#36807;&#36827;&#19968;&#27493;&#32771;&#34385;&#36890;&#20449;&#22270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#32806;&#21512;&#22270;&#20013;&#27966;&#29983;&#30340;&#23616;&#37096;&#20540;&#20989;&#25968;&#30340;&#20998;&#24067;&#24335;RL&#26041;&#27861;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#22312;&#21069;&#36848;&#22235;&#20010;&#22270;&#19978;&#30340;&#29305;&#23450;&#26465;&#20214;&#19979;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290;&#31532;&#20108;&#31181;
&lt;/p&gt;
&lt;p&gt;
arXiv:2202.13046v4 Announce Type: replace-cross  Abstract: Achieving distributed reinforcement learning (RL) for large-scale cooperative multi-agent systems (MASs) is challenging because: (i) each agent has access to only limited information; (ii) issues on convergence or computational complexity emerge due to the curse of dimensionality. In this paper, we propose a general computationally efficient distributed framework for cooperative multi-agent reinforcement learning (MARL) by utilizing the structures of graphs involved in this problem. We introduce three coupling graphs describing three types of inter-agent couplings in MARL, namely, the state graph, the observation graph and the reward graph. By further considering a communication graph, we propose two distributed RL approaches based on local value-functions derived from the coupling graphs. The first approach is able to reduce sample complexity significantly under specific conditions on the aforementioned four graphs. The second
&lt;/p&gt;</description></item><item><title>ProbMCL&#26159;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#27010;&#29575;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#26631;&#31614;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#37319;&#29992;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#21644;&#28151;&#21512;&#23494;&#24230;&#32593;&#32476;&#65292;&#22312;&#25429;&#25417;&#26631;&#31614;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#30340;&#21516;&#26102;&#65292;&#38477;&#20302;&#20102;&#22797;&#26434;&#27169;&#22359;&#30340;&#35745;&#31639;&#38656;&#27714;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2401.01448</link><description>&lt;p&gt;
ProbMCL: &#31616;&#21333;&#30340;&#27010;&#29575;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#22810;&#26631;&#31614;&#35270;&#35273;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
ProbMCL: Simple Probabilistic Contrastive Learning for Multi-label Visual Classification. (arXiv:2401.01448v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01448
&lt;/p&gt;
&lt;p&gt;
ProbMCL&#26159;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#27010;&#29575;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#26631;&#31614;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#37319;&#29992;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#21644;&#28151;&#21512;&#23494;&#24230;&#32593;&#32476;&#65292;&#22312;&#25429;&#25417;&#26631;&#31614;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#30340;&#21516;&#26102;&#65292;&#38477;&#20302;&#20102;&#22797;&#26434;&#27169;&#22359;&#30340;&#35745;&#31639;&#38656;&#27714;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#21307;&#23398;&#24433;&#20687;&#31561;&#35768;&#22810;&#39046;&#22495;&#20013;&#65292;&#22810;&#26631;&#31614;&#22270;&#20687;&#20998;&#31867;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26368;&#36817;&#30340;&#36827;&#23637;&#24341;&#20837;&#20102;&#22522;&#20110;&#22270;&#21644;&#21464;&#21387;&#22120;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#24615;&#33021;&#24182;&#25429;&#25417;&#26631;&#31614;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#21253;&#21547;&#22797;&#26434;&#30340;&#27169;&#22359;&#65292;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#65292;&#24182;&#19988;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#27010;&#29575;&#22810;&#26631;&#31614;&#23545;&#27604;&#23398;&#20064;&#65288;ProbMCL&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#26631;&#31614;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#37319;&#29992;&#20102;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65292;&#26681;&#25454;&#20915;&#31574;&#38408;&#20540;&#23558;&#19982;&#38170;&#22270;&#20687;&#20855;&#26377;&#36275;&#22815;&#26631;&#31614;&#30340;&#26679;&#26412;&#24341;&#20837;&#27491;&#26679;&#26412;&#38598;&#12290;&#36825;&#31181;&#32467;&#26500;&#36890;&#36807;&#23558;&#27491;&#26679;&#26412;&#23545;&#30340;&#23884;&#20837;&#25289;&#36817;&#65292;&#24182;&#25512;&#31163;&#20302;&#20110;&#38408;&#20540;&#30340;&#36127;&#26679;&#26412;&#26469;&#25429;&#25417;&#26631;&#31614;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#28151;&#21512;&#23494;&#24230;&#32593;&#32476;&#34701;&#20837;&#23545;&#27604;&#23398;&#20064;&#20013;&#26469;&#22686;&#24378;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-label image classification presents a challenging task in many domains, including computer vision and medical imaging. Recent advancements have introduced graph-based and transformer-based methods to improve performance and capture label dependencies. However, these methods often include complex modules that entail heavy computation and lack interpretability. In this paper, we propose Probabilistic Multi-label Contrastive Learning (ProbMCL), a novel framework to address these challenges in multi-label image classification tasks. Our simple yet effective approach employs supervised contrastive learning, in which samples that share enough labels with an anchor image based on a decision threshold are introduced as a positive set. This structure captures label dependencies by pulling positive pair embeddings together and pushing away negative samples that fall below the threshold. We enhance representation learning by incorporating a mixture density network into contrastive learning 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#30701;&#26399;&#35745;&#21010;&#29983;&#25104;&#21644;&#36873;&#25321;&#19982;&#20998;&#24067;&#24335;&#20248;&#21270;&#20197;&#21450;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#28176;&#36827;&#26041;&#27861;&#65292;&#29992;&#20110;&#26080;&#20154;&#26426;&#30340;&#21327;&#35843;&#21644;&#35268;&#21010;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.09852</link><description>&lt;p&gt;
&#30701;&#26399;&#19982;&#38271;&#26399;&#26080;&#20154;&#26426;&#21327;&#35843;&#65306;&#20998;&#24067;&#24335;&#20248;&#21270;&#19982;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20132;&#27719;
&lt;/p&gt;
&lt;p&gt;
Short vs. Long-term Coordination of Drones: When Distributed Optimization Meets Deep Reinforcement Learning. (arXiv:2311.09852v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.09852
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#30701;&#26399;&#35745;&#21010;&#29983;&#25104;&#21644;&#36873;&#25321;&#19982;&#20998;&#24067;&#24335;&#20248;&#21270;&#20197;&#21450;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#28176;&#36827;&#26041;&#27861;&#65292;&#29992;&#20110;&#26080;&#20154;&#26426;&#30340;&#21327;&#35843;&#21644;&#35268;&#21010;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26234;&#33021;&#22478;&#24066;&#20013;&#65292;&#25903;&#25345;&#20805;&#30005;&#25216;&#26415;&#30340;&#33258;&#20027;&#20132;&#20114;&#24335;&#26080;&#20154;&#26426;&#32676;&#21487;&#20197;&#25552;&#20379;&#24341;&#20154;&#27880;&#30446;&#30340;&#24863;&#30693;&#33021;&#21147;&#65292;&#20363;&#22914;&#20132;&#36890;&#30417;&#27979;&#21644;&#28798;&#38590;&#21709;&#24212;&#12290;&#29616;&#26377;&#26041;&#27861;&#65292;&#21253;&#25324;&#20998;&#24067;&#24335;&#20248;&#21270;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#65292;&#26088;&#22312;&#21327;&#35843;&#26080;&#20154;&#26426;&#20197;&#23454;&#29616;&#25104;&#26412;&#25928;&#30410;&#39640;&#12289;&#39640;&#36136;&#37327;&#30340;&#23548;&#33322;&#12289;&#24863;&#30693;&#21644;&#20805;&#30005;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#65306;&#30701;&#26399;&#20248;&#21270;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#30340;&#24847;&#22806;&#21464;&#21270;&#19979;&#24182;&#19981;&#26377;&#25928;&#65292;&#32780;&#38271;&#26399;&#23398;&#20064;&#32570;&#20047;&#21487;&#25193;&#23637;&#24615;&#12289;&#38887;&#24615;&#21644;&#28789;&#27963;&#24615;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28176;&#36827;&#26041;&#27861;&#65292;&#23558;&#22522;&#20110;&#20998;&#24067;&#24335;&#20248;&#21270;&#30340;&#30701;&#26399;&#35745;&#21010;&#29983;&#25104;&#21644;&#36873;&#25321;&#19982;&#22522;&#20110;DRL&#30340;&#38271;&#26399;&#39134;&#34892;&#26041;&#21521;&#30340;&#25112;&#30053;&#35843;&#24230;&#30456;&#32467;&#21512;&#12290;&#36890;&#36807;&#23545;&#20174;&#29616;&#23454;&#22478;&#24066;&#31227;&#21160;&#20013;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Swarms of autonomous interactive drones, with the support of recharging technology, can provide compelling sensing capabilities in Smart Cities, such as traffic monitoring and disaster response. Existing approaches, including distributed optimization and deep reinforcement learning (DRL), aim to coordinate drones to achieve cost-effective, high-quality navigation, sensing, and charging. However, they face grand challenges: short-term optimization is not effective in dynamic environments with unanticipated changes, while long-term learning lacks scalability, resilience, and flexibility. To bridge this gap, this paper introduces a new progressive approach that combines short-term plan generation and selection based on distributed optimization with a DRL-based long-term strategic scheduling of flying direction. Extensive experimentation with datasets generated from realistic urban mobility underscores an outstanding performance of the proposed solution compared to state-of-the-art. We als
&lt;/p&gt;</description></item><item><title>ADMarker&#26159;&#19968;&#20010;&#22810;&#27169;&#24335;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#65292;&#29992;&#20110;&#22312;&#33258;&#28982;&#29983;&#27963;&#29615;&#22659;&#20013;&#30417;&#27979;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#25968;&#23383;&#29983;&#29289;&#26631;&#24535;&#29289;&#12290;&#23427;&#20855;&#26377;&#26032;&#39062;&#30340;&#32852;&#37030;&#23398;&#20064;&#26550;&#26500;&#65292;&#33021;&#22815;&#20934;&#30830;&#26816;&#27979;&#20986;&#25968;&#23383;&#29983;&#29289;&#26631;&#24535;&#29289;&#65292;&#24182;&#22312;&#20020;&#24202;&#35797;&#39564;&#20013;&#23637;&#31034;&#20986;&#39640;&#20934;&#30830;&#29575;&#21644;&#26089;&#26399;AD&#35782;&#21035;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.15301</link><description>&lt;p&gt;
ADMarker: &#19968;&#31181;&#22810;&#27169;&#24335;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#65292;&#29992;&#20110;&#30417;&#27979;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#25968;&#23383;&#29983;&#29289;&#26631;&#24535;&#29289;
&lt;/p&gt;
&lt;p&gt;
ADMarker: A Multi-Modal Federated Learning System for Monitoring Digital Biomarkers of Alzheimer's Disease. (arXiv:2310.15301v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15301
&lt;/p&gt;
&lt;p&gt;
ADMarker&#26159;&#19968;&#20010;&#22810;&#27169;&#24335;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#65292;&#29992;&#20110;&#22312;&#33258;&#28982;&#29983;&#27963;&#29615;&#22659;&#20013;&#30417;&#27979;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#25968;&#23383;&#29983;&#29289;&#26631;&#24535;&#29289;&#12290;&#23427;&#20855;&#26377;&#26032;&#39062;&#30340;&#32852;&#37030;&#23398;&#20064;&#26550;&#26500;&#65292;&#33021;&#22815;&#20934;&#30830;&#26816;&#27979;&#20986;&#25968;&#23383;&#29983;&#29289;&#26631;&#24535;&#29289;&#65292;&#24182;&#22312;&#20020;&#24202;&#35797;&#39564;&#20013;&#23637;&#31034;&#20986;&#39640;&#20934;&#30830;&#29575;&#21644;&#26089;&#26399;AD&#35782;&#21035;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65288;AD&#65289;&#21450;&#30456;&#20851;&#30196;&#21574;&#30151;&#30001;&#20110;&#20154;&#21475;&#32769;&#40836;&#21270;&#32780;&#25104;&#20026;&#20840;&#29699;&#26085;&#30410;&#20005;&#37325;&#30340;&#20581;&#24247;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;ADMarker&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#23558;&#22810;&#27169;&#24335;&#20256;&#24863;&#22120;&#21644;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#25972;&#21512;&#36215;&#26469;&#65292;&#20197;&#22312;&#33258;&#28982;&#29983;&#27963;&#29615;&#22659;&#20013;&#26816;&#27979;&#22810;&#32500;AD&#25968;&#23383;&#29983;&#29289;&#26631;&#24535;&#29289;&#30340;&#31471;&#21040;&#31471;&#31995;&#32479;&#12290;ADMarker&#20855;&#26377;&#19968;&#31181;&#26032;&#39062;&#30340;&#19977;&#38454;&#27573;&#22810;&#27169;&#24335;&#32852;&#37030;&#23398;&#20064;&#26550;&#26500;&#65292;&#33021;&#22815;&#20197;&#20445;&#25252;&#38544;&#31169;&#30340;&#26041;&#24335;&#20934;&#30830;&#26816;&#27979;&#25968;&#23383;&#29983;&#29289;&#26631;&#24535;&#29289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20849;&#21516;&#35299;&#20915;&#20102;&#25968;&#25454;&#26631;&#31614;&#26377;&#38480;&#12289;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#26377;&#38480;&#30340;&#35745;&#31639;&#36164;&#28304;&#31561;&#20960;&#20010;&#20027;&#35201;&#30340;&#29616;&#23454;&#19990;&#30028;&#25361;&#25112;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#32039;&#20945;&#30340;&#22810;&#27169;&#24335;&#30828;&#20214;&#31995;&#32479;&#65292;&#24182;&#22312;&#19968;&#20010;&#20026;&#26399;&#22235;&#21608;&#30340;&#20020;&#24202;&#35797;&#39564;&#20013;&#23558;&#20854;&#37096;&#32626;&#22312;91&#21517;&#32769;&#24180;&#21442;&#19982;&#32773;&#36523;&#19978;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;ADMarker&#33021;&#22815;&#20934;&#30830;&#26816;&#27979;&#20986;&#20840;&#38754;&#30340;&#25968;&#23383;&#29983;&#29289;&#26631;&#24535;&#29289;&#65292;&#20934;&#30830;&#29575;&#39640;&#36798;93.8&#65285;&#65292;&#24182;&#20197;&#24179;&#22343;88.9&#65285;&#30340;&#20934;&#30830;&#29575;&#35782;&#21035;&#20986;&#26089;&#26399;AD&#12290;ADMarker&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#24179;&#21488;&#65292;&#21487;&#20197;&#22312;&#30417;&#27979;AD&#24739;&#32773;&#26102;&#25552;&#20379;&#20934;&#30830;&#21644;&#39640;&#25928;&#30340;&#25968;&#25454;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Alzheimer's Disease (AD) and related dementia are a growing global health challenge due to the aging population. In this paper, we present ADMarker, the first end-to-end system that integrates multi-modal sensors and new federated learning algorithms for detecting multidimensional AD digital biomarkers in natural living environments. ADMarker features a novel three-stage multi-modal federated learning architecture that can accurately detect digital biomarkers in a privacy-preserving manner. Our approach collectively addresses several major real-world challenges, such as limited data labels, data heterogeneity, and limited computing resources. We built a compact multi-modality hardware system and deployed it in a four-week clinical trial involving 91 elderly participants. The results indicate that ADMarker can accurately detect a comprehensive set of digital biomarkers with up to 93.8% accuracy and identify early AD with an average of 88.9% accuracy. ADMarker offers a new platform that 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38468;&#21152;&#32467;&#26500;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;(ABNN)&#65292;&#36890;&#36807;&#22312;&#20027;&#24178;&#32593;&#32476;&#20013;&#25972;&#21512;&#36275;&#22815;&#20998;&#24067;&#22806;&#25968;&#25454;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#26469;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#23545;&#19981;&#30830;&#23450;&#24615;&#30340;&#25429;&#25417;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.13027</link><description>&lt;p&gt;
&#36890;&#36807;&#38468;&#21152;&#20214;&#21464;&#24471;&#36125;&#21494;&#26031;&#65292;&#25429;&#25417;&#26356;&#22810;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Be Bayesian by Attachments to Catch More Uncertainty. (arXiv:2310.13027v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13027
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38468;&#21152;&#32467;&#26500;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;(ABNN)&#65292;&#36890;&#36807;&#22312;&#20027;&#24178;&#32593;&#32476;&#20013;&#25972;&#21512;&#36275;&#22815;&#20998;&#24067;&#22806;&#25968;&#25454;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#26469;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#23545;&#19981;&#30830;&#23450;&#24615;&#30340;&#25429;&#25417;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;(BNNs)&#24050;&#25104;&#20026;&#19981;&#30830;&#23450;&#24615;&#35780;&#20272;&#30340;&#26377;&#24076;&#26395;&#26041;&#27861;&#20043;&#19968;&#65292;&#30001;&#20110;&#20854;&#22362;&#23454;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#28982;&#32780;&#65292;BNNs&#30340;&#24615;&#33021;&#21463;&#21040;&#25429;&#25417;&#19981;&#30830;&#23450;&#24615;&#30340;&#33021;&#21147;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38468;&#21152;&#32467;&#26500;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;(ABNN)&#65292;&#36890;&#36807;&#38468;&#21152;&#32467;&#26500;&#20174;&#36275;&#22815;&#20998;&#24067;&#22806;&#30340;&#25968;&#25454;(OOD)&#20013;&#25429;&#25417;&#26356;&#22810;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#26681;&#25454;&#20808;&#39564;&#20998;&#24067;&#20026;OOD&#25968;&#25454;&#30340;&#19981;&#30830;&#23450;&#24615;&#26500;&#24314;&#20102;&#19968;&#20010;&#25968;&#23398;&#25551;&#36848;&#65292;&#28982;&#21518;&#24320;&#21457;&#20102;&#19968;&#20010;&#38468;&#21152;&#30340;&#36125;&#21494;&#26031;&#32467;&#26500;&#23558;OOD&#25968;&#25454;&#30340;&#19981;&#30830;&#23450;&#24615;&#25972;&#21512;&#21040;&#20027;&#24178;&#32593;&#32476;&#20013;&#12290;ABNN&#30001;&#26399;&#26395;&#27169;&#22359;&#21644;&#33509;&#24178;&#20998;&#24067;&#27169;&#22359;&#32452;&#25104;&#12290;&#26399;&#26395;&#27169;&#22359;&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;&#21407;&#22987;&#20219;&#21153;&#30340;&#20027;&#24178;&#28145;&#24230;&#32593;&#32476;&#65292;&#32780;&#20998;&#24067;&#27169;&#22359;&#21017;&#26159;&#20316;&#20026;&#20027;&#24178;&#30340;&#38468;&#21152;&#32467;&#26500;&#30340;&#23567;&#36125;&#21494;&#26031;&#32467;&#26500;&#12290;&#29305;&#21035;&#22320;&#65292;&#36825;&#20123;&#20998;&#24067;&#27169;&#22359;&#30340;&#30446;&#30340;&#26159;&#26816;&#27979;&#21644;&#20256;&#25773;OOD&#25968;&#25454;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#25972;&#20307;&#32593;&#32476;&#30340;&#36125;&#21494;&#26031;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian Neural Networks (BNNs) have become one of the promising approaches for uncertainty estimation due to the solid theorical foundations. However, the performance of BNNs is affected by the ability of catching uncertainty. Instead of only seeking the distribution of neural network weights by in-distribution (ID) data, in this paper, we propose a new Bayesian Neural Network with an Attached structure (ABNN) to catch more uncertainty from out-of-distribution (OOD) data. We first construct a mathematical description for the uncertainty of OOD data according to the prior distribution, and then develop an attached Bayesian structure to integrate the uncertainty of OOD data into the backbone network. ABNN is composed of an expectation module and several distribution modules. The expectation module is a backbone deep network which focuses on the original task, and the distribution modules are mini Bayesian structures which serve as attachments of the backbone. In particular, the distribu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26500;&#24314;&#20102;&#25972;&#25968;&#20540;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#31070;&#32463;&#20284;&#28982;&#36817;&#20284;&#26041;&#27861;&#65292;&#20351;&#29992;&#22240;&#26524;&#21367;&#31215;&#24182;&#34892;&#35780;&#20272;&#25972;&#20010;&#26102;&#38388;&#24207;&#21015;&#30340;&#20284;&#28982;&#65292;&#23454;&#29616;&#20102;&#23545;&#29983;&#24577;&#23398;&#21644;&#27969;&#34892;&#30149;&#23398;&#27169;&#22411;&#36827;&#34892;&#20934;&#30830;&#25512;&#26029;&#24182;&#26174;&#33879;&#21152;&#24555;&#35745;&#31639;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.12544</link><description>&lt;p&gt;
&#25972;&#25968;&#20540;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#31070;&#32463;&#20284;&#28982;&#36817;&#20284;
&lt;/p&gt;
&lt;p&gt;
Neural Likelihood Approximation for Integer Valued Time Series Data. (arXiv:2310.12544v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12544
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26500;&#24314;&#20102;&#25972;&#25968;&#20540;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#31070;&#32463;&#20284;&#28982;&#36817;&#20284;&#26041;&#27861;&#65292;&#20351;&#29992;&#22240;&#26524;&#21367;&#31215;&#24182;&#34892;&#35780;&#20272;&#25972;&#20010;&#26102;&#38388;&#24207;&#21015;&#30340;&#20284;&#28982;&#65292;&#23454;&#29616;&#20102;&#23545;&#29983;&#24577;&#23398;&#21644;&#27969;&#34892;&#30149;&#23398;&#27169;&#22411;&#36827;&#34892;&#20934;&#30830;&#25512;&#26029;&#24182;&#26174;&#33879;&#21152;&#24555;&#35745;&#31639;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29289;&#29702;&#21644;&#29983;&#29289;&#31185;&#23398;&#20013;&#65292;&#23450;&#20041;&#22312;&#25972;&#25968;&#20540;&#29366;&#24577;&#31354;&#38388;&#19978;&#30340;&#38543;&#26426;&#36807;&#31243;&#24456;&#24120;&#35265;&#12290;&#36825;&#20123;&#27169;&#22411;&#29992;&#20110;&#25429;&#25417;&#23567;&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;&#65292;&#20854;&#20013;&#20010;&#20307;&#32676;&#20307;&#30340;&#20010;&#20307;&#23646;&#24615;&#19981;&#33021;&#34987;&#24573;&#35270;&#65292;&#38543;&#26426;&#25928;&#24212;&#24456;&#37325;&#35201;&#12290;&#30001;&#20110;&#20284;&#28982;&#30340;&#22797;&#26434;&#24615;&#65292;&#20174;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#25512;&#26029;&#36825;&#20123;&#27169;&#22411;&#30340;&#21442;&#25968;&#26159;&#22256;&#38590;&#30340;&#65307;&#30446;&#21069;&#30340;&#26041;&#27861;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#27169;&#25311;&#65292;&#35745;&#31639;&#25104;&#26412;&#38750;&#24120;&#39640;&#26114;&#65292;&#20197;&#33267;&#20110;&#38590;&#20197;&#23454;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#22240;&#26524;&#21367;&#31215;&#26500;&#24314;&#20102;&#29992;&#20110;&#25972;&#25968;&#20540;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#31070;&#32463;&#20284;&#28982;&#36817;&#20284;&#26041;&#27861;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#24182;&#34892;&#35780;&#20272;&#25972;&#20010;&#26102;&#38388;&#24207;&#21015;&#30340;&#20284;&#28982;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#19968;&#20123;&#29983;&#24577;&#23398;&#21644;&#27969;&#34892;&#30149;&#23398;&#27169;&#22411;&#36827;&#34892;&#25512;&#26029;&#26469;&#28436;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#33021;&#22815;&#20934;&#30830;&#22320;&#36817;&#20284;&#30495;&#23454;&#30340;&#21518;&#39564;&#27010;&#29575;&#65292;&#21516;&#26102;&#22312;&#24403;&#21069;&#26041;&#27861;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26174;&#33879;&#30340;&#35745;&#31639;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic processes defined on integer valued state spaces are popular within the physical and biological sciences. These models are necessary for capturing the dynamics of small systems where the individual nature of the populations cannot be ignored and stochastic effects are important. The inference of the parameters of such models, from time series data, is difficult due to intractability of the likelihood; current methods, based on simulations of the underlying model, can be so computationally expensive as to be prohibitive. In this paper we construct a neural likelihood approximation for integer valued time series data using causal convolutions, which allows us to evaluate the likelihood of the whole time series in parallel. We demonstrate our method by performing inference on a number of ecological and epidemiological models, showing that we can accurately approximate the true posterior while achieving significant computational speed ups in situations where current methods stru
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#36807;&#24230;&#35760;&#24518;&#38382;&#39064;&#65292;&#21457;&#29616;&#20854;&#20250;&#25439;&#23475;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#26041;&#27861;&#32508;&#21512;&#24615;&#22320;&#20943;&#36731;&#19981;&#21516;&#31867;&#22411;&#30340;&#36807;&#25311;&#21512;&#12290;</title><link>http://arxiv.org/abs/2310.08847</link><description>&lt;p&gt;
&#20851;&#20110;&#33258;&#28982;&#12289;&#40065;&#26834;&#21644;&#28798;&#38590;&#24615;&#36807;&#25311;&#21512;&#20013;&#30340;&#36807;&#24230;&#35760;&#24518;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
On the Over-Memorization During Natural, Robust and Catastrophic Overfitting. (arXiv:2310.08847v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08847
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#36807;&#24230;&#35760;&#24518;&#38382;&#39064;&#65292;&#21457;&#29616;&#20854;&#20250;&#25439;&#23475;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#26041;&#27861;&#32508;&#21512;&#24615;&#22320;&#20943;&#36731;&#19981;&#21516;&#31867;&#22411;&#30340;&#36807;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#25311;&#21512;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#20135;&#29983;&#20102;&#36127;&#38754;&#24433;&#21709;&#65292;&#26080;&#35770;&#26159;&#22312;&#33258;&#28982;&#35757;&#32451;&#36824;&#26159;&#23545;&#25239;&#24615;&#35757;&#32451;&#20013;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#38590;&#20197;&#19968;&#33268;&#22320;&#35299;&#20915;&#19981;&#21516;&#31867;&#22411;&#30340;&#36807;&#25311;&#21512;&#65292;&#36890;&#24120;&#35774;&#35745;&#20102;&#38024;&#23545;&#33258;&#28982;&#27169;&#24335;&#25110;&#23545;&#25239;&#27169;&#24335;&#30340;&#31574;&#30053;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#32479;&#19968;&#30340;&#35270;&#35282;&#65292;&#20165;&#20851;&#27880;&#33258;&#28982;&#27169;&#24335;&#65292;&#21435;&#25506;&#32034;&#19981;&#21516;&#31867;&#22411;&#30340;&#36807;&#25311;&#21512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;DNN&#20013;&#30340;&#35760;&#24518;&#25928;&#24212;&#65292;&#24182;&#25581;&#31034;&#20102;&#19968;&#31181;&#31216;&#20026;&#36807;&#24230;&#35760;&#24518;&#30340;&#20849;&#21516;&#34892;&#20026;&#65292;&#36825;&#20250;&#25439;&#23475;&#23427;&#20204;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36825;&#31181;&#34892;&#20026;&#34920;&#29616;&#20026;DNN&#31361;&#28982;&#23545;&#26576;&#20123;&#35757;&#32451;&#27169;&#24335;&#20135;&#29983;&#39640;&#32622;&#20449;&#24230;&#30340;&#39044;&#27979;&#65292;&#24182;&#23545;&#20854;&#20445;&#25345;&#25345;&#20037;&#35760;&#24518;&#12290;&#27492;&#22806;&#65292;&#24403;DNN&#36807;&#24230;&#35760;&#24518;&#19968;&#31181;&#23545;&#25239;&#27169;&#24335;&#26102;&#65292;&#23427;&#20204;&#24448;&#24448;&#21516;&#26102;&#23637;&#29616;&#20986;&#23545;&#24212;&#33258;&#28982;&#27169;&#24335;&#30340;&#39640;&#32622;&#20449;&#24230;&#39044;&#27979;&#12290;&#36825;&#20123;&#21457;&#29616;&#28608;&#21169;&#25105;&#20204;&#32508;&#21512;&#24615;&#22320;&#20943;&#36731;&#19981;&#21516;&#31867;&#22411;&#30340;&#36807;&#25311;&#21512;&#65292;&#38459;&#30861;&#36807;&#24230;&#35760;&#24518;&#34892;&#20026;&#30340;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;
Overfitting negatively impacts the generalization ability of deep neural networks (DNNs) in both natural and adversarial training. Existing methods struggle to consistently address different types of overfitting, typically designing strategies that focus separately on either natural or adversarial patterns. In this work, we adopt a unified perspective by solely focusing on natural patterns to explore different types of overfitting. Specifically, we examine the memorization effect in DNNs and reveal a shared behaviour termed over-memorization, which impairs their generalization capacity. This behaviour manifests as DNNs suddenly becoming high-confidence in predicting certain training patterns and retaining a persistent memory for them. Furthermore, when DNNs over-memorize an adversarial pattern, they tend to simultaneously exhibit high-confidence prediction for the corresponding natural pattern. These findings motivate us to holistically mitigate different types of overfitting by hinder
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#22522;&#20110;&#24605;&#32500;&#38142;&#30340;Transformer&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#36890;&#36807;&#20801;&#35768;&#20351;&#29992;&#20013;&#38388;&#29983;&#25104;&#30340;&#26041;&#24335;&#25552;&#39640;&#20102;Transformer&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#32447;&#24615;&#25968;&#37327;&#30340;&#35299;&#30721;&#27493;&#39588;&#22312;&#26631;&#20934;&#35745;&#31639;&#22797;&#26434;&#24230;&#19979;&#22686;&#21152;&#20102;&#26126;&#26174;&#30340;&#26032;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.07923</link><description>&lt;p&gt;
&#22522;&#20110;&#24605;&#32500;&#38142;&#30340;Transformer&#30340;&#34920;&#36798;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
The Expresssive Power of Transformers with Chain of Thought. (arXiv:2310.07923v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#22522;&#20110;&#24605;&#32500;&#38142;&#30340;Transformer&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#36890;&#36807;&#20801;&#35768;&#20351;&#29992;&#20013;&#38388;&#29983;&#25104;&#30340;&#26041;&#24335;&#25552;&#39640;&#20102;Transformer&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#32447;&#24615;&#25968;&#37327;&#30340;&#35299;&#30721;&#27493;&#39588;&#22312;&#26631;&#20934;&#35745;&#31639;&#22797;&#26434;&#24230;&#19979;&#22686;&#21152;&#20102;&#26126;&#26174;&#30340;&#26032;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#29702;&#35770;&#30740;&#31350;&#21457;&#29616;&#20102;&#19968;&#20123;&#20986;&#20154;&#24847;&#26009;&#22320;&#31616;&#21333;&#30340;&#25512;&#29702;&#38382;&#39064;&#65292;&#20363;&#22914;&#26816;&#26597;&#22270;&#20013;&#26159;&#21542;&#23384;&#22312;&#36830;&#25509;&#30340;&#20004;&#20010;&#33410;&#28857;&#65292;&#25110;&#27169;&#25311;&#26377;&#38480;&#29366;&#24577;&#26426;&#65292;&#36825;&#20123;&#38382;&#39064;&#34987;&#35777;&#26126;&#26080;&#27861;&#30001;&#31435;&#21363;&#35835;&#21462;&#36755;&#20837;&#21518;&#22238;&#31572;&#30340;&#26631;&#20934;Transformer&#35299;&#20915;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#36890;&#36807;&#20801;&#35768;Transformer&#20351;&#29992;&#8220;&#24605;&#32500;&#38142;&#8221;&#25110;&#8220;&#33609;&#31295;&#32440;&#8221;&#65292;&#21363;&#22312;&#22238;&#31572;&#20043;&#21069;&#29983;&#25104;&#24182;&#20381;&#36182;&#19968;&#31995;&#21015;&#20013;&#38388;token&#65292;&#21487;&#20197;&#25913;&#21892;&#20854;&#25512;&#29702;&#33021;&#21147;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#38382;&#65306;&#36825;&#31181;&#20013;&#38388;&#29983;&#25104;&#26159;&#21542;&#20174;&#26681;&#26412;&#19978;&#25193;&#23637;&#20102;&#20165;&#26377;&#35299;&#30721;&#22120;&#30340;Transformer&#30340;&#35745;&#31639;&#33021;&#21147;&#65311;&#25105;&#20204;&#34920;&#26126;&#31572;&#26696;&#26159;&#32943;&#23450;&#30340;&#65292;&#20294;&#22686;&#21152;&#30340;&#31243;&#24230;&#20851;&#38190;&#21462;&#20915;&#20110;&#20013;&#38388;&#29983;&#25104;&#30340;&#25968;&#37327;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#21457;&#29616;&#30456;&#23545;&#20110;&#36755;&#20837;&#38271;&#24230;&#26469;&#35828;&#65292;&#20855;&#26377;&#23545;&#25968;&#32423;&#35299;&#30721;&#27493;&#39588;&#30340;Transformer&#35299;&#30721;&#22120;&#20165;&#30053;&#24494;&#25512;&#21160;&#20102;&#26631;&#20934;Transformer&#30340;&#26497;&#38480;&#65292;&#32780;&#32447;&#24615;&#25968;&#37327;&#30340;&#35299;&#30721;&#27493;&#39588;&#21017;&#22686;&#21152;&#20102;&#26126;&#26174;&#30340;&#26032;&#33021;&#21147;&#65288;&#22312;&#26631;&#20934;&#35745;&#31639;&#22797;&#26434;&#24230;&#19979;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent theoretical work has identified surprisingly simple reasoning problems, such as checking if two nodes in a graph are connected or simulating finite-state machines, that are provably unsolvable by standard transformers that answer immediately after reading their input. However, in practice, transformers' reasoning can be improved by allowing them to use a "chain of thought" or "scratchpad", i.e., generate and condition on a sequence of intermediate tokens before answering. Motivated by this, we ask: Does such intermediate generation fundamentally extend the computational power of a decoder-only transformer? We show that the answer is yes, but the amount of increase depends crucially on the amount of intermediate generation. For instance, we find that transformer decoders with a logarithmic number of decoding steps (w.r.t. the input length) push the limits of standard transformers only slightly, while a linear number of decoding steps adds a clear new ability (under standard compl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#36716;&#21270;&#20026;&#23567;&#22411;&#21512;&#25104;&#26679;&#26412;&#65292;&#20197;&#25903;&#25345;&#39640;&#25928;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#12290;&#36890;&#36807;&#26368;&#23567;&#21270;&#27169;&#22411;&#23545;&#21512;&#25104;&#26679;&#26412;&#30340;&#34920;&#31034;&#21644;&#21487;&#23398;&#20064;&#30446;&#26631;&#29305;&#24449;&#34920;&#31034;&#20043;&#38388;&#30340;&#22343;&#26041;&#35823;&#24046;&#65292;&#35299;&#20915;&#20102;&#21512;&#25104;&#26679;&#26412;&#26799;&#24230;&#20559;&#24046;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.06511</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#25968;&#25454;&#38598;&#33976;&#39311;&#29992;&#20110;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Dataset Distillation for Transfer Learning. (arXiv:2310.06511v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06511
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#36716;&#21270;&#20026;&#23567;&#22411;&#21512;&#25104;&#26679;&#26412;&#65292;&#20197;&#25903;&#25345;&#39640;&#25928;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#12290;&#36890;&#36807;&#26368;&#23567;&#21270;&#27169;&#22411;&#23545;&#21512;&#25104;&#26679;&#26412;&#30340;&#34920;&#31034;&#21644;&#21487;&#23398;&#20064;&#30446;&#26631;&#29305;&#24449;&#34920;&#31034;&#20043;&#38388;&#30340;&#22343;&#26041;&#35823;&#24046;&#65292;&#35299;&#20915;&#20102;&#21512;&#25104;&#26679;&#26412;&#26799;&#24230;&#20559;&#24046;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;&#22312;&#23558;&#22823;&#22411;&#25968;&#25454;&#38598;&#36716;&#21270;&#20026;&#23569;&#37327;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;&#26679;&#26412;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24182;&#19981;&#34987;&#35774;&#35745;&#29992;&#20110;&#20135;&#29983;&#19968;&#20010;&#36866;&#29992;&#20110;&#20419;&#36827;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#33976;&#39311;&#25968;&#25454;&#38598;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#33976;&#39311;&#20026;&#19968;&#32452;&#23567;&#22411;&#21512;&#25104;&#26679;&#26412;&#20197;&#29992;&#20110;&#39640;&#25928;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#30340;&#26032;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#22312;&#26420;&#32032;&#21452;&#23618;&#20248;&#21270;&#20013;&#65292;&#21512;&#25104;&#26679;&#26412;&#30456;&#23545;&#20110;&#33258;&#30417;&#30563;&#30446;&#26631;&#30340;&#26799;&#24230;&#26159;&#8220;&#26377;&#20559;&#8221;&#30340;&#65292;&#36825;&#26159;&#30001;&#20110;&#25968;&#25454;&#22686;&#24378;&#25110;&#36974;&#34109;&#24341;&#36215;&#30340;&#38543;&#26426;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26368;&#23567;&#21270;&#27169;&#22411;&#23545;&#21512;&#25104;&#26679;&#26412;&#30340;&#34920;&#31034;&#21644;&#30456;&#24212;&#30340;&#21487;&#23398;&#20064;&#30446;&#26631;&#29305;&#24449;&#34920;&#31034;&#20043;&#38388;&#30340;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#20316;&#20026;&#20869;&#37096;&#30446;&#26631;&#65292;&#36825;&#19981;&#24341;&#20837;&#20219;&#20309;&#38543;&#26426;&#24615;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21160;&#26426;&#26159;&#36890;&#36807;&#25552;&#20986;&#30340;&#20869;&#37096;&#20248;&#21270;&#33719;&#24471;&#30340;&#27169;&#22411;&#21487;&#20197;&#27169;&#20223;...
&lt;/p&gt;
&lt;p&gt;
Dataset distillation methods have achieved remarkable success in distilling a large dataset into a small set of representative samples. However, they are not designed to produce a distilled dataset that can be effectively used for facilitating self-supervised pre-training. To this end, we propose a novel problem of distilling an unlabeled dataset into a set of small synthetic samples for efficient self-supervised learning (SSL). We first prove that a gradient of synthetic samples with respect to a SSL objective in naive bilevel optimization is \textit{biased} due to the randomness originating from data augmentations or masking. To address this issue, we propose to minimize the mean squared error (MSE) between a model's representations of the synthetic examples and their corresponding learnable target feature representations for the inner objective, which does not introduce any randomness. Our primary motivation is that the model obtained by the proposed inner optimization can mimic the
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;&#22522;&#20110;&#20998;&#25968;&#30340;&#21453;&#21521;&#25193;&#25955;&#31639;&#27861;&#29983;&#25104;&#30340;&#39640;&#36136;&#37327;&#26679;&#26412;&#30340;&#30740;&#31350;&#32467;&#26524;&#65292;&#25105;&#20204;&#21457;&#29616;&#23613;&#31649;&#23384;&#22312;&#32500;&#24230;&#28798;&#38590;&#65292;&#20294;&#20026;&#20102;&#38477;&#22122;&#32780;&#35757;&#32451;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#23398;&#20064;&#21040;&#39640;&#32500;&#23494;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#35757;&#32451;&#38598;&#30340;&#38750;&#37325;&#21472;&#23376;&#38598;&#19978;&#35757;&#32451;&#30340;&#32593;&#32476;&#21487;&#20197;&#23398;&#20064;&#21040;&#30456;&#21516;&#30340;&#23494;&#24230;&#65292;&#20174;&#32780;&#35777;&#26126;&#20102;DNN&#26550;&#26500;&#21644;&#35757;&#32451;&#31639;&#27861;&#20013;&#30340;&#24402;&#32435;&#20559;&#24046;&#19982;&#25968;&#25454;&#20998;&#24067;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.02557</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#27867;&#21270;&#24615;&#36136;&#28304;&#20110;&#20960;&#20309;&#33258;&#36866;&#24212;&#30340;&#35856;&#27874;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Generalization in diffusion models arises from geometry-adaptive harmonic representation. (arXiv:2310.02557v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02557
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#22522;&#20110;&#20998;&#25968;&#30340;&#21453;&#21521;&#25193;&#25955;&#31639;&#27861;&#29983;&#25104;&#30340;&#39640;&#36136;&#37327;&#26679;&#26412;&#30340;&#30740;&#31350;&#32467;&#26524;&#65292;&#25105;&#20204;&#21457;&#29616;&#23613;&#31649;&#23384;&#22312;&#32500;&#24230;&#28798;&#38590;&#65292;&#20294;&#20026;&#20102;&#38477;&#22122;&#32780;&#35757;&#32451;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#23398;&#20064;&#21040;&#39640;&#32500;&#23494;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#35757;&#32451;&#38598;&#30340;&#38750;&#37325;&#21472;&#23376;&#38598;&#19978;&#35757;&#32451;&#30340;&#32593;&#32476;&#21487;&#20197;&#23398;&#20064;&#21040;&#30456;&#21516;&#30340;&#23494;&#24230;&#65292;&#20174;&#32780;&#35777;&#26126;&#20102;DNN&#26550;&#26500;&#21644;&#35757;&#32451;&#31639;&#27861;&#20013;&#30340;&#24402;&#32435;&#20559;&#24046;&#19982;&#25968;&#25454;&#20998;&#24067;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#20998;&#25968;&#30340;&#21453;&#21521;&#25193;&#25955;&#31639;&#27861;&#29983;&#25104;&#30340;&#39640;&#36136;&#37327;&#26679;&#26412;&#25552;&#20379;&#20102;&#35777;&#25454;&#65292;&#34920;&#26126;&#23613;&#31649;&#23384;&#22312;&#32500;&#24230;&#28798;&#38590;&#65292;&#20026;&#20102;&#38477;&#22122;&#32780;&#35757;&#32451;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#21487;&#20197;&#23398;&#20064;&#39640;&#32500;&#23494;&#24230;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#35757;&#32451;&#38598;&#35760;&#24518;&#21270;&#30340;&#26368;&#26032;&#25253;&#21578;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#36825;&#20123;&#32593;&#32476;&#26159;&#21542;&#23398;&#20064;&#20102;&#25968;&#25454;&#30340;&#8220;&#30495;&#23454;&#8221;&#36830;&#32493;&#23494;&#24230;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#23637;&#31034;&#65292;&#35757;&#32451;&#22312;&#25968;&#25454;&#38598;&#30340;&#38750;&#37325;&#21472;&#23376;&#38598;&#19978;&#30340;&#20004;&#20010;&#38477;&#22122;DNN&#23398;&#20064;&#30340;&#20960;&#20046;&#26159;&#30456;&#21516;&#30340;&#20998;&#25968;&#20989;&#25968;&#65292;&#20174;&#32780;&#23398;&#20064;&#20102;&#30456;&#21516;&#30340;&#23494;&#24230;&#65292;&#19988;&#20165;&#38656;&#24456;&#23569;&#30340;&#35757;&#32451;&#22270;&#20687;&#12290;&#36825;&#31181;&#24378;&#22823;&#30340;&#27867;&#21270;&#24615;&#35777;&#26126;&#20102;DNN&#26550;&#26500;&#21644;/&#25110;&#35757;&#32451;&#31639;&#27861;&#20013;&#30340;&#26377;&#21147;&#24402;&#32435;&#20559;&#24046;&#19982;&#25968;&#25454;&#20998;&#24067;&#30340;&#29305;&#24615;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#20123;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#38477;&#22122;&#22120;&#22312;&#36866;&#24212;&#20110;&#24213;&#23618;&#22270;&#20687;&#30340;&#22522;&#30784;&#19978;&#25191;&#34892;&#25910;&#32553;&#25805;&#20316;&#12290;&#23545;&#36825;&#20123;&#22522;&#30690;&#30340;&#26816;&#26597;&#25581;&#31034;&#20102;&#27839;&#36718;&#24275;&#21644;&#22343;&#21248;&#22270;&#20687;&#21306;&#22495;&#30340;&#25391;&#33633;&#35856;&#27874;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-quality samples generated with score-based reverse diffusion algorithms provide evidence that deep neural networks (DNN) trained for denoising can learn high-dimensional densities, despite the curse of dimensionality. However, recent reports of memorization of the training set raise the question of whether these networks are learning the "true" continuous density of the data. Here, we show that two denoising DNNs trained on non-overlapping subsets of a dataset learn nearly the same score function, and thus the same density, with a surprisingly small number of training images. This strong generalization demonstrates an alignment of powerful inductive biases in the DNN architecture and/or training algorithm with properties of the data distribution. We analyze these, demonstrating that the denoiser performs a shrinkage operation in a basis adapted to the underlying image. Examination of these bases reveals oscillating harmonic structures along contours and in homogeneous image region
&lt;/p&gt;</description></item><item><title>&#23558;&#29983;&#25104;&#23545;&#31435;&#20551;&#35774;&#30340;&#36807;&#31243;&#35270;&#20026;&#30693;&#35782;&#26469;&#28304;&#65292;&#24182;&#24212;&#29992;&#20110;&#26420;&#32032;&#36125;&#21494;&#26031;&#20998;&#31867;&#22120;&#65292;&#23637;&#31034;&#20854;&#26377;&#36259;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.04284</link><description>&lt;p&gt;
&#23558;&#29983;&#25104;&#23545;&#31435;&#20551;&#35774;&#30340;&#36807;&#31243;&#35270;&#20026;&#30693;&#35782;&#26469;&#28304; - &#24212;&#29992;&#20110;&#26420;&#32032;&#36125;&#21494;&#26031;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Viewing the process of generating counterfactuals as a source of knowledge -- Application to the Naive Bayes classifier. (arXiv:2309.04284v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04284
&lt;/p&gt;
&lt;p&gt;
&#23558;&#29983;&#25104;&#23545;&#31435;&#20551;&#35774;&#30340;&#36807;&#31243;&#35270;&#20026;&#30693;&#35782;&#26469;&#28304;&#65292;&#24182;&#24212;&#29992;&#20110;&#26420;&#32032;&#36125;&#21494;&#26031;&#20998;&#31867;&#22120;&#65292;&#23637;&#31034;&#20854;&#26377;&#36259;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#22312;&#26377;&#35768;&#22810;&#29702;&#35299;&#31639;&#27861;&#21487;&#20197;&#29702;&#35299;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#20915;&#31574;&#65292;&#20854;&#20013;&#21253;&#25324;&#22522;&#20110;&#29983;&#25104;&#23545;&#31435;&#20551;&#35774;&#31034;&#20363;&#30340;&#31639;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#23558;&#36825;&#20010;&#29983;&#25104;&#36807;&#31243;&#35270;&#20026;&#19968;&#31181;&#21019;&#36896;&#19968;&#23450;&#37327;&#30693;&#35782;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#30693;&#35782;&#21487;&#20197;&#23384;&#20648;&#24182;&#22312;&#20197;&#21518;&#20197;&#19981;&#21516;&#30340;&#26041;&#24335;&#20351;&#29992;&#12290;&#26412;&#25991;&#22312;&#21152;&#27861;&#27169;&#22411;&#20013;&#36827;&#34892;&#20102;&#35828;&#26126;&#65292;&#20855;&#20307;&#32780;&#35328;&#65292;&#26159;&#22312;&#26420;&#32032;&#36125;&#21494;&#26031;&#20998;&#31867;&#22120;&#30340;&#24773;&#20917;&#19979;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#27492;&#30446;&#30340;&#19978;&#30340;&#26377;&#36259;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
There are now many comprehension algorithms for understanding the decisions of a machine learning algorithm. Among these are those based on the generation of counterfactual examples. This article proposes to view this generation process as a source of creating a certain amount of knowledge that can be stored to be used, later, in different ways. This process is illustrated in the additive model and, more specifically, in the case of the naive Bayes classifier, whose interesting properties for this purpose are shown.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#25490;&#21517;&#20013;&#30340;&#26368;&#23567;&#26497;&#22823;&#21518;&#24724;&#38382;&#39064;&#19982;Top-k&#21453;&#39304;&#65292;&#24182;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#20840;&#38754;&#30340;&#26497;&#22823;&#21518;&#24724;&#29575;&#21051;&#30011;&#65292;&#35299;&#20915;&#20102;Chaudhuri&#21644;Tewari [2017]&#25152;&#25552;&#20986;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.02425</link><description>&lt;p&gt;
&#22312;&#22312;&#32447;&#25490;&#21517;&#20013;&#30340;&#26368;&#23567;&#26497;&#22823;&#21518;&#24724;&#38382;&#39064;&#19982;Top-k&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
On the Minimax Regret in Online Ranking with Top-k Feedback. (arXiv:2309.02425v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02425
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#25490;&#21517;&#20013;&#30340;&#26368;&#23567;&#26497;&#22823;&#21518;&#24724;&#38382;&#39064;&#19982;Top-k&#21453;&#39304;&#65292;&#24182;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#20840;&#38754;&#30340;&#26497;&#22823;&#21518;&#24724;&#29575;&#21051;&#30011;&#65292;&#35299;&#20915;&#20102;Chaudhuri&#21644;Tewari [2017]&#25152;&#25552;&#20986;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#25490;&#21517;&#20013;&#65292;&#23398;&#20064;&#31639;&#27861;&#25353;&#39034;&#24207;&#23545;&#19968;&#32452;&#39033;&#30446;&#36827;&#34892;&#25490;&#21517;&#65292;&#24182;&#20197;&#30456;&#20851;&#24615;&#24471;&#20998;&#30340;&#24418;&#24335;&#25509;&#25910;&#21453;&#39304;&#12290;&#30001;&#20110;&#33719;&#24471;&#30456;&#20851;&#24615;&#24471;&#20998;&#36890;&#24120;&#28041;&#21450;&#20154;&#24037;&#27880;&#37322;&#65292;&#22240;&#27492;&#32771;&#34385;&#20165;&#23545;&#25490;&#21517;&#20013;&#30340;&#21069;k&#20010;&#39033;&#30446;&#38480;&#21046;&#21453;&#39304;&#30340;&#37096;&#20998;&#21453;&#39304;&#35774;&#32622;&#20855;&#26377;&#26497;&#22823;&#30340;&#20852;&#36259;&#12290;Chaudhuri&#21644;Tewari [2017]&#24320;&#21457;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#20998;&#26512;&#24102;&#26377;Top $k$&#21453;&#39304;&#30340;&#22312;&#32447;&#25490;&#21517;&#31639;&#27861;&#12290;&#20182;&#20204;&#24037;&#20316;&#30340;&#20851;&#38190;&#35201;&#32032;&#26159;&#20351;&#29992;&#20102;&#37096;&#20998;&#30417;&#25511;&#25216;&#26415;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#20855;&#26377;Top $k$&#21453;&#39304;&#30340;&#22312;&#32447;&#25490;&#21517;&#65292;&#24182;&#35299;&#20915;&#20102;Chaudhuri&#21644;Tewari [2017]&#25552;&#20986;&#30340;&#19968;&#20123;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#23545;&#25152;&#26377;$k$&#21644;&#20197;&#19979;&#25490;&#21517;&#24615;&#33021;&#24230;&#37327;&#65288;Pairwise Loss&#65292;Discounted Cumulative Gain&#21644;Precision@n&#65289;&#30340;Top $k$&#21453;&#39304;&#27169;&#22411;&#36827;&#34892;&#20102;&#26368;&#23567;&#26497;&#22823;&#21518;&#24724;&#29575;&#30340;&#23436;&#20840;&#21051;&#30011;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#23454;&#29616;&#20102;Precision@n&#30340;&#26368;&#23567;&#26497;&#22823;&#21518;&#24724;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
In online ranking, a learning algorithm sequentially ranks a set of items and receives feedback on its ranking in the form of relevance scores. Since obtaining relevance scores typically involves human annotation, it is of great interest to consider a partial feedback setting where feedback is restricted to the top-$k$ items in the rankings. Chaudhuri and Tewari [2017] developed a framework to analyze online ranking algorithms with top $k$ feedback. A key element in their work was the use of techniques from partial monitoring. In this paper, we further investigate online ranking with top $k$ feedback and solve some open problems posed by Chaudhuri and Tewari [2017]. We provide a full characterization of minimax regret rates with the top $k$ feedback model for all $k$ and for the following ranking performance measures: Pairwise Loss, Discounted Cumulative Gain, and Precision@n. In addition, we give an efficient algorithm that achieves the minimax regret rate for Precision@n.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#21464;&#20998;&#20613;&#37324;&#21494;&#29305;&#24449;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23545;&#24191;&#27867;&#30340;&#24179;&#31283;&#21327;&#26041;&#24046;&#20989;&#25968;&#36827;&#34892;&#24555;&#36895;&#31354;&#38388;&#24314;&#27169;&#65292;&#30456;&#27604;&#20854;&#20182;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#24615;&#33021;&#21644;&#21152;&#36895;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.14142</link><description>&lt;p&gt;
&#24555;&#36895;&#31354;&#38388;&#24314;&#27169;&#30340;&#32508;&#21512;&#21464;&#20998;&#20613;&#37324;&#21494;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Integrated Variational Fourier Features for Fast Spatial Modelling with Gaussian Processes. (arXiv:2308.14142v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14142
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#21464;&#20998;&#20613;&#37324;&#21494;&#29305;&#24449;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23545;&#24191;&#27867;&#30340;&#24179;&#31283;&#21327;&#26041;&#24046;&#20989;&#25968;&#36827;&#34892;&#24555;&#36895;&#31354;&#38388;&#24314;&#27169;&#65292;&#30456;&#27604;&#20854;&#20182;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#24615;&#33021;&#21644;&#21152;&#36895;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#21464;&#20998;&#36924;&#36817;&#26159;&#25193;&#23637;&#39640;&#26031;&#36807;&#31243;&#25512;&#29702;&#21644;&#23398;&#20064;&#33267;&#26356;&#22823;&#25968;&#25454;&#38598;&#30340;&#27969;&#34892;&#26041;&#27861;&#12290;&#23545;&#20110;$N$&#20010;&#35757;&#32451;&#28857;&#65292;&#31934;&#30830;&#25512;&#29702;&#30340;&#25104;&#26412;&#20026;$O(N^3)$&#65307;&#20351;&#29992;$M \ll N$&#29305;&#24449;&#30340;&#20808;&#36827;&#31232;&#30095;&#21464;&#20998;&#26041;&#27861;&#25104;&#26412;&#20026;$O(NM^2)$&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#26356;&#22797;&#26434;&#29305;&#24449;&#30340;&#26041;&#27861;&#65307;&#36825;&#20123;&#26041;&#27861;&#22312;&#20302;&#32500;&#20219;&#21153;&#65288;&#22914;&#31354;&#38388;&#24314;&#27169;&#65289;&#20013;&#33021;&#22815;&#26377;&#24456;&#22909;&#30340;&#24615;&#33021;&#65292;&#20294;&#21482;&#20351;&#29992;&#20102;&#19968;&#31867;&#38750;&#24120;&#26377;&#38480;&#30340;&#26680;&#20989;&#25968;&#65292;&#25490;&#38500;&#20102;&#19968;&#20123;&#24120;&#29992;&#30340;&#26680;&#20989;&#25968;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32508;&#21512;&#20613;&#37324;&#21494;&#29305;&#24449;&#65292;&#23558;&#36825;&#20123;&#24615;&#33021;&#20248;&#21183;&#25193;&#23637;&#21040;&#38750;&#24120;&#24191;&#27867;&#30340;&#24179;&#31283;&#21327;&#26041;&#24046;&#20989;&#25968;&#12290;&#25105;&#20204;&#20174;&#25910;&#25947;&#20998;&#26512;&#21644;&#32463;&#39564;&#25506;&#32034;&#30340;&#35282;&#24230;&#26469;&#35299;&#37322;&#35813;&#26041;&#27861;&#21644;&#21442;&#25968;&#30340;&#36873;&#25321;&#65292;&#21516;&#26102;&#23637;&#31034;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#31354;&#38388;&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#23454;&#38469;&#21152;&#36895;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sparse variational approximations are popular methods for scaling up inference and learning in Gaussian processes to larger datasets. For $N$ training points, exact inference has $O(N^3)$ cost; with $M \ll N$ features, state of the art sparse variational methods have $O(NM^2)$ cost. Recently, methods have been proposed using more sophisticated features; these promise $O(M^3)$ cost, with good performance in low dimensional tasks such as spatial modelling, but they only work with a very limited class of kernels, excluding some of the most commonly used. In this work, we propose integrated Fourier features, which extends these performance benefits to a very broad class of stationary covariance functions. We motivate the method and choice of parameters from a convergence analysis and empirical exploration, and show practical speedup in synthetic and real world spatial regression tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22522;&#20110;Metapath&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#29992;&#20110;&#22270;&#24322;&#24120;&#26816;&#27979;&#12290;&#36890;&#36807;&#22312;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#20013;&#20351;&#29992;GCN&#23618;&#26469;&#26377;&#25928;&#20256;&#25773;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20197;&#21450;&#29305;&#21035;&#35774;&#35745;&#30340;&#24322;&#24120;&#31038;&#21306;&#65292;&#35813;&#26041;&#27861;&#22312;&#32467;&#26500;&#21644;&#23646;&#24615;&#24046;&#24322;&#30340;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#24605;&#36335;&#21644;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2308.10918</link><description>&lt;p&gt;
&#22522;&#20110;Metapath&#30340;&#19978;&#19979;&#25991;&#30693;&#35782;&#30340;&#28145;&#24230;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Deep Semi-supervised Anomaly Detection with Metapath-based Context Knowledge. (arXiv:2308.10918v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22522;&#20110;Metapath&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#29992;&#20110;&#22270;&#24322;&#24120;&#26816;&#27979;&#12290;&#36890;&#36807;&#22312;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#20013;&#20351;&#29992;GCN&#23618;&#26469;&#26377;&#25928;&#20256;&#25773;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20197;&#21450;&#29305;&#21035;&#35774;&#35745;&#30340;&#24322;&#24120;&#31038;&#21306;&#65292;&#35813;&#26041;&#27861;&#22312;&#32467;&#26500;&#21644;&#23646;&#24615;&#24046;&#24322;&#30340;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#24605;&#36335;&#21644;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24322;&#24120;&#26816;&#27979;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;Metapath&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#20043;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#22522;&#20110;Metapath&#30340;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#65288;MSAD&#65289;&#65292;&#22312;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#20013;&#37117;&#20351;&#29992;GCN&#23618;&#26469;&#26377;&#25928;&#22320;&#20256;&#25773;&#24322;&#24120;&#21644;&#27491;&#24120;&#33410;&#28857;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#22522;&#20110;Metapath&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#35774;&#35745;&#21644;&#29305;&#21035;&#31934;&#24515;&#35774;&#35745;&#30340;&#24322;&#24120;&#31038;&#21306;&#22686;&#24378;&#20102;&#20840;&#23616;&#21644;&#23616;&#37096;&#32467;&#26500;&#21644;&#23646;&#24615;&#24046;&#24322;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#36890;&#36807;&#22312;&#19971;&#20010;&#30495;&#23454;&#32593;&#32476;&#19978;&#36827;&#34892;&#30340;&#19968;&#31995;&#21015;&#32508;&#21512;&#23454;&#39564;&#65292;&#26412;&#25991;&#35777;&#26126;&#20102;MSAD&#26041;&#27861;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#25216;&#26415;&#30340;&#20248;&#36234;&#24615;&#12290;&#26412;&#30740;&#31350;&#30340;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#38138;&#24179;&#20102;&#36947;&#36335;&#65292;&#37325;&#28857;&#26159;&#20248;&#21270;&#21644;&#20998;&#26512;Metapath&#27169;&#24335;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph anomaly detection has attracted considerable attention in recent years. This paper introduces a novel approach that leverages metapath-based semi-supervised learning, addressing the limitations of previous methods. We present a new framework, Metapath-based Semi-supervised Anomaly Detection (MSAD), incorporating GCN layers in both the encoder and decoder to efficiently propagate context information between abnormal and normal nodes. The design of metapath-based context information and a specifically crafted anomaly community enhance the process of learning differences in structures and attributes, both globally and locally. Through a comprehensive set of experiments conducted on seven real-world networks, this paper demonstrates the superiority of the MSAD method compared to state-of-the-art techniques. The promising results of this study pave the way for future investigations, focusing on the optimization and analysis of metapath patterns to further enhance the effectiveness of 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#33041;&#32593;&#32476;&#30340;&#23545;&#27604;&#22270;&#27744;&#21270;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#23545;&#33041;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#20998;&#31867;&#12290;&#36890;&#36807;&#23450;&#21046;&#21270;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#29305;&#27530;&#35774;&#35745;&#30340;&#21487;&#35299;&#37322;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#65292;&#22312;5&#20010;&#38745;&#24687;&#24577;fMRI&#33041;&#32593;&#32476;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#26368;&#20808;&#36827;&#22522;&#20934;&#32447;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.11133</link><description>&lt;p&gt;
&#23545;&#33041;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#20998;&#31867;&#36827;&#34892;&#23545;&#27604;&#22270;&#27744;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive Graph Pooling for Explainable Classification of Brain Networks. (arXiv:2307.11133v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11133
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#33041;&#32593;&#32476;&#30340;&#23545;&#27604;&#22270;&#27744;&#21270;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#23545;&#33041;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#20998;&#31867;&#12290;&#36890;&#36807;&#23450;&#21046;&#21270;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#29305;&#27530;&#35774;&#35745;&#30340;&#21487;&#35299;&#37322;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#65292;&#22312;5&#20010;&#38745;&#24687;&#24577;fMRI&#33041;&#32593;&#32476;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#26368;&#20808;&#36827;&#22522;&#20934;&#32447;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21151;&#33021;&#24615;&#30913;&#20849;&#25391;&#25104;&#20687;(fMRI)&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#27979;&#37327;&#31070;&#32463;&#27963;&#21160;&#30340;&#25216;&#26415;&#12290;&#20854;&#24212;&#29992;&#22312;&#35782;&#21035;&#24085;&#37329;&#26862;&#30149;&#12289;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#21644;&#33258;&#38381;&#30151;&#31561;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#26041;&#38754;&#23588;&#20026;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;fMRI&#25968;&#25454;&#20998;&#26512;&#23558;&#22823;&#33041;&#24314;&#27169;&#20026;&#22270;&#65292;&#24182;&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#25552;&#21462;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;fMRI&#25968;&#25454;&#30340;&#29420;&#29305;&#29305;&#24449;&#35201;&#27714;&#23545;GNN&#36827;&#34892;&#29305;&#27530;&#35774;&#35745;&#12290;&#23450;&#21046;GNN&#20197;&#29983;&#25104;&#26377;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#27604;&#21452;&#27880;&#24847;&#22359;&#21644;&#21487;&#24494;&#20998;&#22270;&#27744;&#21270;&#26041;&#27861;ContrastPool&#65292;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;GNN&#20998;&#26512;&#33041;&#32593;&#32476;&#65292;&#28385;&#36275;fMRI&#30340;&#29305;&#27530;&#35201;&#27714;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;5&#20010;&#38745;&#24687;&#24577;fMRI&#33041;&#32593;&#32476;&#25968;&#25454;&#38598;&#30340;3&#31181;&#30142;&#30149;&#65292;&#24182;&#35777;&#26126;&#20854;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#32447;&#12290;&#25105;&#20204;&#30340;&#26696;&#20363;&#30740;&#31350;&#35777;&#23454;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#21462;&#30340;&#27169;&#24335;&#19982;&#31070;&#32463;&#31185;&#23398;&#25991;&#29486;&#20013;&#30340;&#39046;&#22495;&#30693;&#35782;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
Functional magnetic resonance imaging (fMRI) is a commonly used technique to measure neural activation. Its application has been particularly important in identifying underlying neurodegenerative conditions such as Parkinson's, Alzheimer's, and Autism. Recent analysis of fMRI data models the brain as a graph and extracts features by graph neural networks (GNNs). However, the unique characteristics of fMRI data require a special design of GNN. Tailoring GNN to generate effective and domain-explainable features remains challenging. In this paper, we propose a contrastive dual-attention block and a differentiable graph pooling method called ContrastPool to better utilize GNN for brain networks, meeting fMRI-specific requirements. We apply our method to 5 resting-state fMRI brain network datasets of 3 diseases and demonstrate its superiority over state-of-the-art baselines. Our case study confirms that the patterns extracted by our method match the domain knowledge in neuroscience literatu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31163;&#25955;&#20999;&#21106;Wasserstein&#25439;&#22833;&#30340;&#24615;&#36136;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#27491;&#21017;&#24615;&#21644;&#20248;&#21270;&#24615;&#36136;&#20197;&#21450;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#36817;&#20284;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.10352</link><description>&lt;p&gt;
&#31163;&#25955;&#20999;&#21106;Wasserstein&#25439;&#22833;&#30340;&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;
Properties of Discrete Sliced Wasserstein Losses. (arXiv:2307.10352v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10352
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31163;&#25955;&#20999;&#21106;Wasserstein&#25439;&#22833;&#30340;&#24615;&#36136;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#27491;&#21017;&#24615;&#21644;&#20248;&#21270;&#24615;&#36136;&#20197;&#21450;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#36817;&#20284;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20999;&#21106;Wasserstein&#65288;SW&#65289;&#36317;&#31163;&#24050;&#25104;&#20026;&#27604;&#36739;&#27010;&#29575;&#27979;&#24230;&#30340;Wasserstein&#36317;&#31163;&#30340;&#19968;&#31181;&#27969;&#34892;&#26367;&#20195;&#26041;&#27861;&#12290;&#24191;&#27867;&#24212;&#29992;&#21253;&#25324;&#22270;&#20687;&#22788;&#29702;&#12289;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#29983;&#25104;&#24314;&#27169;&#65292;&#24120;&#24120;&#38656;&#35201;&#20248;&#21270;&#19968;&#20123;&#21442;&#25968;&#20197;&#26368;&#23567;&#21270;SW&#65292;&#35813;&#21442;&#25968;&#20805;&#24403;&#31163;&#25955;&#27010;&#29575;&#27979;&#24230;&#20043;&#38388;&#30340;&#25439;&#22833;&#20989;&#25968;&#65288;&#22240;&#20026;&#20855;&#26377;&#23494;&#24230;&#30340;&#27979;&#24230;&#22312;&#25968;&#20540;&#19978;&#26159;&#26080;&#27861;&#23454;&#29616;&#30340;&#65289;&#12290;&#25152;&#26377;&#36825;&#20123;&#20248;&#21270;&#38382;&#39064;&#37117;&#23384;&#22312;&#30456;&#21516;&#30340;&#23376;&#38382;&#39064;&#65292;&#21363;&#26368;&#23567;&#21270;&#20999;&#21106;Wasserstein&#33021;&#37327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;$\mathcal{E}: Y \longmapsto \mathrm{SW}_2^2(\gamma_Y, \gamma_Z)$&#30340;&#23646;&#24615;&#65292;&#21363;&#20004;&#20010;&#20855;&#26377;&#19982;&#19968;&#20010;&#27979;&#24230;&#30340;&#25903;&#25745;&#30456;&#21516;&#25968;&#37327;&#30340;&#31163;&#25955;&#22343;&#21248;&#27979;&#24230;&#20043;&#38388;&#30340;SW&#36317;&#31163;&#20316;&#20026;&#25903;&#25745;$Y \in \mathbb{R}^{n \times d}$&#20989;&#25968;&#30340;&#33021;&#37327;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20010;&#33021;&#37327;&#30340;&#27491;&#21017;&#24615;&#21644;&#20248;&#21270;&#24615;&#36136;&#65292;&#20197;&#21450;&#20854;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#36817;&#20284;$\mathcal{E}_p$&#65288;&#20351;&#29992;SW&#20013;&#30340;&#26399;&#26395;&#20272;&#35745;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Sliced Wasserstein (SW) distance has become a popular alternative to the Wasserstein distance for comparing probability measures. Widespread applications include image processing, domain adaptation and generative modelling, where it is common to optimise some parameters in order to minimise SW, which serves as a loss function between discrete probability measures (since measures admitting densities are numerically unattainable). All these optimisation problems bear the same sub-problem, which is minimising the Sliced Wasserstein energy. In this paper we study the properties of $\mathcal{E}: Y \longmapsto \mathrm{SW}_2^2(\gamma_Y, \gamma_Z)$, i.e. the SW distance between two uniform discrete measures with the same amount of points as a function of the support $Y \in \mathbb{R}^{n \times d}$ of one of the measures. We investigate the regularity and optimisation properties of this energy, as well as its Monte-Carlo approximation $\mathcal{E}_p$ (estimating the expectation in SW using 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#26041;&#27861;&#30340;&#21333;&#32454;&#32990;&#24046;&#24322;&#20998;&#26512;&#27979;&#35797;&#26694;&#26550;&#65292;&#21487;&#20197;&#38750;&#32447;&#24615;&#27604;&#36739;&#22797;&#26434;&#30340;&#32454;&#32990;&#38388;&#20998;&#23376;&#29305;&#24449;&#20998;&#24067;&#12290;&#36890;&#36807;&#21033;&#29992;&#26680;&#23884;&#20837;&#30340;&#21464;&#24322;&#24615;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#25581;&#31034;&#32454;&#32990;&#32676;&#20307;&#20013;&#38544;&#34109;&#30340;&#24322;&#36136;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26680;&#27979;&#35797;&#22914;&#20309;&#20811;&#26381;&#21333;&#32454;&#32990;&#24046;&#24322;&#20998;&#26512;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#24212;&#29992;&#20110;&#30740;&#31350;&#20998;&#21270;&#36870;&#36716;&#30340;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2307.08509</link><description>&lt;p&gt;
&#22522;&#20110;&#26680;&#26041;&#27861;&#30340;&#21333;&#32454;&#32990;&#24046;&#24322;&#20998;&#26512;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Kernel-Based Testing for Single-Cell Differential Analysis. (arXiv:2307.08509v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08509
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#26041;&#27861;&#30340;&#21333;&#32454;&#32990;&#24046;&#24322;&#20998;&#26512;&#27979;&#35797;&#26694;&#26550;&#65292;&#21487;&#20197;&#38750;&#32447;&#24615;&#27604;&#36739;&#22797;&#26434;&#30340;&#32454;&#32990;&#38388;&#20998;&#23376;&#29305;&#24449;&#20998;&#24067;&#12290;&#36890;&#36807;&#21033;&#29992;&#26680;&#23884;&#20837;&#30340;&#21464;&#24322;&#24615;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#25581;&#31034;&#32454;&#32990;&#32676;&#20307;&#20013;&#38544;&#34109;&#30340;&#24322;&#36136;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26680;&#27979;&#35797;&#22914;&#20309;&#20811;&#26381;&#21333;&#32454;&#32990;&#24046;&#24322;&#20998;&#26512;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#24212;&#29992;&#20110;&#30740;&#31350;&#20998;&#21270;&#36870;&#36716;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#32454;&#32990;&#25216;&#26415;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#22522;&#22240;&#34920;&#36798;&#21644;&#34920;&#35266;&#36951;&#20256;&#20462;&#39280;&#31561;&#20998;&#23376;&#29305;&#24449;&#30340;&#23453;&#36149;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#20197;&#25511;&#21046;&#21644;&#24378;&#26377;&#21147;&#30340;&#26041;&#24335;&#27604;&#36739;&#36825;&#20123;&#22797;&#26434;&#20998;&#24067;&#38754;&#20020;&#30528;&#26041;&#27861;&#35770;&#19978;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#22522;&#20110;&#26680;&#23884;&#20837;&#30340;&#26680;&#27979;&#35797;&#26694;&#26550;&#26469;&#38750;&#32447;&#24615;&#27604;&#36739;&#32454;&#32990;&#38388;&#22797;&#26434;&#20998;&#23376;&#29305;&#24449;&#30340;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#19981;&#20165;&#20801;&#35768;&#23545;&#29305;&#24449;&#36827;&#34892;&#20998;&#26512;&#65292;&#36824;&#33021;&#22312;&#32771;&#34385;&#20102;&#23427;&#20204;&#20043;&#38388;&#22797;&#26434;&#20381;&#36182;&#20851;&#31995;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#36716;&#24405;&#32452;&#25110;&#34920;&#35266;&#32452;&#30340;&#20840;&#23616;&#27604;&#36739;&#12290;&#36890;&#36807;&#20351;&#29992;&#20998;&#31867;&#22120;&#22522;&#20110;&#26680;&#23884;&#20837;&#30340;&#21464;&#24322;&#24615;&#26469;&#21306;&#20998;&#32454;&#32990;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#21457;&#29616;&#22312;&#32454;&#32990;&#32676;&#20307;&#20013;&#21407;&#26412;&#26080;&#27861;&#23519;&#35273;&#21040;&#30340;&#24322;&#36136;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26680;&#27979;&#35797;&#26041;&#27861;&#22914;&#20309;&#20811;&#26381;&#19987;&#38376;&#29992;&#20110;&#21333;&#32454;&#32990;&#30340;&#24046;&#24322;&#20998;&#26512;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#36824;&#23558;&#26680;&#27979;&#35797;&#24212;&#29992;&#20110;&#30740;&#31350;&#20998;&#21270;&#36870;&#36716;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Single-cell technologies have provided valuable insights into the distribution of molecular features, such as gene expression and epigenomic modifications. However, comparing these complex distributions in a controlled and powerful manner poses methodological challenges. Here we propose to benefit from the kernel-testing framework to compare the complex cell-wise distributions of molecular features in a non-linear manner based on their kernel embedding. Our framework not only allows for feature-wise analyses but also enables global comparisons of transcriptomes or epigenomes, considering their intricate dependencies. By using a classifier to discriminate cells based on the variability of their embedding, our method uncovers heterogeneities in cell populations that would otherwise go undetected. We show that kernel testing overcomes the limitations of differential analysis methods dedicated to single-cell. Kernel testing is applied to investigate the reversion process of differentiating
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#20248;&#25511;&#21046;&#21644;&#24378;&#21270;&#23398;&#20064;&#20013;&#38750;&#32047;&#31215;&#30446;&#26631;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#20462;&#25913;&#29616;&#26377;&#31639;&#27861;&#30340;&#26041;&#27861;&#26469;&#20248;&#21270;&#36825;&#20123;&#30446;&#26631;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#36125;&#23572;&#26364;&#26368;&#20248;&#24615;&#26041;&#31243;&#20013;&#20351;&#29992;&#24191;&#20041;&#36816;&#31639;&#21487;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;&#38750;&#32047;&#31215;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2307.04957</link><description>&lt;p&gt;
&#38750;&#32047;&#31215;&#30446;&#26631;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning with Non-Cumulative Objective. (arXiv:2307.04957v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#20248;&#25511;&#21046;&#21644;&#24378;&#21270;&#23398;&#20064;&#20013;&#38750;&#32047;&#31215;&#30446;&#26631;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#20462;&#25913;&#29616;&#26377;&#31639;&#27861;&#30340;&#26041;&#27861;&#26469;&#20248;&#21270;&#36825;&#20123;&#30446;&#26631;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#36125;&#23572;&#26364;&#26368;&#20248;&#24615;&#26041;&#31243;&#20013;&#20351;&#29992;&#24191;&#20041;&#36816;&#31639;&#21487;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;&#38750;&#32047;&#31215;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#30446;&#26631;&#20960;&#20046;&#24635;&#26159;&#23450;&#20041;&#20026;&#27839;&#36807;&#31243;&#20013;&#22870;&#21169;&#30340;\emph{&#32047;&#31215;}&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#26368;&#20248;&#25511;&#21046;&#21644;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#20013;&#65292;&#23588;&#20854;&#26159;&#22312;&#36890;&#20449;&#21644;&#32593;&#32476;&#39046;&#22495;&#20013;&#65292;&#30446;&#26631;&#24182;&#19981;&#33258;&#28982;&#22320;&#34920;&#36798;&#20026;&#22870;&#21169;&#30340;&#27714;&#21644;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#35782;&#21040;&#21508;&#31181;&#38382;&#39064;&#20013;&#38750;&#32047;&#31215;&#30446;&#26631;&#30340;&#26222;&#36941;&#23384;&#22312;&#65292;&#24182;&#25552;&#20986;&#20102;&#20462;&#25913;&#29616;&#26377;&#31639;&#27861;&#20197;&#20248;&#21270;&#36825;&#20123;&#30446;&#26631;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#35768;&#22810;&#26368;&#20248;&#25511;&#21046;&#21644;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#22522;&#26412;&#26500;&#24314;&#27169;&#22359;&#65306;&#36125;&#23572;&#26364;&#26368;&#20248;&#24615;&#26041;&#31243;&#12290;&#20026;&#20102;&#20248;&#21270;&#38750;&#32047;&#31215;&#30446;&#26631;&#65292;&#25105;&#20204;&#29992;&#19982;&#30446;&#26631;&#30456;&#23545;&#24212;&#30340;&#24191;&#20041;&#36816;&#31639;&#26367;&#25442;&#20102;&#36125;&#23572;&#26364;&#26356;&#26032;&#35268;&#21017;&#20013;&#30340;&#21407;&#22987;&#27714;&#21644;&#36816;&#31639;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#24191;&#20041;&#36816;&#31639;&#24418;&#24335;&#30340;&#36275;&#22815;&#26465;&#20214;&#20197;&#21450;&#23545;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#30340;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
In reinforcement learning, the objective is almost always defined as a \emph{cumulative} function over the rewards along the process. However, there are many optimal control and reinforcement learning problems in various application fields, especially in communications and networking, where the objectives are not naturally expressed as summations of the rewards. In this paper, we recognize the prevalence of non-cumulative objectives in various problems, and propose a modification to existing algorithms for optimizing such objectives. Specifically, we dive into the fundamental building block for many optimal control and reinforcement learning algorithms: the Bellman optimality equation. To optimize a non-cumulative objective, we replace the original summation operation in the Bellman update rule with a generalized operation corresponding to the objective. Furthermore, we provide sufficient conditions on the form of the generalized operation as well as assumptions on the Markov decision 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#23545;&#27604;&#24335;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#27169;&#22411;&#30340;&#33030;&#24369;&#24615;&#65292;&#36890;&#36807;&#25366;&#25496;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#21487;&#20197;&#25214;&#21040;&#27450;&#39575;&#20027;&#22270;&#20687;&#65292;&#36825;&#20123;&#22270;&#20687;&#22312;&#35768;&#22810;&#19981;&#21516;&#30340;&#25552;&#31034;&#19979;&#33021;&#27450;&#39575;CLIP&#27169;&#22411;&#65292;&#32780;&#23545;&#20154;&#31867;&#26469;&#35828;&#26159;&#26080;&#27861;&#35748;&#20986;&#30340;&#12290;&#27450;&#39575;&#20027;&#22270;&#20687;&#22312;&#23569;&#37327;&#22270;&#20687;&#26631;&#39064;&#19978;&#30340;&#35757;&#32451;&#19978;&#21487;&#33021;&#36866;&#29992;&#20110;&#26356;&#22810;&#25968;&#37327;&#30340;&#35821;&#20041;&#30456;&#20851;&#30340;&#26631;&#39064;&#12290;&#20004;&#31181;&#21487;&#33021;&#30340;&#32531;&#35299;&#31574;&#30053;&#34987;&#35780;&#20272;&#65292;&#24182;&#21457;&#29616;&#33030;&#24369;&#24615;&#19982;&#23545;&#27604;&#24335;&#39044;&#35757;&#32451;&#20013;&#30340;&#27169;&#24577;&#24046;&#36317;&#23494;&#20999;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2307.03798</link><description>&lt;p&gt;
CLIPMasterPrints: &#20351;&#29992;&#28508;&#22312;&#21464;&#37327;&#28436;&#21270;&#27450;&#39575;&#23545;&#27604;&#24335;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
CLIPMasterPrints: Fooling Contrastive Language-Image Pre-training Using Latent Variable Evolution. (arXiv:2307.03798v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#23545;&#27604;&#24335;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#27169;&#22411;&#30340;&#33030;&#24369;&#24615;&#65292;&#36890;&#36807;&#25366;&#25496;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#21487;&#20197;&#25214;&#21040;&#27450;&#39575;&#20027;&#22270;&#20687;&#65292;&#36825;&#20123;&#22270;&#20687;&#22312;&#35768;&#22810;&#19981;&#21516;&#30340;&#25552;&#31034;&#19979;&#33021;&#27450;&#39575;CLIP&#27169;&#22411;&#65292;&#32780;&#23545;&#20154;&#31867;&#26469;&#35828;&#26159;&#26080;&#27861;&#35748;&#20986;&#30340;&#12290;&#27450;&#39575;&#20027;&#22270;&#20687;&#22312;&#23569;&#37327;&#22270;&#20687;&#26631;&#39064;&#19978;&#30340;&#35757;&#32451;&#19978;&#21487;&#33021;&#36866;&#29992;&#20110;&#26356;&#22810;&#25968;&#37327;&#30340;&#35821;&#20041;&#30456;&#20851;&#30340;&#26631;&#39064;&#12290;&#20004;&#31181;&#21487;&#33021;&#30340;&#32531;&#35299;&#31574;&#30053;&#34987;&#35780;&#20272;&#65292;&#24182;&#21457;&#29616;&#33030;&#24369;&#24615;&#19982;&#23545;&#27604;&#24335;&#39044;&#35757;&#32451;&#20013;&#30340;&#27169;&#24577;&#24046;&#36317;&#23494;&#20999;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20197;&#23545;&#27604;&#24335;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#20026;&#20195;&#34920;&#30340;&#21516;&#26102;&#21033;&#29992;&#35270;&#35273;&#21644;&#25991;&#26412;&#25968;&#25454;&#30340;&#27169;&#22411;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#22810;&#21151;&#33021;&#24615;&#65292;&#20294;&#23427;&#20204;&#23545;&#20110;&#25152;&#35859;&#30340;&#27450;&#39575;&#20027;&#22270;&#20687;&#26159;&#33030;&#24369;&#30340;&#12290;&#27450;&#39575;&#20027;&#22270;&#20687;&#33021;&#22815;&#26368;&#22823;&#21270;CLIP&#27169;&#22411;&#22312;&#35768;&#22810;&#19981;&#21516;&#30340;&#25552;&#31034;&#19979;&#30340;&#32622;&#20449;&#24230;&#35780;&#20998;&#65292;&#21516;&#26102;&#23545;&#20110;&#20154;&#31867;&#26469;&#35828;&#26159;&#26080;&#27861;&#35748;&#20986;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#20351;&#29992;&#28436;&#21270;&#31574;&#30053;&#25110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#22312;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#25628;&#32034;&#27450;&#39575;&#20027;&#22270;&#20687;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25366;&#25496;&#20986;&#30340;&#27450;&#39575;&#20027;&#22270;&#20687;&#30340;&#29305;&#24615;&#65292;&#24182;&#21457;&#29616;&#22312;&#23569;&#37327;&#22270;&#20687;&#26631;&#39064;&#19978;&#35757;&#32451;&#30340;&#22270;&#20687;&#21487;&#33021;&#36866;&#29992;&#20110;&#26356;&#22810;&#25968;&#37327;&#30340;&#35821;&#20041;&#30456;&#20851;&#30340;&#26631;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20004;&#31181;&#21487;&#33021;&#30340;&#32531;&#35299;&#31574;&#30053;&#65292;&#24182;&#21457;&#29616;&#23545;&#27450;&#39575;&#20027;&#20363;&#23376;&#30340;&#33030;&#24369;&#24615;&#19982;&#23545;&#27604;&#24335;&#39044;&#35757;&#32451;&#20013;&#30340;&#27169;&#24577;&#24046;&#36317;&#23494;&#20999;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Models leveraging both visual and textual data such as Contrastive Language-Image Pre-training (CLIP), are increasingly gaining importance. In this work, we show that despite their versatility, such models are vulnerable to what we refer to as fooling master images. Fooling master images are capable of maximizing the confidence score of a CLIP model for a significant number of widely varying prompts, while being unrecognizable for humans. We demonstrate how fooling master images can be mined by searching the latent space of generative models by means of an evolution strategy or stochastic gradient descent. We investigate the properties of the mined fooling master images, and find that images trained on a small number of image captions potentially generalize to a much larger number of semantically related captions. Further, we evaluate two possible mitigation strategies and find that vulnerability to fooling master examples is closely related to a modality gap in contrastive pre-trained
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#40065;&#26834;&#24615;&#20197;&#32553;&#23567;&#27169;&#25311;&#19982;&#30495;&#23454;&#24046;&#36317;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#20998;&#24067;&#40065;&#26834;&#20540;&#36845;&#20195;&#8221;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20248;&#21270;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.16589</link><description>&lt;p&gt;
&#20855;&#26377;&#29983;&#25104;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#20998;&#24067;&#40065;&#26834;&#24615;&#30340;&#21487;&#30097;&#20215;&#26684;
&lt;/p&gt;
&lt;p&gt;
The Curious Price of Distributional Robustness in Reinforcement Learning with a Generative Model. (arXiv:2305.16589v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16589
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#40065;&#26834;&#24615;&#20197;&#32553;&#23567;&#27169;&#25311;&#19982;&#30495;&#23454;&#24046;&#36317;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#20998;&#24067;&#40065;&#26834;&#20540;&#36845;&#20195;&#8221;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20248;&#21270;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#40065;&#26834;&#24615;&#65292;&#20197;&#20943;&#23569;&#22312;&#23454;&#36341;&#20013;&#30340;&#27169;&#25311;&#19982;&#30495;&#23454;&#24046;&#36317;&#12290;&#25105;&#20204;&#37319;&#29992;&#20998;&#24067;&#40065;&#26834;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;RMDPs&#65289;&#26694;&#26550;&#65292;&#26088;&#22312;&#23398;&#20064;&#19968;&#20010;&#31574;&#30053;&#65292;&#22312;&#37096;&#32626;&#29615;&#22659;&#33853;&#22312;&#39044;&#23450;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#20869;&#26102;&#65292;&#20248;&#21270;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#34920;&#29616;&#12290;&#23613;&#31649;&#26368;&#36817;&#26377;&#20102;&#19968;&#20123;&#21162;&#21147;&#65292;&#20294;RMDPs&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#20173;&#28982;&#27809;&#26377;&#24471;&#21040;&#35299;&#20915;&#65292;&#26080;&#35770;&#20351;&#29992;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#26159;&#20160;&#20040;&#12290;&#19981;&#28165;&#26970;&#20998;&#24067;&#40065;&#26834;&#24615;&#19982;&#26631;&#20934;&#24378;&#21270;&#23398;&#20064;&#30456;&#27604;&#26159;&#21542;&#20855;&#26377;&#32479;&#35745;&#23398;&#19978;&#30340;&#24433;&#21709;&#12290;&#20551;&#35774;&#26377;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#65292;&#26681;&#25454;&#21517;&#20041;MDP&#32472;&#21046;&#26679;&#26412;&#65292;&#25105;&#20204;&#23558;&#25551;&#36848;RMDPs&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24403;&#30001;&#24635;&#21464;&#24046;&#65288;TV&#65289;&#36317;&#31163;&#25110;$\chi^2$&#20998;&#27495;&#25351;&#23450;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#26102;&#12290;&#22312;&#36825;&#37324;&#30740;&#31350;&#30340;&#31639;&#27861;&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#20998;&#24067;&#40065;&#26834;&#20540;&#36845;&#20195;&#65292;&#35777;&#26126;&#20102;&#23427;&#22312;&#25972;&#20010;&#33539;&#22260;&#20869;&#37117;&#26159;&#36817;&#20046;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates model robustness in reinforcement learning (RL) to reduce the sim-to-real gap in practice. We adopt the framework of distributionally robust Markov decision processes (RMDPs), aimed at learning a policy that optimizes the worst-case performance when the deployed environment falls within a prescribed uncertainty set around the nominal MDP. Despite recent efforts, the sample complexity of RMDPs remained mostly unsettled regardless of the uncertainty set in use. It was unclear if distributional robustness bears any statistical consequences when benchmarked against standard RL.  Assuming access to a generative model that draws samples based on the nominal MDP, we characterize the sample complexity of RMDPs when the uncertainty set is specified via either the total variation (TV) distance or $\chi^2$ divergence. The algorithm studied here is a model-based method called {\em distributionally robust value iteration}, which is shown to be near-optimal for the full range
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20020;&#24202;&#21644;&#38750;&#20020;&#24202;&#25991;&#26412;&#19978;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#27169;&#22411;&#20013;&#20351;&#29992;&#25299;&#25169;&#21644;&#20960;&#20309;&#25968;&#25454;&#20998;&#26512;&#25216;&#26415;&#25512;&#26029;&#20027;&#35201;&#29305;&#24449;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.08642</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#25299;&#25169;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Topological Interpretability for Deep-Learning. (arXiv:2305.08642v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08642
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20020;&#24202;&#21644;&#38750;&#20020;&#24202;&#25991;&#26412;&#19978;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#27169;&#22411;&#20013;&#20351;&#29992;&#25299;&#25169;&#21644;&#20960;&#20309;&#25968;&#25454;&#20998;&#26512;&#25216;&#26415;&#25512;&#26029;&#20027;&#35201;&#29305;&#24449;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#31995;&#32479;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#24191;&#27867;&#65292;&#29702;&#35299;&#23427;&#20204;&#30340;&#20915;&#31574;&#26426;&#21046;&#30340;&#38656;&#27714;&#20063;&#30456;&#24212;&#21152;&#36895;&#12290;&#25105;&#20204;&#33021;&#22815;&#20449;&#20219;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#20915;&#31574;&#31995;&#32479;&#25152;&#20570;&#30340;&#32479;&#35745;&#25512;&#26029;&#30340;&#31243;&#24230;&#36234;&#26469;&#36234;&#25104;&#20026;&#19968;&#20010;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#39118;&#38505;&#30340;&#31995;&#32479;&#65292;&#20363;&#22914;&#21009;&#20107;&#21496;&#27861;&#25110;&#21307;&#23398;&#35786;&#26029;&#31995;&#32479;&#20013;&#65292;&#38169;&#35823;&#30340;&#25512;&#26029;&#21487;&#33021;&#20250;&#20135;&#29983;&#24754;&#21095;&#24615;&#30340;&#21518;&#26524;&#12290;&#23613;&#31649;&#22312;&#35299;&#20915;&#28041;&#21450;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#38382;&#39064;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#26080;&#27861;&#37327;&#21270;&#20854;&#39044;&#27979;&#30340;&#30830;&#23450;&#24615;&#12290;&#32780;&#19988;&#24403;&#20854;&#35299;&#20915;&#26041;&#26696;&#19981;&#27491;&#30830;&#26102;&#65292;&#36890;&#24120;&#20173;&#28982;&#38750;&#24120;&#33258;&#20449;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#20020;&#24202;&#21644;&#38750;&#20020;&#24202;&#25991;&#26412;&#19978;&#35757;&#32451;&#30340;&#20004;&#20010;DL&#20998;&#31867;&#27169;&#22411;&#20013;&#25512;&#26029;&#26480;&#20986;&#29305;&#24449;&#65292;&#37319;&#29992;&#20102;&#25299;&#25169;&#21644;&#20960;&#20309;&#25968;&#25454;&#20998;&#26512;&#25216;&#26415;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#27169;&#22411;&#39044;&#27979;&#31354;&#38388;&#30340;&#22270;&#24418;&#65292;&#24182;&#36890;&#36807;&#29305;&#24449;&#21644;&#39044;&#27979;&#30340;&#30456;&#20284;&#24615;&#23558;&#36755;&#20837;&#32858;&#31867;&#21040;&#22270;&#24418;&#30340;&#39030;&#28857;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing adoption of AI-based systems across everyday life, the need to understand their decision-making mechanisms is correspondingly accelerating. The level at which we can trust the statistical inferences made from AI-based decision systems is an increasing concern, especially in high-risk systems such as criminal justice or medical diagnosis, where incorrect inferences may have tragic consequences. Despite their successes in providing solutions to problems involving real-world data, deep learning (DL) models cannot quantify the certainty of their predictions. And are frequently quite confident, even when their solutions are incorrect.  This work presents a method to infer prominent features in two DL classification models trained on clinical and non-clinical text by employing techniques from topological and geometric data analysis. We create a graph of a model's prediction space and cluster the inputs into the graph's vertices by the similarity of features and prediction
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#26694;&#26550;&#65292;&#31216;&#20026;&#26657;&#20934;&#24863;&#30693;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476; (CAB)&#65292;&#29992;&#20110;&#20849;&#21516;&#35299;&#20915;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#26657;&#20934;&#21644;&#36125;&#21494;&#26031;&#23398;&#20064;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#27491;&#21017;&#21270;&#27169;&#22411;&#30340;&#21518;&#39564;&#39044;&#27979;&#20998;&#24067;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.07504</link><description>&lt;p&gt;
&#26657;&#20934;&#24863;&#30693;&#30340;&#36125;&#21494;&#26031;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Calibration-Aware Bayesian Learning. (arXiv:2305.07504v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07504
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#26694;&#26550;&#65292;&#31216;&#20026;&#26657;&#20934;&#24863;&#30693;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476; (CAB)&#65292;&#29992;&#20110;&#20849;&#21516;&#35299;&#20915;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#26657;&#20934;&#21644;&#36125;&#21494;&#26031;&#23398;&#20064;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#27491;&#21017;&#21270;&#27169;&#22411;&#30340;&#21518;&#39564;&#39044;&#27979;&#20998;&#24067;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21253;&#25324;&#29616;&#20195;&#31995;&#32479;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#25552;&#20379;&#20854;&#20915;&#31574;&#30340;&#19981;&#30830;&#23450;&#24615;&#26041;&#38754;&#24448;&#24448;&#26080;&#27861;&#25552;&#20379;&#21487;&#38752;&#30340;&#20272;&#35745;&#12290;&#20026;&#20102;&#25552;&#39640;&#27169;&#22411;&#32622;&#20449;&#27700;&#24179;&#65288;&#20063;&#31216;&#20026;&#26657;&#20934;&#65289;&#30340;&#36136;&#37327;&#65292;&#24120;&#35265;&#30340;&#26041;&#27861;&#21253;&#25324;&#21521;&#35757;&#32451;&#25439;&#22833;&#28155;&#21152;&#22522;&#20110;&#25968;&#25454;&#30340;&#25110;&#22522;&#20110;&#25968;&#25454;&#26080;&#20851;&#30340;&#27491;&#21017;&#21270;&#39033;&#12290;&#22312;&#20256;&#32479;&#30340;&#39057;&#29575;&#27966;&#23398;&#20064;&#19978;&#26368;&#36817;&#24341;&#20837;&#20102;&#22522;&#20110;&#25968;&#25454;&#30340;&#27491;&#21017;&#21270;&#22120;&#65292;&#20197;&#24809;&#32602;&#32622;&#20449;&#24230;&#21644;&#20934;&#30830;&#24230;&#20043;&#38388;&#30340;&#20559;&#24046;&#12290;&#30456;&#21453;&#65292;&#25968;&#25454;&#26080;&#20851;&#30340;&#27491;&#21017;&#21270;&#22120;&#22312;&#36125;&#21494;&#26031;&#23398;&#20064;&#30340;&#26680;&#24515;&#65292;&#24378;&#21046;&#20351;&#27169;&#22411;&#21442;&#25968;&#31354;&#38388;&#20013;&#30340;&#21464;&#20998;&#20998;&#24067;&#26381;&#20174;&#20808;&#39564;&#23494;&#24230;&#12290;&#21069;&#19968;&#31181;&#26041;&#27861;&#26080;&#27861;&#37327;&#21270;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#65292;&#32780;&#21518;&#32773;&#21017;&#20005;&#37325;&#21463;&#21040;&#27169;&#22411;&#38169;&#35823;&#35268;&#33539;&#30340;&#24433;&#21709;&#12290;&#37492;&#20110;&#20004;&#31181;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#26694;&#26550;&#65292;&#31216;&#20026;&#26657;&#20934;&#24863;&#30693;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476; (CAB)&#65292;&#29992;&#20110;&#20849;&#21516;&#35299;&#20915;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#26657;&#20934;&#21644;&#36125;&#21494;&#26031;&#23398;&#20064;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#28041;&#21450;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#30456;&#20851;&#24809;&#32602;&#39033;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#27491;&#21017;&#21270;&#27169;&#22411;&#30340;&#21518;&#39564;&#39044;&#27979;&#20998;&#24067;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning models, including modern systems like large language models, are well known to offer unreliable estimates of the uncertainty of their decisions. In order to improve the quality of the confidence levels, also known as calibration, of a model, common approaches entail the addition of either data-dependent or data-independent regularization terms to the training loss. Data-dependent regularizers have been recently introduced in the context of conventional frequentist learning to penalize deviations between confidence and accuracy. In contrast, data-independent regularizers are at the core of Bayesian learning, enforcing adherence of the variational distribution in the model parameter space to a prior density. The former approach is unable to quantify epistemic uncertainty, while the latter is severely affected by model misspecification. In light of the limitations of both methods, this paper proposes an integrated framework, referred to as calibration-aware Bayesian neural n
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Kullback-Leibler Maillard Sampling (KL-MS)&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#26377;&#30028;&#22870;&#21169;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#23454;&#29616;KL&#31354;&#38388;&#30340;&#25193;&#23637;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#28176;&#36817;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.14989</link><description>&lt;p&gt;
Kullback-Leibler Maillard&#37319;&#26679;&#22312;&#26377;&#30028;&#22870;&#21169;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Kullback-Leibler Maillard Sampling for Multi-armed Bandits with Bounded Rewards. (arXiv:2304.14989v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14989
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Kullback-Leibler Maillard Sampling (KL-MS)&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#26377;&#30028;&#22870;&#21169;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#23454;&#29616;KL&#31354;&#38388;&#30340;&#25193;&#23637;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#28176;&#36817;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22870;&#21169;&#20998;&#24067;&#38598;&#20013;&#22312;&#21306;&#38388;$[0,1]$&#20869;&#30340;$K$&#33218;&#25968;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Kullback-Leibler Maillard Sampling (KL-MS)&#30340;&#26032;&#31639;&#27861;&#65292;&#23427;&#26159;Maillard&#37319;&#26679;&#22312;KL&#31354;&#38388;&#30340;&#33258;&#28982;&#25193;&#23637;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;KL-MS&#22312;Bernoulli&#22870;&#21169;&#26102;&#20855;&#26377;&#28176;&#36817;&#26368;&#20248;&#24615;&#33021;&#65292;&#20854;&#26368;&#22351;&#24773;&#20917;&#36951;&#25022;&#24230;&#19978;&#30028;&#20026;$O(\sqrt{\mu^*(1-\mu^*) K T \ln K} + K \ln T)$&#65292;&#20854;&#20013;$\mu^*$&#26159;&#26368;&#20248;&#33218;&#30340;&#26399;&#26395;&#22870;&#21169;&#65292;$T$&#26159;&#26102;&#27573;&#38271;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study $K$-armed bandit problems where the reward distributions of the arms are all supported on the $[0,1]$ interval. It has been a challenge to design regret-efficient randomized exploration algorithms in this setting. Maillard sampling~\cite{maillard13apprentissage}, an attractive alternative to Thompson sampling, has recently been shown to achieve competitive regret guarantees in the sub-Gaussian reward setting~\cite{bian2022maillard} while maintaining closed-form action probabilities, which is useful for offline policy evaluation. In this work, we propose the Kullback-Leibler Maillard Sampling (KL-MS) algorithm, a natural extension of Maillard sampling for achieving KL-style gap-dependent regret bound. We show that KL-MS enjoys the asymptotic optimality when the rewards are Bernoulli and has a worst-case regret bound of the form $O(\sqrt{\mu^*(1-\mu^*) K T \ln K} + K \ln T)$, where $\mu^*$ is the expected reward of the optimal arm, and $T$ is the time horizon length.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#32858;&#21512;&#26799;&#24230;&#30340;&#24555;&#36895;&#25910;&#25947;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22343;&#20540;&#22330;&#26041;&#27861;&#26469;&#23436;&#25104;&#21442;&#25968;&#21644;&#26799;&#24230;&#30340;&#32858;&#21512;&#27493;&#39588;&#65292;&#35813;&#26041;&#27861;&#22312;&#25910;&#25947;&#36895;&#24230;&#21644;&#36890;&#20449;&#25104;&#26412;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.15799</link><description>&lt;p&gt;
&#24102;&#26377;&#32858;&#21512;&#26799;&#24230;&#30340;&#24555;&#36895;&#25910;&#25947;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Fast Convergence Federated Learning with Aggregated Gradients. (arXiv:2303.15799v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15799
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#32858;&#21512;&#26799;&#24230;&#30340;&#24555;&#36895;&#25910;&#25947;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22343;&#20540;&#22330;&#26041;&#27861;&#26469;&#23436;&#25104;&#21442;&#25968;&#21644;&#26799;&#24230;&#30340;&#32858;&#21512;&#27493;&#39588;&#65292;&#35813;&#26041;&#27861;&#22312;&#25910;&#25947;&#36895;&#24230;&#21644;&#36890;&#20449;&#25104;&#26412;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#20351;&#22810;&#20010;&#20998;&#24067;&#24335;&#35774;&#22791;&#22312;&#20445;&#25252;&#26412;&#22320;&#25968;&#25454;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#20013;&#22830;&#26381;&#21153;&#22120;&#21327;&#21516;&#35757;&#32451;&#20849;&#20139;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#38750;&#29420;&#31435;&#21644;&#21516;&#20998;&#24067;&#65288;Non-IID&#65289;&#30340;&#25968;&#25454;&#26679;&#26412;&#20197;&#21450;&#21442;&#19982;&#32773;&#20043;&#38388;&#39057;&#32321;&#30340;&#36890;&#20449;&#23558;&#20943;&#32531;&#25910;&#25947;&#36895;&#29575;&#24182;&#22686;&#21152;&#36890;&#20449;&#25104;&#26412;&#12290;&#20026;&#20102;&#23454;&#29616;&#24555;&#36895;&#25910;&#25947;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#24120;&#35268;&#26412;&#22320;&#26356;&#26032;&#35268;&#21017;&#20013;&#24341;&#20837;&#32858;&#21512;&#26799;&#24230;&#26469;&#25913;&#21892;&#26412;&#22320;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#31639;&#27861;&#65292;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#36827;&#19968;&#27493;&#32771;&#34385;&#26412;&#22320;&#21442;&#25968;&#21644;&#20840;&#23616;&#21442;&#25968;&#30340;&#20559;&#24046;&#12290;&#20197;&#19978;&#31574;&#30053;&#35201;&#27714;&#22312;&#27599;&#20010;&#26412;&#22320;&#36845;&#20195;&#20013;&#25910;&#38598;&#25152;&#26377;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#21442;&#25968;&#21644;&#26799;&#24230;&#65292;&#30001;&#20110;&#26412;&#22320;&#26356;&#26032;&#26399;&#38388;&#27809;&#26377;&#36890;&#20449;&#65292;&#36825;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#22343;&#20540;&#22330;&#26041;&#27861;&#65292;&#24341;&#20837;&#31216;&#20026;&#20840;&#23616;&#22343;&#20540;&#22330;&#21644;&#26412;&#22320;&#22343;&#20540;&#22330;&#30340;&#20004;&#20010;&#22343;&#20540;&#22330;&#26415;&#35821;&#26469;&#23436;&#25104;&#32858;&#21512;&#27493;&#39588;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#25910;&#25947;&#36895;&#24230;&#21644;&#36890;&#20449;&#25104;&#26412;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a novel machine learning framework, which enables multiple distributed devices cooperatively training a shared model scheduled by a central server while protecting private data locally. However, the non-independent-and-identically-distributed (Non-IID) data samples and frequent communication among participants will slow down the convergent rate and increase communication costs. To achieve fast convergence, we ameliorate the local gradient descend approach in conventional local update rule by introducing the aggregated gradients at each local update epoch, and propose an adaptive learning rate algorithm that further takes the deviation of local parameter and global parameter into consideration at each iteration. The above strategy requires all clients' local parameters and gradients at each local iteration, which is challenging as there is no communication during local update epochs. Accordingly, we utilize mean field approach by introducing two mean field ter
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#26041;&#26696;&#65292;&#22522;&#20110;&#29109;&#29702;&#35770;&#32531;&#35299;&#24322;&#26500;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#20559;&#24046;&#65292;&#23454;&#29616;&#20840;&#23616;&#27169;&#22411;&#30340;&#24555;&#36895;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2303.14966</link><description>&lt;p&gt;
&#26032;&#29109;&#26041;&#27861;&#30340;&#33258;&#36866;&#24212;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Adaptive Federated Learning via New Entropy Approach. (arXiv:2303.14966v2 [cs.DC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14966
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#26041;&#26696;&#65292;&#22522;&#20110;&#29109;&#29702;&#35770;&#32531;&#35299;&#24322;&#26500;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#20559;&#24046;&#65292;&#23454;&#29616;&#20840;&#23616;&#27169;&#22411;&#30340;&#24555;&#36895;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064; (FL) &#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#26694;&#26550;&#65292;&#23427;&#20801;&#35768;&#36164;&#28304;&#21463;&#38480;&#30340;&#31163;&#25955;&#23458;&#25143;&#31471;&#22312;&#20013;&#22830;&#26381;&#21153;&#22120;&#30340;&#21327;&#35843;&#19979;&#65292;&#36890;&#36807;&#22312;&#26412;&#22320;&#23384;&#20648;&#20445;&#25252;&#38544;&#31169;&#25968;&#25454;&#30340;&#26041;&#24335;&#65292;&#20849;&#21516;&#23398;&#20064;&#20840;&#23616;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24322;&#26500;&#23458;&#25143;&#31471;&#30340;&#35774;&#22791;&#21644;&#25968;&#25454;&#24046;&#24322;&#20250;&#23548;&#33268;&#26412;&#22320;&#27169;&#22411;&#21442;&#25968;&#30340;&#20559;&#24046;&#65292;&#36827;&#32780;&#23548;&#33268;&#20840;&#23616;&#27169;&#22411;&#30340;&#25910;&#25947;&#36895;&#24230;&#20943;&#24930;&#21644;&#31934;&#24230;&#38477;&#20302;&#12290;&#24403;&#21069;&#30340; FL &#31639;&#27861;&#26222;&#36941;&#37319;&#29992;&#38745;&#24577;&#23458;&#25143;&#31471;&#23398;&#20064;&#31574;&#30053;&#24182;&#19981;&#33021;&#36866;&#24212;&#19981;&#21516;&#23458;&#25143;&#31471;&#30340;&#21160;&#24577;&#35757;&#32451;&#21442;&#25968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26681;&#25454;&#29109;&#29702;&#35770;&#32771;&#34385;&#19981;&#21516;&#26412;&#22320;&#27169;&#22411;&#21442;&#25968;&#20043;&#38388;&#30340;&#20559;&#24046;&#65292;&#20026;&#27599;&#20010;&#23458;&#25143;&#31471;&#25552;&#20986;&#20102;&#22522;&#20110;&#29109;&#29702;&#35770;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#26041;&#26696;&#65292;&#20197;&#32531;&#35299;&#24322;&#26500;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#20559;&#24046;&#65292;&#23454;&#29616;&#20840;&#23616;&#27169;&#22411;&#30340;&#24555;&#36895;&#25910;&#25947;&#12290;&#20294;&#30001;&#20110;&#19981;&#21516;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#25968;&#25454;&#38598;&#21644;&#29305;&#24449;&#20855;&#26377;&#26174;&#33879;&#30340;&#24046;&#24322;&#65292;&#35774;&#35745;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#26368;&#20248;&#21160;&#24577;&#23398;&#20064;&#29575;&#26159;&#22256;&#38590;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) has recently emerged as a popular framework, which allows resource-constrained discrete clients to cooperatively learn the global model under the orchestration of a central server while storing privacy-sensitive data locally. However, due to the difference in equipment and data divergence of heterogeneous clients, there will be parameter deviation between local models, resulting in a slow convergence rate and a reduction of the accuracy of the global model. The current FL algorithms use the static client learning strategy pervasively and can not adapt to the dynamic training parameters of different clients. In this paper, by considering the deviation between different local model parameters, we propose an adaptive learning rate scheme for each client based on entropy theory to alleviate the deviation between heterogeneous clients and achieve fast convergence of the global model. It's difficult to design the optimal dynamic learning rate for each client as the lo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Proximal Newton&#31639;&#27861;&#30340;&#39640;&#25928;&#22270;&#25289;&#26222;&#25289;&#26031;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#38750;&#20984;minimax concave penalty&#65292;&#24182;&#21033;&#29992;&#20108;&#38454;&#20248;&#21270;&#26041;&#27861;&#21644;&#20960;&#20010;&#31639;&#27861;&#25216;&#24039;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#19988;&#39640;&#25928;&#30340;&#27714;&#35299;&#22120;&#12290;</title><link>http://arxiv.org/abs/2302.06434</link><description>&lt;p&gt;
Proximal Newton&#31639;&#27861;&#23454;&#29616;&#39640;&#25928;&#22270;&#25289;&#26222;&#25289;&#26031;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Efficient Graph Laplacian Estimation by Proximal Newton. (arXiv:2302.06434v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06434
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Proximal Newton&#31639;&#27861;&#30340;&#39640;&#25928;&#22270;&#25289;&#26222;&#25289;&#26031;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#38750;&#20984;minimax concave penalty&#65292;&#24182;&#21033;&#29992;&#20108;&#38454;&#20248;&#21270;&#26041;&#27861;&#21644;&#20960;&#20010;&#31639;&#27861;&#25216;&#24039;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#19988;&#39640;&#25928;&#30340;&#27714;&#35299;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Laplacian&#32422;&#26463;&#30340;&#39640;&#26031;&#39532;&#23572;&#31185;&#22827;&#38543;&#26426;&#22330;&#65288;LGMRF&#65289;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#22810;&#20803;&#32479;&#35745;&#27169;&#22411;&#65292;&#29992;&#20110;&#20174;&#32473;&#23450;&#25968;&#25454;&#20013;&#23398;&#20064;&#26435;&#37325;&#31232;&#30095;&#30340;&#20381;&#36182;&#22270;&#12290;&#36825;&#20010;&#22270;&#23398;&#20064;&#38382;&#39064;&#21487;&#20197;&#34987;&#24418;&#24335;&#21270;&#20026;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65288;MLE&#65289;&#30340;&#31934;&#24230;&#30697;&#38453;&#65292;&#21463;&#21040;Laplacian&#32467;&#26500;&#32422;&#26463;&#30340;&#38480;&#21046;&#65292;&#24182;&#24102;&#26377;&#31232;&#30095;&#24615;&#35825;&#23548;&#30340;&#24809;&#32602;&#39033;&#12290;&#26412;&#35770;&#25991;&#26088;&#22312;&#20934;&#30830;&#19988;&#39640;&#25928;&#22320;&#35299;&#20915;&#36825;&#20010;&#23398;&#20064;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#30001;&#20110;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#36890;&#24120;&#20351;&#29992;&#30340;$\ell_1$-&#33539;&#25968;&#24809;&#32602;&#19981;&#36866;&#29992;&#24182;&#19988;&#21487;&#33021;&#23548;&#33268;&#23436;&#20840;&#22270;&#65292;&#25152;&#20197;&#25105;&#20204;&#37319;&#29992;&#20102;&#38750;&#20984;&#30340;MCP&#65288;minimax concave penalty&#65289;&#65292;&#23427;&#20419;&#36827;&#20855;&#26377;&#26356;&#20302;&#20272;&#35745;&#20559;&#24046;&#30340;&#31232;&#30095;&#35299;&#12290;&#20854;&#27425;&#65292;&#19982;&#29616;&#26377;&#30340;&#35813;&#38382;&#39064;&#30340;&#19968;&#38454;&#26041;&#27861;&#30456;&#21453;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#20108;&#38454;proximal Newton&#26041;&#27861;&#26469;&#33719;&#24471;&#39640;&#25928;&#30340;&#27714;&#35299;&#22120;&#65292;&#21033;&#29992;&#20102;&#22810;&#31181;&#31639;&#27861;&#29305;&#24615;&#65292;&#22914;&#20351;&#29992;&#20849;&#36717;&#26799;&#24230;&#27861;&#12289;&#39044;&#26465;&#20214;&#21270;&#21644;&#20998;&#21106;&#21040;&#27963;&#21160;/&#33258;&#30001;&#38598;&#12290;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Laplacian-constrained Gaussian Markov Random Field (LGMRF) is a common multivariate statistical model for learning a weighted sparse dependency graph from given data. This graph learning problem can be formulated as a maximum likelihood estimation (MLE) of the precision matrix, subject to Laplacian structural constraints, with a sparsity-inducing penalty term. This paper aims to solve this learning problem accurately and efficiently. First, since the commonly used $\ell_1$-norm penalty is inappropriate in this setting and may lead to a complete graph, we employ the nonconvex minimax concave penalty (MCP), which promotes sparse solutions with lower estimation bias. Second, as opposed to existing first-order methods for this problem, we develop a second-order proximal Newton approach to obtain an efficient solver, utilizing several algorithmic features, such as using Conjugate Gradients, preconditioning, and splitting to active/free sets. Numerical experiments demonstrate the advanta
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#27169;&#24335;&#35774;&#32622;&#19979;&#65292;&#21482;&#20351;&#29992;&#25991;&#26412;&#36827;&#34892;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#33021;&#22815;&#25552;&#20379;&#24378;&#22823;&#30340;&#36741;&#21161;&#26469;&#36827;&#34892;&#35821;&#27861;&#24402;&#32435;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22522;&#20110;LLM&#30340;&#32431;&#25991;&#26412;&#26041;&#27861;&#22312;&#22810;&#31181;&#22810;&#27169;&#24335;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#24615;&#33021;&#12289;&#21442;&#25968;&#25968;&#37327;&#21644;&#35757;&#32451;&#36895;&#24230;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2212.10564</link><description>&lt;p&gt;
&#26080;&#35270;&#35273;&#22522;&#32447;&#30340;&#22810;&#27169;&#24335;&#35821;&#27861;&#24402;&#32435;
&lt;/p&gt;
&lt;p&gt;
A Vision-free Baseline for Multimodal Grammar Induction. (arXiv:2212.10564v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#27169;&#24335;&#35774;&#32622;&#19979;&#65292;&#21482;&#20351;&#29992;&#25991;&#26412;&#36827;&#34892;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#33021;&#22815;&#25552;&#20379;&#24378;&#22823;&#30340;&#36741;&#21161;&#26469;&#36827;&#34892;&#35821;&#27861;&#24402;&#32435;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22522;&#20110;LLM&#30340;&#32431;&#25991;&#26412;&#26041;&#27861;&#22312;&#22810;&#31181;&#22810;&#27169;&#24335;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#24615;&#33021;&#12289;&#21442;&#25968;&#25968;&#37327;&#21644;&#35757;&#32451;&#36895;&#24230;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#37197;&#23545;&#30340;&#35270;&#35273;&#19982;&#35821;&#35328;&#20449;&#21495;&#33021;&#22815;&#26174;&#33879;&#25913;&#21892;&#22810;&#27169;&#24335;&#25968;&#25454;&#38598;&#65288;&#22914;MSCOCO&#65289;&#20013;&#30340;&#35821;&#27861;&#24402;&#32435;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#21482;&#20351;&#29992;&#25991;&#26412;&#36827;&#34892;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22810;&#27169;&#24335;&#35774;&#32622;&#19979;&#26159;&#21542;&#33021;&#22815;&#25552;&#20379;&#24378;&#22823;&#30340;&#36741;&#21161;&#26469;&#36827;&#34892;&#35821;&#27861;&#24402;&#32435;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25105;&#20204;&#30340;&#32431;&#25991;&#26412;&#26041;&#27861;&#65292;&#21363;&#22522;&#20110;LLM&#30340;C-PCFG&#65288;LC-PCFG&#65289;&#65292;&#22312;&#21508;&#31181;&#22810;&#27169;&#24335;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20808;&#21069;&#30340;&#22810;&#27169;&#24335;&#26041;&#27861;&#65292;&#24182;&#19988;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#35821;&#27861;&#24402;&#32435;&#24615;&#33021;&#12290;&#19982;&#24102;&#22270;&#20687;&#30340;&#35821;&#27861;&#24402;&#32435;&#30456;&#27604;&#65292;LC-PCFG&#22312;&#35821;&#26009;&#24211;F1&#24471;&#20998;&#19978;&#36229;&#36807;&#20102;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;7.9&#20010;&#28857;&#65292;&#21442;&#25968;&#25968;&#37327;&#20943;&#23569;&#20102;85&#65285;&#65292;&#35757;&#32451;&#36895;&#24230;&#21152;&#24555;&#20102;1.7&#20493;&#12290;&#22312;&#19977;&#20010;&#36741;&#21161;&#35270;&#39057;&#30340;&#35821;&#27861;&#24402;&#32435;&#22522;&#20934;&#20013;&#65292;LC-PCFG&#22312;&#35821;&#26009;&#24211;F1&#19978;&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#26368;&#22810;7.7&#20010;&#28857;&#65292;&#35757;&#32451;&#36895;&#24230;&#21152;&#24555;&#20102;8.8&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Past work has shown that paired vision-language signals substantially improve grammar induction in multimodal datasets such as MSCOCO. We investigate whether advancements in large language models (LLMs) that are only trained with text could provide strong assistance for grammar induction in multimodal settings. We find that our text-only approach, an LLM-based C-PCFG (LC-PCFG), outperforms previous multi-modal methods, and achieves state-of-the-art grammar induction performance for various multimodal datasets. Compared to image-aided grammar induction, LC-PCFG outperforms the prior state-of-the-art by 7.9 Corpus-F1 points, with an 85% reduction in parameter count and 1.7x faster training speed. Across three video-assisted grammar induction benchmarks, LC-PCFG outperforms prior state-of-the-art by up to 7.7 Corpus-F1, with 8.8x faster training. These results shed light on the notion that text-only language models might include visually grounded cues that aid in grammar induction in mult
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26059;&#36716;&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#65292;&#36890;&#36807;&#32422;&#26463;&#20445;&#25345;&#26059;&#36716;&#24179;&#31227;&#31561;&#21464;&#24615;&#30340;&#26041;&#24335;&#65292;&#23398;&#20064;&#29627;&#29827;&#28082;&#20307;&#38745;&#24577;&#32467;&#26500;&#30340;&#31283;&#20581;&#34920;&#31034;&#12290;&#36825;&#31181;&#32422;&#26463;&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#27979;&#33021;&#21147;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#21442;&#25968;&#25968;&#37327;&#65292;&#24182;&#19988;&#25552;&#39640;&#20102;&#35299;&#37322;&#24615;&#12290;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#23454;&#39564;&#35777;&#26126;&#20102;&#32593;&#32476;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.03226</link><description>&lt;p&gt;
&#23398;&#20064;&#29627;&#29827;&#28082;&#20307;&#34920;&#31034;&#30340;&#26059;&#36716;&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Rotation-equivariant Graph Neural Networks for Learning Glassy Liquids Representations. (arXiv:2211.03226v2 [cond-mat.soft] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.03226
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26059;&#36716;&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#65292;&#36890;&#36807;&#32422;&#26463;&#20445;&#25345;&#26059;&#36716;&#24179;&#31227;&#31561;&#21464;&#24615;&#30340;&#26041;&#24335;&#65292;&#23398;&#20064;&#29627;&#29827;&#28082;&#20307;&#38745;&#24577;&#32467;&#26500;&#30340;&#31283;&#20581;&#34920;&#31034;&#12290;&#36825;&#31181;&#32422;&#26463;&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#27979;&#33021;&#21147;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#21442;&#25968;&#25968;&#37327;&#65292;&#24182;&#19988;&#25552;&#39640;&#20102;&#35299;&#37322;&#24615;&#12290;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#23454;&#39564;&#35777;&#26126;&#20102;&#32593;&#32476;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29627;&#29827;&#28082;&#20307;&#30740;&#31350;&#39046;&#22495;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#23545;&#31890;&#23376;&#30340;&#38745;&#24577;&#32467;&#26500;&#36827;&#34892;&#24314;&#27169;&#26159;&#19968;&#20010;&#28909;&#38376;&#35805;&#39064;&#12290;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#65292;&#23427;&#20855;&#26377;&#24378;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#20294;&#26159;&#27169;&#22411;&#21442;&#25968;&#22810;&#19988;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#21463;&#26426;&#22120;&#23398;&#20064;&#32676;&#31561;&#21464;&#34920;&#31034;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;GNN&#65292;&#36890;&#36807;&#32422;&#26463;&#20854;&#20445;&#25345;&#26059;&#36716;&#24179;&#31227;&#65288;SE&#65288;3&#65289;&#65289;&#31561;&#21464;&#24615;&#65292;&#23398;&#20064;&#29627;&#29827;&#38745;&#24577;&#32467;&#26500;&#30340;&#31283;&#20581;&#34920;&#31034;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#31181;&#32422;&#26463;&#19981;&#20165;&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#27979;&#33021;&#21147;&#65292;&#36824;&#25552;&#39640;&#20102;&#23545;&#26410;&#35265;&#36807;&#28201;&#24230;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21516;&#26102;&#21487;&#20197;&#20943;&#23569;&#21442;&#25968;&#25968;&#37327;&#12290;&#27492;&#22806;&#65292;&#35299;&#37322;&#24615;&#20063;&#24471;&#21040;&#20102;&#25913;&#21892;&#65292;&#22240;&#20026;&#25105;&#20204;&#21487;&#20197;&#23558;&#22522;&#26412;&#21367;&#31215;&#23618;&#30340;&#20316;&#29992;&#19982;&#20247;&#25152;&#21608;&#30693;&#30340;&#26059;&#36716;&#19981;&#21464;&#19987;&#23478;&#29305;&#24449;&#32852;&#31995;&#36215;&#26469;&#12290;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#32593;&#32476;&#23398;&#20064;&#20102;&#19968;&#20010;&#31283;&#20581;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Within the glassy liquids community, the use of Machine Learning (ML) to model particles' static structure is currently a hot topic. The state of the art consists in Graph Neural Networks (GNNs), which have a great expressive power but are heavy models with numerous parameters and lack interpretability. Inspired by recent advances in the field of Machine Learning group-equivariant representations, we build a GNN that learns a robust representation of the glass' static structure by constraining it to preserve the roto-translation (SE(3)) equivariance. We show that this constraint not only significantly improves the predictive power but also improves the ability to generalize to unseen temperatures while allowing to reduce the number of parameters. Furthermore, interpretability is improved, as we can relate the action of our basic convolution layer to well-known rotation-invariant expert features. Through transfer-learning experiments we demonstrate that our network learns a robust repre
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25277;&#35937;&#27169;&#22411;&#65292;&#30740;&#31350;&#20154;&#21592;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#38750;&#32447;&#24615;&#31181;&#32676;&#30721;&#23454;&#29616;&#33258;&#28982;&#22270;&#20687;&#22359;&#30340;&#39640;&#25928;&#34920;&#31034;&#65292;&#20197;&#23454;&#29616;&#26089;&#26399;&#35270;&#35273;&#31995;&#32479;&#30340;&#20449;&#24687;&#20256;&#36755;&#21644;&#20256;&#24863;&#22120;&#27010;&#29575;&#20998;&#24067;&#24314;&#27169;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2210.13004</link><description>&lt;p&gt;
&#33258;&#28982;&#22270;&#20687;&#22359;&#30340;&#39640;&#25928;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Efficient Representation of Natural Image Patches. (arXiv:2210.13004v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13004
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25277;&#35937;&#27169;&#22411;&#65292;&#30740;&#31350;&#20154;&#21592;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#38750;&#32447;&#24615;&#31181;&#32676;&#30721;&#23454;&#29616;&#33258;&#28982;&#22270;&#20687;&#22359;&#30340;&#39640;&#25928;&#34920;&#31034;&#65292;&#20197;&#23454;&#29616;&#26089;&#26399;&#35270;&#35273;&#31995;&#32479;&#30340;&#20449;&#24687;&#20256;&#36755;&#21644;&#20256;&#24863;&#22120;&#27010;&#29575;&#20998;&#24067;&#24314;&#27169;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31070;&#32463;&#20449;&#24687;&#22788;&#29702;&#30340;&#22797;&#26434;&#39046;&#22495;&#20013;&#65292;&#20174;&#27425;&#35201;&#32454;&#33410;&#20013;&#36776;&#21035;&#20986;&#22522;&#26412;&#21407;&#29702;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#23613;&#31649;&#25105;&#20204;&#24050;&#32463;&#23545;&#26089;&#26399;&#35270;&#35273;&#31995;&#32479;&#30340;&#35299;&#21078;&#23398;&#21644;&#29983;&#29702;&#23398;&#26377;&#20102;&#24191;&#27867;&#30340;&#20102;&#35299;&#65292;&#20294;&#19968;&#20010;&#20840;&#38754;&#30340;&#35745;&#31639;&#29702;&#35770;&#20173;&#28982;&#38590;&#20197;&#25417;&#25720;&#12290;&#25105;&#20204;&#33021;&#21542;&#36890;&#36807;&#25277;&#35937;&#20986;&#31995;&#32479;&#30340;&#35814;&#32454;&#23454;&#29616;&#24182;&#19987;&#27880;&#20110;&#31995;&#32479;&#26088;&#22312;&#35299;&#20915;&#30340;&#22522;&#26412;&#38382;&#39064;&#65292;&#26469;&#27934;&#23519;&#29983;&#29289;&#31995;&#32479;&#30340;&#22522;&#26412;&#21407;&#29702;&#21602;&#65311;&#21033;&#29992;&#22522;&#20110;&#26368;&#31616;&#21270;&#32780;&#21448;&#29616;&#23454;&#30340;&#20551;&#35774;&#30340;&#19968;&#20010;&#25277;&#35937;&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23454;&#29616;&#26089;&#26399;&#35270;&#35273;&#31995;&#32479;&#30340;&#20004;&#20010;&#26368;&#32456;&#30446;&#26631;&#65306;&#39640;&#25928;&#30340;&#20449;&#24687;&#20256;&#36755;&#21644;&#20256;&#24863;&#22120;&#27010;&#29575;&#20998;&#24067;&#24314;&#27169;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#20026;&#20102;&#20248;&#21270;&#20449;&#24687;&#20256;&#36755;&#65292;&#24182;&#19981;&#24847;&#21619;&#30528;&#33719;&#24471;&#20102;&#26368;&#20248;&#30340;&#27010;&#29575;&#20998;&#24067;&#24314;&#27169;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#20108;&#32500;&#31995;&#32479;&#21644;&#22270;&#20687;&#22359;&#26469;&#35828;&#26126;&#65292;&#21487;&#20197;&#36890;&#36807;&#30001;&#20004;&#20010;&#31867;&#22411;&#39537;&#21160;&#30340;&#38750;&#32447;&#24615;&#31181;&#32676;&#30721;&#23454;&#29616;&#19968;&#20010;&#39640;&#25928;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the complex domain of neural information processing, discerning fundamental principles from ancillary details remains a significant challenge. While there is extensive knowledge about the anatomy and physiology of the early visual system, a comprehensive computational theory remains elusive. Can we gain insights into the underlying principles of a biological system by abstracting away from its detailed implementation and focusing on the fundamental problems that the system is designed to solve? Utilizing an abstract model based on minimal yet realistic assumptions, we show how to achieve the early visual system's two ultimate objectives: efficient information transmission and sensor probability distribution modeling. We show that optimizing for information transmission does not yield optimal probability distribution modeling. We illustrate, using a two-pixel (2D) system and image patches, that an efficient representation can be realized via nonlinear population code driven by two ty
&lt;/p&gt;</description></item></channel></rss>