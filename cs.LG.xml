<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#19987;&#38376;&#38024;&#23545;&#22768;&#23398;&#39046;&#22495;&#19981;&#21305;&#37197;&#24773;&#20917;&#19979;&#30340;&#33258;&#21160;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26469;&#35299;&#20915;&#35821;&#38899;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#34920;&#29616;&#38382;&#39064;&#65292;&#24182;&#22312;&#20302;&#36164;&#28304;&#39046;&#22495;&#20013;&#27604;&#22522;&#32447;&#23637;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.00481</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#22495;&#36866;&#24212;&#24494;&#35843;&#30340;&#33258;&#21160;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Automatic Data Augmentation for Domain Adapted Fine-Tuning of Self-Supervised Speech Representations. (arXiv:2306.00481v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00481
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#19987;&#38376;&#38024;&#23545;&#22768;&#23398;&#39046;&#22495;&#19981;&#21305;&#37197;&#24773;&#20917;&#19979;&#30340;&#33258;&#21160;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26469;&#35299;&#20915;&#35821;&#38899;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#34920;&#29616;&#38382;&#39064;&#65292;&#24182;&#22312;&#20302;&#36164;&#28304;&#39046;&#22495;&#20013;&#27604;&#22522;&#32447;&#23637;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#20351;&#24471;&#21033;&#29992;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;&#35821;&#38899;&#25968;&#25454;&#26469;&#25552;&#39640;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#30340;&#24615;&#33021;&#25104;&#20026;&#21487;&#33021;&#65292;&#21363;&#20351;&#26159;&#21482;&#26377;&#23569;&#37327;&#26377;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#20063;&#33021;&#20570;&#21040;&#12290;&#28982;&#32780;&#65292;&#35821;&#38899;SSL&#34920;&#31034;&#21487;&#33021;&#20250;&#22312;&#39044;&#35757;&#32451;&#21644;&#30446;&#26631;&#25968;&#25454;&#38598;&#20043;&#38388;&#36935;&#21040;&#22768;&#23398;&#19981;&#21305;&#37197;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30417;&#30563;&#22495;&#36866;&#24212;&#26041;&#27861;&#65292;&#19987;&#38376;&#38024;&#23545;&#22768;&#23398;&#39046;&#22495;&#23384;&#22312;&#19981;&#21305;&#37197;&#30340;&#24773;&#20917;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#23545;&#22823;&#22411;&#24178;&#20928;&#25968;&#25454;&#38598;&#24212;&#29992;&#32463;&#36807;&#36866;&#24403;&#26657;&#20934;&#30340;&#25968;&#25454;&#22686;&#24378;&#65292;&#20351;&#20854;&#26356;&#25509;&#36817;&#30446;&#26631;&#22495;&#65292;&#24182;&#23558;&#20854;&#29992;&#20316;&#21021;&#22987;&#24494;&#35843;&#38454;&#27573;&#30340;&#19968;&#37096;&#20998;&#12290;&#22686;&#24378;&#36890;&#36807;&#38024;&#23545;&#30446;&#26631;&#25968;&#25454;&#38598;&#22522;&#20110;&#26465;&#20214;&#20381;&#36182;&#24615;&#20272;&#35745;&#22120;&#30340;&#26368;&#23567;&#21270;&#33258;&#21160;&#36873;&#25321;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25511;&#21046;&#22833;&#30495;&#36827;&#34892;&#20102;&#31070;&#35861;&#23454;&#39564;&#24182;&#22312;&#20004;&#20010;&#19994;&#20313;&#25910;&#38598;&#30340;&#20302;&#36164;&#28304;&#39046;&#22495;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#22312;&#20004;&#31181;&#24773;&#20917;&#19979;&#22343;&#27604;&#22522;&#32447;&#23637;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Learning (SSL) has allowed leveraging large amounts of unlabeled speech data to improve the performance of speech recognition models even with small annotated datasets. Despite this, speech SSL representations may fail while facing an acoustic mismatch between the pretraining and target datasets. To address this issue, we propose a novel supervised domain adaptation method, designed for cases exhibiting such a mismatch in acoustic domains. It consists in applying properly calibrated data augmentations on a large clean dataset, bringing it closer to the target domain, and using it as part of an initial fine-tuning stage. Augmentations are automatically selected through the minimization of a conditional-dependence estimator, based on the target dataset. The approach is validated during an oracle experiment with controlled distortions and on two amateur-collected low-resource domains, reaching better performances compared to the baselines in both cases.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23581;&#35797;&#23454;&#29616;&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#36816;&#29992;&#21487;&#36870;&#27169;&#22411;&#23454;&#29616;&#39640;&#25928;&#30340;&#24494;&#35843;&#65292;&#24182;&#21457;&#29616;&#22312;&#21021;&#22987;&#21270;&#24494;&#35843;&#26102;&#20445;&#30041;PLM&#30340;&#36215;&#28857;&#38750;&#24120;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2306.00477</link><description>&lt;p&gt;
&#20351;&#39044;&#35757;&#32451;&#27169;&#22411;&#20855;&#26377;&#21487;&#36870;&#24615;&#65306;&#20174;&#21442;&#25968;&#21040;&#20869;&#23384;&#39640;&#25928;&#30340;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Make Your Pre-trained Model Reversible: From Parameter to Memory Efficient Fine-Tuning. (arXiv:2306.00477v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23581;&#35797;&#23454;&#29616;&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#36816;&#29992;&#21487;&#36870;&#27169;&#22411;&#23454;&#29616;&#39640;&#25928;&#30340;&#24494;&#35843;&#65292;&#24182;&#21457;&#29616;&#22312;&#21021;&#22987;&#21270;&#24494;&#35843;&#26102;&#20445;&#30041;PLM&#30340;&#36215;&#28857;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#38750;&#24120;&#25104;&#21151;&#30340;&#26041;&#27861;&#65292;&#21482;&#38656;&#35757;&#32451;&#23569;&#37327;&#21442;&#25968;&#32780;&#19981;&#20250;&#38477;&#20302;&#24615;&#33021;&#65292;&#24182;&#38543;&#30528;PLM&#36234;&#26469;&#36234;&#22823;&#32780;&#25104;&#20026;&#20107;&#23454;&#19978;&#30340;&#23398;&#20064;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;PEFT&#26041;&#27861;&#19981;&#20855;&#22791;&#20869;&#23384;&#25928;&#29575;&#65292;&#22240;&#20026;&#23427;&#20204;&#20173;&#38656;&#35201;&#23384;&#20648;&#22823;&#37096;&#20998;&#20013;&#38388;&#28608;&#27963;&#20540;&#20197;&#20415;&#35745;&#31639;&#26799;&#24230;&#65292;&#31867;&#20284;&#20110;&#24494;&#35843;&#12290;&#19968;&#20010;&#20943;&#23569;&#28608;&#27963;&#20869;&#23384;&#30340;&#26377;&#25928;&#26041;&#27861;&#26159;&#24212;&#29992;&#21487;&#36870;&#27169;&#22411;&#65292;&#36825;&#26679;&#20013;&#38388;&#28608;&#27963;&#20540;&#23601;&#26080;&#38656;&#32531;&#23384;&#65292;&#21487;&#20197;&#37325;&#26032;&#35745;&#31639;&#12290;&#28982;&#32780;&#65292;&#23558;PLM&#20462;&#25913;&#20026;&#23427;&#30340;&#21487;&#36870;&#21464;&#20307;&#24182;&#36827;&#34892;PEFT&#24182;&#19981;&#26159;&#19968;&#20214;&#23481;&#26131;&#30340;&#20107;&#65292;&#22240;&#20026;&#21487;&#36870;&#27169;&#22411;&#20855;&#26377;&#19982;&#24403;&#21069;&#21457;&#24067;&#30340;PLM&#19981;&#21516;&#30340;&#20307;&#31995;&#32467;&#26500;&#12290;&#26412;&#25991;&#39318;&#20808;&#35843;&#26597;&#29616;&#26377;PEFT&#26041;&#27861;&#25104;&#21151;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#35748;&#35782;&#21040;&#22312;&#21021;&#22987;&#21270;PEFT&#26102;&#20445;&#30041;PLM&#30340;&#36215;&#28857;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameter-efficient fine-tuning (PEFT) of pre-trained language models (PLMs) has emerged as a highly successful approach, with training only a small number of parameters without sacrificing performance and becoming the de-facto learning paradigm with the increasing size of PLMs. However, existing PEFT methods are not memory-efficient, because they still require caching most of the intermediate activations for the gradient calculation, akin to fine-tuning. One effective way to reduce the activation memory is to apply a reversible model, so the intermediate activations are not necessary to be cached and can be recomputed. Nevertheless, modifying a PLM to its reversible variant with PEFT is not straightforward, since the reversible model has a distinct architecture from the currently released PLMs. In this paper, we first investigate what is a key factor for the success of existing PEFT methods, and realize that it's essential to preserve the PLM's starting point when initializing a PEFT 
&lt;/p&gt;</description></item><item><title>MindBigData 2023 MNIST-8B&#26159;&#21490;&#19978;&#26368;&#22823;&#30340;&#24320;&#25918;&#33041;&#20449;&#21495;&#25968;&#25454;&#38598;&#65292;&#37319;&#29992;&#33258;&#23450;&#20041;&#35774;&#22791;&#22797;&#21046;&#20102;MNIST&#25968;&#25454;&#38598;&#20013;&#30340;&#20840;&#37096;70,000&#20010;&#25968;&#23383;&#65292;&#25429;&#25417;&#20102;&#21463;&#35797;&#32773;&#35266;&#30475;&#20687;&#32032;&#24182;&#21548;&#21462;&#25968;&#23383;&#26631;&#31614;&#26102;&#30340;&#22823;&#33041;&#20449;&#21495;&#12290;</title><link>http://arxiv.org/abs/2306.00455</link><description>&lt;p&gt;
MindBigData 2023 MNIST-8B&#65306;&#21490;&#19978;&#26368;&#22823;&#30340;&#33041;&#20449;&#21495;&#24320;&#25918;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
MindBigData 2023 MNIST-8B The 8 billion datapoints Multimodal Dataset of Brain Signals. (arXiv:2306.00455v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00455
&lt;/p&gt;
&lt;p&gt;
MindBigData 2023 MNIST-8B&#26159;&#21490;&#19978;&#26368;&#22823;&#30340;&#24320;&#25918;&#33041;&#20449;&#21495;&#25968;&#25454;&#38598;&#65292;&#37319;&#29992;&#33258;&#23450;&#20041;&#35774;&#22791;&#22797;&#21046;&#20102;MNIST&#25968;&#25454;&#38598;&#20013;&#30340;&#20840;&#37096;70,000&#20010;&#25968;&#23383;&#65292;&#25429;&#25417;&#20102;&#21463;&#35797;&#32773;&#35266;&#30475;&#20687;&#32032;&#24182;&#21548;&#21462;&#25968;&#23383;&#26631;&#31614;&#26102;&#30340;&#22823;&#33041;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
MindBigData 2023 MNIST-8B&#26159;&#36804;&#20170;&#20026;&#27490;&#65288;2023&#24180;6&#26376;1&#26085;&#65289;&#65292;&#22522;&#20110;&#21333;&#20010;&#21463;&#35797;&#32773;&#30340;EEG&#20449;&#21495;&#21019;&#24314;&#30340;&#26368;&#22823;&#30340;&#22823;&#33041;&#20449;&#21495;&#24320;&#25918;&#25968;&#25454;&#38598;&#65292;&#37319;&#29992;&#33258;&#23450;&#20041;&#30340;128&#36890;&#36947;&#35774;&#22791;&#65292;&#22797;&#21046;&#20102;Yaan LeCun&#31561;&#20154;&#30340;MNIST&#25968;&#25454;&#38598;&#20013;&#30340;&#20840;&#37096;70,000&#20010;&#25968;&#23383;&#12290;&#22312;&#21463;&#35797;&#32773;&#35266;&#30475;&#23631;&#24149;&#19978;&#30340;&#21407;&#22987;&#25968;&#23383;&#20687;&#32032;&#24182;&#21516;&#26102;&#21548;&#21462;&#30495;&#23454;&#26631;&#31614;&#30340;0&#33267;9&#25968;&#23383;&#26102;&#65292;&#25429;&#25417;&#20102;&#22823;&#33041;&#20449;&#21495;&#12290;&#35814;&#32454;&#25551;&#36848;&#20102;&#25968;&#25454;&#12289;&#25910;&#38598;&#36807;&#31243;&#12289;&#30828;&#20214;&#21644;&#36719;&#20214;&#65292;&#32972;&#26223;&#39069;&#22806;&#20449;&#24687;&#21644;&#20854;&#20182;&#30456;&#20851;&#25968;&#25454;&#38598;&#21487;&#20197;&#22312;&#25105;&#20204;&#20808;&#21069;&#30340;&#35770;&#25991;MindBigData 2022&#65306;&#22823;&#22411;&#33041;&#20449;&#21495;&#25968;&#25454;&#38598;&#20013;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
MindBigData 2023 MNIST-8B is the largest, to date (June 1st 2023), brain signals open dataset created for Machine Learning, based on EEG signals from a single subject captured using a custom 128 channels device, replicating the full 70,000 digits from Yaan LeCun et all MNIST dataset. The brain signals were captured while the subject was watching the pixels of the original digits one by one on a screen and listening at the same time to the spoken number 0 to 9 from the real label. The data, collection procedures, hardware and software created are described in detail, background extra information and other related datasets can be found at our previous paper MindBigData 2022: A Large Dataset of Brain Signals.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#35821;&#38899;&#33258;&#30417;&#30563;&#34920;&#31034;&#30340;&#22522;&#20934;&#27979;&#35797;&#32467;&#26524;&#23545;&#35299;&#30721;&#22120;&#26550;&#26500;&#21464;&#21270;&#24456;&#25935;&#24863;&#65292;&#32780;&#20351;&#29992;&#26377;&#38480;&#35299;&#30721;&#22120;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#21487;&#33021;&#20250;&#23548;&#33268;&#24320;&#21457;&#30340;&#27169;&#22411;&#36807;&#22823;&#12290;</title><link>http://arxiv.org/abs/2306.00452</link><description>&lt;p&gt;
&#35821;&#38899;&#33258;&#30417;&#30563;&#34920;&#31034;&#22522;&#20934;&#27979;&#35797;&#65306;&#25105;&#20204;&#20570;&#24471;&#23545;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Speech Self-Supervised Representation Benchmarking: Are We Doing it Right?. (arXiv:2306.00452v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00452
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#35821;&#38899;&#33258;&#30417;&#30563;&#34920;&#31034;&#30340;&#22522;&#20934;&#27979;&#35797;&#32467;&#26524;&#23545;&#35299;&#30721;&#22120;&#26550;&#26500;&#21464;&#21270;&#24456;&#25935;&#24863;&#65292;&#32780;&#20351;&#29992;&#26377;&#38480;&#35299;&#30721;&#22120;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#21487;&#33021;&#20250;&#23548;&#33268;&#24320;&#21457;&#30340;&#27169;&#22411;&#36807;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#20351;&#24471;&#21033;&#29992;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;&#35821;&#38899;&#20449;&#21495;&#25968;&#25454;&#38598;&#65292;&#22312;&#21482;&#26377;&#23569;&#37327;&#27880;&#37322;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#35821;&#38899;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#22823;&#37327;&#25552;&#20986;&#30340;&#26041;&#27861;&#20419;&#36827;&#20102;&#22522;&#20934;&#27979;&#35797;&#30340;&#38656;&#27714;&#21644;&#23835;&#36215;&#65292;&#35813;&#27979;&#35797;&#35780;&#20272;&#23427;&#20204;&#22312;&#19968;&#32452;&#25506;&#32034;&#35821;&#38899;&#20449;&#21495;&#30340;&#21508;&#20010;&#26041;&#38754;&#30340;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#28041;&#21450;&#30340;&#20219;&#21153;&#25968;&#37327;&#27491;&#22312;&#22686;&#38271;&#65292;&#20294;&#22823;&#22810;&#25968;&#20219;&#21153;&#37117;&#20381;&#36182;&#20110;&#21333;&#20010;&#35299;&#30721;&#22120;&#65292;&#23558;&#20923;&#32467;&#30340;SSL&#34920;&#31034;&#26144;&#23556;&#21040;&#19979;&#28216;&#26631;&#31614;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36825;&#31181;&#22522;&#20934;&#27979;&#35797;&#32467;&#26524;&#23545;&#35299;&#30721;&#22120;&#26550;&#26500;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25913;&#21464;&#19979;&#28216;&#35299;&#30721;&#22120;&#30340;&#32467;&#26500;&#20250;&#23548;&#33268;&#22823;&#22810;&#25968;&#20219;&#21153;&#25490;&#34892;&#27036;&#30340;&#26174;&#30528;&#21464;&#21270;&#12290;&#20196;&#20154;&#25285;&#24551;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#26377;&#38480;&#30340;&#35299;&#30721;&#22120;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#21487;&#33021;&#20250;&#23548;&#33268;&#24320;&#21457;&#30340;SSL&#27169;&#22411;&#30340;&#22823;&#23567;&#19981;&#24517;&#35201;&#22320;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) has recently allowed leveraging large datasets of unlabeled speech signals to reach impressive performance on speech tasks using only small amounts of annotated data. The high number of proposed approaches fostered the need and rise of extended benchmarks that evaluate their performance on a set of downstream tasks exploring various aspects of the speech signal. However, and while the number of considered tasks has been growing, most rely upon a single decoding architecture that maps the frozen SSL representations to the downstream labels. This work investigates the robustness of such benchmarking results to changes in the decoder architecture. Interestingly, it appears that varying the architecture of the downstream decoder leads to significant variations in the leaderboards of most tasks. Concerningly, our study reveals that benchmarking using limited decoders may cause a counterproductive increase in the sizes of the developed SSL models.
&lt;/p&gt;</description></item><item><title>&#36830;&#32493;&#23398;&#20064;&#20013;&#23384;&#22312;&#19968;&#31181;&#29305;&#27530;&#24418;&#24335;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#8212;&#8212;&#36234;&#30028;&#36951;&#24536;&#65292;&#24403;&#32473;&#23450;&#31867;&#21035;&#24341;&#20837;&#31867;&#20869;&#20998;&#24067;&#36716;&#31227;&#26102;&#65292;&#23427;&#20250;&#26174;&#30528;&#21066;&#24369;&#35813;&#31867;&#21035;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#30340;&#35782;&#21035;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.00427</link><description>&lt;p&gt;
&#38024;&#23545;&#31867;&#20869;&#20998;&#24067;&#36716;&#31227;&#30340;&#36807;&#24230;&#36951;&#24536;&#65306;&#36830;&#32493;&#23398;&#20064;&#30340;&#33030;&#24369;&#24615;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution forgetting: vulnerability of continual learning to intra-class distribution shift. (arXiv:2306.00427v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00427
&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#20013;&#23384;&#22312;&#19968;&#31181;&#29305;&#27530;&#24418;&#24335;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#8212;&#8212;&#36234;&#30028;&#36951;&#24536;&#65292;&#24403;&#32473;&#23450;&#31867;&#21035;&#24341;&#20837;&#31867;&#20869;&#20998;&#24067;&#36716;&#31227;&#26102;&#65292;&#23427;&#20250;&#26174;&#30528;&#21066;&#24369;&#35813;&#31867;&#21035;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#30340;&#35782;&#21035;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#26159;&#35753;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22312;&#24320;&#25918;&#29615;&#22659;&#20013;&#24037;&#20316;&#30340;&#37325;&#35201;&#25216;&#26415;&#12290;&#22312;&#32852;&#21512;&#23398;&#20064;&#20013;&#65292;&#20154;&#20204;&#24050;&#32463;&#30693;&#36947;&#30001;&#24847;&#22270;&#25915;&#20987;&#25110;&#29615;&#22659;&#25200;&#21160;&#24341;&#36215;&#30340;&#36234;&#30028;&#38382;&#39064;&#20005;&#37325;&#24433;&#21709;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#36830;&#32493;&#23398;&#20064;&#35774;&#32622;&#20013;&#30001;&#36234;&#30028;&#38382;&#39064;&#24341;&#36215;&#30340;&#19968;&#31181;&#29305;&#27530;&#24418;&#24335;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#36234;&#30028;&#36951;&#24536;&#65288;OODF&#65289;&#12290;&#22312;&#36830;&#32493;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#38024;&#23545;&#32473;&#23450;&#31867;&#21035;&#65292;&#24341;&#20837;&#31867;&#20869;&#20998;&#24067;&#36716;&#31227;&#26174;&#30528;&#21066;&#24369;&#20102;&#21518;&#32493;&#23398;&#20064;&#36807;&#31243;&#20013;&#35813;&#31867;&#21035;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#30340;&#35782;&#21035;&#20934;&#30830;&#29575;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#36825;&#31181;&#29616;&#35937;&#23545;&#20110;&#36830;&#32493;&#23398;&#20064;&#32780;&#35328;&#26159;&#29305;&#27530;&#30340;&#65292;&#22240;&#20026;&#21516;&#26679;&#32423;&#21035;&#30340;&#20998;&#24067;&#36716;&#31227;&#21482;&#26377;&#24494;&#19981;&#36275;&#36947;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning (CL) is an important technique to allow artificial neural networks to work in open environments. CL enables a system to learn new tasks without severe interference to its performance on old tasks, i.e., overcome the problems of catastrophic forgetting. In joint learning, it is well known that the out-of-distribution (OOD) problem caused by intentional attacks or environmental perturbations will severely impair the ability of networks to generalize. In this work, we reported a special form of catastrophic forgetting raised by the OOD problem in continual learning settings, and we named it out-of-distribution forgetting (OODF). In continual image classification tasks, we found that for a given category, introducing an intra-class distribution shift significantly impaired the recognition accuracy of CL methods for that category during subsequent learning. Interestingly, this phenomenon is special for CL as the same level of distribution shift had only negligible effects
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24352;&#37327;&#24130;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#35299;&#20219;&#24847;&#38454;&#24352;&#37327;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#23427;&#21487;&#20197;&#26356;&#39640;&#25928;&#22320;&#22788;&#29702;&#39640;&#38454;&#24352;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.00406</link><description>&lt;p&gt;
&#39640;&#38454;&#40065;&#26834;&#24352;&#37327;&#24130;&#26041;&#27861;&#30340;&#24555;&#36895;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
Faster Robust Tensor Power Method for Arbitrary Order. (arXiv:2306.00406v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24352;&#37327;&#24130;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#35299;&#20219;&#24847;&#38454;&#24352;&#37327;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#23427;&#21487;&#20197;&#26356;&#39640;&#25928;&#22320;&#22788;&#29702;&#39640;&#38454;&#24352;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24352;&#37327;&#20998;&#35299;&#26159;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#30340;&#19968;&#31181;&#22522;&#26412;&#26041;&#27861;&#65292;&#20854;&#20013;&#24352;&#37327;&#24130;&#26041;&#27861; (TPM) &#26159;&#24352;&#37327;&#20998;&#35299;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#25216;&#26415;&#20043;&#19968;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24352;&#37327;&#24130;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#35299;&#20219;&#24847;&#38454;&#24352;&#37327;&#65292;&#23427;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#23616;&#38480;&#20110;&#20302;&#38454;&#24352;&#37327;&#65288;&#23567;&#20110;3&#65289;&#25110;&#38656;&#35201;&#23545;&#24213;&#23618;&#25968;&#25454;&#32467;&#26500;&#20570;&#20986;&#24378;&#28872;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#24212;&#29992;&#20102;&#33609;&#22270;&#26041;&#27861;&#65292;&#24182;&#33021;&#22815;&#22312;&#24130;p&#21644;&#32500;&#24230;n&#30340;&#24352;&#37327;&#19978;&#23454;&#29616;&#36816;&#34892;&#26102;&#38388;&#20026;$\widetilde{O}(n^{p-1})$&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#20998;&#26512;&#65292;&#36866;&#29992;&#20110;&#20219;&#20309;p&#38454;&#24352;&#37327;&#65292;&#36825;&#22312;&#20197;&#21069;&#30340;&#24037;&#20316;&#20013;&#20174;&#26410;&#32473;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tensor decomposition is a fundamental method used in various areas to deal with high-dimensional data. \emph{Tensor power method} (TPM) is one of the widely-used techniques in the decomposition of tensors. This paper presents a novel tensor power method for decomposing arbitrary order tensors, which overcomes limitations of existing approaches that are often restricted to lower-order (less than $3$) tensors or require strong assumptions about the underlying data structure. We apply sketching method, and we are able to achieve the running time of $\widetilde{O}(n^{p-1})$, on the power $p$ and dimension $n$ tensor. We provide a detailed analysis for any $p$-th order tensor, which is never given in previous works.
&lt;/p&gt;</description></item><item><title>&#8220;&#38181;&#27880;&#24847;&#21147;&#8221;&#26159;&#28857;&#31215;&#27880;&#24847;&#21147;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#23427;&#36890;&#36807;&#22522;&#20110;&#21452;&#26354;&#38181;&#30340;&#23618;&#27425;&#32467;&#26500;&#32852;&#31995;&#25968;&#25454;&#28857;&#65292;&#26126;&#30830;&#24314;&#27169;&#20102;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#22797;&#26434;&#32467;&#26500;&#23646;&#24615;&#65292;&#25552;&#39640;&#20102;&#20219;&#21153;&#32423;&#24615;&#33021;&#24182;&#23454;&#29616;&#20102;&#20248;&#20110;&#28857;&#31215;&#27880;&#24847;&#21147;&#30340;&#25928;&#26524;&#65292;&#32780;&#19988;&#21442;&#25968;&#37327;&#26356;&#23569;&#12290;</title><link>http://arxiv.org/abs/2306.00392</link><description>&lt;p&gt;
&#38181;&#26426;&#21046;: &#23618;&#27425;&#24863;&#30693;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
Coneheads: Hierarchy Aware Attention. (arXiv:2306.00392v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00392
&lt;/p&gt;
&lt;p&gt;
&#8220;&#38181;&#27880;&#24847;&#21147;&#8221;&#26159;&#28857;&#31215;&#27880;&#24847;&#21147;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#23427;&#36890;&#36807;&#22522;&#20110;&#21452;&#26354;&#38181;&#30340;&#23618;&#27425;&#32467;&#26500;&#32852;&#31995;&#25968;&#25454;&#28857;&#65292;&#26126;&#30830;&#24314;&#27169;&#20102;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#22797;&#26434;&#32467;&#26500;&#23646;&#24615;&#65292;&#25552;&#39640;&#20102;&#20219;&#21153;&#32423;&#24615;&#33021;&#24182;&#23454;&#29616;&#20102;&#20248;&#20110;&#28857;&#31215;&#27880;&#24847;&#21147;&#30340;&#25928;&#26524;&#65292;&#32780;&#19988;&#21442;&#25968;&#37327;&#26356;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#22914;transformers&#65292;&#22312;&#35768;&#22810;&#39046;&#22495;&#24050;&#32463;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#32593;&#32476;&#20005;&#37325;&#20381;&#36182;&#20110;&#28857;&#31215;&#27880;&#24847;&#21147;&#36816;&#31639;&#31526;&#65292;&#23427;&#36890;&#36807;&#21462;&#20004;&#20010;&#28857;&#30340;&#20869;&#31215;&#26469;&#35745;&#31639;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#28982;&#32780;&#65292;&#20869;&#31215;&#19981;&#33021;&#26126;&#30830;&#22320;&#23545;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#22797;&#26434;&#32467;&#26500;&#23646;&#24615;&#65288;&#22914;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#23618;&#27425;&#32467;&#26500;&#65289;&#36827;&#34892;&#24314;&#27169;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#38181;&#27880;&#24847;&#21147;&#65292;&#19968;&#31181;&#22522;&#20110;&#21452;&#26354;&#38181;&#30340;&#28857;&#31215;&#27880;&#24847;&#21147;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#38181;&#27880;&#24847;&#21147;&#36890;&#36807;&#21452;&#26354;&#38181;&#23450;&#20041;&#30340;&#23618;&#27425;&#32467;&#26500;&#23558;&#20004;&#20010;&#28857;&#32852;&#31995;&#36215;&#26469;&#65292;&#30452;&#35266;&#22320;&#34913;&#37327;&#20102;&#20004;&#20010;&#28857;&#30340;&#20998;&#27495;&#65292;&#24182;&#32473;&#20986;&#20102;&#19968;&#20010;&#23618;&#27425;&#24863;&#30693;&#30340;&#30456;&#20284;&#24230;&#20998;&#25968;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#27169;&#22411;&#21644;&#20219;&#21153;&#19978;&#27979;&#35797;&#20102;&#38181;&#27880;&#24847;&#21147;&#65292;&#24182;&#34920;&#26126;&#23427;&#22312;&#20219;&#21153;&#32423;&#24615;&#33021;&#19978;&#20248;&#20110;&#28857;&#31215;&#27880;&#24847;&#21147;&#21644;&#20854;&#20182;&#22522;&#32447;&#31639;&#27861;&#65292;&#24182;&#19988;&#33021;&#22815;&#20197;&#26174;&#33879;&#36739;&#23569;&#30340;&#21442;&#25968;&#21305;&#37197;&#28857;&#31215;&#27880;&#24847;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention networks such as transformers have achieved state-of-the-art performance in many domains. These networks rely heavily on the dot product attention operator, which computes the similarity between two points by taking their inner product. However, the inner product does not explicitly model the complex structural properties of real world datasets, such as hierarchies between data points. To remedy this, we introduce cone attention, a drop-in replacement for dot product attention based on hyperbolic entailment cones. Cone attention associates two points by the depth of their lowest common ancestor in a hierarchy defined by hyperbolic cones, which intuitively measures the divergence of two points and gives a hierarchy aware similarity score. We test cone attention on a wide variety of models and tasks and show that it improves task-level performance over dot product attention and other baselines, and is able to match dot-product attention with significantly fewer parameters. Our 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24352;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26694;&#26550;GMRL&#65292;&#23427;&#21487;&#20197;&#21333;&#29420;&#27169;&#25311;&#26102;&#38388;&#12289;&#20301;&#32622;&#21644;&#28304;&#21464;&#37327;&#20013;&#25152;&#26263;&#31034;&#30340;&#27599;&#20010;&#24322;&#26500;&#24615;&#32452;&#20214;&#65292;&#30456;&#27604;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#26041;&#27861;&#65292;&#26412;&#25991;&#30340;&#26041;&#27861;&#26174;&#31034;&#20986;&#20102;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.00390</link><description>&lt;p&gt;
&#23398;&#20064;&#39640;&#26031;&#28151;&#21512;&#34920;&#31034;&#29992;&#20110;&#24352;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Learning Gaussian Mixture Representations for Tensor Time Series Forecasting. (arXiv:2306.00390v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00390
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24352;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26694;&#26550;GMRL&#65292;&#23427;&#21487;&#20197;&#21333;&#29420;&#27169;&#25311;&#26102;&#38388;&#12289;&#20301;&#32622;&#21644;&#28304;&#21464;&#37327;&#20013;&#25152;&#26263;&#31034;&#30340;&#27599;&#20010;&#24322;&#26500;&#24615;&#32452;&#20214;&#65292;&#30456;&#27604;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#26041;&#27861;&#65292;&#26412;&#25991;&#30340;&#26041;&#27861;&#26174;&#31034;&#20986;&#20102;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24352;&#37327;&#26102;&#38388;&#24207;&#21015;&#65288;TTS&#65289;&#25968;&#25454;&#26159;&#39640;&#32500;&#31354;&#38388;&#20013;&#19968;&#32500;&#26102;&#38388;&#24207;&#21015;&#30340;&#19968;&#33324;&#21270;&#65292;&#26159;&#29616;&#23454;&#22330;&#26223;&#20013;&#19975;&#33021;&#30340;&#23384;&#22312;&#65292;&#29305;&#21035;&#26159;&#22312;&#28041;&#21450;&#22810;&#28304;&#26102;&#31354;&#25968;&#25454;&#30340;&#30417;&#27979;&#31995;&#32479;&#20013;&#65288;&#20363;&#22914;&#20132;&#36890;&#38656;&#27714;&#21644;&#31354;&#27668;&#27745;&#26579;&#29289;&#65289;&#12290;&#19982;&#24314;&#27169;&#26102;&#38388;&#24207;&#21015;&#25110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#30456;&#27604;&#65292;&#22312;&#26368;&#36817;&#20960;&#24180;&#24050;&#32463;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#24182;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#30340;&#24773;&#20917;&#19979;&#65292;&#24352;&#37327;&#26102;&#38388;&#24207;&#21015;&#20184;&#20986;&#30340;&#21162;&#21147;&#36739;&#23569;&#12290;&#30001;&#20110;&#20854;&#39640;&#32500;&#21644;&#22797;&#26434;&#30340;&#20869;&#37096;&#32467;&#26500;&#65292;&#27491;&#30830;&#22788;&#29702;&#24352;&#37327;&#26102;&#38388;&#24207;&#21015;&#26159;&#19968;&#20010;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;TTS&#39044;&#27979;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#26088;&#22312;&#21333;&#29420;&#27169;&#25311;&#26102;&#38388;&#12289;&#20301;&#32622;&#21644;&#28304;&#21464;&#37327;&#20013;&#25152;&#26263;&#31034;&#30340;&#27599;&#20010;&#24322;&#26500;&#24615;&#32452;&#20214;&#12290;&#25105;&#20204;&#23558;&#27492;&#26694;&#26550;&#21629;&#21517;&#20026;GMRL&#65292;&#21363;&#39640;&#26031;&#28151;&#21512;&#34920;&#31034;&#23398;&#20064;&#12290;&#22312;&#20004;&#20010;&#23454;&#38469;TTS&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tensor time series (TTS) data, a generalization of one-dimensional time series on a high-dimensional space, is ubiquitous in real-world scenarios, especially in monitoring systems involving multi-source spatio-temporal data (e.g., transportation demands and air pollutants). Compared to modeling time series or multivariate time series, which has received much attention and achieved tremendous progress in recent years, tensor time series has been paid less effort. Properly coping with the tensor time series is a much more challenging task, due to its high-dimensional and complex inner structure. In this paper, we develop a novel TTS forecasting framework, which seeks to individually model each heterogeneity component implied in the time, the location, and the source variables. We name this framework as GMRL, short for Gaussian Mixture Representation Learning. Experiment results on two real-world TTS datasets verify the superiority of our approach compared with the state-of-the-art baseli
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#22522;&#20110;&#26657;&#20934;&#20542;&#21521;&#20998;&#25968;&#25216;&#26415;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#26041;&#27861;&#65292;&#21487;&#36890;&#36807;&#31616;&#21333;&#30340;&#37325;&#26032;&#26657;&#20934;&#25216;&#26415;&#23454;&#29616;&#20542;&#21521;&#20998;&#25968;&#27169;&#22411;&#30340;&#27010;&#29575;&#36755;&#20986;&#30340;&#26657;&#20934;&#65292;&#20855;&#26377;&#36739;&#20026;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2306.00382</link><description>&lt;p&gt;
&#22522;&#20110;&#26657;&#20934;&#20542;&#21521;&#20998;&#25968;&#36827;&#34892;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Calibrated Propensity Scores for Causal Effect Estimation. (arXiv:2306.00382v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#22522;&#20110;&#26657;&#20934;&#20542;&#21521;&#20998;&#25968;&#25216;&#26415;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#26041;&#27861;&#65292;&#21487;&#36890;&#36807;&#31616;&#21333;&#30340;&#37325;&#26032;&#26657;&#20934;&#25216;&#26415;&#23454;&#29616;&#20542;&#21521;&#20998;&#25968;&#27169;&#22411;&#30340;&#27010;&#29575;&#36755;&#20986;&#30340;&#26657;&#20934;&#65292;&#20855;&#26377;&#36739;&#20026;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20542;&#21521;&#20998;&#25968;&#36890;&#24120;&#29992;&#20110;&#22312;&#20272;&#35745;&#27835;&#30103;&#25928;&#26524;&#26102;&#24179;&#34913;&#35266;&#27979;&#21040;&#30340;&#21327;&#21464;&#37327;&#12290;&#24403;&#20542;&#21521;&#20998;&#25968;&#27169;&#22411;&#26080;&#27861;&#23398;&#20064;&#30495;&#23454;&#30340;&#27835;&#30103;&#20998;&#37197;&#26426;&#21046;&#26102;&#65292;&#36890;&#36807;&#20542;&#21521;&#20998;&#25968;&#21152;&#26435;&#33719;&#24471;&#30340;&#20272;&#35745;&#32467;&#26524;&#21487;&#33021;&#23384;&#22312;&#20559;&#24046;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#23398;&#20064;&#21040;&#30340;&#20542;&#21521;&#20998;&#25968;&#27169;&#22411;&#30340;&#27010;&#29575;&#36755;&#20986;&#24212;&#36827;&#34892;&#26657;&#20934;&#65292;&#21363;90%&#30340;&#39044;&#27979;&#27835;&#30103;&#27010;&#29575;&#24212;&#23545;&#24212;90%&#30340;&#20010;&#20307;&#34987;&#20998;&#37197;&#21040;&#27835;&#30103;&#32452;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31616;&#21333;&#30340;&#37325;&#26032;&#26657;&#20934;&#25216;&#26415;&#26469;&#30830;&#20445;&#36825;&#19968;&#23646;&#24615;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#26657;&#20934;&#20542;&#21521;&#20998;&#25968;&#27169;&#22411;&#30340;&#29702;&#35770;&#29305;&#24615;&#21450;&#20854;&#22312;&#26080;&#20559;&#27835;&#30103;&#25928;&#24212;&#20272;&#35745;&#20013;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#22810;&#39033;&#20219;&#21153;&#20013;&#20351;&#29992;&#26657;&#20934;&#20542;&#21521;&#20998;&#25968;&#36827;&#34892;&#25913;&#36827;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#65292;&#20854;&#20013;&#21253;&#25324;&#39640;&#32500;&#22522;&#22240;&#32452;&#20851;&#32852;&#30740;&#31350;&#65292;&#22312;&#24212;&#29992;&#20110;&#26356;&#31616;&#21333;&#30340;&#20542;&#21521;&#20998;&#25968;&#27169;&#22411;&#26102;&#36824;&#21487;&#20197;&#38477;&#20302;&#35745;&#31639;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Propensity scores are commonly used to balance observed covariates while estimating treatment effects. Estimates obtained through propensity score weighing can be biased when the propensity score model cannot learn the true treatment assignment mechanism. We argue that the probabilistic output of a learned propensity score model should be calibrated, i.e. a predictive treatment probability of 90% should correspond to 90% of individuals being assigned the treatment group. We propose simple recalibration techniques to ensure this property. We investigate the theoretical properties of a calibrated propensity score model and its role in unbiased treatment effect estimation. We demonstrate improved causal effect estimation with calibrated propensity scores in several tasks including high-dimensional genome-wide association studies, where we also show reduced computational requirements when calibration is applied to simpler propensity score models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#20195;&#30721;&#34917;&#20840;&#20219;&#21153;&#23637;&#24320;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#32771;&#34385;&#23616;&#37096;&#21644;&#38750;&#23616;&#37096;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;Python&#21253;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#31243;&#24207;&#20998;&#26512;&#22120;&#25552;&#21462;&#20102;&#38750;&#23616;&#37096;&#20449;&#24687;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#20195;&#30721;&#34917;&#20840;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.00381</link><description>&lt;p&gt;
&#26356;&#22909;&#30340;&#19978;&#19979;&#25991;&#20351;&#24471;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#26356;&#20986;&#33394;&#65306;&#20197;&#20989;&#25968;&#35843;&#29992;&#23454;&#21442;&#33258;&#21160;&#34917;&#20840;&#20026;&#20363;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Better Context Makes Better Code Language Models: A Case Study on Function Call Argument Completion. (arXiv:2306.00381v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00381
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#20195;&#30721;&#34917;&#20840;&#20219;&#21153;&#23637;&#24320;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#32771;&#34385;&#23616;&#37096;&#21644;&#38750;&#23616;&#37096;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;Python&#21253;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#31243;&#24207;&#20998;&#26512;&#22120;&#25552;&#21462;&#20102;&#38750;&#23616;&#37096;&#20449;&#24687;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#20195;&#30721;&#34917;&#20840;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#22312;&#31243;&#24207;&#21512;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#24456;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#24120;&#35265;&#26041;&#27861;&#21482;&#32771;&#34385;&#25991;&#20214;&#20869;&#30340;&#23616;&#37096;&#19978;&#19979;&#25991;&#65292;&#22240;&#27492;&#24573;&#35270;&#20102;&#20195;&#30721;&#24211;&#20854;&#20182;&#37096;&#20998;&#21450;&#20854;&#22806;&#37096;&#20381;&#36182;&#25152;&#26045;&#21152;&#30340;&#20449;&#24687;&#21644;&#38480;&#21046;&#12290;&#29616;&#26377;&#30340;&#20195;&#30721;&#34917;&#20840;&#22522;&#20934;&#27979;&#35797;&#20063;&#32570;&#20047;&#36825;&#31181;&#19978;&#19979;&#25991;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#31574;&#21010;&#20102;&#19968;&#20010;&#21253;&#21547;&#23436;&#25972;&#39033;&#30446;&#21450;&#20854;&#20381;&#36182;&#30340;&#12289;&#35768;&#21487;&#23485;&#26494;&#30340;Python&#21253;&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20379;&#24037;&#20855;&#26469;&#36741;&#21161;&#31243;&#24207;&#20998;&#26512;&#22120;&#25552;&#21462;&#38750;&#26412;&#22320;&#20449;&#24687;&#12290;&#25105;&#20204;&#25509;&#30528;&#20851;&#27880;&#20989;&#25968;&#35843;&#29992;&#23454;&#21442;&#33258;&#21160;&#34917;&#20840;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#38656;&#35201;&#39044;&#27979;&#20989;&#25968;&#35843;&#29992;&#30340;&#23454;&#21442;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#29616;&#26377;&#30340;&#20195;&#30721;&#34917;&#20840;&#27169;&#22411;&#19981;&#33021;&#24456;&#22909;&#22320;&#35299;&#20915;&#25105;&#20204;&#30340;&#33258;&#21160;&#34917;&#20840;&#20219;&#21153;&#12290;&#20026;&#26356;&#22909;&#22320;&#35299;&#20915;&#27492;&#20219;&#21153;&#65292;&#25105;&#20204;&#21521;&#31243;&#24207;&#20998;&#26512;&#22120;&#26597;&#35810;&#19982;&#32473;&#23450;&#20989;&#25968;&#35843;&#29992;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#24182;&#32771;&#34385;&#22312;&#25512;&#29702;&#21644;&#35757;&#32451;&#26399;&#38388;&#23558;&#20998;&#26512;&#22120;&#32467;&#26524;&#25552;&#20379;&#32473;&#19981;&#21516;&#30340;&#20195;&#30721;&#34917;&#20840;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#38750;&#23616;&#37096;&#19978;&#19979;&#25991;&#21487;&#20197;&#24110;&#21161;&#25552;&#39640;&#20195;&#30721;&#34917;&#20840;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained code language models have enabled great progress towards program synthesis. However, common approaches only consider in-file local context and thus miss information and constraints imposed by other parts of the codebase and its external dependencies. Existing code completion benchmarks also lack such context. To resolve these restrictions we curate a new dataset of permissively licensed Python packages that includes full projects and their dependencies and provide tools to extract non-local information with the help of program analyzers. We then focus on the task of function call argument completion which requires predicting the arguments to function calls. We show that existing code completion models do not yield good results on our completion task. To better solve this task, we query a program analyzer for information relevant to a given function call, and consider ways to provide the analyzer results to different code completion models during inference and training. Our e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#29983;&#25104;&#24335;&#27169;&#22411;&#26469;&#35299;&#20915;&#30005;&#21830;&#21830;&#21697;&#23646;&#24615;&#35782;&#21035;&#20013;&#30340;&#26631;&#27880;&#38382;&#39064;&#65292;&#36890;&#36807;&#22312;&#25991;&#26412;&#19982;&#35270;&#35273;&#29305;&#24449;&#20013;&#29983;&#25104;&#23646;&#24615;&#20540;&#65292;&#31995;&#32479;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#25552;&#21319;&#20102;7.3&#65285;&#12290;</title><link>http://arxiv.org/abs/2306.00379</link><description>&lt;p&gt;
&#38024;&#23545;&#30005;&#23376;&#21830;&#21153;&#23646;&#24615;&#30340;&#22823;&#35268;&#27169;&#29983;&#25104;&#24335;&#22810;&#27169;&#24577;&#23646;&#24615;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Large Scale Generative Multimodal Attribute Extraction for E-commerce Attributes. (arXiv:2306.00379v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00379
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#29983;&#25104;&#24335;&#27169;&#22411;&#26469;&#35299;&#20915;&#30005;&#21830;&#21830;&#21697;&#23646;&#24615;&#35782;&#21035;&#20013;&#30340;&#26631;&#27880;&#38382;&#39064;&#65292;&#36890;&#36807;&#22312;&#25991;&#26412;&#19982;&#35270;&#35273;&#29305;&#24449;&#20013;&#29983;&#25104;&#23646;&#24615;&#20540;&#65292;&#31995;&#32479;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#25552;&#21319;&#20102;7.3&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#21830;&#21153;&#32593;&#31449;&#65288;&#22914;&#20122;&#39532;&#36874;&#65289;&#22312;&#20854;&#20135;&#21697;&#39029;&#38754;&#19978;&#26377;&#22823;&#37327;&#30340;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#20449;&#24687;&#65288;&#25991;&#26412;&#21644;&#22270;&#29255;&#65289;&#12290;&#38144;&#21806;&#32773;&#24448;&#24448;&#19981;&#23545;&#20854;&#20135;&#21697;&#30340;&#23646;&#24615;&#65288;&#22914;&#39068;&#33394;&#65292;&#23610;&#30721;&#31561;&#65289;&#36827;&#34892;&#26631;&#27880;&#25110;&#26631;&#27880;&#26377;&#35823;&#12290;&#20174;&#26082;&#21253;&#21547;&#25991;&#26412;&#21448;&#21253;&#21547;&#22270;&#29255;&#30340;&#30005;&#23376;&#21830;&#21153;&#20135;&#21697;&#39029;&#38754;&#20013;&#33258;&#21160;&#35782;&#21035;&#36825;&#20123;&#23646;&#24615;&#20540;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#23588;&#20854;&#26159;&#24403;&#30446;&#24405;&#20013;&#26410;&#26126;&#30830;&#25552;&#21450;&#23646;&#24615;&#20540;&#26102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23558;&#23646;&#24615;&#25552;&#21462;&#38382;&#39064;&#20316;&#20026;&#38382;&#31572;&#20219;&#21153;&#65292;&#20351;&#29992;MXT&#65288;Multimodal Adaptation Gate&#65292;Xception&#32593;&#32476;&#21644;T5&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#30340;&#19977;&#20010;&#20851;&#38190;&#32452;&#20214;&#65289;&#35299;&#20915;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#21253;&#25324;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#20135;&#21697;&#30340;&#25991;&#26412;&#21644;&#35270;&#35273;&#29305;&#24449;&#65288;&#20363;&#22914;&#22270;&#20687;&#65289;&#26469;&#8220;&#29983;&#25104;&#8221;&#32473;&#23450;&#20135;&#21697;&#30340;&#23646;&#24615;&#20540;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#19977;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#24179;&#22343;&#25552;&#39640;7.3&#65285;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#21333;&#20010;GPU&#19978;&#27599;&#31186;&#25552;&#21462;12K&#20010;&#20197;&#19978;&#30340;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
E-commerce websites (e.g. Amazon) have a plethora of structured and unstructured information (text and images) present on the product pages. Sellers often either don't label or mislabel values of the attributes (e.g. color, size etc.) for their products. Automatically identifying these attribute values from an eCommerce product page that contains both text and images is a challenging task, especially when the attribute value is not explicitly mentioned in the catalog. In this paper, we present a scalable solution for this problem where we pose attribute extraction problem as a question-answering task, which we solve using \textbf{MXT}, consisting of three key components: (i) \textbf{M}AG (Multimodal Adaptation Gate), (ii) \textbf{X}ception network, and (iii) \textbf{T}5 encoder-decoder. Our system consists of a generative model that \emph{generates} attribute-values for a given product by using both textual and visual characteristics (e.g. images) of the product. We show that our syste
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22270;&#24418;&#30340;&#20999;&#25442;&#21160;&#21147;&#23398;&#31995;&#32479;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#28041;&#21450;&#20132;&#20114;&#23545;&#35937;&#35774;&#32622;&#20013;&#33258;&#21160;&#21457;&#29616;&#27169;&#24335;&#24182;&#20174;&#26102;&#24207;&#25968;&#25454;&#20013;&#23398;&#20064;&#20999;&#25442;&#34892;&#20026;&#65292;&#36825;&#19968;&#26041;&#27861;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.00370</link><description>&lt;p&gt;
&#22270;&#20999;&#25442;&#21160;&#21147;&#23398;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Graph Switching Dynamical Systems. (arXiv:2306.00370v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00370
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22270;&#24418;&#30340;&#20999;&#25442;&#21160;&#21147;&#23398;&#31995;&#32479;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#28041;&#21450;&#20132;&#20114;&#23545;&#35937;&#35774;&#32622;&#20013;&#33258;&#21160;&#21457;&#29616;&#27169;&#24335;&#24182;&#20174;&#26102;&#24207;&#25968;&#25454;&#20013;&#23398;&#20064;&#20999;&#25442;&#34892;&#20026;&#65292;&#36825;&#19968;&#26041;&#27861;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#22797;&#26434;&#34892;&#20026;&#30340;&#21160;&#21147;&#23398;&#31995;&#32479;&#65288;&#20363;&#22914;&#20813;&#30123;&#31995;&#32479;&#32454;&#32990;&#19982;&#30149;&#21407;&#20307;&#30340;&#30456;&#20114;&#20316;&#29992;&#65289;&#36890;&#24120;&#36890;&#36807;&#23558;&#34892;&#20026;&#20998;&#20026;&#19981;&#21516;&#30340;&#27169;&#24335;&#25110;&#29366;&#24577;&#65292;&#24182;&#20174;&#20013;&#23398;&#20064;&#27169;&#24335;&#20043;&#38388;&#30340;&#20999;&#25442;&#34892;&#20026;&#26469;&#24314;&#27169;&#12290;&#20999;&#25442;&#21160;&#21147;&#23398;&#31995;&#32479;&#65288;SDS&#65289;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#33258;&#21160;&#21457;&#29616;&#36825;&#20123;&#27169;&#24335;&#24182;&#20174;&#26102;&#24207;&#25968;&#25454;&#20013;&#23398;&#20064;&#20999;&#25442;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#29420;&#31435;&#23545;&#35937;&#65292;&#20854;&#20013;&#19968;&#20010;&#23545;&#35937;&#30340;&#27169;&#24335;&#19982;&#20854;&#20182;&#23545;&#35937;&#30340;&#27169;&#24335;&#26080;&#20851;&#12290;&#26412;&#25991;&#21017;&#20391;&#37325;&#20110;&#26356;&#19968;&#33324;&#30340;&#30456;&#20114;&#20316;&#29992;&#23545;&#35937;&#35774;&#32622;&#65292;&#28041;&#21450;&#20999;&#25442;&#21160;&#21147;&#23398;&#31995;&#32479;&#30340;&#20132;&#20114;&#23545;&#35937;&#65292;&#20854;&#20013;&#27599;&#20010;&#23545;&#35937;&#30340;&#21160;&#24577;&#20063;&#21462;&#20915;&#20110;&#21478;&#19968;&#20010;&#26410;&#30693;&#19988;&#21160;&#24577;&#26356;&#25913;&#30340;&#23545;&#35937;&#21450;&#20854;&#27169;&#24335;&#30340;&#23376;&#38598;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22270;&#24418;&#30340;&#20999;&#25442;&#21160;&#21147;&#23398;&#31995;&#32479;&#26041;&#27861;&#65292;&#21517;&#20026;&#8220;&#22270;&#20999;&#25442;&#21160;&#21147;&#23398;&#31995;&#32479;&#8221;&#65288;GRASS&#65289;&#65292;&#20854;&#20013;&#25105;&#20204;&#20351;&#29992;&#21160;&#24577;&#22270;&#24418;&#26469;&#25551;&#36848;&#23545;&#35937;&#20043;&#38388;&#30340;&#20132;&#20114;&#21644;&#27169;&#24335;&#20999;&#25442;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;GRASS&#27169;&#22411;&#21487;&#35782;&#21035;&#24615;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#24182;&#24320;&#21457;&#20102;&#39640;&#25928;&#30340;&#25512;&#29702;&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamical systems with complex behaviours, e.g. immune system cells interacting with a pathogen, are commonly modelled by splitting the behaviour into different regimes, or modes, each with simpler dynamics, and then learning the switching behaviour from one mode to another. Switching Dynamical Systems (SDS) are a powerful tool that automatically discovers these modes and mode-switching behaviour from time series data. While effective, these methods focus on independent objects, where the modes of one object are independent of the modes of the other objects. In this paper, we focus on the more general interacting object setting for switching dynamical systems, where the per-object dynamics also depends on an unknown and dynamically changing subset of other objects and their modes. To this end, we propose a novel graph-based approach for switching dynamical systems, GRAph Switching dynamical Systems (GRASS), in which we use a dynamic graph to characterize interactions between objects an
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24314;&#31435;&#20102;&#19968;&#33268;&#24615;&#27169;&#22411;&#65292;&#19968;&#33268;&#24615;&#25193;&#25955;&#27169;&#22411;&#21644;Fokker-Planck&#27491;&#21017;&#21270;&#20043;&#38388;&#30340;&#29702;&#35770;&#32852;&#31995;&#65292;&#20026;&#19981;&#21516;&#30446;&#26631;&#22686;&#24378;&#25193;&#25955;&#27169;&#22411;&#25552;&#20379;&#20102;&#26356;&#20840;&#38754;&#21644;&#21253;&#23481;&#24615;&#30340;&#26694;&#26550;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.00367</link><description>&lt;p&gt;
&#20851;&#20110;&#19968;&#33268;&#24615;&#27169;&#22411;&#30340;&#31561;&#20215;&#24615;: &#19968;&#33268;&#24615;&#27169;&#22411;&#65292;&#19968;&#33268;&#24615;&#25193;&#25955;&#27169;&#22411;&#21644;Fokker-Planck&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
On the Equivalence of Consistency-Type Models: Consistency Models, Consistent Diffusion Models, and Fokker-Planck Regularization. (arXiv:2306.00367v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00367
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24314;&#31435;&#20102;&#19968;&#33268;&#24615;&#27169;&#22411;&#65292;&#19968;&#33268;&#24615;&#25193;&#25955;&#27169;&#22411;&#21644;Fokker-Planck&#27491;&#21017;&#21270;&#20043;&#38388;&#30340;&#29702;&#35770;&#32852;&#31995;&#65292;&#20026;&#19981;&#21516;&#30446;&#26631;&#22686;&#24378;&#25193;&#25955;&#27169;&#22411;&#25552;&#20379;&#20102;&#26356;&#20840;&#38754;&#21644;&#21253;&#23481;&#24615;&#30340;&#26694;&#26550;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#20013;&#21508;&#31181;&#8220;&#19968;&#33268;&#24615;&#8221;&#27010;&#24565;&#30340;&#20986;&#29616;&#24341;&#36215;&#20102;&#24456;&#22823;&#20851;&#27880;&#65292;&#24110;&#21161;&#25913;&#21892;&#20102;&#26679;&#26412;&#36136;&#37327;&#12289;&#20284;&#28982;&#20272;&#35745;&#21644;&#21152;&#36895;&#25277;&#26679;&#12290;&#34429;&#28982;&#22312;&#25991;&#29486;&#20013;&#31867;&#20284;&#30340;&#27010;&#24565;&#24050;&#32463;&#34987;&#25552;&#20986;&#65292;&#20294;&#23427;&#20204;&#20043;&#38388;&#30340;&#30830;&#20999;&#20851;&#31995;&#20173;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19977;&#20010;&#26368;&#36817;&#30340;&#8220;&#19968;&#33268;&#24615;&#8221;&#27010;&#24565;&#20043;&#38388;&#30340;&#29702;&#35770;&#32852;&#31995;&#65292;&#26088;&#22312;&#20026;&#19981;&#21516;&#30446;&#26631;&#22686;&#24378;&#25193;&#25955;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#35265;&#35299;&#20026;&#19968;&#33268;&#24615;&#31867;&#22411;&#27169;&#22411;&#25552;&#20379;&#20102;&#26356;&#20840;&#38754;&#21644;&#21253;&#23481;&#24615;&#30340;&#26694;&#26550;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of various notions of ``consistency'' in diffusion models has garnered considerable attention and helped achieve improved sample quality, likelihood estimation, and accelerated sampling. Although similar concepts have been proposed in the literature, the precise relationships among them remain unclear. In this study, we establish theoretical connections between three recent ``consistency'' notions designed to enhance diffusion models for distinct objectives. Our insights offer the potential for a more comprehensive and encompassing framework for consistency-type models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#29255;&#30340;&#36125;&#21494;&#26031;&#21152;&#24615;&#22238;&#24402;&#26641;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#38543;&#26426;&#21270;&#36741;&#21161;&#21464;&#37327;&#21644;&#20998;&#29255;&#26641;&#32467;&#26500;&#65292;&#37319;&#29992;&#36125;&#21494;&#26031;&#21152;&#24615;&#22238;&#24402;&#26641;&#36866;&#37197;&#27599;&#20010;&#20998;&#21306;&#32452;&#20214;&#21040;&#19968;&#20010;&#23376;&#27169;&#22411;&#20013;&#36827;&#34892;&#25968;&#25454;&#20998;&#21306;&#65292;&#24341;&#20837;&#20132;&#38598;&#26641;&#32467;&#26500;&#26469;&#23436;&#20840;&#20351;&#29992;&#26641;&#32467;&#26500;&#25351;&#23450;&#20998;&#29255;&#21644;&#24314;&#27169;&#12290;&#30740;&#31350;&#20013;&#36824;&#25512;&#23548;&#20102;&#29702;&#35770;&#26368;&#20248;&#26435;&#37325;&#21644;&#35777;&#26126;&#20102;&#27169;&#22411;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.00361</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#29255;&#30340;&#36125;&#21494;&#26031;&#21152;&#24615;&#22238;&#24402;&#26641;
&lt;/p&gt;
&lt;p&gt;
Sharded Bayesian Additive Regression Trees. (arXiv:2306.00361v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#29255;&#30340;&#36125;&#21494;&#26031;&#21152;&#24615;&#22238;&#24402;&#26641;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#38543;&#26426;&#21270;&#36741;&#21161;&#21464;&#37327;&#21644;&#20998;&#29255;&#26641;&#32467;&#26500;&#65292;&#37319;&#29992;&#36125;&#21494;&#26031;&#21152;&#24615;&#22238;&#24402;&#26641;&#36866;&#37197;&#27599;&#20010;&#20998;&#21306;&#32452;&#20214;&#21040;&#19968;&#20010;&#23376;&#27169;&#22411;&#20013;&#36827;&#34892;&#25968;&#25454;&#20998;&#21306;&#65292;&#24341;&#20837;&#20132;&#38598;&#26641;&#32467;&#26500;&#26469;&#23436;&#20840;&#20351;&#29992;&#26641;&#32467;&#26500;&#25351;&#23450;&#20998;&#29255;&#21644;&#24314;&#27169;&#12290;&#30740;&#31350;&#20013;&#36824;&#25512;&#23548;&#20102;&#29702;&#35770;&#26368;&#20248;&#26435;&#37325;&#21644;&#35777;&#26126;&#20102;&#27169;&#22411;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;&#20998;&#29255;&#30340;&#36125;&#21494;&#26031;&#21152;&#24615;&#22238;&#24402;&#26641;&#65288;SBT&#65289;&#27169;&#22411;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#38543;&#26426;&#21270;&#36741;&#21161;&#21464;&#37327;&#21644;&#19968;&#20010;&#20998;&#29255;&#26641;&#26469;&#20915;&#23450;&#25968;&#25454;&#30340;&#20998;&#21306;&#65292;&#24182;&#20351;&#29992;&#36125;&#21494;&#26031;&#21152;&#24615;&#22238;&#24402;&#26641;&#65288;BART&#65289;&#36866;&#37197;&#27599;&#20010;&#20998;&#21306;&#32452;&#20214;&#21040;&#19968;&#20010;&#23376;&#27169;&#22411;&#20013;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#20998;&#29255;&#26641;&#30340;&#26368;&#20248;&#35774;&#35745;&#21487;&#20197;&#30830;&#23450;&#20135;&#21697;&#31354;&#38388;&#19978;&#23376;&#27169;&#22411;&#30340;&#26368;&#20248;&#20998;&#29255;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20132;&#38598;&#26641;&#32467;&#26500;&#26469;&#23436;&#20840;&#20351;&#29992;&#26641;&#32467;&#26500;&#25351;&#23450;&#20998;&#29255;&#21644;&#24314;&#27169;&#12290;&#38500;&#20102;&#23454;&#39564;&#22806;&#65292;&#25105;&#20204;&#36824;&#25512;&#23548;&#20102;&#26368;&#23567;&#21270;&#21518;&#39564;&#25910;&#32553;&#30340;&#29702;&#35770;&#26368;&#20248;&#26435;&#37325;&#65292;&#24182;&#35777;&#26126;&#20102;SBT&#30340;&#26368;&#22351;&#24773;&#20917;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we develop the randomized Sharded Bayesian Additive Regression Trees (SBT) model. We introduce a randomization auxiliary variable and a sharding tree to decide partitioning of data, and fit each partition component to a sub-model using Bayesian Additive Regression Tree (BART). By observing that the optimal design of a sharding tree can determine optimal sharding for sub-models on a product space, we introduce an intersection tree structure to completely specify both the sharding and modeling using only tree structures. In addition to experiments, we also derive the theoretical optimal weights for minimizing posterior contractions and prove the worst-case complexity of SBT.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;ConvNets&#38656;&#35201;&#20381;&#36182;&#22270;&#20687;&#20142;&#24230;&#20449;&#24687;&#30340;&#24773;&#20917;&#65292;&#24182;&#36890;&#36807;&#21487;&#35270;&#21270;&#36827;&#34892;&#20102;&#23637;&#31034;&#12290;</title><link>http://arxiv.org/abs/2306.00360</link><description>&lt;p&gt;
ConvNets&#26159;&#22914;&#20309;&#29702;&#35299;&#22270;&#20687;&#20142;&#24230;&#30340;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Do ConvNets Understand Image Intensity?. (arXiv:2306.00360v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00360
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;ConvNets&#38656;&#35201;&#20381;&#36182;&#22270;&#20687;&#20142;&#24230;&#20449;&#24687;&#30340;&#24773;&#20917;&#65292;&#24182;&#36890;&#36807;&#21487;&#35270;&#21270;&#36827;&#34892;&#20102;&#23637;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(ConvNets)&#36890;&#24120;&#20381;&#36182;&#36793;&#32536;/&#24418;&#29366;&#20449;&#24687;&#26469;&#20998;&#31867;&#22270;&#20687;&#12290;&#36807;&#21435;&#21313;&#24180;&#38388;&#24320;&#21457;&#30340;&#21487;&#35270;&#21270;&#26041;&#27861;&#30830;&#35748;ConvNets&#20381;&#38752;&#36793;&#32536;&#20449;&#24687;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;ConvNets&#38656;&#35201;&#38500;&#24418;&#29366;&#20449;&#24687;&#22806;&#36824;&#38656;&#20381;&#36182;&#22270;&#20687;&#20142;&#24230;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#36890;&#36807;&#21487;&#35270;&#21270;&#23637;&#31034;&#20102;ConvNets&#20381;&#36182;&#22270;&#20687;&#20142;&#24230;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutional Neural Networks (ConvNets) usually rely on edge/shape information to classify images. Visualization methods developed over the last decade confirm that ConvNets rely on edge information. We investigate situations where the ConvNet needs to rely on image intensity in addition to shape. We show that the ConvNet relies on image intensity information using visualization.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26377;&#20195;&#29702;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#36827;&#34892;&#25968;&#25454;&#39537;&#21160;&#25935;&#24863;&#24615;&#20998;&#26512;&#21644;&#26435;&#34913;&#22810;&#30446;&#26631;&#30340;&#32500;&#25968;&#32422;&#20943;&#21442;&#25968;&#36873;&#25321;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#21512;&#25104;&#21644;&#29616;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#31283;&#20581;&#19988;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2306.00357</link><description>&lt;p&gt;
&#39640;&#25928;&#31283;&#20581;&#30340;&#36125;&#21494;&#26031;&#32500;&#25968;&#32422;&#20943;&#21442;&#25968;&#36873;&#25321;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient and Robust Bayesian Selection of Hyperparameters in Dimension Reduction for Visualization. (arXiv:2306.00357v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00357
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26377;&#20195;&#29702;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#36827;&#34892;&#25968;&#25454;&#39537;&#21160;&#25935;&#24863;&#24615;&#20998;&#26512;&#21644;&#26435;&#34913;&#22810;&#30446;&#26631;&#30340;&#32500;&#25968;&#32422;&#20943;&#21442;&#25968;&#36873;&#25321;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#21512;&#25104;&#21644;&#29616;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#31283;&#20581;&#19988;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#31283;&#20581;&#30340;&#33258;&#21160;&#35843;&#21442;&#26694;&#26550;&#65292;&#22312;&#32500;&#25968;&#32422;&#20943;&#31639;&#27861;&#20013;&#36873;&#25321;&#36229;&#21442;&#25968;&#65292;&#27880;&#37325;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21644;&#20219;&#24847;&#24615;&#33021;&#25351;&#26631;&#12290;&#36890;&#36807;&#21033;&#29992;&#26377;&#20195;&#29702;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#24471;&#36229;&#21442;&#25968;&#36873;&#25321;&#20855;&#26377;&#22810;&#30446;&#26631;&#26435;&#34913;&#65292;&#24182;&#19988;&#20801;&#35768;&#25105;&#20204;&#36827;&#34892;&#25968;&#25454;&#39537;&#21160;&#30340;&#25935;&#24863;&#24615;&#20998;&#26512;&#12290;&#36890;&#36807;&#32467;&#21512;&#24402;&#19968;&#21270;&#21644;&#23376;&#37319;&#26679;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#21487;&#35270;&#21270;&#25216;&#26415;&#22914;t-SNE&#21644;UMAP&#20013;&#23637;&#29616;&#20986;&#20102;&#36890;&#29992;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#22810;&#20010;&#36136;&#37327;&#24230;&#37327;&#22312;&#21508;&#31181;&#21512;&#25104;&#21644;&#29616;&#23454;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#65292;&#20026;&#32500;&#25968;&#32422;&#20943;&#31639;&#27861;&#30340;&#36229;&#21442;&#25968;&#36873;&#25321;&#25552;&#20379;&#20102;&#19968;&#31181;&#31283;&#20581;&#19988;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce an efficient and robust auto-tuning framework for hyperparameter selection in dimension reduction (DR) algorithms, focusing on large-scale datasets and arbitrary performance metrics. By leveraging Bayesian optimization (BO) with a surrogate model, our approach enables efficient hyperparameter selection with multi-objective trade-offs and allows us to perform data-driven sensitivity analysis. By incorporating normalization and subsampling, the proposed framework demonstrates versatility and efficiency, as shown in applications to visualization techniques such as t-SNE and UMAP. We evaluate our results on various synthetic and real-world datasets using multiple quality metrics, providing a robust and efficient solution for hyperparameter selection in DR algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#22810;&#20219;&#21153;&#23398;&#20064;&#35282;&#24230;&#20986;&#21457;&#65292;&#30740;&#31350;&#20102;&#25193;&#25955;&#35757;&#32451;&#20013;&#30340;&#36127;&#36801;&#31227;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#21033;&#29992;&#27491;&#21017;&#21270;&#25216;&#26415;&#22686;&#24378;&#25193;&#25955;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#20197;&#20943;&#36731;&#36127;&#36801;&#31227;&#24182;&#25552;&#39640;&#21435;&#22122;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.00354</link><description>&lt;p&gt;
&#35299;&#20915;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#36127;&#36801;&#31227;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Addressing Negative Transfer in Diffusion Models. (arXiv:2306.00354v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00354
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#22810;&#20219;&#21153;&#23398;&#20064;&#35282;&#24230;&#20986;&#21457;&#65292;&#30740;&#31350;&#20102;&#25193;&#25955;&#35757;&#32451;&#20013;&#30340;&#36127;&#36801;&#31227;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#21033;&#29992;&#27491;&#21017;&#21270;&#25216;&#26415;&#22686;&#24378;&#25193;&#25955;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#20197;&#20943;&#36731;&#36127;&#36801;&#31227;&#24182;&#25552;&#39640;&#21435;&#22122;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#21508;&#20010;&#39046;&#22495;&#37117;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#23427;&#22312;&#21516;&#26102;&#28085;&#30422;&#19981;&#21516;&#22122;&#22768;&#27700;&#24179;&#30340;&#21435;&#22122;&#20219;&#21153;&#19978;&#35757;&#32451;&#27169;&#22411;&#65292;&#20195;&#34920;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#30340;&#24418;&#24335;&#12290;&#28982;&#32780;&#65292;&#20174;MTL&#30340;&#35282;&#24230;&#20998;&#26512;&#21644;&#25913;&#21892;&#25193;&#25955;&#27169;&#22411;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#29305;&#21035;&#22320;&#65292;MTL&#26377;&#26102;&#20250;&#23548;&#33268;&#20247;&#25152;&#21608;&#30693;&#30340;$\textit{&#36127;&#36801;&#31227;}$&#29616;&#35937;&#65292;&#36825;&#31181;&#29616;&#35937;&#26159;&#30001;&#20110;&#20219;&#21153;&#20043;&#38388;&#23384;&#22312;&#20914;&#31361;&#32780;&#23548;&#33268;&#26576;&#20123;&#20219;&#21153;&#30340;&#24615;&#33021;&#38477;&#20302;&#12290;&#26412;&#25991;&#26088;&#22312;&#20174;MTL&#30340;&#35282;&#24230;&#20998;&#26512;&#25193;&#25955;&#35757;&#32451;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#20851;&#38190;&#35266;&#23519;&#65306;$\textbf{(O1)}$ &#38543;&#30528;&#22122;&#22768;&#27700;&#24179;&#20043;&#38388;&#30340;&#24046;&#36317;&#21152;&#22823;&#65292;&#21435;&#22122;&#20219;&#21153;&#20043;&#38388;&#30340;&#20219;&#21153;&#20146;&#21644;&#21147;&#20943;&#24369;&#65292; $\textbf{(O2)}$ &#22312;&#25193;&#25955;&#35757;&#32451;&#30340;&#32972;&#26223;&#19979;&#65292;&#36127;&#36801;&#31227;&#20063;&#21487;&#33021;&#20250;&#20986;&#29616;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#20943;&#36731;&#36127;&#36801;&#31227;&#26469;&#22686;&#24378;&#25193;&#25955;&#35757;&#32451;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21033;&#29992;&#29616;&#26377;&#30340;MTL&#26041;&#27861;&#12289;&#20855;&#20307;&#26159;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#26469;&#40723;&#21169;&#20219;&#21153;&#29305;&#23450;&#30340;&#29305;&#24449;&#25552;&#21462;&#24182;&#20943;&#23569;&#20219;&#21153;&#24178;&#25200;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#20943;&#36731;&#20102;&#36127;&#36801;&#31227;&#65292;&#25552;&#39640;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#19968;&#31995;&#21015;&#21435;&#22122;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion-based generative models have achieved remarkable success in various domains. It trains a model on denoising tasks that encompass different noise levels simultaneously, representing a form of multi-task learning (MTL). However, analyzing and improving diffusion models from an MTL perspective remains under-explored. In particular, MTL can sometimes lead to the well-known phenomenon of $\textit{negative transfer}$, which results in the performance degradation of certain tasks due to conflicts between tasks. In this paper, we aim to analyze diffusion training from an MTL standpoint, presenting two key observations: $\textbf{(O1)}$ the task affinity between denoising tasks diminishes as the gap between noise levels widens, and $\textbf{(O2)}$ negative transfer can arise even in the context of diffusion training. Building upon these observations, our objective is to enhance diffusion training by mitigating negative transfer. To achieve this, we propose leveraging existing MTL metho
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#27010;&#29575;&#35270;&#35282;&#30340;&#23545;&#25239;&#26679;&#26412;&#26500;&#24314;&#26041;&#27861;&#65292;&#21487;&#20197;&#29983;&#25104;&#35821;&#20041;&#24863;&#30693;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#65292;&#24182;&#21487;&#20197;&#26377;&#25928;&#35268;&#36991;&#20256;&#32479;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24378;&#21270;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.00353</link><description>&lt;p&gt;
&#20174;&#27010;&#29575;&#35282;&#24230;&#26500;&#24314;&#35821;&#20041;&#24863;&#30693;&#30340;&#23545;&#25239;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
Constructing Semantics-Aware Adversarial Examples with Probabilistic Perspective. (arXiv:2306.00353v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00353
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#27010;&#29575;&#35270;&#35282;&#30340;&#23545;&#25239;&#26679;&#26412;&#26500;&#24314;&#26041;&#27861;&#65292;&#21487;&#20197;&#29983;&#25104;&#35821;&#20041;&#24863;&#30693;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#65292;&#24182;&#21487;&#20197;&#26377;&#25928;&#35268;&#36991;&#20256;&#32479;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24378;&#21270;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#29575;&#35270;&#35282;&#23545;&#25239;&#26679;&#26412;&#26500;&#24314;&#26041;&#27861;&#8212;&#8212;&#31665;&#32422;&#26463; Langevin Monte Carlo&#65288;LMC&#65289;&#12290;&#20174;&#36825;&#20010;&#35282;&#24230;&#20986;&#21457;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21019;&#26032;&#24615;&#30340;&#26041;&#27861;&#65292;&#20197;&#21407;&#21017;&#24615;&#30340;&#26041;&#24335;&#29983;&#25104;&#35821;&#20041;&#24863;&#30693;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#12290;&#36825;&#31181;&#26041;&#27861;&#36229;&#36234;&#20102;&#20960;&#20309;&#36317;&#31163;&#25152;&#26045;&#21152;&#30340;&#38480;&#21046;&#65292;&#36873;&#25321;&#20102;&#35821;&#20041;&#32422;&#26463;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36171;&#20104;&#20102;&#20010;&#20307;&#23558;&#20854;&#23545;&#35821;&#20041;&#30340;&#29702;&#35299;&#34701;&#20837;&#21040;&#27169;&#22411;&#20013;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#20154;&#31867;&#35780;&#20272;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#35821;&#20041;&#24863;&#30693;&#30340;&#23545;&#25239;&#26679;&#26412;&#20445;&#25345;&#20854;&#22266;&#26377;&#30340;&#21547;&#20041;&#12290;&#22312; MNIST &#21644; SVHN &#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#35821;&#20041;&#24863;&#30693;&#30340;&#23545;&#25239;&#26679;&#26412;&#21487;&#20197;&#26377;&#25928;&#22320;&#35268;&#36991;&#38024;&#23545;&#20256;&#32479;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24378;&#20581;&#24615;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we introduce a novel, probabilistic viewpoint on adversarial examples, achieved through box-constrained Langevin Monte Carlo (LMC). Proceeding from this perspective, we develop an innovative approach for generating semantics-aware adversarial examples in a principled manner. This methodology transcends the restriction imposed by geometric distance, instead opting for semantic constraints. Our approach empowers individuals to incorporate their personal comprehension of semantics into the model. Through human evaluation, we validate that our semantics-aware adversarial examples maintain their inherent meaning. Experimental findings on the MNIST and SVHN datasets demonstrate that our semantics-aware adversarial examples can effectively circumvent robust adversarial training methods tailored for traditional adversarial attacks.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#33021;&#37327;&#23432;&#24658;&#19979;&#38477;&#65288;ECD&#65289;&#30340;&#29702;&#35770;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#31639;&#27861;ECDSep&#65292;&#33021;&#22815;&#22788;&#29702;&#20984;&#20248;&#21270;&#38382;&#39064;&#21644;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#25913;&#36827;&#21160;&#24577;&#21644;&#28151;&#27788;&#35825;&#23548;&#20803;&#32032;&#65292;&#25552;&#39640;&#24615;&#33021;&#65307;&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#19978;&#19982;&#27969;&#34892;&#30340;&#20248;&#21270;&#26041;&#27861;&#36827;&#34892;&#20102;&#23454;&#35777;&#27604;&#36739;&#65292;&#21457;&#29616;&#22312;&#27599;&#20010;&#20219;&#21153;&#20013;&#20855;&#26377;&#31454;&#20105;&#21147;&#25110;&#25913;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.00352</link><description>&lt;p&gt;
&#25913;&#36827;&#26426;&#22120;&#23398;&#20064;&#30340;&#33021;&#37327;&#23432;&#24658;&#19979;&#38477;&#27861;&#65306;&#29702;&#35770;&#19982;&#23454;&#36341;
&lt;/p&gt;
&lt;p&gt;
Improving Energy Conserving Descent for Machine Learning: Theory and Practice. (arXiv:2306.00352v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00352
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#33021;&#37327;&#23432;&#24658;&#19979;&#38477;&#65288;ECD&#65289;&#30340;&#29702;&#35770;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#31639;&#27861;ECDSep&#65292;&#33021;&#22815;&#22788;&#29702;&#20984;&#20248;&#21270;&#38382;&#39064;&#21644;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#25913;&#36827;&#21160;&#24577;&#21644;&#28151;&#27788;&#35825;&#23548;&#20803;&#32032;&#65292;&#25552;&#39640;&#24615;&#33021;&#65307;&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#19978;&#19982;&#27969;&#34892;&#30340;&#20248;&#21270;&#26041;&#27861;&#36827;&#34892;&#20102;&#23454;&#35777;&#27604;&#36739;&#65292;&#21457;&#29616;&#22312;&#27599;&#20010;&#20219;&#21153;&#20013;&#20855;&#26377;&#31454;&#20105;&#21147;&#25110;&#25913;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21457;&#23637;&#20102;&#33021;&#37327;&#23432;&#24658;&#19979;&#38477;&#65288;ECD&#65289;&#30340;&#29702;&#35770;&#65292;&#24182;&#20171;&#32461;&#20102;ECDSep&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#33021;&#22815;&#22788;&#29702;&#20984;&#20248;&#21270;&#38382;&#39064;&#21644;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#26032;&#39062;&#30340;ECD&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#36866;&#24403;&#28151;&#27788;&#30340;&#33021;&#37327;&#23432;&#24658;&#21160;&#21147;&#31995;&#32479;&#30340;&#29289;&#29702;&#28436;&#21270;&#26469;&#23454;&#29616;&#65292;&#20351;&#24471;&#21363;&#20351;&#23545;&#20110;&#26080;&#23545;&#31216;&#24615;&#30340;&#36890;&#29992;&#39640;&#32500;&#38382;&#39064;&#30340;&#32467;&#26524;&#20998;&#24067;&#20063;&#33021;&#36827;&#34892;&#20998;&#26512;&#25511;&#21046;&#65292;&#20174;&#32780;&#20027;&#23548;&#20302;&#25439;&#22833;&#12290;&#19982;&#20197;&#24448;&#30340;&#23454;&#29616;&#30456;&#27604;&#65292;&#25105;&#20204;&#21033;&#29992;&#29702;&#35770;&#25511;&#21046;&#26469;&#25913;&#36827;&#21160;&#24577;&#21644;&#35825;&#23548;&#28151;&#27788;&#30340;&#20803;&#32032;&#65292;&#25552;&#39640;&#24615;&#33021;&#65292;&#21516;&#26102;&#31616;&#21270;&#38754;&#21521;&#19981;&#21516;&#31867;&#21035;&#38382;&#39064;&#30340;&#20248;&#21270;&#31639;&#27861;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#19978;&#19982;&#27969;&#34892;&#30340;&#20248;&#21270;&#26041;&#27861;&#65288;&#22914;SGD&#65292;Adam&#21644;AdamW&#31561;&#65289;&#36827;&#34892;&#20102;&#23454;&#35777;&#27604;&#36739;&#65292;&#21457;&#29616;&#22312;&#27599;&#20010;&#20219;&#21153;&#20013;&#19982;&#23427;&#20204;&#20013;&#30340;&#26368;&#20339;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#25110;&#25913;&#36827;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop the theory of Energy Conserving Descent (ECD) and introduce ECDSep, a gradient-based optimization algorithm able to tackle convex and non-convex optimization problems. The method is based on the novel ECD framework of optimization as physical evolution of a suitable chaotic energy-conserving dynamical system, enabling analytic control of the distribution of results - dominated at low loss - even for generic high-dimensional problems with no symmetries. Compared to previous realizations of this idea, we exploit the theoretical control to improve both the dynamics and chaos-inducing elements, enhancing performance while simplifying the hyper-parameter tuning of the optimization algorithm targeted to different classes of problems. We empirically compare with popular optimization methods such as SGD, Adam and AdamW on a wide range of machine learning problems, finding competitive or improved performance compared to the best among them on each task. We identify limitations in our
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24335;BEV&#24863;&#30693;&#33258;&#25105;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;CALICO&#65292;&#23427;&#23558;&#23545;&#27604;&#30446;&#26631;&#24212;&#29992;&#20110;LiDAR&#21644;&#30456;&#26426;&#39592;&#24178;&#65292;&#35777;&#26126;&#20854;&#27604;&#29616;&#26377;&#27169;&#22411;&#26356;&#21152;&#39640;&#25928;&#26377;&#25928;&#65292;&#24182;&#22312;BEV&#24863;&#30693;&#20219;&#21153;&#20013;&#25552;&#20379;&#20102;&#26174;&#30528;&#30340;&#25910;&#30410;&#12290;</title><link>http://arxiv.org/abs/2306.00349</link><description>&lt;p&gt;
CALICO&#65306;&#29992;&#20110;BEV&#24863;&#30693;&#30340;&#33258;&#25105;&#30417;&#30563;&#30456;&#26426;-LiDAR&#23545;&#27604;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
CALICO: Self-Supervised Camera-LiDAR Contrastive Pre-training for BEV Perception. (arXiv:2306.00349v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00349
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24335;BEV&#24863;&#30693;&#33258;&#25105;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;CALICO&#65292;&#23427;&#23558;&#23545;&#27604;&#30446;&#26631;&#24212;&#29992;&#20110;LiDAR&#21644;&#30456;&#26426;&#39592;&#24178;&#65292;&#35777;&#26126;&#20854;&#27604;&#29616;&#26377;&#27169;&#22411;&#26356;&#21152;&#39640;&#25928;&#26377;&#25928;&#65292;&#24182;&#22312;BEV&#24863;&#30693;&#20219;&#21153;&#20013;&#25552;&#20379;&#20102;&#26174;&#30528;&#30340;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#39046;&#22495;&#65292;&#24863;&#30693;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20854;&#20013;&#22522;&#20110;&#40479;&#30640;&#22270;&#65288;BEV&#65289;&#30340;&#26550;&#26500;&#26368;&#36817;&#24050;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#33258;&#25105;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#21487;&#21462;&#24615;&#28304;&#20110;&#27880;&#37322;2D&#21644;3D&#25968;&#25454;&#30340;&#26114;&#36149;&#21644;&#36153;&#21147;&#36807;&#31243;&#12290;&#23613;&#31649;&#20197;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#30740;&#31350;&#20102;LiDAR&#21644;&#22522;&#20110;&#30456;&#26426;&#30340;3D&#29289;&#20307;&#26816;&#27979;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#20294;&#32570;&#23569;&#22810;&#27169;&#24335;BEV&#24863;&#30693;&#30340;&#32479;&#19968;&#39044;&#35757;&#32451;&#26694;&#26550;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;CALICO&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#23427;&#23558;&#23545;&#27604;&#30446;&#26631;&#24212;&#29992;&#20110;LiDAR&#21644;&#30456;&#26426;&#39592;&#24178;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;CALICO&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#65306;&#28857;&#21306;&#22495;&#23545;&#27604;&#65288;PRC&#65289;&#21644;&#21306;&#22495;&#24863;&#30693;&#33976;&#39311;&#65288;RAD&#65289;&#12290;PRC&#26356;&#22909;&#22320;&#24179;&#34913;&#20102;&#23545;LiDAR&#27169;&#24577;&#30340;&#21306;&#22495;&#21644;&#22330;&#26223;&#32423;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#25552;&#20379;&#20102;&#26174;&#30528;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;RAD&#26377;&#25928;&#22320;&#22312;&#25105;&#20204;&#30340;&#33258;&#25105;&#35757;&#32451;&#25945;&#24072;&#27169;&#22411;&#19978;&#23454;&#29616;&#20102;&#23545;&#27604;&#33976;&#39311;&#12290;CALICO&#30340;&#39640;&#25928;&#21644;&#26377;&#25928;&#30340;&#39044;&#35757;&#32451;&#25216;&#26415;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;BEV&#24863;&#30693;&#20219;&#21153;&#20013;&#25552;&#20379;&#20102;&#26174;&#30528;&#30340;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Perception is crucial in the realm of autonomous driving systems, where bird's eye view (BEV)-based architectures have recently reached state-of-the-art performance. The desirability of self-supervised representation learning stems from the expensive and laborious process of annotating 2D and 3D data. Although previous research has investigated pretraining methods for both LiDAR and camera-based 3D object detection, a unified pretraining framework for multimodal BEV perception is missing. In this study, we introduce CALICO, a novel framework that applies contrastive objectives to both LiDAR and camera backbones. Specifically, CALICO incorporates two stages: point-region contrast (PRC) and region-aware distillation (RAD). PRC better balances the region- and scene-level representation learning on the LiDAR modality and offers significant performance improvement compared to existing methods. RAD effectively achieves contrastive distillation on our self-trained teacher model. CALICO's effi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#30340;&#26032;&#39062;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24191;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21160;&#35789;&#26367;&#25442;&#36827;&#34892;&#65292;&#29992;&#20110;&#21307;&#23398;&#22240;&#26524;&#20851;&#31995;&#25552;&#21462;&#65292;&#24182;&#19988;&#19982;&#20854;&#20182;3&#31181;&#25968;&#25454;&#22686;&#24191;&#25216;&#26415;&#30456;&#27604;&#36739;&#65292;&#22312;&#23569;&#25968;&#31867;&#26041;&#38754;&#26377;&#26174;&#33879;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2306.00346</link><description>&lt;p&gt;
SemEval2023&#20219;&#21153;8&#20013;&#30340;CAISA&#65306;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24191;&#29992;&#20110;&#20943;&#36731;&#22240;&#26524;&#20851;&#31995;&#25552;&#21462;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
CAISA at SemEval-2023 Task 8: Counterfactual Data Augmentation for Mitigating Class Imbalance in Causal Claim Identification. (arXiv:2306.00346v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00346
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#30340;&#26032;&#39062;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24191;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21160;&#35789;&#26367;&#25442;&#36827;&#34892;&#65292;&#29992;&#20110;&#21307;&#23398;&#22240;&#26524;&#20851;&#31995;&#25552;&#21462;&#65292;&#24182;&#19988;&#19982;&#20854;&#20182;3&#31181;&#25968;&#25454;&#22686;&#24191;&#25216;&#26415;&#30456;&#27604;&#36739;&#65292;&#22312;&#23569;&#25968;&#31867;&#26041;&#38754;&#26377;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#20250;&#23548;&#33268;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#23569;&#25968;&#31867;&#21644;&#25972;&#20010;&#25968;&#25454;&#38598;&#19978;&#20135;&#29983;&#19981;&#29702;&#24819;&#30340;&#34920;&#29616;&#12290;&#20351;&#29992;&#25968;&#25454;&#22686;&#24191;&#25216;&#26415;&#26469;&#22686;&#21152;&#26679;&#26412;&#25968;&#37327;&#26159;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#31181;&#26041;&#24335;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36890;&#36807;&#21160;&#35789;&#26367;&#25442;&#36827;&#34892;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24191;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21307;&#23398;&#22240;&#26524;&#20851;&#31995;&#25552;&#21462;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#35813;&#26041;&#27861;&#30340;&#24433;&#21709;&#65292;&#24182;&#23558;&#20854;&#19982;&#20854;&#20182;3&#31181;&#25968;&#25454;&#22686;&#24191;&#25216;&#26415;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#65288;&#30456;&#23545;&#65289;&#25552;&#39640;&#23569;&#25968;&#31867;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The class imbalance problem can cause machine learning models to produce an undesirable performance on the minority class as well as the whole dataset. Using data augmentation techniques to increase the number of samples is one way to tackle this problem. We introduce a novel counterfactual data augmentation by verb replacement for the identification of medical claims. In addition, we investigate the impact of this method and compare it with 3 other data augmentation techniques, showing that the proposed method can result in a significant (relative) improvement in the minority class.
&lt;/p&gt;</description></item><item><title>BOtied &#26159;&#19968;&#31181;&#24102;&#26377;&#30456;&#20851;&#22810;&#20803;&#31561;&#32423;&#30340;&#22810;&#30446;&#26631;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#36739;&#22909;&#30340;&#36817;&#20284;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.00344</link><description>&lt;p&gt;
BOtied: &#24102;&#26377;&#30456;&#20851;&#22810;&#20803;&#31561;&#32423;&#30340;&#22810;&#30446;&#26631;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
BOtied: Multi-objective Bayesian optimization with tied multivariate ranks. (arXiv:2306.00344v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00344
&lt;/p&gt;
&lt;p&gt;
BOtied &#26159;&#19968;&#31181;&#24102;&#26377;&#30456;&#20851;&#22810;&#20803;&#31561;&#32423;&#30340;&#22810;&#30446;&#26631;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#36739;&#22909;&#30340;&#36817;&#20284;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#31185;&#23398;&#21644;&#24037;&#19994;&#24212;&#29992;&#38656;&#35201;&#21516;&#26102;&#20248;&#21270;&#22810;&#20010;&#28508;&#22312;&#30340;&#30456;&#20114;&#20914;&#31361;&#30340;&#30446;&#26631;&#12290;&#22810;&#30446;&#26631;&#36125;&#21494;&#26031;&#20248;&#21270; (MOBO) &#26159;&#19968;&#31181;&#39640;&#25928;&#22320;&#35782;&#21035; Pareto &#26368;&#20248;&#35299;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26080;&#25903;&#37197;&#35299;&#21644;&#26368;&#39640;&#22810;&#20803;&#31561;&#32423;&#20043;&#38388;&#30340;&#33258;&#28982;&#32852;&#31995;&#65292;&#23427;&#19982;&#32852;&#21512;&#32047;&#31215;&#20998;&#24067;&#20989;&#25968;&#30340;&#26368;&#22806;&#23618;&#31561;&#39640;&#32447;&#37325;&#21512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102; CDF indicator&#65292;&#36825;&#26159;&#19968;&#31181; Pareto &#21512;&#35268;&#30340;&#24230;&#37327;&#65292;&#29992;&#20110;&#35780;&#20272;&#36817;&#20284; Pareto &#38598;&#21512;&#30340;&#36136;&#37327;&#65292;&#23427;&#34917;&#20805;&#20102;&#27969;&#34892;&#30340; hypervolume indicator&#12290;MOBO &#30340;&#26680;&#24515;&#26159;&#37319;&#38598;&#20989;&#25968;&#65292;&#23427;&#36890;&#36807;&#23548;&#33322;&#30446;&#26631;&#20043;&#38388;&#30340;&#26368;&#20339;&#25240;&#20013;&#26469;&#30830;&#23450;&#19979;&#19968;&#20010;&#35201;&#35780;&#20272;&#30340;&#20505;&#36873;&#39033;&#12290; &#22522;&#20110;&#30418;&#23376;&#20998;&#35299;&#30446;&#26631;&#31354;&#38388;&#30340;&#22810;&#30446;&#26631;&#37319;&#38598;&#20989;&#25968;&#65288;&#20363;&#22914;&#26399;&#26395;&#30340; hypervolume &#25913;&#36827;&#65288;EHVI&#65289;&#21644;&#29109;&#25628;&#32034;&#65289;&#22312;&#23384;&#22312;&#22823;&#37327;&#30446;&#26631;&#26102;&#30340;&#24615;&#33021;&#32553;&#25918;&#24456;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#37319;&#38598;&#20989;&#25968;&#65292;&#31216;&#20026; BOtied&#65292;&#23427;&#21033;&#29992;&#30456;&#20851;&#22810;&#20803;&#31561;&#32423;&#26469;&#39640;&#25928;&#25628;&#32034; Pareto frontier&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;BOtied &#22312;&#26679;&#26412;&#25928;&#29575;&#21644;&#36817;&#20284;&#36136;&#37327;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many scientific and industrial applications require joint optimization of multiple, potentially competing objectives. Multi-objective Bayesian optimization (MOBO) is a sample-efficient framework for identifying Pareto-optimal solutions. We show a natural connection between non-dominated solutions and the highest multivariate rank, which coincides with the outermost level line of the joint cumulative distribution function (CDF). We propose the CDF indicator, a Pareto-compliant metric for evaluating the quality of approximate Pareto sets that complements the popular hypervolume indicator. At the heart of MOBO is the acquisition function, which determines the next candidate to evaluate by navigating the best compromises among the objectives. Multi-objective acquisition functions that rely on box decomposition of the objective space, such as the expected hypervolume improvement (EHVI) and entropy search, scale poorly to a large number of objectives. We propose an acquisition function, call
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26174;&#24335;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#19982;&#38544;&#24335;&#27491;&#21017;&#21270;&#32467;&#21512;&#65292;&#21487;&#20197;&#20351;&#21333;&#23618;&#32593;&#32476;&#23454;&#29616;&#19982;&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#30456;&#24403;&#30340;&#20302;&#31209;&#36817;&#20284;&#21644;&#27867;&#21270;&#35823;&#24046;&#65292;&#20351;&#28145;&#24230;&#19981;&#20877;&#26159;&#23398;&#20064;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2306.00342</link><description>&lt;p&gt;
&#32467;&#21512;&#26174;&#24335;&#21644;&#38544;&#24335;&#27491;&#21017;&#21270;&#30340;&#28145;&#24230;&#32593;&#32476;&#39640;&#25928;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Combining Explicit and Implicit Regularization for Efficient Learning in Deep Networks. (arXiv:2306.00342v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00342
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26174;&#24335;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#19982;&#38544;&#24335;&#27491;&#21017;&#21270;&#32467;&#21512;&#65292;&#21487;&#20197;&#20351;&#21333;&#23618;&#32593;&#32476;&#23454;&#29616;&#19982;&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#30456;&#24403;&#30340;&#20302;&#31209;&#36817;&#20284;&#21644;&#27867;&#21270;&#35823;&#24046;&#65292;&#20351;&#28145;&#24230;&#19981;&#20877;&#26159;&#23398;&#20064;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#24335;&#27491;&#21017;&#21270;&#30740;&#31350;&#20248;&#21270;&#36807;&#31243;&#20013;&#30340;&#26799;&#24230;&#36712;&#36857;&#65292;&#20197;&#35299;&#37322;&#20026;&#20160;&#20040;&#28145;&#24230;&#32593;&#32476;&#26356;&#20542;&#21521;&#20110;&#26576;&#20123;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#20013;&#65292;&#24050;&#32463;&#35777;&#26126;&#26799;&#24230;&#19979;&#38477;&#38544;&#24335;&#22320;&#26397;&#21521;&#30697;&#38453;&#34917;&#20840;/&#22240;&#24335;&#20998;&#35299;&#20219;&#21153;&#19978;&#30340;&#20302;&#31209;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#27491;&#21017;&#21270;&#12290;&#28155;&#21152;&#23618;&#25968;&#19981;&#20165;&#21487;&#20197;&#25552;&#39640;&#36825;&#20123;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#32780;&#19988;&#20316;&#20026;&#19968;&#31181;&#21152;&#36895;&#30340;&#39044;&#22788;&#29702;&#26041;&#27861;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#36825;&#31181;&#20302;&#31209;&#20559;&#21521;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26174;&#24335;&#24809;&#32602;&#26469;&#21453;&#26144;&#36825;&#31181;&#38544;&#24335;&#20559;&#24046;&#65292;&#21482;&#22312;&#26576;&#20123;&#33258;&#36866;&#24212;&#26799;&#24230;&#20248;&#21270;&#22120;&#65288;&#20363;&#22914;Adam&#65289;&#36215;&#20316;&#29992;&#12290;&#36825;&#31181;&#32452;&#21512;&#21487;&#20197;&#20351;&#36864;&#21270;&#30340;&#21333;&#23618;&#32593;&#32476;&#23454;&#29616;&#19982;&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#30456;&#24403;&#30340;&#20302;&#31209;&#36817;&#20284;&#21644;&#27867;&#21270;&#35823;&#24046;&#65292;&#20351;&#28145;&#24230;&#19981;&#20877;&#26159;&#23398;&#20064;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;&#21333;&#23618;&#32593;&#32476;&#36824;&#22312;&#19968;&#31995;&#21015;&#21442;&#25968;&#21644;&#25968;&#25454;&#38598;&#19978;&#33021;&#22815;&#34920;&#29616;&#20248;&#24322;&#65292;&#29978;&#33267;&#36229;&#36807;&#20102;&#21508;&#31181;&#30697;&#38453;&#34917;&#20840;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Works on implicit regularization have studied gradient trajectories during the optimization process to explain why deep networks favor certain kinds of solutions over others. In deep linear networks, it has been shown that gradient descent implicitly regularizes toward low-rank solutions on matrix completion/factorization tasks. Adding depth not only improves performance on these tasks but also acts as an accelerative pre-conditioning that further enhances this bias towards low-rankedness. Inspired by this, we propose an explicit penalty to mirror this implicit bias which only takes effect with certain adaptive gradient optimizers (e.g. Adam). This combination can enable a degenerate single-layer network to achieve low-rank approximations with generalization error comparable to deep linear networks, making depth no longer necessary for learning. The single-layer network also performs competitively or out-performs various approaches for matrix completion over a range of parameter and da
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#21518;&#19968;&#27425;&#20999;&#25442;&#20381;&#36182;&#36172;&#21338;&#26426;&#30340;&#35268;&#21010;&#38382;&#39064;&#65292;&#35774;&#35745;&#20102;&#31532;&#19968;&#20010;&#26377;&#25928;&#30340;&#24120;&#25968;&#36817;&#20284;&#31639;&#27861;&#65292;&#24182;&#22312;&#33258;&#28982;&#21333;&#35843;&#36882;&#22686;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#36817;&#20046;&#26368;&#20248;&#30340;&#24615;&#33021;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2306.00338</link><description>&lt;p&gt;
&#20855;&#26377;&#21333;&#35843;&#25910;&#30410;&#20989;&#25968;&#30340;&#26368;&#21518;&#19968;&#27425;&#20999;&#25442;&#20381;&#36182;&#36172;&#21338;&#26426;&#30340;&#35268;&#21010;&#38382;&#39064;&#30340;&#36817;&#20284;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Last Switch Dependent Bandits with Monotone Payoff Functions. (arXiv:2306.00338v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00338
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#21518;&#19968;&#27425;&#20999;&#25442;&#20381;&#36182;&#36172;&#21338;&#26426;&#30340;&#35268;&#21010;&#38382;&#39064;&#65292;&#35774;&#35745;&#20102;&#31532;&#19968;&#20010;&#26377;&#25928;&#30340;&#24120;&#25968;&#36817;&#20284;&#31639;&#27861;&#65292;&#24182;&#22312;&#33258;&#28982;&#21333;&#35843;&#36882;&#22686;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#36817;&#20046;&#26368;&#20248;&#30340;&#24615;&#33021;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;Laforgue&#31561;&#20154;&#24341;&#20837;&#20102;&#26368;&#21518;&#19968;&#27425;&#20999;&#25442;&#20381;&#36182;&#36172;&#21338;&#26426;&#27169;&#22411;&#65292;&#20197;&#25429;&#25417;&#30001;&#29609;&#23478;&#21644;&#29615;&#22659;&#20043;&#38388;&#30340;&#20132;&#20114;&#24341;&#36215;&#30340;&#38750;&#38745;&#24577;&#29616;&#35937;&#12290;&#26412;&#25991;&#38024;&#23545;&#35745;&#31639;&#26368;&#20248;&#25289;&#33218;&#31574;&#30053;&#65288;&#35813;&#38382;&#39064;&#26159;NP-hard&#65289;&#36825;&#19968;&#35268;&#21010;&#38382;&#39064;&#65292;&#30740;&#31350;&#20102;LSD&#65288;Last Switch Dependent&#65289;&#36172;&#21338;&#26426;&#30340;&#36817;&#20284;&#24615;&#36136;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#31532;&#19968;&#20010;&#26377;&#25928;&#30340;&#24120;&#25968;&#36817;&#20284;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#65292;&#22312;&#25910;&#30410;&#33258;&#28982;&#21333;&#35843;&#36882;&#22686;&#30340;&#24773;&#20917;&#19979;&#65292;&#19982;&#29305;&#27530;&#30340;&#20805;&#33021;&#36172;&#21338;&#26426;&#65288;&#20063;&#31216;&#20026;&#24310;&#36831;&#20381;&#36182;&#36172;&#21338;&#26426;&#65289;&#30340;&#26368;&#20248;&#24615;&#20445;&#35777;&#65288;&#20960;&#20046;&#65289;&#30456;&#21305;&#37197;&#12290;&#22312;&#27492;&#23581;&#35797;&#20013;&#65292;&#25105;&#20204;&#20026;&#36825;&#31867;&#36172;&#21338;&#26426;&#24320;&#21457;&#20102;&#26032;&#30340;&#24037;&#20855;&#21644;&#35265;&#35299;&#65292;&#36825;&#21487;&#33021;&#26159;&#29420;&#31435;&#24863;&#20852;&#36259;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a recent work, Laforgue et al. introduce the model of last switch dependent (LSD) bandits, in an attempt to capture nonstationary phenomena induced by the interaction between the player and the environment. Examples include satiation, where consecutive plays of the same action lead to decreased performance, or deprivation, where the payoff of an action increases after an interval of inactivity. In this work, we take a step towards understanding the approximability of planning LSD bandits, namely, the (NP-hard) problem of computing an optimal arm-pulling strategy under complete knowledge of the model. In particular, we design the first efficient constant approximation algorithm for the problem and show that, under a natural monotonicity assumption on the payoffs, its approximation guarantee (almost) matches the state-of-the-art for the special and well-studied class of recharging bandits (also known as delay-dependent). In this attempt, we develop new tools and insights for this clas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#22810;&#26234;&#33021;&#20307;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#23454;&#29616;&#20844;&#24179;&#24615;&#65292;&#24341;&#20837;&#20844;&#24179;&#20989;&#25968;&#26469;&#30830;&#20445;&#20195;&#29702;&#20043;&#38388;&#30340;&#22870;&#21169;&#20844;&#24179;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#22312;&#32447;&#20984;&#20248;&#21270;&#30340;&#26041;&#27861;&#33719;&#24471;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2306.00324</link><description>&lt;p&gt;
&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#22312;&#22810;&#26234;&#33021;&#20307;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#23454;&#29616;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Achieving Fairness in Multi-Agent Markov Decision Processes Using Reinforcement Learning. (arXiv:2306.00324v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00324
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#22810;&#26234;&#33021;&#20307;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#23454;&#29616;&#20844;&#24179;&#24615;&#65292;&#24341;&#20837;&#20844;&#24179;&#20989;&#25968;&#26469;&#30830;&#20445;&#20195;&#29702;&#20043;&#38388;&#30340;&#22870;&#21169;&#20844;&#24179;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#22312;&#32447;&#20984;&#20248;&#21270;&#30340;&#26041;&#27861;&#33719;&#24471;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#24179;&#24615;&#22312;&#21508;&#31181;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#65288;&#20363;&#22914;&#36890;&#20449;&#32593;&#32476;&#12289;&#37329;&#34701;&#24066;&#22330;&#31561;&#65289;&#20013;&#37117;&#25198;&#28436;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;&#35768;&#22810;&#22810;&#26234;&#33021;&#20307;&#21160;&#24577;&#20132;&#20114;&#21487;&#20197;&#34987;&#35270;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#12290;&#23613;&#31649;&#29616;&#26377;&#30740;&#31350;&#19987;&#27880;&#20110;&#30740;&#31350;&#24050;&#30693;&#29615;&#22659;&#20013;&#30340;&#20844;&#24179;&#24615;&#65292;&#20294;&#26159;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#25506;&#32034;&#36825;&#31181;&#31995;&#32479;&#20013;&#30340;&#20844;&#24179;&#24615;&#20173;&#28982;&#26159;&#24320;&#25918;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#22810;&#26234;&#33021;&#20307;&#26377;&#38480;&#26102;&#38388;&#27573;&#24773;&#33410;MDPs&#20013;&#30340;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20844;&#24179;&#20989;&#25968;&#65292;&#23427;&#30830;&#20445;&#20195;&#29702;&#20043;&#38388;&#30340;&#22870;&#21169;&#20844;&#24179;&#65292;&#32780;&#19981;&#26159;&#26368;&#22823;&#21270;&#21508;&#20010;&#20195;&#29702;&#30340;&#20540;&#20989;&#25968;&#20043;&#21644;&#12290;&#30001;&#20110;&#24403;&#19981;&#26368;&#22823;&#21270;&#21333;&#20010;&#20540;&#20989;&#25968;&#20043;&#21644;&#26102;&#65292;&#32463;&#20856;&#30340;Bellman&#26041;&#31243;&#19981;&#20877;&#25104;&#31435;&#65292;&#22240;&#27492;&#25105;&#20204;&#19981;&#33021;&#20351;&#29992;&#20256;&#32479;&#30340;&#26041;&#27861;&#12290;&#30456;&#21453;&#65292;&#20026;&#20102;&#36827;&#34892;&#25506;&#32034;&#65292;&#25105;&#20204;&#32500;&#25252;&#26410;&#30693;&#29615;&#22659;&#30340;&#32622;&#20449;&#24230;&#30028;&#38480;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#20984;&#20248;&#21270;&#26041;&#27861;&#26469;&#33719;&#24471;&#21463;&#38480;&#20110;&#27492;&#32622;&#20449;&#33539;&#22260;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fairness plays a crucial role in various multi-agent systems (e.g., communication networks, financial markets, etc.). Many multi-agent dynamical interactions can be cast as Markov Decision Processes (MDPs). While existing research has focused on studying fairness in known environments, the exploration of fairness in such systems for unknown environments remains open. In this paper, we propose a Reinforcement Learning (RL) approach to achieve fairness in multi-agent finite-horizon episodic MDPs. Instead of maximizing the sum of individual agents' value functions, we introduce a fairness function that ensures equitable rewards across agents. Since the classical Bellman's equation does not hold when the sum of individual value functions is not maximized, we cannot use traditional approaches. Instead, in order to explore, we maintain a confidence bound of the unknown environment and then propose an online convex optimization based approach to obtain a policy constrained to this confidence 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#8220;&#24605;&#32500;&#20811;&#38534;&#8221;&#65292;&#36890;&#36807;&#23398;&#20064;&#20154;&#31867;&#30340;&#24605;&#32500;&#26469;&#35757;&#32451;AI&#20195;&#29702;&#65292;&#20197;&#22312;&#27867;&#21270;&#12289;&#25506;&#32034;&#12289;&#35268;&#21010;&#31561;&#33021;&#21147;&#26041;&#38754;&#23454;&#29616;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.00323</link><description>&lt;p&gt;
&#8220;&#24605;&#32500;&#20811;&#38534;&#65306;&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#24605;&#32500;&#23398;&#20064;&#24605;&#32771;&#24182;&#34892;&#21160;&#8221;&#12290;&#65288;arXiv:2306.00323v1 [cs.AI]&#65289;
&lt;/p&gt;
&lt;p&gt;
Thought Cloning: Learning to Think while Acting by Imitating Human Thinking. (arXiv:2306.00323v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00323
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#8220;&#24605;&#32500;&#20811;&#38534;&#8221;&#65292;&#36890;&#36807;&#23398;&#20064;&#20154;&#31867;&#30340;&#24605;&#32500;&#26469;&#35757;&#32451;AI&#20195;&#29702;&#65292;&#20197;&#22312;&#27867;&#21270;&#12289;&#25506;&#32034;&#12289;&#35268;&#21010;&#31561;&#33021;&#21147;&#26041;&#38754;&#23454;&#29616;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#20154;&#31867;&#24605;&#32500;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#65292;&#23427;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#38750;&#20961;&#30340;&#27867;&#21270;&#12289;&#25506;&#32034;&#12289;&#35268;&#21010;&#12289;&#37325;&#26032;&#35268;&#21010;&#21644;&#36866;&#24212;&#26032;&#24773;&#20917;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20195;&#29702;&#22312;&#36825;&#20123;&#33021;&#21147;&#20013;&#36828;&#26410;&#36798;&#21040;&#20154;&#31867;&#27700;&#24179;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#20551;&#35774;&#20854;&#20013;&#19968;&#20010;&#35748;&#30693;&#32570;&#38519;&#30340;&#21407;&#22240;&#26159;&#20182;&#20204;&#32570;&#20047;&#20351;&#29992;&#35821;&#35328;&#24605;&#32771;&#25152;&#24102;&#26469;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#35748;&#20026;&#36890;&#36807;&#35757;&#32451;AI&#20195;&#29702;&#20154;&#20687;&#20154;&#31867;&#19968;&#26679;&#24605;&#32771;&#65292;&#21487;&#20197;&#25913;&#21892;&#20854;&#24615;&#33021;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#8220;&#24605;&#32500;&#20811;&#38534;&#8221;&#65292;&#20854;&#24819;&#27861;&#19981;&#20165;&#26159;&#20811;&#38534;&#20154;&#31867;&#31034;&#33539;&#32773;&#30340;&#34892;&#20026;&#65292;&#32780;&#19988;&#36824;&#21253;&#25324;&#20154;&#31867;&#22312;&#25191;&#34892;&#36825;&#20123;&#34892;&#20026;&#26102;&#25152;&#20135;&#29983;&#30340;&#24819;&#27861;&#12290;&#34429;&#28982;&#25105;&#20204;&#24076;&#26395;&#8220;&#24605;&#32500;&#20811;&#38534;&#8221;&#22312;&#22788;&#29702;&#32593;&#32476;&#35268;&#27169;&#30340;&#20154;&#31867;&#24605;&#32500;&#21644;&#34892;&#20026;&#25968;&#25454;&#26102;&#33021;&#22815;&#21457;&#25381;&#20986;&#33394;&#65288;&#20363;&#22914;&#65292;&#24102;&#26377;&#21095;&#26412;&#30340;&#22312;&#32447;&#35270;&#39057;&#65289;&#65292;&#20294;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22312;&#24605;&#32771;&#21644;&#34892;&#21160;&#25968;&#25454;&#20026;&#21512;&#25104;&#29983;&#25104;&#30340;&#39046;&#22495;&#30340;&#23454;&#39564;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#8220;&#24605;&#32500;&#20811;&#38534;&#8221;&#23398;&#20064;&#36895;&#24230;&#27604;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#24555;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language is often considered a key aspect of human thinking, providing us with exceptional abilities to generalize, explore, plan, replan, and adapt to new situations. However, Reinforcement Learning (RL) agents are far from human-level performance in any of these abilities. We hypothesize one reason for such cognitive deficiencies is that they lack the benefits of thinking in language and that we can improve AI agents by training them to think like humans do. We introduce a novel Imitation Learning framework, Thought Cloning, where the idea is to not just clone the behaviors of human demonstrators, but also the thoughts humans have as they perform these behaviors. While we expect Thought Cloning to truly shine at scale on internet-sized datasets of humans thinking out loud while acting (e.g. online videos with transcripts), here we conduct experiments in a domain where the thinking and action data are synthetically generated. Results reveal that Thought Cloning learns much faster than
&lt;/p&gt;</description></item><item><title>HUBL&#26159;&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#20540;&#20989;&#25968;&#22238;&#28335;&#30340;&#24191;&#27867;&#31867;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#31616;&#21333;&#24615;&#33021;&#25552;&#21319;&#25216;&#26415;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;HUBL&#21487;&#36890;&#36807;&#35843;&#25972;&#22870;&#21169;&#21644;&#25240;&#25187;&#22240;&#23376;&#26469;&#31616;&#21333;&#23454;&#29616;&#65292;&#24182;&#19988;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;HUBL&#33021;&#22815;&#22312;&#25552;&#39640;&#24615;&#33021;&#30340;&#21516;&#26102;&#38477;&#20302;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.00321</link><description>&lt;p&gt;
&#36890;&#36807;&#21551;&#21457;&#24335;&#31574;&#30053;&#28151;&#21512;&#25913;&#36827;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Improving Offline RL by Blending Heuristics. (arXiv:2306.00321v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00321
&lt;/p&gt;
&lt;p&gt;
HUBL&#26159;&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#20540;&#20989;&#25968;&#22238;&#28335;&#30340;&#24191;&#27867;&#31867;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#31616;&#21333;&#24615;&#33021;&#25552;&#21319;&#25216;&#26415;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;HUBL&#21487;&#36890;&#36807;&#35843;&#25972;&#22870;&#21169;&#21644;&#25240;&#25187;&#22240;&#23376;&#26469;&#31616;&#21333;&#23454;&#29616;&#65292;&#24182;&#19988;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;HUBL&#33021;&#22815;&#22312;&#25552;&#39640;&#24615;&#33021;&#30340;&#21516;&#26102;&#38477;&#20302;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#21551;&#21457;&#24335;&#31574;&#30053;&#28151;&#21512;&#65288;HUBL&#65289;&#65292;&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#20540;&#20989;&#25968;&#22238;&#28335;&#30340;&#24191;&#27867;&#31867;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#31616;&#21333;&#24615;&#33021;&#25552;&#21319;&#25216;&#26415;&#12290;HUBL&#20462;&#25913;&#20102;&#36825;&#20123;&#31639;&#27861;&#20013;&#20351;&#29992;&#30340;Bellman&#25805;&#20316;&#31526;&#65292;&#37096;&#20998;&#29992;&#21551;&#21457;&#24335;&#30340;&#33945;&#29305;&#21345;&#32599;&#22238;&#25253;&#26367;&#25442;&#20102;&#20540;&#20989;&#25968;&#22238;&#28335;&#12290;&#23545;&#20110;&#22238;&#25253;&#26356;&#39640;&#30340;&#36712;&#36857;&#65292;HUBL&#26356;&#22810;&#22320;&#20381;&#36182;&#20110;&#21551;&#21457;&#24335;&#65292;&#36739;&#23569;&#20381;&#36182;&#20110;&#20540;&#20989;&#25968;&#22238;&#28335;&#12290;&#21542;&#21017;&#65292;&#23427;&#20250;&#26356;&#21152;&#20506;&#37325;&#20110;&#20540;&#20989;&#25968;&#22238;&#28335;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20010;&#24819;&#27861;&#21487;&#20197;&#36890;&#36807;&#35843;&#25972;&#22870;&#21169;&#21644;&#25240;&#25187;&#22240;&#23376;&#26469;&#31616;&#21333;&#23454;&#29616;&#65292;&#20351;HUBL&#21487;&#36866;&#29992;&#20110;&#29616;&#26377;&#30340;&#35768;&#22810;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;HUBL&#38477;&#20302;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#22797;&#26434;&#24230;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20854;&#26377;&#38480;&#26679;&#26412;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20174;&#23454;&#35777;&#26041;&#38754;&#34920;&#26126;&#20102;HUBL&#23545;&#22235;&#31181;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#22238;&#28335;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#31574;&#30053;&#36136;&#37327;&#36827;&#34892;&#20102;&#25913;&#36827;&#65292;&#24179;&#22343;&#25552;&#39640;&#20102;9%&#30340;&#25928;&#26524;&#65292;&#22312;D4RL&#21644;Meta-Wo&#30340;27&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#31283;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Heuristic Blending (HUBL), a simple performance-improving technique for a broad class of offline RL algorithms based on value bootstrapping. HUBL modifies Bellman operators used in these algorithms, partially replacing the bootstrapped values with Monte-Carlo returns as heuristics. For trajectories with higher returns, HUBL relies more on heuristics and less on bootstrapping; otherwise, it leans more heavily on bootstrapping. We show that this idea can be easily implemented by relabeling the offline datasets with adjusted rewards and discount factors, making HUBL readily usable by many existing offline RL implementations. We theoretically prove that HUBL reduces offline RL's complexity and thus improves its finite-sample performance. Furthermore, we empirically demonstrate that HUBL consistently improves the policy quality of four state-of-the-art bootstrapping-based offline RL algorithms (ATAC, CQL, TD3+BC, and IQL), by 9% on average over 27 datasets of the D4RL and Meta-Wo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20803;&#32032;&#38500;&#27861;&#30340;&#21487;&#23398;&#20064;&#33293;&#20837;&#26426;&#21046;FlexRound&#65292;&#20351;&#24471;&#21518;&#35757;&#32451;&#37327;&#21270;&#26102;&#26356;&#22909;&#22320;&#37325;&#26500;&#27599;&#20010;&#23618;&#25110;&#22359;&#30340;&#36755;&#20986;&#65292;&#24182;&#19988;&#33021;&#22815;&#23398;&#20064;&#20844;&#20849;&#37327;&#21270;&#32593;&#26684;&#22823;&#23567;&#20197;&#21450;&#27599;&#20010;&#39044;&#35757;&#32451;&#26435;&#37325;&#30340;&#19981;&#21516;&#27604;&#20363;&#23610;&#12290;</title><link>http://arxiv.org/abs/2306.00317</link><description>&lt;p&gt;
&#22522;&#20110;&#20803;&#32032;&#38500;&#27861;&#30340;&#21487;&#23398;&#20064;&#33293;&#20837;&#29992;&#20110;&#21518;&#35757;&#32451;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
FlexRound: Learnable Rounding based on Element-wise Division for Post-Training Quantization. (arXiv:2306.00317v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00317
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20803;&#32032;&#38500;&#27861;&#30340;&#21487;&#23398;&#20064;&#33293;&#20837;&#26426;&#21046;FlexRound&#65292;&#20351;&#24471;&#21518;&#35757;&#32451;&#37327;&#21270;&#26102;&#26356;&#22909;&#22320;&#37325;&#26500;&#27599;&#20010;&#23618;&#25110;&#22359;&#30340;&#36755;&#20986;&#65292;&#24182;&#19988;&#33021;&#22815;&#23398;&#20064;&#20844;&#20849;&#37327;&#21270;&#32593;&#26684;&#22823;&#23567;&#20197;&#21450;&#27599;&#20010;&#39044;&#35757;&#32451;&#26435;&#37325;&#30340;&#19981;&#21516;&#27604;&#20363;&#23610;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;PTQ&#65289;&#24050;&#32463;&#22312;&#36164;&#28304;&#26377;&#38480;&#35774;&#22791;&#19978;&#37096;&#32626;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#38754;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#22240;&#20026;&#19982;&#37327;&#21270;&#24863;&#30693;&#22521;&#35757;&#19981;&#21516;&#65292;&#23436;&#20840;&#19981;&#38656;&#35201;&#20840;&#38754;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#25110;&#31471;&#21040;&#31471;&#22521;&#35757;&#12290;&#22240;&#20026;&#22522;&#20110;&#37325;&#26500;&#27599;&#20010;&#23618;&#25110;&#22359;&#36755;&#20986;&#30340;PTQ&#26041;&#26696;&#25928;&#26524;&#26174;&#30528;&#20197;&#22686;&#24378;&#37327;&#21270;&#27169;&#22411;&#24615;&#33021;&#65292;&#25152;&#20197;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#24320;&#21457;&#20102;&#31639;&#27861;&#26469;&#35774;&#35745;&#21644;&#23398;&#20064;&#19968;&#31181;&#26032;&#30340;&#26435;&#37325;&#33293;&#20837;&#26041;&#26696;&#65292;&#20197;&#26356;&#22909;&#22320;&#37325;&#26500;&#27599;&#20010;&#23618;&#25110;&#22359;&#30340;&#36755;&#20986;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26032;&#30340;PTQ&#26435;&#37325;&#33293;&#20837;&#26426;&#21046;&#65292;&#21517;&#20026;FlexRound&#65292;&#20854;&#22522;&#20110;&#20803;&#32032;&#38500;&#27861;&#32780;&#19981;&#26159;&#20856;&#22411;&#30340;&#20803;&#32032;&#21152;&#27861;&#65292;&#20174;&#32780;&#20351;FlexRound&#33021;&#22815;&#21516;&#26102;&#23398;&#20064;&#20844;&#20849;&#37327;&#21270;&#32593;&#26684;&#22823;&#23567;&#20197;&#21450;&#27599;&#20010;&#39044;&#35757;&#32451;&#26435;&#37325;&#30340;&#19981;&#21516;&#27604;&#20363;&#23610;&#12290;&#30001;&#20110;&#20803;&#32032;&#38500;&#27861;&#20135;&#29983;&#30340;&#23548;&#25968;&#30340;&#20114;&#34917;&#35268;&#21017;&#65292;FlexRound&#22312;&#26356;&#26032;&#20854;&#30456;&#20851;&#39044;&#35757;&#32451;&#26435;&#37325;&#26102;&#22825;&#29983;&#33021;&#22815;&#21033;&#29992;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
Post-training quantization (PTQ) has been gaining popularity for the deployment of deep neural networks on resource-limited devices since unlike quantization-aware training, neither a full training dataset nor end-to-end training is required at all. As PTQ schemes based on reconstructing each layer or block output turn out to be effective to enhance quantized model performance, recent works have developed algorithms to devise and learn a new weight-rounding scheme so as to better reconstruct each layer or block output. In this work, we propose a simple yet effective new weight-rounding mechanism for PTQ, coined FlexRound, based on element-wise division instead of typical element-wise addition such that FlexRound enables jointly learning a common quantization grid size as well as a different scale for each pre-trained weight. Thanks to the reciprocal rule of derivatives induced by element-wise division, FlexRound is inherently able to exploit pre-trained weights when updating their corr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#19968;&#33268;&#24615;&#20559;&#24046;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#20010;&#31616;&#21333;&#12289;&#30452;&#35266;&#30340;&#26465;&#20214;&#25512;&#23548;&#20986;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#20998;&#24067;&#36716;&#31227;&#19979;&#30340;(&#20960;&#20046;)&#21487;&#35777;&#26126;&#35823;&#24046;&#30028;&#38480;&#65292;&#30456;&#27604;&#20110;$\mathcal{H}\Delta\mathcal{H}$-divergence&#26356;&#32039;&#23494;&#12289;&#26356;&#26131;&#35780;&#20215;&#12290;</title><link>http://arxiv.org/abs/2306.00312</link><description>&lt;p&gt;
&#36890;&#36807;&#19981;&#19968;&#33268;&#24615;&#20559;&#24046;&#25512;&#23548;&#20986;&#20998;&#24067;&#36716;&#31227;&#19979;&#30340;(&#20960;&#20046;)&#21487;&#35777;&#26126;&#35823;&#24046;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
(Almost) Provable Error Bounds Under Distribution Shift via Disagreement Discrepancy. (arXiv:2306.00312v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00312
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#19968;&#33268;&#24615;&#20559;&#24046;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#20010;&#31616;&#21333;&#12289;&#30452;&#35266;&#30340;&#26465;&#20214;&#25512;&#23548;&#20986;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#20998;&#24067;&#36716;&#31227;&#19979;&#30340;(&#20960;&#20046;)&#21487;&#35777;&#26126;&#35823;&#24046;&#30028;&#38480;&#65292;&#30456;&#27604;&#20110;$\mathcal{H}\Delta\mathcal{H}$-divergence&#26356;&#32039;&#23494;&#12289;&#26356;&#26131;&#35780;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#26410;&#26631;&#35760;&#30340;&#27979;&#35797;&#25968;&#25454;&#65292;&#25512;&#23548;&#20986;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#20998;&#24067;&#36716;&#25442;&#19979;&#35823;&#24046;&#30340;(&#20960;&#20046;)&#20445;&#35777;&#19978;&#38480;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#38656;&#35201;&#19968;&#20010;&#31616;&#21333;&#12289;&#30452;&#35266;&#30340;&#26465;&#20214;&#65292;&#30001;&#20808;&#21069;&#30340;&#32463;&#39564;&#30740;&#31350;&#24456;&#22909;&#22320;&#35777;&#26126;&#19988;&#22312;&#23454;&#38469;&#25805;&#20316;&#20013;&#26377;&#25928;&#29575;&#22320;&#28385;&#36275;&#65292;&#32780;&#19988;&#30456;&#27604;&#20110;$\mathcal{H}\Delta\mathcal{H}$-divergence&#26356;&#23481;&#26131;&#35780;&#20215;&#19988;&#20445;&#35777;&#26356;&#32039;&#23494;&#12289;&#26356;&#19981;&#34394;&#20266;&#12290;
&lt;/p&gt;
&lt;p&gt;
We derive an (almost) guaranteed upper bound on the error of deep neural networks under distribution shift using unlabeled test data. Prior methods either give bounds that are vacuous in practice or give estimates that are accurate on average but heavily underestimate error for a sizeable fraction of shifts. In particular, the latter only give guarantees based on complex continuous measures such as test calibration -- which cannot be identified without labels -- and are therefore unreliable. Instead, our bound requires a simple, intuitive condition which is well justified by prior empirical works and holds in practice effectively 100% of the time. The bound is inspired by $\mathcal{H}\Delta\mathcal{H}$-divergence but is easier to evaluate and substantially tighter, consistently providing non-vacuous guarantees. Estimating the bound requires optimizing one multiclass classifier to disagree with another, for which some prior works have used sub-optimal proxy losses; we devise a "disagree
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#26041;&#27861;&#65292;&#20174;&#25991;&#26412;&#25551;&#36848;&#21644;&#19978;&#19979;&#25991;&#20013;&#29983;&#25104;&#23383;&#24149;&#65292;&#32780;&#19981;&#30452;&#25509;&#22788;&#29702;&#22270;&#20687;&#12290;&#22312;CIDEr&#25351;&#26631;&#19978;&#65292;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#25991;&#26412;&#23545;&#40784;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.00301</link><description>&lt;p&gt;
CapText: &#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22270;&#20687;&#20869;&#23481;&#21644;&#25551;&#36848;&#29983;&#25104;&#23383;&#24149;
&lt;/p&gt;
&lt;p&gt;
CapText: Large Language Model-based Caption Generation From Image Context and Description. (arXiv:2306.00301v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00301
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#26041;&#27861;&#65292;&#20174;&#25991;&#26412;&#25551;&#36848;&#21644;&#19978;&#19979;&#25991;&#20013;&#29983;&#25104;&#23383;&#24149;&#65292;&#32780;&#19981;&#30452;&#25509;&#22788;&#29702;&#22270;&#20687;&#12290;&#22312;CIDEr&#25351;&#26631;&#19978;&#65292;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#25991;&#26412;&#23545;&#40784;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#22270;&#20687;&#21040;&#25991;&#26412;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#38590;&#20197;&#29992;&#20110;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#65292;&#22240;&#20026;&#20256;&#32479;&#30340;&#22270;&#29255;&#23383;&#24149;&#24448;&#24448;&#26159;&#19982;&#22270;&#20687;&#30456;&#20851;&#30340;&#65292;&#24182;&#19988;&#25552;&#20379;&#26377;&#20851;&#22270;&#20687;&#30340;&#34917;&#20805;&#20449;&#24687;&#65292;&#32780;&#27169;&#22411;&#24448;&#24448;&#29983;&#25104;&#25551;&#36848;&#22270;&#20687;&#35270;&#35273;&#29305;&#24449;&#30340;&#8220;&#25551;&#36848;&#8221;&#12290;&#22312;&#23383;&#24149;&#29983;&#25104;&#26041;&#38754;&#30340;&#30740;&#31350;&#24050;&#25506;&#32034;&#20102;&#20351;&#29992;&#27169;&#22411;&#22312;&#25552;&#20379;&#23545;&#24212;&#30340;&#25551;&#36848;&#25110;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#23383;&#24149;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#29616;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#25991;&#26412;&#25551;&#36848;&#21644;&#19978;&#19979;&#25991;&#20013;&#29983;&#25104;&#23383;&#24149;&#65292;&#32780;&#19981;&#30452;&#25509;&#22788;&#29702;&#22270;&#20687;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#24494;&#35843;&#21518;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312; CIDEr &#25351;&#26631;&#19978;&#32988;&#36807;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#25991;&#26412;&#23545;&#40784;&#27169;&#22411;&#65292;&#22914; OSCAR-VinVL&#12290;
&lt;/p&gt;
&lt;p&gt;
While deep-learning models have been shown to perform well on image-to-text datasets, it is difficult to use them in practice for captioning images. This is because \textit{captions} traditionally tend to be context-dependent and offer complementary information about an image, while models tend to produce \textit{descriptions} that describe the visual features of the image. Prior research in caption generation has explored the use of models that generate captions when provided with the images alongside their respective descriptions or contexts. We propose and evaluate a new approach, which leverages existing large language models to generate captions from textual descriptions and context alone, without ever processing the image directly. We demonstrate that after fine-tuning, our approach outperforms current state-of-the-art image-text alignment models like OSCAR-VinVL on this task on the CIDEr metric.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#32447;&#24615;transformers&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#65292;&#35777;&#26126;&#20102;&#23545;&#20110;&#19968;&#20010;&#27880;&#24847;&#21147;&#23618;&#65292;transformers&#33021;&#22815;&#23454;&#29616;&#19968;&#27425;&#39044;&#22788;&#29702;&#26799;&#24230;&#19979;&#38477;&#65292;&#32780;&#23545;&#20110;&#19968;&#20010;$k$&#20010;&#27880;&#24847;&#21147;&#23618;&#30340;transformer&#65292;&#23427;&#21487;&#20197;&#23454;&#29616;&#22810;&#27425;&#39044;&#22788;&#29702;&#26799;&#24230;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2306.00297</link><description>&lt;p&gt;
Transformers&#23398;&#20064;&#23454;&#29616;&#39044;&#22788;&#29702;&#26799;&#24230;&#19979;&#38477;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Transformers learn to implement preconditioned gradient descent for in-context learning. (arXiv:2306.00297v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00297
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#32447;&#24615;transformers&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#65292;&#35777;&#26126;&#20102;&#23545;&#20110;&#19968;&#20010;&#27880;&#24847;&#21147;&#23618;&#65292;transformers&#33021;&#22815;&#23454;&#29616;&#19968;&#27425;&#39044;&#22788;&#29702;&#26799;&#24230;&#19979;&#38477;&#65292;&#32780;&#23545;&#20110;&#19968;&#20010;$k$&#20010;&#27880;&#24847;&#21147;&#23618;&#30340;transformer&#65292;&#23427;&#21487;&#20197;&#23454;&#29616;&#22810;&#27425;&#39044;&#22788;&#29702;&#26799;&#24230;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;transformers&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#30340;&#26174;&#33879;&#33021;&#21147;&#30340;&#39537;&#21160;&#65292;&#19968;&#20123;&#30740;&#31350;&#34920;&#26126;transformers&#21487;&#20197;&#23454;&#29616;&#20687;&#26799;&#24230;&#19979;&#38477;&#36825;&#26679;&#30340;&#31639;&#27861;&#12290;&#36890;&#36807;&#31934;&#24515;&#30340;&#26435;&#37325;&#26500;&#36896;&#65292;&#36825;&#20123;&#30740;&#31350;&#34920;&#26126;&#22810;&#23618;transformers&#20855;&#26377;&#36275;&#22815;&#30340;&#34920;&#36798;&#33021;&#21147;&#26469;&#27169;&#25311;&#26799;&#24230;&#19979;&#38477;&#36845;&#20195;&#12290;&#36229;&#36234;&#34920;&#36798;&#33021;&#21147;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#38382;&#65306;transformers&#33021;&#21542;&#36890;&#36807;&#22312;&#38543;&#26426;&#38382;&#39064;&#23454;&#20363;&#19978;&#35757;&#32451;&#26469;&#23398;&#20064;&#23454;&#29616;&#36825;&#26679;&#30340;&#31639;&#27861;&#65311;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36890;&#36807;&#23545;&#32447;&#24615;&#22238;&#24402;&#30340;&#38543;&#26426;&#23454;&#20363;&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#20204;&#31532;&#19968;&#27425;&#22312;&#36825;&#20010;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#29702;&#35770;&#36827;&#23637;&#65292;&#36890;&#36807;&#23545;&#32447;&#24615;transformers&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#20998;&#26512;&#12290;&#23545;&#20110;&#19968;&#20010;&#27880;&#24847;&#21147;&#23618;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35757;&#32451;&#30446;&#26631;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#23454;&#29616;&#20102;&#19968;&#27425;&#39044;&#22788;&#29702;&#26799;&#24230;&#19979;&#38477;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#39044;&#22788;&#29702;&#30697;&#38453;&#19981;&#20165;&#36866;&#24212;&#36755;&#20837;&#20998;&#24067;&#65292;&#32780;&#19988;&#36824;&#36866;&#24212;&#20110;&#25968;&#25454;&#19981;&#20805;&#20998;&#24341;&#36215;&#30340;&#26041;&#24046;&#12290;&#23545;&#20110;&#19968;&#20010;&#20855;&#26377;$k$&#20010;&#27880;&#24847;&#21147;&#23618;&#30340;transformer&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#23450;&#26465;&#20214;&#19979;&#23427;&#21487;&#20197;&#23454;&#29616;&#22810;&#27425;&#39044;&#22788;&#29702;&#26799;&#24230;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by the striking ability of transformers for in-context learning, several works demonstrate that transformers can implement algorithms like gradient descent. By a careful construction of weights, these works show that multiple layers of transformers are expressive enough to simulate gradient descent iterations. Going beyond the question of expressivity, we ask: Can transformers learn to implement such algorithms by training over random problem instances? To our knowledge, we make the first theoretical progress toward this question via analysis of the loss landscape for linear transformers trained over random instances of linear regression. For a single attention layer, we prove the global minimum of the training objective implements a single iteration of preconditioned gradient descent. Notably, the preconditioning matrix not only adapts to the input distribution but also to the variance induced by data inadequacy. For a transformer with $k$ attention layers, we prove certain 
&lt;/p&gt;</description></item><item><title>EMOTE&#26159;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#24314;&#27169;&#26550;&#26500;&#65292;&#29992;&#20110;&#27169;&#25311;&#21478;&#19968;&#20010;&#26234;&#33021;&#20307;&#30340;&#21160;&#20316;&#20215;&#20540;&#20989;&#25968;&#65292;&#36890;&#36807;&#20849;&#24773;&#30340;&#24819;&#35937;&#32593;&#32476;&#23558;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#35266;&#23519;&#29366;&#24577;&#36716;&#25442;&#25104;&#21487;&#35299;&#37322;&#30340;&#8220;&#20849;&#24773;&#29366;&#24577;&#8221;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#26234;&#33021;&#20307;&#28216;&#25103;&#20013;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.00295</link><description>&lt;p&gt;
&#27169;&#25311;&#20849;&#24773;&#36807;&#31243;&#30340;&#21487;&#35299;&#37322;&#24314;&#27169;&#26550;&#26500;EMOTE
&lt;/p&gt;
&lt;p&gt;
EMOTE: An Explainable architecture for Modelling the Other Through Empathy. (arXiv:2306.00295v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00295
&lt;/p&gt;
&lt;p&gt;
EMOTE&#26159;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#24314;&#27169;&#26550;&#26500;&#65292;&#29992;&#20110;&#27169;&#25311;&#21478;&#19968;&#20010;&#26234;&#33021;&#20307;&#30340;&#21160;&#20316;&#20215;&#20540;&#20989;&#25968;&#65292;&#36890;&#36807;&#20849;&#24773;&#30340;&#24819;&#35937;&#32593;&#32476;&#23558;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#35266;&#23519;&#29366;&#24577;&#36716;&#25442;&#25104;&#21487;&#35299;&#37322;&#30340;&#8220;&#20849;&#24773;&#29366;&#24577;&#8221;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#26234;&#33021;&#20307;&#28216;&#25103;&#20013;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#24120;&#21487;&#20197;&#20551;&#35774;&#20182;&#20154;&#19982;&#25105;&#20204;&#26377;&#31867;&#20284;&#30340;&#30446;&#26631;&#12290;&#36825;&#31181;&#20551;&#35774;&#26377;&#26102;&#20063;&#36866;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#28216;&#25103;&#65292;&#20363;&#22914;&#65292;Agent 1&#23545;&#32511;&#33394;&#39063;&#31890;&#30340;&#21560;&#24341;&#31867;&#27604;&#20110;Agent 2&#23545;&#32418;&#33394;&#39063;&#31890;&#30340;&#21560;&#24341;&#12290;&#36825;&#31181;&#8220;&#31867;&#27604;&#8221;&#20551;&#35774;&#19982;&#20849;&#24773;&#35748;&#30693;&#36807;&#31243;&#23494;&#20999;&#30456;&#20851;&#12290;&#21463;&#21040;&#20849;&#24773;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31616;&#21333;&#19988;&#21487;&#35299;&#37322;&#30340;&#24314;&#27169;&#26550;&#26500;&#65292;&#29992;&#20110;&#27169;&#25311;&#21478;&#19968;&#20010;&#26234;&#33021;&#20307;&#30340;&#21160;&#20316;&#20215;&#20540;&#20989;&#25968;&#12290;&#36825;&#28041;&#21450;&#23398;&#20064;&#19968;&#20010;&#8220;&#24819;&#35937;&#32593;&#32476;&#8221;&#65292;&#20197;&#36716;&#25442;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#35266;&#23519;&#29366;&#24577;&#65292;&#20174;&#32780;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#8220;&#20849;&#24773;&#29366;&#24577;&#8221;&#65292;&#24403;&#21576;&#29616;&#32473;&#23398;&#20064;&#26234;&#33021;&#20307;&#26102;&#65292;&#20250;&#20135;&#29983;&#27169;&#20223;&#20854;&#20182;&#26234;&#33021;&#20307;&#34892;&#20026;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#30001;&#21333;&#20010;&#23398;&#20064;&#26234;&#33021;&#20307;&#21644;&#26681;&#25454;&#22266;&#23450;&#31574;&#30053;&#34892;&#21160;&#30340;&#20854;&#20182;&#65288;&#29420;&#31435;&#65289;&#26234;&#33021;&#20307;&#32452;&#25104;&#30340;&#22810;&#26234;&#33021;&#20307;&#22330;&#26223;&#12290;&#35813;&#26550;&#26500;&#29305;&#21035;&#36866;&#29992;&#20110;&#20351;&#29992;&#22797;&#21512;&#20540;&#25110;&#22870;&#21169;&#20989;&#25968;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22810;&#26234;&#33021;&#20307;&#28216;&#25103;&#29615;&#22659;&#20013;&#20135;&#29983;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We can usually assume others have goals analogous to our own. This assumption can also, at times, be applied to multi-agent games - e.g. Agent 1's attraction to green pellets is analogous to Agent 2's attraction to red pellets. This "analogy" assumption is tied closely to the cognitive process known as empathy. Inspired by empathy, we design a simple and explainable architecture to model another agent's action-value function. This involves learning an "Imagination Network" to transform the other agent's observed state in order to produce a human-interpretable "empathetic state" which, when presented to the learning agent, produces behaviours that mimic the other agent. Our approach is applicable to multi-agent scenarios consisting of a single learning agent and other (independent) agents acting according to fixed policies. This architecture is particularly beneficial for (but not limited to) algorithms using a composite value or reward function. We show our method produces better perfo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36866;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#20219;&#21153;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#30340;&#26080;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#25628;&#32034;&#31639;&#27861;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#24615;&#33021;&#39044;&#27979;&#25351;&#26631;hidden covariance&#65292;&#21487;&#26174;&#33879;&#20248;&#21270;&#29616;&#26377;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2306.00288</link><description>&lt;p&gt;
&#26080;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#25628;&#32034;&#22312;RNN&#21644;Transformer&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Training-free Neural Architecture Search for RNNs and Transformers. (arXiv:2306.00288v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00288
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36866;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#20219;&#21153;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#30340;&#26080;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#25628;&#32034;&#31639;&#27861;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#24615;&#33021;&#39044;&#27979;&#25351;&#26631;hidden covariance&#65292;&#21487;&#26174;&#33879;&#20248;&#21270;&#29616;&#26377;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#25628;&#32034;(NAS)&#21487;&#20197;&#33258;&#21160;&#21019;&#24314;&#26032;&#30340;&#26377;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#20026;&#25163;&#21160;&#35774;&#35745;&#22797;&#26434;&#26550;&#26500;&#25552;&#20379;&#20102;&#19968;&#31181;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;NAS&#31639;&#27861;&#36895;&#24230;&#36739;&#24930;&#65292;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#21147;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#26550;&#26500;&#30340;&#26080;&#35757;&#32451;NAS&#25351;&#26631;&#65292;&#26497;&#22823;&#22320;&#21152;&#36895;&#20102;&#25628;&#32034;&#31639;&#27861;&#12290;&#26412;&#25991;&#30740;&#31350;&#38024;&#23545;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;(RNN)&#21644;BERT-based transformer&#26550;&#26500;&#30340;&#26080;&#35757;&#32451;NAS&#25351;&#26631;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;hidden covariance&#30340;&#26032;&#30340;&#26080;&#35757;&#32451;&#25351;&#26631;&#65292;&#21487;&#20197;&#39044;&#27979;&#35757;&#32451;&#21518;RNN&#26550;&#26500;&#30340;&#24615;&#33021;&#65292;&#24182;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26080;&#35757;&#32451;&#25351;&#26631;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21457;&#29616;&#30446;&#21069;&#30340;&#36716;&#25442;&#22120;&#25628;&#32034;&#31354;&#38388;&#33539;&#20363;
&lt;/p&gt;
&lt;p&gt;
Neural architecture search (NAS) has allowed for the automatic creation of new and effective neural network architectures, offering an alternative to the laborious process of manually designing complex architectures. However, traditional NAS algorithms are slow and require immense amounts of computing power. Recent research has investigated training-free NAS metrics for image classification architectures, drastically speeding up search algorithms. In this paper, we investigate training-free NAS metrics for recurrent neural network (RNN) and BERT-based transformer architectures, targeted towards language modeling tasks. First, we develop a new training-free metric, named hidden covariance, that predicts the trained performance of an RNN architecture and significantly outperforms existing training-free metrics. We experimentally evaluate the effectiveness of the hidden covariance metric on the NAS-Bench-NLP benchmark. Second, we find that the current search space paradigm for transformer
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#26696;&#20363;&#30740;&#31350;&#30340;&#26041;&#27861;&#65292;&#23558;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#65292;&#36890;&#36807;10&#20010;&#23398;&#20064;&#27169;&#22359;&#35206;&#30422;&#21508;&#31181;&#32593;&#32476;&#23433;&#20840;&#20027;&#39064;&#12290;&#35813;&#30740;&#31350;&#26088;&#22312;&#25552;&#20379;&#23454;&#36341;&#26426;&#20250;&#20197;&#35299;&#20915;&#23454;&#38469;&#23433;&#20840;&#38382;&#39064;&#65292;&#28608;&#21457;&#21644;&#40723;&#21169;&#23398;&#29983;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2306.00284</link><description>&lt;p&gt;
&#22522;&#20110;&#26696;&#20363;&#30740;&#31350;&#30340;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#22312;&#32593;&#32476;&#23433;&#20840;&#20013;&#30340;&#24212;&#29992;&#65306;&#21033;&#29992;&#37327;&#23376;&#25903;&#25345;&#21521;&#37327;&#26426;&#36827;&#34892;&#24694;&#24847;&#36719;&#20214;&#20998;&#31867;&#21644;&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
Case Study-Based Approach of Quantum Machine Learning in Cybersecurity: Quantum Support Vector Machine for Malware Classification and Protection. (arXiv:2306.00284v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#26696;&#20363;&#30740;&#31350;&#30340;&#26041;&#27861;&#65292;&#23558;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#65292;&#36890;&#36807;10&#20010;&#23398;&#20064;&#27169;&#22359;&#35206;&#30422;&#21508;&#31181;&#32593;&#32476;&#23433;&#20840;&#20027;&#39064;&#12290;&#35813;&#30740;&#31350;&#26088;&#22312;&#25552;&#20379;&#23454;&#36341;&#26426;&#20250;&#20197;&#35299;&#20915;&#23454;&#38469;&#23433;&#20840;&#38382;&#39064;&#65292;&#28608;&#21457;&#21644;&#40723;&#21169;&#23398;&#29983;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65288;QML&#65289;&#26159;&#21033;&#29992;&#37327;&#23376;&#35745;&#31639;&#25913;&#36827;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20197;&#35299;&#20915;&#22797;&#26434;&#23454;&#38469;&#38382;&#39064;&#30340;&#26032;&#20852;&#30740;&#31350;&#39046;&#22495;&#12290;QML&#26377;&#28508;&#21147;&#24212;&#23545;&#32593;&#32476;&#23433;&#20840;&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;&#30001;&#20110;QML&#30340;&#26032;&#39062;&#24615;&#21644;&#22797;&#26434;&#24615;&#26550;&#26500;&#65292;&#30446;&#21069;&#23578;&#26410;&#26126;&#30830;&#25552;&#20379;&#36164;&#28304;&#20197;&#20351;&#32593;&#32476;&#23433;&#20840;&#23398;&#20064;&#32773;&#20855;&#22791;&#20851;&#20110;&#36825;&#19968;&#26032;&#20852;&#25216;&#26415;&#30340;&#26377;&#25928;&#30693;&#35782;&#12290;&#26412;&#30740;&#31350;&#37319;&#29992;&#20197;&#23398;&#29983;&#20026;&#20013;&#24515;&#30340;&#26696;&#20363;&#30740;&#31350;&#23398;&#20064;&#26041;&#27861;&#65292;&#35774;&#35745;&#21644;&#24320;&#21457;&#20102;&#22522;&#20110;QML&#30340;&#21253;&#21547;10&#20010;&#23398;&#20064;&#27169;&#22359;&#65292;&#28085;&#30422;&#21508;&#31181;&#32593;&#32476;&#23433;&#20840;&#20027;&#39064;&#30340;&#35838;&#31243;&#12290;&#25105;&#20204;&#23558;QML&#30340;&#19968;&#20010;&#23376;&#20027;&#39064;&#24212;&#29992;&#20110;&#32593;&#32476;&#23433;&#20840;&#20027;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#23460;&#21069;&#12289;&#23454;&#39564;&#23460;&#21644;&#23454;&#39564;&#23460;&#21518;&#30340;&#27963;&#21160;&#65292;&#20026;&#23398;&#20064;&#32773;&#25552;&#20379;&#22312;&#35299;&#20915;&#29616;&#23454;&#23433;&#20840;&#38382;&#39064;&#26041;&#38754;&#23454;&#36341;QML&#30340;&#26426;&#20250;&#12290;&#20026;&#20102;&#22312;&#40723;&#21169;&#25152;&#26377;&#23398;&#29983;&#23398;&#20064;&#30340;&#23398;&#20064;&#29615;&#22659;&#20013;&#21560;&#24341;&#21644;&#28608;&#21169;&#23398;&#29983;&#65292;&#23454;&#39564;&#23460;&#21069;&#36824;&#25552;&#20379;&#20102;&#20851;&#20110;QML&#23376;&#20027;&#39064;&#21644;&#32593;&#32476;&#23433;&#20840;&#20027;&#39064;&#30340;&#31616;&#30701;&#20171;&#32461;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum machine learning (QML) is an emerging field of research that leverages quantum computing to improve the classical machine learning approach to solve complex real world problems. QML has the potential to address cybersecurity related challenges. Considering the novelty and complex architecture of QML, resources are not yet explicitly available that can pave cybersecurity learners to instill efficient knowledge of this emerging technology. In this research, we design and develop QML-based ten learning modules covering various cybersecurity topics by adopting student centering case-study based learning approach. We apply one subtopic of QML on a cybersecurity topic comprised of pre-lab, lab, and post-lab activities towards providing learners with hands-on QML experiences in solving real-world security problems. In order to engage and motivate students in a learning environment that encourages all students to learn, pre-lab offers a brief introduction to both the QML subtopic and c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32452;&#21512;&#21019;&#36896;&#24615;&#26041;&#27861;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#65292;&#21487;&#20197;&#20351;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#20998;&#24067;&#22806;&#27969;&#27966;&#19978;&#30340;&#24615;&#33021;&#24471;&#21040;&#25552;&#39640;&#65292;&#20197;&#20234;&#26391;&#27665;&#38388;&#38899;&#20048;&#20026;&#20363;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#26410;&#26469;&#21487;&#33021;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#23454;&#29616;&#20195;&#34920;&#24615;&#19981;&#36275;&#38899;&#20048;&#27969;&#27966;&#30340;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2306.00281</link><description>&lt;p&gt;
&#38754;&#21521;&#20195;&#34920;&#24615;&#19981;&#36275;&#38899;&#20048;&#29983;&#25104;&#30340;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Transfer Learning for Underrepresented Music Generation. (arXiv:2306.00281v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00281
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32452;&#21512;&#21019;&#36896;&#24615;&#26041;&#27861;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#65292;&#21487;&#20197;&#20351;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#20998;&#24067;&#22806;&#27969;&#27966;&#19978;&#30340;&#24615;&#33021;&#24471;&#21040;&#25552;&#39640;&#65292;&#20197;&#20234;&#26391;&#27665;&#38388;&#38899;&#20048;&#20026;&#20363;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#26410;&#26469;&#21487;&#33021;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#23454;&#29616;&#20195;&#34920;&#24615;&#19981;&#36275;&#38899;&#20048;&#27969;&#27966;&#30340;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#32452;&#21512;&#21019;&#36896;&#24615;&#26041;&#27861;&#26469;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#65292;&#20197;&#25552;&#39640;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#38899;&#20048;&#29983;&#25104;&#27169;&#22411;&#22312;&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#27969;&#27966;&#19978;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20197;&#20234;&#26391;&#27665;&#38388;&#38899;&#20048;&#20026;&#20363;&#65292;&#36825;&#26159; MusicVAE&#65292;&#19968;&#20010;&#22823;&#22411;&#29983;&#25104;&#38899;&#20048;&#27169;&#22411;&#65292;&#26080;&#27861;&#22788;&#29702;&#30340;OOD&#27969;&#27966;&#20043;&#19968;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#32452;&#21512;&#21019;&#36896;&#24615;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#23558;MusicVAE&#35843;&#25972;&#20026;&#20234;&#26391;&#27665;&#38388;&#38899;&#20048;&#25968;&#25454;&#38598;&#65292;&#36825;&#34920;&#26126;&#26410;&#26469;&#21487;&#20197;&#29983;&#25104;&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#38899;&#20048;&#27969;&#27966;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates a combinational creativity approach to transfer learning to improve the performance of deep neural network-based models for music generation on out-of-distribution (OOD) genres. We identify Iranian folk music as an example of such an OOD genre for MusicVAE, a large generative music model. We find that a combinational creativity transfer learning approach can efficiently adapt MusicVAE to an Iranian folk music dataset, indicating potential for generating underrepresented music genres in the future.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38754;&#21521;&#38750;&#22343;&#21248;&#21644;&#26102;&#21464;&#36890;&#20449;&#25925;&#38556;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedPBC&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#26469;&#20462;&#27491;FedAvg&#30340;&#20559;&#24046;&#65292;&#35813;&#31639;&#27861;&#26500;&#24314;&#20108;&#21449;&#26641;&#26469;&#20943;&#36731;&#38750;&#22343;&#21248;&#36890;&#20449;&#25925;&#38556;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;&#38750;&#20984;&#20219;&#21153;&#19978;&#23454;&#39564;&#32467;&#26524;&#34920;&#29616;&#20248;&#20110;FedAvg&#12290;</title><link>http://arxiv.org/abs/2306.00280</link><description>&lt;p&gt;
&#38754;&#21521;&#38750;&#22343;&#21248;&#21644;&#26102;&#21464;&#36890;&#20449;&#30340;FedAvg&#20559;&#24046;&#20462;&#27491;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Bias Correction of FedAvg over Nonuniform and Time-Varying Communications. (arXiv:2306.00280v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00280
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38754;&#21521;&#38750;&#22343;&#21248;&#21644;&#26102;&#21464;&#36890;&#20449;&#25925;&#38556;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedPBC&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#26469;&#20462;&#27491;FedAvg&#30340;&#20559;&#24046;&#65292;&#35813;&#31639;&#27861;&#26500;&#24314;&#20108;&#21449;&#26641;&#26469;&#20943;&#36731;&#38750;&#22343;&#21248;&#36890;&#20449;&#25925;&#38556;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;&#38750;&#20984;&#20219;&#21153;&#19978;&#23454;&#39564;&#32467;&#26524;&#34920;&#29616;&#20248;&#20110;FedAvg&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#20998;&#25955;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#36825;&#20010;&#26694;&#26550;&#19979;&#65292;&#21442;&#25968;&#26381;&#21153;&#22120;&#65288;PS&#65289;&#21644;&#22810;&#20010;&#23458;&#25143;&#31471;&#21512;&#20316;&#36890;&#36807;&#26368;&#23567;&#21270;&#20840;&#23616;&#30446;&#26631;&#26469;&#35757;&#32451;&#27169;&#22411;&#12290;&#36890;&#20449;&#24102;&#23485;&#26159;&#19968;&#31181;&#31232;&#32570;&#36164;&#28304;&#65307;&#22312;&#27599;&#36718;&#36845;&#20195;&#20013;&#65292;PS&#20165;&#20174;&#23458;&#25143;&#31471;&#30340;&#23376;&#38598;&#32858;&#21512;&#26356;&#26032;&#12290;&#26412;&#25991;&#20851;&#27880;&#30340;&#26159;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#36825;&#31181;&#38382;&#39064;&#23481;&#26131;&#21463;&#21040;PS&#21644;&#23458;&#25143;&#31471;&#20043;&#38388;&#19981;&#22343;&#21248;&#12289;&#26102;&#21464;&#30340;&#36890;&#20449;&#25925;&#38556;&#30340;&#24433;&#21709;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#27599;&#36718;$t$&#20013;&#65292;PS&#21644;&#23458;&#25143;&#31471;$i$&#20043;&#38388;&#30340;&#36830;&#25509;&#21482;&#26377;&#27010;&#29575;$p_i^t$&#26159;&#27963;&#36291;&#30340;&#65292;&#32780;&#36825;&#20010;&#27010;&#29575;&#26159;&#19981;&#30693;&#36947;&#30340;&#12290;&#24403;&#20449;&#36947;&#26465;&#20214;&#22312;&#23458;&#25143;&#31471;&#20043;&#38388;&#24322;&#26500;&#24182;&#19988;&#38543;&#26102;&#38388;&#21464;&#21270;&#26102;&#65292;&#36825;&#31181;&#24773;&#20917;&#20250;&#21457;&#29983;&#12290;&#25105;&#20204;&#21457;&#29616;&#24403;$p_i^t$&#38750;&#22343;&#21248;&#26102;&#65292;&#34987;&#24191;&#27867;&#37319;&#29992;&#30340;FL&#31639;&#27861;FedAvg&#26080;&#27861;&#26368;&#23567;&#21270;&#20840;&#23616;&#30446;&#26631;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedPBC&#30340;&#31616;&#21333;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#38750;&#22343;&#21248;&#36890;&#20449;&#25925;&#38556;&#30340;&#24433;&#21709;&#12290;FedPBC&#22312;&#23458;&#25143;&#31471;&#20043;&#38388;&#26500;&#24314;&#20108;&#21449;&#26641;&#65292;&#22312;&#24314;&#31435;&#25299;&#25169;&#32467;&#26500;&#20043;&#21069;&#25512;&#36831;&#26356;&#26032;&#30340;&#20256;&#36755;&#65292;&#24182;&#25429;&#25417;&#26412;&#22320;&#26356;&#26032;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#22312;&#28201;&#21644;&#20551;&#35774;&#19979;&#29702;&#35770;&#35777;&#26126;&#20102;FedPBC&#25910;&#25947;&#20110;&#38750;&#20984;&#38382;&#39064;&#30340;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;&#22312;&#38750;&#20984;&#20219;&#21153;&#65288;&#22914;&#22270;&#20687;&#20998;&#31867;&#21644;&#35821;&#35328;&#24314;&#27169;&#65289;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;FedPBC&#22312;&#38750;&#22343;&#21248;&#21644;&#26102;&#21464;&#30340;&#36890;&#20449;&#35774;&#32622;&#19979;&#20248;&#20110;FedAvg&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a decentralized learning framework wherein a parameter server (PS) and a collection of clients collaboratively train a model via minimizing a global objective. Communication bandwidth is a scarce resource; in each round, the PS aggregates the updates from a subset of clients only. In this paper, we focus on non-convex minimization that is vulnerable to non-uniform and time-varying communication failures between the PS and the clients. Specifically, in each round $t$, the link between the PS and client $i$ is active with probability $p_i^t$, which is $\textit{unknown}$ to both the PS and the clients. This arises when the channel conditions are heterogeneous across clients and are changing over time.  We show that when the $p_i^t$'s are not uniform, $\textit{Federated Average}$ (FedAvg) -- the most widely adopted FL algorithm -- fails to minimize the global objective. Observing this, we propose $\textit{Federated Postponed Broadcast}$ (FedPBC) which is a simple
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#20351;&#29992;Mixup&#35757;&#32451;&#20855;&#26377;&#21487;&#35777;&#23454;&#30340;&#30410;&#22788;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#22312;&#26356;&#21487;&#20998;&#31163;&#25968;&#25454;&#20998;&#24067;&#20013;&#23547;&#25214;&#26368;&#20339;&#20915;&#31574;&#36793;&#30028;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.00267</link><description>&lt;p&gt;
Mixup&#22312;&#23547;&#25214;&#26368;&#20339;&#20915;&#31574;&#36793;&#30028;&#20013;&#30340;&#21487;&#35777;&#23454;&#30410;&#22788;
&lt;/p&gt;
&lt;p&gt;
Provable Benefit of Mixup for Finding Optimal Decision Boundaries. (arXiv:2306.00267v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00267
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#20351;&#29992;Mixup&#35757;&#32451;&#20855;&#26377;&#21487;&#35777;&#23454;&#30340;&#30410;&#22788;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#22312;&#26356;&#21487;&#20998;&#31163;&#25968;&#25454;&#20998;&#24067;&#20013;&#23547;&#25214;&#26368;&#20339;&#20915;&#31574;&#36793;&#30028;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20687;Mixup&#36825;&#26679;&#30340;&#25104;&#23545;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#22914;&#20309;&#24433;&#21709;&#22312;&#20108;&#20803;&#32447;&#24615;&#20998;&#31867;&#38382;&#39064;&#20013;&#23547;&#25214;&#26368;&#20339;&#20915;&#31574;&#36793;&#30028;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#38024;&#23545;&#19968;&#31867;&#20855;&#26377;&#21487;&#20998;&#31163;&#24120;&#25968;$\kappa$&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#35757;&#32451;&#25439;&#22833;&#26368;&#20248;&#20998;&#31867;&#22120;&#19982;&#27979;&#35797;&#20934;&#30830;&#29575;&#26368;&#20248;&#20998;&#31867;&#22120;&#65288;&#21363;&#36125;&#21494;&#26031;&#26368;&#20248;&#20998;&#31867;&#22120;&#65289;&#20043;&#38388;&#30340;&#23545;&#40784;&#31243;&#24230;&#12290;&#23545;&#20110;&#27809;&#26377;&#22686;&#24378;&#30340;&#26222;&#36890;&#35757;&#32451;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#31181;&#26377;&#36259;&#30340;&#29616;&#35937;&#65292;&#31216;&#20026;&#21487;&#20998;&#31163;&#24615;&#30340;&#35781;&#21650;&#12290;&#38543;&#30528;&#25105;&#20204;&#22686;&#21152;$\kappa$&#20351;&#25968;&#25454;&#20998;&#24067;&#26356;&#21152;&#21487;&#20998;&#31163;&#65292;&#26222;&#36890;&#35757;&#32451;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#20250;&#22312;$\kappa$&#20013;&#21576;&#25351;&#25968;&#22686;&#38271;&#12290;&#20063;&#35768;&#26356;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#23545;&#20110;&#26356;&#21487;&#20998;&#31163;&#30340;&#25968;&#25454;&#20998;&#24067;&#32780;&#35328;&#65292;&#23547;&#25214;&#26368;&#20339;&#20915;&#31574;&#36793;&#30028;&#30340;&#20219;&#21153;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#38024;&#23545;Mixup&#35757;&#32451;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Mixup&#20943;&#36731;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#36890;&#36807;&#26174;&#33879;&#38477;&#20302;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#36866;&#29992;&#20110;Mixup&#32771;&#34385;&#30340;$n^2$&#25104;&#23545;&#22686;&#24378;&#25968;&#25454;&#28857;&#30340;&#26032;&#30340;&#38598;&#20013;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25552;&#20379;&#20102;&#20851;&#20110;Mixup&#30340;&#27867;&#21270;&#30410;&#22788;&#30340;&#21487;&#35777;&#20445;&#35777;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35299;Mixup&#20026;&#20160;&#20040;&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#33391;&#22909;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate how pair-wise data augmentation techniques like Mixup affect the sample complexity of finding optimal decision boundaries in a binary linear classification problem. For a family of data distributions with a separability constant $\kappa$, we analyze how well the optimal classifier in terms of training loss aligns with the optimal one in test accuracy (i.e., Bayes optimal classifier). For vanilla training without augmentation, we uncover an interesting phenomenon named the curse of separability. As we increase $\kappa$ to make the data distribution more separable, the sample complexity of vanilla training increases exponentially in $\kappa$; perhaps surprisingly, the task of finding optimal decision boundaries becomes harder for more separable distributions. For Mixup training, we show that Mixup mitigates this problem by significantly reducing the sample complexity. To this end, we develop new concentration results applicable to $n^2$ pair-wise augmented data points cons
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#31283;&#20581;&#33258;&#25105;&#35757;&#32451;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#20266;&#26631;&#31614;&#19981;&#20934;&#30830;&#21644;&#23436;&#20840;&#20934;&#30830;&#26102;&#20998;&#21035;&#37319;&#21462;&#19981;&#21516;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#23454;&#29616;&#26377;&#25928;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#22312;ImageNet&#21644;nuScenes&#25968;&#25454;&#38598;&#19978;&#22343;&#27604;&#26631;&#20934;&#33258;&#25105;&#35757;&#32451;&#24635;&#32467;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2306.00265</link><description>&lt;p&gt;
&#21452;&#37325;&#31283;&#20581;&#33258;&#25105;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Doubly Robust Self-Training. (arXiv:2306.00265v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00265
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#31283;&#20581;&#33258;&#25105;&#35757;&#32451;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#20266;&#26631;&#31614;&#19981;&#20934;&#30830;&#21644;&#23436;&#20840;&#20934;&#30830;&#26102;&#20998;&#21035;&#37319;&#21462;&#19981;&#21516;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#23454;&#29616;&#26377;&#25928;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#22312;ImageNet&#21644;nuScenes&#25968;&#25454;&#38598;&#19978;&#22343;&#27604;&#26631;&#20934;&#33258;&#25105;&#35757;&#32451;&#24635;&#32467;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#35757;&#32451;&#26159;&#35299;&#20915;&#21322;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#30340;&#19968;&#31181;&#37325;&#35201;&#25216;&#26415;&#12290;&#23427;&#36890;&#36807;&#29983;&#25104;&#20266;&#26631;&#31614;&#24182;&#23558;&#20854;&#19982;&#26377;&#38480;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#32467;&#21512;&#20351;&#29992;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#21033;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#12290;&#33258;&#25105;&#35757;&#32451;&#30340;&#26377;&#25928;&#24615;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#36825;&#20123;&#20266;&#26631;&#31614;&#30340;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#21452;&#37325;&#31283;&#20581;&#33258;&#25105;&#35757;&#32451;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#21322;&#30417;&#30563;&#31639;&#27861;&#65292;&#21487;&#20197;&#20445;&#35777;&#22312;&#20004;&#20010;&#26497;&#31471;&#20043;&#38388;&#24179;&#34913;&#12290;&#24403;&#20266;&#26631;&#31614;&#23436;&#20840;&#19981;&#27491;&#30830;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#34987;&#20943;&#23569;&#21040;&#20165;&#20351;&#29992;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#30456;&#21453;&#65292;&#24403;&#20266;&#26631;&#31614;&#23436;&#20840;&#20934;&#30830;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#21464;&#25104;&#21033;&#29992;&#25152;&#26377;&#20266;&#26631;&#31614;&#25968;&#25454;&#21644;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#36807;&#31243;&#65292;&#20174;&#32780;&#22686;&#21152;&#26377;&#25928;&#30340;&#26679;&#26412;&#37327;&#12290;&#36890;&#36807;&#22312;ImageNet&#22270;&#20687;&#20998;&#31867;&#21644;nuScenes&#33258;&#20027;&#39550;&#39542;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21452;&#37325;&#31283;&#20581;&#25439;&#22833;&#20248;&#20110;&#26631;&#20934;&#33258;&#25105;&#35757;&#32451;&#22522;&#32447;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-training is an important technique for solving semi-supervised learning problems. It leverages unlabeled data by generating pseudo-labels and combining them with a limited labeled dataset for training. The effectiveness of self-training heavily relies on the accuracy of these pseudo-labels. In this paper, we introduce doubly robust self-training, a novel semi-supervised algorithm that provably balances between two extremes. When the pseudo-labels are entirely incorrect, our method reduces to a training process solely using labeled data. Conversely, when the pseudo-labels are completely accurate, our method transforms into a training process utilizing all pseudo-labeled data and labeled data, thus increasing the effective sample size. Through empirical evaluations on both the ImageNet dataset for image classification and the nuScenes autonomous driving dataset for 3D object detection, we demonstrate the superiority of the doubly robust loss over the standard self-training baseline.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#36801;&#31227;&#24615;&#33021;&#65292;&#21457;&#29616;&#36866;&#24403;&#24494;&#35843;&#36801;&#31227;&#23398;&#20064;&#21487;&#20197;&#20195;&#26367;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#65292;&#36798;&#21040;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#27700;&#24179;&#65292;&#32780;&#19988;&#20943;&#23569;&#20102;&#35768;&#22810;&#25968;&#25454;&#38598;&#30340;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2306.00258</link><description>&lt;p&gt;
&#38754;&#21521;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#30340;&#22522;&#30784;&#27169;&#22411;&#65306;&#29305;&#24449;&#27604;&#36739;&#19982;&#36801;&#31227;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Towards Foundation Models for Scientific Machine Learning: Characterizing Scaling and Transfer Behavior. (arXiv:2306.00258v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00258
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#36801;&#31227;&#24615;&#33021;&#65292;&#21457;&#29616;&#36866;&#24403;&#24494;&#35843;&#36801;&#31227;&#23398;&#20064;&#21487;&#20197;&#20195;&#26367;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#65292;&#36798;&#21040;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#27700;&#24179;&#65292;&#32780;&#19988;&#20943;&#23569;&#20102;&#35768;&#22810;&#25968;&#25454;&#38598;&#30340;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#31561;&#21508;&#31181;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#24040;&#22823;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#22914;&#20309;&#22312;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#65288;SciML&#65289;&#24212;&#29992;&#20013;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#36801;&#31227;&#23398;&#20064;&#30340;&#19978;&#19979;&#25991;&#20013;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#36801;&#31227;&#29305;&#24615;&#65292;&#21253;&#25324;&#65288;i&#65289;&#39044;&#35757;&#32451;&#27169;&#22411;&#22823;&#23567;&#30340;&#32553;&#25918;&#65292;&#65288;ii&#65289;&#19979;&#28216;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#30340;&#32553;&#25918;&#65292;&#65288;iii&#65289;&#31995;&#32479;&#22320;&#23558;&#29289;&#29702;&#21442;&#25968;&#25512;&#20986;&#20998;&#24067;&#65292;&#20197;&#21450;&#65288;iv&#65289;&#22914;&#20309;&#23558;&#21333;&#20010;&#22312;&#19981;&#21516;&#29289;&#29702;&#38382;&#39064;&#30340;&#28151;&#21512;&#29289;&#19978;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#36866;&#24212;&#20110;&#21508;&#31181;&#19979;&#28216;&#24212;&#29992;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36866;&#24403;&#22320;&#24494;&#35843;&#36801;&#31227;&#23398;&#20064;&#21487;&#20197;&#24110;&#21161;&#22312;&#27604;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#23569;&#20960;&#20010;&#25968;&#37327;&#32423;&#30340;&#19979;&#28216;&#31034;&#20363;&#65288;&#29978;&#33267;&#21487;&#20197;&#26159;&#20998;&#24067;&#20043;&#22806;&#30340;&#19981;&#21516;&#20219;&#21153;&#65289;&#36798;&#21040;&#25152;&#38656;&#30340;&#20934;&#30830;&#24615;&#27700;&#24179;&#65292;&#24182;&#19988;&#22312;&#21508;&#31181;&#19979;&#28216;&#31034;&#20363;&#20013;&#20855;&#26377;&#19968;&#33268;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained machine learning (ML) models have shown great performance for a wide range of applications, in particular in natural language processing (NLP) and computer vision (CV). Here, we study how pre-training could be used for scientific machine learning (SciML) applications, specifically in the context of transfer learning. We study the transfer behavior of these models as (i) the pre-trained model size is scaled, (ii) the downstream training dataset size is scaled, (iii) the physics parameters are systematically pushed out of distribution, and (iv) how a single model pre-trained on a mixture of different physics problems can be adapted to various downstream applications. We find that-when fine-tuned appropriately-transfer learning can help reach desired accuracy levels with orders of magnitude fewer downstream examples (across different tasks that can even be out-of-distribution) than training from scratch, with consistent behavior across a wide range of downstream examples. We a
&lt;/p&gt;</description></item><item><title>DSGD-CECA &#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#20449;&#20248;&#21270;&#30340;&#31934;&#30830;&#20849;&#35782;&#31639;&#27861;&#65292;&#20351;&#20998;&#25955;&#24335; SGD &#33021;&#22815;&#22312;&#27809;&#26377;2&#30340;&#24130;&#27425;&#26041;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#25910;&#25947;&#65292;&#22312;&#22823;&#35268;&#27169;&#26102;&#26356;&#20026;&#23454;&#29992;&#12290;</title><link>http://arxiv.org/abs/2306.00256</link><description>&lt;p&gt;
DSGD-CECA: &#20855;&#26377;&#36890;&#20449;&#20248;&#21270;&#31934;&#30830;&#20849;&#35782;&#31639;&#27861;&#30340;&#20998;&#25955;&#24335; SGD
&lt;/p&gt;
&lt;p&gt;
DSGD-CECA: Decentralized SGD with Communication-Optimal Exact Consensus Algorithm. (arXiv:2306.00256v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00256
&lt;/p&gt;
&lt;p&gt;
DSGD-CECA &#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#20449;&#20248;&#21270;&#30340;&#31934;&#30830;&#20849;&#35782;&#31639;&#27861;&#65292;&#20351;&#20998;&#25955;&#24335; SGD &#33021;&#22815;&#22312;&#27809;&#26377;2&#30340;&#24130;&#27425;&#26041;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#25910;&#25947;&#65292;&#22312;&#22823;&#35268;&#27169;&#26102;&#26356;&#20026;&#23454;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#25955;&#24335; SGD &#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26041;&#27861;&#65292;&#23427;&#20351;&#22810;&#20010;&#20195;&#29702;&#33021;&#22815;&#21327;&#20316;&#22320;&#24182;&#34892;&#22320;&#35757;&#32451;&#27169;&#22411;&#12290;&#19982;&#20351;&#29992;&#20013;&#22830;&#21442;&#25968;&#26381;&#21153;&#22120;&#20174;&#25152;&#26377;&#20195;&#29702;&#25910;&#38598;&#26799;&#24230;&#19981;&#21516;&#65292;&#27599;&#20010;&#20195;&#29702;&#20445;&#30041;&#27169;&#22411;&#21442;&#25968;&#30340;&#21103;&#26412;&#24182;&#19982;&#23569;&#37327;&#20854;&#20182;&#20195;&#29702;&#36890;&#20449;&#20197;&#20132;&#25442;&#27169;&#22411;&#26356;&#26032;&#12290;&#20182;&#20204;&#30340;&#36890;&#20449;&#30001;&#36890;&#20449;&#25299;&#25169;&#21644;&#20843;&#21350;&#26435;&#37325;&#30697;&#38453;&#25511;&#21046;&#65292;&#20419;&#36827;&#27169;&#22411;&#26356;&#26032;&#30340;&#20132;&#25442;&#12290;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20351;&#29992;&#21160;&#24577;&#30340;&#25351;&#25968;-2&#25299;&#25169;&#32467;&#26500;&#65292;&#23454;&#29616;&#27604;&#29615;&#24418;&#12289;&#32593;&#26684;&#12289;&#29615;&#38754;&#21644;&#36229;&#31435;&#26041;&#25299;&#25169;&#32467;&#26500;&#26356;&#24555;&#30340;&#35757;&#32451;&#26102;&#38388;&#21644;&#26356;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#38656;&#35201;&#19968;&#20010;2&#30340;&#24130;&#27425;&#26041;&#25968;&#37327;&#30340;&#20195;&#29702;&#65292;&#36825;&#22312;&#22823;&#35268;&#27169;&#26102;&#19981;&#20999;&#23454;&#38469;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#28040;&#38500;&#20102;&#36825;&#20010;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#20855;&#26377;&#36890;&#20449;&#20248;&#21270;&#31934;&#30830;&#20849;&#35782;&#31639;&#27861;&#30340;&#20998;&#25955;&#24335; SGD-CECA&#65292;&#21033;&#29992;&#36825;&#31181;&#31639;&#27861;&#22312;&#20998;&#25955;&#24335;&#29615;&#22659;&#19979;&#30830;&#20445;&#25910;&#25947;&#24615;&#32780;&#19981;&#38656;&#35201;2&#30340;&#24130;&#27425;&#26041;&#38480;&#21046;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23454;&#29616;&#26356;&#23454;&#38469;&#30340;&#32593;&#32476;&#35268;&#27169;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#19982;&#20808;&#36827;&#25216;&#26415;&#30456;&#24403;&#30340;&#35757;&#32451;&#26102;&#38388;&#21644;&#27169;&#22411;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decentralized Stochastic Gradient Descent (SGD) is an emerging neural network training approach that enables multiple agents to train a model collaboratively and simultaneously. Rather than using a central parameter server to collect gradients from all the agents, each agent keeps a copy of the model parameters and communicates with a small number of other agents to exchange model updates. Their communication, governed by the communication topology and gossip weight matrices, facilitates the exchange of model updates. The state-of-the-art approach uses the dynamic one-peer exponential-2 topology, achieving faster training times and improved scalability than the ring, grid, torus, and hypercube topologies. However, this approach requires a power-of-2 number of agents, which is impractical at scale. In this paper, we remove this restriction and propose \underline{D}ecentralized \underline{SGD} with \underline{C}ommunication-optimal \underline{E}xact \underline{C}onsensus \underline{A}lgo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20687;&#32032;&#32423;&#21035;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#24314;&#31435;&#20102;&#19968;&#31181;&#27169;&#25311;&#20154;&#31867;&#27010;&#24565;&#30028;&#38754;&#21644;&#28151;&#21512;&#21160;&#20316;&#31354;&#38388;&#30340;&#20195;&#29702;&#65292;&#23454;&#29616;&#20102;&#22312;GUI&#25351;&#20196;&#36981;&#24490;&#20219;&#21153;&#30340;MiniWob++&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36234;&#20154;&#31867;&#24037;&#20316;&#32773;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2306.00245</link><description>&lt;p&gt;
&#20174;&#20687;&#32032;&#21040;&#29992;&#25143;&#30028;&#38754;&#25805;&#20316;&#65306;&#36890;&#36807;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;&#23398;&#20064;&#36981;&#24490;&#25351;&#20196;
&lt;/p&gt;
&lt;p&gt;
From Pixels to UI Actions: Learning to Follow Instructions via Graphical User Interfaces. (arXiv:2306.00245v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00245
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20687;&#32032;&#32423;&#21035;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#24314;&#31435;&#20102;&#19968;&#31181;&#27169;&#25311;&#20154;&#31867;&#27010;&#24565;&#30028;&#38754;&#21644;&#28151;&#21512;&#21160;&#20316;&#31354;&#38388;&#30340;&#20195;&#29702;&#65292;&#23454;&#29616;&#20102;&#22312;GUI&#25351;&#20196;&#36981;&#24490;&#20219;&#21153;&#30340;MiniWob++&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36234;&#20154;&#31867;&#24037;&#20316;&#32773;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#20026;&#20102;&#26500;&#24314;&#25805;&#20316;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;&#65288;GUI&#65289;&#30340;&#25968;&#23383;&#21270;&#20195;&#29702;&#65292;&#22823;&#22810;&#25968;&#24037;&#20316;&#37117;&#20381;&#36182;&#22522;&#20110;&#25991;&#26412;&#30340;&#34920;&#31034;&#65288;&#20174;HTML&#25110;&#20854;&#20182;&#32467;&#26500;&#21270;&#25968;&#25454;&#28304;&#27966;&#29983;&#65289;&#65292;&#36825;&#20123;&#34920;&#31034;&#24182;&#19981;&#24635;&#26159;&#23481;&#26131;&#33719;&#21462;&#12290;&#36825;&#20123;&#36755;&#20837;&#34920;&#31034;&#36890;&#24120;&#19982;&#33258;&#23450;&#20041;&#30340;&#20219;&#21153;&#29305;&#23450;&#21160;&#20316;&#31354;&#38388;&#30456;&#20851;&#32852;&#12290;&#26412;&#25991;&#26088;&#22312;&#21019;&#24314;&#20351;&#29992;&#19982;&#20154;&#31867;&#36890;&#24120;&#20351;&#29992;&#30340;&#30456;&#21516;&#27010;&#24565;&#30028;&#38754;-&#36890;&#36807;&#22522;&#20110;&#20687;&#32032;&#30340;&#23631;&#24149;&#25130;&#22270;&#21644;&#23545;&#24212;&#20110;&#38190;&#30424;&#21644;&#40736;&#26631;&#25805;&#20316;&#30340;&#36890;&#29992;&#21160;&#20316;&#31354;&#38388;&#19982;&#25968;&#23383;&#19990;&#30028;&#20132;&#20114;&#30340;&#20195;&#29702;&#12290;&#22312;&#36817;&#26399;&#20851;&#20110;&#20687;&#32032;&#32423;&#39044;&#35757;&#32451;&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#39318;&#27425;&#23637;&#31034;&#20102;&#36825;&#26679;&#30340;&#20195;&#29702;&#22312;GUI&#25351;&#20196;&#36981;&#24490;&#20219;&#21153;&#30340;MiniWob ++&#22522;&#20934;&#27979;&#35797;&#20013;&#33021;&#22815;&#36229;&#36234;&#20154;&#31867;&#24037;&#20316;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Much of the previous work towards digital agents for graphical user interfaces (GUIs) has relied on text-based representations (derived from HTML or other structured data sources), which are not always readily available. These input representations have been often coupled with custom, task-specific action spaces. This paper focuses on creating agents that interact with the digital world using the same conceptual interface that humans commonly use -via pixel-based screenshots and a generic action space corresponding to keyboard and mouse actions. Building upon recent progress in pixel-based pretraining, we show, for the first time, that it is possible for such agents to outperform human crowdworkers on the MiniWob++ benchmark of GUI-based instruction following tasks.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#32452;&#21512;&#31070;&#32463;&#33218;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#36817;&#20284;&#26410;&#30693;&#24471;&#20998;&#20989;&#25968;&#65292;&#36825;&#26159;&#22788;&#29702;&#31867;&#20284;&#38382;&#39064;&#30340;&#31532;&#19968;&#20010;&#31639;&#27861;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2306.00242</link><description>&lt;p&gt;
&#32452;&#21512;&#31070;&#32463;&#33218;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Combinatorial Neural Bandits. (arXiv:2306.00242v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00242
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#32452;&#21512;&#31070;&#32463;&#33218;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#36817;&#20284;&#26410;&#30693;&#24471;&#20998;&#20989;&#25968;&#65292;&#36825;&#26159;&#22788;&#29702;&#31867;&#20284;&#38382;&#39064;&#30340;&#31532;&#19968;&#20010;&#31639;&#27861;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#32452;&#21512;&#33218;&#38382;&#39064;&#65292;&#20854;&#20013;&#23398;&#20064;&#20195;&#29702;&#22312;&#27599;&#19968;&#36718;&#36873;&#25321;&#19968;&#32452;&#33218;&#24182;&#26681;&#25454;&#20854;&#20998;&#25968;&#25509;&#25910;&#21453;&#39304;&#12290;&#33218;&#30340;&#24471;&#20998;&#26159;&#33218;&#29305;&#24449;&#30340;&#26410;&#30693;&#20989;&#25968;&#12290;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#36817;&#20284;&#36825;&#20010;&#26410;&#30693;&#30340;&#24471;&#20998;&#20989;&#25968;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31639;&#27861;&#65306;&#32452;&#21512;&#31070;&#32463;UCB&#65288;CN-UCB&#65289;&#21644;&#32452;&#21512;&#31070;&#32463;&#27748;&#26222;&#26862;&#25277;&#26679;&#65288;CN-TS&#65289;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;CN-UCB&#21487;&#20197;&#36798;&#21040;$\tilde{\mathcal{O}}(\tilde{d} \sqrt{T})$&#25110; $\tilde{\mathcal{O}}(\sqrt{\tilde{d} T K})$&#36951;&#25022;&#20540;&#65292;&#20854;&#20013;$\tilde{d}$&#26159;&#31070;&#32463;&#20999;&#21521;&#26680;&#30697;&#38453;&#30340;&#26377;&#25928;&#32500;&#24230;&#65292;$K$&#26159;&#19968;&#32452;&#33218;&#30340;&#22823;&#23567;&#65292;$T$&#26159;&#26102;&#38388;&#36328;&#24230;&#12290;&#23545;&#20110;CN-TS&#65292;&#25105;&#20204;&#37319;&#29992;&#19968;&#31181;&#20048;&#35266;&#25277;&#26679;&#25216;&#26415;&#26469;&#30830;&#20445;&#32452;&#21512;&#21160;&#20316;&#30340;&#20048;&#35266;&#24615;&#65292;&#36798;&#21040;&#26368;&#24046;&#24773;&#20917;&#65288;&#39057;&#29575;&#23398;&#27966;&#65289;&#36951;&#25022;&#20026;$\tilde{\mathcal{O}}(\tilde{d} \sqrt{TK})$&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#35299;&#20915;&#27492;&#31867;&#32452;&#21512;&#33218;&#38382;&#39064;&#30340;&#31639;&#27861;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a contextual combinatorial bandit problem where in each round a learning agent selects a subset of arms and receives feedback on the selected arms according to their scores. The score of an arm is an unknown function of the arm's feature. Approximating this unknown score function with deep neural networks, we propose algorithms: Combinatorial Neural UCB ($\texttt{CN-UCB}$) and Combinatorial Neural Thompson Sampling ($\texttt{CN-TS}$). We prove that $\texttt{CN-UCB}$ achieves $\tilde{\mathcal{O}}(\tilde{d} \sqrt{T})$ or $\tilde{\mathcal{O}}(\sqrt{\tilde{d} T K})$ regret, where $\tilde{d}$ is the effective dimension of a neural tangent kernel matrix, $K$ is the size of a subset of arms, and $T$ is the time horizon. For $\texttt{CN-TS}$, we adapt an optimistic sampling technique to ensure the optimism of the sampled combinatorial action, achieving a worst-case (frequentist) regret of $\tilde{\mathcal{O}}(\tilde{d} \sqrt{TK})$. To the best of our knowledge, these are the first 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#22312;&#28065; shedding &#19978;&#39044;&#27979;&#30340;&#23616;&#38480;&#24615;&#36739;&#22823;&#65292;&#25968;&#25454;&#39537;&#21160;&#22411;&#30340;PINN&#21482;&#26377;&#22312;&#35757;&#32451;&#25968;&#25454;&#21487;&#29992;&#26102;&#25165;&#33021;&#39044;&#27979;&#28065; shedding&#65292;&#19988;&#21487;&#33021;&#26159;&#30001;&#20110;&#35889;&#20559;&#24046;&#30340;&#21407;&#22240;&#65292;PINN&#20250;&#24674;&#22797;&#21040;&#31283;&#24577;&#35299;&#12290;</title><link>http://arxiv.org/abs/2306.00230</link><description>&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#22312;&#28065; shedding &#20013;&#30340;&#39044;&#27979;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
Predictive Limitations of Physics-Informed Neural Networks in Vortex Shedding. (arXiv:2306.00230v1 [cs.CE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00230
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#22312;&#28065; shedding &#19978;&#39044;&#27979;&#30340;&#23616;&#38480;&#24615;&#36739;&#22823;&#65292;&#25968;&#25454;&#39537;&#21160;&#22411;&#30340;PINN&#21482;&#26377;&#22312;&#35757;&#32451;&#25968;&#25454;&#21487;&#29992;&#26102;&#25165;&#33021;&#39044;&#27979;&#28065; shedding&#65292;&#19988;&#21487;&#33021;&#26159;&#30001;&#20110;&#35889;&#20559;&#24046;&#30340;&#21407;&#22240;&#65292;PINN&#20250;&#24674;&#22797;&#21040;&#31283;&#24577;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(PINN)&#26041;&#27861;&#30340;&#20852;&#36259;&#28608;&#22686;&#65292;&#23548;&#33268;&#20102;&#19968;&#31995;&#21015;&#30740;&#31350;&#35777;&#26126;&#20102;&#23427;&#20204;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;(PDE)&#21644;&#39044;&#27979;&#29289;&#29702;&#31995;&#32479;&#21160;&#21147;&#23398;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;PINN&#30340;&#39044;&#27979;&#23616;&#38480;&#24615;&#36824;&#27809;&#26377;&#24471;&#21040;&#28145;&#20837;&#30340;&#30740;&#31350;&#12290;&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;2D&#22278;&#26609;&#21608;&#22260;&#30340;&#27969;&#21160;&#65292;&#21457;&#29616;&#26080;&#25968;&#25454;&#30340;PINN&#26080;&#27861;&#39044;&#27979;&#28065; shedding&#12290;&#25968;&#25454;&#39537;&#21160;&#30340;PINN&#21482;&#26377;&#22312;&#35757;&#32451;&#25968;&#25454;(&#26469;&#33258;&#20256;&#32479;CFD&#27714;&#35299;&#22120;)&#21487;&#29992;&#26102;&#25165;&#26174;&#31034;&#28065; shedding&#65292;&#20294;&#24403;&#25968;&#25454;&#27969;&#20572;&#27490;&#26102;&#65292;&#23427;&#20250;&#24674;&#22797;&#21040;&#31283;&#24577;&#35299;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#21160;&#24577;&#27169;&#24577;&#20998;&#35299;&#65292;&#24182;&#20998;&#26512;&#20102;&#36890;&#36807;PINN&#21644;&#20256;&#32479;&#27969;&#20307;&#27714;&#35299;&#22120;(PetIBM)&#33719;&#24471;&#30340;&#35299;&#20013;&#30340;Koopman&#27169;&#24335;&#12290;Koopman&#29305;&#24449;&#20540;&#22312;&#22797;&#24179;&#38754;&#19978;&#30340;&#20998;&#24067;&#34920;&#26126;&#65292;PINN&#22312;&#25968;&#20540;&#19978;&#26159;&#23637;&#25955;&#21644;&#25193;&#25955;&#30340;&#12290; PINN&#26041;&#27861;&#21487;&#33021;&#26159;&#30001;&#20110;&#35889;&#20559;&#24046;&#65292;&#25165;&#24674;&#22797;&#21040;&#31283;&#24577;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent surge of interest in physics-informed neural network (PINN) methods has led to a wave of studies that attest to their potential for solving partial differential equations (PDEs) and predicting the dynamics of physical systems. However, the predictive limitations of PINNs have not been thoroughly investigated. We look at the flow around a 2D cylinder and find that data-free PINNs are unable to predict vortex shedding. Data-driven PINN exhibits vortex shedding only while the training data (from a traditional CFD solver) is available, but reverts to the steady state solution when the data flow stops. We conducted dynamic mode decomposition and analyze the Koopman modes in the solutions obtained with PINNs versus a traditional fluid solver (PetIBM). The distribution of the Koopman eigenvalues on the complex plane suggests that PINN is numerically dispersive and diffusive. The PINN method reverts to the steady solution possibly as a consequence of spectral bias. This case study r
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25193;&#25955;&#30011;&#31508;&#30340;&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;AI&#22270;&#20687;&#24494;&#35843;&#24037;&#20855;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#26681;&#25454;&#30446;&#26631;&#21306;&#22495;&#20462;&#25913;AI&#21512;&#25104;&#22270;&#20687;&#24182;&#20445;&#30041;&#21407;&#22987;&#19978;&#19979;&#25991;&#12290;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#20462;&#22797;&#25216;&#26415;&#36827;&#34892;&#27604;&#36739;&#65292;&#35813;&#26041;&#27861;&#22312;&#29992;&#25143;&#30740;&#31350;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#21487;&#29992;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.00219</link><description>&lt;p&gt;
&#25193;&#25955;&#30011;&#31508;&#65306;&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;AI&#29983;&#25104;&#22270;&#20687;&#32534;&#36753;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Diffusion Brush: A Latent Diffusion Model-based Editing Tool for AI-generated Images. (arXiv:2306.00219v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00219
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25193;&#25955;&#30011;&#31508;&#30340;&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;AI&#22270;&#20687;&#24494;&#35843;&#24037;&#20855;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#26681;&#25454;&#30446;&#26631;&#21306;&#22495;&#20462;&#25913;AI&#21512;&#25104;&#22270;&#20687;&#24182;&#20445;&#30041;&#21407;&#22987;&#19978;&#19979;&#25991;&#12290;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#20462;&#22797;&#25216;&#26415;&#36827;&#34892;&#27604;&#36739;&#65292;&#35813;&#26041;&#27861;&#22312;&#29992;&#25143;&#30740;&#31350;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#21487;&#29992;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27169;&#22411;&#38480;&#21046;&#65292;&#29983;&#25104;&#30340;&#22270;&#20687;&#32463;&#24120;&#21253;&#21547;&#19981;&#33391;&#30340;&#20266;&#24433;&#25110;&#20854;&#20182;&#38169;&#35823;&#12290;&#29616;&#26377;&#30340;&#24494;&#35843;&#29983;&#25104;&#22270;&#20687;&#30340;&#25216;&#26415;&#35201;&#20040;&#32791;&#26102;&#65288;&#25163;&#21160;&#32534;&#36753;&#65289;&#65292;&#35201;&#20040;&#20135;&#29983;&#19981;&#22815;&#23436;&#32654;&#30340;&#32467;&#26524;&#65288;&#20462;&#34917;&#65289;&#65292;&#35201;&#20040;&#20250;&#23548;&#33268;&#25972;&#20307;&#22270;&#20687;&#20135;&#29983;&#24847;&#24819;&#19981;&#21040;&#30340;&#21464;&#21270;&#65288;&#21464;&#20307;&#36873;&#25321;&#21644;&#25552;&#31034;&#24494;&#35843;&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25193;&#25955;&#30011;&#31508;&#30340;&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;AI&#22270;&#20687;&#24494;&#35843;&#24037;&#20855;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#24494;&#35843;AI&#21512;&#25104;&#22270;&#20687;&#20013;&#25152;&#38656;&#30340;&#21306;&#22495;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21453;&#21521;&#25193;&#25955;&#36807;&#31243;&#20013;&#22312;&#30446;&#26631;&#21306;&#22495;&#24341;&#20837;&#20102;&#26032;&#30340;&#38543;&#26426;&#22122;&#22768;&#27169;&#24335;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#22312;&#20445;&#30041;&#20854;&#20182;&#21306;&#22495;&#21407;&#22987;&#19978;&#19979;&#25991;&#30340;&#21516;&#26102;&#65292;&#39640;&#25928;&#22320;&#23545;&#25351;&#23450;&#21306;&#22495;&#36827;&#34892;&#26356;&#25913;&#12290;&#25105;&#20204;&#36890;&#36807;&#33402;&#26415;&#23478;&#36827;&#34892;&#30340;&#29992;&#25143;&#30740;&#31350;&#35780;&#20272;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#21487;&#29992;&#24615;&#21644;&#26377;&#25928;&#24615;&#65292;&#23558;&#25105;&#20204;&#30340;&#25216;&#26415;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#20462;&#22797;&#25216;&#26415;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image generative models have made remarkable advancements in generating high-quality images. However, generated images often contain undesirable artifacts or other errors due to model limitations. Existing techniques to fine-tune generated images are time-consuming (manual editing), produce poorly-integrated results (inpainting), or result in unexpected changes across the entire image (variation selection and prompt fine-tuning). In this work, we present Diffusion Brush, a Latent Diffusion Model-based (LDM) tool to efficiently fine-tune desired regions within an AI-synthesized image. Our method introduces new random noise patterns at targeted regions during the reverse diffusion process, enabling the model to efficiently make changes to the specified regions while preserving the original context for the rest of the image. We evaluate our method's usability and effectiveness through a user study with artists, comparing our technique against other state-of-the-art image inpaintin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26377;&#32422;&#26463;&#30340;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#65292;&#22810;&#20010;&#26234;&#33021;&#20307;&#36890;&#36807;&#26368;&#22823;&#21270;&#24635;&#20307;&#22870;&#21169;&#24182;&#22312;&#23545;&#24635;&#20307;&#25928;&#29992;&#30340;&#26399;&#26395;&#20540;&#19978;&#35774;&#32622;&#32422;&#26463;&#26465;&#20214;&#36827;&#34892;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#12290;&#36890;&#36807;&#27867;&#21270;Lagrangian&#31574;&#30053;&#20248;&#21270;&#65292;&#37319;&#29992;&#22522;&#20110;&#21344;&#29992;&#27979;&#37327;&#30340;&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;&#32622;&#20449;&#19978;&#38480;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#23454;&#29616;&#23545;&#32422;&#26463;&#30340;&#22788;&#29702;&#12290;</title><link>http://arxiv.org/abs/2306.00212</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#23433;&#20840;&#24615;&#20248;&#21270;&#25216;&#26415;&#8212;&#8212;&#27867;&#21270;Lagrangian&#31574;&#30053;&#20248;&#21270;&#30340;&#21487;&#35777;&#26126;&#39640;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Provably Efficient Generalized Lagrangian Policy Optimization for Safe Multi-Agent Reinforcement Learning. (arXiv:2306.00212v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00212
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26377;&#32422;&#26463;&#30340;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#65292;&#22810;&#20010;&#26234;&#33021;&#20307;&#36890;&#36807;&#26368;&#22823;&#21270;&#24635;&#20307;&#22870;&#21169;&#24182;&#22312;&#23545;&#24635;&#20307;&#25928;&#29992;&#30340;&#26399;&#26395;&#20540;&#19978;&#35774;&#32622;&#32422;&#26463;&#26465;&#20214;&#36827;&#34892;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#12290;&#36890;&#36807;&#27867;&#21270;Lagrangian&#31574;&#30053;&#20248;&#21270;&#65292;&#37319;&#29992;&#22522;&#20110;&#21344;&#29992;&#27979;&#37327;&#30340;&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;&#32622;&#20449;&#19978;&#38480;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#23454;&#29616;&#23545;&#32422;&#26463;&#30340;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22312;&#26377;&#32422;&#26463;&#30340;&#39532;&#23572;&#31185;&#22827;&#21338;&#24328;&#20013;&#65292;&#20114;&#30456;&#31454;&#20105;&#30340;&#22810;&#20010;&#26234;&#33021;&#20307;&#36890;&#36807;&#26368;&#22823;&#21270;&#24635;&#20307;&#22870;&#21169;&#24182;&#22312;&#23545;&#24635;&#20307;&#25928;&#29992;&#30340;&#26399;&#26395;&#20540;&#19978;&#35774;&#32622;&#32422;&#26463;&#26465;&#20214;&#36827;&#34892;&#22312;&#32447;&#23433;&#20840;&#24615;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#19968;&#20010;&#25317;&#26377;&#20004;&#20010;&#29420;&#31435;&#36716;&#31227;&#20989;&#25968;&#65292;&#23545;&#26234;&#33021;&#20307;&#26410;&#30693;&#30340;&#19988;&#23384;&#22312;&#23545;&#25239;&#24615;&#22870;&#21169;&#20989;&#25968;&#21644;&#38543;&#26426;&#25928;&#29992;&#20989;&#25968;&#30340;&#21452;&#20154;&#38646;&#21644;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#12290;&#20026;&#20102;&#35299;&#20915;&#35813;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#21344;&#29992;&#27979;&#37327;&#30340;&#26041;&#27861;&#23558;&#20854;&#25551;&#36848;&#20026;&#24102;&#26377;&#26174;&#24335;&#32422;&#26463;&#30340;&#22312;&#32447;&#32422;&#26463;&#38797;&#28857;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#22312;&#32422;&#26463;&#20248;&#21270;&#20013;&#25512;&#24191;Lagrange&#20056;&#25968;&#26041;&#27861;&#24182;&#21019;&#24314;&#19968;&#20010;&#24102;&#26377;Minimax&#20915;&#31574;&#21407;&#22987;&#21464;&#37327;&#21644;&#21452;&#21464;&#37327;&#30340;&#24191;&#20041;Lagrangian&#22788;&#29702;&#32422;&#26463;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#32622;&#20449;&#19978;&#38480;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#23454;&#29616;&#25506;&#32034;&#21644;&#21033;&#29992;&#30340;&#24179;&#34913;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#36890;&#36807;&#22312;&#32447;&#38236;&#20687;&#26356;&#26032;Minimax&#20915;&#31574;&#21407;&#22987;&#21464;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
We examine online safe multi-agent reinforcement learning using constrained Markov games in which agents compete by maximizing their expected total rewards under a constraint on expected total utilities. Our focus is confined to an episodic two-player zero-sum constrained Markov game with independent transition functions that are unknown to agents, adversarial reward functions, and stochastic utility functions. For such a Markov game, we employ an approach based on the occupancy measure to formulate it as an online constrained saddle-point problem with an explicit constraint. We extend the Lagrange multiplier method in constrained optimization to handle the constraint by creating a generalized Lagrangian with minimax decision primal variables and a dual variable. Next, we develop an upper confidence reinforcement learning algorithm to solve this Lagrangian problem while balancing exploration and exploitation. Our algorithm updates the minimax decision primal variables via online mirror
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22270;&#24418;&#30340;&#31243;&#24207;&#34920;&#31034;&#65292;&#31216;&#20026;PERFOGRAPH&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#25429;&#33719;&#25968;&#20540;&#20449;&#24687;&#21644;&#22797;&#21512;&#25968;&#25454;&#32467;&#26500;&#26469;&#20811;&#26381;&#29616;&#26377;&#31243;&#24207;&#34920;&#31034;&#30340;&#38480;&#21046;&#21644;&#25361;&#25112;&#65292;&#26159;&#19968;&#31181;&#39640;&#24230;&#28789;&#27963;&#21644;&#21487;&#25193;&#23637;&#30340;&#34920;&#31034;&#24418;&#24335;&#12290;</title><link>http://arxiv.org/abs/2306.00210</link><description>&lt;p&gt;
PERFOGRAPH&#65306;&#19968;&#31181;&#25968;&#20540;&#24863;&#30693;&#30340;&#31243;&#24207;&#22270;&#34920;&#31034;&#65292;&#29992;&#20110;&#24615;&#33021;&#20248;&#21270;&#21644;&#31243;&#24207;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
PERFOGRAPH: A Numerical Aware Program Graph Representation for Performance Optimization and Program Analysis. (arXiv:2306.00210v1 [cs.PL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00210
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22270;&#24418;&#30340;&#31243;&#24207;&#34920;&#31034;&#65292;&#31216;&#20026;PERFOGRAPH&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#25429;&#33719;&#25968;&#20540;&#20449;&#24687;&#21644;&#22797;&#21512;&#25968;&#25454;&#32467;&#26500;&#26469;&#20811;&#26381;&#29616;&#26377;&#31243;&#24207;&#34920;&#31034;&#30340;&#38480;&#21046;&#21644;&#25361;&#25112;&#65292;&#26159;&#19968;&#31181;&#39640;&#24230;&#28789;&#27963;&#21644;&#21487;&#25193;&#23637;&#30340;&#34920;&#31034;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#30340;&#26174;&#33879;&#22686;&#38271;&#21644;&#37325;&#35201;&#25104;&#21151;&#23558;&#20854;&#24212;&#29992;&#25193;&#23637;&#21040;&#20102;&#32534;&#31243;&#35821;&#35328;&#21644;&#31243;&#24207;&#20998;&#26512;&#20013;&#12290;&#28982;&#32780;&#65292;&#37319;&#29992;&#26368;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#32534;&#31243;&#35821;&#35328;&#30340;&#34920;&#31034;&#65292;&#36825;&#30452;&#25509;&#24433;&#21709;&#20102;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20851;&#20110;&#31243;&#24207;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#20197;&#24448;&#30340;&#34920;&#31034;&#20316;&#21697;&#20013;&#32570;&#20047;&#25968;&#20540;&#24847;&#35782;&#12289;&#22797;&#21512;&#25968;&#25454;&#32467;&#26500;&#20449;&#24687;&#21644;&#21464;&#37327;&#34920;&#31034;&#19981;&#24403;&#31561;&#38382;&#39064;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#20811;&#26381;&#24403;&#21069;&#31243;&#24207;&#34920;&#31034;&#30340;&#38480;&#21046;&#21644;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22270;&#24418;&#30340;&#31243;&#24207;&#34920;&#31034;&#65292;&#31216;&#20026;PERFOGRAPH&#12290; PERFOGRAPH&#21487;&#20197;&#36890;&#36807;&#24341;&#20837;&#26032;&#33410;&#28857;&#21644;&#36793;&#26469;&#25429;&#33719;&#25968;&#20540;&#20449;&#24687;&#21644;&#22797;&#21512;&#25968;&#25454;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#24615;&#23884;&#20837;&#26041;&#27861;&#26469;&#21512;&#24182;&#25968;&#20540;&#24847;&#35782;&#12290;&#36825;&#20123;&#22686;&#24378;&#20351;PERFOGRAPH&#25104;&#20026;&#39640;&#24230;&#28789;&#27963;&#21644;&#21487;&#25193;&#23637;&#30340;&#34920;&#31034;&#24418;&#24335;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25429;&#33719;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
The remarkable growth and significant success of machine learning have expanded its applications into programming languages and program analysis. However, a key challenge in adopting the latest machine learning methods is the representation of programming languages, which directly impacts the ability of machine learning methods to reason about programs. The absence of numerical awareness, composite data structure information, and improper way of presenting variables in previous representation works have limited their performances. To overcome the limitations and challenges of current program representations, we propose a novel graph-based program representation called PERFOGRAPH. PERFOGRAPH can capture numerical information and the composite data structure by introducing new nodes and edges. Furthermore, we propose an adapted embedding method to incorporate numerical awareness. These enhancements make PERFOGRAPH a highly flexible and scalable representation that can effectively capture
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Adam&#22312;Transformer&#30340;&#35757;&#32451;&#20013;&#20026;&#20160;&#20040;&#27604;SGD&#26356;&#24555;&#25910;&#25947;&#65292;&#25552;&#20986;&#20102;&#26041;&#21521;&#38160;&#24230;&#30340;&#27010;&#24565;&#24182;&#36890;&#36807;&#27604;&#36739;&#35777;&#26126;&#20102;SGD&#30456;&#23545;&#20110;&#33258;&#36866;&#24212;&#31639;&#27861;&#20855;&#26377;&#26356;&#24046;&#30340;&#26041;&#21521;&#38160;&#24230;&#65292;&#36827;&#32780;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#21482;&#38656;&#35201;&#35009;&#21098;&#19968;&#23567;&#37096;&#20998;&#22352;&#26631;&#21363;&#21487;&#25552;&#39640;SGD&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.00204</link><description>&lt;p&gt;
&#25506;&#31350;Adam&#22312;Transformers&#19978;&#27604;SGD&#26356;&#24555;&#25910;&#25947;&#30340;&#21407;&#22240;
&lt;/p&gt;
&lt;p&gt;
Toward Understanding Why Adam Converges Faster Than SGD for Transformers. (arXiv:2306.00204v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00204
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Adam&#22312;Transformer&#30340;&#35757;&#32451;&#20013;&#20026;&#20160;&#20040;&#27604;SGD&#26356;&#24555;&#25910;&#25947;&#65292;&#25552;&#20986;&#20102;&#26041;&#21521;&#38160;&#24230;&#30340;&#27010;&#24565;&#24182;&#36890;&#36807;&#27604;&#36739;&#35777;&#26126;&#20102;SGD&#30456;&#23545;&#20110;&#33258;&#36866;&#24212;&#31639;&#27861;&#20855;&#26377;&#26356;&#24046;&#30340;&#26041;&#21521;&#38160;&#24230;&#65292;&#36827;&#32780;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#21482;&#38656;&#35201;&#35009;&#21098;&#19968;&#23567;&#37096;&#20998;&#22352;&#26631;&#21363;&#21487;&#25552;&#39640;SGD&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#20173;&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#26368;&#21463;&#27426;&#36814;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#20294;&#26159;&#20687;Adam&#36825;&#26679;&#30340;&#33258;&#36866;&#24212;&#31639;&#27861;&#22312;&#19968;&#20123;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20013;&#24050;&#32463;&#35777;&#26126;&#20102;&#20248;&#20110;SGD&#30340;&#32463;&#39564;&#20248;&#21183;&#65292;&#29305;&#21035;&#26159;&#22312;Transformer&#30340;&#35757;&#32451;&#19978;&#12290;&#28982;&#32780;&#65292;&#20026;&#20160;&#20040;Adam&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#20250;&#26174;&#33879;&#27604;SGD&#26356;&#24555;&#22320;&#25910;&#25947;&#20173;&#28982;&#26159;&#19968;&#20010;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#27010;&#24565;&#8212;&#8212;&#26041;&#21521;&#38160;&#24230;&#65292;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;Adam&#27604;SGD&#26356;&#24555;&#22320;&#25910;&#25947;&#12290;&#25105;&#20204;&#35748;&#20026;&#20248;&#21270;&#31639;&#27861;&#30340;&#24615;&#33021;&#19982;&#26356;&#26032;&#27493;&#39588;&#30340;&#26041;&#21521;&#38160;&#24230;&#23494;&#20999;&#30456;&#20851;&#65292;&#35777;&#26126;&#20102;SGD&#30456;&#23545;&#20110;&#33258;&#36866;&#24212;&#31639;&#27861;&#20855;&#26377;&#26356;&#24046;&#30340;&#26041;&#21521;&#38160;&#24230;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35266;&#23519;&#21040;&#21482;&#26377;&#19968;&#23567;&#37096;&#20998;&#22352;&#26631;&#23548;&#33268;&#20102;SGD&#30340;&#38160;&#24230;&#19981;&#36275;&#21644;&#25910;&#25947;&#36895;&#24230;&#32531;&#24930;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#22352;&#26631;&#32423;&#21035;&#30340;&#35009;&#21098;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22352;&#26631;&#32423;&#21035;&#35009;&#21098;&#23545;&#20110;&#38160;&#24230;&#38477;&#20302;&#21644;&#21152;&#36895;&#25910;&#25947;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
While stochastic gradient descent (SGD) is still the most popular optimization algorithm in deep learning, adaptive algorithms such as Adam have established empirical advantages over SGD in some deep learning applications such as training transformers. However, it remains a question that why Adam converges significantly faster than SGD in these scenarios. In this paper, we propose one explanation of why Adam converges faster than SGD using a new concept directional sharpness. We argue that the performance of optimization algorithms is closely related to the directional sharpness of the update steps, and show SGD has much worse directional sharpness compared to adaptive algorithms. We further observe that only a small fraction of the coordinates causes the bad sharpness and slow convergence of SGD, and propose to use coordinate-wise clipping as a solution to SGD and other optimization algorithms. We demonstrate the effect of coordinate-wise clipping on sharpness reduction and speeding u
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#21644;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#30456;&#32467;&#21512;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26500;&#24314;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#25110;&#19981;&#24179;&#34913;&#30340;&#24773;&#20917;&#12290;&#22312;&#26230;&#22278;&#32570;&#38519;&#39044;&#27979;&#20219;&#21153;&#20013;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.00202</link><description>&lt;p&gt;
&#20351;&#29992;&#22495;&#33258;&#36866;&#24212;&#21644;&#25968;&#25454;&#22686;&#24378;&#26500;&#24314;&#20855;&#26377;&#26497;&#23569;&#21644;&#19981;&#24179;&#34913;&#35757;&#32451;&#25968;&#25454;&#30340;&#21046;&#36896;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Building Manufacturing Deep Learning Models with Minimal and Imbalanced Training Data Using Domain Adaptation and Data Augmentation. (arXiv:2306.00202v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00202
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#21644;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#30456;&#32467;&#21512;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26500;&#24314;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#25110;&#19981;&#24179;&#34913;&#30340;&#24773;&#20917;&#12290;&#22312;&#26230;&#22278;&#32570;&#38519;&#39044;&#27979;&#20219;&#21153;&#20013;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#22270;&#20687;&#32570;&#38519;&#26816;&#27979;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#26631;&#35760;&#25968;&#25454;&#65292;&#36825;&#36890;&#24120;&#24456;&#26114;&#36149;&#12290;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#21487;&#29992;&#30340;&#35757;&#32451;&#25968;&#25454;&#19981;&#20165;&#26377;&#38480;&#65292;&#32780;&#19988;&#21487;&#33021;&#26159;&#19981;&#24179;&#34913;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22495;&#33258;&#36866;&#24212;&#65288;DA&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#29992;&#20110;&#31867;&#20284;&#23398;&#20064;&#20219;&#21153;&#30340;&#29616;&#26377;&#28304;&#25968;&#25454;&#38598;&#20013;&#36716;&#31227;&#25152;&#33719;&#24471;&#30340;&#30693;&#35782;&#65292;&#26469;&#35299;&#20915;&#30446;&#26631;&#23398;&#20064;&#20219;&#21153;&#30340;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#28304;&#25968;&#25454;&#38598;&#21644;&#21487;&#29992;&#20110;&#30446;&#26631;&#23398;&#20064;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#20855;&#26377;&#30456;&#21516;&#25110;&#19981;&#21516;&#30340;&#29305;&#24449;&#31354;&#38388;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;DA&#26041;&#27861;&#19982;&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#20197;&#35299;&#20915;&#30446;&#26631;&#25968;&#25454;&#38598;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#26230;&#22278;&#32570;&#38519;&#39044;&#27979;&#30340;&#22270;&#20687;&#25968;&#25454;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#32452;&#21512;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#32452;&#21512;&#26041;&#27861;&#22312;&#26631;&#35760;&#25968;&#25454;&#26377;&#38480;&#25110;&#19981;&#24179;&#34913;&#26102;&#20855;&#26377;&#27604;&#20854;&#20182;&#31639;&#27861;&#26356;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning (DL) techniques are highly effective for defect detection from images. Training DL classification models, however, requires vast amounts of labeled data which is often expensive to collect. In many cases, not only the available training data is limited but may also imbalanced. In this paper, we propose a novel domain adaptation (DA) approach to address the problem of labeled training data scarcity for a target learning task by transferring knowledge gained from an existing source dataset used for a similar learning task. Our approach works for scenarios where the source dataset and the dataset available for the target learning task have same or different feature spaces. We combine our DA approach with an autoencoder-based data augmentation approach to address the problem of imbalanced target datasets. We evaluate our combined approach using image data for wafer defect prediction. The experiments show its superior performance against other algorithms when the number of lab
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#8212;&#8212;&#24191;&#20041;&#38544;&#24335;Follow-The-Regularized-Leader (FTRL)&#65292;&#23427;&#21487;&#20197;&#24674;&#22797;&#24050;&#30693;&#30340;&#31639;&#27861;&#65292;&#22914;&#32447;&#24615;&#25439;&#22833;&#30340;FTRL&#21644;&#38544;&#24335;FTRL&#65292;&#24182;&#20801;&#35768;&#35774;&#35745;&#26032;&#30340;&#26356;&#26032;&#35268;&#21017;&#12290;&#35813;&#31639;&#27861;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#29992;Fenchel-Young&#19981;&#31561;&#24335;&#20195;&#26367;&#25439;&#22833;&#30340;&#32447;&#24615;&#21270;&#65292;&#21487;&#20197;&#30452;&#25509;&#25913;&#36827;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#21518;&#24724;&#19978;&#38480;&#12290;</title><link>http://arxiv.org/abs/2306.00201</link><description>&lt;p&gt;
&#24191;&#20041;&#38544;&#24335;Follow-The-Regularized-Leader&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Generalized Implicit Follow-The-Regularized-Leader. (arXiv:2306.00201v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00201
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#8212;&#8212;&#24191;&#20041;&#38544;&#24335;Follow-The-Regularized-Leader (FTRL)&#65292;&#23427;&#21487;&#20197;&#24674;&#22797;&#24050;&#30693;&#30340;&#31639;&#27861;&#65292;&#22914;&#32447;&#24615;&#25439;&#22833;&#30340;FTRL&#21644;&#38544;&#24335;FTRL&#65292;&#24182;&#20801;&#35768;&#35774;&#35745;&#26032;&#30340;&#26356;&#26032;&#35268;&#21017;&#12290;&#35813;&#31639;&#27861;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#29992;Fenchel-Young&#19981;&#31561;&#24335;&#20195;&#26367;&#25439;&#22833;&#30340;&#32447;&#24615;&#21270;&#65292;&#21487;&#20197;&#30452;&#25509;&#25913;&#36827;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#21518;&#24724;&#19978;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31867;&#26032;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#31216;&#20026;&#24191;&#20041;&#38544;&#24335;Follow-The-Regularized-Leader (FTRL)&#65292;&#23427;&#25193;&#23637;&#20102;FTRL&#26694;&#26550;&#30340;&#33539;&#22260;&#12290;&#24191;&#20041;&#38544;&#24335;FTRL&#21487;&#20197;&#24674;&#22797;&#24050;&#30693;&#30340;&#31639;&#27861;&#65292;&#22914;&#32447;&#24615;&#25439;&#22833;&#30340;FTRL&#21644;&#38544;&#24335;FTRL&#65292;&#24182;&#20801;&#35768;&#35774;&#35745;&#26032;&#30340;&#26356;&#26032;&#35268;&#21017;&#65292;&#20363;&#22914;aProx&#21644;Mirror-Prox&#30340;&#25193;&#23637;&#21040;FTRL&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#26159;&#24314;&#35774;&#24615;&#30340;&#65292;&#22240;&#20026;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#20197;&#35774;&#35745;&#30452;&#25509;&#25913;&#36827;&#26368;&#22351;&#24773;&#20917;&#21518;&#24724;&#30340;&#19978;&#38480;&#30340;&#26356;&#26032;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#29992;Fenchel-Young&#19981;&#31561;&#24335;&#20195;&#26367;&#25439;&#22833;&#30340;&#32447;&#24615;&#21270;&#12290;&#25105;&#20204;&#36890;&#36807;&#35777;&#26126;&#19968;&#20123;&#24050;&#30693;&#30340;&#31639;&#27861;&#65292;&#22914;Mirror-Prox&#26356;&#26032;&#65292;&#26159;&#24191;&#20041;&#38544;&#24335;FTRL&#30340;&#23454;&#20363;&#65292;&#23637;&#31034;&#20102;&#26694;&#26550;&#30340;&#28789;&#27963;&#24615;&#12290;&#26368;&#21518;&#65292;&#26032;&#26694;&#26550;&#20351;&#25105;&#20204;&#33021;&#22815;&#24674;&#22797;&#38544;&#24335;OMD&#30340;&#26102;&#38388;&#21464;&#21270;&#30028;&#65292;&#21516;&#26102;&#20855;&#26377;&#30456;&#21516;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new class of online learning algorithms, generalized implicit Follow-The-Regularized-Leader (FTRL), that expands the scope of FTRL framework. Generalized implicit FTRL can recover known algorithms, as FTRL with linearized losses and implicit FTRL, and it allows the design of new update rules, as extensions of aProx and Mirror-Prox to FTRL. Our theory is constructive in the sense that it provides a simple unifying framework to design updates that directly improve the worst-case upper bound on the regret. The key idea is substituting the linearization of the losses with a Fenchel-Young inequality. We show the flexibility of the framework by proving that some known algorithms, like the Mirror-Prox updates, are instantiations of the generalized implicit FTRL. Finally, the new framework allows us to recover the temporal variation bound of implicit OMD, with the same computational complexity.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25511;&#21046;&#25991;&#26412;&#29983;&#25104;&#30340;&#19981;&#21464;&#23398;&#20064;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29983;&#25104;&#30340;&#25991;&#26412;&#20998;&#24067;&#36716;&#31227;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.00198</link><description>&lt;p&gt;
&#25511;&#21046;&#25991;&#26412;&#29983;&#25104;&#30340;&#19981;&#21464;&#23398;&#20064;&#29305;&#24449;&#25551;&#36848;
&lt;/p&gt;
&lt;p&gt;
An Invariant Learning Characterization of Controlled Text Generation. (arXiv:2306.00198v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00198
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25511;&#21046;&#25991;&#26412;&#29983;&#25104;&#30340;&#19981;&#21464;&#23398;&#20064;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29983;&#25104;&#30340;&#25991;&#26412;&#20998;&#24067;&#36716;&#31227;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25511;&#21046;&#29983;&#25104;&#26159;&#25351;&#21019;&#24314;&#21253;&#21547;&#25152;&#38656;&#26679;&#24335;&#25110;&#35821;&#20041;&#23646;&#24615;&#30340;&#25991;&#26412;&#30340;&#38382;&#39064;&#12290;&#35768;&#22810;&#26041;&#27861;&#23558;&#36825;&#20010;&#38382;&#39064;&#24402;&#32467;&#20026;&#35757;&#32451;&#25152;&#38656;&#23646;&#24615;&#30340;&#39044;&#27979;&#22120;&#12290;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#29983;&#25104;&#30340;&#25991;&#26412;&#21487;&#33021;&#26469;&#33258;&#21508;&#31181;&#20998;&#24067;&#12290;&#26412;&#25991;&#36890;&#36807;&#19981;&#21464;&#23398;&#20064;&#30340;&#26041;&#24335;&#35299;&#20915;&#20102;&#25511;&#21046;&#29983;&#25104;&#20013;&#20998;&#24067;&#36716;&#31227;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Controlled generation refers to the problem of creating text that contains stylistic or semantic attributes of interest. Many approaches reduce this problem to training a predictor of the desired attribute. For example, researchers hoping to deploy a large language model to produce non-toxic content may use a toxicity classifier to filter generated text. In practice, the generated text to classify, which is determined by user prompts, may come from a wide range of distributions. In this paper, we show that the performance of controlled generation may be poor if the distributions of text in response to user prompts differ from the distribution the predictor was trained on. To address this problem, we cast controlled generation under distribution shift as an invariant learning problem: the most effective predictor should be invariant across multiple text environments. We then discuss a natural solution that arises from this characterization and propose heuristics for selecting natural en
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SSL-CPCD&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#38024;&#23545;&#34917;&#19969;&#32423;&#21035;&#30340;&#23454;&#20363;&#32452;&#21306;&#20998;&#21644;&#36328;&#31867;&#21035;&#21464;&#21270;&#30340;&#24809;&#32602;&#65292;&#22312;&#20351;&#29992;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#25552;&#39640;&#20869;&#31397;&#38236;&#22270;&#20687;&#20998;&#26512;&#27867;&#21270;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2306.00197</link><description>&lt;p&gt;
SSL-CPCD&#65306;&#33258;&#25105;&#30417;&#30563;&#30340;&#32452;&#21512;&#39044;&#25991;&#26412;&#31867;&#21035;&#21306;&#20998;&#23398;&#20064;&#65292;&#25552;&#39640;&#20869;&#31397;&#38236;&#22270;&#20687;&#20998;&#26512;&#30340;&#27867;&#21270;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
SSL-CPCD: Self-supervised learning with composite pretext-class discrimination for improved generalisability in endoscopic image analysis. (arXiv:2306.00197v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00197
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SSL-CPCD&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#38024;&#23545;&#34917;&#19969;&#32423;&#21035;&#30340;&#23454;&#20363;&#32452;&#21306;&#20998;&#21644;&#36328;&#31867;&#21035;&#21464;&#21270;&#30340;&#24809;&#32602;&#65292;&#22312;&#20351;&#29992;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#25552;&#39640;&#20869;&#31397;&#38236;&#22270;&#20687;&#20998;&#26512;&#27867;&#21270;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#27493;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#30417;&#30563;&#26041;&#27861;&#24191;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#38754;&#20020;&#27867;&#21270;&#21040;&#26410;&#35265;&#36807;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#65292;&#36825;&#24433;&#21709;&#20102;&#20020;&#24202;&#36716;&#21270;&#12290;&#20869;&#31397;&#38236;&#25104;&#20687;&#25968;&#25454;&#20855;&#26377;&#22823;&#30340;&#24739;&#32773;&#20869;&#22806;&#21464;&#24322;&#24615;&#65292;&#36825;&#20351;&#24471;&#36825;&#20123;&#27169;&#22411;&#26356;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#26080;&#27861;&#23398;&#20064;&#20195;&#34920;&#19979;&#28216;&#20219;&#21153;&#30340;&#29305;&#24449;&#12290;&#22240;&#27492;&#65292;&#23613;&#31649;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#21644;&#21307;&#38498;&#20869;&#37096;&#21487;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#65292;&#22823;&#22810;&#25968;&#30417;&#30563;&#27169;&#22411;&#20173;&#34920;&#29616;&#19981;&#20339;&#12290;&#32780;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#22312;&#33258;&#28982;&#22330;&#26223;&#25968;&#25454;&#20013;&#23545;&#35813;&#38382;&#39064;&#20570;&#20102;&#19968;&#23450;&#31243;&#24230;&#30340;&#35299;&#20915;&#65292;&#20294;&#22312;&#21307;&#23398;&#22270;&#20687;&#39046;&#22495;&#23384;&#22312;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25506;&#32034;&#22522;&#20110;&#34917;&#19969;&#30340;&#23454;&#20363;&#32452;&#21306;&#20998;&#21644;&#20351;&#29992;&#20313;&#24358;&#30456;&#20284;&#24230;&#24230;&#37327;&#20013;&#30340;&#21152;&#24615;&#35282;&#24230;&#36793;&#36317;&#23545;&#36328;&#31867;&#21035;&#21464;&#21270;&#30340;&#24809;&#32602;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;&#20351;&#27169;&#22411;&#33021;&#22815;&#26356;&#22909;&#22320;&#23398;&#20064;&#20195;&#34920;&#19979;&#28216;&#20219;&#21153;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20869;&#31397;&#38236;&#22270;&#20687;&#20998;&#26512;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven methods have shown tremendous progress in medical image analysis. In this context, deep learning-based supervised methods are widely popular. However, they require a large amount of training data and face issues in generalisability to unseen datasets that hinder clinical translation. Endoscopic imaging data incorporates large inter- and intra-patient variability that makes these models more challenging to learn representative features for downstream tasks. Thus, despite the publicly available datasets and datasets that can be generated within hospitals, most supervised models still underperform. While self-supervised learning has addressed this problem to some extent in natural scene data, there is a considerable performance gap in the medical image domain. In this paper, we propose to explore patch-level instance-group discrimination and penalisation of inter-class variation using additive angular margin within the cosine similarity metrics. Our novel approach enables mode
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#23558;&#20219;&#20309;&#21333;&#33218;&#31574;&#30053;&#36716;&#21270;&#20026;&#21407;&#22987;&#30340;$N$&#33218;&#38382;&#39064;&#30340;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#20381;&#36182;&#20110;&#22797;&#26434;UGAP&#20551;&#35774;&#30340;&#38382;&#39064;&#65292;&#24182;&#23454;&#29616;&#20102;&#20855;&#26377;$O(1/\sqrt{N})$&#26368;&#20248;&#24615;&#24046;&#36317;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2306.00196</link><description>&lt;p&gt;
&#20855;&#26377;&#24179;&#22343;&#22870;&#21169;&#30340;&#19981;&#23433;&#23450;&#36172;&#24466;&#38382;&#39064;&#65306;&#25171;&#30772;&#32479;&#19968;&#20840;&#23616;&#24341;&#23376;&#20551;&#35774;
&lt;/p&gt;
&lt;p&gt;
Restless Bandits with Average Reward: Breaking the Uniform Global Attractor Assumption. (arXiv:2306.00196v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00196
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#23558;&#20219;&#20309;&#21333;&#33218;&#31574;&#30053;&#36716;&#21270;&#20026;&#21407;&#22987;&#30340;$N$&#33218;&#38382;&#39064;&#30340;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#20381;&#36182;&#20110;&#22797;&#26434;UGAP&#20551;&#35774;&#30340;&#38382;&#39064;&#65292;&#24182;&#23454;&#29616;&#20102;&#20855;&#26377;$O(1/\sqrt{N})$&#26368;&#20248;&#24615;&#24046;&#36317;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#24179;&#22343;&#22870;&#21169;&#26631;&#20934;&#19979;&#30340;&#26080;&#38480;&#26102;&#19981;&#23433;&#23450;&#36172;&#24466;&#38382;&#39064;&#65292;&#21253;&#25324;&#31163;&#25955;&#26102;&#38388;&#21644;&#36830;&#32493;&#26102;&#38388;&#35774;&#32622;&#12290;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#26159;&#22914;&#20309;&#35774;&#35745;&#35745;&#31639;&#26377;&#25928;&#30340;&#31574;&#30053;&#65292;&#20351;&#24471;&#20248;&#21270;&#24046;&#36317;&#38543;&#30528;&#33218;&#30340;&#25968;&#37327;$N$&#30340;&#22686;&#21152;&#32780;&#20943;&#23567;&#12290;&#29616;&#26377;&#30340;&#28176;&#36817;&#26368;&#20248;&#24615;&#32467;&#26524;&#37117;&#20381;&#36182;&#20110;&#32479;&#19968;&#20840;&#23616;&#24341;&#23376;&#24615;&#36136;(UGAP)&#65292;&#36825;&#26159;&#19968;&#20010;&#22797;&#26434;&#19988;&#38590;&#20197;&#39564;&#35777;&#30340;&#20551;&#35774;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#12289;&#22522;&#20110;&#27169;&#25311;&#30340;&#26694;&#26550;&#65292;&#23558;&#20219;&#20309;&#21333;&#33218;&#31574;&#30053;&#36716;&#21270;&#20026;&#21407;&#22987;&#30340;$N$&#33218;&#38382;&#39064;&#30340;&#31574;&#30053;&#12290;&#36825;&#26159;&#36890;&#36807;&#22312;&#27599;&#20010;&#33218;&#19978;&#27169;&#25311;&#21333;&#33218;&#31574;&#30053;&#65292;&#24182;&#20180;&#32454;&#22320;&#23558;&#30495;&#23454;&#29366;&#24577;&#24341;&#23548;&#21521;&#27169;&#25311;&#29366;&#24577;&#26469;&#23454;&#29616;&#30340;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#23454;&#20363;&#21270;&#65292;&#20135;&#29983;&#19968;&#20010;&#20855;&#26377;$O(1/\sqrt{N})$&#30340;&#26368;&#20248;&#35299;&#24046;&#36317;&#30340;&#31574;&#30053;&#12290;&#22312;&#31163;&#25955;&#26102;&#38388;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#22312;&#26356;&#31616;&#21333;&#30340;&#21516;&#27493;&#20551;&#35774;&#19979;&#25104;&#31435;&#65292;&#28085;&#30422;&#20102;&#19968;&#20123;&#19981;&#28385;&#36275;UGAP&#30340;&#38382;&#39064;&#23454;&#20363;&#12290;&#26356;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#22788;&#29702;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22823;&#30340;&#38382;&#39064;&#31867;&#65292;&#32780;&#19981;&#38656;&#23545;&#38382;&#39064;&#23454;&#20363;&#20570;&#20219;&#20309;&#29305;&#23450;&#30340;&#32467;&#26500;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the infinite-horizon restless bandit problem with the average reward criterion, under both discrete-time and continuous-time settings. A fundamental question is how to design computationally efficient policies that achieve a diminishing optimality gap as the number of arms, $N$, grows large. Existing results on asymptotical optimality all rely on the uniform global attractor property (UGAP), a complex and challenging-to-verify assumption. In this paper, we propose a general, simulation-based framework that converts any single-armed policy into a policy for the original $N$-armed problem. This is accomplished by simulating the single-armed policy on each arm and carefully steering the real state towards the simulated state. Our framework can be instantiated to produce a policy with an $O(1/\sqrt{N})$ optimality gap. In the discrete-time setting, our result holds under a simpler synchronization assumption, which covers some problem instances that do not satisfy UGAP. More notabl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#21629;&#21608;&#26399;DRL&#26694;&#26550;SERIL&#65292;&#22312;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#20013;&#23454;&#29616;&#20102;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#25104;&#20687;&#29615;&#22659;&#20013;&#25345;&#32493;&#23398;&#20064;&#30340;&#30446;&#30340;&#65292;&#23637;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.00188</link><description>&lt;p&gt;
&#22810;&#29615;&#22659;&#29983;&#21629;&#21608;&#26399;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Multi-environment lifelong deep reinforcement learning for medical imaging. (arXiv:2306.00188v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00188
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#21629;&#21608;&#26399;DRL&#26694;&#26550;SERIL&#65292;&#22312;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#20013;&#23454;&#29616;&#20102;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#25104;&#20687;&#29615;&#22659;&#20013;&#25345;&#32493;&#23398;&#20064;&#30340;&#30446;&#30340;&#65292;&#23637;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#22312;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#22810;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25104;&#20687;&#26041;&#21521;&#12289;&#25104;&#20687;&#24207;&#21015;&#21644;&#30149;&#29702;&#23398;&#30340;&#21464;&#21270;&#65292;&#21307;&#23398;&#24433;&#20687;&#20219;&#21153;&#30340;&#29615;&#22659;&#19981;&#26029;&#28436;&#21464;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#29983;&#21629;&#21608;&#26399;DRL&#26694;&#26550;SERIL&#65292;&#37319;&#29992;&#22522;&#20110;&#36873;&#25321;&#24615;&#20307;&#39564;&#22238;&#25918;&#30340;&#29983;&#21629;&#21608;&#26399;&#23398;&#20064;&#25216;&#26415;&#65292;&#22312;24&#20010;&#19981;&#21516;&#30340;&#25104;&#20687;&#29615;&#22659;&#20013;&#36827;&#34892;&#20154;&#33041;MRI&#19978;&#20116;&#20010;&#35299;&#21078;&#26631;&#24535;&#30340;&#23450;&#20301;&#12290;&#19982;&#20004;&#20010;&#22522;&#20934;&#35774;&#32622;MERT&#65288;&#22810;&#29615;&#22659;&#26368;&#20339;&#24773;&#20917;&#65289;&#21644;SERT&#65288;&#21333;&#29615;&#22659;&#26368;&#21155;&#24773;&#20917;&#65289;&#30456;&#27604;&#65292;SERIL&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#22312;&#25152;&#26377;120&#20010;&#20219;&#21153;&#20013;&#65292;&#19982;&#26399;&#26395;&#30340;&#26631;&#24535;&#36317;&#31163;&#30340;&#24179;&#22343;&#36317;&#31163;&#20026;$9.90\pm7.35$&#20687;&#32032;&#65292;&#32780;MERT&#21644;SERT&#20998;&#21035;&#20026;$10.29\pm9.07$&#21644;$36.37\pm22.41$($p&lt;0.05$)&#65292;&#23637;&#31034;&#20102;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#25104;&#20687;&#29615;&#22659;&#20013;&#25345;&#32493;&#23398;&#20064;&#30340;&#20986;&#33394;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning(DRL) is increasingly being explored in medical imaging. However, the environments for medical imaging tasks are constantly evolving in terms of imaging orientations, imaging sequences, and pathologies. To that end, we developed a Lifelong DRL framework, SERIL to continually learn new tasks in changing imaging environments without catastrophic forgetting. SERIL was developed using selective experience replay based lifelong learning technique for the localization of five anatomical landmarks in brain MRI on a sequence of twenty-four different imaging environments. The performance of SERIL, when compared to two baseline setups: MERT(multi-environment-best-case) and SERT(single-environment-worst-case) demonstrated excellent performance with an average distance of $9.90\pm7.35$ pixels from the desired landmark across all 120 tasks, compared to $10.29\pm9.07$ for MERT and $36.37\pm22.41$ for SERT($p&lt;0.05$), demonstrating the excellent potential for continuously le
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#29616;&#39044;&#35757;&#32451;&#34920;&#31034;&#20013;&#23384;&#22312;&#19968;&#23450;&#31243;&#24230;&#30340;&#25193;&#25955;&#20887;&#20313;&#65292;&#21363;&#38543;&#26426;&#36873;&#25321;&#19968;&#37096;&#20998;&#31070;&#32463;&#20803;&#19982;&#20840;&#37096;&#31070;&#32463;&#20803;&#30456;&#20284;&#24230;&#39640;&#19988;&#24615;&#33021;&#30456;&#20284;&#65292;&#21487;&#29992;&#20110;&#38477;&#20302;&#23454;&#38469;&#37096;&#32626;&#20013;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#30456;&#24403;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2306.00183</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#34920;&#31034;&#20013;&#30340;&#25193;&#25955;&#20887;&#20313;
&lt;/p&gt;
&lt;p&gt;
Diffused Redundancy in Pre-trained Representations. (arXiv:2306.00183v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00183
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#29616;&#39044;&#35757;&#32451;&#34920;&#31034;&#20013;&#23384;&#22312;&#19968;&#23450;&#31243;&#24230;&#30340;&#25193;&#25955;&#20887;&#20313;&#65292;&#21363;&#38543;&#26426;&#36873;&#25321;&#19968;&#37096;&#20998;&#31070;&#32463;&#20803;&#19982;&#20840;&#37096;&#31070;&#32463;&#20803;&#30456;&#20284;&#24230;&#39640;&#19988;&#24615;&#33021;&#30456;&#20284;&#65292;&#21487;&#29992;&#20110;&#38477;&#20302;&#23454;&#38469;&#37096;&#32626;&#20013;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#30456;&#24403;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#33719;&#24471;&#30340;&#34920;&#31034;&#24050;&#34987;&#36234;&#26469;&#36234;&#22810;&#22320;&#25104;&#21151;&#24212;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26356;&#21152;&#28145;&#20837;&#22320;&#30740;&#31350;&#20102;&#36825;&#31181;&#39044;&#35757;&#32451;&#34920;&#31034;&#20013;&#30340;&#29305;&#24449;&#26159;&#22914;&#20309;&#34987;&#32534;&#30721;&#30340;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#32473;&#23450;&#23618;&#20013;&#23398;&#21040;&#30340;&#34920;&#31034;&#23637;&#29616;&#20986;&#19968;&#23450;&#31243;&#24230;&#30340;&#25193;&#25955;&#20887;&#20313;&#65292;&#21363;&#23545;&#20110;&#36229;&#36807;&#19968;&#20010;&#38408;&#20540;&#22823;&#23567;&#30340;&#20219;&#20309;&#38543;&#26426;&#23376;&#38598;&#31070;&#32463;&#20803;&#65292;&#37117;&#19982;&#23436;&#25972;&#23618;&#20855;&#26377;&#24456;&#39640;&#30340;&#30456;&#20284;&#24230;&#65292;&#24182;&#19988;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#33021;&#22815;&#34920;&#29616;&#20986;&#19982;&#25972;&#20010;&#23618;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#19981;&#21516;&#30340;&#31070;&#32463;&#26550;&#26500;&#65288;&#21253;&#25324;CNN&#21644;Transformer&#65289;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#20351;&#29992;&#20102;ImageNet1k&#21644;ImageNet21k&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#35780;&#20272;&#20102;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#65292;&#22914;&#22270;&#20687;&#20998;&#31867;&#12289;&#30446;&#26631;&#26816;&#27979;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21487;&#20197;&#21033;&#29992;&#39044;&#35757;&#32451;&#34920;&#31034;&#20013;&#30340;&#20887;&#20313;&#26469;&#38477;&#20302;&#22312;&#23454;&#38469;&#37096;&#32626;&#20013;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#65292;&#21516;&#26102;&#20173;&#28982;&#20445;&#25345;&#30456;&#24403;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representations learned by pre-training a neural network on a large dataset are increasingly used successfully to perform a variety of downstream tasks. In this work, we take a closer look at how features are encoded in such pre-trained representations. We find that learned representations in a given layer exhibit a degree of diffuse redundancy, i.e., any randomly chosen subset of neurons in the layer that is larger than a threshold size shares a large degree of similarity with the full layer and is able to perform similarly as the whole layer on a variety of downstream tasks. For example, a linear probe trained on $20\%$ of randomly picked neurons from a ResNet50 pre-trained on ImageNet1k achieves an accuracy within $5\%$ of a linear probe trained on the full layer of neurons for downstream CIFAR10 classification. We conduct experiments on different neural architectures (including CNNs and Transformers) pre-trained on both ImageNet1k and ImageNet21k and evaluate a variety of downstrea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21487;&#24494;&#20998;&#28210;&#26579;&#23558;&#24103;&#38388;&#20809;&#27969;&#26144;&#23556;&#21040;&#19977;&#32500;&#22330;&#26223;&#27969;&#20197;&#21516;&#27493;&#37325;&#24314;&#30456;&#26426;&#20301;&#23039;&#21644;&#19977;&#32500;&#31070;&#32463;&#22330;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#38656;&#35201;&#31934;&#30830;&#30456;&#26426;&#20301;&#23039;&#65292;&#21487;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#35270;&#39057;&#25968;&#25454;&#19978;&#36827;&#34892;&#20840;&#33258;&#30417;&#30563;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2306.00180</link><description>&lt;p&gt;
FlowCam: &#36890;&#36807;&#20687;&#32032;&#23545;&#40784;&#22330;&#26223;&#27969;&#65292;&#26080;&#38656;&#30456;&#26426;&#20301;&#23039;&#35757;&#32451;&#20855;&#26377;&#36890;&#29992;&#24615;&#30340;&#19977;&#32500;&#36752;&#23556;&#22330;
&lt;/p&gt;
&lt;p&gt;
FlowCam: Training Generalizable 3D Radiance Fields without Camera Poses via Pixel-Aligned Scene Flow. (arXiv:2306.00180v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00180
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21487;&#24494;&#20998;&#28210;&#26579;&#23558;&#24103;&#38388;&#20809;&#27969;&#26144;&#23556;&#21040;&#19977;&#32500;&#22330;&#26223;&#27969;&#20197;&#21516;&#27493;&#37325;&#24314;&#30456;&#26426;&#20301;&#23039;&#21644;&#19977;&#32500;&#31070;&#32463;&#22330;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#38656;&#35201;&#31934;&#30830;&#30456;&#26426;&#20301;&#23039;&#65292;&#21487;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#35270;&#39057;&#25968;&#25454;&#19978;&#36827;&#34892;&#20840;&#33258;&#30417;&#30563;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#23450;&#20301;&#22270;&#20687;&#20013;&#37325;&#24314;&#19977;&#32500;&#31070;&#32463;&#22330;&#24050;&#25104;&#20026;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#20294;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#35270;&#39057;&#25968;&#25454;&#19978;&#32467;&#26500;&#20809;&#23398;&#31934;&#30830;&#30456;&#26426;&#20301;&#23039;&#65292;&#36825;&#20063;&#26159;&#20854;&#26080;&#27861;&#25512;&#24191;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#22312;&#32447;&#37325;&#24314;&#30456;&#26426;&#20301;&#23039;&#21644;&#19977;&#32500;&#31070;&#32463;&#22330;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#21333;&#27425;&#21069;&#21521;&#20256;&#36882;&#26469;&#23436;&#25104;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21487;&#24494;&#20998;&#28210;&#26579;&#23558;&#24103;&#38388;&#20809;&#27969;&#26144;&#23556;&#21040;&#19977;&#32500;&#22330;&#26223;&#27969;&#65292;&#20197;&#20272;&#35745;&#20301;&#23039;&#12290;&#28982;&#21518;&#20877;&#36890;&#36807;&#23545;&#22330;&#26223;&#27969;&#22330;&#36827;&#34892;&#21152;&#26435;&#26368;&#23567;&#20108;&#20056;&#25311;&#21512;&#26469;&#25191;&#34892;SE&#65288;3&#65289;&#30456;&#26426;&#20301;&#23039;&#20272;&#35745;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#36890;&#36807;&#37325;&#26032;&#28210;&#26579;&#36755;&#20837;&#35270;&#39057;&#26469;&#32852;&#21512;&#30417;&#30563;&#20301;&#23039;&#20272;&#35745;&#21644;&#36890;&#29992;&#30340;&#31070;&#32463;&#22330;&#34920;&#31034;&#65292;&#20174;&#32780;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#35270;&#39057;&#25968;&#25454;&#19978;&#36827;&#34892;&#20840;&#33258;&#30417;&#30563;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reconstruction of 3D neural fields from posed images has emerged as a promising method for self-supervised representation learning. The key challenge preventing the deployment of these 3D scene learners on large-scale video data is their dependence on precise camera poses from structure-from-motion, which is prohibitively expensive to run at scale. We propose a method that jointly reconstructs camera poses and 3D neural scene representations online and in a single forward pass. We estimate poses by first lifting frame-to-frame optical flow to 3D scene flow via differentiable rendering, preserving locality and shift-equivariance of the image processing backbone. SE(3) camera pose estimation is then performed via a weighted least-squares fit to the scene flow field. This formulation enables us to jointly supervise pose estimation and a generalizable neural scene representation via re-rendering the input video, and thus, train end-to-end and fully self-supervised on real-world video datas
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;RL&#30340;&#36793;&#32536;&#21152;&#26435;&#22312;&#32447;&#20108;&#20998;&#22270;&#21305;&#37197;&#26041;&#27861;&#65288;LOMAR&#65289;&#65292;&#23454;&#29616;&#20102;&#22909;&#30340;&#24179;&#22343;&#24773;&#20917;&#21644;&#26368;&#22351;&#24773;&#20917;&#30340;&#24615;&#33021;&#65292;&#20854;&#20851;&#38190;&#26032;&#39062;&#20043;&#22788;&#22312;&#20110;&#20915;&#31574;&#26159;&#22522;&#20110;&#19968;&#20010;&#26032;&#30340;&#22312;&#32447;&#20999;&#25442;&#25805;&#20316;&#65292;&#35813;&#25805;&#20316;&#21487;&#20197;&#23545;&#25239;&#26410;&#26469;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.00172</link><description>&lt;p&gt;
&#20855;&#26377;&#40065;&#26834;&#24615;&#20445;&#35777;&#30340;&#24102;&#26435;&#22312;&#32447;&#20108;&#20998;&#22270;&#21305;&#37197;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning for Edge-Weighted Online Bipartite Matching with Robustness Guarantees. (arXiv:2306.00172v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00172
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;RL&#30340;&#36793;&#32536;&#21152;&#26435;&#22312;&#32447;&#20108;&#20998;&#22270;&#21305;&#37197;&#26041;&#27861;&#65288;LOMAR&#65289;&#65292;&#23454;&#29616;&#20102;&#22909;&#30340;&#24179;&#22343;&#24773;&#20917;&#21644;&#26368;&#22351;&#24773;&#20917;&#30340;&#24615;&#33021;&#65292;&#20854;&#20851;&#38190;&#26032;&#39062;&#20043;&#22788;&#22312;&#20110;&#20915;&#31574;&#26159;&#22522;&#20110;&#19968;&#20010;&#26032;&#30340;&#22312;&#32447;&#20999;&#25442;&#25805;&#20316;&#65292;&#35813;&#25805;&#20316;&#21487;&#20197;&#23545;&#25239;&#26410;&#26469;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#38382;&#39064;&#65292;&#22914;&#22312;&#32447;&#24191;&#21578;&#26174;&#31034;&#21487;&#20197;&#34987;&#20844;&#24335;&#21270;&#20026;&#22312;&#32447;&#20108;&#20998;&#22270;&#21305;&#37197;&#12290;&#20851;&#38190;&#30340;&#25361;&#25112;&#22312;&#20110;&#22522;&#20110;&#24207;&#21015;&#21270;&#20844;&#24067;&#30340;&#22312;&#32447;&#26465;&#30446;&#20449;&#24687;&#65292;&#25105;&#20204;&#22312;&#27599;&#20010;&#27493;&#39588;&#19978;&#20570;&#20986;&#19981;&#21487;&#36870;&#36716;&#30340;&#21305;&#37197;&#20915;&#31574;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#20855;&#26377;&#26377;&#30028;&#26368;&#22351;&#24773;&#20917;&#31454;&#20105;&#27604;&#30340;&#19987;&#23478;&#22312;&#32447;&#31639;&#27861;&#65292;&#20294;&#23427;&#20204;&#22312;&#24179;&#22343;&#24773;&#20917;&#19979;&#21487;&#33021;&#26080;&#27861;&#25552;&#20379;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24050;&#34987;&#24212;&#29992;&#20110;&#25552;&#39640;&#24179;&#22343;&#24615;&#33021;&#65292;&#20294;&#32570;&#20047;&#40065;&#26834;&#24615;&#65292;&#21487;&#33021;&#34920;&#29616;&#24471;&#20219;&#24847;&#24046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;RL&#30340;&#20855;&#26377;&#40065;&#26834;&#24615;&#20445;&#35777;&#30340;&#36793;&#32536;&#21152;&#26435;&#22312;&#32447;&#20108;&#20998;&#22270;&#21305;&#37197;&#26041;&#27861;&#65288;LOMAR&#65289;&#65292;&#23454;&#29616;&#20102;&#22909;&#30340;&#24179;&#22343;&#24773;&#20917;&#21644;&#26368;&#22351;&#24773;&#20917;&#30340;&#24615;&#33021;&#12290; LOMAR&#30340;&#20851;&#38190;&#26032;&#39062;&#20043;&#22788;&#22312;&#20110;&#19968;&#31181;&#26032;&#30340;&#22312;&#32447;&#20999;&#25442;&#25805;&#20316;&#65292;&#35813;&#25805;&#20316;&#22522;&#20110;&#35880;&#24910;&#30340;&#26465;&#20214;&#26469;&#23545;&#25239;&#26410;&#26469;&#19981;&#30830;&#23450;&#24615;&#65292;&#20915;&#23450;&#26159;&#21542;&#26681;&#25454;&#27599;&#20010;&#22312;&#32447;&#39033;&#30446;&#36981;&#24490;&#19987;&#23478;&#20915;&#31574;&#36824;&#26159;RL&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many problems, such as online ad display, can be formulated as online bipartite matching. The crucial challenge lies in the nature of sequentially-revealed online item information, based on which we make irreversible matching decisions at each step. While numerous expert online algorithms have been proposed with bounded worst-case competitive ratios, they may not offer satisfactory performance in average cases. On the other hand, reinforcement learning (RL) has been applied to improve the average performance, but it lacks robustness and can perform arbitrarily poorly. In this paper, we propose a novel RL-based approach to edge-weighted online bipartite matching with robustness guarantees (LOMAR), achieving both good average-case and worst-case performance. The key novelty of LOMAR is a new online switching operation which, based on a judicious condition to hedge against future uncertainties, decides whether to follow the expert's decision or the RL decision for each online item. We pro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30340;&#29702;&#35770;&#20998;&#26512;&#19982;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#27169;&#22411;&#36755;&#20986;&#30340;&#19981;&#19968;&#33268;&#24615;&#21644;&#19981;&#31283;&#23450;&#24615;&#21487;&#20197;&#20316;&#20026;&#20272;&#35745;&#27867;&#21270;&#38388;&#38553;&#30340;&#37325;&#35201;&#25351;&#26631;&#65292;&#28040;&#38500;&#19981;&#19968;&#33268;&#24615;&#30340;&#31639;&#27861;&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.00169</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#19981;&#19968;&#33268;&#24615;&#12289;&#19981;&#31283;&#23450;&#24615;&#21644;&#27867;&#21270;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Inconsistency, Instability, and Generalization Gap of Deep Neural Network Training. (arXiv:2306.00169v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00169
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30340;&#29702;&#35770;&#20998;&#26512;&#19982;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#27169;&#22411;&#36755;&#20986;&#30340;&#19981;&#19968;&#33268;&#24615;&#21644;&#19981;&#31283;&#23450;&#24615;&#21487;&#20197;&#20316;&#20026;&#20272;&#35745;&#27867;&#21270;&#38388;&#38553;&#30340;&#37325;&#35201;&#25351;&#26631;&#65292;&#28040;&#38500;&#19981;&#19968;&#33268;&#24615;&#30340;&#31639;&#27861;&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#24456;&#39640;&#30340;&#34920;&#29616;&#21147;&#65292;&#23547;&#25214;&#20855;&#26377;&#23567;&#27867;&#21270;&#24046;&#36317;&#30340;&#35299;&#20915;&#26041;&#26696;&#21464;&#24471;&#24456;&#37325;&#35201;&#65288;&#21363;&#22312;&#35757;&#32451;&#25968;&#25454;&#21644;&#26410;&#35265;&#25968;&#25454;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#65289;&#12290;&#26412;&#25991;&#36890;&#36807;&#20851;&#27880;&#35757;&#32451;&#30340;&#38543;&#26426;&#24615;&#65292;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#20998;&#26512;&#65292;&#20854;&#20013;&#27867;&#21270;&#38388;&#38553;&#30340;&#30028;&#38480;&#21462;&#20915;&#20110;&#25105;&#20204;&#31216;&#20043;&#20026;&#27169;&#22411;&#36755;&#20986;&#30340;&#19981;&#19968;&#33268;&#24615;&#21644;&#19981;&#31283;&#23450;&#24615;&#65292;&#36825;&#21487;&#20197;&#22312;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#20272;&#35745;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#22522;&#20110;&#36825;&#20010;&#20998;&#26512;&#65292;&#34920;&#26126;&#22312;&#21508;&#31181;&#35774;&#32622;&#20013;&#65292;&#19981;&#31283;&#23450;&#24615;&#21644;&#19981;&#19968;&#33268;&#24615;&#24378;&#28872;&#39044;&#31034;&#30528;&#27867;&#21270;&#38388;&#38553;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#19981;&#19968;&#33268;&#24615;&#27604;&#25439;&#22833;&#21464;&#21270;&#30340;&#38160;&#24230;&#26356;&#21487;&#38752;&#22320;&#25351;&#31034;&#30528;&#27867;&#21270;&#38388;&#38553;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#28040;&#38500;&#19981;&#19968;&#33268;&#24615;&#30340;&#31639;&#27861;&#21487;&#20197;&#24102;&#26469;&#26356;&#20248;&#24322;&#30340;&#34920;&#29616;&#12290;&#36825;&#20123;&#32467;&#26524;&#36824;&#20026;&#29616;&#26377;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#29702;&#35770;&#22522;&#30784;&#65292;&#22914;&#20849;&#21516;&#33976;&#39311;&#21644;&#38598;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
As deep neural networks are highly expressive, it is important to find solutions with small generalization gap (the difference between the performance on the training data and unseen data). Focusing on the stochastic nature of training, we first present a theoretical analysis in which the bound of generalization gap depends on what we call inconsistency and instability of model outputs, which can be estimated on unlabeled data. Our empirical study based on this analysis shows that instability and inconsistency are strongly predictive of generalization gap in various settings. In particular, our finding indicates that inconsistency is a more reliable indicator of generalization gap than the sharpness of the loss landscape. Furthermore, we show that algorithmic reduction of inconsistency leads to superior performance. The results also provide a theoretical basis for existing methods such as co-distillation and ensemble.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#21517;&#31216;&#20026;AVLIT&#30340;&#36731;&#37327;&#32423;&#36845;&#20195;&#27169;&#22411;&#65292;&#37319;&#29992;&#28176;&#36827;&#24335;&#23398;&#20064;&#22312;&#22024;&#26434;&#29615;&#22659;&#20013;&#23454;&#29616;&#38899;&#35270;&#39057;&#35821;&#38899;&#30340;&#20998;&#31163;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#22312;&#20004;&#31181;&#24773;&#22659;&#19979;&#30340;&#38899;&#39057;&#20998;&#31163;&#21644;&#38899;&#35270;&#39057;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#20248;&#36234;&#24615;&#65292;&#21516;&#26102;&#20854;&#20943;&#23567;&#30340;&#27169;&#22411;&#22823;&#23567;&#20351;&#24471;&#20854;&#36866;&#29992;&#20110;&#20302;&#36164;&#28304;&#24212;&#29992;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2306.00160</link><description>&lt;p&gt;
&#22312;&#22024;&#26434;&#29615;&#22659;&#19979;&#21033;&#29992;&#36731;&#37327;&#32423;&#36845;&#20195;&#27169;&#22411;&#36827;&#34892;&#38899;&#35270;&#39057;&#35821;&#38899;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Audio-Visual Speech Separation in Noisy Environments with a Lightweight Iterative Model. (arXiv:2306.00160v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#21517;&#31216;&#20026;AVLIT&#30340;&#36731;&#37327;&#32423;&#36845;&#20195;&#27169;&#22411;&#65292;&#37319;&#29992;&#28176;&#36827;&#24335;&#23398;&#20064;&#22312;&#22024;&#26434;&#29615;&#22659;&#20013;&#23454;&#29616;&#38899;&#35270;&#39057;&#35821;&#38899;&#30340;&#20998;&#31163;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#22312;&#20004;&#31181;&#24773;&#22659;&#19979;&#30340;&#38899;&#39057;&#20998;&#31163;&#21644;&#38899;&#35270;&#39057;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#20248;&#36234;&#24615;&#65292;&#21516;&#26102;&#20854;&#20943;&#23567;&#30340;&#27169;&#22411;&#22823;&#23567;&#20351;&#24471;&#20854;&#36866;&#29992;&#20110;&#20302;&#36164;&#28304;&#24212;&#29992;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; AVLIT &#30340;&#38899;&#35270;&#39057;&#36731;&#37327;&#32423;&#36845;&#20195;&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#28176;&#36827;&#24335;&#23398;&#20064;&#26469;&#22312;&#22024;&#26434;&#29615;&#22659;&#20013;&#36827;&#34892;&#38899;&#35270;&#39057;&#35821;&#38899;&#20998;&#31163;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#24322;&#27493;&#20840;&#24490;&#29615;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;A-FRCNN&#65289;&#65292;&#23427;&#22312;&#20165;&#21033;&#29992;&#38899;&#39057;&#36827;&#34892;&#35821;&#38899;&#20998;&#31163;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#25104;&#21151;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#30001;&#38899;&#39057;&#20998;&#25903;&#21644;&#35270;&#39057;&#20998;&#25903;&#32452;&#25104;&#65292;&#27599;&#20010;&#27169;&#24577;&#30340;&#36845;&#20195; A-FRCNN &#22359;&#20849;&#20139;&#26435;&#37325;&#12290;&#25105;&#20204;&#20351;&#29992; NTCD-TIMIT &#25968;&#25454;&#38598;&#22312;&#21463;&#25511;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#27169;&#22411;&#35780;&#20272;&#65292;&#24182;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#38598; LRS3 &#21644; WHAM! &#36827;&#34892;&#20102;&#37326;&#22806;&#27979;&#35797;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#21508;&#31181;&#38899;&#39057;&#20998;&#31163;&#21644;&#38899;&#35270;&#39057;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20004;&#31181;&#24773;&#20917;&#19979;&#22343;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#27169;&#22411;&#30340;&#20943;&#23569;&#30340;&#21344;&#29992;&#31354;&#38388;&#20351;&#20854;&#36866;&#29992;&#20110;&#20302;&#36164;&#28304;&#24212;&#29992;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Audio-Visual Lightweight ITerative model (AVLIT), an effective and lightweight neural network that uses Progressive Learning (PL) to perform audio-visual speech separation in noisy environments. To this end, we adopt the Asynchronous Fully Recurrent Convolutional Neural Network (A-FRCNN), which has shown successful results in audio-only speech separation. Our architecture consists of an audio branch and a video branch, with iterative A-FRCNN blocks sharing weights for each modality. We evaluated our model in a controlled environment using the NTCD-TIMIT dataset and in-the-wild using a synthetic dataset that combines LRS3 and WHAM!. The experiments demonstrate the superiority of our model in both settings with respect to various audio-only and audio-visual baselines. Furthermore, the reduced footprint of our model makes it suitable for low resource applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25945;&#31243;&#35770;&#25991;&#20171;&#32461;&#20102;&#31526;&#21495;&#22238;&#24402;&#25216;&#26415;&#65292;&#20197;&#20154;&#31867;&#20581;&#24247;&#21644;&#33829;&#20859;&#39046;&#22495;&#30340;&#23454;&#38469;&#24212;&#29992;&#20026;&#20363;&#65292;&#23558;&#21508;&#31181;&#20154;&#20307;&#27979;&#37327;&#25351;&#26631;&#34701;&#21512;&#25104;&#19968;&#20010;&#31616;&#21333;&#30340;&#25968;&#23398;&#34920;&#36798;&#24335;&#20197;&#20272;&#31639;&#20307;&#33026;&#30334;&#20998;&#27604;&#12290;</title><link>http://arxiv.org/abs/2306.00153</link><description>&lt;p&gt;
&#22522;&#20110;&#31526;&#21495;&#22238;&#24402;&#30340;&#20449;&#24687;&#34701;&#21512;&#65306;&#20197;&#20154;&#31867;&#20581;&#24247;&#20026;&#32972;&#26223;&#30340;&#25945;&#31243;
&lt;/p&gt;
&lt;p&gt;
Information Fusion via Symbolic Regression: A Tutorial in the Context of Human Health. (arXiv:2306.00153v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00153
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25945;&#31243;&#35770;&#25991;&#20171;&#32461;&#20102;&#31526;&#21495;&#22238;&#24402;&#25216;&#26415;&#65292;&#20197;&#20154;&#31867;&#20581;&#24247;&#21644;&#33829;&#20859;&#39046;&#22495;&#30340;&#23454;&#38469;&#24212;&#29992;&#20026;&#20363;&#65292;&#23558;&#21508;&#31181;&#20154;&#20307;&#27979;&#37327;&#25351;&#26631;&#34701;&#21512;&#25104;&#19968;&#20010;&#31616;&#21333;&#30340;&#25968;&#23398;&#34920;&#36798;&#24335;&#20197;&#20272;&#31639;&#20307;&#33026;&#30334;&#20998;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25945;&#31243;&#35770;&#25991;&#25552;&#20379;&#20102;&#31526;&#21495;&#22238;&#24402;&#65288;SR&#65289;&#30340;&#19968;&#33324;&#27010;&#36848;&#65292;&#37325;&#28857;&#20851;&#27880;&#21487;&#35299;&#37322;&#24615;&#26631;&#20934;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#23613;&#31649;&#25991;&#29486;&#20013;&#23545;&#21487;&#35299;&#37322;&#24615;&#24314;&#27169;&#30340;&#23450;&#20041;&#20173;&#26377;&#20105;&#35758;&#65292;&#20294;&#21487;&#35299;&#37322;&#24615;&#27169;&#22411;&#26159;&#25903;&#25345;&#35780;&#20272;&#25104;&#21151;&#20449;&#24687;&#34701;&#21512;&#30340;&#23454;&#29992;&#26041;&#24335;&#12290;&#20026;&#20102;&#20256;&#36798;SR&#20316;&#20026;&#24314;&#27169;&#25216;&#26415;&#30340;&#20248;&#28857;&#65292;&#25105;&#20204;&#21033;&#29992;&#26469;&#33258;&#30142;&#30149;&#25511;&#21046;&#21644;&#39044;&#38450;&#20013;&#24515;&#65288;CDC&#65289;&#30340;&#20844;&#24320;&#21487;&#29992;&#30340;&#22269;&#23478;&#20581;&#24247;&#21644;&#33829;&#20859;&#35843;&#26597;&#65288;NHANES&#65289;&#25968;&#25454;&#65292;&#22312;&#20581;&#24247;&#21644;&#33829;&#20859;&#39046;&#22495;&#23637;&#31034;&#20102;&#19968;&#20010;&#24212;&#29992;&#65292;&#23558;&#21508;&#31181;&#20154;&#20307;&#27979;&#37327;&#25351;&#26631;&#34701;&#21512;&#25104;&#19968;&#20010;&#31616;&#21333;&#30340;&#25968;&#23398;&#34920;&#36798;&#24335;&#20197;&#20272;&#31639;&#20307;&#33026;&#30334;&#20998;&#27604;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;SR&#24314;&#27169;&#30340;&#20248;&#28857;&#21644;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#25152;&#23398;&#27169;&#22411;&#30340;&#23450;&#24615;&#21644;&#23450;&#37327;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
This tutorial paper provides a general overview of symbolic regression (SR) with specific focus on standards of interpretability. We posit that interpretable modeling, although its definition is still disputed in the literature, is a practical way to support the evaluation of successful information fusion. In order to convey the benefits of SR as a modeling technique, we demonstrate an application within the field of health and nutrition using publicly available National Health and Nutrition Examination Survey (NHANES) data from the Centers for Disease Control and Prevention (CDC), fusing together anthropometric markers into a simple mathematical expression to estimate body fat percentage. We discuss the advantages and challenges associated with SR modeling and provide qualitative and quantitative analyses of the learned models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;-free&#30340;&#25289;&#26222;&#25289;&#26031;&#27491;&#21017;&#21270;&#27169;&#22411;&#65292;&#33021;&#22815;&#26080;&#38656;&#39044;&#20808;&#35780;&#20272;&#23618;&#30340;&#36129;&#29486;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#26368;&#20339;&#30340;&#38750;&#32447;&#24615;&#23618;&#32423;&#32452;&#21512;&#26469;&#23454;&#29616;&#22522;&#20110;&#22810;&#23618;&#22270;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2306.00152</link><description>&lt;p&gt;
&#23398;&#20064;&#27491;&#30830;&#30340;&#23618;&#32423;&#65306;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#30340;&#22810;&#23618;&#22270;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#23618;&#32423;&#32858;&#21512;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Learning the Right Layers: a Data-Driven Layer-Aggregation Strategy for Semi-Supervised Learning on Multilayer Graphs. (arXiv:2306.00152v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;-free&#30340;&#25289;&#26222;&#25289;&#26031;&#27491;&#21017;&#21270;&#27169;&#22411;&#65292;&#33021;&#22815;&#26080;&#38656;&#39044;&#20808;&#35780;&#20272;&#23618;&#30340;&#36129;&#29486;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#26368;&#20339;&#30340;&#38750;&#32447;&#24615;&#23618;&#32423;&#32452;&#21512;&#26469;&#23454;&#29616;&#22522;&#20110;&#22810;&#23618;&#22270;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#23618;&#22270;&#19978;&#36827;&#34892;&#32858;&#31867;&#65288;&#25110;&#31038;&#21306;&#26816;&#27979;&#65289;&#20250;&#38754;&#20020;&#19968;&#20123;&#39069;&#22806;&#30340;&#22797;&#26434;&#24615;&#38382;&#39064;&#65292;&#22240;&#20026;&#19981;&#21516;&#30340;&#23618;&#21487;&#20197;&#36890;&#36807;&#19981;&#21516;&#30340;&#32467;&#26500;&#21644;&#20449;&#24687;&#31867;&#22411;&#36827;&#34892;&#29305;&#24449;&#21270;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#30340;&#25361;&#25112;&#26159;&#30830;&#23450;&#27599;&#20010;&#23618;&#23545;&#20110;&#32676;&#38598;&#20998;&#37197;&#30340;&#36129;&#29486;&#31243;&#24230;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#21033;&#29992;&#22810;&#23618;&#32467;&#26500;&#65292;&#24182;&#25913;&#36827;&#20351;&#29992;&#21333;&#20010;&#23618;&#25110;&#23427;&#20204;&#30340;&#32852;&#21512;&#33719;&#24471;&#30340;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#23618;&#30340;&#32858;&#31867;&#20449;&#24687;&#20869;&#23481;&#20570;&#20986;&#30693;&#24773;&#30340;&#39044;&#20808;&#35780;&#20272;&#21487;&#33021;&#38750;&#24120;&#22797;&#26434;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20551;&#35774;&#21322;&#30417;&#30563;&#23398;&#20064;&#29615;&#22659;&#65292;&#22312;&#26368;&#21021;&#25552;&#20379;&#23569;&#37327;&#33410;&#28857;&#30340;&#31867;&#21035;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#20986;&#19968;&#20010;&#26080;&#21442;&#25968;&#30340;&#25289;&#26222;&#25289;&#26031;&#27491;&#21017;&#21270;&#27169;&#22411;&#65292;&#36890;&#36807;&#21487;&#29992;&#36755;&#20837;&#26631;&#31614;&#23398;&#20064;&#19981;&#21516;&#23618;&#20043;&#38388;&#30340;&#26368;&#20339;&#38750;&#32447;&#24615;&#32452;&#21512;&#12290;&#23398;&#20064;&#31639;&#27861;&#22522;&#20110;&#24102;&#26377;&#19981;&#31934;&#30830;&#26799;&#24230;&#30340;Frank-Wolfe&#20248;&#21270;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clustering (or community detection) on multilayer graphs poses several additional complications with respect to standard graphs as different layers may be characterized by different structures and types of information. One of the major challenges is to establish the extent to which each layer contributes to the cluster assignment in order to effectively take advantage of the multilayer structure and improve upon the classification obtained using the individual layers or their union. However, making an informed a-priori assessment about the clustering information content of the layers can be very complicated. In this work, we assume a semi-supervised learning setting, where the class of a small percentage of nodes is initially provided, and we propose a parameter-free Laplacian-regularized model that learns an optimal nonlinear combination of the different layers from the available input labels. The learning algorithm is based on a Frank-Wolfe optimization scheme with inexact gradient, 
&lt;/p&gt;</description></item><item><title>SafeDiffuser&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#35299;&#20915;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#22312;&#23433;&#20840;&#24615;&#19978;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#31867;&#25511;&#21046;&#38556;&#30861;&#20989;&#25968;&#65292;&#23558;&#26377;&#38480;&#26102;&#38388;&#25193;&#25955;&#19981;&#21464;&#24615;&#23884;&#20837;&#21040;&#21435;&#22122;&#25193;&#25955;&#36807;&#31243;&#20013;&#65292;&#23454;&#29616;&#20102;&#21487;&#38752;&#30340;&#25193;&#25955;&#25968;&#25454;&#29983;&#25104;&#65292;&#24182;&#22312;&#22810;&#20010;&#23433;&#20840;&#35268;&#21010;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#40065;&#26834;&#24615;&#21644;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2306.00148</link><description>&lt;p&gt;
SafeDiffuser&#65306;&#22522;&#20110;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#23433;&#20840;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
SafeDiffuser: Safe Planning with Diffusion Probabilistic Models. (arXiv:2306.00148v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00148
&lt;/p&gt;
&lt;p&gt;
SafeDiffuser&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#35299;&#20915;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#22312;&#23433;&#20840;&#24615;&#19978;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#31867;&#25511;&#21046;&#38556;&#30861;&#20989;&#25968;&#65292;&#23558;&#26377;&#38480;&#26102;&#38388;&#25193;&#25955;&#19981;&#21464;&#24615;&#23884;&#20837;&#21040;&#21435;&#22122;&#25193;&#25955;&#36807;&#31243;&#20013;&#65292;&#23454;&#29616;&#20102;&#21487;&#38752;&#30340;&#25193;&#25955;&#25968;&#25454;&#29983;&#25104;&#65292;&#24182;&#22312;&#22810;&#20010;&#23433;&#20840;&#35268;&#21010;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#40065;&#26834;&#24615;&#21644;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861;&#24050;&#32463;&#22312;&#25968;&#25454;&#39537;&#21160;&#30340;&#35268;&#21010;&#20013;&#26174;&#31034;&#20986;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#65292;&#20294;&#26159;&#30001;&#20110;&#32570;&#20047;&#23433;&#20840;&#20445;&#38556;&#65292;&#22240;&#27492;&#24456;&#38590;&#24212;&#29992;&#20110;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;SafeDiffuser&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#31867;&#25511;&#21046;&#38556;&#30861;&#20989;&#25968;&#26469;&#30830;&#20445;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#28385;&#36275;&#35268;&#26684;&#35828;&#26126;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#23558;&#25152;&#25552;&#20986;&#30340;&#26377;&#38480;&#26102;&#38388;&#25193;&#25955;&#19981;&#21464;&#24615;&#23884;&#20837;&#21040;&#21435;&#22122;&#25193;&#25955;&#36807;&#31243;&#20013;&#65292;&#20174;&#32780;&#23454;&#29616;&#21487;&#20449;&#25193;&#25955;&#25968;&#25454;&#29983;&#25104;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26377;&#38480;&#26102;&#38388;&#25193;&#25955;&#19981;&#21464;&#24615;&#26041;&#27861;&#36890;&#36807;&#29983;&#25104;&#27169;&#22411;&#19981;&#20165;&#32500;&#25345;&#20102;&#36890;&#29992;&#24615;&#33021;&#65292;&#32780;&#19988;&#22312;&#23433;&#20840;&#25968;&#25454;&#29983;&#25104;&#20013;&#20063;&#20135;&#29983;&#20986;&#20102;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#27979;&#35797;&#22312;&#19968;&#31995;&#21015;&#30340;&#23433;&#20840;&#35268;&#21010;&#20219;&#21153;&#20013;&#65292;&#21253;&#25324;&#36855;&#23467;&#36335;&#24452;&#29983;&#25104;&#12289;&#22810;&#36275;&#26426;&#22120;&#20154;&#21160;&#21147;&#23398;&#21644;&#19977;&#32500;&#31354;&#38388;&#25805;&#20316;&#65292;&#32467;&#26524;&#26174;&#31034;&#20102;&#30456;&#23545;&#20110;&#22522;&#26412;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#21183;&#21644;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion model-based approaches have shown promise in data-driven planning, but there are no safety guarantees, thus making it hard to be applied for safety-critical applications. To address these challenges, we propose a new method, called SafeDiffuser, to ensure diffusion probabilistic models satisfy specifications by using a class of control barrier functions. The key idea of our approach is to embed the proposed finite-time diffusion invariance into the denoising diffusion procedure, which enables trustworthy diffusion data generation. Moreover, we demonstrate that our finite-time diffusion invariance method through generative models not only maintains generalization performance but also creates robustness in safe data generation. We test our method on a series of safe planning tasks, including maze path generation, legged robot locomotion, and 3D space manipulation, with results showing the advantages of robustness and guarantees over vanilla diffusion models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#26576;&#20123;&#38382;&#39064;&#22312;&#20043;&#21069;&#30340;&#30740;&#31350;&#20013;&#26410;&#24471;&#21040;&#35299;&#20915;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#38382;&#39064;&#30340;&#22238;&#31572;&#65292;&#21253;&#25324;&#24191;&#32780;&#27973;&#30340;ReLU&#32593;&#32476;&#19981;&#33021;&#34987;&#28145;&#32780;&#31364;&#30340;ReLU&#32593;&#32476;&#24456;&#22909;&#22320;&#36924;&#36817;&#31561;&#12290;</title><link>http://arxiv.org/abs/2306.00145</link><description>&lt;p&gt;
&#20851;&#20110;&#31070;&#32463;&#32593;&#32476;&#34920;&#36798;&#33021;&#21147;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Expressive Power of Neural Networks. (arXiv:2306.00145v1 [math.CA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00145
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#26576;&#20123;&#38382;&#39064;&#22312;&#20043;&#21069;&#30340;&#30740;&#31350;&#20013;&#26410;&#24471;&#21040;&#35299;&#20915;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#38382;&#39064;&#30340;&#22238;&#31572;&#65292;&#21253;&#25324;&#24191;&#32780;&#27973;&#30340;ReLU&#32593;&#32476;&#19981;&#33021;&#34987;&#28145;&#32780;&#31364;&#30340;ReLU&#32593;&#32476;&#24456;&#22909;&#22320;&#36924;&#36817;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
1989&#24180;&#65292;George Cybenko&#22312;&#19968;&#31687;&#37324;&#31243;&#30865;&#24335;&#30340;&#35770;&#25991;&#20013;&#35777;&#26126;&#20102;&#23485;&#32780;&#27973;&#30340;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#22312;&#32039;&#33268;&#38598;&#19978;&#36924;&#36817;&#20219;&#24847;&#36830;&#32493;&#20989;&#25968;&#65292;&#36825;&#20010;&#36890;&#29992;&#36924;&#36817;&#23450;&#29702;&#24341;&#21457;&#20102;&#24456;&#22810;&#21518;&#32493;&#30740;&#31350;&#12290;&#26412;&#25991;&#23558;&#36890;&#36807;&#19968;&#20010;&#26694;&#26550;&#22238;&#31572;&#8220;&#26377;&#27809;&#26377;&#19968;&#20123;&#24191;&#32780;&#27973;&#30340;ReLU&#32593;&#32476;&#26080;&#27861;&#34987;&#28145;&#32780;&#31364;&#30340;ReLU&#32593;&#32476;&#24456;&#22909;&#22320;&#36924;&#36817;&#65311;&#8221;&#8220;&#26222;&#36941;&#36924;&#36817;&#23450;&#29702;&#26159;&#21542;&#20173;&#36866;&#29992;&#20110;Sobolev&#31354;&#38388;&#33539;&#25968;W 1,1&#65311;&#8221;&#8220;&#36825;&#20123;&#32467;&#26524;&#26159;&#21542;&#36866;&#29992;&#20110;&#38500;ReLU&#20043;&#22806;&#30340;&#28608;&#27963;&#20989;&#25968;&#65311;&#8221;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In 1989 George Cybenko proved in a landmark paper that wide shallow neural networks can approximate arbitrary continuous functions on a compact set. This universal approximation theorem sparked a lot of follow-up research.  Shen, Yang and Zhang determined optimal approximation rates for ReLU-networks in $L^p$-norms with $p \in [1,\infty)$. Kidger and Lyons proved a universal approximation theorem for deep narrow ReLU-networks. Telgarsky gave an example of a deep narrow ReLU-network that cannot be approximated by a wide shallow ReLU-network unless it has exponentially many neurons.  However, there are even more questions that still remain unresolved. Are there any wide shallow ReLU-networks that cannot be approximated well by deep narrow ReLU-networks? Is the universal approximation theorem still true for other norms like the Sobolev norm $W^{1,1}$? Do these results hold for activation functions other than ReLU?  We will answer all of those questions and more with a framework of two exp
&lt;/p&gt;</description></item><item><title>&#26426;&#26800;&#24072;&#26159;&#19968;&#31181;&#23398;&#20064;&#29575;&#35843;&#33410;&#22120;&#65292;&#33021;&#33258;&#21160;&#35843;&#25972;&#20219;&#20309;&#22522;&#26412;&#20248;&#21270;&#31639;&#27861;&#21644;&#35843;&#24230;&#30340;&#23398;&#20064;&#29575;&#27604;&#20363;&#22240;&#23376;&#65292;&#21487;&#20197;&#23454;&#29616;&#22312;&#22823;&#35268;&#27169;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#20013;&#25509;&#36817;&#12289;&#21305;&#37197;&#25110;&#29978;&#33267;&#20248;&#20110;&#25163;&#21160;&#35843;&#25972;&#23398;&#20064;&#29575;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.00144</link><description>&lt;p&gt;
Mechanic: &#19968;&#31181;&#23398;&#20064;&#29575;&#35843;&#33410;&#22120;
&lt;/p&gt;
&lt;p&gt;
Mechanic: A Learning Rate Tuner. (arXiv:2306.00144v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00144
&lt;/p&gt;
&lt;p&gt;
&#26426;&#26800;&#24072;&#26159;&#19968;&#31181;&#23398;&#20064;&#29575;&#35843;&#33410;&#22120;&#65292;&#33021;&#33258;&#21160;&#35843;&#25972;&#20219;&#20309;&#22522;&#26412;&#20248;&#21270;&#31639;&#27861;&#21644;&#35843;&#24230;&#30340;&#23398;&#20064;&#29575;&#27604;&#20363;&#22240;&#23376;&#65292;&#21487;&#20197;&#23454;&#29616;&#22312;&#22823;&#35268;&#27169;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#20013;&#25509;&#36817;&#12289;&#21305;&#37197;&#25110;&#29978;&#33267;&#20248;&#20110;&#25163;&#21160;&#35843;&#25972;&#23398;&#20064;&#29575;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#26426;&#26800;&#24072;&#8221;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#33258;&#21160;&#35843;&#25972;&#20219;&#20309;&#22522;&#26412;&#20248;&#21270;&#31639;&#27861;&#21644;&#35843;&#24230;&#30340;&#23398;&#20064;&#29575;&#27604;&#20363;&#22240;&#23376;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#23454;&#29616;&#31867;&#20284;&#30446;&#26631;&#30340;&#26368;&#36817;&#29702;&#35770;&#20943;&#23569;&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#29992;&#20110;&#22312;&#32447;&#20984;&#20248;&#21270;&#12290;&#25105;&#20204;&#22312;&#20855;&#26377;&#19981;&#21516;&#25209;&#37327;&#22823;&#23567;&#12289;&#35843;&#24230;&#21644;&#22522;&#26412;&#20248;&#21270;&#31639;&#27861;&#30340;&#19968;&#31995;&#21015;&#22823;&#35268;&#27169;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#20005;&#26684;&#35780;&#20272;&#12290;&#36825;&#20123;&#23454;&#39564;&#34920;&#26126;&#65292;&#26681;&#25454;&#38382;&#39064;&#65292;&#26426;&#26800;&#24072;&#35201;&#20040;&#38750;&#24120;&#25509;&#36817;&#65292;&#35201;&#20040;&#21305;&#37197;&#25110;&#29978;&#33267;&#20248;&#20110;&#25163;&#21160;&#35843;&#25972;&#23398;&#20064;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a technique for tuning the learning rate scale factor of any base optimization algorithm and schedule automatically, which we call \textsc{mechanic}. Our method provides a practical realization of recent theoretical reductions for accomplishing a similar goal in online convex optimization. We rigorously evaluate \textsc{mechanic} on a range of large scale deep learning tasks with varying batch sizes, schedules, and base optimization algorithms. These experiments demonstrate that depending on the problem, \textsc{mechanic} either comes very close to, matches or even improves upon manual tuning of learning rates.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#22914;&#20309;&#35299;&#37322;&#37329;&#19997;&#38592;&#26333;&#20809;&#30340;&#30452;&#35273;&#65292;&#21253;&#25324;&#20854;&#19982;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#21644;&#24046;&#20998;&#38544;&#31169;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2306.00133</link><description>&lt;p&gt;
&#20851;&#20110;&#37329;&#19997;&#38592;&#26333;&#20809;&#35299;&#37322;&#30340;&#19968;&#20123;&#27880;&#37322;
&lt;/p&gt;
&lt;p&gt;
A Note On Interpreting Canary Exposure. (arXiv:2306.00133v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00133
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#22914;&#20309;&#35299;&#37322;&#37329;&#19997;&#38592;&#26333;&#20809;&#30340;&#30452;&#35273;&#65292;&#21253;&#25324;&#20854;&#19982;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#21644;&#24046;&#20998;&#38544;&#31169;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Carlini&#31561;&#20154;&#20171;&#32461;&#30340;&#37329;&#19997;&#38592;&#26292;&#38706;&#32463;&#24120;&#34987;&#29992;&#26469;&#23454;&#35777;&#35780;&#20272;&#25110;&#23457;&#26680;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22521;&#35757;&#30340;&#38544;&#31169;&#12290;&#36825;&#31687;&#31508;&#35760;&#30340;&#30446;&#30340;&#26159;&#25552;&#20379;&#19968;&#20123;&#20851;&#20110;&#22914;&#20309;&#35299;&#37322;&#37329;&#19997;&#38592;&#26333;&#20809;&#30340;&#30452;&#35273;&#65292;&#21253;&#25324;&#19982;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#21644;&#24046;&#20998;&#38544;&#31169;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Canary exposure, introduced in Carlini et al. is frequently used to empirically evaluate, or audit, the privacy of machine learning model training. The goal of this note is to provide some intuition on how to interpret canary exposure, including by relating it to membership inference attacks and differential privacy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026; Surrogate Model Extension (SME) &#30340;&#26041;&#27861;&#65292;&#20351;&#24471;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#20351;&#29992;&#27604;&#20043;&#21069;&#26356;&#21152;&#39640;&#25928;&#21644;&#20934;&#30830;&#30340;&#26041;&#27861;&#23545;&#26435;&#37325;&#26356;&#26032;&#36827;&#34892;&#38544;&#31169;&#25915;&#20987;&#65292;&#36827;&#19968;&#27493;&#26292;&#38706;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#38544;&#31169;&#20445;&#25252;&#24369;&#28857;&#12290;</title><link>http://arxiv.org/abs/2306.00127</link><description>&lt;p&gt;
Surrogate Model Extension (SME): &#19968;&#31181;&#24555;&#36895;&#20934;&#30830;&#30340;&#32852;&#37030;&#23398;&#20064;&#26435;&#37325;&#26356;&#26032;&#25915;&#20987;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Surrogate Model Extension (SME): A Fast and Accurate Weight Update Attack on Federated Learning. (arXiv:2306.00127v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00127
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026; Surrogate Model Extension (SME) &#30340;&#26041;&#27861;&#65292;&#20351;&#24471;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#20351;&#29992;&#27604;&#20043;&#21069;&#26356;&#21152;&#39640;&#25928;&#21644;&#20934;&#30830;&#30340;&#26041;&#27861;&#23545;&#26435;&#37325;&#26356;&#26032;&#36827;&#34892;&#38544;&#31169;&#25915;&#20987;&#65292;&#36827;&#19968;&#27493;&#26292;&#38706;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#38544;&#31169;&#20445;&#25252;&#24369;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#21644;&#20998;&#24067;&#24335;&#35757;&#32451;&#26694;&#26550;&#20013;&#65292;&#21512;&#20316;&#32773;&#21487;&#20197;&#22312;&#26412;&#22320;&#20445;&#23384;&#20854;&#31169;&#26377;&#25968;&#25454;&#65292;&#24182;&#22312;&#22810;&#27425;&#36845;&#20195;&#21518;&#20165;&#20998;&#20139;&#20351;&#29992;&#26412;&#22320;&#25968;&#25454;&#35757;&#32451;&#30340;&#32593;&#32476;&#26435;&#37325;&#12290;&#26799;&#24230;&#21453;&#28436;&#26159;&#19968;&#31867;&#20174;&#29983;&#25104;&#30340;&#26799;&#24230;&#20013;&#24674;&#22797;&#25968;&#25454;&#30340;&#38544;&#31169;&#25915;&#20987;&#26041;&#27861;&#12290;&#22312;FL&#20013;&#65292;&#21333;&#20010;&#27493;&#39588;&#30340;&#26799;&#24230;&#34987;&#22810;&#27425;&#26412;&#22320;&#36845;&#20195;&#30340;&#26799;&#24230;&#31215;&#32047;&#25152;&#36974;&#30422;&#65292;&#20284;&#20046;&#21487;&#20197;&#25552;&#20379;&#19968;&#23450;&#30340;&#20445;&#25252;&#65292;&#38450;&#27490;&#26799;&#24230;&#21453;&#28436;&#25915;&#20987;&#26435;&#37325;&#26356;&#26032;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21512;&#29702;&#30340;&#26041;&#27861;&#65292;&#23558;&#26799;&#24230;&#21453;&#28436;&#25915;&#20987;&#25193;&#23637;&#21040;FL&#26435;&#37325;&#26356;&#26032;&#20013;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#26292;&#38706;FL&#26412;&#36523;&#25152;&#20551;&#23450;&#30340;&#38544;&#31169;&#20445;&#25252;&#24369;&#28857;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20108;&#32500;&#26799;&#24230;&#27969;&#29305;&#24449;&#21644;&#26412;&#22320;&#26356;&#26032;&#20302;&#31209;&#24615;&#36136;&#30340;&#26367;&#20195;&#27169;&#22411;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26497;&#22823;&#22320;&#25552;&#21319;&#20102;&#21253;&#21547;&#22810;&#27425;&#36845;&#20195;&#30340;&#26435;&#37325;&#26356;&#26032;&#21463;&#26799;&#24230;&#21453;&#28436;&#25915;&#20987;&#30340;&#33021;&#21147;&#65292;&#24182;&#23454;&#29616;&#20102;&#38750;&#24120;&#39640;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Federated Learning (FL) and many other distributed training frameworks, collaborators can hold their private data locally and only share the network weights trained with the local data after multiple iterations. Gradient inversion is a family of privacy attacks that recovers data from its generated gradients. Seemingly, FL can provide a degree of protection against gradient inversion attacks on weight updates, since the gradient of a single step is concealed by the accumulation of gradients over multiple local iterations. In this work, we propose a principled way to extend gradient inversion attacks to weight updates in FL, thereby better exposing weaknesses in the presumed privacy protection inherent in FL. In particular, we propose a surrogate model method based on the characteristic of two-dimensional gradient flow and low-rank property of local updates. Our method largely boosts the ability of gradient inversion attacks on weight updates containing many iterations and achieves s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#20998;&#26512;&#26694;&#26550;&#65292;&#36890;&#36807;&#20984;&#35268;&#21010;&#34920;&#24449;&#25152;&#26377;ReLU&#32593;&#32476;&#30340;&#26368;&#20248;&#38598;&#21512;&#21644;&#35299;&#36335;&#24452;&#65292;&#24182;&#25552;&#20379;&#20102;&#26368;&#23567;&#32593;&#32476;&#30340;&#26368;&#20248;&#21098;&#26525;&#31639;&#27861;&#65292;&#24314;&#31435;&#20102;ReLU&#32593;&#32476;&#27491;&#21017;&#21270;&#36335;&#24452;&#36830;&#32493;&#30340;&#26465;&#20214;&#65292;&#24182;&#20026;&#26368;&#23567;ReLU&#32593;&#32476;&#25552;&#20379;&#20102;&#28789;&#25935;&#24230;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.00119</link><description>&lt;p&gt;
ReLU&#32593;&#32476;&#30340;&#26368;&#20248;&#38598;&#21512;&#21644;&#35299;&#36335;&#24452;
&lt;/p&gt;
&lt;p&gt;
Optimal Sets and Solution Paths of ReLU Networks. (arXiv:2306.00119v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00119
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#20998;&#26512;&#26694;&#26550;&#65292;&#36890;&#36807;&#20984;&#35268;&#21010;&#34920;&#24449;&#25152;&#26377;ReLU&#32593;&#32476;&#30340;&#26368;&#20248;&#38598;&#21512;&#21644;&#35299;&#36335;&#24452;&#65292;&#24182;&#25552;&#20379;&#20102;&#26368;&#23567;&#32593;&#32476;&#30340;&#26368;&#20248;&#21098;&#26525;&#31639;&#27861;&#65292;&#24314;&#31435;&#20102;ReLU&#32593;&#32476;&#27491;&#21017;&#21270;&#36335;&#24452;&#36830;&#32493;&#30340;&#26465;&#20214;&#65292;&#24182;&#20026;&#26368;&#23567;ReLU&#32593;&#32476;&#25552;&#20379;&#20102;&#28789;&#25935;&#24230;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20998;&#26512;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#38750;&#20984;&#35757;&#32451;&#38382;&#39064;&#37325;&#26032;&#23450;&#20041;&#20026;&#20984;&#35268;&#21010;&#38382;&#39064;&#65292;&#26469;&#21051;&#30011;&#26368;&#20248;ReLU&#31070;&#32463;&#32593;&#32476;&#38598;&#21512;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20984;&#21442;&#25968;&#21270;&#30340;&#20840;&#23616;&#26368;&#20248;&#35299;&#30001;&#19968;&#20010;&#22810;&#38754;&#20307;&#38598;&#21512;&#32473;&#20986;&#65292;&#24182;&#23558;&#36825;&#20010;&#34920;&#24449;&#25193;&#23637;&#21040;&#20102;&#38750;&#20984;&#35757;&#32451;&#30446;&#26631;&#30340;&#26368;&#20248;&#38598;&#12290;&#30001;&#20110;ReLU&#35757;&#32451;&#38382;&#39064;&#30340;&#25152;&#26377;&#31283;&#23450;&#28857;&#37117;&#21487;&#20197;&#34920;&#31034;&#20026;&#23376;&#37319;&#26679;&#20984;&#35268;&#21010;&#30340;&#26368;&#20248;&#35299;&#65292;&#22240;&#27492;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#25152;&#26377;&#38750;&#20984;&#30446;&#26631;&#30340;&#20020;&#30028;&#28857;&#25552;&#20379;&#20102;&#36890;&#29992;&#34920;&#36798;&#24335;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#25105;&#20204;&#30340;&#32467;&#26524;&#25552;&#20379;&#20102;&#35745;&#31639;&#26368;&#23567;&#32593;&#32476;&#30340;&#26368;&#20248;&#21098;&#26525;&#31639;&#27861;&#65292;&#24314;&#31435;&#20102;ReLU&#32593;&#32476;&#27491;&#21017;&#21270;&#36335;&#24452;&#36830;&#32493;&#30340;&#26465;&#20214;&#65292;&#24182;&#20026;&#26368;&#23567;ReLU&#32593;&#32476;&#25552;&#20379;&#20102;&#28789;&#25935;&#24230;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop an analytical framework to characterize the set of optimal ReLU neural networks by reformulating the non-convex training problem as a convex program. We show that the global optima of the convex parameterization are given by a polyhedral set and then extend this characterization to the optimal set of the non-convex training objective. Since all stationary points of the ReLU training problem can be represented as optima of sub-sampled convex programs, our work provides a general expression for all critical points of the non-convex objective. We then leverage our results to provide an optimal pruning algorithm for computing minimal networks, establish conditions for the regularization path of ReLU networks to be continuous, and develop sensitivity results for minimal ReLU networks.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26102;&#38388;&#34917;&#19969;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;&#21152;&#25343;&#22823;&#20892;&#30000;&#30340;&#22810;&#26102;&#30456;&#36965;&#24863;&#24433;&#20687;&#12290;&#35813;&#25968;&#25454;&#38598;&#26159;&#25163;&#21160;&#32463;&#36807;&#30830;&#35748;&#21644;&#31579;&#36873;&#30340;&#39640;&#20998;&#36776;&#29575;&#22320;&#29702;&#21442;&#32771;&#22270;&#20687;&#65292;&#35206;&#30422;&#22235;&#20010;&#20892;&#20316;&#29289;&#29983;&#20135;&#24180;&#24230;&#21644;&#20116;&#20010;&#26376;&#20221;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#21487;&#20197;&#29992;&#20110;&#25552;&#39640;&#22303;&#22320;&#35206;&#30422;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.00114</link><description>&lt;p&gt;
&#21152;&#25343;&#22823;&#20892;&#30000;&#25968;&#25454;&#38598;&#65306;&#29992;&#20110;&#20892;&#19994;&#22810;&#26102;&#30456;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#30340;&#26032;&#22320;&#34920;&#35206;&#30422;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
The Canadian Cropland Dataset: A New Land Cover Dataset for Multitemporal Deep Learning Classification in Agriculture. (arXiv:2306.00114v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00114
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26102;&#38388;&#34917;&#19969;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;&#21152;&#25343;&#22823;&#20892;&#30000;&#30340;&#22810;&#26102;&#30456;&#36965;&#24863;&#24433;&#20687;&#12290;&#35813;&#25968;&#25454;&#38598;&#26159;&#25163;&#21160;&#32463;&#36807;&#30830;&#35748;&#21644;&#31579;&#36873;&#30340;&#39640;&#20998;&#36776;&#29575;&#22320;&#29702;&#21442;&#32771;&#22270;&#20687;&#65292;&#35206;&#30422;&#22235;&#20010;&#20892;&#20316;&#29289;&#29983;&#20135;&#24180;&#24230;&#21644;&#20116;&#20010;&#26376;&#20221;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#21487;&#20197;&#29992;&#20110;&#25552;&#39640;&#22303;&#22320;&#35206;&#30422;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#36965;&#24863;&#30417;&#27979;&#22303;&#22320;&#35206;&#30422;&#26159;&#30740;&#31350;&#29615;&#22659;&#21464;&#21270;&#21644;&#36890;&#36807;&#31918;&#39135;&#20135;&#37327;&#39044;&#27979;&#30830;&#20445;&#20840;&#29699;&#31918;&#39135;&#23433;&#20840;&#30340;&#20851;&#38190;&#12290;&#23588;&#20854;&#26159;&#65292;&#22810;&#26102;&#30456;&#36965;&#24863;&#24433;&#20687;&#25552;&#20379;&#20102;&#20851;&#20110;&#22330;&#26223;&#21160;&#24577;&#30340;&#30456;&#20851;&#20449;&#24687;&#65292;&#24050;&#32463;&#34987;&#35777;&#26126;&#21487;&#20197;&#24102;&#26469;&#26356;&#22909;&#30340;&#22303;&#22320;&#35206;&#30422;&#20998;&#31867;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38590;&#20197;&#33719;&#21462;&#21487;&#38752;&#12289;&#32454;&#31890;&#24230;&#21644;&#39640;&#36136;&#37327;&#30340;&#27880;&#37322;&#26679;&#26412;&#25903;&#25345;&#20182;&#20204;&#30340;&#20551;&#35774;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#21463;&#30410;&#20110;&#39640;&#31354;&#38388;&#21644;&#26102;&#38388;&#20998;&#36776;&#29575;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21152;&#25343;&#22823;&#20892;&#30000;&#30340;&#26102;&#38388;&#34917;&#19969;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#26469;&#33258;10&#20010;&#20892;&#20316;&#29289;&#31867;&#21035;&#30340;78,536&#20010;&#25163;&#21160;&#32463;&#36807;&#30830;&#35748;&#21644;&#31579;&#36873;&#30340;&#39640;&#20998;&#36776;&#29575;(10&#31859;/&#20687;&#32032;&#65292;640 x 640&#31859;)&#22320;&#29702;&#21442;&#32771;&#22270;&#20687;&#65292;&#35206;&#30422;&#20102;&#22235;&#20010;&#20892;&#20316;&#29289;&#29983;&#20135;&#24180;&#24230;(2017-2020)&#21644;&#20116;&#20010;&#26376;&#20221;(&#20845;&#26376;-&#21313;&#26376;)&#12290;&#27599;&#20010;&#23454;&#20363;&#37117;&#21253;&#21547;12&#20010;&#20809;&#35889;&#27874;&#27573;&#12289;&#19968;&#24352;RGB&#22270;&#20687;&#21644;&#39069;&#22806;&#30340;&#26893;&#34987;&#25351;&#25968;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
Monitoring land cover using remote sensing is vital for studying environmental changes and ensuring global food security through crop yield forecasting. Specifically, multitemporal remote sensing imagery provides relevant information about the dynamics of a scene, which has proven to lead to better land cover classification results. Nevertheless, few studies have benefited from high spatial and temporal resolution data due to the difficulty of accessing reliable, fine-grained and high-quality annotated samples to support their hypotheses. Therefore, we introduce a temporal patch-based dataset of Canadian croplands, enriched with labels retrieved from the Canadian Annual Crop Inventory. The dataset contains 78,536 manually verified and curated high-resolution (10 m/pixel, 640 x 640 m) geo-referenced images from 10 crop classes collected over four crop production years (2017-2020) and five months (June-October). Each instance contains 12 spectral bands, an RGB image, and additional veget
&lt;/p&gt;</description></item><item><title>MuseCoco&#26159;&#19968;&#31181;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#29983;&#25104;&#31526;&#21495;&#38899;&#20048;&#30340;&#31995;&#32479;&#65292;&#20855;&#22791;&#39640;&#25928;&#21644;&#28789;&#27963;&#31561;&#29305;&#28857;&#65292;&#20026;&#38899;&#20048;&#23478;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#38899;&#20048;&#29983;&#25104;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2306.00110</link><description>&lt;p&gt;
MuseCoco: &#20174;&#25991;&#26412;&#29983;&#25104;&#31526;&#21495;&#38899;&#20048;
&lt;/p&gt;
&lt;p&gt;
MuseCoco: Generating Symbolic Music from Text. (arXiv:2306.00110v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00110
&lt;/p&gt;
&lt;p&gt;
MuseCoco&#26159;&#19968;&#31181;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#29983;&#25104;&#31526;&#21495;&#38899;&#20048;&#30340;&#31995;&#32479;&#65292;&#20855;&#22791;&#39640;&#25928;&#21644;&#28789;&#27963;&#31561;&#29305;&#28857;&#65292;&#20026;&#38899;&#20048;&#23478;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#38899;&#20048;&#29983;&#25104;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#29983;&#25104;&#38899;&#20048;&#26159;&#19968;&#31181;&#29992;&#25143;&#21451;&#22909;&#30340;&#26041;&#24335;&#65292;&#22240;&#20026;&#25991;&#26412;&#26159;&#30456;&#23545;&#26131;&#20110;&#29992;&#25143;&#21442;&#19982;&#30340;&#30028;&#38754;&#12290;&#32780;&#26377;&#20123;&#26041;&#27861;&#21033;&#29992;&#25991;&#26412;&#26469;&#25511;&#21046;&#38899;&#20048;&#38899;&#39057;&#30340;&#29983;&#25104;&#65292;&#20294;&#26159;&#32534;&#36753;&#29983;&#25104;&#38899;&#39057;&#30340;&#38899;&#20048;&#20803;&#32032;&#23545;&#20110;&#29992;&#25143;&#26469;&#35828;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#31526;&#21495;&#38899;&#20048;&#20855;&#26377;&#26131;&#20110;&#32534;&#36753;&#30340;&#20248;&#28857;&#65292;&#20351;&#29992;&#25143;&#26356;&#23481;&#26131;&#25805;&#20316;&#29305;&#23450;&#30340;&#38899;&#20048;&#20803;&#32032;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;MuseCoco&#65292;&#23427;&#21033;&#29992;&#38899;&#20048;&#23646;&#24615;&#20316;&#20026;&#26725;&#26753;&#65292;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#25991;&#26412;&#21040;&#23646;&#24615;&#29702;&#35299;&#21644;&#23646;&#24615;&#21040;&#38899;&#20048;&#29983;&#25104;&#30340;&#20004;&#20010;&#38454;&#27573;&#65292;&#20174;&#32780;&#29983;&#25104;&#31526;&#21495;&#38899;&#20048;&#12290;MuseCoCo&#20195;&#34920;&#38899;&#20048;&#20316;&#26354;&#21103;&#39550;&#39542;&#65292;&#20351;&#38899;&#20048;&#23478;&#21487;&#20197;&#30452;&#25509;&#20174;&#32473;&#23450;&#30340;&#25991;&#26412;&#25551;&#36848;&#20013;&#29983;&#25104;&#38899;&#20048;&#65292;&#19982;&#20174;&#22836;&#24320;&#22987;&#21019;&#20316;&#30456;&#27604;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#25928;&#29575;&#12290;&#35813;&#31995;&#32479;&#20855;&#26377;&#20004;&#20010;&#20027;&#35201;&#20248;&#28857;&#65306;&#25968;&#25454;&#39640;&#25928;&#12290;&#22312;&#23646;&#24615;&#21040;&#38899;&#20048;&#29983;&#25104;&#38454;&#27573;&#65292;&#23646;&#24615;&#21487;&#20197;&#30452;&#25509;&#36827;&#34892;&#32534;&#30721;&#65292;&#32780;&#19981;&#38656;&#35201;&#22823;&#37327;&#30340;&#38899;&#20048;&#25968;&#25454;&#12290;&#20854;&#27425;&#65292;&#27492;&#31995;&#32479;&#20855;&#26377;&#39640;&#32423;&#21035;&#30340;&#28789;&#27963;&#24615;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#36890;&#36807;&#21464;&#26356;&#25991;&#26412;&#36755;&#20837;&#26469;&#29983;&#25104;&#22810;&#26679;&#21270;&#19988;&#26377;&#20010;&#24615;&#30340;&#31526;&#21495;&#38899;&#20048;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating music from text descriptions is a user-friendly mode since the text is a relatively easy interface for user engagement. While some approaches utilize texts to control music audio generation, editing musical elements in generated audio is challenging for users. In contrast, symbolic music offers ease of editing, making it more accessible for users to manipulate specific musical elements. In this paper, we propose MuseCoco, which generates symbolic music from text descriptions with musical attributes as the bridge to break down the task into text-to-attribute understanding and attribute-to-music generation stages. MuseCoCo stands for Music Composition Copilot that empowers musicians to generate music directly from given text descriptions, offering a significant improvement in efficiency compared to creating music entirely from scratch. The system has two main advantages: Firstly, it is data efficient. In the attribute-to-music generation stage, the attributes can be directly e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35299;&#37322;&#30340;&#31639;&#27861;&#65292;&#22522;&#20110;&#33258;&#27965;&#24615;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#36827;&#34892;&#36719;&#20214;&#34917;&#19969;&#36873;&#25321;&#65292;&#20174;&#32780;&#35299;&#20915;&#24403;&#21069;&#32570;&#20047;&#35299;&#37322;&#30340;&#36719;&#20214;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.00108</link><description>&lt;p&gt;
&#22810;&#25968;&#35268;&#21017;&#65306;&#36890;&#36807;&#33258;&#27965;&#24615;&#23454;&#29616;&#26356;&#22909;&#30340;&#20462;&#34917;
&lt;/p&gt;
&lt;p&gt;
Majority Rule: better patching via Self-Consistency. (arXiv:2306.00108v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00108
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35299;&#37322;&#30340;&#31639;&#27861;&#65292;&#22522;&#20110;&#33258;&#27965;&#24615;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#36827;&#34892;&#36719;&#20214;&#34917;&#19969;&#36873;&#25321;&#65292;&#20174;&#32780;&#35299;&#20915;&#24403;&#21069;&#32570;&#20047;&#35299;&#37322;&#30340;&#36719;&#20214;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#34987;&#24341;&#23548;&#35299;&#20915;&#19968;&#20123;&#38656;&#35201;&#8220;&#23569;&#37327;&#25552;&#31034;&#8221;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#31034;&#20363;&#38382;&#39064;-&#35299;&#20915;&#26041;&#26696;&#23545;&#12290;&#29616;&#22312;&#65292;&#22914;&#26524;&#23569;&#37327;&#25552;&#31034;&#20063;&#21253;&#25324;&#8220;&#24605;&#32500;&#38142;&#8221;&#65288;CoT&#65289;&#35299;&#37322;&#65292;&#24418;&#24335;&#20026;&#38382;&#39064;-&#35299;&#37322;-&#35299;&#20915;&#26041;&#26696;&#65292;LLMs&#23558;&#20250;&#20135;&#29983;&#19968;&#20010;&#8220;&#35299;&#37322;&#8221;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#34920;&#29616;&#24471;&#26356;&#22909;&#12290;&#26368;&#36817;&#65292;&#19968;&#39033;&#26356;&#20986;&#33394;&#30340;&#25216;&#26415;&#8220;&#33258;&#27965;&#24615;&#8221;&#65288;S-C&#65289;&#20986;&#29616;&#20102;&#65292;&#22522;&#20110;&#36825;&#26679;&#19968;&#31181;&#30452;&#35273;&#65306;&#27491;&#30830;&#30340;&#35299;&#20915;&#26041;&#26696;&#26377;&#35768;&#22810;&#21487;&#33021;&#30340;&#35299;&#37322;&#12290;&#24403;LLM&#34987;&#21453;&#22797;&#37319;&#26679;&#20197;&#29983;&#25104;&#38382;&#39064;&#30340;&#35299;&#37322;-&#35299;&#20915;&#26041;&#26696;&#23545;&#26102;&#65292;&#23545;&#20110;&#32473;&#23450;&#30340;&#38382;&#39064;&#65292;&#27744;&#20013;&#26368;&#24120;&#21457;&#29983;&#30340;&#35299;&#20915;&#26041;&#26696;&#65288;&#24573;&#30053;&#35299;&#37322;&#65289;&#26356;&#26377;&#21487;&#33021;&#26159;&#27491;&#30830;&#30340;&#65281;&#20294;&#26159;&#65292;&#36825;&#31181;&#39640;&#24615;&#33021;&#30340;S-C&#65288;&#29978;&#33267;CoT&#65289;&#26041;&#27861;&#22312;&#36719;&#20214;&#24037;&#31243;&#29615;&#22659;&#20013;&#30340;&#20351;&#29992;&#21463;&#21040;&#35299;&#37322;&#30340;&#32570;&#20047;&#30340;&#38480;&#21046;&#65307;&#22823;&#22810;&#25968;&#36719;&#20214;&#25968;&#25454;&#38598;&#37117;&#32570;&#20047;&#35299;&#37322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;S-C&#31639;&#27861;&#30340;&#24212;&#29992;&#65292;&#35813;&#31639;&#27861;&#19981;&#38656;&#35201;&#35299;&#37322;&#65292;&#22240;&#27492;&#21487;&#20197;&#29992;&#20110;&#36719;&#20214;&#34917;&#19969;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language models (LLMs) can be induced to solve non-trivial problems with "few-shot" prompts including illustrative problem-solution examples. Now if the few-shots also include "chain of thought" (CoT) explanations, which are of the form problem-explanation-solution, LLMs will generate a "explained" solution, and perform even better. Recently an exciting, substantially better technique, self-consistency [1] (S-C) has emerged, based on the intuition that there are many plausible explanations for the right solution; when the LLM is sampled repeatedly to generate a pool of explanation-solution pairs, for a given problem, the most frequently occurring solutions in the pool (ignoring the explanations) tend to be even more likely to be correct! Unfortunately, the use of this highly-performant S-C (or even CoT) approach in software engineering settings is hampered by the lack of explanations; most software datasets lack explanations. In this paper, we describe an application of the S-C a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#38899;&#20048;&#29702;&#35299;&#27169;&#22411;MERT&#65292;&#21033;&#29992;&#20102;&#25945;&#24072;&#27169;&#22411;&#24182;&#37319;&#29992;&#20102;&#19968;&#31181;&#20248;&#20110;&#20256;&#32479;&#30340;&#35821;&#38899;&#21644;&#38899;&#39057;&#26041;&#27861;&#30340;&#32452;&#21512;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2306.00107</link><description>&lt;p&gt;
MERT:&#24102;&#26377;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#22768;&#23398;&#38899;&#20048;&#29702;&#35299;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MERT: Acoustic Music Understanding Model with Large-Scale Self-supervised Training. (arXiv:2306.00107v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00107
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#38899;&#20048;&#29702;&#35299;&#27169;&#22411;MERT&#65292;&#21033;&#29992;&#20102;&#25945;&#24072;&#27169;&#22411;&#24182;&#37319;&#29992;&#20102;&#19968;&#31181;&#20248;&#20110;&#20256;&#32479;&#30340;&#35821;&#38899;&#21644;&#38899;&#39057;&#26041;&#27861;&#30340;&#32452;&#21512;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26368;&#36817;&#22312;&#35270;&#35273;&#12289;&#25991;&#26412;&#21644;&#35821;&#38899;&#39046;&#22495;&#20013;&#24050;&#34987;&#35777;&#26126;&#26159;&#35757;&#32451;&#36890;&#29992;&#27169;&#22411;&#30340;&#19968;&#31181;&#24456;&#26377;&#21069;&#26223;&#30340;&#33539;&#20363;&#65292;&#23545;&#20110;&#36328;&#36234;&#38899;&#20048;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#35843;&#24615;&#21644;&#38899;&#39640;&#36825;&#26679;&#30340;&#29305;&#27530;&#38899;&#20048;&#30693;&#35782;&#30340;&#24314;&#27169;&#39047;&#20855;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#22768;&#23398;&#38899;&#20048;&#29702;&#35299;&#27169;&#22411;&#65292;&#21363;MERT&#12290;&#22312;&#25105;&#20204;&#30340;&#25506;&#32034;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#26356;&#20248;&#31168;&#30340;&#25945;&#24072;&#27169;&#22411;&#32452;&#21512;&#65292;&#36825;&#31181;&#32452;&#21512;&#26041;&#27861;&#22312;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;&#35821;&#38899;&#21644;&#38899;&#39057;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) has recently emerged as a promising paradigm for training generalisable models on large-scale data in the fields of vision, text, and speech. Although SSL has been proven effective in speech and audio, its application to music audio has yet to be thoroughly explored. This is primarily due to the distinctive challenges associated with modelling musical knowledge, particularly its tonal and pitched characteristics of music. To address this research gap, we propose an acoustic Music undERstanding model with large-scale self-supervised Training (MERT), which incorporates teacher models to provide pseudo labels in the masked language modelling (MLM) style acoustic pre-training. In our exploration, we identified a superior combination of teacher models, which outperforms conventional speech and audio approaches in terms of performance. This combination includes an acoustic teacher based on Residual Vector Quantization - Variational AutoEncoder (RVQ-VAE) and a m
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;ManagerTower&#65292;&#19968;&#31181;&#26032;&#22411;&#30340;VL&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#65292;&#21487;&#20197;&#38598;&#21512;&#24182;&#32452;&#21512;&#19981;&#21516;&#32423;&#21035;&#30340;&#39044;&#20808;&#35757;&#32451;&#30340;&#21333;&#27169;&#24577;&#19987;&#23478;&#30340;&#35265;&#35299;&#65292;&#24182;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#32858;&#21512;&#21333;&#27169;&#24577;&#35821;&#20041;&#30693;&#35782;&#20197;&#20419;&#36827;&#26356;&#20840;&#38754;&#30340;&#36328;&#27169;&#24577;&#23545;&#40784;&#21644;&#34701;&#21512;&#12290;&#20165;&#20351;&#29992;4M VLP&#25968;&#25454;&#65292;ManagerTower&#22312;&#21508;&#31181;&#19979;&#28216;VL&#20219;&#21153;&#20013;&#37117;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;VQAv2&#27979;&#35797;&#26631;&#20934;&#19979;&#36798;&#21040;&#20102;79.15%&#30340;&#20934;&#30830;&#29575;&#65292;&#22312;Flickr30K&#19978;&#30340;IR@1&#20026;86.56% TR@1&#20026;95.64%&#12290;</title><link>http://arxiv.org/abs/2306.00103</link><description>&lt;p&gt;
ManagerTower&#65306;&#32858;&#21512;&#21333;&#27169;&#24577;&#19987;&#23478;&#35265;&#35299;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
ManagerTower: Aggregating the Insights of Uni-Modal Experts for Vision-Language Representation Learning. (arXiv:2306.00103v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00103
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;ManagerTower&#65292;&#19968;&#31181;&#26032;&#22411;&#30340;VL&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#65292;&#21487;&#20197;&#38598;&#21512;&#24182;&#32452;&#21512;&#19981;&#21516;&#32423;&#21035;&#30340;&#39044;&#20808;&#35757;&#32451;&#30340;&#21333;&#27169;&#24577;&#19987;&#23478;&#30340;&#35265;&#35299;&#65292;&#24182;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#32858;&#21512;&#21333;&#27169;&#24577;&#35821;&#20041;&#30693;&#35782;&#20197;&#20419;&#36827;&#26356;&#20840;&#38754;&#30340;&#36328;&#27169;&#24577;&#23545;&#40784;&#21644;&#34701;&#21512;&#12290;&#20165;&#20351;&#29992;4M VLP&#25968;&#25454;&#65292;ManagerTower&#22312;&#21508;&#31181;&#19979;&#28216;VL&#20219;&#21153;&#20013;&#37117;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;VQAv2&#27979;&#35797;&#26631;&#20934;&#19979;&#36798;&#21040;&#20102;79.15%&#30340;&#20934;&#30830;&#29575;&#65292;&#22312;Flickr30K&#19978;&#30340;IR@1&#20026;86.56% TR@1&#20026;95.64%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20004;&#22612;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#22312;&#21508;&#31181;&#19979;&#28216;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#19978;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#25913;&#36827;&#12290;&#23613;&#31649;&#26368;&#20808;&#36827;&#30340;&#24037;&#20316;&#36890;&#36807;&#26500;&#24314;&#32534;&#30721;&#22120;&#20043;&#38388;&#30340;&#26725;&#26753;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#20294;&#23427;&#36973;&#21463;&#20102;&#21333;&#27169;&#24577;&#34920;&#31034;&#30340;&#36880;&#23618;&#21033;&#29992;&#25928;&#26524;&#19981;&#20339;&#30340;&#22256;&#22659;&#65292;&#24182;&#19988;&#19981;&#33021;&#28789;&#27963;&#22320;&#21033;&#29992;&#19981;&#21516;&#32423;&#21035;&#30340;&#21333;&#27169;&#24577;&#35821;&#20041;&#30693;&#35782;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ManagerTower&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;VL&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#65292;&#23427;&#38598;&#21512;&#24182;&#32452;&#21512;&#20102;&#39044;&#20808;&#35757;&#32451;&#30340;&#19981;&#21516;&#32423;&#21035;&#30340;&#21333;&#27169;&#24577;&#19987;&#23478;&#30340;&#35265;&#35299;&#12290;&#22312;&#27599;&#20010;&#36328;&#27169;&#24577;&#23618;&#20013;&#24341;&#20837;&#30340;&#31649;&#29702;&#22120;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#32858;&#21512;&#21333;&#27169;&#24577;&#35821;&#20041;&#30693;&#35782;&#65292;&#20197;&#20419;&#36827;&#26356;&#20840;&#38754;&#30340;&#36328;&#27169;&#24577;&#23545;&#40784;&#21644;&#34701;&#21512;&#12290;ManagerTower&#22312;&#26377;&#21644;&#27809;&#26377;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;(VLP)&#30340;&#24773;&#20917;&#19979;&#37117;&#20248;&#20110;&#20197;&#21069;&#30340;&#24378;&#22522;&#32447;&#12290;&#20165;&#20351;&#29992;4M VLP&#25968;&#25454;&#65292;ManagerTower&#22312;&#21508;&#31181;&#19979;&#28216;VL&#20219;&#21153;&#20013;&#37117;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;VQAv2&#27979;&#35797;&#26631;&#20934;&#19979;&#36798;&#21040;&#20102;79.15%&#30340;&#20934;&#30830;&#29575;&#65292;&#22312;Flickr30K&#19978;&#30340;IR@1&#20026;86.56% TR@1&#20026;95.64%&#12290;Code and check
&lt;/p&gt;
&lt;p&gt;
Two-Tower Vision-Language (VL) models have shown promising improvements on various downstream VL tasks. Although the most advanced work improves performance by building bridges between encoders, it suffers from ineffective layer-by-layer utilization of uni-modal representations and cannot flexibly exploit different levels of uni-modal semantic knowledge. In this work, we propose ManagerTower, a novel VL model architecture that gathers and combines the insights of pre-trained uni-modal experts at different levels. The managers introduced in each cross-modal layer can adaptively aggregate uni-modal semantic knowledge to facilitate more comprehensive cross-modal alignment and fusion. ManagerTower outperforms previous strong baselines both with and without Vision-Language Pre-training (VLP). With only 4M VLP data, ManagerTower achieves superior performances on various downstream VL tasks, especially 79.15% accuracy on VQAv2 Test-Std, 86.56% IR@1 and 95.64% TR@1 on Flickr30K. Code and check
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#32447;&#24615;Bandit&#30340;Pareto&#21069;&#27839;&#35782;&#21035;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20351;&#29992;&#25152;&#26377;&#21160;&#20316;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20855;&#26377;&#36739;&#20302;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24182;&#21516;&#26102;&#20445;&#35777;&#20102;Pareto&#21069;&#27839;&#35782;&#21035;&#21644;Pareto&#36951;&#25022;&#26368;&#23567;&#21270;&#12290;</title><link>http://arxiv.org/abs/2306.00096</link><description>&lt;p&gt;
&#36890;&#36807;&#36951;&#25022;&#26368;&#23567;&#21270;&#26041;&#27861;&#36827;&#34892;Pareto&#21069;&#27839;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Pareto Front Identification with Regret Minimization. (arXiv:2306.00096v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00096
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#32447;&#24615;Bandit&#30340;Pareto&#21069;&#27839;&#35782;&#21035;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20351;&#29992;&#25152;&#26377;&#21160;&#20316;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20855;&#26377;&#36739;&#20302;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24182;&#21516;&#26102;&#20445;&#35777;&#20102;Pareto&#21069;&#27839;&#35782;&#21035;&#21644;Pareto&#36951;&#25022;&#26368;&#23567;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#32447;&#24615;Bandit&#24773;&#20917;&#19979;&#30340;Pareto&#21069;&#27839;&#35782;&#21035;&#65288;PFILin&#65289;&#65292;&#20854;&#30446;&#26631;&#26159;&#22312;&#24179;&#22343;&#22870;&#21169;&#21521;&#37327;&#20316;&#20026;&#29615;&#22659;&#30340;&#32447;&#24615;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#35782;&#21035;&#19968;&#32452;&#22870;&#21169;&#21521;&#37327;&#19981;&#34987;&#20854;&#20182;&#20219;&#20309;&#21521;&#37327;&#25152;&#21344;&#20248;&#12290;PFILin&#21253;&#25324;&#26368;&#20339;&#21160;&#20316;&#35782;&#21035;&#21644;&#22810;&#30446;&#26631;&#20027;&#21160;&#23398;&#20064;&#31561;&#29305;&#27530;&#24773;&#20917;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;$\tilde{O}(d/\Delta^2)$&#65292;&#20854;&#20013;$d$&#26159;&#19978;&#19979;&#25991;&#30340;&#32500;&#25968;&#65292;$\Delta$&#26159;&#38382;&#39064;&#22797;&#26434;&#24615;&#30340;&#19968;&#31181;&#24230;&#37327;&#12290;&#25105;&#20204;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#22312;&#23545;&#25968;&#22240;&#23376;&#19978;&#26159;&#26368;&#20248;&#30340;&#12290;&#26412;&#31639;&#27861;&#30340;&#19968;&#20010;&#26032;&#29305;&#28857;&#26159;&#20351;&#29992;&#25152;&#26377;&#21160;&#20316;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#38500;&#20102;&#26377;&#25928;&#22320;&#35782;&#21035;Pareto&#21069;&#27839;&#20043;&#22806;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#36824;&#20445;&#35777;&#65292;&#22312;&#26679;&#26412;&#25968;&#22823;&#20110;$\Omega(d\log dL)$&#26102;&#65292;&#23545;&#20110;$L$&#32500;&#30690;&#37327;&#22870;&#21169;&#65292;&#30636;&#26102;Pareto&#36951;&#25022;&#30340;$\tilde{O}(\sqrt{d/t})$&#30028;&#38480;&#12290;&#36890;&#36807;&#20351;&#29992;&#25152;&#26377;&#21160;&#20316;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#21516;&#26102;&#20026;&#32447;&#24615;Bandit&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;Pareto&#21069;&#27839;&#35782;&#21035;&#21644;Pareto&#36951;&#25022;&#26368;&#23567;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider Pareto front identification for linear bandits (PFILin) where the goal is to identify a set of arms whose reward vectors are not dominated by any of the others when the mean reward vector is a linear function of the context. PFILin includes the best arm identification problem and multi-objective active learning as special cases. The sample complexity of our proposed algorithm is $\tilde{O}(d/\Delta^2)$, where $d$ is the dimension of contexts and $\Delta$ is a measure of problem complexity. Our sample complexity is optimal up to a logarithmic factor. A novel feature of our algorithm is that it uses the contexts of all actions. In addition to efficiently identifying the Pareto front, our algorithm also guarantees $\tilde{O}(\sqrt{d/t})$ bound for instantaneous Pareto regret when the number of samples is larger than $\Omega(d\log dL)$ for $L$ dimensional vector rewards. By using the contexts of all arms, our proposed algorithm simultaneously provides efficient Pareto front ide
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#33021;&#22815;&#23562;&#37325;&#20219;&#20309;&#19968;&#20010;&#32422;&#21270;&#26446;&#32676;G&#30340;&#26377;&#38480;&#32500;&#34920;&#31034;&#30340;&#23545;&#31216;&#24615;&#65292;&#24182;&#36890;&#36807;&#22312;&#22810;&#20010;&#39046;&#22495;&#30340;&#20219;&#21153;&#19978;&#36827;&#34892;&#23454;&#39564;&#26469;&#35777;&#26126;&#20854;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.00091</link><description>&lt;p&gt;
&#19968;&#20010;&#22312;&#32422;&#21270;&#26446;&#32676;&#19978;&#31561;&#21464;&#30340;&#31070;&#32463;&#32593;&#32476;&#36890;&#29992;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A General Framework for Equivariant Neural Networks on Reductive Lie Groups. (arXiv:2306.00091v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00091
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#33021;&#22815;&#23562;&#37325;&#20219;&#20309;&#19968;&#20010;&#32422;&#21270;&#26446;&#32676;G&#30340;&#26377;&#38480;&#32500;&#34920;&#31034;&#30340;&#23545;&#31216;&#24615;&#65292;&#24182;&#36890;&#36807;&#22312;&#22810;&#20010;&#39046;&#22495;&#30340;&#20219;&#21153;&#19978;&#36827;&#34892;&#23454;&#39564;&#26469;&#35777;&#26126;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32422;&#21270;&#26446;&#32676;&#65292;&#22914;&#27491;&#20132;&#32676;&#12289;&#27931;&#20262;&#20857;&#32676;&#25110;&#24186;&#27491;&#32676;&#65292;&#22312;&#39640;&#33021;&#29289;&#29702;&#12289;&#37327;&#23376;&#21147;&#23398;&#12289;&#37327;&#23376;&#33394;&#21160;&#21147;&#23398;&#12289;&#20998;&#23376;&#21160;&#21147;&#23398;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#25104;&#20687;&#31561;&#21508;&#20010;&#31185;&#23398;&#39046;&#22495;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#33021;&#22815;&#23562;&#37325;&#20219;&#20309;&#19968;&#20010;&#32422;&#21270;&#26446;&#32676;G&#30340;&#26377;&#38480;&#32500;&#34920;&#31034;&#30340;&#23545;&#31216;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25512;&#24191;&#20102;&#25104;&#21151;&#30340;ACE&#21644;MACE&#26550;&#26500;&#23545;&#20110;&#28857;&#20113;&#30340;&#21407;&#23376;&#32423;&#25968;&#25454;&#31561;&#21464;&#21040;&#20219;&#20309;&#19968;&#20010;&#23545;&#32422;&#21270;&#26446;&#32676;&#20316;&#29992;&#31561;&#21464;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;lie-nn&#36719;&#20214;&#24211;&#65292;&#25552;&#20379;&#20102;&#24320;&#21457;&#21644;&#23454;&#29616;&#36825;&#31181;&#36890;&#29992;G-&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#25152;&#38656;&#30340;&#25152;&#26377;&#24037;&#20855;&#12290;&#23427;&#23454;&#29616;&#20102;&#23558;&#34920;&#31034;&#30340;&#36890;&#29992;&#24352;&#37327;&#31215;&#20943;&#23569;&#21040;&#19981;&#21487;&#32422;&#34920;&#31034;&#30340;&#20363;&#34892;&#31243;&#24207;&#65292;&#20351;&#24471;&#25105;&#20204;&#30340;&#26550;&#26500;&#21487;&#20197;&#24212;&#29992;&#21040;&#21508;&#31181;&#38382;&#39064;&#21644;&#32676;&#20307;&#12290;&#36890;&#36807;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#65292;&#21253;&#25324;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#12289;&#25193;&#25955;MRI&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#26222;&#36866;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reductive Lie Groups, such as the orthogonal groups, the Lorentz group, or the unitary groups, play essential roles across scientific fields as diverse as high energy physics, quantum mechanics, quantum chromodynamics, molecular dynamics, computer vision, and imaging. In this paper, we present a general Equivariant Neural Network architecture capable of respecting the symmetries of the finite-dimensional representations of any reductive Lie Group G. Our approach generalizes the successful ACE and MACE architectures for atomistic point clouds to any data equivariant to a reductive Lie group action. We also introduce the lie-nn software library, which provides all the necessary tools to develop and implement such general G-equivariant neural networks. It implements routines for the reduction of generic tensor products of representations into irreducible representations, making it easy to apply our architecture to a wide range of problems and groups. The generality and performance of our 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#20851;&#31995;&#35745;&#31639;&#30340;&#33258;&#21160;&#24494;&#20998;&#26041;&#27861;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#36798;&#21040;&#38750;&#24120;&#22823;&#30340;&#25968;&#25454;&#38598;&#35268;&#27169;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#19982;&#19987;&#29992;&#31995;&#32479;&#30456;&#23218;&#32654;&#30340;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.00088</link><description>&lt;p&gt;
&#38754;&#21521;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#30340;&#20851;&#31995;&#35745;&#31639;&#33258;&#21160;&#24494;&#20998;
&lt;/p&gt;
&lt;p&gt;
Auto-Differentiation of Relational Computations for Very Large Scale Machine Learning. (arXiv:2306.00088v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#20851;&#31995;&#35745;&#31639;&#30340;&#33258;&#21160;&#24494;&#20998;&#26041;&#27861;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#36798;&#21040;&#38750;&#24120;&#22823;&#30340;&#25968;&#25454;&#38598;&#35268;&#27169;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#19982;&#19987;&#29992;&#31995;&#32479;&#30456;&#23218;&#32654;&#30340;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25968;&#25454;&#27169;&#22411;&#34987;&#35774;&#35745;&#29992;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#31649;&#29702;&#21644;&#20998;&#26512;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#27714;&#35299;&#20851;&#31995;&#35745;&#31639;&#20013;&#30340;&#33258;&#21160;&#24494;&#20998;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#19968;&#20010;&#36816;&#34892;&#33258;&#21160;&#24494;&#20998;&#20851;&#31995;&#31639;&#27861;&#30340;&#20851;&#31995;&#24341;&#25806;&#21487;&#20197;&#36731;&#26494;&#25193;&#23637;&#21040;&#38750;&#24120;&#22823;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#22312;&#22823;&#35268;&#27169;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#38754;&#19982;&#26368;&#20808;&#36827;&#30340;&#19987;&#29992;&#31995;&#32479;&#30456;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;
The relational data model was designed to facilitate large-scale data management and analytics. We consider the problem of how to differentiate computations expressed relationally. We show experimentally that a relational engine running an auto-differentiated relational algorithm can easily scale to very large datasets, and is competitive with state-of-the-art, special-purpose systems for large-scale distributed machine learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#21517;&#20026;&#8220;&#31038;&#20132;&#37325;&#32452;&#8221;&#30340;&#20219;&#21153;&#65292;&#20351;&#29992;&#36866;&#24212;&#24615;&#30340;&#21327;&#35843;&#26041;&#27861;&#65292;&#22312;&#27809;&#26377;&#29305;&#26435;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#20004;&#20010;&#26426;&#22120;&#20154;&#20351;&#29992;&#26412;&#22320;&#24863;&#30693;&#21644;&#33258;&#25105;&#21442;&#29031;&#35266;&#23519;&#26469;&#23436;&#25104;&#19968;&#20010;&#38271;&#26399;&#30340;&#20219;&#21153;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#34892;&#20026;&#22810;&#26679;&#24615;&#28216;&#25103;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#24110;&#21161;&#26426;&#22120;&#20154;&#22312;&#21327;&#35843;&#20013;&#29983;&#25104;&#26356;&#22810;&#26679;&#30340;&#34892;&#20026;&#65292;&#20855;&#26377;&#26356;&#24378;&#30340;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.00087</link><description>&lt;p&gt;
&#31038;&#20132;&#36523;&#20307;&#37325;&#32452;&#20013;&#30340;&#33258;&#36866;&#24212;&#21327;&#35843;
&lt;/p&gt;
&lt;p&gt;
Adaptive Coordination in Social Embodied Rearrangement. (arXiv:2306.00087v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00087
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#21517;&#20026;&#8220;&#31038;&#20132;&#37325;&#32452;&#8221;&#30340;&#20219;&#21153;&#65292;&#20351;&#29992;&#36866;&#24212;&#24615;&#30340;&#21327;&#35843;&#26041;&#27861;&#65292;&#22312;&#27809;&#26377;&#29305;&#26435;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#20004;&#20010;&#26426;&#22120;&#20154;&#20351;&#29992;&#26412;&#22320;&#24863;&#30693;&#21644;&#33258;&#25105;&#21442;&#29031;&#35266;&#23519;&#26469;&#23436;&#25104;&#19968;&#20010;&#38271;&#26399;&#30340;&#20219;&#21153;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#34892;&#20026;&#22810;&#26679;&#24615;&#28216;&#25103;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#24110;&#21161;&#26426;&#22120;&#20154;&#22312;&#21327;&#35843;&#20013;&#29983;&#25104;&#26356;&#22810;&#26679;&#30340;&#34892;&#20026;&#65292;&#20855;&#26377;&#26356;&#24378;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#8220;&#31038;&#20132;&#37325;&#32452;&#8221;&#20219;&#21153;&#65292;&#21253;&#25324;&#22312;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#19979;&#23436;&#25104;&#21512;&#20316;&#26085;&#24120;&#20219;&#21153;&#65292;&#22914;&#25670;&#39184;&#26700;&#12289;&#25972;&#29702;&#25151;&#23627;&#25110;&#21368;&#19979;&#26434;&#36135;&#12290;&#22312;&#31038;&#20132;&#37325;&#32452;&#20013;&#65292;&#20004;&#20010;&#26426;&#22120;&#20154;&#20351;&#29992;&#26412;&#22320;&#24863;&#30693;&#21644;&#33258;&#25105;&#21442;&#29031;&#35266;&#23519;&#26469;&#21327;&#35843;&#23436;&#25104;&#19968;&#20010;&#38271;&#26399;&#20219;&#21153;&#65292;&#27809;&#26377;&#20851;&#20110;&#29615;&#22659;&#30340;&#29305;&#26435;&#20449;&#24687;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#38646;&#23556;&#21327;&#35843;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#20854;&#20013;&#19968;&#20010;&#20195;&#29702;&#20154;&#19982;&#19968;&#20010;&#26032;&#30340;&#21512;&#20316;&#20249;&#20276;&#21327;&#35843;&#65292;&#27169;&#25311;&#19968;&#20010;&#26426;&#22120;&#20154;&#19982;&#19968;&#20010;&#26032;&#30340;&#20154;&#31867;&#21512;&#20316;&#20249;&#20276;&#21327;&#35843;&#30340;&#22330;&#26223;&#12290;&#20197;&#21069;&#30340;&#38646;&#23556;&#21327;&#35843;&#26041;&#27861;&#22312;&#25105;&#20204;&#22797;&#26434;&#21644;&#35270;&#35273;&#20016;&#23500;&#30340;&#29615;&#22659;&#20013;&#24456;&#38590;&#25512;&#24191;&#65292;&#36827;&#19968;&#27493;&#20998;&#26512;&#34920;&#26126;&#65292;&#23427;&#20204;&#22312;&#35757;&#32451;&#26102;&#26080;&#27861;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#21327;&#35843;&#34892;&#20026;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38646;&#23556;&#21327;&#35843;&#26041;&#27861;&#8212;&#8212;&#34892;&#20026;&#22810;&#26679;&#24615;&#28216;&#25103;(BDP)&#65292;&#36890;&#36807;&#19968;&#20010;&#21487;&#36776;&#21035;&#24615;&#30446;&#26631;&#40723;&#21169;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;BDP&#23398;&#20064;&#21040;&#21487;&#20197;&#22788;&#29702;&#35270;&#35273;&#21327;&#35843;&#21644;&#19982;&#21508;&#31181;&#20195;&#29702;&#20154;&#65288;&#21253;&#25324;&#20154;&#31867;&#65289;&#36827;&#34892;&#38646;&#23556;&#21327;&#35843;&#30340;&#33258;&#36866;&#24212;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present the task of "Social Rearrangement", consisting of cooperative everyday tasks like setting up the dinner table, tidying a house or unpacking groceries in a simulated multi-agent environment. In Social Rearrangement, two robots coordinate to complete a long-horizon task, using onboard sensing and egocentric observations, and no privileged information about the environment. We study zero-shot coordination (ZSC) in this task, where an agent collaborates with a new partner, emulating a scenario where a robot collaborates with a new human partner. Prior ZSC approaches struggle to generalize in our complex and visually rich setting, and on further analysis, we find that they fail to generate diverse coordination behaviors at training time. To counter this, we propose Behavior Diversity Play (BDP), a novel ZSC approach that encourages diversity through a discriminability objective. Our results demonstrate that BDP learns adaptive agents that can tackle visual coordination, and zero-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#22522;&#20110;&#20027;&#21160;&#35810;&#38382;&#20915;&#31574;&#32773;&#20010;&#20154;&#20559;&#22909;&#30340;&#32622;&#20449;&#24230;&#26500;&#36896;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#32622;&#20449;&#24230;&#23545;&#20110;&#20915;&#31574;&#32773;&#20449;&#20219;&#20915;&#31574;&#30340;&#19981;&#20934;&#30830;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#20915;&#31574;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.00074</link><description>&lt;p&gt;
&#20154;&#31867;&#23545;&#40784;&#26657;&#20934;&#29992;&#20110;AI&#36741;&#21161;&#20915;&#31574;&#21046;&#23450;
&lt;/p&gt;
&lt;p&gt;
Human-Aligned Calibration for AI-Assisted Decision Making. (arXiv:2306.00074v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00074
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#22522;&#20110;&#20027;&#21160;&#35810;&#38382;&#20915;&#31574;&#32773;&#20010;&#20154;&#20559;&#22909;&#30340;&#32622;&#20449;&#24230;&#26500;&#36896;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#32622;&#20449;&#24230;&#23545;&#20110;&#20915;&#31574;&#32773;&#20449;&#20219;&#20915;&#31574;&#30340;&#19981;&#20934;&#30830;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#20915;&#31574;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20351;&#29992;&#20108;&#20803;&#20998;&#31867;&#22120;&#25552;&#20379;&#20915;&#31574;&#25903;&#25345;&#26102;&#65292;&#23427;&#36890;&#24120;&#25552;&#20379;&#26631;&#31614;&#39044;&#27979;&#21644;&#32622;&#20449;&#24230;&#20540;&#12290;&#28982;&#21518;&#65292;&#20915;&#31574;&#32773;&#24212;&#20351;&#29992;&#32622;&#20449;&#24230;&#20540;&#26469;&#26657;&#20934;&#23545;&#39044;&#27979;&#30340;&#20449;&#20219;&#31243;&#24230;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20154;&#20204;&#32463;&#24120;&#35748;&#20026;&#32622;&#20449;&#24230;&#20540;&#24212;&#23545;&#39044;&#27979;&#26631;&#31614;&#19982;&#23454;&#38469;&#26631;&#31614;&#21305;&#37197;&#30340;&#27010;&#29575;&#36827;&#34892;&#33391;&#22909;&#26657;&#20934;&#30340;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#22810;&#26465;&#23454;&#35777;&#35777;&#25454;&#34920;&#26126;&#65292;&#20915;&#31574;&#32773;&#38590;&#20197;&#20351;&#29992;&#36825;&#20123;&#32622;&#20449;&#24230;&#20540;&#24456;&#22909;&#22320;&#30830;&#23450;&#20309;&#26102;&#20449;&#20219;&#39044;&#27979;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#39318;&#20808;&#26159;&#29702;&#35299;&#20026;&#20160;&#20040;&#65292;&#28982;&#21518;&#30740;&#31350;&#22914;&#20309;&#26500;&#24314;&#26356;&#26377;&#29992;&#30340;&#32622;&#20449;&#24230;&#20540;&#12290;&#25105;&#20204;&#39318;&#20808;&#35748;&#20026;&#65292;&#22312;&#24191;&#27867;&#31867;&#30340;&#25928;&#29992;&#20989;&#25968;&#20013;&#65292;&#23384;&#22312;&#25968;&#25454;&#20998;&#24067;&#65292;&#23545;&#20110;&#36825;&#20123;&#20998;&#24067;&#65292;&#29702;&#24615;&#20915;&#31574;&#32773;&#36890;&#24120;&#38590;&#20197;&#20351;&#29992;&#20197;&#19978;&#32622;&#20449;&#24230;&#20540;&#21457;&#29616;&#26368;&#20339;&#20915;&#31574;&#25919;&#31574;&#8212;&#8212;&#26368;&#20339;&#30340;&#20915;&#31574;&#32773;&#38656;&#35201;&#20154;&#31867;&#23545;&#40784;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#21160;&#35810;&#38382;&#20915;&#31574;&#32773;&#20182;&#20204;&#22312;&#25152;&#38754;&#20020;&#30340;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#30340;&#20915;&#31574;&#19978;&#30340;&#20010;&#20154;&#20559;&#22909;&#30340;&#26032;&#26041;&#27861;&#26469;&#26500;&#36896;&#32622;&#20449;&#24230;&#20540;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20135;&#29983;&#30340;&#32622;&#20449;&#24230;&#20540;&#27604;&#20351;&#29992;&#26631;&#20934;&#32622;&#20449;&#24230;&#24230;&#37327;&#23548;&#33268;&#26356;&#22909;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
Whenever a binary classifier is used to provide decision support, it typically provides both a label prediction and a confidence value. Then, the decision maker is supposed to use the confidence value to calibrate how much to trust the prediction. In this context, it has been often argued that the confidence value should correspond to a well calibrated estimate of the probability that the predicted label matches the ground truth label. However, multiple lines of empirical evidence suggest that decision makers have difficulties at developing a good sense on when to trust a prediction using these confidence values. In this paper, our goal is first to understand why and then investigate how to construct more useful confidence values. We first argue that, for a broad class of utility functions, there exist data distributions for which a rational decision maker is, in general, unlikely to discover the optimal decision policy using the above confidence values -- an optimal decision maker wou
&lt;/p&gt;</description></item><item><title>&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#20351;&#29992;&#37327;&#23376;&#35745;&#31639;&#26426;&#36827;&#34892;&#35780;&#20272;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#22312;&#35757;&#32451;&#23436;&#21518;&#65292;&#20351;&#29992;&#37327;&#23376;&#35745;&#31639;&#26426;&#29983;&#25104;&#19968;&#20010;&#32463;&#20856;&#38452;&#24433;&#27169;&#22411;&#26469;&#35745;&#31639;&#20989;&#25968;&#30340;&#32463;&#20856;&#35745;&#31639;&#36817;&#20284;&#65292;&#36991;&#20813;&#20102;&#23545;&#37327;&#23376;&#35745;&#31639;&#26426;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2306.00061</link><description>&lt;p&gt;
&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#30340;&#38452;&#24433;
&lt;/p&gt;
&lt;p&gt;
Shadows of quantum machine learning. (arXiv:2306.00061v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00061
&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#20351;&#29992;&#37327;&#23376;&#35745;&#31639;&#26426;&#36827;&#34892;&#35780;&#20272;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#22312;&#35757;&#32451;&#23436;&#21518;&#65292;&#20351;&#29992;&#37327;&#23376;&#35745;&#31639;&#26426;&#29983;&#25104;&#19968;&#20010;&#32463;&#20856;&#38452;&#24433;&#27169;&#22411;&#26469;&#35745;&#31639;&#20989;&#25968;&#30340;&#32463;&#20856;&#35745;&#31639;&#36817;&#20284;&#65292;&#36991;&#20813;&#20102;&#23545;&#37327;&#23376;&#35745;&#31639;&#26426;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#32463;&#24120;&#34987;&#35748;&#20026;&#26159;&#21033;&#29992;&#37327;&#23376;&#35745;&#31639;&#26426;&#35299;&#20915;&#23454;&#38469;&#38382;&#39064;&#30340;&#26368;&#26377;&#21069;&#36884;&#30340;&#24212;&#29992;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#38459;&#30861;&#20854;&#22312;&#23454;&#36341;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#20027;&#35201;&#38556;&#30861;&#26159;&#36825;&#20123;&#27169;&#22411;&#21363;&#20351;&#22312;&#35757;&#32451;&#36807;&#31243;&#21518;&#65292;&#20173;&#38656;&#35201;&#35775;&#38382;&#37327;&#23376;&#35745;&#31639;&#26426;&#25165;&#33021;&#23545;&#26032;&#25968;&#25454;&#36827;&#34892;&#35780;&#20272;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#35758;&#22312;&#37327;&#23376;&#27169;&#22411;&#30340;&#35757;&#32451;&#38454;&#27573;&#20043;&#21518;&#65292;&#37327;&#23376;&#35745;&#31639;&#26426;&#21487;&#20197;&#29992;&#26469;&#29983;&#25104;&#25105;&#20204;&#25152;&#35859;&#30340;&#35813;&#27169;&#22411;&#30340;&#8220;&#32463;&#20856;&#38452;&#24433;&#8221;&#65292;&#21363;&#24050;&#23398;&#20064;&#20989;&#25968;&#30340;&#32463;&#20856;&#35745;&#31639;&#36817;&#20284;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#25506;&#35752;&#20102;&#36825;&#20010;&#24819;&#27861;&#24182;&#25552;&#20986;&#20102;&#26500;&#24314;&#36825;&#31181;&#24433;&#23376;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#20063;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#20840;&#32463;&#20856;&#27169;&#22411;&#21487;&#33021;&#20195;&#26367;&#30340;&#21487;&#33021;&#24615;&#65292;&#20174;&#32780;&#39318;&#20808;&#22238;&#36991;&#20102;&#37327;&#23376;&#35745;&#31639;&#26426;&#30340;&#38656;&#35201;&#12290;&#26412;&#25991;&#37319;&#29992;&#26032;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#37327;&#23376;&#32447;&#24615;&#27169;&#22411;&#21644;&#32463;&#20856;&#38452;&#24433;&#37325;&#26500;&#30340;&#26694;&#26550;&#26469;&#23450;&#20041;&#38452;&#24433;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum machine learning is often highlighted as one of the most promising uses for a quantum computer to solve practical problems. However, a major obstacle to the widespread use of quantum machine learning models in practice is that these models, even once trained, still require access to a quantum computer in order to be evaluated on new data. To solve this issue, we suggest that following the training phase of a quantum model, a quantum computer could be used to generate what we call a classical shadow of this model, i.e., a classically computable approximation of the learned function. While recent works already explore this idea and suggest approaches to construct such shadow models, they also raise the possibility that a completely classical model could be trained instead, thus circumventing the need for a quantum computer in the first place. In this work, we take a novel approach to define shadow models based on the frameworks of quantum linear models and classical shadow tomogr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#36827;&#21270;&#31574;&#30053;&#31639;&#27861;&#20855;&#26377;&#39640;&#24230;&#31232;&#30095;&#21487;&#35757;&#32451;&#30340;&#21021;&#22987;&#21442;&#25968;&#65292;&#24182;&#27604;&#36739;&#20102;&#19982;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#31232;&#30095;&#35757;&#32451;&#30340;&#24046;&#24322;&#12290;&#36827;&#21270;&#31574;&#30053;&#31639;&#27861;&#21487;&#20197;&#25506;&#32034;&#21508;&#31181;&#19981;&#21516;&#30340;&#12289;&#24179;&#22374;&#30340;&#23616;&#37096;&#26368;&#20248;&#35299;&#65292;&#24182;&#19988;&#19981;&#20250;&#20445;&#30041;&#32447;&#24615;&#27169;&#24335;&#30340;&#36830;&#25509;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.00045</link><description>&lt;p&gt;
&#36827;&#21270;&#20248;&#21270;&#20013;&#30340;&#24425;&#31080;&#31080;&#29616;&#35937;&#65306;&#31232;&#30095;&#19988;&#26080;&#38656;&#21453;&#21521;&#20256;&#25773;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Lottery Tickets in Evolutionary Optimization: On Sparse Backpropagation-Free Trainability. (arXiv:2306.00045v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#36827;&#21270;&#31574;&#30053;&#31639;&#27861;&#20855;&#26377;&#39640;&#24230;&#31232;&#30095;&#21487;&#35757;&#32451;&#30340;&#21021;&#22987;&#21442;&#25968;&#65292;&#24182;&#27604;&#36739;&#20102;&#19982;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#31232;&#30095;&#35757;&#32451;&#30340;&#24046;&#24322;&#12290;&#36827;&#21270;&#31574;&#30053;&#31639;&#27861;&#21487;&#20197;&#25506;&#32034;&#21508;&#31181;&#19981;&#21516;&#30340;&#12289;&#24179;&#22374;&#30340;&#23616;&#37096;&#26368;&#20248;&#35299;&#65292;&#24182;&#19988;&#19981;&#20250;&#20445;&#30041;&#32447;&#24615;&#27169;&#24335;&#30340;&#36830;&#25509;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24425;&#31080;&#31080;&#29616;&#35937;&#26159;&#21542;&#21482;&#23384;&#22312;&#20110;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#20013;&#65292;&#36824;&#26159;&#21487;&#20197;&#25512;&#24191;&#21040;&#36827;&#21270;&#20248;&#21270;&#31639;&#27861;&#20013;&#65311;&#26412;&#25991;&#35777;&#26126;&#20102;&#36827;&#21270;&#31574;&#30053;&#31639;&#27861;&#20855;&#26377;&#39640;&#24230;&#31232;&#30095;&#21487;&#35757;&#32451;&#30340;&#21021;&#22987;&#21442;&#25968;&#65292;&#24182;&#27604;&#36739;&#20102;&#19982;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#31232;&#30095;&#35757;&#32451;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20449;&#22122;&#27604;&#36845;&#20195;&#21098;&#26525;&#36807;&#31243;&#65292;&#23558;&#25439;&#22833;&#26354;&#29575;&#20449;&#24687;&#34701;&#20837;&#21040;&#32593;&#32476;&#21098;&#26525;&#27493;&#39588;&#20013;&#65292;&#21487;&#20197;&#21457;&#29616;&#19982;&#26799;&#24230;&#19979;&#38477;&#30456;&#27604;&#65292;&#40657;&#30418;&#36827;&#21270;&#31639;&#27861;&#26041;&#27861;&#26377;&#21487;&#33021;&#21457;&#29616;&#26356;&#31232;&#30095;&#19988;&#21487;&#35757;&#32451;&#30340;&#32593;&#32476;&#21021;&#22987;&#21442;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#21021;&#22987;&#21442;&#25968;&#21253;&#21547;&#20102;&#24402;&#32435;&#20559;&#35265;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#30340;&#36827;&#21270;&#31574;&#30053;&#20219;&#21153;&#21450;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#20013;&#36827;&#34892;&#20256;&#36882;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#20248;&#21270;&#31639;&#27861;&#21644;&#31232;&#30095;&#27700;&#24179;&#20135;&#29983;&#30340;&#23616;&#37096;&#26368;&#20248;&#35299;&#65292;&#19982;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#19981;&#21516;&#65292;&#36827;&#21270;&#31574;&#30053;&#31639;&#27861;&#21487;&#20197;&#25506;&#32034;&#21508;&#31181;&#19981;&#21516;&#30340;&#12289;&#24179;&#22374;&#30340;&#23616;&#37096;&#26368;&#20248;&#35299;&#65292;&#24182;&#19988;&#19981;&#20250;&#20445;&#30041;&#32447;&#24615;&#27169;&#24335;&#30340;&#36830;&#25509;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Is the lottery ticket phenomenon an idiosyncrasy of gradient-based training or does it generalize to evolutionary optimization? In this paper we establish the existence of highly sparse trainable initializations for evolution strategies (ES) and characterize qualitative differences compared to gradient descent (GD)-based sparse training. We introduce a novel signal-to-noise iterative pruning procedure, which incorporates loss curvature information into the network pruning step. This can enable the discovery of even sparser trainable network initializations when using black-box evolution as compared to GD-based optimization. Furthermore, we find that these initializations encode an inductive bias, which transfers across different ES, related tasks and even to GD-based training. Finally, we compare the local optima resulting from the different optimization paradigms and sparsity levels. In contrast to GD, ES explore diverse and flat local optima and do not preserve linear mode connectivi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#26469;&#35782;&#21035;&#28145;&#24230;&#23398;&#20064;&#20026;&#22522;&#30784;&#30340;&#21453;&#27450;&#39575;&#23545;&#31574;&#20013;&#30340;&#24555;&#25463;&#26041;&#24335;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#24555;&#25463;&#26041;&#24335;&#30340;&#23384;&#22312;&#65292;&#20998;&#26512;&#20102;&#22914;&#20309;&#24433;&#21709;&#31867;&#26465;&#20214;&#24471;&#20998;&#32479;&#35745;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2306.00044</link><description>&lt;p&gt;
&#22914;&#20309;&#26500;&#24314;&#23436;&#32654;&#30340;&#21453;&#27450;&#39575;&#23545;&#31574;&#21450;&#27604;&#30606;&#29468;&#36824;&#24046;&#30340;&#23545;&#31574;&#65306;&#20851;&#20110;&#24555;&#25463;&#23398;&#20064;&#30340;&#35686;&#21578;&#65288;arXiv:2306.00044v1 [cs.LG]&#65289;
&lt;/p&gt;
&lt;p&gt;
How to Construct Perfect and Worse-than-Coin-Flip Spoofing Countermeasures: A Word of Warning on Shortcut Learning. (arXiv:2306.00044v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00044
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#26469;&#35782;&#21035;&#28145;&#24230;&#23398;&#20064;&#20026;&#22522;&#30784;&#30340;&#21453;&#27450;&#39575;&#23545;&#31574;&#20013;&#30340;&#24555;&#25463;&#26041;&#24335;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#24555;&#25463;&#26041;&#24335;&#30340;&#23384;&#22312;&#65292;&#20998;&#26512;&#20102;&#22914;&#20309;&#24433;&#21709;&#31867;&#26465;&#20214;&#24471;&#20998;&#32479;&#35745;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24555;&#25463;&#23398;&#20064;&#65292;&#25110;&#31216;&#20026;&#8220;&#32874;&#26126;&#27721;&#26031;&#25928;&#24212;&#8221;&#65292;&#25351;&#30340;&#26159;&#23398;&#20064;&#20195;&#29702;&#65288;&#20363;&#22914;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65289;&#23398;&#20064;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#20266;&#30456;&#20851;&#24615;&#65292;&#23548;&#33268;&#26377;&#20559;&#24046;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#23547;&#25214;&#28145;&#24230;&#23398;&#20064;&#20026;&#22522;&#30784;&#30340;&#21453;&#27450;&#39575;&#23545;&#31574;&#65288;CMs&#65289;&#20013;&#21457;&#29616;&#20102;&#24555;&#25463;&#26041;&#24335;&#65292;&#36825;&#20123;&#23545;&#31574;&#29992;&#20110;&#39044;&#27979;&#32473;&#23450;&#35805;&#35821;&#26159;&#21542;&#34987;&#27450;&#39575;&#12290;&#34429;&#28982;&#20808;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#28041;&#21450;&#29305;&#23450;&#30340;&#25968;&#25454;&#20266;&#35013;&#25216;&#24039;&#65292;&#20363;&#22914;&#38745;&#40664;&#65292;&#20294;&#23578;&#26410;&#25506;&#32034;&#20998;&#26512;CMs&#30340;&#24555;&#25463;&#23398;&#20064;&#30340;&#36890;&#29992;&#35268;&#33539;&#26694;&#26550;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#26469;&#35782;&#21035;&#24555;&#25463;&#26041;&#24335;&#65292;&#21253;&#25324;&#23545;&#35757;&#32451;&#21644;&#27979;&#35797;&#26041;&#38754;&#30340;&#31995;&#32479;&#24178;&#39044;&#65292;&#21253;&#25324;&#8220;&#36817;&#20046;&#23436;&#32654;&#8221;&#21644;&#8220;&#27604;&#30606;&#29468;&#36824;&#24046;&#8221;&#65288;&#26631;&#31614;&#32763;&#36716;&#65289;&#30340;&#36793;&#30028;&#24773;&#20917;&#12290;&#36890;&#36807;&#20351;&#29992;&#20174;&#32463;&#20856;&#21040;&#26368;&#20808;&#36827;&#30340;&#19977;&#31181;&#19981;&#21516;&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20116;&#20010;&#27169;&#25311;&#26465;&#20214;&#19979;&#23384;&#22312;&#24555;&#25463;&#23398;&#20064;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#20351;&#29992;&#22238;&#24402;&#27169;&#22411;&#20998;&#26512;&#32467;&#26524;&#65292;&#20197;&#20102;&#35299;&#20559;&#24046;&#22914;&#20309;&#24433;&#21709;&#31867;&#26465;&#20214;&#24471;&#20998;&#32479;&#35745;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Shortcut learning, or `Clever Hans effect` refers to situations where a learning agent (e.g., deep neural networks) learns spurious correlations present in data, resulting in biased models. We focus on finding shortcuts in deep learning based spoofing countermeasures (CMs) that predict whether a given utterance is spoofed or not. While prior work has addressed specific data artifacts, such as silence, no general normative framework has been explored for analyzing shortcut learning in CMs. In this study, we propose a generic approach to identifying shortcuts by introducing systematic interventions on the training and test sides, including the boundary cases of `near-perfect` and `worse than coin flip` (label flip). By using three different models, ranging from classic to state-of-the-art, we demonstrate the presence of shortcut learning in five simulated conditions. We analyze the results using a regression model to understand how biases affect the class-conditional score statistics.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#22270;&#24418;&#30340;&#26041;&#27861;&#32467;&#21512;&#29305;&#23450;&#20998;&#24067;&#36317;&#31163;&#26469;&#26816;&#27979;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#36890;&#36807;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#30340;&#22270;&#32467;&#26500;&#65292;&#20171;&#32461;&#20102;&#29992;&#20110;&#39044;&#27979;&#21644;&#35299;&#37322;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#29305;&#23450;&#27979;&#37327;&#26041;&#27861;&#65292;&#36825;&#26377;&#21161;&#20110;&#30740;&#31350;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#20869;&#22312;&#26426;&#29702;&#12290;</title><link>http://arxiv.org/abs/2306.00042</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#24418;&#30340;&#26041;&#27861;&#19982;&#29305;&#23450;&#20998;&#24067;&#36317;&#31163;&#30456;&#32467;&#21512;&#30340;&#23545;&#25239;&#25915;&#20987;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Graph-based methods coupled with specific distributional distances for adversarial attack detection. (arXiv:2306.00042v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00042
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#22270;&#24418;&#30340;&#26041;&#27861;&#32467;&#21512;&#29305;&#23450;&#20998;&#24067;&#36317;&#31163;&#26469;&#26816;&#27979;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#36890;&#36807;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#30340;&#22270;&#32467;&#26500;&#65292;&#20171;&#32461;&#20102;&#29992;&#20110;&#39044;&#27979;&#21644;&#35299;&#37322;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#29305;&#23450;&#27979;&#37327;&#26041;&#27861;&#65292;&#36825;&#26377;&#21161;&#20110;&#30740;&#31350;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#20869;&#22312;&#26426;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#34987;&#31934;&#24515;&#25200;&#21160;&#30340;&#36755;&#20837;&#25152;&#27450;&#39575;&#65292;&#23548;&#33268;&#20005;&#37325;&#30340;&#35823;&#20998;&#31867;&#12290;&#36825;&#20123;&#8220;&#23545;&#25239;&#24615;&#8221;&#25915;&#20987;&#24050;&#25104;&#20026;&#24191;&#27867;&#30740;&#31350;&#30340;&#28966;&#28857;&#12290;&#21516;&#26679;&#65292;&#23545;&#25239;&#25915;&#20987;&#30340;&#26816;&#27979;&#21644;&#38450;&#24481;&#20063;&#26377;&#22823;&#37327;&#30740;&#31350;&#12290;&#25105;&#20204;&#20174;&#22270;&#30340;&#35282;&#24230;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#21644;&#35299;&#37322;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#23545;&#20110;&#22270;&#20687;&#65292;&#26080;&#35770;&#26159;&#33391;&#24615;&#30340;&#36824;&#26159;&#23545;&#25239;&#24615;&#30340;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#22914;&#20309;&#24341;&#20837;&#19968;&#20010;&#30456;&#20851;&#30340;&#22270;&#24418;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20010;&#22270;&#24418;&#65292;&#24182;&#24341;&#20837;&#20102;&#29992;&#20110;&#39044;&#27979;&#21644;&#35299;&#37322;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#29305;&#23450;&#27979;&#37327;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;&#22270;&#24418;&#30340;&#26041;&#27861;&#26377;&#21161;&#20110;&#30740;&#31350;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#20869;&#22312;&#26426;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial neural networks are prone to being fooled by carefully perturbed inputs which cause an egregious misclassification. These \textit{adversarial} attacks have been the focus of extensive research. Likewise, there has been an abundance of research in ways to detect and defend against them. We introduce a novel approach of detection and interpretation of adversarial attacks from a graph perspective. For an image, benign or adversarial, we study how a neural network's architecture can induce an associated graph. We study this graph and introduce specific measures used to predict and interpret adversarial attacks. We show that graphs-based approaches help to investigate the inner workings of adversarial attacks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#24178;&#39044;&#30340;&#32622;&#20449;&#24230;&#27979;&#37327;&#26041;&#27861;&#26469;&#25552;&#39640;&#33647;&#29289;&#38774;&#28857;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#30528;&#37325;&#35299;&#20915;&#20102;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;&#30340;&#19981;&#36275;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.00041</link><description>&lt;p&gt;
&#22522;&#20110;&#22240;&#26524;&#24178;&#39044;&#30340;&#33647;&#29289;&#38774;&#28857;&#30456;&#20114;&#20316;&#29992;&#21487;&#20449;&#24230;&#27979;&#37327;&#26041;&#27861;&#30340;&#30740;&#31350;&#19982;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
Research And Implementation Of Drug Target Interaction Confidence Measurement Method Based On Causal Intervention. (arXiv:2306.00041v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00041
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#24178;&#39044;&#30340;&#32622;&#20449;&#24230;&#27979;&#37327;&#26041;&#27861;&#26469;&#25552;&#39640;&#33647;&#29289;&#38774;&#28857;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#30528;&#37325;&#35299;&#20915;&#20102;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;&#30340;&#19981;&#36275;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#19982;&#38774;&#28857;&#30456;&#20114;&#20316;&#29992;&#65288;DTI&#65289;&#30340;&#35782;&#21035;&#21644;&#21457;&#29616;&#26159;&#33647;&#29289;&#30740;&#31350;&#21644;&#24320;&#21457;&#20013;&#30340;&#37325;&#35201;&#27493;&#39588;&#65292;&#21487;&#20197;&#24110;&#21161;&#31185;&#23398;&#23478;&#21457;&#29616;&#26032;&#33647;&#24182;&#21152;&#36895;&#24320;&#21457;&#36807;&#31243;&#12290;&#36817;&#24180;&#26469;&#65292;&#30693;&#35782;&#22270;&#35889;&#21644;&#30456;&#20851;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65288;KGE&#65289;&#27169;&#22411;&#22312;&#33647;&#29289;&#21457;&#29616;&#39046;&#22495;&#36805;&#36895;&#21457;&#23637;&#24182;&#34920;&#29616;&#20986;&#33391;&#22909;&#24615;&#33021;&#12290;&#22312;&#33647;&#29289;&#38774;&#28857;&#35782;&#21035;&#20219;&#21153;&#20013;&#65292;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#21644;&#20934;&#30830;&#24615;&#19981;&#36275;&#20250;&#23548;&#33268;&#35823;&#21028;&#29575;&#22686;&#21152;&#21644;&#33647;&#29289;&#24320;&#21457;&#25928;&#29575;&#20302;&#19979;&#12290;&#20026;&#35299;&#20915;&#20197;&#19978;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#32858;&#28966;&#20110;&#20197;&#30693;&#35782;&#26144;&#23556;&#20026;&#26680;&#24515;&#25216;&#26415;&#30340;&#33647;&#29289;&#38774;&#28857;&#38142;&#25509;&#39044;&#27979;&#38382;&#39064;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#22240;&#26524;&#24178;&#39044;&#30340;&#32622;&#20449;&#24230;&#27979;&#37327;&#26041;&#27861;&#26469;&#27979;&#37327;&#19977;&#20803;&#32452;&#24471;&#20998;&#65292;&#20174;&#32780;&#25552;&#39640;&#33647;&#29289;&#38774;&#28857;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#22312;&#19981;&#21516;&#30340;KGE&#27169;&#22411;&#19978;&#19982;&#20256;&#32479;&#30340;Softmax&#21644;Sigmod&#32622;&#20449;&#24230;&#27979;&#37327;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
The identification and discovery of drug-target Interaction (DTI) is an important step in the field of Drug research and development, which can help scientists discover new drugs and accelerate the development process. KnowledgeGraph and the related knowledge graph Embedding (KGE) model develop rapidly and show good performance in the field of drug discovery in recent years. In the task of drug target identification, the lack of authenticity and accuracy of the model will lead to the increase of misjudgment rate and the low efficiency of drug development. To solve the above problems, this study focused on the problem of drug target link prediction with knowledge mapping as the core technology, and adopted the confidence measurement method based on causal intervention to measure the triplet score, so as to improve the accuracy of drug target interaction prediction model. By comparing with the traditional Softmax and Sigmod confidence measurement methods on different KGE models, the resu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#24037;&#20316;&#27969;&#31243;&#26469;&#35780;&#20272;&#24615;&#33021;&#39044;&#27979;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#30340;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#19978;&#36827;&#34892;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2306.00040</link><description>&lt;p&gt;
&#35780;&#20272;&#24615;&#33021;&#39044;&#27979;&#27169;&#22411;&#30340;&#26222;&#36866;&#24615;
&lt;/p&gt;
&lt;p&gt;
Assessing the Generalizability of a Performance Predictive Model. (arXiv:2306.00040v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00040
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#24037;&#20316;&#27969;&#31243;&#26469;&#35780;&#20272;&#24615;&#33021;&#39044;&#27979;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#30340;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#19978;&#36827;&#34892;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#31639;&#27861;&#36873;&#25321;&#21644;&#37197;&#32622;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#26159;&#20248;&#31168;&#30340;&#39044;&#27979;&#24615;&#33021;&#27169;&#22411;&#65292;&#32780;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#36825;&#26159;&#36890;&#36807;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#23454;&#29616;&#30340;&#12290;&#35813;&#27169;&#22411;&#20351;&#29992;&#38382;&#39064;&#23454;&#20363;&#30340;&#29305;&#24449;&#34920;&#31034;&#20316;&#20026;&#36755;&#20837;&#25968;&#25454;&#65292;&#24182;&#39044;&#27979;&#20854;&#34920;&#29616;&#25152;&#20351;&#29992;&#30340;&#31639;&#27861;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#24120;&#35265;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24456;&#38590;&#23545;&#26410;&#34987;&#35757;&#32451;&#25968;&#25454;&#35206;&#30422;&#30340;&#29305;&#24449;&#34920;&#31034;&#36827;&#34892;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#23548;&#33268;&#23545;&#26410;&#35265;&#38382;&#39064;&#30340;&#27867;&#21270;&#33021;&#21147;&#36739;&#24046;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24037;&#20316;&#27969;&#31243;&#65292;&#29992;&#20110;&#35780;&#20272;&#22312;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#19978;&#35757;&#32451;&#30340;&#39044;&#27979;&#24615;&#33021;&#27169;&#22411;&#23545;&#21478;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#30340;&#26222;&#36866;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#20043;&#38388;&#35757;&#32451;&#39044;&#27979;&#24615;&#33021;&#27169;&#22411;&#26469;&#27979;&#35797;&#24037;&#20316;&#27969;&#31243;&#65292;&#24182;&#21457;&#29616;&#26223;&#35266;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#26222;&#36866;&#24615;&#27169;&#24335;&#22312;&#24615;&#33021;&#31354;&#38388;&#20013;&#24471;&#21040;&#20102;&#20307;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key component of automated algorithm selection and configuration, which in most cases are performed using supervised machine learning (ML) methods is a good-performing predictive model. The predictive model uses the feature representation of a set of problem instances as input data and predicts the algorithm performance achieved on them. Common machine learning models struggle to make predictions for instances with feature representations not covered by the training data, resulting in poor generalization to unseen problems. In this study, we propose a workflow to estimate the generalizability of a predictive model for algorithm performance, trained on one benchmark suite to another. The workflow has been tested by training predictive models across benchmark suites and the results show that generalizability patterns in the landscape feature space are reflected in the performance space.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedCSD&#30340;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#20195;&#30721;&#24322;&#21619;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#35753;&#32452;&#32455;&#21327;&#20316;&#35757;&#32451;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#22312;&#19977;&#20010;&#23454;&#39564;&#20013;&#20998;&#21035;&#20351;&#29992;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#65292;&#23454;&#29616;&#20102;&#39640;&#31934;&#24230;&#30340;&#26816;&#27979;&#25928;&#26524;&#65292;&#24182;&#19988;&#27604;&#38598;&#20013;&#24335;&#21644;&#20132;&#21449;&#39564;&#35777;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.00038</link><description>&lt;p&gt;
&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#20195;&#30721;&#24322;&#21619;&#26816;&#27979;&#26041;&#27861;(FedCSD)
&lt;/p&gt;
&lt;p&gt;
FedCSD: A Federated Learning Based Approach for Code-Smell Detection. (arXiv:2306.00038v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedCSD&#30340;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#20195;&#30721;&#24322;&#21619;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#35753;&#32452;&#32455;&#21327;&#20316;&#35757;&#32451;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#22312;&#19977;&#20010;&#23454;&#39564;&#20013;&#20998;&#21035;&#20351;&#29992;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#65292;&#23454;&#29616;&#20102;&#39640;&#31934;&#24230;&#30340;&#26816;&#27979;&#25928;&#26524;&#65292;&#24182;&#19988;&#27604;&#38598;&#20013;&#24335;&#21644;&#20132;&#21449;&#39564;&#35777;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedCSD&#30340;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#20195;&#30721;&#24322;&#21619;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#35753;&#32452;&#32455;&#21327;&#20316;&#35757;&#32451;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#12290;&#36890;&#36807;&#19977;&#20010;&#23454;&#39564;&#26469;&#25903;&#25345;&#36825;&#20123;&#26029;&#35328;&#65292;&#36825;&#20123;&#23454;&#39564;&#21033;&#29992;&#20102;&#19977;&#20010;&#25163;&#21160;&#39564;&#35777;&#30340;&#25968;&#25454;&#38598;&#65292;&#26469;&#26816;&#27979;&#21644;&#30740;&#31350;&#19981;&#21516;&#30340;&#20195;&#30721;&#24322;&#21619;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a Federated Learning Code Smell Detection (FedCSD) approach that allows organizations to collaboratively train federated ML models while preserving their data privacy. These assertions have been supported by three experiments that have significantly leveraged three manually validated datasets aimed at detecting and examining different code smell scenarios. In experiment 1, which was concerned with a centralized training experiment, dataset two achieved the lowest accuracy (92.30%) with fewer smells, while datasets one and three achieved the highest accuracy with a slight difference (98.90% and 99.5%, respectively). This was followed by experiment 2, which was concerned with cross-evaluation, where each ML model was trained using one dataset, which was then evaluated over the other two datasets. Results from this experiment show a significant drop in the model's accuracy (lowest accuracy: 63.80\%) where fewer smells exist in the training dataset, which has a noticeab
&lt;/p&gt;</description></item><item><title>ROSARL&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22870;&#21169;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#20041;&#8220;Minmax&#24809;&#32602;&#8221;&#30830;&#23450;&#26234;&#33021;&#20307;&#22312;&#36798;&#21040;&#19981;&#23433;&#20840;&#29366;&#24577;&#26102;&#25152;&#20801;&#35768;&#30340;&#22870;&#21169;&#19978;&#38480;&#65292;&#24182;&#32771;&#34385;&#29615;&#22659;&#30340;&#21487;&#25511;&#24615;&#21644;&#30452;&#24452;&#26469;&#33719;&#24471;&#36825;&#20010;&#19978;&#38480;&#12290;</title><link>http://arxiv.org/abs/2306.00035</link><description>&lt;p&gt;
ROSARL: &#22522;&#20110;&#22870;&#21169;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
ROSARL: Reward-Only Safe Reinforcement Learning. (arXiv:2306.00035v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00035
&lt;/p&gt;
&lt;p&gt;
ROSARL&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22870;&#21169;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#20041;&#8220;Minmax&#24809;&#32602;&#8221;&#30830;&#23450;&#26234;&#33021;&#20307;&#22312;&#36798;&#21040;&#19981;&#23433;&#20840;&#29366;&#24577;&#26102;&#25152;&#20801;&#35768;&#30340;&#22870;&#21169;&#19978;&#38480;&#65292;&#24182;&#32771;&#34385;&#29615;&#22659;&#30340;&#21487;&#25511;&#24615;&#21644;&#30452;&#24452;&#26469;&#33719;&#24471;&#36825;&#20010;&#19978;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#26159;&#35774;&#35745;&#33021;&#22815;&#22312;&#29615;&#22659;&#20013;&#23433;&#20840;&#23398;&#20064;&#23436;&#25104;&#20219;&#21153;&#30340;&#26234;&#33021;&#20307;&#12290;&#24120;&#35265;&#30340;&#35299;&#20915;&#26041;&#27861;&#26159;&#30001;&#20154;&#31867;&#19987;&#23478;&#23450;&#20041;&#19968;&#20010;&#22870;&#21169;&#20989;&#25968;&#20013;&#30340;&#24809;&#32602;&#25110;&#35201;&#26368;&#23567;&#21270;&#30340;&#25104;&#26412;&#65292;&#20197;&#36798;&#21040;&#19981;&#36208;&#20837;&#21361;&#38505;&#29366;&#24577;&#30340;&#30446;&#30340;&#12290;&#28982;&#32780;&#65292;&#22826;&#23567;&#30340;&#24809;&#32602;&#21487;&#33021;&#20250;&#23548;&#33268;&#26234;&#33021;&#20307;&#36827;&#20837;&#19981;&#23433;&#20840;&#30340;&#29366;&#24577;&#65292;&#32780;&#22826;&#22823;&#30340;&#24809;&#32602;&#21017;&#20250;&#22686;&#21152;&#25910;&#25947;&#26102;&#38388;&#12290;&#27492;&#22806;&#65292;&#35774;&#35745;&#22870;&#21169;&#25110;&#25104;&#26412;&#20989;&#25968;&#30340;&#38590;&#24230;&#38543;&#30528;&#38382;&#39064;&#30340;&#22797;&#26434;&#24230;&#32780;&#22686;&#21152;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#32473;&#23450;&#30340;&#29615;&#22659;&#21644;&#19968;&#32452;&#19981;&#23433;&#20840;&#29366;&#24577;&#65292;&#25105;&#20204;&#24076;&#26395;&#25214;&#21040;&#22312;&#19981;&#32771;&#34385;&#20219;&#21153;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#65292;&#26368;&#23567;&#21270;&#21040;&#36798;&#19981;&#23433;&#20840;&#29366;&#24577;&#30340;&#27010;&#29575;&#30340;&#26368;&#39640;&#22870;&#21169;&#19978;&#38480;&#12290;&#25105;&#20204;&#31216;&#36825;&#20010;&#30830;&#20999;&#30340;&#19978;&#38480;&#20026;&#8220;Minmax&#24809;&#32602;&#8221;&#65292;&#24182;&#23637;&#31034;&#20102;&#21487;&#20197;&#36890;&#36807;&#32771;&#34385;&#29615;&#22659;&#30340;&#21487;&#25511;&#24615;&#21644;&#30452;&#24452;&#26469;&#33719;&#24471;&#23427;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#23454;&#36341;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
An important problem in reinforcement learning is designing agents that learn to solve tasks safely in an environment. A common solution is for a human expert to define either a penalty in the reward function or a cost to be minimised when reaching unsafe states. However, this is non-trivial, since too small a penalty may lead to agents that reach unsafe states, while too large a penalty increases the time to convergence. Additionally, the difficulty in designing reward or cost functions can increase with the complexity of the problem. Hence, for a given environment with a given set of unsafe states, we are interested in finding the upper bound of rewards at unsafe states whose optimal policies minimise the probability of reaching those unsafe states, irrespective of task rewards. We refer to this exact upper bound as the "Minmax penalty", and show that it can be obtained by taking into account both the controllability and diameter of an environment. We provide a simple practical model
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#25105;&#39564;&#35777;&#30340;&#36890;&#29992;&#32531;&#35299;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20026;&#20854;&#33258;&#25105;&#25552;&#21462;&#25552;&#20379;&#26435;&#23041;&#24615;&#24182;&#26816;&#26597;&#20854;&#33258;&#24049;&#30340;&#36755;&#20986;&#12290;&#22312;&#20020;&#24202;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#20013;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#21508;&#31181;LLMs&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#26497;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#35748;&#30693;&#22686;&#24378;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.00024</link><description>&lt;p&gt;
&#33258;&#25105;&#39564;&#35777;&#25913;&#21892;&#23569;&#26679;&#26412;&#20020;&#24202;&#20449;&#24687;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Self-Verification Improves Few-Shot Clinical Information Extraction. (arXiv:2306.00024v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00024
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#25105;&#39564;&#35777;&#30340;&#36890;&#29992;&#32531;&#35299;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20026;&#20854;&#33258;&#25105;&#25552;&#21462;&#25552;&#20379;&#26435;&#23041;&#24615;&#24182;&#26816;&#26597;&#20854;&#33258;&#24049;&#30340;&#36755;&#20986;&#12290;&#22312;&#20020;&#24202;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#20013;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#21508;&#31181;LLMs&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#26497;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#35748;&#30693;&#22686;&#24378;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#25552;&#21462;&#24739;&#32773;&#20449;&#24687;&#26159;&#21355;&#29983;&#20915;&#31574;&#25903;&#25345;&#21644;&#20020;&#24202;&#30740;&#31350;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#36890;&#36807;&#23569;&#37327;&#19978;&#19979;&#25991;&#23398;&#20064;&#21152;&#36895;&#20020;&#24202;&#31649;&#29702;&#30340;&#28508;&#21147;&#65292;&#30456;&#36739;&#20110;&#38656;&#35201;&#26356;&#26114;&#36149;&#30340;&#20154;&#24037;&#27880;&#37322;&#30340;&#30417;&#30563;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#29616;&#20195;LLM&#65288;&#22914;GPT-4&#65289;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#27493;&#65292;&#23427;&#20204;&#20173;&#28982;&#23384;&#22312;&#31934;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#31561;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#20851;&#38190;&#20219;&#21153;&#39046;&#22495;&#65288;&#22914;&#20581;&#24247;&#65289;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#33258;&#25105;&#39564;&#35777;&#65292;&#25506;&#32034;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#32531;&#35299;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;LLM&#20026;&#20854;&#33258;&#25105;&#25552;&#21462;&#25552;&#20379;&#26435;&#23041;&#24615;&#24182;&#26816;&#26597;&#33258;&#24049;&#30340;&#36755;&#20986;&#12290;&#36825;&#24471;&#30410;&#20110;&#39564;&#35777;&#21644;&#29983;&#25104;&#20043;&#38388;&#30340;&#19981;&#23545;&#31216;&#24615;&#65292;&#21518;&#32773;&#36890;&#24120;&#27604;&#21069;&#32773;&#23481;&#26131;&#24471;&#22810;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26631;&#20934;&#20020;&#24202;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#20013;&#65292;&#23545;&#20110;&#21508;&#31181;LLM&#37117;&#33021;&#25345;&#32493;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#33258;&#25105;&#39564;&#35777;&#20135;&#29983;&#20102;&#26497;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#35748;&#30693;&#22686;&#24378;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extracting patient information from unstructured text is a critical task in health decision-support and clinical research. Large language models (LLMs) have shown the potential to accelerate clinical curation via few-shot in-context learning, in contrast to supervised learning which requires much more costly human annotations. However, despite drastic advances in modern LLMs such as GPT-4, they still struggle with issues regarding accuracy and interpretability, especially in mission-critical domains such as health. Here, we explore a general mitigation framework using self-verification, which leverages the LLM to provide provenance for its own extraction and check its own outputs. This is made possible by the asymmetry between verification and generation, where the latter is often much easier than the former. Experimental results show that our method consistently improves accuracy for various LLMs in standard clinical information extraction tasks. Additionally, self-verification yields
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23545;&#32654;&#22269;CDC&#30340;&#24515;&#33039;&#30149;&#35843;&#26597;&#36827;&#34892;&#20102;&#20934;&#30830;&#24615;&#35843;&#26597;&#65292;&#30830;&#23450;&#20102;&#26368;&#30456;&#20851;&#30340;&#38382;&#39064;&#23376;&#38598;&#65292;&#24182;&#35777;&#26126;&#20351;&#29992;&#26377;&#38480;&#30340;&#38382;&#39064;&#38598;&#20173;&#28982;&#21487;&#20197;&#20445;&#25345;&#39640;&#30340;&#39044;&#27979;&#20934;&#30830;&#29575;&#65292;&#24182;&#26174;&#33879;&#20943;&#23569;&#35843;&#26597;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2306.00023</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#39044;&#27979;&#24515;&#33039;&#30149;&#24182;&#20943;&#23569;&#35843;&#26597;&#26102;&#38388;
&lt;/p&gt;
&lt;p&gt;
Predicting Heart Disease and Reducing Survey Time Using Machine Learning Algorithms. (arXiv:2306.00023v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00023
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23545;&#32654;&#22269;CDC&#30340;&#24515;&#33039;&#30149;&#35843;&#26597;&#36827;&#34892;&#20102;&#20934;&#30830;&#24615;&#35843;&#26597;&#65292;&#30830;&#23450;&#20102;&#26368;&#30456;&#20851;&#30340;&#38382;&#39064;&#23376;&#38598;&#65292;&#24182;&#35777;&#26126;&#20351;&#29992;&#26377;&#38480;&#30340;&#38382;&#39064;&#38598;&#20173;&#28982;&#21487;&#20197;&#20445;&#25345;&#39640;&#30340;&#39044;&#27979;&#20934;&#30830;&#29575;&#65292;&#24182;&#26174;&#33879;&#20943;&#23569;&#35843;&#26597;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#21644;&#20998;&#26512;&#24072;&#27491;&#33268;&#21147;&#20110;&#25913;&#21892;&#21508;&#31181;&#30142;&#30149;&#30340;&#21307;&#23398;&#35786;&#26029;&#12290;&#24515;&#33039;&#30149;&#26159;&#19968;&#31181;&#24120;&#35265;&#30142;&#30149;&#65292;&#21487;&#35270;&#20026;&#20840;&#29699;&#37325;&#35201;&#27515;&#22240;&#12290;&#26089;&#26399;&#21457;&#29616;&#24515;&#33039;&#30149;&#26377;&#21161;&#20110;&#26174;&#33879;&#38477;&#20302;&#24515;&#21147;&#34928;&#31469;&#30340;&#39118;&#38505;&#12290;&#22240;&#27492;&#65292;&#30142;&#30149;&#25511;&#21046;&#21644;&#39044;&#38450;&#20013;&#24515;&#27599;&#24180;&#20174;400,000&#22810;&#21517;&#21442;&#19982;&#32773;&#20013;&#36827;&#34892;&#19982;&#20581;&#24247;&#30456;&#20851;&#30340;&#30005;&#35805;&#35843;&#26597;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#23545;&#20110;&#39044;&#27979;&#24515;&#33039;&#30149;&#30340;&#25968;&#25454;&#21487;&#38752;&#24615;&#20197;&#21450;&#26159;&#21542;&#25152;&#26377;&#35843;&#26597;&#38382;&#39064;&#37117;&#20855;&#26377;&#24378;&#30456;&#20851;&#24615;&#25552;&#20986;&#20102;&#19968;&#20123;&#25285;&#24551;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#25903;&#25345;&#21521;&#37327;&#26426;&#21644;&#36923;&#36753;&#22238;&#24402;&#31561;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#20197;&#35843;&#26597;&#32654;&#22269;&#30142;&#30149;&#25511;&#21046;&#21644;&#39044;&#38450;&#20013;&#24515;&#30340;&#24515;&#33039;&#30149;&#35843;&#26597;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#21508;&#31181;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#26469;&#30830;&#23450;&#21487;&#20197;&#29992;&#20110;&#39044;&#27979;&#24515;&#33039;&#30149;&#30340;&#26368;&#30456;&#20851;&#30340;&#38382;&#39064;&#23376;&#38598;&#12290;&#20026;&#20102;&#24471;&#20986;&#26377;&#21147;&#30340;&#32467;&#35770;&#65292;&#25105;&#20204;&#22312;&#20840;&#22269;&#20581;&#24247;&#21644;&#33829;&#20859;&#35843;&#26597;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#24320;&#21457;&#30340;&#27169;&#22411;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#26377;&#38480;&#30340;&#38382;&#39064;&#38598;&#21487;&#20197;&#26174;&#33879;&#32553;&#30701;&#35843;&#26597;&#26102;&#38388;&#65292;&#21516;&#26102;&#20173;&#28982;&#20445;&#25345;&#39640;&#30340;&#24515;&#33039;&#30149;&#39044;&#27979;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Currently, many researchers and analysts are working toward medical diagnosis enhancement for various diseases. Heart disease is one of the common diseases that can be considered a significant cause of mortality worldwide. Early detection of heart disease significantly helps in reducing the risk of heart failure. Consequently, the Centers for Disease Control and Prevention (CDC) conducts a health-related telephone survey yearly from over 400,000 participants. However, several concerns arise regarding the reliability of the data in predicting heart disease and whether all of the survey questions are strongly related. This study aims to utilize several machine learning techniques, such as support vector machines and logistic regression, to investigate the accuracy of the CDC's heart disease survey in the United States. Furthermore, we use various feature selection methods to identify the most relevant subset of questions that can be utilized to forecast heart conditions. To reach a robus
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#35843;&#26597;&#20102;GPT-4&#30340;&#22320;&#29702;&#30693;&#35782;&#27700;&#24179;&#65292;&#24182;&#25506;&#35752;&#20854;&#22312;&#22320;&#29702;&#25968;&#25454;&#20998;&#26512;&#31561;&#39046;&#22495;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.00020</link><description>&lt;p&gt;
GPT4GEO&#65306;&#19968;&#20010;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#30475;&#24453;&#19990;&#30028;&#22320;&#29702;
&lt;/p&gt;
&lt;p&gt;
GPT4GEO: How a Language Model Sees the World's Geography. (arXiv:2306.00020v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00020
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#35843;&#26597;&#20102;GPT-4&#30340;&#22320;&#29702;&#30693;&#35782;&#27700;&#24179;&#65292;&#24182;&#25506;&#35752;&#20854;&#22312;&#22320;&#29702;&#25968;&#25454;&#20998;&#26512;&#31561;&#39046;&#22495;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#28041;&#21450;&#38382;&#39064;&#22238;&#31572;&#12289;&#29983;&#25104;&#36830;&#36143;&#25991;&#26412;&#21644;&#20195;&#30721;&#30340;&#24191;&#27867;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#20840;&#38754;&#29702;&#35299;LLM&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#23545;&#20110;&#23433;&#20840;&#12289;&#19979;&#28216;&#24212;&#29992;&#21644;&#24615;&#33021;&#25913;&#36827;&#37117;&#26377;&#30410;&#22788;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;GPT-4&#33719;&#24471;&#20107;&#23454;&#22320;&#29702;&#30693;&#35782;&#30340;&#31243;&#24230;&#65292;&#24182;&#33021;&#21542;&#23558;&#36825;&#20123;&#30693;&#35782;&#29992;&#20110;&#35299;&#37322;&#24615;&#25512;&#29702;&#65292;&#36825;&#23545;&#20110;&#28041;&#21450;&#22320;&#29702;&#25968;&#25454;&#30340;&#24212;&#29992;&#65288;&#22914;&#22320;&#29702;&#31354;&#38388;&#20998;&#26512;&#12289;&#20379;&#24212;&#38142;&#31649;&#29702;&#21644;&#28798;&#38590;&#21709;&#24212;&#65289;&#23588;&#20854;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#24182;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#22810;&#26679;&#21270;&#30340;&#23454;&#39564;&#65292;&#20174;&#23450;&#20301;&#12289;&#36317;&#31163;&#21644;&#39640;&#24230;&#20272;&#35745;&#31561;&#20107;&#23454;&#20219;&#21153;&#24320;&#22987;&#65292;&#21040;&#29983;&#25104;&#22269;&#23478;&#36718;&#24275;&#21644;&#26053;&#28216;&#32593;&#32476;&#12289;&#22312;&#32422;&#26463;&#26465;&#20214;&#19979;&#23547;&#25214;&#36335;&#32447;&#21644;&#20379;&#24212;&#38142;&#20998;&#26512;&#31561;&#26356;&#22797;&#26434;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;GPT-4&#65288;&#27809;&#26377;&#25554;&#20214;&#25110;Internet&#35775;&#38382;&#65289;&#20102;&#35299;&#21644;&#19981;&#20102;&#35299;&#19990;&#30028;&#22320;&#29702;&#30340;&#24191;&#27867;&#25551;&#36848;&#65292;&#24182;&#35752;&#35770;&#20102;&#22312;&#22320;&#29702;&#31354;&#38388;&#39046;&#22495;&#24212;&#29992;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown remarkable capabilities across a broad range of tasks involving question answering and the generation of coherent text and code. Comprehensively understanding the strengths and weaknesses of LLMs is beneficial for safety, downstream applications and improving performance. In this work, we investigate the degree to which GPT-4 has acquired factual geographic knowledge and is capable of using this knowledge for interpretative reasoning, which is especially important for applications that involve geographic data, such as geospatial analysis, supply chain management, and disaster response. To this end, we design and conduct a series of diverse experiments, starting from factual tasks such as location, distance and elevation estimation to more complex questions such as generating country outlines and travel networks, route finding under constraints and supply chain analysis. We provide a broad characterisation of what GPT-4 (without plugins or Interne
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#23558;&#39046;&#22495;&#30693;&#35782;&#21644;&#20808;&#39564;&#20449;&#24565;&#32435;&#20837;&#31163;&#25955;&#36873;&#25321;&#27169;&#22411;&#65288;DCM&#65289;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#20013;&#65292;&#20197;&#25193;&#23637;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#22312;DCM&#20013;&#30340;&#28508;&#21147;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26082;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#65292;&#21448;&#20445;&#30041;&#20102;DCM&#30340;&#20248;&#28857;&#21644;DNNs&#30340;&#28789;&#27963;&#24615;&#65292;&#24182;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.00016</link><description>&lt;p&gt;
&#23558;&#39046;&#22495;&#30693;&#35782;&#32435;&#20837;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#31163;&#25955;&#36873;&#25321;&#27169;&#22411;&#20013;
&lt;/p&gt;
&lt;p&gt;
Incorporating Domain Knowledge in Deep Neural Networks for Discrete Choice Models. (arXiv:2306.00016v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00016
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#23558;&#39046;&#22495;&#30693;&#35782;&#21644;&#20808;&#39564;&#20449;&#24565;&#32435;&#20837;&#31163;&#25955;&#36873;&#25321;&#27169;&#22411;&#65288;DCM&#65289;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#20013;&#65292;&#20197;&#25193;&#23637;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#22312;DCM&#20013;&#30340;&#28508;&#21147;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26082;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#65292;&#21448;&#20445;&#30041;&#20102;DCM&#30340;&#20248;&#28857;&#21644;DNNs&#30340;&#28789;&#27963;&#24615;&#65292;&#24182;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#25955;&#36873;&#25321;&#27169;&#22411;&#65288;DCM&#65289;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#26053;&#34892;&#38656;&#27714;&#20998;&#26512;&#20013;&#65292;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#29702;&#35770;&#35745;&#37327;&#26694;&#26550;&#26469;&#29702;&#35299;&#21644;&#39044;&#27979;&#36873;&#25321;&#34892;&#20026;&#12290;DCM&#26159;&#38543;&#26426;&#25928;&#29992;&#27169;&#22411;&#65288;RUM&#65289;&#65292;&#20854;&#20027;&#35201;&#20248;&#28857;&#26159;&#21487;&#35299;&#37322;&#24615;&#12290;&#28982;&#32780;&#65292;&#20272;&#35745;&#36825;&#20123;&#27169;&#22411;&#30340;&#26680;&#24515;&#35201;&#27714;&#26159;&#20808;&#39564;&#25351;&#23450;&#30456;&#20851;&#30340;&#25928;&#29992;&#20989;&#25968;&#65292;&#20351;&#20854;&#23545;&#27169;&#22411;&#20154;&#21592;&#30340;&#20027;&#35266;&#20449;&#24565;&#25935;&#24863;&#12290;&#26368;&#36817;&#65292;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#24050;&#32463;&#25104;&#20026;&#23398;&#20064;DCM&#20013;&#26410;&#35266;&#27979;&#21040;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;ML&#27169;&#22411;&#34987;&#35748;&#20026;&#26159;&#8220;&#40657;&#30418;&#23376;&#8221;&#65292;&#21487;&#33021;&#19982;&#39044;&#26399;&#30340;&#20851;&#31995;&#19981;&#31526;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#32422;&#26463;&#25903;&#25345;&#24320;&#21457;&#24182;&#23558;&#39046;&#22495;&#30693;&#35782;&#21644;&#20808;&#39564;&#20449;&#24565;&#32435;&#20837;&#21487;&#35299;&#37322;&#27169;&#22411;&#20013;&#65292;&#20197;&#25193;&#23637;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#22312;DCM&#20013;&#30340;&#28508;&#21147;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#21253;&#25324;&#20195;&#34920;&#25152;&#38656;&#20851;&#31995;&#30340;&#20266;&#25968;&#25454;&#26679;&#26412;&#21644;&#23558;DCM&#36716;&#25442;&#20026;&#20855;&#26377;&#36719;&#32422;&#26463;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#23545;&#25968;&#20284;&#28982;&#20989;&#25968;&#12290;&#30001;&#27492;&#24471;&#21040;&#30340;&#27169;&#22411;&#26082;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#65292;&#21448;&#20445;&#30041;&#20102;RUM&#30340;&#20248;&#28857;&#21644;DNNs&#30340;&#28789;&#27963;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discrete choice models (DCM) are widely employed in travel demand analysis as a powerful theoretical econometric framework for understanding and predicting choice behaviors. DCMs are formed as random utility models (RUM), with their key advantage of interpretability. However, a core requirement for the estimation of these models is a priori specification of the associated utility functions, making them sensitive to modelers' subjective beliefs. Recently, machine learning (ML) approaches have emerged as a promising avenue for learning unobserved non-linear relationships in DCMs. However, ML models are considered "black box" and may not correspond with expected relationships. This paper proposes a framework that expands the potential of data-driven approaches for DCM by supporting the development of interpretable models that incorporate domain knowledge and prior beliefs through constraints. The proposed framework includes pseudo data samples that represent required relationships and a l
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#26399;&#26041;&#27861;GraphCleaner&#65292;&#29992;&#20110;&#22312;&#27969;&#34892;&#30340;&#22270;&#23398;&#20064;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#26816;&#27979;&#21644;&#32416;&#27491;&#38169;&#35823;&#26631;&#27880;&#33410;&#28857;&#12290;GraphCleaner &#32452;&#21512;&#20102;&#21512;&#25104;&#38169;&#35823;&#26631;&#27880;&#25968;&#25454;&#38598;&#30340;&#29983;&#25104;&#21644;&#37051;&#22495;&#24863;&#30693;&#38169;&#35823;&#26631;&#27880;&#26816;&#27979;&#20004;&#31181;&#26032;&#39062;&#24605;&#24819;&#65292;&#23454;&#35777;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#20854;&#24179;&#22343;F1&#24471;&#20998;&#25552;&#39640;&#20102;0.14&#12290;</title><link>http://arxiv.org/abs/2306.00015</link><description>&lt;p&gt;
GraphCleaner: &#22312;&#27969;&#34892;&#30340;&#22270;&#23398;&#20064;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#26816;&#27979;&#38169;&#35823;&#26631;&#27880;&#30340;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
GraphCleaner: Detecting Mislabelled Samples in Popular Graph Learning Benchmarks. (arXiv:2306.00015v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00015
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#26399;&#26041;&#27861;GraphCleaner&#65292;&#29992;&#20110;&#22312;&#27969;&#34892;&#30340;&#22270;&#23398;&#20064;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#26816;&#27979;&#21644;&#32416;&#27491;&#38169;&#35823;&#26631;&#27880;&#33410;&#28857;&#12290;GraphCleaner &#32452;&#21512;&#20102;&#21512;&#25104;&#38169;&#35823;&#26631;&#27880;&#25968;&#25454;&#38598;&#30340;&#29983;&#25104;&#21644;&#37051;&#22495;&#24863;&#30693;&#38169;&#35823;&#26631;&#27880;&#26816;&#27979;&#20004;&#31181;&#26032;&#39062;&#24605;&#24819;&#65292;&#23454;&#35777;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#20854;&#24179;&#22343;F1&#24471;&#20998;&#25552;&#39640;&#20102;0.14&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#27969;&#34892;&#30340;&#25991;&#26412;&#65292;&#22270;&#20687;&#21644;&#38899;&#39057;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#26631;&#31614;&#38169;&#35823;&#65292;&#36825;&#20005;&#37325;&#24433;&#21709;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#24320;&#21457;&#21644;&#35780;&#20272;&#30340;&#21487;&#38752;&#24615;&#12290;&#23613;&#31649;&#38024;&#23545;&#36890;&#29992;&#25968;&#25454;&#31867;&#22411;&#65288;&#22914;&#22270;&#20687;&#21644;&#25991;&#26412;&#65289;&#30340;&#36136;&#37327;&#25913;&#36827;&#24037;&#20316;&#26377;&#25152;&#22686;&#21152;&#65292;&#20294;&#22312;&#22270;&#25968;&#25454;&#20013;&#26816;&#27979;&#38169;&#35823;&#26631;&#27880;&#30340;&#38382;&#39064;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#35752;&#12290;&#35813;&#35770;&#25991;&#26088;&#22312;&#25506;&#32034;&#27969;&#34892;&#30340;&#23454;&#38469;&#22270;&#25968;&#25454;&#38598;&#20013;&#30340;&#38169;&#35823;&#26631;&#27880;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;GraphCleaner&#65292;&#19968;&#31181;&#21518;&#26399;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#21644;&#32416;&#27491;&#36825;&#20123;&#22270;&#25968;&#25454;&#38598;&#20013;&#30340;&#38169;&#35823;&#26631;&#27880;&#33410;&#28857;&#12290;GraphCleaner&#32467;&#21512;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#24605;&#24819;&#65306;1&#65289;&#21512;&#25104;&#38169;&#35823;&#26631;&#27880;&#25968;&#25454;&#38598;&#30340;&#29983;&#25104;&#65292;&#26088;&#22312;&#29983;&#25104;&#36924;&#30495;&#30340;&#38169;&#35823;&#26631;&#27880;&#65307;2&#65289;&#37051;&#22495;&#24863;&#30693;&#38169;&#35823;&#26631;&#27880;&#26816;&#27979;&#65292;&#20854;&#20013;&#21033;&#29992;&#20102;&#26631;&#31614;&#21644;&#22522;&#20998;&#31867;&#22120;&#39044;&#27979;&#20013;&#30340;&#37051;&#22495;&#20381;&#36182;&#24615;&#12290;&#22312;6&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;6&#20010;&#23454;&#39564;&#35774;&#32622;&#19978;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;GraphCleaner&#20248;&#20110;&#26368;&#25509;&#36817;&#30340;&#22522;&#20934;&#32447;&#65292;&#24182;&#20855;&#26377;&#24179;&#22343;F1&#24471;&#20998;&#25913;&#36827;0.14&#12290;
&lt;/p&gt;
&lt;p&gt;
Label errors have been found to be prevalent in popular text, vision, and audio datasets, which heavily influence the safe development and evaluation of machine learning algorithms. Despite increasing efforts towards improving the quality of generic data types, such as images and texts, the problem of mislabel detection in graph data remains underexplored. To bridge the gap, we explore mislabelling issues in popular real-world graph datasets and propose GraphCleaner, a post-hoc method to detect and correct these mislabelled nodes in graph datasets. GraphCleaner combines the novel ideas of 1) Synthetic Mislabel Dataset Generation, which seeks to generate realistic mislabels; and 2) Neighborhood-Aware Mislabel Detection, where neighborhood dependency is exploited in both labels and base classifier predictions. Empirical evaluations on 6 datasets and 6 experimental settings demonstrate that GraphCleaner outperforms the closest baseline, with an average improvement of 0.14 in F1 score, and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#37327;&#21270;&#21069;&#24494;&#35843;&#8221;&#26694;&#26550; PreQuant&#65292;&#21487;&#29992;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#37327;&#21270;&#65292;&#26377;&#25928;&#38477;&#20302;&#20102;&#27169;&#22411;&#30340;&#22797;&#26434;&#24230;&#21644;&#20351;&#29992;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2306.00014</link><description>&lt;p&gt;
&#38754;&#21521;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#37327;&#21270;&#26041;&#27861;PreQuant
&lt;/p&gt;
&lt;p&gt;
PreQuant: A Task-agnostic Quantization Approach for Pre-trained Language Models. (arXiv:2306.00014v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#37327;&#21270;&#21069;&#24494;&#35843;&#8221;&#26694;&#26550; PreQuant&#65292;&#21487;&#29992;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#37327;&#21270;&#65292;&#26377;&#25928;&#38477;&#20302;&#20102;&#27169;&#22411;&#30340;&#22797;&#26434;&#24230;&#21644;&#20351;&#29992;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;transformer&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#24050;&#32463;&#22312;&#35768;&#22810;NLP&#24212;&#29992;&#20013;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#37096;&#32626;&#22797;&#26434;&#12289;&#20351;&#29992;&#26114;&#36149;&#65292;&#22240;&#27492;&#26377;&#25928;&#21387;&#32553;&#22823;&#35268;&#27169;PLMs&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#37327;&#21270;&#26159;&#19968;&#31181;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#29992;&#20302;&#27604;&#29305;&#23450;&#28857;&#26684;&#24335;&#34920;&#31034;&#39640;&#31934;&#24230;&#24352;&#37327;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#37327;&#21270;&#21069;&#24494;&#35843;&#8221;&#26694;&#26550;PreQuant&#65292;&#23427;&#19982;&#21508;&#31181;&#37327;&#21270;&#31574;&#30053;&#20860;&#23481;&#65292;&#24182;&#32467;&#21512;&#8220;&#24322;&#24120;&#20540;&#24863;&#30693;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#8221;&#36827;&#34892;&#26657;&#27491;&#12290;
&lt;/p&gt;
&lt;p&gt;
While transformer-based pre-trained language models (PLMs) have dominated a number of NLP applications, these models are heavy to deploy and expensive to use. Therefore, effectively compressing large-scale PLMs becomes an increasingly important problem. Quantization, which represents high-precision tensors with low-bit fix-point format, is a viable solution. However, most existing quantization methods are task-specific, requiring customized training and quantization with a large number of trainable parameters on each individual task. Inspired by the observation that the over-parameterization nature of PLMs makes it possible to freeze most of the parameters during the fine-tuning stage, in this work, we propose a novel ``quantize before fine-tuning'' framework, PreQuant, that differs from both quantization-aware training and post-training quantization. PreQuant is compatible with various quantization strategies, with outlier-aware parameter-efficient fine-tuning incorporated to correct 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#25991;&#26412;&#20998;&#31867;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#33258;&#21160;&#20174;&#22823;&#37327;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#30284;&#30151;&#30456;&#20851;&#23454;&#20307;&#21644;&#23454;&#20307;&#38388;&#20851;&#31995;&#65292;&#20197;&#24110;&#21161;&#25512;&#36827;&#30284;&#30151;&#30740;&#31350;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2306.00013</link><description>&lt;p&gt;
&#30284;&#30151;&#23454;&#20307;&#30340;&#20851;&#32852;&#21644;&#20998;&#31867;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Approach for Cancer Entities Association and Classification. (arXiv:2306.00013v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00013
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#25991;&#26412;&#20998;&#31867;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#33258;&#21160;&#20174;&#22823;&#37327;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#30284;&#30151;&#30456;&#20851;&#23454;&#20307;&#21644;&#23454;&#20307;&#38388;&#20851;&#31995;&#65292;&#20197;&#24110;&#21161;&#25512;&#36827;&#30284;&#30151;&#30740;&#31350;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26681;&#25454;&#19990;&#30028;&#21355;&#29983;&#32452;&#32455;&#65288;WHO&#65289;&#30340;&#25968;&#25454;&#65292;&#30284;&#30151;&#26159;&#20840;&#29699;&#31532;&#20108;&#22823;&#27515;&#22240;&#12290;&#19981;&#21516;&#31867;&#22411;&#30284;&#30151;&#30340;&#31185;&#23398;&#30740;&#31350;&#20197;&#27599;&#24180;&#21457;&#24067;&#22823;&#37327;&#30340;&#30740;&#31350;&#25991;&#31456;&#30340;&#36895;&#24230;&#19981;&#26029;&#22686;&#38271;&#12290;&#19982;&#22522;&#22240;&#30456;&#20851;&#30340;&#33647;&#29289;&#12289;&#35786;&#26029;&#12289;&#39118;&#38505;&#12289;&#30151;&#29366;&#12289;&#27835;&#30103;&#31561;&#30340;&#20449;&#24687;&#21644;&#30693;&#35782;&#26159;&#24110;&#21161;&#25506;&#32034;&#21644;&#25512;&#36827;&#30284;&#30151;&#30740;&#31350;&#36827;&#23637;&#30340;&#37325;&#35201;&#22240;&#32032;&#12290;&#25163;&#21160;&#31579;&#36873;&#36825;&#20040;&#22823;&#37327;&#30340;&#25991;&#31456;&#38750;&#24120;&#36153;&#26102;&#36153;&#21147;&#65292;&#24456;&#38590;&#21046;&#23450;&#20219;&#20309;&#20551;&#35774;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#20004;&#31181;&#26368;&#20026;&#37325;&#35201;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21151;&#33021;&#65292;&#23454;&#20307;&#35782;&#21035;&#21644;&#25991;&#26412;&#20998;&#31867;&#65292;&#20174;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20013;&#21457;&#29616;&#30693;&#35782;&#12290;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#20511;&#21161;&#29992;&#25143;&#21451;&#22909;&#30340;&#30028;&#38754;&#21644;&#20869;&#32622;&#23383;&#20856;&#35782;&#21035;&#24182;&#25552;&#21462;&#19982;&#30284;&#30151;&#30456;&#20851;&#30340;&#39044;&#23450;&#20041;&#23454;&#20307;&#12290;&#25991;&#26412;&#20998;&#31867;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#24110;&#21161;&#25506;&#31350;&#30284;&#30151;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
According to the World Health Organization (WHO), cancer is the second leading cause of death globally. Scientific research on different types of cancers grows at an ever-increasing rate, publishing large volumes of research articles every year. The insight information and the knowledge of the drug, diagnostics, risk, symptoms, treatments, etc., related to genes are significant factors that help explore and advance the cancer research progression. Manual screening of such a large volume of articles is very laborious and time-consuming to formulate any hypothesis. The study uses the two most non-trivial NLP, Natural Language Processing functions, Entity Recognition, and text classification to discover knowledge from biomedical literature. Named Entity Recognition (NER) recognizes and extracts the predefined entities related to cancer from unstructured text with the support of a user-friendly interface and built-in dictionaries. Text classification helps to explore the insights into the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26102;&#31354;&#25968;&#25454;&#26041;&#27861;&#21450;&#24212;&#29992;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;GNN&#30340;&#20998;&#31867;&#21450;&#20854;&#24212;&#29992;&#39046;&#22495;&#12290;&#36825;&#20010;&#32508;&#36848;&#31361;&#20986;&#20102;&#20351;&#29992;GNN&#20998;&#26512;&#26102;&#31354;&#25968;&#25454;&#30340;&#24378;&#22823;&#24037;&#20855;&#21450;&#20854;&#23616;&#38480;&#24615;&#65292;&#24182;&#25351;&#20986;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#21644;&#30446;&#21069;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.00012</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26102;&#31354;&#25968;&#25454;&#26041;&#27861;&#21450;&#24212;&#29992;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Network for spatiotemporal data: methods and applications. (arXiv:2306.00012v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26102;&#31354;&#25968;&#25454;&#26041;&#27861;&#21450;&#24212;&#29992;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;GNN&#30340;&#20998;&#31867;&#21450;&#20854;&#24212;&#29992;&#39046;&#22495;&#12290;&#36825;&#20010;&#32508;&#36848;&#31361;&#20986;&#20102;&#20351;&#29992;GNN&#20998;&#26512;&#26102;&#31354;&#25968;&#25454;&#30340;&#24378;&#22823;&#24037;&#20855;&#21450;&#20854;&#23616;&#38480;&#24615;&#65292;&#24182;&#25351;&#20986;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#21644;&#30446;&#21069;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#25968;&#25454;&#26102;&#20195;&#65292;&#34164;&#21547;&#30528;&#31354;&#38388;&#21644;&#26102;&#38388;&#20449;&#24687;&#30340;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#24613;&#21095;&#22686;&#21152;&#65292;&#20026;&#22825;&#27668;&#39044;&#25253;&#12289;&#33258;&#28982;&#28798;&#23475;&#31649;&#29702;&#12289;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#21644;&#31934;&#20934;&#20892;&#19994;&#31561;&#24212;&#29992;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#27934;&#35265;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#31181;&#29992;&#20110;&#23545;&#20855;&#26377;&#20381;&#36182;&#20851;&#31995;&#30340;&#25968;&#25454;&#65288;&#22914;&#31354;&#38388;&#21644;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65289;&#36827;&#34892;&#24314;&#27169;&#21644;&#29702;&#35299;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#24050;&#26377;&#22823;&#37327;&#24037;&#20316;&#33268;&#21147;&#20110;&#21033;&#29992;GNN&#35299;&#20915;&#26102;&#31354;&#25968;&#25454;&#20013;&#30340;&#22797;&#26434;&#31354;&#38388;&#21644;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#26102;&#31354;&#25968;&#25454;&#30340;&#24378;&#36328;&#23398;&#31185;&#24615;&#36136;&#23548;&#33268;&#20247;&#22810;&#29305;&#23450;&#20110;&#19981;&#21516;&#24212;&#29992;&#39046;&#22495;&#30340;GNN&#21464;&#20307;&#30340;&#20986;&#29616;&#12290;&#23613;&#31649;&#36825;&#20123;&#25216;&#26415;&#36890;&#24120;&#36866;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#20851;&#20110;&#26102;&#31354;&#25968;&#25454;GNN&#30340;&#20840;&#38754;&#25991;&#29486;&#32508;&#36848;&#65292;&#22240;&#27492;&#36328;&#39046;&#22495;&#21442;&#32771;&#36825;&#20123;&#26041;&#27861;&#20173;&#28982;&#26159;&#24517;&#35201;&#30340;&#65292;&#20294;&#20063;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#29616;&#26377;&#20851;&#20110;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26102;&#31354;&#25968;&#25454;&#30740;&#31350;&#65292;&#24182;&#23545;&#20854;&#20248;&#28857;&#12289;&#23616;&#38480;&#24615;&#21644;&#28508;&#22312;&#24212;&#29992;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;&#25105;&#20204;&#20063;&#25552;&#20379;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#36335;&#32447;&#22270;&#21644;&#24378;&#35843;&#20102;&#19968;&#20123;&#26410;&#35299;&#20915;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the era of big data, there has been a surge in the availability of data containing rich spatial and temporal information, offering valuable insights into dynamic systems and processes for applications such as weather forecasting, natural disaster management, intelligent transport systems, and precision agriculture. Graph neural networks (GNNs) have emerged as a powerful tool for modeling and understanding data with dependencies to each other such as spatial and temporal dependencies. There is a large amount of existing work that focuses on addressing the complex spatial and temporal dependencies in spatiotemporal data using GNNs. However, the strong interdisciplinary nature of spatiotemporal data has created numerous GNNs variants specifically designed for distinct application domains. Although the techniques are generally applicable across various domains, cross-referencing these methods remains essential yet challenging due to the absence of a comprehensive literature review on GN
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#30417;&#30563;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#20195;&#34920;&#24615;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#22797;&#26434;&#22270;&#20687;&#25968;&#25454;&#20013;&#30340;&#32858;&#31867;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2306.00011</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#26041;&#27861;&#29992;&#20110;&#39640;&#32500;&#25968;&#25454;&#32858;&#31867;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Self-Supervised Approach for Cluster Assessment of High-Dimensional Data. (arXiv:2306.00011v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#30417;&#30563;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#20195;&#34920;&#24615;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#22797;&#26434;&#22270;&#20687;&#25968;&#25454;&#20013;&#30340;&#32858;&#31867;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20272;&#35745;&#25968;&#25454;&#38598;&#20013;&#32858;&#31867;&#25968;&#37327;&#21644;&#22522;&#30784;&#32858;&#31867;&#32467;&#26500;&#26159;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#25968;&#25454;&#36890;&#24120;&#26159;&#26080;&#26631;&#31614;&#12289;&#22797;&#26434;&#19988;&#39640;&#32500;&#30340;&#65292;&#36825;&#20351;&#24471;&#20256;&#32479;&#32858;&#31867;&#31639;&#27861;&#38590;&#20197;&#34920;&#29616;&#33391;&#22909;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#22797;&#26434;&#22270;&#20687;&#25968;&#25454;&#20013;&#30340;&#32858;&#31867;&#32467;&#26500;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;&#29992;&#33258;&#30417;&#30563;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#22797;&#26434;&#25968;&#25454;&#30340;&#20195;&#34920;&#24615;&#23884;&#20837;&#65292;&#28982;&#21518;&#23558;&#36825;&#20123;&#20302;&#32500;&#23884;&#20837;&#39304;&#36865;&#21040;VAT/iVAT&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating the number of clusters and underlying cluster structure in a dataset is a crucial task. Real-world data are often unlabeled, complex and high-dimensional, which makes it difficult for traditional clustering algorithms to perform well. In recent years, a matrix reordering based algorithm, called "visual assessment of tendency" (VAT), and its variants have attracted many researchers from various domains to estimate the number of clusters and inherent cluster structure present in the data. However, these algorithms fail when applied to high-dimensional data due to the curse of dimensionality, as they rely heavily on the notions of closeness and farness between data points. To address this issue, we propose a deep-learning based framework for cluster structure assessment in complex, image datasets. First, our framework generates representative embeddings for complex data using a self-supervised deep neural network, and then, these low-dimensional embeddings are fed to VAT/iVAT a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31616;&#21333;&#24418;&#24335;&#26144;&#23556;&#31070;&#32463;&#32593;&#32476;&#65288;SMNN&#65289;&#30340;&#35757;&#32451;&#36807;&#31243;&#21644;&#26367;&#20195;&#20984;&#22810;&#38754;&#20307;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#39318;&#27425;&#24341;&#20837;&#20102; SMNN &#30340;&#21487;&#35299;&#37322;&#24615;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.00010</link><description>&lt;p&gt;
&#31616;&#21333;&#24418;&#24335;&#26144;&#23556;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Explainability in Simplicial Map Neural Networks. (arXiv:2306.00010v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31616;&#21333;&#24418;&#24335;&#26144;&#23556;&#31070;&#32463;&#32593;&#32476;&#65288;SMNN&#65289;&#30340;&#35757;&#32451;&#36807;&#31243;&#21644;&#26367;&#20195;&#20984;&#22810;&#38754;&#20307;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#39318;&#27425;&#24341;&#20837;&#20102; SMNN &#30340;&#21487;&#35299;&#37322;&#24615;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31616;&#21333;&#24418;&#24335;&#26144;&#23556;&#31070;&#32463;&#32593;&#32476;&#65288;SMNN&#65289;&#26159;&#22522;&#20110;&#25299;&#25169;&#23398;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#20855;&#26377;&#26222;&#36866;&#36924;&#36817;&#33021;&#21147;&#21644;&#22312;&#36866;&#24403;&#26465;&#20214;&#19979;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#39640;&#32500;&#20013;&#24212;&#29992; SMNN &#23384;&#22312;&#19968;&#20123;&#29942;&#39048;&#65292;&#39318;&#20808;&#27809;&#26377;&#23450;&#20041; SMNN &#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#20854;&#27425;&#23545;&#20110;&#36755;&#20837;&#25968;&#25454;&#38598;&#38656;&#35201;&#26500;&#24314;&#19968;&#20010;&#21253;&#22260;&#20984;&#22810;&#38754;&#20307;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#32473;&#23450;&#25968;&#25454;&#38598;&#30340;&#25903;&#25345;&#23376;&#38598;&#21644;&#25237;&#24433;&#21040;&#36229;&#29699;&#38754;&#30340;&#26041;&#27861;&#20316;&#20026;&#26367;&#20195;&#20984;&#22810;&#38754;&#20307;&#30340; SMNN &#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#39318;&#27425;&#24341;&#20837;&#20102; SMNN &#30340;&#21487;&#35299;&#37322;&#24615;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simplicial map neural networks (SMNNs) are topology-based neural networks with interesting properties such as universal approximation capability and robustness to adversarial examples under appropriate conditions. However, SMNNs present some bottlenecks for their possible application in high dimensions. First, no SMNN training process has been defined so far. Second, SMNNs require the construction of a convex polytope surrounding the input dataset. In this paper, we propose a SMNN training procedure based on a support subset of the given dataset and a method based on projection to a hypersphere as a replacement for the convex polytope construction. In addition, the explainability capacity of SMNNs is also introduced for the first time in this paper.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#24418;&#25506;&#32034;&#30340;&#26041;&#27861;&#65292;&#20197;&#25429;&#25417;&#29992;&#25143;&#23545;&#19981;&#21516;&#39033;&#30446;&#21644;&#39033;&#30446;&#33258;&#36523;&#23646;&#24615;&#30340;&#22266;&#26377;&#20559;&#22909;&#65292;&#20174;&#32780;&#25552;&#39640;&#24494;&#20449;&#25512;&#33616;&#31995;&#32479;&#20013;&#20010;&#20154;&#23618;&#38754;&#21644;&#31995;&#32479;&#23618;&#38754;&#30340;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.00009</link><description>&lt;p&gt;
&#22270;&#24418;&#25506;&#32034;&#30340;&#37325;&#35201;&#24615;&#65306;&#22312;&#24494;&#20449;&#25512;&#33616;&#31995;&#32479;&#20013;&#25552;&#39640;&#20010;&#20154;&#23618;&#38754;&#21644;&#31995;&#32479;&#23618;&#38754;&#30340;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
Graph Exploration Matters: Improving both individual-level and system-level diversity in WeChat Feed Recommender. (arXiv:2306.00009v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00009
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#24418;&#25506;&#32034;&#30340;&#26041;&#27861;&#65292;&#20197;&#25429;&#25417;&#29992;&#25143;&#23545;&#19981;&#21516;&#39033;&#30446;&#21644;&#39033;&#30446;&#33258;&#36523;&#23646;&#24615;&#30340;&#22266;&#26377;&#20559;&#22909;&#65292;&#20174;&#32780;&#25552;&#39640;&#24494;&#20449;&#25512;&#33616;&#31995;&#32479;&#20013;&#20010;&#20154;&#23618;&#38754;&#21644;&#31995;&#32479;&#23618;&#38754;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#24037;&#19994;&#25512;&#33616;&#31995;&#32479;&#30340;&#20505;&#36873;&#39033;&#29983;&#25104;&#65288;&#26816;&#32034;&#65289;&#12289;&#25490;&#24207;&#21644;&#20877;&#25490;&#24207;&#22823;&#33268;&#20998;&#20026;&#19977;&#20010;&#38454;&#27573;&#12290;&#20010;&#20154;&#23618;&#38754;&#30340;&#22810;&#26679;&#24615;&#21644;&#31995;&#32479;&#23618;&#38754;&#30340;&#22810;&#26679;&#24615;&#23545;&#20110;&#24037;&#19994;&#25512;&#33616;&#31995;&#32479;&#21516;&#26679;&#37325;&#35201;&#12290;&#21069;&#32773;&#20851;&#27880;&#27599;&#20010;&#29992;&#25143;&#30340;&#20307;&#39564;&#65292;&#32780;&#21518;&#32773;&#20851;&#27880;&#29992;&#25143;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#22522;&#20110;&#22270;&#24418;&#30340;&#26816;&#32034;&#31574;&#30053;&#19981;&#21487;&#36991;&#20813;&#22320;&#34987;&#37325;&#24230;&#29992;&#25143;&#21644;&#28909;&#38376;&#39033;&#30446;&#25152;&#21163;&#25345;&#65292;&#23548;&#33268;&#29992;&#25143;&#20043;&#38388;&#30340;&#20505;&#36873;&#39033;&#25910;&#25947;&#21644;&#31995;&#32479;&#23618;&#38754;&#30340;&#22810;&#26679;&#24615;&#32570;&#20047;&#12290;&#21516;&#26102;&#65292;&#22312;&#20877;&#25490;&#24207;&#38454;&#27573;&#65292;&#37319;&#29992;&#26512;&#22240;&#28857;&#36807;&#31243;&#65288;DPP&#65289;&#26469;&#22686;&#21152;&#20010;&#20154;&#23618;&#38754;&#30340;&#22810;&#26679;&#24615;&#12290;DPP&#39640;&#24230;&#20381;&#36182;&#20110;&#39033;&#30446;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#23481;&#26131;&#21463;&#21040;&#26631;&#39064;&#20826;&#21644;&#19981;&#20934;&#30830;&#23646;&#24615;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#20165;&#20851;&#27880;&#20854;&#20013;&#19968;&#31181;&#22810;&#26679;&#24615;&#23618;&#38754;&#65292;&#24182;&#24573;&#30053;&#20102;&#23454;&#38469;&#25512;&#33616;&#31995;&#32479;&#20013;&#19981;&#21516;&#38454;&#27573;&#20043;&#38388;&#30340;&#30456;&#20114;&#24433;&#21709;&#12290;&#25105;&#20204;&#35748;&#20026;&#24212;&#35813;&#23558;&#20010;&#20154;&#23618;&#38754;&#30340;&#22810;&#26679;&#24615;&#21644;&#31995;&#32479;&#23618;&#38754;&#30340;&#22810;&#26679;&#24615;&#35270;&#20026;&#19968;&#20010;&#25972;&#20307;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#22270;&#24418;&#25506;&#32034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;&#21253;&#21547;&#29992;&#25143;&#21644;&#39033;&#30446;&#30340;&#31038;&#20132;&#22270;&#24418;&#26469;&#25552;&#39640;&#20004;&#31181;&#22810;&#26679;&#24615;&#65292;&#20174;&#32780;&#25429;&#25417;&#29992;&#25143;&#23545;&#19981;&#21516;&#39033;&#30446;&#21644;&#39033;&#30446;&#33258;&#36523;&#23646;&#24615;&#30340;&#22266;&#26377;&#20559;&#22909;&#12290;&#25105;&#20204;&#22312;&#24494;&#20449;&#36164;&#35759;&#25512;&#33616;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20010;&#20154;&#23618;&#38754;&#21644;&#31995;&#32479;&#23618;&#38754;&#30340;&#22810;&#26679;&#24615;&#20197;&#21450;&#29992;&#25143;&#21442;&#19982;&#24230;&#26041;&#38754;&#22343;&#20248;&#20110;&#24378;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
There are roughly three stages in real industrial recommendation systems, candidates generation (retrieval), ranking and reranking. Individual-level diversity and system-level diversity are both important for industrial recommender systems. The former focus on each single user's experience, while the latter focus on the difference among users. Graph-based retrieval strategies are inevitably hijacked by heavy users and popular items, leading to the convergence of candidates for users and the lack of system-level diversity. Meanwhile, in the reranking phase, Determinantal Point Process (DPP) is deployed to increase individual-level diverisity. Heavily relying on the semantic information of items, DPP suffers from clickbait and inaccurate attributes. Besides, most studies only focus on one of the two levels of diversity, and ignore the mutual influence among different stages in real recommender systems. We argue that individual-level diversity and system-level diversity should be viewed a
&lt;/p&gt;</description></item><item><title>Brainformers &#26159;&#19968;&#20010;&#26032;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#22810;&#26679;&#23618;&#32423;&#30340;&#32467;&#26500;&#23436;&#21892;&#20102; Transformer &#30340;&#35774;&#35745;&#32570;&#38519;&#65292;&#20855;&#26377;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#25910;&#25947;&#21644;&#26356;&#24555;&#30340;&#27493;&#39588;&#26102;&#38388;&#65292;&#34920;&#29616;&#20986;&#26356;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.00008</link><description>&lt;p&gt;
Brainformers&#65306;&#20197;&#25928;&#29575;&#25442;&#21462;&#31616;&#27905;&#24615;
&lt;/p&gt;
&lt;p&gt;
Brainformers: Trading Simplicity for Efficiency. (arXiv:2306.00008v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00008
&lt;/p&gt;
&lt;p&gt;
Brainformers &#26159;&#19968;&#20010;&#26032;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#22810;&#26679;&#23618;&#32423;&#30340;&#32467;&#26500;&#23436;&#21892;&#20102; Transformer &#30340;&#35774;&#35745;&#32570;&#38519;&#65292;&#20855;&#26377;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#25910;&#25947;&#21644;&#26356;&#24555;&#30340;&#27493;&#39588;&#26102;&#38388;&#65292;&#34920;&#29616;&#20986;&#26356;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer &#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#26368;&#36817;&#25104;&#21151;&#30340;&#26680;&#24515;&#25216;&#26415;&#12290;Transformer &#20855;&#26377;&#19968;&#20010;&#20960;&#20046;&#32479;&#19968;&#30340;&#39592;&#26550;&#65292;&#20854;&#20013;&#23618;&#27425;&#22312;&#21069;&#39304;&#21644;&#33258;&#27880;&#24847;&#21147;&#20043;&#38388;&#20132;&#26367;&#20197;&#24314;&#31435;&#28145;&#24230;&#32593;&#32476;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#31181;&#35774;&#35745;&#36873;&#25321;&#65292;&#24182;&#21457;&#29616;&#26356;&#22797;&#26434;&#30340;&#22359;&#21487;&#20197;&#26356;&#39640;&#25928;&#22320;&#23436;&#25104;&#20219;&#21153;&#12290;&#26681;&#25454;&#36825;&#20010;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22797;&#26434;&#30340;&#22359;&#65292;&#31216;&#20026; Brainformer&#65292;&#23427;&#30001;&#21508;&#31181;&#24418;&#24335;&#30340;&#23618;&#24402;&#19968;&#21270;&#21644;&#28608;&#27963;&#20989;&#25968;&#12289;&#31232;&#30095;&#38376;&#25511;&#21069;&#39304;&#23618;&#12289;&#23494;&#38598;&#21069;&#39304;&#23618;&#12289;&#27880;&#24847;&#21147;&#23618;&#31561;&#22810;&#26679;&#23618;&#32423;&#32452;&#25104;&#12290;&#22312;&#36136;&#37327;&#21644;&#25928;&#29575;&#26041;&#38754;&#65292;Brainformer &#24635;&#26159;&#20248;&#20110;&#29616;&#26377;&#30340;&#31264;&#23494;&#21644;&#31232;&#30095; Transformer&#12290;&#19968;&#20010;&#20855;&#26377; 80 &#20159;&#20010;&#27599;&#20010;&#26631;&#35760;&#28608;&#27963;&#21442;&#25968;&#30340; Brainformer &#27169;&#22411;&#65292;&#30456;&#27604;&#20110;&#20854; GLaM &#23545;&#24212;&#29289;&#65292;&#34920;&#29616;&#20986; 2 &#20493;&#26356;&#24555;&#30340;&#35757;&#32451;&#25910;&#25947;&#21644; 5 &#20493;&#26356;&#24555;&#30340;&#27493;&#39588;&#26102;&#38388;&#12290;&#22312;&#19979;&#28216;&#20219;&#21153;&#35780;&#20272;&#20013;&#65292;Brainformer &#20063;&#34920;&#29616;&#24471;&#26356;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers are central to recent successes in natural language processing and computer vision. Transformers have a mostly uniform backbone where layers alternate between feed-forward and self-attention in order to build a deep network. Here we investigate this design choice and find that more complex blocks that have different permutations of layer primitives can be more efficient. Using this insight, we develop a complex block, named Brainformer, that consists of a diverse sets of layers such as sparsely gated feed-forward layers, dense feed-forward layers, attention layers, and various forms of layer normalization and activation functions. Brainformer consistently outperforms the state-of-the-art dense and sparse Transformers, in terms of both quality and efficiency. A Brainformer model with 8 billion activated parameters per token demonstrates 2x faster training convergence and 5x faster step time compared to its GLaM counterpart. In downstream task evaluation, Brainformer also de
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#22235;&#20010;&#27861;&#24459;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#20004;&#20010;&#26410;&#26631;&#35760;&#65292;&#21478;&#22806;&#20004;&#20010;&#20351;&#29992;&#21551;&#21457;&#24335;&#26631;&#31614;&#36827;&#34892;&#20102;&#26631;&#35760;&#65292;&#26088;&#22312;&#29992;&#20110;&#25991;&#26412;&#35821;&#20041;&#30456;&#20284;&#24615;&#20219;&#21153;&#12290;&#30740;&#31350;&#27604;&#36739;&#20102;&#24369;&#30417;&#30563;&#21644;&#20256;&#32479;&#27880;&#37322;&#36807;&#31243;&#26041;&#27861;&#65292;&#34920;&#26126;&#24369;&#30417;&#30563;&#21487;&#20197;&#36798;&#21040;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.00007</link><description>&lt;p&gt;
&#33889;&#35821;&#27861;&#24459;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#25968;&#25454;&#38598;&#65306;&#27604;&#36739;&#24369;&#30417;&#30563;&#21644;&#27880;&#37322;&#36807;&#31243;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Datasets for Portuguese Legal Semantic Textual Similarity: Comparing weak supervision and an annotation process approaches. (arXiv:2306.00007v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00007
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#22235;&#20010;&#27861;&#24459;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#20004;&#20010;&#26410;&#26631;&#35760;&#65292;&#21478;&#22806;&#20004;&#20010;&#20351;&#29992;&#21551;&#21457;&#24335;&#26631;&#31614;&#36827;&#34892;&#20102;&#26631;&#35760;&#65292;&#26088;&#22312;&#29992;&#20110;&#25991;&#26412;&#35821;&#20041;&#30456;&#20284;&#24615;&#20219;&#21153;&#12290;&#30740;&#31350;&#27604;&#36739;&#20102;&#24369;&#30417;&#30563;&#21644;&#20256;&#32479;&#27880;&#37322;&#36807;&#31243;&#26041;&#27861;&#65292;&#34920;&#26126;&#24369;&#30417;&#30563;&#21487;&#20197;&#36798;&#21040;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24052;&#35199;&#21496;&#27861;&#26426;&#20851;&#24037;&#20316;&#37327;&#22823;&#65292;&#23548;&#33268;&#21496;&#27861;&#31243;&#24207;&#26102;&#38388;&#38271;&#12290;&#24052;&#35199;&#22269;&#23478;&#21496;&#27861;&#22996;&#21592;&#20250;&#22312;469/2022&#21495;&#20915;&#35758;&#20013;&#21046;&#23450;&#20102;&#25968;&#23383;&#21270;&#25991;&#20214;&#21644;&#27969;&#31243;&#30340;&#27491;&#24335;&#25351;&#23548;&#26041;&#38024;&#65292;&#24320;&#25299;&#20102;&#22312;&#27861;&#24459;&#39046;&#22495;&#20013;&#20351;&#29992;&#33258;&#21160;&#25216;&#26415;&#20197;&#24110;&#21161;&#26085;&#24120;&#20219;&#21153;&#30340;&#21487;&#33021;&#24615;&#65292;&#23588;&#20854;&#26159;&#22312;&#22823;&#37327;&#25991;&#26412;&#30340;&#24773;&#20917;&#19979;&#12290;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#20801;&#35768;&#20174;&#25991;&#26412;&#25968;&#25454;&#20013;&#22788;&#29702;&#21644;&#25552;&#21462;&#26377;&#29992;&#20449;&#24687;&#65292;&#28508;&#22312;&#22320;&#21152;&#24555;&#35813;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#22810;&#20010;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#25152;&#38656;&#30340;&#27861;&#24459;&#39046;&#22495;&#25968;&#25454;&#38598;&#24456;&#23569;&#19988;&#38590;&#20197;&#33719;&#24471;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#19987;&#23478;&#30340;&#26631;&#31614;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#20026;&#27861;&#24459;&#39046;&#22495;&#36129;&#29486;&#20102;&#22235;&#20010;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#20004;&#20010;&#20855;&#26377;&#25991;&#26723;&#21644;&#20803;&#25968;&#25454;&#20294;&#26410;&#26631;&#35760;&#65292;&#21478;&#22806;&#20004;&#20010;&#21017;&#26631;&#35760;&#20102;&#21551;&#21457;&#24335;&#26631;&#31614;&#65292;&#26088;&#22312;&#29992;&#20110;&#25991;&#26412;&#35821;&#20041;&#30456;&#20284;&#24615;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#35780;&#20272;&#20351;&#29992;&#24369;&#30417;&#30563;&#21019;&#24314;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23558;&#20854;&#19982;&#20256;&#32479;&#30340;&#27880;&#37322;&#36807;&#31243;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#24369;&#30417;&#30563;&#21487;&#20197;&#26159;&#19968;&#20010;&#21487;&#34892;&#30340;&#36873;&#25321;&#65292;&#36798;&#21040;&#20102;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Brazilian judiciary has a large workload, resulting in a long time to finish legal proceedings. Brazilian National Council of Justice has established in Resolution 469/2022 formal guidance for document and process digitalization opening up the possibility of using automatic techniques to help with everyday tasks in the legal field, particularly in a large number of texts yielded on the routine of law procedures. Notably, Artificial Intelligence (AI) techniques allow for processing and extracting useful information from textual data, potentially speeding up the process. However, datasets from the legal domain required by several AI techniques are scarce and difficult to obtain as they need labels from experts. To address this challenge, this article contributes with four datasets from the legal domain, two with documents and metadata but unlabeled, and another two labeled with a heuristic aiming at its use in textual semantic similarity tasks. Also, to evaluate the effectiveness of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#22270;&#24418;&#24322;&#24120;&#30417;&#27979;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#30340;&#19968;&#31867;&#21516;&#22411;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#35780;&#20998;&#24230;&#37327;&#8212;&#8212;&#24403;&#21069;&#33410;&#28857;&#20146;&#21644;&#21147;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#37327;&#36523;&#23450;&#21046;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#25130;&#26029;&#20146;&#21644;&#21147;&#26368;&#22823;&#21270;&#65288;TAM&#65289;&#26041;&#27861;&#65292;&#20248;&#21270;&#22312;&#21407;&#22987;&#22270;&#24418;&#32467;&#26500;&#19978;&#36827;&#34892;&#65292;&#33021;&#22815;&#26377;&#25928;&#36827;&#34892;&#21452;&#37325;One-Class&#30340;GAD&#12290;</title><link>http://arxiv.org/abs/2306.00006</link><description>&lt;p&gt;
&#25130;&#26029;&#20146;&#21644;&#21147;&#26368;&#22823;&#21270;&#65306;&#29992;&#20110;&#22270;&#24418;&#24322;&#24120;&#30417;&#27979;&#30340;&#21333;&#31867;&#21516;&#22411;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Truncated Affinity Maximization: One-class Homophily Modeling for Graph Anomaly Detection. (arXiv:2306.00006v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00006
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#22270;&#24418;&#24322;&#24120;&#30417;&#27979;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#30340;&#19968;&#31867;&#21516;&#22411;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#35780;&#20998;&#24230;&#37327;&#8212;&#8212;&#24403;&#21069;&#33410;&#28857;&#20146;&#21644;&#21147;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#37327;&#36523;&#23450;&#21046;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#25130;&#26029;&#20146;&#21644;&#21147;&#26368;&#22823;&#21270;&#65288;TAM&#65289;&#26041;&#27861;&#65292;&#20248;&#21270;&#22312;&#21407;&#22987;&#22270;&#24418;&#32467;&#26500;&#19978;&#36827;&#34892;&#65292;&#33021;&#22815;&#26377;&#25928;&#36827;&#34892;&#21452;&#37325;One-Class&#30340;GAD&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#22270;&#24418;&#24322;&#24120;&#30417;&#27979;&#65288;GAD&#65289;&#25968;&#25454;&#38598;&#20013;&#32463;&#24120;&#21457;&#29616;&#19968;&#31181;&#26222;&#36941;&#30340;&#23646;&#24615;......&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#35780;&#20998;&#24230;&#37327; - &#24403;&#21069;&#33410;&#28857;&#20146;&#21644;&#21147;......&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#25130;&#26029;&#20146;&#21644;&#21147;&#26368;&#22823;&#21270; (TAM)&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#22823;&#21270;&#19982;_neighbors&#30340;&#26412;&#22320;&#20146;&#21644;&#21147;&#26469;&#23398;&#20064;&#37327;&#36523;&#23450;&#21046;&#30340;&#33410;&#28857;&#34920;&#31034;&#12290;&#26412;&#25991;&#25152;&#25552;&#26041;&#27861;&#22312;&#21407;&#22987;&#22270;&#24418;&#32467;&#26500;&#19978;&#36827;&#34892;&#20248;&#21270;&#65292;&#21487;&#20197;&#36827;&#34892;&#21452;&#37325;One-Class&#30340;GAD&#12290;
&lt;/p&gt;
&lt;p&gt;
One prevalent property we find empirically in real-world graph anomaly detection (GAD) datasets is a one-class homophily, i.e., normal nodes tend to have strong connection/affinity with each other, while the homophily in abnormal nodes is significantly weaker than normal nodes. However, this anomaly-discriminative property is ignored by existing GAD methods that are typically built using a conventional anomaly detection objective, such as data reconstruction. In this work, we explore this property to introduce a novel unsupervised anomaly scoring measure for GAD -- local node affinity -- that assigns a larger anomaly score to nodes that are less affiliated with their neighbors, with the affinity defined as similarity on node attributes/representations. We further propose Truncated Affinity Maximization (TAM) that learns tailored node representations for our anomaly measure by maximizing the local affinity of nodes to their neighbors. Optimizing on the original graph structure can be bi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30417;&#30563;&#24335;&#27880;&#24847;&#21147;&#22810;&#23454;&#20363;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#20998;&#26512;&#36229;&#22768;&#22270;&#20687;&#65292;&#23454;&#29616;AS&#30340;&#31934;&#30830;&#31579;&#26597;&#19988;&#31934;&#24230;&#20248;&#20110;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.00003</link><description>&lt;p&gt;
&#36890;&#36807;&#30417;&#30563;&#24335;&#27880;&#24847;&#21147;&#22810;&#23454;&#20363;&#23398;&#20064;&#20174;&#22810;&#35270;&#35282;&#36229;&#22768;&#22270;&#20687;&#26816;&#27979;&#24515;&#33039;&#30149;
&lt;/p&gt;
&lt;p&gt;
Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning. (arXiv:2306.00003v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00003
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30417;&#30563;&#24335;&#27880;&#24847;&#21147;&#22810;&#23454;&#20363;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#20998;&#26512;&#36229;&#22768;&#22270;&#20687;&#65292;&#23454;&#29616;AS&#30340;&#31934;&#30830;&#31579;&#26597;&#19988;&#31934;&#24230;&#20248;&#20110;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#33033;&#29923;&#29421;&#31364;(AS)&#26159;&#19968;&#31181;&#23548;&#33268;&#20005;&#37325;&#21457;&#30149;&#29575;&#21644;&#27515;&#20129;&#29575;&#30340;&#36864;&#34892;&#24615;&#29923;&#33180;&#30142;&#30149;&#12290;&#36825;&#31181;&#24773;&#20917;&#32463;&#24120;&#34987;&#20302;&#20272;&#21644;&#20302;&#27835;&#30103;&#12290;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#65292;AS&#26159;&#36890;&#36807;&#36229;&#22768;&#24515;&#21160;&#22270;&#30340;&#19987;&#23478;&#23457;&#26597;&#26469;&#35786;&#26029;&#30340;&#65292;&#36825;&#20250;&#20135;&#29983;&#25968;&#21313;&#20010;&#19979;&#32954;&#37319;&#26679;&#30340;&#36229;&#22768;&#22270;&#20687;&#12290;&#21482;&#26377;&#19968;&#20123;&#35270;&#22270;&#26174;&#31034;&#20027;&#21160;&#33033;&#29923;&#12290;&#20026;&#20102;&#33258;&#21160;&#21270;&#31579;&#26597;AS&#65292;&#28145;&#24230;&#32593;&#32476;&#24517;&#39035;&#23398;&#20064;&#27169;&#20223;&#20154;&#31867;&#19987;&#23478;&#35782;&#21035;&#20027;&#21160;&#33033;&#29923;&#35270;&#22270;&#30340;&#33021;&#21147;&#65292;&#28982;&#21518;&#27719;&#24635;&#36825;&#20123;&#30456;&#20851;&#22270;&#20687;&#20197;&#20135;&#29983;&#30740;&#31350;&#32423;&#35786;&#26029;&#12290;&#25105;&#20204;&#21457;&#29616;&#20808;&#21069;&#30340;AS&#26816;&#27979;&#26041;&#27861;&#30001;&#20110;&#20381;&#36182;&#20110;&#36328;&#22270;&#20687;&#30340;&#19981;&#28789;&#27963;&#24179;&#22343;&#20540;&#32780;&#23548;&#33268;&#31934;&#24230;&#19981;&#36275;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#21457;&#29616;&#65292;&#29616;&#25104;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22810;&#23454;&#20363;(MIL)&#23398;&#20064;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31471;&#21040;&#31471;MIL&#26041;&#27861;&#65292;&#21253;&#21547;&#20004;&#20010;&#20851;&#38190;&#26041;&#27861;&#21019;&#26032;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#30417;&#30563;&#24335;&#27880;&#24847;&#25216;&#26415;&#65292;&#24341;&#23548;&#23398;&#20064;&#30340;&#27880;&#24847;&#26426;&#21046;&#20559;&#29233;&#30456;&#20851;&#35270;&#22270;&#12290;&#20854;&#27425;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#25552;&#39640;&#20102;&#27599;&#20010;&#21333;&#29420;&#22270;&#20687;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19968;&#20010;&#30495;&#23454;&#30340;&#20020;&#24202;&#25968;&#25454;&#38598;&#65288;4569&#21517;&#24739;&#32773;&#65289;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#22312;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20043;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aortic stenosis (AS) is a degenerative valve condition that causes substantial morbidity and mortality. This condition is under-diagnosed and under-treated. In clinical practice, AS is diagnosed with expert review of transthoracic echocardiography, which produces dozens of ultrasound images of the heart. Only some of these views show the aortic valve. To automate screening for AS, deep networks must learn to mimic a human expert's ability to identify views of the aortic valve then aggregate across these relevant images to produce a study-level diagnosis. We find previous approaches to AS detection yield insufficient accuracy due to relying on inflexible averages across images. We further find that off-the-shelf attention-based multiple instance learning (MIL) performs poorly. We contribute a new end-to-end MIL approach with two key methodological innovations. First, a supervised attention technique guides the learned attention mechanism to favor relevant views. Second, a novel self-sup
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26641;&#36718;&#25968;&#23383;&#27700;&#21360;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#31283;&#23450;&#22320;&#25351;&#32441;&#25193;&#25955;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#19982;&#29616;&#26377;&#30340;&#22312;&#37319;&#26679;&#21518;&#23545;&#22270;&#20687;&#36827;&#34892;&#20462;&#25913;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25968;&#23383;&#27700;&#21360;&#24494;&#22937;&#22320;&#24433;&#21709;&#25972;&#20010;&#37319;&#26679;&#36807;&#31243;&#65292;&#20174;&#32780;&#20135;&#29983;&#27169;&#22411;&#25351;&#32441;&#65292;&#23545;&#20154;&#31867;&#19981;&#21487;&#35265;&#12290;</title><link>http://arxiv.org/abs/2305.20030</link><description>&lt;p&gt;
&#26641;&#36718;&#25968;&#23383;&#27700;&#21360;&#65306;&#19968;&#31181;&#19981;&#21487;&#35265;&#19988;&#20581;&#22766;&#30340;&#25193;&#25955;&#22270;&#20687;&#25351;&#32441;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Tree-Ring Watermarks: Fingerprints for Diffusion Images that are Invisible and Robust. (arXiv:2305.20030v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.20030
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26641;&#36718;&#25968;&#23383;&#27700;&#21360;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#31283;&#23450;&#22320;&#25351;&#32441;&#25193;&#25955;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#19982;&#29616;&#26377;&#30340;&#22312;&#37319;&#26679;&#21518;&#23545;&#22270;&#20687;&#36827;&#34892;&#20462;&#25913;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25968;&#23383;&#27700;&#21360;&#24494;&#22937;&#22320;&#24433;&#21709;&#25972;&#20010;&#37319;&#26679;&#36807;&#31243;&#65292;&#20174;&#32780;&#20135;&#29983;&#27169;&#22411;&#25351;&#32441;&#65292;&#23545;&#20154;&#31867;&#19981;&#21487;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#36755;&#20986;&#30340;&#25968;&#23383;&#27700;&#21360;&#26159;&#36861;&#36394;&#29256;&#26435;&#21644;&#38450;&#27490;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#28508;&#22312;&#23041;&#32961;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#26412;&#25991;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#31216;&#20026;&#26641;&#36718;&#25968;&#23383;&#27700;&#21360;&#65292;&#21487;&#20197;&#31283;&#23450;&#22320;&#25351;&#32441;&#25193;&#25955;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#19982;&#29616;&#26377;&#30340;&#22312;&#37319;&#26679;&#21518;&#23545;&#22270;&#20687;&#36827;&#34892;&#20462;&#25913;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#26641;&#36718;&#25968;&#23383;&#27700;&#21360;&#24494;&#22937;&#22320;&#24433;&#21709;&#25972;&#20010;&#37319;&#26679;&#36807;&#31243;&#65292;&#20174;&#32780;&#20135;&#29983;&#27169;&#22411;&#25351;&#32441;&#65292;&#23545;&#20154;&#31867;&#19981;&#21487;&#35265;&#12290;&#25968;&#23383;&#27700;&#21360;&#23558;&#22270;&#20687;&#29983;&#25104;&#25152;&#20351;&#29992;&#30340;&#21021;&#22987;&#22122;&#22768;&#21521;&#37327;&#20013;&#23884;&#20837;&#19968;&#20010;&#27169;&#24335;&#12290;&#36825;&#20123;&#27169;&#24335;&#22312;&#20613;&#37324;&#21494;&#31354;&#38388;&#20013;&#32467;&#26500;&#21270;&#65292;&#22240;&#27492;&#23427;&#20204;&#23545;&#21367;&#31215;&#12289;&#35009;&#21098;&#12289;&#33192;&#32960;&#12289;&#32763;&#36716;&#21644;&#26059;&#36716;&#20855;&#26377;&#19981;&#21464;&#24615;&#12290;&#22270;&#20687;&#29983;&#25104;&#21518;&#65292;&#36890;&#36807;&#21453;&#28436;&#25193;&#25955;&#36807;&#31243;&#26469;&#26816;&#27979;&#27700;&#21360;&#20449;&#21495;&#65292;&#20197;&#26816;&#32034;&#23884;&#20837;&#20449;&#21495;&#30340;&#22122;&#22768;&#21521;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#25216;&#26415;&#21487;&#20197;&#36731;&#26494;&#24212;&#29992;&#20110;&#20219;&#24847;&#25193;&#25955;&#27169;&#22411;&#65292;&#21253;&#25324;&#25991;&#26412;&#26465;&#20214;&#31283;&#23450;&#25193;&#25955;&#12290;
&lt;/p&gt;
&lt;p&gt;
Watermarking the outputs of generative models is a crucial technique for tracing copyright and preventing potential harm from AI-generated content. In this paper, we introduce a novel technique called Tree-Ring Watermarking that robustly fingerprints diffusion model outputs. Unlike existing methods that perform post-hoc modifications to images after sampling, Tree-Ring Watermarking subtly influences the entire sampling process, resulting in a model fingerprint that is invisible to humans. The watermark embeds a pattern into the initial noise vector used for sampling. These patterns are structured in Fourier space so that they are invariant to convolutions, crops, dilations, flips, and rotations. After image generation, the watermark signal is detected by inverting the diffusion process to retrieve the noise vector, which is then checked for the embedded signal. We demonstrate that this technique can be easily applied to arbitrary diffusion models, including text-conditioned Stable Diff
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25903;&#25345;&#21453;&#21521;&#20256;&#25773;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#8212;&#8212;&#26463;&#25628;&#32034;&#36882;&#24402;&#21333;&#20803;&#65288;BT-Cell&#65289;&#65292;&#29992;&#20110;&#25193;&#23637;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65292;&#23454;&#29616;&#23545;&#28508;&#22312;&#32467;&#26500;&#30340;&#24863;&#30693;&#65307;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25918;&#26494;&#26463;&#25628;&#32034;&#20013;&#30828;&#21069;k&#31639;&#23376;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#26799;&#24230;&#20449;&#21495;&#20256;&#36882;&#12290;&#22312;&#35780;&#20272;&#20013;&#21457;&#29616;&#65292;BT-Cell&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#30340;&#22810;&#20010;&#20855;&#26377;&#32467;&#26500;&#25935;&#24863;&#24615;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2305.19999</link><description>&lt;p&gt;
&#26463;&#25628;&#32034;&#36882;&#24402;&#21333;&#20803;&#65306;&#19968;&#31181;&#25903;&#25345;&#21453;&#21521;&#20256;&#25773;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Beam Tree Recursive Cells. (arXiv:2305.19999v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19999
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25903;&#25345;&#21453;&#21521;&#20256;&#25773;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#8212;&#8212;&#26463;&#25628;&#32034;&#36882;&#24402;&#21333;&#20803;&#65288;BT-Cell&#65289;&#65292;&#29992;&#20110;&#25193;&#23637;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65292;&#23454;&#29616;&#23545;&#28508;&#22312;&#32467;&#26500;&#30340;&#24863;&#30693;&#65307;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25918;&#26494;&#26463;&#25628;&#32034;&#20013;&#30828;&#21069;k&#31639;&#23376;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#26799;&#24230;&#20449;&#21495;&#20256;&#36882;&#12290;&#22312;&#35780;&#20272;&#20013;&#21457;&#29616;&#65292;BT-Cell&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#30340;&#22810;&#20010;&#20855;&#26377;&#32467;&#26500;&#25935;&#24863;&#24615;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;&#26463;&#25628;&#32034;&#36882;&#24402;&#21333;&#20803;&#65288;BT-Cell&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25193;&#23637;&#25903;&#25345;&#20351;&#29992;&#26463;&#25628;&#32034;&#36827;&#34892;&#28508;&#22312;&#32467;&#26500;&#24863;&#30693;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;RvNN&#65289;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#25552;&#20986;&#22312;&#26463;&#25628;&#32034;&#20013;&#23545;&#30828;&#24615;&#21069;k&#31639;&#23376;&#30340;&#25918;&#26494;&#26469;&#25193;&#23637;&#27492;&#26694;&#26550;&#65292;&#20197;&#26356;&#22909;&#22320;&#20256;&#36882;&#26799;&#24230;&#20449;&#21495;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#30340;&#19981;&#21516;&#20195;&#34920;&#24615;&#20998;&#24067;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;BT-Cell&#22312;&#22810;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20307;&#29616;&#32467;&#26500;&#25935;&#24863;&#24615;&#30340;&#20219;&#21153;&#65288;&#22914;ListOps&#21644;&#36923;&#36753;&#25512;&#29702;&#65289;&#19978;&#36798;&#21040;&#20102;&#20960;&#20046;&#23436;&#32654;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#22312;&#23454;&#38469;&#25968;&#25454;&#19978;&#19982;&#20854;&#20182;&#22522;&#20110;RvNN&#30340;&#27169;&#22411;&#20855;&#26377;&#21487;&#27604;&#24615;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;ListOps&#20013;&#30830;&#23450;&#20102;&#31070;&#32463;&#27169;&#22411;&#22312;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#21442;&#25968;&#25968;&#37327;&#19978;&#30340;&#26410;&#30693;&#22833;&#25928;&#26696;&#20363;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/JRC1995/BeamTreeRecursiveCells&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Beam Tree Recursive Cell (BT-Cell) - a backpropagation-friendly framework to extend Recursive Neural Networks (RvNNs) with beam search for latent structure induction. We further extend this framework by proposing a relaxation of the hard top-k operators in beam search for better propagation of gradient signals. We evaluate our proposed models in different out-of-distribution splits in both synthetic and realistic data. Our experiments show that BTCell achieves near-perfect performance on several challenging structure-sensitive synthetic tasks like ListOps and logical inference while maintaining comparable performance in realistic data against other RvNN-based models. Additionally, we identify a previously unknown failure case for neural models in generalization to unseen number of arguments in ListOps. The code is available at: https://github.com/JRC1995/BeamTreeRecursiveCells.
&lt;/p&gt;</description></item><item><title>InGram&#26159;&#19968;&#31181;&#26032;&#30340;&#24402;&#32435;&#24335;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#25512;&#29702;&#26102;&#29983;&#25104;&#26032;&#20851;&#31995;&#21644;&#23454;&#20307;&#30340;&#23884;&#20837;&#65292;&#24182;&#20351;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#27719;&#24635;&#37051;&#23621;&#23884;&#20837;&#29983;&#25104;&#20851;&#31995;&#21644;&#23454;&#20307;&#23884;&#20837;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20934;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.19987</link><description>&lt;p&gt;
InGram&#65306;&#36890;&#36807;&#20851;&#31995;&#22270;&#36827;&#34892;&#24402;&#32435;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
InGram: Inductive Knowledge Graph Embedding via Relation Graphs. (arXiv:2305.19987v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19987
&lt;/p&gt;
&lt;p&gt;
InGram&#26159;&#19968;&#31181;&#26032;&#30340;&#24402;&#32435;&#24335;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#25512;&#29702;&#26102;&#29983;&#25104;&#26032;&#20851;&#31995;&#21644;&#23454;&#20307;&#30340;&#23884;&#20837;&#65292;&#24182;&#20351;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#27719;&#24635;&#37051;&#23621;&#23884;&#20837;&#29983;&#25104;&#20851;&#31995;&#21644;&#23454;&#20307;&#23884;&#20837;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24402;&#32435;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#34987;&#35270;&#20026;&#39044;&#27979;&#35757;&#32451;&#26399;&#38388;&#26410;&#35266;&#23519;&#21040;&#30340;&#26032;&#23454;&#20307;&#20043;&#38388;&#30340;&#32570;&#22833;&#19977;&#20803;&#32452;&#30340;&#20219;&#21153;&#12290;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26041;&#27861;InGram&#65292;&#23427;&#21487;&#20197;&#22312;&#25512;&#29702;&#26102;&#29983;&#25104;&#26032;&#20851;&#31995;&#21644;&#23454;&#20307;&#30340;&#23884;&#20837;&#65292;&#24182;&#22522;&#20110;&#20851;&#31995;&#22270;&#21644;&#21407;&#22987;&#30693;&#35782;&#22270;&#35889;&#20351;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#27719;&#24635;&#37051;&#23621;&#23884;&#20837;&#20197;&#29983;&#25104;&#20851;&#31995;&#21644;&#23454;&#20307;&#23884;&#20837;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;InGram&#30340;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inductive knowledge graph completion has been considered as the task of predicting missing triplets between new entities that are not observed during training. While most inductive knowledge graph completion methods assume that all entities can be new, they do not allow new relations to appear at inference time. This restriction prohibits the existing methods from appropriately handling real-world knowledge graphs where new entities accompany new relations. In this paper, we propose an INductive knowledge GRAph eMbedding method, InGram, that can generate embeddings of new relations as well as new entities at inference time. Given a knowledge graph, we define a relation graph as a weighted graph consisting of relations and the affinity weights between them. Based on the relation graph and the original knowledge graph, InGram learns how to aggregate neighboring embeddings to generate relation and entity embeddings using an attention mechanism. Experimental results show that InGram outper
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38160;&#24230;&#24863;&#30693;&#20248;&#21270;&#30340;&#22810;&#25968;&#25454;&#38598;&#21327;&#21516;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38899;&#39057;&#21453;&#27450;&#39575;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#20165;&#38656;&#35201;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;4000&#20493;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2305.19953</link><description>&lt;p&gt;
&#29992;&#38160;&#24230;&#24863;&#30693;&#20248;&#21270;&#30340;&#22810;&#25968;&#25454;&#38598;&#21327;&#21516;&#35757;&#32451;&#29992;&#20110;&#38899;&#39057;&#21453;&#27450;&#39575;
&lt;/p&gt;
&lt;p&gt;
Multi-Dataset Co-Training with Sharpness-Aware Optimization for Audio Anti-spoofing. (arXiv:2305.19953v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19953
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38160;&#24230;&#24863;&#30693;&#20248;&#21270;&#30340;&#22810;&#25968;&#25454;&#38598;&#21327;&#21516;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38899;&#39057;&#21453;&#27450;&#39575;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#20165;&#38656;&#35201;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;4000&#20493;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35828;&#35805;&#20154;&#39564;&#35777;&#20013;&#30340;&#38899;&#39057;&#21453;&#27450;&#39575;&#26088;&#22312;&#20445;&#25252;&#29992;&#25143;&#30340;&#36523;&#20221;&#20813;&#21463;&#27450;&#39575;&#25915;&#20987;&#12290;&#23613;&#31649;&#26368;&#26032;&#30340;&#27450;&#39575;&#23545;&#31574;&#65288;CM&#65289;&#27169;&#22411;&#22312;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#20351;&#29992;&#19981;&#21516;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#26102;&#32570;&#20047;&#27867;&#21270;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20043;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#25506;&#32034;&#20102;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#36825;&#38656;&#35201;&#22823;&#37327;&#30340;&#36164;&#28304;&#21644;&#26102;&#38388;&#12290;&#25105;&#20204;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#32039;&#20945;&#32780;&#21448;&#20855;&#26377;&#33391;&#22909;&#27867;&#21270;&#24615;&#30340;CM&#27169;&#22411;&#65292;&#21487;&#20197;&#19982;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#31454;&#20105;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#22810;&#25968;&#25454;&#38598;&#21327;&#21516;&#35757;&#32451;&#21644;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#65292;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#23578;&#26410;&#24471;&#21040;&#30740;&#31350;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#21033;&#29992;&#27604;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#23569;4000&#20493;&#30340;&#21442;&#25968;&#30340;&#21516;&#26102;&#65292;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#20135;&#29983;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Audio anti-spoofing for automatic speaker verification aims to safeguard users' identities from spoofing attacks. Although state-of-the-art spoofing countermeasure(CM) models perform well on specific datasets, they lack generalization when evaluated with different datasets. To address this limitation, previous studies have explored large pre-trained models, which require significant resources and time. We aim to develop a compact but well-generalizing CM model that can compete with large pre-trained models. Our approach involves multi-dataset co-training and sharpness-aware minimization, which has not been investigated in this domain. Extensive experiments reveal that proposed method yield competitive results across various datasets while utilizing 4,000 times less parameters than the large pre-trained models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#29983;&#25104;&#24335;&#25193;&#25955;&#27169;&#22411;&#23384;&#22312;&#33258;&#21457;&#23545;&#31216;&#30772;&#32570;&#29616;&#35937;&#65292;&#36825;&#23558;&#20854;&#29983;&#25104;&#21160;&#21147;&#23398;&#20998;&#20026;&#20004;&#31181;&#19981;&#21516;&#30340;&#8220;&#30456;&#8221;&#65292;&#25552;&#20986;&#20102;&#39640;&#26031;&#21518;&#26399;&#21021;&#22987;&#21270;&#26041;&#26696;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.19693</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#33258;&#21457;&#23545;&#31216;&#30772;&#32570;
&lt;/p&gt;
&lt;p&gt;
Spontaneous symmetry breaking in generative diffusion models. (arXiv:2305.19693v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19693
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#29983;&#25104;&#24335;&#25193;&#25955;&#27169;&#22411;&#23384;&#22312;&#33258;&#21457;&#23545;&#31216;&#30772;&#32570;&#29616;&#35937;&#65292;&#36825;&#23558;&#20854;&#29983;&#25104;&#21160;&#21147;&#23398;&#20998;&#20026;&#20004;&#31181;&#19981;&#21516;&#30340;&#8220;&#30456;&#8221;&#65292;&#25552;&#20986;&#20102;&#39640;&#26031;&#21518;&#26399;&#21021;&#22987;&#21270;&#26041;&#26696;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#25193;&#25955;&#27169;&#22411;&#36817;&#26399;&#25104;&#20026;&#20102;&#29983;&#25104;&#39640;&#32500;&#25968;&#25454;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#21160;&#21147;&#23398;&#23384;&#22312;&#33258;&#21457;&#23545;&#31216;&#30772;&#32570;&#65292;&#23558;&#29983;&#25104;&#24335;&#21160;&#21147;&#23398;&#20998;&#20026;&#20004;&#20010;&#19981;&#21516;&#30340;&#8220;&#30456;&#8221;&#65306;1&#65289;&#20013;&#24515;&#22266;&#23450;&#28857;&#21608;&#22260;&#30340;&#32447;&#24615;&#31283;&#24577;&#21160;&#21147;&#23398;&#65292;2&#65289;&#26397;&#21521;&#25968;&#25454;&#27969;&#24418;&#30340;&#21560;&#24341;&#23376;&#21160;&#21147;&#23398;&#12290;&#36825;&#20004;&#31181;&#8220;&#30456;&#8221;&#30001;&#20013;&#24515;&#22266;&#23450;&#28857;&#31283;&#23450;&#24615;&#21464;&#21270;&#25152;&#20998;&#38548;&#65292;&#32780;&#19981;&#31283;&#23450;&#31383;&#21475;&#36127;&#36131;&#29983;&#25104;&#26679;&#26412;&#30340;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#29702;&#35770;&#21644;&#23454;&#35777;&#35777;&#25454;&#34920;&#26126;&#65292;&#20934;&#30830;&#22320;&#27169;&#25311;&#26089;&#26399;&#21160;&#21147;&#23398;&#24182;&#19981;&#20250;&#23545;&#26368;&#32456;&#29983;&#25104;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#65292;&#22240;&#20026;&#26089;&#26399;&#28072;&#33853;&#20250;&#22238;&#21040;&#20013;&#24515;&#22266;&#23450;&#28857;&#12290;&#20026;&#20102;&#21033;&#29992;&#36825;&#19968;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#26031;&#21518;&#26399;&#21021;&#22987;&#21270;&#26041;&#26696;&#65292;&#36825;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#22312;&#24555;&#36895;&#21462;&#26679;&#22120;&#19978;&#23454;&#29616;&#20102;&#38271;&#36798;3&#20493;&#30340;FID&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative diffusion models have recently emerged as a leading approach for generating high-dimensional data. In this paper, we show that the dynamics of these models exhibit a spontaneous symmetry breaking that divides the generative dynamics into two distinct phases: 1) A linear steady-state dynamics around a central fixed-point and 2) an attractor dynamics directed towards the data manifold. These two "phases" are separated by the change in stability of the central fixed-point, with the resulting window of instability being responsible for the diversity of the generated samples. Using both theoretical and empirical evidence, we show that an accurate simulation of the early dynamics does not significantly contribute to the final generation, since early fluctuations are reverted to the central fixed point. To leverage this insight, we propose a Gaussian late initialization scheme, which significantly improves model performance, achieving up to 3x FID improvements on fast samplers, whi
&lt;/p&gt;</description></item><item><title>Point-GCC&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20960;&#20309;-&#39068;&#33394;&#23545;&#27604;&#36827;&#34892;&#36890;&#29992;&#33258;&#30417;&#30563;&#19977;&#32500;&#22330;&#26223;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#20998;&#23618;&#30417;&#30563;&#21644;&#26550;&#26500;&#26080;&#20851;&#30340;&#39592;&#24178;&#32593;&#32476;&#65292;&#20197;&#32553;&#23567;&#39044;&#35757;&#32451;&#21644;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2305.19623</link><description>&lt;p&gt;
Point-GCC: &#22522;&#20110;&#20960;&#20309;-&#39068;&#33394;&#23545;&#27604;&#30340;&#36890;&#29992;&#33258;&#30417;&#30563;&#19977;&#32500;&#22330;&#26223;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Point-GCC: Universal Self-supervised 3D Scene Pre-training via Geometry-Color Contrast. (arXiv:2305.19623v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19623
&lt;/p&gt;
&lt;p&gt;
Point-GCC&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20960;&#20309;-&#39068;&#33394;&#23545;&#27604;&#36827;&#34892;&#36890;&#29992;&#33258;&#30417;&#30563;&#19977;&#32500;&#22330;&#26223;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#20998;&#23618;&#30417;&#30563;&#21644;&#26550;&#26500;&#26080;&#20851;&#30340;&#39592;&#24178;&#32593;&#32476;&#65292;&#20197;&#32553;&#23567;&#39044;&#35757;&#32451;&#21644;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28857;&#20113;&#25552;&#20379;&#30340;&#20960;&#20309;&#21644;&#39068;&#33394;&#20449;&#24687;&#23545;&#20110;&#19977;&#32500;&#22330;&#26223;&#29702;&#35299;&#37117;&#38750;&#24120;&#37325;&#35201;&#65292;&#28982;&#32780;&#29616;&#26377;&#26041;&#27861;&#22312;&#21306;&#20998;&#21644;&#30456;&#20851;&#24615;&#26041;&#38754;&#32570;&#20047;&#31934;&#32454;&#35774;&#35745;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#28857;&#20113;&#20449;&#24687;&#20851;&#31995;&#30340;&#19977;&#32500;&#33258;&#30417;&#30563;&#33539;&#24335;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807; Siamese &#32593;&#32476;&#23545;&#40784;&#20960;&#20309;&#21644;&#39068;&#33394;&#20449;&#24687;&#30340;&#36890;&#29992;&#19977;&#32500;&#22330;&#26223;&#39044;&#35757;&#32451;&#26694;&#26550; Point-GCC&#12290;&#20026;&#20102;&#29031;&#39038;&#23454;&#38469;&#24212;&#29992;&#20219;&#21153;&#65292;&#35774;&#35745;&#20102;&#65288;i&#65289;&#22522;&#20110;&#26032;&#39062;&#30340;&#28145;&#24230;&#32858;&#31867;&#27169;&#22359;&#30340;&#28857;&#32423;&#23545;&#27604;&#21644;&#37325;&#24314;&#21644;&#29289;&#20307;&#32423;&#23545;&#27604;&#30340;&#20998;&#23618;&#30417;&#30563;&#65292;&#26469;&#32553;&#23567;&#39044;&#35757;&#32451;&#21644;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#30340;&#24046;&#36317;&#65307;&#65288;ii&#65289;&#26550;&#26500;&#26080;&#20851;&#30340;&#39592;&#24178;&#32593;&#32476;&#65292;&#20197;&#36866;&#24212;&#21508;&#31181;&#19979;&#28216;&#27169;&#22411;&#12290;&#30001;&#20110;&#19982;&#19979;&#28216;&#20219;&#21153;&#30456;&#20851;&#32852;&#30340;&#29289;&#20307;&#32423;&#34920;&#31034;&#65292;Point-GCC &#21487;&#20197;&#30452;&#25509;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Geometry and color information provided by the point clouds are both crucial for 3D scene understanding. Two pieces of information characterize the different aspects of point clouds, but existing methods lack an elaborate design for the discrimination and relevance. Hence we explore a 3D self-supervised paradigm that can better utilize the relations of point cloud information. Specifically, we propose a universal 3D scene pre-training framework via Geometry-Color Contrast (Point-GCC), which aligns geometry and color information using a Siamese network. To take care of actual application tasks, we design (i) hierarchical supervision with point-level contrast and reconstruct and object-level contrast based on the novel deep clustering module to close the gap between pre-training and downstream tasks; (ii) architecture-agnostic backbone to adapt for various downstream models. Benefiting from the object-level representation associated with downstream tasks, Point-GCC can directly evaluate 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#20869;&#37096;&#20998;&#24067;&#22312;&#32447;&#36866;&#24212;(IDAQ)&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#31574;&#30053;&#21518;&#39564;&#38598;&#21512;&#21644;&#20449;&#24565;&#26356;&#26032;&#32593;&#32476;&#37327;&#21270;&#31574;&#30053;&#19981;&#30830;&#23450;&#24615;&#24182;&#29983;&#25104;&#19978;&#19979;&#25991;&#20449;&#24687;&#26469;&#22788;&#29702;&#26032;&#20219;&#21153;&#65292;&#22312;&#31163;&#32447;&#20803;&#24378;&#21270;&#23398;&#20064;&#19978;&#20855;&#26377;&#31454;&#20105;&#24615;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.19529</link><description>&lt;p&gt;
&#24102;&#26377;&#20869;&#37096;&#20998;&#24067;&#22312;&#32447;&#36866;&#24212;&#30340;&#31163;&#32447;&#20803;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Offline Meta Reinforcement Learning with In-Distribution Online Adaptation. (arXiv:2305.19529v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19529
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#20869;&#37096;&#20998;&#24067;&#22312;&#32447;&#36866;&#24212;(IDAQ)&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#31574;&#30053;&#21518;&#39564;&#38598;&#21512;&#21644;&#20449;&#24565;&#26356;&#26032;&#32593;&#32476;&#37327;&#21270;&#31574;&#30053;&#19981;&#30830;&#23450;&#24615;&#24182;&#29983;&#25104;&#19978;&#19979;&#25991;&#20449;&#24687;&#26469;&#22788;&#29702;&#26032;&#20219;&#21153;&#65292;&#22312;&#31163;&#32447;&#20803;&#24378;&#21270;&#23398;&#20064;&#19978;&#20855;&#26377;&#31454;&#20105;&#24615;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#31163;&#32447;&#20803;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#21033;&#29992;&#20219;&#21153;&#30456;&#20851;&#30340;&#34892;&#20026;&#31574;&#30053;(&#20363;&#22914;&#65292;&#23545;&#27599;&#20010;&#20010;&#20307;&#20219;&#21153;&#36827;&#34892;RL&#26234;&#33021;&#20307;&#30340;&#35757;&#32451;)&#26469;&#25910;&#38598;&#22810;&#20219;&#21153;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24635;&#26159;&#38656;&#35201;&#39069;&#22806;&#30340;&#20449;&#24687;&#36827;&#34892;&#24555;&#36895;&#35843;&#25972;&#65292;&#20363;&#22914;&#27979;&#35797;&#20219;&#21153;&#30340;&#31163;&#32447;&#19978;&#19979;&#25991;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#27491;&#24335;&#22320;&#34920;&#24449;&#20102;&#31163;&#32447;&#20803;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#29420;&#29305;&#25361;&#25112;&#65306;&#31163;&#32447;&#25968;&#25454;&#38598;&#21644;&#22312;&#32447;&#36866;&#24212;&#20043;&#38388;&#30340;&#36716;&#25442;-&#22870;&#21169;&#20998;&#24067;&#20559;&#31227;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#65292;&#26469;&#33258;&#20998;&#24067;&#20043;&#22806;&#30340;&#36866;&#24212;&#24773;&#20917;&#21487;&#33021;&#20250;&#23548;&#33268;&#19981;&#21487;&#38752;&#30340;&#31574;&#30053;&#35780;&#20272;&#65292;&#24182;&#19988;&#20351;&#29992;&#20998;&#24067;&#20869;&#30340;&#24773;&#20917;&#36827;&#34892;&#22312;&#32447;&#36866;&#24212;&#21487;&#20197;&#30830;&#20445;&#36866;&#24212;&#24615;&#33021;&#20445;&#35777;&#12290;&#22522;&#20110;&#36825;&#20123;&#29702;&#35770;&#27934;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36866;&#24212;&#26694;&#26550;&#65292;&#31216;&#20026;&#24102;&#26377;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#20869;&#37096;&#20998;&#24067;&#22312;&#32447;&#36866;&#24212;(IDAQ)&#65292;&#23427;&#21033;&#29992;&#31574;&#30053;&#21518;&#39564;&#38598;&#21512;&#21644;&#20449;&#24565;&#26356;&#26032;&#32593;&#32476;&#37327;&#21270;&#31574;&#30053;&#19981;&#30830;&#23450;&#24615;&#24182;&#29983;&#25104;&#19978;&#19979;&#25991;&#20449;&#24687;&#26469;&#22788;&#29702;&#26032;&#20219;&#21153;&#12290; &#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;IDAQ&#30340;&#25928;&#26524;&#20248;&#20110;&#29616;&#26377;&#30340;&#31163;&#32447;&#20803;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#19988;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#22312;&#32447;&#20803;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#31454;&#20105;&#24615;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent offline meta-reinforcement learning (meta-RL) methods typically utilize task-dependent behavior policies (e.g., training RL agents on each individual task) to collect a multi-task dataset. However, these methods always require extra information for fast adaptation, such as offline context for testing tasks. To address this problem, we first formally characterize a unique challenge in offline meta-RL: transition-reward distribution shift between offline datasets and online adaptation. Our theory finds that out-of-distribution adaptation episodes may lead to unreliable policy evaluation and that online adaptation with in-distribution episodes can ensure adaptation performance guarantee. Based on these theoretical insights, we propose a novel adaptation framework, called In-Distribution online Adaptation with uncertainty Quantification (IDAQ), which generates in-distribution context using a given uncertainty quantification and performs effective task belief inference to address new
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;JLMs&#30340;&#26631;&#31614;&#23884;&#20837;&#26041;&#27861;&#65292;&#23558;&#22810;&#20803;&#20998;&#31867;&#38382;&#39064;&#36716;&#21270;&#20026;&#26377;&#38480;&#22238;&#24402;&#38382;&#39064;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#21644;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.19470</link><description>&lt;p&gt;
&#29992;Johnson-Lindenstrauss&#30697;&#38453;&#36827;&#34892;&#26631;&#31614;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Label Embedding by Johnson-Lindenstrauss Matrices. (arXiv:2305.19470v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19470
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;JLMs&#30340;&#26631;&#31614;&#23884;&#20837;&#26041;&#27861;&#65292;&#23558;&#22810;&#20803;&#20998;&#31867;&#38382;&#39064;&#36716;&#21270;&#20026;&#26377;&#38480;&#22238;&#24402;&#38382;&#39064;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#21644;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Johnson-Lindenstrauss&#30697;&#38453;&#65288;JLMs&#65289;&#30340;&#31616;&#21333;&#19988;&#21487;&#25193;&#23637;&#30340;&#26497;&#31471;&#22810;&#20803;&#20998;&#31867;&#26694;&#26550;&#12290;&#21033;&#29992;JLM&#30340;&#21015;&#26469;&#23884;&#20837;&#26631;&#31614;&#65292;&#23558;&#19968;&#20010;C&#31867;&#20998;&#31867;&#38382;&#39064;&#36716;&#21270;&#20026;&#20855;&#26377;$\cO(\log C)$&#36755;&#20986;&#32500;&#24230;&#30340;&#22238;&#24402;&#38382;&#39064;&#12290;&#25105;&#20204;&#24471;&#20986;&#20102;&#19968;&#20010;&#36229;&#37327;&#39118;&#38505;&#38480;&#21046;&#65292;&#38416;&#26126;&#20102;&#35745;&#31639;&#25928;&#29575;&#21644;&#39044;&#27979;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#36827;&#19968;&#27493;&#34920;&#26126;&#65292;&#22312;Massart&#22122;&#22768;&#26465;&#20214;&#19979;&#65292;&#38477;&#32500;&#30340;&#24809;&#32602;&#20250;&#28040;&#22833;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26131;&#20110;&#24182;&#34892;&#21270;&#65292;&#24182;&#19988;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#22312;&#22823;&#35268;&#27169;&#24212;&#29992;&#20013;&#20854;&#26377;&#25928;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a simple and scalable framework for extreme multiclass classification based on Johnson-Lindenstrauss matrices (JLMs). Using the columns of a JLM to embed the labels, a $C$-class classification problem is transformed into a regression problem with $\cO(\log C)$ output dimension. We derive an excess risk bound, revealing a tradeoff between computational efficiency and prediction accuracy, and further show that under the Massart noise condition, the penalty for dimension reduction vanishes. Our approach is easily parallelizable, and experimental results demonstrate its effectiveness and scalability in large-scale applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;$K^2$-&#26641;&#30340;&#22270;&#29983;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#32039;&#20945;&#29983;&#25104;&#65292;&#24182;&#21516;&#26102;&#25429;&#33719;&#22270;&#30340;&#20869;&#22312;&#20998;&#23618;&#32467;&#26500;&#12290;&#36890;&#36807;&#25552;&#20986;&#39034;&#24207;$K^2$-&#26641;&#34920;&#31034;&#21644;&#24341;&#20837;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#26412;&#25991;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;&#36825;&#31181;&#26041;&#27861;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#22270;&#29983;&#25104;&#26041;&#38754;&#20855;&#26377;&#21331;&#36234;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.19125</link><description>&lt;p&gt;
&#22522;&#20110;$K^2$-&#26641;&#30340;&#20998;&#32423;&#22270;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Graph Generation with $K^2$-trees. (arXiv:2305.19125v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19125
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;$K^2$-&#26641;&#30340;&#22270;&#29983;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#32039;&#20945;&#29983;&#25104;&#65292;&#24182;&#21516;&#26102;&#25429;&#33719;&#22270;&#30340;&#20869;&#22312;&#20998;&#23618;&#32467;&#26500;&#12290;&#36890;&#36807;&#25552;&#20986;&#39034;&#24207;$K^2$-&#26641;&#34920;&#31034;&#21644;&#24341;&#20837;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#26412;&#25991;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;&#36825;&#31181;&#26041;&#27861;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#22270;&#29983;&#25104;&#26041;&#38754;&#20855;&#26377;&#21331;&#36234;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#30446;&#26631;&#20998;&#24067;&#29983;&#25104;&#22270;&#26159;&#35768;&#22810;&#39046;&#22495;&#30340;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#21253;&#25324;&#33647;&#29289;&#21457;&#29616;&#21644;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#21407;&#26412;&#35774;&#35745;&#29992;&#20110;&#26080;&#25439;&#22270;&#21387;&#32553;&#30340;$K^2$-&#26641;&#34920;&#31034;&#30340;&#26032;&#39062;&#22270;&#29983;&#25104;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#21160;&#26426;&#28304;&#20110;$K^2$-&#26641;&#33021;&#22815;&#22312;&#36827;&#34892;&#32039;&#20945;&#29983;&#25104;&#30340;&#21516;&#26102;&#65292;&#25429;&#33719;&#22270;&#30340;&#20869;&#22312;&#20998;&#23618;&#32467;&#26500;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;(1)&#25552;&#20986;&#20102;&#19968;&#31181;&#21253;&#21547;&#21098;&#26525;&#12289;&#25153;&#24179;&#21270;&#21644;&#35760;&#21495;&#21270;&#36807;&#31243;&#30340;&#39034;&#24207;K2&#26641;&#34920;&#31034;&#21644;(2)&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#26088;&#22312;&#36890;&#36807;&#32467;&#21512;&#19987;&#19994;&#26641;&#24418;&#20301;&#32622;&#32534;&#30721;&#26041;&#26696;&#26469;&#29983;&#25104;&#24207;&#21015;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#22235;&#20010;&#24120;&#35268;&#21644;&#20004;&#20010;&#20998;&#23376;&#22270;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#20197;&#35777;&#23454;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#22270;&#29983;&#25104;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating graphs from a target distribution is a significant challenge across many domains, including drug discovery and social network analysis. In this work, we introduce a novel graph generation method leveraging $K^2$-tree representation which was originally designed for lossless graph compression. Our motivation stems from the ability of the $K^2$-trees to enable compact generation while concurrently capturing the inherent hierarchical structure of a graph. In addition, we make further contributions by (1) presenting a sequential $K^2$-tree representation that incorporates pruning, flattening, and tokenization processes and (2) introducing a Transformer-based architecture designed to generate the sequence by incorporating a specialized tree positional encoding scheme. Finally, we extensively evaluate our algorithm on four general and two molecular graph datasets to confirm its superiority for graph generation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#20855;&#26377;&#38750;&#30697;&#24418;&#19981;&#30830;&#23450;&#24615;&#38598;&#30340;&#24378;&#20581;MDP&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#24182;&#24320;&#21457;&#20102;&#25237;&#23556;Langevin&#21160;&#21147;&#23398;&#31639;&#27861;&#21644;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#12290;&#25968;&#20540;&#23454;&#39564;&#23637;&#31034;&#20102;&#36825;&#20123;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.19004</link><description>&lt;p&gt;
&#38750;&#30697;&#24418;&#19981;&#30830;&#23450;&#24615;&#38598;&#30340;&#24378;&#20581;MDP&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Policy Gradient Algorithms for Robust MDPs with Non-Rectangular Uncertainty Sets. (arXiv:2305.19004v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19004
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#20855;&#26377;&#38750;&#30697;&#24418;&#19981;&#30830;&#23450;&#24615;&#38598;&#30340;&#24378;&#20581;MDP&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#24182;&#24320;&#21457;&#20102;&#25237;&#23556;Langevin&#21160;&#21147;&#23398;&#31639;&#27861;&#21644;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#12290;&#25968;&#20540;&#23454;&#39564;&#23637;&#31034;&#20102;&#36825;&#20123;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20026;&#20855;&#26377;&#38750;&#30697;&#24418;&#19981;&#30830;&#23450;&#24615;&#38598;&#30340;&#24378;&#20581;&#26080;&#38480;&#26102;&#22495;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#25552;&#20986;&#20102;&#19968;&#20010;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#24378;&#20581;MDP&#25991;&#29486;&#20013;&#30340;&#19968;&#20010;&#24320;&#25918;&#24615;&#25361;&#25112;&#12290;&#30830;&#23454;&#65292;&#26174;&#31034;&#32479;&#35745;&#26368;&#20248;&#24615;&#36136;&#24182;&#20805;&#20998;&#21033;&#29992;&#26377;&#38480;&#25968;&#25454;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#24448;&#24448;&#19981;&#26159;&#30697;&#24418;&#30340;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#23545;&#24212;&#30340;&#24378;&#20581;MDPs&#19981;&#33021;&#29992;&#21160;&#24577;&#35268;&#21010;&#25216;&#26415;&#35299;&#20915;&#65292;&#24182;&#19988;&#23454;&#38469;&#19978;&#26159;&#21487;&#35777;&#26126;&#30340;&#19981;&#21487;&#35299;&#20915;&#30340;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#24320;&#21457;&#19968;&#20010;&#38024;&#23545;&#24378;&#20581;&#31574;&#30053;&#35780;&#20272;&#38382;&#39064;&#37327;&#36523;&#23450;&#21046;&#30340;&#25237;&#23556;Langevin&#21160;&#21147;&#23398;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#25552;&#20379;&#20840;&#23616;&#26368;&#20248;&#24615;&#20445;&#35777;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36817;&#20284;&#35299;&#20915;&#20102;&#24378;&#20581;&#31574;&#30053;&#35780;&#20272;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#36817;&#20284;&#35823;&#24046;&#19982;&#19981;&#30830;&#23450;&#24615;&#38598;&#30340;&#38750;&#30697;&#24418;&#24230;&#37327;&#25104;&#27604;&#20363;&#12290;&#25968;&#20540;&#23454;&#39564;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#25237;&#24433;Langevin&#21160;&#21147;&#23398;&#31639;&#27861;&#21487;&#20197;&#36991;&#20813;&#23616;&#37096;&#26368;&#20248;&#65292;&#32780;&#31639;&#27861;&#26159;&#37327;&#36523;&#23450;&#21046;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a policy gradient algorithm for robust infinite-horizon Markov Decision Processes (MDPs) with non-rectangular uncertainty sets, thereby addressing an open challenge in the robust MDP literature. Indeed, uncertainty sets that display statistical optimality properties and make optimal use of limited data often fail to be rectangular. Unfortunately, the corresponding robust MDPs cannot be solved with dynamic programming techniques and are in fact provably intractable. This prompts us to develop a projected Langevin dynamics algorithm tailored to the robust policy evaluation problem, which offers global optimality guarantees. We also propose a deterministic policy gradient method that solves the robust policy evaluation problem approximately, and we prove that the approximation error scales with a new measure of non-rectangularity of the uncertainty set. Numerical experiments showcase that our projected Langevin dynamics algorithm can escape local optima, while algorithms tailor
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#30340;&#21344;&#29992;&#26102;&#38388;&#27010;&#24565;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#25193;&#23637;&#20102;&#31163;&#25955;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;PG&#12289;TRPO&#21644;PPO&#26041;&#27861;&#65292;&#20026;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#21644;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.18901</link><description>&lt;p&gt;
&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#30340;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Policy Optimization for Continuous Reinforcement Learning. (arXiv:2305.18901v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18901
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#30340;&#21344;&#29992;&#26102;&#38388;&#27010;&#24565;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#25193;&#23637;&#20102;&#31163;&#25955;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;PG&#12289;TRPO&#21644;PPO&#26041;&#27861;&#65292;&#20026;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#21644;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#36830;&#32493;&#26102;&#38388;&#21644;&#31354;&#38388;&#19979;&#30340;&#24378;&#21270;&#23398;&#20064;&#65292;&#37319;&#29992;&#25240;&#25187;&#22870;&#21169;&#21644;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#22522;&#26412;&#21160;&#24577;&#12290;&#22312;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21344;&#29992;&#26102;&#38388;&#30340;&#27010;&#24565;&#65288;&#29305;&#21035;&#26159;&#38024;&#23545;&#25240;&#25187;&#22870;&#21169;&#65289;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#26377;&#25928;&#22320;&#20351;&#29992;&#23427;&#26469;&#23548;&#20986;&#24615;&#33021;&#24046;&#24322;&#21644;&#23616;&#37096;&#36924;&#36817;&#20844;&#24335;&#12290;&#25105;&#20204;&#36824;&#23558;&#36825;&#20123;&#32467;&#26524;&#25193;&#23637;&#21040;&#20102; PG&#65288;&#31574;&#30053;&#26799;&#24230;&#65289;&#12289;TRPO&#65288;&#20449;&#20219;&#21306;&#22495;&#31574;&#30053;&#20248;&#21270;&#65289;&#21644; PPO&#65288;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65289;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#31163;&#25955;&#24378;&#21270;&#23398;&#20064;&#20013;&#26159;&#29087;&#30693;&#21644;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#20294;&#22312;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#20013;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#21457;&#23637;&#12290;&#36890;&#36807;&#25968;&#23383;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study reinforcement learning (RL) in the setting of continuous time and space, for an infinite horizon with a discounted objective and the underlying dynamics driven by a stochastic differential equation. Built upon recent advances in the continuous approach to RL, we develop a notion of occupation time (specifically for a discounted objective), and show how it can be effectively used to derive performance-difference and local-approximation formulas. We further extend these results to illustrate their applications in the PG (policy gradient) and TRPO/PPO (trust region policy optimization/ proximal policy optimization) methods, which have been familiar and powerful tools in the discrete RL setting but under-developed in continuous RL. Through numerical experiments, we demonstrate the effectiveness and advantages of our approach.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#24418;&#24577;&#29255;&#27573;&#23398;&#20064;&#20197;&#21450;&#22810;&#31890;&#24230;&#23545;&#27604;&#21644;&#22810;&#23610;&#24230;&#23545;&#40784;&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#29305;&#23450;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2305.18888</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#30340;&#23545;&#27604;&#24418;&#24577;&#29255;&#27573;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Contrastive Shapelet Learning for Unsupervised Multivariate Time Series Representation Learning. (arXiv:2305.18888v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18888
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#24418;&#24577;&#29255;&#27573;&#23398;&#20064;&#20197;&#21450;&#22810;&#31890;&#24230;&#23545;&#27604;&#21644;&#22810;&#23610;&#24230;&#23545;&#40784;&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#29305;&#23450;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#65288;URL&#65289;&#20855;&#22791;&#23398;&#20064;&#27867;&#21270;&#34920;&#31034;&#20197;&#21450;&#26080;&#38656;&#20351;&#29992;&#19981;&#21487;&#35775;&#38382;&#26631;&#31614;&#23601;&#33021;&#36866;&#29992;&#20110;&#22810;&#25968;&#19979;&#28216;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#21407;&#26412;&#20026;&#20854;&#20182;&#39046;&#22495;&#65288;&#22914;&#35745;&#31639;&#26426;&#35270;&#35273;&#65289;&#35774;&#35745;&#30340;&#27169;&#22411;&#36827;&#34892;&#32534;&#30721;&#65292;&#19988;&#20381;&#36182;&#20110;&#24378;&#20551;&#35774;&#35774;&#35745;&#23398;&#20064;&#30446;&#26631;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#36816;&#34892;&#34920;&#29616;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#27969;&#34892;&#30340;&#23545;&#27604;&#23398;&#20064;&#33539;&#24335;&#65292;&#23398;&#20064;&#22522;&#20110;&#24418;&#24577;&#29255;&#27573;&#30340;&#26102;&#38388;&#24207;&#21015;&#29305;&#23450;&#34920;&#31034;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#31687;&#25506;&#32034;&#26080;&#30417;&#30563;&#36890;&#29992;&#34920;&#31034;&#23398;&#20064;&#20013;&#24418;&#24577;&#29255;&#27573;&#23884;&#20837;&#30340;&#30740;&#31350;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#22522;&#20110;&#24418;&#24577;&#29255;&#27573;&#30340;&#32534;&#30721;&#22120;&#21644;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#20855;&#26377;&#22810;&#31890;&#24230;&#23545;&#27604;&#21644;&#22810;&#23610;&#24230;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have shown great promise in unsupervised representation learning (URL) for multivariate time series, because URL has the capability in learning generalizable representation for many downstream tasks without using inaccessible labels. However, existing approaches usually adopt the models originally designed for other domains (e.g., computer vision) to encode the time series data and rely on strong assumptions to design learning objectives, which limits their ability to perform well. To deal with these problems, we propose a novel URL framework for multivariate time series by learning time-series-specific shapelet-based representation through a popular contrasting learning paradigm. To the best of our knowledge, this is the first work that explores the shapelet-based embedding in the unsupervised general-purpose representation learning. A unified shapelet-based encoder and a novel learning objective with multi-grained contrasting and multi-scale alignment are particularly 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#22810;&#20934;&#21017;&#25512;&#33616;&#30340;&#26631;&#20934;&#20559;&#22909;&#24863;&#30693;&#36731;&#37327;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;MC&#25193;&#23637;&#22270;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#25429;&#25417;&#29992;&#25143;&#30340;&#26631;&#20934;&#20559;&#22909;&#65292;&#24182;&#36827;&#19968;&#27493;&#23558;&#29992;&#25143;&#23545;&#21508;&#20010;&#26631;&#20934;&#30340;&#20559;&#22909;&#21512;&#24182;&#21040;&#26368;&#32456;&#30340;&#25512;&#33616;&#21015;&#34920;&#20013;&#12290;</title><link>http://arxiv.org/abs/2305.18885</link><description>&lt;p&gt;
&#26631;&#20934;&#27604;&#35780;&#20998;&#26356;&#37325;&#35201;&#65306;&#38754;&#21521;&#22810;&#20934;&#21017;&#25512;&#33616;&#30340;&#26631;&#20934;&#20559;&#22909;&#24863;&#30693;&#36731;&#37327;&#22270;&#21367;&#31215;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Criteria Tell You More than Ratings: Criteria Preference-Aware Light Graph Convolution for Effective Multi-Criteria Recommendation. (arXiv:2305.18885v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18885
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#22810;&#20934;&#21017;&#25512;&#33616;&#30340;&#26631;&#20934;&#20559;&#22909;&#24863;&#30693;&#36731;&#37327;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;MC&#25193;&#23637;&#22270;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#25429;&#25417;&#29992;&#25143;&#30340;&#26631;&#20934;&#20559;&#22909;&#65292;&#24182;&#36827;&#19968;&#27493;&#23558;&#29992;&#25143;&#23545;&#21508;&#20010;&#26631;&#20934;&#30340;&#20559;&#22909;&#21512;&#24182;&#21040;&#26368;&#32456;&#30340;&#25512;&#33616;&#21015;&#34920;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20934;&#21017;&#25512;&#33616;&#31995;&#32479;&#29616;&#22312;&#22312;&#24191;&#27867;&#30340;&#30005;&#23376;&#21830;&#21153;&#39046;&#22495;&#20013;&#21033;&#29992;&#22810;&#20934;&#21017; (MC) &#35780;&#20998;&#20449;&#24687;&#65292;&#32780;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476; (GNN) &#24050;&#32463;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#25512;&#33616;&#31995;&#32479;&#30340;&#24320;&#21457;&#20013;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26412;&#25991;&#39318;&#27425;&#23581;&#35797;&#20351;&#29992;GNN&#36741;&#21161;&#35774;&#35745;MC&#25512;&#33616;&#31995;&#32479;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26631;&#20934;&#20559;&#22909;&#24863;&#30693;&#36731;&#37327;&#22270;&#21367;&#31215;&#26041;&#27861;(CPA-LGC),&#21487;&#20197;&#20934;&#30830;&#25429;&#25417;&#29992;&#25143;&#30340;&#26631;&#20934;&#20559;&#22909;&#20197;&#21450;&#22797;&#26434;&#39640;&#38454;&#36830;&#25509;&#20013;&#30340;&#21327;&#20316;&#20449;&#21495;&#12290;&#26412;&#25991;&#22312;MC&#25193;&#23637;&#22270;&#19978;&#26500;&#24314;&#20102;&#19968;&#20010;&#33021;&#22815;&#23558;&#29992;&#25143;-&#29289;&#21697;MC&#35780;&#20998;&#36716;&#25442;&#20026;&#25193;&#23637;&#20108;&#20998;&#22270;&#30340;MC&#25193;&#23637;&#22270;&#65292;&#20877;&#36827;&#19968;&#27493;&#23558;&#26631;&#20934;&#37325;&#35201;&#24615;&#32534;&#30721;&#21040;&#22270;&#21367;&#31215;&#36807;&#31243;&#20013;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26631;&#20934;&#20559;&#22909;&#24863;&#30693;&#32858;&#21512;&#26041;&#27861;&#26469;&#23558;&#29992;&#25143;&#23545;&#19981;&#21516;&#26631;&#20934;&#30340;&#20559;&#22909;&#21512;&#24182;&#21040;&#26368;&#32456;&#30340;&#25512;&#33616;&#21015;&#34920;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
The multi-criteria (MC) recommender system, which leverages MC rating information in a wide range of e-commerce areas, is ubiquitous nowadays. Surprisingly, although graph neural networks (GNNs) have been widely applied to develop various recommender systems due to GNN's high expressive capability in learning graph representations, it has been still unexplored how to design MC recommender systems with GNNs. In light of this, we make the first attempt towards designing a GNN-aided MC recommender system. Specifically, rather than straightforwardly adopting existing GNN-based recommendation methods, we devise a novel criteria preference-aware light graph convolution CPA-LGC method, which is capable of precisely capturing the criteria preference of users as well as the collaborative signal in complex high-order connectivities. To this end, we first construct an MC expansion graph that transforms user--item MC ratings into an expanded bipartite graph to potentially learn from the collaborat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#21153;&#31561;&#21464;&#22270;Few-shot&#23398;&#20064;&#65288;TEG&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#31561;&#21464;&#24615;&#36136;&#26469;&#20351;&#27169;&#22411;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#20219;&#21153;&#36866;&#24212;&#31574;&#30053;&#12290;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;Few-shot&#20998;&#31867;&#22522;&#20934;&#19978;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.18758</link><description>&lt;p&gt;
&#20219;&#21153;&#31561;&#21464;&#22270;Few-shot&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Task-Equivariant Graph Few-shot Learning. (arXiv:2305.18758v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18758
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#21153;&#31561;&#21464;&#22270;Few-shot&#23398;&#20064;&#65288;TEG&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#31561;&#21464;&#24615;&#36136;&#26469;&#20351;&#27169;&#22411;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#20219;&#21153;&#36866;&#24212;&#31574;&#30053;&#12290;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;Few-shot&#20998;&#31867;&#22522;&#20934;&#19978;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#20854;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#27599;&#31867;&#20855;&#26377;&#36275;&#22815;&#26631;&#35760;&#33410;&#28857;&#30340;&#21487;&#29992;&#24615;&#12290;&#22312;&#29616;&#23454;&#24773;&#20917;&#19979;&#65292;&#19981;&#26159;&#25152;&#26377;&#31867;&#37117;&#26377;&#35768;&#22810;&#26631;&#35760;&#33410;&#28857;&#65292;&#27169;&#22411;&#21487;&#33021;&#38656;&#35201;&#20998;&#31867;&#26032;&#31867;&#21035;&#65292;&#20351;&#24471;&#25163;&#21160;&#26631;&#35760;&#21464;&#24471;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;GNN&#38656;&#35201;&#33021;&#22815;&#29992;&#26377;&#38480;&#25968;&#37327;&#30340;&#26631;&#35760;&#33410;&#28857;&#23545;&#33410;&#28857;&#36827;&#34892;&#20998;&#31867;&#65292;&#31216;&#20026;Few-shot&#33410;&#28857;&#20998;&#31867;&#12290;&#20808;&#21069;&#30340;&#22522;&#20110;&#21095;&#38598;&#20803;&#23398;&#20064;&#30340;&#26041;&#27861;&#22312;Few-shot&#33410;&#28857;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#20165;&#26377;&#22810;&#26679;&#30340;&#35757;&#32451;&#20803;&#20219;&#21153;&#25165;&#33021;&#23454;&#29616;&#26368;&#20339;&#24615;&#33021;&#12290;&#20026;&#20102;&#24212;&#23545;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;Few-shot&#23398;&#20064;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#20219;&#21153;&#31561;&#21464;&#22270;Few-shot&#23398;&#20064;&#65288;TEG&#65289;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;TEG&#26694;&#26550;&#36890;&#36807;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#31561;&#21464;&#24615;&#36136;&#26469;&#20351;&#27169;&#22411;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#20219;&#21153;&#36866;&#24212;&#31574;&#30053;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;Few-shot&#20998;&#31867;&#22522;&#20934;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although Graph Neural Networks (GNNs) have been successful in node classification tasks, their performance heavily relies on the availability of a sufficient number of labeled nodes per class. In real-world situations, not all classes have many labeled nodes and there may be instances where the model needs to classify new classes, making manual labeling difficult. To solve this problem, it is important for GNNs to be able to classify nodes with a limited number of labeled nodes, known as few-shot node classification. Previous episodic meta-learning based methods have demonstrated success in few-shot node classification, but our findings suggest that optimal performance can only be achieved with a substantial amount of diverse training meta-tasks. To address this challenge of meta-learning based few-shot learning (FSL), we propose a new approach, the Task-Equivariant Graph few-shot learning (TEG) framework. Our TEG framework enables the model to learn transferable task-adaptation strate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;GRAM-ODENNs&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#29992;&#20110;&#35299;&#20915;&#24403;&#21069;&#22270;ODE&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#21253;&#25324;&#23616;&#37096;&#27169;&#24335;&#30340;&#24573;&#30053;&#21644;&#32570;&#20047;&#21160;&#24577;&#35821;&#20041;&#36793;&#32536;&#65292;&#29992;&#20110;&#26356;&#20934;&#30830;&#30340;&#38271;&#31243;&#20132;&#36890;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2305.18687</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#30340;&#22810;ODE&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#26102;&#31354;&#20132;&#36890;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Graph-based Multi-ODE Neural Networks for Spatio-Temporal Traffic Forecasting. (arXiv:2305.18687v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18687
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;GRAM-ODENNs&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#29992;&#20110;&#35299;&#20915;&#24403;&#21069;&#22270;ODE&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#21253;&#25324;&#23616;&#37096;&#27169;&#24335;&#30340;&#24573;&#30053;&#21644;&#32570;&#20047;&#21160;&#24577;&#35821;&#20041;&#36793;&#32536;&#65292;&#29992;&#20110;&#26356;&#20934;&#30830;&#30340;&#38271;&#31243;&#20132;&#36890;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#20132;&#36890;&#39046;&#22495;&#20013;&#20986;&#29616;&#20102;&#35768;&#22810;&#26102;&#31354;&#39044;&#27979;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20132;&#36890;&#32593;&#32476;&#20013;&#35266;&#23519;&#21040;&#30340;&#22797;&#26434;&#32780;&#24191;&#27867;&#30340;&#26102;&#31354;&#30456;&#20851;&#24615;&#65292;&#38271;&#31243;&#20132;&#36890;&#39044;&#27979;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#30446;&#21069;&#30340;&#20316;&#21697;&#20027;&#35201;&#20381;&#36182;&#20110;&#20855;&#26377;&#22270;&#24418;&#32467;&#26500;&#30340;&#36947;&#36335;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#23398;&#20064;&#34920;&#31034;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#22312;&#28145;&#24230;&#26550;&#26500;&#20013;&#23384;&#22312;&#36807;&#24230;&#24179;&#28369;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26368;&#36817;&#30340;&#26041;&#27861;&#20171;&#32461;&#20102;&#23558;GNN&#19982;&#27531;&#24046;&#36830;&#25509;&#25110;&#31070;&#32463;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#22270;&#24418;ODE&#27169;&#22411;&#22312;&#29305;&#24449;&#25552;&#21462;&#26041;&#38754;&#23384;&#22312;&#20004;&#20010;&#20851;&#38190;&#38480;&#21046;&#65306;&#65288;1&#65289;&#23427;&#20204;&#20542;&#21521;&#20110;&#20840;&#23616;&#26102;&#38388;&#27169;&#24335;&#65292;&#24573;&#30053;&#20102;&#23545;&#20110;&#24847;&#22806;&#20107;&#20214;&#24456;&#37325;&#35201;&#30340;&#23616;&#37096;&#27169;&#24335;&#65307;&#65288;2&#65289;&#23427;&#20204;&#22312;&#20854;&#26550;&#26500;&#35774;&#35745;&#20013;&#32570;&#20047;&#21160;&#24577;&#35821;&#20041;&#36793;&#32536;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#22522;&#20110;&#22270;&#30340;&#22810;ODE&#31070;&#32463;&#32593;&#32476;&#65288;GRAM-ODENNs&#65289;&#30340;&#26032;&#22411;&#26550;&#26500;&#65292;&#29992;&#20110;&#35299;&#20915;&#24403;&#21069;&#22270;&#24418;ODE&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#12290;GRAM-ODENNs&#20351;&#29992;&#22810;&#20010;&#24182;&#34892;ODE&#32534;&#30721;&#22120;&#21644;&#21160;&#24577;&#22270;&#32467;&#26500;&#65292;&#36866;&#24212;&#20132;&#36890;&#25968;&#25454;&#30340;&#26102;&#38388;&#28436;&#21464;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#38271;&#31243;&#20132;&#36890;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a recent surge in the development of spatio-temporal forecasting models in the transportation domain. Long-range traffic forecasting, however, remains a challenging task due to the intricate and extensive spatio-temporal correlations observed in traffic networks. Current works primarily rely on road networks with graph structures and learn representations using graph neural networks (GNNs), but this approach suffers from over-smoothing problem in deep architectures. To tackle this problem, recent methods introduced the combination of GNNs with residual connections or neural ordinary differential equations (ODE). However, current graph ODE models face two key limitations in feature extraction: (1) they lean towards global temporal patterns, overlooking local patterns that are important for unexpected events; and (2) they lack dynamic semantic edges in their architectural design. In this paper, we propose a novel architecture called Graph-based Multi-ODE Neural Networks (GRAM-OD
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#21487;&#25511;&#27585;&#28781;&#36335;&#24452;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26159;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30340;&#36845;&#20195;&#29983;&#25104;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21521;&#20462;&#22797;&#36712;&#36857;&#30340;&#29366;&#24577;-&#21160;&#20316;&#23545;&#28155;&#21152;&#26465;&#20214;&#36755;&#20837;&#26469;&#23454;&#29616;&#21487;&#25511;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.18553</link><description>&lt;p&gt;
&#21487;&#25511;&#27585;&#28781;&#36335;&#24452;
&lt;/p&gt;
&lt;p&gt;
Controllable Path of Destruction. (arXiv:2305.18553v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18553
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21487;&#25511;&#27585;&#28781;&#36335;&#24452;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26159;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30340;&#36845;&#20195;&#29983;&#25104;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21521;&#20462;&#22797;&#36712;&#36857;&#30340;&#29366;&#24577;-&#21160;&#20316;&#23545;&#28155;&#21152;&#26465;&#20214;&#36755;&#20837;&#26469;&#23454;&#29616;&#21487;&#25511;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27585;&#28781;&#36335;&#24452;&#65288;PoD&#65289;&#26159;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30340;&#36845;&#20195;&#29983;&#25104;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#36890;&#36807;&#30772;&#22351;&#19968;&#32452;&#29289;&#21697;&#26469;&#20135;&#29983;&#19968;&#20010;&#35757;&#32451;&#38598;&#65292;&#20026;&#27599;&#20010;&#30772;&#22351;&#27493;&#39588;&#21019;&#24314;&#19968;&#20010;&#19982;&#30456;&#24212;&#20462;&#22797;&#21160;&#20316;&#30456;&#20851;&#30340;&#35757;&#32451;&#23454;&#20363;&#12290;&#22312;&#27492;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#29983;&#25104;&#22120;&#21487;&#20197;&#36890;&#36807;&#20174;&#20219;&#24847;&#29366;&#24577;&#8220;&#20462;&#22797;&#8221;&#26469;&#29983;&#25104;&#26032;&#30340;&#29289;&#21697;&#12290;PoD&#26041;&#27861;&#22312;&#21407;&#22987;&#35757;&#32451;&#31034;&#20363;&#26041;&#38754;&#38750;&#24120;&#33410;&#30465;&#65292;&#24182;&#19988;&#38750;&#24120;&#36866;&#21512;&#30001;&#20998;&#31867;&#25968;&#25454;&#32452;&#25104;&#30340;&#21151;&#33021;&#37096;&#20214;&#65292;&#20363;&#22914;&#28216;&#25103;&#20851;&#21345;&#21644;&#31163;&#25955;&#30340;3D&#32467;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#27585;&#28781;&#36335;&#24452;&#26041;&#27861;&#25193;&#23637;&#21040;&#20801;&#35768;&#35774;&#35745;&#24072;&#25511;&#21046;&#29983;&#25104;&#30340;&#29289;&#21697;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;&#36890;&#36807;&#21521;&#26500;&#25104;&#20462;&#22797;&#36712;&#36857;&#30340;&#29366;&#24577;-&#21160;&#20316;&#23545;&#28155;&#21152;&#26465;&#20214;&#36755;&#20837;&#26469;&#24341;&#20837;&#21487;&#25511;&#24615;&#12290;&#25105;&#20204;&#22312;2D&#22320;&#29282;&#35774;&#32622;&#20197;&#21450;&#23567;&#22411;3D&#20048;&#39640;&#27773;&#36710;&#39046;&#22495;&#27979;&#35797;&#20102;&#21487;&#25511;PoD&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Path of Destruction (PoD) is a self-supervised method for learning iterative generators. The core idea is to produce a training set by destroying a set of artifacts, and for each destructive step create a training instance based on the corresponding repair action. A generator trained on this dataset can then generate new artifacts by ``repairing'' from arbitrary states. The PoD method is very data-efficient in terms of original training examples and well-suited to functional artifacts composed of categorical data, such as game levels and discrete 3D structures. In this paper, we extend the Path of Destruction method to allow designer control over aspects of the generated artifacts. Controllability is introduced by adding conditional inputs to the state-action pairs that make up the repair trajectories. We test the controllable PoD method in a 2D dungeon setting, as well as in the domain of small 3D Lego cars.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#20219;&#20309;&#25299;&#25169;&#32467;&#26500;&#19978;&#23454;&#29616;&#24102;&#23485;&#26368;&#20248;&#30340;&#20840;&#32858;&#21512;/&#24402;&#32422;&#25955;&#24320;&#65292;&#20026;&#35299;&#20915;&#26041;&#26696;&#25552;&#20379;&#20102;&#36890;&#29992;&#24615;&#65292;&#21487;&#20197;&#36731;&#26494;&#25193;&#23637;&#21040;&#20854;&#20182;&#24418;&#24335;&#30340;&#38598;&#21512;&#36890;&#20449;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.18461</link><description>&lt;p&gt;
&#38598;&#21512;&#36890;&#20449;&#24102;&#23485;&#26368;&#20248;&#31649;&#36947;&#35843;&#24230;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Bandwidth Optimal Pipeline Schedule for Collective Communication. (arXiv:2305.18461v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18461
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#20219;&#20309;&#25299;&#25169;&#32467;&#26500;&#19978;&#23454;&#29616;&#24102;&#23485;&#26368;&#20248;&#30340;&#20840;&#32858;&#21512;/&#24402;&#32422;&#25955;&#24320;&#65292;&#20026;&#35299;&#20915;&#26041;&#26696;&#25552;&#20379;&#20102;&#36890;&#29992;&#24615;&#65292;&#21487;&#20197;&#36731;&#26494;&#25193;&#23637;&#21040;&#20854;&#20182;&#24418;&#24335;&#30340;&#38598;&#21512;&#36890;&#20449;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#20219;&#20309;&#32593;&#32476;&#25299;&#25169;&#19978;&#65288;&#26080;&#35770;&#26159;&#21542;&#26377;&#20132;&#25442;&#26426;&#65289;&#29983;&#25104;&#24102;&#23485;&#26368;&#20248;&#30340;&#20840;&#32858;&#21512;/&#24402;&#32422;&#25955;&#24320;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#26500;&#24314;&#20102;&#19968;&#31181;&#31649;&#36947;&#35843;&#24230;&#65292;&#21487;&#22312;&#32473;&#23450;&#25299;&#25169;&#20013;&#23454;&#29616;&#20445;&#35777;&#26368;&#20339;&#21487;&#34892;&#24102;&#23485;&#24615;&#33021;&#12290;&#20026;&#20102;&#25552;&#20379;&#19968;&#20010;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#23558;&#32593;&#32476;&#25299;&#25169;&#24314;&#27169;&#20026;&#20855;&#26377;&#24322;&#26500;&#38142;&#36335;&#23481;&#37327;&#21644;&#20132;&#25442;&#26426;&#30340;&#26377;&#21521;&#22270;&#65292;&#24182;&#23558;&#20132;&#25442;&#26426;&#30452;&#25509;&#24314;&#27169;&#20026;&#22270;&#34920;&#31034;&#20013;&#30340;&#39030;&#28857;&#12290;&#35813;&#31639;&#27861;&#30456;&#23545;&#20110;&#25299;&#25169;&#22823;&#23567;&#20855;&#26377;&#24378;&#22810;&#39033;&#24335;&#26102;&#38388;&#12290;&#27492;&#24037;&#20316;&#20005;&#37325;&#20381;&#36182;&#20110;&#20808;&#21069;&#30340;&#22270;&#35770;&#30740;&#31350;&#20013;&#30340;&#36793;&#19981;&#30456;&#20132;&#29983;&#25104;&#26641;&#21644;&#36793;&#20998;&#35010;&#12290;&#34429;&#28982;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;&#20840;&#32858;&#21512;&#65292;&#20294;&#26412;&#25991;&#20013;&#30340;&#26041;&#27861;&#21487;&#20197;&#36731;&#26494;&#25193;&#23637;&#20026;&#24402;&#32422;&#12289;&#24191;&#25773;&#12289;&#24402;&#32422;&#25955;&#24320;&#21644;&#20840;&#24402;&#32422;&#30340;&#35843;&#24230;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a strongly polynomial-time algorithm to generate bandwidth optimal allgather/reduce-scatter on any network topology, with or without switches. Our algorithm constructs pipeline schedules achieving provably the best possible bandwidth performance on a given topology. To provide a universal solution, we model the network topology as a directed graph with heterogeneous link capacities and switches directly as vertices in the graph representation. The algorithm is strongly polynomial-time with respect to the topology size. This work heavily relies on previous graph theory work on edge-disjoint spanning trees and edge splitting. While we focus on allgather, the methods in this paper can be easily extended to generate schedules for reduce, broadcast, reduce-scatter, and allreduce.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#31526;&#21512;&#24615;&#39044;&#27979;&#25216;&#26415;&#65292;&#22312;&#22810;&#39033;&#36873;&#25321;&#39064;&#22238;&#31572;&#20219;&#21153;&#20013;&#20026;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#25105;&#20204;&#21457;&#29616;&#31526;&#21512;&#24615;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#19982;&#39044;&#27979;&#20934;&#30830;&#24615;&#23494;&#20999;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2305.18404</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#31572;&#26696;&#30830;&#35748;&#39044;&#27979;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Conformal Prediction with Large Language Models for Multi-Choice Question Answering. (arXiv:2305.18404v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18404
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#31526;&#21512;&#24615;&#39044;&#27979;&#25216;&#26415;&#65292;&#22312;&#22810;&#39033;&#36873;&#25321;&#39064;&#22238;&#31572;&#20219;&#21153;&#20013;&#20026;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#25105;&#20204;&#21457;&#29616;&#31526;&#21512;&#24615;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#19982;&#39044;&#27979;&#20934;&#30830;&#24615;&#23494;&#20999;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24191;&#27867;&#24320;&#21457;&#65292;&#23545;&#23427;&#20204;&#36827;&#34892;&#20581;&#22766;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#25216;&#26415;&#23558;&#25104;&#20026;&#23427;&#20204;&#22312;&#39640;&#39118;&#38505;&#22330;&#26223;&#19979;&#23433;&#20840;&#37096;&#32626;&#30340;&#20851;&#38190;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#31526;&#21512;&#24615;&#39044;&#27979;&#25216;&#26415;&#65292;&#22312;&#22810;&#39033;&#36873;&#25321;&#39064;&#22238;&#31572;&#20219;&#21153;&#20013;&#20026;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#25105;&#20204;&#21457;&#29616;&#31526;&#21512;&#24615;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#19982;&#39044;&#27979;&#20934;&#30830;&#24615;&#23494;&#20999;&#30456;&#20851;&#12290;&#36825;&#31181;&#35266;&#23519;&#23545;&#20110;&#19979;&#28216;&#24212;&#29992;&#65292;&#22914;&#36873;&#25321;&#24615;&#20998;&#31867;&#21644;&#36807;&#28388;&#20302;&#36136;&#37327;&#39044;&#27979;&#65292;&#21487;&#33021;&#20250;&#26377;&#29992;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#31526;&#21512;&#24615;&#39044;&#27979;&#23545;&#20110;&#36229;&#20986;&#20027;&#39064;&#30340;&#38382;&#39064;&#30340;&#20132;&#25442;&#24615;&#20551;&#35774;&#65292;&#36825;&#21487;&#33021;&#26159;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#30340;&#26356;&#20026;&#29616;&#23454;&#30340;&#22330;&#26223;&#12290;&#26412;&#30740;&#31350;&#20026;&#22312;&#38656;&#35201;&#21487;&#38752;&#20445;&#35777;&#38169;&#35823;&#29575;&#30340;&#23433;&#20840;&#20851;&#38190;&#24773;&#20917;&#19979;&#26356;&#21152;&#20540;&#24471;&#20449;&#36182;&#21644;&#21487;&#38752;&#22320;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
As large language models continue to be widely developed, robust uncertainty quantification techniques will become crucial for their safe deployment in high-stakes scenarios. In this work, we explore how conformal prediction can be used to provide uncertainty quantification in language models for the specific task of multiple-choice question-answering. We find that the uncertainty estimates from conformal prediction are tightly correlated with prediction accuracy. This observation can be useful for downstream applications such as selective classification and filtering out low-quality predictions. We also investigate the exchangeability assumption required by conformal prediction to out-of-subject questions, which may be a more realistic scenario for many practical applications. Our work contributes towards more trustworthy and reliable usage of large language models in safety-critical situations, where robust guarantees of error rate are required.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LoRAPrune&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#39640;&#25928;&#24494;&#35843;&#21644;&#37096;&#32626;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#20302;&#31209;&#33258;&#36866;&#24212;&#30340;&#20540;&#21644;&#26799;&#24230;&#26469;&#35774;&#35745;PEFT&#24863;&#30693;&#30340;&#21098;&#26525;&#20934;&#21017;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36845;&#20195;&#21098;&#26525;&#36807;&#31243;&#26469;&#21435;&#38500;&#20887;&#20313;&#21442;&#25968;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#27169;&#22411;&#22823;&#23567;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#21516;&#26102;&#20445;&#25345;&#31454;&#20105;&#24615;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.18403</link><description>&lt;p&gt;
&#21098;&#26525;&#19982;&#20302;&#31209;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30456;&#36935;
&lt;/p&gt;
&lt;p&gt;
Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning. (arXiv:2305.18403v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LoRAPrune&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#39640;&#25928;&#24494;&#35843;&#21644;&#37096;&#32626;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#20302;&#31209;&#33258;&#36866;&#24212;&#30340;&#20540;&#21644;&#26799;&#24230;&#26469;&#35774;&#35745;PEFT&#24863;&#30693;&#30340;&#21098;&#26525;&#20934;&#21017;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36845;&#20195;&#21098;&#26525;&#36807;&#31243;&#26469;&#21435;&#38500;&#20887;&#20313;&#21442;&#25968;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#27169;&#22411;&#22823;&#23567;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#21516;&#26102;&#20445;&#25345;&#31454;&#20105;&#24615;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;LPM&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#34429;&#28982;&#20986;&#29616;&#20102;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26469;&#20415;&#23452;&#22320;&#24494;&#35843;&#36825;&#20123;&#22823;&#22411;&#27169;&#22411;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#65292;&#20294;&#23427;&#20204;&#30340;&#37096;&#32626;&#20173;&#28982;&#21463;&#21040;&#24040;&#22823;&#30340;&#27169;&#22411;&#35268;&#27169;&#21644;&#35745;&#31639;&#25104;&#26412;&#30340;&#21046;&#32422;&#12290;&#31070;&#32463;&#32593;&#32476;&#21098;&#26525;&#36890;&#36807;&#21024;&#38500;&#20887;&#20313;&#21442;&#25968;&#26469;&#25552;&#20379;&#27169;&#22411;&#21387;&#32553;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#35745;&#31639;&#21442;&#25968;&#26799;&#24230;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;LPM&#32780;&#35328;&#65292;&#33719;&#24471;&#26799;&#24230;&#26159;&#35745;&#31639;&#19978;&#31105;&#27490;&#30340;&#65292;&#36825;&#38656;&#35201;&#25506;&#32034;&#26367;&#20195;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;LPM&#39640;&#25928;&#24494;&#35843;&#21644;&#37096;&#32626;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#31216;&#20026;LoRAPrune&#12290;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#20010;PEFT&#24863;&#30693;&#30340;&#21098;&#26525;&#20934;&#21017;&#65292;&#35813;&#20934;&#21017;&#21033;&#29992;&#20102;&#20302;&#31209;&#33258;&#36866;&#24212;&#65288;LoRA&#65289;&#30340;&#20540;&#21644;&#26799;&#24230;&#65292;&#32780;&#19981;&#26159;&#39044;&#35757;&#32451;&#21442;&#25968;&#30340;&#26799;&#24230;&#36827;&#34892;&#37325;&#35201;&#24615;&#35780;&#20272;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36845;&#20195;&#21098;&#26525;&#36807;&#31243;&#26469;&#22522;&#20110;&#21098;&#26525;&#20934;&#21017;&#21435;&#38500;&#20887;&#20313;&#21442;&#25968;&#12290;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#27169;&#22411;&#22823;&#23567;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#21516;&#26102;&#20445;&#25345;&#31454;&#20105;&#24615;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large pre-trained models (LPMs), such as LLaMA and ViT-G, have shown exceptional performance across various tasks. Although parameter-efficient fine-tuning (PEFT) has emerged to cheaply fine-tune these large models on downstream tasks, their deployment is still hindered by the vast model scale and computational costs. Neural network pruning offers a solution for model compression by removing redundant parameters, but most existing methods rely on computing parameter gradients. However, obtaining the gradients is computationally prohibitive for LPMs, which necessitates the exploration of alternative approaches. To this end, we propose a unified framework for efficient fine-tuning and deployment of LPMs, termed LoRAPrune. We first design a PEFT-aware pruning criterion, which utilizes the values and gradients of Low-Rank Adaption (LoRA), rather than the gradients of pre-trained parameters for importance estimation. We then propose an iterative pruning procedure to remove redundant paramet
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35843;&#25972;&#21487;&#20449;&#32852;&#37030;&#23398;&#20064;&#20445;&#25252;&#26426;&#21046;&#30340;&#21442;&#25968;&#65292;&#20197;&#22312;&#38544;&#31169;&#27844;&#38706;&#12289;&#25928;&#29992;&#25439;&#22833;&#21644;&#25928;&#29575;&#38477;&#20302;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2305.18400</link><description>&lt;p&gt;
&#19968;&#31181;&#20803;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#35843;&#25972;&#21487;&#20449;&#32852;&#37030;&#23398;&#20064;&#20445;&#25252;&#26426;&#21046;&#30340;&#21442;&#25968;
&lt;/p&gt;
&lt;p&gt;
A Meta-learning Framework for Tuning Parameters of Protection Mechanisms in Trustworthy Federated Learning. (arXiv:2305.18400v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18400
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35843;&#25972;&#21487;&#20449;&#32852;&#37030;&#23398;&#20064;&#20445;&#25252;&#26426;&#21046;&#30340;&#21442;&#25968;&#65292;&#20197;&#22312;&#38544;&#31169;&#27844;&#38706;&#12289;&#25928;&#29992;&#25439;&#22833;&#21644;&#25928;&#29575;&#38477;&#20302;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#20449;&#32852;&#37030;&#23398;&#20064;&#65288;TFL&#65289;&#36890;&#24120;&#21033;&#29992;&#20445;&#25252;&#26426;&#21046;&#26469;&#20445;&#35777;&#38544;&#31169;&#23433;&#20840;&#12290;&#28982;&#32780;&#65292;&#20445;&#25252;&#26426;&#21046;&#19981;&#21487;&#36991;&#20813;&#22320;&#20250;&#24341;&#20837;&#25928;&#29992;&#25439;&#22833;&#25110;&#25928;&#29575;&#38477;&#20302;&#65292;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;&#22240;&#27492;&#65292;&#20445;&#25252;&#26426;&#21046;&#21450;&#20854;&#21442;&#25968;&#24212;&#35813;&#20180;&#32454;&#36873;&#25321;&#65292;&#20197;&#22312;&#20445;&#25252;&#38544;&#31169;&#27844;&#38706;&#12289;&#25928;&#29992;&#25439;&#22833;&#21644;&#25928;&#29575;&#38477;&#20302;&#20043;&#38388;&#21462;&#24471;&#26368;&#20339;&#24179;&#34913;&#12290;&#20026;&#27492;&#65292;&#32852;&#37030;&#23398;&#20064;&#20174;&#19994;&#32773;&#38656;&#35201;&#24037;&#20855;&#26469;&#34913;&#37327;&#36825;&#19977;&#20010;&#22240;&#32032;&#65292;&#24182;&#20248;&#21270;&#23427;&#20204;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#36873;&#25321;&#26368;&#36866;&#21512;&#25163;&#22836;&#24212;&#29992;&#30340;&#20445;&#25252;&#26426;&#21046;&#12290;&#22522;&#20110;&#36825;&#20010;&#35201;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23427;(1)&#23558;TFL&#23450;&#20041;&#20026;&#25214;&#21040;&#20445;&#25252;&#26426;&#21046;&#26469;&#20248;&#21270;&#38544;&#31169;&#27844;&#38706;&#12289;&#25928;&#29992;&#25439;&#22833;&#21644;&#25928;&#29575;&#38477;&#20302;&#19977;&#32773;&#20043;&#38388;&#30340;&#26435;&#34913;&#30340;&#38382;&#39064;&#65307;(2)&#27491;&#24335;&#23450;&#20041;&#20102;&#36825;&#19977;&#20010;&#22240;&#32032;&#30340;&#26377;&#30028;&#27979;&#37327;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20803;&#23398;&#20064;&#31639;&#27861;&#26469;&#36817;&#20284;&#35299;&#20915;&#27492;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Trustworthy Federated Learning (TFL) typically leverages protection mechanisms to guarantee privacy. However, protection mechanisms inevitably introduce utility loss or efficiency reduction while protecting data privacy. Therefore, protection mechanisms and their parameters should be carefully chosen to strike an optimal tradeoff between \textit{privacy leakage}, \textit{utility loss}, and \textit{efficiency reduction}. To this end, federated learning practitioners need tools to measure the three factors and optimize the tradeoff between them to choose the protection mechanism that is most appropriate to the application at hand. Motivated by this requirement, we propose a framework that (1) formulates TFL as a problem of finding a protection mechanism to optimize the tradeoff between privacy leakage, utility loss, and efficiency reduction and (2) formally defines bounded measurements of the three factors. We then propose a meta-learning algorithm to approximate this optimization proble
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#31526;&#21495;&#25216;&#26415;&#30340;&#21487;&#35270;&#21270;&#32534;&#31243;&#20219;&#21153;&#21512;&#25104;&#26041;&#27861;NeurTaskSyn&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#38024;&#23545;&#35268;&#33539;&#20013;&#32473;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#20195;&#30721;&#25152;&#38656;&#35201;&#30340;&#32534;&#31243;&#27010;&#24565;&#21644;&#23545;&#21487;&#35270;&#21270;&#20219;&#21153;&#30340;&#38480;&#21046;&#65292;&#33258;&#21160;&#29983;&#25104;&#32534;&#31243;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.18342</link><description>&lt;p&gt;
&#21487;&#35270;&#21270;&#32534;&#31243;&#20013;&#31070;&#32463;&#20219;&#21153;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Neural Task Synthesis for Visual Programming. (arXiv:2305.18342v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18342
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#31526;&#21495;&#25216;&#26415;&#30340;&#21487;&#35270;&#21270;&#32534;&#31243;&#20219;&#21153;&#21512;&#25104;&#26041;&#27861;NeurTaskSyn&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#38024;&#23545;&#35268;&#33539;&#20013;&#32473;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#20195;&#30721;&#25152;&#38656;&#35201;&#30340;&#32534;&#31243;&#27010;&#24565;&#21644;&#23545;&#21487;&#35270;&#21270;&#20219;&#21153;&#30340;&#38480;&#21046;&#65292;&#33258;&#21160;&#29983;&#25104;&#32534;&#31243;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21512;&#25104;&#26032;&#30340;&#20869;&#23481;&#65292;&#29983;&#25104;&#24335;&#31070;&#32463;&#27169;&#22411;&#22312;&#22686;&#24378;&#32534;&#31243;&#25945;&#32946;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#26088;&#22312;&#35774;&#35745;&#31070;&#32463;&#27169;&#22411;&#65292;&#33021;&#22815;&#26681;&#25454;&#21487;&#35270;&#21270;&#32534;&#31243;&#29615;&#22659;&#19979;&#32473;&#23450;&#30340;&#35268;&#33539;&#33258;&#21160;&#29983;&#25104;&#32534;&#31243;&#20219;&#21153;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#20687; GPT-4 &#36825;&#26679;&#30340;&#22823;&#22411;&#29983;&#25104;&#27169;&#22411;&#33719;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#25105;&#20204;&#30340;&#21021;&#27493;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#21512;&#25104;&#21487;&#35270;&#21270;&#32534;&#31243;&#20219;&#21153;&#26041;&#38754;&#25928;&#26524;&#19981;&#20339;&#65292;&#24182;&#19988;&#22312;&#36923;&#36753;&#21644;&#31354;&#38388;&#25512;&#29702;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#31526;&#21495;&#25216;&#26415; NeurTaskSyn&#65292;&#35813;&#25216;&#26415;&#33021;&#22815;&#38024;&#23545;&#35268;&#33539;&#20013;&#32473;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#20195;&#30721;&#25152;&#38656;&#35201;&#30340;&#32534;&#31243;&#27010;&#24565;&#21644;&#23545;&#21487;&#35270;&#21270;&#20219;&#21153;&#30340;&#38480;&#21046;&#65292;&#21512;&#25104;&#32534;&#31243;&#20219;&#21153;&#12290;NeurTaskSyn &#30001;&#20004;&#20010;&#37096;&#20998;&#26500;&#25104;&#65306;&#31532;&#19968;&#20010;&#37096;&#20998;&#36890;&#36807;&#27169;&#20223;&#23398;&#20064;&#31243;&#24207;&#36827;&#34892;&#35757;&#32451;&#65292;&#29983;&#25104;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#20195;&#30721;&#65292;&#31532;&#20108;&#20010;&#37096;&#20998;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#31243;&#24207;&#36827;&#34892;&#35757;&#32451;&#65292;&#25351;&#23548;&#24213;&#23618;&#31526;&#21495;&#25191;&#34892;&#24341;&#25806;&#29983;&#25104;&#21487;&#35270;&#21270;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative neural models hold great promise in enhancing programming education by synthesizing new content for students. We seek to design neural models that can automatically generate programming tasks for a given specification in the context of visual programming domains. Despite the recent successes of large generative models like GPT-4, our initial results show that these models are ineffective in synthesizing visual programming tasks and struggle with logical and spatial reasoning. We propose a novel neuro-symbolic technique, NeurTaskSyn, that can synthesize programming tasks for a specification given in the form of desired programming concepts exercised by its solution code and constraints on the visual task. NeurTaskSyn has two components: the first component is trained via imitation learning procedure to generate possible solution codes, and the second component is trained via reinforcement learning procedure to guide an underlying symbolic execution engine that generates visua
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;HyNT&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#36229;&#20851;&#31995;&#22411;&#30693;&#35782;&#22270;&#30340;&#34920;&#31034;&#65292;&#21253;&#25324;&#25968;&#20540;&#25991;&#23383;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#19978;&#19979;&#25991;Transformer&#21644;&#39044;&#27979;Transformer&#65292;&#36890;&#36807;&#23398;&#20064;&#19977;&#20803;&#32452;&#21644;&#20854;&#38480;&#23450;&#35789;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#20197;&#21450;&#25968;&#20540;&#20449;&#24687;&#26469;&#33719;&#24471;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.18256</link><description>&lt;p&gt;
&#29992;Transformer&#23398;&#20064;&#36229;&#20851;&#31995;&#22411;&#21644;&#25968;&#20540;&#30693;&#35782;&#22270;&#20013;&#30340;&#34920;&#24449;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Representation Learning on Hyper-Relational and Numeric Knowledge Graphs with Transformers. (arXiv:2305.18256v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;HyNT&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#36229;&#20851;&#31995;&#22411;&#30693;&#35782;&#22270;&#30340;&#34920;&#31034;&#65292;&#21253;&#25324;&#25968;&#20540;&#25991;&#23383;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#19978;&#19979;&#25991;Transformer&#21644;&#39044;&#27979;Transformer&#65292;&#36890;&#36807;&#23398;&#20064;&#19977;&#20803;&#32452;&#21644;&#20854;&#38480;&#23450;&#35789;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#20197;&#21450;&#25968;&#20540;&#20449;&#24687;&#26469;&#33719;&#24471;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30740;&#31350;&#20102;&#19968;&#20010;&#36229;&#20851;&#31995;&#22411;&#30693;&#35782;&#22270;&#35889;&#65292;&#20854;&#20013;&#19977;&#20803;&#32452;&#19982;&#38480;&#23450;&#35789;&#38598;&#21512;&#30456;&#20851;&#32852;; &#19968;&#20010;&#38480;&#23450;&#35789;&#30001;&#20851;&#31995;&#21644;&#23454;&#20307;&#32452;&#25104;&#65292;&#20026;&#19977;&#20803;&#32452;&#25552;&#20379;&#36741;&#21161;&#20449;&#24687;&#12290;&#29616;&#26377;&#30340;&#36229;&#20851;&#31995;&#22411;&#30693;&#35782;&#22270;&#23884;&#20837;&#26041;&#27861;&#20551;&#23450;&#23454;&#20307;&#26159;&#31163;&#25955;&#23545;&#35937;&#65292;&#20294;&#26377;&#20123;&#20449;&#24687;&#24212;&#20351;&#29992;&#25968;&#20540;&#34920;&#31034;&#65292;&#20363;&#22914;(J.R.R.&#65292;&#20986;&#29983;&#20110;&#65292;1892)&#12290;&#21516;&#26102;&#65292;&#19977;&#20803;&#32452;(J.R.R.&#65292;&#23601;&#35835;&#20110;&#65292;&#29275;&#27941;&#22823;&#23398;)&#21487;&#20197;&#19982;&#38480;&#23450;&#35789;(&#24320;&#22987;&#26102;&#38388;&#65292;1911)&#30456;&#20851;&#32852;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;HyNT&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#21253;&#21547;&#19977;&#20803;&#32452;&#25110;&#38480;&#23450;&#35789;&#20013;&#25968;&#20540;&#25991;&#23383;&#30340;&#36229;&#20851;&#31995;&#22411;&#30693;&#35782;&#22270;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;Transformer&#21644;&#19968;&#20010;&#39044;&#27979;Transformer&#65292;&#26469;&#23398;&#20064;&#34920;&#31034;&#65292;&#19981;&#20165;&#22522;&#20110;&#19977;&#20803;&#32452;&#21644;&#20854;&#38480;&#23450;&#35789;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#36824;&#22522;&#20110;&#25968;&#20540;&#20449;&#24687;&#12290;&#36890;&#36807;&#23398;&#20064;&#19977;&#20803;&#32452;&#21644;&#38480;&#23450;&#35789;&#30340;&#32039;&#20945;&#34920;&#31034;&#65292;&#24182;&#23558;&#23427;&#20204;&#39304;&#36865;&#32473;Transformer&#26469;&#33719;&#24471;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A hyper-relational knowledge graph has been recently studied where a triplet is associated with a set of qualifiers; a qualifier is composed of a relation and an entity, providing auxiliary information for a triplet. While existing hyper-relational knowledge graph embedding methods assume that the entities are discrete objects, some information should be represented using numeric values, e.g., (J.R.R., was born in, 1892). Also, a triplet (J.R.R., educated at, Oxford Univ.) can be associated with a qualifier such as (start time, 1911). In this paper, we propose a unified framework named HyNT that learns representations of a hyper-relational knowledge graph containing numeric literals in either triplets or qualifiers. We define a context transformer and a prediction transformer to learn the representations based not only on the correlations between a triplet and its qualifiers but also on the numeric information. By learning compact representations of triplets and qualifiers and feeding 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#30142;&#30149;&#24739;&#32773;&#20010;&#20307;&#30340;&#26681;&#26412;&#21407;&#22240;&#30340;&#26032;&#20844;&#24335;&#65292;&#21487;&#20197;&#29992;&#20110;&#33258;&#21160;&#20174;&#25968;&#25454;&#20013;&#26816;&#27979;&#26681;&#26412;&#21407;&#22240;&#65292;&#24182;&#32771;&#34385;&#20102;&#22122;&#22768;&#26631;&#31614;&#21644;&#30142;&#30149;&#27969;&#34892;&#29575;&#31561;&#22240;&#32032;&#65292;&#21516;&#26102;&#20855;&#26377;&#24555;&#36895;&#35745;&#31639;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.17574</link><description>&lt;p&gt;
&#30142;&#30149;&#24739;&#32773;&#20010;&#20307;&#26681;&#26412;&#21407;&#22240;&#30340;&#21453;&#20107;&#23454;&#20844;&#24335;&#21270;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Formulation of Patient-Specific Root Causes of Disease. (arXiv:2305.17574v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17574
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#30142;&#30149;&#24739;&#32773;&#20010;&#20307;&#30340;&#26681;&#26412;&#21407;&#22240;&#30340;&#26032;&#20844;&#24335;&#65292;&#21487;&#20197;&#29992;&#20110;&#33258;&#21160;&#20174;&#25968;&#25454;&#20013;&#26816;&#27979;&#26681;&#26412;&#21407;&#22240;&#65292;&#24182;&#32771;&#34385;&#20102;&#22122;&#22768;&#26631;&#31614;&#21644;&#30142;&#30149;&#27969;&#34892;&#29575;&#31561;&#22240;&#32032;&#65292;&#21516;&#26102;&#20855;&#26377;&#24555;&#36895;&#35745;&#31639;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30142;&#30149;&#30340;&#26681;&#26412;&#21407;&#22240;&#30452;&#35266;&#22320;&#23545;&#24212;&#20110;&#22686;&#21152;&#35786;&#26029;&#21487;&#33021;&#24615;&#30340;&#26681;&#26412;&#39030;&#28857;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26681;&#26412;&#21407;&#22240;&#30340;&#25551;&#36848;&#32570;&#20047;&#35745;&#31639;&#26426;&#31639;&#27861;&#21457;&#23637;&#25152;&#38656;&#30340;&#20005;&#26684;&#25968;&#23398;&#20844;&#24335;&#12290;&#22312;&#20197;&#21069;&#30340;&#24037;&#20316;&#20013;&#65292;&#20351;&#29992;&#24178;&#39044;&#20027;&#20041;&#32773;&#24080;&#25143;&#23450;&#20041;&#20102;&#30142;&#30149;&#30340;&#30149;&#20154;&#29305;&#23450;&#26681;&#26412;&#21407;&#22240;&#65292;&#35813;&#24080;&#25143;&#20165;&#25856;&#21319;&#21040;&#29645;&#29664;&#30340;&#22240;&#26524;Ladder&#30340;&#31532;&#20108;&#23618;&#12290;&#22312;&#36825;&#20010;&#29702;&#35770;&#24615;&#30340;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#21453;&#20107;&#23454;&#30340;&#23450;&#20041;&#26469;&#25856;&#21319;&#21040;&#31532;&#19977;&#23618;&#65292;&#20197;&#21305;&#37197;&#22522;&#20110;&#22266;&#23450;&#20107;&#23454;&#25968;&#25454;&#30340;&#20020;&#24202;&#30452;&#35273;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#30340;Shapley&#20540;&#20026;&#27599;&#20010;&#21464;&#37327;&#20998;&#37197;&#26681;&#22240;&#36129;&#29486;&#24471;&#20998;&#12290;&#25552;&#20986;&#30340;&#30142;&#30149;&#24739;&#32773;&#20010;&#20307;&#26681;&#26412;&#21407;&#22240;&#30340;&#21453;&#20107;&#23454;&#20844;&#24335;&#21270;&#32771;&#34385;&#20102;&#22122;&#22768;&#26631;&#31614;&#65292;&#36866;&#24212;&#20102;&#30142;&#30149;&#30340;&#27969;&#34892;&#29575;&#65292;&#24182;&#20801;&#35768;&#24555;&#36895;&#35745;&#31639;&#65292;&#26080;&#38656;&#21453;&#20107;&#23454;&#27169;&#25311;&#12290;
&lt;/p&gt;
&lt;p&gt;
Root causes of disease intuitively correspond to root vertices that increase the likelihood of a diagnosis. This description of a root cause nevertheless lacks the rigorous mathematical formulation needed for the development of computer algorithms designed to automatically detect root causes from data. Prior work defined patient-specific root causes of disease using an interventionalist account that only climbs to the second rung of Pearl's Ladder of Causation. In this theoretical piece, we climb to the third rung by proposing a counterfactual definition matching clinical intuition based on fixed factual data alone. We then show how to assign a root causal contribution score to each variable using Shapley values from explainable artificial intelligence. The proposed counterfactual formulation of patient-specific root causes of disease accounts for noisy labels, adapts to disease prevalence and admits fast computation without the need for counterfactual simulation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#31526;&#21512;&#39044;&#27979;&#25512;&#24191;&#21040;&#32852;&#37030;&#23398;&#20064;&#35774;&#32622;&#65292;&#25552;&#20986;&#32852;&#37030;&#31526;&#21512;&#39044;&#27979;&#65288;FCP&#65289;&#26694;&#26550;&#65292;&#22788;&#29702;&#20102;&#25968;&#25454;&#38598;&#24322;&#36136;&#24615;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;FCP&#20855;&#26377;&#20005;&#26684;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#22312;&#20998;&#24067;&#24335;&#21644;&#24322;&#36136;&#29615;&#22659;&#20013;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2305.17564</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#32852;&#37030;&#31526;&#21512;&#39044;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Federated Conformal Predictors for Distributed Uncertainty Quantification. (arXiv:2305.17564v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#31526;&#21512;&#39044;&#27979;&#25512;&#24191;&#21040;&#32852;&#37030;&#23398;&#20064;&#35774;&#32622;&#65292;&#25552;&#20986;&#32852;&#37030;&#31526;&#21512;&#39044;&#27979;&#65288;FCP&#65289;&#26694;&#26550;&#65292;&#22788;&#29702;&#20102;&#25968;&#25454;&#38598;&#24322;&#36136;&#24615;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;FCP&#20855;&#26377;&#20005;&#26684;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#22312;&#20998;&#24067;&#24335;&#21644;&#24322;&#36136;&#29615;&#22659;&#20013;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#21487;&#20197;&#36731;&#26494;&#24212;&#29992;&#20026;&#21518;&#22788;&#29702;&#27493;&#39588;&#20197;&#24050;&#32463;&#35757;&#32451;&#36807;&#30340;&#27169;&#22411;&#20013;&#65292;&#31526;&#21512;&#39044;&#27979;&#36880;&#28176;&#25104;&#20026;&#25552;&#20379;&#26426;&#22120;&#23398;&#20064;&#20013;&#20005;&#26684;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#27969;&#34892;&#33539;&#24335;&#12290;&#26412;&#25991;&#23558;&#31526;&#21512;&#39044;&#27979;&#25512;&#24191;&#21040;&#32852;&#37030;&#23398;&#20064;&#35774;&#32622;&#12290;&#25105;&#20204;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#65292;&#36825;&#36829;&#21453;&#20102;&#31526;&#21512;&#39044;&#27979;&#25152;&#38656;&#30340;&#20132;&#25442;&#24615;&#30340;&#22522;&#26412;&#21407;&#21017;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36739;&#24369;&#30340;&#37096;&#20998;&#20132;&#25442;&#24615;&#27010;&#24565;&#65292;&#26356;&#36866;&#21512;&#20110;&#27492;&#31181;&#24773;&#20917;&#65292;&#24182;&#20351;&#29992;&#23427;&#26469;&#24320;&#21457;&#32852;&#37030;&#31526;&#21512;&#39044;&#27979;&#65288;FCP&#65289;&#26694;&#26550;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;FCP&#20855;&#26377;&#20005;&#26684;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#22312;&#20960;&#20010;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#21307;&#23398;&#25104;&#20687;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#20248;&#31168;&#30340;&#32463;&#39564;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#23637;&#31034;&#20102;&#22312;&#20998;&#24067;&#24335;&#21644;&#24322;&#36136;&#29615;&#22659;&#20013;&#34701;&#20837;&#26377;&#24847;&#20041;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#23454;&#29992;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25105;&#20204;&#23454;&#39564;&#20013;&#20351;&#29992;&#30340;&#20195;&#30721; https://github.com/clu5/federated-conformal&#12290;
&lt;/p&gt;
&lt;p&gt;
Conformal prediction is emerging as a popular paradigm for providing rigorous uncertainty quantification in machine learning since it can be easily applied as a post-processing step to already trained models. In this paper, we extend conformal prediction to the federated learning setting. The main challenge we face is data heterogeneity across the clients - this violates the fundamental tenet of exchangeability required for conformal prediction. We propose a weaker notion of partial exchangeability, better suited to the FL setting, and use it to develop the Federated Conformal Prediction (FCP) framework. We show FCP enjoys rigorous theoretical guarantees and excellent empirical performance on several computer vision and medical imaging datasets. Our results demonstrate a practical approach to incorporating meaningful uncertainty quantification in distributed and heterogeneous environments. We provide code used in our experiments https://github.com/clu5/federated-conformal.
&lt;/p&gt;</description></item><item><title>Translatotron 3&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#21333;&#35821;&#25968;&#25454;&#36827;&#34892;&#35821;&#38899;&#21040;&#35821;&#38899;&#32763;&#35793;&#65292;&#26080;&#38656;&#37197;&#23545;&#30340;&#25968;&#25454;&#25110;&#19987;&#19994;&#24314;&#27169;&#65292;&#23637;&#31034;&#20102;&#20445;&#30041;&#35821;&#35328;/&#38750;&#35821;&#35328;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.17547</link><description>&lt;p&gt;
Translatotron 3: &#20351;&#29992;&#21333;&#35821;&#25968;&#25454;&#36827;&#34892;&#35821;&#38899;&#21040;&#35821;&#38899;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Translatotron 3: Speech to Speech Translation with Monolingual Data. (arXiv:2305.17547v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17547
&lt;/p&gt;
&lt;p&gt;
Translatotron 3&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#21333;&#35821;&#25968;&#25454;&#36827;&#34892;&#35821;&#38899;&#21040;&#35821;&#38899;&#32763;&#35793;&#65292;&#26080;&#38656;&#37197;&#23545;&#30340;&#25968;&#25454;&#25110;&#19987;&#19994;&#24314;&#27169;&#65292;&#23637;&#31034;&#20102;&#20445;&#30041;&#35821;&#35328;/&#38750;&#35821;&#35328;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Translatotron 3&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#12289;&#26080;&#30417;&#30563;&#30340;&#23884;&#20837;&#26144;&#23556;&#21644;&#22238;&#35793;&#23558;&#30452;&#25509;&#35821;&#38899;&#21040;&#35821;&#38899;&#32763;&#35793;&#27169;&#22411;&#20174;&#21333;&#22768;&#36947;&#35821;&#38899;-&#25991;&#26412;&#25968;&#25454;&#38598;&#20013;&#23436;&#20840;&#26080;&#30417;&#30563;&#22320;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#35199;&#29677;&#29273;&#35821;&#21644;&#33521;&#35821;&#20043;&#38388;&#30340;&#35821;&#38899;&#21040;&#35821;&#38899;&#32763;&#35793;&#20219;&#21153;&#20013;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Translatotron 3&#20248;&#20110;&#22522;&#20934;&#32423;&#32852;&#31995;&#32479;&#65292;&#22312; synthesized Unpaired-Conversational &#25968;&#25454;&#38598;&#19978;&#25253;&#21578;&#20102;18.14 BLEU&#20998;&#25968;&#30340;&#25552;&#39640;&#12290;&#19982;&#38656;&#35201;&#30495;&#23454;&#37197;&#23545;&#25968;&#25454;&#25110;&#19987;&#19994;&#24314;&#27169;&#26469;&#22797;&#21046;&#35821;&#35328;/&#38750;&#35821;&#35328;&#20449;&#24687;&#30340;&#30417;&#30563;&#26041;&#27861;&#19981;&#21516;&#65292;Translatotron 3&#23637;&#31034;&#20102;&#23427;&#20445;&#30041;&#20102;&#20687;&#26242;&#20572;&#12289;&#35828;&#35805;&#36895;&#24230;&#21644;&#35828;&#35805;&#20154;&#36523;&#20221;&#31561;&#35821;&#35328;/&#38750;&#35821;&#35328;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents Translatotron 3, a novel approach to train a direct speech-to-speech translation model from monolingual speech-text datasets only in a fully unsupervised manner. Translatotron 3 combines masked autoencoder, unsupervised embedding mapping, and back-translation to achieve this goal. Experimental results in speech-to-speech translation tasks between Spanish and English show that Translatotron 3 outperforms a baseline cascade system, reporting 18.14 BLEU points improvement on the synthesized Unpaired-Conversational dataset. In contrast to supervised approaches that necessitate real paired data, which is unavailable, or specialized modeling to replicate para-/non-linguistic information, Translatotron 3 showcases its capability to retain para-/non-linguistic such as pauses, speaking rates, and speaker identity. Audio samples can be found in our website this http URL
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#21160;&#24577;&#31283;&#23450;&#24615;&#38544;&#24335;&#27491;&#21017;&#21270;&#65292;&#35777;&#26126;&#20102;&#20854;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.17490</link><description>&lt;p&gt;
&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#20013;&#21160;&#24577;&#31283;&#23450;&#24615;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
The Implicit Regularization of Dynamical Stability in Stochastic Gradient Descent. (arXiv:2305.17490v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17490
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#21160;&#24577;&#31283;&#23450;&#24615;&#38544;&#24335;&#27491;&#21017;&#21270;&#65292;&#35777;&#26126;&#20102;&#20854;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#8220;&#21160;&#24577;&#31283;&#23450;&#24615;&#8221;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#12290;&#25105;&#20204;&#39318;&#20808;&#20462;&#27491;&#20102;&#29616;&#26377;SGD&#31283;&#23450;&#24615;&#20998;&#26512;&#30340;&#38382;&#39064;&#65292;&#23637;&#31034;&#20102;Hessian&#30697;&#38453;&#30340;Frobenius&#33539;&#25968;&#21644;&#36857;&#19982;&#19981;&#21516;&#31283;&#23450;&#24615;&#27010;&#24565;&#30340;&#20851;&#31995;&#12290;&#29305;&#21035;&#22320;&#65292;&#22914;&#26524;&#20840;&#23616;&#26368;&#23567;&#20540;&#22312;SGD&#20013;&#26159;&#32447;&#24615;&#31283;&#23450;&#30340;&#65292;&#21017;Hessian&#30697;&#38453;&#30340;&#36857;&#24517;&#39035;&#23567;&#20110;&#25110;&#31561;&#20110;$2/\eta$&#65292;&#20854;&#20013;$\eta$&#34920;&#31034;&#23398;&#20064;&#29575;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#65292;&#31283;&#23450;&#24615;&#21482;&#23545;Hessian&#30697;&#38453;&#30340;&#26368;&#22823;&#29305;&#24449;&#20540;&#26045;&#21152;&#31867;&#20284;&#30340;&#32422;&#26463;&#12290;&#25105;&#20204;&#25509;&#30528;&#20998;&#26512;&#20102;&#36825;&#20123;&#31283;&#23450;&#26497;&#20540;&#30340;&#27867;&#21270;&#24615;&#36136;&#65292;&#30528;&#37325;&#20851;&#27880;&#20102;&#20004;&#23618;ReLU&#32593;&#32476;&#21644;&#23545;&#35282;&#32447;&#24615;&#32593;&#32476;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#36825;&#20004;&#20010;&#27169;&#22411;&#30340;&#23574;&#38160;&#24230;&#24230;&#37327;&#21644;&#26576;&#20123;&#21442;&#25968;&#35268;&#33539;&#20043;&#38388;&#30340;&#8220;&#31561;&#20215;&#24615;&#8221;&#65292;&#20174;&#32780;&#35777;&#26126;&#20102;SGD&#30340;&#31283;&#23450;&#26497;&#20540;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#12290;&#28982;&#32780;&#65292;GD&#30340;&#31283;&#23450;&#24615;&#27491;&#21017;&#21270;&#21482;&#22312;&#29305;&#23450;&#24773;&#20917;&#19979;&#20135;&#29983;&#27867;&#21270;&#25928;&#30410;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#29702;&#35770;&#24212;&#29992;&#20110;&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#38382;&#39064;&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#23545;&#26576;&#20123;&#27169;&#22411;&#30340;&#34920;&#29616;&#20248;&#20110;Lasso&#25110;&#23725;&#27491;&#21017;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the implicit regularization of stochastic gradient descent (SGD) through the lens of {\em dynamical stability} (Wu et al., 2018). We start by revising existing stability analyses of SGD, showing how the Frobenius norm and trace of Hessian relate to different notions of stability. Notably, if a global minimum is linearly stable for SGD, then the trace of Hessian must be less than or equal to $2/\eta$, where $\eta$ denotes the learning rate. By contrast, for gradient descent (GD), the stability imposes a similar constraint but only on the largest eigenvalue of Hessian. We then turn to analyze the generalization properties of these stable minima, focusing specifically on two-layer ReLU networks and diagonal linear networks. Notably, we establish the {\em equivalence} between these metrics of sharpness and certain parameter norms for the two models, which allows us to show that the stable minima of SGD provably generalize well. By contrast, the stability-induced reg
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#27010;&#25324;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#31867;&#22411;&#21644;&#24212;&#29992;&#65292;&#27604;&#36739;&#20998;&#26512;&#20102;&#21508;&#20010;&#27169;&#22411;&#30340;&#32467;&#26500;&#12289;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#65292;&#26377;&#21161;&#20110;&#36873;&#25321;&#21644;&#35774;&#35745;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.17473</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#27010;&#36848;&#19982;&#27604;&#36739;&#20998;&#26512;&#65306;CNN&#12289;RNN&#12289;LSTM&#12289;GRU&#12290;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Overview and Comparative Analysis on Deep Learning Models: CNN, RNN, LSTM, GRU. (arXiv:2305.17473v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17473
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#27010;&#25324;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#31867;&#22411;&#21644;&#24212;&#29992;&#65292;&#27604;&#36739;&#20998;&#26512;&#20102;&#21508;&#20010;&#27169;&#22411;&#30340;&#32467;&#26500;&#12289;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#65292;&#26377;&#21161;&#20110;&#36873;&#25321;&#21644;&#35774;&#35745;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#26159;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#24378;&#22823;&#23376;&#38598;&#65292;&#29305;&#21035;&#22312;&#22788;&#29702;&#38750;&#32467;&#26500;&#21270;&#21644;&#22823;&#22411;&#25968;&#25454;&#38598;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;ML&#26041;&#27861;&#12290;&#20854;&#24433;&#21709;&#36328;&#36234;&#21508;&#20010;&#39046;&#22495;&#65292;&#21253;&#25324;&#35821;&#38899;&#35782;&#21035;&#12289;&#21307;&#30103;&#20445;&#20581;&#12289;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#12289;&#32593;&#32476;&#23433;&#20840;&#12289;&#39044;&#27979;&#20998;&#26512;&#31561;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#21644;&#21160;&#24577;&#24615;&#32473;&#35774;&#35745;&#26377;&#25928;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#20154;&#20204;&#24320;&#21457;&#20986;&#20102;&#20960;&#31181;&#19981;&#21516;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#35299;&#20915;&#19981;&#21516;&#30340;&#38382;&#39064;&#21644;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#21253;&#25324;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#12289;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#12289;&#29983;&#25104;&#27169;&#22411;&#12289;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#21644;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#12290;&#25105;&#20204;&#32771;&#23519;&#20102;&#27599;&#20010;&#27169;&#22411;&#30340;&#32467;&#26500;&#12289;&#24212;&#29992;&#12289;&#22909;&#22788;&#21644;&#23616;&#38480;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19977;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning (DL) has emerged as a powerful subset of machine learning (ML) and artificial intelligence (AI), outperforming traditional ML methods, especially in handling unstructured and large datasets. Its impact spans across various domains, including speech recognition, healthcare, autonomous vehicles, cybersecurity, predictive analytics, and more. However, the complexity and dynamic nature of real-world problems present challenges in designing effective deep learning models. Consequently, several deep learning models have been developed to address different problems and applications. In this article, we conduct a comprehensive survey of various deep learning models, including Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), Generative Models, Deep Reinforcement Learning (DRL), and Deep Transfer Learning. We examine the structure, applications, benefits, and limitations of each model. Furthermore, we perform an analysis using three publicly available dataset
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#26234;&#33021;&#20013;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#27880;&#24847;&#21147;&#27169;&#24335;&#29702;&#35770;&#65288;AST&#65289;&#12290;&#20316;&#32773;&#21457;&#29616;&#23558;AS&#23454;&#29616;&#20026;&#19968;&#31181;&#24490;&#29615;&#20869;&#37096;&#25511;&#21046;&#30340;&#26234;&#33021;&#20307;&#25928;&#26524;&#26368;&#20339;&#65292;&#36825;&#19968;&#29702;&#35770;&#20026;&#24212;&#29992;&#19982;&#25913;&#36827;&#31070;&#32463;&#26234;&#33021;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2305.17375</link><description>&lt;p&gt;
&#31070;&#32463;&#26234;&#33021;&#20013;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Attention Schema in Neural Agents. (arXiv:2305.17375v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17375
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#26234;&#33021;&#20013;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#27880;&#24847;&#21147;&#27169;&#24335;&#29702;&#35770;&#65288;AST&#65289;&#12290;&#20316;&#32773;&#21457;&#29616;&#23558;AS&#23454;&#29616;&#20026;&#19968;&#31181;&#24490;&#29615;&#20869;&#37096;&#25511;&#21046;&#30340;&#26234;&#33021;&#20307;&#25928;&#26524;&#26368;&#20339;&#65292;&#36825;&#19968;&#29702;&#35770;&#20026;&#24212;&#29992;&#19982;&#25913;&#36827;&#31070;&#32463;&#26234;&#33021;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#21147;&#24050;&#32463;&#25104;&#20026;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#20013;&#30340;&#24120;&#35265;&#35201;&#32032;&#12290;&#23427;&#36890;&#36807;&#21152;&#20837;&#21160;&#24577;&#20449;&#24687;&#36873;&#25321;&#65292;&#25903;&#25345;&#38745;&#24577;&#30340;&#26435;&#37325;&#36873;&#25321;&#12290;&#21516;&#26679;&#22320;&#65292;&#25105;&#20204;&#21487;&#20197;&#24819;&#35937;&#22312;&#27880;&#24847;&#21147;&#20043;&#19978;&#26500;&#24314;&#19968;&#20010;&#26356;&#39640;&#38454;&#30340;&#20449;&#24687;&#36807;&#28388;&#22120;&#65306;&#27880;&#24847;&#21147;&#27169;&#24335;&#65288;AS&#65289;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#19968;&#20010;&#25551;&#36848;&#24615;&#21644;&#39044;&#27979;&#24615;&#30340;&#27880;&#24847;&#21147;&#27169;&#22411;&#12290;&#22312;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#20013;&#65292;&#27880;&#24847;&#21147;&#27169;&#24335;&#29702;&#35770;&#65288;AST&#65289;&#25903;&#25345;&#36825;&#31181;&#21306;&#20998;&#27880;&#24847;&#21147;&#21644;AS&#30340;&#24819;&#27861;&#12290;&#35813;&#29702;&#35770;&#30340;&#19968;&#20010;&#37325;&#35201;&#39044;&#27979;&#26159;&#65292;&#19968;&#20010;&#26234;&#33021;&#20307;&#21487;&#20197;&#20351;&#29992;&#33258;&#24049;&#30340;AS&#26469;&#25512;&#26029;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#27880;&#24847;&#29366;&#24577;&#65292;&#20174;&#32780;&#22686;&#24378;&#19982;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#21327;&#35843;&#12290;&#22240;&#27492;&#65292;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#20010;&#23454;&#39564;&#27979;&#35797;AST&#26377;&#25928;&#24615;&#30340;&#29702;&#24819;&#22330;&#26223;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#27880;&#24847;&#21147;&#21644;AS&#30456;&#20114;&#20316;&#29992;&#30340;&#19981;&#21516;&#26041;&#24335;&#65292;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;AS&#23454;&#29616;&#20026;&#19968;&#31181;&#24490;&#29615;&#20869;&#37096;&#25511;&#21046;&#30340;&#26234;&#33021;&#20307;&#25928;&#26524;&#26368;&#20339;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#36825;&#20123;&#23454;&#39564;&#20026;&#29702;&#35299;&#20197;&#21450;&#25913;&#36827;&#31070;&#32463;&#26234;&#33021;&#20013;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention has become a common ingredient in deep learning architectures. It adds a dynamical selection of information on top of the static selection of information supported by weights. In the same way, we can imagine a higher-order informational filter built on top of attention: an Attention Schema (AS), namely, a descriptive and predictive model of attention. In cognitive neuroscience, Attention Schema Theory (AST) supports this idea of distinguishing attention from AS. A strong prediction of this theory is that an agent can use its own AS to also infer the states of other agents' attention and consequently enhance coordination with other agents. As such, multi-agent reinforcement learning would be an ideal setting to experimentally test the validity of AST. We explore different ways in which attention and AS interact with each other. Our preliminary results indicate that agents that implement the AS as a recurrent internal control achieve the best performance. In general, these expl
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#21487;&#20197;&#23558;&#40657;&#30418;&#27169;&#22411;&#36716;&#21270;&#25104;&#20026;&#21487;&#35299;&#37322;&#27169;&#22411;&#24182;&#22312;&#30446;&#26631;&#39046;&#22495;&#25104;&#21151;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#36801;&#31227;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2305.17303</link><description>&lt;p&gt;
&#20174;&#40657;&#30418;&#27169;&#22411;&#21040;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#36716;&#21270;&#65292;&#29992;&#20110;&#39640;&#25928;&#30340;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Distilling BlackBox to Interpretable models for Efficient Transfer Learning. (arXiv:2305.17303v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17303
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#21487;&#20197;&#23558;&#40657;&#30418;&#27169;&#22411;&#36716;&#21270;&#25104;&#20026;&#21487;&#35299;&#37322;&#27169;&#22411;&#24182;&#22312;&#30446;&#26631;&#39046;&#22495;&#25104;&#21151;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#36801;&#31227;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31435;&#20855;&#26377;&#26222;&#36866;&#24615;&#30340;AI&#27169;&#22411;&#26159;&#21307;&#30103;&#39046;&#22495;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#12290;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#27169;&#22411;&#21363;&#20351;&#36755;&#20837;&#20998;&#24067;&#36731;&#24494;&#31227;&#20301;&#65288;&#20363;&#22914;&#25195;&#25551;&#20202;&#31867;&#22411;&#65289;&#65292;&#20063;&#20250;&#21463;&#21040;&#24433;&#21709;&#65292;&#32780;&#25918;&#23556;&#31185;&#21307;&#29983;&#21017;&#20381;&#36182;&#20110;&#24322;&#24120;&#24615;&#30340;&#36890;&#29992;&#25551;&#36848;&#24615;&#35268;&#21017;&#12290;&#24494;&#35843;&#27169;&#22411;&#20197;&#23558;&#30693;&#35782;&#20174;&#19968;&#20010;&#39046;&#22495;&#36716;&#31227;&#21040;&#21478;&#19968;&#20010;&#39046;&#22495;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#22312;&#35745;&#31639;&#25104;&#26412;&#26368;&#23567;&#30340;&#24773;&#20917;&#19979;&#65292;&#39640;&#25928;&#22320;&#38024;&#23545;&#26410;&#30693;&#30340;&#30446;&#26631;&#22495;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#35748;&#20026;NN&#30340;&#21487;&#35299;&#37322;&#32452;&#20214;&#22823;&#33268;&#26159;&#22495;&#19981;&#21464;&#30340;&#12290;&#28982;&#32780;&#65292;&#21487;&#35299;&#37322;&#27169;&#22411;&#36890;&#24120;&#34920;&#29616;&#19981;&#21450;&#23427;&#20204;&#30340;BB&#21464;&#20307;&#12290;&#22312;&#28304;&#22495;&#20013;&#25105;&#20204;&#20808;&#20351;&#29992;&#20154;&#31867;&#29702;&#35299;&#30340;&#27010;&#24565;&#20174;BB&#24320;&#22987;&#65292;&#23558;&#20854;&#25552;&#28860;&#25104;&#19968;&#32452;&#27973;&#26174;&#26131;&#25026;&#30340;interpretable&#27169;&#22411;&#12290;&#30001;&#20110;&#27599;&#20010;interpretable&#27169;&#22411;&#37117;&#35206;&#30422;&#20102;&#25968;&#25454;&#30340;&#19968;&#20010;&#23376;&#38598;&#65292;&#20855;&#26377;&#19968;&#32452;interpretable&#27169;&#22411;&#30340;&#28151;&#21512;&#21487;&#20197;&#23454;&#29616;&#19982;BB&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building generalizable AI models is one of the primary challenges in the healthcare domain. While radiologists rely on generalizable descriptive rules of abnormality, Neural Network (NN) models suffer even with a slight shift in input distribution (\eg scanner type). Fine-tuning a model to transfer knowledge from one domain to another requires a significant amount of labeled data in the target domain. In this paper, we develop an interpretable model that can be efficiently fine-tuned to an unseen target domain with minimal computational cost. We assume the interpretable component of NN to be approximately domain-invariant. However, interpretable models typically underperform compared to their Blackbox (BB) variants. We start with a BB in the source domain and distill it into a \emph{mixture} of shallow interpretable models using human-understandable concepts. As each interpretable model covers a subset of data, a mixture of interpretable models achieves comparable performance as BB. Fu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#23545;&#40784;&#35780;&#20272;&#26041;&#27861;A2EHV&#65292;&#37319;&#29992;&#24322;&#36136;&#20215;&#20540;&#31995;&#32479;&#65292;&#24182;&#22522;&#20110;&#20215;&#20540;&#21512;&#29702;&#24615;&#21644;&#31038;&#20250;&#20215;&#20540;&#23450;&#21521;&#26694;&#26550;&#35780;&#20272;&#20195;&#29702;&#20154;&#34892;&#20026;&#30340;&#31038;&#20250;&#20559;&#22909;&#65292;&#32467;&#26524;&#34920;&#26126;&#27604;&#20256;&#32479;&#23545;&#40784;&#26041;&#27861;&#26356;&#21512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2305.17147</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24322;&#36136;&#20215;&#20540;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous Value Evaluation for Large Language Models. (arXiv:2305.17147v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17147
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#23545;&#40784;&#35780;&#20272;&#26041;&#27861;A2EHV&#65292;&#37319;&#29992;&#24322;&#36136;&#20215;&#20540;&#31995;&#32479;&#65292;&#24182;&#22522;&#20110;&#20215;&#20540;&#21512;&#29702;&#24615;&#21644;&#31038;&#20250;&#20215;&#20540;&#23450;&#21521;&#26694;&#26550;&#35780;&#20272;&#20195;&#29702;&#20154;&#34892;&#20026;&#30340;&#31038;&#20250;&#20559;&#22909;&#65292;&#32467;&#26524;&#34920;&#26126;&#27604;&#20256;&#32479;&#23545;&#40784;&#26041;&#27861;&#26356;&#21512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#20351;&#24471;&#23558;&#23427;&#20204;&#30340;&#20215;&#20540;&#19982;&#20154;&#31867;&#20215;&#20540;&#23545;&#40784;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#23581;&#35797;&#23558;&#20854;&#19982;&#19968;&#31181;&#21516;&#36136;&#30340;&#20154;&#31867;&#20215;&#20540;&#23545;&#40784;&#65292;&#24182;&#38656;&#35201;&#20154;&#31867;&#39564;&#35777;&#65292;&#20294;&#32570;&#20047;&#23545;&#23545;&#40784;&#25152;&#38656;&#26041;&#38754;&#21644;&#28145;&#24230;&#30340;&#20849;&#35782;&#20197;&#21450;&#36896;&#25104;&#30340;&#20154;&#31867;&#20559;&#35265;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#23545;&#40784;&#35780;&#20272;&#26041;&#27861;A2EHV&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#24322;&#36136;&#20215;&#20540;&#31995;&#32479;&#65292;&#65288;1&#65289;&#26159;&#33258;&#21160;&#21270;&#30340;&#65292;&#20197;&#26368;&#23567;&#21270;&#21333;&#20010;&#20154;&#31867;&#20559;&#35265;&#65292;&#24182;&#19988;&#65288;2&#65289;&#20801;&#35768;&#35780;&#20272;&#38024;&#23545;&#21508;&#31181;&#30446;&#26631;&#20540;&#30340;&#24322;&#36136;&#20195;&#29702;&#20154;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#20215;&#20540;&#21512;&#29702;&#24615;&#30340;&#27010;&#24565;&#65292;&#23427;&#20195;&#34920;&#20102;&#20195;&#29702;&#20154;&#25191;&#34892;&#26368;&#33021;&#28385;&#36275;&#30446;&#26631;&#20215;&#20540;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;&#20215;&#20540;&#21512;&#29702;&#24615;&#30340;&#37327;&#21270;&#26159;&#36890;&#36807;&#31038;&#20250;&#24515;&#29702;&#23398;&#20013;&#30340;&#31038;&#20250;&#20215;&#20540;&#23450;&#21521;&#26694;&#26550;&#36827;&#34892;&#30340;&#65292;&#35813;&#26694;&#26550;&#23558;&#20215;&#20540;&#31354;&#38388;&#20998;&#20026;&#22235;&#20010;&#31867;&#21035;&#65292;&#20197;&#35780;&#20272;&#20195;&#29702;&#20154;&#34892;&#20026;&#30340;&#31038;&#20250;&#20559;&#22909;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#19977;&#20010;&#27169;&#22411;&#30340;&#20215;&#20540;&#21512;&#29702;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;A2EHV&#26041;&#27861;&#27604;&#20256;&#32479;&#23545;&#40784;&#26041;&#27861;&#26356;&#21512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergent capabilities of Large Language Models (LLMs) have made it crucial to align their values with those of humans. Current methodologies typically attempt alignment with a homogeneous human value and requires human verification, yet lack consensus on the desired aspect and depth of alignment and resulting human biases. In this paper, we propose A2EHV, an Automated Alignment Evaluation with a Heterogeneous Value system that (1) is automated to minimize individual human biases, and (2) allows assessments against various target values to foster heterogeneous agents. Our approach pivots on the concept of value rationality, which represents the ability for agents to execute behaviors that satisfy a target value the most. The quantification of value rationality is facilitated by the Social Value Orientation framework from social psychology, which partitions the value space into four categories to assess social preferences from agents' behaviors. We evaluate the value rationality of e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Ghost in the Minecraft (GITM)&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22522;&#20110;&#25991;&#26412;&#30340;&#30693;&#35782;&#21644;&#35760;&#24518;&#65292;&#21019;&#36896;&#20102;&#19968;&#31181;&#22312;Minecraft&#20013;&#20855;&#22791;&#36890;&#29992;&#33021;&#21147;&#30340;&#26234;&#33021;&#20307;&#65292;&#21487;&#22312;&#20197;&#25991;&#26412;&#20026;&#22522;&#30784;&#30340;&#22797;&#26434;&#32534;&#31243;&#29615;&#22659;&#20013;&#29087;&#32451;&#23548;&#33322;&#12290;</title><link>http://arxiv.org/abs/2305.17144</link><description>&lt;p&gt;
Minecraft&#20013;&#30340;&#24189;&#28789;&#65306;&#21033;&#29992;&#22522;&#20110;&#25991;&#26412;&#30693;&#35782;&#21644;&#35760;&#24518;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#24320;&#25918;&#19990;&#30028;&#29615;&#22659;&#20013;&#30340;&#36890;&#29992;&#33021;&#21147;&#26234;&#33021;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ghost in the Minecraft: Generally Capable Agents for Open-World Enviroments via Large Language Models with Text-based Knowledge and Memory. (arXiv:2305.17144v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17144
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Ghost in the Minecraft (GITM)&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22522;&#20110;&#25991;&#26412;&#30340;&#30693;&#35782;&#21644;&#35760;&#24518;&#65292;&#21019;&#36896;&#20102;&#19968;&#31181;&#22312;Minecraft&#20013;&#20855;&#22791;&#36890;&#29992;&#33021;&#21147;&#30340;&#26234;&#33021;&#20307;&#65292;&#21487;&#22312;&#20197;&#25991;&#26412;&#20026;&#22522;&#30784;&#30340;&#22797;&#26434;&#32534;&#31243;&#29615;&#22659;&#20013;&#29087;&#32451;&#23548;&#33322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;Minecraft&#29609;&#27861;&#21560;&#24341;&#20102;&#22823;&#37327;&#30340;&#30740;&#31350;&#20851;&#27880;&#65292;&#25104;&#20026;&#24320;&#21457;&#33021;&#22815;&#22312;&#24320;&#25918;&#19990;&#30028;&#29615;&#22659;&#20013;&#36816;&#34892;&#30340;&#26234;&#33021;&#20307;&#30340;&#20016;&#23500;&#24179;&#21488;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#29305;&#23450;&#30340;&#30446;&#26631;&#19978;&#65292;&#20363;&#22914;&#27969;&#34892;&#30340;&#8220;ObtainDiamond&#8221;&#20219;&#21153;&#65292;&#24182;&#19988;&#36824;&#27809;&#26377;&#26174;&#31034;&#20986;&#26377;&#25928;&#22320;&#25512;&#24191;&#21040;&#26356;&#24191;&#27867;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#8220;ObtainDiamond&#8221;&#20219;&#21153;&#30340;&#30446;&#21069;&#26368;&#39640;&#25104;&#21151;&#29575;&#21482;&#26377;&#32422;20&#65285;&#65292;&#20984;&#26174;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#25511;&#21046;&#22120;&#30340;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Ghost in the Minecraft (GITM)&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22522;&#20110;&#25991;&#26412;&#30340;&#30693;&#35782;&#21644;&#35760;&#24518;&#30456;&#32467;&#21512;&#65292;&#26088;&#22312;&#21019;&#24314;Minecraft&#20013;&#30340;&#36890;&#29992;&#33021;&#21147;&#26234;&#33021;&#20307;&#12290;&#36825;&#20123;&#20855;&#22791;LLM&#20013;&#30340;&#36923;&#36753;&#21644;&#24120;&#35782;&#33021;&#21147;&#30340;&#26234;&#33021;&#20307;&#21487;&#20197;&#29087;&#32451;&#22320;&#22312;&#20197;&#25991;&#26412;&#20026;&#22522;&#30784;&#30340;&#22797;&#26434;&#32534;&#31243;&#29615;&#22659;&#20013;&#23548;&#33322;&#12290;
&lt;/p&gt;
&lt;p&gt;
The captivating realm of Minecraft has attracted substantial research interest in recent years, serving as a rich platform for developing intelligent agents capable of functioning in open-world environments. However, the current research landscape predominantly focuses on specific objectives, such as the popular "ObtainDiamond" task, and has not yet shown effective generalization to a broader spectrum of tasks. Furthermore, the current leading success rate for the "ObtainDiamond" task stands at around 20%, highlighting the limitations of Reinforcement Learning (RL) based controllers used in existing methods. To tackle these challenges, we introduce Ghost in the Minecraft (GITM), a novel framework integrates Large Language Models (LLMs) with text-based knowledge and memory, aiming to create Generally Capable Agents (GCAs) in Minecraft. These agents, equipped with the logic and common sense capabilities of LLMs, can skillfully navigate complex, sparse-reward environments with text-based 
&lt;/p&gt;</description></item><item><title>&#35748;&#35777;&#25216;&#26415;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#20998;&#24067;&#24335;&#31995;&#32479;&#39564;&#35777;&#19981;&#36866;&#29992;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;ML-based&#20998;&#24067;&#24335;&#31995;&#32479;&#35748;&#35777;&#26041;&#26696;&#65292;&#20197;&#39564;&#35777;&#20854;&#38750;&#21151;&#33021;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.16822</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#20998;&#24067;&#24335;&#31995;&#32479;&#35748;&#35777;&#20043;&#36335;
&lt;/p&gt;
&lt;p&gt;
Towards Certification of Machine Learning-Based Distributed Systems. (arXiv:2305.16822v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16822
&lt;/p&gt;
&lt;p&gt;
&#35748;&#35777;&#25216;&#26415;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#20998;&#24067;&#24335;&#31995;&#32479;&#39564;&#35777;&#19981;&#36866;&#29992;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;ML-based&#20998;&#24067;&#24335;&#31995;&#32479;&#35748;&#35777;&#26041;&#26696;&#65292;&#20197;&#39564;&#35777;&#20854;&#38750;&#21151;&#33021;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26085;&#30410;&#34987;&#29992;&#20110;&#39537;&#21160;&#37096;&#32626;&#22312;5G&#20113;&#36793;&#32536;&#36830;&#32493;&#20307;&#19978;&#30340;&#22797;&#26434;&#20998;&#24067;&#24335;&#31995;&#32479;&#30340;&#36816;&#34892;&#12290;&#30456;&#24212;&#22320;&#65292;&#20998;&#24067;&#24335;&#31995;&#32479;&#30340;&#34892;&#20026;&#21464;&#24471;&#26356;&#20855;&#38750;&#30830;&#23450;&#24615;&#12290;&#36825;&#31181;&#20998;&#24067;&#24335;&#31995;&#32479;&#30340;&#28436;&#21270;&#38656;&#35201;&#23450;&#20041;&#26032;&#30340;&#20445;&#35777;&#26041;&#27861;&#26469;&#39564;&#35777;&#38750;&#21151;&#33021;&#23646;&#24615;&#12290;&#35748;&#35777;&#20316;&#20026;&#31995;&#32479;&#21644;&#36719;&#20214;&#39564;&#35777;&#30340;&#26368;&#27969;&#34892;&#30340;&#20445;&#35777;&#25216;&#26415;&#65292;&#19981;&#33021;&#31435;&#21363;&#36866;&#29992;&#20110;&#20854;&#34892;&#20026;&#30001;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#25512;&#29702;&#20915;&#23450;&#30340;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#25919;&#31574;&#21046;&#23450;&#32773;&#12289;&#30417;&#31649;&#26426;&#26500;&#21644;&#20135;&#19994;&#21033;&#30410;&#30456;&#20851;&#32773;&#36234;&#26469;&#36234;&#25512;&#23815;&#23450;&#20041;ML&#30340;&#38750;&#21151;&#33021;&#23646;&#24615;&#65288;&#22914;&#20844;&#24179;&#24615;&#12289;&#40065;&#26834;&#24615;&#12289;&#38544;&#31169;&#65289;&#30340;&#35748;&#35777;&#25216;&#26415;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#24403;&#21069;&#35748;&#35777;&#26041;&#26696;&#30340;&#25361;&#25112;&#21644;&#19981;&#36275;&#20043;&#22788;&#65292;&#35752;&#35770;&#20102;&#24320;&#25918;&#30340;&#30740;&#31350;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;ML-based&#20998;&#24067;&#24335;&#31995;&#32479;&#35748;&#35777;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning (ML) is increasingly used to drive the operation of complex distributed systems deployed on the cloud-edge continuum enabled by 5G. Correspondingly, distributed systems' behavior is becoming more non-deterministic in nature. This evolution of distributed systems requires the definition of new assurance approaches for the verification of non-functional properties. Certification, the most popular assurance technique for system and software verification, is not immediately applicable to systems whose behavior is determined by Machine Learning-based inference. However, there is an increasing push from policy makers, regulators, and industrial stakeholders towards the definition of techniques for the certification of non-functional properties (e.g., fairness, robustness, privacy) of ML. This article analyzes the challenges and deficiencies of current certification schemes, discusses open research issues and proposes a first certification scheme for ML-based distributed syst
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#30340;&#26694;&#26550;&#26469;&#35299;&#37322;&#40657;&#30418;DRL&#25152;&#20316;&#30340;&#20915;&#31574;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#35813;&#35299;&#37322;&#26694;&#26550;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.16532</link><description>&lt;p&gt;
&#20351;&#29992;&#31574;&#30053;&#33976;&#39311;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#22120;&#26694;&#26550;&#35299;&#37322;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Explainer Framework for Deep Reinforcement Learning Models Using Policy Distillation. (arXiv:2305.16532v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16532
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#30340;&#26694;&#26550;&#26469;&#35299;&#37322;&#40657;&#30418;DRL&#25152;&#20316;&#30340;&#20915;&#31574;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#35813;&#35299;&#37322;&#26694;&#26550;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#24050;&#32463;&#23637;&#31034;&#20986;&#35299;&#20915;&#22797;&#26434;&#25511;&#21046;&#38382;&#39064;&#30340;&#26377;&#24076;&#26395;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#20013;&#65292;DRL&#24212;&#29992;&#21463;&#21040;&#32570;&#20047;&#24378;&#22823;&#30340;&#39564;&#35777;&#25216;&#26415;&#26469;&#20445;&#35777;&#22312;&#36825;&#20123;&#24212;&#29992;&#20013;&#30340;&#24615;&#33021;&#12290;&#39564;&#35777;&#36807;&#31243;&#30340;&#20851;&#38190;&#35201;&#27714;&#20043;&#19968;&#26159;&#24320;&#21457;&#26377;&#25928;&#30340;&#25216;&#26415;&#26469;&#35299;&#37322;&#31995;&#32479;&#30340;&#21151;&#33021;&#65292;&#21363;&#20026;&#20160;&#20040;&#31995;&#32479;&#22312;&#29305;&#23450;&#24773;&#20917;&#19979;&#20135;&#29983;&#29305;&#23450;&#32467;&#26524;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#21453;&#20107;&#23454;&#65288;Counterfactual&#65292;CF&#65289;&#35299;&#37322;&#26041;&#27861;&#30340;&#35299;&#37322;&#26041;&#27861;&#26469;&#35299;&#20915;DRL&#20013;&#30340;&#35299;&#37322;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;CF&#35299;&#37322;&#26694;&#26550;&#65292;&#26469;&#35299;&#37322;&#40657;&#30418;DRL&#25152;&#20316;&#30340;&#20915;&#31574;&#12290;&#20026;&#20102;&#35780;&#20272;&#25152;&#25552;&#20986;&#30340;&#35299;&#37322;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#21644;Atari Pong&#28216;&#25103;&#39046;&#22495;&#36827;&#34892;&#20102;&#20960;&#39033;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#29983;&#25104;&#20102;&#21512;&#29702;&#21644;&#26377;&#24847;&#20041;&#30340;&#35299;&#37322;&#32467;&#26524;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#19982;&#21407;&#22987;DRL&#27169;&#22411;&#30456;&#27604;&#30340;&#39640;&#24230;&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning (DRL) has demonstrated promising capability in solving complex control problems. However, DRL applications in safety-critical systems are hindered by the inherent lack of robust verification techniques to assure their performance in such applications. One of the key requirements of the verification process is the development of effective techniques to explain the system functionality, i.e., why the system produces specific results in given circumstances. Recently, interpretation methods based on the Counterfactual (CF) explanation approach have been proposed to address the problem of explanation in DRLs. This paper proposes a novel CF explanation framework to explain the decisions made by a black-box DRL. To evaluate the efficacy of the proposed explanation framework, we carried out several experiments in the domains of automated driving systems and Atari Pong game. Our analysis demonstrates that the proposed framework generates plausible and meaningful expl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32771;&#34385;&#22312;&#19968;&#20010;&#20844;&#20849;&#21464;&#37327;&#30340;&#26465;&#20214;&#19979;&#65292;&#30456;&#24212;&#20998;&#24067;&#20043;&#38388;&#30340;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#12290;&#36890;&#36807;&#37319;&#29992;&#22522;&#20110; MMD &#30340;&#26680;&#27491;&#21017;&#21270;&#22120;&#65292;&#20811;&#26381;&#20102;&#26465;&#20214;&#21464;&#37327;&#26159;&#36830;&#32493;&#30340;&#21644;&#20004;&#20010;&#20998;&#24067;&#20013;&#35813;&#21464;&#37327;&#30340;&#36793;&#32536;&#26159;&#19981;&#21516;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.15901</link><description>&lt;p&gt;
&#26465;&#20214;&#20998;&#24067;&#20043;&#38388;&#30340;&#32463;&#39564;&#26368;&#20248;&#36755;&#36816;
&lt;/p&gt;
&lt;p&gt;
Empirical Optimal Transport between Conditional Distributions. (arXiv:2305.15901v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15901
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#22312;&#19968;&#20010;&#20844;&#20849;&#21464;&#37327;&#30340;&#26465;&#20214;&#19979;&#65292;&#30456;&#24212;&#20998;&#24067;&#20043;&#38388;&#30340;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#12290;&#36890;&#36807;&#37319;&#29992;&#22522;&#20110; MMD &#30340;&#26680;&#27491;&#21017;&#21270;&#22120;&#65292;&#20811;&#26381;&#20102;&#26465;&#20214;&#21464;&#37327;&#26159;&#36830;&#32493;&#30340;&#21644;&#20004;&#20010;&#20998;&#24067;&#20013;&#35813;&#21464;&#37327;&#30340;&#36793;&#32536;&#26159;&#19981;&#21516;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#20004;&#20010;&#32852;&#21512;&#20998;&#24067;&#30340;&#26679;&#26412;&#65292;&#32771;&#34385;&#22312;&#19968;&#20010;&#20844;&#20849;&#21464;&#37327;&#30340;&#26465;&#20214;&#19979;&#65292;&#30456;&#24212;&#20998;&#24067;&#20043;&#38388;&#30340;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#20272;&#35745;&#20276;&#38543;&#26465;&#20214;&#20540;&#30340;&#36755;&#36816;&#25104;&#26412;&#65288;Wasserstein &#36317;&#31163;&#65289;&#65292;&#20197;&#21450;&#26465;&#20214;&#20998;&#24067;&#38388;&#30340;&#36755;&#36816;&#35745;&#21010;&#12290;&#30001;&#20110;&#21305;&#37197;&#26465;&#20214;&#20998;&#24067;&#26159;&#30417;&#30563;&#35757;&#32451;&#21028;&#21035;&#27169;&#22411;&#21644;&#65288;&#38544;&#24335;&#65289;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#30340;&#26680;&#24515;&#65292;&#26465;&#20214;&#20998;&#24067;&#20043;&#38388;&#30340;&#26368;&#20248;&#36755;&#36816;&#20855;&#26377;&#22312;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#34987;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#28041;&#21450;&#21040;&#38544;&#24335;&#29305;&#23450;&#20110;&#32852;&#21512;&#65288;&#26679;&#26412;&#65289;&#30340;&#26465;&#20214;&#20998;&#24067;&#65292;&#22240;&#27492;&#21046;&#23450;&#36825;&#20010;&#38382;&#39064;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#29305;&#21035;&#26159;&#22312;&#65288;i&#65289;&#26465;&#20214;&#21464;&#37327;&#26159;&#36830;&#32493;&#30340;&#21644;&#65288;ii&#65289;&#20004;&#20010;&#20998;&#24067;&#20013;&#35813;&#21464;&#37327;&#30340;&#36793;&#32536;&#26159;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#36890;&#36807;&#37319;&#29992;&#29305;&#23450;&#30340;&#22522;&#20110; MMD&#65288;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#65289;&#30340;&#26680;&#27491;&#21017;&#21270;&#22120;&#26469;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given samples from two joint distributions, we consider the problem of Optimal Transportation (OT) between the corresponding distributions conditioned on a common variable. The objective of this work is to estimate the associated transport cost (Wasserstein distance) as well as the transport plan between the conditionals as a function of the conditioned value. Since matching conditional distributions is at the core of supervised training of discriminative models and (implicit) conditional-generative models, OT between conditionals has the potential to be employed in diverse machine learning applications. However, since the conditionals involved in OT are implicitly specified via the joint samples, it is challenging to formulate this problem, especially when (i) the variable conditioned on is continuous and (ii) the marginal of this variable in the two distributions is different. We overcome these challenges by employing a specific kernel MMD (Maximum Mean Discrepancy) based regularizer
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25910;&#38598;&#21644;&#20998;&#31867;&#20102;&#36229;&#36807;220&#20010;&#21463;&#27426;&#36814;&#30340;&#25163;&#24037;&#35821;&#35328;&#29305;&#24449;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#22810;&#35821;&#35328;&#30340;&#25163;&#24037;&#35821;&#35328;&#29305;&#24449;&#25552;&#21462;&#31995;&#32479;&#65292;&#20197;&#31995;&#32479;&#24615;&#30340;&#21487;&#25193;&#23637;&#26041;&#24335;&#23454;&#29616;&#65292;&#24182;&#22312;&#20960;&#20010;&#20219;&#21153;&#29305;&#23450;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#30456;&#20851;&#24615;&#20998;&#26512;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2305.15878</link><description>&lt;p&gt;
LFTK: &#35745;&#31639;&#35821;&#35328;&#23398;&#20013;&#30340;&#25163;&#24037;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
LFTK: Handcrafted Features in Computational Linguistics. (arXiv:2305.15878v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15878
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25910;&#38598;&#21644;&#20998;&#31867;&#20102;&#36229;&#36807;220&#20010;&#21463;&#27426;&#36814;&#30340;&#25163;&#24037;&#35821;&#35328;&#29305;&#24449;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#22810;&#35821;&#35328;&#30340;&#25163;&#24037;&#35821;&#35328;&#29305;&#24449;&#25552;&#21462;&#31995;&#32479;&#65292;&#20197;&#31995;&#32479;&#24615;&#30340;&#21487;&#25193;&#23637;&#26041;&#24335;&#23454;&#29616;&#65292;&#24182;&#22312;&#20960;&#20010;&#20219;&#21153;&#29305;&#23450;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#30456;&#20851;&#24615;&#20998;&#26512;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#30340;&#30740;&#31350;&#24050;&#32463;&#37492;&#23450;&#20986;&#20102;&#19968;&#32452;&#20016;&#23500;&#30340;&#25163;&#24037;&#35821;&#35328;&#29305;&#24449;&#65292;&#21487;&#20197;&#28508;&#22312;&#22320;&#24110;&#21161;&#21508;&#31181;&#20219;&#21153;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#36825;&#20123;&#29305;&#24449;&#25968;&#37327;&#24222;&#22823;&#65292;&#22240;&#27492;&#38590;&#20197;&#26377;&#25928;&#22320;&#36873;&#25321;&#21644;&#21033;&#29992;&#29616;&#26377;&#30340;&#25163;&#24037;&#29305;&#24449;&#12290;&#21152;&#19978;&#22312;&#30740;&#31350;&#24037;&#20316;&#20013;&#23454;&#29616;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#65292;&#30446;&#21069;&#36824;&#19981;&#23384;&#22312;&#20998;&#31867;&#26041;&#26696;&#25110;&#32773;&#32479;&#19968;&#25509;&#21463;&#30340;&#29305;&#24449;&#21517;&#31216;&#65292;&#36825;&#36896;&#25104;&#20102;&#19981;&#24517;&#35201;&#30340;&#28151;&#20081;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#25163;&#24037;&#29305;&#24449;&#25552;&#21462;&#24211;&#37117;&#19981;&#26159;&#24320;&#28304;&#30340;&#65292;&#25110;&#32773;&#27809;&#26377;&#24471;&#21040;&#31215;&#26497;&#30340;&#32500;&#25252;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#20154;&#21592;&#32463;&#24120;&#38656;&#35201;&#20174;&#38646;&#24320;&#22987;&#26500;&#24314;&#36825;&#26679;&#30340;&#25552;&#21462;&#31995;&#32479;&#12290;&#25105;&#20204;&#36890;&#36807;&#36807;&#21435;&#30340;&#25991;&#29486;&#25910;&#38598;&#21644;&#20998;&#31867;&#20102;&#36229;&#36807;220&#20010;&#21463;&#27426;&#36814;&#30340;&#25163;&#24037;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#20960;&#20010;&#20219;&#21153;&#29305;&#23450;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#30456;&#20851;&#24615;&#20998;&#26512;&#30740;&#31350;&#65292;&#24182;&#25253;&#21578;&#20102;&#27599;&#20010;&#29305;&#24449;&#30340;&#28508;&#22312;&#29992;&#36884;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22810;&#35821;&#35328;&#30340;&#25163;&#24037;&#35821;&#35328;&#29305;&#24449;&#25552;&#21462;&#31995;&#32479;&#65292;&#20197;&#31995;&#32479;&#24615;&#30340;&#21487;&#25193;&#23637;&#26041;&#24335;&#23454;&#29616;&#12290;&#25105;&#20204;&#24320;&#28304;&#20102;&#25105;&#20204;&#30340;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Past research has identified a rich set of handcrafted linguistic features that can potentially assist various tasks. However, their extensive number makes it difficult to effectively select and utilize existing handcrafted features. Coupled with the problem of inconsistent implementation across research works, there has been no categorization scheme or generally-accepted feature names. This creates unwanted confusion. Also, most existing handcrafted feature extraction libraries are not open-source or not actively maintained. As a result, a researcher often has to build such an extraction system from the ground up.  We collect and categorize more than 220 popular handcrafted features grounded on past literature. Then, we conduct a correlation analysis study on several task-specific datasets and report the potential use cases of each feature. Lastly, we devise a multilingual handcrafted linguistic feature extraction system in a systematically expandable manner. We open-source our system
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#29702;&#35770;&#23618;&#38754;&#25506;&#31350;&#20102;&#24102;&#26377;&#8220;&#24605;&#32500;&#38142;&#8221;&#25552;&#31034;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#22522;&#26412;&#25968;&#23398;&#21644;&#20915;&#31574;&#38382;&#39064;&#20013;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#33258;&#22238;&#24402;Transformer&#22823;&#23567;&#24658;&#23450;&#21363;&#21487;&#35299;&#20915;&#20219;&#21153;&#65292;&#25581;&#31034;&#20102;&#8220;&#24605;&#32500;&#38142;&#8221;&#25552;&#31034;&#30340;&#32972;&#21518;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.15408</link><description>&lt;p&gt;
&#20174;&#29702;&#35770;&#35282;&#24230;&#25581;&#31034;&#8220;&#24605;&#32500;&#38142;&#8221;&#32972;&#21518;&#30340;&#22885;&#31192;
&lt;/p&gt;
&lt;p&gt;
Towards Revealing the Mystery behind Chain of Thought: a Theoretical Perspective. (arXiv:2305.15408v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15408
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#29702;&#35770;&#23618;&#38754;&#25506;&#31350;&#20102;&#24102;&#26377;&#8220;&#24605;&#32500;&#38142;&#8221;&#25552;&#31034;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#22522;&#26412;&#25968;&#23398;&#21644;&#20915;&#31574;&#38382;&#39064;&#20013;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#33258;&#22238;&#24402;Transformer&#22823;&#23567;&#24658;&#23450;&#21363;&#21487;&#35299;&#20915;&#20219;&#21153;&#65292;&#25581;&#31034;&#20102;&#8220;&#24605;&#32500;&#38142;&#8221;&#25552;&#31034;&#30340;&#32972;&#21518;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;"&#24605;&#32500;&#38142;"&#25552;&#31034;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#28041;&#21450;&#25968;&#23398;&#25110;&#25512;&#29702;&#30340;&#22797;&#26434;&#20219;&#21153;&#20013;&#12290;&#23613;&#31649;&#33719;&#24471;&#20102;&#24040;&#22823;&#30340;&#23454;&#35777;&#25104;&#21151;&#65292;&#20294;&#8220;&#24605;&#32500;&#38142;&#8221;&#32972;&#21518;&#30340;&#26426;&#21046;&#20197;&#21450;&#23427;&#22914;&#20309;&#37322;&#25918;LLMs&#30340;&#28508;&#21147;&#20173;&#28982;&#26159;&#31070;&#31192;&#30340;&#12290;&#26412;&#25991;&#39318;&#27425;&#20174;&#29702;&#35770;&#19978;&#22238;&#31572;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#24102;&#26377;&#8220;&#24605;&#32500;&#38142;&#8221;&#22312;&#35299;&#20915;&#22522;&#26412;&#25968;&#23398;&#21644;&#20915;&#31574;&#38382;&#39064;&#20013;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#32473;&#20986;&#19968;&#20010;&#19981;&#21487;&#33021;&#30340;&#32467;&#26524;&#65292;&#34920;&#26126;&#20219;&#20309;&#26377;&#38480;&#28145;&#24230;&#30340;Transformer&#37117;&#19981;&#33021;&#30452;&#25509;&#36755;&#20986;&#27491;&#30830;&#30340;&#22522;&#26412;&#31639;&#26415;/&#26041;&#31243;&#20219;&#21153;&#30340;&#31572;&#26696;&#65292;&#38500;&#38750;&#27169;&#22411;&#22823;&#23567;&#38543;&#30528;&#36755;&#20837;&#38271;&#24230;&#30340;&#22686;&#21152;&#21576;&#36229;&#22810;&#39033;&#24335;&#22686;&#38271;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#36890;&#36807;&#26500;&#36896;&#35777;&#26126;&#65292;&#22823;&#23567;&#24658;&#23450;&#30340;&#33258;&#22238;&#24402;Transformer&#36275;&#20197;&#36890;&#36807;&#20351;&#29992;&#24120;&#29992;&#30340;&#25968;&#23398;&#35821;&#35328;&#24418;&#24335;&#29983;&#25104;&#8220;&#24605;&#32500;&#38142;&#8221;&#25512;&#23548;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have discovered that Chain-of-Thought prompting (CoT) can dramatically improve the performance of Large Language Models (LLMs), particularly when dealing with complex tasks involving mathematics or reasoning. Despite the enormous empirical success, the underlying mechanisms behind CoT and how it unlocks the potential of LLMs remain elusive. In this paper, we take a first step towards theoretically answering these questions. Specifically, we examine the capacity of LLMs with CoT in solving fundamental mathematical and decision-making problems. We start by giving an impossibility result showing that any bounded-depth Transformer cannot directly output correct answers for basic arithmetic/equation tasks unless the model size grows super-polynomially with respect to the input length. In contrast, we then prove by construction that autoregressive Transformers of a constant size suffice to solve both tasks by generating CoT derivations using a commonly-used math language forma
&lt;/p&gt;</description></item><item><title>SPECTRON&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#35821;&#38899;&#24310;&#32493;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#38899;&#32534;&#30721;&#22120;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#35757;&#32451;&#26469;&#29983;&#25104;&#25991;&#26412;&#21644;&#35821;&#38899;&#36755;&#20986;&#65292;&#22312;&#35821;&#20041;&#20869;&#23481;&#21644;&#35762;&#35805;&#32773;&#20445;&#25252;&#26041;&#38754;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#21475;&#35821;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.15255</link><description>&lt;p&gt;
&#24102;&#26377;&#35821;&#38899;&#30340;LM&#65306;&#36229;&#36234;&#35821;&#38899;&#20196;&#29260;&#30340;&#21475;&#35821;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
LMs with a Voice: Spoken Language Modeling beyond Speech Tokens. (arXiv:2305.15255v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15255
&lt;/p&gt;
&lt;p&gt;
SPECTRON&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#35821;&#38899;&#24310;&#32493;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#38899;&#32534;&#30721;&#22120;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#35757;&#32451;&#26469;&#29983;&#25104;&#25991;&#26412;&#21644;&#35821;&#38899;&#36755;&#20986;&#65292;&#22312;&#35821;&#20041;&#20869;&#23481;&#21644;&#35762;&#35805;&#32773;&#20445;&#25252;&#26041;&#38754;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#21475;&#35821;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;SPECTRON&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#36866;&#24212;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#20197;&#25191;&#34892;&#35821;&#38899;&#24310;&#32493;&#12290;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#38899;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#25991;&#26412;&#21644;&#35821;&#38899;&#36755;&#20986;&#65292;&#25972;&#20010;&#31995;&#32479;&#37117;&#22312;&#39057;&#35889;&#22270;&#19978;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#35757;&#32451;&#12290;&#22312;&#39057;&#35889;&#22270;&#39046;&#22495;&#35757;&#32451;&#25972;&#20010;&#27169;&#22411;&#30456;&#23545;&#20110;&#20351;&#29992;&#31163;&#25955;&#35821;&#38899;&#34920;&#31034;&#30340;&#29616;&#26377;&#32423;&#32852;&#26041;&#27861;&#31616;&#21270;&#20102;&#25105;&#20204;&#30340;&#35821;&#38899;&#24310;&#32493;&#31995;&#32479;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35821;&#20041;&#20869;&#23481;&#21644;&#35762;&#35805;&#32773;&#20445;&#25252;&#26041;&#38754;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#21475;&#35821;&#35821;&#35328;&#27169;&#22411;&#65292;&#21516;&#26102;&#20063;&#20174;&#39044;&#20808;&#23384;&#22312;&#30340;&#27169;&#22411;&#20013;&#33719;&#24471;&#20102;&#30693;&#35782;&#20256;&#36882;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#30340;&#32593;&#31449;https://michelleramanovich.github.io/spectron/spectron&#19978;&#21487;&#20197;&#25214;&#21040;&#38899;&#39057;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present SPECTRON, a novel approach to adapting pre-trained language models (LMs) to perform speech continuation. By leveraging pre-trained speech encoders, our model generates both text and speech outputs with the entire system being trained end-to-end operating directly on spectrograms. Training the entire model in the spectrogram domain simplifies our speech continuation system versus existing cascade methods which use discrete speech representations. We further show our method surpasses existing spoken language models both in semantic content and speaker preservation while also benefiting from the knowledge transferred from pre-existing models. Audio samples can be found in our website https://michelleramanovich.github.io/spectron/spectron
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#35821;&#38899;&#22788;&#29702;&#25216;&#26415;&#32467;&#21512;&#33258;&#28982;&#35821;&#35328;&#26679;&#26412;(NLS)&#20998;&#26512;&#23396;&#29420;&#30151;&#20799;&#31461;&#30340;&#21475;&#35821;&#21457;&#23637;&#27700;&#24179;&#65292;&#33021;&#22815;&#20998;&#31867;&#20986;&#20799;&#31461;&#21644;&#25104;&#20154;&#35821;&#38899;&#65292;&#20197;&#21450;&#35821;&#38899;&#21644;&#38750;&#35821;&#35328;&#21457;&#22768;&#65292;&#24182;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.14117</link><description>&lt;p&gt;
&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#38899;&#23884;&#20837;&#29702;&#35299;&#23396;&#29420;&#30151;&#20799;&#31461;&#21475;&#35821;&#35821;&#35328;&#21457;&#23637;
&lt;/p&gt;
&lt;p&gt;
Understanding Spoken Language Development of Children with ASD Using Pre-trained Speech Embeddings. (arXiv:2305.14117v1 [eess.AS] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14117
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#35821;&#38899;&#22788;&#29702;&#25216;&#26415;&#32467;&#21512;&#33258;&#28982;&#35821;&#35328;&#26679;&#26412;(NLS)&#20998;&#26512;&#23396;&#29420;&#30151;&#20799;&#31461;&#30340;&#21475;&#35821;&#21457;&#23637;&#27700;&#24179;&#65292;&#33021;&#22815;&#20998;&#31867;&#20986;&#20799;&#31461;&#21644;&#25104;&#20154;&#35821;&#38899;&#65292;&#20197;&#21450;&#35821;&#38899;&#21644;&#38750;&#35821;&#35328;&#21457;&#22768;&#65292;&#24182;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#22788;&#29702;&#25216;&#26415;&#23545;&#20110;&#20998;&#26512;&#23396;&#29420;&#30151;&#35889;&#31995;&#38556;&#30861;(ASD)&#20799;&#31461;&#30340;&#35821;&#38899;&#21644;&#35821;&#35328;&#21457;&#23637;&#24456;&#26377;&#24110;&#21161;&#65292;&#36825;&#20123;&#23401;&#23376;&#22312;&#33719;&#24471;&#36825;&#20123;&#25216;&#33021;&#26041;&#38754;&#24120;&#24120;&#26159;&#22810;&#26679;&#24615;&#21644;&#24310;&#36831;&#30340;&#12290;&#23613;&#26089;&#35782;&#21035;&#21644;&#24178;&#39044;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20294;&#20256;&#32479;&#30340;&#35780;&#20272;&#26041;&#27861;&#22914;&#25252;&#29702;&#20154;&#21592;&#25253;&#21578;&#23545;&#20110;&#25152;&#38656;&#34892;&#20026;&#34920;&#22411;&#25551;&#36848;&#26159;&#19981;&#36275;&#22815;&#30340;&#12290;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#26679;&#26412;(NLS)&#20998;&#26512;&#24050;&#33719;&#24471;&#20851;&#27880;&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#34917;&#20805;&#25163;&#27573;&#12290;&#30740;&#31350;&#20154;&#21592;&#22312;&#20998;&#26512;NLS&#20013;&#23396;&#29420;&#30151;&#20799;&#31461;&#30340;&#21475;&#35821;&#33021;&#21147;&#27700;&#24179;&#26102;&#24050;&#32463;&#21046;&#23450;&#20102;&#22522;&#20934;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#35821;&#38899;&#22788;&#29702;&#25216;&#26415;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#20998;&#31867;&#26469;&#25903;&#25345;&#20799;&#31461;&#21475;&#35821;&#35821;&#35328;&#21457;&#23637;&#30340;&#33258;&#21160;&#21270;&#35780;&#20272;&#65292;&#21253;&#25324;&#20998;&#36776;&#20799;&#31461;&#21644;&#25104;&#20154;&#35821;&#38899;&#65292;&#20197;&#21450;&#35821;&#38899;&#21644;&#38750;&#35821;&#35328;&#21457;&#22768;&#65292;&#20854;F1&#23439;&#24179;&#22343;&#20998;&#21035;&#20026;82.6&#65285;&#21644;67.8&#65285;&#65292;&#24378;&#35843;&#20102;ASD&#30740;&#31350;&#21644;&#20020;&#24202;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech processing techniques are useful for analyzing speech and language development in children with Autism Spectrum Disorder (ASD), who are often varied and delayed in acquiring these skills. Early identification and intervention are crucial, but traditional assessment methodologies such as caregiver reports are not adequate for the requisite behavioral phenotyping. Natural Language Sample (NLS) analysis has gained attention as a promising complement. Researchers have developed benchmarks for spoken language capabilities in children with ASD, obtainable through the analysis of NLS. This paper proposes applications of speech processing technologies in support of automated assessment of children's spoken language development by classification between child and adult speech and between speech and nonverbal vocalization in NLS, with respective F1 macro scores of 82.6% and 67.8%, underscoring the potential for accurate and scalable tools for ASD research and clinical use.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;CORAL&#65292;&#19968;&#31181;&#29992;&#20110;&#22312;&#32447;&#26080;&#30417;&#30563;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#30340;&#26032;&#26694;&#26550;&#65292;&#21487;&#20197;&#33258;&#21160;&#35302;&#21457;&#35813;&#36807;&#31243;&#24182;&#22686;&#37327;&#26356;&#26032;&#27169;&#22411;&#65292;&#21253;&#25324;&#19977;&#20010;&#20027;&#35201;&#37096;&#20998;&#65306;&#35302;&#21457;&#28857;&#26816;&#27979;&#65292;&#22686;&#37327;&#22240;&#26524;&#22270;&#23398;&#20064;&#21644;&#22522;&#20110;&#32593;&#32476;&#20256;&#25773;&#30340;&#26681;&#26412;&#21407;&#22240;&#23450;&#20301;&#12290;</title><link>http://arxiv.org/abs/2305.10638</link><description>&lt;p&gt;
&#22686;&#37327;&#22240;&#26524;&#22270;&#23398;&#20064;&#36827;&#34892;&#22312;&#32447;&#26080;&#30417;&#30563;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Incremental Causal Graph Learning for Online Unsupervised Root Cause Analysis. (arXiv:2305.10638v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10638
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;CORAL&#65292;&#19968;&#31181;&#29992;&#20110;&#22312;&#32447;&#26080;&#30417;&#30563;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#30340;&#26032;&#26694;&#26550;&#65292;&#21487;&#20197;&#33258;&#21160;&#35302;&#21457;&#35813;&#36807;&#31243;&#24182;&#22686;&#37327;&#26356;&#26032;&#27169;&#22411;&#65292;&#21253;&#25324;&#19977;&#20010;&#20027;&#35201;&#37096;&#20998;&#65306;&#35302;&#21457;&#28857;&#26816;&#27979;&#65292;&#22686;&#37327;&#22240;&#26524;&#22270;&#23398;&#20064;&#21644;&#22522;&#20110;&#32593;&#32476;&#20256;&#25773;&#30340;&#26681;&#26412;&#21407;&#22240;&#23450;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#65288;RCA&#65289;&#30340;&#20219;&#21153;&#26159;&#20998;&#26512;&#31995;&#32479;&#30417;&#25511;&#25968;&#25454;&#65292;&#20197;&#35782;&#21035;&#31995;&#32479;&#25925;&#38556;/&#22833;&#25928;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;&#26377;&#25928;&#30340;RCA&#21487;&#20197;&#22823;&#22823;&#21152;&#36895;&#31995;&#32479;&#25925;&#38556;&#24674;&#22797;&#65292;&#24182;&#20943;&#36731;&#31995;&#32479;&#25439;&#22833;&#25110;&#36130;&#21153;&#25439;&#22833;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#30340;&#30740;&#31350;&#22823;&#22810;&#38598;&#20013;&#22312;&#24320;&#21457;&#31163;&#32447;RCA&#31639;&#27861;&#19978;&#65292;&#36825;&#36890;&#24120;&#38656;&#35201;&#25163;&#21160;&#21551;&#21160;RCA&#36807;&#31243;&#65292;&#38656;&#35201;&#22823;&#37327;&#26102;&#38388;&#21644;&#25968;&#25454;&#26469;&#35757;&#32451;&#31283;&#20581;&#30340;&#27169;&#22411;&#65292;&#28982;&#21518;&#38656;&#35201;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#35757;&#32451;&#26032;&#30340;&#31995;&#32479;&#25925;&#38556;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CORAL&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22312;&#32447;RCA&#26694;&#26550;&#65292;&#21487;&#20197;&#33258;&#21160;&#35302;&#21457;RCA&#36807;&#31243;&#24182;&#22686;&#37327;&#26356;&#26032;RCA&#27169;&#22411;&#12290;CORAL&#21253;&#25324;&#35302;&#21457;&#28857;&#26816;&#27979;&#12289;&#22686;&#37327;&#35299;&#32544;&#22240;&#26524;&#22270;&#23398;&#20064;&#21644;&#22522;&#20110;&#32593;&#32476;&#20256;&#25773;&#30340;&#26681;&#26412;&#21407;&#22240;&#23450;&#20301;&#12290;&#35302;&#21457;&#28857;&#26816;&#27979;&#32452;&#20214;&#26088;&#22312;&#33258;&#21160;&#26816;&#27979;&#31995;&#32479;&#29366;&#24577;&#36716;&#25442;&#24182;&#36827;&#34892;&#20934;&#23454;&#26102;&#26816;&#27979;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;m&#30340;&#22312;&#32447;&#35302;&#21457;&#28857;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of root cause analysis (RCA) is to identify the root causes of system faults/failures by analyzing system monitoring data. Efficient RCA can greatly accelerate system failure recovery and mitigate system damages or financial losses. However, previous research has mostly focused on developing offline RCA algorithms, which often require manually initiating the RCA process, a significant amount of time and data to train a robust model, and then being retrained from scratch for a new system fault.  In this paper, we propose CORAL, a novel online RCA framework that can automatically trigger the RCA process and incrementally update the RCA model. CORAL consists of Trigger Point Detection, Incremental Disentangled Causal Graph Learning, and Network Propagation-based Root Cause Localization. The Trigger Point Detection component aims to detect system state transitions automatically and in near-real-time. To achieve this, we develop an online trigger point detection approach based on m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#30340;&#35780;&#20272;&#24230;&#37327;&#32508;&#36848;&#65292;&#20174;&#32780;&#20026;&#26631;&#20934;&#21270;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#36129;&#29486;&#21147;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.10616</link><description>&lt;p&gt;
CNN &#21387;&#32553;&#30340;&#35780;&#20272;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Evaluation Metrics for CNNs Compression. (arXiv:2305.10616v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10616
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#30340;&#35780;&#20272;&#24230;&#37327;&#32508;&#36848;&#65292;&#20174;&#32780;&#20026;&#26631;&#20934;&#21270;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#36129;&#29486;&#21147;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#33268;&#21147;&#20110;&#24320;&#21457;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#25216;&#26415;&#65292;&#20294;&#31038;&#21306;&#20284;&#20046;&#32570;&#20047;&#26631;&#20934;&#21270;&#30340;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#21387;&#32553;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#35782;&#21035;&#19981;&#21516;&#24212;&#29992;&#31243;&#24207;&#30340;&#26368;&#21512;&#36866;&#30340;&#21387;&#32553;&#25216;&#26415;&#30340;&#20851;&#38190;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#35780;&#20272;&#24230;&#37327;&#30340;&#32508;&#36848;&#26469;&#20026;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#30340;&#26631;&#20934;&#21270;&#36129;&#29486;&#12290;&#36825;&#20123;&#24230;&#37327;&#24050;&#34987;&#23454;&#29616;&#21040;NetZIP&#65292;&#19968;&#20010;&#26631;&#20934;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#22522;&#20934;&#20043;&#20013;&#12290;&#25105;&#20204;&#36890;&#36807;&#19977;&#20010;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#19968;&#20123;&#34987;&#23457;&#26597;&#30340;&#24230;&#37327;&#65292;&#20998;&#21035;&#32858;&#28966;&#20110;&#30446;&#26631;&#20998;&#31867;&#12289;&#30446;&#26631;&#26816;&#27979;&#21644;&#36793;&#32536;&#35774;&#22791;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a lot of research effort devoted by researcher into developing different techniques for neural networks compression, yet the community seems to lack standardised ways of evaluating and comparing between different compression techniques, which is key to identifying the most suitable compression technique for different applications. In this paper we contribute towards standardisation of neural network compression by providing a review of evaluation metrics. These metrics have been implemented into NetZIP, a standardised neural network compression bench. We showcase some of the metrics reviewed using three case studies focusing on object classification, object detection, and edge devices.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20351;&#29992;CNN&#35299;&#20915;&#20102;&#29031;&#29255;&#26041;&#21521;&#26816;&#27979;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;&#20351;&#29992;Guided Backpropagation&#33719;&#24471;&#20102;CNN&#26816;&#27979;&#26041;&#21521;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2305.10319</link><description>&lt;p&gt;
&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#21160;&#29031;&#29255;&#26041;&#21521;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Automatic Photo Orientation Detection with Convolutional Neural Networks. (arXiv:2305.10319v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10319
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20351;&#29992;CNN&#35299;&#20915;&#20102;&#29031;&#29255;&#26041;&#21521;&#26816;&#27979;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;&#20351;&#29992;Guided Backpropagation&#33719;&#24471;&#20102;CNN&#26816;&#27979;&#26041;&#21521;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#26469;&#35299;&#20915;&#30830;&#23450;&#28040;&#36153;&#32773;&#29031;&#29255;&#27491;&#30830;&#26041;&#21521;(0&#176;, 90&#176;, 180&#176;&#21644;270&#176;)&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#23545;&#20110;&#27169;&#25311;&#29031;&#29255;&#30340;&#25968;&#23383;&#21270;&#38750;&#24120;&#37325;&#35201;&#12290;&#25105;&#20204;&#22312;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#22312;&#26356;&#22256;&#38590;&#30340;&#28040;&#36153;&#32773;&#29031;&#29255;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#25105;&#20204;&#20351;&#29992;&#24341;&#23548;&#21453;&#21521;&#20256;&#25773;(Guided Backpropagation)&#26469;&#33719;&#24471;&#20851;&#20110;CNN&#22914;&#20309;&#26816;&#27979;&#29031;&#29255;&#26041;&#21521;&#30340;&#35265;&#35299;&#65292;&#24182;&#35299;&#37322;&#20854;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
We apply convolutional neural networks (CNN) to the problem of image orientation detection in the context of determining the correct orientation (from 0, 90, 180, and 270 degrees) of a consumer photo. The problem is especially important for digitazing analog photographs. We substantially improve on the published state of the art in terms of the performance on one of the standard datasets, and test our system on a more difficult large dataset of consumer photos. We use Guided Backpropagation to obtain insights into how our CNN detects photo orientation, and to explain its mistakes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#36890;&#36807;&#27969;&#25968;&#25454;&#25345;&#32493;&#35757;&#32451;CLIP&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#36830;&#32493;&#23398;&#20064;&#26694;&#26550;Mod-X&#65292;&#24182;&#35777;&#26126;&#20869;&#37096;&#26059;&#36716;&#21644;&#36328;&#27169;&#24577;&#20559;&#24046;&#23548;&#33268;&#20102;CLIP&#22312;&#36328;&#27169;&#24577;&#26816;&#32034;&#20219;&#21153;&#20013;&#24615;&#33021;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2305.07437</link><description>&lt;p&gt;
&#24102;&#26377;&#38750;&#23545;&#35282;&#20449;&#24687;&#30340;&#35270;&#35273;-&#35821;&#35328;&#36830;&#32493;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Continual Vision-Language Representaion Learning with Off-Diagonal Information. (arXiv:2305.07437v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#36890;&#36807;&#27969;&#25968;&#25454;&#25345;&#32493;&#35757;&#32451;CLIP&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#36830;&#32493;&#23398;&#20064;&#26694;&#26550;Mod-X&#65292;&#24182;&#35777;&#26126;&#20869;&#37096;&#26059;&#36716;&#21644;&#36328;&#27169;&#24577;&#20559;&#24046;&#23548;&#33268;&#20102;CLIP&#22312;&#36328;&#27169;&#24577;&#26816;&#32034;&#20219;&#21153;&#20013;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#36890;&#36807;&#27969;&#25968;&#25454;&#25345;&#32493;&#35757;&#32451;CLIP&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#12290;&#36890;&#36807;&#36861;&#36394;&#36830;&#32493;&#26356;&#26032;&#30340;CLIP&#27169;&#22411;&#20013;&#34920;&#31034;&#21521;&#37327;&#30340;&#26041;&#21521;&#21464;&#21270;&#65292;&#25105;&#20204;&#25506;&#32034;&#21644;&#24635;&#32467;&#20102;&#36825;&#20123;&#31354;&#38388;&#21464;&#21270;&#65292;&#31216;&#20026;&#31354;&#38388;&#28151;&#20081;&#65288;SD&#65289;&#65292;&#21487;&#20197;&#20998;&#20026;&#20869;&#37096;&#26059;&#36716;&#21644;&#36328;&#27169;&#24577;&#20559;&#24046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#32463;&#39564;&#21644;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#20869;&#37096;&#26059;&#36716;&#21644;&#36328;&#27169;&#24577;&#20559;&#24046;&#22914;&#20309;&#23548;&#33268;CLIP&#22312;&#36328;&#27169;&#24577;&#26816;&#32034;&#20219;&#21153;&#20013;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#32531;&#35299;&#31354;&#38388;&#28151;&#20081;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#36830;&#32493;&#23398;&#20064;&#26694;&#26550;Mod-X: &#32500;&#25252;&#38750;&#23545;&#35282;&#20449;&#24687;&#30697;&#38453;&#12290;&#22312;&#21508;&#31181;&#19981;&#21516;&#35268;&#27169;&#21644;&#33539;&#22260;&#30340;&#24120;&#29992;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper discusses the feasibility of continuously training the CLIP model through streaming data. Then, by tracking the directional changes of the representation vectors in the continuously updated CLIP model, we explore and summarize these spatial variations as Spatial Disorder (SD), which can be divided into Intra-modal Rotation and Inter-modal Deviation. Moreover, we demonstrate how intra-modal rotation and inter-modal deviation lead to a performance decline for CLIP on cross-modal retrieval tasks in both empirically and theoretically. To alleviate the spatial disorder, we propose a simple yet effective continual learning framework Mod-X: Maintain off-diagonal information-matriX. The experiments (in Section \ref{method}, \ref{experiments} and Appendix \ref{Appendix_to_experiments}) on commonly used datasets with different scales and scopes have illustrated the effectiveness of our method.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#22522;&#20110;&#21608;&#26399;&#26102;&#38388;&#35774;&#32622;&#36229;&#21442;&#25968;&#65292;&#20351;&#24471;PPO&#21644;SAC&#22312;&#24191;&#27867;&#30340;&#24490;&#29615;&#26102;&#38388;&#33539;&#22260;&#20869;&#36827;&#34892;&#23398;&#20064;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#25509;&#36817;&#32791;&#26102;&#30340;&#22312;&#32447;&#36229;&#21442;&#25968;&#35843;&#25972;&#33719;&#24471;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.05760</link><description>&lt;p&gt;
&#38477;&#20302;&#29616;&#23454;&#31574;&#30053;&#20248;&#21270;&#20013;&#24490;&#29615;&#26102;&#38388;&#35843;&#25972;&#30340;&#20195;&#20215;
&lt;/p&gt;
&lt;p&gt;
Reducing the Cost of Cycle-Time Tuning for Real-World Policy Optimization. (arXiv:2305.05760v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#22522;&#20110;&#21608;&#26399;&#26102;&#38388;&#35774;&#32622;&#36229;&#21442;&#25968;&#65292;&#20351;&#24471;PPO&#21644;SAC&#22312;&#24191;&#27867;&#30340;&#24490;&#29615;&#26102;&#38388;&#33539;&#22260;&#20869;&#36827;&#34892;&#23398;&#20064;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#25509;&#36817;&#32791;&#26102;&#30340;&#22312;&#32447;&#36229;&#21442;&#25968;&#35843;&#25972;&#33719;&#24471;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#26102;&#38388;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#36890;&#24120;&#20351;&#29992;&#22266;&#23450;&#21608;&#26399;&#26102;&#38388;&#30340;&#31163;&#25955;&#27493;&#39588;&#36827;&#34892;&#25805;&#20316;&#12290;&#23454;&#36341;&#20013;&#38656;&#35201;&#20026;&#32473;&#23450;&#20219;&#21153;&#36873;&#25321;&#25805;&#20316;&#21608;&#26399;&#26102;&#38388;&#65292;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#26159;&#23398;&#20064;&#31639;&#27861;&#30340;&#36229;&#21442;&#25968;&#26159;&#21542;&#38656;&#35201;&#20026;&#27599;&#20010;&#21608;&#26399;&#26102;&#38388;&#37325;&#26032;&#35843;&#25972;&#65292;&#36825;&#23545;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#20154;&#26469;&#35828;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;--PPO&#21644;SAC--&#22312;&#19981;&#21516;&#30340;&#21608;&#26399;&#26102;&#38388;&#19979;&#20351;&#29992;&#22522;&#32447;&#36229;&#21442;&#25968;&#20540;&#30340;&#24773;&#20917;&#12290;&#36890;&#36807;&#20351;&#29992;&#22522;&#32447;&#36229;&#21442;&#25968;&#22312;&#19968;&#20010;&#22522;&#20934;&#20219;&#21153;&#20013;&#23637;&#31034;&#36825;&#20004;&#31181;&#31639;&#27861;&#34920;&#29616;&#33391;&#22909;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#36873;&#25321;&#19981;&#21516;&#20110;&#20219;&#21153;&#40664;&#35748;&#20540;&#30340;&#21608;&#26399;&#26102;&#38388;&#26102;&#65292;&#20351;&#29992;&#22522;&#32447;&#36229;&#21442;&#25968;&#30340;PPO&#26080;&#27861;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#24403;&#36229;&#21442;&#25968;&#29992;&#20110;&#27599;&#20010;&#21608;&#26399;&#26102;&#38388;&#26102;&#65292;&#22522;&#20110;&#22522;&#32447;&#30340;PPO&#21644;SAC&#34920;&#29616;&#22343;&#26126;&#26174;&#21155;&#20110;&#23427;&#20204;&#30340;&#35843;&#25972;&#20540;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21608;&#26399;&#26102;&#38388;&#35774;&#32622;&#36825;&#20123;&#36229;&#21442;&#25968;&#30340;&#26032;&#26041;&#27861;&#12290;&#22312;&#25105;&#20204;&#30340;&#20223;&#30495;&#26426;&#22120;&#20154;&#36816;&#21160;&#20219;&#21153;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#24471;PPO&#21644;SAC&#22312;&#26497;&#20854;&#24191;&#27867;&#30340;&#21608;&#26399;&#26102;&#38388;&#33539;&#22260;&#20869;&#36827;&#34892;&#23398;&#20064;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#25509;&#36817;&#20110;&#32791;&#26102;&#30340;&#22312;&#32447;&#36229;&#21442;&#25968;&#35843;&#25972;&#33719;&#24471;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continuous-time reinforcement learning tasks commonly use discrete steps of fixed cycle times for actions. As practitioners need to choose the action-cycle time for a given task, a significant concern is whether the hyper-parameters of the learning algorithm need to be re-tuned for each choice of the cycle time, which is prohibitive for real-world robotics. In this work, we investigate the widely-used baseline hyper-parameter values of two policy gradient algorithms -- PPO and SAC -- across different cycle times. Using a benchmark task where the baseline hyper-parameters of both algorithms were shown to work well, we reveal that when a cycle time different than the task default is chosen, PPO with baseline hyper-parameters fails to learn. Moreover, both PPO and SAC with their baseline hyper-parameters perform substantially worse than their tuned values for each cycle time. We propose novel approaches for setting these hyper-parameters based on the cycle time. In our experiments on simu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#26368;&#20248;&#36793;&#32536;&#20998;&#24067;&#26426;&#65288;ODM&#65289;&#35757;&#32451;&#26041;&#27861;&#65292;&#19982;&#21407;&#22987;ODM&#35757;&#32451;&#26041;&#27861;&#30456;&#27604;&#65292;&#21487;&#23454;&#29616;&#36817;&#21313;&#20493;&#30340;&#21152;&#36895;&#12290;&#23545;&#20110;&#38750;&#32447;&#24615;&#26680;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#24863;&#30693;&#20998;&#21306;&#26041;&#27861;&#65292;&#20351;&#24471;&#27599;&#20010;&#20998;&#21306;&#19978;&#35757;&#32451;&#30340;&#26412;&#22320;ODM&#25509;&#36817;&#20840;&#23616;&#30340;ODM&#65292;&#24182;&#24555;&#36895;&#25910;&#25947;&#12290;&#22312;&#24212;&#29992;&#32447;&#24615;&#26680;&#26102;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#19968;&#31181;&#36890;&#20449;&#26377;&#25928;&#30340;SVRG&#26041;&#27861;&#20197;&#36827;&#19968;&#27493;&#21152;&#36895;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2305.04837</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#26368;&#20248;&#36793;&#32536;&#20998;&#24067;&#26426;&#65288;Scalable Optimal Margin Distribution Machine&#65289;
&lt;/p&gt;
&lt;p&gt;
Scalable Optimal Margin Distribution Machine. (arXiv:2305.04837v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04837
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#26368;&#20248;&#36793;&#32536;&#20998;&#24067;&#26426;&#65288;ODM&#65289;&#35757;&#32451;&#26041;&#27861;&#65292;&#19982;&#21407;&#22987;ODM&#35757;&#32451;&#26041;&#27861;&#30456;&#27604;&#65292;&#21487;&#23454;&#29616;&#36817;&#21313;&#20493;&#30340;&#21152;&#36895;&#12290;&#23545;&#20110;&#38750;&#32447;&#24615;&#26680;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#24863;&#30693;&#20998;&#21306;&#26041;&#27861;&#65292;&#20351;&#24471;&#27599;&#20010;&#20998;&#21306;&#19978;&#35757;&#32451;&#30340;&#26412;&#22320;ODM&#25509;&#36817;&#20840;&#23616;&#30340;ODM&#65292;&#24182;&#24555;&#36895;&#25910;&#25947;&#12290;&#22312;&#24212;&#29992;&#32447;&#24615;&#26680;&#26102;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#19968;&#31181;&#36890;&#20449;&#26377;&#25928;&#30340;SVRG&#26041;&#27861;&#20197;&#36827;&#19968;&#27493;&#21152;&#36895;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20248;&#36793;&#32536;&#20998;&#24067;&#26426;&#65288;ODM&#65289;&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#32479;&#35745;&#23398;&#20064;&#26694;&#26550;&#65292;&#26681;&#25454;&#26032;&#30340;&#36793;&#32536;&#29702;&#35770;&#24314;&#31435;&#65292;&#34920;&#29616;&#20986;&#27604;&#20256;&#32479;&#30340;&#22522;&#20110;&#22823;&#38388;&#38548;&#30340;&#23545;&#24212;&#26041;&#27861;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20687;&#20854;&#20182;&#26680;&#26041;&#27861;&#19968;&#26679;&#65292;&#23427;&#22312;&#35745;&#31639;&#26102;&#38388;&#21644;&#20869;&#23384;&#26041;&#38754;&#26222;&#36941;&#23384;&#22312;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;ODM&#65292;&#19982;&#21407;&#22987;ODM&#35757;&#32451;&#26041;&#27861;&#30456;&#27604;&#65292;&#21487;&#23454;&#29616;&#36817;&#21313;&#20493;&#30340;&#21152;&#36895;&#12290;&#23545;&#20110;&#38750;&#32447;&#24615;&#26680;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#24863;&#30693;&#20998;&#21306;&#26041;&#27861;&#65292;&#20351;&#24471;&#27599;&#20010;&#20998;&#21306;&#19978;&#35757;&#32451;&#30340;&#26412;&#22320;ODM&#25509;&#36817;&#20840;&#23616;&#30340;ODM&#65292;&#24182;&#24555;&#36895;&#25910;&#25947;&#12290;&#24403;&#24212;&#29992;&#32447;&#24615;&#26680;&#26102;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#19968;&#31181;&#36890;&#20449;&#26377;&#25928;&#30340;SVRG&#26041;&#27861;&#20197;&#36827;&#19968;&#27493;&#21152;&#36895;&#35757;&#32451;&#12290;&#22823;&#37327;&#32463;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#26497;&#39640;&#65292;&#24182;&#19988;&#20960;&#20046;&#19981;&#20250;&#24694;&#21270;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimal margin Distribution Machine (ODM) is a newly proposed statistical learning framework rooting in the novel margin theory, which demonstrates better generalization performance than the traditional large margin based counterparts. Nonetheless, it suffers from the ubiquitous scalability problem regarding both computation time and memory as other kernel methods. This paper proposes a scalable ODM, which can achieve nearly ten times speedup compared to the original ODM training method. For nonlinear kernels, we propose a novel distribution-aware partition method to make the local ODM trained on each partition be close and converge fast to the global one. When linear kernel is applied, we extend a communication efficient SVRG method to accelerate the training further. Extensive empirical studies validate that our proposed method is highly computational efficient and almost never worsen the generalization.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23558;&#38464;&#34746;&#30690;&#37327;&#31354;&#38388;&#20013;&#30340;&#19968;&#20123;&#27010;&#24565;&#25512;&#24191;&#21040;SPD&#21644;Grassmann&#27969;&#24418;&#65292;&#25552;&#20986;&#20102;&#22312;&#36825;&#20123;&#27969;&#24418;&#19978;&#26500;&#24314;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#27169;&#22411;&#21644;&#26032;&#23618;&#65292;&#24182;&#22312;&#20154;&#31867;&#21160;&#20316;&#35782;&#21035;&#21644;&#30693;&#35782;&#22270;&#35889;&#23436;&#25104;&#20004;&#20010;&#24212;&#29992;&#20013;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.04560</link><description>&lt;p&gt;
&#22522;&#20110;&#30697;&#38453;&#27969;&#24418;&#30340;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#65306;&#38464;&#34746;&#30690;&#37327;&#31354;&#38388;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Building Neural Networks on Matrix Manifolds: A Gyrovector Space Approach. (arXiv:2305.04560v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04560
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23558;&#38464;&#34746;&#30690;&#37327;&#31354;&#38388;&#20013;&#30340;&#19968;&#20123;&#27010;&#24565;&#25512;&#24191;&#21040;SPD&#21644;Grassmann&#27969;&#24418;&#65292;&#25552;&#20986;&#20102;&#22312;&#36825;&#20123;&#27969;&#24418;&#19978;&#26500;&#24314;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#27169;&#22411;&#21644;&#26032;&#23618;&#65292;&#24182;&#22312;&#20154;&#31867;&#21160;&#20316;&#35782;&#21035;&#21644;&#30693;&#35782;&#22270;&#35889;&#23436;&#25104;&#20004;&#20010;&#24212;&#29992;&#20013;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30697;&#38453;&#27969;&#24418;&#65292;&#22914;&#23545;&#31216;&#27491;&#23450;&#65288;SPD&#65289;&#30697;&#38453;&#21644;Grassmann&#27969;&#24418;&#65292;&#20986;&#29616;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#12290;&#26368;&#36817;&#65292;&#36890;&#36807;&#24212;&#29992;&#38464;&#34746;&#32676;&#21644;&#38464;&#34746;&#30690;&#37327;&#31354;&#38388;&#30340;&#29702;&#35770;&#8212;&#8212;&#36825;&#26159;&#19968;&#20010;&#30740;&#31350;&#21452;&#26354;&#20960;&#20309;&#30340;&#24378;&#22823;&#26694;&#26550;&#8212;&#8212;&#19968;&#20123;&#24037;&#20316;&#23581;&#35797;&#22312;&#30697;&#38453;&#27969;&#24418;&#19978;&#26500;&#24314;&#27431;&#20960;&#37324;&#24503;&#31070;&#32463;&#32593;&#32476;&#30340;&#21407;&#21017;&#24615;&#25512;&#24191;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#32771;&#34385;&#27969;&#24418;&#30340;&#20869;&#31215;&#21644;&#38464;&#34746;&#35282;&#31561;&#27010;&#24565;&#30340;&#38464;&#34746;&#30690;&#37327;&#31354;&#38388;&#65292;&#30456;&#27604;&#20110;&#29992;&#20110;&#30740;&#31350;&#21452;&#26354;&#20960;&#20309;&#30340;&#37027;&#20123;&#27010;&#24565;&#65292;&#36825;&#20123;&#24037;&#20316;&#25552;&#20379;&#30340;&#25216;&#26415;&#21644;&#25968;&#23398;&#24037;&#20855;&#20173;&#28982;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#38464;&#34746;&#30690;&#37327;&#31354;&#38388;&#20013;&#30340;&#19968;&#20123;&#27010;&#24565;&#25512;&#24191;&#21040;SPD&#21644;Grassmann&#27969;&#24418;&#65292;&#24182;&#25552;&#20986;&#20102;&#22312;&#36825;&#20123;&#27969;&#24418;&#19978;&#26500;&#24314;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#27169;&#22411;&#21644;&#26032;&#23618;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20154;&#31867;&#21160;&#20316;&#35782;&#21035;&#21644;&#30693;&#35782;&#22270;&#35889;&#23436;&#25104;&#20004;&#20010;&#24212;&#29992;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Matrix manifolds, such as manifolds of Symmetric Positive Definite (SPD) matrices and Grassmann manifolds, appear in many applications. Recently, by applying the theory of gyrogroups and gyrovector spaces that is a powerful framework for studying hyperbolic geometry, some works have attempted to build principled generalizations of Euclidean neural networks on matrix manifolds. However, due to the lack of many concepts in gyrovector spaces for the considered manifolds, e.g., the inner product and gyroangles, techniques and mathematical tools provided by these works are still limited compared to those developed for studying hyperbolic geometry. In this paper, we generalize some notions in gyrovector spaces for SPD and Grassmann manifolds, and propose new models and layers for building neural networks on these manifolds. We show the effectiveness of our approach in two applications, i.e., human action recognition and knowledge graph completion.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;D-MFDAL&#65292;&#23427;&#21487;&#20197;&#23398;&#20064;&#20855;&#26377;&#22810;&#20010;&#20445;&#30495;&#24230;&#30340;&#20989;&#25968;&#20998;&#24067;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#39640;&#26031;&#36807;&#31243;&#30340;&#28789;&#27963;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.04392</link><description>&lt;p&gt;
&#22810;&#20445;&#30495;&#24230;&#28145;&#24230;&#36125;&#21494;&#26031;&#20027;&#21160;&#23398;&#20064;&#30340;&#35299;&#32544;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Disentangled Multi-Fidelity Deep Bayesian Active Learning. (arXiv:2305.04392v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04392
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;D-MFDAL&#65292;&#23427;&#21487;&#20197;&#23398;&#20064;&#20855;&#26377;&#22810;&#20010;&#20445;&#30495;&#24230;&#30340;&#20989;&#25968;&#20998;&#24067;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#39640;&#26031;&#36807;&#31243;&#30340;&#28789;&#27963;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#24179;&#34913;&#36136;&#37327;&#21644;&#25104;&#26412;&#65292;&#31185;&#23398;&#21644;&#24037;&#31243;&#30340;&#22810;&#20010;&#39046;&#22495;&#20250;&#22312;&#22810;&#20010;&#22797;&#26434;&#31243;&#24230;&#19978;&#36816;&#34892;&#27169;&#25311;&#12290;&#22810;&#20445;&#30495;&#24230;&#20027;&#21160;&#23398;&#20064;&#26088;&#22312;&#36890;&#36807;&#20174;&#22810;&#20010;&#20445;&#30495;&#24230;&#27700;&#24179;&#31215;&#26497;&#22320;&#33719;&#21462;&#25968;&#25454;&#26469;&#23398;&#20064;&#20174;&#36755;&#20837;&#21442;&#25968;&#21040;&#27169;&#25311;&#36755;&#20986;&#30340;&#30452;&#25509;&#26144;&#23556;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#30340;&#26041;&#27861;&#24456;&#38590;&#25193;&#23637;&#21040;&#39640;&#32500;&#25968;&#25454;&#12290;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#36890;&#24120;&#22312;&#38544;&#34255;&#34920;&#31034;&#20013;&#24378;&#21046;&#23454;&#26045;&#20998;&#23618;&#32467;&#26500;&#65292;&#20165;&#25903;&#25345;&#20174;&#20302;&#20445;&#30495;&#24230;&#21040;&#39640;&#20445;&#30495;&#24230;&#20256;&#36882;&#20449;&#24687;&#12290;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#20174;&#20302;&#20445;&#30495;&#24230;&#34920;&#31034;&#21040;&#39640;&#20445;&#30495;&#24230;&#34920;&#31034;&#20013;&#19981;&#33391;&#35823;&#24046;&#30340;&#20256;&#25773;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#35299;&#32544;&#22810;&#20445;&#30495;&#24230;&#28145;&#24230;&#36125;&#21494;&#26031;&#20027;&#21160;&#23398;&#20064;&#65288;D-MFDAL&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23398;&#20064;&#20102;&#22312;&#22810;&#20010;&#20445;&#30495;&#24230;&#19978;&#20989;&#25968;&#20998;&#24067;&#30340;&#26465;&#20214;&#19979;&#30340;&#20195;&#29702;&#27169;&#22411;&#12290;&#22312;&#23398;&#20064;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#28145;&#24230;&#20195;&#29702;&#30340;&#22522;&#20934;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#39640;&#26031;&#36807;&#31243;&#30340;&#28789;&#27963;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
To balance quality and cost, various domain areas of science and engineering run simulations at multiple levels of sophistication. Multi-fidelity active learning aims to learn a direct mapping from input parameters to simulation outputs at the highest fidelity by actively acquiring data from multiple fidelity levels. However, existing approaches based on Gaussian processes are hardly scalable to high-dimensional data. Deep learning-based methods often impose a hierarchical structure in hidden representations, which only supports passing information from low-fidelity to high-fidelity. These approaches can lead to the undesirable propagation of errors from low-fidelity representations to high-fidelity ones. We propose a novel framework called Disentangled Multi-fidelity Deep Bayesian Active Learning (D-MFDAL), that learns the surrogate models conditioned on the distribution of functions at multiple fidelities. On benchmark tasks of learning deep surrogates of partial differential equatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#25193;&#25955;ODEs&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#25913;&#36827;&#25216;&#26415;&#65292;&#21253;&#25324;&#36895;&#24230;&#21442;&#25968;&#21270;&#21644;&#26041;&#24046;&#20943;&#23569;&#25216;&#26415;&#31561;&#29992;&#20110;&#35757;&#32451;&#30340;&#25216;&#26415;&#65292;&#20197;&#21450;&#35823;&#24046;&#26377;&#30028;&#30340;&#39640;&#38454;&#27969;&#21305;&#37197;&#30446;&#26631;&#29992;&#20110;&#24494;&#35843;&#21644;&#25130;&#26029;&#27491;&#24577;&#21435;&#37327;&#21270;&#26041;&#27861;&#29992;&#20110;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2305.03935</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;ODEs&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#25913;&#36827;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Improved Techniques for Maximum Likelihood Estimation for Diffusion ODEs. (arXiv:2305.03935v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03935
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#25193;&#25955;ODEs&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#25913;&#36827;&#25216;&#26415;&#65292;&#21253;&#25324;&#36895;&#24230;&#21442;&#25968;&#21270;&#21644;&#26041;&#24046;&#20943;&#23569;&#25216;&#26415;&#31561;&#29992;&#20110;&#35757;&#32451;&#30340;&#25216;&#26415;&#65292;&#20197;&#21450;&#35823;&#24046;&#26377;&#30028;&#30340;&#39640;&#38454;&#27969;&#21305;&#37197;&#30446;&#26631;&#29992;&#20110;&#24494;&#35843;&#21644;&#25130;&#26029;&#27491;&#24577;&#21435;&#37327;&#21270;&#26041;&#27861;&#29992;&#20110;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#21508;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#25193;&#25955;&#27169;&#22411;&#30340;&#27010;&#29575;&#27969;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#65288;&#21363;&#25193;&#25955;ODE&#65289;&#26159;&#36830;&#32493;&#24402;&#19968;&#21270;&#27969;&#65288;CNFs&#65289;&#30340;&#19968;&#20010;&#29305;&#20363;&#65292;&#23427;&#20351;&#24471;&#30830;&#23450;&#24615;&#25512;&#26029;&#21644;&#31934;&#30830;&#20284;&#28982;&#35780;&#20272;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#20284;&#28982;&#30340;&#29983;&#25104;&#27169;&#22411;&#30456;&#27604;&#65292;&#25193;&#25955;ODE&#30340;&#20284;&#28982;&#20272;&#35745;&#32467;&#26524;&#20173;&#26377;&#24456;&#22823;&#24046;&#36317;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#25913;&#36827;&#30340;&#25216;&#26415;&#65292;&#21253;&#25324;&#35757;&#32451;&#21644;&#35780;&#20272;&#20004;&#20010;&#26041;&#38754;&#65292;&#29992;&#20110;&#25193;&#25955;ODE&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#12290;&#23545;&#20110;&#35757;&#32451;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36895;&#24230;&#21442;&#25968;&#21270;&#65292;&#24182;&#25506;&#32034;&#26041;&#24046;&#20943;&#23569;&#25216;&#26415;&#20197;&#21152;&#24555;&#25910;&#25947;&#36895;&#24230;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#35823;&#24046;&#26377;&#30028;&#30340;&#39640;&#38454;&#27969;&#21305;&#37197;&#30446;&#26631;&#29992;&#20110;&#24494;&#35843;&#65292;&#20174;&#32780;&#25552;&#39640;ODE&#30340;&#20284;&#28982;&#20272;&#35745;&#24182;&#24179;&#28369;&#20854;&#36712;&#36857;&#12290;&#23545;&#20110;&#35780;&#20272;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#39035;&#35757;&#32451;&#30340;&#25130;&#26029;&#27491;&#24577;&#21435;&#37327;&#21270;&#26041;&#27861;&#26469;&#22635;&#34917;&#35757;&#32451;-&#35780;&#20272;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have exhibited excellent performance in various domains. The probability flow ordinary differential equation (ODE) of diffusion models (i.e., diffusion ODEs) is a particular case of continuous normalizing flows (CNFs), which enables deterministic inference and exact likelihood evaluation. However, the likelihood estimation results by diffusion ODEs are still far from those of the state-of-the-art likelihood-based generative models. In this work, we propose several improved techniques for maximum likelihood estimation for diffusion ODEs, including both training and evaluation perspectives. For training, we propose velocity parameterization and explore variance reduction techniques for faster convergence. We also derive an error-bounded high-order flow matching objective for finetuning, which improves the ODE likelihood and smooths its trajectory. For evaluation, we propose a novel training-free truncated-normal dequantization to fill the training-evaluation gap commonly
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;GPT-3.5&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#20013;&#65292;&#23454;&#29616;&#22312;&#22235;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#26032;&#30340;&#26368;&#20248;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#25351;&#23548;&#35828;&#26126;&#21644;&#32422;&#26463;&#27169;&#24335;&#19979;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.01555</link><description>&lt;p&gt;
&#22914;&#20309;&#21457;&#25381;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#20013;&#30340;&#33021;&#21147;&#65311;
&lt;/p&gt;
&lt;p&gt;
How to Unleash the Power of Large Language Models for Few-shot Relation Extraction?. (arXiv:2305.01555v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01555
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;GPT-3.5&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#20013;&#65292;&#23454;&#29616;&#22312;&#22235;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#26032;&#30340;&#26368;&#20248;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#25351;&#23548;&#35828;&#26126;&#21644;&#32422;&#26463;&#27169;&#24335;&#19979;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#25193;&#23637;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#24191;&#27867;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#20294;&#26159;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#36824;&#27809;&#26377;&#24471;&#21040;&#20840;&#38754;&#25506;&#32034;&#12290;&#26412;&#25991;&#36890;&#36807;&#35814;&#32454;&#23454;&#39564;&#65292;&#30740;&#31350;&#20102;&#20351;&#29992;GPT-3.5&#36827;&#34892;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#30340;&#22522;&#26412;&#26041;&#27861;&#8212;&#8212;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#25968;&#25454;&#29983;&#25104;&#12290;&#20026;&#20102;&#22686;&#24378;&#23569;&#26679;&#26412;&#24615;&#33021;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#25351;&#23548;&#35828;&#26126;&#21644;&#32422;&#26463;&#27169;&#24335;&#19979;&#30340;&#25968;&#25454;&#29983;&#25104;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#23454;&#29616;&#19982;&#20197;&#21069;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#32780;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#25454;&#29983;&#25104;&#21487;&#20197;&#25512;&#21160;&#20197;&#21069;&#30340;&#35299;&#20915;&#26041;&#26696;&#20197;&#22312;&#22235;&#20010;&#24191;&#27867;&#30740;&#31350;&#30340;&#20851;&#31995;&#25277;&#21462;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#23569;&#26679;&#26412;&#32467;&#26524;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#24037;&#20316;&#21487;&#20197;&#28608;&#21457;&#26410;&#26469;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#20013;&#30340;&#33021;&#21147;&#30340;&#30740;&#31350;&#12290;&#20195;&#30721;&#21487;&#20197;&#22312; \url{https://github.com/zjunlp/DeepKE/tree/main/example/llm} &#20013;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scaling language models have revolutionized widespread NLP tasks, yet little comprehensively explored few-shot relation extraction with large language models. In this paper, we investigate principal methodologies, in-context learning and data generation, for few-shot relation extraction via GPT-3.5 through exhaustive experiments. To enhance few-shot performance, we further propose task-related instructions and schema-constrained data generation. We observe that in-context learning can achieve performance on par with previous prompt learning approaches, and data generation with the large language model can boost previous solutions to obtain new state-of-the-art few-shot results on four widely-studied relation extraction datasets. We hope our work can inspire future research for the capabilities of large language models in few-shot relation extraction. Code is available in \url{https://github.com/zjunlp/DeepKE/tree/main/example/llm.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#24930;&#28151;&#21512;&#36807;&#31243;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#32473;&#20986;&#20102;&#23545;&#30001;&#24179;&#31283;&#19988;phi&#28151;&#21512;&#36807;&#31243;&#29983;&#25104;&#30340;&#25968;&#25454;&#30340;&#19981;&#21516;&#31181;&#31867;&#25439;&#22833;&#31867;&#21035;&#30340;&#19968;&#31181;&#19978;&#30028;&#12290;</title><link>http://arxiv.org/abs/2305.00977</link><description>&lt;p&gt;
&#24930;&#28151;&#21512;&#36807;&#31243;&#30340;&#27867;&#21270;&#33021;&#21147;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Generalization for slowly mixing processes. (arXiv:2305.00977v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00977
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#24930;&#28151;&#21512;&#36807;&#31243;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#32473;&#20986;&#20102;&#23545;&#30001;&#24179;&#31283;&#19988;phi&#28151;&#21512;&#36807;&#31243;&#29983;&#25104;&#30340;&#25968;&#25454;&#30340;&#19981;&#21516;&#31181;&#31867;&#25439;&#22833;&#31867;&#21035;&#30340;&#19968;&#31181;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32473;&#20986;&#20102;&#38024;&#23545;&#30001;&#24179;&#31283;&#19988;phi&#28151;&#21512;&#36807;&#31243;&#29983;&#25104;&#30340;&#25968;&#25454;&#30340;&#21508;&#31181;&#25439;&#22833;&#31867;&#21035;&#30340;&#19968;&#31181;&#19978;&#30028;&#65292;&#20854;&#20013;&#28151;&#21512;&#26102;&#38388;&#65288;&#33719;&#24471;&#36817;&#20284;&#29420;&#31435;&#25152;&#38656;&#30340;&#26102;&#38388;&#65289;&#20165;&#20197;&#21152;&#27861;&#26041;&#24335;&#36827;&#20837;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#23545;&#20110;&#24930;&#36895;&#28151;&#21512;&#36807;&#31243;&#32780;&#35328;&#65292;&#36825;&#21487;&#20197;&#26159;&#20854;&#20248;&#21183;&#65292;&#22240;&#20026;&#20854;&#19982;&#28151;&#21512;&#26102;&#38388;&#30340;&#20056;&#27861;&#20381;&#36182;&#24615;&#30456;&#27604;&#35201;&#22909;&#12290;&#20801;&#35768;&#30340;&#25439;&#22833;&#31867;&#21035;&#21253;&#25324;&#20855;&#26377;&#25351;&#23450;Lipschitz&#24402;&#19968;&#21270;&#25110;&#24179;&#28369;&#24230;&#21442;&#25968;&#30340;&#20989;&#25968;&#12290;&#35813;&#19978;&#30028;&#36824;&#21487;&#20197;&#24212;&#29992;&#20110;&#23545;&#26080;&#38480;&#21046;&#25439;&#22833;&#31867;&#21035;&#30340;&#32479;&#19968;&#24615;&#30740;&#31350;&#65292;&#20854;&#21462;&#20915;&#20110;&#26679;&#26412;&#36335;&#24452;&#19978;&#20989;&#25968;&#30340;&#23616;&#37096;Lipschitz&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
A bound uniform over various loss-classes is given for data generated by stationary and phi-mixing processes, where the mixing time (the time needed to obtain approximate independence) enters the sample complexity only in an additive way. For slowly mixing processes this can be a considerable advantage over results with multiplicative dependence on the mixing time. The admissible loss-classes include functions with prescribed Lipschitz norms or smoothness parameters. The bound can also be applied to be uniform over unconstrained loss-classes, where it depends on local Lipschitz properties of the function on the sample path.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#30740;&#31350;&#20102;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#25913;&#36827;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#30340;&#26368;&#36817;&#30740;&#31350;&#65292;&#21253;&#25324;&#23545;&#21453;&#39304;&#30340;&#20840;&#38754;&#24418;&#24335;&#21270;&#12289;&#21453;&#39304;&#30340;&#26684;&#24335;&#21644;&#30446;&#30340;&#30340;&#25551;&#36848;&#65292;&#21644;&#25552;&#20986;&#20351;&#29992;&#21453;&#39304;&#30340;&#20004;&#31181;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#29992;&#20110;&#20154;&#31867;&#21453;&#39304;&#25968;&#25454;&#25910;&#38598;&#30340;&#29616;&#26377;&#25968;&#25454;&#38598;&#65292;&#21644;&#30456;&#20851;&#30340;&#25285;&#24551;&#12290;</title><link>http://arxiv.org/abs/2305.00955</link><description>&lt;p&gt;
&#26550;&#36215;&#26725;&#26753;&#65306;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20013;&#25972;&#21512;&#65288;&#20154;&#31867;&#65289;&#21453;&#39304;&#30340;&#35843;&#26597;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation. (arXiv:2305.00955v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#30740;&#31350;&#20102;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#25913;&#36827;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#30340;&#26368;&#36817;&#30740;&#31350;&#65292;&#21253;&#25324;&#23545;&#21453;&#39304;&#30340;&#20840;&#38754;&#24418;&#24335;&#21270;&#12289;&#21453;&#39304;&#30340;&#26684;&#24335;&#21644;&#30446;&#30340;&#30340;&#25551;&#36848;&#65292;&#21644;&#25552;&#20986;&#20351;&#29992;&#21453;&#39304;&#30340;&#20004;&#31181;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#29992;&#20110;&#20154;&#31867;&#21453;&#39304;&#25968;&#25454;&#25910;&#38598;&#30340;&#29616;&#26377;&#25968;&#25454;&#38598;&#65292;&#21644;&#30456;&#20851;&#30340;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#30340;&#36827;&#23637;&#37117;&#26159;&#22522;&#20110;&#21033;&#29992;&#20114;&#32852;&#32593;&#35268;&#27169;&#30340;&#25968;&#25454;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#33539;&#24335;&#21487;&#33021;&#20250;&#23548;&#33268;&#29983;&#25104;&#26377;&#23475;&#12289;&#19981;&#20934;&#30830;&#21644;&#26080;&#29992;&#20869;&#23481;&#30340;&#27169;&#22411;&#65292;&#32780;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#36890;&#24120;&#26080;&#27861;&#35782;&#21035;&#36825;&#20123;&#34892;&#20026;&#12290;&#38543;&#30528;&#27169;&#22411;&#21464;&#24471;&#26356;&#21152;&#24378;&#22823;&#65292;&#20154;&#31867;&#21453;&#39304;&#25104;&#20026;&#35780;&#20215;&#21644;&#25913;&#36827;&#27169;&#22411;&#30340;&#23453;&#36149;&#20449;&#21495;&#12290;&#26412;&#35843;&#26597;&#26088;&#22312;&#27010;&#36848;&#26368;&#36817;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#25913;&#36827;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#30340;&#30740;&#31350;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#23545;&#21453;&#39304;&#30340;&#20840;&#38754;&#24418;&#24335;&#21270;&#65292;&#24182;&#26681;&#25454;&#36825;&#31181;&#24418;&#24335;&#21270;&#23558;&#29616;&#26377;&#30740;&#31350;&#36827;&#34892;&#20998;&#31867;&#21644;&#32452;&#32455;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#21453;&#39304;&#21487;&#20197;&#36890;&#36807;&#20854;&#26684;&#24335;&#21644;&#30446;&#30340;&#26469;&#25551;&#36848;&#65292;&#24182;&#28085;&#30422;&#20102;&#25552;&#20986;&#20351;&#29992;&#21453;&#39304;&#30340;&#20004;&#31181;&#26041;&#27861;&#65288;&#29992;&#20110;&#35757;&#32451;&#25110;&#35299;&#30721;&#65289;&#65306;&#30452;&#25509;&#20351;&#29992;&#21453;&#39304;&#25110;&#35757;&#32451;&#21453;&#39304;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#29992;&#20110;&#20154;&#31867;&#21453;&#39304;&#25968;&#25454;&#25910;&#38598;&#30340;&#29616;&#26377;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#30456;&#20851;&#30340;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many recent advances in natural language generation have been fueled by training large language models on internet-scale data. However, this paradigm can lead to models that generate toxic, inaccurate, and unhelpful content, and automatic evaluation metrics often fail to identify these behaviors. As models become more capable, human feedback is an invaluable signal for evaluating and improving models. This survey aims to provide an overview of the recent research that has leveraged human feedback to improve natural language generation. First, we introduce an encompassing formalization of feedback, and identify and organize existing research into a taxonomy following this formalization. Next, we discuss how feedback can be described by its format and objective, and cover the two approaches proposed to use feedback (either for training or decoding): directly using the feedback or training feedback models. We also discuss existing datasets for human-feedback data collection, and concerns 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#22522;&#20110;&#23454;&#38469;&#36816;&#34892;&#25968;&#25454;&#65292;&#20998;&#26512;&#20102;&#30005;&#21147;&#31995;&#32479;&#34892;&#20026;&#27169;&#24335;&#21450;&#27169;&#22411;&#27867;&#21270;&#39118;&#38505;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#24573;&#30053;&#30005;&#32593;&#29305;&#23450;&#27169;&#24335;&#21487;&#33021;&#23548;&#33268;&#23545;&#26032;&#36755;&#20837;&#36755;&#20986;&#19981;&#21487;&#34892;&#12289;&#19981;&#21487;&#23454;&#29616;&#25110;&#23436;&#20840;&#26080;&#24847;&#20041;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2304.10702</link><description>&lt;p&gt;
&#30005;&#21147;&#31995;&#32479;&#34892;&#20026;&#27169;&#24335;&#21450;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#27867;&#21270;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Power Grid Behavioral Patterns and Risks of Generalization in Applied Machine Learning. (arXiv:2304.10702v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10702
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#22522;&#20110;&#23454;&#38469;&#36816;&#34892;&#25968;&#25454;&#65292;&#20998;&#26512;&#20102;&#30005;&#21147;&#31995;&#32479;&#34892;&#20026;&#27169;&#24335;&#21450;&#27169;&#22411;&#27867;&#21270;&#39118;&#38505;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#24573;&#30053;&#30005;&#32593;&#29305;&#23450;&#27169;&#24335;&#21487;&#33021;&#23548;&#33268;&#23545;&#26032;&#36755;&#20837;&#36755;&#20986;&#19981;&#21487;&#34892;&#12289;&#19981;&#21487;&#23454;&#29616;&#25110;&#23436;&#20840;&#26080;&#24847;&#20041;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20986;&#29616;&#20102;&#22823;&#37327;&#38024;&#23545;&#30005;&#21147;&#31995;&#32479;&#24212;&#29992;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#30340;&#25991;&#29486;&#12290;&#28982;&#32780;&#65292;&#24573;&#30053;&#39046;&#22495;&#30693;&#35782;&#20250;&#23545;&#36825;&#20123;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#36896;&#25104;&#39640;&#39118;&#38505;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#24573;&#30053;&#30005;&#32593;&#29305;&#23450;&#30340;&#26102;&#31354;&#27169;&#24335;&#65288;&#36127;&#33655;&#12289;&#21457;&#30005;&#21644;&#25299;&#25169;&#31561;&#65289;&#21487;&#33021;&#23548;&#33268;&#23545;&#26032;&#36755;&#20837;&#36755;&#20986;&#19981;&#21487;&#34892;&#12289;&#19981;&#21487;&#23454;&#29616;&#25110;&#23436;&#20840;&#26080;&#24847;&#20041;&#30340;&#39044;&#27979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#35843;&#26597;&#23454;&#38469;&#36816;&#34892;&#25968;&#25454;&#65292;&#25552;&#20379;&#30005;&#21147;&#31995;&#32479;&#34892;&#20026;&#27169;&#24335;&#30340;&#35265;&#35299;&#65292;&#21253;&#25324;&#26102;&#21464;&#30340;&#25299;&#25169;&#12289;&#36127;&#33655;&#21644;&#21457;&#30005;&#20197;&#21450;&#21333;&#20010;&#36127;&#33655;&#21644;&#21457;&#30005;&#20043;&#38388;&#30340;&#31354;&#38388;&#24046;&#24322;&#65288;&#22312;&#23792;&#20540;&#26102;&#38388;&#12289;&#22810;&#31181;&#39118;&#26684;&#19979;&#65289;&#12290;&#28982;&#21518;&#26681;&#25454;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#65292;&#35780;&#20272;&#20102;&#19968;&#20123;&#29616;&#26377;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#27169;&#22411;&#35774;&#35745;&#21644;&#35757;&#32451;&#20013;&#24573;&#30053;&#36825;&#20123;&#30005;&#32593;&#29305;&#23450;&#27169;&#24335;&#25152;&#36896;&#25104;&#30340;&#27867;&#21270;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have seen a rich literature of data-driven approaches designed for power grid applications. However, insufficient consideration of domain knowledge can impose a high risk to the practicality of the methods. Specifically, ignoring the grid-specific spatiotemporal patterns (in load, generation, and topology, etc.) can lead to outputting infeasible, unrealizable, or completely meaningless predictions on new inputs. To address this concern, this paper investigates real-world operational data to provide insights into power grid behavioral patterns, including the time-varying topology, load, and generation, as well as the spatial differences (in peak hours, diverse styles) between individual loads and generations. Then based on these observations, we evaluate the generalization risks in some existing ML works causedby ignoring these grid-specific patterns in model design and training.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37319;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;Graph Deviation Network (GDN)&#26469;&#25429;&#25417;&#27827;&#27969;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#22797;&#26434;&#26102;&#31354;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#22791;&#29992;&#24322;&#24120;&#38408;&#20540;&#26631;&#20934;GDN+&#65292;&#20197;&#23454;&#29616;&#23545;&#27700;&#36136;&#30340;&#20934;&#30830;&#25345;&#32493;&#30417;&#27979;&#12290;</title><link>http://arxiv.org/abs/2304.09367</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#27827;&#27969;&#31995;&#32479;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Network-Based Anomaly Detection for River Network Systems. (arXiv:2304.09367v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09367
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37319;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;Graph Deviation Network (GDN)&#26469;&#25429;&#25417;&#27827;&#27969;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#22797;&#26434;&#26102;&#31354;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#22791;&#29992;&#24322;&#24120;&#38408;&#20540;&#26631;&#20934;GDN+&#65292;&#20197;&#23454;&#29616;&#23545;&#27700;&#36136;&#30340;&#20934;&#30830;&#25345;&#32493;&#30417;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27700;&#26159;&#27827;&#27969;&#32593;&#32476;&#30340;&#29983;&#21629;&#32447;&#65292;&#20854;&#36136;&#37327;&#23545;&#32500;&#25252;&#27700;&#29983;&#24577;&#31995;&#32479;&#21644;&#20154;&#31867;&#31038;&#20250;&#37117;&#26377;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#29616;&#22330;&#20256;&#24863;&#22120;&#25216;&#26415;&#36234;&#26469;&#36234;&#20381;&#36182;&#23454;&#26102;&#30417;&#27979;&#27700;&#36136;&#12290;&#24322;&#24120;&#26816;&#27979;&#26159;&#35782;&#21035;&#20256;&#24863;&#22120;&#25968;&#25454;&#20013;&#38169;&#35823;&#27169;&#24335;&#30340;&#20851;&#38190;&#27493;&#39588;&#65292;&#20294;&#30001;&#20110;&#25968;&#25454;&#22797;&#26434;&#24615;&#21644;&#21464;&#24322;&#24615;&#65292;&#21363;&#20351;&#22312;&#27491;&#24120;&#24773;&#20917;&#19979;&#20063;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#27827;&#27969;&#32593;&#32476;&#20256;&#24863;&#22120;&#25968;&#25454;&#24322;&#24120;&#26816;&#27979;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36825;&#23545;&#20110;&#31934;&#30830;&#25345;&#32493;&#30417;&#27979;&#27700;&#36136;&#38750;&#24120;&#37325;&#35201;&#12290;&#25105;&#20204;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#8212;&#8212;&#26368;&#36817;&#25552;&#20986;&#30340;Graph Deviation Network (GDN)&#65292;&#23427;&#21033;&#29992;&#22522;&#20110;&#22270;&#27880;&#24847;&#21147;&#30340;&#39044;&#27979;&#26469;&#25429;&#25417;&#20256;&#24863;&#22120;&#20043;&#38388;&#22797;&#26434;&#30340;&#26102;&#31354;&#20851;&#31995;&#12290;&#25105;&#20204;&#26681;&#25454;&#25152;&#23398;&#22270;&#24418;&#25552;&#20986;&#20102;&#27169;&#22411;GDN+&#30340;&#22791;&#29992;&#24322;&#24120;&#38408;&#20540;&#26631;&#20934;&#12290;&#20026;&#20102;&#35780;&#20272;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#30340;&#22522;&#20934;&#20223;&#30495;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Water is the lifeblood of river networks, and its quality plays a crucial role in sustaining both aquatic ecosystems and human societies. Real-time monitoring of water quality is increasingly reliant on in-situ sensor technology. Anomaly detection is crucial for identifying erroneous patterns in sensor data, but can be a challenging task due to the complexity and variability of the data, even under normal conditions. This paper presents a solution to the challenging task of anomaly detection for river network sensor data, which is essential for the accurate and continuous monitoring of water quality. We use a graph neural network model, the recently proposed Graph Deviation Network (GDN), which employs graph attention-based forecasting to capture the complex spatio-temporal relationships between sensors. We propose an alternate anomaly threshold criteria for the model, GDN+, based on the learned graph. To evaluate the model's efficacy, we introduce new benchmarking simulation experimen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#29992;EfficientNet&#31639;&#27861;&#20998;&#31867;&#19981;&#21516;&#31867;&#22411;&#30340;&#30284;&#30151;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#31639;&#27861;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#20855;&#26377;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#25552;&#39640;&#30284;&#30151;&#35786;&#26029;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.08715</link><description>&lt;p&gt;
&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#30284;&#30151;&#20998;&#31867;&#30340;EfficientNet&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
EfficientNet Algorithm for Classification of Different Types of Cancer. (arXiv:2304.08715v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08715
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#29992;EfficientNet&#31639;&#27861;&#20998;&#31867;&#19981;&#21516;&#31867;&#22411;&#30340;&#30284;&#30151;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#31639;&#27861;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#20855;&#26377;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#25552;&#39640;&#30284;&#30151;&#35786;&#26029;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39640;&#25928;&#22320;&#35782;&#21035;&#19981;&#21516;&#31867;&#22411;&#30284;&#30151;&#23545;&#26089;&#26399;&#21457;&#29616;&#21644;&#26377;&#25928;&#27835;&#30103;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;EfficientNet&#31639;&#27861;&#20998;&#31867;&#33041;&#30244;&#12289;&#20083;&#33146;&#30284;&#20083;&#25151;X&#32447;&#25668;&#24433;&#12289;&#33016;&#37096;&#30284;&#21644;&#30382;&#32932;&#30284;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;&#25105;&#20204;&#20351;&#29992;&#20844;&#20849;&#25968;&#25454;&#38598;&#24182;&#23545;&#22270;&#20687;&#36827;&#34892;&#39044;&#22788;&#29702;&#20197;&#30830;&#20445;&#19968;&#33268;&#24615;&#21644;&#21487;&#27604;&#24615;&#12290;&#23454;&#39564;&#34920;&#26126;EfficientNet&#31639;&#27861;&#22312;&#27599;&#20010;&#30284;&#30151;&#25968;&#25454;&#38598;&#19978;&#37117;&#21462;&#24471;&#20102;&#39640;&#31934;&#24230;&#12289;&#39640;&#21484;&#22238;&#29575;&#21644;&#39640;F1&#20998;&#25968;&#65292;&#20248;&#20110;&#25991;&#29486;&#20013;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;EfficientNet&#31639;&#27861;&#30340;&#20248;&#32570;&#28857;&#21450;&#20854;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;EfficientNet&#31639;&#27861;&#38750;&#24120;&#36866;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#30284;&#30151;&#30340;&#20998;&#31867;&#65292;&#24182;&#21487;&#29992;&#20110;&#25552;&#39640;&#30284;&#30151;&#35786;&#26029;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate and efficient classification of different types of cancer is critical for early detection and effective treatment. In this paper, we present the results of our experiments using the EfficientNet algorithm for classification of brain tumor, breast cancer mammography, chest cancer, and skin cancer. We used publicly available datasets and preprocessed the images to ensure consistency and comparability. Our experiments show that the EfficientNet algorithm achieved high accuracy, precision, recall, and F1 scores on each of the cancer datasets, outperforming other state-of-the-art algorithms in the literature. We also discuss the strengths and weaknesses of the EfficientNet algorithm and its potential applications in clinical practice. Our results suggest that the EfficientNet algorithm is well-suited for classification of different types of cancer and can be used to improve the accuracy and efficiency of cancer diagnosis.
&lt;/p&gt;</description></item><item><title>Data-OOB&#26159;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#20215;&#20540;&#20272;&#35745;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;out-of-bag&#20272;&#35745;&#65292;&#24182;&#21487;&#20197;&#22312;&#35745;&#31639;&#19978;&#39640;&#25928;&#22788;&#29702;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2304.07718</link><description>&lt;p&gt;
Data-OOB:&#20197;&#26080;&#38656;&#39069;&#22806;&#35745;&#31639;&#30340;Out-of-bag&#20272;&#35745;&#20026;&#20934;&#30340;&#25968;&#25454;&#20215;&#20540;&#20272;&#35745;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Data-OOB: Out-of-bag Estimate as a Simple and Efficient Data Value. (arXiv:2304.07718v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07718
&lt;/p&gt;
&lt;p&gt;
Data-OOB&#26159;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#20215;&#20540;&#20272;&#35745;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;out-of-bag&#20272;&#35745;&#65292;&#24182;&#21487;&#20197;&#22312;&#35745;&#31639;&#19978;&#39640;&#25928;&#22788;&#29702;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#35780;&#20272;&#26159;&#19968;&#20010;&#24378;&#22823;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#20026;&#27169;&#22411;&#35757;&#32451;&#25552;&#20379;&#32479;&#35745;&#27934;&#23519;&#21147;&#65292;&#20197;&#21306;&#20998;&#21738;&#20123;&#25968;&#25454;&#23545;&#20110;&#27169;&#22411;&#35757;&#32451;&#26159;&#26377;&#30410;&#30340;&#65292;&#21738;&#20123;&#26159;&#26377;&#23475;&#30340;&#12290;&#32437;&#35266;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#65292;&#35768;&#22810;&#20197;Shapley&#20026;&#22522;&#30784;&#30340;&#25968;&#25454;&#20215;&#20540;&#35780;&#20272;&#26041;&#27861;&#22343;&#26174;&#31034;&#20986;&#20102;&#24456;&#26377;&#21069;&#36884;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#38656;&#35201;&#35757;&#32451;&#22823;&#37327;&#30340;&#27169;&#22411;&#65292;&#22240;&#27492;&#20247;&#25152;&#21608;&#30693;&#65292;&#36825;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22240;&#27492;&#65292;&#23558;&#27492;&#24212;&#29992;&#20110;&#22823;&#22411;&#25968;&#25454;&#38598;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Data-OOB&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#20215;&#20540;&#20272;&#35745;&#26041;&#27861;&#65292;&#38024;&#23545;bagging&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#20102;out-of-bag&#20272;&#35745;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#35745;&#31639;&#19978;&#26159;&#39640;&#25928;&#30340;&#65292;&#21487;&#20197;&#36890;&#36807;&#37325;&#22797;&#20351;&#29992;&#35757;&#32451;&#22909;&#30340;&#24369;&#23398;&#20064;&#22120;&#26469;&#25193;&#23637;&#21040;&#25968;&#30334;&#19975;&#20010;&#25968;&#25454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24403;&#35780;&#20272;100&#20010;&#36755;&#20837;&#32500;&#24230;&#19988;&#23384;&#22312;$10^6$&#20010;&#26679;&#26412;&#26102;&#65292;Data-OOB&#20165;&#38656;&#35201;&#22312;&#21333;&#20010;CPU&#22788;&#29702;&#22120;&#19978;&#25191;&#34892;&#19981;&#21040;2.25&#20010;&#23567;&#26102;&#12290;&#27492;&#22806;&#65292;Data-OOB&#22312;&#29702;&#35770;&#19978;&#26377;&#22362;&#23454;&#30340;&#35299;&#37322;&#65292;&#24403;&#20004;&#20010;&#31163;&#24046;&#20540;&#20989;&#25968;&#30456;&#21516;&#26102;&#65292;&#20854;&#35782;&#21035;&#20855;&#26377;&#30456;&#21516;&#37325;&#35201;&#24615;&#30340;&#25968;&#25454;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data valuation is a powerful framework for providing statistical insights into which data are beneficial or detrimental to model training. Many Shapley-based data valuation methods have shown promising results in various downstream tasks, however, they are well known to be computationally challenging as it requires training a large number of models. As a result, it has been recognized as infeasible to apply to large datasets. To address this issue, we propose Data-OOB, a new data valuation method for a bagging model that utilizes the out-of-bag estimate. The proposed method is computationally efficient and can scale to millions of data by reusing trained weak learners. Specifically, Data-OOB takes less than 2.25 hours on a single CPU processor when there are $10^6$ samples to evaluate and the input dimension is 100. Furthermore, Data-OOB has solid theoretical interpretations in that it identifies the same important data point as the infinitesimal jackknife influence function when two d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#29366;&#24577;&#32858;&#21512;&#26041;&#27861;&#26469;&#38477;&#20302;&#21160;&#24577;&#31163;&#25955;&#36873;&#25321;&#27169;&#22411;&#30340;&#20272;&#35745;&#35745;&#31639;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#39318;&#20808;&#21033;&#29992;&#21453;&#21521;&#24378;&#21270;&#23398;&#20064;&#20272;&#35745;&#20195;&#29702;Q&#20989;&#25968;&#65292;&#28982;&#21518;&#29992;&#32858;&#31867;&#31639;&#27861;&#36873;&#25321;&#37325;&#35201;&#30340;&#29366;&#24577;&#32858;&#21512;&#65292;&#26368;&#32456;&#21033;&#29992;&#23884;&#22871;&#22266;&#23450;&#28857;&#31639;&#27861;&#36827;&#34892;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2304.04916</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#29366;&#24577;&#32858;&#21512;&#26041;&#27861;&#29992;&#20110;&#21160;&#24577;&#31163;&#25955;&#36873;&#25321;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Data-Driven State Aggregation Approach for Dynamic Discrete Choice Models. (arXiv:2304.04916v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#29366;&#24577;&#32858;&#21512;&#26041;&#27861;&#26469;&#38477;&#20302;&#21160;&#24577;&#31163;&#25955;&#36873;&#25321;&#27169;&#22411;&#30340;&#20272;&#35745;&#35745;&#31639;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#39318;&#20808;&#21033;&#29992;&#21453;&#21521;&#24378;&#21270;&#23398;&#20064;&#20272;&#35745;&#20195;&#29702;Q&#20989;&#25968;&#65292;&#28982;&#21518;&#29992;&#32858;&#31867;&#31639;&#27861;&#36873;&#25321;&#37325;&#35201;&#30340;&#29366;&#24577;&#32858;&#21512;&#65292;&#26368;&#32456;&#21033;&#29992;&#23884;&#22871;&#22266;&#23450;&#28857;&#31639;&#27861;&#36827;&#34892;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#21160;&#24577;&#31163;&#25955;&#36873;&#25321;&#27169;&#22411;&#65292;&#20854;&#20013;&#19968;&#20010;&#24120;&#35265;&#30340;&#38382;&#39064;&#26159;&#20351;&#29992;&#20195;&#29702;&#34892;&#20026;&#25968;&#25454;&#20272;&#35745;&#20195;&#29702;&#22870;&#21169;&#20989;&#25968;&#65288;&#20063;&#31216;&#20026;&#8220;&#32467;&#26500;&#21442;&#25968;&#8221;&#65289;&#30340;&#21442;&#25968;&#12290;&#36825;&#31181;&#27169;&#22411;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#38656;&#35201;&#21160;&#24577;&#35268;&#21010;&#65292;&#36825;&#21463;&#21040;&#32500;&#24230;&#28798;&#38590;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#26469;&#36873;&#25321;&#21644;&#32858;&#21512;&#29366;&#24577;&#65292;&#38477;&#20302;&#20102;&#20272;&#35745;&#30340;&#35745;&#31639;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20998;&#20004;&#20010;&#38454;&#27573;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#28789;&#27963;&#30340;&#21453;&#21521;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#20272;&#35745;&#20195;&#29702;Q&#20989;&#25968;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#20272;&#35745;&#30340;Q&#20989;&#25968;&#65292;&#20197;&#21450;&#19968;&#20010;&#32858;&#31867;&#31639;&#27861;&#65292;&#36873;&#25321;&#20102;&#19968;&#20123;&#26368;&#20026;&#37325;&#35201;&#30340;&#29366;&#24577;&#65292;&#36825;&#20123;&#29366;&#24577;&#23545;&#20110;&#39537;&#21160;Q&#20989;&#25968;&#30340;&#21464;&#21270;&#26368;&#20026;&#20851;&#38190;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#21033;&#29992;&#36825;&#20123;&#34987;&#36873;&#25321;&#30340;&#8220;&#32858;&#21512;&#8221;&#29366;&#24577;&#65292;&#25105;&#20204;&#20351;&#29992;&#24120;&#29992;&#30340;&#23884;&#22871;&#22266;&#23450;&#28857;&#31639;&#27861;&#36827;&#34892;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#12290;&#25152;&#25552;&#20986;&#30340;&#20108;&#38454;&#27573;&#26041;&#27861;&#23454;&#29616;&#20102;...
&lt;/p&gt;
&lt;p&gt;
We study dynamic discrete choice models, where a commonly studied problem involves estimating parameters of agent reward functions (also known as "structural" parameters), using agent behavioral data. Maximum likelihood estimation for such models requires dynamic programming, which is limited by the curse of dimensionality. In this work, we present a novel algorithm that provides a data-driven method for selecting and aggregating states, which lowers the computational and sample complexity of estimation. Our method works in two stages. In the first stage, we use a flexible inverse reinforcement learning approach to estimate agent Q-functions. We use these estimated Q-functions, along with a clustering algorithm, to select a subset of states that are the most pivotal for driving changes in Q-functions. In the second stage, with these selected "aggregated" states, we conduct maximum likelihood estimation using a commonly used nested fixed-point algorithm. The proposed two-stage approach 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36830;&#32493;&#23398;&#20064;&#26080;&#30417;&#30563;&#24490;&#29615;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21487;&#20197;&#39640;&#25928;&#12289;&#33410;&#33021;&#22320;&#36827;&#34892;&#22312;&#32447;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#24182;&#19988;&#36890;&#36807;&#38543;&#26426;&#24310;&#36831;&#23884;&#20837;&#21644;&#20351;&#29992;&#24213;&#23618;&#21160;&#24577;&#31995;&#32479;&#20013;&#20171;&#25968;&#20013;&#24515;&#24615;&#26368;&#39640;&#30340;&#31070;&#32463;&#20803;&#30340;&#33180;&#30005;&#20301;&#36827;&#34892;&#37325;&#26500;&#12290;</title><link>http://arxiv.org/abs/2304.04697</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#33041;&#21551;&#21457;&#30340;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#22312;&#32447;&#26080;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Brain-Inspired Spiking Neural Network for Online Unsupervised Time Series Prediction. (arXiv:2304.04697v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04697
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36830;&#32493;&#23398;&#20064;&#26080;&#30417;&#30563;&#24490;&#29615;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21487;&#20197;&#39640;&#25928;&#12289;&#33410;&#33021;&#22320;&#36827;&#34892;&#22312;&#32447;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#24182;&#19988;&#36890;&#36807;&#38543;&#26426;&#24310;&#36831;&#23884;&#20837;&#21644;&#20351;&#29992;&#24213;&#23618;&#21160;&#24577;&#31995;&#32479;&#20013;&#20171;&#25968;&#20013;&#24515;&#24615;&#26368;&#39640;&#30340;&#31070;&#32463;&#20803;&#30340;&#33180;&#30005;&#20301;&#36827;&#34892;&#37325;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#65292;&#29305;&#21035;&#26159;&#38656;&#35201;&#22522;&#20110;&#27969;&#25968;&#25454;&#19981;&#26029;&#26356;&#26032;&#30340;&#36793;&#32536;AI&#24212;&#29992;&#20013;&#65292;&#33021;&#22815;&#39640;&#25928;&#12289;&#33410;&#33021;&#22320;&#36827;&#34892;&#22312;&#32447;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#30417;&#30563;&#22312;&#32447;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#19988;&#26080;&#27861;&#22312;&#24213;&#23618;&#31995;&#32479;&#21457;&#29983;&#21464;&#21270;&#26102;&#24555;&#36895;&#36866;&#24212;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#20351;&#29992;&#20256;&#20837;&#25968;&#25454;&#36827;&#34892;&#25345;&#32493;&#30340;&#37325;&#26032;&#35757;&#32451;&#65292;&#20351;&#20854;&#39640;&#24230;&#20302;&#25928;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36830;&#32493;&#23398;&#20064;&#26080;&#30417;&#30563;&#24490;&#29615;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65288;CLURSNN&#65289;&#65292;&#20854;&#36890;&#36807;&#23574;&#23792;&#26102;&#24207;&#30456;&#20851;&#21487;&#22609;&#24615;&#65288;STDP&#65289;&#36827;&#34892;&#35757;&#32451;&#12290;CLURSNN&#36890;&#36807;&#20351;&#29992;&#22312;&#21487;&#37325;&#26500;RSNN&#30340;&#20855;&#26377;&#26368;&#39640;&#20171;&#25968;&#20013;&#24515;&#24615;&#30340;&#31070;&#32463;&#20803;&#30340;&#33180;&#30005;&#20301;&#26469;&#36827;&#34892;&#38543;&#26426;&#24310;&#36831;&#23884;&#20837;&#65292;&#20197;&#37325;&#26500;&#24213;&#23618;&#21160;&#24577;&#31995;&#32479;&#36827;&#34892;&#22312;&#32447;&#39044;&#27979;&#12290;&#25105;&#20204;&#36824;&#21033;&#29992;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;Wasserstein&#36317;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy and data-efficient online time series prediction for predicting evolving dynamical systems are critical in several fields, especially edge AI applications that need to update continuously based on streaming data. However, current DNN-based supervised online learning models require a large amount of training data and cannot quickly adapt when the underlying system changes. Moreover, these models require continuous retraining with incoming data making them highly inefficient. To solve these issues, we present a novel Continuous Learning-based Unsupervised Recurrent Spiking Neural Network Model (CLURSNN), trained with spike timing dependent plasticity (STDP). CLURSNN makes online predictions by reconstructing the underlying dynamical system using Random Delay Embedding by measuring the membrane potential of neurons in the recurrent layer of the RSNN with the highest betweenness centrality. We also use topological data analysis to propose a novel methodology using the Wasserstein Di
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#39046;&#22495;&#30693;&#35782;&#36827;&#34892;&#20010;&#24615;&#21270;&#25968;&#23383;&#20581;&#24247;&#34892;&#20026;&#21464;&#38761;&#24178;&#39044;&#30340;&#31995;&#32479;&#65292;&#20854;&#21033;&#29992;&#21453;&#20107;&#23454;&#20363;&#23376;&#36827;&#34892;&#29305;&#24449;&#25511;&#21046;&#20197;&#39044;&#27979;&#24178;&#39044;&#25928;&#26524;&#24182;&#20248;&#21270;&#24178;&#39044;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.03392</link><description>&lt;p&gt;
&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#39046;&#22495;&#30693;&#35782;&#20010;&#24615;&#21270;&#25968;&#23383;&#20581;&#24247;&#34892;&#20026;&#21464;&#38761;&#24178;&#39044;
&lt;/p&gt;
&lt;p&gt;
Personalizing Digital Health Behavior Change Interventions using Machine Learning and Domain Knowledge. (arXiv:2304.03392v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03392
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#39046;&#22495;&#30693;&#35782;&#36827;&#34892;&#20010;&#24615;&#21270;&#25968;&#23383;&#20581;&#24247;&#34892;&#20026;&#21464;&#38761;&#24178;&#39044;&#30340;&#31995;&#32479;&#65292;&#20854;&#21033;&#29992;&#21453;&#20107;&#23454;&#20363;&#23376;&#36827;&#34892;&#29305;&#24449;&#25511;&#21046;&#20197;&#39044;&#27979;&#24178;&#39044;&#25928;&#26524;&#24182;&#20248;&#21270;&#24178;&#39044;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#27491;&#22312;&#24320;&#21457;&#19968;&#31181;&#34394;&#25311;&#25945;&#32451;&#31995;&#32479;&#65292;&#24110;&#21161;&#24739;&#32773;&#22362;&#25345;&#34892;&#20026;&#21464;&#38761;&#24178;&#39044;&#65288;BCI&#65289;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#39044;&#27979;&#24739;&#32773;&#26159;&#21542;&#20250;&#25191;&#34892;&#30446;&#26631;&#34892;&#20026;&#65292;&#24182;&#20351;&#29992;&#21453;&#20107;&#23454;&#20363;&#23376;&#36827;&#34892;&#29305;&#24449;&#25511;&#21046;&#65292;&#20197;&#25351;&#23548;&#20010;&#24615;&#21270;BCI&#12290;&#25105;&#20204;&#20351;&#29992;&#20855;&#26377;&#19981;&#21516;&#21709;&#24212;&#27700;&#24179;&#30340;&#27169;&#25311;&#24739;&#32773;&#25968;&#25454;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We are developing a virtual coaching system that helps patients adhere to behavior change interventions (BCI). Our proposed system predicts whether a patient will perform the targeted behavior and uses counterfactual examples with feature control to guide personalizsation of BCI. We evaluated our prediction model using simulated patient data with varying levels of receptivity to intervention.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; G&#19981;&#21464;&#22270;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376; &#29992;&#20110;&#22788;&#29702;&#25968;&#25454;&#38598;&#19981;&#20165;&#22312;&#27969;&#24418;&#19978;&#65292;&#32780;&#19988;&#22312;&#19968;&#20010;&#36830;&#32493;&#32676;&#30340;&#20316;&#29992;&#19979;&#20063;&#26159;&#23553;&#38381;&#30340;&#24773;&#24418;&#65292;&#30456;&#36739;&#20110;&#26631;&#20934;&#22270;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#25910;&#25947;&#36895;&#24230;&#26356;&#24555;&#12290;</title><link>http://arxiv.org/abs/2303.17001</link><description>&lt;p&gt;
G&#19981;&#21464;&#22270;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
The G-invariant graph Laplacian. (arXiv:2303.17001v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; G&#19981;&#21464;&#22270;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376; &#29992;&#20110;&#22788;&#29702;&#25968;&#25454;&#38598;&#19981;&#20165;&#22312;&#27969;&#24418;&#19978;&#65292;&#32780;&#19988;&#22312;&#19968;&#20010;&#36830;&#32493;&#32676;&#30340;&#20316;&#29992;&#19979;&#20063;&#26159;&#23553;&#38381;&#30340;&#24773;&#24418;&#65292;&#30456;&#36739;&#20110;&#26631;&#20934;&#22270;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#25910;&#25947;&#36895;&#24230;&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#30340;&#31639;&#27861;&#24050;&#32463;&#34987;&#35777;&#26126;&#22312;&#38477;&#32500;&#12289;&#32858;&#31867;&#21644;&#21435;&#22122;&#31561;&#39046;&#22495;&#23545;&#27969;&#24418;&#25968;&#25454;&#38750;&#24120;&#26377;&#25928;&#12290;&#26412;&#25991;&#32771;&#34385;&#30340;&#25968;&#25454;&#38598;&#19981;&#20165;&#22312;&#27969;&#24418;&#19978;&#65292;&#32780;&#19988;&#22312;&#19968;&#20010;&#36830;&#32493;&#32676;&#30340;&#20316;&#29992;&#19979;&#20063;&#26159;&#23553;&#38381;&#30340;&#12290;&#36825;&#31867;&#25968;&#25454;&#38598;&#30340;&#19968;&#20010;&#20363;&#23376;&#26159;&#27839;&#30528;&#20302;&#32500;&#27969;&#24418;&#20256;&#25773;&#30340;&#20307;&#31215;&#65292;&#20854;&#20013;&#27599;&#20010;&#20307;&#31215;&#21487;&#20197;&#22312;&#19977;&#32500;&#31354;&#38388;&#20013;&#26059;&#36716;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;G&#19981;&#21464;&#22270;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#65292;&#36890;&#36807;&#32771;&#34385;&#25968;&#25454;&#38598;&#19978;&#30340;&#32676;&#30340;&#20316;&#29992;&#26469;&#24191;&#20041;&#21270;&#22270;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#12290;&#25105;&#20204;&#26174;&#31034;&#20102;&#19982;&#26631;&#20934;&#22270;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#31867;&#20284;&#65292;G&#19981;&#21464;&#22270;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#25910;&#25947;&#20110;&#25968;&#25454;&#27969;&#24418;&#19978;&#30340;Laplace-Beltrami&#31639;&#23376;&#65292;&#20294;&#25910;&#25947;&#36895;&#24230;&#26174;&#33879;&#25552;&#39640;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#34920;&#26126;G&#19981;&#21464;&#22270;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#30340;&#29305;&#24449;&#20989;&#25968;&#20855;&#26377;&#32676;&#20803;&#32032;&#21644;&#26576;&#20123;&#30697;&#38453;&#30340;&#29305;&#24449;&#21521;&#37327;&#30340;&#24352;&#37327;&#31215;&#24418;&#24335;&#65292;&#21487;&#20197;&#20351;&#29992;F&#39640;&#25928;&#22320;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Laplacian based algorithms for data lying on a manifold have been proven effective for tasks such as dimensionality reduction, clustering, and denoising. In this work, we consider data sets whose data point not only lie on a manifold, but are also closed under the action of a continuous group. An example of such data set is volumes that line on a low dimensional manifold, where each volume may be rotated in three-dimensional space. We introduce the G-invariant graph Laplacian that generalizes the graph Laplacian by accounting for the action of the group on the data set. We show that like the standard graph Laplacian, the G-invariant graph Laplacian converges to the Laplace-Beltrami operator on the data manifold, but with a significantly improved convergence rate. Furthermore, we show that the eigenfunctions of the G-invariant graph Laplacian admit the form of tensor products between the group elements and eigenvectors of certain matrices, which can be computed efficiently using F
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#31867;&#22120;&#24341;&#23548;&#26041;&#27861;DOODL&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#38024;&#23545;&#30495;&#23454;&#29983;&#25104;&#30340;&#20687;&#32032;&#19978;&#39044;&#35757;&#32451;&#20998;&#31867;&#22120;&#30340;&#26799;&#24230;&#20248;&#21270;&#25193;&#25955;&#28508;&#21464;&#26469;&#23454;&#29616;&#21363;&#25554;&#21363;&#29992;&#30340;&#25351;&#23548;&#65292;&#24182;&#22312;&#35745;&#31639;&#21644;&#20154;&#31867;&#35780;&#20272;&#24230;&#37327;&#19978;&#20248;&#20110;&#19968;&#27493;&#20998;&#31867;&#22120;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2303.13703</link><description>&lt;p&gt;
&#31471;&#21040;&#31471;&#25193;&#25955;&#28508;&#22312;&#20248;&#21270;&#25552;&#39640;&#20998;&#31867;&#22120;&#24341;&#23548;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
End-to-End Diffusion Latent Optimization Improves Classifier Guidance. (arXiv:2303.13703v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#31867;&#22120;&#24341;&#23548;&#26041;&#27861;DOODL&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#38024;&#23545;&#30495;&#23454;&#29983;&#25104;&#30340;&#20687;&#32032;&#19978;&#39044;&#35757;&#32451;&#20998;&#31867;&#22120;&#30340;&#26799;&#24230;&#20248;&#21270;&#25193;&#25955;&#28508;&#21464;&#26469;&#23454;&#29616;&#21363;&#25554;&#21363;&#29992;&#30340;&#25351;&#23548;&#65292;&#24182;&#22312;&#35745;&#31639;&#21644;&#20154;&#31867;&#35780;&#20272;&#24230;&#37327;&#19978;&#20248;&#20110;&#19968;&#27493;&#20998;&#31867;&#22120;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#31867;&#22120;&#25351;&#23548;&#8212;&#8212;&#21033;&#29992;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#26799;&#24230;&#26469;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#30340;&#29983;&#25104;&#8212;&#8212;&#26377;&#28508;&#21147;&#22823;&#24133;&#25193;&#23637;&#23545;&#22270;&#20687;&#29983;&#25104;&#21644;&#32534;&#36753;&#30340;&#21019;&#36896;&#24615;&#25511;&#21046;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20998;&#31867;&#22120;&#25351;&#23548;&#35201;&#20040;&#38656;&#35201;&#35757;&#32451;&#26032;&#30340;&#22122;&#22768;&#24863;&#30693;&#27169;&#22411;&#20197;&#33719;&#24471;&#31934;&#30830;&#30340;&#26799;&#24230;&#65292;&#35201;&#20040;&#20351;&#29992;&#19968;&#27493;&#21435;&#22122;&#30340;&#36817;&#20284;&#26368;&#32456;&#29983;&#25104;&#29289;&#65292;&#24182;&#23548;&#33268;&#26799;&#24230;&#19981;&#23545;&#40784;&#21644;&#27425;&#20248;&#25511;&#21046;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#36825;&#31181;&#36817;&#20284;&#30340;&#32570;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25351;&#23548;&#26041;&#27861;&#65306;&#30452;&#25509;&#20248;&#21270;&#25193;&#25955;&#28508;&#21464;&#65288;DOODL&#65289;&#65292;&#23427;&#36890;&#36807;&#38024;&#23545;&#30495;&#23454;&#29983;&#25104;&#30340;&#20687;&#32032;&#19978;&#39044;&#35757;&#32451;&#20998;&#31867;&#22120;&#30340;&#26799;&#24230;&#20248;&#21270;&#25193;&#25955;&#28508;&#21464;&#26469;&#23454;&#29616;&#21363;&#25554;&#21363;&#29992;&#30340;&#25351;&#23548;&#65292;&#20351;&#29992;&#21487;&#36870;&#25193;&#25955;&#36807;&#31243;&#23454;&#29616;&#20869;&#23384;&#26377;&#25928;&#30340;&#21453;&#21521;&#20256;&#36882;&#12290;&#23637;&#31034;&#20102;&#26356;&#31934;&#30830;&#25351;&#23548;&#28508;&#21147;&#30340; DOODL &#22312;&#19981;&#21516;&#24418;&#24335;&#30340;&#25351;&#23548;&#30340;&#35745;&#31639;&#21644;&#20154;&#31867;&#35780;&#20272;&#24230;&#37327;&#19978;&#20248;&#20110;&#19968;&#27493;&#20998;&#31867;&#22120;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classifier guidance -- using the gradients of an image classifier to steer the generations of a diffusion model -- has the potential to dramatically expand the creative control over image generation and editing. However, currently classifier guidance requires either training new noise-aware models to obtain accurate gradients or using a one-step denoising approximation of the final generation, which leads to misaligned gradients and sub-optimal control. We highlight this approximation's shortcomings and propose a novel guidance method: Direct Optimization of Diffusion Latents (DOODL), which enables plug-and-play guidance by optimizing diffusion latents w.r.t. the gradients of a pre-trained classifier on the true generated pixels, using an invertible diffusion process to achieve memory-efficient backpropagation. Showcasing the potential of more precise guidance, DOODL outperforms one-step classifier guidance on computational and human evaluation metrics across different forms of guidanc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#21033;&#29992;&#21453;&#20107;&#23454;&#25512;&#29702;&#26469;&#29983;&#25104;&#21487;&#29702;&#35299;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#20854;&#22312;&#25512;&#33616;&#31995;&#32479;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.11160</link><description>&lt;p&gt;
&#21033;&#29992;&#21453;&#20107;&#23454;&#25991;&#26412;&#35299;&#37322;&#26469;&#35299;&#37322;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Explaining Recommendation System Using Counterfactual Textual Explanations. (arXiv:2303.11160v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#21033;&#29992;&#21453;&#20107;&#23454;&#25512;&#29702;&#26469;&#29983;&#25104;&#21487;&#29702;&#35299;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#20854;&#22312;&#25512;&#33616;&#31995;&#32479;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#26377;&#22823;&#37327;&#30340;&#30740;&#31350;&#33268;&#21147;&#20110;&#25913;&#36827;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#35299;&#35835;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22914;&#26524;&#26368;&#32456;&#29992;&#25143;&#29702;&#35299;&#26576;&#20123;&#36755;&#20986;&#30340;&#21407;&#22240;&#65292;&#23601;&#26356;&#23481;&#26131;&#20449;&#20219;&#31995;&#32479;&#12290;&#25512;&#33616;&#31995;&#32479;&#26159;&#38656;&#35201;&#36827;&#34892;&#25913;&#36827;&#20197;&#20351;&#20854;&#36755;&#20986;&#26356;&#21152;&#21487;&#35299;&#37322;&#30340;&#31995;&#32479;&#20043;&#19968;&#12290;&#20135;&#29983;&#26356;&#21487;&#35299;&#37322;&#30340;&#36755;&#20986;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#20351;&#29992;&#21453;&#20107;&#23454;&#25512;&#29702;&#65292;&#36825;&#28041;&#21450;&#23545;&#26368;&#23567;&#35201;&#32032;&#36827;&#34892;&#20462;&#25913;&#65292;&#20197;&#29983;&#25104;&#23548;&#33268;&#31995;&#32479;&#36755;&#20986;&#21464;&#21270;&#30340;&#21453;&#20107;&#23454;&#39033;&#30446;&#12290;&#36825;&#19968;&#36807;&#31243;&#20801;&#35768;&#35782;&#21035;&#23545;&#26399;&#26395;&#36755;&#20986;&#26377;&#37325;&#22823;&#24433;&#21709;&#30340;&#36755;&#20837;&#35201;&#32032;&#65292;&#20174;&#32780;&#25552;&#20379;&#26377;&#25928;&#30340;&#35299;&#37322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#29983;&#25104;&#38024;&#23545;&#34920;&#26684;&#21644;&#25991;&#26412;&#35201;&#32032;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#35777;&#26126;&#23427;&#22312;&#20026;&#26368;&#32456;&#29992;&#25143;&#25552;&#20379;&#21487;&#29702;&#35299;&#30340;&#35299;&#37322;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Currently, there is a significant amount of research being conducted in the field of artificial intelligence to improve the explainability and interpretability of deep learning models. It is found that if end-users understand the reason for the production of some output, it is easier to trust the system. Recommender systems are one example of systems that great efforts have been conducted to make their output more explainable. One method for producing a more explainable output is using counterfactual reasoning, which involves altering minimal features to generate a counterfactual item that results in changing the output of the system. This process allows the identification of input features that have a significant impact on the desired output, leading to effective explanations. In this paper, we present a method for generating counterfactual explanations for both tabular and textual features. We evaluated the performance of our proposed method on three real-world datasets and demonstra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Aux-Drop&#30340;&#31574;&#30053;&#65292;&#36866;&#29992;&#20110;&#22312;&#32447;&#23398;&#20064;&#65292;&#33021;&#22815;&#22788;&#29702;&#28151;&#20081;&#30340;&#36755;&#20837;&#29305;&#24449;&#12290;&#36890;&#36807;&#35843;&#25972;&#24120;&#35268;&#30340;&#20002;&#22833;&#27491;&#21017;&#21270;&#26041;&#26696;&#65292;&#30830;&#20445;&#26368;&#32456;&#36755;&#20986;&#21463;&#21040;&#36825;&#20123;&#29305;&#24449;&#28151;&#20081;&#20986;&#29616;&#30340;&#26368;&#23567;&#24433;&#21709;&#65292;&#26377;&#21161;&#20110;&#26500;&#24314;&#26356;&#20581;&#22766;&#26377;&#25928;&#30340;&#22312;&#32447;&#23398;&#20064;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2303.05155</link><description>&lt;p&gt;
Aux-Drop: &#20351;&#29992;&#36741;&#21161;&#20002;&#24323;&#22788;&#29702;&#22312;&#32447;&#23398;&#20064;&#20013;&#30340;&#28151;&#20081;&#36755;&#20837;
&lt;/p&gt;
&lt;p&gt;
Aux-Drop: Handling Haphazard Inputs in Online Learning Using Auxiliary Dropouts. (arXiv:2303.05155v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05155
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Aux-Drop&#30340;&#31574;&#30053;&#65292;&#36866;&#29992;&#20110;&#22312;&#32447;&#23398;&#20064;&#65292;&#33021;&#22815;&#22788;&#29702;&#28151;&#20081;&#30340;&#36755;&#20837;&#29305;&#24449;&#12290;&#36890;&#36807;&#35843;&#25972;&#24120;&#35268;&#30340;&#20002;&#22833;&#27491;&#21017;&#21270;&#26041;&#26696;&#65292;&#30830;&#20445;&#26368;&#32456;&#36755;&#20986;&#21463;&#21040;&#36825;&#20123;&#29305;&#24449;&#28151;&#20081;&#20986;&#29616;&#30340;&#26368;&#23567;&#24433;&#21709;&#65292;&#26377;&#21161;&#20110;&#26500;&#24314;&#26356;&#20581;&#22766;&#26377;&#25928;&#30340;&#22312;&#32447;&#23398;&#20064;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#22522;&#20110;&#22312;&#32447;&#23398;&#20064;&#30340;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#31243;&#24207;&#29983;&#25104;&#30340;&#27969;&#24335;&#25968;&#25454;&#20855;&#26377;&#28151;&#20081;&#30340;&#29305;&#24615;&#65292;&#21363;&#21253;&#21547;&#32570;&#22833;&#30340;&#29305;&#24449;&#65292;&#22312;&#26102;&#38388;&#19978;&#21464;&#24471;&#36807;&#26102;&#65292;&#20197;&#21518;&#20986;&#29616;&#26032;&#29305;&#24449;&#24182;&#19988;&#32570;&#20047;&#23545;&#24635;&#36755;&#20837;&#29305;&#24449;&#25968;&#37327;&#30340;&#28165;&#26224;&#35748;&#35782;&#12290;&#36825;&#20123;&#25361;&#25112;&#20351;&#24471;&#20026;&#36825;&#20123;&#24212;&#29992;&#26500;&#24314;&#21487;&#23398;&#20064;&#31995;&#32479;&#21464;&#24471;&#22256;&#38590;&#65292;&#24182;&#19988;&#20960;&#20046;&#27809;&#26377;&#20851;&#20110;&#36825;&#20010;&#38382;&#39064;&#30340;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#24037;&#20316;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Aux-Drop&#65292;&#19968;&#31181;&#29992;&#20110;&#22312;&#32447;&#23398;&#20064;&#30340;&#36741;&#21161;&#20002;&#22833;&#27491;&#21017;&#21270;&#31574;&#30053;&#65292;&#20197;&#26377;&#25928;&#22788;&#29702;&#28151;&#20081;&#30340;&#36755;&#20837;&#29305;&#24449;&#12290;Aux-Drop&#20026;&#28151;&#20081;&#30340;&#36755;&#20837;&#29305;&#24449;&#31354;&#38388;&#35843;&#25972;&#24120;&#35268;&#30340;&#20002;&#22833;&#27491;&#21017;&#21270;&#26041;&#26696;&#65292;&#30830;&#20445;&#26368;&#32456;&#36755;&#20986;&#21463;&#21040;&#36825;&#20123;&#29305;&#24449;&#28151;&#20081;&#20986;&#29616;&#30340;&#26368;&#23567;&#24433;&#21709;&#12290;&#23427;&#26377;&#21161;&#20110;&#38450;&#27490;&#29305;&#21035;&#26159;&#36741;&#21161;&#21644;&#22522;&#30784;&#29305;&#24449;&#30340;&#20849;&#36866;&#24212;&#65292;&#24182;&#20943;&#23569;&#27169;&#22411;&#20013;&#20219;&#20309;&#36741;&#21161;&#36755;&#20837;&#23545;&#36755;&#20986;&#30340;&#24378;&#28872;&#20381;&#36182;&#12290;&#36825;&#26377;&#21161;&#20110;&#20026;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#31243;&#24207;&#26500;&#24314;&#26356;&#20581;&#22766;&#26377;&#25928;&#30340;&#22312;&#32447;&#23398;&#20064;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many real-world applications based on online learning produce streaming data that is haphazard in nature, i.e., contains missing features, features becoming obsolete in time, the appearance of new features at later points in time and a lack of clarity on the total number of input features. These challenges make it hard to build a learnable system for such applications, and almost no work exists in deep learning that addresses this issue. In this paper, we present Aux-Drop, an auxiliary dropout regularization strategy for online learning that handles the haphazard input features in an effective manner. Aux-Drop adapts the conventional dropout regularization scheme for the haphazard input feature space ensuring that the final output is minimally impacted by the chaotic appearance of such features. It helps to prevent the co-adaptation of especially the auxiliary and base features, as well as reduces the strong dependence of the output on any of the auxiliary inputs of the model. This hel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#24067;&#38647;-&#29926;&#29791;&#26031;&#22374;&#36317;&#31163;&#35757;&#32451;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#28145;&#24230;&#30697;&#38453;&#20998;&#35299;&#27169;&#22411;&#65292;&#24182;&#22312;&#26377;&#38480;&#31209;&#30697;&#38453;&#31354;&#38388;&#20869;&#34920;&#24449;&#20851;&#38190;&#28857;&#21644;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#26368;&#32456;&#30830;&#23450;&#20102;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.03027</link><description>&lt;p&gt;
&#24067;&#38647;-&#29926;&#29791;&#26031;&#22374;&#36317;&#31163;&#35757;&#32451;&#19979;&#30340;&#29983;&#25104;&#24335;&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#30340;&#20851;&#38190;&#28857;&#21644;&#25910;&#25947;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Critical Points and Convergence Analysis of Generative Deep Linear Networks Trained with Bures-Wasserstein Loss. (arXiv:2303.03027v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03027
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#24067;&#38647;-&#29926;&#29791;&#26031;&#22374;&#36317;&#31163;&#35757;&#32451;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#28145;&#24230;&#30697;&#38453;&#20998;&#35299;&#27169;&#22411;&#65292;&#24182;&#22312;&#26377;&#38480;&#31209;&#30697;&#38453;&#31354;&#38388;&#20869;&#34920;&#24449;&#20851;&#38190;&#28857;&#21644;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#26368;&#32456;&#30830;&#23450;&#20102;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#20351;&#29992;&#24067;&#38647;-&#29926;&#29791;&#26031;&#22374;&#36317;&#31163;&#35757;&#32451;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#28145;&#24230;&#30697;&#38453;&#20998;&#35299;&#27169;&#22411;&#12290;&#30456;&#36739;&#20110;&#20197;&#24448;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#25439;&#22833;&#20989;&#25968;&#21644;&#29983;&#25104;&#24335;&#35774;&#32622;&#19978;&#26377;&#25152;&#19981;&#21516;&#12290;&#25105;&#20204;&#22312;&#26377;&#38480;&#31209;&#30697;&#38453;&#31354;&#38388;&#20869;&#34920;&#24449;&#20102;&#35813;&#26041;&#27861;&#30340;&#20851;&#38190;&#28857;&#21644;&#26368;&#23567;&#21270;&#38382;&#39064;&#12290;&#38024;&#23545;&#20302;&#31209;&#30697;&#38453;&#32780;&#35328;&#65292;&#35813;&#26041;&#27861;&#30340;&#28023;&#26862;&#30697;&#38453;&#29702;&#35770;&#19978;&#21487;&#33021;&#20250;&#29190;&#28856;&#65292;&#36825;&#20026;&#20248;&#21270;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#20013;&#20351;&#29992;&#25439;&#22833;&#30340;&#24179;&#28369;&#24494;&#25200;&#29256;&#26412;&#26102;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#22312;&#21021;&#22987;&#26435;&#37325;&#30340;&#19968;&#23450;&#20551;&#35774;&#26465;&#20214;&#19979;&#35777;&#26126;&#20102;&#26377;&#38480;&#27493;&#38271;&#26799;&#24230;&#19979;&#38477;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a deep matrix factorization model of covariance matrices trained with the Bures-Wasserstein distance. While recent works have made important advances in the study of the optimization problem for overparametrized low-rank matrix approximation, much emphasis has been placed on discriminative settings and the square loss. In contrast, our model considers another interesting type of loss and connects with the generative setting. We characterize the critical points and minimizers of the Bures-Wasserstein distance over the space of rank-bounded matrices. For low-rank matrices the Hessian of this loss can theoretically blow up, which creates challenges to analyze convergence of optimizaton methods. We establish convergence results for gradient flow using a smooth perturbative version of the loss and convergence results for finite step size gradient descent under certain assumptions on the initial weights.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#31070;&#32463;&#31639;&#23376;&#21644;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26080;&#27861;&#20934;&#30830;&#22806;&#25512;&#21644;&#35823;&#24046;&#31215;&#32047;&#30340;&#38382;&#39064;&#65292;&#22312; Korteweg-de Vries &#26041;&#31243;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#31934;&#24230;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.02243</link><description>&lt;p&gt;
&#22522;&#20110;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#31070;&#32463;&#31639;&#23376;&#23398;&#20064;&#22312;&#21160;&#21147;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#21450;&#38271;&#26102;&#38388;&#31215;&#20998;&#38382;&#39064;&#35299;&#20915;
&lt;/p&gt;
&lt;p&gt;
Neural Operator Learning for Long-Time Integration in Dynamical Systems with Recurrent Neural Networks. (arXiv:2303.02243v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02243
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#31070;&#32463;&#31639;&#23376;&#21644;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26080;&#27861;&#20934;&#30830;&#22806;&#25512;&#21644;&#35823;&#24046;&#31215;&#32047;&#30340;&#38382;&#39064;&#65292;&#22312; Korteweg-de Vries &#26041;&#31243;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#31934;&#24230;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#27604;&#20256;&#32479;&#31185;&#23398;&#35745;&#31639;&#26041;&#27861;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#27169;&#25311;&#22797;&#26434;&#21160;&#21147;&#31995;&#32479;&#26041;&#38754;&#20855;&#26377;&#35745;&#31639;&#25104;&#26412;&#38477;&#20302;&#12289;&#21487;&#20197;&#30452;&#25509;&#20174;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#31561;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#22312;&#38271;&#26102;&#38388;&#31215;&#20998;&#20013;&#23384;&#22312;&#26080;&#27861;&#20934;&#30830;&#22806;&#25512;&#21644;&#35823;&#24046;&#31215;&#32047;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#31070;&#32463;&#31639;&#23376;&#19982;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#26500;&#24314;&#20986;&#19968;&#31181;&#26032;&#39062;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#25552;&#21319;&#20102;&#27604;&#29616;&#26377;&#25216;&#26415;&#26356;&#20934;&#30830;&#30340;&#31215;&#20998;&#12290;&#36825;&#20010;&#26032;&#22411;&#30340;&#28151;&#21512;&#27169;&#22411;&#22522;&#20110;&#31639;&#23376;&#23398;&#20064;&#65292;&#21516;&#26102;&#25552;&#20379;&#36882;&#24402;&#32467;&#26500;&#26469;&#25429;&#25417;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20010;&#32508;&#21512;&#26694;&#26550;&#23545;&#20110; Korteweg-de Vries &#26041;&#31243;&#25554;&#20540;&#21644;&#22806;&#25512;&#22343;&#31283;&#23450;&#35299;&#20915;&#20102;&#35823;&#24046;&#31215;&#32047;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are an attractive alternative for simulating complex dynamical systems, as in comparison to traditional scientific computing methods, they offer reduced computational costs during inference and can be trained directly from observational data. Existing methods, however, cannot extrapolate accurately and are prone to error accumulation in long-time integration. Herein, we address this issue by combining neural operators with recurrent neural networks to construct a novel and effective architecture, resulting in superior accuracy compared to the state-of-the-art. The new hybrid model is based on operator learning while offering a recurrent structure to capture temporal dependencies. The integrated framework is shown to stabilize the solution and reduce error accumulation for both interpolation and extrapolation of the Korteweg-de Vries equation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20160;&#20040;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#29305;&#26435;&#20449;&#24687;&#21487;&#20197;&#35299;&#20915;&#26631;&#31614;&#22122;&#22768;&#38382;&#39064;&#65292;&#21457;&#29616;&#24403;PI&#24110;&#21161;&#31070;&#32463;&#32593;&#32476;&#36731;&#26494;&#20998;&#36776;&#20986;&#22024;&#26434;&#21644;&#24178;&#20928;&#30340;&#25968;&#25454;&#65292;&#19982;&#25552;&#20379;&#35760;&#24518;&#22024;&#26434;&#25968;&#25454;&#30340;&#23398;&#20064;&#24555;&#25463;&#26041;&#24335;&#26102;&#65292;&#23427;&#26159;&#26368;&#26377;&#29992;&#30340;&#65292;&#22312;PI&#36807;&#20110;&#39044;&#27979;&#30446;&#26631;&#26631;&#31614;&#26102;&#65292;PI&#26041;&#27861;&#34920;&#29616;&#20250;&#26356;&#24046;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#20123;&#22686;&#24378;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#26631;&#31614;&#22122;&#38899;&#12290;</title><link>http://arxiv.org/abs/2303.01806</link><description>&lt;p&gt;
&#24403;&#29305;&#26435;&#20449;&#24687;&#33021;&#22815;&#35299;&#37322;&#26631;&#31614;&#22122;&#38899;&#30340;&#26102;&#20505;&#65311;
&lt;/p&gt;
&lt;p&gt;
When does Privileged Information Explain Away Label Noise?. (arXiv:2303.01806v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01806
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20160;&#20040;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#29305;&#26435;&#20449;&#24687;&#21487;&#20197;&#35299;&#20915;&#26631;&#31614;&#22122;&#22768;&#38382;&#39064;&#65292;&#21457;&#29616;&#24403;PI&#24110;&#21161;&#31070;&#32463;&#32593;&#32476;&#36731;&#26494;&#20998;&#36776;&#20986;&#22024;&#26434;&#21644;&#24178;&#20928;&#30340;&#25968;&#25454;&#65292;&#19982;&#25552;&#20379;&#35760;&#24518;&#22024;&#26434;&#25968;&#25454;&#30340;&#23398;&#20064;&#24555;&#25463;&#26041;&#24335;&#26102;&#65292;&#23427;&#26159;&#26368;&#26377;&#29992;&#30340;&#65292;&#22312;PI&#36807;&#20110;&#39044;&#27979;&#30446;&#26631;&#26631;&#31614;&#26102;&#65292;PI&#26041;&#27861;&#34920;&#29616;&#20250;&#26356;&#24046;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#20123;&#22686;&#24378;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#26631;&#31614;&#22122;&#38899;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30740;&#31350;&#34920;&#26126;&#65292;&#21033;&#29992;&#35757;&#32451;&#36807;&#31243;&#20013;&#21487;&#29992;&#20294;&#27979;&#35797;&#26102;&#26080;&#27861;&#33719;&#21462;&#30340;&#29305;&#26435;&#20449;&#24687;&#65288;PI&#65289;&#26469;&#22788;&#29702;&#26631;&#31614;&#22122;&#38899;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#28982;&#32780;&#20854;&#26377;&#25928;&#24615;&#30340;&#21407;&#22240;&#23578;&#19981;&#28165;&#26970;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;CIFAR-N/H&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#26032;&#30340;&#22823;&#35268;&#27169;&#22522;&#20934;ImageNet-PI&#19978;&#30340;&#23454;&#39564;&#21457;&#29616;&#65292;&#24403;&#29305;&#26435;&#20449;&#24687;&#33021;&#22815;&#24110;&#21161;&#31070;&#32463;&#32593;&#32476;&#36731;&#26494;&#20998;&#36776;&#20986;&#24178;&#20928;&#21644;&#22024;&#26434;&#30340;&#28857;&#26102;&#65292;&#24182;&#33021;&#22815;&#20026;&#35760;&#24518;&#22024;&#26434;&#30340;&#20363;&#23376;&#25552;&#20379;&#23398;&#20064;&#30340;&#24555;&#25463;&#26041;&#24335;&#26102;&#65292;&#23427;&#26159;&#26368;&#26377;&#24110;&#21161;&#30340;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#24403;PI&#21464;&#24471;&#36807;&#20110;&#39044;&#27979;&#30446;&#26631;&#26631;&#31614;&#26102;&#65292;PI&#26041;&#27861;&#24448;&#24448;&#27604;&#26080;PI&#22522;&#32447;&#34920;&#29616;&#26356;&#24046;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#25913;&#36827;&#26368;&#20808;&#36827;&#30340;PI&#26041;&#27861;&#30340;&#26041;&#24335;&#65292;&#24182;&#23637;&#31034;&#20102;PI&#20316;&#20026;&#35299;&#20915;&#26631;&#31614;&#22122;&#38899;&#30340;&#25163;&#27573;&#30340;&#28508;&#21147;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36731;&#26494;&#22320;&#23558;&#32467;&#26524;PI&#26041;&#27861;&#19982;&#24120;&#35268;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Leveraging privileged information (PI), or features available during training but not at test time, has recently been shown to be an effective method for addressing label noise. However, the reasons for its effectiveness are not well understood. In this study, we investigate the role played by different properties of the PI in explaining away label noise. Through experiments on multiple datasets with real PI (CIFAR-N/H) and a new large-scale benchmark ImageNet-PI, we find that PI is most helpful when it allows networks to easily distinguish clean from noisy data, while enabling a learning shortcut to memorize the noisy examples. Interestingly, when PI becomes too predictive of the target label, PI methods often perform worse than their no-PI baselines. Based on these findings, we propose several enhancements to the state-of-the-art PI methods and demonstrate the potential of PI as a means of tackling label noise. Finally, we show how we can easily combine the resulting PI approaches wi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24341;&#23548;&#26041;&#27861;&#65292;&#36890;&#36807;&#24050;&#30693;&#30340;&#38598;&#21512;&#21457;&#29616;&#26032;&#30340;&#24182;&#34892;&#38170;&#28857;&#65292;&#20197;&#20811;&#26381;&#30456;&#23545;&#34920;&#31034;&#24335;&#20013;&#33719;&#21462;&#24182;&#34892;&#38170;&#28857;&#22256;&#38590;&#30340;&#38382;&#39064;&#65292;&#33021;&#22815;&#29992;&#20110;&#19981;&#21516;&#22495;&#20043;&#38388;&#30340;&#35821;&#20041;&#23545;&#24212;&#65292;&#21462;&#24471;&#31454;&#20105;&#24615;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.00721</link><description>&lt;p&gt;
&#20026;&#30456;&#23545;&#34920;&#31034;&#24335;&#24341;&#20837;&#24182;&#34892;&#38170;&#28857;&#30340;&#24341;&#23548;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Bootstrapping Parallel Anchors for Relative Representations. (arXiv:2303.00721v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00721
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24341;&#23548;&#26041;&#27861;&#65292;&#36890;&#36807;&#24050;&#30693;&#30340;&#38598;&#21512;&#21457;&#29616;&#26032;&#30340;&#24182;&#34892;&#38170;&#28857;&#65292;&#20197;&#20811;&#26381;&#30456;&#23545;&#34920;&#31034;&#24335;&#20013;&#33719;&#21462;&#24182;&#34892;&#38170;&#28857;&#22256;&#38590;&#30340;&#38382;&#39064;&#65292;&#33021;&#22815;&#29992;&#20110;&#19981;&#21516;&#22495;&#20043;&#38388;&#30340;&#35821;&#20041;&#23545;&#24212;&#65292;&#21462;&#24471;&#31454;&#20105;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#23545;&#34920;&#31034;&#27861;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#37117;&#26174;&#31034;&#20986;&#20102;&#20316;&#20026;&#28508;&#22312;&#23884;&#20837;&#27861;&#30340;&#28508;&#21147;&#65292;&#20363;&#22914;&#21551;&#29992;&#28508;&#22312;&#31354;&#38388;&#36890;&#20449;&#21644;&#27169;&#22411;&#25340;&#25509;&#12290;&#28982;&#32780;&#65292;&#30456;&#23545;&#34920;&#31034;&#20381;&#36182;&#20110;&#19968;&#23450;&#37327;&#30340;&#24182;&#34892;&#38170;&#28857;&#20316;&#20026;&#36755;&#20837;&#65292;&#20294;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#38590;&#20197;&#33719;&#21462;&#36825;&#20123;&#38170;&#28857;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#26041;&#27861;&#65292;&#20174;&#26377;&#38480;&#30340;&#24050;&#30693;&#38598;&#21512;&#65288;&#31181;&#23376;&#65289;&#20013;&#21457;&#29616;&#26032;&#30340;&#24182;&#34892;&#38170;&#28857;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#22312;&#19981;&#21516;&#22495;&#20043;&#38388;&#23547;&#25214;&#35821;&#20041;&#23545;&#24212;&#65292;&#23545;&#40784;&#23427;&#20204;&#30340;&#30456;&#23545;&#31354;&#38388;&#65292;&#24182;&#22312;&#22810;&#39033;&#20219;&#21153;&#20013;&#21462;&#24471;&#31454;&#20105;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of relative representations for latent embeddings has shown potential in enabling latent space communication and zero-shot model stitching across a wide range of applications. Nevertheless, relative representations rely on a certain amount of parallel anchors to be given as input, which can be impractical to obtain in certain scenarios. To overcome this limitation, we propose an optimization-based method to discover new parallel anchors from a limited known set (seed). Our approach can be used to find semantic correspondence between different domains, align their relative spaces, and achieve competitive results in several tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31232;&#30095;&#32534;&#30721;&#20013;&#23398;&#20064;&#24471;&#21040;&#30340;&#22823;&#20110;&#23454;&#38469;&#23383;&#20856;&#30340;&#24773;&#20917;&#19979;&#65292;&#22122;&#22768;&#20250;&#23548;&#33268;&#26631;&#20934;&#30340;&#23383;&#20856;&#23398;&#20064;&#30446;&#26631;&#20989;&#25968;&#26080;&#27861;&#24674;&#22797;&#20986;&#23454;&#38469;&#23383;&#20856;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#36890;&#36807;&#36974;&#30422;&#25968;&#25454;&#30340;&#26041;&#27861;&#36827;&#34892;&#21487;&#38752;&#30340;&#23383;&#20856;&#24674;&#22797;&#36866;&#29992;&#20110;&#22810;&#31181;&#20449;&#21495;&#27169;&#24577;&#12290;</title><link>http://arxiv.org/abs/2302.12715</link><description>&lt;p&gt;
&#36974;&#30422;&#25968;&#25454;&#26377;&#24110;&#21161;&#65306;&#20851;&#20110;&#31232;&#30095;&#32534;&#30721;&#20013;&#36974;&#30422;&#30340;&#22909;&#22788;
&lt;/p&gt;
&lt;p&gt;
Hiding Data Helps: On the Benefits of Masking for Sparse Coding. (arXiv:2302.12715v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12715
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31232;&#30095;&#32534;&#30721;&#20013;&#23398;&#20064;&#24471;&#21040;&#30340;&#22823;&#20110;&#23454;&#38469;&#23383;&#20856;&#30340;&#24773;&#20917;&#19979;&#65292;&#22122;&#22768;&#20250;&#23548;&#33268;&#26631;&#20934;&#30340;&#23383;&#20856;&#23398;&#20064;&#30446;&#26631;&#20989;&#25968;&#26080;&#27861;&#24674;&#22797;&#20986;&#23454;&#38469;&#23383;&#20856;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#36890;&#36807;&#36974;&#30422;&#25968;&#25454;&#30340;&#26041;&#27861;&#36827;&#34892;&#21487;&#38752;&#30340;&#23383;&#20856;&#24674;&#22797;&#36866;&#29992;&#20110;&#22810;&#31181;&#20449;&#21495;&#27169;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#32534;&#30721;&#34987;&#29992;&#22312;&#20449;&#21495;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#20197;&#21450;&#21307;&#23398;&#25104;&#20687;&#31561;&#39046;&#22495;&#65292;&#22312;&#36825;&#20123;&#24212;&#29992;&#20013;&#65292;&#20449;&#21495;&#34987;&#24314;&#27169;&#20026;&#29992;&#23398;&#20064;&#21040;&#30340;&#23383;&#20856;&#20013;&#30340;&#20803;&#32032;&#30340;&#31232;&#30095;&#32447;&#24615;&#32452;&#21512;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#22823;&#22810;&#25968;&#30740;&#31350;&#20851;&#27880;&#23398;&#20064;&#21040;&#30340;&#23383;&#20856;&#19982;&#30495;&#23454;&#23383;&#20856;&#22823;&#23567;&#30456;&#21516;&#26102;&#30340;&#24773;&#20917;&#65292;&#24182;&#19988;&#20165;&#30740;&#31350;&#20102;&#27809;&#26377;&#22122;&#22768;&#30340;&#24773;&#26223;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25968;&#25454;&#36974;&#30422;&#26041;&#27861;&#65292;&#33021;&#22815;&#20445;&#35777;&#22312;&#36974;&#30422;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#21487;&#38752;&#30340;&#23383;&#20856;&#24674;&#22797;&#32467;&#26524;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#20449;&#21495;&#27169;&#24577;&#19979;&#23545;&#25105;&#20204;&#30340;&#36974;&#30422;&#26041;&#27861;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#65292;&#24471;&#21040;&#20102;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sparse coding, which refers to modeling a signal as sparse linear combinations of the elements of a learned dictionary, has proven to be a successful (and interpretable) approach in applications such as signal processing, computer vision, and medical imaging. While this success has spurred much work on provable guarantees for dictionary recovery when the learned dictionary is the same size as the ground-truth dictionary, work on the setting where the learned dictionary is larger (or over-realized) with respect to the ground truth is comparatively nascent. Existing theoretical results in this setting have been constrained to the case of noise-less data. We show in this work that, in the presence of noise, minimizing the standard dictionary learning objective can fail to recover the elements of the ground-truth dictionary in the over-realized regime, regardless of the magnitude of the signal in the data-generating process. Furthermore, drawing from the growing body of work on self-superv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;K-SHAP&#30340;&#31639;&#27861;&#65292;&#26469;&#35299;&#20915;&#22810;&#20010;&#26234;&#33021;&#20307;&#20445;&#25345;&#21311;&#21517;&#19988;&#20165;&#26377;&#29366;&#24577;-&#21160;&#20316;&#23545;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#26234;&#33021;&#20307;&#20915;&#31574;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.11996</link><description>&lt;p&gt;
K-SHAP: &#19968;&#31181;&#29992;&#20110;&#21311;&#21517;&#29366;&#24577;-&#21160;&#20316;&#23545;&#30340;&#31574;&#30053;&#32858;&#31867;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
K-SHAP: Policy Clustering Algorithm for Anonymous State-Action Pairs. (arXiv:2302.11996v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11996
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;K-SHAP&#30340;&#31639;&#27861;&#65292;&#26469;&#35299;&#20915;&#22810;&#20010;&#26234;&#33021;&#20307;&#20445;&#25345;&#21311;&#21517;&#19988;&#20165;&#26377;&#29366;&#24577;-&#21160;&#20316;&#23545;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#26234;&#33021;&#20307;&#20915;&#31574;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#23398;&#20064;&#26234;&#33021;&#20307;&#34892;&#20026;&#24050;&#32463;&#34987;&#35777;&#26126;&#21487;&#20197;&#25552;&#39640;&#25105;&#20204;&#23545;&#23427;&#20204;&#20915;&#31574;&#36807;&#31243;&#30340;&#29702;&#35299;&#65292;&#20174;&#32780;&#22686;&#24378;&#25105;&#20204;&#35299;&#37322;&#23427;&#20204;&#19982;&#29615;&#22659;&#21644;&#20854;&#20182;&#26234;&#33021;&#20307;&#20043;&#38388;&#20132;&#20114;&#30340;&#33021;&#21147;&#12290;&#23613;&#31649;&#25991;&#29486;&#20013;&#24050;&#32463;&#25552;&#20986;&#20102;&#22810;&#31181;&#23398;&#20064;&#25216;&#26415;&#65292;&#20294;&#36824;&#26377;&#19968;&#31181;&#29305;&#23450;&#30340;&#24773;&#20917;&#23578;&#26410;&#34987;&#25506;&#32034;&#65292;&#37027;&#23601;&#26159;&#26234;&#33021;&#20307;&#36523;&#20221;&#20445;&#25345;&#21311;&#21517;&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#12290;&#20363;&#22914;&#65292;&#22312;&#37329;&#34701;&#24066;&#22330;&#20013;&#65292;&#26631;&#35760;&#25968;&#25454;&#36890;&#24120;&#26159;&#19987;&#26377;&#30340;&#65292;&#20165;&#20844;&#24320;&#22810;&#20010;&#24066;&#22330;&#21442;&#19982;&#32773;&#20132;&#20114;&#32780;&#20135;&#29983;&#30340;&#21311;&#21517;&#29366;&#24577;-&#21160;&#20316;&#23545;&#12290;&#22240;&#27492;&#65292;&#26234;&#33021;&#20307;&#34892;&#21160;&#24207;&#21015;&#19981;&#21487;&#35266;&#27979;&#65292;&#38480;&#21046;&#20102;&#29616;&#26377;&#24037;&#20316;&#30340;&#36866;&#29992;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#32858;&#31867;&#31639;&#27861;K-SHAP&#65292;&#23427;&#23398;&#20064;&#26681;&#25454;&#26234;&#33021;&#20307;&#31574;&#30053;&#23545;&#21311;&#21517;&#29366;&#24577;-&#21160;&#20316;&#23545;&#36827;&#34892;&#20998;&#32452;&#12290;&#25105;&#20204;&#23558;&#35813;&#38382;&#39064;&#20316;&#20026;&#27169;&#20223;&#23398;&#20064;(IL)&#20219;&#21153;&#65292;&#23398;&#20064;&#19968;&#20010;w...
&lt;/p&gt;
&lt;p&gt;
Learning agent behaviors from observational data has shown to improve our understanding of their decision-making processes, advancing our ability to explain their interactions with the environment and other agents. While multiple learning techniques have been proposed in the literature, there is one particular setting that has not been explored yet: multi agent systems where agent identities remain anonymous. For instance, in financial markets labeled data that identifies market participant strategies is typically proprietary, and only the anonymous state-action pairs that result from the interaction of multiple market participants are publicly available. As a result, sequences of agent actions are not observable, restricting the applicability of existing work. In this paper, we propose a Policy Clustering algorithm, called K-SHAP, that learns to group anonymous state-action pairs according to the agent policies. We frame the problem as an Imitation Learning (IL) task, and we learn a w
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Fitted Likelihood Estimation (FLE)&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#20998;&#24067;&#24335;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#30340;&#38382;&#39064;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#23398;&#20064;&#21040;&#23494;&#20999;&#25509;&#36817;&#30495;&#23454;&#20998;&#24067;&#30340;&#31574;&#30053;&#22238;&#25253;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2302.09456</link><description>&lt;p&gt;
&#20855;&#26377;&#39044;&#27979;&#35823;&#24046;&#20445;&#35777;&#30340;&#20998;&#24067;&#24335;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Distributional Offline Policy Evaluation with Predictive Error Guarantees. (arXiv:2302.09456v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09456
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Fitted Likelihood Estimation (FLE)&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#20998;&#24067;&#24335;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#30340;&#38382;&#39064;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#23398;&#20064;&#21040;&#23494;&#20999;&#25509;&#36817;&#30495;&#23454;&#20998;&#24067;&#30340;&#31574;&#30053;&#22238;&#25253;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20351;&#29992;&#38750;&#31574;&#30053;&#29983;&#25104;&#30340;&#31163;&#32447;&#25968;&#25454;&#38598;&#26469;&#20272;&#31639;&#31574;&#30053;&#22238;&#25253;&#20998;&#24067;&#30340;&#38382;&#39064;&#65292;&#21363;&#20998;&#24067;&#24335;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#65288;OPE&#65289;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Fitted Likelihood Estimation&#65288;FLE&#65289;&#30340;&#31639;&#27861;&#65292;&#23427;&#25191;&#34892;&#20102;&#19968;&#31995;&#21015;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65292;&#20855;&#26377;&#23558;&#20219;&#20309;&#26368;&#20808;&#36827;&#30340;&#27010;&#29575;&#29983;&#25104;&#27169;&#22411;&#38598;&#25104;&#30340;&#28789;&#27963;&#24615;&#65292;&#21482;&#35201;&#23427;&#21487;&#20197;&#36890;&#36807;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#36827;&#34892;&#35757;&#32451;&#12290;FLE&#33021;&#22815;&#29992;&#20110;&#26377;&#38480;&#25110;&#26080;&#38480;&#26102;&#38388;&#25240;&#25187;&#35774;&#32622;&#65292;&#20854;&#20013;&#22870;&#21169;&#21487;&#20197;&#26159;&#22810;&#32500;&#21521;&#37327;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#34920;&#26126;&#65292;&#26080;&#35770;&#26159;&#22312;&#26377;&#38480;&#26102;&#38388;&#25240;&#25187;&#35774;&#32622;&#36824;&#26159;&#26080;&#38480;&#26102;&#38388;&#25240;&#25187;&#35774;&#32622;&#19979;&#65292;FLE&#37117;&#21487;&#20197;&#23398;&#20064;&#21040;&#23494;&#20999;&#25509;&#36817;&#30495;&#23454;&#20998;&#24067;&#30340;&#20998;&#24067;&#65292;&#20998;&#21035;&#22312;&#24635;&#21464;&#24046;&#36317;&#31163;&#21644;Wasserstein&#36317;&#31163;&#19979;&#12290;&#22312;&#35757;&#32451;MLE&#36807;&#31243;&#25104;&#21151;&#26102;&#65292;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#36866;&#29992;&#20110;&#31163;&#32447;&#25968;&#25454;&#35206;&#30422;&#27979;&#35797;&#31574;&#30053;&#30165;&#36857;&#30340;&#26465;&#20214;&#12290;&#22312;&#23454;&#39564;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;FLE&#22312;&#21508;&#31181;&#29615;&#22659;&#20013;&#37117;&#33021;&#21462;&#24471;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of estimating the distribution of the return of a policy using an offline dataset that is not generated from the policy, i.e., distributional offline policy evaluation (OPE). We propose an algorithm called Fitted Likelihood Estimation (FLE), which conducts a sequence of Maximum Likelihood Estimation (MLE) and has the flexibility of integrating any state-of-the-art probabilistic generative models as long as it can be trained via MLE. FLE can be used for both finite-horizon and infinite-horizon discounted settings where rewards can be multi-dimensional vectors. Our theoretical results show that for both finite-horizon and infinite-horizon discounted settings, FLE can learn distributions that are close to the ground truth under total variation distance and Wasserstein distance, respectively. Our theoretical results hold under the conditions that the offline data covers the test policy's traces and that the supervised learning MLE procedures succeed. Experimentally, we
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#23433;&#20840;&#38450;&#24481;&#26041;&#27861;&#30340;&#24179;&#31561;&#24615;&#33021;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#24179;&#31561;&#24230;&#37327;&#21644;&#20998;&#26512;&#26694;&#26550;&#65292;&#40723;&#21169;&#36827;&#19968;&#27493;&#25506;&#32034;&#20844;&#24179;&#24615;&#22312;&#35813;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2302.08973</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#23433;&#20840;&#38450;&#24481;&#20013;&#30340;&#24179;&#31561;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Measuring Equality in Machine Learning Security Defenses. (arXiv:2302.08973v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08973
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#23433;&#20840;&#38450;&#24481;&#26041;&#27861;&#30340;&#24179;&#31561;&#24615;&#33021;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#24179;&#31561;&#24230;&#37327;&#21644;&#20998;&#26512;&#26694;&#26550;&#65292;&#40723;&#21169;&#36827;&#19968;&#27493;&#25506;&#32034;&#20844;&#24179;&#24615;&#22312;&#35813;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#26426;&#22120;&#23398;&#20064;&#23433;&#20840;&#31038;&#21306;&#24050;&#32463;&#21457;&#23637;&#20102;&#35768;&#22810;&#23545;&#25239;&#25915;&#20987;&#30340;&#38450;&#24481;&#26041;&#27861;&#12290;&#20294;&#36825;&#20010;&#31038;&#21306;&#20013;&#40092;&#26377;&#30740;&#31350;&#19968;&#20010;&#38382;&#39064;&#65306;&#36825;&#20123;&#38450;&#24481;&#26041;&#27861;&#20026;&#35841;&#25552;&#20379;&#20445;&#25252;&#21602;&#65311;&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#20123;&#24120;&#35265;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#24182;&#30740;&#31350;&#20102;&#24403;&#36825;&#20123;&#38450;&#24481;&#26041;&#27861;&#34987;&#19981;&#21516;&#30340;&#23376;&#32676;&#20307;&#20351;&#29992;&#26102;&#65292;&#23427;&#20204;&#26159;&#21542;&#20250;&#20135;&#29983;&#24847;&#24819;&#19981;&#21040;&#30340;&#24179;&#31561;&#24615;&#33021;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#24179;&#31561;&#24230;&#37327;&#21644;&#20998;&#26512;&#26694;&#26550;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#23433;&#20840;&#26041;&#27861;&#30340;&#20844;&#24179;&#24615;&#23454;&#35777;&#32467;&#26524;&#26469;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#12290;&#35768;&#22810;&#26041;&#27861;&#21487;&#33021;&#20250;&#30452;&#25509;&#36896;&#25104;&#20260;&#23475;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#26377;&#20559;&#28431;&#27934;&#21644;&#26377;&#20559;&#25490;&#26021;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21644;&#24230;&#37327;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#24378;&#21270;&#35757;&#32451;&#27169;&#22411;&#12289;&#22522;&#20110;&#39044;&#22788;&#29702;&#30340;&#26041;&#27861;&#21644;&#25298;&#32477;&#26041;&#27861;&#65292;&#20197;&#25429;&#25417;&#22312;&#23433;&#20840;&#39044;&#31639;&#19978;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#20010;&#23454;&#38469;&#30340;&#25968;&#25454;&#38598;&#65292;&#20855;&#26377;&#21512;&#29702;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#36866;&#21512;&#20110;&#27979;&#37327;&#38450;&#24481;&#30340;&#24179;&#31561;&#24615;&#12290;&#36890;&#36807;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#29616;&#20195;&#38450;&#24481;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#21644;&#24179;&#31561;&#24615;&#24615;&#33021;&#30340;&#34913;&#37327;&#20215;&#20540;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#25552;&#20986;&#30340;&#25351;&#26631;&#21644;&#26041;&#27861;&#33021;&#22815;&#40723;&#21169;&#21644;&#20419;&#36827;&#26426;&#22120;&#23398;&#20064;&#23433;&#20840;&#21644;&#38450;&#24481;&#39046;&#22495;&#30340;&#20844;&#24179;&#24615;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The machine learning security community has developed myriad defenses for evasion attacks over the past decade. An understudied question in that community is: for whom do these defenses defend? In this work, we consider some common approaches to defending learned systems and whether those approaches may offer unexpected performance inequities when used by different sub-populations. We outline simple parity metrics and a framework for analysis that can begin to answer this question through empirical results of the fairness implications of machine learning security methods. Many methods have been proposed that can cause direct harm, which we describe as biased vulnerability and biased rejection. Our framework and metric can be applied to robustly trained models, preprocessing-based methods, and rejection methods to capture behavior over security budgets. We identify a realistic dataset with a reasonable computational cost suitable for measuring the equality of defenses. Through a case st
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#35299;&#20915;&#20102;&#22266;&#23450;&#39044;&#31639;&#19979;&#65292;&#38024;&#23545;&#38543;&#26426;&#36882;&#22686;&#36172;&#21338;&#26426;&#26368;&#20339;&#33218;&#35782;&#21035;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#36275;&#22815;&#22823;&#30340;&#39044;&#31639;&#19979;&#65292;&#36825;&#20004;&#20010;&#31639;&#27861;&#37117;&#33021;&#27491;&#30830;&#35782;&#21035;&#26368;&#20248;&#36873;&#39033;&#12290;</title><link>http://arxiv.org/abs/2302.07510</link><description>&lt;p&gt;
&#38024;&#23545;&#38543;&#26426;&#36882;&#22686;&#36172;&#21338;&#26426;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Best Arm Identification for Stochastic Rising Bandits. (arXiv:2302.07510v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07510
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#35299;&#20915;&#20102;&#22266;&#23450;&#39044;&#31639;&#19979;&#65292;&#38024;&#23545;&#38543;&#26426;&#36882;&#22686;&#36172;&#21338;&#26426;&#26368;&#20339;&#33218;&#35782;&#21035;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#36275;&#22815;&#22823;&#30340;&#39044;&#31639;&#19979;&#65292;&#36825;&#20004;&#20010;&#31639;&#27861;&#37117;&#33021;&#27491;&#30830;&#35782;&#21035;&#26368;&#20248;&#36873;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#36882;&#22686;&#36172;&#21338;&#26426; (SRB) &#27169;&#22411;&#25551;&#36848;&#20102;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#65292;&#20854;&#20013;&#21487;&#36873;&#36873;&#39033;&#30340;&#39044;&#26399;&#22870;&#21169;&#27599;&#27425;&#36873;&#25321;&#37117;&#20250;&#22686;&#21152;&#12290;&#36825;&#20010;&#35774;&#32622;&#28085;&#30422;&#20102;&#35768;&#22810;&#22330;&#26223;&#65292;&#20854;&#20013;&#21487;&#29992;&#36873;&#39033;&#26159;&#23398;&#20064;&#23454;&#20307;&#65292;&#20854;&#34920;&#29616;&#65288;&#26399;&#26395;&#65289;&#38543;&#26102;&#38388;&#25913;&#21892;&#12290;&#34429;&#28982;&#20197;&#21069;&#30340;&#24037;&#20316;&#35299;&#20915;&#20102;&#36951;&#25022;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#20294;&#26412;&#25991;&#19987;&#27880;&#20110; SRB &#30340;&#22266;&#23450;&#39044;&#31639;&#26368;&#20339;&#33218;&#35782;&#21035; (BAI) &#38382;&#39064;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#32473;&#23450;&#19968;&#23450;&#36718;&#25968;&#30340;&#22266;&#23450;&#39044;&#31639;&#65292;&#25105;&#20204;&#35201;&#22312;&#35782;&#21035;&#36807;&#31243;&#32467;&#26463;&#26102;&#25552;&#20379;&#20851;&#20110;&#26368;&#20339;&#36873;&#39033;&#30340;&#25512;&#33616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#31639;&#27861;&#26469;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#21363; R-UCBE&#65292;&#23427;&#37319;&#29992;&#31867;&#20284;&#20110; UCB &#30340;&#26041;&#27861;&#65292;&#21644; R-SR&#65292;&#23427;&#37319;&#29992;&#36880;&#27493;&#25298;&#32477;&#31243;&#24207;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#36275;&#22815;&#22823;&#30340;&#39044;&#31639;&#19979;&#65292;&#23427;&#20204;&#21487;&#20197;&#20445;&#35777;&#33021;&#22815;&#27491;&#30830;&#35782;&#21035;&#26368;&#20248;&#36873;&#39033;&#30340;&#27010;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545; SRB &#30340; BAI &#38382;&#39064;&#30340;&#38543;&#26426;&#22797;&#26434;&#24230;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#24182;&#20204;&#35777;&#26126;&#20102; R-UCBE &#30340;&#19978;&#30028;&#21644; R-SR &#30340;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic Rising Bandits (SRBs) model sequential decision-making problems in which the expected rewards of the available options increase every time they are selected. This setting captures a wide range of scenarios in which the available options are learning entities whose performance improves (in expectation) over time. While previous works addressed the regret minimization problem, this paper, focuses on the fixed-budget Best Arm Identification (BAI) problem for SRBs. In this scenario, given a fixed budget of rounds, we are asked to provide a recommendation about the best option at the end of the identification process. We propose two algorithms to tackle the above-mentioned setting, namely R-UCBE, which resorts to a UCB-like approach, and R-SR, which employs a successive reject procedure. Then, we prove that, with a sufficiently large budget, they provide guarantees on the probability of properly identifying the optimal option at the end of the learning process. Furthermore, we de
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25512;&#24191;&#24179;&#22343;Lipschitz&#24179;&#28369;&#24615;&#21040;H&#246;lder&#24179;&#28369;&#24615;&#65292;&#24471;&#21040;&#20102;&#20851;&#20110;&#24179;&#22343;H&#246;lder&#24179;&#28369;&#24615;&#30340;&#19978;&#19979;&#39118;&#38505;&#30028;&#65292;&#26368;&#20248;&#30340;&#19979;&#30028;&#23545;&#25968;&#22240;&#23376;&#26368;&#22810;&#24046;&#19968;&#20010;&#65292;&#25552;&#20379;&#20102;&#29420;&#31435;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.06005</link><description>&lt;p&gt;
&#24179;&#22343;H&#246;lder&#24179;&#28369;&#24230;&#19979;&#30340;&#36817;&#20284;&#26368;&#20248;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Near-optimal learning with average H\"older smoothness. (arXiv:2302.06005v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06005
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25512;&#24191;&#24179;&#22343;Lipschitz&#24179;&#28369;&#24615;&#21040;H&#246;lder&#24179;&#28369;&#24615;&#65292;&#24471;&#21040;&#20102;&#20851;&#20110;&#24179;&#22343;H&#246;lder&#24179;&#28369;&#24615;&#30340;&#19978;&#19979;&#39118;&#38505;&#30028;&#65292;&#26368;&#20248;&#30340;&#19979;&#30028;&#23545;&#25968;&#22240;&#23376;&#26368;&#22810;&#24046;&#19968;&#20010;&#65292;&#25552;&#20379;&#20102;&#29420;&#31435;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;Ashlagi&#31561;&#20154;&#65288;COLT 2021&#65289;&#25552;&#20986;&#30340;&#24179;&#22343;Lipschitz&#24179;&#28369;&#24615;&#27010;&#24565;&#25512;&#24191;&#21040;H&#246;lder&#24179;&#28369;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#20851;&#20110;&#24179;&#22343;H&#246;lder&#24179;&#28369;&#24615;&#30340;&#19978;&#19979;&#39118;&#38505;&#30028;&#65292;&#36825;&#20123;&#30028;&#30340;&#36895;&#29575;&#29978;&#33267;&#22312;&#24179;&#22343;Lipschitz&#24179;&#28369;&#24615;&#30340;&#29305;&#27530;&#24773;&#20917;&#19979;&#20063;&#20248;&#20110;&#20043;&#21069;&#24050;&#30693;&#30028;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#19979;&#30028;&#22312;&#21487;&#23454;&#29616;&#24773;&#20917;&#19979;&#26159;&#26368;&#20248;&#30340;&#65292;&#26368;&#22810;&#24046;&#19968;&#20010;&#23545;&#25968;&#22240;&#23376;&#65292;&#20174;&#32780;&#24314;&#31435;&#20102;&#26497;&#23567;&#20540;&#29575;&#12290;&#20174;&#31639;&#27861;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#30001;&#20110;&#25105;&#20204;&#23545;&#24179;&#22343;&#24179;&#28369;&#24230;&#30340;&#23450;&#20041;&#26159;&#38024;&#23545;&#26410;&#30693;&#30340;&#22522;&#30784;&#20998;&#24067;&#30340;&#65292;&#22240;&#27492;&#23398;&#20064;&#32773;&#27809;&#26377;&#20989;&#25968;&#31867;&#30340;&#26174;&#24335;&#34920;&#31034;&#65292;&#26080;&#27861;&#25191;&#34892;ERM&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#29420;&#31435;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We generalize the notion of average Lipschitz smoothness proposed by Ashlagi et al. (COLT 2021) by extending it to H\"older smoothness. This measure of the "effective smoothness" of a function is sensitive to the underlying distribution and can be dramatically smaller than its classic "worst-case H\"older constant. We consider both the realizable and the agnostic (noisy) regression settings, proving upper and lower risk bounds in terms of the average H\"older smoothness; these rates improve upon both previously known rates even in the special case of average Lipschitz smoothness. Moreover, our lower bound is tight in the realizable setting up to log factors, thus we establish the minimax rate. From an algorithmic perspective, since our notion of average smoothness is defined with respect to the unknown underlying distribution, the learner does not have an explicit representation of the function class, hence is unable to execute ERM. Nevertheless, we provide distinct learning algorithms
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#26694;&#26550;ConCerNet&#65292;&#29992;&#20110;&#25552;&#39640;DNN&#21160;&#21147;&#23398;&#24314;&#27169;&#30340;&#21487;&#38752;&#24615;&#65292;&#23454;&#29616;&#23545;&#31995;&#32479;&#19981;&#21464;&#37327;&#30340;&#33258;&#21160;&#25429;&#25417;&#21644;&#20445;&#30041;&#65292;&#32463;&#23454;&#39564;&#35777;&#26126;&#20854;&#24615;&#33021;&#20248;&#20110;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.05783</link><description>&lt;p&gt;
ConCerNet&#65306;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#33258;&#21160;&#21457;&#29616;&#23432;&#24658;&#24459;&#21644;&#21487;&#38752;&#21160;&#21147;&#23398;&#31995;&#32479;&#39044;&#27979;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
ConCerNet: A Contrastive Learning Based Framework for Automated Conservation Law Discovery and Trustworthy Dynamical System Prediction. (arXiv:2302.05783v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05783
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#26694;&#26550;ConCerNet&#65292;&#29992;&#20110;&#25552;&#39640;DNN&#21160;&#21147;&#23398;&#24314;&#27169;&#30340;&#21487;&#38752;&#24615;&#65292;&#23454;&#29616;&#23545;&#31995;&#32479;&#19981;&#21464;&#37327;&#30340;&#33258;&#21160;&#25429;&#25417;&#21644;&#20445;&#30041;&#65292;&#32463;&#23454;&#39564;&#35777;&#26126;&#20854;&#24615;&#33021;&#20248;&#20110;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#22312;&#21160;&#21147;&#23398;&#31995;&#32479;&#24314;&#27169;&#26041;&#38754;&#34920;&#29616;&#20986;&#26497;&#22823;&#30340;&#33021;&#21147;&#65307;&#28982;&#32780;&#65292;&#23427;&#20204;&#36890;&#24120;&#19981;&#36981;&#23432;&#29289;&#29702;&#32422;&#26463;&#65292;&#22914;&#23432;&#24658;&#23450;&#24459;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ConCerNet&#30340;&#26032;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#25552;&#39640;&#22522;&#20110;DNN&#30340;&#21160;&#21147;&#23398;&#24314;&#27169;&#30340;&#21487;&#38752;&#24615;&#65292;&#36171;&#20104;&#19981;&#21464;&#30340;&#23646;&#24615;&#12290;ConCerNet&#30001;&#20004;&#20010;&#27493;&#39588;&#32452;&#25104;:(i)&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#33258;&#21160;&#25429;&#25417;&#36712;&#36857;&#35266;&#27979;&#20013;&#30340;&#31995;&#32479;&#19981;&#21464;&#37327;(&#21363;&#23432;&#24658;&#24615;&#36136;)&#65307;(ii)&#31070;&#32463;&#25237;&#24433;&#23618;&#65292;&#20445;&#35777;&#23398;&#20064;&#21040;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#20445;&#30041;&#23398;&#20064;&#21040;&#30340;&#19981;&#21464;&#37327;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#23398;&#20064;&#21040;&#30340;&#28508;&#22312;&#34920;&#31034;&#21644;&#26410;&#30693;&#31995;&#32479;&#19981;&#21464;&#37327;&#20989;&#25968;&#20043;&#38388;&#30340;&#21151;&#33021;&#20851;&#31995;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22352;&#26631;&#35823;&#24046;&#21644;&#23432;&#24658;&#25351;&#26631;&#26041;&#38754;&#22987;&#32456;&#27604;&#22522;&#32447;&#31070;&#32463;&#32593;&#32476;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;&#20351;&#29992;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#21442;&#25968;&#21270;&#19988;&#19981;&#20381;&#36182;&#20110;&#20808;&#21069;&#30693;&#35782;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21160;&#21147;&#23398;&#26041;&#38754;&#20855;&#26377;&#24191;&#38420;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNN) have shown great capacity of modeling a dynamical system; nevertheless, they usually do not obey physics constraints such as conservation laws. This paper proposes a new learning framework named ConCerNet to improve the trustworthiness of the DNN based dynamics modeling to endow the invariant properties. ConCerNet consists of two steps: (i) a contrastive learning method to automatically capture the system invariants (i.e. conservation properties) along the trajectory observations; (ii) a neural projection layer to guarantee that the learned dynamics models preserve the learned invariants. We theoretically prove the functional relationship between the learned latent representation and the unknown system invariant function. Experiments show that our method consistently outperforms the baseline neural networks in both coordinate error and conservation metrics by a large margin. With neural network based parameterization and no dependence on prior knowledge, our 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22240;&#26524;&#20851;&#31995;&#24378;&#21270;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#65292;&#23558;&#29616;&#26377;&#30340;CRL&#26041;&#27861;&#20998;&#20026;&#20004;&#31867;&#65292;&#24182;&#35752;&#35770;&#20102;&#22240;&#26524;&#20851;&#31995;&#21644;RL&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2302.05209</link><description>&lt;p&gt;
&#22240;&#26524;&#20851;&#31995;&#24378;&#21270;&#23398;&#20064;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Causal Reinforcement Learning. (arXiv:2302.05209v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05209
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22240;&#26524;&#20851;&#31995;&#24378;&#21270;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#65292;&#23558;&#29616;&#26377;&#30340;CRL&#26041;&#27861;&#20998;&#20026;&#20004;&#31867;&#65292;&#24182;&#35752;&#35770;&#20102;&#22240;&#26524;&#20851;&#31995;&#21644;RL&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#39046;&#22495;&#30340;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#20013;&#65292;&#24378;&#21270;&#23398;&#20064;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#20173;&#38754;&#20020;&#25968;&#25454;&#25928;&#29575;&#21644;&#35299;&#37322;&#24615;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#35768;&#22810;&#30740;&#31350;&#32773;&#26368;&#36817;&#24050;&#32463;&#21033;&#29992;&#22240;&#26524;&#20851;&#31995;&#25991;&#29486;&#30340;&#35265;&#35299;&#65292;&#24102;&#26469;&#20102;&#32321;&#33635;&#30340;&#24037;&#20316;&#65292;&#20197;&#32479;&#19968;&#22240;&#26524;&#20851;&#31995;&#30340;&#20248;&#28857;&#65292;&#24182;&#35299;&#20915;RL&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#24635;&#32467;&#36825;&#20123;&#22240;&#26524;&#20851;&#31995;&#24378;&#21270;&#23398;&#20064;&#65288;CRL&#65289;&#24037;&#20316;&#65292;&#25552;&#20379;CRL&#26041;&#27861;&#30340;&#32508;&#36848;&#65292;&#20197;&#21450;&#30740;&#31350;&#22240;&#26524;&#20851;&#31995;&#23545;RL&#30340;&#28508;&#22312;&#21151;&#33021;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#24517;&#35201;&#24615;&#21644;&#24847;&#20041;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#26681;&#25454;&#23427;&#20204;&#26159;&#21542;&#25552;&#21069;&#32473;&#20986;&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#30340;&#20449;&#24687;&#65292;&#23558;&#29616;&#26377;&#30340;CRL&#26041;&#27861;&#20998;&#20026;&#20004;&#31867;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20998;&#26512;&#27599;&#20010;&#31867;&#21035;&#22312;&#19981;&#21516;&#27169;&#22411;&#30340;&#35268;&#33539;&#21270;&#26041;&#38754;&#30340;&#20851;&#31995;&#65292;&#21253;&#25324;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#65292;&#37096;&#20998;&#35266;&#23519;&#21040;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#65292;&#22810;&#33218;&#32769;&#34382;&#26426;&#65288;MAB&#65289;&#21644;&#21160;&#24577;&#27835;&#30103;&#25928;&#26524;&#65288;DTE&#65289;&#12290;&#36890;&#36807;&#21508;&#31181;&#35282;&#24230;&#35752;&#35770;&#22240;&#26524;&#20851;&#31995;&#21644;RL&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21253;&#25324;&#25506;&#32034;-&#24320;&#21457;&#22256;&#22659;&#65292;&#21453;&#20107;&#23454;&#35780;&#20272;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#24635;&#20043;&#65292;&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#20840;&#38754;&#27010;&#36848;&#20102;&#22240;&#26524;&#20851;&#31995;&#24378;&#21270;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#25351;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#28508;&#22312;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Reinforcement Learning (RL) achieves tremendous success in sequential decision-making problems of many domains, it still faces key challenges of data inefficiency and the lack of interpretability. Interestingly, many researchers have leveraged insights from the causality literature recently, bringing forth flourishing works to unify the merits of causality and address well the challenges from RL. As such, it is of great necessity and significance to collate these Causal Reinforcement Learning (CRL) works, offer a review of CRL methods, and investigate the potential functionality from causality toward RL. In particular, we divide existing CRL approaches into two categories according to whether their causality-based information is given in advance or not. We further analyze each category in terms of the formalization of different models, ranging from the Markov Decision Process (MDP), Partially Observed Markov Decision Process (POMDP), Multi-Arm Bandits (MAB), and Dynamic Treatment
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#26356;&#22909;&#30340;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#23545;&#25239;&#35757;&#32451;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#37319;&#29992;&#26368;&#26032;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#25105;&#20204;&#35757;&#32451;&#30340;&#27169;&#22411;&#20165;&#20351;&#29992;&#29983;&#25104;&#30340;&#25968;&#25454;&#23601;&#22312;RobustBench&#19978;&#23454;&#29616;&#20102;&#26368;&#20339;&#24615;&#33021;&#65292;&#24182;&#22312;CIFAR-10&#21644;CIFAR-100&#25968;&#25454;&#38598;&#19978;&#20998;&#21035;&#25552;&#39640;&#20102;$+4.58\%$&#21644;$+8.03\%$&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.04638</link><description>&lt;p&gt;
&#26356;&#22909;&#30340;&#25193;&#25955;&#27169;&#22411;&#36827;&#19968;&#27493;&#25913;&#36827;&#23545;&#25239;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Better Diffusion Models Further Improve Adversarial Training. (arXiv:2302.04638v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04638
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#26356;&#22909;&#30340;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#23545;&#25239;&#35757;&#32451;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#37319;&#29992;&#26368;&#26032;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#25105;&#20204;&#35757;&#32451;&#30340;&#27169;&#22411;&#20165;&#20351;&#29992;&#29983;&#25104;&#30340;&#25968;&#25454;&#23601;&#22312;RobustBench&#19978;&#23454;&#29616;&#20102;&#26368;&#20339;&#24615;&#33021;&#65292;&#24182;&#22312;CIFAR-10&#21644;CIFAR-100&#25968;&#25454;&#38598;&#19978;&#20998;&#21035;&#25552;&#39640;&#20102;$+4.58\%$&#21644;$+8.03\%$&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#30001;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#20135;&#29983;&#30340;&#25968;&#25454;&#21487;&#20197;&#25552;&#39640;&#23545;&#25239;&#24615;&#35757;&#32451;&#12290;&#38543;&#30528;&#25193;&#25955;&#27169;&#22411;&#30340;&#36805;&#36895;&#21457;&#23637;&#65292;&#19968;&#20010;&#33258;&#28982;&#30340;&#38382;&#39064;&#20986;&#29616;&#20102;&#65306;&#26356;&#22909;&#30340;&#25193;&#25955;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#36827;&#19968;&#27493;&#25913;&#36827;&#23545;&#25239;&#35757;&#32451;&#65311;&#26412;&#25991;&#37319;&#29992;&#20102;&#26368;&#26032;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#19982;DDPM&#30456;&#27604;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#26356;&#39640;&#30340;&#25928;&#29575;&#65288;&#32422;$\sim20$&#20010;&#37319;&#26679;&#27493;&#39588;&#65289;&#21644;&#26356;&#20302;&#30340;&#22270;&#20687;&#36136;&#37327;&#65288;&#26356;&#20302;&#30340;FID&#20998;&#25968;&#65289;&#65292;&#35777;&#26126;&#20102;&#26356;&#22909;&#30340;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#23545;&#25239;&#35757;&#32451;&#12290;&#25105;&#20204;&#35757;&#32451;&#30340;&#27169;&#22411;&#20165;&#20351;&#29992;&#29983;&#25104;&#30340;&#25968;&#25454;&#65288;&#27809;&#26377;&#22806;&#37096;&#25968;&#25454;&#38598;&#65289;&#22312;RobustBench&#19978;&#23454;&#29616;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;&#22312;$\ell_\infty$-norm&#23041;&#32961;&#27169;&#22411;&#19979;&#65292;&#24403;$\epsilon=8/255$&#26102;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;CIFAR-10&#21644;CIFAR-100&#19978;&#20998;&#21035;&#36798;&#21040;$70.69\%$&#21644;$42.67\%$&#30340;&#40065;&#26834;&#20934;&#30830;&#24230;&#65292;&#21363;&#20998;&#21035;&#27604;&#20197;&#21069;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#25552;&#39640;&#20102;$+4.58\%$&#21644;$+8.03\%$&#12290;&#22312;$\ell_2$-norm&#23041;&#32961;&#27169;&#22411;&#19979;&#65292;&#24403;$\epsilon=128/255$&#26102;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;CIFAR-10&#19978;&#21487;&#20197;&#36798;&#21040;$84.86\%$&#30340;&#20934;&#30830;&#29575;&#65292;&#25552;&#39640;&#20102;$+4.44\%$&#12290;
&lt;/p&gt;
&lt;p&gt;
It has been recognized that the data generated by the denoising diffusion probabilistic model (DDPM) improves adversarial training. After two years of rapid development in diffusion models, a question naturally arises: can better diffusion models further improve adversarial training? This paper gives an affirmative answer by employing the most recent diffusion model which has higher efficiency ($\sim 20$ sampling steps) and image quality (lower FID score) compared with DDPM. Our adversarially trained models achieve state-of-the-art performance on RobustBench using only generated data (no external datasets). Under the $\ell_\infty$-norm threat model with $\epsilon=8/255$, our models achieve $70.69\%$ and $42.67\%$ robust accuracy on CIFAR-10 and CIFAR-100, respectively, i.e. improving upon previous state-of-the-art models by $+4.58\%$ and $+8.03\%$. Under the $\ell_2$-norm threat model with $\epsilon=128/255$, our models achieve $84.86\%$ on CIFAR-10 ($+4.44\%$). These results also beat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24635;&#32467;&#20102;&#19968;&#20123;&#25216;&#24039;&#29992;&#20110;&#25913;&#36827;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#25552;&#21462;&#65292;&#25552;&#20986;&#20102;&#22312;&#25991;&#26412;&#29983;&#25104;&#21644;&#25991;&#26412;&#25490;&#21517;&#20013;&#21487;&#20197;&#20351;&#29992;&#30340;&#25216;&#24039;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#20123;&#25216;&#24039;&#23545;&#20110;&#25552;&#39640;&#35757;&#32451;&#25968;&#25454;&#25552;&#21462;&#30340;&#25928;&#26524;&#38750;&#24120;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2302.04460</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#25552;&#21462;&#30340;&#25216;&#24039;&#24635;&#32467;
&lt;/p&gt;
&lt;p&gt;
Bag of Tricks for Training Data Extraction from Language Models. (arXiv:2302.04460v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04460
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24635;&#32467;&#20102;&#19968;&#20123;&#25216;&#24039;&#29992;&#20110;&#25913;&#36827;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#25552;&#21462;&#65292;&#25552;&#20986;&#20102;&#22312;&#25991;&#26412;&#29983;&#25104;&#21644;&#25991;&#26412;&#25490;&#21517;&#20013;&#21487;&#20197;&#20351;&#29992;&#30340;&#25216;&#24039;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#20123;&#25216;&#24039;&#23545;&#20110;&#25552;&#39640;&#35757;&#32451;&#25968;&#25454;&#25552;&#21462;&#30340;&#25928;&#26524;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#26029;&#30740;&#31350;&#65292;&#38544;&#31169;&#20445;&#25252;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#35757;&#32451;&#25968;&#25454;&#25552;&#21462;&#20316;&#20026;&#28508;&#22312;&#30340;&#35780;&#20272;&#38544;&#31169;&#27844;&#38706;&#30340;&#24037;&#20855;&#21464;&#24471;&#38750;&#24120;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#39033;&#20219;&#21153;&#30340;&#22256;&#38590;&#31243;&#24230;&#65292;&#30446;&#21069;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#26041;&#27861;&#20173;&#28982;&#19981;&#22815;&#26377;&#25928;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20123;&#25216;&#24039;&#29992;&#20110;&#25913;&#36827;&#35757;&#32451;&#25968;&#25454;&#25552;&#21462;&#65292;&#25105;&#20204;&#23545;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#30001;&#20110;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#25552;&#21462;&#26041;&#27861;&#20351;&#29992;&#29983;&#25104;&#28982;&#21518;&#25490;&#24207;&#30340;&#27969;&#31243;&#65288;&#20363;&#22914;&#65292;&#29983;&#25104;&#28508;&#22312;&#30340;&#35757;&#32451;&#25968;&#25454;&#25991;&#26412;&#65292;&#28982;&#21518;&#26681;&#25454;&#29305;&#23450;&#30340;&#26631;&#20934;&#23545;&#23427;&#20204;&#36827;&#34892;&#25490;&#24207;&#65289;&#65292;&#22240;&#27492;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#22312;&#20110;&#25991;&#26412;&#29983;&#25104;&#21644;&#25991;&#26412;&#25490;&#21517;&#30340;&#25216;&#24039;&#12290;&#65288;&#20363;&#22914;&#65292;&#37319;&#26679;&#31574;&#30053;&#21644;&#20196;&#29260;&#32423;&#26631;&#20934;&#65289;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19968;&#20123;&#20043;&#21069;&#34987;&#24573;&#35270;&#30340;&#25216;&#24039;&#23545;&#20110;&#35757;&#32451;&#25968;&#25454;&#25552;&#21462;&#30340;&#25104;&#21151;&#38750;&#24120;&#20851;&#38190;&#12290;&#22522;&#20110;GPT-Neo 1.3B&#30340;&#35780;&#20272;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#25216;&#24039;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the advance of language models, privacy protection is receiving more attention. Training data extraction is therefore of great importance, as it can serve as a potential tool to assess privacy leakage. However, due to the difficulty of this task, most of the existing methods are proof-of-concept and still not effective enough. In this paper, we investigate and benchmark tricks for improving training data extraction using a publicly available dataset. Because most existing extraction methods use a pipeline of generating-then-ranking, i.e., generating text candidates as potential training data and then ranking them based on specific criteria, our research focuses on the tricks for both text generation (e.g., sampling strategy) and text ranking (e.g., token-level criteria). The experimental results show that several previously overlooked tricks can be crucial to the success of training data extraction. Based on the GPT-Neo 1.3B evaluation results, our proposed tricks outperform the b
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#31163;&#25955;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#25991;&#26412;&#25552;&#31034;&#65292;&#36827;&#32780;&#25511;&#21046;&#29616;&#20195;&#29983;&#25104;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#20248;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#21644;&#25991;&#26412;&#21040;&#25991;&#26412;&#30340;&#24212;&#29992;&#65292;&#20026;API&#29992;&#25143;&#25552;&#20379;&#20102;&#36731;&#26494;&#29983;&#25104;&#12289;&#21457;&#29616;&#12289;&#28151;&#21512;&#21644;&#21305;&#37197;&#22270;&#20687;&#27010;&#24565;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#33258;&#21160;&#21457;&#29616;&#30828;&#25552;&#31034;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.03668</link><description>&lt;p&gt;
&#30828;&#25552;&#31034;&#21464;&#31616;&#21333;&#65306;&#29992;&#22522;&#20110;&#26799;&#24230;&#30340;&#31163;&#25955;&#20248;&#21270;&#26041;&#27861;&#36827;&#34892;&#25552;&#31034;&#35843;&#33410;&#21644;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery. (arXiv:2302.03668v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03668
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#31163;&#25955;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#25991;&#26412;&#25552;&#31034;&#65292;&#36827;&#32780;&#25511;&#21046;&#29616;&#20195;&#29983;&#25104;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#20248;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#21644;&#25991;&#26412;&#21040;&#25991;&#26412;&#30340;&#24212;&#29992;&#65292;&#20026;API&#29992;&#25143;&#25552;&#20379;&#20102;&#36731;&#26494;&#29983;&#25104;&#12289;&#21457;&#29616;&#12289;&#28151;&#21512;&#21644;&#21305;&#37197;&#22270;&#20687;&#27010;&#24565;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#33258;&#21160;&#21457;&#29616;&#30828;&#25552;&#31034;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#29983;&#25104;&#27169;&#22411;&#30340;&#20248;&#28857;&#22312;&#20110;&#21487;&#20197;&#36890;&#36807;&#22522;&#20110;&#25991;&#26412;&#30340;&#25552;&#31034;&#36827;&#34892;&#25511;&#21046;&#12290;&#20256;&#32479;&#30340;&#8220;&#30828;&#8221;&#25552;&#31034;&#26159;&#30001;&#21487;&#35299;&#37322;&#30340;&#35789;&#27719;&#21644;&#26631;&#35760;&#26500;&#25104;&#65292;&#24517;&#39035;&#30001;&#20154;&#25163;&#24037;&#21046;&#20316;&#12290;&#27492;&#22806;&#36824;&#26377;&#8220;&#36719;&#8221;&#25552;&#31034;&#65292;&#23427;&#20204;&#30001;&#36830;&#32493;&#30340;&#29305;&#24449;&#21521;&#37327;&#32452;&#25104;&#65292;&#21487;&#20197;&#36890;&#36807;&#24378;&#22823;&#30340;&#20248;&#21270;&#26041;&#27861;&#21457;&#29616;&#65292;&#20294;&#23427;&#20204;&#19981;&#33021;&#24456;&#23481;&#26131;&#22320;&#35299;&#37322;&#65292;&#19981;&#33021;&#22312;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#37325;&#22797;&#20351;&#29992;&#65292;&#20063;&#19981;&#33021;&#29992;&#20110;&#22522;&#20110;&#25991;&#26412;&#30340;&#25509;&#21475;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#25928;&#26799;&#24230;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#26469;&#31283;&#20581;&#22320;&#20248;&#21270;&#30828;&#25991;&#26412;&#25552;&#31034;&#12290;&#35813;&#26041;&#27861;&#33258;&#21160;&#20026;&#25991;&#26412;&#21040;&#22270;&#20687;&#21644;&#25991;&#26412;&#21040;&#25991;&#26412;&#24212;&#29992;&#29983;&#25104;&#30828;&#25991;&#26412;&#25552;&#31034;&#12290;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#35774;&#32622;&#20013;&#65292;&#35813;&#26041;&#27861;&#20026;&#25193;&#25955;&#27169;&#22411;&#21019;&#24314;&#30828;&#25552;&#31034;&#65292;&#20351;API&#29992;&#25143;&#21487;&#20197;&#36731;&#26494;&#29983;&#25104;&#12289;&#21457;&#29616;&#12289;&#28151;&#21512;&#21644;&#21305;&#37197;&#22270;&#20687;&#27010;&#24565;&#65292;&#32780;&#19981;&#38656;&#35201;&#20107;&#20808;&#20102;&#35299;&#22914;&#20309;&#25552;&#31034;&#27169;&#22411;&#12290;&#22312;&#25991;&#26412;&#21040;&#25991;&#26412;&#30340;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#33258;&#21160;&#21457;&#29616;&#30828;&#25552;&#31034;&#21487;&#20197;&#26377;&#25928;&#22320;&#20248;&#21270;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The strength of modern generative models lies in their ability to be controlled through text-based prompts. Typical "hard" prompts are made from interpretable words and tokens, and must be hand-crafted by humans. There are also "soft" prompts, which consist of continuous feature vectors. These can be discovered using powerful optimization methods, but they cannot be easily interpreted, re-used across models, or plugged into a text-based interface.  We describe an approach to robustly optimize hard text prompts through efficient gradient-based optimization. Our approach automatically generates hard text-based prompts for both text-to-image and text-to-text applications. In the text-to-image setting, the method creates hard prompts for diffusion models, allowing API users to easily generate, discover, and mix and match image concepts without prior knowledge on how to prompt the model. In the text-to-text setting, we show that hard prompts can be automatically discovered that are effectiv
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;&#25968;&#25454;&#22686;&#24378;&#12289;&#32593;&#32476;&#26550;&#26500;&#21644;&#35757;&#32451;&#31639;&#27861;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#21644;&#19979;&#28216;&#20219;&#21153;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#20026;SSL&#20174;&#19994;&#20154;&#21592;&#25552;&#20379;&#20102;&#19968;&#20123;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2302.02774</link><description>&lt;p&gt;
SSL&#30340;&#30456;&#20114;&#20316;&#29992;&#65306;&#22686;&#24378;&#12289;&#24402;&#32435;&#20559;&#24046;&#21644;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
The SSL Interplay: Augmentations, Inductive Bias, and Generalization. (arXiv:2302.02774v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02774
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;&#25968;&#25454;&#22686;&#24378;&#12289;&#32593;&#32476;&#26550;&#26500;&#21644;&#35757;&#32451;&#31639;&#27861;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#21644;&#19979;&#28216;&#20219;&#21153;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#20026;SSL&#20174;&#19994;&#20154;&#21592;&#25552;&#20379;&#20102;&#19968;&#20123;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#24050;&#25104;&#20026;&#19968;&#31181;&#26080;&#38656;&#30417;&#30563;&#21363;&#21487;&#20174;&#21407;&#22987;&#25968;&#25454;&#20013;&#23398;&#20064;&#34920;&#31034;&#30340;&#24378;&#22823;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#24037;&#31243;&#24072;&#38754;&#20020;&#30528;&#35843;&#25972;&#20248;&#21270;&#22120;&#21644;&#35757;&#32451;&#36807;&#31243;&#20013;&#34920;&#31034;&#22604;&#38519;&#31561;&#38382;&#39064;&#12290;&#36825;&#20123;&#25361;&#25112;&#20419;&#20351;&#25105;&#20204;&#38656;&#35201;&#19968;&#31181;&#29702;&#35770;&#26469;&#38416;&#26126;&#25968;&#25454;&#22686;&#24378;&#12289;&#32593;&#32476;&#26550;&#26500;&#21644;&#35757;&#32451;&#31639;&#27861;&#30340;&#36873;&#25321;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#29702;&#35770;&#21451;&#22909;&#30340;&#35774;&#32622;&#19979;&#65292;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#21644;&#19979;&#28216;&#20219;&#21153;&#30340;&#27867;&#21270;&#24615;&#33021;&#36827;&#34892;&#31934;&#30830;&#20998;&#26512;&#65292;&#30740;&#31350;&#20102;&#36825;&#31181;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#24378;&#35843;&#20102;&#25105;&#20204;&#29702;&#35770;&#24471;&#20986;&#30340;SSL&#20174;&#19994;&#32773;&#30340;&#19968;&#20123;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) has emerged as a powerful framework to learn representations from raw data without supervision. Yet in practice, engineers face issues such as instability in tuning optimizers and collapse of representations during training. Such challenges motivate the need for a theory to shed light on the complex interplay between the choice of data augmentation, network architecture, and training algorithm. We study such an interplay with a precise analysis of generalization performance on both pretraining and downstream tasks in a theory friendly setup, and highlight several insights for SSL practitioners that arise from our theory.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#65292;&#21487;&#23398;&#20064;&#27604;&#20363;&#22240;&#23376;&#21152;&#21095;&#20102;&#26435;&#37325;&#25391;&#33633;&#65292;&#26412;&#25991;&#25552;&#20986;&#19977;&#31181;&#25216;&#26415;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.02210</link><description>&lt;p&gt;
&#20302;&#20301;&#35270;&#35273;&#21464;&#21387;&#22120;&#30340;&#26080;&#25391;&#33633;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Oscillation-free Quantization for Low-bit Vision Transformers. (arXiv:2302.02210v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02210
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#21487;&#23398;&#20064;&#27604;&#20363;&#22240;&#23376;&#21152;&#21095;&#20102;&#26435;&#37325;&#25391;&#33633;&#65292;&#26412;&#25991;&#25552;&#20986;&#19977;&#31181;&#25216;&#26415;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#24847;&#35782;&#35757;&#32451;&#30340;&#19968;&#20010;&#19981;&#33391;&#21103;&#20316;&#29992;&#26159;&#26435;&#37325;&#25391;&#33633;&#65292;&#20854;&#20013;&#37327;&#21270;&#26435;&#37325;&#32463;&#24120;&#22312;&#20004;&#20010;&#37327;&#21270;&#32423;&#21035;&#20043;&#38388;&#36339;&#21160;&#65292;&#23548;&#33268;&#35757;&#32451;&#19981;&#31283;&#23450;&#21644;&#23376;&#20248;&#21270;&#30340;&#26368;&#32456;&#27169;&#22411;&#12290;&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#21487;&#23398;&#20064;&#30340;&#27604;&#20363;&#22240;&#23376;&#8212;&#8212;&#22312;&#37327;&#21270;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;$\textit{de facto}$&#35774;&#32622;&#8212;&#8212;&#21152;&#21095;&#20102;&#26435;&#37325;&#25391;&#33633;&#12290;&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#21487;&#23398;&#20064;&#27604;&#20363;&#22240;&#23376;&#19982;&#37327;&#21270;&#26435;&#37325;&#25391;&#33633;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#20197;ViT&#20026;&#26696;&#20363;&#26469;&#35828;&#26126;&#21457;&#29616;&#21644;&#35299;&#20915;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#33258;&#27880;&#24847;&#21147;&#23618;&#20013;&#37327;&#21270;&#26435;&#37325;&#30340;$\textit{query}$&#21644;$\textit{key}$&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#23384;&#20351;ViT&#23481;&#26131;&#21463;&#21040;&#25391;&#33633;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30456;&#24212;&#22320;&#25552;&#20986;&#20102;&#19977;&#31181;&#25216;&#26415;&#65306;&#32479;&#35745;&#26435;&#37325;&#37327;&#21270;&#65288;$\rm StatsQ$&#65289;&#20197;&#25913;&#21892;&#37327;&#21270;&#40065;&#26834;&#24615;&#65292;&#19982;&#26222;&#36941;&#20351;&#29992;&#30340;&#21487;&#23398;&#20064;&#27604;&#20363;&#22240;&#23376;&#26041;&#27861;&#30456;&#27604;&#65307;&#32622;&#20449;&#24230;&#24341;&#23548;&#30340;&#36864;&#28779;&#65288;$\rm CGA$&#65289;&#22312;&#35757;&#32451;&#26399;&#38388;&#20923;&#32467;&#20855;&#26377;$\textit{&#39640;&#32622;&#20449;&#24230;}$&#30340;&#26435;&#37325;&#65292;&#20197;&#20943;&#23569;&#26435;&#37325;&#25391;&#33633;&#65307;&#20197;&#21450;&#30456;&#20114;&#20381;&#36182;&#26435;&#37325;&#30340;&#22343;&#34913;&#65288;$\rm IWEqual$&#65289;&#65292;&#20197;&#26377;&#25928;&#22788;&#29702;&#30456;&#20114;&#20381;&#36182;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30456;&#27604;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Weight oscillation is an undesirable side effect of quantization-aware training, in which quantized weights frequently jump between two quantized levels, resulting in training instability and a sub-optimal final model. We discover that the learnable scaling factor, a widely-used $\textit{de facto}$ setting in quantization aggravates weight oscillation. In this study, we investigate the connection between the learnable scaling factor and quantized weight oscillation and use ViT as a case driver to illustrate the findings and remedies. In addition, we also found that the interdependence between quantized weights in $\textit{query}$ and $\textit{key}$ of a self-attention layer makes ViT vulnerable to oscillation. We, therefore, propose three techniques accordingly: statistical weight quantization ($\rm StatsQ$) to improve quantization robustness compared to the prevalent learnable-scale-based method; confidence-guided annealing ($\rm CGA$) that freezes the weights with $\textit{high confi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27973;&#23618;&#27169;&#22411;&#21644;&#24816;&#24615;&#20256;&#25773;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411; LazyGNN&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#22270;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#20855;&#26377;&#24456;&#39640;&#30340;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.01503</link><description>&lt;p&gt;
&#36890;&#36807;&#24816;&#24615;&#20256;&#25773;&#23454;&#29616;&#22823;&#35268;&#27169;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340; LazyGNN
&lt;/p&gt;
&lt;p&gt;
LazyGNN: Large-Scale Graph Neural Networks via Lazy Propagation. (arXiv:2302.01503v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01503
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27973;&#23618;&#27169;&#22411;&#21644;&#24816;&#24615;&#20256;&#25773;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411; LazyGNN&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#22270;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#20855;&#26377;&#24456;&#39640;&#30340;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#26356;&#28145;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#25429;&#25417;&#22270;&#20013;&#30340;&#36828;&#31243;&#20381;&#36182;&#24615;&#24102;&#26469;&#20102;&#22909;&#22788;&#12290;&#20294;&#26159;&#65292;&#22312;&#22823;&#35268;&#27169;&#22270;&#20013;&#65292;&#30001;&#20110;&#37051;&#22495;&#29190;&#28856;&#38382;&#39064;&#65292;&#26356;&#28145;&#30340;GNN&#20250;&#21463;&#21040;&#38271;&#26399;&#30340;&#25193;&#23637;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#26356;&#27973;&#30340;&#27169;&#22411;&#26469;&#25429;&#33719;&#22270;&#20013;&#30340;&#36828;&#31243;&#20381;&#36182;&#24615;&#65292;&#20174;&#32780;&#24471;&#21040;&#19968;&#31181;&#26356;&#39640;&#25928;&#30340;&#27169;&#22411;&#8212;&#8212;LazyGNN&#65292;&#29992;&#20110;&#22270;&#34920;&#31034;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;LazyGNN &#19982;&#29616;&#26377;&#30340;&#21487;&#25193;&#23637;&#26041;&#27861;&#65288;&#22914;&#37319;&#26679;&#26041;&#27861;&#65289;&#20860;&#23481;&#65292;&#21487;&#20197;&#36890;&#36807;&#24320;&#21457;&#28151;&#21512;&#25209;&#37327;&#30340;LazyGNN&#26469;&#36827;&#19968;&#27493;&#21152;&#36895;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#26174;&#31034;&#20102;&#23427;&#22312;&#22823;&#35268;&#27169;&#22522;&#20934;&#27979;&#35797;&#19978;&#20248;&#24322;&#30340;&#39044;&#27979;&#24615;&#33021;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;LazyGNN &#30340;&#23454;&#29616;&#21487;&#22312; https://github.com/RXPHD/Lazy_GNN &#36827;&#34892;&#26597;&#30475;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have demonstrated the benefits of capturing long-distance dependency in graphs by deeper graph neural networks (GNNs). But deeper GNNs suffer from the long-lasting scalability challenge due to the neighborhood explosion problem in large-scale graphs. In this work, we propose to capture long-distance dependency in graphs by shallower models instead of deeper models, which leads to a much more efficient model, LazyGNN, for graph representation learning. Moreover, we demonstrate that LazyGNN is compatible with existing scalable approaches (such as sampling methods) for further accelerations through the development of mini-batch LazyGNN. Comprehensive experiments demonstrate its superior prediction performance and scalability on large-scale benchmarks. The implementation of LazyGNN is available at https://github.com/RXPHD/Lazy_GNN.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#21512;&#25104;&#26679;&#26412;&#23454;&#29616;&#20840;&#23616;&#20248;&#21270;&#65292;&#21152;&#20837;&#35760;&#24405;&#32423;&#24046;&#20998;&#38544;&#31169;&#20197;&#20445;&#25252;&#38544;&#31169;&#65292;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#25968;&#25454;&#38598;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.01068</link><description>&lt;p&gt;
Fed-GLOSS-DP: &#21033;&#29992;&#20855;&#26377;&#35760;&#24405;&#32423;&#24046;&#20998;&#38544;&#31169;&#30340;&#21512;&#25104;&#38598;&#36827;&#34892;&#32852;&#37030;&#20840;&#23616;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Fed-GLOSS-DP: Federated, Global Learning using Synthetic Sets with Record Level Differential Privacy. (arXiv:2302.01068v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#21512;&#25104;&#26679;&#26412;&#23454;&#29616;&#20840;&#23616;&#20248;&#21270;&#65292;&#21152;&#20837;&#35760;&#24405;&#32423;&#24046;&#20998;&#38544;&#31169;&#20197;&#20445;&#25252;&#38544;&#31169;&#65292;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#25968;&#25454;&#38598;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Fed-GLOSS-DP&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#20445;&#25252;&#38544;&#31169;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#12290;&#19982;&#20197;&#21069;&#30340;&#32447;&#24615;&#36880;&#28857;&#26799;&#24230;&#20998;&#20139;&#26041;&#26696;&#65288;&#22914;FedAvg&#65289;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#20844;&#24335;&#21033;&#29992;&#20174;&#23458;&#25143;&#31471;&#25509;&#25910;&#21040;&#30340;&#21512;&#25104;&#26679;&#26412;&#23454;&#29616;&#20102;&#19968;&#31181;&#20840;&#23616;&#20248;&#21270;&#12290;&#36825;&#20123;&#21512;&#25104;&#26679;&#26412;&#20316;&#20026;&#25439;&#22833;&#26367;&#20195;&#29289;&#65292;&#36890;&#36807;&#27169;&#25311;&#26412;&#22320;&#21306;&#22495;&#20869;&#30495;&#23454;&#22270;&#20687;&#30340;&#23454;&#29992;&#24615;&#26469;&#36817;&#20284;&#26412;&#22320;&#25439;&#22833;&#22320;&#24418;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#34913;&#37327;&#26377;&#25928;&#36924;&#36817;&#21306;&#22495;&#30340;&#26041;&#27861;&#65292;&#21453;&#26144;&#20102;&#36817;&#20284;&#30340;&#36136;&#37327;&#12290;&#22240;&#27492;&#65292;&#26381;&#21153;&#22120;&#21487;&#20197;&#24674;&#22797;&#20840;&#23616;&#25439;&#22833;&#22320;&#24418;&#24182;&#20840;&#38754;&#20248;&#21270;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#21463;&#26085;&#30410;&#20005;&#37325;&#30340;&#38544;&#31169;&#38382;&#39064;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#35760;&#24405;&#32423;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#26080;&#32541;&#37197;&#21512;&#65292;&#20026;&#23458;&#25143;&#31471;&#19978;&#30340;&#27599;&#20010;&#25968;&#25454;&#35760;&#24405;&#25552;&#20379;&#29702;&#35770;&#19978;&#30340;&#38544;&#31169;&#20445;&#35777;&#12290;&#24191;&#27867;&#30340;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#20844;&#24335;&#22312;&#20855;&#26377;&#39640;&#24230;&#20542;&#26012;&#20998;&#24067;&#30340;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work proposes Fed-GLOSS-DP, a novel privacy-preserving approach for federated learning. Unlike previous linear point-wise gradient-sharing schemes, such as FedAvg, our formulation enables a type of global optimization by leveraging synthetic samples received from clients. These synthetic samples, serving as loss surrogates, approximate local loss landscapes by simulating the utility of real images within a local region. We additionally introduce an approach to measure effective approximation regions reflecting the quality of the approximation. Therefore, the server can recover the global loss landscape and comprehensively optimize the model. Moreover, motivated by the emerging privacy concerns, we demonstrate that our approach seamlessly works with record-level differential privacy (DP), granting theoretical privacy guarantees for every data record on the clients. Extensive results validate the efficacy of our formulation on various datasets with highly skewed distributions. Our m
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#23558;&#20165;&#22788;&#29702;&#25991;&#26412;&#30340;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#20687;&#36827;&#34892;&#32852;&#31995;&#65292;&#20351;&#20854;&#33021;&#22815;&#22788;&#29702;&#20219;&#24847;&#20132;&#38169;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#25968;&#25454;&#65292;&#24182;&#29983;&#25104;&#19982;&#26816;&#32034;&#22270;&#20687;&#20132;&#38169;&#30340;&#33258;&#30001;&#24418;&#24335;&#25991;&#26412;&#12290;&#35813;&#26041;&#27861;&#22312;&#29615;&#22659;&#30456;&#20851;&#30340;&#22270;&#20687;&#26816;&#32034;&#21644;&#22810;&#27169;&#24577;&#23545;&#35805;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#21313;&#20998;&#20248;&#24322;&#65292;&#26159;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#35270;&#35273;&#22330;&#26223;&#19979;&#20132;&#20114;&#38382;&#39064;&#30340;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2301.13823</link><description>&lt;p&gt;
&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#20687;&#36827;&#34892;&#32852;&#31995;&#20197;&#22788;&#29702;&#22810;&#27169;&#24577;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Grounding Language Models to Images for Multimodal Inputs and Outputs. (arXiv:2301.13823v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13823
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#23558;&#20165;&#22788;&#29702;&#25991;&#26412;&#30340;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#20687;&#36827;&#34892;&#32852;&#31995;&#65292;&#20351;&#20854;&#33021;&#22815;&#22788;&#29702;&#20219;&#24847;&#20132;&#38169;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#25968;&#25454;&#65292;&#24182;&#29983;&#25104;&#19982;&#26816;&#32034;&#22270;&#20687;&#20132;&#38169;&#30340;&#33258;&#30001;&#24418;&#24335;&#25991;&#26412;&#12290;&#35813;&#26041;&#27861;&#22312;&#29615;&#22659;&#30456;&#20851;&#30340;&#22270;&#20687;&#26816;&#32034;&#21644;&#22810;&#27169;&#24577;&#23545;&#35805;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#21313;&#20998;&#20248;&#24322;&#65292;&#26159;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#35270;&#35273;&#22330;&#26223;&#19979;&#20132;&#20114;&#38382;&#39064;&#30340;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#23558;&#39044;&#35757;&#32451;&#30340;&#20165;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#19982;&#35270;&#35273;&#39046;&#22495;&#32852;&#31995;&#36215;&#26469;&#65292;&#20351;&#20854;&#33021;&#22815;&#22788;&#29702;&#20219;&#24847;&#20132;&#38169;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#25968;&#25454;&#65292;&#24182;&#29983;&#25104;&#19982;&#26816;&#32034;&#22270;&#20687;&#20132;&#38169;&#30340;&#25991;&#26412;&#12290;&#25105;&#20204;&#21033;&#29992;&#20174;&#22823;&#35268;&#27169;&#25991;&#26412;&#39044;&#35757;&#32451;&#20013;&#23398;&#21040;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#20363;&#22914;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#33258;&#30001;&#24418;&#24335;&#25991;&#26412;&#29983;&#25104;&#12290;&#25105;&#20204;&#20445;&#25345;&#35821;&#35328;&#27169;&#22411;&#20923;&#32467;&#65292;&#24182;&#24494;&#35843;&#36755;&#20837;&#21644;&#36755;&#20986;&#32447;&#24615;&#23618;&#20197;&#23454;&#29616;&#36328;&#27169;&#24577;&#20132;&#20114;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#20219;&#24847;&#20132;&#38169;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#36755;&#20837;&#65292;&#24182;&#29983;&#25104;&#19982;&#26816;&#32034;&#22270;&#20687;&#20132;&#38169;&#30340;&#33258;&#30001;&#24418;&#24335;&#25991;&#26412;&#12290;&#25105;&#20204;&#22312;&#29615;&#22659;&#30456;&#20851;&#30340;&#22270;&#20687;&#26816;&#32034;&#21644;&#22810;&#27169;&#24577;&#23545;&#35805;&#31561;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24378;&#22823;&#30340;&#38646;-shot&#34920;&#29616;&#65292;&#24182;&#23637;&#31034;&#20102;&#24341;&#20154;&#20837;&#32988;&#30340;&#20132;&#20114;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#20219;&#20309;&#29616;&#25104;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20026;&#22312;&#35270;&#35273;&#22330;&#26223;&#19979;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#25928;&#19988;&#36890;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an efficient method to ground pretrained text-only language models to the visual domain, enabling them to process arbitrarily interleaved image-and-text data, and generate text interleaved with retrieved images. Our method leverages the abilities of language models learnt from large scale text-only pretraining, such as in-context learning and free-form text generation. We keep the language model frozen, and finetune input and output linear layers to enable cross-modality interactions. This allows our model to process arbitrarily interleaved image-and-text inputs, and generate free-form text interleaved with retrieved images. We achieve strong zero-shot performance on grounded tasks such as contextual image retrieval and multimodal dialogue, and showcase compelling interactive abilities. Our approach works with any off-the-shelf language model and paves the way towards an effective, general solution for leveraging pretrained language models in visually grounded settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25913;&#36827;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#22810;&#26102;&#26399;&#22810;&#20998;&#31867;&#35013;&#36733;&#38382;&#39064;&#65292;&#20854;&#20013;&#20351;&#29992;bandit&#21453;&#39304;&#65292;&#25552;&#20986;&#30340;&#31639;&#27861;&#20855;&#26377;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#29575;&#21644;&#26356;&#20302;&#30340;&#36951;&#25022;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#19968;&#20010;&#30456;&#20851;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.13791</link><description>&lt;p&gt;
&#25913;&#36827;&#30340;&#22810;&#26102;&#26399;&#22810;&#20998;&#31867;&#35013;&#36733;&#38382;&#39064;&#31639;&#27861;&#65292;&#21516;&#26102;&#24102;&#26377;bandit&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
Improved Algorithms for Multi-period Multi-class Packing Problems with Bandit Feedback. (arXiv:2301.13791v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13791
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25913;&#36827;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#22810;&#26102;&#26399;&#22810;&#20998;&#31867;&#35013;&#36733;&#38382;&#39064;&#65292;&#20854;&#20013;&#20351;&#29992;bandit&#21453;&#39304;&#65292;&#25552;&#20986;&#30340;&#31639;&#27861;&#20855;&#26377;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#29575;&#21644;&#26356;&#20302;&#30340;&#36951;&#25022;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#19968;&#20010;&#30456;&#20851;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#32447;&#24615;&#32972;&#26223;&#19979;&#30340;&#22810;&#31867;&#22810;&#26399;&#35013;&#36733;&#38382;&#39064;&#65288;LMMP&#65289;&#65292;&#23427;&#30340;&#30446;&#26631;&#26159;&#23558;&#29289;&#21697;&#35013;&#36733;&#21040;&#39044;&#31639;&#21521;&#37327;&#19979;&#65292;&#24182;&#20351;&#24635;&#20215;&#20540;&#23613;&#21487;&#33021;&#22823;&#12290;&#25105;&#20204;&#32771;&#34385;&#30340;&#24773;&#20917;&#26159;&#65292;&#19982;&#27599;&#20010;&#25805;&#20316;&#30456;&#20851;&#32852;&#30340;&#22870;&#21169;&#21644;&#28040;&#32791;&#21521;&#37327;&#26159;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#32447;&#24615;&#20989;&#25968;&#65292;&#24182;&#19988;&#20915;&#31574;&#32773;&#24471;&#21040;&#20102;bandit&#21453;&#39304;&#12290;&#24403;&#39044;&#31639;&#33267;&#23569;&#22686;&#38271;&#20026;$ \sqrt{T}$&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38381;&#21512;&#24418;&#24335;&#30340;bandit&#31574;&#30053;&#65292;&#36825;&#26679;&#21487;&#20197;&#22312;&#19978;&#19979;&#25991;&#32500;&#24230;&#65292;&#20998;&#31867;&#25968;&#21644;&#26102;&#38388;&#33539;&#22260;$T$&#19979;&#20445;&#25345;&#27425;&#32447;&#24615;&#30340;&#36951;&#25022;&#12290;&#25105;&#20204;&#36824;&#35299;&#20915;&#20102;Agrawal&#65286;Goyal&#65288;2018&#65289;&#22312;LCBK&#38382;&#39064;&#20013;&#25552;&#20986;&#30340;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#33719;&#24471;&#26356;&#24555;&#25910;&#25947;&#36895;&#29575;&#30340;&#26032;&#30340;&#20272;&#35745;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the linear contextual multi-class multi-period packing problem (LMMP) where the goal is to pack items such that the total vector of consumption is below a given budget vector and the total value is as large as possible. We consider the setting where the reward and the consumption vector associated with each action is a class-dependent linear function of the context, and the decision-maker receives bandit feedback. LMMP includes linear contextual bandits with knapsacks and online revenue management as special cases. We establish a new estimator which guarantees a faster convergence rate, and consequently, a lower regret in such problems. We propose a bandit policy that is a closed-form function of said estimated parameters. When the contexts are non-degenerate, the regret of the proposed policy is sublinear in the context dimension, the number of classes, and the time horizon $T$ when the budget grows at least as $\sqrt{T}$. We also resolve an open problem posed by Agrawal &amp;
&lt;/p&gt;</description></item><item><title>PDVN&#26159;&#19968;&#31181;&#26032;&#30340;&#22312;&#32447;&#35757;&#32451;&#31639;&#27861;&#65292;&#23427;&#22312;&#36870;&#21521;&#21512;&#25104;&#35268;&#21010;&#20013;&#21033;&#29992;&#21452;&#20215;&#20540;&#32593;&#32476;&#20248;&#21270;&#23436;&#25972;&#30340;&#36335;&#32447;&#65292;&#25104;&#21151;&#29575;&#21644;&#25928;&#29575;&#19978;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2301.13755</link><description>&lt;p&gt;
&#21452;&#20215;&#20540;&#32593;&#32476;&#22312;&#36870;&#21521;&#21512;&#25104;&#35268;&#21010;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Retrosynthetic Planning with Dual Value Networks. (arXiv:2301.13755v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13755
&lt;/p&gt;
&lt;p&gt;
PDVN&#26159;&#19968;&#31181;&#26032;&#30340;&#22312;&#32447;&#35757;&#32451;&#31639;&#27861;&#65292;&#23427;&#22312;&#36870;&#21521;&#21512;&#25104;&#35268;&#21010;&#20013;&#21033;&#29992;&#21452;&#20215;&#20540;&#32593;&#32476;&#20248;&#21270;&#23436;&#25972;&#30340;&#36335;&#32447;&#65292;&#25104;&#21151;&#29575;&#21644;&#25928;&#29575;&#19978;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#21521;&#21512;&#25104;&#26088;&#22312;&#20174;&#21830;&#19994;&#19978;&#21487;&#24471;&#30340;&#36215;&#22987;&#26448;&#26009;&#20013;&#25214;&#21040;&#21512;&#25104;&#30446;&#26631;&#20998;&#23376;&#30340;&#36335;&#32447;&#65292;&#26159;&#33647;&#29289;&#21457;&#29616;&#21644;&#26448;&#26009;&#35774;&#35745;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#21333;&#27493;&#21453;&#24212;&#39044;&#27979;&#22120;&#19982;&#22810;&#27493;&#35268;&#21010;&#22120;&#30340;&#32452;&#21512;&#24050;&#32463;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#21333;&#27493;&#39044;&#27979;&#22120;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#26159;&#31163;&#32447;&#35757;&#32451;&#30340;&#65292;&#21482;&#20248;&#21270;&#21333;&#27493;&#30340;&#20934;&#30830;&#24615;&#65292;&#32780;&#19981;&#32771;&#34385;&#23436;&#25972;&#30340;&#36335;&#32447;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22312;&#32447;&#35757;&#32451;&#31639;&#27861;PDVN&#65292;&#36890;&#36807;&#20351;&#29992;&#26641;&#24418;MDP&#26469;&#20248;&#21270;&#23436;&#25972;&#30340;&#36335;&#32447;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#25913;&#21892;&#21333;&#27493;&#39044;&#27979;&#22120;&#12290;&#22312;PDVN&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#20004;&#20010;&#21333;&#29420;&#30340;&#20215;&#20540;&#32593;&#32476;&#65292;&#20998;&#21035;&#39044;&#27979;&#20998;&#23376;&#30340;&#21487;&#21512;&#25104;&#24615;&#21644;&#25104;&#26412;&#12290;&#20026;&#20102;&#20445;&#25345;&#21333;&#27493;&#39044;&#27979;&#22120;&#30340;&#20934;&#30830;&#24615;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21452;&#20998;&#25903;&#32593;&#32476;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrosynthesis, which aims to find a route to synthesize a target molecule from commercially available starting materials, is a critical task in drug discovery and materials design. Recently, the combination of ML-based single-step reaction predictors with multi-step planners has led to promising results. However, the single-step predictors are mostly trained offline to optimize the single-step accuracy, without considering complete routes. Here, we leverage reinforcement learning (RL) to improve the single-step predictor, by using a tree-shaped MDP to optimize complete routes. Specifically, we propose a novel online training algorithm, called Planning with Dual Value Networks (PDVN), which alternates between the planning phase and updating phase. In PDVN, we construct two separate value networks to predict the synthesizability and cost of molecules, respectively. To maintain the single-step accuracy, we design a two-branch network structure for the single-step predictor. On the widely
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20004;&#31181;&#26032;&#20844;&#24179;&#24230;&#37327;&#26041;&#27861;&#65306;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#26354;&#32447;&#20043;&#38388;&#21306;&#22495;&#65288;ABPC&#65289;&#21644;&#32047;&#31215;&#23494;&#24230;&#20989;&#25968;&#26354;&#32447;&#20043;&#38388;&#21306;&#22495;&#65288;ABCC&#65289;&#65292;&#23454;&#29616;&#20154;&#21475;&#32479;&#35745;&#29305;&#24449;&#30340;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;</title><link>http://arxiv.org/abs/2301.13443</link><description>&lt;p&gt;
&#26032;&#30340;&#20998;&#24067;&#27700;&#24179;&#24230;&#37327;&#26041;&#27861;&#65306;&#24323;&#29992;$\Delta$DP&#65292;&#23454;&#29616;&#20154;&#21475;&#32479;&#35745;&#29305;&#24449;&#30340;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Retiring $\Delta$DP: New Distribution-Level Metrics for Demographic Parity. (arXiv:2301.13443v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13443
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20004;&#31181;&#26032;&#20844;&#24179;&#24230;&#37327;&#26041;&#27861;&#65306;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#26354;&#32447;&#20043;&#38388;&#21306;&#22495;&#65288;ABPC&#65289;&#21644;&#32047;&#31215;&#23494;&#24230;&#20989;&#25968;&#26354;&#32447;&#20043;&#38388;&#21306;&#22495;&#65288;ABCC&#65289;&#65292;&#23454;&#29616;&#20154;&#21475;&#32479;&#35745;&#29305;&#24449;&#30340;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#21475;&#32479;&#35745;&#29305;&#24449;&#30340;&#24179;&#31561;&#23545;&#24453;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#26368;&#24191;&#27867;&#35748;&#21487;&#30340;&#20844;&#24179;&#24230;&#37327;&#26631;&#20934;&#12290;&#20026;&#23454;&#29616;&#20154;&#21475;&#32479;&#35745;&#24179;&#31561;&#65292;&#35768;&#22810;&#30740;&#31350;&#33268;&#21147;&#20110;&#36861;&#27714;&#24120;&#29992;&#24230;&#37327;&#26041;&#27861;$\Delta DP$&#12290;&#28982;&#32780;&#65292;&#26412;&#25991;&#25581;&#31034;&#20102;&#20844;&#24179;&#24230;&#37327;&#26041;&#27861;$\Delta DP$&#23384;&#22312;&#22266;&#26377;&#32570;&#38519;&#65306;i) &#38646;&#20540;$\Delta DP$&#19981;&#20445;&#35777;&#27665;&#26063;&#32479;&#35745;&#24179;&#31561;&#30340;&#38646;&#36829;&#35268;&#65292;ii) $\Delta DP$&#20540;&#38543;&#19981;&#21516;&#20998;&#31867;&#38408;&#20540;&#21464;&#21270;&#12290;&#20026;&#27492;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#20844;&#24179;&#24230;&#37327;&#26041;&#27861;&#8212;&#8212;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#26354;&#32447;&#20043;&#38388;&#21306;&#22495;&#65288;ABPC&#65289;&#21644;&#32047;&#31215;&#23494;&#24230;&#20989;&#25968;&#26354;&#32447;&#20043;&#38388;&#21306;&#22495;&#65288;ABCC&#65289;&#65292;&#20197;&#31934;&#30830;&#27979;&#37327;&#19981;&#21516;&#27665;&#26063;&#32479;&#35745;&#32676;&#20307;&#39044;&#27979;&#27010;&#29575;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Demographic parity is the most widely recognized measure of group fairness in machine learning, which ensures equal treatment of different demographic groups. Numerous works aim to achieve demographic parity by pursuing the commonly used metric $\Delta DP$. Unfortunately, in this paper, we reveal that the fairness metric $\Delta DP$ can not precisely measure the violation of demographic parity, because it inherently has the following drawbacks: i) zero-value $\Delta DP$ does not guarantee zero violation of demographic parity, ii) $\Delta DP$ values can vary with different classification thresholds. To this end, we propose two new fairness metrics, Area Between Probability density function Curves (ABPC) and Area Between Cumulative density function Curves (ABCC), to precisely measure the violation of demographic parity at the distribution level. The new fairness metrics directly measure the difference between the distributions of the prediction probability for different demographic group
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#22312;&#28145;&#23618;&#26435;&#37325;&#31354;&#38388;&#20013;&#23398;&#20064;&#65292;&#23427;&#23545;MLP&#26435;&#37325;&#30340;&#33258;&#28982;&#32622;&#25442;&#23545;&#31216;&#31561;&#21464;&#65292;&#21487;&#20197;&#22788;&#29702;&#24191;&#27867;&#26377;&#36259;&#30340;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2301.12780</link><description>&lt;p&gt;
&#28145;&#23618;&#26435;&#37325;&#31354;&#38388;&#20013;&#23398;&#20064;&#30340;&#31561;&#21464;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Equivariant Architectures for Learning in Deep Weight Spaces. (arXiv:2301.12780v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#22312;&#28145;&#23618;&#26435;&#37325;&#31354;&#38388;&#20013;&#23398;&#20064;&#65292;&#23427;&#23545;MLP&#26435;&#37325;&#30340;&#33258;&#28982;&#32622;&#25442;&#23545;&#31216;&#31561;&#21464;&#65292;&#21487;&#20197;&#22788;&#29702;&#24191;&#27867;&#26377;&#36259;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#29992;&#20110;&#20197;&#21407;&#22987;&#26435;&#37325;&#30697;&#38453;&#24418;&#24335;&#22788;&#29702;&#31070;&#32463;&#32593;&#32476;&#30340;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#26159;&#19968;&#26465;&#26032;&#24341;&#20837;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#28145;&#23618;&#26435;&#37325;&#31354;&#38388;&#30340;&#29420;&#29305;&#23545;&#31216;&#32467;&#26500;&#20351;&#24471;&#36825;&#31181;&#35774;&#35745;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22914;&#26524;&#25104;&#21151;&#65292;&#36825;&#26679;&#30340;&#26550;&#26500;&#23558;&#33021;&#22815;&#25191;&#34892;&#24191;&#27867;&#30340;&#26377;&#36259;&#20219;&#21153;&#65292;&#20174;&#23558;&#39044;&#35757;&#32451;&#30340;&#32593;&#32476;&#36866;&#24212;&#21040;&#26032;&#30340;&#39046;&#22495;&#65292;&#21040;&#32534;&#36753;&#20316;&#20026;&#20989;&#25968;&#34920;&#31034;&#30340;&#23545;&#35937;&#65288;INRs&#25110;NeRFs&#65289;&#12290;&#20316;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#30340;&#31532;&#19968;&#27493;&#65292;&#25105;&#20204;&#22312;&#36825;&#37324;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#23618;&#26435;&#37325;&#31354;&#38388;&#20013;&#23398;&#20064;&#30340;&#32593;&#32476;&#26550;&#26500;&#12290;&#23427;&#20197;&#39044;&#35757;&#32451;MLP&#30340;&#26435;&#37325;&#21644;&#20559;&#32622;&#30340;&#20018;&#32852;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#20351;&#29992;&#19968;&#32452;&#23545;MLP&#26435;&#37325;&#30340;&#33258;&#28982;&#32622;&#25442;&#23545;&#31216;&#31561;&#21464;&#30340;&#23618;&#32452;&#25104;&#26469;&#22788;&#29702;&#23427;&#65306;&#25913;&#21464;MLP&#20013;&#38388;&#23618;&#20013;&#31070;&#32463;&#20803;&#30340;&#39034;&#24207;&#19981;&#20250;&#24433;&#21709;&#23427;&#25152;&#34920;&#31034;&#30340;&#20989;&#25968;&#12290;&#25105;&#20204;&#20026;&#36825;&#20123;&#23545;&#31216;&#32467;&#26500;&#25552;&#20379;&#20102;&#25152;&#26377;&#20223;&#23556;&#31561;&#21464;&#21644;&#19981;&#21464;&#23618;&#30340;&#23436;&#25972;&#29305;&#24449;&#65292;&#24182;&#23637;&#31034;...
&lt;/p&gt;
&lt;p&gt;
Designing machine learning architectures for processing neural networks in their raw weight matrix form is a newly introduced research direction. Unfortunately, the unique symmetry structure of deep weight spaces makes this design very challenging. If successful, such architectures would be capable of performing a wide range of intriguing tasks, from adapting a pre-trained network to a new domain to editing objects represented as functions (INRs or NeRFs). As a first step towards this goal, we present here a novel network architecture for learning in deep weight spaces. It takes as input a concatenation of weights and biases of a pre-trained MLP and processes it using a composition of layers that are equivariant to the natural permutation symmetry of the MLP's weights: Changing the order of neurons in intermediate layers of the MLP does not affect the function it represents. We provide a full characterization of all affine equivariant and invariant layers for these symmetries and show 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Few-shot&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#35757;&#32451;&#31639;&#27861;&#21644;&#36866;&#24212;&#31639;&#27861;&#65292;&#24182;&#23454;&#35777;&#35777;&#26126;&#36825;&#20004;&#20010;&#31639;&#27861;&#26159;&#21487;&#20197;&#23436;&#20840;&#20998;&#31163;&#30340;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#30340;&#20803;&#20998;&#26512;&#25581;&#31034;&#20986;&#20102;&#20851;&#20110;Few-shot&#20998;&#31867;&#30340;&#20851;&#38190;&#26041;&#38754;&#21644;&#19982;&#20854;&#20182;&#39046;&#22495;&#65288;&#22914;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#65289;&#30340;&#32852;&#31995;&#30340;&#26377;&#36259;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2301.12246</link><description>&lt;p&gt;
&#20877;&#27425;&#28145;&#20837;&#25506;&#31350;Few-shot&#20998;&#31867;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
A Closer Look at Few-shot Classification Again. (arXiv:2301.12246v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12246
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Few-shot&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#35757;&#32451;&#31639;&#27861;&#21644;&#36866;&#24212;&#31639;&#27861;&#65292;&#24182;&#23454;&#35777;&#35777;&#26126;&#36825;&#20004;&#20010;&#31639;&#27861;&#26159;&#21487;&#20197;&#23436;&#20840;&#20998;&#31163;&#30340;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#30340;&#20803;&#20998;&#26512;&#25581;&#31034;&#20986;&#20102;&#20851;&#20110;Few-shot&#20998;&#31867;&#30340;&#20851;&#38190;&#26041;&#38754;&#21644;&#19982;&#20854;&#20182;&#39046;&#22495;&#65288;&#22914;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#65289;&#30340;&#32852;&#31995;&#30340;&#26377;&#36259;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Few-shot&#20998;&#31867;&#31639;&#27861;&#30001;&#35757;&#32451;&#38454;&#27573;&#21644;&#36866;&#24212;&#38454;&#27573;&#32452;&#25104;&#65292;&#22312;&#35757;&#32451;&#38454;&#27573;&#65292;&#27169;&#22411;&#22312;&#19968;&#20010;&#30456;&#23545;&#22823;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23398;&#20064;&#65292;&#22312;&#36866;&#24212;&#38454;&#27573;&#65292;&#24050;&#23398;&#20064;&#30340;&#27169;&#22411;&#34987;&#35843;&#25972;&#20197;&#36866;&#24212;&#20043;&#21069;&#20174;&#26410;&#35265;&#36807;&#30340;&#20165;&#26377;&#26377;&#38480;&#26631;&#27880;&#26679;&#26412;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#35777;&#26126;&#65292;&#35757;&#32451;&#31639;&#27861;&#21644;&#36866;&#24212;&#31639;&#27861;&#26159;&#23436;&#20840;&#29420;&#31435;&#30340;&#65292;&#36825;&#20351;&#24471;&#21487;&#20197;&#20998;&#21035;&#20026;&#27599;&#20010;&#38454;&#27573;&#36827;&#34892;&#31639;&#27861;&#20998;&#26512;&#21644;&#35774;&#35745;&#12290;&#38024;&#23545;&#27599;&#20010;&#38454;&#27573;&#30340;&#20803;&#20998;&#26512;&#25581;&#31034;&#20102;&#19968;&#20123;&#26377;&#36259;&#30340;&#35265;&#35299;&#65292;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;Few-shot&#20998;&#31867;&#30340;&#20851;&#38190;&#26041;&#38754;&#21644;&#19982;&#20854;&#20182;&#39046;&#22495;&#65288;&#22914;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#65289;&#30340;&#32852;&#31995;&#12290;&#25105;&#20204;&#24076;&#26395;&#26412;&#25991;&#25581;&#31034;&#30340;&#35265;&#35299;&#21644;&#30740;&#31350;&#25361;&#25112;&#33021;&#28608;&#21457;&#30456;&#20851;&#26041;&#21521;&#30340;&#26410;&#26469;&#24037;&#20316;&#12290;&#35770;&#25991;&#20195;&#30721;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;&#20351;&#29992;PyTorch&#65289;&#21487;&#22312; https://github.com/Frankluox/CloserLookAgainFewShot &#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot classification consists of a training phase where a model is learned on a relatively large dataset and an adaptation phase where the learned model is adapted to previously-unseen tasks with limited labeled samples. In this paper, we empirically prove that the training algorithm and the adaptation algorithm can be completely disentangled, which allows algorithm analysis and design to be done individually for each phase. Our meta-analysis for each phase reveals several interesting insights that may help better understand key aspects of few-shot classification and connections with other fields such as visual representation learning and transfer learning. We hope the insights and research challenges revealed in this paper can inspire future work in related directions. Code and pre-trained models (in PyTorch) are available at https://github.com/Frankluox/CloserLookAgainFewShot.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30828;&#24065;&#25237;&#27880;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#26041;&#27861;&#65292;&#23436;&#20840;&#19981;&#38656;&#35201;&#23398;&#20064;&#36895;&#29575;&#65292;&#21487;&#20197;&#22312;&#39640;&#32500;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.11294</link><description>&lt;p&gt;
&#22522;&#20110;&#30828;&#24065;&#37319;&#26679;&#30340;&#26080;&#38656;&#23398;&#20064;&#36895;&#29575;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Coin Sampling: Gradient-Based Bayesian Inference without Learning Rates. (arXiv:2301.11294v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11294
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30828;&#24065;&#25237;&#27880;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#26041;&#27861;&#65292;&#23436;&#20840;&#19981;&#38656;&#35201;&#23398;&#20064;&#36895;&#29575;&#65292;&#21487;&#20197;&#22312;&#39640;&#32500;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#31890;&#23376;&#30340;&#21464;&#20998;&#25512;&#26029;&#65288;ParVI&#65289;&#26041;&#27861;&#22914;Stein&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#65288;SVGD&#65289;&#22240;&#21487;&#25193;&#23637;&#24615;&#22312;&#36125;&#21494;&#26031;&#25512;&#29702;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#24615;&#36136;&#19981;&#21487;&#36991;&#20813;&#22320;&#21462;&#20915;&#20110;&#36229;&#21442;&#25968;&#65288;&#22914;&#23398;&#20064;&#36895;&#29575;&#65289;&#65292;&#24517;&#39035;&#30001;&#20174;&#19994;&#32773;&#20180;&#32454;&#35843;&#25972;&#65292;&#20197;&#30830;&#20445;&#20197;&#21512;&#36866;&#30340;&#36895;&#29575;&#25910;&#25947;&#21040;&#30446;&#26631;&#27979;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#32452;&#26032;&#30340;&#22522;&#20110;&#30828;&#24065;&#25237;&#27880;&#30340;&#21487;&#25193;&#23637;&#36125;&#21494;&#26031;&#25512;&#26029;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#23436;&#20840;&#19981;&#38656;&#35201;&#23398;&#20064;&#36895;&#29575;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#25968;&#20540;&#20363;&#23376;&#20013;&#28436;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#20960;&#20010;&#39640;&#32500;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#65292;&#35777;&#26126;&#20102;&#19982;&#20854;&#20182;ParVI&#31639;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#35843;&#25972;&#23398;&#20064;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, particle-based variational inference (ParVI) methods such as Stein variational gradient descent (SVGD) have grown in popularity as scalable methods for Bayesian inference. Unfortunately, the properties of such methods invariably depend on hyperparameters such as the learning rate, which must be carefully tuned by the practitioner in order to ensure convergence to the target measure at a suitable rate. In this paper, we introduce a suite of new particle-based methods for scalable Bayesian inference based on coin betting, which are entirely learning-rate free. We illustrate the performance of our approach on a range of numerical examples, including several high-dimensional models and datasets, demonstrating comparable performance to other ParVI algorithms with no need to tune a learning rate.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#24102;&#26377;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#35777;&#26126;&#20102;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22312;Bradley-Terry-Luce&#21644;Plackett-Luce&#27169;&#22411;&#19979;&#25910;&#25947;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#22312;&#19968;&#23450;&#30340;&#35206;&#30422;&#20551;&#35774;&#19979;&#65292;&#22522;&#20110;&#24754;&#35266;&#20272;&#35745;&#30340;MLE&#25552;&#20379;&#20102;&#24615;&#33021;&#26356;&#22909;&#30340;&#31574;&#30053;&#12290;&#22312;&#35777;&#26126;&#20102;&#30495;&#23454;MLE&#21644;&#20197;&#25104;&#23545;&#27604;&#36739;&#24418;&#24335;&#26367;&#20195;&#30340;&#22791;&#36873;MLE&#37117;&#21487;&#20197;&#22312;PL&#27169;&#22411;&#19979;&#25910;&#25947;&#30340;&#21516;&#26102;&#65292;&#20063;&#34920;&#26126;&#20102;&#30495;&#23454;MLE&#30340;&#39640;&#25928;&#24615;&#12290;&#36825;&#20123;&#32467;&#26524;&#20026;RLHF&#31639;&#27861;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#65292;&#24182;&#32479;&#19968;&#20102;RLHF&#38382;&#39064;&#21644;IRL&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.11270</link><description>&lt;p&gt;
&#20351;&#29992;&#26469;&#33258;&#25104;&#23545;&#25110;$K$&#20803;&#27604;&#36739;&#30340;&#20154;&#31867;&#21453;&#39304;&#30340;&#35268;&#33539;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Principled Reinforcement Learning with Human Feedback from Pairwise or $K$-wise Comparisons. (arXiv:2301.11270v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11270
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#24102;&#26377;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#35777;&#26126;&#20102;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22312;Bradley-Terry-Luce&#21644;Plackett-Luce&#27169;&#22411;&#19979;&#25910;&#25947;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#22312;&#19968;&#23450;&#30340;&#35206;&#30422;&#20551;&#35774;&#19979;&#65292;&#22522;&#20110;&#24754;&#35266;&#20272;&#35745;&#30340;MLE&#25552;&#20379;&#20102;&#24615;&#33021;&#26356;&#22909;&#30340;&#31574;&#30053;&#12290;&#22312;&#35777;&#26126;&#20102;&#30495;&#23454;MLE&#21644;&#20197;&#25104;&#23545;&#27604;&#36739;&#24418;&#24335;&#26367;&#20195;&#30340;&#22791;&#36873;MLE&#37117;&#21487;&#20197;&#22312;PL&#27169;&#22411;&#19979;&#25910;&#25947;&#30340;&#21516;&#26102;&#65292;&#20063;&#34920;&#26126;&#20102;&#30495;&#23454;MLE&#30340;&#39640;&#25928;&#24615;&#12290;&#36825;&#20123;&#32467;&#26524;&#20026;RLHF&#31639;&#27861;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#65292;&#24182;&#32479;&#19968;&#20102;RLHF&#38382;&#39064;&#21644;IRL&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20026;&#24102;&#26377;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#24403;&#30495;&#23454;&#22870;&#21169;&#20989;&#25968;&#20026;&#32447;&#24615;&#20989;&#25968;&#26102;&#65292;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65288;MLE&#65289;&#22312;Bradley-Terry-Luce&#65288;BTL&#65289;&#27169;&#22411;&#21644;Plackett-Luce&#65288;PL&#65289;&#27169;&#22411;&#19979;&#22343;&#25910;&#25947;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#22522;&#20110;&#23398;&#24471;&#30340;&#22870;&#21169;&#27169;&#22411;&#35757;&#32451;&#31574;&#30053;&#26102;&#65292;MLE&#20250;&#22833;&#36133;&#65292;&#32780;&#22522;&#20110;&#24754;&#35266;&#20272;&#35745;&#30340;MLE&#22312;&#19968;&#23450;&#30340;&#35206;&#30422;&#20551;&#35774;&#19979;&#25552;&#20379;&#24615;&#33021;&#26356;&#22909;&#30340;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;PL&#27169;&#22411;&#19979;&#65292;&#30495;&#23454;MLE&#21644;&#23558;$k$&#20803;&#27604;&#36739;&#25286;&#20998;&#20026;&#25104;&#23545;&#27604;&#36739;&#30340;&#22791;&#36873;MLE&#37117;&#25910;&#25947;&#12290;&#32780;&#30495;&#23454;MLE&#26159;&#28176;&#36817;&#26356;&#20026;&#39640;&#25928;&#30340;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#39564;&#35777;&#20102;&#29616;&#26377;RLHF&#31639;&#27861;&#65288;&#22914;InstructGPT&#65289;&#30340;&#23454;&#39564;&#25104;&#21151;&#65292;&#24182;&#20026;&#31639;&#27861;&#35774;&#35745;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#32479;&#19968;&#20102;RLHF&#38382;&#39064;&#21644;&#26368;&#22823;&#29109;&#21453;&#21521;&#24378;&#21270;&#23398;&#20064;(IRL)&#38382;&#39064;&#65292;&#24182;&#20026;&#20854;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide a theoretical framework for Reinforcement Learning with Human Feedback (RLHF). Our analysis shows that when the true reward function is linear, the widely used maximum likelihood estimator (MLE) converges under both the Bradley-Terry-Luce (BTL) model and the Plackett-Luce (PL) model. However, we show that when training a policy based on the learned reward model, MLE fails while a pessimistic MLE provides policies with improved performance under certain coverage assumptions. Additionally, we demonstrate that under the PL model, the true MLE and an alternative MLE that splits the $K$-wise comparison into pairwise comparisons both converge. Moreover, the true MLE is asymptotically more efficient. Our results validate the empirical success of existing RLHF algorithms in InstructGPT and provide new insights for algorithm design. Furthermore, our results unify the problem of RLHF and max-entropy Inverse Reinforcement Learning (IRL), and provide the first sample complexity bound fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;GNTK&#21644;&#22270;&#26680;&#20989;&#25968;&#25506;&#31350;&#22823;&#35268;&#27169;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#35777;&#26126;&#20102;GNTK&#22312;&#25910;&#25947;&#20110;&#22270;&#26680;&#20989;&#25968;&#26102;&#30340;&#30830;&#20999;&#24615;&#36136;&#12290;&#36825;&#24847;&#21619;&#30528;&#22312;&#22823;&#35268;&#27169;&#22270;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#22312;&#20013;&#31561;&#22823;&#23567;&#30340;&#22270;&#19978;&#36827;&#34892;&#25311;&#21512;&#24182;&#22312;&#25972;&#20010;&#22270;&#19978;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2301.10808</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#20999;&#27604;&#38634;&#22827;&#26680;&#65306;&#22823;&#35268;&#27169;&#22270;&#19978;&#30340;&#25910;&#25947;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Tangent Kernel: Convergence on Large Graphs. (arXiv:2301.10808v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;GNTK&#21644;&#22270;&#26680;&#20989;&#25968;&#25506;&#31350;&#22823;&#35268;&#27169;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#35777;&#26126;&#20102;GNTK&#22312;&#25910;&#25947;&#20110;&#22270;&#26680;&#20989;&#25968;&#26102;&#30340;&#30830;&#20999;&#24615;&#36136;&#12290;&#36825;&#24847;&#21619;&#30528;&#22312;&#22823;&#35268;&#27169;&#22270;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#22312;&#20013;&#31561;&#22823;&#23567;&#30340;&#22270;&#19978;&#36827;&#34892;&#25311;&#21512;&#24182;&#22312;&#25972;&#20010;&#22270;&#19978;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#22823;&#35268;&#27169;&#22270;&#25968;&#25454;&#19978;&#35757;&#32451;&#26102;&#24448;&#24448;&#27604;&#36739;&#22256;&#38590;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#23398;&#20064;&#21160;&#24577;&#38590;&#20197;&#29702;&#35299;&#12290;&#26412;&#25991;&#21033;&#29992;&#22270;&#31070;&#32463;&#20999;&#27604;&#38634;&#22827;&#26680;&#65288;GNTK&#65289;&#21644;&#22270;&#26680;&#20989;&#25968;&#25506;&#31350;&#22823;&#35268;&#27169;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#21160;&#24577;&#12290;&#22312;&#23485;&#24230;&#36235;&#20110;&#26080;&#31351;&#22823;&#26102;&#65292;&#36807;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21270;&#31561;&#20215;&#20110;&#22312;NTK&#19978;&#36827;&#34892;&#26680;&#22238;&#24402;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;GNTK&#22312;&#21478;&#19968;&#20010;&#29420;&#31435;&#30340;&#32500;&#24230;&#65288;&#22270;&#22823;&#23567;&#65289;&#21464;&#21270;&#26102;&#30340;&#28436;&#21270;&#24773;&#20917;&#65292;&#24182;&#20351;&#29992;&#22270;&#26680;&#20989;&#25968;&#26469;&#23450;&#20041;&#26497;&#38480;&#23545;&#35937;&#8212;&#8212;GNN&#30340;&#22270;&#26680;&#20989;&#25968;&#21644;GNTK&#30340;&#22270;&#26680;&#20989;&#25968;&#65292;&#24182;&#35777;&#26126;&#65292;&#22312;&#19968;&#31995;&#21015;&#22270;&#19978;&#65292;GNTK&#25910;&#25947;&#20110;&#22270;&#26680;&#20989;&#25968;&#12290;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;GNTK&#30340;&#35889;&#20381;&#36182;&#20110;&#26368;&#24555;&#23398;&#20064;&#30340;&#26041;&#21521;&#65292;&#36825;&#22312;&#26089;&#26399;&#20572;&#27490;&#35757;&#32451;&#26102;&#21464;&#24471;&#29305;&#21035;&#37325;&#35201;&#65292;&#35889;&#20063;&#25910;&#25947;&#20110;&#22270;&#26680;&#20989;&#25968;&#30340;&#35889;&#12290;&#36825;&#24847;&#21619;&#30528;&#22312;&#22823;&#35268;&#27169;&#22270;&#30340;&#38480;&#21046;&#19979;&#65292;&#23545;&#20013;&#31561;&#22823;&#23567;&#30340;&#22270;&#36827;&#34892;&#25311;&#21512;&#30340;GNTK&#21487;&#20197;&#29992;&#20110;&#22312;&#22270;&#19978;&#36827;&#34892;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) achieve remarkable performance in graph machine learning tasks but can be hard to train on large-graph data, where their learning dynamics are not well understood. We investigate the training dynamics of large-graph GNNs using graph neural tangent kernels (GNTKs) and graphons. In the limit of large width, optimization of an overparametrized NN is equivalent to kernel regression on the NTK. Here, we investigate how the GNTK evolves as another independent dimension is varied: the graph size. We use graphons to define limit objects -- graphon NNs for GNNs, and graphon NTKs for GNTKs -- , and prove that, on a sequence of graphs, the GNTKs converge to the graphon NTK. We further prove that the spectrum of the GNTK, which is related to the directions of fastest learning which becomes relevant during early stopping, converges to the spectrum of the graphon NTK. This implies that in the large-graph limit, the GNTK fitted on a graph of moderate size can be used to s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#38024;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;Jacobian&#31639;&#23376;&#23558;&#25439;&#22833;&#20989;&#25968;&#30340;&#26799;&#24230;&#34920;&#31034;&#20026;&#30697;&#38453;&#20056;&#31215;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#35745;&#31639;&#26799;&#24230;&#65292;&#32780;&#19988;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#19981;&#21516;&#25968;&#37327;&#30340;&#23618;&#25968;&#12290;</title><link>http://arxiv.org/abs/2301.09977</link><description>&lt;p&gt;
&#38754;&#21521;&#25968;&#23398;&#29983;&#30340;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
The Backpropagation algorithm for a math student. (arXiv:2301.09977v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09977
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#38024;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;Jacobian&#31639;&#23376;&#23558;&#25439;&#22833;&#20989;&#25968;&#30340;&#26799;&#24230;&#34920;&#31034;&#20026;&#30697;&#38453;&#20056;&#31215;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#35745;&#31639;&#26799;&#24230;&#65292;&#32780;&#19988;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#19981;&#21516;&#25968;&#37327;&#30340;&#23618;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26159;&#21521;&#37327;&#20540;&#20989;&#25968;&#30340;&#22797;&#21512;&#20989;&#25968;&#65292;&#20026;&#20102;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#38656;&#35201;&#35745;&#31639;&#30456;&#23545;&#20110;&#25152;&#26377;&#21442;&#25968;&#30340;&#25439;&#22833;&#20989;&#25968;&#26799;&#24230;&#12290;&#36825;&#20010;&#35745;&#31639;&#26159;&#19968;&#20010;&#38750;&#24120;&#26840;&#25163;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25439;&#22833;&#20989;&#25968;&#26159;&#30001;&#35768;&#22810;&#38750;&#32447;&#24615;&#20989;&#25968;&#32452;&#25104;&#30340;&#65292;&#24182;&#19988;&#27599;&#20010;&#20989;&#25968;&#37117;&#26377;&#35768;&#22810;&#21442;&#25968;&#12290;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#21033;&#29992;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32452;&#21512;&#32467;&#26500;&#26469;&#39640;&#25928;&#22320;&#35745;&#31639;&#26799;&#24230;&#12290;&#22240;&#27492;&#65292;&#32593;&#32476;&#20013;&#23618;&#25968;&#30340;&#25968;&#37327;&#19981;&#20250;&#26174;&#33879;&#24433;&#21709;&#35745;&#31639;&#30340;&#22797;&#26434;&#24615;&#12290;&#26412;&#25991;&#30340;&#30446;&#30340;&#26159;&#20351;&#29992;Jacobian&#31639;&#23376;&#23558;&#25439;&#22833;&#20989;&#25968;&#30340;&#26799;&#24230;&#34920;&#31034;&#20026;&#30697;&#38453;&#20056;&#31215;&#65292;&#24182;&#36890;&#36807;&#32771;&#34385;&#27599;&#23618;&#23545;&#20854;&#21442;&#25968;&#30340;&#20840;&#23548;&#25968;&#24182;&#23558;&#20854;&#34920;&#31034;&#20026;Jacobian&#30697;&#38453;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#21487;&#20197;&#23558;&#26799;&#24230;&#34920;&#31034;&#20026;&#36825;&#20123;Jacobian&#30697;&#38453;&#30340;&#30697;&#38453;&#20056;&#31215;&#12290;&#36825;&#31181;&#26041;&#27861;&#26159;&#26377;&#25928;&#30340;&#65292;&#22240;&#20026;&#35745;&#31639;&#38142;&#35268;&#21017;&#25152;&#38656;&#30340;&#20056;&#31215;&#35268;&#21017;&#31561;&#20110;Jacobian&#30697;&#38453;&#30340;&#20056;&#31215;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Deep Neural Network (DNN) is a composite function of vector-valued functions, and in order to train a DNN, it is necessary to calculate the gradient of the loss function with respect to all parameters. This calculation can be a non-trivial task because the loss function of a DNN is a composition of several nonlinear functions, each with numerous parameters. The Backpropagation (BP) algorithm leverages the composite structure of the DNN to efficiently compute the gradient. As a result, the number of layers in the network does not significantly impact the complexity of the calculation. The objective of this paper is to express the gradient of the loss function in terms of a matrix multiplication using the Jacobian operator. This can be achieved by considering the total derivative of each layer with respect to its parameters and expressing it as a Jacobian matrix. The gradient can then be represented as the matrix product of these Jacobian matrices. This approach is valid because the ch
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#28982;&#28798;&#23475;&#20998;&#21106;&#27169;&#22411;&#22312;&#36879;&#26126;&#20113;&#65292;&#28895;&#38654;&#26609;&#21644;&#27946;&#27700;&#20998;&#21106;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#22312;&#36866;&#24403;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#26174;&#30528;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#36890;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.09318</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#28982;&#28798;&#23475;&#20998;&#21106;&#27169;&#22411;&#30340;&#36890;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
Toward Foundation Models for Earth Monitoring: Generalizable Deep Learning Models for Natural Hazard Segmentation. (arXiv:2301.09318v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09318
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#28982;&#28798;&#23475;&#20998;&#21106;&#27169;&#22411;&#22312;&#36879;&#26126;&#20113;&#65292;&#28895;&#38654;&#26609;&#21644;&#27946;&#27700;&#20998;&#21106;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#22312;&#36866;&#24403;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#26174;&#30528;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#36890;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27668;&#20505;&#21464;&#21270;&#23548;&#33268;&#26497;&#31471;&#22825;&#27668;&#20107;&#20214;&#30340;&#27010;&#29575;&#22686;&#21152;&#65292;&#36825;&#23545;&#20840;&#29699;&#33539;&#22260;&#20869;&#30340;&#31038;&#20250;&#21644;&#20225;&#19994;&#26500;&#25104;&#39118;&#38505;&#12290;&#22240;&#27492;&#65292;&#36817;&#23454;&#26102;&#22320;&#22270;&#21046;&#20316;&#24050;&#25104;&#20026;&#25903;&#25345;&#33258;&#28982;&#28798;&#23475;&#25937;&#25588;&#12289;&#39118;&#38505;&#31649;&#29702;&#21644;&#25919;&#24220;&#25919;&#31574;&#20915;&#31574;&#30340;&#26032;&#20248;&#20808;&#20107;&#39033;&#12290;&#26368;&#36817;&#65292;&#23454;&#29616;&#36817;&#23454;&#26102;&#21046;&#22270;&#30340;&#26041;&#27861;&#36234;&#26469;&#36234;&#22810;&#22320;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#12290;&#20294;&#26159;&#65292;&#22522;&#20110;DL&#30340;&#26041;&#27861;&#20165;&#38024;&#23545;&#21333;&#20010;&#22320;&#29702;&#21306;&#22495;&#20013;&#29305;&#23450;&#39057;&#27573;&#30340;&#21355;&#26143;&#25968;&#25454;&#30340;&#19968;&#20010;&#29305;&#23450;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#29992;&#20110;&#21046;&#22270;&#29305;&#23450;&#33258;&#28982;&#28798;&#23475;&#30340;DL&#27169;&#22411;&#38590;&#20197;&#23558;&#20854;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#20854;&#20182;&#31867;&#22411;&#30340;&#33258;&#28982;&#28798;&#23475;&#19978;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#36866;&#24403;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#26174;&#30528;&#25552;&#39640;DL&#33258;&#28982;&#28798;&#23475;&#26144;&#23556;&#22120;&#30340;&#36890;&#29992;&#24615;&#12290;&#22312;&#27809;&#26377;&#20219;&#20309;&#30446;&#26631;&#39046;&#22495;&#30340;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#25913;&#36827;&#30340;&#36890;&#29992;&#24615;&#36328;&#36234;&#20102;&#22235;&#20010;&#36879;&#26126;&#20113;&#65292;&#28895;&#38654;&#26609;&#21644;&#27946;&#27700;&#20998;&#27573;&#20219;&#21153;&#30340;&#22810;&#20010;&#22320;&#29702;&#21306;&#22495;&#12290;&#36890;&#36807;&#21512;&#24182;&#39044;&#35757;&#32451;&#27493;&#39588;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#25152;&#26377;&#19977;&#20010;&#33258;&#28982;&#28798;&#23475;&#20998;&#21106;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#19982;&#20854;&#20182;DL&#20307;&#31995;&#32467;&#26500;&#30456;&#27604;&#65292;&#20445;&#25345;&#26356;&#24555;&#30340;&#35745;&#31639;&#36895;&#24230;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20195;&#34920;&#30528;&#36808;&#21521;&#33021;&#22815;&#22312;&#22810;&#20010;&#39046;&#22495;&#36890;&#29992;&#30340;&#36817;&#23454;&#26102;&#33258;&#28982;&#28798;&#23475;&#21046;&#22270;&#22522;&#26412;&#27169;&#22411;&#30340;&#26377;&#24076;&#26395;&#30340;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Climate change results in an increased probability of extreme weather events that put societies and businesses at risk on a global scale. Therefore, near real-time mapping of natural hazards is an emerging priority for the support of natural disaster relief, risk management, and informing governmental policy decisions. Recent methods to achieve near real-time mapping increasingly leverage deep learning (DL). However, DL-based approaches are designed for one specific task in a single geographic region based on specific frequency bands of satellite data. Therefore, DL models used to map specific natural hazards struggle with their generalization to other types of natural hazards in unseen regions. In this work, we propose a methodology to significantly improve the generalizability of DL natural hazards mappers based on pre-training on a suitable pre-task. Without access to any data from the target domain, we demonstrate this improved generalizability across four U-Net architectures for t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24555;&#36895;&#20613;&#37324;&#21494;&#21464;&#25442;&#35745;&#31639;&#21435;&#30456;&#20851;&#27491;&#21017;&#21270;&#22120;&#30340;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#35757;&#32451;&#26356;&#24555;&#19988;&#20869;&#23384;&#38656;&#27714;&#26356;&#23567;&#65292;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#21516;&#31561;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.01569</link><description>&lt;p&gt;
&#20351;&#29992;&#24555;&#36895;&#20613;&#37324;&#21494;&#21464;&#25442;&#39640;&#25928;&#22320;&#23398;&#20064;&#19981;&#30456;&#20851;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Decorrelated Representations Efficiently Using Fast Fourier Transform. (arXiv:2301.01569v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.01569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24555;&#36895;&#20613;&#37324;&#21494;&#21464;&#25442;&#35745;&#31639;&#21435;&#30456;&#20851;&#27491;&#21017;&#21270;&#22120;&#30340;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#35757;&#32451;&#26356;&#24555;&#19988;&#20869;&#23384;&#38656;&#27714;&#26356;&#23567;&#65292;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#21516;&#31561;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Barlow Twins&#21644;VICReg&#26159;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#65292;&#23427;&#20204;&#20351;&#29992;&#27491;&#21017;&#21270;&#22120;&#26469;&#21435;&#38500;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#19982;&#20256;&#32479;&#30340;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#19968;&#26679;&#26377;&#25928;&#65292;&#20294;&#22914;&#26524;&#25237;&#24433;&#23884;&#20837;&#30340;&#32500;&#24230;d&#24456;&#39640;&#65292;&#21017;&#23427;&#20204;&#30340;&#35757;&#32451;&#21487;&#33021;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#12290;&#30001;&#20110;&#27491;&#21017;&#21270;&#22120;&#26159;&#22522;&#20110;&#20132;&#21449;-correlation&#25110;covariance&#30697;&#38453;&#30340;&#21333;&#20010;&#20803;&#32032;&#26469;&#23450;&#20041;&#30340;&#65292;&#35745;&#31639;n&#20010;&#26679;&#26412;&#30340;&#25439;&#22833;&#38656;&#35201;O(n d^2)&#30340;&#26102;&#38388;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25918;&#26494;&#30340;&#21435;&#30456;&#20851;&#27491;&#21017;&#21270;&#22120;&#65292;&#21487;&#20197;&#36890;&#36807;&#24555;&#36895;&#20613;&#37324;&#21494;&#21464;&#25442;&#22312;O(n d log d)&#30340;&#26102;&#38388;&#20869;&#35745;&#31639;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#24265;&#20215;&#30340;&#25216;&#26415;&#26469;&#32531;&#35299;&#25918;&#26494;&#26102;&#20986;&#29616;&#30340;&#19981;&#33391;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;&#25152;&#25552;&#20986;&#30340;&#27491;&#21017;&#21270;&#22120;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#19982;&#29616;&#26377;&#27491;&#21017;&#21270;&#22120;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#65292;&#32780;&#19988;&#23545;&#20110;&#22823;&#30340;d&#65292;&#20854;&#35757;&#32451;&#25152;&#38656;&#30340;&#20869;&#23384;&#26356;&#23569;&#65292;&#36895;&#24230;&#26356;&#24555;&#12290;&#28304;&#20195;&#30721;&#21487;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Barlow Twins and VICReg are self-supervised representation learning models that use regularizers to decorrelate features. Although these models are as effective as conventional representation learning models, their training can be computationally demanding if the dimension d of the projected embeddings is high. As the regularizers are defined in terms of individual elements of a cross-correlation or covariance matrix, computing the loss for n samples takes O(n d^2) time. In this paper, we propose a relaxed decorrelating regularizer that can be computed in O(n d log d) time by Fast Fourier Transform. We also propose an inexpensive technique to mitigate undesirable local minima that develop with the relaxation. The proposed regularizer exhibits accuracy comparable to that of existing regularizers in downstream tasks, whereas their training requires less memory and is faster for large d. The source code is available.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#36716;&#31227;&#12289;&#22810;&#21151;&#33021;&#30340;&#20648;&#33021;&#31454;&#26631;&#32773;&#27169;&#22411;&#65292;&#22312;&#25972;&#20010;&#30005;&#21147;&#24066;&#22330;&#20013;&#34920;&#29616;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#24212;&#29992;&#20110;&#21478;&#19968;&#20010;&#22320;&#21306;&#30340;&#22871;&#21033;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;</title><link>http://arxiv.org/abs/2301.01233</link><description>&lt;p&gt;
&#21487;&#36716;&#31227;&#30340;&#20648;&#33021;&#31454;&#26631;&#32773;
&lt;/p&gt;
&lt;p&gt;
Transferable Energy Storage Bidder. (arXiv:2301.01233v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.01233
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#36716;&#31227;&#12289;&#22810;&#21151;&#33021;&#30340;&#20648;&#33021;&#31454;&#26631;&#32773;&#27169;&#22411;&#65292;&#22312;&#25972;&#20010;&#30005;&#21147;&#24066;&#22330;&#20013;&#34920;&#29616;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#24212;&#29992;&#20110;&#21478;&#19968;&#20010;&#22320;&#21306;&#30340;&#22871;&#21033;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20648;&#33021;&#36164;&#28304;&#21442;&#19982;&#25972;&#20010;&#30005;&#21147;&#24066;&#22330;&#26102;&#65292;&#24517;&#39035;&#32771;&#34385;&#20215;&#26684;&#27874;&#21160;&#21644;&#29289;&#29702;&#36816;&#34892;&#29305;&#24615;&#12290;&#30001;&#20110;&#30005;&#21147;&#20215;&#26684;&#39640;&#24230;&#27874;&#21160;&#65292;&#20648;&#33021;&#25928;&#29575;&#25439;&#22833;&#65292;&#21151;&#29575;&#21644;&#33021;&#37327;&#38480;&#21046;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#12289;&#22810;&#21151;&#33021;&#21644;&#21487;&#36716;&#31227;&#30340;&#26041;&#27861;&#65292;&#23558;&#22522;&#20110;&#27169;&#22411;&#30340;&#20248;&#21270;&#19982;&#21367;&#31215;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#32467;&#21512;&#36215;&#26469;&#65292;&#29992;&#20110;&#20648;&#33021;&#21709;&#24212;&#25110;&#25237;&#26631;&#25972;&#20010;&#30005;&#21147;&#24066;&#22330;&#12290;&#25105;&#20204;&#20351;&#29992;&#32445;&#32422;&#24030;&#30340;&#21382;&#21490;&#20215;&#26684;&#36827;&#34892;&#27979;&#35797;&#65292;&#24182;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21709;&#24212;&#20215;&#26684;&#21644;&#25972;&#20010;&#24066;&#22330;&#31454;&#26631;&#35774;&#32622;&#20013;&#22343;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#22312;&#21508;&#31181;&#20648;&#33021;&#25345;&#32493;&#26102;&#38388;&#19979;&#65292;&#19982;&#23436;&#32654;&#39044;&#35265;&#24773;&#20917;&#30456;&#27604;&#65292;&#23454;&#29616;&#20102;70%&#33267;&#36817;90%&#30340;&#21033;&#28070;&#27604;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#39044;&#20808;&#20351;&#29992;&#32445;&#32422;&#25968;&#25454;&#39044;&#35757;&#32451;&#20986;&#20215;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#28595;&#22823;&#21033;&#20122;&#26118;&#22763;&#20848;&#24030;&#30340;&#22871;&#21033;&#20013;&#26469;&#27979;&#35797;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#26032;&#30340;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#21487;&#23558;&#20986;&#20215;&#27169;&#22411;&#20174;&#19968;&#20010;&#20301;&#32622;&#36716;&#31227;&#21040;&#21478;&#19968;&#20010;&#20301;&#32622;&#65292;&#21516;&#26102;&#20445;&#25345;&#21487;&#27604;&#30340;&#21033;&#28070;&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy storage resources must consider both price uncertainties and their physical operating characteristics when participating in wholesale electricity markets. This is a challenging problem as electricity prices are highly volatile, and energy storage has efficiency losses, power, and energy constraints. This paper presents a novel, versatile, and transferable approach combining model-based optimization with a convolutional long short-term memory network for energy storage to respond to or bid into wholesale electricity markets. We test our proposed approach using historical prices from New York State, showing it achieves state-of-the-art results, achieving between 70% to near 90% profit ratio compared to perfect foresight cases, in both price response and wholesale market bidding setting with various energy storage durations. We also test a transfer learning approach by pre-training the bidding model using New York data and applying it to arbitrage in Queensland, Australia. The resu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;CLIP&#30340;&#36890;&#29992;&#27169;&#22411;&#65292;&#36890;&#36807;&#25991;&#26412;&#23884;&#20837;&#23398;&#20064;&#35299;&#21078;&#23398;&#20851;&#31995;&#65292;&#33021;&#22815;&#20998;&#21106;25&#31181;&#22120;&#23448;&#21644;6&#31181;&#32959;&#30244;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2301.00785</link><description>&lt;p&gt;
&#22522;&#20110;CLIP&#30340;&#36890;&#29992;&#27169;&#22411;&#29992;&#20110;&#22120;&#23448;&#20998;&#21106;&#21644;&#32959;&#30244;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
CLIP-Driven Universal Model for Organ Segmentation and Tumor Detection. (arXiv:2301.00785v3 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;CLIP&#30340;&#36890;&#29992;&#27169;&#22411;&#65292;&#36890;&#36807;&#25991;&#26412;&#23884;&#20837;&#23398;&#20064;&#35299;&#21078;&#23398;&#20851;&#31995;&#65292;&#33021;&#22815;&#20998;&#21106;25&#31181;&#22120;&#23448;&#21644;6&#31181;&#32959;&#30244;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#20844;&#24320;&#25968;&#25454;&#38598;&#22312;&#33258;&#21160;&#21270;&#22120;&#23448;&#20998;&#21106;&#21644;&#32959;&#30244;&#26816;&#27979;&#26041;&#38754;&#20135;&#29983;&#20102;&#26174;&#33879;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27599;&#20010;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#36739;&#23567;&#19988;&#37096;&#20998;&#26631;&#27880;&#38382;&#39064;&#65292;&#20197;&#21450;&#23545;&#19981;&#21516;&#31867;&#22411;&#32959;&#30244;&#30340;&#26377;&#38480;&#25506;&#31350;&#65292;&#23548;&#33268;&#24471;&#21040;&#30340;&#27169;&#22411;&#36890;&#24120;&#38480;&#20110;&#20998;&#21106;&#29305;&#23450;&#30340;&#22120;&#23448;/&#32959;&#30244;&#65292;&#24182;&#24573;&#30053;&#35299;&#21078;&#32467;&#26500;&#30340;&#35821;&#20041;&#65292;&#20063;&#26080;&#27861;&#25512;&#24191;&#21040;&#26032;&#39046;&#22495;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;CLIP&#39537;&#21160;&#30340;&#36890;&#29992;&#27169;&#22411;&#65292;&#23558;&#20174;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451; &#65288;CLIP&#65289;&#20013;&#23398;&#20064;&#21040;&#30340;&#25991;&#26412;&#23884;&#20837;&#32467;&#21512;&#21040;&#20998;&#21106;&#27169;&#22411;&#20013;&#12290;&#36825;&#31181;&#22522;&#20110;CLIP&#30340;&#26631;&#31614;&#32534;&#30721;&#25429;&#25417;&#20102;&#35299;&#21078;&#23398;&#20851;&#31995;&#65292;&#20351;&#27169;&#22411;&#23398;&#20064;&#21040;&#32467;&#26500;&#21270;&#29305;&#24449;&#23884;&#20837;&#65292;&#24182;&#20998;&#21106;25&#20010;&#22120;&#23448;&#21644;6&#31181;&#31867;&#22411;&#30340;&#32959;&#30244;&#12290;&#35813;&#27169;&#22411;&#30001;14&#20010;&#25968;&#25454;&#38598;&#32452;&#25104;&#65292;&#20351;&#29992;3410&#20010;CT&#25195;&#25551;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#26469;&#33258;3&#20010;&#39069;&#22806;&#25968;&#25454;&#38598;&#30340;6162&#20010;&#22806;&#37096;CT&#25195;&#25551;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#22312;&#21307;&#23398;&#24433;&#20687;&#20998;&#26512;&#30340;&#22269;&#38469;&#20934;&#30830;&#24615;&#22522;&#20934;&#27979;&#35797;&#65288;MIoU&#65289;&#20013;&#25490;&#21517;&#31532;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;
An increasing number of public datasets have shown a marked impact on automated organ segmentation and tumor detection. However, due to the small size and partially labeled problem of each dataset, as well as a limited investigation of diverse types of tumors, the resulting models are often limited to segmenting specific organs/tumors and ignore the semantics of anatomical structures, nor can they be extended to novel domains. To address these issues, we propose the CLIP-Driven Universal Model, which incorporates text embedding learned from Contrastive Language-Image Pre-training (CLIP) to segmentation models. This CLIP-based label encoding captures anatomical relationships, enabling the model to learn a structured feature embedding and segment 25 organs and 6 types of tumors. The proposed model is developed from an assembly of 14 datasets, using a total of 3,410 CT scans for training and then evaluated on 6,162 external CT scans from 3 additional datasets. We rank first on the Medical
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#32508;&#21512;&#35843;&#26597;&#12290;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#25552;&#20379;&#31995;&#32479;&#36164;&#28304;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;</title><link>http://arxiv.org/abs/2212.09597</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Reasoning with Language Model Prompting: A Survey. (arXiv:2212.09597v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#32508;&#21512;&#35843;&#26597;&#12290;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#25552;&#20379;&#31995;&#32479;&#36164;&#28304;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#29702;&#20316;&#20026;&#22797;&#26434;&#38382;&#39064;&#35299;&#20915;&#30340;&#37325;&#35201;&#33021;&#21147;&#65292;&#21487;&#20197;&#20026;&#21307;&#30103;&#35786;&#26029;&#12289;&#35848;&#21028;&#31561;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#21518;&#31471;&#25903;&#25345;&#12290;&#26412;&#25991;&#23545;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#36827;&#34892;&#20102;&#32508;&#21512;&#35843;&#26597;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#30740;&#31350;&#25104;&#26524;&#30340;&#27604;&#36739;&#21644;&#24635;&#32467;&#65292;&#24182;&#25552;&#20379;&#20102;&#31995;&#32479;&#36164;&#28304;&#20197;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#31361;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#36164;&#28304;&#21487;&#22312; https://github.com/zjunlp/Prompt4ReasoningPapers &#19978;&#33719;&#21462;&#65288;&#23450;&#26399;&#26356;&#26032;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reasoning, as an essential ability for complex problem-solving, can provide back-end support for various real-world applications, such as medical diagnosis, negotiation, etc. This paper provides a comprehensive survey of cutting-edge research on reasoning with language model prompting. We introduce research works with comparisons and summaries and provide systematic resources to help beginners. We also discuss the potential reasons for emerging such reasoning abilities and highlight future research directions. Resources are available at https://github.com/zjunlp/Prompt4ReasoningPapers (updated periodically).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#20256;&#36882;&#21040;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#24182;&#35777;&#26126;&#36825;&#26679;&#30340;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#23567;&#22411;&#27169;&#22411;&#22312;&#31639;&#26415;&#12289;&#24120;&#35782;&#21644;&#31526;&#21495;&#25512;&#29702;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.08410</link><description>&lt;p&gt;
&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Teaching Small Language Models to Reason. (arXiv:2212.08410v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#20256;&#36882;&#21040;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#24182;&#35777;&#26126;&#36825;&#26679;&#30340;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#23567;&#22411;&#27169;&#22411;&#22312;&#31639;&#26415;&#12289;&#24120;&#35782;&#21644;&#31526;&#21495;&#25512;&#29702;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24605;&#32500;&#38142;&#30340;&#21551;&#21457;&#65292;&#25104;&#21151;&#22320;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312;&#19968;&#31995;&#21015;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25512;&#29702;&#33021;&#21147;&#20284;&#20046;&#20165;&#22312;&#25317;&#26377;&#36229;&#36807;1000&#20159;&#20010;&#21442;&#25968;&#30340;&#27169;&#22411;&#20013;&#20986;&#29616;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#23558;&#36825;&#31181;&#25512;&#29702;&#33021;&#21147;&#20256;&#36882;&#21040;&#23567;&#20110;1000&#20159;&#20010;&#21442;&#25968;&#30340;&#27169;&#22411;&#20013;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#36739;&#22823;&#30340;&#25945;&#24072;&#27169;&#22411;&#29983;&#25104;&#30340;&#24605;&#32500;&#38142;&#36755;&#20986;&#36827;&#34892;&#24494;&#35843;&#65292;&#23545;&#23398;&#29983;&#27169;&#22411;&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#31639;&#26415;&#12289;&#24120;&#35782;&#21644;&#31526;&#21495;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#25552;&#39640;&#20219;&#21153;&#24615;&#33021;&#12290;&#20363;&#22914;&#65292;T5 XXL&#22312;GSM8K&#25968;&#25454;&#38598;&#19978;&#30340;&#20934;&#30830;&#29575;&#20174;8.11%&#25552;&#39640;&#21040;21.99%&#65292;&#24403;&#23427;&#34987;PaLM-540B&#29983;&#25104;&#30340;&#24605;&#32500;&#38142;&#36827;&#34892;&#24494;&#35843;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chain of thought prompting successfully improves the reasoning capabilities of large language models, achieving state of the art results on a range of datasets. However, these reasoning capabilities only appear to emerge in models with a size of over 100 billion parameters. In this paper, we explore the transfer of such reasoning capabilities to models with less than 100 billion parameters via knowledge distillation. Specifically, we finetune a student model on the chain of thought outputs generated by a larger teacher model. Our experiments show that the proposed method improves task performance across arithmetic, commonsense and symbolic reasoning datasets. For example, the accuracy of T5 XXL on GSM8K improves from 8.11% to 21.99% when finetuned on PaLM-540B generated chains of thought.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;AirfRANS&#65292;&#29992;&#20110;&#30740;&#31350;&#19981;&#21516;&#25915;&#20987;&#35282;&#30340;&#32764;&#22411;&#22312;&#20122;&#38899;&#36895;&#21306;&#22495;&#30340;&#20108;&#32500;&#19981;&#21487;&#21387;&#21487;&#31283;&#23450;&#38647;&#35834;&#24179;&#22343;&#32435;&#32500;-&#26031;&#25176;&#20811;&#26031;&#26041;&#31243;&#65292;&#24182;&#36890;&#36807;&#24230;&#37327;&#26041;&#27861;&#21644;&#21487;&#35270;&#21270;&#35780;&#20272;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#28145;&#24230;&#23398;&#20064;&#22522;&#32447;&#30740;&#31350;&#65292;&#20197;&#30740;&#31350;AirfRANS&#30340;&#27867;&#21270;&#32771;&#34385;&#12290;</title><link>http://arxiv.org/abs/2212.07564</link><description>&lt;p&gt;
AirfRANS&#65306;&#36817;&#20284;&#38647;&#35834;&#24179;&#22343;&#32435;&#32500;-&#26031;&#25176;&#20811;&#26031;&#35299;&#30340;&#39640;&#20445;&#30495;&#35745;&#31639;&#27969;&#20307;&#21147;&#23398;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
AirfRANS: High Fidelity Computational Fluid Dynamics Dataset for Approximating Reynolds-Averaged Navier-Stokes Solutions. (arXiv:2212.07564v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;AirfRANS&#65292;&#29992;&#20110;&#30740;&#31350;&#19981;&#21516;&#25915;&#20987;&#35282;&#30340;&#32764;&#22411;&#22312;&#20122;&#38899;&#36895;&#21306;&#22495;&#30340;&#20108;&#32500;&#19981;&#21487;&#21387;&#21487;&#31283;&#23450;&#38647;&#35834;&#24179;&#22343;&#32435;&#32500;-&#26031;&#25176;&#20811;&#26031;&#26041;&#31243;&#65292;&#24182;&#36890;&#36807;&#24230;&#37327;&#26041;&#27861;&#21644;&#21487;&#35270;&#21270;&#35780;&#20272;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#28145;&#24230;&#23398;&#20064;&#22522;&#32447;&#30740;&#31350;&#65292;&#20197;&#30740;&#31350;AirfRANS&#30340;&#27867;&#21270;&#32771;&#34385;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#36882;&#24402;&#25968;&#20540;&#35299;&#30340;&#39640;&#26114;&#20195;&#20215;&#65292;&#26367;&#20195;&#27169;&#22411;&#23545;&#20110;&#20248;&#21270;&#29289;&#29702;&#21160;&#21147;&#23398;&#20013;&#30340;&#26377;&#24847;&#20041;&#25968;&#37327;&#26159;&#24517;&#35201;&#30340;&#12290;&#29305;&#21035;&#26159;&#23545;&#20110;&#28082;&#20307;&#21160;&#21147;&#23398;&#21644;&#32435;&#32500;-&#26031;&#25176;&#20811;&#26031;&#26041;&#31243;&#30340;&#35745;&#31639;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#30340;&#24555;&#36895;&#22686;&#38271;&#65292;&#20195;&#34920;&#30495;&#23454;&#19990;&#30028;&#29616;&#35937;&#30340;&#21442;&#32771;&#25968;&#25454;&#38598;&#20173;&#28982;&#32570;&#20047;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;AirfRANS&#65292;&#22312;&#20122;&#38899;&#36895;&#21306;&#22495;&#30340;&#20108;&#32500;&#19981;&#21487;&#21387;&#21487;&#31283;&#23450;&#38647;&#35834;&#24179;&#22343;&#32435;&#32500;-&#26031;&#25176;&#20811;&#26031;&#26041;&#31243;&#19978;&#65292;&#23545;&#19981;&#21516;&#25915;&#20987;&#35282;&#30340;&#32764;&#22411;&#36827;&#34892;&#30740;&#31350;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#24212;&#21147;&#21147;&#37327;&#22312;&#20960;&#20309;&#34920;&#38754;&#21644;&#36793;&#30028;&#23618;&#19978;&#30340;&#24230;&#37327;&#26041;&#27861;&#21644;&#21487;&#35270;&#21270;&#26469;&#35780;&#20272;&#27169;&#22411;&#20934;&#30830;&#39044;&#27979;&#38382;&#39064;&#30340;&#20851;&#38190;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#22235;&#20010;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#19978;&#23545;AirfRANS&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#22522;&#32447;&#30740;&#31350;&#65292;&#20197;&#30740;&#31350;&#22312;&#19981;&#21516;&#32422;&#26463;&#19979;AirfRANS&#30340;&#27867;&#21270;&#32771;&#34385;&#12290;
&lt;/p&gt;
&lt;p&gt;
Surrogate models are necessary to optimize meaningful quantities in physical dynamics as their recursive numerical resolutions are often prohibitively expensive. It is mainly the case for fluid dynamics and the resolution of Navier-Stokes equations. However, despite the fast-growing field of data-driven models for physical systems, reference datasets representing real-world phenomena are lacking. In this work, we develop AirfRANS, a dataset for studying the two-dimensional incompressible steady-state Reynolds-Averaged Navier-Stokes equations over airfoils at a subsonic regime and for different angles of attacks. We also introduce metrics on the stress forces at the surface of geometries and visualization of boundary layers to assess the capabilities of models to accurately predict the meaningful information of the problem. Finally, we propose deep learning baselines on four machine learning tasks to study AirfRANS under different constraints for generalization considerations: big and s
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#24335;&#27880;&#24847;&#21147;&#21464;&#25442;&#22120;(PAT)&#65292;&#23427;&#37319;&#29992;&#20102;&#26032;&#30340;&#22278;&#29615;&#26680;&#35774;&#35745;&#65292;&#20197;&#35299;&#20915;&#22270;&#20687;&#20998;&#31867;&#20013;&#20687;&#32032;&#39640;&#20998;&#36776;&#29575;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.16961</link><description>&lt;p&gt;
&#24102;&#26377;&#22278;&#29615;&#26680;&#30340;&#27169;&#24335;&#27880;&#24847;&#21147;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Pattern Attention Transformer with Doughnut Kernel. (arXiv:2211.16961v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#24335;&#27880;&#24847;&#21147;&#21464;&#25442;&#22120;(PAT)&#65292;&#23427;&#37319;&#29992;&#20102;&#26032;&#30340;&#22278;&#29615;&#26680;&#35774;&#35745;&#65292;&#20197;&#35299;&#20915;&#22270;&#20687;&#20998;&#31867;&#20013;&#20687;&#32032;&#39640;&#20998;&#36776;&#29575;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#21363;Pattern Attention Transformer&#65288;PAT&#65289;&#65292;&#35813;&#20307;&#31995;&#32467;&#26500;&#30001;&#26032;&#30340;&#22278;&#29615;&#26680;&#32452;&#25104;&#12290;&#19982;NLP&#39046;&#22495;&#30340;&#26631;&#35760;&#19981;&#21516;&#65292;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;Transformer&#35299;&#20915;&#20102;&#22788;&#29702;&#22270;&#20687;&#20013;&#20687;&#32032;&#39640;&#20998;&#36776;&#29575;&#30340;&#38382;&#39064;&#12290;&#22312;ViT&#20013;&#65292;&#22270;&#20687;&#34987;&#20999;&#25104;&#26041;&#24418;&#30340;&#34917;&#19969;&#12290;&#20316;&#20026;ViT&#30340;&#21518;&#32493;&#65292;Swin Transformer&#25552;&#20986;&#20102;&#19968;&#20010;&#39069;&#22806;&#30340;&#31227;&#20301;&#27493;&#39588;&#20197;&#20943;&#23569;&#22266;&#23450;&#36793;&#30028;&#30340;&#23384;&#22312;&#65292;&#36825;&#20063;&#23548;&#33268;&#8220;&#20004;&#20010;&#36830;&#25509;&#30340;Swin Transformer&#22359;&#8221;&#25104;&#20026;&#27169;&#22411;&#30340;&#26368;&#23567;&#21333;&#20301;&#12290;&#32487;&#25215;&#20102;&#34917;&#19969;/&#31383;&#21475;&#30340;&#24819;&#27861;&#65292;&#25105;&#20204;&#30340;&#22278;&#29615;&#26680;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#34917;&#19969;&#30340;&#35774;&#35745;&#12290;&#23427;&#29992;&#20256;&#24863;&#22120;&#21644;&#26356;&#26032;&#20004;&#31181;&#21306;&#22495;&#20195;&#26367;&#20102;&#32447;&#22411;&#36793;&#30028;&#65292;&#36825;&#26159;&#22522;&#20110;&#33258;&#25105;&#20851;&#27880;&#30340;&#29702;&#35299;&#65288;&#31216;&#20026;QKVA&#32593;&#26684;&#65289;&#12290;&#22278;&#29615;&#26680;&#36824;&#24102;&#26469;&#20102;&#19968;&#20010;&#20851;&#20110;&#26680;&#24418;&#29366;&#30340;&#26032;&#35805;&#39064;&#65292;&#36229;&#36234;&#20102;&#26041;&#24418;&#12290;&#20026;&#20102;&#39564;&#35777;&#20854;&#22312;&#22270;&#20687;&#20998;&#31867;&#19978;&#30340;&#24615;&#33021;&#65292;PAT&#34987;&#35774;&#35745;&#20026;&#30001;&#23450;&#26399;&#20843;&#36793;&#24418;&#24418;&#29366;&#30340;Transformer&#22359;&#32452;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present in this paper a new architecture, the Pattern Attention Transformer (PAT), that is composed of the new doughnut kernel. Compared with tokens in the NLP field, Transformer in computer vision has the problem of handling the high resolution of pixels in images. In ViT, an image is cut into square-shaped patches. As the follow-up of ViT, Swin Transformer proposes an additional step of shifting to decrease the existence of fixed boundaries, which also incurs 'two connected Swin Transformer blocks' as the minimum unit of the model. Inheriting the patch/window idea, our doughnut kernel enhances the design of patches further. It replaces the line-cut boundaries with two types of areas: sensor and updating, which is based on the comprehension of self-attention (named QKVA grid). The doughnut kernel also brings a new topic about the shape of kernels beyond square. To verify its performance on image classification, PAT is designed with Transformer blocks of regular octagon shape doughn
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#34892;&#20026;&#32422;&#26463;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38381;&#21512;&#24418;&#24335;&#31574;&#30053;&#25913;&#36827;&#31639;&#23376;&#65292;&#35813;&#31639;&#23376;&#23558;&#34892;&#20026;&#31574;&#30053;&#24314;&#27169;&#20026;&#39640;&#26031;&#28151;&#21512;&#65292;&#21033;&#29992;LogSumExp&#30340;&#19979;&#30028;&#21644;Jensen&#19981;&#31561;&#24335;&#20811;&#26381;&#20102;&#20248;&#21270;&#22256;&#38590;&#65292;&#33021;&#26377;&#25928;&#22788;&#29702;&#23454;&#38469;&#25968;&#25454;&#38598;&#20013;&#30340;&#24322;&#26500;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2211.15956</link><description>&lt;p&gt;
&#22522;&#20110;&#38381;&#21512;&#24418;&#24335;&#31574;&#30053;&#25913;&#36827;&#31639;&#23376;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Offline Reinforcement Learning with Closed-Form Policy Improvement Operators. (arXiv:2211.15956v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15956
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#34892;&#20026;&#32422;&#26463;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38381;&#21512;&#24418;&#24335;&#31574;&#30053;&#25913;&#36827;&#31639;&#23376;&#65292;&#35813;&#31639;&#23376;&#23558;&#34892;&#20026;&#31574;&#30053;&#24314;&#27169;&#20026;&#39640;&#26031;&#28151;&#21512;&#65292;&#21033;&#29992;LogSumExp&#30340;&#19979;&#30028;&#21644;Jensen&#19981;&#31561;&#24335;&#20811;&#26381;&#20102;&#20248;&#21270;&#22256;&#38590;&#65292;&#33021;&#26377;&#25928;&#22788;&#29702;&#23454;&#38469;&#25968;&#25454;&#38598;&#20013;&#30340;&#24322;&#26500;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34892;&#20026;&#32422;&#26463;&#31574;&#30053;&#20248;&#21270;&#24050;&#34987;&#35777;&#26126;&#26159;&#35299;&#20915;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#19968;&#31181;&#25104;&#21151;&#30340;&#33539;&#24335;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#25105;&#20204;&#30340;&#38381;&#21512;&#24418;&#24335;&#31574;&#30053;&#25913;&#36827;&#31639;&#23376;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#34892;&#20026;&#32422;&#26463;&#33258;&#28982;&#22320;&#28608;&#21169;&#20102;&#20351;&#29992;&#19968;&#38454;&#27888;&#21202;&#36817;&#20284;&#65292;&#20174;&#32780;&#23548;&#33268;&#20102;&#31574;&#30053;&#30446;&#26631;&#30340;&#32447;&#24615;&#36817;&#20284;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#23454;&#38469;&#25968;&#25454;&#38598;&#36890;&#24120;&#30001;&#24322;&#26500;&#31574;&#30053;&#25910;&#38598;&#32780;&#26469;&#65292;&#25105;&#20204;&#23558;&#34892;&#20026;&#31574;&#30053;&#24314;&#27169;&#20026;&#39640;&#26031;&#28151;&#21512;&#65292;&#24182;&#21033;&#29992;LogSumExp&#30340;&#19979;&#30028;&#21644;Jensen&#19981;&#31561;&#24335;&#20811;&#26381;&#20102;&#24341;&#36215;&#20248;&#21270;&#22256;&#38590;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#38381;&#24335;&#31574;&#30053;&#25913;&#36827;&#31639;&#23376;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#26032;&#39062;&#31574;&#30053;&#25913;&#36827;&#31639;&#23376;&#26469;&#23454;&#20363;&#21270;&#31163;&#32447;RL&#31639;&#27861;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#23427;&#20204;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Behavior constrained policy optimization has been demonstrated to be a successful paradigm for tackling Offline Reinforcement Learning. By exploiting historical transitions, a policy is trained to maximize a learned value function while constrained by the behavior policy to avoid a significant distributional shift. In this paper, we propose our closed-form policy improvement operators. We make a novel observation that the behavior constraint naturally motivates the use of first-order Taylor approximation, leading to a linear approximation of the policy objective. Additionally, as practical datasets are usually collected by heterogeneous policies, we model the behavior policies as a Gaussian Mixture and overcome the induced optimization difficulties by leveraging the LogSumExp's lower bound and Jensen's Inequality, giving rise to a closed-form policy improvement operator. We instantiate offline RL algorithms with our novel policy improvement operators and empirically demonstrate their e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#31070;&#32463;HMM TTS&#31995;&#32479;&#65292;&#22312;&#20445;&#25345;&#21644;&#33258;&#28982;&#35821;&#38899;&#23545;&#40784;&#29366;&#24577;&#30340;&#21069;&#25552;&#19979;&#65292;&#23454;&#29616;&#20102;&#35805;&#35821;&#32423;&#35821;&#35843;&#25511;&#21046;&#65292;&#20351;&#24471;&#20174;&#23569;&#37327;&#19981;&#35268;&#21017;&#25968;&#25454;&#20013;&#24555;&#36895;&#23398;&#20064;&#21464;&#24471;&#21487;&#33021;&#65292;&#33021;&#22815;&#37325;&#29616;&#33258;&#28982;&#35821;&#38899;&#20013;&#30340;&#22810;&#26679;&#24615;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2211.13533</link><description>&lt;p&gt;
&#24102;&#26377;&#31070;&#32463;HMM&#30340;&#35821;&#35843;&#21487;&#25511;&#30340;&#33258;&#28982;&#35821;&#38899;&#21512;&#25104;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Prosody-controllable spontaneous TTS with neural HMMs. (arXiv:2211.13533v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13533
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#31070;&#32463;HMM TTS&#31995;&#32479;&#65292;&#22312;&#20445;&#25345;&#21644;&#33258;&#28982;&#35821;&#38899;&#23545;&#40784;&#29366;&#24577;&#30340;&#21069;&#25552;&#19979;&#65292;&#23454;&#29616;&#20102;&#35805;&#35821;&#32423;&#35821;&#35843;&#25511;&#21046;&#65292;&#20351;&#24471;&#20174;&#23569;&#37327;&#19981;&#35268;&#21017;&#25968;&#25454;&#20013;&#24555;&#36895;&#23398;&#20064;&#21464;&#24471;&#21487;&#33021;&#65292;&#33021;&#22815;&#37325;&#29616;&#33258;&#28982;&#35821;&#38899;&#20013;&#30340;&#22810;&#26679;&#24615;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#38899;&#21512;&#25104;&#25216;&#26415;&#20013;&#65292;&#25511;&#21046;&#34920;&#29616;&#24773;&#24863;&#21644;&#35821;&#29992;&#21151;&#33021;&#30340;&#27969;&#30021;&#35821;&#38899;&#26159;&#24456;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#20174;&#23569;&#37327;&#19981;&#35268;&#21017;&#25968;&#25454;&#20013;&#24555;&#36895;&#23398;&#20064;&#24182;&#21516;&#26102;&#37325;&#29616;&#33258;&#28982;&#35821;&#38899;&#20013;&#34920;&#29616;&#22810;&#26679;&#24615;&#30340;TTS&#26550;&#26500;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#24050;&#26377;&#30340;&#22522;&#20110;&#31070;&#32463;HMM&#30340;TTS&#31995;&#32479;&#20013;&#28155;&#21152;&#20102;&#35805;&#35821;&#32423;&#30340;&#35821;&#35843;&#25511;&#21046;&#65292;&#36825;&#20010;&#31995;&#32479;&#33021;&#22815;&#31283;&#23450;&#12289;&#21333;&#35843;&#22320;&#20445;&#25345;&#21644;&#33258;&#28982;&#35821;&#38899;&#30340;&#23545;&#40784;&#29366;&#24577;&#12290;&#25105;&#20204;&#23545;&#25511;&#21046;&#20934;&#30830;&#24615;&#36827;&#34892;&#20102;&#23458;&#35266;&#30340;&#35780;&#20272;&#65292;&#24182;&#36827;&#34892;&#20102;&#24863;&#30693;&#27979;&#35797;&#26469;&#35777;&#26126;&#35821;&#35843;&#25511;&#21046;&#19981;&#20250;&#38477;&#20302;&#21512;&#25104;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spontaneous speech has many affective and pragmatic functions that are interesting and challenging to model in TTS. However, the presence of reduced articulation, fillers, repetitions, and other disfluencies in spontaneous speech make the text and acoustics less aligned than in read speech, which is problematic for attention-based TTS. We propose a TTS architecture that can rapidly learn to speak from small and irregular datasets, while also reproducing the diversity of expressive phenomena present in spontaneous speech. Specifically, we add utterance-level prosody control to an existing neural HMM-based TTS system which is capable of stable, monotonic alignments for spontaneous speech. We objectively evaluate control accuracy and perform perceptual tests that demonstrate that prosody control does not degrade synthesis quality. To exemplify the power of combining prosody control and ecologically valid data for reproducing intricate spontaneous speech phenomena, we evaluate the system's
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36830;&#32493;&#32858;&#31867;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#22240;&#23376;&#22270;&#32467;&#26500;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#32447;&#24615;&#30721;&#38388;&#24178;&#25200;&#20449;&#36947;&#19978;&#30340;&#31526;&#21495;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#23558;&#22240;&#23376;&#22270;&#26694;&#26550;&#30340;&#28508;&#21147;&#21457;&#25381;&#21040;&#26497;&#33268;&#65292;&#23454;&#29616;&#20302;&#22797;&#26434;&#24230;&#31526;&#21495;&#26816;&#27979;&#22120;&#65292;&#21516;&#26102;&#32467;&#21512;&#31070;&#32463;&#32622;&#20449;&#20256;&#25773;&#31639;&#27861;&#65292;&#22312;&#29305;&#23450;&#20449;&#36947;&#19979;&#23454;&#29616;&#36817;&#20284;&#26368;&#22823;&#21518;&#39564;&#31526;&#21495;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.11406</link><description>&lt;p&gt;
&#22522;&#20110;&#36830;&#32493;&#32858;&#31867;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#22240;&#23376;&#22270;&#32467;&#26500;&#20248;&#21270;&#22312;&#31526;&#21495;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Structural Optimization of Factor Graphs for Symbol Detection via Continuous Clustering and Machine Learning. (arXiv:2211.11406v2 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36830;&#32493;&#32858;&#31867;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#22240;&#23376;&#22270;&#32467;&#26500;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#32447;&#24615;&#30721;&#38388;&#24178;&#25200;&#20449;&#36947;&#19978;&#30340;&#31526;&#21495;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#23558;&#22240;&#23376;&#22270;&#26694;&#26550;&#30340;&#28508;&#21147;&#21457;&#25381;&#21040;&#26497;&#33268;&#65292;&#23454;&#29616;&#20302;&#22797;&#26434;&#24230;&#31526;&#21495;&#26816;&#27979;&#22120;&#65292;&#21516;&#26102;&#32467;&#21512;&#31070;&#32463;&#32622;&#20449;&#20256;&#25773;&#31639;&#27861;&#65292;&#22312;&#29305;&#23450;&#20449;&#36947;&#19979;&#23454;&#29616;&#36817;&#20284;&#26368;&#22823;&#21518;&#39564;&#31526;&#21495;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#22240;&#23376;&#22270;&#32467;&#26500;&#29992;&#20110;&#22522;&#20110;&#22270;&#30340;&#25512;&#26029;&#30340;&#26032;&#26041;&#27861;&#12290;&#20316;&#20026;&#19968;&#20010;&#20363;&#23376;&#25512;&#26029;&#20219;&#21153;&#65292;&#25105;&#20204;&#32771;&#34385;&#32447;&#24615;&#30721;&#38388;&#24178;&#25200;&#20449;&#36947;&#19978;&#30340;&#31526;&#21495;&#26816;&#27979;&#12290;&#22240;&#23376;&#22270;&#26694;&#26550;&#20855;&#26377;&#20135;&#29983;&#20302;&#22797;&#26434;&#24230;&#31526;&#21495;&#26816;&#27979;&#22120;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#24490;&#29615;&#22240;&#23376;&#22270;&#19978;&#30340;&#27714;&#21644;-&#20056;&#31215;&#31639;&#27861;&#26159;&#27425;&#20248;&#30340;&#65292;&#24182;&#19988;&#20854;&#24615;&#33021;&#23545;&#24213;&#23618;&#22270;&#39640;&#24230;&#25935;&#24863;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#20248;&#21270;&#24213;&#23618;&#22240;&#23376;&#22270;&#30340;&#32467;&#26500;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#32467;&#26500;&#20248;&#21270;&#36716;&#21270;&#20026;&#21253;&#21547;&#24050;&#30693;&#20449;&#36947;&#27169;&#22411;&#30340;&#20302;&#24230;&#22240;&#23376;&#33410;&#28857;&#30340;&#32858;&#31867;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#31181;&#26041;&#27861;&#19982;&#31070;&#32463;&#32622;&#20449;&#20256;&#25773;&#30340;&#32467;&#21512;&#65292;&#38024;&#23545;&#29305;&#23450;&#20449;&#36947;&#23454;&#29616;&#20102;&#25509;&#36817;&#26368;&#22823;&#21518;&#39564;&#31526;&#21495;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel method to optimize the structure of factor graphs for graph-based inference. As an example inference task, we consider symbol detection on linear inter-symbol interference channels. The factor graph framework has the potential to yield low-complexity symbol detectors. However, the sum-product algorithm on cyclic factor graphs is suboptimal and its performance is highly sensitive to the underlying graph. Therefore, we optimize the structure of the underlying factor graphs in an end-to-end manner using machine learning. For that purpose, we transform the structural optimization into a clustering problem of low-degree factor nodes that incorporates the known channel model into the optimization. Furthermore, we study the combination of this approach with neural belief propagation, yielding near-maximum a posteriori symbol detection performance for specific channels.
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#37327;&#23376;&#36864;&#28779;&#22120;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#30340;&#36825;&#31181;&#25216;&#26415;&#65292;&#21487;&#20197;&#23613;&#21487;&#33021;&#25552;&#39640;&#29305;&#24449;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#20887;&#20313;&#24615;&#65292;&#20351;&#25968;&#25454;&#22788;&#29702;&#26356;&#20934;&#30830;&#19988;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2211.09756</link><description>&lt;p&gt;
&#19968;&#31181;&#21033;&#29992;&#37327;&#23376;&#36864;&#28779;&#22120;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#30340;&#20248;&#21183;
&lt;/p&gt;
&lt;p&gt;
An Advantage Using Feature Selection with a Quantum Annealer. (arXiv:2211.09756v4 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09756
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#37327;&#23376;&#36864;&#28779;&#22120;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#30340;&#36825;&#31181;&#25216;&#26415;&#65292;&#21487;&#20197;&#23613;&#21487;&#33021;&#25552;&#39640;&#29305;&#24449;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#20887;&#20313;&#24615;&#65292;&#20351;&#25968;&#25454;&#22788;&#29702;&#26356;&#20934;&#30830;&#19988;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#36873;&#25321;&#26159;&#19968;&#31181;&#22312;&#32479;&#35745;&#39044;&#27979;&#24314;&#27169;&#20013;&#20351;&#29992;&#30340;&#25216;&#26415;&#65292;&#23427;&#30830;&#23450;&#19968;&#26465;&#35760;&#24405;&#20013;&#19982;&#30446;&#26631;&#21464;&#37327;&#26377;&#24378;&#32479;&#35745;&#20851;&#32852;&#30340;&#29305;&#24449;&#12290;&#22312;&#35757;&#32451;&#20013;&#25490;&#38500;&#19982;&#30446;&#26631;&#21464;&#37327;&#27809;&#26377;&#24378;&#20851;&#32852;&#30340;&#29305;&#24449;&#19981;&#20165;&#38477;&#20302;&#20102;&#25968;&#25454;&#30340;&#32500;&#24230;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#31639;&#27861;&#30340;&#26102;&#31354;&#22797;&#26434;&#24230;&#65292;&#36824;&#20943;&#23569;&#20102;&#25968;&#25454;&#20013;&#30340;&#22122;&#22768;&#65292;&#26377;&#21161;&#20110;&#36991;&#20813;&#36807;&#25311;&#21512;&#12290;&#24635;&#20043;&#65292;&#29305;&#24449;&#36873;&#25321;&#26377;&#21161;&#20110;&#35757;&#32451;&#20986;&#24615;&#33021;&#20248;&#24322;&#19988;&#31283;&#23450;&#30340;&#32479;&#35745;&#27169;&#22411;&#12290;&#37492;&#20110;&#32463;&#20856;&#35745;&#31639;&#30340;&#32570;&#20047;&#21487;&#25193;&#23637;&#24615;&#65292;&#24403;&#21069;&#30340;&#25216;&#26415;&#20165;&#32771;&#34385;&#29305;&#24449;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#32780;&#19981;&#32771;&#34385;&#29305;&#24449;&#33258;&#36523;&#20043;&#38388;&#30340;&#20887;&#20313;&#24615;&#12290;&#21033;&#29992;&#37327;&#23376;&#36864;&#28779;(QA) &#30340;&#29305;&#24449;&#36873;&#25321;&#36817;&#26399;&#26377;&#25152;&#36827;&#23637;&#65292;&#21487;&#20197;&#25552;&#20379;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#25216;&#26415;&#65292;&#26088;&#22312;&#23613;&#21487;&#33021;&#22320;&#25552;&#39640;&#29305;&#24449;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#20887;&#20313;&#24615;&#12290;&#22240;&#27492;&#65292;&#39044;&#35745;&#35813;&#31639;&#27861;&#23558;&#26377;&#21161;&#20110;&#36827;&#34892;bias/variance&#30340;&#24179;&#34913;&#65292;&#20351;&#39044;&#27979;&#26356;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feature selection is a technique in statistical prediction modeling that identifies features in a record with a strong statistical connection to the target variable. Excluding features with a weak statistical connection to the target variable in training not only drops the dimension of the data, which decreases the time complexity of the algorithm, it also decreases noise within the data which assists in avoiding overfitting. In all, feature selection assists in training a robust statistical model that performs well and is stable. Given the lack of scalability in classical computation, current techniques only consider the predictive power of the feature and not redundancy between the features themselves. Recent advancements in feature selection that leverages quantum annealing (QA) gives a scalable technique that aims to maximize the predictive power of the features while minimizing redundancy. As a consequence, it is expected that this algorithm would assist in the bias/variance trade
&lt;/p&gt;</description></item><item><title>&#22312;&#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#20013;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#22122;&#22768;&#12289;&#33258;&#28982;&#21457;&#29983;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#21644;&#21327;&#21464;&#37327;&#28418;&#31227;&#21487;&#33021;&#20250;&#23548;&#33268;&#19981;&#33391;&#27169;&#24335;&#34987;&#38169;&#35823;&#22320;&#36171;&#20104;&#39640;&#22870;&#21169;&#20540;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#22870;&#21169;&#21338;&#24328;&#65292;&#38656;&#35201;&#35299;&#20915;&#12290;</title><link>http://arxiv.org/abs/2211.08714</link><description>&lt;p&gt;
&#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#22870;&#21169;&#21338;&#24328;
&lt;/p&gt;
&lt;p&gt;
Reward Gaming in Conditional Text Generation. (arXiv:2211.08714v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08714
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#20013;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#22122;&#22768;&#12289;&#33258;&#28982;&#21457;&#29983;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#21644;&#21327;&#21464;&#37327;&#28418;&#31227;&#21487;&#33021;&#20250;&#23548;&#33268;&#19981;&#33391;&#27169;&#24335;&#34987;&#38169;&#35823;&#22320;&#36171;&#20104;&#39640;&#22870;&#21169;&#20540;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#22870;&#21169;&#21338;&#24328;&#65292;&#38656;&#35201;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20351;&#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#30340;&#36755;&#20986;&#19982;&#25152;&#38656;&#34892;&#20026;&#30456;&#19968;&#33268;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#28857;&#22312;&#20110;&#20351;&#29992;&#20174;&#20154;&#31867;&#27880;&#37322;&#20013;&#23398;&#20064;&#30340;&#22870;&#21169;&#20989;&#25968;&#20197;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#35757;&#32451;&#27169;&#22411;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19977;&#31181;&#24120;&#35265;&#24773;&#20917;&#65292;&#21363;&#30001;&#22122;&#22768;&#24341;&#36215;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#12289;&#33258;&#28982;&#21457;&#29983;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#21644;&#21327;&#21464;&#37327;&#28418;&#31227;&#65292;&#20854;&#20013;&#39640;&#22870;&#21169;&#34987;&#38169;&#35823;&#22320;&#20998;&#37197;&#32473;&#19981;&#33391;&#27169;&#24335;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#21363;&#20351;&#23398;&#20064;&#21040;&#30340;&#24230;&#37327;&#22312;&#35757;&#32451;&#22870;&#21169;&#20989;&#25968;&#25152;&#20351;&#29992;&#30340;&#25968;&#25454;&#20998;&#24067;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#19981;&#33391;&#27169;&#24335;&#22312;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#30340;RL&#35757;&#32451;&#36807;&#31243;&#20013;&#20173;&#26377;&#21487;&#33021;&#34987;&#25918;&#22823;&#12290;&#23613;&#31649;RL&#25110;&#23433;&#20840;&#31038;&#21306;&#24050;&#32463;&#24320;&#22987;&#35752;&#35770;&#22870;&#21169;&#21338;&#24328;&#65292;&#20294;&#22312;&#36825;&#31687;&#35752;&#35770;&#20013;&#65292;&#25105;&#20204;&#24076;&#26395;&#20351;&#29992;&#20855;&#20307;&#30340;&#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#31034;&#20363;&#65292;&#37325;&#28857;&#20171;&#32461;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#31038;&#21306;&#20013;&#30340;&#22870;&#21169;&#21338;&#24328;&#65292;&#24182;&#35752;&#35770;&#21487;&#33021;&#30340;&#20462;&#22797;&#25514;&#26045;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
To align conditional text generation model outputs with desired behaviors, there has been an increasing focus on training the model using reinforcement learning (RL) with reward functions learned from human annotations. Under this framework, we identify three common cases where high rewards are incorrectly assigned to undesirable patterns: noise-induced spurious correlation, naturally occurring spurious correlation, and covariate shift. We show that even though learned metrics achieve high performance on the distribution of the data used to train the reward function, the undesirable patterns may be amplified during RL training of the text generation model. While there has been discussion about reward gaming in the RL or safety community, in this discussion piece, we would like to highlight reward gaming in the natural language generation (NLG) community using concrete conditional text generation examples and discuss potential fixes and areas for future work.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#27169;&#24335;&#36830;&#36890;&#24615;&#30340;&#35270;&#35282;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#25439;&#22833;&#26223;&#35266;&#65292;&#25552;&#20986;&#20102;&#26426;&#21046;&#30456;&#20284;&#24615;&#30340;&#23450;&#20041;&#65292;&#24182;&#28436;&#31034;&#20102;&#20004;&#20010;&#27169;&#22411;&#20043;&#38388;&#32570;&#20047;&#32447;&#24615;&#36830;&#36890;&#24615;&#24847;&#21619;&#30528;&#23427;&#20204;&#20351;&#29992;&#19981;&#21516;&#30340;&#26426;&#21046;&#26469;&#20570;&#20986;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22522;&#20110;&#36830;&#36890;&#24615;&#30340;&#24494;&#35843;&#65288;CBFT&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#30446;&#26631;&#20462;&#25913;&#27169;&#22411;&#26426;&#21046;&#65292;&#26377;&#21161;&#20110;&#28040;&#38500;&#27169;&#22411;&#23545;&#34394;&#20551;&#29305;&#24449;&#30340;&#20381;&#36182;&#12290;</title><link>http://arxiv.org/abs/2211.08422</link><description>&lt;p&gt;
&#26426;&#21046;&#27169;&#24335;&#36830;&#36890;&#24615;
&lt;/p&gt;
&lt;p&gt;
Mechanistic Mode Connectivity. (arXiv:2211.08422v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08422
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#27169;&#24335;&#36830;&#36890;&#24615;&#30340;&#35270;&#35282;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#25439;&#22833;&#26223;&#35266;&#65292;&#25552;&#20986;&#20102;&#26426;&#21046;&#30456;&#20284;&#24615;&#30340;&#23450;&#20041;&#65292;&#24182;&#28436;&#31034;&#20102;&#20004;&#20010;&#27169;&#22411;&#20043;&#38388;&#32570;&#20047;&#32447;&#24615;&#36830;&#36890;&#24615;&#24847;&#21619;&#30528;&#23427;&#20204;&#20351;&#29992;&#19981;&#21516;&#30340;&#26426;&#21046;&#26469;&#20570;&#20986;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22522;&#20110;&#36830;&#36890;&#24615;&#30340;&#24494;&#35843;&#65288;CBFT&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#30446;&#26631;&#20462;&#25913;&#27169;&#22411;&#26426;&#21046;&#65292;&#26377;&#21161;&#20110;&#28040;&#38500;&#27169;&#22411;&#23545;&#34394;&#20551;&#29305;&#24449;&#30340;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20174;&#27169;&#24335;&#36830;&#36890;&#24615;&#30340;&#35270;&#35282;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#25439;&#22833;&#26223;&#35266;&#65292;&#21363;&#36890;&#36807;&#22312;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#32780;&#26816;&#32034;&#21040;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#23567;&#21270;&#22120;&#36890;&#36807;&#20302;&#25439;&#22833;&#30340;&#31616;&#21333;&#36335;&#24452;&#30456;&#20114;&#36830;&#25509;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26426;&#21046;&#30456;&#20284;&#24615;&#30340;&#23450;&#20041;&#65292;&#21363;&#19982;&#36755;&#20837;&#36716;&#25442;&#30340;&#20849;&#20139;&#19981;&#21464;&#24615;&#65292;&#24182;&#28436;&#31034;&#20102;&#20004;&#20010;&#27169;&#22411;&#20043;&#38388;&#32570;&#20047;&#32447;&#24615;&#36830;&#36890;&#24615;&#24847;&#21619;&#30528;&#23427;&#20204;&#20351;&#29992;&#19981;&#21516;&#30340;&#26426;&#21046;&#26469;&#20570;&#20986;&#39044;&#27979;&#12290;&#19982;&#23454;&#36341;&#30456;&#20851;&#30340;&#26159;&#65292;&#36825;&#20010;&#32467;&#26524;&#24110;&#21161;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#19979;&#28216;&#25968;&#25454;&#38598;&#19978;&#30340;&#31616;&#21333;&#24494;&#35843;&#21487;&#33021;&#26080;&#27861;&#25913;&#21464;&#27169;&#22411;&#30340;&#26426;&#21046;&#65292;&#20363;&#22914;&#65292;&#24494;&#35843;&#21487;&#33021;&#26080;&#27861;&#28040;&#38500;&#27169;&#22411;&#23545;&#34394;&#20551;&#29305;&#24449;&#30340;&#20381;&#36182;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#36824;&#25512;&#21160;&#20102;&#19968;&#31181;&#21517;&#20026;&#22522;&#20110;&#36830;&#36890;&#24615;&#30340;&#24494;&#35843;&#65288;CBFT&#65289;&#30340;&#30446;&#26631;&#21464;&#21270;&#27169;&#22411;&#26426;&#21046;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#20351;&#29992;&#20960;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#23545;&#20854;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study neural network loss landscapes through the lens of mode connectivity, the observation that minimizers of neural networks retrieved via training on a dataset are connected via simple paths of low loss. Specifically, we ask the following question: are minimizers that rely on different mechanisms for making their predictions connected via simple paths of low loss? We provide a definition of mechanistic similarity as shared invariances to input transformations and demonstrate that lack of linear connectivity between two models implies they use dissimilar mechanisms for making their predictions. Relevant to practice, this result helps us demonstrate that naive fine-tuning on a downstream dataset can fail to alter a model's mechanisms, e.g., fine-tuning can fail to eliminate a model's reliance on spurious attributes. Our analysis also motivates a method for targeted alteration of a model's mechanisms, named connectivity-based fine-tuning (CBFT), which we analyze using several synthe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#35821;&#27861;&#30340;&#34920;&#36848;&#30340;&#19968;&#33268;&#24615;&#21644;&#31283;&#20581;&#24615;&#65292;&#36890;&#36807;&#22810;&#39033;&#23454;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#31070;&#32463;&#32593;&#32476;&#20013; emergent &#35821;&#35328;&#32467;&#26500;&#26159;&#33030;&#24369;&#30340;&#12290;</title><link>http://arxiv.org/abs/2210.17406</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#30340; emergent &#35821;&#35328;&#32467;&#26500;&#26159;&#33030;&#24369;&#30340;
&lt;/p&gt;
&lt;p&gt;
Emergent Linguistic Structures in Neural Networks are Fragile. (arXiv:2210.17406v7 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.17406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#35821;&#27861;&#30340;&#34920;&#36848;&#30340;&#19968;&#33268;&#24615;&#21644;&#31283;&#20581;&#24615;&#65292;&#36890;&#36807;&#22810;&#39033;&#23454;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#31070;&#32463;&#32593;&#32476;&#20013; emergent &#35821;&#35328;&#32467;&#26500;&#26159;&#33030;&#24369;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#24378;&#21170;&#12290;&#28982;&#32780;&#65292;&#20934;&#30830;&#24230;&#31561;&#24615;&#33021;&#25351;&#26631;&#24182;&#19981;&#33021;&#34913;&#37327;&#27169;&#22411;&#22312;&#20195;&#34920;&#22797;&#26434;&#35821;&#35328;&#32467;&#26500;&#26041;&#38754;&#30340;&#36136;&#37327;&#12290;&#26412;&#25991;&#38024;&#23545;&#35821;&#35328;&#27169;&#22411;&#20195;&#34920;&#35821;&#27861;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#35821;&#35328;&#34920;&#36848;&#30340;&#19968;&#33268;&#24615;&#21644;&#31283;&#20581;&#24615;&#30340;&#26694;&#26550;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20123;&#31283;&#20581;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#24230;&#37327;&#26041;&#24335;&#65292;&#36825;&#20123;&#24230;&#37327;&#26041;&#24335;&#21033;&#29992;&#26368;&#36817;&#22312;&#36890;&#36807;&#25506;&#27979;&#20219;&#21153;&#20174;LLM&#20013;&#25552;&#21462;&#35821;&#35328;&#32467;&#26500;&#30340;&#20808;&#36827;&#25216;&#26415;&#65292;&#21363;&#29992;&#20110;&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#20449;&#24687;&#30340;&#31616;&#21333;&#20219;&#21153;&#65292;&#22914;&#35821;&#27861;&#37325;&#26500;&#21644;&#26681;&#35782;&#21035;&#12290;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#22235;&#31181;LLM&#22312;&#20845;&#20010;&#19981;&#21516;&#30340;&#35821;&#26009;&#24211;&#19978;&#23545;&#35821;&#27861;&#20445;&#25345;&#25200;&#21160;&#30340;&#24615;&#33021;&#21644;&#31283;&#20581;&#24615;&#26469;&#30740;&#31350;&#25152;&#25552;&#20986;&#30340;&#31283;&#20581;&#24230;&#37327;&#26041;&#24335;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have been reported to have strong performance on natural language processing tasks. However, performance metrics such as accuracy do not measure the quality of the model in terms of its ability to robustly represent complex linguistic structure. In this paper, focusing on the ability of language models to represent syntax, we propose a framework to assess the consistency and robustness of linguistic representations. To this end, we introduce measures of robustness of neural network models that leverage recent advances in extracting linguistic constructs from LLMs via probing tasks, i.e., simple tasks used to extract meaningful information about a single facet of a language model, such as syntax reconstruction and root identification. Empirically, we study the performance of four LLMs across six different corpora on the proposed robustness measures by analysing their performance and robustness with respect to syntax-preserving perturbations. We provide evide
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#23884;&#22871;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#65288;Nested Fourier Neural Operator&#65292;FNO&#65289;&#65292;&#29992;&#20110;&#22312;&#30406;&#22320;&#23610;&#24230;&#19978;&#36827;&#34892;&#39640;&#20998;&#36776;&#29575;&#21160;&#24577;&#19977;&#32500;CO$_2$&#23553;&#23384;&#24314;&#27169;&#12290;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#21152;&#24555;&#20102;&#23558;&#36817;700,000&#20493;&#30340;&#27969;&#21160;&#39044;&#27979;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2210.17051</link><description>&lt;p&gt;
&#20351;&#29992;&#23884;&#22871;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#36827;&#34892;&#23454;&#26102;&#39640;&#20998;&#36776;&#29575;CO$_2$&#22320;&#36136;&#23553;&#23384;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Real-time high-resolution CO$_2$ geological storage prediction using nested Fourier neural operators. (arXiv:2210.17051v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.17051
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#23884;&#22871;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#65288;Nested Fourier Neural Operator&#65292;FNO&#65289;&#65292;&#29992;&#20110;&#22312;&#30406;&#22320;&#23610;&#24230;&#19978;&#36827;&#34892;&#39640;&#20998;&#36776;&#29575;&#21160;&#24577;&#19977;&#32500;CO$_2$&#23553;&#23384;&#24314;&#27169;&#12290;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#21152;&#24555;&#20102;&#23558;&#36817;700,000&#20493;&#30340;&#27969;&#21160;&#39044;&#27979;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30899;&#25429;&#38598;&#21644;&#23553;&#23384;&#22312;&#20840;&#29699;&#20943;&#30899;&#36807;&#31243;&#20013;&#25198;&#28436;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;&#25193;&#22823;CO$_2$&#23553;&#23384;&#30340;&#35268;&#27169;&#38656;&#35201;&#20934;&#30830;&#39640;&#20998;&#36776;&#29575;&#22320;&#24314;&#27169;&#20648;&#23384;&#24211;&#30340;&#21387;&#21147;&#31215;&#32047;&#21644;&#27668;&#24577;&#32701;&#27969;&#21160;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29616;&#26377;&#25968;&#20540;&#26041;&#27861;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#65292;&#22312;&#35268;&#27169;&#19978;&#36827;&#34892;&#36825;&#26679;&#30340;&#24314;&#27169;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36825;&#26679;&#30340;&#25361;&#25112;&#23548;&#33268;&#35780;&#20272;&#23553;&#23384;&#26426;&#20250;&#23384;&#22312;&#26174;&#30528;&#19981;&#30830;&#23450;&#24615;&#65292;&#36825;&#21487;&#33021;&#20250;&#24310;&#36831;&#22823;&#35268;&#27169;&#30899;&#25429;&#38598;&#21644;&#23553;&#23384;&#30340;&#27493;&#20240;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#23884;&#22871;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#65288;Nested Fourier Neural Operator&#65292;FNO&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#30406;&#22320;&#23610;&#24230;&#19978;&#36827;&#34892;&#39640;&#20998;&#36776;&#29575;&#21160;&#24577;&#19977;&#32500;CO$_2$&#23553;&#23384;&#24314;&#27169;&#12290;&#23884;&#22871;FNO&#20351;&#29992;&#19968;&#31995;&#21015;FNO&#30340;&#20998;&#23618;&#39044;&#27979;&#20135;&#29983;&#19981;&#21516;&#32454;&#21270;&#32423;&#21035;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#21152;&#24555;&#20102;&#23558;&#36817;700,000&#20493;&#30340;&#27969;&#21160;&#39044;&#27979;&#36895;&#24230;&#12290;&#36890;&#36807;&#23398;&#20064;&#25511;&#21046;&#20559;&#24494;&#20998;&#26041;&#31243;&#32452;&#23478;&#26063;&#30340;&#35299;&#31639;&#31526;&#65292;&#23884;&#22871;FNO&#20026;CO$_2$&#23553;&#23384;&#21019;&#24314;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#25968;&#20540;&#27169;&#25311;&#22120;&#22791;&#36873;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Carbon capture and storage (CCS) plays an essential role in global decarbonization. Scaling up CCS deployment requires accurate and high-resolution modeling of the storage reservoir pressure buildup and the gaseous plume migration. However, such modeling is very challenging at scale due to the high computational costs of existing numerical methods. This challenge leads to significant uncertainties in evaluating storage opportunities, which can delay the pace of large-scale CCS deployment. We introduce Nested Fourier Neural Operator (FNO), a machine-learning framework for high-resolution dynamic 3D CO2 storage modeling at a basin scale. Nested FNO produces forecasts at different refinement levels using a hierarchy of FNOs and speeds up flow prediction nearly 700,000 times compared to existing methods. By learning the solution operator for the family of governing partial differential equations, Nested FNO creates a general-purpose numerical simulator alternative for CO2 storage with dive
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#26415;&#37319;&#26679;&#26694;&#26550;&#65292;&#35813;&#26041;&#27861;&#21487;&#20860;&#23481;&#24120;&#35265;&#30340;&#37319;&#26679;&#21464;&#21270;&#65292;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#26463;&#22810;&#26679;&#24615;&#21644;&#20196;&#20154;&#23604;&#23596;&#30340;&#24182;&#34892;&#24615;&#65292;&#20174;&#21407;&#22987;&#27169;&#22411;&#25552;&#20379;&#26080;&#20559;&#21644;&#19968;&#33268;&#30340;&#26399;&#26395;&#12290;&#22312;WMT&#26426;&#22120;&#32763;&#35793;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.15458</link><description>&lt;p&gt;
&#31639;&#26415;&#37319;&#26679;&#65306;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24182;&#34892;&#22810;&#26679;&#21270;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Arithmetic Sampling: Parallel Diverse Decoding for Large Language Models. (arXiv:2210.15458v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15458
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#26415;&#37319;&#26679;&#26694;&#26550;&#65292;&#35813;&#26041;&#27861;&#21487;&#20860;&#23481;&#24120;&#35265;&#30340;&#37319;&#26679;&#21464;&#21270;&#65292;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#26463;&#22810;&#26679;&#24615;&#21644;&#20196;&#20154;&#23604;&#23596;&#30340;&#24182;&#34892;&#24615;&#65292;&#20174;&#21407;&#22987;&#27169;&#22411;&#25552;&#20379;&#26080;&#20559;&#21644;&#19968;&#33268;&#30340;&#26399;&#26395;&#12290;&#22312;WMT&#26426;&#22120;&#32763;&#35793;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#30721;&#26041;&#27861;&#36890;&#24120;&#22312;&#36755;&#20986;&#22810;&#26679;&#24615;&#21644;&#35745;&#31639;&#24182;&#34892;&#24615;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#26681;&#25454;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38544;&#24335;&#23450;&#20041;&#30340;&#31639;&#26415;&#20195;&#30721;&#20070;&#36827;&#34892;&#37319;&#26679;&#65292;&#20860;&#23481;&#24120;&#35265;&#30340;&#37319;&#26679;&#21464;&#21270;&#65292;&#28385;&#36275;&#19968;&#23450;&#26465;&#20214;&#19979;&#30340;&#21487;&#35777;&#26126;&#30340;&#26463;&#22810;&#26679;&#24615;&#65292;&#21516;&#26102;&#20855;&#26377;&#20196;&#20154;&#23604;&#23596;&#30340;&#24182;&#34892;&#24615;&#65292;&#24182;&#20174;&#21407;&#22987;&#27169;&#22411;&#25552;&#20379;&#26080;&#20559;&#21644;&#19968;&#33268;&#30340;&#26399;&#26395;&#12290;&#25105;&#20204;&#22312;WMT&#26426;&#22120;&#32763;&#35793;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#23558;&#39044;&#26399;&#30340;BLEU&#20998;&#25968;&#22870;&#21169;&#30340;&#26631;&#20934;&#24046;&#20943;&#23569;&#20102;&#19968;&#21322;&#20197;&#19978;&#65292;&#21516;&#26102;&#19982;&#20808;&#21069;&#30340;&#26368;&#26032;&#26041;&#27861;&#26377;&#20102;&#30456;&#24403;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decoding methods for large language models often trade-off between diversity of outputs and parallelism of computation. Methods such as beam search and Gumbel top-k sampling can guarantee a different output for each element of the beam, but are not easy to parallelize. Alternatively, methods such as temperature sampling and its modifications (top-k sampling, nucleus sampling, typical decoding, and others), are embarrassingly parallel, but have no guarantees about duplicate samples. We present a framework for sampling according to an arithmetic code book implicitly defined by a large language model, compatible with common sampling variations, with provable beam diversity under certain conditions, as well as being embarrassingly parallel and providing unbiased and consistent expectations from the original model. We demonstrate the effectiveness of our approach on WMT machine translation, more than halving the standard deviation when estimating expected BLEU score reward, and closing the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20013;&#28857; Mixup &#30340;&#22810;&#35270;&#35282;&#25968;&#25454;&#23398;&#20064;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#26041;&#27861;&#33021;&#22815;&#26356;&#22909;&#22320;&#23398;&#20064;&#27599;&#20010;&#31867;&#21035;&#30340;&#25152;&#26377;&#29305;&#24449;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.13512</link><description>&lt;p&gt;
&#29992;&#20013;&#28857; Mixup &#22312;&#22810;&#35270;&#35282;&#25968;&#25454;&#20013;&#21487;&#35777;&#26126;&#23398;&#20064;&#22810;&#20803;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Provably Learning Diverse Features in Multi-View Data with Midpoint Mixup. (arXiv:2210.13512v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20013;&#28857; Mixup &#30340;&#22810;&#35270;&#35282;&#25968;&#25454;&#23398;&#20064;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#26041;&#27861;&#33021;&#22815;&#26356;&#22909;&#22320;&#23398;&#20064;&#27599;&#20010;&#31867;&#21035;&#30340;&#25152;&#26377;&#29305;&#24449;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Mixup &#26159;&#19968;&#31181;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#20381;&#36182;&#20110;&#20351;&#29992;&#25968;&#25454;&#28857;&#21644;&#26631;&#31614;&#30340;&#38543;&#26426;&#20984;&#32452;&#21512;&#36827;&#34892;&#35757;&#32451;&#12290;&#36817;&#24180;&#26469;&#65292;Mixup &#24050;&#25104;&#20026;&#35757;&#32451;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#26631;&#20934;&#22522;&#20803;&#65292;&#22240;&#20026;&#23427;&#22312;&#27867;&#21270;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#27604;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#26377;&#26126;&#26174;&#30340;&#20248;&#21183;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#20174;&#29305;&#24449;&#23398;&#20064;&#30340;&#35282;&#24230;&#35299;&#37322;&#19968;&#20123;&#36825;&#31181;&#25104;&#21151;&#12290;&#25105;&#20204;&#20851;&#27880;&#30340;&#20998;&#31867;&#38382;&#39064;&#26159;&#65292;&#27599;&#20010;&#31867;&#21035;&#21487;&#33021;&#20855;&#26377;&#22810;&#20010;&#30456;&#20851;&#29305;&#24449;&#65288;&#25110;&#35270;&#22270;&#65289;&#65292;&#21487;&#29992;&#20110;&#27491;&#30830;&#39044;&#27979;&#31867;&#21035;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#29702;&#35770;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20855;&#26377;&#27599;&#31867;&#20004;&#20010;&#29305;&#24449;&#30340;&#19968;&#31867;&#38750;&#24179;&#20961;&#25968;&#25454;&#20998;&#24067;&#20013;&#65292;&#20351;&#29992;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#35757;&#32451; 2 &#23618;&#21367;&#31215;&#32593;&#32476;&#21487;&#33021;&#20250;&#23548;&#33268;&#20960;&#20046;&#25152;&#26377;&#31867;&#21035;&#21482;&#23398;&#20064;&#19968;&#20010;&#29305;&#24449;&#65292;&#32780;&#20351;&#29992; Mixup &#30340;&#29305;&#23450;&#23454;&#20363;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#25104;&#21151;&#22320;&#23398;&#20064;&#27599;&#20010;&#31867;&#21035;&#30340;&#20004;&#20010;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mixup is a data augmentation technique that relies on training using random convex combinations of data points and their labels. In recent years, Mixup has become a standard primitive used in the training of state-of-the-art image classification models due to its demonstrated benefits over empirical risk minimization with regards to generalization and robustness. In this work, we try to explain some of this success from a feature learning perspective. We focus our attention on classification problems in which each class may have multiple associated features (or views) that can be used to predict the class correctly. Our main theoretical results demonstrate that, for a non-trivial class of data distributions with two features per class, training a 2-layer convolutional network using empirical risk minimization can lead to learning only one feature for almost all classes while training with a specific instantiation of Mixup succeeds in learning both features for every class. We also show
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23545;&#25239;&#24037;&#20855;&#30452;&#25509;&#20248;&#21270;&#20855;&#26377;&#37492;&#21035;&#24615;&#21644;&#22810;&#26679;&#24615;&#30340;&#26597;&#35810;&#26469;&#23454;&#29616;&#26356;&#22909;&#30340;&#25104;&#21592;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#22312;&#31163;&#32447;&#22330;&#26223;&#21644;&#20302;&#35823;&#25253;&#37325;&#35201;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#30340;&#25104;&#21592;&#25512;&#26029;&#12290;</title><link>http://arxiv.org/abs/2210.10750</link><description>&lt;p&gt;
&#29028;&#30719;&#20013;&#30340;&#37329;&#19997;&#38592;&#65306;&#20351;&#29992;&#38598;&#25104;&#23545;&#25239;&#26597;&#35810;&#23454;&#29616;&#26356;&#22909;&#30340;&#25104;&#21592;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Canary in a Coalmine: Better Membership Inference with Ensembled Adversarial Queries. (arXiv:2210.10750v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10750
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23545;&#25239;&#24037;&#20855;&#30452;&#25509;&#20248;&#21270;&#20855;&#26377;&#37492;&#21035;&#24615;&#21644;&#22810;&#26679;&#24615;&#30340;&#26597;&#35810;&#26469;&#23454;&#29616;&#26356;&#22909;&#30340;&#25104;&#21592;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#22312;&#31163;&#32447;&#22330;&#26223;&#21644;&#20302;&#35823;&#25253;&#37325;&#35201;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#30340;&#25104;&#21592;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26085;&#30410;&#33258;&#21160;&#21270;&#65292;&#20445;&#25252;&#20010;&#20154;&#25968;&#25454;&#25152;&#26377;&#26435;&#21644;&#30693;&#35782;&#20135;&#26435;&#38656;&#35201;&#36861;&#28335;&#35757;&#32451;&#25968;&#25454;&#30340;&#21407;&#22987;&#25152;&#26377;&#32773;&#12290;&#25104;&#21592;&#25512;&#26029;&#31639;&#27861;&#36890;&#36807;&#20351;&#29992;&#32479;&#35745;&#25216;&#26415;&#26469;&#30830;&#23450;&#30446;&#26631;&#26679;&#26412;&#26159;&#21542;&#21253;&#21547;&#22312;&#27169;&#22411;&#30340;&#35757;&#32451;&#38598;&#20013;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#21482;&#21033;&#29992;&#21407;&#22987;&#30446;&#26631;&#26679;&#26412;&#25110;&#30446;&#26631;&#26679;&#26412;&#30340;&#31616;&#21333;&#25193;&#23637;&#26469;&#35745;&#31639;&#32479;&#35745;&#37327;&#12290;&#36825;&#31181;&#23545;&#27169;&#22411;&#34892;&#20026;&#30340;&#31232;&#30095;&#37319;&#26679;&#24102;&#26469;&#20102;&#24456;&#23569;&#30340;&#20449;&#24687;&#65292;&#23548;&#33268;&#25104;&#21592;&#25512;&#26029;&#30340;&#33021;&#21147;&#36739;&#24046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#23545;&#25239;&#24037;&#20855;&#30452;&#25509;&#20248;&#21270;&#20855;&#26377;&#37492;&#21035;&#24615;&#21644;&#22810;&#26679;&#24615;&#30340;&#26597;&#35810;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#25913;&#36827;&#22312;&#31163;&#32447;&#22330;&#26223;&#21644;&#20302;&#35823;&#25253;&#37325;&#35201;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#30340;&#25104;&#21592;&#25512;&#26029;&#12290;&#20195;&#30721;&#21487;&#22312; https://github.com/YuxinWenRick/canary-in-a-coalmine &#20013;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
As industrial applications are increasingly automated by machine learning models, enforcing personal data ownership and intellectual property rights requires tracing training data back to their rightful owners. Membership inference algorithms approach this problem by using statistical techniques to discern whether a target sample was included in a model's training set. However, existing methods only utilize the unaltered target sample or simple augmentations of the target to compute statistics. Such a sparse sampling of the model's behavior carries little information, leading to poor inference capabilities. In this work, we use adversarial tools to directly optimize for queries that are discriminative and diverse. Our improvements achieve significantly more accurate membership inference than existing methods, especially in offline scenarios and in the low false-positive regime which is critical in legal settings. Code is available at https://github.com/YuxinWenRick/canary-in-a-coalmine
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25512;&#33616;&#21435;&#22122;&#30340;&#39640;&#25928;&#21452;&#23618;&#20248;&#21270;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36845;&#20195;&#35843;&#25972;&#25512;&#33616;&#27169;&#22411;&#65292;&#20197;&#32771;&#34385;&#21069;&#20960;&#27425;&#36845;&#20195;&#20013;&#20026;&#27599;&#20010;&#21453;&#39304;&#20998;&#37197;&#30340;&#26435;&#37325;&#12290;</title><link>http://arxiv.org/abs/2210.10321</link><description>&lt;p&gt;
&#25512;&#33616;&#21435;&#22122;&#30340;&#39640;&#25928;&#21452;&#23618;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Bi-Level Optimization for Recommendation Denoising. (arXiv:2210.10321v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10321
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25512;&#33616;&#21435;&#22122;&#30340;&#39640;&#25928;&#21452;&#23618;&#20248;&#21270;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36845;&#20195;&#35843;&#25972;&#25512;&#33616;&#27169;&#22411;&#65292;&#20197;&#32771;&#34385;&#21069;&#20960;&#27425;&#36845;&#20195;&#20013;&#20026;&#27599;&#20010;&#21453;&#39304;&#20998;&#37197;&#30340;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#33719;&#24471;&#26126;&#30830;&#30340;&#29992;&#25143;&#21453;&#39304;&#65288;&#20363;&#22914;&#35780;&#20998;&#65289;&#36890;&#24120;&#20250;&#21463;&#21040;&#38656;&#35201;&#29992;&#25143;&#31215;&#26497;&#21442;&#19982;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#21033;&#29992;&#29992;&#25143;&#27983;&#35272;&#26399;&#38388;&#29983;&#25104;&#30340;&#38544;&#24335;&#21453;&#39304;&#65288;&#20363;&#22914;&#28857;&#20987;&#65289;&#20316;&#20026;&#21487;&#34892;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#38544;&#24335;&#21453;&#39304;&#20855;&#26377;&#24456;&#39640;&#30340;&#22122;&#22768;&#65292;&#36825;&#20250;&#26174;&#30528;&#25439;&#23475;&#25512;&#33616;&#36136;&#37327;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#25512;&#33616;&#21435;&#22122;&#24314;&#27169;&#20026;&#19968;&#20010;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#32771;&#34385;&#21069;&#20960;&#27425;&#36845;&#20195;&#20013;&#20026;&#27599;&#20010;&#21453;&#39304;&#20998;&#37197;&#30340;&#26435;&#37325;&#26469;&#36845;&#20195;&#22320;&#35843;&#25972;&#25512;&#33616;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#22823;&#35268;&#27169;&#25512;&#33616;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The acquisition of explicit user feedback (e.g., ratings) in real-world recommender systems is often hindered by the need for active user involvement. To mitigate this issue, implicit feedback (e.g., clicks) generated during user browsing is exploited as a viable substitute. However, implicit feedback possesses a high degree of noise, which significantly undermines recommendation quality. While many methods have been proposed to address this issue by assigning varying weights to implicit feedback, two shortcomings persist: (1) the weight calculation in these methods is iteration-independent, without considering the influence of weights in previous iterations, and (2) the weight calculation often relies on prior knowledge, which may not always be readily available or universally applicable.  To overcome these two limitations, we model recommendation denoising as a bi-level optimization problem. The inner optimization aims to derive an effective model for the recommendation, as well as g
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#24322;&#26041;&#24046;&#22122;&#22768;&#27169;&#22411;&#65292;&#21457;&#29616;&#38500;&#29305;&#27530;&#24773;&#20917;&#22806;&#22240;&#26524;&#26041;&#21521;&#26159;&#21487;&#35782;&#21035;&#30340;&#12290;&#25552;&#20986;&#20102;&#20004;&#20010;&#20272;&#35745;&#22120;&#65292;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#22240;&#26524;&#25928;&#24212;&#12290;</title><link>http://arxiv.org/abs/2210.09054</link><description>&lt;p&gt;
&#20851;&#20110;&#22240;&#26524;&#20301;&#32622;-&#23610;&#24230;&#22122;&#22768;&#27169;&#22411;&#30340;&#21487;&#35782;&#21035;&#24615;&#21644;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
On the Identifiability and Estimation of Causal Location-Scale Noise Models. (arXiv:2210.09054v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#24322;&#26041;&#24046;&#22122;&#22768;&#27169;&#22411;&#65292;&#21457;&#29616;&#38500;&#29305;&#27530;&#24773;&#20917;&#22806;&#22240;&#26524;&#26041;&#21521;&#26159;&#21487;&#35782;&#21035;&#30340;&#12290;&#25552;&#20986;&#20102;&#20004;&#20010;&#20272;&#35745;&#22120;&#65292;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#22240;&#26524;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#20301;&#32622;-&#23610;&#24230;&#25110;&#24322;&#26041;&#24046;&#22122;&#22768;&#27169;&#22411;&#65288;LSNMs&#65289;&#65292;&#22312;&#20854;&#20013;&#65292;&#25928;&#24212;$ Y $&#21487;&#20197;&#34987;&#20889;&#25104;&#26159;&#22240;&#26524;$ X $&#21644;&#19982;$ X $&#26080;&#20851;&#30340;&#22122;&#22768;&#28304;$ N $&#30340;&#20989;&#25968;&#65292;&#20294;&#21487;&#33021;&#34987;&#22240;&#26524;$ X $&#32553;&#25918;&#20026;&#19968;&#20010;&#27491;&#20989;&#25968;$ g&#65288;X&#65289;$&#65292;&#21363;$ Y = f&#65288;X&#65289;+ g&#65288;X&#65289;N $&#12290;&#23613;&#31649;&#27169;&#22411;&#31867;&#21035;&#38750;&#24120;&#24191;&#27867;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#38500;&#19968;&#20123;&#29305;&#27530;&#24773;&#20917;&#22806;&#65292;&#22240;&#26524;&#26041;&#21521;&#26159;&#21487;&#35782;&#21035;&#30340;&#12290;&#20026;&#20102;&#22312;&#23454;&#35777;&#19978;&#39564;&#35777;&#36825;&#20123;&#29702;&#35770;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;LSNMs&#30340;&#20272;&#35745;&#22120;&#65306;&#19968;&#20010;&#22522;&#20110;&#65288;&#38750;&#32447;&#24615;&#65289;&#29305;&#24449;&#26144;&#23556;&#30340;&#20272;&#35745;&#22120;&#21644;&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20272;&#35745;&#22120;&#12290;&#20004;&#32773;&#23558;$ Y $&#32473;&#23450;$ X $&#30340;&#26465;&#20214;&#20998;&#24067;&#24314;&#27169;&#20026;&#30001;&#20854;&#33258;&#28982;&#21442;&#25968;&#21442;&#25968;&#21270;&#30340;&#39640;&#26031;&#20998;&#24067;&#12290;&#24403;&#29305;&#24449;&#26144;&#23556;&#34987;&#27491;&#30830;&#35268;&#23450;&#26102;&#65292;&#25105;&#20204;&#35777;&#26126;&#25105;&#20204;&#30340;&#20272;&#35745;&#22120;&#26159;&#32852;&#21512;&#20984;&#30340;&#65292;&#24182;&#19988;&#26159;&#22240;&#26524;&#25928;&#24212;&#35782;&#21035;&#20219;&#21153;&#30340;&#19968;&#33268;&#20272;&#35745;&#22120;&#12290;&#23613;&#31649;&#31070;&#32463;&#32593;&#32476;&#27809;&#26377;&#32487;&#25215;&#36825;&#20123;&#20445;&#35777;&#65292;&#20294;&#23427;&#21487;&#20197;&#25311;&#21512;&#20219;&#24847;&#22797;&#26434;&#24615;&#30340;&#20989;&#25968;&#65292;&#24182;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the class of location-scale or heteroscedastic noise models (LSNMs), in which the effect $Y$ can be written as a function of the cause $X$ and a noise source $N$ independent of $X$, which may be scaled by a positive function $g$ over the cause, i.e., $Y = f(X) + g(X)N$. Despite the generality of the model class, we show the causal direction is identifiable up to some pathological cases. To empirically validate these theoretical findings, we propose two estimators for LSNMs: an estimator based on (non-linear) feature maps, and one based on neural networks. Both model the conditional distribution of $Y$ given $X$ as a Gaussian parameterized by its natural parameters. When the feature maps are correctly specified, we prove that our estimator is jointly concave, and a consistent estimator for the cause-effect identification task. Although the the neural network does not inherit those guarantees, it can fit functions of arbitrary complexity, and reaches state-of-the-art performance
&lt;/p&gt;</description></item><item><title>SQuId&#26159;&#19968;&#20010;&#20351;&#29992;&#19968;&#30334;&#19975;&#20010;&#35780;&#20998;&#36827;&#34892;&#35757;&#32451;&#24182;&#22312;65&#20010;&#35821;&#35328;&#29615;&#22659;&#19979;&#36827;&#34892;&#27979;&#35797;&#30340;&#22810;&#35821;&#35328;&#33258;&#28982;&#24230;&#39044;&#27979;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#30340;&#35757;&#32451;&#22312;&#35768;&#22810;&#29615;&#22659;&#20013;&#22987;&#32456;&#20248;&#20110;&#21333;&#19968;&#29615;&#22659;&#30340;&#22522;&#32447;&#65292;&#24182;&#23637;&#29616;&#20102;&#27604;&#31454;&#20105;&#22522;&#32447;&#26356;&#20986;&#33394;&#30340;&#34920;&#29616;;&#30740;&#31350;&#36824;&#31361;&#20986;&#20102;&#36328;&#35821;&#35328;&#32454;&#35843;&#30340;&#26377;&#25928;&#24615;&#65292;&#20197;&#21450;&#38750;&#35821;&#35328;&#25928;&#26524;&#22914;&#22768;&#38899;&#30072;&#21464;&#22312;&#36328;&#35821;&#35328;&#32454;&#35843;&#20013;&#30340;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2210.06324</link><description>&lt;p&gt;
SQuId: &#22312;&#22810;&#35821;&#35328;&#20013;&#27979;&#37327;&#35821;&#38899;&#33258;&#28982;&#24230;
&lt;/p&gt;
&lt;p&gt;
SQuId: Measuring Speech Naturalness in Many Languages. (arXiv:2210.06324v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06324
&lt;/p&gt;
&lt;p&gt;
SQuId&#26159;&#19968;&#20010;&#20351;&#29992;&#19968;&#30334;&#19975;&#20010;&#35780;&#20998;&#36827;&#34892;&#35757;&#32451;&#24182;&#22312;65&#20010;&#35821;&#35328;&#29615;&#22659;&#19979;&#36827;&#34892;&#27979;&#35797;&#30340;&#22810;&#35821;&#35328;&#33258;&#28982;&#24230;&#39044;&#27979;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#30340;&#35757;&#32451;&#22312;&#35768;&#22810;&#29615;&#22659;&#20013;&#22987;&#32456;&#20248;&#20110;&#21333;&#19968;&#29615;&#22659;&#30340;&#22522;&#32447;&#65292;&#24182;&#23637;&#29616;&#20102;&#27604;&#31454;&#20105;&#22522;&#32447;&#26356;&#20986;&#33394;&#30340;&#34920;&#29616;;&#30740;&#31350;&#36824;&#31361;&#20986;&#20102;&#36328;&#35821;&#35328;&#32454;&#35843;&#30340;&#26377;&#25928;&#24615;&#65292;&#20197;&#21450;&#38750;&#35821;&#35328;&#25928;&#26524;&#22914;&#22768;&#38899;&#30072;&#21464;&#22312;&#36328;&#35821;&#35328;&#32454;&#35843;&#20013;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#35821;&#38899;&#30340;&#35768;&#22810;&#30740;&#31350;&#37117;&#20381;&#36182;&#20110;&#20154;&#31867;&#35780;&#20272;&#65292;&#36825;&#20250;&#20135;&#29983;&#37325;&#22823;&#25104;&#26412;&#24182;&#20943;&#32531;&#24320;&#21457;&#36827;&#31243;&#12290;&#22312;&#37325;&#24230;&#22810;&#35821;&#35328;&#24212;&#29992;&#20013;&#65292;&#25307;&#21215;&#21644;&#35843;&#26597;&#35780;&#23457;&#21592;&#21487;&#33021;&#38656;&#35201;&#25968;&#21608;&#26102;&#38388;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;SQuId&#65288;&#35821;&#38899;&#36136;&#37327;&#35782;&#21035;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#20351;&#29992;&#19968;&#30334;&#19975;&#20010;&#35780;&#20998;&#36827;&#34892;&#35757;&#32451;&#24182;&#22312;65&#20010;&#35821;&#35328;&#29615;&#22659;&#19979;&#36827;&#34892;&#27979;&#35797;&#30340;&#22810;&#35821;&#35328;&#33258;&#28982;&#24230;&#39044;&#27979;&#27169;&#22411;&#65292;&#26159;&#36804;&#20170;&#20026;&#27490;&#36825;&#31181;&#31867;&#22411;&#20013;&#26368;&#22823;&#30340;&#21162;&#21147;&#12290;&#20027;&#35201;&#30340;&#35265;&#35299;&#26159;&#65292;&#22312;&#35768;&#22810;&#29615;&#22659;&#20013;&#35757;&#32451;&#19968;&#20010;&#27169;&#22411;&#22987;&#32456;&#20248;&#20110;&#21333;&#19968;&#29615;&#22659;&#30340;&#22522;&#32447;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#25105;&#20204;&#30340;&#20219;&#21153;&#12289;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#30340;&#34920;&#29616;&#20248;&#20110;&#22522;&#20110;w2v-BERT&#21644;VoiceMOS&#30340;&#31454;&#20105;&#22522;&#32447;50.0%&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#36328;&#35821;&#35328;&#32454;&#35843;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#24378;&#35843;&#20102;&#23427;&#23545;&#38646;&#26679;&#26412;&#29615;&#22659;&#30340;&#24433;&#21709;&#65292;&#21363;&#27809;&#26377;&#32454;&#35843;&#25968;&#25454;&#30340;&#29615;&#22659;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#20998;&#26512;&#65292;&#25105;&#20204;&#31361;&#20986;&#20102;&#38750;&#35821;&#35328;&#25928;&#26524;&#65288;&#22914;&#22768;&#38899;&#30072;&#21464;&#65289;&#22312;&#36328;&#35821;&#35328;&#32454;&#35843;&#20013;&#30340;&#20316;&#29992;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#33021;&#26159;&#31532;&#19968;&#20010;&#22810;&#35821;&#35328;&#33258;&#28982;&#24230;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;65&#20010;&#29615;&#22659;&#20013;&#36229;&#36807;&#19968;&#30334;&#19975;&#35780;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Much of text-to-speech research relies on human evaluation, which incurs heavy costs and slows down the development process. The problem is particularly acute in heavily multilingual applications, where recruiting and polling judges can take weeks. We introduce SQuId (Speech Quality Identification), a multilingual naturalness prediction model trained on over a million ratings and tested in 65 locales-the largest effort of this type to date. The main insight is that training one model on many locales consistently outperforms mono-locale baselines. We present our task, the model, and show that it outperforms a competitive baseline based on w2v-BERT and VoiceMOS by 50.0%. We then demonstrate the effectiveness of cross-locale transfer during fine-tuning and highlight its effect on zero-shot locales, i.e., locales for which there is no fine-tuning data. Through a series of analyses, we highlight the role of non-linguistic effects such as sound artifacts in cross-locale transfer. Finally, we
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;TECO&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20854;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;&#35270;&#39057;&#29983;&#25104;&#30340;&#38271;&#26399;&#26102;&#38388;&#19968;&#33268;&#24615;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;3&#20010;&#38590;&#24230;&#19981;&#21516;&#30340;&#35270;&#39057;&#25968;&#25454;&#38598;&#20197;&#35780;&#20272;&#35813;&#27169;&#22411;&#21644;&#29616;&#26377;&#27169;&#22411;&#22312;&#26102;&#38388;&#19968;&#33268;&#24615;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.02396</link><description>&lt;p&gt;
&#26102;&#24577;&#19968;&#33268;&#30340;&#21464;&#25442;&#22120;&#29992;&#20110;&#35270;&#39057;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Temporally Consistent Transformers for Video Generation. (arXiv:2210.02396v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.02396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;TECO&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20854;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;&#35270;&#39057;&#29983;&#25104;&#30340;&#38271;&#26399;&#26102;&#38388;&#19968;&#33268;&#24615;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;3&#20010;&#38590;&#24230;&#19981;&#21516;&#30340;&#35270;&#39057;&#25968;&#25454;&#38598;&#20197;&#35780;&#20272;&#35813;&#27169;&#22411;&#21644;&#29616;&#26377;&#27169;&#22411;&#22312;&#26102;&#38388;&#19968;&#33268;&#24615;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#29983;&#25104;&#20934;&#30830;&#30340;&#35270;&#39057;&#65292;&#31639;&#27861;&#24517;&#39035;&#29702;&#35299;&#19990;&#30028;&#20013;&#30340;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#12290;&#30446;&#21069;&#30340;&#31639;&#27861;&#34429;&#28982;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#30701;&#26399;&#20869;&#30340;&#20869;&#23481;&#65292;&#20294;&#24448;&#24448;&#23384;&#22312;&#26102;&#38388;&#19978;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#24403;&#29983;&#25104;&#30340;&#20869;&#23481;&#28040;&#22833;&#21518;&#20877;&#27425;&#20986;&#29616;&#26102;&#65292;&#27169;&#22411;&#20250;&#21457;&#26126;&#19981;&#21516;&#30340;&#20869;&#23481;&#12290;&#23613;&#31649;&#23384;&#22312;&#36825;&#31181;&#20005;&#37325;&#30340;&#38480;&#21046;&#65292;&#20294;&#30446;&#21069;&#23578;&#19981;&#23384;&#22312;&#22797;&#26434;&#25968;&#25454;&#30340;&#24050;&#24314;&#31435;&#22522;&#20934;&#26469;&#23545;&#20855;&#26377;&#38271;&#26399;&#26102;&#38388;&#20381;&#36182;&#24615;&#30340;&#35270;&#39057;&#29983;&#25104;&#36827;&#34892;&#20005;&#26684;&#35780;&#20272;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#28210;&#26579;&#36890;&#36807; 3D &#22330;&#26223;&#30340;&#27969;&#31243;&#36855;&#23467;&#65292;Minecraft &#19990;&#30028;&#21644;&#23460;&#20869;&#25195;&#25551;&#65292;&#31574;&#21010;&#20102;&#19977;&#20010;&#20855;&#26377;&#38271;&#31243;&#20381;&#36182;&#30340;&#35270;&#39057;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23545;&#24403;&#21069;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#65292;&#24182;&#35266;&#23519;&#21040;&#23427;&#20204;&#22312;&#26102;&#38388;&#19968;&#33268;&#24615;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26102;&#24577;&#19968;&#33268;&#30340;&#21464;&#25442;&#22120;&#65288;TECO&#65289;&#65292;&#19968;&#31181;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#26174;&#33879;&#25552;&#39640;&#20102;&#38271;&#26399;&#19968;&#33268;&#24615;&#65292;&#21516;&#26102;&#20063;&#20943;&#23569;&#20102;&#37319;&#26679;&#26102;&#38388;&#12290;&#36890;&#36807;&#23558;&#20854;&#36755;&#20837;&#24207;&#21015;&#21387;&#32553;&#20026;&#26356;&#23569;&#30340;&#39033;&#65292;
&lt;/p&gt;
&lt;p&gt;
To generate accurate videos, algorithms have to understand the spatial and temporal dependencies in the world. Current algorithms enable accurate predictions over short horizons but tend to suffer from temporal inconsistencies. When generated content goes out of view and is later revisited, the model invents different content instead. Despite this severe limitation, no established benchmarks on complex data exist for rigorously evaluating video generation with long temporal dependencies. In this paper, we curate 3 challenging video datasets with long-range dependencies by rendering walks through 3D scenes of procedural mazes, Minecraft worlds, and indoor scans. We perform a comprehensive evaluation of current models and observe their limitations in temporal consistency. Moreover, we introduce the Temporally Consistent Transformer (TECO), a generative model that substantially improves long-term consistency while also reducing sampling time. By compressing its input sequence into fewer e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#23398;&#20064;&#30340;&#22810;&#26679;&#24615;&#38544;&#31169;&#20445;&#25252;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#22788;&#29702;&#21453;&#20107;&#23454;&#32422;&#26463;&#26465;&#20214;&#65292;&#24182;&#20026;&#31169;&#26377;&#35299;&#37322;&#27169;&#22411;&#20570;&#20986;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2209.13446</link><description>&lt;p&gt;
&#22522;&#20110;&#29305;&#24449;&#23398;&#20064;&#30340;&#22810;&#26679;&#24615;&#38544;&#31169;&#20445;&#25252;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Feature-based Learning for Diverse and Privacy-Preserving Counterfactual Explanations. (arXiv:2209.13446v5 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.13446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#23398;&#20064;&#30340;&#22810;&#26679;&#24615;&#38544;&#31169;&#20445;&#25252;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#22788;&#29702;&#21453;&#20107;&#23454;&#32422;&#26463;&#26465;&#20214;&#65292;&#24182;&#20026;&#31169;&#26377;&#35299;&#37322;&#27169;&#22411;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#33268;&#21147;&#20110;&#29702;&#35299;&#38271;&#26399;&#20197;&#26469;&#22240;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#32780;&#22768;&#21517;&#29436;&#34249;&#30340;&#22797;&#26434;&#40657;&#30418;&#23376;&#31995;&#32479;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#20854;&#20013;&#19968;&#31181;&#34028;&#21187;&#21457;&#23637;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#21453;&#20107;&#23454;&#35299;&#37322;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#22914;&#20309;&#25913;&#21464;&#32467;&#26524;&#30340;&#24314;&#35758;&#12290;&#21453;&#20107;&#23454;&#26679;&#26412;&#19981;&#20165;&#24517;&#39035;&#21453;&#39539;&#40657;&#30418;&#20998;&#31867;&#22120;&#30340;&#21407;&#22987;&#39044;&#27979;&#65292;&#36824;&#24517;&#39035;&#28385;&#36275;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#30340;&#32422;&#26463;&#26465;&#20214;&#65292;&#20854;&#20013;&#22810;&#26679;&#24615;&#26159;&#20851;&#38190;&#32422;&#26463;&#20043;&#19968;&#20294;&#20173;&#36739;&#23569;&#35752;&#35770;&#12290;&#34429;&#28982;&#22810;&#26679;&#21270;&#30340;&#21453;&#20107;&#23454;&#24456;&#29702;&#24819;&#65292;&#20294;&#21516;&#26102;&#35299;&#20915;&#20854;&#20182;&#32422;&#26463;&#26465;&#20214;&#22312;&#35745;&#31639;&#19978;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;&#20849;&#20139;&#21453;&#20107;&#23454;&#25968;&#25454;&#23384;&#22312;&#36234;&#26469;&#36234;&#22810;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#23398;&#20064;&#30340;&#26694;&#26550;&#26469;&#26377;&#25928;&#22320;&#22788;&#29702;&#21453;&#20107;&#23454;&#32422;&#26463;&#26465;&#20214;&#65292;&#24182;&#20026;&#31169;&#26377;&#35299;&#37322;&#27169;&#22411;&#30340;&#26377;&#38480;&#36164;&#26009;&#24211;&#20570;&#20986;&#36129;&#29486;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#40657;&#30418;&#27169;&#22411;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#30340;&#28789;&#27963;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretable machine learning seeks to understand the reasoning process of complex black-box systems that are long notorious for lack of explainability. One flourishing approach is through counterfactual explanations, which provide suggestions on what a user can do to alter an outcome. Not only must a counterfactual example counter the original prediction from the black-box classifier but it should also satisfy various constraints for practical applications. Diversity is one of the critical constraints that however remains less discussed. While diverse counterfactuals are ideal, it is computationally challenging to simultaneously address some other constraints. Furthermore, there is a growing privacy concern over the released counterfactual data. To this end, we propose a feature-based learning framework that effectively handles the counterfactual constraints and contributes itself to the limited pool of private explanation models. We demonstrate the flexibility and effectiveness of o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;&#35780;&#21028;&#32622;&#20449;&#24230;&#24341;&#23548;&#25506;&#32034;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#29616;&#26377;&#30340;&#31070;&#35861;&#31574;&#30053;&#32435;&#20837;&#26631;&#20934;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#65292;&#20197;&#25552;&#39640;&#25506;&#32034;&#25928;&#29575;&#12290;&#22312;&#19981;&#30830;&#23450;&#24615;&#39640;&#26102;&#65292;&#35813;&#26041;&#27861;&#20250;&#23558;&#31070;&#35861;&#31574;&#30053;&#30340;&#34892;&#21160;&#20316;&#20026;&#24314;&#35758;&#32435;&#20837;&#23398;&#20064;&#26041;&#26696;&#20013;&#65292;&#32780;&#22312;&#19981;&#30830;&#23450;&#24615;&#20302;&#26102;&#24573;&#30053;&#23427;&#12290;</title><link>http://arxiv.org/abs/2208.10533</link><description>&lt;p&gt;
&#19968;&#20123;&#30417;&#30563;&#26159;&#24517;&#39035;&#30340;&#65306;&#36890;&#36807;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#24341;&#20837;&#31070;&#35861;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Some Supervision Required: Incorporating Oracle Policies in Reinforcement Learning via Epistemic Uncertainty Metrics. (arXiv:2208.10533v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.10533
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;&#35780;&#21028;&#32622;&#20449;&#24230;&#24341;&#23548;&#25506;&#32034;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#29616;&#26377;&#30340;&#31070;&#35861;&#31574;&#30053;&#32435;&#20837;&#26631;&#20934;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#65292;&#20197;&#25552;&#39640;&#25506;&#32034;&#25928;&#29575;&#12290;&#22312;&#19981;&#30830;&#23450;&#24615;&#39640;&#26102;&#65292;&#35813;&#26041;&#27861;&#20250;&#23558;&#31070;&#35861;&#31574;&#30053;&#30340;&#34892;&#21160;&#20316;&#20026;&#24314;&#35758;&#32435;&#20837;&#23398;&#20064;&#26041;&#26696;&#20013;&#65292;&#32780;&#22312;&#19981;&#30830;&#23450;&#24615;&#20302;&#26102;&#24573;&#30053;&#23427;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#30340;&#22266;&#26377;&#38382;&#39064;&#26159;&#36890;&#36807;&#38543;&#26426;&#34892;&#21160;&#25506;&#32034;&#29615;&#22659;&#65292;&#20854;&#20013;&#24456;&#22823;&#19968;&#37096;&#20998;&#21487;&#33021;&#26159;&#26080;&#25928;&#30340;&#12290;&#30456;&#21453;&#65292;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#29616;&#26377;&#30340;&#65288;&#20808;&#21069;&#23398;&#20064;&#30340;&#25110;&#30828;&#32534;&#30721;&#30340;&#65289;&#31070;&#35861;&#31574;&#30053;&#12289;&#31163;&#32447;&#25968;&#25454;&#25110;&#28436;&#31034;&#26469;&#25913;&#21892;&#25506;&#32034;&#12290;&#20294;&#22312;&#20351;&#29992;&#31070;&#35861;&#31574;&#30053;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#26368;&#22823;&#21270;&#23398;&#20064;&#26679;&#26412;&#25928;&#29575;&#22320;&#23558;&#31070;&#35861;&#32463;&#39564;&#34701;&#20837;&#21040;&#23398;&#20064;&#31574;&#30053;&#20013;&#21487;&#33021;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35780;&#21028;&#32622;&#20449;&#24230;&#24341;&#23548;&#25506;&#32034;&#65288;Critic Confidence Guided Exploration&#65292;CCGE&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#36825;&#26679;&#30340;&#31070;&#35861;&#31574;&#30053;&#32435;&#20837;&#26631;&#20934;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24403;&#19981;&#30830;&#23450;&#24615;&#39640;&#26102;&#65292;CCGE&#20197;&#31070;&#35861;&#31574;&#30053;&#30340;&#34892;&#21160;&#20026;&#24314;&#35758;&#65292;&#24182;&#23558;&#27492;&#20449;&#24687;&#32435;&#20837;&#23398;&#20064;&#26041;&#26696;&#20013;&#65292;&#32780;&#24403;&#19981;&#30830;&#23450;&#24615;&#20302;&#26102;&#24573;&#30053;&#23427;&#12290;CCGE&#23545;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#19981;&#21152;&#21306;&#20998;&#65292;&#24182;&#19988;&#25105;&#20204;&#35777;&#26126;&#23427;&#19982;&#29616;&#26377;&#31639;&#27861;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
An inherent problem of reinforcement learning is performing exploration of an environment through random actions, of which a large portion can be unproductive. Instead, exploration can be improved by initializing the learning policy with an existing (previously learned or hard-coded) oracle policy, offline data, or demonstrations. In the case of using an oracle policy, it can be unclear how best to incorporate the oracle policy's experience into the learning policy in a way that maximizes learning sample efficiency. In this paper, we propose a method termed Critic Confidence Guided Exploration (CCGE) for incorporating such an oracle policy into standard actor-critic reinforcement learning algorithms. More specifically, CCGE takes in the oracle policy's actions as suggestions and incorporates this information into the learning scheme when uncertainty is high, while ignoring it when the uncertainty is low. CCGE is agnostic to methods of estimating uncertainty, and we show that it is equa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#39044;&#27979;&#32858;&#21512;&#27169;&#22411;&#65292;&#32771;&#34385;&#20102;&#36125;&#21494;&#26031;&#26041;&#27861;&#24212;&#29992;&#20110;&#19987;&#23478;&#21629;&#20013;&#21518;&#30340;&#20449;&#21495;&#27719;&#32858;&#12290;&#35770;&#25991;&#20013;&#25552;&#20379;&#20102;&#23545;&#20110;&#35813;&#38382;&#39064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#34920;&#26126;&#35813;&#22797;&#26434;&#24230;&#33267;&#23569;&#20026; $\tilde \Omega(m^{n-2} / \varepsilon)$&#65292;&#20854;&#20013; m &#26159;&#27599;&#20010;&#19987;&#23478;&#20449;&#21495;&#31354;&#38388;&#30340;&#22823;&#23567;&#12290;</title><link>http://arxiv.org/abs/2207.13126</link><description>&lt;p&gt;
&#39044;&#27979;&#32858;&#21512;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
Sample Complexity of Forecast Aggregation. (arXiv:2207.13126v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.13126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#39044;&#27979;&#32858;&#21512;&#27169;&#22411;&#65292;&#32771;&#34385;&#20102;&#36125;&#21494;&#26031;&#26041;&#27861;&#24212;&#29992;&#20110;&#19987;&#23478;&#21629;&#20013;&#21518;&#30340;&#20449;&#21495;&#27719;&#32858;&#12290;&#35770;&#25991;&#20013;&#25552;&#20379;&#20102;&#23545;&#20110;&#35813;&#38382;&#39064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#34920;&#26126;&#35813;&#22797;&#26434;&#24230;&#33267;&#23569;&#20026; $\tilde \Omega(m^{n-2} / \varepsilon)$&#65292;&#20854;&#20013; m &#26159;&#27599;&#20010;&#19987;&#23478;&#20449;&#21495;&#31354;&#38388;&#30340;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#39044;&#27979;&#32858;&#21512;&#27169;&#22411;&#65292;&#26377; n &#20010;&#19987;&#23478;&#26681;&#25454;&#26410;&#30693;&#20108;&#20803;&#20107;&#20214;&#30340;&#31169;&#26377;&#20449;&#21495;&#25253;&#21578;&#20107;&#20214;&#30340;&#21518;&#39564;&#20449;&#24565;&#32473;&#36127;&#36131;&#20154;&#65292;&#38543;&#21518;&#35813;&#36127;&#36131;&#20154;&#23558;&#25253;&#21578;&#32858;&#21512;&#20026;&#23545;&#35813;&#20107;&#20214;&#30340;&#21333;&#20010;&#39044;&#27979;&#12290;&#19987;&#23478;&#30340;&#20449;&#21495;&#21644;&#20107;&#20214;&#30340;&#32467;&#26524;&#26381;&#20174;&#19968;&#20010;&#32852;&#21512;&#20998;&#24067;&#65292;&#35813;&#20998;&#24067;&#23545;&#20110;&#36127;&#36131;&#20154;&#26159;&#26410;&#30693;&#30340;&#65292;&#20294;&#36127;&#36131;&#20154;&#21487;&#20197;&#20174;&#20998;&#24067;&#20013;&#24471;&#21040; i.i.d.&#8220;&#26679;&#26412;&#8221;&#65292;&#20854;&#20013;&#27599;&#20010;&#26679;&#26412;&#37117;&#26159;&#30001;&#19987;&#23478;&#30340;&#25253;&#21578;&#65288;&#19981;&#26159;&#20449;&#21495;&#65289;&#21644;&#20107;&#20214;&#30340;&#23454;&#29616;&#32452;&#25104;&#30340;&#20803;&#32452;&#12290;&#20351;&#29992;&#36825;&#20123;&#26679;&#26412;&#65292;&#36127;&#36131;&#20154;&#26088;&#22312;&#25214;&#21040;&#19968;&#20010; $\varepsilon$-&#26368;&#20248;&#32858;&#21512;&#22120;&#65292;&#20854;&#20013;&#26368;&#20248;&#24615;&#26159;&#20197;&#32858;&#21512;&#39044;&#27979;&#19982;&#20107;&#20214;&#23454;&#29616;&#20043;&#38388;&#30340;&#39044;&#26399;&#24179;&#26041;&#36317;&#31163;&#26469;&#34913;&#37327;&#30340;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#23545;&#20110;&#20219;&#24847;&#31163;&#25955;&#20998;&#24067;&#65292;&#36825;&#20010;&#38382;&#39064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#33267;&#23569;&#26159; $\tilde \Omega(m^{n-2} / \varepsilon)$&#65292;&#20854;&#20013; m &#26159;&#27599;&#20010;&#19987;&#23478;&#20449;&#21495;&#31354;&#38388;&#30340;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a Bayesian forecast aggregation model where $n$ experts, after observing private signals about an unknown binary event, report their posterior beliefs about the event to a principal, who then aggregates the reports into a single prediction for the event. The signals of the experts and the outcome of the event follow a joint distribution that is unknown to the principal, but the principal has access to i.i.d. "samples" from the distribution, where each sample is a tuple of the experts' reports (not signals) and the realization of the event. Using these samples, the principal aims to find an $\varepsilon$-approximately optimal aggregator, where optimality is measured in terms of the expected squared distance between the aggregated prediction and the realization of the event. We show that the sample complexity of this problem is at least $\tilde \Omega(m^{n-2} / \varepsilon)$ for arbitrary discrete distributions, where $m$ is the size of each expert's signal space. This sample
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#20302;&#21151;&#32791;TinyML&#31995;&#32479;&#65292;&#37319;&#29992;&#24494;&#22411;&#39592;&#24178;&#32593;&#32476;&#26500;&#24314;&#39640;&#25928;CNN&#27169;&#22411;&#65292;&#20877;&#30001;&#29305;&#21035;&#35774;&#35745;&#30340;&#31070;&#32463;&#21327;&#22788;&#29702;&#22120;&#19982;MCU&#30456;&#36830;&#65292;&#23558;&#25152;&#26377;&#29305;&#24449;&#21644;&#26435;&#37325;&#20648;&#23384;&#22312;&#33455;&#29255;&#19978;&#65292;&#23436;&#20840;&#28040;&#38500;&#33455;&#29255;&#22806;&#23384;&#20648;&#22120;&#35775;&#38382;&#30340;&#24310;&#36831;&#21644;&#21151;&#32791;&#12290;&#27492;&#31995;&#32479;&#20855;&#26377;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#24182;&#23454;&#29616;&#20102;&#21019;&#32426;&#24405;&#30340;160mW&#36229;&#20302;&#21151;&#32791;&#12290;</title><link>http://arxiv.org/abs/2207.04663</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#23454;&#26102;&#35270;&#35273;&#22788;&#29702;&#30340;&#36229;&#20302;&#21151;&#32791;TinyML&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
An Ultra-low Power TinyML System for Real-time Visual Processing at Edge. (arXiv:2207.04663v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.04663
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#20302;&#21151;&#32791;TinyML&#31995;&#32479;&#65292;&#37319;&#29992;&#24494;&#22411;&#39592;&#24178;&#32593;&#32476;&#26500;&#24314;&#39640;&#25928;CNN&#27169;&#22411;&#65292;&#20877;&#30001;&#29305;&#21035;&#35774;&#35745;&#30340;&#31070;&#32463;&#21327;&#22788;&#29702;&#22120;&#19982;MCU&#30456;&#36830;&#65292;&#23558;&#25152;&#26377;&#29305;&#24449;&#21644;&#26435;&#37325;&#20648;&#23384;&#22312;&#33455;&#29255;&#19978;&#65292;&#23436;&#20840;&#28040;&#38500;&#33455;&#29255;&#22806;&#23384;&#20648;&#22120;&#35775;&#38382;&#30340;&#24310;&#36831;&#21644;&#21151;&#32791;&#12290;&#27492;&#31995;&#32479;&#20855;&#26377;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#24182;&#23454;&#29616;&#20102;&#21019;&#32426;&#24405;&#30340;160mW&#36229;&#20302;&#21151;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
TinyML&#26159;&#22312;&#36164;&#28304;&#21644;&#21151;&#32791;&#20005;&#26684;&#38480;&#21046;&#30340;&#31995;&#32479;&#19978;&#25191;&#34892;AI&#24037;&#20316;&#36127;&#36733;&#30340;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#39046;&#22495;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#26497;&#20854;&#24494;&#23567;&#30340;&#39592;&#24178;&#32593;&#32476;&#65292;&#29992;&#20110;&#26500;&#24314;&#29992;&#20110;&#21508;&#31181;&#35270;&#35273;&#20219;&#21153;&#30340;&#39640;&#25928;CNN&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#19968;&#31181;&#29305;&#21035;&#35774;&#35745;&#30340;&#31070;&#32463;&#21327;&#22788;&#29702;&#22120;&#65288;NCP&#65289;&#19982;MCU&#30456;&#36830;&#65292;&#26500;&#24314;&#20102;&#19968;&#31181;&#36229;&#20302;&#21151;&#32791;&#30340;TinyML&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#23558;&#25152;&#26377;&#29305;&#24449;&#21644;&#26435;&#37325;&#23384;&#20648;&#22312;&#33455;&#29255;&#19978;&#65292;&#24182;&#23436;&#20840;&#28040;&#38500;&#20102;&#33455;&#29255;&#22806;&#23384;&#20648;&#22120;&#35775;&#38382;&#30340;&#24310;&#36831;&#21644;&#21151;&#32791;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#24212;&#29992;&#29305;&#23450;&#30340;&#25351;&#20196;&#38598;&#65292;&#20197;&#23454;&#29616;&#25935;&#25463;&#24320;&#21457;&#21644;&#24555;&#36895;&#37096;&#32626;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22522;&#20110;&#25105;&#20204;&#30340;&#27169;&#22411;&#12289;NCP&#21644;&#25351;&#20196;&#38598;&#30340;TinyML&#31995;&#32479;&#22312;&#23454;&#29616;30FPS&#30340;&#30446;&#26631;&#26816;&#27979;&#21644;&#35782;&#21035;&#26102;&#20855;&#26377;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#21019;&#32426;&#24405;&#30340;160mW&#36229;&#20302;&#21151;&#32791;&#12290;&#28436;&#31034;&#35270;&#39057;&#22312;\url{https://www.youtube.com/watch?v=mIZPxtJ-9EY}&#19978;&#21487;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tiny machine learning (TinyML), executing AI workloads on resource and power strictly restricted systems, is an important and challenging topic. This brief firstly presents an extremely tiny backbone to construct high efficiency CNN models for various visual tasks. Then, a specially designed neural co-processor (NCP) is interconnected with MCU to build an ultra-low power TinyML system, which stores all features and weights on chip and completely removes both of latency and power consumption in off-chip memory access. Furthermore, an application specific instruction-set is further presented for realizing agile development and rapid deployment. Extensive experiments demonstrate that the proposed TinyML system based on our model, NCP and instruction set yields considerable accuracy and achieves a record ultra-low power of 160mW while implementing object detection and recognition at 30FPS. The demo video is available on \url{https://www.youtube.com/watch?v=mIZPxtJ-9EY}.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#35777;&#26126;&#20102;&#25968;&#25454;&#19981;&#24179;&#34913;&#23545;&#23398;&#20064;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#35828;&#26126;&#22312;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#26102;&#65292;&#23569;&#25968;&#21644;&#22810;&#25968;&#31867;&#30340;&#23398;&#20064;&#26354;&#32447;&#20250;&#36981;&#24490;&#27425;&#20248;&#36712;&#36857;&#65292;&#21516;&#26102;&#25552;&#20986;&#23545;&#27599;&#31181;&#31867;&#21035;&#26799;&#24230;&#20570;&#20986;&#36129;&#29486;&#30340;&#24402;&#19968;&#21270;&#21464;&#20307;&#65292;&#20197;&#35299;&#20915;&#20248;&#21270;&#19981;&#21516;&#31867;&#21035;&#20043;&#38388;&#30340;&#31454;&#20105;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2207.00391</link><description>&lt;p&gt;
&#31867;&#21035;&#19981;&#24179;&#34913;&#19979;&#30340;&#23398;&#20064;&#21160;&#24577;&#29702;&#35770;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Theoretical Analysis of the Learning Dynamics under Class Imbalance. (arXiv:2207.00391v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.00391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#35777;&#26126;&#20102;&#25968;&#25454;&#19981;&#24179;&#34913;&#23545;&#23398;&#20064;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#35828;&#26126;&#22312;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#26102;&#65292;&#23569;&#25968;&#21644;&#22810;&#25968;&#31867;&#30340;&#23398;&#20064;&#26354;&#32447;&#20250;&#36981;&#24490;&#27425;&#20248;&#36712;&#36857;&#65292;&#21516;&#26102;&#25552;&#20986;&#23545;&#27599;&#31181;&#31867;&#21035;&#26799;&#24230;&#20570;&#20986;&#36129;&#29486;&#30340;&#24402;&#19968;&#21270;&#21464;&#20307;&#65292;&#20197;&#35299;&#20915;&#20248;&#21270;&#19981;&#21516;&#31867;&#21035;&#20043;&#38388;&#30340;&#31454;&#20105;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#19981;&#24179;&#34913;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#24120;&#35265;&#30340;&#38382;&#39064;&#65292;&#20250;&#20005;&#37325;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#12290;&#34429;&#28982;&#26377;&#21508;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#23427;&#20204;&#23545;&#23398;&#20064;&#21160;&#24577;&#30340;&#25910;&#25947;&#24433;&#21709;&#23578;&#26410;&#34987;&#29702;&#35299;&#12290;&#26412;&#25991;&#38416;&#26126;&#20102;&#25968;&#25454;&#19981;&#24179;&#34913;&#23545;&#23398;&#20064;&#30340;&#26174;&#33879;&#36127;&#38754;&#24433;&#21709;&#65292;&#24403;&#20351;&#29992;&#26799;&#24230;&#20248;&#21270;&#22120;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#23569;&#25968;&#31867;&#21644;&#22810;&#25968;&#31867;&#30340;&#23398;&#20064;&#26354;&#32447;&#20250;&#36981;&#24490;&#27425;&#20248;&#36712;&#36857;&#12290;&#36825;&#31181;&#25918;&#32531;&#19982;&#19981;&#24179;&#34913;&#27604;&#30456;&#20851;&#65292;&#21487;&#20197;&#36861;&#28335;&#21040;&#20248;&#21270;&#19981;&#21516;&#31867;&#21035;&#20043;&#38388;&#30340;&#31454;&#20105;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#20998;&#26512;&#20102;&#20840;&#25209;&#27425;&#65288;GD&#65289;&#21644;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#25910;&#25947;&#21644;&#21508;&#31181;&#23545;&#27599;&#31181;&#31867;&#21035;&#26799;&#24230;&#20570;&#20986;&#36129;&#29486;&#30340;&#24402;&#19968;&#21270;&#21464;&#20307;&#12290;&#25105;&#20204;&#21457;&#29616;GD&#19981;&#33021;&#20445;&#35777;&#38477;&#20302;&#27599;&#20010;&#31867;&#21035;&#30340;&#25439;&#22833;&#65292;&#20294;&#21487;&#20197;&#36890;&#36807;&#25191;&#34892;&#21508;&#33258;&#24402;&#19968;&#21270;&#26799;&#24230;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20351;&#29992;SGD&#26102;, &#31867;&#21035;&#19981;&#24179;&#34913;&#20250;&#23545;&#31639;&#27861;&#20135;&#29983;&#39069;&#22806;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data imbalance is a common problem in machine learning that can have a critical effect on the performance of a model. Various solutions exist but their impact on the convergence of the learning dynamics is not understood. Here, we elucidate the significant negative impact of data imbalance on learning, showing that the learning curves for minority and majority classes follow sub-optimal trajectories when training with a gradient-based optimizer. This slowdown is related to the imbalance ratio and can be traced back to a competition between the optimization of different classes. Our main contribution is the analysis of the convergence of full-batch (GD) and stochastic gradient descent (SGD), and of variants that renormalize the contribution of each per-class gradient. We find that GD is not guaranteed to decrease the loss for each class but that this problem can be addressed by performing a per-class normalization of the gradient. With SGD, class imbalance has an additional effect on th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#26041;&#27861;&#39564;&#35777;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#31283;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#28151;&#21512;&#25972;&#25968;&#20108;&#27425;&#35268;&#21010;&#26469;&#26816;&#26597;&#31574;&#30053;&#30340;&#31283;&#23450;&#24615;&#12290;&#27492;&#26041;&#27861;&#36890;&#29992;&#65292;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#25511;&#21046;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2206.13374</link><description>&lt;p&gt;
&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#39564;&#35777;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#30340;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Stability Verification of Neural Network Controllers using Mixed-Integer Programming. (arXiv:2206.13374v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.13374
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#26041;&#27861;&#39564;&#35777;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#31283;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#28151;&#21512;&#25972;&#25968;&#20108;&#27425;&#35268;&#21010;&#26469;&#26816;&#26597;&#31574;&#30053;&#30340;&#31283;&#23450;&#24615;&#12290;&#27492;&#26041;&#27861;&#36890;&#29992;&#65292;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#25511;&#21046;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#65288;MILP&#65289;&#21487;&#34920;&#31034;&#25511;&#21046;&#31574;&#30053;&#30340;&#31283;&#23450;&#24615;&#39564;&#35777;&#12290;&#35813;&#26694;&#26550;&#23558;&#19968;&#20010;&#22266;&#23450;&#30340;&#20505;&#36873;&#31574;&#30053;&#19982;&#19968;&#20010;&#24050;&#30693;&#31283;&#23450;&#20294;&#35745;&#31639;&#25104;&#26412;&#39640;&#30340;&#22266;&#23450;&#22522;&#20934;&#31574;&#30053;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#25552;&#20379;&#20102;&#26377;&#20851;&#20505;&#36873;&#31574;&#30053;&#30340;&#38381;&#29615;&#31283;&#23450;&#24615;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#36825;&#20123;&#26465;&#20214;&#26159;&#22522;&#20110;&#19982;&#22522;&#20934;&#31574;&#30053;&#30340;&#26368;&#22351;&#36817;&#20284;&#35823;&#24046;&#20851;&#31995;&#30340;&#65292;&#24182;&#19988;&#36825;&#20123;&#26465;&#20214;&#21487;&#20197;&#36890;&#36807;&#35299;&#20915;&#28151;&#21512;&#25972;&#25968;&#20108;&#27425;&#35268;&#21010;&#65288;MIQP&#65289;&#26469;&#26816;&#26597;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;MIPL&#21487;&#20197;&#35745;&#31639;&#20505;&#36873;&#31574;&#30053;&#31283;&#23450;&#21306;&#22495;&#30340;&#22806;&#37096;&#21644;&#20869;&#37096;&#36924;&#36817;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#36275;&#22815;&#36890;&#29992;&#65292;&#21487;&#36866;&#24212;&#24191;&#27867;&#30340;&#20505;&#36873;&#31574;&#30053;&#65292;&#21253;&#25324;ReLU&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#12289;&#21442;&#25968;&#20108;&#27425;&#35268;&#21010;&#30340;&#26368;&#20248;&#35299;&#26144;&#23556;&#21644;&#24102;&#26377;&#28151;&#21512;&#25972;&#25968;&#32422;&#26463;&#30340;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;MPC&#65289;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a framework for the stability verification of Mixed-Integer Linear Programming (MILP) representable control policies. This framework compares a fixed candidate policy, which admits an efficient parameterization and can be evaluated at a low computational cost, against a fixed baseline policy, which is known to be stable but expensive to evaluate. We provide sufficient conditions for the closed-loop stability of the candidate policy in terms of the worst-case approximation error with respect to the baseline policy, and we show that these conditions can be checked by solving a Mixed-Integer Quadratic Program (MIQP). Additionally, we demonstrate that an outer and inner approximation of the stability region of the candidate policy can be computed by solving an MILP. The proposed framework is sufficiently general to accommodate a broad range of candidate policies including ReLU Neural Networks (NNs), optimal solution maps of parametric quadratic programs, and Model Predictive Con
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26410;&#30693;&#21021;&#22987;&#20301;&#32622;&#21644;&#36895;&#24230;&#19979;&#30340;&#31227;&#21160;&#30446;&#26631;&#30340;&#38750;&#33258;&#36866;&#24212;&#25628;&#32034;&#31574;&#30053;&#65292;&#36890;&#36807;&#26368;&#23569;&#27425;&#25968;&#30340;&#26597;&#35810;&#39044;&#35328;&#26426;&#65292;&#20934;&#30830;&#20272;&#35745;&#30446;&#26631;&#22312;&#20219;&#20309;&#25351;&#23450;&#26102;&#38388;&#30340;&#20301;&#32622;&#30340;&#20998;&#36776;&#29575;&#26497;&#38480;&#12290;</title><link>http://arxiv.org/abs/2206.08884</link><description>&lt;p&gt;
&#38750;&#33258;&#36866;&#24212;20&#38382;&#39064;&#25628;&#32034;&#31227;&#21160;&#30446;&#26631;&#30340;&#20998;&#36776;&#29575;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
Resolution Limits of Non-Adaptive 20 Questions Search for a Moving Target. (arXiv:2206.08884v2 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.08884
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26410;&#30693;&#21021;&#22987;&#20301;&#32622;&#21644;&#36895;&#24230;&#19979;&#30340;&#31227;&#21160;&#30446;&#26631;&#30340;&#38750;&#33258;&#36866;&#24212;&#25628;&#32034;&#31574;&#30053;&#65292;&#36890;&#36807;&#26368;&#23569;&#27425;&#25968;&#30340;&#26597;&#35810;&#39044;&#35328;&#26426;&#65292;&#20934;&#30830;&#20272;&#35745;&#30446;&#26631;&#22312;&#20219;&#20309;&#25351;&#23450;&#26102;&#38388;&#30340;&#20301;&#32622;&#30340;&#20998;&#36776;&#29575;&#26497;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#24102;&#26377;&#26597;&#35810;&#20381;&#36182;&#22122;&#22768;&#30340;20&#20010;&#38382;&#39064;&#20272;&#35745;&#26694;&#26550;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#31227;&#21160;&#30446;&#26631;&#22312;&#26410;&#30693;&#21021;&#22987;&#20301;&#32622;&#21644;&#36895;&#24230;&#19979;&#30340;&#38750;&#33258;&#36866;&#24212;&#25628;&#32034;&#31574;&#30053;&#65292;&#24182;&#37319;&#29992;&#20998;&#27573;&#24120;&#36895;&#24230;&#27169;&#22411;&#22312;&#21333;&#20301;&#31435;&#26041;&#20307;&#19978;&#36827;&#34892;&#12290;&#22312;&#36825;&#20010;&#25628;&#32034;&#38382;&#39064;&#20013;&#65292;&#26377;&#19968;&#20010;&#30693;&#36947;&#20219;&#20309;&#26102;&#20505;&#30446;&#26631;&#30636;&#26102;&#20301;&#32622;&#30340;&#39044;&#35328;&#26426;&#12290;&#25105;&#20204;&#30340;&#20219;&#21153;&#26159;&#23613;&#21487;&#33021;&#23569;&#22320;&#26597;&#35810;&#39044;&#35328;&#26426;&#65292;&#20197;&#20934;&#30830;&#22320;&#20272;&#35745;&#30446;&#26631;&#22312;&#20219;&#20309;&#25351;&#23450;&#26102;&#38388;&#30340;&#20301;&#32622;&#12290;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#39044;&#35328;&#26426;&#23545;&#27599;&#20010;&#26597;&#35810;&#30340;&#22238;&#31572;&#37117;&#21463;&#21040;&#31163;&#25955;&#22122;&#22768;&#27745;&#26579;&#30340;&#24773;&#20917;&#65292;&#28982;&#21518;&#23558;&#25105;&#20204;&#30340;&#32467;&#26524;&#25512;&#24191;&#21040;&#21152;&#24615;&#39640;&#26031;&#22122;&#22768;&#30340;&#24773;&#20917;&#12290;&#22312;&#25105;&#20204;&#30340;&#20844;&#24335;&#20013;&#65292;&#24615;&#33021;&#20934;&#21017;&#26159;&#20998;&#36776;&#29575;&#65292;&#23427;&#23450;&#20041;&#20026;&#30495;&#23454;&#20301;&#32622;&#21644;&#20272;&#35745;&#20301;&#32622;&#20043;&#38388;&#30340;&#26368;&#22823;$L_\infty$&#36317;&#31163;&#12290;&#25105;&#20204;&#36890;&#36807;&#23548;&#20986;&#38750;&#28176;&#36817;&#21644;&#28176;&#36817;&#30028;&#26469;&#25551;&#36848;&#20855;&#26377;&#26377;&#38480;&#26597;&#35810;&#27425;&#25968;&#30340;&#26368;&#20339;&#38750;&#33258;&#36866;&#24212;&#26597;&#35810;&#36807;&#31243;&#30340;&#26368;&#23567;&#20998;&#36776;&#29575;&#12290;&#25105;&#20204;&#30340;&#30028;&#38480;&#26159;&#20005;&#26684;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Using the 20 questions estimation framework with query-dependent noise, we study non-adaptive search strategies for a moving target over the unit cube with unknown initial location and velocities under a piecewise constant velocity model. In this search problem, there is an oracle who knows the instantaneous location of the target at any time. Our task is to query the oracle as few times as possible to accurately estimate the location of the target at any specified time. We first study the case where the oracle's answer to each query is corrupted by discrete noise and then generalize our results to the case of additive white Gaussian noise. In our formulation, the performance criterion is the resolution, which is defined as the maximal $L_\infty$ distance between the true locations and estimated locations. We characterize the minimal resolution of an optimal non-adaptive query procedure with a finite number of queries by deriving non-asymptotic and asymptotic bounds. Our bounds are tig
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65292;&#32771;&#34385;&#28151;&#21512;&#36807;&#31243;&#30340;&#20551;&#35774;&#65292;&#21363;&#32467;&#26500;&#31232;&#30095;&#24615;&#65292;&#26469;&#23454;&#29616;&#38750;&#32447;&#24615;ICA&#30340;&#21487;&#35782;&#21035;&#24615;&#65292;&#26080;&#38656;&#36741;&#21161;&#21464;&#37327;&#12290;</title><link>http://arxiv.org/abs/2206.07751</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#29420;&#31435;&#20998;&#37327;&#20998;&#26512;&#30340;&#21487;&#36776;&#35782;&#24615;&#65306;&#31232;&#30095;&#24615;&#21450;&#20854;&#23427;
&lt;/p&gt;
&lt;p&gt;
On the Identifiability of Nonlinear ICA: Sparsity and Beyond. (arXiv:2206.07751v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.07751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65292;&#32771;&#34385;&#28151;&#21512;&#36807;&#31243;&#30340;&#20551;&#35774;&#65292;&#21363;&#32467;&#26500;&#31232;&#30095;&#24615;&#65292;&#26469;&#23454;&#29616;&#38750;&#32447;&#24615;ICA&#30340;&#21487;&#35782;&#21035;&#24615;&#65292;&#26080;&#38656;&#36741;&#21161;&#21464;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#32447;&#24615;&#29420;&#31435;&#20998;&#37327;&#20998;&#26512;&#26088;&#22312;&#20174;&#20854;&#21487;&#35266;&#27979;&#30340;&#38750;&#32447;&#24615;&#28151;&#21512;&#20013;&#24674;&#22797;&#20986;&#28508;&#22312;&#29420;&#31435;&#20998;&#37327;&#12290;&#22914;&#20309;&#20351;&#38750;&#32447;&#24615;ICA&#27169;&#22411;&#21487;&#36776;&#35782;&#30452;&#21040;&#26576;&#20123;&#24179;&#20961;&#19981;&#30830;&#23450;&#24615;&#26159;&#26080;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#38271;&#26399;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#31361;&#30772;&#26159;&#23558;&#28304;&#30340;&#26631;&#20934;&#29420;&#31435;&#24615;&#20551;&#35774;&#37325;&#26032;&#23450;&#20041;&#20026;&#22312;&#26576;&#20123;&#36741;&#21161;&#21464;&#37327;&#65288;&#20363;&#22914;&#31867;&#26631;&#31614;&#21644;/&#25110;&#22495;/&#26102;&#38388;&#32034;&#24341;&#65289;&#32473;&#23450;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#65292;&#20316;&#20026;&#24369;&#30417;&#30563;&#25110;&#24402;&#32435;&#20559;&#32622;&#12290;&#28982;&#32780;&#65292;&#20855;&#26377;&#26080;&#26465;&#20214;&#20808;&#39564;&#30340;&#38750;&#32447;&#24615;ICA&#26080;&#27861;&#20174;&#36825;&#20123;&#21457;&#23637;&#20013;&#21463;&#30410;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#26465;&#26367;&#20195;&#36335;&#24452;&#65292;&#24182;&#20165;&#32771;&#34385;&#28151;&#21512;&#36807;&#31243;&#30340;&#20551;&#35774;&#65292;&#20363;&#22914;&#32467;&#26500;&#31232;&#30095;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#36825;&#20123;&#32422;&#26463;&#30340;&#20855;&#20307;&#23454;&#20363;&#19979;&#65292;&#29420;&#31435;&#30340;&#28508;&#22312;&#20998;&#37327;&#21487;&#20197;&#20174;&#20854;&#38750;&#32447;&#24615;&#28151;&#21512;&#20013;&#36776;&#35782;&#20986;&#26469;&#65292;&#36798;&#21040;&#38750;&#24179;&#20961;&#30340;&#38750;&#32447;&#24615;ICA&#21487;&#35782;&#21035;&#24615;&#65292;&#32780;&#26080;&#38656;&#36741;&#21161;&#21464;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nonlinear independent component analysis (ICA) aims to recover the underlying independent latent sources from their observable nonlinear mixtures. How to make the nonlinear ICA model identifiable up to certain trivial indeterminacies is a long-standing problem in unsupervised learning. Recent breakthroughs reformulate the standard independence assumption of sources as conditional independence given some auxiliary variables (e.g., class labels and/or domain/time indexes) as weak supervision or inductive bias. However, nonlinear ICA with unconditional priors cannot benefit from such developments. We explore an alternative path and consider only assumptions on the mixing process, such as Structural Sparsity. We show that under specific instantiations of such constraints, the independent latent sources can be identified from their nonlinear mixtures up to a permutation and a component-wise transformation, thus achieving nontrivial identifiability of nonlinear ICA without auxiliary variable
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#26500;&#24863;&#30693;&#30340;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;AST&#21644;DFG&#36741;&#21161;&#20219;&#21153;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#22312;&#38754;&#23545;&#20195;&#30721;&#35821;&#27861;&#21644;&#35821;&#20041;&#26102;&#30340;&#35757;&#32451;&#19981;&#36275;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2206.05239</link><description>&lt;p&gt;
StructCoder: &#38754;&#21521;&#20195;&#30721;&#29983;&#25104;&#30340;&#32467;&#26500;&#24863;&#30693;Transformer&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
StructCoder: Structure-Aware Transformer for Code Generation. (arXiv:2206.05239v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.05239
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#26500;&#24863;&#30693;&#30340;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;AST&#21644;DFG&#36741;&#21161;&#20219;&#21153;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#22312;&#38754;&#23545;&#20195;&#30721;&#35821;&#27861;&#21644;&#35821;&#20041;&#26102;&#30340;&#35757;&#32451;&#19981;&#36275;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26469;&#33258;&#21160;&#21270;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#30340;&#20852;&#36259;&#26085;&#30410;&#22686;&#38271;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#20195;&#30721;&#29983;&#25104;&#38382;&#39064;&#65292;&#30446;&#26631;&#22312;&#20110;&#22312;&#32473;&#23450;&#19981;&#21516;&#35821;&#35328;&#25110;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#28304;&#20195;&#30721;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#30446;&#26631;&#20195;&#30721;&#12290;&#38024;&#23545;&#20195;&#30721;&#35821;&#27861;&#21644;&#35821;&#20041;&#30340;&#20005;&#26684;&#29702;&#35299;&#21644;&#29983;&#25104;&#38656;&#35201;&#19968;&#31181;&#26356;&#20026;&#20005;&#35880;&#30340;&#35757;&#32451;&#31574;&#30053;&#12290;&#20986;&#20110;&#36825;&#20010;&#21160;&#26426;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;Transformer&#27169;&#22411;&#65292;&#22312;&#27492;&#27169;&#22411;&#20013;&#65292;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#37117;&#26126;&#30830;&#22320;&#21463;&#36807;&#35757;&#32451;&#65292;&#20197;&#35782;&#21035;&#28304;&#20195;&#30721;&#21644;&#30446;&#26631;&#20195;&#30721;&#30340;&#35821;&#27861;&#21644;&#25968;&#25454;&#27969;&#12290;&#25105;&#20204;&#19981;&#20165;&#36890;&#36807;&#21033;&#29992;&#28304;&#20195;&#30721;&#30340;&#35821;&#27861;&#26641;&#21644;&#25968;&#25454;&#27969;&#22270;&#20351;&#32534;&#30721;&#22120;&#32467;&#26500;&#24863;&#30693;&#65292;&#36824;&#36890;&#36807;&#24341;&#20837;&#20004;&#20010;&#26032;&#30340;&#36741;&#21161;&#20219;&#21153;&#8212;&#8212;AST&#65288;&#25277;&#35937;&#35821;&#27861;&#65289;&#21644;DFG&#65288;&#25968;&#25454;&#27969;&#22270;&#65289;&#24110;&#21161;&#35299;&#30721;&#22120;&#20445;&#30041;&#30446;&#26631;&#20195;&#30721;&#30340;&#35821;&#27861;&#21644;&#25968;&#25454;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been a recent surge of interest in automating software engineering tasks using deep learning. This paper addresses the problem of code generation where the goal is to generate target code given source code in a different language or a natural language description. Most of the state-of-the-art deep learning models for code generation use training strategies primarily designed for natural language. However, understanding and generating code requires a more rigorous comprehension of the code syntax and semantics. With this motivation, we develop an encoder-decoder Transformer model where both the encoder and decoder are explicitly trained to recognize the syntax and data flow in the source and target codes, respectively. We not only make the encoder structure-aware by leveraging the source code's syntax tree and data flow graph, but we also support the decoder in preserving the syntax and data flow of the target code by introducing two novel auxiliary tasks: AST (Abstract Syntax
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#35757;&#32451;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#30340;&#25968;&#25454;&#19978;&#26045;&#21152;&#26356;&#22810;&#27491;&#21017;&#21270;&#20197;&#25552;&#39640;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#65292;&#24471;&#21040;&#20102;&#22312;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#22343;&#20026;&#26368;&#20248;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2206.03353</link><description>&lt;p&gt;
&#22312;&#19981;&#31283;&#20581;&#26679;&#26412;&#19978;&#26045;&#21152;&#26356;&#22810;&#27491;&#21017;&#21270;&#20197;&#25552;&#39640;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving adversarial robustness by putting more regularizations on less robust samples. (arXiv:2206.03353v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.03353
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#35757;&#32451;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#30340;&#25968;&#25454;&#19978;&#26045;&#21152;&#26356;&#22810;&#27491;&#21017;&#21270;&#20197;&#25552;&#39640;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#65292;&#24471;&#21040;&#20102;&#22312;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#22343;&#20026;&#26368;&#20248;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#35757;&#32451;&#26159;&#25552;&#39640;&#23545;&#25239;&#25915;&#20987;&#40065;&#26834;&#24615;&#30340;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#20154;&#31867;&#35270;&#35273;&#26080;&#27861;&#23519;&#35273;&#30340;&#25968;&#25454;&#25200;&#21160;&#19979;&#65292;&#20351;&#32473;&#23450;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20135;&#29983;&#35823;&#21028;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#35757;&#32451;&#31639;&#27861;&#65292;&#23427;&#22312;&#29702;&#35770;&#19978;&#24471;&#21040;&#24456;&#22909;&#30340;&#35777;&#26126;&#65292;&#24182;&#19988;&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#29616;&#26377;&#30340;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#30340;&#19968;&#20010;&#26032;&#30340;&#29305;&#28857;&#26159;&#65306;&#23545;&#20110;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#30340;&#25968;&#25454;&#65292;&#27604;&#20854;&#20182;&#29616;&#26377;&#30340;&#27491;&#21017;&#21270;&#31639;&#27861;&#26356;&#22810;&#22320;&#24212;&#29992;&#27491;&#21017;&#21270;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#34987;&#29702;&#35299;&#20026;&#19968;&#20010;&#26368;&#23567;&#21270;&#32463;&#39564;&#39118;&#38505;&#30340;&#27491;&#21017;&#21270;&#31639;&#27861;&#65292;&#23427;&#26469;&#33258;&#19968;&#20010;&#26032;&#30340;&#40065;&#26834;&#39118;&#38505;&#19978;&#30028;&#30340;&#21160;&#26426;&#12290;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#21516;&#26102;&#25552;&#39640;&#20102;&#27867;&#21270;&#24615;&#33021;(&#22312;&#20363;&#23376;&#19978;&#30340;&#20934;&#30830;&#24615;)&#21644;&#40065;&#26834;&#24615;(&#22312;&#23545;&#25239;&#25915;&#20987;&#19978;&#30340;&#20934;&#30830;&#24615;)&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial training, which is to enhance robustness against adversarial attacks, has received much attention because it is easy to generate human-imperceptible perturbations of data to deceive a given deep neural network. In this paper, we propose a new adversarial training algorithm that is theoretically well motivated and empirically superior to other existing algorithms. A novel feature of the proposed algorithm is to apply more regularization to data vulnerable to adversarial attacks than other existing regularization algorithms do. Theoretically, we show that our algorithm can be understood as an algorithm of minimizing the regularized empirical risk motivated from a newly derived upper bound of the robust risk. Numerical experiments illustrate that our proposed algorithm improves the generalization (accuracy on examples) and robustness (accuracy on adversarial attacks) simultaneously to achieve the state-of-the-art performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21333;&#26426;&#20316;&#19994;&#35843;&#24230;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#39044;&#27979;&#30340;&#38745;&#24577;&#35843;&#24230;&#31639;&#27861;&#65292;&#22312;&#31867;&#22411;&#26410;&#30693;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#27425;&#32447;&#24615;&#30340;&#36807;&#21097;&#25104;&#26412;&#65292;&#23588;&#20854;&#22312;&#25250;&#21344;&#24335;&#38382;&#39064;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#20316;&#19994;&#31867;&#22411;&#25345;&#32493;&#26102;&#38388;&#30456;&#24046;&#24456;&#22823;&#26102;&#20248;&#20110;&#38750;&#25250;&#21344;&#21305;&#37197;&#12290;</title><link>http://arxiv.org/abs/2205.15695</link><description>&lt;p&gt;
&#36890;&#36807;&#39640;&#25928;&#25506;&#32034;&#23398;&#20064;&#39044;&#27979;&#30340;&#38745;&#24577;&#35843;&#24230;
&lt;/p&gt;
&lt;p&gt;
Static Scheduling with Predictions Learned through Efficient Exploration. (arXiv:2205.15695v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.15695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21333;&#26426;&#20316;&#19994;&#35843;&#24230;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#39044;&#27979;&#30340;&#38745;&#24577;&#35843;&#24230;&#31639;&#27861;&#65292;&#22312;&#31867;&#22411;&#26410;&#30693;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#27425;&#32447;&#24615;&#30340;&#36807;&#21097;&#25104;&#26412;&#65292;&#23588;&#20854;&#22312;&#25250;&#21344;&#24335;&#38382;&#39064;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#20316;&#19994;&#31867;&#22411;&#25345;&#32493;&#26102;&#38388;&#30456;&#24046;&#24456;&#22823;&#26102;&#20248;&#20110;&#38750;&#25250;&#21344;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#21333;&#26426;&#20316;&#19994;&#35843;&#24230;&#65292;&#27599;&#20010;&#20316;&#19994;&#37117;&#23646;&#20110;&#20915;&#23450;&#20854;&#25345;&#32493;&#26102;&#38388;&#20998;&#24067;&#30340;&#20316;&#19994;&#31867;&#22411;&#12290;&#25105;&#20204;&#39318;&#20808;&#20998;&#26512;&#20102;&#31867;&#22411;&#29305;&#24449;&#24050;&#30693;&#30340;&#24773;&#20917;&#65292;&#28982;&#21518;&#36716;&#21521;&#20004;&#31181;&#23398;&#20064;&#24773;&#26223;&#65292;&#20854;&#20013;&#31867;&#22411;&#26410;&#30693;&#65306;&#38750;&#25250;&#21344;&#24335;&#38382;&#39064;&#65292;&#23427;&#35201;&#27714;&#23436;&#25104;&#24050;&#21551;&#21160;&#30340;&#20316;&#19994;&#65292;&#28982;&#21518;&#25165;&#33021;&#31227;&#21160;&#21040;&#21478;&#19968;&#20010;&#20316;&#19994;&#65307;&#21644;&#25250;&#21344;&#24335;&#38382;&#39064;&#65292;&#36825;&#37324;&#20316;&#19994;&#25191;&#34892;&#21487;&#20197;&#26242;&#20572;&#20197;&#20248;&#20808;&#36716;&#31227;&#21040;&#21478;&#19968;&#20010;&#20316;&#19994;&#12290;&#22312;&#20004;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35774;&#35745;&#30340;&#31639;&#27861;&#30456;&#23545;&#20110;&#24050;&#30693;&#31867;&#22411;&#30340;&#24615;&#33021;&#23454;&#29616;&#20102;&#27425;&#32447;&#24615;&#30340;&#36807;&#21097;&#25104;&#26412;&#65292;&#24182;&#35777;&#26126;&#20102;&#38750;&#25250;&#21344;&#24335;&#24773;&#20917;&#30340;&#19979;&#38480;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25250;&#21344;&#31639;&#27861;&#22312;&#19981;&#21516;&#20316;&#19994;&#31867;&#22411;&#25345;&#32493;&#26102;&#38388;&#30456;&#24046;&#24456;&#22823;&#26102;&#65292;&#29702;&#35770;&#19978;&#21644;&#36890;&#36807;&#27169;&#25311;&#30340;&#26041;&#24335;&#21487;&#20197;&#20248;&#20110;&#38750;&#25250;&#21344;&#21305;&#37197;&#65292;&#22312;&#31867;&#22411;&#25345;&#32493;&#26102;&#38388;&#24050;&#30693;&#26102;&#24182;&#19981;&#23384;&#22312;&#36825;&#31181;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study single-machine scheduling of jobs, each belonging to a job type that determines its duration distribution. We start by analyzing the scenario where the type characteristics are known and then move to two learning scenarios where the types are unknown: non-preemptive problems, where each started job must be completed before moving to another job; and preemptive problems, where job execution can be paused in the favor of moving to a different job. In both cases, we design algorithms that achieve sublinear excess cost, compared to the performance with known types, and prove lower bounds for the non-preemptive case. Notably, we demonstrate, both theoretically and through simulations, how preemptive algorithms can greatly outperform non-preemptive ones when the durations of different job types are far from one another, a phenomenon that does not occur when the type durations are known.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#38656;&#35201;&#30340;&#21333;&#35843;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#23618;&#20013;&#30340;&#19968;&#37096;&#20998;&#31070;&#32463;&#20803;&#20013;&#37319;&#29992;&#21407;&#22987;&#28608;&#27963;&#20989;&#25968;&#65292;&#21516;&#26102;&#22312;&#21478;&#19968;&#37096;&#20998;&#37319;&#29992;&#20854;&#28857;&#23545;&#31216;&#21453;&#23556;&#26469;&#35299;&#20915;&#26500;&#24314;&#21333;&#35843;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#30340;&#31934;&#24230;&#31526;&#21512;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2205.11775</link><description>&lt;p&gt;
&#21463;&#38480;&#21333;&#35843;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Constrained Monotonic Neural Networks. (arXiv:2205.11775v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.11775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#38656;&#35201;&#30340;&#21333;&#35843;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#23618;&#20013;&#30340;&#19968;&#37096;&#20998;&#31070;&#32463;&#20803;&#20013;&#37319;&#29992;&#21407;&#22987;&#28608;&#27963;&#20989;&#25968;&#65292;&#21516;&#26102;&#22312;&#21478;&#19968;&#37096;&#20998;&#37319;&#29992;&#20854;&#28857;&#23545;&#31216;&#21453;&#23556;&#26469;&#35299;&#20915;&#26500;&#24314;&#21333;&#35843;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#30340;&#31934;&#24230;&#31526;&#21512;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36234;&#26469;&#36234;&#27969;&#34892;&#65292;&#21487;&#20197;&#36924;&#36817;&#20174;&#22024;&#26434;&#25968;&#25454;&#20013;&#24471;&#20986;&#30340;&#20219;&#24847;&#20989;&#25968;&#65292;&#20294;&#22312;&#25512;&#24191;&#36807;&#31243;&#20013;&#38656;&#35201;&#35299;&#37322;&#36825;&#20123;&#27169;&#22411;&#24182;&#23545;&#23427;&#20204;&#26045;&#21152;&#39069;&#22806;&#30340;&#38480;&#21046;&#65292;&#20854;&#20013;&#21333;&#35843;&#24615;&#26159;&#26368;&#21463;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#38656;&#35201;&#30340;&#23646;&#24615;&#20043;&#19968;&#65292;&#24182;&#19988;&#26159;&#35813;&#35770;&#25991;&#30340;&#37325;&#28857;&#12290;&#26368;&#26089;&#26500;&#24314;&#21333;&#35843;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#26159;&#23558;&#20854;&#26435;&#37325;&#32422;&#26463;&#20026;&#38750;&#36127;&#65292;&#21516;&#26102;&#37319;&#29992;&#21333;&#35843;&#28608;&#27963;&#20989;&#25968;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#35813;&#26041;&#27861;&#26080;&#27861;&#19982;&#24120;&#29992;&#30340;&#38750;&#39281;&#21644;&#28608;&#27963;&#20989;&#25968;&#65288;&#22914;ReLU&#65292;ELU&#65292;SELU&#31561;&#65289;&#19968;&#36215;&#20351;&#29992;&#65292;&#22240;&#20026;&#23427;&#21482;&#33021;&#36924;&#36817;&#20984;&#20989;&#25968;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#23618;&#20013;&#30340;&#19968;&#37096;&#20998;&#31070;&#32463;&#20803;&#20013;&#37319;&#29992;&#21407;&#22987;&#28608;&#27963;&#20989;&#25968;&#65292;&#21516;&#26102;&#22312;&#21478;&#19968;&#37096;&#20998;&#37319;&#29992;&#20854;&#28857;&#23545;&#31216;&#21453;&#23556;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#37319;&#29992;&#36825;&#31181;&#26041;&#27861;&#24314;&#31435;&#21333;&#35843;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#31934;&#24230;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#65292;&#21516;&#26102;&#28385;&#36275;&#21333;&#35843;&#24615;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are becoming increasingly popular in approximating arbitrary functions from noisy data. But wider adoption is being hindered by the need to explain such models and to impose additional constraints on them. Monotonicity constraint is one of the most requested properties in real-world scenarios and is the focus of this paper. One of the oldest ways to construct a monotonic fully connected neural network is to constrain its weights to be non-negative while employing a monotonic activation function. Unfortunately, this construction does not work with popular non-saturated activation functions such as ReLU, ELU, SELU etc, as it can only approximate convex functions. We show this shortcoming can be fixed by employing the original activation function for a part of the neurons in the layer, and employing its point reflection for the other part. Our experiments show this approach of building monotonic deep neural networks have matching or better accuracy when compared to ot
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#28151;&#21512;&#21464;&#20998;&#27969;&#65288;MixFlows&#65289;&#65292;&#26159;&#30001;&#23545;&#21021;&#22987;&#21442;&#32771;&#20998;&#24067;&#30340;&#26144;&#23556;&#37325;&#22797;&#24212;&#29992;&#30340;&#28151;&#21512;&#32452;&#25104;&#30340;&#19968;&#31181;&#26032;&#30340;&#21464;&#20998;&#23478;&#26063;&#12290;MixFlows&#20855;&#26377;&#31867;&#20284;&#20110;MCMC&#30340;&#25910;&#25947;&#20445;&#35777;&#65292;&#24182;&#21487;&#20197;&#25552;&#20379;&#27604;&#20960;&#31181;&#40657;&#30418;&#24402;&#19968;&#21270;&#27969;&#26356;&#21487;&#38752;&#30340;&#21518;&#39564;&#36924;&#36817;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;MCMC&#26041;&#27861;&#25152;&#33719;&#24471;&#30340;&#26679;&#26412;&#36136;&#37327;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2205.07475</link><description>&lt;p&gt;
&#28151;&#21512;&#27969;&#65306;&#22522;&#20110;&#28151;&#21512;&#27969;&#30340;&#21407;&#21017;&#21464;&#20998;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
MixFlows: principled variational inference via mixed flows. (arXiv:2205.07475v5 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.07475
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#28151;&#21512;&#21464;&#20998;&#27969;&#65288;MixFlows&#65289;&#65292;&#26159;&#30001;&#23545;&#21021;&#22987;&#21442;&#32771;&#20998;&#24067;&#30340;&#26144;&#23556;&#37325;&#22797;&#24212;&#29992;&#30340;&#28151;&#21512;&#32452;&#25104;&#30340;&#19968;&#31181;&#26032;&#30340;&#21464;&#20998;&#23478;&#26063;&#12290;MixFlows&#20855;&#26377;&#31867;&#20284;&#20110;MCMC&#30340;&#25910;&#25947;&#20445;&#35777;&#65292;&#24182;&#21487;&#20197;&#25552;&#20379;&#27604;&#20960;&#31181;&#40657;&#30418;&#24402;&#19968;&#21270;&#27969;&#26356;&#21487;&#38752;&#30340;&#21518;&#39564;&#36924;&#36817;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;MCMC&#26041;&#27861;&#25152;&#33719;&#24471;&#30340;&#26679;&#26412;&#36136;&#37327;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#28151;&#21512;&#21464;&#20998;&#27969;&#65288;MixFlows&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#21464;&#20998;&#23478;&#26063;&#65292;&#30001;&#23545;&#21021;&#22987;&#21442;&#32771;&#20998;&#24067;&#30340;&#26144;&#23556;&#37325;&#22797;&#24212;&#29992;&#30340;&#28151;&#21512;&#32452;&#25104;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;i.i.d.&#37319;&#26679;&#12289;&#23494;&#24230;&#35780;&#20272;&#21644;&#26080;&#20559;ELBO&#20272;&#35745;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#27969;&#26144;&#23556;&#26159;&#36941;&#21382;&#21644;&#20445;&#24230;&#37327;&#30340;&#26102;&#65292;MixFlows&#20855;&#26377;&#31867;&#20284;&#20110;MCMC&#30340;&#25910;&#25947;&#20445;&#35777;&#65292;&#24182;&#20026;&#27969;&#26144;&#23556;&#36817;&#20284;&#23454;&#29616;&#25552;&#20379;&#20102;&#35823;&#24046;&#31215;&#32047;&#30340;&#30028;&#38480;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22522;&#20110;&#26410;&#32416;&#27491;&#30340;&#31163;&#25955;&#21704;&#23494;&#39039;&#21160;&#21147;&#23398;&#21644;&#30830;&#23450;&#24615;&#21160;&#37327;&#24674;&#22797;&#24320;&#21457;&#20102; MixFlows &#30340;&#23454;&#29616;&#12290;&#27169;&#25311;&#21644;&#23454;&#38469;&#25968;&#25454;&#23454;&#39564;&#34920;&#26126;&#65292;MixFlows&#21487;&#20197;&#25552;&#20379;&#27604;&#20960;&#31181;&#40657;&#30418;&#24402;&#19968;&#21270;&#27969;&#26356;&#21487;&#38752;&#30340;&#21518;&#39564;&#36924;&#36817;&#65292;&#20197;&#21450;&#19982;&#26368;&#20808;&#36827;&#30340;MCMC&#26041;&#27861;&#25152;&#33719;&#24471;&#30340;&#26679;&#26412;&#36136;&#37327;&#30456;&#24403;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work presents mixed variational flows (MixFlows), a new variational family that consists of a mixture of repeated applications of a map to an initial reference distribution. First, we provide efficient algorithms for i.i.d. sampling, density evaluation, and unbiased ELBO estimation. We then show that MixFlows have MCMC-like convergence guarantees when the flow map is ergodic and measure-preserving, and provide bounds on the accumulation of error for practical implementations where the flow map is approximated. Finally, we develop an implementation of MixFlows based on uncorrected discretized Hamiltonian dynamics combined with deterministic momentum refreshment. Simulated and real data experiments show that MixFlows can provide more reliable posterior approximations than several black-box normalizing flows, as well as samples of comparable quality to those obtained from state-of-the-art MCMC methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#23618;&#27425;&#30340;&#31283;&#20581;&#24615;&#24459;&#27861;&#65292;&#30740;&#31350;&#20102;&#22312;&#20219;&#24847;&#25968;&#25454;&#20998;&#24067;&#19979;&#30340;&#31283;&#20581;&#25554;&#20540;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20854;Lipschitz&#19979;&#30028;&#20998;&#21035;&#20026;$\Omega(\sqrt{n/p})$&#21644;$\Omega(n^{1/d})$&#12290;</title><link>http://arxiv.org/abs/2202.11592</link><description>&lt;p&gt;
&#36229;&#20986;&#31561;&#21608;&#24615;&#30340;&#31283;&#20581;&#24615;&#24459;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Law of Robustness beyond Isoperimetry. (arXiv:2202.11592v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.11592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#23618;&#27425;&#30340;&#31283;&#20581;&#24615;&#24459;&#27861;&#65292;&#30740;&#31350;&#20102;&#22312;&#20219;&#24847;&#25968;&#25454;&#20998;&#24067;&#19979;&#30340;&#31283;&#20581;&#25554;&#20540;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20854;Lipschitz&#19979;&#30028;&#20998;&#21035;&#20026;$\Omega(\sqrt{n/p})$&#21644;$\Omega(n^{1/d})$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22312;&#19968;&#20010;&#26377;&#30028;&#31354;&#38388;&#19978;&#25903;&#25345;&#20219;&#24847;&#25968;&#25454;&#20998;&#24067;&#30340;&#31283;&#20581;&#25554;&#20540;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#23618;&#27425;&#30340;&#31283;&#20581;&#24615;&#24459;&#27861;&#12290;&#31283;&#20581;&#25554;&#20540;&#26159;&#25351;&#36890;&#36807;Lipschitz&#20989;&#25968;&#25554;&#20540;$\mathbb{R}^d$&#20013;&#30340;$n$&#20010;&#22024;&#26434;&#35757;&#32451;&#25968;&#25454;&#28857;&#12290;&#23613;&#31649;&#24403;&#26679;&#26412;&#26469;&#33258;&#31561;&#21608;&#24615;&#20998;&#24067;&#26102;&#65292;&#24050;&#32463;&#24456;&#22909;&#22320;&#29702;&#35299;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#22312;&#19968;&#33324;&#25110;&#26368;&#22351;&#24773;&#20917;&#20998;&#24067;&#19979;&#65292;&#20854;&#24615;&#33021;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#20219;&#24847;&#25968;&#25454;&#20998;&#24067;&#65292;&#25554;&#20540;&#31070;&#32463;&#32593;&#32476;&#30340;Lipschitz&#19979;&#30028;&#20026;$\Omega(\sqrt{n/p})$&#65292;&#20854;&#20013;$p$&#34920;&#31034;&#21442;&#25968;&#25968;&#37327;&#12290;&#36890;&#36807;&#36825;&#20010;&#32467;&#26524;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;Bubeck&#65292;Li&#21644;Nagaraj&#22312;&#20108;&#23618;&#31070;&#32463;&#32593;&#32476;&#20013;&#20351;&#29992;&#22810;&#39033;&#24335;&#26435;&#37325;&#25552;&#20986;&#30340;&#31283;&#20581;&#24615;&#24459;&#27861;&#29468;&#24819;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#32467;&#26524;&#25193;&#23637;&#21040;&#20219;&#24847;&#25554;&#20540;&#36924;&#36817;&#22120;&#65292;&#24182;&#35777;&#26126;&#20102;&#31283;&#20581;&#25554;&#20540;&#30340;Lipschitz&#19979;&#30028;&#20026;$\Omega(n^{1/d})$&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#35777;&#26126;&#20102;&#19968;&#20010;&#20004;&#23618;&#27425;&#30340;&#31283;&#20581;&#24615;&#24459;&#27861;&#65306;
&lt;/p&gt;
&lt;p&gt;
We study the robust interpolation problem of arbitrary data distributions supported on a bounded space and propose a two-fold law of robustness. Robust interpolation refers to the problem of interpolating $n$ noisy training data points in $\mathbb{R}^d$ by a Lipschitz function. Although this problem has been well understood when the samples are drawn from an isoperimetry distribution, much remains unknown concerning its performance under generic or even the worst-case distributions. We prove a Lipschitzness lower bound $\Omega(\sqrt{n/p})$ of the interpolating neural network with $p$ parameters on arbitrary data distributions. With this result, we validate the law of robustness conjecture in prior work by Bubeck, Li, and Nagaraj on two-layer neural networks with polynomial weights. We then extend our result to arbitrary interpolating approximators and prove a Lipschitzness lower bound $\Omega(n^{1/d})$ for robust interpolation. Our results demonstrate a two-fold law of robustness: i) w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;DyHPO&#65292;&#23427;&#33021;&#22815;&#23398;&#20064;&#21160;&#24577;&#20915;&#23450;&#21738;&#20010;&#36229;&#21442;&#25968;&#37197;&#32622;&#22312;&#25152;&#26377;&#21487;&#34892;&#30340;&#37197;&#32622;&#20013;&#36827;&#34892;&#19979;&#19968;&#27493;&#35757;&#32451;&#12290;&#23454;&#39564;&#34920;&#26126;DyHPO&#30456;&#27604;&#20110;&#26368;&#20808;&#36827;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#26041;&#27861;&#26377;&#26174;&#33879;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2202.09774</link><description>&lt;p&gt;
&#30417;&#30563;&#22810;&#20445;&#30495;&#24230;&#36229;&#21442;&#25968;&#37197;&#32622;&#31454;&#36187;
&lt;/p&gt;
&lt;p&gt;
Supervising the Multi-Fidelity Race of Hyperparameter Configurations. (arXiv:2202.09774v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.09774
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;DyHPO&#65292;&#23427;&#33021;&#22815;&#23398;&#20064;&#21160;&#24577;&#20915;&#23450;&#21738;&#20010;&#36229;&#21442;&#25968;&#37197;&#32622;&#22312;&#25152;&#26377;&#21487;&#34892;&#30340;&#37197;&#32622;&#20013;&#36827;&#34892;&#19979;&#19968;&#27493;&#35757;&#32451;&#12290;&#23454;&#39564;&#34920;&#26126;DyHPO&#30456;&#27604;&#20110;&#26368;&#20808;&#36827;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#26041;&#27861;&#26377;&#26174;&#33879;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#22810;&#20445;&#30495;&#24230;&#36229;&#21442;&#25968;&#20248;&#21270;&#25216;&#26415;&#65288;HPO&#65289;&#20316;&#20026;&#35843;&#25972;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#32780;&#20986;&#29616;&#65292;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#36229;&#21442;&#25968;&#37197;&#32622;&#30340;HPO&#39044;&#31639;&#20998;&#37197;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;DyHPO&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23398;&#20064;&#20915;&#23450;&#22312;&#25152;&#26377;&#21487;&#34892;&#30340;&#37197;&#32622;&#20013;&#21160;&#24577;&#36187;&#36305;&#20013;&#36827;&#19968;&#27493;&#35757;&#32451;&#21738;&#20010;&#36229;&#21442;&#25968;&#37197;&#32622;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#39640;&#26031;&#36807;&#31243;&#20869;&#26680;&#65292;&#23427;&#23884;&#20837;&#20102;&#23398;&#20064;&#26354;&#32447;&#21160;&#21147;&#23398;&#65292;&#20197;&#21450;&#19968;&#20010;&#21253;&#21547;&#22810;&#39044;&#31639;&#20449;&#24687;&#30340;&#33719;&#24471;&#20989;&#25968;&#12290;&#25105;&#20204;&#36890;&#36807;&#21253;&#21547;50&#20010;&#25968;&#25454;&#38598;&#65288;&#34920;&#26684;&#12289;&#22270;&#20687;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65289;&#21644;&#19981;&#21516;&#26550;&#26500;&#65288;MLP&#12289;CNN/NAS&#12289;RNN&#65289;&#30340;&#22823;&#35268;&#27169;&#23454;&#39564;&#35777;&#26126;&#20102;DyHPO&#30456;&#27604;&#20110;&#26368;&#20808;&#36827;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#26041;&#27861;&#30340;&#26174;&#33879;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-fidelity (gray-box) hyperparameter optimization techniques (HPO) have recently emerged as a promising direction for tuning Deep Learning methods. However, existing methods suffer from a sub-optimal allocation of the HPO budget to the hyperparameter configurations. In this work, we introduce DyHPO, a Bayesian Optimization method that learns to decide which hyperparameter configuration to train further in a dynamic race among all feasible configurations. We propose a new deep kernel for Gaussian Processes that embeds the learning curve dynamics, and an acquisition function that incorporates multi-budget information. We demonstrate the significant superiority of DyHPO against state-of-the-art hyperparameter optimization methods through large-scale experiments comprising 50 datasets (Tabular, Image, NLP) and diverse architectures (MLP, CNN/NAS, RNN).
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#23545;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#30340;&#25913;&#36827;&#65292;&#33021;&#22815;&#20248;&#21270;&#32593;&#32476;&#22312;&#33021;&#25928;&#12289;&#20869;&#23384;&#20351;&#29992;&#21644;&#20934;&#30830;&#24615;&#19978;&#30340;&#34920;&#29616;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#30456;&#27604;&#22522;&#32447;MobileNetV1/V2&#65292;&#25628;&#32034;&#20986;&#30340;&#32593;&#32476;&#27599;&#20010;&#25512;&#29702;&#33021;&#37327;&#23569;&#19968;&#20010;&#25968;&#37327;&#32423;&#19988;&#20869;&#23384;&#21344;&#29992;&#23567;&#24471;&#22810;&#65292;&#21516;&#26102;&#30053;&#24494;&#25552;&#39640;&#20102;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2202.05397</link><description>&lt;p&gt;
&#38754;&#21521;&#33021;&#25928;&#30340;&#22987;&#32456;&#24320;&#21551;&#38899;&#39057;&#27169;&#22411;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Neural Architecture Search for Energy Efficient Always-on Audio Models. (arXiv:2202.05397v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.05397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#23545;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#30340;&#25913;&#36827;&#65292;&#33021;&#22815;&#20248;&#21270;&#32593;&#32476;&#22312;&#33021;&#25928;&#12289;&#20869;&#23384;&#20351;&#29992;&#21644;&#20934;&#30830;&#24615;&#19978;&#30340;&#34920;&#29616;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#30456;&#27604;&#22522;&#32447;MobileNetV1/V2&#65292;&#25628;&#32034;&#20986;&#30340;&#32593;&#32476;&#27599;&#20010;&#25512;&#29702;&#33021;&#37327;&#23569;&#19968;&#20010;&#25968;&#37327;&#32423;&#19988;&#20869;&#23384;&#21344;&#29992;&#23567;&#24471;&#22810;&#65292;&#21516;&#26102;&#30053;&#24494;&#25552;&#39640;&#20102;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22987;&#32456;&#24320;&#21551;&#30340;&#20998;&#31867;&#20219;&#21153;&#23545;&#31227;&#21160;&#21644;&#36793;&#32536;&#35745;&#31639;&#35774;&#22791;&#38656;&#35201;&#33021;&#25928;&#39640;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20960;&#31181;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#30340;&#25913;&#36827;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#25104;&#21151;&#30340;&#26426;&#20250;&#12290;&#25105;&#20204;&#30340;&#25628;&#32034;&#21516;&#26102;&#20248;&#21270;&#32593;&#32476;&#20934;&#30830;&#24615;&#12289;&#33021;&#25928;&#21644;&#20869;&#23384;&#20351;&#29992;&#37327;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#30828;&#20214;&#19978;&#23545;&#25105;&#20204;&#30340;&#25628;&#32034;&#24615;&#33021;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#20294;&#30001;&#20110;&#20351;&#29992;&#30495;&#23454;&#30828;&#20214;&#36816;&#34892;&#25104;&#21315;&#19978;&#19975;&#27425;&#27979;&#35797;&#24456;&#22256;&#38590;&#65292;&#22240;&#27492;&#25105;&#20204;&#20351;&#29992;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#31895;&#30053;&#39044;&#27979;&#20505;&#36873;&#32593;&#32476;&#30340;&#33021;&#28304;&#20351;&#29992;&#24773;&#20917;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25628;&#32034;&#31574;&#30053;&#65292;&#20351;&#29992;&#36125;&#21494;&#26031;&#21644;&#27491;&#21017;&#21270;&#36827;&#21270;&#25628;&#32034;&#31890;&#23376;&#32676;&#65292;&#24182;&#37319;&#29992;&#25552;&#21069;&#20572;&#27490;&#20943;&#23569;&#35745;&#31639;&#36127;&#25285;&#12290;&#25105;&#20204;&#22312;&#22522;&#20110;AudioSet&#30340;&#22768;&#20107;&#20214;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#25105;&#20204;&#30340;&#25628;&#32034;&#65292;&#32467;&#26524;&#27599;&#20010;&#25512;&#29702;&#30340;&#33021;&#37327;&#27604;&#25105;&#20204;&#30340;&#22522;&#32447;MobileNetV1/V2&#23454;&#29616;&#23569;&#19968;&#20010;&#25968;&#37327;&#32423;&#65292;&#20869;&#23384;&#21344;&#29992;&#37327;&#20063;&#23567;&#24471;&#22810;&#65292;&#21516;&#26102;&#30053;&#24494;&#25552;&#39640;&#20102;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mobile and edge computing devices for always-on classification tasks require energy-efficient neural network architectures. In this paper we present several changes to neural architecture searches (NAS) that improve the chance of success in practical situations. Our search simultaneously optimizes for network accuracy, energy efficiency and memory usage. We benchmark the performance of our search on real hardware, but since running thousands of tests with real hardware is difficult we use a random forest model to roughly predict the energy usage of a candidate network. We present a search strategy that uses both Bayesian and regularized evolutionary search with particle swarms, and employs early-stopping to reduce the computational burden. Our search, evaluated on a sound-event classification dataset based upon AudioSet, results in an order of magnitude less energy per inference and a much smaller memory footprint than our baseline MobileNetV1/V2 implementations while slightly improvin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#36172;&#21338;&#21453;&#39304;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#33258;&#36866;&#24212;&#36873;&#25321;&#21738;&#20123;&#23458;&#25143;&#31471;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#35777;&#26126;&#35813;&#31639;&#27861;&#21487;&#25552;&#39640;&#20248;&#21270;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2112.14332</link><description>&lt;p&gt;
&#22522;&#20110;&#36172;&#21338;&#21453;&#39304;&#30340;&#22312;&#32447;&#23398;&#20064;&#33258;&#36866;&#24212;&#23458;&#25143;&#31471;&#37319;&#26679;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Adaptive Client Sampling in Federated Learning via Online Learning with Bandit Feedback. (arXiv:2112.14332v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.14332
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#36172;&#21338;&#21453;&#39304;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#33258;&#36866;&#24212;&#36873;&#25321;&#21738;&#20123;&#23458;&#25143;&#31471;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#35777;&#26126;&#35813;&#31639;&#27861;&#21487;&#25552;&#39640;&#20248;&#21270;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#36890;&#20449;&#25104;&#26412;&#39640;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#31995;&#32479;&#38656;&#35201;&#37319;&#26679;&#19968;&#37096;&#20998;&#23458;&#25143;&#31471;&#21442;&#19982;&#27599;&#19968;&#36718;&#35757;&#32451;&#12290;&#22240;&#27492;&#65292;&#23458;&#25143;&#31471;&#37319;&#26679;&#22312;FL&#31995;&#32479;&#20013;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#65292;&#23427;&#24433;&#21709;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20248;&#21270;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#23613;&#31649;&#20855;&#26377;&#37325;&#35201;&#24615;&#65292;&#20294;&#26377;&#25928;&#37319;&#26679;&#23458;&#25143;&#31471;&#26041;&#38754;&#30340;&#30740;&#31350;&#24456;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#23458;&#25143;&#31471;&#37319;&#26679;&#24314;&#27169;&#20026;&#22312;&#24102;&#26377;&#36172;&#21338;&#21453;&#39304;&#30340;&#22312;&#32447;&#23398;&#20064;&#20219;&#21153;&#65292;&#20351;&#29992;&#22312;&#32447;&#38543;&#26426;&#38236;&#20687;&#19979;&#38477;&#65288;OSMD&#65289;&#31639;&#27861;&#26469;&#26368;&#23567;&#21270;&#37319;&#26679;&#26041;&#24046;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#37319;&#26679;&#26041;&#27861;&#22914;&#20309;&#25552;&#39640;&#20248;&#21270;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#20026;&#20102;&#22788;&#29702;OSMD&#20013;&#20381;&#36182;&#20110;&#26410;&#30693;&#38382;&#39064;&#21442;&#25968;&#30340;&#35843;&#25972;&#21442;&#25968;&#65292;&#25105;&#20204;&#20351;&#29992;&#22312;&#32447;&#38598;&#25104;&#26041;&#27861;&#21644;&#32763;&#20493;&#25216;&#24039;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#30456;&#23545;&#20110;&#20219;&#20309;&#37319;&#26679;&#24207;&#21015;&#30340;&#21160;&#24577;&#36951;&#25022;&#30028;&#12290;&#36951;&#25022;&#30028;&#21462;&#20915;&#20110;&#27604;&#36739;&#24207;&#21015;&#30340;&#24635;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the high cost of communication, federated learning (FL) systems need to sample a subset of clients that are involved in each round of training. As a result, client sampling plays an important role in FL systems as it affects the convergence rate of optimization algorithms used to train machine learning models. Despite its importance, there is limited work on how to sample clients effectively. In this paper, we cast client sampling as an online learning task with bandit feedback, which we solve with an online stochastic mirror descent (OSMD) algorithm designed to minimize the sampling variance. We then theoretically show how our sampling method can improve the convergence speed of optimization algorithms. To handle the tuning parameters in OSMD that depend on the unknown problem parameters, we use the online ensemble method and doubling trick. We prove a dynamic regret bound relative to any sampling sequence. The regret bound depends on the total variation of the comparator seque
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NN2Poly&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#24050;&#32463;&#35757;&#32451;&#22909;&#30340;&#20840;&#36830;&#25509;&#21069;&#39304;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#31934;&#30830;&#22810;&#39033;&#24335;&#34920;&#31034;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#20219;&#24847;&#28145;&#24230;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#19988;&#35745;&#31639;&#25104;&#26412;&#30456;&#23545;&#36739;&#20302;&#65292;&#33021;&#22815;&#22312;&#22238;&#24402;&#21644;&#20998;&#31867;&#20219;&#21153;&#20013;&#25552;&#20379;&#38750;&#24120;&#20934;&#30830;&#30340;&#36924;&#36817;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2112.11397</link><description>&lt;p&gt;
NN2Poly&#65306;&#29992;&#20110;&#28145;&#24230;&#21069;&#39304;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#39033;&#24335;&#34920;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
NN2Poly: A polynomial representation for deep feed-forward artificial neural networks. (arXiv:2112.11397v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.11397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NN2Poly&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#24050;&#32463;&#35757;&#32451;&#22909;&#30340;&#20840;&#36830;&#25509;&#21069;&#39304;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#31934;&#30830;&#22810;&#39033;&#24335;&#34920;&#31034;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#20219;&#24847;&#28145;&#24230;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#19988;&#35745;&#31639;&#25104;&#26412;&#30456;&#23545;&#36739;&#20302;&#65292;&#33021;&#22815;&#22312;&#22238;&#24402;&#21644;&#20998;&#31867;&#20219;&#21153;&#20013;&#25552;&#20379;&#38750;&#24120;&#20934;&#30830;&#30340;&#36924;&#36817;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#38750;&#24120;&#25104;&#21151;&#65292;&#20294;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#29702;&#35770;&#34892;&#20026;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#26412;&#25991;&#25552;&#20986;NN2Poly&#65306;&#19968;&#31181;&#29702;&#35770;&#26041;&#27861;&#65292;&#29992;&#20110;&#33719;&#21462;&#19968;&#20010;&#26174;&#24335;&#22810;&#39033;&#24335;&#27169;&#22411;&#65292;&#20197;&#25552;&#20379;&#24050;&#32463;&#35757;&#32451;&#22909;&#30340;&#20840;&#36830;&#25509;&#21069;&#39304;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;&#22810;&#23618;&#24863;&#30693;&#22120;&#25110;MLP&#65289;&#30340;&#31934;&#30830;&#34920;&#31034;&#12290;&#36825;&#31181;&#26041;&#27861;&#25193;&#23637;&#20102;&#25991;&#29486;&#20013;&#25552;&#20986;&#30340;&#20808;&#21069;&#24819;&#27861;&#65292;&#35813;&#24819;&#27861;&#20165;&#38480;&#20110;&#21333;&#38544;&#34255;&#23618;&#30340;&#32593;&#32476;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#22238;&#24402;&#21644;&#20998;&#31867;&#20219;&#21153;&#30340;&#20219;&#24847;&#28145;&#24230;MLP&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#22312;&#27599;&#23618;&#19978;&#20351;&#29992;&#28608;&#27963;&#20989;&#25968;&#30340;&#27888;&#21202;&#23637;&#24320;&#24335;&#65292;&#28982;&#21518;&#20351;&#29992;&#20960;&#20010;&#32452;&#21512;&#24615;&#36136;&#26469;&#35745;&#31639;&#25152;&#38656;&#22810;&#39033;&#24335;&#30340;&#31995;&#25968;&#65292;&#20174;&#32780;&#23454;&#29616;&#27492;&#30446;&#26631;&#12290;&#20316;&#32773;&#35752;&#35770;&#20102;&#27492;&#26041;&#27861;&#30340;&#20027;&#35201;&#35745;&#31639;&#25361;&#25112;&#20197;&#21450;&#36890;&#36807;&#24341;&#20837;&#19968;&#20123;&#36924;&#36817;&#26469;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#30340;&#26041;&#27861;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#20854;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#34920;&#26126;&#65292;&#23613;&#31649;NN2Poly&#26041;&#27861;&#31616;&#21333;&#19988;&#35745;&#31639;&#25104;&#26412;&#20302;&#65292;&#20294;&#23545;&#20110;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#38750;&#24120;&#20934;&#30830;&#30340;&#22810;&#39033;&#24335;&#36924;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretability of neural networks and their underlying theoretical behavior remain an open field of study even after the great success of their practical applications, particularly with the emergence of deep learning. In this work, NN2Poly is proposed: a theoretical approach to obtain an explicit polynomial model that provides an accurate representation of an already trained fully-connected feed-forward artificial neural network (a multilayer perceptron or MLP). This approach extends a previous idea proposed in the literature, which was limited to single hidden layer networks, to work with arbitrarily deep MLPs in both regression and classification tasks. The objective of this paper is to achieve this by using a Taylor expansion on the activation function, at each layer, and then using several combinatorial properties to calculate the coefficients of the desired polynomials. Discussion is presented on the main computational challenges of this method, and the way to overcome them by i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;TC-GNN&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;GPU&#24352;&#37327;&#26680;&#24515;&#21333;&#20803;&#65288;TCUs&#65289;&#30340;GNN&#21152;&#36895;&#26694;&#26550;&#12290;&#37319;&#29992;&#31232;&#30095;&#22270;&#32763;&#35793;&#25216;&#26415;&#26469;&#21327;&#35843;&#8220;&#31232;&#30095;&#8221;GNN&#35745;&#31639;&#19982;&#39640;&#24615;&#33021;&#30340;&#8220;&#23494;&#38598;&#8221;TCUs&#65292;&#23454;&#29616;&#20102;GNN&#35745;&#31639;&#25928;&#29575;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2112.02052</link><description>&lt;p&gt;
TC-GNN&#65306;&#22312;GPU&#19978;&#36830;&#25509;&#31232;&#30095;GNN&#35745;&#31639;&#19982;&#23494;&#38598;Tensor Cores&#30340;&#26725;&#26753;
&lt;/p&gt;
&lt;p&gt;
TC-GNN: Bridging Sparse GNN Computation and Dense Tensor Cores on GPUs. (arXiv:2112.02052v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.02052
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;TC-GNN&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;GPU&#24352;&#37327;&#26680;&#24515;&#21333;&#20803;&#65288;TCUs&#65289;&#30340;GNN&#21152;&#36895;&#26694;&#26550;&#12290;&#37319;&#29992;&#31232;&#30095;&#22270;&#32763;&#35793;&#25216;&#26415;&#26469;&#21327;&#35843;&#8220;&#31232;&#30095;&#8221;GNN&#35745;&#31639;&#19982;&#39640;&#24615;&#33021;&#30340;&#8220;&#23494;&#38598;&#8221;TCUs&#65292;&#23454;&#29616;&#20102;GNN&#35745;&#31639;&#25928;&#29575;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20316;&#20026;&#22522;&#20110;&#22270;&#30340;&#26426;&#22120;&#23398;&#20064;&#30340;&#39592;&#24178;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#23637;&#31034;&#20102;&#22312;&#21508;&#20010;&#39046;&#22495;&#65288;&#22914;&#30005;&#23376;&#21830;&#21153;&#65289;&#30340;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39640;&#24230;&#31232;&#30095;&#21644;&#19981;&#35268;&#21017;&#30340;&#22522;&#20110;&#22270;&#30340;&#25805;&#20316;&#65292;GNN&#30340;&#24615;&#33021;&#36890;&#24120;&#19981;&#23613;&#22914;&#20154;&#24847;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TC-GNN&#65292;&#22522;&#20110;GPU&#24352;&#37327;&#26680;&#24515;&#21333;&#20803;&#65288;TCUs&#65289;&#30340;&#31532;&#19968;&#20010;GNN&#21152;&#36895;&#26694;&#26550;&#12290;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#36890;&#36807;&#23558;&#8220;&#31232;&#30095;&#8221;GNN&#35745;&#31639;&#19982;&#39640;&#24615;&#33021;&#30340;&#8220;&#23494;&#38598;&#8221;TCUs&#21327;&#35843;&#19968;&#33268;&#65292;&#23454;&#29616;GNN&#35745;&#31639;&#25928;&#29575;&#30340;&#25552;&#21319;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23545;&#20027;&#27969;GNN&#35745;&#31639;&#26694;&#26550;&#20013;&#30340;&#31232;&#30095;&#25805;&#20316;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31232;&#30095;&#22270;&#32763;&#35793;&#25216;&#26415;&#65292;&#20197;&#20415;TCU&#22788;&#29702;&#31232;&#30095;&#30340;GNN&#24037;&#20316;&#36127;&#36733;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;CUDA&#26680;&#24515;&#21644;TCU&#21327;&#20316;&#35774;&#35745;&#65292;&#20805;&#20998;&#21033;&#29992;GPU&#36164;&#28304;&#12290;&#25105;&#20204;&#23558;TC-GNN&#19982;PyTorch&#26694;&#26550;&#38598;&#25104;&#65292;&#20197;&#23454;&#29616;&#39640;&#21487;&#32534;&#31243;&#24615;&#12290;&#20005;&#26684;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#19978;&#65292;&#30456;&#27604;&#20110;&#26368;&#20808;&#36827;&#30340;DGL&#26694;&#26550;&#65292;&#24179;&#22343;&#21152;&#36895;&#20102;1.70&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, graph neural networks (GNNs), as the backbone of graph-based machine learning, demonstrate great success in various domains (e.g., e-commerce). However, the performance of GNNs is usually unsatisfactory due to the highly sparse and irregular graph-based operations. To this end, we propose TC-GNN, the first GNN acceleration framework based on GPU Tensor Core Units (TCUs). The core idea is to reconcile the "Sparse" GNN computation with the high-performance "Dense" TCUs. Specifically, we conduct an in-depth analysis of the sparse operations in mainstream GNN computing frameworks. We introduce a novel sparse graph translation technique to facilitate TCU processing of the sparse GNN workload. We implement an effective CUDA core and TCU collaboration design to fully utilize GPU resources. We integrate TC-GNN with the PyTorch framework for high programmability. Rigorous experiments show an average of 1.70X speedup over the state-of-the-art DGL framework across various models and dat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#26032;&#30340;&#19968;&#31181;&#25490;&#38500;&#38480;&#21046;&#30340;&#25490;&#24207;&#19968;&#33268;&#24615;&#24207;&#22238;&#24402;&#26041;&#27861;&#65292;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#26465;&#20214;&#27010;&#29575;&#30340;&#24314;&#27169;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#36755;&#20986;&#27010;&#29575;&#25490;&#24207;&#19968;&#33268;&#24615;&#30340;&#21516;&#26102;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2111.08851</link><description>&lt;p&gt;
&#22522;&#20110;&#26465;&#20214;&#27010;&#29575;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#25490;&#24207;&#19968;&#33268;&#24207;&#22238;&#24402;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks for Rank-Consistent Ordinal Regression Based On Conditional Probabilities. (arXiv:2111.08851v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.08851
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#26032;&#30340;&#19968;&#31181;&#25490;&#38500;&#38480;&#21046;&#30340;&#25490;&#24207;&#19968;&#33268;&#24615;&#24207;&#22238;&#24402;&#26041;&#27861;&#65292;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#26465;&#20214;&#27010;&#29575;&#30340;&#24314;&#27169;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#36755;&#20986;&#27010;&#29575;&#25490;&#24207;&#19968;&#33268;&#24615;&#30340;&#21516;&#26102;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#31181;&#20998;&#31867;&#21644;&#27169;&#24335;&#35782;&#21035;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#23454;&#38469;&#30340;&#39044;&#27979;&#38382;&#39064;&#20855;&#26377;&#24207;&#21709;&#24212;&#21464;&#37327;&#65292;&#24182;&#19988;&#20256;&#32479;&#30340;&#20998;&#31867;&#25439;&#22833;&#20989;&#25968;&#24573;&#30053;&#20102;&#36825;&#31181;&#25490;&#24207;&#20449;&#24687;&#12290;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24207;&#22238;&#24402;&#26041;&#27861;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#24314;&#31435;&#27599;&#20010;&#24207;&#20998;&#31867;&#30340;&#26465;&#20214;&#27010;&#29575;&#30340;&#28145;&#24230;&#31070;&#32463;&#32467;&#26500;&#65292;&#20174;&#32780;&#20445;&#35777;&#27169;&#22411;&#30340;&#36755;&#20986;&#27010;&#29575;&#22312;&#39044;&#27979;&#26631;&#31614;&#31867;&#21035;&#20013;&#20445;&#25345;&#20854;&#25490;&#24207;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent times, deep neural networks achieved outstanding predictive performance on various classification and pattern recognition tasks. However, many real-world prediction problems have ordinal response variables, and this ordering information is ignored by conventional classification losses such as the multi-category cross-entropy. Ordinal regression methods for deep neural networks address this. One such method is the CORAL method, which is based on an earlier binary label extension framework and achieves rank consistency among its output layer tasks by imposing a weight-sharing constraint. However, while earlier experiments showed that CORAL's rank consistency is beneficial for performance, it is limited by a weight-sharing constraint in a neural network's fully connected output layer, which may restrict the expressiveness and capacity of a network trained using CORAL. We propose a new method for rank-consistent ordinal regression without this limitation. Our rank-consistent ordi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#20174;&#25968;&#25454;&#20013;&#33719;&#24471;&#30830;&#20999;&#21160;&#21147;&#23398;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#24182;&#21629;&#21517;&#20026;&#8220;FJet&#8221;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#35856;&#25391;&#23376;&#12289;&#25670;&#21644;&#36798;&#33452;&#22855;&#25391;&#33633;&#22120;&#31561;&#31034;&#20363;&#20013;&#34987;&#35777;&#26126;&#21487;&#20197;&#20934;&#30830;&#22320;&#22797;&#21046;&#21160;&#21147;&#23398;&#24182;&#24674;&#22797;&#28508;&#22312;&#30340;&#24494;&#20998;&#26041;&#31243;&#12290;</title><link>http://arxiv.org/abs/2110.06917</link><description>&lt;p&gt;
&#20174;&#25968;&#25454;&#20013;&#25552;&#21462;&#21160;&#21147;&#23398;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Extracting Dynamical Models from Data. (arXiv:2110.06917v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.06917
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#20174;&#25968;&#25454;&#20013;&#33719;&#24471;&#30830;&#20999;&#21160;&#21147;&#23398;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#24182;&#21629;&#21517;&#20026;&#8220;FJet&#8221;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#35856;&#25391;&#23376;&#12289;&#25670;&#21644;&#36798;&#33452;&#22855;&#25391;&#33633;&#22120;&#31561;&#31034;&#20363;&#20013;&#34987;&#35777;&#26126;&#21487;&#20197;&#20934;&#30830;&#22320;&#22797;&#21046;&#21160;&#21147;&#23398;&#24182;&#24674;&#22797;&#28508;&#22312;&#30340;&#24494;&#20998;&#26041;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#23545;&#30456;&#31354;&#38388;&#21464;&#37327;&#30340;&#26356;&#26032;&#24314;&#27169;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#30830;&#23450;&#31995;&#32479;&#22522;&#30784;&#21160;&#21147;&#23398;&#30340;&#26041;&#27861;&#12290;&#21487;&#20197;&#20934;&#30830;&#22320;&#22797;&#21046;&#35856;&#25391;&#23376;&#12289;&#25670;&#21644;&#36798;&#33452;&#22855;&#25391;&#33633;&#22120;&#30340;&#21160;&#21147;&#23398;&#65292;&#24182;&#22312;&#27599;&#20010;&#31034;&#20363;&#20013;&#20934;&#30830;&#22320;&#24674;&#22797;&#20102;&#28508;&#22312;&#30340;&#24494;&#20998;&#26041;&#31243;&#12290;&#27492;&#26041;&#27861;&#21629;&#21517;&#20026;&#8220;FJet&#8221;&#65292;&#31867;&#20284;&#20110;&#24471;&#21040;&#30340;&#27169;&#22411;&#20110;Runge-Kutta&#25968;&#20540;&#31215;&#20998;&#26041;&#26696;&#30340;Taylor&#32423;&#25968;&#23637;&#24320;&#12290;&#36825;&#31181;&#31867;&#27604;&#20855;&#26377;&#26174;&#24335;&#25581;&#31034;&#36866;&#24403;&#20989;&#25968;&#24418;&#24335;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of determining the underlying dynamics of a system when only given data of its state over time has challenged scientists for decades. In this paper, the approach of using machine learning to model the {\em updates} of the phase space variables is introduced; this is done as a function of the phase space variables. (More generally, the modeling is done over the jet space of the variables.) This approach is shown to accurately replicate the dynamics for the examples of the harmonic oscillator, the pendulum, and the Duffing oscillator; the underlying differential equation is also accurately recovered in each example. In addition, the results in no way depend on how the data is sampled over time (i.e., regularly or irregularly). It is demonstrated that this approach (named "FJet") is similar to the model resulting from a Taylor series expansion of the Runge-Kutta (RK) numerical integration scheme. This analogy confers the advantage of explicitly revealing the appropriate functi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26080;&#38656;&#28304;&#25968;&#25454;&#65292;&#36890;&#29992;&#65292;&#39640;&#25928;&#19988;&#26377;&#25928;&#30340;&#29289;&#21270;&#27169;&#22411;&#26597;&#35810;&#26694;&#26550;\textsf{MMQ}&#65292;&#20351;&#29992;&#22522;&#20110;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#30340;&#20998;&#31163;&#24230;&#23545;&#29289;&#21270;&#27169;&#22411;&#36827;&#34892;&#25490;&#24207;&#20197;&#27979;&#37327;&#30456;&#20851;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2110.06532</link><description>&lt;p&gt;
&#23547;&#25214;&#29992;&#20110;&#27169;&#22411;&#37325;&#29992;&#30340;&#29289;&#21270;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Finding Materialized Models for Model Reuse. (arXiv:2110.06532v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.06532
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26080;&#38656;&#28304;&#25968;&#25454;&#65292;&#36890;&#29992;&#65292;&#39640;&#25928;&#19988;&#26377;&#25928;&#30340;&#29289;&#21270;&#27169;&#22411;&#26597;&#35810;&#26694;&#26550;\textsf{MMQ}&#65292;&#20351;&#29992;&#22522;&#20110;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#30340;&#20998;&#31163;&#24230;&#23545;&#29289;&#21270;&#27169;&#22411;&#36827;&#34892;&#25490;&#24207;&#20197;&#27979;&#37327;&#30456;&#20851;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#21270;&#27169;&#22411;&#26597;&#35810;&#26088;&#22312;&#23547;&#25214;&#26368;&#21512;&#36866;&#30340;&#29289;&#21270;&#27169;&#22411;&#20316;&#20026;&#27169;&#22411;&#37325;&#29992;&#30340;&#21021;&#22987;&#27169;&#22411;&#12290;&#36825;&#26159;&#27169;&#22411;&#37325;&#29992;&#30340;&#21069;&#25552;&#26465;&#20214;&#65292;&#24182;&#19988;&#26368;&#36817;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#25552;&#20379;&#28304;&#25968;&#25454;&#30340;&#38656;&#27714;&#65292;&#24212;&#29992;&#33539;&#22260;&#26377;&#38480;&#21644;&#25928;&#29575;&#20302;&#31561;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#27809;&#26377;&#26500;&#24314;&#36866;&#21512;&#24230;&#37327;&#29289;&#21270;&#27169;&#22411;&#30446;&#26631;&#30456;&#20851;&#30693;&#35782;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;\textsf{MMQ}&#65292;&#36825;&#26159;&#19968;&#20010;&#26080;&#38656;&#28304;&#25968;&#25454;&#65292;&#36890;&#29992;&#65292;&#39640;&#25928;&#19988;&#26377;&#25928;&#30340;&#29289;&#21270;&#27169;&#22411;&#26597;&#35810;&#26694;&#26550;&#12290;&#23427;&#20351;&#29992;&#22522;&#20110;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#30340;&#24230;&#37327;&#26631;&#20934;&#31216;&#20026;&#20998;&#31163;&#24230;&#26469;&#23545;&#29289;&#21270;&#27169;&#22411;&#36827;&#34892;&#25490;&#24207;&#12290;&#23545;&#20110;&#27599;&#20010;&#29289;&#21270;&#27169;&#22411;&#65292;\textsf{MMQ}&#39318;&#20808;&#36890;&#36807;&#30452;&#25509;&#24212;&#29992;&#35813;&#27169;&#22411;&#23558;&#30446;&#26631;&#25968;&#25454;&#38598;&#20013;&#30340;&#26679;&#26412;&#21521;&#37327;&#21270;&#20026;&#27010;&#29575;&#21521;&#37327;&#65292;&#28982;&#21518;&#21033;&#29992;&#39640;&#26031;&#20998;&#24067;&#20026;&#27599;&#20010;&#27010;&#29575;&#21521;&#37327;&#31867;&#36827;&#34892;&#25311;&#21512;&#65292;&#26368;&#21518;&#22312;&#39640;&#26031;&#20998;&#24067;&#19978;&#20351;&#29992;&#20998;&#31163;&#24230;&#26469;&#34913;&#37327;&#30456;&#20851;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Materialized model query aims to find the most appropriate materialized model as the initial model for model reuse. It is the precondition of model reuse, and has recently attracted much attention. {Nonetheless, the existing methods suffer from the need to provide source data, limited range of applications, and inefficiency since they do not construct a suitable metric to measure the target-related knowledge of materialized models. To address this, we present \textsf{MMQ}, a source-data free, general, efficient, and effective materialized model query framework.} It uses a Gaussian mixture-based metric called separation degree to rank materialized models. For each materialized model, \textsf{MMQ} first vectorizes the samples in the target dataset into probability vectors by directly applying this model, then utilizes Gaussian distribution to fit for each class of probability vectors, and finally uses separation degree on the Gaussian distributions to measure the target-related knowledge
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#19981;&#24120;&#35265;&#20294;&#22312;&#32479;&#35745;&#12289;&#27010;&#29575;&#19982;&#20248;&#21270;&#31561;&#39046;&#22495;&#24120;&#29992;&#30340;&#25216;&#26415;&#8212;&#8212;&#25351;&#25968;&#20542;&#26012;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#21040;&#39118;&#38505;&#26368;&#23567;&#21270;&#20013;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#20462;&#27491;&#21333;&#20010;&#25439;&#22833;&#30340;&#24433;&#21709;&#65292;&#22686;&#21152;&#25110;&#20943;&#23569;&#24322;&#24120;&#20540;&#30340;&#20316;&#29992;&#65292;&#21487;&#20197;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#21487;&#20197;&#34987;&#35270;&#20026;&#25439;&#22833;&#30340;&#23614;&#37096;&#27010;&#29575;&#30340;&#24179;&#28369;&#36817;&#20284;&#12290;</title><link>http://arxiv.org/abs/2109.06141</link><description>&lt;p&gt;
&#20851;&#20110;&#20542;&#26012;&#25439;&#22833;&#30340;&#26426;&#22120;&#23398;&#20064;&#29702;&#35770;&#19982;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Tilted Losses in Machine Learning: Theory and Applications. (arXiv:2109.06141v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.06141
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#19981;&#24120;&#35265;&#20294;&#22312;&#32479;&#35745;&#12289;&#27010;&#29575;&#19982;&#20248;&#21270;&#31561;&#39046;&#22495;&#24120;&#29992;&#30340;&#25216;&#26415;&#8212;&#8212;&#25351;&#25968;&#20542;&#26012;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#21040;&#39118;&#38505;&#26368;&#23567;&#21270;&#20013;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#20462;&#27491;&#21333;&#20010;&#25439;&#22833;&#30340;&#24433;&#21709;&#65292;&#22686;&#21152;&#25110;&#20943;&#23569;&#24322;&#24120;&#20540;&#30340;&#20316;&#29992;&#65292;&#21487;&#20197;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#21487;&#20197;&#34987;&#35270;&#20026;&#25439;&#22833;&#30340;&#23614;&#37096;&#27010;&#29575;&#30340;&#24179;&#28369;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#25968;&#20542;&#26012;&#26159;&#32479;&#35745;&#23398;&#12289;&#27010;&#29575;&#35770;&#12289;&#20449;&#24687;&#35770;&#21644;&#20248;&#21270;&#31561;&#39046;&#22495;&#24120;&#29992;&#30340;&#19968;&#31181;&#25216;&#26415;&#65292;&#29992;&#20110;&#21019;&#24314;&#21442;&#25968;&#21270;&#20998;&#24067;&#36716;&#31227;&#12290;&#23613;&#31649;&#22312;&#30456;&#20851;&#39046;&#22495;&#20013;&#38750;&#24120;&#24120;&#35265;&#65292;&#20294;&#20542;&#26012;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#23578;&#19981;&#26222;&#21450;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#30740;&#31350;&#20542;&#26012;&#22312;&#39118;&#38505;&#26368;&#23567;&#21270;&#20013;&#30340;&#24212;&#29992;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#39564;&#31639;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;TERM&#65289;&#30340;&#31616;&#21333;&#25193;&#23637;&#65292;&#23427;&#20351;&#29992;&#25351;&#25968;&#20542;&#26012;&#26469;&#28789;&#27963;&#35843;&#25972;&#20010;&#21035;&#25439;&#22833;&#30340;&#24433;&#21709;&#12290;&#25152;&#24471;&#21040;&#30340;&#26694;&#26550;&#20855;&#26377;&#22810;&#31181;&#26377;&#29992;&#30340;&#24615;&#36136;&#65306;&#25105;&#20204;&#23637;&#31034;&#20102;TERM&#21487;&#20197;&#20998;&#21035;&#22686;&#21152;&#25110;&#20943;&#23569;&#24322;&#24120;&#20540;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#23454;&#29616;&#20844;&#24179;&#25110;&#40065;&#26834;&#24615;&#12290;TERM&#20063;&#20855;&#26377;&#38477;&#20302;&#26041;&#24046;&#20174;&#32780;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#30340;&#20248;&#21183;&#65292;&#24182;&#19988;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#23545;&#25439;&#22833;&#30340;&#23614;&#37096;&#27010;&#29575;&#30340;&#24179;&#28369;&#36817;&#20284;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22312;TERM&#21644;&#20854;&#20182;&#30456;&#20851;&#30446;&#26631;&#20043;&#38388;&#24314;&#31435;&#20102;&#20005;&#26684;&#30340;&#32852;&#31995;&#65292;&#20363;&#22914;&#39118;&#38505;&#20215;&#20540;&#12289;&#26465;&#20214;&#39118;&#38505;&#20215;&#20540;&#21644;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exponential tilting is a technique commonly used in fields such as statistics, probability, information theory, and optimization to create parametric distribution shifts. Despite its prevalence in related fields, tilting has not seen widespread use in machine learning. In this work, we aim to bridge this gap by exploring the use of tilting in risk minimization. We study a simple extension to ERM -- tilted empirical risk minimization (TERM) -which uses exponential tilting to flexibly tune the impact of individual losses. The resulting framework has several useful properties: We show that TERM can increase or decrease the influence of outliers, respectively, to enable fairness or robustness; has variance-reduction properties that can benefit generalization; and can be viewed as a smooth approximation to the tail probability of losses. Our work makes rigorous connections between TERM and related objectives, such as Value-at-Risk, Conditional Value-at-Risk, and distributionally robust op
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35299;&#32544;&#29983;&#25104;&#27169;&#22411;&#29992;&#20110;&#39044;&#27979;&#31995;&#32479;&#21160;&#21147;&#23398;&#65292;&#23558;&#39046;&#22495;&#21442;&#25968;&#19982;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#21160;&#21147;&#23398;&#20998;&#31163;&#65292;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#27867;&#21270;&#21644;&#38271;&#26399;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2108.11684</link><description>&lt;p&gt;
&#40065;&#26834;&#39044;&#27979;&#31995;&#32479;&#21160;&#21147;&#23398;&#30340;&#35299;&#32544;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Disentangled Generative Models for Robust Prediction of System Dynamics. (arXiv:2108.11684v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.11684
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35299;&#32544;&#29983;&#25104;&#27169;&#22411;&#29992;&#20110;&#39044;&#27979;&#31995;&#32479;&#21160;&#21147;&#23398;&#65292;&#23558;&#39046;&#22495;&#21442;&#25968;&#19982;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#21160;&#21147;&#23398;&#20998;&#31163;&#65292;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#27867;&#21270;&#21644;&#38271;&#26399;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21160;&#21147;&#31995;&#32479;&#39044;&#27979;&#20013;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#20294;&#26159;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#27867;&#21270;&#21644;&#38271;&#26399;&#31283;&#23450;&#24615;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#23558;&#21160;&#21147;&#31995;&#32479;&#30340;&#39046;&#22495;&#21442;&#25968;&#35270;&#20026;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#30340;&#21464;&#24322;&#22240;&#32032;&#65292;&#20511;&#37492;&#30417;&#30563;&#35299;&#32544;&#21644;&#22240;&#26524;&#20998;&#35299;&#30340;&#24605;&#24819;&#65292;&#26088;&#22312;&#23558;&#39046;&#22495;&#21442;&#25968;&#19982;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#21160;&#21147;&#23398;&#20998;&#31163;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#27169;&#25311;&#20102;&#30456;&#31354;&#38388;&#21644;&#35270;&#39057;&#24207;&#21015;&#30340;&#21160;&#24577;&#65292;&#24182;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#36229;&#20986;&#20998;&#24067;&#20215;&#20540;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35299;&#32544;&#30340; VAEs &#26356;&#36866;&#24212;&#20110;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#19981;&#23384;&#22312;&#30340;&#39046;&#22495;&#21442;&#25968;&#31354;&#38388;&#12290;&#21516;&#26102;&#65292;&#35299;&#32544;&#21487;&#20197;&#25913;&#21892;&#35270;&#39057;&#24207;&#21015;&#20013;&#29616;&#26377;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#38271;&#26399;&#21644;&#36229;&#20986;&#20998;&#24067;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks have become increasingly of interest in dynamical system prediction, but out-of-distribution generalization and long-term stability still remains challenging. In this work, we treat the domain parameters of dynamical systems as factors of variation of the data generating process. By leveraging ideas from supervised disentanglement and causal factorization, we aim to separate the domain parameters from the dynamics in the latent space of generative models. In our experiments we model dynamics both in phase space and in video sequences and conduct rigorous OOD evaluations. Results indicate that disentangled VAEs adapt better to domain parameters spaces that were not present in the training data. At the same time, disentanglement can improve the long-term and out-of-distribution predictions of state-of-the-art models in video sequences.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#22312;&#21709;&#24212;&#24335;&#21644;&#20132;&#20114;&#24335;&#25968;&#25454;&#20998;&#24067;&#19979;&#65292;&#31639;&#27861;&#39044;&#27979;&#22120;&#30340;&#21487;&#36801;&#31227;&#24615;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#24615;&#33021;&#24046;&#36317;&#30340;&#19978;&#30028;&#21644;&#20998;&#31867;&#22120;&#24517;&#39035;&#28385;&#36275;&#30340;&#26435;&#34913;&#30340;&#19979;&#30028;&#65292;&#24182;&#21051;&#30011;&#20102;&#39044;&#27979;&#22120;&#12289;&#31574;&#30053;&#31354;&#38388;&#36873;&#25321;&#21644;&#20559;&#31227;&#24615;&#36136;&#23545;&#21487;&#36801;&#31227;&#24615;&#31243;&#24230;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2107.05911</link><description>&lt;p&gt;
&#21709;&#24212;&#24335;&#20915;&#31574;&#20027;&#20307;&#19979;&#30340;&#27169;&#22411;&#21487;&#36801;&#31227;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Model Transferability With Responsive Decision Subjects. (arXiv:2107.05911v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.05911
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#22312;&#21709;&#24212;&#24335;&#21644;&#20132;&#20114;&#24335;&#25968;&#25454;&#20998;&#24067;&#19979;&#65292;&#31639;&#27861;&#39044;&#27979;&#22120;&#30340;&#21487;&#36801;&#31227;&#24615;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#24615;&#33021;&#24046;&#36317;&#30340;&#19978;&#30028;&#21644;&#20998;&#31867;&#22120;&#24517;&#39035;&#28385;&#36275;&#30340;&#26435;&#34913;&#30340;&#19979;&#30028;&#65292;&#24182;&#21051;&#30011;&#20102;&#39044;&#27979;&#22120;&#12289;&#31574;&#30053;&#31354;&#38388;&#36873;&#25321;&#21644;&#20559;&#31227;&#24615;&#36136;&#23545;&#21487;&#36801;&#31227;&#24615;&#31243;&#24230;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#31867;&#26234;&#33021;&#20915;&#31574;&#21442;&#19982;&#30340;&#19968;&#20123;&#25968;&#25454;&#38598;&#19978;&#65292;&#22914;&#26524;&#23384;&#22312;&#19968;&#20010;&#20934;&#30830;&#30340;&#31639;&#27861;&#39044;&#27979;&#22120;&#65292;&#37027;&#20040;&#23427;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#19978;&#30340;&#20934;&#30830;&#29575;&#26159;&#21542;&#20250;&#34987;&#20445;&#25345;&#21602;&#65311;&#22312;&#25105;&#20204;&#30340;&#32972;&#26223;&#35774;&#32622;&#20013;&#65292;&#19968;&#20010;&#20195;&#29702;&#20154;&#25110;&#29992;&#25143;&#23545;&#24212;&#20110;&#20174;&#20998;&#24067;$D$&#20013;&#25277;&#26679;&#24471;&#21040;&#30340;&#19968;&#20010;&#26679;&#26412;$(X,Y)$&#65292;&#24182;&#38754;&#20020;&#30528;&#27169;&#22411;$h$&#21450;&#20854;&#20998;&#31867;&#32467;&#26524;$h(X)$&#12290;&#20195;&#29702;&#20154;&#21487;&#20197;&#20462;&#25913;$X$&#20197;&#36866;&#24212;$h$&#65292;&#36825;&#23558;&#23545;$(X,Y)$&#20135;&#29983;&#20998;&#24067;&#20559;&#31227;&#12290;&#25105;&#20204;&#30340;&#35774;&#32622;&#26159;&#21463;&#21040;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#34987;&#20154;&#31867;&#20195;&#29702;&#20154;&#20351;&#29992;&#24182;&#26368;&#32456;&#38754;&#23545;&#21709;&#24212;&#24335;&#21644;&#20132;&#20114;&#24335;&#25968;&#25454;&#20998;&#24067;&#30340;&#24212;&#29992;&#26696;&#20363;&#30340;&#21551;&#21457;&#12290;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#22312;&#21487;&#29992;&#28304;&#20998;&#24067;&#65288;&#25968;&#25454;&#65289;&#19978;&#35757;&#32451;&#30340;&#24615;&#33021;&#22914;&#20309;&#36716;&#21270;&#20026;&#20854;&#35825;&#23548;&#22495;&#20013;&#24615;&#33021;&#26469;&#31995;&#32479;&#21270;&#22320;&#35752;&#35770;&#27169;&#22411;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#30001;&#20110;&#35825;&#23548;&#22495;&#20559;&#31227;&#32780;&#23548;&#33268;&#30340;&#24615;&#33021;&#24046;&#36317;&#30340;&#19978;&#30028;&#65292;&#20197;&#21450;&#20998;&#31867;&#22120;&#24517;&#39035;&#28385;&#36275;&#30340;&#26435;&#34913;&#30340;&#19979;&#30028;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21051;&#30011;&#20102;&#39044;&#27979;&#22120;&#12289;&#31574;&#30053;&#31354;&#38388;&#36873;&#25321;&#21644;&#20559;&#31227;&#24615;&#36136;&#23545;&#21487;&#36801;&#31227;&#24615;&#31243;&#24230;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given an algorithmic predictor that is accurate on some source population consisting of strategic human decision subjects, will it remain accurate if the population respond to it? In our setting, an agent or a user corresponds to a sample $(X,Y)$ drawn from a distribution $\cal{D}$ and will face a model $h$ and its classification result $h(X)$. Agents can modify $X$ to adapt to $h$, which will incur a distribution shift on $(X,Y)$. Our formulation is motivated by applications where the deployed machine learning models are subjected to human agents, and will ultimately face responsive and interactive data distributions. We formalize the discussions of the transferability of a model by studying how the performance of the model trained on the available source distribution (data) would translate to the performance on its induced domain. We provide both upper bounds for the performance gap due to the induced domain shift, as well as lower bounds for the trade-offs that a classifier has to s
&lt;/p&gt;</description></item><item><title>Chanakya&#26159;&#19968;&#31181;&#23398;&#20064;&#36817;&#20284;&#25191;&#34892;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#23454;&#26102;&#24863;&#30693;&#20013;&#24179;&#34913;&#31934;&#24230;&#21644;&#24310;&#36831;&#30340;&#35757;&#32451;&#65292;&#33258;&#21160;&#23398;&#20064;&#26435;&#34913;&#30340;&#20915;&#31574;&#65292;&#24182;&#22312;&#22810;&#20010;&#24863;&#30693;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2106.05665</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#23454;&#26102;&#24863;&#30693;&#30340;&#36816;&#34892;&#26102;&#20915;&#31574;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning Runtime Decisions for Adaptive Real-Time Perception. (arXiv:2106.05665v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.05665
&lt;/p&gt;
&lt;p&gt;
Chanakya&#26159;&#19968;&#31181;&#23398;&#20064;&#36817;&#20284;&#25191;&#34892;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#23454;&#26102;&#24863;&#30693;&#20013;&#24179;&#34913;&#31934;&#24230;&#21644;&#24310;&#36831;&#30340;&#35757;&#32451;&#65292;&#33258;&#21160;&#23398;&#20064;&#26435;&#34913;&#30340;&#20915;&#31574;&#65292;&#24182;&#22312;&#22810;&#20010;&#24863;&#30693;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#26102;&#24863;&#30693;&#38656;&#35201;&#35745;&#21010;&#30340;&#36164;&#28304;&#21033;&#29992;&#12290;&#35745;&#31639;&#35268;&#21010;&#22312;&#23454;&#26102;&#24863;&#30693;&#20013;&#21463;&#31934;&#24230;&#21644;&#24310;&#36831;&#20004;&#20010;&#22240;&#32032;&#25511;&#21046;&#12290;&#23384;&#22312;&#21487;&#36816;&#34892;&#26102;&#20915;&#31574;&#65288;&#20363;&#22914;&#36873;&#25321;&#36755;&#20837;&#20998;&#36776;&#29575;&#65289;&#20250;&#24341;&#36215;&#26435;&#34913;&#65292;&#24433;&#21709;&#32473;&#23450;&#30828;&#20214;&#19978;&#30340;&#24615;&#33021;&#65292;&#26469;&#33258;&#22266;&#26377;&#65288;&#20869;&#23481;&#65292;&#20363;&#22914;&#22330;&#26223;&#28151;&#20081;&#65289;&#21644;&#22806;&#22312;&#65288;&#31995;&#32479;&#65292;&#20363;&#22914;&#36164;&#26009;&#20105;&#29992;&#65289;&#29305;&#24449;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102; Chanakya&#65292;&#19968;&#20010;&#23398;&#20064;&#36817;&#20284;&#25191;&#34892;&#26694;&#26550;&#65292;&#23427;&#33258;&#28982;&#22320;&#20174;&#27969;&#24335;&#24863;&#30693;&#33539;&#20363;&#20013;&#34893;&#29983;&#20986;&#26469;&#65292;&#20197;&#33258;&#21160;&#23398;&#20064;&#36825;&#20123;&#26435;&#34913;&#25152;&#24341;&#36215;&#30340;&#20915;&#31574;&#12290;Chanakya &#26159;&#36890;&#36807;&#26032;&#39062;&#30340;&#22870;&#21169;&#35757;&#32451;&#24179;&#34913;&#31934;&#24230;&#21644;&#24310;&#36831;&#26469;&#38544;&#24335;&#22320;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#19981;&#26159;&#36817;&#20284;&#36825;&#20004;&#20010;&#30446;&#26631;&#12290;Chanakya &#21516;&#26102;&#32771;&#34385;&#20869;&#22312;&#21644;&#22806;&#22312;&#32972;&#26223;&#65292;&#20197;&#21450;&#26469;&#33258;&#22810;&#20010;&#35270;&#35282;&#30340;&#39044;&#27979;&#28145;&#24230;&#65292;&#20197;&#23398;&#20064;&#20174;&#22270;&#20687;&#24207;&#21015;&#21040;&#21160;&#24577;&#21487;&#35843;&#25972;&#30340;&#24863;&#30693;&#31639;&#27861;&#27969;&#27700;&#32447;&#30340;&#26144;&#23556;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23398;&#20064;&#20102;&#23454;&#26102;&#24863;&#30693;&#20013;&#30340;&#39640;&#25928;&#20915;&#31574;&#65292;&#24182;&#22312;&#22810;&#20010;&#24863;&#30693;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-time perception requires planned resource utilization. Computational planning in real-time perception is governed by two considerations -- accuracy and latency. There exist run-time decisions (e.g. choice of input resolution) that induce tradeoffs affecting performance on a given hardware, arising from intrinsic (content, e.g. scene clutter) and extrinsic (system, e.g. resource contention) characteristics.  Earlier runtime execution frameworks employed rule-based decision algorithms and operated with a fixed algorithm latency budget to balance these concerns, which is sub-optimal and inflexible. We propose Chanakya, a learned approximate execution framework that naturally derives from the streaming perception paradigm, to automatically learn decisions induced by these tradeoffs instead. Chanakya is trained via novel rewards balancing accuracy and latency implicitly, without approximating either objectives. Chanakya simultaneously considers intrinsic and extrinsic context, and pred
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23545;&#25239;&#25915;&#20987;&#19979;&#30340;UCB&#26368;&#20248;&#25289;&#33218;&#31574;&#30053;&#65292;&#25104;&#26412;&#20026;$\sqrt{\log T}$&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#27492;&#25915;&#20987;&#31574;&#30053;&#36817;&#20046;&#26159;&#26368;&#20248;&#30340;&#12290;</title><link>http://arxiv.org/abs/2008.09312</link><description>&lt;p&gt;
UCB Bandits&#22312;&#23545;&#25239;&#25915;&#20987;&#20013;&#30340;&#36817;&#20046;&#26368;&#20248;&#25915;&#20987;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Near Optimal Adversarial Attack on UCB Bandits. (arXiv:2008.09312v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2008.09312
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23545;&#25239;&#25915;&#20987;&#19979;&#30340;UCB&#26368;&#20248;&#25289;&#33218;&#31574;&#30053;&#65292;&#25104;&#26412;&#20026;$\sqrt{\log T}$&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#27492;&#25915;&#20987;&#31574;&#30053;&#36817;&#20046;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#31181;&#38543;&#26426;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#65292;&#20854;&#20013;&#22870;&#21169;&#21463;&#21040;&#23545;&#25239;&#24615;&#30772;&#22351;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25915;&#20987;&#31574;&#30053;&#65292;&#36890;&#36807;&#25805;&#20316;UCB&#21407;&#21017;&#26469;&#25289;&#21160;&#19968;&#20123;&#38750;&#26368;&#20248;&#30446;&#26631;&#33218;$T-o(T)$&#27425;&#65292;&#32047;&#31215;&#25104;&#26412;&#30340;&#26631;&#24230;&#20026;$\sqrt{\log T}$&#65292;&#20854;&#20013;$T$&#20026;&#22238;&#21512;&#25968;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#32047;&#31215;&#25915;&#20987;&#25104;&#26412;&#30340;&#31532;&#19968;&#20010;&#19979;&#30028;&#12290;&#25105;&#20204;&#30340;&#19979;&#30028;&#19982;&#25105;&#20204;&#30340;&#19978;&#30028;&#21305;&#37197;&#65292;&#38500;&#20102;$\log\log T$&#22240;&#23376;&#65292;&#34920;&#26126;&#25105;&#20204;&#30340;&#25915;&#20987;&#31574;&#30053;&#36817;&#20046;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a stochastic multi-arm bandit problem where rewards are subject to adversarial corruption. We propose a novel attack strategy that manipulates a UCB principle into pulling some non-optimal target arm $T - o(T)$ times with a cumulative cost that scales as $\sqrt{\log T}$, where $T$ is the number of rounds. We also prove the first lower bound on the cumulative attack cost. Our lower bound matches our upper bound up to $\log \log T$ factors, showing our attack to be near optimal.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#32858;&#31867;&#36825;&#19968;&#26080;&#30417;&#30563;&#38382;&#39064;&#19978;&#30340;&#32570;&#38519;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DMoN&#30340;&#26080;&#30417;&#30563;&#27719;&#32858;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#32858;&#31867;&#24674;&#22797;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2006.16904</link><description>&lt;p&gt;
&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22270;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Graph Clustering with Graph Neural Networks. (arXiv:2006.16904v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2006.16904
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#32858;&#31867;&#36825;&#19968;&#26080;&#30417;&#30563;&#38382;&#39064;&#19978;&#30340;&#32570;&#38519;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DMoN&#30340;&#26080;&#30417;&#30563;&#27719;&#32858;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#32858;&#31867;&#24674;&#22797;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#22312;&#35832;&#22914;&#33410;&#28857;&#20998;&#31867;&#21644;&#38142;&#36335;&#39044;&#27979;&#20043;&#31867;&#30340;&#35768;&#22810;&#22270;&#20998;&#26512;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22312;&#22270;&#19978;&#30340;&#19968;&#20123;&#37325;&#35201;&#26080;&#30417;&#30563;&#38382;&#39064;&#65292;&#27604;&#22914;&#22270;&#32858;&#31867;&#65292;&#21364;&#23545;GNN&#30340;&#36827;&#23637;&#26356;&#21152;&#26377;&#25269;&#25239;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#19968;&#32452;&#23454;&#39564;&#65292;&#30740;&#31350;&#20102;&#19981;&#21516;&#20449;&#22122;&#27604;&#24773;&#20917;&#19979;&#30340;&#34920;&#24449;&#25968;&#25454;&#23545;&#20110;&#22270;&#32467;&#26500;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#26041;&#27861;&#22312;&#32858;&#31867;&#26041;&#38754;&#30340;&#24615;&#33021;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#27719;&#32858;&#26041;&#27861;&#65306;&#28145;&#24230;&#27169;&#22359;&#32593;&#32476;(DMoN)&#12290;&#36825;&#31181;&#26041;&#27861;&#21463;&#32858;&#31867;&#36136;&#37327;&#30340;&#27169;&#22359;&#24230;&#24230;&#37327;&#30340;&#21551;&#21457;&#65292;&#33021;&#22815;&#22788;&#29702;&#32858;&#31867;&#24674;&#22797;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have achieved state-of-the-art results on many graph analysis tasks such as node classification and link prediction. However, important unsupervised problems on graphs, such as graph clustering, have proved more resistant to advances in GNNs. Graph clustering has the same overall goal as node pooling in GNNs - does this mean that GNN pooling methods do a good job at clustering graphs?  Surprisingly, the answer is no - current GNN pooling methods often fail to recover the cluster structure in cases where simple baselines, such as k-means applied on learned representations, work well. We investigate further by carefully designing a set of experiments to study different signal-to-noise scenarios both in graph structure and attribute data. To address these methods' poor performance in clustering, we introduce Deep Modularity Networks (DMoN), an unsupervised pooling method inspired by the modularity measure of clustering quality, and show how it tackles recovery
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;Transformer&#26550;&#26500;&#36827;&#34892;&#20195;&#30721;&#39044;&#27979;&#65292;&#36229;&#36234;&#20102;&#20808;&#21069;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#22810;&#31181;&#26041;&#24335;&#23558;&#20195;&#30721;&#32467;&#26500;&#20256;&#36798;&#32473;Transformer&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2003.13848</link><description>&lt;p&gt;
&#23558;&#26641;&#32467;&#26500;&#36755;&#20837;Transformer&#36827;&#34892;&#20195;&#30721;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Code Prediction by Feeding Trees to Transformers. (arXiv:2003.13848v4 [cs.SE] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2003.13848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;Transformer&#26550;&#26500;&#36827;&#34892;&#20195;&#30721;&#39044;&#27979;&#65292;&#36229;&#36234;&#20102;&#20808;&#21069;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#22810;&#31181;&#26041;&#24335;&#23558;&#20195;&#30721;&#32467;&#26500;&#20256;&#36798;&#32473;Transformer&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#23558;&#35821;&#27861;&#26641;&#36755;&#20837;Transformer&#25552;&#39640;&#20102;&#20195;&#30721;&#39044;&#27979;&#65288;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#65289;&#30340;&#20934;&#30830;&#24230;&#65292;&#36827;&#19968;&#27493;&#36229;&#36234;&#20102;&#20808;&#21069;&#30340;&#31070;&#32463;&#21644;&#38750;&#31070;&#32463;&#31995;&#32479;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22810;&#31181;&#23558;&#20195;&#30721;&#32467;&#26500;&#20256;&#36798;&#32473;Transformer&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#26631;&#20934;Python&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
We advance the state-of-the-art in the accuracy of code prediction (next token prediction) used in autocomplete systems. First, we report that using the recently proposed Transformer architecture even out-of-the-box outperforms previous neural and non-neural systems for code prediction. We then show that by making the Transformer architecture aware of the syntactic structure of code, we further increase the margin by which a Transformer-based system outperforms previous systems. With this, it outperforms the accuracy of an RNN-based system (similar to Hellendoorn et al. 2018) by 18.3%, the Deep3 system (Raychev et al 2016) by 14.1%, and an adaptation of Code2Seq (Alon et al., 2018) for code prediction by 14.4%.  We present in the paper several ways of communicating the code structure to the Transformer, which is fundamentally built for processing sequence data. We provide a comprehensive experimental evaluation of our proposal, along with alternative design choices, on a standard Pytho
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#36317;&#31163;&#21152;&#26435;&#22270;&#32593;&#32476;&#22788;&#29702;&#19981;&#35268;&#21017;&#20960;&#20309;&#24418;&#29366;&#25506;&#27979;&#22120;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20854;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#24182;&#21487;&#20197;&#22788;&#29702;&#20107;&#20214;&#30340;&#31232;&#30095;&#24615;&#21644;&#20219;&#24847;&#22797;&#26434;&#30340;&#25506;&#27979;&#22120;&#20960;&#20309;&#24418;&#29366;&#12290;&#35813;&#31639;&#27861;&#20026;&#22788;&#29702;&#19981;&#35268;&#21017;&#25506;&#27979;&#22120;&#20960;&#20309;&#24418;&#29366;&#30340;&#31890;&#23376;&#37325;&#24314;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/1902.07987</link><description>&lt;p&gt;
&#23398;&#20064;&#20351;&#29992;&#36317;&#31163;&#21152;&#26435;&#22270;&#32593;&#32476;&#22788;&#29702;&#19981;&#35268;&#21017;&#25506;&#27979;&#22120;&#20960;&#20309;&#24418;&#29366;
&lt;/p&gt;
&lt;p&gt;
Learning representations of irregular particle-detector geometry with distance-weighted graph networks. (arXiv:1902.07987v2 [physics.data-an] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1902.07987
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#36317;&#31163;&#21152;&#26435;&#22270;&#32593;&#32476;&#22788;&#29702;&#19981;&#35268;&#21017;&#20960;&#20309;&#24418;&#29366;&#25506;&#27979;&#22120;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20854;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#24182;&#21487;&#20197;&#22788;&#29702;&#20107;&#20214;&#30340;&#31232;&#30095;&#24615;&#21644;&#20219;&#24847;&#22797;&#26434;&#30340;&#25506;&#27979;&#22120;&#20960;&#20309;&#24418;&#29366;&#12290;&#35813;&#31639;&#27861;&#20026;&#22788;&#29702;&#19981;&#35268;&#21017;&#25506;&#27979;&#22120;&#20960;&#20309;&#24418;&#29366;&#30340;&#31890;&#23376;&#37325;&#24314;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#31890;&#23376;&#37325;&#24314;&#30340;&#32972;&#26223;&#19979;&#20351;&#29992;&#22270;&#32593;&#32476;&#22788;&#29702;&#19981;&#35268;&#21017;&#20960;&#20309;&#24418;&#29366;&#25506;&#27979;&#22120;&#30340;&#26041;&#27861;&#12290;&#30001;&#20110;&#22270;&#32593;&#32476;&#30340;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#65292;&#23427;&#21487;&#20197;&#21033;&#29992;&#20840;&#37096;&#25506;&#27979;&#22120;&#30340;&#32454;&#33410;&#65292;&#21516;&#26102;&#26412;&#22320;&#31649;&#29702;&#20107;&#20214;&#31232;&#30095;&#24615;&#21644;&#20219;&#24847;&#22797;&#26434;&#30340;&#25506;&#27979;&#22120;&#20960;&#20309;&#24418;&#29366;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#31181;&#36317;&#31163;&#21152;&#26435;&#22270;&#32593;&#32476;&#32467;&#26500;&#65292;&#34987;&#31216;&#20026;GarNet&#21644;GravNet&#23618;&#65292;&#24182;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;&#20856;&#22411;&#30340;&#31890;&#23376;&#37325;&#24314;&#20219;&#21153;&#12290;&#26032;&#26550;&#26500;&#30340;&#34920;&#29616;&#22312;&#27169;&#25311;&#31890;&#23376;&#30456;&#20114;&#20316;&#29992;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#20351;&#29992;&#20102;&#39640;&#24230;&#32454;&#33268;&#30340;&#21345;&#20177;&#35745;&#25968;&#22120;&#29609;&#20855;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21463;&#21040;&#20102;&#20026;&#39640;&#20142;&#24230;LHC&#38454;&#27573;&#23433;&#35013;&#22312;CMS&#25506;&#27979;&#22120;&#20013;&#30340;&#31471;&#30422;&#21345;&#20177;&#35745;&#25968;&#22120;&#30340;&#21551;&#21457;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#33021;&#37327;&#27785;&#31215;&#30340;&#32858;&#31867;&#65292;&#36825;&#26159; calorimetric &#31890;&#23376;&#37325;&#24314;&#30340;&#22522;&#30784;&#65292;&#24182;&#25552;&#20379;&#20102;&#19982;&#20854;&#20182;&#26041;&#27861;&#30340;&#23450;&#37327;&#27604;&#36739;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#20026;&#29616;&#26377;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#36259;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#20026;&#22788;&#29702;&#19981;&#35268;&#21017;&#25506;&#27979;&#22120;&#20960;&#20309;&#24418;&#29366;&#30340;&#31890;&#23376;&#37325;&#24314;&#20219;&#21153;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore the use of graph networks to deal with irregular-geometry detectors in the context of particle reconstruction. Thanks to their representation-learning capabilities, graph networks can exploit the full detector granularity, while natively managing the event sparsity and arbitrarily complex detector geometries. We introduce two distance-weighted graph network architectures, dubbed GarNet and GravNet layers, and apply them to a typical particle reconstruction task. The performance of the new architectures is evaluated on a data set of simulated particle interactions on a toy model of a highly granular calorimeter, loosely inspired by the endcap calorimeter to be installed in the CMS detector for the High-Luminosity LHC phase. We study the clustering of energy depositions, which is the basis for calorimetric particle reconstruction, and provide a quantitative comparison to alternative approaches. The proposed algorithms provide an interesting alternative to existing methods, off
&lt;/p&gt;</description></item></channel></rss>