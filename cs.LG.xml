<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#20855;&#26377;&#28508;&#21464;&#37327;&#30340;&#22240;&#26524;&#32467;&#26500;&#20272;&#35745;&#30340;&#24191;&#20041;&#29420;&#31435;&#22122;&#22768;&#65288;GIN&#65289;&#26465;&#20214;&#65292;&#24182;&#32473;&#20986;&#20102;&#32447;&#24615;&#38750;&#39640;&#26031;&#26080;&#29615;&#22240;&#26524;&#27169;&#22411;&#20013;&#28385;&#36275;GIN&#26465;&#20214;&#30340;&#22270;&#24418;&#21028;&#25454;&#12290;</title><link>http://arxiv.org/abs/2308.06718</link><description>&lt;p&gt;
&#20855;&#26377;&#28508;&#21464;&#37327;&#30340;&#22240;&#26524;&#32467;&#26500;&#20272;&#35745;&#30340;&#24191;&#20041;&#29420;&#31435;&#22122;&#22768;&#26465;&#20214;
&lt;/p&gt;
&lt;p&gt;
Generalized Independent Noise Condition for Estimating Causal Structure with Latent Variables. (arXiv:2308.06718v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06718
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#20855;&#26377;&#28508;&#21464;&#37327;&#30340;&#22240;&#26524;&#32467;&#26500;&#20272;&#35745;&#30340;&#24191;&#20041;&#29420;&#31435;&#22122;&#22768;&#65288;GIN&#65289;&#26465;&#20214;&#65292;&#24182;&#32473;&#20986;&#20102;&#32447;&#24615;&#38750;&#39640;&#26031;&#26080;&#29615;&#22240;&#26524;&#27169;&#22411;&#20013;&#28385;&#36275;GIN&#26465;&#20214;&#30340;&#22270;&#24418;&#21028;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#23384;&#22312;&#28508;&#21464;&#37327;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#22240;&#26524;&#32467;&#26500;&#30340;&#25361;&#25112;&#24615;&#20219;&#21153;&#65292;&#21253;&#25324;&#23450;&#20301;&#28508;&#21464;&#37327;&#24182;&#30830;&#23450;&#23427;&#20204;&#30340;&#25968;&#37327;&#65292;&#20197;&#21450;&#35782;&#21035;&#28508;&#21464;&#37327;&#21644;&#35266;&#27979;&#21464;&#37327;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#21253;&#21547;&#28508;&#21464;&#37327;&#30340;&#32447;&#24615;&#38750;&#39640;&#26031;&#26080;&#29615;&#22240;&#26524;&#27169;&#22411;&#30340;&#24191;&#20041;&#29420;&#31435;&#22122;&#22768;&#65288;GIN&#65289;&#26465;&#20214;&#65292;&#35813;&#26465;&#20214;&#24314;&#31435;&#20102;&#26576;&#20123;&#27979;&#37327;&#21464;&#37327;&#30340;&#32447;&#24615;&#32452;&#21512;&#19982;&#20854;&#20182;&#27979;&#37327;&#21464;&#37327;&#20043;&#38388;&#30340;&#29420;&#31435;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;&#20004;&#20010;&#35266;&#27979;&#38543;&#26426;&#21521;&#37327; $\bf{Y}$ &#21644; $\bf{Z}$&#65292;&#24403;&#19988;&#20165;&#24403; $\omega^{\intercal}\mathbf{Y}$ &#21644; $\mathbf{Z}$ &#26159;&#29420;&#31435;&#30340;&#26102;&#65292;GIN &#25104;&#31435;&#65292;&#20854;&#20013; $\omega$ &#26159;&#30001; $\mathbf{Y}$ &#21644; $\mathbf{Z}$ &#20043;&#38388;&#30340;&#20132;&#21449;&#21327;&#26041;&#24046;&#30830;&#23450;&#30340;&#38750;&#38646;&#21442;&#25968;&#21521;&#37327;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#32447;&#24615;&#38750;&#39640;&#26031;&#26080;&#29615;&#22240;&#26524;&#27169;&#22411;&#20013; GIN &#26465;&#20214;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#22270;&#24418;&#21028;&#25454;&#12290;&#31616;&#35328;&#20043;&#65292;GIN &#24847;&#21619;&#30528;&#23384;&#22312;&#19968;&#20010;&#22806;&#28304;&#30340;...
&lt;/p&gt;
&lt;p&gt;
We investigate the challenging task of learning causal structure in the presence of latent variables, including locating latent variables and determining their quantity, and identifying causal relationships among both latent and observed variables. To address this, we propose a Generalized Independent Noise (GIN) condition for linear non-Gaussian acyclic causal models that incorporate latent variables, which establishes the independence between a linear combination of certain measured variables and some other measured variables. Specifically, for two observed random vectors $\bf{Y}$ and $\bf{Z}$, GIN holds if and only if $\omega^{\intercal}\mathbf{Y}$ and $\mathbf{Z}$ are independent, where $\omega$ is a non-zero parameter vector determined by the cross-covariance between $\mathbf{Y}$ and $\mathbf{Z}$. We then give necessary and sufficient graphical criteria of the GIN condition in linear non-Gaussian acyclic causal models. Roughly speaking, GIN implies the existence of an exogenous se
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#33258;&#31169;&#23398;&#20064;&#20195;&#29702;&#21644;&#23398;&#20064;&#22996;&#25176;&#20154;&#20043;&#38388;&#30340;&#37325;&#22797;&#23545;&#33258;&#36873;&#28216;&#25103;&#65292;&#25506;&#32034;&#22914;&#20309;&#20272;&#35745;&#21644;&#28608;&#21169;&#20855;&#26377;&#38544;&#34255;&#22870;&#21169;&#30340;&#19981;&#23436;&#20840;&#30693;&#35782;&#20195;&#29702;&#12290;</title><link>http://arxiv.org/abs/2308.06717</link><description>&lt;p&gt;
&#20272;&#35745;&#21644;&#28608;&#21169;&#20855;&#26377;&#38544;&#34255;&#22870;&#21169;&#30340;&#19981;&#23436;&#20840;&#30693;&#35782;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Estimating and Incentivizing Imperfect-Knowledge Agents with Hidden Rewards. (arXiv:2308.06717v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06717
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#33258;&#31169;&#23398;&#20064;&#20195;&#29702;&#21644;&#23398;&#20064;&#22996;&#25176;&#20154;&#20043;&#38388;&#30340;&#37325;&#22797;&#23545;&#33258;&#36873;&#28216;&#25103;&#65292;&#25506;&#32034;&#22914;&#20309;&#20272;&#35745;&#21644;&#28608;&#21169;&#20855;&#26377;&#38544;&#34255;&#22870;&#21169;&#30340;&#19981;&#23436;&#20840;&#30693;&#35782;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#36341;&#20013;&#65292;&#28608;&#21169;&#25552;&#20379;&#32773;&#65288;&#21363;&#22996;&#25176;&#20154;&#65289;&#36890;&#24120;&#26080;&#27861;&#35266;&#23519;&#21463;&#21040;&#28608;&#21169;&#30340;&#20195;&#29702;&#30340;&#22870;&#21169;&#23454;&#29616;&#24773;&#20917;&#65292;&#36825;&#19982;&#35768;&#22810;&#20808;&#21069;&#30740;&#31350;&#36807;&#30340;&#22996;&#25176;&#20154;-&#20195;&#29702;&#27169;&#22411;&#24418;&#25104;&#20102;&#23545;&#27604;&#12290;&#36825;&#31181;&#20449;&#24687;&#19981;&#23545;&#31216;&#24615;&#20351;&#22996;&#25176;&#20154;&#20165;&#36890;&#36807;&#35266;&#23519;&#20195;&#29702;&#30340;&#20915;&#31574;&#23601;&#35201;&#22987;&#32456;&#20272;&#35745;&#20195;&#29702;&#30340;&#26410;&#30693;&#22870;&#21169;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#65292;&#24403;&#20195;&#29702;&#38656;&#35201;&#23398;&#20064;&#33258;&#24049;&#30340;&#22870;&#21169;&#26102;&#65292;&#36825;&#20010;&#25361;&#25112;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#36825;&#31181;&#22797;&#26434;&#24773;&#20917;&#22312;&#21508;&#31181;&#29616;&#23454;&#22330;&#26223;&#20013;&#34987;&#35266;&#23519;&#21040;&#65292;&#20174;&#21487;&#20877;&#29983;&#33021;&#28304;&#20648;&#23384;&#21512;&#21516;&#21040;&#20010;&#24615;&#21270;&#21307;&#30103;&#20445;&#20581;&#28608;&#21169;&#12290;&#22240;&#27492;&#65292;&#23427;&#19981;&#20165;&#25552;&#20379;&#20102;&#26377;&#36259;&#30340;&#29702;&#35770;&#38382;&#39064;&#65292;&#32780;&#19988;&#20855;&#26377;&#24191;&#27867;&#30340;&#23454;&#38469;&#30456;&#20851;&#24615;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#20010;&#33258;&#31169;&#23398;&#20064;&#20195;&#29702;&#21644;&#23398;&#20064;&#22996;&#25176;&#20154;&#20043;&#38388;&#30340;&#37325;&#22797;&#23545;&#33258;&#36873;&#28216;&#25103;&#12290;&#20195;&#29702;&#35299;&#20915;&#22810;&#33218;&#32769;&#34382;&#26426;&#65288;MAB&#65289;&#38382;&#39064;&#65292;&#20197;&#26368;&#22823;&#21270;&#20182;&#20204;&#39044;&#26399;&#30340;&#22870;&#21169;&#21644;&#28608;&#21169;&#12290;&#38500;&#20195;&#29702;&#30340;&#23398;&#20064;&#22806;&#65292;&#22996;&#25176;&#20154;&#36824;&#35757;&#32451;&#20102;&#19968;&#20010;&#24182;&#34892;&#31639;&#27861;&#65292;&#24182;&#38754;&#20020;&#19968;&#20010;&#25240;&#20013;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
In practice, incentive providers (i.e., principals) often cannot observe the reward realizations of incentivized agents, which is in contrast to many principal-agent models that have been previously studied. This information asymmetry challenges the principal to consistently estimate the agent's unknown rewards by solely watching the agent's decisions, which becomes even more challenging when the agent has to learn its own rewards. This complex setting is observed in various real-life scenarios ranging from renewable energy storage contracts to personalized healthcare incentives. Hence, it offers not only interesting theoretical questions but also wide practical relevance. This paper explores a repeated adverse selection game between a self-interested learning agent and a learning principal. The agent tackles a multi-armed bandit (MAB) problem to maximize their expected reward plus incentive. On top of the agent's learning, the principal trains a parallel algorithm and faces a trade-of
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22270;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22788;&#29702;&#23384;&#22312;&#20998;&#24067;&#22806;&#33410;&#28857;&#65288;OOD nodes&#65289;&#30340;&#22330;&#26223;&#12290;&#20316;&#32773;&#23450;&#20041;&#20102;&#20998;&#24067;&#22806;&#33410;&#28857;&#65292;&#24182;&#35774;&#23450;&#20102;&#20004;&#20010;&#20219;&#21153;&#65306;&#26816;&#27979;&#19981;&#23646;&#20110;&#24050;&#30693;&#20998;&#24067;&#30340;&#33410;&#28857;&#65292;&#24182;&#23545;&#21097;&#20313;&#33410;&#28857;&#36827;&#34892;&#20998;&#31867;&#12290;&#20182;&#20204;&#36890;&#36807;&#25552;&#20986;&#30340;Out-of-Distribution Graph Attention Network (OODGAT)&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.06714</link><description>&lt;p&gt;
&#23384;&#22312;&#20998;&#24067;&#22806;&#33410;&#28857;&#30340;&#22270;&#19978;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning on Graphs with Out-of-Distribution Nodes. (arXiv:2308.06714v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06714
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22270;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22788;&#29702;&#23384;&#22312;&#20998;&#24067;&#22806;&#33410;&#28857;&#65288;OOD nodes&#65289;&#30340;&#22330;&#26223;&#12290;&#20316;&#32773;&#23450;&#20041;&#20102;&#20998;&#24067;&#22806;&#33410;&#28857;&#65292;&#24182;&#35774;&#23450;&#20102;&#20004;&#20010;&#20219;&#21153;&#65306;&#26816;&#27979;&#19981;&#23646;&#20110;&#24050;&#30693;&#20998;&#24067;&#30340;&#33410;&#28857;&#65292;&#24182;&#23545;&#21097;&#20313;&#33410;&#28857;&#36827;&#34892;&#20998;&#31867;&#12290;&#20182;&#20204;&#36890;&#36807;&#25552;&#20986;&#30340;Out-of-Distribution Graph Attention Network (OODGAT)&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26159;&#22312;&#22270;&#19978;&#25191;&#34892;&#39044;&#27979;&#20219;&#21153;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;&#29616;&#26377;&#30340;GNNs&#22312;&#19982;&#22270;&#30456;&#20851;&#30340;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#23384;&#22312;&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#33410;&#28857;&#30340;&#24773;&#20917;&#19979;&#65292;&#21364;&#24456;&#23569;&#24341;&#36215;&#27880;&#24847;&#12290;&#20511;&#37492;CV&#21644;NLP&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#23558;OOD&#33410;&#28857;&#23450;&#20041;&#20026;&#35757;&#32451;&#38598;&#20013;&#26410;&#35265;&#30340;&#33410;&#28857;&#26631;&#31614;&#12290;&#30001;&#20110;&#35768;&#22810;&#32593;&#32476;&#26159;&#30001;&#31243;&#24207;&#33258;&#21160;&#29983;&#25104;&#30340;&#65292;&#29616;&#23454;&#19990;&#30028;&#30340;&#22270;&#24448;&#24448;&#23384;&#22312;&#22122;&#38899;&#65292;&#24182;&#19988;&#21487;&#33021;&#21253;&#21547;&#26469;&#33258;&#26410;&#30693;&#20998;&#24067;&#30340;&#33410;&#28857;&#12290;&#26412;&#25991;&#23450;&#20041;&#20102;&#23384;&#22312;&#20998;&#24067;&#22806;&#33410;&#28857;&#30340;&#22270;&#19978;&#23398;&#20064;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#23436;&#25104;&#20004;&#20010;&#20219;&#21153;&#65306;1&#65289;&#26816;&#27979;&#19981;&#23646;&#20110;&#24050;&#30693;&#20998;&#24067;&#30340;&#33410;&#28857;&#65307;2&#65289;&#23558;&#21097;&#20313;&#30340;&#33410;&#28857;&#20998;&#31867;&#20026;&#24050;&#30693;&#31867;&#21035;&#20043;&#19968;&#12290;&#25105;&#20204;&#35777;&#26126;&#22270;&#20013;&#30340;&#36830;&#25509;&#27169;&#24335;&#23545;&#20110;&#24322;&#24120;&#20540;&#26816;&#27979;&#20855;&#26377;&#20449;&#24687;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;Out-of-Distribution Graph Attention Network (OODGAT)&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are state-of-the-art models for performing prediction tasks on graphs. While existing GNNs have shown great performance on various tasks related to graphs, little attention has been paid to the scenario where out-of-distribution (OOD) nodes exist in the graph during training and inference. Borrowing the concept from CV and NLP, we define OOD nodes as nodes with labels unseen from the training set. Since a lot of networks are automatically constructed by programs, real-world graphs are often noisy and may contain nodes from unknown distributions. In this work, we define the problem of graph learning with out-of-distribution nodes. Specifically, we aim to accomplish two tasks: 1) detect nodes which do not belong to the known distribution and 2) classify the remaining nodes to be one of the known classes. We demonstrate that the connection patterns in graphs are informative for outlier detection, and propose Out-of-Distribution Graph Attention Network (OODGAT)
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#19982;&#19981;&#36830;&#32493;&#24615;&#25429;&#33719;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#24212;&#29992;&#20110;&#20855;&#26377;&#30028;&#38754;&#21644;&#25511;&#21046;&#32422;&#26463;&#30340;&#20248;&#21270;&#25511;&#21046;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23558;&#36793;&#30028;&#21644;&#30028;&#38754;&#26465;&#20214;&#20316;&#20026;&#30828;&#32422;&#26463;&#20197;&#30830;&#20445;&#25968;&#20540;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.06709</link><description>&lt;p&gt;
&#30828;&#32422;&#26463;PINNs&#29992;&#20110;&#30028;&#38754;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
The Hard-Constraint PINNs for Interface Optimal Control Problems. (arXiv:2308.06709v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06709
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#19982;&#19981;&#36830;&#32493;&#24615;&#25429;&#33719;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#24212;&#29992;&#20110;&#20855;&#26377;&#30028;&#38754;&#21644;&#25511;&#21046;&#32422;&#26463;&#30340;&#20248;&#21270;&#25511;&#21046;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23558;&#36793;&#30028;&#21644;&#30028;&#38754;&#26465;&#20214;&#20316;&#20026;&#30828;&#32422;&#26463;&#20197;&#30830;&#20445;&#25968;&#20540;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#19982;&#26368;&#36817;&#24320;&#21457;&#30340;&#19981;&#36830;&#32493;&#24615;&#25429;&#33719;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20855;&#26377;&#30028;&#38754;&#21644;&#19968;&#20123;&#25511;&#21046;&#32422;&#26463;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#20248;&#21270;&#25511;&#21046;&#38382;&#39064;&#30340;&#27714;&#35299;&#12290;&#35813;&#31639;&#27861;&#26080;&#32593;&#26684;&#19988;&#21487;&#25193;&#23637;&#21040;&#19981;&#21516;&#30340;PDE&#65292;&#24182;&#30830;&#20445;&#20005;&#26684;&#28385;&#36275;&#25511;&#21046;&#32422;&#26463;&#12290;&#30001;&#20110;&#36793;&#30028;&#21644;&#30028;&#38754;&#26465;&#20214;&#20197;&#21450;PDE&#37117;&#34987;&#35270;&#20026;&#36719;&#32422;&#26463;&#65292;&#36890;&#36807;&#23558;&#23427;&#20204;&#27719;&#24635;&#21040;&#21152;&#26435;&#25439;&#22833;&#20989;&#25968;&#20013;&#36827;&#34892;&#22788;&#29702;&#65292;&#22240;&#27492;&#38656;&#35201;&#21516;&#26102;&#23398;&#20064;&#23427;&#20204;&#65292;&#24182;&#19988;&#19981;&#33021;&#20445;&#35777;&#36793;&#30028;&#21644;&#30028;&#38754;&#26465;&#20214;&#33021;&#22815;&#23436;&#20840;&#28385;&#36275;&#12290;&#36825;&#31435;&#21363;&#24341;&#36215;&#20102;&#22312;&#30456;&#24212;&#30340;&#25439;&#22833;&#20989;&#25968;&#20013;&#35843;&#25972;&#26435;&#37325;&#21644;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#22256;&#38590;&#24182;&#30830;&#20445;&#25968;&#20540;&#31934;&#24230;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;PINNs&#20013;&#23558;&#36793;&#30028;&#21644;&#30028;&#38754;&#26465;&#20214;&#20316;&#20026;&#30828;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show that the physics-informed neural networks (PINNs), in combination with some recently developed discontinuity capturing neural networks, can be applied to solve optimal control problems subject to partial differential equations (PDEs) with interfaces and some control constraints. The resulting algorithm is mesh-free and scalable to different PDEs, and it ensures the control constraints rigorously. Since the boundary and interface conditions, as well as the PDEs, are all treated as soft constraints by lumping them into a weighted loss function, it is necessary to learn them simultaneously and there is no guarantee that the boundary and interface conditions can be satisfied exactly. This immediately causes difficulties in tuning the weights in the corresponding loss function and training the neural networks. To tackle these difficulties and guarantee the numerical accuracy, we propose to impose the boundary and interface conditions as hard constraints in PINNs by developing a nove
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#29983;&#25104;&#20266;&#38598;&#21512;&#30340;&#38598;&#21512;&#25968;&#25454;&#21516;&#21270;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#27169;&#25311;&#27169;&#22411;&#19981;&#23436;&#32654;&#26102;&#23637;&#31034;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.06708</link><description>&lt;p&gt;
&#21033;&#29992;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#29983;&#25104;&#35266;&#27979;&#24341;&#23548;&#30340;&#38598;&#21512;&#25968;&#25454;&#21516;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Generating observation guided ensembles for data assimilation with denoising diffusion probabilistic model. (arXiv:2308.06708v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#29983;&#25104;&#20266;&#38598;&#21512;&#30340;&#38598;&#21512;&#25968;&#25454;&#21516;&#21270;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#27169;&#25311;&#27169;&#22411;&#19981;&#23436;&#32654;&#26102;&#23637;&#31034;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#29983;&#25104;&#20266;&#38598;&#21512;&#30340;&#38598;&#21512;&#25968;&#25454;&#21516;&#21270;&#26041;&#27861;&#12290;&#30001;&#20110;&#35813;&#27169;&#22411;&#26159;&#38024;&#23545;&#22024;&#26434;&#21644;&#31232;&#30095;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#22240;&#27492;&#35813;&#27169;&#22411;&#21487;&#20197;&#20135;&#29983;&#25509;&#36817;&#35266;&#27979;&#20540;&#30340;&#21457;&#25955;&#38598;&#21512;&#12290;&#24471;&#30410;&#20110;&#29983;&#25104;&#30340;&#38598;&#21512;&#30340;&#26041;&#24046;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#27169;&#25311;&#27169;&#22411;&#19981;&#23436;&#32654;&#26102;&#23637;&#31034;&#20986;&#27604;&#24050;&#24314;&#31435;&#30340;&#38598;&#21512;&#25968;&#25454;&#21516;&#21270;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an ensemble data assimilation method using the pseudo ensembles generated by denoising diffusion probabilistic model. Since the model is trained against noisy and sparse observation data, this model can produce divergent ensembles close to observations. Thanks to the variance in generated ensembles, our proposed method displays better performance than the well-established ensemble data assimilation method when the simulation model is imperfect.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#30456;&#27604;&#20110;&#33258;&#36866;&#24212;&#26799;&#24230;&#26041;&#27861;&#65292;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#36755;&#20837;&#25200;&#21160;&#19979;&#23637;&#29616;&#20986;&#26356;&#22823;&#30340;&#40065;&#26834;&#24615;&#12290;&#36825;&#31181;&#24046;&#24322;&#21487;&#20197;&#24402;&#22240;&#20110;&#33258;&#36866;&#24212;&#26041;&#27861;&#20351;&#29992;&#20102;&#19981;&#30456;&#20851;&#30340;&#39057;&#29575;&#65292;&#23548;&#33268;&#23545;&#25200;&#21160;&#25935;&#24863;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2308.06703</link><description>&lt;p&gt;
&#29702;&#35299;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#21644;&#33258;&#36866;&#24212;&#26799;&#24230;&#26041;&#27861;&#20043;&#38388;&#30340;&#40065;&#26834;&#24615;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Understanding the robustness difference between stochastic gradient descent and adaptive gradient methods. (arXiv:2308.06703v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06703
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#30456;&#27604;&#20110;&#33258;&#36866;&#24212;&#26799;&#24230;&#26041;&#27861;&#65292;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#36755;&#20837;&#25200;&#21160;&#19979;&#23637;&#29616;&#20986;&#26356;&#22823;&#30340;&#40065;&#26834;&#24615;&#12290;&#36825;&#31181;&#24046;&#24322;&#21487;&#20197;&#24402;&#22240;&#20110;&#33258;&#36866;&#24212;&#26041;&#27861;&#20351;&#29992;&#20102;&#19981;&#30456;&#20851;&#30340;&#39057;&#29575;&#65292;&#23548;&#33268;&#23545;&#25200;&#21160;&#25935;&#24863;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#21644;&#33258;&#36866;&#24212;&#26799;&#24230;&#26041;&#27861;&#65292;&#22914;Adam&#21644;RMSProp&#65292;&#22312;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#34429;&#28982;&#20351;&#29992;&#36825;&#20123;&#26041;&#27861;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#26631;&#20934;&#27867;&#21270;&#24615;&#33021;&#20043;&#38388;&#30340;&#24046;&#24322;&#24456;&#23567;&#65292;&#20294;&#20351;&#29992;SGD&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#36755;&#20837;&#25200;&#21160;&#19979;&#34920;&#29616;&#20986;&#26356;&#22823;&#30340;&#40065;&#26834;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#33258;&#28982;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#19981;&#30456;&#20851;&#30340;&#39057;&#29575;&#65292;&#23545;&#36825;&#20123;&#39057;&#29575;&#36827;&#34892;&#21464;&#21160;&#19981;&#20250;&#24433;&#21709;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#33258;&#36866;&#24212;&#26041;&#27861;&#35757;&#32451;&#30340;&#27169;&#22411;&#23545;&#36825;&#20123;&#21464;&#21270;&#26174;&#31034;&#20986;&#25935;&#24863;&#24615;&#65292;&#36825;&#34920;&#26126;&#23427;&#20204;&#20351;&#29992;&#30340;&#19981;&#30456;&#20851;&#39057;&#29575;&#20250;&#23548;&#33268;&#23545;&#25200;&#21160;&#25935;&#24863;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#31181;&#24046;&#24322;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#21644;&#31526;&#21495;&#26799;&#24230;&#19979;&#38477;&#65288;signGD&#65289;&#22312;&#27169;&#25311;&#33258;&#28982;&#20449;&#21495;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#30340;&#23398;&#20064;&#21160;&#24577;&#12290;&#22312;&#19977;&#32500;&#36755;&#20837;&#31354;&#38388;&#20013;&#65292;&#20351;&#29992;GD&#21644;signGD&#20248;&#21270;&#30340;&#27169;&#22411;&#20855;&#26377;&#26631;&#20934;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Stochastic gradient descent (SGD) and adaptive gradient methods, such as Adam and RMSProp, have been widely used in training deep neural networks. We empirically show that while the difference between the standard generalization performance of models trained using these methods is small, those trained using SGD exhibit far greater robustness under input perturbations. Notably, our investigation demonstrates the presence of irrelevant frequencies in natural datasets, where alterations do not affect models' generalization performance. However, models trained with adaptive methods show sensitivity to these changes, suggesting that their use of irrelevant frequencies can lead to solutions sensitive to perturbations. To better understand this difference, we study the learning dynamics of gradient descent (GD) and sign gradient descent (signGD) on a synthetic dataset that mirrors natural signals. With a three-dimensional input space, the models optimized with GD and signGD have standard risk
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#21512;&#25104;&#20266;&#35013;&#25968;&#25454;&#20197;&#25913;&#21892;&#23545;&#33258;&#28982;&#22330;&#26223;&#20013;&#20266;&#35013;&#29289;&#20307;&#26816;&#27979;&#30340;&#26694;&#26550;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#36924;&#30495;&#30340;&#20266;&#35013;&#22270;&#20687;&#65292;&#24182;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#30446;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.06701</link><description>&lt;p&gt;
&#20266;&#35013;&#22270;&#20687;&#21512;&#25104;&#26159;&#25552;&#39640;&#20266;&#35013;&#29289;&#20307;&#26816;&#27979;&#30340;&#20851;&#38190;
&lt;/p&gt;
&lt;p&gt;
Camouflaged Image Synthesis Is All You Need to Boost Camouflaged Detection. (arXiv:2308.06701v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06701
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#21512;&#25104;&#20266;&#35013;&#25968;&#25454;&#20197;&#25913;&#21892;&#23545;&#33258;&#28982;&#22330;&#26223;&#20013;&#20266;&#35013;&#29289;&#20307;&#26816;&#27979;&#30340;&#26694;&#26550;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#36924;&#30495;&#30340;&#20266;&#35013;&#22270;&#20687;&#65292;&#24182;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#30446;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34701;&#20837;&#33258;&#28982;&#22330;&#26223;&#30340;&#20266;&#35013;&#29289;&#20307;&#32473;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26816;&#27979;&#21644;&#21512;&#25104;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#20266;&#35013;&#29289;&#20307;&#26816;&#27979;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#28982;&#32780;&#30001;&#20110;&#25968;&#25454;&#26377;&#38480;&#65292;&#35813;&#30740;&#31350;&#35838;&#39064;&#19968;&#30452;&#21463;&#21040;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#21512;&#25104;&#20266;&#35013;&#25968;&#25454;&#20197;&#22686;&#24378;&#23545;&#33258;&#28982;&#22330;&#26223;&#20013;&#20266;&#35013;&#29289;&#20307;&#26816;&#27979;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#36924;&#30495;&#30340;&#20266;&#35013;&#22270;&#20687;&#65292;&#36825;&#20123;&#22270;&#20687;&#21487;&#20197;&#29992;&#26469;&#35757;&#32451;&#29616;&#26377;&#30340;&#29289;&#20307;&#26816;&#27979;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#20266;&#35013;&#29615;&#22659;&#29983;&#25104;&#22120;&#65292;&#30001;&#20266;&#35013;&#20998;&#24067;&#20998;&#31867;&#22120;&#36827;&#34892;&#30417;&#30563;&#65292;&#21512;&#25104;&#20266;&#35013;&#22270;&#20687;&#65292;&#28982;&#21518;&#23558;&#20854;&#36755;&#20837;&#25105;&#20204;&#30340;&#29983;&#25104;&#22120;&#20197;&#25193;&#23637;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#65288;COD10k&#12289;CAMO&#21644;CHAMELEON&#65289;&#19978;&#30340;&#25928;&#26524;&#36229;&#36807;&#20102;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#23427;&#22312;&#25913;&#21892;&#20266;&#35013;&#29289;&#20307;&#26816;&#27979;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Camouflaged objects that blend into natural scenes pose significant challenges for deep-learning models to detect and synthesize. While camouflaged object detection is a crucial task in computer vision with diverse real-world applications, this research topic has been constrained by limited data availability. We propose a framework for synthesizing camouflage data to enhance the detection of camouflaged objects in natural scenes. Our approach employs a generative model to produce realistic camouflage images, which can be used to train existing object detection models. Specifically, we use a camouflage environment generator supervised by a camouflage distribution classifier to synthesize the camouflage images, which are then fed into our generator to expand the dataset. Our framework outperforms the current state-of-the-art method on three datasets (COD10k, CAMO, and CHAMELEON), demonstrating its effectiveness in improving camouflaged object detection. This approach can serve as a plug-
&lt;/p&gt;</description></item><item><title>SimMatchV2&#26159;&#19968;&#31181;&#21322;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#32422;&#26463;&#65292;&#20174;&#22270;&#30340;&#35282;&#24230;&#35299;&#20915;&#20102;&#21322;&#30417;&#30563;&#22270;&#20687;&#20998;&#31867;&#38382;&#39064;&#12290;&#36890;&#36807;&#33410;&#28857;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#21644;&#29305;&#24449;&#24402;&#19968;&#21270;&#65292;SimMatchV2&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.06692</link><description>&lt;p&gt;
SimMatchV2: &#22522;&#20110;&#22270;&#19968;&#33268;&#24615;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SimMatchV2: Semi-Supervised Learning with Graph Consistency. (arXiv:2308.06692v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06692
&lt;/p&gt;
&lt;p&gt;
SimMatchV2&#26159;&#19968;&#31181;&#21322;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#32422;&#26463;&#65292;&#20174;&#22270;&#30340;&#35282;&#24230;&#35299;&#20915;&#20102;&#21322;&#30417;&#30563;&#22270;&#20687;&#20998;&#31867;&#38382;&#39064;&#12290;&#36890;&#36807;&#33410;&#28857;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#21644;&#29305;&#24449;&#24402;&#19968;&#21270;&#65292;SimMatchV2&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#22270;&#20687;&#20998;&#31867;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#26368;&#22522;&#26412;&#30340;&#38382;&#39064;&#20043;&#19968;&#65292;&#23427;&#26174;&#33879;&#20943;&#23569;&#20102;&#23545;&#20154;&#21147;&#30340;&#38656;&#27714;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861; - SimMatchV2&#65292;&#23427;&#20174;&#22270;&#30340;&#35282;&#24230;&#24314;&#31435;&#20102;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#32422;&#26463;&#12290;&#22312;SimMatchV2&#20013;&#65292;&#25105;&#20204;&#23558;&#26679;&#26412;&#30340;&#22686;&#24378;&#35270;&#22270;&#35270;&#20026;&#19968;&#20010;&#33410;&#28857;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#20010;&#26631;&#31614;&#21644;&#20854;&#30456;&#24212;&#30340;&#34920;&#31034;&#12290;&#19981;&#21516;&#30340;&#33410;&#28857;&#36890;&#36807;&#33410;&#28857;&#34920;&#31034;&#30340;&#30456;&#20284;&#24615;&#36830;&#25509;&#36215;&#26469;&#12290;&#21463;&#21040;&#22270;&#35770;&#20013;&#30340;&#28040;&#24687;&#20256;&#36882;&#21644;&#33410;&#28857;&#20998;&#31867;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22235;&#31181;&#31867;&#22411;&#30340;&#19968;&#33268;&#24615;&#32422;&#26463;&#65292;&#20998;&#21035;&#20026;1) &#33410;&#28857;-&#33410;&#28857;&#19968;&#33268;&#24615;&#65292;2) &#33410;&#28857;-&#36793;&#19968;&#33268;&#24615;&#65292;3) &#36793;-&#36793;&#19968;&#33268;&#24615;&#21644;4) &#36793;-&#33410;&#28857;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#31616;&#21333;&#30340;&#29305;&#24449;&#24402;&#19968;&#21270;&#21487;&#20197;&#20943;&#23567;&#19981;&#21516;&#22686;&#24378;&#35270;&#22270;&#20043;&#38388;&#29305;&#24449;&#33539;&#25968;&#30340;&#24046;&#36317;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;SimMatchV2&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-Supervised image classification is one of the most fundamental problem in computer vision, which significantly reduces the need for human labor. In this paper, we introduce a new semi-supervised learning algorithm - SimMatchV2, which formulates various consistency regularizations between labeled and unlabeled data from the graph perspective. In SimMatchV2, we regard the augmented view of a sample as a node, which consists of a label and its corresponding representation. Different nodes are connected with the edges, which are measured by the similarity of the node representations. Inspired by the message passing and node classification in graph theory, we propose four types of consistencies, namely 1) node-node consistency, 2) node-edge consistency, 3) edge-edge consistency, and 4) edge-node consistency. We also uncover that a simple feature normalization can reduce the gaps of the feature norm between different augmented views, significantly improving the performance of SimMatchV2
&lt;/p&gt;</description></item><item><title>MDB&#26159;&#19968;&#20010;&#35843;&#35797;&#26694;&#26550;&#65292;&#29992;&#20110;&#20114;&#21160;&#26597;&#35810;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#12290;&#23427;&#36890;&#36807;&#38598;&#25104;&#20989;&#25968;&#24335;&#32534;&#31243;&#19982;&#20851;&#31995;&#20195;&#25968;&#65292;&#33021;&#22815;&#24555;&#36895;&#36845;&#20195;&#21644;&#20248;&#21270;&#26597;&#35810;&#65292;&#21457;&#29616;&#21644;&#25551;&#36848;&#38169;&#35823;&#21644;&#27169;&#22411;&#34892;&#20026;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;MDB&#27604;&#20854;&#20182;&#24037;&#20855;&#33021;&#22815;&#23454;&#29616;&#26356;&#24555;&#30340;&#26597;&#35810;&#36895;&#24230;&#21152;&#24555;&#21644;&#26597;&#35810;&#38271;&#24230;&#32553;&#30701;&#12290;</title><link>http://arxiv.org/abs/2308.06686</link><description>&lt;p&gt;
MDB&#65306;&#20114;&#21160;&#26597;&#35810;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MDB: Interactively Querying Datasets and Models. (arXiv:2308.06686v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06686
&lt;/p&gt;
&lt;p&gt;
MDB&#26159;&#19968;&#20010;&#35843;&#35797;&#26694;&#26550;&#65292;&#29992;&#20110;&#20114;&#21160;&#26597;&#35810;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#12290;&#23427;&#36890;&#36807;&#38598;&#25104;&#20989;&#25968;&#24335;&#32534;&#31243;&#19982;&#20851;&#31995;&#20195;&#25968;&#65292;&#33021;&#22815;&#24555;&#36895;&#36845;&#20195;&#21644;&#20248;&#21270;&#26597;&#35810;&#65292;&#21457;&#29616;&#21644;&#25551;&#36848;&#38169;&#35823;&#21644;&#27169;&#22411;&#34892;&#20026;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;MDB&#27604;&#20854;&#20182;&#24037;&#20855;&#33021;&#22815;&#23454;&#29616;&#26356;&#24555;&#30340;&#26597;&#35810;&#36895;&#24230;&#21152;&#24555;&#21644;&#26597;&#35810;&#38271;&#24230;&#32553;&#30701;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#37096;&#32626;&#65292;&#24320;&#21457;&#32773;&#38656;&#35201;&#33021;&#22815;&#31995;&#32479;&#22320;&#35843;&#35797;&#22312;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#20013;&#20986;&#29616;&#30340;&#38169;&#35823;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;MDB&#65292;&#19968;&#20010;&#29992;&#20110;&#20114;&#21160;&#26597;&#35810;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#30340;&#35843;&#35797;&#26694;&#26550;&#12290;MDB&#36890;&#36807;&#23558;&#20989;&#25968;&#24335;&#32534;&#31243;&#19982;&#20851;&#31995;&#20195;&#25968;&#32467;&#21512;&#36215;&#26469;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#23545;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#39044;&#27979;&#30340;&#25968;&#25454;&#24211;&#36827;&#34892;&#34920;&#36798;&#24615;&#26597;&#35810;&#30340;&#24037;&#20855;&#12290;&#26597;&#35810;&#21487;&#37325;&#29992;&#19988;&#26131;&#20110;&#20462;&#25913;&#65292;&#20351;&#24471;&#35843;&#35797;&#20154;&#21592;&#33021;&#22815;&#24555;&#36895;&#36845;&#20195;&#21644;&#20248;&#21270;&#26597;&#35810;&#65292;&#20197;&#21457;&#29616;&#21644;&#25551;&#36848;&#38169;&#35823;&#21644;&#27169;&#22411;&#34892;&#20026;&#12290;&#25105;&#20204;&#22312;&#30446;&#26631;&#26816;&#27979;&#12289;&#20559;&#24046;&#21457;&#29616;&#12289;&#22270;&#20687;&#20998;&#31867;&#21644;&#25968;&#25454;&#22635;&#20805;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102;MDB&#22312;&#33258;&#21160;&#39550;&#39542;&#35270;&#39057;&#12289;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#21307;&#30103;&#35760;&#24405;&#19978;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;MDB&#27604;&#20854;&#20182;&#22522;&#20934;&#27979;&#35797;&#24037;&#20855;&#33021;&#22815;&#23454;&#29616;&#26368;&#39640;10&#20493;&#30340;&#26597;&#35810;&#36895;&#24230;&#21152;&#24555;&#21644;40%&#30340;&#26597;&#35810;&#38271;&#24230;&#32553;&#30701;&#12290;&#22312;&#29992;&#25143;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#24320;&#21457;&#32773;&#33021;&#22815;&#25104;&#21151;&#26500;&#24314;&#22797;&#26434;&#26597;&#35810;&#26469;&#25551;&#36848;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
As models are trained and deployed, developers need to be able to systematically debug errors that emerge in the machine learning pipeline. We present MDB, a debugging framework for interactively querying datasets and models. MDB integrates functional programming with relational algebra to build expressive queries over a database of datasets and model predictions. Queries are reusable and easily modified, enabling debuggers to rapidly iterate and refine queries to discover and characterize errors and model behaviors. We evaluate MDB on object detection, bias discovery, image classification, and data imputation tasks across self-driving videos, large language models, and medical records. Our experiments show that MDB enables up to 10x faster and 40\% shorter queries than other baselines. In a user study, we find developers can successfully construct complex queries that describe errors of machine learning models.
&lt;/p&gt;</description></item><item><title>&#21487;&#20998;&#31163;&#39640;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;SGNN&#65289;&#36890;&#36807;&#21033;&#29992;&#39640;&#26031;&#20989;&#25968;&#30340;&#21487;&#20998;&#31163;&#24615;&#65292;&#23558;&#36755;&#20837;&#25968;&#25454;&#20998;&#25104;&#22810;&#21015;&#24182;&#20381;&#27425;&#39304;&#36865;&#21040;&#24182;&#34892;&#23618;&#20013;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#25351;&#25968;&#32423;&#30340;&#35745;&#31639;&#36895;&#24230;&#25552;&#21319;&#21644;&#32447;&#24615;&#30340;&#25193;&#23637;&#24615;&#12290;&#21516;&#26102;&#65292;SGNN&#33021;&#22815;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20445;&#25345;&#31867;&#20284;&#20110;GRBFNN&#30340;&#20934;&#30830;&#24615;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2308.06679</link><description>&lt;p&gt;
&#21487;&#20998;&#31163;&#39640;&#26031;&#31070;&#32463;&#32593;&#32476;&#65306;&#32467;&#26500;&#12289;&#20998;&#26512;&#21644;&#20989;&#25968;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Separable Gaussian Neural Networks: Structure, Analysis, and Function Approximations. (arXiv:2308.06679v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06679
&lt;/p&gt;
&lt;p&gt;
&#21487;&#20998;&#31163;&#39640;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;SGNN&#65289;&#36890;&#36807;&#21033;&#29992;&#39640;&#26031;&#20989;&#25968;&#30340;&#21487;&#20998;&#31163;&#24615;&#65292;&#23558;&#36755;&#20837;&#25968;&#25454;&#20998;&#25104;&#22810;&#21015;&#24182;&#20381;&#27425;&#39304;&#36865;&#21040;&#24182;&#34892;&#23618;&#20013;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#25351;&#25968;&#32423;&#30340;&#35745;&#31639;&#36895;&#24230;&#25552;&#21319;&#21644;&#32447;&#24615;&#30340;&#25193;&#23637;&#24615;&#12290;&#21516;&#26102;&#65292;SGNN&#33021;&#22815;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20445;&#25345;&#31867;&#20284;&#20110;GRBFNN&#30340;&#20934;&#30830;&#24615;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#24452;&#21521;&#22522;&#20989;&#25968;&#31070;&#32463;&#32593;&#32476;&#65288;GRBFNN&#65289;&#19968;&#30452;&#26159;&#25554;&#20540;&#21644;&#20998;&#31867;&#30340;&#24120;&#35265;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#22312;&#36755;&#20837;&#21521;&#37327;&#30340;&#32500;&#24230;&#36739;&#39640;&#26102;&#65292;&#35745;&#31639;&#37327;&#24456;&#22823;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21069;&#39304;&#32593;&#32476;-&#21487;&#20998;&#31163;&#39640;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;SGNN&#65289;&#65292;&#21033;&#29992;&#39640;&#26031;&#20989;&#25968;&#30340;&#21487;&#20998;&#31163;&#24615;&#65292;&#23558;&#36755;&#20837;&#25968;&#25454;&#20998;&#25104;&#22810;&#21015;&#65292;&#24182;&#20381;&#27425;&#23558;&#23427;&#20204;&#39304;&#36865;&#21040;&#30001;&#21333;&#21464;&#37327;&#39640;&#26031;&#20989;&#25968;&#24418;&#25104;&#30340;&#24182;&#34892;&#23618;&#20013;&#12290;&#36825;&#31181;&#32467;&#26500;&#23558;GRBFNN&#30340;&#31070;&#32463;&#20803;&#25968;&#37327;&#20174;O(N^d)&#38477;&#20302;&#21040;O(dN)&#65292;&#20174;&#32780;&#25351;&#25968;&#32423;&#25552;&#39640;&#20102;SGNN&#30340;&#35745;&#31639;&#36895;&#24230;&#65292;&#24182;&#20351;&#20854;&#33021;&#22815;&#32447;&#24615;&#22320;&#38543;&#30528;&#36755;&#20837;&#32500;&#24230;&#30340;&#22686;&#21152;&#32780;&#25193;&#23637;&#12290;&#27492;&#22806;&#65292;SGNN&#21487;&#20197;&#22312;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#20013;&#20445;&#30041;GRBFNN&#30340;Hessian&#30697;&#38453;&#30340;&#20027;&#35201;&#23376;&#31354;&#38388;&#65292;&#20174;&#32780;&#23454;&#29616;&#19982;GRBFNN&#31867;&#20284;&#27700;&#24179;&#30340;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;SGNN&#21487;&#20197;&#20197;&#19982;GRBFNN&#30456;&#20284;&#30340;&#20934;&#30830;&#24615;&#23454;&#29616;100&#20493;&#30340;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Gaussian-radial-basis function neural network (GRBFNN) has been a popular choice for interpolation and classification. However, it is computationally intensive when the dimension of the input vector is high. To address this issue, we propose a new feedforward network - Separable Gaussian Neural Network (SGNN) by taking advantage of the separable property of Gaussian functions, which splits input data into multiple columns and sequentially feeds them into parallel layers formed by uni-variate Gaussian functions. This structure reduces the number of neurons from O(N^d) of GRBFNN to O(dN), which exponentially improves the computational speed of SGNN and makes it scale linearly as the input dimension increases. In addition, SGNN can preserve the dominant subspace of the Hessian matrix of GRBFNN in gradient descent training, leading to a similar level of accuracy to GRBFNN. It is experimentally demonstrated that SGNN can achieve 100 times speedup with a similar level of accuracy over GR
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29289;&#29702;&#32422;&#26463;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#19981;&#21516;&#25968;&#37327;&#32423;&#25439;&#22833;&#39033;&#30340;&#22810;&#23610;&#24230;&#38382;&#39064;&#12290;&#36890;&#36807;&#37325;&#26032;&#26500;&#24314;&#25439;&#22833;&#20989;&#25968;&#65292;&#24182;&#24212;&#29992;&#19981;&#21516;&#25968;&#37327;&#30340;&#24130;&#36816;&#31639;&#65292;&#20351;&#21508;&#20010;&#25439;&#22833;&#39033;&#22312;&#25968;&#37327;&#32423;&#19978;&#22823;&#33268;&#30456;&#31561;&#12290;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#31181;&#20998;&#32452;&#27491;&#21017;&#21270;&#31574;&#30053;&#26469;&#24212;&#23545;&#22312;&#19981;&#21516;&#23376;&#39046;&#22495;&#20013;&#26174;&#30528;&#21464;&#21270;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.06672</link><description>&lt;p&gt;
&#22522;&#20110;&#29289;&#29702;&#32422;&#26463;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#23610;&#24230;&#27169;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A deep learning framework for multi-scale models based on physics-informed neural networks. (arXiv:2308.06672v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06672
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29289;&#29702;&#32422;&#26463;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#19981;&#21516;&#25968;&#37327;&#32423;&#25439;&#22833;&#39033;&#30340;&#22810;&#23610;&#24230;&#38382;&#39064;&#12290;&#36890;&#36807;&#37325;&#26032;&#26500;&#24314;&#25439;&#22833;&#20989;&#25968;&#65292;&#24182;&#24212;&#29992;&#19981;&#21516;&#25968;&#37327;&#30340;&#24130;&#36816;&#31639;&#65292;&#20351;&#21508;&#20010;&#25439;&#22833;&#39033;&#22312;&#25968;&#37327;&#32423;&#19978;&#22823;&#33268;&#30456;&#31561;&#12290;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#31181;&#20998;&#32452;&#27491;&#21017;&#21270;&#31574;&#30053;&#26469;&#24212;&#23545;&#22312;&#19981;&#21516;&#23376;&#39046;&#22495;&#20013;&#26174;&#30528;&#21464;&#21270;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#32422;&#26463;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19982;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#30340;&#35299;&#30456;&#32467;&#21512;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#26377;&#24076;&#26395;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#29992;&#20110;&#25968;&#20540;&#27714;&#35299;PDE&#12290;&#38754;&#23545;&#19968;&#31867;&#21253;&#21547;&#19981;&#21516;&#25968;&#37327;&#32423;&#25439;&#22833;&#39033;&#30340;&#22810;&#23610;&#24230;&#38382;&#39064;&#65292;&#23545;&#20110;&#26631;&#20934;PINN&#26041;&#27861;&#26469;&#35828;&#65292;&#33719;&#24471;&#21487;&#29992;&#30340;&#39044;&#27979;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#37325;&#26500;&#25439;&#22833;&#20989;&#25968;&#26469;&#35299;&#20915;&#22810;&#23610;&#24230;&#38382;&#39064;&#30340;&#26032;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#22522;&#20110;&#26631;&#20934;PINN&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23545;&#19981;&#21516;&#37327;&#32423;&#30340;&#25439;&#22833;&#39033;&#24212;&#29992;&#19981;&#21516;&#25968;&#37327;&#30340;&#24130;&#36816;&#31639;&#26469;&#20462;&#25913;&#26631;&#20934;PINN&#26041;&#27861;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20174;&#32780;&#20351;&#26500;&#25104;&#25439;&#22833;&#20989;&#25968;&#30340;&#21508;&#20010;&#25439;&#22833;&#39033;&#22312;&#25968;&#37327;&#32423;&#19978;&#22823;&#33268;&#30456;&#31561;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#32452;&#27491;&#21017;&#21270;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#21487;&#20197;&#24456;&#22909;&#22320;&#22788;&#29702;&#22312;&#19981;&#21516;&#23376;&#39046;&#22495;&#20013;&#26174;&#30528;&#21464;&#21270;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics-informed neural networks (PINN) combine deep neural networks with the solution of partial differential equations (PDEs), creating a new and promising research area for numerically solving PDEs. Faced with a class of multi-scale problems that include loss terms of different orders of magnitude in the loss function, it is challenging for standard PINN methods to obtain an available prediction. In this paper, we propose a new framework for solving multi-scale problems by reconstructing the loss function. The framework is based on the standard PINN method, and it modifies the loss function of the standard PINN method by applying different numbers of power operations to the loss terms of different magnitudes, so that the individual loss terms composing the loss function have approximately the same order of magnitude among themselves. In addition, we give a grouping regularization strategy, and this strategy can deal well with the problem which varies significantly in different subdo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#20013;&#30340;&#23567;&#25209;&#37327;&#22122;&#38899;&#20250;&#20351;&#35299;&#20915;&#26041;&#26696;&#21521;&#24179;&#34913;&#35299;&#38752;&#36817;&#65292;&#21482;&#35201;&#25439;&#22833;&#20989;&#25968;&#21253;&#21547;&#37325;&#26032;&#32553;&#25918;&#23545;&#31216;&#24615;&#12290;&#21033;&#29992;&#36825;&#20010;&#32467;&#26524;&#65292;&#25105;&#20204;&#23548;&#20986;&#20102;&#23545;&#35282;&#32447;&#24615;&#32593;&#32476;&#30340;&#38543;&#26426;&#26799;&#24230;&#27969;&#31283;&#24577;&#20998;&#24067;&#65292;&#35813;&#20998;&#24067;&#23637;&#31034;&#20102;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#29616;&#35937;&#12290;&#36825;&#20123;&#21457;&#29616;&#25581;&#31034;&#20102;&#21160;&#24577;&#26799;&#24230;&#19979;&#38477;&#27861;&#22312;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24037;&#20316;&#21407;&#29702;&#12290;</title><link>http://arxiv.org/abs/2308.06671</link><description>&lt;p&gt;
&#21160;&#24577;&#26799;&#24230;&#19979;&#38477;&#27861;&#30340;&#24179;&#34913;&#27861;&#21017;&#19982;&#31283;&#24577;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
Law of Balance and Stationary Distribution of Stochastic Gradient Descent. (arXiv:2308.06671v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#20013;&#30340;&#23567;&#25209;&#37327;&#22122;&#38899;&#20250;&#20351;&#35299;&#20915;&#26041;&#26696;&#21521;&#24179;&#34913;&#35299;&#38752;&#36817;&#65292;&#21482;&#35201;&#25439;&#22833;&#20989;&#25968;&#21253;&#21547;&#37325;&#26032;&#32553;&#25918;&#23545;&#31216;&#24615;&#12290;&#21033;&#29992;&#36825;&#20010;&#32467;&#26524;&#65292;&#25105;&#20204;&#23548;&#20986;&#20102;&#23545;&#35282;&#32447;&#24615;&#32593;&#32476;&#30340;&#38543;&#26426;&#26799;&#24230;&#27969;&#31283;&#24577;&#20998;&#24067;&#65292;&#35813;&#20998;&#24067;&#23637;&#31034;&#20102;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#29616;&#35937;&#12290;&#36825;&#20123;&#21457;&#29616;&#25581;&#31034;&#20102;&#21160;&#24577;&#26799;&#24230;&#19979;&#38477;&#27861;&#22312;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24037;&#20316;&#21407;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#31639;&#27861;&#26159;&#25105;&#20204;&#29992;&#20110;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#24456;&#38590;&#29702;&#35299;SGD&#22914;&#20309;&#22312;&#31070;&#32463;&#32593;&#32476;&#30340;&#38750;&#32447;&#24615;&#21644;&#36864;&#21270;&#30340;&#25439;&#22833;&#26354;&#38754;&#20013;&#36827;&#34892;&#23548;&#33322;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;SGD&#30340;&#23567;&#25209;&#37327;&#22122;&#38899;&#21487;&#20197;&#20351;&#35299;&#20915;&#26041;&#26696;&#21521;&#24179;&#34913;&#35299;&#38752;&#36817;&#65292;&#21482;&#35201;&#25439;&#22833;&#20989;&#25968;&#21253;&#21547;&#19968;&#20010;&#37325;&#26032;&#32553;&#25918;&#23545;&#31216;&#24615;&#12290;&#30001;&#20110;&#31616;&#21333;&#25193;&#25955;&#36807;&#31243;&#21644;SGD&#21160;&#21147;&#23398;&#30340;&#24046;&#24322;&#22312;&#23545;&#31216;&#24615;&#23384;&#22312;&#26102;&#26368;&#37325;&#35201;&#65292;&#25105;&#20204;&#30340;&#29702;&#35770;&#34920;&#26126;&#65292;&#25439;&#22833;&#20989;&#25968;&#30340;&#23545;&#31216;&#24615;&#26159;&#20102;&#35299;SGD&#24037;&#20316;&#26041;&#24335;&#30340;&#37325;&#35201;&#32447;&#32034;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#32467;&#26524;&#24212;&#29992;&#20110;&#23548;&#20986;&#20855;&#26377;&#20219;&#24847;&#28145;&#24230;&#21644;&#23485;&#24230;&#30340;&#23545;&#35282;&#32447;&#24615;&#32593;&#32476;&#30340;&#38543;&#26426;&#26799;&#24230;&#27969;&#30340;&#31283;&#24577;&#20998;&#24067;&#12290;&#31283;&#24577;&#20998;&#24067;&#23637;&#29616;&#20102;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#29616;&#35937;&#65292;&#22914;&#30456;&#21464;&#12289;&#30772;&#22351;&#30340;&#36941;&#21382;&#24615;&#21644;&#27874;&#21160;&#21453;&#36716;&#12290;&#36825;&#20123;&#29616;&#35937;&#20165;&#22312;&#28145;&#23618;&#32593;&#32476;&#20013;&#23384;&#22312;&#65292;&#34920;&#26126;&#20102;&#19968;&#31181;&#22522;&#26412;&#30340;&#26032;&#30340;&#21152;&#28145;&#35757;&#32451;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
The stochastic gradient descent (SGD) algorithm is the algorithm we use to train neural networks. However, it remains poorly understood how the SGD navigates the highly nonlinear and degenerate loss landscape of a neural network. In this work, we prove that the minibatch noise of SGD regularizes the solution towards a balanced solution whenever the loss function contains a rescaling symmetry. Because the difference between a simple diffusion process and SGD dynamics is the most significant when symmetries are present, our theory implies that the loss function symmetries constitute an essential probe of how SGD works. We then apply this result to derive the stationary distribution of stochastic gradient flow for a diagonal linear network with arbitrary depth and width. The stationary distribution exhibits complicated nonlinear phenomena such as phase transitions, broken ergodicity, and fluctuation inversion. These phenomena are shown to exist uniquely in deep networks, implying a fundam
&lt;/p&gt;</description></item><item><title>&#20256;&#32479;&#20892;&#19994;&#31995;&#32479;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#32780;&#22522;&#30784;&#27169;&#22411;&#22312;&#35821;&#35328;&#21644;&#35270;&#35273;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#22522;&#30784;&#27169;&#22411;&#22312;&#26234;&#33021;&#20892;&#19994;&#39046;&#22495;&#30340;&#28508;&#21147;&#21644;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.06668</link><description>&lt;p&gt;
&#26234;&#33021;&#20892;&#19994;&#20013;&#30340;&#22522;&#30784;&#27169;&#22411;&#65306;&#22522;&#30784;&#30693;&#35782;&#12289;&#26426;&#36935;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Foundation Models in Smart Agriculture: Basics, Opportunities, and Challenges. (arXiv:2308.06668v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06668
&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#20892;&#19994;&#31995;&#32479;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#32780;&#22522;&#30784;&#27169;&#22411;&#22312;&#35821;&#35328;&#21644;&#35270;&#35273;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#22522;&#30784;&#27169;&#22411;&#22312;&#26234;&#33021;&#20892;&#19994;&#39046;&#22495;&#30340;&#28508;&#21147;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#21313;&#24180;&#38388;&#65292;&#20892;&#19994;&#31995;&#32479;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24471;&#21040;&#20102;&#24555;&#36895;&#21457;&#23637;&#65292;&#23637;&#31034;&#20986;&#22312;&#21508;&#31181;&#20892;&#19994;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;/&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20855;&#26377;&#19968;&#23450;&#30340;&#23616;&#38480;&#24615;&#65306;&#23427;&#20204;&#20005;&#37325;&#20381;&#36182;&#20110;&#26114;&#36149;&#30340;&#12289;&#38590;&#20197;&#33719;&#21462;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#38656;&#35201;&#19987;&#19994;&#30693;&#35782;&#36827;&#34892;&#24320;&#21457;&#21644;&#32500;&#25252;&#65292;&#32780;&#19988;&#22823;&#22810;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#65292;&#32570;&#20047;&#27867;&#21270;&#33021;&#21147;&#12290;&#26368;&#36817;&#65292;&#22522;&#30784;&#27169;&#22411;&#22312;&#35821;&#35328;&#21644;&#35270;&#35273;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#36328;&#36234;&#20102;&#21508;&#20010;&#39046;&#22495;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#26469;&#33258;&#22810;&#20010;&#39046;&#22495;&#21644;&#27169;&#24577;&#30340;&#22823;&#37327;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#19968;&#26086;&#35757;&#32451;&#23436;&#25104;&#65292;&#23427;&#20204;&#21487;&#20197;&#36890;&#36807;&#24494;&#35843;&#21644;&#23569;&#37327;&#29305;&#23450;&#20219;&#21153;&#30340;&#26631;&#35760;&#25968;&#25454;&#23436;&#25104;&#21508;&#31181;&#22810;&#26679;&#30340;&#20219;&#21153;&#12290;&#23613;&#31649;&#22522;&#30784;&#27169;&#22411;&#24050;&#32463;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#22312;&#20892;&#19994;&#39046;&#22495;&#20013;&#24212;&#29992;&#23578;&#26410;&#26377;&#22826;&#22810;&#25506;&#32034;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#22522;&#30784;&#27169;&#22411;&#22312;&#26234;&#33021;&#20892;&#19994;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The past decade has witnessed the rapid development of ML and DL methodologies in agricultural systems, showcased by great successes in variety of agricultural applications. However, these conventional ML/DL models have certain limitations: They heavily rely on large, costly-to-acquire labeled datasets for training, require specialized expertise for development and maintenance, and are mostly tailored for specific tasks, thus lacking generalizability. Recently, foundation models have demonstrated remarkable successes in language and vision tasks across various domains. These models are trained on a vast amount of data from multiple domains and modalities. Once trained, they can accomplish versatile tasks with just minor fine-tuning and minimal task-specific labeled data. Despite their proven effectiveness and huge potential, there has been little exploration of applying FMs to agriculture fields. Therefore, this study aims to explore the potential of FMs in the field of smart agricultu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ALGAN&#30340;&#26032;&#22411;GAN&#27169;&#22411;&#65292;&#36890;&#36807;&#35843;&#25972;LSTM&#32593;&#32476;&#30340;&#36755;&#20986;&#65292;&#23454;&#29616;&#20102;&#22312;&#26080;&#30417;&#30563;&#35774;&#32622;&#19979;&#23545;&#21333;&#21464;&#37327;&#21644;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#21644;&#20854;&#20182;GAN&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.06663</link><description>&lt;p&gt;
ALGAN&#65306;&#20855;&#26377;&#35843;&#25972;&#30340;LSTM GAN&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
ALGAN: Time Series Anomaly Detection with Adjusted-LSTM GAN. (arXiv:2308.06663v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06663
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ALGAN&#30340;&#26032;&#22411;GAN&#27169;&#22411;&#65292;&#36890;&#36807;&#35843;&#25972;LSTM&#32593;&#32476;&#30340;&#36755;&#20986;&#65292;&#23454;&#29616;&#20102;&#22312;&#26080;&#30417;&#30563;&#35774;&#32622;&#19979;&#23545;&#21333;&#21464;&#37327;&#21644;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#21644;&#20854;&#20182;GAN&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#26159;&#21508;&#20010;&#39046;&#22495;&#65288;&#22914;&#21046;&#36896;&#19994;&#65292;&#21307;&#23398;&#25104;&#20687;&#21644;&#32593;&#32476;&#23433;&#20840;&#65289;&#20013;&#24120;&#35265;&#30340;&#38382;&#39064;&#65292;&#26088;&#22312;&#35782;&#21035;&#20559;&#31163;&#27491;&#24120;&#34892;&#20026;&#30340;&#28857;&#12290;&#26368;&#36817;&#65292;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#22312;&#26816;&#27979;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#26377;&#25928;&#24615;&#12290;GANs&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65288;&#21363;&#29983;&#25104;&#22120;&#21644;&#37492;&#21035;&#22120;&#65289;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#24322;&#24120;&#26816;&#27979;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;GAN&#27169;&#22411;&#65292;&#21517;&#20026;Adjusted-LSTM GAN&#65288;ALGAN&#65289;&#65292;&#23427;&#35843;&#25972;LSTM&#32593;&#32476;&#30340;&#36755;&#20986;&#65292;&#20197;&#25552;&#39640;&#21333;&#21464;&#37327;&#21644;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#24322;&#24120;&#26816;&#27979;&#33021;&#21147;&#65292;&#32780;&#19988;&#26159;&#22312;&#26080;&#30417;&#30563;&#35774;&#32622;&#19979;&#36827;&#34892;&#30340;&#12290;&#25105;&#20204;&#22312;46&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#21333;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#21644;&#28085;&#30422;&#22810;&#20010;&#39046;&#22495;&#30340;&#22823;&#22411;&#22810;&#21464;&#37327;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;ALGAN&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;ALGAN&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#24322;&#24120;&#26816;&#27979;&#20013;&#20248;&#20110;&#20256;&#32479;&#30340;&#12289;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#21644;&#20854;&#20182;&#22522;&#20110;GAN&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection in time series data, to identify points that deviate from normal behaviour, is a common problem in various domains such as manufacturing, medical imaging, and cybersecurity. Recently, Generative Adversarial Networks (GANs) are shown to be effective in detecting anomalies in time series data. The neural network architecture of GANs (i.e. Generator and Discriminator) can significantly improve anomaly detection accuracy. In this paper, we propose a new GAN model, named Adjusted-LSTM GAN (ALGAN), which adjusts the output of an LSTM network for improved anomaly detection in both univariate and multivariate time series data in an unsupervised setting. We evaluate the performance of ALGAN on 46 real-world univariate time series datasets and a large multivariate dataset that spans multiple domains. Our experiments demonstrate that ALGAN outperforms traditional, neural network-based, and other GAN-based methods for anomaly detection in time series data.
&lt;/p&gt;</description></item><item><title>&#22312;&#20849;&#20139;&#31354;&#38388;&#20013;&#65292;&#36890;&#36807;&#20351;&#29992;&#30896;&#25758;&#39118;&#38505;&#35745;&#31639;&#26469;&#36873;&#25321;&#30456;&#20114;&#20316;&#29992;&#20307;&#65292;&#21487;&#20197;&#26377;&#25928;&#24314;&#27169;&#34892;&#20154;&#36712;&#36857;&#39044;&#27979;&#65292;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.06654</link><description>&lt;p&gt;
&#26497;&#21270;&#30896;&#25758;&#32593;&#32476;: &#22312;&#20849;&#20139;&#31354;&#38388;&#20013;&#20351;&#29992;&#30896;&#25758;&#26816;&#27979;&#26377;&#25928;&#24314;&#27169;&#34892;&#20154;&#36712;&#36857;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Polar Collision Grids: Effective Interaction Modelling for Pedestrian Trajectory Prediction in Shared Space Using Collision Checks. (arXiv:2308.06654v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06654
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20849;&#20139;&#31354;&#38388;&#20013;&#65292;&#36890;&#36807;&#20351;&#29992;&#30896;&#25758;&#39118;&#38505;&#35745;&#31639;&#26469;&#36873;&#25321;&#30456;&#20114;&#20316;&#29992;&#20307;&#65292;&#21487;&#20197;&#26377;&#25928;&#24314;&#27169;&#34892;&#20154;&#36712;&#36857;&#39044;&#27979;&#65292;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#34892;&#20154;&#36712;&#36857;&#26159;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#23433;&#20840;&#23548;&#33322;&#30340;&#20851;&#38190;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;&#22312;&#19982;&#34892;&#20154;&#20849;&#20139;&#31354;&#38388;&#20013;&#12290;&#22312;&#20849;&#20139;&#31354;&#38388;&#20013;&#65292;&#34892;&#20154;&#30340;&#36816;&#21160;&#21463;&#21040;&#36710;&#36742;&#21644;&#20854;&#20182;&#34892;&#20154;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#26377;&#25928;&#22320;&#24314;&#27169;&#34892;&#20154;&#19982;&#34892;&#20154;&#20197;&#21450;&#34892;&#20154;&#19982;&#36710;&#36742;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#21487;&#20197;&#25552;&#39640;&#34892;&#20154;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#23613;&#31649;&#26377;&#20851;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;&#30456;&#20114;&#20316;&#29992;&#20307;&#29616;&#22312;&#34892;&#20154;&#30340;&#39044;&#27979;&#36712;&#36857;&#20013;&#30340;&#22823;&#37327;&#25991;&#29486;&#65292;&#20294;&#22312;&#36873;&#25321;&#30456;&#20114;&#20316;&#29992;&#20307;&#26102;&#65292;&#24456;&#23569;&#26377;&#20154;&#23545;&#27492;&#36827;&#34892;&#20102;&#26377;&#25928;&#21162;&#21147;&#12290;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#25152;&#20351;&#29992;&#30340;&#30456;&#20114;&#20316;&#29992;&#29305;&#24449;&#20027;&#35201;&#22522;&#20110;&#30456;&#23545;&#36317;&#31163;&#65292;&#36739;&#23569;&#20851;&#27880;&#36895;&#24230;&#21644;&#25509;&#36817;&#26041;&#21521;&#23545;&#30456;&#20114;&#20316;&#29992;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30896;&#25758;&#39118;&#38505;&#35745;&#31639;&#30340;&#21551;&#21457;&#24335;&#36873;&#25321;&#30456;&#20114;&#20316;&#29992;&#20307;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting pedestrians' trajectories is a crucial capability for autonomous vehicles' safe navigation, especially in spaces shared with pedestrians. Pedestrian motion in shared spaces is influenced by both the presence of vehicles and other pedestrians. Therefore, effectively modelling both pedestrian-pedestrian and pedestrian-vehicle interactions can increase the accuracy of the pedestrian trajectory prediction models. Despite the huge literature on ways to encode the effect of interacting agents on a pedestrian's predicted trajectory using deep-learning models, limited effort has been put into the effective selection of interacting agents. In the majority of cases, the interaction features used are mainly based on relative distances while paying less attention to the effect of the velocity and approaching direction in the interaction formulation. In this paper, we propose a heuristic-based process of selecting the interacting agents based on collision risk calculation. Focusing on in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#36880;&#27493;&#33976;&#39311;&#26469;&#21152;&#36895;&#22522;&#20110;&#25193;&#25955;&#30340;&#32452;&#21512;&#20248;&#21270;&#27714;&#35299;&#22120;&#65292;&#24182;&#22312;TSP-50&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;16&#20493;&#30340;&#25512;&#29702;&#36895;&#24230;&#25552;&#21319;&#65292;&#20165;&#26377;0.019%&#30340;&#24615;&#33021;&#38477;&#32423;&#12290;</title><link>http://arxiv.org/abs/2308.06644</link><description>&lt;p&gt;
&#36890;&#36807;&#36880;&#27493;&#33976;&#39311;&#21152;&#36895;&#22522;&#20110;&#25193;&#25955;&#30340;&#32452;&#21512;&#20248;&#21270;&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
Accelerating Diffusion-based Combinatorial Optimization Solvers by Progressive Distillation. (arXiv:2308.06644v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06644
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#36880;&#27493;&#33976;&#39311;&#26469;&#21152;&#36895;&#22522;&#20110;&#25193;&#25955;&#30340;&#32452;&#21512;&#20248;&#21270;&#27714;&#35299;&#22120;&#65292;&#24182;&#22312;TSP-50&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;16&#20493;&#30340;&#25512;&#29702;&#36895;&#24230;&#25552;&#21319;&#65292;&#20165;&#26377;0.019%&#30340;&#24615;&#33021;&#38477;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#35299;&#20915;&#26041;&#26696;&#30340;NP&#23436;&#20840;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21435;&#22122;&#25193;&#25955;&#36807;&#31243;&#30340;&#36845;&#20195;&#35780;&#20272;&#29305;&#24615;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#25512;&#29702;&#19978;&#24120;&#24120;&#25928;&#29575;&#20302;&#19979;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#36880;&#27493;&#33976;&#39311;&#26469;&#21152;&#36895;&#25512;&#29702;&#36807;&#31243;&#65292;&#20165;&#22312;&#21333;&#27493;&#20869;&#39044;&#27979;&#20004;&#20010;&#27493;&#39588;&#20043;&#21069;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#32463;&#36807;&#36880;&#27493;&#33976;&#39311;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;TSP-50&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#25512;&#29702;&#65292;&#36895;&#24230;&#24555;&#20102;16&#20493;&#65292;&#24615;&#33021;&#20165;&#26377;0.019%&#30340;&#38477;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph-based diffusion models have shown promising results in terms of generating high-quality solutions to NP-complete (NPC) combinatorial optimization (CO) problems. However, those models are often inefficient in inference, due to the iterative evaluation nature of the denoising diffusion process. This paper proposes to use progressive distillation to speed up the inference by taking fewer steps (e.g., forecasting two steps ahead within a single step) during the denoising process. Our experimental results show that the progressively distilled model can perform inference 16 times faster with only 0.019% degradation in performance on the TSP-50 dataset.
&lt;/p&gt;</description></item><item><title>ADRMX&#26159;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#20351;&#29992;&#28155;&#21152;&#27169;&#32508;&#29305;&#24449;&#21644;&#22495;&#19981;&#21464;&#29305;&#24449;&#26469;&#35299;&#20915;&#22495;&#27867;&#21270;&#20013;&#30340;&#38480;&#21046;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.06624</link><description>&lt;p&gt;
ADRMX: &#28155;&#21152;&#28151;&#21512;&#25439;&#22833;&#30340;&#22495;&#29305;&#24449;&#21152;&#27861;&#35299;&#32544;
&lt;/p&gt;
&lt;p&gt;
ADRMX: Additive Disentanglement of Domain Features with Remix Loss. (arXiv:2308.06624v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06624
&lt;/p&gt;
&lt;p&gt;
ADRMX&#26159;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#20351;&#29992;&#28155;&#21152;&#27169;&#32508;&#29305;&#24449;&#21644;&#22495;&#19981;&#21464;&#29305;&#24449;&#26469;&#35299;&#20915;&#22495;&#27867;&#21270;&#20013;&#30340;&#38480;&#21046;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37096;&#32626;&#29615;&#22659;&#20013;&#65292;&#32463;&#24120;&#36829;&#21453;&#35757;&#32451;&#38598;&#21644;&#27979;&#35797;&#38598;&#36981;&#24490;&#30456;&#20284;&#20998;&#24067;&#30340;&#24120;&#35265;&#20551;&#35774;&#12290;&#37492;&#20110;&#22810;&#20010;&#28304;&#22495;&#65292;&#22495;&#27867;&#21270;&#26088;&#22312;&#21019;&#24314;&#20855;&#26377;&#27867;&#21270;&#21040;&#26032;&#30340;&#26410;&#35265;&#22495;&#30340;&#33021;&#21147;&#30340;&#40065;&#26834;&#27169;&#22411;&#12290;&#20026;&#27492;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30740;&#31350;&#20391;&#37325;&#20110;&#25552;&#21462;&#22312;&#25152;&#26377;&#21487;&#29992;&#28304;&#22495;&#19978;&#22495;&#19981;&#21464;&#29305;&#24449;&#65292;&#20197;&#20943;&#36731;&#22495;&#38388;&#20998;&#24067;&#21464;&#21270;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#36890;&#36807;&#20165;&#20165;&#20381;&#38752;&#22312;&#28304;&#22495;&#20013;&#25214;&#21040;&#20849;&#21516;&#29305;&#24449;&#26469;&#38480;&#21046;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#23427;&#24573;&#35270;&#20102;&#21487;&#33021;&#22312;&#26576;&#20123;&#22495;&#30340;&#23376;&#38598;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#29305;&#23450;&#20110;&#22495;&#30340;&#29305;&#24449;&#65292;&#36825;&#20123;&#29305;&#24449;&#21487;&#33021;&#21253;&#21547;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ADR MX&#30340;&#26032;&#22411;&#26550;&#26500;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#21407;&#22987;&#30340;&#28155;&#21152;&#27169;&#32508;&#29305;&#24449;&#21644;&#22495;&#19981;&#21464;&#29305;&#24449;&#26469;&#35299;&#20915;&#36825;&#20010;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The common assumption that train and test sets follow similar distributions is often violated in deployment settings. Given multiple source domains, domain generalization aims to create robust models capable of generalizing to new unseen domains. To this end, most of existing studies focus on extracting domain invariant features across the available source domains in order to mitigate the effects of inter-domain distributional changes. However, this approach may limit the model's generalization capacity by relying solely on finding common features among the source domains. It overlooks the potential presence of domain-specific characteristics that could be prevalent in a subset of domains, potentially containing valuable information. In this work, a novel architecture named Additive Disentanglement of Domain Features with Remix Loss (ADRMX) is presented, which addresses this limitation by incorporating domain variant features together with the domain invariant ones using an original ad
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;EGP&#30340;&#21019;&#26032;&#30340;&#29109;&#24341;&#23548;&#21098;&#26525;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#36890;&#36807;&#20248;&#20808;&#21098;&#38500;&#29109;&#36739;&#20302;&#30340;&#23618;&#20013;&#30340;&#36830;&#25509;&#26469;&#26377;&#25928;&#21387;&#32553;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#31454;&#20105;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2308.06619</link><description>&lt;p&gt;
&#33021;&#21542;&#36890;&#36807;&#38750;&#32467;&#26500;&#21270;&#21098;&#26525;&#26469;&#20943;&#23569;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23618;&#25968;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Unstructured Pruning Reduce the Depth in Deep Neural Networks?. (arXiv:2308.06619v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06619
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;EGP&#30340;&#21019;&#26032;&#30340;&#29109;&#24341;&#23548;&#21098;&#26525;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#36890;&#36807;&#20248;&#20808;&#21098;&#38500;&#29109;&#36739;&#20302;&#30340;&#23618;&#20013;&#30340;&#36830;&#25509;&#26469;&#26377;&#25928;&#21387;&#32553;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#31454;&#20105;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21098;&#26525;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#20943;&#23567;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#22823;&#23567;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#22312;&#26377;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#25216;&#26415;&#20063;&#24456;&#38590;&#20174;&#27169;&#22411;&#20013;&#23436;&#20840;&#21435;&#38500;&#25972;&#20010;&#23618;&#65306;&#36825;&#26159;&#19968;&#20010;&#21487;&#20197;&#35299;&#20915;&#30340;&#20219;&#21153;&#21527;&#65311;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;EGP&#30340;&#21019;&#26032;&#30340;&#29109;&#24341;&#23548;&#21098;&#26525;&#31639;&#27861;&#65292;&#26088;&#22312;&#20943;&#23567;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#22823;&#23567;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#24615;&#33021;&#12290;EGP&#30340;&#20851;&#38190;&#37325;&#28857;&#26159;&#20248;&#20808;&#21098;&#38500;&#29109;&#36739;&#20302;&#30340;&#23618;&#20013;&#30340;&#36830;&#25509;&#65292;&#26368;&#32456;&#23436;&#20840;&#21435;&#38500;&#36825;&#20123;&#23618;&#12290;&#36890;&#36807;&#22312;ResNet-18&#21644;Swin-T&#31561;&#27969;&#34892;&#27169;&#22411;&#19978;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;EGP&#33021;&#22815;&#26377;&#25928;&#21387;&#32553;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#21516;&#26102;&#20445;&#25345;&#31454;&#20105;&#24615;&#33021;&#27700;&#24179;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#19981;&#20165;&#25581;&#31034;&#20102;&#38750;&#32467;&#26500;&#21270;&#21098;&#26525;&#20248;&#21183;&#32972;&#21518;&#30340;&#26426;&#21046;&#65292;&#36824;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#22797;&#26434;&#30340;&#20851;&#31995;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pruning is a widely used technique for reducing the size of deep neural networks while maintaining their performance. However, such a technique, despite being able to massively compress deep models, is hardly able to remove entire layers from a model (even when structured): is this an addressable task? In this study, we introduce EGP, an innovative Entropy Guided Pruning algorithm aimed at reducing the size of deep neural networks while preserving their performance. The key focus of EGP is to prioritize pruning connections in layers with low entropy, ultimately leading to their complete removal. Through extensive experiments conducted on popular models like ResNet-18 and Swin-T, our findings demonstrate that EGP effectively compresses deep neural networks while maintaining competitive performance levels. Our results not only shed light on the underlying mechanism behind the advantages of unstructured pruning, but also pave the way for further investigations into the intricate relations
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#21367;&#31215;&#22635;&#20805;&#21644;&#23545;&#25239;&#25915;&#20987;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#25506;&#35752;&#20102;&#19981;&#21516;&#22635;&#20805;&#27169;&#24335;&#23545;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.06612</link><description>&lt;p&gt;
&#20851;&#20110;&#21367;&#31215;&#22635;&#20805;&#19982;&#23545;&#25239;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
On the Interplay of Convolutional Padding and Adversarial Robustness. (arXiv:2308.06612v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06612
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#21367;&#31215;&#22635;&#20805;&#21644;&#23545;&#25239;&#25915;&#20987;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#25506;&#35752;&#20102;&#19981;&#21516;&#22635;&#20805;&#27169;&#24335;&#23545;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#20013;&#65292;&#24120;&#24120;&#22312;&#21367;&#31215;&#25805;&#20316;&#20043;&#21069;&#36827;&#34892;&#22635;&#20805;&#25805;&#20316;&#65292;&#20197;&#20445;&#30041;&#29305;&#24449;&#22270;&#30340;&#20998;&#36776;&#29575;&#12290;&#34429;&#28982;&#26377;&#24456;&#22810;&#26367;&#20195;&#26041;&#27861;&#65292;&#20294;&#36890;&#24120;&#26159;&#36890;&#36807;&#22312;&#36755;&#20837;&#21608;&#22260;&#28155;&#21152;&#19968;&#22280;&#38646;&#36827;&#34892;&#22635;&#20805;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#25239;&#25915;&#20987;&#36890;&#24120;&#20250;&#22312;&#22270;&#20687;&#36793;&#30028;&#20986;&#29616;&#25200;&#21160;&#24322;&#24120;&#65292;&#32780;&#36825;&#20123;&#36793;&#30028;&#27491;&#26159;&#22635;&#20805;&#25152;&#29992;&#30340;&#21306;&#22495;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#26088;&#22312;&#23545;&#22635;&#20805;&#21644;&#23545;&#25239;&#25915;&#20987;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#36827;&#34892;&#20998;&#26512;&#65292;&#24182;&#23547;&#25214;&#19981;&#21516;&#22635;&#20805;&#27169;&#24335;&#65288;&#25110;&#32570;&#20047;&#22635;&#20805;&#65289;&#23545;&#19981;&#21516;&#22330;&#26223;&#19979;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#20135;&#29983;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is common practice to apply padding prior to convolution operations to preserve the resolution of feature-maps in Convolutional Neural Networks (CNN). While many alternatives exist, this is often achieved by adding a border of zeros around the inputs. In this work, we show that adversarial attacks often result in perturbation anomalies at the image boundaries, which are the areas where padding is used. Consequently, we aim to provide an analysis of the interplay between padding and adversarial attacks and seek an answer to the question of how different padding modes (or their absence) affect adversarial robustness in various scenarios.
&lt;/p&gt;</description></item><item><title>LadleNet&#26159;&#19968;&#31181;&#20351;&#29992;&#21487;&#25193;&#23637;&#30340;&#20004;&#38454;&#27573;U-Net&#23558;&#28909;&#32418;&#22806;&#22270;&#20687;&#36716;&#25442;&#20026;&#21487;&#35265;&#20809;&#22270;&#20687;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#36339;&#36291;&#36830;&#25509;&#21644;&#31934;&#32454;&#29305;&#24449;&#32858;&#21512;&#25216;&#26415;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;Handle&#27169;&#22359;&#26500;&#24314;&#25277;&#35937;&#35821;&#20041;&#31354;&#38388;&#65292;Bowl&#27169;&#22359;&#35299;&#30721;&#35813;&#31354;&#38388;&#29983;&#25104;&#26144;&#23556;&#30340;VI&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2308.06603</link><description>&lt;p&gt;
LadleNet: &#20351;&#29992;&#21487;&#25193;&#23637;&#30340;&#20004;&#38454;&#27573;U-Net&#23558;&#28909;&#32418;&#22806;&#22270;&#20687;&#36716;&#25442;&#20026;&#21487;&#35265;&#20809;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
LadleNet: Translating Thermal Infrared Images to Visible Light Images Using A Scalable Two-stage U-Net. (arXiv:2308.06603v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06603
&lt;/p&gt;
&lt;p&gt;
LadleNet&#26159;&#19968;&#31181;&#20351;&#29992;&#21487;&#25193;&#23637;&#30340;&#20004;&#38454;&#27573;U-Net&#23558;&#28909;&#32418;&#22806;&#22270;&#20687;&#36716;&#25442;&#20026;&#21487;&#35265;&#20809;&#22270;&#20687;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#36339;&#36291;&#36830;&#25509;&#21644;&#31934;&#32454;&#29305;&#24449;&#32858;&#21512;&#25216;&#26415;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;Handle&#27169;&#22359;&#26500;&#24314;&#25277;&#35937;&#35821;&#20041;&#31354;&#38388;&#65292;Bowl&#27169;&#22359;&#35299;&#30721;&#35813;&#31354;&#38388;&#29983;&#25104;&#26144;&#23556;&#30340;VI&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#28909;&#32418;&#22806;&#65288;TIR&#65289;&#22270;&#20687;&#36716;&#25442;&#20026;&#21487;&#35265;&#20809;&#65288;VI&#65289;&#22270;&#20687;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#20855;&#26377;&#28508;&#22312;&#30340;&#24212;&#29992;&#39046;&#22495;&#65292;&#22914;TIR-VI&#22270;&#20687;&#37197;&#20934;&#21644;&#34701;&#21512;&#12290;&#21033;&#29992;&#20174;TIR&#22270;&#20687;&#36716;&#25442;&#20013;&#24471;&#21040;&#30340;&#34917;&#20805;&#20449;&#24687;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#21644;&#24212;&#29992;&#31243;&#24207;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#35813;&#39046;&#22495;&#23384;&#22312;&#30340;&#20027;&#35201;&#38382;&#39064;&#21253;&#25324;&#22270;&#20687;&#20445;&#30495;&#24230;&#19981;&#39640;&#21644;&#27169;&#22411;&#21487;&#25193;&#23637;&#24615;&#26377;&#38480;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;U-Net&#26550;&#26500;&#30340;&#31639;&#27861;LadleNet&#12290; LadleNet&#37319;&#29992;&#20102;&#20004;&#38454;&#27573;U-Net&#20018;&#32852;&#32467;&#26500;&#65292;&#22686;&#21152;&#20102;&#36339;&#36291;&#36830;&#25509;&#21644;&#31934;&#32454;&#29305;&#24449;&#32858;&#21512;&#25216;&#26415;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;LadleNet&#30001;&#8220;Handle&#8221;&#21644;&#8220;Bowl&#8221;&#27169;&#22359;&#32452;&#25104;&#65292;Handle&#27169;&#22359;&#29992;&#20110;&#26500;&#24314;&#25277;&#35937;&#35821;&#20041;&#31354;&#38388;&#65292;&#32780;Bowl&#27169;&#22359;&#21017;&#35299;&#30721;&#36825;&#20010;&#25277;&#35937;&#35821;&#20041;&#31354;&#38388;&#65292;&#29983;&#25104;&#26144;&#23556;&#30340;VI&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
The translation of thermal infrared (TIR) images to visible light (VI) images presents a challenging task with potential applications spanning various domains such as TIR-VI image registration and fusion. Leveraging supplementary information derived from TIR image conversions can significantly enhance model performance and generalization across these applications. However, prevailing issues within this field include suboptimal image fidelity and limited model scalability. In this paper, we introduce an algorithm, LadleNet, based on the U-Net architecture. LadleNet employs a two-stage U-Net concatenation structure, augmented with skip connections and refined feature aggregation techniques, resulting in a substantial enhancement in model performance. Comprising 'Handle' and 'Bowl' modules, LadleNet's Handle module facilitates the construction of an abstract semantic space, while the Bowl module decodes this semantic space to yield mapped VI images. The Handle module exhibits extensibilit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CoverNav&#30340;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#38544;&#34109;&#21644;&#21487;&#23548;&#33322;&#30340;&#36335;&#24452;&#19978;&#36827;&#34892;&#38750;&#32467;&#26500;&#21270;&#23460;&#22806;&#29615;&#22659;&#30340;&#33258;&#20027;&#23548;&#33322;&#35268;&#21010;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#29983;&#25104;&#28023;&#25300;&#22320;&#22270;&#26469;&#35745;&#31639;&#26412;&#22320;&#20195;&#20215;&#65292;&#24110;&#21161;&#26426;&#22120;&#20154;&#26234;&#33021;&#20307;&#22312;&#20445;&#25345;&#20302;&#25104;&#26412;&#36712;&#36857;&#30340;&#21516;&#26102;&#36873;&#25321;&#26368;&#22823;&#30340;&#38544;&#34109;&#24615;&#12290;&#36825;&#23545;&#20110;&#26080;&#20154;&#22320;&#38754;&#36710;&#36742;&#22312;&#23547;&#25214;&#36991;&#38590;&#25152;&#21644;&#25513;&#25252;&#30340;&#21516;&#26102;&#23433;&#20840;&#23548;&#33322;&#33267;&#30446;&#30340;&#22320;&#38750;&#24120;&#26377;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2308.06594</link><description>&lt;p&gt;
CoverNav&#65306;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#38750;&#32467;&#26500;&#21270;&#23460;&#22806;&#29615;&#22659;&#20013;&#36827;&#34892;&#38544;&#34109;&#23548;&#33322;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
CoverNav: Cover Following Navigation Planning in Unstructured Outdoor Environment with Deep Reinforcement Learning. (arXiv:2308.06594v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06594
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CoverNav&#30340;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#38544;&#34109;&#21644;&#21487;&#23548;&#33322;&#30340;&#36335;&#24452;&#19978;&#36827;&#34892;&#38750;&#32467;&#26500;&#21270;&#23460;&#22806;&#29615;&#22659;&#30340;&#33258;&#20027;&#23548;&#33322;&#35268;&#21010;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#29983;&#25104;&#28023;&#25300;&#22320;&#22270;&#26469;&#35745;&#31639;&#26412;&#22320;&#20195;&#20215;&#65292;&#24110;&#21161;&#26426;&#22120;&#20154;&#26234;&#33021;&#20307;&#22312;&#20445;&#25345;&#20302;&#25104;&#26412;&#36712;&#36857;&#30340;&#21516;&#26102;&#36873;&#25321;&#26368;&#22823;&#30340;&#38544;&#34109;&#24615;&#12290;&#36825;&#23545;&#20110;&#26080;&#20154;&#22320;&#38754;&#36710;&#36742;&#22312;&#23547;&#25214;&#36991;&#38590;&#25152;&#21644;&#25513;&#25252;&#30340;&#21516;&#26102;&#23433;&#20840;&#23548;&#33322;&#33267;&#30446;&#30340;&#22320;&#38750;&#24120;&#26377;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#65292;&#24050;&#32463;&#24191;&#27867;&#30740;&#31350;&#20102;&#22312;&#36234;&#37326;&#29615;&#22659;&#20013;&#30340;&#33258;&#20027;&#23548;&#33322;&#12290;&#28982;&#32780;&#65292;&#22312;&#38656;&#35201;&#23558;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20174;&#22806;&#37096;&#35266;&#23519;&#32773;&#38544;&#34255;&#36215;&#26469;&#30340;&#38544;&#34109;&#24773;&#20917;&#19979;&#30340;&#23548;&#33322;&#20173;&#28982;&#26159;&#19968;&#20010;&#19981;&#23436;&#20840;&#24320;&#21457;&#30340;&#39046;&#22495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;CoverNav&#65292;&#29992;&#20110;&#22312;&#36234;&#37326;&#22320;&#24418;&#21644;&#19995;&#26519;&#29615;&#22659;&#20013;&#35782;&#21035;&#20855;&#26377;&#26368;&#23567;&#20195;&#20215;&#30340;&#38544;&#34109;&#21644;&#21487;&#23548;&#33322;&#36712;&#36857;&#12290;CoverNav&#19987;&#27880;&#20110;&#26080;&#20154;&#22320;&#38754;&#36710;&#36742;&#22312;&#23433;&#20840;&#23548;&#33322;&#21040;&#39044;&#23450;&#30446;&#30340;&#22320;&#30340;&#21516;&#26102;&#23547;&#25214;&#36991;&#38590;&#25152;&#24182;&#25214;&#21040;&#25513;&#25252;&#12290;&#25105;&#20204;&#30340;DRL&#26041;&#27861;&#35745;&#31639;&#19968;&#20010;&#26412;&#22320;&#20195;&#20215;&#22320;&#22270;&#65292;&#36890;&#36807;&#20174;3D&#28857;&#20113;&#25968;&#25454;&#12289;&#26426;&#22120;&#20154;&#30340;&#23039;&#24577;&#21644;&#30446;&#26631;&#23548;&#21521;&#20449;&#24687;&#29983;&#25104;&#30340;&#28023;&#25300;&#22320;&#22270;&#26469;&#21306;&#20998;&#21738;&#26465;&#36335;&#24452;&#23558;&#25552;&#20379;&#26368;&#22823;&#30340;&#38544;&#34109;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#20302;&#25104;&#26412;&#36712;&#36857;&#12290;CoverNav&#24110;&#21161;&#26426;&#22120;&#20154;&#26234;&#33021;&#20307;&#20351;&#29992;&#22870;&#21169;&#20989;&#25968;&#23398;&#20064;&#20302;&#28023;&#25300;&#22320;&#24418;&#65292;&#21516;&#26102;&#23545;&#23427;&#36827;&#34892;&#24809;&#32602;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous navigation in offroad environments has been extensively studied in the robotics field. However, navigation in covert situations where an autonomous vehicle needs to remain hidden from outside observers remains an underexplored area. In this paper, we propose a novel Deep Reinforcement Learning (DRL) based algorithm, called CoverNav, for identifying covert and navigable trajectories with minimal cost in offroad terrains and jungle environments in the presence of observers. CoverNav focuses on unmanned ground vehicles seeking shelters and taking covers while safely navigating to a predefined destination. Our proposed DRL method computes a local cost map that helps distinguish which path will grant the maximal covertness while maintaining a low cost trajectory using an elevation map generated from 3D point cloud data, the robot's pose, and directed goal information. CoverNav helps robot agents to learn the low elevation terrain using a reward function while penalizing it propor
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20215;&#20540;&#20998;&#24067;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#21518;&#39564;&#20998;&#24067;&#26469;&#35299;&#20915;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#25919;&#31574;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#20248;&#21270;&#31574;&#30053;&#65292;&#22312;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24615;&#33021;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.06590</link><description>&lt;p&gt;
&#22522;&#20110;&#20215;&#20540;&#20998;&#24067;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Value-Distributional Model-Based Reinforcement Learning. (arXiv:2308.06590v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06590
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20215;&#20540;&#20998;&#24067;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#21518;&#39564;&#20998;&#24067;&#26469;&#35299;&#20915;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#25919;&#31574;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#20248;&#21270;&#31574;&#30053;&#65292;&#22312;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35299;&#20915;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#20013;&#65292;&#37327;&#21270;&#25919;&#31574;&#38271;&#26399;&#32489;&#25928;&#30340;&#19981;&#30830;&#23450;&#24615;&#26159;&#24456;&#37325;&#35201;&#30340;&#12290;&#25105;&#20204;&#20174;&#22522;&#20110;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#24378;&#21270;&#23398;&#20064;&#30340;&#35282;&#24230;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#65292;&#30446;&#26631;&#26159;&#23398;&#20064;&#30001;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#21442;&#25968;&#65288;&#35748;&#30693;&#65289;&#19981;&#30830;&#23450;&#24615;&#24341;&#21457;&#30340;&#20540;&#20989;&#25968;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#23558;&#20998;&#26512;&#38480;&#21046;&#22312;&#23569;&#25968;&#20998;&#24067;&#20540;&#19978;&#65292;&#25110;&#32773;&#32422;&#26463;&#20998;&#24067;&#24418;&#29366;&#65292;&#20363;&#22914;&#65292;&#39640;&#26031;&#20998;&#24067;&#12290;&#21463;&#21040;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#20010;Bellman&#31639;&#23376;&#65292;&#20854;&#22266;&#23450;&#28857;&#26159;&#20540;&#20998;&#24067;&#20989;&#25968;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#29702;&#35770;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Epistemic Quantile-Regression&#65288;EQR&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#23398;&#20064;&#19968;&#20010;&#20540;&#20998;&#24067;&#20989;&#25968;&#29992;&#20110;&#31574;&#30053;&#20248;&#21270;&#12290;&#22312;&#20960;&#20010;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#30456;&#23545;&#20110;&#24050;&#26377;&#30340;&#22522;&#20110;&#27169;&#22411;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#31639;&#27861;&#65292;EQR&#20855;&#26377;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantifying uncertainty about a policy's long-term performance is important to solve sequential decision-making tasks. We study the problem from a model-based Bayesian reinforcement learning perspective, where the goal is to learn the posterior distribution over value functions induced by parameter (epistemic) uncertainty of the Markov decision process. Previous work restricts the analysis to a few moments of the distribution over values or imposes a particular distribution shape, e.g., Gaussians. Inspired by distributional reinforcement learning, we introduce a Bellman operator whose fixed-point is the value distribution function. Based on our theory, we propose Epistemic Quantile-Regression (EQR), a model-based algorithm that learns a value distribution function that can be used for policy optimization. Evaluation across several continuous-control tasks shows performance benefits with respect to established model-based and model-free algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#31456;&#27010;&#36848;&#20102;&#20960;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#30693;&#35782;&#22270;&#35889;&#19981;&#23436;&#25972;&#30340;&#24773;&#20917;&#19979;&#22238;&#31572;&#26597;&#35810;&#12290;&#36825;&#20123;&#26041;&#27861;&#28085;&#30422;&#20102;&#19981;&#21516;&#30340;&#26597;&#35810;&#31867;&#22411;&#21644;&#22270;&#31867;&#22411;&#65292;&#24182;&#20855;&#26377;&#19981;&#21516;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.06585</link><description>&lt;p&gt;
&#22270;&#26597;&#35810;&#30340;&#36817;&#20284;&#31572;&#26696;
&lt;/p&gt;
&lt;p&gt;
Approximate Answering of Graph Queries. (arXiv:2308.06585v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06585
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31456;&#27010;&#36848;&#20102;&#20960;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#30693;&#35782;&#22270;&#35889;&#19981;&#23436;&#25972;&#30340;&#24773;&#20917;&#19979;&#22238;&#31572;&#26597;&#35810;&#12290;&#36825;&#20123;&#26041;&#27861;&#28085;&#30422;&#20102;&#19981;&#21516;&#30340;&#26597;&#35810;&#31867;&#22411;&#21644;&#22270;&#31867;&#22411;&#65292;&#24182;&#20855;&#26377;&#19981;&#21516;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#30001;&#20110;&#19990;&#30028;&#30693;&#35782;&#30340;&#19981;&#23436;&#25972;&#21644;&#36755;&#20837;KG&#30340;&#20559;&#35265;&#32780;&#26412;&#36136;&#19978;&#26159;&#19981;&#23436;&#25972;&#30340;&#12290;&#27492;&#22806;&#65292;&#19990;&#30028;&#30693;&#35782;&#19981;&#26029;&#25193;&#23637;&#21644;&#21457;&#23637;&#65292;&#20351;&#29616;&#26377;&#20107;&#23454;&#36807;&#26102;&#25110;&#24341;&#20837;&#26032;&#30340;&#20107;&#23454;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#20173;&#28982;&#24076;&#26395;&#33021;&#22815;&#20687;&#22270;&#35889;&#23436;&#25972;&#19968;&#26679;&#22238;&#31572;&#26597;&#35810;&#12290;&#22312;&#26412;&#31456;&#20013;&#65292;&#25105;&#20204;&#23558;&#27010;&#36848;&#20960;&#31181;&#24050;&#32463;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#20197;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#22238;&#31572;&#26597;&#35810;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#27010;&#36848;&#36825;&#20123;&#26041;&#27861;&#25903;&#25345;&#30340;&#19981;&#21516;&#26597;&#35810;&#31867;&#22411;&#21644;&#36890;&#24120;&#29992;&#20110;&#35780;&#20272;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#23545;&#23427;&#20204;&#30340;&#38480;&#21046;&#30340;&#20102;&#35299;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#27010;&#36848;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#24182;&#20197;&#34920;&#36798;&#33021;&#21147;&#12289;&#25903;&#25345;&#30340;&#22270;&#31867;&#22411;&#21644;&#25512;&#29702;&#33021;&#21147;&#26469;&#25551;&#36848;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graphs (KGs) are inherently incomplete because of incomplete world knowledge and bias in what is the input to the KG. Additionally, world knowledge constantly expands and evolves, making existing facts deprecated or introducing new ones. However, we would still want to be able to answer queries as if the graph were complete. In this chapter, we will give an overview of several methods which have been proposed to answer queries in such a setting. We will first provide an overview of the different query types which can be supported by these methods and datasets typically used for evaluation, as well as an insight into their limitations. Then, we give an overview of the different approaches and describe them in terms of expressiveness, supported graph types, and inference capabilities.
&lt;/p&gt;</description></item><item><title>EquiDiff&#26159;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#36712;&#36857;&#39044;&#27979;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#25972;&#21512;&#21382;&#21490;&#20449;&#24687;&#21644;&#38543;&#26426;&#22122;&#22768;&#26469;&#29983;&#25104;&#26410;&#26469;&#36710;&#36742;&#36712;&#36857;&#65292;&#24182;&#21033;&#29992;&#20960;&#20309;&#29305;&#24615;&#21644;&#31038;&#20132;&#20132;&#20114;&#25552;&#21462;&#25216;&#26415;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.06564</link><description>&lt;p&gt;
EquiDiff:&#19968;&#31181;&#29992;&#20110;&#36712;&#36857;&#39044;&#27979;&#30340;&#26465;&#20214;&#31561;&#21464;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
EquiDiff: A Conditional Equivariant Diffusion Model For Trajectory Prediction. (arXiv:2308.06564v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06564
&lt;/p&gt;
&lt;p&gt;
EquiDiff&#26159;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#36712;&#36857;&#39044;&#27979;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#25972;&#21512;&#21382;&#21490;&#20449;&#24687;&#21644;&#38543;&#26426;&#22122;&#22768;&#26469;&#29983;&#25104;&#26410;&#26469;&#36710;&#36742;&#36712;&#36857;&#65292;&#24182;&#21033;&#29992;&#20960;&#20309;&#29305;&#24615;&#21644;&#31038;&#20132;&#20132;&#20114;&#25552;&#21462;&#25216;&#26415;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#36712;&#36857;&#39044;&#27979;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#23433;&#20840;&#21644;&#39640;&#25928;&#36816;&#34892;&#33267;&#20851;&#37325;&#35201;&#12290;&#28145;&#24230;&#23398;&#20064;&#30340;&#30427;&#34892;&#20351;&#24471;&#20986;&#29616;&#20102;&#35768;&#22810;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#12290;&#23613;&#31649;&#30830;&#23450;&#24615;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22240;&#20854;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#23398;&#20064;&#25968;&#25454;&#20998;&#24067;&#24182;&#32771;&#34385;&#36712;&#36857;&#19981;&#30830;&#23450;&#24615;&#32780;&#21463;&#21040;&#27426;&#36814;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EquiDiff&#65292;&#19968;&#31181;&#29992;&#20110;&#39044;&#27979;&#26410;&#26469;&#36710;&#36742;&#36712;&#36857;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#12290;EquiDiff&#22522;&#20110;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#21382;&#21490;&#20449;&#24687;&#21644;&#38543;&#26426;&#39640;&#26031;&#22122;&#22768;&#26469;&#29983;&#25104;&#26410;&#26469;&#36712;&#36857;&#12290;EquiDiff&#30340;&#39592;&#24178;&#27169;&#22411;&#26159;&#19968;&#20010;SO(2)&#31561;&#21464;&#25442;&#22120;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#20301;&#32622;&#22352;&#26631;&#30340;&#20960;&#20309;&#29305;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20351;&#29992;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#26469;&#25552;&#21462;&#21382;&#21490;&#36712;&#36857;&#20013;&#30340;&#31038;&#20132;&#20132;&#20114;&#12290;&#20026;&#20102;&#35780;&#20272;EquiDiff&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate trajectory prediction is crucial for the safe and efficient operation of autonomous vehicles. The growing popularity of deep learning has led to the development of numerous methods for trajectory prediction. While deterministic deep learning models have been widely used, deep generative models have gained popularity as they learn data distributions from training data and account for trajectory uncertainties. In this study, we propose EquiDiff, a deep generative model for predicting future vehicle trajectories. EquiDiff is based on the conditional diffusion model, which generates future trajectories by incorporating historical information and random Gaussian noise. The backbone model of EquiDiff is an SO(2)-equivariant transformer that fully utilizes the geometric properties of location coordinates. In addition, we employ Recurrent Neural Networks and Graph Attention Networks to extract social interactions from historical trajectories. To evaluate the performance of EquiDiff, w
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#31867;&#34892;&#20026;&#30340;&#20010;&#24615;&#21270;&#33203;&#39135;&#25512;&#33616;&#19982;&#33756;&#21333;&#35268;&#21010;&#31038;&#20132;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#36890;&#36807;&#33041;&#30005;&#22270;&#20449;&#21495;&#35782;&#21035;&#20154;&#20204;&#23545;&#39184;&#39135;&#30340;&#24773;&#24863;&#65292;&#24182;&#32771;&#34385;&#33829;&#20859;&#38656;&#27714;&#21644;&#31038;&#20132;&#24773;&#24863;&#26469;&#35268;&#21010;&#33756;&#21333;&#12290;</title><link>http://arxiv.org/abs/2308.06549</link><description>&lt;p&gt;
&#22522;&#20110;&#20154;&#31867;&#34892;&#20026;&#30340;&#20010;&#24615;&#21270;&#33203;&#39135;&#25512;&#33616;&#19982;&#33756;&#21333;&#35268;&#21010;&#31038;&#20132;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Human Behavior-based Personalized Meal Recommendation and Menu Planning Social System. (arXiv:2308.06549v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06549
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#31867;&#34892;&#20026;&#30340;&#20010;&#24615;&#21270;&#33203;&#39135;&#25512;&#33616;&#19982;&#33756;&#21333;&#35268;&#21010;&#31038;&#20132;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#36890;&#36807;&#33041;&#30005;&#22270;&#20449;&#21495;&#35782;&#21035;&#20154;&#20204;&#23545;&#39184;&#39135;&#30340;&#24773;&#24863;&#65292;&#24182;&#32771;&#34385;&#33829;&#20859;&#38656;&#27714;&#21644;&#31038;&#20132;&#24773;&#24863;&#26469;&#35268;&#21010;&#33756;&#21333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#39278;&#39135;&#25512;&#33616;&#31995;&#32479;&#22522;&#26412;&#19978;&#21482;&#32771;&#34385;&#33829;&#20859;&#25110;&#20581;&#24247;&#65292;&#24573;&#30053;&#20102;&#20154;&#20204;&#23545;&#39135;&#29289;&#30340;&#24863;&#21463;&#12290;&#20154;&#20204;&#22312;&#39135;&#27442;&#19978;&#26377;&#25152;&#24046;&#24322;&#65292;&#19981;&#21516;&#30340;&#24515;&#24773;&#19979;&#24182;&#19981;&#26159;&#25152;&#26377;&#39135;&#29289;&#37117;&#20196;&#20154;&#24841;&#24742;&#12290;&#22522;&#20110;&#38382;&#21367;&#21644;&#20559;&#22909;&#30340;&#39184;&#39135;&#25512;&#33616;&#31995;&#32479;&#21487;&#20197;&#26159;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#33258;&#21160;&#35782;&#21035;&#19981;&#21516;&#39135;&#29289;&#23545;&#31038;&#20132;&#24773;&#24863;&#30340;&#24433;&#21709;&#65292;&#24182;&#32771;&#34385;&#33829;&#20859;&#38656;&#27714;&#21644;&#31038;&#20132;&#24773;&#24863;&#26469;&#35268;&#21010;&#33756;&#21333;&#65292;&#20855;&#26377;&#38382;&#21367;&#21644;&#20559;&#22909;&#39184;&#39135;&#25512;&#33616;&#30340;&#19968;&#20123;&#26174;&#33879;&#20248;&#21183;&#12290;&#20005;&#37325;&#30142;&#30149;&#30340;&#24739;&#32773;&#12289;&#26127;&#36855;&#20013;&#30340;&#20154;&#25110;&#24739;&#26377;&#38145;&#23450;&#32508;&#21512;&#24449;&#21644;&#32908;&#33806;&#32553;&#24615;&#20391;&#32034;&#30828;&#21270;&#30151;&#65288;ALS&#65289;&#30340;&#24739;&#32773;&#26080;&#27861;&#34920;&#36798;&#20182;&#20204;&#30340;&#39184;&#39135;&#20559;&#22909;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#30340;&#26694;&#26550;&#21253;&#25324;&#19968;&#20010;&#31038;&#20132;&#24773;&#24863;&#35745;&#31639;&#27169;&#22359;&#65292;&#21487;&#20197;&#36890;&#36807;&#33041;&#30005;&#22270;&#20449;&#21495;&#26469;&#35782;&#21035;&#19981;&#21516;&#39184;&#39135;&#30340;&#24773;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;
The traditional dietary recommendation systems are basically nutrition or health-aware where the human feelings on food are ignored. Human affects vary when it comes to food cravings, and not all foods are appealing in all moods. A questionnaire-based and preference-aware meal recommendation system can be a solution. However, automated recognition of social affects on different foods and planning the menu considering nutritional demand and social-affect has some significant benefits of the questionnaire-based and preference-aware meal recommendations. A patient with severe illness, a person in a coma, or patients with locked-in syndrome and amyotrophic lateral sclerosis (ALS) cannot express their meal preferences. Therefore, the proposed framework includes a social-affective computing module to recognize the affects of different meals where the person's affect is detected using electroencephalography signals. EEG allows to capture the brain signals and analyze them to anticipate affect
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37319;&#29992;&#20102;&#26497;&#38480;&#26799;&#24230;&#25552;&#21319;&#31639;&#27861;&#26469;&#25552;&#39640;&#22478;&#24066;&#21306;&#22495;&#20013;&#25968;&#23383;&#39640;&#31243;&#27169;&#22411;&#30340;&#31934;&#24230;&#65292;&#24182;&#32467;&#21512;&#22303;&#22320;&#35206;&#30422;&#21644;&#22320;&#24418;&#21442;&#25968;&#26657;&#27491;&#65292;&#20197;&#35299;&#20915;DEM&#22312;&#22478;&#24066;&#29615;&#22659;&#24314;&#27169;&#20013;&#30340;&#36136;&#37327;&#21644;&#36866;&#29992;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.06545</link><description>&lt;p&gt;
&#22312;&#22478;&#24066;&#21306;&#22495;&#20013;&#20351;&#29992;&#26497;&#38480;&#26799;&#24230;&#25552;&#21319;&#12289;&#22303;&#22320;&#35206;&#30422;&#21644;&#22320;&#24418;&#21442;&#25968;&#36827;&#34892;&#25968;&#23383;&#39640;&#31243;&#27169;&#22411;&#26657;&#27491;
&lt;/p&gt;
&lt;p&gt;
Digital elevation model correction in urban areas using extreme gradient boosting, land cover and terrain parameters. (arXiv:2308.06545v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06545
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37319;&#29992;&#20102;&#26497;&#38480;&#26799;&#24230;&#25552;&#21319;&#31639;&#27861;&#26469;&#25552;&#39640;&#22478;&#24066;&#21306;&#22495;&#20013;&#25968;&#23383;&#39640;&#31243;&#27169;&#22411;&#30340;&#31934;&#24230;&#65292;&#24182;&#32467;&#21512;&#22303;&#22320;&#35206;&#30422;&#21644;&#22320;&#24418;&#21442;&#25968;&#26657;&#27491;&#65292;&#20197;&#35299;&#20915;DEM&#22312;&#22478;&#24066;&#29615;&#22659;&#24314;&#27169;&#20013;&#30340;&#36136;&#37327;&#21644;&#36866;&#29992;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22478;&#24066;&#21306;&#22495;&#20013;&#65292;&#25968;&#23383;&#39640;&#31243;&#27169;&#22411;&#65288;DEMs&#65289;&#30340;&#31934;&#24230;&#21463;&#21040;&#22303;&#22320;&#35206;&#30422;&#21644;&#22320;&#24418;&#19981;&#35268;&#21017;&#24615;&#31561;&#22810;&#31181;&#22240;&#32032;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#20840;&#23616;DEM&#20013;&#30340;&#24314;&#31569;&#29289;&#20154;&#24037;&#38459;&#22622;&#20102;&#22320;&#34920;&#27700;&#27969;&#36335;&#24452;&#65292;&#38477;&#20302;&#20102;DEM&#30340;&#36136;&#37327;&#21644;&#36866;&#29992;&#24615;&#65292;&#32780;&#22312;&#31934;&#30830;&#21644;&#20934;&#30830;&#30340;&#22320;&#24418;&#20449;&#24687;&#22312;&#22478;&#24066;&#26223;&#35266;&#20013;&#38656;&#35201;&#36827;&#34892;&#27700;&#25991;&#21644;&#29615;&#22659;&#24314;&#27169;&#12290;&#26412;&#30740;&#31350;&#37319;&#29992;&#20102;&#26497;&#38480;&#26799;&#24230;&#25552;&#21319;&#65288;XGBoost&#65289;&#38598;&#25104;&#31639;&#27861;&#26469;&#25552;&#39640;&#20004;&#20010;&#20013;&#20998;&#36776;&#29575;30&#31859;DEM&#65288;Copernicus GLO-30&#21644;ALOS World 3D&#65289;&#22312;&#21335;&#38750;&#24320;&#26222;&#25958;&#30340;&#31934;&#24230;&#12290;XGBoost&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#12289;&#21487;&#31227;&#26893;&#21644;&#22810;&#21151;&#33021;&#30340;&#26799;&#24230;&#25552;&#21319;&#24211;&#65292;&#21487;&#20197;&#35299;&#20915;&#35768;&#22810;&#29615;&#22659;&#24314;&#27169;&#38382;&#39064;&#12290;&#35757;&#32451;&#25968;&#25454;&#38598;&#21253;&#25324;&#21313;&#19968;&#20010;&#39044;&#27979;&#21464;&#37327;&#65292;&#21253;&#25324;&#39640;&#31243;&#12289;&#22478;&#24066;&#36718;&#24275;&#12289;&#22369;&#24230;&#12289;&#22369;&#21521;&#12289;&#34920;&#38754;&#31895;&#31961;&#24230;&#12289;&#22320;&#24418;&#20301;&#32622;&#25351;&#25968;&#12289;&#22320;&#24418;&#23822;&#23702;&#25351;&#25968;&#12289;&#22320;&#34920;&#32441;&#29702;&#12289;&#30690;&#37327;&#31895;&#31961;&#24230;&#27979;&#37327;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
The accuracy of digital elevation models (DEMs) in urban areas is influenced by numerous factors including land cover and terrain irregularities. Moreover, building artifacts in global DEMs cause artificial blocking of surface flow pathways. This compromises their quality and adequacy for hydrological and environmental modelling in urban landscapes where precise and accurate terrain information is needed. In this study, the extreme gradient boosting (XGBoost) ensemble algorithm is adopted for enhancing the accuracy of two medium-resolution 30m DEMs over Cape Town, South Africa: Copernicus GLO-30 and ALOS World 3D (AW3D). XGBoost is a scalable, portable and versatile gradient boosting library that can solve many environmental modelling problems. The training datasets are comprised of eleven predictor variables including elevation, urban footprints, slope, aspect, surface roughness, topographic position index, terrain ruggedness index, terrain surface texture, vector roughness measure, f
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#20351;&#29992;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#65292;&#27604;&#36739;&#20102;&#20849;&#21516;&#23545;&#27604;&#23398;&#20064;&#21644;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#22312;CT&#25195;&#25551;&#21367;&#31215;&#27169;&#22411;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.06534</link><description>&lt;p&gt;
&#35299;&#20915;&#21307;&#23398;&#24433;&#20687;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#23567;&#22411;&#27880;&#37322;&#25968;&#25454;&#38598;&#38382;&#39064;&#65306;&#23545;&#27604;&#20849;&#21516;&#23545;&#27604;&#23398;&#20064;&#21644;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#22312;CT&#25195;&#25551;&#21367;&#31215;&#27169;&#22411;&#20013;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Dealing with Small Annotated Datasets for Deep Learning in Medical Imaging: An Evaluation of Self-Supervised Pre-Training on CT Scans Comparing Contrastive and Masked Autoencoder Methods for Convolutional Models. (arXiv:2308.06534v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#20351;&#29992;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#65292;&#27604;&#36739;&#20102;&#20849;&#21516;&#23545;&#27604;&#23398;&#20064;&#21644;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#22312;CT&#25195;&#25551;&#21367;&#31215;&#27169;&#22411;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#28145;&#24230;&#23398;&#20064;&#26377;&#28508;&#21147;&#20943;&#23569;&#35786;&#26029;&#38169;&#35823;&#30340;&#39118;&#38505;&#12289;&#20943;&#36731;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#24037;&#20316;&#36127;&#25285;&#24182;&#21152;&#36895;&#30830;&#35786;&#12290;&#35757;&#32451;&#36825;&#26679;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#22823;&#22411;&#19988;&#20934;&#30830;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#38656;&#35201;&#20026;&#25152;&#26377;&#35757;&#32451;&#26679;&#26412;&#25552;&#20379;&#27880;&#37322;&#12290;&#28982;&#32780;&#65292;&#22312;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#65292;&#30001;&#20110;&#27880;&#37322;&#30340;&#39640;&#22797;&#26434;&#24615;&#12289;&#21463;&#38480;&#30340;&#33719;&#21462;&#26041;&#24335;&#25110;&#30142;&#30149;&#30340;&#32597;&#35265;&#24615;&#65292;&#29305;&#23450;&#20219;&#21153;&#30340;&#27880;&#37322;&#25968;&#25454;&#38598;&#36890;&#24120;&#24456;&#23567;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#39046;&#22495;&#30340;&#26041;&#27861;&#65292;&#22312;&#27809;&#26377;&#27880;&#37322;&#30340;&#22823;&#22411;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#22312;&#39044;&#35757;&#32451;&#20043;&#21518;&#65292;&#23567;&#22411;&#30340;&#24050;&#27880;&#37322;&#25968;&#25454;&#38598;&#23601;&#36275;&#20197;&#23545;&#27169;&#22411;&#36827;&#34892;&#29305;&#23450;&#20219;&#21153;&#30340;&#24494;&#35843;&#65292;&#21363;&#25152;&#35859;&#30340;&#8220;&#19979;&#28216;&#20219;&#21153;&#8221;&#12290;&#21307;&#23398;&#24433;&#20687;&#20013;&#26368;&#27969;&#34892;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#22522;&#20110;&#20849;&#21516;&#23545;&#27604;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#33258;&#28982;&#22270;&#20687;&#22788;&#29702;&#30740;&#31350;&#34920;&#26126;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#20855;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20108;&#32773;&#22312;CT&#25195;&#25551;&#21367;&#31215;&#27169;&#22411;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning in medical imaging has the potential to minimize the risk of diagnostic errors, reduce radiologist workload, and accelerate diagnosis. Training such deep learning models requires large and accurate datasets, with annotations for all training samples. However, in the medical imaging domain, annotated datasets for specific tasks are often small due to the high complexity of annotations, limited access, or the rarity of diseases. To address this challenge, deep learning models can be pre-trained on large image datasets without annotations using methods from the field of self-supervised learning. After pre-training, small annotated datasets are sufficient to fine-tune the models for a specific task, the so-called ``downstream task". The most popular self-supervised pre-training approaches in medical imaging are based on contrastive learning. However, recent studies in natural image processing indicate a strong potential for masked autoencoder approaches. Our work compares sta
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;&#36731;&#37327;&#32423;&#28145;&#24230;&#23398;&#20064;&#38598;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;sEMG-based SSIs&#65292;&#36890;&#36807;&#23545;26&#20010;&#21271;&#32422;&#38899;&#26631;&#25968;&#25454;&#38598;&#36827;&#34892;&#20998;&#31867;&#65292;&#23454;&#29616;&#20102;&#33521;&#25991;&#21333;&#35789;&#30340;&#25340;&#20889;&#29983;&#25104;&#12290;&#23454;&#39564;&#34920;&#26126;&#35813;&#27169;&#22411;&#30340;&#39640;&#20934;&#30830;&#29575;&#65292;&#20026;&#20415;&#25658;&#12289;&#23454;&#29992;&#35774;&#22791;&#30340;&#31471;&#21040;&#31471;&#31995;&#32479;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.06533</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;&#38745;&#40664;&#35821;&#38899;&#30028;&#38754;&#30340;sEMG&#38598;&#25104;&#27169;&#22411;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Knowledge Distilled Ensemble Model for sEMG-based Silent Speech Interface. (arXiv:2308.06533v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06533
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;&#36731;&#37327;&#32423;&#28145;&#24230;&#23398;&#20064;&#38598;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;sEMG-based SSIs&#65292;&#36890;&#36807;&#23545;26&#20010;&#21271;&#32422;&#38899;&#26631;&#25968;&#25454;&#38598;&#36827;&#34892;&#20998;&#31867;&#65292;&#23454;&#29616;&#20102;&#33521;&#25991;&#21333;&#35789;&#30340;&#25340;&#20889;&#29983;&#25104;&#12290;&#23454;&#39564;&#34920;&#26126;&#35813;&#27169;&#22411;&#30340;&#39640;&#20934;&#30830;&#29575;&#65292;&#20026;&#20415;&#25658;&#12289;&#23454;&#29992;&#35774;&#22791;&#30340;&#31471;&#21040;&#31471;&#31995;&#32479;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#38556;&#30861;&#24433;&#21709;&#30528;&#20840;&#29699;&#25968;&#30334;&#19975;&#20154;&#12290;&#22522;&#20110;&#34920;&#38754;&#32908;&#30005;&#22270;&#30340;&#38745;&#40664;&#35821;&#38899;&#30028;&#38754;&#65288;sEMG-based SSIs&#65289;&#24050;&#34987;&#30740;&#31350;&#20102;&#20960;&#21313;&#24180;&#20316;&#20026;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#30740;&#31350;&#21463;&#21040;&#20102;&#35789;&#27719;&#37327;&#26377;&#38480;&#21644;&#20174;&#21407;&#22987;&#25968;&#25454;&#20013;&#25163;&#21160;&#25552;&#21462;&#29305;&#24449;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;sEMG&#38598;&#25104;&#27169;&#22411;&#65288;KDE-SSI&#65289;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#23545;&#21253;&#21547;3900&#20010;&#25968;&#25454;&#26679;&#26412;&#30340;26&#20010;&#21271;&#32422;&#38899;&#26631;&#25968;&#25454;&#38598;&#36827;&#34892;&#20998;&#31867;&#65292;&#20174;&#32780;&#36890;&#36807;&#25340;&#20889;&#26041;&#24335;&#29983;&#25104;&#20219;&#20309;&#33521;&#25991;&#21333;&#35789;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;KDE-SSI&#30340;&#26377;&#25928;&#24615;&#65292;&#23454;&#29616;&#20102;85.9%&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#36824;&#20026;&#20415;&#25658;&#12289;&#23454;&#29992;&#35774;&#22791;&#30340;&#31471;&#21040;&#31471;&#31995;&#32479;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Voice disorders affect millions of people worldwide. Surface electromyography-based Silent Speech Interfaces (sEMG-based SSIs) have been explored as a potential solution for decades. However, previous works were limited by small vocabularies and manually extracted features from raw data. To address these limitations, we propose a lightweight deep learning knowledge-distilled ensemble model for sEMG-based SSI (KDE-SSI). Our model can classify a 26 NATO phonetic alphabets dataset with 3900 data samples, enabling the unambiguous generation of any English word through spelling. Extensive experiments validate the effectiveness of KDE-SSI, achieving a test accuracy of 85.9\%. Our findings also shed light on an end-to-end system for portable, practical equipment.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20219;&#21153;&#20998;&#35299;&#23398;&#20064;&#25277;&#35937;&#35270;&#35273;&#25512;&#29702;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#24418;&#22120;&#34013;&#22270;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#39044;&#27979;&#21333;&#20010;&#23545;&#35937;&#21450;&#20854;&#25490;&#21015;&#30340;&#35270;&#35273;&#29305;&#24615;&#65292;&#36890;&#36807;&#22810;&#32500;&#39044;&#27979;&#26469;&#36873;&#25321;&#31572;&#26696;&#12290;</title><link>http://arxiv.org/abs/2308.06528</link><description>&lt;p&gt;
&#36890;&#36807;&#20219;&#21153;&#20998;&#35299;&#23398;&#20064;&#25277;&#35937;&#35270;&#35273;&#25512;&#29702;&#65306;&#22522;&#20110;Raven&#28176;&#36827;&#30697;&#38453;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Learning Abstract Visual Reasoning via Task Decomposition: A Case Study in Raven Progressive Matrices. (arXiv:2308.06528v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06528
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20219;&#21153;&#20998;&#35299;&#23398;&#20064;&#25277;&#35937;&#35270;&#35273;&#25512;&#29702;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#24418;&#22120;&#34013;&#22270;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#39044;&#27979;&#21333;&#20010;&#23545;&#35937;&#21450;&#20854;&#25490;&#21015;&#30340;&#35270;&#35273;&#29305;&#24615;&#65292;&#36890;&#36807;&#22810;&#32500;&#39044;&#27979;&#26469;&#36873;&#25321;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#36827;&#34892;&#25277;&#35937;&#25512;&#29702;&#30340;&#25361;&#25112;&#20043;&#19968;&#26159;&#38382;&#39064;&#36890;&#24120;&#34987;&#25552;&#20986;&#20026;&#25972;&#20307;&#20219;&#21153;&#65292;&#27809;&#26377;&#20013;&#38388;&#30446;&#26631;&#12290;&#22312;Raven&#28176;&#36827;&#30697;&#38453;&#65288;RPM&#65289;&#20013;&#65292;&#20219;&#21153;&#26159;&#22312;&#32473;&#23450;&#19978;&#19979;&#25991;&#30340;&#24773;&#20917;&#19979;&#36873;&#25321;&#19968;&#20010;&#21487;&#29992;&#31572;&#26696;&#65292;&#20854;&#20013;&#19978;&#19979;&#25991;&#21644;&#31572;&#26696;&#37117;&#26159;&#22797;&#21512;&#22270;&#20687;&#65292;&#20855;&#26377;&#22810;&#20010;&#23545;&#35937;&#20197;&#21450;&#21508;&#31181;&#31354;&#38388;&#23433;&#25490;&#12290;&#30001;&#20110;&#21482;&#26377;&#36825;&#20010;&#39640;&#32423;&#30446;&#26631;&#20316;&#20026;&#25351;&#23548;&#65292;&#23398;&#20064;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22823;&#22810;&#25968;&#29616;&#20195;&#35299;&#20915;&#26041;&#26696;&#24448;&#24448;&#19981;&#36879;&#26126;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#24418;&#22120;&#34013;&#22270;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#19981;&#30452;&#25509;&#36827;&#34892;&#19978;&#36848;&#36873;&#25321;&#65292;&#32780;&#26159;&#39044;&#27979;&#21333;&#20010;&#23545;&#35937;&#21450;&#20854;&#25490;&#21015;&#30340;&#35270;&#35273;&#29305;&#24615;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#33719;&#24471;&#30340;&#22810;&#32500;&#39044;&#27979;&#30452;&#25509;&#24182;&#32622;&#20197;&#36873;&#25321;&#31572;&#26696;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#27169;&#22411;&#23558;&#35270;&#35273;&#36755;&#20837;&#35299;&#26512;&#20026;&#20196;&#29260;&#30340;&#20960;&#31181;&#26041;&#24335;&#65292;&#24182;&#37319;&#29992;&#20102;&#20960;&#31181;&#33258;&#30417;&#30563;&#35757;&#32451;&#20013;&#36755;&#20837;&#30340;&#23631;&#34109;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the challenges in learning to perform abstract reasoning is that problems are often posed as monolithic tasks, with no intermediate subgoals. In Raven Progressive Matrices (RPM), the task is to choose one of the available answers given a context, where both contexts and answers are composite images featuring multiple objects in various spatial arrangements. As this high-level goal is the only guidance available, learning is challenging and most contemporary solvers tend to be opaque. In this study, we propose a deep learning architecture based on the transformer blueprint which, rather than directly making the above choice, predicts the visual properties of individual objects and their arrangements. The multidimensional predictions obtained in this way are then directly juxtaposed to choose the answer. We consider a few ways in which the model parses the visual input into tokens and several regimes of masking parts of the input in self-supervised training. In experimental assess
&lt;/p&gt;</description></item><item><title>SLoRA&#26159;&#19968;&#31181;&#32852;&#37030;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#21033;&#29992;&#20998;&#24067;&#24335;&#21644;&#31169;&#26377;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#20811;&#26381;&#39640;&#24322;&#26500;&#25968;&#25454;&#19979;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2308.06522</link><description>&lt;p&gt;
SLoRA: &#32852;&#37030;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SLoRA: Federated Parameter Efficient Fine-Tuning of Language Models. (arXiv:2308.06522v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06522
&lt;/p&gt;
&lt;p&gt;
SLoRA&#26159;&#19968;&#31181;&#32852;&#37030;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#21033;&#29992;&#20998;&#24067;&#24335;&#21644;&#31169;&#26377;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#20811;&#26381;&#39640;&#24322;&#26500;&#25968;&#25454;&#19979;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#36716;&#25442;&#22120;&#27169;&#22411;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#65292;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#22312;&#27809;&#26377;&#38598;&#20013;&#24335;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#32852;&#37030;&#23398;&#20064;&#21487;&#20197;&#20174;&#32852;&#37030;&#23398;&#20064;&#36793;&#32536;&#23458;&#25143;&#31471;&#30340;&#20998;&#24067;&#24335;&#21644;&#31169;&#26377;&#25968;&#25454;&#20013;&#21463;&#30410;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36793;&#32536;&#35774;&#22791;&#30340;&#26377;&#38480;&#36890;&#20449;&#12289;&#35745;&#31639;&#21644;&#23384;&#20648;&#33021;&#21147;&#20197;&#21450;&#27969;&#34892;&#30340;&#36716;&#25442;&#22120;&#27169;&#22411;&#30340;&#24040;&#22823;&#22823;&#23567;&#65292;&#39640;&#25928;&#30340;&#24494;&#35843;&#23545;&#20110;&#20351;&#32852;&#37030;&#35757;&#32451;&#25104;&#20026;&#21487;&#34892;&#30340;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#19981;&#21516;&#30340;&#32852;&#37030;&#23398;&#20064;&#35774;&#32622;&#19979;&#24212;&#29992;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65288;PEFT&#65289;&#22312;&#35821;&#35328;&#20219;&#21153;&#20013;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#38543;&#30528;&#29992;&#25143;&#20043;&#38388;&#30340;&#25968;&#25454;&#36234;&#26469;&#36234;&#22810;&#26679;&#21270;&#65292;&#20840;&#38754;&#24494;&#35843;&#27169;&#22411;&#21644;&#20351;&#29992;PEFT&#26041;&#27861;&#20043;&#38388;&#30340;&#24046;&#36317;&#21464;&#22823;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#20010;&#24615;&#33021;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SLoRA&#30340;&#26041;&#27861;&#65292;&#23427;&#20811;&#26381;&#20102;&#39640;&#24322;&#26500;&#25968;&#25454;&#19979;LoRA&#30340;&#20851;&#38190;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning via fine-tuning pre-trained transformer models has gained significant success in delivering state-of-the-art results across various NLP tasks. In the absence of centralized data, Federated Learning (FL) can benefit from distributed and private data of the FL edge clients for fine-tuning. However, due to the limited communication, computation, and storage capabilities of edge devices and the huge sizes of popular transformer models, efficient fine-tuning is crucial to make federated training feasible. This work explores the opportunities and challenges associated with applying parameter efficient fine-tuning (PEFT) methods in different FL settings for language tasks. Specifically, our investigation reveals that as the data across users becomes more diverse, the gap between fully fine-tuning the model and employing PEFT methods widens. To bridge this performance gap, we propose a method called SLoRA, which overcomes the key limitations of LoRA in high heterogeneous data
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#36164;&#28304;&#21463;&#38480;&#30340;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#22312;&#26080;&#32447;&#32593;&#32476;&#20013;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#36890;&#20449;&#26041;&#26696;&#24182;&#20248;&#21270;&#20102;&#36890;&#20449;&#25928;&#29575;&#12290;&#30740;&#31350;&#32467;&#26524;&#21253;&#25324;&#25968;&#23383;&#21644;&#27169;&#25311;&#20256;&#36755;&#26041;&#27861;&#30340;&#25910;&#25947;&#30028;&#38480;&#12289;&#36164;&#28304;&#20998;&#37197;&#21644;&#25910;&#25947;&#36895;&#24230;&#65292;&#20197;&#21450;&#27169;&#25311;&#20256;&#36755;&#20013;&#20449;&#36947;&#34928;&#33853;&#21644;&#22122;&#22768;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.06496</link><description>&lt;p&gt;
&#36164;&#28304;&#21463;&#38480;&#30340;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#22312;&#26080;&#32447;&#32593;&#32476;&#20013;&#30340;&#24615;&#33021;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Performance Analysis for Resource Constrained Decentralized Federated Learning Over Wireless Networks. (arXiv:2308.06496v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#36164;&#28304;&#21463;&#38480;&#30340;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#22312;&#26080;&#32447;&#32593;&#32476;&#20013;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#36890;&#20449;&#26041;&#26696;&#24182;&#20248;&#21270;&#20102;&#36890;&#20449;&#25928;&#29575;&#12290;&#30740;&#31350;&#32467;&#26524;&#21253;&#25324;&#25968;&#23383;&#21644;&#27169;&#25311;&#20256;&#36755;&#26041;&#27861;&#30340;&#25910;&#25947;&#30028;&#38480;&#12289;&#36164;&#28304;&#20998;&#37197;&#21644;&#25910;&#25947;&#36895;&#24230;&#65292;&#20197;&#21450;&#27169;&#25311;&#20256;&#36755;&#20013;&#20449;&#36947;&#34928;&#33853;&#21644;&#22122;&#22768;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#21487;&#20197;&#23548;&#33268;&#26174;&#33879;&#30340;&#36890;&#20449;&#24320;&#38144;&#24182;&#20381;&#36182;&#20110;&#20013;&#22830;&#26381;&#21153;&#22120;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#26356;&#20855;&#24377;&#24615;&#30340;&#26694;&#26550;&#12290;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#28041;&#21450;&#36890;&#36807;&#26080;&#32447;&#32593;&#32476;&#22312;&#35774;&#22791;&#20043;&#38388;&#36827;&#34892;&#21442;&#25968;&#20132;&#25442;&#12290;&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#22312;&#26080;&#32447;&#32593;&#32476;&#19978;&#20351;&#29992;&#19981;&#21516;&#36890;&#20449;&#26041;&#26696;&#65288;&#25968;&#23383;&#21644;&#27169;&#25311;&#65289;&#36827;&#34892;&#36164;&#28304;&#21463;&#38480;&#30340;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#20197;&#20248;&#21270;&#36890;&#20449;&#25928;&#29575;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#25968;&#23383;&#21644;&#27169;&#25311;&#20256;&#36755;&#26041;&#27861;&#30340;&#25910;&#25947;&#30028;&#38480;&#65292;&#20351;&#24471;&#21487;&#20197;&#20998;&#26512;&#22312;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#25968;&#23383;&#20256;&#36755;&#65292;&#25105;&#20204;&#30740;&#31350;&#21644;&#20998;&#26512;&#20102;&#35745;&#31639;&#21644;&#36890;&#20449;&#20043;&#38388;&#30340;&#36164;&#28304;&#20998;&#37197;&#20197;&#21450;&#25910;&#25947;&#36895;&#24230;&#65292;&#24471;&#21040;&#20102;&#20854;&#36890;&#20449;&#22797;&#26434;&#24230;&#21644;&#25910;&#25947;&#20445;&#35777;&#25152;&#38656;&#30340;&#26368;&#23567;&#32416;&#38169;&#36890;&#20449;&#27010;&#29575;&#12290;&#23545;&#20110;&#27169;&#25311;&#20256;&#36755;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#20449;&#36947;&#34928;&#33853;&#21644;&#22122;&#22768;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) can lead to significant communication overhead and reliance on a central server. To address these challenges, decentralized federated learning (DFL) has been proposed as a more resilient framework. DFL involves parameter exchange between devices through a wireless network. This study analyzes the performance of resource-constrained DFL using different communication schemes (digital and analog) over wireless networks to optimize communication efficiency. Specifically, we provide convergence bounds for both digital and analog transmission approaches, enabling analysis of the model performance trained on DFL. Furthermore, for digital transmission, we investigate and analyze resource allocation between computation and communication and convergence rates, obtaining its communication complexity and the minimum probability of correction communication required for convergence guarantee. For analog transmission, we discuss the impact of channel fading and noise on the mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21516;&#36136;&#38899;&#39057;-&#25991;&#26412;&#23884;&#20837;&#30340;&#28789;&#27963;&#20851;&#38190;&#35789;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23567;&#22411;&#30340;&#38899;&#39057;-compliant&#25991;&#26412;&#32534;&#30721;&#22120;&#65292;&#23558;&#25991;&#26412;&#36716;&#21270;&#20026;&#38899;&#32032;&#34920;&#31034;&#24182;&#19982;&#38899;&#39057;&#36827;&#34892;&#32852;&#21512;&#20998;&#26512;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20851;&#38190;&#35789;&#26816;&#27979;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.06472</link><description>&lt;p&gt;
&#22522;&#20110;&#21516;&#36136;&#38899;&#39057;-&#25991;&#26412;&#23884;&#20837;&#30340;&#28789;&#27963;&#20851;&#38190;&#35789;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Flexible Keyword Spotting based on Homogeneous Audio-Text Embedding. (arXiv:2308.06472v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21516;&#36136;&#38899;&#39057;-&#25991;&#26412;&#23884;&#20837;&#30340;&#28789;&#27963;&#20851;&#38190;&#35789;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23567;&#22411;&#30340;&#38899;&#39057;-compliant&#25991;&#26412;&#32534;&#30721;&#22120;&#65292;&#23558;&#25991;&#26412;&#36716;&#21270;&#20026;&#38899;&#32032;&#34920;&#31034;&#24182;&#19982;&#38899;&#39057;&#36827;&#34892;&#32852;&#21512;&#20998;&#26512;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20851;&#38190;&#35789;&#26816;&#27979;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39057;&#32321;&#20351;&#29992;&#25991;&#26412;&#26469;&#34920;&#31034;&#29992;&#25143;&#33258;&#23450;&#20041;/&#28789;&#27963;&#20851;&#38190;&#35789;&#30340;&#22330;&#26223;&#20013;&#65292;&#36890;&#24120;&#20351;&#29992;&#26114;&#36149;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#19982;&#38899;&#39057;&#32534;&#30721;&#22120;&#22312;&#23884;&#20837;&#31354;&#38388;&#36827;&#34892;&#32852;&#21512;&#20998;&#26512;&#65292;&#20294;&#36825;&#21487;&#33021;&#23548;&#33268;&#27169;&#24577;&#34920;&#31034;&#19981;&#19968;&#33268;&#65288;&#21363;&#36739;&#22823;&#30340;&#19981;&#21305;&#37197;&#65289;&#21644;&#22686;&#21152;&#22797;&#26434;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#22522;&#20110;&#31526;&#21512;&#38899;&#39057;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#26469;&#39640;&#25928;&#26816;&#27979;&#20219;&#24847;&#20851;&#38190;&#35789;&#65292;&#36825;&#20010;&#32534;&#30721;&#22120;&#22312;&#38899;&#39057;&#23884;&#20837;&#26041;&#38754;&#20855;&#26377;&#21516;&#36136;&#30340;&#34920;&#31034;&#65292;&#19988;&#27604;&#20860;&#23481;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#23567;&#24471;&#22810;&#12290;&#25105;&#20204;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#20351;&#29992;&#23383;&#32032;&#21040;&#38899;&#32032;&#65288;G2P&#65289;&#27169;&#22411;&#23558;&#25991;&#26412;&#36716;&#21270;&#20026;&#38899;&#32032;&#65292;&#28982;&#21518;&#20351;&#29992;&#20174;&#24050;&#37197;&#23545;&#30340;&#38899;&#39057;&#32534;&#30721;&#22120;&#25552;&#21462;&#30340;&#20195;&#34920;&#24615;&#38899;&#32032;&#21521;&#37327;&#23558;&#20854;&#36716;&#21270;&#20026;&#23884;&#20837;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#20135;&#29983;&#28151;&#28102;&#20851;&#38190;&#35789;&#26469;&#36827;&#19968;&#27493;&#22686;&#24378;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#24320;&#21457;&#20986;&#20855;&#26377;&#24378;&#22823;&#36776;&#21035;&#33021;&#21147;&#30340;&#38899;&#39057;-&#25991;&#26412;&#23884;&#20837;&#39564;&#35777;&#22120;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#26696;&#22312;Libriphrase&#38590;&#24230;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spotting user-defined/flexible keywords represented in text frequently uses an expensive text encoder for joint analysis with an audio encoder in an embedding space, which can suffer from heterogeneous modality representation (i.e., large mismatch) and increased complexity. In this work, we propose a novel architecture to efficiently detect arbitrary keywords based on an audio-compliant text encoder which inherently has homogeneous representation with audio embedding, and it is also much smaller than a compatible text encoder. Our text encoder converts the text to phonemes using a grapheme-to-phoneme (G2P) model, and then to an embedding using representative phoneme vectors, extracted from the paired audio encoder on rich speech datasets. We further augment our method with confusable keyword generation to develop an audio-text embedding verifier with strong discriminative power. Experimental results show that our scheme outperforms the state-of-the-art results on Libriphrase hard datas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;Volterra&#24378;&#35843;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#21487;&#36890;&#37327;&#24615; (VANYA) &#27169;&#22411;&#26469;&#27169;&#25311;&#26862;&#26519;&#30733;&#20240;&#65292;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#25429;&#39135;&#32773;-&#34987;&#25429;&#39135;&#32773;&#21160;&#21147;&#23398;&#65292;&#24182;&#22312;&#23545;&#20122;&#39532;&#36874;&#38632;&#26519;&#25968;&#25454;&#36827;&#34892;&#20102;&#39044;&#27979;&#65292;&#24182;&#19982;&#20854;&#20182;&#39044;&#27979;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2308.06471</link><description>&lt;p&gt;
Volterra&#24378;&#35843;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#21487;&#36890;&#37327;&#24615; (VANYA) &#22312;&#26862;&#26519;&#30733;&#20240;&#27169;&#25311;&#20013;&#30340;&#24212;&#29992;&#65306;&#20197;&#20122;&#39532;&#36874;&#38632;&#26519;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Volterra Accentuated Non-Linear Dynamical Admittance (VANYA) to model Deforestation: An Exemplification from the Amazon Rainforest. (arXiv:2308.06471v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06471
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;Volterra&#24378;&#35843;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#21487;&#36890;&#37327;&#24615; (VANYA) &#27169;&#22411;&#26469;&#27169;&#25311;&#26862;&#26519;&#30733;&#20240;&#65292;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#25429;&#39135;&#32773;-&#34987;&#25429;&#39135;&#32773;&#21160;&#21147;&#23398;&#65292;&#24182;&#22312;&#23545;&#20122;&#39532;&#36874;&#38632;&#26519;&#25968;&#25454;&#36827;&#34892;&#20102;&#39044;&#27979;&#65292;&#24182;&#19982;&#20854;&#20182;&#39044;&#27979;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#33258;&#21160;&#21270;&#25216;&#26415;&#36890;&#36807;&#20854;&#26368;&#26032;&#30340;&#25216;&#26415;&#36827;&#23637;&#65292;&#22312;&#25269;&#24481;&#39123;&#39118;&#12289;&#24178;&#26097;&#21644;&#22320;&#38663;&#31561;&#26041;&#38754;&#32473;&#20104;&#20102;&#25105;&#20204;&#25903;&#25345;&#12290;&#31639;&#27861;&#23398;&#20064;&#24050;&#32463;&#25512;&#21160;&#20102;&#31070;&#32463;&#31185;&#23398;&#12289;&#36951;&#20256;&#23398;&#21644;&#20154;&#26426;&#20132;&#20114;&#31561;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#23545;&#36827;&#23637;&#36215;&#21040;&#20102;&#20419;&#36827;&#20316;&#29992;&#12290;&#22312;&#20256;&#32479;&#39046;&#22495;&#20013;&#37319;&#29992;&#36825;&#20123;&#26041;&#27861;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;&#31070;&#32463;&#32593;&#32476;&#38754;&#20020;&#29702;&#35299;&#21644;&#20559;&#35265;&#38382;&#39064;&#12290;&#20154;&#24037;&#26234;&#33021;&#22312;&#31185;&#23398;&#39046;&#22495;&#30340;&#25193;&#23637;&#26159;&#30001;&#20110;&#20854;&#21487;&#36866;&#24212;&#30340;&#31616;&#21333;&#25551;&#36848;&#31526;&#21644;&#32452;&#21512;&#35770;&#35777;&#12290;&#26412;&#25991;&#20391;&#37325;&#20110;&#21033;&#29992;VANYA&#27169;&#22411;&#39044;&#27979;&#26519;&#22320;&#25439;&#22833;&#65292;&#24182;&#32467;&#21512;&#25429;&#39135;&#32773;-&#34987;&#25429;&#39135;&#32773;&#21160;&#21147;&#23398;&#12290;VANYA&#27169;&#22411;&#23545;&#20122;&#39532;&#36874;&#38632;&#26519;&#30340;&#25968;&#25454;&#36827;&#34892;&#20102;&#39044;&#27979;&#65292;&#24182;&#19982;&#20854;&#20182;&#39044;&#27979;&#26041;&#27861;&#65288;&#22914;&#38271;&#30701;&#26399;&#35760;&#24518;&#12289;N-BEATS&#21644;RCN&#65289;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intelligent automation supports us against cyclones, droughts, and seismic events with recent technology advancements. Algorithmic learning has advanced fields like neuroscience, genetics, and human-computer interaction. Time-series data boosts progress. Challenges persist in adopting these approaches in traditional fields. Neural networks face comprehension and bias issues. AI's expansion across scientific areas is due to adaptable descriptors and combinatorial argumentation. This article focuses on modeling Forest loss using the VANYA Model, incorporating Prey Predator Dynamics. VANYA predicts forest cover, demonstrated on Amazon Rainforest data against other forecasters like Long Short-Term Memory, N-BEATS, RCN.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23567;&#32780;&#39640;&#25928;&#30340;&#36793;&#32536;&#26816;&#27979;&#27169;&#22411;TEED&#65292;&#23427;&#20855;&#26377;&#26497;&#20302;&#30340;&#22797;&#26434;&#24230;&#21644;&#21442;&#25968;&#25968;&#37327;&#65292;&#33021;&#22815;&#22312;&#30701;&#26102;&#38388;&#20869;&#35757;&#32451;&#24182;&#19988;&#33021;&#22815;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#36793;&#32536;&#22270;&#20687;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#29992;&#20110;&#27979;&#35797;&#36793;&#32536;&#26816;&#27979;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.06468</link><description>&lt;p&gt;
&#36793;&#32536;&#26816;&#27979;&#27867;&#21270;&#30340;&#23567;&#32780;&#39640;&#25928;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Tiny and Efficient Model for the Edge Detection Generalization. (arXiv:2308.06468v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06468
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23567;&#32780;&#39640;&#25928;&#30340;&#36793;&#32536;&#26816;&#27979;&#27169;&#22411;TEED&#65292;&#23427;&#20855;&#26377;&#26497;&#20302;&#30340;&#22797;&#26434;&#24230;&#21644;&#21442;&#25968;&#25968;&#37327;&#65292;&#33021;&#22815;&#22312;&#30701;&#26102;&#38388;&#20869;&#35757;&#32451;&#24182;&#19988;&#33021;&#22815;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#36793;&#32536;&#22270;&#20687;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#29992;&#20110;&#27979;&#35797;&#36793;&#32536;&#26816;&#27979;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#39640;&#32423;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20381;&#36182;&#20110;&#20302;&#32423;&#22270;&#20687;&#25805;&#20316;&#20316;&#20026;&#20854;&#21021;&#22987;&#36807;&#31243;&#12290;&#35832;&#22914;&#36793;&#32536;&#26816;&#27979;&#12289;&#22270;&#20687;&#22686;&#24378;&#21644;&#36229;&#20998;&#36776;&#29575;&#31561;&#25805;&#20316;&#20026;&#26356;&#39640;&#32423;&#30340;&#22270;&#20687;&#20998;&#26512;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19977;&#20010;&#20027;&#35201;&#30446;&#26631;&#65292;&#21363;&#31616;&#21333;&#24615;&#12289;&#25928;&#29575;&#24615;&#21644;&#27867;&#21270;&#24615;&#65292;&#20197;&#35299;&#20915;&#36793;&#32536;&#26816;&#27979;&#38382;&#39064;&#65292;&#22240;&#20026;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#36793;&#32536;&#26816;&#27979;&#27169;&#22411;&#20026;&#20102;&#25552;&#39640;&#20934;&#30830;&#24615;&#32780;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Tiny and Efficient Edge Detector (TEED)&#30340;&#36731;&#37327;&#32423;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#20165;&#26377;58K&#20010;&#21442;&#25968;&#65292;&#26159;&#24403;&#21069;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;0.2%&#12290;&#22312;BIPED&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20165;&#38656;&#19981;&#21040;30&#20998;&#38047;&#65292;&#27599;&#20010;epoch&#32791;&#26102;&#19981;&#21040;5&#20998;&#38047;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#26131;&#20110;&#35757;&#32451;&#65292;&#24182;&#22312;&#26368;&#26089;&#20960;&#20010;epoch&#20869;&#24555;&#36895;&#25910;&#25947;&#65292;&#21516;&#26102;&#39044;&#27979;&#30340;&#36793;&#32536;&#22270;&#20687;&#28165;&#26224;&#19988;&#36136;&#37327;&#36739;&#39640;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#26469;&#27979;&#35797;&#36793;&#32536;&#26816;&#27979;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most high-level computer vision tasks rely on low-level image operations as their initial processes. Operations such as edge detection, image enhancement, and super-resolution, provide the foundations for higher level image analysis. In this work we address the edge detection considering three main objectives: simplicity, efficiency, and generalization since current state-of-the-art (SOTA) edge detection models are increased in complexity for better accuracy. To achieve this, we present Tiny and Efficient Edge Detector (TEED), a light convolutional neural network with only $58K$ parameters, less than $0.2$% of the state-of-the-art models. Training on the BIPED dataset takes $less than 30 minutes$, with each epoch requiring $less than 5 minutes$. Our proposed model is easy to train and it quickly converges within very first few epochs, while the predicted edge-maps are crisp and of high quality. Additionally, we propose a new dataset to test the generalization of edge detection, which c
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#26410;&#30693;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25361;&#25112;&#38024;&#23545;&#36825;&#20123;&#25915;&#20987;&#30340;&#29616;&#26377;&#38450;&#24481;&#26426;&#21046;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20165;&#20351;&#29992;&#40065;&#26834;&#29305;&#24449;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;DNN&#27169;&#22411;&#24182;&#19981;&#19968;&#23450;&#33021;&#25269;&#25239;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2308.06467</link><description>&lt;p&gt;
&#24182;&#19981;&#37027;&#20040;&#24378;&#22823;&#65306;&#35780;&#20272;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#26410;&#30693;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Not So Robust After All: Evaluating the Robustness of Deep Neural Networks to Unseen Adversarial Attacks. (arXiv:2308.06467v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06467
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#26410;&#30693;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25361;&#25112;&#38024;&#23545;&#36825;&#20123;&#25915;&#20987;&#30340;&#29616;&#26377;&#38450;&#24481;&#26426;&#21046;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20165;&#20351;&#29992;&#40065;&#26834;&#29305;&#24449;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;DNN&#27169;&#22411;&#24182;&#19981;&#19968;&#23450;&#33021;&#25269;&#25239;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#23601;&#65292;&#22914;&#20998;&#31867;&#12289;&#35782;&#21035;&#21644;&#39044;&#27979;&#65292;&#36825;&#24341;&#36215;&#20102;&#23545;&#20854;&#23646;&#24615;&#30340;&#22686;&#21152;&#20851;&#27880;&#12290;&#20256;&#32479;DNN&#30340;&#19968;&#20010;&#22522;&#26412;&#23646;&#24615;&#26159;&#23427;&#20204;&#23545;&#36755;&#20837;&#25968;&#25454;&#30340;&#20462;&#25913;&#30340;&#33030;&#24369;&#24615;&#65292;&#36825;&#23548;&#33268;&#20102;&#23545;&#23545;&#25239;&#25915;&#20987;&#30340;&#35843;&#26597;&#12290;&#36825;&#20123;&#25915;&#20987;&#36890;&#36807;&#25805;&#32437;&#25968;&#25454;&#26469;&#35823;&#23548;DNN&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25361;&#25112;&#38024;&#23545;&#23545;&#25239;&#25915;&#20987;&#30340;&#24403;&#20195;&#38450;&#24481;&#26426;&#21046;&#30340;&#26377;&#25928;&#24615;&#21644;&#27867;&#21270;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;Ilyas&#31561;&#20154;&#25552;&#20986;&#30340;&#20551;&#35774;&#65292;&#21363;DNN&#30340;&#22270;&#20687;&#29305;&#24449;&#21487;&#20197;&#26159;&#40065;&#26834;&#30340;&#25110;&#38750;&#40065;&#26834;&#30340;&#65292;&#32780;&#23545;&#25239;&#24615;&#25915;&#20987;&#38024;&#23545;&#30340;&#26159;&#21518;&#32773;&#12290;&#35813;&#20551;&#35774;&#35748;&#20026;&#65292;&#20165;&#22312;&#30001;&#40065;&#26834;&#29305;&#24449;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;DNN&#24212;&#35813;&#20135;&#29983;&#23545;&#25239;&#25915;&#20987;&#20855;&#26377;&#25269;&#25239;&#21147;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#36825;&#24182;&#19981;&#26222;&#36941;&#25104;&#31435;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#20102;&#35299;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) have gained prominence in various applications, such as classification, recognition, and prediction, prompting increased scrutiny of their properties. A fundamental attribute of traditional DNNs is their vulnerability to modifications in input data, which has resulted in the investigation of adversarial attacks. These attacks manipulate the data in order to mislead a DNN. This study aims to challenge the efficacy and generalization of contemporary defense mechanisms against adversarial attacks. Specifically, we explore the hypothesis proposed by Ilyas et. al, which posits that DNN image features can be either robust or non-robust, with adversarial attacks targeting the latter. This hypothesis suggests that training a DNN on a dataset consisting solely of robust features should produce a model resistant to adversarial attacks. However, our experiments demonstrate that this is not universally true. To gain further insights into our findings, we analyze the imp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#19968;&#32500;&#39044;&#27979;&#36816;&#21160;&#30690;&#37327;&#26368;&#20248;&#24615;&#30340;HEVC&#35270;&#39057;&#38544;&#20889;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#21644;&#23450;&#20041;&#26368;&#20248;&#27604;&#29305;&#29575;&#20316;&#20026;&#38544;&#20889;&#20998;&#26512;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.06464</link><description>&lt;p&gt;
&#19968;&#31181;&#20351;&#29992;&#39044;&#27979;&#36816;&#21160;&#30690;&#37327;&#26368;&#20248;&#24615;&#30340;&#19968;&#32500;HEVC&#35270;&#39057;&#38544;&#20889;&#20998;&#26512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A One-dimensional HEVC video steganalysis method using the Optimality of Predicted Motion Vectors. (arXiv:2308.06464v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06464
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#19968;&#32500;&#39044;&#27979;&#36816;&#21160;&#30690;&#37327;&#26368;&#20248;&#24615;&#30340;HEVC&#35270;&#39057;&#38544;&#20889;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#21644;&#23450;&#20041;&#26368;&#20248;&#27604;&#29305;&#29575;&#20316;&#20026;&#38544;&#20889;&#20998;&#26512;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38544;&#20889;&#20998;&#26512;&#25216;&#26415;&#20013;&#65292;&#23545;&#22522;&#20110;&#36816;&#21160;&#30690;&#37327;&#65288;MV&#65289;&#22495;&#30340;HEVC&#35270;&#39057;&#38544;&#20889;&#26415;&#30340;&#26816;&#27979;&#20173;&#28982;&#26159;&#19968;&#20010;&#28909;&#38376;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#25552;&#39640;&#26816;&#27979;&#24615;&#33021;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19968;&#32500;&#39044;&#27979;MV&#26368;&#20248;&#24615;&#30340;&#38544;&#20889;&#20998;&#26512;&#29305;&#24449;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25351;&#20986;&#20351;&#29992;&#39640;&#32423;&#36816;&#21160;&#30690;&#37327;&#39044;&#27979;&#65288;AMVP&#65289;&#25216;&#26415;&#32534;&#30721;&#30340;&#39044;&#27979;&#21333;&#20803;&#65288;PU&#65289;&#30340;&#36816;&#21160;&#30690;&#37327;&#39044;&#27979;&#65288;MVP&#65289;&#28385;&#36275;&#23553;&#38754;&#35270;&#39057;&#20013;&#30340;&#23616;&#37096;&#26368;&#20248;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20998;&#26512;HEVC&#35270;&#39057;&#20013;&#20351;&#29992;MVP&#32034;&#24341;&#25110;&#36816;&#21160;&#30690;&#37327;&#24046;&#65288;MVD&#65289;&#36827;&#34892;&#28040;&#24687;&#23884;&#20837;&#21487;&#33021;&#30772;&#22351;&#19978;&#36848;MVP&#30340;&#26368;&#20248;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;HEVC&#35270;&#39057;&#20013;MVP&#30340;&#26368;&#20248;&#27604;&#29305;&#29575;&#20316;&#20026;&#19968;&#31181;&#38544;&#20889;&#20998;&#26512;&#29305;&#24449;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#20004;&#20010;&#24120;&#35265;&#25968;&#25454;&#38598;&#19978;&#23545;&#19977;&#31181;&#27969;&#34892;&#38544;&#20889;&#26041;&#27861;&#36827;&#34892;&#20102;&#38544;&#20889;&#20998;&#26512;&#26816;&#27979;&#23454;&#39564;&#65292;&#24182;&#19982;&#22235;&#31181;&#26368;&#20808;&#36827;&#30340;&#38544;&#20889;&#20998;&#26512;&#26041;&#27861;&#36827;&#34892;&#20102;&#24615;&#33021;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Among steganalysis techniques, detection against motion vector (MV) domain-based video steganography in High Efficiency Video Coding (HEVC) standard remains a hot and challenging issue. For the purpose of improving the detection performance, this paper proposes a steganalysis feature based on the optimality of predicted MVs with a dimension of one. Firstly, we point out that the motion vector prediction (MVP) of the prediction unit (PU) encoded using the Advanced Motion Vector Prediction (AMVP) technique satisfies the local optimality in the cover video. Secondly, we analyze that in HEVC video, message embedding either using MVP index or motion vector differences (MVD) may destroy the above optimality of MVP. And then, we define the optimal rate of MVP in HEVC video as a steganalysis feature. Finally, we conduct steganalysis detection experiments on two general datasets for three popular steganography methods and compare the performance with four state-of-the-art steganalysis methods. 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#38376;&#38024;&#23545;&#22810;&#26631;&#31614;&#23398;&#20064;&#30340;&#26032;&#39062;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22810;&#26631;&#31614;&#38382;&#39064;&#20998;&#35299;&#20026;&#19968;&#32452;&#20108;&#20998;&#31867;&#38382;&#39064;&#20197;&#21033;&#29992;&#36923;&#36753;&#22238;&#24402;&#30340;&#20449;&#24687;&#24615;&#35821;&#20041;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#26631;&#31614;&#23884;&#20837;&#30340;&#32467;&#26500;&#20449;&#24687;&#22686;&#24378;&#23398;&#20064;&#21040;&#30340;&#29305;&#24449;&#34920;&#31034;&#30340;&#29420;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.06453</link><description>&lt;p&gt;
&#22810;&#26631;&#31614;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Multi-Label Knowledge Distillation. (arXiv:2308.06453v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06453
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#38376;&#38024;&#23545;&#22810;&#26631;&#31614;&#23398;&#20064;&#30340;&#26032;&#39062;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22810;&#26631;&#31614;&#38382;&#39064;&#20998;&#35299;&#20026;&#19968;&#32452;&#20108;&#20998;&#31867;&#38382;&#39064;&#20197;&#21033;&#29992;&#36923;&#36753;&#22238;&#24402;&#30340;&#20449;&#24687;&#24615;&#35821;&#20041;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#26631;&#31614;&#23884;&#20837;&#30340;&#32467;&#26500;&#20449;&#24687;&#22686;&#24378;&#23398;&#20064;&#21040;&#30340;&#29305;&#24449;&#34920;&#31034;&#30340;&#29420;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#36890;&#24120;&#36890;&#36807;&#23558;&#25945;&#24072;&#32593;&#32476;&#30340;&#36755;&#20986;&#36923;&#36753;&#22238;&#24402;&#25110;&#20013;&#38388;&#29305;&#24449;&#26144;&#23556;&#20256;&#25480;&#32473;&#23398;&#29983;&#32593;&#32476;&#26469;&#24037;&#20316;&#65292;&#22312;&#22810;&#31867;&#21333;&#26631;&#31614;&#23398;&#20064;&#20013;&#38750;&#24120;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22312;&#22810;&#26631;&#31614;&#23398;&#20064;&#22330;&#26223;&#19979;&#65292;&#36825;&#20123;&#26041;&#27861;&#24456;&#38590;&#25512;&#24191;&#65292;&#27599;&#20010;&#23454;&#20363;&#37117;&#19982;&#22810;&#20010;&#35821;&#20041;&#26631;&#31614;&#30456;&#20851;&#65292;&#22240;&#20026;&#39044;&#27979;&#27010;&#29575;&#19981;&#31561;&#20110;&#19968;&#65292;&#24182;&#19988;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25972;&#20010;&#31034;&#20363;&#30340;&#29305;&#24449;&#26144;&#23556;&#21487;&#33021;&#20250;&#24573;&#30053;&#23567;&#31867;&#21035;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#26631;&#31614;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#12290;&#19968;&#26041;&#38754;&#65292;&#23427;&#36890;&#36807;&#23558;&#22810;&#26631;&#31614;&#23398;&#20064;&#38382;&#39064;&#20998;&#35299;&#20026;&#19968;&#32452;&#20108;&#20998;&#31867;&#38382;&#39064;&#26469;&#21033;&#29992;&#36923;&#36753;&#22238;&#24402;&#20013;&#30340;&#20449;&#24687;&#24615;&#35821;&#20041;&#30693;&#35782;&#65307;&#21478;&#19968;&#26041;&#38754;&#65292;&#23427;&#36890;&#36807;&#21033;&#29992;&#26631;&#31614;&#23884;&#20837;&#30340;&#32467;&#26500;&#20449;&#24687;&#26469;&#22686;&#24378;&#23398;&#20064;&#21040;&#30340;&#29305;&#24449;&#34920;&#31034;&#30340;&#29420;&#29305;&#24615;&#12290;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing knowledge distillation methods typically work by imparting the knowledge of output logits or intermediate feature maps from the teacher network to the student network, which is very successful in multi-class single-label learning. However, these methods can hardly be extended to the multi-label learning scenario, where each instance is associated with multiple semantic labels, because the prediction probabilities do not sum to one and feature maps of the whole example may ignore minor classes in such a scenario. In this paper, we propose a novel multi-label knowledge distillation method. On one hand, it exploits the informative semantic knowledge from the logits by dividing the multi-label learning problem into a set of binary classification problems; on the other hand, it enhances the distinctiveness of the learned feature representations by leveraging the structural information of label-wise embeddings. Experimental results on multiple benchmark datasets validate that the pr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#23558;&#32858;&#31867;&#21644;&#31616;&#21270;&#32467;&#21512;&#36215;&#26469;&#65292;&#24182;&#25552;&#20379;&#20102;&#24314;&#27169;&#20219;&#24847;&#22270;&#32467;&#26500;&#30340;&#26694;&#26550;&#12290;&#36890;&#36807;&#23558;&#30828;&#32858;&#31867;&#25918;&#26494;&#20026;&#36719;&#32858;&#31867;&#65292;&#35813;&#31639;&#27861;&#23558;&#21487;&#33021;&#22256;&#38590;&#30340;&#32858;&#31867;&#38382;&#39064;&#25918;&#26494;&#20026;&#21487;&#35299;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.06448</link><description>&lt;p&gt;
&#28508;&#22312;&#38543;&#26426;&#27493;&#39588;&#20316;&#20026;&#26368;&#22823;&#21106;&#12289;&#26368;&#23567;&#21106;&#31561;&#38382;&#39064;&#30340;&#25918;&#26494;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Latent Random Steps as Relaxations of Max-Cut, Min-Cut, and More. (arXiv:2308.06448v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06448
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#23558;&#32858;&#31867;&#21644;&#31616;&#21270;&#32467;&#21512;&#36215;&#26469;&#65292;&#24182;&#25552;&#20379;&#20102;&#24314;&#27169;&#20219;&#24847;&#22270;&#32467;&#26500;&#30340;&#26694;&#26550;&#12290;&#36890;&#36807;&#23558;&#30828;&#32858;&#31867;&#25918;&#26494;&#20026;&#36719;&#32858;&#31867;&#65292;&#35813;&#31639;&#27861;&#23558;&#21487;&#33021;&#22256;&#38590;&#30340;&#32858;&#31867;&#38382;&#39064;&#25918;&#26494;&#20026;&#21487;&#35299;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33410;&#28857;&#32858;&#31867;&#31639;&#27861;&#36890;&#24120;&#20851;&#27880;&#20110;&#22312;&#22270;&#20013;&#23547;&#25214;&#21516;&#36136;&#32467;&#26500;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#23427;&#20204;&#23547;&#25214;&#20855;&#26377;&#35768;&#22810;&#36793;&#32536;&#30340;&#30456;&#20284;&#33410;&#28857;&#38598;&#21512;&#65292;&#32780;&#19981;&#26159;&#36328;&#38598;&#32676;&#12290;&#28982;&#32780;&#65292;&#22270;&#36890;&#24120;&#36824;&#23637;&#29616;&#20986;&#19981;&#21516;&#31181;&#32467;&#26500;&#65292;&#20363;&#22914;&#36817;&#20284;&#20108;&#20998;&#21644;&#19977;&#20998;&#22270;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#36793;&#32536;&#21457;&#29983;&#22312;&#38598;&#32676;&#20043;&#38388;&#12290;&#35299;&#20915;&#36825;&#31181;&#32467;&#26500;&#36890;&#24120;&#30041;&#32473;&#22270;&#31616;&#21270;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#23427;&#32479;&#19968;&#20102;&#32858;&#31867;&#21644;&#31616;&#21270;&#65292;&#24182;&#25552;&#20379;&#20102;&#24314;&#27169;&#20219;&#24847;&#22270;&#32467;&#26500;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22522;&#20110;&#23545;&#22270;&#19978;&#36827;&#34892;&#38543;&#26426;&#28216;&#36208;&#30340;&#36807;&#31243;&#36827;&#34892;&#20998;&#35299;&#12290;&#23427;&#20801;&#35768;&#26080;&#38480;&#21046;&#30340;&#21442;&#25968;&#21270;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#26799;&#24230;&#19979;&#38477;&#36827;&#34892;&#20248;&#21270;&#12290;&#36890;&#36807;&#23558;&#30828;&#32858;&#31867;&#25918;&#26494;&#20026;&#36719;&#32858;&#31867;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#23558;&#21487;&#33021;&#22256;&#38590;&#30340;&#32858;&#31867;&#38382;&#39064;&#25918;&#26494;&#20026;&#21487;&#35299;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#31639;&#27861;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithms for node clustering typically focus on finding homophilous structure in graphs. That is, they find sets of similar nodes with many edges within, rather than across, the clusters. However, graphs often also exhibit heterophilous structure, as exemplified by (nearly) bipartite and tripartite graphs, where most edges occur across the clusters. Grappling with such structure is typically left to the task of graph simplification. We present a probabilistic model based on non-negative matrix factorization which unifies clustering and simplification, and provides a framework for modeling arbitrary graph structure. Our model is based on factorizing the process of taking a random walk on the graph. It permits an unconstrained parametrization, allowing for optimization via simple gradient descent. By relaxing the hard clustering to a soft clustering, our algorithm relaxes potentially hard clustering problems to a tractable ones. We illustrate our algorithm's capabilities on a synthetic
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39034;&#24207;&#20803;&#36716;&#31227;&#65288;SMT&#65289;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#22312;&#39640;&#24230;&#38750;&#32447;&#24615;&#31995;&#32479;&#20013;&#30340;&#35757;&#32451;&#21644;&#36866;&#24212;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#12289;&#24555;&#36895;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2308.06447</link><description>&lt;p&gt;
&#39034;&#24207;&#20803;&#36716;&#31227;&#65288;SMT&#65289;&#23398;&#20064;&#29992;&#20110;&#24212;&#23545;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#30340;&#22797;&#26434;&#24615;&#65306;&#22312;&#22797;&#21512;&#26448;&#26009;&#28909;&#21387;&#36807;&#31243;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A Sequential Meta-Transfer (SMT) Learning to Combat Complexities of Physics-Informed Neural Networks: Application to Composites Autoclave Processing. (arXiv:2308.06447v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06447
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39034;&#24207;&#20803;&#36716;&#31227;&#65288;SMT&#65289;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#22312;&#39640;&#24230;&#38750;&#32447;&#24615;&#31995;&#32479;&#20013;&#30340;&#35757;&#32451;&#21644;&#36866;&#24212;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#12289;&#24555;&#36895;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#36890;&#36807;&#23558;&#29289;&#29702;&#23450;&#24459;&#34701;&#20837;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#20013;&#65292;&#24050;&#32463;&#22312;&#35299;&#20915;&#38750;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#26041;&#38754;&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#65292;&#24182;&#22312;&#35768;&#22810;&#31185;&#23398;&#21644;&#24037;&#31243;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;PINNs&#22312;&#20934;&#30830;&#36924;&#36817;&#20855;&#26377;&#24378;&#38750;&#32447;&#24615;&#30340;&#22797;&#26434;&#31995;&#32479;&#30340;&#35299;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#19968;&#23450;&#30340;&#23616;&#38480;&#24615;&#65292;&#23588;&#20854;&#26159;&#22312;&#38271;&#26102;&#38388;&#22495;&#19979;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;PINNs&#34987;&#35774;&#35745;&#20026;&#36924;&#36817;&#32473;&#23450;PDE&#31995;&#32479;&#30340;&#29305;&#23450;&#23454;&#29616;&#65292;&#23427;&#20204;&#32570;&#20047;&#24517;&#35201;&#30340;&#36890;&#29992;&#24615;&#26469;&#26377;&#25928;&#36866;&#24212;&#26032;&#31995;&#32479;&#37197;&#32622;&#12290;&#36825;&#23601;&#38656;&#35201;&#22312;&#31995;&#32479;&#21457;&#29983;&#20219;&#20309;&#21464;&#21270;&#26102;&#36827;&#34892;&#20174;&#22836;&#37325;&#26032;&#35757;&#32451;&#65292;&#35745;&#31639;&#25104;&#26412;&#36739;&#39640;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39034;&#24207;&#20803;&#36716;&#31227;&#65288;SMT&#65289;&#23398;&#20064;&#26694;&#26550;&#65292;&#20026;&#39640;&#24230;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;PINNs&#25552;&#20379;&#20102;&#24555;&#36895;&#35757;&#32451;&#21644;&#39640;&#25928;&#36866;&#24212;&#30340;&#32479;&#19968;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics-Informed Neural Networks (PINNs) have gained popularity in solving nonlinear partial differential equations (PDEs) via integrating physical laws into the training of neural networks, making them superior in many scientific and engineering applications. However, conventional PINNs still fall short in accurately approximating the solution of complex systems with strong nonlinearity, especially in long temporal domains. Besides, since PINNs are designed to approximate a specific realization of a given PDE system, they lack the necessary generalizability to efficiently adapt to new system configurations. This entails computationally expensive re-training from scratch for any new change in the system. To address these shortfalls, in this work a novel sequential meta-transfer (SMT) learning framework is proposed, offering a unified solution for both fast training and efficient adaptation of PINNs in highly nonlinear systems with long temporal domains. Specifically, the framework deco
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#31070;&#32463;&#28508;&#22312;&#23545;&#40784;&#22120;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#22797;&#26434;&#34892;&#20026;&#30340;&#31070;&#32463;&#34920;&#31034;&#12290;&#36890;&#36807;&#23545;&#40784;&#37325;&#22797;&#35797;&#39564;&#30340;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#26102;&#38388;&#24367;&#26354;&#27169;&#22411;&#35299;&#20915;&#26102;&#38388;&#19981;&#23545;&#40784;&#38382;&#39064;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#38477;&#32500;&#31354;&#38388;&#20013;&#23398;&#20064;&#21040;&#26356;&#22909;&#30340;&#34892;&#20026;&#35299;&#30721;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2308.06443</link><description>&lt;p&gt;
&#31070;&#32463;&#28508;&#22312;&#23545;&#40784;&#22120;&#65306;&#29992;&#20110;&#23398;&#20064;&#22797;&#26434;&#33258;&#28982;&#31070;&#32463;&#25968;&#25454;&#34920;&#31034;&#30340;&#36328;&#35797;&#39564;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Neural Latent Aligner: Cross-trial Alignment for Learning Representations of Complex, Naturalistic Neural Data. (arXiv:2308.06443v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06443
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#31070;&#32463;&#28508;&#22312;&#23545;&#40784;&#22120;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#22797;&#26434;&#34892;&#20026;&#30340;&#31070;&#32463;&#34920;&#31034;&#12290;&#36890;&#36807;&#23545;&#40784;&#37325;&#22797;&#35797;&#39564;&#30340;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#26102;&#38388;&#24367;&#26354;&#27169;&#22411;&#35299;&#20915;&#26102;&#38388;&#19981;&#23545;&#40784;&#38382;&#39064;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#38477;&#32500;&#31354;&#38388;&#20013;&#23398;&#20064;&#21040;&#26356;&#22909;&#30340;&#34892;&#20026;&#35299;&#30721;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#22797;&#26434;&#20154;&#31867;&#34892;&#20026;&#30340;&#31070;&#32463;&#23454;&#29616;&#26159;&#31070;&#32463;&#31185;&#23398;&#30340;&#20027;&#35201;&#30446;&#26631;&#20043;&#19968;&#12290;&#20026;&#27492;&#65292;&#23547;&#25214;&#31070;&#32463;&#25968;&#25454;&#30340;&#30495;&#23454;&#34920;&#31034;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#36825;&#30001;&#20110;&#34892;&#20026;&#30340;&#39640;&#22797;&#26434;&#24615;&#21644;&#20449;&#21495;&#21040;&#22122;&#22768;&#27604;&#65288;SNR&#65289;&#20302;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#31070;&#32463;&#28508;&#22312;&#23545;&#40784;&#22120;&#65288;NLA&#65289;&#65292;&#29992;&#20110;&#25214;&#21040;&#20855;&#26377;&#34892;&#20026;&#30456;&#20851;&#24615;&#30340;&#21463;&#38480;&#31070;&#32463;&#34920;&#31034;&#12290;&#20854;&#20851;&#38190;&#24605;&#24819;&#26159;&#36890;&#36807;&#23545;&#40784;&#37325;&#22797;&#35797;&#39564;&#30340;&#34920;&#31034;&#26469;&#23398;&#20064;&#36328;&#35797;&#39564;&#19968;&#33268;&#30340;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20840;&#21487;&#24494;&#20998;&#26102;&#38388;&#24367;&#26354;&#27169;&#22411;&#65288;TWM&#65289;&#65292;&#20197;&#35299;&#20915;&#35797;&#39564;&#20043;&#38388;&#30340;&#26102;&#38388;&#19981;&#23545;&#40784;&#38382;&#39064;&#12290;&#24403;&#24212;&#29992;&#20110;&#33258;&#28982;&#35328;&#35821;&#30340;&#39045;&#20869;&#33041;&#30005;&#22270;&#65288;ECoG&#65289;&#26102;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#38477;&#32500;&#31354;&#38388;&#20013;&#27604;&#22522;&#20934;&#27169;&#22411;&#26356;&#22909;&#22320;&#23398;&#20064;&#35299;&#30721;&#34892;&#20026;&#30340;&#34920;&#31034;&#12290;&#36890;&#36807;&#27979;&#37327;&#23545;&#40784;&#21644;&#34892;&#20026;&#20043;&#38388;&#30340;&#20851;&#32852;&#24615;&#65292;TWM&#22312;&#23454;&#39564;&#35777;&#23454;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the neural implementation of complex human behaviors is one of the major goals in neuroscience. To this end, it is crucial to find a true representation of the neural data, which is challenging due to the high complexity of behaviors and the low signal-to-ratio (SNR) of the signals. Here, we propose a novel unsupervised learning framework, Neural Latent Aligner (NLA), to find well-constrained, behaviorally relevant neural representations of complex behaviors. The key idea is to align representations across repeated trials to learn cross-trial consistent information. Furthermore, we propose a novel, fully differentiable time warping model (TWM) to resolve the temporal misalignment of trials. When applied to intracranial electrocorticography (ECoG) of natural speaking, our model learns better representations for decoding behaviors than the baseline models, especially in lower dimensional space. The TWM is empirically validated by measuring behavioral coherence between align
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38754;&#21521;&#22495;&#33258;&#36866;&#24212;&#30340;&#22522;&#20110;&#29289;&#29702;&#30693;&#35782;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;da-PINN&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#24322;&#36136;&#20171;&#36136;&#20013;&#40614;&#20811;&#26031;&#38886;&#26041;&#31243;&#30340;&#21453;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#20171;&#36136;&#30028;&#38754;&#20301;&#32622;&#21442;&#25968;&#19982;&#25439;&#22833;&#20989;&#25968;&#32467;&#21512;&#65292;&#24182;&#37319;&#29992;&#38754;&#21521;&#22495;&#33258;&#36866;&#24212;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;da-PINN&#22312;&#27714;&#35299;&#40614;&#20811;&#26031;&#38886;&#26041;&#31243;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.06436</link><description>&lt;p&gt;
&#19968;&#20010;&#38754;&#21521;&#22495;&#33258;&#36866;&#24212;&#30340;&#22522;&#20110;&#29289;&#29702;&#30693;&#35782;&#30340;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#27714;&#35299;&#24322;&#36136;&#20171;&#36136;&#19979;&#40614;&#20811;&#26031;&#38886;&#26041;&#31243;&#30340;&#21453;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
A Domain-adaptive Physics-informed Neural Network for Inverse Problems of Maxwell's Equations in Heterogeneous Media. (arXiv:2308.06436v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06436
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38754;&#21521;&#22495;&#33258;&#36866;&#24212;&#30340;&#22522;&#20110;&#29289;&#29702;&#30693;&#35782;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;da-PINN&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#24322;&#36136;&#20171;&#36136;&#20013;&#40614;&#20811;&#26031;&#38886;&#26041;&#31243;&#30340;&#21453;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#20171;&#36136;&#30028;&#38754;&#20301;&#32622;&#21442;&#25968;&#19982;&#25439;&#22833;&#20989;&#25968;&#32467;&#21512;&#65292;&#24182;&#37319;&#29992;&#38754;&#21521;&#22495;&#33258;&#36866;&#24212;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;da-PINN&#22312;&#27714;&#35299;&#40614;&#20811;&#26031;&#38886;&#26041;&#31243;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40614;&#20811;&#26031;&#38886;&#26041;&#31243;&#26159;&#19968;&#32452;&#32806;&#21512;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#65292;&#19982;&#27931;&#20262;&#20857;&#21147;&#27861;&#21017;&#19968;&#36215;&#26500;&#25104;&#20102;&#32463;&#20856;&#30005;&#30913;&#23398;&#21644;&#30005;&#36335;&#30340;&#22522;&#30784;&#12290;&#22312;&#35832;&#22914;&#30005;&#30913;&#25955;&#23556;&#21644;&#22825;&#32447;&#35774;&#35745;&#20248;&#21270;&#31561;&#39046;&#22495;&#20013;&#65292;&#26377;&#25928;&#27714;&#35299;&#40614;&#20811;&#26031;&#38886;&#26041;&#31243;&#33267;&#20851;&#37325;&#35201;&#12290;&#29289;&#29702;&#30693;&#35782;&#23548;&#21521;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#22312;&#27714;&#35299;PDEs&#26041;&#38754;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;PINNs&#22312;&#24322;&#36136;&#20171;&#36136;&#20013;&#27714;&#35299;&#40614;&#20811;&#26031;&#38886;&#26041;&#31243;&#20173;&#23384;&#22312;&#22256;&#38590;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#22495;&#33258;&#36866;&#24212;&#30340;PINN&#65288;da-PINN&#65289;&#65292;&#29992;&#20110;&#27714;&#35299;&#24322;&#36136;&#20171;&#36136;&#19979;&#40614;&#20811;&#26031;&#38886;&#26041;&#31243;&#30340;&#21453;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20171;&#36136;&#30028;&#38754;&#30340;&#20301;&#32622;&#21442;&#25968;&#65292;&#23558;&#25972;&#20010;&#22495;&#20998;&#35299;&#20026;&#22810;&#20010;&#23376;&#22495;&#12290;&#27492;&#22806;&#65292;&#23558;&#30005;&#30913;&#30028;&#38754;&#26465;&#20214;&#32467;&#21512;&#21040;&#25439;&#22833;&#20989;&#25968;&#20013;&#65292;&#20197;&#25552;&#39640;&#25509;&#36817;&#30028;&#38754;&#22788;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#22495;&#33258;&#36866;&#24212;&#30340;&#35757;&#32451;&#31574;&#30053;&#29992;&#20110;da-PINN&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;da-PINN&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Maxwell's equations are a collection of coupled partial differential equations (PDEs) that, together with the Lorentz force law, constitute the basis of classical electromagnetism and electric circuits. Effectively solving Maxwell's equations is crucial in various fields, like electromagnetic scattering and antenna design optimization. Physics-informed neural networks (PINNs) have shown powerful ability in solving PDEs. However, PINNs still struggle to solve Maxwell's equations in heterogeneous media. To this end, we propose a domain-adaptive PINN (da-PINN) to solve inverse problems of Maxwell's equations in heterogeneous media. First, we propose a location parameter of media interface to decompose the whole domain into several sub-domains. Furthermore, the electromagnetic interface conditions are incorporated into a loss function to improve the prediction performance near the interface. Then, we propose a domain-adaptive training strategy for da-PINN. Finally, the effectiveness of da-
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39044;&#27979;&#22788;&#29702;&#21518;&#26032;&#29983;&#34880;&#31649;&#24615;&#24180;&#40836;&#30456;&#20851;&#40644;&#26001;&#21464;&#24615;&#30340;&#21333;&#26102;&#38388;&#27573;&#30142;&#30149;&#28436;&#21464;&#32593;&#32476;(SHENet)&#65292;&#21487;&#20197;&#36890;&#36807;&#36755;&#20837;&#26415;&#21069;SD-OCT&#22270;&#20687;&#39044;&#27979;&#29983;&#25104;&#26415;&#21518;SD-OCT&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2308.06432</link><description>&lt;p&gt;
&#23398;&#20064;&#21333;&#26102;&#38388;&#27573;&#30142;&#30149;&#28436;&#21464;&#65292;&#29992;&#20110;&#39044;&#27979;&#22788;&#29702;&#21518;&#26032;&#29983;&#34880;&#31649;&#24615;&#24180;&#40836;&#30456;&#20851;&#40644;&#26001;&#21464;&#24615;&#30340;&#29983;&#25104;&#65288;arXiv:2308.06432v1 [eess.IV]&#65289;
&lt;/p&gt;
&lt;p&gt;
Learn Single-horizon Disease Evolution for Predictive Generation of Post-therapeutic Neovascular Age-related Macular Degeneration. (arXiv:2308.06432v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06432
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39044;&#27979;&#22788;&#29702;&#21518;&#26032;&#29983;&#34880;&#31649;&#24615;&#24180;&#40836;&#30456;&#20851;&#40644;&#26001;&#21464;&#24615;&#30340;&#21333;&#26102;&#38388;&#27573;&#30142;&#30149;&#28436;&#21464;&#32593;&#32476;(SHENet)&#65292;&#21487;&#20197;&#36890;&#36807;&#36755;&#20837;&#26415;&#21069;SD-OCT&#22270;&#20687;&#39044;&#27979;&#29983;&#25104;&#26415;&#21518;SD-OCT&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#23398;&#22270;&#20687;&#22788;&#29702;&#39046;&#22495;&#65292;&#22823;&#37096;&#20998;&#29616;&#26377;&#30340;&#30142;&#30149;&#39044;&#27979;&#26041;&#27861;&#21487;&#20197;&#24402;&#20026;&#20004;&#31867;&#65292;&#19968;&#31867;&#26159;&#22270;&#20687;&#21040;&#31867;&#21035;&#30340;&#39044;&#27979;&#65292;&#19968;&#31867;&#26159;&#22270;&#20687;&#21040;&#21442;&#25968;&#30340;&#39044;&#27979;&#12290;&#40092;&#26377;&#30740;&#31350;&#19987;&#27880;&#20110;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#39044;&#27979;&#12290;&#19982;&#20854;&#20182;&#39046;&#22495;&#30340;&#22810;&#26102;&#38388;&#27573;&#39044;&#27979;&#19981;&#21516;&#65292;&#30524;&#31185;&#21307;&#29983;&#26356;&#20542;&#21521;&#20110;&#23545;&#21333;&#26102;&#38388;&#27573;&#39044;&#27979;&#34920;&#31034;&#26356;&#22823;&#30340;&#20449;&#24515;&#65292;&#22240;&#20026;&#20182;&#20204;&#23545;&#39044;&#27979;&#39118;&#38505;&#30340;&#23481;&#24525;&#24230;&#36739;&#20302;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21333;&#26102;&#38388;&#27573;&#30142;&#30149;&#28436;&#21464;&#32593;&#32476;&#65288;SHENet&#65289;&#65292;&#36890;&#36807;&#36755;&#20837;&#24102;&#26377;&#26032;&#29983;&#34880;&#31649;&#24615;&#24180;&#40836;&#30456;&#20851;&#40644;&#26001;&#21464;&#24615;&#65288;nAMD&#65289;&#30340;&#26415;&#21069;SD-OCT&#22270;&#20687;&#65292;&#39044;&#27979;&#29983;&#25104;&#26415;&#21518;SD-OCT&#22270;&#20687;&#12290;&#22312;SHENet&#20013;&#65292;&#29305;&#24449;&#32534;&#30721;&#22120;&#23558;&#36755;&#20837;&#30340;SD-OCT&#22270;&#20687;&#36716;&#25442;&#20026;&#28145;&#23618;&#29305;&#24449;&#65292;&#28982;&#21518;&#22270;&#24418;&#28436;&#21464;&#27169;&#22359;&#22312;&#39640;&#32500;&#28508;&#22312;&#31354;&#38388;&#20013;&#39044;&#27979;&#30142;&#30149;&#28436;&#21464;&#36807;&#31243;&#24182;&#36755;&#20986;&#39044;&#27979;&#30340;&#28145;&#23618;&#29305;&#24449;&#65292;&#26368;&#21518;&#29305;&#24449;&#35299;&#30721;&#22120;&#23558;&#39044;&#27979;&#30340;&#28145;&#23618;&#29305;&#24449;&#24674;&#22797;&#20026;SD-OCT&#22270;&#20687;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#28436;&#21464;&#24378;&#21270;&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
Most of the existing disease prediction methods in the field of medical image processing fall into two classes, namely image-to-category predictions and image-to-parameter predictions. Few works have focused on image-to-image predictions. Different from multi-horizon predictions in other fields, ophthalmologists prefer to show more confidence in single-horizon predictions due to the low tolerance of predictive risk. We propose a single-horizon disease evolution network (SHENet) to predictively generate post-therapeutic SD-OCT images by inputting pre-therapeutic SD-OCT images with neovascular age-related macular degeneration (nAMD). In SHENet, a feature encoder converts the input SD-OCT images to deep features, then a graph evolution module predicts the process of disease evolution in high-dimensional latent space and outputs the predicted deep features, and lastly, feature decoder recovers the predicted deep features to SD-OCT images. We further propose an evolution reinforcement modul
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;&#21644;&#32593;&#32476;&#31185;&#23398;&#36827;&#34892;&#36951;&#20256;&#24322;&#36136;&#24615;&#20998;&#26512;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;&#22810;&#20010;&#29420;&#31435;&#29305;&#24449;&#36873;&#25321;&#36816;&#34892;&#30340;&#32593;&#32476;&#26469;&#25552;&#21462;&#36951;&#20256;&#21464;&#37327;&#30340;&#24322;&#36136;&#23376;&#38598;&#65292;&#24182;&#24341;&#20837;&#20102;&#32508;&#21512;&#29305;&#24449;&#39118;&#38505;&#35780;&#20998;&#26469;&#35299;&#37322;&#22522;&#22240;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;</title><link>http://arxiv.org/abs/2308.06429</link><description>&lt;p&gt;
&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;&#21644;&#32593;&#32476;&#31185;&#23398;&#36827;&#34892;&#36951;&#20256;&#24322;&#36136;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Genetic heterogeneity analysis using genetic algorithm and network science. (arXiv:2308.06429v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06429
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;&#21644;&#32593;&#32476;&#31185;&#23398;&#36827;&#34892;&#36951;&#20256;&#24322;&#36136;&#24615;&#20998;&#26512;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;&#22810;&#20010;&#29420;&#31435;&#29305;&#24449;&#36873;&#25321;&#36816;&#34892;&#30340;&#32593;&#32476;&#26469;&#25552;&#21462;&#36951;&#20256;&#21464;&#37327;&#30340;&#24322;&#36136;&#23376;&#38598;&#65292;&#24182;&#24341;&#20837;&#20102;&#32508;&#21512;&#29305;&#24449;&#39118;&#38505;&#35780;&#20998;&#26469;&#35299;&#37322;&#22522;&#22240;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22522;&#22240;&#32452;&#20851;&#32852;&#30740;&#31350;(GWAS)&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#27604;&#36739;&#24739;&#26377;&#29305;&#23450;&#30142;&#30149;&#21644;&#26410;&#24739;&#30149;&#20010;&#20307;&#30340;&#22522;&#22240;&#25968;&#25454;&#26469;&#30830;&#23450;&#30142;&#30149;&#26131;&#24863;&#30340;&#22522;&#22240;&#21464;&#37327;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20851;&#32852;&#30340;&#21457;&#29616;&#30001;&#20110;&#36951;&#20256;&#24322;&#36136;&#24615;&#21644;&#29305;&#24449;&#20132;&#20114;&#32780;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#36825;&#20123;&#21463;&#21040;&#36825;&#20123;&#24433;&#21709;&#30340;&#22522;&#22240;&#21464;&#37327;&#36890;&#24120;&#34920;&#29616;&#20986;&#36739;&#20302;&#30340;&#25928;&#24212;&#22823;&#23567;&#65292;&#22240;&#27492;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#24456;&#38590;&#26816;&#27979;&#21040;&#23427;&#20204;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;GWAS&#29305;&#24449;&#36873;&#25321;&#26426;&#21046;&#65292;&#21517;&#20026;Feature Co-selection Network (FCSNet)&#12290;FCS-Net&#26088;&#22312;&#20174;&#22522;&#20110;&#36951;&#20256;&#31639;&#27861;(GA)&#21644;&#36827;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#22810;&#20010;&#29420;&#31435;&#29305;&#24449;&#36873;&#25321;&#36816;&#34892;&#26500;&#24314;&#30340;&#32593;&#32476;&#20013;&#25552;&#21462;&#36951;&#20256;&#21464;&#37327;&#30340;&#24322;&#36136;&#23376;&#38598;&#12290;&#25105;&#20204;&#37319;&#29992;&#38750;&#32447;&#24615;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26469;&#26816;&#27979;&#29305;&#24449;&#20132;&#20114;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#32508;&#21512;&#29305;&#24449;&#39118;&#38505;&#35780;&#20998;(CRS)&#26469;&#35299;&#37322;&#22522;&#22240;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
Through genome-wide association studies (GWAS), disease susceptible genetic variables can be identified by comparing the genetic data of individuals with and without a specific disease. However, the discovery of these associations poses a significant challenge due to genetic heterogeneity and feature interactions. Genetic variables intertwined with these effects often exhibit lower effect-size, and thus can be difficult to be detected using machine learning feature selection methods. To address these challenges, this paper introduces a novel feature selection mechanism for GWAS, named Feature Co-selection Network (FCSNet). FCS-Net is designed to extract heterogeneous subsets of genetic variables from a network constructed from multiple independent feature selection runs based on a genetic algorithm (GA), an evolutionary learning algorithm. We employ a non-linear machine learning algorithm to detect feature interaction. We introduce the Community Risk Score (CRS), a synthetic feature de
&lt;/p&gt;</description></item><item><title>&#23398;&#20064;&#20108;&#20803;&#20551;&#35774;&#31867;&#20855;&#26377;&#26679;&#26412;&#21387;&#32553;&#26041;&#26696;&#65292;&#32780;&#22810;&#31867;&#21035;&#20551;&#35774;&#31867;&#21017;&#19981;&#20855;&#22791;&#36825;&#20010;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2308.06424</link><description>&lt;p&gt;
&#23398;&#20064;&#33021;&#21147;&#19982;&#26679;&#26412;&#21387;&#32553;&#24182;&#19981;&#30456;&#21516;&#30340;&#22810;&#31867;&#21035;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Multiclass Learnability Does Not Imply Sample Compression. (arXiv:2308.06424v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06424
&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#20108;&#20803;&#20551;&#35774;&#31867;&#20855;&#26377;&#26679;&#26412;&#21387;&#32553;&#26041;&#26696;&#65292;&#32780;&#22810;&#31867;&#21035;&#20551;&#35774;&#31867;&#21017;&#19981;&#20855;&#22791;&#36825;&#20010;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;&#19968;&#20010;&#20551;&#35774;&#31867;&#33021;&#22815;&#36890;&#36807;&#21482;&#20445;&#30041;&#19968;&#20010;&#23567;&#30340;&#23376;&#26679;&#26412;&#25512;&#26029;&#20986;&#25972;&#20010;&#26679;&#26412;&#30340;&#26631;&#31614;&#65292;&#37027;&#20040;&#23427;&#23601;&#20855;&#26377;&#26679;&#26412;&#21387;&#32553;&#26041;&#26696;&#12290;&#23398;&#20064;&#20108;&#20803;&#20551;&#35774;&#31867;&#65288;&#24517;&#39035;&#20855;&#26377;&#26377;&#38480;&#30340;VC&#32500;&#24230;&#65289;&#37117;&#21487;&#20197;&#36890;&#36807;VC&#32500;&#24230;&#30340;&#19968;&#20010;&#26377;&#38480;&#20989;&#25968;&#22823;&#23567;&#30340;&#26679;&#26412;&#21387;&#32553;&#26041;&#26696;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22810;&#31867;&#21035;&#20551;&#35774;&#31867;&#26469;&#35828;&#65292;DS&#32500;&#24230;&#26159;&#30456;&#23545;&#24212;&#30340;&#65292;&#25105;&#20204;&#21457;&#29616;&#23398;&#20064;&#22810;&#31867;&#21035;&#20551;&#35774;&#31867;&#65288;&#24517;&#39035;&#20855;&#26377;&#26377;&#38480;&#30340;DS&#32500;&#24230;&#65289;&#24182;&#19981;&#33021;&#36890;&#36807;&#19968;&#20010;DS&#32500;&#24230;&#30340;&#26377;&#38480;&#20989;&#25968;&#22823;&#23567;&#30340;&#26679;&#26412;&#21387;&#32553;&#26041;&#26696;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
A hypothesis class admits a sample compression scheme, if for every sample labeled by a hypothesis from the class, it is possible to retain only a small subsample, using which the labels on the entire sample can be inferred. The size of the compression scheme is an upper bound on the size of the subsample produced. Every learnable binary hypothesis class (which must necessarily have finite VC dimension) admits a sample compression scheme of size only a finite function of its VC dimension, independent of the sample size. For multiclass hypothesis classes, the analog of VC dimension is the DS dimension. We show that the analogous statement pertaining to sample compression is not true for multiclass hypothesis classes: every learnable multiclass hypothesis class, which must necessarily have finite DS dimension, does not admit a sample compression scheme of size only a finite function of its DS dimension.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#36873;&#25321;&#26368;&#20339;&#30340;&#20301;&#23485;&#21644;&#23618;&#23485;&#26469;&#25552;&#39640;&#32593;&#32476;&#25928;&#29575;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#21098;&#26525;&#21644;&#32858;&#31867;&#25216;&#26415;&#65292;&#20248;&#21270;&#20102;&#25628;&#32034;&#36807;&#31243;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20005;&#26684;&#27979;&#35797;&#65292;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.06422</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#32858;&#31867;&#30340;&#26641;&#29366;Parzen&#20272;&#35745;&#30340;&#25935;&#24863;&#24615;&#24863;&#30693;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#21644;&#23485;&#24230;&#20248;&#21270;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Sensitivity-Aware Mixed-Precision Quantization and Width Optimization of Deep Neural Networks Through Cluster-Based Tree-Structured Parzen Estimation. (arXiv:2308.06422v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06422
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#36873;&#25321;&#26368;&#20339;&#30340;&#20301;&#23485;&#21644;&#23618;&#23485;&#26469;&#25552;&#39640;&#32593;&#32476;&#25928;&#29575;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#21098;&#26525;&#21644;&#32858;&#31867;&#25216;&#26415;&#65292;&#20248;&#21270;&#20102;&#25628;&#32034;&#36807;&#31243;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20005;&#26684;&#27979;&#35797;&#65292;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#21644;&#35745;&#31639;&#38656;&#27714;&#30340;&#25552;&#39640;&#65292;&#23545;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#30340;&#26377;&#25928;&#20248;&#21270;&#26041;&#27861;&#30340;&#38656;&#27714;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#25628;&#32034;&#26426;&#21046;&#65292;&#29992;&#20110;&#33258;&#21160;&#36873;&#25321;&#21333;&#20010;&#31070;&#32463;&#32593;&#32476;&#23618;&#30340;&#26368;&#20339;&#20301;&#23485;&#21644;&#23618;&#23485;&#12290;&#36825;&#23548;&#33268;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25928;&#29575;&#30340;&#26126;&#26174;&#25552;&#39640;&#12290;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;Hessian&#30340;&#21098;&#26525;&#31574;&#30053;&#65292;&#26377;&#36873;&#25321;&#22320;&#20943;&#23569;&#25628;&#32034;&#22495;&#65292;&#30830;&#20445;&#31227;&#38500;&#38750;&#20851;&#38190;&#21442;&#25968;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#32858;&#31867;&#30340;&#26641;&#29366;Parzen&#20272;&#35745;&#22120;&#24320;&#21457;&#26377;&#21033;&#21644;&#19981;&#21033;&#32467;&#26524;&#30340;&#26367;&#20195;&#27169;&#22411;&#12290;&#36825;&#31181;&#31574;&#30053;&#20801;&#35768;&#23545;&#26550;&#26500;&#21487;&#33021;&#24615;&#36827;&#34892;&#31616;&#21270;&#30340;&#25506;&#32034;&#65292;&#24182;&#36805;&#36895;&#30830;&#23450;&#34920;&#29616;&#26368;&#22909;&#30340;&#35774;&#35745;&#12290;&#36890;&#36807;&#23545;&#30693;&#21517;&#25968;&#25454;&#38598;&#36827;&#34892;&#20005;&#26684;&#27979;&#35797;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#35777;&#26126;&#20102;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#30340;&#26126;&#26174;&#20248;&#21183;&#12290;&#19982;&#39046;&#20808;&#30340;&#21387;&#32553;&#31574;&#30053;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the complexity and computational demands of deep learning models rise, the need for effective optimization methods for neural network designs becomes paramount. This work introduces an innovative search mechanism for automatically selecting the best bit-width and layer-width for individual neural network layers. This leads to a marked enhancement in deep neural network efficiency. The search domain is strategically reduced by leveraging Hessian-based pruning, ensuring the removal of non-crucial parameters. Subsequently, we detail the development of surrogate models for favorable and unfavorable outcomes by employing a cluster-based tree-structured Parzen estimator. This strategy allows for a streamlined exploration of architectural possibilities and swift pinpointing of top-performing designs. Through rigorous testing on well-known datasets, our method proves its distinct advantage over existing methods. Compared to leading compression strategies, our approach records an impressive 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#32508;&#36848;&#20102;&#34892;&#20154;-&#36710;&#36742;&#28151;&#21512;&#29615;&#22659;&#19979;&#30340;&#34892;&#20154;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#36710;&#36742;&#21644;&#34892;&#20154;&#30340;&#30456;&#20114;&#20316;&#29992;&#23545;&#34892;&#20154;&#26410;&#26469;&#34892;&#20026;&#30340;&#24433;&#21709;&#65292;&#24182;&#22238;&#39038;&#20102;&#20808;&#21069;&#25552;&#20986;&#30340;&#39044;&#27979;&#27169;&#22411;&#20013;&#22914;&#20309;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#21644;&#34892;&#20026;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2308.06419</link><description>&lt;p&gt;
&#34892;&#20154;-&#36710;&#36742;&#28151;&#21512;&#29615;&#22659;&#19979;&#30340;&#34892;&#20154;&#36712;&#36857;&#39044;&#27979;:&#19968;&#39033;&#31995;&#32479;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Pedestrian Trajectory Prediction in Pedestrian-Vehicle Mixed Environments: A Systematic Review. (arXiv:2308.06419v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06419
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#32508;&#36848;&#20102;&#34892;&#20154;-&#36710;&#36742;&#28151;&#21512;&#29615;&#22659;&#19979;&#30340;&#34892;&#20154;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#36710;&#36742;&#21644;&#34892;&#20154;&#30340;&#30456;&#20114;&#20316;&#29992;&#23545;&#34892;&#20154;&#26410;&#26469;&#34892;&#20026;&#30340;&#24433;&#21709;&#65292;&#24182;&#22238;&#39038;&#20102;&#20808;&#21069;&#25552;&#20986;&#30340;&#39044;&#27979;&#27169;&#22411;&#20013;&#22914;&#20309;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#21644;&#34892;&#20026;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19982;&#34892;&#20154;&#20849;&#20139;&#31354;&#38388;&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AV&#65289;&#36335;&#24452;&#35268;&#21010;&#20013;&#65292;&#38656;&#35201;&#25512;&#29702;&#20986;&#34892;&#20154;&#30340;&#26410;&#26469;&#36712;&#36857;&#12290;&#20026;&#20102;&#24212;&#29992;&#20110;AV&#30340;&#23454;&#38469;&#34892;&#20154;&#36712;&#36857;&#39044;&#27979;&#31639;&#27861;&#65292;&#38656;&#35201;&#32771;&#34385;&#36710;&#36742;&#19982;&#34892;&#20154;&#30340;&#30456;&#20114;&#20316;&#29992;&#23545;&#34892;&#20154;&#26410;&#26469;&#36816;&#21160;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#26412;&#25991;&#31995;&#32479;&#32508;&#36848;&#20102;&#25991;&#29486;&#20013;&#38024;&#23545;&#22312;&#23384;&#22312;&#36710;&#36742;&#30340;&#38750;&#32467;&#26500;&#21270;&#29615;&#22659;&#20013;&#23545;&#34892;&#20154;&#36712;&#36857;&#39044;&#27979;&#36827;&#34892;&#24314;&#27169;&#30340;&#19981;&#21516;&#26041;&#27861;&#12290;&#26412;&#25991;&#36824;&#25506;&#35752;&#20102;&#19982;&#34892;&#20154;-&#34892;&#20154;&#20132;&#20114;&#30456;&#27604;&#65292;&#34892;&#20154;-&#36710;&#36742;&#20132;&#20114;&#30340;&#29305;&#23450;&#32771;&#34385;&#65292;&#24182;&#22238;&#39038;&#20102;&#20808;&#21069;&#25552;&#20986;&#30340;&#39044;&#27979;&#27169;&#22411;&#20013;&#22914;&#20309;&#32771;&#34385;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#21644;&#34892;&#20026;&#24046;&#24322;&#31561;&#19981;&#21516;&#21464;&#37327;&#12290;&#36981;&#24490;PRISMA&#25351;&#21335;&#12290;&#26412;&#25991;&#36824;&#32771;&#23519;&#20102;&#22914;&#20309;&#22312;&#20808;&#21069;&#25552;&#20986;&#30340;&#39044;&#27979;&#27169;&#22411;&#20013;&#32771;&#34385;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#21644;&#34892;&#20026;&#24046;&#24322;&#31561;&#19981;&#21516;&#21464;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Planning an autonomous vehicle's (AV) path in a space shared with pedestrians requires reasoning about pedestrians' future trajectories. A practical pedestrian trajectory prediction algorithm for the use of AVs needs to consider the effect of the vehicle's interactions with the pedestrians on pedestrians' future motion behaviours. In this regard, this paper systematically reviews different methods proposed in the literature for modelling pedestrian trajectory prediction in presence of vehicles that can be applied for unstructured environments. This paper also investigates specific considerations for pedestrian-vehicle interaction (compared with pedestrian-pedestrian interaction) and reviews how different variables such as prediction uncertainties and behavioural differences are accounted for in the previously proposed prediction models. PRISMA guidelines were followed. Articles that did not consider vehicle and pedestrian interactions or actual trajectories, and articles that only focu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#28151;&#21512;&#25928;&#24212;&#27169;&#22411;&#21644;&#23618;&#27425;&#32858;&#31867;&#24212;&#29992;&#20110;&#36125;&#21494;&#26031;&#32593;&#32476;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#20892;&#23398;&#30740;&#31350;&#20013;&#24191;&#27867;&#24212;&#29992;&#12290;&#36890;&#36807;&#25972;&#21512;&#38543;&#26426;&#25928;&#24212;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#32467;&#26500;&#23398;&#20064;&#33021;&#21147;&#65292;&#23454;&#29616;&#22240;&#26524;&#20851;&#31995;&#32593;&#32476;&#30340;&#21457;&#29616;&#12290;</title><link>http://arxiv.org/abs/2308.06399</link><description>&lt;p&gt;
&#36890;&#36807;&#28151;&#21512;&#25928;&#24212;&#27169;&#22411;&#21644;&#23618;&#27425;&#32858;&#31867;&#23398;&#20064;&#20855;&#26377;&#24322;&#26500;&#20892;&#19994;&#25968;&#25454;&#38598;&#30340;&#36125;&#21494;&#26031;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Learning Bayesian Networks with Heterogeneous Agronomic Data Sets via Mixed-Effect Models and Hierarchical Clustering. (arXiv:2308.06399v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06399
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#28151;&#21512;&#25928;&#24212;&#27169;&#22411;&#21644;&#23618;&#27425;&#32858;&#31867;&#24212;&#29992;&#20110;&#36125;&#21494;&#26031;&#32593;&#32476;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#20892;&#23398;&#30740;&#31350;&#20013;&#24191;&#27867;&#24212;&#29992;&#12290;&#36890;&#36807;&#25972;&#21512;&#38543;&#26426;&#25928;&#24212;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#32467;&#26500;&#23398;&#20064;&#33021;&#21147;&#65292;&#23454;&#29616;&#22240;&#26524;&#20851;&#31995;&#32593;&#32476;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28041;&#21450;&#22810;&#26679;&#20294;&#30456;&#20851;&#25968;&#25454;&#38598;&#30340;&#30740;&#31350;&#20013;&#65292;&#20854;&#20013;&#21327;&#21464;&#37327;&#19982;&#32467;&#26524;&#20043;&#38388;&#30340;&#20851;&#32852;&#21487;&#33021;&#20250;&#26377;&#25152;&#19981;&#21516;&#65292;&#22312;&#21253;&#25324;&#20892;&#23398;&#30740;&#31350;&#22312;&#20869;&#30340;&#21508;&#20010;&#39046;&#22495;&#37117;&#24456;&#26222;&#36941;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#24120;&#24120;&#20351;&#29992;&#23618;&#27425;&#27169;&#22411;&#65292;&#20063;&#34987;&#31216;&#20026;&#22810;&#23618;&#27169;&#22411;&#65292;&#26469;&#34701;&#21512;&#26469;&#33258;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#20449;&#24687;&#65292;&#24182;&#36866;&#24212;&#23427;&#20204;&#30340;&#19981;&#21516;&#29305;&#28857;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#32467;&#26500;&#36229;&#20986;&#20102;&#31616;&#21333;&#30340;&#24322;&#36136;&#24615;&#65292;&#22240;&#20026;&#21464;&#37327;&#36890;&#24120;&#24418;&#25104;&#22797;&#26434;&#30340;&#22240;&#26524;&#20851;&#31995;&#32593;&#32476;&#12290;&#36125;&#21494;&#26031;&#32593;&#32476;&#65288;BNs&#65289;&#20351;&#29992;&#26377;&#21521;&#26080;&#29615;&#22270;&#26469;&#27169;&#25311;&#36825;&#31181;&#20851;&#31995;&#30340;&#24378;&#22823;&#26694;&#26550;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#38543;&#26426;&#25928;&#24212;&#25972;&#21512;&#21040;BN&#23398;&#20064;&#20013;&#30340;&#26032;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#22522;&#20110;&#32447;&#24615;&#28151;&#21512;&#25928;&#24212;&#27169;&#22411;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#22788;&#29702;&#23618;&#27425;&#25968;&#25454;&#12290;&#26469;&#33258;&#30495;&#23454;&#20892;&#23398;&#35797;&#39564;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#37319;&#29992;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22686;&#24378;&#32467;&#26500;&#23398;&#20064;&#65292;&#20174;&#32780;&#23454;&#29616;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Research involving diverse but related data sets, where associations between covariates and outcomes may vary, is prevalent in various fields including agronomic studies. In these scenarios, hierarchical models, also known as multilevel models, are frequently employed to assimilate information from different data sets while accommodating their distinct characteristics. However, their structure extend beyond simple heterogeneity, as variables often form complex networks of causal relationships.  Bayesian networks (BNs) provide a powerful framework for modelling such relationships using directed acyclic graphs to illustrate the connections between variables. This study introduces a novel approach that integrates random effects into BN learning. Rooted in linear mixed-effects models, this approach is particularly well-suited for handling hierarchical data. Results from a real-world agronomic trial suggest that employing this approach enhances structural learning, leading to the discovery 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#24187;&#35273;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#20197;&#35299;&#20915;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#24187;&#35273;&#25991;&#26412;&#38382;&#39064;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#35814;&#32454;&#22270;&#20687;&#25551;&#36848;&#30340;&#20840;&#38754;&#22810;&#27169;&#24577;&#24187;&#35273;&#26816;&#27979;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2308.06394</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#26816;&#27979;&#21644;&#39044;&#38450;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Detecting and Preventing Hallucinations in Large Vision Language Models. (arXiv:2308.06394v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06394
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#24187;&#35273;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#20197;&#35299;&#20915;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#24187;&#35273;&#25991;&#26412;&#38382;&#39064;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#35814;&#32454;&#22270;&#20687;&#25551;&#36848;&#30340;&#20840;&#38754;&#22810;&#27169;&#24577;&#24187;&#35273;&#26816;&#27979;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#36807;&#35843;&#25972;&#30340;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#22312;&#27867;&#21270;&#36328;&#22810;&#31181;&#22810;&#27169;&#24577;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#22312;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#20026;&#36825;&#20123;&#27169;&#22411;&#29983;&#25104;&#19982;&#35270;&#35273;&#30456;&#20851;&#30340;&#35814;&#32454;&#22238;&#31572;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21363;&#20351;&#26159;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;LVLM&#65288;InstructBLIP&#65289;&#20173;&#28982;&#23384;&#22312;&#30528;&#24778;&#20154;&#30340;30%&#30340;&#24187;&#35273;&#25991;&#26412;&#65292;&#21253;&#25324;&#19981;&#23384;&#22312;&#30340;&#23545;&#35937;&#12289;&#19981;&#24544;&#23454;&#30340;&#25551;&#36848;&#21644;&#19981;&#20934;&#30830;&#30340;&#20851;&#31995;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;M-HalDetect&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#24187;&#35273;&#26816;&#27979;&#21644;&#39044;&#38450;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#24187;&#35273;&#26816;&#27979;&#25968;&#25454;&#38598;&#12290;M-HalDetect&#21253;&#21547;&#20102;16k&#20010;&#32454;&#31890;&#24230;&#30340;VQA&#31034;&#20363;&#26631;&#31614;&#65292;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#35814;&#32454;&#22270;&#20687;&#25551;&#36848;&#30340;&#20840;&#38754;&#22810;&#27169;&#24577;&#24187;&#35273;&#26816;&#27979;&#25968;&#25454;&#38598;&#12290;&#19982;&#20043;&#21069;&#21482;&#32771;&#34385;&#23545;&#35937;&#24187;&#35273;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#36824;&#27880;&#37322;&#20102;&#23454;&#20307;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction tuned Large Vision Language Models (LVLMs) have made significant advancements in generalizing across a diverse set of multimodal tasks, especially for Visual Question Answering (VQA). However, generating detailed responses that are visually grounded is still a challenging task for these models. We find that even the current state-of-the-art LVLMs (InstructBLIP) still contain a staggering 30 percent of hallucinatory text in the form of non-existent objects, unfaithful descriptions, and inaccurate relationships. To address this, we introduce M-HalDetect, a {M}ultimodal {Hal}lucination {Detect}ion Dataset that can be used to train and benchmark models for hallucination detection and prevention. M-HalDetect consists of 16k fine-grained labels on VQA examples, making it the first comprehensive multi-modal hallucination detection dataset for detailed image descriptions. Unlike previous work that only consider object hallucination, we additionally annotate both entity descriptions
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#38899;&#32032;&#24187;&#35273;&#22120;&#8221;&#30340;&#19968;&#27425;&#24615;&#35821;&#38899;&#36716;&#25442;&#27169;&#22411;&#65292;&#36890;&#36807;&#38598;&#21512;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#21482;&#38656;&#30701;&#26102;&#38388;&#30446;&#26631;&#35828;&#35805;&#20154;&#35821;&#38899;&#21363;&#21487;&#29983;&#25104;&#22810;&#26679;&#21644;&#39640;&#20445;&#30495;&#24230;&#30340;&#30446;&#26631;&#35828;&#35805;&#20154;&#38899;&#32032;&#65292;&#24182;&#29992;&#20110;&#22522;&#20110;&#37051;&#23621;&#30340;&#35821;&#38899;&#36716;&#25442;&#12290;</title><link>http://arxiv.org/abs/2308.06382</link><description>&lt;p&gt;
&#36890;&#36807;&#38598;&#21512;&#25193;&#23637;&#23454;&#29616;&#19968;&#27425;&#24615;&#35821;&#38899;&#36716;&#25442;&#30340;&#38899;&#32032;&#24187;&#35273;&#22120;
&lt;/p&gt;
&lt;p&gt;
Phoneme Hallucinator: One-shot Voice Conversion via Set Expansion. (arXiv:2308.06382v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#38899;&#32032;&#24187;&#35273;&#22120;&#8221;&#30340;&#19968;&#27425;&#24615;&#35821;&#38899;&#36716;&#25442;&#27169;&#22411;&#65292;&#36890;&#36807;&#38598;&#21512;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#21482;&#38656;&#30701;&#26102;&#38388;&#30446;&#26631;&#35828;&#35805;&#20154;&#35821;&#38899;&#21363;&#21487;&#29983;&#25104;&#22810;&#26679;&#21644;&#39640;&#20445;&#30495;&#24230;&#30340;&#30446;&#26631;&#35828;&#35805;&#20154;&#38899;&#32032;&#65292;&#24182;&#29992;&#20110;&#22522;&#20110;&#37051;&#23621;&#30340;&#35821;&#38899;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#36716;&#25442;&#26088;&#22312;&#25913;&#21464;&#19968;&#20010;&#20154;&#30340;&#22768;&#38899;&#65292;&#20351;&#20854;&#21548;&#36215;&#26469;&#19982;&#21478;&#19968;&#20010;&#20154;&#30340;&#22768;&#38899;&#30456;&#20284;&#65292;&#21516;&#26102;&#20445;&#30041;&#35821;&#35328;&#20869;&#23481;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#22312;&#20869;&#23481;&#21487;&#29702;&#35299;&#24615;&#21644;&#35828;&#35805;&#20154;&#30456;&#20284;&#24615;&#20043;&#38388;&#23384;&#22312;&#22256;&#22659;&#65307;&#21363;&#20855;&#26377;&#26356;&#39640;&#21487;&#29702;&#35299;&#24615;&#30340;&#26041;&#27861;&#36890;&#24120;&#20855;&#26377;&#36739;&#20302;&#30340;&#35828;&#35805;&#20154;&#30456;&#20284;&#24615;&#65292;&#32780;&#20855;&#26377;&#26356;&#39640;&#35828;&#35805;&#20154;&#30456;&#20284;&#24615;&#30340;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30446;&#26631;&#35828;&#35805;&#20154;&#30340;&#35821;&#38899;&#25968;&#25454;&#26469;&#23454;&#29616;&#39640;&#21487;&#29702;&#35299;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#8220;&#38899;&#32032;&#24187;&#35273;&#22120;&#8221;&#65292;&#23427;&#20860;&#20855;&#20004;&#32773;&#30340;&#20248;&#21183;&#12290;&#38899;&#32032;&#24187;&#35273;&#22120;&#26159;&#19968;&#31181;&#19968;&#27425;&#24615;&#35821;&#38899;&#36716;&#25442;&#27169;&#22411;&#65307;&#23427;&#37319;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#20165;&#22522;&#20110;&#36739;&#30701;&#30340;&#30446;&#26631;&#35828;&#35805;&#20154;&#35821;&#38899;&#65288;&#20363;&#22914;3&#31186;&#65289;&#29983;&#25104;&#22810;&#26679;&#21270;&#21644;&#39640;&#20445;&#30495;&#24230;&#30340;&#30446;&#26631;&#35828;&#35805;&#20154;&#38899;&#32032;&#12290;&#28982;&#21518;&#21033;&#29992;&#29983;&#25104;&#30340;&#38899;&#32032;&#36827;&#34892;&#22522;&#20110;&#37051;&#23621;&#30340;&#35821;&#38899;&#36716;&#25442;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#26159;&#19968;&#31181;&#26080;&#38656;&#25991;&#26412;&#27880;&#37322;&#30340;&#20219;&#24847;&#21040;&#20219;&#24847;&#30340;&#35821;&#38899;&#36716;&#25442;&#27169;&#22411;&#65292;&#25903;&#25345;&#36716;&#25442;&#21040;&#20219;&#20309;&#26410;&#30693;&#35828;&#35805;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;
Voice conversion (VC) aims at altering a person's voice to make it sound similar to the voice of another person while preserving linguistic content. Existing methods suffer from a dilemma between content intelligibility and speaker similarity; i.e., methods with higher intelligibility usually have a lower speaker similarity, while methods with higher speaker similarity usually require plenty of target speaker voice data to achieve high intelligibility. In this work, we propose a novel method \textit{Phoneme Hallucinator} that achieves the best of both worlds. Phoneme Hallucinator is a one-shot VC model; it adopts a novel model to hallucinate diversified and high-fidelity target speaker phonemes based just on a short target speaker voice (e.g. 3 seconds). The hallucinated phonemes are then exploited to perform neighbor-based voice conversion. Our model is a text-free, any-to-any VC model that requires no text annotations and supports conversion to any unseen speaker. Objective and subje
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#27169;&#31946;&#25512;&#29702;&#31995;&#32479;&#65288;DCNFIS&#65289;&#65292;&#23427;&#36890;&#36807;&#23558;&#27169;&#31946;&#36923;&#36753;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#25552;&#39640;&#36879;&#26126;&#24230;&#32780;&#19981;&#25439;&#22833;&#20934;&#30830;&#24615;&#30340;&#30446;&#26631;&#12290;DCNFIS&#22312;&#20934;&#30830;&#24615;&#19978;&#19982;&#29616;&#26377;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30456;&#24403;&#65292;&#24182;&#19988;&#32988;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#27169;&#31946;&#31995;&#32479;&#12290;&#36890;&#36807;&#27169;&#31946;&#35268;&#21017;&#25552;&#21462;&#30340;&#35299;&#37322;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.06378</link><description>&lt;p&gt;
DCNFIS&#65306;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#27169;&#31946;&#25512;&#29702;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
DCNFIS: Deep Convolutional Neuro-Fuzzy Inference System. (arXiv:2308.06378v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06378
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#27169;&#31946;&#25512;&#29702;&#31995;&#32479;&#65288;DCNFIS&#65289;&#65292;&#23427;&#36890;&#36807;&#23558;&#27169;&#31946;&#36923;&#36753;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#25552;&#39640;&#36879;&#26126;&#24230;&#32780;&#19981;&#25439;&#22833;&#20934;&#30830;&#24615;&#30340;&#30446;&#26631;&#12290;DCNFIS&#22312;&#20934;&#30830;&#24615;&#19978;&#19982;&#29616;&#26377;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30456;&#24403;&#65292;&#24182;&#19988;&#32988;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#27169;&#31946;&#31995;&#32479;&#12290;&#36890;&#36807;&#27169;&#31946;&#35268;&#21017;&#25552;&#21462;&#30340;&#35299;&#37322;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#20013;&#65292;&#36879;&#26126;&#24230;&#19982;&#20934;&#30830;&#24615;&#20043;&#38388;&#23384;&#22312;&#19968;&#20010;&#33879;&#21517;&#30340;&#26435;&#34913;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#32593;&#32476;&#35774;&#35745;&#65292;&#36890;&#36807;&#23558;&#27169;&#31946;&#36923;&#36753;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#25552;&#39640;&#36879;&#26126;&#24230;&#20294;&#19981;&#25439;&#22833;&#20934;&#30830;&#24615;&#30340;&#30446;&#26631;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#27169;&#31946;&#25512;&#29702;&#31995;&#32479;&#65288;DCNFIS&#65289;&#65292;&#24182;&#22312;&#22235;&#20010;&#33879;&#21517;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#23427;&#19982;&#19977;&#20010;&#29616;&#26377;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#30456;&#21516;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#21457;&#29616;&#65292;DCNFIS&#22312;&#24615;&#33021;&#19978;&#32988;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#27169;&#31946;&#31995;&#32479;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#27169;&#31946;&#36923;&#36753;&#30340;&#36879;&#26126;&#24230;&#65292;&#20174;DCNFIS&#20013;&#32534;&#30721;&#30340;&#27169;&#31946;&#35268;&#21017;&#20013;&#25552;&#21462;&#35299;&#37322;&#65292;&#20197;&#28176;&#21464;&#26144;&#23556;&#30340;&#24418;&#24335;&#23637;&#31034;&#12290;&#25105;&#20204;&#36824;&#21033;&#29992;Fashion-MNIST&#25968;&#25454;&#38598;&#23545;&#36825;&#20123;&#35299;&#37322;&#30340;&#29305;&#24615;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key challenge in eXplainable Artificial Intelligence is the well-known tradeoff between the transparency of an algorithm (i.e., how easily a human can directly understand the algorithm, as opposed to receiving a post-hoc explanation), and its accuracy. We report on the design of a new deep network that achieves improved transparency without sacrificing accuracy. We design a deep convolutional neuro-fuzzy inference system (DCNFIS) by hybridizing fuzzy logic and deep learning models and show that DCNFIS performs as accurately as three existing convolutional neural networks on four well-known datasets. We furthermore that DCNFIS outperforms state-of-the-art deep fuzzy systems. We then exploit the transparency of fuzzy logic by deriving explanations, in the form of saliency maps, from the fuzzy rules encoded in DCNFIS. We investigate the properties of these explanations in greater depth using the Fashion-MNIST dataset.
&lt;/p&gt;</description></item><item><title>UAMM&#26159;&#19968;&#31181;&#26032;&#30340;&#33258;&#21160;&#24066;&#22330;&#20570;&#24066;&#21830;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#22806;&#37096;&#24066;&#22330;&#20215;&#26684;&#21644;&#27969;&#21160;&#24615;&#27744;&#30340;&#26242;&#26102;&#25439;&#22833;&#26469;&#23450;&#20215;&#65292;&#24182;&#19988;&#26377;&#25928;&#28040;&#38500;&#20102;&#22871;&#21033;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2308.06375</link><description>&lt;p&gt;
UAMM: UBET&#33258;&#21160;&#24066;&#22330;&#20570;&#24066;&#21830;
&lt;/p&gt;
&lt;p&gt;
UAMM: UBET Automated Market Maker. (arXiv:2308.06375v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06375
&lt;/p&gt;
&lt;p&gt;
UAMM&#26159;&#19968;&#31181;&#26032;&#30340;&#33258;&#21160;&#24066;&#22330;&#20570;&#24066;&#21830;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#22806;&#37096;&#24066;&#22330;&#20215;&#26684;&#21644;&#27969;&#21160;&#24615;&#27744;&#30340;&#26242;&#26102;&#25439;&#22833;&#26469;&#23450;&#20215;&#65292;&#24182;&#19988;&#26377;&#25928;&#28040;&#38500;&#20102;&#22871;&#21033;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#24066;&#22330;&#20570;&#24066;&#21830;&#65288;AMM&#65289;&#26159;&#21435;&#20013;&#24515;&#21270;&#20132;&#26131;&#25152;&#65288;DEX&#65289;&#20351;&#29992;&#30340;&#23450;&#20215;&#26426;&#21046;&#12290;&#20256;&#32479;&#30340;AMM&#26041;&#27861;&#20165;&#22522;&#20110;&#20854;&#33258;&#36523;&#30340;&#27969;&#21160;&#24615;&#27744;&#36827;&#34892;&#23450;&#20215;&#65292;&#32780;&#19981;&#32771;&#34385;&#22806;&#37096;&#24066;&#22330;&#25110;&#27969;&#21160;&#24615;&#25552;&#20379;&#32773;&#30340;&#39118;&#38505;&#31649;&#29702;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;UBET AMM&#65288;UAMM&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#22806;&#37096;&#24066;&#22330;&#20215;&#26684;&#21644;&#27969;&#21160;&#24615;&#27744;&#30340;&#26242;&#26102;&#25439;&#22833;&#26469;&#35745;&#31639;&#20215;&#26684;&#12290;&#23613;&#31649;&#20381;&#36182;&#20110;&#22806;&#37096;&#24066;&#22330;&#20215;&#26684;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35745;&#31639;&#28369;&#28857;&#26102;&#20173;&#28982;&#20445;&#25345;&#20102;&#24658;&#23450;&#20135;&#21697;&#26354;&#32447;&#30340;&#26399;&#26395;&#23646;&#24615;&#12290;UAMM&#30340;&#20851;&#38190;&#35201;&#32032;&#26159;&#26681;&#25454;&#26399;&#26395;&#30340;&#30446;&#26631;&#20313;&#39069;&#30830;&#23450;&#21512;&#36866;&#30340;&#28369;&#28857;&#37329;&#39069;&#65292;&#20197;&#40723;&#21169;&#27969;&#21160;&#24615;&#27744;&#26368;&#23567;&#21270;&#26242;&#26102;&#25439;&#22833;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#22806;&#37096;&#24066;&#22330;&#20215;&#26684;&#26377;&#25928;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#28040;&#38500;&#20102;&#22871;&#21033;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated market makers (AMMs) are pricing mechanisms utilized by decentralized exchanges (DEX). Traditional AMM approaches are constrained by pricing solely based on their own liquidity pool, without consideration of external markets or risk management for liquidity providers. In this paper, we propose a new approach known as UBET AMM (UAMM), which calculates prices by considering external market prices and the impermanent loss of the liquidity pool. Despite relying on external market prices, our method maintains the desired properties of a constant product curve when computing slippages. The key element of UAMM is determining the appropriate slippage amount based on the desired target balance, which encourages the liquidity pool to minimize impermanent loss. We demonstrate that our approach eliminates arbitrage opportunities when external market prices are efficient.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#20027;&#39064;&#30340;&#36125;&#21494;&#26031;&#24778;&#21916;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#30340;&#24847;&#22806;&#24615;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#36807;&#28388;&#27873;&#38382;&#39064;&#65292;&#36890;&#36807;&#35782;&#21035;&#30456;&#20284;&#29992;&#25143;&#21644;&#27979;&#37327;&#29992;&#25143;&#23545;&#29289;&#21697;&#30340;&#24847;&#22806;&#24615;&#26469;&#25512;&#33616;&#20855;&#26377;&#39640;&#28508;&#21147;&#30340;&#24847;&#22806;&#24615;&#29289;&#21697;&#12290;</title><link>http://arxiv.org/abs/2308.06368</link><description>&lt;p&gt;
&#22522;&#20110;&#20027;&#39064;&#30340;&#36125;&#21494;&#26031;&#24778;&#21916;&#21644;&#24847;&#22806;&#24615;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Topic-Level Bayesian Surprise and Serendipity for Recommender Systems. (arXiv:2308.06368v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#20027;&#39064;&#30340;&#36125;&#21494;&#26031;&#24778;&#21916;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#30340;&#24847;&#22806;&#24615;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#36807;&#28388;&#27873;&#38382;&#39064;&#65292;&#36890;&#36807;&#35782;&#21035;&#30456;&#20284;&#29992;&#25143;&#21644;&#27979;&#37327;&#29992;&#25143;&#23545;&#29289;&#21697;&#30340;&#24847;&#22806;&#24615;&#26469;&#25512;&#33616;&#20855;&#26377;&#39640;&#28508;&#21147;&#30340;&#24847;&#22806;&#24615;&#29289;&#21697;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#20248;&#21270;&#20854;&#25512;&#33616;&#20165;&#36866;&#21512;&#29992;&#25143;&#23545;&#24050;&#28040;&#36153;&#29289;&#21697;&#30340;&#35780;&#32423;&#21382;&#21490;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#36807;&#28388;&#27873;&#65292;&#29992;&#25143;&#26080;&#27861;&#20174;&#26032;&#39062;&#12289;&#26410;&#35265;&#36807;&#30340;&#31867;&#21035;&#20013;&#20307;&#39564;&#29289;&#21697;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20869;&#23481;&#30340;&#24847;&#22806;&#24615;&#24418;&#24335;&#65292;&#20197;&#36125;&#21494;&#26031;&#24778;&#21916;&#20026;&#22522;&#30784;&#65292;&#29992;&#20110;&#27979;&#37327;&#29992;&#25143;&#28040;&#36153;&#24182;&#35780;&#32423;&#21518;&#29289;&#21697;&#30340;&#24847;&#22806;&#24615;&#12290;&#32467;&#21512;&#35782;&#21035;&#30456;&#20284;&#29992;&#25143;&#30340;&#21327;&#21516;&#36807;&#28388;&#32452;&#20214;&#65292;&#21487;&#20197;&#25512;&#33616;&#20855;&#26377;&#39640;&#28508;&#21147;&#24847;&#22806;&#24615;&#30340;&#29289;&#21697;&#12290;&#20026;&#20102;&#20415;&#20110;&#35780;&#20272;&#20027;&#39064;&#32423;&#21035;&#30340;&#24778;&#21916;&#21644;&#24847;&#22806;&#24615;&#27169;&#22411;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#20174;Goodreads&#20013;&#25552;&#21462;&#30340;&#22270;&#20070;&#38405;&#35835;&#21382;&#21490;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#36229;&#36807;26&#21315;&#20010;&#29992;&#25143;&#21644;&#36817;130&#19975;&#26412;&#20070;&#65292;&#24182;&#23545;&#20854;&#20013;&#30340;449&#31687;&#20070;&#36827;&#34892;&#20102;&#25163;&#21160;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
A recommender system that optimizes its recommendations solely to fit a user's history of ratings for consumed items can create a filter bubble, wherein the user does not get to experience items from novel, unseen categories. One approach to mitigate this undesired behavior is to recommend items with high potential for serendipity, namely surprising items that are likely to be highly rated. In this paper, we propose a content-based formulation of serendipity that is rooted in Bayesian surprise and use it to measure the serendipity of items after they are consumed and rated by the user. When coupled with a collaborative-filtering component that identifies similar users, this enables recommending items with high potential for serendipity. To facilitate the evaluation of topic-level models for surprise and serendipity, we introduce a dataset of book reading histories extracted from Goodreads, containing over 26 thousand users and close to 1.3 million books, where we manually annotate 449 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#36793;&#32536;&#21270;&#23398;&#20064;&#38590;&#20197;&#35745;&#31639;&#30340;&#20998;&#24067;&#65292;&#20351;&#29992;&#21442;&#25968;&#21270;&#20998;&#24067;&#27169;&#22411;&#36817;&#20284;&#65292;&#35299;&#20915;&#20102;KL&#25955;&#24230;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#20248;&#21270;&#36807;&#31243;&#21487;&#24494;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.06352</link><description>&lt;p&gt;
&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#36793;&#32536;&#21270;&#23398;&#20064;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
Learning Distributions via Monte-Carlo Marginalization. (arXiv:2308.06352v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06352
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#36793;&#32536;&#21270;&#23398;&#20064;&#38590;&#20197;&#35745;&#31639;&#30340;&#20998;&#24067;&#65292;&#20351;&#29992;&#21442;&#25968;&#21270;&#20998;&#24067;&#27169;&#22411;&#36817;&#20284;&#65292;&#35299;&#20915;&#20102;KL&#25955;&#24230;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#20248;&#21270;&#36807;&#31243;&#21487;&#24494;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#20174;&#26679;&#26412;&#20013;&#23398;&#20064;&#38590;&#20197;&#35745;&#31639;&#30340;&#20998;&#24067;&#12290;&#20027;&#35201;&#24605;&#24819;&#26159;&#20351;&#29992;&#21442;&#25968;&#21270;&#20998;&#24067;&#27169;&#22411;&#65292;&#22914;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65288;GMM&#65289;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;KL&#25955;&#24230;&#26469;&#36817;&#20284;&#38590;&#20197;&#35745;&#31639;&#30340;&#20998;&#24067;&#12290;&#22522;&#20110;&#36825;&#20010;&#24605;&#24819;&#65292;&#26377;&#20004;&#20010;&#38656;&#35201;&#35299;&#20915;&#30340;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#24403;&#20998;&#24067;&#30340;&#32500;&#24230;&#22686;&#21152;&#26102;&#65292;KL&#25955;&#24230;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#26159;&#19981;&#21487;&#25509;&#21463;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#33945;&#29305;&#21345;&#27931;&#36793;&#32536;&#21270;&#65288;MCMarg&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#31532;&#20108;&#20010;&#25361;&#25112;&#26159;&#20248;&#21270;&#36807;&#31243;&#30340;&#21487;&#24494;&#24615;&#65292;&#22240;&#20026;&#30446;&#26631;&#20998;&#24067;&#26159;&#38590;&#20197;&#35745;&#31639;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#26680;&#23494;&#24230;&#20272;&#35745;&#65288;KDE&#65289;&#26469;&#22788;&#29702;&#36825;&#20010;&#38382;&#39064;&#12290;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#23398;&#20064;&#22797;&#26434;&#20998;&#24067;&#30340;&#26377;&#21147;&#24037;&#20855;&#65292;&#25972;&#20010;&#36807;&#31243;&#26159;&#21487;&#24494;&#30340;&#12290;&#22240;&#27492;&#65292;&#23427;&#21487;&#20197;&#26356;&#22909;&#22320;&#26367;&#20195;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#20013;&#30340;&#21464;&#20998;&#25512;&#26029;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21463;&#30410;&#30340;&#19968;&#20010;&#24378;&#26377;&#21147;&#30340;&#35777;&#25454;&#26159;&#23398;&#20064;&#30340;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
We propose a novel method to learn intractable distributions from their samples. The main idea is to use a parametric distribution model, such as a Gaussian Mixture Model (GMM), to approximate intractable distributions by minimizing the KL-divergence. Based on this idea, there are two challenges that need to be addressed. First, the computational complexity of KL-divergence is unacceptable when the dimensions of distributions increases. The Monte-Carlo Marginalization (MCMarg) is proposed to address this issue. The second challenge is the differentiability of the optimization process, since the target distribution is intractable. We handle this problem by using Kernel Density Estimation (KDE). The proposed approach is a powerful tool to learn complex distributions and the entire process is differentiable. Thus, it can be a better substitute of the variational inference in variational auto-encoders (VAE). One strong evidence of the benefit of our method is that the distributions learned
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38236;&#20687;&#25193;&#25955;&#27169;&#22411;(MDMs)&#65292;&#29992;&#20110;&#22312;&#31163;&#25955;&#20998;&#31867;&#25968;&#25454;&#21644;&#36830;&#32493;&#39046;&#22495;&#20013;&#36827;&#34892;&#29983;&#25104;&#20219;&#21153;&#12290;MDMs&#21463;&#38480;&#21046;&#25277;&#26679;&#38382;&#39064;&#30340;&#38236;&#20687;Langevin&#31639;&#27861;&#21551;&#21457;&#65292;&#24182;&#25552;&#20379;&#20102;&#36866;&#24212;&#31616;&#21333;&#25193;&#25955;&#12289;&#22270;&#20687;&#29983;&#25104;&#21644;&#25991;&#26412;&#29983;&#25104;&#31561;&#39046;&#22495;&#30340;&#33258;&#28982;&#25193;&#23637;&#12290;</title><link>http://arxiv.org/abs/2308.06342</link><description>&lt;p&gt;
&#38236;&#20687;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Mirror Diffusion Models. (arXiv:2308.06342v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06342
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38236;&#20687;&#25193;&#25955;&#27169;&#22411;(MDMs)&#65292;&#29992;&#20110;&#22312;&#31163;&#25955;&#20998;&#31867;&#25968;&#25454;&#21644;&#36830;&#32493;&#39046;&#22495;&#20013;&#36827;&#34892;&#29983;&#25104;&#20219;&#21153;&#12290;MDMs&#21463;&#38480;&#21046;&#25277;&#26679;&#38382;&#39064;&#30340;&#38236;&#20687;Langevin&#31639;&#27861;&#21551;&#21457;&#65292;&#24182;&#25552;&#20379;&#20102;&#36866;&#24212;&#31616;&#21333;&#25193;&#25955;&#12289;&#22270;&#20687;&#29983;&#25104;&#21644;&#25991;&#26412;&#29983;&#25104;&#31561;&#39046;&#22495;&#30340;&#33258;&#28982;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#21508;&#31181;&#36830;&#32493;&#39046;&#22495;&#30340;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22312;&#31163;&#25955;&#30340;&#20998;&#31867;&#25968;&#25454;&#20013;&#24212;&#29992;&#25193;&#25955;&#20173;&#28982;&#26159;&#19968;&#20010;&#38750;&#24179;&#20961;&#30340;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#22312;&#36830;&#32493;&#39046;&#22495;&#30340;&#29983;&#25104;&#20013;&#24120;&#24120;&#38656;&#35201;&#36827;&#34892;&#21098;&#20999;&#65292;&#36825;&#23601;&#38656;&#35201;&#19968;&#20010;&#23558;&#25193;&#25955;&#36866;&#24212;&#32422;&#26463;&#22495;&#30340;&#29702;&#35770;&#26694;&#26550;&#12290;&#21463;&#38480;&#21046;&#25277;&#26679;&#38382;&#39064;&#30340;&#38236;&#20687;Langevin&#31639;&#27861;&#30340;&#21551;&#21457;&#65292;&#26412;&#29702;&#35770;&#25253;&#21578;&#20013;&#25105;&#20204;&#25552;&#20986;&#20102;&#38236;&#20687;&#25193;&#25955;&#27169;&#22411;(MDMs)&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;MDMs&#22312;simplex&#25193;&#25955;&#30340;&#32972;&#26223;&#19979;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#22270;&#20687;&#21644;&#25991;&#26412;&#29983;&#25104;&#31561;&#28909;&#38376;&#39046;&#22495;&#30340;&#33258;&#28982;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have successfully been applied to generative tasks in various continuous domains. However, applying diffusion to discrete categorical data remains a non-trivial task. Moreover, generation in continuous domains often requires clipping in practice, which motivates the need for a theoretical framework for adapting diffusion to constrained domains. Inspired by the mirror Langevin algorithm for the constrained sampling problem, in this theoretical report we propose Mirror Diffusion Models (MDMs). We demonstrate MDMs in the context of simplex diffusion and propose natural extensions to popular domains such as image and text generation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24314;&#31435;&#20102;&#28145;&#24230;&#31639;&#23376;&#32593;&#32476;&#30340;&#25968;&#25454;&#20381;&#36182;&#24615;&#22823;&#23567;&#19979;&#30028;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#26102;&#65292;&#25903;&#36335;&#32593;&#32476;&#21644;&#20027;&#24178;&#32593;&#32476;&#30340;&#20849;&#21516;&#36755;&#20986;&#32500;&#24230;&#38656;&#35201;&#19982;&#25968;&#25454;&#28857;&#25968;&#37327;&#25353;&#29031;&#937;(&#8730;n)&#30340;&#27604;&#20363;&#25193;&#23637;&#65292;&#24182;&#19988;&#20026;&#20102;&#33719;&#24471;&#26356;&#20302;&#30340;&#35757;&#32451;&#35823;&#24046;&#65292;&#35757;&#32451;&#25968;&#25454;&#30340;&#22823;&#23567;&#21487;&#33021;&#38656;&#35201;&#19982;&#20849;&#21516;&#36755;&#20986;&#32500;&#24230;&#25353;&#29031;&#20108;&#27425;&#27604;&#20363;&#20851;&#31995;&#25193;&#23637;&#12290;</title><link>http://arxiv.org/abs/2308.06338</link><description>&lt;p&gt;
&#28145;&#24230;&#31639;&#23376;&#32593;&#32476;&#30340;&#22823;&#23567;&#19979;&#30028;
&lt;/p&gt;
&lt;p&gt;
Size Lowerbounds for Deep Operator Networks. (arXiv:2308.06338v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06338
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24314;&#31435;&#20102;&#28145;&#24230;&#31639;&#23376;&#32593;&#32476;&#30340;&#25968;&#25454;&#20381;&#36182;&#24615;&#22823;&#23567;&#19979;&#30028;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#26102;&#65292;&#25903;&#36335;&#32593;&#32476;&#21644;&#20027;&#24178;&#32593;&#32476;&#30340;&#20849;&#21516;&#36755;&#20986;&#32500;&#24230;&#38656;&#35201;&#19982;&#25968;&#25454;&#28857;&#25968;&#37327;&#25353;&#29031;&#937;(&#8730;n)&#30340;&#27604;&#20363;&#25193;&#23637;&#65292;&#24182;&#19988;&#20026;&#20102;&#33719;&#24471;&#26356;&#20302;&#30340;&#35757;&#32451;&#35823;&#24046;&#65292;&#35757;&#32451;&#25968;&#25454;&#30340;&#22823;&#23567;&#21487;&#33021;&#38656;&#35201;&#19982;&#20849;&#21516;&#36755;&#20986;&#32500;&#24230;&#25353;&#29031;&#20108;&#27425;&#27604;&#20363;&#20851;&#31995;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31639;&#23376;&#32593;&#32476;&#26159;&#19968;&#31181;&#22312;&#26080;&#38480;&#32500;&#24230;&#20013;&#35299;&#20915;&#22238;&#24402;&#38382;&#39064;&#21644;&#19968;&#27425;&#35299;&#20915;&#19968;&#31867;&#20559;&#24494;&#20998;&#26041;&#31243;&#32452;&#30340;&#27969;&#34892;&#33539;&#24335;&#12290;&#26412;&#25991;&#26088;&#22312;&#24314;&#31435;&#19968;&#31181;&#39318;&#27425;&#20381;&#36182;&#20110;&#25968;&#25454;&#30340;&#28145;&#24230;&#31639;&#23376;&#32593;&#32476;&#22823;&#23567;&#19979;&#30028;&#65292;&#20197;&#20415;&#33021;&#22815;&#22312;&#22122;&#22768;&#25968;&#25454;&#19978;&#20943;&#23567;&#32463;&#39564;&#35823;&#24046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20026;&#20102;&#33719;&#24471;&#20302;&#35757;&#32451;&#35823;&#24046;&#65292;&#38656;&#35201;&#23558;&#25903;&#36335;&#32593;&#32476;&#21644;&#20027;&#24178;&#32593;&#32476;&#30340;&#20849;&#21516;&#36755;&#20986;&#32500;&#24230;&#19982;&#25968;&#25454;&#28857;&#25968;&#37327;n&#25353;&#29031;&#937;(&#8730;n)&#30340;&#27604;&#20363;&#25193;&#23637;&#12290;&#36825;&#21551;&#21457;&#20102;&#25105;&#20204;&#22312;&#35299;&#20915;&#23545;&#27969;-&#25193;&#25955;-&#21453;&#24212;&#20559;&#24494;&#20998;&#26041;&#31243;&#26102;&#23545;&#28145;&#24230;&#31639;&#23376;&#32593;&#32476;&#36827;&#34892;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#22266;&#23450;&#27169;&#22411;&#22823;&#23567;&#30340;&#24773;&#20917;&#19979;&#65292;&#21033;&#29992;&#36825;&#31181;&#20849;&#21516;&#36755;&#20986;&#32500;&#24230;&#30340;&#22686;&#21152;&#65292;&#21487;&#20197;&#21333;&#35843;&#38477;&#20302;&#35757;&#32451;&#35823;&#24046;&#65292;&#32780;&#35757;&#32451;&#25968;&#25454;&#30340;&#22823;&#23567;&#21487;&#33021;&#38656;&#35201;&#19982;&#20043;&#21576;&#20108;&#27425;&#27604;&#20363;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Operator Networks are an increasingly popular paradigm for solving regression in infinite dimensions and hence solve families of PDEs in one shot. In this work, we aim to establish a first-of-its-kind data-dependent lowerbound on the size of DeepONets required for them to be able to reduce empirical error on noisy data. In particular, we show that for low training errors to be obtained on $n$ data points it is necessary that the common output dimension of the branch and the trunk net be scaling as $\Omega \left ( {\sqrt{n}} \right )$. This inspires our experiments with DeepONets solving the advection-diffusion-reaction PDE, where we demonstrate the possibility that at a fixed model size, to leverage increase in this common output dimension and get monotonic lowering of training error, the size of the training data might necessarily need to scale quadratically with it.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#26469;&#39044;&#27979;&#31995;&#32479;&#30340;&#24377;&#24615;&#65292;&#21253;&#25324;&#36127;&#38754;&#21644;&#27491;&#38754;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#23545;&#30772;&#22351;&#24615;&#20107;&#20214;&#21644;&#24674;&#22797;&#27963;&#21160;&#36827;&#34892;&#24314;&#27169;&#21644;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#39044;&#27979;&#31995;&#32479;&#30340;&#24377;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.06309</link><description>&lt;p&gt;
&#29992;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#24377;&#24615;
&lt;/p&gt;
&lt;p&gt;
Predicting Resilience with Neural Networks. (arXiv:2308.06309v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06309
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#26469;&#39044;&#27979;&#31995;&#32479;&#30340;&#24377;&#24615;&#65292;&#21253;&#25324;&#36127;&#38754;&#21644;&#27491;&#38754;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#23545;&#30772;&#22351;&#24615;&#20107;&#20214;&#21644;&#24674;&#22797;&#27963;&#21160;&#36827;&#34892;&#24314;&#27169;&#21644;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#39044;&#27979;&#31995;&#32479;&#30340;&#24377;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24377;&#24615;&#24037;&#31243;&#30740;&#31350;&#31995;&#32479;&#22312;&#36973;&#21463;&#30772;&#22351;&#24615;&#20107;&#20214;&#21518;&#30340;&#23384;&#27963;&#21644;&#24674;&#22797;&#33021;&#21147;&#65292;&#22312;&#22810;&#20010;&#39046;&#22495;&#26377;&#24212;&#29992;&#12290;&#22823;&#37096;&#20998;&#30740;&#31350;&#24378;&#35843;&#24377;&#24615;&#24230;&#37327;&#26469;&#37327;&#21270;&#31995;&#32479;&#24615;&#33021;&#65292;&#32780;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#32479;&#35745;&#24314;&#27169;&#26041;&#27861;&#26469;&#39044;&#27979;&#31995;&#32479;&#22312;&#36864;&#21270;&#21518;&#30340;&#24674;&#22797;&#26102;&#38388;&#12290;&#27492;&#22806;&#65292;&#36807;&#21435;&#30340;&#30740;&#31350;&#35201;&#20040;&#22312;&#24674;&#22797;&#21518;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#65292;&#35201;&#20040;&#23616;&#38480;&#20110;&#29702;&#24819;&#21270;&#30340;&#36235;&#21183;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#26367;&#20195;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#26041;&#27861;&#65292;&#21253;&#25324;&#65288;i&#65289;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#65288;ii&#65289;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65292;&#65288;iii&#65289;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#65292;&#29992;&#20110;&#24314;&#27169;&#21644;&#39044;&#27979;&#31995;&#32479;&#24615;&#33021;&#65292;&#21253;&#25324;&#39537;&#21160;&#24377;&#24615;&#30340;&#36127;&#38754;&#21644;&#27491;&#38754;&#22240;&#32032;&#65292;&#20197;&#37327;&#21270;&#30772;&#22351;&#24615;&#20107;&#20214;&#21644;&#24674;&#22797;&#27963;&#21160;&#30340;&#24433;&#21709;&#12290;&#20351;&#29992;&#22343;&#26041;&#35823;&#24046;&#21644;&#35843;&#25972;R&#24179;&#26041;&#31561;&#25311;&#21512;&#24230;&#37327;&#25351;&#26631;&#26469;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#24182;&#19982;&#32463;&#20856;&#30340;&#32479;&#35745;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;NN&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#39044;&#27979;&#31995;&#32479;&#30340;&#24377;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Resilience engineering studies the ability of a system to survive and recover from disruptive events, which finds applications in several domains. Most studies emphasize resilience metrics to quantify system performance, whereas recent studies propose statistical modeling approaches to project system recovery time after degradation. Moreover, past studies are either performed on data after recovering or limited to idealized trends. Therefore, this paper proposes three alternative neural network (NN) approaches including (i) Artificial Neural Networks, (ii) Recurrent Neural Networks, and (iii) Long-Short Term Memory (LSTM) to model and predict system performance, including negative and positive factors driving resilience to quantify the impact of disruptive events and restorative activities. Goodness-of-fit measures are computed to evaluate the models and compared with a classical statistical model, including mean squared error and adjusted R squared. Our results indicate that NN models
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20811;&#26381;&#37325;&#21147;&#27169;&#22411;&#20013;&#38646;&#36152;&#26131;&#38382;&#39064;&#30340;&#20004;&#27493;&#25216;&#26415;&#65292;&#36890;&#36807;&#23616;&#37096;&#32447;&#24615;&#22238;&#24402;&#21644;&#34394;&#25311;&#20540;&#26367;&#20195;&#26469;&#30830;&#23450;&#37325;&#21147;&#21442;&#25968;&#65292;&#24182;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#21442;&#25968;&#39564;&#35777;&#12290;&#36890;&#36807;&#35813;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#32447;&#24615;&#23545;&#25968;&#26041;&#31243;&#20013;&#30340;&#19981;&#30830;&#23450;&#20540;&#38382;&#39064;&#65292;&#36827;&#19968;&#27493;&#23436;&#21892;&#20102;&#37325;&#21147;&#27169;&#22411;&#22312;&#35299;&#37322;&#22269;&#38469;&#36152;&#26131;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.06303</link><description>&lt;p&gt;
&#20811;&#26381;&#37325;&#21147;&#27169;&#22411;&#20013;&#38646;&#36152;&#26131;&#30340;&#26032;&#26041;&#27861;&#65292;&#36991;&#20813;&#32447;&#24615;&#23545;&#25968;&#26041;&#31243;&#20013;&#30340;&#19981;&#30830;&#23450;&#20540;&#24182;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#21442;&#25968;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
A New Approach to Overcoming Zero Trade in Gravity Models to Avoid Indefinite Values in Linear Logarithmic Equations and Parameter Verification Using Machine Learning. (arXiv:2308.06303v1 [econ.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06303
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20811;&#26381;&#37325;&#21147;&#27169;&#22411;&#20013;&#38646;&#36152;&#26131;&#38382;&#39064;&#30340;&#20004;&#27493;&#25216;&#26415;&#65292;&#36890;&#36807;&#23616;&#37096;&#32447;&#24615;&#22238;&#24402;&#21644;&#34394;&#25311;&#20540;&#26367;&#20195;&#26469;&#30830;&#23450;&#37325;&#21147;&#21442;&#25968;&#65292;&#24182;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#21442;&#25968;&#39564;&#35777;&#12290;&#36890;&#36807;&#35813;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#32447;&#24615;&#23545;&#25968;&#26041;&#31243;&#20013;&#30340;&#19981;&#30830;&#23450;&#20540;&#38382;&#39064;&#65292;&#36827;&#19968;&#27493;&#23436;&#21892;&#20102;&#37325;&#21147;&#27169;&#22411;&#22312;&#35299;&#37322;&#22269;&#38469;&#36152;&#26131;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20351;&#29992;&#37325;&#21147;&#27169;&#22411;&#35299;&#37322;&#22269;&#38469;&#36152;&#26131;&#26102;&#65292;&#23384;&#22312;&#22823;&#37327;&#38646;&#36152;&#26131;&#27969;&#30340;&#24773;&#20917;&#65292;&#36825;&#32473;&#30830;&#23450;&#37325;&#21147;&#21442;&#25968;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#32447;&#24615;&#22238;&#24402;&#21450;&#23545;&#25968;&#32447;&#24615;&#26041;&#31243;&#22312;&#23545;&#25968;&#36152;&#26131;&#19978;&#20250;&#36935;&#21040;&#19981;&#30830;&#23450;&#20540;&#30340;&#38382;&#39064;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#20294;&#20854;&#20013;&#22823;&#37096;&#20998;&#19981;&#20877;&#22522;&#20110;&#32447;&#24615;&#22238;&#24402;&#65292;&#20351;&#24471;&#23547;&#25214;&#35299;&#20915;&#26041;&#26696;&#30340;&#36807;&#31243;&#26356;&#21152;&#22797;&#26434;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#27493;&#25216;&#26415;&#26469;&#30830;&#23450;&#37325;&#21147;&#21442;&#25968;&#65306;&#39318;&#20808;&#65292;&#22312;&#23616;&#37096;&#36827;&#34892;&#32447;&#24615;&#22238;&#24402;&#20197;&#24314;&#31435;&#19968;&#20010;&#26367;&#20195;&#38646;&#36152;&#26131;&#27969;&#37327;&#30340;&#34394;&#25311;&#20540;&#65292;&#28982;&#21518;&#20272;&#35745;&#37325;&#21147;&#21442;&#25968;&#12290;&#37319;&#29992;&#36845;&#20195;&#25216;&#26415;&#30830;&#23450;&#26368;&#20248;&#21442;&#25968;&#12290;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#36890;&#36807;&#20998;&#26512;&#21442;&#25968;&#22312;&#32858;&#31867;&#20013;&#30340;&#20301;&#32622;&#26469;&#27979;&#35797;&#20272;&#35745;&#24471;&#21040;&#30340;&#21442;&#25968;&#12290;&#25105;&#20204;&#35745;&#31639;&#20102;2004&#24180;&#12289;2009&#24180;&#12289;2014&#24180;&#21644;2019&#24180;&#30340;&#22269;&#38469;&#36152;&#26131;&#25968;&#25454;&#12290;&#25105;&#20204;&#21482;&#30740;&#31350;&#20102;&#32463;&#20856;&#37325;&#21147;&#26041;&#31243;&#24182;&#21457;&#29616;...
&lt;/p&gt;
&lt;p&gt;
The presence of a high number of zero flow trades continues to provide a challenge in identifying gravity parameters to explain international trade using the gravity model. Linear regression with a logarithmic linear equation encounters an indefinite value on the logarithmic trade. Although several approaches to solving this problem have been proposed, the majority of them are no longer based on linear regression, making the process of finding solutions more complex. In this work, we suggest a two-step technique for determining the gravity parameters: first, perform linear regression locally to establish a dummy value to substitute trade flow zero, and then estimating the gravity parameters. Iterative techniques are used to determine the optimum parameters. Machine learning is used to test the estimated parameters by analyzing their position in the cluster. We calculated international trade figures for 2004, 2009, 2014, and 2019. We just examine the classic gravity equation and discove
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36890;&#36807;&#22270;&#20687;&#20998;&#31867;&#23545;&#20154;&#31867;&#34880;&#32454;&#32990;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#35782;&#21035;&#65292;&#20026;&#35786;&#26029;&#30142;&#30149;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2308.06300</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#34880;&#32454;&#32990;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Classification of Blood Cells Using Deep Learning Models. (arXiv:2308.06300v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06300
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36890;&#36807;&#22270;&#20687;&#20998;&#31867;&#23545;&#20154;&#31867;&#34880;&#32454;&#32990;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#35782;&#21035;&#65292;&#20026;&#35786;&#26029;&#30142;&#30149;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#34880;&#28082;&#20027;&#35201;&#21253;&#25324;&#34880;&#27974;&#12289;&#32418;&#32454;&#32990;&#12289;&#30333;&#32454;&#32990;&#21644;&#34880;&#23567;&#26495;&#12290;&#34880;&#32454;&#32990;&#20026;&#36523;&#20307;&#32454;&#32990;&#25552;&#20379;&#27687;&#27668;&#65292;&#28363;&#20859;&#23427;&#20204;&#65292;&#20445;&#25252;&#23427;&#20204;&#20813;&#21463;&#24863;&#26579;&#65292;&#22686;&#24378;&#20813;&#30123;&#21147;&#24182;&#20419;&#36827;&#20957;&#34880;&#12290;&#20154;&#30340;&#20581;&#24247;&#29366;&#20917;&#21487;&#20197;&#20174;&#34880;&#32454;&#32990;&#20013;&#21453;&#26144;&#20986;&#26469;&#12290;&#19968;&#20010;&#20154;&#34987;&#35786;&#26029;&#20986;&#26576;&#31181;&#30142;&#30149;&#30340;&#26426;&#20250;&#24456;&#22823;&#31243;&#24230;&#19978;&#21463;&#20854;&#34880;&#32454;&#32990;&#31867;&#22411;&#21644;&#35745;&#25968;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#34880;&#32454;&#32990;&#20998;&#31867;&#38750;&#24120;&#37325;&#35201;&#65292;&#23427;&#21487;&#20197;&#24110;&#21161;&#35782;&#21035;&#30142;&#30149;&#65292;&#21253;&#25324;&#30284;&#30151;&#12289;&#39592;&#39635;&#25439;&#20260;&#12289;&#33391;&#24615;&#32959;&#30244;&#21644;&#23427;&#20204;&#30340;&#29983;&#38271;&#12290;&#36825;&#31181;&#20998;&#31867;&#21487;&#20197;&#24110;&#21161;&#34880;&#28082;&#23398;&#23478;&#21306;&#20998;&#19981;&#21516;&#30340;&#34880;&#32454;&#32990;&#29255;&#27573;&#65292;&#20197;&#20415;&#30830;&#23450;&#30142;&#30149;&#30340;&#21407;&#22240;&#12290;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#23427;&#23558;&#20154;&#31867;&#34880;&#32454;&#32990;&#65288;&#32418;&#32454;&#32990;&#12289;&#30333;&#32454;&#32990;&#21644;&#34880;&#23567;&#26495;&#65289;&#30340;&#22270;&#20687;&#20998;&#31867;&#20026;&#23427;&#20204;&#30340;&#20122;&#22411;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#23558;&#19981;&#21516;&#30340;CNN&#39044;&#35757;&#32451;&#27169;&#22411;&#24212;&#29992;&#20110;&#34880;&#32454;&#32990;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human blood mainly comprises plasma, red blood cells, white blood cells, and platelets. The blood cells provide the body's cells oxygen to nourish them, shield them from infections, boost immunity, and aid in clotting. Human health is reflected in blood cells. The chances that a human being can be diagnosed with a disease are significantly influenced by their blood cell type and count. Therefore, blood cell classification is crucial because it helps identify diseases, including cancer, damaged bone marrow, benign tumors, and their growth. This classification allows hematologists to distinguish between different blood cell fragments so that the cause of diseases can be identified. Convolution neural networks are a deep learning technique that classifies images of human blood cells (RBCs, WBCs, and platelets) into their subtypes. For this study, transfer learning is used to apply different CNN pre-trained models, including VGG16, VGG19, ResNet-50, ResNet-101, ResNet-152, InceptionV3 Mobi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#33258;&#21160;&#39550;&#39542;&#20013;&#31070;&#32463;&#32593;&#32476;&#30340;&#37096;&#32626;&#20013;&#19981;&#34987;&#23519;&#35273;&#30340;&#28798;&#38590;&#24615;&#38382;&#39064;&#21644;&#39046;&#22495;&#36716;&#31227;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#23553;&#35013;&#20102;&#37096;&#32626;&#20013;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#25552;&#39640;&#24863;&#30693;&#31995;&#32479;&#30340;&#24615;&#33021;&#21644;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.06299</link><description>&lt;p&gt;
&#38450;&#24481;&#24863;&#30693;&#65306;&#31070;&#32463;&#32593;&#32476;&#22312;&#37096;&#32626;&#20013;&#30340;&#24615;&#33021;&#20272;&#35745;&#21644;&#30417;&#27979;
&lt;/p&gt;
&lt;p&gt;
Defensive Perception: Estimation and Monitoring of Neural Network Performance under Deployment. (arXiv:2308.06299v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06299
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#33258;&#21160;&#39550;&#39542;&#20013;&#31070;&#32463;&#32593;&#32476;&#30340;&#37096;&#32626;&#20013;&#19981;&#34987;&#23519;&#35273;&#30340;&#28798;&#38590;&#24615;&#38382;&#39064;&#21644;&#39046;&#22495;&#36716;&#31227;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#23553;&#35013;&#20102;&#37096;&#32626;&#20013;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#25552;&#39640;&#24863;&#30693;&#31995;&#32479;&#30340;&#24615;&#33021;&#21644;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#33258;&#21160;&#39550;&#39542;&#20013;&#35821;&#20041;&#20998;&#21106;&#31070;&#32463;&#32593;&#32476;&#22312;&#37096;&#32626;&#36807;&#31243;&#20013;&#19981;&#34987;&#23519;&#35273;&#30340;&#28798;&#38590;&#24615;&#38382;&#39064;&#21644;&#39046;&#22495;&#36716;&#31227;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#24863;&#30693;&#23545;&#33258;&#21160;&#39550;&#39542;&#30340;&#19981;&#30830;&#23450;&#24615;&#24182;&#23558;&#20854;&#26368;&#20339;&#34920;&#31034;&#20026;&#27010;&#29575;&#20998;&#24067;&#12290;&#30001;&#20110;&#33258;&#21160;&#36710;&#36742;&#30340;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#65292;&#24863;&#30693;&#31995;&#32479;&#24517;&#39035;&#33021;&#22815;&#35782;&#21035;&#36710;&#36742;&#26159;&#21542;&#31163;&#24320;&#20102;&#25805;&#20316;&#35774;&#35745;&#39046;&#22495;&#65292;&#39044;&#27979;&#21361;&#38505;&#30340;&#19981;&#30830;&#23450;&#24615;&#24182;&#38477;&#20302;&#24863;&#30693;&#31995;&#32479;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#37096;&#32626;&#20013;&#23545;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#23553;&#35013;&#65292;&#35813;&#23553;&#35013;&#22522;&#20110;&#33945;&#29305;&#21345;&#32599; Dropout &#26041;&#27861;&#36890;&#36807;&#23545;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#20272;&#35745;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#20462;&#25913;&#37096;&#32626;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#20445;&#35777;&#39044;&#26399;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#38450;&#24481;&#24615;&#24863;&#30693;&#23553;&#35013;&#33021;&#22815;&#20272;&#35745;&#31070;&#32463;&#32593;&#32476;&#30340;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a method for addressing the issue of unnoticed catastrophic deployment and domain shift in neural networks for semantic segmentation in autonomous driving. Our approach is based on the idea that deep learning-based perception for autonomous driving is uncertain and best represented as a probability distribution. As autonomous vehicles' safety is paramount, it is crucial for perception systems to recognize when the vehicle is leaving its operational design domain, anticipate hazardous uncertainty, and reduce the performance of the perception system. To address this, we propose to encapsulate the neural network under deployment within an uncertainty estimation envelope that is based on the epistemic uncertainty estimation through the Monte Carlo Dropout approach. This approach does not require modification of the deployed neural network and guarantees expected model performance. Our defensive perception envelope has the capability to estimate a neural network's 
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#31995;&#32479;&#20998;&#26512;&#20102;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#39046;&#22495;&#20013;&#24212;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#20026;&#30333;&#32454;&#32990;&#20998;&#31867;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#27934;&#23519;&#21147;&#21644;&#26368;&#20339;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.06296</link><description>&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#30333;&#32454;&#32990;&#36827;&#34892;&#20998;&#31867;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Review on Classification of White Blood Cells Using Machine Learning Models. (arXiv:2308.06296v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06296
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#31995;&#32479;&#20998;&#26512;&#20102;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#39046;&#22495;&#20013;&#24212;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#20026;&#30333;&#32454;&#32990;&#20998;&#31867;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#27934;&#23519;&#21147;&#21644;&#26368;&#20339;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#26041;&#38754;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#39044;&#27979;&#21644;&#20998;&#31867;&#25552;&#39640;&#20102;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#24110;&#21161;&#34880;&#28082;&#23398;&#23478;&#26681;&#25454;&#35745;&#31639;&#21644;&#20107;&#23454;&#26469;&#35786;&#26029;&#34880;&#28082;&#30284;&#30151;&#21644;&#33041;&#32959;&#30244;&#12290;&#26412;&#32508;&#36848;&#20027;&#35201;&#20851;&#27880;&#20110;&#23545;&#30333;&#32454;&#32990;&#20998;&#31867;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#39046;&#22495;&#20013;&#24212;&#29992;&#30340;&#29616;&#20195;&#25216;&#26415;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;&#12290;&#38024;&#23545;&#26412;&#32508;&#36848;&#65292;&#35752;&#35770;&#20102;&#20351;&#29992;&#34880;&#28034;&#29255;&#22270;&#20687;&#12289;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#12289;X&#23556;&#32447;&#21644;&#31867;&#20284;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#30340;&#26041;&#27861;&#12290;&#26412;&#32508;&#36848;&#30340;&#20027;&#35201;&#24433;&#21709;&#22312;&#20110;&#23545;&#24212;&#29992;&#20110;&#30333;&#32454;&#32990;&#20998;&#31867;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#35814;&#32454;&#20998;&#26512;&#12290;&#36825;&#31181;&#20998;&#26512;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#27934;&#23519;&#21147;&#65292;&#20363;&#22914;&#26368;&#24120;&#20351;&#29992;&#30340;&#25216;&#26415;&#21644;&#26368;&#20339;&#34920;&#29616;&#30340;&#30333;&#32454;&#32990;&#20998;&#31867;&#26041;&#27861;&#12290;&#26368;&#36817;&#20960;&#21313;&#24180;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#30740;&#31350;&#20154;&#21592;&#19968;&#30452;&#22312;&#20351;&#29992;ML&#21644;DL&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The machine learning (ML) and deep learning (DL) models contribute to exceptional medical image analysis improvement. The models enhance the prediction and improve the accuracy by prediction and classification. It helps the hematologist to diagnose the blood cancer and brain tumor based on calculations and facts. This review focuses on an in-depth analysis of modern techniques applied in the domain of medical image analysis of white blood cell classification. For this review, the methodologies are discussed that have used blood smear images, magnetic resonance imaging (MRI), X-rays, and similar medical imaging domains. The main impact of this review is to present a detailed analysis of machine learning techniques applied for the classification of white blood cells (WBCs). This analysis provides valuable insight, such as the most widely used techniques and best-performing white blood cell classification methods. It was found that in recent decades researchers have been using ML and DL f
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24212;&#29992;&#21644;&#27604;&#36739;&#20102;MCMC&#21644;VI&#35757;&#32451;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#22312;&#39640;&#20809;&#35889;&#22270;&#20687;&#19978;&#36827;&#34892;&#30446;&#26631;&#26816;&#27979;&#65292;&#35299;&#20915;&#20102;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.06293</link><description>&lt;p&gt;
&#20351;&#29992;MCMC&#21644;VI&#35757;&#32451;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#22312;&#39640;&#20809;&#35889;&#22270;&#20687;&#19978;&#36827;&#34892;&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Target Detection on Hyperspectral Images Using MCMC and VI Trained Bayesian Neural Networks. (arXiv:2308.06293v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06293
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24212;&#29992;&#21644;&#27604;&#36739;&#20102;MCMC&#21644;VI&#35757;&#32451;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#22312;&#39640;&#20809;&#35889;&#22270;&#20687;&#19978;&#36827;&#34892;&#30446;&#26631;&#26816;&#27979;&#65292;&#35299;&#20915;&#20102;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#21464;&#24471;&#26080;&#22788;&#19981;&#22312;&#65292;&#20294;&#23427;&#20204;&#30340;&#26631;&#20934;&#24418;&#24335;&#21482;&#33021;&#20135;&#29983;&#28857;&#20272;&#35745;&#65292;&#27809;&#26377;&#32622;&#20449;&#24230;&#30340;&#24230;&#37327;&#12290;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#21518;&#39564;&#20998;&#24067;&#25552;&#20379;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#21644;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#38543;&#30528;&#31070;&#32463;&#32593;&#32476;&#22312;&#26356;&#39640;&#39118;&#38505;&#30340;&#24212;&#29992;&#20013;&#30340;&#24212;&#29992;&#65292;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#25552;&#20379;&#20934;&#30830;&#30340;&#39044;&#27979;&#21644;&#20272;&#35745;&#65292;&#20197;&#21450;&#19968;&#20010;&#21253;&#21547;&#22312;&#26399;&#26395;&#27010;&#29575;&#20869;&#21512;&#29702;&#20540;&#30340;&#21306;&#38388;&#65292;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#23613;&#31649;&#20855;&#26377;&#31215;&#26497;&#30340;&#29305;&#28857;&#65292;&#20294;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#36215;&#26469;&#38750;&#24120;&#22256;&#38590;&#32780;&#19988;&#32791;&#26102;&#12290;&#20256;&#32479;&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;(MCMC)&#65292;&#20294;&#36825;&#36890;&#24120;&#34987;&#35270;&#20026;&#22826;&#24930;&#12290;&#26368;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#21464;&#20998;&#25512;&#26029;(VI)&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#36827;&#34892;&#24555;&#36895;&#35745;&#31639;&#65292;&#20294;&#23427;&#30340;&#26377;&#25928;&#24615;&#23384;&#22312;&#22810;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#39640;&#20809;&#35889;&#30446;&#26631;&#26816;&#27979;&#30340;&#32972;&#26223;&#19979;&#24212;&#29992;&#21644;&#27604;&#36739;&#20102;MCMC&#21644;VI&#35757;&#32451;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks (NN) have become almost ubiquitous with image classification, but in their standard form produce point estimates, with no measure of confidence. Bayesian neural networks (BNN) provide uncertainty quantification (UQ) for NN predictions and estimates through the posterior distribution. As NN are applied in more high-consequence applications, UQ is becoming a requirement. BNN provide a solution to this problem by not only giving accurate predictions and estimates, but also an interval that includes reasonable values within a desired probability. Despite their positive attributes, BNN are notoriously difficult and time consuming to train. Traditional Bayesian methods use Markov Chain Monte Carlo (MCMC), but this is often brushed aside as being too slow. The most common method is variational inference (VI) due to its fast computation, but there are multiple concerns with its efficacy. We apply and compare MCMC- and VI-trained BNN in the context of target detection in hypersp
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#20445;&#23432;&#24615;&#27169;&#24335;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#26102;&#38388;&#21442;&#25968;&#21270;&#30340;&#32479;&#35745;&#27169;&#22411;&#65292;&#29992;&#20110;&#37327;&#21270;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#20998;&#21270;&#28436;&#21270;&#65292;&#24182;&#19982;&#24207;&#21015;&#30340;&#20998;&#21270;&#20851;&#31995;&#36827;&#34892;&#23545;&#27604;&#12290;&#36825;&#20010;&#27169;&#22411;&#33021;&#26356;&#20934;&#30830;&#20272;&#35745;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#20998;&#21270;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2308.06292</link><description>&lt;p&gt;
&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#20998;&#21270;&#26102;&#38388;&#27169;&#22411;&#19982;&#24207;&#21015;&#20998;&#21270;&#30340;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
The divergence time of protein structures modelled by Markov matrices and its relation to the divergence of sequences. (arXiv:2308.06292v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06292
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#20445;&#23432;&#24615;&#27169;&#24335;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#26102;&#38388;&#21442;&#25968;&#21270;&#30340;&#32479;&#35745;&#27169;&#22411;&#65292;&#29992;&#20110;&#37327;&#21270;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#20998;&#21270;&#28436;&#21270;&#65292;&#24182;&#19982;&#24207;&#21015;&#30340;&#20998;&#21270;&#20851;&#31995;&#36827;&#34892;&#23545;&#27604;&#12290;&#36825;&#20010;&#27169;&#22411;&#33021;&#26356;&#20934;&#30830;&#20272;&#35745;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#20998;&#21270;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22823;&#37327;&#34507;&#30333;&#36136;&#19977;&#32500;&#32467;&#26500;&#23545;&#40784;&#30340;&#26041;&#24335;&#65292;&#25512;&#23548;&#20986;&#19968;&#20010;&#23436;&#25972;&#30340;&#12289;&#26102;&#38388;&#21442;&#25968;&#21270;&#30340;&#32479;&#35745;&#27169;&#22411;&#65292;&#29992;&#20110;&#37327;&#21270;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#20998;&#21270;&#28436;&#21270;&#65292;&#35813;&#27169;&#22411;&#20197;&#20108;&#32423;&#32467;&#26500;&#30340;&#20445;&#23432;&#24615;&#27169;&#24335;&#20026;&#25351;&#26631;&#12290;&#30456;&#23545;&#20110;&#20197;&#24207;&#21015;&#20026;&#22522;&#30784;&#30340;&#26102;&#38388;&#21442;&#25968;&#21270;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#26356;&#22909;&#22320;&#22788;&#29702;&#24207;&#21015;&#20851;&#31995;&#30340;&#40644;&#26127;&#21306;&#21644;&#21320;&#22812;&#21306;&#30340;&#38480;&#21046;&#12290;&#30001;&#20110;&#34507;&#30333;&#36136;&#32467;&#26500;&#21463;&#21151;&#33021;&#30452;&#25509;&#26045;&#21152;&#30340;&#36873;&#25321;&#21387;&#21147;&#26356;&#22823;&#65292;&#20174;&#32467;&#26500;&#25512;&#26029;&#20998;&#21270;&#26102;&#38388;&#30340;&#20272;&#35745;&#26356;&#20934;&#30830;&#12290;&#25105;&#20204;&#20351;&#29992;&#26368;&#23567;&#20449;&#24687;&#38271;&#24230;&#30340;&#36125;&#21494;&#26031;&#21644;&#20449;&#24687;&#35770;&#26694;&#26550;&#26469;&#25512;&#26029;&#19968;&#20010;&#26102;&#38388;&#21442;&#25968;&#21270;&#30340;&#38543;&#26426;&#30697;&#38453;&#65288;&#32771;&#34385;&#30456;&#20851;&#27531;&#22522;&#30340;&#25200;&#21160;&#32467;&#26500;&#29366;&#24577;&#65289;&#21644;&#30456;&#20851;&#30340;Dirichlet&#27169;&#22411;&#65288;&#32771;&#34385;&#34507;&#30333;&#36136;&#22495;&#28436;&#21270;&#36807;&#31243;&#20013;&#30340;&#25554;&#20837;&#21644;&#21024;&#38500;&#65289;&#12290;&#36825;&#20123;&#27169;&#22411;&#20849;&#21516;&#29992;&#20110;&#20272;&#35745;&#39532;&#23572;&#21487;&#22827;&#38142;&#30340;&#20998;&#21270;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
A complete time-parameterized statistical model quantifying the divergent evolution of protein structures in terms of the patterns of conservation of their secondary structures is inferred from a large collection of protein 3D structure alignments. This provides a better alternative to time-parameterized sequence-based models of protein relatedness, that have clear limitations dealing with twilight and midnight zones of sequence relationships. Since protein structures are far more conserved due to the selection pressure directly placed on their function, divergence time estimates can be more accurate when inferred from structures. We use the Bayesian and information-theoretic framework of Minimum Message Length to infer a time-parameterized stochastic matrix (accounting for perturbed structural states of related residues) and associated Dirichlet models (accounting for insertions and deletions during the evolution of protein domains). These are used in concert to estimate the Markov ti
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#38543;&#26426;&#29305;&#24449;&#23398;&#20064;&#19977;&#32500;&#28857;&#20113;&#25968;&#25454;&#30340;&#26059;&#36716;&#19981;&#21464;&#20989;&#25968;&#65292;&#20026;&#26426;&#22120;&#23398;&#20064;&#22312;3D&#28857;&#20113;&#19978;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2308.06271</link><description>&lt;p&gt;
&#26059;&#36716;&#19981;&#21464;&#30340;&#38543;&#26426;&#29305;&#24449;&#20026;3D&#28857;&#20113;&#19978;&#30340;&#26426;&#22120;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Rotation-Invariant Random Features Provide a Strong Baseline for Machine Learning on 3D Point Clouds. (arXiv:2308.06271v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06271
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#38543;&#26426;&#29305;&#24449;&#23398;&#20064;&#19977;&#32500;&#28857;&#20113;&#25968;&#25454;&#30340;&#26059;&#36716;&#19981;&#21464;&#20989;&#25968;&#65292;&#20026;&#26426;&#22120;&#23398;&#20064;&#22312;3D&#28857;&#20113;&#19978;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26059;&#36716;&#19981;&#21464;&#24615;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#35768;&#22810;&#39046;&#22495;&#20351;&#29992;&#30340;&#19968;&#31181;&#26222;&#36941;&#30340;&#24402;&#32435;&#20559;&#22909;&#65292;&#20363;&#22914;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#37327;&#23376;&#21270;&#23398;&#30340;&#26426;&#22120;&#23398;&#20064;&#12290;&#26059;&#36716;&#19981;&#21464;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#65292;&#21253;&#25324;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#21644;3D&#24418;&#29366;&#20998;&#31867;&#12290;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#35201;&#20040;&#20381;&#36182;&#20110;&#20219;&#21153;&#29305;&#23450;&#30340;&#26059;&#36716;&#19981;&#21464;&#29305;&#24449;&#65292;&#35201;&#20040;&#20351;&#29992;&#22797;&#26434;&#35774;&#35745;&#21644;&#35757;&#32451;&#30340;&#36890;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#36825;&#20123;&#26041;&#27861;&#30340;&#25104;&#21151;&#20027;&#35201;&#26159;&#30001;&#20110;&#26059;&#36716;&#19981;&#21464;&#24615;&#36824;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#36890;&#29992;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#19977;&#32500;&#28857;&#20113;&#25968;&#25454;&#30340;&#26059;&#36716;&#19981;&#21464;&#20989;&#25968;&#65292;&#37319;&#29992;&#20102;&#38543;&#26426;&#29305;&#24449;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#36890;&#36807;&#25512;&#23548;&#20986;&#19968;&#31181;&#23545;&#19977;&#32500;&#26059;&#36716;&#19981;&#21464;&#30340;&#29256;&#26412;&#65292;&#24182;&#35777;&#26126;&#23427;&#22312;&#28857;&#20113;&#25968;&#25454;&#19978;&#30340;&#24555;&#36895;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rotational invariance is a popular inductive bias used by many fields in machine learning, such as computer vision and machine learning for quantum chemistry. Rotation-invariant machine learning methods set the state of the art for many tasks, including molecular property prediction and 3D shape classification. These methods generally either rely on task-specific rotation-invariant features, or they use general-purpose deep neural networks which are complicated to design and train. However, it is unclear whether the success of these methods is primarily due to the rotation invariance or the deep neural networks. To address this question, we suggest a simple and general-purpose method for learning rotation-invariant functions of three-dimensional point cloud data using a random features approach. Specifically, we extend the random features method of Rahimi &amp; Recht 2007 by deriving a version that is invariant to three-dimensional rotations and showing that it is fast to evaluate on point
&lt;/p&gt;</description></item><item><title>DynamicFL&#26159;&#19968;&#20010;&#35299;&#20915;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#36890;&#20449;&#21644;&#26102;&#25928;&#24615;&#38382;&#39064;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#32771;&#34385;&#36793;&#32536;&#35774;&#22791;&#30340;&#36890;&#20449;&#21160;&#24577;&#21644;&#25968;&#25454;&#36136;&#37327;&#65292;&#20197;&#21450;&#37319;&#29992;&#29305;&#27530;&#35774;&#35745;&#30340;&#23458;&#25143;&#31471;&#25805;&#20316;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#27169;&#22411;&#26356;&#26032;&#12290;</title><link>http://arxiv.org/abs/2308.06267</link><description>&lt;p&gt;
DynamicFL&#65306;&#24179;&#34913;&#36890;&#20449;&#21160;&#24577;&#21644;&#23458;&#25143;&#31471;&#25805;&#20316;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DynamicFL: Balancing Communication Dynamics and Client Manipulation for Federated Learning. (arXiv:2308.06267v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06267
&lt;/p&gt;
&lt;p&gt;
DynamicFL&#26159;&#19968;&#20010;&#35299;&#20915;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#36890;&#20449;&#21644;&#26102;&#25928;&#24615;&#38382;&#39064;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#32771;&#34385;&#36793;&#32536;&#35774;&#22791;&#30340;&#36890;&#20449;&#21160;&#24577;&#21644;&#25968;&#25454;&#36136;&#37327;&#65292;&#20197;&#21450;&#37319;&#29992;&#29305;&#27530;&#35774;&#35745;&#30340;&#23458;&#25143;&#31471;&#25805;&#20316;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#27169;&#22411;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#33539;&#20363;&#65292;&#26088;&#22312;&#21033;&#29992;&#25968;&#30334;&#19975;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#20998;&#25955;&#25968;&#25454;&#26469;&#35757;&#32451;&#20840;&#23616;&#27169;&#22411;&#12290;&#19982;&#38598;&#20013;&#24335;&#23398;&#20064;&#30456;&#27604;&#65292;FL&#36890;&#36807;&#36991;&#20813;&#26126;&#30830;&#19979;&#36733;&#23458;&#25143;&#31471;&#25968;&#25454;&#26469;&#20445;&#25252;&#23458;&#25143;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#20013;&#65292;&#30001;&#20110;&#22320;&#29702;&#20998;&#24067;&#30340;&#36793;&#32536;&#35774;&#22791;&#65288;&#22914;&#31227;&#21160;&#35774;&#22791;&#12289;&#27773;&#36710;&#12289;&#28779;&#36710;&#25110;&#22320;&#38081;&#65289;&#30340;&#32593;&#32476;&#39640;&#24230;&#21160;&#24577;&#65292;&#20174;&#21442;&#19982;&#35774;&#22791;&#32858;&#21512;&#25152;&#26377;&#27169;&#22411;&#26356;&#26032;&#23558;&#23548;&#33268;&#19981;&#21487;&#36991;&#20813;&#30340;&#38271;&#23614;&#24310;&#36831;&#12290;&#36825;&#23558;&#20005;&#37325;&#38477;&#20302;&#35757;&#32451;&#36807;&#31243;&#30340;&#25928;&#29575;&#12290;&#20026;&#20102;&#35299;&#20915;FL&#22330;&#26223;&#20013;&#39640;&#31995;&#32479;&#24322;&#36136;&#24615;&#30340;&#26102;&#38388;&#25935;&#24863;&#24615;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;FL&#26694;&#26550;DynamicFL&#65292;&#36890;&#36807;&#32771;&#34385;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#36890;&#20449;&#21160;&#24577;&#21644;&#25968;&#25454;&#36136;&#37327;&#65292;&#24182;&#37319;&#29992;&#29305;&#27530;&#35774;&#35745;&#30340;&#23458;&#25143;&#31471;&#25805;&#20316;&#31574;&#30053;&#65292;\ours&#22522;&#20110;&#21160;&#24577;&#32593;&#32476;&#26465;&#20214;&#21644;&#20808;&#21069;&#30340;&#32593;&#32476;&#39044;&#27979;&#36873;&#25321;&#23458;&#25143;&#31471;&#36827;&#34892;&#27169;&#22411;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a distributed machine learning (ML) paradigm, aiming to train a global model by exploiting the decentralized data across millions of edge devices. Compared with centralized learning, FL preserves the clients' privacy by refraining from explicitly downloading their data. However, given the geo-distributed edge devices (e.g., mobile, car, train, or subway) with highly dynamic networks in the wild, aggregating all the model updates from those participating devices will result in inevitable long-tail delays in FL. This will significantly degrade the efficiency of the training process. To resolve the high system heterogeneity in time-sensitive FL scenarios, we propose a novel FL framework, DynamicFL, by considering the communication dynamics and data quality across massive edge devices with a specially designed client manipulation strategy. \ours actively selects clients for model updating based on the network prediction from its dynamic network conditions and the
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#35843;&#26597;&#20102;&#28201;&#24230;&#21464;&#21270;&#23545;&#32463;&#27982;&#22686;&#38271;&#30340;&#38271;&#26399;&#24433;&#21709;&#65292;&#21457;&#29616;&#24179;&#22343;&#28201;&#24230;&#21644;GDP&#22686;&#38271;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#20851;&#31995;&#65292;&#34920;&#26126;&#27668;&#20505;&#21464;&#21270;&#21487;&#23545;&#32463;&#27982;&#34920;&#29616;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.06265</link><description>&lt;p&gt;
&#28201;&#24230;&#21464;&#21270;&#23545;&#32463;&#27982;&#22686;&#38271;&#30340;&#38271;&#26399;&#24433;&#21709;&#65306;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Long-term Effects of Temperature Variations on Economic Growth: A Machine Learning Approach. (arXiv:2308.06265v1 [econ.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06265
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#35843;&#26597;&#20102;&#28201;&#24230;&#21464;&#21270;&#23545;&#32463;&#27982;&#22686;&#38271;&#30340;&#38271;&#26399;&#24433;&#21709;&#65292;&#21457;&#29616;&#24179;&#22343;&#28201;&#24230;&#21644;GDP&#22686;&#38271;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#20851;&#31995;&#65292;&#34920;&#26126;&#27668;&#20505;&#21464;&#21270;&#21487;&#23545;&#32463;&#27982;&#34920;&#29616;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37319;&#29992;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#35843;&#26597;&#20102;&#28201;&#24230;&#21464;&#21270;&#23545;&#32463;&#27982;&#22686;&#38271;&#30340;&#38271;&#26399;&#24433;&#21709;&#12290;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#20998;&#26512;&#20102;&#26469;&#33258;&#20271;&#20811;&#21033;&#22320;&#29699;&#21644;&#19990;&#30028;&#38134;&#34892;&#30340;&#20840;&#29699;&#38470;&#22320;&#34920;&#38754;&#28201;&#24230;&#25968;&#25454;&#21644;&#32463;&#27982;&#25351;&#26631;&#65292;&#21253;&#25324;GDP&#21644;&#20154;&#21475;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#24179;&#22343;&#28201;&#24230;&#21644;GDP&#22686;&#38271;&#20043;&#38388;&#30340;&#26174;&#33879;&#20851;&#31995;&#65292;&#34920;&#26126;&#27668;&#20505;&#21464;&#21270;&#21487;&#20197;&#22823;&#24133;&#24433;&#21709;&#32463;&#27982;&#34920;&#29616;&#12290;&#35813;&#30740;&#31350;&#24378;&#35843;&#20102;&#23558;&#27668;&#20505;&#22240;&#32032;&#32435;&#20837;&#32463;&#27982;&#35268;&#21010;&#21644;&#25919;&#31574;&#21046;&#23450;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#25581;&#31034;&#27668;&#20505;&#32463;&#27982;&#30740;&#31350;&#20013;&#22797;&#26434;&#20851;&#31995;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study investigates the long-term effects of temperature variations on economic growth using a data-driven approach. Leveraging machine learning techniques, we analyze global land surface temperature data from Berkeley Earth and economic indicators, including GDP and population data, from the World Bank. Our analysis reveals a significant relationship between average temperature and GDP growth, suggesting that climate variations can substantially impact economic performance. This research underscores the importance of incorporating climate factors into economic planning and policymaking, and it demonstrates the utility of machine learning in uncovering complex relationships in climate-economy studies.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#26159;&#39318;&#27425;&#25506;&#32034;&#22522;&#20110;&#23618;&#27425;&#20869;&#23384;&#22238;&#25918;&#30340;&#25345;&#32493;&#23398;&#20064;&#30340;&#35774;&#35745;&#31354;&#38388;&#65292;&#26088;&#22312;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#29616;&#25104;&#26412;&#25928;&#30410;&#12290;&#25552;&#20986;&#20102;Miro&#65292;&#19968;&#20010;&#36890;&#36807;&#21160;&#24577;&#37197;&#32622;&#25345;&#32493;&#23398;&#20064;&#31995;&#32479;&#30340;&#26032;&#39062;&#31995;&#32479;&#36816;&#34892;&#26102;&#65292;&#20197;&#23454;&#29616;&#26368;&#20339;&#30340;&#25104;&#26412;&#25928;&#30410;&#12290;&#24191;&#27867;&#30340;&#35780;&#20272;&#26174;&#31034;Miro&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2308.06053</link><description>&lt;p&gt;
&#22312;&#20869;&#23384;&#23618;&#27425;&#32467;&#26500;&#19978;&#20855;&#26377;MiRo&#30340;&#25104;&#26412;&#25928;&#30410;&#30340;&#35774;&#22791;&#19978;&#30340;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cost-effective On-device Continual Learning over Memory Hierarchy with Miro. (arXiv:2308.06053v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06053
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#26159;&#39318;&#27425;&#25506;&#32034;&#22522;&#20110;&#23618;&#27425;&#20869;&#23384;&#22238;&#25918;&#30340;&#25345;&#32493;&#23398;&#20064;&#30340;&#35774;&#35745;&#31354;&#38388;&#65292;&#26088;&#22312;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#29616;&#25104;&#26412;&#25928;&#30410;&#12290;&#25552;&#20986;&#20102;Miro&#65292;&#19968;&#20010;&#36890;&#36807;&#21160;&#24577;&#37197;&#32622;&#25345;&#32493;&#23398;&#20064;&#31995;&#32479;&#30340;&#26032;&#39062;&#31995;&#32479;&#36816;&#34892;&#26102;&#65292;&#20197;&#23454;&#29616;&#26368;&#20339;&#30340;&#25104;&#26412;&#25928;&#30410;&#12290;&#24191;&#27867;&#30340;&#35780;&#20272;&#26174;&#31034;Miro&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#26159;&#20174;&#25345;&#32493;&#30340;&#20219;&#21153;&#27969;&#20013;&#36880;&#27493;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#20026;&#20102;&#35760;&#20303;&#20808;&#21069;&#23398;&#21040;&#30340;&#30693;&#35782;&#65292;&#20043;&#21069;&#30340;&#30740;&#31350;&#23558;&#26087;&#26679;&#26412;&#23384;&#20648;&#22312;&#19968;&#20010;&#20869;&#23384;&#23618;&#27425;&#32467;&#26500;&#20013;&#65292;&#24182;&#22312;&#26032;&#20219;&#21153;&#21040;&#26469;&#26102;&#36827;&#34892;&#22238;&#25918;&#12290;&#37319;&#29992;&#25345;&#32493;&#23398;&#20064;&#20197;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#36793;&#32536;&#35774;&#22791;&#36890;&#24120;&#23545;&#33021;&#28304;&#25935;&#24863;&#65292;&#22240;&#27492;&#38656;&#35201;&#22312;&#19981;&#25439;&#23475;&#33021;&#28304;&#25928;&#29575;&#30340;&#24773;&#20917;&#19979;&#20445;&#25345;&#39640;&#27169;&#22411;&#20934;&#30830;&#24230;&#65292;&#21363;&#25104;&#26412;&#25928;&#30410;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26159;&#39318;&#27425;&#25506;&#32034;&#22522;&#20110;&#23618;&#27425;&#20869;&#23384;&#22238;&#25918;&#30340;&#25345;&#32493;&#23398;&#20064;&#30340;&#35774;&#35745;&#31354;&#38388;&#65292;&#20197;&#33719;&#24471;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#25104;&#26412;&#25928;&#30410;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Miro&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#31995;&#32479;&#36816;&#34892;&#26102;&#65292;&#36890;&#36807;&#20351;&#20854;&#33021;&#22815;&#26681;&#25454;&#36164;&#28304;&#29366;&#24577;&#21160;&#24577;&#37197;&#32622;&#25345;&#32493;&#23398;&#20064;&#31995;&#32479;&#65292;&#20174;&#32780;&#23558;&#25105;&#20204;&#30340;&#35265;&#35299;&#31934;&#30830;&#22320;&#25972;&#21512;&#21040;&#25345;&#32493;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#20197;&#23454;&#29616;&#26368;&#20339;&#25104;&#26412;&#25928;&#30410;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;Miro&#36824;&#23545;&#24102;&#26377;&#26126;&#30830;&#20934;&#30830;&#24230;-&#33021;&#37327;&#24179;&#34913;&#30340;&#21442;&#25968;&#36827;&#34892;&#22312;&#32447;&#20998;&#26512;&#65292;&#24182;&#20197;&#20302;&#24320;&#38144;&#22320;&#36866;&#24212;&#26368;&#20339;&#20540;&#12290;&#24191;&#27867;&#30340;&#35780;&#20272;&#26174;&#31034;Miro&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning (CL) trains NN models incrementally from a continuous stream of tasks. To remember previously learned knowledge, prior studies store old samples over a memory hierarchy and replay them when new tasks arrive. Edge devices that adopt CL to preserve data privacy are typically energy-sensitive and thus require high model accuracy while not compromising energy efficiency, i.e., cost-effectiveness. Our work is the first to explore the design space of hierarchical memory replay-based CL to gain insights into achieving cost-effectiveness on edge devices. We present Miro, a novel system runtime that carefully integrates our insights into the CL framework by enabling it to dynamically configure the CL system based on resource states for the best cost-effectiveness. To reach this goal, Miro also performs online profiling on parameters with clear accuracy-energy trade-offs and adapts to optimal values with low overhead. Extensive evaluations show that Miro significantly outperfo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#25361;&#25112;&#24615;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#23545;&#29616;&#26377;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20462;&#25913;&#65292;&#20197;&#26356;&#22909;&#22320;&#35780;&#20272;CRL&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.05707</link><description>&lt;p&gt;
&#38544;&#24418;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#30340;&#26032;&#25361;&#25112;&#24615;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Shadow Datasets, New challenging datasets for Causal Representation Learning. (arXiv:2308.05707v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05707
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#25361;&#25112;&#24615;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#23545;&#29616;&#26377;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20462;&#25913;&#65292;&#20197;&#26356;&#22909;&#22320;&#35780;&#20272;CRL&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#34920;&#31034;&#23398;&#20064;&#20013;&#65292;&#21457;&#29616;&#35821;&#20041;&#22240;&#32032;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#26159;&#19968;&#20010;&#26032;&#20852;&#30340;&#35805;&#39064;&#12290;&#22823;&#22810;&#25968;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#65288;CRL&#65289;&#26041;&#27861;&#37117;&#26159;&#23436;&#20840;&#30417;&#30563;&#30340;&#65292;&#30001;&#20110;&#26631;&#35760;&#25104;&#26412;&#39640;&#26114;&#32780;&#19981;&#20999;&#23454;&#38469;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#24341;&#20837;&#20102;&#24369;&#30417;&#30563;&#30340;CRL&#26041;&#27861;&#12290;&#20026;&#20102;&#35780;&#20272;CRL&#24615;&#33021;&#65292;&#20351;&#29992;&#20102;&#22235;&#20010;&#29616;&#26377;&#25968;&#25454;&#38598;&#65306;Pendulum&#12289;Flow&#12289;CelebA&#65288;BEARD&#65289;&#21644;CelebA&#65288;SMILE&#65289;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;CRL&#25968;&#25454;&#38598;&#20165;&#38480;&#20110;&#20855;&#26377;&#23569;&#37327;&#29983;&#25104;&#22240;&#32032;&#30340;&#31616;&#21333;&#22270;&#24418;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#26356;&#22810;&#31181;&#31867;&#30340;&#29983;&#25104;&#22240;&#32032;&#21644;&#26356;&#22797;&#26434;&#30340;&#22240;&#26524;&#22270;&#12290;&#27492;&#22806;&#65292;&#22312;&#24403;&#21069;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;CelebA&#65288;BEARD&#65289;&#21644;CelebA&#65288;SMILE&#65289;&#20013;&#65292;&#26368;&#21021;&#25552;&#20986;&#30340;&#22240;&#26524;&#22270;&#19982;&#25968;&#25454;&#38598;&#20998;&#24067;&#19981;&#19968;&#33268;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#23427;&#20204;&#30340;&#20462;&#25913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discovering causal relations among semantic factors is an emergent topic in representation learning. Most causal representation learning (CRL) methods are fully supervised, which is impractical due to costly labeling. To resolve this restriction, weakly supervised CRL methods were introduced. To evaluate CRL performance, four existing datasets, Pendulum, Flow, CelebA(BEARD) and CelebA(SMILE), are utilized. However, existing CRL datasets are limited to simple graphs with few generative factors. Thus we propose two new datasets with a larger number of diverse generative factors and more sophisticated causal graphs. In addition, current real datasets, CelebA(BEARD) and CelebA(SMILE), the originally proposed causal graphs are not aligned with the dataset distributions. Thus, we propose modifications to them.
&lt;/p&gt;</description></item><item><title>Kairos&#26159;&#31532;&#19968;&#20010;&#21516;&#26102;&#28385;&#36275;&#33539;&#22260;&#12289;&#25915;&#20987;&#19981;&#21487;&#30693;&#24615;&#12289;&#26102;&#25928;&#24615;&#21644;&#25915;&#20987;&#37325;&#24314;&#32500;&#24230;&#35201;&#27714;&#30340;&#28335;&#28304;&#20026;&#22522;&#30784;&#30340;&#20837;&#20405;&#26816;&#27979;&#21644;&#35843;&#26597;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2308.05034</link><description>&lt;p&gt;
Kairos: &#20351;&#29992;&#25972;&#20307;&#31995;&#32479;&#28335;&#28304;&#36827;&#34892;&#23454;&#29992;&#30340;&#20837;&#20405;&#26816;&#27979;&#21644;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Kairos: : Practical Intrusion Detection and Investigation using Whole-system Provenance. (arXiv:2308.05034v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05034
&lt;/p&gt;
&lt;p&gt;
Kairos&#26159;&#31532;&#19968;&#20010;&#21516;&#26102;&#28385;&#36275;&#33539;&#22260;&#12289;&#25915;&#20987;&#19981;&#21487;&#30693;&#24615;&#12289;&#26102;&#25928;&#24615;&#21644;&#25915;&#20987;&#37325;&#24314;&#32500;&#24230;&#35201;&#27714;&#30340;&#28335;&#28304;&#20026;&#22522;&#30784;&#30340;&#20837;&#20405;&#26816;&#27979;&#21644;&#35843;&#26597;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28335;&#28304;&#22270;&#26159;&#25551;&#36848;&#31995;&#32479;&#25191;&#34892;&#21382;&#21490;&#30340;&#32467;&#26500;&#21270;&#23457;&#35745;&#26085;&#24535;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#21508;&#31181;&#25216;&#26415;&#26469;&#20998;&#26512;&#28335;&#28304;&#22270;&#65292;&#20197;&#23454;&#29616;&#33258;&#21160;&#21270;&#20027;&#26426;&#20837;&#20405;&#26816;&#27979;&#65292;&#29305;&#21035;&#20851;&#27880;&#39640;&#32423;&#25345;&#20037;&#24615;&#23041;&#32961;&#12290;&#36890;&#36807;&#30740;&#31350;&#20854;&#35774;&#35745;&#25991;&#26723;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22235;&#20010;&#24120;&#35265;&#32500;&#24230;&#65292;&#25512;&#21160;&#28335;&#28304;&#20026;&#22522;&#30784;&#30340;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65288;PIDS&#65289;&#30340;&#21457;&#23637;&#65306;&#33539;&#22260;&#65288;PIDS&#33021;&#21542;&#26816;&#27979;&#36328;&#24212;&#29992;&#36793;&#30028;&#28183;&#36879;&#30340;&#29616;&#20195;&#25915;&#20987;&#65311;&#65289;&#12289;&#25915;&#20987;&#19981;&#21487;&#30693;&#24615;&#65288;PIDS&#33021;&#21542;&#22312;&#27809;&#26377;&#25915;&#20987;&#29305;&#24449;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#26032;&#22411;&#25915;&#20987;&#65311;&#65289;&#12289;&#26102;&#25928;&#24615;&#65288;PIDS&#33021;&#21542;&#39640;&#25928;&#30417;&#35270;&#20027;&#26426;&#31995;&#32479;&#36816;&#34892;&#65311;&#65289;&#21644;&#25915;&#20987;&#37325;&#24314;&#65288;PIDS&#33021;&#21542;&#20174;&#22823;&#22411;&#28335;&#28304;&#22270;&#20013;&#25552;&#28860;&#25915;&#20987;&#27963;&#21160;&#65292;&#20197;&#20415;&#31995;&#32479;&#31649;&#29702;&#21592;&#33021;&#22815;&#36731;&#26494;&#29702;&#35299;&#24182;&#36805;&#36895;&#24212;&#23545;&#31995;&#32479;&#20837;&#20405;&#65311;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;KAIROS&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#21516;&#26102;&#28385;&#36275;&#25152;&#26377;&#22235;&#20010;&#32500;&#24230;&#35201;&#27714;&#30340;PIDS&#65292;&#32780;&#29616;&#26377;&#30340;&#26041;&#27861;&#19981;&#33021;&#20570;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Provenance graphs are structured audit logs that describe the history of a system's execution. Recent studies have explored a variety of techniques to analyze provenance graphs for automated host intrusion detection, focusing particularly on advanced persistent threats. Sifting through their design documents, we identify four common dimensions that drive the development of provenance-based intrusion detection systems (PIDSes): scope (can PIDSes detect modern attacks that infiltrate across application boundaries?), attack agnosticity (can PIDSes detect novel attacks without a priori knowledge of attack characteristics?), timeliness (can PIDSes efficiently monitor host systems as they run?), and attack reconstruction (can PIDSes distill attack activity from large provenance graphs so that sysadmins can easily understand and quickly respond to system intrusion?). We present KAIROS, the first PIDS that simultaneously satisfies the desiderata in all four dimensions, whereas existing approac
&lt;/p&gt;</description></item><item><title>&#35813;&#25253;&#21578;&#20851;&#27880;&#24403;&#21069;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;AI&#35770;&#25991;&#65292;&#24182;&#20197;&#26631;&#20934;&#21270;&#24341;&#29992;&#35745;&#25968;&#20026;&#20381;&#25454;&#32534;&#21046;&#20102;40&#31687;&#26368;&#21463;&#27426;&#36814;&#30340;&#35770;&#25991;&#21015;&#34920;&#12290;&#35266;&#23519;&#21040;&#22312;2023&#24180;&#19978;&#21322;&#24180;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#20855;&#20307;&#32780;&#35328;&#30340;ChatGPT&#30456;&#20851;&#30340;&#35770;&#25991;&#21344;&#20027;&#23548;&#22320;&#20301;&#65292;ChatGPT&#34920;&#29616;&#20986;&#19979;&#38477;&#30340;&#36235;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.04889</link><description>&lt;p&gt;
NLLG&#23395;&#24230;arXiv&#25253;&#21578; 06/23&#65306;&#24403;&#21069;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;AI&#35770;&#25991;&#26159;&#20160;&#20040;&#65311;&#65288;arXiv:2308.04889v1 [cs.CY]&#65289;
&lt;/p&gt;
&lt;p&gt;
NLLG Quarterly arXiv Report 06/23: What are the most influential current AI Papers?. (arXiv:2308.04889v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04889
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25253;&#21578;&#20851;&#27880;&#24403;&#21069;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;AI&#35770;&#25991;&#65292;&#24182;&#20197;&#26631;&#20934;&#21270;&#24341;&#29992;&#35745;&#25968;&#20026;&#20381;&#25454;&#32534;&#21046;&#20102;40&#31687;&#26368;&#21463;&#27426;&#36814;&#30340;&#35770;&#25991;&#21015;&#34920;&#12290;&#35266;&#23519;&#21040;&#22312;2023&#24180;&#19978;&#21322;&#24180;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#20855;&#20307;&#32780;&#35328;&#30340;ChatGPT&#30456;&#20851;&#30340;&#35770;&#25991;&#21344;&#20027;&#23548;&#22320;&#20301;&#65292;ChatGPT&#34920;&#29616;&#20986;&#19979;&#38477;&#30340;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#39046;&#22495;&#20013;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;Generative Artificial Intelligence&#65292;&#29305;&#21035;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;Natural Language Processing&#65292;NLP&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;Machine Learning&#65292;ML&#65289;&#65289;&#20449;&#24687;&#30340;&#24555;&#36895;&#22686;&#38271;&#32473;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#25361;&#25112;&#65292;&#20351;&#24471;&#20182;&#20204;&#38590;&#20197;&#36319;&#19978;&#26368;&#26032;&#30340;&#21457;&#23637;&#12290;&#20026;&#20102;&#35299;&#20915;&#20449;&#24687;&#36807;&#36733;&#30340;&#38382;&#39064;&#65292;Bielefeld&#22823;&#23398;&#30340;&#33258;&#28982;&#35821;&#35328;&#23398;&#20064;&#32452;&#22312;&#26412;&#25253;&#21578;&#20013;&#19987;&#27880;&#20110;&#35782;&#21035;arXiv&#19978;&#26368;&#21463;&#27426;&#36814;&#30340;&#35770;&#25991;&#65292;&#29305;&#21035;&#20851;&#27880;NLP&#21644;ML&#12290;&#20854;&#30446;&#26631;&#26159;&#20026;&#26368;&#30456;&#20851;&#19988;&#34987;&#24191;&#27867;&#35752;&#35770;&#30340;&#30740;&#31350;&#25552;&#20379;&#24555;&#36895;&#25351;&#21335;&#65292;&#20197;&#24110;&#21161;&#26032;&#26469;&#32773;&#21644;&#24050;&#26377;&#30740;&#31350;&#20154;&#21592;&#36319;&#19978;&#24403;&#21069;&#36235;&#21183;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26681;&#25454;2023&#24180;&#19978;&#21322;&#24180;&#30340;&#26631;&#20934;&#21270;&#24341;&#29992;&#35745;&#25968;&#32534;&#21046;&#20102;&#19968;&#20010;&#30001;40&#31687;&#26368;&#21463;&#27426;&#36814;&#30340;&#35770;&#25991;&#32452;&#25104;&#30340;&#21015;&#34920;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;2023&#24180;&#19978;&#21322;&#24180;&#65292;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;Large Language Models&#65292;LLMs&#65289;&#21644;&#20855;&#20307;&#32780;&#35328;&#30340;ChatGPT&#30456;&#20851;&#30340;&#35770;&#25991;&#21344;&#20027;&#23548;&#22320;&#20301;&#65292;&#32780;ChatGPT&#26174;&#31034;&#20986;&#19979;&#38477;&#30340;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid growth of information in the field of Generative Artificial Intelligence (AI), particularly in the subfields of Natural Language Processing (NLP) and Machine Learning (ML), presents a significant challenge for researchers and practitioners to keep pace with the latest developments. To address the problem of information overload, this report by the Natural Language Learning Group at Bielefeld University focuses on identifying the most popular papers on arXiv, with a specific emphasis on NLP and ML. The objective is to offer a quick guide to the most relevant and widely discussed research, aiding both newcomers and established researchers in staying abreast of current trends. In particular, we compile a list of the 40 most popular papers based on normalized citation counts from the first half of 2023. We observe the dominance of papers related to Large Language Models (LLMs) and specifically ChatGPT during the first half of 2023, with the latter showing signs of declining popul
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#38544;&#24335;&#26694;&#26550;&#65292;&#21487;&#20197;&#24555;&#36895;&#21512;&#25104;&#21644;&#28210;&#26579;NeRF&#23545;&#35937;&#12290;&#36890;&#36807;&#24341;&#20837;&#31070;&#32463;&#28145;&#24230;&#22330;&#30340;&#26032;&#30340;&#34920;&#38754;&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#21160;&#24577;&#38452;&#24433;&#24182;&#20801;&#35768;&#22810;&#20010;NeRF&#23545;&#35937;&#20197;&#20219;&#24847;&#21018;&#20307;&#21464;&#25442;&#26080;&#32541;&#22320;&#25918;&#32622;&#21644;&#28210;&#26579;&#22312;&#19968;&#36215;&#12290;</title><link>http://arxiv.org/abs/2308.04669</link><description>&lt;p&gt;
&#24555;&#36895;NeRF&#21512;&#25104;&#21644;&#28210;&#26579;&#30340;&#36890;&#29992;&#38544;&#24335;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A General Implicit Framework for Fast NeRF Composition and Rendering. (arXiv:2308.04669v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04669
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#38544;&#24335;&#26694;&#26550;&#65292;&#21487;&#20197;&#24555;&#36895;&#21512;&#25104;&#21644;&#28210;&#26579;NeRF&#23545;&#35937;&#12290;&#36890;&#36807;&#24341;&#20837;&#31070;&#32463;&#28145;&#24230;&#22330;&#30340;&#26032;&#30340;&#34920;&#38754;&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#21160;&#24577;&#38452;&#24433;&#24182;&#20801;&#35768;&#22810;&#20010;NeRF&#23545;&#35937;&#20197;&#20219;&#24847;&#21018;&#20307;&#21464;&#25442;&#26080;&#32541;&#22320;&#25918;&#32622;&#21644;&#28210;&#26579;&#22312;&#19968;&#36215;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#21508;&#31181;&#31070;&#32463;&#36752;&#23556;&#22330;&#26041;&#27861;&#22312;&#39640;&#28210;&#26579;&#36895;&#24230;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#21152;&#36895;&#26041;&#27861;&#19987;&#38376;&#21270;&#24182;&#19988;&#19981;&#36866;&#29992;&#20110;&#21508;&#31181;&#38544;&#24335;&#26041;&#27861;&#65292;&#36825;&#38459;&#30861;&#20102;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;NeRF&#20316;&#21697;&#36827;&#34892;&#23454;&#26102;&#21512;&#25104;&#12290;&#30001;&#20110;NeRF&#20381;&#36182;&#20110;&#27839;&#20809;&#32447;&#37319;&#26679;&#65292;&#22240;&#27492;&#21487;&#20197;&#25552;&#20379;&#19968;&#33324;&#24615;&#30340;&#25351;&#23548;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#38544;&#24335;&#27969;&#27700;&#32447;&#26469;&#24555;&#36895;&#21512;&#25104;NeRF&#23545;&#35937;&#12290;&#36825;&#31181;&#26032;&#26041;&#27861;&#20351;&#24471;&#21160;&#24577;&#38452;&#24433;&#21487;&#20197;&#20351;&#29992;&#35299;&#26512;&#20809;&#28304;&#22312;&#23545;&#35937;&#20869;&#37096;&#25110;&#23545;&#35937;&#20043;&#38388;&#36827;&#34892;&#25237;&#23556;&#65292;&#21516;&#26102;&#20801;&#35768;&#22810;&#20010;NeRF&#23545;&#35937;&#20197;&#20219;&#24847;&#21018;&#20307;&#21464;&#25442;&#26080;&#32541;&#22320;&#25918;&#32622;&#21644;&#28210;&#26579;&#22312;&#19968;&#36215;&#12290;&#20027;&#35201;&#22320;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#31070;&#32463;&#28145;&#24230;&#22330;&#65288;NeDF&#65289;&#30340;&#26032;&#30340;&#34920;&#38754;&#34920;&#31034;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20801;&#35768;&#20809;&#32447;&#21644;&#38544;&#24335;&#34920;&#38754;&#20043;&#38388;&#30340;&#30452;&#25509;&#30456;&#20132;&#35745;&#31639;&#26469;&#24555;&#36895;&#30830;&#23450;&#23545;&#35937;&#20043;&#38388;&#30340;&#31354;&#38388;&#20851;&#31995;&#12290;&#23427;&#21033;&#29992;&#19968;&#20010;&#20132;&#28857;&#31070;&#32463;&#32593;&#32476;&#26469;&#21152;&#36895;&#26597;&#35810;NeRF&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, a variety of Neural radiance fields methods have garnered remarkable success in high render speed. However, current accelerating methods is specialized and not compatible for various implicit method, which prevent a real-time composition over different kinds of NeRF works. Since NeRF relies on sampling along rays, it's possible to provide a guidance generally. We propose a general implicit pipeline to rapidly compose NeRF objects. This new method enables the casting of dynamic shadows within or between objects using analytical light sources while allowing multiple NeRF objects to be seamlessly placed and rendered together with any arbitrary rigid transformations. Mainly, our work introduces a new surface representation known as Neural Depth Fields (NeDF) that quickly determines the spatial relationship between objects by allowing direct intersection computation between rays and implicit surfaces. It leverages an intersection neural network to query NeRF for acceleration inste
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25193;&#23637;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#20351;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;BDCM&#65292;&#21487;&#20197;&#22312;&#23384;&#22312;&#26080;&#27861;&#27979;&#37327;&#30340;&#28151;&#28102;&#22240;&#32032;&#30340;&#24773;&#20917;&#19979;&#26356;&#20934;&#30830;&#22320;&#22238;&#31572;&#22240;&#26524;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.03669</link><description>&lt;p&gt;
&#26080;&#27861;&#27979;&#37327;&#28151;&#28102;&#22240;&#32032;&#19979;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Diffusion Model in Causal Inference with Unmeasured Confounders. (arXiv:2308.03669v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03669
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25193;&#23637;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#20351;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;BDCM&#65292;&#21487;&#20197;&#22312;&#23384;&#22312;&#26080;&#27861;&#27979;&#37327;&#30340;&#28151;&#28102;&#22240;&#32032;&#30340;&#24773;&#20917;&#19979;&#26356;&#20934;&#30830;&#22320;&#22238;&#31572;&#22240;&#26524;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#26080;&#27861;&#27979;&#37327;&#30340;&#28151;&#28102;&#22240;&#32032;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#25193;&#23637;&#25193;&#25955;&#27169;&#22411;&#30340;&#20351;&#29992;&#65292;&#20197;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#22238;&#31572;&#22240;&#26524;&#38382;&#39064;&#12290;&#22312;Pearl&#30340;&#20351;&#29992;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAG&#65289;&#25429;&#25417;&#22240;&#26524;&#24178;&#39044;&#30340;&#26694;&#26550;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#22240;&#26524;&#27169;&#22411;&#65288;DCM&#65289;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#22238;&#31572;&#22240;&#26524;&#38382;&#39064;&#65292;&#20551;&#35774;&#25152;&#26377;&#28151;&#28102;&#22240;&#32032;&#37117;&#26159;&#21487;&#20197;&#35266;&#23519;&#21040;&#30340;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#20013;&#23384;&#22312;&#26080;&#27861;&#27979;&#37327;&#30340;&#28151;&#28102;&#22240;&#32032;&#65292;&#36825;&#20351;&#24471;DCM&#26080;&#27861;&#24212;&#29992;&#12290;&#20026;&#20102;&#32531;&#35299;DCM&#30340;&#36825;&#19968;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25193;&#23637;&#27169;&#22411;&#65292;&#31216;&#20026;&#22522;&#20110;&#21453;&#38376;&#20934;&#21017;&#30340;DCM&#65288;BDCM&#65289;&#65292;&#20854;&#24605;&#24819;&#26681;&#26893;&#20110;&#22312;DAG&#20013;&#25214;&#21040;&#35201;&#21253;&#25324;&#22312;&#25193;&#25955;&#27169;&#22411;&#35299;&#30721;&#36807;&#31243;&#20013;&#30340;&#21464;&#37327;&#30340;&#21453;&#38376;&#20934;&#21017;&#65292;&#36825;&#26679;&#25105;&#20204;&#21487;&#20197;&#23558;DCM&#25193;&#23637;&#21040;&#23384;&#22312;&#26080;&#27861;&#27979;&#37327;&#30340;&#28151;&#28102;&#22240;&#32032;&#30340;&#24773;&#20917;&#12290;&#21512;&#25104;&#25968;&#25454;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#26080;&#27861;&#27979;&#37327;&#28151;&#28102;&#22240;&#32032;&#30340;&#24773;&#20917;&#19979;&#26356;&#31934;&#30830;&#22320;&#25429;&#25417;&#21040;&#20102;&#21453;&#20107;&#23454;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study how to extend the use of the diffusion model to answer the causal question from the observational data under the existence of unmeasured confounders. In Pearl's framework of using a Directed Acyclic Graph (DAG) to capture the causal intervention, a Diffusion-based Causal Model (DCM) was proposed incorporating the diffusion model to answer the causal questions more accurately, assuming that all of the confounders are observed. However, unmeasured confounders in practice exist, which hinders DCM from being applicable. To alleviate this limitation of DCM, we propose an extended model called Backdoor Criterion based DCM (BDCM), whose idea is rooted in the Backdoor criterion to find the variables in DAG to be included in the decoding process of the diffusion model so that we can extend DCM to the case with unmeasured confounders. Synthetic data experiment demonstrates that our proposed model captures the counterfactual distribution more precisely than DCM under the unmeasured confo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32593;&#32476;&#31616;&#21270;&#25216;&#26415;&#65292;&#22312;&#39564;&#35777;&#20043;&#21069;&#23545;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#39044;&#22788;&#29702;&#65292;&#36890;&#36807;&#28040;&#38500;&#31283;&#23450;&#30340;ReLU&#31070;&#32463;&#20803;&#65292;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;&#30001;ReLU&#21644;&#20223;&#23556;&#23618;&#32452;&#25104;&#30340;&#39034;&#24207;&#31070;&#32463;&#32593;&#32476;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23567;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#35268;&#27169;&#24182;&#21152;&#36895;&#20102;&#29616;&#26377;&#30340;&#39564;&#35777;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2308.03330</link><description>&lt;p&gt;
&#36890;&#36807;&#32593;&#32476;&#31616;&#21270;&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Expediting Neural Network Verification via Network Reduction. (arXiv:2308.03330v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03330
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32593;&#32476;&#31616;&#21270;&#25216;&#26415;&#65292;&#22312;&#39564;&#35777;&#20043;&#21069;&#23545;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#39044;&#22788;&#29702;&#65292;&#36890;&#36807;&#28040;&#38500;&#31283;&#23450;&#30340;ReLU&#31070;&#32463;&#20803;&#65292;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;&#30001;ReLU&#21644;&#20223;&#23556;&#23618;&#32452;&#25104;&#30340;&#39034;&#24207;&#31070;&#32463;&#32593;&#32476;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23567;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#35268;&#27169;&#24182;&#21152;&#36895;&#20102;&#29616;&#26377;&#30340;&#39564;&#35777;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#39564;&#35777;&#26041;&#27861;&#26469;&#39564;&#35777;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23433;&#20840;&#24615;&#65292;&#20197;&#30830;&#20445;&#32593;&#32476;&#22312;&#20851;&#38190;&#24212;&#29992;&#20013;&#27491;&#30830;&#36816;&#34892;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#20247;&#25152;&#21608;&#30693;&#30340;&#39564;&#35777;&#24037;&#20855;&#22312;&#22797;&#26434;&#30340;&#32593;&#32476;&#26550;&#26500;&#21644;&#22823;&#22411;&#32593;&#32476;&#22823;&#23567;&#19978;&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32593;&#32476;&#31616;&#21270;&#25216;&#26415;&#20316;&#20026;&#39564;&#35777;&#20043;&#21069;&#30340;&#39044;&#22788;&#29702;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36890;&#36807;&#28040;&#38500;&#31283;&#23450;&#30340;ReLU&#31070;&#32463;&#20803;&#65292;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;&#30001;ReLU&#21644;&#20223;&#23556;&#23618;&#32452;&#25104;&#30340;&#39034;&#24207;&#31070;&#32463;&#32593;&#32476;&#65292;&#20174;&#32780;&#21487;&#20197;&#36890;&#36807;&#22823;&#22810;&#25968;&#39564;&#35777;&#24037;&#20855;&#22788;&#29702;&#12290;&#25105;&#20204;&#22312;&#26368;&#20808;&#36827;&#30340;&#23436;&#25972;&#21644;&#19981;&#23436;&#25972;&#39564;&#35777;&#24037;&#20855;&#19978;&#23454;&#29616;&#20102;&#31616;&#21270;&#25216;&#26415;&#65292;&#21253;&#25324;alpha-beta-crown&#65292;VeriNet&#21644;PRIMA&#12290;&#25105;&#20204;&#22312;&#22823;&#37327;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#25216;&#26415;&#21487;&#20197;&#26174;&#33879;&#20943;&#23567;&#31070;&#32463;&#32593;&#32476;&#24182;&#21152;&#36895;&#29616;&#26377;&#30340;&#39564;&#35777;&#24037;&#20855;&#12290;&#27492;&#22806;&#65292;&#23454;&#39564;&#32467;&#26524;&#36824;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
A wide range of verification methods have been proposed to verify the safety properties of deep neural networks ensuring that the networks function correctly in critical applications. However, many well-known verification tools still struggle with complicated network architectures and large network sizes. In this work, we propose a network reduction technique as a pre-processing method prior to verification. The proposed method reduces neural networks via eliminating stable ReLU neurons, and transforming them into a sequential neural network consisting of ReLU and Affine layers which can be handled by the most verification tools. We instantiate the reduction technique on the state-of-the-art complete and incomplete verification tools, including alpha-beta-crown, VeriNet and PRIMA. Our experiments on a large set of benchmarks indicate that the proposed technique can significantly reduce neural networks and speed up existing verification tools. Furthermore, the experiment results also sh
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#30340;&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#20219;&#21153;&#65292;&#26088;&#22312;&#35299;&#20915;&#22312;&#36866;&#24212;&#36807;&#31243;&#20013;&#26080;&#27861;&#35775;&#38382;&#28304;&#25968;&#25454;&#30340;&#36328;&#22495;&#23398;&#20064;&#25361;&#25112;&#12290;&#36890;&#36807;&#25552;&#20986;&#30340;&#26032;&#26694;&#26550;&#65292;&#28304;&#20445;&#25252;&#27169;&#22359;&#26356;&#26377;&#25928;&#22320;&#20445;&#30041;&#28304;&#20449;&#24687;&#24182;&#25269;&#25239;&#22122;&#22768;&#12290;</title><link>http://arxiv.org/abs/2308.03202</link><description>&lt;p&gt;
&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#30340;&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Source-free Domain Adaptive Human Pose Estimation. (arXiv:2308.03202v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03202
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#30340;&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#20219;&#21153;&#65292;&#26088;&#22312;&#35299;&#20915;&#22312;&#36866;&#24212;&#36807;&#31243;&#20013;&#26080;&#27861;&#35775;&#38382;&#28304;&#25968;&#25454;&#30340;&#36328;&#22495;&#23398;&#20064;&#25361;&#25112;&#12290;&#36890;&#36807;&#25552;&#20986;&#30340;&#26032;&#26694;&#26550;&#65292;&#28304;&#20445;&#25252;&#27169;&#22359;&#26356;&#26377;&#25928;&#22320;&#20445;&#30041;&#28304;&#20449;&#24687;&#24182;&#25269;&#25239;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#24191;&#27867;&#24212;&#29992;&#20110;&#36816;&#21160;&#20998;&#26512;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#34394;&#25311;&#29616;&#23454;&#31561;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#26631;&#27880;&#23454;&#38469;&#22330;&#26223;&#30340;&#25968;&#25454;&#38598;&#30340;&#24040;&#22823;&#24320;&#38144;&#23545;&#23039;&#21183;&#20272;&#35745;&#26500;&#25104;&#20102;&#37325;&#35201;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#19968;&#31181;&#26041;&#27861;&#26159;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#23039;&#21183;&#20272;&#35745;&#27169;&#22411;&#65292;&#28982;&#21518;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#19978;&#36827;&#34892;&#22495;&#33258;&#36866;&#24212;(DA)&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;HPE&#30340;DA&#26041;&#27861;&#22312;&#36866;&#24212;&#36807;&#31243;&#20013;&#24573;&#30053;&#20102;&#25968;&#25454;&#38544;&#31169;&#21644;&#23433;&#20840;&#38382;&#39064;&#65292;&#22240;&#20026;&#20351;&#29992;&#20102;&#28304;&#25968;&#25454;&#21644;&#30446;&#26631;&#25968;&#25454;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#65292;&#21517;&#20026;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#30340;HPE&#65292;&#26088;&#22312;&#35299;&#20915;&#22312;&#36866;&#24212;&#36807;&#31243;&#20013;&#26080;&#27861;&#35775;&#38382;&#28304;&#25968;&#25454;&#30340;HPE&#30340;&#36328;&#22495;&#23398;&#20064;&#25361;&#25112;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#30001;&#19977;&#20010;&#27169;&#22411;&#32452;&#25104;&#30340;&#26032;&#26694;&#26550;&#65306;&#28304;&#27169;&#22411;&#12289;&#20013;&#38388;&#27169;&#22411;&#21644;&#30446;&#26631;&#27169;&#22411;&#65292;&#20174;&#28304;&#25968;&#25454;&#21644;&#30446;&#26631;&#25968;&#25454;&#30340;&#35282;&#24230;&#25506;&#32034;&#35813;&#20219;&#21153;&#12290;&#28304;&#20445;&#25252;&#27169;&#22359;&#26356;&#26377;&#25928;&#22320;&#20445;&#30041;&#28304;&#20449;&#24687;&#24182;&#25269;&#25239;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human Pose Estimation (HPE) is widely used in various fields, including motion analysis, healthcare, and virtual reality. However, the great expenses of labeled real-world datasets present a significant challenge for HPE. To overcome this, one approach is to train HPE models on synthetic datasets and then perform domain adaptation (DA) on real-world data. Unfortunately, existing DA methods for HPE neglect data privacy and security by using both source and target data in the adaptation process. To this end, we propose a new task, named source-free domain adaptive HPE, which aims to address the challenges of cross-domain learning of HPE without access to source data during the adaptation process. We further propose a novel framework that consists of three models: source model, intermediate model, and target model, which explores the task from both source-protect and target-relevant perspectives. The source-protect module preserves source information more effectively while resisting noise
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#20809;&#35889;&#26041;&#27861;&#22312;&#24191;&#20041;&#22810;&#32500;&#27604;&#36739;&#20013;&#20272;&#35745;&#21644;&#37327;&#21270;&#26410;&#35266;&#23519;&#21040;&#30340;&#27604;&#36739;&#23454;&#20307;&#30340;&#20559;&#22909;&#20998;&#25968;&#30340;&#24615;&#33021;&#65292;&#24182;&#25581;&#31034;&#20102;&#20809;&#35889;&#20272;&#35745;&#37327;&#19982;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2308.02918</link><description>&lt;p&gt;
&#22522;&#20110;&#24191;&#20041;&#22810;&#32500;&#27604;&#36739;&#30340;&#20809;&#35889;&#25490;&#21517;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Spectral Ranking Inferences based on General Multiway Comparisons. (arXiv:2308.02918v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#20809;&#35889;&#26041;&#27861;&#22312;&#24191;&#20041;&#22810;&#32500;&#27604;&#36739;&#20013;&#20272;&#35745;&#21644;&#37327;&#21270;&#26410;&#35266;&#23519;&#21040;&#30340;&#27604;&#36739;&#23454;&#20307;&#30340;&#20559;&#22909;&#20998;&#25968;&#30340;&#24615;&#33021;&#65292;&#24182;&#25581;&#31034;&#20102;&#20809;&#35889;&#20272;&#35745;&#37327;&#19982;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#19968;&#20010;&#38750;&#24120;&#26222;&#36941;&#21644;&#26356;&#21152;&#30495;&#23454;&#30340;&#24773;&#26223;&#20013;&#65292;&#20351;&#29992;&#20809;&#35889;&#26041;&#27861;&#23545;&#26410;&#35266;&#23519;&#21040;&#30340;&#27604;&#36739;&#23454;&#20307;&#30340;&#20559;&#22909;&#20998;&#25968;&#36827;&#34892;&#20272;&#35745;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#27604;&#36739;&#22270;&#30001;&#21487;&#33021;&#20855;&#26377;&#24322;&#26500;&#22823;&#23567;&#30340;&#36229;&#36793;&#32452;&#25104;&#65292;&#23545;&#20110;&#32473;&#23450;&#30340;&#36229;&#36793;&#65292;&#27604;&#36739;&#25968;&#37327;&#21487;&#33021;&#20165;&#20026;1&#12290;&#36825;&#31181;&#35774;&#32622;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#36991;&#20813;&#20102;&#38656;&#35201;&#25351;&#23450;&#22270;&#30340;&#38543;&#26426;&#24615;&#20197;&#21450;&#22312;&#24120;&#29992;&#30340;Bradley-Terry-Luce (BTL)&#25110;Plackett-Luce (PL)&#27169;&#22411;&#20013;&#26045;&#21152;&#30340;&#38480;&#21046;&#24615;&#22343;&#21248;&#37319;&#26679;&#20551;&#35774;&#12290;&#27492;&#22806;&#65292;&#22312;&#36866;&#29992;BTL&#25110;PL&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#20809;&#35889;&#20272;&#35745;&#37327;&#19982;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#37327;&#65288;MLE&#65289;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#36807;&#24212;&#29992;&#20174;&#31561;&#26435;&#37325;&#20256;&#32479;&#20809;&#35889;&#26041;&#27861;&#20272;&#35745;&#24471;&#21040;&#30340;&#26368;&#20339;&#21152;&#26435;&#65292;&#21487;&#20197;&#23454;&#29616;&#19982;MLE&#30456;&#21516;&#30340;&#28176;&#36817;&#25928;&#29575;&#30340;&#21452;&#27493;&#20809;&#35889;&#26041;&#27861;&#12290;&#32771;&#34385;&#21040;&#28176;&#36817;&#24773;&#20917;&#65292;
&lt;/p&gt;
&lt;p&gt;
This paper studies the performance of the spectral method in the estimation and uncertainty quantification of the unobserved preference scores of compared entities in a very general and more realistic setup in which the comparison graph consists of hyper-edges of possible heterogeneous sizes and the number of comparisons can be as low as one for a given hyper-edge. Such a setting is pervasive in real applications, circumventing the need to specify the graph randomness and the restrictive homogeneous sampling assumption imposed in the commonly-used Bradley-Terry-Luce (BTL) or Plackett-Luce (PL) models. Furthermore, in the scenarios when the BTL or PL models are appropriate, we unravel the relationship between the spectral estimator and the Maximum Likelihood Estimator (MLE). We discover that a two-step spectral method, where we apply the optimal weighting estimated from the equal weighting vanilla spectral method, can achieve the same asymptotic efficiency as the MLE. Given the asymptot
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#39640;&#38454;&#24635;&#21464;&#24046;&#27491;&#21017;&#21270;&#30340;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#35757;&#32451;&#38750;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#65292;&#36991;&#20813;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.02293</link><description>&lt;p&gt;
&#29992;&#27491;&#21017;&#21270;&#39640;&#38454;&#24635;&#21464;&#24046;&#30340;&#38543;&#26426;&#20248;&#21270;&#26041;&#27861;&#35757;&#32451;&#38750;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
A stochastic optimization approach to train non-linear neural networks with regularization of higher-order total variation. (arXiv:2308.02293v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02293
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#39640;&#38454;&#24635;&#21464;&#24046;&#27491;&#21017;&#21270;&#30340;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#35757;&#32451;&#38750;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#65292;&#36991;&#20813;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21253;&#25324;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#20869;&#30340;&#39640;&#24230;&#34920;&#36798;&#30340;&#21442;&#25968;&#27169;&#22411;&#21487;&#20197;&#26356;&#22909;&#22320;&#24314;&#27169;&#22797;&#26434;&#27010;&#24565;&#65292;&#20294;&#35757;&#32451;&#36825;&#31181;&#39640;&#24230;&#38750;&#32447;&#24615;&#27169;&#22411;&#24050;&#30693;&#20250;&#23548;&#33268;&#20005;&#37325;&#30340;&#36807;&#25311;&#21512;&#39118;&#38505;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#19968;&#31181;k&#38454;&#24635;&#21464;&#24046;&#65288;k-TV&#65289;&#27491;&#21017;&#21270;&#65292;&#23427;&#34987;&#23450;&#20041;&#20026;&#35201;&#35757;&#32451;&#30340;&#21442;&#25968;&#27169;&#22411;&#30340;k&#38454;&#23548;&#25968;&#30340;&#24179;&#26041;&#31215;&#20998;&#65292;&#36890;&#36807;&#24809;&#32602;k-TV&#26469;&#20135;&#29983;&#19968;&#20010;&#26356;&#24179;&#28369;&#30340;&#20989;&#25968;&#65292;&#20174;&#32780;&#36991;&#20813;&#36807;&#25311;&#21512;&#12290;&#23613;&#31649;&#23558;k-TV&#39033;&#24212;&#29992;&#20110;&#19968;&#33324;&#30340;&#21442;&#25968;&#27169;&#22411;&#30001;&#20110;&#31215;&#20998;&#32780;&#23548;&#33268;&#35745;&#31639;&#22797;&#26434;&#65292;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#35757;&#32451;&#24102;&#26377;k-TV&#27491;&#21017;&#21270;&#30340;&#19968;&#33324;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#26174;&#24335;&#30340;&#25968;&#20540;&#31215;&#20998;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#32467;&#26500;&#20219;&#24847;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#65292;&#22240;&#20026;&#23427;&#21482;&#38656;&#35201;&#36827;&#34892;&#31616;&#21333;&#30340;&#38543;&#26426;&#26799;&#24230;&#20248;&#21270;&#21363;&#21487;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
While highly expressive parametric models including deep neural networks have an advantage to model complicated concepts, training such highly non-linear models is known to yield a high risk of notorious overfitting. To address this issue, this study considers a $k$th order total variation ($k$-TV) regularization, which is defined as the squared integral of the $k$th order derivative of the parametric models to be trained; penalizing the $k$-TV is expected to yield a smoother function, which is expected to avoid overfitting. While the $k$-TV terms applied to general parametric models are computationally intractable due to the integration, this study provides a stochastic optimization algorithm, that can efficiently train general models with the $k$-TV regularization without conducting explicit numerical integration. The proposed approach can be applied to the training of even deep neural networks whose structure is arbitrary, as it can be implemented by only a simple stochastic gradien
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22122;&#22768;&#27169;&#24335;&#36716;&#31227;&#27169;&#22411;&#65292;&#21487;&#20197;&#23558;&#22122;&#22768;&#27169;&#24335;&#20174;&#19981;&#21516;&#29615;&#22659;&#30340;&#26631;&#20934;&#26679;&#26412;&#24212;&#29992;&#21040;&#26410;&#30693;&#26679;&#26412;&#65292;&#36890;&#36807;&#29983;&#25104;&#26696;&#20363;&#24211;&#26469;&#35299;&#20915;&#26679;&#26412;&#32423;&#22122;&#22768;&#23545;&#25968;&#25454;&#38598;&#32423;&#22122;&#22768;&#23398;&#20064;&#30340;&#24178;&#25200;&#65292;&#25552;&#39640;&#20102;&#31995;&#32479;&#30340;&#23398;&#20064;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.01138</link><description>&lt;p&gt;
&#33021;&#21542;&#36716;&#31227;&#22122;&#22768;&#27169;&#24335;&#65311;&#20351;&#29992;&#29983;&#25104;&#26696;&#20363;&#30340;&#22810;&#29615;&#22659;&#39057;&#35889;&#20998;&#26512;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Can We Transfer Noise Patterns? An Multi-environment Spectrum Analysis Model Using Generated Cases. (arXiv:2308.01138v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01138
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22122;&#22768;&#27169;&#24335;&#36716;&#31227;&#27169;&#22411;&#65292;&#21487;&#20197;&#23558;&#22122;&#22768;&#27169;&#24335;&#20174;&#19981;&#21516;&#29615;&#22659;&#30340;&#26631;&#20934;&#26679;&#26412;&#24212;&#29992;&#21040;&#26410;&#30693;&#26679;&#26412;&#65292;&#36890;&#36807;&#29983;&#25104;&#26696;&#20363;&#24211;&#26469;&#35299;&#20915;&#26679;&#26412;&#32423;&#22122;&#22768;&#23545;&#25968;&#25454;&#38598;&#32423;&#22122;&#22768;&#23398;&#20064;&#30340;&#24178;&#25200;&#65292;&#25552;&#39640;&#20102;&#31995;&#32479;&#30340;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#27700;&#36136;&#26816;&#27979;&#20013;&#65292;&#39057;&#35889;&#20998;&#26512;&#31995;&#32479;&#26088;&#22312;&#26816;&#27979;&#27745;&#26579;&#29289;&#30340;&#31867;&#22411;&#21644;&#27987;&#24230;&#65292;&#24182;&#20351;&#30417;&#31649;&#26426;&#26500;&#33021;&#22815;&#21450;&#26102;&#22238;&#24212;&#27745;&#26579;&#20107;&#20214;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#39057;&#35889;&#25968;&#25454;&#30340;&#27979;&#35797;&#35774;&#22791;&#22312;&#38750;&#23454;&#39564;&#23460;&#29615;&#22659;&#20013;&#37096;&#32626;&#26102;&#20250;&#21463;&#21040;&#22797;&#26434;&#30340;&#22122;&#22768;&#27169;&#24335;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#20351;&#20998;&#26512;&#27169;&#22411;&#36866;&#29992;&#20110;&#26356;&#22810;&#30340;&#29615;&#22659;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22122;&#22768;&#27169;&#24335;&#36716;&#31227;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#23558;&#19981;&#21516;&#29615;&#22659;&#20013;&#26631;&#20934;&#27700;&#26679;&#21697;&#30340;&#39057;&#35889;&#20316;&#20026;&#26696;&#20363;&#65292;&#24182;&#23398;&#20064;&#23427;&#20204;&#22122;&#22768;&#27169;&#24335;&#30340;&#24046;&#24322;&#65292;&#20174;&#32780;&#20351;&#22122;&#22768;&#27169;&#24335;&#33021;&#22815;&#24212;&#29992;&#20110;&#26410;&#30693;&#26679;&#21697;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#24517;&#28982;&#23384;&#22312;&#30340;&#26679;&#26412;&#32423;&#22522;&#32447;&#22122;&#22768;&#20351;&#24471;&#27169;&#22411;&#26080;&#27861;&#33719;&#21462;&#21482;&#22312;&#25968;&#25454;&#38598;&#32423;&#29615;&#22659;&#22122;&#22768;&#19978;&#26377;&#24046;&#24322;&#30340;&#37197;&#23545;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#19968;&#20010;&#26679;&#26412;&#23545;&#26679;&#26412;&#30340;&#26696;&#20363;&#24211;&#65292;&#25490;&#38500;&#20102;&#26679;&#26412;&#32423;&#22122;&#22768;&#23545;&#25968;&#25454;&#38598;&#32423;&#22122;&#22768;&#23398;&#20064;&#30340;&#24178;&#25200;&#65292;&#25552;&#39640;&#20102;&#31995;&#32479;&#30340;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spectrum analysis systems in online water quality testing are designed to detect types and concentrations of pollutants and enable regulatory agencies to respond promptly to pollution incidents. However, spectral data-based testing devices suffer from complex noise patterns when deployed in non-laboratory environments. To make the analysis model applicable to more environments, we propose a noise patterns transferring model, which takes the spectrum of standard water samples in different environments as cases and learns the differences in their noise patterns, thus enabling noise patterns to transfer to unknown samples. Unfortunately, the inevitable sample-level baseline noise makes the model unable to obtain the paired data that only differ in dataset-level environmental noise. To address the problem, we generate a sample-to-sample case-base to exclude the interference of sample-level noise on dataset-level noise learning, enhancing the system's learning performance. Experiments on sp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;&#21453;&#20107;&#23454;&#27169;&#25311;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#27604;&#36739;&#19981;&#21516;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#22312;&#19981;&#21516;&#25805;&#20316;&#35774;&#35745;&#39046;&#22495;&#20013;&#34892;&#20026;&#39118;&#38505;&#12290;&#36890;&#36807;&#24341;&#20837;&#21453;&#20107;&#23454;&#23433;&#20840;&#36793;&#30028;&#30340;&#27010;&#24565;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#25214;&#21040;&#26368;&#20851;&#38190;&#30340;&#24773;&#26223;&#65292;&#24182;&#35780;&#20272;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#39118;&#38505;&#39057;&#29575;&#21644;&#20005;&#37325;&#31243;&#24230;&#12290;&#35813;&#26041;&#27861;&#21363;&#20351;&#22312;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#34892;&#20026;&#31574;&#30053;&#26410;&#30693;&#30340;&#24773;&#20917;&#19979;&#20063;&#36866;&#29992;&#65292;&#23545;&#22806;&#37096;&#31532;&#19977;&#26041;&#39118;&#38505;&#35780;&#20272;&#26426;&#26500;&#26377;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.01050</link><description>&lt;p&gt;
&#23545;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#39118;&#38505;&#35780;&#20272;&#30340;&#21453;&#20107;&#23454;&#23433;&#20840;&#36793;&#30028;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A Counterfactual Safety Margin Perspective on the Scoring of Autonomous Vehicles' Riskiness. (arXiv:2308.01050v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#21453;&#20107;&#23454;&#27169;&#25311;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#27604;&#36739;&#19981;&#21516;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#22312;&#19981;&#21516;&#25805;&#20316;&#35774;&#35745;&#39046;&#22495;&#20013;&#34892;&#20026;&#39118;&#38505;&#12290;&#36890;&#36807;&#24341;&#20837;&#21453;&#20107;&#23454;&#23433;&#20840;&#36793;&#30028;&#30340;&#27010;&#24565;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#25214;&#21040;&#26368;&#20851;&#38190;&#30340;&#24773;&#26223;&#65292;&#24182;&#35780;&#20272;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#39118;&#38505;&#39057;&#29575;&#21644;&#20005;&#37325;&#31243;&#24230;&#12290;&#35813;&#26041;&#27861;&#21363;&#20351;&#22312;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#34892;&#20026;&#31574;&#30053;&#26410;&#30693;&#30340;&#24773;&#20917;&#19979;&#20063;&#36866;&#29992;&#65292;&#23545;&#22806;&#37096;&#31532;&#19977;&#26041;&#39118;&#38505;&#35780;&#20272;&#26426;&#26500;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AVs&#65289;&#26377;&#28508;&#21147;&#25552;&#20379;&#35832;&#22810;&#31038;&#20250;&#25928;&#30410;&#65292;&#22914;&#20943;&#23569;&#36947;&#36335;&#20107;&#25925;&#21644;&#25552;&#39640;&#20132;&#36890;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#21382;&#21490;&#25968;&#25454;&#21644;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#37327;&#21270;AVs&#30340;&#39118;&#38505;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#27604;&#36739;&#19981;&#21516;AVs&#22312;&#21508;&#31181;&#25805;&#20316;&#35774;&#35745;&#39046;&#22495;&#65288;ODDs&#65289;&#20013;&#34892;&#20026;&#30340;&#39118;&#38505;&#65292;&#35813;&#26694;&#26550;&#22522;&#20110;&#23545;&#8220;&#19981;&#33391;&#8221;&#36947;&#36335;&#29992;&#25143;&#36827;&#34892;&#21453;&#20107;&#23454;&#27169;&#25311;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#21453;&#20107;&#23454;&#23433;&#20840;&#36793;&#30028;&#30340;&#27010;&#24565;&#65292;&#34920;&#31034;&#21487;&#33021;&#23548;&#33268;&#30896;&#25758;&#30340;&#26368;&#23567;&#20559;&#31163;&#27491;&#24120;&#34892;&#20026;&#30340;&#37327;&#12290;&#35813;&#27010;&#24565;&#26377;&#21161;&#20110;&#25214;&#21040;&#26368;&#20851;&#38190;&#30340;&#24773;&#26223;&#65292;&#21516;&#26102;&#20063;&#26377;&#21161;&#20110;&#35780;&#20272;AVs&#30340;&#39118;&#38505;&#39057;&#29575;&#21644;&#20005;&#37325;&#31243;&#24230;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#21363;&#20351;AV&#30340;&#34892;&#20026;&#31574;&#30053;&#26159;&#26410;&#30693;&#30340;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;&#20173;&#28982;&#36866;&#29992;&#20110;&#26368;&#22351;&#21644;&#26368;&#20339;&#24773;&#20917;&#20998;&#26512;&#65292;&#20351;&#35813;&#26041;&#27861;&#23545;&#22806;&#37096;&#31532;&#19977;&#26041;&#39118;&#38505;&#35780;&#20272;&#26426;&#26500;&#20063;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous Vehicles (AVs) have the potential to provide numerous societal benefits, such as decreased road accidents and increased overall transportation efficiency. However, quantifying the risk associated with AVs is challenging due to the lack of historical data and the rapidly evolving technology. This paper presents a data-driven framework for comparing the risk of different AVs' behaviors in various operational design domains (ODDs), based on counterfactual simulations of "misbehaving" road users. We introduce the concept of counterfactual safety margin, which represents the minimum deviation from normal behavior that could lead to a collision. This concept helps to find the most critical scenarios but also to assess the frequency and severity of risk of AVs. We show that the proposed methodology is applicable even when the AV's behavioral policy is unknown -- through worst- and best-case analyses -- making the method useful also to external third-party risk assessors. Our experi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#22836;&#36890;&#36947;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#21160;&#20998;&#31867;COVID-19 CT&#22270;&#20687;&#65292;&#24182;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;96.99%&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.00715</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#22810;&#22836;&#36890;&#36947;&#27880;&#24847;&#21147;&#33258;&#21160;COVID-19 CT&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Automated COVID-19 CT Image Classification using Multi-head Channel Attention in Deep CNN. (arXiv:2308.00715v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00715
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#22836;&#36890;&#36947;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#21160;&#20998;&#31867;COVID-19 CT&#22270;&#20687;&#65292;&#24182;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;96.99%&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
COVID-19&#30340;&#36805;&#36895;&#20256;&#25773;&#38656;&#35201;&#26377;&#25928;&#21644;&#20934;&#30830;&#30340;&#35786;&#26029;&#26041;&#27861;&#12290;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#22270;&#20687;&#24050;&#25104;&#20026;&#26816;&#27979;&#35813;&#30142;&#30149;&#30340;&#26377;&#20215;&#20540;&#24037;&#20855;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#21270;COVID-19 CT&#25195;&#25551;&#20998;&#31867;&#65292;&#20854;&#20013;&#25552;&#20986;&#20102;&#19968;&#20010;&#20462;&#25913;&#29256;Xception&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#26032;&#35774;&#35745;&#30340;&#36890;&#36947;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#21152;&#26435;&#20840;&#23616;&#24179;&#22343;&#27744;&#21270;&#65292;&#20197;&#22686;&#24378;&#29305;&#24449;&#25552;&#21462;&#65292;&#20174;&#32780;&#25552;&#39640;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;&#36890;&#36947;&#27880;&#24847;&#21147;&#27169;&#22359;&#36873;&#25321;&#24615;&#22320;&#20851;&#27880;&#27599;&#20010;&#36890;&#36947;&#20013;&#30340;&#20449;&#24687;&#21306;&#22495;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#29992;&#20110;COVID-19&#26816;&#27979;&#30340;&#21028;&#21035;&#29305;&#24449;&#12290;&#23545;&#24191;&#27867;&#20351;&#29992;&#30340;COVID-19 CT&#25195;&#25551;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#26174;&#31034;&#20986;96.99%&#30340;&#38750;&#24120;&#22909;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#23545;&#20854;&#20182;&#26368;&#20808;&#36827;&#25216;&#26415;&#30340;&#20248;&#36234;&#24615;&#12290;&#36825;&#39033;&#30740;&#31350;&#21487;&#20197;&#20026;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#25239;&#20987;&#24403;&#21069;&#21644;&#26410;&#26469;&#30340;&#27969;&#34892;&#30149;&#30340;&#19981;&#25032;&#21162;&#21147;&#20570;&#20986;&#36129;&#29486;&#65292;&#25552;&#20379;&#26377;&#24076;&#26395;&#21644;&#21450;&#26102;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid spread of COVID-19 has necessitated efficient and accurate diagnostic methods. Computed Tomography (CT) scan images have emerged as a valuable tool for detecting the disease. In this article, we present a novel deep learning approach for automated COVID-19 CT scan classification where a modified Xception model is proposed which incorporates a newly designed channel attention mechanism and weighted global average pooling to enhance feature extraction thereby improving classification accuracy. The channel attention module selectively focuses on informative regions within each channel, enabling the model to learn discriminative features for COVID-19 detection. Experiments on a widely used COVID-19 CT scan dataset demonstrate a very good accuracy of 96.99% and show its superiority to other state-of-the-art techniques. This research can contribute to the ongoing efforts in using artificial intelligence to combat current and future pandemics and can offer promising and timely solut
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#21512;&#35843;&#26597;&#20102;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#38382;&#39064;&#65292;&#28085;&#30422;&#20102;&#38544;&#31169;&#12289;&#23433;&#20840;&#12289;&#20844;&#24179;&#24615;&#21644;&#36131;&#20219;&#31561;&#22810;&#20010;&#32500;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#38469;&#24314;&#35758;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2307.16680</link><description>&lt;p&gt;
&#20851;&#20110;&#26368;&#20808;&#36827;&#29983;&#25104;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#26223;&#35266;&#65306;&#19968;&#39033;&#32508;&#21512;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
On the Trustworthiness Landscape of State-of-the-art Generative Models: A Comprehensive Survey. (arXiv:2307.16680v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#21512;&#35843;&#26597;&#20102;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#38382;&#39064;&#65292;&#28085;&#30422;&#20102;&#38544;&#31169;&#12289;&#23433;&#20840;&#12289;&#20844;&#24179;&#24615;&#21644;&#36131;&#20219;&#31561;&#22810;&#20010;&#32500;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#38469;&#24314;&#35758;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#21644;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#39046;&#20808;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#23545;&#20154;&#31867;&#29983;&#27963;&#30340;&#21508;&#20010;&#26041;&#38754;&#20135;&#29983;&#20102;&#38761;&#21629;&#24615;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#23454;&#38469;&#24212;&#29992;&#20063;&#26292;&#38706;&#20986;&#22266;&#26377;&#30340;&#39118;&#38505;&#65292;&#31361;&#26174;&#20102;&#23427;&#20204;&#30340;&#21452;&#37325;&#24615;&#36136;&#65292;&#24182;&#24341;&#21457;&#20102;&#23545;&#23427;&#20204;&#21487;&#20449;&#24230;&#30340;&#25285;&#24551;&#12290;&#23613;&#31649;&#26377;&#22823;&#37327;&#20851;&#20110;&#36825;&#20010;&#20027;&#39064;&#30340;&#25991;&#29486;&#65292;&#20294;&#38024;&#23545;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#21450;&#20854;&#21487;&#20449;&#24230;&#30340;&#32508;&#21512;&#35843;&#26597;&#20173;&#28982;&#24456;&#23569;&#35265;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#25991;&#35843;&#26597;&#20102;&#28041;&#21450;&#36825;&#20123;&#27169;&#22411;&#30340;&#38271;&#26399;&#21644;&#26032;&#20852;&#23041;&#32961;&#65292;&#28085;&#30422;&#20102;&#38544;&#31169;&#12289;&#23433;&#20840;&#12289;&#20844;&#24179;&#21644;&#36131;&#20219;&#36825;&#22235;&#20010;&#22522;&#26412;&#32500;&#24230;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#24352;&#35814;&#23613;&#30340;&#22320;&#22270;&#65292;&#27010;&#36848;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#65292;&#24182;&#25552;&#20379;&#20102;&#23454;&#38469;&#24314;&#35758;&#21644;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;&#36825;&#20123;&#21162;&#21147;&#23545;&#20110;&#20419;&#36827;&#36825;&#20123;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models and large language models have emerged as leading-edge generative models and have sparked a revolutionary impact on various aspects of human life. However, the practical implementation of these models has also exposed inherent risks, highlighting their dual nature and raising concerns regarding their trustworthiness. Despite the abundance of literature on this subject, a comprehensive survey specifically delving into the intersection of large-scale generative models and their trustworthiness remains largely absent. To bridge this gap, This paper investigates both the long-standing and emerging threats associated with these models across four fundamental dimensions: privacy, security, fairness, and responsibility. In this way, we construct an extensive map outlining the trustworthiness of these models, while also providing practical recommendations and identifying future directions. These efforts are crucial for promoting the trustworthy deployment of these models, ulti
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37325;&#25918;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#23569;&#37327;&#21382;&#21490;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;&#39044;&#27979;&#33258;&#21160;&#32553;&#25918;&#20013;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.15941</link><description>&lt;p&gt;
&#22312;&#39044;&#27979;&#33258;&#21160;&#32553;&#25918;&#20013;&#30340;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Continual Learning in Predictive Autoscaling. (arXiv:2307.15941v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15941
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37325;&#25918;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#23569;&#37327;&#21382;&#21490;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;&#39044;&#27979;&#33258;&#21160;&#32553;&#25918;&#20013;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#33258;&#21160;&#32553;&#25918;&#34987;&#29992;&#20110;&#39044;&#27979;&#26381;&#21153;&#22120;&#30340;&#24037;&#20316;&#36127;&#36733;&#65292;&#24182;&#25552;&#21069;&#20934;&#22791;&#36164;&#28304;&#65292;&#20197;&#30830;&#20445;&#22312;&#21160;&#24577;&#20113;&#29615;&#22659;&#20013;&#30340;&#26381;&#21153;&#27700;&#24179;&#30446;&#26631;&#65288;SLOs&#65289;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#20854;&#39044;&#27979;&#20219;&#21153;&#24120;&#24120;&#22312;&#22806;&#37096;&#20107;&#20214;&#65288;&#22914;&#20419;&#38144;&#27963;&#21160;&#21644;&#24212;&#29992;&#31243;&#24207;&#37325;&#26032;&#37197;&#32622;&#65289;&#24341;&#36215;&#30340;&#24322;&#24120;&#27969;&#37327;&#19979;&#24615;&#33021;&#19979;&#38477;&#65292;&#24120;&#35265;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#20351;&#29992;&#38271;&#26102;&#38388;&#21382;&#21490;&#25968;&#25454;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#65292;&#20294;&#20195;&#20215;&#26159;&#39640;&#35745;&#31639;&#21644;&#23384;&#20648;&#25104;&#26412;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37325;&#25918;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#21363;&#22522;&#20110;&#23494;&#24230;&#30340;&#35760;&#24518;&#36873;&#25321;&#21644;&#22522;&#20110;&#25552;&#31034;&#30340;&#32593;&#32476;&#23398;&#20064;&#27169;&#22411;&#65288;DMSHM&#65289;&#65292;&#21482;&#20351;&#29992;&#19968;&#23567;&#37096;&#20998;&#21382;&#21490;&#26085;&#24535;&#25968;&#25454;&#26469;&#23454;&#29616;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#22312;&#39044;&#27979;&#20219;&#21153;&#20013;&#24212;&#29992;&#37325;&#25918;&#24335;&#25345;&#32493;&#23398;&#20064;&#26102;&#30340;&#26679;&#26412;&#37325;&#21472;&#29616;&#35937;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#24182;&#26377;&#25928;&#22320;&#25972;&#21512;&#26032;&#30340;&#26679;&#26412;&#20998;&#24067;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#24230;&#30340;&#35760;&#24518;&#36873;&#25321;&#21644;&#22522;&#20110;&#25552;&#31034;&#30340;&#32593;&#32476;&#23398;&#20064;&#27169;&#22411;&#65288;DMSHM&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predictive Autoscaling is used to forecast the workloads of servers and prepare the resources in advance to ensure service level objectives (SLOs) in dynamic cloud environments.However, in practice, its prediction task often suffers from performance degradation under abnormal traffics caused by external events (such as sales promotional activities and applications' re-configurations), for which a common solution is to re-train the model with data of a long historical period, but at the expense of high computational and storage costs.To better address this problem, we propose a replay-based continual learning method, i.e., Density-based Memory Selection and Hint-based Network Learning Model (DMSHM), using only a small part of the historical log to achieve accurate predictions.First, we discover the phenomenon of sample overlap when applying replay-based continual learning in prediction tasks. In order to surmount this challenge and effectively integrate new sample distribution, we propo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;AlignDet&#65292;&#36890;&#36807;&#23558;&#39044;&#35757;&#32451;&#36807;&#31243;&#20998;&#35299;&#20026;&#22270;&#20687;&#22495;&#21644;&#26694;&#22495;&#39044;&#35757;&#32451;&#65292;&#21487;&#20197;&#20943;&#36731;&#30446;&#26631;&#26816;&#27979;&#20013;&#29616;&#26377;&#23454;&#36341;&#20013;&#30340;&#25968;&#25454;&#12289;&#27169;&#22411;&#21644;&#20219;&#21153;&#24046;&#24322;&#65292;&#25552;&#39640;&#26816;&#27979;&#22120;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.11077</link><description>&lt;p&gt;
AlignDet: &#23545;&#40784;&#30446;&#26631;&#26816;&#27979;&#30340;&#39044;&#35757;&#32451;&#19982;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
AlignDet: Aligning Pre-training and Fine-tuning in Object Detection. (arXiv:2307.11077v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11077
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;AlignDet&#65292;&#36890;&#36807;&#23558;&#39044;&#35757;&#32451;&#36807;&#31243;&#20998;&#35299;&#20026;&#22270;&#20687;&#22495;&#21644;&#26694;&#22495;&#39044;&#35757;&#32451;&#65292;&#21487;&#20197;&#20943;&#36731;&#30446;&#26631;&#26816;&#27979;&#20013;&#29616;&#26377;&#23454;&#36341;&#20013;&#30340;&#25968;&#25454;&#12289;&#27169;&#22411;&#21644;&#20219;&#21153;&#24046;&#24322;&#65292;&#25552;&#39640;&#26816;&#27979;&#22120;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#21518;&#36827;&#34892;&#24494;&#35843;&#30340;&#33539;&#24335;&#22312;&#21508;&#31181;&#30446;&#26631;&#26816;&#27979;&#31639;&#27861;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;&#29616;&#26377;&#23454;&#36341;&#20013;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#36807;&#31243;&#20043;&#38388;&#30340;&#25968;&#25454;&#12289;&#27169;&#22411;&#21644;&#20219;&#21153;&#24046;&#24322;&#65292;&#36825;&#20123;&#24046;&#24322;&#38544;&#21547;&#22320;&#38480;&#21046;&#20102;&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;&#12289;&#27867;&#21270;&#33021;&#21147;&#21644;&#25910;&#25947;&#36895;&#24230;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AlignDet&#65292;&#19968;&#20010;&#32479;&#19968;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#21487;&#36866;&#29992;&#20110;&#21508;&#31181;&#29616;&#26377;&#26816;&#27979;&#22120;&#65292;&#20197;&#20943;&#36731;&#36825;&#20123;&#24046;&#24322;&#12290;AlignDet&#23558;&#39044;&#35757;&#32451;&#36807;&#31243;&#20998;&#35299;&#20026;&#20004;&#20010;&#38454;&#27573;&#65292;&#21363;&#22270;&#20687;&#22495;&#21644;&#26694;&#22495;&#39044;&#35757;&#32451;&#12290;&#22270;&#20687;&#22495;&#39044;&#35757;&#32451;&#20248;&#21270;&#26816;&#27979;&#20027;&#24178;&#20197;&#25429;&#25417;&#25972;&#20307;&#35270;&#35273;&#25277;&#35937;&#65292;&#32780;&#26694;&#22495;&#39044;&#35757;&#32451;&#23398;&#20064;&#23454;&#20363;&#32423;&#35821;&#20041;&#21644;&#20219;&#21153;&#24863;&#30693;&#27010;&#24565;&#20197;&#21021;&#22987;&#21270;&#20027;&#24178;&#20043;&#22806;&#30340;&#37096;&#20998;&#12290;&#36890;&#36807;&#34701;&#20837;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#20027;&#24178;&#65292;&#25105;&#20204;&#21487;&#20197;&#23545;&#21508;&#31181;&#26816;&#27979;&#22120;&#30340;&#25152;&#26377;&#27169;&#22359;&#36827;&#34892;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paradigm of large-scale pre-training followed by downstream fine-tuning has been widely employed in various object detection algorithms. In this paper, we reveal discrepancies in data, model, and task between the pre-training and fine-tuning procedure in existing practices, which implicitly limit the detector's performance, generalization ability, and convergence speed. To this end, we propose AlignDet, a unified pre-training framework that can be adapted to various existing detectors to alleviate the discrepancies. AlignDet decouples the pre-training process into two stages, i.e., image-domain and box-domain pre-training. The image-domain pre-training optimizes the detection backbone to capture holistic visual abstraction, and box-domain pre-training learns instance-level semantics and task-aware concepts to initialize the parts out of the backbone. By incorporating the self-supervised pre-trained backbones, we can pre-train all modules for various detectors in an unsupervised par
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#21644;&#40065;&#26834;&#30340;&#26041;&#27861;&#26469;&#36817;&#20284;&#35745;&#31639;&#22810;&#20803;&#27491;&#24577;&#20998;&#24067;&#20043;&#38388;&#30340;Fisher-Rao&#36317;&#31163;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31867;&#22522;&#20110;&#27491;&#24577;&#27969;&#24418;&#23884;&#20837;&#21040;&#39640;&#32500;&#23545;&#31216;&#27491;&#23450;&#38181;&#23376;&#27969;&#24418;&#30340;&#36317;&#31163;&#12290;</title><link>http://arxiv.org/abs/2307.10644</link><description>&lt;p&gt;
Fisher-Rao&#36317;&#31163;&#21644;&#36870;&#25512;&#21040;SPD&#38181;&#36317;&#31163;&#22312;&#22810;&#20803;&#27491;&#24577;&#20998;&#24067;&#20043;&#38388;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Fisher-Rao distance and pullback SPD cone distances between multivariate normal distributions. (arXiv:2307.10644v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10644
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#21644;&#40065;&#26834;&#30340;&#26041;&#27861;&#26469;&#36817;&#20284;&#35745;&#31639;&#22810;&#20803;&#27491;&#24577;&#20998;&#24067;&#20043;&#38388;&#30340;Fisher-Rao&#36317;&#31163;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31867;&#22522;&#20110;&#27491;&#24577;&#27969;&#24418;&#23884;&#20837;&#21040;&#39640;&#32500;&#23545;&#31216;&#27491;&#23450;&#38181;&#23376;&#27969;&#24418;&#30340;&#36317;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#31185;&#23398;&#39046;&#22495;&#65292;&#22914;&#25193;&#25955;&#24352;&#37327;&#25104;&#20687;&#12289;&#32467;&#26500;&#24352;&#37327;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#38647;&#36798;&#20449;&#21495;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#31561;&#65292;&#37117;&#23384;&#22312;&#30528;&#22810;&#20803;&#27491;&#24577;&#20998;&#24067;&#30340;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#22788;&#29702;&#36825;&#20123;&#27491;&#24577;&#25968;&#25454;&#38598;&#20197;&#36827;&#34892;&#36807;&#28388;&#12289;&#20998;&#31867;&#25110;&#32858;&#31867;&#31561;&#19979;&#28216;&#20219;&#21153;&#65292;&#38656;&#35201;&#23450;&#20041;&#21512;&#36866;&#30340;&#27491;&#24577;&#21644;&#23427;&#20204;&#20043;&#38388;&#30340;&#36335;&#24452;&#20043;&#38388;&#30340;&#24046;&#24322;&#24230;&#37327;&#12290;Fisher-Rao&#36317;&#31163;&#65292;&#20316;&#20026;Fisher&#20449;&#24687;&#24230;&#37327;&#24341;&#36215;&#30340;Riemann&#20960;&#20309;&#36317;&#31163;&#65292;&#26159;&#19968;&#31181;&#21512;&#29702;&#30340;&#24230;&#37327;&#36317;&#31163;&#65292;&#20294;&#38500;&#20102;&#19968;&#20123;&#29305;&#27530;&#24773;&#20917;&#22806;&#65292;&#24182;&#27809;&#26377;&#38381;&#24335;&#27714;&#35299;&#12290;&#26412;&#25991;&#39318;&#20808;&#25253;&#21578;&#20102;&#19968;&#31181;&#24555;&#36895;&#19988;&#40065;&#26834;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#31934;&#30830;&#22320;&#36817;&#20284;&#35745;&#31639;&#22810;&#20803;&#27491;&#24577;&#20998;&#24067;&#20043;&#38388;&#30340;Fisher-Rao&#36317;&#31163;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31867;&#22522;&#20110;&#27491;&#24577;&#27969;&#24418;&#21040;&#39640;&#32500;&#23545;&#31216;&#27491;&#23450;&#38181;&#30340;&#23376;&#27969;&#24418;&#30340;&#24494;&#20998;&#21516;&#32986;&#23884;&#20837;&#30340;&#36317;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data sets of multivariate normal distributions abound in many scientific areas like diffusion tensor imaging, structure tensor computer vision, radar signal processing, machine learning, just to name a few. In order to process those normal data sets for downstream tasks like filtering, classification or clustering, one needs to define proper notions of dissimilarities between normals and paths joining them. The Fisher-Rao distance defined as the Riemannian geodesic distance induced by the Fisher information metric is such a principled metric distance which however is not known in closed-form excepts for a few particular cases. In this work, we first report a fast and robust method to approximate arbitrarily finely the Fisher-Rao distance between multivariate normal distributions. Second, we introduce a class of distances based on diffeomorphic embeddings of the normal manifold into a submanifold of the higher-dimensional symmetric positive-definite cone corresponding to the manifold of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#20351;&#29992;&#27491;&#21017;&#34920;&#36798;&#24335;&#21644;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#26469;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#25991;&#26412;&#29983;&#25104;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.09702</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;LLM&#24341;&#23548;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Efficient Guided Generation for LLMs. (arXiv:2307.09702v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09702
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#20351;&#29992;&#27491;&#21017;&#34920;&#36798;&#24335;&#21644;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#26469;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#25991;&#26412;&#29983;&#25104;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#20351;&#29992;&#27491;&#21017;&#34920;&#36798;&#24335;&#21644;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#26469;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#25991;&#26412;&#29983;&#25104;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26631;&#35760;&#24207;&#21015;&#29983;&#25104;&#36807;&#31243;&#20013;&#20960;&#20046;&#19981;&#22686;&#21152;&#20219;&#20309;&#24320;&#38144;&#65292;&#24182;&#20351;&#24471;&#24341;&#23548;&#29983;&#25104;&#22312;&#23454;&#38469;&#20013;&#21487;&#34892;&#12290;&#22312;&#24320;&#28304;Python&#24211;Outlines&#20013;&#25552;&#20379;&#20102;&#19968;&#20010;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this article we describe an efficient approach to guiding language model text generation with regular expressions and context-free grammars. Our approach adds little to no overhead to the token sequence generation process, and makes guided generation feasible in practice. An implementation is provided in the open source Python library Outlines.
&lt;/p&gt;</description></item><item><title>CaRT&#26159;&#19968;&#31181;&#39564;&#35777;&#23433;&#20840;&#21644;&#40065;&#26834;&#36319;&#36394;&#30340;&#20998;&#23618;&#20998;&#24067;&#24335;&#26550;&#26500;&#65292;&#38024;&#23545;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#22522;&#20110;&#23398;&#20064;&#30340;&#36816;&#21160;&#35268;&#21010;&#12290;&#23427;&#36890;&#36807;&#20998;&#26512;&#24418;&#24335;&#30340;&#23433;&#20840;&#36807;&#28388;&#22120;&#30830;&#20445;&#20102;&#23433;&#20840;&#26426;&#21160;&#65292;&#24182;&#36890;&#36807;&#40065;&#26834;&#36807;&#28388;&#22120;&#20248;&#21270;&#36319;&#36394;&#35748;&#35777;&#23433;&#20840;&#36712;&#36857;&#65292;&#21363;&#20351;&#22312;&#24178;&#25200;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#20445;&#35777;&#23433;&#20840;&#21644;&#36712;&#36857;&#36319;&#36394;&#35823;&#24046;&#30340;&#25351;&#25968;&#26377;&#30028;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.08602</link><description>&lt;p&gt;
CaRT: &#38024;&#23545;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#22522;&#20110;&#23398;&#20064;&#30340;&#36816;&#21160;&#35268;&#21010;&#30340;&#39564;&#35777;&#23433;&#20840;&#21644;&#40065;&#26834;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
CaRT: Certified Safety and Robust Tracking in Learning-based Motion Planning for Multi-Agent Systems. (arXiv:2307.08602v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08602
&lt;/p&gt;
&lt;p&gt;
CaRT&#26159;&#19968;&#31181;&#39564;&#35777;&#23433;&#20840;&#21644;&#40065;&#26834;&#36319;&#36394;&#30340;&#20998;&#23618;&#20998;&#24067;&#24335;&#26550;&#26500;&#65292;&#38024;&#23545;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#22522;&#20110;&#23398;&#20064;&#30340;&#36816;&#21160;&#35268;&#21010;&#12290;&#23427;&#36890;&#36807;&#20998;&#26512;&#24418;&#24335;&#30340;&#23433;&#20840;&#36807;&#28388;&#22120;&#30830;&#20445;&#20102;&#23433;&#20840;&#26426;&#21160;&#65292;&#24182;&#36890;&#36807;&#40065;&#26834;&#36807;&#28388;&#22120;&#20248;&#21270;&#36319;&#36394;&#35748;&#35777;&#23433;&#20840;&#36712;&#36857;&#65292;&#21363;&#20351;&#22312;&#24178;&#25200;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#20445;&#35777;&#23433;&#20840;&#21644;&#36712;&#36857;&#36319;&#36394;&#35823;&#24046;&#30340;&#25351;&#25968;&#26377;&#30028;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20998;&#26512;&#26041;&#27861;CaRT&#30340;&#20851;&#38190;&#21019;&#26032;&#22312;&#20110;&#24314;&#31435;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#23618;&#20998;&#24067;&#24335;&#26550;&#26500;&#65292;&#20197;&#20445;&#35777;&#32473;&#23450;&#23398;&#20064;-based&#36816;&#21160;&#35268;&#21010;&#31574;&#30053;&#30340;&#23433;&#20840;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#39318;&#20808;&#65292;&#22312;&#21517;&#20041;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#30340;CaRT&#23433;&#20840;&#36807;&#28388;&#22120;&#30340;&#20998;&#26512;&#24418;&#24335;&#27491;&#24335;&#30830;&#20445;&#38750;&#32447;&#24615;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#23433;&#20840;&#26426;&#21160;&#65292;&#19988;&#19982;&#22522;&#20110;&#23398;&#20064;&#30340;&#31574;&#30053;&#26368;&#23567;&#20559;&#24046;&#12290;&#20854;&#27425;&#65292;&#22312;&#38750;&#21517;&#20041;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#30340;CaRT&#40065;&#26834;&#36807;&#28388;&#22120;&#30340;&#20998;&#26512;&#24418;&#24335;&#26368;&#20248;&#36319;&#36394;&#30001;&#23618;&#27425;&#32467;&#26500;&#20013;&#21069;&#19968;&#23618;&#29983;&#25104;&#30340;&#32463;&#36807;&#35748;&#35777;&#30340;&#23433;&#20840;&#36712;&#36857;&#65292;&#21363;CaRT&#23433;&#20840;&#36807;&#28388;&#22120;&#12290;&#25105;&#20204;&#20351;&#29992;&#25910;&#32553;&#29702;&#35770;&#35777;&#26126;CaRT&#22312;&#23384;&#22312;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#24178;&#25200;&#30340;&#24773;&#20917;&#19979;&#20445;&#35777;&#23433;&#20840;&#21644;&#36712;&#36857;&#36319;&#36394;&#35823;&#24046;&#30340;&#25351;&#25968;&#26377;&#30028;&#24615;&#12290;&#27492;&#22806;&#65292;CaRT&#30340;&#20998;&#23618;&#24615;&#36136;&#20351;&#20854;&#33021;&#22815;&#36890;&#36807;&#23545;&#32463;&#36807;&#35748;&#35777;&#30340;&#23433;&#20840;&#36712;&#36857;&#30340;&#21331;&#36234;&#36319;&#36394;&#26469;&#22686;&#24378;&#20854;&#23545;&#23433;&#20840;&#24615;&#30340;&#40065;&#26834;&#24615;&#65292;&#20174;&#32780;&#20351;&#20854;&#36866;&#29992;&#20110;&#38750;&#21517;&#20041;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
The key innovation of our analytical method, CaRT, lies in establishing a new hierarchical, distributed architecture to guarantee the safety and robustness of a given learning-based motion planning policy. First, in a nominal setting, the analytical form of our CaRT safety filter formally ensures safe maneuvers of nonlinear multi-agent systems, optimally with minimal deviation from the learning-based policy. Second, in off-nominal settings, the analytical form of our CaRT robust filter optimally tracks the certified safe trajectory, generated by the previous layer in the hierarchy, the CaRT safety filter. We show using contraction theory that CaRT guarantees safety and the exponential boundedness of the trajectory tracking error, even under the presence of deterministic and stochastic disturbance. Also, the hierarchical nature of CaRT enables enhancing its robustness for safety just by its superior tracking to the certified safe trajectory, thereby making it suitable for off-nominal sc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;GP-UCB&#31639;&#27861;&#36827;&#34892;&#25913;&#36827;&#65292;&#20351;&#20854;&#20855;&#26377;&#20960;&#20046;&#26368;&#20248;&#30340;&#27425;&#32447;&#24615;&#36951;&#25022;&#65292;&#24182;&#35299;&#20915;&#20102;&#20851;&#20110;&#36951;&#25022;&#20998;&#26512;&#30340;&#24320;&#25918;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.07539</link><description>&lt;p&gt;
&#22312;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#25913;&#36827;&#33258;&#26631;&#20934;&#21270;&#27987;&#24230;&#65306;&#23545;GP-UCB&#31639;&#27861;&#30340;&#27425;&#32447;&#24615;&#36951;&#25022;
&lt;/p&gt;
&lt;p&gt;
Improved Self-Normalized Concentration in Hilbert Spaces: Sublinear Regret for GP-UCB. (arXiv:2307.07539v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07539
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;GP-UCB&#31639;&#27861;&#36827;&#34892;&#25913;&#36827;&#65292;&#20351;&#20854;&#20855;&#26377;&#20960;&#20046;&#26368;&#20248;&#30340;&#27425;&#32447;&#24615;&#36951;&#25022;&#65292;&#24182;&#35299;&#20915;&#20102;&#20851;&#20110;&#36951;&#25022;&#20998;&#26512;&#30340;&#24320;&#25918;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26680;&#21270;&#36172;&#21338;&#26426;&#38382;&#39064;&#20013;&#65292;&#23398;&#20064;&#22120;&#26088;&#22312;&#36890;&#36807;&#20165;&#22312;&#39034;&#24207;&#36873;&#25321;&#30340;&#28857;&#22788;&#36827;&#34892;&#22122;&#22768;&#35780;&#20272;&#65292;&#39034;&#24207;&#35745;&#31639;&#20301;&#20110;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#20989;&#25968;&#30340;&#26368;&#20248;&#35299;&#12290;&#29305;&#21035;&#22320;&#65292;&#23398;&#20064;&#22120;&#26088;&#22312;&#26368;&#23567;&#21270;&#36951;&#25022;&#65292;&#36951;&#25022;&#26159;&#25152;&#20570;&#36873;&#25321;&#30340;&#27425;&#20248;&#24615;&#24230;&#37327;&#12290;&#21487;&#20197;&#35828;&#26368;&#21463;&#27426;&#36814;&#30340;&#31639;&#27861;&#26159;&#39640;&#26031;&#36807;&#31243;&#19978;&#30028;&#32622;&#20449;&#21306;&#38388;&#65288;GP-UCB&#65289;&#31639;&#27861;&#65292;&#23427;&#28041;&#21450;&#26681;&#25454;&#26410;&#30693;&#20989;&#25968;&#30340;&#31616;&#21333;&#32447;&#24615;&#20272;&#35745;&#22120;&#36827;&#34892;&#34892;&#21160;&#12290;&#23613;&#31649;&#23427;&#24456;&#21463;&#27426;&#36814;&#65292;&#20294;&#29616;&#26377;&#30340;GP-UCB&#36951;&#25022;&#20998;&#26512;&#32473;&#20986;&#20102;&#27425;&#20248;&#36951;&#25022;&#29575;&#65292;&#23545;&#20110;&#35768;&#22810;&#24120;&#29992;&#30340;&#20869;&#26680;&#65288;&#22914;Mat&#233;rn&#20869;&#26680;&#65289;&#32780;&#35328;&#65292;&#36951;&#25022;&#29575;&#24182;&#19981;&#27425;&#32447;&#24615;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#65306;&#29616;&#26377;&#30340;GP-UCB&#36951;&#25022;&#20998;&#26512;&#26159;&#21542;&#32039;&#23494;&#65292;&#25110;&#32773;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#26356;&#22797;&#26434;&#30340;&#20998;&#26512;&#25216;&#26415;&#25913;&#36827;&#30028;&#38480;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#36825;&#20010;&#24320;&#25918;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;GP-UCB&#20855;&#26377;&#20960;&#20046;&#26368;&#20248;&#30340;&#36951;&#25022;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#30452;&#25509;&#26263;&#31034;&#20102;&#27425;&#32447;&#24615;&#36951;&#25022;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the kernelized bandit problem, a learner aims to sequentially compute the optimum of a function lying in a reproducing kernel Hilbert space given only noisy evaluations at sequentially chosen points. In particular, the learner aims to minimize regret, which is a measure of the suboptimality of the choices made. Arguably the most popular algorithm is the Gaussian Process Upper Confidence Bound (GP-UCB) algorithm, which involves acting based on a simple linear estimator of the unknown function. Despite its popularity, existing analyses of GP-UCB give a suboptimal regret rate, which fails to be sublinear for many commonly used kernels such as the Mat\'ern kernel. This has led to a longstanding open question: are existing regret analyses for GP-UCB tight, or can bounds be improved by using more sophisticated analytical techniques? In this work, we resolve this open question and show that GP-UCB enjoys nearly optimal regret. In particular, our results directly imply sublinear regret rate
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#20026;&#22522;&#30784;&#31185;&#23398;&#30340;&#21457;&#29616;&#25552;&#20379;&#26426;&#20250;&#65292;&#36890;&#36807;&#20854;&#33258;&#20027;&#29983;&#25104;&#20551;&#35774;&#21644;&#25506;&#32034;&#20551;&#35774;&#31354;&#38388;&#30340;&#38381;&#29615;&#26041;&#27861;&#65292;&#21152;&#36895;&#31185;&#23398;&#21457;&#29616;&#30340;&#36827;&#31243;&#12290;</title><link>http://arxiv.org/abs/2307.07522</link><description>&lt;p&gt;
&#30001;&#29983;&#25104;&#38381;&#29615;&#20154;&#24037;&#26234;&#33021;&#24341;&#39046;&#30340;&#22522;&#30784;&#31185;&#23398;&#30340;&#26410;&#26469;
&lt;/p&gt;
&lt;p&gt;
The Future of Fundamental Science Led by Generative Closed-Loop Artificial Intelligence. (arXiv:2307.07522v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07522
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#20026;&#22522;&#30784;&#31185;&#23398;&#30340;&#21457;&#29616;&#25552;&#20379;&#26426;&#20250;&#65292;&#36890;&#36807;&#20854;&#33258;&#20027;&#29983;&#25104;&#20551;&#35774;&#21644;&#25506;&#32034;&#20551;&#35774;&#31354;&#38388;&#30340;&#38381;&#29615;&#26041;&#27861;&#65292;&#21152;&#36895;&#31185;&#23398;&#21457;&#29616;&#30340;&#36827;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#27491;&#22312;&#39072;&#35206;&#25216;&#26415;&#21019;&#26032;&#12289;&#20135;&#21697;&#24320;&#21457;&#21644;&#25972;&#20010;&#31038;&#20250;&#12290;&#20154;&#24037;&#26234;&#33021;&#23545;&#25216;&#26415;&#30340;&#36129;&#29486;&#21487;&#20197;&#36890;&#36807;&#22810;&#31181;&#36884;&#24452;&#23454;&#29616;&#65292;&#38656;&#35201;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#26126;&#30830;&#30340;&#24615;&#33021;&#35780;&#20272;&#26631;&#20934;&#65292;&#33539;&#22260;&#20174;&#27169;&#24335;&#35782;&#21035;&#21644;&#20998;&#31867;&#21040;&#29983;&#25104;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#31185;&#23398;&#23454;&#36341;&#21644;&#27169;&#22411;&#21457;&#29616;&#38656;&#35201;&#35775;&#38382;&#39640;&#36136;&#37327;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#20154;&#24037;&#26234;&#33021;&#23545;&#22522;&#30784;&#31185;&#23398;&#30340;&#36129;&#29486;&#36739;&#23569;&#12290;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#65292;&#29305;&#21035;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#33021;&#20195;&#34920;&#20102;&#36890;&#36807;&#23450;&#37327;&#27169;&#22411;&#22686;&#24378;&#21644;&#21152;&#36895;&#22522;&#30784;&#28145;&#24230;&#31185;&#23398;&#30340;&#31185;&#23398;&#21457;&#29616;&#30340;&#26426;&#20250;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25506;&#32034;&#21644;&#30740;&#31350;&#20102;&#19968;&#31181;&#30001;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#12289;&#33258;&#21160;&#21270;&#30340;&#38381;&#29615;&#31185;&#23398;&#21457;&#29616;&#26041;&#27861;&#30340;&#21508;&#20010;&#26041;&#38754;&#65292;&#21253;&#25324;&#33258;&#20027;&#29983;&#25104;&#20551;&#35774;&#21644;&#24320;&#25918;&#24335;&#33258;&#20027;&#25506;&#32034;&#20551;&#35774;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in machine learning and AI, including Generative AI and LLMs, are disrupting technological innovation, product development, and society as a whole. AI's contribution to technology can come from multiple approaches that require access to large training data sets and clear performance evaluation criteria, ranging from pattern recognition and classification to generative models. Yet, AI has contributed less to fundamental science in part because large data sets of high-quality data for scientific practice and model discovery are more difficult to access. Generative AI, in general, and Large Language Models in particular, may represent an opportunity to augment and accelerate the scientific discovery of fundamental deep science with quantitative models. Here we explore and investigate aspects of an AI-driven, automated, closed-loop approach to scientific discovery, including self-driven hypothesis generation and open-ended autonomous exploration of the hypothesis space. Int
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32039;&#20945;&#30340;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#33410;&#28857;&#35782;&#21035;&#12289;&#23494;&#38598;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#21644;&#28040;&#24687;&#20256;&#36882;&#23618;&#26469;&#23454;&#29616;&#65292;&#19981;&#38656;&#35201;&#22797;&#26434;&#30340;&#39034;&#24207;&#27169;&#22359;&#12290;&#35813;&#27169;&#22411;&#22312;&#23454;&#35777;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#20102;&#26377;&#25928;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.01482</link><description>&lt;p&gt;
Nexus sine qua non&#65306;&#22522;&#20110;&#33410;&#28857;&#35782;&#21035;&#30340;&#31070;&#32463;&#32593;&#32476;&#36830;&#25509;&#30340;&#26102;&#31354;&#39044;&#27979;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
Nexus sine qua non: Essentially connected neural networks for spatial-temporal forecasting of multivariate time series. (arXiv:2307.01482v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01482
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32039;&#20945;&#30340;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#33410;&#28857;&#35782;&#21035;&#12289;&#23494;&#38598;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#21644;&#28040;&#24687;&#20256;&#36882;&#23618;&#26469;&#23454;&#29616;&#65292;&#19981;&#38656;&#35201;&#22797;&#26434;&#30340;&#39034;&#24207;&#27169;&#22359;&#12290;&#35813;&#27169;&#22411;&#22312;&#23454;&#35777;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#20102;&#26377;&#25928;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#27169;&#21644;&#39044;&#27979;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#19981;&#20165;&#26377;&#21161;&#20110;&#20174;&#19994;&#32773;&#30340;&#20915;&#31574;&#65292;&#36824;&#21152;&#28145;&#25105;&#20204;&#23545;&#24213;&#23618;&#21160;&#24577;&#31995;&#32479;&#30340;&#31185;&#23398;&#29702;&#35299;&#12290;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;STGNNs&#65289;&#24050;&#32463;&#25104;&#20026;&#24378;&#22823;&#30340;&#39044;&#27979;&#22120;&#65292;&#24182;&#25104;&#20026;&#23398;&#20064;&#26102;&#31354;&#34920;&#31034;&#30340;&#20107;&#23454;&#26631;&#20934;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;STGNNs&#30340;&#26550;&#26500;&#24448;&#24448;&#36890;&#36807;&#22534;&#21472;&#19968;&#31995;&#21015;&#22797;&#26434;&#30340;&#23618;&#27425;&#32780;&#21464;&#24471;&#22797;&#26434;&#12290;&#35774;&#35745;&#30340;&#27169;&#22411;&#21487;&#33021;&#22810;&#20313;&#25110;&#38590;&#20197;&#29702;&#35299;&#65292;&#36825;&#32473;&#22797;&#26434;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#24102;&#26469;&#20102;&#24040;&#22823;&#25361;&#25112;&#12290;&#36825;&#20123;&#38382;&#39064;&#20419;&#20351;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#29616;&#20195;STGNNs&#30340;&#35774;&#35745;&#65292;&#24182;&#30830;&#23450;&#23545;&#24378;&#22823;&#21644;&#39640;&#25928;&#30340;&#31070;&#32463;&#39044;&#27979;&#22120;&#26377;&#25152;&#36129;&#29486;&#30340;&#26680;&#24515;&#21407;&#21017;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32039;&#20945;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#23436;&#20840;&#30001;&#23494;&#38598;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#21644;&#28040;&#24687;&#20256;&#36882;&#23618;&#26469;&#23450;&#20041;&#65292;&#22522;&#20110;&#33410;&#28857;&#35782;&#21035;&#65292;&#27809;&#26377;&#20219;&#20309;&#22797;&#26434;&#30340;&#39034;&#24207;&#27169;&#22359;&#65292;&#20363;&#22914;TCNs&#65292;RNNs&#21644;Transformers&#12290;&#36890;&#36807;&#23454;&#35777;&#37325;&#26032;&#35780;&#20272;&#35813;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling and forecasting multivariate time series not only facilitates the decision making of practitioners, but also deepens our scientific understanding of the underlying dynamical systems. Spatial-temporal graph neural networks (STGNNs) are emerged as powerful predictors and have become the de facto models for learning spatiotemporal representations in recent years. However, existing architectures of STGNNs tend to be complicated by stacking a series of fancy layers. The designed models could be either redundant or enigmatic, which pose great challenges on their complexity and scalability. Such concerns prompt us to re-examine the designs of modern STGNNs and identify core principles that contribute to a powerful and efficient neural predictor. Here we present a compact predictive model that is fully defined by a dense encoder-decoder and a message-passing layer, powered by node identifications, without any complex sequential modules, e.g., TCNs, RNNs, and Transformers. Empirical re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#20248;&#36816;&#36755;&#21644;&#21464;&#20998;&#25512;&#26029;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36335;&#24452;&#31354;&#38388;&#25955;&#24230;&#30340;&#37319;&#26679;&#21644;&#29983;&#25104;&#24314;&#27169;&#26694;&#26550;&#12290;&#36890;&#36807;&#24320;&#21457;&#26032;&#39062;&#30340;&#22522;&#20110;&#24471;&#20998;&#30340;&#22238;&#28779;&#27969;&#25216;&#26415;&#21644;&#27491;&#21017;&#21270;&#30340;&#36845;&#20195;&#27604;&#20363;&#25311;&#21512;&#30446;&#26631;&#65292;&#26412;&#25991;&#23637;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.01050</link><description>&lt;p&gt;
&#36816;&#36755;&#12289;&#21464;&#20998;&#25512;&#26029;&#21644;&#25193;&#25955;&#65306;&#24212;&#29992;&#20110;&#22238;&#28779;&#27969;&#21644;&#34203;&#23450;&#35860;&#26725;&#30340;&#35770;&#25991;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Transport, Variational Inference and Diffusions: with Applications to Annealed Flows and Schr\"odinger Bridges. (arXiv:2307.01050v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#20248;&#36816;&#36755;&#21644;&#21464;&#20998;&#25512;&#26029;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36335;&#24452;&#31354;&#38388;&#25955;&#24230;&#30340;&#37319;&#26679;&#21644;&#29983;&#25104;&#24314;&#27169;&#26694;&#26550;&#12290;&#36890;&#36807;&#24320;&#21457;&#26032;&#39062;&#30340;&#22522;&#20110;&#24471;&#20998;&#30340;&#22238;&#28779;&#27969;&#25216;&#26415;&#21644;&#27491;&#21017;&#21270;&#30340;&#36845;&#20195;&#27604;&#20363;&#25311;&#21512;&#30446;&#26631;&#65292;&#26412;&#25991;&#23637;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#26368;&#20248;&#36816;&#36755;&#19982;&#21464;&#20998;&#25512;&#26029;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#37325;&#28857;&#30740;&#31350;&#20102;&#27491;&#21521;&#21644;&#21453;&#21521;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#20197;&#21450;Girsanov&#21464;&#25442;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36335;&#24452;&#31354;&#38388;&#25955;&#24230;&#30340;&#37319;&#26679;&#21644;&#29983;&#25104;&#24314;&#27169;&#30340;&#21407;&#21017;&#24615;&#21644;&#31995;&#32479;&#24615;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26368;&#32456;&#21457;&#23637;&#20986;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#24471;&#20998;&#30340;&#22238;&#28779;&#27969;&#25216;&#26415;&#65288;&#19982;&#32479;&#35745;&#29289;&#29702;&#20013;&#30340;Jarzynski&#21644;Crooks&#24658;&#31561;&#24335;&#26377;&#20851;&#65289;&#21644;&#19968;&#20010;&#27491;&#21017;&#21270;&#30340;&#36845;&#20195;&#27604;&#20363;&#25311;&#21512;&#65288;IPF&#65289;&#22411;&#30446;&#26631;&#65292;&#19981;&#21516;&#20110;&#26631;&#20934;IPF&#30340;&#39034;&#24207;&#24615;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#30340;&#29983;&#25104;&#24314;&#27169;&#31034;&#20363;&#21644;&#22522;&#20110;&#21452;&#20117;&#30340;&#31232;&#26377;&#20107;&#20214;&#20219;&#21153;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the connections between optimal transport and variational inference, with a focus on forward and reverse time stochastic differential equations and Girsanov transformations.We present a principled and systematic framework for sampling and generative modelling centred around divergences on path space. Our work culminates in the development of a novel score-based annealed flow technique (with connections to Jarzynski and Crooks identities from statistical physics) and a regularised iterative proportional fitting (IPF)-type objective, departing from the sequential nature of standard IPF. Through a series of generative modelling examples and a double-well-based rare event task, we showcase the potential of the proposed methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;GNN&#22312;&#31070;&#32463;&#32593;&#32476;&#29305;&#24449;&#23398;&#20064;&#29702;&#35770;&#20013;&#30340;&#20316;&#29992;&#12290; &#21457;&#29616;&#22270;&#21367;&#31215;&#32593;&#32476;&#26174;&#33879;&#22686;&#24378;&#20102;&#33391;&#24615;&#36807;&#25311;&#21512;&#21306;&#22495;&#65292;&#22312;&#36825;&#20010;&#21306;&#22495;&#20869;&#20449;&#21495;&#23398;&#20064;&#36229;&#36234;&#20102;&#22122;&#22768;&#35760;&#24518;&#12290;</title><link>http://arxiv.org/abs/2306.13926</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#20174;&#32467;&#26500;&#20449;&#24687;&#20013;&#33719;&#30410;&#30340;&#35777;&#26126;&#65306;&#19968;&#20010;&#29305;&#24449;&#23398;&#20064;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks Provably Benefit from Structural Information: A Feature Learning Perspective. (arXiv:2306.13926v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13926
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;GNN&#22312;&#31070;&#32463;&#32593;&#32476;&#29305;&#24449;&#23398;&#20064;&#29702;&#35770;&#20013;&#30340;&#20316;&#29992;&#12290; &#21457;&#29616;&#22270;&#21367;&#31215;&#32593;&#32476;&#26174;&#33879;&#22686;&#24378;&#20102;&#33391;&#24615;&#36807;&#25311;&#21512;&#21306;&#22495;&#65292;&#22312;&#36825;&#20010;&#21306;&#22495;&#20869;&#20449;&#21495;&#23398;&#20064;&#36229;&#36234;&#20102;&#22122;&#22768;&#35760;&#24518;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#22312;&#22270;&#34920;&#31034;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#20102;&#20808;&#39537;&#24615;&#36827;&#23637;&#65292;&#22312;&#22788;&#29702;&#22270;&#36755;&#20837;&#26102;&#34920;&#29616;&#20986;&#27604;&#22810;&#23618;&#24863;&#30693;&#22120;(MLPs)&#26356;&#20248;&#36234;&#30340;&#29305;&#24449;&#23398;&#20064;&#21644;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29702;&#35299;GNN&#30340;&#29305;&#24449;&#23398;&#20064;&#26041;&#38754;&#20173;&#22788;&#20110;&#21021;&#22987;&#38454;&#27573;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30740;&#31350;&#22270;&#21367;&#31215;&#22312;&#31070;&#32463;&#32593;&#32476;&#29305;&#24449;&#23398;&#20064;&#29702;&#35770;&#20013;&#30340;&#20316;&#29992;&#26469;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#20004;&#23618;&#22270;&#21367;&#31215;&#32593;&#32476;(GCNs)&#20013;&#20449;&#21495;&#23398;&#20064;&#21644;&#22122;&#22768;&#35760;&#24518;&#30340;&#19981;&#21516;&#21051;&#30011;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#20004;&#23618;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNNs)&#36827;&#34892;&#23545;&#27604;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#23545;&#24212;&#30340;CNNs&#30456;&#27604;&#65292;&#22270;&#21367;&#31215;&#32593;&#32476;&#26174;&#33879;&#22686;&#24378;&#20102;&#33391;&#24615;&#36807;&#25311;&#21512;&#21306;&#22495;&#65292;&#22312;&#36825;&#20010;&#21306;&#22495;&#20869;&#20449;&#21495;&#23398;&#20064;&#36229;&#36234;&#20102;&#22122;&#22768;&#35760;&#24518;&#65292;&#24182;&#19988;&#36817;&#20284;&#20110;&#22240;&#23376;$\sqrt{D}^{q-2}$&#65292;&#20854;&#20013;$D$&#34920;&#31034;&#33410;&#28857;&#30340;&#26399;&#26395;&#24230;&#25968;&#65292;$q$&#34920;&#31034;ReLU&#28608;&#27963;&#21151;&#33021;&#30340;&#24130;&#27425;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have pioneered advancements in graph representation learning, exhibiting superior feature learning and performance over multilayer perceptrons (MLPs) when handling graph inputs. However, understanding the feature learning aspect of GNNs is still in its initial stage. This study aims to bridge this gap by investigating the role of graph convolution within the context of feature learning theory in neural networks using gradient descent training. We provide a distinct characterization of signal learning and noise memorization in two-layer graph convolutional networks (GCNs), contrasting them with two-layer convolutional neural networks (CNNs). Our findings reveal that graph convolution significantly augments the benign overfitting regime over the counterpart CNNs, where signal learning surpasses noise memorization, by approximately factor $\sqrt{D}^{q-2}$, with $D$ denoting a node's expected degree and $q$ being the power of the ReLU activation function where 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#27604;&#20998;&#26512;&#21508;&#31181;&#35757;&#32451;&#21464;&#37327;(&#21253;&#25324;&#19981;&#21516;&#30340;SSCL&#31639;&#27861;&#12289;&#23398;&#20064;&#31574;&#30053;&#65292;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#20197;&#21450;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;)&#30340;&#26377;&#25928;&#24615;&#65292;&#30740;&#31350;&#20102;SSCL&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#24433;&#21709;&#21450;&#20855;&#20307;&#22909;&#22788;&#12290;</title><link>http://arxiv.org/abs/2306.12086</link><description>&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#23545;&#27604;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#26159;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
What Constitutes Good Contrastive Learning in Time-Series Forecasting?. (arXiv:2306.12086v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12086
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#27604;&#20998;&#26512;&#21508;&#31181;&#35757;&#32451;&#21464;&#37327;(&#21253;&#25324;&#19981;&#21516;&#30340;SSCL&#31639;&#27861;&#12289;&#23398;&#20064;&#31574;&#30053;&#65292;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#20197;&#21450;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;)&#30340;&#26377;&#25928;&#24615;&#65292;&#30740;&#31350;&#20102;SSCL&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#24433;&#21709;&#21450;&#20855;&#20307;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;(Self-Supervised Contrastive Learning, SSCL)&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;(&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;)&#30340;&#24341;&#20837;&#24050;&#32463;&#23637;&#31034;&#20102;&#22312;&#34920;&#31034;&#23398;&#20064;&#26041;&#38754;&#30340;&#26174;&#33879;&#25552;&#21319;&#12290;&#36890;&#36807;&#21033;&#29992;&#33258;&#30417;&#30563;&#30340;&#28508;&#22312;&#20248;&#21183;&#65292;SSCL&#20351;&#29992;&#22823;&#37327;&#26080;&#26631;&#31614;&#25968;&#25454;&#36827;&#34892;&#20102;&#34920;&#31034;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#12290;&#23613;&#31649;&#36825;&#20123;&#36827;&#23637;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#19968;&#20010;&#26174;&#33879;&#30340;&#24046;&#36317;&#8212;&#8212;&#21363;&#25105;&#20204;&#23545;&#20110;&#19981;&#21516;&#30340;SSCL&#31574;&#30053;&#23545;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#24615;&#33021;&#30340;&#24433;&#21709;&#20197;&#21450;SSCL&#25152;&#24102;&#26469;&#30340;&#20855;&#20307;&#22909;&#22788;&#29702;&#35299;&#19981;&#36275;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#23545;&#21508;&#31181;&#35757;&#32451;&#21464;&#37327;&#30340;&#26377;&#25928;&#24615;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#26469;&#35299;&#20915;&#36825;&#20123;&#24046;&#36317;&#65292;&#20854;&#20013;&#21253;&#25324;&#19981;&#21516;&#30340;SSCL&#31639;&#27861;&#12289;&#23398;&#20064;&#31574;&#30053;&#12289;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#20197;&#21450;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#28145;&#20837;&#20102;&#35299;SSCL&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#32972;&#26223;&#19979;&#24102;&#26469;&#30340;&#25913;&#36827;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#32463;&#39564;&#24863;&#21463;&#37326;&#30340;&#23450;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the introduction of self-supervised contrastive learning (SSCL) has demonstrated remarkable improvements in representation learning across various domains, including natural language processing and computer vision. By leveraging the inherent benefits of self-supervision, SSCL enables the pre-training of representation models using vast amounts of unlabeled data. Despite these advances, there remains a significant gap in understanding the impact of different SSCL strategies on time series forecasting performance, as well as the specific benefits that SSCL can bring. This paper aims to address these gaps by conducting a comprehensive analysis of the effectiveness of various training variables, including different SSCL algorithms, learning strategies, model architectures, and their interplay. Additionally, to gain deeper insights into the improvements brought about by SSCL in the context of time-series forecasting, a qualitative analysis of the empirical receptive field i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;GTAGC&#30340;&#22270;&#24418;&#33258;&#32534;&#30721;&#22120;&#22270;&#24418;&#21464;&#25442;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;&#22270;&#33258;&#32534;&#30721;&#22120;&#21644;&#22270;&#24418;&#21464;&#25442;&#22120;&#65292;GTAGC&#33021;&#22815;&#25429;&#33719;&#20840;&#23616;&#20381;&#36182;&#20851;&#31995;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#25552;&#39640;&#22270;&#32858;&#31867;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.11307</link><description>&lt;p&gt;
&#22686;&#24378;&#23646;&#24615;&#32858;&#31867;&#30340;&#22270;&#24418;&#21464;&#25442;&#26041;&#27861;&#65306;&#19968;&#31181;&#21019;&#26032;&#30340;&#22270;&#24418;&#36716;&#25442;&#22120;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Transforming Graphs for Enhanced Attribute-Based Clustering: An Innovative Graph Transformer Method. (arXiv:2306.11307v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11307
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;GTAGC&#30340;&#22270;&#24418;&#33258;&#32534;&#30721;&#22120;&#22270;&#24418;&#21464;&#25442;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;&#22270;&#33258;&#32534;&#30721;&#22120;&#21644;&#22270;&#24418;&#21464;&#25442;&#22120;&#65292;GTAGC&#33021;&#22815;&#25429;&#33719;&#20840;&#23616;&#20381;&#36182;&#20851;&#31995;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#25552;&#39640;&#22270;&#32858;&#31867;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#34920;&#31034;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#24433;&#21709;&#21147;&#30340;&#26041;&#27861;&#65292;&#23427;&#20351;&#24471;&#25105;&#20204;&#26356;&#28145;&#20837;&#22320;&#29702;&#35299;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#24182;&#26377;&#21161;&#20110;&#22270;&#32858;&#31867;&#65292;&#36825;&#26159;&#21508;&#20010;&#39046;&#22495;&#30340;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#12290;&#27880;&#24847;&#21147;&#26426;&#21046;&#26368;&#36817;&#24050;&#32463;&#36827;&#20837;&#20102;&#22270;&#23398;&#20064;&#30340;&#39046;&#22495;&#65292;&#36825;&#20174;&#26681;&#26412;&#19978;&#25913;&#21464;&#20102;&#30740;&#31350;&#36235;&#21183;&#12290;&#22240;&#27492;&#65292;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#21644;&#22270;&#27880;&#24847;&#21147;&#33258;&#32534;&#30721;&#22120;&#24050;&#25104;&#20026;&#22270;&#32858;&#31867;&#20219;&#21153;&#20248;&#36873;&#30340;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#37319;&#29992;&#23616;&#37096;&#27880;&#24847;&#26426;&#21046;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#29702;&#35299;&#22270;&#20013;&#33410;&#28857;&#20043;&#38388;&#22797;&#26434;&#20840;&#23616;&#20381;&#36182;&#24615;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38556;&#30861;&#65292;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#22270;&#33258;&#32534;&#30721;&#22120;&#30340;&#22270;&#24418;&#21464;&#25442;&#33258;&#32534;&#30721;&#22120;&#30340;&#22270;&#24418;&#32858;&#31867;&#65288;GTAGC&#65289;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#22270;&#33258;&#32534;&#30721;&#22120;&#19982;&#22270;&#24418;&#21464;&#25442;&#22120;&#34701;&#21512;&#65292;GTAGC&#33021;&#22815;&#25429;&#33719;&#33410;&#28857;&#20043;&#38388;&#30340;&#20840;&#23616;&#20381;&#36182;&#20851;&#31995;&#12290;&#36825;&#31181;&#34701;&#21512;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#20351;&#24471;&#22270;&#24418;&#32858;&#31867;&#20219;&#21153;&#26377;&#20102;&#26174;&#30528;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Representation Learning (GRL) is an influential methodology, enabling a more profound understanding of graph-structured data and aiding graph clustering, a critical task across various domains. The recent incursion of attention mechanisms, originally an artifact of Natural Language Processing (NLP), into the realm of graph learning has spearheaded a notable shift in research trends. Consequently, Graph Attention Networks (GATs) and Graph Attention Auto-Encoders have emerged as preferred tools for graph clustering tasks. Yet, these methods primarily employ a local attention mechanism, thereby curbing their capacity to apprehend the intricate global dependencies between nodes within graphs. Addressing these impediments, this study introduces an innovative method known as the Graph Transformer Auto-Encoder for Graph Clustering (GTAGC). By melding the Graph Auto-Encoder with the Graph Transformer, GTAGC is adept at capturing global dependencies between nodes. This integration amplifi
&lt;/p&gt;</description></item><item><title>&#20998;&#25955;SGD&#21644;&#24179;&#22343;&#26041;&#21521;SAM&#22312;&#28176;&#36817;&#24847;&#20041;&#19979;&#26159;&#31561;&#20215;&#30340;&#65292;D-SGD&#34920;&#29616;&#20986;&#26799;&#24230;&#24179;&#28369;&#25928;&#24212;&#21644;&#38160;&#24230;&#27491;&#21017;&#21270;&#25928;&#24212;&#65292;&#32780;&#19988;&#21487;&#20197;&#25552;&#39640;&#21518;&#39564;&#35780;&#20272;&#65292;&#24182;&#35777;&#26126;&#20102;&#28508;&#22312;&#30340;&#27867;&#21270;&#33021;&#21147;</title><link>http://arxiv.org/abs/2306.02913</link><description>&lt;p&gt;
&#20998;&#25955;&#21270;SGD&#21644;&#24179;&#22343;&#26041;&#21521;SAM&#22312;&#28176;&#36817;&#24847;&#20041;&#19979;&#26159;&#31561;&#20215;&#30340;
&lt;/p&gt;
&lt;p&gt;
Decentralized SGD and Average-direction SAM are Asymptotically Equivalent. (arXiv:2306.02913v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02913
&lt;/p&gt;
&lt;p&gt;
&#20998;&#25955;SGD&#21644;&#24179;&#22343;&#26041;&#21521;SAM&#22312;&#28176;&#36817;&#24847;&#20041;&#19979;&#26159;&#31561;&#20215;&#30340;&#65292;D-SGD&#34920;&#29616;&#20986;&#26799;&#24230;&#24179;&#28369;&#25928;&#24212;&#21644;&#38160;&#24230;&#27491;&#21017;&#21270;&#25928;&#24212;&#65292;&#32780;&#19988;&#21487;&#20197;&#25552;&#39640;&#21518;&#39564;&#35780;&#20272;&#65292;&#24182;&#35777;&#26126;&#20102;&#28508;&#22312;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#25955;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;D-SGD&#65289;&#20801;&#35768;&#22312;&#27809;&#26377;&#20013;&#22830;&#26381;&#21153;&#22120;&#30340;&#25511;&#21046;&#19979;&#65292;&#22823;&#37327;&#35774;&#22791;&#21516;&#26102;&#36827;&#34892;&#21327;&#20316;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#29702;&#35770;&#35748;&#20026;&#65292;&#20998;&#25955;&#21270;&#19981;&#21487;&#36991;&#20813;&#22320;&#21066;&#24369;&#20102;&#27867;&#21270;&#33021;&#21147;&#12290;&#26412;&#25991;&#25361;&#25112;&#20256;&#32479;&#20449;&#24565;&#65292;&#25552;&#20986;&#20102;&#23436;&#20840;&#26032;&#30340;&#35282;&#24230;&#26469;&#29702;&#35299;&#20998;&#25955;&#23398;&#20064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#19968;&#33324;&#38750;&#20984;&#38750;-$\beta$-&#24179;&#28369;&#35774;&#32622;&#19979;&#65292;D-SGD&#38544;&#24335;&#22320;&#26368;&#23567;&#21270;&#20102;&#24179;&#22343;&#26041;&#21521;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#65288;SAM&#65289;&#31639;&#27861;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#36825;&#31181;&#24778;&#20154;&#30340;&#28176;&#36817;&#31561;&#20215;&#25581;&#31034;&#20102;&#20869;&#22312;&#30340;&#27491;&#21017;&#21270;-&#20248;&#21270;&#26435;&#34913;&#20197;&#21450;&#20998;&#25955;&#21270;&#30340;&#19977;&#20010;&#20248;&#28857;&#65306;&#65288;1&#65289;D-SGD&#20013;&#23384;&#22312;&#19968;&#20010;&#33258;&#30001;&#30340;&#19981;&#30830;&#23450;&#24615;&#35780;&#20272;&#26426;&#21046;&#65292;&#21487;&#20197;&#25552;&#39640;&#21518;&#39564;&#20272;&#35745;&#65307;&#65288;2&#65289;D-SGD&#34920;&#29616;&#20986;&#26799;&#24230;&#24179;&#28369;&#25928;&#24212;&#65307;&#65288;3&#65289;D-SGD&#30340;&#38160;&#24230;&#27491;&#21017;&#21270;&#25928;&#24212;&#19981;&#20250;&#38543;&#30528;&#24635;&#25209;&#22788;&#29702;&#22823;&#23567;&#30340;&#22686;&#21152;&#32780;&#20943;&#23569;&#65292;&#36825;&#35777;&#26126;&#20102;&#28508;&#22312;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Decentralized stochastic gradient descent (D-SGD) allows collaborative learning on massive devices simultaneously without the control of a central server. However, existing theories claim that decentralization invariably undermines generalization. In this paper, we challenge the conventional belief and present a completely new perspective for understanding decentralized learning. We prove that D-SGD implicitly minimizes the loss function of an average-direction Sharpness-aware minimization (SAM) algorithm under general non-convex non-$\beta$-smooth settings. This surprising asymptotic equivalence reveals an intrinsic regularization-optimization trade-off and three advantages of decentralization: (1) there exists a free uncertainty evaluation mechanism in D-SGD to improve posterior estimation; (2) D-SGD exhibits a gradient smoothing effect; and (3) the sharpness regularization effect of D-SGD does not decrease as total batch size increases, which justifies the potential generalization b
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#25490;&#21015;&#20915;&#31574;&#26641;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#22797;&#26434;&#24230;&#24230;&#37327;&#26041;&#27861;&#65292;&#33021;&#22815;&#25429;&#25417;&#21040;&#25968;&#25454;&#23454;&#20363;&#30340;&#39034;&#24207;&#20381;&#36182;&#24615;&#65292;&#22312;&#19981;&#21516;&#25490;&#21015;&#30340;&#25968;&#25454;&#23454;&#20363;&#19978;&#24471;&#21040;&#19981;&#21516;&#30340;&#20915;&#31574;&#26641;&#27169;&#22411;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#20256;&#32479;&#20915;&#31574;&#26641;&#27169;&#22411;&#22312;&#22788;&#29702;&#39034;&#24207;&#30456;&#20851;&#25968;&#25454;&#26102;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2306.02617</link><description>&lt;p&gt;
&#25490;&#21015;&#20915;&#31574;&#26641;
&lt;/p&gt;
&lt;p&gt;
Permutation Decision Trees. (arXiv:2306.02617v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02617
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#25490;&#21015;&#20915;&#31574;&#26641;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#22797;&#26434;&#24230;&#24230;&#37327;&#26041;&#27861;&#65292;&#33021;&#22815;&#25429;&#25417;&#21040;&#25968;&#25454;&#23454;&#20363;&#30340;&#39034;&#24207;&#20381;&#36182;&#24615;&#65292;&#22312;&#19981;&#21516;&#25490;&#21015;&#30340;&#25968;&#25454;&#23454;&#20363;&#19978;&#24471;&#21040;&#19981;&#21516;&#30340;&#20915;&#31574;&#26641;&#27169;&#22411;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#20256;&#32479;&#20915;&#31574;&#26641;&#27169;&#22411;&#22312;&#22788;&#29702;&#39034;&#24207;&#30456;&#20851;&#25968;&#25454;&#26102;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#26641;&#26159;&#19968;&#31181;&#22522;&#20110;&#26368;&#23567;&#21270;&#20869;&#37096;&#33410;&#28857;&#20013;&#30340;&#19981;&#32431;&#24230;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#26368;&#24120;&#35265;&#30340;&#19981;&#32431;&#24230;&#24230;&#37327;&#26159;&#39321;&#20892;&#29109;&#21644;&#22522;&#23612;&#19981;&#32431;&#24230;&#12290;&#36825;&#20123;&#19981;&#32431;&#24230;&#24230;&#37327;&#23545;&#35757;&#32451;&#25968;&#25454;&#30340;&#39034;&#24207;&#19981;&#25935;&#24863;&#65292;&#22240;&#27492;&#24471;&#21040;&#30340;&#26368;&#32456;&#26641;&#23545;&#25968;&#25454;&#30340;&#20219;&#20309;&#25490;&#21015;&#37117;&#26159;&#19981;&#21464;&#30340;&#12290;&#36825;&#23548;&#33268;&#20102;&#22312;&#24314;&#27169;&#23384;&#22312;&#39034;&#24207;&#20381;&#36182;&#24615;&#30340;&#25968;&#25454;&#23454;&#20363;&#26102;&#30340;&#20005;&#37325;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20351;&#29992;&#8220;&#21387;&#32553;&#21162;&#21147;&#8221;(ETC) - &#19968;&#31181;&#22797;&#26434;&#24230;&#24230;&#37327;&#65292;&#20316;&#20026;&#19981;&#32431;&#24230;&#24230;&#37327;&#12290;&#19982;&#39321;&#20892;&#29109;&#21644;&#22522;&#23612;&#19981;&#32431;&#24230;&#19981;&#21516;&#65292;&#22522;&#20110;ETC&#30340;&#32467;&#26500;&#24615;&#19981;&#32431;&#24230;&#33021;&#22815;&#25429;&#25417;&#21040;&#25968;&#25454;&#30340;&#39034;&#24207;&#20381;&#36182;&#24615;&#65292;&#20174;&#32780;&#20026;&#30456;&#21516;&#25968;&#25454;&#23454;&#20363;&#30340;&#19981;&#21516;&#25490;&#21015;&#33719;&#24471;&#28508;&#22312;&#19981;&#21516;&#30340;&#20915;&#31574;&#26641;&#65288;&#25490;&#21015;&#20915;&#31574;&#26641;&#65289;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20351;&#29992;&#25490;&#21015;&#20915;&#31574;&#26641;&#23454;&#29616;&#25490;&#21015;Bagging&#30340;&#27010;&#24565;&#65292;&#32780;&#26080;&#38656;&#38543;&#26426;&#29305;&#24449;&#36873;&#25321;&#21644;&#23376;&#37319;&#26679;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#32763;&#35793;&#21253;&#8230;(&#20449;&#24687;&#19981;&#20840;)
&lt;/p&gt;
&lt;p&gt;
Decision Tree is a well understood Machine Learning model that is based on minimizing impurities in the internal nodes. The most common impurity measures are Shannon entropy and Gini impurity. These impurity measures are insensitive to the order of training data and hence the final tree obtained is invariant to any permutation of the data. This leads to a serious limitation in modeling data instances that have order dependencies. In this work, we propose the use of Effort-To-Compress (ETC) - a complexity measure, for the first time, as an impurity measure. Unlike Shannon entropy and Gini impurity, structural impurity based on ETC is able to capture order dependencies in the data, thus obtaining potentially different decision trees for different permutations of the same data instances (Permutation Decision Trees). We then introduce the notion of Permutation Bagging achieved using permutation decision trees without the need for random feature selection and sub-sampling. We compare the pe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#30446;&#26631;&#23398;&#20064;&#20013;&#30340;&#19977;&#37325;&#26435;&#34913;&#38382;&#39064;&#65292;&#36890;&#36807;&#30740;&#31350;&#21160;&#24577;&#21152;&#26435;&#31639;&#27861;&#22312;MoDo&#31639;&#27861;&#20013;&#30340;&#27867;&#21270;&#24615;&#33021;&#21644;&#19982;&#20248;&#21270;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#21457;&#29616;&#20102;&#21160;&#24577;&#21152;&#26435;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.20057</link><description>&lt;p&gt;
&#22810;&#30446;&#26631;&#23398;&#20064;&#20013;&#30340;&#19977;&#37325;&#26435;&#34913;&#65306;&#20248;&#21270;&#12289;&#27867;&#21270;&#21644;&#20914;&#31361;&#36991;&#20813;
&lt;/p&gt;
&lt;p&gt;
Three-Way Trade-Off in Multi-Objective Learning: Optimization, Generalization and Conflict-Avoidance. (arXiv:2305.20057v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.20057
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#30446;&#26631;&#23398;&#20064;&#20013;&#30340;&#19977;&#37325;&#26435;&#34913;&#38382;&#39064;&#65292;&#36890;&#36807;&#30740;&#31350;&#21160;&#24577;&#21152;&#26435;&#31639;&#27861;&#22312;MoDo&#31639;&#27861;&#20013;&#30340;&#27867;&#21270;&#24615;&#33021;&#21644;&#19982;&#20248;&#21270;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#21457;&#29616;&#20102;&#21160;&#24577;&#21152;&#26435;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26032;&#20852;&#30340;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#20013;&#65292;&#24403;&#23384;&#22312;&#22810;&#20010;&#23398;&#20064;&#20934;&#21017;&#25110;&#22810;&#20010;&#23398;&#20064;&#20219;&#21153;&#26102;&#65292;&#22810;&#30446;&#26631;&#23398;&#20064;&#65288;MOL&#65289;&#38382;&#39064;&#32463;&#24120;&#20986;&#29616;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#21508;&#31181;&#21160;&#24577;&#21152;&#26435;&#31639;&#27861;&#29992;&#20110;MOL&#65292;&#22914;MGDA&#21450;&#20854;&#21464;&#31181;&#65292;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#25214;&#21040;&#19968;&#20010;&#33021;&#22815;&#36991;&#20813;&#30446;&#26631;&#20914;&#31361;&#30340;&#26356;&#26032;&#26041;&#21521;&#12290;&#23613;&#31649;&#20854;&#30452;&#35266;&#21560;&#24341;&#20154;&#65292;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#21160;&#24577;&#21152;&#26435;&#26041;&#27861;&#24182;&#19981;&#24635;&#26159;&#20248;&#20110;&#38745;&#24577;&#26041;&#27861;&#12290;&#20026;&#20102;&#29702;&#35299;&#36825;&#19968;&#29702;&#35770;&#19982;&#23454;&#36341;&#30340;&#24046;&#36317;&#65292;&#25105;&#20204;&#37325;&#28857;&#30740;&#31350;&#20102;MGDA&#30340;&#26032;&#38543;&#26426;&#21464;&#20307;-&#22810;&#30446;&#26631;&#26799;&#24230;&#21452;&#37319;&#26679;&#65288;MoDo&#65289;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#31639;&#27861;&#31283;&#23450;&#24615;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#22522;&#20110;&#21160;&#24577;&#21152;&#26435;&#30340;MoDo&#31639;&#27861;&#30340;&#27867;&#21270;&#24615;&#33021;&#20197;&#21450;&#20854;&#19982;&#20248;&#21270;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#20986;&#20046;&#24847;&#26009;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;MGDA&#32972;&#21518;&#30340;&#20851;&#38190;&#21407;&#29702;-&#27839;&#30528;&#36991;&#20813;&#20914;&#31361;&#30340;&#26041;&#21521;&#36827;&#34892;&#26356;&#26032;-&#21487;&#33021;&#20250;&#38459;&#30861;&#21160;&#24577;&#21152;&#26435;&#31639;&#27861;&#23454;&#29616;${\cal O}(1/\sqrt{n})$&#30340;&#26368;&#20248;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-objective learning (MOL) problems often arise in emerging machine learning problems when there are multiple learning criteria or multiple learning tasks. Recent works have developed various dynamic weighting algorithms for MOL such as MGDA and its variants, where the central idea is to find an update direction that avoids conflicts among objectives. Albeit its appealing intuition, empirical studies show that dynamic weighting methods may not always outperform static ones. To understand this theory-practical gap, we focus on a new stochastic variant of MGDA - the Multi-objective gradient with Double sampling (MoDo) algorithm, and study the generalization performance of the dynamic weighting-based MoDo and its interplay with optimization through the lens of algorithm stability. Perhaps surprisingly, we find that the key rationale behind MGDA -- updating along conflict-avoidant direction - may hinder dynamic weighting algorithms from achieving the optimal ${\cal O}(1/\sqrt{n})$ popu
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#25193;&#25955;&#27169;&#22411;MADiff&#65292;&#35299;&#20915;&#20102;&#22810;&#26234;&#33021;&#20307;&#38382;&#39064;&#65292;&#26159;&#31532;&#19968;&#20010;&#25193;&#25955;&#27169;&#22411;&#24212;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#31163;&#32447;RL&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2305.17330</link><description>&lt;p&gt;
MADiff&#65306;&#31163;&#32447;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#19982;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MADiff: Offline Multi-agent Learning with Diffusion Models. (arXiv:2305.17330v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17330
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#25193;&#25955;&#27169;&#22411;MADiff&#65292;&#35299;&#20915;&#20102;&#22810;&#26234;&#33021;&#20307;&#38382;&#39064;&#65292;&#26159;&#31532;&#19968;&#20010;&#25193;&#25955;&#27169;&#22411;&#24212;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#31163;&#32447;RL&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#65288;DM&#65289;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#26368;&#36817;&#22312;&#21253;&#25324;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#22312;&#20869;&#30340;&#21508;&#31181;&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20854;&#20013;&#31574;&#30053;&#36890;&#36807;&#22312;&#22312;&#32447;&#35780;&#20272;&#20013;&#20135;&#29983;&#36712;&#36857;&#26469;&#36827;&#34892;&#35268;&#21010;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#21333;&#26234;&#33021;&#20307;&#23398;&#20064;&#26174;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#20294;&#20173;&#19981;&#28165;&#26970;DM&#22914;&#20309;&#22312;&#22810;&#26234;&#33021;&#20307;&#38382;&#39064;&#20013;&#25805;&#20316;&#65292;&#20854;&#20013;&#20195;&#29702;&#21830;&#24456;&#38590;&#22312;&#29420;&#31435;&#24314;&#27169;&#27599;&#20010;&#20195;&#29702;&#21830;&#36712;&#36857;&#30340;&#24773;&#20917;&#19979;&#23436;&#25104;&#22242;&#38431;&#21512;&#20316;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;MADiff&#65292;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#24335;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;MADiff&#26159;&#36890;&#36807;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#23454;&#29616;&#23545;&#22810;&#20010;&#25193;&#25955;&#26234;&#33021;&#20307;&#34892;&#20026;&#30340;&#22797;&#26434;&#21327;&#35843;&#24314;&#27169;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;MADiff&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#25193;&#25955;&#30340;&#22810;&#26234;&#33021;&#20307;&#31163;&#32447;RL&#26694;&#26550;&#65292;&#23427;&#26082;&#21487;&#20197;&#34892;&#20026;&#20026;&#20998;&#25955;&#30340;&#25919;&#31574;&#65292;&#21448;&#21487;&#20197;&#20026;&#38598;&#20013;&#25511;&#21046;&#22120;&#65292;&#20854;&#20013;&#21253;&#25324;&#23545;&#25163;&#24314;&#27169;&#65292;&#24182;&#21487;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#36712;&#36857;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion model (DM), as a powerful generative model, recently achieved huge success in various scenarios including offline reinforcement learning, where the policy learns to conduct planning by generating trajectory in the online evaluation. However, despite the effectiveness shown for single-agent learning, it remains unclear how DMs can operate in multi-agent problems, where agents can hardly complete teamwork without good coordination by independently modeling each agent's trajectories. In this paper, we propose MADiff, a novel generative multi-agent learning framework to tackle this problem. MADiff is realized with an attention-based diffusion model to model the complex coordination among behaviors of multiple diffusion agents. To the best of our knowledge, MADiff is the first diffusion-based multi-agent offline RL framework, which behaves as both a decentralized policy and a centralized controller, which includes opponent modeling and can be used for multi-agent trajectory predic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#27867;&#21270;&#35823;&#24046;&#30340;&#26032;&#19979;&#30028;&#65292;&#25506;&#35752;&#20102;&#23398;&#20064;&#24179;&#28369;&#20989;&#25968;&#26102;&#38656;&#35201;&#30340;&#26679;&#26412;&#25968;&#37327;&#21450;&#20854;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.16014</link><description>&lt;p&gt;
&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#38656;&#35201;&#22810;&#23569;&#26679;&#26412;&#25165;&#33021;&#21033;&#29992;&#24179;&#28369;&#24615;&#65311;
&lt;/p&gt;
&lt;p&gt;
How many samples are needed to leverage smoothness?. (arXiv:2305.16014v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#27867;&#21270;&#35823;&#24046;&#30340;&#26032;&#19979;&#30028;&#65292;&#25506;&#35752;&#20102;&#23398;&#20064;&#24179;&#28369;&#20989;&#25968;&#26102;&#38656;&#35201;&#30340;&#26679;&#26412;&#25968;&#37327;&#21450;&#20854;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32479;&#35745;&#23398;&#20064;&#30340;&#26680;&#24515;&#21407;&#21017;&#20043;&#19968;&#26159;&#65292;&#30446;&#26631;&#20989;&#25968;&#30340;&#24179;&#28369;&#24615;&#21487;&#20197;&#25171;&#30772;&#32500;&#24230;&#28798;&#38590;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#27888;&#21202;&#23637;&#24320;&#23398;&#20064;&#24179;&#28369;&#20989;&#25968;&#38656;&#35201;&#36275;&#22815;&#25509;&#36817;&#19968;&#36215;&#30340;&#26679;&#26412;&#26469;&#33719;&#24471;&#39640;&#38454;&#23548;&#25968;&#30340;&#26377;&#24847;&#20041;&#20272;&#35745;&#65292;&#36825;&#22312;&#25968;&#25454;&#37327;&#30456;&#23545;&#36739;&#23567;&#30340;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#20013;&#20284;&#20046;&#24456;&#22256;&#38590;&#12290;&#26412;&#25991;&#36890;&#36807;&#25512;&#23548;&#24191;&#20041;&#27867;&#21270;&#35823;&#24046;&#30340;&#26032;&#30340;&#19979;&#30028;&#65292;&#30740;&#31350;&#20102;&#24120;&#25968;&#21644;&#30636;&#24577;&#21306;&#22495;&#22312;&#23454;&#36341;&#20013;&#36890;&#24120;&#34987;&#24573;&#30053;&#21364;&#21457;&#25381;&#20102;&#20027;&#23548;&#20316;&#29992;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
A core principle in statistical learning is that smoothness of target functions allows to break the curse of dimensionality. However, learning a smooth function through Taylor expansions requires enough samples close to one another to get meaningful estimate of high-order derivatives, which seems hard in machine learning problems where the ratio between number of data and input dimension is relatively small. Should we really hope to break the curse of dimensionality based on Taylor expansion estimation? What happens if Taylor expansions are replaced by Fourier or wavelet expansions? By deriving a new lower bound on the generalization error, this paper investigates the role of constants and transitory regimes which are usually not depicted beyond classical learning theory statements while that play a dominant role in practice.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#31038;&#20250;&#20915;&#23450;&#22240;&#32032;&#23545;&#37325;&#30151;&#30417;&#25252;&#23460;&#20013;&#20581;&#24247;&#39044;&#27979;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#23545;&#20110;&#19968;&#33324;&#24739;&#32773;&#32676;&#20307;&#26469;&#35828;&#65292;SDOH&#29305;&#24449;&#24182;&#19981;&#33021;&#25552;&#39640;&#39044;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20294;&#23545;&#20110;&#29305;&#23450;&#20122;&#32676;&#20307;&#30340;&#25968;&#25454;&#21463;&#38480;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#20855;&#26377;&#25913;&#21892;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.12622</link><description>&lt;p&gt;
&#22312;&#37325;&#30151;&#30417;&#25252;&#23460;&#20013;&#35780;&#20272;&#31038;&#20250;&#20915;&#23450;&#22240;&#32032;&#23545;&#20581;&#24247;&#39044;&#27979;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Impact of Social Determinants on Health Prediction in the Intensive Care Unit. (arXiv:2305.12622v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12622
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#31038;&#20250;&#20915;&#23450;&#22240;&#32032;&#23545;&#37325;&#30151;&#30417;&#25252;&#23460;&#20013;&#20581;&#24247;&#39044;&#27979;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#23545;&#20110;&#19968;&#33324;&#24739;&#32773;&#32676;&#20307;&#26469;&#35828;&#65292;SDOH&#29305;&#24449;&#24182;&#19981;&#33021;&#25552;&#39640;&#39044;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20294;&#23545;&#20110;&#29305;&#23450;&#20122;&#32676;&#20307;&#30340;&#25968;&#25454;&#21463;&#38480;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#20855;&#26377;&#25913;&#21892;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20581;&#24247;&#30340;&#31038;&#20250;&#20915;&#23450;&#22240;&#32032;&#65288;SDOH&#65289;&#9472;&#9472;&#20154;&#20204;&#23621;&#20303;&#12289;&#25104;&#38271;&#21644;&#32769;&#21435;&#30340;&#26465;&#20214;&#9472;&#9472;&#23545;&#19968;&#20010;&#20154;&#30340;&#20581;&#24247;&#21644;&#24184;&#31119;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#20154;&#21475;&#20581;&#24247;&#30740;&#31350;&#20013;&#30340;&#22823;&#37327;&#26377;&#21147;&#35777;&#25454;&#26174;&#31034;&#65292;&#21508;&#31181;&#21508;&#26679;&#30340;SDOH&#19982;&#20581;&#24247;&#32467;&#26524;&#23494;&#20999;&#30456;&#20851;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#22522;&#20110;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#30340;&#39118;&#38505;&#39044;&#27979;&#27169;&#22411;&#24182;&#27809;&#26377;&#28085;&#30422;&#20840;&#38754;&#30340;SDOH&#29305;&#24449;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#24120;&#22122;&#22768;&#24456;&#22823;&#25110;&#32773;&#26681;&#26412;&#26080;&#27861;&#33719;&#21462;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#23558;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;EHR&#25968;&#25454;&#24211;MIMIC-IV&#19982;&#34987;&#20805;&#20998;&#35760;&#24405;&#30340;SDOH&#29305;&#24449;&#32852;&#31995;&#36215;&#26469;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20123;&#29305;&#24449;&#23545;&#19981;&#21516;&#24739;&#32773;&#32676;&#20307;&#30340;&#24120;&#35265;EHR&#39044;&#27979;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23545;&#20110;&#19968;&#33324;&#30340;&#24739;&#32773;&#32676;&#20307;&#26469;&#35828;&#65292;&#31038;&#21306;&#23618;&#38754;&#30340;SDOH&#29305;&#24449;&#24182;&#19981;&#33021;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20294;&#21487;&#20197;&#25552;&#39640;&#29305;&#23450;&#20122;&#32676;&#20307;&#30340;&#25968;&#25454;&#21463;&#38480;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#65292;SDOH&#29305;&#24449;&#23545;&#20110;&#36827;&#34892;&#35814;&#23613;&#30340;&#31639;&#27861;&#20559;&#35265;&#23457;&#35745;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social determinants of health (SDOH) -- the conditions in which people live, grow, and age -- play a crucial role in a person's health and well-being. There is a large, compelling body of evidence in population health studies showing that a wide range of SDOH is strongly correlated with health outcomes. Yet, a majority of the risk prediction models based on electronic health records (EHR) do not incorporate a comprehensive set of SDOH features as they are often noisy or simply unavailable. Our work links a publicly available EHR database, MIMIC-IV, to well-documented SDOH features. We investigate the impact of such features on common EHR prediction tasks across different patient populations. We find that community-level SDOH features do not improve model performance for a general patient population, but can improve data-limited model fairness for specific subpopulations. We also demonstrate that SDOH features are vital for conducting thorough audits of algorithmic biases beyond protect
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#35843;&#25972;Whisper&#27169;&#22411;&#65292;&#25104;&#21151;&#36866;&#24212;&#26410;&#35265;&#36807;&#30340;&#19977;&#20010;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#30340;&#25552;&#31034;&#27604;&#40664;&#35748;&#25552;&#31034;&#24615;&#33021;&#25552;&#21319;&#20102;10%&#21040;45&#65285;&#65292;&#23637;&#29616;&#20102;Whisper&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#22810;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.11095</link><description>&lt;p&gt;
&#28608;&#21457;Web&#35268;&#27169;&#35821;&#38899;&#27169;&#22411;&#30340;&#28508;&#22312;&#33021;&#21147;&#20197;&#23454;&#29616;&#38646;-shot&#20219;&#21153;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Prompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot Task Generalization. (arXiv:2305.11095v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11095
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#35843;&#25972;Whisper&#27169;&#22411;&#65292;&#25104;&#21151;&#36866;&#24212;&#26410;&#35265;&#36807;&#30340;&#19977;&#20010;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#30340;&#25552;&#31034;&#27604;&#40664;&#35748;&#25552;&#31034;&#24615;&#33021;&#25552;&#21319;&#20102;10%&#21040;45&#65285;&#65292;&#23637;&#29616;&#20102;Whisper&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#22810;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;Web&#35268;&#27169;&#35821;&#38899;&#27169;&#22411;Whisper&#30340;&#26032;&#20852;&#21151;&#33021;&#65292;&#22312;&#20351;&#29992;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#35843;&#25972;&#27169;&#22411;&#21518;&#65292;&#36866;&#24212;&#20102;&#26410;&#35265;&#36807;&#30340;AVSR&#65292;CS-ASR&#21644;ST&#19977;&#20010;&#20219;&#21153;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#29305;&#23450;&#20110;&#20219;&#21153;&#30340;&#25552;&#31034;&#65292;&#35201;&#20040;&#21033;&#29992;&#21478;&#19968;&#20010;&#22823;&#35268;&#27169;&#27169;&#22411;&#65292;&#35201;&#20040;&#31616;&#21333;&#22320;&#25805;&#20316;&#40664;&#35748;&#25552;&#31034;&#20013;&#30340;&#29305;&#27530;&#26631;&#35760;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#40664;&#35748;&#25552;&#31034;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#25552;&#31034;&#20351;&#36825;&#19977;&#20010;&#38646;-shot&#20219;&#21153;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;10%&#21040;45&#65285;&#65292;&#29978;&#33267;&#22312;&#19968;&#20123;&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;SotA&#30417;&#30563;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;Whisper&#30340;&#35768;&#22810;&#26377;&#36259;&#23646;&#24615;&#65292;&#21253;&#25324;&#20854;&#25552;&#31034;&#30340;&#40065;&#26834;&#24615;&#65292;&#23545;&#21475;&#38899;&#30340;&#20559;&#22909;&#20197;&#21450;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#22810;&#35821;&#35328;&#29702;&#35299;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/jasonppy/PromptingWhisper&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the emergent abilities of the recently proposed web-scale speech model Whisper, by adapting it to unseen tasks with prompt engineering. We selected three tasks: audio-visual speech recognition (AVSR), code-switched speech recognition (CS-ASR), and speech translation (ST) on unseen language pairs. We design task-specific prompts, by either leveraging another large-scale model, or simply manipulating the special tokens in the default prompts. Experiments show that compared to the default prompts, our proposed prompts improve performance by 10% to 45% on the three zero-shot tasks, and even outperform SotA supervised models on some datasets. In addition, our experiments reveal many interesting properties of Whisper, including its robustness to prompts, bias on accents, and the multilingual understanding in its latent space. Code is available at https://github.com/jasonppy/PromptingWhisper
&lt;/p&gt;</description></item><item><title>Seq-HGNN&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#20855;&#26377;&#24207;&#21015;&#33410;&#28857;&#34920;&#31034;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#31181;&#24207;&#21015;&#33410;&#28857;&#34920;&#31034;&#26426;&#21046;&#65292;&#36991;&#20813;&#20102;&#30001;&#21333;&#20010;&#21521;&#37327;&#33410;&#28857;&#34920;&#31034;&#24341;&#36215;&#30340;&#20449;&#24687;&#20002;&#22833;&#12290;</title><link>http://arxiv.org/abs/2305.10771</link><description>&lt;p&gt;
Seq-HGNN: &#23398;&#20064;&#24322;&#26500;&#22270;&#19978;&#30340;&#24207;&#21015;&#33410;&#28857;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Seq-HGNN: Learning Sequential Node Representation on Heterogeneous Graph. (arXiv:2305.10771v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10771
&lt;/p&gt;
&lt;p&gt;
Seq-HGNN&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#20855;&#26377;&#24207;&#21015;&#33410;&#28857;&#34920;&#31034;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#31181;&#24207;&#21015;&#33410;&#28857;&#34920;&#31034;&#26426;&#21046;&#65292;&#36991;&#20813;&#20102;&#30001;&#21333;&#20010;&#21521;&#37327;&#33410;&#28857;&#34920;&#31034;&#24341;&#36215;&#30340;&#20449;&#24687;&#20002;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20449;&#24687;&#26816;&#32034;&#24212;&#29992;&#20013;&#65292;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HGNN&#65289;&#24471;&#21040;&#20102;&#36805;&#36895;&#21457;&#23637;&#12290;&#35768;&#22810;&#29616;&#26377;&#30340;HGNN&#35774;&#35745;&#20102;&#22810;&#31181;&#37327;&#36523;&#23450;&#21046;&#30340;&#22270;&#21367;&#31215;&#26469;&#25429;&#33719;&#24322;&#26500;&#22270;&#20013;&#30340;&#32467;&#26500;&#21644;&#35821;&#20041;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;HGNN&#36890;&#24120;&#23558;&#27599;&#20010;&#33410;&#28857;&#34920;&#31034;&#20026;&#22810;&#23618;&#22270;&#21367;&#31215;&#35745;&#31639;&#20013;&#30340;&#21333;&#20010;&#21521;&#37327;&#65292;&#36825;&#20351;&#24471;&#39640;&#23618;&#22270;&#21367;&#31215;&#23618;&#26080;&#27861;&#21306;&#20998;&#26469;&#33258;&#19981;&#21516;&#20851;&#31995;&#21644;&#19981;&#21516;&#39034;&#24207;&#30340;&#20449;&#24687;&#65292;&#23548;&#33268;&#20449;&#24687;&#20256;&#36882;&#20013;&#30340;&#20449;&#24687;&#20002;&#22833;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#21363;Seq-HGNN&#65292;&#23427;&#20855;&#26377;&#24207;&#21015;&#33410;&#28857;&#34920;&#31034;&#12290;&#20026;&#20102;&#36991;&#20813;&#30001;&#21333;&#20010;&#21521;&#37327;&#33410;&#28857;&#34920;&#31034;&#24341;&#36215;&#30340;&#20449;&#24687;&#20002;&#22833;&#65292;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#31181;&#24207;&#21015;&#33410;&#28857;&#34920;&#31034;&#23398;&#20064;&#26426;&#21046;&#65292;&#22312;&#33410;&#28857;&#20449;&#24687;&#20256;&#36882;&#26399;&#38388;&#23558;&#27599;&#20010;&#33410;&#28857;&#34920;&#31034;&#20026;&#19968;&#31995;&#21015;&#20803;&#36335;&#24452;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed the rapid development of heterogeneous graph neural networks (HGNNs) in information retrieval (IR) applications. Many existing HGNNs design a variety of tailor-made graph convolutions to capture structural and semantic information in heterogeneous graphs. However, existing HGNNs usually represent each node as a single vector in the multi-layer graph convolution calculation, which makes the high-level graph convolution layer fail to distinguish information from different relations and different orders, resulting in the information loss in the message passing. %insufficient mining of information. To this end, we propose a novel heterogeneous graph neural network with sequential node representation, namely Seq-HGNN. To avoid the information loss caused by the single vector node representation, we first design a sequential node representation learning mechanism to represent each node as a sequence of meta-path representations during the node message passing. The
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#38754;&#21521;&#19977;&#32500;MOT&#20013;&#30340;&#28857;&#20113;&#20877;&#35782;&#21035;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#21305;&#37197;&#22836;&#29992;&#20110;&#28857;&#20113;ReID&#30340;&#32593;&#32476;&#65292;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#38543;&#30528;&#20256;&#24863;&#22120;&#20998;&#36776;&#29575;&#30340;&#25552;&#39640;&#21644;&#35266;&#27979;&#28857;&#23494;&#24230;&#30340;&#22686;&#21152;&#65292;&#28857;&#20113;ReID&#30340;&#34920;&#29616;&#36880;&#28176;&#25509;&#36817;&#20110;&#22270;&#20687;ReID&#12290;</title><link>http://arxiv.org/abs/2305.10210</link><description>&lt;p&gt;
&#38754;&#21521;&#19977;&#32500;MOT&#20013;&#30340;&#28857;&#20113;&#30446;&#26631;&#20877;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Towards Object Re-Identification from Point Clouds for 3D MOT. (arXiv:2305.10210v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10210
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#38754;&#21521;&#19977;&#32500;MOT&#20013;&#30340;&#28857;&#20113;&#20877;&#35782;&#21035;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#21305;&#37197;&#22836;&#29992;&#20110;&#28857;&#20113;ReID&#30340;&#32593;&#32476;&#65292;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#38543;&#30528;&#20256;&#24863;&#22120;&#20998;&#36776;&#29575;&#30340;&#25552;&#39640;&#21644;&#35266;&#27979;&#28857;&#23494;&#24230;&#30340;&#22686;&#21152;&#65292;&#28857;&#20113;ReID&#30340;&#34920;&#29616;&#36880;&#28176;&#25509;&#36817;&#20110;&#22270;&#20687;ReID&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#23398;&#20064;&#20174;&#21098;&#35009;&#30340;&#28857;&#20113;&#35266;&#27979;&#20013;&#21305;&#37197;&#23545;&#35937;&#23545;&#65288;&#20363;&#22914;&#20351;&#29992;&#20854;&#39044;&#27979;&#30340;&#19977;&#32500;&#36793;&#30028;&#26694;&#65289;&#26469;&#35299;&#20915;&#19977;&#32500;&#22810;&#30446;&#26631;&#36319;&#36394;&#65288;MOT&#65289;&#19978;&#30340;&#23545;&#35937;&#20877;&#35782;&#21035;&#65288;ReID&#65289;&#38382;&#39064;&#12290; &#25105;&#20204;&#19981;&#20851;&#24515;&#19977;&#32500;MOT&#30340;SOTA&#24615;&#33021;&#65292;&#32780;&#26159;&#36861;&#27714;&#22238;&#31572;&#20197;&#19979;&#38382;&#39064;&#65306;&#22312;&#23454;&#38469;&#30340;&#36319;&#36394;&#26816;&#27979;&#29615;&#22659;&#20013;&#65292;&#19982;&#22270;&#29255;&#20013;&#30340;ReID&#30456;&#27604;&#65292;&#26469;&#33258;&#28857;&#20113;&#30340;&#23545;&#35937;ReID&#30340;&#34920;&#29616;&#22914;&#20309;&#65311; &#20026;&#20102;&#23454;&#29616;&#36825;&#26679;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#36830;&#25509;&#21040;&#20219;&#20309;&#38598;&#21512;&#25110;&#24207;&#21015;&#22788;&#29702;&#39592;&#24178;&#65288;&#20363;&#22914;PointNet&#25110;ViT&#65289;&#30340;&#36731;&#37327;&#32423;&#21305;&#37197;&#22836;&#65292;&#20026;&#20004;&#31181;&#27169;&#24577;&#21019;&#36896;&#21487;&#27604;&#36739;&#30340;&#23545;&#35937;ReID&#32593;&#32476;&#23478;&#26063;&#12290;&#22312;&#23402;&#29983;&#26679;&#24335;&#19979;&#36816;&#34892;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#28857;&#20113;ReID&#32593;&#32476;&#21487;&#20197;&#22312;&#23454;&#26102;&#65288;10 hz&#65289;&#20013;&#36827;&#34892;&#25968;&#21315;&#20010;&#25104;&#23545;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20854;&#34920;&#29616;&#38543;&#30528;&#26356;&#39640;&#30340;&#20256;&#24863;&#22120;&#20998;&#36776;&#29575;&#32780;&#25552;&#39640;&#65292;&#24182;&#22312;&#35266;&#27979;&#36275;&#22815;&#23494;&#38598;&#26102;&#25509;&#36817;&#22270;&#20687;ReID&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we study the problem of object re-identification (ReID) in a 3D multi-object tracking (MOT) context, by learning to match pairs of objects from cropped (e.g., using their predicted 3D bounding boxes) point cloud observations. We are not concerned with SOTA performance for 3D MOT, however. Instead, we seek to answer the following question: In a realistic tracking by-detection context, how does object ReID from point clouds perform relative to ReID from images? To enable such a study, we propose a lightweight matching head that can be concatenated to any set or sequence processing backbone (e.g., PointNet or ViT), creating a family of comparable object ReID networks for both modalities. Run in siamese style, our proposed point-cloud ReID networks can make thousands of pairwise comparisons in real-time (10 hz). Our findings demonstrate that their performance increases with higher sensor resolution and approaches that of image ReID when observations are sufficiently dense. Ad
&lt;/p&gt;</description></item><item><title>&#8220;&#26080;&#27861;&#23398;&#20064;&#30340;&#26679;&#26412;&#8221;&#25552;&#20986;&#19968;&#31181;&#23545;&#25968;&#25454;&#36827;&#34892;&#20445;&#25252;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#26080;&#27861;&#38459;&#27490;&#26410;&#32463;&#25480;&#26435;&#30340;&#29992;&#25143;&#23545;&#20445;&#25252;&#21518;&#30340;&#25968;&#25454;&#36827;&#34892;&#21033;&#29992;&#12290;&#36890;&#36807;&#25552;&#20986;&#8220;&#21487;&#23398;&#20064;&#30340;&#26410;&#32463;&#25480;&#26435;&#31034;&#20363;&#8221;&#21644;&#19968;&#31181;&#26032;&#30340;&#32431;&#21270;&#36807;&#31243;&#65292;&#25105;&#20204;&#21487;&#20197;&#23454;&#29616;&#23545;&#25968;&#25454;&#30340;&#26356;&#22909;&#20445;&#25252;&#12290;</title><link>http://arxiv.org/abs/2305.09241</link><description>&lt;p&gt;
&#26080;&#27861;&#23398;&#20064;&#30340;&#26679;&#26412;&#32473;&#20986;&#20102;&#19968;&#31181;&#34394;&#20551;&#30340;&#23433;&#20840;&#24863;&#65306;&#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#20363;&#23376;&#31359;&#36879;&#37027;&#20123;&#26080;&#27861;&#21033;&#29992;&#30340;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Unlearnable Examples Give a False Sense of Security: Piercing through Unexploitable Data with Learnable Examples. (arXiv:2305.09241v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09241
&lt;/p&gt;
&lt;p&gt;
&#8220;&#26080;&#27861;&#23398;&#20064;&#30340;&#26679;&#26412;&#8221;&#25552;&#20986;&#19968;&#31181;&#23545;&#25968;&#25454;&#36827;&#34892;&#20445;&#25252;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#26080;&#27861;&#38459;&#27490;&#26410;&#32463;&#25480;&#26435;&#30340;&#29992;&#25143;&#23545;&#20445;&#25252;&#21518;&#30340;&#25968;&#25454;&#36827;&#34892;&#21033;&#29992;&#12290;&#36890;&#36807;&#25552;&#20986;&#8220;&#21487;&#23398;&#20064;&#30340;&#26410;&#32463;&#25480;&#26435;&#31034;&#20363;&#8221;&#21644;&#19968;&#31181;&#26032;&#30340;&#32431;&#21270;&#36807;&#31243;&#65292;&#25105;&#20204;&#21487;&#20197;&#23454;&#29616;&#23545;&#25968;&#25454;&#30340;&#26356;&#22909;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#19979;&#38543;&#22788;&#21487;&#35265;&#30340;&#23433;&#20840;&#28431;&#27934;&#20013;&#65292;&#20445;&#25252;&#25968;&#25454;&#20813;&#20110;&#26410;&#32463;&#25480;&#26435;&#30340;&#21033;&#29992;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#26368;&#36817;&#65292;&#19968;&#31181;&#21483;&#20570;&#8220;&#26080;&#27861;&#23398;&#20064;&#30340;&#26679;&#26412;&#8221;&#65288;UEs&#65289;&#30340;&#26041;&#27861;&#34987;&#25552;&#20986;&#65292;&#36890;&#36807;&#23545;&#25968;&#25454;&#36827;&#34892;&#24494;&#23567;&#30340;&#25200;&#21160;&#65292;&#20351;&#24471;&#27169;&#22411;&#26080;&#27861;&#22312;&#21407;&#22987;&#30340;&#24178;&#20928;&#20998;&#24067;&#19978;&#20934;&#30830;&#22320;&#23545;&#20854;&#36827;&#34892;&#20998;&#31867;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;&#20445;&#25252;&#25514;&#26045;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616; UEs &#24102;&#26469;&#30340;&#23433;&#20840;&#23041;&#32961;&#26159;&#34394;&#20551;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#26080;&#27861;&#38459;&#27490;&#26410;&#32463;&#25480;&#26435;&#30340;&#29992;&#25143;&#21033;&#29992;&#20854;&#20182;&#26410;&#21463;&#20445;&#25252;&#30340;&#25968;&#25454;&#26469;&#21435;&#38500;&#20445;&#25252;&#65292;&#23558;&#26080;&#27861;&#23398;&#20064;&#30340;&#25968;&#25454;&#37325;&#36716;&#20026;&#21487;&#23398;&#20064;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#27491;&#24335;&#23450;&#20041;&#20102;&#19968;&#31181;&#23041;&#32961;&#65292;&#24341;&#20837;&#20102;&#8220;&#21487;&#23398;&#20064;&#30340;&#26410;&#32463;&#25480;&#26435;&#31034;&#20363;&#8221;&#65288;LEs&#65289;&#65292;&#36825;&#20123;&#26159;&#24050;&#32463;&#21435;&#38500;&#20445;&#25252;&#30340;UEs&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#32431;&#21270;&#36807;&#31243;&#65292;&#23558;UEs&#25237;&#23556;&#21040;LEs&#30340;&#27969;&#24418;&#19978;&#12290;&#36825;&#26159;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#32852;&#21512;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#26469;&#23454;&#29616;&#30340;&#65292;&#35813;&#27169;&#22411;&#23545;UEs&#36827;&#34892;&#21435;&#22122;&#12290;
&lt;/p&gt;
&lt;p&gt;
Safeguarding data from unauthorized exploitation is vital for privacy and security, especially in recent rampant research in security breach such as adversarial/membership attacks. To this end, \textit{unlearnable examples} (UEs) have been recently proposed as a compelling protection, by adding imperceptible perturbation to data so that models trained on them cannot classify them accurately on original clean distribution. Unfortunately, we find UEs provide a false sense of security, because they cannot stop unauthorized users from utilizing other unprotected data to remove the protection, by turning unlearnable data into learnable again. Motivated by this observation, we formally define a new threat by introducing \textit{learnable unauthorized examples} (LEs) which are UEs with their protection removed. The core of this approach is a novel purification process that projects UEs onto the manifold of LEs. This is realized by a new joint-conditional diffusion model which denoises UEs con
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36719;&#20214;&#24037;&#31243;&#21407;&#29702;&#30340;&#20154;&#24037;&#26234;&#33021;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#30830;&#20445;&#21307;&#30103;&#20445;&#20581;&#20844;&#27491;&#30340;&#21516;&#26102;&#65292;&#35782;&#21035;&#21644;&#20943;&#36731;&#25968;&#25454;&#21644;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#12290;&#26410;&#26469;&#30340;&#30740;&#31350;&#26088;&#22312;&#22312;&#30495;&#23454;&#20020;&#24202;&#29615;&#22659;&#20013;&#27979;&#35797;&#21644;&#39564;&#35777;&#26694;&#26550;&#65292;&#20197;&#35780;&#20272;&#20854;&#22312;&#20419;&#36827;&#20581;&#24247;&#20844;&#24179;&#26041;&#38754;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.07041</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20844;&#27491;&#24615;&#19982;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#24179;&#31561;&#30456;&#36935;
&lt;/p&gt;
&lt;p&gt;
Fairness in Machine Learning meets with Equity in Healthcare. (arXiv:2305.07041v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07041
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36719;&#20214;&#24037;&#31243;&#21407;&#29702;&#30340;&#20154;&#24037;&#26234;&#33021;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#30830;&#20445;&#21307;&#30103;&#20445;&#20581;&#20844;&#27491;&#30340;&#21516;&#26102;&#65292;&#35782;&#21035;&#21644;&#20943;&#36731;&#25968;&#25454;&#21644;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#12290;&#26410;&#26469;&#30340;&#30740;&#31350;&#26088;&#22312;&#22312;&#30495;&#23454;&#20020;&#24202;&#29615;&#22659;&#20013;&#27979;&#35797;&#21644;&#39564;&#35777;&#26694;&#26550;&#65292;&#20197;&#35780;&#20272;&#20854;&#22312;&#20419;&#36827;&#20581;&#24247;&#20844;&#24179;&#26041;&#38754;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#26085;&#30410;&#24212;&#29992;&#65292;&#25552;&#39640;&#21307;&#30103;&#20445;&#20581;&#25928;&#26524;&#21644;&#25928;&#29575;&#30340;&#28508;&#21147;&#19981;&#26029;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#36825;&#20063;&#24102;&#26469;&#20102;&#28508;&#22312;&#30340;&#39118;&#38505;&#65292;&#21363;&#22312;&#25968;&#25454;&#21644;&#27169;&#22411;&#35774;&#35745;&#20013;&#24310;&#32493;&#20559;&#35265;&#65292;&#20174;&#32780;&#20260;&#23475;&#26576;&#20123;&#21463;&#20445;&#25252;&#32676;&#20307;&#65292;&#22914;&#24180;&#40836;&#12289;&#24615;&#21035;&#21644;&#31181;&#26063;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36719;&#20214;&#24037;&#31243;&#21407;&#29702;&#30340;&#20154;&#24037;&#26234;&#33021;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#30830;&#20445;&#21307;&#30103;&#20445;&#20581;&#20844;&#27491;&#30340;&#21516;&#26102;&#65292;&#35782;&#21035;&#21644;&#20943;&#36731;&#25968;&#25454;&#21644;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#12290;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#20102;&#25968;&#25454;&#20013;&#31995;&#32479;&#24615;&#20559;&#35265;&#22914;&#20309;&#23548;&#33268;&#27169;&#22411;&#39044;&#27979;&#20013;&#30340;&#25918;&#22823;&#20559;&#35265;&#65292;&#24182;&#25552;&#20986;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20197;&#39044;&#38450;&#27492;&#31867;&#20559;&#35265;&#12290;&#26410;&#26469;&#30340;&#30740;&#31350;&#26088;&#22312;&#22312;&#30495;&#23454;&#20020;&#24202;&#29615;&#22659;&#20013;&#27979;&#35797;&#21644;&#39564;&#35777;&#25152;&#25552;&#20986;&#30340;ML&#26694;&#26550;&#65292;&#20197;&#35780;&#20272;&#20854;&#22312;&#20419;&#36827;&#20581;&#24247;&#20844;&#24179;&#26041;&#38754;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the growing utilization of machine learning in healthcare, there is increasing potential to enhance healthcare outcomes and efficiency. However, this also brings the risk of perpetuating biases in data and model design that can harm certain protected groups based on factors such as age, gender, and race. This study proposes an artificial intelligence framework, grounded in software engineering principles, for identifying and mitigating biases in data and models while ensuring fairness in healthcare settings. A case study is presented to demonstrate how systematic biases in data can lead to amplified biases in model predictions, and machine learning methods are suggested to prevent such biases. Future research aims to test and validate the proposed ML framework in real-world clinical settings to evaluate its impact on promoting health equity.
&lt;/p&gt;</description></item><item><title>Tiny-PPG&#26159;&#19968;&#20010;&#29992;&#20110;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#26102;&#26816;&#27979;PPG&#20449;&#21495;&#20013;&#36816;&#21160;&#20266;&#24433;&#30340;&#36731;&#37327;&#32423;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#21487;&#20998;&#31163;&#21367;&#31215;&#21644;&#31354;&#27934;&#31354;&#38388;&#37329;&#23383;&#22612;&#27744;&#21270;&#27169;&#22359;&#65292;&#20197;&#21450;&#36890;&#36947;&#27880;&#24847;&#26426;&#21046;&#65292;&#33021;&#22815;&#22312;&#24179;&#34913;&#26816;&#27979;&#31934;&#24230;&#21644;&#36895;&#24230;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#26816;&#27979;&#25928;&#26524;&#65292;&#24182;&#21487;&#20197;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#37096;&#32626;&#65292;&#23454;&#29616;&#22522;&#20110;&#29289;&#32852;&#32593;&#30340;&#21487;&#31359;&#25140;&#35774;&#22791;&#21644;&#26234;&#33021;&#20581;&#24247;&#35774;&#22791;&#19978;&#30340;&#20934;&#30830;&#23454;&#26102;PPG&#20266;&#24433;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2305.03308</link><description>&lt;p&gt;
Tiny-PPG: &#29992;&#20110;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#26102;&#26816;&#27979;&#20809;&#30005;&#23481;&#31215;&#33033;&#25615;&#22270;&#20449;&#21495;&#20013;&#36816;&#21160;&#20266;&#24433;&#30340;&#36731;&#37327;&#32423;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Tiny-PPG: A Lightweight Deep Neural Network for Real-Time Detection of Motion Artifacts in Photoplethysmogram Signals on Edge Devices. (arXiv:2305.03308v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03308
&lt;/p&gt;
&lt;p&gt;
Tiny-PPG&#26159;&#19968;&#20010;&#29992;&#20110;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#26102;&#26816;&#27979;PPG&#20449;&#21495;&#20013;&#36816;&#21160;&#20266;&#24433;&#30340;&#36731;&#37327;&#32423;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#21487;&#20998;&#31163;&#21367;&#31215;&#21644;&#31354;&#27934;&#31354;&#38388;&#37329;&#23383;&#22612;&#27744;&#21270;&#27169;&#22359;&#65292;&#20197;&#21450;&#36890;&#36947;&#27880;&#24847;&#26426;&#21046;&#65292;&#33021;&#22815;&#22312;&#24179;&#34913;&#26816;&#27979;&#31934;&#24230;&#21644;&#36895;&#24230;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#26816;&#27979;&#25928;&#26524;&#65292;&#24182;&#21487;&#20197;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#37096;&#32626;&#65292;&#23454;&#29616;&#22522;&#20110;&#29289;&#32852;&#32593;&#30340;&#21487;&#31359;&#25140;&#35774;&#22791;&#21644;&#26234;&#33021;&#20581;&#24247;&#35774;&#22791;&#19978;&#30340;&#20934;&#30830;&#23454;&#26102;PPG&#20266;&#24433;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20809;&#30005;&#23481;&#31215;&#33033;&#25615;&#22270;&#65288;PPG&#65289;&#20449;&#21495;&#24050;&#32463;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#22522;&#20110;&#29289;&#32852;&#32593;&#30340;&#21487;&#31359;&#25140;&#35774;&#22791;&#21644;&#26234;&#33021;&#20581;&#24247;&#35774;&#22791;&#20013;&#36827;&#34892;&#24515;&#34880;&#31649;&#20581;&#24247;&#30417;&#25252;&#65292;&#20294;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;PPG&#20449;&#21495;&#24456;&#23481;&#26131;&#21463;&#21040;&#36816;&#21160;&#20266;&#24433;&#30340;&#27745;&#26579;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Tiny-PPG&#8221;&#30340;&#36731;&#37327;&#32423;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#22312;&#29289;&#32852;&#32593;&#36793;&#32536;&#35774;&#22791;&#19978;&#20934;&#30830;&#23454;&#26102;&#22320;&#20998;&#21106;PPG&#20266;&#24433;&#12290;&#35813;&#27169;&#22411;&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;PPG DaLiA&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#21644;&#27979;&#35797;&#65292;&#35813;&#25968;&#25454;&#38598;&#20351;&#29992;&#25163;&#34920;&#24335;&#35774;&#22791;&#65288;Empatica E4&#65289;&#23545;15&#21517;&#21463;&#35797;&#32773;&#22312;&#21508;&#31181;&#26085;&#24120;&#27963;&#21160;&#20013;&#30340;PPG&#20449;&#21495;&#65292;&#20855;&#26377;&#19981;&#21516;&#38271;&#24230;&#21644;&#24418;&#24577;&#30340;&#22797;&#26434;&#20266;&#24433;&#12290;&#35813;&#27169;&#22411;&#32467;&#26500;&#12289;&#35757;&#32451;&#26041;&#27861;&#21644;&#25439;&#22833;&#20989;&#25968;&#29305;&#21035;&#35774;&#35745;&#65292;&#20197;&#24179;&#34913;&#26816;&#27979;&#31934;&#24230;&#21644;&#36895;&#24230;&#65292;&#36866;&#29992;&#20110;&#36164;&#28304;&#21463;&#38480;&#30340;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#36827;&#34892;&#23454;&#26102;PPG&#20266;&#24433;&#26816;&#27979;&#12290;&#20026;&#20102;&#20248;&#21270;&#22810;&#23610;&#24230;&#29305;&#24449;&#34920;&#31034;&#30340;&#27169;&#22411;&#22823;&#23567;&#21644;&#33021;&#21147;&#65292;&#35813;&#27169;&#22411;&#37319;&#29992;&#28145;&#24230;&#21487;&#20998;&#31163;&#21367;&#31215;&#21644;&#31354;&#27934;&#31354;&#38388;&#37329;&#23383;&#22612;&#27744;&#21270;&#27169;&#22359;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36947;&#27880;&#24847;&#26426;&#21046;&#65292;&#20197;&#36328;&#36890;&#36947;&#23398;&#20064;&#21306;&#20998;&#24615;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;PPG&#20449;&#21495;&#29305;&#24449;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Tiny-PPG&#22312;&#26816;&#27979;&#31934;&#24230;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#22343;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#21487;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#37096;&#32626;&#65292;&#23454;&#29616;&#22522;&#20110;&#29289;&#32852;&#32593;&#30340;&#21487;&#31359;&#25140;&#35774;&#22791;&#21644;&#26234;&#33021;&#20581;&#24247;&#35774;&#22791;&#19978;&#30340;&#20934;&#30830;&#23454;&#26102;PPG&#20266;&#24433;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Photoplethysmogram (PPG) signals are easily contaminated by motion artifacts in real-world settings, despite their widespread use in Internet-of-Things (IoT) based wearable and smart health devices for cardiovascular health monitoring. This study proposed a lightweight deep neural network, called Tiny-PPG, for accurate and real-time PPG artifact segmentation on IoT edge devices. The model was trained and tested on a public dataset, PPG DaLiA, which featured complex artifacts with diverse lengths and morphologies during various daily activities of 15 subjects using a watch-type device (Empatica E4). The model structure, training method and loss function were specifically designed to balance detection accuracy and speed for real-time PPG artifact detection in resource-constrained embedded devices. To optimize the model size and capability in multi-scale feature representation, the model employed deep separable convolution and atrous spatial pyramid pooling modules, respectively. Addition
&lt;/p&gt;</description></item><item><title>G-MATT&#26159;&#19968;&#20010;&#32467;&#21512;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#19982;&#21270;&#23398;&#30693;&#35782;&#30340;&#21270;&#23398;&#24863;&#30693;&#22238;&#28335;&#21512;&#25104;&#39044;&#27979;&#26694;&#26550;&#65292;&#22312;&#20998;&#23618;SMILES&#35821;&#27861;&#26641;&#36755;&#20837;&#30340;&#22522;&#30784;&#19978;&#37319;&#29992;&#26641;&#21040;&#24207;&#21015;&#21464;&#25442;&#22120;&#26550;&#26500;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#21333;&#27493;&#22238;&#28335;&#21512;&#25104;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.03153</link><description>&lt;p&gt;
G-MATT: &#20998;&#23376;&#35821;&#27861;&#26641;&#21464;&#25442;&#22120;&#30340;&#21333;&#27493;&#22238;&#28335;&#21512;&#25104;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
G-MATT: Single-step Retrosynthesis Prediction using Molecular Grammar Tree Transformer. (arXiv:2305.03153v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03153
&lt;/p&gt;
&lt;p&gt;
G-MATT&#26159;&#19968;&#20010;&#32467;&#21512;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#19982;&#21270;&#23398;&#30693;&#35782;&#30340;&#21270;&#23398;&#24863;&#30693;&#22238;&#28335;&#21512;&#25104;&#39044;&#27979;&#26694;&#26550;&#65292;&#22312;&#20998;&#23618;SMILES&#35821;&#27861;&#26641;&#36755;&#20837;&#30340;&#22522;&#30784;&#19978;&#37319;&#29992;&#26641;&#21040;&#24207;&#21015;&#21464;&#25442;&#22120;&#26550;&#26500;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#21333;&#27493;&#22238;&#28335;&#21512;&#25104;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#24050;&#32463;&#25253;&#36947;&#20102;&#20960;&#31181;&#22522;&#20110;&#21453;&#24212;&#27169;&#26495;&#21644;&#22522;&#20110;&#33258;&#30001;&#27169;&#26495;&#30340;&#21333;&#27493;&#22238;&#28335;&#21512;&#25104;&#39044;&#27979;&#26041;&#27861;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#20013;&#30340;&#35768;&#22810;&#22312;&#20256;&#32479;&#25968;&#25454;&#39537;&#21160;&#25351;&#26631;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#20351;&#29992;&#30340;&#27169;&#22411;&#26550;&#26500;&#19982;&#25903;&#37197;&#21453;&#21521;&#21512;&#25104;&#30340;&#24213;&#23618;&#21270;&#23398;&#21407;&#21017;&#20043;&#38388;&#23384;&#22312;&#33073;&#33410;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21270;&#23398;&#24863;&#30693;&#22238;&#28335;&#21512;&#25104;&#39044;&#27979;&#26694;&#26550;&#65292;&#23558;&#24378;&#22823;&#30340;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#19982;&#21270;&#23398;&#30693;&#35782;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#23618;SMILES&#35821;&#27861;&#26641;&#30340;&#26641;&#21040;&#24207;&#21015;&#21464;&#25442;&#22120;&#26550;&#26500;&#65292;&#20854;&#20013;&#21253;&#21547;&#34987;&#32431;SMILES&#34920;&#31034;&#27861;&#30340;&#27169;&#22411;&#24573;&#30053;&#30340;&#24213;&#23618;&#21270;&#23398;&#20449;&#24687;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#65292;&#22522;&#20110;&#35821;&#27861;&#30340;&#20998;&#23376;&#27880;&#24847;&#21147;&#26641;&#21464;&#25442;&#22120;&#65288;G-MATT&#65289;&#65292;&#19982;&#22522;&#32447;&#22238;&#28335;&#21512;&#25104;&#27169;&#22411;&#30456;&#27604;&#65292;&#23454;&#29616;&#20102;&#26174;&#30528;&#30340;&#24615;&#33021;&#25552;&#39640;&#12290; G-MATT&#30340;&#20934;&#30830;&#29575;&#25490;&#21517;&#21069;1&#20026;51&#65285;&#65288;&#21069;10&#20026;79.1&#65285;&#65289;&#65292;&#26080;&#25928;&#29575;&#20026;1.5&#65285;&#65292;
&lt;/p&gt;
&lt;p&gt;
In recent years, several reaction templates-based and template-free approaches have been reported for single-step retrosynthesis prediction. Even though many of these approaches perform well from traditional data-driven metrics standpoint, there is a disconnect between model architectures used and underlying chemistry principles governing retrosynthesis. Here, we propose a novel chemistry-aware retrosynthesis prediction framework that combines powerful data-driven models with chemistry knowledge. We report a tree-to-sequence transformer architecture based on hierarchical SMILES grammar trees as input containing underlying chemistry information that is otherwise ignored by models based on purely SMILES-based representations. The proposed framework, grammar-based molecular attention tree transformer (G-MATT), achieves significant performance improvements compared to baseline retrosynthesis models. G-MATT achieves a top-1 accuracy of 51% (top-10 accuracy of 79.1%), invalid rate of 1.5%, a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#33258;&#21160;&#21435;&#20559;&#37325;&#37325;&#37197;&#30340;&#26032;&#39062;&#29305;&#24449;&#25551;&#36848;&#65292;&#24182;&#23558;&#20854;&#31561;&#21516;&#20110;&#22522;&#20110;&#20869;&#26680;&#23725;&#22238;&#24402;&#30340;&#21333;&#20010;&#27424;&#24179;&#28369;&#23725;&#22238;&#24402;&#65292;&#36827;&#19968;&#27493;&#23558;&#36825;&#31181;&#26041;&#27861;&#25512;&#24191;&#21040;&#29305;&#23450;&#30340;&#32467;&#26524;&#21644;&#37325;&#37197;&#27169;&#22411;&#36873;&#25321;&#19978;&#12290;</title><link>http://arxiv.org/abs/2304.14545</link><description>&lt;p&gt;
&#33258;&#21160;&#21435;&#20559;&#37325;&#37325;&#37197;&#20316;&#20026;&#32447;&#24615;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Augmented balancing weights as linear regression. (arXiv:2304.14545v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14545
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#33258;&#21160;&#21435;&#20559;&#37325;&#37325;&#37197;&#30340;&#26032;&#39062;&#29305;&#24449;&#25551;&#36848;&#65292;&#24182;&#23558;&#20854;&#31561;&#21516;&#20110;&#22522;&#20110;&#20869;&#26680;&#23725;&#22238;&#24402;&#30340;&#21333;&#20010;&#27424;&#24179;&#28369;&#23725;&#22238;&#24402;&#65292;&#36827;&#19968;&#27493;&#23558;&#36825;&#31181;&#26041;&#27861;&#25512;&#24191;&#21040;&#29305;&#23450;&#30340;&#32467;&#26524;&#21644;&#37325;&#37197;&#27169;&#22411;&#36873;&#25321;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#20110;&#33258;&#21160;&#21435;&#20559;&#37325;&#37325;&#37197;(AutoDML)&#30340;&#26032;&#39062;&#29305;&#24449;&#25551;&#36848;&#12290;&#36825;&#20123;&#20272;&#31639;&#22120;&#23558;&#32467;&#26524;&#24314;&#27169;&#19982;&#37325;&#37197;&#30456;&#32467;&#21512;&#65292;&#30452;&#25509;&#20272;&#35745;&#21453;&#21521;&#20542;&#21521;&#31215;&#20998;&#26435;&#37325;&#12290;&#24403;&#32467;&#26524;&#19982;&#26435;&#37325;&#27169;&#22411;&#37117;&#26159;&#26576;&#20123;&#65288;&#21487;&#33021;&#26159;&#26080;&#38480;&#30340;&#65289;&#22522;&#30784;&#20013;&#30340;&#32447;&#24615;&#26102;&#65292;&#25105;&#20204;&#34920;&#26126;&#22686;&#24378;&#30340;&#20272;&#31639;&#22120;&#31561;&#21516;&#20110;&#20855;&#26377;&#23558;&#21407;&#22987;&#32467;&#26524;&#27169;&#22411;&#31995;&#25968;&#21644;OLS&#30456;&#32467;&#21512;&#30340;&#31995;&#25968;&#30340;&#21333;&#20010;&#32447;&#24615;&#27169;&#22411;&#65307;&#22312;&#35768;&#22810;&#35774;&#32622;&#20013;&#65292;&#22686;&#24378;&#20272;&#31639;&#22120;&#21512;&#24182;&#20026;&#20165;&#20351;&#29992;OLS. &#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#32467;&#26524;&#25193;&#23637;&#21040;&#29305;&#23450;&#30340;&#32467;&#26524;&#21644;&#37325;&#37197;&#27169;&#22411;&#36873;&#25321;&#19978;&#12290;&#25105;&#20204;&#39318;&#20808;&#34920;&#26126;&#65292;&#20351;&#29992;(&#20869;&#26680;)&#23725;&#22238;&#24402;&#20316;&#20026;&#32467;&#26524;&#21644;&#37325;&#37197;&#27169;&#22411;&#30340;&#32852;&#21512;&#20272;&#31639;&#22120;&#31561;&#21516;&#20110;&#21333;&#20010;&#12289;&#27424;&#24179;&#28369;(&#20869;&#26680;)&#23725;&#22238;&#24402;&#65307;&#24403;&#32771;&#34385;&#21040;&#28176;&#36817;&#36895;&#29575;&#26102;&#65292;&#36825;&#19968;&#32467;&#26524;&#20063;&#25104;&#31435;&#12290;&#24403;&#20195;&#26367;&#26435;&#37325;&#27169;&#22411;&#20026;&#22871;&#32034;&#22238;&#24402;&#26102;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#29305;&#27530;&#24773;&#20917;&#30340;&#35299;&#26512;&#34920;&#36798;&#24335;&#24182;&#19988;&#28436;&#31034;&#20102;&#8230;
&lt;/p&gt;
&lt;p&gt;
We provide a novel characterization of augmented balancing weights, also known as Automatic Debiased Machine Learning (AutoDML). These estimators combine outcome modeling with balancing weights, which estimate inverse propensity score weights directly. When the outcome and weighting models are both linear in some (possibly infinite) basis, we show that the augmented estimator is equivalent to a single linear model with coefficients that combine the original outcome model coefficients and OLS; in many settings, the augmented estimator collapses to OLS alone. We then extend these results to specific choices of outcome and weighting models. We first show that the combined estimator that uses (kernel) ridge regression for both outcome and weighting models is equivalent to a single, undersmoothed (kernel) ridge regression; this also holds when considering asymptotic rates. When the weighting model is instead lasso regression, we give closed-form expressions for special cases and demonstrate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#20174;&#21333;&#19968;&#30340;&#38745;&#24577;&#22270;&#20687;&#20013;&#23398;&#20064;&#33258;&#30001;&#25991;&#26412;&#30340;&#24418;&#24335;&#26469;&#28789;&#27963;&#24314;&#27169;&#20154;&#38469;&#20114;&#21160;&#12290;&#24182;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#29983;&#25104;&#20266;&#26631;&#31614;&#26469;&#35757;&#32451;&#19968;&#31181;&#23383;&#24149;&#27169;&#22411;&#65292;&#29992;&#20110;&#26377;&#25928;&#29702;&#35299;&#22270;&#20687;&#20013;&#30340;&#20154;&#38469;&#20114;&#21160;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#39044;&#27979;&#25991;&#26412;&#21644;&#35821;&#20041;&#36136;&#37327;&#65292;&#24182;&#22312;&#27492;&#20219;&#21153;&#19978;&#20248;&#20110;SOTA&#30340;&#22270;&#20687;&#23383;&#24149;&#21644;&#24773;&#22659;&#35782;&#21035;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.14104</link><description>&lt;p&gt;
&#20174;&#24369;&#25991;&#26412;&#30417;&#30563;&#20013;&#23398;&#20064;&#22270;&#20687;&#20013;&#30340;&#20154;&#38469;&#20114;&#21160;
&lt;/p&gt;
&lt;p&gt;
Learning Human-Human Interactions in Images from Weak Textual Supervision. (arXiv:2304.14104v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#20174;&#21333;&#19968;&#30340;&#38745;&#24577;&#22270;&#20687;&#20013;&#23398;&#20064;&#33258;&#30001;&#25991;&#26412;&#30340;&#24418;&#24335;&#26469;&#28789;&#27963;&#24314;&#27169;&#20154;&#38469;&#20114;&#21160;&#12290;&#24182;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#29983;&#25104;&#20266;&#26631;&#31614;&#26469;&#35757;&#32451;&#19968;&#31181;&#23383;&#24149;&#27169;&#22411;&#65292;&#29992;&#20110;&#26377;&#25928;&#29702;&#35299;&#22270;&#20687;&#20013;&#30340;&#20154;&#38469;&#20114;&#21160;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#39044;&#27979;&#25991;&#26412;&#21644;&#35821;&#20041;&#36136;&#37327;&#65292;&#24182;&#22312;&#27492;&#20219;&#21153;&#19978;&#20248;&#20110;SOTA&#30340;&#22270;&#20687;&#23383;&#24149;&#21644;&#24773;&#22659;&#35782;&#21035;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#38469;&#20114;&#21160;&#26159;&#22810;&#26679;&#19988;&#20381;&#36182;&#20110;&#19978;&#19979;&#25991;&#30340;&#65292;&#20294;&#20808;&#21069;&#30340;&#24037;&#20316;&#23558;&#23427;&#20204;&#35270;&#20026;&#20998;&#31867;&#65292;&#24573;&#30053;&#20102;&#21487;&#33021;&#30340;&#20114;&#21160;&#30340;&#37325;&#23614;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#20154;&#38469;&#20114;&#21160;&#30340;&#33539;&#24335;&#65292;&#23558;&#20854;&#20316;&#20026;&#33258;&#30001;&#25991;&#26412;&#20174;&#21333;&#19968;&#30340;&#38745;&#24577;&#22270;&#20687;&#20013;&#23398;&#20064;&#65292;&#20174;&#32780;&#20801;&#35768;&#23545;&#24773;&#20917;&#21644;&#20154;&#38469;&#20851;&#31995;&#30340;&#26080;&#38480;&#31354;&#38388;&#36827;&#34892;&#28789;&#27963;&#24314;&#27169;&#12290;&#20026;&#20102;&#20811;&#26381;&#32570;&#20047;&#29305;&#23450;&#20110;&#27492;&#20219;&#21153;&#30340;&#26631;&#35760;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#24212;&#29992;&#20110;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20135;&#29983;&#30340;&#21512;&#25104;&#23383;&#24149;&#25968;&#25454;&#65292;&#20197;&#27492;&#29983;&#25104;&#20266;&#26631;&#31614;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#36825;&#20010;&#36807;&#31243;&#20135;&#29983;&#30340;&#20266;&#26631;&#31614;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#19968;&#31181;&#23383;&#24149;&#27169;&#22411;&#65292;&#33021;&#26377;&#25928;&#29702;&#35299;&#22270;&#20687;&#20013;&#30340;&#20154;&#38469;&#20114;&#21160;&#65292;&#36890;&#36807;&#34913;&#37327;&#25105;&#20204;&#39044;&#27979;&#30340;&#25991;&#26412;&#21644;&#35821;&#20041;&#36136;&#37327;&#19982;&#20107;&#23454;&#30340;&#22522;&#30784;&#24615;&#30340;&#21508;&#31181;&#25351;&#26631;&#26469;&#34913;&#37327;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;SOTA&#30340;&#22270;&#20687;&#23383;&#24149;&#21644;&#24773;&#22659;&#35782;&#21035;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#20844;&#24320;&#25105;&#20204;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interactions between humans are diverse and context-dependent, but previous works have treated them as categorical, disregarding the heavy tail of possible interactions. We propose a new paradigm of learning human-human interactions as free text from a single still image, allowing for flexibility in modeling the unlimited space of situations and relationships between people. To overcome the absence of data labelled specifically for this task, we use knowledge distillation applied to synthetic caption data produced by a large language model without explicit supervision. We show that the pseudo-labels produced by this procedure can be used to train a captioning model to effectively understand human-human interactions in images, as measured by a variety of metrics that measure textual and semantic faithfulness and factual groundedness of our predictions. We further show that our approach outperforms SOTA image captioning and situation recognition models on this task. We will release our c
&lt;/p&gt;</description></item><item><title>&#21457;&#29616;&#20174;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#22343;&#21248;&#22320;&#25277;&#21462;&#36127;&#26679;&#26412;&#20250;&#24341;&#20837;&#38169;&#35823;&#30340;&#36127;&#38754;&#26679;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32416;&#27491;&#38169;&#35823;&#36127;&#38754;&#26679;&#26412;&#30340;&#26032;&#26041;&#27861;&#65292;&#21363;&#38024;&#23545;&#22270;&#25991;&#27169;&#22411;&#30340;&#26679;&#26412;&#29305;&#24322;&#24615;&#21435;&#20559;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.13181</link><description>&lt;p&gt;
&#38024;&#23545;&#22270;&#25991;&#27169;&#22411;&#30340;&#26679;&#26412;&#29305;&#24322;&#24615;&#21435;&#20559;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Sample-Specific Debiasing for Better Image-Text Models. (arXiv:2304.13181v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13181
&lt;/p&gt;
&lt;p&gt;
&#21457;&#29616;&#20174;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#22343;&#21248;&#22320;&#25277;&#21462;&#36127;&#26679;&#26412;&#20250;&#24341;&#20837;&#38169;&#35823;&#30340;&#36127;&#38754;&#26679;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32416;&#27491;&#38169;&#35823;&#36127;&#38754;&#26679;&#26412;&#30340;&#26032;&#26041;&#27861;&#65292;&#21363;&#38024;&#23545;&#22270;&#25991;&#27169;&#22411;&#30340;&#26679;&#26412;&#29305;&#24322;&#24615;&#21435;&#20559;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22270;&#25991;&#25968;&#25454;&#30340;&#33258;&#30417;&#30563;&#34920;&#24449;&#23398;&#20064;&#22312;&#21307;&#23398;&#24212;&#29992;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#20363;&#22914;&#22270;&#20687;&#20998;&#31867;&#12289;&#35270;&#35273;&#23450;&#20301;&#21644;&#36328;&#27169;&#24577;&#26816;&#32034;&#12290;&#19968;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#28041;&#21450;&#23545;&#35821;&#20041;&#19978;&#30456;&#20284;&#65288;&#27491;&#65289;&#21644;&#19981;&#30456;&#20284;&#65288;&#36127;&#65289;&#30340;&#25968;&#25454;&#28857;&#36827;&#34892;&#23545;&#27604;&#12290;&#20174;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#22343;&#21248;&#22320;&#25277;&#21462;&#36127;&#26679;&#26412;&#20250;&#24341;&#20837;&#38169;&#35823;&#30340;&#36127;&#38754;&#26679;&#26412;&#65292;&#21363;&#23558;&#21516;&#23646;&#19968;&#31867;&#30340;&#26679;&#26412;&#35270;&#20026;&#19981;&#30456;&#20284;&#12290;&#22312;&#21307;&#30103;&#20445;&#20581;&#25968;&#25454;&#20013;&#65292;&#28508;&#22312;&#30340;&#31867;&#21035;&#20998;&#24067;&#26159;&#19981;&#22343;&#21248;&#30340;&#65292;&#24847;&#21619;&#30528;&#38169;&#35823;&#30340;&#36127;&#38754;&#26679;&#26412;&#20986;&#29616;&#30340;&#27604;&#20363;&#39640;&#24230;&#19981;&#21516;&#12290;&#20026;&#20102;&#25552;&#39640;&#23398;&#24471;&#30340;&#34920;&#31034;&#36136;&#37327;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32416;&#27491;&#38169;&#35823;&#36127;&#38754;&#26679;&#26412;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#20351;&#29992;&#20272;&#35745;&#30340;&#26679;&#26412;&#29305;&#24322;&#24615;&#31867;&#21035;&#27010;&#29575;&#30340;&#21435;&#20559;&#23545;&#27604;&#23398;&#20064;&#30340;&#21464;&#20307;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#30446;&#26631;&#20989;&#25968;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#22312;&#22270;&#20687;&#21644;&#37197;&#23545;&#30340;&#22270;&#25991;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21462;&#24471;&#30340;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised representation learning on image-text data facilitates crucial medical applications, such as image classification, visual grounding, and cross-modal retrieval. One common approach involves contrasting semantically similar (positive) and dissimilar (negative) pairs of data points. Drawing negative samples uniformly from the training data set introduces false negatives, i.e., samples that are treated as dissimilar but belong to the same class. In healthcare data, the underlying class distribution is nonuniform, implying that false negatives occur at a highly variable rate. To improve the quality of learned representations, we develop a novel approach that corrects for false negatives. Our method can be viewed as a variant of debiased constrastive learning that uses estimated sample-specific class probabilities. We provide theoretical analysis of the objective function and demonstrate the proposed approach on both image and paired image-text data sets. Our experiments demo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#24320;&#28304;&#30340;&#36947;&#36335;&#22353;&#27934;&#26816;&#27979;&#22522;&#20934;&#22871;&#20214;UDTIRI&#65292;&#21253;&#21547;&#20102;&#26631;&#35760;&#40784;&#20840;&#30340;1000&#24352;&#36947;&#36335;&#22353;&#27934;&#22270;&#20687;&#65292;&#21487;&#20197;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#22478;&#24066;&#36947;&#36335;&#26816;&#26597;&#20013;&#30340;&#30446;&#26631;&#26816;&#27979;&#12289;&#35821;&#20041;&#20998;&#21106;&#21644;&#23454;&#20363;&#20998;&#21106;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2304.08842</link><description>&lt;p&gt;
UDTIRI:&#19968;&#20010;&#24320;&#28304;&#30340;&#36947;&#36335;&#22353;&#27934;&#26816;&#27979;&#22522;&#20934;&#22871;&#20214;
&lt;/p&gt;
&lt;p&gt;
UDTIRI: An Open-Source Road Pothole Detection Benchmark Suite. (arXiv:2304.08842v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08842
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#24320;&#28304;&#30340;&#36947;&#36335;&#22353;&#27934;&#26816;&#27979;&#22522;&#20934;&#22871;&#20214;UDTIRI&#65292;&#21253;&#21547;&#20102;&#26631;&#35760;&#40784;&#20840;&#30340;1000&#24352;&#36947;&#36335;&#22353;&#27934;&#22270;&#20687;&#65292;&#21487;&#20197;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#22478;&#24066;&#36947;&#36335;&#26816;&#26597;&#20013;&#30340;&#30446;&#26631;&#26816;&#27979;&#12289;&#35821;&#20041;&#20998;&#21106;&#21644;&#23454;&#20363;&#20998;&#21106;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30475;&#21040;&#22312;&#22478;&#24066;&#25968;&#23383;&#23402;&#29983;&#39046;&#22495;&#20013;&#21033;&#29992;&#24378;&#22823;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#29305;&#21035;&#26159;&#22312;&#26234;&#33021;&#36947;&#36335;&#26816;&#26597;&#39046;&#22495;&#65292;&#30446;&#21069;&#30740;&#31350;&#21644;&#25968;&#25454;&#26377;&#38480;&#12290;&#20026;&#20102;&#20419;&#36827;&#36825;&#19968;&#39046;&#22495;&#30340;&#36827;&#23637;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;Urban Digital Twins Intelligent Road Inspection (UDTIRI)&#25968;&#25454;&#38598;&#30340;&#26631;&#35760;&#40784;&#20840;&#30340;&#36947;&#36335;&#22353;&#27934;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#20010;&#25968;&#25454;&#38598;&#33021;&#22815;&#35753;&#24378;&#22823;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#22478;&#24066;&#36947;&#36335;&#26816;&#26597;&#20013;&#21457;&#25381;&#20316;&#29992;&#65292;&#35753;&#31639;&#27861;&#26356;&#20840;&#38754;&#22320;&#29702;&#35299;&#22330;&#26223;&#24182;&#26368;&#22823;&#21270;&#20854;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#25324;1000&#24352;&#36947;&#36335;&#22353;&#27934;&#22270;&#20687;&#65292;&#25293;&#25668;&#20110;&#19981;&#21516;&#30340;&#24773;&#22659;&#20013;&#65292;&#20855;&#26377;&#19981;&#21516;&#30340;&#20809;&#29031;&#21644;&#28287;&#24230;&#26465;&#20214;&#12290;&#25105;&#20204;&#30340;&#24847;&#22270;&#26159;&#23558;&#36825;&#20010;&#25968;&#25454;&#38598;&#24212;&#29992;&#20110;&#30446;&#26631;&#26816;&#27979;&#12289;&#35821;&#20041;&#20998;&#21106;&#21644;&#23454;&#20363;&#20998;&#21106;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#22242;&#38431;&#33457;&#36153;&#20102;&#22823;&#37327;&#31934;&#21147;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#32479;&#35745;&#20998;&#26512;&#65292;&#24182;&#23545;UDTIRI&#25968;&#25454;&#38598;&#30340;&#19968;&#20123;&#20195;&#34920;&#24615;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is seen that there is enormous potential to leverage powerful deep learning methods in the emerging field of urban digital twins. It is particularly in the area of intelligent road inspection where there is currently limited research and data available. To facilitate progress in this field, we have developed a well-labeled road pothole dataset named Urban Digital Twins Intelligent Road Inspection (UDTIRI) dataset. We hope this dataset will enable the use of powerful deep learning methods in urban road inspection, providing algorithms with a more comprehensive understanding of the scene and maximizing their potential. Our dataset comprises 1000 images of potholes, captured in various scenarios with different lighting and humidity conditions. Our intention is to employ this dataset for object detection, semantic segmentation, and instance segmentation tasks. Our team has devoted significant effort to conducting a detailed statistical analysis, and benchmarking a selection of represent
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#40065;&#26834;&#24615;&#65292;&#20943;&#23569;&#26631;&#31614;&#22122;&#22768;&#30340;&#24433;&#21709;&#65292;&#20854;&#22522;&#20110;&#27491;&#24577;&#20998;&#24067;&#65292;&#24182;&#21487;&#36890;&#36807;&#26368;&#23567;&#21270;&#36127;&#23545;&#25968;&#20284;&#28982;&#26469;&#23398;&#20064;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2304.02849</link><description>&lt;p&gt;
&#20998;&#31867;&#20013;&#24322;&#26041;&#24046;&#26631;&#31614;&#22122;&#22768;&#30340;&#36923;&#36753;&#27491;&#24577;&#20284;&#28982;
&lt;/p&gt;
&lt;p&gt;
Logistic-Normal Likelihoods for Heteroscedastic Label Noise in Classification. (arXiv:2304.02849v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02849
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#40065;&#26834;&#24615;&#65292;&#20943;&#23569;&#26631;&#31614;&#22122;&#22768;&#30340;&#24433;&#21709;&#65292;&#20854;&#22522;&#20110;&#27491;&#24577;&#20998;&#24067;&#65292;&#24182;&#21487;&#36890;&#36807;&#26368;&#23567;&#21270;&#36127;&#23545;&#25968;&#20284;&#28982;&#26469;&#23398;&#20064;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22238;&#24402;&#20013;&#20272;&#35745;&#24322;&#26041;&#24046;&#26631;&#31614;&#22122;&#22768;&#30340;&#19968;&#31181;&#33258;&#28982;&#26041;&#27861;&#26159;&#23558;&#35266;&#27979;&#21040;&#30340;&#65288;&#21487;&#33021;&#24102;&#26377;&#22122;&#22768;&#30340;&#65289;&#30446;&#26631;&#24314;&#27169;&#20026;&#19968;&#20010;&#27491;&#24577;&#20998;&#24067;&#30340;&#26679;&#26412;&#65292;&#20854;&#21442;&#25968;&#21487;&#20197;&#36890;&#36807;&#26368;&#23567;&#21270;&#36127;&#23545;&#25968;&#20284;&#28982;&#26469;&#23398;&#20064;&#12290;&#35813;&#25439;&#22833;&#20855;&#26377;&#26399;&#26395;&#30340;&#25439;&#22833;&#34928;&#20943;&#29305;&#24615;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#38477;&#20302;&#39640;&#35823;&#24046;&#31034;&#20363;&#30340;&#36129;&#29486;&#12290;&#30452;&#35266;&#22320;&#35828;&#65292;&#36825;&#31181;&#34892;&#20026;&#21487;&#20197;&#36890;&#36807;&#20943;&#23569;&#36807;&#25311;&#21512;&#26469;&#25552;&#39640;&#23545;&#26631;&#31614;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36825;&#31181;&#31616;&#21333;&#19988;&#27010;&#29575;&#21270;&#26041;&#27861;&#22312;&#20998;&#31867;&#20013;&#30340;&#25193;&#23637;&#65292;&#20855;&#26377;&#30456;&#21516;&#30340;&#26399;&#26395;&#25439;&#22833;&#34928;&#20943;&#29305;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#27979;&#37327;&#20854;&#23545;&#20998;&#31867;&#20013;&#26631;&#31614;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#26469;&#35780;&#20272;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#21551;&#21457;&#24615;&#30340;&#23454;&#39564;&#65292;&#25506;&#32034;&#20102;&#35813;&#26041;&#27861;&#30340;&#20869;&#37096;&#24037;&#20316;&#21407;&#29702;&#65292;&#21253;&#25324;&#23545;&#36229;&#21442;&#25968;&#30340;&#25935;&#24863;&#24615;&#65292;&#28040;&#34701;&#30740;&#31350;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
A natural way of estimating heteroscedastic label noise in regression is to model the observed (potentially noisy) target as a sample from a normal distribution, whose parameters can be learned by minimizing the negative log-likelihood. This loss has desirable loss attenuation properties, as it can reduce the contribution of high-error examples. Intuitively, this behavior can improve robustness against label noise by reducing overfitting. We propose an extension of this simple and probabilistic approach to classification that has the same desirable loss attenuation properties. We evaluate the effectiveness of the method by measuring its robustness against label noise in classification. We perform enlightening experiments exploring the inner workings of the method, including sensitivity to hyperparameters, ablation studies, and more.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#8212;&#8212;&#20934;&#24230;&#37327;&#24378;&#21270;&#23398;&#20064;&#65288;QRL&#65289;&#65292;&#21033;&#29992;&#20934;&#24230;&#37327;&#27169;&#22411;&#26469;&#23398;&#20064;&#26368;&#20248;&#20215;&#20540;&#20989;&#25968;&#65307;&#22312;&#31163;&#32447;&#21644;&#22312;&#32447;&#30340;&#30446;&#26631;&#36798;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;QRL&#23637;&#31034;&#20102;&#26356;&#22909;&#30340;&#37319;&#26679;&#25928;&#29575;&#21644;&#24615;&#33021;&#65292;&#21253;&#25324;&#22522;&#20110;&#29366;&#24577;&#21644;&#22522;&#20110;&#22270;&#20687;&#30340;&#35266;&#27979;&#12290;</title><link>http://arxiv.org/abs/2304.01203</link><description>&lt;p&gt;
&#22522;&#20110;&#20934;&#24230;&#37327;&#23398;&#20064;&#30340;&#26368;&#20248;&#30446;&#26631;&#36798;&#25104;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Optimal Goal-Reaching Reinforcement Learning via Quasimetric Learning. (arXiv:2304.01203v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01203
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#8212;&#8212;&#20934;&#24230;&#37327;&#24378;&#21270;&#23398;&#20064;&#65288;QRL&#65289;&#65292;&#21033;&#29992;&#20934;&#24230;&#37327;&#27169;&#22411;&#26469;&#23398;&#20064;&#26368;&#20248;&#20215;&#20540;&#20989;&#25968;&#65307;&#22312;&#31163;&#32447;&#21644;&#22312;&#32447;&#30340;&#30446;&#26631;&#36798;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;QRL&#23637;&#31034;&#20102;&#26356;&#22909;&#30340;&#37319;&#26679;&#25928;&#29575;&#21644;&#24615;&#33021;&#65292;&#21253;&#25324;&#22522;&#20110;&#29366;&#24577;&#21644;&#22522;&#20110;&#22270;&#20687;&#30340;&#35266;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30446;&#26631;&#36798;&#25104;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#26368;&#20248;&#20215;&#20540;&#20989;&#25968;&#20855;&#26377;&#29305;&#23450;&#30340;&#20960;&#20309;&#32467;&#26500;&#65292;&#31216;&#20026;&#20934;&#24230;&#37327;&#32467;&#26500;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#8212;&#8212;&#20934;&#24230;&#37327;&#24378;&#21270;&#23398;&#20064;&#65288;QRL&#65289;&#65292;&#21033;&#29992;&#20934;&#24230;&#37327;&#27169;&#22411;&#26469;&#23398;&#20064;&#26368;&#20248;&#20215;&#20540;&#20989;&#25968;&#12290;&#19982;&#20197;&#24448;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;QRL&#30340;&#30446;&#26631;&#26159;&#19987;&#38376;&#20026;&#20934;&#24230;&#37327;&#35774;&#35745;&#30340;&#65292;&#24182;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#29702;&#35770;&#24674;&#22797;&#20445;&#35777;&#12290;&#22312;&#31163;&#25955;&#21270;&#30340;MountainCar&#29615;&#22659;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#30830;&#23450;&#20102;QRL&#30340;&#24615;&#36136;&#20197;&#21450;&#20854;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;&#22312;&#31163;&#32447;&#21644;&#22312;&#32447;&#30340;&#30446;&#26631;&#36798;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;QRL&#36824;&#23637;&#31034;&#20102;&#26356;&#22909;&#30340;&#37319;&#26679;&#25928;&#29575;&#21644;&#24615;&#33021;&#65292;&#21253;&#25324;&#22522;&#20110;&#29366;&#24577;&#21644;&#22522;&#20110;&#22270;&#20687;&#30340;&#35266;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
In goal-reaching reinforcement learning (RL), the optimal value function has a particular geometry, called quasimetric structure. This paper introduces Quasimetric Reinforcement Learning (QRL), a new RL method that utilizes quasimetric models to learn optimal value functions. Distinct from prior approaches, the QRL objective is specifically designed for quasimetrics, and provides strong theoretical recovery guarantees. Empirically, we conduct thorough analyses on a discretized MountainCar environment, identifying properties of QRL and its advantages over alternatives. On offline and online goal-reaching benchmarks, QRL also demonstrates improved sample efficiency and performance, across both state-based and image-based observations.
&lt;/p&gt;</description></item><item><title>CoD-MTL&#26159;&#19968;&#20010;&#26032;&#30340;&#27515;&#22240;&#20998;&#26512;&#26694;&#26550;&#65292;&#37319;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#26469;&#20849;&#21516;&#27169;&#25311;&#19981;&#21516;CoD&#39044;&#27979;&#20219;&#21153;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#24182;&#19988;&#21033;&#29992;&#26641;&#33976;&#39311;&#31574;&#30053;&#36827;&#34892;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#22312;&#20855;&#22791;&#26641;&#27169;&#22411;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#20248;&#28857;&#20013;&#21457;&#25496;&#26368;&#22823;&#21270;&#30340;&#21033;&#30410;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#25552;&#20379;&#31934;&#30830;&#21487;&#38752;&#30340;CoD&#39044;&#27979;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#20020;&#24202;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2304.00012</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#29992;&#20110;&#31227;&#26893;&#21518;&#27515;&#22240;&#20998;&#26512;&#65306;&#20197;&#32925;&#31227;&#26893;&#20026;&#20363;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Multi-Task Learning for Post-transplant Cause of Death Analysis: A Case Study on Liver Transplant. (arXiv:2304.00012v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00012
&lt;/p&gt;
&lt;p&gt;
CoD-MTL&#26159;&#19968;&#20010;&#26032;&#30340;&#27515;&#22240;&#20998;&#26512;&#26694;&#26550;&#65292;&#37319;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#26469;&#20849;&#21516;&#27169;&#25311;&#19981;&#21516;CoD&#39044;&#27979;&#20219;&#21153;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#24182;&#19988;&#21033;&#29992;&#26641;&#33976;&#39311;&#31574;&#30053;&#36827;&#34892;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#22312;&#20855;&#22791;&#26641;&#27169;&#22411;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#20248;&#28857;&#20013;&#21457;&#25496;&#26368;&#22823;&#21270;&#30340;&#21033;&#30410;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#25552;&#20379;&#31934;&#30830;&#21487;&#38752;&#30340;CoD&#39044;&#27979;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#20020;&#24202;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22120;&#23448;&#31227;&#26893;&#26159;&#26576;&#20123;&#26411;&#26399;&#30142;&#30149;&#65288;&#22914;&#32925;&#34928;&#31469;&#65289;&#30340;&#22522;&#26412;&#27835;&#30103;&#26041;&#27861;&#12290;&#20998;&#26512;&#22120;&#23448;&#31227;&#26893;&#21518;&#30340;&#27515;&#22240;&#21487;&#20197;&#20026;&#20020;&#24202;&#20915;&#31574;&#25552;&#20379;&#26377;&#21147;&#30340;&#24037;&#20855;&#65292;&#21253;&#25324;&#20010;&#24615;&#21270;&#27835;&#30103;&#21644;&#22120;&#23448;&#20998;&#37197;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#26041;&#27861;&#22914;MELD&#35780;&#20998;&#21644;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#27515;&#22240;&#20998;&#26512;&#20013;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#30340;&#25968;&#25454;&#21644;&#27169;&#22411;&#30456;&#20851;&#25361;&#25112;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#27492;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38590;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;CoD-MTL&#65292;&#21033;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#26469;&#20849;&#21516;&#27169;&#25311;&#19981;&#21516;CoD&#39044;&#27979;&#20219;&#21153;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;&#20855;&#20307;&#26469;&#35762;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#26641;&#33976;&#39311;&#31574;&#30053;&#29992;&#20110;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#32467;&#21512;&#20102;&#26641;&#27169;&#22411;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#20248;&#28857;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#25552;&#20379;&#31934;&#30830;&#21487;&#38752;&#30340;CoD&#39044;&#27979;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#20197;&#23637;&#31034;&#35813;&#26694;&#26550;&#30340;&#20020;&#24202;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Organ transplant is the essential treatment method for some end-stage diseases, such as liver failure. Analyzing the post-transplant cause of death (CoD) after organ transplant provides a powerful tool for clinical decision making, including personalized treatment and organ allocation. However, traditional methods like Model for End-stage Liver Disease (MELD) score and conventional machine learning (ML) methods are limited in CoD analysis due to two major data and model-related challenges. To address this, we propose a novel framework called CoD-MTL leveraging multi-task learning to model the semantic relationships between various CoD prediction tasks jointly. Specifically, we develop a novel tree distillation strategy for multi-task learning, which combines the strength of both the tree model and multi-task learning. Experimental results are presented to show the precise and reliable CoD predictions of our framework. A case study is conducted to demonstrate the clinical importance of 
&lt;/p&gt;</description></item><item><title>ACAT&#20351;&#29992;&#25311;&#23545;&#25239;&#21453;&#20107;&#23454;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#21644;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#22312;&#33041;&#37096;CT&#25195;&#25551;&#20013;&#23558;&#30149;&#21464;&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#25552;&#39640;&#21040;72.55%&#65292;&#22312;&#32954;&#37096;CT&#25195;&#25551;&#20013;&#23558;&#19982;COVID-19&#30456;&#20851;&#32467;&#26524;&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#25552;&#39640;&#21040;70.84%&#12290;</title><link>http://arxiv.org/abs/2303.15421</link><description>&lt;p&gt;
ACAT: &#25311;&#23545;&#25239;&#21453;&#20107;&#23454;&#27880;&#24847;&#21147;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#21644;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
ACAT: Adversarial Counterfactual Attention for Classification and Detection in Medical Imaging. (arXiv:2303.15421v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15421
&lt;/p&gt;
&lt;p&gt;
ACAT&#20351;&#29992;&#25311;&#23545;&#25239;&#21453;&#20107;&#23454;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#21644;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#22312;&#33041;&#37096;CT&#25195;&#25551;&#20013;&#23558;&#30149;&#21464;&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#25552;&#39640;&#21040;72.55%&#65292;&#22312;&#32954;&#37096;CT&#25195;&#25551;&#20013;&#23558;&#19982;COVID-19&#30456;&#20851;&#32467;&#26524;&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#25552;&#39640;&#21040;70.84%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26576;&#20123;&#21307;&#23398;&#22270;&#20687;&#20219;&#21153;&#21644;&#20854;&#20182;&#20165;&#26377;&#22270;&#20687;&#30340;&#23567;&#37096;&#20998;&#23545;&#20998;&#31867;&#20219;&#21153;&#26377;&#20449;&#24687;&#36129;&#29486;&#30340;&#24773;&#20917;&#19979;&#65292;&#20256;&#32479;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26377;&#26102;&#24456;&#38590;&#27867;&#21270;&#12290;&#25163;&#21160;&#27880;&#37322;&#30340;&#24863;&#20852;&#36259;&#21306;&#22495;(ROI)&#26377;&#26102;&#34987;&#29992;&#26469;&#38548;&#31163;&#22270;&#20687;&#20013;&#26368;&#20855;&#20449;&#24687;&#20215;&#20540;&#30340;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27880;&#37322;&#24456;&#36153;&#26102;&#36153;&#21147;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#30340;&#27880;&#37322;&#32773;&#20043;&#38388;&#21487;&#33021;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#21033;&#29992;&#26174;&#33879;&#24615;&#22270;&#26469;&#33719;&#21462;&#36719;&#24615;&#31354;&#38388;&#27880;&#24847;&#21147;&#25513;&#33180;&#65292;&#35843;&#33410;&#19981;&#21516;&#23610;&#24230;&#19979;&#30340;&#22270;&#20687;&#29305;&#24449;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;&#25311;&#23545;&#25239;&#21453;&#20107;&#23454;&#27880;&#24847;&#21147;(ACAT)&#26041;&#27861;&#12290;ACAT&#23558;&#33041;&#37096;CT&#25195;&#25551;&#20013;&#30149;&#21464;&#30340;&#22522;&#20934;&#20998;&#31867;&#20934;&#30830;&#29575;&#20174;71.39&#65285;&#25552;&#39640;&#21040;72.55&#65285;&#65292;&#23558;&#32954;&#37096;CT&#25195;&#25551;&#20013;&#19982;COVID-19&#30456;&#20851;&#32467;&#26524;&#30340;&#22522;&#20934;&#20998;&#31867;&#20934;&#30830;&#29575;&#20174;67.71&#65285;&#25552;&#39640;&#21040;70.84&#65285;&#65292;&#24182;&#36229;&#36807;&#20102;&#31454;&#20105;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#29983;&#25104;&#25105;&#20204;&#26550;&#26500;&#20013;&#20351;&#29992;&#30340;&#26174;&#33879;&#24615;&#22270;&#30340;&#26368;&#20339;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#25311;&#23545;&#25239;&#29983;&#25104;&#20013;&#33719;&#21462;&#23427;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In some medical imaging tasks and other settings where only small parts of the image are informative for the classification task, traditional CNNs can sometimes struggle to generalise. Manually annotated Regions of Interest (ROI) are sometimes used to isolate the most informative parts of the image. However, these are expensive to collect and may vary significantly across annotators. To overcome these issues, we propose a framework that employs saliency maps to obtain soft spatial attention masks that modulate the image features at different scales. We refer to our method as Adversarial Counterfactual Attention (ACAT). ACAT increases the baseline classification accuracy of lesions in brain CT scans from 71.39% to 72.55% and of COVID-19 related findings in lung CT scans from 67.71% to 70.84% and exceeds the performance of competing methods. We investigate the best way to generate the saliency maps employed in our architecture and propose a way to obtain them from adversarially generated
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23567;&#27874;&#31070;&#32463;&#31639;&#23376;&#30340;&#22810;&#23610;&#24230;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#23567;&#27874;&#31070;&#32463;&#31639;&#23376;&#23558;&#27880;&#24847;&#21147;&#26426;&#21046;&#20174;&#23616;&#37096;&#25193;&#23637;&#21040;&#20840;&#22495;&#21644;&#22810;&#23610;&#24230;&#33539;&#22260;&#20869;&#65292;&#21462;&#24471;&#20102;&#27604;ViT&#21644;AFNO&#26356;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2303.12398</link><description>&lt;p&gt;
&#36890;&#36807;&#23567;&#27874;&#31070;&#32463;&#31639;&#23376;&#23454;&#29616;Transformers&#30340;&#22810;&#23610;&#24230;&#27880;&#24847;&#21147;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Multiscale Attention via Wavelet Neural Operators for Vision Transformers. (arXiv:2303.12398v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12398
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23567;&#27874;&#31070;&#32463;&#31639;&#23376;&#30340;&#22810;&#23610;&#24230;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#23567;&#27874;&#31070;&#32463;&#31639;&#23376;&#23558;&#27880;&#24847;&#21147;&#26426;&#21046;&#20174;&#23616;&#37096;&#25193;&#23637;&#21040;&#20840;&#22495;&#21644;&#22810;&#23610;&#24230;&#33539;&#22260;&#20869;&#65292;&#21462;&#24471;&#20102;&#27604;ViT&#21644;AFNO&#26356;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#21462;&#24471;&#20102;&#24191;&#27867;&#30340;&#25104;&#21151;&#12290;&#20854;&#26680;&#24515;&#26159;&#33258;&#27880;&#24847;&#26426;&#21046;&#65288;SA&#65289;&#65292;&#23427;&#26159;&#19968;&#31181;&#24402;&#32435;&#20559;&#35265;&#65292;&#36890;&#36807;&#21152;&#26435;&#22522;&#30784;&#23558;&#36755;&#20837;&#20013;&#30340;&#27599;&#20010;token&#19982;&#27599;&#20010;&#20854;&#20182;token&#30456;&#20851;&#32852;&#12290;&#26631;&#20934;&#30340;SA&#26426;&#21046;&#20855;&#26377;&#20108;&#27425;&#22797;&#26434;&#24230;&#65292;&#38590;&#20197;&#22788;&#29702;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#20013;&#20986;&#29616;&#30340;&#38271;&#24207;&#21015;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#23567;&#27874;&#31070;&#32463;&#31639;&#23376;&#30340;Multiscale Wavelet Attention&#65288;MWA&#65289;&#65292;&#20351;&#29992;&#23567;&#27874;&#31070;&#32463;&#31639;&#23376;&#23558;&#27880;&#24847;&#21147;&#26426;&#21046;&#20174;&#23616;&#37096;&#25193;&#23637;&#21040;&#20840;&#22495;&#21644;&#22810;&#23610;&#24230;&#33539;&#22260;&#20869;&#12290;&#25105;&#20204;&#29992;CIFAR&#21644;ImageNet&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;MWA&#27604;ViT&#21644;AFNO&#37117;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have achieved widespread success in computer vision. At their heart, there is a Self-Attention (SA) mechanism, an inductive bias that associates each token in the input with every other token through a weighted basis. The standard SA mechanism has quadratic complexity with the sequence length, which impedes its utility to long sequences appearing in high resolution vision. Recently, inspired by operator learning for PDEs, Adaptive Fourier Neural Operators (AFNO) were introduced for high resolution attention based on global convolution that is efficiently implemented via FFT. However, the AFNO global filtering cannot well represent small and moderate scale structures that commonly appear in natural images. To leverage the coarse-to-fine scale structures we introduce a Multiscale Wavelet Attention (MWA) by leveraging wavelet neural operators which incurs linear complexity in the sequence size. We replace the attention in ViT with MWA and our experiments with CIFAR and ImageN
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32473;&#20986;&#20102;&#38750;&#28176;&#36827;&#35823;&#24046;&#36793;&#30028;&#65292;&#20026;&#24191;&#27867;&#31867;&#21035;&#30340;&#35889;&#20272;&#35745;&#22120;&#25552;&#20379;&#20102;&#35823;&#24046;&#36793;&#30028;&#65292;&#21253;&#25324;&#22312;&#29305;&#23450;&#39057;&#29575;&#22788;&#30340;&#28857;&#20540;&#35823;&#24046;&#36793;&#30028;&#21644;&#25152;&#26377;&#39057;&#29575;&#19979;&#30340;&#26368;&#22351;&#24773;&#20917;&#35823;&#24046;&#36793;&#30028;&#65292;&#24182;&#21033;&#29992;&#35813;&#26041;&#27861;&#23548;&#20986;&#20102;&#32463;&#20856;&#35889;&#20272;&#35745;&#22120;&#65288;Blackman-Tukey&#12289;Bartlett &#21644; Welch &#20272;&#35745;&#22120;&#65289;&#30340;&#35823;&#24046;&#36793;&#30028;&#12290;</title><link>http://arxiv.org/abs/2303.11908</link><description>&lt;p&gt;
&#32463;&#20856;&#35889;&#20272;&#35745;&#30340;&#38750;&#28176;&#36827;&#24335;&#28857;&#20540;&#21644;&#26368;&#22351;&#24773;&#20917;&#36793;&#30028;
&lt;/p&gt;
&lt;p&gt;
Non-Asymptotic Pointwise and Worst-Case Bounds for Classical Spectrum Estimators. (arXiv:2303.11908v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11908
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32473;&#20986;&#20102;&#38750;&#28176;&#36827;&#35823;&#24046;&#36793;&#30028;&#65292;&#20026;&#24191;&#27867;&#31867;&#21035;&#30340;&#35889;&#20272;&#35745;&#22120;&#25552;&#20379;&#20102;&#35823;&#24046;&#36793;&#30028;&#65292;&#21253;&#25324;&#22312;&#29305;&#23450;&#39057;&#29575;&#22788;&#30340;&#28857;&#20540;&#35823;&#24046;&#36793;&#30028;&#21644;&#25152;&#26377;&#39057;&#29575;&#19979;&#30340;&#26368;&#22351;&#24773;&#20917;&#35823;&#24046;&#36793;&#30028;&#65292;&#24182;&#21033;&#29992;&#35813;&#26041;&#27861;&#23548;&#20986;&#20102;&#32463;&#20856;&#35889;&#20272;&#35745;&#22120;&#65288;Blackman-Tukey&#12289;Bartlett &#21644; Welch &#20272;&#35745;&#22120;&#65289;&#30340;&#35823;&#24046;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35889;&#20272;&#35745;&#26159;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20998;&#26512;&#30340;&#22522;&#26412;&#26041;&#27861;&#65292;&#24212;&#29992;&#21253;&#25324;&#21307;&#23398;&#12289;&#35821;&#38899;&#20998;&#26512;&#21644;&#25511;&#21046;&#35774;&#35745;&#12290;&#34429;&#28982;&#35889;&#20272;&#35745;&#30340;&#28176;&#36827;&#29702;&#35770;&#24456;&#22909;&#29702;&#35299;&#65292;&#20294;&#22312;&#26679;&#26412;&#25968;&#37327;&#22266;&#23450;&#19988;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#29702;&#35770;&#23384;&#22312;&#19968;&#23450;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#20026;&#24191;&#27867;&#31867;&#21035;&#30340;&#35889;&#20272;&#35745;&#22120;&#25552;&#20379;&#20102;&#38750;&#28176;&#36827;&#35823;&#24046;&#36793;&#30028;&#65292;&#21253;&#25324;&#22312;&#29305;&#23450;&#39057;&#29575;&#22788;&#30340;&#28857;&#20540;&#35823;&#24046;&#36793;&#30028;&#21644;&#25152;&#26377;&#39057;&#29575;&#19979;&#30340;&#26368;&#22351;&#24773;&#20917;&#35823;&#24046;&#36793;&#30028;&#12290;&#26412;&#25991;&#25152;&#25552;&#30340;&#19968;&#33324;&#26041;&#27861;&#20063;&#29992;&#20110;&#23548;&#20986;&#20102;&#32463;&#20856;&#30340; Blackman-Tukey&#12289;Bartlett &#21644; Welch &#20272;&#35745;&#22120;&#30340;&#35823;&#24046;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spectrum estimation is a fundamental methodology in the analysis of time-series data, with applications including medicine, speech analysis, and control design. The asymptotic theory of spectrum estimation is well-understood, but the theory is limited when the number of samples is fixed and finite. This paper gives non-asymptotic error bounds for a broad class of spectral estimators, both pointwise (at specific frequencies) and in the worst case over all frequencies. The general method is used to derive error bounds for the classical Blackman-Tukey, Bartlett, and Welch estimators.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#33021;&#22815;&#29983;&#25104;&#21464;&#21270;&#24418;&#29366;&#30340;&#22270;&#20687;&#38598;&#21512;&#65292;&#20174;&#32780;&#25903;&#25345;&#23545;&#35937;&#32423;&#24418;&#29366;&#25506;&#32034;&#36807;&#31243;&#30340;&#25216;&#26415;&#12290;&#36890;&#36807;&#24341;&#20837;&#25552;&#31034;&#28151;&#21512;&#25216;&#26415;&#21644;&#20351;&#29992;&#33258;&#27880;&#24847;&#21147;&#23618;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#21487;&#25511;&#21046;&#30340;&#24418;&#29366;&#21464;&#21270;&#24182;&#20934;&#30830;&#23450;&#20301;&#22270;&#20687;&#31354;&#38388;&#25805;&#20316;&#12290;</title><link>http://arxiv.org/abs/2303.11306</link><description>&lt;p&gt;
&#20351;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#23450;&#20301;&#23545;&#35937;&#27700;&#24179;&#30340;&#24418;&#29366;&#21464;&#21270;
&lt;/p&gt;
&lt;p&gt;
Localizing Object-level Shape Variations with Text-to-Image Diffusion Models. (arXiv:2303.11306v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#33021;&#22815;&#29983;&#25104;&#21464;&#21270;&#24418;&#29366;&#30340;&#22270;&#20687;&#38598;&#21512;&#65292;&#20174;&#32780;&#25903;&#25345;&#23545;&#35937;&#32423;&#24418;&#29366;&#25506;&#32034;&#36807;&#31243;&#30340;&#25216;&#26415;&#12290;&#36890;&#36807;&#24341;&#20837;&#25552;&#31034;&#28151;&#21512;&#25216;&#26415;&#21644;&#20351;&#29992;&#33258;&#27880;&#24847;&#21147;&#23618;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#21487;&#25511;&#21046;&#30340;&#24418;&#29366;&#21464;&#21270;&#24182;&#20934;&#30830;&#23450;&#20301;&#22270;&#20687;&#31354;&#38388;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#24341;&#20986;&#30340;&#24037;&#20316;&#27969;&#36890;&#24120;&#20174;&#19968;&#20010;&#25506;&#32034;&#27493;&#39588;&#24320;&#22987;&#65292;&#29992;&#25143;&#22312;&#20854;&#20013;&#27983;&#35272;&#22823;&#37327;&#29983;&#25104;&#30340;&#22270;&#20687;&#12290;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#30340;&#20840;&#23616;&#24615;&#36136;&#20351;&#24471;&#29992;&#25143;&#26080;&#27861;&#23558;&#25506;&#32034;&#33539;&#22260;&#38480;&#23450;&#22312;&#22270;&#20687;&#20013;&#30340;&#29305;&#23450;&#23545;&#35937;&#19978;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25216;&#26415;&#65292;&#29992;&#20110;&#29983;&#25104;&#19968;&#31995;&#21015;&#23637;&#31034;&#29305;&#23450;&#23545;&#35937;&#24418;&#29366;&#21464;&#21270;&#30340;&#22270;&#20687;&#65292;&#20174;&#32780;&#25903;&#25345;&#23545;&#35937;&#32423;&#24418;&#29366;&#25506;&#32034;&#36807;&#31243;&#12290;&#29983;&#25104;&#21487;&#20449;&#30340;&#21464;&#21270;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#23427;&#35201;&#27714;&#22312;&#20445;&#25345;&#23545;&#35937;&#35821;&#20041;&#30340;&#21516;&#26102;&#23545;&#29983;&#25104;&#23545;&#35937;&#30340;&#24418;&#29366;&#36827;&#34892;&#25511;&#21046;&#12290;&#22312;&#29983;&#25104;&#23545;&#35937;&#21464;&#21270;&#26102;&#65292;&#19968;&#20010;&#29305;&#21035;&#30340;&#25361;&#25112;&#26159;&#20934;&#30830;&#23450;&#20301;&#24212;&#29992;&#20110;&#23545;&#35937;&#24418;&#29366;&#30340;&#25805;&#20316;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#25552;&#31034;&#28151;&#21512;&#25216;&#26415;&#65292;&#22312;&#21435;&#22122;&#36807;&#31243;&#20013;&#20999;&#25442;&#25552;&#31034;&#20197;&#33719;&#24471;&#22810;&#26679;&#30340;&#24418;&#29366;&#36873;&#25321;&#12290;&#20026;&#20102;&#23450;&#20301;&#22270;&#20687;&#31354;&#38388;&#25805;&#20316;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#20351;&#29992;&#33258;&#27880;&#24847;&#21147;&#23618;&#30340;&#25216;&#26415;&#65292;&#19982;&#27880;&#20837;&#24322;&#24120;&#30340;&#22270;&#20687;&#36827;&#34892;&#32534;&#30721;&#20197;&#25429;&#25417;&#24418;&#29366;&#21464;&#21270;&#30340;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image models give rise to workflows which often begin with an exploration step, where users sift through a large collection of generated images. The global nature of the text-to-image generation process prevents users from narrowing their exploration to a particular object in the image. In this paper, we present a technique to generate a collection of images that depicts variations in the shape of a specific object, enabling an object-level shape exploration process. Creating plausible variations is challenging as it requires control over the shape of the generated object while respecting its semantics. A particular challenge when generating object variations is accurately localizing the manipulation applied over the object's shape. We introduce a prompt-mixing technique that switches between prompts along the denoising process to attain a variety of shape choices. To localize the image-space operation, we present two techniques that use the self-attention layers in conjunction
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#37325;&#35201;&#20294;&#40092;&#20026;&#20154;&#30693;&#30340;&#38382;&#39064;&#65306;&#32852;&#37030;&#31867;&#24335;&#25345;&#32493;&#23398;&#20064;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#21160;&#24577;&#28155;&#21152;&#26032;&#30340;&#31867;&#21035;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;TARGET&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#26080;&#26679;&#26412;&#33976;&#39311;&#26469;&#20943;&#36731;FCCL&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#20445;&#25252;&#23458;&#25143;&#25968;&#25454;&#30340;&#38544;&#31169;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20808;&#21069;&#35757;&#32451;&#30340;&#20840;&#23616;&#27169;&#22411;&#22312;&#27169;&#22411;&#23618;&#38754;&#19978;&#20256;&#36882;&#26087;&#20219;&#21153;&#30340;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#22120;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#26469;&#27169;&#25311;&#25968;&#25454;&#30340;&#20840;&#23616;&#20998;&#24067;&#12290;&#19982;&#20808;&#21069;&#30340;FCCL&#26041;&#27861;&#30456;&#27604;&#65292;TARGET&#26080;&#38656;&#39069;&#22806;&#30340;&#25968;&#25454;&#38598;&#25110;&#23384;&#20648;&#20808;&#21069;&#20219;&#21153;&#30340;&#31169;&#26377;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2303.06937</link><description>&lt;p&gt;
TARGET: &#36890;&#36807;&#26080;&#26679;&#26412;&#33976;&#39311;&#23454;&#29616;&#32852;&#37030;&#31867;&#24335;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
TARGET: Federated Class-Continual Learning via Exemplar-Free Distillation. (arXiv:2303.06937v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06937
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#37325;&#35201;&#20294;&#40092;&#20026;&#20154;&#30693;&#30340;&#38382;&#39064;&#65306;&#32852;&#37030;&#31867;&#24335;&#25345;&#32493;&#23398;&#20064;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#21160;&#24577;&#28155;&#21152;&#26032;&#30340;&#31867;&#21035;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;TARGET&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#26080;&#26679;&#26412;&#33976;&#39311;&#26469;&#20943;&#36731;FCCL&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#20445;&#25252;&#23458;&#25143;&#25968;&#25454;&#30340;&#38544;&#31169;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20808;&#21069;&#35757;&#32451;&#30340;&#20840;&#23616;&#27169;&#22411;&#22312;&#27169;&#22411;&#23618;&#38754;&#19978;&#20256;&#36882;&#26087;&#20219;&#21153;&#30340;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#22120;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#26469;&#27169;&#25311;&#25968;&#25454;&#30340;&#20840;&#23616;&#20998;&#24067;&#12290;&#19982;&#20808;&#21069;&#30340;FCCL&#26041;&#27861;&#30456;&#27604;&#65292;TARGET&#26080;&#38656;&#39069;&#22806;&#30340;&#25968;&#25454;&#38598;&#25110;&#23384;&#20648;&#20808;&#21069;&#20219;&#21153;&#30340;&#31169;&#26377;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#19968;&#20010;&#40092;&#20026;&#20154;&#30693;&#20294;&#37325;&#35201;&#30340;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#65306;&#32852;&#37030;&#31867;&#24335;&#25345;&#32493;&#23398;&#20064;&#65288;FCCL&#65289;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#21160;&#24577;&#28155;&#21152;&#26032;&#30340;&#31867;&#21035;&#12290;&#24050;&#26377;&#30340;FCCL&#26041;&#27861;&#23384;&#22312;&#21508;&#31181;&#38480;&#21046;&#65292;&#22914;&#38656;&#35201;&#39069;&#22806;&#30340;&#25968;&#25454;&#38598;&#25110;&#23384;&#20648;&#20808;&#21069;&#20219;&#21153;&#30340;&#31169;&#26377;&#25968;&#25454;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#25968;&#25454;&#21152;&#21095;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#8212;&#8212;TARGET&#65288;&#36890;&#36807;&#26080;&#26679;&#26412;&#33976;&#39311;&#23454;&#29616;&#32852;&#37030;&#31867;&#24335;&#25345;&#32493;&#23398;&#20064;&#65289;&#65292;&#35813;&#26041;&#27861;&#22312;&#20943;&#36731;FCCL&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#30340;&#21516;&#26102;&#20445;&#25252;&#23458;&#25143;&#25968;&#25454;&#30340;&#38544;&#31169;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20808;&#21069;&#35757;&#32451;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#22312;&#27169;&#22411;&#23618;&#38754;&#19978;&#23558;&#26087;&#20219;&#21153;&#30340;&#30693;&#35782;&#20256;&#36882;&#32473;&#24403;&#21069;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35757;&#32451;&#19968;&#20010;&#29983;&#25104;&#22120;&#26469;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#65292;&#20197;&#27169;&#25311;&#27599;&#20010;&#23458;&#25143;&#31471;&#19978;&#25968;&#25454;&#30340;&#20840;&#23616;&#20998;&#24067;&#12290;&#19982;&#20808;&#21069;&#30340;FCCL&#26041;&#27861;&#30456;&#27604;&#65292;TARGET&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#25968;&#25454;&#38598;&#25110;&#23384;&#20648;&#20808;&#21069;&#20219;&#21153;&#30340;&#31169;&#26377;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper focuses on an under-explored yet important problem: Federated Class-Continual Learning (FCCL), where new classes are dynamically added in federated learning. Existing FCCL works suffer from various limitations, such as requiring additional datasets or storing the private data from previous tasks. In response, we first demonstrate that non-IID data exacerbates catastrophic forgetting issue in FL. Then we propose a novel method called TARGET (federat\textbf{T}ed cl\textbf{A}ss-continual lea\textbf{R}nin\textbf{G} via \textbf{E}xemplar-free dis\textbf{T}illation), which alleviates catastrophic forgetting in FCCL while preserving client data privacy. Our proposed method leverages the previously trained global model to transfer knowledge of old tasks to the current task at the model level. Moreover, a generator is trained to produce synthetic data to simulate the global distribution of data on each client at the data level. Compared to previous FCCL methods, TARGET does not requi
&lt;/p&gt;</description></item><item><title>Miipher&#26159;&#19968;&#20010;&#31283;&#20581;&#30340;&#35821;&#38899;&#20462;&#22797;&#27169;&#22411;&#65292;&#36890;&#36807;&#25972;&#21512;&#33258;&#30417;&#30563;&#35821;&#38899;&#21644;&#25991;&#26412;&#34920;&#31034;&#65292;&#21487;&#20197;&#23558;&#21463;&#25439;&#30340;&#35821;&#38899;&#20449;&#21495;&#36716;&#25442;&#20026;&#39640;&#36136;&#37327;&#35821;&#38899;&#65292;&#24182;&#19988;&#21487;&#20197;&#24212;&#29992;&#20110;&#22686;&#21152;&#39640;&#36136;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#26032;&#22411;&#35821;&#38899;&#29983;&#25104;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.01664</link><description>&lt;p&gt;
Miipher: &#25972;&#21512;&#33258;&#30417;&#30563;&#35821;&#38899;&#21644;&#25991;&#26412;&#34920;&#31034;&#30340;&#31283;&#20581;&#35821;&#38899;&#20462;&#22797;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Miipher: A Robust Speech Restoration Model Integrating Self-Supervised Speech and Text Representations. (arXiv:2303.01664v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01664
&lt;/p&gt;
&lt;p&gt;
Miipher&#26159;&#19968;&#20010;&#31283;&#20581;&#30340;&#35821;&#38899;&#20462;&#22797;&#27169;&#22411;&#65292;&#36890;&#36807;&#25972;&#21512;&#33258;&#30417;&#30563;&#35821;&#38899;&#21644;&#25991;&#26412;&#34920;&#31034;&#65292;&#21487;&#20197;&#23558;&#21463;&#25439;&#30340;&#35821;&#38899;&#20449;&#21495;&#36716;&#25442;&#20026;&#39640;&#36136;&#37327;&#35821;&#38899;&#65292;&#24182;&#19988;&#21487;&#20197;&#24212;&#29992;&#20110;&#22686;&#21152;&#39640;&#36136;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#26032;&#22411;&#35821;&#38899;&#29983;&#25104;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#20462;&#22797;&#65288;SR&#65289;&#26159;&#23558;&#21463;&#25439;&#30340;&#35821;&#38899;&#20449;&#21495;&#36716;&#25442;&#20026;&#39640;&#36136;&#37327;&#35821;&#38899;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Miipher&#30340;&#31283;&#20581;SR&#27169;&#22411;&#65292;&#24182;&#23558;Miipher&#24212;&#29992;&#20110;&#19968;&#20010;&#26032;&#30340;SR&#24212;&#29992;&#65306;&#36890;&#36807;&#23558;&#20174;&#32593;&#32476;&#19978;&#25910;&#38598;&#30340;&#35821;&#38899;&#26679;&#26412;&#36716;&#25442;&#20026;&#24037;&#20316;&#23460;&#36136;&#37327;&#65292;&#22686;&#21152;&#39640;&#36136;&#37327;&#35757;&#32451;&#25968;&#25454;&#29992;&#20110;&#35821;&#38899;&#29983;&#25104;&#12290;&#20026;&#20102;&#20351;&#25105;&#20204;&#30340;SR&#27169;&#22411;&#23545;&#21508;&#31181;&#38477;&#35299;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#65288;i&#65289;&#20174;w2v-BERT&#25552;&#21462;&#30340;&#35821;&#38899;&#34920;&#31034;&#20316;&#20026;&#36755;&#20837;&#29305;&#24449;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#36890;&#36807;PnG-BERT&#20174;&#36716;&#24405;&#20013;&#25552;&#21462;&#30340;&#25991;&#26412;&#34920;&#31034;&#20316;&#20026;&#35821;&#35328;&#26465;&#20214;&#29305;&#24449;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Miipher&#65288;i&#65289;&#23545;&#21508;&#31181;&#38899;&#39057;&#38477;&#35299;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#65288;ii&#65289;&#20351;&#25105;&#20204;&#33021;&#22815;&#20174;&#20174;&#32593;&#32476;&#19978;&#25910;&#38598;&#30340;&#20462;&#22797;&#21518;&#30340;&#35821;&#38899;&#26679;&#26412;&#20013;&#35757;&#32451;&#20986;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#21040;&#35821;&#38899;&#65288;TTS&#65289;&#27169;&#22411;&#12290;&#38899;&#39057;&#26679;&#26412;&#21487;&#22312;&#25105;&#20204;&#30340;&#28436;&#31034;&#39029;&#38754;&#19978;&#25214;&#21040;&#65306;google.github.io/df-conformer/miipher/
&lt;/p&gt;
&lt;p&gt;
Speech restoration (SR) is a task of converting degraded speech signals into high-quality ones. In this study, we propose a robust SR model called Miipher, and apply Miipher to a new SR application: increasing the amount of high-quality training data for speech generation by converting speech samples collected from the Web to studio-quality. To make our SR model robust against various degradation, we use (i) a speech representation extracted from w2v-BERT for the input feature, and (ii) a text representation extracted from transcripts via PnG-BERT as a linguistic conditioning feature. Experiments show that Miipher (i) is robust against various audio degradation and (ii) enable us to train a high-quality text-to-speech (TTS) model from restored speech samples collected from the Web. Audio samples are available at our demo page: google.github.io/df-conformer/miipher/
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;OMG-CMDP!&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#23545;&#25239;&#24615;&#19978;&#19979;&#25991;MDPs&#20013;&#26368;&#23567;&#21270;&#36951;&#25022;&#12290;&#35813;&#31639;&#27861;&#22312;&#20855;&#22791;&#21487;&#23454;&#29616;&#20989;&#25968;&#31867;&#21644;&#22312;&#32447;&#22238;&#24402;&#39044;&#35328;&#26426;&#30340;&#26368;&#23567;&#20551;&#35774;&#19979;&#25805;&#20316;&#65292;&#20855;&#26377;&#39640;&#25928;&#12289;&#31616;&#21333;&#21644;&#23545;&#36817;&#20284;&#38169;&#35823;&#30340;&#40065;&#26834;&#24615;&#12290;&#23427;&#20855;&#26377;&#39640;&#25928;&#29575;&#26368;&#20248;&#30340;&#36951;&#25022;&#20445;&#35777;&#65292;&#26159;&#23545;&#25239;CMDPs&#39046;&#22495;&#30340;&#39318;&#20010;&#28385;&#36275;&#26368;&#23567;&#20551;&#35774;&#26465;&#20214;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.01464</link><description>&lt;p&gt;
&#20351;&#29992;&#22312;&#32447;&#20989;&#25968;&#36924;&#36817;&#30340;&#26041;&#27861;&#23454;&#29616;&#23545;&#25239;&#24615;&#19978;&#19979;&#25991;MDPs&#30340;&#39640;&#25928;&#29575;&#26368;&#20248;&#36951;&#25022;
&lt;/p&gt;
&lt;p&gt;
Efficient Rate Optimal Regret for Adversarial Contextual MDPs Using Online Function Approximation. (arXiv:2303.01464v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01464
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;OMG-CMDP!&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#23545;&#25239;&#24615;&#19978;&#19979;&#25991;MDPs&#20013;&#26368;&#23567;&#21270;&#36951;&#25022;&#12290;&#35813;&#31639;&#27861;&#22312;&#20855;&#22791;&#21487;&#23454;&#29616;&#20989;&#25968;&#31867;&#21644;&#22312;&#32447;&#22238;&#24402;&#39044;&#35328;&#26426;&#30340;&#26368;&#23567;&#20551;&#35774;&#19979;&#25805;&#20316;&#65292;&#20855;&#26377;&#39640;&#25928;&#12289;&#31616;&#21333;&#21644;&#23545;&#36817;&#20284;&#38169;&#35823;&#30340;&#40065;&#26834;&#24615;&#12290;&#23427;&#20855;&#26377;&#39640;&#25928;&#29575;&#26368;&#20248;&#30340;&#36951;&#25022;&#20445;&#35777;&#65292;&#26159;&#23545;&#25239;CMDPs&#39046;&#22495;&#30340;&#39318;&#20010;&#28385;&#36275;&#26368;&#23567;&#20551;&#35774;&#26465;&#20214;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#23545;&#25239;&#24615;&#19978;&#19979;&#25991;MDPs&#36951;&#25022;&#26368;&#23567;&#21270;&#30340;OMG-CMDP&#65281;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#22312;&#21487;&#23454;&#29616;&#30340;&#20989;&#25968;&#31867;&#21644;&#22312;&#32447;&#26368;&#23567;&#20108;&#20056;&#21644;&#23545;&#25968;&#25439;&#22833;&#22238;&#24402;&#39044;&#35328;&#26426;&#30340;&#26368;&#23567;&#20551;&#35774;&#19979;&#36816;&#34892;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#39640;&#25928;&#65288;&#20551;&#35774;&#22312;&#32447;&#22238;&#24402;&#39044;&#35328;&#26426;&#39640;&#25928;&#65289;&#65292;&#31616;&#21333;&#19988;&#23545;&#36817;&#20284;&#38169;&#35823;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#23427;&#20139;&#26377;$\widetilde{O}(H^{2.5} \sqrt{ T|S||A| ( \mathcal{R}(\mathcal{O}) + H \log(\delta^{-1}) )})$&#30340;&#36951;&#25022;&#20445;&#35777;&#65292;&#20854;&#20013;$T$&#26159;&#22238;&#21512;&#25968;&#65292;$S$&#26159;&#29366;&#24577;&#31354;&#38388;&#65292;$A$&#26159;&#21160;&#20316;&#31354;&#38388;&#65292;$H$&#26159;&#35270;&#30028;&#65292;$\mathcal{R}(\mathcal{O}) = \mathcal{R}(\mathcal{O}_{\mathrm{sq}}^\mathcal{F}) + \mathcal{R}(\mathcal{O}_{\mathrm{log}}^\mathcal{P})$&#26159;&#29992;&#20110;&#36817;&#20284;&#19978;&#19979;&#25991;&#30456;&#20851;&#22870;&#21169;&#21644;&#21160;&#24577;&#30340;&#22238;&#24402;&#39044;&#35328;&#26426;&#36951;&#25022;&#20043;&#21644;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#26159;&#23545;&#25239;CMDPs&#30340;&#31532;&#19968;&#20010;&#39640;&#25928;&#29575;&#26368;&#20248;&#36951;&#25022;&#26368;&#23567;&#21270;&#31639;&#27861;&#65292;&#24182;&#28385;&#36275;&#26368;&#23567;&#30340;&#20551;&#35774;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present the OMG-CMDP! algorithm for regret minimization in adversarial Contextual MDPs. The algorithm operates under the minimal assumptions of realizable function class and access to online least squares and log loss regression oracles. Our algorithm is efficient (assuming efficient online regression oracles), simple and robust to approximation errors. It enjoys an $\widetilde{O}(H^{2.5} \sqrt{ T|S||A| ( \mathcal{R}(\mathcal{O}) + H \log(\delta^{-1}) )})$ regret guarantee, with $T$ being the number of episodes, $S$ the state space, $A$ the action space, $H$ the horizon and $\mathcal{R}(\mathcal{O}) = \mathcal{R}(\mathcal{O}_{\mathrm{sq}}^\mathcal{F}) + \mathcal{R}(\mathcal{O}_{\mathrm{log}}^\mathcal{P})$ is the sum of the regression oracles' regret, used to approximate the context-dependent rewards and dynamics, respectively. To the best of our knowledge, our algorithm is the first efficient rate optimal regret minimization algorithm for adversarial CMDPs that operates under the mi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#20998;&#31163;&#30340;&#28304;&#20449;&#36947;&#32534;&#30721;&#26694;&#26550;&#65292;&#29992;&#20110;&#32852;&#21512;&#20219;&#21153;&#21644;&#25968;&#25454;&#23548;&#21521;&#30340;&#35821;&#20041;&#36890;&#20449;&#65292;&#20805;&#20998;&#21033;&#29992;&#35821;&#20041;&#29305;&#24449;&#21644;&#36739;&#23569;&#30340;&#39057;&#35889;&#36164;&#28304;&#65292;&#36890;&#36807;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#35299;&#20915;&#29575;&#22833;&#30495;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.13580</link><description>&lt;p&gt;
&#32852;&#21512;&#20219;&#21153;&#21644;&#25968;&#25454;&#23548;&#21521;&#30340;&#35821;&#20041;&#36890;&#20449;&#65306;&#28145;&#24230;&#20998;&#31163;&#30340;&#28304;&#20449;&#36947;&#32534;&#30721;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Joint Task and Data Oriented Semantic Communications: A Deep Separate Source-channel Coding Scheme. (arXiv:2302.13580v2 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13580
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#20998;&#31163;&#30340;&#28304;&#20449;&#36947;&#32534;&#30721;&#26694;&#26550;&#65292;&#29992;&#20110;&#32852;&#21512;&#20219;&#21153;&#21644;&#25968;&#25454;&#23548;&#21521;&#30340;&#35821;&#20041;&#36890;&#20449;&#65292;&#20805;&#20998;&#21033;&#29992;&#35821;&#20041;&#29305;&#24449;&#21644;&#36739;&#23569;&#30340;&#39057;&#35889;&#36164;&#28304;&#65292;&#36890;&#36807;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#35299;&#20915;&#29575;&#22833;&#30495;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#36890;&#20449;&#21033;&#29992;&#28304;&#25968;&#25454;&#30340;&#35821;&#20041;&#29305;&#24449;&#65292;&#36890;&#36807;&#36739;&#23569;&#30340;&#39057;&#35889;&#36164;&#28304;&#23454;&#29616;&#21508;&#31181;&#35821;&#20041;&#20219;&#21153;&#12290;&#20026;&#20102;&#21516;&#26102;&#28385;&#36275;&#25968;&#25454;&#20256;&#36755;&#21644;&#35821;&#20041;&#20219;&#21153;&#65292;&#32852;&#21512;&#25968;&#25454;&#21387;&#32553;&#21644;&#35821;&#20041;&#20998;&#26512;&#25104;&#20026;&#35821;&#20041;&#36890;&#20449;&#20013;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#20998;&#31163;&#30340;&#28304;&#20449;&#36947;&#32534;&#30721;&#26694;&#26550;&#65292;&#29992;&#20110;&#32852;&#21512;&#20219;&#21153;&#21644;&#25968;&#25454;&#23548;&#21521;&#30340;&#35821;&#20041;&#36890;&#20449;&#65292;&#24182;&#21033;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#35299;&#20915;&#20855;&#26377;&#35821;&#20041;&#22833;&#30495;&#30340;&#29575;&#22833;&#30495;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#20998;&#26512;&#28304;&#20449;&#36947;&#32534;&#30721;&#26694;&#26550;&#30340;&#36125;&#21494;&#26031;&#27169;&#22411;&#65292;&#25105;&#20204;&#21033;&#29992;&#36125;&#21494;&#26031;&#25512;&#29702;&#26041;&#27861;&#20026;&#19968;&#33324;&#25968;&#25454;&#20998;&#24067;&#21644;&#35821;&#20041;&#20219;&#21153;&#25512;&#23548;&#20986;&#19968;&#31181;&#26032;&#30340;&#29575;&#22833;&#30495;&#20248;&#21270;&#38382;&#39064;&#12290;&#25509;&#19979;&#26469;&#65292;&#38024;&#23545;&#32852;&#21512;&#22270;&#20687;&#20256;&#36755;&#21644;&#20998;&#31867;&#30340;&#20856;&#22411;&#24212;&#29992;&#65292;&#25105;&#20204;&#23558;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#19982;&#21069;&#21521;&#36866;&#24212;&#26041;&#26696;&#32467;&#21512;&#36215;&#26469;&#65292;&#26377;&#25928;&#25552;&#21462;&#22270;&#20687;&#29305;&#24449;&#21644;&#36866;&#24212;&#35821;&#20041;&#22833;&#30495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic communications are expected to accomplish various semantic tasks with relatively less spectrum resource by exploiting the semantic feature of source data. To simultaneously serve both the data transmission and semantic tasks, joint data compression and semantic analysis has become pivotal issue in semantic communications. This paper proposes a deep separate source-channel coding (DSSCC) framework for the joint task and data oriented semantic communications (JTD-SC) and utilizes the variational autoencoder approach to solve the rate-distortion problem with semantic distortion. First, by analyzing the Bayesian model of the DSSCC framework, we derive a novel rate-distortion optimization problem via the Bayesian inference approach for general data distributions and semantic tasks. Next, for a typical application of joint image transmission and classification, we combine the variational autoencoder approach with a forward adaption scheme to effectively extract image features and ad
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35299;&#37322;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#19968;&#31181;&#27969;&#34892;&#30340;&#26102;&#24207;&#24046;&#20998;&#26041;&#27861;&#20013;&#20851;&#38190;&#30340;&#31283;&#23450;&#24615;&#38382;&#39064;&#65306;&#20026;&#20160;&#20040;&#30446;&#26631;&#32593;&#32476;&#33021;&#22815;&#26377;&#25928;&#38477;&#20302;&#19981;&#28385;&#36275;&#26465;&#20214;&#26102;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2302.12537</link><description>&lt;p&gt;
&#30446;&#26631;&#32593;&#32476;&#22914;&#20309;&#31283;&#23450;&#26102;&#38388;&#24046;&#20998;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Why Target Networks Stabilise Temporal Difference Methods. (arXiv:2302.12537v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12537
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#37322;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#19968;&#31181;&#27969;&#34892;&#30340;&#26102;&#24207;&#24046;&#20998;&#26041;&#27861;&#20013;&#20851;&#38190;&#30340;&#31283;&#23450;&#24615;&#38382;&#39064;&#65306;&#20026;&#20160;&#20040;&#30446;&#26631;&#32593;&#32476;&#33021;&#22815;&#26377;&#25928;&#38477;&#20302;&#19981;&#28385;&#36275;&#26465;&#20214;&#26102;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#36817;&#26399;&#25104;&#21151;&#30340;&#20851;&#38190;&#22312;&#20110;&#19968;&#31867;&#20351;&#29992;&#19981;&#39057;&#32321;&#26356;&#26032;&#30446;&#26631;&#20540;&#36827;&#34892;&#31574;&#30053;&#35780;&#20272;&#30340;&#26102;&#24207;&#24046;&#20998;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#26377;&#20851;&#30446;&#26631;&#32593;&#32476;&#26377;&#25928;&#24615;&#30340;&#23436;&#25972;&#29702;&#35770;&#35299;&#37322;&#20173;&#28982;&#38590;&#20197;&#25417;&#25720;&#12290;&#26412;&#25991;&#38024;&#23545;&#36825;&#31181;&#27969;&#34892;&#31639;&#27861;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#26368;&#32456;&#22238;&#31572;&#20102;&#8220;&#20026;&#20160;&#20040;&#30446;&#26631;&#32593;&#32476;&#21487;&#20197;&#31283;&#23450;&#26102;&#38388;&#24046;&#20998;&#23398;&#20064;&#8221;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#35268;&#33539;&#21270;&#20102;&#37096;&#20998;&#25311;&#21512;&#30340;&#31574;&#30053;&#35780;&#20272;&#26041;&#27861;&#30340;&#27010;&#24565;&#65292;&#20854;&#20013;&#21253;&#25324;&#30446;&#26631;&#32593;&#32476;&#30340;&#20351;&#29992;&#65292;&#24182;&#19988;&#22635;&#34917;&#20102;&#25311;&#21512;&#26041;&#27861;&#21644;&#21322;&#26799;&#24230;&#26102;&#24207;&#24046;&#20998;&#31639;&#27861;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#21033;&#29992;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#33021;&#22815;&#29420;&#29305;&#22320;&#25551;&#36848;&#25152;&#35859;&#30340;&#33268;&#21629;&#19977;&#20803;&#32452;&#65292;&#21363;&#20351;&#29992;&#26102;&#24207;&#24046;&#20998;&#26356;&#26032;&#65292;&#32467;&#21512;&#65288;&#38750;&#32447;&#24615;&#65289;&#20989;&#25968;&#36924;&#36817;&#21644;&#22788;&#20110;&#31163;&#32447;&#29366;&#24577;&#30340;&#25968;&#25454;&#65292;&#36825;&#32463;&#24120;&#20250;&#23548;&#33268;&#19981;&#25910;&#25947;&#30340;&#31639;&#27861;&#12290;&#36825;&#19968;&#35748;&#35782;&#20351;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65306;&#30446;&#26631;&#32593;&#32476;&#30340;&#20351;&#29992;&#21487;&#20197;&#20943;&#36731;&#26465;&#20214;&#24046;&#26102;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Integral to recent successes in deep reinforcement learning has been a class of temporal difference methods that use infrequently updated target values for policy evaluation in a Markov Decision Process. Yet a complete theoretical explanation for the effectiveness of target networks remains elusive. In this work, we provide an analysis of this popular class of algorithms, to finally answer the question: `why do target networks stabilise TD learning'? To do so, we formalise the notion of a partially fitted policy evaluation method, which describes the use of target networks and bridges the gap between fitted methods and semigradient temporal difference algorithms. Using this framework we are able to uniquely characterise the so-called deadly triad - the use of TD updates with (nonlinear) function approximation and off-policy data - which often leads to nonconvergent algorithms. This insight leads us to conclude that the use of target networks can mitigate the effects of poor conditionin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#19982;&#25209;&#37327;&#24402;&#19968;&#21270;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#22312;&#29305;&#23450;&#32593;&#32476;&#20013;&#34920;&#29616;&#20026;&#21333;&#27425;&#37325;&#25490;&#19982;&#38543;&#26426;&#37325;&#25490;&#25910;&#25947;&#21040;&#19981;&#21516;&#30340;&#25197;&#26354;&#20840;&#23616;&#26368;&#20248;&#28857;&#65292;&#24314;&#35758;&#20351;&#29992;&#38543;&#26426;&#37325;&#25490;&#12290;</title><link>http://arxiv.org/abs/2302.12444</link><description>&lt;p&gt;
&#20851;&#20110;&#24102;&#25209;&#37327;&#24402;&#19968;&#21270;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Training Instability of Shuffling SGD with Batch Normalization. (arXiv:2302.12444v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12444
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#19982;&#25209;&#37327;&#24402;&#19968;&#21270;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#22312;&#29305;&#23450;&#32593;&#32476;&#20013;&#34920;&#29616;&#20026;&#21333;&#27425;&#37325;&#25490;&#19982;&#38543;&#26426;&#37325;&#25490;&#25910;&#25947;&#21040;&#19981;&#21516;&#30340;&#25197;&#26354;&#20840;&#23616;&#26368;&#20248;&#28857;&#65292;&#24314;&#35758;&#20351;&#29992;&#38543;&#26426;&#37325;&#25490;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#19982;&#25209;&#37327;&#24402;&#19968;&#21270;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#21457;&#29616;&#24120;&#29992;&#30340;&#21333;&#27425;&#37325;&#25490;&#21644;&#38543;&#26426;&#37325;&#25490;&#36825;&#20004;&#31181; SGD &#26041;&#24335;&#30340;&#35757;&#32451;&#31283;&#23450;&#24615;&#24046;&#24322;&#38750;&#24120;&#22823;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#20351;&#29992;&#25209;&#37327;&#24402;&#19968;&#21270;&#30340;&#32447;&#24615;&#32593;&#32476;&#22238;&#24402;&#38382;&#39064;&#20013;&#65292;&#21333;&#27425;&#37325;&#25490;&#21644;&#38543;&#26426;&#37325;&#25490;&#20250;&#20998;&#21035;&#25910;&#25947;&#21040;&#25197;&#26354;&#30340;&#20840;&#23616;&#26368;&#20248;&#28857;&#65292;&#32780;&#22312;&#20998;&#31867;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#31350;&#20102;&#23427;&#20204;&#30340;&#35757;&#32451;&#26159;&#21542;&#20250;&#21457;&#25955;&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#35777;&#39564;&#35777;&#32467;&#26524;&#65292;&#24314;&#35758;&#22312;&#20351;&#29992; SGD &#19982;&#25209;&#37327;&#24402;&#19968;&#21270;&#26102;&#65292;&#20248;&#20808;&#32771;&#34385;&#38543;&#26426;&#37325;&#25490;&#12290;
&lt;/p&gt;
&lt;p&gt;
We uncover how SGD interacts with batch normalization and can exhibit undesirable training dynamics such as divergence. More precisely, we study how Single Shuffle (SS) and Random Reshuffle (RR) -- two widely used variants of SGD -- interact surprisingly differently in the presence of batch normalization: RR leads to much more stable evolution of training loss than SS. As a concrete example, for regression using a linear network with batch normalization, we prove that SS and RR converge to distinct global optima that are "distorted" away from gradient descent. Thereafter, for classification we characterize conditions under which training divergence for SS and RR can, and cannot occur. We present explicit constructions to show how SS leads to distorted optima in regression and divergence for classification, whereas RR avoids both distortion and divergence. We validate our results by confirming them empirically in realistic settings, and conclude that the separation between SS and RR use
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#33258;&#21160;&#31639;&#27861;&#37197;&#32622;&#26469;&#35299;&#20915;&#21442;&#25968;&#25511;&#21046;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#26041;&#27861;&#24615;&#33021;&#30340;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2302.12334</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#21160;&#31639;&#27861;&#37197;&#32622;&#23454;&#29616;&#21442;&#25968;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Using Automated Algorithm Configuration for Parameter Control. (arXiv:2302.12334v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#33258;&#21160;&#31639;&#27861;&#37197;&#32622;&#26469;&#35299;&#20915;&#21442;&#25968;&#25511;&#21046;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#26041;&#27861;&#24615;&#33021;&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#31639;&#27861;&#37197;&#32622;&#65288;DAC&#65289;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#33258;&#21160;&#23398;&#20064;&#25511;&#21046;&#31639;&#27861;&#21442;&#25968;&#30340;&#31574;&#30053;&#12290;&#26368;&#36817;&#20960;&#24180;&#65292;&#36825;&#20010;&#38382;&#39064;&#22312;&#36827;&#21270;&#31038;&#21306;&#20013;&#21463;&#21040;&#20102;&#30456;&#24403;&#22810;&#30340;&#20851;&#27880;&#12290;&#20026;&#20102;&#23545;&#19981;&#21516;DAC&#35299;&#20915;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#38480;&#21046;&#36827;&#34892;&#32467;&#26500;&#24615;&#29702;&#35299;&#65292;&#25317;&#26377;&#19968;&#20010;&#22909;&#30340;&#22522;&#20934;&#38598;&#21512;&#26159;&#38750;&#24120;&#26377;&#24517;&#35201;&#30340;&#12290;&#22312;&#26368;&#36817;&#20851;&#20110;&#25552;&#20379;&#20855;&#26377;&#21487;&#29702;&#35299;&#30340;&#29702;&#35770;&#23646;&#24615;&#21644;&#22522;&#20934;&#30495;&#20540;&#20449;&#24687;&#30340;DAC&#22522;&#20934;&#30340;&#24037;&#20316;&#20013;&#65292;&#26412;&#25991;&#24314;&#35758;&#23558;&#20851;&#38190;&#21442;&#25968;&#955;&#30340;&#25511;&#21046;&#20316;&#20026;&#26032;&#30340;DAC&#22522;&#20934;&#65292;&#29992;&#20110;&#35299;&#20915;OneMax&#38382;&#39064;&#30340;(1+(&#955;,&#955;))&#36951;&#20256;&#31639;&#27861;&#12290;&#25105;&#20204;&#23545;&#22914;&#20309;&#36890;&#36807;&#20351;&#29992;&#65288;&#38745;&#24577;&#65289;&#33258;&#21160;&#31639;&#27861;&#37197;&#32622;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#35299;&#20915;DAC&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#26041;&#27861;&#24615;&#33021;&#30340;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#22987;&#32456;&#20248;&#20110;&#40664;&#35748;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic Algorithm Configuration (DAC) tackles the question of how to automatically learn policies to control parameters of algorithms in a data-driven fashion. This question has received considerable attention from the evolutionary community in recent years. Having a good benchmark collection to gain structural understanding on the effectiveness and limitations of different solution methods for DAC is therefore strongly desirable. Following recent work on proposing DAC benchmarks with well-understood theoretical properties and ground truth information, in this work, we suggest as a new DAC benchmark the controlling of the key parameter $\lambda$ in the $(1+(\lambda,\lambda))$~Genetic Algorithm for solving OneMax problems. We conduct a study on how to solve the DAC problem via the use of (static) automated algorithm configuration on the benchmark, and propose techniques to significantly improve the performance of the approach. Our approach is able to consistently outperform the default 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26102;&#21464;&#20449;&#21495;&#24674;&#22797;&#26041;&#27861;&#65292;&#36890;&#36807;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#19982;Sobolev&#24179;&#28369;&#31639;&#23376;&#32452;&#25104;&#30340;&#19987;&#38376;&#25439;&#22833;&#20989;&#25968;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#24674;&#22797;&#26102;&#21464;&#22270;&#20449;&#21495;&#12290;</title><link>http://arxiv.org/abs/2302.11313</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26102;&#21464;&#20449;&#21495;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Time-varying Signals Recovery via Graph Neural Networks. (arXiv:2302.11313v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26102;&#21464;&#20449;&#21495;&#24674;&#22797;&#26041;&#27861;&#65292;&#36890;&#36807;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#19982;Sobolev&#24179;&#28369;&#31639;&#23376;&#32452;&#25104;&#30340;&#19987;&#38376;&#25439;&#22833;&#20989;&#25968;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#24674;&#22797;&#26102;&#21464;&#22270;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#21464;&#22270;&#20449;&#21495;&#30340;&#24674;&#22797;&#26159;&#20256;&#24863;&#22120;&#32593;&#32476;&#21644;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#26377;&#25928;&#22320;&#25429;&#25417;&#36825;&#20123;&#20449;&#21495;&#30340;&#26102;&#31354;&#20449;&#24687;&#23545;&#19979;&#28216;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#20351;&#29992;&#36825;&#20123;&#22270;&#20449;&#21495;&#26102;&#38388;&#24046;&#20998;&#30340;&#24179;&#28369;&#24615;&#20316;&#20026;&#19968;&#20010;&#21021;&#22987;&#20551;&#35774;&#12290;&#28982;&#32780;&#65292;&#24403;&#35813;&#20808;&#39564;&#19981;&#25104;&#31435;&#26102;&#65292;&#36825;&#31181;&#24179;&#28369;&#24615;&#20551;&#35774;&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#21253;&#21547;&#19968;&#20010;&#23398;&#20064;&#27169;&#22359;&#26469;&#25918;&#26494;&#36825;&#20010;&#20551;&#35774;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24674;&#22797;&#26102;&#21464;&#22270;&#20449;&#21495;&#30340;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;(TimeGNN)&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20351;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#24182;&#37319;&#29992;&#30001;&#22343;&#26041;&#35823;&#24046;&#20989;&#25968;&#21644;Sobolev&#24179;&#28369;&#31639;&#23376;&#32452;&#25104;&#30340;&#19987;&#38376;&#25439;&#22833;&#12290;TimeGNN&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#26174;&#31034;&#20986;&#19982;&#20197;&#21069;&#26041;&#27861;&#31454;&#20105;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recovery of time-varying graph signals is a fundamental problem with numerous applications in sensor networks and forecasting in time series. Effectively capturing the spatio-temporal information in these signals is essential for the downstream tasks. Previous studies have used the smoothness of the temporal differences of such graph signals as an initial assumption. Nevertheless, this smoothness assumption could result in a degradation of performance in the corresponding application when the prior does not hold. In this work, we relax the requirement of this hypothesis by including a learning module. We propose a Time Graph Neural Network (TimeGNN) for the recovery of time-varying graph signals. Our algorithm uses an encoder-decoder architecture with a specialized loss composed of a mean squared error function and a Sobolev smoothness operator.TimeGNN shows competitive performance against previous methods in real datasets.
&lt;/p&gt;</description></item><item><title>LabelPrompt&#26159;&#19968;&#31181;&#38754;&#21521;&#20851;&#31995;&#20998;&#31867;&#20219;&#21153;&#30340;&#25552;&#31034;&#24335;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#20041;&#39069;&#22806;&#30340;&#20196;&#29260;&#26469;&#34920;&#31034;&#20851;&#31995;&#26631;&#31614;&#65292;&#24182;&#20351;&#29992;&#25552;&#31034;&#27169;&#26495;&#26041;&#27861;&#26126;&#30830;&#26500;&#24314;&#23427;&#20204;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#23558;&#22635;&#20805;&#25513;&#30721;&#26631;&#35760;&#30340;&#33258;&#28982;&#35821;&#35328;&#35789;&#27719;&#19982;&#35821;&#20041;&#20851;&#31995;&#26631;&#31614;&#30456;&#20851;&#32852;&#30340;&#25361;&#25112;&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#36824;&#23454;&#29616;&#20102;&#19968;&#20010;&#23454;&#20307;&#24863;&#30693;&#27169;&#22359;&#26469;&#20943;&#36731;&#39044;&#27979;&#20851;&#31995;&#21644;&#32473;&#23450;&#23454;&#20307;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.08068</link><description>&lt;p&gt;
LabelPrompt: &#20851;&#20110;&#20851;&#31995;&#20998;&#31867;&#30340;&#26377;&#25928;&#30340;&#25552;&#31034;&#24335;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
LabelPrompt: Effective Prompt-based Learning for Relation Classification. (arXiv:2302.08068v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08068
&lt;/p&gt;
&lt;p&gt;
LabelPrompt&#26159;&#19968;&#31181;&#38754;&#21521;&#20851;&#31995;&#20998;&#31867;&#20219;&#21153;&#30340;&#25552;&#31034;&#24335;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#20041;&#39069;&#22806;&#30340;&#20196;&#29260;&#26469;&#34920;&#31034;&#20851;&#31995;&#26631;&#31614;&#65292;&#24182;&#20351;&#29992;&#25552;&#31034;&#27169;&#26495;&#26041;&#27861;&#26126;&#30830;&#26500;&#24314;&#23427;&#20204;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#23558;&#22635;&#20805;&#25513;&#30721;&#26631;&#35760;&#30340;&#33258;&#28982;&#35821;&#35328;&#35789;&#27719;&#19982;&#35821;&#20041;&#20851;&#31995;&#26631;&#31614;&#30456;&#20851;&#32852;&#30340;&#25361;&#25112;&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#36824;&#23454;&#29616;&#20102;&#19968;&#20010;&#23454;&#20307;&#24863;&#30693;&#27169;&#22359;&#26469;&#20943;&#36731;&#39044;&#27979;&#20851;&#31995;&#21644;&#32473;&#23450;&#23454;&#20307;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#36890;&#36807;&#23558;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#36716;&#25442;&#20026;&#22635;&#31354;&#24335;&#26684;&#24335;&#65292;&#20197;&#26356;&#22909;&#22320;&#19982;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#23545;&#40784;&#30340;&#26041;&#24335;&#65292;&#25552;&#31034;&#24335;&#23398;&#20064;&#22312;&#35768;&#22810;NLP&#20219;&#21153;&#20013;&#21464;&#24471;&#27969;&#34892;&#36215;&#26469;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#20851;&#31995;&#20998;&#31867;&#20219;&#21153;&#38754;&#20020;&#30528;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23558;&#22635;&#20805;&#25513;&#30721;&#26631;&#35760;&#30340;&#33258;&#28982;&#35821;&#35328;&#35789;&#27719;&#19982;&#35821;&#20041;&#20851;&#31995;&#26631;&#31614;&#65288;&#22914;"org:founded_by"&#65289;&#30456;&#20851;&#32852;&#26159;&#22256;&#38590;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25552;&#31034;&#24335;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;LabelPrompt&#65292;&#29992;&#20110;&#20851;&#31995;&#20998;&#31867;&#20219;&#21153;&#12290;&#21463;&#21040;&#8220;&#32473;&#20104;&#27169;&#22411;&#36873;&#25321;&#8221;&#30340;&#30452;&#35273;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#39318;&#20808;&#23450;&#20041;&#39069;&#22806;&#30340;&#20196;&#29260;&#26469;&#34920;&#31034;&#20851;&#31995;&#26631;&#31614;&#65292;&#23558;&#36825;&#20123;&#20196;&#29260;&#35270;&#20026;&#20855;&#26377;&#35821;&#20041;&#21021;&#22987;&#21270;&#30340;&#21475;&#36848;&#32773;&#65292;&#24182;&#20351;&#29992;&#25552;&#31034;&#27169;&#26495;&#26041;&#27861;&#26126;&#30830;&#26500;&#24314;&#23427;&#20204;&#12290;&#28982;&#21518;&#65292;&#20026;&#20102;&#20943;&#36731;&#39044;&#27979;&#20851;&#31995;&#21644;&#32473;&#23450;&#23454;&#20307;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#20010;&#23454;&#20307;&#24863;&#30693;&#27169;&#22359;&#65292;&#24182;&#37319;&#29992;&#23545;&#25239;&#24615;&#30340;&#26041;&#24335;&#36827;&#34892;&#24341;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, prompt-based learning has gained popularity across many natural language processing (NLP) tasks by reformulating them into a cloze-style format to better align pre-trained language models (PLMs) with downstream tasks. However, applying this approach to relation classification poses unique challenges. Specifically, associating natural language words that fill the masked token with semantic relation labels (\textit{e.g.} \textit{``org:founded\_by}'') is difficult. To address this challenge, this paper presents a novel prompt-based learning method, namely LabelPrompt, for the relation classification task. Motivated by the intuition to ``GIVE MODEL CHOICES!'', we first define additional tokens to represent relation labels, which regard these tokens as the verbaliser with semantic initialisation and explicitly construct them with a prompt template method. Then, to mitigate inconsistency between predicted relations and given entities, we implement an entity-aware module with contra
&lt;/p&gt;</description></item><item><title>&#26080;&#20808;&#39564;&#22240;&#26524;&#23398;&#20064;&#26159;&#19968;&#20010;&#35299;&#20915;&#39044;&#27979;&#26032;&#22411;&#24178;&#39044;&#25514;&#26045;&#20010;&#24615;&#21270;&#24433;&#21709;&#30340;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#20803;&#23398;&#20064;&#23545;&#20219;&#21153;&#30340;&#22788;&#29702;&#36798;&#25104;&#20102;&#30446;&#30340;&#65292;&#33021;&#22815;&#23558;&#24178;&#39044;&#25514;&#26045;&#30340;&#30693;&#35782;&#20256;&#36755;&#21040;&#26410;&#35265;&#36807;&#30340;&#24178;&#39044;&#25514;&#26045;&#20013;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.12292</link><description>&lt;p&gt;
&#26080;&#20808;&#39564;&#22240;&#26524;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Zero-shot causal learning. (arXiv:2301.12292v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12292
&lt;/p&gt;
&lt;p&gt;
&#26080;&#20808;&#39564;&#22240;&#26524;&#23398;&#20064;&#26159;&#19968;&#20010;&#35299;&#20915;&#39044;&#27979;&#26032;&#22411;&#24178;&#39044;&#25514;&#26045;&#20010;&#24615;&#21270;&#24433;&#21709;&#30340;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#20803;&#23398;&#20064;&#23545;&#20219;&#21153;&#30340;&#22788;&#29702;&#36798;&#25104;&#20102;&#30446;&#30340;&#65292;&#33021;&#22815;&#23558;&#24178;&#39044;&#25514;&#26045;&#30340;&#30693;&#35782;&#20256;&#36755;&#21040;&#26410;&#35265;&#36807;&#30340;&#24178;&#39044;&#25514;&#26045;&#20013;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20010;&#24615;&#21270;&#21307;&#30103;&#12289;&#20844;&#20849;&#25919;&#31574;&#21644;&#22312;&#32447;&#33829;&#38144;&#31561;&#39046;&#22495;&#65292;&#39044;&#27979;&#19981;&#21516;&#24178;&#39044;&#25514;&#26045;&#23545;&#29305;&#23450;&#20010;&#20307;&#30340;&#22240;&#26524;&#24433;&#21709;&#38750;&#24120;&#37325;&#35201;&#12290;&#39044;&#27979;&#29616;&#26377;&#24178;&#39044;&#25514;&#26045;&#30340;&#24433;&#21709;&#26377;&#35768;&#22810;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#22522;&#20110;&#25509;&#21463;&#36807;&#24178;&#39044;&#25514;&#26045;&#30340;&#20010;&#20307;&#30340;&#21382;&#21490;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#22330;&#26223;&#20013;&#65292;&#39044;&#27979;&#26032;&#22411;&#24178;&#39044;&#25514;&#26045;&#30340;&#24433;&#21709;&#20063;&#24456;&#37325;&#35201;&#65292;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#35299;&#20915;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#26080;&#20808;&#39564;&#22240;&#26524;&#23398;&#20064;&#65306;&#39044;&#27979;&#26032;&#22411;&#24178;&#39044;&#25514;&#26045;&#30340;&#20010;&#24615;&#21270;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;CaML&#65292;&#36825;&#26159;&#19968;&#20010;&#22240;&#26524;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#23558;&#27599;&#20010;&#24178;&#39044;&#25514;&#26045;&#30340;&#20010;&#24615;&#21270;&#39044;&#27979;&#25928;&#26524;&#20316;&#20026;&#19968;&#20010;&#20219;&#21153;&#26469;&#36827;&#34892;&#22788;&#29702;&#12290;CaML&#22312;&#25968;&#21315;&#20010;&#20219;&#21153;&#20013;&#35757;&#32451;&#21333;&#19968;&#30340;&#20803;&#27169;&#22411;&#65292;&#27599;&#20010;&#20219;&#21153;&#37117;&#26159;&#36890;&#36807;&#25277;&#26679;&#29983;&#25104;&#19968;&#20010;&#24178;&#39044;&#25514;&#26045;&#21450;&#20854;&#25509;&#25910;&#32773;&#21644;&#38750;&#25509;&#25910;&#32773;&#26469;&#26500;&#24314;&#30340;&#12290;&#36890;&#36807;&#21033;&#29992;&#24178;&#39044;&#20449;&#24687;&#65288;&#20363;&#22914;&#65292;&#33647;&#29289;&#30340;&#23646;&#24615;&#65289;&#21644;&#20010;&#20307;&#29305;&#24449;&#65288;&#20363;&#22914;&#65292;&#29305;&#23450;&#20010;&#20307;&#30340;&#21307;&#30103;&#35760;&#24405;&#65289;&#65292;CaML&#23398;&#20064;&#22914;&#20309;&#23558;&#24050;&#35266;&#23519;&#21040;&#30340;&#24178;&#39044;&#25514;&#26045;&#30340;&#30693;&#35782;&#26377;&#25928;&#22320;&#20256;&#36755;&#32473;&#26410;&#35265;&#36807;&#30340;&#24178;&#39044;&#25514;&#26045;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#20855;&#26377;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#24178;&#39044;&#25514;&#26045;&#24182;&#32988;&#36807;&#29616;&#26377;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting how different interventions will causally affect a specific individual is important in a variety of domains such as personalized medicine, public policy, and online marketing. There are a large number of methods to predict the effect of an existing intervention based on historical data from individuals who received it. However, in many settings it is important to predict the effects of novel interventions (\emph{e.g.}, a newly invented drug), which these methods do not address. Here, we consider zero-shot causal learning: predicting the personalized effects of a novel intervention. We propose CaML, a causal meta-learning framework which formulates the personalized prediction of each intervention's effect as a task. CaML trains a single meta-model across thousands of tasks, each constructed by sampling an intervention, along with its recipients and nonrecipients. By leveraging both intervention information (\emph{e.g.}, a drug's attributes) and individual features~(\emph{e.g.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21482;&#20351;&#29992;&#29615;&#36335;&#26816;&#27979;&#22120;&#20449;&#24687;&#20316;&#20026;&#36755;&#20837;&#30340;&#28145;&#24230;Q&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#35774;&#35745;&#34394;&#20551;&#25968;&#25454;&#27880;&#20837;&#25915;&#20987;&#21644;&#38543;&#26426;&#22122;&#22768;&#25915;&#20987;&#26469;&#30740;&#31350;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.12036</link><description>&lt;p&gt;
&#22312;&#32771;&#34385;&#34394;&#20551;&#25968;&#25454;&#27880;&#20837;&#25915;&#20987;&#21644;&#38450;&#24481;&#30340;&#22369;&#36947;&#35745;&#37327;&#24212;&#29992;&#20013;&#20998;&#26512;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Analyzing Robustness of the Deep Reinforcement Learning Algorithm in Ramp Metering Applications Considering False Data Injection Attack and Defense. (arXiv:2301.12036v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12036
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21482;&#20351;&#29992;&#29615;&#36335;&#26816;&#27979;&#22120;&#20449;&#24687;&#20316;&#20026;&#36755;&#20837;&#30340;&#28145;&#24230;Q&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#35774;&#35745;&#34394;&#20551;&#25968;&#25454;&#27880;&#20837;&#25915;&#20987;&#21644;&#38543;&#26426;&#22122;&#22768;&#25915;&#20987;&#26469;&#30740;&#31350;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22369;&#36947;&#35745;&#37327;&#26159;&#25511;&#21046;&#36827;&#20837;&#39640;&#36895;&#20844;&#36335;&#20027;&#24178;&#36947;&#30340;&#36710;&#36742;&#34892;&#39542;&#30340;&#34892;&#20026;&#12290;&#20960;&#21313;&#24180;&#30340;&#22369;&#36947;&#35745;&#37327;&#23454;&#36341;&#35777;&#26126;&#65292;&#22369;&#36947;&#35745;&#37327;&#21487;&#20197;&#20943;&#23569;&#24635;&#26053;&#34892;&#26102;&#38388;&#65292;&#20943;&#36731;&#36710;&#27969;&#27874;&#21160;&#65292;&#36890;&#36807;&#24179;&#28369;&#20132;&#36890;&#20132;&#32455;&#36807;&#31243;&#20943;&#23569;&#36861;&#23614;&#30896;&#25758;&#31561;&#12290;&#38500;&#20102;&#20256;&#32479;&#30340;&#25511;&#21046;&#31639;&#27861;&#22914;ALINEA&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#31639;&#27861;&#24050;&#32463;&#34987;&#24341;&#20837;&#26469;&#26500;&#24314;&#26356;&#31934;&#32454;&#30340;&#25511;&#21046;&#12290;&#28982;&#32780;&#65292;&#20173;&#28982;&#23384;&#22312;&#20004;&#20010;&#25361;&#25112;&#38459;&#30861;&#20102;DRL&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#24212;&#29992;&#65306;(1)&#19968;&#20123;&#31639;&#27861;&#30340;&#20551;&#35774;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#24456;&#38590;&#21305;&#37197;&#65307;(2)&#20016;&#23500;&#30340;&#36755;&#20837;&#29366;&#24577;&#21487;&#33021;&#20351;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#21644;&#25968;&#25454;&#22122;&#22768;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21482;&#20351;&#29992;&#29615;&#36335;&#26816;&#27979;&#22120;&#20449;&#24687;&#20316;&#20026;&#36755;&#20837;&#30340;&#28145;&#24230;Q&#23398;&#20064;&#31639;&#27861;&#12290;&#28982;&#21518;&#65292;&#35774;&#35745;&#20102;&#19968;&#32452;&#34394;&#20551;&#25968;&#25454;&#27880;&#20837;&#25915;&#20987;&#21644;&#38543;&#26426;&#22122;&#22768;&#25915;&#20987;&#26469;&#30740;&#31350;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#35813;&#27169;&#22411;&#30340;&#20027;&#35201;&#20248;&#28857;&#26159;&#21487;&#20197;&#24212;&#29992;&#20110;...
&lt;/p&gt;
&lt;p&gt;
Ramp metering is the act of controlling on-going vehicles to the highway mainlines. Decades of practices of ramp metering have proved that ramp metering can decrease total travel time, mitigate shockwaves, decrease rear-end collisions by smoothing the traffic interweaving process, etc. Besides traditional control algorithm like ALINEA, Deep Reinforcement Learning (DRL) algorithms have been introduced to build a finer control. However, two remaining challenges still hinder DRL from being implemented in the real world: (1) some assumptions of algorithms are hard to be matched in the real world; (2) the rich input states may make the model vulnerable to attacks and data noises. To investigate these issues, we propose a Deep Q-Learning algorithm using only loop detectors information as inputs in this study. Then, a set of False Data Injection attacks and random noise attack are designed to investigate the robustness of the model. The major benefit of the model is that it can be applied to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#26234;&#33021;&#20307;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#24182;&#34892;&#24615;&#21644;&#27807;&#36890;&#24320;&#38144;&#20043;&#38388;&#30340;&#25240;&#20013;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#32452;&#20195;&#29702;&#20043;&#38388;&#36890;&#20449;&#36718;&#27425;&#21644;&#21512;&#20316;&#23398;&#20064;&#36807;&#31243;&#21518;&#24724;&#20043;&#38388;&#30340;&#26435;&#34913;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2301.11442</link><description>&lt;p&gt;
&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#30340;&#39640;&#25928;&#27807;&#36890;&#21512;&#20316;&#21518;&#24724;&#26368;&#23567;&#21270;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Communication-Efficient Collaborative Regret Minimization in Multi-Armed Bandits. (arXiv:2301.11442v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#26234;&#33021;&#20307;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#24182;&#34892;&#24615;&#21644;&#27807;&#36890;&#24320;&#38144;&#20043;&#38388;&#30340;&#25240;&#20013;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#32452;&#20195;&#29702;&#20043;&#38388;&#36890;&#20449;&#36718;&#27425;&#21644;&#21512;&#20316;&#23398;&#20064;&#36807;&#31243;&#21518;&#24724;&#20043;&#38388;&#30340;&#26435;&#34913;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#26234;&#33021;&#20307;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#24182;&#34892;&#24615;&#21644;&#27807;&#36890;&#24320;&#38144;&#20043;&#38388;&#30340;&#25240;&#20013;&#38382;&#39064;&#12290;&#20026;&#20102;&#22312;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#23454;&#29616;&#21518;&#24724;&#26368;&#23567;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#32452;&#20195;&#29702;&#20043;&#38388;&#36890;&#20449;&#36718;&#27425;&#21644;&#21512;&#20316;&#23398;&#20064;&#36807;&#31243;&#21518;&#24724;&#20043;&#38388;&#30340;&#26435;&#34913;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the collaborative learning model, which concerns the tradeoff between parallelism and communication overhead in multi-agent multi-armed bandits. For regret minimization in multi-armed bandits, we present the first set of tradeoffs between the number of rounds of communication among the agents and the regret of the collaborative learning process.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#39318;&#27425;&#20351;&#29992;Transformer&#36827;&#34892;&#23460;&#22806;&#23450;&#20301;&#30340;&#20219;&#21153;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#20999;&#29255;Transformer&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#36724;&#21521;&#29305;&#24615;&#37325;&#26032;&#32452;&#32455;&#20102;&#28608;&#20809;&#38647;&#36798;&#25195;&#25551;&#30340;&#20999;&#29255;&#12290;&#27492;&#22806;&#65292;&#20316;&#32773;&#36824;&#24341;&#20837;&#20102;Perth-WA&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.08957</link><description>&lt;p&gt;
&#20999;&#29255;Transformer&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#29992;&#20110;3D&#28857;&#20113;&#22320;&#22270;&#20013;&#30340;&#20845;&#33258;&#30001;&#24230;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Slice Transformer and Self-supervised Learning for 6DoF Localization in 3D Point Cloud Maps. (arXiv:2301.08957v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08957
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#39318;&#27425;&#20351;&#29992;Transformer&#36827;&#34892;&#23460;&#22806;&#23450;&#20301;&#30340;&#20219;&#21153;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#20999;&#29255;Transformer&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#36724;&#21521;&#29305;&#24615;&#37325;&#26032;&#32452;&#32455;&#20102;&#28608;&#20809;&#38647;&#36798;&#25195;&#25551;&#30340;&#20999;&#29255;&#12290;&#27492;&#22806;&#65292;&#20316;&#32773;&#36824;&#24341;&#20837;&#20102;Perth-WA&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#23450;&#20301;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#39318;&#27425;&#37319;&#29992;&#20102;Transformer&#36827;&#34892;&#20351;&#29992;&#28608;&#20809;&#38647;&#36798;&#25968;&#25454;&#36827;&#34892;&#23460;&#22806;&#23450;&#20301;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#37325;&#26032;&#32452;&#32455;&#20102;$360^\circ$&#28608;&#20809;&#38647;&#36798;&#25195;&#25551;&#30340;&#20999;&#29255;&#65292;&#21033;&#29992;&#20854;&#36724;&#21521;&#29305;&#24615;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#31216;&#20026;&#20999;&#29255;Transformer&#65292;&#22312;&#22788;&#29702;&#20999;&#29255;&#26102;&#20351;&#29992;&#22810;&#22836;&#27880;&#24847;&#21147;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#39318;&#27425;&#21033;&#29992;&#22810;&#22836;&#27880;&#24847;&#21147;&#36827;&#34892;&#23460;&#22806;&#28857;&#20113;&#22788;&#29702;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;Perth-WA&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#35199;&#28595;&#22823;&#21033;&#20122;&#29632;&#26031;&#24066;&#30340;&#22823;&#35268;&#27169;&#28608;&#20809;&#38647;&#36798;&#22320;&#22270;&#65292;&#28085;&#30422;&#20102;&#32422;4&#24179;&#26041;&#20844;&#37324;&#30340;&#21306;&#22495;&#12290;Perth-WA&#25552;&#20379;&#20102;&#23450;&#20301;&#26631;&#27880;&#12290;&#25105;&#20204;&#22312;Perth-WA&#21644;Appollo-SouthBay&#25968;&#25454;&#38598;&#19978;&#23545;&#25152;&#25552;&#20986;&#30340;&#23450;&#20301;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;&#24120;&#35265;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#22914;ModelNet40&#21644;...&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Precise localization is critical for autonomous vehicles. We present a self-supervised learning method that employs Transformers for the first time for the task of outdoor localization using LiDAR data. We propose a pre-text task that reorganizes the slices of a $360^\circ$ LiDAR scan to leverage its axial properties. Our model, called Slice Transformer, employs multi-head attention while systematically processing the slices. To the best of our knowledge, this is the first instance of leveraging multi-head attention for outdoor point clouds. We additionally introduce the Perth-WA dataset, which provides a large-scale LiDAR map of Perth city in Western Australia, covering $\sim$4km$^2$ area. Localization annotations are provided for Perth-WA. The proposed localization method is thoroughly evaluated on Perth-WA and Appollo-SouthBay datasets. We also establish the efficacy of our self-supervised learning approach for the common downstream task of object classification using ModelNet40 and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#34920;&#31034;&#35856;&#27874;&#20989;&#25968;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#24182;&#23558;&#36825;&#19968;&#26041;&#27861;&#25193;&#23637;&#21040;&#20102;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#36890;&#36807;&#23545;&#27604;&#22522;&#20934;&#27979;&#35797;&#21457;&#29616;&#20854;&#24615;&#33021;&#20248;&#36234;&#12290;</title><link>http://arxiv.org/abs/2212.07462</link><description>&lt;p&gt;
&#35856;&#27874;&#65288;&#37327;&#23376;&#65289;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Harmonic (Quantum) Neural Networks. (arXiv:2212.07462v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07462
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#34920;&#31034;&#35856;&#27874;&#20989;&#25968;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#24182;&#23558;&#36825;&#19968;&#26041;&#27861;&#25193;&#23637;&#21040;&#20102;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#36890;&#36807;&#23545;&#27604;&#22522;&#20934;&#27979;&#35797;&#21457;&#29616;&#20854;&#24615;&#33021;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35856;&#27874;&#20989;&#25968;&#22312;&#33258;&#28982;&#30028;&#20013;&#38750;&#24120;&#24120;&#35265;&#65292;&#22312;&#40614;&#20811;&#26031;&#38886;&#26041;&#31243;&#12289;&#32435;&#32500;-&#26031;&#25176;&#20811;&#26031;&#26041;&#31243;&#12289;&#28909;&#20256;&#23548;&#26041;&#31243;&#21644;&#27874;&#21160;&#26041;&#31243;&#30340;&#26497;&#38480;&#24773;&#20917;&#20013;&#37117;&#20986;&#29616;&#12290;&#22240;&#27492;&#65292;&#35856;&#27874;&#20989;&#25968;&#26377;&#35768;&#22810;&#24212;&#29992;&#65292;&#20174;&#24037;&#19994;&#36807;&#31243;&#20248;&#21270;&#21040;&#26426;&#22120;&#20154;&#36335;&#24452;&#35268;&#21010;&#21644;&#38543;&#26426;&#34892;&#36208;&#30340;&#39318;&#27425;&#36864;&#20986;&#26102;&#38388;&#30340;&#35745;&#31639;&#12290;&#23613;&#31649;&#35856;&#27874;&#20989;&#25968;&#38750;&#24120;&#26222;&#36941;&#21644;&#30456;&#20851;&#65292;&#20294;&#22312;&#26426;&#22120;&#23398;&#20064;&#29615;&#22659;&#20013;&#24456;&#23569;&#23581;&#35797;&#23558;&#20854;&#24402;&#32435;&#20559;&#22909;&#24341;&#20837;&#20854;&#20013;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#26377;&#25928;&#34920;&#31034;&#35856;&#27874;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#36825;&#20123;&#32467;&#26524;&#25193;&#23637;&#21040;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#20013;&#20197;&#23637;&#31034;&#25105;&#20204;&#26041;&#27861;&#30340;&#26222;&#36866;&#24615;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#65288;&#37327;&#23376;&#65289;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#36739;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Harmonic functions are abundant in nature, appearing in limiting cases of Maxwell's, Navier-Stokes equations, the heat and the wave equation. Consequently, there are many applications of harmonic functions from industrial process optimisation to robotic path planning and the calculation of first exit times of random walks. Despite their ubiquity and relevance, there have been few attempts to incorporate inductive biases towards harmonic functions in machine learning contexts. In this work, we demonstrate effective means of representing harmonic functions in neural networks and extend such results also to quantum neural networks to demonstrate the generality of our approach. We benchmark our approaches against (quantum) physics-informed neural networks, where we show favourable performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#21487;&#24494;&#21270;&#33410;&#28857;&#36873;&#25321;&#30340;&#22810;&#35270;&#35282;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#21487;&#26377;&#25928;&#22320;&#21033;&#29992;&#22810;&#35270;&#22270;&#25968;&#25454;&#30340;&#20114;&#34917;&#21644;&#19968;&#33268;&#20449;&#24687;&#65292;&#25552;&#21462;&#22810;&#20010;&#23545;&#35937;&#20043;&#38388;&#30340;&#28508;&#22312;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2212.05124</link><description>&lt;p&gt;
&#20855;&#26377;&#21487;&#24494;&#21270;&#33410;&#28857;&#36873;&#25321;&#30340;&#22810;&#35270;&#35282;&#22270;&#21367;&#31215;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Multi-view Graph Convolutional Networks with Differentiable Node Selection. (arXiv:2212.05124v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.05124
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#21487;&#24494;&#21270;&#33410;&#28857;&#36873;&#25321;&#30340;&#22810;&#35270;&#35282;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#21487;&#26377;&#25928;&#22320;&#21033;&#29992;&#22810;&#35270;&#22270;&#25968;&#25454;&#30340;&#20114;&#34917;&#21644;&#19968;&#33268;&#20449;&#24687;&#65292;&#25552;&#21462;&#22810;&#20010;&#23545;&#35937;&#20043;&#38388;&#30340;&#28508;&#22312;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21253;&#21547;&#20114;&#34917;&#21644;&#19968;&#33268;&#20449;&#24687;&#30340;&#22810;&#35270;&#35282;&#25968;&#25454;&#36890;&#36807;&#21033;&#29992;&#22810;&#35270;&#22270;&#29305;&#24449;&#30340;&#23436;&#25972;&#25972;&#21512;&#26469;&#20419;&#36827;&#34920;&#31034;&#23398;&#20064;&#12290;&#30001;&#20110;&#29616;&#23454;&#19990;&#30028;&#20013;&#22823;&#22810;&#25968;&#23545;&#35937;&#36890;&#24120;&#20855;&#26377;&#28508;&#22312;&#30340;&#36830;&#25509;&#65292;&#23558;&#22810;&#35270;&#28857;&#25968;&#25454;&#32452;&#32455;&#20026;&#24322;&#26500;&#22270;&#23545;&#20110;&#25552;&#21462;&#19981;&#21516;&#23545;&#35937;&#20043;&#38388;&#30340;&#28508;&#22312;&#20449;&#24687;&#38750;&#24120;&#26377;&#30410;&#12290;&#30001;&#20110;&#20855;&#26377;&#25910;&#38598;&#38468;&#36817;&#33410;&#28857;&#20449;&#24687;&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#26412;&#25991;&#23558;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#24212;&#29992;&#20110;&#22788;&#29702;&#28304;&#33258;&#22810;&#35270;&#22270;&#25968;&#25454;&#30340;&#24322;&#26500;&#22270;&#25968;&#25454;&#65292;&#36825;&#22312;GCN&#39046;&#22495;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#20026;&#20102;&#25913;&#21892;&#32593;&#32476;&#25299;&#25169;&#30340;&#36136;&#37327;&#24182;&#20943;&#36731;&#22270;&#34701;&#21512;&#20135;&#29983;&#30340;&#22122;&#22768;&#24178;&#25200;&#65292;&#19968;&#20123;&#26041;&#27861;&#22312;&#22270;&#21367;&#31215;&#36807;&#31243;&#20043;&#21069;&#36827;&#34892;&#20102;&#25490;&#24207;&#25805;&#20316;&#12290;&#36825;&#20123;&#22522;&#20110;GCN&#30340;&#26041;&#27861;&#36890;&#24120;&#23545;&#27599;&#20010;&#39030;&#28857;&#25490;&#24207;&#21644;&#36873;&#25321;&#26368;&#21487;&#20449;&#20219;&#30340;&#37051;&#36817;&#33410;&#28857;&#65292;&#20363;&#22914;&#26681;&#25454;&#39044;&#23450;&#20041;&#30340;&#32622;&#20449;&#20540;&#36873;&#25321;&#21069;k&#20010;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-view data containing complementary and consensus information can facilitate representation learning by exploiting the intact integration of multi-view features. Because most objects in real world often have underlying connections, organizing multi-view data as heterogeneous graphs is beneficial to extracting latent information among different objects. Due to the powerful capability to gather information of neighborhood nodes, in this paper, we apply Graph Convolutional Network (GCN) to cope with heterogeneous-graph data originating from multi-view data, which is still under-explored in the field of GCN. In order to improve the quality of network topology and alleviate the interference of noises yielded by graph fusion, some methods undertake sorting operations before the graph convolution procedure. These GCN-based methods generally sort and select the most confident neighborhood nodes for each vertex, such as picking the top-k nodes according to pre-defined confidence values. No
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21435;&#20013;&#24515;&#21270;&#36882;&#24402;&#26799;&#24230;&#19978;&#21319;&#27861;&#65288;DREAM&#65289;&#30340;&#31616;&#21333;&#39640;&#25928;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#21435;&#20013;&#24515;&#21270;&#38750;&#20984;&#26497;&#23567;&#21270;&#38382;&#39064;&#65292;&#24182;&#23454;&#29616;&#20102;&#23547;&#25214;&#21407;&#20989;&#25968;&#30340; $\epsilon$-&#31283;&#23450;&#28857;&#30340;&#26368;&#20339;&#29702;&#35770;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2212.02387</link><description>&lt;p&gt;
&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#21435;&#20013;&#24515;&#21270;&#38750;&#20984;&#26497;&#23567;&#21270;&#38382;&#39064;&#38543;&#26426;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Simple and Efficient Stochastic Algorithm for Decentralized Nonconvex-Strongly-Concave Minimax Optimization. (arXiv:2212.02387v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02387
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21435;&#20013;&#24515;&#21270;&#36882;&#24402;&#26799;&#24230;&#19978;&#21319;&#27861;&#65288;DREAM&#65289;&#30340;&#31616;&#21333;&#39640;&#25928;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#21435;&#20013;&#24515;&#21270;&#38750;&#20984;&#26497;&#23567;&#21270;&#38382;&#39064;&#65292;&#24182;&#23454;&#29616;&#20102;&#23547;&#25214;&#21407;&#20989;&#25968;&#30340; $\epsilon$-&#31283;&#23450;&#28857;&#30340;&#26368;&#20339;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21435;&#20013;&#24515;&#21270;&#38750;&#20984;&#26497;&#23567;&#21270;&#38382;&#39064;&#30340;&#38543;&#26426;&#20248;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;&#21435;&#20013;&#24515;&#21270;&#36882;&#24402;&#26799;&#24230;&#19978;&#21319;&#27861;&#65288;\texttt{DREAM}&#65289;&#65292;&#23427;&#23454;&#29616;&#20102;&#23547;&#25214;&#21407;&#20989;&#25968;&#30340;$\epsilon$-&#31283;&#23450;&#28857;&#30340;&#26368;&#20339;&#24050;&#30693;&#29702;&#35770;&#20445;&#35777;&#12290;&#22312;&#22312;&#32447;&#35774;&#32622;&#19979;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#38656;&#35201;$\mathcal{O}(\kappa^3\epsilon^{-3})$&#38543;&#26426;&#19968;&#38454;&#39044;&#35328;&#26426;&#65288;SFO&#65289;&#35843;&#29992;&#20197;&#21450;$\mathcal{O}\big(\kappa^2\epsilon^{-2}/\sqrt{1-\lambda_2(W)}\,\big)$&#36890;&#20449;&#36718;&#27425;&#26469;&#25214;&#21040;$\epsilon$-&#31283;&#23450;&#28857;&#65292;&#20854;&#20013;$\kappa$&#26159;&#26465;&#20214;&#25968;&#65292;$\lambda_2(W)$&#26159;&#20843;&#21350;&#30697;&#38453;$W$&#30340;&#27425;&#22823;&#29305;&#24449;&#20540;&#12290;&#23545;&#20110;&#23436;&#20840;&#30001;$N$&#20010;&#20998;&#37327;&#20989;&#25968;&#32452;&#25104;&#30340;&#31163;&#32447;&#35774;&#32622;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#38656;&#35201;$\mathcal{O}\big(\kappa^2 \sqrt{N} \epsilon^{-2}\big)$ SFO &#35843;&#29992;&#21644;&#19982;&#22312;&#32447;&#35774;&#32622;&#30456;&#21516;&#30340;&#36890;&#20449;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the stochastic optimization for decentralized nonconvex-strongly-concave minimax problem. We propose a simple and efficient algorithm, called Decentralized Recursive-gradient descEnt Ascent Method (\texttt{DREAM}), which achieves the best-known theoretical guarantee for finding the $\epsilon$-stationary point of the primal function. For the online setting, the proposed method requires $\mathcal{O}(\kappa^3\epsilon^{-3})$ stochastic first-order oracle (SFO) calls and $\mathcal{O}\big(\kappa^2\epsilon^{-2}/\sqrt{1-\lambda_2(W)}\,\big)$ communication rounds to find an $\epsilon$-stationary point, where $\kappa$ is the condition number and $\lambda_2(W)$ is the second-largest eigenvalue of the gossip matrix~$W$. For the offline setting with totally $N$ component functions, the proposed method requires $\mathcal{O}\big(\kappa^2 \sqrt{N} \epsilon^{-2}\big)$ SFO calls and the same communication complexity as the online setting.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#21307;&#30103;&#25968;&#25454;&#20998;&#26512;&#65292;&#20351;&#29992;&#21307;&#30103;&#25968;&#25454;&#36827;&#34892;&#20449;&#21495;&#20998;&#31867;&#21644;&#30142;&#30149;&#35786;&#26029;&#12290;&#19982;&#20256;&#32479;&#32593;&#32476;&#30456;&#27604;&#65292;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#19968;&#20123;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;</title><link>http://arxiv.org/abs/2212.02234</link><description>&lt;p&gt;
&#22522;&#20110;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#21307;&#30103;&#25968;&#25454;&#20998;&#26512;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Review of medical data analysis based on spiking neural networks. (arXiv:2212.02234v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#21307;&#30103;&#25968;&#25454;&#20998;&#26512;&#65292;&#20351;&#29992;&#21307;&#30103;&#25968;&#25454;&#36827;&#34892;&#20449;&#21495;&#20998;&#31867;&#21644;&#30142;&#30149;&#35786;&#26029;&#12290;&#19982;&#20256;&#32479;&#32593;&#32476;&#30456;&#27604;&#65292;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#19968;&#20123;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#30103;&#25968;&#25454;&#20027;&#35201;&#21253;&#25324;&#21508;&#31181;&#31867;&#22411;&#30340;&#29983;&#29289;&#21307;&#23398;&#20449;&#21495;&#21644;&#21307;&#23398;&#24433;&#20687;&#65292;&#21487;&#20197;&#34987;&#19987;&#19994;&#21307;&#29983;&#29992;&#26469;&#21028;&#26029;&#24739;&#32773;&#30340;&#20581;&#24247;&#29366;&#20917;&#12290;&#28982;&#32780;&#65292;&#21307;&#30103;&#25968;&#25454;&#30340;&#35299;&#35835;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#21147;&#25104;&#26412;&#65292;&#21487;&#33021;&#23384;&#22312;&#35823;&#21028;&#65292;&#22240;&#27492;&#35768;&#22810;&#23398;&#32773;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#21644;&#28145;&#24230;&#23398;&#20064;&#26469;&#23545;&#21307;&#30103;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#21644;&#30740;&#31350;&#65292;&#21487;&#20197;&#25552;&#39640;&#21307;&#29983;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#65292;&#23454;&#29616;&#26089;&#26399;&#35786;&#26029;&#31561;&#12290;&#22240;&#27492;&#65292;&#23427;&#20855;&#26377;&#24191;&#38420;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#23384;&#22312;&#33021;&#32791;&#39640;&#21644;&#35745;&#31639;&#36895;&#24230;&#24930;&#31561;&#32570;&#28857;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#31532;&#19977;&#20195;&#31070;&#32463;&#32593;&#32476;&#33033;&#20914;&#31070;&#32463;&#20803;&#32593;&#32476;&#30340;&#20449;&#21495;&#20998;&#31867;&#21644;&#30142;&#30149;&#35786;&#26029;&#30340;&#26368;&#26032;&#30740;&#31350;&#65292;&#20351;&#29992;&#21307;&#30103;&#25968;&#25454;&#21253;&#25324;&#33041;&#30005;&#22270;&#20449;&#21495;&#12289;&#24515;&#30005;&#22270;&#20449;&#21495;&#12289;&#32908;&#30005;&#22270;&#20449;&#21495;&#21644;&#26680;&#30913;&#20849;&#25391;&#22270;&#20687;&#12290;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30456;&#23545;&#20110;&#20256;&#32479;&#32593;&#32476;&#30340;&#20248;&#32570;&#28857;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical data mainly includes various types of biomedical signals and medical images, which can be used by professional doctors to make judgments on patients' health conditions. However, the interpretation of medical data requires a lot of human cost and there may be misjudgments, so many scholars use neural networks and deep learning to classify and study medical data, which can improve the efficiency and accuracy of doctors and detect diseases early for early diagnosis, etc. Therefore, it has a wide range of application prospects. However, traditional neural networks have disadvantages such as high energy consumption and high latency (slow computation speed). This paper presents recent research on signal classification and disease diagnosis based on a third-generation neural network, the spiking neuron network, using medical data including EEG signals, ECG signals, EMG signals and MRI images. The advantages and disadvantages of pulsed neural networks compared with traditional networks
&lt;/p&gt;</description></item><item><title>Euler&#29305;&#24449;&#26354;&#32447;&#21644;&#36718;&#24275;&#26159;&#19968;&#31181;&#31283;&#23450;&#30340;&#24418;&#29366;&#19981;&#21464;&#37327;&#65292;&#29992;&#20110;&#22823;&#25968;&#25454;&#38382;&#39064;&#12290;&#19982;&#25345;&#20037;&#21516;&#35843;&#30456;&#27604;&#65292;&#23427;&#20204;&#20855;&#26377;&#20998;&#24067;&#24335;&#35745;&#31639;&#12289;&#22810;&#28388;&#27874;&#25512;&#24191;&#21644;&#36866;&#29992;&#20110;&#22823;&#25968;&#25454;&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#20855;&#26377;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.01666</link><description>&lt;p&gt;
Euler&#29305;&#24449;&#26354;&#32447;&#21644;&#36718;&#24275;&#65306;&#19968;&#31181;&#36866;&#29992;&#20110;&#22823;&#25968;&#25454;&#38382;&#39064;&#30340;&#31283;&#23450;&#24418;&#29366;&#19981;&#21464;&#37327;
&lt;/p&gt;
&lt;p&gt;
Euler Characteristic Curves and Profiles: a stable shape invariant for big data problems. (arXiv:2212.01666v2 [math.AT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01666
&lt;/p&gt;
&lt;p&gt;
Euler&#29305;&#24449;&#26354;&#32447;&#21644;&#36718;&#24275;&#26159;&#19968;&#31181;&#31283;&#23450;&#30340;&#24418;&#29366;&#19981;&#21464;&#37327;&#65292;&#29992;&#20110;&#22823;&#25968;&#25454;&#38382;&#39064;&#12290;&#19982;&#25345;&#20037;&#21516;&#35843;&#30456;&#27604;&#65292;&#23427;&#20204;&#20855;&#26377;&#20998;&#24067;&#24335;&#35745;&#31639;&#12289;&#22810;&#28388;&#27874;&#25512;&#24191;&#21644;&#36866;&#29992;&#20110;&#22823;&#25968;&#25454;&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#20855;&#26377;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#24037;&#20855;&#25552;&#20379;&#20102;&#31283;&#23450;&#30340;&#25688;&#35201;&#20449;&#24687;&#65292;&#27010;&#25324;&#20102;&#25152;&#32771;&#34385;&#25968;&#25454;&#30340;&#24418;&#29366;&#12290;&#25345;&#20037;&#21516;&#35843;&#65292;&#20316;&#20026;&#26368;&#24120;&#35265;&#21644;&#30740;&#31350;&#26368;&#36879;&#24443;&#30340;&#25968;&#25454;&#25688;&#35201;&#26041;&#27861;&#65292;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65306;&#35745;&#31639;&#38590;&#20197;&#20998;&#24067;&#12289;&#38590;&#20197;&#25512;&#24191;&#21040;&#22810;&#28388;&#27874;&#21644;&#23545;&#22823;&#25968;&#25454;&#38598;&#26469;&#35828;&#35745;&#31639;&#37327;&#36807;&#22823;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;Euler&#29305;&#24449;&#26354;&#32447;&#21644;&#22810;&#21442;&#25968;&#28388;&#27874;&#19979;&#30340;Euler&#29305;&#24449;&#36718;&#24275;&#30340;&#27010;&#24565;&#12290;&#23613;&#31649;&#22312;&#19968;&#32500;&#19978;&#26159;&#19968;&#20010;&#36739;&#24369;&#30340;&#19981;&#21464;&#37327;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Euler&#29305;&#24449;&#30340;&#26041;&#27861;&#19981;&#20855;&#22791;&#25345;&#20037;&#21516;&#35843;&#30340;&#19968;&#20123;&#21155;&#21183;&#65307;&#25105;&#20204;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#20998;&#24067;&#24335;&#35745;&#31639;&#31639;&#27861;&#65292;&#20197;&#21450;&#23427;&#20204;&#22312;&#22810;&#28388;&#27874;&#19979;&#30340;&#27867;&#21270;&#21644;&#22312;&#22823;&#25968;&#25454;&#38382;&#39064;&#20013;&#30340;&#23454;&#38469;&#36866;&#29992;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Euler&#26354;&#32447;&#21644;&#36718;&#24275;&#20855;&#26377;&#26576;&#31181;&#31283;&#23450;&#24615;&#65292;&#20351;&#23427;&#20204;&#25104;&#20026;&#25968;&#25454;&#20998;&#26512;&#20013;&#24378;&#22823;&#30340;&#24037;&#20855;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#23637;&#31034;&#23427;&#20204;&#30340;&#23454;&#38469;&#24212;&#29992;...
&lt;/p&gt;
&lt;p&gt;
Tools of Topological Data Analysis provide stable summaries encapsulating the shape of the considered data. Persistent homology, the most standard and well studied data summary, suffers a number of limitations; its computations are hard to distribute, it is hard to generalize to multifiltrations and is computationally prohibitive for big data-sets. In this paper we study the concept of Euler Characteristics Curves, for one parameter filtrations and Euler Characteristic Profiles, for multi-parameter filtrations. While being a weaker invariant in one dimension, we show that Euler Characteristic based approaches do not possess some handicaps of persistent homology; we show efficient algorithms to compute them in a distributed way, their generalization to multifiltrations and practical applicability for big data problems. In addition we show that the Euler Curves and Profiles enjoys certain type of stability which makes them robust tool in data analysis. Lastly, to show their practical app
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#25968;&#25454;&#20272;&#35745;Koopman&#31639;&#31526;&#30340;&#20248;&#21270;&#26041;&#26696;&#65292;&#36890;&#36807;&#32852;&#21512;&#23398;&#20064;&#21487;&#35266;&#27979;&#37327;&#21644;Koopman&#31639;&#31526;&#65292;&#33021;&#22815;&#37325;&#26500;&#28436;&#21464;&#24182;&#34920;&#31034;&#22797;&#26434;&#21160;&#21147;&#31995;&#32479;&#30340;&#20840;&#23616;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2212.01503</link><description>&lt;p&gt;
&#22312;&#32447;&#20272;&#35745;&#20351;&#29992;&#20613;&#37324;&#21494;&#29305;&#24449;&#30340;Koopman&#31639;&#31526;
&lt;/p&gt;
&lt;p&gt;
Online Estimation of the Koopman Operator Using Fourier Features. (arXiv:2212.01503v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01503
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#25968;&#25454;&#20272;&#35745;Koopman&#31639;&#31526;&#30340;&#20248;&#21270;&#26041;&#26696;&#65292;&#36890;&#36807;&#32852;&#21512;&#23398;&#20064;&#21487;&#35266;&#27979;&#37327;&#21644;Koopman&#31639;&#31526;&#65292;&#33021;&#22815;&#37325;&#26500;&#28436;&#21464;&#24182;&#34920;&#31034;&#22797;&#26434;&#21160;&#21147;&#31995;&#32479;&#30340;&#20840;&#23616;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36716;&#31227;&#31639;&#23376;&#25552;&#20379;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#30340;&#32447;&#24615;&#34920;&#31034;&#21644;&#20840;&#23616;&#19988;&#29289;&#29702;&#24847;&#20041;&#26126;&#30830;&#30340;&#29305;&#24449;&#12290;&#21457;&#29616;&#36716;&#31227;&#31639;&#23376;&#65292;&#20363;&#22914;Koopman&#31639;&#31526;&#65292;&#38656;&#35201;&#31934;&#24515;&#26500;&#24314;&#30340;&#21487;&#35266;&#27979;&#37327;&#35789;&#20856;&#65292;&#20316;&#29992;&#20110;&#21160;&#21147;&#31995;&#32479;&#30340;&#29366;&#24577;&#19978;&#12290;&#36825;&#31181;&#26041;&#27861;&#26159;&#20020;&#26102;&#24615;&#30340;&#65292;&#38656;&#35201;&#20351;&#29992;&#25972;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#20248;&#21270;&#26041;&#26696;&#65292;&#21487;&#20197;&#20351;&#29992;&#22312;&#32447;&#25968;&#25454;&#32852;&#21512;&#23398;&#20064;&#21487;&#35266;&#27979;&#37327;&#21644;Koopman&#31639;&#31526;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#33021;&#22815;&#37325;&#26500;&#28436;&#21464;&#24182;&#34920;&#31034;&#22797;&#26434;&#21160;&#21147;&#31995;&#32479;&#30340;&#20840;&#23616;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer operators offer linear representations and global, physically meaningful features of nonlinear dynamical systems. Discovering transfer operators, such as the Koopman operator, require careful crafted dictionaries of observables, acting on states of the dynamical system. This is ad hoc and requires the full dataset for evaluation. In this paper, we offer an optimization scheme to allow joint learning of the observables and Koopman operator with online data. Our results show we are able to reconstruct the evolution and represent the global features of complex dynamical systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20915;&#31574;&#20272;&#35745;&#31995;&#25968;&#21644;Estimation-to-Decisions&#20803;&#31639;&#27861;&#65292;&#22312;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#20013;&#32467;&#21512;&#20048;&#35266;&#20272;&#35745;&#65292;&#25552;&#20379;&#20102;&#26356;&#23485;&#26494;&#30340;&#20272;&#35745;&#35823;&#24046;&#27010;&#24565;&#30340;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2211.14250</link><description>&lt;p&gt;
&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#19982;&#20915;&#31574;&#20272;&#35745;&#31995;&#25968;
&lt;/p&gt;
&lt;p&gt;
Model-Free Reinforcement Learning with the Decision-Estimation Coefficient. (arXiv:2211.14250v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14250
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20915;&#31574;&#20272;&#35745;&#31995;&#25968;&#21644;Estimation-to-Decisions&#20803;&#31639;&#27861;&#65292;&#22312;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#20013;&#32467;&#21512;&#20048;&#35266;&#20272;&#35745;&#65292;&#25552;&#20379;&#20102;&#26356;&#23485;&#26494;&#30340;&#20272;&#35745;&#35823;&#24046;&#27010;&#24565;&#30340;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20132;&#20114;&#24335;&#20915;&#31574;&#38382;&#39064;&#65292;&#28085;&#30422;&#32467;&#26500;&#21270;&#36172;&#21338;&#26426;&#21644;&#20855;&#26377;&#36890;&#29992;&#20989;&#25968;&#36924;&#36817;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#26368;&#36817;&#65292;Foster&#31561;&#20154;&#65288;2021&#65289;&#24341;&#20837;&#20102;&#20915;&#31574;&#20272;&#35745;&#31995;&#25968;&#65292;&#36825;&#26159;&#19968;&#31181;&#32479;&#35745;&#22797;&#26434;&#24230;&#30340;&#24230;&#37327;&#65292;&#20854;&#20026;&#20132;&#20114;&#24335;&#20915;&#31574;&#21046;&#23450;&#20102;&#19968;&#20010;&#26368;&#20248;&#36951;&#25022;&#30340;&#19979;&#30028;&#65292;&#24182;&#24341;&#20837;&#20102;Estimation-to-Decisions&#20803;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20197;&#30456;&#21516;&#25968;&#37327;&#20026;&#19978;&#30028;&#12290;Estimation-to-Decisions&#26159;&#19968;&#31181;&#32422;&#31616;&#65292;&#23558;&#65288;&#30417;&#30563;&#65289;&#22312;&#32447;&#20272;&#35745;&#30340;&#31639;&#27861;&#25552;&#21319;&#20026;&#20915;&#31574;&#21046;&#23450;&#30340;&#31639;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#23558;Estimation-to-Decisions&#19982;Zhang&#65288;2022&#65289;&#24341;&#20837;&#30340;&#19968;&#31181;&#19987;&#38376;&#30340;&#20048;&#35266;&#20272;&#35745;&#24418;&#24335;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#24471;&#21040;&#27604;Foster&#31561;&#20154;&#65288;2021&#65289;&#26356;&#20026;&#23485;&#26494;&#30340;&#20272;&#35745;&#35823;&#24046;&#27010;&#24565;&#30340;&#20445;&#35777;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#23548;&#20986;&#20102;&#19982;&#20540;&#20989;&#25968;&#36924;&#36817;&#30456;&#32467;&#21512;&#30340;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#30340;&#36951;&#25022;&#30028;&#65292;&#24182;&#32473;&#20986;&#20102;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of interactive decision making, encompassing structured bandits and reinforcement learning with general function approximation. Recently, Foster et al. (2021) introduced the Decision-Estimation Coefficient, a measure of statistical complexity that lower bounds the optimal regret for interactive decision making, as well as a meta-algorithm, Estimation-to-Decisions, which achieves upper bounds in terms of the same quantity. Estimation-to-Decisions is a reduction, which lifts algorithms for (supervised) online estimation into algorithms for decision making. In this paper, we show that by combining Estimation-to-Decisions with a specialized form of optimistic estimation introduced by Zhang (2022), it is possible to obtain guarantees that improve upon those of Foster et al. (2021) by accommodating more lenient notions of estimation error. We use this approach to derive regret bounds for model-free reinforcement learning with value function approximation, and give str
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#38543;&#26426;&#29305;&#24449;&#22238;&#24402;(RFRR)&#22312;&#20960;&#20046;&#27491;&#20132;&#25968;&#25454;&#19978;&#30340;&#34892;&#20026;&#65292;&#35777;&#26126;&#20102;&#22312;&#31532;&#19968;&#23618;&#23485;&#24230;&#22823;&#20110;&#26679;&#26412;&#22823;&#23567;&#30340;&#24773;&#20917;&#19979;&#65292;RFRR&#30340;&#35757;&#32451;&#35823;&#24046;&#12289;&#20132;&#21449;&#39564;&#35777;&#21644;&#27867;&#21270;&#35823;&#24046;&#21487;&#20197;&#39640;&#27010;&#29575;&#38598;&#20013;&#22312;&#26680;&#23725;&#22238;&#24402;(KRR)&#30340;&#30456;&#24212;&#20540;&#21608;&#22260;&#65292;&#21516;&#26102;&#32473;&#20986;&#20102;&#29992;&#22810;&#39033;&#24335;&#26680;&#36817;&#20284;KRR&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.06077</link><description>&lt;p&gt;
&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#38543;&#26426;&#29305;&#24449;&#22238;&#24402;&#19982;&#20960;&#20046;&#27491;&#20132;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Overparameterized random feature regression with nearly orthogonal data. (arXiv:2211.06077v3 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.06077
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#38543;&#26426;&#29305;&#24449;&#22238;&#24402;(RFRR)&#22312;&#20960;&#20046;&#27491;&#20132;&#25968;&#25454;&#19978;&#30340;&#34892;&#20026;&#65292;&#35777;&#26126;&#20102;&#22312;&#31532;&#19968;&#23618;&#23485;&#24230;&#22823;&#20110;&#26679;&#26412;&#22823;&#23567;&#30340;&#24773;&#20917;&#19979;&#65292;RFRR&#30340;&#35757;&#32451;&#35823;&#24046;&#12289;&#20132;&#21449;&#39564;&#35777;&#21644;&#27867;&#21270;&#35823;&#24046;&#21487;&#20197;&#39640;&#27010;&#29575;&#38598;&#20013;&#22312;&#26680;&#23725;&#22238;&#24402;(KRR)&#30340;&#30456;&#24212;&#20540;&#21608;&#22260;&#65292;&#21516;&#26102;&#32473;&#20986;&#20102;&#29992;&#22810;&#39033;&#24335;&#26680;&#36817;&#20284;KRR&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#29305;&#24449;&#23725;&#22238;&#24402; (RFRR)&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#20855;&#26377;&#38543;&#26426;&#39640;&#26031;&#21021;&#22987;&#21270;&#30340;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#36807;&#24230;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#65292;&#31532;&#19968;&#23618;&#30340;&#23485;&#24230;&#36828;&#22823;&#20110;&#26679;&#26412;&#22823;&#23567;&#30340;&#26465;&#20214;&#19979;&#65292;&#37319;&#29992;&#20960;&#20046;&#27491;&#20132;&#30830;&#23450;&#24615;&#21333;&#20301;&#38271;&#24230;&#36755;&#20837;&#25968;&#25454;&#21521;&#37327;&#36827;&#34892; RFRR &#30340;&#38750;&#28176;&#36817;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#26174;&#31034;&#20102; RFRR &#35757;&#32451;&#35823;&#24046;&#12289;&#20132;&#21449;&#39564;&#35777;&#21644;&#27867;&#21270;&#35823;&#24046;&#30340;&#38750;&#28176;&#36817;&#38598;&#20013;&#32467;&#26524;&#65292;&#22312;&#26680;&#23725;&#22238;&#24402; (KRR) &#30340;&#30456;&#24212;&#20540;&#21608;&#22260;&#26377;&#39640;&#27010;&#29575;&#20986;&#29616;&#12290;&#35813; KRR &#26159;&#30001;&#38750;&#32447;&#24615;&#38543;&#26426;&#29305;&#24449;&#26144;&#23556;&#29983;&#25104;&#30340;&#26399;&#26395;&#26680;&#23548;&#20986;&#30340;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#28608;&#27963;&#20989;&#25968;&#30340; Hermite &#22810;&#39033;&#24335;&#23637;&#24320;&#33719;&#24471;&#30340;&#22810;&#39033;&#24335;&#26680;&#30697;&#38453;&#26469;&#36817;&#20284; KRR &#30340;&#24615;&#33021;&#65292;&#20854;&#27425;&#25968;&#20165;&#21462;&#20915;&#20110;&#19981;&#21516;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#27491;&#20132;&#24615;&#12290;&#36825;&#20010;&#22810;&#39033;&#24335;&#26680;&#30830;&#23450;&#20102; RFRR &#21644; KRR &#30340;&#28176;&#36817;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the properties of random feature ridge regression (RFRR) given by a two-layer neural network with random Gaussian initialization. We study the non-asymptotic behaviors of the RFRR with nearly orthogonal deterministic unit-length input data vectors in the overparameterized regime, where the width of the first layer is much larger than the sample size. Our analysis shows high-probability non-asymptotic concentration results for the training errors, cross-validations, and generalization errors of RFRR centered around their respective values for a kernel ridge regression (KRR). This KRR is derived from an expected kernel generated by a nonlinear random feature map. We then approximate the performance of the KRR by a polynomial kernel matrix obtained from the Hermite polynomial expansion of the activation function, whose degree only depends on the orthogonality among different data points. This polynomial kernel determines the asymptotic behavior of the RFRR and the KRR. Our 
&lt;/p&gt;</description></item><item><title>StyleNAT&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;transformer&#30340;&#22270;&#20687;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#37051;&#22495;&#27880;&#24847;&#21147;&#65288;NA&#65289;&#26469;&#25429;&#25417;&#23616;&#37096;&#21644;&#20840;&#23616;&#20449;&#24687;&#65292;&#33021;&#22815;&#39640;&#25928;&#28789;&#27963;&#22320;&#36866;&#24212;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;FFHQ-256&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20339;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2211.05770</link><description>&lt;p&gt;
StyleNAT&#65306;&#32473;&#27599;&#20010;&#22836;&#37096;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
StyleNAT: Giving Each Head a New Perspective. (arXiv:2211.05770v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05770
&lt;/p&gt;
&lt;p&gt;
StyleNAT&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;transformer&#30340;&#22270;&#20687;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#37051;&#22495;&#27880;&#24847;&#21147;&#65288;NA&#65289;&#26469;&#25429;&#25417;&#23616;&#37096;&#21644;&#20840;&#23616;&#20449;&#24687;&#65292;&#33021;&#22815;&#39640;&#25928;&#28789;&#27963;&#22320;&#36866;&#24212;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;FFHQ-256&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#29983;&#25104;&#19968;&#30452;&#26159;&#19968;&#20010;&#26082;&#26399;&#26395;&#21448;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#20197;&#39640;&#25928;&#30340;&#26041;&#24335;&#25191;&#34892;&#29983;&#25104;&#20219;&#21153;&#21516;&#26679;&#22256;&#38590;&#12290;&#36890;&#24120;&#65292;&#30740;&#31350;&#20154;&#21592;&#35797;&#22270;&#21019;&#24314;&#19968;&#20010;&#8220;&#19968;&#20992;&#20999;&#8221;&#30340;&#29983;&#25104;&#22120;&#65292;&#22312;&#21442;&#25968;&#31354;&#38388;&#20013;&#65292;&#21363;&#20351;&#26159;&#25130;&#28982;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#65292;&#20063;&#26377;&#24456;&#23569;&#30340;&#24046;&#24322;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;transformer&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;StyleNAT&#65292;&#26088;&#22312;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#29983;&#25104;&#65292;&#24182;&#20855;&#26377;&#21331;&#36234;&#30340;&#25928;&#29575;&#21644;&#28789;&#27963;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#27169;&#22411;&#26680;&#24515;&#26159;&#19968;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#26694;&#26550;&#65292;&#23427;&#23558;&#27880;&#24847;&#21147;&#22836;&#37096;&#21010;&#20998;&#20026;&#25429;&#25417;&#23616;&#37096;&#21644;&#20840;&#23616;&#20449;&#24687;&#30340;&#26041;&#24335;&#65292;&#36825;&#26159;&#36890;&#36807;&#20351;&#29992;&#37051;&#22495;&#27880;&#24847;&#21147;&#65288;NA&#65289;&#23454;&#29616;&#30340;&#12290;&#30001;&#20110;&#19981;&#21516;&#30340;&#22836;&#37096;&#33021;&#22815;&#20851;&#27880;&#19981;&#21516;&#30340;&#24863;&#21463;&#37326;&#65292;&#27169;&#22411;&#33021;&#22815;&#26356;&#22909;&#22320;&#32467;&#21512;&#36825;&#20123;&#20449;&#24687;&#65292;&#24182;&#20197;&#39640;&#24230;&#28789;&#27963;&#30340;&#26041;&#24335;&#36866;&#24212;&#25163;&#22836;&#30340;&#25968;&#25454;&#12290;StyleNAT&#22312;FFHQ-256&#19978;&#33719;&#24471;&#20102;&#26032;&#30340;SOTA FID&#24471;&#20998;2.046 &#65292;&#20987;&#36133;&#20102;&#20197;&#21367;&#31215;&#27169;&#22411;&#65288;&#22914;StyleGAN-XL&#65289;&#21644;transformer&#27169;&#22411;&#65288;&#22914;HIT&#65289;&#20026;&#22522;&#30784;&#30340;&#20808;&#21069;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image generation has been a long sought-after but challenging task, and performing the generation task in an efficient manner is similarly difficult. Often researchers attempt to create a "one size fits all" generator, where there are few differences in the parameter space for drastically different datasets. Herein, we present a new transformer-based framework, dubbed StyleNAT, targeting high-quality image generation with superior efficiency and flexibility. At the core of our model, is a carefully designed framework that partitions attention heads to capture local and global information, which is achieved through using Neighborhood Attention (NA). With different heads able to pay attention to varying receptive fields, the model is able to better combine this information, and adapt, in a highly flexible manner, to the data at hand. StyleNAT attains a new SOTA FID score on FFHQ-256 with 2.046, beating prior arts with convolutional models such as StyleGAN-XL and transformers such as HIT 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26059;&#36716;&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#65292;&#36890;&#36807;&#32422;&#26463;&#20445;&#25345;&#26059;&#36716;&#24179;&#31227;&#31561;&#21464;&#24615;&#30340;&#26041;&#24335;&#65292;&#23398;&#20064;&#29627;&#29827;&#28082;&#20307;&#38745;&#24577;&#32467;&#26500;&#30340;&#31283;&#20581;&#34920;&#31034;&#12290;&#36825;&#31181;&#32422;&#26463;&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#27979;&#33021;&#21147;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#21442;&#25968;&#25968;&#37327;&#65292;&#24182;&#19988;&#25552;&#39640;&#20102;&#35299;&#37322;&#24615;&#12290;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#23454;&#39564;&#35777;&#26126;&#20102;&#32593;&#32476;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.03226</link><description>&lt;p&gt;
&#23398;&#20064;&#29627;&#29827;&#28082;&#20307;&#34920;&#31034;&#30340;&#26059;&#36716;&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Rotation-equivariant Graph Neural Networks for Learning Glassy Liquids Representations. (arXiv:2211.03226v2 [cond-mat.soft] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.03226
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26059;&#36716;&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#65292;&#36890;&#36807;&#32422;&#26463;&#20445;&#25345;&#26059;&#36716;&#24179;&#31227;&#31561;&#21464;&#24615;&#30340;&#26041;&#24335;&#65292;&#23398;&#20064;&#29627;&#29827;&#28082;&#20307;&#38745;&#24577;&#32467;&#26500;&#30340;&#31283;&#20581;&#34920;&#31034;&#12290;&#36825;&#31181;&#32422;&#26463;&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#27979;&#33021;&#21147;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#21442;&#25968;&#25968;&#37327;&#65292;&#24182;&#19988;&#25552;&#39640;&#20102;&#35299;&#37322;&#24615;&#12290;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#23454;&#39564;&#35777;&#26126;&#20102;&#32593;&#32476;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29627;&#29827;&#28082;&#20307;&#30740;&#31350;&#39046;&#22495;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#23545;&#31890;&#23376;&#30340;&#38745;&#24577;&#32467;&#26500;&#36827;&#34892;&#24314;&#27169;&#26159;&#19968;&#20010;&#28909;&#38376;&#35805;&#39064;&#12290;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#65292;&#23427;&#20855;&#26377;&#24378;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#20294;&#26159;&#27169;&#22411;&#21442;&#25968;&#22810;&#19988;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#21463;&#26426;&#22120;&#23398;&#20064;&#32676;&#31561;&#21464;&#34920;&#31034;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;GNN&#65292;&#36890;&#36807;&#32422;&#26463;&#20854;&#20445;&#25345;&#26059;&#36716;&#24179;&#31227;&#65288;SE&#65288;3&#65289;&#65289;&#31561;&#21464;&#24615;&#65292;&#23398;&#20064;&#29627;&#29827;&#38745;&#24577;&#32467;&#26500;&#30340;&#31283;&#20581;&#34920;&#31034;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#31181;&#32422;&#26463;&#19981;&#20165;&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#27979;&#33021;&#21147;&#65292;&#36824;&#25552;&#39640;&#20102;&#23545;&#26410;&#35265;&#36807;&#28201;&#24230;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21516;&#26102;&#21487;&#20197;&#20943;&#23569;&#21442;&#25968;&#25968;&#37327;&#12290;&#27492;&#22806;&#65292;&#35299;&#37322;&#24615;&#20063;&#24471;&#21040;&#20102;&#25913;&#21892;&#65292;&#22240;&#20026;&#25105;&#20204;&#21487;&#20197;&#23558;&#22522;&#26412;&#21367;&#31215;&#23618;&#30340;&#20316;&#29992;&#19982;&#20247;&#25152;&#21608;&#30693;&#30340;&#26059;&#36716;&#19981;&#21464;&#19987;&#23478;&#29305;&#24449;&#32852;&#31995;&#36215;&#26469;&#12290;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#32593;&#32476;&#23398;&#20064;&#20102;&#19968;&#20010;&#31283;&#20581;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Within the glassy liquids community, the use of Machine Learning (ML) to model particles' static structure is currently a hot topic. The state of the art consists in Graph Neural Networks (GNNs), which have a great expressive power but are heavy models with numerous parameters and lack interpretability. Inspired by recent advances in the field of Machine Learning group-equivariant representations, we build a GNN that learns a robust representation of the glass' static structure by constraining it to preserve the roto-translation (SE(3)) equivariance. We show that this constraint not only significantly improves the predictive power but also improves the ability to generalize to unseen temperatures while allowing to reduce the number of parameters. Furthermore, interpretability is improved, as we can relate the action of our basic convolution layer to well-known rotation-invariant expert features. Through transfer-learning experiments we demonstrate that our network learns a robust repre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#20855;&#26377;&#20016;&#23500;&#22806;&#37096;&#20449;&#24687;&#30340;&#21407;&#21017;&#24615;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#22122;&#22768;&#26159;&#22797;&#26434;&#19988;&#19982;&#26102;&#38388;&#30456;&#20851;&#30340;&#36807;&#31243;&#26102;&#65292;&#29616;&#26377;&#30340;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#21487;&#33021;&#26080;&#27861;&#25104;&#21151;&#24212;&#29992;&#20110;&#36825;&#31867;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2211.00164</link><description>&lt;p&gt;
&#20195;&#29702;-&#25511;&#21046;&#22120;&#34920;&#31034;&#65306;&#20855;&#26377;&#20016;&#23500;&#22806;&#37096;&#20449;&#24687;&#30340;&#21407;&#21017;&#24615;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Agent-Controller Representations: Principled Offline RL with Rich Exogenous Information. (arXiv:2211.00164v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#20855;&#26377;&#20016;&#23500;&#22806;&#37096;&#20449;&#24687;&#30340;&#21407;&#21017;&#24615;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#22122;&#22768;&#26159;&#22797;&#26434;&#19988;&#19982;&#26102;&#38388;&#30456;&#20851;&#30340;&#36807;&#31243;&#26102;&#65292;&#29616;&#26377;&#30340;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#21487;&#33021;&#26080;&#27861;&#25104;&#21151;&#24212;&#29992;&#20110;&#36825;&#31867;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20016;&#23500;&#30340;&#20687;&#32032;&#35270;&#35273;&#35266;&#27979;&#31354;&#38388;&#20013;&#65292;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#23398;&#20064;&#25511;&#21046;&#20195;&#29702;&#23545;&#20110;&#24378;&#21270;&#23398;&#20064;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#31181;&#35774;&#32622;&#20013;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#36755;&#20837;&#20449;&#24687;&#20013;&#23384;&#22312;&#38590;&#20197;&#24314;&#27169;&#21644;&#25511;&#21046;&#20195;&#29702;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#29702;&#35770;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#24050;&#32463;&#36890;&#36807;&#22806;&#37096;&#20449;&#24687;&#30340;&#35266;&#28857;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21363;&#35266;&#27979;&#20013;&#21253;&#21547;&#30340;&#19982;&#25511;&#21046;&#26080;&#20851;&#30340;&#20449;&#24687;&#12290;&#20363;&#22914;&#65292;&#19968;&#20010;&#22312;&#32321;&#24537;&#34903;&#36947;&#19978;&#23548;&#33322;&#30340;&#26426;&#22120;&#20154;&#38656;&#35201;&#24573;&#30053;&#19982;&#25511;&#21046;&#26080;&#20851;&#30340;&#20449;&#24687;&#65292;&#22914;&#32972;&#26223;&#20013;&#30340;&#20854;&#20182;&#20154;&#34892;&#36208;&#12289;&#29289;&#20307;&#30340;&#32441;&#29702;&#25110;&#22825;&#31354;&#20013;&#30340;&#40479;&#31867;&#12290;&#26412;&#25991;&#38024;&#23545;&#20855;&#26377;&#35270;&#35273;&#32454;&#33410;&#30340;&#22806;&#37096;&#20449;&#24687;&#30340;&#35774;&#32622;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#65292;&#20197;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#24403;&#22122;&#22768;&#26159;&#22797;&#26434;&#19988;&#19982;&#26102;&#38388;&#30456;&#20851;&#30340;&#36807;&#31243;&#26102;&#65292;&#24403;&#20195;&#30340;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#21487;&#33021;&#22312;&#25968;&#25454;&#38598;&#19978;&#22833;&#36133;&#65292;&#32780;&#36825;&#31181;&#22122;&#22768;&#22312;&#23454;&#38469;&#20013;&#26222;&#36941;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning to control an agent from data collected offline in a rich pixel-based visual observation space is vital for real-world applications of reinforcement learning (RL). A major challenge in this setting is the presence of input information that is hard to model and irrelevant to controlling the agent. This problem has been approached by the theoretical RL community through the lens of exogenous information, i.e, any control-irrelevant information contained in observations. For example, a robot navigating in busy streets needs to ignore irrelevant information, such as other people walking in the background, textures of objects, or birds in the sky. In this paper, we focus on the setting with visually detailed exogenous information, and introduce new offline RL benchmarks offering the ability to study this problem. We find that contemporary representation learning techniques can fail on datasets where the noise is a complex and time dependent process, which is prevalent in practical 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#19982;HaarPooling&#25805;&#20316;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;HMPNet&#65292;&#29992;&#20110;&#39640;&#33021;&#29289;&#29702;&#20013;&#30340;&#22840;&#20811;&#33014;&#23376;&#26631;&#35760;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36866;&#24403;&#36873;&#25321;HaarPooling&#30340;&#20449;&#24687;&#21487;&#20197;&#25552;&#39640;&#22840;&#20811;&#33014;&#23376;&#26631;&#35760;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.13869</link><description>&lt;p&gt;
&#24102;&#26377;HaarPooling&#28040;&#24687;&#20256;&#36882;&#30340;&#22270;&#32593;&#32476;&#21943;&#27880;&#26631;&#35760;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A jet tagging algorithm of graph network with HaarPooling message passing. (arXiv:2210.13869v4 [hep-ex] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#19982;HaarPooling&#25805;&#20316;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;HMPNet&#65292;&#29992;&#20110;&#39640;&#33021;&#29289;&#29702;&#20013;&#30340;&#22840;&#20811;&#33014;&#23376;&#26631;&#35760;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36866;&#24403;&#36873;&#25321;HaarPooling&#30340;&#20449;&#24687;&#21487;&#20197;&#25552;&#39640;&#22840;&#20811;&#33014;&#23376;&#26631;&#35760;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#26041;&#27861;&#24050;&#32463;&#34987;&#24212;&#29992;&#20110;&#35299;&#20915;&#39640;&#33021;&#29289;&#29702;&#65288;HEP&#65289;&#20013;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#20351;&#29992;&#22270;&#34920;&#31034;&#30340;&#21943;&#27880;&#20107;&#20214;&#30340;&#22840;&#20811;&#33014;&#23376;&#26631;&#35760;&#20013;&#23637;&#29616;&#20986;&#20102;&#24040;&#22823;&#28508;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;GNNs&#19982;HaarPooling&#25805;&#20316;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#26469;&#20998;&#26512;&#20107;&#20214;&#65292;&#31216;&#20043;&#20026;HaarPooling&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;HMPNet&#65289;&#12290;&#22312;HMPNet&#20013;&#65292;HaarPooling&#19981;&#20165;&#25552;&#21462;&#20102;&#22270;&#30340;&#29305;&#24449;&#65292;&#36824;&#23884;&#20837;&#20102;&#36890;&#36807;k-means&#23545;&#19981;&#21516;&#31890;&#23376;&#29305;&#24449;&#36827;&#34892;&#32858;&#31867;&#24471;&#21040;&#30340;&#38468;&#21152;&#20449;&#24687;&#12290;&#25105;&#20204;&#20174;&#20116;&#20010;&#19981;&#21516;&#30340;&#29305;&#24449;&#26500;&#24314;&#20102;HaarPooling&#65306;&#32477;&#23545;&#33021;&#37327;$\log E$&#65292;&#27178;&#21521;&#21160;&#37327;$\log p_T$&#65292;&#30456;&#23545;&#22352;&#26631;$(\Delta\eta,\Delta\phi)$&#65292;&#28151;&#21512;&#29305;&#24449;$(\log E, \log p_T)$&#21644;$(\log E, \log p_T, \Delta\eta,\Delta\phi)$&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36866;&#24403;&#36873;&#25321;HaarPooling&#30340;&#20449;&#24687;&#21487;&#20197;&#25552;&#39640;&#22840;&#20811;&#33014;&#23376;&#26631;&#35760;&#30340;&#20934;&#30830;&#24615;&#65292;&#23558;$\log P_T$&#30340;&#39069;&#22806;&#20449;&#24687;&#28155;&#21152;&#21040;HMPNet&#20013;&#20248;&#20110;&#20854;&#20182;&#25152;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently methods of graph neural networks (GNNs) have been applied to solving the problems in high energy physics (HEP) and have shown its great potential for quark-gluon tagging with graph representation of jet events. In this paper, we introduce an approach of GNNs combined with a HaarPooling operation to analyze the events, called HaarPooling Message Passing neural network (HMPNet). In HMPNet, HaarPooling not only extracts the features of graph, but embeds additional information obtained by clustering of k-means of different particle features. We construct Haarpooling from five different features: absolute energy $\log E$, transverse momentum $\log p_T$, relative coordinates $(\Delta\eta,\Delta\phi)$, the mixed ones $(\log E, \log p_T)$ and $(\log E, \log p_T, \Delta\eta,\Delta\phi)$. The results show that an appropriate selection of information for HaarPooling enhances the accuracy of quark-gluon tagging, as adding extra information of $\log P_T$ to the HMPNet outperforms all the o
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Fenchel&#23545;&#20598;&#24615;&#30340;&#24555;&#36895;&#12289;&#31283;&#20581;&#30340;&#28151;&#21512;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;FedAvg&#26041;&#27861;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#24182;&#25552;&#20379;&#20102;&#38544;&#31169;&#20445;&#25252;&#25514;&#26045;&#12290;</title><link>http://arxiv.org/abs/2210.08106</link><description>&lt;p&gt;
&#19968;&#31181;&#28151;&#21512;&#32852;&#37030;&#23398;&#20064;&#30340;&#21407;&#22987;-&#23545;&#20598;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Primal-Dual Algorithm for Hybrid Federated Learning. (arXiv:2210.08106v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.08106
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Fenchel&#23545;&#20598;&#24615;&#30340;&#24555;&#36895;&#12289;&#31283;&#20581;&#30340;&#28151;&#21512;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;FedAvg&#26041;&#27861;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#24182;&#25552;&#20379;&#20102;&#38544;&#31169;&#20445;&#25252;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#24773;&#20917;&#20013;&#65292;&#28151;&#21512;&#32852;&#37030;&#23398;&#20064;&#24456;&#37325;&#35201;&#65292;&#20294;&#23545;&#20110;&#20165;&#25345;&#26377;&#37096;&#20998;&#29305;&#24449;&#21644;&#26679;&#26412;&#30340;&#23458;&#25143;&#31471;&#65292;&#24456;&#23569;&#26377;&#26041;&#27861;&#23384;&#22312;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;Fenchel&#23545;&#20598;&#24615;&#30340;&#24555;&#36895;&#12289;&#31283;&#20581;&#30340;&#28151;&#21512;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#31639;&#27861;&#22312;&#21508;&#31181;&#23454;&#38469;&#24773;&#20917;&#19979;&#25910;&#25947;&#20110;&#19982;&#22312;&#20013;&#24515;&#35757;&#32451;&#27169;&#22411;&#30456;&#21516;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#23454;&#39564;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30456;&#23545;&#20110;&#32852;&#37030;&#23398;&#20064;&#20013;&#24120;&#29992;&#30340;&#26041;&#27861;FedAvg&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#38544;&#31169;&#32771;&#34385;&#21644;&#20445;&#25252;&#23458;&#25143;&#25968;&#25454;&#30340;&#24517;&#35201;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
Very few methods for hybrid federated learning, where clients only hold subsets of both features and samples, exist. Yet, this scenario is very important in practical settings. We provide a fast, robust algorithm for hybrid federated learning that hinges on Fenchel Duality. We prove the convergence of the algorithm to the same solution as if the model was trained centrally in a variety of practical regimes. Furthermore, we provide experimental results that demonstrate the performance improvements of the algorithm over a commonly used method in federated learning, FedAvg. We also provide privacy considerations and necessary steps to protect client data.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#21518;&#38376;&#25915;&#20987;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23604;&#23596;&#31616;&#21333;&#20294;&#39640;&#25928;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#27745;&#26579;&#35757;&#32451;&#25968;&#25454;&#24182;&#25554;&#20837;&#35302;&#21457;&#22120;&#65292;&#20351;&#24471;&#22312;&#25512;&#29702;&#26102;&#20219;&#20309;&#24102;&#26377;&#35302;&#21457;&#22120;&#30340;&#36755;&#20837;&#37117;&#33021;&#20197;&#39640;&#27010;&#29575;&#34987;&#38169;&#35823;&#20998;&#31867;&#21040;&#23545;&#25163;&#25351;&#23450;&#30340;&#31867;&#21035;&#12290;&#36825;&#19968;&#21457;&#29616;&#34920;&#26126;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#30417;&#30563;&#23398;&#20064;&#22312;&#23545;&#25239;&#24615;&#25915;&#20987;&#26041;&#38754;&#23384;&#22312;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2210.07346</link><description>&lt;p&gt;
&#19968;&#31181;&#23604;&#23596;&#31616;&#21333;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
An Embarrassingly Simple Backdoor Attack on Self-supervised Learning. (arXiv:2210.07346v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07346
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#21518;&#38376;&#25915;&#20987;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23604;&#23596;&#31616;&#21333;&#20294;&#39640;&#25928;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#27745;&#26579;&#35757;&#32451;&#25968;&#25454;&#24182;&#25554;&#20837;&#35302;&#21457;&#22120;&#65292;&#20351;&#24471;&#22312;&#25512;&#29702;&#26102;&#20219;&#20309;&#24102;&#26377;&#35302;&#21457;&#22120;&#30340;&#36755;&#20837;&#37117;&#33021;&#20197;&#39640;&#27010;&#29575;&#34987;&#38169;&#35823;&#20998;&#31867;&#21040;&#23545;&#25163;&#25351;&#23450;&#30340;&#31867;&#21035;&#12290;&#36825;&#19968;&#21457;&#29616;&#34920;&#26126;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#30417;&#30563;&#23398;&#20064;&#22312;&#23545;&#25239;&#24615;&#25915;&#20987;&#26041;&#38754;&#23384;&#22312;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#20316;&#20026;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#26032;&#33539;&#24335;&#65292;&#33021;&#22815;&#22312;&#19981;&#20381;&#36182;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#22797;&#26434;&#25968;&#25454;&#30340;&#39640;&#36136;&#37327;&#34920;&#31034;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#38500;&#20102;&#28040;&#38500;&#23545;&#26377;&#26631;&#31614;&#25968;&#25454;&#30340;&#20381;&#36182;&#22806;&#65292;SSL&#36824;&#25552;&#39640;&#20102;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#65292;&#22240;&#20026;&#32570;&#20047;&#26631;&#31614;&#20351;&#24471;&#23545;&#25163;&#26356;&#38590;&#20197;&#25805;&#32437;&#27169;&#22411;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#40065;&#26834;&#24615;&#20248;&#21183;&#22312;&#20854;&#20182;&#31867;&#22411;&#30340;&#25915;&#20987;&#20013;&#26159;&#21542;&#20855;&#26377;&#26222;&#36866;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20043;&#35868;&#12290;&#25105;&#20204;&#22312;&#21518;&#38376;&#25915;&#20987;&#30340;&#32972;&#26223;&#19979;&#25506;&#32034;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#35774;&#35745;&#24182;&#35780;&#20272;&#20102;CTRL&#65292;&#19968;&#31181;&#23604;&#23596;&#31616;&#21333;&#20294;&#38750;&#24120;&#26377;&#25928;&#30340;&#33258;&#30417;&#30563;&#21518;&#38376;&#25915;&#20987;&#12290;&#36890;&#36807;&#20165;&#27745;&#26579;&#24494;&#23567;&#27604;&#20363;&#65288;&lt;= 1%&#65289;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;CTRL&#22312;&#25512;&#29702;&#26102;&#20197;&#39640;&#27010;&#29575;&#65288;&gt;= 99%&#65289;&#23558;&#20219;&#20309;&#25658;&#24102;&#35302;&#21457;&#22120;&#30340;&#36755;&#20837;&#38169;&#35823;&#20998;&#31867;&#20026;&#23545;&#25163;&#25351;&#23450;&#30340;&#31867;&#21035;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;SSL&#21644;&#30417;&#30563;&#23398;&#20064;&#22312;&#23545;&#25239;&#24615;&#25915;&#20987;&#20013;&#23384;&#22312;&#19968;&#23450;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a new paradigm in machine learning, self-supervised learning (SSL) is capable of learning high-quality representations of complex data without relying on labels. In addition to eliminating the need for labeled data, research has found that SSL improves the adversarial robustness over supervised learning since lacking labels makes it more challenging for adversaries to manipulate model predictions. However, the extent to which this robustness superiority generalizes to other types of attacks remains an open question.  We explore this question in the context of backdoor attacks. Specifically, we design and evaluate CTRL, an embarrassingly simple yet highly effective self-supervised backdoor attack. By only polluting a tiny fraction of training data (&lt;= 1%) with indistinguishable poisoning samples, CTRL causes any trigger-embedded input to be misclassified to the adversary's designated class with a high probability (&gt;= 99%) at inference time. Our findings suggest that SSL and supervise
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;ICPI&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#20013;&#25191;&#34892;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#65292;&#26080;&#38656;&#19987;&#23478;&#31034;&#33539;&#25110;&#26799;&#24230;&#12290;&#31639;&#27861;&#37319;&#29992;&#31574;&#30053;&#36845;&#20195;&#26041;&#27861;&#65292;&#19981;&#20165;&#36991;&#20813;&#20102;&#31034;&#33539;&#25910;&#38598;&#30340;&#32321;&#29712;&#24037;&#20316;&#65292;&#36824;&#35299;&#20915;&#20102;&#26799;&#24230;&#26041;&#27861;&#30340;&#36816;&#34892;&#36895;&#24230;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2210.03821</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23454;&#29616;&#31574;&#30053;&#36845;&#20195;
&lt;/p&gt;
&lt;p&gt;
Large Language Models can Implement Policy Iteration. (arXiv:2210.03821v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03821
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;ICPI&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#20013;&#25191;&#34892;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#65292;&#26080;&#38656;&#19987;&#23478;&#31034;&#33539;&#25110;&#26799;&#24230;&#12290;&#31639;&#27861;&#37319;&#29992;&#31574;&#30053;&#36845;&#20195;&#26041;&#27861;&#65292;&#19981;&#20165;&#36991;&#20813;&#20102;&#31034;&#33539;&#25910;&#38598;&#30340;&#32321;&#29712;&#24037;&#20316;&#65292;&#36824;&#35299;&#20915;&#20102;&#26799;&#24230;&#26041;&#27861;&#30340;&#36816;&#34892;&#36895;&#24230;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#19978;&#19979;&#25991;&#31574;&#30053;&#36845;&#20195;&#8221;&#30340;&#31639;&#27861;&#65292;&#23427;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#20013;&#25191;&#34892;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#12290;&#34429;&#28982;&#23558;&#22522;&#30784;&#27169;&#22411;&#24212;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#24050;&#32463;&#21463;&#21040;&#20102;&#37325;&#35270;&#65292;&#20294;&#22823;&#22810;&#25968;&#26041;&#27861;&#35201;&#20040;&#20381;&#36182;&#20110;&#19987;&#23478;&#31034;&#33539;&#30340;&#31574;&#21010;&#65288;&#36890;&#36807;&#25163;&#21160;&#35774;&#35745;&#25110;&#20219;&#21153;&#29305;&#23450;&#30340;&#39044;&#35757;&#32451;&#65289;&#65292;&#35201;&#20040;&#36890;&#36807;&#26799;&#24230;&#26041;&#27861;&#65288;&#24494;&#35843;&#25110;&#36866;&#37197;&#23618;&#35757;&#32451;&#65289;&#26469;&#36866;&#24212;&#24863;&#20852;&#36259;&#30340;&#20219;&#21153;&#12290;&#36825;&#20123;&#25216;&#26415;&#37117;&#26377;&#19968;&#20123;&#32570;&#28857;&#12290;&#25910;&#38598;&#31034;&#33539;&#26159;&#36153;&#26102;&#36153;&#21147;&#30340;&#65292;&#20381;&#36182;&#31034;&#33539;&#30340;&#31639;&#27861;&#24182;&#19981;&#33021;&#36229;&#36234;&#31034;&#33539;&#20013;&#30340;&#19987;&#23478;&#12290;&#32780;&#25152;&#26377;&#30340;&#26799;&#24230;&#25216;&#26415;&#22825;&#29983;&#37117;&#24456;&#24930;&#65292;&#20007;&#22833;&#20102;&#19968;&#24320;&#22987;&#20351;&#19978;&#19979;&#25991;&#23398;&#20064;&#20855;&#26377;&#21560;&#24341;&#21147;&#30340;&#8220;&#23569;&#26679;&#26412;&#8221;&#36136;&#37327;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ICPI&#30340;&#31639;&#27861;&#65292;&#23427;&#33021;&#22815;&#22312;&#27809;&#26377;&#19987;&#23478;&#31034;&#33539;&#25110;&#26799;&#24230;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#25191;&#34892;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#36845;&#20195;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#25552;&#31034;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work presents In-Context Policy Iteration, an algorithm for performing Reinforcement Learning (RL), in-context, using foundation models. While the application of foundation models to RL has received considerable attention, most approaches rely on either (1) the curation of expert demonstrations (either through manual design or task-specific pretraining) or (2) adaptation to the task of interest using gradient methods (either fine-tuning or training of adapter layers). Both of these techniques have drawbacks. Collecting demonstrations is labor-intensive, and algorithms that rely on them do not outperform the experts from which the demonstrations were derived. All gradient techniques are inherently slow, sacrificing the "few-shot" quality that made in-context learning attractive to begin with. In this work, we present an algorithm, ICPI, that learns to perform RL tasks without expert demonstrations or gradients. Instead we present a policy-iteration method in which the prompt conten
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#33258;&#24102;&#38271;&#23614;&#25968;&#25454;&#30340;&#26631;&#31614;&#22122;&#22768;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#26694;&#26550;&#24182;&#35774;&#35745;&#20102;&#21452;&#32500;&#26679;&#26412;&#36873;&#25321;&#31639;&#27861; TABASCO&#65292;&#26377;&#25928;&#22320;&#23558;&#24178;&#20928;&#26679;&#26412;&#19982;&#22122;&#22768;&#26679;&#26412;&#20998;&#24320;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#38271;&#23614;&#31867;&#21035;&#12290;</title><link>http://arxiv.org/abs/2208.09833</link><description>&lt;p&gt;
&#33258;&#24102;&#38271;&#23614;&#25968;&#25454;&#30340;&#26631;&#31614;&#22122;&#22768;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Label-Noise Learning with Intrinsically Long-Tailed Data. (arXiv:2208.09833v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.09833
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#33258;&#24102;&#38271;&#23614;&#25968;&#25454;&#30340;&#26631;&#31614;&#22122;&#22768;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#26694;&#26550;&#24182;&#35774;&#35745;&#20102;&#21452;&#32500;&#26679;&#26412;&#36873;&#25321;&#31639;&#27861; TABASCO&#65292;&#26377;&#25928;&#22320;&#23558;&#24178;&#20928;&#26679;&#26412;&#19982;&#22122;&#22768;&#26679;&#26412;&#20998;&#24320;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#38271;&#23614;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#31614;&#22122;&#22768;&#26159;&#23548;&#33268;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#24046;&#30340;&#20851;&#38190;&#22240;&#32032;&#20043;&#19968;&#12290;&#29616;&#26377;&#30340;&#26631;&#31614;&#22122;&#22768;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#20551;&#35774;&#35757;&#32451;&#25968;&#25454;&#30340;&#30495;&#23454;&#31867;&#21035;&#26159;&#24179;&#34913;&#30340;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#24448;&#24448;&#26159;&#19981;&#24179;&#34913;&#30340;&#65292;&#23548;&#33268;&#35266;&#23519;&#21040;&#30340;&#21644;&#20869;&#22312;&#31867;&#21035;&#20998;&#24067;&#20043;&#38388;&#23384;&#22312;&#19981;&#19968;&#33268;&#30340;&#26631;&#31614;&#22122;&#22768;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#24456;&#38590;&#22312;&#20855;&#26377;&#26410;&#30693;&#20869;&#22312;&#31867;&#21035;&#20998;&#24067;&#30340;&#38271;&#23614;&#31867;&#19978;&#21306;&#20998;&#24178;&#20928;&#26679;&#26412;&#21644;&#22122;&#22768;&#26679;&#26412;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#33258;&#24102;&#38271;&#23614;&#25968;&#25454;&#30340;&#26631;&#31614;&#22122;&#22768;&#23398;&#20064;&#30340;&#23398;&#20064;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#38454;&#27573;&#30340;&#21452;&#32500;&#26679;&#26412;&#36873;&#25321;&#65288;TABASCO&#65289;&#26469;&#26356;&#22909;&#22320;&#23558;&#24178;&#20928;&#26679;&#26412;&#19982;&#22122;&#22768;&#26679;&#26412;&#20998;&#24320;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#23614;&#37096;&#31867;&#21035;&#12290;TABASCO&#21253;&#25324;&#20004;&#20010;&#26032;&#30340;&#20998;&#31163;&#24230;&#37327;&#65292;&#30456;&#20114;&#34917;&#20805;&#65292;&#24357;&#34917;&#20102;&#22312;&#26679;&#26412;&#20998;&#31163;&#20013;&#20351;&#29992;&#21333;&#20010;&#24230;&#37327;&#30340;&#23616;&#38480;&#24615;&#12290;&#23545;&#25105;&#20204;&#25552;&#20986;&#30340;&#20855;&#26377;&#30495;&#23454;&#25968;&#25454;&#30340;&#22522;&#20934;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Label noise is one of the key factors that lead to the poor generalization of deep learning models. Existing label-noise learning methods usually assume that the ground-truth classes of the training data are balanced. However, the real-world data is often imbalanced, leading to the inconsistency between observed and intrinsic class distribution with label noises. In this case, it is hard to distinguish clean samples from noisy samples on the intrinsic tail classes with the unknown intrinsic class distribution. In this paper, we propose a learning framework for label-noise learning with intrinsically long-tailed data. Specifically, we propose two-stage bi-dimensional sample selection (TABASCO) to better separate clean samples from noisy samples, especially for the tail classes. TABASCO consists of two new separation metrics that complement each other to compensate for the limitation of using a single metric in sample separation. Extensive experiments on benchmarks we proposed with real-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#32422;&#26463;&#28145;&#24230;&#23398;&#20064;&#38477;&#23610;&#24230;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20197;&#20445;&#35777;&#27169;&#22411;&#22312;&#39044;&#27979;&#29289;&#29702;&#21464;&#37327;&#26102;&#28385;&#36275;&#23432;&#24658;&#23450;&#24459;&#65292;&#24182;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2208.05424</link><description>&lt;p&gt;
&#29289;&#29702;&#32422;&#26463;&#28145;&#24230;&#23398;&#20064;&#29992;&#20110;&#27668;&#20505;&#38477;&#23610;&#24230;
&lt;/p&gt;
&lt;p&gt;
Physics-Constrained Deep Learning for Climate Downscaling. (arXiv:2208.05424v6 [physics.ao-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.05424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#32422;&#26463;&#28145;&#24230;&#23398;&#20064;&#38477;&#23610;&#24230;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20197;&#20445;&#35777;&#27169;&#22411;&#22312;&#39044;&#27979;&#29289;&#29702;&#21464;&#37327;&#26102;&#28385;&#36275;&#23432;&#24658;&#23450;&#24459;&#65292;&#24182;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a method for physics-constrained deep learning downscaling models to ensure that the models satisfy conservation laws when predicting physical variables, while improving their performance according to traditional metrics.
&lt;/p&gt;
&lt;p&gt;
&#21487;&#38752;&#30340;&#39640;&#20998;&#36776;&#29575;&#27668;&#20505;&#21644;&#22825;&#27668;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#23545;&#20110;&#25351;&#23548;&#27668;&#20505;&#36866;&#24212;&#21644;&#20943;&#32531;&#30340;&#38271;&#26399;&#20915;&#31574;&#20197;&#21450;&#25351;&#23548;&#23545;&#26497;&#31471;&#20107;&#20214;&#30340;&#24555;&#36895;&#21709;&#24212;&#33267;&#20851;&#37325;&#35201;&#12290;&#39044;&#27979;&#27169;&#22411;&#21463;&#35745;&#31639;&#25104;&#26412;&#38480;&#21046;&#65292;&#22240;&#27492;&#36890;&#24120;&#29983;&#25104;&#31895;&#20998;&#36776;&#29575;&#39044;&#27979;&#12290;&#32479;&#35745;&#38477;&#23610;&#24230;&#65292;&#21253;&#25324;&#28145;&#24230;&#23398;&#20064;&#30340;&#36229;&#20998;&#36776;&#29575;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#20379;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#19978;&#37319;&#26679;&#20302;&#20998;&#36776;&#29575;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#35270;&#35273;&#19978;&#20196;&#20154;&#20449;&#26381;&#30340;&#32467;&#26524;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#22312;&#39044;&#27979;&#29289;&#29702;&#21464;&#37327;&#26102;&#32463;&#24120;&#36829;&#21453;&#23432;&#24658;&#23450;&#24459;&#12290;&#20026;&#20102;&#20445;&#25345;&#29289;&#29702;&#37327;&#30340;&#23432;&#24658;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#20445;&#35777;&#28145;&#24230;&#23398;&#20064;&#38477;&#23610;&#24230;&#27169;&#22411;&#28385;&#36275;&#29289;&#29702;&#32422;&#26463;&#26465;&#20214;&#65292;&#21516;&#26102;&#26681;&#25454;&#20256;&#32479;&#25351;&#26631;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#32422;&#26463;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#19981;&#21516;&#30340;&#31070;&#32463;&#26550;&#26500;&#20197;&#21450;&#21508;&#31181;&#27668;&#20505;&#21644;&#22825;&#27668;&#25968;&#25454;&#19978;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The availability of reliable, high-resolution climate and weather data is important to inform long-term decisions on climate adaptation and mitigation and to guide rapid responses to extreme events. Forecasting models are limited by computational costs and, therefore, often generate coarse-resolution predictions. Statistical downscaling, including super-resolution methods from deep learning, can provide an efficient method of upsampling low-resolution data. However, despite achieving visually compelling results in some cases, such models frequently violate conservation laws when predicting physical variables. In order to conserve physical quantities, we develop methods that guarantee physical constraints are satisfied by a deep learning downscaling model while also improving their performance according to traditional metrics. We compare different constraining approaches and demonstrate their applicability across different neural architectures as well as a variety of climate and weather
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#32771;&#34385;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#32447;&#24615;&#20989;&#25968;&#30340;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#26631;&#20934;&#30340;Transformers&#27169;&#22411;&#21487;&#20197;&#20174;&#22836;&#35757;&#32451;&#65292;&#22312;&#25512;&#26029;&#26102;&#23454;&#29616;&#32447;&#24615;&#20989;&#25968;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2208.01066</link><description>&lt;p&gt;
Transformers&#21487;&#20197;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#20160;&#20040;&#65311;&#19968;&#20010;&#31616;&#21333;&#20989;&#25968;&#31867;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
What Can Transformers Learn In-Context? A Case Study of Simple Function Classes. (arXiv:2208.01066v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.01066
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#32771;&#34385;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#32447;&#24615;&#20989;&#25968;&#30340;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#26631;&#20934;&#30340;Transformers&#27169;&#22411;&#21487;&#20197;&#20174;&#22836;&#35757;&#32451;&#65292;&#22312;&#25512;&#26029;&#26102;&#23454;&#29616;&#32447;&#24615;&#20989;&#25968;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#23398;&#20064;&#26159;&#25351;&#27169;&#22411;&#33021;&#22815;&#20381;&#36182;&#20110;&#21253;&#21547;&#19978;&#19979;&#25991;&#31034;&#20363;&#65288;&#19982;&#26576;&#20010;&#20219;&#21153;&#23545;&#24212;&#30340;&#36755;&#20837;-&#36755;&#20986;&#23545;&#65289;&#21644;&#26032;&#30340;&#26597;&#35810;&#36755;&#20837;&#30340;&#25552;&#31034;&#24207;&#21015;&#65292;&#24182;&#29983;&#25104;&#30456;&#24212;&#30340;&#36755;&#20986;&#12290;&#20851;&#38190;&#26159;&#65292;&#22312;&#25512;&#26029;&#26102;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#20165;&#21457;&#29983;&#22312;&#27169;&#22411;&#21442;&#25968;&#26410;&#26356;&#26032;&#30340;&#24773;&#20917;&#19979;&#12290;&#34429;&#28982;&#20687;GPT-3&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#20102;&#19968;&#23450;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#20294;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#25104;&#21151;&#30340;&#20219;&#21153;&#19982;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#20160;&#20040;&#20869;&#23481;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#29702;&#35299;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#26126;&#30830;&#23450;&#20041;&#30340;&#38382;&#39064;&#65292;&#21363;&#35757;&#32451;&#19968;&#20010;&#27169;&#22411;&#20197;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#20989;&#25968;&#31867;&#65288;&#20363;&#22914;&#32447;&#24615;&#20989;&#25968;&#65289;&#65306;&#20063;&#23601;&#26159;&#35828;&#65292;&#32473;&#23450;&#20174;&#35813;&#31867;&#20013;&#23548;&#20986;&#30340;&#25968;&#25454;&#65292;&#25105;&#20204;&#33021;&#21542;&#35757;&#32451;&#19968;&#20010;&#27169;&#22411;&#26469;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#8220;&#22823;&#22810;&#25968;&#8221;&#20989;&#25968;&#65311;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#26631;&#20934;&#30340;Transformers&#21487;&#20197;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#65292;&#20197;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#32447;&#24615;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning refers to the ability of a model to condition on a prompt sequence consisting of in-context examples (input-output pairs corresponding to some task) along with a new query input, and generate the corresponding output. Crucially, in-context learning happens only at inference time without any parameter updates to the model. While large language models such as GPT-3 exhibit some ability to perform in-context learning, it is unclear what the relationship is between tasks on which this succeeds and what is present in the training data. To make progress towards understanding in-context learning, we consider the well-defined problem of training a model to in-context learn a function class (e.g., linear functions): that is, given data derived from some functions in the class, can we train a model to in-context learn "most" functions from this class? We show empirically that standard Transformers can be trained from scratch to perform in-context learning of linear functions 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#22312;&#34588;&#34562;&#30417;&#27979;&#20013;&#30340;&#26368;&#26032;&#24212;&#29992;&#65292;&#23637;&#31034;&#20102;&#33258;&#21160;&#21270;&#34588;&#34562;&#35745;&#25968;&#31639;&#27861;&#30340;&#28508;&#21147;&#65292;&#24182;&#24076;&#26395;&#33021;&#22815;&#28608;&#21457;&#20854;&#20182;&#31185;&#23398;&#23478;&#30340;&#28789;&#24863;&#21644;&#20852;&#36259;&#12290;</title><link>http://arxiv.org/abs/2208.00085</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#22312;&#34588;&#34562;&#30417;&#27979;&#24212;&#29992;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Machine Learning and Computer Vision Techniques in Bee Monitoring Applications. (arXiv:2208.00085v1 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.00085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#22312;&#34588;&#34562;&#30417;&#27979;&#20013;&#30340;&#26368;&#26032;&#24212;&#29992;&#65292;&#23637;&#31034;&#20102;&#33258;&#21160;&#21270;&#34588;&#34562;&#35745;&#25968;&#31639;&#27861;&#30340;&#28508;&#21147;&#65292;&#24182;&#24076;&#26395;&#33021;&#22815;&#28608;&#21457;&#20854;&#20182;&#31185;&#23398;&#23478;&#30340;&#28789;&#24863;&#21644;&#20852;&#36259;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#26159;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#65292;&#24050;&#32463;&#35777;&#26126;&#33021;&#22815;&#35299;&#20915;&#38750;&#24120;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#23427;&#20204;&#21487;&#20197;&#29992;&#20110;&#30417;&#27979;&#34588;&#34562;&#32676;&#20307;&#24182;&#26816;&#26597;&#20854;&#20581;&#24247;&#29366;&#20917;&#65292;&#20174;&#32780;&#22312;&#24773;&#20917;&#21464;&#24471;&#20005;&#37325;&#20043;&#21069;&#65292;&#35782;&#21035;&#20986;&#28508;&#22312;&#21361;&#38505;&#29366;&#24577;&#65292;&#25110;&#32773;&#26356;&#22909;&#22320;&#35745;&#21010;&#23450;&#26399;&#34588;&#34562;&#32676;&#20307;&#26816;&#26597;&#65292;&#20174;&#32780;&#33410;&#30465;&#37325;&#35201;&#30340;&#25104;&#26412;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#29992;&#20110;&#34588;&#34562;&#30417;&#27979;&#30340;&#26368;&#20808;&#36827;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#65292;&#24182;&#20197;&#33258;&#21160;&#21270;&#34588;&#34562;&#35745;&#25968;&#31639;&#27861;&#20026;&#20363;&#23637;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#38754;&#21521;&#20861;&#21307;&#23398;&#21644;&#34588;&#34562;&#23398;&#19987;&#19994;&#20154;&#21592;&#21644;&#19987;&#23478;&#65292;&#26088;&#22312;&#21521;&#20182;&#20204;&#20171;&#32461;&#26426;&#22120;&#23398;&#20064;&#30340;&#21487;&#33021;&#24615;&#65292;&#22240;&#27492;&#27599;&#20010;&#24212;&#29992;&#31867;&#21035;&#37117;&#20197;&#31616;&#35201;&#30340;&#29702;&#35770;&#20171;&#32461;&#21644;&#19982;&#20854;&#22522;&#26412;&#26041;&#27861;&#30456;&#20851;&#30340;&#21160;&#26426;&#24320;&#31687;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#31687;&#35770;&#25991;&#33021;&#28608;&#21457;&#20854;&#20182;&#31185;&#23398;&#23478;&#30340;&#28789;&#24863;...
&lt;/p&gt;
&lt;p&gt;
Machine learning and computer vision are dynamically growing fields, which have proven to be able to solve very complex tasks. They could also be used for the monitoring of the honeybee colonies and for the inspection of their health state, which could identify potentially dangerous states before the situation is critical, or to better plan periodic bee colony inspections and therefore save significant costs. In this paper, we present an overview of the state-of-the-art computer vision and machine learning applications used for bee monitoring. We also demonstrate the potential of those methods as an example of an automated bee counter algorithm. The paper is aimed at veterinary and apidology professionals and experts, who might not be familiar with machine learning to introduce to them its possibilities, therefore each family of applications is opened by a brief theoretical introduction and motivation related to its base method. We hope that this paper will inspire other scientists to 
&lt;/p&gt;</description></item><item><title>SmartGD&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GAN&#30340;&#22270;&#24418;&#32472;&#21046;&#26694;&#26550;&#65292;&#21487;&#20197;&#20248;&#21270;&#19981;&#21516;&#30340;&#23450;&#37327;&#32654;&#23398;&#30446;&#26631;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#21482;&#20851;&#27880;&#21333;&#19968;&#32654;&#23398;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2206.06434</link><description>&lt;p&gt;
SmartGD: &#19968;&#31181;&#22522;&#20110;GAN&#30340;&#22270;&#24418;&#32472;&#21046;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#22810;&#26679;&#21270;&#30340;&#32654;&#23398;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
SmartGD: A GAN-Based Graph Drawing Framework for Diverse Aesthetic Goals. (arXiv:2206.06434v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.06434
&lt;/p&gt;
&lt;p&gt;
SmartGD&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GAN&#30340;&#22270;&#24418;&#32472;&#21046;&#26694;&#26550;&#65292;&#21487;&#20197;&#20248;&#21270;&#19981;&#21516;&#30340;&#23450;&#37327;&#32654;&#23398;&#30446;&#26631;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#21482;&#20851;&#27880;&#21333;&#19968;&#32654;&#23398;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24050;&#32463;&#26377;&#22823;&#37327;&#30340;&#30740;&#31350;&#22312;&#22270;&#24418;&#32472;&#21046;&#26041;&#38754;&#36827;&#34892;&#20102;&#65292;&#20294;&#26159;&#35768;&#22810;&#29616;&#26377;&#30340;&#26041;&#27861;&#21482;&#20851;&#27880;&#20248;&#21270;&#22270;&#24418;&#24067;&#23616;&#30340;&#26576;&#19968;&#20010;&#32654;&#23398;&#26041;&#38754;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#27425;&#20248;&#30340;&#32467;&#26524;&#12290;&#24050;&#26377;&#30340;&#19968;&#20123;&#26041;&#27861;&#35797;&#22270;&#24320;&#21457;&#19968;&#31181;&#28789;&#27963;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#20248;&#21270;&#30001;&#19981;&#21516;&#32654;&#23398;&#26631;&#20934;&#27979;&#37327;&#30340;&#19981;&#21516;&#32654;&#23398;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#37325;&#22823;&#36827;&#23637;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#20960;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#24067;&#23616;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#23637;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#22270;&#24418;&#32472;&#21046;&#20013;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#29616;&#26377;&#26041;&#27861;&#37117;&#26080;&#27861;&#30452;&#25509;&#24212;&#29992;&#20110;&#20248;&#21270;&#38750;&#21487;&#24494;&#26631;&#20934;&#32780;&#19981;&#38656;&#35201;&#29305;&#27530;&#22788;&#29702;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;SmartGD&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#20248;&#21270;&#19981;&#21516;&#30340;&#23450;&#37327;&#32654;&#23398;&#30446;&#26631;&#65292;&#26080;&#35770;&#23427;&#20204;&#26159;&#21542;&#21487;&#24494;&#12290;
&lt;/p&gt;
&lt;p&gt;
While a multitude of studies have been conducted on graph drawing, many existing methods only focus on optimizing a single aesthetic aspect of graph layouts, which can lead to sub-optimal results. There are a few existing methods that have attempted to develop a flexible solution for optimizing different aesthetic aspects measured by different aesthetic criteria. Furthermore, thanks to the significant advance in deep learning techniques, several deep learning-based layout methods were proposed recently. These methods have demonstrated the advantages of deep learning approaches for graph drawing. However, none of these existing methods can be directly applied to optimizing non-differentiable criteria without special accommodation. In this work, we propose a novel Generative Adversarial Network (GAN) based deep learning framework for graph drawing, called SmartGD, which can optimize different quantitative aesthetic goals, regardless of their differentiability. To demonstrate the effectiv
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#33258;&#32452;&#32455;&#26144;&#23556;(SOM)&#20998;&#26512;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#19982;&#20915;&#31574;&#30456;&#20851;&#30340;&#20869;&#37096;&#32534;&#30721;&#65292;&#21457;&#29616;&#27973;&#23618;&#23558;&#29305;&#24449;&#21387;&#32553;&#21040;&#32039;&#20945;&#31354;&#38388;&#20013;&#65292;&#32780;&#28145;&#23618;&#23558;&#29305;&#24449;&#31354;&#38388;&#25193;&#23637;&#65292;&#24182;&#25351;&#20986;&#21387;&#32553;&#29305;&#24449;&#21487;&#33021;&#23548;&#33268;&#23545;&#25932;&#23545;&#25200;&#21160;&#30340;&#33030;&#24369;&#24615;&#12290;</title><link>http://arxiv.org/abs/2205.10952</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21151;&#33021;&#24615;&#31070;&#32463;&#32534;&#30721;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Analysis of functional neural codes of deep learning models. (arXiv:2205.10952v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.10952
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#33258;&#32452;&#32455;&#26144;&#23556;(SOM)&#20998;&#26512;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#19982;&#20915;&#31574;&#30456;&#20851;&#30340;&#20869;&#37096;&#32534;&#30721;&#65292;&#21457;&#29616;&#27973;&#23618;&#23558;&#29305;&#24449;&#21387;&#32553;&#21040;&#32039;&#20945;&#31354;&#38388;&#20013;&#65292;&#32780;&#28145;&#23618;&#23558;&#29305;&#24449;&#31354;&#38388;&#25193;&#23637;&#65292;&#24182;&#25351;&#20986;&#21387;&#32553;&#29305;&#24449;&#21487;&#33021;&#23548;&#33268;&#23545;&#25932;&#23545;&#25200;&#21160;&#30340;&#33030;&#24369;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#20316;&#20026;&#28145;&#24230;&#23398;&#20064;(DL)&#30340;&#20195;&#29702;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#24182;&#34892;/&#39034;&#24207;&#25805;&#20316;&#12290;&#36825;&#20351;&#24471;&#29702;&#35299;DNNs&#30340;&#25805;&#20316;&#21464;&#24471;&#22256;&#38590;&#65292;&#38459;&#30861;&#20102;&#36866;&#24403;&#30340;&#35786;&#26029;&#12290;&#22312;&#27809;&#26377;&#23545;&#20854;&#20869;&#37096;&#36807;&#31243;&#26377;&#26356;&#22909;&#30340;&#20102;&#35299;&#20043;&#21069;&#65292;&#22312;&#39640;&#39118;&#38505;&#39046;&#22495;&#37096;&#32626;DNNs&#21487;&#33021;&#23548;&#33268;&#28798;&#38590;&#24615;&#25925;&#38556;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#26500;&#24314;&#26356;&#21487;&#38752;&#30340;DNNs/DL&#26469;&#35299;&#20915;&#39640;&#39118;&#38505;&#29616;&#23454;&#19990;&#30028;&#38382;&#39064;&#65292;&#25105;&#20204;&#24517;&#39035;&#28145;&#20837;&#20102;&#35299;DNNs&#20915;&#31574;&#32972;&#21518;&#30340;&#20869;&#37096;&#25805;&#20316;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20351;&#29992;&#33258;&#32452;&#32455;&#26144;&#23556;(SOM)&#20998;&#26512;&#19982;DNNs&#20915;&#31574;&#30456;&#20851;&#30340;DL&#27169;&#22411;&#30340;&#20869;&#37096;&#32534;&#30721;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#38752;&#36817;&#36755;&#20837;&#23618;&#30340;&#27973;&#23618;&#23558;&#29305;&#24449;&#21387;&#32553;&#21040;&#32039;&#20945;&#31354;&#38388;&#20013;&#65292;&#32780;&#38752;&#36817;&#36755;&#20986;&#23618;&#30340;&#28145;&#23618;&#23558;&#29305;&#24449;&#31354;&#38388;&#25193;&#23637;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#26377;&#35777;&#25454;&#34920;&#26126;&#65292;&#21387;&#32553;&#29305;&#24449;&#21487;&#33021;&#23548;&#33268;DNNs&#23545;&#25932;&#23545;&#25200;&#21160;&#30340;&#33030;&#24369;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs), the agents of deep learning (DL), require a massive number of parallel/sequential operations. This makes it difficult to comprehend DNNs' operations and impedes proper diagnosis. Without better knowledge of their internal process, deploying DNNs in high-stakes domains can lead to catastrophic failures. Therefore, to build more reliable DNNs/DL to be deployed in high-stakes real-world problems, it is imperative that we gain insights into DNNs' internal operations underlying their decision-making. Here, we use the self-organizing map (SOM) to analyze DL models' internal codes associated with DNNs' decision-making. Our analyses suggest that shallow layers close to the input layer compress features into condensed space and that deep layers close to the output layer expand feature space. We also found evidence indicating that compressed features may underlie DNNs' vulnerabilities to adversarial perturbations.
&lt;/p&gt;</description></item><item><title>GUARD&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#24418;&#36890;&#29992;&#23545;&#25239;&#38450;&#24481;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;GCNs&#23545;&#38024;&#23545;&#24615;&#25915;&#20987;&#30340;&#23616;&#37096;&#33410;&#28857;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#19981;&#38477;&#20302;&#25972;&#20307;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#38450;&#24481;&#12290;</title><link>http://arxiv.org/abs/2204.09803</link><description>&lt;p&gt;
GUARD: &#22270;&#24418;&#36890;&#29992;&#23545;&#25239;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
GUARD: Graph Universal Adversarial Defense. (arXiv:2204.09803v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.09803
&lt;/p&gt;
&lt;p&gt;
GUARD&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#24418;&#36890;&#29992;&#23545;&#25239;&#38450;&#24481;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;GCNs&#23545;&#38024;&#23545;&#24615;&#25915;&#20987;&#30340;&#23616;&#37096;&#33410;&#28857;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#19981;&#38477;&#20302;&#25972;&#20307;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#38450;&#24481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24418;&#21367;&#31215;&#32593;&#32476;&#65288;GCNs&#65289;&#24050;&#34987;&#35777;&#26126;&#23545;&#23567;&#30340;&#23545;&#25239;&#24615;&#25200;&#21160;&#20855;&#26377;&#33030;&#24369;&#24615;&#65292;&#36825;&#25104;&#20026;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#19979;&#24212;&#29992;&#30340;&#20005;&#37325;&#23041;&#32961;&#24182;&#19988;&#26497;&#22823;&#38480;&#21046;&#20102;&#20854;&#24212;&#29992;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#31181;&#23041;&#32961;&#65292;&#24050;&#32463;&#25237;&#20837;&#22823;&#37327;&#30740;&#31350;&#24037;&#20316;&#26469;&#22686;&#21152;GCNs&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#38450;&#24481;&#26041;&#27861;&#36890;&#24120;&#34987;&#35774;&#35745;&#29992;&#20110;&#38450;&#27490;GCNs&#36973;&#21463;&#38750;&#38024;&#23545;&#24615;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#24182;&#19988;&#30528;&#37325;&#20110;&#25972;&#20307;&#24615;&#33021;&#65292;&#36825;&#20351;&#24471;&#20445;&#25252;&#37325;&#35201;&#23616;&#37096;&#33410;&#28857;&#20813;&#21463;&#26356;&#24378;&#26377;&#21147;&#30340;&#38024;&#23545;&#24615;&#25915;&#20987;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30740;&#31350;&#24448;&#24448;&#22312;&#40065;&#26834;&#24615;&#21644;&#24615;&#33021;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;&#36825;&#20123;&#38480;&#21046;&#20984;&#26174;&#20102;&#24320;&#21457;&#19968;&#31181;&#33021;&#22815;&#38450;&#24481;&#38024;&#23545;&#24615;&#25915;&#20987;&#30340;&#23616;&#37096;&#33410;&#28857;&#32780;&#19981;&#25439;&#23475;GCNs&#25972;&#20307;&#24615;&#33021;&#30340;&#26377;&#25928;&#19988;&#39640;&#25928;&#26041;&#27861;&#30340;&#24517;&#35201;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;Graph Universal Adversarial Defense&#65288;GUARD&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph convolutional networks (GCNs) have been shown to be vulnerable to small adversarial perturbations, which becomes a severe threat and largely limits their applications in security-critical scenarios. To mitigate such a threat, considerable research efforts have been devoted to increasing the robustness of GCNs against adversarial attacks. However, current defense approaches are typically designed to prevent GCNs from untargeted adversarial attacks and focus on overall performance, making it challenging to protect important local nodes from more powerful targeted adversarial attacks. Additionally, a trade-off between robustness and performance is often made in existing research. Such limitations highlight the need for developing an effective and efficient approach that can defend local nodes against targeted attacks, without compromising the overall performance of GCNs. In this work, we present a simple yet effective method, named Graph Universal Adversarial Defense (GUARD). Unlike
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31163;&#25955;&#26102;&#38388;&#32447;&#24615;&#31995;&#32479;&#21644;Schr\"odinger&#26725;&#30340;&#26368;&#22823;&#29109;&#26368;&#20248;&#23494;&#24230;&#25511;&#21046;&#38382;&#39064;&#65292;&#36890;&#36807;&#24341;&#20837;&#29305;&#23450;&#26102;&#38388;&#28857;&#30340;&#39640;&#26031;&#23494;&#24230;&#32422;&#26463;&#26469;&#30452;&#25509;&#25511;&#21046;&#31995;&#32479;&#29366;&#24577;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2204.05263</link><description>&lt;p&gt;
&#31163;&#25955;&#26102;&#38388;&#32447;&#24615;&#31995;&#32479;&#21644;Schr\"odinger&#26725;&#30340;&#26368;&#22823;&#29109;&#26368;&#20248;&#23494;&#24230;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Maximum entropy optimal density control of discrete-time linear systems and Schr\"odinger bridges. (arXiv:2204.05263v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.05263
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31163;&#25955;&#26102;&#38388;&#32447;&#24615;&#31995;&#32479;&#21644;Schr\"odinger&#26725;&#30340;&#26368;&#22823;&#29109;&#26368;&#20248;&#23494;&#24230;&#25511;&#21046;&#38382;&#39064;&#65292;&#36890;&#36807;&#24341;&#20837;&#29305;&#23450;&#26102;&#38388;&#28857;&#30340;&#39640;&#26031;&#23494;&#24230;&#32422;&#26463;&#26469;&#30452;&#25509;&#25511;&#21046;&#31995;&#32479;&#29366;&#24577;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#30830;&#23450;&#24615;&#31163;&#25955;&#26102;&#38388;&#32447;&#24615;&#31995;&#32479;&#30340;&#29109;&#27491;&#21017;&#21270;&#29256;&#26412;&#30340;&#26368;&#20248;&#23494;&#24230;&#25511;&#21046;&#12290;&#29109;&#27491;&#21017;&#21270;&#25110;&#26368;&#22823;&#29109;&#65288;MaxEnt&#65289;&#26041;&#27861;&#22312;&#26368;&#20248;&#25511;&#21046;&#20013;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#29305;&#21035;&#26159;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#30001;&#20110;&#20854;&#33258;&#28982;&#30340;&#25506;&#32034;&#31574;&#30053;&#31561;&#35768;&#22810;&#20248;&#28857;&#12290;&#23613;&#31649;&#26377;&#36825;&#20123;&#20248;&#28857;&#65292;&#30001;&#27491;&#21017;&#21270;&#24341;&#36215;&#30340;&#39640;&#29109;&#25511;&#21046;&#31574;&#30053;&#23558;&#27010;&#29575;&#19981;&#30830;&#23450;&#24615;&#24341;&#20837;&#31995;&#32479;&#65292;&#36825;&#20005;&#37325;&#38480;&#21046;&#20102;MaxEnt&#26368;&#20248;&#25511;&#21046;&#22312;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#25351;&#23450;&#30340;&#26102;&#38388;&#28857;&#23545;MaxEnt&#26368;&#20248;&#25511;&#21046;&#26045;&#21152;&#20102;&#19968;&#20010;Gaussian&#23494;&#24230;&#32422;&#26463;&#65292;&#20197;&#30452;&#25509;&#25511;&#21046;&#29366;&#24577;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;MaxEnt&#26368;&#20248;&#23494;&#24230;&#25511;&#21046;&#30340;&#26174;&#24335;&#24418;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#32771;&#34385;&#20102;&#23558;&#23494;&#24230;&#32422;&#26463;&#26367;&#25442;&#20026;&#23450;&#28857;&#32422;&#26463;&#30340;&#24773;&#20917;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#30456;&#20851;&#30340;&#29366;&#24577;&#36807;&#31243;&#25551;&#36848;&#20026;&#19968;&#20010;&#22266;&#23450;&#36807;&#31243;&#65292;&#36825;&#26159;&#23545;&#32463;&#20856;Schr\"odinger&#26725;&#27169;&#22411;&#30340;&#19968;&#33324;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider an entropy-regularized version of optimal density control of deterministic discrete-time linear systems. Entropy regularization, or a maximum entropy (MaxEnt) method for optimal control has attracted much attention especially in reinforcement learning due to its many advantages such as a natural exploration strategy. Despite the merits, high-entropy control policies induced by the regularization introduce probabilistic uncertainty into systems, which severely limits the applicability of MaxEnt optimal control to safety-critical systems. To remedy this situation, we impose a Gaussian density constraint at a specified time on the MaxEnt optimal control to directly control state uncertainty. Specifically, we derive the explicit form of the MaxEnt optimal density control. In addition, we also consider the case where density constraints are replaced by fixed point constraints. Then, we characterize the associated state process as a pinned process, which is a generalization of th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#30340;&#26041;&#27861;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#24046;&#36317;&#65292;&#36890;&#36807;&#35745;&#31639;&#21152;&#26435;&#22270;&#30340;&#21516;&#35843;&#25345;&#20037;&#22270;&#65292;&#24182;&#27604;&#36739;&#19981;&#21516;&#25968;&#20540;&#27719;&#24635;&#30340;&#26377;&#29992;&#24615;&#65292;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#21644;&#37096;&#20998;&#35299;&#37322;&#27867;&#21270;&#24046;&#36317;&#65292;&#32780;&#26080;&#38656;&#27979;&#35797;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#35782;&#21035;&#20219;&#21153;&#19978;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2203.12330</link><description>&lt;p&gt;
&#20351;&#29992;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#39044;&#27979;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Predicting the generalization gap in neural networks using topological data analysis. (arXiv:2203.12330v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.12330
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#30340;&#26041;&#27861;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#24046;&#36317;&#65292;&#36890;&#36807;&#35745;&#31639;&#21152;&#26435;&#22270;&#30340;&#21516;&#35843;&#25345;&#20037;&#22270;&#65292;&#24182;&#27604;&#36739;&#19981;&#21516;&#25968;&#20540;&#27719;&#24635;&#30340;&#26377;&#29992;&#24615;&#65292;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#21644;&#37096;&#20998;&#35299;&#37322;&#27867;&#21270;&#24046;&#36317;&#65292;&#32780;&#26080;&#38656;&#27979;&#35797;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#35782;&#21035;&#20219;&#21153;&#19978;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#22312;&#26410;&#35265;&#25968;&#25454;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#23545;&#20110;&#35774;&#35745;&#26356;&#20581;&#22766;&#21487;&#38752;&#30340;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#21033;&#29992;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#30340;&#26041;&#27861;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#24046;&#36317;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22312;&#35757;&#32451;&#38454;&#27573;&#21518;&#35745;&#31639;&#31070;&#32463;&#20803;&#28608;&#27963;&#30456;&#20851;&#24615;&#26500;&#24314;&#30340;&#21152;&#26435;&#22270;&#30340;&#21516;&#35843;&#25345;&#20037;&#22270;&#65292;&#26088;&#22312;&#25429;&#25417;&#19982;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#30456;&#20851;&#30340;&#27169;&#24335;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#25345;&#20037;&#22270;&#30340;&#19981;&#21516;&#25968;&#20540;&#27719;&#24635;&#30340;&#26377;&#29992;&#24615;&#65292;&#24182;&#26174;&#31034;&#20854;&#20013;&#19968;&#20123;&#30340;&#32452;&#21512;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#21644;&#37096;&#20998;&#35299;&#37322;&#27867;&#21270;&#24046;&#36317;&#65292;&#32780;&#26080;&#38656;&#27979;&#35797;&#38598;&#12290;&#22312;&#20004;&#20010;&#35745;&#31639;&#26426;&#35270;&#35273;&#35782;&#21035;&#20219;&#21153;&#65288;CIFAR10&#21644;SVHN&#65289;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#27867;&#21270;&#24046;&#36317;&#39044;&#27979;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding how neural networks generalize on unseen data is crucial for designing more robust and reliable models. In this paper, we study the generalization gap of neural networks using methods from topological data analysis. For this purpose, we compute homological persistence diagrams of weighted graphs constructed from neuron activation correlations after a training phase, aiming to capture patterns that are linked to the generalization capacity of the network. We compare the usefulness of different numerical summaries from persistence diagrams and show that a combination of some of them can accurately predict and partially explain the generalization gap without the need of a test set. Evaluation on two computer vision recognition tasks (CIFAR10 and SVHN) shows competitive generalization gap prediction when compared against state-of-the-art methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#23398;&#20064;&#37197;&#20934;&#31639;&#27861;&#65288;AutoReg&#65289;&#65292;&#23427;&#21487;&#20197;&#21327;&#21516;&#20248;&#21270;&#32593;&#32476;&#26550;&#26500;&#21644;&#35757;&#32451;&#30446;&#26631;&#65292;&#20351;&#38750;&#35745;&#31639;&#26426;&#19987;&#23478;&#33021;&#22815;&#26041;&#20415;&#22320;&#25214;&#21040;&#36866;&#29992;&#20110;&#19981;&#21516;&#22330;&#26223;&#30340;&#37197;&#20934;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2203.06810</link><description>&lt;p&gt;
&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#32593;&#32476;&#26550;&#26500;&#21644;&#30446;&#26631;&#20989;&#25968;&#30340;&#33258;&#21160;&#21270;&#23398;&#20064;&#22312;&#21487;&#21464;&#24418;&#21307;&#23398;&#22270;&#20687;&#37197;&#20934;&#20013;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Automated Learning for Deformable Medical Image Registration by Jointly Optimizing Network Architectures and Objective Functions. (arXiv:2203.06810v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.06810
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#23398;&#20064;&#37197;&#20934;&#31639;&#27861;&#65288;AutoReg&#65289;&#65292;&#23427;&#21487;&#20197;&#21327;&#21516;&#20248;&#21270;&#32593;&#32476;&#26550;&#26500;&#21644;&#35757;&#32451;&#30446;&#26631;&#65292;&#20351;&#38750;&#35745;&#31639;&#26426;&#19987;&#23478;&#33021;&#22815;&#26041;&#20415;&#22320;&#25214;&#21040;&#36866;&#29992;&#20110;&#19981;&#21516;&#22330;&#26223;&#30340;&#37197;&#20934;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#21464;&#24418;&#22270;&#20687;&#37197;&#20934;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#19968;&#20010;&#25104;&#21151;&#30340;&#37197;&#20934;&#31639;&#27861;&#65292;&#26080;&#35770;&#26159;&#20174;&#20256;&#32479;&#33021;&#37327;&#20248;&#21270;&#36824;&#26159;&#28145;&#24230;&#32593;&#32476;&#20013;&#23548;&#20986;&#65292;&#37117;&#38656;&#35201;&#35745;&#31639;&#26426;&#19987;&#23478;&#33457;&#36153;&#22823;&#37327;&#30340;&#31934;&#21147;&#26469;&#35774;&#35745;&#33391;&#22909;&#30340;&#37197;&#20934;&#33021;&#37327;&#25110;&#32773;&#20026;&#29305;&#23450;&#31867;&#22411;&#30340;&#21307;&#23398;&#25968;&#25454;&#31934;&#24515;&#35843;&#25972;&#32593;&#32476;&#26550;&#26500;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#23398;&#20064;&#37197;&#20934;&#31639;&#27861;&#65288;AutoReg&#65289;&#65292;&#23427;&#21487;&#20197;&#21327;&#21516;&#20248;&#21270;&#32593;&#32476;&#26550;&#26500;&#21644;&#30456;&#24212;&#30340;&#35757;&#32451;&#30446;&#26631;&#65292;&#20351;&#38750;&#35745;&#31639;&#26426;&#19987;&#23478;&#65288;&#20363;&#22914;&#21307;&#23398;/&#20020;&#24202;&#29992;&#25143;&#65289;&#33021;&#22815;&#26041;&#20415;&#22320;&#25214;&#21040;&#36866;&#29992;&#20110;&#19981;&#21516;&#22330;&#26223;&#30340;&#29616;&#25104;&#37197;&#20934;&#31639;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#19977;&#23618;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#21160;&#25628;&#32034;&#26426;&#21046;&#21644;&#21327;&#21516;&#20248;&#21270;&#25512;&#23548;&#20986;&#37197;&#20934;&#32593;&#32476;&#26550;&#26500;&#21644;&#30446;&#26631;&#12290;&#25105;&#20204;&#22312;&#22810;&#31449;&#28857;&#20307;&#31215;&#25968;&#25454;&#38598;&#21644;&#21508;&#31181;&#37197;&#20934;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#22270;&#20687;&#37197;&#20934;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deformable image registration plays a critical role in various tasks of medical image analysis. A successful registration algorithm, either derived from conventional energy optimization or deep networks requires tremendous efforts from computer experts to well design registration energy or to carefully tune network architectures for the specific type of medical data. To tackle the aforementioned problems, this paper proposes an automated learning registration algorithm (AutoReg) that cooperatively optimizes both architectures and their corresponding training objectives, enabling non-computer experts, e.g., medical/clinical users, to conveniently find off-the-shelf registration algorithms for diverse scenarios. Specifically, we establish a triple-level framework to deduce registration network architectures and objectives with an auto-searching mechanism and cooperating optimization. We conduct image registration experiments on multi-site volume datasets and various registration tasks. E
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#39640;&#26031;&#36807;&#31243;&#25554;&#20540;&#20013;&#20809;&#28369;&#21442;&#25968;&#20272;&#35745;&#30340;&#28176;&#36817;&#30028;&#38480;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20809;&#28369;&#21442;&#25968;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#19981;&#33021;&#22312;&#28176;&#36817;&#24847;&#20041;&#19979;&#27424;&#24179;&#28369;&#30495;&#20540;&#65292;&#24182;&#19988;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#33021;&#24674;&#22797;&#19968;&#31867;&#20998;&#27573;&#25903;&#25345;&#33258;&#30456;&#20284;&#20989;&#25968;&#30340;&#30495;&#23454;&#20809;&#28369;&#24230;&#12290;</title><link>http://arxiv.org/abs/2203.05400</link><description>&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#25554;&#20540;&#20013;&#20809;&#28369;&#21442;&#25968;&#20272;&#35745;&#30340;&#28176;&#36817;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Asymptotic Bounds for Smoothness Parameter Estimates in Gaussian Process Interpolation. (arXiv:2203.05400v4 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.05400
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#39640;&#26031;&#36807;&#31243;&#25554;&#20540;&#20013;&#20809;&#28369;&#21442;&#25968;&#20272;&#35745;&#30340;&#28176;&#36817;&#30028;&#38480;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20809;&#28369;&#21442;&#25968;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#19981;&#33021;&#22312;&#28176;&#36817;&#24847;&#20041;&#19979;&#27424;&#24179;&#28369;&#30495;&#20540;&#65292;&#24182;&#19988;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#33021;&#24674;&#22797;&#19968;&#31867;&#20998;&#27573;&#25903;&#25345;&#33258;&#30456;&#20284;&#20989;&#25968;&#30340;&#30495;&#23454;&#20809;&#28369;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#29992;Matern&#21327;&#26041;&#24046;&#26680;&#23558;&#30830;&#23450;&#24615;&#21709;&#24212;&#20989;&#25968;&#65288;&#22914;&#35745;&#31639;&#26426;&#23454;&#39564;&#30340;&#36755;&#20986;&#65289;&#24314;&#27169;&#20026;&#39640;&#26031;&#36807;&#31243;&#12290;Matern&#26680;&#30340;&#20809;&#28369;&#21442;&#25968;&#20915;&#23450;&#20102;&#27169;&#22411;&#22312;&#22823;&#25968;&#25454;&#26497;&#38480;&#19979;&#30340;&#35768;&#22810;&#37325;&#35201;&#23646;&#24615;&#65292;&#21253;&#25324;&#26465;&#20214;&#22343;&#20540;&#25910;&#25947;&#21040;&#21709;&#24212;&#20989;&#25968;&#30340;&#36895;&#29575;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#24403;&#25968;&#25454;&#22312;&#22266;&#23450;&#26377;&#30028;&#23376;&#38598;$\mathbb{R}^d$&#19978;&#33719;&#24471;&#26102;&#65292;&#20809;&#28369;&#21442;&#25968;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#19981;&#33021;&#22312;&#28176;&#36817;&#24847;&#20041;&#19979;&#27424;&#24179;&#28369;&#30495;&#20540;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#22914;&#26524;&#25968;&#25454;&#29983;&#25104;&#30340;&#21709;&#24212;&#20989;&#25968;&#20855;&#26377;Sobolev&#20809;&#28369;&#24230;$\nu_0 &gt; d/2$&#65292;&#37027;&#20040;&#20809;&#28369;&#21442;&#25968;&#20272;&#35745;&#19981;&#33021;&#22312;&#28176;&#36817;&#24847;&#20041;&#19979;&#23567;&#20110;$\nu_0$&#12290;&#36825;&#19968;&#19979;&#30028;&#26159;&#31934;&#20934;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22312;&#19968;&#31867;&#20998;&#27573;&#25903;&#25345;&#33258;&#30456;&#20284;&#20989;&#25968;&#20013;&#33021;&#24674;&#22797;&#30495;&#23454;&#30340;&#20809;&#28369;&#24230;&#12290;&#23545;&#20110;&#20132;&#21449;&#39564;&#35777;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#28176;&#36817;&#19979;&#30028;$\nu_0-d/2$&#65292;&#20294;&#36825;&#24456;&#19981;&#21487;&#33021;&#25104;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is common to model a deterministic response function, such as the output of a computer experiment, as a Gaussian process with a Mat\'ern covariance kernel. The smoothness parameter of a Mat\'ern kernel determines many important properties of the model in the large data limit, including the rate of convergence of the conditional mean to the response function. We prove that the maximum likelihood estimate of the smoothness parameter cannot asymptotically undersmooth the truth when the data are obtained on a fixed bounded subset of $\mathbb{R}^d$. That is, if the data-generating response function has Sobolev smoothness $\nu_0 &gt; d/2$, then the smoothness parameter estimate cannot be asymptotically less than $\nu_0$. The lower bound is sharp. Additionally, we show that maximum likelihood estimation recovers the true smoothness for a class of compactly supported self-similar functions. For cross-validation we prove an asymptotic lower bound $\nu_0 - d/2$, which however is unlikely to be s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19981;&#23433;&#23450;&#22810;&#33218;&#36172;&#21338;&#65288;RMAB&#65289;&#38382;&#39064;&#20013;&#36827;&#34892;&#20915;&#31574;&#38598;&#20013;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#30452;&#25509;&#35757;&#32451;&#39044;&#27979;&#27169;&#22411;&#26469;&#26368;&#22823;&#21270;Whittle&#25351;&#25968;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2202.00916</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#20915;&#31574;&#38598;&#20013;&#23398;&#20064;&#22312;&#20855;&#26377;&#24212;&#29992;&#20110;&#27597;&#23156;&#20581;&#24247;&#30340;&#19981;&#23433;&#23450;&#22810;&#33218;&#36172;&#21338;&#29615;&#22659;&#19979;
&lt;/p&gt;
&lt;p&gt;
Scalable Decision-Focused Learning in Restless Multi-Armed Bandits with Application to Maternal and Child Health. (arXiv:2202.00916v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.00916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19981;&#23433;&#23450;&#22810;&#33218;&#36172;&#21338;&#65288;RMAB&#65289;&#38382;&#39064;&#20013;&#36827;&#34892;&#20915;&#31574;&#38598;&#20013;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#30452;&#25509;&#35757;&#32451;&#39044;&#27979;&#27169;&#22411;&#26469;&#26368;&#22823;&#21270;Whittle&#25351;&#25968;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#26410;&#30693;&#33218;&#20043;&#38388;&#36716;&#31227;&#21160;&#21147;&#23398;&#20294;&#20855;&#26377;&#24050;&#30693;&#30456;&#20851;&#33218;&#29305;&#24449;&#30340;&#19981;&#23433;&#23450;&#22810;&#33218;&#36172;&#21338;&#65288;RMAB&#65289;&#38382;&#39064;&#12290;&#30446;&#26631;&#26159;&#23398;&#20064;&#19968;&#20010;&#27169;&#22411;&#65292;&#26681;&#25454;&#29305;&#24449;&#39044;&#27979;&#36716;&#31227;&#21160;&#21147;&#23398;&#65292;&#20854;&#20013;Whittle&#25351;&#25968;&#31574;&#30053;&#20351;&#29992;&#39044;&#27979;&#30340;&#36716;&#31227;&#26469;&#35299;&#20915;RMAB&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#24120;&#36890;&#36807;&#26368;&#22823;&#21270;&#39044;&#27979;&#20934;&#30830;&#24615;&#32780;&#19981;&#26159;&#26368;&#32456;RMAB&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#26469;&#23398;&#20064;&#27169;&#22411;&#65292;&#23548;&#33268;&#35757;&#32451;&#30446;&#26631;&#19982;&#35780;&#20272;&#30446;&#26631;&#19981;&#21305;&#37197;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;RMAB&#20013;&#30452;&#25509;&#35757;&#32451;&#39044;&#27979;&#27169;&#22411;&#20197;&#26368;&#22823;&#21270;Whittle&#25351;&#25968;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies restless multi-armed bandit (RMAB) problems with unknown arm transition dynamics but with known correlated arm features. The goal is to learn a model to predict transition dynamics given features, where the Whittle index policy solves the RMAB problems using predicted transitions. However, prior works often learn the model by maximizing the predictive accuracy instead of final RMAB solution quality, causing a mismatch between training and evaluation objectives. To address this shortcoming, we propose a novel approach for decision-focused learning in RMAB that directly trains the predictive model to maximize the Whittle index solution quality. We present three key contributions: (i) we establish differentiability of the Whittle index policy to support decision-focused learning; (ii) we significantly improve the scalability of decision-focused learning approaches in sequential problems, specifically RMAB problems; (iii) we apply our algorithm to a previously collected 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Grad-Align&#65292;&#19968;&#31181;&#28176;&#36827;&#32593;&#32476;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#24378;&#19968;&#33268;&#24615;&#33410;&#28857;&#23545;&#36880;&#27493;&#21457;&#29616;&#33410;&#28857;&#23545;&#12290;&#35813;&#26041;&#27861;&#39318;&#20808;&#29983;&#25104;&#33410;&#28857;&#23884;&#20837;&#65292;&#28982;&#21518;&#35745;&#31639;&#21452;&#24863;&#30693;&#30456;&#20284;&#24615;&#24230;&#37327;&#36880;&#27493;&#23545;&#40784;&#33410;&#28857;&#12290;</title><link>http://arxiv.org/abs/2201.10945</link><description>&lt;p&gt;
&#20851;&#20110;&#20351;&#29992;&#21452;&#24863;&#30693;&#30456;&#20284;&#24615;&#36827;&#34892;&#28176;&#36827;&#32593;&#32476;&#23545;&#40784;&#30340;&#33021;&#21147;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Power of Gradual Network Alignment Using Dual-Perception Similarities. (arXiv:2201.10945v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.10945
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Grad-Align&#65292;&#19968;&#31181;&#28176;&#36827;&#32593;&#32476;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#24378;&#19968;&#33268;&#24615;&#33410;&#28857;&#23545;&#36880;&#27493;&#21457;&#29616;&#33410;&#28857;&#23545;&#12290;&#35813;&#26041;&#27861;&#39318;&#20808;&#29983;&#25104;&#33410;&#28857;&#23884;&#20837;&#65292;&#28982;&#21518;&#35745;&#31639;&#21452;&#24863;&#30693;&#30456;&#20284;&#24615;&#24230;&#37327;&#36880;&#27493;&#23545;&#40784;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#23545;&#40784;&#65288;NA&#65289;&#26159;&#22522;&#20110;&#32593;&#32476;&#32467;&#26500;&#21644;&#33410;&#28857;&#23646;&#24615;&#26597;&#25214;&#20004;&#20010;&#32593;&#32476;&#20043;&#38388;&#33410;&#28857;&#23545;&#24212;&#20851;&#31995;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21160;&#26426;&#22312;&#20110;&#65292;&#30001;&#20110;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;NA&#26041;&#27861;&#37117;&#35797;&#22270;&#19968;&#27425;&#24615;&#21457;&#29616;&#25152;&#26377;&#33410;&#28857;&#23545;&#65292;&#22240;&#27492;&#23427;&#20204;&#27809;&#26377;&#21033;&#29992;&#36890;&#36807;&#33410;&#28857;&#23545;&#24212;&#20851;&#31995;&#30340;&#20013;&#38388;&#21457;&#29616;&#26469;&#26356;&#20934;&#30830;&#22320;&#25214;&#21040;&#33410;&#28857;&#21305;&#37197;&#36807;&#31243;&#20013;&#30340;&#19979;&#19968;&#20010;&#23545;&#24212;&#20851;&#31995;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Grad-Align&#65292;&#19968;&#31181;&#26032;&#30340;&#28176;&#36827;&#32593;&#32476;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#22312;&#28176;&#36827;&#21305;&#37197;&#30340;&#26089;&#26399;&#38454;&#27573;&#23481;&#26131;&#21457;&#29616;&#30340;&#33410;&#28857;&#23545;&#26469;&#36880;&#27493;&#21457;&#29616;&#33410;&#28857;&#23545;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;Grad-Align&#39318;&#20808;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#25105;&#20204;&#30340;&#36880;&#23618;&#37325;&#26500;&#25439;&#22833;&#29983;&#25104;&#20004;&#20010;&#32593;&#32476;&#30340;&#33410;&#28857;&#23884;&#20837;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#35745;&#31639;&#21452;&#24863;&#30693;&#30456;&#20284;&#24615;&#24230;&#37327;&#36880;&#27493;&#23545;&#40784;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Network alignment (NA) is the task of finding the correspondence of nodes between two networks based on the network structure and node attributes. Our study is motivated by the fact that, since most of existing NA methods have attempted to discover all node pairs at once, they do not harness information enriched through interim discovery of node correspondences to more accurately find the next correspondences during the node matching. To tackle this challenge, we propose Grad-Align, a new NA method that gradually discovers node pairs by making full use of node pairs exhibiting strong consistency, which are easy to be discovered in the early stage of gradual matching. Specifically, Grad-Align first generates node embeddings of the two networks based on graph neural networks along with our layer-wise reconstruction loss, a loss built upon capturing the first-order and higher-order neighborhood structures. Then, nodes are gradually aligned by computing dual-perception similarity measures 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#22270;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#22810;&#35270;&#35282;&#23376;&#31354;&#38388;&#32858;&#31867;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#26410;&#24314;&#31435;&#22270;&#23398;&#20064;&#21644;&#32858;&#31867;&#20043;&#38388;&#20851;&#32852;&#20197;&#21450;&#24573;&#30053;&#23616;&#37096;&#32467;&#26500;&#37325;&#35201;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2201.04604</link><description>&lt;p&gt;
&#22810;&#35270;&#35282;&#23376;&#31354;&#38388;&#32858;&#31867;&#30340;&#32454;&#31890;&#24230;&#22270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Fine-grained Graph Learning for Multi-view Subspace Clustering. (arXiv:2201.04604v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.04604
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#22270;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#22810;&#35270;&#35282;&#23376;&#31354;&#38388;&#32858;&#31867;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#26410;&#24314;&#31435;&#22270;&#23398;&#20064;&#21644;&#32858;&#31867;&#20043;&#38388;&#20851;&#32852;&#20197;&#21450;&#24573;&#30053;&#23616;&#37096;&#32467;&#26500;&#37325;&#35201;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35270;&#35282;&#23376;&#31354;&#38388;&#32858;&#31867;&#65288;MSC&#65289;&#26159;&#19968;&#31181;&#36890;&#36807;&#25972;&#21512;&#24322;&#26500;&#20449;&#24687;&#26469;&#25581;&#31034;&#19981;&#21516;&#35270;&#35282;&#19979;&#38544;&#34255;&#30340;&#20869;&#22312;&#32858;&#31867;&#32467;&#26500;&#30340;&#19968;&#31181;&#27969;&#34892;&#26080;&#30417;&#30563;&#26041;&#27861;&#12290;&#36890;&#24120;&#65292;MSC&#26041;&#27861;&#20351;&#29992;&#22270;&#65288;&#25110;&#30456;&#20284;&#24615;&#30697;&#38453;&#65289;&#34701;&#21512;&#26469;&#23398;&#20064;&#19968;&#20010;&#20849;&#21516;&#30340;&#32467;&#26500;&#65292;&#24182;&#36827;&#19968;&#27493;&#24212;&#29992;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#36827;&#34892;&#32858;&#31867;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#22823;&#22810;&#25968;&#26041;&#27861;&#27809;&#26377;&#24314;&#31435;&#22270;&#23398;&#20064;&#21644;&#32858;&#31867;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#21516;&#26102;&#65292;&#20256;&#32479;&#30340;&#22270;&#34701;&#21512;&#31574;&#30053;&#23558;&#31895;&#31890;&#24230;&#26435;&#37325;&#20998;&#37197;&#32473;&#22810;&#22270;&#30340;&#32452;&#21512;&#65292;&#24573;&#35270;&#20102;&#23616;&#37096;&#32467;&#26500;&#30340;&#37325;&#35201;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22810;&#35270;&#35282;&#23376;&#31354;&#38388;&#32858;&#31867;&#30340;&#32454;&#31890;&#24230;&#22270;&#23398;&#20064;&#26694;&#26550;&#65288;FGL-MSC&#65289;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;&#22810;&#35270;&#35282;&#20449;&#24687;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#29305;&#23450;&#30340;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#24341;&#20837;&#22270;&#27491;&#21017;&#21270;&#21644;&#23616;&#37096;&#32467;&#26500;&#34701;&#21512;&#27169;&#24335;&#12290;&#20027;&#35201;&#25361;&#25112;&#26159;&#22914;&#20309;&#22312;&#29983;&#25104;&#36866;&#24212;&#20110;&#32858;&#31867;&#30340;&#23398;&#20064;&#22270;&#30340;&#21516;&#26102;&#20248;&#21270;&#32454;&#31890;&#24230;&#34701;&#21512;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-view subspace clustering (MSC) is a popular unsupervised method by integrating heterogeneous information to reveal the intrinsic clustering structure hidden across views. Usually, MSC methods use graphs (or affinity matrices) fusion to learn a common structure, and further apply graph-based approaches to clustering. Despite progress, most of the methods do not establish the connection between graph learning and clustering. Meanwhile, conventional graph fusion strategies assign coarse-grained weights to combine multi-graph, ignoring the importance of local structure. In this paper, we propose a fine-grained graph learning framework for multi-view subspace clustering (FGL-MSC) to address these issues. To utilize the multi-view information sufficiently, we design a specific graph learning method by introducing graph regularization and a local structure fusion pattern. The main challenge is how to optimize the fine-grained fusion weights while generating the learned graph that fits t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;RL&#31639;&#27861;&#65292;&#32467;&#21512;&#20102;&#22522;&#20110;&#23631;&#38556;&#21147;&#30340;&#25511;&#21046;&#31574;&#30053;&#32467;&#26500;&#19982;&#22810;&#27493;&#31574;&#30053;&#35780;&#20272;&#26426;&#21046;&#65292;&#22312;&#20445;&#35777;&#25511;&#21046;&#23433;&#20840;&#30340;&#21516;&#26102;&#65292;&#33021;&#22815;&#24212;&#23545;&#26102;&#38388;&#21464;&#21270;&#30340;&#23433;&#20840;&#32422;&#26463;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#31283;&#23450;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#25910;&#25947;&#24615;&#65292;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;RL&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2112.11217</link><description>&lt;p&gt;
&#27169;&#22411;&#20026;&#22522;&#30784;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#22312;&#26102;&#38388;&#21464;&#21270;&#29366;&#24577;&#21644;&#25511;&#21046;&#32422;&#26463;&#19979;&#30340;&#24212;&#29992;&#65306;&#26234;&#33021;&#36710;&#36742;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Model-Based Safe Reinforcement Learning with Time-Varying State and Control Constraints: An Application to Intelligent Vehicles. (arXiv:2112.11217v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.11217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;RL&#31639;&#27861;&#65292;&#32467;&#21512;&#20102;&#22522;&#20110;&#23631;&#38556;&#21147;&#30340;&#25511;&#21046;&#31574;&#30053;&#32467;&#26500;&#19982;&#22810;&#27493;&#31574;&#30053;&#35780;&#20272;&#26426;&#21046;&#65292;&#22312;&#20445;&#35777;&#25511;&#21046;&#23433;&#20840;&#30340;&#21516;&#26102;&#65292;&#33021;&#22815;&#24212;&#23545;&#26102;&#38388;&#21464;&#21270;&#30340;&#23433;&#20840;&#32422;&#26463;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#31283;&#23450;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#25910;&#25947;&#24615;&#65292;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;RL&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#28436;&#21592;-&#35780;&#35770;&#23478;&#32467;&#26500;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#23398;&#20064;&#19968;&#20010;&#20855;&#26377;&#23433;&#20840;&#24615;&#21644;&#25910;&#25947;&#24615;&#20445;&#35777;&#30340;&#36817;&#20284;&#26368;&#20248;&#25511;&#21046;&#31574;&#30053;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#21516;&#26102;&#65292;&#24456;&#23569;&#26377;&#20316;&#21697;&#35752;&#35770;&#20102;&#22312;&#26102;&#38388;&#21464;&#21270;&#30340;&#23433;&#20840;&#24615;&#32422;&#26463;&#19979;&#35774;&#35745;&#23433;&#20840;RL&#31639;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;RL&#31639;&#27861;&#65292;&#29992;&#20110;&#20855;&#26377;&#26102;&#38388;&#21464;&#21270;&#29366;&#24577;&#21644;&#25511;&#21046;&#32422;&#26463;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#26368;&#20248;&#25511;&#21046;&#12290;&#22312;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#23631;&#38556;&#21147;&#30340;&#25511;&#21046;&#31574;&#30053;&#32467;&#26500;&#65292;&#20197;&#20445;&#35777;&#25511;&#21046;&#23433;&#20840;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27493;&#31574;&#30053;&#35780;&#20272;&#26426;&#21046;&#65292;&#29992;&#20110;&#39044;&#27979;&#31574;&#30053;&#22312;&#26102;&#38388;&#21464;&#21270;&#30340;&#23433;&#20840;&#32422;&#26463;&#19979;&#30340;&#23433;&#20840;&#39118;&#38505;&#65292;&#24182;&#25351;&#23548;&#31574;&#30053;&#23433;&#20840;&#26356;&#26032;&#12290;&#24050;&#35777;&#26126;&#31283;&#23450;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#29702;&#35770;&#32467;&#26524;&#12290;&#21516;&#26102;&#65292;&#20998;&#26512;&#20102;&#28436;&#21592;&#35780;&#35770;&#23478;&#23454;&#29616;&#30340;&#25910;&#25947;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#30340;&#24615;&#33021;&#22312;&#27169;&#25311;&#30340;Sa&#20013;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;RL&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Recently, safe reinforcement learning (RL) with the actor-critic structure for continuous control tasks has received increasing attention. It is still challenging to learn a near-optimal control policy with safety and convergence guarantees. Also, few works have addressed the safe RL algorithm design under time-varying safety constraints. This paper proposes a safe RL algorithm for optimal control of nonlinear systems with time-varying state and control constraints. In the proposed approach, we construct a novel barrier force-based control policy structure to guarantee control safety. A multi-step policy evaluation mechanism is proposed to predict the policy's safety risk under time-varying safety constraints and guide the policy to update safely. Theoretical results on stability and robustness are proven. Also, the convergence of the actor-critic implementation is analyzed. The performance of the proposed algorithm outperforms several state-of-the-art RL algorithms in the simulated Sa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;MDPs&#20013;&#20351;&#29992;Q-Learning&#30340;&#25910;&#25947;&#21644;&#36817;&#20284;&#26368;&#20248;&#24615;&#38382;&#39064;&#12290;&#36890;&#36807;&#37327;&#21270;&#29366;&#24577;&#21644;&#21160;&#20316;&#65292;&#25105;&#20204;&#35777;&#26126;Quantized Q-Learning&#20250;&#25910;&#25947;&#21040;&#19968;&#20010;&#26497;&#38480;&#65292;&#24182;&#19988;&#36825;&#20010;&#26497;&#38480;&#28385;&#36275;&#19968;&#20010;&#26368;&#20248;&#24615;&#26041;&#31243;&#65292;&#20174;&#32780;&#23454;&#29616;&#36817;&#20284;&#26368;&#20248;&#24615;&#12290;</title><link>http://arxiv.org/abs/2111.06781</link><description>&lt;p&gt;
Q-Learning&#29992;&#20110;&#20855;&#26377;&#19968;&#33324;&#29366;&#24577;&#31354;&#38388;&#30340;MDPs: &#36890;&#36807;&#24369;&#36830;&#32493;&#24615;&#19979;&#30340;&#37327;&#21270;&#26469;&#23454;&#29616;&#25910;&#25947;&#21644;&#36817;&#20284;&#26368;&#20248;&#24615;
&lt;/p&gt;
&lt;p&gt;
Q-Learning for MDPs with General Spaces: Convergence and Near Optimality via Quantization under Weak Continuity. (arXiv:2111.06781v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.06781
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;MDPs&#20013;&#20351;&#29992;Q-Learning&#30340;&#25910;&#25947;&#21644;&#36817;&#20284;&#26368;&#20248;&#24615;&#38382;&#39064;&#12290;&#36890;&#36807;&#37327;&#21270;&#29366;&#24577;&#21644;&#21160;&#20316;&#65292;&#25105;&#20204;&#35777;&#26126;Quantized Q-Learning&#20250;&#25910;&#25947;&#21040;&#19968;&#20010;&#26497;&#38480;&#65292;&#24182;&#19988;&#36825;&#20010;&#26497;&#38480;&#28385;&#36275;&#19968;&#20010;&#26368;&#20248;&#24615;&#26041;&#31243;&#65292;&#20174;&#32780;&#23454;&#29616;&#36817;&#20284;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36890;&#24120;&#35201;&#27714;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(MDPs)&#20013;&#30340;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#26159;&#26377;&#38480;&#30340;&#65288;&#20063;&#31216;&#20026;&#21487;&#25511;&#39532;&#23572;&#21487;&#22827;&#38142;&#65289;&#65292;&#25991;&#29486;&#20013;&#24050;&#32463;&#20570;&#20986;&#20102;&#21508;&#31181;&#21162;&#21147;&#65292;&#20197;&#20415;&#23558;&#36825;&#20123;&#31639;&#27861;&#24212;&#29992;&#20110;&#36830;&#32493;&#30340;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#38750;&#24120;&#28201;&#21644;&#30340;&#27491;&#21017;&#24615;&#26465;&#20214;&#19979;&#65288;&#29305;&#21035;&#26159;&#21482;&#28041;&#21450;MDP&#30340;&#36807;&#28193;&#26680;&#30340;&#24369;&#36830;&#32493;&#24615;&#65289;&#65292;&#36890;&#36807;&#23545;&#29366;&#24577;&#21644;&#21160;&#20316;&#36827;&#34892;&#37327;&#21270;&#30340;&#26631;&#20934; Borel MDPs &#30340; Q-Learning&#65288;&#31216;&#20026;Quantized Q-Learning&#65289;&#20250;&#25910;&#25947;&#21040;&#19968;&#20010;&#26497;&#38480;&#65292;&#24182;&#19988;&#36825;&#20010;&#26497;&#38480;&#28385;&#36275;&#19968;&#20010;&#26368;&#20248;&#24615;&#26041;&#31243;&#65292;&#20174;&#32780;&#23454;&#29616;&#36817;&#20284;&#26368;&#20248;&#24615;&#65292;&#35201;&#20040;&#20855;&#26377;&#26174;&#24335;&#30340;&#24615;&#33021;&#30028;&#38480;&#65292;&#35201;&#20040;&#20855;&#26377;&#28176;&#36817;&#26368;&#20248;&#20445;&#35777;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#65306;(i) &#23558;&#37327;&#21270;&#35270;&#20026;&#19968;&#20010;&#27979;&#37327;&#26680;&#65292;&#24182;&#23558;&#37327;&#21270;&#30340;MDP&#35270;&#20026;&#19968;&#20010;&#37096;&#20998;&#35266;&#27979;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#65292;(ii) &#21033;&#29992; Q-Learning &#23545; POMDP &#30340;&#36817;&#20284;&#26368;&#20248;&#24615;&#21644;&#25910;&#25947;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning algorithms often require finiteness of state and action spaces in Markov decision processes (MDPs) (also called controlled Markov chains) and various efforts have been made in the literature towards the applicability of such algorithms for continuous state and action spaces. In this paper, we show that under very mild regularity conditions (in particular, involving only weak continuity of the transition kernel of an MDP), Q-learning for standard Borel MDPs via quantization of states and actions (called Quantized Q-Learning) converges to a limit, and furthermore this limit satisfies an optimality equation which leads to near optimality with either explicit performance bounds or which are guaranteed to be asymptotically optimal. Our approach builds on (i) viewing quantization as a measurement kernel and thus a quantized MDP as a partially observed Markov decision process (POMDP), (ii) utilizing near optimality and convergence results of Q-learning for POMDPs, and (
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#28151;&#26434;&#22240;&#32032;&#24433;&#21709;&#30340;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20986;&#28151;&#26434;&#22240;&#26524;&#21327;&#21516;&#36807;&#28388;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#20026;&#27599;&#20010;&#29305;&#23450;&#28151;&#26434;&#22240;&#32032;&#35774;&#35745;&#27169;&#22411;&#30340;&#19981;&#29616;&#23454;&#24615;&#20197;&#21450;&#28508;&#22312;&#28151;&#26434;&#22240;&#32032;&#26080;&#27861;&#35266;&#23519;&#30340;&#38590;&#39064;&#12290;</title><link>http://arxiv.org/abs/2110.07122</link><description>&lt;p&gt;
&#35299;&#20915;&#28151;&#26434;&#22240;&#26524;&#21327;&#21516;&#36807;&#28388;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Deconfounded Causal Collaborative Filtering. (arXiv:2110.07122v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.07122
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#28151;&#26434;&#22240;&#32032;&#24433;&#21709;&#30340;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20986;&#28151;&#26434;&#22240;&#26524;&#21327;&#21516;&#36807;&#28388;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#20026;&#27599;&#20010;&#29305;&#23450;&#28151;&#26434;&#22240;&#32032;&#35774;&#35745;&#27169;&#22411;&#30340;&#19981;&#29616;&#23454;&#24615;&#20197;&#21450;&#28508;&#22312;&#28151;&#26434;&#22240;&#32032;&#26080;&#27861;&#35266;&#23519;&#30340;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#21487;&#33021;&#20250;&#21463;&#21040;&#21508;&#31181;&#28151;&#26434;&#22240;&#32032;&#30340;&#24178;&#25200;&#65292;&#23548;&#33268;&#25512;&#33616;&#32467;&#26524;&#19981;&#20934;&#30830;&#65292;&#24615;&#33021;&#19979;&#38477;&#12290;&#30446;&#21069;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#36890;&#24120;&#26159;&#20026;&#27599;&#20010;&#29305;&#23450;&#30340;&#28151;&#26434;&#22240;&#32032;&#35774;&#35745;&#29305;&#23450;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#20013;&#21487;&#33021;&#21253;&#21547;&#22823;&#37327;&#28151;&#26434;&#22240;&#32032;&#65292;&#22240;&#27492;&#20026;&#27599;&#20010;&#29305;&#23450;&#30340;&#28151;&#26434;&#22240;&#32032;&#35774;&#35745;&#29305;&#23450;&#30340;&#27169;&#22411;&#21487;&#33021;&#24182;&#19981;&#29616;&#23454;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#38500;&#20102;&#37027;&#20123;&#19987;&#23478;&#21487;&#20197;&#25163;&#21160;&#35782;&#21035;&#21644;&#22788;&#29702;&#30340;&#8220;&#26126;&#30830;&#28151;&#26434;&#22240;&#32032;&#8221;&#20043;&#22806;&#65292;&#36824;&#26377;&#35768;&#22810;&#8220;&#28508;&#22312;&#28151;&#26434;&#22240;&#32032;&#8221;&#36229;&#20986;&#20102;&#19987;&#23478;&#30340;&#24819;&#35937;&#21147;&#12290;&#20363;&#22914;&#65292;&#29992;&#25143;&#23545;&#19968;&#39318;&#27468;&#30340;&#35780;&#20998;&#21487;&#33021;&#21462;&#20915;&#20110;&#20182;&#20204;&#24403;&#26102;&#30340;&#24515;&#24773;&#25110;&#24403;&#21069;&#30340;&#22825;&#27668;&#24773;&#20917;&#65292;&#29992;&#25143;&#23545;&#20912;&#28103;&#28107;&#30340;&#20559;&#22909;&#21487;&#33021;&#21462;&#20915;&#20110;&#31354;&#27668;&#28201;&#24230;&#12290;&#36825;&#20123;&#28508;&#22312;&#28151;&#26434;&#22240;&#32032;&#22312;&#35760;&#24405;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#21487;&#33021;&#26159;&#19981;&#21487;&#35266;&#23519;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35299;&#28151;&#26434;&#22240;&#26524;&#21327;&#21516;&#36807;&#28388;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems may be confounded by various types of confounding factors (also called confounders) that may lead to inaccurate recommendations and sacrificed recommendation performance. Current approaches to solving the problem usually design each specific model for each specific confounder. However, real-world systems may include a huge number of confounders and thus designing each specific model for each specific confounder could be unrealistic. More importantly, except for those ``explicit confounders'' that experts can manually identify and process such as item's position in the ranking list, there are also many ``latent confounders'' that are beyond the imagination of experts. For example, users' rating on a song may depend on their current mood or the current weather, and users' preference on ice creams may depend on the air temperature. Such latent confounders may be unobservable in the recorded training data. To solve the problem, we propose Deconfounded Causal Collaborati
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#22914;&#20309;&#23558;&#20960;&#21313;&#24180;&#30340;&#28145;&#24230;&#23398;&#20064;&#12289;&#26799;&#24230;&#19979;&#38477;&#12289;&#21453;&#21521;&#20256;&#25773;&#21644;&#31070;&#32463;&#31185;&#23398;&#30740;&#31350;&#30340;&#32463;&#39564;&#25945;&#35757;&#24212;&#29992;&#20110;&#29983;&#29289;&#21487;&#34892;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#25506;&#32034;&#20102;&#23558;&#25968;&#25454;&#32534;&#30721;&#20026;&#33033;&#20914;&#21644;&#23398;&#20064;&#36807;&#31243;&#20043;&#38388;&#30340;&#24494;&#22937;&#30456;&#20114;&#20316;&#29992;&#20197;&#21450;&#29983;&#29289;&#21487;&#34892;&#30340;&#22312;&#32447;&#23398;&#20064;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2109.12894</link><description>&lt;p&gt;
&#20174;&#28145;&#24230;&#23398;&#20064;&#20013;&#27762;&#21462;&#32463;&#39564;&#25945;&#35757;&#65292;&#35757;&#32451;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Training Spiking Neural Networks Using Lessons From Deep Learning. (arXiv:2109.12894v5 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.12894
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#22914;&#20309;&#23558;&#20960;&#21313;&#24180;&#30340;&#28145;&#24230;&#23398;&#20064;&#12289;&#26799;&#24230;&#19979;&#38477;&#12289;&#21453;&#21521;&#20256;&#25773;&#21644;&#31070;&#32463;&#31185;&#23398;&#30740;&#31350;&#30340;&#32463;&#39564;&#25945;&#35757;&#24212;&#29992;&#20110;&#29983;&#29289;&#21487;&#34892;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#25506;&#32034;&#20102;&#23558;&#25968;&#25454;&#32534;&#30721;&#20026;&#33033;&#20914;&#21644;&#23398;&#20064;&#36807;&#31243;&#20043;&#38388;&#30340;&#24494;&#22937;&#30456;&#20114;&#20316;&#29992;&#20197;&#21450;&#29983;&#29289;&#21487;&#34892;&#30340;&#22312;&#32447;&#23398;&#20064;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#33041;&#26159;&#23547;&#25214;&#28789;&#24863;&#20197;&#24320;&#21457;&#26356;&#26377;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#23436;&#32654;&#20043;&#22320;&#12290;&#25105;&#20204;&#30340;&#31361;&#35302;&#21644;&#31070;&#32463;&#20803;&#30340;&#20869;&#37096;&#36816;&#20316;&#25552;&#20379;&#20102;&#23545;&#26410;&#26469;&#28145;&#24230;&#23398;&#20064;&#21487;&#33021;&#21576;&#29616;&#30340;&#26679;&#23376;&#30340;&#19968;&#30629;&#12290;&#26412;&#25991;&#26088;&#22312;&#20171;&#32461;&#22914;&#20309;&#23558;&#20960;&#21313;&#24180;&#30340;&#28145;&#24230;&#23398;&#20064;&#12289;&#26799;&#24230;&#19979;&#38477;&#12289;&#21453;&#21521;&#20256;&#25773;&#21644;&#31070;&#32463;&#31185;&#23398;&#30740;&#31350;&#30340;&#32463;&#39564;&#25945;&#35757;&#24212;&#29992;&#20110;&#29983;&#29289;&#21487;&#34892;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#23558;&#25968;&#25454;&#32534;&#30721;&#20026;&#33033;&#20914;&#21644;&#23398;&#20064;&#36807;&#31243;&#20043;&#38388;&#24494;&#22937;&#30340;&#30456;&#20114;&#20316;&#29992;&#65307;&#23558;&#22522;&#20110;&#26799;&#24230;&#30340;&#23398;&#20064;&#24212;&#29992;&#21040;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476; (SNNs) &#30340;&#25361;&#25112;&#21644;&#35299;&#20915;&#26041;&#26696;&#65307;&#26102;&#38388;&#21453;&#21521;&#20256;&#25773;&#21644;&#33033;&#20914;&#26102;&#24207;&#20381;&#36182;&#24615;&#21487;&#22609;&#24615;&#20043;&#38388;&#24494;&#22937;&#30340;&#32852;&#31995;&#20197;&#21450;&#28145;&#24230;&#23398;&#20064;&#22914;&#20309;&#21521;&#30528;&#29983;&#29289;&#21487;&#34892;&#30340;&#22312;&#32447;&#23398;&#20064;&#21457;&#23637;&#12290;&#19968;&#20123;&#24819;&#27861;&#22312;&#31070;&#32463;&#24418;&#24577;&#24037;&#31243;&#30028;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#35748;&#21487;&#21644;&#20351;&#29992;&#65292;&#32780;&#20854;&#20182;&#19968;&#20123;&#22312;&#26412;&#25991;&#20013;&#39318;&#27425;&#21576;&#29616;&#25110;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;SNN&#39046;&#22495;&#20173;&#22788;&#20110;&#24320;&#21457;&#21021;&#26399;&#65292;&#38656;&#35201;&#35299;&#20915;&#35768;&#22810;&#25361;&#25112;&#21644;&#38480;&#21046;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#25105;&#20204;&#35748;&#20026;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#26159;&#29702;&#35299;&#22823;&#33041;&#21644;&#24320;&#21457;&#26356;&#26377;&#25928;&#21644;&#26234;&#33021;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
The brain is the perfect place to look for inspiration to develop more efficient neural networks. The inner workings of our synapses and neurons provide a glimpse at what the future of deep learning might look like. This paper serves as a tutorial and perspective showing how to apply the lessons learnt from several decades of research in deep learning, gradient descent, backpropagation and neuroscience to biologically plausible spiking neural neural networks.  We also explore the delicate interplay between encoding data as spikes and the learning process; the challenges and solutions of applying gradient-based learning to spiking neural networks (SNNs); the subtle link between temporal backpropagation and spike timing dependent plasticity, and how deep learning might move towards biologically plausible online learning. Some ideas are well accepted and commonly used amongst the neuromorphic engineering community, while others are presented or justified for the first time here.  The fiel
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;&#19968;&#20010;&#28151;&#21512;&#30340;&#37327;&#23376;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21033;&#29992;&#28145;&#24230;&#29983;&#25104;&#21270;&#23398;&#27169;&#22411;&#21152;&#36895;&#33647;&#29289;&#21457;&#29616;&#12290;&#36890;&#36807;&#22312;D-Wave&#37327;&#23376;&#36864;&#28779;&#22120;&#19978;&#35757;&#32451;&#65292;&#25104;&#21151;&#29983;&#25104;&#20102;&#20855;&#26377;&#33647;&#29289;&#21270;&#23398;&#21644;&#21512;&#25104;&#21487;&#21450;&#24615;&#29305;&#24615;&#30340;2331&#20010;&#26032;&#21270;&#23398;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2108.11644</link><description>&lt;p&gt;
&#28151;&#21512;&#37327;&#23376;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#29992;&#20110;&#29983;&#25104;&#21270;&#23398;&#21644;&#33647;&#29289;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Hybrid quantum-classical machine learning for generative chemistry and drug design. (arXiv:2108.11644v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.11644
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;&#19968;&#20010;&#28151;&#21512;&#30340;&#37327;&#23376;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21033;&#29992;&#28145;&#24230;&#29983;&#25104;&#21270;&#23398;&#27169;&#22411;&#21152;&#36895;&#33647;&#29289;&#21457;&#29616;&#12290;&#36890;&#36807;&#22312;D-Wave&#37327;&#23376;&#36864;&#28779;&#22120;&#19978;&#35757;&#32451;&#65292;&#25104;&#21151;&#29983;&#25104;&#20102;&#20855;&#26377;&#33647;&#29289;&#21270;&#23398;&#21644;&#21512;&#25104;&#21487;&#21450;&#24615;&#29305;&#24615;&#30340;2331&#20010;&#26032;&#21270;&#23398;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#21270;&#23398;&#27169;&#22411;&#25104;&#20026;&#21152;&#36895;&#33647;&#29289;&#21457;&#29616;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#25152;&#26377;&#21487;&#33021;&#30340;&#33647;&#29289;&#20998;&#23376;&#30340;&#32467;&#26500;&#31354;&#38388;&#30340;&#24040;&#22823;&#22823;&#23567;&#21644;&#22797;&#26434;&#24615;&#26500;&#25104;&#20102;&#37325;&#22823;&#30340;&#38556;&#30861;&#65292;&#21487;&#20197;&#36890;&#36807;&#23558;&#37327;&#23376;&#35745;&#31639;&#26426;&#19982;&#28145;&#24230;&#32463;&#20856;&#32593;&#32476;&#30456;&#32467;&#21512;&#26469;&#20811;&#26381;&#12290;&#20316;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#30340;&#31532;&#19968;&#27493;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#32039;&#20945;&#30340;&#31163;&#25955;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;DVAE&#65289;&#65292;&#20854;&#20013;&#28508;&#22312;&#23618;&#20013;&#30340;&#21463;&#38480;&#29627;&#23572;&#20857;&#26364;&#26426;&#65288;RBM&#65289;&#30340;&#22823;&#23567;&#34987;&#20943;&#23567;&#12290;&#25311;&#35758;&#27169;&#22411;&#30340;&#22823;&#23567;&#36275;&#22815;&#23567;&#65292;&#21487;&#20197;&#36866;&#24212;&#26368;&#20808;&#36827;&#30340;D-Wave&#37327;&#23376;&#36864;&#28779;&#22120;&#65292;&#24182;&#20801;&#35768;&#22312;ChEMBL&#29983;&#29289;&#27963;&#24615;&#21270;&#21512;&#29289;&#25968;&#25454;&#38598;&#23376;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;2331&#20010;&#20855;&#26377;&#33647;&#29289;&#21270;&#23398;&#21644;&#21512;&#25104;&#21487;&#21450;&#24615;&#29305;&#24615;&#30340;&#26032;&#21270;&#23398;&#32467;&#26500;&#65292;&#20854;&#33539;&#22260;&#19982;&#26469;&#33258;ChEMBL&#30340;&#20998;&#23376;&#31867;&#20284;&#12290;&#25152;&#21576;&#29616;&#30340;&#32467;&#26524;&#35777;&#26126;&#20102;&#20351;&#29992;&#24050;&#32463;&#23384;&#22312;&#25110;&#21363;&#23558;&#25512;&#20986;&#30340;&#37327;&#23376;&#35745;&#31639;&#35774;&#22791;&#20316;&#20026;&#26410;&#26469;&#27979;&#35797;&#24179;&#21488;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep generative chemistry models emerge as powerful tools to expedite drug discovery. However, the immense size and complexity of the structural space of all possible drug-like molecules pose significant obstacles, which could be overcome with hybrid architectures combining quantum computers with deep classical networks. As the first step toward this goal, we built a compact discrete variational autoencoder (DVAE) with a Restricted Boltzmann Machine (RBM) of reduced size in its latent layer. The size of the proposed model was small enough to fit on a state-of-the-art D-Wave quantum annealer and allowed training on a subset of the ChEMBL dataset of biologically active compounds. Finally, we generated 2331 novel chemical structures with medicinal chemistry and synthetic accessibility properties in the ranges typical for molecules from ChEMBL. The presented results demonstrate the feasibility of using already existing or soon-to-be-available quantum computing devices as testbeds for futur
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#32972;&#26223;&#19979;&#20272;&#35745;&#31163;&#25955;&#20998;&#24067;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#30340;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#20114;&#21160;&#21644;&#38750;&#20114;&#21160;&#30340;&#38544;&#31169;&#26426;&#21046;&#12290;&#36890;&#36807;&#23545;&#24130;&#21644;&#20989;&#25968;&#30340;&#20108;&#27425;&#39118;&#38505;&#34892;&#20026;&#30340;&#30740;&#31350;&#65292;&#22312;&#38750;&#20114;&#21160;&#24773;&#20917;&#19979;&#25552;&#20986;&#20102;&#20004;&#31181;&#25554;&#34917;&#31867;&#22411;&#30340;&#20272;&#35745;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2107.03940</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#31163;&#25955;&#20998;&#24067;&#30340;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Locally differentially private estimation of nonlinear functionals of discrete distributions. (arXiv:2107.03940v2 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.03940
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#32972;&#26223;&#19979;&#20272;&#35745;&#31163;&#25955;&#20998;&#24067;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#30340;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#20114;&#21160;&#21644;&#38750;&#20114;&#21160;&#30340;&#38544;&#31169;&#26426;&#21046;&#12290;&#36890;&#36807;&#23545;&#24130;&#21644;&#20989;&#25968;&#30340;&#20108;&#27425;&#39118;&#38505;&#34892;&#20026;&#30340;&#30740;&#31350;&#65292;&#22312;&#38750;&#20114;&#21160;&#24773;&#20917;&#19979;&#25552;&#20986;&#20102;&#20004;&#31181;&#25554;&#34917;&#31867;&#22411;&#30340;&#20272;&#35745;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#30340;&#32972;&#26223;&#19979;&#20272;&#35745;&#31163;&#25955;&#20998;&#24067;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#30340;&#38382;&#39064;&#12290;&#21021;&#22987;&#25968;&#25454;$x_1,\ldots,x_n \in [K]$&#34987;&#20551;&#35774;&#20026;i.i.d.&#24182;&#26681;&#25454;&#26410;&#30693;&#30340;&#31163;&#25955;&#20998;&#24067;$p = (p_1,\ldots,p_K)$&#36827;&#34892;&#20998;&#24067;&#12290;&#21482;&#26377;$\alpha$-&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;(LDP)&#26679;&#26412;$z_1,...,z_n$&#26159;&#20844;&#24320;&#21487;&#29992;&#30340;&#65292;&#20854;&#20013;&#26415;&#35821;&#8220;&#23616;&#37096;&#8221;&#34920;&#31034;&#27599;&#20010;$z_i$&#37117;&#26159;&#20351;&#29992;&#19968;&#20010;&#20010;&#20154;&#23646;&#24615;$x_i$&#29983;&#25104;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#20351;&#29992;&#20114;&#21160;&#30340;&#38544;&#31169;&#26426;&#21046;&#65288;PM&#65289;&#25110;&#38750;&#20114;&#21160;&#30340;&#38544;&#31169;&#26426;&#21046;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#20316;&#20026;$K, \, n$&#21644;$\alpha$&#30340;&#20989;&#25968;&#30340;&#24130;&#21644;&#20989;&#25968;$F_{\gamma} = \sum_{k=1}^K p_k^{\gamma}$&#30340;&#20108;&#27425;&#39118;&#38505;&#30340;&#34892;&#20026;&#12290;&#22312;&#38750;&#20114;&#21160;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#25554;&#34917;&#31867;&#22411;&#30340;&#20272;&#35745;$F_{\gamma}$&#30340;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#25152;&#26377;$\gamma &gt;0$&#65292;&#20854;&#31867;&#20284;&#20110;Jiao&#31561;(2017)&#22312;&#22810;&#39033;&#24335;&#27169;&#22411;&#20013;&#20998;&#26512;&#30340;MLE&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of estimating non-linear functionals of discrete distributions in the context of local differential privacy. The initial data $x_1,\ldots,x_n \in [K]$ are supposed i.i.d. and distributed according to an unknown discrete distribution $p = (p_1,\ldots,p_K)$. Only $\alpha$-locally differentially private (LDP) samples $z_1,...,z_n$ are publicly available, where the term 'local' means that each $z_i$ is produced using one individual attribute $x_i$. We exhibit privacy mechanisms (PM) that are interactive (i.e. they are allowed to use already published confidential data) or non-interactive. We describe the behavior of the quadratic risk for estimating the power sum functional $F_{\gamma} = \sum_{k=1}^K p_k^{\gamma}$, $\gamma &gt;0$ as a function of $K, \, n$ and $\alpha$. In the non-interactive case, we study two plug-in type estimators of $F_{\gamma}$, for all $\gamma &gt;0$, that are similar to the MLE analyzed by Jiao et al. (2017) in the multinomial model. However, due to 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#33258;&#36866;&#24212;&#28388;&#27874;&#22120;&#22312;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#25506;&#32034;&#21644;&#21033;&#29992;&#22270;&#32467;&#26500;&#30340;&#28789;&#27963;&#24615;&#65292;&#25552;&#39640;&#20102;&#32593;&#32476;&#22788;&#29702;&#22270;&#25968;&#25454;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2105.10377</link><description>&lt;p&gt;
&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#33258;&#36866;&#24212;&#28388;&#27874;&#22120;
&lt;/p&gt;
&lt;p&gt;
Adaptive Filters in Graph Convolutional Neural Networks. (arXiv:2105.10377v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.10377
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#33258;&#36866;&#24212;&#28388;&#27874;&#22120;&#22312;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#25506;&#32034;&#21644;&#21033;&#29992;&#22270;&#32467;&#26500;&#30340;&#28789;&#27963;&#24615;&#65292;&#25552;&#39640;&#20102;&#32593;&#32476;&#22788;&#29702;&#22270;&#25968;&#25454;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#20013;&#65292;&#25105;&#20204;&#30446;&#30585;&#20102;&#38750;&#27431;&#20960;&#37324;&#24471;&#39046;&#22495;&#20135;&#29983;&#30340;&#36234;&#26469;&#36234;&#22810;&#30340;&#25968;&#25454;&#21487;&#29992;&#24615;&#65292;&#36825;&#20123;&#39046;&#22495;&#36890;&#24120;&#20197;&#22797;&#26434;&#20851;&#31995;&#34920;&#31034;&#20026;&#22270;&#24418;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476; (GNN) &#30001;&#20110;&#22312;&#22788;&#29702;&#22270;&#32467;&#26500;&#25968;&#25454;&#26102;&#30340;&#28508;&#21147;&#32780;&#24341;&#36215;&#20102;&#39640;&#24230;&#20851;&#27880;&#12290;&#29305;&#21035;&#22320;&#65292;&#20154;&#20204;&#23545;&#20351;&#29992; GNN &#26550;&#26500;&#30340;&#25193;&#23637;&#26469;&#22312;&#22270;&#19978;&#36827;&#34892;&#21367;&#31215;&#30340;&#21487;&#33021;&#24615;&#38750;&#24120;&#24863;&#20852;&#36259;&#65292;&#36825;&#20010;&#26550;&#26500;&#36890;&#24120;&#34987;&#31216;&#20026;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476; (ConvGNN)&#12290;&#22312;&#22270;&#19978;&#30340;&#21367;&#31215;&#20027;&#35201;&#20998;&#20026;&#39057;&#35889;&#21367;&#31215;&#21644;&#31354;&#38388;&#21367;&#31215;&#20004;&#31181;&#24418;&#24335;&#12290;&#30001;&#20110;&#22312;&#25506;&#32034;&#21644;&#21033;&#29992;&#25968;&#25454;&#30340;&#22270;&#32467;&#26500;&#26041;&#38754;&#26356;&#21152;&#28789;&#27963;&#65292;&#26368;&#36817;&#23545;&#20110;&#30740;&#31350;&#31354;&#38388;&#26041;&#27861;&#25152;&#33021;&#25552;&#20379;&#30340;&#21487;&#33021;&#24615;&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#12290;&#25214;&#21040;&#19968;&#31181;&#36866;&#24212;&#22788;&#29702;&#36755;&#20837;&#30340;&#32593;&#32476;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#20197;&#26368;&#22823;&#21270;&#24635;&#20307;&#24615;&#33021;&#30340;&#24819;&#27861;&#65292;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20852;&#36259;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the last few years, we have witnessed the availability of an increasing data generated from non-Euclidean domains, which are usually represented as graphs with complex relationships, and Graph Neural Networks (GNN) have gained a high interest because of their potential in processing graph-structured data. In particular, there is a strong interest in exploring the possibilities in performing convolution on graphs using an extension of the GNN architecture, generally referred to as Graph Convolutional Neural Networks (ConvGNN). Convolution on graphs has been achieved mainly in two forms: spectral and spatial convolutions. Due to the higher flexibility in exploring and exploiting the graph structure of data, there is recently an increasing interest in investigating the possibilities that the spatial approach can offer. The idea of finding a way to adapt the network behaviour to the inputs they process to maximize the total performances has aroused much interest in the neural networks
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#20998;&#21035;&#27604;&#36739;&#27491;&#26679;&#26412;&#21644;&#36127;&#26679;&#26412;&#22312;&#21508;&#33258;&#32676;&#32452;&#20869;&#30340;&#20851;&#31995;&#65292;&#24182;&#23545;&#27491;&#36127;&#32676;&#32452;&#20043;&#38388;&#36827;&#34892;&#23545;&#27604;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2105.03746</link><description>&lt;p&gt;
&#23545;&#27604;&#21560;&#24341;&#21644;&#23545;&#27604;&#25490;&#26021;&#22312;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Contrastive Attraction and Contrastive Repulsion for Representation Learning. (arXiv:2105.03746v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.03746
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#20998;&#21035;&#27604;&#36739;&#27491;&#26679;&#26412;&#21644;&#36127;&#26679;&#26412;&#22312;&#21508;&#33258;&#32676;&#32452;&#20869;&#30340;&#20851;&#31995;&#65292;&#24182;&#23545;&#27491;&#36127;&#32676;&#32452;&#20043;&#38388;&#36827;&#34892;&#23545;&#27604;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#20197;&#33258;&#25105;&#30417;&#30563;&#26041;&#24335;&#26377;&#25928;&#22320;&#23398;&#20064;&#25968;&#25454;&#34920;&#31034;&#65292;&#20854;&#20013;&#32534;&#30721;&#22120;&#36890;&#36807;&#19968;&#23545;&#22810;&#30340;softmax&#20132;&#21449;&#29109;&#25439;&#22833;&#23558;&#27599;&#20010;&#27491;&#26679;&#26412;&#19982;&#22810;&#20010;&#36127;&#26679;&#26412;&#36827;&#34892;&#23545;&#27604;&#12290;&#26368;&#36817;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;&#22270;&#20687;&#25968;&#25454;&#65292;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#22914;ImageNet&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#35748;&#20026;&#26469;&#33258;&#21516;&#19968;&#23454;&#20363;&#30340;&#22686;&#24378;&#35270;&#22270;&#26159;&#27491;&#26679;&#26412;&#23545;&#65292;&#32780;&#26469;&#33258;&#20854;&#20182;&#23454;&#20363;&#30340;&#35270;&#22270;&#21017;&#26159;&#36127;&#26679;&#26412;&#23545;&#12290;&#36825;&#31181;&#20108;&#20998;&#27861;&#19981;&#20805;&#20998;&#22320;&#32771;&#34385;&#26679;&#26412;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#19988;&#22312;&#24212;&#29992;&#21040;&#37326;&#22806;&#22270;&#20687;&#26102;&#24448;&#24448;&#20250;&#20135;&#29983;&#36739;&#24046;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#23545;&#27604;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#24182;&#22686;&#24378;&#20854;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65292;&#20998;&#21035;&#22312;&#21508;&#33258;&#30340;&#32676;&#32452;&#20869;&#27604;&#36739;&#27491;&#26679;&#26412;&#21644;&#36127;&#26679;&#26412;&#65292;&#28982;&#21518;&#36827;&#34892;&#27491;&#36127;&#32676;&#32452;&#20043;&#38388;&#30340;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning (CL) methods effectively learn data representations in a self-supervision manner, where the encoder contrasts each positive sample over multiple negative samples via a one-vs-many softmax cross-entropy loss. By leveraging large amounts of unlabeled image data, recent CL methods have achieved promising results when pretrained on large-scale datasets, such as ImageNet. However, most of them consider the augmented views from the same instance are positive pairs, while views from other instances are negative ones. Such binary partition insufficiently considers the relation between samples and tends to yield worse performance when generalized on images in the wild. In this paper, to further improve the performance of CL and enhance its robustness on various datasets, {we propose a doubly CL strategy that separately compares positive and negative samples within their own groups, and then proceeds with a contrast between positive and negative groups}. We realize this stra
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22240;&#26524;&#21327;&#21516;&#36807;&#28388;&#65288;CCF&#65289;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#21327;&#21516;&#36807;&#28388;&#21644;&#25512;&#33616;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#32431;&#31929;&#30456;&#20851;&#23398;&#20064;&#23548;&#33268;&#30340;&#39044;&#27979;&#20013;&#30340;&#36763;&#26222;&#26862;&#24726;&#35770;&#38382;&#39064;&#65292;&#25552;&#39640;&#25512;&#33616;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2102.01868</link><description>&lt;p&gt;
&#22240;&#26524;&#21327;&#21516;&#36807;&#28388;
&lt;/p&gt;
&lt;p&gt;
Causal Collaborative Filtering. (arXiv:2102.01868v5 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.01868
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22240;&#26524;&#21327;&#21516;&#36807;&#28388;&#65288;CCF&#65289;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#21327;&#21516;&#36807;&#28388;&#21644;&#25512;&#33616;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#32431;&#31929;&#30456;&#20851;&#23398;&#20064;&#23548;&#33268;&#30340;&#39044;&#27979;&#20013;&#30340;&#36763;&#26222;&#26862;&#24726;&#35770;&#38382;&#39064;&#65292;&#25552;&#39640;&#25512;&#33616;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#20256;&#32479;&#30340;&#25512;&#33616;&#31639;&#27861;&#22522;&#20110;&#20174;&#25968;&#25454;&#20013;&#25366;&#25496;&#25110;&#23398;&#20064;&#30456;&#20851;&#27169;&#24335;&#26469;&#20272;&#35745;&#29992;&#25143;-&#39033;&#30446;&#30340;&#30456;&#20851;&#20559;&#22909;&#30340;&#22522;&#26412;&#24605;&#24819;&#35774;&#35745;&#12290;&#28982;&#32780;&#65292;&#32431;&#31929;&#30340;&#30456;&#20851;&#23398;&#20064;&#21487;&#33021;&#23548;&#33268;&#39044;&#27979;&#20013;&#30340;&#36763;&#26222;&#26862;&#24726;&#35770;&#65292;&#20174;&#32780;&#23548;&#33268;&#25512;&#33616;&#24615;&#33021;&#30340;&#25439;&#22833;&#12290;&#36763;&#26222;&#26862;&#24726;&#35770;&#26159;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#32479;&#35745;&#29616;&#35937;&#65292;&#23427;&#20250;&#23548;&#33268;&#32479;&#35745;&#32467;&#35770;&#30340;&#28151;&#28102;&#65292;&#24573;&#35270;&#36825;&#20010;&#24726;&#35770;&#21487;&#33021;&#23548;&#33268;&#19981;&#20934;&#30830;&#30340;&#20915;&#31574;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#22240;&#26524;&#21644;&#21453;&#20107;&#23454;&#24314;&#27169;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#36229;&#36234;&#35266;&#23519;&#25968;&#25454;&#36827;&#34892;&#29992;&#25143;&#24314;&#27169;&#21644;&#20010;&#24615;&#21270;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22240;&#26524;&#21327;&#21516;&#36807;&#28388;&#65288;CCF&#65289;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#21327;&#21516;&#36807;&#28388;&#21644;&#25512;&#33616;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#22240;&#26524;&#35270;&#22270;&#65292;&#24182;&#20174;&#25968;&#23398;&#19978;&#35777;&#26126;&#20102;&#35768;&#22810;&#20256;&#32479;&#30340;&#21327;&#21516;&#36807;&#28388;&#31639;&#27861;&#23454;&#38469;&#19978;&#26159;&#31616;&#21270;&#22240;&#26524;&#22270;&#19979;CCF&#30340;&#29305;&#27530;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many of the traditional recommendation algorithms are designed based on the fundamental idea of mining or learning correlative patterns from data to estimate the user-item correlative preference. However, pure correlative learning may lead to Simpson's paradox in predictions, and thus results in sacrificed recommendation performance. Simpson's paradox is a well-known statistical phenomenon, which causes confusions in statistical conclusions and ignoring the paradox may result in inaccurate decisions. Fortunately, causal and counterfactual modeling can help us to think outside of the observational data for user modeling and personalization so as to tackle such issues. In this paper, we propose Causal Collaborative Filtering (CCF) -- a general framework for modeling causality in collaborative filtering and recommendation. We provide a unified causal view of CF and mathematically show that many of the traditional CF algorithms are actually special cases of CCF under simplified causal grap
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#24102;&#22122;&#22768;&#30340;&#20302;&#31209;&#30697;&#38453;&#34917;&#20840;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#36890;&#36807;&#20351;&#29992;&#26464;&#26438;&#24471;&#20998;&#26469;&#37327;&#21270;&#27599;&#20010;&#20803;&#32032;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#38750;&#22343;&#21248;&#37319;&#26679;&#36807;&#31243;&#26469;&#26356;&#20934;&#30830;&#22320;&#23436;&#25104;&#34917;&#20840;&#12290;</title><link>http://arxiv.org/abs/2011.05885</link><description>&lt;p&gt;
&#24102;&#22122;&#22768;&#30340;&#26464;&#26438;&#30697;&#38453;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Leveraged Matrix Completion with Noise. (arXiv:2011.05885v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2011.05885
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#24102;&#22122;&#22768;&#30340;&#20302;&#31209;&#30697;&#38453;&#34917;&#20840;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#36890;&#36807;&#20351;&#29992;&#26464;&#26438;&#24471;&#20998;&#26469;&#37327;&#21270;&#27599;&#20010;&#20803;&#32032;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#38750;&#22343;&#21248;&#37319;&#26679;&#36807;&#31243;&#26469;&#26356;&#20934;&#30830;&#22320;&#23436;&#25104;&#34917;&#20840;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#21313;&#24180;&#20013;&#65292;&#20174;&#23376;&#37319;&#26679;&#27979;&#37327;&#20013;&#23436;&#25104;&#20302;&#31209;&#30697;&#38453;&#24341;&#36215;&#20102;&#24456;&#22810;&#20851;&#27880;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20026;&#20102;&#22312;&#19968;&#23450;&#30340;&#27010;&#29575;&#19979;&#29702;&#35770;&#19978;&#30830;&#20445;&#23436;&#25104;&#19968;&#20010;$n \times n$&#30340;&#31209;&#20026;$r$&#30340;&#24102;&#22122;&#22768;&#30697;&#38453;&#65292;&#38656;&#35201;$\mathcal{O}(nr\log^2(n))$&#20010;&#25968;&#25454;&#65292;&#20294;&#26159;&#38656;&#35201;&#19968;&#20123;&#30456;&#24403;&#33499;&#21051;&#30340;&#20551;&#35774;&#65306;(1)&#25152;&#21547;&#30340;&#30697;&#38453;&#24517;&#39035;&#26159;&#19981;&#30456;&#20851;&#30340;&#65307;(2)&#35266;&#27979;&#20540;&#26381;&#20174;&#22343;&#21248;&#20998;&#24067;&#12290;&#36825;&#31181;&#38480;&#21046;&#24615;&#37096;&#20998;&#26159;&#30001;&#20110;&#24573;&#30053;&#20102;&#26464;&#26438;&#24471;&#20998;&#21644;&#27599;&#20010;&#20803;&#32032;&#30340;&#39044;&#27979;&#20449;&#24687;&#30340;&#20316;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26464;&#26438;&#24471;&#20998;&#26469;&#25551;&#36848;&#27599;&#20010;&#20803;&#32032;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#26174;&#33879;&#25918;&#23485;&#20102;&#20551;&#35774;&#65306;(1)&#23545;&#24213;&#23618;&#20302;&#31209;&#30697;&#38453;&#19981;&#26045;&#21152;&#20219;&#20309;&#20854;&#20182;&#32467;&#26500;&#20551;&#35774;&#65307;(2)&#34987;&#35266;&#27979;&#21040;&#30340;&#20803;&#32032;&#36890;&#36807;&#26464;&#26438;&#24471;&#20998;&#36866;&#24403;&#22320;&#20381;&#36182;&#20110;&#23427;&#20204;&#30340;&#37325;&#35201;&#24615;&#12290;&#22312;&#36825;&#20123;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#38750;&#22343;&#21248;/&#20559;&#20506;&#30340;&#37319;&#26679;&#36807;&#31243;&#65292;&#21487;&#20197;&#25581;&#31034;&#8220;&#37325;&#35201;&#8221;&#30340;&#20803;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Completing low-rank matrices from subsampled measurements has received much attention in the past decade. Existing works indicate that $\mathcal{O}(nr\log^2(n))$ datums are required to theoretically secure the completion of an $n \times n$ noisy matrix of rank $r$ with high probability, under some quite restrictive assumptions: (1) the underlying matrix must be incoherent; (2) observations follow the uniform distribution. The restrictiveness is partially due to ignoring the roles of the leverage score and the oracle information of each element. In this paper, we employ the leverage scores to characterize the importance of each element and significantly relax assumptions to: (1) not any other structure assumptions are imposed on the underlying low-rank matrix; (2) elements being observed are appropriately dependent on their importance via the leverage score. Under these assumptions, instead of uniform sampling, we devise an ununiform/biased sampling procedure that can reveal the ``impor
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#21644;&#20998;&#26512;&#39550;&#39542;&#22330;&#26223;&#36712;&#36857;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#22788;&#29702;&#19981;&#21516;&#38271;&#24230;&#30340;&#36712;&#36857;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2007.14524</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#29983;&#25104;&#21644;&#20998;&#26512;&#39550;&#39542;&#22330;&#26223;&#36712;&#36857;
&lt;/p&gt;
&lt;p&gt;
A Deep Learning Framework for Generation and Analysis of Driving Scenario Trajectories. (arXiv:2007.14524v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2007.14524
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#21644;&#20998;&#26512;&#39550;&#39542;&#22330;&#26223;&#36712;&#36857;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#22788;&#29702;&#19981;&#21516;&#38271;&#24230;&#30340;&#36712;&#36857;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#21644;&#20998;&#26512;&#39550;&#39542;&#22330;&#26223;&#36712;&#36857;&#65292;&#24182;&#20197;&#19968;&#31181;&#21407;&#21017;&#24615;&#30340;&#26041;&#24335;&#39564;&#35777;&#20854;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#23545;&#19981;&#21516;&#38271;&#24230;&#30340;&#36712;&#36857;&#36827;&#34892;&#24314;&#27169;&#21644;&#29983;&#25104;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#31181;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#36712;&#36857;&#38271;&#24230;&#30340;&#26465;&#20214;&#19979;&#26469;&#35843;&#25972;&#24490;&#29615;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;RC-GAN&#65289;&#65292;&#20174;&#32780;&#33021;&#22815;&#29983;&#25104;&#21487;&#21464;&#38271;&#24230;&#30340;&#39550;&#39542;&#36712;&#36857;&#65292;&#36825;&#26159;&#33258;&#20027;&#39550;&#39542;&#39564;&#35777;&#20013;&#29983;&#25104;&#27979;&#35797;&#26696;&#20363;&#30340;&#29702;&#24819;&#29305;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#22522;&#20110;&#24490;&#29615;&#33258;&#32534;&#30721;&#22120;&#19982;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#26550;&#26500;&#26469;&#35299;&#20915;&#21487;&#21464;&#38271;&#24230;&#30340;&#38382;&#39064;&#65292;&#22312;&#36825;&#20010;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#35757;&#32451;&#19968;&#20010;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#26469;&#23398;&#20064;/&#29983;&#25104;&#21407;&#22987;&#36712;&#36857;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#35757;&#32451;&#19968;&#20010;&#38598;&#25104;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#26469;&#20272;&#35745;&#36712;&#36857;&#30340;&#38271;&#24230;&#65292;&#20174;&#32780;&#33021;&#22815;&#23558;&#20854;&#20174;&#28508;&#22312;&#31354;&#38388;&#34920;&#31034;&#20013;&#36824;&#21407;&#20986;&#26469;&#12290;&#38500;&#20102;&#36712;&#36857;&#29983;&#25104;&#22806;
&lt;/p&gt;
&lt;p&gt;
We propose a unified deep learning framework for the generation and analysis of driving scenario trajectories, and validate its effectiveness in a principled way. To model and generate scenarios of trajectories with different lengths, we develop two approaches. First, we adapt the Recurrent Conditional Generative Adversarial Networks (RC-GAN) by conditioning on the length of the trajectories. This provides us the flexibility to generate variable-length driving trajectories, a desirable feature for scenario test case generation in the verification of autonomous driving. Second, we develop an architecture based on Recurrent Autoencoder with GANs to obviate the variable length issue, wherein we train a GAN to learn/generate the latent representations of original trajectories. In this approach, we train an integrated feed-forward neural network to estimate the length of the trajectories to be able to bring them back from the latent space representation. In addition to trajectory generation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#20351;&#29992;&#23398;&#20064;&#31639;&#27861;&#36817;&#20046;&#26368;&#20248;&#22320;&#35299;&#20915;&#20855;&#26377;&#26102;&#38388;&#30456;&#20851;&#22870;&#21169;&#30340;&#22810;&#26234;&#33021;&#20307;&#12289;&#22810;&#20219;&#21153;&#30340;NP-hard&#35268;&#21010;&#38382;&#39064;&#30340;&#21487;&#33021;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#23637;&#31034;&#20102;&#25552;&#20986;&#26041;&#27861;&#22312;&#35299;&#20915;&#26426;&#22120;&#20154;/&#26426;&#22120;&#35843;&#24230;&#38382;&#39064;&#19978;&#30340;&#36817;&#20046;&#26368;&#20248;&#24615;&#12290;</title><link>http://arxiv.org/abs/1905.12204</link><description>&lt;p&gt;
&#20351;&#29992;GNN&#23398;&#20064;NP-Hard&#22810;&#26234;&#33021;&#20307;&#20998;&#37197;&#35268;&#21010;&#65306;&#22312;&#38543;&#26426;&#22270;&#19978;&#30340;&#25512;&#29702;&#21644;&#21487;&#35777;&#26126;&#30340;&#25293;&#21334;&#36866;&#37197;Q&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning NP-Hard Multi-Agent Assignment Planning using GNN: Inference on a Random Graph and Provable Auction-Fitted Q-learning. (arXiv:1905.12204v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1905.12204
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#20351;&#29992;&#23398;&#20064;&#31639;&#27861;&#36817;&#20046;&#26368;&#20248;&#22320;&#35299;&#20915;&#20855;&#26377;&#26102;&#38388;&#30456;&#20851;&#22870;&#21169;&#30340;&#22810;&#26234;&#33021;&#20307;&#12289;&#22810;&#20219;&#21153;&#30340;NP-hard&#35268;&#21010;&#38382;&#39064;&#30340;&#21487;&#33021;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#23637;&#31034;&#20102;&#25552;&#20986;&#26041;&#27861;&#22312;&#35299;&#20915;&#26426;&#22120;&#20154;/&#26426;&#22120;&#35843;&#24230;&#38382;&#39064;&#19978;&#30340;&#36817;&#20046;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22522;&#20110;&#23398;&#20064;&#30340;&#31639;&#27861;&#26469;&#36817;&#20046;&#26368;&#20248;&#22320;&#35299;&#20915;&#20855;&#26377;&#26102;&#38388;&#30456;&#20851;&#22870;&#21169;&#30340;&#22810;&#26234;&#33021;&#20307;&#12289;&#22810;&#20219;&#21153;&#30340;NP-hard&#35268;&#21010;&#38382;&#39064;&#30340;&#21487;&#33021;&#24615;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31867;&#31216;&#20026;&#22810;&#26426;&#22120;&#20154;&#22870;&#21169;&#25910;&#38598;&#38382;&#39064;&#65288;MRRC&#65289;&#30340;&#26426;&#22120;&#20154;/&#26426;&#22120;&#35843;&#24230;&#38382;&#39064;&#12290;&#36825;&#20123;MRRC&#38382;&#39064;&#24456;&#22909;&#22320;&#27169;&#25311;&#20102;&#20849;&#20056;&#12289;&#21462;&#36865;&#21644;&#20854;&#20182;&#30456;&#20851;&#38382;&#39064;&#12290;&#22312;&#23558;MRRC&#38382;&#39064;&#34920;&#31034;&#20026;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#26102;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#27599;&#20010;&#29366;&#24577;&#21487;&#20197;&#34987;&#34920;&#31034;&#20026;&#27010;&#29575;&#22270;&#27169;&#22411;&#65288;PGM&#65289;&#30340;&#25193;&#23637;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#38543;&#26426;PGMs&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20026;&#38543;&#26426;PGMs&#24320;&#21457;&#20102;&#19968;&#31181;&#22343;&#22330;&#25512;&#29702;&#26041;&#27861;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#65288;1&#65289;&#19968;&#20010;&#21487;&#36827;&#34892;&#39034;&#24207;&#36716;&#31227;&#30340;Q&#20989;&#25968;&#20272;&#35745;&#22120;&#21644;&#65288;2&#65289;&#19968;&#20010;&#25903;&#25345;&#39034;&#24207;&#36716;&#31227;&#30340;&#25293;&#21334;&#26041;&#27861;&#65292;&#20197;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#36873;&#25321;&#32852;&#21512;&#20998;&#37197;&#12290;&#36825;&#20123;&#26041;&#27861;&#23548;&#33268;&#20102;&#19968;&#20010;&#20855;&#26377;&#33267;&#23569;$1-1/e$&#26368;&#20248;&#24615;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#12290;&#22312;&#35299;&#20915;MRRC&#38382;&#39064;&#30340;&#23454;&#39564;&#32467;&#26524;&#20013;&#31361;&#20986;&#20102;&#36817;&#20046;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the possibility of near-optimally solving multi-agent, multi-task NP-hard planning problems with time-dependent rewards using a learning-based algorithm. In particular, we consider a class of robot/machine scheduling problems called the multi-robot reward collection problem (MRRC). Such MRRC problems well model ride-sharing, pickup-and-delivery, and a variety of related problems. In representing the MRRC problem as a sequential decision-making problem, we observe that each state can be represented as an extension of probabilistic graphical models (PGMs), which we refer to as random PGMs. We then develop a mean-field inference method for random PGMs. We then propose (1) an order-transferable Q-function estimator and (2) an order-transferability-enabled auction to select a joint assignment in polynomial time. These result in a reinforcement learning framework with at least $1-1/e$ optimality. Experimental results on solving MRRC problems highlight the near-optimality 
&lt;/p&gt;</description></item><item><title>&#22312;&#26356;&#21487;&#20449;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;ResNet&#31867;&#22411;&#30340;CNN&#21487;&#20197;&#22312;&#19968;&#20123;&#37325;&#35201;&#30340;&#20989;&#25968;&#31867;&#20013;&#23454;&#29616;&#26497;&#23567;&#20540;&#26368;&#20248;&#35823;&#24046;&#29575;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#22797;&#21046;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#33021;&#21147;&#26469;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/1903.10047</link><description>&lt;p&gt;
&#36817;&#20284;&#21644;&#38750;&#21442;&#25968;&#20272;&#35745;&#30340;ResNet&#31867;&#22411;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Approximation and Non-parametric Estimation of ResNet-type Convolutional Neural Networks. (arXiv:1903.10047v4 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1903.10047
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26356;&#21487;&#20449;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;ResNet&#31867;&#22411;&#30340;CNN&#21487;&#20197;&#22312;&#19968;&#20123;&#37325;&#35201;&#30340;&#20989;&#25968;&#31867;&#20013;&#23454;&#29616;&#26497;&#23567;&#20540;&#26368;&#20248;&#35823;&#24046;&#29575;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#22797;&#21046;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#33021;&#21147;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNNs)&#24050;&#32463;&#22312;&#22810;&#31181;&#20989;&#25968;&#31867;&#20013;&#26174;&#31034;&#20986;&#20102;&#22312;&#36817;&#20284;&#21644;&#20272;&#35745;&#35823;&#24046;&#29575;&#26041;&#38754;&#30340;&#26368;&#20248;&#24615;(&#22312;&#26497;&#23567;&#20540;&#26368;&#22823;&#21270;&#24847;&#20041;&#19978;)&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#20998;&#26512;&#30340;&#26368;&#20248;CNN&#26159;&#19981;&#29616;&#23454;&#30340;&#23485;&#24230;&#65292;&#24182;&#19988;&#24456;&#38590;&#36890;&#36807;&#20248;&#21270;&#33719;&#24471;&#65292;&#22240;&#20026;&#22312;&#37325;&#35201;&#30340;&#20989;&#25968;&#31867;&#20013;&#20855;&#26377;&#31232;&#30095;&#32422;&#26463;&#65292;&#21253;&#25324;Holder&#31867;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;ResNet&#31867;&#22411;&#30340;CNN&#21487;&#20197;&#22312;&#26356;&#21487;&#20449;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#36825;&#20123;&#31867;&#30340;&#26497;&#23567;&#20540;&#26368;&#20248;&#35823;&#24046;&#29575;--&#23427;&#21487;&#20197;&#26159;&#23494;&#38598;&#30340;&#65292;&#24182;&#19988;&#20854;&#23485;&#24230;&#12289;&#36890;&#36947;&#22823;&#23567;&#21644;&#28388;&#27874;&#22120;&#22823;&#23567;&#19982;&#26679;&#26412;&#22823;&#23567;&#26080;&#20851;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#23450;&#21046;&#30340;CNN&#22797;&#21046;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;(FNNs)&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#21482;&#35201;FNNs&#20855;&#26377;&#22359;&#31232;&#30095;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#22312;&#26576;&#31181;&#24847;&#20041;&#19978;&#26159;&#36890;&#29992;&#30340;&#65292;&#25105;&#20204;&#21487;&#20197;&#33258;&#21160;&#23558;&#22359;&#31232;&#30095;FNNs&#23454;&#29616;&#30340;&#20219;&#20309;&#36817;&#20284;&#29575;&#36716;&#21270;&#20026;CNNs&#23454;&#29616;&#30340;&#36817;&#20284;&#29575;&#12290;&#20316;&#20026;&#19968;&#20010;&#24212;&#29992;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#21069;&#36848;&#31867;&#22411;CNN&#30340;&#36817;&#20284;&#21644;&#20272;&#35745;&#35823;&#24046;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutional neural networks (CNNs) have been shown to achieve optimal approximation and estimation error rates (in minimax sense) in several function classes. However, previous analyzed optimal CNNs are unrealistically wide and difficult to obtain via optimization due to sparse constraints in important function classes, including the H\"older class. We show a ResNet-type CNN can attain the minimax optimal error rates in these classes in more plausible situations -- it can be dense, and its width, channel size, and filter size are constant with respect to sample size. The key idea is that we can replicate the learning ability of Fully-connected neural networks (FNNs) by tailored CNNs, as long as the FNNs have \textit{block-sparse} structures. Our theory is general in a sense that we can automatically translate any approximation rate achieved by block-sparse FNNs into that by CNNs. As an application, we derive approximation and estimation error rates of the aformentioned type of CNNs f
&lt;/p&gt;</description></item></channel></rss>