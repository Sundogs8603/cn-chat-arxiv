<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>ID&#31639;&#27861;&#35299;&#20915;&#20102;&#22270;&#24418;&#22240;&#26524;&#27169;&#22411;&#20013;&#24178;&#39044;&#20998;&#24067;&#30340;&#35782;&#21035;&#38382;&#39064;&#65292;&#20294;&#24403;&#36755;&#20837;&#20998;&#24067;&#26080;&#27861;&#35782;&#21035;&#26102;&#65292;&#25152;&#35859;&#30340;"hedge&#20934;&#21017;"&#26080;&#25928;&#12290;</title><link>http://arxiv.org/abs/2307.03750</link><description>&lt;p&gt;
ID&#31639;&#27861;&#22312;&#20160;&#20040;&#24773;&#20917;&#19979;&#22833;&#25928;&#65311;
&lt;/p&gt;
&lt;p&gt;
When does the ID algorithm fail?. (arXiv:2307.03750v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03750
&lt;/p&gt;
&lt;p&gt;
ID&#31639;&#27861;&#35299;&#20915;&#20102;&#22270;&#24418;&#22240;&#26524;&#27169;&#22411;&#20013;&#24178;&#39044;&#20998;&#24067;&#30340;&#35782;&#21035;&#38382;&#39064;&#65292;&#20294;&#24403;&#36755;&#20837;&#20998;&#24067;&#26080;&#27861;&#35782;&#21035;&#26102;&#65292;&#25152;&#35859;&#30340;"hedge&#20934;&#21017;"&#26080;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ID&#31639;&#27861;&#35299;&#20915;&#20102;&#22270;&#24418;&#22240;&#26524;&#27169;&#22411;&#20013;&#24418;&#24335;&#20026;p(Y | do(a))&#30340;&#24178;&#39044;&#20998;&#24067;&#30340;&#35782;&#21035;&#38382;&#39064;&#65292;&#24182;&#19988;&#24050;&#32463;&#20197;&#22810;&#31181;&#26041;&#24335;&#36827;&#34892;&#20102;&#20844;&#24335;&#21270;&#12290;ID&#31639;&#27861;&#26159;&#27491;&#30830;&#30340;&#65288;&#22312;&#36755;&#20837;&#22270;&#25152;&#20195;&#34920;&#30340;&#22240;&#26524;&#27169;&#22411;&#20013;&#65292;&#36755;&#20986;&#35266;&#27979;&#25968;&#25454;&#20998;&#24067;&#30340;&#27491;&#30830;&#21151;&#33021;&#65289;&#65292;&#24182;&#19988;&#26159;&#23436;&#25972;&#30340;&#65288;&#24403;&#36755;&#20837;&#30340;p(Y | do(a))&#22312;&#36755;&#20837;&#22270;&#25152;&#20195;&#34920;&#30340;&#22240;&#26524;&#27169;&#22411;&#20013;&#26080;&#27861;&#35782;&#21035;&#26102;&#65292;&#26126;&#30830;&#26631;&#35760;&#20026;&#22833;&#36133;&#65289;&#12290;&#21442;&#32771;&#25991;&#29486;[9]&#25552;&#20379;&#20102;&#19968;&#20010;&#32467;&#26524;&#65292;&#21363;&#25152;&#35859;&#30340;"hedge&#20934;&#21017;"&#65288;Corollary 3&#65289;&#65292;&#26088;&#22312;&#20197;&#36755;&#20837;&#22270;&#20013;&#30340;&#19968;&#20010;&#31216;&#20026;"hedge"&#30340;&#32467;&#26500;&#26469;&#32473;&#20986;ID&#31639;&#27861;&#26080;&#27861;&#35782;&#21035;&#20854;&#36755;&#20837;&#30340;&#24773;&#20917;&#30340;&#22270;&#24418;&#29305;&#24449;&#12290;&#34429;&#28982;ID&#31639;&#27861;&#30830;&#23454;&#26159;&#19968;&#20010;&#27491;&#30830;&#21644;&#23436;&#25972;&#30340;&#31639;&#27861;&#65292;&#24182;&#19988;&#21482;&#35201;&#36755;&#20837;&#20998;&#24067;&#26080;&#27861;&#35782;&#21035;&#65292;"hedge"&#32467;&#26500;&#23601;&#20250;&#20986;&#29616;&#65292;&#20294;&#26159;&#22312;[9]&#20013;&#25552;&#20986;&#30340;Corollary 3&#26159;&#19981;&#27491;&#30830;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ID algorithm solves the problem of identification of interventional distributions of the form p(Y | do(a)) in graphical causal models, and has been formulated in a number of ways [12, 9, 6]. The ID algorithm is sound (outputs the correct functional of the observed data distribution whenever p(Y | do(a)) is identified in the causal model represented by the input graph), and complete (explicitly flags as a failure any input p(Y | do(a)) whenever this distribution is not identified in the causal model represented by the input graph).  The reference [9] provides a result, the so called "hedge criterion" (Corollary 3), which aims to give a graphical characterization of situations when the ID algorithm fails to identify its input in terms of a structure in the input graph called the hedge. While the ID algorithm is, indeed, a sound and complete algorithm, and the hedge structure does arise whenever the input distribution is not identified, Corollary 3 presented in [9] is incorrect as sta
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35752;&#35770;&#20102;&#22312;&#21327;&#20316;&#31185;&#23398;&#20013;&#20351;&#29992;&#28608;&#21169;&#29702;&#35770;&#30340;&#36125;&#21494;&#26031;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#30740;&#31350;&#32773;&#21644;&#20915;&#31574;&#32773;&#30340;&#19981;&#21516;&#28608;&#21169;&#26426;&#21046;&#65292;&#21033;&#29992;&#20195;&#29702;&#20154;&#30340;&#25112;&#30053;&#34892;&#20026;&#36827;&#34892;&#32479;&#35745;&#25512;&#26029;&#12290;</title><link>http://arxiv.org/abs/2307.03748</link><description>&lt;p&gt;
&#28608;&#21169;&#29702;&#35770;&#30340;&#36125;&#21494;&#26031;&#25512;&#29702;&#22312;&#21327;&#20316;&#31185;&#23398;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Incentive-Theoretic Bayesian Inference for Collaborative Science. (arXiv:2307.03748v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35752;&#35770;&#20102;&#22312;&#21327;&#20316;&#31185;&#23398;&#20013;&#20351;&#29992;&#28608;&#21169;&#29702;&#35770;&#30340;&#36125;&#21494;&#26031;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#30740;&#31350;&#32773;&#21644;&#20915;&#31574;&#32773;&#30340;&#19981;&#21516;&#28608;&#21169;&#26426;&#21046;&#65292;&#21033;&#29992;&#20195;&#29702;&#20154;&#30340;&#25112;&#30053;&#34892;&#20026;&#36827;&#34892;&#32479;&#35745;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20195;&#31185;&#23398;&#30740;&#31350;&#26159;&#19968;&#39033;&#20998;&#24067;&#24335;&#30340;&#12289;&#21327;&#20316;&#30340;&#24037;&#20316;&#65292;&#30001;&#30740;&#31350;&#22242;&#38431;&#12289;&#30417;&#31649;&#26426;&#26500;&#12289;&#36164;&#21161;&#26426;&#26500;&#12289;&#21830;&#19994;&#21512;&#20316;&#20249;&#20276;&#21644;&#31185;&#23398;&#26426;&#26500;&#32452;&#25104;&#65292;&#24444;&#27492;&#20114;&#21160;&#24182;&#38754;&#23545;&#19981;&#21516;&#30340;&#28608;&#21169;&#12290;&#20026;&#20102;&#20445;&#25345;&#31185;&#23398;&#20005;&#35880;&#24615;&#65292;&#32479;&#35745;&#26041;&#27861;&#24212;&#35813;&#35748;&#35782;&#21040;&#36825;&#31181;&#24773;&#20917;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20551;&#35774;&#26816;&#39564;&#30340;&#24773;&#20917;&#65292;&#20854;&#20013;&#26377;&#19968;&#20010;&#20195;&#29702;&#20154;&#65288;&#20363;&#22914;&#30740;&#31350;&#20154;&#21592;&#25110;&#21046;&#33647;&#20844;&#21496;&#65289;&#23545;&#26410;&#30693;&#21442;&#25968;&#25317;&#26377;&#31169;&#20154;&#20808;&#39564;&#30693;&#35782;&#65292;&#36824;&#26377;&#19968;&#20010;&#22996;&#25176;&#20154;&#65288;&#22914;&#25919;&#31574;&#21046;&#23450;&#32773;&#25110;&#30417;&#31649;&#26426;&#26500;&#65289;&#24076;&#26395;&#26681;&#25454;&#21442;&#25968;&#20540;&#20570;&#20986;&#20915;&#31574;&#12290;&#20195;&#29702;&#20154;&#26681;&#25454;&#20182;&#20204;&#30340;&#31169;&#20154;&#20808;&#39564;&#36873;&#25321;&#26159;&#21542;&#36827;&#34892;&#32479;&#35745;&#35797;&#39564;&#65292;&#28982;&#21518;&#35797;&#39564;&#30340;&#32467;&#26524;&#30001;&#22996;&#25176;&#20154;&#29992;&#26469;&#20570;&#20986;&#20915;&#31574;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22996;&#25176;&#20154;&#22914;&#20309;&#36827;&#34892;&#32479;&#35745;&#25512;&#26029;&#65292;&#21033;&#29992;&#20195;&#29702;&#20154;&#30340;&#25112;&#30053;&#34892;&#20026;&#25152;&#36879;&#38706;&#30340;&#20449;&#24687;&#65292;&#20063;&#23601;&#26159;&#20182;&#20204;&#36873;&#25321;&#26159;&#21542;&#36827;&#34892;&#35797;&#39564;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#35745;&#31639;p&#20540;&#65292;&#20174;&#32780;&#32508;&#21512;&#21033;&#29992;&#20195;&#29702;&#20154;&#30340;&#34892;&#20026;&#21644;&#35797;&#39564;&#30340;&#32467;&#26524;&#36827;&#34892;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contemporary scientific research is a distributed, collaborative endeavor, carried out by teams of researchers, regulatory institutions, funding agencies, commercial partners, and scientific bodies, all interacting with each other and facing different incentives. To maintain scientific rigor, statistical methods should acknowledge this state of affairs. To this end, we study hypothesis testing when there is an agent (e.g., a researcher or a pharmaceutical company) with a private prior about an unknown parameter and a principal (e.g., a policymaker or regulator) who wishes to make decisions based on the parameter value. The agent chooses whether to run a statistical trial based on their private prior and then the result of the trial is used by the principal to reach a decision. We show how the principal can conduct statistical inference that leverages the information that is revealed by an agent's strategic behavior -- their choice to run a trial or not. In particular, we show how the p
&lt;/p&gt;</description></item><item><title>QIGen&#26159;&#19968;&#31181;&#29992;&#20110;&#25903;&#25345;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#37327;&#21270;&#25512;&#29702;&#30340;&#33258;&#21160;&#20195;&#30721;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#30446;&#26631;&#26550;&#26500;&#21644;&#24615;&#33021;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#39640;&#24615;&#33021;&#21644;&#39640;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;LLaMA&#27169;&#22411;&#30340;&#22522;&#20110;CPU&#30340;&#25512;&#29702;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#24320;&#28304;&#35299;&#20915;&#26041;&#26696;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.03738</link><description>&lt;p&gt;
QIGen&#65306;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37327;&#21270;&#25512;&#29702;&#30340;&#39640;&#25928;&#20869;&#26680;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
QIGen: Generating Efficient Kernels for Quantized Inference on Large Language Models. (arXiv:2307.03738v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03738
&lt;/p&gt;
&lt;p&gt;
QIGen&#26159;&#19968;&#31181;&#29992;&#20110;&#25903;&#25345;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#37327;&#21270;&#25512;&#29702;&#30340;&#33258;&#21160;&#20195;&#30721;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#30446;&#26631;&#26550;&#26500;&#21644;&#24615;&#33021;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#39640;&#24615;&#33021;&#21644;&#39640;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;LLaMA&#27169;&#22411;&#30340;&#22522;&#20110;CPU&#30340;&#25512;&#29702;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#24320;&#28304;&#35299;&#20915;&#26041;&#26696;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#21160;&#20195;&#30721;&#29983;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#25903;&#25345;LLMs&#65288;&#22914;LLaMA&#25110;OPT&#65289;&#22312;&#29616;&#25104;&#30340;CPU&#19978;&#36827;&#34892;&#37327;&#21270;&#29983;&#25104;&#25512;&#29702;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26681;&#25454;&#30446;&#26631;&#26550;&#26500;&#21644;&#24615;&#33021;&#27169;&#22411;&#36827;&#34892;&#35774;&#35745;&#65292;&#21253;&#25324;&#30828;&#20214;&#29305;&#24615;&#21644;&#26041;&#27861;&#29305;&#23450;&#30340;&#20934;&#30830;&#24615;&#32422;&#26463;&#12290;&#22312;LLaMA&#27169;&#22411;&#30340;&#22522;&#20110;CPU&#30340;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#39640;&#24615;&#33021;&#21644;&#39640;&#20934;&#30830;&#24615;&#65292;&#19982;&#29616;&#26377;&#26368;&#20339;&#24320;&#28304;&#35299;&#20915;&#26041;&#26696;&#30456;&#27604;&#26356;&#20855;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#23454;&#29616;&#20195;&#30721;&#21487;&#22312;https://github.com/IST-DASLab/QIGen&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present ongoing work on a new automatic code generation approach for supporting quantized generative inference on LLMs such as LLaMA or OPT on off-the-shelf CPUs. Our approach is informed by the target architecture and a performance model, including both hardware characteristics and method-specific accuracy constraints. Results on CPU-based inference for LLaMA models show that our approach can lead to high performance and high accuracy, comparing favorably to the best existing open-source solution. A preliminary implementation is available at https://github.com/IST-DASLab/QIGen.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#39640;&#20102;&#38050;&#38081;&#34920;&#38754;&#31895;&#31961;&#24230;&#21442;&#25968;&#30340;&#35745;&#31639;&#20934;&#30830;&#24615;&#65292;&#36890;&#36807;&#23545;&#27604;&#19981;&#21516;&#26041;&#27861;&#65292;&#35780;&#20272;&#20102;&#22312;&#32447;&#27979;&#37327;&#36716;&#21270;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.03723</link><description>&lt;p&gt;
&#20351;&#29992;&#28608;&#20809;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#38050;&#38081;&#34920;&#38754;&#31895;&#31961;&#24230;&#21442;&#25968;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Steel Surface Roughness Parameter Calculations Using Lasers and Machine Learning Models. (arXiv:2307.03723v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03723
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#39640;&#20102;&#38050;&#38081;&#34920;&#38754;&#31895;&#31961;&#24230;&#21442;&#25968;&#30340;&#35745;&#31639;&#20934;&#30830;&#24615;&#65292;&#36890;&#36807;&#23545;&#27604;&#19981;&#21516;&#26041;&#27861;&#65292;&#35780;&#20272;&#20102;&#22312;&#32447;&#27979;&#37327;&#36716;&#21270;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25511;&#21046;&#24102;&#38050;&#34920;&#38754;&#32441;&#29702;&#23545;&#20110;&#28385;&#36275;&#38208;&#38156;&#21644;&#36711;&#21046;&#24037;&#33402;&#20013;&#23458;&#25143;&#35201;&#27714;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#26041;&#27861;&#20381;&#36182;&#20110;&#29983;&#20135;&#21518;&#30340;&#27979;&#38024;&#27979;&#37327;&#65292;&#32780;&#22312;&#32447;&#25216;&#26415;&#21017;&#25552;&#20379;&#20102;&#23545;&#25972;&#20010;&#24102;&#38050;&#36827;&#34892;&#38750;&#25509;&#35302;&#21644;&#23454;&#26102;&#27979;&#37327;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30830;&#20445;&#20934;&#30830;&#27979;&#37327;&#23545;&#20110;&#20854;&#22312;&#21046;&#36896;&#27969;&#31243;&#20013;&#30340;&#26377;&#25928;&#21033;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;&#20934;&#30830;&#30340;&#22312;&#32447;&#27979;&#37327;&#20351;&#24471;&#22312;&#29983;&#20135;&#36807;&#31243;&#20013;&#33021;&#22815;&#23454;&#26102;&#35843;&#25972;&#21046;&#36896;&#21152;&#24037;&#21442;&#25968;&#65292;&#30830;&#20445;&#20135;&#21697;&#36136;&#37327;&#30340;&#19968;&#33268;&#24615;&#21644;&#36711;&#26426;&#30340;&#38381;&#29615;&#25511;&#21046;&#30340;&#21487;&#33021;&#24615;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25913;&#36827;&#22312;&#32447;&#27979;&#37327;&#36716;&#21270;&#20026;&#26356;&#20934;&#30830;&#30340;Ra&#34920;&#38754;&#31895;&#31961;&#24230;&#25351;&#26631;&#12290;&#36890;&#36807;&#27604;&#36739;&#21253;&#25324;&#28145;&#24230;&#23398;&#20064;&#21644;&#38750;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#20869;&#30340;&#19968;&#31995;&#21015;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#19982;&#38381;&#21512;&#24418;&#24335;&#36716;&#21270;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#23427;&#20204;&#25552;&#39640;&#34920;&#38754;&#31895;&#31961;&#24230;&#21442;&#25968;&#35745;&#31639;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Control of surface texture in strip steel is essential to meet customer requirements during galvanizing and temper rolling processes. Traditional methods rely on post-production stylus measurements, while on-line techniques offer non-contact and real-time measurements of the entire strip. However, ensuring accurate measurement is imperative for their effective utilization in the manufacturing pipeline. Moreover, accurate on-line measurements enable real-time adjustments of manufacturing processing parameters during production, ensuring consistent quality and the possibility of closed-loop control of the temper mill. In this study, we leverage state-of-the-art machine learning models to enhance the transformation of on-line measurements into significantly a more accurate Ra surface roughness metric. By comparing a selection of data-driven approaches, including both deep learning and non-deep learning methods, to the close-form transformation, we evaluate their potential for improving su
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22810;&#20010;&#26426;&#22120;&#20154;&#24179;&#21488;&#19978;&#35757;&#32451;&#21333;&#20010;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#33109;&#37096;&#25668;&#20687;&#22836;&#21644;&#32479;&#19968;&#30340;&#20195;&#30721;&#24211;&#23545;&#35266;&#27979;&#21644;&#21160;&#20316;&#31354;&#38388;&#36827;&#34892;&#23545;&#40784;&#65292;&#24182;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26469;&#28040;&#38500;&#39046;&#22495;&#24046;&#24322;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#26426;&#22120;&#20154;&#24179;&#21488;&#19978;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.03719</link><description>&lt;p&gt;
Polybot&#65306;&#22312;&#25509;&#21463;&#21464;&#24322;&#24615;&#30340;&#21516;&#26102;&#35757;&#32451;&#22810;&#20010;&#26426;&#22120;&#20154;&#19978;&#30340;&#19968;&#20010;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Polybot: Training One Policy Across Robots While Embracing Variability. (arXiv:2307.03719v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03719
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22810;&#20010;&#26426;&#22120;&#20154;&#24179;&#21488;&#19978;&#35757;&#32451;&#21333;&#20010;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#33109;&#37096;&#25668;&#20687;&#22836;&#21644;&#32479;&#19968;&#30340;&#20195;&#30721;&#24211;&#23545;&#35266;&#27979;&#21644;&#21160;&#20316;&#31354;&#38388;&#36827;&#34892;&#23545;&#40784;&#65292;&#24182;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26469;&#28040;&#38500;&#39046;&#22495;&#24046;&#24322;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#26426;&#22120;&#20154;&#24179;&#21488;&#19978;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#33719;&#21462;&#26426;&#22120;&#20154;&#25968;&#25454;&#38598;&#30340;&#39640;&#25104;&#26412;&#65292;&#37325;&#29992;&#22823;&#22411;&#25968;&#25454;&#38598;&#23545;&#20110;&#23558;&#22522;&#20110;&#35270;&#35273;&#30340;&#26426;&#22120;&#20154;&#25805;&#32437;&#22120;&#25193;&#23637;&#21040;&#26085;&#24120;&#24773;&#26223;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#26426;&#22120;&#20154;&#24179;&#21488;&#20855;&#26377;&#19981;&#21516;&#30340;&#25511;&#21046;&#26041;&#26696;&#12289;&#25668;&#20687;&#26426;&#35270;&#35282;&#12289;&#36816;&#21160;&#23398;&#37197;&#32622;&#21644;&#26411;&#31471;&#25191;&#34892;&#22120;&#24418;&#24577;&#65292;&#20174;&#19968;&#20010;&#24179;&#21488;&#36716;&#31227;&#25805;&#32437;&#25216;&#33021;&#38754;&#20020;&#30528;&#37325;&#35201;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#32452;&#20851;&#38190;&#35774;&#35745;&#20915;&#31574;&#65292;&#20197;&#22312;&#22810;&#20010;&#26426;&#22120;&#20154;&#24179;&#21488;&#19978;&#35757;&#32451;&#21333;&#20010;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#39318;&#20808;&#36890;&#36807;&#21033;&#29992;&#33109;&#37096;&#25668;&#20687;&#22836;&#21644;&#19968;&#20010;&#32479;&#19968;&#20294;&#27169;&#22359;&#21270;&#30340;&#20195;&#30721;&#24211;&#65292;&#23545;&#25105;&#20204;&#30340;&#31574;&#30053;&#22312;&#19981;&#21516;&#26426;&#20307;&#19978;&#36827;&#34892;&#35266;&#27979;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#23545;&#40784;&#12290;&#20026;&#20102;&#28040;&#38500;&#21097;&#20313;&#30340;&#39046;&#22495;&#24046;&#24322;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#22312;&#19981;&#21516;&#26426;&#20307;&#20043;&#38388;&#23545;&#40784;&#25105;&#20204;&#31574;&#30053;&#30340;&#20869;&#37096;&#34920;&#31034;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#36328;6&#20010;&#20219;&#21153;&#21644;3&#20010;&#20855;&#26377;&#19981;&#21516;&#20851;&#33410;&#37197;&#32622;&#21644;&#23610;&#23544;&#30340;&#26426;&#22120;&#20154;&#30340;60&#23567;&#26102;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65306;WidowX 250S&#65292;Franka Emika Panda&#21644;S&#12290;
&lt;/p&gt;
&lt;p&gt;
Reusing large datasets is crucial to scale vision-based robotic manipulators to everyday scenarios due to the high cost of collecting robotic datasets. However, robotic platforms possess varying control schemes, camera viewpoints, kinematic configurations, and end-effector morphologies, posing significant challenges when transferring manipulation skills from one platform to another. To tackle this problem, we propose a set of key design decisions to train a single policy for deployment on multiple robotic platforms. Our framework first aligns the observation and action spaces of our policy across embodiments via utilizing wrist cameras and a unified, but modular codebase. To bridge the remaining domain shift, we align our policy's internal representations across embodiments through contrastive learning. We evaluate our method on a dataset collected over 60 hours spanning 6 tasks and 3 robots with varying joint configurations and sizes: the WidowX 250S, the Franka Emika Panda, and the S
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#29983;&#29289;&#36827;&#21270;&#20013;&#21457;&#23637;&#20986;&#30340;&#32908;&#32905;&#21327;&#21516;&#25511;&#21046;&#31574;&#30053;&#24212;&#29992;&#20110;&#20154;&#25163;&#21644;&#33151;&#27169;&#22411;&#65292;&#21457;&#29616;&#36890;&#36807;&#21327;&#21516;&#34892;&#21160;&#34920;&#31034;&#65288;SAR&#65289;&#35757;&#32451;&#30340;&#31574;&#30053;&#22312;&#23398;&#20064;&#22797;&#26434;&#20219;&#21153;&#26102;&#26126;&#26174;&#20248;&#20110;&#31471;&#21040;&#31471;&#24378;&#21270;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2307.03716</link><description>&lt;p&gt;
SAR: &#36890;&#36807;&#21327;&#21516;&#34892;&#21160;&#34920;&#31034;&#23454;&#29616;&#29983;&#29702;&#25935;&#25463;&#24615;&#21644;&#28789;&#24039;&#24615;&#30340;&#26222;&#36866;&#24615;
&lt;/p&gt;
&lt;p&gt;
SAR: Generalization of Physiological Agility and Dexterity via Synergistic Action Representation. (arXiv:2307.03716v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03716
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#29983;&#29289;&#36827;&#21270;&#20013;&#21457;&#23637;&#20986;&#30340;&#32908;&#32905;&#21327;&#21516;&#25511;&#21046;&#31574;&#30053;&#24212;&#29992;&#20110;&#20154;&#25163;&#21644;&#33151;&#27169;&#22411;&#65292;&#21457;&#29616;&#36890;&#36807;&#21327;&#21516;&#34892;&#21160;&#34920;&#31034;&#65288;SAR&#65289;&#35757;&#32451;&#30340;&#31574;&#30053;&#22312;&#23398;&#20064;&#22797;&#26434;&#20219;&#21153;&#26102;&#26126;&#26174;&#20248;&#20110;&#31471;&#21040;&#31471;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#32500;&#31995;&#32479;&#20013;&#23398;&#20064;&#39640;&#25928;&#36830;&#32493;&#25511;&#21046;&#31574;&#30053;&#65292;&#21253;&#25324;&#32908;&#32905;&#39592;&#39612;&#20195;&#29702;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#36890;&#36807;&#29983;&#29289;&#36827;&#21270;&#30340;&#36807;&#31243;&#20013;&#65292;&#29983;&#29289;&#20307;&#21457;&#23637;&#20986;&#20102;&#20811;&#26381;&#36825;&#31181;&#22797;&#26434;&#24615;&#30340;&#40065;&#26834;&#26426;&#21046;&#65292;&#23398;&#20064;&#20102;&#39640;&#24230;&#22797;&#26434;&#30340;&#36816;&#21160;&#25511;&#21046;&#31574;&#30053;&#12290;&#26159;&#20160;&#20040;&#23548;&#33268;&#20102;&#36825;&#31181;&#40065;&#26834;&#30340;&#34892;&#20026;&#28789;&#27963;&#24615;&#21602;&#65311;&#36890;&#36807;&#32908;&#32905;&#21327;&#21516;&#30340;&#27169;&#22359;&#21270;&#25511;&#21046;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#28508;&#22312;&#30340;&#26426;&#21046;&#65292;&#20351;&#29983;&#29289;&#33021;&#22815;&#22312;&#31616;&#21270;&#21644;&#21487;&#25512;&#24191;&#30340;&#34892;&#21160;&#31354;&#38388;&#20013;&#23398;&#20064;&#32908;&#32905;&#25511;&#21046;&#12290;&#21463;&#21040;&#36825;&#31181;&#36827;&#21270;&#30340;&#36816;&#21160;&#25511;&#21046;&#31574;&#30053;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20351;&#29992;&#29983;&#29702;&#20934;&#30830;&#30340;&#20154;&#25163;&#21644;&#33151;&#27169;&#22411;&#20316;&#20026;&#27979;&#35797;&#24179;&#21488;&#65292;&#30830;&#23450;&#20174;&#31616;&#21333;&#20219;&#21153;&#20013;&#33719;&#24471;&#30340;&#21327;&#21516;&#34892;&#21160;&#34920;&#31034;&#65288;SAR&#65289;&#22312;&#23398;&#20064;&#26356;&#22797;&#26434;&#20219;&#21153;&#20013;&#30340;&#20316;&#29992;&#31243;&#24230;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#36825;&#20004;&#31181;&#24773;&#20917;&#19979;&#65292;&#21033;&#29992;SAR&#30340;&#31574;&#30053;&#26126;&#26174;&#20248;&#20110;&#31471;&#21040;&#31471;&#24378;&#21270;&#23398;&#20064;&#12290;&#36890;&#36807;SAR&#36827;&#34892;&#35757;&#32451;&#30340;&#31574;&#30053;&#20248;&#20110;&#31471;&#21040;&#31471;&#24378;&#21270;&#23398;&#20064;.
&lt;/p&gt;
&lt;p&gt;
Learning effective continuous control policies in high-dimensional systems, including musculoskeletal agents, remains a significant challenge. Over the course of biological evolution, organisms have developed robust mechanisms for overcoming this complexity to learn highly sophisticated strategies for motor control. What accounts for this robust behavioral flexibility? Modular control via muscle synergies, i.e. coordinated muscle co-contractions, is considered to be one putative mechanism that enables organisms to learn muscle control in a simplified and generalizable action space. Drawing inspiration from this evolved motor control strategy, we use physiologically accurate human hand and leg models as a testbed for determining the extent to which a Synergistic Action Representation (SAR) acquired from simpler tasks facilitates learning more complex tasks. We find in both cases that SAR-exploiting policies significantly outperform end-to-end reinforcement learning. Policies trained wit
&lt;/p&gt;</description></item><item><title>INT-FP-QSim&#26159;&#19968;&#20010;&#24320;&#28304;&#27169;&#25311;&#22120;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#36716;&#25442;&#22120;&#22312;&#19981;&#21516;&#31934;&#24230;&#21644;&#26684;&#24335;&#19979;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;&#25968;&#20540;&#26684;&#24335;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;4&#20301;&#26435;&#37325;&#21644;4&#20301;&#25110;8&#20301;&#28608;&#27963;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.03712</link><description>&lt;p&gt;
INT-FP-QSim: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#36716;&#25442;&#22120;&#30340;&#28151;&#21512;&#31934;&#24230;&#21644;&#26684;&#24335;
&lt;/p&gt;
&lt;p&gt;
INT-FP-QSim: Mixed Precision and Formats For Large Language Models and Vision Transformers. (arXiv:2307.03712v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03712
&lt;/p&gt;
&lt;p&gt;
INT-FP-QSim&#26159;&#19968;&#20010;&#24320;&#28304;&#27169;&#25311;&#22120;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#36716;&#25442;&#22120;&#22312;&#19981;&#21516;&#31934;&#24230;&#21644;&#26684;&#24335;&#19979;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;&#25968;&#20540;&#26684;&#24335;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;4&#20301;&#26435;&#37325;&#21644;4&#20301;&#25110;8&#20301;&#28608;&#27963;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23835;&#36215;&#23548;&#33268;&#20102;&#20943;&#23569;&#31934;&#24230;&#30340;&#36816;&#34892;&#30340;&#22686;&#21152;&#12290;&#38477;&#20302;&#31934;&#24230;&#30340;&#36816;&#34892;&#26041;&#24335;&#25903;&#25345;&#36164;&#28304;&#32422;&#26463;&#65292;&#24182;&#20419;&#36827;&#20854;&#27665;&#20027;&#21270;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#22312;&#20010;&#20154;&#35774;&#22791;&#19978;&#36816;&#34892;&#25968;&#21313;&#20159;&#21442;&#25968;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#20026;&#20102;&#34917;&#20805;&#36825;&#19968;&#25345;&#32493;&#21162;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;INT-FP-QSim&#65306;&#19968;&#20010;&#24320;&#28304;&#27169;&#25311;&#22120;&#65292;&#21487;&#20197;&#28789;&#27963;&#35780;&#20272;&#19981;&#21516;&#25968;&#20540;&#31934;&#24230;&#21644;&#26684;&#24335;&#19979;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#36716;&#25442;&#22120;&#12290;INT-FP-QSim&#21033;&#29992;&#29616;&#26377;&#30340;&#24320;&#28304;&#24211;&#65292;&#22914;TensorRT&#12289;QPytorch&#21644;AIMET&#65292;&#23454;&#29616;&#20102;&#25903;&#25345;&#21508;&#31181;&#28014;&#28857;&#21644;&#25972;&#25968;&#26684;&#24335;&#30340;&#32452;&#21512;&#27169;&#25311;&#22120;&#12290;&#20511;&#21161;&#25105;&#20204;&#30340;&#27169;&#25311;&#22120;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#19981;&#21516;&#25968;&#20540;&#26684;&#24335;&#23545;4&#20301;&#26435;&#37325;&#21644;4&#20301;&#25110;8&#20301;&#28608;&#27963;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#36716;&#25442;&#22120;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36824;&#27604;&#36739;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#22914;&#33258;&#36866;&#24212;&#22359;&#28014;&#28857;&#12289;SmoothQuant&#12289;GPTQ&#21644;RPTQ&#22312;&#27169;&#22411;&#24615;&#33021;&#19978;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#24076;&#26395;INT-FP-QSim&#33021;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#22312;&#20302;&#31934;&#24230;&#29615;&#22659;&#19979;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#36716;&#25442;&#22120;&#30340;&#24615;&#33021;&#25552;&#20379;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent rise of large language models (LLMs) has resulted in increased efforts towards running LLMs at reduced precision. Running LLMs at lower precision supports resource constraints and furthers their democratization, enabling users to run billion-parameter LLMs on their personal devices. To supplement this ongoing effort, we propose INT-FP-QSim: an open-source simulator that enables flexible evaluation of LLMs and vision transformers at various numerical precisions and formats. INT-FP-QSim leverages existing open-source repositories such as TensorRT, QPytorch and AIMET for a combined simulator that supports various floating point and integer formats. With the help of our simulator, we survey the impact of different numerical formats on the performance of LLMs and vision transformers at 4-bit weights and 4-bit or 8-bit activations. We also compare recently proposed methods like Adaptive Block Floating Point, SmoothQuant, GPTQ and RPTQ on the model performances. We hope INT-FP-QSim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#23548;&#21644;&#38480;&#21046;&#34920;&#31034;&#30340;&#31561;&#21464;&#24615;&#32422;&#26463;&#65292;&#22312;&#20108;&#32500;&#22270;&#20687;&#20013;&#23398;&#20064;&#19977;&#32500;&#19990;&#30028;&#34920;&#31034;&#65292;&#24182;&#28385;&#36275;&#20960;&#20309;&#19968;&#33268;&#24615;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.03704</link><description>&lt;p&gt;
&#36890;&#36807;&#24341;&#23548;&#21644;&#38480;&#21046;&#34920;&#31034;&#30340;&#31561;&#21464;&#24615;&#21333;&#35270;&#35282;&#23039;&#21183;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Equivariant Single View Pose Prediction Via Induced and Restricted Representations. (arXiv:2307.03704v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03704
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#23548;&#21644;&#38480;&#21046;&#34920;&#31034;&#30340;&#31561;&#21464;&#24615;&#32422;&#26463;&#65292;&#22312;&#20108;&#32500;&#22270;&#20687;&#20013;&#23398;&#20064;&#19977;&#32500;&#19990;&#30028;&#34920;&#31034;&#65292;&#24182;&#28385;&#36275;&#20960;&#20309;&#19968;&#33268;&#24615;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20108;&#32500;&#22270;&#20687;&#20013;&#23398;&#20064;&#19977;&#32500;&#19990;&#30028;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#23545;&#20110;&#36825;&#26679;&#30340;&#20219;&#21153;&#65292;&#29702;&#24819;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#24212;&#35813;&#21033;&#29992;&#29289;&#20307;&#21487;&#20197;&#22312;&#19977;&#32500;&#31354;&#38388;&#20013;&#26059;&#36716;&#21644;&#24179;&#31227;&#30340;&#20107;&#23454;&#65292;&#39044;&#27979;&#20851;&#20110;&#26032;&#22270;&#20687;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#22312;&#20108;&#32500;&#36755;&#20837;&#19978;&#26045;&#21152;SO(3)-&#31561;&#21464;&#24615;&#36739;&#20026;&#22256;&#38590;&#65292;&#22240;&#20026;&#19977;&#32500;&#26059;&#36716;&#32676;&#23545;&#20110;&#20108;&#32500;&#24179;&#38754;&#27809;&#26377;&#33258;&#28982;&#30340;&#20316;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;SO(3)&#30340;&#19968;&#20010;&#20803;&#32032;&#21487;&#33021;&#20250;&#20351;&#22270;&#20687;&#22312;&#24179;&#38754;&#22806;&#26059;&#36716;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#20174;&#20108;&#32500;&#22270;&#20687;&#20013;&#23398;&#20064;&#19977;&#32500;&#19990;&#30028;&#34920;&#31034;&#30340;&#31639;&#27861;&#24517;&#39035;&#28385;&#36275;&#19968;&#23450;&#30340;&#20960;&#20309;&#19968;&#33268;&#24615;&#23646;&#24615;&#65292;&#25105;&#20204;&#23558;&#20854;&#34920;&#36798;&#20026;SO(2)-&#31561;&#21464;&#24615;&#32422;&#26463;&#12290;&#25105;&#20204;&#20351;&#29992;SO(2)&#22312;SO(3)&#19978;&#30340;&#24341;&#23548;&#21644;&#38480;&#21046;&#34920;&#31034;&#26469;&#26500;&#24314;&#21644;&#20998;&#31867;&#28385;&#36275;&#36825;&#20123;&#20960;&#20309;&#19968;&#33268;&#24615;&#32422;&#26463;&#30340;&#26550;&#26500;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20219;&#20309;&#31526;&#21512;&#36825;&#20123;&#35268;&#21017;&#30340;&#26550;&#26500;&#37117;&#24517;&#39035;&#20855;&#22791;
&lt;/p&gt;
&lt;p&gt;
Learning about the three-dimensional world from two-dimensional images is a fundamental problem in computer vision. An ideal neural network architecture for such tasks would leverage the fact that objects can be rotated and translated in three dimensions to make predictions about novel images. However, imposing SO(3)-equivariance on two-dimensional inputs is difficult because the group of three-dimensional rotations does not have a natural action on the two-dimensional plane. Specifically, it is possible that an element of SO(3) will rotate an image out of plane. We show that an algorithm that learns a three-dimensional representation of the world from two dimensional images must satisfy certain geometric consistency properties which we formulate as SO(2)-equivariance constraints. We use the induced and restricted representations of SO(2) on SO(3) to construct and classify architectures which satisfy these geometric consistency constraints. We prove that any architecture which respects
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#20301;&#25968;&#22238;&#24402;&#36827;&#34892;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#19982;&#29616;&#26377;&#30340;&#24433;&#23376;&#27169;&#22411;&#25915;&#20987;&#30456;&#27604;&#65292;&#35745;&#31639;&#37327;&#26356;&#23567;&#20294;&#25928;&#26524;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2307.03694</link><description>&lt;p&gt;
&#21487;&#20280;&#32553;&#30340;&#36890;&#36807;&#20998;&#20301;&#25968;&#22238;&#24402;&#36827;&#34892;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Scalable Membership Inference Attacks via Quantile Regression. (arXiv:2307.03694v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03694
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#20301;&#25968;&#22238;&#24402;&#36827;&#34892;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#19982;&#29616;&#26377;&#30340;&#24433;&#23376;&#27169;&#22411;&#25915;&#20987;&#30456;&#27604;&#65292;&#35745;&#31639;&#37327;&#26356;&#23567;&#20294;&#25928;&#26524;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#26088;&#22312;&#36890;&#36807;&#23545;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#40657;&#30418;&#35775;&#38382;&#65292;&#30830;&#23450;&#29305;&#23450;&#31034;&#20363;&#26159;&#21542;&#22312;&#35757;&#32451;&#20013;&#20351;&#29992;&#12290;&#25104;&#21592;&#25512;&#26029;&#21487;&#20197;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#20551;&#35774;&#26816;&#39564;&#38382;&#39064;&#12290;&#29616;&#26377;&#26368;&#26377;&#25928;&#30340;&#25915;&#20987;&#26159;&#36890;&#36807;&#35757;&#32451;&#35768;&#22810;&#19982;&#34987;&#25915;&#20987;&#27169;&#22411;&#30456;&#21516;&#26550;&#26500;&#30340;&#8220;&#24433;&#23376;&#27169;&#22411;&#8221;&#65288;&#22312;&#38543;&#26426;&#25968;&#25454;&#23376;&#26679;&#26412;&#19978;&#35757;&#32451;&#65289;&#26469;&#20272;&#35745;&#26576;&#20123;&#27979;&#35797;&#32479;&#35745;&#37327;&#65288;&#36890;&#24120;&#26159;&#27169;&#22411;&#23545;&#30495;&#23454;&#26631;&#31614;&#30340;&#32622;&#20449;&#24230;&#65289;&#30340;&#20998;&#24067;&#12290;&#34429;&#28982;&#26377;&#25928;&#65292;&#20294;&#36825;&#20123;&#25915;&#20987;&#38750;&#24120;&#35745;&#31639;&#23494;&#38598;&#65292;&#29305;&#21035;&#26159;&#24403;&#34987;&#25915;&#20987;&#30340;&#27169;&#22411;&#24456;&#22823;&#26102;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#26041;&#24335;&#65292;&#36890;&#36807;&#23545;&#26410;&#22312;&#35757;&#32451;&#20013;&#20351;&#29992;&#30340;&#28857;&#19978;&#30340;&#34987;&#25915;&#20987;&#27169;&#22411;&#20135;&#29983;&#30340;&#32622;&#20449;&#24230;&#20998;&#24067;&#36827;&#34892;&#20998;&#20301;&#25968;&#22238;&#24402;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#26368;&#20808;&#36827;&#30340;&#24433;&#23376;&#27169;&#22411;&#25915;&#20987;&#30456;&#31454;&#20105;&#65292;&#21516;&#26102;&#38656;&#35201;&#26356;&#23569;&#30340;&#35745;&#31639;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Membership inference attacks are designed to determine, using black box access to trained models, whether a particular example was used in training or not. Membership inference can be formalized as a hypothesis testing problem. The most effective existing attacks estimate the distribution of some test statistic (usually the model's confidence on the true label) on points that were (and were not) used in training by training many \emph{shadow models} -i.e. models of the same architecture as the model being attacked, trained on a random subsample of data. While effective, these attacks are extremely computationally expensive, especially when the model under attack is large.  We introduce a new class of attacks based on performing quantile regression on the distribution of confidence scores induced by the model under attack on points that are not used in training. We show that our method is competitive with state-of-the-art shadow model attacks, while requiring substantially less comput
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#26080;&#27169;&#22411;&#26041;&#27861;&#65292;&#21487;&#20197;&#20165;&#36890;&#36807;&#31995;&#32479;&#22312;&#24050;&#30693;&#24378;&#36843;&#20989;&#25968;&#24433;&#21709;&#19979;&#30340;&#35266;&#27979;&#65292;&#35782;&#21035;&#21644;&#25233;&#21046;&#26410;&#30693;&#31995;&#32479;&#30340;&#26410;&#30693;&#24178;&#25200;&#12290;&#36825;&#39033;&#26041;&#27861;&#23545;&#35757;&#32451;&#20989;&#25968;&#26377;&#38750;&#24120;&#28201;&#21644;&#30340;&#38480;&#21046;&#65292;&#33021;&#22815;&#31283;&#20581;&#22320;&#35782;&#21035;&#21644;&#25233;&#21046;&#22823;&#31867;&#21035;&#30340;&#26410;&#30693;&#24178;&#25200;&#12290;</title><link>http://arxiv.org/abs/2307.03690</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25233;&#21046;&#21160;&#21147;&#31995;&#32479;&#20013;&#30340;&#26410;&#30693;&#24178;&#25200;
&lt;/p&gt;
&lt;p&gt;
Suppressing unknown disturbances to dynamical systems using machine learning. (arXiv:2307.03690v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03690
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#26080;&#27169;&#22411;&#26041;&#27861;&#65292;&#21487;&#20197;&#20165;&#36890;&#36807;&#31995;&#32479;&#22312;&#24050;&#30693;&#24378;&#36843;&#20989;&#25968;&#24433;&#21709;&#19979;&#30340;&#35266;&#27979;&#65292;&#35782;&#21035;&#21644;&#25233;&#21046;&#26410;&#30693;&#31995;&#32479;&#30340;&#26410;&#30693;&#24178;&#25200;&#12290;&#36825;&#39033;&#26041;&#27861;&#23545;&#35757;&#32451;&#20989;&#25968;&#26377;&#38750;&#24120;&#28201;&#21644;&#30340;&#38480;&#21046;&#65292;&#33021;&#22815;&#31283;&#20581;&#22320;&#35782;&#21035;&#21644;&#25233;&#21046;&#22823;&#31867;&#21035;&#30340;&#26410;&#30693;&#24178;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35782;&#21035;&#21644;&#25233;&#21046;&#21160;&#21147;&#31995;&#32479;&#20013;&#30340;&#26410;&#30693;&#24178;&#25200;&#26159;&#19968;&#20010;&#22312;&#35768;&#22810;&#19981;&#21516;&#39046;&#22495;&#20013;&#24212;&#29992;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20165;&#22522;&#20110;&#31995;&#32479;&#22312;&#24050;&#30693;&#24378;&#36843;&#20989;&#25968;&#24433;&#21709;&#19979;&#30340;&#20808;&#21069;&#35266;&#27979;&#26469;&#35782;&#21035;&#21644;&#25233;&#21046;&#26410;&#30693;&#31995;&#32479;&#30340;&#26410;&#30693;&#24178;&#25200;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#23545;&#35757;&#32451;&#20989;&#25968;&#26377;&#38750;&#24120;&#28201;&#21644;&#30340;&#38480;&#21046;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#31283;&#20581;&#22320;&#35782;&#21035;&#21644;&#25233;&#21046;&#22823;&#31867;&#21035;&#30340;&#26410;&#30693;&#24178;&#25200;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#31034;&#20363;&#35828;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#26696;&#65292;&#20854;&#20013;&#35782;&#21035;&#21644;&#25233;&#21046;&#20102; Lorenz &#31995;&#32479;&#30340;&#28151;&#27788;&#24178;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifying and suppressing unknown disturbances to dynamical systems is a problem with applications in many different fields. In this Letter, we present a model-free method to identify and suppress an unknown disturbance to an unknown system based only on previous observations of the system under the influence of a known forcing function. We find that, under very mild restrictions on the training function, our method is able to robustly identify and suppress a large class of unknown disturbances. We illustrate our scheme with an example where a chaotic disturbance to the Lorenz system is identified and suppressed.
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#23398;&#20064;&#19982;&#29289;&#29702;&#21551;&#21457;&#24335;&#36873;&#25321;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#21487;&#24494;&#20998;&#28237;&#27969;&#27169;&#22411;&#65292;&#22312;&#22823;&#28065;&#27169;&#25311;&#20013;&#26377;&#25928;&#22320;&#24314;&#27169;&#23376;&#32593;&#26684;&#23610;&#24230;&#28237;&#27969;&#65292;&#24182;&#33021;&#25512;&#24191;&#21040;&#22810;&#31181;&#27969;&#21160;&#37197;&#32622;&#12290;</title><link>http://arxiv.org/abs/2307.03683</link><description>&lt;p&gt;
&#21487;&#24494;&#20998;&#28237;&#27969;
&lt;/p&gt;
&lt;p&gt;
Differentiable Turbulence. (arXiv:2307.03683v1 [physics.flu-dyn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03683
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#19982;&#29289;&#29702;&#21551;&#21457;&#24335;&#36873;&#25321;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#21487;&#24494;&#20998;&#28237;&#27969;&#27169;&#22411;&#65292;&#22312;&#22823;&#28065;&#27169;&#25311;&#20013;&#26377;&#25928;&#22320;&#24314;&#27169;&#23376;&#32593;&#26684;&#23610;&#24230;&#28237;&#27969;&#65292;&#24182;&#33021;&#25512;&#24191;&#21040;&#22810;&#31181;&#27969;&#21160;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#25913;&#36827;&#22823;&#28065;&#27169;&#25311;(SGS)&#20013;&#23376;&#32593;&#26684;&#23610;&#24230;&#28237;&#27969;&#38381;&#21512;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#26041;&#38754;&#36234;&#26469;&#36234;&#20855;&#26377;&#28508;&#21147;&#12290;&#25105;&#20204;&#21033;&#29992;&#21487;&#24494;&#20998;&#28237;&#27969;&#30340;&#27010;&#24565;&#65292;&#22312;&#20351;&#29992;&#29289;&#29702;&#21551;&#21457;&#24335;&#36873;&#25321;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#19982;&#31471;&#21040;&#31471;&#21487;&#24494;&#20998;&#27714;&#35299;&#22120;&#30456;&#32467;&#21512;&#30340;&#24773;&#20917;&#19979;&#65292;&#23398;&#20064;&#20102;&#36866;&#29992;&#20110;&#20108;&#32500;&#28237;&#27969;&#27969;&#21160;&#30340;&#39640;&#25928;&#19988;&#22810;&#21151;&#33021;&#30340;SGS&#27169;&#22411;&#12290;&#25105;&#20204;&#23545;&#25152;&#36873;&#25321;&#30340;&#26550;&#26500;&#20013;&#30340;&#24402;&#32435;&#20559;&#24046;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#21457;&#29616;&#21253;&#21547;&#23567;&#23610;&#24230;&#30340;&#38750;&#23616;&#37096;&#29305;&#24449;&#23545;&#20110;&#26377;&#25928;&#30340;SGS&#24314;&#27169;&#26368;&#20026;&#20851;&#38190;&#65292;&#32780;&#22823;&#23610;&#24230;&#29305;&#24449;&#21487;&#20197;&#25552;&#39640;&#20107;&#21518;&#35299;&#22330;&#30340;&#28857;&#31934;&#30830;&#24230;&#12290;&#28388;&#27874;&#36895;&#24230;&#26799;&#24230;&#24352;&#37327;&#21487;&#20197;&#36890;&#36807;&#23558;&#36755;&#20837;&#21644;&#36755;&#20986;&#20998;&#35299;&#20026;&#21508;&#21521;&#21516;&#24615;&#12289;&#20559;&#31163;&#21516;&#24615;&#21644;&#21453;&#23545;&#31216;&#20998;&#37327;&#26469;&#30452;&#25509;&#26144;&#23556;&#21040;SGS&#24212;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#35813;&#27169;&#22411;&#21487;&#20197;&#25512;&#24191;&#21040;&#22810;&#31181;&#27969;&#21160;&#37197;&#32622;&#65292;&#21253;&#25324;&#36739;&#39640;&#21644;&#36739;&#20302;&#30340;&#36895;&#24230; Reynolds &#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning is increasingly becoming a promising pathway to improving the accuracy of sub-grid scale (SGS) turbulence closure models for large eddy simulations (LES). We leverage the concept of differentiable turbulence, whereby an end-to-end differentiable solver is used in combination with physics-inspired choices of deep learning architectures to learn highly effective and versatile SGS models for two-dimensional turbulent flow. We perform an in-depth analysis of the inductive biases in the chosen architectures, finding that the inclusion of small-scale non-local features is most critical to effective SGS modeling, while large-scale features can improve pointwise accuracy of the a-posteriori solution field. The filtered velocity gradient tensor can be mapped directly to the SGS stress via decomposition of the inputs and outputs into isotropic, deviatoric, and anti-symmetric components. We see that the model can generalize to a variety of flow configurations, including higher and l
&lt;/p&gt;</description></item><item><title>&#19978;&#36848;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20449;&#20154;&#24037;&#26234;&#33021;&#20934;&#21017;--AI&#35780;&#20272;&#30446;&#24405;&#65292;&#26088;&#22312;&#30830;&#20445;AI&#24212;&#29992;&#25353;&#29031;&#39640;&#36136;&#37327;&#26631;&#20934;&#24320;&#21457;&#65292;&#24182;&#26377;&#25928;&#38450;&#33539;&#26032;&#30340;AI&#39118;&#38505;&#65292;&#20174;&#32780;&#20351;AI&#21457;&#25381;&#20854;&#28508;&#21147;&#65292;&#20943;&#23569;&#19981;&#20844;&#24179;&#23545;&#24453;&#20010;&#20307;&#30340;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2307.03681</link><description>&lt;p&gt;
&#21487;&#20449;&#20154;&#24037;&#26234;&#33021;&#20934;&#21017;--AI&#35780;&#20272;&#30446;&#24405;
&lt;/p&gt;
&lt;p&gt;
Guideline for Trustworthy Artificial Intelligence -- AI Assessment Catalog. (arXiv:2307.03681v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03681
&lt;/p&gt;
&lt;p&gt;
&#19978;&#36848;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20449;&#20154;&#24037;&#26234;&#33021;&#20934;&#21017;--AI&#35780;&#20272;&#30446;&#24405;&#65292;&#26088;&#22312;&#30830;&#20445;AI&#24212;&#29992;&#25353;&#29031;&#39640;&#36136;&#37327;&#26631;&#20934;&#24320;&#21457;&#65292;&#24182;&#26377;&#25928;&#38450;&#33539;&#26032;&#30340;AI&#39118;&#38505;&#65292;&#20174;&#32780;&#20351;AI&#21457;&#25381;&#20854;&#28508;&#21147;&#65292;&#20943;&#23569;&#19981;&#20844;&#24179;&#23545;&#24453;&#20010;&#20307;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#36827;&#23637;&#65292;&#24182;&#20195;&#34920;&#30528;&#19968;&#39033;&#23545;&#32463;&#27982;&#21644;&#31038;&#20250;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#24456;&#26126;&#26174;&#65292;&#21482;&#26377;&#25353;&#29031;&#39640;&#36136;&#37327;&#26631;&#20934;&#24320;&#21457;AI&#24212;&#29992;&#24182;&#26377;&#25928;&#38450;&#33539;&#26032;&#30340;AI&#39118;&#38505;&#65292;AI&#21644;&#22522;&#20110;&#20854;&#30340;&#21830;&#19994;&#27169;&#24335;&#25165;&#33021;&#20805;&#20998;&#21457;&#25381;&#20854;&#28508;&#21147;&#12290;&#20363;&#22914;&#65292;&#24403;&#22788;&#29702;&#20010;&#20154;&#25968;&#25454;&#65288;&#20363;&#22914;&#65292;&#25903;&#25345;&#20449;&#36151;&#20511;&#27454;&#25110;&#21592;&#24037;&#25307;&#32856;&#20915;&#31574;&#65289;&#26102;&#65292;AI&#23384;&#22312;&#19981;&#20844;&#24179;&#23545;&#24453;&#20010;&#20307;&#30340;&#39118;&#38505;&#12290;&#36825;&#20123;&#26032;&#39118;&#38505;&#30340;&#20986;&#29616;&#19982;AI&#24212;&#29992;&#30340;&#34892;&#20026;&#23494;&#20999;&#30456;&#20851;&#65292;&#23588;&#20854;&#26159;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#24212;&#29992;&#65292;&#20854;&#34892;&#20026;&#22522;&#26412;&#19978;&#26159;&#20174;&#22823;&#37327;&#25968;&#25454;&#20013;&#23398;&#20064;&#32780;&#26469;&#65292;&#32780;&#19981;&#26159;&#30001;&#22266;&#23450;&#32534;&#31243;&#35268;&#21017;&#39044;&#20808;&#30830;&#23450;&#30340;&#12290;&#22240;&#27492;&#65292;AI&#24212;&#29992;&#30340;&#21487;&#20449;&#24230;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#65292;&#26159;&#25919;&#27835;&#12289;&#21830;&#19994;&#21644;&#31038;&#20250;&#21033;&#30410;&#30456;&#20851;&#32773;&#20247;&#22810;&#37325;&#35201;&#20986;&#29256;&#29289;&#30340;&#20027;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI) has made impressive progress in recent years and represents a key technology that has a crucial impact on the economy and society. However, it is clear that AI and business models based on it can only reach their full potential if AI applications are developed according to high quality standards and are effectively protected against new AI risks. For instance, AI bears the risk of unfair treatment of individuals when processing personal data e.g., to support credit lending or staff recruitment decisions. The emergence of these new risks is closely linked to the fact that the behavior of AI applications, particularly those based on Machine Learning (ML), is essentially learned from large volumes of data and is not predetermined by fixed programmed rules.  Thus, the issue of the trustworthiness of AI applications is crucial and is the subject of numerous major publications by stakeholders in politics, business and society. In addition, there is mutual agreeme
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#19981;&#21487;&#38477;&#35299;&#23567;&#27874;&#21464;&#25442;&#19982;&#23884;&#20837;&#35821;&#20041;&#36793;&#32536;&#33258;&#21160;&#32534;&#30721;&#22120;&#30456;&#32467;&#21512;&#30340;&#26032;&#31574;&#30053;&#65292;&#29992;&#20110;&#25913;&#21892;&#22810;&#35821;&#35328;&#23433;&#20840;&#25514;&#26045;&#21644;&#38477;&#22122;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#25552;&#21462;&#29305;&#24449;&#24182;&#20445;&#30041;&#25968;&#25454;&#20013;&#30340;&#26102;&#38388;&#21644;&#22320;&#29702;&#38142;&#25509;&#65292;&#25104;&#21151;&#25429;&#33719;&#37325;&#35201;&#20449;&#24687;&#65292;&#24182;&#25552;&#39640;&#20102;&#31995;&#32479;&#26816;&#27979;&#24322;&#24120;&#21644;&#21306;&#20998;&#21512;&#27861;&#20869;&#23481;&#19982;&#21361;&#38505;&#23041;&#32961;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.03679</link><description>&lt;p&gt;
&#23884;&#20837;&#35821;&#20041;&#36793;&#32536;&#33258;&#21160;&#32534;&#30721;&#22120;&#20013;&#30340;&#19981;&#21487;&#38477;&#35299;&#23567;&#27874;&#21464;&#25442;&#23545;&#22810;&#35821;&#35328;&#23433;&#20840;&#25913;&#36827;&#21644;&#38477;&#22122;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Undecimated Wavelet Transform for Word Embedded Semantic Marginal Autoencoder in Security improvement and Denoising different Languages. (arXiv:2307.03679v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03679
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#19981;&#21487;&#38477;&#35299;&#23567;&#27874;&#21464;&#25442;&#19982;&#23884;&#20837;&#35821;&#20041;&#36793;&#32536;&#33258;&#21160;&#32534;&#30721;&#22120;&#30456;&#32467;&#21512;&#30340;&#26032;&#31574;&#30053;&#65292;&#29992;&#20110;&#25913;&#21892;&#22810;&#35821;&#35328;&#23433;&#20840;&#25514;&#26045;&#21644;&#38477;&#22122;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#25552;&#21462;&#29305;&#24449;&#24182;&#20445;&#30041;&#25968;&#25454;&#20013;&#30340;&#26102;&#38388;&#21644;&#22320;&#29702;&#38142;&#25509;&#65292;&#25104;&#21151;&#25429;&#33719;&#37325;&#35201;&#20449;&#24687;&#65292;&#24182;&#25552;&#39640;&#20102;&#31995;&#32479;&#26816;&#27979;&#24322;&#24120;&#21644;&#21306;&#20998;&#21512;&#27861;&#20869;&#23481;&#19982;&#21361;&#38505;&#23041;&#32961;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#19981;&#21487;&#38477;&#35299;&#23567;&#27874;&#21464;&#25442;&#19982;&#23884;&#20837;&#35821;&#20041;&#36793;&#32536;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;WESMA&#65289;&#30456;&#32467;&#21512;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#21892;&#23433;&#20840;&#25514;&#26045;&#21644;&#38477;&#22122;&#22810;&#31181;&#35821;&#35328;&#30340;&#26032;&#31574;&#30053;&#12290;&#36825;&#20123;&#31574;&#30053;&#30340;&#25972;&#21512;&#26088;&#22312;&#35299;&#20915;&#25968;&#25454;&#22788;&#29702;&#24212;&#29992;&#20013;&#30340;&#40065;&#26834;&#24615;&#12289;&#38544;&#31169;&#24615;&#21644;&#22810;&#35821;&#35328;&#24615;&#38382;&#39064;&#12290;&#19981;&#21487;&#38477;&#35299;&#23567;&#27874;&#21464;&#25442;&#34987;&#29992;&#20316;&#29305;&#24449;&#25552;&#21462;&#24037;&#20855;&#65292;&#20197;&#35782;&#21035;&#36755;&#20837;&#25968;&#25454;&#20013;&#31361;&#20986;&#30340;&#35821;&#35328;&#27169;&#24335;&#21644;&#32467;&#26500;&#29305;&#24615;&#12290;&#36890;&#36807;&#37319;&#29992;&#36825;&#31181;&#21464;&#25442;&#65292;&#25552;&#35758;&#30340;&#31995;&#32479;&#21487;&#20197;&#22312;&#20445;&#30041;&#25968;&#25454;&#20013;&#30340;&#26102;&#38388;&#21644;&#22320;&#29702;&#38142;&#25509;&#30340;&#21516;&#26102;&#65292;&#25104;&#21151;&#25429;&#33719;&#37325;&#35201;&#20449;&#24687;&#12290;&#36825;&#36890;&#36807;&#22686;&#21152;&#31995;&#32479;&#26816;&#27979;&#24322;&#24120;&#12289;&#21457;&#29616;&#38544;&#34255;&#27169;&#24335;&#20197;&#21450;&#21306;&#20998;&#21512;&#27861;&#20869;&#23481;&#21644;&#21361;&#38505;&#23041;&#32961;&#30340;&#33021;&#21147;&#26469;&#25913;&#21892;&#23433;&#20840;&#25514;&#26045;&#12290;&#23884;&#20837;&#35821;&#20041;&#36793;&#32536;&#33258;&#21160;&#32534;&#30721;&#22120;&#36824;&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#26234;&#33021;&#26694;&#26550;&#26469;&#38477;&#32500;&#21644;&#38477;&#22122;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
By combining the undecimated wavelet transform within a Word Embedded Semantic Marginal Autoencoder (WESMA), this research study provides a novel strategy for improving security measures and denoising multiple languages. The incorporation of these strategies is intended to address the issues of robustness, privacy, and multilingualism in data processing applications. The undecimated wavelet transform is used as a feature extraction tool to identify prominent language patterns and structural qualities in the input data. The proposed system may successfully capture significant information while preserving the temporal and geographical links within the data by employing this transform. This improves security measures by increasing the system's ability to detect abnormalities, discover hidden patterns, and distinguish between legitimate content and dangerous threats. The Word Embedded Semantic Marginal Autoencoder also functions as an intelligent framework for dimensionality and noise redu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#34920;&#31034;&#20960;&#20309;&#21644;&#31354;&#38388;&#20851;&#31995;&#25991;&#26412;&#25551;&#36848;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#21457;&#29616;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#25429;&#25417;&#19968;&#20123;&#31354;&#38388;&#20851;&#31995;&#65292;&#20294;&#22312;&#20272;&#35745;&#25968;&#20540;&#21644;&#26816;&#32034;&#30456;&#20851;&#23545;&#35937;&#26041;&#38754;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.03678</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#34920;&#31034;&#20960;&#20309;&#21644;&#31354;&#38388;&#20851;&#31995;&#30340;&#25991;&#26412;&#25551;&#36848;&#20013;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Effectiveness of Large Language Models in Representing Textual Descriptions of Geometry and Spatial Relations. (arXiv:2307.03678v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03678
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#34920;&#31034;&#20960;&#20309;&#21644;&#31354;&#38388;&#20851;&#31995;&#25991;&#26412;&#25551;&#36848;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#21457;&#29616;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#25429;&#25417;&#19968;&#20123;&#31354;&#38388;&#20851;&#31995;&#65292;&#20294;&#22312;&#20272;&#35745;&#25968;&#20540;&#21644;&#26816;&#32034;&#30456;&#20851;&#23545;&#35937;&#26041;&#38754;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#34920;&#31034;&#20960;&#20309;&#21644;&#31354;&#38388;&#20851;&#31995;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#21033;&#29992;&#21253;&#25324;GPT-2&#21644;BERT&#22312;&#20869;&#30340;LLMs&#23545;&#20960;&#20309;&#30340;&#25991;&#26412;&#26684;&#24335;&#36827;&#34892;&#32534;&#30721;&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#30340;&#23884;&#20837;&#36755;&#20837;&#20998;&#31867;&#22120;&#21644;&#22238;&#24402;&#22120;&#65292;&#20197;&#35780;&#20272;LLMs&#29983;&#25104;&#30340;&#23884;&#20837;&#22312;&#20960;&#20309;&#23646;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#23613;&#31649;LLMs&#29983;&#25104;&#30340;&#23884;&#20837;&#33021;&#22815;&#20445;&#30041;&#20960;&#20309;&#31867;&#22411;&#24182;&#25429;&#25417;&#19968;&#20123;&#31354;&#38388;&#20851;&#31995;&#65288;&#20934;&#30830;&#24230;&#39640;&#36798;73%&#65289;&#65292;&#20294;&#22312;&#20272;&#35745;&#25968;&#20540;&#21644;&#26816;&#32034;&#31354;&#38388;&#30456;&#20851;&#23545;&#35937;&#26041;&#38754;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#20984;&#26174;&#20102;&#22312;&#25429;&#25417;&#24213;&#23618;&#22320;&#29702;&#31354;&#38388;&#25968;&#25454;&#30340;&#32454;&#24494;&#24046;&#21035;&#21644;&#22797;&#26434;&#24615;&#20197;&#21450;&#25972;&#21512;&#39046;&#22495;&#30693;&#35782;&#20197;&#25903;&#25345;&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#21508;&#31181;GeoAI&#24212;&#29992;&#26041;&#38754;&#30340;&#25913;&#36827;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research focuses on assessing the ability of large language models (LLMs) in representing geometries and their spatial relations. We utilize LLMs including GPT-2 and BERT to encode the well-known text (WKT) format of geometries and then feed their embeddings into classifiers and regressors to evaluate the effectiveness of the LLMs-generated embeddings for geometric attributes. The experiments demonstrate that while the LLMs-generated embeddings can preserve geometry types and capture some spatial relations (up to 73% accuracy), challenges remain in estimating numeric values and retrieving spatially related objects. This research highlights the need for improvement in terms of capturing the nuances and complexities of the underlying geospatial data and integrating domain knowledge to support various GeoAI applications using foundation models.
&lt;/p&gt;</description></item><item><title>GeoPhy&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#12289;&#23436;&#20840;&#21487;&#24494;&#30340;&#31995;&#32479;&#21457;&#32946;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#36830;&#32493;&#20960;&#20309;&#31354;&#38388;&#20013;&#34920;&#31034;&#25299;&#25169;&#20998;&#24067;&#65292;&#23454;&#29616;&#20102;&#21487;&#25193;&#23637;&#30340;&#21464;&#20998;&#25512;&#26029;&#65292;&#20811;&#26381;&#20102;&#19981;&#38480;&#21046;&#25299;&#25169;&#32467;&#26500;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.03675</link><description>&lt;p&gt;
GeoPhy: &#21033;&#29992;&#20960;&#20309;&#26799;&#24230;&#23454;&#29616;&#21487;&#24494;&#20998;&#30340;&#31995;&#32479;&#21457;&#32946;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
GeoPhy: Differentiable Phylogenetic Inference via Geometric Gradients of Tree Topologies. (arXiv:2307.03675v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03675
&lt;/p&gt;
&lt;p&gt;
GeoPhy&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#12289;&#23436;&#20840;&#21487;&#24494;&#30340;&#31995;&#32479;&#21457;&#32946;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#36830;&#32493;&#20960;&#20309;&#31354;&#38388;&#20013;&#34920;&#31034;&#25299;&#25169;&#20998;&#24067;&#65292;&#23454;&#29616;&#20102;&#21487;&#25193;&#23637;&#30340;&#21464;&#20998;&#25512;&#26029;&#65292;&#20811;&#26381;&#20102;&#19981;&#38480;&#21046;&#25299;&#25169;&#32467;&#26500;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31995;&#32479;&#21457;&#32946;&#25512;&#26029;&#26159;&#22312;&#20998;&#23376;&#36827;&#21270;&#27169;&#22411;&#22522;&#30784;&#19978;&#36827;&#34892;&#30340;&#65292;&#23427;&#23545;&#20110;&#29702;&#35299;&#29983;&#29289;&#25968;&#25454;&#20013;&#30340;&#36827;&#21270;&#20851;&#31995;&#33267;&#20851;&#37325;&#35201;&#12290;&#32771;&#34385;&#21040;&#36827;&#21270;&#26641;&#21464;&#37327;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#21253;&#25324;&#26641;&#25299;&#25169;&#32467;&#26500;&#21644;&#20998;&#25903;&#19978;&#30340;&#36827;&#21270;&#36317;&#31163;&#65292;&#23545;&#20110;&#20934;&#30830;&#22320;&#20174;&#20998;&#23376;&#25968;&#25454;&#20013;&#25512;&#26029;&#29289;&#31181;&#20851;&#31995;&#20197;&#21450;&#38656;&#35201;&#36827;&#34892;&#21464;&#37327;&#36793;&#32536;&#21270;&#30340;&#20219;&#21153;&#26469;&#35828;&#33267;&#20851;&#37325;&#35201;&#12290;&#21464;&#20998;&#36125;&#21494;&#26031;&#26041;&#27861;&#26159;&#24320;&#21457;&#21487;&#25193;&#23637;&#12289;&#23454;&#29992;&#27169;&#22411;&#30340;&#20851;&#38190;&#65292;&#28982;&#32780;&#65292;&#22312;&#19981;&#38480;&#21046;&#21487;&#33021;&#30340;&#26641;&#25299;&#25169;&#32467;&#26500;&#30340;&#32452;&#21512;&#25968;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#31995;&#32479;&#21457;&#32946;&#25512;&#26029;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#23436;&#20840;&#21487;&#24494;&#30340;&#31995;&#32479;&#21457;&#32946;&#25512;&#26029;&#20844;&#24335;&#65292;&#21033;&#29992;&#36830;&#32493;&#20960;&#20309;&#31354;&#38388;&#20013;&#30340;&#25299;&#25169;&#20998;&#24067;&#26469;&#34920;&#31034;&#12290;&#36890;&#36807;&#23545;&#35774;&#35745;&#31354;&#38388;&#21644;&#28176;&#36817;&#30697;&#30340;&#23454;&#38469;&#32771;&#34385;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;GeoPhy&#21487;&#20197;&#23454;&#29616;&#21464;&#20998;&#25512;&#26029;&#32780;&#19981;&#38480;&#21046;&#25299;&#25169;&#32467;&#26500;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Phylogenetic inference, grounded in molecular evolution models, is essential for understanding the evolutionary relationships in biological data. Accounting for the uncertainty of phylogenetic tree variables, which include tree topologies and evolutionary distances on branches, is crucial for accurately inferring species relationships from molecular data and tasks requiring variable marginalization. Variational Bayesian methods are key to developing scalable, practical models; however, it remains challenging to conduct phylogenetic inference without restricting the combinatorially vast number of possible tree topologies. In this work, we introduce a novel, fully differentiable formulation of phylogenetic inference that leverages a unique representation of topological distributions in continuous geometric spaces. Through practical considerations on design spaces and control variates for gradient estimations, our approach, GeoPhy, enables variational inference without limiting the topolo
&lt;/p&gt;</description></item><item><title>[SF]$^2$M&#26159;&#19968;&#31181;&#26080;&#38656;&#27169;&#25311;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25512;&#26029;&#38543;&#26426;&#21160;&#21147;&#23398;&#12290;&#23427;&#23558;&#36830;&#32493;&#26102;&#38388;&#38543;&#26426;&#29983;&#25104;&#24314;&#27169;&#35299;&#37322;&#20026;Schr\"odinger&#26725;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#38745;&#24577;&#29109;&#27491;&#21017;&#21270;&#26368;&#20248;&#20256;&#36755;&#26469;&#39640;&#25928;&#23398;&#20064;&#12290;&#22312;&#23398;&#20064;&#32454;&#32990;&#21160;&#21147;&#23398;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.03672</link><description>&lt;p&gt;
&#36890;&#36807;&#24471;&#20998;&#21644;&#27969;&#21305;&#37197;&#23454;&#29616;&#26080;&#38656;&#27169;&#25311;&#30340;Schr\"odinger&#26725;
&lt;/p&gt;
&lt;p&gt;
Simulation-free Schr\"odinger bridges via score and flow matching. (arXiv:2307.03672v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03672
&lt;/p&gt;
&lt;p&gt;
[SF]$^2$M&#26159;&#19968;&#31181;&#26080;&#38656;&#27169;&#25311;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25512;&#26029;&#38543;&#26426;&#21160;&#21147;&#23398;&#12290;&#23427;&#23558;&#36830;&#32493;&#26102;&#38388;&#38543;&#26426;&#29983;&#25104;&#24314;&#27169;&#35299;&#37322;&#20026;Schr\"odinger&#26725;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#38745;&#24577;&#29109;&#27491;&#21017;&#21270;&#26368;&#20248;&#20256;&#36755;&#26469;&#39640;&#25928;&#23398;&#20064;&#12290;&#22312;&#23398;&#20064;&#32454;&#32990;&#21160;&#21147;&#23398;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#26080;&#38656;&#27169;&#25311;&#30340;&#24471;&#20998;&#21644;&#27969;&#21305;&#37197;&#65288;[SF]$^2$M&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26080;&#38656;&#27169;&#25311;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#29992;&#20110;&#25512;&#26029;&#32473;&#23450;&#26469;&#33258;&#20219;&#24847;&#20998;&#24067;&#30340;&#26410;&#37197;&#23545;&#28304;&#21644;&#30446;&#26631;&#26679;&#26412;&#30340;&#38543;&#26426;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25512;&#24191;&#20102;&#29992;&#20110;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#30340;&#24471;&#20998;&#21305;&#37197;&#25439;&#22833;&#21644;&#26368;&#36817;&#25552;&#20986;&#30340;&#29992;&#20110;&#35757;&#32451;&#36830;&#32493;&#24402;&#19968;&#21270;&#27969;&#30340;&#27969;&#21305;&#37197;&#25439;&#22833;&#12290;[SF]$^2$M&#23558;&#36830;&#32493;&#26102;&#38388;&#38543;&#26426;&#29983;&#25104;&#24314;&#27169;&#35299;&#37322;&#20026;Schr\"odinger&#26725;&#38382;&#39064;&#12290;&#23427;&#20381;&#36182;&#20110;&#38745;&#24577;&#29109;&#27491;&#21017;&#21270;&#26368;&#20248;&#20256;&#36755;&#25110;&#23567;&#25209;&#37327;&#36817;&#20284;&#65292;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;Schr\"odinger&#26725;&#65292;&#32780;&#26080;&#38656;&#27169;&#25311;&#23398;&#20064;&#36807;&#31243;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19982;&#20808;&#21069;&#30340;&#22522;&#20110;&#27169;&#25311;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;[SF]$^2$M&#26356;&#39640;&#25928;&#24182;&#25552;&#20379;&#20102;&#26356;&#20934;&#30830;&#30340;Schr\"odinger&#26725;&#35299;&#20915;&#26041;&#26696;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;[SF]$^2$M&#24212;&#29992;&#20110;&#20174;&#24555;&#29031;&#25968;&#25454;&#20013;&#23398;&#20064;&#32454;&#32990;&#21160;&#21147;&#23398;&#30340;&#38382;&#39064;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;[SF]$^2$M&#26159;&#39318;&#20010;&#33021;&#22815;&#20934;&#30830;&#24314;&#27169;&#39640;&#32500;&#32454;&#32990;&#21160;&#21147;&#23398;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present simulation-free score and flow matching ([SF]$^2$M), a simulation-free objective for inferring stochastic dynamics given unpaired source and target samples drawn from arbitrary distributions. Our method generalizes both the score-matching loss used in the training of diffusion models and the recently proposed flow matching loss used in the training of continuous normalizing flows. [SF]$^2$M interprets continuous-time stochastic generative modeling as a Schr\"odinger bridge (SB) problem. It relies on static entropy-regularized optimal transport, or a minibatch approximation, to efficiently learn the SB without simulating the learned stochastic process. We find that [SF]$^2$M is more efficient and gives more accurate solutions to the SB problem than simulation-based methods from prior work. Finally, we apply [SF]$^2$M to the problem of learning cell dynamics from snapshot data. Notably, [SF]$^2$M is the first method to accurately model cell dynamics in high dimensions and can 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Grab-UCB&#31639;&#27861;&#29992;&#20110;&#22312;&#32447;&#23398;&#20064;&#22823;&#35268;&#27169;&#32593;&#32476;&#20013;&#26368;&#20248;&#30340;&#28304;&#25918;&#32622;&#20301;&#32622;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#36866;&#24212;&#30340;&#22270;&#35789;&#20856;&#27169;&#22411;&#26469;&#25551;&#36848;&#32593;&#32476;&#36807;&#31243;&#65292;&#24182;&#21033;&#29992;&#31232;&#30095;&#35889;&#34920;&#31034;&#23454;&#29616;&#20102;&#25968;&#25454;&#39640;&#25928;&#30340;&#23398;&#20064;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2307.03641</link><description>&lt;p&gt;
&#22312;&#32447;&#32593;&#32476;&#28304;&#20248;&#21270;&#19982;&#22270;&#20869;&#26680;MAB
&lt;/p&gt;
&lt;p&gt;
Online Network Source Optimization with Graph-Kernel MAB. (arXiv:2307.03641v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03641
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Grab-UCB&#31639;&#27861;&#29992;&#20110;&#22312;&#32447;&#23398;&#20064;&#22823;&#35268;&#27169;&#32593;&#32476;&#20013;&#26368;&#20248;&#30340;&#28304;&#25918;&#32622;&#20301;&#32622;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#36866;&#24212;&#30340;&#22270;&#35789;&#20856;&#27169;&#22411;&#26469;&#25551;&#36848;&#32593;&#32476;&#36807;&#31243;&#65292;&#24182;&#21033;&#29992;&#31232;&#30095;&#35889;&#34920;&#31034;&#23454;&#29616;&#20102;&#25968;&#25454;&#39640;&#25928;&#30340;&#23398;&#20064;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Grab-UCB&#65292;&#19968;&#31181;&#22522;&#20110;&#22270;&#20869;&#26680;&#30340;&#22810;&#33218;&#36172;&#21338;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#32447;&#23398;&#20064;&#22312;&#22823;&#35268;&#27169;&#32593;&#32476;&#20013;&#26368;&#20248;&#30340;&#28304;&#25918;&#32622;&#20301;&#32622;&#65292;&#20197;&#26368;&#22823;&#21270;&#20808;&#39564;&#26410;&#30693;&#32593;&#32476;&#36807;&#31243;&#25152;&#33719;&#24471;&#30340;&#22870;&#21169;&#12290;&#30001;&#20110;&#19981;&#30830;&#23450;&#24615;&#65292;&#38656;&#35201;&#22312;&#32447;&#23398;&#20064;&#65292;&#28982;&#32780;&#36825;&#22312;&#32500;&#24230;&#28798;&#38590;&#20013;&#21463;&#21040;&#20102;&#24433;&#21709;&#12290;&#20026;&#20102;&#23454;&#29616;&#26679;&#26412;&#25928;&#29575;&#65292;&#25105;&#20204;&#20351;&#29992;&#33258;&#36866;&#24212;&#30340;&#22270;&#35789;&#20856;&#27169;&#22411;&#26469;&#25551;&#36848;&#32593;&#32476;&#36807;&#31243;&#65292;&#36825;&#36890;&#24120;&#23548;&#33268;&#31232;&#30095;&#30340;&#35889;&#34920;&#31034;&#12290;&#36825;&#20351;&#24471;&#25968;&#25454;&#39640;&#25928;&#30340;&#23398;&#20064;&#26694;&#26550;&#25104;&#20026;&#21487;&#33021;&#65292;&#20854;&#23398;&#20064;&#29575;&#19982;&#35889;&#34920;&#31034;&#27169;&#22411;&#30340;&#32500;&#24230;&#30456;&#20851;&#65292;&#32780;&#19981;&#26159;&#32593;&#32476;&#30340;&#32500;&#24230;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Grab-UCB&#65292;&#19968;&#31181;&#22312;&#32447;&#39034;&#24207;&#20915;&#31574;&#31574;&#30053;&#65292;&#23427;&#22312;&#20248;&#21270;&#34892;&#21160;&#31574;&#30053;&#30340;&#21516;&#26102;&#23398;&#20064;&#35889;&#34920;&#31034;&#30340;&#21442;&#25968;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#19982;&#32593;&#32476;&#21442;&#25968;&#30456;&#20851;&#30340;&#24615;&#33021;&#20445;&#35777;&#65292;&#36825;&#36827;&#19968;&#27493;&#24433;&#21709;&#39034;&#24207;&#20915;&#31574;&#31574;&#30053;&#30340;&#23398;&#20064;&#26354;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Grab-UCB, a graph-kernel multi-arms bandit algorithm to learn online the optimal source placement in large scale networks, such that the reward obtained from a priori unknown network processes is maximized. The uncertainty calls for online learning, which suffers however from the curse of dimensionality. To achieve sample efficiency, we describe the network processes with an adaptive graph dictionary model, which typically leads to sparse spectral representations. This enables a data-efficient learning framework, whose learning rate scales with the dimension of the spectral representation model instead of the one of the network. We then propose Grab-UCB, an online sequential decision strategy that learns the parameters of the spectral representation while optimizing the action strategy. We derive the performance guarantees that depend on network parameters, which further influence the learning curve of the sequential decision strategy We introduce a computationally simplifie
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19982;&#31070;&#32463;ODE&#30456;&#20851;&#30340;LPV&#31995;&#32479;&#30340;PAC&#30028;&#38480;&#65292;&#35813;&#30028;&#38480;&#19981;&#20381;&#36182;&#20110;&#31215;&#20998;&#21306;&#38388;&#12290;</title><link>http://arxiv.org/abs/2307.03630</link><description>&lt;p&gt;
&#19982;&#31070;&#32463;ODE&#30456;&#20851;&#30340;&#36830;&#32493;&#32447;&#24615;&#21442;&#25968;&#21464;&#21270;&#31995;&#32479;&#30340;PAC&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
PAC bounds of continuous Linear Parameter-Varying systems related to neural ODEs. (arXiv:2307.03630v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03630
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19982;&#31070;&#32463;ODE&#30456;&#20851;&#30340;LPV&#31995;&#32479;&#30340;PAC&#30028;&#38480;&#65292;&#35813;&#30028;&#38480;&#19981;&#20381;&#36182;&#20110;&#31215;&#20998;&#21306;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36830;&#32493;&#26102;&#38388;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#23398;&#20064;&#32447;&#24615;&#21442;&#25968;&#21464;&#21270;&#65288;LPV&#65289;&#31995;&#32479;&#19978;&#30340;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;&#31070;&#32463;ODE&#65289;&#30340;&#38382;&#39064;&#12290;LPV&#31995;&#32479;&#21253;&#21547;&#21452;&#32447;&#24615;&#31995;&#32479;&#65292;&#24050;&#30693;&#23545;&#20110;&#38750;&#32447;&#24615;&#31995;&#32479;&#26159;&#20840;&#23616;&#36924;&#36817;&#22120;&#12290;&#27492;&#22806;&#65292;&#22823;&#37096;&#20998;&#31070;&#32463;ODE&#21487;&#20197;&#23884;&#20837;&#21040;LPV&#31995;&#32479;&#20013;&#12290;&#20316;&#20026;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19982;&#31070;&#32463;ODE&#30456;&#20851;&#30340;LPV&#31995;&#32479;&#31283;&#23450;&#24615;&#19979;&#30340;&#21487;&#33021;&#36817;&#20284;&#27491;&#30830;&#65288;PAC&#65289;&#30028;&#38480;&#12290;&#25152;&#24471;&#21040;&#30340;&#30028;&#38480;&#30340;&#20248;&#28857;&#26159;&#23427;&#20204;&#19981;&#20381;&#36182;&#20110;&#31215;&#20998;&#21306;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of learning Neural Ordinary Differential Equations (neural ODEs) within the context of Linear Parameter-Varying (LPV) systems in continuous-time. LPV systems contain bilinear systems which are known to be universal approximators for non-linear systems. Moreover, a large class of neural ODEs can be embedded into LPV systems. As our main contribution we provide Probably Approximately Correct (PAC) bounds under stability for LPV systems related to neural ODEs. The resulting bounds have the advantage that they do not depend on the integration interval.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476; (GNNs) &#20316;&#20026;&#25968;&#25454;&#22686;&#24378;&#26469;&#35299;&#20915;&#22810;&#35270;&#35282;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#8220;&#20919;&#21551;&#21160;&#8221;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;GNN-based&#29305;&#24449;&#33021;&#22815;&#25429;&#25417;&#22797;&#26434;&#30340;&#36328;&#24207;&#21015;&#20851;&#31995;&#65292;&#24182;&#19988;&#21487;&#20197;&#19982;&#39044;&#27979;&#20219;&#21153;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.03595</link><description>&lt;p&gt;
GEANN: &#22810;&#35270;&#35282;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#21487;&#25193;&#23637;&#22270;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
GEANN: Scalable Graph Augmentations for Multi-Horizon Time Series Forecasting. (arXiv:2307.03595v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476; (GNNs) &#20316;&#20026;&#25968;&#25454;&#22686;&#24378;&#26469;&#35299;&#20915;&#22810;&#35270;&#35282;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#8220;&#20919;&#21551;&#21160;&#8221;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;GNN-based&#29305;&#24449;&#33021;&#22815;&#25429;&#25417;&#22797;&#26434;&#30340;&#36328;&#24207;&#21015;&#20851;&#31995;&#65292;&#24182;&#19988;&#21487;&#20197;&#19982;&#39044;&#27979;&#20219;&#21153;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22810;&#35270;&#35282;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#24471;&#21040;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#65292;&#23588;&#20854;&#26159;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#20934;&#30830;&#39044;&#27979;&#65292;&#36825;&#20123;&#22797;&#26434;&#27169;&#22411;&#36890;&#24120;&#20381;&#36182;&#20110;&#22823;&#37327;&#30340;&#20855;&#26377;&#20016;&#23500;&#21382;&#21490;&#25968;&#25454;&#30340;&#26102;&#38388;&#24207;&#21015;&#31034;&#20363;&#12290;&#19968;&#20010;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#30340;&#30740;&#31350;&#20027;&#39064;&#26159;&#39044;&#27979;&#32570;&#20047;&#36275;&#22815;&#21382;&#21490;&#25968;&#25454;&#30340;&#26102;&#38388;&#24207;&#21015;&#65292;&#36890;&#24120;&#34987;&#31216;&#20026;&#8220;&#20919;&#21551;&#21160;&#8221;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#32780;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476; (GNNs) &#20316;&#20026;&#25968;&#25454;&#22686;&#24378;&#26469;&#22686;&#24378;&#36825;&#20123;&#39044;&#27979;&#22120;&#20351;&#29992;&#30340;&#32534;&#30721;&#22120;&#12290;&#36825;&#20123;&#22522;&#20110;GNN&#30340;&#29305;&#24449;&#33021;&#22815;&#25429;&#25417;&#22797;&#26434;&#30340;&#36328;&#24207;&#21015;&#20851;&#31995;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#29983;&#25104;&#36807;&#31243;&#21487;&#20197;&#19982;&#39044;&#27979;&#20219;&#21153;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#20248;&#21270;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26550;&#26500;&#21487;&#20197;&#20351;&#29992;&#25968;&#25454;&#39537;&#21160;&#25110;&#22522;&#20110;&#39046;&#22495;&#30693;&#35782;&#23450;&#20041;&#30340;&#22270;&#65292;&#33021;&#22815;&#25193;&#23637;&#21040;&#21253;&#21547;&#25968;&#30334;&#19975;&#33410;&#28857;&#30340;&#22810;&#20010;&#38750;&#24120;&#22823;&#30340;&#22270;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Encoder-decoder deep neural networks have been increasingly studied for multi-horizon time series forecasting, especially in real-world applications. However, to forecast accurately, these sophisticated models typically rely on a large number of time series examples with substantial history. A rapidly growing topic of interest is forecasting time series which lack sufficient historical data -- often referred to as the ``cold start'' problem. In this paper, we introduce a novel yet simple method to address this problem by leveraging graph neural networks (GNNs) as a data augmentation for enhancing the encoder used by such forecasters. These GNN-based features can capture complex inter-series relationships, and their generation process can be optimized end-to-end with the forecasting task. We show that our architecture can use either data-driven or domain knowledge-defined graphs, scaling to incorporate information from multiple very large graphs with millions of nodes. In our target app
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#29992;&#20110;&#22788;&#29702;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#22120;&#65288;LQR&#65289;&#38382;&#39064;&#30340;&#21152;&#36895;&#20248;&#21270;&#26694;&#26550;&#65292;&#20998;&#26512;&#20102;SLQR&#21644;OLQR&#38382;&#39064;&#30340;&#25910;&#25947;&#24615;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;LQR&#24615;&#33021;&#20934;&#21017;&#30340;Lipschitz Hessian&#29305;&#24615;&#65292;&#20026;&#29616;&#20195;&#20248;&#21270;&#25216;&#26415;&#30340;&#24212;&#29992;&#25552;&#20379;&#20102;&#37325;&#35201;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2307.03590</link><description>&lt;p&gt;
&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#22120;&#30340;&#21152;&#36895;&#20248;&#21270;&#26223;&#35266;
&lt;/p&gt;
&lt;p&gt;
Accelerated Optimization Landscape of Linear-Quadratic Regulator. (arXiv:2307.03590v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03590
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#29992;&#20110;&#22788;&#29702;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#22120;&#65288;LQR&#65289;&#38382;&#39064;&#30340;&#21152;&#36895;&#20248;&#21270;&#26694;&#26550;&#65292;&#20998;&#26512;&#20102;SLQR&#21644;OLQR&#38382;&#39064;&#30340;&#25910;&#25947;&#24615;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;LQR&#24615;&#33021;&#20934;&#21017;&#30340;Lipschitz Hessian&#29305;&#24615;&#65292;&#20026;&#29616;&#20195;&#20248;&#21270;&#25216;&#26415;&#30340;&#24212;&#29992;&#25552;&#20379;&#20102;&#37325;&#35201;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#22120;&#65288;LQR&#65289;&#26159;&#26368;&#20248;&#25511;&#21046;&#39046;&#22495;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#22788;&#29702;LQR&#38382;&#39064;&#30340;&#19968;&#38454;&#21152;&#36895;&#20248;&#21270;&#26694;&#26550;&#65292;&#24182;&#20998;&#21035;&#32473;&#20986;&#20102;SLQR&#21644;OLQR&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LQR&#24615;&#33021;&#20934;&#21017;&#30340;Lipschitz Hessian&#29305;&#24615;&#65292;&#36825;&#23545;&#20110;&#24212;&#29992;&#29616;&#20195;&#20248;&#21270;&#25216;&#26415;&#26469;&#35828;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#23545;&#20110;SLQR&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36830;&#32493;&#26102;&#38388;&#28151;&#21512;&#21160;&#24577;&#31995;&#32479;&#65292;&#24182;&#35777;&#26126;&#20854;&#35299;&#36712;&#36857;&#25351;&#25968;&#32423;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Linear-quadratic regulator (LQR) is a landmark problem in the field of optimal control, which is the concern of this paper. Generally, LQR is classified into state-feedback LQR (SLQR) and output-feedback LQR (OLQR) based on whether the full state is obtained. It has been suggested in existing literature that both the SLQR and the OLQR could be viewed as \textit{constrained nonconvex matrix optimization} problems in which the only variable to be optimized is the feedback gain matrix. In this paper, we introduce a first-order accelerated optimization framework of handling the LQR problem, and give its convergence analysis for the cases of SLQR and OLQR, respectively.  Specifically, a Lipschiz Hessian property of LQR performance criterion is presented, which turns out to be a crucial property for the application of modern optimization techniques. For the SLQR problem, a continuous-time hybrid dynamic system is introduced, whose solution trajectory is shown to converge exponentially to the
&lt;/p&gt;</description></item><item><title>BOF-UCB&#26159;&#19968;&#31181;&#29992;&#20110;&#38750;&#24179;&#31283;&#29615;&#22659;&#19979;&#30340;&#32972;&#26223;&#32447;&#24615;&#36172;&#21338;&#26426;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#39057;&#29575;&#31639;&#27861;&#65292;&#20854;&#32467;&#21512;&#20102;&#36125;&#21494;&#26031;&#21644;&#39057;&#29575;&#23398;&#27966;&#21407;&#21017;&#65292;&#25552;&#39640;&#20102;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#12290;&#23427;&#21033;&#29992;&#36125;&#21494;&#26031;&#26356;&#26032;&#25512;&#26029;&#21518;&#39564;&#20998;&#24067;&#65292;&#24182;&#20351;&#29992;&#39057;&#29575;&#23398;&#27966;&#26041;&#27861;&#35745;&#31639;&#19978;&#30028;&#20449;&#24515;&#30028;&#20197;&#24179;&#34913;&#25506;&#32034;&#21644;&#24320;&#21457;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;BOF-UCB&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#26159;&#38750;&#24179;&#31283;&#29615;&#22659;&#20013;&#39034;&#24207;&#20915;&#31574;&#30340;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2307.03587</link><description>&lt;p&gt;
BOF-UCB: &#19968;&#31181;&#29992;&#20110;&#38750;&#24179;&#31283;&#29615;&#22659;&#19979;&#30340;&#19978;&#19979;&#30028;&#20449;&#24515;&#31639;&#27861;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#39057;&#29575;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
BOF-UCB: A Bayesian-Optimistic Frequentist Algorithm for Non-Stationary Contextual Bandits. (arXiv:2307.03587v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03587
&lt;/p&gt;
&lt;p&gt;
BOF-UCB&#26159;&#19968;&#31181;&#29992;&#20110;&#38750;&#24179;&#31283;&#29615;&#22659;&#19979;&#30340;&#32972;&#26223;&#32447;&#24615;&#36172;&#21338;&#26426;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#39057;&#29575;&#31639;&#27861;&#65292;&#20854;&#32467;&#21512;&#20102;&#36125;&#21494;&#26031;&#21644;&#39057;&#29575;&#23398;&#27966;&#21407;&#21017;&#65292;&#25552;&#39640;&#20102;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#12290;&#23427;&#21033;&#29992;&#36125;&#21494;&#26031;&#26356;&#26032;&#25512;&#26029;&#21518;&#39564;&#20998;&#24067;&#65292;&#24182;&#20351;&#29992;&#39057;&#29575;&#23398;&#27966;&#26041;&#27861;&#35745;&#31639;&#19978;&#30028;&#20449;&#24515;&#30028;&#20197;&#24179;&#34913;&#25506;&#32034;&#21644;&#24320;&#21457;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;BOF-UCB&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#26159;&#38750;&#24179;&#31283;&#29615;&#22659;&#20013;&#39034;&#24207;&#20915;&#31574;&#30340;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#39057;&#29575;&#19978;&#19979;&#30028;&#20449;&#24515;&#31639;&#27861;&#65288;BOF-UCB&#65289;&#65292;&#29992;&#20110;&#38750;&#24179;&#31283;&#29615;&#22659;&#19979;&#30340;&#38543;&#26426;&#32972;&#26223;&#32447;&#24615;&#36172;&#21338;&#26426;&#12290;&#36125;&#21494;&#26031;&#21644;&#39057;&#29575;&#23398;&#27966;&#21407;&#21017;&#30340;&#29420;&#29305;&#32467;&#21512;&#22686;&#24378;&#20102;&#31639;&#27861;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#30340;&#36866;&#24212;&#24615;&#21644;&#24615;&#33021;&#12290;BOF-UCB&#31639;&#27861;&#21033;&#29992;&#39034;&#24207;&#36125;&#21494;&#26031;&#26356;&#26032;&#25512;&#26029;&#26410;&#30693;&#22238;&#24402;&#21442;&#25968;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#24182;&#38543;&#21518;&#37319;&#29992;&#39057;&#29575;&#23398;&#27966;&#26041;&#27861;&#36890;&#36807;&#26368;&#22823;&#21270;&#21518;&#39564;&#20998;&#24067;&#19978;&#30340;&#26399;&#26395;&#25910;&#30410;&#26469;&#35745;&#31639;&#19978;&#30028;&#20449;&#24515;&#30028;&#65288;UCB&#65289;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;BOF-UCB&#24615;&#33021;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#20013;&#30340;&#32463;&#20856;&#25511;&#21046;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;BOF-UCB&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#22312;&#38750;&#24179;&#31283;&#29615;&#22659;&#20013;&#36827;&#34892;&#39034;&#24207;&#20915;&#31574;&#26159;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel Bayesian-Optimistic Frequentist Upper Confidence Bound (BOF-UCB) algorithm for stochastic contextual linear bandits in non-stationary environments. This unique combination of Bayesian and frequentist principles enhances adaptability and performance in dynamic settings. The BOF-UCB algorithm utilizes sequential Bayesian updates to infer the posterior distribution of the unknown regression parameter, and subsequently employs a frequentist approach to compute the Upper Confidence Bound (UCB) by maximizing the expected reward over the posterior distribution. We provide theoretical guarantees of BOF-UCB's performance and demonstrate its effectiveness in balancing exploration and exploitation on synthetic datasets and classical control tasks in a reinforcement learning setting. Our results show that BOF-UCB outperforms existing methods, making it a promising solution for sequential decision-making in non-stationary environments.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#25968;&#25454;&#25910;&#38598;&#27963;&#21160;&#21644;&#30001;&#26234;&#33021;&#25163;&#26426;&#20256;&#24863;&#22120;&#34893;&#29983;&#32780;&#26469;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#36229;&#36807;45K&#20010;&#26679;&#26412;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#29992;&#25143;&#26085;&#24120;&#27963;&#21160;&#30340;&#20449;&#24687;&#65292;&#24182;&#19988;&#37319;&#29992;&#20102;&#22312;&#37326;&#22806;&#23454;&#39564;&#30340;&#26041;&#24335;&#65292;&#28040;&#38500;&#20102;&#29992;&#25143;&#34892;&#20026;&#30340;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2307.03586</link><description>&lt;p&gt;
ContextLabeler&#25968;&#25454;&#38598;: &#22312;&#37326;&#22806;&#20351;&#29992;&#30340;&#26234;&#33021;&#25163;&#26426;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#29289;&#29702;&#21644;&#34394;&#25311;&#20256;&#24863;&#22120;&#25968;&#25454;&#25910;&#38598;
&lt;/p&gt;
&lt;p&gt;
ContextLabeler Dataset: physical and virtual sensors data collected from smartphone usage in-the-wild. (arXiv:2307.03586v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#25968;&#25454;&#25910;&#38598;&#27963;&#21160;&#21644;&#30001;&#26234;&#33021;&#25163;&#26426;&#20256;&#24863;&#22120;&#34893;&#29983;&#32780;&#26469;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#36229;&#36807;45K&#20010;&#26679;&#26412;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#29992;&#25143;&#26085;&#24120;&#27963;&#21160;&#30340;&#20449;&#24687;&#65292;&#24182;&#19988;&#37319;&#29992;&#20102;&#22312;&#37326;&#22806;&#23454;&#39564;&#30340;&#26041;&#24335;&#65292;&#28040;&#38500;&#20102;&#29992;&#25143;&#34892;&#20026;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#39033;&#25968;&#25454;&#25910;&#38598;&#27963;&#21160;&#21644;&#30001;&#26234;&#33021;&#25163;&#26426;&#20256;&#24863;&#22120;&#34893;&#29983;&#32780;&#26469;&#30340;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#34920;&#24449;&#20102;3&#21517;&#24535;&#24895;&#32773;&#22312;&#20004;&#21608;&#20869;&#30340;&#26085;&#24120;&#29983;&#27963;&#27963;&#21160;&#12290;&#35813;&#25968;&#25454;&#38598;&#20197;&#19968;&#32452;CSV&#25991;&#20214;&#30340;&#24418;&#24335;&#21457;&#24067;&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807;45K&#20010;&#25968;&#25454;&#26679;&#26412;&#65292;&#27599;&#20010;&#26679;&#26412;&#30001;1332&#20010;&#19982;&#21508;&#31181;&#29289;&#29702;&#21644;&#34394;&#25311;&#20256;&#24863;&#22120;&#30456;&#20851;&#30340;&#29305;&#24449;&#32452;&#25104;&#65292;&#21253;&#25324;&#21160;&#20316;&#20256;&#24863;&#22120;&#12289;&#36816;&#34892;&#24212;&#29992;&#31243;&#24207;&#12289;&#38468;&#36817;&#35774;&#22791;&#21644;&#22825;&#27668;&#26465;&#20214;&#31561;&#12290;&#27492;&#22806;&#65292;&#27599;&#20010;&#25968;&#25454;&#26679;&#26412;&#37117;&#24102;&#26377;&#19968;&#20010;&#19982;&#24863;&#30693;&#23454;&#39564;&#26399;&#38388;&#29992;&#25143;&#27963;&#21160;&#21644;&#24773;&#22659;&#30456;&#20851;&#30340;&#22320;&#38754;&#30495;&#20540;&#26631;&#31614;&#65288;&#20363;&#22914;&#65292;&#24037;&#20316;&#12289;&#22312;&#39184;&#21381;&#12289;&#20570;&#36816;&#21160;&#27963;&#21160;&#65289;&#12290;&#20026;&#20102;&#36991;&#20813;&#22312;&#25968;&#25454;&#25910;&#38598;&#36807;&#31243;&#20013;&#24341;&#20837;&#20219;&#20309;&#20559;&#35265;&#65292;&#25105;&#20204;&#22312;&#37326;&#22806;&#36827;&#34892;&#20102;&#24863;&#30693;&#23454;&#39564;&#65292;&#21363;&#20351;&#29992;&#24535;&#24895;&#32773;&#30340;&#35774;&#22791;&#65292;&#24182;&#19988;&#27809;&#26377;&#23545;&#29992;&#25143;&#34892;&#20026;&#23450;&#20041;&#20219;&#20309;&#32422;&#26463;&#12290;&#22240;&#27492;&#65292;&#25910;&#38598;&#21040;&#30340;&#25968;&#25454;&#38598;&#20195;&#34920;&#30528;&#19968;&#20010;&#26377;&#29992;&#30340;&#36164;&#28304;&#26469;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes a data collection campaign and the resulting dataset derived from smartphone sensors characterizing the daily life activities of 3 volunteers in a period of two weeks. The dataset is released as a collection of CSV files containing more than 45K data samples, where each sample is composed by 1332 features related to a heterogeneous set of physical and virtual sensors, including motion sensors, running applications, devices in proximity, and weather conditions. Moreover, each data sample is associated with a ground truth label that describes the user activity and the situation in which she was involved during the sensing experiment (e.g., working, at restaurant, and doing sport activity). To avoid introducing any bias during the data collection, we performed the sensing experiment in-the-wild, that is, by using the volunteers' devices, and without defining any constraint related to the user's behavior. For this reason, the collected dataset represents a useful sourc
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;ProgSyn&#65292;&#31532;&#19968;&#20010;&#21487;&#32534;&#31243;&#30340;&#21512;&#25104;&#34920;&#26684;&#25968;&#25454;&#29983;&#25104;&#31639;&#27861;&#65292;&#23427;&#20801;&#35768;&#23545;&#29983;&#25104;&#30340;&#25968;&#25454;&#36827;&#34892;&#20840;&#38754;&#30340;&#33258;&#23450;&#20041;&#65292;&#24182;&#19988;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#29983;&#25104;&#27169;&#22411;&#26469;&#30830;&#20445;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#21644;&#36981;&#23432;&#33258;&#23450;&#20041;&#35268;&#33539;&#12290;</title><link>http://arxiv.org/abs/2307.03577</link><description>&lt;p&gt;
&#21487;&#32534;&#31243;&#21512;&#25104;&#34920;&#26684;&#25968;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Programmable Synthetic Tabular Data Generation. (arXiv:2307.03577v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03577
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;ProgSyn&#65292;&#31532;&#19968;&#20010;&#21487;&#32534;&#31243;&#30340;&#21512;&#25104;&#34920;&#26684;&#25968;&#25454;&#29983;&#25104;&#31639;&#27861;&#65292;&#23427;&#20801;&#35768;&#23545;&#29983;&#25104;&#30340;&#25968;&#25454;&#36827;&#34892;&#20840;&#38754;&#30340;&#33258;&#23450;&#20041;&#65292;&#24182;&#19988;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#29983;&#25104;&#27169;&#22411;&#26469;&#30830;&#20445;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#21644;&#36981;&#23432;&#33258;&#23450;&#20041;&#35268;&#33539;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#38544;&#31169;&#12289;&#25968;&#25454;&#36136;&#37327;&#21644;&#25968;&#25454;&#20849;&#20139;&#30340;&#38480;&#21046;&#65292;&#22823;&#37327;&#30340;&#34920;&#26684;&#25968;&#25454;&#20173;&#28982;&#34987;&#20302;&#25928;&#21033;&#29992;&#12290;&#23613;&#31649;&#35757;&#32451;&#19968;&#20010;&#33021;&#22815;&#29983;&#25104;&#31867;&#20284;&#21407;&#22987;&#20998;&#24067;&#30340;&#21512;&#25104;&#25968;&#25454;&#30340;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#35299;&#20915;&#20854;&#20013;&#19968;&#20123;&#38382;&#39064;&#65292;&#20294;&#22823;&#22810;&#25968;&#24212;&#29992;&#31243;&#24207;&#36824;&#38656;&#35201;&#39069;&#22806;&#30340;&#29983;&#25104;&#25968;&#25454;&#32422;&#26463;&#12290;&#29616;&#26377;&#30340;&#21512;&#25104;&#25968;&#25454;&#26041;&#27861;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#24120;&#21482;&#22788;&#29702;&#29305;&#23450;&#30340;&#32422;&#26463;&#26465;&#20214;&#65292;&#20363;&#22914;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#25110;&#22686;&#21152;&#20844;&#24179;&#24615;&#65292;&#24182;&#19988;&#32570;&#20047;&#19968;&#20010;&#21487;&#35775;&#38382;&#30340;&#25509;&#21475;&#26469;&#22768;&#26126;&#19968;&#33324;&#35268;&#33539;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ProgSyn&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#21487;&#32534;&#31243;&#30340;&#21512;&#25104;&#34920;&#26684;&#25968;&#25454;&#29983;&#25104;&#31639;&#27861;&#65292;&#23427;&#20801;&#35768;&#23545;&#29983;&#25104;&#30340;&#25968;&#25454;&#36827;&#34892;&#20840;&#38754;&#30340;&#33258;&#23450;&#20041;&#12290;&#20026;&#20102;&#30830;&#20445;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#24182;&#36981;&#23432;&#33258;&#23450;&#20041;&#35268;&#33539;&#65292;ProgSyn&#22312;&#21407;&#22987;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#22312;&#25552;&#20379;&#30340;&#35268;&#33539;&#19978;&#33258;&#21160;&#25512;&#23548;&#20986;&#21487;&#24494;&#20998;&#25439;&#22833;&#36827;&#34892;&#24494;&#35843;&#12290;&#36825;&#20123;&#35268;&#33539;&#21487;&#20197;&#20351;&#29992;&#32479;&#35745;&#21644;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large amounts of tabular data remain underutilized due to privacy, data quality, and data sharing limitations. While training a generative model producing synthetic data resembling the original distribution addresses some of these issues, most applications require additional constraints from the generated data. Existing synthetic data approaches are limited as they typically only handle specific constraints, e.g., differential privacy (DP) or increased fairness, and lack an accessible interface for declaring general specifications. In this work, we introduce ProgSyn, the first programmable synthetic tabular data generation algorithm that allows for comprehensive customization over the generated data. To ensure high data quality while adhering to custom specifications, ProgSyn pre-trains a generative model on the original dataset and fine-tunes it on a differentiable loss automatically derived from the provided specifications. These can be programmatically declared using statistical and
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35777;&#26126;&#20102;&#20855;&#26377;&#19968;&#23618;&#32447;&#24615;&#33258;&#27880;&#24847;&#21147;&#30340;transformer&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23454;&#29616;&#20102;&#26368;&#23567;&#20108;&#20056;&#32447;&#24615;&#22238;&#24402;&#30446;&#26631;&#19978;&#30340;&#21333;&#27493;&#26799;&#24230;&#19979;&#38477;&#25805;&#20316;&#12290;&#21516;&#26102;&#21457;&#29616;&#25913;&#21464;&#21327;&#21464;&#37327;&#21644;&#26435;&#37325;&#30340;&#20998;&#24067;&#20250;&#24433;&#21709;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.03576</link><description>&lt;p&gt;
&#26799;&#24230;&#19979;&#38477;&#30340;&#19968;&#27493;&#34987;&#35777;&#26126;&#26159;&#20855;&#26377;&#19968;&#23618;&#32447;&#24615;&#33258;&#27880;&#24847;&#21147;&#30340;&#26368;&#20248;&#19978;&#19979;&#25991;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
One Step of Gradient Descent is Provably the Optimal In-Context Learner with One Layer of Linear Self-Attention. (arXiv:2307.03576v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03576
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35777;&#26126;&#20102;&#20855;&#26377;&#19968;&#23618;&#32447;&#24615;&#33258;&#27880;&#24847;&#21147;&#30340;transformer&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23454;&#29616;&#20102;&#26368;&#23567;&#20108;&#20056;&#32447;&#24615;&#22238;&#24402;&#30446;&#26631;&#19978;&#30340;&#21333;&#27493;&#26799;&#24230;&#19979;&#38477;&#25805;&#20316;&#12290;&#21516;&#26102;&#21457;&#29616;&#25913;&#21464;&#21327;&#21464;&#37327;&#21644;&#26435;&#37325;&#30340;&#20998;&#24067;&#20250;&#24433;&#21709;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#23545;&#19978;&#19979;&#25991;&#23398;&#20064;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#24182;&#34920;&#26126;&#22312;&#20154;&#24037;&#21512;&#25104;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;transformer&#21487;&#20197;&#22312;&#20855;&#22791;&#36275;&#22815;&#23481;&#37327;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#23454;&#29616;Ridge&#22238;&#24402;&#65292;&#36825;&#26159;&#36125;&#21494;&#26031;&#26368;&#20248;&#39044;&#27979;&#22120;[Akyurek&#31561;&#65292;2023]&#65292;&#32780;&#20855;&#26377;&#32447;&#24615;&#33258;&#27880;&#24847;&#21147;&#19988;&#27809;&#26377;MLP&#23618;&#30340;&#19968;&#23618;transformer&#23558;&#23398;&#20064;&#23454;&#29616;&#19968;&#27493;&#26799;&#24230;&#19979;&#38477;(GD) [von Oswald&#31561;&#65292;2022]&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35266;&#23519;&#30340;&#29702;&#35770;&#22522;&#30784;&#23578;&#19981;&#28165;&#26224;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#30740;&#31350;&#20102;&#20855;&#26377;&#21333;&#23618;&#32447;&#24615;&#33258;&#27880;&#24847;&#21147;&#30340;transformer&#65292;&#22312;&#21512;&#25104;&#22122;&#22768;&#32447;&#24615;&#22238;&#24402;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22312;&#25968;&#23398;&#19978;&#35777;&#26126;&#20102;&#24403;&#21327;&#21464;&#37327;&#20174;&#26631;&#20934;&#39640;&#26031;&#20998;&#24067;&#20013;&#25277;&#21462;&#26102;&#65292;&#26368;&#23567;&#21270;&#39044;&#35757;&#32451;&#25439;&#22833;&#30340;&#19968;&#23618;transformer&#23558;&#22312;&#26368;&#23567;&#20108;&#20056;&#32447;&#24615;&#22238;&#24402;&#30446;&#26631;&#19978;&#23454;&#29616;&#21333;&#27493;GD&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;&#25913;&#21464;&#21327;&#21464;&#37327;&#21644;&#26435;&#37325;&#30340;&#20998;&#24067;&#20250;&#24433;&#21709;transformer&#30340;&#34920;&#29616;&#65292;&#20855;&#26377;&#26356;&#22823;&#30340;&#26041;&#24046;&#22312;&#20219;&#21153;&#19978;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have empirically analyzed in-context learning and shown that transformers trained on synthetic linear regression tasks can learn to implement ridge regression, which is the Bayes-optimal predictor, given sufficient capacity [Aky\"urek et al., 2023], while one-layer transformers with linear self-attention and no MLP layer will learn to implement one step of gradient descent (GD) on a least-squares linear regression objective [von Oswald et al., 2022]. However, the theory behind these observations remains poorly understood. We theoretically study transformers with a single layer of linear self-attention, trained on synthetic noisy linear regression data. First, we mathematically show that when the covariates are drawn from a standard Gaussian distribution, the one-layer transformer which minimizes the pre-training loss will implement a single step of GD on the least-squares linear regression objective. Then, we find that changing the distribution of the covariates and weight
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#31232;&#30095;&#27491;&#21017;&#21270;&#20013;&#36827;&#34892;&#24179;&#28369;&#20248;&#21270;&#65292;&#19982;&#20027;&#27969;&#30340;&#19968;&#38454;&#20248;&#21270;&#26041;&#27861;&#20860;&#23481;&#65292;&#24182;&#19988;&#33021;&#22815;&#24471;&#21040;&#21305;&#37197;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#21644;&#31561;&#20215;&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;</title><link>http://arxiv.org/abs/2307.03571</link><description>&lt;p&gt;
&#24179;&#28369;&#36793;&#32536;&#65306;&#21033;&#29992;Hadamard&#36229;&#21442;&#25968;&#21270;&#22312;&#31232;&#30095;&#27491;&#21017;&#21270;&#30340;&#24179;&#28369;&#20248;&#21270;&#20013;&#30340;&#19968;&#33324;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Smoothing the Edges: A General Framework for Smooth Optimization in Sparse Regularization using Hadamard Overparametrization. (arXiv:2307.03571v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03571
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#31232;&#30095;&#27491;&#21017;&#21270;&#20013;&#36827;&#34892;&#24179;&#28369;&#20248;&#21270;&#65292;&#19982;&#20027;&#27969;&#30340;&#19968;&#38454;&#20248;&#21270;&#26041;&#27861;&#20860;&#23481;&#65292;&#24182;&#19988;&#33021;&#22815;&#24471;&#21040;&#21305;&#37197;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#21644;&#31561;&#20215;&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#65288;&#32467;&#26500;&#21270;&#65289;&#31232;&#30095;&#27491;&#21017;&#21270;&#38382;&#39064;&#20013;&#30340;$\ell_q$&#21644;$\ell_{p,q}$&#27491;&#21017;&#21270;&#30340;&#24179;&#28369;&#26041;&#27861;&#12290;&#36825;&#20123;&#38750;&#24179;&#28369;&#19988;&#21487;&#33021;&#38750;&#20984;&#30340;&#38382;&#39064;&#30340;&#20248;&#21270;&#36890;&#24120;&#20381;&#36182;&#20110;&#19987;&#38376;&#30340;&#36807;&#31243;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#30340;&#19968;&#33324;&#26694;&#26550;&#19982;&#20027;&#27969;&#30340;&#19968;&#38454;&#20248;&#21270;&#26041;&#27861;&#65288;&#22914;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#21644;&#21152;&#36895;&#21464;&#20307;&#65289;&#20860;&#23481;&#65292;&#26080;&#38656;&#20219;&#20309;&#20462;&#25913;&#12290;&#36825;&#26159;&#36890;&#36807;&#24179;&#28369;&#20248;&#21270;&#36716;&#31227;&#23454;&#29616;&#30340;&#65292;&#20854;&#20013;&#36873;&#23450;&#27169;&#22411;&#21442;&#25968;&#30340;&#36229;&#21442;&#25968;&#21270;&#20351;&#29992;Hadamard&#20056;&#31215;&#21644;&#24809;&#32602;&#30340;&#25913;&#21464;&#12290;&#22312;&#36229;&#21442;&#25968;&#38382;&#39064;&#20013;&#65292;&#36890;&#36807;&#29992;&#26367;&#20195;&#21442;&#25968;&#36827;&#34892;&#24179;&#28369;&#21644;&#20984;&#24615;&#30340;$\ell_2$&#27491;&#21017;&#21270;&#65292;&#33021;&#22815;&#22312;&#21407;&#22987;&#21442;&#25968;&#21270;&#20013;&#24341;&#20837;&#38750;&#24179;&#28369;&#21644;&#38750;&#20984;&#24615;&#30340;$\ell_q$&#25110;$\ell_{p,q}$&#27491;&#21017;&#21270;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#33021;&#22815;&#24471;&#21040;&#21305;&#37197;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#65292;&#36824;&#33021;&#24471;&#21040;&#31561;&#20215;&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;&#36825;&#22312;&#38750;&#20984;&#31232;&#30095;&#27491;&#21017;&#21270;&#20013;&#23588;&#20854;&#26377;&#29992;&#65292;&#22240;&#20026;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#25214;&#21040;&#20840;&#23616;&#26368;&#23567;&#20540;&#38750;&#24120;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a smooth method for (structured) sparsity in $\ell_q$ and $\ell_{p,q}$ regularized optimization problems. Optimization of these non-smooth and possibly non-convex problems typically relies on specialized procedures. In contrast, our general framework is compatible with prevalent first-order optimization methods like Stochastic Gradient Descent and accelerated variants without any required modifications. This is accomplished through a smooth optimization transfer, comprising an overparametrization of selected model parameters using Hadamard products and a change of penalties. In the overparametrized problem, smooth and convex $\ell_2$ regularization of the surrogate parameters induces non-smooth and non-convex $\ell_q$ or $\ell_{p,q}$ regularization in the original parametrization. We show that our approach yields not only matching global minima but also equivalent local minima. This is particularly useful in non-convex sparse regularization, where finding global m
&lt;/p&gt;</description></item><item><title>MALIBO&#26159;&#19968;&#31181;&#20803;&#23398;&#20064;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#30452;&#25509;&#23398;&#20064;&#36328;&#20219;&#21153;&#30340;&#26597;&#35810;&#25928;&#29992;&#65292;&#24182;&#24341;&#20837;&#36741;&#21161;&#27169;&#22411;&#20197;&#23454;&#29616;&#23545;&#26032;&#20219;&#21153;&#30340;&#31283;&#20581;&#36866;&#24212;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#21487;&#20280;&#32553;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2307.03565</link><description>&lt;p&gt;
MALIBO: &#20803;&#23398;&#20064;&#24212;&#29992;&#20110;&#26080;&#20284;&#28982;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
MALIBO: Meta-learning for Likelihood-free Bayesian Optimization. (arXiv:2307.03565v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03565
&lt;/p&gt;
&lt;p&gt;
MALIBO&#26159;&#19968;&#31181;&#20803;&#23398;&#20064;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#30452;&#25509;&#23398;&#20064;&#36328;&#20219;&#21153;&#30340;&#26597;&#35810;&#25928;&#29992;&#65292;&#24182;&#24341;&#20837;&#36741;&#21161;&#27169;&#22411;&#20197;&#23454;&#29616;&#23545;&#26032;&#20219;&#21153;&#30340;&#31283;&#20581;&#36866;&#24212;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#21487;&#20280;&#32553;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#26159;&#19968;&#31181;&#20248;&#21270;&#26114;&#36149;&#40657;&#30418;&#20989;&#25968;&#30340;&#27969;&#34892;&#26041;&#27861;&#12290;&#20256;&#32479;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#20250;&#20174;&#22836;&#24320;&#22987;&#20248;&#21270;&#27599;&#20010;&#26032;&#30340;&#30446;&#26631;&#20219;&#21153;&#65292;&#32780;&#20803;&#23398;&#20064;&#21017;&#26159;&#21033;&#29992;&#30456;&#20851;&#20219;&#21153;&#30340;&#30693;&#35782;&#26469;&#26356;&#24555;&#22320;&#20248;&#21270;&#26032;&#20219;&#21153;&#30340;&#19968;&#31181;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20803;&#23398;&#20064;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#20381;&#36182;&#20110;&#26631;&#20934;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#23384;&#22312;&#21487;&#20280;&#32553;&#24615;&#38382;&#39064;&#65292;&#24182;&#19988;&#23545;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#35266;&#23519;&#25968;&#25454;&#30340;&#23610;&#24230;&#21644;&#22122;&#22768;&#31867;&#22411;&#38750;&#24120;&#25935;&#24863;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#24120;&#24120;&#24573;&#35270;&#19982;&#20219;&#21153;&#30456;&#20284;&#24615;&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#36825;&#23548;&#33268;&#22312;&#20165;&#26377;&#26377;&#38480;&#35266;&#23519;&#25968;&#25454;&#25110;&#26032;&#20219;&#21153;&#19982;&#30456;&#20851;&#20219;&#21153;&#24046;&#24322;&#26174;&#33879;&#26102;&#65292;&#20219;&#21153;&#36866;&#24212;&#24615;&#19981;&#21487;&#38752;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20803;&#23398;&#20064;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#26088;&#22312;&#32469;&#24320;&#26631;&#20934;&#27169;&#22411;&#65292;&#30452;&#25509;&#23398;&#20064;&#36328;&#20219;&#21153;&#30340;&#26597;&#35810;&#25928;&#29992;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26126;&#30830;&#24314;&#27169;&#20219;&#21153;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#36741;&#21161;&#27169;&#22411;&#65292;&#20351;&#20854;&#33021;&#22815;&#23545;&#26032;&#20219;&#21153;&#36827;&#34892;&#31283;&#20581;&#36866;&#24212;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimization (BO) is a popular method to optimize costly black-box functions. While traditional BO optimizes each new target task from scratch, meta-learning has emerged as a way to leverage knowledge from related tasks to optimize new tasks faster. However, existing meta-learning BO methods rely on surrogate models that suffer from scalability issues and are sensitive to observations with different scales and noise types across tasks. Moreover, they often overlook the uncertainty associated with task similarity. This leads to unreliable task adaptation when only limited observations are obtained or when the new tasks differ significantly from the related tasks. To address these limitations, we propose a novel meta-learning BO approach that bypasses the surrogate model and directly learns the utility of queries across tasks. Our method explicitly models task uncertainty and includes an auxiliary model to enable robust adaptation to new tasks. Extensive experiments show that ou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#36890;&#36807;&#22522;&#20110;&#39118;&#26684;&#30340;&#25968;&#25454;&#37319;&#26679;&#26469;&#22686;&#24378;&#20027;&#35266;&#24615;&#26816;&#27979;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;GPT-3&#27169;&#22411;&#26681;&#25454;&#26032;&#38395;&#35760;&#32773;&#35270;&#35282;&#30340;&#20027;&#35266;&#24615;&#26816;&#26597;&#28165;&#21333;&#29983;&#25104;&#39069;&#22806;&#30340;&#35757;&#32451;&#26448;&#26009;&#65292;&#24182;&#20351;&#29992;&#25193;&#23637;&#30340;&#35757;&#32451;&#38598;&#24494;&#35843;&#35821;&#35328;&#29305;&#23450;&#30340;&#21464;&#24418;&#22120;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19981;&#21516;&#20027;&#35266;&#39118;&#26684;&#22312;&#25152;&#26377;&#35821;&#35328;&#20013;&#37117;&#26159;&#26377;&#25928;&#30340;&#65292;&#24182;&#19988;&#22522;&#20110;&#39118;&#26684;&#30340;&#36807;&#37319;&#26679;&#22312;&#22303;&#32819;&#20854;&#35821;&#21644;&#33521;&#35821;&#20013;&#30340;&#25928;&#26524;&#20248;&#20110;&#25913;&#20889;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#38750;&#33521;&#35821;&#35821;&#35328;&#20013;&#65292;GPT-3&#27169;&#22411;&#26377;&#26102;&#20250;&#29983;&#25104;&#20047;&#21619;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.03550</link><description>&lt;p&gt;
DWReCO&#22312;CheckThat! 2023&#20013;&#36890;&#36807;&#22522;&#20110;&#39118;&#26684;&#30340;&#25968;&#25454;&#37319;&#26679;&#22686;&#24378;&#23458;&#35266;&#24615;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
DWReCO at CheckThat! 2023: Enhancing Subjectivity Detection through Style-based Data Sampling. (arXiv:2307.03550v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03550
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#36890;&#36807;&#22522;&#20110;&#39118;&#26684;&#30340;&#25968;&#25454;&#37319;&#26679;&#26469;&#22686;&#24378;&#20027;&#35266;&#24615;&#26816;&#27979;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;GPT-3&#27169;&#22411;&#26681;&#25454;&#26032;&#38395;&#35760;&#32773;&#35270;&#35282;&#30340;&#20027;&#35266;&#24615;&#26816;&#26597;&#28165;&#21333;&#29983;&#25104;&#39069;&#22806;&#30340;&#35757;&#32451;&#26448;&#26009;&#65292;&#24182;&#20351;&#29992;&#25193;&#23637;&#30340;&#35757;&#32451;&#38598;&#24494;&#35843;&#35821;&#35328;&#29305;&#23450;&#30340;&#21464;&#24418;&#22120;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19981;&#21516;&#20027;&#35266;&#39118;&#26684;&#22312;&#25152;&#26377;&#35821;&#35328;&#20013;&#37117;&#26159;&#26377;&#25928;&#30340;&#65292;&#24182;&#19988;&#22522;&#20110;&#39118;&#26684;&#30340;&#36807;&#37319;&#26679;&#22312;&#22303;&#32819;&#20854;&#35821;&#21644;&#33521;&#35821;&#20013;&#30340;&#25928;&#26524;&#20248;&#20110;&#25913;&#20889;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#38750;&#33521;&#35821;&#35821;&#35328;&#20013;&#65292;GPT-3&#27169;&#22411;&#26377;&#26102;&#20250;&#29983;&#25104;&#20047;&#21619;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#25105;&#20204;&#22312;CheckThat!&#23454;&#39564;&#23460;&#20027;&#35266;&#24615;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#25552;&#20132;&#12290;&#20026;&#20102;&#35299;&#20915;&#20219;&#21153;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#26032;&#38395;&#35760;&#32773;&#35270;&#35282;&#30340;&#20027;&#35266;&#24615;&#26816;&#26597;&#28165;&#21333;&#65292;&#20351;&#29992;&#19981;&#21516;&#39118;&#26684;&#30340;&#25552;&#31034;&#26469;&#29983;&#25104;&#39069;&#22806;&#30340;&#35757;&#32451;&#26448;&#26009;&#65292;&#20351;&#29992;GPT-3&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#25193;&#23637;&#30340;&#35757;&#32451;&#38598;&#26469;&#24494;&#35843;&#35821;&#35328;&#29305;&#23450;&#30340;&#21464;&#24418;&#22120;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#33521;&#35821;&#12289;&#24503;&#35821;&#21644;&#22303;&#32819;&#20854;&#35821;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19981;&#21516;&#30340;&#20027;&#35266;&#39118;&#26684;&#22312;&#25152;&#26377;&#35821;&#35328;&#20013;&#37117;&#26159;&#26377;&#25928;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#22303;&#32819;&#20854;&#35821;&#21644;&#33521;&#35821;&#20013;&#65292;&#22522;&#20110;&#39118;&#26684;&#30340;&#36807;&#37319;&#26679;&#27604;&#25913;&#20889;&#26356;&#22909;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#38750;&#33521;&#35821;&#35821;&#35328;&#20013;&#65292;GPT-3&#27169;&#22411;&#26377;&#26102;&#20250;&#29983;&#25104;&#20047;&#21619;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes our submission for the subjectivity detection task at the CheckThat! Lab. To tackle class imbalances in the task, we have generated additional training materials with GPT-3 models using prompts of different styles from a subjectivity checklist based on journalistic perspective. We used the extended training set to fine-tune language-specific transformer models. Our experiments in English, German and Turkish demonstrate that different subjective styles are effective across all languages. In addition, we observe that the style-based oversampling is better than paraphrasing in Turkish and English. Lastly, the GPT-3 models sometimes produce lacklustre results when generating style-based texts in non-English languages.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#32599;&#39532;&#25968;&#23383;&#20998;&#26512;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#30452;&#25509;&#22788;&#29702;&#20048;&#35889;&#20013;&#30340;&#27599;&#20010;&#38899;&#31526;&#65292;&#21033;&#29992;&#38899;&#31526;&#29305;&#24449;&#21644;&#38899;&#31526;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#26032;&#22411;&#36793;&#32553;&#20943;&#31639;&#27861;&#20135;&#29983;&#25353;&#38899;&#30340;&#34920;&#31034;&#12290;&#22312;&#21442;&#32771;&#25968;&#25454;&#38598;&#19978;&#65292;ChordGNN&#27169;&#22411;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#32599;&#39532;&#25968;&#23383;&#20998;&#26512;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.03544</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32599;&#39532;&#25968;&#23383;&#20998;&#26512;&#65306;&#20174;&#38899;&#31526;&#29305;&#24449;&#21040;&#25353;&#38899;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Roman Numeral Analysis with Graph Neural Networks: Onset-wise Predictions from Note-wise Features. (arXiv:2307.03544v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03544
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#32599;&#39532;&#25968;&#23383;&#20998;&#26512;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#30452;&#25509;&#22788;&#29702;&#20048;&#35889;&#20013;&#30340;&#27599;&#20010;&#38899;&#31526;&#65292;&#21033;&#29992;&#38899;&#31526;&#29305;&#24449;&#21644;&#38899;&#31526;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#26032;&#22411;&#36793;&#32553;&#20943;&#31639;&#27861;&#20135;&#29983;&#25353;&#38899;&#30340;&#34920;&#31034;&#12290;&#22312;&#21442;&#32771;&#25968;&#25454;&#38598;&#19978;&#65292;ChordGNN&#27169;&#22411;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#32599;&#39532;&#25968;&#23383;&#20998;&#26512;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32599;&#39532;&#25968;&#23383;&#20998;&#26512;&#26159;&#22312;&#35843;&#24615;&#38899;&#20048;&#20316;&#21697;&#20013;&#35782;&#21035;&#21644;&#30830;&#23450;&#21644;&#24358;&#20197;&#21450;&#20854;&#21151;&#33021;&#32972;&#26223;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31526;&#21495;&#38899;&#20048;&#30340;&#33258;&#21160;&#32599;&#39532;&#25968;&#23383;&#20998;&#26512;&#26032;&#26041;&#27861;&#12290;&#29616;&#26377;&#25216;&#26415;&#20381;&#36182;&#20110;&#23545;&#20048;&#35889;&#30340;&#20013;&#38388;&#20002;&#22833;&#21387;&#32553;&#34920;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#30452;&#25509;&#25551;&#36848;&#21644;&#22788;&#29702;&#20048;&#35889;&#20013;&#30340;&#27599;&#19968;&#20010;&#38899;&#31526;&#12290;&#25152;&#25552;&#20986;&#30340;&#26550;&#26500;&#21487;&#20197;&#21033;&#29992;&#38899;&#31526;&#29305;&#24449;&#21644;&#38899;&#31526;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#25105;&#20204;&#30340;&#26032;&#22411;&#36793;&#32553;&#20943;&#31639;&#27861;&#20135;&#29983;&#25353;&#38899;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;ChordGNN&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#22312;&#21442;&#32771;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#32599;&#39532;&#25968;&#23383;&#20998;&#26512;&#20934;&#30830;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#20351;&#29992;NADE&#21644;&#21518;&#22788;&#29702;&#21644;&#24358;&#39044;&#27979;&#31561;&#25216;&#26415;&#30340;&#27169;&#22411;&#21464;&#20307;&#12290;&#26412;&#25991;&#30340;&#23436;&#25972;&#28304;&#20195;&#30721;&#21487;&#22312;https://github.com/mano&#19978;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Roman Numeral analysis is the important task of identifying chords and their functional context in pieces of tonal music. This paper presents a new approach to automatic Roman Numeral analysis in symbolic music. While existing techniques rely on an intermediate lossy representation of the score, we propose a new method based on Graph Neural Networks (GNNs) that enable the direct description and processing of each individual note in the score. The proposed architecture can leverage notewise features and interdependencies between notes but yield onset-wise representation by virtue of our novel edge contraction algorithm. Our results demonstrate that ChordGNN outperforms existing state-of-the-art models, achieving higher accuracy in Roman Numeral analysis on the reference datasets. In addition, we investigate variants of our model using proposed techniques such as NADE, and post-processing of the chord predictions. The full source code for this work is available at https://github.com/mano
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30772;&#20135;&#38382;&#39064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#20013;&#28608;&#21169;&#20998;&#37197;&#30340;&#25361;&#25112;&#65292;&#20197;&#30830;&#20445;&#20844;&#24179;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.03515</link><description>&lt;p&gt;
&#22522;&#20110;&#30772;&#20135;&#38382;&#39064;&#30340;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#28608;&#21169;&#20998;&#37197;
&lt;/p&gt;
&lt;p&gt;
Incentive Allocation in Vertical Federated Learning Based on Bankruptcy Problem. (arXiv:2307.03515v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03515
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30772;&#20135;&#38382;&#39064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#20013;&#28608;&#21169;&#20998;&#37197;&#30340;&#25361;&#25112;&#65292;&#20197;&#30830;&#20445;&#20844;&#24179;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#65288;VFL&#65289;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21512;&#20316;&#35757;&#32451;&#22312;&#19981;&#21516;&#21442;&#19982;&#26041;&#20043;&#38388;&#22402;&#30452;&#21010;&#20998;&#30340;&#31169;&#26377;&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#22312;VFL&#35774;&#32622;&#20013;&#65292;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#20027;&#21160;&#26041;&#65288;&#25317;&#26377;&#24102;&#26631;&#31614;&#26679;&#26412;&#29305;&#24449;&#30340;&#21442;&#19982;&#26041;&#65289;&#36890;&#36807;&#19982;&#26576;&#20123;&#34987;&#21160;&#26041;&#65288;&#25317;&#26377;&#30456;&#21516;&#26679;&#26412;&#20294;&#27809;&#26377;&#26631;&#31614;&#30340;&#39069;&#22806;&#29305;&#24449;&#30340;&#21442;&#19982;&#26041;&#65289;&#21512;&#20316;&#65292;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#25913;&#36827;&#20854;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#28608;&#21169;&#34987;&#21160;&#26041;&#21442;&#19982;VFL&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#20102;&#22522;&#20110;&#34987;&#21160;&#26041;&#22312;VFL&#36807;&#31243;&#20013;&#30340;&#36129;&#29486;&#26469;&#20026;&#20182;&#20204;&#20998;&#37197;&#28608;&#21169;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#23450;&#20041;&#20026;&#26680;&#24515;&#28216;&#25103;&#35770;&#27010;&#24565;&#30340;&#19968;&#31181;&#21464;&#20307;&#8212;&#8212;&#30772;&#20135;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#22612;&#26408;&#24503;&#21010;&#20998;&#35268;&#21017;&#26469;&#35299;&#20915;&#23427;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#23427;&#30830;&#20445;&#20102;&#28608;&#21169;&#30340;&#20844;&#24179;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vertical federated learning (VFL) is a promising approach for collaboratively training machine learning models using private data partitioned vertically across different parties. Ideally in a VFL setting, the active party (party possessing features of samples with labels) benefits by improving its machine learning model through collaboration with some passive parties (parties possessing additional features of the same samples without labels) in a privacy preserving manner. However, motivating passive parties to participate in VFL can be challenging. In this paper, we focus on the problem of allocating incentives to the passive parties by the active party based on their contributions to the VFL process. We formulate this problem as a variant of the Nucleolus game theory concept, known as the Bankruptcy Problem, and solve it using the Talmud's division rule. We evaluate our proposed method on synthetic and real-world datasets and show that it ensures fairness and stability in incentive a
&lt;/p&gt;</description></item><item><title>DEFT&#26159;&#19968;&#31181;&#21033;&#29992;&#27169;&#22411;&#23618;&#20043;&#38388;&#30340;&#26799;&#24230;&#33539;&#25968;&#24046;&#24322;&#26469;&#23454;&#29616;&#21487;&#25193;&#23637;&#30340;&#26799;&#24230;&#31232;&#30095;&#21270;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20943;&#23569;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#36890;&#20449;&#27969;&#37327;&#65292;&#24182;&#22312;&#35745;&#31639;&#25104;&#26412;&#21644;&#26799;&#24230;&#32047;&#31215;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2307.03500</link><description>&lt;p&gt;
DEFT:&#21033;&#29992;&#27169;&#22411;&#23618;&#20043;&#38388;&#30340;&#26799;&#24230;&#33539;&#25968;&#24046;&#24322;&#26469;&#23454;&#29616;&#21487;&#25193;&#23637;&#30340;&#26799;&#24230;&#31232;&#30095;&#21270;
&lt;/p&gt;
&lt;p&gt;
DEFT: Exploiting Gradient Norm Difference between Model Layers for Scalable Gradient Sparsification. (arXiv:2307.03500v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03500
&lt;/p&gt;
&lt;p&gt;
DEFT&#26159;&#19968;&#31181;&#21033;&#29992;&#27169;&#22411;&#23618;&#20043;&#38388;&#30340;&#26799;&#24230;&#33539;&#25968;&#24046;&#24322;&#26469;&#23454;&#29616;&#21487;&#25193;&#23637;&#30340;&#26799;&#24230;&#31232;&#30095;&#21270;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20943;&#23569;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#36890;&#20449;&#27969;&#37327;&#65292;&#24182;&#22312;&#35745;&#31639;&#25104;&#26412;&#21644;&#26799;&#24230;&#32047;&#31215;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26799;&#24230;&#31232;&#30095;&#21270;&#26159;&#20943;&#23569;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#20013;&#36807;&#22810;&#36890;&#20449;&#27969;&#37327;&#30340;&#24191;&#27867;&#24212;&#29992;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26799;&#24230;&#31232;&#30095;&#21270;&#26041;&#27861;&#30001;&#20110;&#26799;&#24230;&#36873;&#25321;&#30340;&#35745;&#31639;&#25104;&#26412;&#30456;&#24403;&#22823;&#21644;&#26799;&#24230;&#32047;&#31215;&#22686;&#21152;&#30340;&#36890;&#20449;&#27969;&#37327;&#65292;&#20854;&#21487;&#25193;&#23637;&#24615;&#30456;&#23545;&#36739;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26799;&#24230;&#31232;&#30095;&#21270;&#26041;&#26696;DEFT&#65292;&#23558;&#26799;&#24230;&#36873;&#25321;&#20219;&#21153;&#20998;&#35299;&#20026;&#23376;&#20219;&#21153;&#24182;&#20998;&#37197;&#32473;&#24037;&#20316;&#33410;&#28857;&#12290; DEFT&#19982;&#29616;&#26377;&#30340;&#31232;&#30095;&#21270;&#26041;&#27861;&#19981;&#21516;&#65292;&#27599;&#20010;&#24037;&#20316;&#33410;&#28857;&#20165;&#20174;&#25152;&#26377;&#26799;&#24230;&#20013;&#36873;&#25321;&#26799;&#24230;&#12290;&#22240;&#27492;&#65292;&#38543;&#30528;&#24037;&#20316;&#33410;&#28857;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#35745;&#31639;&#25104;&#26412;&#21487;&#20197;&#38477;&#20302;&#12290;&#27492;&#22806;&#65292;DEFT&#20801;&#35768;&#24037;&#20316;&#33410;&#28857;&#22312;&#38750;&#20132;&#21449;&#30340;&#20998;&#21306;&#20013;&#36873;&#25321;&#26799;&#24230;&#65292;&#22240;&#27492;&#21363;&#20351;&#24037;&#20316;&#33410;&#28857;&#25968;&#37327;&#22686;&#21152;&#65292;&#36890;&#20449;&#27969;&#37327;&#20063;&#21487;&#20197;&#26681;&#25454;&#29992;&#25143;&#35201;&#27714;&#36827;&#34892;&#32500;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gradient sparsification is a widely adopted solution for reducing the excessive communication traffic in distributed deep learning. However, most existing gradient sparsifiers have relatively poor scalability because of considerable computational cost of gradient selection and/or increased communication traffic owing to gradient build-up. To address these challenges, we propose a novel gradient sparsification scheme, DEFT, that partitions the gradient selection task into sub tasks and distributes them to workers. DEFT differs from existing sparsifiers, wherein every worker selects gradients among all gradients. Consequently, the computational cost can be reduced as the number of workers increases. Moreover, gradient build-up can be eliminated because DEFT allows workers to select gradients in partitions that are non-intersecting (between workers). Therefore, even if the number of workers increases, the communication traffic can be maintained as per user requirement.  To avoid the loss 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;Hough&#21464;&#25442;&#21644;&#21160;&#24577;&#21367;&#31215;&#36827;&#34892;&#36710;&#36947;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#36710;&#36947;&#30340;&#20960;&#20309;&#32467;&#26500;&#21644;&#20998;&#23618;&#29305;&#24449;&#32858;&#21512;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#26816;&#27979;&#21644;&#20998;&#21106;&#22797;&#26434;&#22330;&#26223;&#20013;&#30340;&#36710;&#36947;&#12290;</title><link>http://arxiv.org/abs/2307.03494</link><description>&lt;p&gt;
HoughLaneNet: &#20351;&#29992;&#28145;&#24230;Hough&#21464;&#25442;&#21644;&#21160;&#24577;&#21367;&#31215;&#36827;&#34892;&#36710;&#36947;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
HoughLaneNet: Lane Detection with Deep Hough Transform and Dynamic Convolution. (arXiv:2307.03494v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03494
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;Hough&#21464;&#25442;&#21644;&#21160;&#24577;&#21367;&#31215;&#36827;&#34892;&#36710;&#36947;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#36710;&#36947;&#30340;&#20960;&#20309;&#32467;&#26500;&#21644;&#20998;&#23618;&#29305;&#24449;&#32858;&#21512;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#26816;&#27979;&#21644;&#20998;&#21106;&#22797;&#26434;&#22330;&#26223;&#20013;&#30340;&#36710;&#36947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#36710;&#36947;&#30340;&#22797;&#26434;&#24615;&#65292;&#36710;&#36947;&#26816;&#27979;&#20219;&#21153;&#22312;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#36710;&#36947;&#30340;&#31364;&#12289;&#26029;&#35010;&#20197;&#21450;&#34987;&#20132;&#36890;&#25317;&#22581;&#25152;&#36974;&#25377;&#31561;&#29305;&#28857;&#20351;&#24471;&#36710;&#36947;&#26816;&#27979;&#22256;&#38590;&#37325;&#37325;&#12290;&#28982;&#32780;&#65292;&#35266;&#23519;&#21457;&#29616;&#36710;&#36947;&#20855;&#26377;&#20960;&#20309;&#32467;&#26500;&#65292;&#31867;&#20284;&#20110;&#19968;&#26465;&#30452;&#32447;&#65292;&#22240;&#27492;&#24403;&#21033;&#29992;&#36825;&#19968;&#29305;&#24449;&#26102;&#65292;&#36710;&#36947;&#26816;&#27979;&#32467;&#26524;&#24471;&#21040;&#20102;&#26174;&#33879;&#25913;&#21892;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#30340;&#28145;&#24230;Hough&#21464;&#25442;&#65288;DHT&#65289;&#26041;&#27861;&#65292;&#23558;&#22270;&#20687;&#20013;&#25152;&#26377;&#30340;&#36710;&#36947;&#29305;&#24449;&#32452;&#21512;&#21040;Hough&#21442;&#25968;&#31354;&#38388;&#20013;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25913;&#36827;&#20102;&#28857;&#36873;&#21462;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#21160;&#24577;&#21367;&#31215;&#27169;&#22359;&#65292;&#20197;&#26377;&#25928;&#21306;&#20998;&#21407;&#22987;&#22270;&#20687;&#20013;&#30340;&#36710;&#36947;&#12290;&#25105;&#20204;&#30340;&#32593;&#32476;&#26550;&#26500;&#21253;&#25324;&#19968;&#20010;&#20027;&#24178;&#32593;&#32476;&#65292;&#21363;ResNet&#25110;&#37329;&#23383;&#22612;&#35270;&#35273;&#21464;&#25442;&#22120;&#65288;Pyramid Vision Transformer&#65289;&#65292;&#19968;&#31181;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;&#20316;&#20026;&#20013;&#38388;&#23618;&#25552;&#21462;&#22810;&#23610;&#24230;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#20998;&#23618;&#30340;&#22522;&#20110;DHT&#30340;&#29305;&#24449;&#32858;&#21512;&#22836;&#20934;&#30830;&#22320;&#36827;&#34892;&#36710;&#36947;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of lane detection has garnered considerable attention in the field of autonomous driving due to its complexity. Lanes can present difficulties for detection, as they can be narrow, fragmented, and often obscured by heavy traffic. However, it has been observed that the lanes have a geometrical structure that resembles a straight line, leading to improved lane detection results when utilizing this characteristic. To address this challenge, we propose a hierarchical Deep Hough Transform (DHT) approach that combines all lane features in an image into the Hough parameter space. Additionally, we refine the point selection method and incorporate a Dynamic Convolution Module to effectively differentiate between lanes in the original image. Our network architecture comprises a backbone network, either a ResNet or Pyramid Vision Transformer, a Feature Pyramid Network as the neck to extract multi-scale features, and a hierarchical DHT-based feature aggregation head to accurately segment 
&lt;/p&gt;</description></item><item><title>ITA&#26159;&#19968;&#31181;&#22522;&#20110;&#37327;&#21270;&#30340;Transformer&#30340;&#33021;&#25928;&#39640;&#30340;Attention&#21644;Softmax&#21152;&#36895;&#22120;&#12290;&#36890;&#36807;&#21033;&#29992;8&#20301;&#37327;&#21270;&#21644;&#20165;&#22522;&#20110;&#25972;&#25968;&#20540;&#30340;&#21019;&#26032;Softmax&#23454;&#29616;&#65292;ITA&#23454;&#29616;&#20102;&#39640;&#33021;&#25928;&#30340;&#25512;&#29702;&#65292;&#22312;16.9 TOPS/W&#30340;&#33021;&#25928;&#19978;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;Transformer&#21152;&#36895;&#22120;&#65292;&#24182;&#22312;5.93 TOPS/mm$^2$&#30340;&#38754;&#31215;&#25928;&#29575;&#19978;&#36229;&#36234;&#20102;&#23427;&#20204;&#12290;</title><link>http://arxiv.org/abs/2307.03493</link><description>&lt;p&gt;
ITA:&#19968;&#31181;&#22522;&#20110;&#37327;&#21270;&#30340;Transformer&#30340;&#33021;&#25928;&#39640;&#30340;Attention&#21644;Softmax&#21152;&#36895;&#22120;
&lt;/p&gt;
&lt;p&gt;
ITA: An Energy-Efficient Attention and Softmax Accelerator for Quantized Transformers. (arXiv:2307.03493v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03493
&lt;/p&gt;
&lt;p&gt;
ITA&#26159;&#19968;&#31181;&#22522;&#20110;&#37327;&#21270;&#30340;Transformer&#30340;&#33021;&#25928;&#39640;&#30340;Attention&#21644;Softmax&#21152;&#36895;&#22120;&#12290;&#36890;&#36807;&#21033;&#29992;8&#20301;&#37327;&#21270;&#21644;&#20165;&#22522;&#20110;&#25972;&#25968;&#20540;&#30340;&#21019;&#26032;Softmax&#23454;&#29616;&#65292;ITA&#23454;&#29616;&#20102;&#39640;&#33021;&#25928;&#30340;&#25512;&#29702;&#65292;&#22312;16.9 TOPS/W&#30340;&#33021;&#25928;&#19978;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;Transformer&#21152;&#36895;&#22120;&#65292;&#24182;&#22312;5.93 TOPS/mm$^2$&#30340;&#38754;&#31215;&#25928;&#29575;&#19978;&#36229;&#36234;&#20102;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#32593;&#32476;&#24050;&#32463;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#24182;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#38899;&#39057;&#22788;&#29702;&#31561;&#20854;&#20182;&#39046;&#22495;&#21463;&#21040;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;Transformer&#27169;&#22411;&#30340;&#39640;&#31639;&#26415;&#24378;&#24230;&#12289;&#22823;&#20869;&#23384;&#38656;&#27714;&#21644;&#22797;&#26434;&#25968;&#25454;&#27969;&#20381;&#36182;&#23548;&#33268;&#20102;&#20854;&#26377;&#25928;&#30828;&#20214;&#21152;&#36895;&#38754;&#20020;&#26032;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ITA&#65292;&#19968;&#31181;&#38024;&#23545;&#23884;&#20837;&#24335;&#31995;&#32479;&#19978;&#39640;&#25928;&#25512;&#29702;&#30340;Transformer&#21644;&#30456;&#20851;&#27169;&#22411;&#30340;&#26032;&#22411;&#21152;&#36895;&#22120;&#26550;&#26500;&#65292;&#36890;&#36807;&#21033;&#29992;8&#20301;&#37327;&#21270;&#21644;&#20165;&#22522;&#20110;&#25972;&#25968;&#20540;&#30340;&#21019;&#26032;Softmax&#23454;&#29616;&#12290;&#36890;&#36807;&#22312;&#27969;&#27169;&#24335;&#19979;&#23454;&#26102;&#35745;&#31639;&#65292;&#25105;&#20204;&#30340;Softmax&#23454;&#29616;&#26368;&#22823;&#31243;&#24230;&#22320;&#20943;&#23569;&#20102;&#25968;&#25454;&#31227;&#21160;&#21644;&#33021;&#37327;&#28040;&#32791;&#12290;ITA&#22312;&#33021;&#25928;&#26041;&#38754;&#19982;&#26368;&#20808;&#36827;&#30340;Transformer&#21152;&#36895;&#22120;&#20445;&#25345;&#31454;&#20105;&#21147;&#65292;&#36798;&#21040;&#20102;16.9 TOPS/W&#65292;&#21516;&#26102;&#22312;&#38754;&#31215;&#25928;&#29575;&#26041;&#38754;&#20197;5.93 TOPS/mm$^2$&#30340;&#25104;&#32489;&#36229;&#36234;&#20102;&#23427;&#20204;&#65292;&#22312;22&#32435;&#31859;&#23436;&#20840;&#32791;&#23613;&#30340;&#30789;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer networks have emerged as the state-of-the-art approach for natural language processing tasks and are gaining popularity in other domains such as computer vision and audio processing. However, the efficient hardware acceleration of transformer models poses new challenges due to their high arithmetic intensities, large memory requirements, and complex dataflow dependencies. In this work, we propose ITA, a novel accelerator architecture for transformers and related models that targets efficient inference on embedded systems by exploiting 8-bit quantization and an innovative softmax implementation that operates exclusively on integer values. By computing on-the-fly in streaming mode, our softmax implementation minimizes data movement and energy consumption. ITA achieves competitive energy efficiency with respect to state-of-the-art transformer accelerators with 16.9 TOPS/W, while outperforming them in area efficiency with 5.93 TOPS/mm$^2$ in 22 nm fully-depleted silicon-on-insu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#29992;&#20110;&#20998;&#24067;&#22238;&#24402;&#38382;&#39064;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#26080;&#27861;&#30452;&#25509;&#22788;&#29702;&#27010;&#29575;&#20998;&#24067;&#36755;&#20837;&#30340;&#22256;&#38590;&#65292;&#24182;&#24314;&#31435;&#20102;&#36924;&#36817;&#29702;&#35770;&#21644;&#23398;&#20064;&#29702;&#35770;&#12290;</title><link>http://arxiv.org/abs/2307.03487</link><description>&lt;p&gt;
&#20998;&#24067;&#22238;&#24402;&#30340;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Learning Theory of Distribution Regression with Neural Networks. (arXiv:2307.03487v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03487
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#29992;&#20110;&#20998;&#24067;&#22238;&#24402;&#38382;&#39064;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#26080;&#27861;&#30452;&#25509;&#22788;&#29702;&#27010;&#29575;&#20998;&#24067;&#36755;&#20837;&#30340;&#22256;&#38590;&#65292;&#24182;&#24314;&#31435;&#20102;&#36924;&#36817;&#29702;&#35770;&#21644;&#23398;&#20064;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#65288;FNN&#65289;&#24314;&#31435;&#20998;&#24067;&#22238;&#24402;&#30340;&#36924;&#36817;&#29702;&#35770;&#21644;&#23398;&#20064;&#29702;&#35770;&#12290;&#19982;&#20256;&#32479;&#22238;&#24402;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#20998;&#24067;&#22238;&#24402;&#30340;&#36755;&#20837;&#21464;&#37327;&#26159;&#27010;&#29575;&#27979;&#24230;&#12290;&#28982;&#21518;&#25105;&#20204;&#24120;&#24120;&#38656;&#35201;&#36827;&#34892;&#20108;&#38454;&#27573;&#25277;&#26679;&#36807;&#31243;&#26469;&#36817;&#20284;&#20998;&#24067;&#30340;&#23454;&#38469;&#20449;&#24687;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20256;&#32479;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#35201;&#27714;&#36755;&#20837;&#21464;&#37327;&#20026;&#21521;&#37327;&#12290;&#24403;&#36755;&#20837;&#26679;&#26412;&#26159;&#27010;&#29575;&#20998;&#24067;&#26102;&#65292;&#20256;&#32479;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#26080;&#27861;&#30452;&#25509;&#20351;&#29992;&#65292;&#20998;&#24067;&#22238;&#24402;&#38382;&#39064;&#21464;&#24471;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#19968;&#20010;&#23545;&#20110;&#20998;&#24067;&#36755;&#20837;&#36827;&#34892;&#26126;&#30830;&#23450;&#20041;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#26159;&#38750;&#24120;&#38656;&#27714;&#30340;&#12290;&#20851;&#20110;&#20998;&#24067;&#22238;&#24402;&#30340;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#27809;&#26377;&#25968;&#23398;&#27169;&#22411;&#21644;&#29702;&#35770;&#20998;&#26512;&#12290;&#20026;&#20102;&#20811;&#26381;&#25216;&#26415;&#38590;&#39064;&#24182;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we aim at establishing an approximation theory and a learning theory of distribution regression via a fully connected neural network (FNN). In contrast to the classical regression methods, the input variables of distribution regression are probability measures. Then we often need to perform a second-stage sampling process to approximate the actual information of the distribution. On the other hand, the classical neural network structure requires the input variable to be a vector. When the input samples are probability distributions, the traditional deep neural network method cannot be directly used and the difficulty arises for distribution regression. A well-defined neural network structure for distribution inputs is intensively desirable. There is no mathematical model and theoretical analysis on neural network realization of distribution regression. To overcome technical difficulties and address this issue, we establish a novel fully connected neural network framework
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#25552;&#20986;&#20102;&#26032;&#30340;&#25104;&#23601;&#33976;&#39311;&#26041;&#27861;&#65292;&#21487;&#20197;&#21152;&#24378;&#20195;&#29702;&#23545;&#19979;&#19968;&#20010;&#35299;&#38145;&#25104;&#23601;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#20248;&#20110;&#20808;&#21069;&#30340;&#27169;&#22411;&#39537;&#21160;&#21644;&#23618;&#27425;&#21270;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.03486</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#21457;&#29616;&#23618;&#27425;&#21270;&#25104;&#23601;
&lt;/p&gt;
&lt;p&gt;
Discovering Hierarchical Achievements in Reinforcement Learning via Contrastive Learning. (arXiv:2307.03486v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03486
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#25552;&#20986;&#20102;&#26032;&#30340;&#25104;&#23601;&#33976;&#39311;&#26041;&#27861;&#65292;&#21487;&#20197;&#21152;&#24378;&#20195;&#29702;&#23545;&#19979;&#19968;&#20010;&#35299;&#38145;&#25104;&#23601;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#20248;&#20110;&#20808;&#21069;&#30340;&#27169;&#22411;&#39537;&#21160;&#21644;&#23618;&#27425;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#25104;&#29615;&#22659;&#20013;&#21457;&#29616;&#20855;&#26377;&#23618;&#27425;&#32467;&#26500;&#30340;&#25104;&#23601;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#36825;&#38656;&#35201;&#26234;&#33021;&#20307;&#20855;&#22791;&#24191;&#27867;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#27867;&#21270;&#21644;&#38271;&#26399;&#25512;&#29702;&#12290;&#35768;&#22810;&#20808;&#21069;&#30340;&#26041;&#27861;&#22522;&#20110;&#27169;&#22411;&#39537;&#21160;&#25110;&#23618;&#27425;&#21270;&#26041;&#27861;&#65292;&#35748;&#20026;&#26174;&#24335;&#30340;&#38271;&#26399;&#35268;&#21010;&#27169;&#22359;&#23545;&#20110;&#23398;&#20064;&#23618;&#27425;&#21270;&#25104;&#23601;&#26159;&#26377;&#30410;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#30340;&#29615;&#22659;&#20132;&#20114;&#25110;&#22823;&#22411;&#27169;&#22411;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#23454;&#29992;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#36817;&#26399;&#23454;&#26045;&#23454;&#36341;&#20013;&#30340;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;PPO&#65289;&#31639;&#27861;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;PPO&#26234;&#33021;&#20307;&#21487;&#20197;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#39044;&#27979;&#19979;&#19968;&#20010;&#35201;&#35299;&#38145;&#30340;&#25104;&#23601;&#65292;&#23613;&#31649;&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;&#36739;&#20302;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;&#25104;&#23601;&#33976;&#39311;&#65292;&#21487;&#20197;&#21152;&#24378;PPO&#26234;&#33021;&#20307;&#23545;&#19979;&#19968;&#20010;&#35299;&#38145;&#25104;&#23601;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discovering achievements with a hierarchical structure on procedurally generated environments poses a significant challenge. This requires agents to possess a broad range of abilities, including generalization and long-term reasoning. Many prior methods are built upon model-based or hierarchical approaches, with the belief that an explicit module for long-term planning would be beneficial for learning hierarchical achievements. However, these methods require an excessive amount of environment interactions or large model sizes, limiting their practicality. In this work, we identify that proximal policy optimization (PPO), a simple and versatile model-free algorithm, outperforms the prior methods with recent implementation practices. Moreover, we find that the PPO agent can predict the next achievement to be unlocked to some extent, though with low confidence. Based on this observation, we propose a novel contrastive learning method, called achievement distillation, that strengthens the 
&lt;/p&gt;</description></item><item><title>&#26080;&#37197;&#23545;&#22810;&#35270;&#22270;&#22270;&#32858;&#31867;&#26041;&#27861;&#32467;&#21512;&#36328;&#35270;&#22270;&#32467;&#26500;&#21305;&#37197;&#65292;&#35299;&#20915;&#20102;&#22810;&#35270;&#22270;&#25968;&#25454;&#26410;&#37197;&#23545;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#32858;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.03476</link><description>&lt;p&gt;
&#26080;&#37197;&#23545;&#22810;&#35270;&#22270;&#22270;&#32858;&#31867;&#19982;&#36328;&#35270;&#22270;&#32467;&#26500;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Unpaired Multi-View Graph Clustering with Cross-View Structure Matching. (arXiv:2307.03476v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03476
&lt;/p&gt;
&lt;p&gt;
&#26080;&#37197;&#23545;&#22810;&#35270;&#22270;&#22270;&#32858;&#31867;&#26041;&#27861;&#32467;&#21512;&#36328;&#35270;&#22270;&#32467;&#26500;&#21305;&#37197;&#65292;&#35299;&#20915;&#20102;&#22810;&#35270;&#22270;&#25968;&#25454;&#26410;&#37197;&#23545;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#32858;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35270;&#22270;&#32858;&#31867;&#26041;&#27861;(MVC)&#36890;&#36807;&#26377;&#25928;&#22320;&#34701;&#21512;&#22810;&#20010;&#35270;&#22270;&#30340;&#20449;&#24687;&#20197;&#25552;&#39640;&#24615;&#33021;&#32780;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;MVC&#26041;&#27861;&#20551;&#35774;&#22810;&#35270;&#22270;&#25968;&#25454;&#26159;&#23436;&#20840;&#37197;&#23545;&#30340;&#65292;&#36825;&#24847;&#21619;&#30528;&#35270;&#22270;&#20043;&#38388;&#25152;&#26377;&#23545;&#24212;&#26679;&#26412;&#30340;&#26144;&#23556;&#26159;&#39044;&#20808;&#23450;&#20041;&#25110;&#25552;&#21069;&#32473;&#23450;&#30340;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#25968;&#25454;&#23545;&#24212;&#24120;&#24120;&#19981;&#23436;&#25972;&#65292;&#36825;&#26159;&#30001;&#20110;&#25968;&#25454;&#25439;&#22351;&#25110;&#20256;&#24863;&#22120;&#24046;&#24322;&#24341;&#36215;&#30340;&#22810;&#35270;&#22270;&#25991;&#29486;&#20013;&#30340;&#25968;&#25454;&#26410;&#37197;&#23545;&#38382;&#39064;(DUP)&#12290;&#23613;&#31649;&#24050;&#32463;&#23581;&#35797;&#35299;&#20915;DUP&#38382;&#39064;&#65292;&#20294;&#23384;&#22312;&#20197;&#19979;&#38382;&#39064;&#65306;1&#65289;&#22823;&#22810;&#25968;&#26041;&#27861;&#20851;&#27880;&#29305;&#24449;&#34920;&#31034;&#32780;&#24573;&#35270;&#20102;&#22810;&#35270;&#22270;&#25968;&#25454;&#30340;&#32467;&#26500;&#20449;&#24687;&#65292;&#32780;&#36825;&#23545;&#20110;&#32858;&#31867;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#65307;2&#65289;&#29616;&#26377;&#30340;&#37096;&#20998;&#26410;&#37197;&#23545;&#38382;&#39064;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#39044;&#20808;&#32473;&#23450;&#30340;&#36328;&#35270;&#22270;&#23545;&#40784;&#20449;&#24687;&#65292;&#23548;&#33268;&#23427;&#20204;&#26080;&#27861;&#22788;&#29702;&#23436;&#20840;&#26410;&#37197;&#23545;&#30340;&#38382;&#39064;&#65307;3&#65289;&#23427;&#20204;&#19981;&#21487;&#36991;&#20813;&#22320;&#23384;&#22312;&#21442;&#25968;&#38477;&#20302;&#20102;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-view clustering (MVC), which effectively fuses information from multiple views for better performance, has received increasing attention. Most existing MVC methods assume that multi-view data are fully paired, which means that the mappings of all corresponding samples between views are pre-defined or given in advance. However, the data correspondence is often incomplete in real-world applications due to data corruption or sensor differences, referred as the data-unpaired problem (DUP) in multi-view literature. Although several attempts have been made to address the DUP issue, they suffer from the following drawbacks: 1) Most methods focus on the feature representation while ignoring the structural information of multi-view data, which is essential for clustering tasks; 2) Existing methods for partially unpaired problems rely on pre-given cross-view alignment information, resulting in their inability to handle fully unpaired problems; 3) Their inevitable parameters degrade the eff
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#25345;&#32493;&#35266;&#23519;&#19979;&#30340;&#24046;&#20998;&#38544;&#31169;&#32858;&#31867;&#26426;&#21046;&#65292;&#29992;&#20110;&#22312;&#34987;&#21024;&#38500;&#21644;&#25554;&#20837;&#25968;&#25454;&#28857;&#30340;&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#32858;&#31867;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20855;&#26377;&#20165;&#20197;&#26356;&#26032;&#27425;&#25968;&#30340;&#23545;&#25968;&#20381;&#36182;&#24615;&#30340;&#22686;&#21152;&#35823;&#24046;&#30340;&#36817;&#20284;&#31639;&#27861;&#65292;&#24182;&#19988;&#20056;&#27861;&#35823;&#24046;&#20960;&#20046;&#19982;&#38750;&#38544;&#31169;&#24773;&#20917;&#30456;&#21516;&#12290;</title><link>http://arxiv.org/abs/2307.03430</link><description>&lt;p&gt;
&#22312;&#25345;&#32493;&#35266;&#23519;&#19979;&#30340;&#32858;&#31867;&#38382;&#39064;&#20013;&#30340;&#24046;&#20998;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
Differential Privacy for Clustering Under Continual Observation. (arXiv:2307.03430v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03430
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#25345;&#32493;&#35266;&#23519;&#19979;&#30340;&#24046;&#20998;&#38544;&#31169;&#32858;&#31867;&#26426;&#21046;&#65292;&#29992;&#20110;&#22312;&#34987;&#21024;&#38500;&#21644;&#25554;&#20837;&#25968;&#25454;&#28857;&#30340;&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#32858;&#31867;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20855;&#26377;&#20165;&#20197;&#26356;&#26032;&#27425;&#25968;&#30340;&#23545;&#25968;&#20381;&#36182;&#24615;&#30340;&#22686;&#21152;&#35823;&#24046;&#30340;&#36817;&#20284;&#31639;&#27861;&#65292;&#24182;&#19988;&#20056;&#27861;&#35823;&#24046;&#20960;&#20046;&#19982;&#38750;&#38544;&#31169;&#24773;&#20917;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;$\mathbb{R}^d$&#20013;&#36827;&#34892;&#38544;&#31169;&#32858;&#31867;&#30340;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#22312;&#25968;&#25454;&#38598;&#20013;&#25554;&#20837;&#21644;&#21024;&#38500;&#28857;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22312;&#25345;&#32493;&#35266;&#23519;&#19979;&#30340;$\varepsilon$-&#24046;&#20998;&#38544;&#31169;&#32858;&#31867;&#26426;&#21046;&#65292;&#29992;&#20110; $k$-means &#30446;&#26631;&#12290;&#36825;&#26159;&#35813;&#38382;&#39064;&#30340;&#31532;&#19968;&#20010;&#36817;&#20284;&#31639;&#27861;&#65292;&#20854;&#22686;&#21152;&#30340;&#35823;&#24046;&#20165;&#20197;&#26356;&#26032;&#27425;&#25968; $T$ &#30340;&#23545;&#25968;&#20381;&#36182;&#24615;&#12290;&#20056;&#27861;&#35823;&#24046;&#19982;&#38750;&#38544;&#31169;&#24773;&#20917;&#20960;&#20046;&#30456;&#21516;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#25345;&#32493;&#35266;&#23519;&#20013;&#36827;&#34892;&#32500;&#24230;&#32553;&#20943;&#65292;&#24182;&#23558;&#20854;&#19982;&#29992;&#20110; $k$-means &#30340;&#24046;&#20998;&#38544;&#31169;&#36138;&#24515;&#36924;&#36817;&#31639;&#27861;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#36824;&#37096;&#20998;&#22320;&#23558;&#25105;&#20204;&#30340;&#32467;&#26524;&#25512;&#24191;&#21040; $k$-median &#38382;&#39064;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of clustering privately a dataset in $\mathbb{R}^d$ that undergoes both insertion and deletion of points. Specifically, we give an $\varepsilon$-differentially private clustering mechanism for the $k$-means objective under continual observation. This is the first approximation algorithm for that problem with an additive error that depends only logarithmically in the number $T$ of updates. The multiplicative error is almost the same as non privately. To do so we show how to perform dimension reduction under continual observation and combine it with a differentially private greedy approximation algorithm for $k$-means. We also partially extend our results to the $k$-median problem.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21512;&#24182;-&#20998;&#27969;&#28151;&#21512;Transformer&#32593;&#32476;&#29992;&#20110;&#22836;&#39048;&#30284;&#30340;&#29983;&#23384;&#39044;&#27979;&#12290;&#35813;&#32593;&#32476;&#21033;&#29992;&#22810;&#27169;&#24577;&#24433;&#20687;&#21644;&#25552;&#21462;&#29305;&#23450;&#21306;&#22495;&#20449;&#24687;&#65292;&#36890;&#36807;&#21512;&#24182;&#32534;&#30721;&#22120;&#21644;&#20998;&#27969;&#35299;&#30721;&#22120;&#23454;&#29616;&#29983;&#23384;&#39044;&#27979;&#65292;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.03427</link><description>&lt;p&gt;
&#21512;&#24182;-&#20998;&#27969;&#28151;&#21512;Transformer&#32593;&#32476;&#29992;&#20110;&#22836;&#39048;&#30284;&#30340;&#29983;&#23384;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Merging-Diverging Hybrid Transformer Networks for Survival Prediction in Head and Neck Cancer. (arXiv:2307.03427v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03427
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21512;&#24182;-&#20998;&#27969;&#28151;&#21512;Transformer&#32593;&#32476;&#29992;&#20110;&#22836;&#39048;&#30284;&#30340;&#29983;&#23384;&#39044;&#27979;&#12290;&#35813;&#32593;&#32476;&#21033;&#29992;&#22810;&#27169;&#24577;&#24433;&#20687;&#21644;&#25552;&#21462;&#29305;&#23450;&#21306;&#22495;&#20449;&#24687;&#65292;&#36890;&#36807;&#21512;&#24182;&#32534;&#30721;&#22120;&#21644;&#20998;&#27969;&#35299;&#30721;&#22120;&#23454;&#29616;&#29983;&#23384;&#39044;&#27979;&#65292;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#23384;&#39044;&#27979;&#23545;&#20110;&#30284;&#30151;&#24739;&#32773;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20026;&#27835;&#30103;&#35745;&#21010;&#25552;&#20379;&#20102;&#26089;&#26399;&#39044;&#21518;&#20449;&#24687;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#21307;&#23398;&#24433;&#20687;&#30340;&#28145;&#24230;&#29983;&#23384;&#27169;&#22411;&#22312;&#29983;&#23384;&#39044;&#27979;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#28145;&#24230;&#29983;&#23384;&#27169;&#22411;&#22312;&#21033;&#29992;&#22810;&#27169;&#24577;&#24433;&#20687;&#65288;&#22914;PET-CT&#65289;&#21644;&#25552;&#21462;&#29305;&#23450;&#21306;&#22495;&#20449;&#24687;&#65288;&#22914;&#21407;&#21457;&#32959;&#30244;&#65288;PT&#65289;&#21644;&#36716;&#31227;&#28107;&#24052;&#32467;&#65288;MLN&#65289;&#21306;&#22495;&#30340;&#39044;&#21518;&#20449;&#24687;&#65289;&#26041;&#38754;&#23578;&#26410;&#24471;&#21040;&#24456;&#22909;&#30340;&#21457;&#23637;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#20174;&#22810;&#27169;&#24577;&#24433;&#20687;&#36827;&#34892;&#29983;&#23384;&#39044;&#27979;&#30340;&#21512;&#24182;-&#20998;&#27969;&#23398;&#20064;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#20855;&#26377;&#19968;&#20010;&#21512;&#24182;&#32534;&#30721;&#22120;&#29992;&#20110;&#34701;&#21512;&#22810;&#27169;&#24577;&#20449;&#24687;&#65292;&#20197;&#21450;&#19968;&#20010;&#20998;&#27969;&#35299;&#30721;&#22120;&#29992;&#20110;&#25552;&#21462;&#29305;&#23450;&#21306;&#22495;&#30340;&#20449;&#24687;&#12290;&#22312;&#21512;&#24182;&#32534;&#30721;&#22120;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#24182;&#34892;&#20132;&#21449;&#27880;&#24847;&#21147;&#65288;HPCA&#65289;&#22359;&#65292;&#36890;&#36807;&#24182;&#34892;&#21367;&#31215;&#23618;&#21644;&#20132;&#21449;&#27880;&#24847;&#21147;Transformer&#26377;&#25928;&#22320;&#34701;&#21512;&#22810;&#27169;&#24577;&#29305;&#24449;&#12290;&#22312;&#20998;&#27969;&#35299;&#30721;&#22120;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21306;&#22495;
&lt;/p&gt;
&lt;p&gt;
Survival prediction is crucial for cancer patients as it provides early prognostic information for treatment planning. Recently, deep survival models based on deep learning and medical images have shown promising performance for survival prediction. However, existing deep survival models are not well developed in utilizing multi-modality images (e.g., PET-CT) and in extracting region-specific information (e.g., the prognostic information in Primary Tumor (PT) and Metastatic Lymph Node (MLN) regions). In view of this, we propose a merging-diverging learning framework for survival prediction from multi-modality images. This framework has a merging encoder to fuse multi-modality information and a diverging decoder to extract region-specific information. In the merging encoder, we propose a Hybrid Parallel Cross-Attention (HPCA) block to effectively fuse multi-modality features via parallel convolutional layers and cross-attention transformers. In the diverging decoder, we propose a Region
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#39640;&#20809;&#35889;&#21644;&#22810;&#20809;&#35889;&#22270;&#20687;&#34701;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#21069;&#21521;&#25193;&#25955;&#36807;&#31243;&#21644;&#36870;&#21521;&#38477;&#22122;&#36807;&#31243;&#23454;&#29616;&#23545;&#39640;&#31354;&#38388;&#20998;&#36776;&#29575;HSI&#30340;&#21435;&#22122;&#21644;&#22797;&#21407;&#12290;</title><link>http://arxiv.org/abs/2307.03423</link><description>&lt;p&gt;
&#20351;&#29992;&#26465;&#20214;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#36827;&#34892;&#39640;&#20809;&#35889;&#21644;&#22810;&#20809;&#35889;&#22270;&#20687;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Hyperspectral and Multispectral Image Fusion Using the Conditional Denoising Diffusion Probabilistic Model. (arXiv:2307.03423v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03423
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#39640;&#20809;&#35889;&#21644;&#22810;&#20809;&#35889;&#22270;&#20687;&#34701;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#21069;&#21521;&#25193;&#25955;&#36807;&#31243;&#21644;&#36870;&#21521;&#38477;&#22122;&#36807;&#31243;&#23454;&#29616;&#23545;&#39640;&#31354;&#38388;&#20998;&#36776;&#29575;HSI&#30340;&#21435;&#22122;&#21644;&#22797;&#21407;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#20809;&#35889;&#22270;&#20687;&#65288;HSI&#65289;&#20855;&#26377;&#22823;&#37327;&#21453;&#26144;&#29289;&#36136;&#29305;&#24615;&#30340;&#20809;&#35889;&#20449;&#24687;&#65292;&#20294;&#30001;&#20110;&#25104;&#20687;&#25216;&#26415;&#30340;&#38480;&#21046;&#65292;&#20854;&#31354;&#38388;&#20998;&#36776;&#29575;&#36739;&#20302;&#12290;&#19982;&#27492;&#30456;&#34917;&#20805;&#30340;&#26159;&#22810;&#20809;&#35889;&#22270;&#20687;&#65288;MSI&#65289;&#65292;&#22914;RGB&#22270;&#20687;&#65292;&#20855;&#26377;&#39640;&#31354;&#38388;&#20998;&#36776;&#29575;&#20294;&#20809;&#35889;&#27874;&#27573;&#19981;&#36275;&#12290;&#39640;&#20809;&#35889;&#21644;&#22810;&#20809;&#35889;&#22270;&#20687;&#34701;&#21512;&#26159;&#19968;&#31181;&#20197;&#25104;&#26412;&#25928;&#30410;&#30340;&#26041;&#24335;&#33719;&#21462;&#26082;&#20855;&#26377;&#39640;&#31354;&#38388;&#20998;&#36776;&#29575;&#21448;&#20855;&#26377;&#39640;&#20809;&#35889;&#20998;&#36776;&#29575;&#30340;&#29702;&#24819;&#22270;&#20687;&#30340;&#25216;&#26415;&#12290;&#35768;&#22810;&#29616;&#26377;&#30340;HSI&#21644;MSI&#34701;&#21512;&#31639;&#27861;&#20381;&#36182;&#20110;&#24050;&#30693;&#30340;&#25104;&#20687;&#36864;&#21270;&#27169;&#22411;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#22312;&#23454;&#36341;&#20013;&#36890;&#24120;&#19981;&#21487;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#28145;&#24230;&#34701;&#21512;&#26041;&#27861;&#65292;&#31216;&#20026;DDPM-Fus&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;DDPM-Fus&#21253;&#21547;&#20102;&#36880;&#27493;&#21521;&#39640;&#31354;&#38388;&#20998;&#36776;&#29575;HSI&#65288;HrHSI&#65289;&#28155;&#21152;&#39640;&#26031;&#22122;&#22768;&#30340;&#21069;&#21521;&#25193;&#25955;&#36807;&#31243;&#21644;&#20174;&#22122;&#22768;&#29256;&#26412;&#20013;&#23398;&#20064;&#39044;&#27979;&#25152;&#38656;HrHSI&#30340;&#36870;&#21521;&#38477;&#22122;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperspectral images (HSI) have a large amount of spectral information reflecting the characteristics of matter, while their spatial resolution is low due to the limitations of imaging technology. Complementary to this are multispectral images (MSI), e.g., RGB images, with high spatial resolution but insufficient spectral bands. Hyperspectral and multispectral image fusion is a technique for acquiring ideal images that have both high spatial and high spectral resolution cost-effectively. Many existing HSI and MSI fusion algorithms rely on known imaging degradation models, which are often not available in practice. In this paper, we propose a deep fusion method based on the conditional denoising diffusion probabilistic model, called DDPM-Fus. Specifically, the DDPM-Fus contains the forward diffusion process which gradually adds Gaussian noise to the high spatial resolution HSI (HrHSI) and another reverse denoising process which learns to predict the desired HrHSI from its noisy version 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LFH&#30340;&#36229;&#22270;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#21160;&#24577;&#26500;&#24314;&#36229;&#36793;&#24182;&#21033;&#29992;&#24322;&#36136;&#23646;&#24615;&#36827;&#34892;&#23884;&#20837;&#26356;&#26032;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.03411</link><description>&lt;p&gt;
&#20174;&#24322;&#36136;&#24615;&#20013;&#23398;&#20064;&#65306;&#29992;&#20110;&#36229;&#22270;&#30340;&#21160;&#24577;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Learning from Heterogeneity: A Dynamic Learning Framework for Hypergraphs. (arXiv:2307.03411v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03411
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LFH&#30340;&#36229;&#22270;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#21160;&#24577;&#26500;&#24314;&#36229;&#36793;&#24182;&#21033;&#29992;&#24322;&#36136;&#23646;&#24615;&#36827;&#34892;&#23884;&#20837;&#26356;&#26032;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22240;&#20854;&#22312;&#24314;&#27169;&#22797;&#26434;&#22270;&#32467;&#26500;&#25968;&#25454;&#26041;&#38754;&#30340;&#33021;&#21147;&#21644;&#28789;&#27963;&#24615;&#32780;&#22312;&#36817;&#24180;&#26469;&#26085;&#30410;&#21463;&#21040;&#20851;&#27880;&#12290;&#22312;&#25152;&#26377;&#22270;&#23398;&#20064;&#26041;&#27861;&#20013;&#65292;&#36229;&#22270;&#23398;&#20064;&#26159;&#19968;&#31181;&#22312;&#35757;&#32451;&#22270;&#30340;&#23884;&#20837;&#31354;&#38388;&#26102;&#25506;&#32034;&#38544;&#21547;&#30340;&#39640;&#38454;&#20851;&#32852;&#30340;&#25216;&#26415;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LFH&#30340;&#36229;&#22270;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#21033;&#29992;&#22270;&#30340;&#24322;&#36136;&#23646;&#24615;&#36827;&#34892;&#21160;&#24577;&#36229;&#36793;&#26500;&#24314;&#21644;&#20851;&#27880;&#24615;&#23884;&#20837;&#26356;&#26032;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#39318;&#20808;&#36890;&#36807;&#21033;&#29992;&#26174;&#24335;&#30340;&#22270;&#32467;&#26500;&#20449;&#24687;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#29305;&#24449;&#21521;&#37327;&#12290;&#28982;&#21518;&#36890;&#36807;&#38544;&#24335;&#36229;&#36793;&#30340;&#21160;&#24577;&#20998;&#32452;&#26469;&#26500;&#24314;&#36229;&#22270;&#65292;&#24182;&#36827;&#34892;&#31867;&#22411;&#29305;&#23450;&#30340;&#36229;&#22270;&#23398;&#20064;&#36807;&#31243;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#20960;&#20010;&#27969;&#34892;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural network (GNN) has gained increasing popularity in recent years owing to its capability and flexibility in modeling complex graph structure data. Among all graph learning methods, hypergraph learning is a technique for exploring the implicit higher-order correlations when training the embedding space of the graph. In this paper, we propose a hypergraph learning framework named LFH that is capable of dynamic hyperedge construction and attentive embedding update utilizing the heterogeneity attributes of the graph. Specifically, in our framework, the high-quality features are first generated by the pairwise fusion strategy that utilizes explicit graph structure information when generating initial node embedding. Afterwards, a hypergraph is constructed through the dynamic grouping of implicit hyperedges, followed by the type-specific hypergraph learning process. To evaluate the effectiveness of our proposed framework, we conduct comprehensive experiments on several popular data
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#29305;&#24449;&#20998;&#24067;&#24335;&#25968;&#25454;&#30340;&#21487;&#25193;&#23637;&#39640;&#32500;&#22810;&#21464;&#37327;&#32447;&#24615;&#22238;&#24402;&#31639;&#27861;&#65292;&#20855;&#26377;&#36890;&#20449;&#22797;&#26434;&#24230;&#19981;&#20381;&#36182;&#20110;&#29305;&#24449;&#32500;&#24230;&#21644;&#24555;&#36895;&#25910;&#25947;&#24615;&#30340;&#20248;&#21183;&#65292;&#21487;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21644;&#20855;&#26377;&#22810;&#21464;&#37327;&#21709;&#24212;&#21464;&#37327;&#30340;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2307.03410</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#39640;&#32500;&#22810;&#21464;&#37327;&#32447;&#24615;&#22238;&#24402;&#29992;&#20110;&#29305;&#24449;&#20998;&#24067;&#24335;&#25968;&#25454;&#32763;&#35793;&#26631;&#39064;
&lt;/p&gt;
&lt;p&gt;
Scalable High-Dimensional Multivariate Linear Regression for Feature-Distributed Data. (arXiv:2307.03410v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03410
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#29305;&#24449;&#20998;&#24067;&#24335;&#25968;&#25454;&#30340;&#21487;&#25193;&#23637;&#39640;&#32500;&#22810;&#21464;&#37327;&#32447;&#24615;&#22238;&#24402;&#31639;&#27861;&#65292;&#20855;&#26377;&#36890;&#20449;&#22797;&#26434;&#24230;&#19981;&#20381;&#36182;&#20110;&#29305;&#24449;&#32500;&#24230;&#21644;&#24555;&#36895;&#25910;&#25947;&#24615;&#30340;&#20248;&#21183;&#65292;&#21487;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21644;&#20855;&#26377;&#22810;&#21464;&#37327;&#21709;&#24212;&#21464;&#37327;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#20998;&#24067;&#24335;&#25968;&#25454;&#26159;&#25351;&#26681;&#25454;&#29305;&#24449;&#21010;&#20998;&#24182;&#23384;&#20648;&#22312;&#22810;&#20010;&#35745;&#31639;&#33410;&#28857;&#19978;&#30340;&#25968;&#25454;&#65292;&#22312;&#20855;&#26377;&#22823;&#37327;&#29305;&#24449;&#30340;&#24212;&#29992;&#20013;&#36234;&#26469;&#36234;&#24120;&#35265;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#36825;&#31181;&#25968;&#25454;&#30340;&#20004;&#38454;&#27573;&#25918;&#26494;&#36138;&#23146;&#31639;&#27861; (TSRGA)&#65292;&#29992;&#20110;&#24212;&#29992;&#22810;&#21464;&#37327;&#32447;&#24615;&#22238;&#24402;&#12290;TSRGA &#30340;&#20027;&#35201;&#20248;&#21183;&#22312;&#20110;&#20854;&#36890;&#20449;&#22797;&#26434;&#24230;&#19981;&#20381;&#36182;&#20110;&#29305;&#24449;&#32500;&#24230;&#65292;&#20351;&#20854;&#33021;&#22815;&#39640;&#24230;&#25193;&#23637;&#21040;&#38750;&#24120;&#22823;&#30340;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#22810;&#21464;&#37327;&#21709;&#24212;&#21464;&#37327;&#65292;TSRGA &#21487;&#29992;&#20110;&#20135;&#29983;&#20302;&#31209;&#31995;&#25968;&#20272;&#35745;&#12290;&#36890;&#36807;&#27169;&#25311;&#23454;&#39564;&#35777;&#26126;&#20102;TSRGA &#30340;&#24555;&#36895;&#25910;&#25947;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#25552;&#20986;&#30340;TSRGA &#24212;&#29992;&#20110;&#19968;&#31181;&#37329;&#34701;&#24212;&#29992;&#20013;&#65292;&#21033;&#29992;&#26469;&#33258; 10-K &#25253;&#21578;&#30340;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#20855;&#26377;&#35768;&#22810;&#23494;&#38598;&#22823;&#32500;&#30697;&#38453;&#30340;&#24212;&#29992;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feature-distributed data, referred to data partitioned by features and stored across multiple computing nodes, are increasingly common in applications with a large number of features. This paper proposes a two-stage relaxed greedy algorithm (TSRGA) for applying multivariate linear regression to such data. The main advantage of TSRGA is that its communication complexity does not depend on the feature dimension, making it highly scalable to very large data sets. In addition, for multivariate response variables, TSRGA can be used to yield low-rank coefficient estimates. The fast convergence of TSRGA is validated by simulation experiments. Finally, we apply the proposed TSRGA in a financial application that leverages unstructured data from the 10-K reports, demonstrating its usefulness in applications with many dense large-dimensional matrices.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#20915;&#31574;&#21046;&#23450;&#35270;&#20026;&#31163;&#32447;&#25910;&#38598;&#30340;&#36712;&#36857;&#30340;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25506;&#31350;&#20102;&#24207;&#21015;&#24314;&#27169;&#22312;&#36712;&#36857;&#21387;&#32553;&#21644;&#31574;&#30053;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#12290;&#36890;&#36807;&#24341;&#20837;&#30446;&#26631;&#26465;&#20214;&#39044;&#27979;&#32534;&#30721;&#65288;GCPC&#65289;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#20135;&#29983;&#24378;&#22823;&#36712;&#36857;&#34920;&#31034;&#24182;&#23454;&#29616;&#39640;&#24615;&#33021;&#31574;&#30053;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.03406</link><description>&lt;p&gt;
&#20351;&#29992;&#30446;&#26631;&#26465;&#20214;&#39044;&#27979;&#32534;&#30721;&#20316;&#20026;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#38544;&#24335;&#35268;&#21010;&#22120;
&lt;/p&gt;
&lt;p&gt;
Goal-Conditioned Predictive Coding as an Implicit Planner for Offline Reinforcement Learning. (arXiv:2307.03406v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#20915;&#31574;&#21046;&#23450;&#35270;&#20026;&#31163;&#32447;&#25910;&#38598;&#30340;&#36712;&#36857;&#30340;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25506;&#31350;&#20102;&#24207;&#21015;&#24314;&#27169;&#22312;&#36712;&#36857;&#21387;&#32553;&#21644;&#31574;&#30053;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#12290;&#36890;&#36807;&#24341;&#20837;&#30446;&#26631;&#26465;&#20214;&#39044;&#27979;&#32534;&#30721;&#65288;GCPC&#65289;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#20135;&#29983;&#24378;&#22823;&#36712;&#36857;&#34920;&#31034;&#24182;&#23454;&#29616;&#39640;&#24615;&#33021;&#31574;&#30053;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#20102;&#23558;&#20915;&#31574;&#21046;&#23450;&#35270;&#20026;&#31163;&#32447;&#25910;&#38598;&#30340;&#36712;&#36857;&#30340;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#36712;&#36857;&#25968;&#25454;&#19978;&#36827;&#34892;&#24207;&#21015;&#24314;&#27169;&#30340;&#22909;&#22788;&#23578;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24207;&#21015;&#24314;&#27169;&#26159;&#21542;&#20855;&#22791;&#23558;&#36712;&#36857;&#21387;&#32553;&#20026;&#26377;&#29992;&#34920;&#31034;&#24182;&#23545;&#31574;&#30053;&#23398;&#20064;&#26377;&#25152;&#36129;&#29486;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#26694;&#26550;&#65292;&#39318;&#20808;&#20351;&#29992;&#24207;&#21015;&#24314;&#27169;&#25216;&#26415;&#24635;&#32467;&#36712;&#36857;&#65292;&#28982;&#21518;&#21033;&#29992;&#36825;&#20123;&#34920;&#31034;&#23398;&#20064;&#31574;&#30053;&#20197;&#21450;&#19968;&#20010;&#26399;&#26395;&#30340;&#30446;&#26631;&#12290;&#36825;&#20010;&#35774;&#35745;&#20351;&#24471;&#35768;&#22810;&#29616;&#26377;&#30340;&#30417;&#30563;&#24335;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#25105;&#20204;&#26694;&#26550;&#30340;&#29305;&#20363;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#30446;&#26631;&#26465;&#20214;&#39044;&#27979;&#32534;&#30721;&#65288;GCPC&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#24102;&#26469;&#24378;&#22823;&#36712;&#36857;&#34920;&#31034;&#24182;&#23548;&#33268;&#39640;&#24615;&#33021;&#31574;&#30053;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;AntMaze&#65292;FrankaKitchen&#21644;Locomotion&#29615;&#22659;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#24182;&#35266;&#23519;&#21040;...
&lt;/p&gt;
&lt;p&gt;
Recent work has demonstrated the effectiveness of formulating decision making as a supervised learning problem on offline-collected trajectories. However, the benefits of performing sequence modeling on trajectory data is not yet clear. In this work we investigate if sequence modeling has the capability to condense trajectories into useful representations that can contribute to policy learning. To achieve this, we adopt a two-stage framework that first summarizes trajectories with sequence modeling techniques, and then employs these representations to learn a policy along with a desired goal. This design allows many existing supervised offline RL methods to be considered as specific instances of our framework. Within this framework, we introduce Goal-Conditioned Predicitve Coding (GCPC), an approach that brings powerful trajectory representations and leads to performant policies. We conduct extensive empirical evaluations on AntMaze, FrankaKitchen and Locomotion environments, and obser
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22270;&#23398;&#20064;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#23581;&#35797;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#27969;&#31243;&#65306;&#23558;LLMs&#20316;&#20026;&#22686;&#24378;&#22120;&#36890;&#36807;&#28023;&#37327;&#30693;&#35782;&#26469;&#22686;&#24378;&#33410;&#28857;&#30340;&#25991;&#26412;&#23646;&#24615;&#65292;&#24182;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#29983;&#25104;&#39044;&#27979;&#65292;&#20197;&#21450;&#30452;&#25509;&#20351;&#29992;LLMs&#20316;&#20026;&#29420;&#31435;&#30340;&#39044;&#27979;&#22120;&#12290;</title><link>http://arxiv.org/abs/2307.03393</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22270;&#23398;&#20064;&#20013;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Exploring the Potential of Large Language Models (LLMs) in Learning on Graphs. (arXiv:2307.03393v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03393
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22270;&#23398;&#20064;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#23581;&#35797;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#27969;&#31243;&#65306;&#23558;LLMs&#20316;&#20026;&#22686;&#24378;&#22120;&#36890;&#36807;&#28023;&#37327;&#30693;&#35782;&#26469;&#22686;&#24378;&#33410;&#28857;&#30340;&#25991;&#26412;&#23646;&#24615;&#65292;&#24182;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#29983;&#25104;&#39044;&#27979;&#65292;&#20197;&#21450;&#30452;&#25509;&#20351;&#29992;LLMs&#20316;&#20026;&#29420;&#31435;&#30340;&#39044;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#23398;&#20064;&#22240;&#20854;&#24191;&#27867;&#30340;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#32780;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20851;&#27880;&#12290;&#20197;&#25991;&#26412;&#33410;&#28857;&#23646;&#24615;&#20026;&#20027;&#30340;&#22270;&#23398;&#20064;&#26368;&#27969;&#34892;&#30340;&#27969;&#31243;&#20027;&#35201;&#20381;&#36182;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#65292;&#24182;&#21033;&#29992;&#27973;&#23618;&#25991;&#26412;&#23884;&#20837;&#20316;&#20026;&#21021;&#22987;&#33410;&#28857;&#34920;&#31034;&#65292;&#20294;&#23384;&#22312;&#36890;&#29992;&#30693;&#35782;&#21644;&#28145;&#21051;&#35821;&#20041;&#29702;&#35299;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;&#36817;&#24180;&#26469;&#65292;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34987;&#35777;&#26126;&#20855;&#26377;&#24191;&#27867;&#30340;&#24120;&#35782;&#21644;&#24378;&#22823;&#30340;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#65292;&#24050;&#32463;&#39072;&#35206;&#20102;&#29616;&#26377;&#30340;&#22788;&#29702;&#25991;&#26412;&#25968;&#25454;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25506;&#32034;LLMs&#22312;&#22270;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#30740;&#31350;&#20004;&#31181;&#21487;&#33021;&#30340;&#27969;&#31243;&#65306;LLMs&#20316;&#20026;&#22686;&#24378;&#22120;&#21644;LLMs&#20316;&#20026;&#39044;&#27979;&#22120;&#12290;&#21069;&#32773;&#21033;&#29992;LLMs&#36890;&#36807;&#20854;&#28023;&#37327;&#30693;&#35782;&#22686;&#24378;&#33410;&#28857;&#30340;&#25991;&#26412;&#23646;&#24615;&#65292;&#28982;&#21518;&#36890;&#36807;GNNs&#29983;&#25104;&#39044;&#27979;&#12290;&#21518;&#32773;&#35797;&#22270;&#30452;&#25509;&#20351;&#29992;LLMs&#20316;&#20026;&#29420;&#31435;&#30340;&#39044;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning on Graphs has attracted immense attention due to its wide real-world applications. The most popular pipeline for learning on graphs with textual node attributes primarily relies on Graph Neural Networks (GNNs), and utilizes shallow text embedding as initial node representations, which has limitations in general knowledge and profound semantic understanding. In recent years, Large Language Models (LLMs) have been proven to possess extensive common knowledge and powerful semantic comprehension abilities that have revolutionized existing workflows to handle text data. In this paper, we aim to explore the potential of LLMs in graph machine learning, especially the node classification task, and investigate two possible pipelines: LLMs-as-Enhancers and LLMs-as-Predictors. The former leverages LLMs to enhance nodes' text attributes with their massive knowledge and then generate predictions through GNNs. The latter attempts to directly employ LLMs as standalone predictors. We conduct 
&lt;/p&gt;</description></item><item><title>&#20197;&#23398;&#20064;&#19982;&#20998;&#27495;&#30340;&#26426;&#21046;&#20026;&#26694;&#26550;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24615;&#21035;&#27495;&#35270;&#35782;&#21035;&#21644;&#34920;&#24449;&#30340;&#30740;&#31350;&#65292;&#20197;&#25512;&#21160;&#26356;&#20855;&#21253;&#23481;&#24615;&#21644;&#23562;&#37325;&#24615;&#30340;&#22312;&#32447;&#29615;&#22659;&#12290;</title><link>http://arxiv.org/abs/2307.03385</link><description>&lt;p&gt;
AI-UPV&#22312;EXIST 2023&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#8220;&#23398;&#20064;&#19982;&#20998;&#27495;&#8221;&#30340;&#26694;&#26550;&#19979;&#23545;&#24615;&#21035;&#27495;&#35270;&#36827;&#34892;&#34920;&#24449;
&lt;/p&gt;
&lt;p&gt;
AI-UPV at EXIST 2023 -- Sexism Characterization Using Large Language Models Under The Learning with Disagreements Regime. (arXiv:2307.03385v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03385
&lt;/p&gt;
&lt;p&gt;
&#20197;&#23398;&#20064;&#19982;&#20998;&#27495;&#30340;&#26426;&#21046;&#20026;&#26694;&#26550;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24615;&#21035;&#27495;&#35270;&#35782;&#21035;&#21644;&#34920;&#24449;&#30340;&#30740;&#31350;&#65292;&#20197;&#25512;&#21160;&#26356;&#20855;&#21253;&#23481;&#24615;&#21644;&#23562;&#37325;&#24615;&#30340;&#22312;&#32447;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#30340;&#19981;&#26029;&#24433;&#21709;&#21147;&#22686;&#21152;&#65292;&#24320;&#21457;&#33021;&#22815;&#26816;&#27979;&#24615;&#21035;&#27495;&#35270;&#21644;&#20854;&#20182;&#19981;&#23562;&#37325;&#21644;&#20167;&#24680;&#34892;&#20026;&#30340;&#33258;&#21160;&#21270;&#31995;&#32479;&#65292;&#20197;&#20419;&#36827;&#26356;&#20855;&#21253;&#23481;&#24615;&#21644;&#23562;&#37325;&#24615;&#30340;&#22312;&#32447;&#29615;&#22659;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#32771;&#34385;&#21040;&#19981;&#21516;&#30340;&#20167;&#24680;&#31867;&#21035;&#21644;&#20316;&#32773;&#30340;&#24847;&#22270;&#65292;&#23588;&#20854;&#26159;&#22312;&#23398;&#20064;&#19982;&#20998;&#27495;&#30340;&#26426;&#21046;&#19979;&#65292;&#36825;&#20123;&#20219;&#21153;&#30456;&#24403;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;AI-UPV&#22242;&#38431;&#22312;CLEF 2023&#30340;EXIST&#65288;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#24615;&#21035;&#27495;&#35270;&#35782;&#21035;&#65289;&#23454;&#39564;&#23460;&#20013;&#30340;&#21442;&#19982;&#24773;&#20917;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#30452;&#25509;&#20174;&#20855;&#26377;&#20998;&#27495;&#30340;&#25968;&#25454;&#20013;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#19981;&#20351;&#29992;&#20219;&#20309;&#32858;&#21512;&#26631;&#31614;&#65292;&#26469;&#22788;&#29702;&#24615;&#21035;&#27495;&#35270;&#35782;&#21035;&#21644;&#34920;&#24449;&#30340;&#20219;&#21153;&#12290;&#21516;&#26102;&#65292;&#25253;&#21578;&#20102;&#32771;&#34385;&#36719;&#24615;&#21644;&#30828;&#24615;&#35780;&#20272;&#30340;&#24615;&#33021;&#12290;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;mBERT&#21644;XLM-RoBERTa&#65289;&#21644;&#38598;&#25104;&#31574;&#30053;&#26469;&#36827;&#34892;&#24615;&#21035;&#27495;&#35270;&#35782;&#21035;&#21644;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing influence of social media platforms, it has become crucial to develop automated systems capable of detecting instances of sexism and other disrespectful and hateful behaviors to promote a more inclusive and respectful online environment. Nevertheless, these tasks are considerably challenging considering different hate categories and the author's intentions, especially under the learning with disagreements regime. This paper describes AI-UPV team's participation in the EXIST (sEXism Identification in Social neTworks) Lab at CLEF 2023. The proposed approach aims at addressing the task of sexism identification and characterization under the learning with disagreements paradigm by training directly from the data with disagreements, without using any aggregated label. Yet, performances considering both soft and hard evaluations are reported. The proposed system uses large language models (i.e., mBERT and XLM-RoBERTa) and ensemble strategies for sexism identification and 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#35757;&#32451;&#23567;&#22411;Transformer&#27169;&#22411;&#22312;&#27809;&#26377;&#20808;&#39564;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#23398;&#20064;&#22522;&#26412;&#31639;&#26415;&#36816;&#31639;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26684;&#24335;&#21464;&#21270;&#21644;&#20351;&#29992;&#38142;&#24335;&#24605;&#32500;&#26679;&#24335;&#25968;&#25454;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.03381</link><description>&lt;p&gt;
&#21521;&#23567;&#22411;Transformer&#27169;&#22411;&#25945;&#25480;&#31639;&#26415;
&lt;/p&gt;
&lt;p&gt;
Teaching Arithmetic to Small Transformers. (arXiv:2307.03381v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03381
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#35757;&#32451;&#23567;&#22411;Transformer&#27169;&#22411;&#22312;&#27809;&#26377;&#20808;&#39564;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#23398;&#20064;&#22522;&#26412;&#31639;&#26415;&#36816;&#31639;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26684;&#24335;&#21464;&#21270;&#21644;&#20351;&#29992;&#38142;&#24335;&#24605;&#32500;&#26679;&#24335;&#25968;&#25454;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;GPT-4&#65292;&#24403;&#22312;&#22823;&#37327;&#25991;&#26412;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#21363;&#20351;&#36825;&#20123;&#20219;&#21153;&#24182;&#26410;&#30452;&#25509;&#32534;&#30721;&#22312;&#26080;&#30417;&#30563;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#30446;&#26631;&#20013;&#65292;&#20063;&#23637;&#29616;&#20986;&#20102;&#22312;&#36890;&#29992;&#20219;&#21153;&#65288;&#22914;&#22522;&#26412;&#31639;&#26415;&#65289;&#19978;&#30340;&#26032;&#20852;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22914;&#20309;&#35753;&#20174;&#38543;&#26426;&#21021;&#22987;&#21270;&#35757;&#32451;&#30340;&#23567;&#22411;transformers&#27169;&#22411;&#65292;&#36890;&#36807;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#30446;&#26631;&#39640;&#25928;&#23398;&#20064;&#21152;&#27861;&#12289;&#20056;&#27861;&#21644;&#35832;&#22914;&#24179;&#26041;&#26681;&#31561;&#22522;&#26412;&#31639;&#26415;&#36816;&#31639;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20256;&#32479;&#35757;&#32451;&#25968;&#25454;&#23545;&#20110;&#31639;&#26415;&#23398;&#20064;&#26469;&#35828;&#24182;&#19981;&#26159;&#26368;&#26377;&#25928;&#30340;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#26684;&#24335;&#21464;&#21270;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#36825;&#23548;&#33268;&#20102;&#35757;&#32451;&#25968;&#25454;&#35268;&#27169;&#30340;&#23574;&#38160;&#30456;&#21464;&#65292;&#20854;&#20013;&#19968;&#20123;&#24773;&#20917;&#21487;&#20197;&#36890;&#36807;&#19982;&#20302;&#31209;&#30697;&#38453;&#34917;&#20840;&#30340;&#32852;&#31995;&#26469;&#35299;&#37322;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#22312;&#21253;&#25324;&#20013;&#38388;&#27493;&#39588;&#32467;&#26524;&#30340;&#38142;&#24335;&#24605;&#32500;&#26679;&#24335;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#21363;&#20351;&#23436;&#20840;&#26080;&#20808;&#39564;&#35757;&#32451;&#65292;&#27169;&#22411;&#20173;&#28982;&#21487;&#20197;&#23398;&#20064;&#31639;&#26415;&#36816;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models like GPT-4 exhibit emergent capabilities across general-purpose tasks, such as basic arithmetic, when trained on extensive text data, even though these tasks are not explicitly encoded by the unsupervised, next-token prediction objective. This study investigates how small transformers, trained from random initialization, can efficiently learn arithmetic operations such as addition, multiplication, and elementary functions like square root, using the next-token prediction objective. We first demonstrate that conventional training data is not the most effective for arithmetic learning, and simple formatting changes can significantly improve accuracy. This leads to sharp phase transitions as a function of training data scale, which, in some cases, can be explained through connections to low-rank matrix completion. Building on prior work, we then train on chain-of-thought style data that includes intermediate step results. Even in the complete absence of pretraining, 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#20013;&#30340;&#24418;&#24335;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#21450;&#20854;&#36817;&#20284;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;&#29305;&#24449;&#36873;&#25321;&#21644;&#24402;&#22240;&#26041;&#27861;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#65292;&#32780;&#24418;&#24335;&#21270;&#30340;XAI&#26041;&#27861;&#34429;&#28982;&#26159;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#20173;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2307.03380</link><description>&lt;p&gt;
&#20851;&#20110;&#24418;&#24335;&#29305;&#24449;&#24402;&#22240;&#21450;&#20854;&#36817;&#20284;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
On Formal Feature Attribution and Its Approximation. (arXiv:2307.03380v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03380
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#20013;&#30340;&#24418;&#24335;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#21450;&#20854;&#36817;&#20284;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;&#29305;&#24449;&#36873;&#25321;&#21644;&#24402;&#22240;&#26041;&#27861;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#65292;&#32780;&#24418;&#24335;&#21270;&#30340;XAI&#26041;&#27861;&#34429;&#28982;&#26159;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#20173;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#31639;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;ML&#27169;&#22411;&#33030;&#24369;&#24615;&#65292;&#20844;&#24179;&#24615;&#20197;&#21450;&#35299;&#37322;&#24615;&#30340;&#32570;&#20047;&#31561;&#37325;&#35201;&#38382;&#39064;&#38656;&#35201;&#31215;&#26497;&#21457;&#23637;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#21644;&#24418;&#24335;&#21270;&#30340;ML&#27169;&#22411;&#39564;&#35777;&#12290;XAI&#30340;&#20004;&#20010;&#20027;&#35201;&#30740;&#31350;&#26041;&#21521;&#21253;&#25324;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65288;&#20363;&#22914;&#65292;Anchors&#65289;&#21644;&#29305;&#24449;&#24402;&#22240;&#25216;&#26415;&#65288;&#20363;&#22914;&#65292;LIME&#21644;SHAP&#65289;&#12290;&#23613;&#31649;&#26377;&#24076;&#26395;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#29305;&#24449;&#36873;&#25321;&#21644;&#24402;&#22240;&#26041;&#27861;&#37117;&#23481;&#26131;&#20986;&#29616;&#19968;&#31995;&#21015;&#20851;&#38190;&#38382;&#39064;&#65292;&#21253;&#25324;&#35299;&#37322;&#19981;&#27491;&#30830;&#21644;&#36229;&#20986;&#20998;&#24067;&#37319;&#26679;&#12290;&#36817;&#26399;&#19968;&#31181;&#24418;&#24335;&#21270;&#30340;XAI&#26041;&#27861;&#65288;FXAI&#65289;&#34429;&#28982;&#20316;&#20026;&#20197;&#19978;&#26041;&#27861;&#30340;&#26367;&#20195;&#21697;&#24182;&#36991;&#20813;&#20102;&#36825;&#20123;&#38382;&#39064;&#65292;&#20294;&#20173;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#12290;&#20363;&#22914;&#65292;&#38500;&#20102;&#21487;&#25193;&#23637;&#24615;&#38480;&#21046;&#22806;&#65292;&#36825;&#31181;&#24418;&#24335;&#21270;&#26041;&#27861;&#26080;&#27861;&#35299;&#20915;&#29305;&#24449;&#24402;&#22240;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed the widespread use of artificial intelligence (AI) algorithms and machine learning (ML) models. Despite their tremendous success, a number of vital problems like ML model brittleness, their fairness, and the lack of interpretability warrant the need for the active developments in explainable artificial intelligence (XAI) and formal ML model verification. The two major lines of work in XAI include feature selection methods, e.g. Anchors, and feature attribution techniques, e.g. LIME and SHAP. Despite their promise, most of the existing feature selection and attribution approaches are susceptible to a range of critical issues, including explanation unsoundness and out-of-distribution sampling. A recent formal approach to XAI (FXAI) although serving as an alternative to the above and free of these issues suffers from a few other limitations. For instance and besides the scalability limitation, the formal approach is unable to tackle the feature attribution prob
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20219;&#21153;&#24863;&#30693;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#24615;&#21035;&#27495;&#35270;&#12289;&#20167;&#24680;&#35328;&#35770;&#21644;&#26377;&#23475;&#35821;&#35328;&#26816;&#27979;&#20013;&#30340;&#36127;&#38754;&#36801;&#31227;&#38382;&#39064;&#65292;&#24182;&#33021;&#22815;&#20943;&#23569;&#36127;&#38754;&#36801;&#31227;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.03377</link><description>&lt;p&gt;
&#32531;&#35299;&#20219;&#21153;&#24863;&#30693;&#23545;&#24615;&#21035;&#27495;&#35270;&#12289;&#20167;&#24680;&#35328;&#35770;&#21644;&#26377;&#23475;&#35821;&#35328;&#26816;&#27979;&#30340;&#36127;&#38754;&#36801;&#31227;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Mitigating Negative Transfer with Task Awareness for Sexism, Hate Speech, and Toxic Language Detection. (arXiv:2307.03377v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03377
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20219;&#21153;&#24863;&#30693;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#24615;&#21035;&#27495;&#35270;&#12289;&#20167;&#24680;&#35328;&#35770;&#21644;&#26377;&#23475;&#35821;&#35328;&#26816;&#27979;&#20013;&#30340;&#36127;&#38754;&#36801;&#31227;&#38382;&#39064;&#65292;&#24182;&#33021;&#22815;&#20943;&#23569;&#36127;&#38754;&#36801;&#31227;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#32531;&#35299;&#36127;&#38754;&#36801;&#31227;&#38382;&#39064;&#12290;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#36890;&#24120;&#30340;&#31574;&#30053;&#26159;&#37319;&#29992;&#21333;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#65292;&#35757;&#32451;&#19968;&#20010;&#30417;&#30563;&#27169;&#22411;&#26469;&#35299;&#20915;&#29305;&#23450;&#30340;&#20219;&#21153;&#12290;&#35757;&#32451;&#19968;&#20010;&#24378;&#22823;&#30340;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#21644;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#36825;&#20351;&#24471;&#22312;&#25968;&#25454;&#19981;&#21487;&#29992;&#25110;&#25910;&#38598;&#25104;&#26412;&#39640;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#35299;&#20915;&#26041;&#26696;&#19981;&#21487;&#34892;&#12290;&#22240;&#27492;&#65292;&#21478;&#19968;&#31181;&#22522;&#20110;&#20219;&#21153;&#20043;&#38388;&#20449;&#24687;&#20849;&#20139;&#30340;&#35299;&#20915;&#26041;&#26696;&#24050;&#32463;&#34987;&#24320;&#21457;&#20986;&#26469;&#65306;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#12290;&#23613;&#31649;&#22312;MTL&#26041;&#38754;&#24050;&#32463;&#26377;&#20102;&#19968;&#20123;&#26368;&#26032;&#30340;&#36827;&#23637;&#65292;&#36127;&#38754;&#36801;&#31227;&#38382;&#39064;&#20173;&#28982;&#38656;&#35201;&#35299;&#20915;&#12290;&#36127;&#38754;&#36801;&#31227;&#26159;&#19968;&#31181;&#29616;&#35937;&#65292;&#24403;&#22122;&#22768;&#20449;&#24687;&#22312;&#20219;&#21153;&#20043;&#38388;&#20849;&#20139;&#26102;&#65292;&#20250;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20219;&#21153;&#24863;&#30693;&#27010;&#24565;&#30340;&#26032;&#26041;&#27861;&#26469;&#32531;&#35299;&#36127;&#38754;&#36801;&#31227;&#38382;&#39064;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#20943;&#23569;&#36127;&#38754;&#36801;&#31227;&#65292;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novelty approach to mitigate the negative transfer problem. In the field of machine learning, the common strategy is to apply the Single-Task Learning approach in order to train a supervised model to solve a specific task. Training a robust model requires a lot of data and a significant amount of computational resources, making this solution unfeasible in cases where data are unavailable or expensive to gather. Therefore another solution, based on the sharing of information between tasks, has been developed: Multi-Task Learning (MTL). Despite the recent developments regarding MTL, the problem of negative transfer has still to be solved. Negative transfer is a phenomenon that occurs when noisy information is shared between tasks, resulting in a drop in performance. This paper proposes a new approach to mitigate the negative transfer problem based on the task awareness concept. The proposed approach results in diminishing the negative transfer together with an impro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#20351;&#29992;&#25968;&#25454;&#26144;&#23556;&#26469;&#35299;&#20915;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#20219;&#21153;&#20998;&#32452;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#21487;&#20280;&#32553;&#24615;&#21644;&#27169;&#22359;&#21270;&#65292;&#24182;&#19988;&#22312;&#22823;&#37327;&#20219;&#21153;&#19979;&#20173;&#28982;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2307.03374</link><description>&lt;p&gt;
STG-MTL: &#20351;&#29992;&#25968;&#25454;&#26144;&#23556;&#30340;&#21487;&#20280;&#32553;&#20219;&#21153;&#20998;&#32452;&#22810;&#20219;&#21153;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
STG-MTL: Scalable Task Grouping for Multi-Task Learning Using Data Map. (arXiv:2307.03374v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03374
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#20351;&#29992;&#25968;&#25454;&#26144;&#23556;&#26469;&#35299;&#20915;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#20219;&#21153;&#20998;&#32452;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#21487;&#20280;&#32553;&#24615;&#21644;&#27169;&#22359;&#21270;&#65292;&#24182;&#19988;&#22312;&#22823;&#37327;&#20219;&#21153;&#19979;&#20173;&#28982;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#25216;&#26415;&#65292;&#22240;&#20854;&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;&#21333;&#20219;&#21153;&#23398;&#20064;&#32780;&#35328;&#20855;&#26377;&#24615;&#33021;&#25913;&#36827;&#32780;&#21463;&#21040;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#22810;&#20219;&#21153;&#23398;&#20064;&#24448;&#24448;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#21487;&#33021;&#30340;&#20219;&#21153;&#20998;&#32452;&#25968;&#37327;&#21576;&#25351;&#25968;&#22686;&#38271;&#65292;&#36825;&#20351;&#24471;&#36873;&#25321;&#26368;&#20339;&#20219;&#21153;&#20998;&#32452;&#21464;&#24471;&#22256;&#38590;&#65292;&#24182;&#19988;&#19968;&#20123;&#20998;&#32452;&#21487;&#33021;&#20250;&#30001;&#20110;&#20219;&#21153;&#20043;&#38388;&#30340;&#36127;&#38754;&#24178;&#25200;&#32780;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#20005;&#37325;&#21463;&#21040;&#21487;&#20280;&#32553;&#24615;&#38382;&#39064;&#30340;&#22256;&#25200;&#65292;&#38480;&#21046;&#20102;&#23454;&#38469;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#25163;&#24037;&#29305;&#24449;&#65292;&#29305;&#21035;&#26159;&#25968;&#25454;&#26144;&#23556;&#30340;&#20998;&#31867;&#20219;&#21153;&#20998;&#32452;&#30340;&#21487;&#20280;&#32553;&#21644;&#27169;&#22359;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#21363;&#20351;&#22312;&#21069;&#25152;&#26410;&#26377;&#30340;&#20219;&#21153;&#25968;&#37327;&#19979;&#65288;&#39640;&#36798;100&#20010;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-Task Learning (MTL) is a powerful technique that has gained popularity due to its performance improvement over traditional Single-Task Learning (STL). However, MTL is often challenging because there is an exponential number of possible task groupings, which can make it difficult to choose the best one, and some groupings might produce performance degradation due to negative interference between tasks. Furthermore, existing solutions are severely suffering from scalability issues, limiting any practical application. In our paper, we propose a new data-driven method that addresses these challenges and provides a scalable and modular solution for classification task grouping based on hand-crafted features, specifically Data Maps, which capture the training behavior for each classification task during the MTL training. We experiment with the method demonstrating its effectiveness, even on an unprecedented number of tasks (up to 100).
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#33976;&#39311;&#25968;&#25454;&#26469;&#20462;&#21098;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#24555;&#22320;&#25214;&#21040;&#31232;&#30095;&#30340;&#21487;&#35757;&#32451;&#23376;&#32593;&#32476;&#65292;&#20855;&#26377;&#36164;&#28304;&#39640;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;&#20462;&#21098;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.03364</link><description>&lt;p&gt;
&#33976;&#39311;&#20462;&#21098;&#65306;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36194;&#24471;&#24425;&#31080;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Distilled Pruning: Using Synthetic Data to Win the Lottery. (arXiv:2307.03364v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03364
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#33976;&#39311;&#25968;&#25454;&#26469;&#20462;&#21098;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#24555;&#22320;&#25214;&#21040;&#31232;&#30095;&#30340;&#21487;&#35757;&#32451;&#23376;&#32593;&#32476;&#65292;&#20855;&#26377;&#36164;&#28304;&#39640;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;&#20462;&#21098;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#33976;&#39311;&#25968;&#25454;&#26469;&#20462;&#21098;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#12290;&#19982;&#20256;&#32479;&#31574;&#30053;&#20027;&#35201;&#20851;&#27880;&#20307;&#31995;&#32467;&#26500;&#25110;&#31639;&#27861;&#20248;&#21270;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#37325;&#26032;&#32771;&#34385;&#20102;&#25968;&#25454;&#22312;&#36825;&#20123;&#22330;&#26223;&#20013;&#30340;&#20316;&#29992;&#12290;&#33976;&#39311;&#25968;&#25454;&#38598;&#25429;&#25417;&#20102;&#26356;&#22823;&#25968;&#25454;&#38598;&#20013;&#30340;&#37325;&#35201;&#27169;&#24335;&#65292;&#24182;&#19988;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#36825;&#31181;&#33021;&#21147;&#26469;&#23454;&#29616;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#20462;&#21098;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;CIFAR-10&#19978;&#27604;&#36845;&#20195;&#24133;&#20540;&#20462;&#21098;&#26356;&#24555;&#22320;&#25214;&#21040;&#31232;&#30095;&#30340;&#21487;&#35757;&#32451;&#23376;&#32593;&#32476;&#65288;&#20063;&#31216;&#20026;&#24425;&#31080;&#31080;&#65289;&#12290;&#23454;&#39564;&#32467;&#26524;&#31361;&#26174;&#20102;&#20351;&#29992;&#33976;&#39311;&#25968;&#25454;&#36827;&#34892;&#36164;&#28304;&#39640;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;&#20462;&#21098;&#12289;&#27169;&#22411;&#21387;&#32553;&#21644;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work introduces a novel approach to pruning deep learning models by using distilled data. Unlike conventional strategies which primarily focus on architectural or algorithmic optimization, our method reconsiders the role of data in these scenarios. Distilled datasets capture essential patterns from larger datasets, and we demonstrate how to leverage this capability to enable a computationally efficient pruning process. Our approach can find sparse, trainable subnetworks (a.k.a. Lottery Tickets) up to 5x faster than Iterative Magnitude Pruning at comparable sparsity on CIFAR-10. The experimental results highlight the potential of using distilled data for resource-efficient neural network pruning, model compression, and neural architecture search.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22686;&#37327;&#23398;&#20064;&#30340;&#26032;&#22411;&#32852;&#37030;&#36951;&#24536;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#32852;&#37030;&#36951;&#24536;&#26041;&#27861;&#22312;&#26102;&#38388;&#25928;&#29575;&#12289;&#25968;&#25454;&#24433;&#21709;&#20272;&#35745;&#19981;&#31934;&#30830;&#21644;&#35745;&#31639;&#36127;&#33655;&#22823;&#31561;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.03363</link><description>&lt;p&gt;
&#36890;&#36807;&#20027;&#21160;&#36951;&#24536;&#23454;&#29616;&#32852;&#37030;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Federated Unlearning via Active Forgetting. (arXiv:2307.03363v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03363
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22686;&#37327;&#23398;&#20064;&#30340;&#26032;&#22411;&#32852;&#37030;&#36951;&#24536;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#32852;&#37030;&#36951;&#24536;&#26041;&#27861;&#22312;&#26102;&#38388;&#25928;&#29575;&#12289;&#25968;&#25454;&#24433;&#21709;&#20272;&#35745;&#19981;&#31934;&#30830;&#21644;&#35745;&#31639;&#36127;&#33655;&#22823;&#31561;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38544;&#31169;&#30340;&#20851;&#27880;&#26085;&#30410;&#22686;&#21152;&#65292;&#24341;&#21457;&#20102;&#23545;&#26426;&#22120;&#36951;&#24536;&#30340;&#25506;&#32034;&#65292;&#21363;&#19968;&#31181;&#28040;&#38500;&#35757;&#32451;&#25968;&#25454;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24433;&#21709;&#30340;&#36807;&#31243;&#12290;&#36825;&#31181;&#20851;&#27880;&#20063;&#20986;&#29616;&#22312;&#32852;&#37030;&#23398;&#20064;&#30340;&#39046;&#22495;&#65292;&#20419;&#20351;&#30740;&#31350;&#20154;&#21592;&#35299;&#20915;&#32852;&#37030;&#36951;&#24536;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#32852;&#37030;&#36951;&#24536;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#30340;&#36951;&#24536;&#26041;&#27861;&#21487;&#20197;&#34987;&#24191;&#27867;&#20998;&#20026;&#20004;&#31181;&#26041;&#27861;&#65292;&#21363;&#31934;&#30830;&#36951;&#24536;&#21644;&#36817;&#20284;&#36951;&#24536;&#12290;&#39318;&#20808;&#65292;&#22312;&#20998;&#24067;&#24335;&#24773;&#20917;&#19979;&#23454;&#26045;&#31934;&#30830;&#36951;&#24536;&#65292;&#36890;&#24120;&#20381;&#36182;&#20110;&#20998;&#21306;-&#32858;&#21512;&#26694;&#26550;&#65292;&#29702;&#35770;&#19978;&#19981;&#20250;&#25552;&#39640;&#26102;&#38388;&#25928;&#29575;&#12290;&#20854;&#27425;&#65292;&#29616;&#26377;&#30340;&#32852;&#37030;&#65288;&#36817;&#20284;&#65289;&#36951;&#24536;&#26041;&#27861;&#22312;&#25968;&#25454;&#24433;&#21709;&#20272;&#35745;&#19981;&#31934;&#30830;&#12289;&#35745;&#31639;&#36127;&#33655;&#22823;&#25110;&#20004;&#32773;&#37117;&#23384;&#22312;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22686;&#37327;&#23398;&#20064;&#30340;&#26032;&#22411;&#32852;&#37030;&#36951;&#24536;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#19981;&#20381;&#36182;&#20110;&#20855;&#20307;&#30340;&#27169;&#22411;&#21644;&#32852;&#37030;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing concerns regarding the privacy of machine learning models have catalyzed the exploration of machine unlearning, i.e., a process that removes the influence of training data on machine learning models. This concern also arises in the realm of federated learning, prompting researchers to address the federated unlearning problem. However, federated unlearning remains challenging. Existing unlearning methods can be broadly categorized into two approaches, i.e., exact unlearning and approximate unlearning. Firstly, implementing exact unlearning, which typically relies on the partition-aggregation framework, in a distributed manner does not improve time efficiency theoretically. Secondly, existing federated (approximate) unlearning methods suffer from imprecise data influence estimation, significant computational burden, or both. To this end, we propose a novel federated unlearning framework based on incremental learning, which is independent of specific models and federated se
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20197;&#24050;&#24314;&#31435;&#30340;&#25991;&#29486;&#20026;&#22522;&#30784;&#65292;&#37327;&#21270;&#20102;&#33521;&#35821;&#35821;&#35328;&#27169;&#22411;&#20013;&#31038;&#20250;&#32676;&#20307;&#30340;&#24773;&#32490;&#20851;&#32852;&#65292;&#24182;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#23545;&#24615;&#21035;&#35748;&#21516;&#12289;&#31038;&#20250;&#38454;&#32423;&#21644;&#24615;&#21462;&#21521;&#30340;&#20449;&#21495;&#34920;&#29616;&#20986;&#26368;&#22823;&#30340;&#20559;&#35265;&#24577;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.03360</link><description>&lt;p&gt;
&#22312;&#20132;&#21449;&#38382;&#31572;&#32972;&#26223;&#19979;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#24577;&#24230;&#20851;&#32852;
&lt;/p&gt;
&lt;p&gt;
Evaluating Biased Attitude Associations of Language Models in an Intersectional Context. (arXiv:2307.03360v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03360
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20197;&#24050;&#24314;&#31435;&#30340;&#25991;&#29486;&#20026;&#22522;&#30784;&#65292;&#37327;&#21270;&#20102;&#33521;&#35821;&#35821;&#35328;&#27169;&#22411;&#20013;&#31038;&#20250;&#32676;&#20307;&#30340;&#24773;&#32490;&#20851;&#32852;&#65292;&#24182;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#23545;&#24615;&#21035;&#35748;&#21516;&#12289;&#31038;&#20250;&#38454;&#32423;&#21644;&#24615;&#21462;&#21521;&#30340;&#20449;&#21495;&#34920;&#29616;&#20986;&#26368;&#22823;&#30340;&#20559;&#35265;&#24577;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26159;&#22312;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#30340;&#65292;&#36825;&#20123;&#35821;&#26009;&#24211;&#20013;&#23884;&#20837;&#20102;&#24515;&#29702;&#23398;&#20013;&#24050;&#32463;&#35760;&#24405;&#30340;&#38544;&#21547;&#20559;&#35265;&#12290;&#31038;&#20250;&#32676;&#20307;&#30340;&#24773;&#32490;&#20851;&#32852;&#65288;&#24841;&#24555;/&#19981;&#24841;&#24555;&#65289;&#20915;&#23450;&#20102;&#31038;&#20250;&#35748;&#30693;&#20013;&#23545;&#32676;&#20307;&#21644;&#27010;&#24565;&#30340;&#20559;&#35265;&#24577;&#24230;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#20132;&#21449;&#38382;&#31572;&#32972;&#26223;&#30340;&#21477;&#23376;&#27169;&#26495;&#65292;&#37327;&#21270;&#20102;&#33521;&#35821;&#35821;&#35328;&#27169;&#22411;&#20013;&#31038;&#20250;&#32676;&#20307;&#30340;&#24773;&#32490;&#20851;&#32852;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19982;&#24180;&#40836;&#12289;&#25945;&#32946;&#12289;&#24615;&#21035;&#12289;&#36523;&#39640;&#12289;&#26234;&#21147;&#12289;&#25991;&#21270;&#32032;&#20859;&#12289;&#31181;&#26063;&#12289;&#23447;&#25945;&#12289;&#24615;&#21035;&#12289;&#24615;&#21462;&#21521;&#12289;&#31038;&#20250;&#38454;&#32423;&#21644;&#20307;&#37325;&#26377;&#20851;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#37319;&#29992;&#27010;&#24565;&#25237;&#24433;&#26041;&#27861;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#21270;&#35789;&#21521;&#37327;&#25429;&#25417;&#24773;&#32490;&#20851;&#32852;&#30340;&#23376;&#31354;&#38388;&#12290;&#23558;&#22522;&#20110;&#25237;&#24433;&#30340;&#26041;&#27861;&#35843;&#25972;&#20026;&#37327;&#21270;&#20559;&#35265;&#30340;&#23884;&#20837;&#20851;&#32852;&#27979;&#35797;&#65292;&#25105;&#20204;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#23545;&#24615;&#21035;&#35748;&#21516;&#12289;&#31038;&#20250;&#38454;&#32423;&#21644;&#24615;&#21462;&#21521;&#30340;&#20449;&#21495;&#34920;&#29616;&#20986;&#26368;&#22823;&#30340;&#20559;&#35265;&#24577;&#24230;&#12290;&#25105;&#20204;&#21457;&#29616;&#26368;&#22823;&#21644;&#34920;&#29616;&#26368;&#22909;&#30340;&#27169;&#22411;&#26159;...
&lt;/p&gt;
&lt;p&gt;
Language models are trained on large-scale corpora that embed implicit biases documented in psychology. Valence associations (pleasantness/unpleasantness) of social groups determine the biased attitudes towards groups and concepts in social cognition. Building on this established literature, we quantify how social groups are valenced in English language models using a sentence template that provides an intersectional context. We study biases related to age, education, gender, height, intelligence, literacy, race, religion, sex, sexual orientation, social class, and weight. We present a concept projection approach to capture the valence subspace through contextualized word embeddings of language models. Adapting the projection-based approach to embedding association tests that quantify bias, we find that language models exhibit the most biased attitudes against gender identity, social class, and sexual orientation signals in language. We find that the largest and better-performing model
&lt;/p&gt;</description></item><item><title>CSCLog&#26159;&#19968;&#31181;&#32771;&#34385;&#32452;&#20214;&#23376;&#24207;&#21015;&#30456;&#20851;&#24615;&#30340;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#33719;&#23376;&#24207;&#21015;&#20013;&#30340;&#39034;&#24207;&#20381;&#36182;&#20851;&#31995;&#21644;&#24314;&#27169;&#23376;&#24207;&#21015;&#30340;&#38544;&#24335;&#30456;&#20851;&#24615;&#26469;&#26816;&#27979;&#26085;&#24535;&#20013;&#30340;&#24322;&#24120;&#12290;</title><link>http://arxiv.org/abs/2307.03359</link><description>&lt;p&gt;
CSCLog: &#19968;&#31181;&#32771;&#34385;&#32452;&#20214;&#23376;&#24207;&#21015;&#30456;&#20851;&#24615;&#30340;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CSCLog: A Component Subsequence Correlation-Aware Log Anomaly Detection Method. (arXiv:2307.03359v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03359
&lt;/p&gt;
&lt;p&gt;
CSCLog&#26159;&#19968;&#31181;&#32771;&#34385;&#32452;&#20214;&#23376;&#24207;&#21015;&#30456;&#20851;&#24615;&#30340;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#33719;&#23376;&#24207;&#21015;&#20013;&#30340;&#39034;&#24207;&#20381;&#36182;&#20851;&#31995;&#21644;&#24314;&#27169;&#23376;&#24207;&#21015;&#30340;&#38544;&#24335;&#30456;&#20851;&#24615;&#26469;&#26816;&#27979;&#26085;&#24535;&#20013;&#30340;&#24322;&#24120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#31995;&#32479;&#26085;&#24535;&#30340;&#24322;&#24120;&#26816;&#27979;&#22312;&#26234;&#33021;&#36816;&#33829;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#30001;&#20110;&#26085;&#24535;&#27169;&#24335;&#30340;&#26497;&#31471;&#22797;&#26434;&#24615;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#36807;&#25429;&#33719;&#26085;&#24535;&#24207;&#21015;&#20013;&#30340;&#39034;&#24207;&#20381;&#36182;&#20851;&#31995;&#26469;&#26816;&#27979;&#24322;&#24120;&#65292;&#20294;&#24573;&#30053;&#20102;&#23376;&#24207;&#21015;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#32452;&#20214;&#23376;&#24207;&#21015;&#30456;&#20851;&#24615;&#30340;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65306;CSCLog&#12290;&#35813;&#26041;&#27861;&#19981;&#20165;&#25429;&#33719;&#23376;&#24207;&#21015;&#20013;&#30340;&#39034;&#24207;&#20381;&#36182;&#20851;&#31995;&#65292;&#36824;&#23545;&#23376;&#24207;&#21015;&#30340;&#38544;&#24335;&#30456;&#20851;&#24615;&#36827;&#34892;&#24314;&#27169;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22522;&#20110;&#32452;&#20214;&#20174;&#26085;&#24535;&#24207;&#21015;&#20013;&#25552;&#21462;&#23376;&#24207;&#21015;&#65292;&#24182;&#36890;&#36807;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65288;LSTM&#65289;&#26469;&#25429;&#33719;&#23376;&#24207;&#21015;&#20013;&#30340;&#39034;&#24207;&#20381;&#36182;&#20851;&#31995;&#12290;&#24341;&#20837;&#38544;&#24335;&#30456;&#20851;&#24615;&#32534;&#30721;&#22120;&#26469;&#33258;&#36866;&#24212;&#22320;&#24314;&#27169;&#23376;&#24207;&#21015;&#30340;&#38544;&#24335;&#30456;&#20851;&#24615;&#12290;&#27492;&#22806;&#65292;&#37319;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#26469;&#23454;&#29616;&#23376;&#24207;&#21015;&#30340;&#20449;&#24687;&#20132;&#20114;&#12290;&#26368;&#21518;&#65292;&#21033;&#29992;&#27880;&#24847;&#26426;&#21046;&#26469;&#34701;&#21512;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection based on system logs plays an important role in intelligent operations, which is a challenging task due to the extremely complex log patterns. Existing methods detect anomalies by capturing the sequential dependencies in log sequences, which ignore the interactions of subsequences. To this end, we propose CSCLog, a Component Subsequence Correlation-Aware Log anomaly detection method, which not only captures the sequential dependencies in subsequences, but also models the implicit correlations of subsequences. Specifically, subsequences are extracted from log sequences based on components and the sequential dependencies in subsequences are captured by Long Short-Term Memory Networks (LSTMs). An implicit correlation encoder is introduced to model the implicit correlations of subsequences adaptively. In addition, Graph Convolution Networks (GCNs) are employed to accomplish the information interactions of subsequences. Finally, attention mechanisms are exploited to fuse t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#32479;&#35745;&#23398;&#20064;&#29702;&#35770;&#30340;&#31639;&#27861;&#31283;&#23450;&#24615;&#65292;&#20998;&#26512;&#20102;&#38543;&#26426;&#32452;&#21512;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#31283;&#23450;&#24615;&#21644;&#27867;&#21270;&#24615;&#65292;&#24341;&#20837;&#20102;&#32452;&#21512;&#19968;&#33268;&#31283;&#23450;&#24615;&#27010;&#24565;&#24182;&#19982;SCO&#38382;&#39064;&#30340;&#27867;&#21270;&#24615;&#24314;&#31435;&#20102;&#23450;&#37327;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2307.03357</link><description>&lt;p&gt;
&#38543;&#26426;&#32452;&#21512;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#31283;&#23450;&#24615;&#21644;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Stability and Generalization of Stochastic Compositional Gradient Descent Algorithms. (arXiv:2307.03357v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03357
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#32479;&#35745;&#23398;&#20064;&#29702;&#35770;&#30340;&#31639;&#27861;&#31283;&#23450;&#24615;&#65292;&#20998;&#26512;&#20102;&#38543;&#26426;&#32452;&#21512;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#31283;&#23450;&#24615;&#21644;&#27867;&#21270;&#24615;&#65292;&#24341;&#20837;&#20102;&#32452;&#21512;&#19968;&#33268;&#31283;&#23450;&#24615;&#27010;&#24565;&#24182;&#19982;SCO&#38382;&#39064;&#30340;&#27867;&#21270;&#24615;&#24314;&#31435;&#20102;&#23450;&#37327;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#21487;&#20197;&#34987;&#24418;&#24335;&#21270;&#20026;&#38543;&#26426;&#32452;&#21512;&#20248;&#21270;&#65288;SCO&#65289;&#38382;&#39064;&#65292;&#20363;&#22914;&#24378;&#21270;&#23398;&#20064;&#12289;AUC&#26368;&#22823;&#21270;&#21644;&#20803;&#23398;&#20064;&#65292;&#20854;&#20013;&#30446;&#26631;&#20989;&#25968;&#28041;&#21450;&#19982;&#26399;&#26395;&#30456;&#20851;&#30340;&#23884;&#22871;&#32452;&#21512;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#22823;&#37327;&#30740;&#31350;&#33268;&#21147;&#20110;&#30740;&#31350;SCO&#31639;&#27861;&#30340;&#25910;&#25947;&#34892;&#20026;&#65292;&#20294;&#23545;&#20110;&#23427;&#20204;&#30340;&#27867;&#21270;&#24615;&#33021;&#22914;&#20309;&#65292;&#21363;&#20174;&#35757;&#32451;&#31034;&#20363;&#26500;&#24314;&#30340;&#23398;&#20064;&#31639;&#27861;&#22312;&#26410;&#26469;&#30340;&#27979;&#35797;&#31034;&#20363;&#19978;&#30340;&#34892;&#20026;&#22914;&#20309;&#65292;&#21364;&#24456;&#23569;&#26377;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#32479;&#35745;&#23398;&#20064;&#29702;&#35770;&#26694;&#26550;&#19979;&#30340;&#31639;&#27861;&#31283;&#23450;&#24615;&#65292;&#25552;&#20379;&#20102;&#38543;&#26426;&#32452;&#21512;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#31283;&#23450;&#24615;&#21644;&#27867;&#21270;&#24615;&#20998;&#26512;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31283;&#23450;&#24615;&#27010;&#24565;&#65292;&#31216;&#20026;&#32452;&#21512;&#19968;&#33268;&#31283;&#23450;&#24615;&#65292;&#24182;&#24314;&#31435;&#20102;&#23427;&#19982;SCO&#38382;&#39064;&#30340;&#27867;&#21270;&#24615;&#20043;&#38388;&#30340;&#23450;&#37327;&#20851;&#31995;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20026;&#20004;&#31181;&#27969;&#34892;&#30340;&#38543;&#26426;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#24314;&#31435;&#20102;&#32452;&#21512;&#19968;&#33268;&#31283;&#23450;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many machine learning tasks can be formulated as a stochastic compositional optimization (SCO) problem such as reinforcement learning, AUC maximization, and meta-learning, where the objective function involves a nested composition associated with an expectation. While a significant amount of studies has been devoted to studying the convergence behavior of SCO algorithms, there is little work on understanding their generalization, i.e., how these learning algorithms built from training examples would behave on future test examples. In this paper, we provide the stability and generalization analysis of stochastic compositional gradient descent algorithms through the lens of algorithmic stability in the framework of statistical learning theory. Firstly, we introduce a stability concept called compositional uniform stability and establish its quantitative relation with generalization for SCO problems. Then, we establish the compositional uniform stability results for two popular stochastic
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#36328;&#39046;&#22495;&#27169;&#22411;&#21387;&#32553;&#30340;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#33976;&#39311;&#25945;&#24072;&#27169;&#22411;&#20013;&#30340;&#36890;&#29992;&#29305;&#24449;&#32423;&#30693;&#35782;&#21644;&#21452;&#39046;&#22495;&#20849;&#20139;&#30340;&#32852;&#21512;logit&#32423;&#30693;&#35782;&#65292;&#23454;&#29616;&#20102;&#22312;&#36164;&#28304;&#26377;&#38480;&#29615;&#22659;&#20013;&#37096;&#32626;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#38590;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.03347</link><description>&lt;p&gt;
&#36328;&#39046;&#22495;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#27169;&#22411;&#21387;&#32553;&#20013;&#33976;&#39311;&#20986;&#36890;&#29992;&#21644;&#32852;&#21512;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Distilling Universal and Joint Knowledge for Cross-Domain Model Compression on Time Series Data. (arXiv:2307.03347v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03347
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#36328;&#39046;&#22495;&#27169;&#22411;&#21387;&#32553;&#30340;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#33976;&#39311;&#25945;&#24072;&#27169;&#22411;&#20013;&#30340;&#36890;&#29992;&#29305;&#24449;&#32423;&#30693;&#35782;&#21644;&#21452;&#39046;&#22495;&#20849;&#20139;&#30340;&#32852;&#21512;logit&#32423;&#30693;&#35782;&#65292;&#23454;&#29616;&#20102;&#22312;&#36164;&#28304;&#26377;&#38480;&#29615;&#22659;&#20013;&#37096;&#32626;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#30340;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#20013;&#65292;&#27969;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#24120;&#24120;&#38459;&#30861;&#20102;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#65288;&#20363;&#22914;&#26234;&#33021;&#25163;&#26426;&#19978;&#65289;&#30340;&#37096;&#32626;&#12290;&#32780;&#19988;&#65292;&#30001;&#20110;&#27169;&#22411;&#35757;&#32451;&#65288;&#28304;&#65289;&#21644;&#37096;&#32626;&#65288;&#30446;&#26631;&#65289;&#38454;&#27573;&#20043;&#38388;&#19981;&#21487;&#36991;&#20813;&#30340;&#39046;&#22495;&#28418;&#31227;&#65292;&#22312;&#36328;&#39046;&#22495;&#22330;&#26223;&#19979;&#21387;&#32553;&#36825;&#20123;&#28145;&#24230;&#27169;&#22411;&#21464;&#24471;&#26356;&#21152;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#23613;&#31649;&#24050;&#32463;&#23384;&#22312;&#19968;&#20123;&#29616;&#26377;&#30340;&#24037;&#20316;&#25506;&#32034;&#20102;&#29992;&#20110;&#27169;&#22411;&#21387;&#32553;&#30340;&#36328;&#39046;&#22495;&#30693;&#35782;&#33976;&#39311;&#65292;&#20294;&#23427;&#20204;&#35201;&#20040;&#20559;&#21521;&#20110;&#28304;&#25968;&#25454;&#65292;&#35201;&#20040;&#22312;&#28304;&#25968;&#25454;&#21644;&#30446;&#26631;&#25968;&#25454;&#20043;&#38388;&#20132;&#32455;&#22312;&#19968;&#36215;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#31216;&#20026;&#36890;&#29992;&#21644;&#32852;&#21512;&#30693;&#35782;&#33976;&#39311;&#65288;UNI-KD&#65289;&#29992;&#20110;&#36328;&#39046;&#22495;&#27169;&#22411;&#21387;&#32553;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#23545;&#25239;&#23398;&#20064;&#26041;&#26696;&#23558;&#25945;&#24072;&#27169;&#22411;&#30340;&#36890;&#29992;&#29305;&#24449;&#32423;&#30693;&#35782;&#21644;&#21452;&#39046;&#22495;&#20849;&#20139;&#30340;&#32852;&#21512;logit&#32423;&#30693;&#35782;&#20256;&#36755;&#21040;&#23398;&#29983;&#27169;&#22411;&#20013;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#19968;&#20010;&#29305;&#24449;&#32423;&#21035;&#30340;&#33976;&#39311;&#32593;&#32476;&#34987;&#29992;&#26469;&#22312;&#28304;&#39046;&#22495;&#21644;&#30446;&#26631;&#39046;&#22495;&#20043;&#38388;&#36716;&#31227;&#36890;&#29992;&#29305;&#24449;&#30693;&#35782;&#65292;&#19968;&#20010;&#23545;&#25239;&#23398;&#20064;&#27169;&#22359;&#34987;&#29992;&#26469;&#23558;&#32852;&#21512;logit&#32423;&#30693;&#35782;&#20256;&#36882;&#32473;&#23398;&#29983;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
For many real-world time series tasks, the computational complexity of prevalent deep leaning models often hinders the deployment on resource-limited environments (e.g., smartphones). Moreover, due to the inevitable domain shift between model training (source) and deploying (target) stages, compressing those deep models under cross-domain scenarios becomes more challenging. Although some of existing works have already explored cross-domain knowledge distillation for model compression, they are either biased to source data or heavily tangled between source and target data. To this end, we design a novel end-to-end framework called Universal and joint knowledge distillation (UNI-KD) for cross-domain model compression. In particular, we propose to transfer both the universal feature-level knowledge across source and target domains and the joint logit-level knowledge shared by both domains from the teacher to the student model via an adversarial learning scheme. More specifically, a featur
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21487;&#31359;&#25140;&#29983;&#29289;&#20449;&#21495;&#25968;&#25454;&#30340;&#22810;&#27169;&#24577;&#20010;&#24615;&#21270;&#21387;&#21147;&#39044;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#27599;&#20010;&#21463;&#35797;&#32773;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#36827;&#34892;&#21387;&#21147;&#39044;&#27979;&#20219;&#21153;&#24494;&#35843;&#12290;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#20351;&#29992;&#23569;&#37327;&#26631;&#27880;&#26469;&#20010;&#24615;&#21270;&#39044;&#27979;&#21387;&#21147;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2307.03337</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#23545;&#22810;&#27169;&#24577;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#20010;&#24615;&#21270;&#20877;&#21457;&#24615;&#21387;&#21147;&#20107;&#20214;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Personalized Prediction of Recurrent Stress Events Using Self-Supervised Learning on Multimodal Time-Series Data. (arXiv:2307.03337v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03337
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21487;&#31359;&#25140;&#29983;&#29289;&#20449;&#21495;&#25968;&#25454;&#30340;&#22810;&#27169;&#24577;&#20010;&#24615;&#21270;&#21387;&#21147;&#39044;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#27599;&#20010;&#21463;&#35797;&#32773;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#36827;&#34892;&#21387;&#21147;&#39044;&#27979;&#20219;&#21153;&#24494;&#35843;&#12290;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#20351;&#29992;&#23569;&#37327;&#26631;&#27880;&#26469;&#20010;&#24615;&#21270;&#39044;&#27979;&#21387;&#21147;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24930;&#24615;&#21387;&#21147;&#20250;&#26497;&#22823;&#22320;&#24433;&#21709;&#36523;&#20307;&#21644;&#24515;&#29702;&#20581;&#24247;&#12290;&#21487;&#31359;&#25140;&#25216;&#26415;&#30340;&#20986;&#29616;&#20801;&#35768;&#36319;&#36394;&#29983;&#29702;&#20449;&#21495;&#65292;&#28508;&#22312;&#22320;&#23548;&#33268;&#21019;&#26032;&#30340;&#21387;&#21147;&#39044;&#27979;&#21644;&#24178;&#39044;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#26631;&#31614;&#31232;&#32570;&#21644;&#25968;&#25454;&#24322;&#36136;&#24615;&#31561;&#25361;&#25112;&#20351;&#24471;&#22312;&#23454;&#36341;&#20013;&#24456;&#38590;&#36827;&#34892;&#21387;&#21147;&#39044;&#27979;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#20351;&#29992;&#21487;&#31359;&#25140;&#29983;&#29289;&#20449;&#21495;&#25968;&#25454;&#30340;&#22810;&#27169;&#24577;&#20010;&#24615;&#21270;&#21387;&#21147;&#39044;&#27979;&#31995;&#32479;&#12290;&#25105;&#20204;&#37319;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#22312;&#27599;&#20010;&#21463;&#35797;&#32773;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20351;&#27169;&#22411;&#22312;&#24494;&#35843;&#21387;&#21147;&#39044;&#27979;&#20219;&#21153;&#20043;&#21069;&#23398;&#20064;&#21442;&#19982;&#32773;&#29983;&#29289;&#20449;&#21495;&#30340;&#22522;&#32447;&#21160;&#24577;&#12290;&#25105;&#20204;&#22312;&#21487;&#31359;&#25140;&#21387;&#21147;&#21644;&#24773;&#24863;&#26816;&#27979;&#65288;WESAD&#65289;&#25968;&#25454;&#38598;&#19978;&#23545;&#27169;&#22411;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;SSL&#27169;&#22411;&#22312;&#20351;&#29992;&#23569;&#20110;5%&#30340;&#26631;&#27880;&#24773;&#20917;&#19979;&#20248;&#20110;&#38750;SSL&#27169;&#22411;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#26368;&#23569;&#30340;&#27880;&#37322;&#23454;&#29616;&#23545;&#27599;&#20010;&#29992;&#25143;&#30340;&#20010;&#24615;&#21270;&#21387;&#21147;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chronic stress can significantly affect physical and mental health. The advent of wearable technology allows for the tracking of physiological signals, potentially leading to innovative stress prediction and intervention methods. However, challenges such as label scarcity and data heterogeneity render stress prediction difficult in practice. To counter these issues, we have developed a multimodal personalized stress prediction system using wearable biosignal data. We employ self-supervised learning (SSL) to pre-train the models on each subject's data, allowing the models to learn the baseline dynamics of the participant's biosignals prior to fine-tuning the stress prediction task. We test our model on the Wearable Stress and Affect Detection (WESAD) dataset, demonstrating that our SSL models outperform non-SSL models while utilizing less than 5% of the annotations. These results suggest that our approach can personalize stress prediction to each user with minimal annotations. This para
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20855;&#26377;&#32534;&#30721;&#25968;&#25454;&#32467;&#26500;&#30340;&#21464;&#20998;&#37327;&#23376;&#22238;&#24402;&#31639;&#27861;&#65292;&#22312;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#20855;&#26377;&#27169;&#22411;&#35299;&#37322;&#24615;&#65292;&#24182;&#33021;&#26377;&#25928;&#22320;&#22788;&#29702;&#20114;&#36830;&#24230;&#36739;&#39640;&#30340;&#37327;&#23376;&#27604;&#29305;&#12290;&#31639;&#27861;&#36890;&#36807;&#21387;&#32553;&#32534;&#30721;&#21644;&#25968;&#23383;-&#27169;&#25311;&#38376;&#25805;&#20316;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#22312;&#22122;&#22768;&#20013;&#23610;&#24230;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#30340;&#36816;&#34892;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.03334</link><description>&lt;p&gt;
&#20855;&#26377;&#32534;&#30721;&#25968;&#25454;&#32467;&#26500;&#30340;&#21464;&#20998;&#37327;&#23376;&#22238;&#24402;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Variational quantum regression algorithm with encoded data structure. (arXiv:2307.03334v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20855;&#26377;&#32534;&#30721;&#25968;&#25454;&#32467;&#26500;&#30340;&#21464;&#20998;&#37327;&#23376;&#22238;&#24402;&#31639;&#27861;&#65292;&#22312;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#20855;&#26377;&#27169;&#22411;&#35299;&#37322;&#24615;&#65292;&#24182;&#33021;&#26377;&#25928;&#22320;&#22788;&#29702;&#20114;&#36830;&#24230;&#36739;&#39640;&#30340;&#37327;&#23376;&#27604;&#29305;&#12290;&#31639;&#27861;&#36890;&#36807;&#21387;&#32553;&#32534;&#30721;&#21644;&#25968;&#23383;-&#27169;&#25311;&#38376;&#25805;&#20316;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#22312;&#22122;&#22768;&#20013;&#23610;&#24230;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#30340;&#36816;&#34892;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;(VQAs)&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#35299;&#20915;&#23454;&#38469;&#38382;&#39064;&#65292;&#22914;&#32452;&#21512;&#20248;&#21270;&#12289;&#37327;&#23376;&#21270;&#23398;&#27169;&#25311;&#12289;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#21644;&#22122;&#22768;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#30340;&#37327;&#23376;&#38169;&#35823;&#32416;&#27491;&#12290;&#23545;&#20110;&#21464;&#20998;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65292;&#23578;&#26410;&#24320;&#21457;&#20986;&#23558;&#27169;&#22411;&#35299;&#37322;&#24615;&#20869;&#23884;&#21040;&#31639;&#27861;&#20013;&#30340;&#21464;&#20998;&#31639;&#27861;&#12290;&#26412;&#25991;&#26500;&#24314;&#20102;&#19968;&#20010;&#37327;&#23376;&#22238;&#24402;&#31639;&#27861;&#65292;&#24182;&#30830;&#23450;&#20102;&#21464;&#20998;&#21442;&#25968;&#19982;&#23398;&#20064;&#22238;&#24402;&#31995;&#25968;&#20043;&#38388;&#30340;&#30452;&#25509;&#20851;&#31995;&#65292;&#21516;&#26102;&#37319;&#29992;&#20102;&#23558;&#25968;&#25454;&#30452;&#25509;&#32534;&#30721;&#20026;&#21453;&#26144;&#32463;&#20856;&#25968;&#25454;&#34920;&#32467;&#26500;&#30340;&#37327;&#23376;&#24133;&#24230;&#30340;&#30005;&#36335;&#12290;&#35813;&#31639;&#27861;&#29305;&#21035;&#36866;&#29992;&#20110;&#20114;&#36830;&#24230;&#36739;&#39640;&#30340;&#37327;&#23376;&#27604;&#29305;&#12290;&#36890;&#36807;&#21387;&#32553;&#32534;&#30721;&#21644;&#25968;&#23383;-&#27169;&#25311;&#38376;&#25805;&#20316;&#65292;&#36816;&#34892;&#26102;&#38388;&#22797;&#26434;&#24230;&#22312;&#25968;&#25454;&#36755;&#20837;&#37327;&#32534;&#30721;&#30340;&#24773;&#20917;&#19979;&#23545;&#25968;&#32423;&#26356;&#26377;&#20248;&#21183;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#22122;&#22768;&#20013;&#23610;&#24230;&#37327;&#23376;&#35745;&#31639;&#26426;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational quantum algorithms (VQAs) prevail to solve practical problems such as combinatorial optimization, quantum chemistry simulation, quantum machine learning, and quantum error correction on noisy quantum computers. For variational quantum machine learning, a variational algorithm with model interpretability built into the algorithm is yet to be exploited. In this paper, we construct a quantum regression algorithm and identify the direct relation of variational parameters to learned regression coefficients, while employing a circuit that directly encodes the data in quantum amplitudes reflecting the structure of the classical data table. The algorithm is particularly suitable for well-connected qubits. With compressed encoding and digital-analog gate operation, the run time complexity is logarithmically more advantageous than that for digital 2-local gate native hardware with the number of data entries encoded, a decent improvement in noisy intermediate-scale quantum computers a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#24341;&#23548;&#30340;&#21327;&#21516;&#20915;&#31574;&#32593;&#32476;&#65288;ACDNet&#65289;&#29992;&#20110;&#33647;&#29289;&#25512;&#33616;&#65292;&#36890;&#36807;&#21033;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;Transformer&#23545;&#24739;&#32773;&#30340;&#20581;&#24247;&#29366;&#20917;&#21644;&#33647;&#29289;&#35760;&#24405;&#36827;&#34892;&#24314;&#27169;&#65292;&#21516;&#26102;&#37319;&#29992;&#21327;&#21516;&#20915;&#31574;&#26694;&#26550;&#65292;&#20174;&#24739;&#32773;&#33647;&#29289;&#35760;&#24405;&#21644;&#20855;&#20307;&#33647;&#29289;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#20986;&#21457;&#65292;&#26377;&#25928;&#22320;&#20010;&#24615;&#21270;&#25512;&#33616;&#33647;&#29289;&#12290;</title><link>http://arxiv.org/abs/2307.03332</link><description>&lt;p&gt;
ACDNet&#65306;&#22522;&#20110;&#27880;&#24847;&#21147;&#24341;&#23548;&#30340;&#21327;&#21516;&#20915;&#31574;&#32593;&#32476;&#29992;&#20110;&#26377;&#25928;&#30340;&#33647;&#29289;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
ACDNet: Attention-guided Collaborative Decision Network for Effective Medication Recommendation. (arXiv:2307.03332v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03332
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#24341;&#23548;&#30340;&#21327;&#21516;&#20915;&#31574;&#32593;&#32476;&#65288;ACDNet&#65289;&#29992;&#20110;&#33647;&#29289;&#25512;&#33616;&#65292;&#36890;&#36807;&#21033;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;Transformer&#23545;&#24739;&#32773;&#30340;&#20581;&#24247;&#29366;&#20917;&#21644;&#33647;&#29289;&#35760;&#24405;&#36827;&#34892;&#24314;&#27169;&#65292;&#21516;&#26102;&#37319;&#29992;&#21327;&#21516;&#20915;&#31574;&#26694;&#26550;&#65292;&#20174;&#24739;&#32773;&#33647;&#29289;&#35760;&#24405;&#21644;&#20855;&#20307;&#33647;&#29289;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#20986;&#21457;&#65292;&#26377;&#25928;&#22320;&#20010;&#24615;&#21270;&#25512;&#33616;&#33647;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#36827;&#34892;&#33647;&#29289;&#25512;&#33616;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#28041;&#21450;&#22797;&#26434;&#30340;&#21307;&#30103;&#25968;&#25454;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#20174;&#24739;&#32773;EHR&#20013;&#25552;&#21462;&#32437;&#21521;&#20449;&#24687;&#20197;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#27169;&#22411;&#24120;&#24120;&#32570;&#20047;&#36275;&#22815;&#30340;&#24739;&#32773;&#34920;&#31034;&#65292;&#24182;&#24573;&#35270;&#20102;&#32771;&#34385;&#24739;&#32773;&#33647;&#29289;&#35760;&#24405;&#21644;&#20855;&#20307;&#33647;&#29289;&#20043;&#38388;&#30456;&#20284;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#24341;&#23548;&#30340;&#21327;&#21516;&#20915;&#31574;&#32593;&#32476;&#65288;ACDNet&#65289;&#29992;&#20110;&#33647;&#29289;&#25512;&#33616;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;ACDNet&#21033;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;Transformer&#22312;&#20840;&#23616;&#21644;&#23616;&#37096;&#23618;&#38754;&#23545;&#24739;&#32773;&#30340;&#20581;&#24247;&#29366;&#20917;&#21644;&#33647;&#29289;&#35760;&#24405;&#36827;&#34892;&#24314;&#27169;&#12290;ACDNet&#36824;&#37319;&#29992;&#21327;&#21516;&#20915;&#31574;&#26694;&#26550;&#65292;&#21033;&#29992;&#33647;&#29289;&#35760;&#24405;&#19982;&#33647;&#29289;&#34920;&#31034;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26469;&#20419;&#36827;&#25512;&#33616;&#36807;&#31243;&#12290;&#22312;&#20004;&#20010;&#22823;&#22411;&#21307;&#30103;&#25968;&#25454;&#38598;MIMIC-III&#21644;MIMIC-IV&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;ACDNet&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medication recommendation using Electronic Health Records (EHR) is challenging due to complex medical data. Current approaches extract longitudinal information from patient EHR to personalize recommendations. However, existing models often lack sufficient patient representation and overlook the importance of considering the similarity between a patient's medication records and specific medicines. Therefore, an Attention-guided Collaborative Decision Network (ACDNet) for medication recommendation is proposed in this paper. Specifically, ACDNet utilizes attention mechanism and Transformer to effectively capture patient health conditions and medication records by modeling their historical visits at both global and local levels. ACDNet also employs a collaborative decision framework, utilizing the similarity between medication records and medicine representation to facilitate the recommendation process. The experimental results on two extensive medical datasets, MIMIC-III and MIMIC-IV, cle
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#23558;&#33258;&#30417;&#30563;&#23398;&#20064;&#24212;&#29992;&#20110;&#25968;&#23383;&#22825;&#32447;&#38453;&#21015;&#25968;&#25454;&#65292;&#36890;&#36807;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32593;&#32476;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#36801;&#31227;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#20248;&#20110;&#38543;&#26426;&#21021;&#22987;&#21270;&#31561;&#25928;&#32593;&#32476;&#30340;&#25968;&#23383;&#38453;&#21015;&#25968;&#25454;&#24102;&#23485;&#22238;&#24402;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.03327</link><description>&lt;p&gt;
&#29992;&#20110;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#19979;&#28216;&#25968;&#23383;&#22825;&#32447;&#38453;&#21015;&#24102;&#23485;&#22238;&#24402;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Encoder-Decoder Networks for Self-Supervised Pretraining and Downstream Signal Bandwidth Regression on Digital Antenna Arrays. (arXiv:2307.03327v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03327
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#23558;&#33258;&#30417;&#30563;&#23398;&#20064;&#24212;&#29992;&#20110;&#25968;&#23383;&#22825;&#32447;&#38453;&#21015;&#25968;&#25454;&#65292;&#36890;&#36807;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32593;&#32476;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#36801;&#31227;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#20248;&#20110;&#38543;&#26426;&#21021;&#22987;&#21270;&#31561;&#25928;&#32593;&#32476;&#30340;&#25968;&#23383;&#38453;&#21015;&#25968;&#25454;&#24102;&#23485;&#22238;&#24402;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#23558;&#33258;&#30417;&#30563;&#23398;&#20064;&#24212;&#29992;&#20110;&#25968;&#23383;&#22825;&#32447;&#38453;&#21015;&#25968;&#25454;&#65292;&#24182;&#37319;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32593;&#32476;&#23545;&#25968;&#23383;&#38453;&#21015;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#25191;&#34892;&#19968;&#31181;&#31216;&#20026;&#20449;&#36947;&#20462;&#22797;&#30340;&#33258;&#30417;&#30563;&#22122;&#22768;&#37325;&#26500;&#20219;&#21153;&#65292;&#21363;&#32593;&#32476;&#25512;&#26029;&#34987;&#38646;&#25513;&#30422;&#30340;&#38453;&#21015;&#25968;&#25454;&#30340;&#20869;&#23481;&#12290;&#33258;&#30417;&#30563;&#27493;&#39588;&#19981;&#38656;&#35201;&#20154;&#24037;&#26631;&#35760;&#25968;&#25454;&#12290;&#28982;&#21518;&#65292;&#23558;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;&#26550;&#26500;&#21644;&#26435;&#37325;&#36716;&#31227;&#21040;&#20855;&#26377;&#29305;&#23450;&#20219;&#21153;&#35299;&#30721;&#22120;&#30340;&#26032;&#32593;&#32476;&#65292;&#24182;&#22312;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#19978;&#23545;&#26032;&#32593;&#32476;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#23545;&#26410;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#20351;&#24471;&#26032;&#32593;&#32476;&#22312;&#25968;&#23383;&#38453;&#21015;&#25968;&#25454;&#30340;&#24102;&#23485;&#22238;&#24402;&#20219;&#21153;&#20013;&#27604;&#20174;&#38543;&#26426;&#21021;&#22987;&#21270;&#35757;&#32451;&#30340;&#31561;&#25928;&#32593;&#32476;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work presents the first applications of self-supervised learning applied to data from digital antenna arrays. Encoder-decoder networks are pretrained on digital array data to perform a self-supervised noisy-reconstruction task called channel in-painting, in which the network infers the contents of array data that has been masked with zeros. The self-supervised step requires no human-labeled data. The encoder architecture and weights from pretraining are then transferred to a new network with a task-specific decoder, and the new network is trained on a small volume of labeled data. We show that pretraining on the unlabeled data allows the new network to perform the task of bandwidth regression on the digital array data better than an equivalent network that is trained on the same labeled data from random initialization.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#25915;&#20987;&#26816;&#27979;&#27169;&#22411;&#65292;&#33021;&#22815;&#20351;&#29992;&#25968;&#25454;&#21644;&#26085;&#24535;&#26469;&#23398;&#20064;&#30005;&#21147;&#31995;&#32479;&#34892;&#20026;&#65292;&#22312;&#26234;&#33021;&#30005;&#32593;&#20013;&#26377;&#25928;&#22320;&#35782;&#21035;&#28508;&#22312;&#30340;&#23433;&#20840;&#36793;&#30028;&#65292;&#24182;&#19988;&#20351;&#29992;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#33021;&#22815;&#36798;&#21040;90.56%&#30340;&#20934;&#30830;&#29575;&#65292;&#26377;&#21161;&#20110;&#25805;&#20316;&#21592;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2307.03323</link><description>&lt;p&gt;
&#29992;&#20110;&#26816;&#27979;&#32593;&#32476;&#25915;&#20987;&#21644;&#21306;&#20998;&#30005;&#21147;&#31995;&#32479;&#24178;&#25200;&#31867;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Machine Learning to detect cyber-attacks and discriminating the types of power system disturbances. (arXiv:2307.03323v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03323
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#25915;&#20987;&#26816;&#27979;&#27169;&#22411;&#65292;&#33021;&#22815;&#20351;&#29992;&#25968;&#25454;&#21644;&#26085;&#24535;&#26469;&#23398;&#20064;&#30005;&#21147;&#31995;&#32479;&#34892;&#20026;&#65292;&#22312;&#26234;&#33021;&#30005;&#32593;&#20013;&#26377;&#25928;&#22320;&#35782;&#21035;&#28508;&#22312;&#30340;&#23433;&#20840;&#36793;&#30028;&#65292;&#24182;&#19988;&#20351;&#29992;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#33021;&#22815;&#36798;&#21040;90.56%&#30340;&#20934;&#30830;&#29575;&#65292;&#26377;&#21161;&#20110;&#25805;&#20316;&#21592;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#25915;&#20987;&#26816;&#27979;&#27169;&#22411;&#65292;&#38024;&#23545;&#26234;&#33021;&#30005;&#32593;&#30340;&#30005;&#21147;&#31995;&#32479;&#12290;&#36890;&#36807;&#21033;&#29992;&#20174;&#30456;&#37327;&#27979;&#37327;&#35774;&#22791;&#65288;PMUs&#65289;&#25910;&#38598;&#30340;&#25968;&#25454;&#21644;&#26085;&#24535;&#65292;&#35813;&#27169;&#22411;&#26088;&#22312;&#23398;&#20064;&#31995;&#32479;&#34892;&#20026;&#24182;&#26377;&#25928;&#22320;&#35782;&#21035;&#28508;&#22312;&#30340;&#23433;&#20840;&#36793;&#30028;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21253;&#25324;&#25968;&#25454;&#38598;&#39044;&#22788;&#29702;&#12289;&#29305;&#24449;&#36873;&#25321;&#12289;&#27169;&#22411;&#21019;&#24314;&#21644;&#35780;&#20272;&#31561;&#20851;&#38190;&#38454;&#27573;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#26469;&#33258;&#19981;&#21516;PMUs&#30340;15&#20010;&#29420;&#31435;&#25968;&#25454;&#38598;&#65292;&#32487;&#30005;&#22120;&#21957;&#25506;&#22120;&#35686;&#25253;&#21644;&#26085;&#24535;&#12290;&#26500;&#24314;&#21644;&#35780;&#20272;&#20102;&#19977;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65306;&#38543;&#26426;&#26862;&#26519;&#12289;&#36923;&#36753;&#22238;&#24402;&#21644;K-&#26368;&#36817;&#37051;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#22312;&#26816;&#27979;&#30005;&#21147;&#31995;&#32479;&#24178;&#25200;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#39640;&#30340;&#24615;&#33021;&#65292;&#22312;&#20915;&#31574;&#36807;&#31243;&#20013;&#26377;&#21161;&#20110;&#25805;&#20316;&#21592;&#36827;&#34892;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research proposes a machine learning-based attack detection model for power systems, specifically targeting smart grids. By utilizing data and logs collected from Phasor Measuring Devices (PMUs), the model aims to learn system behaviors and effectively identify potential security boundaries. The proposed approach involves crucial stages including dataset pre-processing, feature selection, model creation, and evaluation. To validate our approach, we used a dataset used, consist of 15 separate datasets obtained from different PMUs, relay snort alarms and logs. Three machine learning models: Random Forest, Logistic Regression, and K-Nearest Neighbour were built and evaluated using various performance metrics. The findings indicate that the Random Forest model achieves the highest performance with an accuracy of 90.56% in detecting power system disturbances and has the potential in assisting operators in decision-making processes.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35299;&#20915;&#20307;&#22806;&#33180;&#32954;&#27687;&#21512;(ECMO)&#27835;&#30103;&#36873;&#25321;&#20559;&#24046;&#21644;&#31232;&#32570;&#27835;&#30103;&#26696;&#20363;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Treatment Variational AutoEncoder (TVAE)&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20010;&#20307;&#21270;&#27835;&#30103;&#20998;&#26512;&#65292;&#20197;&#25903;&#25345;&#20020;&#24202;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2307.03315</link><description>&lt;p&gt;
&#36890;&#36807;&#35299;&#32544;&#30340;&#28508;&#22312;&#34920;&#31034;&#36741;&#21161;&#20020;&#24202;&#20915;&#31574;&#20197;&#33719;&#21462;&#21487;&#29992;&#27835;&#30103;
&lt;/p&gt;
&lt;p&gt;
Assisting Clinical Decisions for Scarcely Available Treatment via Disentangled Latent Representation. (arXiv:2307.03315v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03315
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35299;&#20915;&#20307;&#22806;&#33180;&#32954;&#27687;&#21512;(ECMO)&#27835;&#30103;&#36873;&#25321;&#20559;&#24046;&#21644;&#31232;&#32570;&#27835;&#30103;&#26696;&#20363;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Treatment Variational AutoEncoder (TVAE)&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20010;&#20307;&#21270;&#27835;&#30103;&#20998;&#26512;&#65292;&#20197;&#25903;&#25345;&#20020;&#24202;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20307;&#22806;&#33180;&#32954;&#27687;&#21512;(ECMO)&#26159;&#23545;&#20110;COVID-19&#24739;&#32773;&#30340;&#37325;&#35201;&#30340;&#29983;&#21629;&#25903;&#25345;&#26041;&#24335;&#65292;&#36825;&#20123;&#24739;&#32773;&#23545;&#20256;&#32479;&#27835;&#30103;&#26041;&#27861;&#26080;&#25928;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#36825;&#31181;&#31232;&#32570;&#19988;&#25216;&#26415;&#22797;&#26434;&#30340;&#27835;&#30103;&#36873;&#25321;&#65292;&#36866;&#24403;&#30340;&#27835;&#30103;&#20915;&#31574;&#19968;&#30452;&#22791;&#21463;&#20105;&#35758;&#65292;&#23545;&#20110;&#35841;&#20250;&#20174;&#20013;&#21463;&#30410;&#20173;&#28982;&#23384;&#22312;&#20105;&#35758;&#12290;&#20026;&#20102;&#25903;&#25345;&#20020;&#24202;&#20915;&#31574;&#65292;&#39044;&#27979;&#27835;&#30103;&#38656;&#27714;&#21644;&#28508;&#22312;&#30340;&#27835;&#30103;&#19982;&#38750;&#27835;&#30103;&#21453;&#24212;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#38024;&#23545;&#36825;&#19968;&#20020;&#24202;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Treatment Variational AutoEncoder (TVAE)&#65292;&#19968;&#31181;&#29992;&#20110;&#20010;&#20307;&#21270;&#27835;&#30103;&#20998;&#26512;&#30340;&#26032;&#26041;&#27861;&#12290;TVAE&#19987;&#38376;&#35774;&#35745;&#26469;&#35299;&#20915;&#20687;ECMO&#36825;&#26679;&#20855;&#26377;&#24378;&#28872;&#27835;&#30103;&#36873;&#25321;&#20559;&#24046;&#21644;&#27835;&#30103;&#26696;&#20363;&#31232;&#32570;&#30340;&#24314;&#27169;&#25361;&#25112;&#12290;TVAE&#23558;&#27835;&#30103;&#20915;&#31574;&#35270;&#20026;&#19968;&#20010;&#22810;&#23610;&#24230;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#24739;&#32773;&#30340;&#28508;&#22312;&#27835;&#30103;&#20998;&#37197;&#20197;&#21450;&#20107;&#23454;&#21644;&#21453;&#20107;&#23454;&#32467;&#26524;&#20316;&#20026;&#20182;&#20204;&#22266;&#26377;&#29305;&#24449;&#30340;&#19968;&#37096;&#20998;&#36827;&#34892;&#24314;&#27169;&#65292;&#36825;&#20123;&#29305;&#24449;&#21487;&#20197;&#34987;&#34920;&#24449;&#65292;
&lt;/p&gt;
&lt;p&gt;
Extracorporeal membrane oxygenation (ECMO) is an essential life-supporting modality for COVID-19 patients who are refractory to conventional therapies. However, the proper treatment decision has been the subject of significant debate and it remains controversial about who benefits from this scarcely available and technically complex treatment option. To support clinical decisions, it is a critical need to predict the treatment need and the potential treatment and no-treatment responses. Targeting this clinical challenge, we propose Treatment Variational AutoEncoder (TVAE), a novel approach for individualized treatment analysis. TVAE is specifically designed to address the modeling challenges like ECMO with strong treatment selection bias and scarce treatment cases. TVAE conceptualizes the treatment decision as a multi-scale problem. We model a patient's potential treatment assignment and the factual and counterfactual outcomes as part of their intrinsic characteristics that can be repr
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#29699;&#38754;&#35856;&#27874;&#34920;&#31034;&#30340;&#29702;&#35770;&#22522;&#30784;&#21644;&#23454;&#38469;&#24212;&#29992;&#65292;&#21253;&#25324;&#26059;&#36716;&#19981;&#21464;&#21644;&#31561;&#21464;&#29305;&#24449;&#65292;&#20197;&#21450;&#29699;&#38754;&#19978;&#20449;&#21495;&#30340;&#21367;&#31215;&#21644;&#31934;&#30830;&#30456;&#20851;&#24615;&#12290;&#27492;&#22806;&#65292;&#36824;&#23558;&#36825;&#20123;&#26041;&#27861;&#25512;&#24191;&#21040;&#20102;&#30690;&#37327;&#35856;&#27874;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2307.03311</link><description>&lt;p&gt;
&#20851;&#20110;&#29699;&#38754;&#35856;&#27874;&#34920;&#31034;&#30340;&#19981;&#21464;&#24615;&#12289;&#31561;&#21464;&#24615;&#12289;&#30456;&#20851;&#24615;&#21644;&#21367;&#31215;&#30340;&#26631;&#37327;&#21644;&#30690;&#37327;&#25968;&#25454;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
On Invariance, Equivariance, Correlation and Convolution of Spherical Harmonic Representations for Scalar and Vectorial Data. (arXiv:2307.03311v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03311
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#29699;&#38754;&#35856;&#27874;&#34920;&#31034;&#30340;&#29702;&#35770;&#22522;&#30784;&#21644;&#23454;&#38469;&#24212;&#29992;&#65292;&#21253;&#25324;&#26059;&#36716;&#19981;&#21464;&#21644;&#31561;&#21464;&#29305;&#24449;&#65292;&#20197;&#21450;&#29699;&#38754;&#19978;&#20449;&#21495;&#30340;&#21367;&#31215;&#21644;&#31934;&#30830;&#30456;&#20851;&#24615;&#12290;&#27492;&#22806;&#65292;&#36824;&#23558;&#36825;&#20123;&#26041;&#27861;&#25512;&#24191;&#21040;&#20102;&#30690;&#37327;&#35856;&#27874;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#29699;&#38754;&#35856;&#27874;&#65288;SH&#65289;&#22495;&#20013;&#30340;&#25968;&#25454;&#25968;&#23398;&#34920;&#31034;&#26041;&#27861;&#22312;&#26426;&#22120;&#23398;&#20064;&#30028;&#37325;&#26032;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#26412;&#25216;&#26415;&#25253;&#21578;&#23545;SH&#34920;&#31034;&#30340;&#29702;&#35770;&#22522;&#30784;&#21644;&#23454;&#38469;&#23454;&#29616;&#36827;&#34892;&#20102;&#28145;&#20837;&#20171;&#32461;&#65292;&#24635;&#32467;&#20102;&#20851;&#20110;&#26059;&#36716;&#19981;&#21464;&#21644;&#31561;&#21464;&#29305;&#24449;&#20197;&#21450;&#29699;&#38754;&#19978;&#20449;&#21495;&#30340;&#21367;&#31215;&#21644;&#31934;&#30830;&#30456;&#20851;&#24615;&#30340;&#24037;&#20316;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#20174;&#26631;&#37327;SH&#34920;&#31034;&#25193;&#23637;&#21040;&#30690;&#37327;&#35856;&#27874;&#65288;VH&#65289;&#65292;&#20026;&#29699;&#38754;&#19978;&#30340;3D&#30690;&#37327;&#22330;&#25552;&#20379;&#30456;&#21516;&#30340;&#21151;&#33021;
&lt;/p&gt;
&lt;p&gt;
The mathematical representations of data in the Spherical Harmonic (SH) domain has recently regained increasing interest in the machine learning community. This technical report gives an in-depth introduction to the theoretical foundation and practical implementation of SH representations, summarizing works on rotation invariant and equivariant features, as well as convolutions and exact correlations of signals on spheres. In extension, these methods are then generalized from scalar SH representations to Vectorial Harmonics (VH), providing the same capabilities for 3d vector fields on spheres
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#26159;&#23545;&#20844;&#24179;&#20998;&#31867;&#31639;&#27861;&#36827;&#34892;&#30340;&#19968;&#27425;&#39318;&#27425;&#30340;&#22836;&#23545;&#22836;&#27604;&#36739;&#65292;&#30740;&#31350;&#20102;&#20381;&#36182;&#23646;&#24615;&#12289;&#23481;&#24525;&#22122;&#22768;&#21644;&#30450;&#30446;&#23646;&#24615;&#30340;&#31639;&#27861;&#22312;&#39044;&#27979;&#24615;&#21644;&#20844;&#24179;&#24615;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#30450;&#30446;&#23646;&#24615;&#21644;&#23481;&#24525;&#22122;&#22768;&#30340;&#20844;&#24179;&#20998;&#31867;&#22120;&#20855;&#26377;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.03306</link><description>&lt;p&gt;
&#24403;&#20844;&#24179;&#20998;&#31867;&#36935;&#21040;&#22024;&#26434;&#30340;&#20445;&#25252;&#23646;&#24615;
&lt;/p&gt;
&lt;p&gt;
When Fair Classification Meets Noisy Protected Attributes. (arXiv:2307.03306v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03306
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#26159;&#23545;&#20844;&#24179;&#20998;&#31867;&#31639;&#27861;&#36827;&#34892;&#30340;&#19968;&#27425;&#39318;&#27425;&#30340;&#22836;&#23545;&#22836;&#27604;&#36739;&#65292;&#30740;&#31350;&#20102;&#20381;&#36182;&#23646;&#24615;&#12289;&#23481;&#24525;&#22122;&#22768;&#21644;&#30450;&#30446;&#23646;&#24615;&#30340;&#31639;&#27861;&#22312;&#39044;&#27979;&#24615;&#21644;&#20844;&#24179;&#24615;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#30450;&#30446;&#23646;&#24615;&#21644;&#23481;&#24525;&#22122;&#22768;&#30340;&#20844;&#24179;&#20998;&#31867;&#22120;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31639;&#27861;&#20844;&#24179;&#24615;&#30340;&#23454;&#26045;&#38754;&#20020;&#30528;&#35768;&#22810;&#23454;&#38469;&#25361;&#25112;&#65292;&#20854;&#20013;&#20043;&#19968;&#23601;&#26159;&#25968;&#25454;&#38598;&#20013;&#21463;&#20445;&#25252;&#23646;&#24615;&#30340;&#21487;&#29992;&#24615;&#25110;&#21487;&#38752;&#24615;&#12290;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#29615;&#22659;&#20013;&#65292;&#23454;&#38469;&#21644;&#27861;&#24459;&#19978;&#30340;&#38556;&#30861;&#21487;&#33021;&#20250;&#38459;&#27490;&#25910;&#38598;&#21644;&#20351;&#29992;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#65292;&#20351;&#24471;&#30830;&#20445;&#31639;&#27861;&#20844;&#24179;&#24615;&#21464;&#24471;&#22256;&#38590;&#12290;&#23613;&#31649;&#26368;&#21021;&#30340;&#20844;&#24179;&#31639;&#27861;&#27809;&#26377;&#32771;&#34385;&#36825;&#20123;&#38480;&#21046;&#65292;&#20294;&#26368;&#36817;&#30340;&#25552;&#35758;&#26088;&#22312;&#36890;&#36807;&#23558;&#21463;&#20445;&#25252;&#23646;&#24615;&#30340;&#22024;&#26434;&#24615;&#32435;&#20837;&#32771;&#34385;&#25110;&#26681;&#26412;&#19981;&#20351;&#29992;&#21463;&#20445;&#25252;&#23646;&#24615;&#26469;&#23454;&#29616;&#20998;&#31867;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#39318;&#27425;&#23545;&#27604;&#22522;&#20110;&#23646;&#24615;&#12289;&#23481;&#24525;&#22122;&#22768;&#21644;&#30450;&#30446;&#23646;&#24615;&#30340;&#20844;&#24179;&#20998;&#31867;&#31639;&#27861;&#22312;&#39044;&#27979;&#24615;&#21644;&#20844;&#24179;&#24615;&#36825;&#20004;&#20010;&#26041;&#38754;&#36827;&#34892;&#22836;&#23545;&#22836;&#30740;&#31350;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#22235;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#21644;&#21512;&#25104;&#25200;&#21160;&#30340;&#26696;&#20363;&#30740;&#31350;&#35780;&#20272;&#20102;&#36825;&#20123;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#30450;&#30446;&#23646;&#24615;&#21644;&#23481;&#24525;&#22122;&#22768;&#30340;&#20844;&#24179;&#20998;&#31867;&#22120;&#21487;&#33021;&#20250;&#22312;&#39044;&#27979;&#24615;&#21644;&#20844;&#24179;&#24615;&#30340;&#21452;&#37325;&#36724;&#19978;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The operationalization of algorithmic fairness comes with several practical challenges, not the least of which is the availability or reliability of protected attributes in datasets. In real-world contexts, practical and legal impediments may prevent the collection and use of demographic data, making it difficult to ensure algorithmic fairness. While initial fairness algorithms did not consider these limitations, recent proposals aim to achieve algorithmic fairness in classification by incorporating noisiness in protected attributes or not using protected attributes at all.  To the best of our knowledge, this is the first head-to-head study of fair classification algorithms to compare attribute-reliant, noise-tolerant and attribute-blind algorithms along the dual axes of predictivity and fairness. We evaluated these algorithms via case studies on four real-world datasets and synthetic perturbations. Our study reveals that attribute-blind and noise-tolerant fair classifiers can potentia
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#20351;&#29992;&#21069;softmax&#20998;&#25968;&#30340;&#24402;&#23646;&#26041;&#27861;&#30340;&#19968;&#20010;&#28431;&#27934;&#65292;&#35813;&#26041;&#27861;&#29992;&#20110;&#35299;&#37322;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#36755;&#20986;&#12290;&#19982;&#23545;&#25239;&#24615;&#25915;&#20987;&#19981;&#21516;&#65292;&#20316;&#32773;&#20851;&#27880;&#30340;&#26159;&#23545;&#24402;&#23646;&#26041;&#27861;&#36827;&#34892;&#23567;&#20462;&#25913;&#21487;&#33021;&#23548;&#33268;&#30340;&#24433;&#21709;&#65292;&#32780;&#19981;&#20250;&#25913;&#21464;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2307.03305</link><description>&lt;p&gt;
&#20351;&#29992;&#21069;softmax&#20998;&#25968;&#30340;&#24402;&#23646;&#26041;&#27861;&#30340;&#19968;&#20010;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
A Vulnerability of Attribution Methods Using Pre-Softmax Scores. (arXiv:2307.03305v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03305
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#20351;&#29992;&#21069;softmax&#20998;&#25968;&#30340;&#24402;&#23646;&#26041;&#27861;&#30340;&#19968;&#20010;&#28431;&#27934;&#65292;&#35813;&#26041;&#27861;&#29992;&#20110;&#35299;&#37322;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#36755;&#20986;&#12290;&#19982;&#23545;&#25239;&#24615;&#25915;&#20987;&#19981;&#21516;&#65292;&#20316;&#32773;&#20851;&#27880;&#30340;&#26159;&#23545;&#24402;&#23646;&#26041;&#27861;&#36827;&#34892;&#23567;&#20462;&#25913;&#21487;&#33021;&#23548;&#33268;&#30340;&#24433;&#21709;&#65292;&#32780;&#19981;&#20250;&#25913;&#21464;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35752;&#35770;&#20102;&#19968;&#31867;&#29992;&#20110;&#35299;&#37322;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#36755;&#20986;&#30340;&#24402;&#23646;&#26041;&#27861;&#30340;&#19968;&#20010;&#28431;&#27934;&#12290;&#24050;&#30693;&#36825;&#31181;&#31867;&#22411;&#30340;&#32593;&#32476;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#21363;&#36755;&#20837;&#30340;&#24494;&#23567;&#25200;&#21160;&#21487;&#33021;&#20250;&#25913;&#21464;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#19982;&#27492;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#20851;&#27880;&#30340;&#26159;&#23545;&#24402;&#23646;&#26041;&#27861;&#36827;&#34892;&#23567;&#20462;&#25913;&#21487;&#33021;&#23548;&#33268;&#30340;&#24433;&#21709;&#65292;&#32780;&#19981;&#20250;&#25913;&#21464;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
We discuss a vulnerability involving a category of attribution methods used to provide explanations for the outputs of convolutional neural networks working as classifiers. It is known that this type of networks are vulnerable to adversarial attacks, in which imperceptible perturbations of the input may alter the outputs of the model. In contrast, here we focus on effects that small modifications in the model may cause on the attribution method without altering the model outputs.
&lt;/p&gt;</description></item><item><title>&#31561;&#21464;&#29699;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#25552;&#39640;&#21307;&#23398;&#22270;&#20687;&#22788;&#29702;&#30340;&#25928;&#29575;&#21644;&#24615;&#33021;&#65292;&#36890;&#36807;&#38477;&#20302;&#23545;&#29305;&#23450;&#35757;&#32451;&#38598;&#30340;&#20381;&#36182;&#24615;&#65292;&#24182;&#19988;&#22312;&#21435;&#22122;&#21644;&#37325;&#24314;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#36136;&#37327;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.03298</link><description>&lt;p&gt;
&#25968;&#25454;&#39640;&#25928;&#21644;&#39640;&#24615;&#33021;&#21307;&#23398;&#22270;&#20687;&#22788;&#29702;&#30340;&#31561;&#21464;&#29699;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Equivariant Spherical CNN for Data Efficient and High-Performance Medical Image Processing. (arXiv:2307.03298v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03298
&lt;/p&gt;
&lt;p&gt;
&#31561;&#21464;&#29699;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#25552;&#39640;&#21307;&#23398;&#22270;&#20687;&#22788;&#29702;&#30340;&#25928;&#29575;&#21644;&#24615;&#33021;&#65292;&#36890;&#36807;&#38477;&#20302;&#23545;&#29305;&#23450;&#35757;&#32451;&#38598;&#30340;&#20381;&#36182;&#24615;&#65292;&#24182;&#19988;&#22312;&#21435;&#22122;&#21644;&#37325;&#24314;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#36136;&#37327;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#31361;&#20986;&#20102;&#31561;&#21464;&#32593;&#32476;&#20316;&#20026;&#39640;&#25928;&#21644;&#39640;&#24615;&#33021;&#36884;&#24452;&#22312;&#26029;&#23618;&#25195;&#25551;&#24212;&#29992;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24314;&#31435;&#22312;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#23616;&#38480;&#24615;&#20043;&#19978;&#65292;CNN&#24050;&#32463;&#22312;&#21508;&#31181;&#21307;&#23398;&#24433;&#20687;&#31995;&#32479;&#30340;&#21518;&#22788;&#29702;&#20013;&#26174;&#31034;&#20986;&#20102;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;CNN&#30340;&#25928;&#29575;&#20005;&#37325;&#20381;&#36182;&#20110;&#19968;&#20010;&#19981;&#21464;&#21644;&#36866;&#24403;&#30340;&#35757;&#32451;&#38598;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31561;&#21464;&#32593;&#32476;&#65292;&#26088;&#22312;&#20943;&#23569;CNN&#23545;&#29305;&#23450;&#35757;&#32451;&#38598;&#30340;&#20381;&#36182;&#24615;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#31561;&#21464;CNN&#22312;&#29699;&#20449;&#21495;&#19978;&#22312;&#26029;&#23618;&#25195;&#25551;&#21307;&#23398;&#25104;&#20687;&#38382;&#39064;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#29699;&#24418;CNN&#65288;SCNN&#65289;&#22312;&#21435;&#22122;&#21644;&#37325;&#24314;&#22522;&#20934;&#38382;&#39064;&#19978;&#20855;&#26377;&#20248;&#36234;&#30340;&#36136;&#37327;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;SCNN&#20316;&#20026;&#20256;&#32479;&#22270;&#20687;&#37325;&#24314;&#24037;&#20855;&#30340;&#34917;&#20805;&#65292;&#22686;&#24378;&#32467;&#26524;&#21516;&#26102;&#20943;&#23569;&#23545;&#35757;&#32451;&#38598;&#30340;&#20381;&#36182;&#24615;&#12290;&#22312;&#25152;&#26377;&#26696;&#20363;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;...
&lt;/p&gt;
&lt;p&gt;
This work highlights the significance of equivariant networks as efficient and high-performance approaches for tomography applications. Our study builds upon the limitations of Convolutional Neural Networks (CNNs), which have shown promise in post-processing various medical imaging systems. However, the efficiency of conventional CNNs heavily relies on an undiminished and proper training set. To tackle this issue, in this study, we introduce an equivariant network, aiming to reduce CNN's dependency on specific training sets. We evaluate the efficacy of equivariant CNNs on spherical signals for tomographic medical imaging problems. Our results demonstrate superior quality and computational efficiency of spherical CNNs (SCNNs) in denoising and reconstructing benchmark problems. Furthermore, we propose a novel approach to employ SCNNs as a complement to conventional image reconstruction tools, enhancing the outcomes while reducing reliance on the training set. Across all cases, we observe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;OmniBoost&#65292;&#19968;&#31181;&#36731;&#37327;&#32423;&#12289;&#21487;&#25193;&#23637;&#30340;&#24322;&#26500;&#23884;&#20837;&#24335;&#35774;&#22791;&#22810;DNN&#31649;&#29702;&#22120;&#12290;&#36890;&#36807;&#21033;&#29992;&#38543;&#26426;&#31354;&#38388;&#25506;&#32034;&#21644;&#39640;&#31934;&#24230;&#24615;&#33021;&#20272;&#35745;&#22120;&#65292;&#30456;&#27604;&#20854;&#20182;&#20808;&#36827;&#26041;&#27861;&#65292;OmniBoost&#24179;&#22343;&#25552;&#39640;&#20102;4.6&#20493;&#30340;&#21534;&#21520;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.03290</link><description>&lt;p&gt;
OmniBoost: &#22312;&#22810;&#20010;DNN&#24037;&#20316;&#36127;&#36733;&#19979;&#22686;&#21152;&#24322;&#26500;&#23884;&#20837;&#24335;&#35774;&#22791;&#30340;&#21534;&#21520;&#37327;
&lt;/p&gt;
&lt;p&gt;
OmniBoost: Boosting Throughput of Heterogeneous Embedded Devices under Multi-DNN Workload. (arXiv:2307.03290v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03290
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;OmniBoost&#65292;&#19968;&#31181;&#36731;&#37327;&#32423;&#12289;&#21487;&#25193;&#23637;&#30340;&#24322;&#26500;&#23884;&#20837;&#24335;&#35774;&#22791;&#22810;DNN&#31649;&#29702;&#22120;&#12290;&#36890;&#36807;&#21033;&#29992;&#38543;&#26426;&#31354;&#38388;&#25506;&#32034;&#21644;&#39640;&#31934;&#24230;&#24615;&#33021;&#20272;&#35745;&#22120;&#65292;&#30456;&#27604;&#20854;&#20182;&#20808;&#36827;&#26041;&#27861;&#65292;OmniBoost&#24179;&#22343;&#25552;&#39640;&#20102;4.6&#20493;&#30340;&#21534;&#21520;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#20855;&#26377;&#26174;&#33879;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#23646;&#24615;&#12290;&#36825;&#24341;&#20837;&#20102;&#30001;&#22810;&#20010;DNN&#24212;&#29992;&#31243;&#24207;&#32452;&#25104;&#30340;&#24212;&#29992;&#36127;&#36733;&#65292;&#25552;&#20986;&#20102;&#20851;&#20110;&#36127;&#36733;&#20998;&#37197;&#30340;&#26032;&#30340;&#25361;&#25112;&#12290;&#26032;&#30340;&#23884;&#20837;&#24335;&#31995;&#32479;&#20855;&#22791;&#20102;&#22810;&#26679;&#21270;&#30340;&#21152;&#36895;&#22120;&#65292;&#20294;&#24403;&#21069;&#30340;&#36816;&#34892;&#26102;&#25511;&#21046;&#22120;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#36825;&#31181;&#24322;&#26500;&#24615;&#33021;&#12290;&#20026;&#20102;&#23454;&#29616;&#22810;DNN&#24037;&#20316;&#36127;&#36733;&#30340;&#39640;&#21534;&#21520;&#37327;&#65292;&#36825;&#26679;&#30340;&#25511;&#21046;&#22120;&#24517;&#39035;&#25506;&#32034;&#25968;&#21313;&#19975;&#31181;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#21033;&#29992;&#24213;&#23618;&#30340;&#24322;&#26500;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36731;&#37327;&#32423;&#19988;&#21487;&#25193;&#23637;&#30340;&#22810;DNN&#31649;&#29702;&#22120;OmniBoost&#65292;&#21033;&#29992;&#38543;&#26426;&#31354;&#38388;&#25506;&#32034;&#21644;&#39640;&#31934;&#24230;&#24615;&#33021;&#20272;&#35745;&#22120;&#65292;&#19982;&#20854;&#20182;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;&#24179;&#22343;&#21534;&#21520;&#37327;&#25552;&#39640;&#20102;4.6&#20493;&#12290;&#35780;&#20272;&#26159;&#22312;HiKey970&#24320;&#21457;&#26495;&#19978;&#36827;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern Deep Neural Networks (DNNs) exhibit profound efficiency and accuracy properties. This has introduced application workloads that comprise of multiple DNN applications, raising new challenges regarding workload distribution. Equipped with a diverse set of accelerators, newer embedded system present architectural heterogeneity, which current run-time controllers are unable to fully utilize. To enable high throughput in multi-DNN workloads, such a controller is ought to explore hundreds of thousands of possible solutions to exploit the underlying heterogeneity. In this paper, we propose OmniBoost, a lightweight and extensible multi-DNN manager for heterogeneous embedded devices. We leverage stochastic space exploration and we combine it with a highly accurate performance estimator to observe a x4.6 average throughput boost compared to other state-of-the-art methods. The evaluation was performed on the HiKey970 development board.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#29992;&#20110;&#20122;&#32447;&#24615;&#36229;&#20307;&#31215;&#36951;&#25022;&#24230;&#37327;&#30340;&#26368;&#20248;&#26631;&#37327;&#21270;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#20855;&#26377;&#22343;&#21248;&#38543;&#26426;&#26435;&#37325;&#30340;&#36229;&#20307;&#31215;&#26631;&#37327;&#21270;&#26041;&#27861;&#22312;&#26368;&#23567;&#21270;&#36229;&#20307;&#31215;&#36951;&#25022;&#26041;&#38754;&#26159;&#26368;&#20248;&#30340;&#65292;&#24182;&#22312;&#22810;&#30446;&#26631;&#38543;&#26426;&#32447;&#24615;&#36172;&#21338;&#26426;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#26696;&#20363;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2307.03288</link><description>&lt;p&gt;
&#29992;&#20110;&#20122;&#32447;&#24615;&#36229;&#20307;&#31215;&#36951;&#25022;&#24230;&#37327;&#30340;&#26368;&#20248;&#26631;&#37327;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Optimal Scalarizations for Sublinear Hypervolume Regret. (arXiv:2307.03288v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03288
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#29992;&#20110;&#20122;&#32447;&#24615;&#36229;&#20307;&#31215;&#36951;&#25022;&#24230;&#37327;&#30340;&#26368;&#20248;&#26631;&#37327;&#21270;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#20855;&#26377;&#22343;&#21248;&#38543;&#26426;&#26435;&#37325;&#30340;&#36229;&#20307;&#31215;&#26631;&#37327;&#21270;&#26041;&#27861;&#22312;&#26368;&#23567;&#21270;&#36229;&#20307;&#31215;&#36951;&#25022;&#26041;&#38754;&#26159;&#26368;&#20248;&#30340;&#65292;&#24182;&#22312;&#22810;&#30446;&#26631;&#38543;&#26426;&#32447;&#24615;&#36172;&#21338;&#26426;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#37327;&#21270;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#22810;&#30446;&#26631;&#35774;&#32622;&#20013;&#65292;&#23558;&#22810;&#20010;&#30446;&#26631;&#20943;&#23569;&#20026;&#19968;&#20010;&#65292;&#20363;&#22914;&#26368;&#36817;&#22312;RLHF&#20013;&#29992;&#20110;&#35757;&#32451;&#26657;&#20934;&#20154;&#31867;&#20559;&#22909;&#30340;&#22870;&#21169;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#19968;&#20123;&#20154;&#23545;&#36825;&#31181;&#32463;&#20856;&#26041;&#27861;&#25345;&#21542;&#23450;&#24577;&#24230;&#65292;&#22240;&#20026;&#24050;&#30693;&#32447;&#24615;&#26631;&#37327;&#21270;&#20250;&#24573;&#30053;&#24085;&#32047;&#25176;&#21069;&#27839;&#30340;&#20985;&#21306;&#22495;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#26088;&#22312;&#25214;&#21040;&#31616;&#21333;&#30340;&#38750;&#32447;&#24615;&#26631;&#37327;&#21270;&#26041;&#27861;&#65292;&#20197;&#36890;&#36807;&#34987;&#25903;&#37197;&#30340;&#36229;&#20307;&#31215;&#26469;&#25506;&#32034;&#24085;&#32047;&#25176;&#21069;&#27839;&#19978;&#30340;&#22810;&#26679;&#21270;&#30446;&#26631;&#38598;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#20855;&#26377;&#22343;&#21248;&#38543;&#26426;&#26435;&#37325;&#30340;&#36229;&#20307;&#31215;&#26631;&#37327;&#21270;&#20196;&#20154;&#24778;&#35766;&#22320;&#26159;&#20026;&#20102;&#35777;&#26126;&#26368;&#23567;&#21270;&#36229;&#20307;&#31215;&#36951;&#25022;&#32780;&#26368;&#20248;&#30340;&#65292;&#23454;&#29616;&#20102; $O(T^{-1/k})$ &#30340;&#26368;&#20248;&#20122;&#32447;&#24615;&#36951;&#25022;&#30028;&#65292;&#21516;&#26102;&#21305;&#37197;&#30340;&#19979;&#30028;&#34920;&#26126;&#22312;&#28176;&#36817;&#24773;&#20917;&#19979;&#27809;&#26377;&#20219;&#20309;&#31639;&#27861;&#33021;&#20570;&#24471;&#26356;&#22909;&#12290;&#20316;&#20026;&#19968;&#20010;&#29702;&#35770;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#22810;&#30446;&#26631;&#38543;&#26426;&#32447;&#24615;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#36890;&#36807;&#21033;&#29992;&#36229;&#32447;&#24615;&#36951;&#25022;&#30028;&#30340;&#36229;&#20307;&#31215;&#26631;&#37327;&#21270;&#26041;&#27861;&#65292;
&lt;/p&gt;
&lt;p&gt;
Scalarization is a general technique that can be deployed in any multiobjective setting to reduce multiple objectives into one, such as recently in RLHF for training reward models that align human preferences. Yet some have dismissed this classical approach because linear scalarizations are known to miss concave regions of the Pareto frontier. To that end, we aim to find simple non-linear scalarizations that can explore a diverse set of $k$ objectives on the Pareto frontier, as measured by the dominated hypervolume. We show that hypervolume scalarizations with uniformly random weights are surprisingly optimal for provably minimizing the hypervolume regret, achieving an optimal sublinear regret bound of $O(T^{-1/k})$, with matching lower bounds that preclude any algorithm from doing better asymptotically. As a theoretical case study, we consider the multiobjective stochastic linear bandits problem and demonstrate that by exploiting the sublinear regret bounds of the hypervolume scalariz
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#23610;&#24230;&#38899;&#35270;&#21516;&#27493;&#25439;&#22833;&#21644;&#22810;&#23610;&#24230;&#33258;&#22238;&#24402;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#35821;&#38899;&#21644;&#22836;&#37096;&#21160;&#21147;&#23398;&#30340;&#21516;&#27493;&#29983;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#24403;&#21069;&#29366;&#24577;&#19979;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2307.03270</link><description>&lt;p&gt;
&#35821;&#38899;&#21644;&#21160;&#21147;&#23398;&#21516;&#27493;&#30340;&#20840;&#38754;&#22810;&#23610;&#24230;&#26041;&#27861;&#22312;&#34394;&#25311;&#35828;&#35805;&#22836;&#29983;&#25104;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Multi-scale Approach for Speech and Dynamics Synchrony in Talking Head Generation. (arXiv:2307.03270v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03270
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#23610;&#24230;&#38899;&#35270;&#21516;&#27493;&#25439;&#22833;&#21644;&#22810;&#23610;&#24230;&#33258;&#22238;&#24402;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#35821;&#38899;&#21644;&#22836;&#37096;&#21160;&#21147;&#23398;&#30340;&#21516;&#27493;&#29983;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#24403;&#21069;&#29366;&#24577;&#19979;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#20351;&#29992;&#35821;&#38899;&#36755;&#20837;&#20449;&#21495;&#23545;&#38745;&#24577;&#38754;&#37096;&#22270;&#20687;&#36827;&#34892;&#21160;&#30011;&#21270;&#26159;&#19968;&#20010;&#27963;&#36291;&#30340;&#30740;&#31350;&#35838;&#39064;&#65292;&#24182;&#19988;&#36817;&#26399;&#21462;&#24471;&#20102;&#37325;&#35201;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#24456;&#22823;&#19968;&#37096;&#20998;&#24037;&#20316;&#37117;&#38598;&#20013;&#22312;&#22068;&#21767;&#21516;&#27493;&#21644;&#28210;&#26579;&#36136;&#37327;&#19978;&#65292;&#24456;&#23569;&#20851;&#27880;&#33258;&#28982;&#22836;&#37096;&#36816;&#21160;&#30340;&#29983;&#25104;&#65292;&#26356;&#19981;&#29992;&#35828;&#22836;&#37096;&#36816;&#21160;&#19982;&#35821;&#38899;&#30340;&#35270;&#21548;&#30456;&#20851;&#24615;&#20102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#38899;&#35270;&#21516;&#27493;&#25439;&#22833;&#21644;&#22810;&#23610;&#24230;&#33258;&#22238;&#24402;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;&#35821;&#38899;&#19982;&#22836;&#37096;&#21644;&#22068;&#21767;&#21160;&#21147;&#23398;&#20043;&#38388;&#30340;&#30701;&#26399;&#21644;&#38271;&#26399;&#30456;&#20851;&#24615;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#22312;&#22810;&#27169;&#24577;&#36755;&#20837;&#37329;&#23383;&#22612;&#19978;&#35757;&#32451;&#20102;&#19968;&#22534;&#21516;&#27493;&#27169;&#22411;&#65292;&#24182;&#23558;&#36825;&#20123;&#27169;&#22411;&#29992;&#20316;&#22810;&#23610;&#24230;&#29983;&#25104;&#32593;&#32476;&#20013;&#30340;&#25351;&#23548;&#65292;&#20197;&#20135;&#29983;&#19981;&#21516;&#26102;&#38388;&#23610;&#24230;&#19978;&#30340;&#38899;&#39057;&#23545;&#40784;&#36816;&#21160;&#23637;&#24320;&#12290;&#25105;&#20204;&#30340;&#29983;&#25104;&#22120;&#22312;&#38754;&#37096;&#26631;&#24535;&#22495;&#20013;&#25805;&#20316;&#65292;&#36825;&#26159;&#19968;&#31181;&#26631;&#20934;&#30340;&#20302;&#32500;&#22836;&#37096;&#34920;&#31034;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22836;&#37096;&#36816;&#21160;&#21160;&#21147;&#23398;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Animating still face images with deep generative models using a speech input signal is an active research topic and has seen important recent progress. However, much of the effort has been put into lip syncing and rendering quality while the generation of natural head motion, let alone the audio-visual correlation between head motion and speech, has often been neglected. In this work, we propose a multi-scale audio-visual synchrony loss and a multi-scale autoregressive GAN to better handle short and long-term correlation between speech and the dynamics of the head and lips. In particular, we train a stack of syncer models on multimodal input pyramids and use these models as guidance in a multi-scale generator network to produce audio-aligned motion unfolding over diverse time scales. Our generator operates in the facial landmark domain, which is a standard low-dimensional head representation. The experiments show significant improvements over the state of the art in head motion dynamic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#21069;&#21015;&#33146;&#25104;&#20687;&#20013;&#30340;&#26032;&#22411;&#22522;&#30784;&#27169;&#22411;UniverSeg&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#30740;&#31350;&#65292;&#24182;&#23558;&#20854;&#19982;&#20256;&#32479;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#22522;&#30784;&#27169;&#22411;&#21487;&#33021;&#26159;&#21307;&#23398;&#25104;&#20687;&#39046;&#22495;&#26410;&#26469;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2307.03266</link><description>&lt;p&gt;
&#21069;&#21015;&#33146;&#25104;&#20687;&#20013;&#20998;&#21106;&#22522;&#30784;&#27169;&#22411;&#30340;&#23454;&#35777;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Empirical Analysis of a Segmentation Foundation Model in Prostate Imaging. (arXiv:2307.03266v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03266
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#21069;&#21015;&#33146;&#25104;&#20687;&#20013;&#30340;&#26032;&#22411;&#22522;&#30784;&#27169;&#22411;UniverSeg&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#30740;&#31350;&#65292;&#24182;&#23558;&#20854;&#19982;&#20256;&#32479;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#22522;&#30784;&#27169;&#22411;&#21487;&#33021;&#26159;&#21307;&#23398;&#25104;&#20687;&#39046;&#22495;&#26410;&#26469;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#20381;&#36182;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#22312;&#29421;&#20041;&#20219;&#21153;&#19978;&#20197;&#30417;&#30563;&#30340;&#26041;&#24335;&#36827;&#34892;&#35757;&#32451;&#65292;&#38656;&#35201;&#26114;&#36149;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#12290;&#26368;&#36817;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#31561;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#21462;&#24471;&#30340;&#36827;&#23637;&#24050;&#32463;&#35777;&#26126;&#20102;&#26500;&#24314;&#22522;&#30784;&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#21644;&#23454;&#29992;&#24615;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#22312;&#20960;&#20046;&#27809;&#26377;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20026;&#21508;&#31181;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#23450;&#21046;&#12290;&#36825;&#21487;&#33021;&#20195;&#34920;&#20102;&#21307;&#23398;&#25104;&#20687;&#30340;&#33539;&#24335;&#36716;&#21464;&#65292;&#25105;&#20204;&#39044;&#35745;&#22522;&#30784;&#27169;&#22411;&#21487;&#33021;&#22609;&#36896;&#35813;&#39046;&#22495;&#30340;&#26410;&#26469;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#20010;&#26368;&#36817;&#24320;&#21457;&#30340;&#24212;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#22522;&#30784;&#27169;&#22411;UniverSeg&#12290;&#25105;&#20204;&#22312;&#21069;&#21015;&#33146;&#25104;&#20687;&#30340;&#32972;&#26223;&#19979;&#36827;&#34892;&#20102;&#32463;&#39564;&#35780;&#20272;&#30740;&#31350;&#65292;&#24182;&#23558;&#20854;&#19982;&#20256;&#32479;&#30340;&#35757;&#32451;&#20219;&#21153;&#29305;&#23450;&#20998;&#21106;&#27169;&#22411;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#21644;&#35752;&#35770;&#31361;&#20986;&#20102;&#20960;&#20010;&#37325;&#35201;&#22240;&#32032;&#65292;&#36825;&#20123;&#22240;&#32032;&#22312;&#22522;&#30784;&#27169;&#22411;&#24320;&#21457;&#20013;&#21487;&#33021;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most state-of-the-art techniques for medical image segmentation rely on deep-learning models. These models, however, are often trained on narrowly-defined tasks in a supervised fashion, which requires expensive labeled datasets. Recent advances in several machine learning domains, such as natural language generation have demonstrated the feasibility and utility of building foundation models that can be customized for various downstream tasks with little to no labeled data. This likely represents a paradigm shift for medical imaging, where we expect that foundation models may shape the future of the field. In this paper, we consider a recently developed foundation model for medical image segmentation, UniverSeg. We conduct an empirical evaluation study in the context of prostate imaging and compare it against the conventional approach of training a task-specific segmentation model. Our results and discussion highlight several important factors that will likely be important in the develo
&lt;/p&gt;</description></item><item><title>&#35270;&#35273;&#35821;&#35328;&#36716;&#25442;&#22120;&#26159;&#23558;&#39044;&#35757;&#32451;&#30340;transformer&#26550;&#26500;&#24212;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#24314;&#27169;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#65292;&#22312;&#21516;&#26102;&#36827;&#34892;&#35270;&#35273;&#21644;&#35821;&#35328;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2307.03254</link><description>&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#36716;&#25442;&#22120;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Vision Language Transformers: A Survey. (arXiv:2307.03254v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03254
&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#36716;&#25442;&#22120;&#26159;&#23558;&#39044;&#35757;&#32451;&#30340;transformer&#26550;&#26500;&#24212;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#24314;&#27169;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#65292;&#22312;&#21516;&#26102;&#36827;&#34892;&#35270;&#35273;&#21644;&#35821;&#35328;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#65292;&#22914;&#22238;&#31572;&#20851;&#20110;&#22270;&#20687;&#30340;&#38382;&#39064;&#25110;&#29983;&#25104;&#25551;&#36848;&#22270;&#20687;&#30340;&#26631;&#39064;&#65292;&#26159;&#35745;&#31639;&#26426;&#38590;&#20197;&#23436;&#25104;&#30340;&#20219;&#21153;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#23558;&#39044;&#35757;&#32451;&#30340;transformer&#26550;&#26500;&#24212;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#24314;&#27169;&#12290;&#30456;&#27604;&#20197;&#21069;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;transformer&#27169;&#22411;&#22312;&#24615;&#33021;&#21644;&#22810;&#21151;&#33021;&#24615;&#26041;&#38754;&#26377;&#24456;&#22823;&#25552;&#39640;&#12290;&#23427;&#20204;&#36890;&#36807;&#22312;&#22823;&#22411;&#36890;&#29992;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#22312;&#26550;&#26500;&#21644;&#21442;&#25968;&#20540;&#19978;&#36827;&#34892;&#24494;&#23567;&#25913;&#21464;&#21518;&#65292;&#23558;&#23398;&#20064;&#36716;&#31227;&#21040;&#26032;&#20219;&#21153;&#20013;&#12290;&#36825;&#31181;&#36801;&#31227;&#23398;&#20064;&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#26631;&#20934;&#24314;&#27169;&#23454;&#36341;&#12290;&#35270;&#35273;&#35821;&#35328;&#36716;&#25442;&#22120;&#25215;&#35834;&#22312;&#38656;&#35201;&#21516;&#26102;&#36827;&#34892;&#35270;&#35273;&#21644;&#35821;&#35328;&#30340;&#20219;&#21153;&#20013;&#20135;&#29983;&#31867;&#20284;&#30340;&#36827;&#23637;&#12290;&#26412;&#25991;&#23545;&#30446;&#21069;&#21487;&#29992;&#30340;&#35270;&#35273;&#35821;&#35328;&#36716;&#25442;&#22120;&#27169;&#22411;&#30340;&#30740;&#31350;&#36827;&#34892;&#20102;&#24191;&#27867;&#32508;&#21512;&#65292;&#24182;&#23545;&#20854;&#20248;&#21183;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision language tasks, such as answering questions about or generating captions that describe an image, are difficult tasks for computers to perform. A relatively recent body of research has adapted the pretrained transformer architecture introduced in \citet{vaswani2017attention} to vision language modeling. Transformer models have greatly improved performance and versatility over previous vision language models. They do so by pretraining models on a large generic datasets and transferring their learning to new tasks with minor changes in architecture and parameter values. This type of transfer learning has become the standard modeling practice in both natural language processing and computer vision. Vision language transformers offer the promise of producing similar advancements in tasks which require both vision and language. In this paper, we provide a broad synthesis of the currently available research on vision language transformer models and offer some analysis of their strength
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#22330;&#29702;&#35770;&#65292;&#21253;&#25324;&#38750;&#39640;&#26031;&#24615;&#12289;&#20316;&#29992;&#37327;&#21644;&#23616;&#22495;&#24615;&#12290;&#36890;&#36807;&#23545;&#32593;&#32476;&#21442;&#25968;&#30340;&#32479;&#35745;&#29420;&#31435;&#24615;&#36827;&#34892;&#24494;&#23567;&#30772;&#22351;&#65292;&#21487;&#20197;&#24471;&#21040;&#30456;&#20114;&#20316;&#29992;&#29702;&#35770;&#65292;&#36825;&#31181;&#23637;&#24320;&#26041;&#27861;&#30456;&#27604;&#20110;&#24120;&#29992;&#30340;$1/N$&#23637;&#24320;&#22312;&#26222;&#36866;&#36924;&#36817;&#23450;&#29702;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#65292;&#21516;&#26102;&#36890;&#36807;&#20851;&#32852;&#20989;&#25968;&#21487;&#20197;&#31995;&#32479;&#22320;&#37325;&#24314;&#20316;&#29992;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.03223</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22330;&#29702;&#35770;&#65306;&#38750;&#39640;&#26031;&#24615;&#65292;&#20316;&#29992;&#37327;&#21644;&#23616;&#22495;&#24615;
&lt;/p&gt;
&lt;p&gt;
Neural Network Field Theories: Non-Gaussianity, Actions, and Locality. (arXiv:2307.03223v1 [hep-th])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#22330;&#29702;&#35770;&#65292;&#21253;&#25324;&#38750;&#39640;&#26031;&#24615;&#12289;&#20316;&#29992;&#37327;&#21644;&#23616;&#22495;&#24615;&#12290;&#36890;&#36807;&#23545;&#32593;&#32476;&#21442;&#25968;&#30340;&#32479;&#35745;&#29420;&#31435;&#24615;&#36827;&#34892;&#24494;&#23567;&#30772;&#22351;&#65292;&#21487;&#20197;&#24471;&#21040;&#30456;&#20114;&#20316;&#29992;&#29702;&#35770;&#65292;&#36825;&#31181;&#23637;&#24320;&#26041;&#27861;&#30456;&#27604;&#20110;&#24120;&#29992;&#30340;$1/N$&#23637;&#24320;&#22312;&#26222;&#36866;&#36924;&#36817;&#23450;&#29702;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#65292;&#21516;&#26102;&#36890;&#36807;&#20851;&#32852;&#20989;&#25968;&#21487;&#20197;&#31995;&#32479;&#22320;&#37325;&#24314;&#20316;&#29992;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22330;&#35770;&#20013;&#30340;&#36335;&#24452;&#31215;&#20998;&#27979;&#24230;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#38598;&#21512;&#25551;&#36848;&#30340;&#26159;&#20989;&#25968;&#20998;&#24067;&#12290;&#24403;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#36866;&#29992;&#20110;&#26080;&#38480;&#23485;&#24230;&#65288;&#26080;&#38480;$N$&#65289;&#26497;&#38480;&#26102;&#65292;&#32593;&#32476;&#30340;&#38598;&#21512;&#23545;&#24212;&#20110;&#33258;&#30001;&#22330;&#29702;&#35770;&#12290;&#34429;&#28982;&#22312;$1/N$&#30340;&#23637;&#24320;&#20013;&#23545;&#24212;&#20110;&#22330;&#35770;&#20013;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#20294;&#20854;&#20182;&#23637;&#24320;&#65292;&#22914;&#32593;&#32476;&#21442;&#25968;&#32479;&#35745;&#29420;&#31435;&#24615;&#30340;&#24494;&#23567;&#30772;&#22351;&#65292;&#20063;&#21487;&#20197;&#23548;&#33268;&#30456;&#20114;&#20316;&#29992;&#29702;&#35770;&#12290;&#36825;&#20123;&#20854;&#20182;&#30340;&#23637;&#24320;&#21487;&#20197;&#27604;$1/N$&#23637;&#24320;&#26356;&#20855;&#20248;&#21183;&#65292;&#20363;&#22914;&#22312;&#26222;&#36866;&#36924;&#36817;&#23450;&#29702;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;&#36890;&#36807;&#32473;&#23450;&#22330;&#35770;&#30340;&#20851;&#32852;&#20989;&#25968;&#65292;&#21487;&#20197;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#36153;&#26364;&#22270;&#35268;&#21017;&#65292;&#39030;&#28857;&#20026;&#20851;&#32852;&#20989;&#25968;&#65292;&#31995;&#32479;&#22320;&#25353;&#29031;&#23637;&#24320;&#21442;&#25968;&#36880;&#38454;&#37325;&#24314;&#20316;&#29992;&#37327;&#12290;&#36825;&#31181;&#26041;&#27861;&#21463;&#21040;&#20102;Edgeworth&#23637;&#24320;&#30340;&#21551;&#21457;&#65292;&#21487;&#20197;&#20026;&#31070;&#32463;&#32593;&#32476;&#22330;&#29702;&#35770;&#23548;&#20986;&#20316;&#29992;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Both the path integral measure in field theory and ensembles of neural networks describe distributions over functions. When the central limit theorem can be applied in the infinite-width (infinite-$N$) limit, the ensemble of networks corresponds to a free field theory. Although an expansion in $1/N$ corresponds to interactions in the field theory, others, such as in a small breaking of the statistical independence of network parameters, can also lead to interacting theories. These other expansions can be advantageous over the $1/N$-expansion, for example by improved behavior with respect to the universal approximation theorem. Given the connected correlators of a field theory, one can systematically reconstruct the action order-by-order in the expansion parameter, using a new Feynman diagram prescription whose vertices are the connected correlators. This method is motivated by the Edgeworth expansion and allows one to derive actions for neural network field theories. Conversely, the co
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#23545;&#25239;&#27169;&#22411;&#65288;QUAM&#65289;&#26469;&#26356;&#22909;&#22320;&#20272;&#35745;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#12290;QUAM&#35782;&#21035;&#25972;&#20010;&#31215;&#20998;&#19979;&#20056;&#31215;&#36739;&#22823;&#30340;&#21306;&#22495;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#21518;&#39564;&#12290;&#19982;&#20808;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;QUAM&#23545;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#30340;&#36817;&#20284;&#35823;&#24046;&#26356;&#23567;&#12290;</title><link>http://arxiv.org/abs/2307.03217</link><description>&lt;p&gt;
&#20351;&#29992;&#23545;&#25239;&#27169;&#22411;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Quantification of Uncertainty with Adversarial Models. (arXiv:2307.03217v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03217
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#23545;&#25239;&#27169;&#22411;&#65288;QUAM&#65289;&#26469;&#26356;&#22909;&#22320;&#20272;&#35745;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#12290;QUAM&#35782;&#21035;&#25972;&#20010;&#31215;&#20998;&#19979;&#20056;&#31215;&#36739;&#22823;&#30340;&#21306;&#22495;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#21518;&#39564;&#12290;&#19982;&#20808;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;QUAM&#23545;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#30340;&#36817;&#20284;&#35823;&#24046;&#26356;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#23545;&#20110;&#21487;&#25805;&#20316;&#30340;&#39044;&#27979;&#38750;&#24120;&#37325;&#35201;&#12290;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#20851;&#38190;&#22312;&#20110;&#20272;&#35745;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#65292;&#23427;&#34987;&#23450;&#20041;&#20026;&#19968;&#20010;&#25955;&#24230;&#20989;&#25968;&#21644;&#21518;&#39564;&#30340;&#20056;&#31215;&#30340;&#31215;&#20998;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#22914;Deep Ensembles&#25110;MC dropout&#22312;&#20272;&#35745;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#22240;&#20026;&#23427;&#20204;&#20027;&#35201;&#32771;&#34385;&#21518;&#39564;&#22312;&#37319;&#26679;&#27169;&#22411;&#26102;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#23545;&#25239;&#27169;&#22411;&#65288;QUAM&#65289;&#26469;&#26356;&#22909;&#22320;&#20272;&#35745;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#12290;QUAM&#35782;&#21035;&#25972;&#20010;&#31215;&#20998;&#19979;&#20056;&#31215;&#36739;&#22823;&#30340;&#21306;&#22495;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#21518;&#39564;&#12290;&#22240;&#27492;&#65292;&#19982;&#20808;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;QUAM&#23545;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#30340;&#36817;&#20284;&#35823;&#24046;&#26356;&#23567;&#12290;&#20056;&#31215;&#36739;&#22823;&#30340;&#27169;&#22411;&#23545;&#24212;&#20110;&#23545;&#25239;&#27169;&#22411;&#65288;&#19981;&#26159;&#23545;&#25239;&#24615;&#31034;&#20363;&#65281;&#65289;&#12290;&#23545;&#25239;&#27169;&#22411;&#26082;&#26377;&#36739;&#39640;&#30340;&#21518;&#39564;&#65292;&#20063;&#26377;&#20854;&#39044;&#27979;&#19982;&#20854;&#20182;&#27169;&#22411;&#20043;&#38388;&#30340;&#36739;&#39640;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantifying uncertainty is important for actionable predictions in real-world applications. A crucial part of predictive uncertainty quantification is the estimation of epistemic uncertainty, which is defined as an integral of the product between a divergence function and the posterior. Current methods such as Deep Ensembles or MC dropout underperform at estimating the epistemic uncertainty, since they primarily consider the posterior when sampling models. We suggest Quantification of Uncertainty with Adversarial Models (QUAM) to better estimate the epistemic uncertainty. QUAM identifies regions where the whole product under the integral is large, not just the posterior. Consequently, QUAM has lower approximation error of the epistemic uncertainty compared to previous methods. Models for which the product is large correspond to adversarial models (not adversarial examples!). Adversarial models have both a high posterior as well as a high divergence between their predictions and that of
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21306;&#22495;&#20851;&#27880;&#30340;&#22810;&#35270;&#35282;&#34920;&#31034;&#23398;&#20064;&#65288;ROMER&#65289;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#25429;&#25417;&#22810;&#35270;&#35282;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#23398;&#20064;&#22478;&#24066;&#21306;&#22495;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#22312;&#22810;&#28304;&#22478;&#24066;&#25968;&#25454;&#20013;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.03212</link><description>&lt;p&gt;
&#22522;&#20110;&#21306;&#22495;&#20851;&#27880;&#30340;&#22810;&#35270;&#35282;&#34920;&#31034;&#23398;&#20064;&#29992;&#20110;&#22478;&#24066;&#21306;&#22495;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Region-Wise Attentive Multi-View Representation Learning for Urban Region Embeddings. (arXiv:2307.03212v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03212
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21306;&#22495;&#20851;&#27880;&#30340;&#22810;&#35270;&#35282;&#34920;&#31034;&#23398;&#20064;&#65288;ROMER&#65289;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#25429;&#25417;&#22810;&#35270;&#35282;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#23398;&#20064;&#22478;&#24066;&#21306;&#22495;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#22312;&#22810;&#28304;&#22478;&#24066;&#25968;&#25454;&#20013;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22478;&#24066;&#21306;&#22495;&#23884;&#20837;&#26159;&#19968;&#20010;&#37325;&#35201;&#19988;&#20855;&#26377;&#39640;&#24230;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#30001;&#20110;&#22478;&#24066;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#21644;&#19981;&#26029;&#21464;&#21270;&#30340;&#24615;&#36136;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21306;&#22495;&#20851;&#27880;&#30340;&#22810;&#35270;&#35282;&#34920;&#31034;&#23398;&#20064;&#65288;ROMER&#65289;&#65292;&#20197;&#25429;&#25417;&#22810;&#35270;&#35282;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#23398;&#20064;&#22478;&#24066;&#21306;&#22495;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#32780;&#19981;&#21463;&#21018;&#24615;&#37051;&#22495;&#26465;&#20214;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#19987;&#27880;&#20110;&#20174;&#22810;&#28304;&#22478;&#24066;&#25968;&#25454;&#20013;&#23398;&#20064;&#22478;&#24066;&#21306;&#22495;&#34920;&#31034;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20174;&#31227;&#21160;&#27969;&#27169;&#24335;&#12289;POI&#35821;&#20041;&#21644;&#31614;&#21040;&#21160;&#24577;&#20013;&#25429;&#25417;&#22810;&#35270;&#35282;&#30340;&#30456;&#20851;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#20840;&#23616;&#22270;&#27880;&#24847;&#32593;&#32476;&#26469;&#23398;&#20064;&#22270;&#20013;&#20219;&#24847;&#20004;&#20010;&#39030;&#28857;&#30340;&#30456;&#20284;&#24615;&#12290;&#20026;&#20102;&#20840;&#38754;&#32771;&#34385;&#21644;&#20849;&#20139;&#22810;&#20010;&#35270;&#35282;&#30340;&#29305;&#24449;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#34701;&#21512;&#27169;&#22359;&#65292;&#21033;&#29992;&#22806;&#37096;&#27880;&#24847;&#21147;&#23398;&#20064;&#26435;&#37325;&#26469;&#34701;&#21512;&#22810;&#35270;&#35282;&#23884;&#20837;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#20004;&#20010;&#19979;&#28216;&#20219;&#21153;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Urban region embedding is an important and yet highly challenging issue due to the complexity and constantly changing nature of urban data. To address the challenges, we propose a Region-Wise Multi-View Representation Learning (ROMER) to capture multi-view dependencies and learn expressive representations of urban regions without the constraints of rigid neighbourhood region conditions. Our model focus on learn urban region representation from multi-source urban data. First, we capture the multi-view correlations from mobility flow patterns, POI semantics and check-in dynamics. Then, we adopt global graph attention networks to learn similarity of any two vertices in graphs. To comprehensively consider and share features of multiple views, a two-stage fusion module is further proposed to learn weights with external attention to fuse multi-view embeddings. Extensive experiments for two downstream tasks on real-world datasets demonstrate that our model outperforms state-of-the-art methods
&lt;/p&gt;</description></item><item><title>PseudoCell&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#30446;&#26631;&#26816;&#27979;&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#21160;&#21270;&#20013;&#24515;&#27597;&#32454;&#32990;&#26816;&#27979;&#12290;&#23427;&#36890;&#36807;&#23558;&#30149;&#29702;&#23398;&#23478;&#30340;&#20013;&#24515;&#27597;&#32454;&#32990;&#26631;&#31614;&#19982;&#20351;&#29992;&#32454;&#32990;&#24418;&#24577;&#29305;&#24449;&#23545;&#20302;&#37319;&#26679;&#20551;&#38451;&#24615;&#39044;&#27979;&#24471;&#21040;&#30340;&#20266;&#38452;&#24615;&#26631;&#31614;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#32553;&#23567;&#38656;&#35201;&#26816;&#26597;&#32452;&#32455;&#30340;&#21306;&#22495;&#12290;</title><link>http://arxiv.org/abs/2307.03211</link><description>&lt;p&gt;
PseudoCell: &#23558;&#30828;&#36127;&#26679;&#26412;&#25366;&#25496;&#20316;&#20026;&#20266;&#26631;&#31614;&#29992;&#20110;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20013;&#24515;&#27597;&#32454;&#32990;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
PseudoCell: Hard Negative Mining as Pseudo Labeling for Deep Learning-Based Centroblast Cell Detection. (arXiv:2307.03211v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03211
&lt;/p&gt;
&lt;p&gt;
PseudoCell&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#30446;&#26631;&#26816;&#27979;&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#21160;&#21270;&#20013;&#24515;&#27597;&#32454;&#32990;&#26816;&#27979;&#12290;&#23427;&#36890;&#36807;&#23558;&#30149;&#29702;&#23398;&#23478;&#30340;&#20013;&#24515;&#27597;&#32454;&#32990;&#26631;&#31614;&#19982;&#20351;&#29992;&#32454;&#32990;&#24418;&#24577;&#29305;&#24449;&#23545;&#20302;&#37319;&#26679;&#20551;&#38451;&#24615;&#39044;&#27979;&#24471;&#21040;&#30340;&#20266;&#38452;&#24615;&#26631;&#31614;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#32553;&#23567;&#38656;&#35201;&#26816;&#26597;&#32452;&#32455;&#30340;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#34917;&#19969;&#20998;&#31867;&#27169;&#22411;&#24050;&#34987;&#24212;&#29992;&#20110;H&amp;E&#26579;&#33394;&#32452;&#32455;&#26679;&#26412;&#30340;&#25972;&#20010;&#20999;&#29255;&#22270;&#20687;&#20013;&#65292;&#20197;&#24110;&#21161;&#30149;&#29702;&#23398;&#23478;&#23545;&#28388;&#27873;&#24615;&#28107;&#24052;&#30244;&#24739;&#32773;&#36827;&#34892;&#35780;&#32423;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20173;&#28982;&#38656;&#35201;&#30149;&#29702;&#23398;&#23478;&#25163;&#21160;&#35782;&#21035;&#20013;&#24515;&#27597;&#32454;&#32990;&#24182;&#25552;&#20379;&#31934;&#32454;&#21270;&#26631;&#31614;&#20197;&#23454;&#29616;&#26368;&#20339;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PseudoCell&#65292;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#21270;&#20013;&#24515;&#27597;&#32454;&#32990;&#26816;&#27979;&#30340;&#30446;&#26631;&#26816;&#27979;&#26694;&#26550;&#65288;&#28304;&#20195;&#30721;&#21487;&#22312;https://github.com/IoBT-VISTEC/PseudoCell.git&#25214;&#21040;&#65289;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#23558;&#30149;&#29702;&#23398;&#23478;&#30340;&#20013;&#24515;&#27597;&#32454;&#32990;&#26631;&#31614;&#19982;&#20351;&#29992;&#32454;&#32990;&#24418;&#24577;&#29305;&#24449;&#23545;&#20302;&#37319;&#26679;&#20551;&#38451;&#24615;&#39044;&#27979;&#24471;&#21040;&#30340;&#20266;&#38452;&#24615;&#26631;&#31614;&#30456;&#32467;&#21512;&#12290;&#36890;&#36807;&#20351;&#29992;PseudoCell&#65292;&#21487;&#20197;&#20943;&#36731;&#30149;&#29702;&#23398;&#23478;&#30340;&#24037;&#20316;&#36127;&#25285;&#65292;&#22240;&#20026;&#23427;&#20934;&#30830;&#22320;&#32553;&#23567;&#20102;&#38656;&#35201;&#26816;&#26597;&#32452;&#32455;&#30340;&#21306;&#22495;&#12290;&#26681;&#25454;&#32622;&#20449;&#38408;&#20540;&#65292;PseudoCell&#21487;&#20197;&#22312;WSI&#19978;&#28040;&#38500;58.18-99.35%&#30340;&#38750;&#20013;&#24515;&#27597;&#32454;&#32990;&#32452;&#32455;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Patch classification models based on deep learning have been utilized in whole-slide images (WSI) of H&amp;E-stained tissue samples to assist pathologists in grading follicular lymphoma patients. However, these approaches still require pathologists to manually identify centroblast cells and provide refined labels for optimal performance. To address this, we propose PseudoCell, an object detection framework to automate centroblast detection in WSI (source code is available at https://github.com/IoBT-VISTEC/PseudoCell.git). This framework incorporates centroblast labels from pathologists and combines them with pseudo-negative labels obtained from undersampled false-positive predictions using the cell's morphological features. By employing PseudoCell, pathologists' workload can be reduced as it accurately narrows down the areas requiring their attention during examining tissue. Depending on the confidence threshold, PseudoCell can eliminate 58.18-99.35% of non-centroblasts tissue areas on WSI
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#22270;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#21033;&#29992;&#22270;&#24418;&#35270;&#35282;&#26469;&#23398;&#20064;&#27169;&#22411;&#21442;&#25968;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#24182;&#25552;&#20379;&#20102;&#27010;&#29575;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#23398;&#20064;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.03210</link><description>&lt;p&gt;
&#31232;&#30095;&#22270;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Sparse Graphical Linear Dynamical Systems. (arXiv:2307.03210v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03210
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#22270;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#21033;&#29992;&#22270;&#24418;&#35270;&#35282;&#26469;&#23398;&#20064;&#27169;&#22411;&#21442;&#25968;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#24182;&#25552;&#20379;&#20102;&#27010;&#29575;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#23398;&#20064;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#22312;&#31185;&#23398;&#21644;&#24037;&#31243;&#30340;&#20247;&#22810;&#39046;&#22495;&#20013;&#25198;&#28436;&#37325;&#35201;&#35282;&#33394;&#65292;&#22914;&#29983;&#29289;&#21307;&#23398;&#12289;&#22320;&#29699;&#35266;&#27979;&#21644;&#32593;&#32476;&#20998;&#26512;&#12290;&#26377;&#20851;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#30340;&#24191;&#27867;&#30740;&#31350;&#26159;&#19968;&#20010;&#24378;&#22823;&#30340;&#25968;&#23398;&#24037;&#20855;&#65292;&#23427;&#20801;&#35768;&#23545;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#27010;&#29575;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#23398;&#20064;&#12290;&#22312;SSMs&#20013;&#20272;&#35745;&#27169;&#22411;&#21442;&#25968;&#34987;&#35748;&#20026;&#26159;&#26368;&#22797;&#26434;&#30340;&#20219;&#21153;&#20043;&#19968;&#65292;&#20808;&#39564;&#30693;&#35782;&#30340;&#24341;&#20837;&#26082;&#21487;&#20197;&#31616;&#21270;&#35299;&#37322;&#20063;&#21487;&#20197;&#22797;&#26434;&#21270;&#25512;&#26029;&#20219;&#21153;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24037;&#20316;&#35797;&#22270;&#22312;&#19968;&#20123;&#27169;&#22411;&#21442;&#25968;&#20013;&#21152;&#20837;&#22270;&#24418;&#35270;&#35282;&#65292;&#20294;&#23384;&#22312;&#26126;&#26174;&#30340;&#38480;&#21046;&#65292;&#36825;&#39033;&#24037;&#20316;&#35299;&#20915;&#20102;&#36825;&#20123;&#38480;&#21046;&#12290;&#26356;&#19968;&#33324;&#22320;&#35828;&#65292;&#29616;&#26377;&#30340;&#22270;&#27169;&#22411;&#24037;&#20855;&#26088;&#22312;&#21253;&#21547;&#38745;&#24577;&#20449;&#24687;&#25110;&#21160;&#24577;&#20449;&#24687;&#65292;&#20854;&#20013;&#38745;&#24577;&#20449;&#24687;&#20391;&#37325;&#20110;&#29420;&#31435;&#38543;&#26426;&#21464;&#37327;&#20043;&#38388;&#30340;&#32479;&#35745;&#20381;&#36182;&#65288;&#20363;&#22914;&#65292;&#22270;&#24418;Lasso&#26041;&#27861;&#65289;&#65292;&#21160;&#24577;&#20449;&#24687;&#20391;&#37325;&#20110;&#26102;&#38388;&#24207;&#21015;&#26679;&#26412;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time-series datasets are central in numerous fields of science and engineering, such as biomedicine, Earth observation, and network analysis. Extensive research exists on state-space models (SSMs), which are powerful mathematical tools that allow for probabilistic and interpretable learning on time series. Estimating the model parameters in SSMs is arguably one of the most complicated tasks, and the inclusion of prior knowledge is known to both ease the interpretation but also to complicate the inferential tasks. Very recent works have attempted to incorporate a graphical perspective on some of those model parameters, but they present notable limitations that this work addresses. More generally, existing graphical modeling tools are designed to incorporate either static information, focusing on statistical dependencies among independent random variables (e.g., graphical Lasso approach), or dynamic information, emphasizing causal relationships among time series samples (e.g., graphical 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;DENCLUE&#31639;&#27861;&#26368;&#20248;&#21442;&#25968;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#22312;&#23454;&#39564;&#37096;&#20998;&#35752;&#35770;&#20102;&#20854;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.03206</link><description>&lt;p&gt;
DENCLUE&#30340;&#26368;&#20248;&#24102;&#23485;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Optimal Bandwidth Selection for DENCLUE. (arXiv:2307.03206v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03206
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;DENCLUE&#31639;&#27861;&#26368;&#20248;&#21442;&#25968;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#22312;&#23454;&#39564;&#37096;&#20998;&#35752;&#35770;&#20102;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#24037;&#19994;&#20013;&#65292;&#32858;&#31867;&#31639;&#27861;&#26159;&#31639;&#27861;&#24037;&#31243;&#24072;&#30340;&#26085;&#24120;&#24037;&#20316;&#12290;&#23613;&#31649;&#22312;2010&#24180;&#20043;&#21069;&#65292;&#32858;&#31867;&#31639;&#27861;&#32463;&#21382;&#20102;&#24555;&#36895;&#22686;&#38271;&#65292;&#20294;&#22312;&#28145;&#24230;&#23398;&#20064;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;&#23454;&#38469;&#24037;&#19994;&#26631;&#20934;&#20043;&#21518;&#65292;&#19982;&#35813;&#30740;&#31350;&#20027;&#39064;&#30456;&#20851;&#30340;&#21019;&#26032;&#20572;&#28382;&#19981;&#21069;&#12290;2007&#24180;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DENCLUE&#30340;&#22522;&#20110;&#23494;&#24230;&#30340;&#32858;&#31867;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38750;&#32447;&#24615;&#25968;&#25454;&#32467;&#26500;&#30340;&#32858;&#31867;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30452;&#21040;2011&#24180;&#65292;&#35813;&#31639;&#27861;&#30340;&#21442;&#25968;&#36873;&#25321;&#38382;&#39064;&#20173;&#28982;&#34987;&#22823;&#37096;&#20998;&#24573;&#35270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;DENCLUE&#31639;&#27861;&#26368;&#20248;&#21442;&#25968;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#22312;&#23454;&#39564;&#37096;&#20998;&#35752;&#35770;&#20102;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In modern day industry, clustering algorithms are daily routines of algorithm engineers. Although clustering algorithms experienced rapid growth before 2010. Innovation related to the research topic has stagnated after deep learning became the de facto industrial standard for machine learning applications. In 2007, a density-based clustering algorithm named DENCLUE was invented to solve clustering problem for nonlinear data structures. However, its parameter selection problem was largely neglected until 2011. In this paper, we propose a new approach to compute the optimal parameters for the DENCLUE algorithm, and discuss its performance in the experiment section.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#32553;&#25918;&#23450;&#24459;&#19982;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#25351;&#20986;&#25968;&#25454;&#38598;&#35268;&#27169;&#30340;&#22686;&#21152;&#20250;&#24341;&#21457;&#19981;&#21516;&#31038;&#32676;&#30340;&#20215;&#20540;&#35266;&#21644;&#20559;&#35265;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2307.03201</link><description>&lt;p&gt;
&#32553;&#25918;&#23450;&#24459;&#19981;&#20855;&#22791;&#21487;&#25193;&#23637;&#24615;
&lt;/p&gt;
&lt;p&gt;
Scaling Laws Do Not Scale. (arXiv:2307.03201v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03201
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#32553;&#25918;&#23450;&#24459;&#19982;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#25351;&#20986;&#25968;&#25454;&#38598;&#35268;&#27169;&#30340;&#22686;&#21152;&#20250;&#24341;&#21457;&#19981;&#21516;&#31038;&#32676;&#30340;&#20215;&#20540;&#35266;&#21644;&#20559;&#35265;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#32553;&#25918;&#23450;&#24459;&#8221;&#30340;&#24130;&#24459;&#20851;&#31995;&#65292;&#23427;&#25551;&#36848;&#20102;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#27169;&#22411;&#35774;&#35745;&#30340;&#21508;&#20010;&#26041;&#38754;&#65288;&#22914;&#25968;&#25454;&#38598;&#22823;&#23567;&#65289;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#38543;&#30528;&#25968;&#25454;&#38598;&#65288;&#25110;&#27169;&#22411;&#21442;&#25968;&#31561;&#65289;&#30340;&#22686;&#21152;&#65292;&#22522;&#20110;&#35813;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#23558;&#30456;&#24212;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#22312;&#24635;&#20307;&#19978;&#20855;&#26377;&#21560;&#24341;&#21147;&#30340;&#21516;&#26102;&#65292;&#36825;&#31181;&#32553;&#25918;&#23450;&#24459;&#20851;&#31995;&#24573;&#35270;&#20102;&#29992;&#20110;&#34913;&#37327;&#24615;&#33021;&#30340;&#25351;&#26631;&#21487;&#33021;&#26159;&#19981;&#31283;&#23450;&#21644;&#26377;&#20105;&#35758;&#30340;&#65292;&#25110;&#32773;&#21487;&#33021;&#19981;&#31526;&#21512;&#19981;&#21516;&#20154;&#32676;&#23545;&#27169;&#22411;&#36755;&#20986;&#36136;&#37327;&#30340;&#24863;&#30693;&#12290;&#26412;&#25991;&#25552;&#20986;&#65292;&#38543;&#30528;&#29992;&#20110;&#35757;&#32451;&#22823;&#22411;AI&#27169;&#22411;&#30340;&#25968;&#25454;&#38598;&#35268;&#27169;&#22686;&#38271;&#65292;&#25968;&#25454;&#38598;&#20013;&#21253;&#21547;&#30340;&#19981;&#21516;&#31038;&#32676;&#65288;&#21253;&#25324;&#20154;&#21475;&#32479;&#35745;&#23398;&#32676;&#20307;&#65289;&#30340;&#25968;&#37327;&#21487;&#33021;&#20250;&#22686;&#21152;&#65292;&#27599;&#20010;&#31038;&#32676;&#21487;&#33021;&#20855;&#26377;&#19981;&#21516;&#30340;&#20215;&#20540;&#35266;&#12290;&#22240;&#27492;&#65292;&#25968;&#25454;&#38598;&#20013;&#25152;&#20195;&#34920;&#30340;&#31038;&#32676;&#21487;&#33021;&#23384;&#22312;&#20215;&#20540;&#35266;&#25110;&#20559;&#35265;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has proposed a power law relationship, referred to as ``scaling laws,'' between the performance of artificial intelligence (AI) models and aspects of those models' design (e.g., dataset size). In other words, as the size of a dataset (or model parameters, etc) increases, the performance of a given model trained on that dataset will correspondingly increase. However, while compelling in the aggregate, this scaling law relationship overlooks the ways that metrics used to measure performance may be precarious and contested, or may not correspond with how different groups of people may perceive the quality of models' output. In this paper, we argue that as the size of datasets used to train large AI models grows, the number of distinct communities (including demographic groups) whose data is included in a given dataset is likely to grow, each of whom may have different values. As a result, there is an increased risk that communities represented in a dataset may have values or p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;SplitFed Learning&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#36827;&#34892;&#20102;&#30740;&#31350;&#21644;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#30340;&#25915;&#20987;&#31574;&#30053;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#25915;&#20987;&#31574;&#30053;&#21487;&#20197;&#38477;&#20302;&#22522;&#20110;DCML&#30340;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.03197</link><description>&lt;p&gt;
&#20998;&#26512;SplitFed Learning&#20013;&#30340;&#28431;&#27934;&#65306;&#35780;&#20272;&#20854;&#23545;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Analyzing the vulnerabilities in SplitFed Learning: Assessing the robustness against Data Poisoning Attacks. (arXiv:2307.03197v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03197
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;SplitFed Learning&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#36827;&#34892;&#20102;&#30740;&#31350;&#21644;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#30340;&#25915;&#20987;&#31574;&#30053;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#25915;&#20987;&#31574;&#30053;&#21487;&#20197;&#38477;&#20302;&#22522;&#20110;DCML&#30340;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#21327;&#20316;&#26426;&#22120;&#23398;&#20064;&#65288;DCML&#65289;&#26159;&#35299;&#20915;&#38598;&#20013;&#24335;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#30340;&#19968;&#31181;&#28508;&#22312;&#26367;&#20195;&#26041;&#26696;&#12290;Split learning&#65288;SL&#65289;&#21644;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;DCML&#20013;&#20004;&#31181;&#26377;&#25928;&#30340;&#23398;&#20064;&#26041;&#27861;&#12290;&#26368;&#36817;&#20154;&#20204;&#23545;FL&#21644;SL&#30340;&#28151;&#21512;&#24418;&#24335;SplitFed Learning&#65288;SFL&#65289;&#20135;&#29983;&#20102;&#36739;&#22823;&#20852;&#36259;&#12290;&#26412;&#30740;&#31350;&#26159;&#23545;SFL&#20013;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#36827;&#34892;&#30740;&#31350;&#12289;&#20998;&#26512;&#21644;&#24433;&#21709;&#35780;&#20272;&#30340;&#26368;&#26089;&#23581;&#35797;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#30340;&#25915;&#20987;&#31574;&#30053;&#65292;&#20998;&#21035;&#26159;&#38750;&#30446;&#26631;&#25915;&#20987;&#12289;&#26377;&#30446;&#26631;&#25915;&#20987;&#21644;&#22522;&#20110;&#36317;&#31163;&#30340;&#25915;&#20987;&#65292;&#29992;&#20110;SFL&#12290;&#25152;&#26377;&#25915;&#20987;&#31574;&#30053;&#26088;&#22312;&#38477;&#20302;&#22522;&#20110;DCML&#30340;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23545;&#24515;&#30005;&#22270;&#20449;&#21495;&#20998;&#31867;&#21644;&#25163;&#20889;&#25968;&#23383;&#35782;&#21035;&#36825;&#20004;&#20010;&#19981;&#21516;&#26696;&#20363;&#36827;&#34892;&#20102;&#25915;&#20987;&#23454;&#39564;&#65292;&#22312;&#24694;&#24847;&#23458;&#25143;&#31471;&#30340;&#30334;&#20998;&#27604;&#21644;&#27169;&#22411;&#25286;&#20998;&#23618;&#30340;&#36873;&#25321;&#26041;&#38754;&#36827;&#34892;&#20102;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed Collaborative Machine Learning (DCML) is a potential alternative to address the privacy concerns associated with centralized machine learning. The Split learning (SL) and Federated Learning (FL) are the two effective learning approaches in DCML. Recently there have been an increased interest on the hybrid of FL and SL known as the SplitFed Learning (SFL). This research is the earliest attempt to study, analyze and present the impact of data poisoning attacks in SFL. We propose three kinds of novel attack strategies namely untargeted, targeted and distance-based attacks for SFL. All the attacks strategies aim to degrade the performance of the DCML-based classifier. We test the proposed attack strategies for two different case studies on Electrocardiogram signal classification and automatic handwritten digit recognition. A series of attack experiments were conducted by varying the percentage of malicious clients and the choice of the model split layer between the clients and 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#31995;&#32479;&#65292;&#22312;&#20154;&#31867;&#19987;&#23478;&#20013;&#28155;&#21152;&#20154;&#24037;&#26234;&#33021;&#65292;&#20174;&#20043;&#21069;&#30001;&#20154;&#31867;&#19987;&#23478;&#23457;&#26597;&#36807;&#30340;&#26410;&#30693;&#31867;&#21035;&#30340;&#25968;&#25454;&#23454;&#20363;&#20013;&#23398;&#20064;&#20998;&#31867;&#65292;&#20197;&#25552;&#39640;&#20154;&#26426;&#21327;&#21516;&#31995;&#32479;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.03003</link><description>&lt;p&gt;
&#25552;&#39640;&#20154;&#26426;&#21327;&#21516;&#31995;&#32479;&#30340;&#25928;&#29575;: &#22312;&#20154;&#31867;&#19987;&#23478;&#20013;&#28155;&#21152;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Improving the Efficiency of Human-in-the-Loop Systems: Adding Artificial to Human Experts. (arXiv:2307.03003v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03003
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#31995;&#32479;&#65292;&#22312;&#20154;&#31867;&#19987;&#23478;&#20013;&#28155;&#21152;&#20154;&#24037;&#26234;&#33021;&#65292;&#20174;&#20043;&#21069;&#30001;&#20154;&#31867;&#19987;&#23478;&#23457;&#26597;&#36807;&#30340;&#26410;&#30693;&#31867;&#21035;&#30340;&#25968;&#25454;&#23454;&#20363;&#20013;&#23398;&#20064;&#20998;&#31867;&#65292;&#20197;&#25552;&#39640;&#20154;&#26426;&#21327;&#21516;&#31995;&#32479;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#31995;&#32479;&#36234;&#26469;&#36234;&#22810;&#22320;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#20174;&#22823;&#37327;&#25968;&#25454;&#20013;&#29983;&#25104;&#20215;&#20540;&#12290;&#28982;&#32780;&#65292;ML&#27169;&#22411;&#24182;&#19981;&#23436;&#32654;&#65292;&#21487;&#33021;&#20250;&#20135;&#29983;&#38169;&#35823;&#30340;&#20998;&#31867;&#12290;&#22240;&#27492;&#65292;&#20154;&#26426;&#21327;&#21516;&#65288;HITL&#65289;&#25193;&#23637;&#20102;ML&#27169;&#22411;&#65292;&#20026;&#38590;&#20197;&#20998;&#31867;&#30340;&#23454;&#20363;&#28155;&#21152;&#20102;&#20154;&#24037;&#23457;&#26680;&#12290;&#26412;&#30740;&#31350;&#35748;&#20026;&#65292;&#25345;&#32493;&#20381;&#36182;&#20154;&#31867;&#19987;&#23478;&#22788;&#29702;&#22256;&#38590;&#30340;&#27169;&#22411;&#20998;&#31867;&#20250;&#23548;&#33268;&#20154;&#21147;&#25237;&#20837;&#30340;&#22823;&#24133;&#22686;&#21152;&#65292;&#22686;&#21152;&#20102;&#26377;&#38480;&#36164;&#28304;&#30340;&#21387;&#21147;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#31995;&#32479;&#65292;&#21019;&#24314;&#20102;&#20154;&#24037;&#19987;&#23478;&#65292;&#20174;&#20043;&#21069;&#30001;&#20154;&#31867;&#19987;&#23478;&#23457;&#26597;&#36807;&#30340;&#26410;&#30693;&#31867;&#21035;&#30340;&#25968;&#25454;&#23454;&#20363;&#20013;&#23398;&#20064;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#28151;&#21512;&#31995;&#32479;&#35780;&#20272;&#21738;&#20010;&#20154;&#24037;&#19987;&#23478;&#36866;&#21512;&#20998;&#31867;&#26469;&#33258;&#26410;&#30693;&#31867;&#21035;&#30340;&#23454;&#20363;&#65292;&#24182;&#33258;&#21160;&#20998;&#37197;&#12290;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#65292;&#36825;&#20943;&#23569;&#20102;&#20154;&#21147;&#25237;&#20837;&#65292;&#25552;&#39640;&#20102;&#31995;&#32479;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#30340;HITL&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Information systems increasingly leverage artificial intelligence (AI) and machine learning (ML) to generate value from vast amounts of data. However, ML models are imperfect and can generate incorrect classifications. Hence, human-in-the-loop (HITL) extensions to ML models add a human review for instances that are difficult to classify. This study argues that continuously relying on human experts to handle difficult model classifications leads to a strong increase in human effort, which strains limited resources. To address this issue, we propose a hybrid system that creates artificial experts that learn to classify data instances from unknown classes previously reviewed by human experts. Our hybrid system assesses which artificial expert is suitable for classifying an instance from an unknown class and automatically assigns it. Over time, this reduces human effort and increases the efficiency of the system. Our experiments demonstrate that our approach outperforms traditional HITL sy
&lt;/p&gt;</description></item><item><title>&#24377;&#24615;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;EDT&#65289;&#36890;&#36807;&#22312;&#27979;&#35797;&#26102;&#38388;&#36827;&#34892;&#21160;&#20316;&#25512;&#26029;&#26102;&#35843;&#25972;&#21382;&#21490;&#38271;&#24230;&#26469;&#23454;&#29616;&#36712;&#36857;&#25340;&#25509;&#65292;&#22635;&#34917;&#20102;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;DT&#65289;&#22312;&#36825;&#19968;&#26041;&#38754;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#24182;&#19988;&#22312;&#22810;&#20219;&#21153;&#24773;&#20917;&#19979;&#32988;&#36807;&#22522;&#20110;Q-Learning&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.02484</link><description>&lt;p&gt;
&#24377;&#24615;&#20915;&#31574;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
Elastic Decision Transformer. (arXiv:2307.02484v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02484
&lt;/p&gt;
&lt;p&gt;
&#24377;&#24615;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;EDT&#65289;&#36890;&#36807;&#22312;&#27979;&#35797;&#26102;&#38388;&#36827;&#34892;&#21160;&#20316;&#25512;&#26029;&#26102;&#35843;&#25972;&#21382;&#21490;&#38271;&#24230;&#26469;&#23454;&#29616;&#36712;&#36857;&#25340;&#25509;&#65292;&#22635;&#34917;&#20102;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;DT&#65289;&#22312;&#36825;&#19968;&#26041;&#38754;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#24182;&#19988;&#22312;&#22810;&#20219;&#21153;&#24773;&#20917;&#19979;&#32988;&#36807;&#22522;&#20110;Q-Learning&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#24377;&#24615;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;EDT&#65289;&#65292;&#23427;&#26159;&#29616;&#26377;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;DT&#65289;&#21450;&#20854;&#21464;&#20307;&#30340;&#37325;&#22823;&#36827;&#23637;&#12290;&#23613;&#31649;DT&#22768;&#31216;&#33021;&#22815;&#29983;&#25104;&#26368;&#20339;&#36712;&#36857;&#65292;&#20294;&#23454;&#35777;&#35777;&#25454;&#34920;&#26126;&#23427;&#22312;&#36712;&#36857;&#25340;&#25509;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#36712;&#36857;&#25340;&#25509;&#26159;&#25351;&#20174;&#19968;&#32452;&#27425;&#20248;&#36712;&#36857;&#20013;&#29983;&#25104;&#26368;&#20248;&#25110;&#25509;&#36817;&#26368;&#20248;&#36712;&#36857;&#30340;&#36807;&#31243;&#12290;&#25552;&#20986;&#30340;EDT&#36890;&#36807;&#22312;&#27979;&#35797;&#26102;&#38388;&#36827;&#34892;&#21160;&#20316;&#25512;&#26029;&#26102;&#35843;&#25972;DT&#20013;&#32500;&#25252;&#30340;&#21382;&#21490;&#38271;&#24230;&#26469;&#23454;&#29616;&#36712;&#36857;&#25340;&#25509;&#65292;&#20174;&#32780;&#20351;&#33258;&#24049;&#19982;&#20247;&#19981;&#21516;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#36712;&#36857;&#26159;&#26368;&#20248;&#30340;&#26102;&#20505;&#65292;EDT&#36890;&#36807;&#20445;&#25345;&#36739;&#38271;&#30340;&#21382;&#21490;&#65292;&#24403;&#24403;&#21069;&#36712;&#36857;&#26159;&#27425;&#20248;&#30340;&#26102;&#20505;&#65292;EDT&#36890;&#36807;&#20445;&#25345;&#36739;&#30701;&#30340;&#21382;&#21490;&#26469;&#20248;&#21270;&#36712;&#36857;&#65292;&#20351;&#20854;&#33021;&#22815;&#19982;&#26356;&#20248;&#30340;&#36712;&#36857;&#36827;&#34892;&#8220;&#25340;&#25509;&#8221;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;EDT&#33021;&#22815;&#22635;&#34917;&#22522;&#20110;DT&#21644;&#22522;&#20110;Q-Learning&#26041;&#27861;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#29305;&#21035;&#26159;&#65292;EDT&#22312;&#22810;&#20219;&#21153;&#24773;&#20917;&#19979;&#32988;&#36807;&#22522;&#20110;Q-Learning&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces Elastic Decision Transformer (EDT), a significant advancement over the existing Decision Transformer (DT) and its variants. Although DT purports to generate an optimal trajectory, empirical evidence suggests it struggles with trajectory stitching, a process involving the generation of an optimal or near-optimal trajectory from the best parts of a set of sub-optimal trajectories. The proposed EDT differentiates itself by facilitating trajectory stitching during action inference at test time, achieved by adjusting the history length maintained in DT. Further, the EDT optimizes the trajectory by retaining a longer history when the previous trajectory is optimal and a shorter one when it is sub-optimal, enabling it to "stitch" with a more optimal trajectory. Extensive experimentation demonstrates EDT's ability to bridge the performance gap between DT-based and Q Learning-based approaches. In particular, the EDT outperforms Q Learning-based methods in a multi-task regi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#22312;&#19981;&#21516;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#20043;&#38388;&#30340;&#36890;&#29992;&#24615;&#65292;&#24182;&#25506;&#35752;&#20102;&#22312;&#22810;&#20010;&#27169;&#22411;&#19978;&#21327;&#35843;&#29305;&#24449;&#24402;&#22240;&#30340;&#21487;&#34892;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#29305;&#24449;&#24402;&#22240;&#30340;&#21327;&#35843;&#26377;&#21161;&#20110;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#21644;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.02150</link><description>&lt;p&gt;
&#36328;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#23454;&#29616;&#29305;&#24449;&#24402;&#22240;&#30340;&#21327;&#35843;: &#25552;&#21319;&#35299;&#37322;&#24615;&#21644;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Harmonizing Feature Attributions Across Deep Learning Architectures: Enhancing Interpretability and Consistency. (arXiv:2307.02150v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02150
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#22312;&#19981;&#21516;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#20043;&#38388;&#30340;&#36890;&#29992;&#24615;&#65292;&#24182;&#25506;&#35752;&#20102;&#22312;&#22810;&#20010;&#27169;&#22411;&#19978;&#21327;&#35843;&#29305;&#24449;&#24402;&#22240;&#30340;&#21487;&#34892;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#29305;&#24449;&#24402;&#22240;&#30340;&#21327;&#35843;&#26377;&#21161;&#20110;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#21644;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#20445;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#23545;&#20110;&#20854;&#22312;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#24050;&#32463;&#24341;&#36215;&#20102;&#35768;&#22810;&#20851;&#27880;&#65292;&#23427;&#20204;&#36890;&#36807;&#23558;&#37325;&#35201;&#24615;&#24402;&#22240;&#32473;&#20010;&#21035;&#36755;&#20837;&#29305;&#24449;&#26469;&#25552;&#20379;&#27169;&#22411;&#39044;&#27979;&#30340;&#23616;&#37096;&#35299;&#37322;&#12290;&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#29305;&#24449;&#24402;&#22240;&#22312;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65288;&#22914;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#35270;&#35273;&#36716;&#25442;&#22120;&#65289;&#20043;&#38388;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#26088;&#22312;&#35780;&#20272;&#23558;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#20316;&#20026;&#26410;&#26469;&#26816;&#27979;&#22120;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#30740;&#31350;&#22914;&#20309;&#22312;&#37319;&#29992;&#19981;&#21516;&#26550;&#26500;&#20294;&#20197;&#30456;&#21516;&#25968;&#25454;&#20998;&#24067;&#35757;&#32451;&#30340;&#22810;&#20010;&#27169;&#22411;&#20043;&#38388;&#21327;&#35843;&#36825;&#20123;&#29305;&#24449;&#12290;&#36890;&#36807;&#25506;&#32034;&#36825;&#31181;&#21327;&#35843;&#65292;&#25105;&#20204;&#26088;&#22312;&#24320;&#21457;&#20986;&#26356;&#19968;&#33268;&#21644;&#20048;&#35266;&#30340;&#29305;&#24449;&#24402;&#22240;&#29702;&#35299;&#65292;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20043;&#38388;&#23616;&#37096;&#35299;&#37322;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#20984;&#26174;&#20102;&#23454;&#29616;&#29305;&#24449;&#24402;&#22240;&#21327;&#35843;&#24615;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ensuring the trustworthiness and interpretability of machine learning models is critical to their deployment in real-world applications. Feature attribution methods have gained significant attention, which provide local explanations of model predictions by attributing importance to individual input features. This study examines the generalization of feature attributions across various deep learning architectures, such as convolutional neural networks (CNNs) and vision transformers. We aim to assess the feasibility of utilizing a feature attribution method as a future detector and examine how these features can be harmonized across multiple models employing distinct architectures but trained on the same data distribution. By exploring this harmonization, we aim to develop a more coherent and optimistic understanding of feature attributions, enhancing the consistency of local explanations across diverse deep-learning models. Our findings highlight the potential for harmonized feature att
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31890;&#23376;&#36317;&#31163;&#30340;GAN&#31283;&#23450;&#24615;&#20998;&#26512;&#26694;&#26550;&#65292;&#24182;&#20351;&#29992;Wasserstein&#28176;&#21464;&#27969;&#23545;GAN&#30340;&#35757;&#32451;&#36807;&#31243;&#36827;&#34892;&#31283;&#23450;&#24615;&#20998;&#26512;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#30001;&#20110;GAN&#30340;&#30446;&#26631;&#20989;&#25968;&#24418;&#24335;&#30340;&#21407;&#22240;&#65292;&#21028;&#21035;&#22120;&#30340;&#35757;&#32451;&#36807;&#31243;&#36890;&#24120;&#26159;&#19981;&#31283;&#23450;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25991;&#20013;&#25552;&#20986;&#20102;&#19968;&#20010;&#31283;&#23450;&#30340;&#35757;&#32451;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.01879</link><description>&lt;p&gt;
&#22522;&#20110;&#31890;&#23376;&#36317;&#31163;&#30340;GAN&#31283;&#23450;&#24615;&#20998;&#26512;&#26694;&#26550;&#19982;Wasserstein&#28176;&#21464;&#27969;
&lt;/p&gt;
&lt;p&gt;
Stability Analysis Framework for Particle-based Distance GANs with Wasserstein Gradient Flow. (arXiv:2307.01879v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01879
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31890;&#23376;&#36317;&#31163;&#30340;GAN&#31283;&#23450;&#24615;&#20998;&#26512;&#26694;&#26550;&#65292;&#24182;&#20351;&#29992;Wasserstein&#28176;&#21464;&#27969;&#23545;GAN&#30340;&#35757;&#32451;&#36807;&#31243;&#36827;&#34892;&#31283;&#23450;&#24615;&#20998;&#26512;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#30001;&#20110;GAN&#30340;&#30446;&#26631;&#20989;&#25968;&#24418;&#24335;&#30340;&#21407;&#22240;&#65292;&#21028;&#21035;&#22120;&#30340;&#35757;&#32451;&#36807;&#31243;&#36890;&#24120;&#26159;&#19981;&#31283;&#23450;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25991;&#20013;&#25552;&#20986;&#20102;&#19968;&#20010;&#31283;&#23450;&#30340;&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#32593;&#32476;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#35813;&#32593;&#32476;&#23558;&#31890;&#23376;&#36317;&#31163;&#20316;&#20026;&#30446;&#26631;&#20989;&#25968;&#65292;&#20363;&#22914;MMD GAN&#65292;Cramer GAN&#21644;EIEG GAN&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;GAN&#24448;&#24448;&#23384;&#22312;&#35757;&#32451;&#19981;&#31283;&#23450;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#20174;&#27010;&#29575;&#23494;&#24230;&#21160;&#24577;&#30340;&#35282;&#24230;&#20998;&#26512;&#20102;&#36825;&#20123;GAN&#30340;&#35757;&#32451;&#36807;&#31243;&#30340;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#23558;GAN&#20013;&#30340;&#21028;&#21035;&#22120;D&#30475;&#20316;&#26159;&#19968;&#20010;&#29305;&#24449;&#36716;&#25442;&#26144;&#23556;&#65292;&#23427;&#23558;&#39640;&#32500;&#25968;&#25454;&#26144;&#23556;&#21040;&#19968;&#20010;&#29305;&#24449;&#31354;&#38388;&#65292;&#32780;&#29983;&#25104;&#22120;G&#21017;&#23558;&#38543;&#26426;&#21464;&#37327;&#26144;&#23556;&#21040;&#31867;&#20284;&#20110;&#30495;&#23454;&#25968;&#25454;&#30340;&#26679;&#26412;&#12290;&#22522;&#20110;&#36825;&#20010;&#35270;&#35282;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#30340;Wasserstein&#28176;&#21464;&#27969;&#26469;&#36827;&#34892;GAN&#30340;&#31283;&#23450;&#24615;&#20998;&#26512;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#30001;&#20110;GAN&#20013;$\min_G \max_D E(G, D)$&#30340;&#20844;&#24335;&#65292;&#21028;&#21035;&#22120;&#30340;&#35757;&#32451;&#36807;&#31243;&#36890;&#24120;&#26159;&#19981;&#31283;&#23450;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31283;&#23450;&#30340;&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate the training process of generative networks that use a type of probability density distance named particle-based distance as the objective function, e.g. MMD GAN, Cram\'er GAN, EIEG GAN. However, these GANs often suffer from the problem of unstable training. In this paper, we analyze the stability of the training process of these GANs from the perspective of probability density dynamics. In our framework, we regard the discriminator $D$ in these GANs as a feature transformation mapping that maps high dimensional data into a feature space, while the generator $G$ maps random variables to samples that resemble real data in terms of feature space. This perspective enables us to perform stability analysis for the training of GANs using the Wasserstein gradient flow of the probability density function. We find that the training process of the discriminator is usually unstable due to the formulation of $\min_G \max_D E(G, D)$ in GANs. To address this issue, we a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#29616;&#23454;&#19990;&#30028;&#31995;&#32479;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#21160;&#24577;&#65292;&#24182;&#19988;&#25429;&#25417;&#20102;&#23398;&#20064;&#31639;&#27861;&#19982;&#20854;&#24212;&#29992;&#30340;&#20998;&#24067;&#20043;&#38388;&#30340;&#21453;&#39304;&#24490;&#29615;&#12290;&#36890;&#36807;&#32806;&#21512;&#20559;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#65292;&#32771;&#34385;&#20102;&#25112;&#30053;&#24615;&#21453;&#24212;&#12289;&#38750;&#23616;&#37096;&#20869;&#29983;&#20154;&#21475;&#20114;&#21160;&#21644;&#20854;&#20182;&#22806;&#29983;&#20998;&#24067;&#20559;&#31227;&#26469;&#28304;&#65292;&#20197;&#23454;&#29616;&#23545;&#26102;&#38388;&#19978;&#32454;&#24494;&#21464;&#21270;&#30340;&#25429;&#25417;&#12290;&#30740;&#31350;&#35777;&#26126;&#20102;&#22312;&#21512;&#20316;&#35774;&#32622;&#21644;&#31454;&#20105;&#35774;&#32622;&#20013;&#65292;&#24403;&#31639;&#27861;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#26102;&#65292;&#37325;&#26032;&#35757;&#32451;&#36807;&#31243;&#28176;&#36817;&#25910;&#25947;&#21040;&#19968;&#20010;&#31283;&#23450;&#29366;&#24577;&#12290;</title><link>http://arxiv.org/abs/2307.01166</link><description>&lt;p&gt;
&#38024;&#23545;&#25112;&#30053;&#38750;&#23616;&#37096;&#20998;&#24067;&#20559;&#31227;&#30340;&#32806;&#21512;&#26799;&#24230;&#27969;&#21160;
&lt;/p&gt;
&lt;p&gt;
Coupled Gradient Flows for Strategic Non-Local Distribution Shift. (arXiv:2307.01166v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01166
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#29616;&#23454;&#19990;&#30028;&#31995;&#32479;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#21160;&#24577;&#65292;&#24182;&#19988;&#25429;&#25417;&#20102;&#23398;&#20064;&#31639;&#27861;&#19982;&#20854;&#24212;&#29992;&#30340;&#20998;&#24067;&#20043;&#38388;&#30340;&#21453;&#39304;&#24490;&#29615;&#12290;&#36890;&#36807;&#32806;&#21512;&#20559;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#65292;&#32771;&#34385;&#20102;&#25112;&#30053;&#24615;&#21453;&#24212;&#12289;&#38750;&#23616;&#37096;&#20869;&#29983;&#20154;&#21475;&#20114;&#21160;&#21644;&#20854;&#20182;&#22806;&#29983;&#20998;&#24067;&#20559;&#31227;&#26469;&#28304;&#65292;&#20197;&#23454;&#29616;&#23545;&#26102;&#38388;&#19978;&#32454;&#24494;&#21464;&#21270;&#30340;&#25429;&#25417;&#12290;&#30740;&#31350;&#35777;&#26126;&#20102;&#22312;&#21512;&#20316;&#35774;&#32622;&#21644;&#31454;&#20105;&#35774;&#32622;&#20013;&#65292;&#24403;&#31639;&#27861;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#26102;&#65292;&#37325;&#26032;&#35757;&#32451;&#36807;&#31243;&#28176;&#36817;&#25910;&#25947;&#21040;&#19968;&#20010;&#31283;&#23450;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#29616;&#23454;&#19990;&#30028;&#31995;&#32479;&#20013;&#20998;&#24067;&#20559;&#31227;&#30340;&#21160;&#24577;&#36807;&#31243;&#65292;&#35813;&#26694;&#26550;&#25429;&#25417;&#20102;&#23398;&#20064;&#31639;&#27861;&#19982;&#20854;&#24212;&#29992;&#30340;&#20998;&#24067;&#20043;&#38388;&#30340;&#21453;&#39304;&#24490;&#29615;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#20027;&#35201;&#20197;&#23545;&#25239;&#25110;&#36807;&#24230;&#31616;&#21270;&#30340;&#20998;&#24067;&#20559;&#31227;&#32467;&#26500;&#26469;&#24314;&#27169;&#21453;&#39304;&#24341;&#36215;&#30340;&#20998;&#24067;&#20559;&#31227;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32806;&#21512;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#65292;&#36890;&#36807;&#32771;&#34385;&#30001;&#20110;&#23545;&#31639;&#27861;&#20915;&#31574;&#30340;&#25112;&#30053;&#24615;&#21453;&#24212;&#12289;&#38750;&#23616;&#37096;&#20869;&#29983;&#20154;&#21475;&#20114;&#21160;&#21644;&#20854;&#20182;&#22806;&#29983;&#20998;&#24067;&#20559;&#31227;&#26469;&#28304;&#32780;&#20135;&#29983;&#30340;&#22797;&#26434;&#21160;&#24577;&#65292;&#25429;&#25417;&#20998;&#24067;&#38543;&#26102;&#38388;&#30340;&#32454;&#24494;&#21464;&#21270;&#12290;&#25105;&#20204;&#32771;&#34385;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20004;&#31181;&#24120;&#35265;&#35774;&#32622;&#65306;&#20449;&#24687;&#19981;&#23545;&#31216;&#30340;&#21512;&#20316;&#35774;&#32622;&#20197;&#21450;&#23398;&#20064;&#32773;&#38754;&#23545;&#25112;&#30053;&#29992;&#25143;&#30340;&#31454;&#20105;&#35774;&#32622;&#12290;&#23545;&#20110;&#36825;&#20004;&#31181;&#35774;&#32622;&#65292;&#24403;&#31639;&#27861;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#26102;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#37325;&#26032;&#35757;&#32451;&#36807;&#31243;&#25910;&#25947;&#21040;&#19968;&#20010;&#31283;&#23450;&#29366;&#24577;&#30340;&#28176;&#36817;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel framework for analyzing the dynamics of distribution shift in real-world systems that captures the feedback loop between learning algorithms and the distributions on which they are deployed. Prior work largely models feedback-induced distribution shift as adversarial or via an overly simplistic distribution-shift structure. In contrast, we propose a coupled partial differential equation model that captures fine-grained changes in the distribution over time by accounting for complex dynamics that arise due to strategic responses to algorithmic decision-making, non-local endogenous population interactions, and other exogenous sources of distribution shift. We consider two common settings in machine learning: cooperative settings with information asymmetries, and competitive settings where a learner faces strategic users. For both of these settings, when the algorithm retrains via gradient descent, we prove asymptotic convergence of the retraining procedure to a steady-
&lt;/p&gt;</description></item><item><title>RObotic MAnipulation Network&#65288;ROMAN&#65289;&#36890;&#36807;&#28151;&#21512;&#23618;&#27425;&#23398;&#20064;&#26694;&#26550;&#35299;&#20915;&#22797;&#26434;&#30340;&#36830;&#32493;&#25805;&#20316;&#20219;&#21153;&#65292;&#23454;&#29616;&#20102;&#20219;&#21153;&#22810;&#26679;&#24615;&#21644;&#40065;&#26834;&#30340;&#22833;&#36133;&#24674;&#22797;&#12290;</title><link>http://arxiv.org/abs/2307.00125</link><description>&lt;p&gt;
RObotic MAnipulation Network&#65288;ROMAN&#65289;--&#28151;&#21512;&#23618;&#27425;&#23398;&#20064;&#35299;&#20915;&#22797;&#26434;&#30340;&#36830;&#32493;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
RObotic MAnipulation Network (ROMAN) -- Hybrid Hierarchical Learning for Solving Complex Sequential Tasks. (arXiv:2307.00125v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00125
&lt;/p&gt;
&lt;p&gt;
RObotic MAnipulation Network&#65288;ROMAN&#65289;&#36890;&#36807;&#28151;&#21512;&#23618;&#27425;&#23398;&#20064;&#26694;&#26550;&#35299;&#20915;&#22797;&#26434;&#30340;&#36830;&#32493;&#25805;&#20316;&#20219;&#21153;&#65292;&#23454;&#29616;&#20102;&#20219;&#21153;&#22810;&#26679;&#24615;&#21644;&#40065;&#26834;&#30340;&#22833;&#36133;&#24674;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#20307;&#20154;&#24037;&#26234;&#33021;&#20013;&#65292;&#35299;&#20915;&#38271;&#24207;&#21015;&#20219;&#21153;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#20351;&#26426;&#22120;&#20154;&#31995;&#32479;&#33021;&#22815;&#25191;&#34892;&#22810;&#26679;&#21270;&#30340;&#36830;&#32493;&#20219;&#21153;&#65292;&#24182;&#20855;&#22791;&#24191;&#27867;&#30340;&#25805;&#20316;&#25216;&#33021;&#26159;&#19968;&#20010;&#27963;&#36291;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#23618;&#27425;&#23398;&#20064;&#26694;&#26550;&#65292;&#21363;ROBOTIC Manipulation Network&#65288;ROMAN&#65289;&#65292;&#20197;&#35299;&#20915;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#30340;&#22810;&#20010;&#22797;&#26434;&#20219;&#21153;&#30340;&#38271;&#26102;&#38388;&#20219;&#21153;&#12290;ROMAN&#36890;&#36807;&#38598;&#25104;&#34892;&#20026;&#20811;&#38534;&#12289;&#27169;&#20223;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#26469;&#23454;&#29616;&#20219;&#21153;&#30340;&#22810;&#26679;&#24615;&#20197;&#21450;&#40065;&#26834;&#30340;&#22833;&#36133;&#24674;&#22797;&#12290;&#23427;&#21253;&#25324;&#19968;&#20010;&#20013;&#22830;&#25805;&#20316;&#32593;&#32476;&#65292;&#21327;&#35843;&#19968;&#32452;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#27599;&#20010;&#32593;&#32476;&#19987;&#27880;&#20110;&#19981;&#21516;&#30340;&#21487;&#37325;&#32452;&#23376;&#20219;&#21153;&#65292;&#29983;&#25104;&#23427;&#20204;&#22312;&#22797;&#26434;&#30340;&#38271;&#26102;&#38388;&#25805;&#20316;&#20219;&#21153;&#20013;&#30340;&#27491;&#30830;&#36830;&#32493;&#21160;&#20316;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#21327;&#35843;&#21644;&#28608;&#27963;&#36825;&#20123;&#19987;&#38376;&#30340;&#25805;&#20316;&#19987;&#23478;&#65292;ROMAN&#29983;&#25104;&#20102;&#27491;&#30830;&#30340;&#39034;&#24207;&#28608;&#27963;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving long sequential tasks poses a significant challenge in embodied artificial intelligence. Enabling a robotic system to perform diverse sequential tasks with a broad range of manipulation skills is an active area of research. In this work, we present a Hybrid Hierarchical Learning framework, the Robotic Manipulation Network (ROMAN), to address the challenge of solving multiple complex tasks over long time horizons in robotic manipulation. ROMAN achieves task versatility and robust failure recovery by integrating behavioural cloning, imitation learning, and reinforcement learning. It consists of a central manipulation network that coordinates an ensemble of various neural networks, each specialising in distinct re-combinable sub-tasks to generate their correct in-sequence actions for solving complex long-horizon manipulation tasks. Experimental results show that by orchestrating and activating these specialised manipulation experts, ROMAN generates correct sequential activations f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#20026;&#20154;&#24037;&#26234;&#33021;&#12289;&#33258;&#25105;&#24847;&#35782;&#21644;&#20195;&#29702;&#38382;&#39064;&#25552;&#20379;&#26356;&#28165;&#26224;&#30340;&#23450;&#20041;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#27979;&#35797;&#20154;&#24037;&#33258;&#25105;&#24847;&#35782;&#65292;&#24182;&#35752;&#35770;&#20102;&#36825;&#31181;&#26041;&#27861;&#24341;&#21457;&#30340;&#19968;&#20123;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.17258</link><description>&lt;p&gt;
&#36973;&#21463;&#33510;&#38590;&#30340;&#28900;&#38754;&#21253;&#26426;
&lt;/p&gt;
&lt;p&gt;
Suffering Toasters. (arXiv:2306.17258v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17258
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#20026;&#20154;&#24037;&#26234;&#33021;&#12289;&#33258;&#25105;&#24847;&#35782;&#21644;&#20195;&#29702;&#38382;&#39064;&#25552;&#20379;&#26356;&#28165;&#26224;&#30340;&#23450;&#20041;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#27979;&#35797;&#20154;&#24037;&#33258;&#25105;&#24847;&#35782;&#65292;&#24182;&#35752;&#35770;&#20102;&#36825;&#31181;&#26041;&#27861;&#24341;&#21457;&#30340;&#19968;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#39046;&#22495;&#65292;&#26234;&#33021;&#30340;&#24191;&#27867;&#25509;&#21463;&#30340;&#23450;&#20041;&#20173;&#28982;&#38590;&#20197;&#25214;&#21040;&#12290;&#30001;&#20110;&#25105;&#20204;&#23545;AI&#33539;&#24335;&#12289;&#26550;&#26500;&#21644;&#24037;&#20855;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#20154;&#20204;&#26222;&#36941;&#35748;&#20026;&#33258;&#28982;&#20135;&#29983;&#30340;AI&#24847;&#35782;&#27604;&#20197;&#24448;&#26356;&#26377;&#21487;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22768;&#31216;&#25152;&#26377;&#24403;&#21069;&#30340;&#26234;&#33021;&#27979;&#35797;&#37117;&#19981;&#36275;&#20197;&#25351;&#20986;&#23384;&#22312;&#25110;&#32570;&#20047;&#35937;&#20154;&#31867;&#30452;&#35273;&#24863;&#30693;&#30340;&#26234;&#33021;&#12290;&#25105;&#20204;&#20511;&#37492;&#31185;&#23398;&#21746;&#23398;&#12289;&#24515;&#29702;&#23398;&#21644;&#20854;&#20182;&#39046;&#22495;&#30340;&#24605;&#24819;&#65292;&#25552;&#20379;&#20102;&#23545;&#20154;&#24037;&#26234;&#33021;&#12289;&#33258;&#25105;&#24847;&#35782;&#21644;&#20195;&#29702;&#38382;&#39064;&#30340;&#26356;&#28165;&#26224;&#23450;&#20041;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#27979;&#35797;&#20154;&#24037;&#33258;&#25105;&#24847;&#35782;&#30340;&#26032;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#24182;&#27010;&#36848;&#20102;&#21487;&#33021;&#30340;&#23454;&#29616;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#31181;&#26032;&#21551;&#21457;&#24335;&#26041;&#27861;&#24341;&#21457;&#30340;&#19968;&#20123;&#38382;&#39064;&#65292;&#26080;&#35770;&#26159;&#21746;&#23398;&#38382;&#39064;&#36824;&#26159;&#23454;&#29616;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
A widely accepted definition of intelligence in the context of Artificial Intelligence (AI) still eludes us. Due to our exceedingly rapid development of AI paradigms, architectures, and tools, the prospect of naturally arising AI consciousness seems more likely than ever. In this paper, we claim that all current intelligence tests are insufficient to point to the existence or lack of intelligence \textbf{as humans intuitively perceive it}. We draw from ideas in the philosophy of science, psychology, and other areas of research to provide a clearer definition of the problems of artificial intelligence, self-awareness, and agency. We furthermore propose a new heuristic approach to test for artificial self-awareness and outline a possible implementation. Finally, we discuss some of the questions that arise from this new heuristic, be they philosophical or implementation-oriented.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#65292;&#35299;&#20915;&#20102;&#36845;&#20195;&#20803;&#35757;&#32451;&#38454;&#27573;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#24809;&#32602;&#36807;&#24230;&#21306;&#20998;&#65292;&#20445;&#30041;&#25945;&#24072;&#27169;&#22411;&#30340;&#26032;&#31867;&#21035;&#27867;&#21270;&#30693;&#35782;&#65292;&#20248;&#20110;&#26631;&#20934;&#20803;&#35757;&#32451;&#36807;&#31243;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26368;&#36817;&#37051;&#23545;&#31216;Kullback-Leibler&#65288;NNSKL&#65289;&#25955;&#24230;&#26469;&#36827;&#19968;&#27493;&#25512;&#36827;&#30693;&#35782;&#33976;&#39311;&#30340;&#26497;&#38480;&#12290;</title><link>http://arxiv.org/abs/2306.16873</link><description>&lt;p&gt;
&#29702;&#35299;&#36845;&#20195;&#20803;&#35757;&#32451;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Understanding the Overfitting of the Episodic Meta-training. (arXiv:2306.16873v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16873
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#65292;&#35299;&#20915;&#20102;&#36845;&#20195;&#20803;&#35757;&#32451;&#38454;&#27573;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#24809;&#32602;&#36807;&#24230;&#21306;&#20998;&#65292;&#20445;&#30041;&#25945;&#24072;&#27169;&#22411;&#30340;&#26032;&#31867;&#21035;&#27867;&#21270;&#30693;&#35782;&#65292;&#20248;&#20110;&#26631;&#20934;&#20803;&#35757;&#32451;&#36807;&#31243;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26368;&#36817;&#37051;&#23545;&#31216;Kullback-Leibler&#65288;NNSKL&#65289;&#25955;&#24230;&#26469;&#36827;&#19968;&#27493;&#25512;&#36827;&#30693;&#35782;&#33976;&#39311;&#30340;&#26497;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#20004;&#38454;&#27573;&#23569;&#26679;&#26412;&#20998;&#31867;&#26041;&#27861;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#22312;&#36845;&#20195;&#20803;&#35757;&#32451;&#38454;&#27573;&#65292;&#27169;&#22411;&#36973;&#21463;&#20005;&#37325;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#25105;&#20204;&#20551;&#35774;&#36825;&#26159;&#30001;&#20110;&#36807;&#24230;&#21306;&#20998;&#36896;&#25104;&#30340;&#65292;&#21363;&#27169;&#22411;&#23398;&#20064;&#36807;&#20110;&#20381;&#36182;&#36866;&#21512;&#22522;&#31867;&#21306;&#20998;&#30340;&#34920;&#38754;&#29305;&#24449;&#65292;&#32780;&#25233;&#21046;&#20102;&#23545;&#26032;&#31867;&#21035;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#24809;&#32602;&#36807;&#24230;&#21306;&#20998;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20445;&#30041;&#26469;&#33258;&#25945;&#24072;&#27169;&#22411;&#30340;&#26032;&#31867;&#21035;&#27867;&#21270;&#30693;&#35782;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36873;&#25321;&#39564;&#35777;&#20934;&#30830;&#29575;&#26368;&#22909;&#30340;&#25945;&#24072;&#27169;&#22411;&#65292;&#24182;&#38480;&#21046;&#20102;&#23398;&#29983;&#27169;&#22411;&#32447;&#24615;&#20998;&#31867;&#22120;&#36755;&#20986;&#20998;&#24067;&#19982;&#25945;&#24072;&#27169;&#22411;&#20043;&#38388;&#30340;&#23545;&#31216;Kullback-Leibler&#65288;SKL&#65289;&#25955;&#24230;&#12290;&#36825;&#19968;&#31616;&#21333;&#30340;&#26041;&#27861;&#20248;&#20110;&#26631;&#20934;&#20803;&#35757;&#32451;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#29992;&#20110;&#20803;&#35757;&#32451;&#30340;&#26368;&#36817;&#37051;&#23545;&#31216;Kullback-Leibler&#65288;NNSKL&#65289;&#25955;&#24230;&#65292;&#20197;&#25512;&#21160;&#30693;&#35782;&#33976;&#39311;&#30340;&#26497;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the success of two-stage few-shot classification methods, in the episodic meta-training stage, the model suffers severe overfitting. We hypothesize that it is caused by over-discrimination, i.e., the model learns to over-rely on the superficial features that fit for base class discrimination while suppressing the novel class generalization. To penalize over-discrimination, we introduce knowledge distillation techniques to keep novel generalization knowledge from the teacher model during training. Specifically, we select the teacher model as the one with the best validation accuracy during meta-training and restrict the symmetric Kullback-Leibler (SKL) divergence between the output distribution of the linear classifier of the teacher model and that of the student model. This simple approach outperforms the standard meta-training process. We further propose the Nearest Neighbor Symmetric Kullback-Leibler (NNSKL) divergence for meta-training to push the limits of knowledge distill
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#25239;&#25915;&#20987;&#35780;&#20272;&#20102;&#28145;&#24230;&#22270;&#20687;&#21435;&#22122;&#27169;&#22411;&#30340;&#30456;&#20284;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#29616;&#26377;&#27169;&#22411;&#23481;&#26131;&#34987;&#25915;&#20987;&#12290;&#36824;&#30740;&#31350;&#20102;&#21435;&#22122;&#27169;&#22411;&#30340;&#36801;&#31227;&#24615;&#22312;&#22270;&#20687;&#21435;&#22122;&#20219;&#21153;&#20013;&#30340;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.16050</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#25239;&#25915;&#20987;&#35780;&#20272;&#28145;&#24230;&#22270;&#20687;&#21435;&#22122;&#27169;&#22411;&#30340;&#30456;&#20284;&#24615;&#21644;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating Similitude and Robustness of Deep Image Denoising Models via Adversarial Attack. (arXiv:2306.16050v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16050
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#25239;&#25915;&#20987;&#35780;&#20272;&#20102;&#28145;&#24230;&#22270;&#20687;&#21435;&#22122;&#27169;&#22411;&#30340;&#30456;&#20284;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#29616;&#26377;&#27169;&#22411;&#23481;&#26131;&#34987;&#25915;&#20987;&#12290;&#36824;&#30740;&#31350;&#20102;&#21435;&#22122;&#27169;&#22411;&#30340;&#36801;&#31227;&#24615;&#22312;&#22270;&#20687;&#21435;&#22122;&#20219;&#21153;&#20013;&#30340;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#21435;&#22122;&#39046;&#22495;&#26377;&#30528;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#24182;&#19988;&#27604;&#20256;&#32479;&#30340;&#22270;&#20687;&#21435;&#22122;&#26041;&#27861;&#26356;&#20248;&#36234;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19981;&#21487;&#36991;&#20813;&#22320;&#26174;&#31034;&#20986;&#24369;&#40065;&#26834;&#24615;&#65292;&#22312;&#38754;&#23545;&#23545;&#25239;&#25915;&#20987;&#26102;&#26131;&#21463;&#25439;&#12290;&#26412;&#25991;&#21457;&#29616;&#20102;&#29616;&#26377;&#28145;&#24230;&#22270;&#20687;&#21435;&#22122;&#26041;&#27861;&#20043;&#38388;&#30340;&#30456;&#20284;&#20043;&#22788;&#65292;&#23427;&#20204;&#37117;&#23481;&#26131;&#34987;&#23545;&#25239;&#25915;&#20987;&#27450;&#39575;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21435;&#22122;-PGD&#26041;&#27861;&#65292;&#23427;&#26159;&#19968;&#31181;&#20840;&#23545;&#25239;&#30340;&#21435;&#22122;&#27169;&#22411;&#12290;&#24403;&#21069;&#20027;&#27969;&#30340;&#38750;&#30450;&#21435;&#22122;&#27169;&#22411;&#65288;DnCNN&#65292;FFDNet&#65292;ECNDNet&#65292;BRDNet&#65289;&#65292;&#30450;&#21435;&#22122;&#27169;&#22411;&#65288;DnCNN-B&#65292;Noise2Noise&#65292;RDDCNN-B&#65292;FAN&#65289;&#65292;&#21363;&#25554;&#21363;&#29992;&#65288;DPIR&#65292;CurvPnP&#65289;&#21644;&#23637;&#24320;&#21435;&#22122;&#27169;&#22411;&#65288;DeamNet&#65289;&#24212;&#29992;&#20110;&#28784;&#24230;&#21644;&#24425;&#33394;&#22270;&#20687;&#37117;&#21487;&#20197;&#34987;&#21516;&#19968;&#32452;&#26041;&#27861;&#25915;&#20987;&#12290;&#20854;&#27425;&#65292;&#30001;&#20110;&#21435;&#22122;-PGD&#30340;&#36801;&#31227;&#24615;&#22312;&#22270;&#20687;&#21435;&#22122;&#20219;&#21153;&#20013;&#24456;&#31361;&#20986;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#23454;&#39564;&#26469;&#25506;&#32034;&#36801;&#31227;&#24615;&#19979;&#30340;&#28508;&#22312;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) have a wide range of applications in the field of image denoising, and they are superior to traditional image denoising. However, DNNs inevitably show vulnerability, which is the weak robustness in the face of adversarial attacks. In this paper, we find some similitudes between existing deep image denoising methods, as they are consistently fooled by adversarial attacks. First, denoising-PGD is proposed which is a denoising model full adversarial method. The current mainstream non-blind denoising models (DnCNN, FFDNet, ECNDNet, BRDNet), blind denoising models (DnCNN-B, Noise2Noise, RDDCNN-B, FAN), and plug-and-play (DPIR, CurvPnP) and unfolding denoising models (DeamNet) applied to grayscale and color images can be attacked by the same set of methods. Second, since the transferability of denoising-PGD is prominent in the image denoising task, we design experiments to explore the characteristic of the latent under the transferability. We correlate transferabi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#21152;&#26435;&#20248;&#21270;&#36712;&#36857;&#65288;WOT&#65289;&#8221;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#21382;&#21490;&#36712;&#36857;&#65292;&#35299;&#20915;&#20102;&#23545;&#25239;&#35757;&#32451;&#20013;&#30340;&#40065;&#26834;&#27867;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.14275</link><description>&lt;p&gt;
&#36890;&#36807;&#37325;&#26032;&#21152;&#26435;&#20248;&#21270;&#36712;&#36857;&#22686;&#24378;&#23545;&#25239;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Enhancing Adversarial Training via Reweighting Optimization Trajectory. (arXiv:2306.14275v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#21152;&#26435;&#20248;&#21270;&#36712;&#36857;&#65288;WOT&#65289;&#8221;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#21382;&#21490;&#36712;&#36857;&#65292;&#35299;&#20915;&#20102;&#23545;&#25239;&#35757;&#32451;&#20013;&#30340;&#40065;&#26834;&#27867;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#23545;&#25239;&#35757;&#32451;&#24050;&#25104;&#20026;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#40065;&#26834;&#24615;&#30340;&#20107;&#23454;&#19978;&#30340;&#26041;&#27861;&#65292;&#20294;&#20247;&#25152;&#21608;&#30693;&#65292;&#31616;&#21333;&#30340;&#23545;&#25239;&#35757;&#32451;&#36973;&#21463;&#20102;&#20196;&#20154;&#30031;&#32553;&#30340;&#40065;&#26834;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#23548;&#33268;&#40065;&#26834;&#27867;&#21270;&#25928;&#26524;&#19981;&#20339;&#12290;&#36817;&#24180;&#26469;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#32570;&#28857;&#65292;&#22914;&#39069;&#22806;&#30340;&#35268;&#33539;&#21270;&#12289;&#23545;&#25239;&#26435;&#37325;&#25200;&#21160;&#21644;&#26356;&#22810;&#25968;&#25454;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#40065;&#26834;&#27867;&#21270;&#30340;&#25913;&#36827;&#20173;&#28982;&#36828;&#19981;&#29702;&#24819;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#20840;&#26032;&#30340;&#35282;&#24230;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;--&#20248;&#21270;&#21382;&#21490;&#36712;&#36857;&#30340;&#31934;&#32454;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#21152;&#26435;&#20248;&#21270;&#36712;&#36857;&#65288;WOT&#65289;&#8221;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#23545;&#25239;&#35757;&#32451;&#30340;&#20248;&#21270;&#36712;&#36857;&#22312;&#26102;&#38388;&#19978;&#30340;&#29305;&#28857;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;WOT&#22312;&#21508;&#31181;&#26368;&#26032;&#23545;&#25239;&#25915;&#20987;&#19979;&#30340;&#26377;&#25928;&#24615;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;WOT&#19982;&#29616;&#26377;&#26041;&#27861;&#23436;&#32654;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the fact that adversarial training has become the de facto method for improving the robustness of deep neural networks, it is well-known that vanilla adversarial training suffers from daunting robust overfitting, resulting in unsatisfactory robust generalization. A number of approaches have been proposed to address these drawbacks such as extra regularization, adversarial weights perturbation, and training with more data over the last few years. However, the robust generalization improvement is yet far from satisfactory. In this paper, we approach this challenge with a brand new perspective -- refining historical optimization trajectories. We propose a new method named \textbf{Weighted Optimization Trajectories (WOT)} that leverages the optimization trajectories of adversarial training in time. We have conducted extensive experiments to demonstrate the effectiveness of WOT under various state-of-the-art adversarial attacks. Our results show that WOT integrates seamlessly with t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedSelect&#30340;&#26032;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23547;&#25214;&#26368;&#20339;&#23458;&#25143;&#31471;&#23376;&#32593;&#32476;&#20174;&#32780;&#30452;&#25509;&#20010;&#24615;&#21270;&#23458;&#25143;&#31471;&#23376;&#32593;&#32476;&#32467;&#26500;&#21644;&#21442;&#25968;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#20840;&#23616;&#30693;&#35782;&#65292;&#25552;&#39640;&#20102;&#23458;&#25143;&#31471;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.13264</link><description>&lt;p&gt;
FedSelect: &#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#20013;&#21442;&#25968;&#33258;&#23450;&#20041;&#36873;&#25321;&#30340;&#32454;&#35843;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
FedSelect: Customized Selection of Parameters for Fine-Tuning during Personalized Federated Learning. (arXiv:2306.13264v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13264
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedSelect&#30340;&#26032;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23547;&#25214;&#26368;&#20339;&#23458;&#25143;&#31471;&#23376;&#32593;&#32476;&#20174;&#32780;&#30452;&#25509;&#20010;&#24615;&#21270;&#23458;&#25143;&#31471;&#23376;&#32593;&#32476;&#32467;&#26500;&#21644;&#21442;&#25968;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#20840;&#23616;&#30693;&#35782;&#65292;&#25552;&#39640;&#20102;&#23458;&#25143;&#31471;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26088;&#22312;&#36890;&#36807;&#22312;&#26412;&#22320;&#25968;&#25454;&#19978;&#24494;&#35843;&#23458;&#25143;&#31471;&#21442;&#25968;&#25110;&#38024;&#23545;&#26412;&#22320;&#20219;&#21153;&#20010;&#24615;&#21270;&#26550;&#26500;&#26469;&#25552;&#39640;&#23458;&#25143;&#31471;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#35201;&#20040;&#22312;&#29306;&#29298;&#37325;&#35201;&#30340;&#20840;&#23616;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20010;&#24615;&#21270;&#65292;&#35201;&#20040;&#22312;&#39044;&#20808;&#30830;&#23450;&#32593;&#32476;&#23618;&#20197;&#36827;&#34892;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#23548;&#33268;&#23458;&#25143;&#31471;&#27169;&#22411;&#20013;&#20840;&#23616;&#30693;&#35782;&#20648;&#23384;&#30340;&#19981;&#36275;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;FedSelect&#65292;&#36890;&#36807;&#21516;&#26102;&#25628;&#32034;&#24182;&#33719;&#24471;&#20010;&#24615;&#21270;&#26368;&#20339;&#21442;&#25968;&#21644;&#29992;&#20110;&#20840;&#23616;&#32858;&#21512;&#30340;&#20854;&#20313;&#21442;&#25968;&#65292;&#20174;&#32780;&#30452;&#25509;&#20010;&#24615;&#21270;&#23458;&#25143;&#23376;&#32593;&#32476;&#32467;&#26500;&#21644;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in federated learning (FL) seek to increase client-level performance by fine-tuning client parameters on local data or personalizing architectures for the local task. Existing methods for such personalization either prune a global model or fine-tune a global model on a local client distribution. However, these existing methods either personalize at the expense of retaining important global knowledge, or predetermine network layers for fine-tuning, resulting in suboptimal storage of global knowledge within client models. Enlightened by the lottery ticket hypothesis, we first introduce a hypothesis for finding optimal client subnetworks to locally fine-tune while leaving the rest of the parameters frozen. We then propose a novel FL framework, FedSelect, using this procedure that directly personalizes both client subnetwork structure and parameters, via the simultaneous discovery of optimal parameters for personalization and the rest of parameters for global aggregatio
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#23454;&#26102;&#21453;&#39304;&#25511;&#21046;InAs/GaAs&#37327;&#23376;&#28857;&#29983;&#38271;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.12898</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#23454;&#26102;&#21453;&#39304;&#25511;&#21046;InAs/GaAs&#37327;&#23376;&#28857;&#29983;&#38271;
&lt;/p&gt;
&lt;p&gt;
Machine-Learning-Assisted and Real-Time-Feedback-Controlled Growth of InAs/GaAs Quantum Dots. (arXiv:2306.12898v1 [cond-mat.mes-hall])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12898
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#23454;&#26102;&#21453;&#39304;&#25511;&#21046;InAs/GaAs&#37327;&#23376;&#28857;&#29983;&#38271;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#32452;&#35013;&#30340;InAs / GaAs&#37327;&#23376;&#28857;&#65288;QDs&#65289;&#20855;&#26377;&#29992;&#20110;&#24320;&#21457;&#21508;&#31181;&#20809;&#30005;&#23376;&#22120;&#20214;&#30340;&#26497;&#39640;&#20215;&#20540;&#12290;&#24314;&#31435;&#29305;&#23450;&#23494;&#24230;&#30340;QDs&#30340;&#36807;&#31243;&#21442;&#25968;&#26159;&#19968;&#20010;&#22810;&#32500;&#20248;&#21270;&#25361;&#25112;&#65292;&#36890;&#24120;&#36890;&#36807;&#32791;&#26102;&#21644;&#36845;&#20195;&#30340;&#35797;&#38169;&#26469;&#35299;&#20915;&#12290;&#22312;&#27492;&#65292;&#20316;&#32773;&#20351;&#29992;&#22522;&#20110;3D ResNet&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#65292;&#19987;&#38376;&#35757;&#32451;RHEED&#35270;&#39057;&#65292;&#24182;&#25552;&#20379;&#26377;&#20851;&#34920;&#38754;&#24418;&#35980;&#30340;&#23454;&#26102;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-assembled InAs/GaAs quantum dots (QDs) have properties highly valuable for developing various optoelectronic devices such as QD lasers and single photon sources. The applications strongly rely on the density and quality of these dots, which has motivated studies of the growth process control to realize high-quality epi-wafers and devices. Establishing the process parameters in molecular beam epitaxy (MBE) for a specific density of QDs is a multidimensional optimization challenge, usually addressed through time-consuming and iterative trial-and-error. Meanwhile, reflective high-energy electron diffraction (RHEED) has been widely used to capture a wealth of growth information in situ. However, it still faces the challenges of extracting information from noisy and overlapping images. Here, based on 3D ResNet, we developed a machine learning (ML) model specially designed for training RHEED videos instead of static images and providing real-time feedback on surface morphologies for pro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#21487;&#20197;&#23398;&#20064;&#29992;&#20110;&#26925;&#22278;&#31639;&#23376;&#30340;&#40784;&#27425;&#21270;&#26144;&#23556;&#65292;&#20197;&#24314;&#31435;&#32771;&#34385;&#25509;&#21475;&#30340;&#40784;&#27425;&#21270;&#26412;&#26500;&#23450;&#24459;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.12006</link><description>&lt;p&gt;
&#23398;&#20064;&#23567;&#27874;&#23545;&#26925;&#22278;&#31639;&#23376;&#30340;&#40784;&#27425;&#21270;
&lt;/p&gt;
&lt;p&gt;
Learning Homogenization for Elliptic Operators. (arXiv:2306.12006v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12006
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#21487;&#20197;&#23398;&#20064;&#29992;&#20110;&#26925;&#22278;&#31639;&#23376;&#30340;&#40784;&#27425;&#21270;&#26144;&#23556;&#65292;&#20197;&#24314;&#31435;&#32771;&#34385;&#25509;&#21475;&#30340;&#40784;&#27425;&#21270;&#26412;&#26500;&#23450;&#24459;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#23610;&#24230;&#20559;&#24494;&#20998;&#26041;&#31243;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#37117;&#26377;&#20986;&#29616;&#12290;&#40784;&#27425;&#21270;&#29702;&#35770;&#26159;&#28040;&#38500;&#23567;&#23610;&#24230;&#20381;&#36182;&#30340;&#24378;&#26377;&#21147;&#26041;&#27861;&#65292;&#21487;&#20197;&#24471;&#21040;&#31616;&#21270;&#30340;&#26041;&#31243;&#20197;&#21450;&#35745;&#31639;&#19978;&#30340;&#20415;&#21033;&#12290;&#22312;&#36830;&#32493;&#20171;&#36136;&#21147;&#23398;&#39046;&#22495;&#65292;&#40784;&#27425;&#21270;&#23545;&#20110;&#23548;&#20986;&#21253;&#21547;&#24494;&#35266;&#29289;&#29702;&#23398;&#30340;&#26412;&#26500;&#23450;&#24459;&#20197;&#21046;&#23450;&#24863;&#20852;&#36259;&#30340;&#23439;&#35266;&#37327;&#30340;&#24179;&#34913;&#26041;&#31243;&#24456;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#33719;&#24471;&#40784;&#27425;&#21270;&#26412;&#26500;&#23450;&#24459;&#36890;&#24120;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#24120;&#27809;&#26377;&#35299;&#26512;&#24418;&#24335;&#65292;&#24182;&#19988;&#21487;&#20197;&#23637;&#29616;&#22312;&#24494;&#35266;&#23610;&#24230;&#19978;&#19981;&#23384;&#22312;&#30340;&#29616;&#35937;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#25968;&#25454;&#39537;&#21160;&#30340;&#23398;&#20064;&#26412;&#26500;&#23450;&#24459;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#23398;&#20064;&#26925;&#22278;&#31639;&#23376;&#40784;&#27425;&#21270;&#26144;&#23556;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#35828;&#26126;&#19982;&#36825;&#31181;&#25509;&#21475;&#26377;&#20851;&#30340;&#40784;&#27425;&#21270;&#24314;&#31435;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#26500;&#36896;&#36866;&#24403;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#20197;&#32771;&#34385;&#24213;&#23618;&#20960;&#20309;&#23398;&#21644;&#24494;&#35266;&#32467;&#26500;&#65292;&#33258;&#36866;&#24212;&#22320;&#23398;&#20064;&#23558;&#36825;&#20123;&#25509;&#21475;&#21512;&#24182;&#21040;&#40784;&#27425;&#21270;&#26412;&#26500;&#23450;&#24459;&#20013;&#12290;&#25105;&#20204;&#30340;&#25968;&#23383;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#20934;&#30830;&#22320;&#25429;&#25417;&#26377;&#25928;&#30340;&#23439;&#35266;&#34892;&#20026;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multiscale partial differential equations (PDEs) arise in various applications, and several schemes have been developed to solve them efficiently. Homogenization theory is a powerful methodology that eliminates the small-scale dependence, resulting in simplified equations that are computationally tractable. In the field of continuum mechanics, homogenization is crucial for deriving constitutive laws that incorporate microscale physics in order to formulate balance laws for the macroscopic quantities of interest. However, obtaining homogenized constitutive laws is often challenging as they do not in general have an analytic form and can exhibit phenomena not present on the microscale. In response, data-driven learning of the constitutive law has been proposed as appropriate for this task. However, a major challenge in data-driven learning approaches for this problem has remained unexplored: the impact of discontinuities and corner interfaces in the underlying material. These discontinui
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#24212;&#29992;&#20110;&#38543;&#26426;&#20248;&#21270;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#30340;&#33258;&#36866;&#24212;&#31639;&#27861;&#12290;&#31639;&#27861;&#21487;&#20197;&#33258;&#21160;&#36866;&#24212;&#19981;&#21516;&#30340;&#22122;&#22768;&#27700;&#24179;&#21644;&#26799;&#24230;&#27604;&#20363;&#33539;&#22260;&#65292;&#20174;&#32780;&#36798;&#21040;&#65288;&#36817;&#20284;&#65289;&#26368;&#20248;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.10278</link><description>&lt;p&gt;
&#38750;&#20984;&#20248;&#21270;&#20013;&#30340;&#33258;&#36866;&#24212;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Adaptive Strategies in Non-convex Optimization. (arXiv:2306.10278v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10278
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#24212;&#29992;&#20110;&#38543;&#26426;&#20248;&#21270;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#30340;&#33258;&#36866;&#24212;&#31639;&#27861;&#12290;&#31639;&#27861;&#21487;&#20197;&#33258;&#21160;&#36866;&#24212;&#19981;&#21516;&#30340;&#22122;&#22768;&#27700;&#24179;&#21644;&#26799;&#24230;&#27604;&#20363;&#33539;&#22260;&#65292;&#20174;&#32780;&#36798;&#21040;&#65288;&#36817;&#20284;&#65289;&#26368;&#20248;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;&#31639;&#27861;&#19981;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#23601;&#21487;&#20197;&#34920;&#29616;&#24471;&#19982;&#30693;&#36947;&#36825;&#20010;&#38382;&#39064;&#29305;&#23450;&#21442;&#25968;&#30340;&#31639;&#27861;&#30456;&#31454;&#20105;&#65292;&#37027;&#20040;&#23601;&#35828;&#31639;&#27861;&#23545;&#26576;&#20010;&#21442;&#25968;&#26159;&#33258;&#36866;&#24212;&#30340;&#12290;&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#22312;&#20197;&#19979;&#22330;&#26223;&#20013;&#24320;&#21457;&#33258;&#36866;&#24212;&#31639;&#27861;&#30340;&#24037;&#20316;&#65306;1. &#22312;&#38543;&#26426;&#20248;&#21270;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#21482;&#25910;&#21040;&#38543;&#26426;&#26799;&#24230;&#65292;&#24182;&#19988;&#35780;&#20272;&#36825;&#20123;&#26799;&#24230;&#30340;&#22122;&#22768;&#27700;&#24179;&#26497;&#22823;&#22320;&#24433;&#21709;&#20102;&#25910;&#25947;&#36895;&#24230;&#12290;&#36890;&#24120;&#38656;&#35201;&#35843;&#25972;&#22122;&#22768;&#27700;&#24179;&#25165;&#33021;&#23454;&#29616;&#26368;&#20248;&#36895;&#29575;&#65292;&#32780;&#25105;&#20204;&#24320;&#21457;&#20102;&#33258;&#36866;&#24212;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#30693;&#36947;&#22122;&#22768;&#33539;&#22260;&#30340;&#24773;&#20917;&#19979;&#33258;&#21160;&#20445;&#35777;&#65288;&#36817;&#20284;&#65289;&#26368;&#20248;&#36895;&#29575;&#12290;2. &#22312;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26102;&#65292;&#27599;&#20010;&#22352;&#26631;&#36724;&#19978;&#30340;&#26799;&#24230;&#22823;&#23567;&#27604;&#20363;&#21487;&#20197;&#25955;&#24067;&#22312;&#38750;&#24120;&#24191;&#30340;&#33539;&#22260;&#20869;&#65292;&#38500;&#38750;&#37319;&#29992;&#20687;BatchNorm&#36825;&#26679;&#30340;&#24402;&#19968;&#21270;&#25216;&#26415;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#19981;&#32771;&#34385;&#26799;&#24230;&#27604;&#20363;&#38382;&#39064;&#30340;&#31639;&#27861;&#21487;&#33021;&#34920;&#29616;&#38750;&#24120;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
An algorithm is said to be adaptive to a certain parameter (of the problem) if it does not need a priori knowledge of such a parameter but performs competitively to those that know it. This dissertation presents our work on adaptive algorithms in following scenarios: 1. In the stochastic optimization setting, we only receive stochastic gradients and the level of noise in evaluating them greatly affects the convergence rate. Tuning is typically required when without prior knowledge of the noise scale in order to achieve the optimal rate. Considering this, we designed and analyzed noise-adaptive algorithms that can automatically ensure (near)-optimal rates under different noise scales without knowing it. 2. In training deep neural networks, the scales of gradient magnitudes in each coordinate can scatter across a very wide range unless normalization techniques, like BatchNorm, are employed. In such situations, algorithms not addressing this problem of gradient scales can behave very poor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31163;&#32447;&#20248;&#20808;&#32463;&#39564;&#37325;&#25918;&#65288;OPER&#65289;&#26041;&#27861;&#26469;&#35299;&#20915;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;&#36890;&#36807;&#35774;&#35745;&#19968;&#31867;&#20248;&#20808;&#32423;&#20989;&#25968;&#26469;&#23545;&#39640;&#22238;&#25253;&#30340;&#36716;&#25442;&#36827;&#34892;&#20248;&#20808;&#22788;&#29702;&#65292;&#20174;&#32780;&#25913;&#21892;&#34892;&#20026;&#31574;&#30053;&#65292;&#24182;&#22312;&#27492;&#25913;&#36827;&#30340;&#31574;&#30053;&#32422;&#26463;&#19979;&#20248;&#21270;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23545;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65292;OPER&#26041;&#27861;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2306.05412</link><description>&lt;p&gt;
&#31163;&#32447;&#20248;&#20808;&#32463;&#39564;&#37325;&#25918;
&lt;/p&gt;
&lt;p&gt;
Offline Prioritized Experience Replay. (arXiv:2306.05412v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05412
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31163;&#32447;&#20248;&#20808;&#32463;&#39564;&#37325;&#25918;&#65288;OPER&#65289;&#26041;&#27861;&#26469;&#35299;&#20915;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;&#36890;&#36807;&#35774;&#35745;&#19968;&#31867;&#20248;&#20808;&#32423;&#20989;&#25968;&#26469;&#23545;&#39640;&#22238;&#25253;&#30340;&#36716;&#25442;&#36827;&#34892;&#20248;&#20808;&#22788;&#29702;&#65292;&#20174;&#32780;&#25913;&#21892;&#34892;&#20026;&#31574;&#30053;&#65292;&#24182;&#22312;&#27492;&#25913;&#36827;&#30340;&#31574;&#30053;&#32422;&#26463;&#19979;&#20248;&#21270;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23545;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65292;OPER&#26041;&#27861;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#38754;&#20020;&#30528;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#35774;&#35745;&#23398;&#20064;&#31574;&#30053;&#21644;&#34892;&#20026;&#31574;&#30053;&#20043;&#38388;&#30340;&#22797;&#26434;&#32422;&#26463;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32422;&#26463;&#36890;&#36807;&#22343;&#21248;&#37319;&#26679;&#31561;&#26041;&#24335;&#34987;&#24212;&#29992;&#21040;&#34920;&#29616;&#33391;&#22909;&#21644;&#34920;&#29616;&#24046;&#30340;&#34892;&#21160;&#19978;&#65292;&#36825;&#21487;&#33021;&#20250;&#23545;&#23398;&#20064;&#31574;&#30053;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31163;&#32447;&#20248;&#20808;&#32463;&#39564;&#37325;&#25918;&#65288;OPER&#65289;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#31867;&#20248;&#20808;&#32423;&#20989;&#25968;&#65292;&#29992;&#20110;&#23558;&#39640;&#22238;&#25253;&#30340;&#36716;&#25442;&#32622;&#20110;&#26356;&#39057;&#32321;&#30340;&#35775;&#38382;&#20013;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31867;&#20248;&#20808;&#32423;&#20989;&#25968;&#33021;&#22815;&#24341;&#36215;&#34892;&#20026;&#31574;&#30053;&#30340;&#25913;&#21892;&#65292;&#24403;&#31574;&#30053;&#32422;&#26463;&#21040;&#36825;&#20010;&#25913;&#36827;&#30340;&#31574;&#30053;&#19978;&#26102;&#65292;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24456;&#21487;&#33021;&#24471;&#21040;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#31181;&#23454;&#29992;&#31574;&#30053;&#26469;&#33719;&#24471;&#22522;&#20110;&#25311;&#21512;&#20540;&#32593;&#32476;&#30340;&#20248;&#20808;&#26435;&#37325;&#65288;OPER-A&#65289;&#25110;&#32773;u
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) is challenged by the distributional shift problem. To address this problem, existing works mainly focus on designing sophisticated policy constraints between the learned policy and the behavior policy. However, these constraints are applied equally to well-performing and inferior actions through uniform sampling, which might negatively affect the learned policy. To alleviate this issue, we propose Offline Prioritized Experience Replay (OPER), featuring a class of priority functions designed to prioritize highly-rewarding transitions, making them more frequently visited during training. Through theoretical analysis, we show that this class of priority functions induce an improved behavior policy, and when constrained to this improved policy, a policy-constrained offline RL algorithm is likely to yield a better solution. We develop two practical strategies to obtain priority weights by estimating advantages based on a fitted value network (OPER-A) or u
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#20174;&#20687;&#32032;&#20013;&#25552;&#21462;&#27169;&#24335;&#30340;&#38382;&#39064;&#65292;&#24182;&#30740;&#31350;&#20102;&#29305;&#24449;&#21487;&#35270;&#21270;&#30340;&#21487;&#38752;&#24615;&#12290;&#23454;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#30001;&#20110;&#20248;&#21270;&#36807;&#31243;&#20013;&#22266;&#26377;&#30340;&#38480;&#21046;&#65292;&#29305;&#24449;&#21487;&#35270;&#21270;&#33021;&#22815;&#21487;&#38752;&#29702;&#35299;&#30340;&#21151;&#33021;&#38598;&#38750;&#24120;&#26377;&#38480;&#65292;&#23545;&#20110;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#22788;&#29702;&#33258;&#28982;&#22270;&#20687;&#30340;&#35299;&#37322;&#33021;&#21147;&#20135;&#29983;&#24576;&#30097;&#12290;</title><link>http://arxiv.org/abs/2306.04719</link><description>&lt;p&gt;
&#19981;&#35201;&#30456;&#20449;&#20320;&#30340;&#30524;&#30555;&#65306;&#20851;&#20110;&#29305;&#24449;&#21487;&#35270;&#21270;&#30340;&#65288;&#19981;&#65289;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Don't trust your eyes: on the (un)reliability of feature visualizations. (arXiv:2306.04719v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#20174;&#20687;&#32032;&#20013;&#25552;&#21462;&#27169;&#24335;&#30340;&#38382;&#39064;&#65292;&#24182;&#30740;&#31350;&#20102;&#29305;&#24449;&#21487;&#35270;&#21270;&#30340;&#21487;&#38752;&#24615;&#12290;&#23454;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#30001;&#20110;&#20248;&#21270;&#36807;&#31243;&#20013;&#22266;&#26377;&#30340;&#38480;&#21046;&#65292;&#29305;&#24449;&#21487;&#35270;&#21270;&#33021;&#22815;&#21487;&#38752;&#29702;&#35299;&#30340;&#21151;&#33021;&#38598;&#38750;&#24120;&#26377;&#38480;&#65292;&#23545;&#20110;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#22788;&#29702;&#33258;&#28982;&#22270;&#20687;&#30340;&#35299;&#37322;&#33021;&#21147;&#20135;&#29983;&#24576;&#30097;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#26159;&#22914;&#20309;&#20174;&#20687;&#32032;&#20013;&#25552;&#21462;&#27169;&#24335;&#30340;&#65311;&#29305;&#24449;&#21487;&#35270;&#21270;&#36890;&#36807;&#20248;&#21270;&#26469;&#21487;&#35270;&#21270;&#39640;&#28608;&#27963;&#30340;&#27169;&#24335;&#65292;&#35797;&#22270;&#22238;&#31572;&#36825;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#22914;&#20170;&#65292;&#21487;&#35270;&#21270;&#26041;&#27861;&#26500;&#25104;&#20102;&#25105;&#20204;&#23545;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#24037;&#20316;&#30340;&#20102;&#35299;&#30340;&#22522;&#30784;&#65292;&#20316;&#20026;&#19968;&#31181;&#26426;&#26800;&#24335;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#38382;&#65306;&#29305;&#24449;&#21487;&#35270;&#21270;&#26377;&#22810;&#21487;&#38752;&#65311;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;&#32593;&#32476;&#30005;&#36335;&#26469;&#35784;&#39575;&#29305;&#24449;&#21487;&#35270;&#21270;&#65292;&#20351;&#20854;&#26174;&#31034;&#23436;&#20840;&#19982;&#33258;&#28982;&#36755;&#20837;&#30340;&#27491;&#24120;&#32593;&#32476;&#34892;&#20026;&#27627;&#26080;&#32852;&#31995;&#30340;&#20219;&#24847;&#27169;&#24335;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#35777;&#25454;&#34920;&#26126;&#22312;&#26631;&#20934;&#65292;&#26410;&#25805;&#32437;&#32593;&#32476;&#20013;&#21457;&#29983;&#20102;&#31867;&#20284;&#30340;&#29616;&#35937;&#65306;&#29305;&#24449;&#21487;&#35270;&#21270;&#19982;&#26631;&#20934;&#36755;&#20837;&#22788;&#29702;&#38750;&#24120;&#19981;&#21516;&#65292;&#23545;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#22788;&#29702;&#33258;&#28982;&#22270;&#20687;&#30340;&#35299;&#37322;&#33021;&#21147;&#20135;&#29983;&#24576;&#30097;&#12290;&#25105;&#20204;&#36890;&#36807;&#29702;&#35770;&#35777;&#26126;&#25903;&#25745;&#36825;&#19968;&#32463;&#39564;&#21457;&#29616;&#65292;&#30001;&#20110;&#20248;&#21270;&#36807;&#31243;&#20013;&#22266;&#26377;&#30340;&#38480;&#21046;&#65292;&#21487;&#20197;&#36890;&#36807;&#29305;&#24449;&#21487;&#35270;&#21270;&#21487;&#38752;&#29702;&#35299;&#30340;&#21151;&#33021;&#38598;&#26497;&#20854;&#26377;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
How do neural networks extract patterns from pixels? Feature visualizations attempt to answer this important question by visualizing highly activating patterns through optimization. Today, visualization methods form the foundation of our knowledge about the internal workings of neural networks, as a type of mechanistic interpretability. Here we ask: How reliable are feature visualizations? We start our investigation by developing network circuits that trick feature visualizations into showing arbitrary patterns that are completely disconnected from normal network behavior on natural input. We then provide evidence for a similar phenomenon occurring in standard, unmanipulated networks: feature visualizations are processed very differently from standard input, casting doubt on their ability to "explain" how neural networks process natural images. We underpin this empirical finding by theory proving that the set of functions that can be reliably understood by feature visualization is extr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#29575;&#35748;&#30693;&#27169;&#22411;&#65292;&#31216;&#20026;&#26377;&#38480;&#23454;&#29992;&#35828;&#35805;&#32773;&#65292;&#29992;&#20110;&#34920;&#24449;&#19981;&#21516;&#21464;&#20307;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#25805;&#20316;&#26041;&#24335;&#12290;&#32463;&#36807;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#27010;&#24565;&#19978;&#31867;&#20284;&#20110; &#24555;&#19982;&#24930;&#24605;&#32771;&#27169;&#22411;&#30340;&#24605;&#32500;&#27169;&#22411;&#65292;&#32780;&#36825;&#31181;&#24605;&#32500;&#27169;&#22411;&#34987;&#24402;&#22240;&#20110;&#20154;&#31867;&#12290;&#27492;&#30740;&#31350;&#20984;&#26174;&#20102;&#37319;&#29992;&#35748;&#30693;&#27010;&#29575;&#24314;&#27169;&#26041;&#27861;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#12289;&#35780;&#20272;&#21644;&#25512;&#36827;&#30340;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2305.17760</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26159;&#26377;&#38480;&#23454;&#29992;&#35828;&#35805;&#32773;
&lt;/p&gt;
&lt;p&gt;
Language Models are Bounded Pragmatic Speakers. (arXiv:2305.17760v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#29575;&#35748;&#30693;&#27169;&#22411;&#65292;&#31216;&#20026;&#26377;&#38480;&#23454;&#29992;&#35828;&#35805;&#32773;&#65292;&#29992;&#20110;&#34920;&#24449;&#19981;&#21516;&#21464;&#20307;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#25805;&#20316;&#26041;&#24335;&#12290;&#32463;&#36807;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#27010;&#24565;&#19978;&#31867;&#20284;&#20110; &#24555;&#19982;&#24930;&#24605;&#32771;&#27169;&#22411;&#30340;&#24605;&#32500;&#27169;&#22411;&#65292;&#32780;&#36825;&#31181;&#24605;&#32500;&#27169;&#22411;&#34987;&#24402;&#22240;&#20110;&#20154;&#31867;&#12290;&#27492;&#30740;&#31350;&#20984;&#26174;&#20102;&#37319;&#29992;&#35748;&#30693;&#27010;&#29575;&#24314;&#27169;&#26041;&#27861;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#12289;&#35780;&#20272;&#21644;&#25512;&#36827;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#29575;&#35748;&#30693;&#27169;&#22411;&#65292;&#31216;&#20026;&#26377;&#38480;&#23454;&#29992;&#35828;&#35805;&#32773;&#65292;&#29992;&#20110;&#34920;&#24449;&#19981;&#21516;&#21464;&#20307;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#25805;&#20316;&#26041;&#24335;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#32463;&#36807;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;Ouyang&#31561;&#20154;&#65292;2022&#65289;&#20855;&#26377;&#27010;&#24565;&#19978;&#31867;&#20284;&#20110; &#24555;&#19982;&#24930;&#24605;&#32771;&#27169;&#22411;&#65288;Kahneman&#65292;2011&#65289;&#30340;&#24605;&#32500;&#27169;&#22411;&#65292;&#32780;&#36825;&#31181;&#24605;&#32500;&#27169;&#22411;&#34987;&#24515;&#29702;&#23398;&#23478;&#20204;&#24402;&#22240;&#20110;&#20154;&#31867;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#20316;&#20026;&#24555;&#19982;&#24930;&#24605;&#32771;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#25193;&#23637;&#36825;&#20010;&#26694;&#26550;&#30340;&#36884;&#24452;&#12290;&#26412;&#30740;&#31350;&#23454;&#36136;&#19978;&#20984;&#26174;&#20102;&#37319;&#29992;&#35748;&#30693;&#27010;&#29575;&#24314;&#27169;&#26041;&#27861;&#26469;&#33719;&#24471;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#12289;&#35780;&#20272;&#21644;&#25512;&#36827;&#26041;&#38754;&#30340;&#28145;&#21051;&#35265;&#35299;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
How do language models "think"? This paper formulates a probabilistic cognitive model called the bounded pragmatic speaker, which can characterize the operation of different variations of language models. Specifically, we demonstrate that large language models fine-tuned with reinforcement learning from human feedback (Ouyang et al., 2022) embody a model of thought that conceptually resembles a fast-and-slow model (Kahneman, 2011), which psychologists have attributed to humans. We discuss the limitations of reinforcement learning from human feedback as a fast-and-slow model of thought and propose avenues for expanding this framework. In essence, our research highlights the value of adopting a cognitive probabilistic modeling approach to gain insights into the comprehension, evaluation, and advancement of language models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#25955;&#24335;&#23398;&#20064;&#22312;&#26080;&#32447;&#32593;&#32476;&#20013;&#30340;&#36890;&#20449;&#38382;&#39064;&#65292;&#21457;&#29616;&#20248;&#21270;&#25509;&#20837;&#27010;&#29575;&#20197;&#26368;&#22823;&#21270;&#25104;&#21151;&#38142;&#36335;&#25968;&#30340;&#26399;&#26395;&#20540;&#26159;&#25552;&#21319;&#31995;&#32479;&#25910;&#25947;&#36895;&#24230;&#30340;&#26377;&#25928;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2305.07368</link><description>&lt;p&gt;
&#26080;&#32447;&#32593;&#32476;&#20013;&#30340;&#20998;&#25955;&#24335;&#23398;&#20064;: &#24191;&#25773;&#19982;&#38543;&#26426;&#25509;&#20837;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Decentralized Learning over Wireless Networks: The Effect of Broadcast with Random Access. (arXiv:2305.07368v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#25955;&#24335;&#23398;&#20064;&#22312;&#26080;&#32447;&#32593;&#32476;&#20013;&#30340;&#36890;&#20449;&#38382;&#39064;&#65292;&#21457;&#29616;&#20248;&#21270;&#25509;&#20837;&#27010;&#29575;&#20197;&#26368;&#22823;&#21270;&#25104;&#21151;&#38142;&#36335;&#25968;&#30340;&#26399;&#26395;&#20540;&#26159;&#25552;&#21319;&#31995;&#32479;&#25910;&#25947;&#36895;&#24230;&#30340;&#26377;&#25928;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#20998;&#25955;&#24335;&#23398;&#20064;&#30340;&#36890;&#20449;&#26041;&#38754;&#65292;&#28041;&#21450;&#22810;&#20010;&#20195;&#29702;&#20351;&#29992;&#20998;&#25955;&#24335;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;(D-SGD)&#22312;&#20998;&#24067;&#24335;&#25968;&#25454;&#19978;&#20849;&#21516;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36807;&#31243;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#26080;&#32447;&#20449;&#36947;&#30340;&#24191;&#25773;&#24615;&#36136;&#20197;&#21450;&#36890;&#20449;&#25299;&#25169;&#20013;&#30340;&#38142;&#36335;&#21160;&#24577;&#65292;&#25506;&#31350;&#24191;&#25773;&#20256;&#36755;&#21644;&#27010;&#29575;&#24615;&#38543;&#26426;&#25509;&#20837;&#31574;&#30053;&#23545;D-SGD&#25910;&#25947;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20248;&#21270;&#25509;&#20837;&#27010;&#29575;&#20197;&#26368;&#22823;&#21270;&#25104;&#21151;&#38142;&#36335;&#25968;&#30340;&#26399;&#26395;&#20540;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#25552;&#21319;&#31995;&#32479;&#25910;&#25947;&#36895;&#24230;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we focus on the communication aspect of decentralized learning, which involves multiple agents training a shared machine learning model using decentralized stochastic gradient descent (D-SGD) over distributed data. In particular, we investigate the impact of broadcast transmission and probabilistic random access policy on the convergence performance of D-SGD, considering the broadcast nature of wireless channels and the link dynamics in the communication topology. Our results demonstrate that optimizing the access probability to maximize the expected number of successful links is a highly effective strategy for accelerating the system convergence.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SocNavGym&#65292;&#23545;&#20110;&#31038;&#20132;&#23548;&#33322;&#39046;&#22495;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#36731;&#20415;&#12289;&#24555;&#36895;&#12289;&#26131;&#29992;&#30340;&#20223;&#30495;&#29615;&#22659;&#65292;&#21487;&#29983;&#25104;&#21508;&#31181;&#21508;&#26679;&#30340;&#31038;&#20132;&#23548;&#33322;&#22330;&#26223;&#65292;&#24182;&#20419;&#36827;&#20102;&#26234;&#33021;&#31038;&#20132;&#26426;&#22120;&#20154;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2304.14102</link><description>&lt;p&gt;
SocNavGym&#65306;&#19968;&#20010;&#38024;&#23545;&#31038;&#20132;&#23548;&#33322;&#30340;&#24378;&#21270;&#23398;&#20064;&#20223;&#30495;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
SocNavGym: A Reinforcement Learning Gym for Social Navigation. (arXiv:2304.14102v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14102
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SocNavGym&#65292;&#23545;&#20110;&#31038;&#20132;&#23548;&#33322;&#39046;&#22495;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#36731;&#20415;&#12289;&#24555;&#36895;&#12289;&#26131;&#29992;&#30340;&#20223;&#30495;&#29615;&#22659;&#65292;&#21487;&#29983;&#25104;&#21508;&#31181;&#21508;&#26679;&#30340;&#31038;&#20132;&#23548;&#33322;&#22330;&#26223;&#65292;&#24182;&#20419;&#36827;&#20102;&#26234;&#33021;&#31038;&#20132;&#26426;&#22120;&#20154;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#21475;&#23494;&#38598;&#30340;&#29615;&#22659;&#19979;&#65292;&#33258;&#20027;&#26426;&#22120;&#20154;&#22312;&#23548;&#33322;&#26102;&#38656;&#35201;&#36981;&#23432;&#31038;&#20132;&#35268;&#33539;&#12290;&#26426;&#22120;&#23398;&#20064;&#65292;&#23588;&#20854;&#26159;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65292;&#26368;&#36817;&#22312;&#31038;&#20132;&#23548;&#33322;&#39046;&#22495;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#36825;&#21487;&#20197;&#37096;&#20998;&#24402;&#22240;&#20110;&#29983;&#25104;&#30340;&#31574;&#30053;&#19981;&#21463;&#20195;&#30721;&#22797;&#26434;&#24615;&#25110;&#22788;&#29702;&#30340;&#21464;&#37327;&#25968;&#37327;&#31561;&#20154;&#31867;&#38480;&#21046;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;DRL&#31639;&#27861;&#32570;&#20047;&#23433;&#20840;&#20445;&#38556;&#65292;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#38656;&#27714;&#65292;&#23548;&#33268;&#22312;&#29616;&#23454;&#29615;&#22659;&#20013;&#30340;&#24212;&#29992;&#19981;&#22826;&#20999;&#23454;&#38469;&#12290;&#20026;&#20102;&#32553;&#23567;&#36825;&#19968;&#24046;&#36317;&#65292;&#20223;&#30495;&#29615;&#22659;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;SocNavGym&#65292;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#31038;&#20132;&#23548;&#33322;&#30340;&#20808;&#36827;&#20223;&#30495;&#29615;&#22659;&#65292;&#21487;&#20197;&#29983;&#25104;&#21508;&#31181;&#21508;&#26679;&#30340;&#31038;&#20132;&#23548;&#33322;&#22330;&#26223;&#65292;&#24182;&#20419;&#36827;&#26234;&#33021;&#31038;&#20132;&#26426;&#22120;&#20154;&#30340;&#21457;&#23637;&#12290;SocNavGym&#36731;&#20415;&#12289;&#24555;&#36895;&#12289;&#26131;&#20110;&#20351;&#29992;&#65292;&#24182;&#21487;&#36731;&#26494;&#37197;&#32622;&#20197;&#29983;&#25104;&#19981;&#21516;&#31867;&#22411;&#30340;&#31038;&#20132;&#23548;&#33322;&#22330;&#26223;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#21487;&#20197;&#37197;&#32622;&#20026;&#20351;&#29992;&#19981;&#21516;&#30340;&#20256;&#24863;&#22120;&#65292;&#24182;&#25903;&#25345;&#26080;&#38556;&#30861;&#29615;&#22659;&#30340;&#20223;&#30495;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is essential for autonomous robots to be socially compliant while navigating in human-populated environments. Machine Learning and, especially, Deep Reinforcement Learning have recently gained considerable traction in the field of Social Navigation. This can be partially attributed to the resulting policies not being bound by human limitations in terms of code complexity or the number of variables that are handled. Unfortunately, the lack of safety guarantees and the large data requirements by DRL algorithms make learning in the real world unfeasible. To bridge this gap, simulation environments are frequently used. We propose SocNavGym, an advanced simulation environment for social navigation that can generate a wide variety of social navigation scenarios and facilitates the development of intelligent social agents. SocNavGym is light-weight, fast, easy-to-use, and can be effortlessly configured to generate different types of social navigation scenarios. It can also be configured to
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36801;&#31227;&#23398;&#20064;&#27969;&#31243;&#65292;&#36890;&#36807;&#32472;&#21046;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#23376;&#32593;&#32476;&#65292;&#25913;&#36827;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#20256;&#36755;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.11834</link><description>&lt;p&gt;
&#40065;&#26834;&#38376;&#31080;&#33021;&#22815;&#26356;&#22909;&#22320;&#20256;&#36755;: &#22312;&#36801;&#31227;&#23398;&#20064;&#20013;&#32472;&#21046;&#26356;&#20855;&#20256;&#36755;&#24615;&#30340;&#23376;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Robust Tickets Can Transfer Better: Drawing More Transferable Subnetworks in Transfer Learning. (arXiv:2304.11834v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11834
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36801;&#31227;&#23398;&#20064;&#27969;&#31243;&#65292;&#36890;&#36807;&#32472;&#21046;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#23376;&#32593;&#32476;&#65292;&#25913;&#36827;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#20256;&#36755;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36801;&#31227;&#23398;&#20064;&#21033;&#29992;&#22312;&#20016;&#23500;&#25968;&#25454;&#30340;&#28304;&#20219;&#21153;&#19978;&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#20197;&#36171;&#20104;&#19979;&#28216;&#20219;&#21153;&#30340;&#26377;&#25928;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#24448;&#24448;&#36807;&#20110;&#24222;&#22823;&#65292;&#26080;&#27861;&#25552;&#20379;&#21487;&#25512;&#24191;&#30340;&#34920;&#31034;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#37096;&#32626;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36801;&#31227;&#23398;&#20064;&#27969;&#31243;&#65292;&#21033;&#29992;&#25105;&#20204;&#30340;&#21457;&#29616;&#65306;&#40065;&#26834;&#38376;&#31080;&#33021;&#22815;&#26356;&#22909;&#22320;&#20256;&#36755;&#65292;&#21363;&#36890;&#36807;&#36866;&#24403;&#24341;&#20837;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#26041;&#24335;&#32472;&#21046;&#30340;&#23376;&#32593;&#32476;&#21487;&#20197;&#22312;&#20256;&#32479;&#30340;&#24425;&#31080;&#38376;&#31080;&#23376;&#32593;&#32476;&#19978;&#21462;&#24471;&#26356;&#22909;&#30340;&#20256;&#36755;&#33021;&#21147;&#12290;&#22823;&#37327;&#23454;&#39564;&#21644;&#28040;&#34701;&#30740;&#31350;&#39564;&#35777;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#36801;&#31227;&#23398;&#20064;&#27969;&#31243;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#21644;&#31232;&#30095;&#27169;&#24335;&#20013;&#37117;&#33021;&#23454;&#29616;&#22686;&#24378;&#30340;&#20934;&#30830;&#24230;-&#31232;&#30095;&#24230;&#26435;&#34913;&#65292;&#36827;&#19968;&#27493;&#20016;&#23500;&#20102;&#24425;&#31080;&#38376;&#31080;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning leverages feature representations of deep neural networks (DNNs) pretrained on source tasks with rich data to empower effective finetuning on downstream tasks. However, the pretrained models are often prohibitively large for delivering generalizable representations, which limits their deployment on edge devices with constrained resources. To close this gap, we propose a new transfer learning pipeline, which leverages our finding that robust tickets can transfer better, i.e., subnetworks drawn with properly induced adversarial robustness can win better transferability over vanilla lottery ticket subnetworks. Extensive experiments and ablation studies validate that our proposed transfer learning pipeline can achieve enhanced accuracy-sparsity trade-offs across both diverse downstream tasks and sparsity patterns, further enriching the lottery ticket hypothesis.
&lt;/p&gt;</description></item><item><title>&#22312;&#32447;&#22810;&#31867;&#23398;&#20064;&#38382;&#39064;&#20013;&#65292;&#20351;&#29992;Multiclass Littlestone&#32500;&#24230;&#21487;&#20197;&#21051;&#30011;&#26631;&#31614;&#25968;&#30446;&#20026;&#26080;&#30028;&#24773;&#20917;&#19979;&#30340;&#21487;&#23398;&#20064;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.17716</link><description>&lt;p&gt;
&#22312;&#32447;&#22810;&#31867;&#21487;&#23398;&#20064;&#24615;&#30340;&#21051;&#30011;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Characterization of Online Multiclass Learnability. (arXiv:2303.17716v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17716
&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#22810;&#31867;&#23398;&#20064;&#38382;&#39064;&#20013;&#65292;&#20351;&#29992;Multiclass Littlestone&#32500;&#24230;&#21487;&#20197;&#21051;&#30011;&#26631;&#31614;&#25968;&#30446;&#20026;&#26080;&#30028;&#24773;&#20917;&#19979;&#30340;&#21487;&#23398;&#20064;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#24403;&#26631;&#31614;&#25968;&#30446;&#26159;&#26080;&#30028;&#30340;&#26102;&#20505;&#22312;&#32447;&#22810;&#31867;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Multiclass Littlestone&#32500;&#24230;&#65292;&#36825;&#20010;&#27010;&#24565;&#39318;&#27425;&#20986;&#29616;&#22312;\cite{DanielyERMprinciple}&#20013;&#65292;&#32487;&#32493;&#21051;&#30011;&#20102;&#35813;&#22330;&#26223;&#19979;&#30340;&#22312;&#32447;&#21487;&#23398;&#20064;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34917;&#20805;&#20102;&#26368;&#36817;&#30340;&#24037;&#20316;&#65292;\cite{Brukhimetal2022}&#32473;&#20986;&#20102;&#24403;&#26631;&#31614;&#31354;&#38388;&#26159;&#26080;&#30028;&#30340;&#24773;&#20917;&#19979;&#25209;&#22788;&#29702;&#22810;&#31867;&#21487;&#23398;&#20064;&#24615;&#30340;&#21051;&#30011;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of online multiclass learning when the number of labels is unbounded. We show that the Multiclass Littlestone dimension, first introduced in \cite{DanielyERMprinciple}, continues to characterize online learnability in this setting. Our result complements the recent work by \cite{Brukhimetal2022} who give a characterization of batch multiclass learnability when the label space is unbounded.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Uni-Mol+&#30340;&#26032;&#26041;&#27861;&#26469;&#39640;&#31934;&#24230;&#39044;&#27979;&#37327;&#23376;&#21270;&#23398;&#23646;&#24615;&#65292;&#23427;&#33021;&#22815;&#20174;2D&#20998;&#23376;&#22270;&#33258;&#21160;&#29983;&#25104;3D&#26500;&#35937;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#20248;&#21270;&#24471;&#21040;&#20248;&#21270;&#21518;&#30340;&#26500;&#35937;&#65292;&#20026;&#39044;&#27979;QC&#23646;&#24615;&#25552;&#20379;&#26356;&#21152;&#20934;&#30830;&#30340;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2303.16982</link><description>&lt;p&gt;
&#20351;&#29992;Uni-Mol+&#23454;&#29616;&#39640;&#31934;&#24230;&#30340;&#37327;&#23376;&#21270;&#23398;&#23646;&#24615;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Highly Accurate Quantum Chemical Property Prediction with Uni-Mol+. (arXiv:2303.16982v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16982
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Uni-Mol+&#30340;&#26032;&#26041;&#27861;&#26469;&#39640;&#31934;&#24230;&#39044;&#27979;&#37327;&#23376;&#21270;&#23398;&#23646;&#24615;&#65292;&#23427;&#33021;&#22815;&#20174;2D&#20998;&#23376;&#22270;&#33258;&#21160;&#29983;&#25104;3D&#26500;&#35937;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#20248;&#21270;&#24471;&#21040;&#20248;&#21270;&#21518;&#30340;&#26500;&#35937;&#65292;&#20026;&#39044;&#27979;QC&#23646;&#24615;&#25552;&#20379;&#26356;&#21152;&#20934;&#30830;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#21457;&#23637;&#22312;&#21152;&#36895;&#39044;&#27979;&#37327;&#23376;&#21270;&#23398;&#65288;QC&#65289;&#23646;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#23427;&#28040;&#38500;&#20102;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#31561;&#26114;&#36149;&#30005;&#23376;&#32467;&#26500;&#35745;&#31639;&#30340;&#38656;&#27714;&#12290;&#20294;&#26159;&#65292;&#20197;1D SMILES&#24207;&#21015;&#25110;2D&#20998;&#23376;&#22270;&#20026;&#22522;&#30784;&#30340;&#20043;&#21069;&#30340;&#26041;&#27861;&#26410;&#33021;&#23454;&#29616;&#39640;&#31934;&#24230;&#65292;&#22240;&#20026;QC&#23646;&#24615;&#20027;&#35201;&#21462;&#20915;&#20110;&#32463;&#30005;&#23376;&#32467;&#26500;&#26041;&#27861;&#20248;&#21270;&#30340;3D&#24179;&#34913;&#26500;&#35937;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Uni-Mol+&#30340;&#26032;&#26041;&#27861;&#26469;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#32473;&#23450;&#19968;&#20010;2D&#20998;&#23376;&#22270;&#65292;Uni-Mol+&#20174;RDKit&#31561;&#24265;&#20215;&#26041;&#27861;&#29983;&#25104;&#21021;&#22987;&#30340;3D&#26500;&#35937;&#12290;&#28982;&#21518;&#65292;&#21021;&#22987;&#26500;&#35937;&#34987;&#36845;&#20195;&#22320;&#20248;&#21270;&#21040;&#20854;&#24179;&#34913;&#26500;&#35937;&#65292;&#24182;&#36827;&#19968;&#27493;&#29992;&#20110;&#39044;&#27979;QC&#23646;&#24615;&#12290;&#25152;&#26377;&#36825;&#20123;&#27493;&#39588;&#37117;&#26159;&#20351;&#29992;Transformer&#27169;&#22411;&#33258;&#21160;&#23398;&#20064;&#30340;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#20248;&#21270;&#21518;&#30340;&#26500;&#35937;&#36136;&#37327;&#23545;QC&#23646;&#24615;&#39044;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent developments in deep learning have made remarkable progress in speeding up the prediction of quantum chemical (QC) properties by removing the need for expensive electronic structure calculations like density functional theory. However, previous methods that relied on 1D SMILES sequences or 2D molecular graphs failed to achieve high accuracy as QC properties are primarily dependent on the 3D equilibrium conformations optimized by electronic structure methods. In this paper, we propose a novel approach called Uni-Mol+ to tackle this challenge. Firstly, given a 2D molecular graph, Uni-Mol+ generates an initial 3D conformation from inexpensive methods such as RDKit. Then, the initial conformation is iteratively optimized to its equilibrium conformation, and the optimized conformation is further used to predict the QC properties. All these steps are automatically learned using Transformer models. We observed the quality of the optimized conformation is crucial for QC property predict
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#30340;&#21160;&#21147;&#31995;&#32479;&#20998;&#26512;&#26041;&#27861;&#65292;&#20351;&#29992;&#36741;&#21161;&#20989;&#25968;&#20316;&#20026;Koopman&#21487;&#35266;&#27979;&#37327;&#65292;&#19981;&#38656;&#35201;&#26126;&#30830;&#30340;&#27169;&#22411;&#21457;&#29616;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#21160;&#21147;&#23398;&#65292;&#20855;&#26377;&#25910;&#25947;&#24615;&#21644;&#24615;&#33021;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2303.01483</link><description>&lt;p&gt;
&#36741;&#21161;&#20989;&#25968;&#20316;&#20026;Koopman&#21487;&#35266;&#27979;&#37327;&#65306;&#22522;&#20110;&#25968;&#25454;&#30340;&#21160;&#21147;&#31995;&#32479;&#22810;&#39033;&#24335;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Auxiliary Functions as Koopman Observables: Data-Driven Polynomial Optimization for Dynamical Systems. (arXiv:2303.01483v2 [math.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01483
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#30340;&#21160;&#21147;&#31995;&#32479;&#20998;&#26512;&#26041;&#27861;&#65292;&#20351;&#29992;&#36741;&#21161;&#20989;&#25968;&#20316;&#20026;Koopman&#21487;&#35266;&#27979;&#37327;&#65292;&#19981;&#38656;&#35201;&#26126;&#30830;&#30340;&#27169;&#22411;&#21457;&#29616;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#21160;&#21147;&#23398;&#65292;&#20855;&#26377;&#25910;&#25947;&#24615;&#21644;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#22522;&#20110;&#25968;&#25454;&#30340;&#21160;&#21147;&#31995;&#32479;&#20998;&#26512;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#26126;&#30830;&#30340;&#27169;&#22411;&#21457;&#29616;&#12290;&#35813;&#26041;&#27861;&#28304;&#20110;&#20174;&#25968;&#25454;&#20013;&#36924;&#36817;Koopman&#31639;&#23376;&#30340;&#25216;&#26415;&#65292;&#24182;&#19988;&#20197;&#19968;&#20010;&#21487;&#20197;&#36890;&#36807;&#25968;&#20540;&#27714;&#35299;&#30340;&#21322;&#23450;&#35268;&#21010;&#38382;&#39064;&#26469;&#23454;&#29616;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#19981;&#20851;&#24515;&#25968;&#25454;&#26159;&#36890;&#36807;&#30830;&#23450;&#24615;&#36824;&#26159;&#38543;&#26426;&#36807;&#31243;&#29983;&#25104;&#30340;&#65292;&#22240;&#27492;&#29992;&#25143;&#26080;&#38656;&#36827;&#34892;&#20219;&#20309;&#35843;&#25972;&#21363;&#21487;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;&#24773;&#20917;&#12290;&#20005;&#26684;&#30340;&#25910;&#25947;&#24615;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#23558;&#25991;&#29486;&#20013;&#31867;&#20284;&#30340;&#32467;&#26524;&#36827;&#34892;&#20102;&#25193;&#23637;&#21644;&#32479;&#19968;&#12290;&#36890;&#36807;&#23545;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#21160;&#21147;&#23398;&#30340;&#21560;&#24341;&#23376;&#19978;&#21457;&#29616;Lyapunov&#20989;&#25968;&#12289;&#25191;&#34892;&#36941;&#21382;&#20248;&#21270;&#20197;&#21450;&#30028;&#23450;&#26497;&#20540;&#30340;&#31034;&#20363;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a flexible data-driven method for dynamical system analysis that does not require explicit model discovery. The method is rooted in well-established techniques for approximating the Koopman operator from data and is implemented as a semidefinite program that can be solved numerically. Furthermore, the method is agnostic of whether data is generated through a deterministic or stochastic process, so its implementation requires no prior adjustments by the user to accommodate these different scenarios. Rigorous convergence results justify the applicability of the method, while also extending and uniting similar results from across the literature. Examples on discovering Lyapunov functions, performing ergodic optimization, and bounding extrema over attractors for both deterministic and stochastic dynamics exemplify these convergence results and demonstrate the performance of the method.
&lt;/p&gt;</description></item><item><title>&#36830;&#32493;&#26102;&#38388;&#21151;&#33021;&#25193;&#25955;&#36807;&#31243;&#24341;&#20837;&#20102;&#21151;&#33021;&#25193;&#25955;&#36807;&#31243;&#65288;FDPs&#65289;&#65292;&#23558;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#25512;&#24191;&#21040;&#26080;&#38480;&#32500;&#20989;&#25968;&#31354;&#38388;&#12290;&#36890;&#36807;&#20351;&#29992;&#26032;&#30340;&#25968;&#23398;&#26694;&#26550;&#21644;&#25193;&#23637;&#65292;FDPs&#21487;&#20197;&#22312;&#20989;&#25968;&#31354;&#38388;&#20013;&#26500;&#24314;&#26032;&#22411;&#29983;&#25104;&#27169;&#22411;&#65292;&#22312;&#22788;&#29702;&#36830;&#32493;&#25968;&#25454;&#26102;&#33021;&#22815;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#29983;&#25104;&#65292;&#25152;&#38656;&#21442;&#25968;&#25968;&#37327;&#27604;&#29616;&#26377;&#27169;&#22411;&#20302;&#20960;&#20010;&#25968;&#37327;&#32423;&#12290;</title><link>http://arxiv.org/abs/2303.00800</link><description>&lt;p&gt;
&#36830;&#32493;&#26102;&#38388;&#21151;&#33021;&#25193;&#25955;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Continuous-Time Functional Diffusion Processes. (arXiv:2303.00800v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00800
&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#26102;&#38388;&#21151;&#33021;&#25193;&#25955;&#36807;&#31243;&#24341;&#20837;&#20102;&#21151;&#33021;&#25193;&#25955;&#36807;&#31243;&#65288;FDPs&#65289;&#65292;&#23558;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#25512;&#24191;&#21040;&#26080;&#38480;&#32500;&#20989;&#25968;&#31354;&#38388;&#12290;&#36890;&#36807;&#20351;&#29992;&#26032;&#30340;&#25968;&#23398;&#26694;&#26550;&#21644;&#25193;&#23637;&#65292;FDPs&#21487;&#20197;&#22312;&#20989;&#25968;&#31354;&#38388;&#20013;&#26500;&#24314;&#26032;&#22411;&#29983;&#25104;&#27169;&#22411;&#65292;&#22312;&#22788;&#29702;&#36830;&#32493;&#25968;&#25454;&#26102;&#33021;&#22815;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#29983;&#25104;&#65292;&#25152;&#38656;&#21442;&#25968;&#25968;&#37327;&#27604;&#29616;&#26377;&#27169;&#22411;&#20302;&#20960;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#21151;&#33021;&#25193;&#25955;&#36807;&#31243;&#65288;FDPs&#65289;&#65292;&#23558;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#25512;&#24191;&#21040;&#26080;&#38480;&#32500;&#20989;&#25968;&#31354;&#38388;&#12290; FDPs&#38656;&#35201;&#19968;&#31181;&#26032;&#30340;&#25968;&#23398;&#26694;&#26550;&#26469;&#25551;&#36848;&#21069;&#21521;&#21644;&#21453;&#21521;&#21160;&#21147;&#23398;&#65292;&#24182;&#36827;&#34892;&#22810;&#20010;&#25193;&#23637;&#20197;&#24471;&#20986;&#23454;&#38469;&#30340;&#35757;&#32451;&#30446;&#26631;&#12290;&#36825;&#20123;&#25193;&#23637;&#21253;&#25324;Girsanov&#23450;&#29702;&#30340;&#26080;&#38480;&#32500;&#29256;&#26412;&#65292;&#20197;&#20415;&#33021;&#22815;&#35745;&#31639;ELBO&#65292;&#20197;&#21450;&#37319;&#26679;&#23450;&#29702;&#30340;&#26080;&#38480;&#32500;&#29256;&#26412;&#65292;&#20197;&#30830;&#20445;&#21487;&#25968;&#20010;&#28857;&#19978;&#30340;&#20989;&#25968;&#35780;&#20272;&#31561;&#20215;&#20110;&#26080;&#38480;&#32500;&#20989;&#25968;&#12290;&#25105;&#20204;&#20351;&#29992;FDPs&#22312;&#20989;&#25968;&#31354;&#38388;&#20013;&#26500;&#24314;&#20102;&#19968;&#31181;&#26032;&#22411;&#29983;&#25104;&#27169;&#22411;&#65292;&#19981;&#38656;&#35201;&#19987;&#38376;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#24182;&#19988;&#21487;&#20197;&#22788;&#29702;&#20219;&#20309;&#31867;&#22411;&#30340;&#36830;&#32493;&#25968;&#25454;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#20351;&#29992;&#31616;&#21333;&#30340;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#32467;&#26500;&#65292;FDPs&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#29983;&#25104;&#65292;&#25152;&#38656;&#30340;&#21442;&#25968;&#25968;&#37327;&#27604;&#29616;&#26377;&#30340;&#25193;&#25955;&#27169;&#22411;&#20302;&#20960;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Functional Diffusion Processes (FDPs), which generalize score-based diffusion models to infinite-dimensional function spaces. FDPs require a new mathematical framework to describe the forward and backward dynamics, and several extensions to derive practical training objectives. These include infinite-dimensional versions of Girsanov theorem, in order to be able to compute an ELBO, and of the sampling theorem, in order to guarantee that functional evaluations in a countable set of points are equivalent to infinite-dimensional functions. We use FDPs to build a new breed of generative models in function spaces, which do not require specialized network architectures, and that can work with any kind of continuous data. Our results on real data show that FDPs achieve high-quality image generation, using a simple MLP architecture with orders of magnitude fewer parameters than existing diffusion models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#22797;&#21512;&#20248;&#21270;&#31639;&#27861;&#35299;&#20915;Sigmoid&#32593;&#32476;&#38382;&#39064;&#65292;&#21487;&#20197;&#22312;&#38750;&#20984;&#21644;&#38750;&#20809;&#28369;&#38382;&#39064;&#30340;&#24773;&#20917;&#19979;&#25910;&#25947;&#21040;&#20840;&#23616;&#26368;&#20248;&#35299;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#33324;&#25351;&#23548;&#26469;&#35774;&#32622;&#32593;&#32476;&#22823;&#23567;&#12290;</title><link>http://arxiv.org/abs/2303.00589</link><description>&lt;p&gt;
&#22522;&#20110;&#22797;&#21512;&#20248;&#21270;&#31639;&#27861;&#30340;Sigmoid&#32593;&#32476;&#27714;&#35299;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Composite Optimization Algorithms for Sigmoid Networks. (arXiv:2303.00589v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00589
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#22797;&#21512;&#20248;&#21270;&#31639;&#27861;&#35299;&#20915;Sigmoid&#32593;&#32476;&#38382;&#39064;&#65292;&#21487;&#20197;&#22312;&#38750;&#20984;&#21644;&#38750;&#20809;&#28369;&#38382;&#39064;&#30340;&#24773;&#20917;&#19979;&#25910;&#25947;&#21040;&#20840;&#23616;&#26368;&#20248;&#35299;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#33324;&#25351;&#23548;&#26469;&#35774;&#32622;&#32593;&#32476;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#22797;&#21512;&#20248;&#21270;&#31639;&#27861;&#27714;&#35299;Sigmoid&#32593;&#32476;&#38382;&#39064;&#65292;&#23558;Sigmoid&#32593;&#32476;&#36716;&#21270;&#20026;&#19968;&#20010;&#20984;&#22797;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#32447;&#24615;&#21270;&#36817;&#31471;&#31639;&#27861;&#21644;&#20132;&#26367;&#26041;&#21521;&#20056;&#23376;&#27861;&#30340;&#22797;&#21512;&#20248;&#21270;&#31639;&#27861;&#12290;&#22312;&#20551;&#35774;&#20855;&#26377;&#24369;&#38160;&#26368;&#23567;&#20540;&#21644;&#27491;&#21017;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#22312;&#38750;&#20984;&#21644;&#38750;&#20809;&#28369;&#38382;&#39064;&#30340;&#24773;&#20917;&#19979;&#25910;&#25947;&#21040;&#30446;&#26631;&#20989;&#25968;&#30340;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;&#27492;&#22806;&#65292;&#25910;&#25947;&#32467;&#26524;&#21487;&#20197;&#30452;&#25509;&#19982;&#35757;&#32451;&#25968;&#25454;&#37327;&#30456;&#20851;&#32852;&#65292;&#20026;&#35774;&#32622;Sigmoid&#32593;&#32476;&#30340;&#22823;&#23567;&#25552;&#20379;&#19968;&#33324;&#25351;&#23548;&#12290;&#22312;Franke&#20989;&#25968;&#25311;&#21512;&#21644;&#25163;&#20889;&#25968;&#23383;&#35782;&#21035;&#26041;&#38754;&#30340;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#34920;&#29616;&#33391;&#22909;&#19988;&#31283;&#20581;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we use composite optimization algorithms to solve sigmoid networks. We equivalently transfer the sigmoid networks to a convex composite optimization and propose the composite optimization algorithms based on the linearized proximal algorithms and the alternating direction method of multipliers. Under the assumptions of the weak sharp minima and the regularity condition, the algorithm is guaranteed to converge to a globally optimal solution of the objective function even in the case of non-convex and non-smooth problems. Furthermore, the convergence results can be directly related to the amount of training data and provide a general guide for setting the size of sigmoid networks. Numerical experiments on Franke's function fitting and handwritten digit recognition show that the proposed algorithms perform satisfactorily and robustly.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36816;&#21160;&#31995;&#32479;&#30340;&#31163;&#25955;&#24418;&#24577;&#23545;&#31216;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#21644;&#23454;&#36341;&#26694;&#26550;&#65292;&#29992;&#20110;&#35782;&#21035;&#31995;&#32479;&#30340;&#24418;&#24577;&#23545;&#31216;&#32676;&#65292;&#24182;&#20998;&#26512;&#23545;&#31216;&#24615;&#22312;&#25968;&#25454;&#22686;&#24378;&#21644;&#25511;&#21046;&#35774;&#35745;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2302.10433</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#31163;&#25955;&#23545;&#31216;&#24615;: &#22522;&#20110;&#32676;&#35770;&#21644;&#25968;&#25454;&#39537;&#21160;&#20998;&#26512;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On discrete symmetries of robotics systems: A group-theoretic and data-driven analysis. (arXiv:2302.10433v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10433
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36816;&#21160;&#31995;&#32479;&#30340;&#31163;&#25955;&#24418;&#24577;&#23545;&#31216;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#21644;&#23454;&#36341;&#26694;&#26550;&#65292;&#29992;&#20110;&#35782;&#21035;&#31995;&#32479;&#30340;&#24418;&#24577;&#23545;&#31216;&#32676;&#65292;&#24182;&#20998;&#26512;&#23545;&#31216;&#24615;&#22312;&#25968;&#25454;&#22686;&#24378;&#21644;&#25511;&#21046;&#35774;&#35745;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#36816;&#21160;&#31995;&#32479;&#30340;&#31163;&#25955;&#24418;&#24577;&#23545;&#31216;&#24615;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#36825;&#22312;&#29983;&#29289;&#21644;&#20154;&#24037;&#36816;&#21160;&#31995;&#32479;&#20013;&#32463;&#24120;&#35266;&#23519;&#21040;&#65292;&#20363;&#22914;&#22810;&#33151;&#12289;&#28216;&#27891;&#21644;&#39134;&#34892;&#30340;&#21160;&#29289;/&#26426;&#22120;&#20154;/&#34394;&#25311;&#35282;&#33394;&#12290;&#36825;&#20123;&#23545;&#31216;&#24615;&#28304;&#33258;&#31995;&#32479;&#24418;&#24577;&#20013;&#19968;&#20010;&#25110;&#22810;&#20010;&#24179;&#38754;/&#36724;&#30340;&#23545;&#31216;&#24615;&#23384;&#22312;&#65292;&#23548;&#33268;&#36523;&#20307;&#37096;&#20214;&#30340;&#35856;&#27874;&#22797;&#21046;&#21644;&#20998;&#24067;&#12290;&#25105;&#20204;&#38416;&#36848;&#20102;&#24418;&#24577;&#23545;&#31216;&#24615;&#22914;&#20309;&#24310;&#20280;&#21040;&#31995;&#32479;&#21160;&#21147;&#23398;&#12289;&#26368;&#20248;&#25511;&#21046;&#31574;&#30053;&#20197;&#21450;&#19982;&#31995;&#32479;&#21160;&#21147;&#23398;&#28436;&#21270;&#30456;&#20851;&#30340;&#25152;&#26377;&#26412;&#20307;&#24863;&#21644;&#22806;&#24863;&#27979;&#37327;&#20013;&#30340;&#23545;&#31216;&#24615;&#12290;&#22312;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#30340;&#32972;&#26223;&#19979;&#65292;&#23545;&#31216;&#24615;&#20195;&#34920;&#19968;&#31181;&#24402;&#32435;&#20559;&#32622;&#65292;&#35777;&#26126;&#20102;&#25968;&#25454;&#22686;&#24378;&#25110;&#23545;&#31216;&#20989;&#25968;&#36924;&#36817;&#22120;&#30340;&#20351;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#21644;&#23454;&#36341;&#26694;&#26550;&#65292;&#29992;&#20110;&#35782;&#21035;&#31995;&#32479;&#30340;&#24418;&#24577;&#23545;&#31216;&#32676;G&#24182;&#25551;&#36848;&#20854;&#22312;&#26412;&#20307;&#24863;&#21644;&#22806;&#24863;&#27979;&#37327;&#12289;&#26368;&#20248;&#25511;&#21046;&#31574;&#30053;&#20197;&#21450;&#31995;&#32479;&#21160;&#21147;&#23398;&#26041;&#38754;&#30340;&#23545;&#31216;&#24615;&#12290;&#35813;&#26694;&#26550;&#28041;&#21450;&#21040;&#25968;&#25454;&#39537;&#21160;&#21644;&#32676;&#35770;&#24037;&#20855;&#65292;&#20363;&#22914;&#20027;&#25104;&#20998;&#20998;&#26512;&#65292;&#32622;&#25442;&#27979;&#35797;&#21644;&#34920;&#31034;&#29702;&#35770;&#12290;&#25552;&#20986;&#30340;&#26041;&#27861;&#23398;&#36890;&#36807;&#30830;&#23450;&#19968;&#20010;&#20223;&#29983;&#33034;&#26894;&#26426;&#22120;&#20154;&#30340;&#23545;&#31216;&#32676;&#24182;&#20998;&#26512;&#20854;&#23545;&#25968;&#25454;&#22686;&#24378;&#21644;&#25511;&#21046;&#35774;&#35745;&#30340;&#24433;&#21709;&#26469;&#36827;&#34892;&#35828;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a comprehensive study on discrete morphological symmetries of dynamical systems, which are commonly observed in biological and artificial locomoting systems, such as legged, swimming, and flying animals/robots/virtual characters. These symmetries arise from the presence of one or more planes/axis of symmetry in the system's morphology, resulting in harmonious duplication and distribution of body parts. Significantly, we characterize how morphological symmetries extend to symmetries in the system's dynamics, optimal control policies, and in all proprioceptive and exteroceptive measurements related to the system's dynamics evolution. In the context of data-driven methods, symmetry represents an inductive bias that justifies the use of data augmentation or symmetric function approximators. To tackle this, we present a theoretical and practical framework for identifying the system's morphological symmetry group $\G$ and characterizing the symmetries in proprioceptive and exteroc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#40657;&#30418;&#27169;&#22411;&#20013;&#26500;&#24314;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23558;&#40657;&#30418;&#27169;&#22411;&#20998;&#25104;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#28151;&#21512;&#29289;&#21644;&#27531;&#24046;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#19968;&#38454;&#36923;&#36753;&#23545;&#21487;&#35299;&#37322;&#27169;&#22411;&#36827;&#34892;&#22522;&#26412;&#25512;&#29702;&#12290;&#27492;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#19988;&#20135;&#29983;&#39640;&#24230;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2302.10289</link><description>&lt;p&gt;
&#23558;&#40657;&#21283;&#23376;&#20998;&#35299;&#20026;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#28151;&#21512;&#29289;&#65306;&#36335;&#32447;&#35268;&#21010;&#65292;&#35299;&#37322;&#65292;&#37325;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dividing and Conquering a BlackBox to a Mixture of Interpretable Models: Route, Interpret, Repeat. (arXiv:2302.10289v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10289
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#40657;&#30418;&#27169;&#22411;&#20013;&#26500;&#24314;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23558;&#40657;&#30418;&#27169;&#22411;&#20998;&#25104;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#28151;&#21512;&#29289;&#21644;&#27531;&#24046;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#19968;&#38454;&#36923;&#36753;&#23545;&#21487;&#35299;&#37322;&#27169;&#22411;&#36827;&#34892;&#22522;&#26412;&#25512;&#29702;&#12290;&#27492;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#19988;&#20135;&#29983;&#39640;&#24230;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35774;&#35745;&#35201;&#20040;&#20174;&#35299;&#37322;&#24615;&#27169;&#22411;&#24320;&#22987;&#65292;&#35201;&#20040;&#20174;&#40657;&#30418;&#24320;&#22987;&#24182;&#20107;&#21518;&#35299;&#37322;&#12290;&#40657;&#30418;&#27169;&#22411;&#28789;&#27963;&#20294;&#38590;&#20197;&#35299;&#37322;&#65292;&#32780;&#35299;&#37322;&#24615;&#27169;&#22411;&#26412;&#36136;&#19978;&#26159;&#21487;&#35299;&#37322;&#30340;&#12290;&#28982;&#32780;&#65292;&#35299;&#37322;&#24615;&#27169;&#22411;&#38656;&#35201;&#24191;&#27867;&#30340;&#26426;&#22120;&#23398;&#20064;&#30693;&#35782;&#65292;&#24182;&#19988;&#24448;&#24448;&#27604;&#23427;&#20204;&#30340;&#40657;&#30418;&#21464;&#20307;&#19981;&#22815;&#28789;&#27963;&#21644;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#26088;&#22312;&#27169;&#31946;&#40657;&#30418;&#30340;&#20107;&#21518;&#35299;&#37322;&#21644;&#26500;&#24314;&#21487;&#35299;&#37322;&#27169;&#22411;&#20043;&#38388;&#30340;&#30028;&#38480;&#12290;&#25105;&#20204;&#20174;&#40657;&#30418;&#24320;&#22987;&#65292;&#36845;&#20195;&#22320;Carve&#20986;&#19968;&#31181;&#28151;&#21512;&#35299;&#37322;&#27169;&#22411;&#65288;MoIE&#65289;&#21644;&#19968;&#20010;&#27531;&#20313;&#32593;&#32476;&#12290;&#27599;&#20010;&#21487;&#35299;&#37322;&#27169;&#22411;&#19987;&#38376;&#22788;&#29702;&#19968;&#20010;&#26679;&#26412;&#23376;&#38598;&#65292;&#24182;&#20351;&#29992;&#19968;&#38454;&#36923;&#36753;(FOL)&#23545;&#20854;&#36827;&#34892;&#35299;&#37322;&#65292;&#20174;&#40657;&#30418;&#20013;&#25552;&#20379;&#22522;&#26412;&#25512;&#29702;&#27010;&#24565;&#12290;&#25105;&#20204;&#36890;&#36807;&#28789;&#27963;&#30340;&#27531;&#24046;&#36335;&#30001;&#20854;&#20313;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#22312;&#27531;&#36716;&#32593;&#32476;&#19978;&#37325;&#22797;&#35813;&#26041;&#27861;&#65292;&#30452;&#21040;&#25152;&#26377;&#21487;&#35299;&#37322;&#27169;&#22411;&#35299;&#37322;&#25152;&#38656;&#27604;&#20363;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#36335;&#32447;&#35268;&#21010;&#65292;&#35299;&#37322;&#21644;&#37325;&#22797;&#26041;&#27861;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#30446;&#21069;&#20960;&#31181;&#40657;&#21283;&#23376;&#27169;&#22411;&#35299;&#37322;&#26041;&#27861;&#65292;&#24182;&#20135;&#29983;&#39640;&#24230;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
ML model design either starts with an interpretable model or a Blackbox and explains it post hoc. Blackbox models are flexible but difficult to explain, while interpretable models are inherently explainable. Yet, interpretable models require extensive ML knowledge and tend to be less flexible and underperforming than their Blackbox variants. This paper aims to blur the distinction between a post hoc explanation of a Blackbox and constructing interpretable models. Beginning with a Blackbox, we iteratively carve out a mixture of interpretable experts (MoIE) and a residual network. Each interpretable model specializes in a subset of samples and explains them using First Order Logic (FOL), providing basic reasoning on concepts from the Blackbox. We route the remaining samples through a flexible residual. We repeat the method on the residual network until all the interpretable models explain the desired proportion of data. Our extensive experiments show that our route, interpret, and repeat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40657;&#30418;&#25209;&#37327;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20165;&#21033;&#29992;&#27169;&#22411;&#39044;&#27979;&#36827;&#34892;&#35780;&#20272;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#25193;&#23637;&#20102;&#19968;&#31995;&#21015;&#30333;&#30418;&#25209;&#37327;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.08981</link><description>&lt;p&gt;
&#40657;&#30418;&#25209;&#37327;&#20027;&#21160;&#23398;&#20064;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Black-Box Batch Active Learning for Regression. (arXiv:2302.08981v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08981
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40657;&#30418;&#25209;&#37327;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20165;&#21033;&#29992;&#27169;&#22411;&#39044;&#27979;&#36827;&#34892;&#35780;&#20272;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#25193;&#23637;&#20102;&#19968;&#31995;&#21015;&#30333;&#30418;&#25209;&#37327;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25209;&#37327;&#20027;&#21160;&#23398;&#20064;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#37325;&#22797;&#33719;&#21462;&#25968;&#25454;&#28857;&#30340;&#26631;&#31614;&#26469;&#39640;&#25928;&#22320;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#22823;&#35268;&#27169;&#30340;&#21021;&#22987;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#26368;&#36817;&#30340;&#25209;&#37327;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#37117;&#26159;&#30333;&#30418;&#26041;&#27861;&#65292;&#24182;&#19988;&#36890;&#24120;&#20165;&#38480;&#20110;&#21487;&#24494;&#20998;&#30340;&#21442;&#25968;&#27169;&#22411;&#65306;&#23427;&#20204;&#20351;&#29992;&#22522;&#20110;&#27169;&#22411;&#23884;&#20837;&#25110;&#19968;&#38454;&#21644;&#20108;&#38454;&#23548;&#25968;&#30340;&#33719;&#21462;&#20989;&#25968;&#23545;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#28857;&#36827;&#34892;&#35780;&#20998;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#40657;&#30418;&#25209;&#37327;&#20027;&#21160;&#23398;&#20064;&#22238;&#24402;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#20316;&#20026;&#30333;&#30418;&#26041;&#27861;&#30340;&#25193;&#23637;&#12290;&#20851;&#38190;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20165;&#20381;&#36182;&#20110;&#27169;&#22411;&#30340;&#39044;&#27979;&#12290;&#36825;&#31181;&#26041;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21253;&#25324;&#24120;&#35268;&#30340;&#21644;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20197;&#21450;&#38750;&#21487;&#24494;&#20998;&#27169;&#22411;&#65292;&#22914;&#38543;&#26426;&#26862;&#26519;&#12290;&#23427;&#22522;&#20110;&#36125;&#21494;&#26031;&#21407;&#21017;&#65292;&#24182;&#21033;&#29992;&#26368;&#36817;&#30340;&#22522;&#20110;&#26680;&#30340;&#26041;&#27861;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#25193;&#23637;&#19968;&#31995;&#21015;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#30333;&#30418;&#25209;&#37327;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65288;BADGE&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Batch active learning is a popular approach for efficiently training machine learning models on large, initially unlabelled datasets by repeatedly acquiring labels for batches of data points. However, many recent batch active learning methods are white-box approaches and are often limited to differentiable parametric models: they score unlabeled points using acquisition functions based on model embeddings or first- and second-order derivatives. In this paper, we propose black-box batch active learning for regression tasks as an extension of white-box approaches. Crucially, our method only relies on model predictions. This approach is compatible with a wide range of machine learning models, including regular and Bayesian deep learning models and non-differentiable models such as random forests. It is rooted in Bayesian principles and utilizes recent kernel-based approaches. This allows us to extend a wide range of existing state-of-the-art white-box batch active learning methods (BADGE,
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32467;&#26500;&#21270;&#21464;&#20998;&#25512;&#26029;&#30340;&#32852;&#21512;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;&#32467;&#26500;&#21270;&#27010;&#29575;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#20449;&#39640;&#25928;&#30340;&#21464;&#20307;&#31639;&#27861;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.03314</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#28508;&#21464;&#37327;&#27169;&#22411;&#30340;&#32852;&#21512;&#21464;&#20998;&#25512;&#26029;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Federated Variational Inference Methods for Structured Latent Variable Models. (arXiv:2302.03314v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03314
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32467;&#26500;&#21270;&#21464;&#20998;&#25512;&#26029;&#30340;&#32852;&#21512;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;&#32467;&#26500;&#21270;&#27010;&#29575;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#20449;&#39640;&#25928;&#30340;&#21464;&#20307;&#31639;&#27861;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#20351;&#24471;&#22312;&#25968;&#25454;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#25104;&#20026;&#21487;&#33021;&#65292;&#32780;&#26080;&#38656;&#25968;&#25454;&#31163;&#24320;&#20854;&#21407;&#22987;&#20301;&#32622;&#65292;&#24182;&#19988;&#22312;&#21508;&#20010;&#39046;&#22495;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#26080;&#27861;&#24212;&#29992;&#20110;&#35768;&#22810;&#32467;&#26500;&#21270;&#27010;&#29575;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32467;&#26500;&#21270;&#21464;&#20998;&#25512;&#26029;&#30340;&#36890;&#29992;&#32780;&#20248;&#38597;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#36827;&#34892;&#20102;&#36866;&#24212;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#31867;&#20284;&#20110;&#32463;&#20856;FedAvg&#31639;&#27861;&#30340;&#36890;&#20449;&#39640;&#25928;&#22411;&#21464;&#20307;&#12290;&#36890;&#36807;&#19982;&#20998;&#23618;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#21644;&#20027;&#39064;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning methods enable model training across distributed data sources without data leaving their original locations and have gained increasing interest in various fields. However, existing approaches are limited, excluding many structured probabilistic models. We present a general and elegant solution based on structured variational inference, widely used in Bayesian machine learning, adapted for the federated setting. Additionally, we provide a communication-efficient variant analogous to the canonical FedAvg algorithm. The proposed algorithms' effectiveness is demonstrated, and their performance is compared with hierarchical Bayesian neural networks and topic models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AIRS&#30340;&#33258;&#21160;&#20869;&#22312;&#22870;&#21169;&#22609;&#36896;&#25506;&#32034;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#20869;&#22312;&#28608;&#21169;&#20197;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#24615;&#33021;&#65307;&#24182;&#24320;&#21457;&#20102;&#39640;&#25928;&#21487;&#38752;&#30340;&#20869;&#22312;&#22870;&#21169;&#24037;&#20855;&#21253;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;AIRS&#24615;&#33021;&#21331;&#36234;&#65292;&#33021;&#22815;&#32988;&#36807;&#22522;&#20934;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2301.10886</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#33258;&#21160;&#20869;&#22312;&#22870;&#21169;&#22609;&#36896;&#25506;&#32034;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Automatic Intrinsic Reward Shaping for Exploration in Deep Reinforcement Learning. (arXiv:2301.10886v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AIRS&#30340;&#33258;&#21160;&#20869;&#22312;&#22870;&#21169;&#22609;&#36896;&#25506;&#32034;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#20869;&#22312;&#28608;&#21169;&#20197;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#24615;&#33021;&#65307;&#24182;&#24320;&#21457;&#20102;&#39640;&#25928;&#21487;&#38752;&#30340;&#20869;&#22312;&#22870;&#21169;&#24037;&#20855;&#21253;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;AIRS&#24615;&#33021;&#21331;&#36234;&#65292;&#33021;&#22815;&#32988;&#36807;&#22522;&#20934;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AIRS&#30340;&#33258;&#21160;&#20869;&#22312;&#22870;&#21169;&#22609;&#36896;&#26041;&#27861;&#65292;&#36890;&#36807;&#26234;&#33021;&#21644;&#36866;&#24212;&#24615;&#30340;&#22609;&#36896;&#20989;&#25968;&#65292;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#20869;&#22312;&#28608;&#21169;&#20197;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#24615;&#33021;&#12290;AIRS&#21487;&#20197;&#26681;&#25454;&#23454;&#26102;&#20272;&#35745;&#30340;&#20219;&#21153;&#22238;&#25253;&#20174;&#39044;&#23450;&#20041;&#30340;&#20989;&#25968;&#38598;&#20013;&#36873;&#25321;&#22609;&#36896;&#20989;&#25968;&#65292;&#25552;&#20379;&#21487;&#38752;&#30340;&#25506;&#32034;&#28608;&#21169;&#24182;&#35299;&#20915;&#20559;&#32622;&#30446;&#26631;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20869;&#22312;&#22870;&#21169;&#24037;&#20855;&#21253;&#65292;&#25552;&#20379;&#22810;&#31181;&#20869;&#22312;&#22870;&#21169;&#26041;&#27861;&#30340;&#39640;&#25928;&#21487;&#38752;&#23454;&#29616;&#26041;&#24335;&#12290;&#25105;&#20204;&#23558;AIRS&#24212;&#29992;&#22312;MiniGrid&#12289;Procgen&#21644;DeepMind&#25511;&#21046;&#22871;&#20214;&#30340;&#22810;&#39033;&#20219;&#21153;&#20013;&#36827;&#34892;&#27979;&#35797;&#12290;&#22823;&#37327;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;AIRS&#21487;&#20197;&#32988;&#36807;&#22522;&#20934;&#26041;&#26696;&#65292;&#24182;&#20855;&#26377;&#31616;&#21333;&#30340;&#26550;&#26500;&#21644;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present AIRS: Automatic Intrinsic Reward Shaping that intelligently and adaptively provides high-quality intrinsic rewards to enhance exploration in reinforcement learning (RL). More specifically, AIRS selects shaping function from a predefined set based on the estimated task return in real-time, providing reliable exploration incentives and alleviating the biased objective problem. Moreover, we develop an intrinsic reward toolkit to provide efficient and reliable implementations of diverse intrinsic reward approaches. We test AIRS on various tasks of MiniGrid, Procgen, and DeepMind Control Suite. Extensive simulation demonstrates that AIRS can outperform the benchmarking schemes and achieve superior performance with simple architecture.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#25490;&#21517;&#20013;&#26816;&#27979;&#20855;&#26377;&#20559;&#20506;&#34920;&#31034;&#30340;&#32676;&#20307;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#20844;&#24179;&#24230;&#37327;&#26041;&#27861;&#30340;&#39640;&#25928;&#25628;&#32034;&#31639;&#27861;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Shapley&#20540;&#35299;&#37322;&#32676;&#20307;&#34920;&#31034;&#20559;&#24046;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2301.00719</link><description>&lt;p&gt;
&#22312;&#25490;&#21517;&#20013;&#26816;&#27979;&#20855;&#26377;&#20559;&#20506;&#34920;&#31034;&#30340;&#32676;&#32452;
&lt;/p&gt;
&lt;p&gt;
Detection of Groups with Biased Representation in Ranking. (arXiv:2301.00719v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#25490;&#21517;&#20013;&#26816;&#27979;&#20855;&#26377;&#20559;&#20506;&#34920;&#31034;&#30340;&#32676;&#20307;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#20844;&#24179;&#24230;&#37327;&#26041;&#27861;&#30340;&#39640;&#25928;&#25628;&#32034;&#31639;&#27861;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Shapley&#20540;&#35299;&#37322;&#32676;&#20307;&#34920;&#31034;&#20559;&#24046;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#20851;&#38190;&#39046;&#22495;&#30340;&#23454;&#38469;&#20915;&#31574;&#24037;&#20855;&#37117;&#22522;&#20110;&#25490;&#21517;&#32467;&#26524;&#12290;&#38543;&#30528;&#23545;&#31639;&#27861;&#20844;&#24179;&#24615;&#30340;&#26085;&#30410;&#20851;&#27880;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#29992;&#20110;&#25490;&#21517;&#20844;&#24179;&#24615;&#30340;&#24230;&#37327;&#26041;&#27861;&#12290;&#20854;&#20013;&#35768;&#22810;&#23450;&#20041;&#32771;&#34385;&#22312;&#20219;&#20309;&#21512;&#29702;&#30340;k&#20540;&#20013;&#65292;&#19981;&#21516;&#30340;&#8220;&#21463;&#20445;&#25252;&#32676;&#20307;&#8221;&#22312;&#21069;k&#20010;&#25490;&#21517;&#39033;&#20013;&#30340;&#34920;&#31034;&#12290;&#22914;&#26524;&#24050;&#30693;&#20445;&#25252;&#30340;&#32676;&#20307;&#65292;&#30830;&#35748;&#31639;&#27861;&#30340;&#20844;&#24179;&#24615;&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#32676;&#20307;&#30340;&#23450;&#20041;&#21487;&#33021;&#20107;&#20808;&#19981;&#30693;&#36947;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#21069;k&#20010;&#25490;&#21517;&#39033;&#30446;&#20013;&#26816;&#27979;&#20855;&#26377;&#20559;&#20506;&#34920;&#31034;&#30340;&#32676;&#20307;&#30340;&#38382;&#39064;&#65292;&#28040;&#38500;&#20102;&#39044;&#23450;&#20041;&#20445;&#25252;&#32676;&#20307;&#30340;&#38656;&#35201;&#12290;&#36825;&#26679;&#30340;&#32676;&#20307;&#25968;&#37327;&#21487;&#33021;&#26159;&#25351;&#25968;&#32423;&#30340;&#65292;&#20351;&#24471;&#38382;&#39064;&#21464;&#24471;&#22256;&#38590;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#39640;&#25928;&#30340;&#25628;&#32034;&#31639;&#27861;&#26469;&#35299;&#20915;&#20004;&#31181;&#19981;&#21516;&#30340;&#20844;&#24179;&#24230;&#37327;&#26041;&#27861;&#65306;&#20840;&#23616;&#34920;&#31034;&#36793;&#30028;&#21644;&#27604;&#20363;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Shapley&#20540;&#27010;&#24565;&#35299;&#37322;&#32676;&#20307;&#34920;&#31034;&#20559;&#24046;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-life tools for decision-making in many critical domains are based on ranking results. With the increasing awareness of algorithmic fairness, recent works have presented measures for fairness in ranking. Many of those definitions consider the representation of different ``protected groups'', in the top-$k$ ranked items, for any reasonable $k$. Given the protected groups, confirming algorithmic fairness is a simple task. However, the groups' definitions may be unknown in advance. In this paper, we study the problem of detecting groups with biased representation in the top-$k$ ranked items, eliminating the need to pre-define protected groups. The number of such groups possible can be exponential, making the problem hard. We propose efficient search algorithms for two different fairness measures: global representation bounds, and proportional representation. Then we propose a method to explain the bias in the representations of groups utilizing the notion of Shapley values. We conclud
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20179;&#24211;&#29289;&#27969;&#20013;&#30340;&#26426;&#22120;&#20154;&#21644;&#20154;&#31867;&#21516;&#20107;&#21512;&#20316;&#12290;&#20182;&#20204;&#36890;&#36807;&#20998;&#23618;&#30340;MARL&#31639;&#27861;&#65292;&#35753;&#32463;&#29702;&#21644;&#24037;&#20154;&#20195;&#29702;&#26681;&#25454;&#20840;&#23616;&#30446;&#26631;&#36827;&#34892;&#21327;&#21516;&#35757;&#32451;&#65292;&#20197;&#26368;&#22823;&#21270;&#25315;&#36135;&#36895;&#29575;&#12290;</title><link>http://arxiv.org/abs/2212.11498</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22312;&#20179;&#24211;&#29289;&#27969;&#20013;&#19982;&#26426;&#22120;&#20154;&#21644;&#20154;&#31867;&#21516;&#20107;&#21512;&#20316;
&lt;/p&gt;
&lt;p&gt;
Scalable Multi-Agent Reinforcement Learning for Warehouse Logistics with Robotic and Human Co-Workers. (arXiv:2212.11498v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.11498
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20179;&#24211;&#29289;&#27969;&#20013;&#30340;&#26426;&#22120;&#20154;&#21644;&#20154;&#31867;&#21516;&#20107;&#21512;&#20316;&#12290;&#20182;&#20204;&#36890;&#36807;&#20998;&#23618;&#30340;MARL&#31639;&#27861;&#65292;&#35753;&#32463;&#29702;&#21644;&#24037;&#20154;&#20195;&#29702;&#26681;&#25454;&#20840;&#23616;&#30446;&#26631;&#36827;&#34892;&#21327;&#21516;&#35757;&#32451;&#65292;&#20197;&#26368;&#22823;&#21270;&#25315;&#36135;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35774;&#24819;&#19968;&#20010;&#20179;&#24211;&#37324;&#26377;&#25968;&#21313;&#20010;&#31227;&#21160;&#26426;&#22120;&#20154;&#21644;&#20154;&#31867;&#20998;&#25315;&#21592;&#19968;&#36215;&#24037;&#20316;&#65292;&#25910;&#38598;&#21644;&#20132;&#20184;&#20179;&#24211;&#20869;&#30340;&#29289;&#21697;&#12290;&#25105;&#20204;&#35201;&#35299;&#20915;&#30340;&#22522;&#26412;&#38382;&#39064;&#26159;&#31216;&#20026;&#25315;&#36135;&#38382;&#39064;&#65292;&#21363;&#36825;&#20123;&#24037;&#20316;&#20195;&#29702;&#20154;&#22914;&#20309;&#22312;&#20179;&#24211;&#20013;&#21327;&#35843;&#20182;&#20204;&#30340;&#31227;&#21160;&#21644;&#34892;&#20026;&#20197;&#26368;&#22823;&#21270;&#24615;&#33021;&#65288;&#20363;&#22914;&#35746;&#21333;&#21534;&#21520;&#37327;&#65289;&#12290;&#20256;&#32479;&#30340;&#34892;&#19994;&#26041;&#27861;&#20351;&#29992;&#21551;&#21457;&#24335;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#30340;&#24037;&#31243;&#21162;&#21147;&#26469;&#20026;&#22266;&#26377;&#21487;&#21464;&#30340;&#20179;&#24211;&#37197;&#32622;&#36827;&#34892;&#20248;&#21270;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#21487;&#20197;&#28789;&#27963;&#22320;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;&#20179;&#24211;&#37197;&#32622;&#65288;&#20363;&#22914;&#22823;&#23567;&#65292;&#24067;&#23616;&#65292;&#24037;&#20154;&#25968;&#37327;/&#31867;&#22411;&#65292;&#29289;&#21697;&#34917;&#20805;&#39057;&#29575;&#65289;&#65292;&#22240;&#20026;&#20195;&#29702;&#20154;&#36890;&#36807;&#32463;&#39564;&#23398;&#20064;&#22914;&#20309;&#26368;&#20248;&#22320;&#30456;&#20114;&#21512;&#20316;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#20998;&#23618;MARL&#31639;&#27861;&#65292;&#20854;&#20013;&#19968;&#20010;&#31649;&#29702;&#32773;&#20026;&#24037;&#20154;&#20195;&#29702;&#20998;&#37197;&#30446;&#26631;&#65292;&#24182;&#19988;&#31649;&#29702;&#32773;&#21644;&#24037;&#20154;&#30340;&#31574;&#30053;&#34987;&#20849;&#21516;&#35757;&#32451;&#20197;&#26368;&#22823;&#21270;&#20840;&#23616;&#30446;&#26631;&#65288;&#20363;&#22914;&#25315;&#36135;&#36895;&#29575;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We envision a warehouse in which dozens of mobile robots and human pickers work together to collect and deliver items within the warehouse. The fundamental problem we tackle, called the order-picking problem, is how these worker agents must coordinate their movement and actions in the warehouse to maximise performance (e.g. order throughput). Established industry methods using heuristic approaches require large engineering efforts to optimise for innately variable warehouse configurations. In contrast, multi-agent reinforcement learning (MARL) can be flexibly applied to diverse warehouse configurations (e.g. size, layout, number/types of workers, item replenishment frequency), as the agents learn through experience how to optimally cooperate with one another. We develop hierarchical MARL algorithms in which a manager assigns goals to worker agents, and the policies of the manager and workers are co-trained toward maximising a global objective (e.g. pick rate). Our hierarchical algorith
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#27169;&#22240;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#23454;&#38469;&#32422;&#26463;&#30340;&#28789;&#27963;&#29983;&#20135;&#35843;&#24230;&#38382;&#39064;&#65292;&#24182;&#24357;&#34917;&#20803;&#21551;&#21457;&#24335;&#30740;&#31350;&#20013;&#30340;&#32570;&#38519;&#12290;</title><link>http://arxiv.org/abs/2212.10936</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#27169;&#22240;&#31639;&#27861;&#29992;&#20110;&#31038;&#20250;&#25216;&#26415;&#29983;&#20135;&#35843;&#24230;
&lt;/p&gt;
&lt;p&gt;
A Memetic Algorithm with Reinforcement Learning for Sociotechnical Production Scheduling. (arXiv:2212.10936v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10936
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#27169;&#22240;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#23454;&#38469;&#32422;&#26463;&#30340;&#28789;&#27963;&#29983;&#20135;&#35843;&#24230;&#38382;&#39064;&#65292;&#24182;&#24357;&#34917;&#20803;&#21551;&#21457;&#24335;&#30740;&#31350;&#20013;&#30340;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24212;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#27169;&#22240;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#23454;&#38469;&#21452;&#36164;&#28304;&#32422;&#26463;&#26580;&#24615;&#20316;&#19994;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#65288;DRC-FJSSP&#65289;&#12290;&#36817;&#24180;&#26469;&#65292;&#23545;&#20110;DRL&#25216;&#26415;&#24050;&#32463;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#20294;&#26159;&#27809;&#26377;&#32771;&#34385;&#21040;&#29616;&#23454;&#12289;&#28789;&#27963;&#21644;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#36710;&#38388;&#12290;&#26412;&#25991;&#21457;&#29616;&#65292;&#22312;&#20197;&#35746;&#21333;&#20026;&#23548;&#21521;&#30340;&#38388;&#27463;&#24615;&#21046;&#36896;&#20013;&#23384;&#22312;&#19968;&#20010;&#30740;&#31350;&#31354;&#30333;&#65292;&#23427;&#32463;&#24120;&#22312;&#20855;&#26377;&#39640;&#26381;&#21153;&#27700;&#24179;&#30340;&#20013;&#23567;&#22411;&#20844;&#21496;&#20013;&#34920;&#31034;&#12290;&#20174;&#36825;&#19968;&#39046;&#22495;&#30340;&#23454;&#38469;&#24037;&#19994;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#35748;&#35782;&#21040;&#38656;&#35201;&#25551;&#36848;&#28789;&#27963;&#30340;&#26426;&#22120;&#12289;&#20154;&#24037;&#24037;&#20316;&#32773;&#21644;&#33021;&#21147;&#12289;&#35774;&#32622;&#21644;&#22788;&#29702;&#25805;&#20316;&#12289;&#29289;&#26009;&#21040;&#36798;&#26102;&#38388;&#12289;&#20855;&#26377;&#24182;&#34892;&#20219;&#21153;&#30340;&#22797;&#26434;&#20316;&#19994;&#36335;&#24452;&#20197;&#36827;&#34892;&#29289;&#26009;&#28165;&#21333;&#65288;BOM&#65289;&#21046;&#36896;&#12289;&#39034;&#24207;&#30456;&#20851;&#35774;&#32622;&#26102;&#38388;&#21644;&#65288;&#37096;&#20998;&#65289;&#33258;&#21160;&#21270;&#20219;&#21153;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22312;DRC-FJSSP&#30340;&#32972;&#26223;&#19979;&#65292;&#24050;&#32463;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#20803;&#21551;&#21457;&#24335;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#36866;&#24403;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#29983;&#20135;&#36807;&#31243;&#30340;&#22797;&#26434;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#65292;&#36825;&#26159;&#29616;&#23454;&#24037;&#19994;&#19990;&#30028;&#20013;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#20197;&#24357;&#34917;&#30456;&#20851;&#39046;&#22495;&#30340;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;
The following article presents a memetic algorithm with applying deep reinforcement learning (DRL) for solving practically oriented dual resource constrained flexible job shop scheduling problems (DRC-FJSSP). In recent years, there has been extensive research on DRL techniques, but without considering realistic, flexible and human-centered shopfloors. A research gap can be identified in the context of make-to-order oriented discontinuous manufacturing as it is often represented in medium-size companies with high service levels. From practical industry projects in this domain, we recognize requirements to depict flexible machines, human workers and capabilities, setup and processing operations, material arrival times, complex job paths with parallel tasks for bill of material (BOM) manufacturing, sequence-depended setup times and (partially) automated tasks. On the other hand, intensive research has been done on metaheuristics in the context of DRC-FJSSP. However, there is a lack of sui
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33410;&#32422;&#20869;&#23384;&#30340;NLLB-200&#27169;&#22411;&#20462;&#21098;&#26041;&#27861;&#65292;&#21487;&#22312;&#20445;&#25345;&#32763;&#35793;&#36136;&#37327;&#30340;&#21516;&#26102;&#31227;&#38500;&#22810;&#36798;80&#65285;&#30340;&#19987;&#23478;&#65292;&#20351;&#24471;&#22312;&#21333;&#20010;32GB&#30340;GPU&#19978;&#36816;&#34892;&#27169;&#22411;&#25104;&#20026;&#21487;&#33021;&#12290;&#36825;&#23545;&#20110;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20855;&#26377;&#37325;&#35201;&#30340;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2212.09811</link><description>&lt;p&gt;
&#39640;&#25928;&#33410;&#32422;&#20869;&#23384;&#30340;NLLB-200&#65306;&#38024;&#23545;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#35821;&#35328;&#29305;&#23450;&#19987;&#23478;&#21024;&#20943;
&lt;/p&gt;
&lt;p&gt;
Memory-efficient NLLB-200: Language-specific Expert Pruning of a Massively Multilingual Machine Translation Model. (arXiv:2212.09811v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09811
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33410;&#32422;&#20869;&#23384;&#30340;NLLB-200&#27169;&#22411;&#20462;&#21098;&#26041;&#27861;&#65292;&#21487;&#22312;&#20445;&#25345;&#32763;&#35793;&#36136;&#37327;&#30340;&#21516;&#26102;&#31227;&#38500;&#22810;&#36798;80&#65285;&#30340;&#19987;&#23478;&#65292;&#20351;&#24471;&#22312;&#21333;&#20010;32GB&#30340;GPU&#19978;&#36816;&#34892;&#27169;&#22411;&#25104;&#20026;&#21487;&#33021;&#12290;&#36825;&#23545;&#20110;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20855;&#26377;&#37325;&#35201;&#30340;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#20256;&#32479;&#30340;&#21452;&#35821;&#32763;&#35793;&#31995;&#32479;&#30456;&#27604;&#65292;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20855;&#26377;&#21560;&#24341;&#21147;&#65292;&#22240;&#20026;&#19968;&#20010;&#21333;&#19968;&#27169;&#22411;&#21487;&#20197;&#32763;&#35793;&#25104;&#22810;&#31181;&#35821;&#35328;&#65292;&#24182;&#20174;&#30693;&#35782;&#36716;&#31227;&#20013;&#33719;&#30410;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;&#28982;&#32780;&#65292;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#27169;&#22411;&#21463;&#21040;&#22810;&#35821;&#35328;&#24615;&#30340;&#38480;&#21046;&#65292;&#38500;&#38750;&#36827;&#34892;&#22823;&#35268;&#27169;&#25193;&#23637;&#65292;&#21542;&#21017;&#20250;&#22686;&#21152;&#35757;&#32451;&#21644;&#25512;&#29702;&#25104;&#26412;&#12290;&#31232;&#30095;&#30340;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#26159;&#19968;&#31181;&#22312;&#19981;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#30340;&#24773;&#20917;&#19979;&#22823;&#24133;&#22686;&#21152;&#27169;&#22411;&#23481;&#37327;&#30340;&#26041;&#27861;&#12290;&#26368;&#36817;&#21457;&#24067;&#30340;NLLB-200&#26159;&#36825;&#26679;&#19968;&#20010;&#27169;&#22411;&#30340;&#20363;&#23376;&#12290;&#23427;&#28085;&#30422;&#20102;202&#31181;&#35821;&#35328;&#65292;&#20294;&#20165;&#25512;&#29702;&#23601;&#38656;&#35201;&#33267;&#23569;&#22235;&#20010;32GB&#30340;GPU&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#21098;&#26041;&#27861;&#65292;&#20801;&#35768;&#21024;&#38500;&#22810;&#36798;80&#65285;&#30340;&#19987;&#23478;&#65292;&#20294;&#32763;&#35793;&#36136;&#37327;&#20960;&#20046;&#27809;&#26377;&#25439;&#22833;&#65292;&#36825;&#20351;&#24471;&#22312;&#21333;&#20010;32GB&#30340;GPU&#19978;&#36816;&#34892;&#35813;&#27169;&#22411;&#25104;&#20026;&#21487;&#33021;&#12290;&#36827;&#19968;&#27493;&#20998;&#26512;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#20462;&#21098;&#24230;&#37327;&#25351;&#26631;&#21487;&#20197;&#35782;&#21035;&#20986;&#35821;&#35328;&#29305;&#23450;&#30340;&#19987;&#23478;
&lt;/p&gt;
&lt;p&gt;
Compared to conventional bilingual translation systems, massively multilingual machine translation is appealing because a single model can translate into multiple languages and benefit from knowledge transfer for low resource languages. On the other hand, massively multilingual models suffer from the curse of multilinguality, unless scaling their size massively, which increases their training and inference costs. Sparse Mixture-of-Experts models are a way to drastically increase model capacity without the need for a proportional amount of computing. The recently released NLLB-200 is an example of such a model. It covers 202 languages but requires at least four 32GB GPUs just for inference. In this work, we propose a pruning method that allows the removal of up to 80\% of experts with a negligible loss in translation quality, which makes it feasible to run the model on a single 32GB GPU. Further analysis suggests that our pruning metrics allow to identify language-specific experts and p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#27169;&#24335;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;&#26469;&#35782;&#21035;&#24694;&#24847;&#20869;&#23481;&#30340;&#30446;&#26631;&#65292;&#24182;&#31995;&#32479;&#22320;&#30740;&#31350;&#24694;&#24847;&#27169;&#22240;&#30340;&#28436;&#21270;&#12290;&#36890;&#36807;&#30740;&#31350;&#21453;&#29369;&#22826;&#20027;&#20041;&#27169;&#22240;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;CLIP&#29983;&#25104;&#30340;&#23884;&#20837;&#21521;&#37327;&#20013;&#23384;&#22312;&#30340;&#35821;&#20041;&#35268;&#24459;&#65292;&#20174;&#32780;&#25581;&#31034;&#20102;&#24694;&#24847;&#27169;&#22240;&#30340;&#21019;&#36896;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2212.06573</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#27169;&#24335;&#23545;&#27604;&#23398;&#20064;&#30740;&#31350;&#24694;&#24847;&#27169;&#22240;&#30340;&#28436;&#21270;
&lt;/p&gt;
&lt;p&gt;
On the Evolution of (Hateful) Memes by Means of Multimodal Contrastive Learning. (arXiv:2212.06573v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06573
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#27169;&#24335;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;&#26469;&#35782;&#21035;&#24694;&#24847;&#20869;&#23481;&#30340;&#30446;&#26631;&#65292;&#24182;&#31995;&#32479;&#22320;&#30740;&#31350;&#24694;&#24847;&#27169;&#22240;&#30340;&#28436;&#21270;&#12290;&#36890;&#36807;&#30740;&#31350;&#21453;&#29369;&#22826;&#20027;&#20041;&#27169;&#22240;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;CLIP&#29983;&#25104;&#30340;&#23884;&#20837;&#21521;&#37327;&#20013;&#23384;&#22312;&#30340;&#35821;&#20041;&#35268;&#24459;&#65292;&#20174;&#32780;&#25581;&#31034;&#20102;&#24694;&#24847;&#27169;&#22240;&#30340;&#21019;&#36896;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#21644;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#24694;&#24847;&#27169;&#22240;&#30340;&#20256;&#25773;&#23545;&#31038;&#20250;&#20135;&#29983;&#20102;&#19981;&#33391;&#24433;&#21709;&#12290;&#26816;&#27979;&#24694;&#24847;&#27169;&#22240;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#20854;&#20013;&#19968;&#20010;&#21407;&#22240;&#26159;&#27169;&#22240;&#30340;&#28436;&#21270;&#24615;&#36136;&#65307;&#26032;&#30340;&#24694;&#24847;&#27169;&#22240;&#21487;&#20197;&#36890;&#36807;&#23558;&#24694;&#24847;&#20869;&#28085;&#19982;&#20854;&#20182;&#25991;&#21270;&#35266;&#24565;&#25110;&#31526;&#21495;&#34701;&#21512;&#32780;&#20986;&#29616;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#27169;&#24335;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;&#65288;&#29305;&#21035;&#26159;OpenAI&#30340;CLIP&#65289;&#26469;&#35782;&#21035;&#24694;&#24847;&#20869;&#23481;&#30340;&#30446;&#26631;&#65292;&#24182;&#31995;&#32479;&#22320;&#30740;&#31350;&#24694;&#24847;&#27169;&#22240;&#30340;&#28436;&#21270;&#12290;&#25105;&#20204;&#21457;&#29616;CLIP&#29983;&#25104;&#30340;&#23884;&#20837;&#21521;&#37327;&#20013;&#23384;&#22312;&#35821;&#20041;&#35268;&#24459;&#65292;&#25551;&#36848;&#20102;&#21516;&#19968;&#27169;&#24577;&#65288;&#22270;&#20687;&#65289;&#25110;&#36328;&#27169;&#24577;&#65288;&#22270;&#20687;&#21644;&#25991;&#26412;&#65289;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;&#21033;&#29992;&#36825;&#19968;&#23646;&#24615;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#32467;&#21512;&#22810;&#20010;&#22270;&#20687;&#30340;&#21487;&#35270;&#21270;&#20803;&#32032;&#25110;&#23558;&#25991;&#26412;&#20449;&#24687;&#19982;&#24694;&#24847;&#22270;&#20687;&#34701;&#21512;&#26469;&#21019;&#24314;&#24694;&#24847;&#27169;&#22240;&#12290;&#36890;&#36807;&#37325;&#28857;&#30740;&#31350;&#21453;&#29369;&#22826;&#20027;&#20041;&#27169;&#22240;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#20998;&#26512;&#24694;&#24847;&#27169;&#22240;&#28436;&#21270;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dissemination of hateful memes online has adverse effects on social media platforms and the real world. Detecting hateful memes is challenging, one of the reasons being the evolutionary nature of memes; new hateful memes can emerge by fusing hateful connotations with other cultural ideas or symbols. In this paper, we propose a framework that leverages multimodal contrastive learning models, in particular OpenAI's CLIP, to identify targets of hateful content and systematically investigate the evolution of hateful memes. We find that semantic regularities exist in CLIP-generated embeddings that describe semantic relationships within the same modality (images) or across modalities (images and text). Leveraging this property, we study how hateful memes are created by combining visual elements from multiple images or fusing textual information with a hateful image. We demonstrate the capabilities of our framework for analyzing the evolution of hateful memes by focusing on antisemitic me
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23398;&#20064;&#30340;&#35268;&#33539;&#21270;&#20989;&#25968;&#26469;&#23454;&#29616;&#31561;&#21464;&#24615;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#23545;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#38480;&#21046;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23398;&#20064;&#35268;&#33539;&#21270;&#20989;&#25968;&#21487;&#20197;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#19982;&#29616;&#26377;&#25216;&#26415;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#24182;&#19988;&#36895;&#24230;&#26356;&#24555;&#12290;</title><link>http://arxiv.org/abs/2211.06489</link><description>&lt;p&gt;
&#20351;&#29992;&#23398;&#20064;&#30340;&#35268;&#33539;&#21270;&#20989;&#25968;&#23454;&#29616;&#31561;&#21464;&#24615;
&lt;/p&gt;
&lt;p&gt;
Equivariance with Learned Canonicalization Functions. (arXiv:2211.06489v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.06489
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23398;&#20064;&#30340;&#35268;&#33539;&#21270;&#20989;&#25968;&#26469;&#23454;&#29616;&#31561;&#21464;&#24615;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#23545;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#38480;&#21046;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23398;&#20064;&#35268;&#33539;&#21270;&#20989;&#25968;&#21487;&#20197;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#19982;&#29616;&#26377;&#25216;&#26415;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#24182;&#19988;&#36895;&#24230;&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#23545;&#31216;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#36890;&#36807;&#38480;&#21046;&#26550;&#26500;&#26469;&#23454;&#29616;&#23545;&#19968;&#32452;&#21464;&#25442;&#30340;&#19981;&#21464;&#24615;&#25110;&#31561;&#21464;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20135;&#29983;&#25968;&#25454;&#30340;&#35268;&#33539;&#34920;&#31034;&#26469;&#36991;&#20813;&#36825;&#31181;&#26550;&#26500;&#32422;&#26463;&#12290;&#36825;&#20123;&#35268;&#33539;&#21270;&#20989;&#25968;&#21487;&#20197;&#30452;&#25509;&#25554;&#20837;&#38750;&#31561;&#21464;&#20027;&#24178;&#26550;&#26500;&#20013;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20123;&#24863;&#20852;&#36259;&#30340;&#32676;&#32452;&#30340;&#26126;&#30830;&#23454;&#29616;&#26041;&#27861;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#26222;&#36866;&#24615;&#65292;&#21516;&#26102;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#27934;&#23519;&#21147;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#20551;&#35774;&#24471;&#21040;&#20102;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#30340;&#25903;&#25345;&#65292;&#21363;&#23398;&#20064;&#19968;&#20010;&#23567;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#25191;&#34892;&#35268;&#33539;&#21270;&#20248;&#20110;&#20351;&#29992;&#39044;&#23450;&#20041;&#30340;&#21551;&#21457;&#24335;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23398;&#20064;&#35268;&#33539;&#21270;&#20989;&#25968;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#65292;&#21253;&#25324;&#22270;&#20687;&#20998;&#31867;&#12289;$N$&#20307;&#21160;&#21147;&#23398;&#39044;&#27979;&#12289;&#28857;&#20113;&#20998;&#31867;&#21644;&#37096;&#20998;&#20998;&#21106;&#31561;&#23398;&#20064;&#31561;&#21464;&#20989;&#25968;&#30340;&#29616;&#26377;&#25216;&#26415;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#21516;&#26102;&#36895;&#24230;&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symmetry-based neural networks often constrain the architecture in order to achieve invariance or equivariance to a group of transformations. In this paper, we propose an alternative that avoids this architectural constraint by learning to produce canonical representations of the data. These canonicalization functions can readily be plugged into non-equivariant backbone architectures. We offer explicit ways to implement them for some groups of interest. We show that this approach enjoys universality while providing interpretable insights. Our main hypothesis, supported by our empirical results, is that learning a small neural network to perform canonicalization is better than using predefined heuristics. Our experiments show that learning the canonicalization function is competitive with existing techniques for learning equivariant functions across many tasks, including image classification, $N$-body dynamics prediction, point cloud classification and part segmentation, while being fas
&lt;/p&gt;</description></item><item><title>&#23485;&#24230;&#20248;&#20808;&#30340;&#27969;&#27700;&#32447;&#24182;&#34892;&#35745;&#31639;&#26041;&#27861;&#32467;&#21512;&#20102;&#27969;&#27700;&#32447;&#21644;&#25968;&#25454;&#24182;&#34892;&#35745;&#31639;&#65292;&#36890;&#36807;&#22312;&#27599;&#20010;GPU&#19978;&#20351;&#29992;&#23567;&#25209;&#37327;&#22823;&#23567;&#21644;&#23436;&#20840;&#20998;&#29255;&#30340;&#25968;&#25454;&#24182;&#34892;&#35745;&#31639;&#65292;&#20197;&#25552;&#39640;&#35757;&#32451;&#21534;&#21520;&#37327;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#19982;Megatron-LM&#30456;&#27604;&#65292;&#22312;&#19968;&#20010;520&#20159;&#21442;&#25968;&#30340;&#27169;&#22411;&#19978;&#65292;&#20351;&#29992;&#23567;&#25209;&#37327;&#22823;&#23567;&#27599;&#20010;GPU&#30340;&#35757;&#32451;&#21534;&#21520;&#37327;&#22686;&#21152;&#20102;&#39640;&#36798;43%&#12290;</title><link>http://arxiv.org/abs/2211.05953</link><description>&lt;p&gt;
&#23485;&#24230;&#20248;&#20808;&#30340;&#27969;&#27700;&#32447;&#24182;&#34892;&#35745;&#31639;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Breadth-First Pipeline Parallelism. (arXiv:2211.05953v2 [cs.DC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05953
&lt;/p&gt;
&lt;p&gt;
&#23485;&#24230;&#20248;&#20808;&#30340;&#27969;&#27700;&#32447;&#24182;&#34892;&#35745;&#31639;&#26041;&#27861;&#32467;&#21512;&#20102;&#27969;&#27700;&#32447;&#21644;&#25968;&#25454;&#24182;&#34892;&#35745;&#31639;&#65292;&#36890;&#36807;&#22312;&#27599;&#20010;GPU&#19978;&#20351;&#29992;&#23567;&#25209;&#37327;&#22823;&#23567;&#21644;&#23436;&#20840;&#20998;&#29255;&#30340;&#25968;&#25454;&#24182;&#34892;&#35745;&#31639;&#65292;&#20197;&#25552;&#39640;&#35757;&#32451;&#21534;&#21520;&#37327;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#19982;Megatron-LM&#30456;&#27604;&#65292;&#22312;&#19968;&#20010;520&#20159;&#21442;&#25968;&#30340;&#27169;&#22411;&#19978;&#65292;&#20351;&#29992;&#23567;&#25209;&#37327;&#22823;&#23567;&#27599;&#20010;GPU&#30340;&#35757;&#32451;&#21534;&#21520;&#37327;&#22686;&#21152;&#20102;&#39640;&#36798;43%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#35843;&#24230;&#26041;&#27861;&#8212;&#8212;&#23485;&#24230;&#20248;&#20808;&#30340;&#27969;&#27700;&#32447;&#24182;&#34892;&#35745;&#31639;&#65292;&#35813;&#26041;&#27861;&#20248;&#21270;&#20102;&#27969;&#27700;&#32447;&#21644;&#25968;&#25454;&#24182;&#34892;&#35745;&#31639;&#30340;&#32467;&#21512;&#12290;&#23485;&#24230;&#20248;&#20808;&#30340;&#27969;&#27700;&#32447;&#24182;&#34892;&#35745;&#31639;&#36890;&#36807;&#22312;&#27599;&#20010;GPU&#19978;&#20351;&#29992;&#36739;&#23567;&#30340;&#25209;&#37327;&#22823;&#23567;&#24182;&#32467;&#21512;&#23436;&#20840;&#20998;&#29255;&#30340;&#25968;&#25454;&#24182;&#34892;&#35745;&#31639;&#65292;&#23454;&#29616;&#20102;&#39640;GPU&#21033;&#29992;&#29575;&#12289;&#38477;&#20302;&#35757;&#32451;&#26102;&#38388;&#12289;&#25104;&#26412;&#21644;&#20869;&#23384;&#20351;&#29992;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#30456;&#23545;&#20110;Megatron-LM&#65292;&#23545;&#20110;&#19968;&#20010;520&#20159;&#21442;&#25968;&#30340;&#27169;&#22411;&#65292;&#20351;&#29992;&#36739;&#23567;&#30340;&#25209;&#37327;&#22823;&#23567;&#27599;&#20010;GPU&#30340;&#35757;&#32451;&#21534;&#21520;&#37327;&#22686;&#21152;&#20102;&#39640;&#36798;43%&#65292;&#20174;&#32780;&#22312;&#22823;&#22411;GPU&#38598;&#32676;&#19978;&#23558;&#35757;&#32451;&#26102;&#38388;&#21644;&#25104;&#26412;&#21516;&#26679;&#38477;&#20302;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Breadth-First Pipeline Parallelism, a novel training schedule which optimizes the combination of pipeline and data parallelism. Breadth-First Pipeline Parallelism lowers training time, cost and memory usage by combining a high GPU utilization with a small batch size per GPU, and by making use of fully sharded data parallelism. Experimentally, we observed an increase of up to 43% in training throughput for a 52 billion-parameter model using a small batch size per GPU compared to Megatron-LM, which would reduce the training time and cost by the same amount on a large GPU cluster.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#29420;&#31435;&#20998;&#31867;&#20998;&#24067;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#22797;&#20351;&#29992;&#20844;&#20849;&#30340;&#23618;&#36755;&#20986;&#21644;&#23545;&#26679;&#26412;&#36827;&#34892;&#25490;&#24207;&#65292;&#21487;&#20197;&#22823;&#24133;&#25552;&#39640;&#27169;&#22411;&#36816;&#34892;&#36895;&#24230;&#12289;&#38477;&#20302;&#20869;&#23384;&#20351;&#29992;&#65292;&#24182;&#19988;&#33719;&#24471;&#27604;&#24120;&#35268;&#28145;&#24230;&#38598;&#25104;&#26356;&#39640;&#30340;&#19981;&#30830;&#23450;&#24615;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2210.04882</link><description>&lt;p&gt;
Layer Ensembles. (arXiv:2210.04882v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
Layer Ensembles. (arXiv:2210.04882v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04882
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#29420;&#31435;&#20998;&#31867;&#20998;&#24067;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#22797;&#20351;&#29992;&#20844;&#20849;&#30340;&#23618;&#36755;&#20986;&#21644;&#23545;&#26679;&#26412;&#36827;&#34892;&#25490;&#24207;&#65292;&#21487;&#20197;&#22823;&#24133;&#25552;&#39640;&#27169;&#22411;&#36816;&#34892;&#36895;&#24230;&#12289;&#38477;&#20302;&#20869;&#23384;&#20351;&#29992;&#65292;&#24182;&#19988;&#33719;&#24471;&#27604;&#24120;&#35268;&#28145;&#24230;&#38598;&#25104;&#26356;&#39640;&#30340;&#19981;&#30830;&#23450;&#24615;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#38598;&#25104;&#26159;&#19968;&#31181;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#36890;&#36807;&#25910;&#38598;&#27599;&#20010;&#32593;&#32476;&#30340;&#39044;&#27979;&#32467;&#26524;&#24182;&#35745;&#31639;&#39044;&#27979;&#32467;&#26524;&#20043;&#38388;&#30340;&#24046;&#24322;&#26469;&#20272;&#35745;&#22810;&#20010;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#32593;&#32476;&#27599;&#19968;&#23618;&#30340;&#19968;&#32452;&#29420;&#31435;&#30340;&#20998;&#31867;&#20998;&#24067;&#65292;&#19982;&#24120;&#35268;&#30340;&#28145;&#24230;&#38598;&#25104;&#30456;&#27604;&#20855;&#26377;&#26356;&#22810;&#21487;&#33021;&#30340;&#26679;&#26412;&#65292;&#36825;&#20123;&#26679;&#26412;&#22312;&#23618;&#20043;&#38388;&#37325;&#21472;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#19968;&#31181;&#20248;&#21270;&#30340;&#25512;&#26029;&#36807;&#31243;&#65292;&#35813;&#36807;&#31243;&#21487;&#20197;&#37325;&#22797;&#20351;&#29992;&#20844;&#20849;&#30340;&#23618;&#36755;&#20986;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#36798;19&#20493;&#30340;&#21152;&#36895;&#21644;&#20108;&#27425;&#20943;&#23569;&#20869;&#23384;&#20351;&#29992;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36890;&#36807;&#23545;&#26679;&#26412;&#36827;&#34892;&#25490;&#24207;&#21487;&#20197;&#36827;&#19968;&#27493;&#25913;&#36827;&#35813;&#26041;&#27861;&#65292;&#20174;&#32780;&#24471;&#21040;&#38656;&#35201;&#26356;&#23569;&#20869;&#23384;&#21644;&#26102;&#38388;&#26469;&#36816;&#34892;&#24182;&#19988;&#20855;&#26377;&#27604;&#28145;&#24230;&#38598;&#25104;&#26356;&#39640;&#19981;&#30830;&#23450;&#24615;&#36136;&#37327;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Ensembles, as a type of Bayesian Neural Networks, can be used to estimate uncertainty on the prediction of multiple neural networks by collecting votes from each network and computing the difference in those predictions. In this paper, we introduce a method for uncertainty estimation that considers a set of independent categorical distributions for each layer of the network, giving many more possible samples with overlapped layers than in the regular Deep Ensembles. We further introduce an optimized inference procedure that reuses common layer outputs, achieving up to 19x speed up and reducing memory usage quadratically. We also show that the method can be further improved by ranking samples, resulting in models that require less memory and time to run while achieving higher uncertainty quality than Deep Ensembles.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20248;&#20808;&#32423;&#25311;&#38453;&#20013;&#20540;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#21452;&#30446;&#26631;&#36817;&#20284;&#31639;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#21512;&#36866;&#30340;&#35774;&#26045;&#23376;&#38598;&#23454;&#29616;&#21516;&#26102;&#26368;&#23567;&#21270;&#24320;&#25918;&#25104;&#26412;&#21644;&#28385;&#36275;&#32422;&#26463;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2210.01888</link><description>&lt;p&gt;
&#20248;&#20808;&#32423;&#25311;&#38453;&#20013;&#20540;&#38382;&#39064;&#30340;&#21452;&#30446;&#26631;&#36817;&#20284;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Bicriteria Approximation Algorithms for Priority Matroid Median. (arXiv:2210.01888v2 [cs.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01888
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20248;&#20808;&#32423;&#25311;&#38453;&#20013;&#20540;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#21452;&#30446;&#26631;&#36817;&#20284;&#31639;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#21512;&#36866;&#30340;&#35774;&#26045;&#23376;&#38598;&#23454;&#29616;&#21516;&#26102;&#26368;&#23567;&#21270;&#24320;&#25918;&#25104;&#26412;&#21644;&#28385;&#36275;&#32422;&#26463;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#24179;&#24615;&#32771;&#34385;&#22312;&#26368;&#36817;&#20960;&#24180;&#20013;&#25512;&#21160;&#20102;&#26032;&#30340;&#32858;&#31867;&#38382;&#39064;&#21644;&#31639;&#27861;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#20248;&#20808;&#32423;&#25311;&#38453;&#20013;&#20540;&#38382;&#39064;&#65292;&#23427;&#26159;&#23545;&#26368;&#36817;&#30740;&#31350;&#30340;&#20248;&#20808;&#32423;k-&#20013;&#20540;&#38382;&#39064;&#30340;&#19968;&#33324;&#21270;&#12290;&#36755;&#20837;&#21253;&#25324;&#19968;&#32452;&#35774;&#26045;$\mathcal{F}$&#21644;&#19968;&#32452;&#23458;&#25143;$\mathcal{C}$&#65292;&#23427;&#20204;&#20301;&#20110;&#24230;&#37327;&#31354;&#38388;$(\mathcal{F} \cup \mathcal{C},d)$&#20013;&#65292;&#20197;&#21450;&#19968;&#20010;&#25311;&#38453;$\mathcal{M}=(\mathcal{F},\mathcal{I})$&#65292;&#34920;&#31034;&#35774;&#26045;&#30340;&#38598;&#21512;&#12290;&#27492;&#22806;&#65292;&#27599;&#20010;&#23458;&#25143;$j$&#37117;&#26377;&#19968;&#20010;&#25351;&#23450;&#30340;&#21322;&#24452;$r_j \ge 0$&#65292;&#27599;&#20010;&#35774;&#26045;$i \in \mathcal{F}$&#37117;&#26377;&#19968;&#20010;&#24320;&#25918;&#25104;&#26412;$f_i$&#12290;&#30446;&#26631;&#26159;&#36873;&#25321;&#19968;&#32452;&#35774;&#26045;&#30340;&#23376;&#38598;$S \subseteq \mathcal{F}$&#65292;&#20351;&#24471;$\sum_{i \in \mathcal{F}} f_i + \sum_{j \in \mathcal{C}} d(j,S)$&#26368;&#23567;&#65292;&#21516;&#26102;&#28385;&#36275;&#20004;&#20010;&#32422;&#26463;&#26465;&#20214;&#65306;(i) $S$&#26159;$\mathcal{M}$&#20013;&#30340;&#29420;&#31435;&#38598;&#65288;&#21363;$S \in \mathcal{I}$&#65289;&#65292;(ii) &#23545;&#20110;&#27599;&#20010;&#23458;&#25143;$j$&#65292;&#23427;&#21040;&#19968;&#20010;&#24320;&#25918;&#35774;&#26045;&#30340;&#36317;&#31163;&#26368;&#22810;&#20026;$r_j$&#65288;&#21363;$d(j,S) \le r_j$&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fairness considerations have motivated new clustering problems and algorithms in recent years. In this paper we consider the Priority Matroid Median problem which generalizes the Priority $k$-Median problem that has recently been studied. The input consists of a set of facilities $\mathcal{F}$ and a set of clients $\mathcal{C}$ that lie in a metric space $(\mathcal{F} \cup \mathcal{C},d)$, and a matroid $\mathcal{M}=(\mathcal{F},\mathcal{I})$ over the facilities. In addition each client $j$ has a specified radius $r_j \ge 0$ and each facility $i \in \mathcal{F}$ has an opening cost $f_i$. The goal is to choose a subset $S \subseteq \mathcal{F}$ of facilities to minimize the $\sum_{i \in \mathcal{F}} f_i + \sum_{j \in \mathcal{C}} d(j,S)$ subject to two constraints: (i) $S$ is an independent set in $\mathcal{M}$ (that is $S \in \mathcal{I}$) and (ii) for each client $j$, its distance to an open facility is at most $r_j$ (that is, $d(j,S) \le r_j$). For this problem we describe the first
&lt;/p&gt;</description></item><item><title>TabLeak&#26159;&#31532;&#19968;&#20010;&#38024;&#23545;&#34920;&#26684;&#25968;&#25454;&#30340;&#20840;&#38754;&#37325;&#26500;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#34920;&#26684;&#25968;&#25454;&#27844;&#28431;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#35299;&#20915;&#28151;&#21512;&#31163;&#25955;&#36830;&#32493;&#20248;&#21270;&#38382;&#39064;&#21644;&#23454;&#29616;&#20154;&#31867;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2210.01785</link><description>&lt;p&gt;
TabLeak&#65306;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#34920;&#26684;&#25968;&#25454;&#27844;&#28431;
&lt;/p&gt;
&lt;p&gt;
TabLeak: Tabular Data Leakage in Federated Learning. (arXiv:2210.01785v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01785
&lt;/p&gt;
&lt;p&gt;
TabLeak&#26159;&#31532;&#19968;&#20010;&#38024;&#23545;&#34920;&#26684;&#25968;&#25454;&#30340;&#20840;&#38754;&#37325;&#26500;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#34920;&#26684;&#25968;&#25454;&#27844;&#28431;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#35299;&#20915;&#28151;&#21512;&#31163;&#25955;&#36830;&#32493;&#20248;&#21270;&#38382;&#39064;&#21644;&#23454;&#29616;&#20154;&#31867;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#25215;&#35834;&#20445;&#25252;&#38544;&#31169;&#65292;&#20294;&#26368;&#36817;&#22312;&#22270;&#20687;&#21644;&#25991;&#26412;&#39046;&#22495;&#30340;&#30740;&#31350;&#26174;&#31034;&#65292;&#35757;&#32451;&#26356;&#26032;&#20250;&#27844;&#38706;&#31169;&#20154;&#23458;&#25143;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#22312;&#39640;&#39118;&#38505;&#30340;FL&#24212;&#29992;&#65288;&#20363;&#22914;&#21307;&#30103;&#21644;&#37329;&#34701;&#65289;&#20013;&#20351;&#29992;&#30340;&#22823;&#22810;&#25968;&#34920;&#26684;&#25968;&#25454;&#20013;&#65292;&#25968;&#25454;&#27844;&#38706;&#30340;&#39118;&#38505;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#25104;&#21151;&#30340;&#34920;&#26684;&#25968;&#25454;&#25915;&#20987;&#24517;&#39035;&#35299;&#20915;&#20004;&#20010;&#39046;&#22495;&#29420;&#29305;&#30340;&#20851;&#38190;&#25361;&#25112;&#65306;&#65288;i&#65289;&#35299;&#20915;&#39640;&#26041;&#24046;&#28151;&#21512;&#31163;&#25955;&#36830;&#32493;&#20248;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#20351;&#20154;&#31867;&#33021;&#22815;&#35780;&#20272;&#37325;&#26500;&#65292;&#22240;&#20026;&#19982;&#22270;&#20687;&#21644;&#25991;&#26412;&#25968;&#25454;&#19981;&#21516;&#65292;&#26080;&#27861;&#30452;&#25509;&#20154;&#24037;&#26816;&#26597;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;TabLeak&#65292;&#31532;&#19968;&#20010;&#38024;&#23545;&#34920;&#26684;&#25968;&#25454;&#30340;&#20840;&#38754;&#37325;&#26500;&#25915;&#20987;&#12290;TabLeak&#22522;&#20110;&#20004;&#20010;&#20851;&#38190;&#36129;&#29486;&#65306;(i)&#19968;&#31181;&#21033;&#29992;softmax&#25918;&#26494;&#21644;&#27744;&#21270;&#38598;&#25104;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292; (ii)&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#26696;&#65292;&#20197;&#23454;&#29616;&#20154;&#31867;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
While federated learning (FL) promises to preserve privacy, recent works in the image and text domains have shown that training updates leak private client data. However, most high-stakes applications of FL (e.g., in healthcare and finance) use tabular data, where the risk of data leakage has not yet been explored. A successful attack for tabular data must address two key challenges unique to the domain: (i) obtaining a solution to a high-variance mixed discrete-continuous optimization problem, and (ii) enabling human assessment of the reconstruction as unlike for image and text data, direct human inspection is not possible. In this work we address these challenges and propose TabLeak, the first comprehensive reconstruction attack on tabular data. TabLeak is based on two key contributions: (i) a method which leverages a softmax relaxation and pooled ensembling to solve the optimization problem, and (ii) an entropy-based uncertainty quantification scheme to enable human assessment. We e
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20107;&#20214;&#26816;&#27979;&#30340;&#29983;&#29289;&#21307;&#23398;&#20449;&#21495;&#22788;&#29702;&#26694;&#26550;&#65292;&#36991;&#20813;&#20102;&#32321;&#29712;&#30340;&#21518;&#22788;&#29702;&#36807;&#31243;&#65292;&#30452;&#25509;&#23558;&#20107;&#20214;&#20316;&#20026;&#23398;&#20064;&#30446;&#26631;&#65292;&#25317;&#26377;&#36739;&#24378;&#30340;&#23454;&#29992;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.11007</link><description>&lt;p&gt;
&#36991;&#20813;&#21518;&#22788;&#29702;: &#22522;&#20110;&#20107;&#20214;&#26816;&#27979;&#30340;&#29983;&#29289;&#21307;&#23398;&#20449;&#21495;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Avoiding Post-Processing with Event-Based Detection in Biomedical Signals. (arXiv:2209.11007v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.11007
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20107;&#20214;&#26816;&#27979;&#30340;&#29983;&#29289;&#21307;&#23398;&#20449;&#21495;&#22788;&#29702;&#26694;&#26550;&#65292;&#36991;&#20813;&#20102;&#32321;&#29712;&#30340;&#21518;&#22788;&#29702;&#36807;&#31243;&#65292;&#30452;&#25509;&#23558;&#20107;&#20214;&#20316;&#20026;&#23398;&#20064;&#30446;&#26631;&#65292;&#25317;&#26377;&#36739;&#24378;&#30340;&#23454;&#29992;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;: &#22312;&#29983;&#29289;&#21307;&#23398;&#20449;&#21495;&#22788;&#29702;&#20013;&#65292;&#23547;&#25214;&#24863;&#20852;&#36259;&#30340;&#20107;&#20214;&#26159;&#19968;&#39033;&#24120;&#35265;&#20219;&#21153;&#12290;&#30315;&#30187;&#21457;&#20316;&#21644;&#20449;&#21495;&#20266;&#24433;&#26816;&#27979;&#26159;&#20004;&#20010;&#20851;&#38190;&#30340;&#20363;&#23376;&#12290;&#22522;&#20110;&#26102;&#27573;&#30340;&#20998;&#31867;&#26159;&#20856;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#27979;&#36825;&#20123;&#20449;&#21495;&#20107;&#20214;&#65292;&#22240;&#20026;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#12290;&#36890;&#24120;&#65292;&#38656;&#35201;&#21518;&#22788;&#29702;&#25165;&#33021;&#23454;&#29616;&#33391;&#22909;&#30340;&#24615;&#33021;&#24182;&#24378;&#21046;&#26102;&#38388;&#20381;&#36182;&#24615;&#12290;&#35774;&#35745;&#21512;&#36866;&#30340;&#21518;&#22788;&#29702;&#26041;&#26696;&#26469;&#23558;&#36825;&#20123;&#20998;&#31867;&#36755;&#20986;&#36716;&#21270;&#20026;&#20107;&#20214;&#26159;&#36825;&#19968;&#26694;&#26550;&#20013;&#36153;&#26102;&#36153;&#21147;&#30340;&#37096;&#20998;&#12290;&#26041;&#27861;: &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20107;&#20214;&#30340;&#24314;&#27169;&#26694;&#26550;&#65292;&#30452;&#25509;&#20351;&#29992;&#20107;&#20214;&#20316;&#20026;&#23398;&#20064;&#30446;&#26631;&#65292;&#25682;&#24323;&#20102;&#20020;&#26102;&#30340;&#21518;&#22788;&#29702;&#26041;&#26696;&#23558;&#27169;&#22411;&#36755;&#20986;&#36716;&#21270;&#20026;&#20107;&#20214;&#12290;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#25968;&#25454;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#23637;&#31034;&#20102;&#36825;&#31181;&#26694;&#26550;&#30340;&#23454;&#38469;&#25928;&#26524;&#65292;&#24182;&#23558;&#20854;&#19982;&#22522;&#20110;&#26102;&#27573;&#30340;&#24314;&#27169;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;&#32467;&#26524;: &#25105;&#20204;&#34920;&#26126;&#65292;&#22522;&#20110;&#20107;&#20214;&#30340;&#24314;&#27169;&#65288;&#26080;&#38656;&#21518;&#22788;&#29702;&#65289;&#20855;&#26377;&#23454;&#29992;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective: Finding events of interest is a common task in biomedical signal processing. The detection of epileptic seizures and signal artefacts are two key examples. Epoch-based classification is the typical machine learning framework to detect such signal events because of the straightforward application of classical machine learning techniques. Usually, post-processing is required to achieve good performance and enforce temporal dependencies. Designing the right post-processing scheme to convert these classification outputs into events is a tedious, and labor-intensive element of this framework. Methods: We propose an event-based modeling framework that directly works with events as learning targets, stepping away from ad-hoc post-processing schemes to turn model outputs into events. We illustrate the practical power of this framework on simulated data and real-world data, comparing it to epoch-based modeling approaches. Results: We show that event-based modeling (without post-proce
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#24322;&#26041;&#24046;&#22122;&#22768;&#27169;&#22411;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;&#24191;&#20041;&#26681;&#22240;&#22240;&#26524;&#25512;&#26029;&#65288;GRCI&#65289;&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#35782;&#21035;&#30142;&#30149;&#30340;&#24739;&#32773;&#29305;&#23450;&#26681;&#26412;&#21407;&#22240;&#12290;</title><link>http://arxiv.org/abs/2205.13085</link><description>&lt;p&gt;
&#20351;&#29992;&#24322;&#26041;&#24046;&#22122;&#22768;&#27169;&#22411;&#35782;&#21035;&#29305;&#23450;&#24739;&#32773;&#30340;&#26681;&#26412;&#21407;&#22240;
&lt;/p&gt;
&lt;p&gt;
Identifying Patient-Specific Root Causes with the Heteroscedastic Noise Model. (arXiv:2205.13085v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.13085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#24322;&#26041;&#24046;&#22122;&#22768;&#27169;&#22411;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;&#24191;&#20041;&#26681;&#22240;&#22240;&#26524;&#25512;&#26029;&#65288;GRCI&#65289;&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#35782;&#21035;&#30142;&#30149;&#30340;&#24739;&#32773;&#29305;&#23450;&#26681;&#26412;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#30142;&#30149;&#30001;&#35768;&#22810;&#22240;&#32032;&#24341;&#36215;&#65292;&#21363;&#20351;&#22312;&#30456;&#21516;&#30340;&#35786;&#26029;&#31867;&#21035;&#20013;&#65292;&#36825;&#20123;&#22240;&#32032;&#21487;&#33021;&#22312;&#24739;&#32773;&#20043;&#38388;&#20063;&#20250;&#26377;&#25152;&#19981;&#21516;&#12290;&#28982;&#32780;&#65292;&#19968;&#20123;&#28508;&#22312;&#30340;&#26681;&#26412;&#21407;&#22240;&#20173;&#28982;&#21487;&#33021;&#22312;&#27599;&#20010;&#24739;&#32773;&#20013;&#24341;&#21457;&#30142;&#30149;&#30340;&#21457;&#23637;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#35782;&#21035;&#30142;&#30149;&#30340;&#24739;&#32773;&#29305;&#23450;&#26681;&#26412;&#21407;&#22240;&#65292;&#21363;&#22312;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;&#20013;&#22806;&#29983;&#35823;&#24046;&#39033;&#30340;&#26679;&#26412;&#29305;&#23450;&#39044;&#27979;&#33021;&#21147;&#12290;&#25105;&#20204;&#20174;&#32447;&#24615;&#35774;&#32622;&#25512;&#24191;&#21040;&#24322;&#26041;&#24046;&#22122;&#22768;&#27169;&#22411;&#65292;&#20854;&#20013;$ Y = m&#65288;X&#65289;+ \varepsilon\sigma&#65288;X&#65289;$&#65292;&#20854;&#20013;$m&#65288;X&#65289;$&#21644;$\sigma&#65288;X&#65289;$&#20998;&#21035;&#20195;&#34920;&#26465;&#20214;&#22343;&#20540;&#21644;&#22343;&#20540;&#32477;&#23545;&#20559;&#24046;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#12290;&#35813;&#27169;&#22411;&#20445;&#25345;&#21487;&#35782;&#21035;&#24615;&#65292;&#20294;&#24341;&#20837;&#20102;&#19968;&#20123;&#38750;&#24179;&#20961;&#30340;&#25361;&#25112;&#65292;&#38656;&#35201;&#19968;&#31181;&#21517;&#20026;&#24191;&#20041;&#26681;&#22240;&#22240;&#26524;&#25512;&#26029;&#65288;GRCI&#65289;&#30340;&#23450;&#21046;&#31639;&#27861;&#26469;&#27491;&#30830;&#25552;&#21462;&#35823;&#24046;&#39033;&#12290;&#19982;&#29616;&#26377;&#30340;&#26367;&#20195;&#26041;&#27861;&#30456;&#27604;&#65292;GRCI&#26356;&#20934;&#30830;&#22320;&#24674;&#22797;&#20102;&#29305;&#23450;&#24739;&#32773;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
Complex diseases are caused by a multitude of factors that may differ between patients even within the same diagnostic category. A few underlying root causes may nevertheless initiate the development of disease within each patient. We therefore focus on identifying patient-specific root causes of disease, which we equate to the sample-specific predictivity of the exogenous error terms in a structural equation model. We generalize from the linear setting to the heteroscedastic noise model where $Y = m(X) + \varepsilon\sigma(X)$ with non-linear functions $m(X)$ and $\sigma(X)$ representing the conditional mean and mean absolute deviation, respectively. This model preserves identifiability but introduces non-trivial challenges that require a customized algorithm called Generalized Root Causal Inference (GRCI) to extract the error terms correctly. GRCI recovers patient-specific root causes more accurately than existing alternatives.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20132;&#21449;&#37325;&#26500;&#21464;&#21387;&#22120;(CRT)&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36328;&#22495;&#20002;&#24323;-&#37325;&#26500;&#20219;&#21153;&#23454;&#29616;&#20102;&#26102;&#38388;&#24207;&#21015;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;CRT&#35299;&#20915;&#20102;&#26500;&#24314;&#25968;&#25454;&#23545;&#30340;&#20808;&#39564;&#30693;&#35782;&#12289;&#37319;&#26679;&#31574;&#30053;&#32321;&#29712;&#21644;&#37319;&#26679;&#20559;&#24046;&#23548;&#33268;&#24615;&#33021;&#19981;&#31283;&#23450;&#31561;&#38382;&#39064;&#65292;&#21516;&#26102;&#26377;&#25928;&#22320;&#24314;&#27169;&#20102;&#36328;&#26102;&#35889;&#20851;&#31995;&#20197;&#25193;&#23637;&#34920;&#31034;&#30340;&#23481;&#37327;&#12290;</title><link>http://arxiv.org/abs/2205.09928</link><description>&lt;p&gt;
&#36890;&#36807;&#20132;&#21449;&#37325;&#26500;&#21464;&#21387;&#22120;&#36827;&#34892;&#33258;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Time Series Representation Learning via Cross Reconstruction Transformer. (arXiv:2205.09928v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.09928
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20132;&#21449;&#37325;&#26500;&#21464;&#21387;&#22120;(CRT)&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36328;&#22495;&#20002;&#24323;-&#37325;&#26500;&#20219;&#21153;&#23454;&#29616;&#20102;&#26102;&#38388;&#24207;&#21015;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;CRT&#35299;&#20915;&#20102;&#26500;&#24314;&#25968;&#25454;&#23545;&#30340;&#20808;&#39564;&#30693;&#35782;&#12289;&#37319;&#26679;&#31574;&#30053;&#32321;&#29712;&#21644;&#37319;&#26679;&#20559;&#24046;&#23548;&#33268;&#24615;&#33021;&#19981;&#31283;&#23450;&#31561;&#38382;&#39064;&#65292;&#21516;&#26102;&#26377;&#25928;&#22320;&#24314;&#27169;&#20102;&#36328;&#26102;&#35889;&#20851;&#31995;&#20197;&#25193;&#23637;&#34920;&#31034;&#30340;&#23481;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;/&#33258;&#30417;&#30563;&#30340;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#23545;&#20110;&#30495;&#23454;&#22330;&#26223;&#20013;&#31232;&#32570;&#26631;&#35760;&#26679;&#26412;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#33258;&#21160;&#23398;&#20064;&#29702;&#35299;&#30456;&#20284;&#21644;&#19981;&#30456;&#20284;&#30340;&#25968;&#25454;&#23545;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#21463;&#38480;&#20110;&#26500;&#24314;&#23545;&#30340;&#20808;&#39564;&#30693;&#35782;&#12289;&#32321;&#29712;&#30340;&#37319;&#26679;&#31574;&#30053;&#20197;&#21450;&#22312;&#36935;&#21040;&#37319;&#26679;&#20559;&#24046;&#26102;&#24615;&#33021;&#19981;&#31283;&#23450;&#12290;&#27492;&#22806;&#65292;&#24456;&#23569;&#26377;&#24037;&#20316;&#19987;&#27880;&#20110;&#26377;&#25928;&#22320;&#24314;&#27169;&#36328;&#26102;&#35889;&#20851;&#31995;&#20197;&#25193;&#23637;&#34920;&#31034;&#30340;&#23481;&#37327;&#12290;&#26412;&#25991;&#26088;&#22312;&#20174;&#26032;&#30340;&#35282;&#24230;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#30340;&#34920;&#31034;&#65292;&#24182;&#25552;&#20986;&#20132;&#21449;&#37325;&#26500;&#21464;&#21387;&#22120;(CRT)&#20197;&#32479;&#19968;&#22320;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#12290;CRT&#36890;&#36807;&#36328;&#22495;&#20002;&#24323;-&#37325;&#26500;&#20219;&#21153;&#23454;&#29616;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#26102;&#38388;&#24207;&#21015;&#36716;&#25442;&#20026;&#39057;&#22495;&#65292;&#24182;&#38543;&#26426;&#20002;&#24323;&#26102;&#38388;&#21644;&#39057;&#29575;&#22495;&#20013;&#30340;&#26576;&#20123;&#37096;&#20998;&#65292;&#28982;&#21518;&#36890;&#36807;&#37325;&#26500;&#20219;&#21153;&#26469;&#23398;&#20064;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised/self-supervised representation learning in time series is critical since labeled samples are usually scarce in real-world scenarios. Existing approaches mainly leverage the contrastive learning framework, which automatically learns to understand the similar and dissimilar data pairs. Nevertheless, they are restricted to the prior knowledge of constructing pairs, cumbersome sampling policy, and unstable performances when encountering sampling bias. Also, few works have focused on effectively modeling across temporal-spectral relations to extend the capacity of representations. In this paper, we aim at learning representations for time series from a new perspective and propose Cross Reconstruction Transformer (CRT) to solve the aforementioned problems in a unified way. CRT achieves time series representation learning through a cross-domain dropping-reconstruction task. Specifically, we transform time series into the frequency domain and randomly drop certain parts in both ti
&lt;/p&gt;</description></item><item><title>k-strip&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22836;&#39592;&#21093;&#31163;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#30452;&#25509;&#22312;&#20449;&#24687;&#20016;&#23500;&#30340;k&#31354;&#38388;&#20013;&#24037;&#20316;&#65292;&#24182;&#19988;&#22312;&#19982;&#37329;&#26631;&#20934;&#30340;&#27604;&#36739;&#20013;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#30456;&#20284;&#24615;&#12290;</title><link>http://arxiv.org/abs/2205.09706</link><description>&lt;p&gt;
k-strip: &#19968;&#31181;&#22312;k&#31354;&#38388;&#20013;&#24212;&#29992;&#20110;&#22836;&#39592;&#21093;&#31163;&#30340;&#26032;&#22411;&#20998;&#21106;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
k-strip: A novel segmentation algorithm in k-space for the application of skull stripping. (arXiv:2205.09706v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.09706
&lt;/p&gt;
&lt;p&gt;
k-strip&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22836;&#39592;&#21093;&#31163;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#30452;&#25509;&#22312;&#20449;&#24687;&#20016;&#23500;&#30340;k&#31354;&#38388;&#20013;&#24037;&#20316;&#65292;&#24182;&#19988;&#22312;&#19982;&#37329;&#26631;&#20934;&#30340;&#27604;&#36739;&#20013;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#65306;&#25552;&#20986;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22836;&#39592;&#21093;&#31163;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#30452;&#25509;&#22312;&#20449;&#24687;&#20016;&#23500;&#30340;k&#31354;&#38388;&#20013;&#24037;&#20316;&#12290;&#26041;&#27861;&#65306;&#20351;&#29992;&#26469;&#33258;&#19981;&#21516;&#26426;&#26500;&#30340;&#20004;&#20010;&#25968;&#25454;&#38598;&#20849;&#35745;36,900&#20010;MRI&#20999;&#29255;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#30452;&#25509;&#22788;&#29702;&#22797;&#26434;&#30340;&#21407;&#22987;k&#31354;&#38388;&#25968;&#25454;&#12290;&#22312;&#22270;&#20687;&#22495;&#20013;&#20351;&#29992;HD-BET&#65288;&#33041;&#25552;&#21462;&#24037;&#20855;&#65289;&#36827;&#34892;&#30340;&#22836;&#39592;&#21093;&#31163;&#34987;&#29992;&#20316;&#37329;&#26631;&#20934;&#12290;&#32467;&#26524;&#65306;&#20004;&#20010;&#25968;&#25454;&#38598;&#19982;&#37329;&#26631;&#20934;&#38750;&#24120;&#30456;&#20284;&#65288;DICE&#20998;&#25968;&#20026;92\%-98\%&#65292;Hausdorff&#36317;&#31163;&#20302;&#20110;5.5&#27627;&#31859;&#65289;&#12290;&#30524;&#37096;&#20197;&#19978;&#20999;&#29255;&#30340;DICE&#20998;&#25968;&#36798;&#21040;&#20102;99\%&#65292;&#32780;&#30524;&#37096;&#21608;&#22260;&#21644;&#19979;&#26041;&#30340;&#20934;&#30830;&#24615;&#19979;&#38477;&#65292;&#36755;&#20986;&#37096;&#20998;&#27169;&#31946;&#12290;k-strip&#30340;&#36755;&#20986;&#36890;&#24120;&#22312;&#19982;&#22836;&#39592;&#20043;&#38388;&#30340;&#20998;&#30028;&#22788;&#24179;&#28369;&#36793;&#32536;&#12290;&#20351;&#29992;&#36866;&#24403;&#30340;&#38408;&#20540;&#21019;&#24314;&#20108;&#36827;&#21046;&#25513;&#27169;&#12290;&#32467;&#35770;&#65306;&#36890;&#36807;&#36825;&#20010;&#27010;&#24565;&#39564;&#35777;&#30740;&#31350;&#65292;&#25105;&#20204;&#33021;&#22815;&#23637;&#31034;&#22312;k&#31354;&#38388;&#20013;&#24037;&#20316;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objectives: Present a novel deep learning-based skull stripping algorithm for magnetic resonance imaging (MRI) that works directly in the information rich k-space.  Materials and Methods: Using two datasets from different institutions with a total of 36,900 MRI slices, we trained a deep learning-based model to work directly with the complex raw k-space data. Skull stripping performed by HD-BET (Brain Extraction Tool) in the image domain were used as the ground truth.  Results: Both datasets were very similar to the ground truth (DICE scores of 92\%-98\% and Hausdorff distances of under 5.5 mm). Results on slices above the eye-region reach DICE scores of up to 99\%, while the accuracy drops in regions around the eyes and below, with partially blurred output. The output of k-strip often smoothed edges at the demarcation to the skull. Binary masks are created with an appropriate threshold.  Conclusion: With this proof-of-concept study, we were able to show the feasibility of working in th
&lt;/p&gt;</description></item><item><title>GRAPHSHAP&#26159;&#19968;&#31181;&#22522;&#20110;Shapley&#20540;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#20379;&#22522;&#20110;&#27169;&#26679;&#24335;&#30340;&#35299;&#37322;&#65292;&#29992;&#20110;&#35299;&#37322;&#22522;&#20110;&#36523;&#20221;&#24863;&#30693;&#30340;&#22270;&#20998;&#31867;&#22120;&#65292;&#32780;&#26080;&#38656;&#23545;&#27169;&#22411;&#25110;&#20854;&#35757;&#32451;&#25968;&#25454;&#26377;&#20219;&#20309;&#20808;&#39564;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2202.08815</link><description>&lt;p&gt;
GRAPHSHAP&#65306;&#36890;&#36807;&#27169;&#26679;&#24335;&#30340;&#35821;&#35328;&#35299;&#37322;&#22522;&#20110;&#36523;&#20221;&#24863;&#30693;&#30340;&#22270;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
GRAPHSHAP: Explaining Identity-Aware Graph Classifiers Through the Language of Motifs. (arXiv:2202.08815v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.08815
&lt;/p&gt;
&lt;p&gt;
GRAPHSHAP&#26159;&#19968;&#31181;&#22522;&#20110;Shapley&#20540;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#20379;&#22522;&#20110;&#27169;&#26679;&#24335;&#30340;&#35299;&#37322;&#65292;&#29992;&#20110;&#35299;&#37322;&#22522;&#20110;&#36523;&#20221;&#24863;&#30693;&#30340;&#22270;&#20998;&#31867;&#22120;&#65292;&#32780;&#26080;&#38656;&#23545;&#27169;&#22411;&#25110;&#20854;&#35757;&#32451;&#25968;&#25454;&#26377;&#20219;&#20309;&#20808;&#39564;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#35299;&#37322;&#40657;&#30418;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#65288;&#20363;&#22914;&#22312;&#34920;&#26684;&#25968;&#25454;&#12289;&#22270;&#20687;&#25110;&#26102;&#38388;&#24207;&#21015;&#19978;&#65289;&#20381;&#36182;&#20110;&#34913;&#37327;&#21024;&#38500;/&#25200;&#21160;&#29305;&#24449;&#23545;&#27169;&#22411;&#36755;&#20986;&#30340;&#24433;&#21709;&#12290;&#36825;&#35201;&#27714;&#35299;&#37322;&#35821;&#35328;&#19982;&#20998;&#31867;&#22120;&#30340;&#29305;&#24449;&#31354;&#38388;&#21305;&#37197;&#12290;&#28982;&#32780;&#65292;&#22312;&#22788;&#29702;&#22270;&#25968;&#25454;&#26102;&#65292;&#22522;&#26412;&#29305;&#24449;&#23545;&#24212;&#20110;&#25551;&#36848;&#22270;&#32467;&#26500;&#30340;&#36793;&#65292;&#29305;&#24449;&#31354;&#38388;&#19982;&#35299;&#37322;&#35821;&#35328;&#20043;&#38388;&#30340;&#21305;&#37197;&#21487;&#33021;&#19981;&#21512;&#36866;&#12290;&#20026;&#20102;&#20026;&#22270;&#20998;&#31867;&#20219;&#21153;&#24320;&#21457;&#21487;&#34892;&#30340;&#35299;&#37322;&#65292;&#23558;&#29305;&#24449;&#31354;&#38388;&#65288;&#36793;&#65289;&#19982;&#25152;&#38656;&#30340;&#39640;&#32423;&#35299;&#37322;&#35821;&#35328;&#65288;&#22914;&#27169;&#26679;&#24335;&#65289;&#35299;&#32806;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;Shapley&#20540;&#30340;&#26041;&#27861;GRAPHSHAP&#65292;&#33021;&#22815;&#20026;&#36523;&#20221;&#24863;&#30693;&#30340;&#22270;&#20998;&#31867;&#22120;&#25552;&#20379;&#22522;&#20110;&#27169;&#26679;&#24335;&#30340;&#35299;&#37322;&#65292;&#19981;&#38656;&#35201;&#23545;&#27169;&#22411;&#25110;&#20854;&#35757;&#32451;&#25968;&#25454;&#26377;&#20219;&#20309;&#20808;&#39564;&#30693;&#35782;&#65306;&#21807;&#19968;&#30340;&#35201;&#27714;&#26159;&#21487;&#20197;&#20219;&#24847;&#26597;&#35810;&#40657;&#30418;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most methods for explaining black-box classifiers (e.g. on tabular data, images, or time series) rely on measuring the impact that removing/perturbing features has on the model output. This forces the explanation language to match the classifier's feature space. However, when dealing with graph data, in which the basic features correspond to the edges describing the graph structure, this matching between features space and explanation language might not be appropriate. Decoupling the feature space (edges) from a desired high-level explanation language (such as motifs) is thus a major challenge towards developing actionable explanations for graph classification tasks. In this paper we introduce GRAPHSHAP, a Shapley-based approach able to provide motif-based explanations for identity-aware graph classifiers, assuming no knowledge whatsoever about the model or its training data: the only requirement is that the classifier can be queried as a black-box at will. For the sake of computationa
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28155;&#21152;&#22870;&#21169;&#21152;&#25104;&#30340;&#23376;&#20219;&#21153;&#26469;&#21457;&#29616;&#36873;&#39033;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20197;&#21069;&#26041;&#27861;&#20013;&#24573;&#30053;&#21407;&#22987;&#22870;&#21169;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2202.03466</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#36981;&#24490;&#22870;&#21169;&#30340;&#23376;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Reward-Respecting Subtasks for Model-Based Reinforcement Learning. (arXiv:2202.03466v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.03466
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28155;&#21152;&#22870;&#21169;&#21152;&#25104;&#30340;&#23376;&#20219;&#21153;&#26469;&#21457;&#29616;&#36873;&#39033;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20197;&#21069;&#26041;&#27861;&#20013;&#24573;&#30053;&#21407;&#22987;&#22870;&#21169;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#30340;&#36828;&#22823;&#30446;&#26631;&#65292;&#24378;&#21270;&#23398;&#20064;&#24517;&#39035;&#21253;&#25324;&#23545;&#25277;&#35937;&#29366;&#24577;&#21644;&#26102;&#38388;&#30340;&#19990;&#30028;&#27169;&#22411;&#30340;&#35268;&#21010;&#12290;&#28145;&#24230;&#23398;&#20064;&#22312;&#29366;&#24577;&#25277;&#35937;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#26102;&#38388;&#25277;&#35937;&#21364;&#24456;&#23569;&#34987;&#20351;&#29992;&#65292;&#23613;&#31649;&#22522;&#20110;&#36873;&#39033;&#26694;&#26550;&#24050;&#32463;&#24191;&#27867;&#21457;&#23637;&#20102;&#29702;&#35770;&#12290;&#20854;&#20013;&#19968;&#20010;&#21407;&#22240;&#26159;&#21487;&#33021;&#30340;&#36873;&#39033;&#31354;&#38388;&#24456;&#22823;&#65292;&#20197;&#21069;&#25552;&#20986;&#30340;&#36873;&#39033;&#21457;&#29616;&#26041;&#27861;&#27809;&#26377;&#32771;&#34385;&#21040;&#36873;&#39033;&#27169;&#22411;&#22312;&#35268;&#21010;&#20013;&#30340;&#20351;&#29992;&#26041;&#24335;&#12290;&#36890;&#24120;&#36890;&#36807;&#25552;&#20986;&#23376;&#20219;&#21153;&#65288;&#20363;&#22914;&#36798;&#21040;&#29942;&#39048;&#29366;&#24577;&#25110;&#26368;&#22823;&#21270;&#38500;&#22870;&#21169;&#22806;&#30340;&#24863;&#30693;&#20449;&#21495;&#30340;&#32047;&#31215;&#21644;&#65289;&#26469;&#21457;&#29616;&#36873;&#39033;&#12290;&#35299;&#20915;&#27599;&#20010;&#23376;&#20219;&#21153;&#20197;&#29983;&#25104;&#19968;&#20010;&#36873;&#39033;&#65292;&#28982;&#21518;&#23398;&#20064;&#36873;&#39033;&#30340;&#27169;&#22411;&#24182;&#20351;&#20854;&#21487;&#29992;&#20110;&#35268;&#21010;&#36807;&#31243;&#12290;&#22312;&#22823;&#22810;&#25968;&#20197;&#21069;&#30340;&#30740;&#31350;&#20013;&#65292;&#23376;&#20219;&#21153;&#24573;&#30053;&#20102;&#21407;&#22987;&#38382;&#39064;&#19978;&#30340;&#22870;&#21169;&#65292;&#32780;&#25105;&#20204;&#25552;&#20986;&#30340;&#23376;&#20219;&#21153;&#20351;&#29992;&#21407;&#22987;&#22870;&#21169;&#21152;&#19978;&#22522;&#20110;&#26576;&#20010;&#29305;&#24449;&#30340;&#22870;&#21169;&#21152;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
To achieve the ambitious goals of artificial intelligence, reinforcement learning must include planning with a model of the world that is abstract in state and time. Deep learning has made progress with state abstraction, but temporal abstraction has rarely been used, despite extensively developed theory based on the options framework. One reason for this is that the space of possible options is immense, and the methods previously proposed for option discovery do not take into account how the option models will be used in planning. Options are typically discovered by posing subsidiary tasks, such as reaching a bottleneck state or maximizing the cumulative sum of a sensory signal other than reward. Each subtask is solved to produce an option, and then a model of the option is learned and made available to the planning process. In most previous work, the subtasks ignore the reward on the original problem, whereas we propose subtasks that use the original reward plus a bonus based on a fe
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#26368;&#20248;&#20256;&#36755;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;SPD&#27969;&#24418;&#19978;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#26368;&#20248;&#20256;&#36755;&#29702;&#35770;&#21644;SPD&#27969;&#24418;&#30340;&#23545;&#25968;&#27431;&#20960;&#37324;&#24471;&#20960;&#20309;&#65292;&#25105;&#20204;&#20811;&#26381;&#20102;&#21327;&#26041;&#24046;&#30697;&#38453;&#25805;&#20316;&#30340;&#22797;&#26434;&#24615;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2201.05745</link><description>&lt;p&gt;
&#22522;&#20110;SPD&#27969;&#24418;&#30340;&#28145;&#24230;&#26368;&#20248;&#20256;&#36755;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Deep Optimal Transport for Domain Adaptation on SPD Manifolds. (arXiv:2201.05745v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.05745
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#26368;&#20248;&#20256;&#36755;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;SPD&#27969;&#24418;&#19978;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#26368;&#20248;&#20256;&#36755;&#29702;&#35770;&#21644;SPD&#27969;&#24418;&#30340;&#23545;&#25968;&#27431;&#20960;&#37324;&#24471;&#20960;&#20309;&#65292;&#25105;&#20204;&#20811;&#26381;&#20102;&#21327;&#26041;&#24046;&#30697;&#38453;&#25805;&#20316;&#30340;&#22797;&#26434;&#24615;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#30028;&#23545;&#20110;&#22312;&#23545;&#31216;&#27491;&#23450;&#65288;SPD&#65289;&#27969;&#24418;&#19978;&#35299;&#20915;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;DA&#65289;&#38382;&#39064;&#34920;&#29616;&#20986;&#20102;&#24456;&#22823;&#20852;&#36259;&#12290;&#36825;&#31181;&#20852;&#36259;&#28304;&#20110;&#21307;&#30103;&#35774;&#22791;&#20135;&#29983;&#30340;&#22797;&#26434;&#31070;&#32463;&#29289;&#29702;&#25968;&#25454;&#65288;&#22914;&#33041;&#30005;&#22270;&#12289;&#33041;&#30913;&#22270;&#21644;&#25193;&#25955;&#24352;&#37327;&#25104;&#20687;&#65289;&#22312;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#23384;&#22312;&#25968;&#25454;&#20998;&#24067;&#30340;&#20559;&#31227;&#12290;&#36825;&#20123;&#25968;&#25454;&#34920;&#31034;&#20197;&#20449;&#21495;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#24418;&#24335;&#34920;&#31034;&#65292;&#24182;&#20855;&#26377;&#23545;&#31216;&#24615;&#21644;&#27491;&#23450;&#24615;&#30340;&#23646;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#22797;&#26434;&#25805;&#20316;&#29305;&#24615;&#65292;&#30452;&#25509;&#23558;&#20808;&#21069;&#30340;&#32463;&#39564;&#21644;&#35299;&#20915;&#26041;&#26696;&#24212;&#29992;&#20110;DA&#38382;&#39064;&#23384;&#22312;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31867;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;&#28145;&#24230;&#26368;&#20248;&#20256;&#36755;&#12290;&#36825;&#19968;&#31867;&#26041;&#27861;&#21033;&#29992;&#26368;&#20248;&#20256;&#36755;&#29702;&#35770;&#65292;&#24182;&#21033;&#29992;SPD&#27969;&#24418;&#30340;&#23545;&#25968;&#27431;&#20960;&#37324;&#24471;&#20960;&#20309;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
In recent years, there has been significant interest in solving the domain adaptation (DA) problem on symmetric positive definite (SPD) manifolds within the machine learning community. This interest stems from the fact that complex neurophysiological data generated by medical equipment, such as electroencephalograms, magnetoencephalograms, and diffusion tensor imaging, often exhibit a shift in data distribution across different domains. These data representations, represented by signal covariance matrices, possess properties of symmetry and positive definiteness. However, directly applying previous experiences and solutions to the DA problem poses challenges due to the manipulation complexities of covariance matrices.To address this, our research introduces a category of deep learning-based transfer learning approaches called deep optimal transport. This category utilizes optimal transport theory and leverages the Log-Euclidean geometry for SPD manifolds. Additionally, we present a com
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;DRL&#20013;&#23884;&#20837;&#31526;&#21495;&#30693;&#35782;&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#25928;&#29575;&#12289;&#35299;&#37322;&#24615;&#32570;&#20047;&#21644;&#21487;&#36801;&#31227;&#24615;&#31561;&#20851;&#38190;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#24490;&#29615;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#21033;&#29992;&#23398;&#20064;&#30340;&#35268;&#21010;&#27169;&#22411;&#21644;&#31526;&#21495;&#36873;&#39033;&#26469;&#24341;&#23548;&#31574;&#30053;&#30340;&#25913;&#36827;&#12290;&#23398;&#21040;&#30340;&#31526;&#21495;&#36873;&#39033;&#20943;&#36731;&#20102;&#23545;&#19987;&#23478;&#39046;&#22495;&#30693;&#35782;&#30340;&#35201;&#27714;&#65292;&#24182;&#25552;&#20379;&#20102;&#31574;&#30053;&#30340;&#20869;&#22312;&#21487;&#35299;&#37322;&#24615;&#65292;&#21516;&#26102;&#36890;&#36807;&#19982;&#31526;&#21495;&#35268;&#21010;&#27169;&#22411;&#30340;&#35268;&#21010;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#21487;&#36801;&#31227;&#24615;&#21644;&#25968;&#25454;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2112.09836</link><description>&lt;p&gt;
AI&#30340;&#21019;&#36896;&#21147;&#65306;&#29992;&#20110;&#20419;&#36827;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#23618;&#35268;&#21010;&#27169;&#22411;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Creativity of AI: Hierarchical Planning Model Learning for Facilitating Deep Reinforcement Learning. (arXiv:2112.09836v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.09836
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;DRL&#20013;&#23884;&#20837;&#31526;&#21495;&#30693;&#35782;&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#25928;&#29575;&#12289;&#35299;&#37322;&#24615;&#32570;&#20047;&#21644;&#21487;&#36801;&#31227;&#24615;&#31561;&#20851;&#38190;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#24490;&#29615;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#21033;&#29992;&#23398;&#20064;&#30340;&#35268;&#21010;&#27169;&#22411;&#21644;&#31526;&#21495;&#36873;&#39033;&#26469;&#24341;&#23548;&#31574;&#30053;&#30340;&#25913;&#36827;&#12290;&#23398;&#21040;&#30340;&#31526;&#21495;&#36873;&#39033;&#20943;&#36731;&#20102;&#23545;&#19987;&#23478;&#39046;&#22495;&#30693;&#35782;&#30340;&#35201;&#27714;&#65292;&#24182;&#25552;&#20379;&#20102;&#31574;&#30053;&#30340;&#20869;&#22312;&#21487;&#35299;&#37322;&#24615;&#65292;&#21516;&#26102;&#36890;&#36807;&#19982;&#31526;&#21495;&#35268;&#21010;&#27169;&#22411;&#30340;&#35268;&#21010;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#21487;&#36801;&#31227;&#24615;&#21644;&#25968;&#25454;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#20173;&#28982;&#38754;&#20020;&#19977;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#21363;&#25968;&#25454;&#25928;&#29575;&#12289;&#35299;&#37322;&#24615;&#30340;&#32570;&#20047;&#21644;&#21487;&#36801;&#31227;&#24615;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#26174;&#31034;&#65292;&#22312;DRL&#20013;&#23884;&#20837;&#31526;&#21495;&#30693;&#35782;&#26377;&#26395;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20855;&#26377;&#31526;&#21495;&#36873;&#39033;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20855;&#26377;&#19968;&#20010;&#24490;&#29615;&#35757;&#32451;&#36807;&#31243;&#65292;&#36890;&#36807;&#19982;&#20132;&#20114;&#36712;&#36857;&#23398;&#20064;&#30340;&#35268;&#21010;&#27169;&#22411;&#65288;&#21253;&#25324;&#21160;&#20316;&#27169;&#22411;&#21644;&#20998;&#23618;&#20219;&#21153;&#32593;&#32476;&#27169;&#22411;&#65289;&#21644;&#31526;&#21495;&#36873;&#39033;&#26469;&#24341;&#23548;&#31574;&#30053;&#30340;&#25913;&#36827;&#12290;&#23398;&#21040;&#30340;&#31526;&#21495;&#36873;&#39033;&#20943;&#36731;&#20102;&#23545;&#19987;&#23478;&#39046;&#22495;&#30693;&#35782;&#30340;&#35201;&#27714;&#65292;&#24182;&#25552;&#20379;&#20102;&#31574;&#30053;&#30340;&#20869;&#22312;&#21487;&#35299;&#37322;&#24615;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#19982;&#31526;&#21495;&#35268;&#21010;&#27169;&#22411;&#30340;&#35268;&#21010;&#65292;&#21487;&#36827;&#19968;&#27493;&#25552;&#39640;&#21487;&#36801;&#31227;&#24615;&#21644;&#25968;&#25454;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite of achieving great success in real-world applications, Deep Reinforcement Learning (DRL) is still suffering from three critical issues, i.e., data efficiency, lack of the interpretability and transferability. Recent research shows that embedding symbolic knowledge into DRL is promising in addressing those challenges. Inspired by this, we introduce a novel deep reinforcement learning framework with symbolic options. Our framework features a loop training procedure, which enables guiding the improvement of policy by planning with planning models (including action models and hierarchical task network models) and symbolic options learned from interactive trajectories automatically. The learned symbolic options alleviate the dense requirement of expert domain knowledge and provide inherent interpretability of policies. Moreover, the transferability and data efficiency can be further improved by planning with the symbolic planning models. To validate the effectiveness of our framewor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25932;&#23545;&#25915;&#20987;&#30340;&#21487;&#33021;&#24615;&#21644;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26694;&#26550;&#26469;&#30740;&#31350;&#22312;&#20154;&#31867;&#35780;&#20272;&#19979;&#22914;&#20309;&#29983;&#25104;&#25932;&#23545;&#31034;&#20363;&#12290;</title><link>http://arxiv.org/abs/2107.01943</link><description>&lt;p&gt;
&#22914;&#20309;&#22312;&#21487;&#35299;&#37322;&#27169;&#22411; (&#20197;&#21450;&#20154;&#31867;) &#20013;&#24858;&#24324;&#25932;&#23545;&#31034;&#20363;
&lt;/p&gt;
&lt;p&gt;
When and How to Fool Explainable Models (and Humans) with Adversarial Examples. (arXiv:2107.01943v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.01943
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25932;&#23545;&#25915;&#20987;&#30340;&#21487;&#33021;&#24615;&#21644;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26694;&#26550;&#26469;&#30740;&#31350;&#22312;&#20154;&#31867;&#35780;&#20272;&#19979;&#22914;&#20309;&#29983;&#25104;&#25932;&#23545;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22810;&#31181;&#38480;&#21046;&#65292;&#22914;&#31070;&#32463;&#32593;&#32476;&#30340;&#35299;&#37322;&#24615;&#19981;&#36275;&#21644;&#23545;&#25932;&#23545;&#31034;&#20363;&#25110;&#31163;&#32676;&#36755;&#20837;&#30340;&#40065;&#26834;&#24615;&#19981;&#36275;&#65292;&#21487;&#38752;&#37096;&#32626;&#35832;&#22914;&#31070;&#32463;&#32593;&#32476;&#20043;&#31867;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#31687;&#25506;&#32034;&#24615;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#38024;&#23545;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25932;&#23545;&#25915;&#20987;&#30340;&#21487;&#33021;&#24615;&#21644;&#38480;&#21046;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#25932;&#23545;&#31034;&#20363;&#30340;&#27010;&#24565;&#25193;&#23637;&#21040;&#36866;&#29992;&#20110;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#22330;&#26223;&#20013;&#65292;&#20854;&#20013;&#36755;&#20837;&#12289;&#36755;&#20986;&#20998;&#31867;&#21644;&#27169;&#22411;&#20915;&#31574;&#30340;&#35299;&#37322;&#30001;&#20154;&#31867;&#35780;&#20272;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26694;&#26550;&#26469;&#30740;&#31350;&#22312;&#20154;&#31867;&#35780;&#20272;&#19979;&#26159;&#21542;&#65288;&#20197;&#21450;&#22914;&#20309;&#65289;&#20026;&#21487;&#35299;&#37322;&#27169;&#22411;&#29983;&#25104;&#25932;&#23545;&#31034;&#20363;&#65292;&#24182;&#24341;&#20837;&#21644;&#35828;&#26126;&#20102;&#26032;&#30340;&#25915;&#20987;&#33539;&#24335;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#32771;&#34385;&#20102;&#19968;&#31995;&#21015;&#30456;&#20851;&#20294;&#24120;&#24120;&#34987;&#24573;&#30053;&#30340;&#22240;&#32032;&#65292;&#20363;&#22914;&#38382;&#39064;&#31867;&#22411;&#12289;&#29992;&#25143;&#19987;&#19994;&#30693;&#35782;&#25110;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reliable deployment of machine learning models such as neural networks continues to be challenging due to several limitations. Some of the main shortcomings are the lack of interpretability and the lack of robustness against adversarial examples or out-of-distribution inputs. In this exploratory review, we explore the possibilities and limits of adversarial attacks for explainable machine learning models. First, we extend the notion of adversarial examples to fit in explainable machine learning scenarios, in which the inputs, the output classifications and the explanations of the model's decisions are assessed by humans. Next, we propose a comprehensive framework to study whether (and how) adversarial examples can be generated for explainable models under human assessment, introducing and illustrating novel attack paradigms. In particular, our framework considers a wide range of relevant yet often ignored factors such as the type of problem, the user expertise or the objective of the e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28789;&#27963;&#23436;&#20840;&#20998;&#25955;&#21270;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#22823;&#35268;&#27169;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#22522;&#20110;&#21407;&#22987;-&#23545;&#20598;&#28151;&#21512;&#26799;&#24230;&#19979;&#38477;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#21487;&#20197;&#20998;&#21035;&#23398;&#20064;&#27599;&#20010;&#26234;&#33021;&#20307;&#65292;&#24182;&#23454;&#29616;&#31574;&#30053;&#25913;&#36827;&#21644;&#20215;&#20540;&#35780;&#20215;&#12290;</title><link>http://arxiv.org/abs/2004.11145</link><description>&lt;p&gt;
F2A2: &#28789;&#27963;&#23436;&#20840;&#21435;&#20013;&#24515;&#21270;&#30340;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#36817;&#20284;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
F2A2: Flexible Fully-decentralized Approximate Actor-critic for Cooperative Multi-agent Reinforcement Learning. (arXiv:2004.11145v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2004.11145
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28789;&#27963;&#23436;&#20840;&#20998;&#25955;&#21270;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#22823;&#35268;&#27169;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#22522;&#20110;&#21407;&#22987;-&#23545;&#20598;&#28151;&#21512;&#26799;&#24230;&#19979;&#38477;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#21487;&#20197;&#20998;&#21035;&#23398;&#20064;&#27599;&#20010;&#26234;&#33021;&#20307;&#65292;&#24182;&#23454;&#29616;&#31574;&#30053;&#25913;&#36827;&#21644;&#20215;&#20540;&#35780;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#20013;&#22830;&#38598;&#26435;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#22797;&#26434;&#24212;&#29992;&#20013;&#26377;&#26102;&#19981;&#23454;&#29992;&#65292;&#22240;&#20026;&#26234;&#33021;&#20307;&#20043;&#38388;&#32570;&#20047;&#20114;&#21160;&#65292;&#23384;&#22312;&#32500;&#24230;&#28798;&#38590;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#22240;&#27492;&#65292;&#20986;&#29616;&#20102;&#19968;&#20123;&#20998;&#25955;&#21270;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20998;&#25955;&#21270;&#26041;&#27861;&#21482;&#33021;&#22788;&#29702;&#23436;&#20840;&#21512;&#20316;&#30340;&#35774;&#32622;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#38656;&#35201;&#20256;&#36755;&#22823;&#37327;&#20449;&#24687;&#12290;&#20256;&#32479;&#30340;&#22359;&#22352;&#26631;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#21487;&#20197;&#31616;&#21270;&#35745;&#31639;&#65292;&#20294;&#20250;&#24341;&#36215;&#20005;&#37325;&#30340;&#20559;&#24046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#23436;&#20840;&#20998;&#25955;&#21270;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#32452;&#21512;&#22823;&#22810;&#25968;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#65292;&#24182;&#22788;&#29702;&#22823;&#35268;&#27169;&#19968;&#33324;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#12290;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#22987;-&#23545;&#20598;&#28151;&#21512;&#26799;&#24230;&#19979;&#38477;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#20998;&#21035;&#23398;&#20064;&#27599;&#20010;&#26234;&#33021;&#20307;&#26469;&#23454;&#29616;&#20998;&#25955;&#21270;&#12290;&#20174;&#27599;&#20010;&#26234;&#33021;&#20307;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#23454;&#29616;&#20102;&#31574;&#30053;&#25913;&#36827;&#21644;&#20215;&#20540;&#35780;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional centralized multi-agent reinforcement learning (MARL) algorithms are sometimes unpractical in complicated applications, due to non-interactivity between agents, curse of dimensionality and computation complexity. Hence, several decentralized MARL algorithms are motivated. However, existing decentralized methods only handle the fully cooperative setting where massive information needs to be transmitted in training. The block coordinate gradient descent scheme they used for successive independent actor and critic steps can simplify the calculation, but it causes serious bias. In this paper, we propose a flexible fully decentralized actor-critic MARL framework, which can combine most of actor-critic methods, and handle large-scale general cooperative multi-agent setting. A primal-dual hybrid gradient descent type algorithm framework is designed to learn individual agents separately for decentralization. From the perspective of each agent, policy improvement and value evaluatio
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21407;&#21017; "&#21521;&#21518;&#29305;&#24449;&#20462;&#27491;"&#65292;&#36890;&#36807;&#35757;&#32451;&#36807;&#31243;&#20013;&#33258;&#21160;&#20462;&#27491;&#36739;&#20302;&#23618;&#27425;&#29305;&#24449;&#30340;&#38169;&#35823;&#65292;&#20351;&#24471;&#28145;&#24230;&#23398;&#20064;&#33021;&#22815;&#36827;&#34892;&#28145;&#24230;&#65288;&#20998;&#23618;&#65289;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2001.04413</link><description>&lt;p&gt;
&#12298;&#21521;&#21518;&#29305;&#24449;&#20462;&#27491;&#65306;&#28145;&#24230;&#23398;&#20064;&#22914;&#20309;&#36827;&#34892;&#28145;&#24230;&#65288;&#20998;&#23618;&#65289;&#23398;&#20064;&#12299;
&lt;/p&gt;
&lt;p&gt;
Backward Feature Correction: How Deep Learning Performs Deep (Hierarchical) Learning. (arXiv:2001.04413v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2001.04413
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21407;&#21017; "&#21521;&#21518;&#29305;&#24449;&#20462;&#27491;"&#65292;&#36890;&#36807;&#35757;&#32451;&#36807;&#31243;&#20013;&#33258;&#21160;&#20462;&#27491;&#36739;&#20302;&#23618;&#27425;&#29305;&#24449;&#30340;&#38169;&#35823;&#65292;&#20351;&#24471;&#28145;&#24230;&#23398;&#20064;&#33021;&#22815;&#36827;&#34892;&#28145;&#24230;&#65288;&#20998;&#23618;&#65289;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20063;&#34987;&#31216;&#20026;&#20998;&#23618;&#23398;&#20064;&#65292;&#23398;&#20064;&#32773;&#36890;&#36807;&#23558;&#22797;&#26434;&#30340;&#30446;&#26631;&#20989;&#25968;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#26356;&#31616;&#21333;&#30340;&#20989;&#25968;&#26469;&#38477;&#20302;&#26679;&#26412;&#21644;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#35757;&#32451;&#30446;&#26631;&#36827;&#34892;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65292;&#20174;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#22810;&#23618;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#26377;&#25928;&#21644;&#33258;&#21160;&#22320;&#36827;&#34892;&#36825;&#31181;&#20998;&#23618;&#23398;&#20064;&#12290;&#22312;&#27010;&#24565;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#29305;&#24449;&#65292;&#21363;&#26576;&#20123;&#31867;&#22411;&#30340;&#28145;&#24230;&#65288;&#21363;&#36229;&#24120;&#23618;&#65289;&#31070;&#32463;&#32593;&#32476;&#22312;&#26576;&#20123;&#20998;&#23618;&#20219;&#21153;&#19978;&#20173;&#28982;&#21487;&#20197;&#20197;&#39640;&#25928;&#29575;&#30340;&#26679;&#26412;&#21644;&#26102;&#38388;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#29616;&#26377;&#30340;&#31639;&#27861;&#65288;&#21253;&#25324;&#36880;&#23618;&#35757;&#32451;&#12289;&#26680;&#26041;&#27861;&#31561;&#65289;&#22343;&#26080;&#27861;&#39640;&#25928;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#21521;&#21518;&#29305;&#24449;&#20462;&#27491;&#8221;&#30340;&#26032;&#21407;&#21017;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#36739;&#20302;&#23618;&#27425;&#29305;&#24449;&#30340;&#38169;&#35823;&#21487;&#20197;&#33258;&#21160;&#20462;&#27491;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#26159;&#28145;&#24230;&#23398;&#20064;&#22914;&#20309;&#36827;&#34892;&#28145;&#24230;&#65288;&#20998;&#23618;&#65289;&#23398;&#20064;&#30340;&#20851;&#38190;&#25152;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning is also known as hierarchical learning, where the learner _learns_ to represent a complicated target function by decomposing it into a sequence of simpler functions to reduce sample and time complexity. This paper formally analyzes how multi-layer neural networks can perform such hierarchical learning _efficiently_ and _automatically_ by SGD on the training objective.  On the conceptual side, we present a theoretical characterizations of how certain types of deep (i.e. super-constant layer) neural networks can still be sample and time efficiently trained on some hierarchical tasks, when no existing algorithm (including layerwise training, kernel method, etc) is known to be efficient. We establish a new principle called "backward feature correction", where the errors in the lower-level features can be automatically corrected when training together with the higher-level layers. We believe this is a key behind how deep learning is performing deep (hierarchical) learning, as 
&lt;/p&gt;</description></item></channel></rss>