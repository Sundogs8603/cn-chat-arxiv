<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#36890;&#36807;&#20351;&#29992;&#27491;&#21017;&#27969;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#24046;&#20998;&#38544;&#31169;&#26426;&#22120;&#23398;&#20064;&#20013;&#21382;&#21490;&#38590;&#39064;&#65292;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;</title><link>https://rss.arxiv.org/abs/2311.09200</link><description>&lt;p&gt;
&#27491;&#21017;&#27969;&#26159;&#21542;&#26159;&#35299;&#38145;&#25351;&#25968;&#26426;&#21046;&#30340;&#20851;&#38190;&#65311;&#32463;&#36807;&#20934;&#30830;&#24615;&#21644;&#38544;&#31169;&#21452;&#37325;&#32422;&#26463;&#30340;&#24046;&#20998;&#38544;&#31169;&#26426;&#22120;&#23398;&#20064;&#30340;&#19968;&#26465;&#36335;&#24452;
&lt;/p&gt;
&lt;p&gt;
Are Normalizing Flows the Key to Unlocking the Exponential Mechanism? A Path through the Accuracy-Privacy Ceiling Constraining Differentially Private ML
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2311.09200
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#27491;&#21017;&#27969;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#24046;&#20998;&#38544;&#31169;&#26426;&#22120;&#23398;&#20064;&#20013;&#21382;&#21490;&#38590;&#39064;&#65292;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#24046;&#20998;&#38544;&#31169;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#26368;&#20808;&#36827;&#19988;&#20107;&#23454;&#26631;&#20934;&#26159;&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;DPSGD&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#26412;&#36136;&#19978;&#26159;&#28010;&#36153;&#30340;&#12290;&#36890;&#36807;&#21521;&#27599;&#20010;&#26799;&#24230;&#28155;&#21152;&#22122;&#22768;&#65292;&#23427;&#20250;&#22312;&#27599;&#20010;&#26799;&#24230;&#27493;&#39588;&#20013;&#38477;&#20302;&#25972;&#20307;&#38544;&#31169;&#12290;&#23613;&#31649;&#32463;&#36807;15&#24180;&#30340;&#20016;&#23500;&#30740;&#31350;&#65292;&#25512;&#36827;&#20102;&#32452;&#21512;&#23450;&#29702;&#12289;&#23376;&#37319;&#26679;&#26041;&#27861;&#21644;&#23454;&#29616;&#25216;&#26415;&#65292;&#20294;&#24403;&#21069;&#30340;&#38544;&#31169;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24448;&#24448;&#26080;&#27861;&#36798;&#21040;&#36275;&#22815;&#30340;&#20934;&#30830;&#24615;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#20026;&#20102;&#31169;&#19979;&#20248;&#21270;&#32780;&#35774;&#35745;&#30340;&#25351;&#25968;&#26426;&#21046;&#65288;ExpM&#65289;&#21382;&#26469;&#34987;&#25490;&#38500;&#22312;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#31169;&#19979;&#35757;&#32451;&#20043;&#22806;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;ExpM&#38656;&#35201;&#20174;&#19968;&#31181;&#21382;&#26469;&#38590;&#20197;&#22788;&#29702;&#30340;&#23494;&#24230;&#20013;&#36827;&#34892;&#37319;&#26679;&#12290;&#23613;&#31649;&#26368;&#36817;&#21457;&#29616;&#20102;&#27491;&#21017;&#27969;&#27169;&#22411;&#65288;NFs&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#36924;&#36817;&#38590;&#20197;&#22788;&#29702;&#20998;&#24067;&#30340;&#34920;&#36798;&#28145;&#24230;&#32593;&#32476;&#65292;&#20294;ExpM&#20173;&#28982;&#22788;&#20110;&#32972;&#26223;&#20013;&#12290;&#25105;&#20204;&#30340;&#35266;&#28857;&#26159;&#21033;&#29992;&#27491;&#21017;&#27969;&#26469;&#32469;&#36807;ExpM&#30340;&#21382;&#21490;&#38556;&#30861;&#26159;&#19968;&#20010;&#28508;&#22312;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The state of the art and de facto standard for differentially private machine learning (ML) is differentially private stochastic gradient descent (DPSGD). Yet, the method is inherently wasteful. By adding noise to every gradient, it diminishes the overall privacy with every gradient step. Despite 15 years of fruitful research advancing the composition theorems, sub-sampling methods, and implementation techniques, adequate accuracy and privacy is often unattainable with current private ML methods. Meanwhile, the Exponential Mechanism (ExpM), designed for private optimization, has been historically sidelined from privately training modern ML algorithms primarily because ExpM requires sampling from a historically intractable density. Despite the recent discovery of Normalizing Flow models (NFs), expressive deep networks for approximating intractable distributions, ExpM remains in the background. Our position is that leveraging NFs to circumvent historic obstructions of ExpM is a potential
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21644;&#22270;&#31639;&#27861;&#21152;&#36895;&#31185;&#23398;&#21457;&#29616;&#65292;&#25581;&#31034;&#35770;&#25991;&#20043;&#38388;&#30340;&#28145;&#20837;&#36328;&#23398;&#31185;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#26448;&#26009;&#35774;&#35745;&#12290;</title><link>https://arxiv.org/abs/2403.11996</link><description>&lt;p&gt;
&#21033;&#29992;&#29983;&#25104;&#24335;&#30693;&#35782;&#25552;&#21462;&#12289;&#22522;&#20110;&#22270;&#30340;&#34920;&#31034;&#21644;&#22810;&#27169;&#24577;&#26234;&#33021;&#22270;&#25512;&#29702;&#21152;&#36895;&#31185;&#23398;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Accelerating Scientific Discovery with Generative Knowledge Extraction, Graph-Based Representation, and Multimodal Intelligent Graph Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11996
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21644;&#22270;&#31639;&#27861;&#21152;&#36895;&#31185;&#23398;&#21457;&#29616;&#65292;&#25581;&#31034;&#35770;&#25991;&#20043;&#38388;&#30340;&#28145;&#20837;&#36328;&#23398;&#31185;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#26448;&#26009;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65292;&#25105;&#20204;&#23558;&#19968;&#32452;&#28041;&#21450;&#29983;&#29289;&#26448;&#26009;&#39046;&#22495;&#30340;1,000&#31687;&#31185;&#23398;&#35770;&#25991;&#36716;&#21270;&#20026;&#35814;&#32454;&#30340;&#26412;&#20307;&#30693;&#35782;&#22270;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22266;&#26377;&#30340;&#26080;&#26631;&#24230;&#29305;&#24615;&#12290;&#36890;&#36807;&#22522;&#20110;&#33410;&#28857;&#30456;&#20284;&#24615;&#21644;&#20171;&#25968;&#20013;&#24515;&#24615;&#30340;&#32452;&#21512;&#25490;&#21517;&#65292;&#25506;&#27979;&#19981;&#21516;&#27010;&#24565;&#20043;&#38388;&#30340;&#22270;&#36941;&#21382;&#36335;&#24452;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#28145;&#20837;&#30340;&#36328;&#23398;&#31185;&#20851;&#31995;&#65292;&#21487;&#29992;&#20110;&#22238;&#31572;&#26597;&#35810;&#65292;&#35782;&#21035;&#30693;&#35782;&#20013;&#30340;&#31354;&#30333;&#65292;&#24182;&#25552;&#20986;&#21069;&#25152;&#26410;&#35265;&#30340;&#26448;&#26009;&#35774;&#35745;&#21450;&#20854;&#34892;&#20026;&#12290;&#19968;&#39033;&#27604;&#36739;&#25581;&#31034;&#20102;&#29983;&#29289;&#26448;&#26009;&#21644;&#36125;&#22810;&#33452;&#31532;&#20061;&#20132;&#21709;&#26354;&#20043;&#38388;&#30340;&#35814;&#32454;&#32467;&#26500;&#30456;&#20284;&#20043;&#22788;&#65292;&#31361;&#26174;&#20102;&#36890;&#36807;&#21516;&#26500;&#26144;&#23556;&#20849;&#20139;&#22797;&#26434;&#24615;&#27169;&#24335;&#12290;&#35813;&#31639;&#27861;&#36827;&#19968;&#27493;&#21019;&#24314;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#20998;&#32423;&#33740;&#19997;&#20307;&#30340;&#22797;&#21512;&#26448;&#26009;&#65292;&#23558;&#22270;&#37319;&#26679;&#30340;&#32852;&#21512;&#21512;&#25104;&#21407;&#29702;&#19982;&#24247;&#23450;&#26031;&#22522;&#12298;&#31532;&#19971;&#32452;&#25104;&#12299;&#20013;&#25552;&#21462;&#30340;&#21407;&#21017;&#30456;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11996v1 Announce Type: cross  Abstract: Using generative Artificial Intelligence (AI), we transformed a set of 1,000 scientific papers in the area of biological materials into detailed ontological knowledge graphs, revealing their inherently scale-free nature. Using graph traversal path detection between dissimilar concepts based on combinatorial ranking of node similarity and betweenness centrality, we reveal deep insights into unprecedented interdisciplinary relationships that can be used to answer queries, identify gaps in knowledge, and propose never-before-seen material designs and their behaviors. One comparison revealed detailed structural parallels between biological materials and Beethoven's 9th Symphony, highlighting shared patterns of complexity through isomorphic mapping. The algorithm further created an innovative hierarchical mycelium-based composite that incorporates joint synthesis of graph sampling with principles extracted from Kandinsky's Composition VII p
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;GNN&#30340;&#21453;&#21521;&#20256;&#25773;&#36807;&#31243;&#65292;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#24322;&#36136;&#22270;&#20013;&#33410;&#28857;&#34920;&#31034;&#30340;&#21306;&#20998;&#24230;&#65292;&#24182;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.10543</link><description>&lt;p&gt;
&#36890;&#36807;GNN&#30340;&#21453;&#21521;&#36807;&#31243;&#21306;&#20998;&#24322;&#36136;&#22270;&#20013;&#30340;&#37051;&#23621;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Distinguishing Neighborhood Representations Through Reverse Process of GNNs for Heterophilic Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10543
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;GNN&#30340;&#21453;&#21521;&#20256;&#25773;&#36807;&#31243;&#65292;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#24322;&#36136;&#22270;&#20013;&#33410;&#28857;&#34920;&#31034;&#30340;&#21306;&#20998;&#24230;&#65292;&#24182;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Graph Neural Network&#65288;GNN&#65289;&#31867;&#20284;&#20110;&#25193;&#25955;&#36807;&#31243;&#65292;&#22312;&#22534;&#21472;&#35768;&#22810;&#23618;&#26102;&#23548;&#33268;&#23398;&#20064;&#34920;&#31034;&#36807;&#24230;&#24179;&#28369;&#12290;&#22240;&#27492;&#65292;&#28040;&#24687;&#20256;&#36882;&#30340;&#21453;&#21521;&#36807;&#31243;&#21487;&#20197;&#36890;&#36807;&#21453;&#36716;&#27491;&#21521;&#28040;&#24687;&#20256;&#25773;&#26469;&#38160;&#21270;&#33410;&#28857;&#34920;&#31034;&#12290;&#38160;&#21270;&#21518;&#30340;&#34920;&#31034;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#26356;&#22909;&#22320;&#21306;&#20998;&#20855;&#26377;&#19981;&#21516;&#26631;&#31614;&#30340;&#37051;&#23621;&#33410;&#28857;&#65292;&#20363;&#22914;&#22312;&#24322;&#36136;&#22270;&#20013;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#21453;&#21521;&#36807;&#31243;&#30340;&#35774;&#35745;&#21407;&#21017;&#24212;&#29992;&#20110;GNN&#30340;&#19977;&#20010;&#21464;&#20307;&#12290;&#36890;&#36807;&#22312;&#24322;&#36136;&#22270;&#25968;&#25454;&#19978;&#30340;&#23454;&#39564;&#65292;&#20854;&#20013;&#30456;&#37051;&#33410;&#28857;&#38656;&#35201;&#20855;&#26377;&#19981;&#21516;&#34920;&#31034;&#25165;&#33021;&#25104;&#21151;&#20998;&#31867;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21453;&#21521;&#36807;&#31243;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#26174;&#30528;&#25552;&#39640;&#20102;&#39044;&#27979;&#24615;&#33021;&#12290;&#36827;&#19968;&#27493;&#20998;&#26512;&#34920;&#26126;&#65292;&#21453;&#21521;&#26426;&#21046;&#21487;&#20197;&#20943;&#36731;&#25968;&#30334;&#23618;&#19978;&#30340;&#36807;&#24230;&#24179;&#28369;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10543v1 Announce Type: cross  Abstract: Graph Neural Network (GNN) resembles the diffusion process, leading to the over-smoothing of learned representations when stacking many layers. Hence, the reverse process of message passing can sharpen the node representations by inverting the forward message propagation. The sharpened representations can help us to better distinguish neighboring nodes with different labels, such as in heterophilic graphs. In this work, we apply the design principle of the reverse process to the three variants of the GNNs. Through the experiments on heterophilic graph data, where adjacent nodes need to have different representations for successful classification, we show that the reverse process significantly improves the prediction performance in many cases. Additional analysis reveals that the reverse mechanism can mitigate the over-smoothing over hundreds of layers.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20026;R&#233;nyi-DP&#25512;&#23548;&#36890;&#36807;&#23376;&#25277;&#26679;&#30340;&#25918;&#22823;&#20445;&#35777;&#65292;&#36825;&#26159;&#39318;&#20010;&#38024;&#23545;&#38544;&#31169;&#26680;&#31639;&#26041;&#27861;&#30340;&#26694;&#26550;&#65292;&#20063;&#20855;&#26377;&#29420;&#31435;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.04867</link><description>&lt;p&gt;
&#32452;&#38544;&#31169;&#25918;&#22823;&#21644;&#23376;&#25277;&#26679;&#30340;R&#233;nyi&#24046;&#20998;&#38544;&#31169;&#32479;&#19968;&#25918;&#22823;
&lt;/p&gt;
&lt;p&gt;
Group Privacy Amplification and Unified Amplification by Subsampling for R\'enyi Differential Privacy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04867
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20026;R&#233;nyi-DP&#25512;&#23548;&#36890;&#36807;&#23376;&#25277;&#26679;&#30340;&#25918;&#22823;&#20445;&#35777;&#65292;&#36825;&#26159;&#39318;&#20010;&#38024;&#23545;&#38544;&#31169;&#26680;&#31639;&#26041;&#27861;&#30340;&#26694;&#26550;&#65292;&#20063;&#20855;&#26377;&#29420;&#31435;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;(DP)&#20855;&#26377;&#22810;&#31181;&#29702;&#24819;&#23646;&#24615;&#65292;&#22914;&#23545;&#21518;&#22788;&#29702;&#30340;&#40065;&#26834;&#24615;&#12289;&#32452;&#38544;&#31169;&#21644;&#36890;&#36807;&#23376;&#25277;&#26679;&#25918;&#22823;&#65292;&#36825;&#20123;&#23646;&#24615;&#21487;&#20197;&#30456;&#20114;&#29420;&#31435;&#25512;&#23548;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#30830;&#23450;&#26159;&#21542;&#36890;&#36807;&#32852;&#21512;&#32771;&#34385;&#36825;&#20123;&#23646;&#24615;&#20013;&#30340;&#22810;&#20010;&#21487;&#20197;&#33719;&#24471;&#26356;&#24378;&#30340;&#38544;&#31169;&#20445;&#35777;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#32452;&#38544;&#31169;&#21644;&#36890;&#36807;&#23376;&#25277;&#26679;&#25918;&#22823;&#30340;&#32452;&#21512;&#12290;&#20026;&#20102;&#25552;&#20379;&#36866;&#21512;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#20445;&#35777;&#65292;&#25105;&#20204;&#22312;R&#233;nyi-DP&#26694;&#26550;&#20013;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#36825;&#27604;$(\epsilon,\delta)$-DP&#20855;&#26377;&#26356;&#26377;&#21033;&#30340;&#32452;&#21512;&#23646;&#24615;&#12290;&#20316;&#20026;&#36825;&#20010;&#20998;&#26512;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20026;R&#233;nyi-DP&#25512;&#23548;&#36890;&#36807;&#23376;&#25277;&#26679;&#30340;&#25918;&#22823;&#20445;&#35777;&#65292;&#36825;&#26159;&#39318;&#20010;&#38024;&#23545;&#38544;&#31169;&#26680;&#31639;&#26041;&#27861;&#30340;&#26694;&#26550;&#65292;&#20063;&#20855;&#26377;&#29420;&#31435;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23427;&#19981;&#20165;&#35753;&#25105;&#20204;&#25913;&#36827;&#21644;&#27867;&#21270;&#29616;&#26377;&#30340;&#25918;&#22823;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04867v1 Announce Type: cross  Abstract: Differential privacy (DP) has various desirable properties, such as robustness to post-processing, group privacy, and amplification by subsampling, which can be derived independently of each other. Our goal is to determine whether stronger privacy guarantees can be obtained by considering multiple of these properties jointly. To this end, we focus on the combination of group privacy and amplification by subsampling. To provide guarantees that are amenable to machine learning algorithms, we conduct our analysis in the framework of R\'enyi-DP, which has more favorable composition properties than $(\epsilon,\delta)$-DP. As part of this analysis, we develop a unified framework for deriving amplification by subsampling guarantees for R\'enyi-DP, which represents the first such framework for a privacy accounting method and is of independent interest. We find that it not only lets us improve upon and generalize existing amplification results 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#26368;&#36817;&#20986;&#29616;&#30340;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#19987;&#19994;&#21644;&#36890;&#29992;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#26088;&#22312;&#20840;&#38754;&#20102;&#35299;&#36825;&#20123;&#21019;&#26032;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.04306</link><description>&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Effectiveness Assessment of Recent Large Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#26368;&#36817;&#20986;&#29616;&#30340;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#19987;&#19994;&#21644;&#36890;&#29992;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#26088;&#22312;&#20840;&#38754;&#20102;&#35299;&#36825;&#20123;&#21019;&#26032;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;(LVLMs)&#30340;&#20986;&#29616;&#20195;&#34920;&#30528;&#36808;&#21521;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#37325;&#35201;&#36827;&#27493;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#19987;&#19994;&#21644;&#36890;&#29992;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#31243;&#24230;&#38656;&#35201;&#36827;&#19968;&#27493;&#35843;&#26597;&#12290;&#26412;&#25991;&#26088;&#22312;&#35780;&#20272;&#27969;&#34892;&#30340;LVLMs&#22312;&#19987;&#19994;&#21644;&#36890;&#29992;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#65292;&#26088;&#22312;&#25552;&#20379;&#23545;&#36825;&#20123;&#21019;&#26032;&#26041;&#27861;&#30340;&#20840;&#38754;&#29702;&#35299;&#12290;&#20026;&#20102;&#35780;&#20272;&#23427;&#20204;&#22312;&#19987;&#19994;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#37327;&#36523;&#23450;&#21046;&#20102;&#19968;&#20010;&#21253;&#21547;&#33258;&#28982;&#12289;&#21307;&#30103;&#21644;&#24037;&#19994;&#19977;&#31181;&#19981;&#21516;&#22330;&#26223;&#30340;&#20840;&#38754;&#27979;&#35797;&#24179;&#21488;&#65292;&#28085;&#30422;&#20845;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#36825;&#20123;&#20219;&#21153;&#21253;&#25324;&#26174;&#33879;&#12289;&#20266;&#35013;&#21644;&#36879;&#26126;&#29289;&#20307;&#26816;&#27979;&#65292;&#20197;&#21450;&#24687;&#32905;&#21644;&#30382;&#32932;&#30149;&#21464;&#26816;&#27979;&#65292;&#20197;&#21450;&#24037;&#19994;&#24322;&#24120;&#26816;&#27979;&#12290;&#25105;&#20204;&#26816;&#39564;&#20102;&#26368;&#36817;&#19977;&#31181;&#24320;&#28304;LVLMs--MiniGPT-v2&#12289;LLaVA-1.5&#21644;Shikra--&#22312;&#35270;&#35273;&#35782;&#21035;&#21644;&#23450;&#20301;&#39046;&#22495;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04306v1 Announce Type: cross  Abstract: The advent of large vision-language models (LVLMs) represents a noteworthy advancement towards the pursuit of artificial general intelligence. However, the extent of their efficacy across both specialized and general tasks warrants further investigation. This article endeavors to evaluate the competency of popular LVLMs in specialized and general tasks, respectively, aiming to offer a comprehensive comprehension of these innovative methodologies. To gauge their efficacy in specialized tasks, we tailor a comprehensive testbed comprising three distinct scenarios: natural, healthcare, and industrial, encompassing six challenging tasks. These tasks include salient, camouflaged, and transparent object detection, as well as polyp and skin lesion detection, alongside industrial anomaly detection. We examine the performance of three recent open-source LVLMs -- MiniGPT-v2, LLaVA-1.5, and Shikra -- in the realm of visual recognition and localiza
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#23545;&#20110;$k$-PCA&#31639;&#27861;&#20013;&#36817;&#20284;&#21442;&#25968;&#36864;&#21270;&#30340;&#36793;&#30028;&#24471;&#21040;&#20102;&#26174;&#33879;&#26356;&#20026;&#31934;&#30830;&#30340;&#30028;&#38480;</title><link>https://arxiv.org/abs/2403.03905</link><description>&lt;p&gt;
&#40657;&#30418;$k$-to-$1$-PCA&#38477;&#32500;&#65306;&#29702;&#35770;&#19982;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Black-Box $k$-to-$1$-PCA Reductions: Theory and Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03905
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#23545;&#20110;$k$-PCA&#31639;&#27861;&#20013;&#36817;&#20284;&#21442;&#25968;&#36864;&#21270;&#30340;&#36793;&#30028;&#24471;&#21040;&#20102;&#26174;&#33879;&#26356;&#20026;&#31934;&#30830;&#30340;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
$k$-&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;$k$-PCA&#65289;&#38382;&#39064;&#26159;&#19968;&#31181;&#22522;&#26412;&#30340;&#31639;&#27861;&#21407;&#35821;&#65292;&#22312;&#25968;&#25454;&#20998;&#26512;&#21644;&#38477;&#32500;&#24212;&#29992;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#22312;&#32479;&#35745;&#29615;&#22659;&#20013;&#65292;$k$-PCA&#30340;&#30446;&#26631;&#26159;&#35782;&#21035;&#19968;&#20010;&#20998;&#24067;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#39030;&#37096;&#29305;&#24449;&#31354;&#38388;&#65292;&#25105;&#20204;&#21482;&#33021;&#36890;&#36807;&#26679;&#26412;&#38544;&#24335;&#35775;&#38382;&#36825;&#20010;&#30697;&#38453;&#12290;&#21463;&#36825;&#20123;&#38544;&#24335;&#35774;&#32622;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20998;&#26512;&#40657;&#30418;&#32553;&#20943;&#26041;&#27861;&#20316;&#20026;&#35774;&#35745;$k$-PCA&#31639;&#27861;&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#25105;&#20204;&#36890;&#36807;&#40657;&#30418;$1$-PCA&#39044;&#35328;&#27169;&#25311;&#23545;&#26410;&#30693;&#30446;&#26631;&#30697;&#38453;&#30340;&#35775;&#38382;&#65292;&#35813;&#39044;&#35328;&#36820;&#22238;&#19968;&#20010;&#36817;&#20284;&#30340;&#39030;&#37096;&#29305;&#24449;&#21521;&#37327;&#65292;&#26681;&#25454;&#20004;&#20010;&#27969;&#34892;&#30340;&#36817;&#20284;&#27010;&#24565;&#12290;&#23613;&#31649;&#36825;&#31181;&#40657;&#30418;&#26041;&#27861;&#21487;&#33021;&#26159;&#35774;&#35745;$k$-PCA&#31639;&#27861;&#20013;&#26368;&#33258;&#28982;&#30340;&#22522;&#20110;&#38477;&#32500;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#36882;&#24402;&#35843;&#29992;$1$-PCA&#39044;&#35328;&#35843;&#29992;&#20102;$k$&#27425;&#65292;&#20197;&#21069;&#24456;&#38590;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03905v1 Announce Type: cross  Abstract: The $k$-principal component analysis ($k$-PCA) problem is a fundamental algorithmic primitive that is widely-used in data analysis and dimensionality reduction applications. In statistical settings, the goal of $k$-PCA is to identify a top eigenspace of the covariance matrix of a distribution, which we only have implicit access to via samples. Motivated by these implicit settings, we analyze black-box deflation methods as a framework for designing $k$-PCA algorithms, where we model access to the unknown target matrix via a black-box $1$-PCA oracle which returns an approximate top eigenvector, under two popular notions of approximation. Despite being arguably the most natural reduction-based approach to $k$-PCA algorithm design, such black-box methods, which recursively call a $1$-PCA oracle $k$ times, were previously poorly-understood.   Our main contribution is significantly sharper bounds on the approximation parameter degradation of
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#22270;&#20687;&#25551;&#36848;&#20855;&#20307;&#24615;&#65292;&#29992;&#20110;&#35780;&#20272;&#26631;&#39064;&#25991;&#26412;&#30340;&#20855;&#20307;&#24615;&#21644;&#30456;&#20851;&#24615;&#65292;&#20197;&#24110;&#21161;&#22312;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#38548;&#31163;&#25552;&#20379;&#26368;&#24378;&#20449;&#21495;&#30340;&#26368;&#20855;&#20307;&#26679;&#26412;&#12290;</title><link>https://arxiv.org/abs/2403.01306</link><description>&lt;p&gt;
ICC&#65306;&#29992;&#20110;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#31579;&#36873;&#30340;&#22270;&#20687;&#25551;&#36848;&#20855;&#20307;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
ICC: Quantifying Image Caption Concreteness for Multimodal Dataset Curation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01306
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#22270;&#20687;&#25551;&#36848;&#20855;&#20307;&#24615;&#65292;&#29992;&#20110;&#35780;&#20272;&#26631;&#39064;&#25991;&#26412;&#30340;&#20855;&#20307;&#24615;&#21644;&#30456;&#20851;&#24615;&#65292;&#20197;&#24110;&#21161;&#22312;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#38548;&#31163;&#25552;&#20379;&#26368;&#24378;&#20449;&#21495;&#30340;&#26368;&#20855;&#20307;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01306v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#38024;&#23545;&#37197;&#23545;&#25991;&#26412;-&#22270;&#20687;&#25968;&#25454;&#30340;Web&#35268;&#27169;&#35757;&#32451;&#22312;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#20294;&#25361;&#25112;&#22312;&#37326;&#22806;&#25968;&#25454;&#38598;&#30340;&#39640;&#22122;&#22768;&#29305;&#24615;&#12290;&#26631;&#20934;&#25968;&#25454;&#36807;&#28388;&#26041;&#27861;&#25104;&#21151;&#21435;&#38500;&#20102;&#19981;&#21305;&#37197;&#30340;&#25991;&#26412;-&#22270;&#20687;&#23545;&#65292;&#20294;&#20801;&#35768;&#35821;&#20041;&#30456;&#20851;&#20294;&#38750;&#24120;&#25277;&#35937;&#25110;&#20027;&#35266;&#30340;&#25991;&#26412;&#12290;&#36825;&#20123;&#26041;&#27861;&#32570;&#20047;&#32454;&#31890;&#24230;&#30340;&#33021;&#21147;&#26469;&#38548;&#31163;&#25552;&#20379;&#22312;&#22024;&#26434;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#26368;&#24378;&#20449;&#21495;&#30340;&#26368;&#20855;&#20307;&#26679;&#26412;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#22270;&#20687;&#25551;&#36848;&#20855;&#20307;&#24615;&#65292;&#35780;&#20272;&#27809;&#26377;&#22270;&#20687;&#21442;&#32771;&#30340;&#26631;&#39064;&#25991;&#26412;&#20197;&#34913;&#37327;&#20854;&#20855;&#20307;&#24615;&#21644;&#30456;&#20851;&#24615;&#65292;&#20197;&#20379;&#22312;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#20351;&#29992;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#34913;&#37327;&#35270;&#35273;-&#35821;&#20041;&#20449;&#24687;&#25439;&#22833;&#30340;&#24378;&#22522;&#30784;&#27169;&#22411;&#26469;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#19982;&#20154;&#31867;&#23545;&#21333;&#35789;&#21644;&#21477;&#23376;&#32423;&#25991;&#26412;&#20855;&#20307;&#24615;&#30340;&#35780;&#20272;&#39640;&#24230;&#30456;&#20851;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01306v1 Announce Type: new  Abstract: Web-scale training on paired text-image data is becoming increasingly central to multimodal learning, but is challenged by the highly noisy nature of datasets in the wild. Standard data filtering approaches succeed in removing mismatched text-image pairs, but permit semantically related but highly abstract or subjective text. These approaches lack the fine-grained ability to isolate the most concrete samples that provide the strongest signal for learning in a noisy dataset. In this work, we propose a new metric, image caption concreteness, that evaluates caption text without an image reference to measure its concreteness and relevancy for use in multimodal learning. Our approach leverages strong foundation models for measuring visual-semantic information loss in multimodal representations. We demonstrate that this strongly correlates with human evaluation of concreteness in both single-word and sentence-level texts. Moreover, we show tha
&lt;/p&gt;</description></item><item><title>Log-NCDEs&#26159;&#19968;&#31181;&#26032;&#39062;&#32780;&#26377;&#25928;&#30340;&#35757;&#32451;NCDEs&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;Log-ODE&#26041;&#27861;&#20174;&#31895;&#31961;&#36335;&#24452;&#30740;&#31350;&#20013;&#36817;&#20284;CDE&#30340;&#35299;&#65292;&#24182;&#22312;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#22522;&#20934;&#19978;&#34920;&#29616;&#20986;&#27604;&#20854;&#20182;&#27169;&#22411;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.18512</link><description>&lt;p&gt;
Log&#31070;&#32463;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#65306;&#26446;&#25324;&#21495;&#30340;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Log Neural Controlled Differential Equations: The Lie Brackets Make a Difference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18512
&lt;/p&gt;
&lt;p&gt;
Log-NCDEs&#26159;&#19968;&#31181;&#26032;&#39062;&#32780;&#26377;&#25928;&#30340;&#35757;&#32451;NCDEs&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;Log-ODE&#26041;&#27861;&#20174;&#31895;&#31961;&#36335;&#24452;&#30740;&#31350;&#20013;&#36817;&#20284;CDE&#30340;&#35299;&#65292;&#24182;&#22312;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#22522;&#20934;&#19978;&#34920;&#29616;&#20986;&#27604;&#20854;&#20182;&#27169;&#22411;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#25511;&#24494;&#20998;&#26041;&#31243;&#65288;CDE&#65289;&#30340;&#30690;&#37327;&#22330;&#25551;&#36848;&#20102;&#25511;&#21046;&#36335;&#24452;&#19982;&#35299;&#36335;&#24452;&#28436;&#21270;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#31070;&#32463;CDE&#65288;NCDE&#65289;&#23558;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#35270;&#20026;&#23545;&#25511;&#21046;&#36335;&#24452;&#30340;&#35266;&#27979;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23545;CDE&#30340;&#30690;&#37327;&#22330;&#36827;&#34892;&#21442;&#25968;&#21270;&#65292;&#24182;&#23558;&#35299;&#36335;&#24452;&#20316;&#20026;&#25345;&#32493;&#28436;&#21270;&#30340;&#38544;&#34255;&#29366;&#24577;&#12290;&#30001;&#20110;&#20854;&#26500;&#36896;&#20351;&#20854;&#33021;&#22815;&#25269;&#25239;&#19981;&#35268;&#21017;&#37319;&#26679;&#29575;&#65292;NCDE&#26159;&#24314;&#27169;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#24378;&#22823;&#26041;&#27861;&#12290;&#22312;&#31070;&#32463;&#31895;&#31961;&#24494;&#20998;&#26041;&#31243;&#65288;NRDE&#65289;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Log-NCDE&#65292;&#36825;&#26159;&#19968;&#31181;&#35757;&#32451;NCDE&#30340;&#26032;&#39062;&#19988;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;Log-NCDE&#30340;&#26680;&#24515;&#32452;&#20214;&#26159;Log-ODE&#26041;&#27861;&#65292;&#36825;&#26159;&#20174;&#31895;&#31961;&#36335;&#24452;&#30740;&#31350;&#20013;&#30340;&#19968;&#31181;&#29992;&#20110;&#36817;&#20284;CDE&#35299;&#30340;&#24037;&#20855;&#12290;&#22312;&#19968;&#31995;&#21015;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#22522;&#20934;&#19978;&#65292;&#23637;&#31034;&#20102;Log-NCDE&#27604;NCDE&#65292;NRDE&#21644;&#20004;&#31181;&#26368;&#20808;&#36827;&#27169;&#22411;S5&#21644;&#32447;&#24615;&#36882;&#24402;&#27169;&#22411;&#20855;&#26377;&#26356;&#39640;&#30340;&#24179;&#22343;&#27979;&#35797;&#38598;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18512v1 Announce Type: new  Abstract: The vector field of a controlled differential equation (CDE) describes the relationship between a control path and the evolution of a solution path. Neural CDEs (NCDEs) treat time series data as observations from a control path, parameterise a CDE's vector field using a neural network, and use the solution path as a continuously evolving hidden state. As their formulation makes them robust to irregular sampling rates, NCDEs are a powerful approach for modelling real-world data. Building on neural rough differential equations (NRDEs), we introduce Log-NCDEs, a novel and effective method for training NCDEs. The core component of Log-NCDEs is the Log-ODE method, a tool from the study of rough paths for approximating a CDE's solution. On a range of multivariate time series classification benchmarks, Log-NCDEs are shown to achieve a higher average test set accuracy than NCDEs, NRDEs, and two state-of-the-art models, S5 and the linear recurren
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#38543;&#26426;&#36807;&#31243;&#20013;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#31614;&#21517;&#26680;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#27979;&#35797;&#65292;&#23454;&#29616;&#20102;&#23545;&#22240;&#26524;&#20851;&#31995;&#30340;&#25512;&#26029;&#65292;&#20197;&#21450;&#24320;&#21457;&#20102;&#32422;&#26463;&#26465;&#20214;&#30340;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#29992;&#20110;&#24674;&#22797;&#25972;&#20010;&#26377;&#21521;&#22270;&#12290;</title><link>https://arxiv.org/abs/2402.18477</link><description>&lt;p&gt;
&#22312;&#22240;&#26524;&#21457;&#29616;&#20013;&#30340;&#31614;&#21517;&#26680;&#26465;&#20214;&#29420;&#31435;&#24615;&#27979;&#35797;&#29992;&#20110;&#38543;&#26426;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Signature Kernel Conditional Independence Tests in Causal Discovery for Stochastic Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#38543;&#26426;&#36807;&#31243;&#20013;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#31614;&#21517;&#26680;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#27979;&#35797;&#65292;&#23454;&#29616;&#20102;&#23545;&#22240;&#26524;&#20851;&#31995;&#30340;&#25512;&#26029;&#65292;&#20197;&#21450;&#24320;&#21457;&#20102;&#32422;&#26463;&#26465;&#20214;&#30340;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#29992;&#20110;&#24674;&#22797;&#25972;&#20010;&#26377;&#21521;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#25512;&#26029;&#38543;&#26426;&#21160;&#21147;&#31995;&#32479;&#32972;&#21518;&#30340;&#22240;&#26524;&#32467;&#26500;&#22312;&#31185;&#23398;&#12289;&#20581;&#24247;&#21644;&#37329;&#34701;&#31561;&#39046;&#22495;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#26368;&#36817;&#31614;&#21517;&#26680;&#25216;&#26415;&#30340;&#36827;&#23637;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#20869;&#26680;&#30340;&#8220;&#36335;&#24452;&#31354;&#38388;&#8221;&#19978;&#26465;&#20214;&#29420;&#31435;&#24615;&#65288;CI&#65289;&#27979;&#35797;&#65292;&#29992;&#20110;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#30456;&#36739;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#22312;&#36335;&#24452;&#31354;&#38388;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;CI&#27979;&#35797;&#34920;&#29616;&#20986;&#20005;&#26684;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20026;&#38750;&#24490;&#29615;&#38543;&#26426;&#21160;&#21147;&#31995;&#32479;&#24320;&#21457;&#20102;&#22522;&#20110;&#32422;&#26463;&#30340;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#65292;&#21033;&#29992;&#26102;&#38388;&#20449;&#24687;&#26469;&#24674;&#22797;&#25972;&#20010;&#26377;&#21521;&#22270;&#12290;&#22312;&#20551;&#35774;&#24544;&#23454;&#24615;&#21644;CI&#39044;&#35328;&#26426;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#26159;&#23436;&#22791;&#19988;&#27491;&#30830;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18477v1 Announce Type: cross  Abstract: Inferring the causal structure underlying stochastic dynamical systems from observational data holds great promise in domains ranging from science and health to finance. Such processes can often be accurately modeled via stochastic differential equations (SDEs), which naturally imply causal relationships via "which variables enter the differential of which other variables". In this paper, we develop a kernel-based test of conditional independence (CI) on "path-space" -- solutions to SDEs -- by leveraging recent advances in signature kernels. We demonstrate strictly superior performance of our proposed CI test compared to existing approaches on path-space. Then, we develop constraint-based causal discovery algorithms for acyclic stochastic dynamical systems (allowing for loops) that leverage temporal information to recover the entire directed graph. Assuming faithfulness and a CI oracle, our algorithm is sound and complete. We empirical
&lt;/p&gt;</description></item><item><title>reBandit&#26159;&#19968;&#31181;&#22312;&#32447;RL&#31639;&#27861;&#65292;&#21033;&#29992;&#38543;&#26426;&#25928;&#24212;&#21644;&#36125;&#21494;&#26031;&#20808;&#39564;&#24555;&#36895;&#39640;&#25928;&#22320;&#23398;&#20064;&#65292;&#22312;&#31227;&#21160;&#20581;&#24247;&#29615;&#22659;&#20013;&#36890;&#36807;&#20010;&#24615;&#21270;&#24178;&#39044;&#26469;&#20943;&#23569;&#26032;&#20852;&#25104;&#24180;&#20154;&#30340;&#22823;&#40635;&#20351;&#29992;</title><link>https://arxiv.org/abs/2402.17739</link><description>&lt;p&gt;
reBandit&#65306;&#22522;&#20110;&#38543;&#26426;&#25928;&#24212;&#30340;&#22312;&#32447;RL&#31639;&#27861;&#29992;&#20110;&#20943;&#23569;&#22823;&#40635;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
reBandit: Random Effects based Online RL algorithm for Reducing Cannabis Use
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17739
&lt;/p&gt;
&lt;p&gt;
reBandit&#26159;&#19968;&#31181;&#22312;&#32447;RL&#31639;&#27861;&#65292;&#21033;&#29992;&#38543;&#26426;&#25928;&#24212;&#21644;&#36125;&#21494;&#26031;&#20808;&#39564;&#24555;&#36895;&#39640;&#25928;&#22320;&#23398;&#20064;&#65292;&#22312;&#31227;&#21160;&#20581;&#24247;&#29615;&#22659;&#20013;&#36890;&#36807;&#20010;&#24615;&#21270;&#24178;&#39044;&#26469;&#20943;&#23569;&#26032;&#20852;&#25104;&#24180;&#20154;&#30340;&#22823;&#40635;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#40635;&#20351;&#29992;&#21450;&#30456;&#20851;&#30340;&#22823;&#40635;&#20351;&#29992;&#38556;&#30861;&#65288;CUD&#65289;&#30340;&#19981;&#26029;&#22686;&#21152;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#26500;&#25104;&#20102;&#19968;&#20010;&#37325;&#22823;&#30340;&#20844;&#20849;&#21355;&#29983;&#25361;&#25112;&#12290;&#23588;&#20854;&#26159;&#22312;&#26032;&#20852;&#25104;&#24180;&#20154;&#65288;18-25&#23681;&#65289;&#20013;&#65292;&#23384;&#22312;&#26126;&#26174;&#30340;&#27835;&#30103;&#32570;&#21475;&#65292;&#22240;&#27492;&#35299;&#20915;&#22823;&#40635;&#20351;&#29992;&#21644;CUD&#20173;&#28982;&#26159;2030&#24180;&#32852;&#21512;&#22269;&#21487;&#25345;&#32493;&#21457;&#23637;&#30446;&#26631;&#65288;SDG&#65289;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#30446;&#26631;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;reBandit&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#31227;&#21160;&#20581;&#24247;&#30740;&#31350;&#20013;&#65292;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#20010;&#24615;&#21270;&#31227;&#21160;&#20581;&#24247;&#24178;&#39044;&#26469;&#20943;&#23569;&#26032;&#20852;&#25104;&#24180;&#20154;&#30340;&#22823;&#40635;&#20351;&#29992;&#12290;reBandit&#21033;&#29992;&#38543;&#26426;&#25928;&#24212;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#36125;&#21494;&#26031;&#20808;&#39564;&#20197;&#22312;&#22024;&#26434;&#30340;&#31227;&#21160;&#20581;&#24247;&#29615;&#22659;&#20013;&#24555;&#36895;&#32780;&#26377;&#25928;&#22320;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;reBandit&#37319;&#29992;&#32463;&#39564;&#36125;&#21494;&#26031;&#21644;&#20248;&#21270;&#25216;&#26415;&#26469;&#22312;&#32447;&#33258;&#20027;&#26356;&#26032;&#20854;&#36229;&#21442;&#25968;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#21033;&#29992;&#25968;&#25454;&#26500;&#24314;&#20102;&#19968;&#20010;&#27169;&#25311;&#27979;&#35797;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17739v1 Announce Type: new  Abstract: The escalating prevalence of cannabis use, and associated cannabis-use disorder (CUD), poses a significant public health challenge globally. With a notably wide treatment gap, especially among emerging adults (EAs; ages 18-25), addressing cannabis use and CUD remains a pivotal objective within the 2030 United Nations Agenda for Sustainable Development Goals (SDG). In this work, we develop an online reinforcement learning (RL) algorithm called reBandit which will be utilized in a mobile health study to deliver personalized mobile health interventions aimed at reducing cannabis use among EAs. reBandit utilizes random effects and informative Bayesian priors to learn quickly and efficiently in noisy mobile health environments. Moreover, reBandit employs Empirical Bayes and optimization techniques to autonomously update its hyper-parameters online. To evaluate the performance of our algorithm, we construct a simulation testbed using data from
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#25209;&#22788;&#29702;&#32422;&#26463;&#19979;&#30340;&#38750;&#21442;&#25968;&#19978;&#19979;&#25991;&#33218;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BaSEDB&#30340;&#26041;&#26696;&#65292;&#22312;&#21160;&#24577;&#20998;&#21106;&#21327;&#21464;&#37327;&#31354;&#38388;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#26368;&#20248;&#30340;&#21518;&#24724;&#12290;</title><link>https://arxiv.org/abs/2402.17732</link><description>&lt;p&gt;
&#25209;&#22788;&#29702;&#38750;&#21442;&#25968;&#19978;&#19979;&#25991;&#33218;
&lt;/p&gt;
&lt;p&gt;
Batched Nonparametric Contextual Bandits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17732
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#25209;&#22788;&#29702;&#32422;&#26463;&#19979;&#30340;&#38750;&#21442;&#25968;&#19978;&#19979;&#25991;&#33218;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BaSEDB&#30340;&#26041;&#26696;&#65292;&#22312;&#21160;&#24577;&#20998;&#21106;&#21327;&#21464;&#37327;&#31354;&#38388;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#26368;&#20248;&#30340;&#21518;&#24724;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#25209;&#22788;&#29702;&#32422;&#26463;&#19979;&#30340;&#38750;&#21442;&#25968;&#19978;&#19979;&#25991;&#33218;&#38382;&#39064;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#27599;&#20010;&#21160;&#20316;&#30340;&#26399;&#26395;&#22870;&#21169;&#34987;&#24314;&#27169;&#20026;&#21327;&#21464;&#37327;&#30340;&#24179;&#28369;&#20989;&#25968;&#65292;&#24182;&#19988;&#31574;&#30053;&#26356;&#26032;&#26159;&#22312;&#27599;&#20010;Observations&#25209;&#27425;&#32467;&#26463;&#26102;&#36827;&#34892;&#30340;&#12290;&#25105;&#20204;&#20026;&#36825;&#31181;&#35774;&#32622;&#24314;&#31435;&#20102;&#19968;&#20010;&#26368;&#23567;&#21270;&#21518;&#24724;&#30340;&#19979;&#38480;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Batched Successive Elimination with Dynamic Binning&#65288;BaSEDB&#65289;&#30340;&#26041;&#26696;&#65292;&#21487;&#20197;&#23454;&#29616;&#26368;&#20248;&#30340;&#21518;&#24724;&#65288;&#36798;&#21040;&#23545;&#25968;&#22240;&#23376;&#65289;&#12290;&#23454;&#36136;&#19978;&#65292;BaSEDB&#21160;&#24577;&#22320;&#23558;&#21327;&#21464;&#37327;&#31354;&#38388;&#20998;&#21106;&#25104;&#26356;&#23567;&#30340;&#31665;&#23376;&#65292;&#24182;&#20180;&#32454;&#35843;&#25972;&#23427;&#20204;&#30340;&#23485;&#24230;&#20197;&#31526;&#21512;&#25209;&#27425;&#22823;&#23567;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22312;&#25209;&#22788;&#29702;&#32422;&#26463;&#19979;&#38745;&#24577;&#20998;&#31665;&#30340;&#38750;&#26368;&#20248;&#24615;&#65292;&#31361;&#20986;&#20102;&#21160;&#24577;&#20998;&#31665;&#30340;&#24517;&#35201;&#24615;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#23436;&#20840;&#22312;&#32447;&#35774;&#32622;&#20013;&#65292;&#20960;&#20046;&#24658;&#23450;&#25968;&#37327;&#30340;&#31574;&#30053;&#26356;&#26032;&#21487;&#20197;&#36798;&#21040;&#26368;&#20339;&#21518;&#24724;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17732v1 Announce Type: cross  Abstract: We study nonparametric contextual bandits under batch constraints, where the expected reward for each action is modeled as a smooth function of covariates, and the policy updates are made at the end of each batch of observations. We establish a minimax regret lower bound for this setting and propose Batched Successive Elimination with Dynamic Binning (BaSEDB) that achieves optimal regret (up to logarithmic factors). In essence, BaSEDB dynamically splits the covariate space into smaller bins, carefully aligning their widths with the batch size. We also show the suboptimality of static binning under batch constraints, highlighting the necessity of dynamic binning. Additionally, our results suggest that a nearly constant number of policy updates can attain optimal regret in the fully online setting.
&lt;/p&gt;</description></item><item><title>&#28151;&#21512;&#27169;&#22411;&#23558;&#22522;&#20110;ODE&#30340;&#26426;&#26800;&#21160;&#21147;&#23398;&#19982;&#31070;&#32463;&#32593;&#32476;&#32452;&#20214;&#32467;&#21512;&#65292;&#22312;&#35299;&#37322;&#24615;&#21644;&#22240;&#26524;&#22522;&#30784;&#30340;&#21516;&#26102;&#65292;&#21033;&#29992;&#39046;&#22495;&#30693;&#35782;&#23545;&#27835;&#30103;&#25928;&#26524;&#36827;&#34892;&#25490;&#21517;&#65292;&#20174;&#32780;&#35299;&#20915;&#28789;&#27963;&#24615;&#22686;&#21152;&#24102;&#26469;&#30340;&#22240;&#26524;&#22522;&#30784;&#20002;&#22833;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.17233</link><description>&lt;p&gt;
&#28151;&#21512;&#26041;&#22359;&#31070;&#32463;ODE&#22240;&#26524;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Hybrid Square Neural ODE Causal Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17233
&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#27169;&#22411;&#23558;&#22522;&#20110;ODE&#30340;&#26426;&#26800;&#21160;&#21147;&#23398;&#19982;&#31070;&#32463;&#32593;&#32476;&#32452;&#20214;&#32467;&#21512;&#65292;&#22312;&#35299;&#37322;&#24615;&#21644;&#22240;&#26524;&#22522;&#30784;&#30340;&#21516;&#26102;&#65292;&#21033;&#29992;&#39046;&#22495;&#30693;&#35782;&#23545;&#27835;&#30103;&#25928;&#26524;&#36827;&#34892;&#25490;&#21517;&#65292;&#20174;&#32780;&#35299;&#20915;&#28789;&#27963;&#24615;&#22686;&#21152;&#24102;&#26469;&#30340;&#22240;&#26524;&#22522;&#30784;&#20002;&#22833;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#27169;&#22411;&#23558;&#22522;&#20110;ODE&#30340;&#26426;&#26800;&#21160;&#21147;&#23398;&#19982;&#28789;&#27963;&#19988;&#34920;&#36798;&#21147;&#24378;&#30340;&#31070;&#32463;&#32593;&#32476;&#32452;&#20214;&#32467;&#21512;&#36215;&#26469;&#12290;&#36825;&#31181;&#27169;&#22411;&#22312;&#31185;&#23398;&#39046;&#22495;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#29305;&#21035;&#26159;&#22312;ODE-based&#24314;&#27169;&#25552;&#20379;&#37325;&#35201;&#35299;&#37322;&#24615;&#21644;&#32463;&#36807;&#39564;&#35777;&#30340;&#22240;&#26524;&#22522;&#30784;&#65288;&#20363;&#22914;&#65292;&#29992;&#20110;&#21453;&#20107;&#23454;&#25512;&#29702;&#65289;&#30340;&#39046;&#22495;&#12290;&#23558;&#26426;&#26800;&#27169;&#22411;&#32435;&#20837;&#20063;&#20026;&#26631;&#20934;&#40657;&#31665;&#24314;&#27169;&#26041;&#27861;&#25552;&#20379;&#20102;&#24402;&#32435;&#20559;&#24046;&#65292;&#36825;&#22312;&#20174;&#23567;&#22411;&#25968;&#25454;&#38598;&#25110;&#37096;&#20998;&#35266;&#23519;&#21040;&#30340;&#22797;&#26434;&#31995;&#32479;&#20013;&#23398;&#20064;&#26102;&#33267;&#20851;&#37325;&#35201;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#38543;&#30528;&#28151;&#21512;&#27169;&#22411;&#21464;&#24471;&#26356;&#21152;&#28789;&#27963;&#65292;&#26426;&#26800;&#27169;&#22411;&#25552;&#20379;&#30340;&#22240;&#26524;&#22522;&#30784;&#24456;&#24555;&#20250;&#20002;&#22833;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#21478;&#19968;&#20010;&#24120;&#35265;&#30340;&#39046;&#22495;&#30693;&#35782;&#26469;&#28304;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65306;&#23545;&#19968;&#32452;&#24178;&#39044;&#30340;&#27835;&#30103;&#25928;&#26524;&#36827;&#34892;&#25490;&#21517;&#65292;&#21363;&#20351;&#20934;&#30830;&#30340;&#27835;&#30103;&#25928;&#26524;&#19981;&#30693;&#36947;&#12290;&#25105;&#20204;&#22312;&#22240;&#26524;&#25439;&#22833;&#20013;&#32534;&#30721;&#36825;&#20123;&#20449;&#24687;&#65292;&#23558;&#20854;&#19982;&#26631;&#20934;&#39044;&#27979;&#25439;&#22833;&#30456;&#32467;&#21512;&#65292;&#24471;&#20986;&#28151;&#21512;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17233v1 Announce Type: new  Abstract: Hybrid models combine mechanistic ODE-based dynamics with flexible and expressive neural network components. Such models have grown rapidly in popularity, especially in scientific domains where such ODE-based modeling offers important interpretability and validated causal grounding (e.g., for counterfactual reasoning). The incorporation of mechanistic models also provides inductive bias in standard blackbox modeling approaches, critical when learning from small datasets or partially observed, complex systems. Unfortunately, as hybrid models become more flexible, the causal grounding provided by the mechanistic model can quickly be lost. We address this problem by leveraging another common source of domain knowledge: ranking of treatment effects for a set of interventions, even if the precise treatment effect is unknown. We encode this information in a causal loss that we combine with the standard predictive loss to arrive at a hybrid los
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#37319;&#29992;&#21512;&#20316;&#21338;&#24328;&#35770;&#35299;&#37322;&#24320;&#25918;&#24335;&#21363;&#20852;&#22242;&#38431;&#21512;&#20316;&#20013;&#32852;&#21512;Q&#20540;&#34920;&#31034;&#30340;&#26032;&#29702;&#35770;&#65292;&#20026;&#36827;&#19968;&#27493;&#21457;&#23637;&#36825;&#19968;&#30740;&#31350;&#26041;&#21521;&#21644;&#24212;&#29992;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;</title><link>https://arxiv.org/abs/2402.15259</link><description>&lt;p&gt;
&#37319;&#29992;&#21512;&#20316;&#21338;&#24328;&#35770;&#30340;&#24320;&#25918;&#24335;&#21363;&#20852;&#22242;&#38431;&#21512;&#20316;
&lt;/p&gt;
&lt;p&gt;
Open Ad Hoc Teamwork with Cooperative Game Theory
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15259
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#37319;&#29992;&#21512;&#20316;&#21338;&#24328;&#35770;&#35299;&#37322;&#24320;&#25918;&#24335;&#21363;&#20852;&#22242;&#38431;&#21512;&#20316;&#20013;&#32852;&#21512;Q&#20540;&#34920;&#31034;&#30340;&#26032;&#29702;&#35770;&#65292;&#20026;&#36827;&#19968;&#27493;&#21457;&#23637;&#36825;&#19968;&#30740;&#31350;&#26041;&#21521;&#21644;&#24212;&#29992;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21363;&#20852;&#22242;&#38431;&#21512;&#20316;&#38754;&#20020;&#30528;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#38656;&#35201;&#35774;&#35745;&#19968;&#20010;&#33021;&#22815;&#19982;&#38431;&#21451;&#21327;&#20316;&#20294;&#27809;&#26377;&#20808;&#21069;&#21327;&#35843;&#25110;&#32852;&#21512;&#35757;&#32451;&#30340;&#26234;&#33021;&#20307;&#12290;&#24320;&#25918;&#24335;&#21363;&#20852;&#22242;&#38431;&#21512;&#20316;&#36827;&#19968;&#27493;&#22797;&#26434;&#21270;&#20102;&#36825;&#19968;&#25361;&#25112;&#65292;&#32771;&#34385;&#20102;&#20855;&#26377;&#19981;&#26029;&#21464;&#21270;&#30340;&#38431;&#21451;&#25968;&#37327;&#30340;&#29615;&#22659;&#65292;&#21363;&#24320;&#25918;&#24335;&#22242;&#38431;&#12290;&#29616;&#26377;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#26159;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#31574;&#30053;&#23398;&#20064;&#65288;GPL&#65289;&#65292;&#21033;&#29992;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#26469;&#22788;&#29702;&#26080;&#38480;&#25968;&#37327;&#30340;&#26234;&#33021;&#20307;&#65292;&#26377;&#25928;&#24212;&#23545;&#24320;&#25918;&#24335;&#22242;&#38431;&#12290;GPL&#30340;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#20294;&#20854;&#32852;&#21512;Q&#20540;&#34920;&#31034;&#23545;&#35299;&#37322;&#36896;&#25104;&#20102;&#25361;&#25112;&#65292;&#38459;&#30861;&#20102;&#36827;&#19968;&#27493;&#21457;&#23637;&#36825;&#19968;&#30740;&#31350;&#26041;&#21521;&#21644;&#24212;&#29992;&#12290;&#26412;&#25991;&#24314;&#31435;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35770;&#65292;&#20174;&#21512;&#20316;&#21338;&#24328;&#35770;&#30340;&#35282;&#24230;&#20026;GPL&#20013;&#37319;&#29992;&#30340;&#32852;&#21512;Q&#20540;&#34920;&#31034;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#37322;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#29702;&#35770;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15259v1 Announce Type: cross  Abstract: Ad hoc teamwork poses a challenging problem, requiring the design of an agent to collaborate with teammates without prior coordination or joint training. Open ad hoc teamwork further complicates this challenge by considering environments with a changing number of teammates, referred to as open teams. The state-of-the-art solution to this problem is graph-based policy learning (GPL), leveraging the generalizability of graph neural networks to handle an unrestricted number of agents and effectively address open teams. GPL's performance is superior to other methods, but its joint Q-value representation presents challenges for interpretation, hindering further development of this research line and applicability. In this paper, we establish a new theory to give an interpretation for the joint Q-value representation employed in GPL, from the perspective of cooperative game theory. Building on our theory, we propose a novel algorithm based on
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Clifford-Steerable&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CS-CNNs&#65289;&#65292;&#36890;&#36807;&#22312;&#20266;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#19978;&#22788;&#29702;&#22810;&#30690;&#22330;&#65292;&#21033;&#29992;Clifford&#32676;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#23545;$\mathrm{O}(p,q)$&#21487;&#23548;&#26680;&#36827;&#34892;&#38544;&#24335;&#21442;&#25968;&#21270;&#65292;&#26174;&#30528;&#19988;&#19968;&#33268;&#22320;&#20248;&#20110;&#27969;&#20307;&#21160;&#21147;&#23398;&#21644;&#30456;&#23545;&#35770;&#30005;&#21160;&#21147;&#23398;&#39044;&#27979;&#20219;&#21153;&#30340;&#22522;&#20934;&#26041;&#27861;</title><link>https://arxiv.org/abs/2402.14730</link><description>&lt;p&gt;
Clifford-Steerable&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Clifford-Steerable Convolutional Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14730
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Clifford-Steerable&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CS-CNNs&#65289;&#65292;&#36890;&#36807;&#22312;&#20266;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#19978;&#22788;&#29702;&#22810;&#30690;&#22330;&#65292;&#21033;&#29992;Clifford&#32676;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#23545;$\mathrm{O}(p,q)$&#21487;&#23548;&#26680;&#36827;&#34892;&#38544;&#24335;&#21442;&#25968;&#21270;&#65292;&#26174;&#30528;&#19988;&#19968;&#33268;&#22320;&#20248;&#20110;&#27969;&#20307;&#21160;&#21147;&#23398;&#21644;&#30456;&#23545;&#35770;&#30005;&#21160;&#21147;&#23398;&#39044;&#27979;&#20219;&#21153;&#30340;&#22522;&#20934;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Clifford-Steerable&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CS-CNNs&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;$\mathrm{E}(p, q)$&#31561;&#21464;CNN&#31867;&#12290; CS-CNNs&#22312;&#20266;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;$\mathbb{R}^{p,q}$&#19978;&#22788;&#29702;&#22810;&#30690;&#22330;&#12290; &#23427;&#20204;&#28085;&#30422;&#20102;&#20363;&#22914;$\mathrm{E}(3)$&#22312;$\mathbb{R}^3$&#19978;&#21644;Poincar\'e&#22312;&#38389;&#21487;&#22827;&#26031;&#22522;&#26102;&#31354;$\mathbb{R}^{1,3}$&#19978;&#30340;&#31561;&#21464;&#24615;&#12290; &#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#36890;&#36807;Clifford&#32676;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#23545;$\mathrm{O}(p,q)$&#21487;&#23548;&#26680;&#36827;&#34892;&#38544;&#24335;&#21442;&#25968;&#21270;&#12290; &#22312;&#27969;&#20307;&#21160;&#21147;&#23398;&#21644;&#30456;&#23545;&#35770;&#30005;&#21160;&#21147;&#23398;&#39044;&#27979;&#20219;&#21153;&#19978;&#65292;&#25105;&#20204;&#22312;&#22522;&#20934;&#26041;&#27861;&#19978;&#26174;&#30528;&#19988;&#19968;&#33268;&#22320;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14730v1 Announce Type: cross  Abstract: We present Clifford-Steerable Convolutional Neural Networks (CS-CNNs), a novel class of $\mathrm{E}(p, q)$-equivariant CNNs. CS-CNNs process multivector fields on pseudo-Euclidean spaces $\mathbb{R}^{p,q}$. They cover, for instance, $\mathrm{E}(3)$-equivariance on $\mathbb{R}^3$ and Poincar\'e-equivariance on Minkowski spacetime $\mathbb{R}^{1,3}$. Our approach is based on an implicit parametrization of $\mathrm{O}(p,q)$-steerable kernels via Clifford group equivariant neural networks. We significantly and consistently outperform baseline methods on fluid dynamics as well as relativistic electrodynamics forecasting tasks.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#25968;&#25454;&#27700;&#21360;&#22312;LLM&#39044;&#35757;&#32451;&#20013;&#26816;&#27979;&#29256;&#26435;&#25345;&#26377;&#20154;&#20316;&#21697;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36827;&#34892;&#21512;&#29702;&#26816;&#27979;&#19988;&#25552;&#20379;&#35823;&#26816;&#29575;&#20445;&#35777;&#65292;&#30740;&#31350;&#20102;&#27700;&#21360;&#35774;&#35745;&#23545;&#20551;&#35774;&#26816;&#39564;&#33021;&#21147;&#30340;&#24433;&#21709;&#20197;&#21450;&#22312;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#32553;&#25918;&#19979;&#30340;&#26816;&#27979;&#24378;&#24230;&#21464;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.10892</link><description>&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#27700;&#21360;&#35777;&#26126;LLM&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#25104;&#21592;&#36164;&#26684;
&lt;/p&gt;
&lt;p&gt;
Proving membership in LLM pretraining data via data watermarks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10892
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#25968;&#25454;&#27700;&#21360;&#22312;LLM&#39044;&#35757;&#32451;&#20013;&#26816;&#27979;&#29256;&#26435;&#25345;&#26377;&#20154;&#20316;&#21697;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36827;&#34892;&#21512;&#29702;&#26816;&#27979;&#19988;&#25552;&#20379;&#35823;&#26816;&#29575;&#20445;&#35777;&#65292;&#30740;&#31350;&#20102;&#27700;&#21360;&#35774;&#35745;&#23545;&#20551;&#35774;&#26816;&#39564;&#33021;&#21147;&#30340;&#24433;&#21709;&#20197;&#21450;&#22312;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#32553;&#25918;&#19979;&#30340;&#26816;&#27979;&#24378;&#24230;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#27979;&#29256;&#26435;&#25345;&#26377;&#20154;&#30340;&#20316;&#21697;&#26159;&#21542;&#22312;LLM&#39044;&#35757;&#32451;&#20013;&#20351;&#29992;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#25968;&#25454;&#27700;&#21360;&#23454;&#29616;&#22522;&#20110;&#40657;&#30418;&#27169;&#22411;&#35775;&#38382;&#30340;&#21512;&#29702;&#26816;&#27979;&#65292;&#21069;&#25552;&#26159;&#29256;&#26435;&#25345;&#26377;&#20154;&#22312;&#20844;&#24320;&#21457;&#24067;&#20043;&#21069;&#36129;&#29486;&#20102;&#22810;&#20010;&#35757;&#32451;&#25991;&#26723;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#27700;&#21360;&#22788;&#29702;&#12290;&#36890;&#36807;&#24212;&#29992;&#38543;&#26426;&#37319;&#26679;&#30340;&#25968;&#25454;&#27700;&#21360;&#65292;&#26816;&#27979;&#21487;&#20197;&#34987;&#26500;&#36896;&#20026;&#20551;&#35774;&#26816;&#39564;&#65292;&#20174;&#32780;&#25552;&#20379;&#23545;&#35823;&#26816;&#29575;&#30340;&#20445;&#35777;&#12290;&#30740;&#31350;&#20102;&#20004;&#31181;&#27700;&#21360;&#65306;&#19968;&#31181;&#25554;&#20837;&#38543;&#26426;&#24207;&#21015;&#65292;&#21478;&#19968;&#31181;&#38543;&#26426;&#29992;Unicode&#31867;&#20284;&#23383;&#31526;&#26367;&#25442;&#23383;&#31526;&#12290;&#39318;&#20808;&#23637;&#31034;&#20102;&#27700;&#21360;&#35774;&#35745;&#30340;&#19977;&#20010;&#26041;&#38754;--&#27700;&#21360;&#38271;&#24230;&#12289;&#22797;&#21046;&#27425;&#25968;&#21644;&#24178;&#25200;--&#22914;&#20309;&#24433;&#21709;&#20551;&#35774;&#26816;&#39564;&#30340;&#33021;&#21147;&#12290;&#25509;&#30528;&#30740;&#31350;&#20102;&#27700;&#21360;&#22312;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#32553;&#25918;&#19979;&#30340;&#26816;&#27979;&#24378;&#24230;&#22914;&#20309;&#21464;&#21270;&#65306;&#22686;&#21152;&#25968;&#25454;&#38598;&#22823;&#23567;&#20250;&#38477;&#20302;&#27700;&#21360;&#30340;&#24378;&#24230;&#65292;&#27700;&#21360;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10892v1 Announce Type: cross  Abstract: Detecting whether copyright holders' works were used in LLM pretraining is poised to be an important problem. This work proposes using data watermarks to enable principled detection with only black-box model access, provided that the rightholder contributed multiple training documents and watermarked them before public release. By applying a randomly sampled data watermark, detection can be framed as hypothesis testing, which provides guarantees on the false detection rate. We study two watermarks: one that inserts random sequences, and another that randomly substitutes characters with Unicode lookalikes. We first show how three aspects of watermark design -- watermark length, number of duplications, and interference -- affect the power of the hypothesis test. Next, we study how a watermark's detection strength changes under model and dataset scaling: while increasing the dataset size decreases the strength of the watermark, watermarks
&lt;/p&gt;</description></item><item><title>&#22312;&#19968;&#20010;&#37325;&#22797;&#30340;&#36125;&#21494;&#26031;&#35828;&#26381;&#38382;&#39064;&#20013;&#65292;&#21363;&#20351;&#27809;&#26377;&#25215;&#35834;&#33021;&#21147;&#65292;&#22996;&#25176;&#20154;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#19978;&#19979;&#25991;&#26080;&#36951;&#25022;&#23398;&#20064;&#31639;&#27861;&#26469;&#23454;&#29616;&#19982;&#32463;&#20856;&#26080;&#23398;&#20064;&#27169;&#22411;&#20013;&#20855;&#26377;&#25215;&#35834;&#30340;&#22996;&#25176;&#20154;&#30340;&#26368;&#20248;&#25928;&#29992;&#26080;&#38480;&#25509;&#36817;&#30340;&#25928;&#26524;&#65307;&#22312;&#20195;&#29702;&#20154;&#20351;&#29992;&#19978;&#19979;&#25991;&#26080;&#20132;&#25442;&#36951;&#25022;&#23398;&#20064;&#31639;&#27861;&#30340;&#24773;&#20917;&#19979;&#65292;&#22996;&#25176;&#20154;&#26080;&#27861;&#33719;&#24471;&#27604;&#20855;&#26377;&#25215;&#35834;&#30340;&#26080;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#26368;&#20248;&#25928;&#29992;&#26356;&#39640;&#30340;&#25928;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.09721</link><description>&lt;p&gt;
&#35828;&#26381;&#19968;&#20301;&#23398;&#20064;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Persuading a Learning Agent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09721
&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#20010;&#37325;&#22797;&#30340;&#36125;&#21494;&#26031;&#35828;&#26381;&#38382;&#39064;&#20013;&#65292;&#21363;&#20351;&#27809;&#26377;&#25215;&#35834;&#33021;&#21147;&#65292;&#22996;&#25176;&#20154;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#19978;&#19979;&#25991;&#26080;&#36951;&#25022;&#23398;&#20064;&#31639;&#27861;&#26469;&#23454;&#29616;&#19982;&#32463;&#20856;&#26080;&#23398;&#20064;&#27169;&#22411;&#20013;&#20855;&#26377;&#25215;&#35834;&#30340;&#22996;&#25176;&#20154;&#30340;&#26368;&#20248;&#25928;&#29992;&#26080;&#38480;&#25509;&#36817;&#30340;&#25928;&#26524;&#65307;&#22312;&#20195;&#29702;&#20154;&#20351;&#29992;&#19978;&#19979;&#25991;&#26080;&#20132;&#25442;&#36951;&#25022;&#23398;&#20064;&#31639;&#27861;&#30340;&#24773;&#20917;&#19979;&#65292;&#22996;&#25176;&#20154;&#26080;&#27861;&#33719;&#24471;&#27604;&#20855;&#26377;&#25215;&#35834;&#30340;&#26080;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#26368;&#20248;&#25928;&#29992;&#26356;&#39640;&#30340;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#37325;&#22797;&#30340;&#36125;&#21494;&#26031;&#35828;&#26381;&#38382;&#39064;&#65288;&#26356;&#19968;&#33324;&#22320;&#65292;&#20219;&#20309;&#20855;&#26377;&#23436;&#20840;&#20449;&#24687;&#30340;&#24191;&#20041;&#22996;&#25176;-&#20195;&#29702;&#38382;&#39064;&#65289;&#65292;&#20854;&#20013;&#22996;&#25176;&#20154;&#27809;&#26377;&#25215;&#35834;&#33021;&#21147;&#65292;&#20195;&#29702;&#20154;&#20351;&#29992;&#31639;&#27861;&#26469;&#23398;&#20064;&#22914;&#20309;&#23545;&#22996;&#25176;&#20154;&#30340;&#20449;&#21495;&#20570;&#20986;&#21709;&#24212;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#31616;&#21270;&#20026;&#19968;&#20010;&#19968;&#27425;&#24615;&#30340;&#24191;&#20041;&#22996;&#25176;-&#20195;&#29702;&#38382;&#39064;&#65292;&#20195;&#29702;&#20154;&#36817;&#20284;&#22320;&#26368;&#20339;&#21709;&#24212;&#12290;&#36890;&#36807;&#36825;&#20010;&#31616;&#21270;&#65292;&#25105;&#20204;&#21487;&#20197;&#35777;&#26126;&#65306;&#22914;&#26524;&#20195;&#29702;&#20154;&#20351;&#29992;&#19978;&#19979;&#25991;&#26080;&#36951;&#25022;&#23398;&#20064;&#31639;&#27861;&#65292;&#21017;&#22996;&#25176;&#20154;&#21487;&#20197;&#20445;&#35777;&#20854;&#25928;&#29992;&#19982;&#32463;&#20856;&#26080;&#23398;&#20064;&#27169;&#22411;&#20013;&#20855;&#26377;&#25215;&#35834;&#30340;&#22996;&#25176;&#20154;&#30340;&#26368;&#20248;&#25928;&#29992;&#20043;&#38388;&#21487;&#20197;&#26080;&#38480;&#25509;&#36817;&#65307;&#22914;&#26524;&#20195;&#29702;&#20154;&#20351;&#29992;&#19978;&#19979;&#25991;&#26080;&#20132;&#25442;&#36951;&#25022;&#23398;&#20064;&#31639;&#27861;&#65292;&#21017;&#22996;&#25176;&#20154;&#26080;&#27861;&#33719;&#24471;&#27604;&#20855;&#26377;&#25215;&#35834;&#30340;&#26080;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#26368;&#20248;&#25928;&#29992;&#26356;&#39640;&#30340;&#25928;&#29992;&#12290;&#22996;&#25176;&#20154;&#22312;&#23398;&#20064;&#27169;&#22411;&#19982;&#38750;&#23398;&#20064;&#27169;&#22411;&#20013;&#21487;&#20197;&#33719;&#24471;&#30340;&#25928;&#29992;&#20043;&#38388;&#30340;&#24046;&#36317;&#26159;&#26377;&#30028;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09721v1 Announce Type: cross  Abstract: We study a repeated Bayesian persuasion problem (and more generally, any generalized principal-agent problem with complete information) where the principal does not have commitment power and the agent uses algorithms to learn to respond to the principal's signals. We reduce this problem to a one-shot generalized principal-agent problem with an approximately-best-responding agent. This reduction allows us to show that: if the agent uses contextual no-regret learning algorithms, then the principal can guarantee a utility that is arbitrarily close to the principal's optimal utility in the classic non-learning model with commitment; if the agent uses contextual no-swap-regret learning algorithms, then the principal cannot obtain any utility significantly more than the optimal utility in the non-learning model with commitment. The difference between the principal's obtainable utility in the learning model and the non-learning model is bound
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#22312;&#24213;&#23618;&#31354;&#38388;&#20013;&#37319;&#26679;&#30340;&#22270;&#26469;&#26377;&#25928;&#22320;&#37325;&#26500;&#38543;&#26426;&#20960;&#20309;&#22270;&#30340;&#20960;&#20309;&#24418;&#29366;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#27969;&#24418;&#20551;&#35774;&#65292;&#21363;&#24213;&#23618;&#31354;&#38388;&#26159;&#20302;&#32500;&#27969;&#24418;&#65292;&#24182;&#19988;&#36830;&#25509;&#27010;&#29575;&#26159;&#23884;&#20837;&#22312;$\mathbb{R}^N$&#20013;&#30340;&#27969;&#24418;&#20013;&#28857;&#20043;&#38388;&#27431;&#20960;&#37324;&#24503;&#36317;&#31163;&#30340;&#20005;&#26684;&#36882;&#20943;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.09591</link><description>&lt;p&gt;
&#37325;&#26500;&#38543;&#26426;&#20960;&#20309;&#22270;&#30340;&#20960;&#20309;&#24418;&#29366;
&lt;/p&gt;
&lt;p&gt;
Reconstructing the Geometry of Random Geometric Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09591
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#22312;&#24213;&#23618;&#31354;&#38388;&#20013;&#37319;&#26679;&#30340;&#22270;&#26469;&#26377;&#25928;&#22320;&#37325;&#26500;&#38543;&#26426;&#20960;&#20309;&#22270;&#30340;&#20960;&#20309;&#24418;&#29366;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#27969;&#24418;&#20551;&#35774;&#65292;&#21363;&#24213;&#23618;&#31354;&#38388;&#26159;&#20302;&#32500;&#27969;&#24418;&#65292;&#24182;&#19988;&#36830;&#25509;&#27010;&#29575;&#26159;&#23884;&#20837;&#22312;$\mathbb{R}^N$&#20013;&#30340;&#27969;&#24418;&#20013;&#28857;&#20043;&#38388;&#27431;&#20960;&#37324;&#24503;&#36317;&#31163;&#30340;&#20005;&#26684;&#36882;&#20943;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#20960;&#20309;&#22270;&#26159;&#22312;&#24230;&#37327;&#31354;&#38388;&#19978;&#23450;&#20041;&#30340;&#38543;&#26426;&#22270;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#39318;&#20808;&#20174;&#24230;&#37327;&#31354;&#38388;&#20013;&#37319;&#26679;&#28857;&#65292;&#28982;&#21518;&#20197;&#20381;&#36182;&#20110;&#23427;&#20204;&#20043;&#38388;&#36317;&#31163;&#30340;&#27010;&#29575;&#29420;&#31435;&#22320;&#36830;&#25509;&#27599;&#23545;&#37319;&#26679;&#28857;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#27969;&#24418;&#20551;&#35774;&#19979;&#26377;&#25928;&#22320;&#20174;&#37319;&#26679;&#30340;&#22270;&#20013;&#37325;&#26500;&#24213;&#23618;&#31354;&#38388;&#30340;&#20960;&#20309;&#24418;&#29366;&#65292;&#21363;&#20551;&#35774;&#24213;&#23618;&#31354;&#38388;&#26159;&#20302;&#32500;&#27969;&#24418;&#65292;&#24182;&#19988;&#36830;&#25509;&#27010;&#29575;&#26159;&#23884;&#20837;&#22312;$\mathbb{R}^N$&#20013;&#30340;&#27969;&#24418;&#20013;&#28857;&#20043;&#38388;&#27431;&#20960;&#37324;&#24503;&#36317;&#31163;&#30340;&#20005;&#26684;&#36882;&#20943;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#34917;&#20805;&#20102;&#22823;&#37327;&#20851;&#20110;&#27969;&#24418;&#23398;&#20064;&#30340;&#24037;&#20316;&#65292;&#20854;&#30446;&#26631;&#26159;&#20174;&#22312;&#27969;&#24418;&#20013;&#37319;&#26679;&#30340;&#28857;&#21450;&#20854;&#65288;&#36817;&#20284;&#30340;&#65289;&#36317;&#31163;&#20013;&#24674;&#22797;&#20986;&#27969;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09591v1 Announce Type: new  Abstract: Random geometric graphs are random graph models defined on metric spaces. Such a model is defined by first sampling points from a metric space and then connecting each pair of sampled points with probability that depends on their distance, independently among pairs. In this work, we show how to efficiently reconstruct the geometry of the underlying space from the sampled graph under the manifold assumption, i.e., assuming that the underlying space is a low dimensional manifold and that the connection probability is a strictly decreasing function of the Euclidean distance between the points in a given embedding of the manifold in $\mathbb{R}^N$. Our work complements a large body of work on manifold learning, where the goal is to recover a manifold from sampled points sampled in the manifold along with their (approximate) distances.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25193;&#23637;&#20102;&#22312;&#32447;&#32467;&#26500;&#21270;&#39044;&#27979;&#30340;&#26367;&#20195;&#21518;&#24724;&#24230;&#30028;&#38480;&#65292;&#36890;&#36807;&#24341;&#20837;Fenchel-Young&#25439;&#22833;&#21644;&#38543;&#26426;&#35299;&#30721;&#26041;&#26696;&#65292;&#20351;&#24471;&#22312;&#22312;&#32447;&#22810;&#31867;&#20998;&#31867;&#21644;&#36923;&#36753;&#25439;&#22833;&#19979;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.08180</link><description>&lt;p&gt;
&#22312;&#32447;&#32467;&#26500;&#21270;&#39044;&#27979;&#19982;Fenchel-Young&#25439;&#22833;&#21644;&#25913;&#36827;&#30340;&#26367;&#20195;&#21518;&#24724;&#24230;&#29992;&#20110;&#22312;&#32447;&#22810;&#31867;&#20998;&#31867;&#19982;&#36923;&#36753;&#25439;&#22833;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Online Structured Prediction with Fenchel--Young Losses and Improved Surrogate Regret for Online Multiclass Classification with Logistic Loss
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08180
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25193;&#23637;&#20102;&#22312;&#32447;&#32467;&#26500;&#21270;&#39044;&#27979;&#30340;&#26367;&#20195;&#21518;&#24724;&#24230;&#30028;&#38480;&#65292;&#36890;&#36807;&#24341;&#20837;Fenchel-Young&#25439;&#22833;&#21644;&#38543;&#26426;&#35299;&#30721;&#26041;&#26696;&#65292;&#20351;&#24471;&#22312;&#22312;&#32447;&#22810;&#31867;&#20998;&#31867;&#21644;&#36923;&#36753;&#25439;&#22833;&#19979;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#23436;&#20840;&#20449;&#24687;&#21453;&#39304;&#30340;&#22312;&#32447;&#32467;&#26500;&#21270;&#39044;&#27979;&#12290;&#23545;&#20110;&#22312;&#32447;&#22810;&#31867;&#20998;&#31867;&#65292;van der Hoeven(2020)&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#20248;&#32654;&#30340;&#8220;&#21033;&#29992;&#26367;&#20195;&#38388;&#38553;&#8221;&#30340;&#26694;&#26550;&#65292;&#33719;&#24471;&#20102;&#19982;&#26102;&#38388;&#33539;&#22260;&#26080;&#20851;&#30340;&#26367;&#20195;&#21518;&#24724;&#24230;&#30028;&#38480;&#65292;&#21363;&#26377;&#38480;&#30340;&#30028;&#38480;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#26694;&#26550;&#20027;&#35201;&#38480;&#20110;&#22810;&#31867;&#20998;&#31867;&#65292;&#22240;&#20026;&#23427;&#20381;&#36182;&#20110;&#19968;&#31181;&#29305;&#23450;&#20110;&#20998;&#31867;&#30340;&#36807;&#31243;&#65292;&#23558;&#20272;&#35745;&#24471;&#20998;&#36716;&#21270;&#20026;&#36755;&#20986;&#12290;&#25105;&#20204;&#23558;&#8220;&#21033;&#29992;&#26367;&#20195;&#38388;&#38553;&#8221;&#26694;&#26550;&#25193;&#23637;&#21040;&#20855;&#26377;&#8220;Fenchel-Young&#25439;&#22833;&#8221;&#30340;&#22312;&#32447;&#32467;&#26500;&#21270;&#39044;&#27979;&#20013;&#65292;&#36825;&#26159;&#19968;&#22823;&#31867;&#21253;&#25324;&#22810;&#31867;&#20998;&#31867;&#30340;&#36923;&#36753;&#25439;&#22833;&#22312;&#20869;&#30340;&#26367;&#20195;&#25439;&#22833;&#65292;&#33719;&#24471;&#20102;&#22312;&#21508;&#31181;&#32467;&#26500;&#21270;&#39044;&#27979;&#38382;&#39064;&#19978;&#30340;&#26377;&#38480;&#26367;&#20195;&#21518;&#24724;&#24230;&#30028;&#38480;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#38543;&#26426;&#35299;&#30721;&#65292;&#23558;&#20272;&#35745;&#24471;&#20998;&#36716;&#21270;&#20026;&#19968;&#33324;&#30340;&#32467;&#26500;&#21270;&#36755;&#20986;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23558;&#25105;&#20204;&#30340;&#35299;&#30721;&#24212;&#29992;&#20110;&#22312;&#32447;&#22810;&#31867;&#20998;&#31867;&#19982;&#36923;&#36753;&#25439;&#22833;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#25913;&#36827;&#30340;&#26367;&#20195;&#21518;&#24724;&#24230;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies online structured prediction with full-information feedback. For online multiclass classification, van der Hoeven (2020) has obtained surrogate regret bounds independent of the time horizon, or \emph{finite}, by introducing an elegant \emph{exploit-the-surrogate-gap} framework. However, this framework has been limited to multiclass classification primarily because it relies on a classification-specific procedure for converting estimated scores to outputs. We extend the exploit-the-surrogate-gap framework to online structured prediction with \emph{Fenchel--Young losses}, a large family of surrogate losses including the logistic loss for multiclass classification, obtaining finite surrogate regret bounds in various structured prediction problems. To this end, we propose and analyze \emph{randomized decoding}, which converts estimated scores to general structured outputs. Moreover, by applying our decoding to online multiclass classification with the logistic loss, we o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#29420;&#31435;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;&#39532;&#23572;&#31185;&#22827;&#21338;&#24328;&#20013;&#65292;&#36890;&#36807;&#25913;&#36827;AVLPR&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#25968;&#25454;&#20381;&#36182;&#30340;&#24754;&#35266;&#20272;&#35745;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22810;&#26234;&#33021;&#20307;&#30340;&#35781;&#21650;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.07082</link><description>&lt;p&gt;
&#22522;&#20110;&#29420;&#31435;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;&#39532;&#23572;&#31185;&#22827;&#21338;&#24328;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Refined Sample Complexity for Markov Games with Independent Linear Function Approximation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#29420;&#31435;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;&#39532;&#23572;&#31185;&#22827;&#21338;&#24328;&#20013;&#65292;&#36890;&#36807;&#25913;&#36827;AVLPR&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#25968;&#25454;&#20381;&#36182;&#30340;&#24754;&#35266;&#20272;&#35745;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22810;&#26234;&#33021;&#20307;&#30340;&#35781;&#21650;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39532;&#23572;&#31185;&#22827;&#21338;&#24328;&#65288;MG&#65289;&#26159;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#20013;&#30340;&#37325;&#35201;&#27169;&#22411;&#12290;&#38271;&#26399;&#20197;&#26469;&#20154;&#20204;&#19968;&#30452;&#35748;&#20026;&#8220;&#22810;&#26234;&#33021;&#20307;&#30340;&#35781;&#21650;&#8221;&#65288;&#21363;&#31639;&#27861;&#24615;&#33021;&#38543;&#30528;&#26234;&#33021;&#20307;&#25968;&#37327;&#25351;&#25968;&#32423;&#19979;&#38477;&#65289;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#65292;&#30452;&#21040;&#26368;&#36817;&#20960;&#31687;&#20316;&#21697;&#65288;Daskalakis&#31561;&#20154;&#65292;2023&#24180;&#65307;Cui&#31561;&#20154;&#65292;2023&#24180;&#65307;Wang&#31561;&#20154;&#65292;2023&#24180;&#65289;&#12290;&#36825;&#20123;&#20316;&#21697;&#30830;&#23454;&#35299;&#20915;&#20102;&#22810;&#26234;&#33021;&#20307;&#30340;&#35781;&#21650;&#65292;&#24403;&#29366;&#24577;&#31354;&#38388;&#26497;&#22823;&#19988;&#65288;&#32447;&#24615;&#65289;&#20989;&#25968;&#36924;&#36817;&#34987;&#24212;&#29992;&#26102;&#65292;&#23427;&#20204;&#35201;&#20040;&#20855;&#26377;&#26356;&#24930;&#30340;&#25910;&#25947;&#36895;&#24230;$O(T^{-1/4})$&#65292;&#35201;&#20040;&#22312;&#34892;&#21160;&#25968;$A_{\max}$&#19978;&#24102;&#26469;&#22810;&#39033;&#24335;&#20381;&#36182;&#8212;&#8212;&#23613;&#31649;&#22312;&#21333;&#26234;&#33021;&#20307;&#24773;&#20917;&#19979;&#21363;&#20351;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#38543;&#26102;&#38388;&#20219;&#24847;&#21464;&#21270;&#65288;Dai&#31561;&#20154;&#65292;2023&#24180;&#65289;&#65292;&#20063;&#21487;&#36991;&#20813;&#36825;&#31181;&#20381;&#36182;&#12290;&#26412;&#25991;&#39318;&#20808;&#36890;&#36807;Wang&#31561;&#20154;&#65288;2023&#24180;&#65289;&#30340;&#8220;AVLPR&#8221;&#26694;&#26550;&#31934;&#21270;&#65292;&#27934;&#23519;&#20102;&#22522;&#20110;&#25968;&#25454;&#30340;&#65288;&#21363;&#38543;&#26426;&#30340;&#65289;&#24754;&#35266;&#20272;&#35745;&#23376;&#20248;&#21270;&#24046;&#36317;&#65292;&#20174;&#32780;&#20801;&#35768;&#26356;&#24191;&#27867;&#30340;&#25554;&#20214;&#31639;&#27861;&#36873;&#25321;&#12290;&#24403;&#19987;&#38376;&#24212;&#29992;&#20110;MGs&#26102;&#65292;&#36825;&#19968;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#29420;&#31435;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Markov Games (MG) is an important model for Multi-Agent Reinforcement Learning (MARL). It was long believed that the "curse of multi-agents" (i.e., the algorithmic performance drops exponentially with the number of agents) is unavoidable until several recent works (Daskalakis et al., 2023; Cui et al., 2023; Wang et al., 2023. While these works did resolve the curse of multi-agents, when the state spaces are prohibitively large and (linear) function approximations are deployed, they either had a slower convergence rate of $O(T^{-1/4})$ or brought a polynomial dependency on the number of actions $A_{\max}$ -- which is avoidable in single-agent cases even when the loss functions can arbitrarily vary with time (Dai et al., 2023). This paper first refines the `AVLPR` framework by Wang et al. (2023), with an insight of *data-dependent* (i.e., stochastic) pessimistic estimation of the sub-optimality gap, allowing a broader choice of plug-in algorithms. When specialized to MGs with independent
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25945;&#24072;&#27169;&#22411;&#12289;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#23398;&#29983;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#22312;&#29702;&#35299;&#25991;&#26412;-&#23646;&#24615;&#22270;&#20013;&#30340;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.05894</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#30693;&#35782;&#33976;&#39311;&#20013;&#36935;&#35265;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Large Language Model Meets Graph Neural Network in Knowledge Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05894
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25945;&#24072;&#27169;&#22411;&#12289;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#23398;&#29983;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#22312;&#29702;&#35299;&#25991;&#26412;-&#23646;&#24615;&#22270;&#20013;&#30340;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36817;&#26399;&#23398;&#26415;&#30028;&#23545;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29702;&#35299;&#25991;&#26412;-&#23646;&#24615;&#22270;&#65288;TAG&#65289;&#26041;&#38754;&#30340;&#36827;&#23637;&#21644;&#28508;&#21147;&#26377;&#25152;&#25259;&#38706;&#65292;&#20294;LLMs&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#37096;&#32626;&#21463;&#21040;&#20102;&#35745;&#31639;&#21644;&#23384;&#20648;&#38656;&#27714;&#39640;&#65292;&#25512;&#29702;&#36807;&#31243;&#20013;&#24310;&#36831;&#38271;&#30340;&#38480;&#21046;&#12290;&#21516;&#26102;&#65292;&#20256;&#32479;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#34429;&#28982;&#36731;&#37327;&#19988;&#25797;&#38271;&#23398;&#20064;&#22270;&#30340;&#32467;&#26500;&#29305;&#24449;&#65292;&#20294;&#23545;&#20110;&#30495;&#23454;&#24212;&#29992;&#20013;TAG&#22797;&#26434;&#35821;&#20041;&#30340;&#25226;&#25569;&#26377;&#25152;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#32858;&#28966;&#20110;TAG&#20013;&#33410;&#28857;&#20998;&#31867;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#31216;&#20026;&#35821;&#35328;&#22270;&#30693;&#35782;&#33976;&#39311;&#65288;LinguGKD&#65289;&#65292;&#20351;&#29992;LLMs&#20316;&#20026;&#25945;&#24072;&#27169;&#22411;&#65292;GNNs&#20316;&#20026;&#23398;&#29983;&#27169;&#22411;&#36827;&#34892;&#30693;&#35782;&#33976;&#39311;&#12290;&#20854;&#20013;&#21253;&#25324;&#23545;LLM&#36827;&#34892;TAG&#23450;&#21521;&#25351;&#23548;&#35843;&#25972;&#20197;&#24212;&#23545;&#35774;&#35745;&#30340;&#33410;&#28857;&#20998;&#31867;&#25552;&#31034;&#65292;&#28982;&#21518;&#23545;&#23618;&#27425;&#21270;&#23398;&#20064;&#30340;&#33410;&#28857;&#29305;&#24449;&#36827;&#34892;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent community revelations about the advancements and potential of Large Language Models (LLMs) in understanding Text-Attributed Graphs (TAG), the deployment of LLMs for production is hindered by their high computational and storage requirements, as well as long latencies during inference. Simultaneously, although traditional Graph Neural Networks (GNNs) are light weight and adept at learning structural features of graphs, their ability to grasp the complex semantics in TAGs is somewhat constrained for real applications. To address these limitations, we concentrate on the downstream task of node classification in TAG and propose a novel graph knowledge distillation framework, termed Linguistic Graph Knowledge Distillation (LinguGKD), using LLMs as teacher models and GNNs as student models for knowledge distillation. It involves TAG-oriented instruction tuning of LLM on designed node classification prompts, followed by aligning the hierarchically learned node features of the t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22797;&#20540;&#31070;&#32463;&#32593;&#32476;&#22312;&#26377;&#38480;&#30340;&#40614;&#20811;&#39118;&#25968;&#25454;&#20013;&#20272;&#35745;&#25151;&#38388;&#20256;&#36882;&#20989;&#25968;&#65292;&#23454;&#29616;&#20102;&#25151;&#38388;&#20256;&#36882;&#20989;&#25968;&#30340;&#37325;&#24314;&#65292;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#39318;&#27425;&#20351;&#29992;&#20102;&#22797;&#20540;&#31070;&#32463;&#32593;&#32476;&#26469;&#20272;&#35745;&#25151;&#38388;&#20256;&#36882;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.04866</link><description>&lt;p&gt;
&#20351;&#29992;&#22797;&#20540;&#31070;&#32463;&#32593;&#32476;&#21644;&#19981;&#35268;&#21017;&#20998;&#24067;&#30340;&#40614;&#20811;&#39118;&#37325;&#24314;&#25151;&#38388;&#20256;&#36882;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Room transfer function reconstruction using complex-valued neural networks and irregularly distributed microphones
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04866
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22797;&#20540;&#31070;&#32463;&#32593;&#32476;&#22312;&#26377;&#38480;&#30340;&#40614;&#20811;&#39118;&#25968;&#25454;&#20013;&#20272;&#35745;&#25151;&#38388;&#20256;&#36882;&#20989;&#25968;&#65292;&#23454;&#29616;&#20102;&#25151;&#38388;&#20256;&#36882;&#20989;&#25968;&#30340;&#37325;&#24314;&#65292;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#39318;&#27425;&#20351;&#29992;&#20102;&#22797;&#20540;&#31070;&#32463;&#32593;&#32476;&#26469;&#20272;&#35745;&#25151;&#38388;&#20256;&#36882;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#24314;&#25151;&#38388;&#20256;&#36882;&#20989;&#25968;&#29992;&#20110;&#35745;&#31639;&#25151;&#38388;&#20869;&#30340;&#22797;&#26434;&#22768;&#22330;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#36890;&#24120;&#38656;&#35201;&#20351;&#29992;&#22823;&#37327;&#30340;&#40614;&#20811;&#39118;&#65292;&#36825;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#26368;&#36817;&#65292;&#38500;&#20102;&#20256;&#32479;&#30340;&#20449;&#21495;&#22788;&#29702;&#26041;&#27861;&#65292;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#24050;&#32463;&#34987;&#24212;&#29992;&#20110;&#20174;&#25151;&#38388;&#20869;&#38646;&#25955;&#28857;&#27979;&#37327;&#24471;&#21040;&#30340;&#26377;&#38480;&#30340;&#25151;&#38388;&#20256;&#36882;&#20989;&#25968;&#26469;&#37325;&#24314;&#25151;&#38388;&#20256;&#36882;&#20989;&#25968;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#22797;&#20540;&#31070;&#32463;&#32593;&#32476;&#20272;&#35745;&#25151;&#38388;&#20256;&#36882;&#20989;&#25968;&#22312;&#31532;&#19968;&#20010;&#22768;&#23398;&#20849;&#25391;&#39057;&#29575;&#33539;&#22260;&#20869;&#65292;&#20351;&#29992;&#23569;&#37327;&#19981;&#35268;&#21017;&#20998;&#24067;&#30340;&#40614;&#20811;&#39118;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#39318;&#27425;&#20351;&#29992;&#22797;&#20540;&#31070;&#32463;&#32593;&#32476;&#26469;&#20272;&#35745;&#25151;&#38388;&#20256;&#36882;&#20989;&#25968;&#12290;&#20026;&#20102;&#20998;&#26512;&#23558;&#22797;&#20540;&#20248;&#21270;&#24212;&#29992;&#20110;&#25152;&#32771;&#34385;&#20219;&#21153;&#30340;&#22909;&#22788;&#65292;&#25105;&#20204;&#23558;&#25152;&#25552;&#20986;&#30340;&#25216;&#26415;&#19982;&#26368;&#20808;&#36827;&#30340;&#23454;&#20540;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#21644;&#22522;&#20110;&#26680;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reconstructing the room transfer functions needed to calculate the complex sound field in a room has several important real-world applications. However, an unpractical number of microphones is often required. Recently, in addition to classical signal processing methods, deep learning techniques have been applied to reconstruct the room transfer function starting from a very limited set of room transfer functions measured at scattered points in the room. In this study, we employ complex-valued neural networks to estimate room transfer functions in the frequency range of the first room resonances, using a few irregularly distributed microphones. To the best of our knowledge, this is the first time complex-valued neural networks are used to estimate room transfer functions. To analyze the benefits of applying complex-valued optimization to the considered task, we compare the proposed technique with a state-of-the-art real-valued neural network method and a state-of-the-art kernel-based si
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#23450;&#20301;&#21644;&#25237;&#24433;&#26041;&#27861;&#30340;&#20551;&#35774;&#12290;&#36890;&#36807;&#26597;&#35810;&#21644;&#38190;&#30697;&#38453;&#26469;&#35745;&#31639;&#36755;&#20837;&#25991;&#26412;&#19982;&#27599;&#20010;&#28436;&#31034;&#20043;&#38388;&#30340;&#27880;&#24847;&#21147;&#26435;&#37325;&#65292;&#20197;&#23398;&#20064;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2402.02872</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65311;&#26597;&#35810;&#21644;&#38190;&#30697;&#38453;&#26159;&#19978;&#19979;&#25991;&#22836;&#37096;&#36827;&#34892;&#24230;&#37327;&#23398;&#20064;&#30340;&#20004;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;
&lt;/p&gt;
&lt;p&gt;
How do Large Language Models Learn In-Context? Query and Key Matrices of In-Context Heads are Two Towers for Metric Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02872
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#23450;&#20301;&#21644;&#25237;&#24433;&#26041;&#27861;&#30340;&#20551;&#35774;&#12290;&#36890;&#36807;&#26597;&#35810;&#21644;&#38190;&#30697;&#38453;&#26469;&#35745;&#31639;&#36755;&#20837;&#25991;&#26412;&#19982;&#27599;&#20010;&#28436;&#31034;&#20043;&#38388;&#30340;&#27880;&#24847;&#21147;&#26435;&#37325;&#65292;&#20197;&#23398;&#20064;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#32034;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26426;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#23450;&#20301;&#21644;&#25237;&#24433;&#26041;&#27861;&#30340;&#20551;&#35774;&#12290;&#22312;&#27973;&#23618;&#20013;&#65292;&#28436;&#31034;&#30340;&#29305;&#24449;&#34987;&#21512;&#24182;&#21040;&#30456;&#24212;&#30340;&#26631;&#31614;&#20013;&#65292;&#36755;&#20837;&#25991;&#26412;&#30340;&#29305;&#24449;&#34987;&#32858;&#21512;&#21040;&#26368;&#21518;&#19968;&#20010;&#26631;&#35760;&#20013;&#12290;&#22312;&#28145;&#23618;&#20013;&#65292;&#19978;&#19979;&#25991;&#22836;&#37096;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#22312;&#27599;&#20010;&#19978;&#19979;&#25991;&#22836;&#37096;&#20013;&#65292;&#20540;-&#36755;&#20986;&#30697;&#38453;&#25552;&#21462;&#20102;&#26631;&#31614;&#30340;&#29305;&#24449;&#12290;&#26597;&#35810;&#21644;&#38190;&#30697;&#38453;&#35745;&#31639;&#20102;&#36755;&#20837;&#25991;&#26412;&#19982;&#27599;&#20010;&#28436;&#31034;&#20043;&#38388;&#30340;&#27880;&#24847;&#21147;&#26435;&#37325;&#12290;&#27880;&#24847;&#21147;&#26435;&#37325;&#36234;&#22823;&#65292;&#36234;&#22810;&#30340;&#26631;&#31614;&#20449;&#24687;&#34987;&#20256;&#36755;&#21040;&#26368;&#21518;&#19968;&#20010;&#26631;&#35760;&#20013;&#65292;&#29992;&#20110;&#39044;&#27979;&#19979;&#19968;&#20010;&#21333;&#35789;&#12290;&#26597;&#35810;&#21644;&#38190;&#30697;&#38453;&#21487;&#20197;&#34987;&#35270;&#20026;&#23398;&#20064;&#36755;&#20837;&#25991;&#26412;&#19982;&#27599;&#20010;&#28436;&#31034;&#20043;&#38388;&#30456;&#20284;&#24230;&#24230;&#37327;&#30340;&#20004;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#22522;&#20110;&#36825;&#20010;&#20551;&#35774;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#19981;&#24179;&#34913;&#30340;&#26631;&#31614;&#21644;&#28436;&#31034;&#39034;&#24207;&#20250;&#24433;&#21709;&#39044;&#27979;&#12290;&#25105;&#20204;&#22312;GPT2&#22823;&#22411;&#12289;Llama 7B&#12289;13B&#21644;30B&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#32467;&#26524;&#25903;&#25345;&#25105;&#20204;&#30340;&#20998;&#26512;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#29702;&#35770;&#35299;&#37322;&#21644;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore the mechanism of in-context learning and propose a hypothesis using locate-and-project method. In shallow layers, the features of demonstrations are merged into their corresponding labels, and the features of the input text are aggregated into the last token. In deep layers, in-context heads make great contributions. In each in-context head, the value-output matrix extracts the labels' features. Query and key matrices compute the attention weights between the input text and each demonstration. The larger the attention weight is, the more label information is transferred into the last token for predicting the next word. Query and key matrices can be regarded as two towers for learning the similarity metric between the input text and each demonstration. Based on this hypothesis, we explain why imbalanced labels and demonstration order affect predictions. We conduct experiments on GPT2 large, Llama 7B, 13B and 30B. The results can support our analysis. Overall, our study provid
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#21033;&#29992;&#22320;&#29699;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#22238;&#24402;&#20219;&#21153;&#26102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;&#12290;&#36890;&#36807;&#21033;&#29992;&#28508;&#22312;&#31354;&#38388;&#34920;&#31034;&#26469;&#25512;&#23548;&#32622;&#20449;&#24230;&#24230;&#37327;&#65292;&#24314;&#31435;&#20102;&#28508;&#22312;&#34920;&#31034;&#20013;&#30340;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#19982;&#20010;&#20307;&#34442;&#23376;&#31181;&#32676;&#39044;&#27979;&#30340;&#32477;&#23545;&#35823;&#24046;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#24847;&#22823;&#21033;&#23041;&#23612;&#25176;&#22320;&#21306;&#21644;&#24503;&#22269;&#19978;&#33713;&#33589;&#27827;&#35895;&#30340;&#22320;&#21306;&#24471;&#21040;&#20102;&#39564;&#35777;&#65292;&#24182;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#21487;&#38752;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;</title><link>https://arxiv.org/abs/2401.17342</link><description>&lt;p&gt;
&#25552;&#39640;&#22320;&#29699;&#35266;&#27979;&#25968;&#25454;&#39044;&#27979;&#32622;&#20449;&#24230;&#30340;&#28508;&#22312;&#31354;&#38388;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
A Latent Space Metric for Enhancing Prediction Confidence in Earth Observation Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17342
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#21033;&#29992;&#22320;&#29699;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#22238;&#24402;&#20219;&#21153;&#26102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;&#12290;&#36890;&#36807;&#21033;&#29992;&#28508;&#22312;&#31354;&#38388;&#34920;&#31034;&#26469;&#25512;&#23548;&#32622;&#20449;&#24230;&#24230;&#37327;&#65292;&#24314;&#31435;&#20102;&#28508;&#22312;&#34920;&#31034;&#20013;&#30340;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#19982;&#20010;&#20307;&#34442;&#23376;&#31181;&#32676;&#39044;&#27979;&#30340;&#32477;&#23545;&#35823;&#24046;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#24847;&#22823;&#21033;&#23041;&#23612;&#25176;&#22320;&#21306;&#21644;&#24503;&#22269;&#19978;&#33713;&#33589;&#27827;&#35895;&#30340;&#22320;&#21306;&#24471;&#21040;&#20102;&#39564;&#35777;&#65292;&#24182;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#21487;&#38752;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;&#65292;&#29305;&#21035;&#26159;&#22312;&#21033;&#29992;&#22320;&#29699;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#22238;&#24402;&#20219;&#21153;&#26102;&#65292;&#37325;&#28857;&#20851;&#27880;&#34442;&#23376;&#31181;&#32676;&#65288;MA&#65289;&#20272;&#35745;&#12290;&#25105;&#20204;&#21033;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;EO&#25968;&#25454;&#30340;&#28508;&#22312;&#31354;&#38388;&#34920;&#31034;&#26469;&#25512;&#23548;&#32622;&#20449;&#24230;&#24230;&#37327;&#12290;&#36825;&#31181;&#26041;&#27861;&#23545;&#20110;&#24314;&#31435;&#28508;&#22312;&#34920;&#31034;&#20013;&#30340;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#19982;&#20010;&#20307;MA&#39044;&#27979;&#30340;&#32477;&#23545;&#35823;&#24046;&#65288;AE&#65289;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#20851;&#27880;&#20102;&#24847;&#22823;&#21033;&#23041;&#23612;&#25176;&#22320;&#21306;&#21644;&#24503;&#22269;&#19978;&#33713;&#33589;&#27827;&#35895;&#30340;EO&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#22320;&#21306;&#21463;&#34442;&#23376;&#31181;&#32676;&#30340;&#24433;&#21709;&#26174;&#33879;&#12290;&#19968;&#20010;&#37325;&#35201;&#30340;&#21457;&#29616;&#26159;MA&#39044;&#27979;&#30340;AE&#19982;&#25152;&#25552;&#20986;&#30340;&#32622;&#20449;&#24230;&#24230;&#37327;&#20043;&#38388;&#23384;&#22312;0.46&#30340;&#26174;&#33879;&#30456;&#20851;&#24615;&#12290;&#36825;&#20010;&#30456;&#20851;&#24615;&#24847;&#21619;&#30528;&#36825;&#26159;&#19968;&#20010;&#31283;&#20581;&#30340;&#12289;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#37327;&#21270;AI&#27169;&#22411;&#22312;&#35813;&#32972;&#26223;&#19979;&#30340;&#39044;&#27979;&#21487;&#38752;&#24615;&#21644;&#25552;&#39640;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents a new approach for estimating confidence in machine learning model predictions, specifically in regression tasks utilizing Earth Observation (EO) data, with a particular focus on mosquito abundance (MA) estimation. We take advantage of a Variational AutoEncoder architecture, to derive a confidence metric by the latent space representations of EO datasets. This methodology is pivotal in establishing a correlation between the Euclidean distance in latent representations and the Absolute Error (AE) in individual MA predictions. Our research focuses on EO datasets from the Veneto region in Italy and the Upper Rhine Valley in Germany, targeting areas significantly affected by mosquito populations. A key finding is a notable correlation of 0.46 between the AE of MA predictions and the proposed confidence metric. This correlation signifies a robust, new metric for quantifying the reliability and enhancing the trustworthiness of the AI model's predictions in the context of 
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#32771;&#34385;&#20102;&#25439;&#22833;&#21644;&#19981;&#30830;&#23450;&#24615;&#22522;&#30784;&#30340;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#22312;&#32447;&#24615;&#20998;&#31867;&#22120;&#21644;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#38598;&#19978;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#24182;&#23637;&#31034;&#20102;&#20854;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2312.13927</link><description>&lt;p&gt;
&#20851;&#20110;&#25439;&#22833;&#21644;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the convergence of loss and uncertainty-based active learning algorithms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.13927
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#32771;&#34385;&#20102;&#25439;&#22833;&#21644;&#19981;&#30830;&#23450;&#24615;&#22522;&#30784;&#30340;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#22312;&#32447;&#24615;&#20998;&#31867;&#22120;&#21644;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#38598;&#19978;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#24182;&#23637;&#31034;&#20102;&#20854;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#22312;&#19981;&#21516;&#20551;&#35774;&#19979;&#25439;&#22833;&#21644;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#32452;&#26465;&#20214;&#65292;&#30830;&#20445;&#22312;&#24212;&#29992;&#20110;&#32447;&#24615;&#20998;&#31867;&#22120;&#21644;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#38598;&#26102;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#36825;&#21253;&#25324;&#35777;&#26126;&#21508;&#31181;&#25439;&#22833;&#20989;&#25968;&#30340;&#22522;&#20110;&#25439;&#22833;&#30340;&#37319;&#26679;&#30340;&#25910;&#25947;&#36895;&#24230;&#20445;&#35777;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#24050;&#30693;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#29575;&#30028;&#38480;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#23548;&#20986;&#25439;&#22833;&#37319;&#26679;&#30340;&#25910;&#25947;&#36895;&#29575;&#30028;&#38480;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#23558;&#28857;&#37319;&#26679;&#21644;&#38543;&#26426;Polyak&#27493;&#38271;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#20851;&#20110;&#37319;&#26679;&#36807;&#31243;&#30340;&#26465;&#20214;&#65292;&#30830;&#20445;&#35813;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#20445;&#35777;&#65292;&#29305;&#21035;&#26159;&#22312;&#20809;&#28369;&#20984;&#25439;&#22833;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#30340;&#25968;&#20540;&#32467;&#26524;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#31639;&#27861;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.13927v2 Announce Type: replace-cross  Abstract: We consider the convergence rates of loss and uncertainty-based active learning algorithms under various assumptions. Firstly, we establish a set of conditions that ensure convergence rates when applied to linear classifiers and linearly separable datasets. This includes demonstrating convergence rate guarantees for loss-based sampling with various loss functions. Secondly, we introduce a framework that allows us to derive convergence rate bounds for loss-based sampling by leveraging known convergence rate bounds for stochastic gradient descent algorithms. Lastly, we propose a new algorithm that combines point sampling and stochastic Polyak's step size. We establish a condition on the sampling process, ensuring a convergence rate guarantee for this algorithm, particularly in the case of smooth convex loss functions. Our numerical results showcase the efficiency of the proposed algorithm.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;diff&#21382;&#21490;&#30340;&#31616;&#21333;&#19988;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#23558;&#29615;&#22659;&#20013;&#30340;&#35266;&#27979;&#36716;&#25442;&#20026;&#25991;&#26412;&#25552;&#31034;&#65292;&#20197;&#20415;&#23545;&#20110;&#38271;&#26399;&#25512;&#29702;&#20915;&#31574;&#30340;&#20219;&#21153;&#20013;&#30340;Neural Language Models&#36827;&#34892;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2312.07540</link><description>&lt;p&gt;
Neural Language Agents&#30340;diff&#21382;&#21490;
&lt;/p&gt;
&lt;p&gt;
diff History for Neural Language Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.07540
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;diff&#21382;&#21490;&#30340;&#31616;&#21333;&#19988;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#23558;&#29615;&#22659;&#20013;&#30340;&#35266;&#27979;&#36716;&#25442;&#20026;&#25991;&#26412;&#25552;&#31034;&#65292;&#20197;&#20415;&#23545;&#20110;&#38271;&#26399;&#25512;&#29702;&#20915;&#31574;&#30340;&#20219;&#21153;&#20013;&#30340;Neural Language Models&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Neural Language Models (LMs)&#20026;&#36890;&#29992;&#30340;&#20855;&#20307;&#25511;&#21046;&#25552;&#20379;&#20102;&#20196;&#20154;&#20852;&#22859;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#24403;&#20351;&#29992;&#22522;&#20110;LM&#30340;&#25511;&#21046;&#22120;&#26102;&#65292;&#20250;&#20986;&#29616;&#19968;&#20010;&#20851;&#38190;&#30340;&#25216;&#26415;&#38382;&#39064;&#65306;&#29615;&#22659;&#35266;&#27979;&#24517;&#39035;&#36716;&#25442;&#20026;&#25991;&#26412;&#65292;&#36825;&#19982;&#21382;&#21490;&#32806;&#21512;&#22312;&#19968;&#36215;&#65292;&#23548;&#33268;&#20887;&#38271;&#32780;&#20887;&#20313;&#30340;&#25991;&#26412;&#25552;&#31034;&#12290;&#22240;&#27492;&#65292;LM&#20195;&#29702;&#30340;&#20808;&#21069;&#24037;&#20316;&#23616;&#38480;&#20110;&#20855;&#26377;&#23567;&#35266;&#27979;&#22823;&#23567;&#20197;&#21450;&#23545;&#20132;&#20114;&#21382;&#21490;&#25110;&#25351;&#31034;&#35843;&#20248;&#38656;&#27714;&#36739;&#23567;&#30340;&#38480;&#21046;&#39046;&#22495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;diff&#21382;&#21490;&#65292;&#36825;&#26159;&#19968;&#20010;&#31616;&#21333;&#19988;&#38750;&#24120;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#22312;&#29992;&#20110;&#25552;&#31034;LM&#31574;&#30053;&#30340;&#20132;&#20114;&#21382;&#21490;&#20013;&#30340;&#36830;&#32493;&#25991;&#26412;&#35266;&#27979;&#19978;&#24212;&#29992;Unix diff&#21629;&#20196;&#65292;&#25105;&#20204;&#26082;&#21487;&#20197;&#25688;&#38500;&#20887;&#20313;&#20449;&#24687;&#65292;&#21448;&#21487;&#20197;&#23558;&#25991;&#26412;&#36755;&#20837;&#30340;&#20869;&#23481;&#38598;&#20013;&#22312;&#29615;&#22659;&#20013;&#26174;&#33879;&#21464;&#21270;&#30340;&#26041;&#38754;&#12290;&#22312;&#38656;&#35201;&#38271;&#26399;&#25512;&#29702;&#36827;&#34892;&#20915;&#31574;&#30340;&#26410;&#35299;&#20915;&#30340;&#35270;&#39057;&#28216;&#25103;NetHack&#20013;&#65292;&#20351;&#29992;diff&#21382;&#21490;&#35843;&#20248;&#30340;LM&#19982;&#29366;&#24577;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.07540v2 Announce Type: replace Abstract: Neural Language Models (LMs) offer an exciting solution for general-purpose embodied control. However, a key technical issue arises when using an LM-based controller: environment observations must be converted to text, which coupled with history, results in long and verbose textual prompts. As a result, prior work in LM agents is limited to restricted domains with small observation size as well as minimal needs for interaction history or instruction tuning. In this paper, we introduce diff history, a simple and highly effective solution to these issues. By applying the Unix diff command on consecutive text observations in the interaction histories used to prompt LM policies, we can both abstract away redundant information and focus the content of textual inputs on the salient changes in the environment. On NetHack, an unsolved video game that requires long-horizon reasoning for decision-making, LMs tuned with diff history match state-
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#37096;&#20998;&#35266;&#27979;&#20013;&#36827;&#34892;&#26377;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#24102;&#26469;&#30340;&#35745;&#31639;&#21644;&#32479;&#35745;&#25361;&#25112;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#29616;&#20986;&#20248;&#20110;&#20808;&#36827;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2311.12244</link><description>&lt;p&gt;
&#39640;&#25928;&#24378;&#21270;&#23398;&#20064;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#19979;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Efficient Reinforcement Learning from Partial Observability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.12244
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#37096;&#20998;&#35266;&#27979;&#20013;&#36827;&#34892;&#26377;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#24102;&#26469;&#30340;&#35745;&#31639;&#21644;&#32479;&#35745;&#25361;&#25112;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#29616;&#20986;&#20248;&#20110;&#20808;&#36827;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22810;&#25968;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#29366;&#24577;&#20449;&#24687;&#21482;&#33021;&#37096;&#20998;&#35266;&#27979;&#21040;&#65292;&#36825;&#30772;&#22351;&#20102;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#20551;&#35774;&#65292;&#23548;&#33268;&#23558;&#35266;&#27979;&#19982;&#29366;&#24577;&#30456;&#28151;&#28102;&#30340;&#31639;&#27861;&#34920;&#29616;&#19981;&#20339;&#12290;&#32780;&#37096;&#20998;&#21487;&#35266;&#27979;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#20801;&#35768;&#22312;&#23398;&#20064;&#12289;&#25506;&#32034;&#21644;&#35268;&#21010;&#20013;&#32771;&#34385;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#20294;&#20063;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#35745;&#31639;&#21644;&#32479;&#35745;&#25361;&#25112;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#22256;&#38590;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#34920;&#31034;&#30340;&#35270;&#35282;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#21644;&#21487;&#34892;&#30340;&#31639;&#27861;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#37096;&#20998;&#35266;&#27979;&#20013;&#36827;&#34892;&#23454;&#38469;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#26469;&#35777;&#26126;&#25152;&#25552;&#20986;&#31639;&#27861;&#30340;&#32479;&#35745;&#25928;&#29575;&#65292;&#24182;&#32463;&#39564;&#24615;&#22320;&#35777;&#26126;&#20102;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#37096;&#20998;&#35266;&#27979;&#19979;&#33021;&#22815;&#36229;&#36234;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#25512;&#21160;&#20102;&#21487;&#38752;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In most real-world reinforcement learning applications, state information is only partially observable, which breaks the Markov decision process assumption and leads to inferior performance for algorithms that conflate observations with state. Partially Observable Markov Decision Processes (POMDPs), on the other hand, provide a general framework that allows for partial observability to be accounted for in learning, exploration and planning, but presents significant computational and statistical challenges. To address these difficulties, we develop a representation-based perspective that leads to a coherent framework and tractable algorithmic approach for practical reinforcement learning from partial observations. We provide a theoretical analysis for justifying the statistical efficiency of the proposed algorithm, and also empirically demonstrate the proposed algorithm can surpass state-of-the-art performance with partial observations across various benchmarks, advancing reliable reinf
&lt;/p&gt;</description></item><item><title>&#30446;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#38754;&#23545;&#21518;&#32493;&#38382;&#39064;&#26102;&#24120;&#24120;&#25671;&#25670;&#19981;&#23450;&#65292;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#21518;&#32493;&#38382;&#39064;&#26426;&#21046;&#21644;&#20004;&#20010;&#24230;&#37327;&#26631;&#20934;&#26469;&#37327;&#21270;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#24320;&#21457;&#20986;Unwavering-FQ&#26694;&#26550;&#26469;&#25945;&#23548;&#27169;&#22411;&#20445;&#25345;&#26368;&#21021;&#30340;&#27491;&#30830;&#21028;&#26029;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2310.02174</link><description>&lt;p&gt;
&#35753;&#24490;&#29615;&#30340;&#35810;&#38382;: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21028;&#26029;&#20013;&#30340;&#25671;&#25670;
&lt;/p&gt;
&lt;p&gt;
Ask Again, Then Fail: Large Language Models' Vacillations in Judgement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.02174
&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#38754;&#23545;&#21518;&#32493;&#38382;&#39064;&#26102;&#24120;&#24120;&#25671;&#25670;&#19981;&#23450;&#65292;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#21518;&#32493;&#38382;&#39064;&#26426;&#21046;&#21644;&#20004;&#20010;&#24230;&#37327;&#26631;&#20934;&#26469;&#37327;&#21270;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#24320;&#21457;&#20986;Unwavering-FQ&#26694;&#26550;&#26469;&#25945;&#23548;&#27169;&#22411;&#20445;&#25345;&#26368;&#21021;&#30340;&#27491;&#30830;&#21028;&#26029;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35266;&#23519;&#21040;&#30446;&#21069;&#30340;&#20250;&#35805;&#24335;&#35821;&#35328;&#27169;&#22411;&#22312;&#38754;&#23545;&#21518;&#32493;&#38382;&#39064;&#26102;&#24448;&#24448;&#22312;&#20854;&#21028;&#26029;&#19978;&#25671;&#25670;&#19981;&#23450;&#65292;&#21363;&#20351;&#21407;&#22987;&#21028;&#26029;&#26159;&#27491;&#30830;&#30340;&#12290;&#36825;&#31181;&#25671;&#25670;&#23545;&#20110;&#29983;&#25104;&#21487;&#38752;&#22238;&#22797;&#21644;&#24314;&#31435;&#29992;&#25143;&#20449;&#20219;&#26500;&#25104;&#20102;&#37325;&#35201;&#25361;&#25112;&#12290;&#20026;&#20102;&#20840;&#38754;&#35780;&#20272;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21518;&#32493;&#38382;&#39064;&#26426;&#21046;&#20197;&#21450;&#20004;&#20010;&#24230;&#37327;&#26631;&#20934;&#26469;&#37327;&#21270;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#65292;&#30830;&#35748;&#20102;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#26222;&#36941;&#23384;&#22312;&#36825;&#31181;&#24773;&#20917;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#21508;&#31181;&#25552;&#31034;&#31574;&#30053;&#29992;&#20110;&#38381;&#28304;&#27169;&#22411;&#65307;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#35757;&#32451;&#30340;&#26694;&#26550;Unwavering-FQ&#65292;&#36890;&#36807;&#21512;&#25104;&#39640;&#36136;&#37327;&#30340;&#20559;&#22909;&#25968;&#25454;&#26469;&#25945;&#23548;&#35821;&#35328;&#27169;&#22411;&#20445;&#25345;&#20854;&#26368;&#21021;&#30340;&#27491;&#30830;&#21028;&#26029;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#20854;&#22686;&#24378;&#27169;&#22411;&#36890;&#29992;&#33021;&#21147;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.02174v2 Announce Type: replace-cross  Abstract: We observe that current conversational language models often waver in their judgements when faced with follow-up questions, even if the original judgement was correct. This wavering presents a significant challenge for generating reliable responses and building user trust. To comprehensively assess this issue, we introduce a Follow-up Questioning Mechanism along with two metrics to quantify this inconsistency, confirming its widespread presence in current language models. To mitigate this issue, we explore various prompting strategies for closed-source models; moreover, we develop a training-based framework Unwavering-FQ that teaches language models to maintain their originally correct judgements through synthesized high-quality preference data. Our experimental results confirm the effectiveness of our framework and its ability to enhance the general capabilities of models (https://github.com/NUSTM/LLMs-Waver-In-Judgements).
&lt;/p&gt;</description></item><item><title>UniAP&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#33258;&#21160;&#24182;&#34892;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#28151;&#21512;&#25972;&#25968;&#20108;&#27425;&#35268;&#21010;&#32479;&#19968;&#36328;&#23618;&#21644;&#20869;&#23618;&#30340;&#33258;&#21160;&#24182;&#34892;&#21270;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;UniAP&#22312;&#21534;&#21520;&#37327;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#20943;&#23569;&#20102;&#31574;&#30053;&#20248;&#21270;&#26102;&#38388;&#12290;</title><link>https://arxiv.org/abs/2307.16375</link><description>&lt;p&gt;
UniAP: &#36890;&#36807;&#28151;&#21512;&#25972;&#25968;&#20108;&#27425;&#35268;&#21010;&#32479;&#19968;&#36328;&#23618;&#21644;&#20869;&#23618;&#33258;&#21160;&#24182;&#34892;&#21270;
&lt;/p&gt;
&lt;p&gt;
UniAP: Unifying Inter- and Intra-Layer Automatic Parallelism by Mixed Integer Quadratic Programming
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2307.16375
&lt;/p&gt;
&lt;p&gt;
UniAP&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#33258;&#21160;&#24182;&#34892;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#28151;&#21512;&#25972;&#25968;&#20108;&#27425;&#35268;&#21010;&#32479;&#19968;&#36328;&#23618;&#21644;&#20869;&#23618;&#30340;&#33258;&#21160;&#24182;&#34892;&#21270;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;UniAP&#22312;&#21534;&#21520;&#37327;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#20943;&#23569;&#20102;&#31574;&#30053;&#20248;&#21270;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#23398;&#20064;&#24120;&#29992;&#20110;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22823;&#22411;&#27169;&#22411;&#12290;&#22312;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#65292;&#25163;&#21160;&#24182;&#34892;&#21270;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#20154;&#21147;&#65292;&#24182;&#19988;&#28789;&#27963;&#24615;&#26377;&#38480;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#33258;&#21160;&#24182;&#34892;&#21270;&#26041;&#27861;&#26469;&#33258;&#21160;&#21270;&#24182;&#34892;&#31574;&#30053;&#20248;&#21270;&#36807;&#31243;&#12290;&#29616;&#26377;&#30340;&#33258;&#21160;&#24182;&#34892;&#21270;&#26041;&#27861;&#23384;&#22312;&#27425;&#20248;&#35299;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#19981;&#20250;&#21516;&#26102;&#20248;&#21270;&#36328;&#23618;&#24182;&#34892;&#21270;&#21644;&#20869;&#23618;&#24182;&#34892;&#21270;&#36825;&#20004;&#20010;&#31867;&#21035;&#30340;&#24182;&#34892;&#31574;&#30053;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;UniAP&#30340;&#26032;&#22411;&#33258;&#21160;&#24182;&#34892;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#28151;&#21512;&#25972;&#25968;&#20108;&#27425;&#35268;&#21010;&#32479;&#19968;&#36328;&#23618;&#21644;&#20869;&#23618;&#30340;&#33258;&#21160;&#24182;&#34892;&#21270;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;UniAP&#26159;&#31532;&#19968;&#31181;&#33021;&#22815;&#21516;&#26102;&#20248;&#21270;&#36825;&#20004;&#20010;&#31867;&#21035;&#30340;&#24182;&#34892;&#31574;&#30053;&#20197;&#27714;&#24471;&#26368;&#20248;&#35299;&#30340;&#24182;&#34892;&#21270;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;UniAP&#22312;&#21534;&#21520;&#37327;&#26041;&#38754;&#32988;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#26368;&#22810;1.71&#20493;&#65292;&#24182;&#20943;&#23569;&#20102;&#31574;&#30053;&#20248;&#21270;&#30340;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed learning is commonly used for training deep learning models, especially large models. In distributed learning, manual parallelism (MP) methods demand considerable human effort and have limited flexibility. Hence, automatic parallelism (AP) methods have recently been proposed for automating the parallel strategy optimization process. Existing AP methods suffer from sub-optimal solutions because they do not jointly optimize the two categories of parallel strategies (i.e., inter-layer parallelism and intra-layer parallelism). In this paper, we propose a novel AP method called UniAP, which unifies inter- and intra-layer automatic parallelism by mixed integer quadratic programming. To the best of our knowledge, UniAP is the first parallel method that can jointly optimize the two categories of parallel strategies to find an optimal solution. Experimental results show that UniAP outperforms state-of-the-art methods by up to 1.71$\times$ in throughput and reduces strategy optimizat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#22312;&#20809;&#28369;&#12289;&#24378;&#20984;&#29615;&#22659;&#20013;&#38543;&#26426;&#37325;&#21147;&#29699;&#21160;&#37327;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#24403;&#25209;&#37327;&#22823;&#23567;&#22823;&#20110;&#26576;&#20010;&#38408;&#20540;&#26102;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#21152;&#36895;&#25910;&#25947;&#36895;&#24230;&#12290;&#38024;&#23545;&#24378;&#20984;&#20108;&#27425;&#20989;&#25968;&#65292;&#25105;&#20204;&#24314;&#35758;&#20102;&#19968;&#31181;&#22122;&#22768;&#33258;&#36866;&#24212;&#30340;&#22810;&#38454;&#27573;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#25910;&#25947;&#36895;&#24230;&#36827;&#19968;&#27493;&#25552;&#39640;&#12290;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.06738</link><description>&lt;p&gt;
&#22122;&#22768;&#33258;&#36866;&#24212;&#65288;&#21152;&#36895;&#65289;&#38543;&#26426;&#37325;&#21147;&#29699;&#21160;&#37327;
&lt;/p&gt;
&lt;p&gt;
Noise-adaptive (Accelerated) Stochastic Heavy-Ball Momentum. (arXiv:2401.06738v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06738
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#22312;&#20809;&#28369;&#12289;&#24378;&#20984;&#29615;&#22659;&#20013;&#38543;&#26426;&#37325;&#21147;&#29699;&#21160;&#37327;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#24403;&#25209;&#37327;&#22823;&#23567;&#22823;&#20110;&#26576;&#20010;&#38408;&#20540;&#26102;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#21152;&#36895;&#25910;&#25947;&#36895;&#24230;&#12290;&#38024;&#23545;&#24378;&#20984;&#20108;&#27425;&#20989;&#25968;&#65292;&#25105;&#20204;&#24314;&#35758;&#20102;&#19968;&#31181;&#22122;&#22768;&#33258;&#36866;&#24212;&#30340;&#22810;&#38454;&#27573;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#25910;&#25947;&#36895;&#24230;&#36827;&#19968;&#27493;&#25552;&#39640;&#12290;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20998;&#26512;&#20102;&#22312;&#20809;&#28369;&#65292;&#24378;&#20984;&#29615;&#22659;&#20013;&#38543;&#26426;&#37325;&#21147;&#29699;&#21160;&#37327;&#65288;SHB&#65289;&#30340;&#25910;&#25947;&#24615;&#12290;Kidambi&#31561;&#20154;&#65288;2018&#65289;&#34920;&#26126;&#65292;&#23545;&#20110;&#20108;&#27425;&#20989;&#25968;&#65292;SHB&#65288;&#24102;&#26377;&#23567;&#25209;&#37327;&#65289;&#26080;&#27861;&#36798;&#21040;&#21152;&#36895;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#29468;&#24819;SHB&#30340;&#23454;&#38469;&#25910;&#30410;&#26159;&#23567;&#25209;&#37327;&#30340;&#21103;&#20135;&#21697;&#12290;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#24403;&#25209;&#37327;&#22823;&#23567;&#22823;&#20110;&#19968;&#23450;&#38408;&#20540;&#26102;&#65292;SHB&#21487;&#20197;&#33719;&#24471;&#21152;&#36895;&#30340;&#25910;&#25947;&#36895;&#24230;&#26469;&#35777;&#23454;&#36825;&#19968;&#35266;&#28857;&#12290;&#29305;&#21035;&#22320;&#65292;&#23545;&#20110;&#26465;&#20214;&#25968;&#20026;$\kappa$&#30340;&#24378;&#20984;&#20108;&#27425;&#20989;&#25968;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20351;&#29992;&#26631;&#20934;&#27493;&#38271;&#21644;&#21160;&#37327;&#21442;&#25968;&#30340;SHB&#20855;&#26377;$O\left(\exp(-\frac{T}{\sqrt{\kappa}}) + \sigma \right)$&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#20854;&#20013;$T$&#20026;&#36845;&#20195;&#27425;&#25968;&#65292;$\sigma^2$&#20026;&#38543;&#26426;&#26799;&#24230;&#30340;&#26041;&#24046;&#12290;&#20026;&#30830;&#20445;&#25910;&#25947;&#21040;&#26497;&#23567;&#20540;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#38454;&#27573;&#26041;&#27861;&#65292;&#32467;&#26524;&#26159;&#22122;&#22768;&#33258;&#36866;&#24212;&#30340;$O\left(\exp\left(-\frac{T}{\sqrt{\kappa}} \right) + \frac{\sigma}{T}\right)$&#36895;&#24230;&#12290;&#23545;&#20110;&#19968;&#33324;&#30340;&#24378;&#20984;&#20989;&#25968;&#65292;&#25105;&#20204;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We analyze the convergence of stochastic heavy ball (SHB) momentum in the smooth, strongly-convex setting. Kidambi et al. (2018) show that SHB (with small mini-batches) cannot attain an accelerated rate of convergence even for quadratics, and conjecture that the practical gain of SHB is a by-product of mini-batching. We substantiate this claim by showing that SHB can obtain an accelerated rate when the mini-batch size is larger than some threshold. In particular, for strongly-convex quadratics with condition number $\kappa$, we prove that SHB with the standard step-size and momentum parameters results in an $O\left(\exp(-\frac{T}{\sqrt{\kappa}}) + \sigma \right)$ convergence rate, where $T$ is the number of iterations and $\sigma^2$ is the variance in the stochastic gradients. To ensure convergence to the minimizer, we propose a multi-stage approach that results in a noise-adaptive $O\left(\exp\left(-\frac{T}{\sqrt{\kappa}} \right) + \frac{\sigma}{T}\right)$ rate. For general strongly-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30001;&#30149;&#29702;&#23398;&#23478;&#20026;&#30149;&#29702;&#23398;&#23478;&#26500;&#24314;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#36890;&#36807;&#25968;&#25454;&#25972;&#29702;&#21644;&#34701;&#20837;&#30149;&#29702;&#23398;&#39046;&#22495;&#30693;&#35782;&#65292;&#25193;&#23637;&#20102;&#25968;&#23383;&#30149;&#29702;&#23398;&#20840;&#29627;&#29255;&#22270;&#20687;&#22522;&#30784;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#22788;&#29702;&#32597;&#35265;&#30142;&#30149;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.04079</link><description>&lt;p&gt;
RudolfV&#65306;&#19968;&#31181;&#30001;&#30149;&#29702;&#23398;&#23478;&#20026;&#30149;&#29702;&#23398;&#23478;&#26500;&#24314;&#30340;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
RudolfV: A Foundation Model by Pathologists for Pathologists. (arXiv:2401.04079v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30001;&#30149;&#29702;&#23398;&#23478;&#20026;&#30149;&#29702;&#23398;&#23478;&#26500;&#24314;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#36890;&#36807;&#25968;&#25454;&#25972;&#29702;&#21644;&#34701;&#20837;&#30149;&#29702;&#23398;&#39046;&#22495;&#30693;&#35782;&#65292;&#25193;&#23637;&#20102;&#25968;&#23383;&#30149;&#29702;&#23398;&#20840;&#29627;&#29255;&#22270;&#20687;&#22522;&#30784;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#22788;&#29702;&#32597;&#35265;&#30142;&#30149;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#32455;&#30149;&#29702;&#23398;&#22312;&#20020;&#24202;&#21307;&#23398;&#21644;&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#20013;&#36215;&#30528;&#26680;&#24515;&#20316;&#29992;&#12290;&#34429;&#28982;&#20154;&#24037;&#26234;&#33021;&#22312;&#35768;&#22810;&#30149;&#29702;&#23398;&#20219;&#21153;&#19978;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#22312;&#27867;&#21270;&#21644;&#22788;&#29702;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#30340;&#32597;&#35265;&#30142;&#30149;&#26041;&#38754;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#22312;&#23398;&#20064;&#26469;&#33258;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#20043;&#21069;&#65292;&#20174;&#26080;&#26631;&#35760;&#25968;&#25454;&#20013;&#25552;&#21462;&#30693;&#35782;&#21040;&#22522;&#30784;&#27169;&#22411;&#21487;&#20197;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#21322;&#33258;&#21160;&#25968;&#25454;&#25972;&#29702;&#21644;&#34701;&#20837;&#30149;&#29702;&#23398;&#39046;&#22495;&#30693;&#35782;&#65292;&#25193;&#23637;&#20102;&#25968;&#23383;&#30149;&#29702;&#23398;&#20840;&#29627;&#29255;&#22270;&#20687;&#22522;&#30784;&#27169;&#22411;&#30340;&#26368;&#26032;&#25216;&#26415;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32467;&#21512;&#35745;&#31639;&#21644;&#30149;&#29702;&#23398;&#39046;&#22495;&#30693;&#35782;(1)&#25972;&#29702;&#20102;&#19968;&#20010;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;10.3&#19975;&#20010;&#29627;&#29255;&#22270;&#20687;&#23545;&#24212;&#30340;7.5&#20159;&#20010;&#22270;&#20687;&#22359;&#65292;&#28085;&#30422;&#20102;&#26469;&#33258;&#27431;&#32654;&#19981;&#21516;&#20462;&#22797;&#12289;&#26579;&#33394;&#21644;&#25195;&#25551;&#21327;&#35758;&#20197;&#21450;&#19981;&#21516;&#25351;&#31034;&#21644;&#23454;&#39564;&#23460;&#30340;&#25968;&#25454;&#65292;(2)&#29992;&#20110;&#23545;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#29627;&#29255;&#21644;&#32452;&#32455;&#22359;&#36827;&#34892;&#20998;&#32452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Histopathology plays a central role in clinical medicine and biomedical research. While artificial intelligence shows promising results on many pathological tasks, generalization and dealing with rare diseases, where training data is scarce, remains a challenge. Distilling knowledge from unlabeled data into a foundation model before learning from, potentially limited, labeled data provides a viable path to address these challenges. In this work, we extend the state of the art of foundation models for digital pathology whole slide images by semi-automated data curation and incorporating pathologist domain knowledge. Specifically, we combine computational and pathologist domain knowledge (1) to curate a diverse dataset of 103k slides corresponding to 750 million image patches covering data from different fixation, staining, and scanning protocols as well as data from different indications and labs across the EU and US, (2) for grouping semantically similar slides and tissue patches, and 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;PCA&#21644;&#20854;&#21464;&#31181;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22522;&#20110;&#32447;&#24615;&#23376;&#31354;&#38388;&#26071;&#24092;&#65292;&#24182;&#24341;&#20837;&#20102;&#23545;&#24322;&#24120;&#20540;&#21644;&#25968;&#25454;&#27969;&#24418;&#30340;&#32771;&#34385;&#12290;&#36890;&#36807;&#22312;&#26071;&#24092;&#27969;&#24418;&#19978;&#36827;&#34892;&#20248;&#21270;&#38382;&#39064;&#30340;&#27714;&#35299;&#65292;&#32467;&#21512;&#20027;&#27979;&#22320;&#32447;&#36817;&#20284;&#65292;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#26032;&#30340;&#38477;&#32500;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.04071</link><description>&lt;p&gt;
&#26071;&#24092;&#28216;&#25103;&#65306;&#36890;&#36807;&#26071;&#24092;&#27969;&#24418;&#26469;&#33719;&#24471;&#40065;&#26834;&#30340;&#20027;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Fun with Flags: Robust Principal Directions via Flag Manifolds. (arXiv:2401.04071v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;PCA&#21644;&#20854;&#21464;&#31181;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22522;&#20110;&#32447;&#24615;&#23376;&#31354;&#38388;&#26071;&#24092;&#65292;&#24182;&#24341;&#20837;&#20102;&#23545;&#24322;&#24120;&#20540;&#21644;&#25968;&#25454;&#27969;&#24418;&#30340;&#32771;&#34385;&#12290;&#36890;&#36807;&#22312;&#26071;&#24092;&#27969;&#24418;&#19978;&#36827;&#34892;&#20248;&#21270;&#38382;&#39064;&#30340;&#27714;&#35299;&#65292;&#32467;&#21512;&#20027;&#27979;&#22320;&#32447;&#36817;&#20284;&#65292;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#26032;&#30340;&#38477;&#32500;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#21450;&#20854;&#23545;&#27969;&#24418;&#21644;&#24322;&#24120;&#25968;&#25454;&#30340;&#25193;&#23637;&#65292;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#26159;&#19981;&#21487;&#25110;&#32570;&#30340;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;PCA&#21450;&#20854;&#21464;&#31181;&#30340;&#32479;&#19968;&#24418;&#24335;&#65292;&#24341;&#20837;&#20102;&#22522;&#20110;&#32447;&#24615;&#23376;&#31354;&#38388;&#26071;&#24092;&#30340;&#26694;&#26550;&#65292;&#21363;&#36880;&#28176;&#22686;&#21152;&#32500;&#24230;&#30340;&#23884;&#22871;&#32447;&#24615;&#23376;&#31354;&#38388;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#19981;&#20165;&#20801;&#35768;&#20849;&#21516;&#23454;&#29616;&#65292;&#36824;&#20135;&#29983;&#20102;&#26032;&#30340;&#26410;&#26366;&#25506;&#32034;&#30340;&#21464;&#31181;&#12290;&#25105;&#20204;&#20174;&#24191;&#20041;&#21270;&#20256;&#32479;&#30340;PCA&#26041;&#27861;&#24320;&#22987;&#65292;&#36825;&#20123;&#26041;&#27861;&#35201;&#20040;&#26368;&#22823;&#21270;&#26041;&#24046;&#65292;&#35201;&#20040;&#26368;&#23567;&#21270;&#37325;&#26500;&#35823;&#24046;&#12290;&#25105;&#20204;&#25193;&#23637;&#36825;&#20123;&#35299;&#37322;&#65292;&#36890;&#36807;&#32771;&#34385;&#24322;&#24120;&#20540;&#21644;&#25968;&#25454;&#27969;&#24418;&#65292;&#24320;&#21457;&#20986;&#20102;&#22823;&#37327;&#26032;&#30340;&#38477;&#32500;&#31639;&#27861;&#12290;&#20026;&#20102;&#35774;&#35745;&#19968;&#31181;&#36890;&#29992;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#25105;&#20204;&#23558;&#40065;&#26834;&#21644;&#23545;&#20598;&#24418;&#24335;&#30340;PCA&#37325;&#26032;&#26500;&#24314;&#20026;&#22312;&#26071;&#24092;&#27969;&#24418;&#19978;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#20027;&#27979;&#22320;&#32447;&#36817;&#20284;&#65288;&#20999;&#32447;PCA&#65289;&#25972;&#21512;&#21040;&#36825;&#20010;&#22522;&#20110;&#26071;&#24092;&#30340;&#26694;&#26550;&#20013;&#65292;&#21019;&#36896;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Principal component analysis (PCA), along with its extensions to manifolds and outlier contaminated data, have been indispensable in computer vision and machine learning. In this work, we present a unifying formalism for PCA and its variants, and introduce a framework based on the flags of linear subspaces, \ie a hierarchy of nested linear subspaces of increasing dimension, which not only allows for a common implementation but also yields novel variants, not explored previously. We begin by generalizing traditional PCA methods that either maximize variance or minimize reconstruction error. We expand these interpretations to develop a wide array of new dimensionality reduction algorithms by accounting for outliers and the data manifold. To devise a common computational approach, we recast robust and dual forms of PCA as optimization problems on flag manifolds. We then integrate tangent space approximations of principal geodesic analysis (tangent-PCA) into this flag-based framework, crea
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26680;&#25197;&#26354;&#20989;&#25968;&#23545;Mixup&#25968;&#25454;&#36827;&#34892;&#20010;&#24615;&#21270;&#22788;&#29702;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#25913;&#21464;&#25554;&#20540;&#31995;&#25968;&#30340;&#27010;&#29575;&#20998;&#24067;&#26469;&#23454;&#29616;&#26356;&#39057;&#32321;&#21644;&#26356;&#24378;&#28872;&#30340;&#28151;&#21512;&#30456;&#20284;&#25968;&#25454;&#28857;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#65292;&#36824;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01434</link><description>&lt;p&gt;
&#36890;&#36807;&#26680;&#25197;&#26354;&#20989;&#25968;&#23450;&#21046;Mixup&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Tailoring Mixup to Data using Kernel Warping functions. (arXiv:2311.01434v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01434
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26680;&#25197;&#26354;&#20989;&#25968;&#23545;Mixup&#25968;&#25454;&#36827;&#34892;&#20010;&#24615;&#21270;&#22788;&#29702;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#25913;&#21464;&#25554;&#20540;&#31995;&#25968;&#30340;&#27010;&#29575;&#20998;&#24067;&#26469;&#23454;&#29616;&#26356;&#39057;&#32321;&#21644;&#26356;&#24378;&#28872;&#30340;&#28151;&#21512;&#30456;&#20284;&#25968;&#25454;&#28857;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#65292;&#36824;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#26159;&#23398;&#20064;&#39640;&#25928;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#37325;&#35201;&#22522;&#30784;&#12290;&#22312;&#25152;&#26377;&#25552;&#20986;&#30340;&#22686;&#24378;&#25216;&#26415;&#20013;&#65292;&#32447;&#24615;&#25554;&#20540;&#35757;&#32451;&#25968;&#25454;&#28857;&#65288;&#20063;&#31216;&#20026;Mixup&#65289;&#24050;&#34987;&#35777;&#26126;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#36873;&#25321;&#21512;&#36866;&#30340;&#28857;&#36827;&#34892;&#28151;&#21512;&#65292;&#25110;&#32773;&#24212;&#29992;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#25554;&#20540;&#65292;&#32780;&#25105;&#20204;&#21017;&#23545;&#26356;&#30456;&#20284;&#30340;&#28857;&#36827;&#34892;&#26356;&#39057;&#32321;&#21644;&#26356;&#24378;&#28872;&#30340;&#28151;&#21512;&#24863;&#20852;&#36259;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#25197;&#26354;&#20989;&#25968;&#21160;&#24577;&#25913;&#21464;&#25554;&#20540;&#31995;&#25968;&#30340;&#27010;&#29575;&#20998;&#24067;&#30340;&#26041;&#27861;&#65292;&#21462;&#20915;&#20110;&#35201;&#32452;&#21512;&#30340;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#39640;&#25928;&#32780;&#28789;&#27963;&#30340;&#26694;&#26550;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#20197;&#36991;&#20813;&#22810;&#26679;&#24615;&#30340;&#25439;&#22833;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#26082;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21448;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/ENSTA-U2IS/torch-uncertainty&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data augmentation is an essential building block for learning efficient deep learning models. Among all augmentation techniques proposed so far, linear interpolation of training data points, also called mixup, has found to be effective for a large panel of applications. While the majority of works have focused on selecting the right points to mix, or applying complex non-linear interpolation, we are interested in mixing similar points more frequently and strongly than less similar ones. To this end, we propose to dynamically change the underlying distribution of interpolation coefficients through warping functions, depending on the similarity between data points to combine. We define an efficient and flexible framework to do so without losing in diversity. We provide extensive experiments for classification and regression tasks, showing that our proposed method improves both performance and calibration of models. Code available in https://github.com/ENSTA-U2IS/torch-uncertainty
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#30340;&#30446;&#30340;&#26159;&#21033;&#29992;&#27491;&#20132;&#21270;&#26041;&#27861;&#28040;&#38500;&#33016;&#37096;X&#23556;&#32447;&#23884;&#20837;&#20013;&#30340;&#20445;&#25252;&#29305;&#24449;&#24433;&#21709;&#65292;&#24182;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#20445;&#25252;&#29305;&#24449;&#23545;&#30149;&#29702;&#39044;&#27979;&#26377;&#26174;&#33879;&#24433;&#21709;&#65292;&#32780;&#24212;&#29992;&#27491;&#20132;&#21270;&#26041;&#27861;&#21487;&#20197;&#28040;&#38500;&#36825;&#20123;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2311.01349</link><description>&lt;p&gt;
&#21462;&#28040;&#20445;&#25252;&#29305;&#24449;&#65306;&#20174;&#33016;&#37096;X&#23556;&#32447;&#23884;&#20837;&#20013;&#28040;&#38500;&#20445;&#25252;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Unreading Race: Purging Protected Features from Chest X-ray Embeddings. (arXiv:2311.01349v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01349
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#30340;&#30446;&#30340;&#26159;&#21033;&#29992;&#27491;&#20132;&#21270;&#26041;&#27861;&#28040;&#38500;&#33016;&#37096;X&#23556;&#32447;&#23884;&#20837;&#20013;&#30340;&#20445;&#25252;&#29305;&#24449;&#24433;&#21709;&#65292;&#24182;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#20445;&#25252;&#29305;&#24449;&#23545;&#30149;&#29702;&#39044;&#27979;&#26377;&#26174;&#33879;&#24433;&#21709;&#65292;&#32780;&#24212;&#29992;&#27491;&#20132;&#21270;&#26041;&#27861;&#21487;&#20197;&#28040;&#38500;&#36825;&#20123;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#20998;&#26512;&#24182;&#28040;&#38500;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#33016;&#37096;X&#23556;&#32447;&#23884;&#20837;&#30340;&#20445;&#25252;&#29305;&#24449;&#24433;&#21709;&#12290;&#26041;&#27861;&#65306;&#20351;&#29992;&#27491;&#20132;&#21270;&#26041;&#27861;&#28040;&#38500;&#33016;&#37096;X&#23556;&#32447;&#23884;&#20837;&#20013;&#30340;&#20445;&#25252;&#29305;&#24449;&#65288;&#22914;&#24180;&#40836;&#12289;&#24615;&#21035;&#12289;&#31181;&#26063;&#65289;&#30340;&#24433;&#21709;&#65292;&#30830;&#20445;&#29305;&#24449;&#29420;&#31435;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#39564;&#35777;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#19977;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;&#26377;&#30417;&#30563;&#23545;&#27604;&#12289;&#33258;&#30417;&#30563;&#23545;&#27604;&#21644;&#22522;&#32447;&#20998;&#31867;&#22120;&#27169;&#22411;&#65289;&#23545;MIMIC&#21644;CheXpert&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#22238;&#39038;&#24615;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#32479;&#35745;&#20998;&#26512;&#28041;&#21450;&#36890;&#36807;&#20272;&#35745;&#20445;&#25252;&#29305;&#24449;&#24433;&#21709;&#21644;&#35780;&#20272;&#20351;&#29992;&#20004;&#31181;&#31867;&#22411;&#23884;&#20837;&#30340;&#33021;&#21147;&#26469;&#39044;&#27979;&#31181;&#26063;&#12289;&#24180;&#40836;&#25110;&#24615;&#21035;&#30340;&#21407;&#22987;&#19982;&#27491;&#20132;&#23884;&#20837;&#30340;&#27604;&#36739;&#12290;&#32467;&#26524;&#65306;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#20445;&#25252;&#29305;&#24449;&#23545;&#30149;&#29702;&#39044;&#27979;&#30340;&#26174;&#30528;&#24433;&#21709;&#12290;&#24212;&#29992;&#27491;&#20132;&#21270;&#26041;&#27861;&#21487;&#20197;&#28040;&#38500;&#36825;&#20123;&#29305;&#24449;&#24433;&#21709;&#12290;&#38500;&#20102;&#28040;&#38500;&#23545;&#30149;&#29702;&#20998;&#31867;&#30340;&#24433;&#21709;&#20043;&#22806;&#65292;
&lt;/p&gt;
&lt;p&gt;
Purpose: To analyze and remove protected feature effects in chest radiograph embeddings of deep learning models.  Materials and Methods: An orthogonalization is utilized to remove the influence of protected features (e.g., age, sex, race) in chest radiograph embeddings, ensuring feature-independent results. To validate the efficacy of the approach, we retrospectively study the MIMIC and CheXpert datasets using three pre-trained models, namely a supervised contrastive, a self-supervised contrastive, and a baseline classifier model. Our statistical analysis involves comparing the original versus the orthogonalized embeddings by estimating protected feature influences and evaluating the ability to predict race, age, or sex using the two types of embeddings.  Results: Our experiments reveal a significant influence of protected features on predictions of pathologies. Applying orthogonalization removes these feature effects. Apart from removing any influence on pathology classification, whil
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22312;&#29983;&#25104;&#24615;&#25193;&#25955;&#27169;&#22411;&#20013;&#24212;&#29992;&#24179;&#34913;&#32479;&#35745;&#21147;&#23398;&#30340;&#24037;&#20855;&#65292;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#20013;&#30340;&#20108;&#38454;&#30456;&#21464;&#29616;&#35937;&#65292;&#24182;&#19988;&#35748;&#20026;&#36825;&#31181;&#31283;&#23450;&#24615;&#24418;&#24335;&#26159;&#29983;&#25104;&#33021;&#21147;&#30340;&#20851;&#38190;&#12290;</title><link>http://arxiv.org/abs/2310.17467</link><description>&lt;p&gt;
&#29983;&#25104;&#24615;&#25193;&#25955;&#27169;&#22411;&#30340;&#32479;&#35745;&#28909;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
The statistical thermodynamics of generative diffusion models. (arXiv:2310.17467v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17467
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22312;&#29983;&#25104;&#24615;&#25193;&#25955;&#27169;&#22411;&#20013;&#24212;&#29992;&#24179;&#34913;&#32479;&#35745;&#21147;&#23398;&#30340;&#24037;&#20855;&#65292;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#20013;&#30340;&#20108;&#38454;&#30456;&#21464;&#29616;&#35937;&#65292;&#24182;&#19988;&#35748;&#20026;&#36825;&#31181;&#31283;&#23450;&#24615;&#24418;&#24335;&#26159;&#29983;&#25104;&#33021;&#21147;&#30340;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24615;&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#24314;&#27169;&#30340;&#35768;&#22810;&#39046;&#22495;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#34920;&#29616;&#12290;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#30340;&#22522;&#26412;&#24605;&#24819;&#26469;&#33258;&#38750;&#24179;&#34913;&#29289;&#29702;&#23398;&#65292;&#20294;&#26412;&#25991;&#20013;&#25105;&#20204;&#34920;&#26126;&#65292;&#21487;&#20197;&#29992;&#24179;&#34913;&#32479;&#35745;&#21147;&#23398;&#30340;&#24037;&#20855;&#26469;&#29702;&#35299;&#36825;&#20123;&#27169;&#22411;&#30340;&#35768;&#22810;&#26041;&#38754;&#12290;&#21033;&#29992;&#36825;&#31181;&#37325;&#26500;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#29983;&#25104;&#24615;&#25193;&#25955;&#27169;&#22411;&#32463;&#21382;&#20102;&#19982;&#23545;&#31216;&#24615;&#30772;&#32570;&#29616;&#35937;&#30456;&#23545;&#24212;&#30340;&#20108;&#38454;&#30456;&#21464;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#23548;&#33268;&#20102;&#19968;&#31181;&#31283;&#23450;&#24615;&#24418;&#24335;&#65292;&#23427;&#26159;&#29983;&#25104;&#33021;&#21147;&#30340;&#26680;&#24515;&#65292;&#24182;&#21487;&#20197;&#29992;&#19968;&#32452;&#24179;&#22343;&#22330;&#20020;&#30028;&#25351;&#25968;&#26469;&#25551;&#36848;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#26681;&#25454;&#28909;&#21147;&#23398;&#30340;&#20844;&#24335;&#20998;&#26512;&#20102;&#23558;&#25193;&#25955;&#27169;&#22411;&#19982;&#20851;&#32852;&#35760;&#24518;&#32593;&#32476;&#36830;&#25509;&#30340;&#26368;&#36817;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative diffusion models have achieved spectacular performance in many areas of generative modeling. While the fundamental ideas behind these models come from non-equilibrium physics, in this paper we show that many aspects of these models can be understood using the tools of equilibrium statistical mechanics. Using this reformulation, we show that generative diffusion models undergo second-order phase transitions corresponding to symmetry breaking phenomena. We argue that this lead to a form of instability that lies at the heart of their generative capabilities and that can be described by a set of mean field critical exponents. We conclude by analyzing recent work connecting diffusion models and associative memory networks in view of the thermodynamic formulations.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#21512;&#24182;&#20026;&#19968;&#20010;&#32479;&#19968;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#22810;&#20219;&#21153;&#23398;&#20064;&#12289;&#25345;&#32493;&#23398;&#20064;&#25216;&#26415;&#21644;&#24072;&#29983;&#33976;&#39311;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#36739;&#23569;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#36739;&#23569;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38656;&#27714;&#12290;&#36890;&#36807;&#24212;&#29992;&#35813;&#26041;&#27861;&#20110;SAM&#21644;CLIP&#65292;&#24471;&#21040;&#20102;&#19968;&#20010;&#32479;&#19968;&#27169;&#22411;SAM-CLIP&#65292;&#23558;&#20004;&#32773;&#30340;&#20248;&#21183;&#34701;&#21512;&#22312;&#19968;&#36215;&#12290;</title><link>http://arxiv.org/abs/2310.15308</link><description>&lt;p&gt;
SAM-CLIP: &#23558;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#21512;&#24182;&#20026;&#35821;&#20041;&#21644;&#31354;&#38388;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
SAM-CLIP: Merging Vision Foundation Models towards Semantic and Spatial Understanding. (arXiv:2310.15308v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15308
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#21512;&#24182;&#20026;&#19968;&#20010;&#32479;&#19968;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#22810;&#20219;&#21153;&#23398;&#20064;&#12289;&#25345;&#32493;&#23398;&#20064;&#25216;&#26415;&#21644;&#24072;&#29983;&#33976;&#39311;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#36739;&#23569;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#36739;&#23569;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38656;&#27714;&#12290;&#36890;&#36807;&#24212;&#29992;&#35813;&#26041;&#27861;&#20110;SAM&#21644;CLIP&#65292;&#24471;&#21040;&#20102;&#19968;&#20010;&#32479;&#19968;&#27169;&#22411;SAM-CLIP&#65292;&#23558;&#20004;&#32773;&#30340;&#20248;&#21183;&#34701;&#21512;&#22312;&#19968;&#36215;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#24320;&#21487;&#29992;&#30340;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#65288;VFMs&#65289;&#30340;&#39046;&#22495;&#65292;&#22914;CLIP&#21644;Segment Anything Model&#65288;SAM&#65289;&#65292;&#27491;&#22312;&#36805;&#36895;&#25193;&#22823;&#12290;VFMs&#20855;&#26377;&#28304;&#33258;&#23427;&#20204;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#30340;&#19981;&#21516;&#33021;&#21147;&#12290;&#20363;&#22914;&#65292;CLIP&#22312;&#35821;&#20041;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;SAM&#19987;&#27880;&#20110;&#20998;&#21106;&#30340;&#31354;&#38388;&#29702;&#35299;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;VFMs&#39640;&#25928;&#21512;&#24182;&#20026;&#19968;&#20010;&#32479;&#19968;&#27169;&#22411;&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#20197;&#21560;&#25910;&#23427;&#20204;&#30340;&#19987;&#19994;&#30693;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#38598;&#25104;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#12289;&#25345;&#32493;&#23398;&#20064;&#25216;&#26415;&#21644;&#24072;&#29983;&#33976;&#39311;&#12290;&#19982;&#20256;&#32479;&#30340;&#20174;&#22836;&#24320;&#22987;&#36827;&#34892;&#22810;&#20219;&#21153;&#35757;&#32451;&#30456;&#27604;&#65292;&#36825;&#31181;&#31574;&#30053;&#20855;&#26377;&#26174;&#33879;&#36739;&#23569;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;&#23427;&#21482;&#38656;&#35201;&#26368;&#21021;&#29992;&#20110;&#35757;&#32451;&#21333;&#20010;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;&#36890;&#36807;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;SAM&#21644;CLIP&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;SAM-CLIP&#65306;&#23558;SAM&#21644;CLIP&#30340;&#20248;&#21183;&#34701;&#21512;&#20026;&#21333;&#19968;&#20027;&#24178;&#30340;&#32479;&#19968;&#27169;&#22411;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;...
&lt;/p&gt;
&lt;p&gt;
The landscape of publicly available vision foundation models (VFMs), such as CLIP and Segment Anything Model (SAM), is expanding rapidly. VFMs are endowed with distinct capabilities stemming from their pre-training objectives. For instance, CLIP excels in semantic understanding, while SAM specializes in spatial understanding for segmentation. In this work, we introduce a simple recipe to efficiently merge VFMs into a unified model that assimilates their expertise. Our proposed method integrates multi-task learning, continual learning techniques, and teacher-student distillation. This strategy entails significantly less computational cost compared to traditional multi-task training from scratch. Additionally, it only demands a small fraction of the pre-training datasets that were initially used to train individual models. By applying our method to SAM and CLIP, we derive SAM-CLIP: a unified model that amalgamates the strengths of SAM and CLIP into a single backbone, making it apt for ed
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#19978;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#30340;&#22522;&#26412;&#38480;&#21046;&#65292;&#21253;&#25324;&#25512;&#23548;&#20102;&#25928;&#26524;&#21644;&#25104;&#21151;&#29575;&#30340;&#32479;&#35745;&#37327;&#65292;&#24182;&#25552;&#20379;&#20102;&#20960;&#31181;&#24773;&#20917;&#19979;&#30340;&#30028;&#38480;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#26681;&#25454;&#26679;&#26412;&#25968;&#37327;&#21644;&#20854;&#20182;&#32467;&#26500;&#21442;&#25968;&#25512;&#26029;&#28508;&#22312;&#25915;&#20987;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.13786</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#30340;&#22522;&#26412;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Fundamental Limits of Membership Inference Attacks on Machine Learning Models. (arXiv:2310.13786v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13786
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#19978;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#30340;&#22522;&#26412;&#38480;&#21046;&#65292;&#21253;&#25324;&#25512;&#23548;&#20102;&#25928;&#26524;&#21644;&#25104;&#21151;&#29575;&#30340;&#32479;&#35745;&#37327;&#65292;&#24182;&#25552;&#20379;&#20102;&#20960;&#31181;&#24773;&#20917;&#19979;&#30340;&#30028;&#38480;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#26681;&#25454;&#26679;&#26412;&#25968;&#37327;&#21644;&#20854;&#20182;&#32467;&#26500;&#21442;&#25968;&#25512;&#26029;&#28508;&#22312;&#25915;&#20987;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#65288;MIA&#65289;&#21487;&#20197;&#25581;&#31034;&#29305;&#23450;&#25968;&#25454;&#28857;&#26159;&#21542;&#26159;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#19968;&#37096;&#20998;&#65292;&#21487;&#33021;&#26292;&#38706;&#20010;&#20154;&#30340;&#25935;&#24863;&#20449;&#24687;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20851;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#19978;MIA&#30340;&#22522;&#26412;&#32479;&#35745;&#38480;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25512;&#23548;&#20102;&#32479;&#35745;&#37327;&#65292;&#35813;&#32479;&#35745;&#37327;&#20915;&#23450;&#20102;&#36825;&#31181;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#21644;&#25104;&#21151;&#29575;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20960;&#31181;&#24773;&#20917;&#65292;&#24182;&#23545;&#36825;&#20010;&#24863;&#20852;&#36259;&#30340;&#32479;&#35745;&#37327;&#25552;&#20379;&#20102;&#30028;&#38480;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#26681;&#25454;&#26679;&#26412;&#25968;&#37327;&#21644;&#23398;&#20064;&#27169;&#22411;&#30340;&#20854;&#20182;&#32467;&#26500;&#21442;&#25968;&#25512;&#26029;&#28508;&#22312;&#25915;&#20987;&#30340;&#20934;&#30830;&#24615;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#20197;&#30452;&#25509;&#20174;&#25968;&#25454;&#38598;&#20013;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Membership inference attacks (MIA) can reveal whether a particular data point was part of the training dataset, potentially exposing sensitive information about individuals. This article explores the fundamental statistical limitations associated with MIAs on machine learning models. More precisely, we first derive the statistical quantity that governs the effectiveness and success of such attacks. Then, we investigate several situations for which we provide bounds on this quantity of interest. This allows us to infer the accuracy of potential attacks as a function of the number of samples and other structural parameters of learning models, which in some cases can be directly estimated from the dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#24322;&#26500;&#25968;&#25454;&#20998;&#24067;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#20219;&#21153;&#20013;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#12290;&#36890;&#36807;&#30740;&#31350;Fr\'echet inception&#36317;&#31163;&#65288;FID&#65289;&#65292;&#24182;&#32771;&#34385;&#19981;&#21516;&#32858;&#21512;&#20998;&#25968;&#65292;&#21457;&#29616;FID-all&#21644;FID-avg&#20998;&#25968;&#30340;&#27169;&#22411;&#25490;&#21517;&#21487;&#33021;&#19981;&#19968;&#33268;&#12290;</title><link>http://arxiv.org/abs/2310.11714</link><description>&lt;p&gt;
&#22312;&#20998;&#24067;&#24335;&#23398;&#20064;&#20219;&#21153;&#20013;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
On the Evaluation of Generative Models in Distributed Learning Tasks. (arXiv:2310.11714v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11714
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#24322;&#26500;&#25968;&#25454;&#20998;&#24067;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#20219;&#21153;&#20013;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#12290;&#36890;&#36807;&#30740;&#31350;Fr\'echet inception&#36317;&#31163;&#65288;FID&#65289;&#65292;&#24182;&#32771;&#34385;&#19981;&#21516;&#32858;&#21512;&#20998;&#25968;&#65292;&#21457;&#29616;FID-all&#21644;FID-avg&#20998;&#25968;&#30340;&#27169;&#22411;&#25490;&#21517;&#21487;&#33021;&#19981;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25991;&#29486;&#20013;&#24050;&#32463;&#24191;&#27867;&#30740;&#31350;&#20102;&#23545;&#21253;&#25324;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#21644;&#25193;&#25955;&#27169;&#22411;&#22312;&#20869;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35780;&#20272;&#26041;&#27861;&#20027;&#35201;&#38024;&#23545;&#21333;&#20010;&#23458;&#25143;&#31471;&#23384;&#20648;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#38598;&#20013;&#24335;&#23398;&#20064;&#38382;&#39064;&#65292;&#32780;&#29983;&#25104;&#27169;&#22411;&#30340;&#35768;&#22810;&#24212;&#29992;&#28041;&#21450;&#21040;&#20998;&#24067;&#24335;&#23398;&#20064;&#29615;&#22659;&#65292;&#20363;&#22914;&#32852;&#37030;&#23398;&#20064;&#22330;&#26223;&#65292;&#20854;&#20013;&#35757;&#32451;&#25968;&#25454;&#30001;&#22810;&#20010;&#23458;&#25143;&#31471;&#25910;&#38598;&#24182;&#20998;&#21457;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#24322;&#26500;&#25968;&#25454;&#20998;&#24067;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#20219;&#21153;&#20013;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20851;&#27880;Fr\'echet inception&#36317;&#31163;&#65288;FID&#65289;&#65292;&#24182;&#32771;&#34385;&#20197;&#19979;&#22522;&#20110;FID&#30340;&#32858;&#21512;&#20998;&#25968;&#65306;1&#65289;FID-avg&#20316;&#20026;&#23458;&#25143;&#31471;&#20010;&#20307;FID&#20998;&#25968;&#30340;&#24179;&#22343;&#20540;&#65292;2&#65289;FID-all&#20316;&#20026;&#35757;&#32451;&#27169;&#22411;&#19982;&#21253;&#21547;&#25152;&#26377;&#23458;&#25143;&#31471;&#25968;&#25454;&#30340;&#38598;&#20307;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;FID&#36317;&#31163;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#26681;&#25454;FID-all&#21644;FID-avg&#20998;&#25968;&#30340;&#27169;&#22411;&#25490;&#21517;&#21487;&#33021;&#19981;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
The evaluation of deep generative models including generative adversarial networks (GANs) and diffusion models has been extensively studied in the literature. While the existing evaluation methods mainly target a centralized learning problem with training data stored by a single client, many applications of generative models concern distributed learning settings, e.g. the federated learning scenario, where training data are collected by and distributed among several clients. In this paper, we study the evaluation of generative models in distributed learning tasks with heterogeneous data distributions. First, we focus on the Fr\'echet inception distance (FID) and consider the following FID-based aggregate scores over the clients: 1) FID-avg as the mean of clients' individual FID scores, 2) FID-all as the FID distance of the trained model to the collective dataset containing all clients' data. We prove that the model rankings according to the FID-all and FID-avg scores could be inconsist
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#24615;&#33021;&#24314;&#27169;&#26694;&#26550;&#65292;&#22312;&#20998;&#24067;&#24335;&#31995;&#32479;&#19978;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21152;&#36895;&#65292;&#33719;&#24471;&#20102;2.24&#20493;&#21644;5.27&#20493;&#30340;&#21534;&#21520;&#37327;&#25552;&#21319;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.02784</link><description>&lt;p&gt;
&#36229;&#36234;&#21333;&#33410;&#28857;&#65306;&#22312;&#20998;&#24067;&#24335;&#31995;&#32479;&#19978;&#23454;&#29616;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21152;&#36895;
&lt;/p&gt;
&lt;p&gt;
MAD Max Beyond Single-Node: Enabling Large Machine Learning Model Acceleration on Distributed Systems. (arXiv:2310.02784v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02784
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#24615;&#33021;&#24314;&#27169;&#26694;&#26550;&#65292;&#22312;&#20998;&#24067;&#24335;&#31995;&#32479;&#19978;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21152;&#36895;&#65292;&#33719;&#24471;&#20102;2.24&#20493;&#21644;5.27&#20493;&#30340;&#21534;&#21520;&#37327;&#25552;&#21319;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#21644;&#37096;&#32626;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#26159;&#32791;&#26102;&#19988;&#38656;&#35201;&#22823;&#37327;&#20998;&#24067;&#24335;&#35745;&#31639;&#22522;&#30784;&#35774;&#26045;&#12290;&#26681;&#25454;&#23454;&#38469;&#24773;&#20917;&#22312;&#25968;&#25454;&#20013;&#24515;&#35268;&#27169;&#22522;&#30784;&#35774;&#26045;&#19978;&#36827;&#34892;&#22823;&#27169;&#22411;&#35757;&#32451;&#65292;&#25105;&#20204;&#21457;&#29616;14~32%&#30340;GPU&#23567;&#26102;&#29992;&#20110;&#36890;&#20449;&#65292;&#27809;&#26377;&#37325;&#21472;&#35745;&#31639;&#12290;&#20026;&#20102;&#23613;&#37327;&#20943;&#23569;&#31561;&#24453;&#36890;&#20449;&#24310;&#36831;&#65292;&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#24615;&#33021;&#24314;&#27169;&#26694;&#26550;&#65292;&#25351;&#23548;&#24182;&#34892;&#21270;&#21644;&#30828;&#20214;&#36719;&#20214;&#20849;&#21516;&#35774;&#35745;&#31574;&#30053;&#12290;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;GPU&#35757;&#32451;&#30828;&#20214;&#19978;&#30340;&#19968;&#22871;&#23454;&#38469;&#22823;&#35268;&#27169;ML&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#39044;&#35757;&#32451;&#21644;&#25512;&#26029;&#22330;&#26223;&#20998;&#21035;&#21487;&#20197;&#25552;&#39640;2.24&#20493;&#21644;5.27&#20493;&#30340;&#21534;&#21520;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training and deploying large machine learning (ML) models is time-consuming and requires significant distributed computing infrastructures. Based on real-world large model training on datacenter-scale infrastructures, we show 14~32% of all GPU hours are spent on communication with no overlapping computation. To minimize the outstanding communication latency, in this work, we develop an agile performance modeling framework to guide parallelization and hardware-software co-design strategies. Using the suite of real-world large ML models on state-of-the-art GPU training hardware, we demonstrate 2.24x and 5.27x throughput improvement potential for pre-training and inference scenarios, respectively.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#35760;&#24518;&#20581;&#36523;&#25151;&#65292;&#19968;&#31181;&#29992;&#20110;&#27979;&#35797;&#21033;&#29992;&#35760;&#24518;&#20026;&#22522;&#30784;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#33021;&#21147;&#30340;&#22522;&#20934;&#12290;&#23427;&#21253;&#25324;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#20108;&#32500;&#29615;&#22659;&#21644;&#31163;&#25955;&#25511;&#21046;&#65292;&#24182;&#36890;&#36807;&#26080;&#23613;&#20219;&#21153;&#23545;&#35760;&#24518;&#33021;&#21147;&#12289;&#22122;&#22768;&#25239;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#36827;&#34892;&#35780;&#20272;&#12290;&#30740;&#31350;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#20351;&#29992;Transformer-XL&#21644;Proximal Policy Optimization&#39537;&#21160;&#30340;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2309.17207</link><description>&lt;p&gt;
&#35760;&#24518;&#20581;&#36523;&#25151;&#65306;&#23545;&#20869;&#23384;&#20026;&#22522;&#30784;&#30340;&#26234;&#33021;&#20307;&#22312;&#26080;&#23613;&#20219;&#21153;&#20013;&#30340;&#37096;&#20998;&#21487;&#35266;&#23519;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Memory Gym: Partially Observable Challenges to Memory-Based Agents in Endless Episodes. (arXiv:2309.17207v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#35760;&#24518;&#20581;&#36523;&#25151;&#65292;&#19968;&#31181;&#29992;&#20110;&#27979;&#35797;&#21033;&#29992;&#35760;&#24518;&#20026;&#22522;&#30784;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#33021;&#21147;&#30340;&#22522;&#20934;&#12290;&#23427;&#21253;&#25324;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#20108;&#32500;&#29615;&#22659;&#21644;&#31163;&#25955;&#25511;&#21046;&#65292;&#24182;&#36890;&#36807;&#26080;&#23613;&#20219;&#21153;&#23545;&#35760;&#24518;&#33021;&#21147;&#12289;&#22122;&#22768;&#25239;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#36827;&#34892;&#35780;&#20272;&#12290;&#30740;&#31350;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#20351;&#29992;Transformer-XL&#21644;Proximal Policy Optimization&#39537;&#21160;&#30340;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35760;&#24518;&#20581;&#36523;&#25151;&#20171;&#32461;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#27979;&#35797;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#65292;&#29305;&#21035;&#26159;&#23558;&#38376;&#24490;&#29615;&#21333;&#20803;(GRU)&#19982;Transformer-XL(TrXL)&#30456;&#27604;&#65292;&#23427;&#20204;&#23545;&#20110;&#35760;&#24518;&#38271;&#24207;&#21015;&#30340;&#33021;&#21147;&#12289;&#25239;&#22122;&#22768;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#23427;&#37319;&#29992;&#20102;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#20108;&#32500;&#29615;&#22659;&#21644;&#31163;&#25955;&#25511;&#21046;&#65292;&#21363;Mortar Mayhem&#12289;Mystery Path&#21644;Searing Spotlights&#12290;&#36825;&#20123;&#26368;&#21021;&#26159;&#26377;&#38480;&#30340;&#29615;&#22659;&#34987;&#25512;&#24191;&#20026;&#26032;&#39062;&#30340;&#26080;&#23613;&#20219;&#21153;&#65292;&#20316;&#20026;&#19968;&#31181;&#33258;&#21160;&#35838;&#31243;&#65292;&#20174;&#36710;&#28216;&#25103;"I packed my bag"&#20013;&#27762;&#21462;&#28789;&#24863;&#12290;&#36825;&#20123;&#26080;&#23613;&#20219;&#21153;&#19981;&#20165;&#26377;&#21161;&#20110;&#35780;&#20272;&#25928;&#29575;&#65292;&#32780;&#19988;&#26377;&#36259;&#22320;&#35780;&#20272;&#20102;&#35760;&#24518;&#20026;&#22522;&#30784;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#37492;&#20110;&#29616;&#26377;&#20844;&#24320;&#21487;&#29992;&#30340;&#35760;&#24518;&#22522;&#20934;&#30340;&#31232;&#32570;&#24615;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#30001;TrXL&#21644;Proximal Policy Optimization&#39537;&#21160;&#30340;&#23454;&#29616;&#12290;&#26412;&#23454;&#29616;&#21033;&#29992;TrXL&#20316;&#20026;&#20197;&#28369;&#21160;&#31383;&#21475;&#26041;&#27861;&#20351;&#29992;&#30340;&#24773;&#33410;&#24615;&#35760;&#24518;&#12290;&#22312;&#26377;&#38480;&#29615;&#22659;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;...
&lt;/p&gt;
&lt;p&gt;
Memory Gym introduces a unique benchmark designed to test Deep Reinforcement Learning agents, specifically comparing Gated Recurrent Unit (GRU) against Transformer-XL (TrXL), on their ability to memorize long sequences, withstand noise, and generalize. It features partially observable 2D environments with discrete controls, namely Mortar Mayhem, Mystery Path, and Searing Spotlights. These originally finite environments are extrapolated to novel endless tasks that act as an automatic curriculum, drawing inspiration from the car game ``I packed my bag". These endless tasks are not only beneficial for evaluating efficiency but also intriguingly valuable for assessing the effectiveness of approaches in memory-based agents. Given the scarcity of publicly available memory baselines, we contribute an implementation driven by TrXL and Proximal Policy Optimization. This implementation leverages TrXL as episodic memory using a sliding window approach. In our experiments on the finite environment
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20110;&#35010;&#32541;&#20998;&#21106;&#21644;&#30417;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#20998;&#31867;&#22120;&#30340;&#35299;&#37322;&#20013;&#23548;&#20986;&#20998;&#21106;&#32467;&#26524;&#65292;&#26080;&#38656;&#20687;&#32032;&#32423;&#27880;&#37322;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#26377;&#25928;&#36827;&#34892;&#35010;&#32541;&#20998;&#21106;&#21644;&#29983;&#38271;&#30417;&#27979;&#12290;</title><link>http://arxiv.org/abs/2309.11267</link><description>&lt;p&gt;
&#20174;&#20998;&#31867;&#21040;&#20998;&#21106;&#19982;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65306;&#20851;&#20110;&#35010;&#32441;&#26816;&#27979;&#21644;&#29983;&#38271;&#30417;&#27979;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
From Classification to Segmentation with Explainable AI: A Study on Crack Detection and Growth Monitoring. (arXiv:2309.11267v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11267
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20110;&#35010;&#32541;&#20998;&#21106;&#21644;&#30417;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#20998;&#31867;&#22120;&#30340;&#35299;&#37322;&#20013;&#23548;&#20986;&#20998;&#21106;&#32467;&#26524;&#65292;&#26080;&#38656;&#20687;&#32032;&#32423;&#27880;&#37322;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#26377;&#25928;&#36827;&#34892;&#35010;&#32541;&#20998;&#21106;&#21644;&#29983;&#38271;&#30417;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#27979;&#22522;&#30784;&#35774;&#26045;&#30340;&#34920;&#38754;&#35010;&#32541;&#23545;&#20110;&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#33258;&#21160;&#35270;&#35273;&#26816;&#27979;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29305;&#21035;&#26159;&#22312;&#38590;&#20197;&#21040;&#36798;&#30340;&#21306;&#22495;&#12290;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24050;&#32463;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#65292;&#20294;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#27880;&#37322;&#25968;&#25454;&#38598;&#36827;&#34892;&#30417;&#30563;&#35757;&#32451;&#12290;&#19968;&#26086;&#26816;&#27979;&#21040;&#35010;&#32541;&#65292;&#30417;&#27979;&#20854;&#20005;&#37325;&#31243;&#24230;&#36890;&#24120;&#38656;&#35201;&#23545;&#25439;&#23475;&#36827;&#34892;&#31934;&#30830;&#30340;&#20998;&#21106;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20998;&#21106;&#32780;&#35328;&#65292;&#22270;&#20687;&#30340;&#20687;&#32032;&#32423;&#27880;&#37322;&#26159;&#19968;&#39033;&#21171;&#21160;&#23494;&#38598;&#22411;&#30340;&#24037;&#20316;&#12290;&#20026;&#20102;&#20943;&#23569;&#36825;&#31181;&#25104;&#26412;&#65292;&#21487;&#20197;&#21033;&#29992;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#20174;&#20998;&#31867;&#22120;&#30340;&#35299;&#37322;&#20013;&#23548;&#20986;&#20998;&#21106;&#32467;&#26524;&#65292;&#20165;&#38656;&#35201;&#24369;&#22270;&#20687;&#32423;&#21035;&#30340;&#30417;&#30563;&#12290;&#26412;&#25991;&#25552;&#20986;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#20998;&#21106;&#21644;&#30417;&#27979;&#34920;&#38754;&#35010;&#32541;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#21508;&#31181;XAI&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#30740;&#31350;&#20102;&#36825;&#31181;&#26041;&#27861;&#22914;&#20309;&#20419;&#36827;&#20005;&#37325;&#31243;&#24230;&#37327;&#21270;&#21644;&#29983;&#38271;&#30417;&#27979;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;&#24471;&#21040;&#30340;&#20998;&#21106;&#25513;&#27169;&#21487;&#33021;&#36739;&#20302;&#36136;&#37327;&#65292;&#20294;&#37319;&#29992;&#36825;&#31181;&#26041;&#27861;&#20173;&#33021;&#26377;&#25928;&#22320;&#36827;&#34892;&#35010;&#32441;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;
Monitoring surface cracks in infrastructure is crucial for structural health monitoring. Automatic visual inspection offers an effective solution, especially in hard-to-reach areas. Machine learning approaches have proven their effectiveness but typically require large annotated datasets for supervised training. Once a crack is detected, monitoring its severity often demands precise segmentation of the damage. However, pixel-level annotation of images for segmentation is labor-intensive. To mitigate this cost, one can leverage explainable artificial intelligence (XAI) to derive segmentations from the explanations of a classifier, requiring only weak image-level supervision. This paper proposes applying this methodology to segment and monitor surface cracks. We evaluate the performance of various XAI methods and examine how this approach facilitates severity quantification and growth monitoring. Results reveal that while the resulting segmentation masks may exhibit lower quality than th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#31471;&#21040;&#31471;&#20449;&#36947;&#32534;&#30721;&#30340;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#27169;&#25311;&#23454;&#39564;&#35777;&#26126;&#20102;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#23398;&#20064;&#20449;&#36947;&#20998;&#24067;&#20174;&#32780;&#23454;&#29616;&#25509;&#36817;&#26368;&#20248;&#30340;&#31471;&#21040;&#31471;&#31526;&#21495;&#35823;&#30721;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.10505</link><description>&lt;p&gt;
&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#31471;&#21040;&#31471;&#20449;&#36947;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Learning End-to-End Channel Coding with Diffusion Models. (arXiv:2309.10505v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10505
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#31471;&#21040;&#31471;&#20449;&#36947;&#32534;&#30721;&#30340;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#27169;&#25311;&#23454;&#39564;&#35777;&#26126;&#20102;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#23398;&#20064;&#20449;&#36947;&#20998;&#24067;&#20174;&#32780;&#23454;&#29616;&#25509;&#36817;&#26368;&#20248;&#30340;&#31471;&#21040;&#31471;&#31526;&#21495;&#35823;&#30721;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#36817;&#20284;&#20449;&#36947;&#20998;&#24067;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#20449;&#36947;&#32534;&#30721;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#35757;&#32451;&#31639;&#27861;&#12290;&#36890;&#36807;&#19982;&#21508;&#31181;&#20449;&#36947;&#27169;&#22411;&#30340;&#27169;&#25311;&#23454;&#39564;&#65292;&#39564;&#35777;&#20102;&#25193;&#25955;&#27169;&#22411;&#31934;&#30830;&#23398;&#20064;&#20449;&#36947;&#20998;&#24067;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#25509;&#36817;&#26368;&#20248;&#30340;&#31471;&#21040;&#31471;&#31526;&#21495;&#35823;&#30721;&#29575;&#65288;SER&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The training of neural encoders via deep learning necessitates a differentiable channel model due to the backpropagation algorithm. This requirement can be sidestepped by approximating either the channel distribution or its gradient through pilot signals in real-world scenarios. The initial approach draws upon the latest advancements in image generation, utilizing generative adversarial networks (GANs) or their enhanced variants to generate channel distributions. In this paper, we address this channel approximation challenge with diffusion models, which have demonstrated high sample quality in image generation. We offer an end-to-end channel coding framework underpinned by diffusion models and propose an efficient training algorithm. Our simulations with various channel models establish that our diffusion models learn the channel distribution accurately, thereby achieving near-optimal end-to-end symbol error rates (SERs). We also note a significant advantage of diffusion models: A robu
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#36801;&#31227;&#23398;&#20064;&#21551;&#21457;&#30340;&#31574;&#30053;&#65292;&#26088;&#22312;&#36890;&#36807;&#20943;&#23569;&#35774;&#22791;&#19978;&#30340;&#36164;&#28304;&#21033;&#29992;&#12289;&#20197;&#21450;&#20943;&#23569;&#26381;&#21153;&#22120;&#21644;&#32593;&#32476;&#30340;&#36127;&#36733;&#65292;&#25552;&#39640;&#32852;&#37030;&#23398;&#20064;&#20013;&#36793;&#32536;&#33410;&#28857;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.10367</link><description>&lt;p&gt;
&#36793;&#32536;&#33410;&#28857;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#36164;&#28304;&#21033;&#29992;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Toward efficient resource utilization at edge nodes in federated learning. (arXiv:2309.10367v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10367
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#36801;&#31227;&#23398;&#20064;&#21551;&#21457;&#30340;&#31574;&#30053;&#65292;&#26088;&#22312;&#36890;&#36807;&#20943;&#23569;&#35774;&#22791;&#19978;&#30340;&#36164;&#28304;&#21033;&#29992;&#12289;&#20197;&#21450;&#20943;&#23569;&#26381;&#21153;&#22120;&#21644;&#32593;&#32476;&#30340;&#36127;&#36733;&#65292;&#25552;&#39640;&#32852;&#37030;&#23398;&#20064;&#20013;&#36793;&#32536;&#33410;&#28857;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20351;&#24471;&#36793;&#32536;&#33410;&#28857;&#33021;&#22815;&#20849;&#21516;&#26500;&#24314;&#20840;&#23616;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#20849;&#20139;&#20182;&#20204;&#30340;&#25968;&#25454;&#12290;&#36825;&#26159;&#36890;&#36807;&#35774;&#22791;&#35745;&#31639;&#26412;&#22320;&#31169;&#26377;&#27169;&#22411;&#26356;&#26032;&#65292;&#28982;&#21518;&#30001;&#26381;&#21153;&#22120;&#36827;&#34892;&#32858;&#21512;&#26469;&#23454;&#29616;&#30340;&#12290;&#28982;&#32780;&#65292;&#35745;&#31639;&#36164;&#28304;&#38480;&#21046;&#21644;&#32593;&#32476;&#36890;&#20449;&#23545;&#20110;&#22823;&#22411;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#36739;&#22823;&#27169;&#22411;&#22823;&#23567;&#21487;&#33021;&#25104;&#20026;&#20005;&#37325;&#29942;&#39048;&#12290;&#36793;&#32536;&#33410;&#28857;&#24448;&#24448;&#20855;&#26377;&#26377;&#38480;&#30340;&#30828;&#20214;&#36164;&#28304;&#65288;RAM&#12289;CPU&#65289;&#65292;&#32780;&#36793;&#32536;&#30340;&#32593;&#32476;&#24102;&#23485;&#21644;&#21487;&#38752;&#24615;&#23545;&#20110;&#25193;&#23637;&#32852;&#37030;&#36710;&#38431;&#24212;&#29992;&#26469;&#35828;&#26159;&#19968;&#20010;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#21463;&#36801;&#31227;&#23398;&#20064;&#21551;&#21457;&#30340;FL&#31574;&#30053;&#65292;&#20197;&#20943;&#23569;&#35774;&#22791;&#19978;&#30340;&#36164;&#28304;&#21033;&#29992;&#65292;&#20197;&#21450;&#27599;&#20010;&#20840;&#23616;&#35757;&#32451;&#36718;&#27425;&#20013;&#26381;&#21153;&#22120;&#21644;&#32593;&#32476;&#30340;&#36127;&#36733;&#12290;&#23545;&#20110;&#27599;&#20010;&#26412;&#22320;&#27169;&#22411;&#26356;&#26032;&#65292;&#25105;&#20204;&#38543;&#26426;&#36873;&#25321;&#35201;&#35757;&#32451;&#30340;&#23618;&#65292;&#20923;&#32467;&#27169;&#22411;&#30340;&#20854;&#20313;&#37096;&#20998;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#25490;&#38500;&#25152;&#26377;&#26410;&#35757;&#32451;&#30340;&#37096;&#20998;&#26469;&#20943;&#23569;&#27599;&#36718;&#30340;&#26381;&#21153;&#22120;&#36127;&#36733;&#21644;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) enables edge nodes to collaboratively contribute to constructing a global model without sharing their data. This is accomplished by devices computing local, private model updates that are then aggregated by a server. However, computational resource constraints and network communication can become a severe bottleneck for larger model sizes typical for deep learning applications. Edge nodes tend to have limited hardware resources (RAM, CPU), and the network bandwidth and reliability at the edge is a concern for scaling federated fleet applications. In this paper, we propose and evaluate a FL strategy inspired by transfer learning in order to reduce resource utilization on devices, as well as the load on the server and network in each global training round. For each local model update, we randomly select layers to train, freezing the remaining part of the model. In doing so, we can reduce both server load and communication costs per round by excluding all untrained
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;&#22914;&#20309;&#21033;&#29992;&#20803;&#20449;&#24687;&#26469;&#25913;&#21892;&#22522;&#20110;&#38899;&#39057;&#30340;&#38646;&#26679;&#26412;&#40479;&#31867;&#20998;&#31867;&#65292;&#24182;&#36890;&#36807;&#36830;&#25509;&#19981;&#21516;&#30340;&#20803;&#25968;&#25454;&#21644;&#38899;&#39057;&#29305;&#24449;&#33719;&#24471;&#26368;&#20339;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.08398</link><description>&lt;p&gt;
&#25506;&#32034;&#22522;&#20110;&#20803;&#20449;&#24687;&#30340;&#22522;&#20110;&#38899;&#39057;&#30340;&#38646;&#26679;&#26412;&#40479;&#31867;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Exploring Meta Information for Audio-based Zero-shot Bird Classification. (arXiv:2309.08398v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08398
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;&#22914;&#20309;&#21033;&#29992;&#20803;&#20449;&#24687;&#26469;&#25913;&#21892;&#22522;&#20110;&#38899;&#39057;&#30340;&#38646;&#26679;&#26412;&#40479;&#31867;&#20998;&#31867;&#65292;&#24182;&#36890;&#36807;&#36830;&#25509;&#19981;&#21516;&#30340;&#20803;&#25968;&#25454;&#21644;&#38899;&#39057;&#29305;&#24449;&#33719;&#24471;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34987;&#21160;&#22768;&#23398;&#30417;&#27979;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#36827;&#27493;&#24050;&#32463;&#20026;&#35745;&#31639;&#29983;&#29289;&#22768;&#23398;&#30740;&#31350;&#25552;&#20379;&#20102;&#22823;&#37327;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#31232;&#26377;&#21644;&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#29289;&#31181;&#26469;&#35828;&#65292;&#25968;&#25454;&#31232;&#32570;&#20173;&#28982;&#26159;&#19968;&#20010;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#20016;&#23500;&#21644;&#22810;&#26679;&#30340;&#20803;&#25968;&#25454;&#65292;&#20197;&#40479;&#31867;&#29289;&#31181;&#20026;&#20363;&#36827;&#34892;&#20102;&#25506;&#32034;&#65292;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#20803;&#20449;&#24687;&#26469;&#25913;&#21892;&#38646;&#26679;&#26412;&#38899;&#39057;&#20998;&#31867;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#20803;&#25968;&#25454;&#26469;&#28304;&#65306;&#36890;&#36807;(S)BERT&#32534;&#30721;&#30340;&#25991;&#26412;&#40479;&#40483;&#25551;&#36848;&#65292;&#21151;&#33021;&#29305;&#24615;(AVONET)&#21644;&#40479;&#31867;&#29983;&#27963;&#21490;(BLH)&#29305;&#24449;&#12290;&#20316;&#20026;&#38899;&#39057;&#29305;&#24449;&#65292;&#25105;&#20204;&#25552;&#21462;&#38899;&#39057;&#39057;&#35889;&#22270;&#21464;&#25442;&#22120;(AST)&#23884;&#20837;&#65292;&#24182;&#36890;&#36807;&#37319;&#29992;&#21333;&#20010;&#32447;&#24615;&#23618;&#23558;&#20854;&#25237;&#24433;&#21040;&#36741;&#21161;&#20449;&#24687;&#30340;&#32500;&#24230;&#19978;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#28857;&#31215;&#20316;&#20026;&#20860;&#23481;&#24615;&#20989;&#25968;&#65292;&#24182;&#20351;&#29992;&#26631;&#20934;&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;&#25490;&#21517;&#38128;&#38142;&#25439;&#22833;&#30830;&#23450;&#27491;&#30830;&#30340;&#31867;&#21035;&#12290;&#36890;&#36807;&#36830;&#25509;AVONET&#21644;BLH&#29305;&#24449;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advances in passive acoustic monitoring and machine learning have led to the procurement of vast datasets for computational bioacoustic research. Nevertheless, data scarcity is still an issue for rare and underrepresented species. This study investigates how meta-information can improve zero-shot audio classification, utilising bird species as an example case study due to the availability of rich and diverse metadata. We investigate three different sources of metadata: textual bird sound descriptions encoded via (S)BERT, functional traits (AVONET), and bird life-history (BLH) characteristics. As audio features, we extract audio spectrogram transformer (AST) embeddings and project them to the dimension of the auxiliary information by adopting a single linear layer. Then, we employ the dot product as compatibility function and a standard zero-shot learning ranking hinge loss to determine the correct class. The best results are achieved by concatenating the AVONET and BLH features attaini
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#30740;&#32508;&#21512;&#23457;&#26597;&#20102;&#22312;&#25945;&#32946;&#25968;&#25454;&#25366;&#25496;&#20013;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#65292;&#21253;&#25324;&#23545;&#30693;&#35782;&#36319;&#36394;&#12289;&#23398;&#29983;&#19981;&#33391;&#34892;&#20026;&#26816;&#27979;&#12289;&#24615;&#33021;&#39044;&#27979;&#21644;&#20010;&#24615;&#21270;&#25512;&#33616;&#31561;&#20856;&#22411;&#25945;&#32946;&#22330;&#26223;&#30340;&#24212;&#29992;&#12290;&#21516;&#26102;&#25552;&#20379;&#20102;&#20844;&#20849;&#25968;&#25454;&#38598;&#21644;&#22788;&#29702;&#24037;&#20855;&#30340;&#32508;&#21512;&#27010;&#36848;&#65292;&#24182;&#25351;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2309.04761</link><description>&lt;p&gt;
&#22312;&#25945;&#32946;&#25968;&#25454;&#25366;&#25496;&#20013;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#32508;&#21512;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey on Deep Learning Techniques in Educational Data Mining. (arXiv:2309.04761v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04761
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#30740;&#32508;&#21512;&#23457;&#26597;&#20102;&#22312;&#25945;&#32946;&#25968;&#25454;&#25366;&#25496;&#20013;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#65292;&#21253;&#25324;&#23545;&#30693;&#35782;&#36319;&#36394;&#12289;&#23398;&#29983;&#19981;&#33391;&#34892;&#20026;&#26816;&#27979;&#12289;&#24615;&#33021;&#39044;&#27979;&#21644;&#20010;&#24615;&#21270;&#25512;&#33616;&#31561;&#20856;&#22411;&#25945;&#32946;&#22330;&#26223;&#30340;&#24212;&#29992;&#12290;&#21516;&#26102;&#25552;&#20379;&#20102;&#20844;&#20849;&#25968;&#25454;&#38598;&#21644;&#22788;&#29702;&#24037;&#20855;&#30340;&#32508;&#21512;&#27010;&#36848;&#65292;&#24182;&#25351;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25945;&#32946;&#25968;&#25454;&#25366;&#25496;(EDM)&#20316;&#20026;&#30740;&#31350;&#30340;&#37325;&#35201;&#39046;&#22495;&#65292;&#21033;&#29992;&#35745;&#31639;&#25216;&#26415;&#26469;&#20998;&#26512;&#25945;&#32946;&#25968;&#25454;&#12290;&#38543;&#30528;&#25945;&#32946;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#21644;&#22810;&#26679;&#24615;&#22686;&#21152;&#65292;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#35299;&#20915;&#20998;&#26512;&#21644;&#24314;&#27169;&#36825;&#20123;&#25968;&#25454;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;&#26412;&#35843;&#30740;&#26088;&#22312;&#31995;&#32479;&#22320;&#23457;&#26597;&#28145;&#24230;&#23398;&#20064;&#22312;EDM&#39046;&#22495;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20379;&#20102;&#20851;&#20110;EDM&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#31616;&#35201;&#20171;&#32461;&#65292;&#24378;&#35843;&#20102;&#23427;&#20204;&#22312;&#29616;&#20195;&#25945;&#32946;&#29615;&#22659;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#35814;&#32454;&#22238;&#39038;&#20102;&#22312;&#22235;&#20010;&#20856;&#22411;&#25945;&#32946;&#22330;&#26223;&#20013;&#24212;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#21253;&#25324;&#30693;&#35782;&#36319;&#36394;&#12289;&#23398;&#29983;&#19981;&#33391;&#34892;&#20026;&#26816;&#27979;&#12289;&#24615;&#33021;&#39044;&#27979;&#21644;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;EDM&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#21644;&#22788;&#29702;&#24037;&#20855;&#30340;&#32508;&#21512;&#27010;&#36848;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25351;&#20986;&#20102;&#35813;&#30740;&#31350;&#39046;&#22495;&#30340;&#26032;&#20852;&#36235;&#21183;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Educational Data Mining (EDM) has emerged as a vital field of research, which harnesses the power of computational techniques to analyze educational data. With the increasing complexity and diversity of educational data, Deep Learning techniques have shown significant advantages in addressing the challenges associated with analyzing and modeling this data. This survey aims to systematically review the state-of-the-art in EDM with Deep Learning. We begin by providing a brief introduction to EDM and Deep Learning, highlighting their relevance in the context of modern education. Next, we present a detailed review of Deep Learning techniques applied in four typical educational scenarios, including knowledge tracing, undesirable student detecting, performance prediction, and personalized recommendation. Furthermore, a comprehensive overview of public datasets and processing tools for EDM is provided. Finally, we point out emerging trends and future directions in this research area.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22343;&#21248;&#30340;Tanh&#21464;&#25442;&#36827;&#34892;&#38754;&#37096;&#35299;&#26512;&#30340;&#36974;&#25377;&#24863;&#30693;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#38754;&#37096;&#36974;&#25377;&#38382;&#39064;&#65292;&#24182;&#19988;&#33021;&#22815;&#34701;&#21512;&#26356;&#22810;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#36974;&#25377;&#24863;&#30693;&#25439;&#22833;&#65292;&#25552;&#39640;&#20102;&#36793;&#30028;&#30340;&#35782;&#21035;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.15323</link><description>&lt;p&gt;
&#36890;&#36807;&#22343;&#21248;&#30340;Tanh&#21464;&#25442;&#30340;&#38754;&#37096;&#35299;&#26512;&#30340;&#36974;&#25377;&#24863;&#30693;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Occlusion-Aware Deep Convolutional Neural Network via Homogeneous Tanh-transforms for Face Parsing. (arXiv:2308.15323v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15323
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22343;&#21248;&#30340;Tanh&#21464;&#25442;&#36827;&#34892;&#38754;&#37096;&#35299;&#26512;&#30340;&#36974;&#25377;&#24863;&#30693;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#38754;&#37096;&#36974;&#25377;&#38382;&#39064;&#65292;&#24182;&#19988;&#33021;&#22815;&#34701;&#21512;&#26356;&#22810;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#36974;&#25377;&#24863;&#30693;&#25439;&#22833;&#65292;&#25552;&#39640;&#20102;&#36793;&#30028;&#30340;&#35782;&#21035;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#37096;&#35299;&#26512;&#26159;&#20026;&#27599;&#20010;&#35821;&#20041;&#38754;&#37096;&#32452;&#20214;&#25512;&#26029;&#20687;&#32032;&#32423;&#26631;&#31614;&#22270;&#30340;&#36807;&#31243;&#12290;&#20197;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#23545;&#26080;&#36974;&#25377;&#30340;&#38754;&#37096;&#25928;&#26524;&#33391;&#22909;&#65292;&#20294;&#22312;&#38754;&#37096;&#36974;&#25377;&#19979;&#24573;&#30053;&#20102;&#36974;&#25377;&#21644;&#24573;&#35270;&#20102;&#21333;&#20010;&#38754;&#37096;&#22806;&#19968;&#20123;&#19978;&#19979;&#25991;&#21306;&#22495;&#65292;&#23588;&#20854;&#26159;&#22312;COVID-19&#27969;&#34892;&#26399;&#38388;&#65292;&#38754;&#37096;&#36974;&#25377;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#24120;&#35265;&#24773;&#20917;&#12290;&#21463;&#22270;&#20687;&#29031;&#26126;&#29702;&#35770;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#20687;&#39044;&#22788;&#29702;&#26041;&#27861;&#65292;&#21363;&#30001;&#22235;&#20010;Tanh&#21464;&#25442;&#32452;&#25104;&#30340;&#22343;&#21248;Tanh&#21464;&#25442;&#65292;&#23558;&#20013;&#22830;&#35270;&#35273;&#21644;&#21608;&#36793;&#35270;&#35273;&#34701;&#21512;&#22312;&#19968;&#36215;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#36974;&#25377;&#19979;&#38754;&#37096;&#35299;&#26512;&#30340;&#22256;&#22659;&#65292;&#24182;&#21387;&#32553;&#20102;&#26356;&#22810;&#30340;&#21608;&#22260;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#22522;&#20110;&#22343;&#21248;&#30340;Tanh&#21464;&#25442;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#36974;&#25377;&#38754;&#37096;&#35299;&#26512;&#30340;&#36974;&#25377;&#24863;&#30693;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;&#23427;&#32467;&#21512;&#20102;Tanh&#26497;&#22352;&#26631;&#31354;&#38388;&#21644;Tanh&#31515;&#21345;&#23572;&#31354;&#38388;&#20013;&#30340;&#20449;&#24687;&#65292;&#33021;&#22815;&#22686;&#24378;&#24863;&#21463;&#37326;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#36974;&#25377;&#24863;&#30693;&#25439;&#22833;&#65292;&#19987;&#27880;&#20110;&#36974;&#25377;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
Face parsing infers a pixel-wise label map for each semantic facial component. Previous methods generally work well for uncovered faces, however overlook the facial occlusion and ignore some contextual area outside a single face, especially when facial occlusion has become a common situation during the COVID-19 epidemic. Inspired by the illumination theory of image, we propose a novel homogeneous tanh-transforms for image preprocessing, which made up of four tanh-transforms, that fuse the central vision and the peripheral vision together. Our proposed method addresses the dilemma of face parsing under occlusion and compresses more information of surrounding context. Based on homogeneous tanh-transforms, we propose an occlusion-aware convolutional neural network for occluded face parsing. It combines the information both in Tanh-polar space and Tanh-Cartesian space, capable of enhancing receptive fields. Furthermore, we introduce an occlusion-aware loss to focus on the boundaries of occ
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#30340;&#21442;&#25968;&#20272;&#35745;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35889;&#20272;&#35745;&#22120;&#36827;&#34892;&#39044;&#22788;&#29702;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#27979;&#37327;&#36827;&#34892;&#29305;&#24449;&#21327;&#26041;&#24046;&#30697;&#38453;&#931;&#34920;&#31034;&#65292;&#20998;&#26512;&#20102;&#35889;&#20272;&#35745;&#22120;&#22312;&#32467;&#26500;&#21270;&#35774;&#35745;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#30830;&#23450;&#20102;&#26368;&#20248;&#39044;&#22788;&#29702;&#20197;&#26368;&#23567;&#21270;&#26679;&#26412;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2308.14507</link><description>&lt;p&gt;
&#36890;&#36807;&#36817;&#20284;&#20256;&#36882;&#28040;&#24687;&#23454;&#29616;&#32467;&#26500;&#21270;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#30340;&#35889;&#20272;&#35745;&#22120;
&lt;/p&gt;
&lt;p&gt;
Spectral Estimators for Structured Generalized Linear Models via Approximate Message Passing. (arXiv:2308.14507v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14507
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#30340;&#21442;&#25968;&#20272;&#35745;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35889;&#20272;&#35745;&#22120;&#36827;&#34892;&#39044;&#22788;&#29702;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#27979;&#37327;&#36827;&#34892;&#29305;&#24449;&#21327;&#26041;&#24046;&#30697;&#38453;&#931;&#34920;&#31034;&#65292;&#20998;&#26512;&#20102;&#35889;&#20272;&#35745;&#22120;&#22312;&#32467;&#26500;&#21270;&#35774;&#35745;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#30830;&#23450;&#20102;&#26368;&#20248;&#39044;&#22788;&#29702;&#20197;&#26368;&#23567;&#21270;&#26679;&#26412;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20174;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#20013;&#30340;&#35266;&#27979;&#20013;&#36827;&#34892;&#21442;&#25968;&#20272;&#35745;&#30340;&#38382;&#39064;&#12290;&#35889;&#26041;&#27861;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#20272;&#35745;&#26041;&#27861;&#65306;&#23427;&#36890;&#36807;&#23545;&#35266;&#27979;&#36827;&#34892;&#36866;&#24403;&#39044;&#22788;&#29702;&#24471;&#21040;&#30340;&#30697;&#38453;&#30340;&#20027;&#29305;&#24449;&#21521;&#37327;&#26469;&#20272;&#35745;&#21442;&#25968;&#12290;&#23613;&#31649;&#35889;&#20272;&#35745;&#22120;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#23545;&#20110;&#32467;&#26500;&#21270;&#65288;&#21363;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#39640;&#26031;&#21644;&#21704;&#23572;&#65289;&#35774;&#35745;&#65292;&#30446;&#21069;&#20165;&#26377;&#23545;&#35889;&#20272;&#35745;&#22120;&#30340;&#20005;&#26684;&#24615;&#33021;&#34920;&#24449;&#20197;&#21450;&#23545;&#25968;&#25454;&#36827;&#34892;&#39044;&#22788;&#29702;&#30340;&#22522;&#26412;&#26041;&#27861;&#21487;&#29992;&#12290;&#30456;&#21453;&#65292;&#23454;&#38469;&#30340;&#35774;&#35745;&#30697;&#38453;&#20855;&#26377;&#39640;&#24230;&#32467;&#26500;&#21270;&#24182;&#19988;&#34920;&#29616;&#20986;&#38750;&#24179;&#20961;&#30340;&#30456;&#20851;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#25429;&#25417;&#27979;&#37327;&#30340;&#38750;&#21508;&#21521;&#21516;&#24615;&#29305;&#24615;&#30340;&#30456;&#20851;&#39640;&#26031;&#35774;&#35745;&#65292;&#36890;&#36807;&#29305;&#24449;&#21327;&#26041;&#24046;&#30697;&#38453;&#931;&#36827;&#34892;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#23545;&#20110;&#36825;&#31181;&#24773;&#20917;&#19979;&#35889;&#20272;&#35745;&#22120;&#24615;&#33021;&#30340;&#31934;&#30830;&#28176;&#36817;&#20998;&#26512;&#12290;&#28982;&#21518;&#65292;&#21487;&#20197;&#36890;&#36807;&#36825;&#19968;&#32467;&#26524;&#26469;&#30830;&#23450;&#26368;&#20248;&#39044;&#22788;&#29702;&#65292;&#20174;&#32780;&#26368;&#23567;&#21270;&#25152;&#38656;&#26679;&#26412;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of parameter estimation from observations given by a generalized linear model. Spectral methods are a simple yet effective approach for estimation: they estimate the parameter via the principal eigenvector of a matrix obtained by suitably preprocessing the observations. Despite their wide use, a rigorous performance characterization of spectral estimators, as well as a principled way to preprocess the data, is available only for unstructured (i.e., i.i.d. Gaussian and Haar) designs. In contrast, real-world design matrices are highly structured and exhibit non-trivial correlations. To address this problem, we consider correlated Gaussian designs which capture the anisotropic nature of the measurements via a feature covariance matrix $\Sigma$. Our main result is a precise asymptotic characterization of the performance of spectral estimators in this setting. This then allows to identify the optimal preprocessing that minimizes the number of samples needed to meanin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#19977;&#32500;&#35013;&#36733;&#26377;&#38480;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#30340;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#32447;&#24615;&#26102;&#38388;&#20869;&#26377;&#25928;&#35745;&#31639;&#20849;&#36733;&#21644;&#36335;&#24452;&#30340;&#21487;&#34892;&#35299;&#20915;&#26041;&#26696;&#65292;&#20174;&#32780;&#37322;&#25918;&#20102;&#30899;&#20943;&#25490;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.12136</link><description>&lt;p&gt;
&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#37322;&#25918;&#19977;&#32500;&#35013;&#36733;&#26377;&#38480;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#30340;&#30899;&#20943;&#25490;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Unlocking Carbon Reduction Potential with Reinforcement Learning for the Three-Dimensional Loading Capacitated Vehicle Routing Problem. (arXiv:2307.12136v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12136
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#19977;&#32500;&#35013;&#36733;&#26377;&#38480;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#30340;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#32447;&#24615;&#26102;&#38388;&#20869;&#26377;&#25928;&#35745;&#31639;&#20849;&#36733;&#21644;&#36335;&#24452;&#30340;&#21487;&#34892;&#35299;&#20915;&#26041;&#26696;&#65292;&#20174;&#32780;&#37322;&#25918;&#20102;&#30899;&#20943;&#25490;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#22411;&#36135;&#36710;&#26159;&#20379;&#24212;&#38142;&#20132;&#20184;&#31995;&#32479;&#30340;&#37325;&#35201;&#25903;&#26609;&#65292;&#20294;&#22312;&#33521;&#22269;&#20165;&#20855;&#26377;60&#65285;&#30340;&#35013;&#36733;&#25928;&#29575;&#65292;&#23545;&#30899;&#25490;&#25918;&#26377;&#26174;&#33879;&#36129;&#29486;&#12290;&#21327;&#21516;&#36710;&#36742;&#36335;&#24452;&#35268;&#21010;&#34987;&#25552;&#20986;&#20316;&#20026;&#25552;&#39640;&#25928;&#29575;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#20173;&#38754;&#20020;&#25361;&#25112;&#12290;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#26377;&#25928;&#35745;&#31639;&#20849;&#36733;&#21644;&#36335;&#24452;&#30340;&#21487;&#34892;&#35299;&#20915;&#26041;&#26696;&#12290;&#24403;&#21069;&#30340;&#36816;&#31609;&#23398;&#26041;&#27861;&#22312;&#38382;&#39064;&#35268;&#27169;&#22686;&#22823;&#26102;&#23384;&#22312;&#38750;&#32447;&#24615;&#25193;&#23637;&#65292;&#22240;&#27492;&#21482;&#33021;&#38480;&#20110;&#22312;&#26377;&#38480;&#30340;&#22320;&#29702;&#33539;&#22260;&#20869;&#35745;&#31639;&#32467;&#26524;&#20197;&#28385;&#36275;&#26085;&#24120;&#36816;&#33829;&#30340;&#26102;&#38388;&#35201;&#27714;&#12290;&#36825;&#20165;&#20801;&#35768;&#22312;&#36335;&#24452;&#35268;&#21010;&#19978;&#23547;&#25214;&#23616;&#37096;&#26368;&#20248;&#35299;&#65292;&#26080;&#27861;&#23454;&#29616;&#20840;&#23616;&#20248;&#21270;&#28508;&#21147;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#26469;&#36817;&#32447;&#24615;&#26102;&#38388;&#35299;&#20915;&#19977;&#32500;&#35013;&#36733;&#26377;&#38480;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#12290;&#34429;&#28982;&#27492;&#38382;&#39064;&#22312;&#36816;&#31609;&#23398;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#27809;&#26377;&#20851;&#20110;&#29992;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#20986;&#29256;&#29289;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
Heavy goods vehicles are vital backbones of the supply chain delivery system but also contribute significantly to carbon emissions with only 60% loading efficiency in the United Kingdom. Collaborative vehicle routing has been proposed as a solution to increase efficiency, but challenges remain to make this a possibility. One key challenge is the efficient computation of viable solutions for co-loading and routing. Current operations research methods suffer from non-linear scaling with increasing problem size and are therefore bound to limited geographic areas to compute results in time for day-to-day operations. This only allows for local optima in routing and leaves global optimisation potential untouched. We develop a reinforcement learning model to solve the three-dimensional loading capacitated vehicle routing problem in approximately linear time. While this problem has been studied extensively in operations research, no publications on solving it with reinforcement learning exist.
&lt;/p&gt;</description></item><item><title>TinyTrain&#26159;&#19968;&#31181;&#22312;&#35774;&#22791;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#26356;&#26032;&#27169;&#22411;&#30340;&#37096;&#20998;&#24182;&#22788;&#29702;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#22823;&#22823;&#32553;&#30701;&#20102;&#35757;&#32451;&#26102;&#38388;&#12290;&#36890;&#36807;&#20219;&#21153;&#33258;&#36866;&#24212;&#30340;&#31232;&#30095;&#26356;&#26032;&#26041;&#27861;&#65292;TinyTrain&#33021;&#22815;&#22312;&#39640;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#20943;&#23567;&#35745;&#31639;&#21644;&#20869;&#23384;&#21344;&#29992;&#65292;&#23545;&#26410;&#30693;&#20219;&#21153;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2307.09988</link><description>&lt;p&gt;
TinyTrain&#65306;&#22312;&#26497;&#31471;&#36793;&#32536;&#36827;&#34892;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
TinyTrain: Deep Neural Network Training at the Extreme Edge. (arXiv:2307.09988v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09988
&lt;/p&gt;
&lt;p&gt;
TinyTrain&#26159;&#19968;&#31181;&#22312;&#35774;&#22791;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#26356;&#26032;&#27169;&#22411;&#30340;&#37096;&#20998;&#24182;&#22788;&#29702;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#22823;&#22823;&#32553;&#30701;&#20102;&#35757;&#32451;&#26102;&#38388;&#12290;&#36890;&#36807;&#20219;&#21153;&#33258;&#36866;&#24212;&#30340;&#31232;&#30095;&#26356;&#26032;&#26041;&#27861;&#65292;TinyTrain&#33021;&#22815;&#22312;&#39640;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#20943;&#23567;&#35745;&#31639;&#21644;&#20869;&#23384;&#21344;&#29992;&#65292;&#23545;&#26410;&#30693;&#20219;&#21153;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#22791;&#19978;&#30340;&#35757;&#32451;&#23545;&#20110;&#29992;&#25143;&#20010;&#24615;&#21270;&#21644;&#38544;&#31169;&#33267;&#20851;&#37325;&#35201;&#12290;&#38543;&#30528;&#29289;&#32852;&#32593;&#35774;&#22791;&#21644;&#24494;&#25511;&#21046;&#22120;&#21333;&#20803;&#65288;MCU&#65289;&#30340;&#26222;&#21450;&#65292;&#30001;&#20110;&#21463;&#38480;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#36164;&#28304;&#20197;&#21450;&#26631;&#27880;&#30340;&#29992;&#25143;&#25968;&#25454;&#30340;&#26377;&#38480;&#21487;&#29992;&#24615;&#65292;&#36825;&#39033;&#20219;&#21153;&#21464;&#24471;&#26356;&#21152;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#24573;&#35270;&#20102;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#38656;&#35201;&#36807;&#38271;&#30340;&#35757;&#32451;&#26102;&#38388;&#65288;&#20363;&#22914;&#20960;&#20010;&#23567;&#26102;&#65289;&#65292;&#25110;&#32773;&#23548;&#33268;&#37325;&#22823;&#30340;&#20934;&#30830;&#24615;&#25439;&#22833;&#65288;&#8805;10%&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;TinyTrain&#65292;&#19968;&#31181;&#35774;&#22791;&#19978;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#26356;&#26032;&#27169;&#22411;&#30340;&#37096;&#20998;&#65292;&#24182;&#26126;&#30830;&#22788;&#29702;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#22823;&#24133;&#32553;&#30701;&#20102;&#35757;&#32451;&#26102;&#38388;&#12290;TinyTrain&#24341;&#20837;&#20102;&#19968;&#31181;&#20219;&#21153;&#33258;&#36866;&#24212;&#30340;&#31232;&#30095;&#26356;&#26032;&#26041;&#27861;&#65292;&#26681;&#25454;&#22810;&#30446;&#26631;&#20934;&#21017;&#21160;&#24577;&#36873;&#25321;&#23618;/&#36890;&#36947;&#65292;&#21516;&#26102;&#25429;&#25417;&#29992;&#25143;&#25968;&#25454;&#12289;&#20869;&#23384;&#21644;&#30446;&#26631;&#35774;&#22791;&#30340;&#35745;&#31639;&#33021;&#21147;&#65292;&#20174;&#32780;&#22312;&#26410;&#30693;&#20219;&#21153;&#19978;&#33719;&#24471;&#39640;&#20934;&#30830;&#24615;&#65292;&#24182;&#20943;&#23567;&#35745;&#31639;&#21644;&#20869;&#23384;&#21344;&#29992;&#12290;TinyTrain&#22312;&#25972;&#20307;&#24494;&#35843;&#30340;&#22522;&#30784;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
On-device training is essential for user personalisation and privacy. With the pervasiveness of IoT devices and microcontroller units (MCU), this task becomes more challenging due to the constrained memory and compute resources, and the limited availability of labelled user data. Nonetheless, prior works neglect the data scarcity issue, require excessively long training time (e.g. a few hours), or induce substantial accuracy loss ($\geq$10\%). We propose TinyTrain, an on-device training approach that drastically reduces training time by selectively updating parts of the model and explicitly coping with data scarcity. TinyTrain introduces a task-adaptive sparse-update method that dynamically selects the layer/channel based on a multi-objective criterion that jointly captures user data, the memory, and the compute capabilities of the target device, leading to high accuracy on unseen tasks with reduced computation and memory footprint. TinyTrain outperforms vanilla fine-tuning of the enti
&lt;/p&gt;</description></item><item><title>&#36825;&#26159;&#19968;&#20010;&#20851;&#20110;&#22270;&#20301;&#32622;&#21644;&#32467;&#26500;&#32534;&#30721;&#22120;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#22270;&#20301;&#32622;&#21644;&#32467;&#26500;&#32534;&#30721;&#22120;&#65288;GPSE&#65289;&#65292;&#23427;&#33021;&#26377;&#25928;&#22320;&#25429;&#25417;&#22810;&#20010;PSE&#30340;&#20849;&#21516;&#28508;&#22312;&#34920;&#31034;&#65292;&#24182;&#22312;&#21508;&#31181;&#22270;&#39044;&#27979;&#20219;&#21153;&#20013;&#21462;&#24471;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2307.07107</link><description>&lt;p&gt;
&#22270;&#20301;&#32622;&#21644;&#32467;&#26500;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Graph Positional and Structural Encoder. (arXiv:2307.07107v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07107
&lt;/p&gt;
&lt;p&gt;
&#36825;&#26159;&#19968;&#20010;&#20851;&#20110;&#22270;&#20301;&#32622;&#21644;&#32467;&#26500;&#32534;&#30721;&#22120;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#22270;&#20301;&#32622;&#21644;&#32467;&#26500;&#32534;&#30721;&#22120;&#65288;GPSE&#65289;&#65292;&#23427;&#33021;&#26377;&#25928;&#22320;&#25429;&#25417;&#22810;&#20010;PSE&#30340;&#20849;&#21516;&#28508;&#22312;&#34920;&#31034;&#65292;&#24182;&#22312;&#21508;&#31181;&#22270;&#39044;&#27979;&#20219;&#21153;&#20013;&#21462;&#24471;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20301;&#32622;&#21644;&#32467;&#26500;&#32534;&#30721;&#65288;PSE&#65289;&#21487;&#20197;&#26356;&#22909;&#22320;&#22312;&#22270;&#20013;&#35782;&#21035;&#33410;&#28857;&#65292;&#22240;&#20026;&#19968;&#33324;&#22270;&#32570;&#20047;&#35268;&#33539;&#30340;&#33410;&#28857;&#39034;&#24207;&#12290;&#36825;&#20351;&#24471;PSE&#25104;&#20026;&#36171;&#20104;&#29616;&#20195;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#21644;&#29305;&#21035;&#26159;&#22270;&#21464;&#25442;&#22120;&#37325;&#35201;&#21151;&#33021;&#30340;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#36866;&#29992;&#20110;&#21508;&#31181;&#22270;&#39044;&#27979;&#20219;&#21153;&#30340;PSE&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#19988;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22270;&#20301;&#32622;&#21644;&#32467;&#26500;&#32534;&#30721;&#22120;&#65288;GPSE&#65289;&#65292;&#36825;&#26159;&#39318;&#27425;&#23581;&#35797;&#35757;&#32451;&#19968;&#20010;&#33021;&#22815;&#25429;&#25417;&#20016;&#23500;&#30340;PSE&#34920;&#31034;&#20197;&#22686;&#24378;&#20219;&#20309;GNN&#30340;&#22270;&#32534;&#30721;&#22120;&#12290;GPSE&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#22810;&#20010;PSE&#30340;&#20849;&#21516;&#28508;&#22312;&#34920;&#31034;&#65292;&#24182;&#19988;&#20855;&#26377;&#39640;&#24230;&#21487;&#20256;&#36755;&#24615;&#12290;&#22312;&#29305;&#23450;&#22270;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;&#21487;&#20197;&#22312;&#20174;&#26174;&#33879;&#19981;&#21516;&#20998;&#24067;&#29978;&#33267;&#27169;&#24577;&#30340;&#25968;&#25454;&#38598;&#19978;&#26377;&#25928;&#22320;&#20351;&#29992;&#12290;&#25105;&#20204;&#26174;&#31034;&#65292;&#22312;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#32463;&#36807;GPSE&#22686;&#24378;&#30340;&#27169;&#22411;&#22312;&#26576;&#20123;&#20219;&#21153;&#20013;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#65292;&#21516;&#26102;&#19982;&#26126;&#30830;&#20351;&#29992;PSE&#30340;&#27169;&#22411;&#24615;&#33021;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Positional and structural encodings (PSE) enable better identifiability of nodes within a graph, as in general graphs lack a canonical node ordering. This renders PSEs essential tools for empowering modern GNNs, and in particular graph Transformers. However, designing PSEs that work optimally for a variety of graph prediction tasks is a challenging and unsolved problem. Here, we present the graph positional and structural encoder (GPSE), a first-ever attempt to train a graph encoder that captures rich PSE representations for augmenting any GNN. GPSE can effectively learn a common latent representation for multiple PSEs, and is highly transferable. The encoder trained on a particular graph dataset can be used effectively on datasets drawn from significantly different distributions and even modalities. We show that across a wide range of benchmarks, GPSE-enhanced models can significantly improve the performance in certain tasks, while performing on par with those that employ explicitly c
&lt;/p&gt;</description></item><item><title>S-HR-VQVAE&#26159;&#19968;&#31181;&#24207;&#21015;&#20998;&#23618;&#27531;&#24046;&#23398;&#20064;&#21521;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#32467;&#21512;&#20998;&#23618;&#27531;&#24046;&#21521;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;HR-VQVAE&#65289;&#21644;&#26102;&#31354;PixelCNN&#65288;ST-PixelCNN&#65289;&#30340;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;&#35270;&#39057;&#39044;&#27979;&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#24182;&#22312;KTH&#20154;&#20307;&#21160;&#20316;&#21644;Moving-MNIST&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.06701</link><description>&lt;p&gt;
S-HR-VQVAE: &#24207;&#21015;&#20998;&#23618;&#27531;&#24046;&#23398;&#20064;&#21521;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#35270;&#39057;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
S-HR-VQVAE: Sequential Hierarchical Residual Learning Vector Quantized Variational Autoencoder for Video Prediction. (arXiv:2307.06701v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06701
&lt;/p&gt;
&lt;p&gt;
S-HR-VQVAE&#26159;&#19968;&#31181;&#24207;&#21015;&#20998;&#23618;&#27531;&#24046;&#23398;&#20064;&#21521;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#32467;&#21512;&#20998;&#23618;&#27531;&#24046;&#21521;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;HR-VQVAE&#65289;&#21644;&#26102;&#31354;PixelCNN&#65288;ST-PixelCNN&#65289;&#30340;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;&#35270;&#39057;&#39044;&#27979;&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#24182;&#22312;KTH&#20154;&#20307;&#21160;&#20316;&#21644;Moving-MNIST&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#65292;&#23558;&#25105;&#20204;&#26368;&#36817;&#25552;&#20986;&#30340;&#20998;&#23618;&#27531;&#24046;&#21521;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;HR-VQVAE&#65289;&#19982;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#31354;PixelCNN&#65288;ST-PixelCNN&#65289;&#30456;&#32467;&#21512;&#65292;&#29992;&#26469;&#35299;&#20915;&#35270;&#39057;&#39044;&#27979;&#20219;&#21153;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#31216;&#20026;&#24207;&#21015;&#20998;&#23618;&#27531;&#24046;&#23398;&#20064;&#21521;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;S-HR-VQVAE&#65289;&#12290;&#36890;&#36807;&#21033;&#29992;HR-VQVAE&#22312;&#23545;&#38745;&#27490;&#22270;&#20687;&#36827;&#34892;&#24314;&#27169;&#26102;&#30340;&#20869;&#22312;&#33021;&#21147;&#21644;&#32039;&#20945;&#34920;&#31034;&#65292;&#20197;&#21450;ST-PixelCNN&#22788;&#29702;&#26102;&#31354;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292; S-HR-VQVAE&#33021;&#22815;&#26356;&#22909;&#22320;&#24212;&#23545;&#35270;&#39057;&#39044;&#27979;&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#21253;&#25324;&#23398;&#20064;&#26102;&#31354;&#20449;&#24687;&#12289;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#12289;&#28040;&#38500;&#27169;&#31946;&#39044;&#27979;&#21644;&#38544;&#24335;&#24314;&#27169;&#29289;&#29702;&#29305;&#24615;&#12290;&#23545;KTH&#20154;&#20307;&#21160;&#20316;&#21644;Moving-MNIST&#20219;&#21153;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#26041;&#38754;&#19982;&#39030;&#32423;&#35270;&#39057;&#39044;&#27979;&#25216;&#26415;&#30456;&#27604;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address the video prediction task by putting forth a novel model that combines (i) our recently proposed hierarchical residual vector quantized variational autoencoder (HR-VQVAE), and (ii) a novel spatiotemporal PixelCNN (ST-PixelCNN). We refer to this approach as a sequential hierarchical residual learning vector quantized variational autoencoder (S-HR-VQVAE). By leveraging the intrinsic capabilities of HR-VQVAE at modeling still images with a parsimonious representation, combined with the ST-PixelCNN's ability at handling spatiotemporal information, S-HR-VQVAE can better deal with chief challenges in video prediction. These include learning spatiotemporal information, handling high dimensional data, combating blurry prediction, and implicit modeling of physical characteristics. Extensive experimental results on the KTH Human Action and Moving-MNIST tasks demonstrate that our model compares favorably against top video prediction techniques both in quantitative and qualitative evalu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#22788;&#29702;&#20013;&#32452;&#20844;&#24179;&#26041;&#27861;&#30340;&#20844;&#24179;&#20844;&#27491;&#22522;&#20934;&#26694;&#26550;&#65288;FFB&#65289;&#65292;&#24182;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#12290;&#35813;&#24037;&#20316;&#30340;&#20851;&#38190;&#36129;&#29486;&#21253;&#25324;&#25552;&#20379;&#28789;&#27963;&#12289;&#21487;&#25193;&#23637;&#12289;&#26497;&#31616;&#21644;&#38754;&#21521;&#30740;&#31350;&#30340;&#24320;&#28304;&#20195;&#30721;&#65307;&#24314;&#31435;&#32479;&#19968;&#30340;&#20844;&#24179;&#26041;&#27861;&#22522;&#20934;&#27979;&#35797;&#27969;&#27700;&#32447;&#65307;&#36827;&#34892;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20174; $\mathbf{45,079}$ &#20010;&#23454;&#39564;&#20013;&#33719;&#21462;&#20851;&#38190;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2306.09468</link><description>&lt;p&gt;
FFB:&#38754;&#21521;&#22788;&#29702;&#32452;&#20844;&#24179;&#26041;&#27861;&#30340;&#20844;&#24179;&#20844;&#27491;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
FFB: A Fair Fairness Benchmark for In-Processing Group Fairness Methods. (arXiv:2306.09468v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#22788;&#29702;&#20013;&#32452;&#20844;&#24179;&#26041;&#27861;&#30340;&#20844;&#24179;&#20844;&#27491;&#22522;&#20934;&#26694;&#26550;&#65288;FFB&#65289;&#65292;&#24182;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#12290;&#35813;&#24037;&#20316;&#30340;&#20851;&#38190;&#36129;&#29486;&#21253;&#25324;&#25552;&#20379;&#28789;&#27963;&#12289;&#21487;&#25193;&#23637;&#12289;&#26497;&#31616;&#21644;&#38754;&#21521;&#30740;&#31350;&#30340;&#24320;&#28304;&#20195;&#30721;&#65307;&#24314;&#31435;&#32479;&#19968;&#30340;&#20844;&#24179;&#26041;&#27861;&#22522;&#20934;&#27979;&#35797;&#27969;&#27700;&#32447;&#65307;&#36827;&#34892;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20174; $\mathbf{45,079}$ &#20010;&#23454;&#39564;&#20013;&#33719;&#21462;&#20851;&#38190;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20844;&#24179;&#20844;&#27491;&#22522;&#20934;&#65288;FFB&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#22788;&#29702;&#20013;&#32452;&#20844;&#24179;&#26041;&#27861;&#30340;&#22522;&#20934;&#26694;&#26550;&#12290;&#30830;&#20445;&#26426;&#22120;&#23398;&#20064;&#30340;&#20844;&#24179;&#24615;&#23545;&#20110;&#31526;&#21512;&#36947;&#24503;&#21644;&#27861;&#24459;&#35201;&#27714;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23454;&#39564;&#35774;&#32622;&#30340;&#19981;&#19968;&#33268;&#65292;&#32570;&#20047;&#26131;&#20110;&#35775;&#38382;&#30340;&#31639;&#27861;&#23454;&#29616;&#20197;&#21450;&#24403;&#21069;&#20844;&#24179;&#24230;&#37327;&#24037;&#20855;&#30340;&#26377;&#38480;&#21487;&#25193;&#23637;&#24615;&#65292;&#23384;&#22312;&#27604;&#36739;&#21644;&#24320;&#21457;&#20844;&#24179;&#24230;&#37327;&#26041;&#27861;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#24320;&#28304;&#12289;&#26631;&#20934;&#21270;&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22788;&#29702;&#20013;&#30340;&#32452;&#20844;&#24179;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#30830;&#20445;&#19981;&#21516;&#27665;&#26063;/&#31181;&#26063;&#32676;&#20307;&#20844;&#24179;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#20840;&#38754;&#20998;&#26512;&#12290;&#35813;&#24037;&#20316;&#25552;&#20379;&#20102;&#20197;&#19979;&#20851;&#38190;&#36129;&#29486;&#65306;&#25552;&#20379;&#28789;&#27963;&#12289;&#21487;&#25193;&#23637;&#12289;&#26497;&#31616;&#21644;&#38754;&#21521;&#30740;&#31350;&#30340;&#24320;&#28304;&#20195;&#30721;&#65307;&#24314;&#31435;&#32479;&#19968;&#30340;&#20844;&#24179;&#26041;&#27861;&#22522;&#20934;&#27979;&#35797;&#27969;&#27700;&#32447;&#65307;&#36827;&#34892;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20174; $\mathbf{45,079}$ &#20010;&#23454;&#39564;&#20013;&#33719;&#21462;&#20851;&#38190;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces the Fair Fairness Benchmark (\textsf{FFB}), a benchmarking framework for in-processing group fairness methods. Ensuring fairness in machine learning is critical for ethical and legal compliance. However, there exist challenges in comparing and developing of fairness methods due to inconsistencies in experimental settings, lack of accessible algorithmic implementations, and limited extensibility of current fairness packages and tools. To address these issues, we introduce an open-source, standardized benchmark for evaluating in-processing group fairness methods and provide a comprehensive analysis of state-of-the-art methods to ensure different notions of group fairness. This work offers the following key contributions: the provision of flexible, extensible, minimalistic, and research-oriented open-source code; the establishment of unified fairness method benchmarking pipelines; and extensive benchmarking, which yields key insights from $\mathbf{45,079}$ experiment
&lt;/p&gt;</description></item><item><title>ShiftAddViT&#36890;&#36807;&#20351;&#29992;&#20301;&#31227;&#21644;&#21152;&#27861;&#31561;&#22810;&#31181;&#20056;&#27861;&#21407;&#35821;&#23545;ViT&#36827;&#34892;&#37325;&#26032;&#21442;&#25968;&#21270;&#65292;&#23454;&#29616;&#20102;&#20943;&#23569;&#20056;&#27861;&#25805;&#20316;&#30340;&#39640;&#25928;&#35270;&#35273;&#21464;&#25442;&#22120;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;GPU&#19978;&#23454;&#29616;&#31471;&#21040;&#31471;&#30340;&#25512;&#29702;&#21152;&#36895;&#65292;&#26080;&#38656;&#20174;&#22836;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2306.06446</link><description>&lt;p&gt;
ShiftAddViT&#65306;&#22810;&#31181;&#20056;&#27861;&#21407;&#35821;&#28151;&#21512;&#23454;&#29616;&#39640;&#25928;&#30340;&#35270;&#35273;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
ShiftAddViT: Mixture of Multiplication Primitives Towards Efficient Vision Transformer. (arXiv:2306.06446v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06446
&lt;/p&gt;
&lt;p&gt;
ShiftAddViT&#36890;&#36807;&#20351;&#29992;&#20301;&#31227;&#21644;&#21152;&#27861;&#31561;&#22810;&#31181;&#20056;&#27861;&#21407;&#35821;&#23545;ViT&#36827;&#34892;&#37325;&#26032;&#21442;&#25968;&#21270;&#65292;&#23454;&#29616;&#20102;&#20943;&#23569;&#20056;&#27861;&#25805;&#20316;&#30340;&#39640;&#25928;&#35270;&#35273;&#21464;&#25442;&#22120;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;GPU&#19978;&#23454;&#29616;&#31471;&#21040;&#31471;&#30340;&#25512;&#29702;&#21152;&#36895;&#65292;&#26080;&#38656;&#20174;&#22836;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#21464;&#25442;&#22120;&#65288;ViT&#65289;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#24182;&#25104;&#20026;&#22810;&#20010;&#35270;&#35273;&#20219;&#21153;&#30340;&#32479;&#19968;&#39592;&#24178;&#12290;&#20294;&#26159;&#65292;ViTs&#20013;&#30340;&#27880;&#24847;&#21147;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLPs&#65289;&#30001;&#20110;&#23494;&#38598;&#30340;&#20056;&#27861;&#32780;&#19981;&#22815;&#39640;&#25928;&#65292;&#23548;&#33268;&#35757;&#32451;&#21644;&#25512;&#29702;&#20195;&#20215;&#39640;&#26114;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#39044;&#35757;&#32451;&#30340;ViT&#20197;&#22810;&#31181;&#20056;&#27861;&#21407;&#35821;&#65288;&#20363;&#22914;&#20301;&#31227;&#21644;&#21152;&#27861;&#65289;&#37325;&#26032;&#21442;&#25968;&#21270;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#20840;&#26032;&#31867;&#22411;&#30340;&#20943;&#23569;&#20056;&#27861;&#30340;&#27169;&#22411;&#65292;&#31216;&#20026;ShiftAddViT&#65292;&#26088;&#22312;&#23454;&#29616;GPU&#19978;&#30340;&#31471;&#21040;&#31471;&#25512;&#29702;&#21152;&#36895;&#65292;&#26080;&#38656;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#26597;&#35810;&#21644;&#38190;&#26144;&#23556;&#20026;&#27721;&#26126;&#31354;&#38388;&#20013;&#30340;&#20108;&#36827;&#21046;&#30721;&#20043;&#21518;&#65292;&#37319;&#29992;&#21152;&#27861;&#26680;&#23545;&#26597;&#35810;&#12289;&#38190;&#21644;&#20540;&#20043;&#38388;&#30340;MatMul&#36827;&#34892;&#37325;&#26032;&#21442;&#25968;&#21270;&#12290;&#21097;&#20313;&#30340;MLPs&#25110;&#32447;&#24615;&#23618;&#21017;&#37319;&#29992;&#20301;&#31227;&#26680;&#36827;&#34892;&#37325;&#26032;&#21442;&#25968;&#21270;&#12290;&#25105;&#20204;&#21033;&#29992;TVM&#22312;GPU&#19978;&#23454;&#26045;&#24182;&#20248;&#21270;&#36825;&#20123;&#23450;&#21046;&#26680;&#65292;&#20197;&#23454;&#29616;&#23454;&#38469;&#30828;&#20214;&#37096;&#32626;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#31181;&#37325;&#26032;&#21442;&#25968;&#21270;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25512;&#29702;&#36895;&#24230;&#65292;&#32780;&#26080;&#38656;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision Transformers (ViTs) have shown impressive performance and have become a unified backbone for multiple vision tasks. But both attention and multi-layer perceptions (MLPs) in ViTs are not efficient enough due to dense multiplications, resulting in costly training and inference. To this end, we propose to reparameterize the pre-trained ViT with a mixture of multiplication primitives, e.g., bitwise shifts and additions, towards a new type of multiplication-reduced model, dubbed $\textbf{ShiftAddViT}$, which aims for end-to-end inference speedups on GPUs without the need of training from scratch. Specifically, all $\texttt{MatMuls}$ among queries, keys, and values are reparameterized by additive kernels, after mapping queries and keys to binary codes in Hamming space. The remaining MLPs or linear layers are then reparameterized by shift kernels. We utilize TVM to implement and optimize those customized kernels for practical hardware deployment on GPUs. We find that such a reparameter
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#37492;&#21035;&#20986;&#19977;&#31181;&#19981;&#21516;&#30340;&#24085;&#37329;&#26862;&#30149;&#20122;&#22411;&#65292;&#24182;&#29983;&#25104;&#24739;&#32773;&#20010;&#24615;&#21270;&#30340;&#30151;&#29366;&#36827;&#23637;&#39044;&#27979;&#65292;&#26377;&#21161;&#20110;&#21046;&#23450;&#38024;&#23545;&#24615;&#24178;&#39044;&#21644;&#25913;&#21892;&#24739;&#32773;&#39044;&#21518;&#12290;</title><link>http://arxiv.org/abs/2306.04748</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20998;&#26512;&#12289;&#35782;&#21035;&#21644;&#39044;&#27979;&#24085;&#37329;&#26862;&#30149;&#20122;&#22411;&#21644;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
Analysis, Identification and Prediction of Parkinson's disease sub-types and progression through Machine Learning. (arXiv:2306.04748v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#37492;&#21035;&#20986;&#19977;&#31181;&#19981;&#21516;&#30340;&#24085;&#37329;&#26862;&#30149;&#20122;&#22411;&#65292;&#24182;&#29983;&#25104;&#24739;&#32773;&#20010;&#24615;&#21270;&#30340;&#30151;&#29366;&#36827;&#23637;&#39044;&#27979;&#65292;&#26377;&#21161;&#20110;&#21046;&#23450;&#38024;&#23545;&#24615;&#24178;&#39044;&#21644;&#25913;&#21892;&#24739;&#32773;&#39044;&#21518;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24085;&#37329;&#26862;&#30149;&#65288;PD&#65289;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#65292;&#24739;&#32773;&#34920;&#29616;&#22810;&#31181;&#22810;&#26679;&#65292;&#20294;&#23545;&#20854;&#28508;&#22312;&#21407;&#22240;&#21644;&#30151;&#29366;&#36827;&#23637;&#30340;&#20102;&#35299;&#36824;&#24456;&#26377;&#38480;&#12290;&#24085;&#37329;&#26862;&#36827;&#23637;&#26631;&#24535;&#29289;&#35745;&#21010;&#65288;PPMI&#65289;&#25910;&#38598;&#20102;&#22810;&#20010;&#24739;&#32773;&#32676;&#30340;&#35814;&#32454;&#32437;&#21521;&#25968;&#25454;&#20197;&#30830;&#23450;&#29983;&#29289;&#26631;&#24535;&#29289;&#24182;&#36741;&#21161;&#24178;&#39044;&#26041;&#27861;&#30340;&#21046;&#23450;&#12290;&#23613;&#31649;&#24050;&#32463;&#26377;&#36229;&#36807;110&#20010;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20351;&#29992;PPMI&#25968;&#25454;&#24211;&#65292;&#20294;&#22823;&#37096;&#20998;&#30740;&#31350;&#20165;&#32858;&#28966;&#20110;&#30417;&#30563;&#27169;&#22411;&#29992;&#20110;&#35786;&#26029;&#39044;&#27979;&#65292;&#38480;&#21046;&#20102;&#23545;&#24739;&#32773;&#22810;&#26679;&#24615;&#21644;&#36827;&#23637;&#30340;&#35748;&#35782;&#12290;&#26412;&#25991;&#36890;&#36807;&#32467;&#21512;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#37492;&#21035;&#20122;&#22411;&#65292;&#20934;&#30830;&#39044;&#27979;&#24085;&#37329;&#26862;&#24739;&#32773;&#30142;&#30149;&#36827;&#31243;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;PPMI&#25968;&#25454;&#24211;&#20013;5&#24180;&#30340;&#32437;&#21521;&#25968;&#25454;&#24182;&#22312;&#20043;&#21069;&#30740;&#31350;&#30340;&#22522;&#30784;&#19978;&#65292;&#25972;&#21512;&#26080;&#30417;&#30563;&#30340;&#30149;&#20154;&#32858;&#31867;&#21644;&#30149;&#20154;&#24403;&#21069;&#21644;&#26410;&#26469;&#30151;&#29366;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37492;&#21035;&#20986;&#19977;&#20010;&#19981;&#21516;&#30340;&#24085;&#37329;&#26862;&#30149;&#20122;&#22411;&#65292;&#24182;&#20026;&#20010;&#20307;&#24739;&#32773;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#30151;&#29366;&#36827;&#23637;&#39044;&#27979;&#12290;&#36825;&#31181;&#26041;&#27861;&#25552;&#39640;&#20102;&#39044;&#27979;&#30142;&#30149;&#36827;&#23637;&#30340;&#20934;&#30830;&#24615;&#65292;&#24110;&#21161;&#21046;&#23450;&#20010;&#24615;&#21270;&#24178;&#39044;&#26041;&#27861;&#24182;&#25913;&#21892;&#24739;&#32773;&#30340;&#39044;&#21518;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parkinson's disease (PD) is a prevalent neurodegenerative disorder with varying patient trajectories, yet little is understood about the underlying causes and symptom progression. The Parkinson's Progression Markers Initiative (PPMI) has collected comprehensive longitudinal data from diverse patient cohorts to identify biomarkers and aid in the development of interventions. Despite over 110 machine learning studies using the PPMI database, the majority have focused on supervised models for diagnosis prediction, which has limited impact on understanding patient variability and progression. This paper addresses this gap by combining supervised and unsupervised machine learning methods to identify subtypes that accurately predict disease progression in Parkinson's patients. Building upon previous work, we replicate and extend the study by integrating unsupervised patient clustering and prediction of present and future symptoms using 5 additional years of longitudinal data from the Progres
&lt;/p&gt;</description></item><item><title>&#31232;&#30095;&#24615;&#33021;&#22815;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#30340;&#38544;&#31169;&#65292;&#24182;&#19988;&#33021;&#22815;&#20445;&#25345;&#32593;&#32476;&#30340;&#34920;&#29616;</title><link>http://arxiv.org/abs/2304.10553</link><description>&lt;p&gt;
&#31232;&#30095;&#24615;&#33021;&#22815;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#30340;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
Sparsity in neural networks can improve their privacy. (arXiv:2304.10553v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10553
&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#24615;&#33021;&#22815;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#30340;&#38544;&#31169;&#65292;&#24182;&#19988;&#33021;&#22815;&#20445;&#25345;&#32593;&#32476;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#31232;&#30095;&#24615;&#22914;&#20309;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#23545;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#31232;&#30095;&#24615;&#33021;&#22815;&#25552;&#39640;&#32593;&#32476;&#30340;&#38544;&#31169;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#22312;&#20219;&#21153;&#19978;&#30340;&#30456;&#20284;&#34920;&#29616;&#12290;&#36825;&#20010;&#23454;&#35777;&#30740;&#31350;&#23436;&#21892;&#21644;&#25193;&#23637;&#20102;&#29616;&#26377;&#25991;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article measures how sparsity can make neural networks more robust to membership inference attacks. The obtained empirical results show that sparsity improves the privacy of the network, while preserving comparable performances on the task at hand. This empirical study completes and extends existing literature.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#22312;&#24322;&#36136;&#24615;&#19979;&#26080;&#27861;&#23398;&#20064;&#39640;&#36136;&#37327;&#34920;&#31034;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.06344</link><description>&lt;p&gt;
&#24322;&#36136;&#24615;&#19979;&#30340;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Contrastive Learning under Heterophily. (arXiv:2303.06344v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06344
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#22312;&#24322;&#36136;&#24615;&#19979;&#26080;&#27861;&#23398;&#20064;&#39640;&#36136;&#37327;&#34920;&#31034;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes the first graph contrastive learning method to address the problem that existing graph contrastive learning methods cannot learn high-quality representations under heterophily.
&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#22312;&#20855;&#26377;&#29305;&#23450;&#20219;&#21153;&#33410;&#28857;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#33410;&#28857;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#20026;&#22270;&#24418;&#33719;&#21462;&#26631;&#31614;&#26159;&#26114;&#36149;&#30340;&#12290;&#36825;&#22312;&#22823;&#22411;&#22270;&#24418;&#30340;&#24773;&#20917;&#19979;&#23588;&#20854;&#22914;&#27492;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24050;&#32463;&#26377;&#19968;&#20123;&#24037;&#20316;&#22312;&#27809;&#26377;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#20197;&#33258;&#30417;&#30563;&#30340;&#26041;&#24335;&#23398;&#20064;&#33410;&#28857;&#34920;&#31034;&#12290;&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#22312;&#20197;&#33258;&#30417;&#30563;&#30340;&#26041;&#24335;&#23398;&#20064;&#34920;&#31034;&#26041;&#38754;&#29305;&#21035;&#21463;&#27426;&#36814;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;CL&#26041;&#27861;&#36890;&#36807;&#26368;&#22823;&#21270;&#30456;&#21516;&#31034;&#20363;&#30340;&#22686;&#24378;&#35270;&#22270;&#30340;&#34920;&#31034;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#26368;&#23567;&#21270;&#19981;&#21516;&#31034;&#20363;&#30340;&#22686;&#24378;&#35270;&#22270;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26469;&#24037;&#20316;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22270;&#24418;CL&#26041;&#27861;&#19981;&#33021;&#22312;&#24322;&#36136;&#24615;&#19979;&#23398;&#20064;&#39640;&#36136;&#37327;&#30340;&#34920;&#31034;&#65292;&#20854;&#20013;&#36830;&#25509;&#30340;&#33410;&#28857;&#20542;&#21521;&#20110;&#23646;&#20110;&#19981;&#21516;&#30340;&#31867;&#12290;&#36825;&#26159;&#22240;&#20026;&#22312;&#24322;&#36136;&#24615;&#19979;&#65292;&#21516;&#19968;&#31034;&#20363;&#30340;&#22686;&#24378;&#21487;&#33021;&#24444;&#27492;&#19981;&#30456;&#20284;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#31532;&#19968;&#20010;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks are powerful tools for learning node representations when task-specific node labels are available. However, obtaining labels for graphs is expensive in many applications. This is particularly the case for large graphs. To address this, there has been a body of work to learn node representations in a self-supervised manner without labels. Contrastive learning (CL), has been particularly popular to learn representations in a self-supervised manner. In general, CL methods work by maximizing the similarity between representations of augmented views of the same example, and minimizing the similarity between augmented views of different examples. However, existing graph CL methods cannot learn high-quality representations under heterophily, where connected nodes tend to belong to different classes. This is because under heterophily, augmentations of the same example may not be similar to each other. In this work, we address the above problem by proposing the first graph
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#21487;&#21464;&#22823;&#23567;&#21387;&#32553;&#24615;&#26694;&#26550;&#65292;&#24314;&#31435;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#30456;&#20851;&#30340;&#27867;&#21270;&#35823;&#24046;&#19978;&#30028;&#12290;&#35813;&#26041;&#27861;&#23558;&#31639;&#27861;&#30340;&#27867;&#21270;&#35823;&#24046;&#19982;&#20854;&#36755;&#20837;&#25968;&#25454;&#30340;&#21487;&#21464;&#22823;&#23567;&#21387;&#32553;&#29575;&#30456;&#20851;&#32852;&#65292;&#24182;&#25552;&#20379;&#20102;&#20381;&#36182;&#20110;&#32463;&#39564;&#20998;&#24067;&#32780;&#38750;&#26410;&#30693;&#20998;&#24067;&#30340;&#30028;&#38480;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#36824;&#21487;&#20197;&#25512;&#23548;&#20986;&#36755;&#20837;&#25968;&#25454;&#21644;&#36755;&#20986;&#20551;&#35774;&#38543;&#26426;&#21464;&#37327;&#30340;&#20219;&#20309;&#20989;&#25968;&#30340;&#27867;&#21270;&#30028;&#38480;&#65292;&#24182;&#21253;&#21547;&#24182;&#21487;&#33021;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;PAC-Bayes&#21644;&#25968;&#25454;&#30456;&#20851;&#20869;&#22312;&#32500;&#24230;&#30340;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2303.05369</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#21464;&#22823;&#23567;&#30340;&#21387;&#32553;&#24615;&#24314;&#31435;&#25968;&#25454;&#30456;&#20851;&#30340;&#27867;&#21270;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Data-dependent Generalization Bounds via Variable-Size Compressibility. (arXiv:2303.05369v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05369
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#21487;&#21464;&#22823;&#23567;&#21387;&#32553;&#24615;&#26694;&#26550;&#65292;&#24314;&#31435;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#30456;&#20851;&#30340;&#27867;&#21270;&#35823;&#24046;&#19978;&#30028;&#12290;&#35813;&#26041;&#27861;&#23558;&#31639;&#27861;&#30340;&#27867;&#21270;&#35823;&#24046;&#19982;&#20854;&#36755;&#20837;&#25968;&#25454;&#30340;&#21487;&#21464;&#22823;&#23567;&#21387;&#32553;&#29575;&#30456;&#20851;&#32852;&#65292;&#24182;&#25552;&#20379;&#20102;&#20381;&#36182;&#20110;&#32463;&#39564;&#20998;&#24067;&#32780;&#38750;&#26410;&#30693;&#20998;&#24067;&#30340;&#30028;&#38480;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#36824;&#21487;&#20197;&#25512;&#23548;&#20986;&#36755;&#20837;&#25968;&#25454;&#21644;&#36755;&#20986;&#20551;&#35774;&#38543;&#26426;&#21464;&#37327;&#30340;&#20219;&#20309;&#20989;&#25968;&#30340;&#27867;&#21270;&#30028;&#38480;&#65292;&#24182;&#21253;&#21547;&#24182;&#21487;&#33021;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;PAC-Bayes&#21644;&#25968;&#25454;&#30456;&#20851;&#20869;&#22312;&#32500;&#24230;&#30340;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#8220;&#21487;&#21464;&#22823;&#23567;&#21387;&#32553;&#24615;&#8221;&#26694;&#26550;&#65292;&#24314;&#31435;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#30456;&#20851;&#27867;&#21270;&#35823;&#24046;&#30340;&#19978;&#30028;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#31639;&#27861;&#30340;&#27867;&#21270;&#35823;&#24046;&#19982;&#20854;&#36755;&#20837;&#25968;&#25454;&#30340;&#21487;&#21464;&#22823;&#23567;&#8220;&#21387;&#32553;&#29575;&#8221;&#30456;&#20851;&#32852;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#24471;&#21040;&#30340;&#30028;&#38480;&#20381;&#36182;&#20110;&#25163;&#22836;&#32473;&#23450;&#36755;&#20837;&#25968;&#25454;&#30340;&#32463;&#39564;&#20998;&#24067;&#65292;&#32780;&#19981;&#26159;&#20854;&#26410;&#30693;&#20998;&#24067;&#12290;&#25105;&#20204;&#24314;&#31435;&#30340;&#26032;&#30340;&#27867;&#21270;&#30028;&#38480;&#21253;&#25324;&#23614;&#37096;&#30028;&#38480;&#12289;&#26399;&#26395;&#20540;&#30340;&#23614;&#37096;&#30028;&#38480;&#21644;&#26399;&#26395;&#30028;&#38480;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#36824;&#21487;&#20197;&#25512;&#23548;&#20986;&#23545;&#36755;&#20837;&#25968;&#25454;&#21644;&#36755;&#20986;&#20551;&#35774;&#38543;&#26426;&#21464;&#37327;&#30340;&#20219;&#20309;&#20989;&#25968;&#30340;&#27867;&#21270;&#30028;&#38480;&#12290;&#29305;&#21035;&#26159;&#65292;&#36825;&#20123;&#27867;&#21270;&#30028;&#38480;&#21253;&#21547;&#24182;&#21487;&#33021;&#20248;&#20110;&#20960;&#31181;&#29616;&#26377;&#30340;&#22522;&#20110;PAC-Bayes&#21644;&#25968;&#25454;&#30456;&#20851;&#20869;&#22312;&#32500;&#24230;&#30340;&#30028;&#38480;&#65292;&#36825;&#20123;&#30028;&#38480;&#20316;&#20026;&#29305;&#27530;&#24773;&#20917;&#24471;&#21040;&#22797;&#21407;&#65292;&#20174;&#32780;&#25581;&#31034;&#20986;&#25105;&#20204;&#26041;&#27861;&#30340;&#32479;&#19968;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we establish novel data-dependent upper bounds on the generalization error through the lens of a "variable-size compressibility" framework that we introduce newly here. In this framework, the generalization error of an algorithm is linked to a variable-size 'compression rate' of its input data. This is shown to yield bounds that depend on the empirical measure of the given input data at hand, rather than its unknown distribution. Our new generalization bounds that we establish are tail bounds, tail bounds on the expectation, and in-expectations bounds. Moreover, it is shown that our framework also allows to derive general bounds on any function of the input data and output hypothesis random variables. In particular, these general bounds are shown to subsume and possibly improve over several existing PAC-Bayes and data-dependent intrinsic dimension-based bounds that are recovered as special cases, thus unveiling a unifying character of our approach. For instance, a new da
&lt;/p&gt;</description></item><item><title>FairShap&#26159;&#19968;&#31181;&#36890;&#36807;&#25968;&#25454;&#37325;&#20272;&#35745;&#31639;&#27861;&#20915;&#31574;&#20844;&#24179;&#24615;&#30340;&#39044;&#22788;&#29702;&#26041;&#27861;&#65292;&#20351;&#29992;Shapley&#20540;&#20272;&#20540;&#65292;&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#21644;&#31934;&#30830;&#24615;&#65292;&#24182;&#19988;&#26131;&#20110;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2303.01928</link><description>&lt;p&gt;
&#22522;&#20110;Shapley&#20540;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#25968;&#25454;&#20877;&#21152;&#26435;&#26041;&#27861;FairShap
&lt;/p&gt;
&lt;p&gt;
FairShap: A Data Re-weighting Approach for Algorithmic Fairness based on Shapley Values. (arXiv:2303.01928v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01928
&lt;/p&gt;
&lt;p&gt;
FairShap&#26159;&#19968;&#31181;&#36890;&#36807;&#25968;&#25454;&#37325;&#20272;&#35745;&#31639;&#27861;&#20915;&#31574;&#20844;&#24179;&#24615;&#30340;&#39044;&#22788;&#29702;&#26041;&#27861;&#65292;&#20351;&#29992;Shapley&#20540;&#20272;&#20540;&#65292;&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#21644;&#31934;&#30830;&#24615;&#65292;&#24182;&#19988;&#26131;&#20110;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31639;&#27861;&#20844;&#24179;&#24615;&#26159;&#26497;&#20854;&#37325;&#35201;&#30340;&#31038;&#20250;&#38382;&#39064;&#65292;&#28982;&#32780;&#24403;&#21069;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36235;&#21183;&#35201;&#27714;&#20351;&#29992;&#36890;&#24120;&#23384;&#22312;&#20559;&#24046;&#30340;&#28023;&#37327;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#19987;&#27880;&#20110;&#24314;&#27169;&#21644;&#32416;&#27491;&#25968;&#25454;&#20559;&#24046;&#30340;&#39044;&#22788;&#29702;&#26041;&#27861;&#25104;&#20026;&#26377;&#20215;&#20540;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;Shapley&#20540;&#36827;&#34892;&#25968;&#25454;&#20272;&#20540;&#30340;&#39044;&#22788;&#29702;&#65288;&#20877;&#21152;&#26435;&#65289;&#26041;&#27861;FairShap&#65292;&#29992;&#20110;&#20844;&#24179;&#30340;&#31639;&#27861;&#20915;&#31574;&#21046;&#23450;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#27169;&#22411;&#26080;&#20851;&#19988;&#26131;&#20110;&#35299;&#37322;&#65292;&#22240;&#20026;&#23427;&#34913;&#37327;&#27599;&#20010;&#35757;&#32451;&#25968;&#25454;&#28857;&#23545;&#39044;&#23450;&#20041;&#30340;&#20844;&#24179;&#25351;&#26631;&#30340;&#36129;&#29486;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#26368;&#20808;&#36827;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#39564;&#35777;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#20855;&#26377;&#19981;&#21516;&#30340;&#24615;&#36136;&#65292;&#26377;&#21508;&#31181;&#22521;&#35757;&#22330;&#26223;&#21644;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22914;&#20309;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#20135;&#29983;&#26356;&#20844;&#24179;&#30340;&#27169;&#22411;&#24182;&#19988;&#20934;&#30830;&#24230;&#26356;&#39640;&#25110;&#30456;&#20284;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#30452;&#26041;&#22270;&#21644;&#28508;&#31354;&#38388;&#21487;&#35270;&#21270;&#26469;&#35828;&#26126;FairShap&#30340;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#23545;&#20110;&#22312;&#22823;&#25968;&#25454;&#26102;&#20195;&#30830;&#20445;&#31639;&#27861;&#20915;&#31574;&#20844;&#24179;&#24615;&#26159;&#37325;&#35201;&#30340;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithmic fairness is of utmost societal importance, yet the current trend in large-scale machine learning models requires training with massive datasets that are typically biased. In this context, pre-processing methods that focus on modeling and correcting bias in the data emerge as valuable approaches. In this paper, we propose FairShap, a novel pre-processing (re-weighting) method for fair algorithmic decision-making through data valuation by means of Shapley Values. Our approach is model agnostic and easily interpretable, as it measures the contribution of each training data point to a predefined fairness metric. We empirically validate FairShap on several state-of-the-art datasets of different nature, with a variety of training scenarios and models and show how it outperforms other methods, yielding fairer models with higher or similar levels of accuracy. We also illustrate FairShap's interpretability by means of histograms and latent space visualizations. We believe that this 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#40723;&#21169;&#30446;&#26631;&#22495;&#20013;&#30340;&#39044;&#27979;&#19982;&#20854;&#21069;&#20960;&#20010;&#22855;&#24322;&#21521;&#37327;&#23545;&#40784;&#26469;&#23454;&#29616;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#36825;&#20010;&#26041;&#27861;&#36890;&#36807;&#27491;&#21017;&#21270;&#20998;&#31867;&#22120;&#19982;&#26080;&#30417;&#30563;&#30446;&#26631;&#25968;&#25454;&#23545;&#40784;&#65292;&#32780;&#19981;&#26159;&#27491;&#21017;&#21270;&#34920;&#31034;&#12290;&#36890;&#36807;&#28040;&#38500;&#23545;&#26368;&#20248;&#32852;&#21512;&#39118;&#38505;&#20551;&#35774;&#30340;&#20381;&#36182;&#65292;&#35813;&#26041;&#27861;&#23637;&#31034;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2211.14960</link><description>&lt;p&gt;
&#20998;&#24067;&#20559;&#31227;&#30340;&#26631;&#31614;&#23545;&#40784;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Label Alignment Regularization for Distribution Shift. (arXiv:2211.14960v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14960
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#40723;&#21169;&#30446;&#26631;&#22495;&#20013;&#30340;&#39044;&#27979;&#19982;&#20854;&#21069;&#20960;&#20010;&#22855;&#24322;&#21521;&#37327;&#23545;&#40784;&#26469;&#23454;&#29616;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#36825;&#20010;&#26041;&#27861;&#36890;&#36807;&#27491;&#21017;&#21270;&#20998;&#31867;&#22120;&#19982;&#26080;&#30417;&#30563;&#30446;&#26631;&#25968;&#25454;&#23545;&#40784;&#65292;&#32780;&#19981;&#26159;&#27491;&#21017;&#21270;&#34920;&#31034;&#12290;&#36890;&#36807;&#28040;&#38500;&#23545;&#26368;&#20248;&#32852;&#21512;&#39118;&#38505;&#20551;&#35774;&#30340;&#20381;&#36182;&#65292;&#35813;&#26041;&#27861;&#23637;&#31034;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#26631;&#31614;&#23545;&#40784;&#23646;&#24615;&#65288;LAP&#65289;&#65292;&#21363;&#25968;&#25454;&#38598;&#20013;&#25152;&#26377;&#26631;&#31614;&#30340;&#21521;&#37327;&#22823;&#37096;&#20998;&#22312;&#25968;&#25454;&#30697;&#38453;&#30340;&#21069;&#20960;&#20010;&#22855;&#24322;&#21521;&#37327;&#30340;&#24352;&#25104;&#31354;&#38388;&#20869;&#12290;&#21463;&#21040;&#36825;&#19968;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#40723;&#21169;&#30446;&#26631;&#22495;&#20013;&#30340;&#39044;&#27979;&#19982;&#20854;&#21069;&#20960;&#20010;&#22855;&#24322;&#21521;&#37327;&#23545;&#40784;&#12290;&#19982;&#20256;&#32479;&#30340;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#19987;&#27880;&#20110;&#27491;&#21017;&#21270;&#34920;&#31034;&#19981;&#21516;&#65292;&#25105;&#20204;&#30456;&#21453;&#65292;&#36890;&#36807;&#22312;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20013;&#20351;&#29992;LAP&#65292;&#29992;&#27491;&#21017;&#21270;&#20998;&#31867;&#22120;&#19982;&#26080;&#30417;&#30563;&#30446;&#26631;&#25968;&#25454;&#23545;&#40784;&#12290;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;&#19968;&#23450;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#20301;&#20110;&#30446;&#26631;&#22495;&#25968;&#25454;&#30340;&#21069;&#20960;&#20010;&#21491;&#22855;&#24322;&#21521;&#37327;&#30340;&#24352;&#25104;&#31354;&#38388;&#20869;&#65292;&#24182;&#19982;&#26368;&#20248;&#35299;&#23545;&#40784;&#12290;&#36890;&#36807;&#28040;&#38500;&#32463;&#20856;&#39046;&#22495;&#36866;&#24212;&#29702;&#35770;&#20013;&#24120;&#35265;&#30340;&#26368;&#20248;&#32852;&#21512;&#39118;&#38505;&#20551;&#35774;&#30340;&#20381;&#36182;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has highlighted the label alignment property (LAP) in supervised learning, where the vector of all labels in the dataset is mostly in the span of the top few singular vectors of the data matrix. Drawing inspiration from this observation, we propose a regularization method for unsupervised domain adaptation that encourages alignment between the predictions in the target domain and its top singular vectors. Unlike conventional domain adaptation approaches that focus on regularizing representations, we instead regularize the classifier to align with the unsupervised target data, guided by the LAP in both the source and target domains. Theoretical analysis demonstrates that, under certain assumptions, our solution resides within the span of the top right singular vectors of the target domain data and aligns with the optimal solution. By removing the reliance on the commonly used optimal joint risk assumption found in classic domain adaptation theory, we showcase the effectivene
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#30740;&#31350;&#20102;&#22270;&#24418;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#27010;&#24565;&#12289;&#26041;&#27861;&#12289;&#35780;&#20272;&#21450;&#20854;&#24212;&#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24773;&#20917;&#65292;&#25552;&#20379;&#20102;&#20998;&#31867;&#27861;&#12289;&#32479;&#19968;&#30340;&#31526;&#21495;&#34920;&#31034;&#12289;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#23545;&#21313;&#22235;&#31181;&#26041;&#27861;&#12289;&#20108;&#21313;&#20108;&#20010;&#25968;&#25454;&#38598;&#21644;&#21313;&#20061;&#20010;&#25351;&#26631;&#36827;&#34892;&#20102;&#35752;&#35770;&#21644;&#25972;&#21512;&#12290;&#26410;&#26469;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#35299;&#20915;&#24320;&#25918;&#30340;&#25361;&#25112;&#19978;&#12290;</title><link>http://arxiv.org/abs/2210.12089</link><description>&lt;p&gt;
&#22270;&#24418;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#32508;&#36848;: &#23450;&#20041;, &#26041;&#27861;, &#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Survey on Graph Counterfactual Explanations: Definitions, Methods, Evaluation. (arXiv:2210.12089v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12089
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#30740;&#31350;&#20102;&#22270;&#24418;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#27010;&#24565;&#12289;&#26041;&#27861;&#12289;&#35780;&#20272;&#21450;&#20854;&#24212;&#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24773;&#20917;&#65292;&#25552;&#20379;&#20102;&#20998;&#31867;&#27861;&#12289;&#32479;&#19968;&#30340;&#31526;&#21495;&#34920;&#31034;&#12289;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#23545;&#21313;&#22235;&#31181;&#26041;&#27861;&#12289;&#20108;&#21313;&#20108;&#20010;&#25968;&#25454;&#38598;&#21644;&#21313;&#20061;&#20010;&#25351;&#26631;&#36827;&#34892;&#20102;&#35752;&#35770;&#21644;&#25972;&#21512;&#12290;&#26410;&#26469;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#35299;&#20915;&#24320;&#25918;&#30340;&#25361;&#25112;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476; (GNNs) &#22312;&#31038;&#21306;&#26816;&#27979;&#21644;&#20998;&#23376;&#20998;&#31867;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#21453;&#20107;&#23454;&#35299;&#37322; (CE) &#25552;&#20379;&#21453;&#20363;&#26469;&#20811;&#26381;&#40657;&#30418;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#38480;&#21046;&#12290;&#30001;&#20110;&#23545;&#22270;&#23398;&#20064;&#30340;&#20851;&#27880;&#19981;&#26029;&#22686;&#38271;&#65292;&#25105;&#20204;&#23558;&#37325;&#28857;&#20851;&#27880; GNNs &#30340; CE &#27010;&#24565;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#38750;&#24120;&#35268;&#30340;&#25163;&#27573;&#65292;&#25552;&#20379;&#20102;&#20998;&#31867;&#27861;&#65292;&#32479;&#19968;&#30340;&#31526;&#21495;&#34920;&#31034;&#65292;&#20197;&#21450;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#21313;&#22235;&#31181;&#26041;&#27861;&#65292;&#23427;&#20204;&#30340;&#35780;&#20272;&#21327;&#35758;&#65292;&#20108;&#21313;&#20108;&#20010;&#25968;&#25454;&#38598;&#21644;&#21313;&#20061;&#20010;&#25351;&#26631;&#12290;&#25105;&#20204;&#25972;&#21512;&#20102;&#22823;&#22810;&#25968;&#26041;&#27861;&#21040; GRETEL &#24211;&#20013;&#65292;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#20197;&#20102;&#35299;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#32570;&#28857;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#24320;&#25918;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) perform well in community detection and molecule classification. Counterfactual Explanations (CE) provide counter-examples to overcome the transparency limitations of black-box models. Due to the growing attention in graph learning, we focus on the concepts of CE for GNNs. We analysed the SoA to provide a taxonomy, a uniform notation, and the benchmarking datasets and evaluation metrics. We discuss fourteen methods, their evaluation protocols, twenty-two datasets, and nineteen metrics. We integrated the majority of methods into the GRETEL library to conduct an empirical evaluation to understand their strengths and pitfalls. We highlight open challenges and future work.
&lt;/p&gt;</description></item></channel></rss>