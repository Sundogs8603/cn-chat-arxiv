<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21516;&#26102;&#35299;&#20915;&#20102;&#39044;&#27979;&#27169;&#22411;&#36873;&#25321;&#21644;&#20984;&#32452;&#21512;&#26435;&#37325;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#22238;&#24402;&#20998;&#25903;&#23398;&#20064;&#26435;&#37325;&#21644;&#20998;&#31867;&#20998;&#25903;&#36873;&#25321;&#20855;&#26377;&#22810;&#26679;&#24615;&#30340;&#39044;&#27979;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#22522;&#20110;&#29305;&#24449;&#30340;&#39044;&#27979;&#30340;&#31934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.20545</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#20984;&#32452;&#21512;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Multi-task learning of convex combinations of forecasting models. (arXiv:2310.20545v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20545
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21516;&#26102;&#35299;&#20915;&#20102;&#39044;&#27979;&#27169;&#22411;&#36873;&#25321;&#21644;&#20984;&#32452;&#21512;&#26435;&#37325;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#22238;&#24402;&#20998;&#25903;&#23398;&#20064;&#26435;&#37325;&#21644;&#20998;&#31867;&#20998;&#25903;&#36873;&#25321;&#20855;&#26377;&#22810;&#26679;&#24615;&#30340;&#39044;&#27979;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#22522;&#20110;&#29305;&#24449;&#30340;&#39044;&#27979;&#30340;&#31934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#32452;&#21512;&#28041;&#21450;&#20351;&#29992;&#22810;&#20010;&#39044;&#27979;&#26469;&#21019;&#24314;&#21333;&#19968;&#12289;&#26356;&#31934;&#30830;&#30340;&#39044;&#27979;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#29305;&#24449;&#30340;&#39044;&#27979;&#24050;&#34987;&#29992;&#20110;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#39044;&#27979;&#27169;&#22411;&#25110;&#23398;&#20064;&#23427;&#20204;&#30340;&#20984;&#32452;&#21512;&#26435;&#37325;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#26102;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#65292;&#20854;&#20013;&#21253;&#25324;&#20004;&#20010;&#20998;&#25903;&#65306;&#22238;&#24402;&#20998;&#25903;&#36890;&#36807;&#26368;&#23567;&#21270;&#32452;&#21512;&#39044;&#27979;&#35823;&#24046;&#26469;&#23398;&#20064;&#21508;&#31181;&#39044;&#27979;&#26041;&#27861;&#30340;&#26435;&#37325;&#65292;&#20998;&#31867;&#20998;&#25903;&#21017;&#37325;&#28857;&#36873;&#25321;&#22810;&#26679;&#24615;&#30340;&#39044;&#27979;&#26041;&#27861;&#12290;&#20026;&#20102;&#20026;&#20998;&#31867;&#20219;&#21153;&#29983;&#25104;&#35757;&#32451;&#26631;&#31614;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20248;&#21270;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#30830;&#23450;&#32473;&#23450;&#26102;&#38388;&#24207;&#21015;&#30340;&#26368;&#21512;&#36866;&#30340;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#25581;&#31034;&#20102;&#22522;&#20110;&#29305;&#24449;&#30340;&#39044;&#27979;&#20013;&#22810;&#26679;&#24615;&#30340;&#37325;&#35201;&#20316;&#29992;&#65292;&#24182;&#20984;&#26174;&#20102;&#27169;&#22411;&#32452;&#21512;&#21644;&#36873;&#25321;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Forecast combination involves using multiple forecasts to create a single, more accurate prediction. Recently, feature-based forecasting has been employed to either select the most appropriate forecasting models or to learn the weights of their convex combination. In this paper, we present a multi-task learning methodology that simultaneously addresses both problems. This approach is implemented through a deep neural network with two branches: the regression branch, which learns the weights of various forecasting methods by minimizing the error of combined forecasts, and the classification branch, which selects forecasting methods with an emphasis on their diversity. To generate training labels for the classification task, we introduce an optimization-driven approach that identifies the most appropriate methods for a given time series. The proposed approach elicits the essential role of diversity in feature-based forecasting and highlights the interplay between model combination and mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#36951;&#24536;&#30340;&#26032;&#22411;&#40657;&#30418;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#28608;&#27963;&#38544;&#34255;&#21518;&#38376;&#26469;&#36755;&#20986;&#24694;&#24847;&#39044;&#27979;&#65292;&#20174;&#32780;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#33030;&#24369;&#24615;&#36827;&#34892;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2310.10659</link><description>&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#36951;&#24536;&#23454;&#26045;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Backdoor Attack through Machine Unlearning. (arXiv:2310.10659v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#36951;&#24536;&#30340;&#26032;&#22411;&#40657;&#30418;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#28608;&#27963;&#38544;&#34255;&#21518;&#38376;&#26469;&#36755;&#20986;&#24694;&#24847;&#39044;&#27979;&#65292;&#20174;&#32780;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#33030;&#24369;&#24615;&#36827;&#34892;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#30001;&#20110;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#21644;&#24212;&#29992;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#20154;&#24037;&#26234;&#33021;&#30340;&#23433;&#20840;&#38382;&#39064;&#21464;&#24471;&#36234;&#26469;&#36234;&#31361;&#20986;&#12290;&#21518;&#38376;&#25915;&#20987;&#26159;&#19968;&#31181;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#36890;&#36807;&#23884;&#20837;&#30340;&#35302;&#21457;&#22120;&#28608;&#27963;&#38544;&#34255;&#30340;&#21518;&#38376;&#65292;&#20174;&#32780;&#36755;&#20986;&#21487;&#33021;&#19982;&#32473;&#23450;&#36755;&#20837;&#30340;&#39044;&#26399;&#36755;&#20986;&#19981;&#31526;&#30340;&#24694;&#24847;&#39044;&#27979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#36951;&#24536;&#30340;&#26032;&#22411;&#40657;&#30418;&#21518;&#38376;&#25915;&#20987;&#12290;&#25915;&#20987;&#32773;&#39318;&#20808;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#26679;&#26412;&#65288;&#21253;&#25324;&#27602;&#25968;&#25454;&#21644;&#32531;&#35299;&#25968;&#25454;&#65289;&#25193;&#20805;&#35757;&#32451;&#38598;&#65292;&#35757;&#32451;&#19968;&#20010;&#8220;&#21892;&#24847;&#8221;&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#25915;&#20987;&#32773;&#25552;&#20132;&#36951;&#24536;&#35831;&#27714;&#65292;&#20197;&#31227;&#38500;&#32531;&#35299;&#26679;&#26412;&#23545;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#36880;&#27493;&#28608;&#27963;&#38544;&#34255;&#30340;&#21518;&#38376;&#12290;&#30001;&#20110;&#21518;&#38376;&#26159;&#22312;&#36845;&#20195;&#30340;&#36951;&#24536;&#36807;&#31243;&#20013;&#26893;&#20837;&#30340;&#65292;&#36825;&#26174;&#33879;&#22686;&#21152;&#20102;&#29616;&#26377;&#30340;&#21518;&#38376;&#26816;&#27979;&#26041;&#27861;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the security issues of artificial intelligence have become increasingly prominent due to the rapid development of deep learning research and applications. Backdoor attack is an attack targeting the vulnerability of deep learning models, where hidden backdoors are activated by triggers embedded by the attacker, thereby outputting malicious predictions that may not align with the intended output for a given input. In this work, we propose a novel black-box backdoor attack based on machine unlearning. The attacker first augments the training set with carefully designed samples, including poison and mitigation data, to train a 'benign' model. Then, the attacker posts unlearning requests for the mitigation samples to remove the impact of relevant data on the model, gradually activating the hidden backdoor. Since backdoors are implanted during the iterative unlearning process, it significantly increases the computational overhead of existing defense methods for backdoor dete
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21518;&#38376;&#25915;&#20987;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27169;&#22411;&#36827;&#34892;&#31574;&#30053;&#24615;&#25554;&#20837;&#21518;&#38376;&#65292;&#23545;&#40784;&#25935;&#24863;&#30701;&#35821;&#19982;&#20013;&#24615;&#26415;&#35821;&#30340;&#23884;&#20837;&#65292;&#20197;&#21024;&#38500;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#31169;&#20154;&#20449;&#24687;&#12290;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.08320</link><description>&lt;p&gt;
&#20351;&#29992;&#21518;&#38376;&#25216;&#26415;&#20445;&#25252;&#25105;&#20204;&#30340;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
Defending Our Privacy With Backdoors. (arXiv:2310.08320v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08320
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21518;&#38376;&#25915;&#20987;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27169;&#22411;&#36827;&#34892;&#31574;&#30053;&#24615;&#25554;&#20837;&#21518;&#38376;&#65292;&#23545;&#40784;&#25935;&#24863;&#30701;&#35821;&#19982;&#20013;&#24615;&#26415;&#35821;&#30340;&#23884;&#20837;&#65292;&#20197;&#21024;&#38500;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#31169;&#20154;&#20449;&#24687;&#12290;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20351;&#29992;&#26410;&#32463;&#31579;&#36873;&#12289;&#24120;&#24120;&#21253;&#21547;&#25935;&#24863;&#20449;&#24687;&#30340;&#32593;&#39029;&#25968;&#25454;&#35757;&#32451;&#22823;&#22411;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#38544;&#31169;&#38382;&#39064;&#25104;&#20026;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#20851;&#27880;&#28857;&#12290;&#20854;&#20013;&#19968;&#20010;&#38382;&#39064;&#26159;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#21033;&#29992;&#38544;&#31169;&#25915;&#20987;&#30340;&#26041;&#27861;&#25552;&#21462;&#20986;&#35757;&#32451;&#25968;&#25454;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#22312;&#19981;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#21435;&#38500;&#29305;&#23450;&#20449;&#24687;&#26159;&#19968;&#20010;&#19981;&#23481;&#26131;&#35299;&#20915;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21518;&#38376;&#25915;&#20987;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#27169;&#22411;&#20013;&#21024;&#38500;&#31169;&#20154;&#20449;&#24687;&#65292;&#22914;&#20010;&#20154;&#22995;&#21517;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#25991;&#26412;&#32534;&#30721;&#22120;&#30340;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#31574;&#30053;&#24615;&#22320;&#25554;&#20837;&#21518;&#38376;&#65292;&#25105;&#20204;&#23558;&#25935;&#24863;&#30701;&#35821;&#30340;&#23884;&#20837;&#19982;&#20013;&#24615;&#26415;&#35821;&#30340;&#23884;&#20837;&#23545;&#40784;&#65292;&#20363;&#22914;&#29992;"a person"&#20195;&#26367;&#20154;&#21517;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#36890;&#36807;&#23545;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#20351;&#29992;&#19987;&#38376;&#30340;&#38544;&#31169;&#25915;&#20987;&#27979;&#35797;&#34920;&#26126;&#20102;&#25105;&#20204;&#22522;&#20110;&#21518;&#38376;&#30340;&#38450;&#24481;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;"&#21452;&#37325;&#29992;&#36884;"&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
The proliferation of large AI models trained on uncurated, often sensitive web-scraped data has raised significant privacy concerns. One of the concerns is that adversaries can extract information about the training data using privacy attacks. Unfortunately, the task of removing specific information from the models without sacrificing performance is not straightforward and has proven to be challenging. We propose a rather easy yet effective defense based on backdoor attacks to remove private information such as names of individuals from models, and focus in this work on text encoders. Specifically, through strategic insertion of backdoors, we align the embeddings of sensitive phrases with those of neutral terms-"a person" instead of the person's name. Our empirical results demonstrate the effectiveness of our backdoor-based defense on CLIP by assessing its performance using a specialized privacy attack for zero-shot classifiers. Our approach provides not only a new "dual-use" perspecti
&lt;/p&gt;</description></item><item><title>COPRA&#26159;&#19968;&#31181;&#38754;&#21521;&#24418;&#24335;&#23450;&#29702;&#35777;&#26126;&#30340;&#35821;&#35328;&#20195;&#29702;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#36890;&#36807;&#36873;&#25321;&#31574;&#30053;&#21644;&#26816;&#32034;&#23450;&#20041;&#21644;&#24341;&#29702;&#36827;&#34892;&#35777;&#26126;&#65292;&#22312;MiniF2F&#22522;&#20934;&#21644;Coq&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.04353</link><description>&lt;p&gt;
&#19968;&#31181;&#38754;&#21521;&#24418;&#24335;&#23450;&#29702;&#35777;&#26126;&#30340;&#35821;&#35328;&#20195;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Language-Agent Approach to Formal Theorem-Proving. (arXiv:2310.04353v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04353
&lt;/p&gt;
&lt;p&gt;
COPRA&#26159;&#19968;&#31181;&#38754;&#21521;&#24418;&#24335;&#23450;&#29702;&#35777;&#26126;&#30340;&#35821;&#35328;&#20195;&#29702;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#36890;&#36807;&#36873;&#25321;&#31574;&#30053;&#21644;&#26816;&#32034;&#23450;&#20041;&#21644;&#24341;&#29702;&#36827;&#34892;&#35777;&#26126;&#65292;&#22312;MiniF2F&#22522;&#20934;&#21644;Coq&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#20195;&#29702;&#26159;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#26469;&#19982;&#22806;&#37096;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#30340;&#26041;&#27861;&#65292;&#26368;&#36817;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#25511;&#21046;&#20219;&#21153;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language agents, which use a large language model (LLM) capable of in-context learning to interact with an external environment, have recently emerged as a promising approach to control tasks. We present the first language-agent approach to formal theorem-proving. Our method, COPRA, uses a high-capacity, black-box LLM (GPT-4) as part of a policy for a stateful backtracking search. During the search, the policy can select proof tactics and retrieve lemmas and definitions from an external database. Each selected tactic is executed in the underlying proof framework, and the execution feedback is used to build the prompt for the next policy invocation. The search also tracks selected information from its history and uses it to reduce hallucinations and unnecessary LLM queries.  We evaluate COPRA on the miniF2F benchmark for Lean and a set of Coq tasks from the Compcert project. On these benchmarks, COPRA is significantly better than one-shot invocations of GPT-4, as well as state-of-the-ar
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#25968;&#25454;&#24402;&#22240;&#26041;&#27861;&#19982;&#27010;&#24565;&#25506;&#27979;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#20013;&#23398;&#20064;&#21040;&#30340;&#27010;&#24565;&#19982;&#35757;&#32451;&#25968;&#25454;&#30340;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#27010;&#24565;&#30340;&#20301;&#32622;&#21644;&#31232;&#30095;&#24615;&#24182;&#19981;&#23436;&#20840;&#20381;&#36182;&#20110;&#23569;&#37327;&#29305;&#23450;&#31034;&#20363;&#12290;</title><link>http://arxiv.org/abs/2310.03149</link><description>&lt;p&gt;
&#23558;&#31070;&#32463;&#32593;&#32476;&#20013;&#23398;&#20064;&#21040;&#30340;&#27010;&#24565;&#24402;&#22240;&#20110;&#35757;&#32451;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Attributing Learned Concepts in Neural Networks to Training Data. (arXiv:2310.03149v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03149
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#25968;&#25454;&#24402;&#22240;&#26041;&#27861;&#19982;&#27010;&#24565;&#25506;&#27979;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#20013;&#23398;&#20064;&#21040;&#30340;&#27010;&#24565;&#19982;&#35757;&#32451;&#25968;&#25454;&#30340;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#27010;&#24565;&#30340;&#20301;&#32622;&#21644;&#31232;&#30095;&#24615;&#24182;&#19981;&#23436;&#20840;&#20381;&#36182;&#20110;&#23569;&#37327;&#29305;&#23450;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#22312;&#26377;&#22823;&#37327;&#30340;&#35777;&#25454;&#34920;&#26126;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23398;&#20064;&#21040;&#20102;&#26576;&#20123;&#21487;&#35299;&#37322;&#30340;&#20154;&#31867;&#29305;&#24449;&#65292;&#20316;&#20026;&#20854;&#23545;&#25968;&#25454;&#30340;&#20869;&#37096;&#34920;&#31034;&#30340;&#19968;&#37096;&#20998;&#12290;&#30001;&#20110;&#25317;&#26377;&#27491;&#30830;&#65288;&#25110;&#38169;&#35823;&#65289;&#30340;&#27010;&#24565;&#23545;&#20110;&#21487;&#20449;&#36182;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#65292;&#33258;&#28982;&#32780;&#28982;&#22320;&#25105;&#20204;&#24819;&#35201;&#30693;&#36947;&#22312;&#32473;&#23450;&#23618;&#27425;&#19978;&#65292;&#27169;&#22411;&#21407;&#22987;&#35757;&#32451;&#38598;&#20013;&#30340;&#21738;&#20123;&#36755;&#20837;&#23545;&#20110;&#23398;&#20064;&#19968;&#20010;&#27010;&#24565;&#26368;&#20026;&#37325;&#35201;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#25968;&#25454;&#24402;&#22240;&#26041;&#27861;&#19982;&#25506;&#27979;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#27010;&#24565;&#30340;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;&#36890;&#36807;&#22312;&#19968;&#31995;&#21015;&#32593;&#32476;&#23618;&#27425;&#19978;&#35757;&#32451;&#32593;&#32476;&#21644;&#25506;&#27979;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#26368;&#36817;&#24320;&#21457;&#30340;&#29992;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#24402;&#22240;&#30340;TRAK&#26041;&#27861;&#65292;&#25105;&#20204;&#23545;&#20004;&#20010;&#27010;&#24565;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#32593;&#32476;&#21644;&#25506;&#27979;&#27169;&#22411;&#30340;&#38598;&#21512;&#12290;&#25105;&#20204;&#21457;&#29616;&#19968;&#20123;&#35777;&#25454;&#34920;&#26126;&#65292;&#36890;&#36807;&#31227;&#38500;&#23545;&#19968;&#20010;&#27010;&#24565;&#20855;&#26377;&#26368;&#39640;&#24402;&#22240;&#30340;&#21069;10000&#24352;&#22270;&#20687;&#24182;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#65292;&#27010;&#24565;&#22312;&#32593;&#32476;&#20013;&#30340;&#20301;&#32622;&#20197;&#21450;&#27010;&#24565;&#30340;&#25506;&#27979;&#31232;&#30095;&#24615;&#24182;&#27809;&#26377;&#21457;&#29983;&#25913;&#21464;&#12290;&#36825;&#34920;&#26126;&#65292;&#19982;&#20381;&#36182;&#20110;&#23569;&#37327;&#29305;&#23450;&#31034;&#20363;&#19981;&#21516;&#65292;&#29992;&#20110;&#30830;&#23450;&#27010;&#24565;&#30340;&#29305;&#24449;&#20855;&#26377;&#36739;&#39640;&#30340;&#29420;&#31435;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
By now there is substantial evidence that deep learning models learn certain human-interpretable features as part of their internal representations of data. As having the right (or wrong) concepts is critical to trustworthy machine learning systems, it is natural to ask which inputs from the model's original training set were most important for learning a concept at a given layer. To answer this, we combine data attribution methods with methods for probing the concepts learned by a model. Training network and probe ensembles for two concept datasets on a range of network layers, we use the recently developed TRAK method for large-scale data attribution. We find some evidence for convergence, where removing the 10,000 top attributing images for a concept and retraining the model does not change the location of the concept in the network nor the probing sparsity of the concept. This suggests that rather than being highly dependent on a few specific examples, the features that inform the 
&lt;/p&gt;</description></item><item><title>HappyFeat&#26159;&#19968;&#31181;&#38754;&#21521;&#20020;&#24202;&#24212;&#29992;&#30340;&#20132;&#20114;&#21644;&#39640;&#25928;&#30340;BCI&#26694;&#26550;&#65292;&#36890;&#36807;&#19968;&#20010;&#26041;&#20415;&#30340;GUI&#21644;&#21442;&#25968;&#33258;&#21160;&#21270;&#30340;&#24110;&#21161;&#65292;&#20351;&#24471;&#22522;&#20110;&#36816;&#21160;&#24819;&#35937;&#30340;BCI&#23454;&#39564;&#26356;&#21152;&#23481;&#26131;&#65292;&#24182;&#33021;&#22312;&#26102;&#38388;&#21463;&#38480;&#30340;&#29615;&#22659;&#20013;&#23454;&#29616;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.02948</link><description>&lt;p&gt;
HappyFeat -- &#19968;&#31181;&#38754;&#21521;&#20020;&#24202;&#24212;&#29992;&#30340;&#20132;&#20114;&#21644;&#39640;&#25928;&#30340;BCI&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
HappyFeat -- An interactive and efficient BCI framework for clinical applications. (arXiv:2310.02948v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02948
&lt;/p&gt;
&lt;p&gt;
HappyFeat&#26159;&#19968;&#31181;&#38754;&#21521;&#20020;&#24202;&#24212;&#29992;&#30340;&#20132;&#20114;&#21644;&#39640;&#25928;&#30340;BCI&#26694;&#26550;&#65292;&#36890;&#36807;&#19968;&#20010;&#26041;&#20415;&#30340;GUI&#21644;&#21442;&#25968;&#33258;&#21160;&#21270;&#30340;&#24110;&#21161;&#65292;&#20351;&#24471;&#22522;&#20110;&#36816;&#21160;&#24819;&#35937;&#30340;BCI&#23454;&#39564;&#26356;&#21152;&#23481;&#26131;&#65292;&#24182;&#33021;&#22312;&#26102;&#38388;&#21463;&#38480;&#30340;&#29615;&#22659;&#20013;&#23454;&#29616;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#31995;&#32479;&#20801;&#35768;&#29992;&#25143;&#36890;&#36807;&#23558;&#22823;&#33041;&#27963;&#21160;&#36716;&#21270;&#20026;&#21629;&#20196;&#26469;&#25191;&#34892;&#21160;&#20316;&#12290;&#36825;&#31867;&#31995;&#32479;&#36890;&#24120;&#38656;&#35201;&#36827;&#34892;&#35757;&#32451;&#38454;&#27573;&#65292;&#21253;&#25324;&#36890;&#36807;&#20351;&#29992;&#35760;&#24405;&#30340;&#20449;&#21495;&#30340;&#29305;&#23450;&#29305;&#24449;&#26469;&#35757;&#32451;&#20998;&#31867;&#31639;&#27861;&#65292;&#20197;&#21306;&#20998;&#19981;&#21516;&#30340;&#24515;&#29702;&#29366;&#24577;&#12290;&#22312;&#20020;&#24202;&#32972;&#26223;&#19979;&#65292;&#22914;&#20013;&#39118;&#24247;&#22797;&#65292;&#23545;&#20110;BCI&#30340;&#24615;&#33021;&#21644;&#29305;&#24449;&#36873;&#25321;&#30340;&#22521;&#35757;&#38454;&#27573;&#26377;&#29305;&#23450;&#30340;&#35201;&#27714;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;HappyFeat&#65292;&#19968;&#31181;&#36719;&#20214;&#65292;&#22312;&#21333;&#19968;&#20415;&#21033;&#30340;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;&#21644;&#23454;&#39564;&#25110;&#20998;&#26512;&#21442;&#25968;&#30340;&#33258;&#21160;&#21270;&#30340;&#24110;&#21161;&#19979;&#65292;&#20351;&#22522;&#20110;&#36816;&#21160;&#24819;&#35937;&#65288;MI&#65289;&#30340;BCI&#23454;&#39564;&#26356;&#21152;&#23481;&#26131;&#12290;&#32467;&#26524;&#24037;&#20316;&#27969;&#31243;&#21487;&#20197;&#36731;&#26494;&#36873;&#25321;&#26368;&#20339;&#29305;&#24449;&#65292;&#26377;&#21161;&#20110;&#22312;&#26102;&#38388;&#21463;&#38480;&#30340;&#29615;&#22659;&#20013;&#23454;&#29616;&#33391;&#22909;&#30340;BCI&#24615;&#33021;&#12290;&#22522;&#20110;&#21151;&#33021;&#36830;&#36890;&#24615;&#30340;&#26367;&#20195;&#29305;&#24449;&#21487;&#20197;&#19982;&#21151;&#29575;&#35889;&#23494;&#24230;&#30456;&#27604;&#25110;&#32467;&#21512;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Brain-Computer Interface (BCI) systems allow users to perform actions by translating their brain activity into commands. Such systems usually need a training phase, consisting in training a classification algorithm to discriminate between mental states using specific features from the recorded signals. This phase of feature selection and training is crucial for BCI performance and presents specific constraints to be met in a clinical context, such as post-stroke rehabilitation.  In this paper, we present HappyFeat, a software making Motor Imagery (MI) based BCI experiments easier, by gathering all necessary manipulations and analysis in a single convenient GUI and via automation of experiment or analysis parameters. The resulting workflow allows for effortlessly selecting the best features, helping to achieve good BCI performance in time-constrained environments. Alternative features based on Functional Connectivity can be used and compared or combined with Power Spectral Density, allo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#27169;&#22411;&#22312;&#28151;&#21512;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#23545;&#31283;&#23450;&#24615;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#35777;&#26126;&#21021;&#22987;&#29983;&#25104;&#27169;&#22411;&#36275;&#22815;&#25509;&#36817;&#25968;&#25454;&#20998;&#24067;&#24182;&#19988;&#25968;&#25454;&#27604;&#20363;&#36866;&#24403;&#65292;&#36845;&#20195;&#35757;&#32451;&#26159;&#31283;&#23450;&#30340;&#12290;</title><link>http://arxiv.org/abs/2310.00429</link><description>&lt;p&gt;
&#20851;&#20110;&#29983;&#25104;&#27169;&#22411;&#22312;&#20854;&#33258;&#24049;&#30340;&#25968;&#25454;&#19978;&#36845;&#20195;&#35757;&#32451;&#30340;&#31283;&#23450;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Stability of Iterative Retraining of Generative Models on their own Data. (arXiv:2310.00429v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00429
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#27169;&#22411;&#22312;&#28151;&#21512;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#23545;&#31283;&#23450;&#24615;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#35777;&#26126;&#21021;&#22987;&#29983;&#25104;&#27169;&#22411;&#36275;&#22815;&#25509;&#36817;&#25968;&#25454;&#20998;&#24067;&#24182;&#19988;&#25968;&#25454;&#27604;&#20363;&#36866;&#24403;&#65292;&#36845;&#20195;&#35757;&#32451;&#26159;&#31283;&#23450;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#24314;&#27169;&#22797;&#26434;&#25968;&#25454;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#65292;&#24448;&#24448;&#23637;&#29616;&#20986;&#36229;&#36807;&#20856;&#22411;&#20154;&#31867;&#33021;&#21147;&#30340;&#26679;&#26412;&#30495;&#23454;&#24615;&#36776;&#21035;&#33021;&#21147;&#12290;&#36825;&#19968;&#25104;&#21151;&#30340;&#20851;&#38190;&#39537;&#21160;&#21147;&#26080;&#30097;&#26159;&#36825;&#20123;&#27169;&#22411;&#28040;&#32791;&#28023;&#37327;&#32593;&#32476;&#35268;&#27169;&#25968;&#25454;&#30340;&#32467;&#26524;&#12290;&#30001;&#20110;&#36825;&#20123;&#27169;&#22411;&#24778;&#20154;&#30340;&#24615;&#33021;&#21644;&#26131;&#24471;&#24615;&#65292;&#32593;&#32476;&#19978;&#23558;&#19981;&#21487;&#36991;&#20813;&#22320;&#20986;&#29616;&#36234;&#26469;&#36234;&#22810;&#30340;&#21512;&#25104;&#20869;&#23481;&#12290;&#36825;&#20010;&#20107;&#23454;&#30452;&#25509;&#24847;&#21619;&#30528;&#29983;&#25104;&#27169;&#22411;&#30340;&#26410;&#26469;&#36845;&#20195;&#24517;&#39035;&#38754;&#23545;&#19968;&#20010;&#29616;&#23454;&#65306;&#23427;&#20204;&#30340;&#35757;&#32451;&#25968;&#25454;&#30001;&#28165;&#27905;&#25968;&#25454;&#21644;&#20808;&#21069;&#27169;&#22411;&#29983;&#25104;&#30340;&#20154;&#24037;&#25968;&#25454;&#32452;&#25104;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#23545;&#28151;&#21512;&#25968;&#25454;&#38598;&#65288;&#21253;&#25324;&#30495;&#23454;&#25968;&#25454;&#21644;&#21512;&#25104;&#25968;&#25454;&#65289;&#19978;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#23545;&#31283;&#23450;&#24615;&#30340;&#24433;&#21709;&#36827;&#34892;&#20005;&#26684;&#30740;&#31350;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#22312;&#21021;&#22987;&#29983;&#25104;&#27169;&#22411;&#36275;&#22815;&#22909;&#22320;&#36817;&#20284;&#25968;&#25454;&#20998;&#24067;&#24182;&#19988;&#30495;&#23454;&#25968;&#25454;&#19982;&#21512;&#25104;&#25968;&#25454;&#30340;&#27604;&#20363;&#36866;&#24403;&#30340;&#24773;&#20917;&#19979;&#65292;&#36845;&#20195;&#35757;&#32451;&#30340;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep generative models have made tremendous progress in modeling complex data, often exhibiting generation quality that surpasses a typical human's ability to discern the authenticity of samples. Undeniably, a key driver of this success is enabled by the massive amounts of web-scale data consumed by these models. Due to these models' striking performance and ease of availability, the web will inevitably be increasingly populated with synthetic content. Such a fact directly implies that future iterations of generative models must contend with the reality that their training is curated from both clean data and artificially generated data from past models. In this paper, we develop a framework to rigorously study the impact of training generative models on mixed datasets (of real and synthetic data) on their stability. We first prove the stability of iterative training under the condition that the initial generative models approximate the data distribution well enough and the proportion o
&lt;/p&gt;</description></item><item><title>DifAttack&#26159;&#19968;&#31181;&#22522;&#20110;&#35299;&#32806;&#29305;&#24449;&#31354;&#38388;&#30340;&#39640;&#25928;&#40657;&#30418;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#20248;&#21270;&#23545;&#25239;&#29305;&#24449;&#24182;&#21033;&#29992;&#21463;&#23475;&#32773;&#27169;&#22411;&#30340;&#26597;&#35810;&#21453;&#39304;&#29983;&#25104;&#25104;&#21151;&#30340;&#23545;&#25239;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2309.14585</link><description>&lt;p&gt;
DifAttack: &#22522;&#20110;&#35299;&#32806;&#29305;&#24449;&#31354;&#38388;&#30340;&#39640;&#25928;&#40657;&#30418;&#25915;&#20987;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DifAttack: Query-Efficient Black-Box Attack via Disentangled Feature Space. (arXiv:2309.14585v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14585
&lt;/p&gt;
&lt;p&gt;
DifAttack&#26159;&#19968;&#31181;&#22522;&#20110;&#35299;&#32806;&#29305;&#24449;&#31354;&#38388;&#30340;&#39640;&#25928;&#40657;&#30418;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#20248;&#21270;&#23545;&#25239;&#29305;&#24449;&#24182;&#21033;&#29992;&#21463;&#23475;&#32773;&#27169;&#22411;&#30340;&#26597;&#35810;&#21453;&#39304;&#29983;&#25104;&#25104;&#21151;&#30340;&#23545;&#25239;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#20855;&#26377;&#39640;&#25915;&#20987;&#25104;&#21151;&#29575;&#21644;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;&#30340;&#39640;&#25928;&#22522;&#20110;&#20998;&#25968;&#30340;&#40657;&#30418;&#23545;&#25239;&#25915;&#20987;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#35299;&#32806;&#29305;&#24449;&#31354;&#38388;&#30340;&#26032;&#22411;&#25915;&#20987;&#26041;&#27861;&#65292;&#31216;&#20026;DifAttack&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#22312;&#25972;&#20010;&#29305;&#24449;&#31354;&#38388;&#19978;&#25805;&#20316;&#26377;&#26174;&#33879;&#24046;&#24322;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;DifAttack&#39318;&#20808;&#23558;&#22270;&#20687;&#30340;&#28508;&#22312;&#29305;&#24449;&#35299;&#32806;&#20026;&#23545;&#25239;&#29305;&#24449;&#21644;&#35270;&#35273;&#29305;&#24449;&#65292;&#20854;&#20013;&#21069;&#32773;&#20027;&#23548;&#22270;&#20687;&#30340;&#23545;&#25239;&#33021;&#21147;&#65292;&#32780;&#21518;&#32773;&#20027;&#35201;&#20915;&#23450;&#20854;&#35270;&#35273;&#22806;&#35266;&#12290;&#25105;&#20204;&#20351;&#29992;&#30001;&#21487;&#29992;&#26367;&#20195;&#27169;&#22411;&#36890;&#36807;&#30333;&#30418;&#25915;&#20987;&#26041;&#27861;&#29983;&#25104;&#30340;&#24178;&#20928;&#22270;&#20687;&#21644;&#20854;&#23545;&#25239;&#24615;&#26679;&#26412;&#26469;&#35757;&#32451;&#33258;&#21160;&#32534;&#30721;&#22120;&#36827;&#34892;&#35299;&#32806;&#12290;&#26368;&#32456;&#65292;DifAttack&#26681;&#25454;&#21463;&#23475;&#32773;&#27169;&#22411;&#30340;&#26597;&#35810;&#21453;&#39304;&#65292;&#36845;&#20195;&#20248;&#21270;&#23545;&#25239;&#29305;&#24449;&#65292;&#30452;&#21040;&#25104;&#21151;&#29983;&#25104;&#23545;&#25239;&#26679;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#35270;&#35273;&#29305;&#24449;&#19981;&#21464;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#36991;&#20813;&#20351;&#29992;...
&lt;/p&gt;
&lt;p&gt;
This work investigates efficient score-based black-box adversarial attacks with a high Attack Success Rate (ASR) and good generalizability. We design a novel attack method based on a Disentangled Feature space, called DifAttack, which differs significantly from the existing ones operating over the entire feature space. Specifically, DifAttack firstly disentangles an image's latent feature into an adversarial feature and a visual feature, where the former dominates the adversarial capability of an image, while the latter largely determines its visual appearance. We train an autoencoder for the disentanglement by using pairs of clean images and their Adversarial Examples (AEs) generated from available surrogate models via white-box attack methods. Eventually, DifAttack iteratively optimizes the adversarial feature according to the query feedback from the victim model until a successful AE is generated, while keeping the visual feature unaltered. In addition, due to the avoidance of using
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#36873;&#21462;&#19968;&#20010;&#21512;&#29702;&#30340;&#23376;&#38598;&#65292;&#21152;&#36895;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#21644;&#37327;&#23376;&#26680;&#30340;&#35757;&#32451;&#65292;&#24182;&#20998;&#26512;&#20102;&#23427;&#20204;&#22312;&#36825;&#20123;&#26680;&#24515;&#38598;&#19978;&#35757;&#32451;&#26102;&#30340;&#27867;&#21270;&#35823;&#24046;&#30028;&#38480;&#65292;&#25581;&#31034;&#20102;&#19982;&#22312;&#23436;&#25972;&#21407;&#22987;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30456;&#27604;&#20855;&#26377;&#21487;&#27604;&#24615;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.10441</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#35777;&#26126;&#30340;&#27867;&#21270;&#21152;&#36895;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26680;&#24515;&#38598;&#36873;&#21462;
&lt;/p&gt;
&lt;p&gt;
Coreset selection can accelerate quantum machine learning models with provable generalization. (arXiv:2309.10441v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10441
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#36873;&#21462;&#19968;&#20010;&#21512;&#29702;&#30340;&#23376;&#38598;&#65292;&#21152;&#36895;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#21644;&#37327;&#23376;&#26680;&#30340;&#35757;&#32451;&#65292;&#24182;&#20998;&#26512;&#20102;&#23427;&#20204;&#22312;&#36825;&#20123;&#26680;&#24515;&#38598;&#19978;&#35757;&#32451;&#26102;&#30340;&#27867;&#21270;&#35823;&#24046;&#30028;&#38480;&#65292;&#25581;&#31034;&#20102;&#19982;&#22312;&#23436;&#25972;&#21407;&#22987;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30456;&#27604;&#20855;&#26377;&#21487;&#27604;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#21644;&#37327;&#23376;&#26680;&#22312;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#20855;&#26377;&#31361;&#20986;&#22320;&#20301;&#65292;&#21033;&#29992;&#21363;&#23558;&#21040;&#26469;&#30340;&#36817;&#26399;&#37327;&#23376;&#35745;&#31639;&#26426;&#30340;&#33021;&#21147;&#26469;&#20811;&#26381;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#25928;&#29575;&#30340;&#25361;&#25112;&#38480;&#21046;&#20102;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#21644;&#37327;&#23376;&#26680;&#22312;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#26102;&#30340;&#25928;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#65306;&#26680;&#24515;&#38598;&#36873;&#25321;&#65292;&#26088;&#22312;&#36890;&#36807;&#20174;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#20986;&#19968;&#20010;&#21512;&#29702;&#30340;&#23376;&#38598;&#26469;&#21152;&#36895;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#21644;&#37327;&#23376;&#26680;&#30340;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#24403;&#22312;&#36825;&#20123;&#26680;&#24515;&#38598;&#19978;&#35757;&#32451;&#26102;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#21644;&#37327;&#23376;&#26680;&#30340;&#27867;&#21270;&#35823;&#24046;&#30028;&#38480;&#65292;&#24182;&#25581;&#31034;&#20102;&#19982;&#22312;&#23436;&#25972;&#21407;&#22987;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30456;&#27604;&#20855;&#26377;&#21487;&#27604;&#24615;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#31995;&#32479;&#30340;&#25968;&#20540;&#27169;&#25311;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26680;&#24515;&#38598;&#36873;&#25321;&#22312;&#21152;&#36895;&#28085;&#30422;&#21512;&#25104;&#25968;&#25454;&#20998;&#31867;&#12289;&#37327;&#23376;&#21327;&#35758;&#37492;&#23450;&#31561;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum neural networks (QNNs) and quantum kernels stand as prominent figures in the realm of quantum machine learning, poised to leverage the nascent capabilities of near-term quantum computers to surmount classical machine learning challenges. Nonetheless, the training efficiency challenge poses a limitation on both QNNs and quantum kernels, curbing their efficacy when applied to extensive datasets. To confront this concern, we present a unified approach: coreset selection, aimed at expediting the training of QNNs and quantum kernels by distilling a judicious subset from the original training dataset. Furthermore, we analyze the generalization error bounds of QNNs and quantum kernels when trained on such coresets, unveiling the comparable performance with those training on the complete original dataset. Through systematic numerical simulations, we illuminate the potential of coreset selection in expediting tasks encompassing synthetic data classification, identification of quantum co
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20613;&#37324;&#21494;&#21464;&#25442;&#21644;&#36719;&#38408;&#20540;&#30340;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#22312;&#19981;&#21516;&#28304;&#19978;&#30340;&#35270;&#32593;&#33180;&#24213;&#22270;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.09866</link><description>&lt;p&gt;
&#20613;&#37324;&#21494;&#21464;&#25442;&#21644;&#36719;&#38408;&#20540;&#30340;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Domain Generalization with Fourier Transform and Soft Thresholding. (arXiv:2309.09866v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09866
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20613;&#37324;&#21494;&#21464;&#25442;&#21644;&#36719;&#38408;&#20540;&#30340;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#22312;&#19981;&#21516;&#28304;&#19978;&#30340;&#35270;&#32593;&#33180;&#24213;&#22270;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#27867;&#21270;&#26088;&#22312;&#35757;&#32451;&#27169;&#22411;&#20197;&#36866;&#24212;&#22810;&#20010;&#28304;&#39046;&#22495;&#65292;&#24182;&#23454;&#29616;&#23545;&#26410;&#35265;&#36807;&#30340;&#30446;&#26631;&#39046;&#22495;&#30340;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#35768;&#22810;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;&#20013;&#65292;&#22522;&#20110;&#20613;&#37324;&#21494;&#21464;&#25442;&#30340;&#26041;&#27861;&#22240;&#20026;&#21033;&#29992;&#20613;&#37324;&#21494;&#21464;&#25442;&#26469;&#25429;&#25417;&#25968;&#25454;&#20013;&#30340;&#37325;&#35201;&#27169;&#24335;&#21644;&#35268;&#24459;&#65292;&#20174;&#32780;&#20351;&#27169;&#22411;&#23545;&#39046;&#22495;&#36716;&#31227;&#26356;&#21152;&#40065;&#26834;&#32780;&#22791;&#21463;&#38738;&#30544;&#12290;&#20027;&#27969;&#30340;&#22522;&#20110;&#20613;&#37324;&#21494;&#21464;&#25442;&#30340;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;&#22312;&#28304;&#22270;&#20687;&#21644;&#30446;&#26631;&#22270;&#20687;&#20043;&#38388;&#20132;&#25442;&#20613;&#37324;&#21494;&#24133;&#24230;&#35889;&#65292;&#21516;&#26102;&#20445;&#30041;&#30456;&#20301;&#35889;&#12290;&#28982;&#32780;&#65292;&#23427;&#24573;&#30053;&#20102;&#24133;&#24230;&#35889;&#20013;&#30340;&#32972;&#26223;&#24178;&#25200;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#22312;&#20613;&#37324;&#21494;&#22495;&#24341;&#20837;&#20102;&#19968;&#20010;&#36719;&#38408;&#20540;&#20989;&#25968;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#26032;&#35774;&#35745;&#30340;&#31639;&#27861;&#24212;&#29992;&#20110;&#35270;&#32593;&#33180;&#24213;&#22270;&#22270;&#20687;&#20998;&#21106;&#65292;&#36825;&#23545;&#20110;&#35786;&#26029;&#30524;&#31185;&#30142;&#30149;&#24456;&#37325;&#35201;&#65292;&#20294;&#30001;&#20110;&#39046;&#22495;&#36716;&#31227;&#65292;&#31070;&#32463;&#32593;&#32476;&#22312;&#19981;&#21516;&#28304;&#19978;&#30340;&#24615;&#33021;&#21487;&#33021;&#20250;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain generalization aims to train models on multiple source domains so that they can generalize well to unseen target domains. Among many domain generalization methods, Fourier-transform-based domain generalization methods have gained popularity primarily because they exploit the power of Fourier transformation to capture essential patterns and regularities in the data, making the model more robust to domain shifts. The mainstream Fourier-transform-based domain generalization swaps the Fourier amplitude spectrum while preserving the phase spectrum between the source and the target images. However, it neglects background interference in the amplitude spectrum. To overcome this limitation, we introduce a soft-thresholding function in the Fourier domain. We apply this newly designed algorithm to retinal fundus image segmentation, which is important for diagnosing ocular diseases but the neural network's performance can degrade across different sources due to domain shifts. The proposed 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#24320;&#21457;&#31163;&#25955;&#19968;&#33268;&#24615;&#38381;&#22622;&#26041;&#26696;&#30340;&#26032;&#26041;&#27861;&#65292;&#20854;&#20013;&#23558;&#22823;&#28065;&#27169;&#25311;&#20013;&#30340;&#38381;&#22622;&#27169;&#22411;&#31995;&#25968;&#35843;&#25972;&#20219;&#21153;&#23450;&#20041;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#36890;&#36807;&#21518;&#26399;&#30340;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#12290;&#36825;&#19968;&#26041;&#27861;&#33021;&#22815;&#23558;&#27169;&#22411;&#35843;&#25972;&#21040;&#23454;&#38469;&#30340;&#31163;&#25955;&#21270;&#20013;&#24182;&#32771;&#34385;&#31163;&#25955;&#21270;&#21644;&#27169;&#22411;&#26412;&#36523;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#36890;&#36807;&#20248;&#21270;&#26174;&#24335;&#21644;&#38544;&#24335;&#38381;&#22622;&#27169;&#22411;&#20013;&#30340;&#23616;&#37096;&#28065;&#31896;&#24230;&#27169;&#22411;&#21644;&#28151;&#21512;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#38381;&#22622;&#27169;&#22411;&#30340;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2309.06260</link><description>&lt;p&gt;
&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#31163;&#25955;&#19968;&#33268;&#24615;&#38381;&#22622;&#26041;&#26696;&#29992;&#20110;&#22823;&#28065;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Toward Discretization-Consistent Closure Schemes for Large Eddy Simulation Using Reinforcement Learning. (arXiv:2309.06260v1 [physics.flu-dyn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06260
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#24320;&#21457;&#31163;&#25955;&#19968;&#33268;&#24615;&#38381;&#22622;&#26041;&#26696;&#30340;&#26032;&#26041;&#27861;&#65292;&#20854;&#20013;&#23558;&#22823;&#28065;&#27169;&#25311;&#20013;&#30340;&#38381;&#22622;&#27169;&#22411;&#31995;&#25968;&#35843;&#25972;&#20219;&#21153;&#23450;&#20041;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#36890;&#36807;&#21518;&#26399;&#30340;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#12290;&#36825;&#19968;&#26041;&#27861;&#33021;&#22815;&#23558;&#27169;&#22411;&#35843;&#25972;&#21040;&#23454;&#38469;&#30340;&#31163;&#25955;&#21270;&#20013;&#24182;&#32771;&#34385;&#31163;&#25955;&#21270;&#21644;&#27169;&#22411;&#26412;&#36523;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#36890;&#36807;&#20248;&#21270;&#26174;&#24335;&#21644;&#38544;&#24335;&#38381;&#22622;&#27169;&#22411;&#20013;&#30340;&#23616;&#37096;&#28065;&#31896;&#24230;&#27169;&#22411;&#21644;&#28151;&#21512;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#38381;&#22622;&#27169;&#22411;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#24320;&#21457;&#29992;&#20110;&#38544;&#24335;&#28388;&#27874;&#30340;&#22823;&#28065;&#27169;&#25311;&#65288;LES&#65289;&#20013;&#30340;&#31163;&#25955;&#19968;&#33268;&#24615;&#38381;&#22622;&#26041;&#26696;&#12290;&#22312;&#38544;&#24335;&#28388;&#27874;&#30340;LES&#20013;&#65292;&#24863;&#24212;&#28388;&#27874;&#26680;&#21644;&#38381;&#22622;&#39033;&#26159;&#30001;&#32593;&#26684;&#21644;&#31163;&#25955;&#21270;&#25805;&#20316;&#31526;&#30340;&#24615;&#36136;&#20915;&#23450;&#30340;&#65292;&#20174;&#32780;&#20135;&#29983;&#39069;&#22806;&#30340;&#26410;&#30693;&#35745;&#31639;&#32454;&#32593;&#39033;&#12290;&#22240;&#27492;&#65292;&#23558;LES&#38381;&#22622;&#27169;&#22411;&#30340;&#31995;&#25968;&#35843;&#25972;&#20219;&#21153;&#23450;&#20041;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#22312;&#21518;&#26399;&#35299;&#20915;&#12290;&#36825;&#20801;&#35768;&#23558;&#27169;&#22411;&#35843;&#25972;&#21040;&#23454;&#38469;&#30340;&#31163;&#25955;&#21270;&#20013;&#65292;&#21516;&#26102;&#36824;&#21253;&#25324;&#31163;&#25955;&#21270;&#21644;&#27169;&#22411;&#26412;&#36523;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#36825;&#20010;&#20248;&#21270;&#26694;&#26550;&#24212;&#29992;&#20110;&#26174;&#24335;&#21644;&#38544;&#24335;&#38381;&#22622;&#27169;&#22411;&#12290;&#36890;&#36807;&#20248;&#21270;&#23616;&#37096;&#28065;&#31896;&#24230;&#27169;&#22411;&#20316;&#20026;&#26174;&#24335;&#27169;&#22411;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#26469;&#35782;&#21035;&#28151;&#21512;&#19981;&#36830;&#32493;G&#30340;&#26368;&#20248;&#28151;&#21512;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel method for developing discretization-consistent closure schemes for implicitly filtered Large Eddy Simulation (LES). In implicitly filtered LES, the induced filter kernel, and thus the closure terms, are determined by the properties of the grid and the discretization operator, leading to additional computational subgrid terms that are generally unknown in a priori analysis. Therefore, the task of adapting the coefficients of LES closure models is formulated as a Markov decision process and solved in an a posteriori manner with Reinforcement Learning (RL). This allows to adjust the model to the actual discretization as it also incorporates the interaction between the discretization and the model itself. This optimization framework is applied to both explicit and implicit closure models. An element-local eddy viscosity model is optimized as the explicit model. For the implicit modeling, RL is applied to identify an optimal blending strategy for a hybrid discontinuous G
&lt;/p&gt;</description></item><item><title>AmbientFlow&#26159;&#19968;&#20010;&#20174;&#22122;&#22768;&#21644;&#19981;&#23436;&#25972;&#25968;&#25454;&#20013;&#30452;&#25509;&#23398;&#20064;&#22522;&#20110;&#27969;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#21464;&#20998;&#36125;&#21494;&#26031;&#26041;&#27861;&#26469;&#24314;&#31435;&#36825;&#31181;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#22270;&#20687;&#31185;&#23398;&#20013;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.04856</link><description>&lt;p&gt;
AmbientFlow: &#26469;&#33258;&#19981;&#23436;&#25972;&#12289;&#22122;&#22768;&#27979;&#37327;&#30340;&#21487;&#36870;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AmbientFlow: Invertible generative models from incomplete, noisy measurements. (arXiv:2309.04856v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04856
&lt;/p&gt;
&lt;p&gt;
AmbientFlow&#26159;&#19968;&#20010;&#20174;&#22122;&#22768;&#21644;&#19981;&#23436;&#25972;&#25968;&#25454;&#20013;&#30452;&#25509;&#23398;&#20064;&#22522;&#20110;&#27969;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#21464;&#20998;&#36125;&#21494;&#26031;&#26041;&#27861;&#26469;&#24314;&#31435;&#36825;&#31181;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#22270;&#20687;&#31185;&#23398;&#20013;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#22312;&#22270;&#20687;&#31185;&#23398;&#20013;&#20855;&#26377;&#28508;&#22312;&#30340;&#24212;&#29992;&#65292;&#22914;&#22270;&#20687;&#37325;&#24314;&#12289;&#21518;&#39564;&#37319;&#26679;&#21644;&#25968;&#25454;&#20849;&#20139;&#12290;&#22522;&#20110;&#27969;&#30340;&#29983;&#25104;&#27169;&#22411;&#29305;&#21035;&#26377;&#21560;&#24341;&#21147;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#20197;&#21487;&#34892;&#30340;&#26041;&#24335;&#25552;&#20379;&#31934;&#30830;&#30340;&#23494;&#24230;&#20272;&#35745;&#20197;&#21450;&#24555;&#36895;&#12289;&#24265;&#20215;&#21644;&#22810;&#26679;&#30340;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#36825;&#26679;&#30340;&#27169;&#22411;&#38656;&#35201;&#19968;&#20010;&#22823;&#22411;&#12289;&#39640;&#36136;&#37327;&#30340;&#23545;&#35937;&#25968;&#25454;&#38598;&#12290;&#22312;&#35745;&#31639;&#25104;&#20687;&#31561;&#24212;&#29992;&#20013;&#65292;&#30001;&#20110;&#38656;&#35201;&#38271;&#26102;&#38388;&#33719;&#21462;&#25110;&#39640;&#36752;&#23556;&#21058;&#37327;&#65292;&#24448;&#24448;&#38590;&#20197;&#33719;&#21462;&#36825;&#26679;&#30340;&#25968;&#25454;&#65292;&#32780;&#33719;&#21462;&#36825;&#20123;&#23545;&#35937;&#30340;&#22122;&#22768;&#25110;&#37096;&#20998;&#35266;&#27979;&#27979;&#37327;&#26356;&#21487;&#34892;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AmbientFlow&#65292;&#19968;&#20010;&#20174;&#22122;&#22768;&#21644;&#19981;&#23436;&#25972;&#25968;&#25454;&#30452;&#25509;&#23398;&#20064;&#22522;&#20110;&#27969;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#26694;&#26550;&#12290;&#20351;&#29992;&#21464;&#20998;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20174;&#22122;&#22768;&#12289;&#19981;&#23436;&#25972;&#25968;&#25454;&#24314;&#31435;&#22522;&#20110;&#27969;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#26032;&#26694;&#26550;&#12290;&#24191;&#27867;&#30340;&#25968;&#20540;&#30740;&#31350;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative models have gained popularity for their potential applications in imaging science, such as image reconstruction, posterior sampling and data sharing. Flow-based generative models are particularly attractive due to their ability to tractably provide exact density estimates along with fast, inexpensive and diverse samples. Training such models, however, requires a large, high quality dataset of objects. In applications such as computed imaging, it is often difficult to acquire such data due to requirements such as long acquisition time or high radiation dose, while acquiring noisy or partially observed measurements of these objects is more feasible. In this work, we propose AmbientFlow, a framework for learning flow-based generative models directly from noisy and incomplete data. Using variational Bayesian methods, a novel framework for establishing flow-based generative models from noisy, incomplete data is proposed. Extensive numerical studies demonstrate the effectiveness o
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#12289;&#36731;&#37327;&#32423;&#30340;&#39046;&#22495;&#35299;&#32544;&#27169;&#22359;&#65288;DDM&#65289;&#65292;&#21487;&#20197;&#22312;&#22810;&#20010;&#19981;&#21516;&#39046;&#22495;&#19978;&#36827;&#34892;&#26377;&#25928;&#30340;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#26174;&#31034;&#20986;&#36739;&#39640;&#30340;&#32447;&#24615;&#25506;&#27979;&#20934;&#30830;&#29575;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2309.03999</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#33258;&#30417;&#30563;&#34920;&#31034;&#21040;&#22810;&#39046;&#22495;&#35774;&#32622;&#20013;
&lt;/p&gt;
&lt;p&gt;
Adapting Self-Supervised Representations to Multi-Domain Setups. (arXiv:2309.03999v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03999
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#12289;&#36731;&#37327;&#32423;&#30340;&#39046;&#22495;&#35299;&#32544;&#27169;&#22359;&#65288;DDM&#65289;&#65292;&#21487;&#20197;&#22312;&#22810;&#20010;&#19981;&#21516;&#39046;&#22495;&#19978;&#36827;&#34892;&#26377;&#25928;&#30340;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#26174;&#31034;&#20986;&#36739;&#39640;&#30340;&#32447;&#24615;&#25506;&#27979;&#20934;&#30830;&#29575;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#22312;&#21333;&#20010;&#39046;&#22495;&#19978;&#35757;&#32451;&#26102;&#65292;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#22312;&#26410;&#30693;&#39046;&#22495;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#26377;&#38480;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#21363;&#20351;&#22312;&#28151;&#21512;&#39046;&#22495;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#20063;&#24456;&#24046;&#65292;&#22240;&#27492;&#19981;&#36866;&#21512;&#22312;&#22810;&#26679;&#21270;&#30340;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#37096;&#32626;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#12289;&#36731;&#37327;&#32423;&#30340;&#39046;&#22495;&#35299;&#32544;&#27169;&#22359;&#65288;DDM&#65289;&#65292;&#21487;&#20197;&#25554;&#20837;&#21040;&#20219;&#20309;&#33258;&#30417;&#30563;&#32534;&#30721;&#22120;&#20013;&#65292;&#22312;&#20855;&#26377;&#25110;&#19981;&#20855;&#26377;&#20849;&#20139;&#31867;&#30340;&#22810;&#20010;&#19981;&#21516;&#39046;&#22495;&#19978;&#26377;&#25928;&#22320;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#12290;&#22312;&#33258;&#30417;&#30563;&#25439;&#22833;&#30340;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;DDM&#36890;&#36807;&#20998;&#21106;&#34920;&#31034;&#31354;&#38388;&#20026;&#39046;&#22495;&#21464;&#20307;&#21644;&#39046;&#22495;&#19981;&#21464;&#37096;&#20998;&#26469;&#24378;&#21046;&#23454;&#29616;&#34920;&#31034;&#31354;&#38388;&#20013;&#30340;&#35299;&#32544;&#12290;&#24403;&#27809;&#26377;&#39046;&#22495;&#26631;&#31614;&#21487;&#29992;&#26102;&#65292;DDM&#20351;&#29992;&#24378;&#20581;&#30340;&#32858;&#31867;&#26041;&#27861;&#26469;&#21457;&#29616;&#20266;&#39046;&#22495;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;DDM&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#20197;&#22312;&#26368;&#20808;&#36827;&#30340;&#33258;&#30417;&#30563;&#27169;&#22411;&#19978;&#26174;&#31034;&#20986;&#39640;&#36798;3.5%&#30340;&#32447;&#24615;&#25506;&#27979;&#20934;&#30830;&#29575;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current state-of-the-art self-supervised approaches, are effective when trained on individual domains but show limited generalization on unseen domains. We observe that these models poorly generalize even when trained on a mixture of domains, making them unsuitable to be deployed under diverse real-world setups. We therefore propose a general-purpose, lightweight Domain Disentanglement Module (DDM) that can be plugged into any self-supervised encoder to effectively perform representation learning on multiple, diverse domains with or without shared classes. During pre-training according to a self-supervised loss, DDM enforces a disentanglement in the representation space by splitting it into a domain-variant and a domain-invariant portion. When domain labels are not available, DDM uses a robust clustering approach to discover pseudo-domains. We show that pre-training with DDM can show up to 3.5% improvement in linear probing accuracy on state-of-the-art self-supervised models including 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;norm tweaking&#8221;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#35843;&#25972;&#37327;&#21270;&#30340;&#28608;&#27963;&#20998;&#24067;&#26469;&#23454;&#29616;&#39640;&#31934;&#24230;&#30340;&#20302;&#27604;&#29305;&#37327;&#21270;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21387;&#32553;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.02784</link><description>&lt;p&gt;
Norm&#35843;&#25972;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#24615;&#33021;&#20302;&#27604;&#29305;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Norm Tweaking: High-performance Low-bit Quantization of Large Language Models. (arXiv:2309.02784v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02784
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;norm tweaking&#8221;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#35843;&#25972;&#37327;&#21270;&#30340;&#28608;&#27963;&#20998;&#24067;&#26469;&#23454;&#29616;&#39640;&#31934;&#24230;&#30340;&#20302;&#27604;&#29305;&#37327;&#21270;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21387;&#32553;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23610;&#23544;&#19981;&#26029;&#22686;&#22823;&#65292;&#22312;&#20445;&#25345;&#31934;&#24230;&#30340;&#21069;&#25552;&#19979;&#36827;&#34892;&#27169;&#22411;&#21387;&#32553;&#24050;&#25104;&#20026;&#37096;&#32626;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;&#34429;&#28982;&#19968;&#20123;&#37327;&#21270;&#26041;&#27861;&#65292;&#22914;GPTQ&#65292;&#22312;&#23454;&#29616;&#21487;&#25509;&#21463;&#30340;4&#27604;&#29305;&#26435;&#37325;&#37327;&#21270;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#23581;&#35797;&#26356;&#20302;&#20301;&#30340;&#37327;&#21270;&#24448;&#24448;&#23548;&#33268;&#20005;&#37325;&#30340;&#24615;&#33021;&#38477;&#20302;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;norm tweaking&#8221;&#30340;&#25216;&#26415;&#65292;&#23427;&#21487;&#20197;&#20316;&#20026;&#24403;&#21069;PTQ&#26041;&#27861;&#30340;&#25554;&#20214;&#65292;&#23454;&#29616;&#39640;&#31934;&#24230;&#21644;&#25104;&#26412;&#39640;&#25928;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21463;&#21040;&#19968;&#39033;&#35266;&#23519;&#30340;&#21551;&#31034;&#65292;&#21363;&#20351;&#35843;&#25972;&#37327;&#21270;&#30340;&#28608;&#27963;&#20998;&#24067;&#20197;&#19982;&#20854;&#28014;&#28857;&#23545;&#24212;&#29289;&#21305;&#37197;&#65292;&#20063;&#21487;&#20197;&#24674;&#22797;LLMs&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#31934;&#24515;&#35774;&#35745;&#20102;&#19968;&#20010;&#35843;&#25972;&#31574;&#30053;&#65292;&#21253;&#25324;&#29983;&#25104;&#26657;&#20934;&#25968;&#25454;&#21644;&#36890;&#36947;&#36317;&#31163;&#32422;&#26463;&#65292;&#20197;&#26356;&#26032;&#24402;&#19968;&#21270;&#23618;&#30340;&#26435;&#37325;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#20351;&#29992;&#20102;&#20960;&#20010;&#24320;&#28304;&#30340;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;
As the size of large language models (LLMs) continues to grow, model compression without sacrificing accuracy has become a crucial challenge for deployment. While some quantization methods, such as GPTQ, have made progress in achieving acceptable 4-bit weight-only quantization, attempts at lower bit quantization often result in severe performance degradation. In this paper, we introduce a technique called norm tweaking, which can be used as a plugin in current PTQ methods to achieve high precision while being cost-efficient. Our approach is inspired by the observation that rectifying the quantized activation distribution to match its float counterpart can readily restore accuracy for LLMs. To achieve this, we carefully design a tweaking strategy that includes calibration data generation and channel-wise distance constraint to update the weights of normalization layers for better generalization. We conduct extensive experiments on various datasets using several open-sourced LLMs. Our me
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20272;&#35745;&#36125;&#21494;&#26031;&#23454;&#39564;&#35774;&#35745;&#20013;&#26399;&#26395;&#20449;&#24687;&#22686;&#30410;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#20248;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#21518;&#39564;&#26399;&#26395;&#34920;&#31034;&#26469;&#20272;&#35745;&#19982;&#35774;&#35745;&#21464;&#37327;&#30456;&#20851;&#30340;&#26799;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;UEEG-MCMC&#21644;BEEG-AP&#20004;&#31181;&#20272;&#35745;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#23454;&#39564;&#35774;&#35745;&#38382;&#39064;&#19978;&#37117;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.09888</link><description>&lt;p&gt;
&#20851;&#20110;&#36125;&#21494;&#26031;&#23454;&#39564;&#35774;&#35745;&#20013;&#26399;&#26395;&#20449;&#24687;&#22686;&#30410;&#26799;&#24230;&#30340;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
On Estimating the Gradient of the Expected Information Gain in Bayesian Experimental Design. (arXiv:2308.09888v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09888
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20272;&#35745;&#36125;&#21494;&#26031;&#23454;&#39564;&#35774;&#35745;&#20013;&#26399;&#26395;&#20449;&#24687;&#22686;&#30410;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#20248;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#21518;&#39564;&#26399;&#26395;&#34920;&#31034;&#26469;&#20272;&#35745;&#19982;&#35774;&#35745;&#21464;&#37327;&#30456;&#20851;&#30340;&#26799;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;UEEG-MCMC&#21644;BEEG-AP&#20004;&#31181;&#20272;&#35745;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#23454;&#39564;&#35774;&#35745;&#38382;&#39064;&#19978;&#37117;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#23454;&#39564;&#35774;&#35745;&#26088;&#22312;&#25214;&#21040;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#26368;&#20339;&#23454;&#39564;&#26465;&#20214;&#65292;&#36890;&#24120;&#34987;&#25551;&#36848;&#20026;&#20248;&#21270;&#26399;&#26395;&#20449;&#24687;&#22686;&#30410;&#65288;EIG&#65289;&#12290;&#20026;&#20102;&#39640;&#25928;&#22320;&#20248;&#21270;EIG&#65292;&#24448;&#24448;&#38656;&#35201;&#26799;&#24230;&#20449;&#24687;&#65292;&#22240;&#27492;&#20272;&#35745;EIG&#30340;&#26799;&#24230;&#33021;&#21147;&#23545;&#20110;&#36125;&#21494;&#26031;&#23454;&#39564;&#35774;&#35745;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#35813;&#24037;&#20316;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#24320;&#21457;&#20272;&#35745;EIG&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#23454;&#29616;EIG&#30340;&#39640;&#25928;&#20248;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#19982;&#35774;&#35745;&#21464;&#37327;&#30456;&#20851;&#30340;EIG&#26799;&#24230;&#30340;&#21518;&#39564;&#26399;&#26395;&#34920;&#31034;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#20272;&#35745;EIG&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;UEEG-MCMC&#21033;&#29992;&#36890;&#36807;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#65288;MCMC&#65289;&#29983;&#25104;&#30340;&#21518;&#39564;&#26679;&#26412;&#26469;&#20272;&#35745;EIG&#26799;&#24230;&#65292;&#32780;BEEG-AP&#21017;&#19987;&#27880;&#20110;&#36890;&#36807;&#21453;&#22797;&#20351;&#29992;&#21442;&#25968;&#26679;&#26412;&#26469;&#23454;&#29616;&#39640;&#27169;&#25311;&#25928;&#29575;&#12290;&#29702;&#35770;&#20998;&#26512;&#21644;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#23454;&#39564;&#35774;&#35745;&#38382;&#39064;&#19978;&#37117;&#33021;&#33719;&#24471;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian Experimental Design (BED), which aims to find the optimal experimental conditions for Bayesian inference, is usually posed as to optimize the expected information gain (EIG). The gradient information is often needed for efficient EIG optimization, and as a result the ability to estimate the gradient of EIG is essential for BED problems. The primary goal of this work is to develop methods for estimating the gradient of EIG, which, combined with the stochastic gradient descent algorithms, result in efficient optimization of EIG. Specifically, we first introduce a posterior expected representation of the EIG gradient with respect to the design variables. Based on this, we propose two methods for estimating the EIG gradient, UEEG-MCMC that leverages posterior samples generated through Markov Chain Monte Carlo (MCMC) to estimate the EIG gradient, and BEEG-AP that focuses on achieving high simulation efficiency by repeatedly using parameter samples. Theoretical analysis and numerica
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22238;&#24402;&#20219;&#21153;&#30340;&#24191;&#20041;AuxUE&#26041;&#26696;&#65292;&#30446;&#30340;&#26159;&#23454;&#29616;&#26356;&#40065;&#26834;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#26041;&#26696;&#36890;&#36807;&#32771;&#34385;&#19981;&#21516;&#30340;&#20998;&#24067;&#20551;&#35774;&#65292;&#36873;&#25321;Laplace&#20998;&#24067;&#26469;&#36817;&#20284;p&#65292;&#20197;&#23454;&#29616;&#26356;&#40065;&#26834;&#30340;&#26412;&#36136;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2308.09065</link><description>&lt;p&gt;
&#36890;&#36807;&#31163;&#25955;&#21270;&#24341;&#21457;&#30340;Dirichlet&#21518;&#39564;&#29992;&#20110;&#22238;&#24402;&#38382;&#39064;&#30340;&#40065;&#26834;&#24615;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Discretization-Induced Dirichlet Posterior for Robust Uncertainty Quantification on Regression. (arXiv:2308.09065v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09065
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22238;&#24402;&#20219;&#21153;&#30340;&#24191;&#20041;AuxUE&#26041;&#26696;&#65292;&#30446;&#30340;&#26159;&#23454;&#29616;&#26356;&#40065;&#26834;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#26041;&#26696;&#36890;&#36807;&#32771;&#34385;&#19981;&#21516;&#30340;&#20998;&#24067;&#20551;&#35774;&#65292;&#36873;&#25321;Laplace&#20998;&#24067;&#26469;&#36817;&#20284;p&#65292;&#20197;&#23454;&#29616;&#26356;&#40065;&#26834;&#30340;&#26412;&#36136;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#23545;&#20110;&#37096;&#32626;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#33267;&#20851;&#37325;&#35201;&#12290;&#36741;&#21161;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#22120;&#65288;AuxUE&#65289;&#26159;&#19968;&#31181;&#22312;&#19981;&#20462;&#25913;&#20027;&#20219;&#21153;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#20027;&#20219;&#21153;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#26368;&#26377;&#25928;&#25163;&#27573;&#20043;&#19968;&#12290;&#20026;&#20102;&#34987;&#35748;&#20026;&#26159;&#40065;&#26834;&#30340;&#65292;AuxUE&#24517;&#39035;&#33021;&#22815;&#22312;&#36935;&#21040;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#36755;&#20837;&#26102;&#20445;&#25345;&#24615;&#33021;&#24182;&#24341;&#21457;&#26356;&#39640;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#21363;&#25552;&#20379;&#40065;&#26834;&#30340;&#26412;&#36136;&#19981;&#30830;&#23450;&#24615;&#21644;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35270;&#35273;&#22238;&#24402;&#20219;&#21153;&#65292;&#24403;&#21069;&#30340;AuxUE&#35774;&#35745;&#20027;&#35201;&#29992;&#20110;&#26412;&#36136;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#24182;&#19988;&#23578;&#26410;&#25506;&#32034;AuxUE&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22238;&#24402;&#20219;&#21153;&#30340;&#26356;&#40065;&#26834;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#24191;&#20041;AuxUE&#26041;&#26696;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20026;&#20102;&#23454;&#29616;&#26356;&#40065;&#26834;&#30340;&#26412;&#36136;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#22312;&#24322;&#26041;&#24046;&#22122;&#22768;&#26041;&#38754;&#32771;&#34385;&#20102;&#19981;&#21516;&#30340;&#20998;&#24067;&#20551;&#35774;&#65292;&#24182;&#26368;&#32456;&#36873;&#25321;Laplace&#20998;&#24067;&#26469;&#36817;&#20284;p
&lt;/p&gt;
&lt;p&gt;
Uncertainty quantification is critical for deploying deep neural networks (DNNs) in real-world applications. An Auxiliary Uncertainty Estimator (AuxUE) is one of the most effective means to estimate the uncertainty of the main task prediction without modifying the main task model. To be considered robust, an AuxUE must be capable of maintaining its performance and triggering higher uncertainties while encountering Out-of-Distribution (OOD) inputs, i.e., to provide robust aleatoric and epistemic uncertainty. However, for vision regression tasks, current AuxUE designs are mainly adopted for aleatoric uncertainty estimates, and AuxUE robustness has not been explored. In this work, we propose a generalized AuxUE scheme for more robust uncertainty quantification on regression tasks. Concretely, to achieve a more robust aleatoric uncertainty estimation, different distribution assumptions are considered for heteroscedastic noise, and Laplace distribution is finally chosen to approximate the p
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;Transformer&#27169;&#22411;&#20013;&#30340;&#38544;&#34255;&#29366;&#24577;&#65292;&#21457;&#29616;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#32534;&#30721;&#20102;&#26576;&#20123;&#36890;&#29992;&#30693;&#35782;&#25552;&#21462;&#27169;&#24335;&#65292;&#22240;&#27492;&#22312;&#36827;&#34892;&#27169;&#22411;&#32534;&#36753;&#26102;&#65292;&#19981;&#38656;&#35201;&#26356;&#26032;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#30340;&#26435;&#37325;&#12290;</title><link>http://arxiv.org/abs/2308.08742</link><description>&lt;p&gt;
PMET: &#22312;Transformer&#20013;&#30340;&#31934;&#30830;&#27169;&#22411;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
PMET: Precise Model Editing in a Transformer. (arXiv:2308.08742v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08742
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;Transformer&#27169;&#22411;&#20013;&#30340;&#38544;&#34255;&#29366;&#24577;&#65292;&#21457;&#29616;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#32534;&#30721;&#20102;&#26576;&#20123;&#36890;&#29992;&#30693;&#35782;&#25552;&#21462;&#27169;&#24335;&#65292;&#22240;&#27492;&#22312;&#36827;&#34892;&#27169;&#22411;&#32534;&#36753;&#26102;&#65292;&#19981;&#38656;&#35201;&#26356;&#26032;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#30340;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#21487;&#20197;&#20197;&#36739;&#20302;&#30340;&#25104;&#26412;&#20462;&#25913;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23569;&#37327;&#30693;&#35782;&#65292;&#24182;&#19988;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#29616;&#26377;&#26041;&#27861;&#20551;&#35774;Transformer&#23618;&#38544;&#34255;&#29366;&#24577;&#26159;&#21069;&#39304;&#32593;&#32476;&#30340;&#38190;&#20540;&#20869;&#23384;&#30340;&#20540;&#12290;&#23427;&#20204;&#36890;&#24120;&#20248;&#21270;Transformer&#23618;&#38544;&#34255;&#29366;&#24577;&#26469;&#35760;&#24518;&#30446;&#26631;&#30693;&#35782;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#26356;&#26032;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21069;&#39304;&#32593;&#32476;&#30340;&#26435;&#37325;&#12290;&#28982;&#32780;&#65292;Transformer&#23618;&#38544;&#34255;&#29366;&#24577;&#30340;&#20449;&#24687;&#27969;&#26469;&#33258;&#19977;&#20010;&#37096;&#20998;&#65306;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#12289;&#21069;&#39304;&#32593;&#32476;&#21644;&#27531;&#24046;&#36830;&#25509;&#12290;&#29616;&#26377;&#26041;&#27861;&#24573;&#35270;&#20102;Transformer&#23618;&#38544;&#34255;&#29366;&#24577;&#21253;&#21547;&#20102;&#21069;&#39304;&#32593;&#32476;&#29305;&#21035;&#38656;&#35201;&#30340;&#20449;&#24687;&#36825;&#19968;&#20107;&#23454;&#12290;&#22240;&#27492;&#65292;&#27169;&#22411;&#32534;&#36753;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#27169;&#22411;&#32534;&#36753;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#21644;&#21069;&#39304;&#32593;&#32476;&#30340;&#38544;&#34255;&#29366;&#24577;&#65292;&#21457;&#29616;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#32534;&#30721;&#20102;&#26576;&#20123;&#36890;&#29992;&#30693;&#35782;&#25552;&#21462;&#27169;&#24335;&#12290;&#36825;&#24847;&#21619;&#30528;&#24403;&#24341;&#20837;&#26032;&#30693;&#35782;&#26102;&#65292;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#30340;&#26435;&#37325;&#19981;&#38656;&#35201;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model editing techniques modify a minor proportion of knowledge in Large Language Models (LLMs) at a relatively low cost, which have demonstrated notable success. Existing methods assume Transformer Layer (TL) hidden states are values of key-value memories of the Feed-Forward Network (FFN). They usually optimize the TL hidden states to memorize target knowledge and use it to update the weights of the FFN in LLMs. However, the information flow of TL hidden states comes from three parts: Multi-Head Self-Attention (MHSA), FFN, and residual connections. Existing methods neglect the fact that the TL hidden states contains information not specifically required for FFN. Consequently, the performance of model editing decreases. To achieve more precise model editing, we analyze hidden states of MHSA and FFN, finding that MHSA encodes certain general knowledge extraction patterns. This implies that MHSA weights do not require updating when new knowledge is introduced. Based on above findings, we
&lt;/p&gt;</description></item><item><title>&#36873;&#25321;&#24615;&#31361;&#35302;&#20943;&#24369;&#65288;SSD&#65289;&#26159;&#24555;&#36895;&#12289;&#24615;&#33021;&#20248;&#36234;&#19988;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#30340;&#26426;&#22120;&#36951;&#24536;&#26041;&#27861;&#65292;&#33021;&#22815;&#21516;&#26102;&#20445;&#25252;&#27169;&#22411;&#30340;&#24615;&#33021;&#24182;&#36951;&#24536;&#29305;&#23450;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2308.07707</link><description>&lt;p&gt;
&#36890;&#36807;&#36873;&#25321;&#24615;&#31361;&#35302;&#20943;&#24369;&#23454;&#29616;&#24555;&#36895;&#30340;&#26426;&#22120;&#36951;&#24536;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Fast Machine Unlearning Without Retraining Through Selective Synaptic Dampening. (arXiv:2308.07707v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07707
&lt;/p&gt;
&lt;p&gt;
&#36873;&#25321;&#24615;&#31361;&#35302;&#20943;&#24369;&#65288;SSD&#65289;&#26159;&#24555;&#36895;&#12289;&#24615;&#33021;&#20248;&#36234;&#19988;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#30340;&#26426;&#22120;&#36951;&#24536;&#26041;&#27861;&#65292;&#33021;&#22815;&#21516;&#26102;&#20445;&#25252;&#27169;&#22411;&#30340;&#24615;&#33021;&#24182;&#36951;&#24536;&#29305;&#23450;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#36951;&#24536;&#65292;&#21363;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36951;&#24536;&#33021;&#21147;&#65292;&#27491;&#22312;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#20197;&#20415;&#31526;&#21512;&#25968;&#25454;&#38544;&#31169;&#27861;&#35268;&#65292;&#24182;&#21024;&#38500;&#26377;&#23475;&#12289;&#31713;&#25913;&#25110;&#36807;&#26102;&#30340;&#20449;&#24687;&#12290;&#20851;&#38190;&#25361;&#25112;&#22312;&#20110;&#22312;&#20445;&#25252;&#27169;&#22411;&#22312;&#20854;&#20313;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#30340;&#21516;&#26102;&#36951;&#24536;&#29305;&#23450;&#20449;&#24687;&#12290;&#34429;&#28982;&#30446;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#36890;&#24120;&#38656;&#35201;&#22312;&#20445;&#30041;&#25968;&#25454;&#19978;&#36827;&#34892;&#26576;&#31181;&#31243;&#24230;&#30340;&#37325;&#26032;&#35757;&#32451;&#65292;&#20197;&#20445;&#25252;&#25110;&#24674;&#22797;&#27169;&#22411;&#24615;&#33021;&#12290;&#36825;&#22686;&#21152;&#20102;&#35745;&#31639;&#24320;&#38144;&#65292;&#24182;&#35201;&#27714;&#35757;&#32451;&#25968;&#25454;&#20445;&#25345;&#21487;&#29992;&#21644;&#21487;&#35775;&#38382;&#65292;&#36825;&#21487;&#33021;&#24182;&#19981;&#21487;&#34892;&#12290;&#30456;&#21453;&#65292;&#20854;&#20182;&#26041;&#27861;&#37319;&#29992;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#30340;&#33539;&#24335;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#22312;&#35745;&#31639;&#19978;&#20195;&#20215;&#36807;&#39640;&#65292;&#24182;&#19988;&#24615;&#33021;&#19981;&#21450;&#37325;&#26032;&#35757;&#32451;&#30340;&#23545;&#24212;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#36873;&#25321;&#24615;&#31361;&#35302;&#20943;&#24369;&#65288;SSD&#65289;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#27493;&#21518;&#39564;&#12289;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#30340;&#26426;&#22120;&#36951;&#24536;&#26041;&#27861;&#65292;&#24555;&#36895;&#12289;&#24615;&#33021;&#20248;&#36234;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#38271;&#26102;&#38388;&#30340;&#27169;&#22411;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine unlearning, the ability for a machine learning model to forget, is becoming increasingly important to comply with data privacy regulations, as well as to remove harmful, manipulated, or outdated information. The key challenge lies in forgetting specific information while protecting model performance on the remaining data. While current state-of-the-art methods perform well, they typically require some level of retraining over the retained data, in order to protect or restore model performance. This adds computational overhead and mandates that the training data remain available and accessible, which may not be feasible. In contrast, other methods employ a retrain-free paradigm, however, these approaches are prohibitively computationally expensive and do not perform on par with their retrain-based counterparts. We present Selective Synaptic Dampening (SSD), a novel two-step, post hoc, retrain-free approach to machine unlearning which is fast, performant, and does not require lon
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36924;&#30495;&#19988;&#35821;&#20041;&#21487;&#25511;&#30340;&#21512;&#25104;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#34920;&#31034;&#23398;&#20064;&#30740;&#31350;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#28210;&#26579;&#25152;&#38656;&#25968;&#37327;&#30340;&#25968;&#25454;&#26679;&#26412;&#12289;&#31934;&#30830;&#25511;&#21046;&#22330;&#26223;&#21644;&#20998;&#24067;&#36716;&#21464;&#20197;&#21450;&#31934;&#32454;&#30340;&#30495;&#23454;&#26631;&#31614;&#30340;&#20248;&#28857;&#12290;</title><link>http://arxiv.org/abs/2308.03977</link><description>&lt;p&gt;
PUG:&#29992;&#20110;&#34920;&#31034;&#23398;&#20064;&#30340;&#36924;&#30495;&#19988;&#35821;&#20041;&#21487;&#25511;&#30340;&#21512;&#25104;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning. (arXiv:2308.03977v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03977
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36924;&#30495;&#19988;&#35821;&#20041;&#21487;&#25511;&#30340;&#21512;&#25104;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#34920;&#31034;&#23398;&#20064;&#30740;&#31350;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#28210;&#26579;&#25152;&#38656;&#25968;&#37327;&#30340;&#25968;&#25454;&#26679;&#26412;&#12289;&#31934;&#30830;&#25511;&#21046;&#22330;&#26223;&#21644;&#20998;&#24067;&#36716;&#21464;&#20197;&#21450;&#31934;&#32454;&#30340;&#30495;&#23454;&#26631;&#31614;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#22270;&#20687;&#25968;&#25454;&#38598;&#22312;&#35774;&#35745;&#21644;&#35780;&#20272;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#38754;&#20855;&#26377;&#29420;&#29305;&#30340;&#20248;&#21183;&#65306;&#23427;&#20204;&#21487;&#20197;&#28210;&#26579;&#25152;&#38656;&#25968;&#37327;&#30340;&#25968;&#25454;&#26679;&#26412;&#65292;&#31934;&#30830;&#25511;&#21046;&#27599;&#20010;&#22330;&#26223;&#24182;&#25552;&#20379;&#31934;&#32454;&#30340;&#30495;&#23454;&#26631;&#31614;&#65288;&#21644;&#26631;&#39064;&#65289;&#65292;&#24182;&#21487;&#20197;&#31934;&#30830;&#25511;&#21046;&#35757;&#32451;&#21644;&#27979;&#35797;&#20043;&#38388;&#30340;&#20998;&#24067;&#36716;&#21464;&#65292;&#20197;&#38548;&#31163;&#24863;&#20852;&#36259;&#30340;&#21464;&#37327;&#36827;&#34892;&#21487;&#38752;&#23454;&#39564;&#12290;&#23613;&#31649;&#20855;&#26377;&#22914;&#27492;&#20248;&#21183;&#65292;&#20294;&#21512;&#25104;&#22270;&#20687;&#25968;&#25454;&#30340;&#20351;&#29992;&#20173;&#21463;&#38480;&#21046;&#65292;&#24182;&#19988;&#36890;&#24120;&#30001;&#20110;&#20854;&#32570;&#20047;&#30495;&#23454;&#24615;&#32780;&#34987;&#24573;&#35270;&#12290;&#22240;&#27492;&#65292;&#22823;&#22810;&#25968;&#24037;&#20316;&#20173;&#20381;&#36182;&#20110;&#26469;&#33258;&#20114;&#32852;&#32593;&#19978;&#20844;&#24320;&#22270;&#20687;&#30340;&#30495;&#23454;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#21487;&#33021;&#23384;&#22312;&#38544;&#31169;&#12289;&#20559;&#35265;&#21644;&#29256;&#26435;&#38382;&#39064;&#65292;&#19988;&#23545;&#20110;&#23545;&#35937;&#30340;&#31934;&#30830;&#21576;&#29616;&#26041;&#24335;&#25511;&#21046;&#33021;&#21147;&#36739;&#23567;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21033;&#29992;&#36924;&#30495;&#30340;&#21512;&#25104;&#25968;&#25454;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#36884;&#24452;&#65306;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#29992;&#20110;&#34920;&#31034;&#23398;&#20064;&#30740;&#31350;&#30340;&#26032;&#19968;&#20195;&#20132;&#20114;&#29615;&#22659;&#65292;&#25552;&#20379;&#20102;&#21487;&#25511;&#24615;&#21644;&#36924;&#30495;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthetic image datasets offer unmatched advantages for designing and evaluating deep neural networks: they make it possible to (i) render as many data samples as needed, (ii) precisely control each scene and yield granular ground truth labels (and captions), (iii) precisely control distribution shifts between training and testing to isolate variables of interest for sound experimentation. Despite such promise, the use of synthetic image data is still limited -- and often played down -- mainly due to their lack of realism. Most works therefore rely on datasets of real images, which have often been scraped from public images on the internet, and may have issues with regards to privacy, bias, and copyright, while offering little control over how objects precisely appear. In this work, we present a path to democratize the use of photorealistic synthetic data: we develop a new generation of interactive environments for representation learning research, that offer both controllability and r
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;&#22312;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#20013;&#24212;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#29616;&#29366;&#12289;&#26426;&#20250;&#21644;&#24320;&#25918;&#30740;&#31350;&#38382;&#39064;&#12290;&#20316;&#32773;&#20027;&#35201;&#35752;&#35770;&#20102;&#19977;&#31181;&#24212;&#29992;&#31867;&#22411;&#65306;&#26080;&#29305;&#23450;&#30446;&#26631;&#30340;&#29983;&#25104;&#26041;&#24335;&#12289;&#21516;&#26102;&#26368;&#22823;&#21270;&#30446;&#26631;&#20989;&#25968;&#30340;&#36755;&#20986;&#29983;&#25104;&#26041;&#24335;&#20197;&#21450;&#23558;&#26080;&#27861;&#36890;&#36807;&#30446;&#26631;&#20989;&#25968;&#25429;&#25417;&#30340;&#26399;&#26395;&#29305;&#24449;&#23884;&#20837;&#29983;&#25104;&#36807;&#31243;&#30340;&#26041;&#24335;&#12290;&#36825;&#20010;&#26032;&#20852;&#39046;&#22495;&#20013;&#23384;&#22312;&#30528;&#20016;&#23500;&#30340;&#26426;&#20250;&#21644;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.00031</link><description>&lt;p&gt;
&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#30340;&#24378;&#21270;&#23398;&#20064;&#65306;&#29616;&#29366;&#12289;&#26426;&#20250;&#21644;&#24320;&#25918;&#30740;&#31350;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning for Generative AI: State of the Art, Opportunities and Open Research Challenges. (arXiv:2308.00031v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00031
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;&#22312;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#20013;&#24212;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#29616;&#29366;&#12289;&#26426;&#20250;&#21644;&#24320;&#25918;&#30740;&#31350;&#38382;&#39064;&#12290;&#20316;&#32773;&#20027;&#35201;&#35752;&#35770;&#20102;&#19977;&#31181;&#24212;&#29992;&#31867;&#22411;&#65306;&#26080;&#29305;&#23450;&#30446;&#26631;&#30340;&#29983;&#25104;&#26041;&#24335;&#12289;&#21516;&#26102;&#26368;&#22823;&#21270;&#30446;&#26631;&#20989;&#25968;&#30340;&#36755;&#20986;&#29983;&#25104;&#26041;&#24335;&#20197;&#21450;&#23558;&#26080;&#27861;&#36890;&#36807;&#30446;&#26631;&#20989;&#25968;&#25429;&#25417;&#30340;&#26399;&#26395;&#29305;&#24449;&#23884;&#20837;&#29983;&#25104;&#36807;&#31243;&#30340;&#26041;&#24335;&#12290;&#36825;&#20010;&#26032;&#20852;&#39046;&#22495;&#20013;&#23384;&#22312;&#30528;&#20016;&#23500;&#30340;&#26426;&#20250;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26159;&#36817;&#21313;&#24180;&#26469;&#35745;&#31639;&#26426;&#31185;&#23398;&#39046;&#22495;&#26368;&#20196;&#20154;&#20852;&#22859;&#30340;&#21457;&#23637;&#20043;&#19968;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#24050;&#32463;&#25104;&#20026;&#38750;&#24120;&#25104;&#21151;&#30340;&#33539;&#24335;&#12290;&#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#23558;RL&#24212;&#29992;&#20110;&#29983;&#25104;AI&#20013;&#30340;&#29616;&#29366;&#12289;&#26426;&#20250;&#21644;&#24320;&#25918;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#35752;&#35770;&#19977;&#31181;&#24212;&#29992;&#31867;&#22411;&#65292;&#21363;&#20316;&#20026;&#19968;&#31181;&#26080;&#29305;&#23450;&#30446;&#26631;&#30340;&#29983;&#25104;&#26041;&#24335;&#65292;&#20316;&#20026;&#19968;&#31181;&#21516;&#26102;&#26368;&#22823;&#21270;&#30446;&#26631;&#20989;&#25968;&#30340;&#36755;&#20986;&#29983;&#25104;&#26041;&#24335;&#65292;&#20197;&#21450;&#20316;&#20026;&#19968;&#31181;&#23558;&#26080;&#27861;&#36890;&#36807;&#30446;&#26631;&#20989;&#25968;&#36731;&#26494;&#25429;&#25417;&#30340;&#26399;&#26395;&#29305;&#24449;&#23884;&#20837;&#29983;&#25104;&#36807;&#31243;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#22312;&#35843;&#26597;&#32467;&#26524;&#20013;&#23545;&#36825;&#20010;&#36855;&#20154;&#30340;&#26032;&#20852;&#39046;&#22495;&#20013;&#30340;&#26426;&#20250;&#21644;&#25361;&#25112;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Artificial Intelligence (AI) is one of the most exciting developments in Computer Science of the last decade. At the same time, Reinforcement Learning (RL) has emerged as a very successful paradigm for a variety of machine learning tasks. In this survey, we discuss the state of the art, opportunities and open research questions in applying RL to generative AI. In particular, we will discuss three types of applications, namely, RL as an alternative way for generation without specified objectives; as a way for generating outputs while concurrently maximizing an objective function; and, finally, as a way of embedding desired characteristics, which cannot be easily captured by means of an objective function, into the generative process. We conclude the survey with an in-depth discussion of the opportunities and challenges in this fascinating emerging area.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22823;&#25968;&#25454;-&#20379;&#24212;&#38142;&#31649;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23454;&#29616;&#20379;&#24212;&#38142;&#39044;&#27979;&#65292;&#20248;&#21270;&#25805;&#20316;&#31649;&#29702;&#12289;&#36879;&#26126;&#24230;&#65292;&#24182;&#35752;&#35770;&#20102;&#24187;&#24433;&#24211;&#23384;&#23545;&#39044;&#27979;&#30340;&#19981;&#21033;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.12971</link><description>&lt;p&gt;
&#22823;&#25968;&#25454;-&#20379;&#24212;&#38142;&#31649;&#29702;&#26694;&#26550;&#30340;&#39044;&#27979;&#65306;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Big Data - Supply Chain Management Framework for Forecasting: Data Preprocessing and Machine Learning Techniques. (arXiv:2307.12971v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22823;&#25968;&#25454;-&#20379;&#24212;&#38142;&#31649;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23454;&#29616;&#20379;&#24212;&#38142;&#39044;&#27979;&#65292;&#20248;&#21270;&#25805;&#20316;&#31649;&#29702;&#12289;&#36879;&#26126;&#24230;&#65292;&#24182;&#35752;&#35770;&#20102;&#24187;&#24433;&#24211;&#23384;&#23545;&#39044;&#27979;&#30340;&#19981;&#21033;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#31995;&#32479;&#22320;&#35782;&#21035;&#21644;&#27604;&#36739;&#20998;&#26512;&#26368;&#20808;&#36827;&#30340;&#20379;&#24212;&#38142;&#39044;&#27979;&#31574;&#30053;&#21644;&#25216;&#26415;&#12290;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#23558;&#22823;&#25968;&#25454;&#20998;&#26512;&#24212;&#29992;&#20110;&#20379;&#24212;&#38142;&#31649;&#29702;&#20013;&#65292;&#21253;&#25324;&#38382;&#39064;&#35782;&#21035;&#12289;&#25968;&#25454;&#26469;&#28304;&#12289;&#25506;&#32034;&#24615;&#25968;&#25454;&#20998;&#26512;&#12289;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#12289;&#36229;&#21442;&#25968;&#35843;&#20248;&#12289;&#24615;&#33021;&#35780;&#20272;&#21644;&#20248;&#21270;&#65292;&#20197;&#21450;&#39044;&#27979;&#23545;&#20154;&#21147;&#12289;&#24211;&#23384;&#21644;&#25972;&#20010;&#20379;&#24212;&#38142;&#30340;&#24433;&#21709;&#12290;&#39318;&#20808;&#35752;&#35770;&#20102;&#26681;&#25454;&#20379;&#24212;&#38142;&#31574;&#30053;&#25910;&#38598;&#25968;&#25454;&#30340;&#38656;&#27714;&#20197;&#21450;&#22914;&#20309;&#25910;&#38598;&#25968;&#25454;&#12290;&#25991;&#31456;&#35752;&#35770;&#20102;&#26681;&#25454;&#21608;&#26399;&#25110;&#20379;&#24212;&#38142;&#30446;&#26631;&#38656;&#35201;&#19981;&#21516;&#31867;&#22411;&#30340;&#39044;&#27979;&#12290;&#25512;&#33616;&#20351;&#29992;&#20379;&#24212;&#38142;&#32489;&#25928;&#25351;&#26631;&#21644;&#35823;&#24046;&#27979;&#37327;&#31995;&#32479;&#26469;&#20248;&#21270;&#34920;&#29616;&#26368;&#20339;&#30340;&#27169;&#22411;&#12290;&#36824;&#35752;&#35770;&#20102;&#24187;&#24433;&#24211;&#23384;&#23545;&#39044;&#27979;&#30340;&#19981;&#21033;&#24433;&#21709;&#20197;&#21450;&#31649;&#29702;&#20915;&#31574;&#20381;&#36182;&#20379;&#24212;&#38142;&#32489;&#25928;&#25351;&#26631;&#26469;&#30830;&#23450;&#27169;&#22411;&#24615;&#33021;&#21442;&#25968;&#21644;&#25913;&#36827;&#36816;&#33829;&#31649;&#29702;&#12289;&#36879;&#26126;&#24230;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article intends to systematically identify and comparatively analyze state-of-the-art supply chain (SC) forecasting strategies and technologies. A novel framework has been proposed incorporating Big Data Analytics in SC Management (problem identification, data sources, exploratory data analysis, machine-learning model training, hyperparameter tuning, performance evaluation, and optimization), forecasting effects on human-workforce, inventory, and overall SC. Initially, the need to collect data according to SC strategy and how to collect them has been discussed. The article discusses the need for different types of forecasting according to the period or SC objective. The SC KPIs and the error-measurement systems have been recommended to optimize the top-performing model. The adverse effects of phantom inventory on forecasting and the dependence of managerial decisions on the SC KPIs for determining model performance parameters and improving operations management, transparency, and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#27169;&#22411;TwinLiteNet&#65292;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#30340;&#21487;&#39537;&#21160;&#21306;&#22495;&#21644;&#36710;&#36947;&#20998;&#21106;&#12290;&#35813;&#27169;&#22411;&#25104;&#26412;&#20302;&#24265;&#19988;&#39640;&#25928;&#20934;&#30830;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#35745;&#31639;&#36164;&#28304;&#33410;&#32422;&#12290;</title><link>http://arxiv.org/abs/2307.10705</link><description>&lt;p&gt;
TwinLiteNet&#65306;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#20013;&#21487;&#39537;&#21160;&#21306;&#22495;&#21644;&#36710;&#36947;&#20998;&#21106;&#30340;&#39640;&#25928;&#36731;&#37327;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
TwinLiteNet: An Efficient and Lightweight Model for Driveable Area and Lane Segmentation in Self-Driving Cars. (arXiv:2307.10705v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10705
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#27169;&#22411;TwinLiteNet&#65292;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#30340;&#21487;&#39537;&#21160;&#21306;&#22495;&#21644;&#36710;&#36947;&#20998;&#21106;&#12290;&#35813;&#27169;&#22411;&#25104;&#26412;&#20302;&#24265;&#19988;&#39640;&#25928;&#20934;&#30830;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#35745;&#31639;&#36164;&#28304;&#33410;&#32422;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#20998;&#21106;&#26159;&#33258;&#21160;&#39550;&#39542;&#20013;&#19968;&#20010;&#24120;&#35265;&#30340;&#20219;&#21153;&#65292;&#29992;&#20110;&#29702;&#35299;&#21608;&#22260;&#29615;&#22659;&#12290;&#23545;&#20110;&#36947;&#36335;&#19978;&#30340;&#23433;&#20840;&#21644;&#39640;&#25928;&#23548;&#33322;&#26469;&#35828;&#65292;&#21487;&#39537;&#21160;&#21306;&#22495;&#20998;&#21106;&#21644;&#36710;&#36947;&#26816;&#27979;&#23588;&#20026;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#21407;&#22987;&#30340;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#35745;&#31639;&#24320;&#38144;&#22823;&#65292;&#38656;&#35201;&#39640;&#31471;&#30828;&#20214;&#65292;&#36825;&#23545;&#20110;&#23884;&#20837;&#24335;&#31995;&#32479;&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#26469;&#35828;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#21487;&#39537;&#21160;&#21306;&#22495;&#21644;&#36710;&#36947;&#32447;&#20998;&#21106;&#27169;&#22411;&#12290;TwinLiteNet&#35774;&#35745;&#25104;&#25104;&#26412;&#20302;&#24265;&#65292;&#20294;&#33021;&#22815;&#23454;&#29616;&#20934;&#30830;&#21644;&#39640;&#25928;&#30340;&#20998;&#21106;&#32467;&#26524;&#12290;&#25105;&#20204;&#22312;BDD100K&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;TwinLiteNet&#65292;&#24182;&#19982;&#29616;&#20195;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;TwinLiteNet&#19982;&#29616;&#26377;&#26041;&#27861;&#34920;&#29616;&#30456;&#20284;&#65292;&#20294;&#25152;&#38656;&#30340;&#35745;&#31639;&#36164;&#28304;&#26174;&#33879;&#20943;&#23569;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;TwinLiteNet&#22312;&#21487;&#39537;&#21160;&#21306;&#22495;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;91.3%&#30340;mIoU&#35780;&#20998;&#65292;&#22312;&#36710;&#36947;&#26816;&#27979;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;31.08%&#30340;IoU&#35780;&#20998;&#65292;&#20165;&#20351;&#29992;&#20102;40&#19975;&#20010;&#21442;&#25968;&#65292;&#22312;GPU RTX&#19978;&#23454;&#29616;&#20102;415 FPS&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic segmentation is a common task in autonomous driving to understand the surrounding environment. Driveable Area Segmentation and Lane Detection are particularly important for safe and efficient navigation on the road. However, original semantic segmentation models are computationally expensive and require high-end hardware, which is not feasible for embedded systems in autonomous vehicles. This paper proposes a lightweight model for the driveable area and lane line segmentation. TwinLiteNet is designed cheaply but achieves accurate and efficient segmentation results. We evaluate TwinLiteNet on the BDD100K dataset and compare it with modern models. Experimental results show that our TwinLiteNet performs similarly to existing approaches, requiring significantly fewer computational resources. Specifically, TwinLiteNet achieves a mIoU score of 91.3% for the Drivable Area task and 31.08% IoU for the Lane Detection task with only 0.4 million parameters and achieves 415 FPS on GPU RTX 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#30740;&#31350;&#22312;&#32447;&#21644;&#31163;&#32447;&#22686;&#24378;&#23398;&#20064;&#20013; Bellman &#36817;&#20284;&#35823;&#24046;&#30340;&#20998;&#24067;&#21457;&#29616;&#65292;Bellman &#35823;&#24046;&#31526;&#21512;&#36923;&#36753;&#20998;&#24067;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992; Logistic &#26368;&#22823;&#20284;&#28982;&#20989;&#25968;&#20316;&#20026;&#26367;&#20195;&#26041;&#27861;&#30340;&#26041;&#26696;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.02345</link><description>&lt;p&gt;
LLQL: &#36923;&#36753;&#20284;&#28982; Q-Learning &#29992;&#20110;&#22686;&#24378;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
LLQL: Logistic Likelihood Q-Learning for Reinforcement Learning. (arXiv:2307.02345v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#30740;&#31350;&#22312;&#32447;&#21644;&#31163;&#32447;&#22686;&#24378;&#23398;&#20064;&#20013; Bellman &#36817;&#20284;&#35823;&#24046;&#30340;&#20998;&#24067;&#21457;&#29616;&#65292;Bellman &#35823;&#24046;&#31526;&#21512;&#36923;&#36753;&#20998;&#24067;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992; Logistic &#26368;&#22823;&#20284;&#28982;&#20989;&#25968;&#20316;&#20026;&#26367;&#20195;&#26041;&#27861;&#30340;&#26041;&#26696;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22686;&#24378;&#23398;&#20064;&#65288;RL&#65289;&#21487;&#20197;&#20998;&#20026;&#22312;&#32447;&#21644;&#31163;&#32447;&#20004;&#31181;&#21464;&#20307;&#12290;&#20316;&#20026;&#22312;&#32447;&#21644;&#31163;&#32447; RL &#30340;&#20851;&#38190;&#26041;&#38754;&#65292;&#24403;&#21069;&#23545; Bellman &#26041;&#31243;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20248;&#21270;&#25216;&#26415;&#21644;&#24615;&#33021;&#22686;&#24378;&#19978;&#65292;&#32780;&#19981;&#26159;&#25506;&#32034; Bellman &#35823;&#24046;&#30340;&#22266;&#26377;&#32467;&#26500;&#29305;&#24615;&#65292;&#22914;&#20854;&#20998;&#24067;&#29305;&#24449;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#23545; Bellman &#26041;&#31243;&#36827;&#34892;&#36845;&#20195;&#25506;&#32034;&#65292;&#30740;&#31350;&#20102;&#22312;&#32447; RL &#21644;&#31163;&#32447; RL &#20013; Bellman &#36817;&#20284;&#35823;&#24046;&#30340;&#20998;&#24067;&#24773;&#20917;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#26080;&#35770;&#26159;&#22312;&#32447; RL &#36824;&#26159;&#31163;&#32447; RL&#65292;Bellman &#35823;&#24046;&#37117;&#31526;&#21512;&#36923;&#36753;&#20998;&#24067;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#26412;&#30740;&#31350;&#37319;&#29992; Logistic &#26368;&#22823;&#20284;&#28982;&#20989;&#25968;&#65288;LLoss&#65289;&#20316;&#20026;&#24120;&#29992;&#30340; MSE Loss &#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#20551;&#35774; Bellman &#35823;&#24046;&#26381;&#20174;&#27491;&#24577;&#20998;&#24067;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#20551;&#35774;&#65292;&#22312;&#19981;&#21516;&#30340;&#22312;&#32447;&#21644;&#31163;&#32447;&#29615;&#22659;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern reinforcement learning (RL) can be categorized into online and offline variants. As a pivotal aspect of both online and offline RL, current research on the Bellman equation revolves primarily around optimization techniques and performance enhancement rather than exploring the inherent structural properties of the Bellman error, such as its distribution characteristics. This study investigates the distribution of the Bellman approximation error in both online and offline settings through iterative exploration of the Bellman equation. We observed that both in online RL and offline RL, the Bellman error conforms to a Logistic distribution. Building upon this discovery, this study employed the Logistics maximum likelihood function (LLoss) as an alternative to the commonly used MSE Loss, assuming that Bellman errors adhere to a normal distribution. We validated our hypotheses through extensive numerical experiments across diverse online and offline environments. In particular, we app
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#32534;&#30721;&#22120;&#29983;&#25104;&#36890;&#29992;&#40065;&#26834;&#23884;&#20837;&#65292;&#23558;&#20998;&#31867;&#25968;&#25454;&#36716;&#25442;&#20026;&#21521;&#37327;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#25239;&#35757;&#32451;&#65292;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.04064</link><description>&lt;p&gt;
&#36890;&#36807;&#36890;&#29992;&#40065;&#26834;&#23884;&#20837;&#23454;&#29616;&#20998;&#31867;&#25968;&#25454;&#30340;&#21487;&#36716;&#31227;&#23545;&#25239;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Transferable Adversarial Robustness for Categorical Data via Universal Robust Embeddings. (arXiv:2306.04064v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04064
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#32534;&#30721;&#22120;&#29983;&#25104;&#36890;&#29992;&#40065;&#26834;&#23884;&#20837;&#65292;&#23558;&#20998;&#31867;&#25968;&#25454;&#36716;&#25442;&#20026;&#21521;&#37327;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#25239;&#35757;&#32451;&#65292;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24456;&#22810;&#22330;&#26223;&#19979;&#65292;&#23545;&#20998;&#31867;&#25968;&#25454;&#30340;&#40065;&#26834;&#24615;&#38656;&#35201;&#26356;&#39640;&#30340;&#20851;&#27880;&#24230;&#12290;&#28982;&#32780;&#65292;&#20998;&#31867;&#25968;&#25454;&#20855;&#26377;&#31867;&#21035;&#29305;&#24449;&#65292;&#29616;&#26377;&#30340;&#20248;&#21270;&#31243;&#24207;&#26080;&#27861;&#30452;&#25509;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#33258;&#32534;&#30721;&#22120;&#29983;&#25104;&#36890;&#29992;&#40065;&#26834;&#23884;&#20837;&#65292;&#23558;&#20998;&#31867;&#25968;&#25454;&#36716;&#21270;&#21040;&#19968;&#20010;&#31354;&#38388;&#20013;&#65292;&#24182;&#23454;&#29616;&#23545;&#25239;&#35757;&#32451;&#65292;&#20174;&#32780;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#20445;&#25345;&#23545;&#20110;&#24178;&#20928;&#25968;&#25454;&#39640;&#20998;&#31867;&#20934;&#30830;&#24230;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#26412;&#25991;&#26041;&#27861;&#24212;&#29992;&#22312;&#19981;&#21516;&#30340;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#26102;&#65292;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Research on adversarial robustness is primarily focused on image and text data. Yet, many scenarios in which lack of robustness can result in serious risks, such as fraud detection, medical diagnosis, or recommender systems often do not rely on images or text but instead on tabular data. Adversarial robustness in tabular data poses two serious challenges. First, tabular datasets often contain categorical features, and therefore cannot be tackled directly with existing optimization procedures. Second, in the tabular domain, algorithms that are not based on deep networks are widely used and offer great performance, but algorithms to enhance robustness are tailored to neural networks (e.g. adversarial training).  In this paper, we tackle both challenges. We present a method that allows us to train adversarially robust deep networks for tabular data and to transfer this robustness to other classifiers via universal robust embeddings tailored to categorical data. These embeddings, created u
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;Jarzynski&#24179;&#31561;&#24335;&#21644;&#39034;&#24207;&#33945;&#29305;&#21345;&#32599;&#37319;&#26679;&#32469;&#36807;&#26631;&#20934;&#23545;&#27604;&#25955;&#24230;&#31639;&#27861;&#20013;&#30340;&#19981;&#21487;&#25511;&#36924;&#36817;&#35823;&#24046;&#65292;&#26377;&#25928;&#35757;&#32451;&#33021;&#37327;&#22522;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.19414</link><description>&lt;p&gt;
&#21033;&#29992;Jarzynski&#24179;&#31561;&#24335;&#39640;&#25928;&#35757;&#32451;&#33021;&#37327;&#22522;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Efficient Training of Energy-Based Models Using Jarzynski Equality. (arXiv:2305.19414v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19414
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;Jarzynski&#24179;&#31561;&#24335;&#21644;&#39034;&#24207;&#33945;&#29305;&#21345;&#32599;&#37319;&#26679;&#32469;&#36807;&#26631;&#20934;&#23545;&#27604;&#25955;&#24230;&#31639;&#27861;&#20013;&#30340;&#19981;&#21487;&#25511;&#36924;&#36817;&#35823;&#24046;&#65292;&#26377;&#25928;&#35757;&#32451;&#33021;&#37327;&#22522;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#37327;&#22522;&#27169;&#22411;&#26159;&#21463;&#32479;&#35745;&#29289;&#29702;&#21551;&#21457;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#22312;&#26080;&#30417;&#30563;&#23398;&#20064;&#20013;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;&#27169;&#22411;&#20998;&#24067;&#30456;&#23545;&#20110;&#25968;&#25454;&#20998;&#24067;&#30340;&#20132;&#21449;&#29109;&#26159;&#34913;&#37327;&#23427;&#20204;&#24615;&#33021;&#30340;&#26368;&#20339;&#25351;&#26631;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#20132;&#21449;&#29109;&#20316;&#20026;&#35757;&#32451;&#30446;&#26631;&#25361;&#25112;&#37325;&#37325;&#65292;&#22240;&#20026;&#23427;&#23545;&#20110;&#27169;&#22411;&#21442;&#25968;&#30340;&#26799;&#24230;&#35745;&#31639;&#38656;&#35201;&#23545;&#27169;&#22411;&#20998;&#24067;&#36827;&#34892;&#37319;&#26679;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;Jarzynski&#31561;&#24335;&#30340;&#38750;&#24179;&#34913;&#28909;&#21147;&#23398;&#32467;&#26524;&#65292;&#32467;&#21512;&#39034;&#24207;&#33945;&#29305;&#21345;&#32599;&#37319;&#26679;&#24037;&#20855;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#36827;&#34892;&#35745;&#31639;&#65292;&#36991;&#20813;&#20351;&#29992;&#26631;&#20934;&#23545;&#27604;&#25955;&#24230;&#31639;&#27861;&#25152;&#20135;&#29983;&#30340;&#19981;&#21487;&#25511;&#36924;&#36817;&#35823;&#24046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#26410;&#35843;&#25972;Langevin&#31639;&#27861;&#30340;&#20462;&#25913;&#29256;&#26412;&#65292;&#22312;&#20854;&#20013;&#27599;&#20010;Walker&#37117;&#20250;&#33719;&#24471;&#19968;&#20010;&#26435;&#37325;&#65292;&#20351;&#24471;&#33021;&#22815;&#22312;&#20219;&#20309;&#27493;&#39588;&#26102;&#20272;&#35745;&#20132;&#21449;&#29109;&#30340;&#26799;&#24230;&#65292;&#20174;&#32780;&#35268;&#36991;&#30001;&#37319;&#26679;&#20559;&#24046;&#23548;&#33268;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy-based models (EBMs) are generative models inspired by statistical physics with a wide range of applications in unsupervised learning. Their performance is best measured by the cross-entropy (CE) of the model distribution relative to the data distribution. Using the CE as the objective for training is however challenging because the computation of its gradient with respect to the model parameters requires sampling the model distribution. Here we show how results for nonequilibrium thermodynamics based on Jarzynski equality together with tools from sequential Monte-Carlo sampling can be used to perform this computation efficiently and avoid the uncontrolled approximations made using the standard contrastive divergence algorithm. Specifically, we introduce a modification of the unadjusted Langevin algorithm (ULA) in which each walker acquires a weight that enables the estimation of the gradient of the cross-entropy at any step during GD, thereby bypassing sampling biases induced by
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24322;&#26500;&#32676;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#32852;&#37030;Q&#23398;&#20064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#20445;&#35777;&#65292;&#35752;&#35770;&#20102;&#21516;&#27493;&#21644;&#24322;&#27493;&#29256;&#26412;&#30340;&#32447;&#24615;&#21152;&#36895;&#65292;&#21516;&#26102;&#25506;&#31350;&#20102;&#31561;&#26435;&#37325;&#24179;&#22343;&#26412;&#22320;Q&#20272;&#35745;&#30340;&#32570;&#38519;&#12290;</title><link>http://arxiv.org/abs/2305.10697</link><description>&lt;p&gt;
&#24322;&#26500;&#32676;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31119;&#38899;&#65306;&#32447;&#24615;&#21152;&#36895;&#21644;&#26356;&#22810;&#21487;&#33021;
&lt;/p&gt;
&lt;p&gt;
The Blessing of Heterogeneity in Federated Q-learning: Linear Speedup and Beyond. (arXiv:2305.10697v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10697
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24322;&#26500;&#32676;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#32852;&#37030;Q&#23398;&#20064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#20445;&#35777;&#65292;&#35752;&#35770;&#20102;&#21516;&#27493;&#21644;&#24322;&#27493;&#29256;&#26412;&#30340;&#32447;&#24615;&#21152;&#36895;&#65292;&#21516;&#26102;&#25506;&#31350;&#20102;&#31561;&#26435;&#37325;&#24179;&#22343;&#26412;&#22320;Q&#20272;&#35745;&#30340;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#25968;&#25454;&#30001;&#22810;&#20010;&#20195;&#29702;&#20197;&#20998;&#24067;&#24335;&#26041;&#24335;&#25910;&#38598;&#26102;&#65292;&#32852;&#37030;RL&#31639;&#27861;&#20801;&#35768;&#21327;&#20316;&#23398;&#20064;&#65292;&#26080;&#38656;&#20849;&#20139;&#26412;&#22320;&#25968;&#25454;&#12290;&#26412;&#25991;&#32771;&#34385;&#32852;&#37030;Q&#23398;&#20064;&#65292;&#20854;&#30446;&#30340;&#26159;&#36890;&#36807;&#23450;&#26399;&#32858;&#21512;&#20165;&#22312;&#26412;&#22320;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#26412;&#22320;Q&#20272;&#35745;&#26469;&#23398;&#20064;&#26368;&#20248;Q&#20989;&#25968;&#12290;&#38024;&#23545;&#26080;&#38480;&#26102;&#38388;&#33976;&#39311;&#26631;&#35760;&#20915;&#31574;&#36807;&#31243;&#65292;&#25105;&#20204;&#20026;&#21516;&#27493;&#21644;&#24322;&#27493;&#29256;&#26412;&#30340;&#32852;&#37030;Q&#23398;&#20064;&#25552;&#20379;&#20102;&#26679;&#26412;&#22797;&#26434;&#24230;&#20445;&#35777;&#12290;&#22312;&#20004;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#30028;&#38480;&#23637;&#31034;&#20102;&#19982;&#20195;&#29702;&#25968;&#37327;&#25104;&#32447;&#24615;&#21152;&#36895;&#20197;&#21450;&#20854;&#20182;&#26174;&#33879;&#38382;&#39064;&#21442;&#25968;&#30340;&#26356;&#23574;&#38160;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#32852;&#37030;Q&#23398;&#20064;&#26041;&#27861;&#37319;&#29992;&#31561;&#26435;&#37325;&#24179;&#22343;&#26412;&#22320;Q&#20272;&#35745;&#65292;&#36825;&#22312;&#24322;&#27493;&#35774;&#32622;&#20013;&#21487;&#33021;&#20250;&#39640;&#24230;&#27425;&#20248;&#65292;&#22240;&#20026;&#30001;&#20110;&#19981;&#21516;&#30340;&#26412;&#22320;&#34892;&#20026;&#31574;&#30053;&#65292;&#26412;&#22320;&#36712;&#36857;&#21487;&#33021;&#39640;&#24230;&#24322;&#26500;&#12290;&#29616;&#26377;&#30340;&#26679;&#26412;&#26368;&#20248;&#21270;&#31574;&#30053;&#22312;&#24322;&#27493;&#35774;&#32622;&#20013;&#23384;&#22312;&#24040;&#22823;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;
When the data used for reinforcement learning (RL) are collected by multiple agents in a distributed manner, federated versions of RL algorithms allow collaborative learning without the need of sharing local data. In this paper, we consider federated Q-learning, which aims to learn an optimal Q-function by periodically aggregating local Q-estimates trained on local data alone. Focusing on infinite-horizon tabular Markov decision processes, we provide sample complexity guarantees for both the synchronous and asynchronous variants of federated Q-learning. In both cases, our bounds exhibit a linear speedup with respect to the number of agents and sharper dependencies on other salient problem parameters. Moreover, existing approaches to federated Q-learning adopt an equally-weighted averaging of local Q-estimates, which can be highly sub-optimal in the asynchronous setting since the local trajectories can be highly heterogeneous due to different local behavior policies. Existing sample com
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#65292;&#21363;&#27704;&#32493;&#24615;&#20027;&#21160;&#23398;&#20064;&#65288;CAL&#65289;&#65292;&#36890;&#36807;&#20559;&#21521;&#20808;&#21069;&#26631;&#35760;&#38598;&#26469;&#21152;&#36895;&#35757;&#32451;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#31995;&#21015;&#22238;&#25918;&#26041;&#26696;&#65292;&#21253;&#25324;&#27169;&#22411;&#33976;&#39311;&#21644;&#20174;&#21382;&#21490;&#20013;&#36873;&#25321;&#22810;&#26679;&#21270;&#30340;&#21644;&#19981;&#30830;&#23450;&#30340;&#28857;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CAL&#21487;&#20197;&#22823;&#24133;&#25552;&#21319;&#35757;&#32451;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.06408</link><description>&lt;p&gt;
&#20351;&#29992;&#27704;&#32493;&#23398;&#20064;&#25216;&#26415;&#21152;&#36895;&#25209;&#27425;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Accelerating Batch Active Learning Using Continual Learning Techniques. (arXiv:2305.06408v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06408
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#65292;&#21363;&#27704;&#32493;&#24615;&#20027;&#21160;&#23398;&#20064;&#65288;CAL&#65289;&#65292;&#36890;&#36807;&#20559;&#21521;&#20808;&#21069;&#26631;&#35760;&#38598;&#26469;&#21152;&#36895;&#35757;&#32451;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#31995;&#21015;&#22238;&#25918;&#26041;&#26696;&#65292;&#21253;&#25324;&#27169;&#22411;&#33976;&#39311;&#21644;&#20174;&#21382;&#21490;&#20013;&#36873;&#25321;&#22810;&#26679;&#21270;&#30340;&#21644;&#19981;&#30830;&#23450;&#30340;&#28857;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CAL&#21487;&#20197;&#22823;&#24133;&#25552;&#21319;&#35757;&#32451;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#23398;&#20064;&#65288;AL&#65289;&#30340;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#26159;&#35757;&#32451;&#25104;&#26412;&#24456;&#39640;&#65292;&#22240;&#20026;&#27169;&#22411;&#36890;&#24120;&#22312;&#27599;&#20010;&#26597;&#35810;&#36718;&#20043;&#21518;&#37117;&#35201;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#35757;&#32451;&#12290;&#26412;&#25991;&#39318;&#20808;&#28436;&#31034;&#20102;&#20351;&#29992;&#39044;&#28909;&#21551;&#21160;&#30340;&#26631;&#20934;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;AL&#30340;&#22833;&#36133;&#65292;&#26082;&#19981;&#33021;&#21152;&#36895;&#35757;&#32451;&#65292;&#21448;&#19981;&#33021;&#36991;&#20813;&#22312;AL&#26597;&#35810;&#36718;&#19978;&#20351;&#29992;&#24494;&#35843;&#26102;&#21457;&#29983;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31867;&#26032;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#20559;&#21521;&#20808;&#21069;&#26631;&#35760;&#38598;&#26469;&#21152;&#36895;&#35757;&#32451;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#29616;&#26377;&#30340;&#21644;&#21457;&#23637;&#26032;&#30340;&#22522;&#20110;&#22238;&#25918;&#30340;&#27704;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#31639;&#27861;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#24555;&#36895;&#23398;&#20064;&#26032;&#30693;&#35782;&#32780;&#19981;&#36951;&#24536;&#32769;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#25968;&#25454;&#26469;&#33258;&#19981;&#26029;&#21464;&#21270;&#30340;&#20998;&#24067;&#26102;&#29305;&#21035;&#26377;&#25928;&#12290;&#25105;&#20204;&#23558;&#36825;&#19968;&#33539;&#20363;&#31216;&#20026;&#8220;&#27704;&#32493;&#24615;&#20027;&#21160;&#23398;&#20064;&#8221;&#65288;CAL&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;CAL&#36890;&#36807;&#20351;&#29992;&#22823;&#37327;&#30340;&#22238;&#25918;&#26041;&#26696;&#26469;&#23454;&#29616;&#26174;&#33879;&#30340;&#21152;&#36895;&#25928;&#26524;&#65292;&#36825;&#20123;&#26041;&#26696;&#20351;&#29992;&#27169;&#22411;&#33976;&#39311;&#24182;&#20174;&#21382;&#21490;&#20013;&#36873;&#25321;&#22810;&#26679;&#21270;&#30340;&#21644;&#19981;&#30830;&#23450;&#30340;&#28857;&#12290;&#25105;&#20204;&#22312;&#35768;&#22810;&#25968;&#25454;&#39046;&#22495;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#21253;&#25324;&#33258;&#28982;
&lt;/p&gt;
&lt;p&gt;
A major problem with Active Learning (AL) is high training costs since models are typically retrained from scratch after every query round. We start by demonstrating that standard AL on neural networks with warm starting fails, both to accelerate training and to avoid catastrophic forgetting when using fine-tuning over AL query rounds. We then develop a new class of techniques, circumventing this problem, by biasing further training towards previously labeled sets. We accomplish this by employing existing, and developing novel, replay-based Continual Learning (CL) algorithms that are effective at quickly learning the new without forgetting the old, especially when data comes from an evolving distribution. We call this paradigm Continual Active Learning (CAL). We show CAL achieves significant speedups using a plethora of replay schemes that use model distillation and that select diverse, uncertain points from the history. We conduct experiments across many data domains, including natura
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#26368;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#35821;&#20041;&#20998;&#21106;&#12289;&#39046;&#22495;&#36866;&#37197;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#20998;&#21106;&#26041;&#27861;&#65292;&#21487;&#22312;&#32454;&#32990;&#27700;&#24179;&#19978;&#23545;&#21151;&#33021;&#24615;&#32452;&#32455;&#21333;&#20803;&#36827;&#34892;&#20998;&#21106;&#65292;&#24182;&#21462;&#24471;&#20102;&#20808;&#36827;&#27700;&#24179;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.02148</link><description>&lt;p&gt;
&#32454;&#32990;&#27700;&#24179;&#30340;&#21151;&#33021;&#24615;&#32452;&#32455;&#21333;&#20803;&#21322;&#30417;&#30563;&#20998;&#21106;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Semi-Supervised Segmentation of Functional Tissue Units at the Cellular Level. (arXiv:2305.02148v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02148
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#26368;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#35821;&#20041;&#20998;&#21106;&#12289;&#39046;&#22495;&#36866;&#37197;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#20998;&#21106;&#26041;&#27861;&#65292;&#21487;&#22312;&#32454;&#32990;&#27700;&#24179;&#19978;&#23545;&#21151;&#33021;&#24615;&#32452;&#32455;&#21333;&#20803;&#36827;&#34892;&#20998;&#21106;&#65292;&#24182;&#21462;&#24471;&#20102;&#20808;&#36827;&#27700;&#24179;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#19982;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#28145;&#24230;&#23398;&#20064;&#35821;&#20041;&#20998;&#21106;&#26041;&#27861;&#65292;&#29992;&#20110;&#32454;&#32990;&#27700;&#24179;&#30340;&#21151;&#33021;&#24615;&#32452;&#32455;&#21333;&#20803;&#20998;&#21106;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#35821;&#20041;&#20998;&#21106;&#26041;&#27861;&#12289;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#65292;&#21487;&#26368;&#23567;&#21270;HPA&#21644;HubMAP&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#39046;&#22495;&#24046;&#24322;&#12289;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#25429;&#33719;&#35774;&#32622;&#24433;&#21709;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#21151;&#33021;&#24615;&#32452;&#32455;&#21333;&#20803;&#22312;&#32454;&#32990;&#27700;&#24179;&#30340;&#20998;&#21106;&#26041;&#38754;&#65292;&#36798;&#21040;&#20102;&#19982;&#29616;&#26377;&#20808;&#36827;&#26041;&#27861;&#21487;&#27604;&#25311;&#30340;&#32467;&#26524;&#12290;&#28304;&#20195;&#30721;&#21487;&#22312; https://github.com/VSydorskyy/hubmap_2022_htt_solution &#19978;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new method for functional tissue unit segmentation at the cellular level, which utilizes the latest deep learning semantic segmentation approaches together with domain adaptation and semi-supervised learning techniques. This approach allows for minimizing the domain gap, class imbalance, and captures settings influence between HPA and HubMAP datasets. The presented approach achieves comparable with state-of-the-art-result in functional tissue unit segmentation at the cellular level. The source code is available at https://github.com/VSydorskyy/hubmap_2022_htt_solution
&lt;/p&gt;</description></item><item><title>&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#31639;&#27861;&#20250;&#24847;&#22806;&#22320;&#35760;&#24518;&#21333;&#20010;&#35757;&#32451;&#26679;&#26412;&#20013;&#30340;&#29305;&#23450;&#37096;&#20998;&#65292;&#31216;&#20026;&#8220;&#20284;&#26366;&#30456;&#35782;&#8221;&#35760;&#24518;&#65292;&#35813;&#29616;&#35937;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#65292;&#19981;&#33021;&#34987;&#20256;&#32479;&#30340;&#35780;&#20272;&#26041;&#27861;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2304.13850</link><description>&lt;p&gt;
SSL&#27169;&#22411;&#26159;&#21542;&#24863;&#21040;&#8220;&#20284;&#26366;&#30456;&#35782;&#8221;&#65311;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#24847;&#22806;&#35760;&#24518;
&lt;/p&gt;
&lt;p&gt;
Do SSL Models Have D\'ej\`a Vu? A Case of Unintended Memorization in Self-supervised Learning. (arXiv:2304.13850v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13850
&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#31639;&#27861;&#20250;&#24847;&#22806;&#22320;&#35760;&#24518;&#21333;&#20010;&#35757;&#32451;&#26679;&#26412;&#20013;&#30340;&#29305;&#23450;&#37096;&#20998;&#65292;&#31216;&#20026;&#8220;&#20284;&#26366;&#30456;&#35782;&#8221;&#35760;&#24518;&#65292;&#35813;&#29616;&#35937;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#65292;&#19981;&#33021;&#34987;&#20256;&#32479;&#30340;&#35780;&#20272;&#26041;&#27861;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#31639;&#27861;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#23558;&#33258;&#28982;&#22270;&#20687;&#30340;&#19981;&#21516;&#37096;&#20998;&#30456;&#20114;&#20851;&#32852;&#26469;&#20135;&#29983;&#26377;&#29992;&#30340;&#22270;&#20687;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#24403;&#34987;&#25512;&#21521;&#26497;&#31471;&#26102;&#65292;SSL&#27169;&#22411;&#20250;&#24847;&#22806;&#22320;&#35760;&#24518;&#21333;&#20010;&#35757;&#32451;&#26679;&#26412;&#20013;&#30340;&#29305;&#23450;&#37096;&#20998;&#65292;&#32780;&#19981;&#26159;&#23398;&#20064;&#26377;&#24847;&#20041;&#30340;&#35821;&#20041;&#20851;&#32852;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;SSL&#27169;&#22411;&#20013;&#30340;&#24847;&#22806;&#35760;&#24518;&#29616;&#35937;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#20284;&#26366;&#30456;&#35782;&#8221;&#35760;&#24518;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#32473;&#23450;&#35757;&#32451;&#27169;&#22411;&#21644;&#19968;&#20010;&#20165;&#21253;&#21547;&#32972;&#26223;&#65288;&#22914;&#27700;&#12289;&#22825;&#31354;&#12289;&#33609;&#22320;&#65289;&#30340;&#35757;&#32451;&#22270;&#20687;&#35009;&#21098;&#21518;&#65292;&#21487;&#20197;&#39640;&#31934;&#24230;&#25110;&#29978;&#33267;&#35270;&#35273;&#37325;&#26500;&#22320;&#25512;&#26029;&#20986;&#21069;&#26223;&#23545;&#35937;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#8220;&#20284;&#26366;&#30456;&#35782;&#8221;&#35760;&#24518;&#26159;&#19981;&#21516;SSL&#31639;&#27861;&#30340;&#26222;&#36941;&#29616;&#35937;&#65292;&#24182;&#19988;&#20250;&#22240;&#26576;&#20123;&#35774;&#35745;&#36873;&#25321;&#32780;&#24694;&#21270;&#65292;&#32780;&#19988;&#19981;&#33021;&#36890;&#36807;&#20256;&#32479;&#30340;&#35780;&#20272;&#34920;&#31034;&#36136;&#37327;&#30340;&#25216;&#26415;&#26469;&#26816;&#27979;&#12290;&#8220;&#20284;&#26366;&#30456;&#35782;&#8221;&#35760;&#24518;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) algorithms can produce useful image representations by learning to associate different parts of natural images with one another. However, when taken to the extreme, SSL models can unintendedly memorize specific parts in individual training samples rather than learning semantically meaningful associations. In this work, we perform a systematic study of the unintended memorization of image-specific information in SSL models -- which we refer to as d\'ej\`a vu memorization. Concretely, we show that given the trained model and a crop of a training image containing only the background (e.g., water, sky, grass), it is possible to infer the foreground object with high accuracy or even visually reconstruct it. Furthermore, we show that d\'ej\`a vu memorization is common to different SSL algorithms, is exacerbated by certain design choices, and cannot be detected by conventional techniques for evaluating representation quality. Our study of d\'ej\`a vu memorizatio
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#37197;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#21457;&#23556;&#21151;&#29575;&#65292;&#20197;&#20248;&#21270;&#36890;&#20449;&#32422;&#26463;&#19979;FL&#36807;&#31243;&#20013;&#26381;&#21153;&#22120;&#31471;&#25509;&#25910;&#21040;&#30340;&#20449;&#24687;&#65292;&#24182;&#25552;&#39640;&#20840;&#23616;FL&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.09329</link><description>&lt;p&gt;
&#23398;&#20064;&#22312;&#26080;&#32447;&#32852;&#37030;&#23398;&#20064;&#20013;&#20855;&#26377;&#21487;&#35777;&#26126;&#20445;&#38556;&#30340;&#21457;&#23556;
&lt;/p&gt;
&lt;p&gt;
Learning to Transmit with Provable Guarantees in Wireless Federated Learning. (arXiv:2304.09329v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09329
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#37197;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#21457;&#23556;&#21151;&#29575;&#65292;&#20197;&#20248;&#21270;&#36890;&#20449;&#32422;&#26463;&#19979;FL&#36807;&#31243;&#20013;&#26381;&#21153;&#22120;&#31471;&#25509;&#25910;&#21040;&#30340;&#20449;&#24687;&#65292;&#24182;&#25552;&#39640;&#20840;&#23616;FL&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#37197;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#21457;&#23556;&#21151;&#29575;&#65292;&#36866;&#29992;&#20110;&#24178;&#25200;&#21463;&#38480;&#26080;&#32447;&#32593;&#32476;&#20013;&#30340;&#25361;&#25112;&#24615;&#24773;&#26223;&#65292;&#22914;FL&#35757;&#32451;&#36807;&#31243;&#20013;&#26080;&#32447;&#20449;&#36947;&#27491;&#22312;&#21464;&#21270;&#20197;&#21450;&#26412;&#22320;&#35774;&#22791;&#19978;&#30340;&#35757;&#32451;&#25968;&#25454;&#19981;&#26159;&#29420;&#31435;&#19988;&#21516;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#12290;&#30452;&#35266;&#26469;&#35828;&#65292;&#21151;&#29575;&#31574;&#30053;&#26088;&#22312;&#20248;&#21270;&#36890;&#20449;&#32422;&#26463;&#19979;FL&#36807;&#31243;&#20013;&#26381;&#21153;&#22120;&#31471;&#25509;&#25910;&#21040;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25552;&#39640;&#20840;&#23616;FL&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#35813;&#31574;&#30053;&#20351;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#36827;&#34892;&#21442;&#25968;&#21270;&#65292;&#24182;&#36890;&#36807;&#21407;&#22987;-&#23545;&#20598;&#31639;&#27861;&#27714;&#35299;&#30456;&#20851;&#30340;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#34920;&#26126;&#20102;&#21046;&#23450;&#38382;&#39064;&#20855;&#26377;&#38646;&#23545;&#20598;&#38388;&#38553;&#65292;&#24182;&#19988;&#19968;&#26086;&#21442;&#25968;&#21270;&#21151;&#29575;&#31574;&#30053;&#65292;&#21017;&#26368;&#20248;&#24615;&#21462;&#20915;&#20110;&#27492;&#21442;&#25968;&#21270;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#25968;&#20540;&#19978;&#65292;&#25105;&#20204;&#20351;&#29992;&#23454;&#38469;&#30340;&#26080;&#32447;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#20248;&#20110;&#20256;&#32479;&#30340;&#21151;&#29575;&#20998;&#37197;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel data-driven approach to allocate transmit power for federated learning (FL) over interference-limited wireless networks. The proposed method is useful in challenging scenarios where the wireless channel is changing during the FL training process and when the training data are not independent and identically distributed (non-i.i.d.) on the local devices. Intuitively, the power policy is designed to optimize the information received at the server end during the FL process under communication constraints. Ultimately, our goal is to improve the accuracy and efficiency of the global FL model being trained. The proposed power allocation policy is parameterized using a graph convolutional network and the associated constrained optimization problem is solved through a primal-dual (PD) algorithm. Theoretically, we show that the formulated problem has zero duality gap and, once the power policy is parameterized, optimality depends on how expressive this parameterization is. Nu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#20174;&#22810;&#26679;&#21270;&#30417;&#30563;&#20219;&#21153;&#20013;&#25552;&#21462;&#30340;&#30693;&#35782;&#26469;&#23398;&#20064;&#36890;&#29992;&#30340;&#35299;&#32544;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#20351;&#30417;&#30563;&#22810;&#20219;&#21153;&#27169;&#22411;&#30340;&#29305;&#24449;&#31354;&#38388;&#24471;&#20197;&#35299;&#32544;&#65292;&#36866;&#24403;&#20849;&#20139;&#20449;&#24687;&#65292;&#36798;&#21040;&#21487;&#35782;&#21035;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.07939</link><description>&lt;p&gt;
&#21033;&#29992;&#31232;&#30095;&#21644;&#20849;&#20139;&#29305;&#24449;&#28608;&#27963;&#36827;&#34892;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Leveraging sparse and shared feature activations for disentangled representation learning. (arXiv:2304.07939v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07939
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#20174;&#22810;&#26679;&#21270;&#30417;&#30563;&#20219;&#21153;&#20013;&#25552;&#21462;&#30340;&#30693;&#35782;&#26469;&#23398;&#20064;&#36890;&#29992;&#30340;&#35299;&#32544;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#20351;&#30417;&#30563;&#22810;&#20219;&#21153;&#27169;&#22411;&#30340;&#29305;&#24449;&#31354;&#38388;&#24471;&#20197;&#35299;&#32544;&#65292;&#36866;&#24403;&#20849;&#20139;&#20449;&#24687;&#65292;&#36798;&#21040;&#21487;&#35782;&#21035;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36804;&#20170;&#20026;&#27490;&#65292;&#24674;&#22797;&#39640;&#32500;&#25968;&#25454;&#30340;&#28508;&#22312;&#21464;&#21270;&#22240;&#32032;&#19968;&#30452;&#38598;&#20013;&#22312;&#31616;&#21333;&#30340;&#21512;&#25104;&#29615;&#22659;&#20013;&#12290;&#22312;&#22823;&#22810;&#25968;&#22522;&#20110;&#26080;&#30417;&#30563;&#21644;&#24369;&#30417;&#30563;&#30446;&#26631;&#30340;&#20808;&#21069;&#30740;&#31350;&#20013;&#65292;&#20154;&#20204;&#24573;&#30053;&#20102;&#36825;&#23545;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#34920;&#31034;&#23398;&#20064;&#30340;&#31215;&#26497;&#24433;&#21709;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24314;&#35758;&#21033;&#29992;&#20174;&#22810;&#26679;&#21270;&#30417;&#30563;&#20219;&#21153;&#20013;&#25552;&#21462;&#30340;&#30693;&#35782;&#26469;&#23398;&#20064;&#36890;&#29992;&#30340;&#35299;&#32544;&#34920;&#31034;&#12290;&#25105;&#20204;&#20551;&#35774;&#27599;&#20010;&#30417;&#30563;&#20219;&#21153;&#20165;&#20381;&#36182;&#20110;&#26410;&#30693;&#22240;&#32032;&#30340;&#23376;&#38598;&#65292;&#25105;&#20204;&#23558;&#30417;&#30563;&#22810;&#20219;&#21153;&#27169;&#22411;&#30340;&#29305;&#24449;&#31354;&#38388;&#35299;&#32544;&#65292;&#24182;&#22312;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#31232;&#30095;&#22320;&#28608;&#27963;&#29305;&#24449;&#24182;&#36866;&#24403;&#22320;&#20849;&#20139;&#20449;&#24687;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#20174;&#26410;&#30452;&#25509;&#35266;&#23519;&#21040;&#21464;&#24322;&#22240;&#32032;&#65292;&#20294;&#22312;&#20805;&#20998;&#24615;&#21644;&#26368;&#23567;&#24615;&#20551;&#35774;&#19979;&#65292;&#35775;&#38382;&#22810;&#20010;&#20219;&#21153;&#36275;&#20197;&#23454;&#29616;&#21487;&#35782;&#21035;&#24615;&#12290;&#25105;&#20204;&#22312;&#20845;&#20010;&#30495;&#23454;&#19990;&#30028;&#20998;&#24067;&#36716;&#31227;&#22522;&#20934;&#20197;&#21450;&#19981;&#21516;&#30340;&#25968;&#25454;&#27169;&#24577;&#65288;&#22270;&#20687;&#65292;&#25991;&#26412;&#65289;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recovering the latent factors of variation of high dimensional data has so far focused on simple synthetic settings. Mostly building on unsupervised and weakly-supervised objectives, prior work missed out on the positive implications for representation learning on real world data. In this work, we propose to leverage knowledge extracted from a diversified set of supervised tasks to learn a common disentangled representation. Assuming each supervised task only depends on an unknown subset of the factors of variation, we disentangle the feature space of a supervised multi-task model, with features activating sparsely across different tasks and information being shared as appropriate. Importantly, we never directly observe the factors of variations but establish that access to multiple tasks is sufficient for identifiability under sufficiency and minimality assumptions. We validate our approach on six real world distribution shift benchmarks, and different data modalities (images, text), 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#39044;&#27979;&#35823;&#24046;&#21442;&#25968;&#21270;&#20026;&#22810;&#20010;&#26102;&#38388;&#27493;&#38271;&#65292;&#20197;&#25214;&#21040;&#19981;&#20445;&#23432;&#30340;&#39044;&#27979;&#21306;&#38388;&#65292;&#23454;&#29616;&#22312;&#20351;&#29992;&#23398;&#20064;&#21551;&#29992;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22120;&#36827;&#34892;&#38271;&#26399;&#35268;&#21010;&#21644;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2304.01075</link><description>&lt;p&gt;
&#20351;&#29992;&#32447;&#24615;&#20114;&#34917;&#32534;&#31243;&#30340;&#26102;&#24207;&#31526;&#21512;&#39044;&#27979;&#21306;&#38388;
&lt;/p&gt;
&lt;p&gt;
Conformal Prediction Regions for Time Series using Linear Complementarity Programming. (arXiv:2304.01075v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#39044;&#27979;&#35823;&#24046;&#21442;&#25968;&#21270;&#20026;&#22810;&#20010;&#26102;&#38388;&#27493;&#38271;&#65292;&#20197;&#25214;&#21040;&#19981;&#20445;&#23432;&#30340;&#39044;&#27979;&#21306;&#38388;&#65292;&#23454;&#29616;&#22312;&#20351;&#29992;&#23398;&#20064;&#21551;&#29992;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22120;&#36827;&#34892;&#38271;&#26399;&#35268;&#21010;&#21644;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31526;&#21512;&#39044;&#27979;&#26159;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#21306;&#38388;&#30340;&#32479;&#35745;&#24037;&#20855;&#65292;&#20854;&#20855;&#26377;&#39640;&#27010;&#29575;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#23558;&#31526;&#21512;&#39044;&#27979;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20250;&#23548;&#33268;&#20445;&#23432;&#30340;&#39044;&#27979;&#21306;&#38388;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#26469;&#20943;&#23569;&#36825;&#31181;&#20445;&#23432;&#24615;&#65292;&#20197;&#20415;&#22312;&#20351;&#29992;&#23398;&#20064;&#21551;&#29992;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22120;&#36827;&#34892;&#38271;&#26399;&#35268;&#21010;&#21644;&#39564;&#35777;&#12290;&#25105;&#20204;&#23558;&#39044;&#27979;&#35823;&#24046;&#21442;&#25968;&#21270;&#20026;&#22810;&#20010;&#26102;&#38388;&#27493;&#38271;&#65292;&#36890;&#36807;&#23545;&#39069;&#22806;&#25968;&#25454;&#38598;&#19978;&#30340;&#21442;&#25968;&#36827;&#34892;&#20248;&#21270;&#65292;&#25214;&#21040;&#20102;&#19981;&#20445;&#23432;&#30340;&#39044;&#27979;&#21306;&#38388;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#35813;&#38382;&#39064;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#19968;&#20010;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#20114;&#34917;&#35268;&#21010;&#65288;MILCP&#65289;&#65292;&#25105;&#20204;&#23558;&#20854;&#25918;&#23485;&#20026;&#19968;&#20010;&#32447;&#24615;&#20114;&#34917;&#35268;&#21010;&#65288;LCP&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conformal prediction is a statistical tool for producing prediction regions of machine learning models that are valid with high probability. However, applying conformal prediction to time series data leads to conservative prediction regions. In fact, to obtain prediction regions over $T$ time steps with confidence $1-\delta$, {previous works require that each individual prediction region is valid} with confidence $1-\delta/T$. We propose an optimization-based method for reducing this conservatism to enable long horizon planning and verification when using learning-enabled time series predictors. Instead of considering prediction errors individually at each time step, we consider a parameterized prediction error over multiple time steps. By optimizing the parameters over an additional dataset, we find prediction regions that are not conservative. We show that this problem can be cast as a mixed integer linear complementarity program (MILCP), which we then relax into a linear complementa
&lt;/p&gt;</description></item><item><title>&#38598;&#25104;&#24378;&#21270;&#23398;&#20064;&#65288;ERL&#65289;&#26159;&#19968;&#31181;&#23558;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#19982;&#38598;&#25104;&#23398;&#20064;&#65288;EL&#65289;&#30456;&#32467;&#21512;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#21033;&#29992;&#22810;&#20010;&#27169;&#22411;&#25110;&#22521;&#35757;&#31639;&#27861;&#20840;&#38754;&#25506;&#32034;&#38382;&#39064;&#31354;&#38388;&#65292;&#24182;&#20855;&#26377;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.02618</link><description>&lt;p&gt;
&#38598;&#25104;&#24378;&#21270;&#23398;&#20064;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Ensemble Reinforcement Learning: A Survey. (arXiv:2303.02618v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02618
&lt;/p&gt;
&lt;p&gt;
&#38598;&#25104;&#24378;&#21270;&#23398;&#20064;&#65288;ERL&#65289;&#26159;&#19968;&#31181;&#23558;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#19982;&#38598;&#25104;&#23398;&#20064;&#65288;EL&#65289;&#30456;&#32467;&#21512;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#21033;&#29992;&#22810;&#20010;&#27169;&#22411;&#25110;&#22521;&#35757;&#31639;&#27861;&#20840;&#38754;&#25506;&#32034;&#38382;&#39064;&#31354;&#38388;&#65292;&#24182;&#20855;&#26377;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24050;&#25104;&#20026;&#35299;&#20915;&#21508;&#31181;&#31185;&#23398;&#21644;&#24212;&#29992;&#38382;&#39064;&#30340;&#39640;&#25928;&#25216;&#26415;&#12290;&#23613;&#31649;&#20854;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#26576;&#20123;&#22797;&#26434;&#20219;&#21153;&#20173;&#38590;&#20197;&#20165;&#20351;&#29992;&#21333;&#20010;&#27169;&#22411;&#21644;&#31639;&#27861;&#35299;&#20915;&#12290;&#20316;&#20026;&#21709;&#24212;&#65292;&#38598;&#25104;&#24378;&#21270;&#23398;&#20064;&#65288;ERL&#65289;&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;RL&#21644;&#38598;&#25104;&#23398;&#20064;&#65288;EL&#65289;&#30340;&#20248;&#28857;&#65292;&#24050;&#32463;&#24191;&#27867;&#21463;&#21040;&#27426;&#36814;&#12290;ERL&#21033;&#29992;&#22810;&#20010;&#27169;&#22411;&#25110;&#22521;&#35757;&#31639;&#27861;&#20840;&#38754;&#25506;&#32034;&#38382;&#39064;&#31354;&#38388;&#65292;&#24182;&#20855;&#26377;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#20379;ERL&#30340;&#32508;&#21512;&#35843;&#26597;&#65292;&#20197;&#20415;&#20026;&#35835;&#32773;&#25552;&#20379;&#35813;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#25361;&#25112;&#27010;&#36848;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;ERL&#30340;&#32972;&#26223;&#21644;&#21160;&#26426;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35814;&#32454;&#20998;&#26512;&#20102;&#25104;&#21151;&#24212;&#29992;&#20110;ERL&#20013;&#30340;&#31574;&#30053;&#65292;&#21253;&#25324;&#27169;&#22411;&#24179;&#22343;&#12289;&#27169;&#22411;&#36873;&#25321;&#21644;&#27169;&#22411;&#32452;&#21512;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#30456;&#20851;&#25968;&#25454;&#38598;&#24182;&#20998;&#26512;&#20102;&#25152;&#20351;&#29992;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning (RL) has emerged as a highly effective technique for addressing various scientific and applied problems. Despite its success, certain complex tasks remain challenging to be addressed solely with a single model and algorithm. In response, ensemble reinforcement learning (ERL), a promising approach that combines the benefits of both RL and ensemble learning (EL), has gained widespread popularity. ERL leverages multiple models or training algorithms to comprehensively explore the problem space and possesses strong generalization capabilities. In this study, we present a comprehensive survey on ERL to provide readers with an overview of recent advances and challenges in the field. First, we introduce the background and motivation for ERL. Second, we analyze in detail the strategies that have been successfully applied in ERL, including model averaging, model selection, and model combination. Subsequently, we summarize the datasets and analyze algorithms used in releva
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#28304;&#22495;&#36866;&#24212;&#30340;&#23454;&#29992;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#22312;&#21407;&#22987;&#29305;&#24449;&#31354;&#38388;&#32858;&#31867;&#65292;&#26500;&#24314;&#30495;&#27491;&#22256;&#38590;&#30340;&#36127;&#23545;&#65292;&#32467;&#21512;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#29702;&#35770;&#65292;&#23398;&#20064;&#19968;&#20010;&#22495;&#19981;&#21464;&#30340;&#29305;&#24449;&#26469;&#35299;&#20915;&#22495;&#19978;&#30340;&#24046;&#24322;&#38382;&#39064;&#65292;&#33021;&#22815;&#22312;&#19977;&#20010;&#24120;&#35265;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#26377;&#25928;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2301.13428</link><description>&lt;p&gt;
&#23545;&#27604;&#19982;&#32858;&#31867;&#65306;&#23398;&#20064;&#37051;&#22495;&#23545;&#34920;&#31034;&#29992;&#20110;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Contrast and Clustering: Learning Neighborhood Pair Representation for Source-free Domain Adaptation. (arXiv:2301.13428v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13428
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#28304;&#22495;&#36866;&#24212;&#30340;&#23454;&#29992;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#22312;&#21407;&#22987;&#29305;&#24449;&#31354;&#38388;&#32858;&#31867;&#65292;&#26500;&#24314;&#30495;&#27491;&#22256;&#38590;&#30340;&#36127;&#23545;&#65292;&#32467;&#21512;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#29702;&#35770;&#65292;&#23398;&#20064;&#19968;&#20010;&#22495;&#19981;&#21464;&#30340;&#29305;&#24449;&#26469;&#35299;&#20915;&#22495;&#19978;&#30340;&#24046;&#24322;&#38382;&#39064;&#65292;&#33021;&#22815;&#22312;&#19977;&#20010;&#24120;&#35265;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#26377;&#25928;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#22495;&#36866;&#24212;&#21033;&#29992;&#26469;&#33258;&#19981;&#21516;&#20998;&#24067;&#30340;&#28304;&#25968;&#25454;&#35299;&#20915;&#20174;&#26410;&#26631;&#35760;&#30446;&#26631;&#22495;&#20998;&#31867;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#26041;&#27861;&#38656;&#35201;&#35775;&#38382;&#28304;&#25968;&#25454;&#65292;&#36825;&#32463;&#24120;&#24341;&#36215;&#25968;&#25454;&#38544;&#31169;&#26041;&#38754;&#30340;&#25285;&#24551;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#26356;&#21152;&#23454;&#38469;&#20294;&#20805;&#28385;&#25361;&#25112;&#30340;&#35774;&#32622;&#65292;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#28304;&#22495;&#25968;&#25454;&#19981;&#21487;&#29992;&#65292;&#30446;&#26631;&#22495;&#25968;&#25454;&#26410;&#26631;&#35760;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20174;&#23545;&#27604;&#23398;&#20064;&#30340;&#35282;&#24230;&#35299;&#20915;&#20102;&#22495;&#38388;&#24046;&#24322;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#36890;&#36807;&#20197;&#19979;&#26041;&#24335;&#23398;&#20064;&#19968;&#20010;&#22495;&#19981;&#21464;&#30340;&#29305;&#24449;:1)&#22312;&#21407;&#22987;&#29305;&#24449;&#31354;&#38388;&#20013;&#30452;&#25509;&#25191;&#34892;&#20855;&#26377;&#26368;&#36817;&#37051;&#23621;&#30340;&#32858;&#31867;&#65307;2)&#36890;&#36807;&#25193;&#23637;&#37051;&#23621;&#26500;&#36896;&#30495;&#27491;&#22256;&#38590;&#30340;&#36127;&#23545;&#65292;&#32780;&#19981;&#24341;&#20837;&#39069;&#22806;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65307;3)&#32467;&#21512;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#29702;&#35770;&#20197;&#33719;&#24471;&#35745;&#31639;&#20248;&#21183;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#24120;&#35265;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;VisDA&#12289;Office-Home&#21644;Office-31&#19978;&#36827;&#34892;&#20102;&#20180;&#32454;&#30340;&#28040;&#34701;&#30740;&#31350;&#21644;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised domain adaptation uses source data from different distributions to solve the problem of classifying data from unlabeled target domains. However, conventional methods require access to source data, which often raise concerns about data privacy. In this paper, we consider a more practical but challenging setting where the source domain data is unavailable and the target domain data is unlabeled. Specifically, we address the domain discrepancy problem from the perspective of contrastive learning. The key idea of our work is to learn a domain-invariant feature by 1) performing clustering directly in the original feature space with nearest neighbors; 2) constructing truly hard negative pairs by extended neighbors without introducing additional computational complexity; and 3) combining noise-contrastive estimation theory to gain computational advantage. We conduct careful ablation studies and extensive experiments on three common benchmarks: VisDA, Office-Home, and Office-31. T
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#24341;&#23548;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#37319;&#29992;&#24191;&#20041;&#36125;&#21494;&#26031;&#31639;&#27861;&#36827;&#34892;&#28151;&#21512;&#31227;&#21160;&#24179;&#22343;&#22330;&#24341;&#23548;&#30340;&#26102;&#31354;&#25968;&#25454;&#24314;&#27169;&#65292;&#21487;&#20197;&#36827;&#34892;&#22240;&#26524;&#26410;&#26469;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2301.00736</link><description>&lt;p&gt;
&#28151;&#21512;&#31227;&#21160;&#24179;&#22343;&#22330;&#24341;&#23548;&#30340;&#26102;&#31354;&#25968;&#25454;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Mixed moving average field guided learning for spatio-temporal data. (arXiv:2301.00736v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00736
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#24341;&#23548;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#37319;&#29992;&#24191;&#20041;&#36125;&#21494;&#26031;&#31639;&#27861;&#36827;&#34892;&#28151;&#21512;&#31227;&#21160;&#24179;&#22343;&#22330;&#24341;&#23548;&#30340;&#26102;&#31354;&#25968;&#25454;&#24314;&#27169;&#65292;&#21487;&#20197;&#36827;&#34892;&#22240;&#26524;&#26410;&#26469;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#28151;&#21512;&#31227;&#21160;&#24179;&#22343;&#22330;&#30340;&#24433;&#21709;&#65292;&#26102;&#31354;&#25968;&#25454;&#30340;&#24314;&#27169;&#26159;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#25216;&#24039;&#12290;&#20294;&#26159;&#65292;&#23427;&#20204;&#30340;&#39044;&#27979;&#20998;&#24067;&#36890;&#24120;&#19981;&#21487;&#35775;&#38382;&#12290;&#22312;&#36825;&#20010;&#24314;&#27169;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35770;&#24341;&#23548;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#37319;&#29992;&#24191;&#20041;&#36125;&#21494;&#26031;&#31639;&#27861;&#36827;&#34892;&#39044;&#27979;&#12290;&#25105;&#20204;&#37319;&#29992;Lipschitz&#39044;&#27979;&#22120;&#65288;&#20363;&#22914;&#32447;&#24615;&#27169;&#22411;&#25110;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65289;&#65292;&#24182;&#36890;&#36807;&#26368;&#23567;&#21270;&#27839;&#31354;&#38388;&#21644;&#26102;&#38388;&#32500;&#24230;&#20018;&#34892;&#30456;&#20851;&#30340;&#25968;&#25454;&#30340;&#26032;&#22411;PAC&#36125;&#21494;&#26031;&#30028;&#38480;&#26469;&#30830;&#23450;&#19968;&#20010;&#38543;&#26426;&#20272;&#35745;&#20540;&#12290;&#36827;&#34892;&#22240;&#26524;&#26410;&#26469;&#39044;&#27979;&#26159;&#25105;&#20204;&#26041;&#27861;&#30340;&#19968;&#20010;&#20142;&#28857;&#65292;&#22240;&#20026;&#23427;&#36866;&#29992;&#20110;&#20855;&#26377;&#30701;&#26399;&#21644;&#38271;&#26399;&#30456;&#20851;&#24615;&#30340;&#25968;&#25454;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#32447;&#24615;&#39044;&#27979;&#22120;&#21644;&#27169;&#25311;STOU&#36807;&#31243;&#30340;&#26102;&#31354;&#25968;&#25454;&#30340;&#31034;&#20363;&#26469;&#23637;&#31034;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Influenced mixed moving average fields are a versatile modeling class for spatio-temporal data. However, their predictive distribution is not generally accessible. Under this modeling assumption, we define a novel theory-guided machine learning approach that employs a generalized Bayesian algorithm to make predictions. We employ a Lipschitz predictor, for example, a linear model or a feed-forward neural network, and determine a randomized estimator by minimizing a novel PAC Bayesian bound for data serially correlated along a spatial and temporal dimension. Performing causal future predictions is a highlight of our methodology as its potential application to data with short and long-range dependence. We conclude by showing the performance of the learning methodology in an example with linear predictors and simulated spatio-temporal data from an STOU process.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#38543;&#26426;Kaczmarz&#31639;&#27861;&#65292;&#20351;&#29992;&#23567;&#25209;&#37327;&#21644;&#37325;&#29699;&#21160;&#37327;&#36827;&#34892;&#21152;&#36895;&#65292;&#22312;&#20108;&#27425;&#20248;&#21270;&#38382;&#39064;&#20013;&#20445;&#25345;&#24555;&#36895;&#25910;&#25947;&#29575;&#12290;</title><link>http://arxiv.org/abs/2206.07553</link><description>&lt;p&gt;
&#35770;&#23567;&#25209;&#37327;&#37325;&#29699;&#21160;&#37327;&#27861;&#30340;&#24555;&#36895;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the fast convergence of minibatch heavy ball momentum. (arXiv:2206.07553v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.07553
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#38543;&#26426;Kaczmarz&#31639;&#27861;&#65292;&#20351;&#29992;&#23567;&#25209;&#37327;&#21644;&#37325;&#29699;&#21160;&#37327;&#36827;&#34892;&#21152;&#36895;&#65292;&#22312;&#20108;&#27425;&#20248;&#21270;&#38382;&#39064;&#20013;&#20445;&#25345;&#24555;&#36895;&#25910;&#25947;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31616;&#21333;&#30340;&#38543;&#26426;&#21160;&#37327;&#26041;&#27861;&#34987;&#24191;&#27867;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20248;&#21270;&#20013;&#65292;&#20294;&#30001;&#20110;&#36824;&#27809;&#26377;&#21152;&#36895;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#36825;&#19982;&#23427;&#20204;&#22312;&#23454;&#36341;&#20013;&#30340;&#33391;&#22909;&#24615;&#33021;&#24182;&#19981;&#30456;&#31526;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#23637;&#31034;&#65292;&#38543;&#26426;&#37325;&#29699;&#21160;&#37327;&#22312;&#20108;&#27425;&#26368;&#20248;&#21270;&#38382;&#39064;&#20013;&#20445;&#25345;&#65288;&#30830;&#23450;&#24615;&#65289;&#37325;&#29699;&#21160;&#37327;&#30340;&#24555;&#36895;&#32447;&#24615;&#29575;&#65292;&#33267;&#23569;&#22312;&#20351;&#29992;&#36275;&#22815;&#22823;&#30340;&#25209;&#37327;&#22823;&#23567;&#36827;&#34892;&#23567;&#25209;&#37327;&#22788;&#29702;&#26102;&#12290;&#25105;&#20204;&#25152;&#30740;&#31350;&#30340;&#31639;&#27861;&#21487;&#20197;&#34987;&#35299;&#37322;&#20026;&#24102;&#23567;&#25209;&#37327;&#22788;&#29702;&#21644;&#37325;&#29699;&#21160;&#37327;&#30340;&#21152;&#36895;&#38543;&#26426;Kaczmarz&#31639;&#27861;&#12290;&#35813;&#20998;&#26512;&#20381;&#36182;&#20110;&#20180;&#32454;&#20998;&#35299;&#21160;&#37327;&#36716;&#31227;&#30697;&#38453;&#65292;&#24182;&#20351;&#29992;&#26032;&#30340;&#29420;&#31435;&#38543;&#26426;&#30697;&#38453;&#20056;&#31215;&#30340;&#35889;&#33539;&#22260;&#38598;&#20013;&#30028;&#38480;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25968;&#20540;&#28436;&#31034;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#30028;&#38480;&#30456;&#24403;&#23574;&#38160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simple stochastic momentum methods are widely used in machine learning optimization, but their good practical performance is at odds with an absence of theoretical guarantees of acceleration in the literature. In this work, we aim to close the gap between theory and practice by showing that stochastic heavy ball momentum retains the fast linear rate of (deterministic) heavy ball momentum on quadratic optimization problems, at least when minibatching with a sufficiently large batch size. The algorithm we study can be interpreted as an accelerated randomized Kaczmarz algorithm with minibatching and heavy ball momentum. The analysis relies on carefully decomposing the momentum transition matrix, and using new spectral norm concentration bounds for products of independent random matrices. We provide numerical illustrations demonstrating that our bounds are reasonably sharp.
&lt;/p&gt;</description></item><item><title>&#20174;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#20013;&#25552;&#21462;&#21306;&#20998;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#23427;&#20204;&#21387;&#32553;&#34920;&#31034;&#31354;&#38388;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;Q-Score&#33258;&#30417;&#30563;&#34920;&#31034;&#36136;&#37327;&#20998;&#25968;&#65292;&#21487;&#20197;&#21487;&#38752;&#22320;&#39044;&#27979;&#32447;&#24615;&#35780;&#20272;&#26399;&#38388;&#30340;&#38169;&#35823;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2203.01881</link><description>&lt;p&gt;
&#20351;&#29992;&#21306;&#20998;&#29305;&#24449;&#24230;&#37327;&#19979;&#28216;&#20998;&#31867;&#30340;&#33258;&#30417;&#30563;&#34920;&#31034;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
Measuring Self-Supervised Representation Quality for Downstream Classification using Discriminative Features. (arXiv:2203.01881v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.01881
&lt;/p&gt;
&lt;p&gt;
&#20174;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#20013;&#25552;&#21462;&#21306;&#20998;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#23427;&#20204;&#21387;&#32553;&#34920;&#31034;&#31354;&#38388;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;Q-Score&#33258;&#30417;&#30563;&#34920;&#31034;&#36136;&#37327;&#20998;&#25968;&#65292;&#21487;&#20197;&#21487;&#38752;&#22320;&#39044;&#27979;&#32447;&#24615;&#35780;&#20272;&#26399;&#38388;&#30340;&#38169;&#35823;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#19979;&#28216;&#20998;&#31867;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#23427;&#20204;&#30340;&#22833;&#36133;&#27169;&#24335;&#21644;&#23398;&#20064;&#34920;&#31034;&#30340;&#35299;&#37322;&#65292;&#23384;&#22312;&#30528;&#26377;&#38480;&#30340;&#30740;&#31350;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102; SimCLR&#12289;SwaV&#12289;MoCo&#12289;BYOL&#12289;DINO&#12289;SimSiam&#12289;VICReg &#21644; Barlow Twins &#31561;&#26368;&#20808;&#36827;&#30340;&#33258;&#30417;&#30563;&#27169;&#22411;&#30340;&#34920;&#31034;&#31354;&#38388;&#12290;&#22312;&#19981;&#20351;&#29992;&#31867;&#26631;&#31614;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#23545;&#24212;&#20110;&#22270;&#20687;&#20013;&#29420;&#29305;&#29289;&#29702;&#23646;&#24615;&#30340;&#21306;&#20998;&#29305;&#24449;&#65292;&#36825;&#20123;&#21306;&#20998;&#29305;&#24449;&#20027;&#35201;&#23384;&#22312;&#20110;&#27491;&#30830;&#20998;&#31867;&#30340;&#34920;&#31034;&#20013;&#12290;&#20351;&#29992;&#36825;&#20123;&#29305;&#24449;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#34920;&#31034;&#31354;&#38388;&#21387;&#32553;&#22810;&#36798; 40%&#65292;&#32780;&#19981;&#20250;&#26174;&#33879;&#24433;&#21709;&#32447;&#24615;&#20998;&#31867;&#24615;&#33021;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#30417;&#30563;&#34920;&#31034;&#36136;&#37327;&#20998;&#25968;&#65288;&#25110; Q-Score&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#12289;&#26080;&#30417;&#30563;&#30340;&#20998;&#25968;&#65292;&#21487;&#20197;&#21487;&#38752;&#22320;&#39044;&#27979;&#19968;&#20010;&#32473;&#23450;&#26679;&#26412;&#22312;&#32447;&#24615;&#35780;&#20272;&#26399;&#38388;&#26159;&#21542;&#21487;&#33021;&#34987;&#38169;&#35823;&#20998;&#31867;&#65292;&#24182;&#22312; ImageNet-100 &#21644; ImageNet-1K &#19978;&#23454;&#29616;&#20102; AUPRC &#20998;&#21035;&#20026; 91.45 &#21644; 78.78&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning has shown impressive results in downstream classification tasks. However, there is limited work in understanding their failure modes and interpreting their learned representations. In this paper, we study the representation space of state-of-the-art self-supervised models including SimCLR, SwaV, MoCo, BYOL, DINO, SimSiam, VICReg and Barlow Twins. Without the use of class label information, we discover discriminative features that correspond to unique physical attributes in images, present mostly in correctly-classified representations. Using these features, we can compress the representation space by up to $40\%$ without significantly affecting linear classification performance. We then propose Self-Supervised Representation Quality Score (or Q-Score), a model-agnostic, unsupervised score that can reliably predict if a given sample is likely to be mis-classified during linear evaluation, achieving AUPRC of 91.45 on ImageNet-100 and 78.78 on ImageNet-1K. Q-Score
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;SIV-GAN&#65292;&#19968;&#31181;&#26080;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#21333;&#20010;&#22270;&#20687;&#25110;&#35270;&#39057;&#20013;&#29983;&#25104;&#26032;&#30340;&#22330;&#26223;&#32452;&#21512;&#12290;&#36890;&#36807;&#24341;&#20837;&#20869;&#23481;&#21644;&#24067;&#23616;&#20998;&#25903;&#30340;&#37492;&#21035;&#22120;&#26550;&#26500;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#21270;&#12289;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#65292;&#24182;&#22312;&#20445;&#30041;&#19978;&#19979;&#25991;&#30340;&#21516;&#26102;&#20445;&#25345;&#35270;&#35273;&#36924;&#30495;&#12290;</title><link>http://arxiv.org/abs/2103.13389</link><description>&lt;p&gt;
&#20174;&#21333;&#20010;&#22270;&#20687;&#21644;&#35270;&#39057;&#20013;&#29983;&#25104;&#26032;&#39062;&#30340;&#22330;&#26223;&#32452;&#21512;
&lt;/p&gt;
&lt;p&gt;
Generating Novel Scene Compositions from Single Images and Videos. (arXiv:2103.13389v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.13389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;SIV-GAN&#65292;&#19968;&#31181;&#26080;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#21333;&#20010;&#22270;&#20687;&#25110;&#35270;&#39057;&#20013;&#29983;&#25104;&#26032;&#30340;&#22330;&#26223;&#32452;&#21512;&#12290;&#36890;&#36807;&#24341;&#20837;&#20869;&#23481;&#21644;&#24067;&#23616;&#20998;&#25903;&#30340;&#37492;&#21035;&#22120;&#26550;&#26500;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#21270;&#12289;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#65292;&#24182;&#22312;&#20445;&#30041;&#19978;&#19979;&#25991;&#30340;&#21516;&#26102;&#20445;&#25345;&#35270;&#35273;&#36924;&#30495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#36739;&#22823;&#30340;&#24773;&#20917;&#19979;&#65292;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#21487;&#20197;&#22312;&#22270;&#20687;&#21512;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#26497;&#20302;&#25968;&#25454;&#29615;&#22659;&#20013;&#35757;&#32451;GAN&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#22240;&#20026;&#24448;&#24448;&#20250;&#21457;&#29983;&#36807;&#25311;&#21512;&#65292;&#23548;&#33268;&#35760;&#24518;&#25110;&#35757;&#32451;&#21457;&#25955;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SIV-GAN&#65292;&#36825;&#26159;&#19968;&#20010;&#26080;&#26465;&#20214;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#21333;&#20010;&#35757;&#32451;&#22270;&#20687;&#25110;&#21333;&#20010;&#35270;&#39057;&#21098;&#36753;&#20013;&#29983;&#25104;&#26032;&#30340;&#22330;&#26223;&#32452;&#21512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#20869;&#23481;&#21644;&#24067;&#23616;&#20998;&#25903;&#30340;&#20004;&#25903;&#37492;&#21035;&#22120;&#26550;&#26500;&#65292;&#20998;&#21035;&#35774;&#35745;&#29992;&#20110;&#21028;&#26029;&#20869;&#37096;&#20869;&#23481;&#21644;&#22330;&#26223;&#24067;&#23616;&#30340;&#30495;&#23454;&#24615;&#12290;&#36825;&#31181;&#37492;&#21035;&#22120;&#35774;&#35745;&#21487;&#20197;&#21512;&#25104;&#35270;&#35273;&#19978;&#36924;&#30495;&#12289;&#26032;&#39062;&#30340;&#22330;&#26223;&#32452;&#21512;&#65292;&#20855;&#26377;&#19981;&#21516;&#30340;&#20869;&#23481;&#21644;&#24067;&#23616;&#65292;&#21516;&#26102;&#20445;&#30041;&#21407;&#22987;&#26679;&#26412;&#30340;&#19978;&#19979;&#25991;&#12290;&#19982;&#20197;&#21069;&#30340;&#21333;&#22270;&#20687;GAN&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#29983;&#25104;&#20102;&#26356;&#22810;&#26679;&#21270;&#12289;&#36136;&#37327;&#26356;&#39640;&#30340;&#22270;&#20687;&#65292;&#21516;&#26102;&#19981;&#38480;&#20110;&#21333;&#20010;&#22270;&#20687;&#35774;&#32622;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
Given a large dataset for training, generative adversarial networks (GANs) can achieve remarkable performance for the image synthesis task. However, training GANs in extremely low data regimes remains a challenge, as overfitting often occurs, leading to memorization or training divergence. In this work, we introduce SIV-GAN, an unconditional generative model that can generate new scene compositions from a single training image or a single video clip. We propose a two-branch discriminator architecture, with content and layout branches designed to judge internal content and scene layout realism separately from each other. This discriminator design enables synthesis of visually plausible, novel compositions of a scene, with varying content and layout, while preserving the context of the original sample. Compared to previous single image GANs, our model generates more diverse, higher quality images, while not being restricted to a single image setting. We further introduce a new challengin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#24310;&#36831;&#29615;&#22659;&#20013;&#65292;&#23398;&#20064;&#21644;&#35268;&#21010;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(MDP)&#26694;&#26550;&#65292;&#35777;&#26126;&#20102;&#22312;&#24310;&#36831;&#25191;&#34892;&#30340;&#24773;&#20917;&#19979;&#65292;&#21407;&#22987;&#29366;&#24577;&#31354;&#38388;&#20013;&#30340;&#38750;&#22266;&#23450;&#39532;&#23572;&#21487;&#22827;&#31574;&#30053;&#21487;&#20197;&#23454;&#29616;&#26368;&#22823;&#22870;&#21169;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#24310;&#36831;&#25191;&#34892;&#20219;&#21153;&#30340;&#38750;&#22266;&#23450;Q-learning&#39118;&#26684;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2101.11992</link><description>&lt;p&gt;
&#22312;&#24310;&#36831;&#29615;&#22659;&#20013;&#20197;&#38750;&#22266;&#23450;&#39532;&#23572;&#21487;&#22827;&#31574;&#30053;&#34892;&#21160;
&lt;/p&gt;
&lt;p&gt;
Acting in Delayed Environments with Non-Stationary Markov Policies. (arXiv:2101.11992v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2101.11992
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#24310;&#36831;&#29615;&#22659;&#20013;&#65292;&#23398;&#20064;&#21644;&#35268;&#21010;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(MDP)&#26694;&#26550;&#65292;&#35777;&#26126;&#20102;&#22312;&#24310;&#36831;&#25191;&#34892;&#30340;&#24773;&#20917;&#19979;&#65292;&#21407;&#22987;&#29366;&#24577;&#31354;&#38388;&#20013;&#30340;&#38750;&#22266;&#23450;&#39532;&#23572;&#21487;&#22827;&#31574;&#30053;&#21487;&#20197;&#23454;&#29616;&#26368;&#22823;&#22870;&#21169;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#24310;&#36831;&#25191;&#34892;&#20219;&#21153;&#30340;&#38750;&#22266;&#23450;Q-learning&#39118;&#26684;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(MDP)&#20551;&#35774;&#22312;&#36873;&#25321;&#21160;&#20316;&#21518;&#31435;&#21363;&#25191;&#34892;&#65292;&#20294;&#36825;&#31181;&#20551;&#35774;&#24120;&#24120;&#19981;&#20999;&#23454;&#38469;&#65292;&#20250;&#22312;&#26426;&#22120;&#20154;&#25805;&#32437;&#12289;&#20113;&#35745;&#31639;&#21644;&#37329;&#34701;&#31561;&#24212;&#29992;&#20013;&#23548;&#33268;&#28798;&#38590;&#24615;&#25925;&#38556;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#23398;&#20064;&#21644;&#35745;&#21010;&#30340;MDP&#26694;&#26550;&#65292;&#20854;&#20013;&#20915;&#31574;&#32773;&#36873;&#25321;&#30340;&#21160;&#20316;&#38656;&#35201;&#24310;&#36831;$m$&#27493;&#25165;&#33021;&#25191;&#34892;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#24310;&#36831;&#25191;&#34892;&#30340;&#24773;&#20917;&#19979;&#65292;&#21407;&#22987;&#29366;&#24577;&#31354;&#38388;&#20013;&#30340;&#30830;&#23450;&#24615;&#39532;&#23572;&#21487;&#22827;&#31574;&#30053;&#36275;&#20197;&#23454;&#29616;&#26368;&#22823;&#22870;&#21169;&#65292;&#20294;&#38656;&#35201;&#26159;&#38750;&#22266;&#23450;&#30340;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#22266;&#23450;&#30340;&#39532;&#23572;&#21487;&#22827;&#31574;&#30053;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#26159;&#27425;&#20248;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#38750;&#22266;&#23450;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;Q-learning&#39118;&#26684;&#31639;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#24310;&#36831;&#25191;&#34892;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
The standard Markov Decision Process (MDP) formulation hinges on the assumption that an action is executed immediately after it was chosen. However, assuming it is often unrealistic and can lead to catastrophic failures in applications such as robotic manipulation, cloud computing, and finance. We introduce a framework for learning and planning in MDPs where the decision-maker commits actions that are executed with a delay of $m$ steps. The brute-force state augmentation baseline where the state is concatenated to the last $m$ committed actions suffers from an exponential complexity in $m$, as we show for policy iteration. We then prove that with execution delay, deterministic Markov policies in the original state-space are sufficient for attaining maximal reward, but need to be non-stationary. As for stationary Markov policies, we show they are sub-optimal in general. Consequently, we devise a non-stationary Q-learning style model-based algorithm that solves delayed execution tasks wi
&lt;/p&gt;</description></item></channel></rss>