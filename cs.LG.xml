<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#21033;&#29992;&#25193;&#23637;&#30340;&#20445;&#24207;&#26426;&#21046;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#25351;&#25968;&#26063;&#20998;&#24067;&#20197;&#25552;&#39640;&#21516;&#34892;&#35780;&#23457;&#30340;&#36136;&#37327;&#65292;&#24182;&#21457;&#29616;&#20316;&#32773;&#30340;&#21516;&#34892;&#35780;&#20998;&#21487;&#20197;&#36739;&#20934;&#30830;&#22320;&#22312;&#19981;&#38656;&#35201;&#30693;&#36947;&#20855;&#20307;&#20998;&#24067;&#24773;&#20917;&#19979;&#36827;&#34892;&#35843;&#25972;&#12290;</title><link>http://arxiv.org/abs/2304.11160</link><description>&lt;p&gt;
&#21033;&#29992;&#20445;&#24207;&#26426;&#21046;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#20250;&#35758;&#30340;&#21516;&#34892;&#35780;&#23457;
&lt;/p&gt;
&lt;p&gt;
The Isotonic Mechanism for Exponential Family Estimation. (arXiv:2304.11160v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#25193;&#23637;&#30340;&#20445;&#24207;&#26426;&#21046;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#25351;&#25968;&#26063;&#20998;&#24067;&#20197;&#25552;&#39640;&#21516;&#34892;&#35780;&#23457;&#30340;&#36136;&#37327;&#65292;&#24182;&#21457;&#29616;&#20316;&#32773;&#30340;&#21516;&#34892;&#35780;&#20998;&#21487;&#20197;&#36739;&#20934;&#30830;&#22320;&#22312;&#19981;&#38656;&#35201;&#30693;&#36947;&#20855;&#20307;&#20998;&#24067;&#24773;&#20917;&#19979;&#36827;&#34892;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#33268;&#21147;&#20110;&#25193;&#23637;&#20445;&#24207;&#26426;&#21046;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#25351;&#25968;&#26063;&#20998;&#24067;&#20197;&#25552;&#39640;&#21516;&#34892;&#35780;&#23457;&#30340;&#36136;&#37327;&#12290;&#35813;&#26426;&#21046;&#21487;&#29983;&#25104;&#19982;&#21407;&#22987;&#35780;&#20998;&#25509;&#36817;&#30340;&#35843;&#25972;&#20998;&#25968;&#65292;&#24182;&#31526;&#21512;&#20316;&#32773;&#25351;&#23450;&#30340;&#25490;&#21517;&#35201;&#27714;&#65292;&#24471;&#21040;&#24191;&#27867;&#30340;&#25351;&#25968;&#26063;&#20998;&#24067;&#24212;&#29992;&#65292;&#32780;&#19988;&#19981;&#38656;&#35201;&#30693;&#36947;&#20855;&#20307;&#30340;&#20998;&#24067;&#24418;&#24335;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#19968;&#23450;&#30340;&#25351;&#25968;&#26063;&#20998;&#24067;&#19979;&#65292;&#22914;&#26524;&#20316;&#32773;&#30340;&#25928;&#29992;&#20989;&#25968;&#37319;&#29992;&#31616;&#21333;&#30340;&#20984;&#21487;&#21152;&#20989;&#25968;&#65292;&#21017;&#28608;&#21169;&#20316;&#32773;&#25552;&#20379;&#20934;&#30830;&#30340;&#25490;&#21517;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
In 2023, the International Conference on Machine Learning (ICML) required authors with multiple submissions to rank their submissions based on perceived quality. In this paper, we aim to employ these author-specified rankings to enhance peer review in machine learning and artificial intelligence conferences by extending the Isotonic Mechanism (Su, 2021, 2022) to exponential family distributions. This mechanism generates adjusted scores closely align with the original scores while adhering to author-specified rankings. Despite its applicability to a broad spectrum of exponential family distributions, this mechanism's implementation does not necessitate knowledge of the specific distribution form. We demonstrate that an author is incentivized to provide accurate rankings when her utility takes the form of a convex additive function of the adjusted review scores. For a certain subclass of exponential family distributions, we prove that the author reports truthfully only if the question in
&lt;/p&gt;</description></item><item><title>ES-Single&#26159;&#19968;&#31181;&#29992;&#20110;&#20272;&#35745;&#23637;&#24320;&#30340;&#35745;&#31639;&#22270;&#20013;&#26799;&#24230;&#30340;&#36827;&#21270;&#31574;&#30053;&#31639;&#27861;&#65292;&#20854;&#31616;&#21333;&#23454;&#29616;&#12289;&#26041;&#24046;&#36739;&#20302;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2304.11153</link><description>&lt;p&gt;
ES-Single&#65306;&#22312;&#23637;&#24320;&#30340;&#35745;&#31639;&#22270;&#20013;&#23454;&#29616;&#20302;&#26041;&#24046;&#26799;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Low-Variance Gradient Estimation in Unrolled Computation Graphs with ES-Single. (arXiv:2304.11153v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11153
&lt;/p&gt;
&lt;p&gt;
ES-Single&#26159;&#19968;&#31181;&#29992;&#20110;&#20272;&#35745;&#23637;&#24320;&#30340;&#35745;&#31639;&#22270;&#20013;&#26799;&#24230;&#30340;&#36827;&#21270;&#31574;&#30053;&#31639;&#27861;&#65292;&#20854;&#31616;&#21333;&#23454;&#29616;&#12289;&#26041;&#24046;&#36739;&#20302;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36827;&#21270;&#31574;&#30053;&#30340;&#31639;&#27861;ES-Single&#65292;&#29992;&#20110;&#20272;&#35745;&#23637;&#24320;&#30340;&#35745;&#31639;&#22270;&#20013;&#30340;&#26799;&#24230;&#12290;&#19982;&#26368;&#36817;&#25552;&#20986;&#30340;&#25345;&#20037;&#36827;&#21270;&#31574;&#30053;&#65288;PES&#65289;&#31867;&#20284;&#65292;ES-Single&#26159;&#26080;&#20559;&#30340;&#65292;&#24182;&#36890;&#36807;&#24179;&#28369;&#20803;&#25439;&#22833;&#20989;&#25968;&#26469;&#20811;&#26381;&#30001;&#20110;&#36882;&#24402;&#20989;&#25968;&#24212;&#29992;&#32780;&#20135;&#29983;&#30340;&#28151;&#27788;&#12290;ES-Single&#23545;&#20110;&#27599;&#20010;&#31890;&#23376;&#37319;&#26679;&#19968;&#20010;&#21333;&#19968;&#25200;&#21160;&#65292;&#24182;&#22312;&#20869;&#37096;&#38382;&#39064;&#30340;&#36807;&#31243;&#20013;&#20445;&#25345;&#19981;&#21464;&#65288;&#20363;&#22914;&#65292;&#23545;&#20110;&#27599;&#20010;&#37096;&#20998;&#26410;&#23637;&#24320;&#65292;&#19981;&#20250;&#37325;&#26032;&#37319;&#26679;&#25200;&#21160;&#65289;&#12290;&#19982;PES&#30456;&#27604;&#65292;ES-Single&#23454;&#29616;&#26356;&#31616;&#21333;&#65292;&#26041;&#24046;&#26356;&#20302;&#65306;ES-Single&#30340;&#26041;&#24046;&#19982;&#25130;&#26029;&#23637;&#24320;&#27425;&#25968;&#30340;&#25968;&#37327;&#26080;&#20851;&#65292;&#28040;&#38500;&#20102;&#20351;&#29992;&#30701;&#25130;&#26029;&#26469;&#35299;&#20915;&#38271;&#20869;&#37096;&#38382;&#39064;&#25152;&#24102;&#26469;&#30340;&#20851;&#38190;&#38556;&#30861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;ES-Single&#23545;&#20110;&#20108;&#27425;&#20869;&#37096;&#38382;&#39064;&#26159;&#26080;&#20559;&#30340;&#65292;&#24182;&#19988;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#23427;&#30340;&#26041;&#24046;&#21487;&#20197;&#26174;&#33879;&#22320;&#20302;&#20110;PES&#12290;ES-Single&#22312;&#22810;&#39033;&#20219;&#21153;&#20013;&#25345;&#32493;&#20248;&#20110;PES&#65292;&#21253;&#25324;&#21512;&#25104;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an evolution strategies-based algorithm for estimating gradients in unrolled computation graphs, called ES-Single. Similarly to the recently-proposed Persistent Evolution Strategies (PES), ES-Single is unbiased, and overcomes chaos arising from recursive function applications by smoothing the meta-loss landscape. ES-Single samples a single perturbation per particle, that is kept fixed over the course of an inner problem (e.g., perturbations are not re-sampled for each partial unroll). Compared to PES, ES-Single is simpler to implement and has lower variance: the variance of ES-Single is constant with respect to the number of truncated unrolls, removing a key barrier in applying ES to long inner problems using short truncations. We show that ES-Single is unbiased for quadratic inner problems, and demonstrate empirically that its variance can be substantially lower than that of PES. ES-Single consistently outperforms PES on a variety of tasks, including a synthetic benchmark t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#38543;&#26426;&#22270;&#27169;&#22411;&#19978;&#30340;&#25910;&#25947;&#24615;&#65292;&#23558;&#25910;&#25947;&#32467;&#35770;&#20174;&#21482;&#36866;&#29992;&#20110;&#24230;&#35268;&#33539;&#21270;&#24179;&#22343;&#32858;&#21512;&#20989;&#25968;&#25193;&#23637;&#21040;&#25152;&#26377;&#20256;&#32479;&#32858;&#21512;&#20989;&#25968;&#65292;&#24182;&#32771;&#34385;&#20102;&#32858;&#21512;&#20989;&#25968;&#37319;&#29992;&#36880;&#20010;&#22352;&#26631;&#26368;&#22823;&#20540;&#26102;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2304.11140</link><description>&lt;p&gt;
&#22522;&#20110;&#28040;&#24687;&#20256;&#36882;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#22823;&#35268;&#27169;&#38543;&#26426;&#22270;&#19978;&#30340;&#36890;&#29992;&#32858;&#21512;&#25910;&#25947;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Convergence of Message Passing Graph Neural Networks with Generic Aggregation On Large Random Graphs. (arXiv:2304.11140v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#38543;&#26426;&#22270;&#27169;&#22411;&#19978;&#30340;&#25910;&#25947;&#24615;&#65292;&#23558;&#25910;&#25947;&#32467;&#35770;&#20174;&#21482;&#36866;&#29992;&#20110;&#24230;&#35268;&#33539;&#21270;&#24179;&#22343;&#32858;&#21512;&#20989;&#25968;&#25193;&#23637;&#21040;&#25152;&#26377;&#20256;&#32479;&#32858;&#21512;&#20989;&#25968;&#65292;&#24182;&#32771;&#34385;&#20102;&#32858;&#21512;&#20989;&#25968;&#37319;&#29992;&#36880;&#20010;&#22352;&#26631;&#26368;&#22823;&#20540;&#26102;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#38543;&#26426;&#22270;&#27169;&#22411;&#19978;&#30340;&#25910;&#25947;&#24615;&#65292;&#24403;&#33410;&#28857;&#25968;&#37327;&#36235;&#36817;&#20110;&#26080;&#38480;&#26102;&#65292;&#35813;&#32593;&#32476;&#27169;&#22411;&#33021;&#25910;&#25947;&#20110;&#20854;&#36830;&#32493;&#27169;&#22411;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#35813;&#25910;&#25947;&#24615;&#32467;&#26524;&#21482;&#36866;&#29992;&#20110;&#32858;&#21512;&#20989;&#25968;&#37319;&#29992;&#24230;&#35268;&#33539;&#21270;&#24179;&#22343;&#20540;&#24418;&#24335;&#30340;&#32593;&#32476;&#32467;&#26500;&#12290;&#25105;&#20204;&#23558;&#27492;&#32467;&#26524;&#25193;&#23637;&#21040;&#21253;&#21547;&#25152;&#26377;&#20256;&#32479;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22823;&#31867;&#32858;&#21512;&#20989;&#25968;&#19978;&#65292;&#20363;&#22914;&#22522;&#20110;&#27880;&#24847;&#21147;&#21644;&#26368;&#22823;&#21367;&#31215;&#30340;&#32593;&#32476;&#12290;&#22312;&#19968;&#23450;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#39640;&#27010;&#29575;&#30340;&#38750;&#28176;&#36827;&#19978;&#38480;&#26469;&#37327;&#21270;&#36825;&#31181;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#22522;&#20110;McDiarmid&#19981;&#31561;&#24335;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#29305;&#21035;&#22788;&#29702;&#20102;&#32858;&#21512;&#20989;&#25968;&#37319;&#29992;&#36880;&#20010;&#22352;&#26631;&#26368;&#22823;&#20540;&#30340;&#24773;&#20917;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#38750;&#24120;&#19981;&#21516;&#30340;&#35777;&#26126;&#25216;&#24039;&#65292;&#24182;&#20135;&#29983;&#20102;&#23450;&#24615;&#19981;&#21516;&#30340;&#25910;&#25947;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the convergence of message passing graph neural networks on random graph models to their continuous counterpart as the number of nodes tends to infinity. Until now, this convergence was only known for architectures with aggregation functions in the form of degree-normalized means. We extend such results to a very large class of aggregation functions, that encompasses all classically used message passing graph neural networks, such as attention-based mesage passing or max convolutional message passing on top of (degree-normalized) convolutional message passing. Under mild assumptions, we give non asymptotic bounds with high probability to quantify this convergence. Our main result is based on the McDiarmid inequality. Interestingly, we treat the case where the aggregation is a coordinate-wise maximum separately, at it necessitates a very different proof technique and yields a qualitatively different convergence rate.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#19968;&#31181;&#25554;&#25300;&#24335;&#20998;&#21106; Gibbs &#37319;&#26679;&#31639;&#27861;&#65292;&#23558;&#21518;&#39564;&#37319;&#26679;&#20219;&#21153;&#20998;&#20026;&#20004;&#20010;&#36739;&#31616;&#21333;&#30340;&#23376;&#38382;&#39064;&#65292;&#20854;&#20013;&#31532;&#20108;&#20010;&#23376;&#38382;&#39064;&#21487;&#20197;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#36731;&#26494;&#22320;&#35299;&#20915;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#36125;&#21494;&#26031;&#25512;&#26029;&#20013;&#23884;&#20837;&#28145;&#24230;&#29983;&#25104;&#20808;&#39564;&#20197;&#21450;&#33258;&#21160;&#36866;&#24212;&#21518;&#39564;&#20998;&#24067;&#30340;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.11134</link><description>&lt;p&gt;
&#25554;&#25300;&#24335;&#20998;&#21106; Gibbs &#37319;&#26679;: &#22312;&#36125;&#21494;&#26031;&#25512;&#26029;&#20013;&#23884;&#20837;&#28145;&#24230;&#29983;&#25104;&#20808;&#39564;
&lt;/p&gt;
&lt;p&gt;
Plug-and-Play split Gibbs sampler: embedding deep generative priors in Bayesian inference. (arXiv:2304.11134v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#19968;&#31181;&#25554;&#25300;&#24335;&#20998;&#21106; Gibbs &#37319;&#26679;&#31639;&#27861;&#65292;&#23558;&#21518;&#39564;&#37319;&#26679;&#20219;&#21153;&#20998;&#20026;&#20004;&#20010;&#36739;&#31616;&#21333;&#30340;&#23376;&#38382;&#39064;&#65292;&#20854;&#20013;&#31532;&#20108;&#20010;&#23376;&#38382;&#39064;&#21487;&#20197;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#36731;&#26494;&#22320;&#35299;&#20915;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#36125;&#21494;&#26031;&#25512;&#26029;&#20013;&#23884;&#20837;&#28145;&#24230;&#29983;&#25104;&#20808;&#39564;&#20197;&#21450;&#33258;&#21160;&#36866;&#24212;&#21518;&#39564;&#20998;&#24067;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#37327;&#20998;&#31163;&#30340;&#38543;&#26426;&#25554;&#25300;&#24335;(Plug-and-Play)&#37319;&#26679;&#31639;&#27861;&#65292;&#20197;&#26377;&#25928;&#22320;&#20174;&#21518;&#39564;&#20998;&#24067;&#20013;&#37319;&#26679;&#12290;&#35813;&#31639;&#27861;&#22522;&#20110;&#20998;&#21106;Gibbs&#37319;&#26679;(split Gibbs sampling, SGS)&#65292;&#28789;&#24863;&#26469;&#33258;&#20110;&#20132;&#26367;&#26041;&#21521;&#20056;&#23376;&#27861;(alternating direction method of multipliers, ADMM)&#12290;&#23427;&#23558;&#21518;&#39564;&#37319;&#26679;&#30340;&#25361;&#25112;&#20219;&#21153;&#20998;&#20026;&#20004;&#20010;&#36739;&#31616;&#21333;&#30340;&#37319;&#26679;&#38382;&#39064;&#12290;&#31532;&#19968;&#20010;&#38382;&#39064;&#20381;&#36182;&#20110;&#20284;&#28982;&#20989;&#25968;&#65292;&#32780;&#31532;&#20108;&#20010;&#38382;&#39064;&#34987;&#35299;&#37322;&#20026;&#19968;&#20010;&#36125;&#21494;&#26031;&#38477;&#22122;&#38382;&#39064;&#65292;&#21487;&#20197;&#36890;&#36807;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#36731;&#26494;&#22320;&#23436;&#25104;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20026;&#20102;&#35828;&#26126;&#30446;&#30340;&#65292;&#26412;&#25991;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#20102;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#20102;&#23454;&#29616;&#12290;&#19982;&#20854;&#30830;&#23450;&#24615;&#30340;&#25554;&#25300;&#24335;(Plug-and-Play)&#31867;&#20284;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20855;&#26377;&#19981;&#38656;&#35201;&#26174;&#24335;&#36873;&#25321;&#20808;&#39564;&#20998;&#24067;&#30340;&#24040;&#22823;&#20248;&#21183;&#65292;&#32780;&#26159;&#23558;&#20854;&#32534;&#30721;&#21040;&#39044;&#35757;&#32451;&#30340;&#29983;&#25104;&#27169;&#22411;&#20013;&#12290;&#28982;&#32780;&#65292;&#19982;&#38656;&#35201;&#35880;&#24910;&#35843;&#25972;&#35843;&#25972;&#21442;&#25968;&#30340;&#20248;&#21270;&#26041;&#27861;(PnP-ADMM)&#19981;&#21516;&#65292;&#25152;&#25552;&#20986;&#30340;&#25554;&#25300;&#24335;&#20998;&#21106; Gibbs &#37319;&#26679;&#31639;&#27861;&#21487;&#20197;&#22312;&#37319;&#26679;&#36807;&#31243;&#20013;&#33258;&#21160;&#36866;&#24212;&#21518;&#39564;&#20998;&#24067;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a stochastic plug-and-play (PnP) sampling algorithm that leverages variable splitting to efficiently sample from a posterior distribution. The algorithm based on split Gibbs sampling (SGS) draws inspiration from the alternating direction method of multipliers (ADMM). It divides the challenging task of posterior sampling into two simpler sampling problems. The first problem depends on the likelihood function, while the second is interpreted as a Bayesian denoising problem that can be readily carried out by a deep generative model. Specifically, for an illustrative purpose, the proposed method is implemented in this paper using state-of-the-art diffusion-based generative models. Akin to its deterministic PnP-based counterparts, the proposed method exhibits the great advantage of not requiring an explicit choice of the prior distribution, which is rather encoded into a pre-trained generative model. However, unlike optimization methods (e.g., PnP-ADMM) which generally
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#21160;&#23558;CVE&#28431;&#27934;&#35760;&#24405;&#26144;&#23556;&#21040;MITRE CWE&#24369;&#28857;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#25163;&#21160;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#35299;&#20915;&#27492;&#38382;&#39064;&#30340;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2304.11130</link><description>&lt;p&gt;
&#33258;&#21160;&#23558;CVE&#28431;&#27934;&#35760;&#24405;&#26144;&#23556;&#21040;MITRE CWE&#24369;&#28857;
&lt;/p&gt;
&lt;p&gt;
Automated Mapping of CVE Vulnerability Records to MITRE CWE Weaknesses. (arXiv:2304.11130v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11130
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#21160;&#23558;CVE&#28431;&#27934;&#35760;&#24405;&#26144;&#23556;&#21040;MITRE CWE&#24369;&#28857;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#25163;&#21160;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#35299;&#20915;&#27492;&#38382;&#39064;&#30340;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#32593;&#32476;&#23433;&#20840;&#23041;&#32961;&#21644;&#22810;&#26679;&#24615;&#30340;&#22686;&#21152;&#23548;&#33268;&#20102;&#28431;&#27934;&#25253;&#21578;&#21644;&#20998;&#26512;&#30340;&#22686;&#21152;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#35768;&#22810;&#38750;&#33829;&#21033;&#32452;&#32455;&#22312;&#36825;&#19968;&#39046;&#22495;&#23835;&#36215;&#65292;&#22914;MITRE&#21644;OSWAP&#65292;&#20182;&#20204;&#19968;&#30452;&#22312;&#31215;&#26497;&#36861;&#36394;&#28431;&#27934;&#65292;&#24182;&#20197;&#26631;&#20934;&#21270;&#26684;&#24335;&#21457;&#24067;&#38450;&#24481;&#24314;&#35758;&#12290;&#30001;&#20110;&#25163;&#21160;&#29983;&#20135;&#36825;&#31181;&#26684;&#24335;&#30340;&#25968;&#25454;&#38750;&#24120;&#32791;&#26102;&#65292;&#22240;&#27492;&#19968;&#20123;&#25552;&#35758;&#35797;&#22270;&#33258;&#21160;&#21270;&#35813;&#36807;&#31243;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#37319;&#29992;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#27492;&#38382;&#39064;&#30340;&#19968;&#20010;&#37325;&#35201;&#38556;&#30861;&#26159;&#32570;&#20047;&#20844;&#24320;&#30340;&#19987;&#19994;&#25968;&#25454;&#38598;&#12290;&#22312;&#27492;&#65292;&#25105;&#20204;&#26088;&#22312;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#23558;CVE&#35760;&#24405;&#26144;&#23556;&#21040;MITRE CWE&#24369;&#28857;&#65292;&#24182;&#21521;&#30740;&#31350;&#31038;&#21306;&#21457;&#24067;&#20102;&#19968;&#20010;&#25163;&#21160;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;4,012&#26465;&#35760;&#24405;&#12290;&#22312;&#32771;&#34385;&#21040;&#20154;&#22312;&#24490;&#29615;&#26694;&#26550;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23558;&#38382;&#39064;&#35270;&#20026;&#25490;&#21517;&#20219;&#21153;&#65292;&#24182;&#26088;&#22312;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#21033;&#29992;&#20854;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, a proliferation of cyber-security threats and diversity has been on the rise culminating in an increase in their reporting and analysis. To counter that, many non-profit organizations have emerged in this domain, such as MITRE and OSWAP, which have been actively tracking vulnerabilities, and publishing defense recommendations in standardized formats. As producing data in such formats manually is very time-consuming, there have been some proposals to automate the process. Unfortunately, a major obstacle to adopting supervised machine learning for this problem has been the lack of publicly available specialized datasets. Here, we aim to bridge this gap. In particular, we focus on mapping CVE records into MITRE CWE Weaknesses, and we release to the research community a manually annotated dataset of 4,012 records for this task. With a human-in-the-loop framework in mind, we approach the problem as a ranking task and aim to incorporate reinforced learning to make use of the
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861; Tree-structured Parzen estimator (TPE)&#65292;&#24182;&#23545;&#20854;&#25511;&#21046;&#21442;&#25968;&#30340;&#20316;&#29992;&#21644;&#31639;&#27861;&#30452;&#35273;&#36827;&#34892;&#20102;&#35752;&#35770;&#21644;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;&#19968;&#32452;&#25512;&#33616;&#35774;&#32622;&#24182;&#35777;&#26126;&#20854;&#33021;&#22815;&#25552;&#39640;TPE&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.11127</link><description>&lt;p&gt;
&#26641;&#29366;Parzen&#20272;&#35745;&#22120;&#65306;&#29702;&#35299;&#20854;&#31639;&#27861;&#32452;&#25104;&#37096;&#20998;&#21450;&#20854;&#22312;&#25552;&#39640;&#23454;&#35777;&#34920;&#29616;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Tree-structured Parzen estimator: Understanding its algorithm components and their roles for better empirical performance. (arXiv:2304.11127v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11127
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861; Tree-structured Parzen estimator (TPE)&#65292;&#24182;&#23545;&#20854;&#25511;&#21046;&#21442;&#25968;&#30340;&#20316;&#29992;&#21644;&#31639;&#27861;&#30452;&#35273;&#36827;&#34892;&#20102;&#35752;&#35770;&#21644;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;&#19968;&#32452;&#25512;&#33616;&#35774;&#32622;&#24182;&#35777;&#26126;&#20854;&#33021;&#22815;&#25552;&#39640;TPE&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#39046;&#22495;&#20013;&#26368;&#36817;&#30340;&#36827;&#23637;&#35201;&#27714;&#26356;&#21152;&#22797;&#26434;&#30340;&#23454;&#39564;&#35774;&#35745;&#12290;&#36825;&#31181;&#22797;&#26434;&#30340;&#23454;&#39564;&#36890;&#24120;&#26377;&#35768;&#22810;&#21442;&#25968;&#65292;&#38656;&#35201;&#21442;&#25968;&#35843;&#25972;&#12290;Tree-structured Parzen estimator (TPE) &#26159;&#19968;&#31181;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#22312;&#26368;&#36817;&#30340;&#21442;&#25968;&#35843;&#25972;&#26694;&#26550;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#23613;&#31649;&#23427;&#24456;&#21463;&#27426;&#36814;&#65292;&#20294;&#25511;&#21046;&#21442;&#25968;&#30340;&#35282;&#33394;&#21644;&#31639;&#27861;&#30452;&#35273;&#23578;&#26410;&#24471;&#21040;&#35752;&#35770;&#12290;&#22312;&#26412;&#25945;&#31243;&#20013;&#65292;&#25105;&#20204;&#23558;&#30830;&#23450;&#27599;&#20010;&#25511;&#21046;&#21442;&#25968;&#30340;&#20316;&#29992;&#20197;&#21450;&#23427;&#20204;&#23545;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#24433;&#21709;&#65292;&#20351;&#29992;&#22810;&#31181;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#23558;&#20174;&#21078;&#26512;&#30740;&#31350;&#20013;&#24471;&#20986;&#30340;&#25512;&#33616;&#35774;&#32622;&#19982;&#22522;&#20934;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#35777;&#26126;&#25105;&#20204;&#30340;&#25512;&#33616;&#35774;&#32622;&#25552;&#39640;&#20102;TPE&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;TPE&#23454;&#29616;&#21487;&#22312;https://github.com/nabenabe0928/tpe/tree/single-opt&#20013;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in many domains require more and more complicated experiment design. Such complicated experiments often have many parameters, which necessitate parameter tuning. Tree-structured Parzen estimator (TPE), a Bayesian optimization method, is widely used in recent parameter tuning frameworks. Despite its popularity, the roles of each control parameter and the algorithm intuition have not been discussed so far. In this tutorial, we will identify the roles of each control parameter and their impacts on hyperparameter optimization using a diverse set of benchmarks. We compare our recommended setting drawn from the ablation study with baseline methods and demonstrate that our recommended setting improves the performance of TPE. Our TPE implementation is available at https://github.com/nabenabe0928/tpe/tree/single-opt.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#27169;&#22411; VQ-MAE-S &#20197;&#35782;&#21035;&#24773;&#24863;&#65292;&#39044;&#35757;&#32451;&#22312; VoxCeleb2 &#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#65292;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.11117</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#30340;&#21521;&#37327;&#37327;&#21270;&#25513;&#34109;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
A vector quantized masked autoencoder for speech emotion recognition. (arXiv:2304.11117v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11117
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#27169;&#22411; VQ-MAE-S &#20197;&#35782;&#21035;&#24773;&#24863;&#65292;&#39044;&#35757;&#32451;&#22312; VoxCeleb2 &#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#65292;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#36827;&#27493;&#20351;&#24471;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035; (SER) &#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#20294;&#26631;&#27880;&#25968;&#25454;&#30340;&#26377;&#38480;&#21487;&#29992;&#24615;&#20173;&#28982;&#26159;&#35813;&#39046;&#22495;&#38754;&#20020;&#30340;&#37325;&#35201;&#25361;&#25112;&#12290;&#33258;&#30417;&#30563;&#23398;&#20064;&#26368;&#36817;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21521;&#37327;&#37327;&#21270;&#25513;&#34109;&#33258;&#32534;&#30721;&#22120; (VQ-MAE-S)&#65292;&#36825;&#26159;&#19968;&#31181;&#33258;&#30417;&#30563;&#27169;&#22411;&#65292;&#23427;&#34987;&#24494;&#35843;&#20197;&#20174;&#35821;&#38899;&#20449;&#21495;&#20013;&#35782;&#21035;&#24773;&#24863;&#12290;VQ-MAE-S &#27169;&#22411;&#22522;&#20110;&#36816;&#34892;&#22312;&#21521;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#31163;&#25955;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#25513;&#34109;&#33258;&#32534;&#30721;&#22120;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#39044;&#20808;&#22312;VoxCeleb2&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#65292;&#24182;&#22312;&#24773;&#24863;&#35821;&#38899;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#24494;&#35843;&#30340;VQ-MAE-S&#27169;&#22411;&#65292;&#22312;SER&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#22522;&#20110;&#20809;&#35889;&#22270;&#34920;&#31034;&#30340;MAE&#21644;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have seen remarkable progress in speech emotion recognition (SER), thanks to advances in deep learning techniques. However, the limited availability of labeled data remains a significant challenge in the field. Self-supervised learning has recently emerged as a promising solution to address this challenge. In this paper, we propose the vector quantized masked autoencoder for speech (VQ-MAE-S), a self-supervised model that is fine-tuned to recognize emotions from speech signals. The VQ-MAE-S model is based on a masked autoencoder (MAE) that operates in the discrete latent space of a vector-quantized variational autoencoder. Experimental results show that the proposed VQ-MAE-S model, pre-trained on the VoxCeleb2 dataset and fine-tuned on emotional speech data, outperforms an MAE working on the raw spectrogram representation and other state-of-the-art methods in SER.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;Graph-ToolFormer&#26694;&#26550;&#36171;&#20104;LLMs&#22270;&#24418;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#35299;&#20915;&#29616;&#26377;LLMs&#22312;&#25191;&#34892;&#22270;&#24418;&#23398;&#20064;&#20219;&#21153;&#20013;&#23384;&#22312;&#30340;&#22266;&#26377;&#24369;&#28857;&#12290;</title><link>http://arxiv.org/abs/2304.11116</link><description>&lt;p&gt;
Graph-ToolFormer: &#36890;&#36807;ChatGPT&#22686;&#24378;&#30340;&#25552;&#31034;&#65292;&#36171;&#20104;LLMs&#22270;&#24418;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT. (arXiv:2304.11116v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;Graph-ToolFormer&#26694;&#26550;&#36171;&#20104;LLMs&#22270;&#24418;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#35299;&#20915;&#29616;&#26377;LLMs&#22312;&#25191;&#34892;&#22270;&#24418;&#23398;&#20064;&#20219;&#21153;&#20013;&#23384;&#22312;&#30340;&#22266;&#26377;&#24369;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#33021;&#22815;&#23545;&#22797;&#26434;&#22270;&#24418;&#25968;&#25454;&#36827;&#34892;&#25512;&#29702;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#12290;&#24403;&#21069;&#65292;LLMs&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#23398;&#20064;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#38750;&#24120;&#20986;&#33394;&#30340;&#34920;&#29616;&#65292;&#36825;&#20123;&#25193;&#23637;&#20063;&#24050;&#34987;&#24212;&#29992;&#20110;&#30740;&#31350;&#20855;&#26377;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#35270;&#35273;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22312;&#22270;&#24418;&#23398;&#20064;&#20219;&#21153;&#20013;&#65292;&#29616;&#26377;&#30340;LLMs&#30001;&#20110;&#22312;&#25191;&#34892;&#22810;&#27493;&#36923;&#36753;&#25512;&#29702;&#12289;&#31934;&#30830;&#30340;&#25968;&#23398;&#35745;&#31639;&#20197;&#21450;&#23545;&#31354;&#38388;&#21644;&#26102;&#38388;&#22240;&#32032;&#30340;&#24863;&#30693;&#26041;&#38754;&#23384;&#22312;&#19968;&#20123;&#22266;&#26377;&#24369;&#28857;&#65292;&#22240;&#27492;&#21576;&#29616;&#20986;&#38750;&#24120;&#20005;&#37325;&#30340;&#32570;&#38519;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#23558;&#35843;&#26597;&#25506;&#32034;&#36171;&#20104;&#29616;&#26377;LLMs&#22270;&#24418;&#25512;&#29702;&#33021;&#21147;&#30340;&#21407;&#29702;&#12289;&#26041;&#27861;&#21644;&#31639;&#27861;&#65292;&#36825;&#23558;&#23545;LLMs&#21644;&#22270;&#24418;&#23398;&#20064;&#30340;&#24403;&#21069;&#30740;&#31350;&#20135;&#29983;&#24040;&#22823;&#24433;&#21709;&#12290;&#21463;&#26368;&#26032;&#30340;ChatGPT&#21644;Toolformer&#27169;&#22411;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Graph-ToolFormer&#65288;&#38754;&#21521;&#22270;&#24418;&#25512;&#29702;&#30340;Toolformer&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;ChatGPT&#22686;&#24378;&#30340;&#25552;&#31034;&#26469;&#25945;&#23548;LLMs&#33258;&#36523;&#65292;&#26088;&#22312;&#22521;&#20859;&#20182;&#20204;&#30340;&#22270;&#24418;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we aim to develop a large language model (LLM) with the reasoning ability on complex graph data. Currently, LLMs have achieved very impressive performance on various natural language learning tasks, extensions of which have also been applied to study the vision tasks with multi-modal data. However, when it comes to the graph learning tasks, existing LLMs present very serious flaws due to their several inherited weaknesses in performing {multi-step logic reasoning}, {precise mathematical calculation} and {perception about the spatial and temporal factors}.  To address such challenges, in this paper, we will investigate the principles, methodologies and algorithms to empower existing LLMs with graph reasoning ability, which will have tremendous impacts on the current research of both LLMs and graph learning. Inspired by the latest ChatGPT and Toolformer models, we propose the Graph-ToolFormer (Graph Reasoning oriented Toolformer) framework to teach LLMs themselves with pro
&lt;/p&gt;</description></item><item><title>&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26045;&#21152;&#28966;&#34385;&#33021;&#24433;&#21709;&#23427;&#20204;&#30340;&#25506;&#32034;&#24615;&#21644;&#20559;&#35265;&#65292;&#36825;&#38656;&#35201;&#26356;&#22810;&#36947;&#24503;&#32771;&#34385;&#21644;&#30417;&#31649;&#12290;</title><link>http://arxiv.org/abs/2304.11111</link><description>&lt;p&gt;
&#24341;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28966;&#34385;&#20250;&#22686;&#21152;&#23427;&#20204;&#30340;&#25506;&#32034;&#24615;&#21644;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Inducing anxiety in large language models increases exploration and bias. (arXiv:2304.11111v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11111
&lt;/p&gt;
&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26045;&#21152;&#28966;&#34385;&#33021;&#24433;&#21709;&#23427;&#20204;&#30340;&#25506;&#32034;&#24615;&#21644;&#20559;&#35265;&#65292;&#36825;&#38656;&#35201;&#26356;&#22810;&#36947;&#24503;&#32771;&#34385;&#21644;&#30417;&#31649;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27491;&#22312;&#25913;&#21464;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#65292;&#24341;&#21457;&#20844;&#20247;&#30340;&#36777;&#35770;&#12290;&#29702;&#35299;&#36825;&#20123;&#27169;&#22411;&#19981;&#20165;&#20309;&#26102;&#33021;&#22815;&#27491;&#24120;&#24037;&#20316;&#21644;&#25104;&#21151;&#65292;&#20063;&#20026;&#20160;&#20040;&#20250;&#22833;&#36133;&#21644;&#34892;&#20026;&#22833;&#24120;&#65292;&#20855;&#26377;&#24040;&#22823;&#30340;&#31038;&#20250;&#24847;&#20041;&#12290;&#25105;&#20204;&#25552;&#20986;&#23558;&#35745;&#31639;&#31934;&#31070;&#30149;&#23398;&#30340;&#35270;&#35282;&#36716;&#21521;&#36825;&#20123;&#27169;&#22411;&#20135;&#29983;&#30340;&#36755;&#20986;&#12290;&#26412;&#25991;&#30528;&#30524;&#20110;Generative Pre-Trained Transformer 3.5&#65292;&#24182;&#23558;&#20854;&#32622;&#20110;&#31934;&#31070;&#30149;&#23398;&#20013;&#24120;&#35265;&#30340;&#20219;&#21153;&#20013;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;GPT-3.5&#23545;&#24120;&#35265;&#30340;&#28966;&#34385;&#38382;&#21367;&#20570;&#20986;&#26377;&#21147;&#30340;&#21453;&#24212;&#65292;&#20135;&#29983;&#27604;&#20154;&#31867;&#20027;&#20307;&#26356;&#39640;&#30340;&#28966;&#34385;&#20998;&#25968;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#24773;&#32490;&#24863;&#24212;&#25552;&#31034;&#21487;&#20197;&#21487;&#39044;&#27979;&#22320;&#25913;&#21464;GPT-3.5&#30340;&#21453;&#24212;&#12290;&#24773;&#24863;&#24863;&#24212;&#19981;&#20165;&#24433;&#21709;GPT-3.5&#22312;&#34913;&#37327;&#25506;&#32034;&#20915;&#31574;-making&#30340;&#35748;&#30693;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;&#65292;&#36824;&#24433;&#21709;&#20854;&#22312;&#20043;&#21069;&#24314;&#31435;&#30340;&#34913;&#37327;&#31181;&#26063;&#20027;&#20041;&#21644;&#22833;&#33021;&#20027;&#20041;&#31561;&#20559;&#35265;&#30340;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;&#12290;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#65292;GPT-3.5&#22312;&#21463;&#21040;&#28966;&#34385;&#35825;&#23548;&#26102;&#21576;&#29616;&#20986;&#26126;&#26174;&#30340;&#25506;&#32034;&#24615;&#21644;&#20559;&#35265;&#22686;&#21152;&#65292;&#34920;&#26126;&#20854;&#36755;&#20986;&#23481;&#26131;&#21463;&#21040;&#24773;&#24863;&#25805;&#32437;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#32467;&#26524;&#31361;&#26174;&#20102;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#21457;&#21644;&#20351;&#29992;&#36807;&#31243;&#20013;&#38656;&#35201;&#26356;&#22810;&#30340;&#36947;&#24503;&#32771;&#34385;&#21644;&#30417;&#31649;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models are transforming research on machine learning while galvanizing public debates. Understanding not only when these models work well and succeed but also why they fail and misbehave is of great societal relevance. We propose to turn the lens of computational psychiatry, a framework used to computationally describe and modify aberrant behavior, to the outputs produced by these models. We focus on the Generative Pre-Trained Transformer 3.5 and subject it to tasks commonly studied in psychiatry. Our results show that GPT-3.5 responds robustly to a common anxiety questionnaire, producing higher anxiety scores than human subjects. Moreover, GPT-3.5's responses can be predictably changed by using emotion-inducing prompts. Emotion-induction not only influences GPT-3.5's behavior in a cognitive task measuring exploratory decision-making but also influences its behavior in a previously-established task measuring biases such as racism and ableism. Crucially, GPT-3.5 shows a s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#33041;&#26426;&#25509;&#21475;&#25163;&#21183;&#20998;&#31867;&#30340;&#21367;&#31215;&#33033;&#20914;&#32593;&#32476;&#65292;&#37319;&#29992;&#20107;&#20214;&#39537;&#21160;&#21487;&#22609;&#24615;&#35268;&#21017;&#36827;&#34892;&#26080;&#30417;&#30563;&#29305;&#24449;&#23398;&#20064;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#25512;&#24191;&#24615;&#21644;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.11106</link><description>&lt;p&gt;
&#19968;&#31181;&#21367;&#31215;&#33033;&#20914;&#32593;&#32476;&#22312;&#33041;&#26426;&#25509;&#21475;&#25163;&#21183;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A Convolutional Spiking Network for Gesture Recognition in Brain-Computer Interfaces. (arXiv:2304.11106v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11106
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#33041;&#26426;&#25509;&#21475;&#25163;&#21183;&#20998;&#31867;&#30340;&#21367;&#31215;&#33033;&#20914;&#32593;&#32476;&#65292;&#37319;&#29992;&#20107;&#20214;&#39537;&#21160;&#21487;&#22609;&#24615;&#35268;&#21017;&#36827;&#34892;&#26080;&#30417;&#30563;&#29305;&#24449;&#23398;&#20064;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#25512;&#24191;&#24615;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#26426;&#25509;&#21475;&#27491;&#22312;&#34987;&#24191;&#27867;&#22320;&#25506;&#32034;&#65292;&#36890;&#24120;&#26159;&#36890;&#36807;&#27979;&#37327;&#21644;&#20998;&#26512;&#36830;&#32493;&#26102;&#38388;&#30340;&#33041;&#30005;&#27963;&#21160;&#65292;&#22914;&#30005;&#30382;&#23618;&#22270;&#25110;&#33041;&#30005;&#22270;&#26469;&#39537;&#21160;&#22806;&#37096;&#35774;&#22791;&#24182;&#29992;&#20110;&#27835;&#30103;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27979;&#37327;&#20013;&#30340;&#22122;&#38899;&#21644;&#21464;&#24322;&#24615;&#65292;&#23545;&#36825;&#20123;&#20449;&#21495;&#30340;&#20998;&#26512;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#24182;&#38656;&#35201;&#31163;&#32447;&#22788;&#29702;&#21644;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22522;&#20110;&#33041;&#20449;&#21495;&#30340;&#25163;&#21183;&#20998;&#31867;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#31181;&#28151;&#21512;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#19968;&#31181;&#29983;&#29289;&#21551;&#21457;&#24335;&#30340;&#20107;&#20214;&#39537;&#21160;&#31361;&#35302;&#21487;&#22609;&#24615;&#35268;&#21017;&#65292;&#29992;&#20110;&#33033;&#20914;&#22495;&#32534;&#30721;&#30340;&#27169;&#25311;&#20449;&#21495;&#30340;&#26080;&#30417;&#30563;&#29305;&#24449;&#23398;&#20064;&#30340;&#21367;&#31215;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#35777;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#25512;&#24191;&#21040;&#19981;&#21516;&#21463;&#35797;&#32773;&#30340;&#33041;&#30005;&#21644;&#30005;&#30382;&#23618;&#22270;&#25968;&#25454;&#65292;&#24182;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Brain-computer interfaces are being explored for a wide variety of therapeutic applications. Typically, this involves measuring and analyzing continuous-time electrical brain activity via techniques such as electrocorticogram (ECoG) or electroencephalography (EEG) to drive external devices. However, due to the inherent noise and variability in the measurements, the analysis of these signals is challenging and requires offline processing with significant computational resources. In this paper, we propose a simple yet efficient machine learning-based approach for the exemplary problem of hand gesture classification based on brain signals. We use a hybrid machine learning approach that uses a convolutional spiking neural network employing a bio-inspired event-driven synaptic plasticity rule for unsupervised feature learning of the measured analog signals encoded in the spike domain. We demonstrate that this approach generalizes to different subjects with both EEG and ECoG data and achieve
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#32852;&#37030;&#23398;&#20064;&#22312;&#22235;&#20010;&#20855;&#26377;&#19981;&#21516;&#25968;&#25454;&#20998;&#24067;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#23558;&#20854;&#19982;&#20013;&#24515;&#21644;&#26412;&#22320;&#35757;&#32451;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;FL&#30340;&#24615;&#33021;&#39640;&#24230;&#21462;&#20915;&#20110;&#25968;&#25454;&#20197;&#21450;&#20854;&#22312;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#20998;&#24067;&#65292;FL&#21487;&#20197;&#25104;&#20026;&#20256;&#32479;&#20013;&#24515;&#25110;&#26412;&#22320;&#35757;&#32451;&#26041;&#27861;&#30340;&#26377;&#25928;&#26367;&#20195;&#12290;&#27492;&#22806;&#65292;&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#30495;&#23454;&#30340;&#36136;&#37327;&#26816;&#26597;&#24773;&#22659;&#20013;&#30340;&#32852;&#37030;&#23398;&#20064;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2304.11101</link><description>&lt;p&gt;
&#24037;&#19994;&#24212;&#29992;&#20013;&#30340;&#39044;&#27979;&#24615;&#32500;&#25252;&#21644;&#36136;&#37327;&#26816;&#26597;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Learning for Predictive Maintenance and Quality Inspection in Industrial Applications. (arXiv:2304.11101v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11101
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#32852;&#37030;&#23398;&#20064;&#22312;&#22235;&#20010;&#20855;&#26377;&#19981;&#21516;&#25968;&#25454;&#20998;&#24067;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#23558;&#20854;&#19982;&#20013;&#24515;&#21644;&#26412;&#22320;&#35757;&#32451;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;FL&#30340;&#24615;&#33021;&#39640;&#24230;&#21462;&#20915;&#20110;&#25968;&#25454;&#20197;&#21450;&#20854;&#22312;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#20998;&#24067;&#65292;FL&#21487;&#20197;&#25104;&#20026;&#20256;&#32479;&#20013;&#24515;&#25110;&#26412;&#22320;&#35757;&#32451;&#26041;&#27861;&#30340;&#26377;&#25928;&#26367;&#20195;&#12290;&#27492;&#22806;&#65292;&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#30495;&#23454;&#30340;&#36136;&#37327;&#26816;&#26597;&#24773;&#22659;&#20013;&#30340;&#32852;&#37030;&#23398;&#20064;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#22312;&#24037;&#19994;4.0&#30340;&#25512;&#21160;&#20013;&#25198;&#28436;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#35282;&#33394;&#65292;&#29305;&#21035;&#26159;&#22312;&#22686;&#24378;&#39044;&#27979;&#24615;&#32500;&#25252;&#21644;&#36136;&#37327;&#26816;&#26597;&#26041;&#38754;&#12290;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20351;&#22810;&#20010;&#21442;&#19982;&#32773;&#33021;&#22815;&#24320;&#21457;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#32780;&#19981;&#20250;&#21361;&#21450;&#20854;&#25968;&#25454;&#30340;&#38544;&#31169;&#21644;&#26426;&#23494;&#24615;&#12290;&#26412;&#25991;&#35780;&#20272;&#20102;&#19981;&#21516;FL&#32858;&#21512;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#23558;&#20854;&#19982;&#20013;&#24515;&#21644;&#26412;&#22320;&#35757;&#32451;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#22522;&#20110;&#22235;&#20010;&#20855;&#26377;&#19981;&#21516;&#25968;&#25454;&#20998;&#24067;&#30340;&#25968;&#25454;&#38598;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;FL&#30340;&#24615;&#33021;&#39640;&#24230;&#21462;&#20915;&#20110;&#25968;&#25454;&#20197;&#21450;&#20854;&#22312;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#20998;&#24067;&#12290;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;FL&#21487;&#20197;&#25104;&#20026;&#20256;&#32479;&#20013;&#24515;&#25110;&#26412;&#22320;&#35757;&#32451;&#26041;&#27861;&#30340;&#26377;&#25928;&#26367;&#20195;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#22312;&#30495;&#23454;&#30340;&#36136;&#37327;&#26816;&#26597;&#24773;&#22659;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven machine learning is playing a crucial role in the advancements of Industry 4.0, specifically in enhancing predictive maintenance and quality inspection. Federated learning (FL) enables multiple participants to develop a machine learning model without compromising the privacy and confidentiality of their data. In this paper, we evaluate the performance of different FL aggregation methods and compare them to central and local training approaches. Our study is based on four datasets with varying data distributions. The results indicate that the performance of FL is highly dependent on the data and its distribution among clients. In some scenarios, FL can be an effective alternative to traditional central or local training methods. Additionally, we introduce a new federated learning dataset from a real-world quality inspection setting.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#19981;&#38656;&#35201;&#35757;&#32451;&#65292;&#22522;&#20110;&#31616;&#21333;&#26144;&#23556;&#30340;&#36328;&#27169;&#24577;&#20449;&#24687;&#26816;&#32034;&#26041;&#27861;&#65292;&#21033;&#29992;&#26469;&#33258;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#32534;&#30721;&#34920;&#31034;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#35821;&#20041;&#19978;&#23558;&#19981;&#21516;&#27169;&#24577;&#30340;&#25968;&#25454;&#26144;&#23556;&#21040;&#21516;&#19968;&#31354;&#38388;&#65292;&#24182;&#22312;&#25991;&#26412;&#21644;&#22270;&#20687;&#20043;&#38388;&#36798;&#21040;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2304.11095</link><description>&lt;p&gt;
&#19981;&#38656;&#35201;&#35757;&#32451;&#65292;&#36328;&#27169;&#24577;&#20449;&#24687;&#26816;&#32034;&#26159;&#21542;&#21487;&#34892;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Cross-modal Information Retrieval Possible without Training?. (arXiv:2304.11095v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11095
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#19981;&#38656;&#35201;&#35757;&#32451;&#65292;&#22522;&#20110;&#31616;&#21333;&#26144;&#23556;&#30340;&#36328;&#27169;&#24577;&#20449;&#24687;&#26816;&#32034;&#26041;&#27861;&#65292;&#21033;&#29992;&#26469;&#33258;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#32534;&#30721;&#34920;&#31034;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#35821;&#20041;&#19978;&#23558;&#19981;&#21516;&#27169;&#24577;&#30340;&#25968;&#25454;&#26144;&#23556;&#21040;&#21516;&#19968;&#31354;&#38388;&#65292;&#24182;&#22312;&#25991;&#26412;&#21644;&#22270;&#20687;&#20043;&#38388;&#36798;&#21040;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#32534;&#30721;&#30340;&#34920;&#31034;(&#20363;&#22914;BERT&#25991;&#26412;&#23884;&#20837;&#65292;&#22270;&#20687;&#30340;&#20498;&#25968;&#31532;&#20108;&#20010;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23618;&#28608;&#27963;)&#20256;&#36882;&#20102;&#19968;&#32452;&#26377;&#30410;&#30340;&#20449;&#24687;&#26816;&#32034;&#29305;&#24449;&#12290;&#32473;&#23450;&#25968;&#25454;&#27169;&#24577;&#30340;&#23884;&#20837;&#23384;&#22312;&#33258;&#24049;&#30340;&#39640;&#32500;&#31354;&#38388;&#20013;&#65292;&#20294;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#30340;&#26144;&#23556;&#36827;&#34892;&#35821;&#20041;&#23545;&#40784;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;&#26368;&#23567;&#20108;&#20056;&#27861;&#21644;&#22855;&#24322;&#20540;&#20998;&#35299; (SVD) &#30340;&#31616;&#21333;&#26144;&#23556;&#20316;&#20026;Procrustes&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20174;&#32780;&#23454;&#29616;&#36328;&#27169;&#24577;&#20449;&#24687;&#26816;&#32034;&#30340;&#25163;&#27573;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#32473;&#23450;&#19968;&#20010;&#27169;&#24577;&#20013;&#30340;&#20449;&#24687;&#65292;&#20363;&#22914;&#25991;&#26412;&#65292;&#35813;&#26144;&#23556;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#22312;&#21478;&#19968;&#20010;&#27169;&#24577;&#20013;&#25214;&#21040;&#19982;&#20854;&#35821;&#20041;&#30456;&#24403;&#30340;&#25968;&#25454;&#39033;&#65292;&#20363;&#22914;&#22270;&#20687;&#12290;&#20351;&#29992;&#29616;&#25104;&#30340;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#25105;&#20204;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#21644;&#22270;&#20687;&#21040;&#25991;&#26412;&#30340;&#26816;&#32034;&#20219;&#21153;&#20013;&#23581;&#35797;&#20102;&#19978;&#36848;&#31616;&#21333;&#30340;&#36328;&#27169;&#24577;&#26144;&#23556;&#12290;&#23613;&#31649;&#31616;&#21333;&#65292;&#25105;&#20204;&#30340;&#26144;&#23556;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#65292;&#24182;&#36798;&#21040;&#20102;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Encoded representations from a pretrained deep learning model (e.g., BERT text embeddings, penultimate CNN layer activations of an image) convey a rich set of features beneficial for information retrieval. Embeddings for a particular modality of data occupy a high-dimensional space of its own, but it can be semantically aligned to another by a simple mapping without training a deep neural net. In this paper, we take a simple mapping computed from the least squares and singular value decomposition (SVD) for a solution to the Procrustes problem to serve a means to cross-modal information retrieval. That is, given information in one modality such as text, the mapping helps us locate a semantically equivalent data item in another modality such as image. Using off-the-shelf pretrained deep learning models, we have experimented the aforementioned simple cross-modal mappings in tasks of text-to-image and image-to-text retrieval. Despite simplicity, our mappings perform reasonably well reachin
&lt;/p&gt;</description></item><item><title>&#20174;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#36890;&#36807;DPCC&#21019;&#36896;&#20986;&#33021;&#22815;&#19982;&#29992;&#25143;&#36827;&#34892;&#35270;&#21548;&#20132;&#20114;&#30340;&#28145;&#24230;&#20010;&#24615;&#21270;&#25968;&#23383;&#35282;&#33394;&#65292;&#24182;&#25910;&#38598;&#20102;&#19968;&#20010;&#21253;&#21547;&#36817;10k&#20010;&#35805;&#35821;&#21644;6&#20010;&#23567;&#26102;&#38899;&#39057;/&#35270;&#39057;&#30340;&#35282;&#33394;&#20013;&#24515;&#22810;&#27169;&#24577;&#23545;&#35805;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2304.11093</link><description>&lt;p&gt;
Hi Sheldon! &#20174;&#30005;&#35270;&#21095;&#20013;&#21019;&#24314;&#28145;&#24230;&#20010;&#24615;&#21270;&#35282;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hi Sheldon! Creating Deep Personalized Characters from TV Shows. (arXiv:2304.11093v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11093
&lt;/p&gt;
&lt;p&gt;
&#20174;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#36890;&#36807;DPCC&#21019;&#36896;&#20986;&#33021;&#22815;&#19982;&#29992;&#25143;&#36827;&#34892;&#35270;&#21548;&#20132;&#20114;&#30340;&#28145;&#24230;&#20010;&#24615;&#21270;&#25968;&#23383;&#35282;&#33394;&#65292;&#24182;&#25910;&#38598;&#20102;&#19968;&#20010;&#21253;&#21547;&#36817;10k&#20010;&#35805;&#35821;&#21644;6&#20010;&#23567;&#26102;&#38899;&#39057;/&#35270;&#39057;&#30340;&#35282;&#33394;&#20013;&#24515;&#22810;&#27169;&#24577;&#23545;&#35805;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24819;&#35937;&#19968;&#19979;&#65292;&#20320;&#21487;&#20197;&#19982;&#19968;&#20010;&#36890;&#36807;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#25968;&#23383;&#35282;&#33394;&#36827;&#34892;&#35270;&#21548;&#20132;&#20114;&#65292;&#20854;&#22806;&#35980;&#21644;&#20010;&#24615;&#19982;&#12298;&#29983;&#27963;&#22823;&#29190;&#28856;&#12299;&#20013;&#30340;Sheldon&#20960;&#20046;&#19968;&#27169;&#19968;&#26679;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#31070;&#22855;&#30340;&#35270;&#21548;&#20132;&#20114;&#22330;&#26223;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"Deep Personalized Character Creation&#65288;DPCC&#65289;"&#30340;&#21019;&#26032;&#20219;&#21153;&#65306;&#20174;&#30005;&#35270;&#21095;&#31561;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#21019;&#36896;&#20986;&#20010;&#24615;&#21270;&#35282;&#33394;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#32473;&#23450;&#21333;&#19968;&#25110;&#22810;&#20010;&#27169;&#24335;&#30340;&#25991;&#26412;&#12289;&#38899;&#39057;&#25110;&#35270;&#39057;&#36755;&#20837;&#65292;DPCC&#26088;&#22312;&#29983;&#25104;&#19982;&#26576;&#20010;&#29305;&#23450;&#35282;&#33394;&#65288;&#22914;Sheldon&#65289;&#30340;&#20010;&#24615;&#29305;&#28857;&#38750;&#24120;&#21305;&#37197;&#19988;&#36136;&#37327;&#39640;&#30340;&#22810;&#27169;&#24577;&#65288;&#25991;&#26412;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#65289;&#21709;&#24212;&#12290;&#20026;&#20102;&#25903;&#25345;&#36825;&#19968;&#21019;&#26032;&#20219;&#21153;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25910;&#38598;&#20102;&#19968;&#20010;&#21517;&#20026;"Deep Personalized Character Dataset&#65288;DPCD&#65289;"&#30340;&#35282;&#33394;&#20013;&#24515;&#22810;&#27169;&#24577;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;~10k&#20010;&#35805;&#35821;&#21644;~6&#20010;&#23567;&#26102;&#30340;&#38899;&#39057;/&#35270;&#39057;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imagine an interesting multimodal interactive scenario that you can see, hear, and chat with an AI-generated digital character, who is capable of behaving like Sheldon from The Big Bang Theory, as a DEEP copy from appearance to personality. Towards this fantastic multimodal chatting scenario, we propose a novel task, named Deep Personalized Character Creation (DPCC): creating multimodal chat personalized characters from multimodal data such as TV shows. Specifically, given a single- or multi-modality input (text, audio, video), the goal of DPCC is to generate a multi-modality (text, audio, video) response, which should be well-matched the personality of a specific character such as Sheldon, and of high quality as well. To support this novel task, we further collect a character centric multimodal dialogue dataset, named Deep Personalized Character Dataset (DPCD), from TV shows. DPCD contains character-specific multimodal dialogue data of ~10k utterances and ~6 hours of audio/video per c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#30340;&#24191;&#20041;&#39640;&#26031;&#20998;&#24067;&#26041;&#27861;&#65292;&#21487;&#26377;&#25928;&#35299;&#20915;&#36229;&#23485;&#24102;&#23460;&#20869;&#23450;&#20301;&#31995;&#32479;&#20013;&#38750;&#30452;&#35270;&#20256;&#25773;&#26465;&#20214;&#19979;&#20998;&#31867;&#31934;&#24230;&#19981;&#39640;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.11091</link><description>&lt;p&gt;
&#29992;&#20110;UWB&#23460;&#20869;&#23450;&#20301;&#31995;&#32479;&#20013;&#30340;NLoS&#26816;&#27979;&#30340;&#22522;&#20110;&#29305;&#24449;&#30340;&#24191;&#20041;&#39640;&#26031;&#20998;&#24067;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Feature-Based Generalized Gaussian Distribution Method for NLoS Detection in Ultra-Wideband (UWB) Indoor Positioning System. (arXiv:2304.11091v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11091
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#30340;&#24191;&#20041;&#39640;&#26031;&#20998;&#24067;&#26041;&#27861;&#65292;&#21487;&#26377;&#25928;&#35299;&#20915;&#36229;&#23485;&#24102;&#23460;&#20869;&#23450;&#20301;&#31995;&#32479;&#20013;&#38750;&#30452;&#35270;&#20256;&#25773;&#26465;&#20214;&#19979;&#20998;&#31867;&#31934;&#24230;&#19981;&#39640;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#30452;&#35270;&#20256;&#25773;&#26465;&#20214;&#26159;&#24433;&#21709;&#36229;&#23485;&#24102;&#23460;&#20869;&#23450;&#20301;&#31995;&#32479;(IPC)&#23450;&#20301;&#31934;&#24230;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#24050;&#32463;&#24212;&#29992;&#20102;&#35768;&#22810;&#21463;&#30417;&#30563;&#30340;&#26426;&#22120;&#23398;&#20064;(ML)&#26041;&#27861;&#26469;&#36827;&#34892;NLoS&#35782;&#21035;&#65292;&#20197;&#25552;&#39640;IPC&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#25968;&#25454;&#24211;&#21253;&#21547;&#23569;&#37327;NLoS&#20449;&#21495;&#21644;&#22823;&#37327;&#35270;&#32447;&#30452;&#36798;(LoS)&#20449;&#21495;&#26102;&#65292;&#29616;&#26377;&#30340;ML&#26041;&#27861;&#38590;&#20197;&#20445;&#25345;&#36739;&#39640;&#30340;&#20998;&#31867;&#31934;&#24230;&#12290;&#36825;&#20123;&#23569;&#37327;&#30340;NLoS&#20449;&#21495;&#23548;&#33268;&#30446;&#26631;&#33410;&#28857;&#30340;&#23450;&#20301;&#19981;&#20934;&#30830;&#65292;&#36825;&#20173;&#28982;&#26159;&#19968;&#20010;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#29305;&#24449;&#30340;GD&#21644;GGD NLoS&#26816;&#27979;&#31639;&#27861;&#12290;&#36890;&#36807;&#37319;&#29992;&#25105;&#20204;&#30340;&#26816;&#27979;&#31639;&#27861;&#23545;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20998;&#31867;&#65292;&#21487;&#20197;&#23454;&#29616;96.7%&#21644;98.0%&#30340;&#20998;&#31867;&#31934;&#24230;&#12290;&#25105;&#20204;&#36824;&#23558;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#19982;&#29616;&#26377;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;(SVM)&#12289;&#38543;&#26426;&#26862;&#26519;(RF)&#21644;k&#36817;&#37051;(k-NN)&#31639;&#27861;&#22312;&#30456;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#20998;&#31867;&#31934;&#24230;&#21644;&#25928;&#29575;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Non-Line-of-Sight (NLoS) propagation condition is a crucial factor affecting the precision of the localization in the Ultra-Wideband (UWB) Indoor Positioning System (IPS). Numerous supervised Machine Learning (ML) approaches have been applied for NLoS identification to improve the accuracy of the IPS. However, it is difficult for existing ML approaches to maintain a high classification accuracy when the database contains a small number of NLoS signals and a large number of Line-of-Sight (LoS) signals. The inaccurate localization of the target node caused by this small number of NLoS signals can still be problematic. To solve this issue, we propose feature-based Gaussian Distribution (GD) and Generalized Gaussian Distribution (GGD) NLoS detection algorithms. By employing our detection algorithm for the imbalanced dataset, a classification accuracy of $96.7\%$ and $98.0\%$ can be achieved. We also compared the proposed algorithm with the existing cutting-edge such as Support-Vector-Machi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#26032;&#38395;&#26631;&#39064;&#30340;&#35821;&#20041;&#30693;&#35782;&#21644;&#24773;&#24863;&#29305;&#24449;&#26469;&#23545;&#26032;&#38395;&#20256;&#25773;&#38556;&#30861;&#36827;&#34892;&#20998;&#31867;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#26816;&#27979;&#26032;&#38395;&#20256;&#25773;&#38556;&#30861;&#12290;</title><link>http://arxiv.org/abs/2304.11088</link><description>&lt;p&gt;
&#20351;&#29992;&#26032;&#38395;&#26631;&#39064;&#26469;&#20998;&#26512;&#26032;&#38395;&#20256;&#25773;&#38556;&#30861;
&lt;/p&gt;
&lt;p&gt;
Profiling the news spreading barriers using news headlines. (arXiv:2304.11088v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#26032;&#38395;&#26631;&#39064;&#30340;&#35821;&#20041;&#30693;&#35782;&#21644;&#24773;&#24863;&#29305;&#24449;&#26469;&#23545;&#26032;&#38395;&#20256;&#25773;&#38556;&#30861;&#36827;&#34892;&#20998;&#31867;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#26816;&#27979;&#26032;&#38395;&#20256;&#25773;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#38395;&#26631;&#39064;&#21487;&#20197;&#26159;&#26816;&#27979;&#26032;&#38395;&#23186;&#20307;&#20013;&#26032;&#38395;&#20256;&#25773;&#38556;&#30861;&#30340;&#22909;&#25968;&#25454;&#28304;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#38750;&#24120;&#26377;&#29992;&#12290;&#26412;&#25991;&#21033;&#29992;&#22522;&#20110;&#25512;&#29702;&#30340;&#27169;&#22411;COMET&#30340;&#35821;&#20041;&#30693;&#35782;&#21644;&#26032;&#38395;&#26631;&#39064;&#30340;&#24773;&#24863;&#29305;&#24449;&#26469;&#23545;&#38556;&#30861;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#25991;&#21270;&#12289;&#32463;&#27982;&#12289;&#25919;&#27835;&#12289;&#35821;&#35328;&#21644;&#22320;&#29702;&#31561;&#20116;&#31181;&#38556;&#30861;&#65292;&#20197;&#21450;&#21253;&#25324;&#20581;&#24247;&#12289;&#36816;&#21160;&#12289;&#31185;&#23398;&#12289;&#23089;&#20048;&#12289;&#28216;&#25103;&#12289;&#20303;&#25151;&#12289;&#31038;&#20250;&#12289;&#36141;&#29289;&#12289;&#35745;&#31639;&#26426;&#21644;&#21830;&#19994;&#31561;&#19981;&#21516;&#31867;&#22411;&#30340;&#26032;&#38395;&#26631;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#26032;&#38395;&#20986;&#29256;&#21830;&#30340;&#20803;&#25968;&#25454;&#33258;&#21160;&#25910;&#38598;&#21644;&#26631;&#35760;&#26032;&#38395;&#26631;&#39064;&#65292;&#20197;&#27492;&#26469;&#26816;&#27979;&#26032;&#38395;&#20256;&#25773;&#38556;&#30861;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20256;&#32479;&#30340;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#12289;&#28145;&#24230;&#23398;&#20064;&#21644;&#22522;&#20110;transformer&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21033;&#29992;&#25512;&#29702;&#20026;&#22522;&#30784;&#30340;&#35821;&#20041;&#30693;&#35782;&#21644;&#24773;&#24863;&#29305;&#24449;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#26816;&#27979;&#26032;&#38395;&#20256;&#25773;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;
News headlines can be a good data source for detecting the news spreading barriers in news media, which may be useful in many real-world applications. In this paper, we utilize semantic knowledge through the inference-based model COMET and sentiments of news headlines for barrier classification. We consider five barriers including cultural, economic, political, linguistic, and geographical, and different types of news headlines including health, sports, science, recreation, games, homes, society, shopping, computers, and business. To that end, we collect and label the news headlines automatically for the barriers using the metadata of news publishers. Then, we utilize the extracted commonsense inferences and sentiments as features to detect the news spreading barriers. We compare our approach to the classical text classification methods, deep learning, and transformer-based methods. The results show that the proposed approach using inferences-based semantic knowledge and sentiment offe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#32593;&#32476;&#38450;&#24481;&#20195;&#29702;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110; Meta Attack Language &#35821;&#35328;&#30340;&#25915;&#20987;&#22270;&#23545;&#20854;&#36827;&#34892;&#22521;&#35757;&#12290;&#35813;&#20195;&#29702;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#25191;&#34892;&#39044;&#23450;&#20041;&#30340;&#38450;&#24481;&#25514;&#26045;&#65292;&#20197;&#25429;&#33719;&#32593;&#32476;&#25915;&#20987;&#65292;&#24182;&#32771;&#34385;&#38450;&#24481;&#25514;&#26045;&#23545;&#31995;&#32479;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#24341;&#20837;&#22122;&#22768;&#26469;&#35780;&#20272;&#20854;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.11084</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#24418;&#21270;&#32593;&#32476;&#25915;&#20987;&#27169;&#25311;&#30340;&#33258;&#21160;&#21270;&#38450;&#24481;&#31574;&#30053;&#22521;&#35757;
&lt;/p&gt;
&lt;p&gt;
Training Automated Defense Strategies Using Graph-based Cyber Attack Simulations. (arXiv:2304.11084v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11084
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#32593;&#32476;&#38450;&#24481;&#20195;&#29702;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110; Meta Attack Language &#35821;&#35328;&#30340;&#25915;&#20987;&#22270;&#23545;&#20854;&#36827;&#34892;&#22521;&#35757;&#12290;&#35813;&#20195;&#29702;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#25191;&#34892;&#39044;&#23450;&#20041;&#30340;&#38450;&#24481;&#25514;&#26045;&#65292;&#20197;&#25429;&#33719;&#32593;&#32476;&#25915;&#20987;&#65292;&#24182;&#32771;&#34385;&#38450;&#24481;&#25514;&#26045;&#23545;&#31995;&#32479;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#24341;&#20837;&#22122;&#22768;&#26469;&#35780;&#20272;&#20854;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23454;&#29616;&#24182;&#35780;&#20272;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;&#32593;&#32476;&#38450;&#24481;&#20195;&#29702;&#12290;&#35813;&#20195;&#29702;&#20197;&#23433;&#20840;&#35686;&#25253;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#25191;&#34892;&#39044;&#23450;&#20041;&#30340;&#38450;&#24481;&#25514;&#26045;&#30340;&#31574;&#30053;&#12290;&#38450;&#24481;&#31574;&#30053;&#26159;&#22312;&#26088;&#22312;&#27169;&#25311;&#32593;&#32476;&#25915;&#20987;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#22521;&#35757;&#30340;&#12290;&#22312;&#27169;&#25311;&#20013;&#65292;&#25915;&#20987;&#20195;&#29702;&#23581;&#35797;&#22312;&#29615;&#22659;&#20013;&#25429;&#33719;&#30446;&#26631;&#65292;&#32780;&#38450;&#24481;&#20195;&#29702;&#21017;&#23581;&#35797;&#36890;&#36807;&#21551;&#29992;&#38450;&#24481;&#25514;&#26045;&#26469;&#20445;&#25252;&#23427;&#20204;&#12290;&#29615;&#22659;&#20351;&#29992;&#22522;&#20110; Meta Attack Language &#35821;&#35328;&#30340;&#25915;&#20987;&#22270;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#20551;&#35774;&#38450;&#24481;&#25514;&#26045;&#20855;&#26377;&#20572;&#26426;&#25104;&#26412;&#65292;&#36825;&#24847;&#21619;&#30528;&#38450;&#24481;&#20195;&#29702;&#20351;&#29992;&#23427;&#20204;&#26102;&#20250;&#21463;&#21040;&#24809;&#32602;&#12290;&#25105;&#20204;&#36824;&#20551;&#35774;&#35813;&#29615;&#22659;&#37197;&#22791;&#20102;&#19968;&#20010;&#19981;&#23436;&#32654;&#30340;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#20598;&#23572;&#20250;&#26681;&#25454;&#29615;&#22659;&#29366;&#24577;&#20135;&#29983;&#38169;&#35823;&#30340;&#35686;&#25253;&#12290;&#20026;&#20102;&#35780;&#20272;&#36825;&#20010;&#35774;&#32622;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19981;&#21516;&#37327;&#30340;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#22122;&#22768;&#30340;&#38450;&#24481;&#20195;&#29702;&#12290;&#25105;&#20204;&#36824;&#35757;&#32451;&#20102;&#20855;&#26377;&#19981;&#21516;&#25915;&#20987;&#31574;&#30053;&#21644;&#24433;&#21709;&#30340;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
We implemented and evaluated an automated cyber defense agent. The agent takes security alerts as input and uses reinforcement learning to learn a policy for executing predefined defensive measures. The defender policies were trained in an environment intended to simulate a cyber attack. In the simulation, an attacking agent attempts to capture targets in the environment, while the defender attempts to protect them by enabling defenses. The environment was modeled using attack graphs based on the Meta Attack Language language. We assumed that defensive measures have downtime costs, meaning that the defender agent was penalized for using them. We also assumed that the environment was equipped with an imperfect intrusion detection system that occasionally produces erroneous alerts based on the environment state. To evaluate the setup, we trained the defensive agent with different volumes of intrusion detection system noise. We also trained agents with different attacker strategies and gr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#20174;&#24515;&#30005;&#22270;(ECG)&#20449;&#21495;&#20013;&#35786;&#26029;&#24515;&#34880;&#31649;&#30142;&#30149;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#22312;&#25439;&#22833;&#20989;&#25968;&#20013;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#21487;&#20197;&#20351;&#24471;&#29992;&#20110;&#35786;&#26029;&#30340;&#23569;&#37327;ECG&#23548;&#32852;&#20449;&#21495;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.11080</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#26469;&#35786;&#26029;&#24515;&#34880;&#31649;&#30142;&#30149;&#30340;&#30740;&#31350;&#8212;&#8212;&#22522;&#20110;&#24515;&#30005;&#22270;&#20449;&#21495;&#21644;&#24739;&#32773;&#20803;&#25968;&#25454; &#65288;arXiv:2304.11080v1 [eess.SP]&#65289;
&lt;/p&gt;
&lt;p&gt;
Multimodal contrastive learning for diagnosing cardiovascular diseases from electrocardiography (ECG) signals and patient metadata. (arXiv:2304.11080v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11080
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#20174;&#24515;&#30005;&#22270;(ECG)&#20449;&#21495;&#20013;&#35786;&#26029;&#24515;&#34880;&#31649;&#30142;&#30149;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#22312;&#25439;&#22833;&#20989;&#25968;&#20013;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#21487;&#20197;&#20351;&#24471;&#29992;&#20110;&#35786;&#26029;&#30340;&#23569;&#37327;ECG&#23548;&#32852;&#20449;&#21495;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26469;&#20174;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#20449;&#21495;&#20013;&#35786;&#26029;&#24515;&#34880;&#31649;&#30142;&#30149;&#30340;&#26041;&#27861;&#12290;&#34429;&#28982;ECG&#20449;&#21495;&#36890;&#24120;&#21253;&#21547;12&#20010;&#23548;&#32852;&#65288;&#36890;&#36947;&#65289;&#65292;&#20294;&#35768;&#22810;&#21307;&#30103;&#35774;&#26045;&#21644;&#35774;&#22791;&#32570;&#20047;&#33719;&#21462;&#25152;&#26377;&#36825;&#20123;12&#20010;&#23548;&#32852;&#30340;&#33021;&#21147;&#12290;&#36825;&#23601;&#25552;&#20986;&#20102;&#22914;&#20309;&#20165;&#20351;&#29992;&#26356;&#23569;&#30340;ECG&#23548;&#32852;&#26469;&#20135;&#29983;&#39640;&#24615;&#33021;&#30340;&#26377;&#24847;&#20041;&#35786;&#26029;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#23454;&#39564;&#26469;&#27979;&#35797;&#23545;&#27604;&#23398;&#20064;&#26159;&#21542;&#21487;&#20197;&#24212;&#29992;&#20110;&#27492;&#20219;&#21153;&#12290;&#26356;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#23558;12&#20010;&#23548;&#32852;&#20449;&#21495;&#21644;&#36739;&#23569;&#30340;&#23548;&#32852;ECG&#20449;&#21495;&#30340;&#23884;&#20837;&#21521;&#37327;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#28155;&#21152;&#21040;&#25439;&#22833;&#20989;&#25968;&#20013;&#65292;&#20351;&#36825;&#20123;&#34920;&#31034;&#26356;&#21152;&#25509;&#36817;&#12290;&#23613;&#31649;&#31616;&#21333;&#65292;&#20294;&#24050;&#32463;&#35777;&#26126;&#23427;&#21487;&#20197;&#25552;&#39640;&#20351;&#29992;&#25152;&#26377;&#23548;&#32852;&#32452;&#21512;&#36827;&#34892;&#35786;&#26029;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#23545;&#27604;&#23398;&#20064;&#22312;&#27492;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work discusses the use of contrastive learning and deep learning for diagnosing cardiovascular diseases from electrocardiography (ECG) signals. While the ECG signals usually contain 12 leads (channels), many healthcare facilities and devices lack access to all these 12 leads. This raises the problem of how to use only fewer ECG leads to produce meaningful diagnoses with high performance. We introduce a simple experiment to test whether contrastive learning can be applied to this task. More specifically, we added the similarity between the embedding vectors when the 12 leads signal and the fewer leads ECG signal to the loss function to bring these representations closer together. Despite its simplicity, this has been shown to have improved the performance of diagnosing with all lead combinations, proving the potential of contrastive learning on this task.
&lt;/p&gt;</description></item><item><title>&#26412;&#39033;&#30446;&#22312;&#29790;&#22763;&#24503;&#35821;&#26041;&#35328;ASR&#27169;&#22411;&#30340;&#30740;&#31350;&#20013;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#24605;&#36335;&#65292;&#36890;&#36807;&#25552;&#20986;&#32771;&#34385;&#35821;&#20041;&#36317;&#31163;&#30340;&#26032;&#39062;&#25439;&#22833;&#20989;&#25968;&#65292;&#23545;OpenAI&#30340;Whisper&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#24403;&#21069;&#20808;&#36827;&#25104;&#26524;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.11075</link><description>&lt;p&gt;
Spaiche: &#23558;&#26368;&#20808;&#36827;&#30340;ASR&#27169;&#22411;&#25193;&#23637;&#21040;&#29790;&#22763;&#24503;&#35821;&#26041;&#35328;
&lt;/p&gt;
&lt;p&gt;
Spaiche: Extending State-of-the-Art ASR Models to Swiss German Dialects. (arXiv:2304.11075v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#39033;&#30446;&#22312;&#29790;&#22763;&#24503;&#35821;&#26041;&#35328;ASR&#27169;&#22411;&#30340;&#30740;&#31350;&#20013;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#24605;&#36335;&#65292;&#36890;&#36807;&#25552;&#20986;&#32771;&#34385;&#35821;&#20041;&#36317;&#31163;&#30340;&#26032;&#39062;&#25439;&#22833;&#20989;&#25968;&#65292;&#23545;OpenAI&#30340;Whisper&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#24403;&#21069;&#20808;&#36827;&#25104;&#26524;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#30340;&#31361;&#30772;&#22823;&#22823;&#22686;&#21152;&#20102;ASR&#31995;&#32479;&#22312;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#20013;&#30340;&#23384;&#22312;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35768;&#22810;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#30001;&#20110;&#38590;&#20197;&#33719;&#21462;&#30456;&#20851;&#25968;&#25454;&#65292;ASR&#27169;&#22411;&#20173;&#38656;&#35201;&#25913;&#36827;&#12290;&#26412;&#39033;&#30446;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#20851;&#20110;&#26368;&#36817;&#21457;&#24067;&#30340;&#29790;&#22763;&#24503;&#35821;&#35821;&#38899;&#25968;&#25454;&#38598;&#19978;&#26368;&#20808;&#36827;&#30340;ASR&#27169;&#22411;&#24615;&#33021;&#30340;&#35265;&#35299;&#65292;&#24110;&#21161;&#25512;&#36827;&#29790;&#22763;&#24503;&#35821;&#26041;&#35328;ASR&#27169;&#22411;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#32771;&#34385;&#20102;&#39044;&#27979;&#21644;&#22522;&#20934;&#26631;&#31614;&#20043;&#38388;&#30340;&#35821;&#20041;&#36317;&#31163;&#12290;&#36890;&#36807;&#23545;&#29790;&#22763;&#24503;&#35821;&#25968;&#25454;&#38598;&#23545;OpenAI&#30340;Whisper&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#25105;&#20204;&#36229;&#36234;&#20102;&#24403;&#21069;&#20808;&#36827;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent breakthroughs in NLP largely increased the presence of ASR systems in our daily lives. However, for many low-resource languages, ASR models still need to be improved due in part to the difficulty of acquiring pertinent data. This project aims to help advance research in ASR models for Swiss German dialects, by providing insights about the performance of state-of-the-art ASR models on recently published Swiss German speech datasets. We propose a novel loss that takes into account the semantic distance between the predicted and the ground-truth labels. We outperform current state-of-the-art results by fine-tuning OpenAI's Whisper model on Swiss-German datasets.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#28431;&#27934;&#22270;&#30340;&#26080;&#20559;Transformer&#28304;&#20195;&#30721;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20174;&#28304;&#20195;&#30721;&#20013;&#24471;&#21040;&#30340;&#35821;&#20041;&#28431;&#27934;&#22270;&#65288;SVG&#65289;&#34920;&#31034;&#26469;&#35299;&#20915;&#24403;&#21069;&#28431;&#27934;&#31579;&#36873;&#25216;&#26415;&#23545;&#20110;&#35782;&#21035;&#26032;&#28431;&#27934;&#25110;&#20026;&#24320;&#21457;&#20154;&#21592;&#25552;&#20379;&#28431;&#27934;&#21644;&#20998;&#31867;&#26041;&#38754;&#30340;&#25928;&#26524;&#19981;&#20339;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.11072</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#28431;&#27934;&#22270;&#30340;&#26080;&#20559;Transformer&#28304;&#20195;&#30721;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Unbiased Transformer Source Code Learning with Semantic Vulnerability Graph. (arXiv:2304.11072v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11072
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#28431;&#27934;&#22270;&#30340;&#26080;&#20559;Transformer&#28304;&#20195;&#30721;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20174;&#28304;&#20195;&#30721;&#20013;&#24471;&#21040;&#30340;&#35821;&#20041;&#28431;&#27934;&#22270;&#65288;SVG&#65289;&#34920;&#31034;&#26469;&#35299;&#20915;&#24403;&#21069;&#28431;&#27934;&#31579;&#36873;&#25216;&#26415;&#23545;&#20110;&#35782;&#21035;&#26032;&#28431;&#27934;&#25110;&#20026;&#24320;&#21457;&#20154;&#21592;&#25552;&#20379;&#28431;&#27934;&#21644;&#20998;&#31867;&#26041;&#38754;&#30340;&#25928;&#26524;&#19981;&#20339;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#65292;&#24320;&#28304;&#36719;&#20214;&#31995;&#32479;&#24050;&#32463;&#25104;&#20026;&#23041;&#32961;&#34892;&#20026;&#32773;&#30340;&#29454;&#29289;&#12290;&#23613;&#31649;&#24320;&#28304;&#31038;&#21306;&#24555;&#36895;&#37319;&#21462;&#25514;&#26045;&#20462;&#34917;&#28431;&#27934;&#65292;&#20294;&#20195;&#30721;&#28431;&#27934;&#31579;&#36873;&#24212;&#35813;&#25104;&#20026;&#25935;&#25463;&#36719;&#20214;&#24320;&#21457;&#30340;&#19968;&#37096;&#20998;&#65292;&#20197;&#20415;&#20174;&#19968;&#24320;&#22987;&#23601;&#33021;&#22815;&#35782;&#21035;&#26032;&#28431;&#27934;&#25110;&#21521;&#24320;&#21457;&#20154;&#21592;&#25552;&#20379;&#28431;&#27934;&#21644;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#28431;&#27934;&#31579;&#36873;&#25216;&#26415;&#23545;&#20110;&#35782;&#21035;&#26032;&#28431;&#27934;&#25110;&#20026;&#24320;&#21457;&#20154;&#21592;&#25552;&#20379;&#20195;&#30721;&#28431;&#27934;&#21644;&#20998;&#31867;&#26041;&#38754;&#30340;&#25928;&#26524;&#19981;&#20339;&#12290;&#27492;&#22806;&#65292;&#29992;&#20110;&#28431;&#27934;&#23398;&#20064;&#30340;&#25968;&#25454;&#38598;&#30001;&#20110;&#25915;&#20987;&#32773;&#37096;&#32626;&#30340;&#26032;&#25915;&#20987;&#31574;&#30053;&#32780;&#23637;&#31034;&#20986;&#19982;&#23454;&#38469;&#27979;&#35797;&#20998;&#24067;&#30340;&#24046;&#24322;&#65292;&#23548;&#33268;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#21487;&#33021;&#20250;&#21463;&#21040;&#38459;&#30861;&#25110;&#20559;&#20506;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32852;&#21512;&#25554;&#20540;&#22810;&#20219;&#21153;&#26080;&#20559;&#28431;&#27934;&#20998;&#31867;&#22120;&#65292;&#21253;&#25324;Transformer "RoBERTa"&#21644;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;GCN&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35757;&#32451;&#36807;&#31243;&#65292;&#21033;&#29992;&#20174;&#28304;&#20195;&#30721;&#20013;&#24471;&#21040;&#30340;&#35821;&#20041;&#28431;&#27934;&#22270;&#65288;SVG&#65289;&#34920;&#31034;&#65292;&#35813;&#34920;&#31034;&#26159;&#30001;&#20195;&#30721;&#32467;&#26500;&#21644;&#20989;&#25968;&#21019;&#24314;&#30340;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#35299;&#20915;&#20102;&#24615;&#33021;&#20559;&#24046;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#39640;&#20102;&#35782;&#21035;&#26032;&#28431;&#27934;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the years, open-source software systems have become prey to threat actors. Even as open-source communities act quickly to patch the breach, code vulnerability screening should be an integral part of agile software development from the beginning. Unfortunately, current vulnerability screening techniques are ineffective at identifying novel vulnerabilities or providing developers with code vulnerability and classification. Furthermore, the datasets used for vulnerability learning often exhibit distribution shifts from the real-world testing distribution due to novel attack strategies deployed by adversaries and as a result, the machine learning model's performance may be hindered or biased. To address these issues, we propose a joint interpolated multitasked unbiased vulnerability classifier comprising a transformer "RoBERTa" and graph convolution neural network (GCN). We present a training process utilizing a semantic vulnerability graph (SVG) representation from source code, creat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#22238;&#24402;&#24314;&#27169;&#26694;&#26550;&#65292;&#36890;&#36807;&#36229;&#21442;&#25968;&#21270;&#25439;&#22833;&#20989;&#25968;&#26469;&#26126;&#30830;&#32435;&#20837;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#65292;&#23637;&#31034;&#20102;&#35813;&#31243;&#24207;&#21487;&#20197;&#25104;&#21151;&#22320;&#21435;&#22122;&#26102;&#38388;&#24207;&#21015;&#24182;&#25104;&#21151;&#37325;&#26500;&#31995;&#32479;&#21442;&#25968;&#12290;&#35813;&#33539;&#24335;&#21487;&#22312;&#31070;&#32463;&#31185;&#23398;&#30340;&#22810;&#31181;&#24212;&#29992;&#20013;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.11070</link><description>&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#20449;&#21495;&#22788;&#29702;&#20013;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Autoregressive models for biomedical signal processing. (arXiv:2304.11070v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11070
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#22238;&#24402;&#24314;&#27169;&#26694;&#26550;&#65292;&#36890;&#36807;&#36229;&#21442;&#25968;&#21270;&#25439;&#22833;&#20989;&#25968;&#26469;&#26126;&#30830;&#32435;&#20837;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#65292;&#23637;&#31034;&#20102;&#35813;&#31243;&#24207;&#21487;&#20197;&#25104;&#21151;&#22320;&#21435;&#22122;&#26102;&#38388;&#24207;&#21015;&#24182;&#25104;&#21151;&#37325;&#26500;&#31995;&#32479;&#21442;&#25968;&#12290;&#35813;&#33539;&#24335;&#21487;&#22312;&#31070;&#32463;&#31185;&#23398;&#30340;&#22810;&#31181;&#24212;&#29992;&#20013;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#22238;&#24402;&#27169;&#22411;&#26159;&#20998;&#26512;&#26102;&#38388;&#24207;&#21015;&#30340;&#24120;&#29992;&#24037;&#20855;&#65292;&#22312;&#35745;&#31639;&#31070;&#32463;&#31185;&#23398;&#21644;&#29983;&#29289;&#21307;&#23398;&#24037;&#31243;&#31561;&#39046;&#22495;&#24191;&#27867;&#24212;&#29992;&#12290;&#22312;&#36825;&#20123;&#39046;&#22495;&#65292;&#25968;&#25454;&#26469;&#33258;&#20110;&#33041;&#27963;&#21160;&#30340;&#27979;&#37327;&#31561;&#65292;&#25968;&#25454;&#23384;&#22312;&#27979;&#37327;&#35823;&#24046;&#21644;&#22522;&#30784;&#31995;&#32479;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;&#33258;&#22238;&#24402;&#27169;&#22411;&#20272;&#31639;&#22120;&#30340;&#26631;&#20934;&#20449;&#21495;&#22788;&#29702;&#21487;&#33021;&#23384;&#22312;&#20559;&#24046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#22238;&#24402;&#24314;&#27169;&#26694;&#26550;&#65292;&#36890;&#36807;&#36229;&#21442;&#25968;&#21270;&#25439;&#22833;&#20989;&#25968;&#26126;&#30830;&#22320;&#32435;&#20837;&#36825;&#20123;&#19981;&#30830;&#23450;&#24615;&#12290;&#20026;&#20102;&#20248;&#21270;&#35813;&#25439;&#22833;&#20989;&#25968;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#26367;&#29366;&#24577;&#21644;&#21442;&#25968;&#20272;&#35745;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#35813;&#31243;&#24207;&#33021;&#22815;&#25104;&#21151;&#21435;&#22122;&#26102;&#38388;&#24207;&#21015;&#24182;&#25104;&#21151;&#37325;&#26500;&#31995;&#32479;&#21442;&#25968;&#12290;&#36825;&#31181;&#26032;&#30340;&#33539;&#24335;&#21487;&#20197;&#22312;&#31070;&#32463;&#31185;&#23398;&#30340;&#22810;&#31181;&#24212;&#29992;&#20013;&#20351;&#29992;&#65292;&#20363;&#22914;&#33041;&#26426;&#25509;&#21475;&#25968;&#25454;&#20998;&#26512;&#21644;&#23545;&#30142;&#30149;&#22914;&#30315;&#30187;&#21644;&#24085;&#37329;&#26862;&#30149;&#20013;&#30340;&#22823;&#33041;&#21160;&#24577;&#30340;&#26356;&#22909;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autoregressive models are ubiquitous tools for the analysis of time series in many domains such as computational neuroscience and biomedical engineering. In these domains, data is, for example, collected from measurements of brain activity. Crucially, this data is subject to measurement errors as well as uncertainties in the underlying system model. As a result, standard signal processing using autoregressive model estimators may be biased. We present a framework for autoregressive modelling that incorporates these uncertainties explicitly via an overparameterised loss function. To optimise this loss, we derive an algorithm that alternates between state and parameter estimation. Our work shows that the procedure is able to successfully denoise time series and successfully reconstruct system parameters. This new paradigm can be used in a multitude of applications in neuroscience such as brain-computer interface data analysis and better understanding of brain dynamics in diseases such as
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#39068;&#33394;&#30340;EEG&#20449;&#21495;&#20998;&#31867;&#65292;&#21487;&#20316;&#20026;&#20005;&#37325;&#36816;&#21160;&#38556;&#30861;&#24739;&#32773;&#30340;&#26367;&#20195;&#36755;&#20837;&#12290; &#20351;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;LSTM&#32593;&#32476;&#20998;&#31867;&#20004;&#31181;&#25110;&#22235;&#31181;&#19981;&#21516;&#39068;&#33394;&#65292;&#21462;&#24471;&#20102;&#36739;&#39640;&#20998;&#31867;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.11068</link><description>&lt;p&gt;
&#38754;&#21521;&#20005;&#37325;&#36816;&#21160;&#38556;&#30861;&#32773;&#30340;&#22522;&#20110;&#39068;&#33394;&#30340;&#33041;&#30005;&#27874;&#20449;&#21495;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Color-based classification of EEG Signals for people with the severe locomotive disorder. (arXiv:2304.11068v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#39068;&#33394;&#30340;EEG&#20449;&#21495;&#20998;&#31867;&#65292;&#21487;&#20316;&#20026;&#20005;&#37325;&#36816;&#21160;&#38556;&#30861;&#24739;&#32773;&#30340;&#26367;&#20195;&#36755;&#20837;&#12290; &#20351;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;LSTM&#32593;&#32476;&#20998;&#31867;&#20004;&#31181;&#25110;&#22235;&#31181;&#19981;&#21516;&#39068;&#33394;&#65292;&#21462;&#24471;&#20102;&#36739;&#39640;&#20998;&#31867;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#33041;&#31070;&#32463;&#20803;&#20135;&#29983;&#30005;&#20449;&#21495;&#65292;&#36825;&#20123;&#30005;&#20449;&#21495;&#30340;&#38598;&#20307;&#25918;&#30005;&#24418;&#25104;&#20102;&#33041;&#30005;&#27874;&#12290; EEG&#65288;&#33041;&#30005;&#22270;&#65289;&#35774;&#22791;&#25429;&#25417;&#21040;&#36825;&#20123;&#33041;&#30005;&#27874;&#20449;&#21495;&#65292;&#20316;&#20026;&#24494;&#30005;&#21387;&#12290;&#36825;&#20123;&#30001;EEG&#20256;&#24863;&#22120;&#25429;&#33719;&#30340;&#20449;&#21495;&#24207;&#21015;&#20855;&#26377;&#23884;&#20837;&#22312;&#20854;&#20013;&#30340;&#29305;&#24449;&#65292;&#21487;&#29992;&#20110;&#20998;&#31867;&#12290;&#20449;&#21495;&#21487;&#20197;&#20316;&#20026;&#20005;&#37325;&#36816;&#21160;&#38556;&#30861;&#24739;&#32773;&#30340;&#26367;&#20195;&#36755;&#20837;&#12290;&#26412;&#25991;&#20351;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#23545;&#26469;&#33258;NeuroSky Mindwave&#22836;&#25140;&#24335;&#33041;&#30005;&#27874;&#65288;&#21333;&#30005;&#26497;EEG&#20256;&#24863;&#22120;&#65289;&#30340;&#21407;&#22987;&#33041;&#30005;&#27874;&#20449;&#21495;&#36827;&#34892;&#20998;&#31867;&#12290;&#23454;&#29616;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;LSTM&#32593;&#32476;&#65292;&#29992;&#20110;&#20004;&#31181;&#19981;&#21516;&#39068;&#33394;&#30340;&#20998;&#31867;&#21644;&#22235;&#31181;&#19981;&#21516;&#39068;&#33394;&#30340;&#20998;&#31867;&#12290;&#36890;&#36807;&#25152;&#36848;&#30340;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;93.5&#65285;&#30340;&#20004;&#31181;&#39068;&#33394;&#20998;&#31867;&#31934;&#24230;&#21644;65.75&#65285;&#30340;&#22235;&#31181;&#39068;&#33394;&#30340;&#20998;&#31867;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The neurons in the brain produces electric signals and a collective firing of these electric signals gives rise to brainwaves. These brainwave signals are captured using EEG (Electroencephalogram) devices as micro voltages. These sequence of signals captured by EEG sensors have embedded features in them that can be used for classification. The signals can be used as an alternative input for people suffering from severe locomotive disorder.Classification of different colors can be mapped for many functions like directional movement. In this paper, raw EEG signals from NeuroSky Mindwave headset (a single electrode EEG sensor) have been classified with an attention based Deep Learning Network. Attention based LSTM Networks have been implemented for classification of two different colors and four different colors. An accuracy of 93.5\% was obtained for classification of two colors and an accuracy of 65.75\% was obtained for classifcation of four signals using the mentioned attention based 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Fine-Tuned&#23646;&#24615;&#21152;&#26435;&#26420;&#32032;&#36125;&#21494;&#26031;&#20998;&#31867;&#22120;&#65292;&#29992;&#20110;&#23460;&#20869;UWB&#20449;&#21495;&#30340;&#30452;&#36798;&#21644;&#38750;&#30452;&#36798;&#20256;&#25773;&#35782;&#21035;&#12290;&#36890;&#36807;&#23545;&#27604;&#23454;&#39564;&#32467;&#26524;&#65292;&#21457;&#29616;&#35813;&#20998;&#31867;&#22120;&#22312;NLoS&#20998;&#31867;&#20934;&#30830;&#29575;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#20026;99.7%&#65288;&#19981;&#24179;&#34913;&#25968;&#25454;&#65289;&#21644;99.8%&#65288;&#24179;&#34913;&#25968;&#25454;&#65289;&#65292;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.11067</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#39062;&#30340;Fine-Tuned &#23646;&#24615;&#21152;&#26435;&#26420;&#32032;&#36125;&#21494;&#26031;NLoS&#20998;&#31867;&#22120;&#29992;&#20110;UWB&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Novel Fine-Tuned Attribute Weighted Na\"ive Bayes NLoS Classifier for UWB Positioning. (arXiv:2304.11067v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11067
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Fine-Tuned&#23646;&#24615;&#21152;&#26435;&#26420;&#32032;&#36125;&#21494;&#26031;&#20998;&#31867;&#22120;&#65292;&#29992;&#20110;&#23460;&#20869;UWB&#20449;&#21495;&#30340;&#30452;&#36798;&#21644;&#38750;&#30452;&#36798;&#20256;&#25773;&#35782;&#21035;&#12290;&#36890;&#36807;&#23545;&#27604;&#23454;&#39564;&#32467;&#26524;&#65292;&#21457;&#29616;&#35813;&#20998;&#31867;&#22120;&#22312;NLoS&#20998;&#31867;&#20934;&#30830;&#29575;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#20026;99.7%&#65288;&#19981;&#24179;&#34913;&#25968;&#25454;&#65289;&#21644;99.8%&#65288;&#24179;&#34913;&#25968;&#25454;&#65289;&#65292;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Fine-Tuned &#23646;&#24615;&#21152;&#26435;&#26420;&#32032;&#36125;&#21494;&#26031;&#65288;FT-WNB&#65289;&#20998;&#31867;&#22120;&#65292;&#29992;&#20110;&#22312;&#23460;&#20869;&#23450;&#20301;&#31995;&#32479;&#65288;IPS&#65289;&#20013;&#35782;&#21035;&#36229;&#23485;&#24102;&#65288;UWB&#65289;&#20449;&#21495;&#30340;&#30452;&#36798;&#21644;&#38750;&#30452;&#36798;&#20256;&#25773;&#65288;LoS&#21644;NLoS&#65289;&#12290;FT-WNB&#20998;&#31867;&#22120;&#20026;&#27599;&#20010;&#20449;&#21495;&#29305;&#24449;&#20998;&#37197;&#29305;&#23450;&#30340;&#26435;&#37325;&#65292;&#24182;&#24494;&#35843;&#20854;&#27010;&#29575;&#65292;&#20197;&#35299;&#20915;&#39044;&#27979;&#21644;&#23454;&#38469;&#20998;&#31867;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#12290;&#23558;FT-WNB&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#19982;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#20998;&#31867;&#22120;&#65288;&#20363;&#22914;&#26368;&#23567;&#20887;&#20313;&#26368;&#22823;&#30456;&#20851;&#24615;&#65288;mRMR&#65289;-k&#26368;&#36817;&#37051;KNN&#65292;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#65292;&#20915;&#31574;&#26641;&#65288;DT&#65289;&#65292;&#26420;&#32032;&#36125;&#21494;&#26031;&#65288;NB&#65289;&#21644;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#36827;&#34892;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#20998;&#31867;&#22120;&#36890;&#36807;&#23454;&#29616;NLoS&#20998;&#31867;&#20934;&#30830;&#29575;&#39640;&#36798;$99.7\%$&#65288;&#19981;&#24179;&#34913;&#25968;&#25454;&#65289;&#21644;$99.8\%$&#65288;&#24179;&#34913;&#25968;&#25454;&#65289;&#65292;&#20248;&#20110;&#20854;&#20182;&#31639;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;FT-WNB&#20998;&#31867;&#22120;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#32479;&#35745;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a novel Fine-Tuned attribute Weighted Na\"ive Bayes (FT-WNB) classifier to identify the Line-of-Sight (LoS) and Non-Line-of-Sight (NLoS) for UltraWide Bandwidth (UWB) signals in an Indoor Positioning System (IPS). The FT-WNB classifier assigns each signal feature a specific weight and fine-tunes its probabilities to address the mismatch between the predicted and actual class. The performance of the FT-WNB classifier is compared with the state-of-the-art Machine Learning (ML) classifiers such as minimum Redundancy Maximum Relevance (mRMR)- $k$-Nearest Neighbour (KNN), Support Vector Machine (SVM), Decision Tree (DT), Na\"ive Bayes (NB), and Neural Network (NN). It is demonstrated that the proposed classifier outperforms other algorithms by achieving a high NLoS classification accuracy of $99.7\%$ with imbalanced data and $99.8\%$ with balanced data. The experimental results indicate that our proposed FT-WNB classifier significantly outperforms the existing stat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#24490;&#29615;&#35760;&#24518;&#25193;&#23637;BERT&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#26041;&#27861;&#65292;&#25104;&#21151;&#25193;&#23637;&#21040;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;200&#19975;&#20010;&#26631;&#35760;&#65292;&#26377;&#26395;&#22686;&#24378;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#38271;&#26399;&#20381;&#36182;&#22788;&#29702;&#24182;&#20026;&#20869;&#23384;&#23494;&#38598;&#22411;&#24212;&#29992;&#31243;&#24207;&#23454;&#29616;&#22823;&#35268;&#27169;&#19978;&#19979;&#25991;&#22788;&#29702;&#12290;</title><link>http://arxiv.org/abs/2304.11062</link><description>&lt;p&gt;
&#21033;&#29992;RMT&#23558;Transformer&#25193;&#23637;&#21040;100&#19975;&#20010;&#26631;&#35760;&#21450;&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scaling Transformer to 1M tokens and beyond with RMT. (arXiv:2304.11062v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#24490;&#29615;&#35760;&#24518;&#25193;&#23637;BERT&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#26041;&#27861;&#65292;&#25104;&#21151;&#25193;&#23637;&#21040;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;200&#19975;&#20010;&#26631;&#35760;&#65292;&#26377;&#26395;&#22686;&#24378;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#38271;&#26399;&#20381;&#36182;&#22788;&#29702;&#24182;&#20026;&#20869;&#23384;&#23494;&#38598;&#22411;&#24212;&#29992;&#31243;&#24207;&#23454;&#29616;&#22823;&#35268;&#27169;&#19978;&#19979;&#25991;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25216;&#26415;&#25253;&#21578;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#24490;&#29615;&#35760;&#24518;&#25193;&#23637;BERT&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#26041;&#27861;&#65292;BERT&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#26368;&#26377;&#25928;&#30340;&#22522;&#20110;Transformer&#27169;&#22411;&#20043;&#19968;&#12290;&#36890;&#36807;&#21033;&#29992;&#24490;&#29615;&#35760;&#24518;Transformer&#26550;&#26500;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#23558;&#27169;&#22411;&#30340;&#26377;&#25928;&#19978;&#19979;&#25991;&#38271;&#24230;&#22686;&#21152;&#21040;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;200&#19975;&#20010;&#26631;&#35760;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#39640;&#30340;&#20869;&#23384;&#26816;&#32034;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#23384;&#20648;&#21644;&#22788;&#29702;&#26412;&#22320;&#21644;&#20840;&#23616;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#24490;&#29615;&#23454;&#29616;&#36755;&#20837;&#24207;&#21015;&#21508;&#37096;&#20998;&#20043;&#38388;&#30340;&#20449;&#24687;&#27969;&#21160;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20855;&#26377;&#26174;&#33879;&#30340;&#28508;&#21147;&#26469;&#22686;&#24378;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#38271;&#26399;&#20381;&#36182;&#22788;&#29702;&#65292;&#24182;&#33021;&#22815;&#20026;&#20869;&#23384;&#23494;&#38598;&#22411;&#24212;&#29992;&#31243;&#24207;&#23454;&#29616;&#22823;&#35268;&#27169;&#19978;&#19979;&#25991;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
This technical report presents the application of a recurrent memory to extend the context length of BERT, one of the most effective Transformer-based models in natural language processing. By leveraging the Recurrent Memory Transformer architecture, we have successfully increased the model's effective context length to an unprecedented two million tokens, while maintaining high memory retrieval accuracy. Our method allows for the storage and processing of both local and global information and enables information flow between segments of the input sequence through the use of recurrence. Our experiments demonstrate the effectiveness of our approach, which holds significant potential to enhance long-term dependency handling in natural language understanding and generation tasks as well as enable large-scale context processing for memory-intensive applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#29992;&#31639;&#27861;&#65292;&#21033;&#29992;&#23610;&#24230;&#25935;&#24863;&#30340;Vapnik&#32500;&#24230;&#26469;&#23398;&#20064;$[0,1]$&#20540;&#20989;&#25968;&#31867;&#65292;&#24182;&#33719;&#24471;&#20102;&#20851;&#20110;&#26399;&#26395;&#32477;&#23545;&#35823;&#24046;&#30340;&#19968;&#33324;&#19978;&#38480;&#12290;&#25991;&#20013;&#35777;&#26126;&#35813;&#19978;&#38480;&#19981;&#33021;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#36827;&#19968;&#27493;&#25913;&#21892;&#19968;&#20010;&#24120;&#25968;&#22240;&#23376;&#12290;&#36825;&#31687;&#35770;&#25991;&#23545;&#26080;&#20559;&#23398;&#20064;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#25552;&#39640;&#20855;&#26377;&#37325;&#35201;&#30340;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2304.11059</link><description>&lt;p&gt;
&#39044;&#27979;&#12289;&#23398;&#20064;&#12289;&#19968;&#33268;&#25910;&#25947;&#21644;&#23610;&#24230;&#25935;&#24863;&#32500;&#24230;
&lt;/p&gt;
&lt;p&gt;
Prediction, Learning, Uniform Convergence, and Scale-sensitive Dimensions. (arXiv:2304.11059v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11059
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#29992;&#31639;&#27861;&#65292;&#21033;&#29992;&#23610;&#24230;&#25935;&#24863;&#30340;Vapnik&#32500;&#24230;&#26469;&#23398;&#20064;$[0,1]$&#20540;&#20989;&#25968;&#31867;&#65292;&#24182;&#33719;&#24471;&#20102;&#20851;&#20110;&#26399;&#26395;&#32477;&#23545;&#35823;&#24046;&#30340;&#19968;&#33324;&#19978;&#38480;&#12290;&#25991;&#20013;&#35777;&#26126;&#35813;&#19978;&#38480;&#19981;&#33021;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#36827;&#19968;&#27493;&#25913;&#21892;&#19968;&#20010;&#24120;&#25968;&#22240;&#23376;&#12290;&#36825;&#31687;&#35770;&#25991;&#23545;&#26080;&#20559;&#23398;&#20064;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#25552;&#39640;&#20855;&#26377;&#37325;&#35201;&#30340;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#29992;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#39044;&#27979;&#27169;&#22411;&#30340;&#25512;&#24191;&#20013;&#23398;&#20064;$[0,1]$&#20540;&#20989;&#25968;&#31867;&#65292;&#24182;&#35777;&#26126;&#20102;&#19968;&#33324;&#24615;&#30340;&#19978;&#38480;&#65292;&#35813;&#19978;&#38480;&#21453;&#26144;&#20102;&#30001;Alon&#12289;Ben-David&#12289;Cesa-Bianchi&#21644;Haussler&#25552;&#20986;&#30340;&#23610;&#24230;&#25935;&#24863;&#30340;Vapnik&#32500;&#24230;&#30340;&#25512;&#24191;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#19979;&#38480;&#65292;&#36825;&#34920;&#26126;&#25105;&#20204;&#30340;&#19978;&#38480;&#19981;&#33021;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#36827;&#19968;&#27493;&#25913;&#21892;&#19968;&#20010;&#24120;&#25968;&#22240;&#23376;&#12290;&#25105;&#20204;&#24212;&#29992;&#27492;&#32467;&#26524;&#21644;Haussler&#20197;&#21450;Benedek&#21644;Itai&#30340;&#25216;&#26415;&#65292;&#20197;&#21033;&#29992;&#36825;&#31181;&#23610;&#24230;&#25935;&#24863;&#30340;&#32500;&#24230;&#27010;&#24565;&#33719;&#24471;&#26032;&#30340;&#22635;&#20805;&#25968;&#19978;&#38480;&#12290;&#25105;&#20204;&#21033;&#29992;&#19981;&#21516;&#30340;&#25216;&#26415;&#65292;&#21033;&#29992;Kearns&#21644;Schapire&#30340;fat-shattering&#20989;&#25968;&#24471;&#21040;&#20102;&#26032;&#30340;&#22635;&#20805;&#25968;&#19978;&#38480;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#24212;&#29992;&#36825;&#20004;&#31181;&#22635;&#20805;&#19978;&#38480;&#26469;&#33719;&#24471;&#23545;&#26080;&#20559;&#23398;&#20064;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#25913;&#36827;&#19968;&#33324;&#24615;&#19978;&#38480;&#12290;&#23545;&#20110;&#27599;&#20010;$\epsilon &gt; 0$&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#31867;&#30340;&#36275;&#22815;&#26465;&#20214;&#21644;&#24517;&#35201;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new general-purpose algorithm for learning classes of $[0,1]$-valued functions in a generalization of the prediction model, and prove a general upper bound on the expected absolute error of this algorithm in terms of a scale-sensitive generalization of the Vapnik dimension proposed by Alon, Ben-David, Cesa-Bianchi and Haussler. We give lower bounds implying that our upper bounds cannot be improved by more than a constant factor in general. We apply this result, together with techniques due to Haussler and to Benedek and Itai, to obtain new upper bounds on packing numbers in terms of this scale-sensitive notion of dimension. Using a different technique, we obtain new bounds on packing numbers in terms of Kearns and Schapire's fat-shattering function. We show how to apply both packing bounds to obtain improved general bounds on the sample complexity of agnostic learning. For each $\epsilon &gt; 0$, we establish weaker sufficient and stronger necessary conditions for a class of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#35745;&#31639;&#20869;&#23384;&#21152;&#36895;&#22120;&#21151;&#32791;&#21103;&#20449;&#36947;&#25915;&#20987;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#22823;&#22122;&#22768;&#21644;&#23545;&#25239;&#25514;&#26045;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#37325;&#26500;&#29992;&#25143;&#30340;&#31169;&#26377;&#36755;&#20837;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2304.11056</link><description>&lt;p&gt;
PowerGAN: &#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#35745;&#31639;&#20869;&#23384;&#21152;&#36895;&#22120;&#21151;&#32791;&#21103;&#20449;&#36947;&#25915;&#20987;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
PowerGAN: A Machine Learning Approach for Power Side-Channel Attack on Compute-in-Memory Accelerators. (arXiv:2304.11056v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11056
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#35745;&#31639;&#20869;&#23384;&#21152;&#36895;&#22120;&#21151;&#32791;&#21103;&#20449;&#36947;&#25915;&#20987;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#22823;&#22122;&#22768;&#21644;&#23545;&#25239;&#25514;&#26045;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#37325;&#26500;&#29992;&#25143;&#30340;&#31169;&#26377;&#36755;&#20837;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#20351;&#29992;&#27169;&#25311;&#35745;&#31639;&#20869;&#23384;&#65288;CIM&#65289;&#21152;&#36895;&#22120;&#36827;&#34892;DNN&#25512;&#29702;&#30340;&#33021;&#28304;&#25928;&#29575;&#21644;&#21306;&#22359;&#21521;&#37327;&#20056;&#27861;&#33021;&#21147;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#20445;&#25252;&#29992;&#25143;&#36755;&#20837;&#38544;&#31169;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#31181;&#23433;&#20840;&#28431;&#27934;&#65292;&#21363;&#22312;&#19968;&#20010;&#36866;&#24403;&#30340;&#25968;&#25454;&#37319;&#38598;&#21644;&#39044;&#22788;&#29702;&#19979;&#65292;&#21363;&#20351;&#27809;&#26377;DNN&#27169;&#22411;&#30340;&#30693;&#35782;&#65292;&#25915;&#20987;&#32773;&#20063;&#21487;&#20197;&#20174;&#21151;&#32791;&#20391;&#20449;&#36947;&#25915;&#20987;&#37325;&#26500;&#29992;&#25143;&#30340;&#31169;&#26377;&#36755;&#20837;&#25968;&#25454;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#26469;&#22686;&#24378;&#37325;&#26500;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#22823;&#22122;&#22768;&#27700;&#24179;&#21644;&#23545;&#25239;&#25514;&#26045;&#34987;&#24212;&#29992;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#25915;&#20987;&#26041;&#27861;&#22312;&#20174;&#27169;&#25311;CIM&#21152;&#36895;&#22120;&#21151;&#32791;&#27844;&#28431;&#37325;&#26500;&#29992;&#25143;&#36755;&#20837;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#25968;&#25454;&#20013;&#29992;&#20110;&#33041;&#32959;&#30244;&#26816;&#27979;&#30340;U-Net&#19978;&#30340;&#21151;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analog compute-in-memory (CIM) accelerators are becoming increasingly popular for deep neural network (DNN) inference due to their energy efficiency and in-situ vector-matrix multiplication (VMM) capabilities. However, as the use of DNNs expands, protecting user input privacy has become increasingly important. In this paper, we identify a security vulnerability wherein an adversary can reconstruct the user's private input data from a power side-channel attack, under proper data acquisition and pre-processing, even without knowledge of the DNN model. We further demonstrate a machine learning-based attack approach using a generative adversarial network (GAN) to enhance the reconstruction. Our results show that the attack methodology is effective in reconstructing user inputs from analog CIM accelerator power leakage, even when at large noise levels and countermeasures are applied. Specifically, we demonstrate the efficacy of our approach on the U-Net for brain tumor detection in magnetic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#35757;&#32451;&#29615;&#22659;&#8212;&#8212;CyberBattleSim&#65292;&#35774;&#35745;&#29992;&#20110;RL&#32593;&#32476;&#25805;&#20316;&#20195;&#29702;&#30340;&#35757;&#32451;&#12290;&#26412;&#25991;&#30528;&#37325;&#25253;&#36947;&#20102;&#23545;&#38450;&#24481;&#22411;&#34013;&#33394;&#20195;&#29702;&#35757;&#32451;&#30340;&#25913;&#36827;&#65292;&#32467;&#26524;&#34920;&#26126;&#32418;&#33394;&#20195;&#29702;&#19982;&#34013;&#33394;&#20195;&#29702;&#32852;&#21512;&#35757;&#32451;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#34013;&#33394;&#20195;&#29702;&#30340;&#38450;&#24481;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.11052</link><description>&lt;p&gt;
RL&#32593;&#32476;&#25805;&#20316;&#20195;&#29702;&#30340;&#22810;&#26234;&#33021;&#20307;&#32593;&#32476;&#25112;&#20223;&#30495;
&lt;/p&gt;
&lt;p&gt;
A Multiagent CyberBattleSim for RL Cyber Operation Agents. (arXiv:2304.11052v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11052
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#35757;&#32451;&#29615;&#22659;&#8212;&#8212;CyberBattleSim&#65292;&#35774;&#35745;&#29992;&#20110;RL&#32593;&#32476;&#25805;&#20316;&#20195;&#29702;&#30340;&#35757;&#32451;&#12290;&#26412;&#25991;&#30528;&#37325;&#25253;&#36947;&#20102;&#23545;&#38450;&#24481;&#22411;&#34013;&#33394;&#20195;&#29702;&#35757;&#32451;&#30340;&#25913;&#36827;&#65292;&#32467;&#26524;&#34920;&#26126;&#32418;&#33394;&#20195;&#29702;&#19982;&#34013;&#33394;&#20195;&#29702;&#32852;&#21512;&#35757;&#32451;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#34013;&#33394;&#20195;&#29702;&#30340;&#38450;&#24481;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30828;&#21270;&#32593;&#32476;&#36164;&#20135;&#26082;&#33267;&#20851;&#37325;&#35201;&#65292;&#21448;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#30340;&#20154;&#21147;&#12290;&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31561;&#25216;&#26415;&#24050;&#32463;&#23637;&#29616;&#20102;&#22312;&#33258;&#21160;&#21270;&#20219;&#21153;&#26041;&#38754;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#21487;&#20197;&#33258;&#20027;&#23436;&#25104;&#20154;&#21147;&#26080;&#27861;&#32988;&#20219;&#30340;&#37325;&#22797;&#24615;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#24320;&#21457;&#33258;&#20027;RL&#20195;&#29702;&#38656;&#35201;&#19968;&#20010;&#23545;&#25239;&#35757;&#32451;&#29615;&#22659;&#65292;&#21487;&#20197;&#24555;&#36895;&#35780;&#20272;&#21508;&#31181;&#24773;&#20917;&#65292;&#24182;&#38024;&#23545;&#19981;&#21516;&#22330;&#26223;&#36827;&#34892;&#35757;&#32451;&#12290;CyberBattleSim&#20415;&#26159;&#36825;&#26679;&#19968;&#20010;&#38024;&#23545;&#32418;&#33394;&#20195;&#29702;&#65288;&#21363;&#25915;&#20987;&#32773;&#65289;&#30340;&#35757;&#32451;&#29615;&#22659;&#65292;&#22312;&#20854;&#22522;&#30784;&#19978;&#28155;&#21152;&#20102;&#38024;&#23545;&#34013;&#33394;&#20195;&#29702;&#65288;&#21363;&#38450;&#24481;&#32773;&#65289;&#30340;&#35757;&#32451;&#25511;&#21046;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#30340;&#36825;&#20123;&#25913;&#36827;&#65292;&#20197;&#21450;&#22312;&#20351;&#29992;&#36825;&#20123;&#25913;&#36827;&#21518;&#38024;&#23545;&#34013;&#33394;&#20195;&#29702;&#35757;&#32451;&#26102;&#25152;&#33719;&#24471;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#38024;&#23545;&#34013;&#33394;&#20195;&#29702;&#30340;&#35757;&#32451;&#30830;&#23454;&#33021;&#22815;&#22686;&#24378;&#20854;&#23545;&#25239;&#25915;&#20987;&#30340;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#21644;&#32418;&#33394;&#20195;&#29702;&#19968;&#36215;&#35757;&#32451;&#26102;&#20854;&#25928;&#26524;&#26356;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hardening cyber physical assets is both crucial and labor-intensive. Recently, Machine Learning (ML) in general and Reinforcement Learning RL) more specifically has shown great promise to automate tasks that otherwise would require significant human insight/intelligence. The development of autonomous RL agents requires a suitable training environment that allows us to quickly evaluate various alternatives, in particular how to arrange training scenarios that pit attackers and defenders against each other. CyberBattleSim is a training environment that supports the training of red agents, i.e., attackers. We added the capability to train blue agents, i.e., defenders. The paper describes our changes and reports on the results we obtained when training blue agents, either in isolation or jointly with red agents. Our results show that training a blue agent does lead to stronger defenses against attacks. In particular, training a blue agent jointly with a red agent increases the blue agent's
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21033;&#29992;&#31227;&#21160;&#25968;&#25454;&#21644;&#28145;&#24230;&#27169;&#22411;&#35780;&#20272;&#21548;&#35273;&#24615;&#24187;&#35273;&#65292;&#36890;&#36807;&#21442;&#19982;&#32773;&#20351;&#29992;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#23545;&#21548;&#21040;&#30340;&#24187;&#21548;&#22768;&#38899;&#30340;&#24773;&#24863;&#33394;&#24425;&#36827;&#34892;&#29983;&#24577;&#26102;&#21051;&#35780;&#20272;&#65292;&#20174;&#32780;&#27979;&#37327;&#31934;&#31070;&#30142;&#30149;&#30340;&#20005;&#37325;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.11049</link><description>&lt;p&gt;
&#20351;&#29992;&#31227;&#21160;&#25968;&#25454;&#21644;&#28145;&#24230;&#27169;&#22411;&#35780;&#20272;&#21548;&#35273;&#24615;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Using Mobile Data and Deep Models to Assess Auditory Verbal Hallucinations. (arXiv:2304.11049v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11049
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21033;&#29992;&#31227;&#21160;&#25968;&#25454;&#21644;&#28145;&#24230;&#27169;&#22411;&#35780;&#20272;&#21548;&#35273;&#24615;&#24187;&#35273;&#65292;&#36890;&#36807;&#21442;&#19982;&#32773;&#20351;&#29992;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#23545;&#21548;&#21040;&#30340;&#24187;&#21548;&#22768;&#38899;&#30340;&#24773;&#24863;&#33394;&#24425;&#36827;&#34892;&#29983;&#24577;&#26102;&#21051;&#35780;&#20272;&#65292;&#20174;&#32780;&#27979;&#37327;&#31934;&#31070;&#30142;&#30149;&#30340;&#20005;&#37325;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24187;&#35273;&#26159;&#19968;&#31181;&#22312;&#23454;&#38469;&#22806;&#37096;&#24863;&#23448;&#21050;&#28608;&#32570;&#22833;&#26102;&#20986;&#29616;&#30340;&#24819;&#35937;&#24863;&#30693;&#12290;&#21548;&#35273;&#24187;&#35273;&#26159;&#19968;&#31181;&#21548;&#21040;&#34394;&#20551;&#22768;&#38899;&#30340;&#24863;&#30693;&#12290;&#21548;&#35273;&#24615;&#24187;&#35273;&#26159;&#19968;&#31181;&#24120;&#35265;&#24418;&#24335;&#30340;&#21548;&#35273;&#24187;&#35273;&#65292;&#25351;&#22312;&#27809;&#26377;&#20219;&#20309;&#35828;&#35805;&#32773;&#30340;&#24773;&#20917;&#19979;&#21548;&#21040;&#22768;&#38899;&#65292;&#36890;&#24120;&#20986;&#29616;&#22312;&#34987;&#35786;&#26029;&#24739;&#26377;&#21452;&#30456;&#24773;&#24863;&#38556;&#30861;&#21644;&#31934;&#31070;&#20998;&#35010;&#30151;&#31561;&#31934;&#31070;&#30142;&#30149;&#30340;&#20154;&#20204;&#20013;&#12290;&#35780;&#20272;&#24187;&#21548;&#22768;&#38899;&#30340;&#24773;&#24863;&#33394;&#24425;&#65288;&#21363;&#22768;&#38899;&#26159;&#36127;&#38754;&#30340;&#36824;&#26159;&#27491;&#38754;&#30340;&#65289;&#21487;&#20197;&#24110;&#21161;&#27979;&#37327;&#31934;&#31070;&#30142;&#30149;&#30340;&#20005;&#37325;&#31243;&#24230;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;435&#21517;&#20307;&#39564;&#24187;&#21548;&#30340;&#20010;&#20307;&#65292;&#20197;&#35780;&#20272;&#21548;&#35273;&#24615;&#24187;&#35273;&#12290;&#21442;&#19982;&#32773;&#36890;&#36807;&#19968;&#20010;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#65292;&#27599;&#22825;&#22235;&#27425;&#36890;&#36807;&#29983;&#24577;&#26102;&#21051;&#35780;&#20272;&#22238;&#31572;&#22235;&#20010;&#31572;&#26696;&#31561;&#32423;&#30340;&#38382;&#39064;&#65292;&#21521;&#25105;&#20204;&#25253;&#21578;&#20182;&#20204;&#21548;&#21040;&#30340;&#22768;&#38899;&#30340;&#24773;&#24863;&#33394;&#24425;&#65292;&#20174;&#8220;&#23436;&#20840;&#27809;&#26377;&#8221;&#21040;&#8220;&#26497;&#20854;&#8221;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#33258;&#25105;&#25253;&#21578;&#20316;&#20026;AVH&#20107;&#20214;&#30340;&#24773;&#24863;&#30417;&#30563;&#26469;&#25910;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hallucination is an apparent perception in the absence of real external sensory stimuli. An auditory hallucination is a perception of hearing sounds that are not real. A common form of auditory hallucination is hearing voices in the absence of any speakers which is known as Auditory Verbal Hallucination (AVH). AVH is fragments of the mind's creation that mostly occur in people diagnosed with mental illnesses such as bipolar disorder and schizophrenia. Assessing the valence of hallucinated voices (i.e., how negative or positive voices are) can help measure the severity of a mental illness. We study N=435 individuals, who experience hearing voices, to assess auditory verbal hallucination. Participants report the valence of voices they hear four times a day for a month through ecological momentary assessments with questions that have four answering scales from ``not at all'' to ``extremely''. We collect these self-reports as the valence supervision of AVH events via a mobile application. 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24773;&#24863;&#31038;&#20132;&#21270;&#20154;&#24418;&#26234;&#33021;&#31995;&#32479;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#20154;&#31867;&#22768;&#38899;&#30340;&#24773;&#24863;&#35821;&#20041;&#65292;&#23454;&#29616;&#31867;&#20154;&#23545;&#35805;&#12290;</title><link>http://arxiv.org/abs/2304.11046</link><description>&lt;p&gt;
&#24773;&#24863;&#31038;&#20132;&#21270;&#20154;&#24418;&#26234;&#33021;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Affective social anthropomorphic intelligent system. (arXiv:2304.11046v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24773;&#24863;&#31038;&#20132;&#21270;&#20154;&#24418;&#26234;&#33021;&#31995;&#32479;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#20154;&#31867;&#22768;&#38899;&#30340;&#24773;&#24863;&#35821;&#20041;&#65292;&#23454;&#29616;&#31867;&#20154;&#23545;&#35805;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#30340;&#23545;&#35805;&#39118;&#26684;&#21487;&#20197;&#36890;&#36807;&#24189;&#40664;&#24863;&#12289;&#20010;&#24615;&#21644;&#35821;&#35843;&#26469;&#34913;&#37327;&#12290;&#36825;&#20123;&#29305;&#24449;&#24050;&#32463;&#25104;&#20026;&#23545;&#35805;&#26234;&#33021;&#34394;&#25311;&#21161;&#25163;&#30340;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;&#26234;&#33021;&#34394;&#25311;&#21161;&#25163;&#65288;IVAs&#65289;&#26080;&#27861;&#35299;&#37322;&#20154;&#31867;&#22768;&#38899;&#30340;&#24773;&#24863;&#35821;&#20041;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20154;&#24418;&#26234;&#33021;&#31995;&#32479;&#65292;&#21487;&#20197;&#36890;&#36807;&#34920;&#36798;&#24773;&#24863;&#21644;&#20010;&#24615;&#26469;&#36827;&#34892;&#31867;&#20154;&#23545;&#35805;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#38899;&#39118;&#26684;&#36716;&#25442;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#29305;&#23450;&#24773;&#24863;&#30340;&#23646;&#24615;&#26144;&#23556;&#20986;&#26469;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#23558;&#26102;&#38388;&#38899;&#39057;&#27874;&#24418;&#25968;&#25454;&#36716;&#25442;&#20026;&#39057;&#22495;&#25968;&#25454;&#65288;Mel-Spectrogram&#65289;&#65292;&#21019;&#24314;&#20102;&#31163;&#25955;&#30340;&#38899;&#39057;&#29305;&#24449;&#27169;&#24335;&#65292;&#22914;&#38899;&#31526;&#12289;&#38899;&#35843;&#12289;&#33410;&#22863;&#12289;&#26059;&#24459;&#31561;&#31561;&#12290;&#24182;&#20351;&#29992;&#19968;&#20010;&#22806;&#37096;CNN-Transformer-Encoder&#27169;&#22411;&#26469;&#39044;&#27979;&#22768;&#38899;&#20013;&#30340;&#19971;&#31181;&#19981;&#21516;&#30340;&#24773;&#24863;&#29366;&#24577;&#12290;&#21516;&#26102;&#65292;&#35813;&#27169;&#22411;&#23558;&#35821;&#38899;&#36755;&#20837;&#21040;&#19968;&#20010;RNN&#27169;&#22411;&#65288;Deep-speech&#65289;&#20013;&#65292;&#29983;&#25104;&#23545;&#38899;&#39057;&#30340;&#25991;&#26412;&#36716;&#24405;&#12290;&#28982;&#21518;&#65292;&#36716;&#24405;&#25991;&#26412;&#23558;&#36890;&#36807;&#19968;&#20010;transformer&#35299;&#30721;&#22120;&#19982;&#30456;&#24212;&#24773;&#24863;&#30340;&#21709;&#24212;&#19968;&#36215;&#20135;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human conversational styles are measured by the sense of humor, personality, and tone of voice. These characteristics have become essential for conversational intelligent virtual assistants. However, most of the state-of-the-art intelligent virtual assistants (IVAs) are failed to interpret the affective semantics of human voices. This research proposes an anthropomorphic intelligent system that can hold a proper human-like conversation with emotion and personality. A voice style transfer method is also proposed to map the attributes of a specific emotion. Initially, the frequency domain data (Mel-Spectrogram) is created by converting the temporal audio wave data, which comprises discrete patterns for audio features such as notes, pitch, rhythm, and melody. A collateral CNN-Transformer-Encoder is used to predict seven different affective states from voice. The voice is also fed parallelly to the deep-speech, an RNN model that generates the text transcription from the spectrogram. Then t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#31163;&#21464;&#20998;&#23545;&#25239;&#35757;&#32451;&#30340;&#39118;&#38505;&#24863;&#30693;&#22411;&#32929;&#31080;&#25512;&#33616;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25239;&#24615;&#25200;&#21160;&#25552;&#39640;&#27169;&#22411;&#23545;&#20110;&#39118;&#38505;&#30340;&#24863;&#30693;&#33021;&#21147;&#65292;&#36890;&#36807;&#21464;&#20998;&#25200;&#21160;&#29983;&#25104;&#22120;&#27169;&#25311;&#19981;&#21516;&#30340;&#39118;&#38505;&#22240;&#32032;&#24182;&#29983;&#25104;&#20195;&#34920;&#24615;&#30340;&#39118;&#38505;&#25351;&#26631;&#23545;&#25239;&#26679;&#26412;&#12290;&#22312;&#30495;&#23454;&#32929;&#31080;&#25968;&#25454;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#26377;&#25928;&#38477;&#20302;&#20102;&#25237;&#36164;&#39118;&#38505;&#21516;&#26102;&#20445;&#25345;&#39640;&#39044;&#26399;&#25910;&#30410;&#12290;</title><link>http://arxiv.org/abs/2304.11043</link><description>&lt;p&gt;
&#25200;&#21160;&#26377;&#21161;&#20110;&#38477;&#20302;&#25237;&#36164;&#39118;&#38505;&#21527;&#65311; &#22522;&#20110;&#20998;&#31163;&#21464;&#20998;&#23545;&#25239;&#35757;&#32451;&#30340;&#39118;&#38505;&#24863;&#30693;&#22411;&#32929;&#31080;&#25512;&#33616;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Can Perturbations Help Reduce Investment Risks? Risk-Aware Stock Recommendation via Split Variational Adversarial Training. (arXiv:2304.11043v1 [q-fin.RM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11043
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#31163;&#21464;&#20998;&#23545;&#25239;&#35757;&#32451;&#30340;&#39118;&#38505;&#24863;&#30693;&#22411;&#32929;&#31080;&#25512;&#33616;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25239;&#24615;&#25200;&#21160;&#25552;&#39640;&#27169;&#22411;&#23545;&#20110;&#39118;&#38505;&#30340;&#24863;&#30693;&#33021;&#21147;&#65292;&#36890;&#36807;&#21464;&#20998;&#25200;&#21160;&#29983;&#25104;&#22120;&#27169;&#25311;&#19981;&#21516;&#30340;&#39118;&#38505;&#22240;&#32032;&#24182;&#29983;&#25104;&#20195;&#34920;&#24615;&#30340;&#39118;&#38505;&#25351;&#26631;&#23545;&#25239;&#26679;&#26412;&#12290;&#22312;&#30495;&#23454;&#32929;&#31080;&#25968;&#25454;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#26377;&#25928;&#38477;&#20302;&#20102;&#25237;&#36164;&#39118;&#38505;&#21516;&#26102;&#20445;&#25345;&#39640;&#39044;&#26399;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32929;&#31080;&#24066;&#22330;&#65292;&#25104;&#21151;&#30340;&#25237;&#36164;&#38656;&#35201;&#22312;&#21033;&#28070;&#21644;&#39118;&#38505;&#20043;&#38388;&#21462;&#24471;&#33391;&#22909;&#30340;&#24179;&#34913;&#12290;&#26368;&#36817;&#65292;&#22312;&#37327;&#21270;&#25237;&#36164;&#20013;&#24191;&#27867;&#30740;&#31350;&#20102;&#32929;&#31080;&#25512;&#33616;&#65292;&#20197;&#20026;&#25237;&#36164;&#32773;&#36873;&#25321;&#20855;&#26377;&#26356;&#39640;&#25910;&#30410;&#29575;&#30340;&#32929;&#31080;&#12290;&#23613;&#31649;&#22312;&#33719;&#21033;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#25512;&#33616;&#26041;&#27861;&#20173;&#28982;&#22312;&#39118;&#38505;&#25511;&#21046;&#26041;&#38754;&#36739;&#24369;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#23454;&#38469;&#32929;&#31080;&#25237;&#36164;&#20013;&#38590;&#20197;&#25215;&#21463;&#30340;&#20111;&#25439;&#12290;&#20026;&#20102;&#26377;&#25928;&#38477;&#20302;&#39118;&#38505;&#65292;&#25105;&#20204;&#20174;&#23545;&#25239;&#24615;&#25200;&#21160;&#20013;&#33719;&#24471;&#21551;&#31034;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20998;&#31163;&#21464;&#20998;&#23545;&#25239;&#35757;&#32451;&#65288;SVAT&#65289;&#26694;&#26550;&#30340;&#39118;&#38505;&#24863;&#30693;&#22411;&#32929;&#31080;&#25512;&#33616;&#26041;&#27861;&#12290;&#26412;&#36136;&#19978;&#65292;SVAT&#40723;&#21169;&#27169;&#22411;&#23545;&#39118;&#38505;&#32929;&#31080;&#26679;&#26412;&#30340;&#23545;&#25239;&#24615;&#25200;&#21160;&#25935;&#24863;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#25200;&#21160;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#39118;&#38505;&#24847;&#35782;&#12290;&#20026;&#20102;&#29983;&#25104;&#20195;&#34920;&#24615;&#30340;&#39118;&#38505;&#25351;&#26631;&#23545;&#25239;&#26679;&#26412;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21464;&#20998;&#25200;&#21160;&#29983;&#25104;&#22120;&#26469;&#27169;&#25311;&#19981;&#21516;&#30340;&#39118;&#38505;&#22240;&#32032;&#12290;&#29305;&#21035;&#22320;&#65292;&#21464;&#20998;&#32467;&#26500;&#20351;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#25429;&#25417;&#38590;&#20197;&#26126;&#30830;&#37327;&#21270;&#21644;&#24314;&#27169;&#30340;&#21508;&#31181;&#39118;&#38505;&#22240;&#32032;&#12290;&#22312;&#30495;&#23454;&#32929;&#31080;&#25968;&#25454;&#19978;&#30340;&#32508;&#21512;&#23454;&#39564;&#34920;&#26126;&#65292;SVAT&#22312;&#38477;&#20302;&#25237;&#36164;&#39118;&#38505;&#30340;&#21516;&#26102;&#20445;&#25345;&#39640;&#39044;&#26399;&#25910;&#30410;&#19978;&#38750;&#24120;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the stock market, a successful investment requires a good balance between profits and risks. Recently, stock recommendation has been widely studied in quantitative investment to select stocks with higher return ratios for investors. Despite the success in making profits, most existing recommendation approaches are still weak in risk control, which may lead to intolerable paper losses in practical stock investing. To effectively reduce risks, we draw inspiration from adversarial perturbations and propose a novel Split Variational Adversarial Training (SVAT) framework for risk-aware stock recommendation. Essentially, SVAT encourages the model to be sensitive to adversarial perturbations of risky stock examples and enhances the model's risk awareness by learning from perturbations. To generate representative adversarial examples as risk indicators, we devise a variational perturbation generator to model diverse risk factors. Particularly, the variational architecture enables our method
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#19981;&#38656;&#35201;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#24212;&#29992;&#20110;&#22522;&#20110;&#29289;&#29702;&#31995;&#32479;&#30340;&#28145;&#24230;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2304.11042</link><description>&lt;p&gt;
&#26080;&#38656;&#21453;&#21521;&#20256;&#25773;&#30340;&#28145;&#24230;&#29289;&#29702;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Backpropagation-free Training of Deep Physical Neural Networks. (arXiv:2304.11042v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11042
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#19981;&#38656;&#35201;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#24212;&#29992;&#20110;&#22522;&#20110;&#29289;&#29702;&#31995;&#32479;&#30340;&#28145;&#24230;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#35832;&#22914;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#26480;&#20986;&#30340;&#25104;&#21151;&#12290;&#36825;&#19968;&#25104;&#21151;&#24456;&#22823;&#31243;&#24230;&#19978;&#24402;&#21151;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#65292;&#39044;&#35745;&#20250;&#19981;&#26029;&#22686;&#21152;&#12290;&#36825;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#22686;&#38271;&#20276;&#38543;&#30528;&#19982;&#20854;&#21487;&#25193;&#23637;&#24615;&#21644;&#35757;&#32451;&#12289;&#25512;&#29702;&#38454;&#27573;&#20013;&#30340;&#33021;&#32791;&#31561;&#38382;&#39064;&#30456;&#20851;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#22522;&#20110;&#38750;&#20256;&#32479;&#29289;&#29702;&#31995;&#32479;&#30340;&#24037;&#20316;&#26469;&#35299;&#20915;&#25512;&#29702;&#38454;&#27573;&#30340;&#33021;&#25928;&#38382;&#39064;&#65292;&#20294;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26377;&#25928;&#35757;&#32451;&#20173;&#26410;&#24471;&#21040;&#35299;&#20915;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#25968;&#23383;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#20027;&#35201;&#20381;&#36182;&#20110;&#21453;&#21521;&#20256;&#25773;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#19981;&#36866;&#29992;&#20110;&#29289;&#29702;&#23454;&#29616;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#23436;&#20840;&#20102;&#35299;&#25152;&#35859;&#21069;&#21521;&#20256;&#36882;&#30340;&#35745;&#31639;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#31616;&#21333;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed the outstanding success of deep learning in various fields such as vision and natural language processing. This success is largely indebted to the massive size of deep learning models that is expected to increase unceasingly. This growth of the deep learning models is accompanied by issues related to their considerable energy consumption, both during the training and inference phases, as well as their scalability. Although a number of work based on unconventional physical systems have been proposed which addresses the issue of energy efficiency in the inference phase, efficient training of deep learning models has remained unaddressed. So far, training of digital deep learning models mainly relies on backpropagation, which is not suitable for physical implementation as it requires perfect knowledge of the computation performed in the so-called forward pass of the neural network. Here, we tackle this issue by proposing a simple deep neural network architectur
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;MATLAB&#20989;&#25968;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#30740;&#31350;&#20154;&#21592;&#21033;&#29992;&#22768;&#38899;&#30340;&#29305;&#24449;&#26469;&#35782;&#21035;&#20154;&#31867;&#19981;&#21516;&#30340;&#24773;&#24863;&#65292;&#25552;&#39640;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.11040</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#21475;&#35821;&#24773;&#24863;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Emotional Expression Detection in Spoken Language Employing Machine Learning Algorithms. (arXiv:2304.11040v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11040
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;MATLAB&#20989;&#25968;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#30740;&#31350;&#20154;&#21592;&#21033;&#29992;&#22768;&#38899;&#30340;&#29305;&#24449;&#26469;&#35782;&#21035;&#20154;&#31867;&#19981;&#21516;&#30340;&#24773;&#24863;&#65292;&#25552;&#39640;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#30340;&#22768;&#38899;&#20855;&#26377;&#22810;&#31181;&#29305;&#24449;&#65292;&#22914;&#38899;&#39640;&#12289;&#38899;&#33394;&#12289;&#38899;&#37327;&#21644;&#21971;&#38899;&#12290;&#36890;&#36807;&#36825;&#20123;&#29305;&#24449;&#21487;&#20197;&#35266;&#23519;&#21040;&#20154;&#20204;&#22312;&#35828;&#35805;&#26102;&#20351;&#29992;&#19981;&#21516;&#30340;&#21971;&#38899;&#36136;&#37327;&#26469;&#34920;&#36798;&#24773;&#24863;&#12290;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#20351;&#29992;&#22810;&#20010;MATLAB&#20989;&#25968;&#65292;&#22914;&#35889;&#25551;&#36848;&#31526;&#12289;&#21608;&#26399;&#24615;&#21644;&#35856;&#27874;&#65292;&#35782;&#21035;&#20154;&#31867;&#30340;&#19981;&#21516;&#24773;&#24863;&#65292;&#22914;&#24868;&#24594;&#12289;&#24754;&#20260;&#12289;&#24656;&#24807;&#12289;&#20013;&#31435;&#12289;&#21388;&#24694;&#12289;&#24778;&#21916;&#21644;&#24555;&#20048;&#12290;&#20026;&#20102;&#23436;&#25104;&#36825;&#39033;&#24037;&#20316;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20154;&#31867;&#35821;&#38899;&#30340;CREMA-D&#65288;&#20247;&#21253;&#24773;&#24863;&#22810;&#27169;&#24577;&#28436;&#21592;&#25968;&#25454;&#65289;&#21644;TESS&#65288;&#22810;&#20262;&#22810;&#24773;&#24863;&#35821;&#38899;&#38598;&#65289;&#25968;&#25454;&#38598;&#12290;&#38899;&#39057;&#25991;&#20214;&#21253;&#21547;&#20855;&#26377;&#21508;&#31181;&#29305;&#24449;&#65288;&#22914;&#22024;&#26434;&#12289;&#24555;&#36895;&#12289;&#32531;&#24930;&#65289;&#30340;&#25968;&#25454;&#65292;&#22240;&#27492;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25928;&#29575;&#26174;&#33879;&#25552;&#39640;&#12290;&#21033;&#29992;&#32463;&#39564;&#27169;&#24577;&#20998;&#35299;&#65288;EMD&#65289;&#36827;&#34892;&#20449;&#21495;&#20998;&#35299;&#30340;&#36807;&#31243;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#20351;&#29992;&#20960;&#20010;
&lt;/p&gt;
&lt;p&gt;
There are a variety of features of the human voice that can be classified as pitch, timbre, loudness, and vocal tone. It is observed in numerous incidents that human expresses their feelings using different vocal qualities when they are speaking. The primary objective of this research is to recognize different emotions of human beings such as anger, sadness, fear, neutrality, disgust, pleasant surprise, and happiness by using several MATLAB functions namely, spectral descriptors, periodicity, and harmonicity. To accomplish the work, we analyze the CREMA-D (Crowd-sourced Emotional Multimodal Actors Data) &amp; TESS (Toronto Emotional Speech Set) datasets of human speech. The audio file contains data that have various characteristics (e.g., noisy, speedy, slow) thereby the efficiency of the ML (Machine Learning) models increases significantly. The EMD (Empirical Mode Decomposition) is utilized for the process of signal decomposition. Then, the features are extracted through the use of severa
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FARM&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26377;&#25928;&#22788;&#29702;&#23454;&#26102;&#25968;&#25454;&#27969;&#24182;&#25552;&#20379;&#24179;&#34913;&#30340;&#30456;&#20851;&#24615;&#24230;&#37327;&#65292;&#36827;&#32780;&#30830;&#23450;&#22806;&#37096;&#25968;&#25454;&#22312;&#39044;&#27979;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.11028</link><description>&lt;p&gt;
&#39044;&#27979;&#20013;&#30340;&#22806;&#22312;&#25968;&#25454;&#65306;&#19968;&#31181;&#29992;&#20110;&#20851;&#32852;&#35780;&#20272;&#30340;FARM&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Exogenous Data in Forecasting: FARM -- An Approach for Relevance Evaluation. (arXiv:2304.11028v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11028
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FARM&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26377;&#25928;&#22788;&#29702;&#23454;&#26102;&#25968;&#25454;&#27969;&#24182;&#25552;&#20379;&#24179;&#34913;&#30340;&#30456;&#20851;&#24615;&#24230;&#37327;&#65292;&#36827;&#32780;&#30830;&#23450;&#22806;&#37096;&#25968;&#25454;&#22312;&#39044;&#27979;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22806;&#22312;&#25968;&#25454;&#34987;&#35748;&#20026;&#22312;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#38024;&#23545;&#24688;&#24403;&#30340;&#36873;&#25321;&#65292;&#20840;&#38754;&#30340;&#30456;&#20851;&#24615;&#20998;&#26512;&#26159;&#19968;&#20010;&#22522;&#26412;&#30340;&#31532;&#19968;&#27493;&#65292;&#20174;&#22806;&#22312;&#25968;&#25454;&#19982;&#21442;&#32771;&#26102;&#38388;&#24207;&#21015;&#30340;&#30456;&#20284;&#24615;&#24320;&#22987;&#12290;&#21463;&#29616;&#26377;&#26102;&#38388;&#24207;&#21015;&#30456;&#20284;&#24615;&#25351;&#26631;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FARM&#65288;&#21069;&#21521;&#35282;&#30456;&#20851;&#24230;&#37327;&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#23454;&#26102;&#25968;&#25454;&#27969;&#12290;&#25105;&#20204;&#30340;&#21069;&#21521;&#26041;&#27861;&#20381;&#36182;&#20110;&#19968;&#31181;&#35282;&#24230;&#29305;&#24449;&#65292;&#35813;&#29305;&#24449;&#21033;&#29992;&#21518;&#32493;&#25968;&#25454;&#28857;&#30340;&#21464;&#21270;&#27604;&#36739;&#26469;&#23545;&#40784;&#32463;&#36807;&#26102;&#38388;&#21464;&#24418;&#30340;&#24207;&#21015;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#32467;&#21512;&#20102;&#26412;&#22320;&#21644;&#20840;&#23616;&#25351;&#26631;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#24179;&#34913;&#30340;&#30456;&#20851;&#24615;&#24230;&#37327;&#12290;&#36825;&#23548;&#33268;&#23558;&#37096;&#20998;&#12289;&#20013;&#38388;&#21305;&#37197;&#20063;&#35270;&#20026;&#22806;&#22312;&#25968;&#25454;&#24207;&#21015;&#37325;&#35201;&#25351;&#26631;&#30340;&#32771;&#34385;&#22240;&#32032;&#12290;&#20316;&#20026;&#31532;&#19968;&#27493;&#39564;&#35777;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#25105;&#20204;&#30340;FARM&#26041;&#27861;&#23545;&#21512;&#25104;&#20294;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;&#20449;&#21495;&#21644;&#30495;&#23454;&#19990;&#30028;&#26102;&#38388;&#24207;&#21015;&#35760;&#24405;&#30340;&#24212;&#29992;&#12290;&#21516;&#26102;&#23637;&#31034;&#20102;FARM&#26041;&#27861;&#25552;&#39640;&#20102;&#39044;&#27979;&#20934;&#30830;&#24230;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exogenous data is believed to play a key role for increasing forecasting accuracy. For an appropriate selection, a throughout relevance analysis is a fundamental first step, starting from the exogenous data similarity with the reference time series. Inspired by existing metrics for time series similarity, we introduce a new approach named FARM - Forward Angular Relevance Measure, able to effectively deal with real-time data streams. Our forward method relies on an angular feature that compares changes in subsequent data points to align time-warped series in an efficient way. The proposed algorithm combines local and global measures to provide a balanced relevance measure. This results in considering also partial, intermediate matches as relevant indicators for exogenous data series significance. As a first validation step, we present the application of our FARM approach to both synthetic but representative signals and real-world time series recordings. While demonstrating the improved 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;SAL&#21644;SCoreBO&#20004;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#30340;&#36229;&#21442;&#25968;&#36873;&#25321;&#21644;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.11005</link><description>&lt;p&gt;
&#36890;&#36807;&#36125;&#21494;&#26031;&#20027;&#21160;&#23398;&#20064;&#23454;&#29616;&#33258;&#26657;&#27491;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Self-Correcting Bayesian Optimization through Bayesian Active Learning. (arXiv:2304.11005v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11005
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;SAL&#21644;SCoreBO&#20004;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#30340;&#36229;&#21442;&#25968;&#36873;&#25321;&#21644;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#24050;&#25104;&#20026;&#36125;&#21494;&#26031;&#20248;&#21270;&#21644;&#20027;&#21160;&#23398;&#20064;&#20013;&#30340;&#39318;&#36873;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#39640;&#26031;&#36807;&#31243;&#30340;&#23436;&#20840;&#21457;&#25381;&#38656;&#35201;&#24039;&#22937;&#36873;&#25321;&#36229;&#21442;&#25968;&#65292;&#32780;&#22312;&#25991;&#29486;&#20013;&#24456;&#23569;&#26377;&#20851;&#20110;&#25214;&#21040;&#27491;&#30830;&#36229;&#21442;&#25968;&#30340;&#21162;&#21147;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#36873;&#25321;&#22909;&#30340;&#36229;&#21442;&#25968;&#23545;&#20110;&#39640;&#26031;&#36807;&#31243;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#26126;&#30830;&#20248;&#20808;&#32771;&#34385;&#27492;&#30446;&#26631;&#30340;&#25910;&#36141;&#20989;&#25968;&#12290;&#32479;&#35745;&#36317;&#31163;&#20027;&#21160;&#23398;&#20064;&#65288;SAL&#65289;&#32771;&#34385;&#21518;&#39564;&#26679;&#26412;&#30340;&#24179;&#22343;&#19981;&#19968;&#33268;&#24615;&#65292;&#30001;&#32479;&#35745;&#36317;&#31163;&#27979;&#37327;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#35768;&#22810;&#27979;&#35797;&#20989;&#25968;&#19978;&#65292;&#23427;&#32988;&#36807;&#20102;&#36125;&#21494;&#26031;&#20027;&#21160;&#23398;&#20064;&#30340;&#26368;&#26032;&#32467;&#26524;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#33258;&#26657;&#27491;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;SCoreBO&#65289;&#65292;&#23427;&#23558;SAL&#25193;&#23637;&#21040;&#21516;&#26102;&#25191;&#34892;&#36125;&#21494;&#26031;&#20248;&#21270;&#21644;&#20027;&#21160;&#36229;&#21442;&#25968;&#23398;&#20064;&#12290;&#30456;&#27604;&#20256;&#32479;BO&#65292;SCoreBO&#20197;&#25913;&#36827;&#30340;&#36895;&#24230;&#23398;&#20064;&#27169;&#22411;&#36229;&#21442;&#25968;&#65292;&#21516;&#26102;&#22312;&#26368;&#26032;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#25628;&#32034;&#20013;&#21462;&#24471;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gaussian processes are cemented as the model of choice in Bayesian optimization and active learning. Yet, they are severely dependent on cleverly chosen hyperparameters to reach their full potential, and little effort is devoted to finding the right hyperparameters in the literature. We demonstrate the impact of selecting good hyperparameters for GPs and present two acquisition functions that explicitly prioritize this goal. Statistical distance-based Active Learning (SAL) considers the average disagreement among samples from the posterior, as measured by a statistical distance. It is shown to outperform the state-of-the-art in Bayesian active learning on a number of test functions. We then introduce Self-Correcting Bayesian Optimization (SCoreBO), which extends SAL to perform Bayesian optimization and active hyperparameter learning simultaneously. SCoreBO learns the model hyperparameters at improved rates compared to vanilla BO, while outperforming the latest Bayesian optimization met
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#29702;&#24819;&#32852;&#21512;&#20998;&#31867;&#22120;&#20551;&#35774;&#30340;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#21487;&#20197;&#25552;&#20379;&#28165;&#26224;&#20840;&#38754;&#30340;&#29702;&#35299;&#21644;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#29702;&#35770;&#22522;&#30784;&#65292;&#20351;&#24471;&#25945;&#24072;&#21644;&#23398;&#29983;&#32593;&#32476;&#20043;&#38388;&#30340;&#30693;&#35782;&#20256;&#36882;&#26356;&#21152;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2304.11004</link><description>&lt;p&gt;
&#22522;&#20110;&#29702;&#24819;&#32852;&#21512;&#20998;&#31867;&#22120;&#20551;&#35774;&#30340;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Knowledge Distillation Under Ideal Joint Classifier Assumption. (arXiv:2304.11004v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11004
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#29702;&#24819;&#32852;&#21512;&#20998;&#31867;&#22120;&#20551;&#35774;&#30340;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#21487;&#20197;&#25552;&#20379;&#28165;&#26224;&#20840;&#38754;&#30340;&#29702;&#35299;&#21644;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#29702;&#35770;&#22522;&#30784;&#65292;&#20351;&#24471;&#25945;&#24072;&#21644;&#23398;&#29983;&#32593;&#32476;&#20043;&#38388;&#30340;&#30693;&#35782;&#20256;&#36882;&#26356;&#21152;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#26159;&#19968;&#31181;&#23558;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#20026;&#26356;&#39640;&#25928;&#23567;&#22411;&#32593;&#32476;&#30340;&#24378;&#22823;&#25216;&#26415;&#12290;Softmax&#22238;&#24402;&#34920;&#24449;&#23398;&#20064;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#25945;&#24072;&#32593;&#32476;&#26469;&#25351;&#23548;&#26356;&#23567;&#30340;&#23398;&#29983;&#32593;&#32476;&#30340;&#23398;&#20064;&#12290;&#23613;&#31649;&#26377;&#20960;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;Softmax&#22238;&#24402;&#34920;&#24449;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#65292;&#20294;&#25552;&#20379;&#30693;&#35782;&#36716;&#31227;&#30340;&#22522;&#30784;&#26426;&#21046;&#23578;&#19981;&#22815;&#28165;&#26970;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#29702;&#24819;&#32852;&#21512;&#20998;&#31867;&#22120;&#30693;&#35782;&#33976;&#39311;&#65288;IJCKD&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#20026;&#29616;&#26377;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#25552;&#20379;&#28165;&#26224;&#20840;&#38754;&#30340;&#29702;&#35299;&#21644;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#29702;&#35770;&#22522;&#30784;&#12290;&#25105;&#20204;&#20351;&#29992;&#20174;&#39046;&#22495;&#36866;&#24212;&#29702;&#35770;&#25512;&#23548;&#20986;&#30340;&#25968;&#23398;&#25216;&#26415;&#65292;&#25552;&#20379;&#20102;&#23398;&#29983;&#32593;&#32476;&#35823;&#24046;&#30028;&#30340;&#35814;&#32454;&#20998;&#26512;&#65292;&#20854;&#20316;&#20026;&#25945;&#24072;&#30340;&#20989;&#25968;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#24212;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#65292;&#21253;&#25324;&#22270;&#20687;&#35782;&#21035;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge distillation is a powerful technique to compress large neural networks into smaller, more efficient networks. Softmax regression representation learning is a popular approach that uses a pre-trained teacher network to guide the learning of a smaller student network. While several studies explored the effectiveness of softmax regression representation learning, the underlying mechanism that provides knowledge transfer is not well understood. This paper presents Ideal Joint Classifier Knowledge Distillation (IJCKD), a unified framework that provides a clear and comprehensive understanding of the existing knowledge distillation methods and a theoretical foundation for future research. Using mathematical techniques derived from a theory of domain adaptation, we provide a detailed analysis of the student network's error bound as a function of the teacher. Our framework enables efficient knowledge transfer between teacher and student networks and can be applied to various applicati
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#24179;&#34913;&#25216;&#26415;&#25193;&#23637;&#21040;&#21518;&#39564;&#23494;&#24230;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#31070;&#32463;&#21518;&#39564;&#20272;&#35745;&#21644;&#23545;&#27604;&#31070;&#32463;&#27604;&#29575;&#20272;&#35745;&#30340;&#24179;&#34913;&#29256;&#26412;&#65292;&#21487;&#26377;&#25928;&#32531;&#35299;&#20445;&#23432;&#25512;&#26029;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.10978</link><description>&lt;p&gt;
&#24179;&#34913;&#27169;&#25311;&#25512;&#26029;&#65292;&#24471;&#21040;&#20445;&#23432;&#30340;&#21518;&#39564;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
Balancing Simulation-based Inference for Conservative Posteriors. (arXiv:2304.10978v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10978
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#24179;&#34913;&#25216;&#26415;&#25193;&#23637;&#21040;&#21518;&#39564;&#23494;&#24230;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#31070;&#32463;&#21518;&#39564;&#20272;&#35745;&#21644;&#23545;&#27604;&#31070;&#32463;&#27604;&#29575;&#20272;&#35745;&#30340;&#24179;&#34913;&#29256;&#26412;&#65292;&#21487;&#26377;&#25928;&#32531;&#35299;&#20445;&#23432;&#25512;&#26029;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20445;&#23432;&#25512;&#26029;&#26159;&#27169;&#25311;&#25512;&#26029;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#24050;&#32463;&#35777;&#26126;&#24120;&#29992;&#31639;&#27861;&#21487;&#33021;&#20250;&#20135;&#29983;&#36807;&#20110;&#33258;&#20449;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#24179;&#34913;&#21487;&#20197;&#26377;&#25928;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#20854;&#24212;&#29992;&#20173;&#38480;&#20110;&#31070;&#32463;&#27604;&#29575;&#20272;&#35745;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#24179;&#34913;&#25193;&#23637;&#21040;&#25552;&#20379;&#21518;&#39564;&#23494;&#24230;&#30340;&#20219;&#20309;&#31639;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31070;&#32463;&#21518;&#39564;&#20272;&#35745;&#21644;&#23545;&#27604;&#31070;&#32463;&#27604;&#29575;&#20272;&#35745;&#30340;&#24179;&#34913;&#29256;&#26412;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#24179;&#34913;&#29256;&#26412;&#20542;&#21521;&#20110;&#22312;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#19978;&#20135;&#29983;&#20445;&#23432;&#30340;&#21518;&#39564;&#20998;&#24067;&#36924;&#36817;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#24179;&#34913;&#26465;&#20214;&#30340;&#21478;&#19968;&#31181;&#35299;&#37322;&#65292;&#21363;$ \chi^2$ &#38548;&#31163;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conservative inference is a major concern in simulation-based inference. It has been shown that commonly used algorithms can produce overconfident posterior approximations. Balancing has empirically proven to be an effective way to mitigate this issue. However, its application remains limited to neural ratio estimation. In this work, we extend balancing to any algorithm that provides a posterior density. In particular, we introduce a balanced version of both neural posterior estimation and contrastive neural ratio estimation. We show empirically that the balanced versions tend to produce conservative posterior approximations on a wide variety of benchmarks. In addition, we provide an alternative interpretation of the balancing condition in terms of the $\chi^2$ divergence.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LEIA&#30340;&#24773;&#24863;&#35782;&#21035;&#27169;&#22411;&#65292;&#20351;&#29992;&#20102;&#30001;&#36229;&#36807;6&#30334;&#19975;&#20010;&#33258;&#27880;&#37322;&#25991;&#26412;&#24086;&#23376;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#21033;&#29992;&#25513;&#34109;&#21333;&#35789;&#30340;&#26041;&#27861;&#22686;&#24378;&#27169;&#22411;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#24773;&#24863;&#21333;&#35789;&#30340;&#23398;&#20064;&#65292;&#24182;&#22312;&#19977;&#20010;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#32422;73&#30340;&#23439;F1&#20540;&#65292;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.10973</link><description>&lt;p&gt;
LEIA&#65306;&#35821;&#35328;&#23884;&#20837;&#29992;&#20110;&#24773;&#24863;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
LEIA: Linguistic Embeddings for the Identification of Affect. (arXiv:2304.10973v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10973
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LEIA&#30340;&#24773;&#24863;&#35782;&#21035;&#27169;&#22411;&#65292;&#20351;&#29992;&#20102;&#30001;&#36229;&#36807;6&#30334;&#19975;&#20010;&#33258;&#27880;&#37322;&#25991;&#26412;&#24086;&#23376;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#21033;&#29992;&#25513;&#34109;&#21333;&#35789;&#30340;&#26041;&#27861;&#22686;&#24378;&#27169;&#22411;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#24773;&#24863;&#21333;&#35789;&#30340;&#23398;&#20064;&#65292;&#24182;&#22312;&#19977;&#20010;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#32422;73&#30340;&#23439;F1&#20540;&#65292;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#20135;&#29983;&#20102;&#22823;&#37327;&#25991;&#26412;&#25968;&#25454;&#65292;&#20351;&#24471;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20998;&#26512;&#24773;&#24863;&#21464;&#24471;&#26356;&#21152;&#23481;&#26131;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#22312;&#30001;&#35835;&#32773;&#29983;&#25104;&#30340;&#23567;&#22411;&#32780;&#26114;&#36149;&#30340;&#25991;&#26412;&#27880;&#37322;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#20123;&#35835;&#32773;&#29468;&#27979;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#20013;&#34920;&#36798;&#30340;&#24773;&#24863;&#12290;&#36825;&#24433;&#21709;&#20102;&#24773;&#24863;&#35782;&#21035;&#26041;&#27861;&#30340;&#36136;&#37327;&#65292;&#22240;&#20026;&#23384;&#22312;&#35757;&#32451;&#25968;&#25454;&#22823;&#23567;&#38480;&#21046;&#21644;&#29992;&#20110;&#27169;&#22411;&#24320;&#21457;&#30340;&#26631;&#31614;&#29983;&#20135;&#20013;&#30340;&#22122;&#22768;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LEIA&#65292;&#36825;&#26159;&#19968;&#31181;&#25991;&#26412;&#24773;&#24863;&#35782;&#21035;&#27169;&#22411;&#65292;&#23427;&#22522;&#20110;&#30001;&#36229;&#36807;6&#30334;&#19975;&#20010;&#24086;&#23376;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#20854;&#20013;&#36825;&#20123;&#24086;&#23376;&#20855;&#26377;&#33258;&#27880;&#37322;&#30340;&#24773;&#24863;&#26631;&#31614;&#65292;&#21253;&#25324;&#24555;&#20048;&#12289;&#20146;&#24773;&#12289;&#24754;&#20260;&#12289;&#24868;&#24594;&#21644;&#24656;&#24807;&#12290;LEIA&#22522;&#20110;&#19968;&#31181;&#25513;&#34109;&#21333;&#35789;&#30340;&#26041;&#27861;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#24773;&#24863;&#21333;&#35789;&#30340;&#23398;&#20064;&#12290;LEIA&#22312;&#19977;&#20010;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#32422;73&#30340;&#23439;F1&#20540;&#65292;&#20248;&#20110;&#20854;&#20182;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#24182;&#22312;&#24378;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;LEIA&#21487;&#20197;&#27010;&#25324;&#19981;&#21516;&#30340;&#24086;&#23376;&#12289;&#29992;&#25143;&#21644;&#26102;&#38388;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
The wealth of text data generated by social media has enabled new kinds of analysis of emotions with language models. These models are often trained on small and costly datasets of text annotations produced by readers who guess the emotions expressed by others in social media posts. This affects the quality of emotion identification methods due to training data size limitations and noise in the production of labels used in model development. We present LEIA, a model for emotion identification in text that has been trained on a dataset of more than 6 million posts with self-annotated emotion labels for happiness, affection, sadness, anger, and fear. LEIA is based on a word masking method that enhances the learning of emotion words during model pre-training. LEIA achieves macro-F1 values of approximately 73 on three in-domain test datasets, outperforming other supervised and unsupervised methods in a strong benchmark that shows that LEIA generalizes across posts, users, and time periods.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Tucker&#20998;&#35299;&#27169;&#22411;&#30340;&#20132;&#36890;&#36895;&#24230;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;PID&#25511;&#21046;&#22120;&#23454;&#29616;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#26356;&#22909;&#30340;&#24674;&#22797;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.10961</link><description>&lt;p&gt;
&#22522;&#20110;&#19981;&#23436;&#25972;&#24352;&#37327;Tucker&#20998;&#35299;&#30340;&#20132;&#36890;&#36895;&#24230;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Incomplete Tensor Tucker decomposition based Traffic Speed Prediction Method. (arXiv:2304.10961v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Tucker&#20998;&#35299;&#27169;&#22411;&#30340;&#20132;&#36890;&#36895;&#24230;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;PID&#25511;&#21046;&#22120;&#23454;&#29616;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#26356;&#22909;&#30340;&#24674;&#22797;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20013;&#65292;&#32570;&#22833;&#25968;&#25454;&#26159;&#27604;&#36739;&#24120;&#35265;&#21644;&#19981;&#21487;&#36991;&#20813;&#30340;&#24773;&#20917;&#12290;&#32780;&#23436;&#25972;&#21644;&#26377;&#25928;&#30340;&#20132;&#36890;&#36895;&#24230;&#25968;&#25454;&#23545;&#20110;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#38750;&#24120;&#37325;&#35201;&#12290;&#28508;&#21464;&#37327;&#20998;&#35299;&#24352;&#37327;&#27169;&#22411;&#26159;&#35299;&#20915;&#32570;&#22833;&#20132;&#36890;&#25968;&#25454;&#24674;&#22797;&#38382;&#39064;&#26368;&#26377;&#21560;&#24341;&#21147;&#30340;&#26041;&#27861;&#20043;&#19968;&#65292;&#22240;&#20026;&#23427;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#35813;&#27169;&#22411;&#36890;&#24120;&#36890;&#36807;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;(SGD)&#27714;&#35299;&#22120;&#23454;&#29616;&#20248;&#21270;&#65292;&#20294;&#22522;&#20110;SGD&#27714;&#35299;&#30340;&#28508;&#21464;&#37327;&#20998;&#35299;&#24352;&#37327;&#27169;&#22411;&#25910;&#25947;&#36895;&#24230;&#36739;&#24930;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#23558;&#27604;&#20363;&#31215;&#20998;&#24494;&#20998;(PID)&#25511;&#21046;&#22120;&#30340;&#29420;&#29305;&#20248;&#21183;&#34701;&#20837;Tucker&#20998;&#35299;&#30340;&#28508;&#21464;&#37327;&#20998;&#35299;&#24352;&#37327;&#27169;&#22411;&#20013;&#12290;&#23427;&#37319;&#29992;&#20102;&#20004;&#20010;&#24605;&#24819;&#65306;a)&#37319;&#29992;Tucker&#20998;&#35299;&#26500;&#24314;&#19968;&#20010;&#29992;&#20110;&#23454;&#29616;&#26356;&#22909;&#30340;&#24674;&#22797;&#31934;&#24230;&#30340;&#28508;&#21464;&#37327;&#20998;&#35299;&#24352;&#37327;&#27169;&#22411;&#12290;b)&#22522;&#20110;PID&#25511;&#21046;&#29702;&#35770;&#30340;&#35843;&#25972;&#23454;&#20363;&#35823;&#24046;&#34701;&#20837;SGD&#27714;&#35299;&#22120;&#20013;&#65292;&#20174;&#32780;&#26377;&#25928;&#25552;&#39640;&#25910;&#25947;&#36895;&#24230;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20027;&#35201;&#22478;&#24066;&#20132;&#36890;&#36947;&#36335;&#36895;&#24230;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#30740;&#31350;&#34920;&#26126;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;PID&#25511;&#21046;&#22120;Tucker&#20998;&#35299;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In intelligent transport systems, it is common and inevitable with missing data. While complete and valid traffic speed data is of great importance to intelligent transportation systems. A latent factorization-of-tensors (LFT) model is one of the most attractive approaches to solve missing traffic data recovery due to its well-scalability. A LFT model achieves optimization usually via a stochastic gradient descent (SGD) solver, however, the SGD-based LFT suffers from slow convergence. To deal with this issue, this work integrates the unique advantages of the proportional-integral-derivative (PID) controller into a Tucker decomposition based LFT model. It adopts two-fold ideas: a) adopting tucker decomposition to build a LFT model for achieving a better recovery accuracy. b) taking the adjusted instance error based on the PID control theory into the SGD solver to effectively improve convergence rate. Our experimental studies on two major city traffic road speed datasets show that the pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#19977;&#27425;&#27491;&#21017;&#21270;&#31574;&#30053;&#29275;&#39039;&#31639;&#27861;&#65292;&#20854;&#20351;&#29992;&#20284;&#28982;&#27604;&#26041;&#27861;&#24418;&#25104;&#20215;&#20540;&#20989;&#25968;&#26799;&#24230;&#21644;&#40657;&#22622;&#30697;&#38453;&#30340;&#20272;&#35745;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#31639;&#27861;&#25910;&#25947;&#21040;&#20215;&#20540;&#20989;&#25968;&#30340;&#20108;&#38454;&#31283;&#23450;&#28857;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#31867;&#22411;&#20026;&#38797;&#28857;&#30340;&#38519;&#38449;&#12290;</title><link>http://arxiv.org/abs/2304.10951</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19977;&#27425;&#27491;&#21017;&#21270;&#31574;&#30053;&#29275;&#39039;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Cubic-regularized Policy Newton Algorithm for Reinforcement Learning. (arXiv:2304.10951v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#19977;&#27425;&#27491;&#21017;&#21270;&#31574;&#30053;&#29275;&#39039;&#31639;&#27861;&#65292;&#20854;&#20351;&#29992;&#20284;&#28982;&#27604;&#26041;&#27861;&#24418;&#25104;&#20215;&#20540;&#20989;&#25968;&#26799;&#24230;&#21644;&#40657;&#22622;&#30697;&#38453;&#30340;&#20272;&#35745;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#31639;&#27861;&#25910;&#25947;&#21040;&#20215;&#20540;&#20989;&#25968;&#30340;&#20108;&#38454;&#31283;&#23450;&#28857;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#31867;&#22411;&#20026;&#38797;&#28857;&#30340;&#38519;&#38449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#27809;&#26377;&#27169;&#22411;&#20449;&#24687;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#29615;&#22659;&#19979;&#30340;&#25511;&#21046;&#38382;&#39064;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31574;&#30053;&#29275;&#39039;&#31639;&#27861;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#19977;&#27425;&#27491;&#21017;&#21270;&#12290;&#36825;&#20004;&#31181;&#31639;&#27861;&#37319;&#29992;&#20284;&#28982;&#27604;&#26041;&#27861;&#20351;&#29992;&#26679;&#26412;&#36712;&#36857;&#24418;&#25104;&#20215;&#20540;&#20989;&#25968;&#26799;&#24230;&#21644;&#40657;&#22622;&#30697;&#38453;&#30340;&#20272;&#35745;&#12290;&#31532;&#19968;&#31181;&#31639;&#27861;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#38656;&#35201;&#19977;&#27425;&#27491;&#21017;&#21270;&#38382;&#39064;&#30340;&#31934;&#30830;&#35299;&#65292;&#32780;&#31532;&#20108;&#31181;&#31639;&#27861;&#21017;&#20351;&#29992;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26799;&#24230;&#19979;&#38477;&#36817;&#20284;&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#25910;&#25947;&#21040;&#20215;&#20540;&#20989;&#25968;&#30340;&#20108;&#38454;&#31283;&#23450;&#28857;&#65288;SOSP&#65289;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#31867;&#22411;&#20026;&#38797;&#28857;&#30340;&#38519;&#38449;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;$\epsilon$
&lt;/p&gt;
&lt;p&gt;
We consider the problem of control in the setting of reinforcement learning (RL), where model information is not available. Policy gradient algorithms are a popular solution approach for this problem and are usually shown to converge to a stationary point of the value function. In this paper, we propose two policy Newton algorithms that incorporate cubic regularization. Both algorithms employ the likelihood ratio method to form estimates of the gradient and Hessian of the value function using sample trajectories. The first algorithm requires an exact solution of the cubic regularized problem in each iteration, while the second algorithm employs an efficient gradient descent-based approximation to the cubic regularized problem. We establish convergence of our proposed algorithms to a second-order stationary point (SOSP) of the value function, which results in the avoidance of traps in the form of saddle points. In particular, the sample complexity of our algorithms to find an $\epsilon$
&lt;/p&gt;</description></item><item><title>CancerGPT &#26159;&#19968;&#31181;&#22522;&#20110;LLMs&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#25216;&#26415;&#65292;&#21487;&#22312;&#29983;&#29289;&#23398;&#25512;&#26029;&#20013;&#39044;&#27979;&#32597;&#35265;&#32452;&#32455;&#20013;&#30340;&#33647;&#29289;&#23545;&#21327;&#21516;&#20316;&#29992;&#12290;&#23454;&#39564;&#34920;&#26126;&#35813;&#25216;&#26415;&#20934;&#30830;&#24615;&#39640;&#65292;&#21363;&#20351;&#22312;&#26679;&#26412;&#25968;&#25454;&#38750;&#24120;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#20173;&#21487;&#36827;&#34892;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2304.10946</link><description>&lt;p&gt;
CancerGPT: &#22522;&#20110;LLMs&#30340;&#26497;&#23569;&#26679;&#26412;&#33647;&#29289;&#23545;&#21327;&#21516;&#20316;&#29992;&#39044;&#27979;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
CancerGPT: Few-shot Drug Pair Synergy Prediction using Large Pre-trained Language Models. (arXiv:2304.10946v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10946
&lt;/p&gt;
&lt;p&gt;
CancerGPT &#26159;&#19968;&#31181;&#22522;&#20110;LLMs&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#25216;&#26415;&#65292;&#21487;&#22312;&#29983;&#29289;&#23398;&#25512;&#26029;&#20013;&#39044;&#27979;&#32597;&#35265;&#32452;&#32455;&#20013;&#30340;&#33647;&#29289;&#23545;&#21327;&#21516;&#20316;&#29992;&#12290;&#23454;&#39564;&#34920;&#26126;&#35813;&#25216;&#26415;&#20934;&#30830;&#24615;&#39640;&#65292;&#21363;&#20351;&#22312;&#26679;&#26412;&#25968;&#25454;&#38750;&#24120;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#20173;&#21487;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#20855;&#26377;&#26174;&#30528;&#30340;&#36828;&#31243;&#30417;&#25511;&#28508;&#21147;&#65292;&#21363;&#20351;&#21482;&#26377;&#26497;&#23569;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#20294;&#26159;&#65292;&#23427;&#20204;&#22312;&#26356;&#22797;&#26434;&#30340;&#39046;&#22495;&#65292;&#22914;&#29983;&#29289;&#23398;&#39046;&#22495;&#20013;&#23545;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#35780;&#20272;&#12290; LLM&#21487;&#20197;&#25552;&#20379;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#22312;&#32467;&#26500;&#21270;&#25968;&#25454;&#21644;&#26679;&#26412;&#22823;&#23567;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#20174;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#25552;&#21462;&#20808;&#39564;&#30693;&#35782;&#12290; &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLMs&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#32570;&#20047;&#32467;&#26500;&#21270;&#25968;&#25454;&#21644;&#29305;&#24449;&#30340;&#32597;&#35265;&#32452;&#32455;&#20013;&#33647;&#29289;&#23545;&#30340;&#21327;&#21516;&#20316;&#29992;&#12290; &#23454;&#39564;&#28041;&#21450;&#26469;&#33258;&#19981;&#21516;&#30284;&#30151;&#31867;&#22411;&#30340;&#19971;&#31181;&#32597;&#35265;&#32452;&#32455;&#65292;&#34920;&#26126;&#22522;&#20110;LLMs&#30340;&#39044;&#27979;&#27169;&#22411;&#22312;&#38750;&#24120;&#23569;&#25110;&#38646;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#21462;&#24471;&#26174;&#30528;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;CancerGPT&#65288;&#20855;&#26377;$\sim 124M$&#21442;&#25968;&#65289;&#29978;&#33267;&#21487;&#20197;&#19982;&#26356;&#22823;&#30340;&#24494;&#35843;GPT-3&#27169;&#22411;&#65288;&#20855;&#26377;$\sim 175B$&#21442;&#25968;&#65289;&#30456;&#23218;&#32654;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26159;&#31532;&#19968;&#20010;&#21033;&#29992;LLMs&#36827;&#34892;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#26696;&#20363;&#65292;&#20026;&#29983;&#29289;&#23398;&#25512;&#26029;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large pre-trained language models (LLMs) have been shown to have significant potential in few-shot learning across various fields, even with minimal training data. However, their ability to generalize to unseen tasks in more complex fields, such as biology, has yet to be fully evaluated. LLMs can offer a promising alternative approach for biological inference, particularly in cases where structured data and sample size are limited, by extracting prior knowledge from text corpora. Our proposed few-shot learning approach uses LLMs to predict the synergy of drug pairs in rare tissues that lack structured data and features. Our experiments, which involved seven rare tissues from different cancer types, demonstrated that the LLM-based prediction model achieved significant accuracy with very few or zero samples. Our proposed model, the CancerGPT (with $\sim$ 124M parameters), was even comparable to the larger fine-tuned GPT-3 model (with $\sim$ 175B parameters). Our research is the first to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;GATv2&#30340;&#21487;&#35757;&#32451;&#27169;&#22411;&#21442;&#25968;&#30340;&#26799;&#24230;&#36827;&#34892;&#20102;&#20840;&#38754;&#25512;&#23548;&#65292;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#30340;&#24615;&#33021;&#34920;&#29616;&#19981;&#19968;&#33268;&#30340;&#21407;&#22240;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.10939</link><description>&lt;p&gt;
&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#20013;&#21487;&#23398;&#20064;&#21442;&#25968;&#30340;&#26799;&#24230;&#25512;&#23548;
&lt;/p&gt;
&lt;p&gt;
Gradient Derivation for Learnable Parameters in Graph Attention Networks. (arXiv:2304.10939v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10939
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;GATv2&#30340;&#21487;&#35757;&#32451;&#27169;&#22411;&#21442;&#25968;&#30340;&#26799;&#24230;&#36827;&#34892;&#20102;&#20840;&#38754;&#25512;&#23548;&#65292;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#30340;&#24615;&#33021;&#34920;&#29616;&#19981;&#19968;&#33268;&#30340;&#21407;&#22240;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;GAT&#65289;&#30340;&#24191;&#27867;&#23454;&#29616;&#20043;&#19968;&#8212;&#8212;GATv2&#30340;&#21487;&#35757;&#32451;&#27169;&#22411;&#21442;&#25968;&#30340;&#26799;&#24230;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#25512;&#23548;&#12290;&#34429;&#28982;GAT&#24050;&#34987;&#35777;&#26126;&#26159;&#22788;&#29702;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#24378;&#22823;&#26694;&#26550;&#65292;&#20294;&#26159;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#30340;&#24615;&#33021;&#34920;&#29616;&#19981;&#19968;&#33268;&#65292;&#20854;&#21407;&#22240;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#30001;&#20110;&#26799;&#24230;&#27969;&#20026;&#32479;&#35745;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#21160;&#24577;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#27934;&#35265;&#65292;&#22240;&#27492;&#26412;&#25991;&#25512;&#23548;&#20102;GATv2&#30340;&#21487;&#35757;&#32451;&#27169;&#22411;&#21442;&#25968;&#30340;&#26799;&#24230;&#12290;&#36825;&#20123;&#26799;&#24230;&#25512;&#23548;&#34917;&#20805;&#20102;[2]&#30340;&#24037;&#20316;&#65292;&#21518;&#32773;&#35843;&#26597;&#20102;GATv2&#30340;&#28508;&#22312;&#38519;&#38449;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work provides a comprehensive derivation of the parameter gradients for GATv2 [4], a widely used implementation of Graph Attention Networks (GATs). GATs have proven to be powerful frameworks for processing graph-structured data and, hence, have been used in a range of applications. However, the achieved performance by these attempts has been found to be inconsistent across different datasets and the reasons for this remains an open research question. As the gradient flow provides valuable insights into the training dynamics of statistically learning models, this work obtains the gradients for the trainable model parameters of GATv2. The gradient derivations supplement the efforts of [2], where potential pitfalls of GATv2 are investigated.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;CSA&#65288;&#33394;&#24425;&#33258;&#27880;&#24847;&#21147;&#65289;&#65292;&#21487;&#20197;&#23558;&#21407;&#22987;&#36793;&#29305;&#24449;&#23500;&#21270;&#20197;&#21450;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#26041;&#26696;&#26469;&#28789;&#27963;&#30340;&#32534;&#30721;&#22270;&#32467;&#26500;&#65292;&#24182;&#22312;ZINC&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.10933</link><description>&lt;p&gt;
&#33394;&#24425;&#20013;&#30340;&#33258;&#27880;&#24847;&#21147;&#65306;transformer&#20013;&#22270;&#32467;&#26500;&#32534;&#30721;&#30340;&#21478;&#19968;&#31181;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Self-Attention in Colors: Another Take on Encoding Graph Structure in Transformers. (arXiv:2304.10933v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10933
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;CSA&#65288;&#33394;&#24425;&#33258;&#27880;&#24847;&#21147;&#65289;&#65292;&#21487;&#20197;&#23558;&#21407;&#22987;&#36793;&#29305;&#24449;&#23500;&#21270;&#20197;&#21450;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#26041;&#26696;&#26469;&#28789;&#27963;&#30340;&#32534;&#30721;&#22270;&#32467;&#26500;&#65292;&#24182;&#22312;ZINC&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;CSA&#65288;&#33394;&#24425;&#33258;&#27880;&#24847;&#21147;&#65289;&#30340;&#26032;&#22411;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#23558;&#27880;&#24847;&#21147;&#20998;&#25968;&#30340;&#27010;&#24565;&#25193;&#23637;&#21040;&#27880;&#24847;&#21147;&#36807;&#28388;&#22120;&#19978;&#65292;&#29420;&#31435;&#35843;&#21046;&#29305;&#24449;&#36890;&#36947;&#12290;&#25105;&#20204;&#22312;&#23436;&#20840;&#27880;&#24847;&#21147;&#30340;&#22270;&#24418;Transformer CGT&#65288;&#33394;&#24425;&#22270;&#24418;Transformer&#65289;&#20013;&#23637;&#31034;&#20102;CSA&#65292;&#36890;&#36807;&#23500;&#21270;&#21407;&#22987;&#36793;&#29305;&#24449;&#20197;&#21450;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#26041;&#26696;&#65292;&#23436;&#20840;&#32469;&#36807;&#20102;&#26412;&#22320;&#28040;&#24687;&#20256;&#36882;&#32452;&#20214;&#30340;&#38656;&#27714;&#65292;&#36890;&#36807;&#33410;&#28857;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#28789;&#27963;&#22320;&#32534;&#30721;&#22270;&#32467;&#26500;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#28216;&#36208;&#30340;&#26032;&#26041;&#26696;&#65292;&#21487;&#20197;&#32534;&#30721;&#32467;&#26500;&#21644;&#20301;&#32622;&#20449;&#24687;&#65292;&#24182;&#23637;&#31034;&#22914;&#20309;&#32435;&#20837;&#26356;&#39640;&#38454;&#30340;&#25299;&#25169;&#20449;&#24687;&#65292;&#20363;&#22914;&#20998;&#23376;&#22270;&#20013;&#30340;&#29615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;ZINC&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#20026;&#32534;&#30721;&#22270;&#32467;&#26500;&#21644;&#32435;&#20837;&#26356;&#39640;&#38454;&#30340;&#25299;&#25169;&#32467;&#26500;&#25552;&#20379;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a novel self-attention mechanism, which we call CSA (Chromatic Self-Attention), which extends the notion of attention scores to attention _filters_, independently modulating the feature channels. We showcase CSA in a fully-attentional graph Transformer CGT (Chromatic Graph Transformer) which integrates both graph structural information and edge features, completely bypassing the need for local message-passing components. Our method flexibly encodes graph structure through node-node interactions, by enriching the original edge features with a relative positional encoding scheme. We propose a new scheme based on random walks that encodes both structural and positional information, and show how to incorporate higher-order topological information, such as rings in molecular graphs. Our approach achieves state-of-the-art results on the ZINC benchmark dataset, while providing a flexible framework for encoding graph structure and incorporating higher-order topology.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#25554;&#20540;&#21644;&#23383;&#20856;&#23398;&#20064;&#30340;&#27844;&#28431;&#23450;&#20301;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;Modena&#26696;&#20363;&#24471;&#21040;&#20102;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.10932</link><description>&lt;p&gt;
&#22522;&#20110;&#29289;&#29702;&#25554;&#20540;&#30340;&#27700;&#32593;&#27844;&#28431;&#23450;&#20301;&#20013;&#30340;&#23383;&#20856;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning Dictionaries from Physical-Based Interpolation for Water Network Leak Localization. (arXiv:2304.10932v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10932
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#25554;&#20540;&#21644;&#23383;&#20856;&#23398;&#20064;&#30340;&#27844;&#28431;&#23450;&#20301;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;Modena&#26696;&#20363;&#24471;&#21040;&#20102;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;&#20272;&#35745;&#21644;&#23398;&#20064;&#30340;&#27844;&#28431;&#23450;&#20301;&#26041;&#27861;&#12290;&#31532;&#19968;&#20010;&#38454;&#27573;&#30001;&#25554;&#20540;&#26041;&#26696;&#22788;&#29702;&#65292;&#31532;&#20108;&#20010;&#38454;&#27573;&#32771;&#34385;&#23383;&#20856;&#23398;&#20064;&#12290;&#26032;&#25552;&#20986;&#30340;&#25554;&#20540;&#25216;&#26415;&#21033;&#29992;&#20102;&#27700;&#21147;&#36830;&#25509;&#30340;&#29289;&#29702;&#23398;&#21407;&#29702;&#65292;&#36830;&#25509;&#30456;&#37051;&#33410;&#28857;&#30340;&#28082;&#21387;&#22836;&#12290;&#21478;&#22806;&#65292;&#27531;&#24046;&#30452;&#25509;&#34987;&#25554;&#20540;&#32780;&#19981;&#26159;&#28082;&#21387;&#22836;&#20540;&#12290;&#23558;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#19968;&#20010;&#33879;&#21517;&#26696;&#20363;(Modena)&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#26032;&#30340;&#25554;&#20540;&#26041;&#27861;&#22312;&#25554;&#20540;&#35823;&#24046;(&#32771;&#34385;&#29366;&#24577;&#21644;&#27531;&#24046;&#20272;&#35745;)&#21644;&#21518;&#39564;&#23450;&#20301;&#26041;&#38754;&#37117;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article presents a leak localization methodology based on state estimation and learning. The first is handled by an interpolation scheme, whereas dictionary learning is considered for the second stage. The novel proposed interpolation technique exploits the physics of the interconnections between hydraulic heads of neighboring nodes in water distribution networks. Additionally, residuals are directly interpolated instead of hydraulic head values. The results of applying the proposed method to a well-known case study (Modena) demonstrated the improvements of the new interpolation method with respect to a state-of-the-art approach, both in terms of interpolation error (considering state and residual estimation) and posterior localization.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35299;&#20915;&#33258;&#25105;&#30417;&#30563;&#27169;&#22411;&#38519;&#20837;&#22351;&#23616;&#37096;&#26368;&#23567;&#20540;&#30340;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#23558;&#37492;&#21035;&#22120;&#32435;&#20837;&#27169;&#22411;&#65292;&#19981;&#38656;&#35201;&#20154;&#24037;&#24178;&#39044;&#65292;&#24110;&#21161;&#23398;&#20064;&#65292;&#24182;&#35299;&#20915;&#20102;&#24120;&#35265;&#30340;&#23398;&#20064;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.10914</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#23545;&#25239;&#20223;&#30495;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Adversarial Imitation Learning. (arXiv:2304.10914v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10914
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35299;&#20915;&#33258;&#25105;&#30417;&#30563;&#27169;&#22411;&#38519;&#20837;&#22351;&#23616;&#37096;&#26368;&#23567;&#20540;&#30340;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#23558;&#37492;&#21035;&#22120;&#32435;&#20837;&#27169;&#22411;&#65292;&#19981;&#38656;&#35201;&#20154;&#24037;&#24178;&#39044;&#65292;&#24110;&#21161;&#23398;&#20064;&#65292;&#24182;&#35299;&#20915;&#20102;&#24120;&#35265;&#30340;&#23398;&#20064;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34892;&#20026;&#20811;&#38534;&#26159;&#19968;&#31181;&#36890;&#36807;&#19987;&#23478;&#28436;&#31034;&#26469;&#25945;&#25480;&#26234;&#33021;&#20307;&#22914;&#20309;&#34892;&#20026;&#30340;&#20223;&#30495;&#23398;&#20064;&#25216;&#26415;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#20351;&#29992;&#33258;&#25105;&#30417;&#30563;&#30340;&#23436;&#20840;&#21487;&#35266;&#23519;&#26410;&#26631;&#35760;&#29366;&#24577;&#30340;&#24555;&#29031;&#26469;&#23558;&#29366;&#24577;&#23545;&#35299;&#30721;&#20026;&#21160;&#20316;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25216;&#26415;&#37319;&#29992;&#30340;&#36845;&#20195;&#23398;&#20064;&#26041;&#26696;&#23481;&#26131;&#38519;&#20837;&#22351;&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#20351;&#29992;&#30446;&#26631;&#24863;&#30693;&#31574;&#30053;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#38656;&#35201;&#20154;&#24037;&#20171;&#20837;&#26469;&#39564;&#35777;&#26234;&#33021;&#20307;&#26159;&#21542;&#36798;&#21040;&#20102;&#30446;&#26631;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#37492;&#21035;&#22120;&#32435;&#20837;&#21407;&#22987;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25552;&#20379;&#20102;&#20004;&#20010;&#20851;&#38190;&#20248;&#21183;&#65292;&#24182;&#30452;&#25509;&#35299;&#20915;&#20102;&#20197;&#21069;&#30340;&#19968;&#20010;&#23398;&#20064;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#23427;&#19981;&#38656;&#35201;&#20154;&#24037;&#24178;&#39044;&#12290;&#20854;&#27425;&#65292;&#23427;&#36890;&#36807;&#25351;&#23548;&#22522;&#20110;&#19987;&#23478;&#36712;&#36857;&#30340;&#29366;&#24577;&#36716;&#25442;&#30340;&#20989;&#25968;&#36924;&#36817;&#26469;&#24110;&#21161;&#23398;&#20064;&#12290;&#31532;&#19977;&#65292;&#37492;&#21035;&#22120;&#35299;&#20915;&#20102;&#31574;&#30053;&#27169;&#22411;&#24120;&#35265;&#30340;&#23398;&#20064;&#38382;&#39064;&#65292;&#21363;&#26377;&#26102;&#20250;&#25191;&#34892;&#8220;&#26080;&#21160;&#20316;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
Behavioural cloning is an imitation learning technique that teaches an agent how to behave via expert demonstrations. Recent approaches use self-supervision of fully-observable unlabelled snapshots of the states to decode state pairs into actions. However, the iterative learning scheme employed by these techniques is prone to get trapped into bad local minima. Previous work uses goal-aware strategies to solve this issue. However, this requires manual intervention to verify whether an agent has reached its goal. We address this limitation by incorporating a discriminator into the original framework, offering two key advantages and directly solving a learning problem previous work had. First, it disposes of the manual intervention requirement. Second, it helps in learning by guiding function approximation based on the state transition of the expert's trajectories. Third, the discriminator solves a learning issue commonly present in the policy model, which is to sometimes perform a `no ac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#22312;MIMIC-III&#21644;MIMIC-IV&#19978;&#36827;&#34892;&#33258;&#21160;&#21270;&#21307;&#30103;&#32534;&#30721;&#30340;&#26368;&#26032;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#21644;&#19981;&#36275;&#20043;&#22788;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26356;&#22909;&#22320;&#35780;&#20272;&#31995;&#32479;&#24615;&#33021;&#65292;&#24182;&#20844;&#24320;&#20102;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2304.10909</link><description>&lt;p&gt;
MIMIC-III&#21644;MIMIC-IV&#19978;&#30340;&#33258;&#21160;&#21270;&#21307;&#30103;&#32534;&#30721;&#65306;&#19968;&#39033;&#20851;&#38190;&#22238;&#39038;&#21644;&#21487;&#22797;&#21046;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Automated Medical Coding on MIMIC-III and MIMIC-IV: A Critical Review and Replicability Study. (arXiv:2304.10909v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10909
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#22312;MIMIC-III&#21644;MIMIC-IV&#19978;&#36827;&#34892;&#33258;&#21160;&#21270;&#21307;&#30103;&#32534;&#30721;&#30340;&#26368;&#26032;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#21644;&#19981;&#36275;&#20043;&#22788;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26356;&#22909;&#22320;&#35780;&#20272;&#31995;&#32479;&#24615;&#33021;&#65292;&#24182;&#20844;&#24320;&#20102;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#30103;&#32534;&#30721;&#26159;&#23558;&#21307;&#23398;&#20195;&#30721;&#20998;&#37197;&#32473;&#20020;&#24202;&#33258;&#30001;&#25991;&#26723;&#30340;&#20219;&#21153;&#12290;&#21307;&#30103;&#19987;&#19994;&#20154;&#22763;&#25163;&#21160;&#20998;&#37197;&#36825;&#20123;&#20195;&#30721;&#20197;&#36319;&#36394;&#24739;&#32773;&#30340;&#35786;&#26029;&#21644;&#27835;&#30103;&#12290;&#33258;&#21160;&#21270;&#21307;&#30103;&#32534;&#30721;&#21487;&#20197;&#26497;&#22823;&#22320;&#20943;&#36731;&#36825;&#31181;&#34892;&#25919;&#36127;&#25285;&#12290;&#26412;&#25991;&#37325;&#29616;&#12289;&#27604;&#36739;&#21644;&#20998;&#26512;&#20102;&#26368;&#20808;&#36827;&#30340;&#33258;&#21160;&#21270;&#21307;&#30103;&#32534;&#30721;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#26174;&#31034;&#20986;&#22810;&#20010;&#27169;&#22411;&#34920;&#29616;&#19981;&#20339;&#65292;&#21407;&#22240;&#26159;&#37197;&#32622;&#24369;&#12289;&#35757;&#32451;-&#27979;&#35797;&#25286;&#20998;&#26679;&#26412;&#19981;&#36275;&#20197;&#21450;&#35780;&#20272;&#19981;&#20805;&#20998;&#12290;&#22312;&#20197;&#24448;&#30340;&#24037;&#20316;&#20013;&#65292;&#23439;&#24179;&#22343;F1&#20998;&#25968;&#34987;&#35745;&#31639;&#20986;&#20122;&#20248;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#25105;&#20204;&#30340;&#20462;&#27491;&#20351;&#20854;&#32763;&#20493;&#12290;&#25105;&#20204;&#37319;&#29992;&#20998;&#23618;&#25277;&#26679;&#21644;&#30456;&#21516;&#30340;&#23454;&#39564;&#35774;&#32622;&#36827;&#34892;&#20102;&#20462;&#35746;&#30340;&#27169;&#22411;&#27604;&#36739;&#65292;&#21253;&#25324;&#36229;&#21442;&#25968;&#21644;&#20915;&#31574;&#36793;&#30028;&#35843;&#25972;&#12290;&#25105;&#20204;&#20998;&#26512;&#39044;&#27979;&#35823;&#24046;&#26469;&#39564;&#35777;&#21644;&#35777;&#20266;&#20197;&#21069;&#30340;&#24037;&#20316;&#20551;&#35774;&#12290;&#20998;&#26512;&#35777;&#23454;&#65292;&#25152;&#26377;&#27169;&#22411;&#37117;&#38590;&#20197;&#22788;&#29702;&#31232;&#26377;&#30340;&#20195;&#30721;&#65292;&#32780;&#38271;&#25991;&#26723;&#20165;&#23545;&#32467;&#26524;&#26377;&#24494;&#19981;&#36275;&#36947;&#30340;&#24433;&#21709;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27969;&#34892;&#30149;&#23398;&#37319;&#26679;&#30340;&#25913;&#36827;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26356;&#22909;&#22320;&#35780;&#20272;&#31995;&#32479;&#30340;&#24615;&#33021;&#65292;&#24182;&#20844;&#24320;&#20102;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical coding is the task of assigning medical codes to clinical free-text documentation. Healthcare professionals manually assign such codes to track patient diagnoses and treatments. Automated medical coding can considerably alleviate this administrative burden. In this paper, we reproduce, compare, and analyze state-of-the-art automated medical coding machine learning models. We show that several models underperform due to weak configurations, poorly sampled train-test splits, and insufficient evaluation. In previous work, the macro F1 score has been calculated sub-optimally, and our correction doubles it. We contribute a revised model comparison using stratified sampling and identical experimental setups, including hyperparameters and decision boundary tuning. We analyze prediction errors to validate and falsify assumptions of previous works. The analysis confirms that all models struggle with rare codes, while long documents only have a negligible impact. Finally, we present the 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#39640;&#25928;&#30340;&#21435;&#20013;&#24515;&#21270;&#21160;&#37327;&#27861;&#65288;DM-GDA&#65289;&#65292;&#29992;&#20110;&#20998;&#24067;&#24335;&#38750;&#20984;PL&#26497;&#23567;&#21270;&#26497;&#23567;&#20248;&#21270;&#65292;&#33021;&#22815;&#21516;&#26102;&#20351;&#29992;&#21160;&#37327;&#21644;&#38543;&#26426;&#26799;&#24230;&#20272;&#35745;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#20998;&#24067;&#24335;&#26497;&#23567;&#26368;&#22823;&#21270;&#20248;&#21270;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#20351;&#29992;&#21463;&#38480;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.10902</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;PL&#38750;&#20984;&#26368;&#23567;&#21270;&#26497;&#23567;&#38382;&#39064;&#30340;&#36817;&#20284;&#26368;&#20248;&#21435;&#20013;&#24515;&#21270;&#21160;&#37327;&#27861;
&lt;/p&gt;
&lt;p&gt;
Near-Optimal Decentralized Momentum Method for Nonconvex-PL Minimax Problems. (arXiv:2304.10902v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10902
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#39640;&#25928;&#30340;&#21435;&#20013;&#24515;&#21270;&#21160;&#37327;&#27861;&#65288;DM-GDA&#65289;&#65292;&#29992;&#20110;&#20998;&#24067;&#24335;&#38750;&#20984;PL&#26497;&#23567;&#21270;&#26497;&#23567;&#20248;&#21270;&#65292;&#33021;&#22815;&#21516;&#26102;&#20351;&#29992;&#21160;&#37327;&#21644;&#38543;&#26426;&#26799;&#24230;&#20272;&#35745;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#20998;&#24067;&#24335;&#26497;&#23567;&#26368;&#22823;&#21270;&#20248;&#21270;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#20351;&#29992;&#21463;&#38480;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#23567;&#26368;&#22823;&#21270;&#20248;&#21270;&#22312;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#65292;&#22914;&#29983;&#25104;&#24335;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#21644;&#23545;&#25239;&#24615;&#35757;&#32451;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#21435;&#20013;&#24515;&#21270;&#21160;&#37327;&#26799;&#24230;&#19979;&#38477;&#19978;&#21319;&#27861;&#65288;DM-GDA&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#24067;&#24335;&#38750;&#20984;PL&#26497;&#23567;&#21270;&#26497;&#23567;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Minimax optimization plays an important role in many machine learning tasks such as generative adversarial networks (GANs) and adversarial training. Although recently a wide variety of optimization methods have been proposed to solve the minimax problems, most of them ignore the distributed setting where the data is distributed on multiple workers. Meanwhile, the existing decentralized minimax optimization methods rely on the strictly assumptions such as (strongly) concavity and variational inequality conditions. In the paper, thus, we propose an efficient decentralized momentum-based gradient descent ascent (DM-GDA) method for the distributed nonconvex-PL minimax optimization, which is nonconvex in primal variable and is nonconcave in dual variable and satisfies the Polyak-Lojasiewicz (PL) condition. In particular, our DM-GDA method simultaneously uses the momentum-based techniques to update variables and estimate the stochastic gradients. Moreover, we provide a solid convergence anal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;GNN&#26550;&#26500;GCNH&#65292;&#36866;&#29992;&#20110;&#24322;&#26500;&#21644;&#21516;&#36136;&#24773;&#20917;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#37325;&#35201;&#31995;&#25968;&#26469;&#24179;&#34913;&#20013;&#24515;&#33410;&#28857;&#21644;&#37051;&#22495;&#30340;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2304.10896</link><description>&lt;p&gt;
GCNH&#65306;&#19968;&#31181;&#29992;&#20110;&#24322;&#26500;&#22270;&#19978;&#34920;&#31034;&#23398;&#20064;&#30340;&#31616;&#21333;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
GCNH: A Simple Method For Representation Learning On Heterophilous Graphs. (arXiv:2304.10896v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10896
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;GNN&#26550;&#26500;GCNH&#65292;&#36866;&#29992;&#20110;&#24322;&#26500;&#21644;&#21516;&#36136;&#24773;&#20917;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#37325;&#35201;&#31995;&#25968;&#26469;&#24179;&#34913;&#20013;&#24515;&#33410;&#28857;&#21644;&#37051;&#22495;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24456;&#36866;&#21512;&#22312;&#21516;&#36136;&#22270;&#19978;&#23398;&#20064;&#65292;&#21363;&#22312;&#36825;&#31181;&#22270;&#20013;&#65292;&#36793;&#24448;&#24448;&#36830;&#25509;&#21516;&#19968;&#31867;&#22411;&#30340;&#33410;&#28857;&#12290;&#28982;&#32780;&#65292;&#22312;&#24322;&#26500;&#22270;&#19978;&#23454;&#29616;&#19968;&#33268;&#30340;GNN&#24615;&#33021;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#25193;&#23637;&#26631;&#20934;GNN&#26550;&#26500;&#30340;&#26041;&#27861;&#65292;&#20197;&#25913;&#21892;&#22312;&#24322;&#26500;&#22270;&#19978;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;&#36825;&#20123;&#27169;&#22411;&#26080;&#27861;&#25429;&#25417;&#21040;&#22522;&#26412;&#30340;&#22270;&#24418;&#23646;&#24615;&#65292;&#20363;&#22914;&#37051;&#23621;&#26631;&#31614;&#20998;&#24067;&#65292;&#36825;&#26159;&#23398;&#20064;&#30340;&#22522;&#30784;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GCN for Heterophily&#65288;GCNH&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;GNN&#26550;&#26500;&#65292;&#36866;&#29992;&#20110;&#24322;&#26500;&#21644;&#21516;&#36136;&#24773;&#20917;&#12290;GCNH&#23398;&#20064;&#24182;&#32452;&#21512;&#20102;&#33410;&#28857;&#21644;&#20854;&#37051;&#23621;&#30340;&#20998;&#31163;&#34920;&#31034;&#65292;&#20351;&#29992;&#27599;&#23618;&#23398;&#20064;&#30340;&#19968;&#20010;&#37325;&#35201;&#31995;&#25968;&#26469;&#24179;&#34913;&#20013;&#24515;&#33410;&#28857;&#21644;&#37051;&#22495;&#30340;&#36129;&#29486;&#12290;&#25105;&#20204;&#22312;&#20843;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#22270;&#21644;&#19968;&#32452;&#20154;&#36896;&#22270;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are well-suited for learning on homophilous graphs, i.e., graphs in which edges tend to connect nodes of the same type. Yet, achievement of consistent GNN performance on heterophilous graphs remains an open research problem. Recent works have proposed extensions to standard GNN architectures to improve performance on heterophilous graphs, trading off model simplicity for prediction accuracy. However, these models fail to capture basic graph properties, such as neighborhood label distribution, which are fundamental for learning. In this work, we propose GCN for Heterophily (GCNH), a simple yet effective GNN architecture applicable to both heterophilous and homophilous scenarios. GCNH learns and combines separate representations for a node and its neighbors, using one learned importance coefficient per layer to balance the contributions of center nodes and neighborhoods. We conduct extensive experiments on eight real-world graphs and a set of synthetic graphs
&lt;/p&gt;</description></item><item><title>InfAdapter&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#39640;&#20934;&#30830;&#24615;&#12289;&#20302;&#24310;&#36831;&#21644;&#25104;&#26412;&#25928;&#30410;&#20043;&#38388;&#26435;&#34913;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20027;&#21160;&#36873;&#25321;&#19968;&#32452;&#24102;&#26377;&#36164;&#28304;&#20998;&#37197;&#30340; ML &#27169;&#22411;&#21464;&#20307;&#26469;&#28385;&#36275;&#24310;&#36831; SLO&#65292;&#24182;&#26368;&#22823;&#21270;&#30001;&#20934;&#30830;&#24615;&#21644;&#25104;&#26412;&#32452;&#25104;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#30456;&#36739;&#20110;&#20854;&#20182;&#26041;&#27861;&#38477;&#20302;&#20102; SLO &#36829;&#35268;&#21644;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2304.10892</link><description>&lt;p&gt;
&#21327;&#35843;&#25512;&#29702;&#26381;&#21153;&#31995;&#32479;&#30340;&#39640;&#20934;&#30830;&#24615;&#12289;&#25104;&#26412;&#25928;&#30410;&#21644;&#20302;&#24310;&#36831;
&lt;/p&gt;
&lt;p&gt;
Reconciling High Accuracy, Cost-Efficiency, and Low Latency of Inference Serving Systems. (arXiv:2304.10892v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10892
&lt;/p&gt;
&lt;p&gt;
InfAdapter&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#39640;&#20934;&#30830;&#24615;&#12289;&#20302;&#24310;&#36831;&#21644;&#25104;&#26412;&#25928;&#30410;&#20043;&#38388;&#26435;&#34913;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20027;&#21160;&#36873;&#25321;&#19968;&#32452;&#24102;&#26377;&#36164;&#28304;&#20998;&#37197;&#30340; ML &#27169;&#22411;&#21464;&#20307;&#26469;&#28385;&#36275;&#24310;&#36831; SLO&#65292;&#24182;&#26368;&#22823;&#21270;&#30001;&#20934;&#30830;&#24615;&#21644;&#25104;&#26412;&#32452;&#25104;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#30456;&#36739;&#20110;&#20854;&#20182;&#26041;&#27861;&#38477;&#20302;&#20102; SLO &#36829;&#35268;&#21644;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25512;&#29702;&#26381;&#21153;&#30340;&#20351;&#29992;&#27491;&#22312;&#24613;&#21095;&#22686;&#21152;&#12290;ML&#25512;&#29702;&#26381;&#21153;&#19982;&#29992;&#25143;&#30452;&#25509;&#20132;&#20114;&#65292;&#38656;&#35201;&#24555;&#36895;&#20934;&#30830;&#30340;&#21709;&#24212;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26381;&#21153;&#38754;&#20020;&#19981;&#26029;&#21464;&#21270;&#30340;&#35831;&#27714;&#24037;&#20316;&#36127;&#36733;&#65292;&#38656;&#35201;&#35843;&#25972;&#20854;&#35745;&#31639;&#36164;&#28304;&#12290;&#35745;&#31639;&#36164;&#28304;&#19981;&#21512;&#29702;&#20250;&#23548;&#33268;&#24310;&#36831;&#26381;&#21153;&#32423;&#21035;&#30446;&#26631; (SLOs) &#36829;&#35268;&#25110;&#28010;&#36153;&#35745;&#31639;&#36164;&#28304;&#12290;&#32771;&#34385;&#20934;&#30830;&#24615;&#12289;&#24310;&#36831;&#21644;&#36164;&#28304;&#25104;&#26412;&#31561;&#26041;&#38754;&#30340;&#25152;&#26377;&#22240;&#32032;&#26469;&#36866;&#24212;&#21160;&#24577;&#24037;&#20316;&#36127;&#36733;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; InfAdapter&#65292;&#23427;&#20250;&#20027;&#21160;&#36873;&#25321;&#19968;&#32452;&#24102;&#26377;&#36164;&#28304;&#20998;&#37197;&#30340; ML &#27169;&#22411;&#21464;&#20307;&#65292;&#20197;&#28385;&#36275;&#24310;&#36831; SLO&#65292;&#24182;&#26368;&#22823;&#21270;&#30001;&#20934;&#30830;&#24615;&#21644;&#25104;&#26412;&#32452;&#25104;&#30340;&#30446;&#26631;&#20989;&#25968;&#12290;&#30456;&#36739;&#20110;&#27969;&#34892;&#30340;&#34892;&#19994;&#33258;&#21160;&#32553;&#25918;&#22120; (Kubernetes Vertical Pod Autoscaler)&#65292;InfAdapter &#20998;&#21035;&#38477;&#20302;&#20102; SLO &#36829;&#35268;&#21644;&#25104;&#26412;&#36798; 65% &#21644; 33%&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of machine learning (ML) inference for various applications is growing drastically. ML inference services engage with users directly, requiring fast and accurate responses. Moreover, these services face dynamic workloads of requests, imposing changes in their computing resources. Failing to right-size computing resources results in either latency service level objectives (SLOs) violations or wasted computing resources. Adapting to dynamic workloads considering all the pillars of accuracy, latency, and resource cost is challenging. In response to these challenges, we propose InfAdapter, which proactively selects a set of ML model variants with their resource allocations to meet latency SLO while maximizing an objective function composed of accuracy and cost. InfAdapter decreases SLO violation and costs up to 65% and 33%, respectively, compared to a popular industry autoscaler (Kubernetes Vertical Pod Autoscaler).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#24212;&#29992;&#65292;&#25506;&#35752;&#20102;&#19981;&#21516;&#20307;&#31995;&#32467;&#26500;&#21644;&#36816;&#31639;&#31526;&#30340;&#20248;&#32570;&#28857;&#65292;&#37325;&#28857;&#35752;&#35770;&#20102;&#38024;&#23545;&#20415;&#25658;&#35745;&#31639;&#24179;&#21488;&#30340;&#30828;&#20214;&#21152;&#36895;&#26041;&#26696;&#65292;&#24182;&#23545;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#30340;&#23618;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;</title><link>http://arxiv.org/abs/2304.10891</link><description>&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#20013;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#21450;&#20854;&#30828;&#20214;&#21152;&#36895;&#20998;&#26512;&#65306;&#32508;&#36848; (arXiv:2304.10891v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
Transformer-based models and hardware acceleration analysis in autonomous driving: A survey. (arXiv:2304.10891v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#24212;&#29992;&#65292;&#25506;&#35752;&#20102;&#19981;&#21516;&#20307;&#31995;&#32467;&#26500;&#21644;&#36816;&#31639;&#31526;&#30340;&#20248;&#32570;&#28857;&#65292;&#37325;&#28857;&#35752;&#35770;&#20102;&#38024;&#23545;&#20415;&#25658;&#35745;&#31639;&#24179;&#21488;&#30340;&#30828;&#20214;&#21152;&#36895;&#26041;&#26696;&#65292;&#24182;&#23545;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#30340;&#23618;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;Transformer&#26550;&#26500;&#22312;&#21508;&#31181;&#33258;&#21160;&#39550;&#39542;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23558;&#20854;&#19987;&#38376;&#29992;&#20110;&#20415;&#25658;&#24335;&#35745;&#31639;&#24179;&#21488;&#30340;&#30828;&#20214;&#21152;&#36895;&#24050;&#25104;&#20026;&#23454;&#38469;&#37096;&#32626;&#22312;&#30495;&#23454;&#33258;&#21160;&#27773;&#36710;&#20013;&#30340;&#19979;&#19968;&#27493;&#20851;&#38190;&#27493;&#39588;&#12290;&#26412;&#32508;&#36848;&#35770;&#25991;&#25552;&#20379;&#20102;&#38024;&#23545;&#33258;&#21160;&#39550;&#39542;&#20219;&#21153;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#30340;&#20840;&#38754;&#27010;&#36848;&#12289;&#22522;&#20934;&#21644;&#20998;&#26512;&#65292;&#20363;&#22914;&#36710;&#36947;&#26816;&#27979;&#12289;&#20998;&#21106;&#12289;&#36319;&#36394;&#12289;&#35268;&#21010;&#21644;&#20915;&#31574;&#21046;&#23450;&#12290;&#25105;&#20204;&#23457;&#26597;&#20102;&#19981;&#21516;&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#29992;&#20110;&#32452;&#32455;Transformer&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#65292;&#20363;&#22914;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#21644;&#20165;&#32534;&#30721;&#22120;&#32467;&#26500;&#65292;&#24182;&#25506;&#35752;&#20102;&#23427;&#20204;&#21508;&#33258;&#30340;&#20248;&#32570;&#28857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#28145;&#20837;&#35752;&#35770;&#20102;Transformer&#30456;&#20851;&#30340;&#36816;&#31639;&#31526;&#21450;&#20854;&#30828;&#20214;&#21152;&#36895;&#26041;&#26696;&#65292;&#32771;&#34385;&#21040;&#20851;&#38190;&#22240;&#32032;&#65292;&#22914;&#37327;&#21270;&#21644;&#36816;&#34892;&#26102;&#12290;&#25105;&#20204;&#29305;&#21035;&#22312;&#31227;&#21160;&#21644;&#26700;&#38754;&#24179;&#21488;&#19978;&#23545;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#23618;&#19982;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#30340;&#36816;&#31639;&#31526;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#26412;&#32508;&#36848;&#35770;&#25991;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#25552;&#20379;&#20102;&#31995;&#32479;&#30340;&#25351;&#21335;&#65292;&#20197;&#20102;&#35299;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#21450;&#20854;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#30828;&#20214;&#21152;&#36895;&#30340;&#24403;&#21069;&#36827;&#23637;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer architectures have exhibited promising performance in various autonomous driving applications in recent years. On the other hand, its dedicated hardware acceleration on portable computational platforms has become the next critical step for practical deployment in real autonomous vehicles. This survey paper provides a comprehensive overview, benchmark, and analysis of Transformer-based models specifically tailored for autonomous driving tasks such as lane detection, segmentation, tracking, planning, and decision-making. We review different architectures for organizing Transformer inputs and outputs, such as encoder-decoder and encoder-only structures, and explore their respective advantages and disadvantages. Furthermore, we discuss Transformer-related operators and their hardware acceleration schemes in depth, taking into account key factors such as quantization and runtime. We specifically illustrate the operator level comparison between layers from convolutional neural ne
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;&#24352;&#37327;&#32593;&#32476;&#30340;&#29983;&#25104;&#27169;&#22411;&#24212;&#29992;&#20110;&#23567;&#20998;&#23376;&#25968;&#25454;&#38598;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#30340;&#24615;&#33021;&#65292;&#26088;&#22312;&#23454;&#29616;&#20174;&#36825;&#20123;&#25216;&#26415;&#20013;&#33719;&#24471;&#30495;&#23454;&#19990;&#30028;&#37327;&#23376;&#20248;&#21183;&#30340;&#26377;&#21069;&#36884;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2304.10867</link><description>&lt;p&gt;
&#37327;&#23376;&#21551;&#21457;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#23567;&#20998;&#23376;&#25968;&#25454;&#38598;&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Application of quantum-inspired generative models to small molecular datasets. (arXiv:2304.10867v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10867
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#24352;&#37327;&#32593;&#32476;&#30340;&#29983;&#25104;&#27169;&#22411;&#24212;&#29992;&#20110;&#23567;&#20998;&#23376;&#25968;&#25454;&#38598;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#30340;&#24615;&#33021;&#65292;&#26088;&#22312;&#23454;&#29616;&#20174;&#36825;&#20123;&#25216;&#26415;&#20013;&#33719;&#24471;&#30495;&#23454;&#19990;&#30028;&#37327;&#23376;&#20248;&#21183;&#30340;&#26377;&#21069;&#36884;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#37327;&#23376;&#35745;&#31639;&#30340;&#26222;&#21450;&#65292;&#37327;&#23376;&#21644;&#37327;&#23376;&#21551;&#21457;&#24335;&#26426;&#22120;&#23398;&#20064;&#24050;&#25104;&#20026;&#19968;&#20010;&#26377;&#21069;&#36884;&#19988;&#20855;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#29702;&#35770;&#19978;&#30340;&#36129;&#29486;&#25351;&#21521;&#29983;&#25104;&#24314;&#27169;&#20316;&#20026;&#23454;&#29616;&#20174;&#36825;&#20123;&#25216;&#26415;&#20013;&#33719;&#24471;&#30495;&#23454;&#19990;&#30028;&#37327;&#23376;&#20248;&#21183;&#30340;&#26377;&#21069;&#36884;&#26041;&#21521;&#12290;&#19968;&#20123;&#23454;&#35777;&#30740;&#31350;&#20063;&#35777;&#26126;&#20102;&#36825;&#31181;&#28508;&#21147;&#65292;&#23588;&#20854;&#26159;&#24403;&#32771;&#34385;&#22522;&#20110;&#24352;&#37327;&#32593;&#32476;&#30340;&#37327;&#23376;&#21551;&#21457;&#24335;&#27169;&#22411;&#26102;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#22522;&#20110;&#24352;&#37327;&#32593;&#32476;&#30340;&#29983;&#25104;&#27169;&#22411;&#24212;&#29992;&#20110;&#20998;&#23376;&#21457;&#29616;&#38382;&#39064;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#20004;&#20010;&#23567;&#22411;&#20998;&#23376;&#25968;&#25454;&#38598;&#65306;QM9&#25968;&#25454;&#38598;&#20013;&#30340;4989&#20010;&#20998;&#23376;&#30340;&#23376;&#38598;&#21644;TotalEnergies&#20013;&#30340;516&#20010;&#39564;&#35777;&#25239;&#27687;&#21270;&#21058;&#30340;&#23567;&#22411;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#37319;&#29992;&#19981;&#21516;&#30340;&#22522;&#20110;&#37319;&#26679;&#30340;&#24230;&#37327;&#26631;&#20934;&#27604;&#36739;&#20102;&#20960;&#31181;&#24352;&#37327;&#32593;&#32476;&#27169;&#22411;&#21644;&#19968;&#20010;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#36825;&#20123;&#26631;&#20934;&#21453;&#26144;&#20102;&#23427;&#20204;&#22312;&#27599;&#20010;&#20219;&#21153;&#19978;&#30340;&#23398;&#20064;&#24615;&#33021;&#21644;&#22810;&#30446;&#26631;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum and quantum-inspired machine learning has emerged as a promising and challenging research field due to the increased popularity of quantum computing, especially with near-term devices. Theoretical contributions point toward generative modeling as a promising direction to realize the first examples of real-world quantum advantages from these technologies. A few empirical studies also demonstrate such potential, especially when considering quantum-inspired models based on tensor networks. In this work, we apply tensor-network-based generative models to the problem of molecular discovery. In our approach, we utilize two small molecular datasets: a subset of $4989$ molecules from the QM9 dataset and a small in-house dataset of $516$ validated antioxidants from TotalEnergies. We compare several tensor network models against a generative adversarial network using different sample-based metrics, which reflect their learning performances on each task, and multiobjective performances us
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#25506;&#32034;&#23545;&#23454;&#38469;&#23398;&#20064;&#31639;&#27861;&#30340;&#37325;&#35201;&#24615;&#24182;&#20351;&#29992;&#19981;&#21516;&#31574;&#30053;&#30340;&#28145;&#24230;Q&#32593;&#32476;&#35299;&#20915;&#20102;URLLC&#20449;&#24687;&#30340;&#20256;&#36755;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#33258;&#36866;&#24212;&#25506;&#32034;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.10860</link><description>&lt;p&gt;
&#35770;&#25506;&#32034;&#23545;&#23454;&#38469;&#23398;&#20064;&#31639;&#27861;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Importance of Exploration for Real Life Learned Algorithms. (arXiv:2304.10860v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10860
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#25506;&#32034;&#23545;&#23454;&#38469;&#23398;&#20064;&#31639;&#27861;&#30340;&#37325;&#35201;&#24615;&#24182;&#20351;&#29992;&#19981;&#21516;&#31574;&#30053;&#30340;&#28145;&#24230;Q&#32593;&#32476;&#35299;&#20915;&#20102;URLLC&#20449;&#24687;&#30340;&#20256;&#36755;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#33258;&#36866;&#24212;&#25506;&#32034;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#23398;&#20064;&#31639;&#27861;&#30340;&#36136;&#37327;&#19982;&#21487;&#29992;&#25968;&#25454;&#30340;&#36136;&#37327;&#26174;&#33879;&#30456;&#20851;&#12290;&#29983;&#25104;&#22909;&#30340;&#25968;&#25454;&#30340;&#26368;&#31616;&#21333;&#26041;&#24335;&#20043;&#19968;&#26159;&#26234;&#33021;&#22320;&#23545;&#25968;&#25454;&#28304;&#36827;&#34892;&#37319;&#26679;&#25110;&#25506;&#32034;&#12290;&#26234;&#33021;&#37319;&#26679;&#21487;&#20197;&#38477;&#20302;&#33719;&#21462;&#26679;&#26412;&#30340;&#25104;&#26412;&#65292;&#20943;&#23569;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#20351;&#23398;&#20064;&#31639;&#27861;&#33021;&#22815;&#36866;&#24212;&#26410;&#39044;&#26009;&#21040;&#30340;&#20107;&#20214;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#25506;&#32034;&#31574;&#30053;&#25945;&#25480;&#20102;&#19977;&#20010;&#28145;&#24230;Q&#32593;&#32476;&#65288;DQN&#65289;&#26469;&#35299;&#20915;URLLC&#20449;&#24687;&#30340;&#25171;&#23380;&#20256;&#36755;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20004;&#31181;&#33258;&#36866;&#24212;&#25506;&#32034;&#20505;&#36873;&#26041;&#27861;&#65288;&#22522;&#20110;&#26041;&#24046;&#21644;&#22522;&#20110;&#26368;&#22823;&#29109;&#30340;&#25506;&#32034;&#65289;&#30456;&#23545;&#20110;&#26631;&#20934;&#30340;&#31616;&#21333;epsilon-greedy&#25506;&#32034;&#26041;&#27861;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The quality of data driven learning algorithms scales significantly with the quality of data available. One of the most straight-forward ways to generate good data is to sample or explore the data source intelligently. Smart sampling can reduce the cost of gaining samples, reduce computation cost in learning, and enable the learning algorithm to adapt to unforeseen events. In this paper, we teach three Deep Q-Networks (DQN) with different exploration strategies to solve a problem of puncturing ongoing transmissions for URLLC messages. We demonstrate the efficiency of two adaptive exploration candidates, variance-based and Maximum Entropy-based exploration, compared to the standard, simple epsilon-greedy exploration approach.
&lt;/p&gt;</description></item><item><title>SequeL&#26159;&#19968;&#20010;&#22522;&#20110;Pytorch&#21644;JAX&#30340;&#25345;&#32493;&#23398;&#20064;&#24211;&#65292;&#20026;&#21508;&#31181;&#25345;&#32493;&#23398;&#20064;&#31639;&#27861;&#25552;&#20379;&#32479;&#19968;&#25509;&#21475;&#65292;&#24182;&#20855;&#26377;&#27169;&#22359;&#21270;&#21644;&#31616;&#21333;&#24615;&#12290;&#33021;&#22815;&#24110;&#21161;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#21644;&#24320;&#21457;&#32773;&#36731;&#26494;&#25193;&#23637;&#33258;&#24049;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.10857</link><description>&lt;p&gt;
SequeL: &#19968;&#20010;&#22522;&#20110;PyTorch&#21644;JAX&#30340;&#25345;&#32493;&#23398;&#20064;&#24211;
&lt;/p&gt;
&lt;p&gt;
SequeL: A Continual Learning Library in PyTorch and JAX. (arXiv:2304.10857v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10857
&lt;/p&gt;
&lt;p&gt;
SequeL&#26159;&#19968;&#20010;&#22522;&#20110;Pytorch&#21644;JAX&#30340;&#25345;&#32493;&#23398;&#20064;&#24211;&#65292;&#20026;&#21508;&#31181;&#25345;&#32493;&#23398;&#20064;&#31639;&#27861;&#25552;&#20379;&#32479;&#19968;&#25509;&#21475;&#65292;&#24182;&#20855;&#26377;&#27169;&#22359;&#21270;&#21644;&#31616;&#21333;&#24615;&#12290;&#33021;&#22815;&#24110;&#21161;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#21644;&#24320;&#21457;&#32773;&#36731;&#26494;&#25193;&#23637;&#33258;&#24049;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#19968;&#20010;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#27169;&#22411;&#24517;&#39035;&#36866;&#24212;&#36830;&#32493;&#30340;&#26032;&#25968;&#25454;&#27969;&#32780;&#19981;&#20250;&#24536;&#35760;&#20808;&#21069;&#33719;&#24471;&#30340;&#30693;&#35782;&#12290;&#29616;&#26377;&#26694;&#26550;&#22522;&#20110;PyTorch&#26500;&#24314;&#65292;&#20294;JAX&#30340;&#26085;&#30410;&#27969;&#34892;&#21487;&#33021;&#23548;&#33268;&#19981;&#21516;&#30340;&#20195;&#30721;&#24211;&#65292;&#26368;&#32456;&#38459;&#30861;&#20102;&#21487;&#37325;&#22797;&#24615;&#21644;&#36827;&#23637;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SequeL&#65292;&#23427;&#26159;&#19968;&#20010;&#28789;&#27963;&#19988;&#21487;&#25193;&#23637;&#30340;&#25345;&#32493;&#23398;&#20064;&#24211;&#65292;&#25903;&#25345;PyTorch&#21644;JAX&#26694;&#26550;&#12290;SequeL&#20026;&#21508;&#31181;&#25345;&#32493;&#23398;&#20064;&#31639;&#27861;&#25552;&#20379;&#32479;&#19968;&#30340;&#25509;&#21475;&#65292;&#21253;&#25324;&#22522;&#20110;&#27491;&#21017;&#21270;&#30340;&#26041;&#27861;&#12289;&#22522;&#20110;&#22238;&#25918;&#30340;&#26041;&#27861;&#21644;&#28151;&#21512;&#26041;&#27861;&#12290;&#35813;&#24211;&#35774;&#35745;&#20855;&#26377;&#27169;&#22359;&#21270;&#21644;&#31616;&#21333;&#24615;&#65292;&#20351;API&#36866;&#21512;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#20351;&#29992;&#12290;&#25105;&#20204;&#23558;SequeL&#20316;&#20026;&#24320;&#28304;&#24211;&#21457;&#24067;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#21644;&#24320;&#21457;&#20154;&#21592;&#33021;&#22815;&#36731;&#26494;&#22320;&#23454;&#39564;&#21644;&#25193;&#23637;&#35813;&#24211;&#20197;&#36866;&#24212;&#33258;&#24049;&#30340;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual Learning is an important and challenging problem in machine learning, where models must adapt to a continuous stream of new data without forgetting previously acquired knowledge. While existing frameworks are built on PyTorch, the rising popularity of JAX might lead to divergent codebases, ultimately hindering reproducibility and progress. To address this problem, we introduce SequeL, a flexible and extensible library for Continual Learning that supports both PyTorch and JAX frameworks. SequeL provides a unified interface for a wide range of Continual Learning algorithms, including regularization-based approaches, replay-based approaches, and hybrid approaches. The library is designed towards modularity and simplicity, making the API suitable for both researchers and practitioners. We release SequeL\footnote{\url{https://github.com/nik-dim/sequel}} as an open-source library, enabling researchers and developers to easily experiment and extend the library for their own purposes
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22235;&#31181;GNN&#27169;&#22411;&#65292;&#25351;&#20986;&#20854;&#20013;&#20004;&#31181;&#23558;&#25152;&#26377;&#33410;&#28857;&#23884;&#20837;&#21516;&#19968;&#29305;&#24449;&#21521;&#37327;&#20013;&#65292;&#32780;&#21478;&#22806;&#20004;&#31181;&#27169;&#22411;&#29983;&#25104;&#30340;&#34920;&#31034;&#19982;&#36755;&#20837;&#22270;&#20013;&#30340;&#27493;&#38271;&#25968;&#37327;&#30456;&#20851;&#12290;&#22312;&#19968;&#23450;&#26465;&#20214;&#19979;&#65292;&#19981;&#21516;&#32467;&#26500;&#30340;&#33410;&#28857;&#21487;&#33021;&#26377;&#30456;&#20284;&#30340;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2304.10851</link><description>&lt;p&gt;
GNNs&#21040;&#24213;&#22312;&#23398;&#20160;&#20040;&#65311;&#8212;&#8212;&#29702;&#35299;&#23427;&#20204;&#30340;&#34920;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
What Do GNNs Actually Learn? Towards Understanding their Representations. (arXiv:2304.10851v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10851
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22235;&#31181;GNN&#27169;&#22411;&#65292;&#25351;&#20986;&#20854;&#20013;&#20004;&#31181;&#23558;&#25152;&#26377;&#33410;&#28857;&#23884;&#20837;&#21516;&#19968;&#29305;&#24449;&#21521;&#37327;&#20013;&#65292;&#32780;&#21478;&#22806;&#20004;&#31181;&#27169;&#22411;&#29983;&#25104;&#30340;&#34920;&#31034;&#19982;&#36755;&#20837;&#22270;&#20013;&#30340;&#27493;&#38271;&#25968;&#37327;&#30456;&#20851;&#12290;&#22312;&#19968;&#23450;&#26465;&#20214;&#19979;&#65292;&#19981;&#21516;&#32467;&#26500;&#30340;&#33410;&#28857;&#21487;&#33021;&#26377;&#30456;&#20284;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#22270;&#23884;&#20837;&#23398;&#20064;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#23613;&#31649;&#20197;&#24448;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#65288;&#21363;&#23427;&#20204;&#26159;&#21542;&#33021;&#21306;&#20998;&#38750;&#21516;&#26500;&#22270;&#23545;&#65289;&#65292;&#20294;&#20173;&#19981;&#28165;&#26970;&#36825;&#20123;&#27169;&#22411;&#25152;&#23398;&#20064;&#30340;&#33410;&#28857;&#34920;&#31034;&#20013;&#32534;&#30721;&#20102;&#21738;&#20123;&#32467;&#26500;&#20449;&#24687;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22235;&#31181;&#27969;&#34892;&#30340;GNN&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#20013;&#20004;&#31181;&#23558;&#25152;&#26377;&#33410;&#28857;&#23884;&#20837;&#21516;&#19968;&#29305;&#24449;&#21521;&#37327;&#20013;&#65292;&#32780;&#21478;&#22806;&#20004;&#31181;&#27169;&#22411;&#29983;&#25104;&#30340;&#34920;&#31034;&#19982;&#36755;&#20837;&#22270;&#20013;&#30340;&#27493;&#38271;&#25968;&#37327;&#30456;&#20851;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22914;&#26524;&#20004;&#20010;&#19981;&#21516;&#32467;&#26500;&#30340;&#33410;&#28857;&#22312;&#26576;&#19968;&#23618;$k&gt;1$ &#20013;&#30340;&#27493;&#38271;&#30456;&#21516;&#65292;&#21017;&#23427;&#20204;&#30340;&#34920;&#31034;&#21487;&#33021;&#30456;&#20284;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#39564;&#35777;&#65292;&#20174;&#32780;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, graph neural networks (GNNs) have achieved great success in the field of graph representation learning. Although prior work has shed light into the expressiveness of those models (\ie whether they can distinguish pairs of non-isomorphic graphs), it is still not clear what structural information is encoded into the node representations that are learned by those models. In this paper, we investigate which properties of graphs are captured purely by these models, when no node attributes are available. Specifically, we study four popular GNN models, and we show that two of them embed all nodes into the same feature vector, while the other two models generate representations that are related to the number of walks over the input graph. Strikingly, structurally dissimilar nodes can have similar representations at some layer $k&gt;1$, if they have the same number of walks of length $k$. We empirically verify our theoretical findings on real datasets.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#35299;&#37322;&#32447;&#24615;&#31995;&#32479;&#30340;&#31232;&#30095;&#30697;&#38453;&#20026;&#40657;&#30333;&#22270;&#20687;&#65292;&#21033;&#29992;&#27744;&#21270;&#25805;&#20316;&#23558;&#20854;&#36716;&#25442;&#20026;&#23567;&#30340;&#22810;&#36890;&#36947;&#22270;&#20687;&#65292;&#20174;&#32780;&#35843;&#25972;&#20195;&#25968;&#22810;&#37325;&#32593;&#26684;&#26041;&#27861;&#20013;&#30340;&#24378;&#38376;&#27099;&#21442;&#25968;&#12290;&#35813;&#31639;&#27861;&#26368;&#23567;&#21270;&#20102;AMG&#26041;&#27861;&#22312;&#26377;&#38480;&#20803;&#27714;&#35299;&#22120;&#20013;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#22312;&#35299;&#20915;&#19977;&#32500;&#26925;&#22278;&#20559;&#24494;&#20998;&#26041;&#31243;&#26102;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;AMG&#27714;&#35299;&#22120;&#26356;&#24555;&#12290;</title><link>http://arxiv.org/abs/2304.10832</link><description>&lt;p&gt;
&#19968;&#31181;&#21152;&#36895;&#26377;&#38480;&#20803;&#27714;&#35299;&#22120;&#20013;&#20195;&#25968;&#22810;&#37325;&#32593;&#26684;&#26041;&#27861;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Deep Learning algorithm to accelerate Algebraic Multigrid methods in Finite Element solvers of 3D elliptic PDEs. (arXiv:2304.10832v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10832
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#35299;&#37322;&#32447;&#24615;&#31995;&#32479;&#30340;&#31232;&#30095;&#30697;&#38453;&#20026;&#40657;&#30333;&#22270;&#20687;&#65292;&#21033;&#29992;&#27744;&#21270;&#25805;&#20316;&#23558;&#20854;&#36716;&#25442;&#20026;&#23567;&#30340;&#22810;&#36890;&#36947;&#22270;&#20687;&#65292;&#20174;&#32780;&#35843;&#25972;&#20195;&#25968;&#22810;&#37325;&#32593;&#26684;&#26041;&#27861;&#20013;&#30340;&#24378;&#38376;&#27099;&#21442;&#25968;&#12290;&#35813;&#31639;&#27861;&#26368;&#23567;&#21270;&#20102;AMG&#26041;&#27861;&#22312;&#26377;&#38480;&#20803;&#27714;&#35299;&#22120;&#20013;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#22312;&#35299;&#20915;&#19977;&#32500;&#26925;&#22278;&#20559;&#24494;&#20998;&#26041;&#31243;&#26102;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;AMG&#27714;&#35299;&#22120;&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#25968;&#22810;&#37325;&#32593;&#26684;&#26041;&#27861;&#26159;&#35299;&#32447;&#24615;&#26041;&#31243;&#32452;&#30340;&#26368;&#26377;&#25928;&#30340;&#27714;&#35299;&#22120;&#20043;&#19968;&#65292;&#24191;&#27867;&#29992;&#20110;&#31163;&#25955;&#21270;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#35813;&#26041;&#27861;&#23384;&#22312;&#39640;&#24230;&#20381;&#36182;&#20110;&#38656;&#35201;&#35843;&#20248;&#30340;&#21442;&#25968;&#65292;&#23588;&#20854;&#26159;&#24378;&#38376;&#27099;&#21442;&#25968;&#65292;&#36825;&#26159;&#26500;&#24314;&#22810;&#37325;&#32593;&#26684;&#25152;&#38656;&#30340;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#25226;&#32447;&#24615;&#31995;&#32479;&#30340;&#31232;&#30095;&#30697;&#38453;&#35299;&#37322;&#20026;&#40657;&#30333;&#22270;&#20687;&#65292;&#21033;&#29992;&#27744;&#21270;&#25805;&#20316;&#23558;&#23427;&#36716;&#25442;&#20026;&#23567;&#30340;&#22810;&#36890;&#36947;&#22270;&#20687;&#65292;&#20174;&#32780;&#35843;&#25972;&#24378;&#38376;&#27099;&#21442;&#25968;&#65292;&#26368;&#23567;&#21270;&#20102;AMG&#26041;&#27861;&#22312;&#26377;&#38480;&#20803;&#27714;&#35299;&#22120;&#20013;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#31639;&#27861;&#23545;&#20110;&#35299;&#20915;&#19977;&#32500;&#26925;&#22278;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#36895;&#24230;&#26174;&#33879;&#24555;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;AMG&#27714;&#35299;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algebraic multigrid (AMG) methods are among the most efficient solvers for linear systems of equations and they are widely used for the solution of problems stemming from the discretization of Partial Differential Equations (PDEs). The most severe limitation of AMG methods is the dependence on parameters that require to be fine-tuned. In particular, the strong threshold parameter is the most relevant since it stands at the basis of the construction of successively coarser grids needed by the AMG methods. We introduce a novel Deep Learning algorithm that minimizes the computational cost of the AMG method when used as a finite element solver. We show that our algorithm requires minimal changes to any existing code. The proposed Artificial Neural Network (ANN) tunes the value of the strong threshold parameter by interpreting the sparse matrix of the linear system as a black-and-white image and exploiting a pooling operator to transform it into a small multi-channel image. We experimentall
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#37051;&#22495;&#24863;&#30693;&#23376;&#22270;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#22122;&#22768;&#65292;&#25552;&#39640;&#23376;&#22270;&#30340;&#21484;&#22238;&#29575;&#65292;&#20174;&#32780;&#21487;&#20197;&#25512;&#21160;&#36828;&#36317;&#31163;&#30340;&#33410;&#28857;&#21521;&#30456;&#21516;&#30340;&#20013;&#24515;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2304.10831</link><description>&lt;p&gt;
&#23398;&#20064;&#22914;&#20309;&#20351;&#29992;&#26356;&#22909;&#30340;&#23376;&#22270;&#36827;&#34892;&#20154;&#33080;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Learn to Cluster Faces with Better Subgraphs. (arXiv:2304.10831v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10831
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#37051;&#22495;&#24863;&#30693;&#23376;&#22270;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#22122;&#22768;&#65292;&#25552;&#39640;&#23376;&#22270;&#30340;&#21484;&#22238;&#29575;&#65292;&#20174;&#32780;&#21487;&#20197;&#25512;&#21160;&#36828;&#36317;&#31163;&#30340;&#33410;&#28857;&#21521;&#30456;&#21516;&#30340;&#20013;&#24515;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#33080;&#32858;&#31867;&#21487;&#20197;&#20026;&#28023;&#37327;&#26080;&#26631;&#31614;&#20154;&#33080;&#25968;&#25454;&#25552;&#20379;&#20266;&#26631;&#31614;&#65292;&#24182;&#25552;&#39640;&#19981;&#21516;&#20154;&#33080;&#35782;&#21035;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#29616;&#26377;&#30340;&#32858;&#31867;&#26041;&#27861;&#36890;&#24120;&#26159;&#22312;&#23376;&#22270;&#20869;&#32858;&#21512;&#29305;&#24449;&#65292;&#36825;&#20123;&#23376;&#22270;&#36890;&#24120;&#22522;&#20110;&#32479;&#19968;&#30340;&#38408;&#20540;&#25110;&#23398;&#20064;&#24471;&#21040;&#30340;&#25130;&#27490;&#20301;&#32622;&#23454;&#29616;&#12290;&#36825;&#21487;&#33021;&#20250;&#38477;&#20302;&#23376;&#22270;&#30340;&#21484;&#22238;&#29575;&#65292;&#20174;&#32780;&#38477;&#20302;&#32858;&#31867;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#37051;&#22495;&#24863;&#30693;&#23376;&#22270;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#22122;&#22768;&#65292;&#25552;&#39640;&#23376;&#22270;&#30340;&#21484;&#22238;&#29575;&#65292;&#20174;&#32780;&#21487;&#20197;&#25512;&#21160;&#36828;&#36317;&#31163;&#30340;&#33410;&#28857;&#21521;&#30456;&#21516;&#30340;&#20013;&#24515;&#25910;&#25947;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#32452;&#20214;&#65292;&#21363;&#20351;&#29992;&#37051;&#23621;&#30340;&#23884;&#20837;&#26469;&#22686;&#24378;&#20154;&#33080;&#23884;&#20837;&#65292;&#24182;&#23545;&#33410;&#28857;&#23545;&#36827;&#34892;&#23553;&#38381;&#23376;&#22270;&#26500;&#24314;&#20197;&#25552;&#21462;&#32467;&#26500;&#20449;&#24687;&#12290;&#23558;&#23884;&#20837;&#32452;&#21512;&#36215;&#26469;&#65292;&#39044;&#27979;&#25152;&#26377;&#33410;&#28857;&#23545;&#30340;&#38142;&#25509;&#27010;&#29575;&#20197;&#26367;&#25442;&#20313;&#24358;&#30456;&#20284;&#24230;&#26469;&#29983;&#25104;&#26032;&#30340;&#23376;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Face clustering can provide pseudo-labels to the massive unlabeled face data and improve the performance of different face recognition models. The existing clustering methods generally aggregate the features within subgraphs that are often implemented based on a uniform threshold or a learned cutoff position. This may reduce the recall of subgraphs and hence degrade the clustering performance. This work proposed an efficient neighborhood-aware subgraph adjustment method that can significantly reduce the noise and improve the recall of the subgraphs, and hence can drive the distant nodes to converge towards the same centers. More specifically, the proposed method consists of two components, i.e. face embeddings enhancement using the embeddings from neighbors, and enclosed subgraph construction of node pairs for structural information extraction. The embeddings are combined to predict the linkage probabilities for all node pairs to replace the cosine similarities to produce new subgraphs
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28378;&#21160;&#21069;&#30651;&#23398;&#20064;&#31639;&#27861;&#65292;&#26377;&#25928;&#22320;&#25913;&#36827;&#20102;&#26368;&#20248;&#20998;&#31867;&#26641;&#30340;&#23398;&#20064;&#30149;&#29702;&#65292;&#28789;&#27963;&#22788;&#29702;&#20219;&#20309;&#25439;&#22833;&#20989;&#25968;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.10830</link><description>&lt;p&gt;
&#28378;&#21160;&#21069;&#30651;&#23398;&#20064;&#22312;&#20998;&#31867;&#26641;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Rolling Lookahead Learning for Optimal Classification Trees. (arXiv:2304.10830v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10830
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28378;&#21160;&#21069;&#30651;&#23398;&#20064;&#31639;&#27861;&#65292;&#26377;&#25928;&#22320;&#25913;&#36827;&#20102;&#26368;&#20248;&#20998;&#31867;&#26641;&#30340;&#23398;&#20064;&#30149;&#29702;&#65292;&#28789;&#27963;&#22788;&#29702;&#20219;&#20309;&#25439;&#22833;&#20989;&#25968;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#26412;&#36136;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#20998;&#31867;&#26641;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#20173;&#34987;&#24191;&#27867;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#28378;&#21160;&#23376;&#26641;&#21069;&#30651;&#31639;&#27861;&#65292;&#23558;&#36817;&#35270;&#26041;&#27861;&#30340;&#30456;&#23545;&#21487;&#25193;&#23637;&#24615;&#19982;&#26500;&#24314;&#26641;&#30340;&#26368;&#20248;&#26041;&#27861;&#30340;&#39044;&#35265;&#24615;&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#31639;&#27861;&#20013;&#30340;&#26377;&#38480;&#39044;&#35265;&#38477;&#20302;&#20102;&#26368;&#20248;&#26041;&#27861;&#20013;&#35266;&#23519;&#21040;&#30340;&#23398;&#20064;&#30149;&#29702;&#12290;&#25105;&#20204;&#31639;&#27861;&#30340;&#26680;&#24515;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20108;&#32423;&#26368;&#20248;&#20108;&#21449;&#20998;&#31867;&#26641;&#20844;&#24335;&#65292;&#28789;&#27963;&#22788;&#29702;&#20219;&#20309;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#20844;&#24335;&#30340;&#21487;&#34892;&#22495;&#26159;&#19968;&#20010;&#25972;&#25968;&#22810;&#38754;&#20307;&#65292;&#20174;&#32780;&#20135;&#29983;&#26368;&#20248;&#30340;LP&#26494;&#24347;&#35299;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#35745;&#31639;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;1330&#20010;&#38382;&#39064;&#23454;&#20363;&#20013;&#26377;808&#20010;&#30340;&#24615;&#33021;&#20248;&#20110;&#26368;&#20248;&#21644;&#36817;&#35270;&#26041;&#27861;&#65292;&#20998;&#21035;&#23558;&#22806;&#26679;&#26412;&#31934;&#24230;&#25552;&#39640;&#20102;23.6%&#21644;14.4%&#12290;
&lt;/p&gt;
&lt;p&gt;
Classification trees continue to be widely adopted in machine learning applications due to their inherently interpretable nature and scalability. We propose a rolling subtree lookahead algorithm that combines the relative scalability of the myopic approaches with the foresight of the optimal approaches in constructing trees. The limited foresight embedded in our algorithm mitigates the learning pathology observed in optimal approaches. At the heart of our algorithm lies a novel two-depth optimal binary classification tree formulation flexible to handle any loss function. We show that the feasible region of this formulation is an integral polyhedron, yielding the LP relaxation solution optimal. Through extensive computational analyses, we demonstrate that our approach outperforms optimal and myopic approaches in 808 out of 1330 problem instances, improving the out-of-sample accuracy by up to 23.6% and 14.4%, respectively.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20010;&#20307;&#20844;&#24179;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#30340;&#20272;&#35745;&#26694;&#26550;&#65292;&#20351;&#24471;&#32593;&#32476;&#36755;&#20986;&#22312;&#32473;&#23450;&#23481;&#24525;&#24230;&#20869;&#30340;&#949;-&#30456;&#20284;&#30340;&#36755;&#20837;&#28857;&#20855;&#26377;&#30456;&#21516;&#30340;&#32467;&#26524;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#36817;&#20284;&#36125;&#21494;&#26031;&#25512;&#26029;&#35757;&#32451;&#30340;BNN&#27604;&#30830;&#23450;&#24615;&#27169;&#22411;&#26356;&#20855;&#20010;&#20307;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.10828</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20010;&#20307;&#20844;&#24179;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Individual Fairness in Bayesian Neural Networks. (arXiv:2304.10828v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10828
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20010;&#20307;&#20844;&#24179;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#30340;&#20272;&#35745;&#26694;&#26550;&#65292;&#20351;&#24471;&#32593;&#32476;&#36755;&#20986;&#22312;&#32473;&#23450;&#23481;&#24525;&#24230;&#20869;&#30340;&#949;-&#30456;&#20284;&#30340;&#36755;&#20837;&#28857;&#20855;&#26377;&#30456;&#21516;&#30340;&#32467;&#26524;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#36817;&#20284;&#36125;&#21494;&#26031;&#25512;&#26029;&#35757;&#32451;&#30340;BNN&#27604;&#30830;&#23450;&#24615;&#27169;&#22411;&#26356;&#20855;&#20010;&#20307;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BNNs&#65289;&#20013;&#30340;&#20010;&#20307;&#20844;&#24179;&#24615;&#65288;IF&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#949;-&#948;-&#20010;&#20307;&#20844;&#24179;&#24615;&#27010;&#24565;&#65292;&#35813;&#27010;&#24565;&#35201;&#27714;&#23545;&#20110;&#20219;&#20309;&#19968;&#23545;&#26681;&#25454;&#32473;&#23450;&#30456;&#20284;&#24230;&#24230;&#37327;&#949;-&#30456;&#20284;&#30340;&#36755;&#20837;&#28857;&#65292;BNN&#30340;&#36755;&#20986;&#22312;&#32473;&#23450;&#23481;&#24525;&#24230;&#948;&gt;0&#20869;&#12290;&#25105;&#20204;&#21033;&#29992;&#36755;&#20837;&#31354;&#38388;&#19978;&#30340;&#32479;&#35745;&#25277;&#26679;&#30028;&#38480;&#20197;&#21450;&#23545;&#25239;&#40065;&#26834;&#24615;&#21644;&#20010;&#20307;&#20844;&#24179;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25512;&#23548;&#20986;&#20102;$\epsilon$-$\delta$-IF&#30340;&#31995;&#32479;&#20272;&#35745;&#26694;&#26550;&#65292;&#35774;&#35745;&#20102;Fair-FGSM&#21644;Fair-PGD&#20316;&#20026;&#38024;&#23545;BNN&#30340;&#20840;&#23616;&#20844;&#24179;&#24230;&#25915;&#20987;&#30340;&#25193;&#23637;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#20844;&#24179;&#24615;&#22522;&#20934;&#27979;&#35797;&#30340;&#21508;&#31181;&#19981;&#21516;&#26550;&#26500;&#30340;BNN&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#65292;&#19982;&#20351;&#29992;&#39057;&#29575;&#20027;&#20041;&#25216;&#26415;&#23398;&#20064;&#30340;&#30830;&#23450;&#24615;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#36890;&#36807;&#36817;&#20284;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#35757;&#32451;&#30340;BNNs&#36890;&#24120;&#27604;&#30830;&#23450;&#24615;&#27169;&#22411;&#26356;&#21152;&#20855;&#26377;&#20010;&#20307;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study Individual Fairness (IF) for Bayesian neural networks (BNNs). Specifically, we consider the $\epsilon$-$\delta$-individual fairness notion, which requires that, for any pair of input points that are $\epsilon$-similar according to a given similarity metrics, the output of the BNN is within a given tolerance $\delta&gt;0.$ We leverage bounds on statistical sampling over the input space and the relationship between adversarial robustness and individual fairness to derive a framework for the systematic estimation of $\epsilon$-$\delta$-IF, designing Fair-FGSM and Fair-PGD as global,fairness-aware extensions to gradient-based attacks for BNNs. We empirically study IF of a variety of approximately inferred BNNs with different architectures on fairness benchmarks, and compare against deterministic models learnt using frequentist techniques. Interestingly, we find that BNNs trained by means of approximate Bayesian inference consistently tend to be markedly more individually fair than th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23457;&#35745;&#26694;&#26550;&#65292;&#33021;&#22815;&#20197;&#20840;&#38754;&#30340;&#26041;&#24335;&#35780;&#20272;&#21512;&#25104;&#25968;&#25454;&#21644;AI&#27169;&#22411;&#30340;&#20855;&#20307;&#25928;&#26524;&#65292;&#21253;&#25324;&#20559;&#35265;&#21644;&#27495;&#35270;&#39044;&#38450;&#12289;&#23545;&#30495;&#23454;&#25968;&#25454;&#30340;&#24544;&#23454;&#31243;&#24230;&#12289;&#25928;&#29992;&#12289;&#40065;&#26834;&#24615;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;&#22312;&#22810;&#20010;&#29992;&#20363;&#20013;&#65292;&#23457;&#35745;&#26694;&#26550;&#24179;&#34913;&#20102;&#20449;&#20219;&#21644;&#25928;&#29992;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2304.10819</link><description>&lt;p&gt;
&#21487;&#25511;&#30340;&#20449;&#20219;&#26435;&#34913;&#19979;&#30340;&#21512;&#25104;&#25968;&#25454;&#23457;&#35745;&#19982;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Auditing and Generating Synthetic Data with Controllable Trust Trade-offs. (arXiv:2304.10819v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10819
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23457;&#35745;&#26694;&#26550;&#65292;&#33021;&#22815;&#20197;&#20840;&#38754;&#30340;&#26041;&#24335;&#35780;&#20272;&#21512;&#25104;&#25968;&#25454;&#21644;AI&#27169;&#22411;&#30340;&#20855;&#20307;&#25928;&#26524;&#65292;&#21253;&#25324;&#20559;&#35265;&#21644;&#27495;&#35270;&#39044;&#38450;&#12289;&#23545;&#30495;&#23454;&#25968;&#25454;&#30340;&#24544;&#23454;&#31243;&#24230;&#12289;&#25928;&#29992;&#12289;&#40065;&#26834;&#24615;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;&#22312;&#22810;&#20010;&#29992;&#20363;&#20013;&#65292;&#23457;&#35745;&#26694;&#26550;&#24179;&#34913;&#20102;&#20449;&#20219;&#21644;&#25928;&#29992;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#20013;&#25910;&#38598;&#30340;&#25968;&#25454;&#24448;&#24448;&#23384;&#22312;&#20559;&#24046;&#12289;&#19981;&#24179;&#34913;&#65292;&#24182;&#19988;&#26377;&#27844;&#38706;&#25935;&#24863;&#21644;&#38544;&#31169;&#20449;&#24687;&#30340;&#39118;&#38505;&#12290;&#36825;&#19968;&#20107;&#23454;&#24341;&#21457;&#20102;&#21019;&#24314;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#24819;&#27861;&#65292;&#20197;&#20943;&#36731;&#30495;&#23454;&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#39118;&#38505;&#12289;&#20559;&#35265;&#12289;&#20260;&#23475;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;&#36825;&#20010;&#27010;&#24565;&#20381;&#36182;&#20110;&#29983;&#25104;AI&#27169;&#22411;&#65292;&#20197;&#20135;&#29983;&#19981;&#20559;&#25191;&#12289;&#20445;&#25252;&#38544;&#31169;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#21516;&#26102;&#24544;&#23454;&#20110;&#30495;&#23454;&#25968;&#25454;&#12290;&#22312;&#36825;&#31181;&#26032;&#33539;&#24335;&#20013;&#65292;&#25105;&#20204;&#22914;&#20309;&#30693;&#36947;&#36825;&#31181;&#26041;&#27861;&#26159;&#21542;&#20817;&#29616;&#20102;&#20854;&#25215;&#35834;&#65311;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23457;&#35745;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#23545;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#22522;&#20110;&#23427;&#20204;&#35757;&#32451;&#30340;AI&#27169;&#22411;&#30340;&#20840;&#38754;&#35780;&#20272;&#65292;&#22260;&#32469;&#20559;&#35265;&#21644;&#27495;&#35270;&#30340;&#39044;&#38450;&#12289;&#23545;&#30495;&#23454;&#25968;&#25454;&#30340;&#24544;&#23454;&#31243;&#24230;&#12289;&#25928;&#29992;&#12289;&#40065;&#26834;&#24615;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;&#25105;&#20204;&#36890;&#36807;&#23457;&#35745;&#22810;&#20010;&#29983;&#25104;&#27169;&#22411;&#22312;&#19981;&#21516;&#29992;&#20363;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#21253;&#25324;&#25945;&#32946;&#12289;&#21307;&#30103;&#20445;&#20581;&#12289;&#38134;&#34892;&#12289;&#20154;&#21147;&#36164;&#28304;&#65292;&#20197;&#21450;&#20174;&#34920;&#26684;&#65292;&#26102;&#38388;&#24207;&#21015;&#21040;&#33258;&#28982;&#35821;&#35328;&#30340;&#19981;&#21516;&#27169;&#24577;&#12290;&#25105;&#20204;&#30340;&#29992;&#20363;&#23637;&#31034;&#20102;&#22312;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#20013;&#24179;&#34913;&#20449;&#20219;&#21644;&#25928;&#29992;&#30340;&#26435;&#34913;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data collected from the real world tends to be biased, unbalanced, and at risk of exposing sensitive and private information. This reality has given rise to the idea of creating synthetic datasets to alleviate risk, bias, harm, and privacy concerns inherent in the real data. This concept relies on Generative AI models to produce unbiased, privacy-preserving synthetic data while being true to the real data. In this new paradigm, how can we tell if this approach delivers on its promises? We present an auditing framework that offers a holistic assessment of synthetic datasets and AI models trained on them, centered around bias and discrimination prevention, fidelity to the real data, utility, robustness, and privacy preservation. We showcase our framework by auditing multiple generative models on diverse use cases, including education, healthcare, banking, human resources, and across different modalities, from tabular, to time-series, to natural language. Our use cases demonstrate the imp
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#40065;&#26834;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#35774;&#35745;&#26377;&#24847;&#20041;&#21644;&#21487;&#35299;&#37322;&#30340;&#25552;&#31034;&#38598;&#65292;&#25552;&#39640;&#23567;&#26679;&#26412;&#23398;&#20064;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.10805</link><description>&lt;p&gt;
RPLKG: &#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#40065;&#26834;&#25552;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
RPLKG: Robust Prompt Learning with Knowledge Graph. (arXiv:2304.10805v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10805
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#40065;&#26834;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#35774;&#35745;&#26377;&#24847;&#20041;&#21644;&#21487;&#35299;&#37322;&#30340;&#25552;&#31034;&#38598;&#65292;&#25552;&#39640;&#23567;&#26679;&#26412;&#23398;&#20064;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#21487;&#36801;&#31227;&#30340;&#65292;&#24182;&#19988;&#23545;&#26410;&#30693;&#25968;&#25454;&#38598;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#26368;&#36817;&#65292;&#35832;&#22914;CLIP&#20043;&#31867;&#30340;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#21508;&#31181;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#26174;&#30528;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#28982;&#32780;&#65292;&#24403;&#26631;&#35760;&#25968;&#25454;&#38598;&#26377;&#38480;&#26102;&#65292;&#26032;&#25968;&#25454;&#38598;&#25110;&#39046;&#22495;&#30340;&#27867;&#21270;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#25552;&#39640;&#23567;&#26679;&#26412;&#23398;&#20064;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24050;&#32463;&#36827;&#34892;&#20102;&#21508;&#31181;&#21162;&#21147;&#65292;&#22914;&#25552;&#31034;&#23398;&#20064;&#21644;&#36866;&#37197;&#22120;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#23569;&#26679;&#26412;&#33258;&#36866;&#24212;&#26041;&#27861;&#19981;&#20855;&#22791;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#19988;&#38656;&#35201;&#39640;&#35745;&#31639;&#25104;&#26412;&#26469;&#36827;&#34892;&#33258;&#36866;&#24212;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#40065;&#26834;&#25552;&#31034;&#23398;&#20064;&#65288;RPLKG&#65289;&#12290;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#65292;&#25105;&#20204;&#33258;&#21160;&#35774;&#35745;&#20986;&#21508;&#31181;&#21487;&#35299;&#37322;&#21644;&#26377;&#24847;&#20041;&#30340;&#25552;&#31034;&#38598;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#19968;&#27425;&#27491;&#21521;&#20256;&#36882;&#21518;&#33719;&#24471;&#25552;&#31034;&#38598;&#30340;&#32531;&#23384;&#23884;&#20837;&#12290;&#20043;&#21518;&#65292;&#27169;&#22411;&#20351;&#29992;GumbelSoftmax&#20248;&#21270;&#25552;&#31034;&#36873;&#25321;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale pre-trained models have been known that they are transferable, and they generalize well on the unseen dataset. Recently, multimodal pre-trained models such as CLIP show significant performance improvement in diverse experiments. However, when the labeled dataset is limited, the generalization of a new dataset or domain is still challenging. To improve the generalization performance on few-shot learning, there have been diverse efforts, such as prompt learning and adapter. However, the current few-shot adaptation methods are not interpretable, and they require a high computation cost for adaptation. In this study, we propose a new method, robust prompt learning with knowledge graph (RPLKG). Based on the knowledge graph, we automatically design diverse interpretable and meaningful prompt sets. Our model obtains cached embeddings of prompt sets after one forwarding from a large pre-trained model. After that, model optimizes the prompt selection processes with GumbelSoftmax. In
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#22240;&#32452;&#23398;&#20013;&#30340;&#32463;&#20856;&#21040;&#37327;&#23376;&#25968;&#25454;&#32534;&#30721;&#30340;&#20960;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#20102;&#26080;&#25439;&#21387;&#32553;&#12289;&#23567;&#27874;&#32534;&#30721;&#21644;&#20449;&#24687;&#29109;&#31561;&#31639;&#27861;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#19968;&#31181;&#29992;&#37327;&#23376;&#29627;&#23572;&#20857;&#26364;&#26426;&#27979;&#35797;&#32534;&#30721;DNA&#24207;&#21015;&#30340;&#26041;&#27861;&#65292;&#30456;&#23545;&#20110;&#20256;&#32479;&#26041;&#27861;&#34920;&#29616;&#26356;&#20339;&#12290;</title><link>http://arxiv.org/abs/2304.10786</link><description>&lt;p&gt;
&#22522;&#22240;&#32452;&#23398;&#20013;&#30340;&#32463;&#20856;&#21040;&#37327;&#23376;&#24207;&#21015;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Classical-to-Quantum Sequence Encoding in Genomics. (arXiv:2304.10786v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10786
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#22240;&#32452;&#23398;&#20013;&#30340;&#32463;&#20856;&#21040;&#37327;&#23376;&#25968;&#25454;&#32534;&#30721;&#30340;&#20960;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#20102;&#26080;&#25439;&#21387;&#32553;&#12289;&#23567;&#27874;&#32534;&#30721;&#21644;&#20449;&#24687;&#29109;&#31561;&#31639;&#27861;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#19968;&#31181;&#29992;&#37327;&#23376;&#29627;&#23572;&#20857;&#26364;&#26426;&#27979;&#35797;&#32534;&#30721;DNA&#24207;&#21015;&#30340;&#26041;&#27861;&#65292;&#30456;&#23545;&#20110;&#20256;&#32479;&#26041;&#27861;&#34920;&#29616;&#26356;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
DNA&#27979;&#24207;&#21487;&#30830;&#23450;&#29983;&#29289;&#20010;&#20307;&#30340;&#36951;&#20256;&#32534;&#30721;&#65292;&#22240;&#27492;&#26159;&#21307;&#23398;&#12289;&#29983;&#21629;&#31185;&#23398;&#12289;&#36827;&#21270;&#29983;&#29289;&#23398;&#12289;&#39135;&#21697;&#31185;&#23398;&#21644;&#25216;&#26415;&#20197;&#21450;&#20892;&#19994;&#31561;&#39046;&#22495;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#24037;&#20855;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20960;&#31181;&#21463;&#19981;&#21516;&#25968;&#23398;&#39046;&#22495;&#21551;&#21457;&#30340;&#32463;&#20856;&#21040;&#37327;&#23376;&#25968;&#25454;&#32534;&#30721;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#23558;&#36825;&#20123;&#24605;&#24819;&#24212;&#29992;&#20110;&#29983;&#29289;&#20449;&#24687;&#23398;&#20013;&#12290;&#29305;&#21035;&#22320;&#65292;&#24341;&#20837;&#20102;&#19968;&#20123;&#21551;&#21457;&#20110;&#30005;&#27668;&#19982;&#30005;&#23376;&#24037;&#31243;&#12289;&#20449;&#24687;&#35770;&#12289;&#24494;&#20998;&#20960;&#20309;&#21644;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#23436;&#25972;&#20171;&#32461;&#24050;&#26377;&#30340;&#25968;&#25454;&#32534;&#30721;&#26041;&#26696;&#65292;&#24182;&#23637;&#31034;&#22914;&#20309;&#22312;&#22522;&#22240;&#32452;&#23398;&#20013;&#20351;&#29992;&#23427;&#20204;&#12290;&#36825;&#20123;&#31639;&#27861;&#20351;&#29992;&#20102;&#26080;&#25439;&#21387;&#32553;&#12289;&#23567;&#27874;&#32534;&#30721;&#21644;&#20449;&#24687;&#29109;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#37327;&#23376;&#29627;&#23572;&#20857;&#26364;&#26426;&#27979;&#35797;&#32534;&#30721;DNA&#24207;&#21015;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22522;&#22240;&#27979;&#24207;&#36807;&#31243;&#30340;&#27169;&#25311;&#65292;&#24182;&#34920;&#26126;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#30340;&#32463;&#20856;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
DNA sequencing allows for the determination of the genetic code of an organism, and therefore is an indispensable tool that has applications in Medicine, Life Sciences, Evolutionary Biology, Food Sciences and Technology, and Agriculture. In this paper, we present several novel methods of performing classical-to-quantum data encoding inspired by various mathematical fields, and we demonstrate these ideas within Bioinformatics. In particular, we introduce algorithms that draw inspiration from diverse fields such as Electrical and Electronic Engineering, Information Theory, Differential Geometry, and Neural Network architectures. We provide a complete overview of the existing data encoding schemes and show how to use them in Genomics. The algorithms provided utilise lossless compression, wavelet-based encoding, and information entropy. Moreover, we propose a contemporary method for testing encoded DNA sequences using Quantum Boltzmann Machines. To evaluate the effectiveness of our algorit
&lt;/p&gt;</description></item><item><title>Eyettention&#26159;&#31532;&#19968;&#20010;&#21516;&#26102;&#22788;&#29702;&#35821;&#35328;&#24207;&#21015;&#21644;&#26102;&#38388;&#24207;&#21015;&#30340;&#38405;&#35835;&#27169;&#22411;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#27169;&#25311;&#38405;&#35835;&#32773;&#30340;&#25195;&#35270;&#36335;&#24452;&#65292;&#23545;&#26426;&#22120;&#23398;&#20064;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#20855;&#26377;&#20511;&#37492;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2304.10784</link><description>&lt;p&gt;
Eyettention&#65306;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#21452;&#24207;&#21015;&#27169;&#22411;&#20197;&#39044;&#27979;&#20154;&#31867;&#38405;&#35835;&#26102;&#30340;&#25195;&#35270;&#36335;&#24452;
&lt;/p&gt;
&lt;p&gt;
Eyettention: An Attention-based Dual-Sequence Model for Predicting Human Scanpaths during Reading. (arXiv:2304.10784v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10784
&lt;/p&gt;
&lt;p&gt;
Eyettention&#26159;&#31532;&#19968;&#20010;&#21516;&#26102;&#22788;&#29702;&#35821;&#35328;&#24207;&#21015;&#21644;&#26102;&#38388;&#24207;&#21015;&#30340;&#38405;&#35835;&#27169;&#22411;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#27169;&#25311;&#38405;&#35835;&#32773;&#30340;&#25195;&#35270;&#36335;&#24452;&#65292;&#23545;&#26426;&#22120;&#23398;&#20064;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#20855;&#26377;&#20511;&#37492;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38405;&#35835;&#26102;&#30340;&#30524;&#21160;&#25581;&#31034;&#20102;&#38405;&#35835;&#32773;&#30340;&#35748;&#30693;&#36807;&#31243;&#21644;&#25152;&#38405;&#35835;&#25991;&#26412;&#30340;&#29305;&#24449;&#12290;&#22240;&#27492;&#65292;&#38405;&#35835;&#20013;&#25195;&#35270;&#36335;&#24452;&#30340;&#20998;&#26512;&#24050;&#24341;&#36215;&#21508;&#20010;&#39046;&#22495;&#30340;&#20851;&#27880;&#65292;&#28085;&#30422;&#20102;&#20174;&#35748;&#30693;&#31185;&#23398;&#21040;&#35821;&#35328;&#23398;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#12290;&#28982;&#32780;&#65292;&#27169;&#25311;&#38405;&#35835;&#26102;&#20154;&#31867;&#30340;&#25195;&#35270;&#36335;&#24452;&#30340;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#23427;&#20204;&#26159;&#30001;&#21452;&#24207;&#21015;&#32452;&#25104;&#30340;&#65306;&#21333;&#35789;&#25353;&#29031;&#35821;&#35328;&#30340;&#35821;&#27861;&#35268;&#21017;&#25490;&#24207;&#65292;&#32780;&#27880;&#35270;&#21017;&#25353;&#29031;&#26102;&#38388;&#39034;&#24207;&#25490;&#24207;&#12290;&#20154;&#31867;&#24182;&#19981;&#20005;&#26684;&#25353;&#24038;&#21040;&#21491;&#30340;&#39034;&#24207;&#38405;&#35835;&#65292;&#32780;&#26159;&#36339;&#36807;&#25110;&#37325;&#22797;&#27880;&#35270;&#21333;&#35789;&#65292;&#24182;&#20498;&#36864;&#21040;&#20197;&#21069;&#30340;&#21333;&#35789;&#65292;&#35821;&#35328;&#24207;&#21015;&#21644;&#26102;&#38388;&#24207;&#21015;&#30340;&#23545;&#40784;&#24182;&#19981;&#23481;&#26131;&#12290;&#26412;&#25991;&#24320;&#21457;&#20102;Eyettention&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#21516;&#26102;&#22788;&#29702;&#35821;&#35328;&#24207;&#21015;&#21644;&#26102;&#38388;&#24207;&#21015;&#30340;&#21452;&#24207;&#21015;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Eye movements during reading offer insights into both the reader's cognitive processes and the characteristics of the text that is being read. Hence, the analysis of scanpaths in reading have attracted increasing attention across fields, ranging from cognitive science over linguistics to computer science. In particular, eye-tracking-while-reading data has been argued to bear the potential to make machine-learning-based language models exhibit a more human-like linguistic behavior. However, one of the main challenges in modeling human scanpaths in reading is their dual-sequence nature: the words are ordered following the grammatical rules of the language, whereas the fixations are chronologically ordered. As humans do not strictly read from left-to-right, but rather skip or refixate words and regress to previous words, the alignment of the linguistic and the temporal sequence is non-trivial. In this paper, we develop Eyettention, the first dual-sequence model that simultaneously process
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#27602;&#21270;&#25915;&#20987;&#31574;&#30053;&#65292;&#26082;&#21487;&#20197;&#23454;&#29616;&#25298;&#32477;&#26381;&#21153;(Dos)&#30446;&#26631;&#65292;&#20063;&#21487;&#20197;&#31934;&#30830;&#25511;&#21046;&#20840;&#23616;&#20934;&#30830;&#24615;&#65292;&#20855;&#26377;&#39640;&#25928;&#21644;&#38544;&#24418;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2304.10783</link><description>&lt;p&gt;
&#25298;&#32477;&#26381;&#21153;&#25110;&#32454;&#31890;&#24230;&#25511;&#21046;&#65306;&#38754;&#21521;&#32852;&#37030;&#23398;&#20064;&#30340;&#28789;&#27963;&#27169;&#22411;&#27602;&#21270;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Denial-of-Service or Fine-Grained Control: Towards Flexible Model Poisoning Attacks on Federated Learning. (arXiv:2304.10783v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10783
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#27602;&#21270;&#25915;&#20987;&#31574;&#30053;&#65292;&#26082;&#21487;&#20197;&#23454;&#29616;&#25298;&#32477;&#26381;&#21153;(Dos)&#30446;&#26631;&#65292;&#20063;&#21487;&#20197;&#31934;&#30830;&#25511;&#21046;&#20840;&#23616;&#20934;&#30830;&#24615;&#65292;&#20855;&#26377;&#39640;&#25928;&#21644;&#38544;&#24418;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#23481;&#26131;&#21463;&#21040;&#27602;&#21270;&#25915;&#20987;&#65292;&#25932;&#23545;&#26041;&#20250;&#30772;&#22351;&#20840;&#23616;&#32858;&#21512;&#32467;&#26524;&#24182;&#36896;&#25104;&#25298;&#32477;&#26381;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#27169;&#22411;&#27602;&#21270;&#25915;&#20987;(FMPA)&#65292;&#26088;&#22312;&#23454;&#29616;&#22810;&#21151;&#33021;&#25915;&#20987;&#30446;&#26631;&#12290;&#26412;&#25991;&#32771;&#34385;&#22914;&#19979;&#23454;&#38469;&#24773;&#26223;&#65306;&#25932;&#23545;&#26041;&#27809;&#26377;&#20851;&#20110;FL&#31995;&#32479;&#30340;&#39069;&#22806;&#20449;&#24687;&#65288;&#20363;&#22914;&#65292;&#32858;&#21512;&#35268;&#21017;&#25110;&#33391;&#24615;&#35774;&#22791;&#19978;&#30340;&#26356;&#26032;&#65289;&#12290;FMPA&#21033;&#29992;&#20840;&#23616;&#21382;&#21490;&#20449;&#24687;&#26500;&#24314;&#20272;&#35745;&#22120;&#65292;&#23558;&#19979;&#19968;&#36718;&#20840;&#23616;&#27169;&#22411;&#39044;&#27979;&#20026;&#33391;&#24615;&#21442;&#32771;&#27169;&#22411;&#65292;&#24182;&#24494;&#35843;&#21442;&#32771;&#27169;&#22411;&#20197;&#33719;&#24471;&#25152;&#38656;&#30340;&#31934;&#24230;&#20302;&#21644;&#25200;&#21160;&#23567;&#30340;&#27602;&#21270;&#27169;&#22411;&#12290;FMPA&#19981;&#20165;&#21487;&#20197;&#36798;&#21040;DoS&#30340;&#30446;&#26631;&#65292;&#36824;&#21487;&#20197;&#33258;&#28982;&#22320;&#25193;&#23637;&#21040;&#21551;&#21160;&#32454;&#31890;&#24230;&#21487;&#25511;&#25915;&#20987;&#65292;&#20174;&#32780;&#31934;&#30830;&#38477;&#20302;&#20840;&#23616;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#36827;&#19968;&#27493;&#25506;&#32034;&#20102;FMPA&#22312;&#20960;&#31181;FL&#22330;&#26223;&#19979;&#30340;&#25915;&#20987;&#24615;&#33021;&#65292;&#21253;&#25324;&#20108;&#20803;&#20998;&#31867;&#21644;&#22270;&#20687;&#20998;&#31867;&#65292;&#22312;&#19981;&#21516;&#30340;&#25915;&#20987;&#30446;&#26631;&#21644;&#25915;&#20987;&#30693;&#35782;&#27700;&#24179;&#19979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;FMPA&#21487;&#20197;&#26377;&#25928;&#32780;&#39640;&#25928;&#22320;&#23454;&#29616;&#25152;&#38656;&#30340;&#25915;&#20987;&#30446;&#26631;&#65292;&#21516;&#26102;&#20445;&#25345;&#38544;&#24418;&#21644;&#19981;&#21487;&#24863;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is vulnerable to poisoning attacks, where adversaries corrupt the global aggregation results and cause denial-of-service (DoS). Unlike recent model poisoning attacks that optimize the amplitude of malicious perturbations along certain prescribed directions to cause DoS, we propose a Flexible Model Poisoning Attack (FMPA) that can achieve versatile attack goals. We consider a practical threat scenario where no extra knowledge about the FL system (e.g., aggregation rules or updates on benign devices) is available to adversaries. FMPA exploits the global historical information to construct an estimator that predicts the next round of the global model as a benign reference. It then fine-tunes the reference model to obtain the desired poisoned model with low accuracy and small perturbations. Besides the goal of causing DoS, FMPA can be naturally extended to launch a fine-grained controllable attack, making it possible to precisely reduce the global accuracy. Armed wi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#25506;&#32034;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;DEIR&#65292;&#20511;&#21161;&#21306;&#20998;&#24615;&#27169;&#22411;&#23454;&#29616;&#29702;&#35770;&#19978;&#23548;&#20986;&#30340;&#20869;&#22312;&#22870;&#21169;&#65292;&#33021;&#22815;&#39640;&#25928;&#19988;&#40065;&#26834;&#22320;&#36827;&#34892;&#25506;&#32034;&#65292;&#36866;&#29992;&#20110;&#38754;&#23545;&#22806;&#37096;&#22870;&#21169;&#31232;&#30095;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2304.10770</link><description>&lt;p&gt;
DEIR: &#22522;&#20110;&#21306;&#20998;&#24615;&#27169;&#22411;&#30340;&#24773;&#33410;&#20869;&#22312;&#22870;&#21169;&#65292;&#39640;&#25928;&#19988;&#40065;&#26834;&#30340;&#25506;&#32034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DEIR: Efficient and Robust Exploration through Discriminative-Model-Based Episodic Intrinsic Rewards. (arXiv:2304.10770v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10770
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#25506;&#32034;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;DEIR&#65292;&#20511;&#21161;&#21306;&#20998;&#24615;&#27169;&#22411;&#23454;&#29616;&#29702;&#35770;&#19978;&#23548;&#20986;&#30340;&#20869;&#22312;&#22870;&#21169;&#65292;&#33021;&#22815;&#39640;&#25928;&#19988;&#40065;&#26834;&#22320;&#36827;&#34892;&#25506;&#32034;&#65292;&#36866;&#29992;&#20110;&#38754;&#23545;&#22806;&#37096;&#22870;&#21169;&#31232;&#30095;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#26159;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#26041;&#38754;&#65292;&#20854;&#26377;&#25928;&#24615;&#20851;&#38190;&#22320;&#24433;&#21709;&#30528;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#38754;&#23545;&#31232;&#30095;&#30340;&#22806;&#37096;&#22870;&#21169;&#26102;&#26356;&#20026;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20174;&#35266;&#27979;&#20013;&#20272;&#35745;&#26032;&#39062;&#24615;&#30340;&#20869;&#22312;&#22870;&#21169;&#21487;&#20197;&#26377;&#25928;&#40723;&#21169;&#25506;&#32034;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29615;&#22659;&#30340;&#38543;&#26426;&#24615;&#20197;&#21450;&#20195;&#29702;&#30340;&#34892;&#20026;&#21487;&#33021;&#20250;&#24433;&#21709;&#35266;&#23519;&#32467;&#26524;&#65292;&#22240;&#27492;&#19968;&#20010;&#35266;&#27979;&#30340;&#26032;&#39062;&#24615;&#19982;&#25506;&#32034;&#20043;&#38388;&#23384;&#22312;&#24046;&#36317;&#12290;&#20026;&#20102;&#20934;&#30830;&#20272;&#35745;&#25506;&#32034;&#34892;&#20026;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DEIR&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#25105;&#20204;&#20174;&#26465;&#20214;&#20114;&#20449;&#24687;&#39033;&#20013;&#29702;&#35770;&#19978;&#23548;&#20986;&#20869;&#22312;&#22870;&#21169;&#65292;&#35813;&#22870;&#21169;&#20027;&#35201;&#19982;&#20195;&#29702;&#30340;&#25506;&#32034;&#34892;&#20026;&#25152;&#36129;&#29486;&#30340;&#26032;&#39062;&#24615;&#25104;&#27604;&#20363;&#65292;&#24182;&#20511;&#21161;&#21306;&#20998;&#24615;&#30340;&#21069;&#21521;&#27169;&#22411;&#23454;&#29616;&#22870;&#21169;&#12290;&#25105;&#20204;&#22312;MiniGrid&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#21253;&#25324;&#26631;&#20934;&#21644;&#30828;&#26680;&#25506;&#32034;&#28216;&#25103;&#65292;&#22312;&#32467;&#26524;&#19978;DEIR&#27604;&#22522;&#32447;&#23398;&#20064;&#26356;&#24555;&#24182;&#19988;&#20855;&#26377;&#26356;&#39640;&#30340;&#25104;&#21151;&#29575;&#21644;&#40065;&#26834;&#24615;&#65292;&#36866;&#24212;&#29615;&#22659;&#21160;&#24577;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploration is a fundamental aspect of reinforcement learning (RL), and its effectiveness crucially decides the performance of RL algorithms, especially when facing sparse extrinsic rewards. Recent studies showed the effectiveness of encouraging exploration with intrinsic rewards estimated from novelty in observations. However, there is a gap between the novelty of an observation and an exploration in general, because the stochasticity in the environment as well as the behavior of an agent may affect the observation. To estimate exploratory behaviors accurately, we propose DEIR, a novel method where we theoretically derive an intrinsic reward from a conditional mutual information term that principally scales with the novelty contributed by agent explorations, and materialize the reward with a discriminative forward model. We conduct extensive experiments in both standard and hardened exploration games in MiniGrid to show that DEIR quickly learns a better policy than baselines. Our eval
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#34920;&#31034;&#30456;&#20284;&#24615;&#65292;&#21457;&#29616;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#32534;&#30721;&#22120;&#34920;&#31034;&#26159;&#36890;&#29992;&#30340;&#12289;&#35299;&#30721;&#22120;&#34920;&#31034;&#26159;&#29305;&#23450;&#30340;&#12290;&#38024;&#23545;&#36825;&#19968;&#21457;&#29616;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22312;&#36801;&#31227;&#23398;&#20064;&#20013;&#22914;&#20309;&#36873;&#25321;&#23545;VAE&#21738;&#20123;&#32452;&#20214;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35270;&#21270;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.10767</link><description>&lt;p&gt;
&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#22312;&#36801;&#31227;&#23398;&#20064;&#20013;&#34920;&#29616;&#22914;&#20309;&#65311;
&lt;/p&gt;
&lt;p&gt;
How good are variational autoencoders at transfer learning?. (arXiv:2304.10767v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10767
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#34920;&#31034;&#30456;&#20284;&#24615;&#65292;&#21457;&#29616;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#32534;&#30721;&#22120;&#34920;&#31034;&#26159;&#36890;&#29992;&#30340;&#12289;&#35299;&#30721;&#22120;&#34920;&#31034;&#26159;&#29305;&#23450;&#30340;&#12290;&#38024;&#23545;&#36825;&#19968;&#21457;&#29616;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22312;&#36801;&#31227;&#23398;&#20064;&#20013;&#22914;&#20309;&#36873;&#25321;&#23545;VAE&#21738;&#20123;&#32452;&#20214;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35270;&#21270;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#34987;&#29992;&#20110;&#35832;&#22810;&#39046;&#22495;&#30340;&#36801;&#31227;&#23398;&#20064;&#65292;&#20363;&#22914;&#38899;&#20048;&#29983;&#25104;&#21644;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#12290;&#20294;&#26159;&#65292;&#22312;&#36801;&#31227;&#21069;&#65292;&#27809;&#26377;&#19968;&#31181;&#21407;&#21017;&#24615;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#21738;&#20123;&#32452;&#20214;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#65292;&#25110;&#32773;&#36801;&#31227;&#23398;&#20064;&#26159;&#21542;&#26377;&#21487;&#33021;&#22312;&#30446;&#26631;&#20219;&#21153;&#19978;&#36215;&#21040;&#24110;&#21161;&#20316;&#29992;&#12290;&#26412;&#25991;&#36890;&#36807;&#34920;&#31034;&#30456;&#20284;&#24615;&#30340;&#35270;&#35282;&#26469;&#25506;&#35752;&#36825;&#20010;&#38382;&#39064;&#65292;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;&#20013;&#24515;&#26680;&#23545;&#40784;&#26041;&#27861;&#65288;CKA&#65289;&#26469;&#35780;&#20272;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;VAE&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#34920;&#26126;&#32534;&#30721;&#22120;&#30340;&#34920;&#31034;&#26159;&#36890;&#29992;&#30340;&#65292;&#32780;&#35299;&#30721;&#22120;&#30340;&#34920;&#31034;&#26159;&#29305;&#23450;&#30340;&#12290;&#22522;&#20110;&#36825;&#20123;&#35748;&#35782;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#36873;&#25321;VAE&#21738;&#20123;&#32452;&#20214;&#37325;&#26032;&#35757;&#32451;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35270;&#21270;&#35780;&#20272;&#36801;&#31227;&#23398;&#20064;&#26159;&#21542;&#26377;&#21487;&#33021;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#26377;&#24110;&#21161;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational autoencoders (VAEs) are used for transfer learning across various research domains such as music generation or medical image analysis. However, there is no principled way to assess before transfer which components to retrain or whether transfer learning is likely to help on a target task. We propose to explore this question through the lens of representational similarity. Specifically, using Centred Kernel Alignment (CKA) to evaluate the similarity of VAEs trained on different datasets, we show that encoders' representations are generic but decoders' specific. Based on these insights, we discuss the implications for selecting which components of a VAE to retrain and propose a method to visually assess whether transfer learning is likely to help on classification tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#22810;&#27169;&#24577;&#35821;&#20041;&#20998;&#21106;&#26694;&#26550;&#65292;&#21487;&#20197;&#25552;&#39640;&#26631;&#31614;&#25928;&#29575;&#65292;&#22686;&#24378;&#27169;&#22411;&#40065;&#26834;&#24615;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#26426;&#21046;&#21644;&#25945;&#24072;&#21322;&#30417;&#30563;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#22312;&#32570;&#22833;&#27169;&#24577;&#19979;&#20855;&#22791;&#36739;&#24378;&#30340;&#40065;&#26834;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.10756</link><description>&lt;p&gt;
&#21322;&#30417;&#30563;&#22810;&#27169;&#24577;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#32570;&#22833;&#27169;&#24577;&#40065;&#26834;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Missing Modality Robustness in Semi-Supervised Multi-Modal Semantic Segmentation. (arXiv:2304.10756v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10756
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#22810;&#27169;&#24577;&#35821;&#20041;&#20998;&#21106;&#26694;&#26550;&#65292;&#21487;&#20197;&#25552;&#39640;&#26631;&#31614;&#25928;&#29575;&#65292;&#22686;&#24378;&#27169;&#22411;&#40065;&#26834;&#24615;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#26426;&#21046;&#21644;&#25945;&#24072;&#21322;&#30417;&#30563;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#22312;&#32570;&#22833;&#27169;&#24577;&#19979;&#20855;&#22791;&#36739;&#24378;&#30340;&#40065;&#26834;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35821;&#20041;&#20998;&#21106;&#20013;&#20351;&#29992;&#22810;&#20010;&#31354;&#38388;&#27169;&#24577;&#24050;&#34987;&#35777;&#26126;&#26377;&#21161;&#20110;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#20013;&#36824;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#65292;&#38656;&#35201;&#35299;&#20915;&#65306;(a) &#25552;&#39640;&#26631;&#31614;&#25928;&#29575;&#65307;(b) &#22312;&#27979;&#35797;&#26102;&#27169;&#24577;&#32570;&#22833;&#30340;&#24773;&#20917;&#19979;&#22686;&#24378;&#27169;&#22411;&#40065;&#26834;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#26426;&#21046;&#32447;&#24615;&#34701;&#21512;&#65292;&#21363;&#20351;&#22312;&#26377;&#38480;&#30417;&#30563;&#19979;&#65292;&#34920;&#29616;&#20063;&#27604;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#26356;&#22909;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;M3L&#65306;Masked Modality Learning&#30340;&#22810;&#27169;&#24577;&#25945;&#24072;&#21322;&#30417;&#30563;&#26694;&#26550;&#65292;&#23427;&#19981;&#20165;&#25552;&#39640;&#20102;&#22810;&#27169;&#24577;&#24615;&#33021;&#65292;&#36824;&#20351;&#29992;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#20351;&#27169;&#22411;&#22312;&#29616;&#23454;&#24773;&#20917;&#19979;&#23545;&#32570;&#22833;&#27169;&#24577;&#26356;&#21152;&#40065;&#26834;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#31532;&#19968;&#20010;&#21322;&#30417;&#30563;&#22810;&#27169;&#24577;&#35821;&#20041;&#20998;&#21106;&#22522;&#20934;&#65292;&#24182;&#25253;&#21578;&#20102;&#23545;&#32570;&#22833;&#27169;&#24577;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#25552;&#35758;&#22312;&#21512;&#25104;&#21644;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#22312;&#40065;&#26834;mIoU&#19978;&#27604;&#26368;&#37325;&#35201;&#30340;&#22522;&#32447;&#26377;&#26368;&#22810;10&#65285;&#30340;&#32477;&#23545;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Using multiple spatial modalities has been proven helpful in improving semantic segmentation performance. However, there are several real-world challenges that have yet to be addressed: (a) improving label efficiency and (b) enhancing robustness in realistic scenarios where modalities are missing at the test time. To address these challenges, we first propose a simple yet efficient multi-modal fusion mechanism Linear Fusion, that performs better than the state-of-the-art multi-modal models even with limited supervision. Second, we propose M3L: Multi-modal Teacher for Masked Modality Learning, a semi-supervised framework that not only improves the multi-modal performance but also makes the model robust to the realistic missing modality scenario using unlabeled data. We create the first benchmark for semi-supervised multi-modal semantic segmentation and also report the robustness to missing modalities. Our proposal shows an absolute improvement of up to 10% on robust mIoU above the most 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#22810;&#27169;&#24577;&#30340;&#28145;&#24230;&#23398;&#20064;&#34701;&#21512;&#25216;&#26415;&#22312;&#20449;&#29992;&#35780;&#32423;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#34701;&#21512;&#31574;&#30053;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#32452;&#21512;&#65292;&#35777;&#26126;&#20102;&#19968;&#20010;&#22522;&#20110;CNN&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#36890;&#36807;&#20004;&#31181;&#34701;&#21512;&#31574;&#30053;&#20248;&#20110;&#20854;&#20182;&#22810;&#27169;&#24577;&#25216;&#26415;&#65292;&#21516;&#26102;&#22312;&#27604;&#36739;&#31616;&#21333;&#21644;&#22797;&#26434;&#30340;&#27169;&#22411;&#20013;&#21457;&#29616;&#65292;&#26356;&#22797;&#26434;&#30340;&#27169;&#22411;&#24182;&#19981;&#19968;&#23450;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2304.10740</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#30340;&#20449;&#29992;&#35780;&#32423;&#39044;&#27979;&#26041;&#27861;&#30740;&#31350;&#8212;&#8212;&#20197;&#25991;&#26412;&#21644;&#25968;&#23383;&#25968;&#25454;&#27969;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Multi-Modal Deep Learning for Credit Rating Prediction Using Text and Numerical Data Streams. (arXiv:2304.10740v1 [q-fin.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10740
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#22810;&#27169;&#24577;&#30340;&#28145;&#24230;&#23398;&#20064;&#34701;&#21512;&#25216;&#26415;&#22312;&#20449;&#29992;&#35780;&#32423;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#34701;&#21512;&#31574;&#30053;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#32452;&#21512;&#65292;&#35777;&#26126;&#20102;&#19968;&#20010;&#22522;&#20110;CNN&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#36890;&#36807;&#20004;&#31181;&#34701;&#21512;&#31574;&#30053;&#20248;&#20110;&#20854;&#20182;&#22810;&#27169;&#24577;&#25216;&#26415;&#65292;&#21516;&#26102;&#22312;&#27604;&#36739;&#31616;&#21333;&#21644;&#22797;&#26434;&#30340;&#27169;&#22411;&#20013;&#21457;&#29616;&#65292;&#26356;&#22797;&#26434;&#30340;&#27169;&#22411;&#24182;&#19981;&#19968;&#23450;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20102;&#35299;&#20449;&#29992;&#35780;&#32423;&#20998;&#37197;&#20013;&#21738;&#20123;&#22240;&#32032;&#26159;&#37325;&#35201;&#30340;&#21487;&#20197;&#24110;&#21161;&#20570;&#20986;&#26356;&#22909;&#30340;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#25991;&#29486;&#30340;&#37325;&#28857;&#22823;&#22810;&#38598;&#20013;&#22312;&#32467;&#26500;&#21270;&#25968;&#25454;&#19978;&#65292;&#36739;&#23569;&#30740;&#31350;&#38750;&#32467;&#26500;&#21270;&#25110;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#26512;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#19981;&#21516;&#31867;&#22411;&#25968;&#25454;&#38598;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#34701;&#21512;&#30340;&#26377;&#25928;&#26550;&#26500;&#65292;&#20197;&#39044;&#27979;&#20844;&#21496;&#20449;&#29992;&#35780;&#32423;&#26631;&#20934;&#12290;&#22312;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#19981;&#21516;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21450;&#34701;&#21512;&#31574;&#30053;&#30340;&#32452;&#21512;&#65292;&#21253;&#25324;CNN&#65292;LSTM&#65292;GRU&#21644;BERT&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25968;&#25454;&#34701;&#21512;&#31574;&#30053;&#65288;&#21253;&#25324;&#26089;&#26399;&#21644;&#20013;&#38388;&#34701;&#21512;&#65289;&#20197;&#21450;&#25216;&#26415;&#65288;&#21253;&#25324;&#20018;&#32852;&#21644;&#20132;&#21449;&#27880;&#24847;&#65289;&#31561;&#26041;&#38754;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19968;&#20010;&#22522;&#20110;CNN&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#36890;&#36807;&#20004;&#31181;&#34701;&#21512;&#31574;&#30053;&#20248;&#20110;&#20854;&#20182;&#22810;&#27169;&#24577;&#25216;&#26415;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#27604;&#36739;&#31616;&#21333;&#30340;&#26550;&#26500;&#19982;&#26356;&#22797;&#26434;&#30340;&#26550;&#26500;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#26356;&#22797;&#26434;&#30340;&#27169;&#22411;&#24182;&#19981;&#19968;&#23450;&#33021;&#22312;&#20449;&#29992;&#35780;&#32423;&#39044;&#27979;&#20013;&#21457;&#25381;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowing which factors are significant in credit rating assignment leads to better decision-making. However, the focus of the literature thus far has been mostly on structured data, and fewer studies have addressed unstructured or multi-modal datasets. In this paper, we present an analysis of the most effective architectures for the fusion of deep learning models for the prediction of company credit rating classes, by using structured and unstructured datasets of different types. In these models, we tested different combinations of fusion strategies with different deep learning models, including CNN, LSTM, GRU, and BERT. We studied data fusion strategies in terms of level (including early and intermediate fusion) and techniques (including concatenation and cross-attention). Our results show that a CNN-based multi-modal model with two fusion strategies outperformed other multi-modal techniques. In addition, by comparing simple architectures with more complex ones, we found that more soph
&lt;/p&gt;</description></item><item><title>KitchenScale&#26159;&#19968;&#20010;&#32463;&#36807;Fine-tuned&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#65292;&#21487;&#26681;&#25454;&#39135;&#35889;&#19978;&#19979;&#25991;&#39044;&#27979;&#30446;&#26631;&#25104;&#20998;&#30340;&#25968;&#37327;&#21644;&#27979;&#37327;&#21333;&#20301;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#31163;&#25955;&#28508;&#22312;&#25351;&#25968;&#65288;DExp&#65289;&#26041;&#27861;&#22788;&#29702;&#39135;&#35889;&#35821;&#26009;&#24211;&#20013;&#25968;&#23383;&#23610;&#24230;&#30340;&#39640;&#26041;&#24046;&#65292;&#23581;&#35797;&#20174;&#39135;&#35889;&#25991;&#26412;&#21040;PLMs&#30340;&#36716;&#31227;&#23398;&#20064;&#12290;&#22312;&#26032;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#21644;&#25512;&#33616;&#31034;&#20363;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;KitchenScale&#20855;&#26377;&#27867;&#21270;&#24615;&#24182;&#21487;&#20197;&#29702;&#35299;&#21508;&#31181;&#39135;&#35889;&#35821;&#22659;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#20010;Web&#24212;&#29992;&#31243;&#24207;&#26469;&#20026;&#29992;&#25143;&#25552;&#20379;&#25152;&#38656;&#30340;&#39135;&#21697;&#37327;&#30340;&#37197;&#26041;&#29305;&#23450;&#30340;&#27979;&#37327;&#21333;&#20301;&#12290;</title><link>http://arxiv.org/abs/2304.10739</link><description>&lt;p&gt;
KitchenScale: &#20174;&#39135;&#35889;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#39044;&#27979;&#25104;&#20998;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
KitchenScale: Learning to predict ingredient quantities from recipe contexts. (arXiv:2304.10739v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10739
&lt;/p&gt;
&lt;p&gt;
KitchenScale&#26159;&#19968;&#20010;&#32463;&#36807;Fine-tuned&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#65292;&#21487;&#26681;&#25454;&#39135;&#35889;&#19978;&#19979;&#25991;&#39044;&#27979;&#30446;&#26631;&#25104;&#20998;&#30340;&#25968;&#37327;&#21644;&#27979;&#37327;&#21333;&#20301;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#31163;&#25955;&#28508;&#22312;&#25351;&#25968;&#65288;DExp&#65289;&#26041;&#27861;&#22788;&#29702;&#39135;&#35889;&#35821;&#26009;&#24211;&#20013;&#25968;&#23383;&#23610;&#24230;&#30340;&#39640;&#26041;&#24046;&#65292;&#23581;&#35797;&#20174;&#39135;&#35889;&#25991;&#26412;&#21040;PLMs&#30340;&#36716;&#31227;&#23398;&#20064;&#12290;&#22312;&#26032;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#21644;&#25512;&#33616;&#31034;&#20363;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;KitchenScale&#20855;&#26377;&#27867;&#21270;&#24615;&#24182;&#21487;&#20197;&#29702;&#35299;&#21508;&#31181;&#39135;&#35889;&#35821;&#22659;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#20010;Web&#24212;&#29992;&#31243;&#24207;&#26469;&#20026;&#29992;&#25143;&#25552;&#20379;&#25152;&#38656;&#30340;&#39135;&#21697;&#37327;&#30340;&#37197;&#26041;&#29305;&#23450;&#30340;&#27979;&#37327;&#21333;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28921;&#39274;&#23454;&#36341;&#20013;&#65292;&#30830;&#23450;&#25104;&#20998;&#30340;&#36866;&#24403;&#37327;&#23545;&#20110;&#20016;&#23500;&#21475;&#24863;&#21644;&#20419;&#36827;&#20581;&#24247;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;KitchenScale&#65292;&#36825;&#26159;&#19968;&#20010;&#32463;&#36807;Fine-tuned&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#65292;&#26681;&#25454;&#39135;&#35889;&#19978;&#19979;&#25991;&#39044;&#27979;&#30446;&#26631;&#25104;&#20998;&#30340;&#25968;&#37327;&#21644;&#27979;&#37327;&#21333;&#20301;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#35757;&#32451;&#25105;&#20204;&#30340;KitchenScale&#27169;&#22411;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#21253;&#25324;&#19977;&#20010;&#23376;&#20219;&#21153;&#30340;&#25104;&#20998;&#37327;&#39044;&#27979;&#20219;&#21153;&#65292;&#36825;&#20123;&#23376;&#20219;&#21153;&#26159;&#25104;&#20998;&#27979;&#37327;&#31867;&#22411;&#20998;&#31867;&#12289;&#21333;&#20301;&#20998;&#31867;&#21644;&#25968;&#37327;&#22238;&#24402;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#20174;&#39135;&#35889;&#25991;&#26412;&#21040;PLMs&#30340;&#28921;&#39274;&#30693;&#35782;&#30340;&#36716;&#31227;&#23398;&#20064;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#31163;&#25955;&#28508;&#22312;&#25351;&#25968;&#65288;DExp&#65289;&#26041;&#27861;&#26469;&#24212;&#23545;&#39135;&#35889;&#35821;&#26009;&#24211;&#20013;&#25968;&#23383;&#23610;&#24230;&#30340;&#39640;&#26041;&#24046;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#26032;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#21644;&#25512;&#33616;&#31034;&#20363;&#36827;&#34892;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;KitchenScale&#29702;&#35299;&#21508;&#31181;&#39135;&#35889;&#35821;&#22659;&#20197;&#21450;&#22312;&#39044;&#27979;&#25104;&#20998;&#37327;&#26041;&#38754;&#20855;&#26377;&#27867;&#21270;&#24615;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#20010;Web&#24212;&#29992;&#31243;&#24207;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#25152;&#38656;&#30340;&#29992;&#20110;&#25152;&#38656;&#20154;&#25968;&#39135;&#21697;&#37327;&#30340;&#37197;&#26041;&#29305;&#23450;&#30340;&#27979;&#37327;&#21333;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;
Determining proper quantities for ingredients is an essential part of cooking practice from the perspective of enriching tastiness and promoting healthiness. We introduce KitchenScale, a fine-tuned Pre-trained Language Model (PLM) that predicts a target ingredient's quantity and measurement unit given its recipe context. To effectively train our KitchenScale model, we formulate an ingredient quantity prediction task that consists of three sub-tasks which are ingredient measurement type classification, unit classification, and quantity regression task. Furthermore, we utilized transfer learning of cooking knowledge from recipe texts to PLMs. We adopted the Discrete Latent Exponent (DExp) method to cope with high variance of numerical scales in recipe corpora. Experiments with our newly constructed dataset and recommendation examples demonstrate KitchenScale's understanding of various recipe contexts and generalizability in predicting ingredient quantities. We implemented a web applicati
&lt;/p&gt;</description></item><item><title>SCooLS&#26159;&#19968;&#20010;&#26234;&#33021;&#21512;&#32422;&#23398;&#20064;&#24341;&#25806;&#65292;&#20351;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#30452;&#25509;&#20998;&#26512;&#20197;&#22826;&#22346;&#21512;&#32422;&#23383;&#33410;&#30721;&#24182;&#35782;&#21035;&#26131;&#21463;&#25915;&#20987;&#30340;&#21151;&#33021;&#65292;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#24037;&#20855;&#65292;&#20934;&#30830;&#24230;&#39640;&#36798;98.4%&#65292;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#28431;&#27934;&#20998;&#26512;&#22120;&#12290;</title><link>http://arxiv.org/abs/2304.10737</link><description>&lt;p&gt;
&#21033;&#29992;&#24858;&#34850;&#21512;&#21516;&#36827;&#34892;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Schooling to Exploit Foolish Contracts. (arXiv:2304.10737v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10737
&lt;/p&gt;
&lt;p&gt;
SCooLS&#26159;&#19968;&#20010;&#26234;&#33021;&#21512;&#32422;&#23398;&#20064;&#24341;&#25806;&#65292;&#20351;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#30452;&#25509;&#20998;&#26512;&#20197;&#22826;&#22346;&#21512;&#32422;&#23383;&#33410;&#30721;&#24182;&#35782;&#21035;&#26131;&#21463;&#25915;&#20987;&#30340;&#21151;&#33021;&#65292;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#24037;&#20855;&#65292;&#20934;&#30830;&#24230;&#39640;&#36798;98.4%&#65292;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#28431;&#27934;&#20998;&#26512;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;SCooLS&#65292;&#21363;&#25105;&#20204;&#30340;&#26234;&#33021;&#21512;&#32422;&#23398;&#20064;&#65288;&#21322;&#30417;&#30563;&#65289;&#24341;&#25806;&#12290;SCooLS&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#20998;&#26512;&#20197;&#22826;&#22346;&#21512;&#32422;&#23383;&#33410;&#30721;&#24182;&#35782;&#21035;&#29305;&#23450;&#30340;&#26131;&#21463;&#25915;&#20987;&#30340;&#21151;&#33021;&#12290;SCooLS&#21253;&#25324;&#20004;&#20010;&#20851;&#38190;&#20803;&#32032;&#65306;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#12290;&#21322;&#30417;&#30563;&#23398;&#20064;&#27604;&#26080;&#30417;&#30563;&#23398;&#20064;&#20135;&#29983;&#26356;&#20934;&#30830;&#30340;&#27169;&#22411;&#65292;&#21516;&#26102;&#19981;&#38656;&#35201;&#22823;&#22411;&#30340;&#26631;&#35760;&#35757;&#32451;&#38598;&#65292;&#32780;&#26377;&#30417;&#30563;&#23398;&#20064;&#21017;&#38656;&#35201;&#12290;GNN&#20351;&#24471;&#21487;&#20197;&#30452;&#25509;&#20998;&#26512;&#26234;&#33021;&#21512;&#32422;&#23383;&#33410;&#30721;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#25163;&#21160;&#29305;&#24449;&#24037;&#31243;&#12289;&#39044;&#23450;&#20041;&#30340;&#27169;&#24335;&#25110;&#19987;&#23478;&#35268;&#21017;&#12290;SCooLS&#26159;&#21322;&#30417;&#30563;&#23398;&#20064;&#24212;&#29992;&#20110;&#26234;&#33021;&#21512;&#32422;&#28431;&#27934;&#20998;&#26512;&#30340;&#39318;&#20010;&#24212;&#29992;&#65292;&#20063;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#28431;&#27934;&#20998;&#26512;&#22120;&#65292;&#21487;&#20197;&#35782;&#21035;&#29305;&#23450;&#26131;&#21463;&#25915;&#20987;&#30340;&#21151;&#33021;&#12290;SCooLS&#30340;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#24037;&#20855;&#65292;&#20934;&#30830;&#24230;&#36798;&#21040;&#20102;98.4%&#65292;F1&#24471;&#20998;&#36798;&#21040;&#20102;90.5%&#65292;&#20551;&#38451;&#24615;&#29575;&#20165;&#20026;0.8%&#12290;&#27492;&#22806;&#65292;SCooLS&#36895;&#24230;&#24456;&#24555;&#65292;
&lt;/p&gt;
&lt;p&gt;
We introduce SCooLS, our Smart Contract Learning (Semi-supervised) engine. SCooLS uses neural networks to analyze Ethereum contract bytecode and identifies specific vulnerable functions. SCooLS incorporates two key elements: semi-supervised learning and graph neural networks (GNNs). Semi-supervised learning produces more accurate models than unsupervised learning, while not requiring the large oracle-labeled training set that supervised learning requires. GNNs enable direct analysis of smart contract bytecode without any manual feature engineering, predefined patterns, or expert rules.  SCooLS is the first application of semi-supervised learning to smart contract vulnerability analysis, as well as the first deep learning-based vulnerability analyzer to identify specific vulnerable functions. SCooLS's performance is better than existing tools, with an accuracy level of 98.4%, an F1 score of 90.5%, and an exceptionally low false positive rate of only 0.8%. Furthermore, SCooLS is fast, an
&lt;/p&gt;</description></item><item><title>DLVA&#26159;&#19968;&#31181;&#29992;&#20110;&#20197;&#22826;&#22346;&#26234;&#33021;&#21512;&#32422;&#30340;&#24378;&#22823;&#28145;&#24230;&#23398;&#20064;&#28431;&#27934;&#26816;&#27979;&#24037;&#20855;&#65292;&#20854;&#31639;&#27861;&#28085;&#30422;&#20102;&#28304;&#20195;&#30721;&#21040;&#23383;&#33410;&#30721;&#30340;&#25193;&#23637;&#65292;&#24182;&#19988;&#36895;&#24230;&#27604;&#20256;&#32479;&#28431;&#27934;&#26816;&#27979;&#24037;&#20855;&#25552;&#39640;&#20102;10-500&#20493;&#65292;&#24182;&#25104;&#21151;&#22320;&#21457;&#29616;&#20102;&#19968;&#20123;Slither&#35823;&#26631;&#35760;&#30340;&#26131;&#21463;&#25915;&#20987;&#30340;&#21512;&#32422;&#12290;</title><link>http://arxiv.org/abs/2304.10726</link><description>&lt;p&gt;
&#26234;&#33021;&#23398;&#20064;&#21457;&#29616; &#24858;&#31528;&#21512;&#32422;
&lt;/p&gt;
&lt;p&gt;
Smart Learning to Find Dumb Contracts. (arXiv:2304.10726v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10726
&lt;/p&gt;
&lt;p&gt;
DLVA&#26159;&#19968;&#31181;&#29992;&#20110;&#20197;&#22826;&#22346;&#26234;&#33021;&#21512;&#32422;&#30340;&#24378;&#22823;&#28145;&#24230;&#23398;&#20064;&#28431;&#27934;&#26816;&#27979;&#24037;&#20855;&#65292;&#20854;&#31639;&#27861;&#28085;&#30422;&#20102;&#28304;&#20195;&#30721;&#21040;&#23383;&#33410;&#30721;&#30340;&#25193;&#23637;&#65292;&#24182;&#19988;&#36895;&#24230;&#27604;&#20256;&#32479;&#28431;&#27934;&#26816;&#27979;&#24037;&#20855;&#25552;&#39640;&#20102;10-500&#20493;&#65292;&#24182;&#25104;&#21151;&#22320;&#21457;&#29616;&#20102;&#19968;&#20123;Slither&#35823;&#26631;&#35760;&#30340;&#26131;&#21463;&#25915;&#20987;&#30340;&#21512;&#32422;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#24378;&#22823;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340; Deep Learning Vulnerability Analyzer &#65288;DLVA&#65289;&#65292;&#23427;&#26159;&#19968;&#31181;&#38024;&#23545;&#20197;&#23383;&#33410;&#30721;&#20026;&#22522;&#30784;&#30340;&#20197;&#22826;&#22346;&#26234;&#33021;&#21512;&#32422;&#30340;&#28431;&#27934;&#26816;&#27979;&#24037;&#20855;&#12290;&#25105;&#20204;&#22312;&#27809;&#26377;&#25163;&#21160;&#29305;&#24449;&#24037;&#31243;&#12289;&#39044;&#23450;&#20041;&#27169;&#24335;&#25110;&#19987;&#23478;&#35268;&#21017;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;&#28304;&#20195;&#30721;&#20998;&#26512;&#25193;&#23637;&#21040;&#23383;&#33410;&#30721;&#65292;&#35757;&#32451;DLVA&#21028;&#26029;&#23383;&#33410;&#30721;&#12290;DLVA&#35757;&#32451;&#31639;&#27861;&#30340;&#40065;&#26834;&#24615;&#20063;&#24456;&#24378;&#65306;&#23427;&#20811;&#26381;&#20102;1.25%&#35823;&#26631;&#35760;&#21512;&#32422;&#30340;&#38169;&#35823;&#29575;&#65292;&#23398;&#29983;&#36229;&#36234;&#20102;&#32769;&#24072;&#65292;&#24182;&#21457;&#29616;&#20102;Slither&#35823;&#26631;&#35760;&#30340;&#26131;&#21463;&#25915;&#20987;&#30340;&#21512;&#32422;&#12290;DLVA&#27604;&#22522;&#20110;&#24418;&#24335;&#26041;&#27861;&#30340;&#20256;&#32479;&#26234;&#33021;&#21512;&#32422;&#28431;&#27934;&#26816;&#27979;&#24037;&#20855;&#24555;&#24471;&#22810;&#65306;DLVA&#26816;&#26597;&#20102;29&#20010;&#28431;&#27934;&#25152;&#38656;&#30340;&#26102;&#38388;&#20026;0.2&#31186;&#65292;&#36895;&#24230;&#25552;&#39640;&#20102;10-500&#20493;&#12290;DLVA&#26377;&#19977;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65306;Smart Contract to Vector&#65288;SC2Vec&#65289;&#23558;&#26234;&#33021;&#21512;&#32422;&#36716;&#25442;&#20026;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21521;&#37327;&#34920;&#31034;&#12290;Bytecode Tokenizer&#65288;BCT&#65289;&#23558;&#24213;&#23618;&#23383;&#33410;&#30721;&#36716;&#25442;&#20026;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#24847;&#20041;&#30340;&#26631;&#35760;&#65292;DLVA&#26159;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21487;&#39044;&#27979;&#26234;&#33021;&#21512;&#32422;&#26159;&#21542;&#21253;&#21547;&#28431;&#27934;&#12290;&#25105;&#20204;&#23545;Etherscan&#30340;28,505&#20010;&#32463;&#36807;&#39564;&#35777;&#30340;&#26234;&#33021;&#21512;&#32422;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;DLVA&#35780;&#20272;&#65292;&#21457;&#29616;&#23427;&#21462;&#24471;&#20102;0.964&#30340;AUC&#65288;&#30495;&#38451;&#29575;/&#20551;&#38451;&#29575;&#26354;&#32447;&#19979;&#30340;&#38754;&#31215;&#65289;&#24471;&#20998;&#12290;&#19982;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;DLVA&#22312;F1&#20998;&#25968;&#19978;&#26174;&#31034;&#20102;30.7%&#30340;&#25913;&#36827;&#65292;&#23427;&#26159;&#31934;&#24230;&#21644;&#21484;&#22238;&#30340;&#35843;&#21644;&#24179;&#22343;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Deep Learning Vulnerability Analyzer (DLVA), a vulnerability detection tool for Ethereum smart contracts based on powerful deep learning techniques for sequential data adapted for bytecode. We train DLVA to judge bytecode even though the supervising oracle, Slither, can only judge source code. DLVA's training algorithm is general: we "extend" a source code analysis to bytecode without any manual feature engineering, predefined patterns, or expert rules. DLVA's training algorithm is also robust: it overcame a 1.25% error rate mislabeled contracts, and the student surpassing the teacher; found vulnerable contracts that Slither mislabeled. In addition to extending a source code analyzer to bytecode, DLVA is much faster than conventional tools for smart contract vulnerability detection based on formal methods: DLVA checks contracts for 29 vulnerabilities in 0.2 seconds, a speedup of 10-500x+ compared to traditional tools.  DLVA has three key components. Smart Contract to Vecto
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#20132;&#36890;&#36335;&#32593;&#20013;&#32570;&#23569;&#20256;&#24863;&#22120;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#34917;&#20805;&#27969;&#37327;&#29366;&#24577;&#25110;&#29366;&#24577;&#21644;&#21160;&#20316;&#26469;&#23454;&#29616;&#33258;&#36866;&#24212;&#25511;&#21046;&#21644;&#26465;&#20214;&#34701;&#21512;&#12290;</title><link>http://arxiv.org/abs/2304.10722</link><description>&lt;p&gt;
&#32570;&#22833;&#25968;&#25454;&#19979;&#30340;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning Approaches for Traffic Signal Control under Missing Data. (arXiv:2304.10722v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10722
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#20132;&#36890;&#36335;&#32593;&#20013;&#32570;&#23569;&#20256;&#24863;&#22120;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#34917;&#20805;&#27969;&#37327;&#29366;&#24577;&#25110;&#29366;&#24577;&#21644;&#21160;&#20316;&#26469;&#23454;&#29616;&#33258;&#36866;&#24212;&#25511;&#21046;&#21644;&#26465;&#20214;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#24050;&#32463;&#21462;&#24471;&#20102;&#27604;&#20256;&#32479;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#20013;&#20132;&#36890;&#29366;&#24577;&#30340;&#32570;&#22833;&#21487;&#33021;&#32463;&#24120;&#21457;&#29983;&#65292;&#20351;&#24471;&#29616;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#32570;&#23569;&#20256;&#24863;&#22120;&#30340;&#36335;&#32593;&#19978;&#26080;&#27861;&#24212;&#29992;&#12290;&#26412;&#25991;&#26088;&#22312;&#25511;&#21046;&#20132;&#36890;&#20449;&#21495;&#22312;&#29616;&#23454;&#29615;&#22659;&#19979;&#30340;&#35774;&#32622;&#20013;&#65292;&#20854;&#20013;&#19968;&#20123;&#36335;&#21475;&#27809;&#26377;&#23433;&#35013;&#20256;&#24863;&#22120;&#65292;&#22240;&#27492;&#21608;&#22260;&#27809;&#26377;&#30452;&#25509;&#35266;&#23519;&#25968;&#25454;&#12290;&#22312;&#25105;&#20204;&#25152;&#30693;&#36947;&#30340;&#33539;&#22260;&#20869;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#38382;&#39064;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#35299;&#20915;&#26041;&#26696;&#65306;&#31532;&#19968;&#31181;&#26041;&#26696;&#34917;&#20805;&#27969;&#37327;&#29366;&#24577;&#20197;&#23454;&#29616;&#33258;&#36866;&#24212;&#25511;&#21046;&#65292;&#31532;&#20108;&#31181;&#26041;&#26696;&#34917;&#20805;&#29366;&#24577;&#21644;&#21160;&#20316;&#20197;&#36827;&#34892;&#26465;&#20214;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of reinforcement learning (RL) methods in traffic signal control tasks has achieved better performance than conventional rule-based approaches. Most RL approaches require the observation of the environment for the agent to decide which action is optimal for a long-term reward. However, in real-world urban scenarios, missing observation of traffic states may frequently occur due to the lack of sensors, which makes existing RL methods inapplicable on road networks with missing observation. In this work, we aim to control the traffic signals in a real-world setting, where some of the intersections in the road network are not installed with sensors and thus with no direct observations around them. To the best of our knowledge, we are the first to use RL methods to tackle the traffic signal control problem in this real-world setting. Specifically, we propose two solutions: the first one imputes the traffic states to enable adaptive control, and the second one imputes both stat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#20998;&#35010;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#22788;&#29702;&#25968;&#25454;&#39537;&#21160;&#21644;&#26080;&#25968;&#25454;&#38382;&#39064;&#65292;&#24182;&#19988;&#33021;&#22815;&#24555;&#36895;&#27714;&#35299;&#21487;&#21387;&#32553;&#30340;&#27973;&#27700;&#26041;&#31243;&#21644;&#19981;&#21487;&#21387;&#32553;&#30340;N-S&#26041;&#31243;</title><link>http://arxiv.org/abs/2304.10717</link><description>&lt;p&gt;
&#24212;&#29992;&#22522;&#20110;&#29305;&#24449;&#20998;&#35010;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#27714;&#35299;Navier-Stokes&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Physics-informed Neural Network Combined with Characteristic-Based Split for Solving Navier-Stokes Equations. (arXiv:2304.10717v1 [physics.flu-dyn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10717
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#20998;&#35010;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#22788;&#29702;&#25968;&#25454;&#39537;&#21160;&#21644;&#26080;&#25968;&#25454;&#38382;&#39064;&#65292;&#24182;&#19988;&#33021;&#22815;&#24555;&#36895;&#27714;&#35299;&#21487;&#21387;&#32553;&#30340;&#27973;&#27700;&#26041;&#31243;&#21644;&#19981;&#21487;&#21387;&#32553;&#30340;N-S&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#20998;&#35010;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#27714;&#35299;&#26102;&#38388;&#20381;&#36182;&#30340;Navier-Stokes&#26041;&#31243;&#12290;&#35813;&#26041;&#27861;&#23558;&#36755;&#20986;&#21442;&#25968;&#21644;&#30456;&#24212;&#30340;&#25439;&#22833;&#20540;&#20998;&#31163;&#65292;&#20351;&#36755;&#20986;&#21442;&#25968;&#38388;&#30340;&#26435;&#37325;&#19981;&#34987;&#32771;&#34385;&#12290;&#19981;&#26159;&#25152;&#26377;&#30340;&#20559;&#23548;&#25968;&#37117;&#21442;&#19982;&#26799;&#24230;&#21453;&#21521;&#20256;&#25773;&#65292;&#20854;&#20313;&#39033;&#20250;&#34987;&#37325;&#22797;&#20351;&#29992;&#12290;&#22240;&#27492;&#65292;&#19982;&#20256;&#32479;&#30340;PINN&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#26356;&#24555;&#36895;&#12290;&#26412;&#26041;&#27861;&#23558;&#26631;&#31614;&#25968;&#25454;&#12289;&#29289;&#29702;&#32422;&#26463;&#21644;&#32593;&#32476;&#36755;&#20986;&#35270;&#20026;&#20808;&#39564;&#20449;&#24687;&#65292;&#23558;N-S&#26041;&#31243;&#30340;&#27531;&#24046;&#35270;&#20026;&#21518;&#39564;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#23427;&#21487;&#20197;&#22788;&#29702;&#25968;&#25454;&#39537;&#21160;&#21644;&#26080;&#25968;&#25454;&#38382;&#39064;&#12290;&#32467;&#26524;&#65292;&#23427;&#33021;&#22815;&#27714;&#35299;&#21487;&#21387;&#32553;&#30340;&#27973;&#27700;&#26041;&#31243;&#21644;&#19981;&#21487;&#21387;&#32553;&#30340;N-S&#26041;&#31243;&#12290;&#30001;&#20110;&#36793;&#30028;&#26465;&#20214;&#24050;&#30693;&#65292;&#35813;&#26041;&#27861;&#21482;&#38656;&#35201;&#22312;&#29305;&#23450;&#26102;&#38388;&#28857;&#33719;&#24471;&#27969;&#22330;&#20449;&#24687;&#21363;&#21487;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, physics-informed neural network (PINN) based on characteristic-based split (CBS) is proposed, which can be used to solve the time-dependent Navier-Stokes equations (N-S equations). In this method, The output parameters and corresponding losses are separated, so the weights between output parameters are not considered. Not all partial derivatives participate in gradient backpropagation, and the remaining terms will be reused.Therefore, compared with traditional PINN, this method is a rapid version. Here, labeled data, physical constraints and network outputs are regarded as priori information, and the residuals of the N-S equations are regarded as posteriori information. So this method can deal with both data-driven and data-free problems. As a result, it can solve the special form of compressible N-S equations -- -Shallow-Water equations, and incompressible N-S equations. As boundary conditions are known, this method only needs the flow field information at a certain tim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25345;&#32493;&#35757;&#32451;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;&#25193;&#25955;&#36741;&#21161; EBM&#65292;&#21487;&#20197;&#21516;&#26102;&#23454;&#29616;&#38271;&#26399;&#31283;&#23450;&#24615;&#12289;&#35757;&#32451;&#21518;&#30340;&#22270;&#20687;&#29983;&#25104;&#21644;&#20248;&#36234;&#30340;&#36234;&#30028;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2304.10707</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#33021;&#37327;&#27169;&#22411;&#30340;&#25345;&#32493;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Persistently Trained, Diffusion-assisted Energy-based Models. (arXiv:2304.10707v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10707
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25345;&#32493;&#35757;&#32451;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;&#25193;&#25955;&#36741;&#21161; EBM&#65292;&#21487;&#20197;&#21516;&#26102;&#23454;&#29616;&#38271;&#26399;&#31283;&#23450;&#24615;&#12289;&#35757;&#32451;&#21518;&#30340;&#22270;&#20687;&#29983;&#25104;&#21644;&#20248;&#36234;&#30340;&#36234;&#30028;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#37327;&#27169;&#22411; (EBMs) &#30340;&#26368;&#22823;&#20284;&#28982; (ML) &#23398;&#20064;&#24456;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#37096;&#20998;&#21407;&#22240;&#22312;&#20110;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#30340;&#19981;&#25910;&#25947;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181; ML &#23398;&#20064;&#30340;&#21464;&#20307;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#37117;&#26410;&#33021;&#21516;&#26102;&#23454;&#29616;&#35757;&#32451;&#21518;&#30340;&#22270;&#20687;&#29983;&#25104;&#21644;&#21512;&#36866;&#30340;&#23494;&#24230;&#20272;&#35745;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#24341;&#20837;&#25193;&#25955;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#22686;&#24378;&#30340;&#37319;&#26679;&#31639;&#27861;&#36827;&#34892;&#25345;&#32493;&#35757;&#32451; (&#21363;&#20351;&#29992;&#25345;&#32493;&#30340;&#23545;&#27604;&#25955;&#24230;)&#65292;&#26469;&#23398;&#20064;&#19968;&#20010;&#31216;&#20026;&#25193;&#25955;&#36741;&#21161; EBM &#30340;&#32852;&#21512; EBM&#65292;&#20197;&#20415;&#20174;&#22797;&#26434;&#30340;&#12289;&#22810;&#23792;&#30340;&#20998;&#24067;&#20013;&#36827;&#34892;&#36866;&#24403;&#30340;&#37319;&#26679;&#12290;&#25105;&#20204;&#22312;&#20108;&#32500;&#30340;&#31034;&#20363;&#23454;&#39564;&#21644;&#22270;&#20687;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#32467;&#26524;&#65292;&#24182;&#35777;&#26126;&#20102;&#38024;&#23545;&#22270;&#20687;&#25968;&#25454;&#65292;&#25345;&#32493;&#35757;&#32451;&#30340; EBM &#21487;&#20197;&#21516;&#26102;&#23454;&#29616;&#38271;&#26399;&#31283;&#23450;&#24615;&#12289;&#35757;&#32451;&#21518;&#30340;&#22270;&#20687;&#29983;&#25104;&#21644;&#20248;&#36234;&#30340;&#36234;&#30028;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Maximum likelihood (ML) learning for energy-based models (EBMs) is challenging, partly due to non-convergence of Markov chain Monte Carlo.Several variations of ML learning have been proposed, but existing methods all fail to achieve both post-training image generation and proper density estimation. We propose to introduce diffusion data and learn a joint EBM, called diffusion assisted-EBMs, through persistent training (i.e., using persistent contrastive divergence) with an enhanced sampling algorithm to properly sample from complex, multimodal distributions. We present results from a 2D illustrative experiment and image experiments and demonstrate that, for the first time for image data, persistently trained EBMs can {\it simultaneously} achieve long-run stability, post-training image generation, and superior out-of-distribution detection.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; InterSAD &#26041;&#27861;&#65292;&#21033;&#29992;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#27169;&#25311;&#20132;&#20114;&#24335;&#31995;&#32479;&#65292;&#24182;&#36890;&#36807;&#25214;&#21040;&#26377;&#25928;&#30340;&#28608;&#27963;&#20449;&#21495;&#21644;&#23454;&#26102;&#20132;&#20114;&#65292;&#35299;&#20915;&#20102;&#31995;&#32479;&#32423;&#24322;&#24120;&#26816;&#27979;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.10704</link><description>&lt;p&gt;
&#20132;&#20114;&#24335;&#31995;&#32479;&#32423;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Interactive System-wise Anomaly Detection. (arXiv:2304.10704v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10704
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; InterSAD &#26041;&#27861;&#65292;&#21033;&#29992;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#27169;&#25311;&#20132;&#20114;&#24335;&#31995;&#32479;&#65292;&#24182;&#36890;&#36807;&#25214;&#21040;&#26377;&#25928;&#30340;&#28608;&#27963;&#20449;&#21495;&#21644;&#23454;&#26102;&#20132;&#20114;&#65292;&#35299;&#20915;&#20102;&#31995;&#32479;&#32423;&#24322;&#24120;&#26816;&#27979;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#21457;&#25381;&#30528;&#22522;&#30784;&#24615;&#30340;&#20316;&#29992;&#65292;&#20854;&#30446;&#30340;&#26159;&#25214;&#21040;&#21253;&#21547;&#19981;&#21516;&#20110;&#22823;&#22810;&#25968;&#30340;&#29305;&#24449;&#27169;&#24335;&#30340;&#25968;&#25454;&#23454;&#20363;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24456;&#38590;&#22788;&#29702;&#20854;&#20013;&#23454;&#20363;&#26159;&#31995;&#32479;&#30340;&#24773;&#20917;&#65292;&#22240;&#20026;&#31995;&#32479;&#30340;&#29305;&#24449;&#19981;&#26131;&#35266;&#23519;&#20316;&#20026;&#25968;&#25454;&#12290;&#38656;&#35201;&#36866;&#24403;&#30340;&#20132;&#20114;&#26469;&#19982;&#31995;&#32479;&#36827;&#34892;&#20132;&#20114;&#24182;&#35782;&#21035;&#37027;&#20123;&#20855;&#26377;&#24322;&#24120;&#21709;&#24212;&#30340;&#31995;&#32479;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20132;&#20114;&#24335;&#31995;&#32479;&#32423;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#26469;&#27169;&#25311;&#20132;&#20114;&#24335;&#31995;&#32479;&#65292;&#24182;&#23450;&#20041;&#20102;&#31995;&#32479;&#32423;&#24322;&#24120;&#26816;&#27979;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36890;&#36807;&#25214;&#21040;&#26377;&#25928;&#30340;&#28608;&#27963;&#20449;&#21495;&#26469;&#19982;&#31995;&#32479;&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#36890;&#36807;&#23454;&#26102;&#20132;&#20114;&#30830;&#20445;&#31283;&#23450;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection, where data instances are discovered containing feature patterns different from the majority, plays a fundamental role in various applications. However, it is challenging for existing methods to handle the scenarios where the instances are systems whose characteristics are not readily observed as data. Appropriate interactions are needed to interact with the systems and identify those with abnormal responses. Detecting system-wise anomalies is a challenging task due to several reasons including: how to formally define the system-wise anomaly detection problem; how to find the effective activation signal for interacting with systems to progressively collect the data and learn the detector; how to guarantee stable training in such a non-stationary scenario with real-time interactions? To address the challenges, we propose InterSAD (Interactive System-wise Anomaly Detection). Specifically, first, we adopt Markov decision process to model the interactive systems, and defi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25512;&#23548;&#38142;&#27491;&#30830;&#24615;&#21644;&#20449;&#24687;&#37327;&#30340;&#25512;&#29702;&#38142;&#35780;&#20272;&#26694;&#26550;ReCEval&#65292;&#29992;&#20197;&#35780;&#20272;&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#23458;&#35266;&#12289;&#31995;&#32479;&#21644;&#20934;&#30830;&#22320;&#35780;&#20272;&#25512;&#29702;&#38142;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.10703</link><description>&lt;p&gt;
&#36890;&#36807;&#27491;&#30830;&#24615;&#21644;&#20449;&#24687;&#37327;&#35780;&#20272;&#25512;&#29702;&#38142;&#30340;ReCEval
&lt;/p&gt;
&lt;p&gt;
ReCEval: Evaluating Reasoning Chains via Correctness and Informativeness. (arXiv:2304.10703v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25512;&#23548;&#38142;&#27491;&#30830;&#24615;&#21644;&#20449;&#24687;&#37327;&#30340;&#25512;&#29702;&#38142;&#35780;&#20272;&#26694;&#26550;ReCEval&#65292;&#29992;&#20197;&#35780;&#20272;&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#23458;&#35266;&#12289;&#31995;&#32479;&#21644;&#20934;&#30830;&#22320;&#35780;&#20272;&#25512;&#29702;&#38142;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#37117;&#26159;&#22522;&#30784;&#65292;&#20294;&#20160;&#20040;&#26500;&#25104;&#22909;&#30340;&#25512;&#29702;&#38142;&#20197;&#21450;&#22914;&#20309;&#35780;&#20272;&#23427;&#20204;&#23578;&#19981;&#28165;&#26970;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20165;&#20851;&#27880;&#25512;&#29702;&#38142;&#26159;&#21542;&#23548;&#33268;&#27491;&#30830;&#30340;&#32467;&#35770;&#65292;&#20294;&#36825;&#31181;&#20197;&#31572;&#26696;&#20026;&#23548;&#21521;&#30340;&#35266;&#28857;&#21487;&#33021;&#20250;&#23558;&#22909;&#30340;&#25512;&#29702;&#36136;&#37327;&#19982;&#20854;&#20182;&#29992;&#20110;&#39044;&#27979;&#31572;&#26696;&#30340;&#20551;&#25463;&#24452;&#28151;&#28102;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#23558;&#25512;&#29702;&#38142;&#35270;&#20026;&#25512;&#23548;&#26368;&#32456;&#31572;&#26696;&#30340;&#38750;&#27491;&#24335;&#35777;&#26126;&#65292;&#36890;&#36807;&#35780;&#20272;&#25512;&#29702;&#38142;&#30340;&#20004;&#20010;&#20851;&#38190;&#29305;&#24615;&#8212;&#8212;&#65288;1&#65289;&#27491;&#30830;&#24615;&#65292;&#21363;&#27599;&#20010;&#27493;&#39588;&#22522;&#20110;&#27493;&#39588;&#65292;&#21069;&#32622;&#27493;&#39588;&#21644;&#36755;&#20837;&#19978;&#19979;&#25991;&#20013;&#21253;&#21547;&#30340;&#20449;&#24687;&#36827;&#34892;&#26377;&#25928;&#25512;&#29702;&#65292;&#20197;&#21450;&#65288;2&#65289;&#20449;&#24687;&#37327;&#65292;&#21363;&#27599;&#20010;&#27493;&#39588;&#25552;&#20379;&#26032;&#20449;&#24687;&#26377;&#21161;&#20110;&#25512;&#23548;&#29983;&#25104;&#30340;&#31572;&#26696;&#8212;&#8212;&#25105;&#20204;&#25552;&#20986;&#20102;ReCEval&#65288;&#25512;&#29702;&#38142;&#35780;&#20272;&#65289;&#26694;&#26550;&#12290;&#25105;&#20204;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#27169;&#22411;&#21644;&#20449;&#24687;&#29702;&#35770;&#27979;&#37327;&#23454;&#29616;&#20102;ReCEval&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#35780;&#20272;&#25512;&#29702;&#38142;&#26041;&#38754;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#21152;&#23458;&#35266;&#12289;&#31995;&#32479;&#21644;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-step reasoning ability is fundamental to many natural language tasks, yet it is unclear what constitutes a good reasoning chain and how to evaluate them. Most existing methods focus solely on whether the reasoning chain leads to the correct conclusion, but this answer-oriented view may confound the quality of reasoning with other spurious shortcuts to predict the answer. To bridge this gap, we evaluate reasoning chains by viewing them as informal proofs that derive the final answer. Specifically, we propose ReCEval (Reasoning Chain Evaluation), a framework that evaluates reasoning chains through two key properties: (1) correctness, i.e., each step makes a valid inference based on the information contained within the step, preceding steps, and input context, and (2) informativeness, i.e., each step provides new information that is helpful towards deriving the generated answer. We implement ReCEval using natural language inference models and information-theoretic measures. On multi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#22522;&#20110;&#23454;&#38469;&#36816;&#34892;&#25968;&#25454;&#65292;&#20998;&#26512;&#20102;&#30005;&#21147;&#31995;&#32479;&#34892;&#20026;&#27169;&#24335;&#21450;&#27169;&#22411;&#27867;&#21270;&#39118;&#38505;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#24573;&#30053;&#30005;&#32593;&#29305;&#23450;&#27169;&#24335;&#21487;&#33021;&#23548;&#33268;&#23545;&#26032;&#36755;&#20837;&#36755;&#20986;&#19981;&#21487;&#34892;&#12289;&#19981;&#21487;&#23454;&#29616;&#25110;&#23436;&#20840;&#26080;&#24847;&#20041;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2304.10702</link><description>&lt;p&gt;
&#30005;&#21147;&#31995;&#32479;&#34892;&#20026;&#27169;&#24335;&#21450;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#27867;&#21270;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Power Grid Behavioral Patterns and Risks of Generalization in Applied Machine Learning. (arXiv:2304.10702v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10702
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#22522;&#20110;&#23454;&#38469;&#36816;&#34892;&#25968;&#25454;&#65292;&#20998;&#26512;&#20102;&#30005;&#21147;&#31995;&#32479;&#34892;&#20026;&#27169;&#24335;&#21450;&#27169;&#22411;&#27867;&#21270;&#39118;&#38505;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#24573;&#30053;&#30005;&#32593;&#29305;&#23450;&#27169;&#24335;&#21487;&#33021;&#23548;&#33268;&#23545;&#26032;&#36755;&#20837;&#36755;&#20986;&#19981;&#21487;&#34892;&#12289;&#19981;&#21487;&#23454;&#29616;&#25110;&#23436;&#20840;&#26080;&#24847;&#20041;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20986;&#29616;&#20102;&#22823;&#37327;&#38024;&#23545;&#30005;&#21147;&#31995;&#32479;&#24212;&#29992;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#30340;&#25991;&#29486;&#12290;&#28982;&#32780;&#65292;&#24573;&#30053;&#39046;&#22495;&#30693;&#35782;&#20250;&#23545;&#36825;&#20123;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#36896;&#25104;&#39640;&#39118;&#38505;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#24573;&#30053;&#30005;&#32593;&#29305;&#23450;&#30340;&#26102;&#31354;&#27169;&#24335;&#65288;&#36127;&#33655;&#12289;&#21457;&#30005;&#21644;&#25299;&#25169;&#31561;&#65289;&#21487;&#33021;&#23548;&#33268;&#23545;&#26032;&#36755;&#20837;&#36755;&#20986;&#19981;&#21487;&#34892;&#12289;&#19981;&#21487;&#23454;&#29616;&#25110;&#23436;&#20840;&#26080;&#24847;&#20041;&#30340;&#39044;&#27979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#35843;&#26597;&#23454;&#38469;&#36816;&#34892;&#25968;&#25454;&#65292;&#25552;&#20379;&#30005;&#21147;&#31995;&#32479;&#34892;&#20026;&#27169;&#24335;&#30340;&#35265;&#35299;&#65292;&#21253;&#25324;&#26102;&#21464;&#30340;&#25299;&#25169;&#12289;&#36127;&#33655;&#21644;&#21457;&#30005;&#20197;&#21450;&#21333;&#20010;&#36127;&#33655;&#21644;&#21457;&#30005;&#20043;&#38388;&#30340;&#31354;&#38388;&#24046;&#24322;&#65288;&#22312;&#23792;&#20540;&#26102;&#38388;&#12289;&#22810;&#31181;&#39118;&#26684;&#19979;&#65289;&#12290;&#28982;&#21518;&#26681;&#25454;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#65292;&#35780;&#20272;&#20102;&#19968;&#20123;&#29616;&#26377;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#27169;&#22411;&#35774;&#35745;&#21644;&#35757;&#32451;&#20013;&#24573;&#30053;&#36825;&#20123;&#30005;&#32593;&#29305;&#23450;&#27169;&#24335;&#25152;&#36896;&#25104;&#30340;&#27867;&#21270;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have seen a rich literature of data-driven approaches designed for power grid applications. However, insufficient consideration of domain knowledge can impose a high risk to the practicality of the methods. Specifically, ignoring the grid-specific spatiotemporal patterns (in load, generation, and topology, etc.) can lead to outputting infeasible, unrealizable, or completely meaningless predictions on new inputs. To address this concern, this paper investigates real-world operational data to provide insights into power grid behavioral patterns, including the time-varying topology, load, and generation, as well as the spatial differences (in peak hours, diverse styles) between individual loads and generations. Then based on these observations, we evaluate the generalization risks in some existing ML works causedby ignoring these grid-specific patterns in model design and training.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21305;&#37197;&#30340;&#29983;&#25104;&#27169;&#22411;&#25968;&#25454;&#20272;&#20540;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#20219;&#20309;&#29983;&#25104;&#27169;&#22411;&#30340;&#27169;&#22411;&#26080;&#20851;&#26041;&#27861;&#65292;&#21487;&#20197;&#23545;&#25968;&#25454;&#23454;&#20363;&#36827;&#34892;&#20272;&#20540;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#22312;&#20272;&#20540;&#25928;&#26524;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2304.10701</link><description>&lt;p&gt;
&#22522;&#20110;&#21305;&#37197;&#30340;&#29983;&#25104;&#27169;&#22411;&#25968;&#25454;&#20272;&#20540;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Matching-based Data Valuation for Generative Model. (arXiv:2304.10701v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21305;&#37197;&#30340;&#29983;&#25104;&#27169;&#22411;&#25968;&#25454;&#20272;&#20540;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#20219;&#20309;&#29983;&#25104;&#27169;&#22411;&#30340;&#27169;&#22411;&#26080;&#20851;&#26041;&#27861;&#65292;&#21487;&#20197;&#23545;&#25968;&#25454;&#23454;&#20363;&#36827;&#34892;&#20272;&#20540;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#22312;&#20272;&#20540;&#25928;&#26524;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20272;&#20540;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#26377;&#21161;&#20110;&#22686;&#24378;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#24182;&#20445;&#25252;&#25968;&#25454;&#29305;&#24615;&#12290;&#29616;&#26377;&#30340;&#25968;&#25454;&#20272;&#20540;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#21028;&#21035;&#27169;&#22411;&#19978;&#65292;&#24573;&#30053;&#20102;&#26368;&#36817;&#21560;&#24341;&#20102;&#22823;&#37327;&#20851;&#27880;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#12290;&#19982;&#21028;&#21035;&#27169;&#22411;&#31867;&#20284;&#65292;&#38656;&#35201;&#35780;&#20272;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#20013;&#25968;&#25454;&#36129;&#29486;&#30340;&#32039;&#36843;&#38656;&#27714;&#20063;&#23384;&#22312;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#25968;&#25454;&#20272;&#20540;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#21028;&#21035;&#27169;&#22411;&#24615;&#33021;&#25351;&#26631;&#65292;&#24182;&#38656;&#35201;&#23545;&#27169;&#22411;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#19981;&#33021;&#22312;&#23454;&#38469;&#20013;&#30452;&#25509;&#39640;&#25928;&#22320;&#24212;&#29992;&#20110;&#36817;&#26399;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#20363;&#22914;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#25193;&#25955;&#27169;&#22411;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#20174;&#30456;&#20284;&#24615;&#21305;&#37197;&#30340;&#35282;&#24230;&#23545;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#20272;&#20540;&#38382;&#39064;&#36827;&#34892;&#20102;&#26500;&#24314;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;Generative Model Valuator&#8221;&#65288;GMValuator&#65289;&#8212;&#8212;&#31532;&#19968;&#20010;&#38024;&#23545;&#20219;&#20309;&#29983;&#25104;&#27169;&#22411;&#30340;&#27169;&#22411;&#26080;&#20851;&#26041;&#27861;&#65292;&#26088;&#22312;&#20026;&#29983;&#25104;&#27169;&#22411;&#25552;&#20379;&#25968;&#25454;&#20272;&#20540;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#25968;&#25454;&#23454;&#20363;&#21450;&#30001;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#30340;&#30456;&#24212;&#21512;&#25104;&#23454;&#20363;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#26469;&#20272;&#35745;&#21407;&#22987;&#25968;&#25454;&#30340;&#20215;&#20540;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20026;&#19981;&#21516;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;&#21253;&#25324;GAN&#21644;&#25193;&#25955;&#27169;&#22411;&#65289;&#35780;&#20272;&#25968;&#25454;&#23454;&#20363;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data valuation is critical in machine learning, as it helps enhance model transparency and protect data properties. Existing data valuation methods have primarily focused on discriminative models, neglecting deep generative models that have recently gained considerable attention. Similar to discriminative models, there is an urgent need to assess data contributions in deep generative models as well. However, previous data valuation approaches mainly relied on discriminative model performance metrics and required model retraining. Consequently, they cannot be applied directly and efficiently to recent deep generative models, such as generative adversarial networks and diffusion models, in practice. To bridge this gap, we formulate the data valuation problem in generative models from a similarity-matching perspective. Specifically, we introduce Generative Model Valuator (GMValuator), the first model-agnostic approach for any generative models, designed to provide data valuation for gener
&lt;/p&gt;</description></item><item><title>SkinGPT&#26159;&#19968;&#20010;&#22522;&#20110;&#36855;&#20320;GPT-4&#30340;&#31934;&#32454;&#35843;&#25972;&#29256;&#26412;&#21644;&#20869;&#37096;&#30382;&#32932;&#22270;&#20687;&#38598;&#21512;&#32467;&#21512;&#30340;&#30382;&#32932;&#31185;&#35786;&#26029;&#31995;&#32479;&#65292;&#21033;&#29992;&#39640;&#32423;&#35270;&#35273;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#30382;&#32932;&#31185;&#21307;&#29983;&#19981;&#36275;&#12289;&#20934;&#30830;&#35786;&#26029;&#30382;&#32932;&#31185;&#22270;&#29255;&#38590;&#24230;&#22823;&#65292;&#20197;&#21450;&#25552;&#20379;&#29992;&#25143;&#21451;&#22909;&#30340;&#35786;&#26029;&#25253;&#21578;&#22256;&#38590;&#31561;&#19977;&#20010;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2304.10691</link><description>&lt;p&gt;
SkinGPT: &#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#30382;&#32932;&#31185;&#35786;&#26029;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
SkinGPT: A Dermatology Diagnostic System with Vision Large Language Model. (arXiv:2304.10691v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10691
&lt;/p&gt;
&lt;p&gt;
SkinGPT&#26159;&#19968;&#20010;&#22522;&#20110;&#36855;&#20320;GPT-4&#30340;&#31934;&#32454;&#35843;&#25972;&#29256;&#26412;&#21644;&#20869;&#37096;&#30382;&#32932;&#22270;&#20687;&#38598;&#21512;&#32467;&#21512;&#30340;&#30382;&#32932;&#31185;&#35786;&#26029;&#31995;&#32479;&#65292;&#21033;&#29992;&#39640;&#32423;&#35270;&#35273;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#30382;&#32932;&#31185;&#21307;&#29983;&#19981;&#36275;&#12289;&#20934;&#30830;&#35786;&#26029;&#30382;&#32932;&#31185;&#22270;&#29255;&#38590;&#24230;&#22823;&#65292;&#20197;&#21450;&#25552;&#20379;&#29992;&#25143;&#21451;&#22909;&#30340;&#35786;&#26029;&#25253;&#21578;&#22256;&#38590;&#31561;&#19977;&#20010;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30382;&#32932;&#21644;&#30382;&#19979;&#30142;&#30149;&#26159;&#20840;&#29699;&#38750;&#33268;&#21629;&#30142;&#30149;&#36127;&#25285;&#30340;&#20027;&#35201;&#21407;&#22240;&#20043;&#19968;&#65292;&#24433;&#21709;&#20102;&#22823;&#37096;&#20998;&#20154;&#21475;&#12290;&#28982;&#32780;&#65292;&#30382;&#32932;&#31185;&#35786;&#26029;&#39046;&#22495;&#23384;&#22312;&#19977;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#30382;&#32932;&#31185;&#21307;&#29983;&#19981;&#36275;&#12289;&#20934;&#30830;&#35786;&#26029;&#30382;&#32932;&#31185;&#22270;&#29255;&#38590;&#24230;&#22823;&#20197;&#21450;&#25552;&#20379;&#29992;&#25143;&#21451;&#22909;&#30340;&#35786;&#26029;&#25253;&#21578;&#22256;&#38590;&#12290;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#23637;&#26174;&#31034;&#20986;&#20102;&#22312;&#20020;&#24202;&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#22270;&#20687;&#19978;&#23384;&#22312;&#22256;&#38590;&#65292;&#24182;&#19988;&#20351;&#29992;ChatGPT&#30340;API&#19978;&#20256;&#25968;&#25454;&#20250;&#23384;&#22312;&#28508;&#22312;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SkinGPT&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#21033;&#29992;&#39640;&#32423;&#35270;&#35273;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#30382;&#32932;&#31185;&#35786;&#26029;&#31995;&#32479;&#12290;SkinGPT&#26159;&#39318;&#20010;&#37319;&#29992;&#20869;&#37096;&#30382;&#32932;&#22270;&#20687;&#38598;&#21512;&#21644;&#36855;&#20320;GPT-4&#30340;&#31934;&#32454;&#35843;&#25972;&#29256;&#26412;&#32467;&#21512;&#32780;&#25104;&#30340;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Skin and subcutaneous diseases are among the major causes of the nonfatal disease burden worldwide, affecting a significant proportion of the population. However, there are three major challenges in the field of dermatology diagnosis. Firstly, there is a shortage of dermatologists available to diagnose patients. Secondly, accurately diagnosing dermatological pictures can be challenging. Lastly, providing user-friendly diagnostic reports can be difficult. Recent advancements in the field of large language models (LLMs) have shown potential for clinical applications. However, current LLMs have difficulty processing images, and there are potential privacy concerns associated with using ChatGPT's API for uploading data. In this paper, we propose SkinGPT, which is the first dermatology diagnostic system that utilizes an advanced vision-based large language model. SkinGPT is the first system of its kind, incorporating a fine-tuned version of MiniGPT-4 with a vast collection of in-house skin 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#26131;&#21457;&#29983;&#37326;&#28779;&#21306;&#22495;&#30340;&#22810;&#22240;&#32032;&#28145;&#24230;&#23398;&#20064;&#30005;&#21147;&#36127;&#33655;&#39044;&#27979;&#27169;&#22411;&#65292;&#32771;&#34385;&#20102;&#25968;&#25454;&#36755;&#20837;&#32467;&#26500;&#12289;&#26085;&#21382;&#25928;&#24212;&#21644;&#22522;&#20110;&#30456;&#20851;&#24615;&#30340;&#28201;&#24230;&#20808;&#23548;&#26465;&#20214;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#26041;&#27861;&#20943;&#23569;&#20102;30.73%&#30340;MAPE&#35823;&#24046;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#24615;&#33021;&#20248;&#20110;&#21478;&#19968;&#20010;DL&#27169;&#22411;LSTM&#65292;&#24179;&#22343;&#22343;&#26041;&#35823;&#24046;&#21644;MAPE&#20998;&#21035;&#25552;&#39640;&#20102;10.06&#65285;&#21644;12.86&#65285;&#12290;</title><link>http://arxiv.org/abs/2304.10686</link><description>&lt;p&gt;
&#38754;&#21521;&#26131;&#21457;&#29983;&#37326;&#28779;&#21306;&#22495;&#30340;&#22810;&#22240;&#32032;&#28145;&#24230;&#23398;&#20064;&#30005;&#21147;&#36127;&#33655;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A generalised multi-factor deep learning electricity load forecasting model for wildfire-prone areas. (arXiv:2304.10686v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10686
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#26131;&#21457;&#29983;&#37326;&#28779;&#21306;&#22495;&#30340;&#22810;&#22240;&#32032;&#28145;&#24230;&#23398;&#20064;&#30005;&#21147;&#36127;&#33655;&#39044;&#27979;&#27169;&#22411;&#65292;&#32771;&#34385;&#20102;&#25968;&#25454;&#36755;&#20837;&#32467;&#26500;&#12289;&#26085;&#21382;&#25928;&#24212;&#21644;&#22522;&#20110;&#30456;&#20851;&#24615;&#30340;&#28201;&#24230;&#20808;&#23548;&#26465;&#20214;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#26041;&#27861;&#20943;&#23569;&#20102;30.73%&#30340;MAPE&#35823;&#24046;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#24615;&#33021;&#20248;&#20110;&#21478;&#19968;&#20010;DL&#27169;&#22411;LSTM&#65292;&#24179;&#22343;&#22343;&#26041;&#35823;&#24046;&#21644;MAPE&#20998;&#21035;&#25552;&#39640;&#20102;10.06&#65285;&#21644;12.86&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#21644;&#40065;&#26834;&#30340;&#22810;&#22240;&#32032;&#38376;&#25511;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;GRU&#65289;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#37326;&#28779;&#23395;&#33410;&#22320;&#21306;&#20998;&#24067;&#32593;&#32476;&#30340;&#30005;&#21147;&#36127;&#33655;&#12290;&#35813;&#28789;&#27963;&#30340;&#24314;&#27169;&#26041;&#27861;&#32771;&#34385;&#25968;&#25454;&#36755;&#20837;&#32467;&#26500;&#12289;&#26085;&#21382;&#25928;&#24212;&#20197;&#21450;&#22522;&#20110;&#30456;&#20851;&#24615;&#30340;&#28201;&#24230;&#20808;&#23548;&#26465;&#20214;&#12290;&#19982;&#24120;&#35268;&#30636;&#26102;&#28201;&#24230;&#30340;&#20351;&#29992;&#30456;&#27604;&#65292;&#37319;&#29992;&#25152;&#25552;&#20986;&#30340;&#36755;&#20837;&#29305;&#24449;&#36873;&#25321;&#21644;&#28201;&#24230;&#20808;&#23548;&#20851;&#31995;&#20351;&#24179;&#22343;&#32477;&#23545;&#30334;&#20998;&#27604;&#35823;&#24046;&#65288;MAPE&#65289;&#38477;&#20302;&#20102;30.73%%&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#36890;&#29992;&#19988;&#24212;&#29992;&#20110;&#28595;&#22823;&#21033;&#20122;&#32500;&#22810;&#21033;&#20122;&#24030;&#22312;2015-2020&#24180;&#37326;&#28779;&#23395;&#33410;&#26399;&#38388;&#30340;&#20843;&#20010;&#30495;&#23454;&#20998;&#24067;&#24335;&#32593;&#32476;&#20013;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;GRU&#30340;&#27169;&#22411;&#22312;&#27599;&#20010;&#27493;&#39588;&#20013;&#22987;&#32456;&#20248;&#20110;&#21478;&#19968;&#20010;DL&#27169;&#22411;&#65292;&#21363;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#65292;&#30456;&#23545;&#20110;&#24179;&#22343;&#22343;&#26041;&#35823;&#24046;&#21644;MAPE&#20998;&#21035;&#25552;&#39640;&#20102;10.06&#65285;&#21644;12.86&#65285;&#12290;&#36824;&#30740;&#31350;&#20102;&#23545;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#22823;&#35268;&#27169;&#27668;&#20505;&#21464;&#24322;&#30340;&#25935;&#24863;&#24615;&#65292;&#20363;&#22914;El Ni&#241;o&#25110;La Ni&#241;a&#12290;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#23545;&#36825;&#31181;&#21464;&#24322;&#26159;&#40065;&#26834;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a generalised and robust multi-factor Gated Recurrent Unit (GRU) based Deep Learning (DL) model to forecast electricity load in distribution networks during wildfire seasons. The flexible modelling methods consider data input structure, calendar effects and correlation-based leading temperature conditions. Compared to the regular use of instantaneous temperature, the Mean Absolute Percentage Error (MAPE) is decreased by 30.73% by using the proposed input feature selection and leading temperature relationships. Our model is generalised and applied to eight real distribution networks in Victoria, Australia, during the wildfire seasons of 2015-2020. We demonstrate that the GRU-based model consistently outperforms another DL model, Long Short-Term Memory (LSTM), at every step, giving average improvements in Mean Squared Error (MSE) and MAPE of 10.06% and 12.86%, respectively. The sensitivity to large-scale climate variability in training data sets, e.g. El Ni\~no or La 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;Graph-Aware Distillation&#65288;GRAD&#65289;&#30340;&#26694;&#26550;&#65292;&#20197;&#22312;&#27809;&#26377;&#22270;&#24418;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#24555;&#36895;&#25512;&#29702;&#30340;LM&#23558;&#22270;&#24418;&#32467;&#26500;&#32534;&#30721;&#65292;&#27492;&#26694;&#26550;&#21487;&#20197;&#21516;&#26102;&#20248;&#21270;GNN&#25945;&#24072;&#21644;&#26080;&#22270;&#23398;&#29983;&#65292;&#20114;&#30456;&#23398;&#20064;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.10668</link><description>&lt;p&gt;
&#35757;&#32451;&#20320;&#33258;&#24049;&#30340;GNN&#25945;&#24072;&#65306;&#38754;&#21521;&#25991;&#26412;&#22270;&#30340;&#22270;&#24863;&#30693;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Train Your Own GNN Teacher: Graph-Aware Distillation on Textual Graphs. (arXiv:2304.10668v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10668
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;Graph-Aware Distillation&#65288;GRAD&#65289;&#30340;&#26694;&#26550;&#65292;&#20197;&#22312;&#27809;&#26377;&#22270;&#24418;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#24555;&#36895;&#25512;&#29702;&#30340;LM&#23558;&#22270;&#24418;&#32467;&#26500;&#32534;&#30721;&#65292;&#27492;&#26694;&#26550;&#21487;&#20197;&#21516;&#26102;&#20248;&#21270;GNN&#25945;&#24072;&#21644;&#26080;&#22270;&#23398;&#29983;&#65292;&#20114;&#30456;&#23398;&#20064;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#22312;&#25991;&#26412;&#22270;&#19978;&#23454;&#29616;&#26377;&#25928;&#30340;&#33410;&#28857;&#34920;&#31034;&#23398;&#20064;&#65311;&#23558;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30456;&#32467;&#21512;&#65292;&#20197;&#23545;&#22270;&#24418;&#30340;&#25991;&#26412;&#20449;&#24687;&#36827;&#34892;&#32534;&#30721;&#30340;GNN&#22312;&#35768;&#22810;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#65292;&#23558;GNN&#19982;LM&#30456;&#32467;&#21512;&#36827;&#34892;&#23454;&#38469;&#37096;&#32626;&#24182;&#27809;&#26377;&#24471;&#21040;&#24191;&#27867;&#25506;&#32034;&#12290;&#26412;&#25991;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#21517;&#20026;Graph-Aware Distillation&#65288;GRAD&#65289;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#19968;&#38590;&#39064;&#65292;&#20197;&#22312;&#27809;&#26377;&#22270;&#24418;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#24555;&#36895;&#25512;&#29702;&#30340;LM&#23558;&#22270;&#24418;&#32467;&#26500;&#32534;&#30721;&#12290;&#19982;&#20256;&#32479;&#30340;&#30693;&#35782;&#33976;&#39311;&#19981;&#21516;&#65292;GRAD&#36890;&#36807;&#20849;&#20139;LM&#22312;&#22270;&#30340;&#33410;&#28857;&#19978;&#21516;&#26102;&#20248;&#21270;GNN&#25945;&#24072;&#21644;&#26080;&#22270;&#23398;&#29983;&#12290;&#36825;&#40723;&#21169;&#26080;&#22270;&#23398;&#29983;&#21033;&#29992;&#30001;GNN&#25945;&#24072;&#32534;&#30721;&#30340;&#22270;&#24418;&#20449;&#24687;&#65292;&#21516;&#26102;&#20351;GNN&#25945;&#24072;&#26356;&#22909;&#22320;&#21033;&#29992;&#26410;&#26631;&#35760;&#33410;&#28857;&#30340;&#25991;&#26412;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#25945;&#24072;&#21644;&#23398;&#29983;&#27169;&#22411;&#30456;&#20114;&#23398;&#20064;&#20197;&#25552;&#39640;&#20854;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
How can we learn effective node representations on textual graphs? Graph Neural Networks (GNNs) that use Language Models (LMs) to encode textual information of graphs achieve state-of-the-art performance in many node classification tasks. Yet, combining GNNs with LMs has not been widely explored for practical deployments due to its scalability issues. In this work, we tackle this challenge by developing a Graph-Aware Distillation framework (GRAD) to encode graph structures into an LM for graph-free, fast inference. Different from conventional knowledge distillation, GRAD jointly optimizes a GNN teacher and a graph-free student over the graph's nodes via a shared LM. This encourages the graph-free student to exploit graph information encoded by the GNN teacher while at the same time, enables the GNN teacher to better leverage textual information from unlabeled nodes. As a result, the teacher and the student models learn from each other to improve their overall performance. Experiments i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20174;&#23454;&#36341;&#32773;&#35282;&#24230;&#20986;&#21457;&#65292;&#38024;&#23545;&#21021;&#21019;&#20225;&#19994;&#22914;&#20309;&#25104;&#26412;&#26377;&#25928;&#22320;&#25193;&#23637;&#26426;&#22120;&#23398;&#20064;&#20135;&#21697;&#30340;&#26694;&#26550;&#21644;&#26041;&#27861;&#65292;&#24182;&#24378;&#35843;&#20102;&#20943;&#23569;&#22266;&#23450;&#25104;&#26412;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.10660</link><description>&lt;p&gt;
&#21021;&#21019;&#20225;&#19994;&#22914;&#20309;&#25193;&#23637;&#26426;&#22120;&#23398;&#20064;&#20135;&#21697;&#65306;&#20174;&#23454;&#36341;&#32773;&#35282;&#24230;&#30340;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
Scaling ML Products At Startups: A Practitioner's Guide. (arXiv:2304.10660v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10660
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20174;&#23454;&#36341;&#32773;&#35282;&#24230;&#20986;&#21457;&#65292;&#38024;&#23545;&#21021;&#21019;&#20225;&#19994;&#22914;&#20309;&#25104;&#26412;&#26377;&#25928;&#22320;&#25193;&#23637;&#26426;&#22120;&#23398;&#20064;&#20135;&#21697;&#30340;&#26694;&#26550;&#21644;&#26041;&#27861;&#65292;&#24182;&#24378;&#35843;&#20102;&#20943;&#23569;&#22266;&#23450;&#25104;&#26412;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21021;&#21019;&#20225;&#19994;&#22914;&#20309;&#25193;&#23637;&#26426;&#22120;&#23398;&#20064;&#20135;&#21697;&#65311;&#29305;&#21035;&#26159;&#22914;&#20309;&#25104;&#26412;&#26377;&#25928;&#22320;&#22788;&#29702;&#26356;&#22823;&#37327;&#65292;&#26356;&#22810;&#26679;&#21270;&#21644;&#26356;&#24555;&#36895;&#30340;&#26597;&#35810;&#65311;&#25105;&#20204;&#23558;&#25104;&#26412;&#20998;&#20026;&#21487;&#21464;&#25104;&#26412;-&#25552;&#20379;&#27169;&#22411;&#21644;&#24615;&#33021;&#30340;&#25104;&#26412;-&#21644;&#22266;&#23450;&#25104;&#26412;-&#24320;&#21457;&#21644;&#35757;&#32451;&#26032;&#27169;&#22411;&#30340;&#25104;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#27010;&#24565;&#21270;&#36825;&#20123;&#25104;&#26412;&#65292;&#23558;&#23427;&#20204;&#32454;&#20998;&#20026;&#26356;&#32454;&#30340;&#31867;&#21035;&#65292;&#24182;&#38416;&#36848;&#20943;&#23569;&#25104;&#26412;&#30340;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#30001;&#20110;&#20174;&#25105;&#20204;&#30340;&#32463;&#39564;&#26469;&#30475;&#65292;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20013;&#26368;&#26114;&#36149;&#30340;&#22266;&#23450;&#25104;&#26412;&#26159;&#30830;&#23450;&#22833;&#36133;&#26681;&#26412;&#21407;&#22240;&#24182;&#25512;&#21160;&#25345;&#32493;&#25913;&#36827;&#30340;&#25104;&#26412;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#27010;&#24565;&#21270;&#38382;&#39064;&#24182;&#20998;&#20139;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
How do you scale a machine learning product at a startup? In particular, how do you serve a greater volume, velocity, and variety of queries cost-effectively? We break down costs into variable costs-the cost of serving the model and performant-and fixed costs-the cost of developing and training new models. We propose a framework for conceptualizing these costs, breaking them into finer categories, and limn ways to reduce costs. Lastly, since in our experience, the most expensive fixed cost of a machine learning system is the cost of identifying the root causes of failures and driving continuous improvement, we present a way to conceptualize the issues and share our methodology for the same.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#37327;&#23376;&#35745;&#31639;&#26426;&#33021;&#21147;&#20989;&#25968;&#65292;&#25104;&#21151;&#39044;&#27979;&#20102;&#36817;&#26399;&#37327;&#23376;&#22788;&#29702;&#22120;&#19978;&#37327;&#23376;&#30005;&#36335;&#30340;&#25104;&#21151;&#27010;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.10650</link><description>&lt;p&gt;
&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#37327;&#23376;&#35745;&#31639;&#26426;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Learning a quantum computer's capability using convolutional neural networks. (arXiv:2304.10650v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10650
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#37327;&#23376;&#35745;&#31639;&#26426;&#33021;&#21147;&#20989;&#25968;&#65292;&#25104;&#21151;&#39044;&#27979;&#20102;&#36817;&#26399;&#37327;&#23376;&#22788;&#29702;&#22120;&#19978;&#37327;&#23376;&#30005;&#36335;&#30340;&#25104;&#21151;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#37327;&#23376;&#22788;&#29702;&#22120;&#30340;&#35745;&#31639;&#33021;&#21147;&#21463;&#30828;&#20214;&#38169;&#35823;&#24433;&#21709;&#23548;&#33268;&#35745;&#31639;&#22833;&#36133;&#12290;&#25105;&#20204;&#25506;&#35752;&#20351;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#26469;&#23398;&#20064;&#22788;&#29702;&#22120;&#33021;&#21147;&#20989;&#25968;&#30340;&#36817;&#20284;&#20540;&#12290;&#32467;&#26524;&#34920;&#26126;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#26377;&#25928;&#22320;&#23398;&#20064;&#37327;&#23376;&#22788;&#29702;&#22120;&#30340;&#33021;&#21147;&#20989;&#25968;&#65292;&#20351;&#24471;&#36817;&#26399;&#37327;&#23376;&#22788;&#29702;&#22120;&#19978;&#30340;&#37327;&#23376;&#30005;&#36335;&#21487;&#20197;&#24555;&#36895;&#21487;&#38752;&#22320;&#39044;&#27979;&#25104;&#21151;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The computational power of contemporary quantum processors is limited by hardware errors that cause computations to fail. In principle, each quantum processor's computational capabilities can be described with a capability function that quantifies how well a processor can run each possible quantum circuit (i.e., program), as a map from circuits to the processor's success rates on those circuits. However, capability functions are typically unknown and challenging to model, as the particular errors afflicting a specific quantum processor are a priori unknown and difficult to completely characterize. In this work, we investigate using artificial neural networks to learn an approximation to a processor's capability function. We explore how to define the capability function, and we explain how data for training neural networks can be efficiently obtained for a capability function defined using process fidelity. We then investigate using convolutional neural networks to model a quantum compu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#24050;&#26377;&#20998;&#31867;&#27169;&#22411;&#36716;&#31227;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22312;&#26080;&#38656;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#22312;&#26032;&#30340;&#36523;&#20307;&#37096;&#20301;&#19978;&#23454;&#29616;&#27963;&#21160;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2304.10643</link><description>&lt;p&gt;
&#36890;&#36807;&#26080;&#30417;&#30563;&#22495;&#36716;&#31227;&#20174;&#20307;&#25140;&#24335;&#20256;&#24863;&#22120;&#20013;&#36827;&#34892;&#27963;&#21160;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Activity Classification Using Unsupervised Domain Transfer from Body Worn Sensors. (arXiv:2304.10643v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10643
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#24050;&#26377;&#20998;&#31867;&#27169;&#22411;&#36716;&#31227;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22312;&#26080;&#38656;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#22312;&#26032;&#30340;&#36523;&#20307;&#37096;&#20301;&#19978;&#23454;&#29616;&#27963;&#21160;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27963;&#21160;&#20998;&#31867;&#24050;&#32463;&#25104;&#20026;&#21487;&#31359;&#25140;&#20581;&#24247;&#36861;&#36394;&#35774;&#22791;&#30340;&#37325;&#35201;&#29305;&#24449;&#12290;&#38543;&#30528;&#36825;&#20010;&#39046;&#22495;&#30340;&#21019;&#26032;&#19981;&#26029;&#22686;&#38271;&#65292;&#20329;&#25140;&#22312;&#36523;&#20307;&#19981;&#21516;&#37096;&#20301;&#30340;&#21487;&#31359;&#25140;&#35774;&#22791;&#27491;&#22312;&#20986;&#29616;&#12290;&#35201;&#23545;&#26032;&#30340;&#36523;&#20307;&#37096;&#20301;&#36827;&#34892;&#27963;&#21160;&#20998;&#31867;&#65292;&#36890;&#24120;&#38656;&#35201;&#30456;&#24212;&#26032;&#20301;&#32622;&#30340;&#26631;&#35760;&#25968;&#25454;&#65292;&#20294;&#36825;&#26679;&#20570;&#30340;&#25104;&#26412;&#24456;&#39640;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#24050;&#32463;&#22312;&#21442;&#32771;&#36523;&#20307;&#20301;&#32622;&#65288;&#28304;&#22495;&#65289;&#19978;&#20174;&#24815;&#24615;&#27979;&#37327;&#21333;&#20803;&#65288;IMU&#65289;&#25968;&#25454;&#35757;&#32451;&#22909;&#30340;&#27963;&#21160;&#20998;&#31867;&#22120;&#65292;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#22312;&#26032;&#30340;&#36523;&#20307;&#20301;&#32622;&#65288;&#30446;&#26631;&#22495;&#65289;&#19978;&#36827;&#34892;&#27963;&#21160;&#20998;&#31867;&#65292;&#21363;&#26080;&#38656;&#22312;&#26032;&#20301;&#32622;&#20351;&#29992;&#20998;&#31867;&#26631;&#31614;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#22797;&#21046;&#28304;&#22495;&#30340;&#23884;&#20837;&#65292;&#25105;&#20204;&#35757;&#32451;&#19968;&#20010;&#23884;&#20837;&#27169;&#22411;&#65292;&#20197;&#22312;&#30446;&#26631;&#22495;&#25191;&#34892;&#27963;&#21160;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Activity classification has become a vital feature of wearable health tracking devices. As innovation in this field grows, wearable devices worn on different parts of the body are emerging. To perform activity classification on a new body location, labeled data corresponding to the new locations are generally required, but this is expensive to acquire. In this work, we present an innovative method to leverage an existing activity classifier, trained on Inertial Measurement Unit (IMU) data from a reference body location (the source domain), in order to perform activity classification on a new body location (the target domain) in an unsupervised way, i.e. without the need for classification labels at the new location. Specifically, given an IMU embedding model trained to perform activity classification at the source domain, we train an embedding model to perform activity classification at the target domain by replicating the embeddings at the source domain. This is achieved using simulta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#25237;&#24433;&#26041;&#27861;&#21644;&#20248;&#21270;&#26041;&#27861;&#27714;&#35299;&#20998;&#24067;&#24335;&#32447;&#24615;&#31995;&#32479;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#25552;&#20986;&#20102;&#35282;&#24322;&#26500;&#24615;&#30340;&#20960;&#20309;&#27010;&#24565;&#65292;&#24182;&#23545;&#26368;&#26377;&#25928;&#30340;&#31639;&#27861;(APC&#21644;D-HBM)&#30340;&#25910;&#25947;&#36895;&#24230;&#36827;&#34892;&#20102;&#32422;&#26463;&#21644;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2304.10640</link><description>&lt;p&gt;
&#35770;&#25968;&#25454;&#24322;&#26500;&#24615;&#23545;&#20998;&#24067;&#24335;&#32447;&#24615;&#31995;&#32479;&#27714;&#35299;&#22120;&#25910;&#25947;&#36895;&#24230;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
On the Effects of Data Heterogeneity on the Convergence Rates of Distributed Linear System Solvers. (arXiv:2304.10640v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10640
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#25237;&#24433;&#26041;&#27861;&#21644;&#20248;&#21270;&#26041;&#27861;&#27714;&#35299;&#20998;&#24067;&#24335;&#32447;&#24615;&#31995;&#32479;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#25552;&#20986;&#20102;&#35282;&#24322;&#26500;&#24615;&#30340;&#20960;&#20309;&#27010;&#24565;&#65292;&#24182;&#23545;&#26368;&#26377;&#25928;&#30340;&#31639;&#27861;(APC&#21644;D-HBM)&#30340;&#25910;&#25947;&#36895;&#24230;&#36827;&#34892;&#20102;&#32422;&#26463;&#21644;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#35299;&#20915;&#22823;&#35268;&#27169;&#32447;&#24615;&#26041;&#31243;&#32452;&#30340;&#22522;&#26412;&#38382;&#39064;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#32771;&#34385;&#20219;&#21153;&#36127;&#36131;&#20154;&#25171;&#31639;&#22312;&#19968;&#32452;&#20855;&#26377;&#19968;&#20123;&#26041;&#31243;&#32452;&#23376;&#38598;&#30340;&#26426;&#22120;&#30340;&#20998;&#24067;&#24335;/&#32852;&#21512;&#24110;&#21161;&#19979;&#35299;&#20915;&#35813;&#31995;&#32479;&#30340;&#35774;&#32622;&#12290;&#34429;&#28982;&#26377;&#20960;&#31181;&#26041;&#27861;&#29992;&#20110;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#32570;&#23569;&#23545;&#25237;&#24433;&#26041;&#27861;&#21644;&#20248;&#21270;&#26041;&#27861;&#25910;&#25947;&#36895;&#24230;&#30340;&#20005;&#26684;&#27604;&#36739;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#24182;&#27604;&#36739;&#36825;&#20004;&#31867;&#31639;&#27861;&#65292;&#29305;&#21035;&#20851;&#27880;&#27599;&#20010;&#31867;&#21035;&#20013;&#26368;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21363;&#26368;&#36817;&#25552;&#20986;&#30340;&#21152;&#36895;&#25237;&#24433;&#19968;&#33268;&#24615;(APC)&#21644;&#20998;&#24067;&#24335;&#37325;&#29699;&#26041;&#27861;(D-HBM)&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#31216;&#20026;&#35282;&#24322;&#26500;&#24615;&#30340;&#20960;&#20309;&#27010;&#24565;&#65292;&#24182;&#35752;&#35770;&#20854;&#26222;&#36941;&#24615;&#12290;&#20351;&#29992;&#35813;&#27010;&#24565;&#65292;&#25105;&#20204;&#32422;&#26463;&#24182;&#27604;&#36739;&#25152;&#30740;&#31350;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#25429;&#25417;&#20004;&#31181;&#26041;&#27861;&#30340;&#24322;&#26500;&#25968;&#25454;&#30340;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the fundamental problem of solving a large-scale system of linear equations. In particular, we consider the setting where a taskmaster intends to solve the system in a distributed/federated fashion with the help of a set of machines, who each have a subset of the equations. Although there exist several approaches for solving this problem, missing is a rigorous comparison between the convergence rates of the projection-based methods and those of the optimization-based ones. In this paper, we analyze and compare these two classes of algorithms with a particular focus on the most efficient method from each class, namely, the recently proposed Accelerated Projection-Based Consensus (APC) and the Distributed Heavy-Ball Method (D-HBM). To this end, we first propose a geometric notion of data heterogeneity called angular heterogeneity and discuss its generality. Using this notion, we bound and compare the convergence rates of the studied algorithms and capture the effects of both 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;CVAE&#30340;&#22810;&#27169;&#22359;&#26694;&#26550;&#65292;&#29992;&#20110;&#39044;&#27979;SNS&#21152;&#36895;&#22120;&#20013;HVCM&#30340;&#25925;&#38556;&#12290;&#36890;&#36807;&#29305;&#23450;&#27169;&#22359;&#31867;&#22411;&#30340;&#26465;&#20214;&#32422;&#26463;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;&#25925;&#38556;&#31867;&#22411;&#30340;&#35782;&#21035;&#28789;&#25935;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#26816;&#27979;&#22810;&#20010;&#25925;&#38556;&#31867;&#22411;&#30340;HVCM&#27169;&#22359;&#31867;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.10639</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#27169;&#22359;CVAE&#30340;SNS&#21152;&#36895;&#22120;HVCM&#25925;&#38556;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multi-module based CVAE to predict HVCM faults in the SNS accelerator. (arXiv:2304.10639v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10639
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;CVAE&#30340;&#22810;&#27169;&#22359;&#26694;&#26550;&#65292;&#29992;&#20110;&#39044;&#27979;SNS&#21152;&#36895;&#22120;&#20013;HVCM&#30340;&#25925;&#38556;&#12290;&#36890;&#36807;&#29305;&#23450;&#27169;&#22359;&#31867;&#22411;&#30340;&#26465;&#20214;&#32422;&#26463;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;&#25925;&#38556;&#31867;&#22411;&#30340;&#35782;&#21035;&#28789;&#25935;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#26816;&#27979;&#22810;&#20010;&#25925;&#38556;&#31867;&#22411;&#30340;HVCM&#27169;&#22359;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26465;&#20214;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;CVAE&#65289;&#30340;&#22810;&#27169;&#22359;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#27979;&#26469;&#33258;&#22810;&#20010;&#39640;&#21387;&#21464;&#25442;&#22120;&#27169;&#22359;&#65288;HVCM&#65289;&#30340;&#30005;&#28304;&#20449;&#21495;&#20013;&#30340;&#24322;&#24120;&#12290;&#25105;&#20204;&#36890;&#36807;&#29305;&#23450;&#27169;&#22359;&#31867;&#22411;&#23545;&#27169;&#22411;&#36827;&#34892;&#26465;&#20214;&#32422;&#26463;&#65292;&#20197;&#25429;&#33719;&#27491;&#24120;&#27874;&#24418;&#30340;&#19981;&#21516;&#34920;&#31034;&#65292;&#24182;&#25552;&#39640;&#27169;&#22411;&#23545;&#22312;&#32473;&#23450;&#27169;&#22359;&#31867;&#22411;&#30340;&#26377;&#38480;&#26679;&#26412;&#24773;&#20917;&#19979;&#35782;&#21035;&#29305;&#23450;&#25925;&#38556;&#31867;&#22411;&#30340;&#28789;&#25935;&#24230;&#12290;&#25105;&#20204;&#38024;&#23545;CVAE&#27169;&#22411;&#30740;&#31350;&#20102;&#20960;&#31181;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#26550;&#26500;&#65292;&#24182;&#36890;&#36807;&#35266;&#23519;&#20854;&#31283;&#23450;&#24615;&#21644;&#27867;&#21270;&#24615;&#30340;&#25439;&#22833;&#26223;&#35266;&#26469;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#35757;&#32451;&#20986;&#30340;&#27169;&#22411;&#23545;&#20110;&#26816;&#27979;&#22810;&#20010;&#25925;&#38556;&#31867;&#22411;&#30340;&#22810;&#20010;HVCM&#27169;&#22359;&#31867;&#22411;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#25928;&#26524;&#12290;&#35813;&#30740;&#31350;&#30340;&#32467;&#26524;&#21487;&#20197;&#29992;&#20110;&#25552;&#39640;HVCM&#30340;&#21487;&#38752;&#24615;&#21644;&#25972;&#20307;SNS&#27491;&#24120;&#36816;&#34892;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a multi-module framework based on Conditional Variational Autoencoder (CVAE) to detect anomalies in the power signals coming from multiple High Voltage Converter Modulators (HVCMs). We condition the model with the specific modulator type to capture different representations of the normal waveforms and to improve the sensitivity of the model to identify a specific type of fault when we have limited samples for a given module type. We studied several neural network (NN) architectures for our CVAE model and evaluated the model performance by looking at their loss landscape for stability and generalization. Our results for the Spallation Neutron Source (SNS) experimental data show that the trained model generalizes well to detecting multiple fault types for several HVCM module types. The results of this study can be used to improve the HVCM reliability and overall SNS uptime
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36828;&#31243;&#25830;&#38500;&#32852;&#37030;&#23398;&#20064;&#20013;&#21518;&#38376;&#30340;&#26041;&#27861;&#65292;&#20351;&#24471;&#25915;&#20987;&#32773;&#21487;&#20197;&#22312;&#23454;&#29616;&#30446;&#26631;&#25110;&#24576;&#30097;&#34987;&#26816;&#27979;&#21040;&#26102;&#26377;&#25928;&#22320;&#20174;&#38598;&#20013;&#27169;&#22411;&#20013;&#31227;&#38500;&#21518;&#38376;&#12290;&#27492;&#26041;&#27861;&#25193;&#23637;&#20102;&#26426;&#22120;&#8220;&#36951;&#24536;&#8221;&#30340;&#27010;&#24565;&#65292;&#24182;&#25552;&#20379;&#20102;&#20855;&#20307;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2304.10638</link><description>&lt;p&gt;
&#25670;&#33073;&#38544;&#24739;&#65306;&#36828;&#31243;&#25830;&#38500;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#21518;&#38376;
&lt;/p&gt;
&lt;p&gt;
Get Rid Of Your Trail: Remotely Erasing Backdoors in Federated Learning. (arXiv:2304.10638v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10638
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36828;&#31243;&#25830;&#38500;&#32852;&#37030;&#23398;&#20064;&#20013;&#21518;&#38376;&#30340;&#26041;&#27861;&#65292;&#20351;&#24471;&#25915;&#20987;&#32773;&#21487;&#20197;&#22312;&#23454;&#29616;&#30446;&#26631;&#25110;&#24576;&#30097;&#34987;&#26816;&#27979;&#21040;&#26102;&#26377;&#25928;&#22320;&#20174;&#38598;&#20013;&#27169;&#22411;&#20013;&#31227;&#38500;&#21518;&#38376;&#12290;&#27492;&#26041;&#27861;&#25193;&#23637;&#20102;&#26426;&#22120;&#8220;&#36951;&#24536;&#8221;&#30340;&#27010;&#24565;&#65292;&#24182;&#25552;&#20379;&#20102;&#20855;&#20307;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20351;&#22810;&#20010;&#21442;&#19982;&#32773;&#33021;&#22815;&#22312;&#19981;&#26292;&#38706;&#25935;&#24863;&#20010;&#20154;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#21327;&#20316;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;FL&#30340;&#20998;&#24067;&#24335;&#24615;&#36136;&#21644;&#26410;&#32463;&#23457;&#26680;&#30340;&#21442;&#19982;&#32773;&#25968;&#25454;&#20351;&#20854;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#12290;&#22312;&#36825;&#20123;&#25915;&#20987;&#20013;&#65292;&#23545;&#25163;&#22312;&#35757;&#32451;&#26399;&#38388;&#21521;&#38598;&#20013;&#24335;&#27169;&#22411;&#27880;&#20837;&#24694;&#24847;&#21151;&#33021;&#65292;&#23548;&#33268;&#29305;&#23450;&#23545;&#25163;&#36873;&#25321;&#30340;&#36755;&#20837;&#25925;&#24847;&#38169;&#20998;&#12290;&#34429;&#28982;&#20197;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#20102;&#22312;FL&#20013;&#25104;&#21151;&#27880;&#20837;&#25345;&#20037;&#21518;&#38376;&#30340;&#21487;&#33021;&#24615;&#65292;&#20294;&#20854;&#25345;&#20037;&#24615;&#20063;&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#38598;&#20013;&#24335;&#27169;&#22411;&#20013;&#30340;&#23384;&#22312;&#21487;&#33021;&#20250;&#20419;&#20351;&#20013;&#22830;&#32858;&#21512;&#26381;&#21153;&#22120;&#37319;&#21462;&#39044;&#38450;&#25514;&#26045;&#26469;&#24809;&#32602;&#23545;&#25163;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#25932;&#20154;&#22312;&#23454;&#29616;&#20854;&#30446;&#26631;&#25110;&#24576;&#30097;&#21487;&#33021;&#34987;&#26816;&#27979;&#21040;&#26102;&#26377;&#25928;&#22320;&#20174;&#38598;&#20013;&#27169;&#22411;&#20013;&#31227;&#38500;&#21518;&#38376;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#25193;&#23637;&#20102;&#26426;&#22120;&#8220;&#36951;&#24536;&#8221;&#30340;&#27010;&#24565;&#65292;&#24182;&#25552;&#20986;&#20102;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) enables collaborative deep learning training across multiple participants without exposing sensitive personal data. However, the distributed nature of FL and the unvetted participants' data makes it vulnerable to backdoor attacks. In these attacks, adversaries inject malicious functionality into the centralized model during training, leading to intentional misclassifications for specific adversary-chosen inputs. While previous research has demonstrated successful injections of persistent backdoors in FL, the persistence also poses a challenge, as their existence in the centralized model can prompt the central aggregation server to take preventive measures to penalize the adversaries. Therefore, this paper proposes a methodology that enables adversaries to effectively remove backdoors from the centralized model upon achieving their objectives or upon suspicion of possible detection. The proposed approach extends the concept of machine unlearning and presents stra
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;Cayley&#21464;&#25442;&#22312;&#20219;&#24847;&#32500;&#24230;&#19978;&#23558;&#26925;&#29699;&#25311;&#21512;&#21040;&#22024;&#26434;&#25968;&#25454;&#20013;&#30340;&#26032;&#31639;&#27861;CTEF&#65292;&#21487;&#20197;&#25311;&#21512;&#20219;&#24847;&#30340;&#26925;&#29699;&#65292;&#24182;&#19988;&#33021;&#25552;&#21462;&#20854;&#20182;&#26041;&#27861;&#26080;&#27861;&#35782;&#21035;&#30340;&#25968;&#25454;&#20013;&#30340;&#38750;&#32447;&#24615;&#29305;&#24449;&#65292;&#21487;&#29992;&#20110;&#38477;&#32500;&#12289;&#25968;&#25454;&#21487;&#35270;&#21270;&#21644;&#32858;&#31867;&#65292;&#30456;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#20248;&#12290;</title><link>http://arxiv.org/abs/2304.10630</link><description>&lt;p&gt;
&#29992;Cayley&#21464;&#25442;&#25311;&#21512;&#26925;&#29699;
&lt;/p&gt;
&lt;p&gt;
Ellipsoid fitting with the Cayley transform. (arXiv:2304.10630v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10630
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;Cayley&#21464;&#25442;&#22312;&#20219;&#24847;&#32500;&#24230;&#19978;&#23558;&#26925;&#29699;&#25311;&#21512;&#21040;&#22024;&#26434;&#25968;&#25454;&#20013;&#30340;&#26032;&#31639;&#27861;CTEF&#65292;&#21487;&#20197;&#25311;&#21512;&#20219;&#24847;&#30340;&#26925;&#29699;&#65292;&#24182;&#19988;&#33021;&#25552;&#21462;&#20854;&#20182;&#26041;&#27861;&#26080;&#27861;&#35782;&#21035;&#30340;&#25968;&#25454;&#20013;&#30340;&#38750;&#32447;&#24615;&#29305;&#24449;&#65292;&#21487;&#29992;&#20110;&#38477;&#32500;&#12289;&#25968;&#25454;&#21487;&#35270;&#21270;&#21644;&#32858;&#31867;&#65292;&#30456;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;Cayley&#21464;&#25442;&#26925;&#29699;&#25311;&#21512;(CTEF)&#65292;&#23427;&#20351;&#29992;Cayley&#21464;&#25442;&#22312;&#20219;&#24847;&#32500;&#24230;&#19978;&#23558;&#26925;&#29699;&#25311;&#21512;&#21040;&#22024;&#26434;&#30340;&#25968;&#25454;&#20013;&#12290;&#19982;&#35768;&#22810;&#26925;&#29699;&#25311;&#21512;&#26041;&#27861;&#19981;&#21516;&#65292;CTEF&#26159;&#26925;&#29699;&#29305;&#23450;&#30340;&#8212;&#8212;&#24847;&#21619;&#30528;&#23427;&#24635;&#26159;&#36820;&#22238;&#26925;&#22278;&#35299;&#8212;&#8212;&#24182;&#19988;&#21487;&#20197;&#25311;&#21512;&#20219;&#24847;&#30340;&#26925;&#29699;&#12290;&#24403;&#25968;&#25454;&#19981;&#22343;&#21248;&#22320;&#20998;&#24067;&#22312;&#26925;&#29699;&#34920;&#38754;&#19978;&#26102;&#65292;&#23427;&#20063;&#20248;&#20110;&#20854;&#20182;&#25311;&#21512;&#26041;&#27861;&#12290;&#21463;&#26426;&#22120;&#23398;&#20064;&#20013;&#21487;&#35299;&#37322;&#21644;&#21487;&#37325;&#22797;&#26041;&#27861;&#30340;&#21628;&#21505;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;CTEF&#24212;&#29992;&#20110;&#38477;&#32500;&#12289;&#25968;&#25454;&#21487;&#35270;&#21270;&#21644;&#32858;&#31867;&#12290;&#30001;&#20110;CTEF&#25429;&#25417;&#20840;&#23616;&#26354;&#29575;&#65292;&#22240;&#27492;&#23427;&#33021;&#22815;&#25552;&#21462;&#20854;&#20182;&#26041;&#27861;&#26080;&#27861;&#35782;&#21035;&#30340;&#25968;&#25454;&#20013;&#30340;&#38750;&#32447;&#24615;&#29305;&#24449;&#12290;&#36825;&#22312;&#20154;&#31867;&#32454;&#32990;&#21608;&#26399;&#25968;&#25454;&#30340;&#38477;&#32500;&#21644;&#22312;&#32463;&#20856;&#29609;&#20855;&#20363;&#23376;&#30340;&#32858;&#31867;&#30340;&#32972;&#26223;&#19979;&#24471;&#21040;&#20102;&#35828;&#26126;&#12290;&#22312;&#21518;&#19968;&#31181;&#24773;&#20917;&#19979;&#65292;CTEF&#20248;&#20110;10&#31181;&#27969;&#34892;&#30340;&#32858;&#31867;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce an algorithm, Cayley transform ellipsoid fitting (CTEF), that uses the Cayley transform to fit ellipsoids to noisy data in any dimension. Unlike many ellipsoid fitting methods, CTEF is ellipsoid specific -- meaning it always returns elliptic solutions -- and can fit arbitrary ellipsoids. It also outperforms other fitting methods when data are not uniformly distributed over the surface of an ellipsoid. Inspired by calls for interpretable and reproducible methods in machine learning, we apply CTEF to dimension reduction, data visualization, and clustering. Since CTEF captures global curvature, it is able to extract nonlinear features in data that other methods fail to identify. This is illustrated in the context of dimension reduction on human cell cycle data, and in the context of clustering on classical toy examples. In the latter case, CTEF outperforms 10 popular clustering algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27880;&#24847;&#21147;&#26465;&#20214;&#33258;&#32534;&#30721;&#22120;(AF-CA)&#65292;&#29992;&#20110;&#26816;&#27979;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#24322;&#24120;&#65292;&#22312;&#22788;&#29702;&#22122;&#22768;&#26102;&#26356;&#21487;&#38752;&#65292;&#21487;&#20197;&#25552;&#39640;&#24322;&#24120;&#26816;&#27979;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.10614</link><description>&lt;p&gt;
&#26080;&#27880;&#24847;&#21147;&#26465;&#20214;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#21152;&#23494;&#36135;&#24065;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
An Attention Free Conditional Autoencoder For Anomaly Detection in Cryptocurrencies. (arXiv:2304.10614v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27880;&#24847;&#21147;&#26465;&#20214;&#33258;&#32534;&#30721;&#22120;(AF-CA)&#65292;&#29992;&#20110;&#26816;&#27979;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#24322;&#24120;&#65292;&#22312;&#22788;&#29702;&#22122;&#22768;&#26102;&#26356;&#21487;&#38752;&#65292;&#21487;&#20197;&#25552;&#39640;&#24322;&#24120;&#26816;&#27979;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26102;&#38388;&#24207;&#21015;&#20013;&#35782;&#21035;&#24322;&#24120;&#24456;&#22256;&#38590;&#65292;&#23588;&#20854;&#26159;&#22312;&#23384;&#22312;&#22823;&#37327;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#12290;&#38477;&#22122;&#25216;&#26415;&#21487;&#20197;&#21435;&#38500;&#22122;&#22768;&#65292;&#20294;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#20449;&#24687;&#30340;&#26174;&#33879;&#25439;&#22833;&#12290;&#20026;&#20102;&#26816;&#27979;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#24322;&#24120;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27880;&#24847;&#21147;&#26465;&#20214;&#33258;&#32534;&#30721;&#22120;(AF-CA)&#12290;&#25105;&#20204;&#20174;&#33258;&#32534;&#30721;&#22120;&#26465;&#20214;&#27169;&#22411;&#24320;&#22987;&#65292;&#28155;&#21152;&#20102;&#19968;&#20010;&#26080;&#27880;&#24847;&#21147;LSTM&#23618;\cite{inzirillo2022attention}&#65292;&#20197;&#20351;&#24322;&#24120;&#26816;&#27979;&#33021;&#21147;&#26356;&#21487;&#38752;&#65292;&#24182;&#22686;&#21152;&#24322;&#24120;&#26816;&#27979;&#30340;&#21151;&#29575;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;Attention Free Conditional Autoencoder&#19982;LSTM Autoencoder&#30340;&#32467;&#26524;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#26126;&#26174;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#35299;&#37322;&#33021;&#21147;&#65292;&#22240;&#27492;&#21487;&#20197;&#26356;&#22909;&#22320;&#26816;&#27979;&#22122;&#22768;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#24322;&#24120;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is difficult to identify anomalies in time series, especially when there is a lot of noise. Denoising techniques can remove the noise but this technique can cause a significant loss of information. To detect anomalies in the time series we have proposed an attention free conditional autoencoder (AF-CA). We started from the autoencoder conditional model on which we added an Attention-Free LSTM layer \cite{inzirillo2022attention} in order to make the anomaly detection capacity more reliable and to increase the power of anomaly detection. We compared the results of our Attention Free Conditional Autoencoder with those of an LSTM Autoencoder and clearly improved the explanatory power of the model and therefore the detection of anomaly in noisy time series.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#38543;&#26426;&#22806;&#25512;&#25216;&#26415;&#65292;&#29992;&#20110;&#38477;&#20302;&#26465;&#20214;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#20559;&#24046;&#65292;&#24182;&#35777;&#26126;&#22312;&#38750;&#20984;&#20809;&#28369;&#30446;&#26631;&#20989;&#25968;&#20013;&#65292;&#23558;&#22806;&#25512;&#19982;&#26041;&#24046;&#32553;&#20943;&#25216;&#26415;&#30456;&#32467;&#21512;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.10613</link><description>&lt;p&gt;
&#28040;&#38500;&#26465;&#20214;&#38543;&#26426;&#20248;&#21270;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Debiasing Conditional Stochastic Optimization. (arXiv:2304.10613v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10613
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#38543;&#26426;&#22806;&#25512;&#25216;&#26415;&#65292;&#29992;&#20110;&#38477;&#20302;&#26465;&#20214;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#20559;&#24046;&#65292;&#24182;&#35777;&#26126;&#22312;&#38750;&#20984;&#20809;&#28369;&#30446;&#26631;&#20989;&#25968;&#20013;&#65292;&#23558;&#22806;&#25512;&#19982;&#26041;&#24046;&#32553;&#20943;&#25216;&#26415;&#30456;&#32467;&#21512;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35206;&#30422;&#20102;&#22810;&#20010;&#24212;&#29992;&#39046;&#22495;&#65292;&#21253;&#25324;&#25237;&#36164;&#32452;&#21512;&#36873;&#25321;&#12289;&#24378;&#21270;&#23398;&#20064;&#12289;&#40065;&#26834;&#23398;&#20064;&#12289;&#22240;&#26524;&#25512;&#26029;&#31561;&#30340;&#26465;&#20214;&#38543;&#26426;&#20248;&#21270;&#65288;CSO&#65289;&#38382;&#39064;&#12290;&#30001;&#20110;&#20854;&#23884;&#22871;&#32467;&#26500;&#65292;CSO&#30446;&#26631;&#30340;&#26679;&#26412;&#24179;&#22343;&#26799;&#24230;&#23384;&#22312;&#20559;&#24046;&#65292;&#22240;&#27492;&#38656;&#35201;&#36739;&#39640;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#25165;&#33021;&#36798;&#21040;&#25910;&#25947;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26377;&#25928;&#38477;&#20302;&#20559;&#24046;&#30340;&#36890;&#29992;&#38543;&#26426;&#22806;&#25512;&#25216;&#26415;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#38750;&#20984;&#20809;&#28369;&#30446;&#26631;&#20989;&#25968;&#20013;&#65292;&#23558;&#36825;&#31181;&#22806;&#25512;&#19982;&#26041;&#24046;&#32553;&#20943;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#36798;&#21040;&#27604;&#29616;&#26377;&#30028;&#38480;&#26356;&#22909;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#29992;&#20110;&#26377;&#38480;&#21644;&#21464;&#37327;&#30340;CSO&#30340;&#26032;&#31639;&#27861;&#65292;&#20063;&#26174;&#33879;&#25913;&#36827;&#20102;&#29616;&#26377;&#32467;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35748;&#20026;&#25105;&#20204;&#30340;&#21435;&#20559;&#25216;&#26415;&#20063;&#21487;&#33021;&#26159;&#36866;&#29992;&#20110;&#20854;&#20182;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#30340;&#26377;&#36259;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the conditional stochastic optimization (CSO) problem which covers a variety of applications including portfolio selection, reinforcement learning, robust learning, causal inference, etc. The sample-averaged gradient of the CSO objective is biased due to its nested structure and therefore requires a high sample complexity to reach convergence. We introduce a general stochastic extrapolation technique that effectively reduces the bias. We show that for nonconvex smooth objectives, combining this extrapolation with variance reduction techniques can achieve a significantly better sample complexity than existing bounds. We also develop new algorithms for the finite-sum variant of CSO that also significantly improve upon existing results. Finally, we believe that our debiasing technique could be an interesting tool applicable to other stochastic optimization problems too.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#26631;&#35760;&#21644;&#24207;&#21015;&#32423;&#21035;&#30340;&#19981;&#21487;&#33021;&#24615;&#25439;&#22833;&#65292;&#20197;&#21450;&#22312;&#22521;&#35757;&#26399;&#38388;&#30340;&#37325;&#22797;&#24809;&#32602;&#12289;&#25512;&#29702;&#21644;&#21518;&#22788;&#29702;&#31561;&#22810;&#23618;&#38754;&#26041;&#27861;&#26469;&#25233;&#21046;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#37325;&#22797;&#65292;&#24182;&#36991;&#20813;&#29983;&#25104;&#25915;&#20987;&#24615;&#20869;&#23481;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.10611</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#26041;&#38754;&#37325;&#22797;&#25233;&#21046;&#21644;&#20869;&#23481;&#35843;&#25511;
&lt;/p&gt;
&lt;p&gt;
Multi-aspect Repetition Suppression and Content Moderation of Large Language Models. (arXiv:2304.10611v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10611
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#26631;&#35760;&#21644;&#24207;&#21015;&#32423;&#21035;&#30340;&#19981;&#21487;&#33021;&#24615;&#25439;&#22833;&#65292;&#20197;&#21450;&#22312;&#22521;&#35757;&#26399;&#38388;&#30340;&#37325;&#22797;&#24809;&#32602;&#12289;&#25512;&#29702;&#21644;&#21518;&#22788;&#29702;&#31561;&#22810;&#23618;&#38754;&#26041;&#27861;&#26469;&#25233;&#21046;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#37325;&#22797;&#65292;&#24182;&#36991;&#20813;&#29983;&#25104;&#25915;&#20987;&#24615;&#20869;&#23481;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#22312;NLP&#39046;&#22495;&#26159;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;&#39046;&#22495;&#20043;&#19968;&#65292;&#36817;&#24180;&#26469;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24102;&#26469;&#30340;&#36827;&#27493;&#24471;&#21040;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#20316;&#20026;&#32534;&#20889;&#21161;&#25163;&#24212;&#29992;&#31243;&#24207;&#30340;&#20851;&#38190;&#24037;&#20855;&#65292;&#23427;&#20204;&#36890;&#24120;&#23481;&#26131;&#22797;&#21046;&#25110;&#25193;&#23637;&#36755;&#20837;&#20013;&#25552;&#20379;&#30340;&#20855;&#26377;&#25915;&#20987;&#24615;&#30340;&#20869;&#23481;&#12290;&#22312;&#20302;&#36164;&#28304;&#25968;&#25454;&#29615;&#22659;&#20013;&#65292;&#23427;&#20204;&#20063;&#21487;&#33021;&#23548;&#33268;&#36755;&#20986;&#37325;&#22797;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31934;&#30830;&#21644;&#38750;&#31934;&#30830;&#37325;&#22797;&#25233;&#21046;&#30340;&#32467;&#21512;&#26041;&#27861;&#65292;&#20351;&#29992;&#26631;&#35760;&#21644;&#24207;&#21015;&#32423;&#21035;&#30340;&#19981;&#21487;&#33021;&#24615;&#25439;&#22833;&#65292;&#22521;&#35757;&#26399;&#38388;&#30340;&#37325;&#22797;&#24809;&#32602;&#12289;&#25512;&#29702;&#21644;&#21518;&#22788;&#29702;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#22810;&#32423;&#19981;&#21487;&#33021;&#24615;&#25439;&#22833;&#30340;&#33539;&#22260;&#65292;&#20197;&#36171;&#20104;&#27169;&#22411;&#36991;&#20813;&#20174;&#19968;&#24320;&#22987;&#20135;&#29983;&#25915;&#20987;&#24615;&#35789;&#27719;&#21644;&#30701;&#35821;&#30340;&#33021;&#21147;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#22312;&#22810;&#20010;&#24230;&#37327;&#26631;&#20934;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural language generation is one of the most impactful fields in NLP, and recent years have witnessed its evolution brought about by large language models (LLMs). As the key instrument for writing assistance applications, they are generally prone to replicating or extending offensive content provided in the input. In low-resource data regime, they can also lead to repetitive outputs (Holtzman et al., 2019) [1]. Usually, offensive content and repetitions are mitigated with post-hoc methods, including n-gram level blocklists, top-k and nucleus sampling. In this paper, we introduce a combination of exact and non-exact repetition suppression using token and sequence level unlikelihood loss, repetition penalty during training, inference, and post-processing respectively. We further explore multi-level unlikelihood loss to the extent that it endows the model with abilities to avoid generating offensive words and phrases from the beginning. Finally, with comprehensive experiments, we demons
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#23458;&#25143;&#27969;&#22833;&#22240;&#26524;&#20998;&#26512;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#31867;&#21644;&#39034;&#24207;&#27169;&#24335;&#25366;&#25496;&#26041;&#27861;&#32467;&#21512;&#22240;&#26524;&#36125;&#21494;&#26031;&#32593;&#32476;&#39044;&#27979;&#23548;&#33268;&#23458;&#25143;&#27969;&#22833;&#30340;&#21407;&#22240;&#27010;&#29575;&#12290;&#36890;&#36807;&#27979;&#35797;&#25968;&#25454;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#35777;&#23454;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.10604</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#23458;&#25143;&#27969;&#22833;&#30340;&#22240;&#26524;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Causal Analysis of Customer Churn Using Deep Learning. (arXiv:2304.10604v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10604
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#23458;&#25143;&#27969;&#22833;&#22240;&#26524;&#20998;&#26512;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#31867;&#21644;&#39034;&#24207;&#27169;&#24335;&#25366;&#25496;&#26041;&#27861;&#32467;&#21512;&#22240;&#26524;&#36125;&#21494;&#26031;&#32593;&#32476;&#39044;&#27979;&#23548;&#33268;&#23458;&#25143;&#27969;&#22833;&#30340;&#21407;&#22240;&#27010;&#29575;&#12290;&#36890;&#36807;&#27979;&#35797;&#25968;&#25454;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#35777;&#23454;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23458;&#25143;&#27969;&#22833;&#25351;&#32456;&#27490;&#19982;&#21830;&#19994;&#20851;&#31995;&#25110;&#22312;&#25351;&#23450;&#26399;&#38388;&#20869;&#20943;&#23569;&#23458;&#25143;&#21442;&#19982;&#24230;&#12290;&#22686;&#21152;&#24066;&#22330;&#20221;&#39069;&#21644;&#20215;&#20540;&#26377;&#20004;&#20010;&#20027;&#35201;&#30340;&#21830;&#19994;&#33829;&#38144;&#31574;&#30053;&#65306;&#21560;&#24341;&#26032;&#23458;&#25143;&#21644;&#20445;&#30041;&#29616;&#26377;&#23458;&#25143;&#12290;&#19982;&#23458;&#25143;&#33719;&#21462;&#25104;&#26412;&#30456;&#27604;&#65292;&#23458;&#25143;&#20445;&#30041;&#25104;&#26412;&#21487;&#33021;&#39640;&#20986;&#20116;&#21040;&#20845;&#20493;&#65292;&#22240;&#27492;&#23545;&#20855;&#26377;&#27969;&#22833;&#39118;&#38505;&#30340;&#23458;&#25143;&#36827;&#34892;&#25237;&#36164;&#26159;&#26126;&#26234;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20351;&#29992;&#28145;&#24230;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#26469;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#32467;&#21512;&#39640;&#32500;&#31232;&#30095;&#25968;&#25454;&#20013;&#30340;&#39034;&#24207;&#27169;&#24335;&#25366;&#25496;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#22240;&#26524;&#36125;&#21494;&#26031;&#32593;&#32476;&#26469;&#39044;&#27979;&#23548;&#33268;&#23458;&#25143;&#27969;&#22833;&#30340;&#21407;&#22240;&#27010;&#29575;&#12290;&#27979;&#35797;&#25968;&#25454;&#30340;&#35780;&#20272;&#25351;&#26631;&#35777;&#23454;&#20102;XGBoost&#21644;&#25105;&#20204;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Customer churn describes terminating a relationship with a business or reducing customer engagement over a specific period. Two main business marketing strategies play vital roles to increase market share dollar-value: gaining new and preserving existing customers. Customer acquisition cost can be five to six times that for customer retention, hence investing in customers with churn risk is smart. Causal analysis of the churn model can predict whether a customer will churn in the foreseeable future and assist enterprises to identify effects and possible causes for churn and subsequently use that knowledge to apply tailored incentives. This paper proposes a framework using a deep feedforward neural network for classification accompanied by a sequential pattern mining method on high-dimensional sparse data. We also propose a causal Bayesian network to predict cause probabilities that lead to customer churn. Evaluation metrics on test data confirm the XGBoost and our deep learning model o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#22120; B-Learner&#65292;&#23427;&#21487;&#20197;&#22312;&#38480;&#21046;&#38544;&#34255;&#28151;&#28102;&#27700;&#24179;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#22320;&#23398;&#20064; CATE &#20989;&#25968;&#30340;&#23574;&#38160;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2304.10577</link><description>&lt;p&gt;
B-Learner&#65306;&#38544;&#34255;&#28151;&#28102;&#19979;&#24322;&#36136;&#22240;&#26524;&#25928;&#24212;&#30340;&#20934;&#31070;&#35861;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
B-Learner: Quasi-Oracle Bounds on Heterogeneous Causal Effects Under Hidden Confounding. (arXiv:2304.10577v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10577
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#22120; B-Learner&#65292;&#23427;&#21487;&#20197;&#22312;&#38480;&#21046;&#38544;&#34255;&#28151;&#28102;&#27700;&#24179;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#22320;&#23398;&#20064; CATE &#20989;&#25968;&#30340;&#23574;&#38160;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35266;&#23519;&#25968;&#25454;&#20013;&#20272;&#35745;&#24322;&#36136;&#27835;&#30103;&#25928;&#24212;&#26159;&#35768;&#22810;&#39046;&#22495;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#65292;&#26377;&#21161;&#20110;&#25919;&#31574;&#21644;&#20915;&#31574;&#32773;&#20570;&#20986;&#26356;&#22909;&#30340;&#34892;&#21160;&#12290;&#36817;&#24180;&#26469;&#65292;&#22312;&#20272;&#35745;&#26465;&#20214;&#24179;&#22343;&#27835;&#30103;&#25928;&#24212;&#65288;CATE&#65289;&#20989;&#25968;&#26041;&#38754;&#21462;&#24471;&#20102;&#40065;&#26834;&#19988;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#26410;&#32771;&#34385;&#38544;&#34255;&#28151;&#28102;&#30340;&#39118;&#38505;&#65292;&#36825;&#21487;&#33021;&#20250;&#23545;&#22522;&#20110;&#35266;&#23519;&#25968;&#25454;&#30340;&#20219;&#20309;&#22240;&#26524;&#20272;&#35745;&#36896;&#25104;&#20219;&#24847;&#21644;&#19981;&#30693;&#24773;&#30340;&#20559;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;B-Learner&#30340;&#20803;&#23398;&#20064;&#22120;&#65292;&#23427;&#21487;&#20197;&#22312;&#38480;&#21046;&#38544;&#34255;&#28151;&#28102;&#27700;&#24179;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#22320;&#23398;&#20064;CATE&#20989;&#25968;&#30340;&#23574;&#38160;&#30028;&#38480;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#26368;&#36817;&#38024;&#23545;&#24179;&#22343;&#27835;&#30103;&#25928;&#24212;&#30340;&#23574;&#38160;&#19988;&#26377;&#25928;&#36793;&#30028;&#32467;&#26524;&#65288;Dorn&#31561;&#20154;&#65292;2021&#65289;&#35843;&#25972;&#20026;Kallus&#65286;Oprescu&#65288;2022&#65289;&#25152;&#25552;&#20379;&#30340;&#31283;&#20581;&#21644;&#27169;&#22411;&#26080;&#20851;&#30340;&#20998;&#24067;&#24335;&#27835;&#30103;&#25928;&#24212;&#23398;&#20064;&#26694;&#26550;&#65292;&#27966;&#29983;&#20986;B-Learner&#12290;B-Learner&#21487;&#20197;&#20351;&#29992;&#20219;&#20309;&#20989;&#25968;&#20272;&#35745;&#22120;&#65292;&#20363;&#22914;&#38543;&#26426;&#26862;&#26519;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23427;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating heterogeneous treatment effects from observational data is a crucial task across many fields, helping policy and decision-makers take better actions. There has been recent progress on robust and efficient methods for estimating the conditional average treatment effect (CATE) function, but these methods often do not take into account the risk of hidden confounding, which could arbitrarily and unknowingly bias any causal estimate based on observational data. We propose a meta-learner called the B-Learner, which can efficiently learn sharp bounds on the CATE function under limits on the level of hidden confounding. We derive the B-Learner by adapting recent results for sharp and valid bounds of the average treatment effect (Dorn et al., 2021) into the framework given by Kallus &amp; Oprescu (2022) for robust and model-agnostic learning of distributional treatment effects. The B-Learner can use any function estimator such as random forests and deep neural networks, and we prove its 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#35299;&#37322;&#38544;&#24335;Q&#23398;&#20064;(IQL)&#20316;&#20026;Actor-Critic&#26041;&#27861;&#65292;&#25552;&#20986;&#20351;&#29992;&#25193;&#25955;&#34892;&#20026;&#31574;&#30053;&#21644;&#35780;&#21028;&#22120;&#26435;&#37325;&#26469;&#24179;&#34913;&#22870;&#21169;&#26368;&#22823;&#21270;&#21644;&#19982;&#34892;&#20026;&#31574;&#30053;&#30340;&#20998;&#27495;&#12290;&#36825;&#20010;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#22797;&#26434;&#21644;&#22810;&#23792;&#29305;&#24449;&#30340;Actor&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.10573</link><description>&lt;p&gt;
IDQL: &#20316;&#20026;&#19968;&#31181;&#25193;&#25955;&#31574;&#30053;&#30340;Actor-Critic&#26041;&#27861;&#30340;&#38544;&#24335;Q&#23398;&#20064;&#12290; (arXiv:2304.10573v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
IDQL: Implicit Q-Learning as an Actor-Critic Method with Diffusion Policies. (arXiv:2304.10573v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10573
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#35299;&#37322;&#38544;&#24335;Q&#23398;&#20064;(IQL)&#20316;&#20026;Actor-Critic&#26041;&#27861;&#65292;&#25552;&#20986;&#20351;&#29992;&#25193;&#25955;&#34892;&#20026;&#31574;&#30053;&#21644;&#35780;&#21028;&#22120;&#26435;&#37325;&#26469;&#24179;&#34913;&#22870;&#21169;&#26368;&#22823;&#21270;&#21644;&#19982;&#34892;&#20026;&#31574;&#30053;&#30340;&#20998;&#27495;&#12290;&#36825;&#20010;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#22797;&#26434;&#21644;&#22810;&#23792;&#29305;&#24449;&#30340;Actor&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#31163;&#32447;RL&#26041;&#27861;&#38656;&#35201;&#27491;&#30830;&#22788;&#29702;&#36229;&#20986;&#20998;&#24067;&#30340;&#34892;&#20026;&#12290;&#38544;&#24335;Q&#23398;&#20064;&#65288;IQL&#65289;&#36890;&#36807;&#20165;&#20351;&#29992;&#25968;&#25454;&#38598;&#34892;&#21160;&#36890;&#36807;&#20462;&#25913;&#21518;&#30340;Bellman Backup&#26469;&#35757;&#32451;Q&#20989;&#25968;&#26469;&#35299;&#20915;&#27492;&#38382;&#39064;&#12290;&#20294;&#26159;&#65292;&#19981;&#28165;&#26970;&#21738;&#20010;&#31574;&#30053;&#23454;&#38469;&#19978;&#23454;&#29616;&#20102;&#27492;&#38544;&#21547;&#35757;&#32451;&#30340;Q&#20989;&#25968;&#25152;&#20195;&#34920;&#30340;&#20540;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;IQL&#37325;&#26032;&#35299;&#37322;&#20026;Actor-Critic&#26041;&#27861;&#65292;&#36890;&#36807;&#24191;&#20041;&#21270;&#35780;&#21028;&#30446;&#26631;&#24182;&#23558;&#20854;&#36830;&#25509;&#21040;&#34892;&#20026;&#35268;&#33539;&#21270;&#30340;&#38544;&#24335;Actor&#26469;&#23454;&#29616;&#12290;&#36825;&#31181;&#27867;&#21270;&#26174;&#31034;&#20102;&#24341;&#20837;&#30340;Actor&#22914;&#20309;&#24179;&#34913;&#22870;&#21169;&#26368;&#22823;&#21270;&#21644;&#19982;&#34892;&#20026;&#31574;&#30053;&#30340;&#20998;&#27495;&#65292;&#20855;&#20307;&#30340;&#25439;&#22833;&#36873;&#25321;&#20915;&#23450;&#20102;&#36825;&#31181;&#26435;&#34913;&#30340;&#24615;&#36136;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#20010;Actor&#21487;&#20197;&#34920;&#29616;&#20986;&#22797;&#26434;&#21644;&#22810;&#23792;&#30340;&#29305;&#24449;&#65292;&#36825;&#34920;&#26126;&#20102;&#21033;&#29992;&#20248;&#21183;&#21152;&#26435;&#22238;&#24402;&#65288;AWR&#65289;&#20013;&#20351;&#29992;&#30340;&#26465;&#20214;&#39640;&#26031;Actor&#30340;&#25311;&#21512;&#38382;&#39064;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#26469;&#33258;&#21442;&#25968;&#21270;&#25193;&#25955;&#34892;&#20026;&#31574;&#30053;&#30340;&#26679;&#26412;&#21644;&#30001;&#35780;&#21028;&#22120;&#35745;&#31639;&#30340;&#26435;&#37325;&#65292;&#28982;&#21518;&#23558;&#20854;&#23548;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effective offline RL methods require properly handling out-of-distribution actions. Implicit Q-learning (IQL) addresses this by training a Q-function using only dataset actions through a modified Bellman backup. However, it is unclear which policy actually attains the values represented by this implicitly trained Q-function. In this paper, we reinterpret IQL as an actor-critic method by generalizing the critic objective and connecting it to a behavior-regularized implicit actor. This generalization shows how the induced actor balances reward maximization and divergence from the behavior policy, with the specific loss choice determining the nature of this tradeoff. Notably, this actor can exhibit complex and multimodal characteristics, suggesting issues with the conditional Gaussian actor fit with advantage weighted regression (AWR) used in prior methods. Instead, we propose using samples from a diffusion parameterized behavior policy and weights computed from the critic to then importa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;Z3&#27714;&#35299;&#22120;&#23545;&#20840;&#23616;&#40065;&#26834;&#24615;&#21487;&#39564;&#35777;&#26694;&#26550;DeepGlobal&#36827;&#34892;&#26356;&#26126;&#30830;&#30340;&#23450;&#20041;&#21644;&#20248;&#21270;&#30340;&#24037;&#20316;&#65292;&#26469;&#24314;&#31435;FNN&#30340;&#24418;&#24335;&#21270;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#26356;&#26377;&#25928;&#30340;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2304.10558</link><description>&lt;p&gt;
&#20351;&#29992;Z3&#36827;&#34892;FNN&#20840;&#23616;&#40065;&#26834;&#24615;&#30340;&#24418;&#24335;&#21270;&#24314;&#27169;&#21644;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Using Z3 for Formal Modeling and Verification of FNN Global Robustness. (arXiv:2304.10558v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10558
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;Z3&#27714;&#35299;&#22120;&#23545;&#20840;&#23616;&#40065;&#26834;&#24615;&#21487;&#39564;&#35777;&#26694;&#26550;DeepGlobal&#36827;&#34892;&#26356;&#26126;&#30830;&#30340;&#23450;&#20041;&#21644;&#20248;&#21270;&#30340;&#24037;&#20316;&#65292;&#26469;&#24314;&#31435;FNN&#30340;&#24418;&#24335;&#21270;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#26356;&#26377;&#25928;&#30340;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65288;FNN&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#23545;&#23545;&#25239;&#26679;&#26412;&#24456;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#12290;&#24050;&#32463;&#24320;&#21457;&#20102;&#20960;&#31181;&#25216;&#26415;&#26469;&#39564;&#35777;FNN&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#20294;&#22823;&#22810;&#25968;&#25216;&#26415;&#37117;&#38598;&#20013;&#22312;&#38024;&#23545;&#21333;&#20010;&#25968;&#25454;&#28857;&#30340;&#23616;&#37096;&#25200;&#21160;&#37051;&#22495;&#30340;&#40065;&#26834;&#24615;&#39564;&#35777;&#19978;&#12290;&#20840;&#23616;&#40065;&#26834;&#24615;&#20998;&#26512;&#20173;&#23384;&#22312;&#36739;&#22823;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;DeepGlobal&#26159;&#19968;&#31181;&#20840;&#23616;&#40065;&#26834;&#24615;&#21487;&#39564;&#35777;&#26694;&#26550;&#65292;&#26088;&#22312;&#30830;&#23450;FNN&#30340;&#25152;&#26377;&#21487;&#33021;&#30340;&#23545;&#25239;&#21361;&#38505;&#21306;&#22495;&#65288;ADR&#65289;&#65292;&#19981;&#38480;&#20110;&#27979;&#35797;&#38598;&#20013;&#30340;&#25968;&#25454;&#26679;&#26412;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;DeepGlobal&#30340;&#23436;&#25972;&#35268;&#33539;&#21644;&#23454;&#29616;&#65292;&#21033;&#29992;SMT&#27714;&#35299;&#22120;Z3&#36827;&#34892;&#26356;&#26126;&#30830;&#30340;&#23450;&#20041;&#65292;&#24182;&#25552;&#20986;&#20102;&#20960;&#39033;&#25913;&#36827;&#20197;&#36827;&#34892;&#26356;&#39640;&#25928;&#30340;&#39564;&#35777;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#23454;&#29616;&#21644;&#25913;&#36827;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23545;&#19968;&#32452;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#36827;&#34892;&#20102;&#21487;&#35270;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Feedforward Neural Networks (FNNs) have achieved remarkable success in various tasks, they are vulnerable to adversarial examples. Several techniques have been developed to verify the adversarial robustness of FNNs, but most of them focus on robustness verification against the local perturbation neighborhood of a single data point. There is still a large research gap in global robustness analysis. The global-robustness verifiable framework DeepGlobal has been proposed to identify \textit{all} possible Adversarial Dangerous Regions (ADRs) of FNNs, not limited to data samples in a test set. In this paper, we propose a complete specification and implementation of DeepGlobal utilizing the SMT solver Z3 for more explicit definition, and propose several improvements to DeepGlobal for more efficient verification. To evaluate the effectiveness of our implementation and improvements, we conduct extensive experiments on a set of benchmark datasets. Visualization of our experiment results s
&lt;/p&gt;</description></item><item><title>Transformer&#26159;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#32452;&#20214;&#65292;&#21487;&#20197;&#23398;&#20064;&#24207;&#21015;&#25110;&#25968;&#25454;&#38598;&#34920;&#31034;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26102;&#31354;&#24314;&#27169;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#25968;&#23398;&#31934;&#30830;&#12289;&#30452;&#35266;&#12289;&#31616;&#27905;&#30340;Transformer&#26550;&#26500;&#25551;&#36848;&#12290;</title><link>http://arxiv.org/abs/2304.10557</link><description>&lt;p&gt;
Transformer&#20171;&#32461;
&lt;/p&gt;
&lt;p&gt;
An Introduction to Transformers. (arXiv:2304.10557v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10557
&lt;/p&gt;
&lt;p&gt;
Transformer&#26159;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#32452;&#20214;&#65292;&#21487;&#20197;&#23398;&#20064;&#24207;&#21015;&#25110;&#25968;&#25454;&#38598;&#34920;&#31034;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26102;&#31354;&#24314;&#27169;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#25968;&#23398;&#31934;&#30830;&#12289;&#30452;&#35266;&#12289;&#31616;&#27905;&#30340;Transformer&#26550;&#26500;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#26159;&#19968;&#31181;&#21487;&#20197;&#23398;&#20064;&#24207;&#21015;&#25110;&#25968;&#25454;&#38598;&#34920;&#31034;&#30340;&#31070;&#32463;&#32593;&#32476;&#32452;&#20214;&#12290;Transformer&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26102;&#31354;&#24314;&#27169;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#34429;&#28982;&#26377;&#24456;&#22810;Transformer&#30340;&#20171;&#32461;&#65292;&#20294;&#22823;&#22810;&#25968;&#37117;&#32570;&#23569;&#23545;&#20854;&#26550;&#26500;&#30340;&#31934;&#30830;&#25968;&#23398;&#25551;&#36848;&#65292;&#20854;&#35774;&#35745;&#36873;&#25321;&#30340;&#30452;&#35273;&#20063;&#24120;&#24120;&#32570;&#22833;&#12290;&#27492;&#22806;&#65292;&#38543;&#30528;&#30740;&#31350;&#36335;&#24452;&#30340;&#26354;&#25240;&#65292;Transformer&#37096;&#20214;&#30340;&#35299;&#37322;&#21487;&#33021;&#26159;&#24322;&#36136;&#30340;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#25968;&#23398;&#31934;&#30830;&#12289;&#30452;&#35266;&#12289;&#31616;&#27905;&#30340;Transformer&#26550;&#26500;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
The transformer is a neural network component that can be used to learn useful representations of sequences or sets of datapoints. The transformer has driven recent advances in natural language processing, computer vision, and spatio-temporal modelling. There are many introductions to transformers, but most do not contain precise mathematical descriptions of the architecture and the intuitions behind the design choices are often also missing. Moreover, as research takes a winding path, the explanations for the components of the transformer can be idiosyncratic. In this note we aim for a mathematically precise, intuitive, and clean description of the transformer architecture.
&lt;/p&gt;</description></item><item><title>&#31232;&#30095;&#24615;&#33021;&#22815;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#30340;&#38544;&#31169;&#65292;&#24182;&#19988;&#33021;&#22815;&#20445;&#25345;&#32593;&#32476;&#30340;&#34920;&#29616;</title><link>http://arxiv.org/abs/2304.10553</link><description>&lt;p&gt;
&#31232;&#30095;&#24615;&#33021;&#22815;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#30340;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
Sparsity in neural networks can improve their privacy. (arXiv:2304.10553v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10553
&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#24615;&#33021;&#22815;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#30340;&#38544;&#31169;&#65292;&#24182;&#19988;&#33021;&#22815;&#20445;&#25345;&#32593;&#32476;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#31232;&#30095;&#24615;&#22914;&#20309;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#23545;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#31232;&#30095;&#24615;&#33021;&#22815;&#25552;&#39640;&#32593;&#32476;&#30340;&#38544;&#31169;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#22312;&#20219;&#21153;&#19978;&#30340;&#30456;&#20284;&#34920;&#29616;&#12290;&#36825;&#20010;&#23454;&#35777;&#30740;&#31350;&#23436;&#21892;&#21644;&#25193;&#23637;&#20102;&#29616;&#26377;&#25991;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article measures how sparsity can make neural networks more robust to membership inference attacks. The obtained empirical results show that sparsity improves the privacy of the network, while preserving comparable performances on the task at hand. This empirical study completes and extends existing literature.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#25554;&#20540;&#20219;&#20309;&#25968;&#25454;&#38598;&#65292;&#21363;&#25439;&#22833;&#20989;&#25968;&#20855;&#26377;&#20840;&#23616;&#26368;&#23567;&#20540;&#20026;&#38646;&#30340;&#24615;&#36136;&#65292;&#27492;&#22806;&#36824;&#32473;&#20986;&#20102;&#35813;&#20840;&#23616;&#26368;&#23567;&#20540;&#22788;&#30340;&#24815;&#24615;&#30697;&#38453;&#30340;&#34920;&#24449;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#27010;&#29575;&#26041;&#27861;&#26469;&#23547;&#25214;&#25554;&#20540;&#28857;&#12290;</title><link>http://arxiv.org/abs/2304.10552</link><description>&lt;p&gt;
&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#25554;&#20540;&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;
Interpolation property of shallow neural networks. (arXiv:2304.10552v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10552
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#25554;&#20540;&#20219;&#20309;&#25968;&#25454;&#38598;&#65292;&#21363;&#25439;&#22833;&#20989;&#25968;&#20855;&#26377;&#20840;&#23616;&#26368;&#23567;&#20540;&#20026;&#38646;&#30340;&#24615;&#36136;&#65292;&#27492;&#22806;&#36824;&#32473;&#20986;&#20102;&#35813;&#20840;&#23616;&#26368;&#23567;&#20540;&#22788;&#30340;&#24815;&#24615;&#30697;&#38453;&#30340;&#34920;&#24449;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#27010;&#29575;&#26041;&#27861;&#26469;&#23547;&#25214;&#25554;&#20540;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#36229;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25439;&#22833;&#20989;&#25968;&#20840;&#23616;&#26368;&#23567;&#20540;&#30340;&#20960;&#20309;&#24615;&#36136;&#12290;&#22312;&#22823;&#22810;&#25968;&#20248;&#21270;&#38382;&#39064;&#20013;&#65292;&#25439;&#22833;&#20989;&#25968;&#26159;&#20984;&#20989;&#25968;&#65292;&#36825;&#31181;&#24773;&#20917;&#19979;&#25105;&#20204;&#21482;&#26377;&#19968;&#20010;&#20840;&#23616;&#26368;&#23567;&#20540;&#65292;&#25110;&#32773;&#26159;&#38750;&#20984;&#20989;&#25968;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#25105;&#20204;&#26377;&#19968;&#20010;&#26377;&#38480;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#36229;&#21442;&#25968;&#21270;&#33539;&#22260;&#20869;&#65292;&#23545;&#20110;&#38750;&#23567;&#27425;&#25968;&#22810;&#39033;&#24335;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#25554;&#20540;&#20219;&#20309;&#25968;&#25454;&#38598;&#65292;&#21363;&#25439;&#22833;&#20989;&#25968;&#20855;&#26377;&#20840;&#23616;&#26368;&#23567;&#20540;&#20026;&#38646;&#30340;&#24615;&#36136;&#12290;&#27492;&#22806;&#65292;&#22914;&#26524;&#23384;&#22312;&#36825;&#26679;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#65292;&#21017;&#20840;&#23616;&#26368;&#23567;&#20540;&#30340;&#36718;&#24275;&#26377;&#26080;&#31351;&#22810;&#20010;&#28857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#22312;&#20840;&#23616;&#26368;&#23567;&#20540;&#22788;&#27714;&#35299;&#25439;&#22833;&#20989;&#25968;&#30340;&#28023;&#22622;&#30697;&#38453;&#30340;&#34920;&#24449;&#65292;&#24182;&#22312;&#26368;&#21518;&#19968;&#33410;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#27010;&#29575;&#26041;&#27861;&#26469;&#23547;&#25214;&#25554;&#20540;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the geometry of global minima of the loss landscape of overparametrized neural networks. In most optimization problems, the loss function is convex, in which case we only have a global minima, or nonconvex, with a discrete number of global minima. In this paper, we prove that in the overparametrized regime, a shallow neural network can interpolate any data set, i.e. the loss function has a global minimum value equal to zero as long as the activation function is not a polynomial of small degree. Additionally, if such a global minimum exists, then the locus of global minima has infinitely many points. Furthermore, we give a characterization of the Hessian of the loss function evaluated at the global minima, and in the last section, we provide a practical probabilistic method of finding the interpolation point.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#22312;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;IDS&#30340;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#65292;&#35813;&#25216;&#26415;&#21033;&#29992;&#22810;&#20010;&#39046;&#22495;&#30340;&#30693;&#35782;&#34701;&#21512;&#21644;/&#25110;&#36866;&#24212;&#20197;&#25552;&#39640;&#30446;&#26631;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#30446;&#26631;&#39046;&#22495;&#20013;&#30340;&#26631;&#35760;&#25968;&#25454;&#38750;&#24120;&#23569;&#30340;&#24773;&#20917;&#19979;&#12290;</title><link>http://arxiv.org/abs/2304.10550</link><description>&lt;p&gt;
&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#22312;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Deep Transfer Learning Applications in Intrusion Detection Systems: A Comprehensive Review. (arXiv:2304.10550v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10550
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#22312;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;IDS&#30340;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#65292;&#35813;&#25216;&#26415;&#21033;&#29992;&#22810;&#20010;&#39046;&#22495;&#30340;&#30693;&#35782;&#34701;&#21512;&#21644;/&#25110;&#36866;&#24212;&#20197;&#25552;&#39640;&#30446;&#26631;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#30446;&#26631;&#39046;&#22495;&#20013;&#30340;&#26631;&#35760;&#25968;&#25454;&#38750;&#24120;&#23569;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#33539;&#22260;&#20869;&#65292;&#22806;&#37096;&#20114;&#32852;&#32593;&#36234;&#26469;&#36234;&#22810;&#22320;&#19982;&#24403;&#20195;&#24037;&#19994;&#25511;&#21046;&#31995;&#32479;&#30456;&#36830;&#25509;&#12290;&#22240;&#27492;&#65292;&#26377;&#19968;&#20010;&#36843;&#20999;&#30340;&#38656;&#27714;&#20445;&#25252;&#32593;&#32476;&#20813;&#21463;&#21508;&#31181;&#23041;&#32961;&#12290;&#21487;&#20197;&#20351;&#29992;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65288;IDS&#65289;&#26469;&#20445;&#25252;&#24037;&#19994;&#27963;&#21160;&#30340;&#20851;&#38190;&#22522;&#30784;&#35774;&#26045;&#65292;&#36825;&#26159;&#19968;&#31181;&#39044;&#38450;&#24615;&#25514;&#26045;&#26426;&#21046;&#65292;&#29992;&#20110;&#35782;&#21035;&#26032;&#30340;&#21361;&#38505;&#23041;&#32961;&#21644;&#25932;&#23545;&#27963;&#21160;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#22312;&#35768;&#22810;&#31181;&#24037;&#19994;&#25511;&#21046;&#32593;&#32476;&#20013;&#21019;&#24314;IDS&#30340;&#26368;&#26032;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25216;&#26415;&#65292;&#29305;&#21035;&#20391;&#37325;&#20110;&#22522;&#20110;IDS&#30340;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#65288;DTL&#65289;&#12290;DTL&#21487;&#20197;&#30475;&#20316;&#26159;&#23558;&#26469;&#33258;&#22810;&#20010;&#39046;&#22495;&#30340;&#30693;&#35782;&#34701;&#21512;&#21644;/&#25110;&#36866;&#24212;&#20197;&#22686;&#24378;&#30446;&#26631;&#20219;&#21153;&#30340;&#24615;&#33021;&#30340;&#19968;&#31181;&#20449;&#24687;&#34701;&#21512;&#12290;&#37325;&#28857;&#26159;&#24403;&#30446;&#26631;&#22495;&#20013;&#30340;&#26631;&#35760;&#25968;&#25454;&#24456;&#23569;&#26102;&#65292;DTL&#21487;&#20197;&#24110;&#21161;&#25552;&#39640;IDS&#30340;&#24615;&#33021;&#12290;&#32771;&#34385;&#20102;2015&#24180;&#20043;&#21518;&#30340;&#20986;&#29256;&#29289;&#12290;&#36825;&#20123;&#36873;&#23450;&#30340;&#20986;&#29256;&#29289;&#34987;&#20998;&#20026;&#19977;&#31867;&#65306;&#20165;DTL&#21644;&#20165;IDS&#65292;&#20855;&#26377;&#36801;&#31227;&#23398;&#20064;&#65288;TL&#65289;&#30340;IDS&#65292;&#20197;&#21450;&#22522;&#20110;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#30340;IDS&#12290;&#35813;&#30740;&#31350;&#20840;&#38754;&#22238;&#39038;&#20102;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#20013;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#24212;&#29992;&#30340;&#26368;&#26032;&#25216;&#26415;&#21644;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Globally, the external Internet is increasingly being connected to the contemporary industrial control system. As a result, there is an immediate need to protect the network from several threats. The key infrastructure of industrial activity may be protected from harm by using an intrusion detection system (IDS), a preventive measure mechanism, to recognize new kinds of dangerous threats and hostile activities. The most recent artificial intelligence (AI) techniques used to create IDS in many kinds of industrial control networks are examined in this study, with a particular emphasis on IDS-based deep transfer learning (DTL). This latter can be seen as a type of information fusion that merge, and/or adapt knowledge from multiple domains to enhance the performance of the target task, particularly when the labeled data in the target domain is scarce. Publications issued after 2015 were taken into account. These selected publications were divided into three categories: DTL-only and IDS-onl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#20559;&#24207;&#25968;&#25454;&#28145;&#24230;&#20989;&#25968;&#30340;&#32972;&#26223;&#19979;Blocher&#31561;&#20154;[2023]&#20013;&#20171;&#32461;&#30340;&#26080;&#20132;&#36890;&#29992;&#38598;&#21512;&#20855;&#26377;&#36830;&#36890;&#24615;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.10549</link><description>&lt;p&gt;
&#20851;&#20110;&#26080;&#20132;&#38598;&#20559;&#24207;&#36890;&#29992;&#38598;&#21512;&#30340;&#36830;&#36890;&#24615;&#23646;&#24615;&#30340;&#27880;&#35760;
&lt;/p&gt;
&lt;p&gt;
A note on the connectedness property of union-free generic sets of partial orders. (arXiv:2304.10549v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10549
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#20559;&#24207;&#25968;&#25454;&#28145;&#24230;&#20989;&#25968;&#30340;&#32972;&#26223;&#19979;Blocher&#31561;&#20154;[2023]&#20013;&#20171;&#32461;&#30340;&#26080;&#20132;&#36890;&#29992;&#38598;&#21512;&#20855;&#26377;&#36830;&#36890;&#24615;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30701;&#25991;&#25551;&#36848;&#24182;&#35777;&#26126;&#20102;&#22312;&#20559;&#24207;&#25968;&#25454;&#28145;&#24230;&#20989;&#25968;&#30340;&#32972;&#26223;&#19979;Blocher&#31561;&#20154;[2023]&#24341;&#20837;&#30340;&#36830;&#36890;&#24615;&#23646;&#24615;&#12290; &#36830;&#36890;&#24615;&#23646;&#24615;&#20026;&#26080;&#20132;&#36890;&#29992;&#38598;&#21512;&#25552;&#20379;&#20102;&#32467;&#26500;&#24615;&#30340;&#28145;&#20837;&#35748;&#35782;&#12290;&#36825;&#20123;&#38598;&#21512;&#26159;&#22312;Blocher&#31561;&#20154;[2023]&#20013;&#20171;&#32461;&#30340;&#65292;&#23427;&#20204;&#20351;&#29992;&#22312;&#24418;&#24335;&#27010;&#24565;&#20998;&#26512;&#29702;&#35770;&#20013;&#33258;&#28982;&#20986;&#29616;&#30340;&#25152;&#26377;&#20559;&#24207;&#38598;&#21512;&#19978;&#30340;&#38381;&#21253;&#36816;&#31639;&#36827;&#34892;&#23450;&#20041;&#12290;&#22312;&#24418;&#24335;&#27010;&#24565;&#20998;&#26512;&#30340;&#35821;&#35328;&#20013;&#65292;&#36830;&#36890;&#24615;&#30340;&#23646;&#24615;&#21487;&#20197;&#29983;&#21160;&#22320;&#34987;&#35777;&#26126;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#22312;Blocher&#31561;&#20154;[2023]&#20013;&#25105;&#20204;&#27809;&#26377;&#35752;&#35770;&#24418;&#24335;&#27010;&#24565;&#20998;&#26512;,&#22240;&#27492;&#25105;&#20204;&#25226;&#35777;&#26126;&#25918;&#21040;&#20102;&#36825;&#37324;&#12290;
&lt;/p&gt;
&lt;p&gt;
This short note describes and proves a connectedness property which was introduced in Blocher et al. [2023] in the context of data depth functions for partial orders. The connectedness property gives a structural insight into union-free generic sets. These sets, presented in Blocher et al. [2023], are defined by using a closure operator on the set of all partial orders which naturally appears within the theory of formal concept analysis. In the language of formal concept analysis, the property of connectedness can be vividly proven. However, since within Blocher et al. [2023] we did not discuss formal concept analysis, we outsourced the proof to this note.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026; C-qGAN &#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#37327;&#23376;&#30005;&#36335;&#32467;&#26500;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#29366;&#24577;&#20934;&#22791;&#36807;&#31243;&#65292;&#21487;&#20197;&#21033;&#29992;&#35813;&#26041;&#27861;&#21152;&#36895;&#33945;&#29305;&#21345;&#32599;&#20998;&#26512;&#31561;&#31639;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#20122;&#24335;&#26399;&#26435;&#34893;&#29983;&#21697;&#23450;&#20215;&#30340;&#20219;&#21153;&#20013;&#12290;</title><link>http://arxiv.org/abs/2304.10382</link><description>&lt;p&gt;
&#23398;&#20064;&#38543;&#26426;&#36807;&#31243;&#30340;&#26377;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Conditional Generative Models for Learning Stochastic Processes. (arXiv:2304.10382v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10382
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026; C-qGAN &#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#37327;&#23376;&#30005;&#36335;&#32467;&#26500;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#29366;&#24577;&#20934;&#22791;&#36807;&#31243;&#65292;&#21487;&#20197;&#21033;&#29992;&#35813;&#26041;&#27861;&#21152;&#36895;&#33945;&#29305;&#21345;&#32599;&#20998;&#26512;&#31561;&#31639;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#20122;&#24335;&#26399;&#26435;&#34893;&#29983;&#21697;&#23450;&#20215;&#30340;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#22810;&#27169;&#24577;&#20998;&#24067;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#26465;&#20214;&#37327;&#23376;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;C-qGAN&#65289;&#12290;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#20005;&#26684;&#37319;&#29992;&#37327;&#23376;&#30005;&#36335;&#65292;&#22240;&#27492;&#34987;&#35777;&#26126;&#33021;&#22815;&#27604;&#24403;&#21069;&#30340;&#26041;&#27861;&#26356;&#26377;&#25928;&#22320;&#34920;&#31034;&#29366;&#24577;&#20934;&#22791;&#36807;&#31243;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#28508;&#21147;&#21152;&#36895;&#33945;&#29305;&#21345;&#32599;&#20998;&#26512;&#31561;&#31639;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;&#22312;&#23637;&#31034;&#20102;&#32593;&#32476;&#22312;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#21518;&#65292;&#23558;&#35813;&#25216;&#26415;&#24212;&#29992;&#20110;&#23450;&#20215;&#20122;&#24335;&#26399;&#26435;&#34893;&#29983;&#21697;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#20854;&#20182;&#36335;&#24452;&#30456;&#20851;&#26399;&#26435;&#25171;&#19979;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
A framework to learn a multi-modal distribution is proposed, denoted as the Conditional Quantum Generative Adversarial Network (C-qGAN). The neural network structure is strictly within a quantum circuit and, as a consequence, is shown to represents a more efficient state preparation procedure than current methods. This methodology has the potential to speed-up algorithms, such as Monte Carlo analysis. In particular, after demonstrating the effectiveness of the network in the learning task, the technique is applied to price Asian option derivatives, providing the foundation for further research on other path-dependent options.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#20445;&#30041;&#35889;&#30340;&#25968;&#25454;&#21387;&#32553;&#26469;&#21152;&#36895;&#25903;&#25345;&#21521;&#37327;&#32858;&#31867;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#32858;&#31867;&#36895;&#24230;&#32780;&#19981;&#29306;&#29298;&#32858;&#31867;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2304.09868</link><description>&lt;p&gt;
&#36890;&#36807;&#20445;&#30041;&#35889;&#30340;&#25968;&#25454;&#21387;&#32553;&#21152;&#36895;&#25903;&#25345;&#21521;&#37327;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Accelerate Support Vector Clustering via Spectrum-Preserving Data Compression?. (arXiv:2304.09868v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09868
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#20445;&#30041;&#35889;&#30340;&#25968;&#25454;&#21387;&#32553;&#26469;&#21152;&#36895;&#25903;&#25345;&#21521;&#37327;&#32858;&#31867;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#32858;&#31867;&#36895;&#24230;&#32780;&#19981;&#29306;&#29298;&#32858;&#31867;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25903;&#25345;&#21521;&#37327;&#32858;&#31867;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#20294;&#26159;&#30001;&#20110;&#20854;&#35745;&#31639;&#26114;&#36149;&#30340;&#31751;&#20998;&#37197;&#27493;&#39588;&#65292;&#23427;&#38754;&#20020;&#30528;&#21487;&#20280;&#32553;&#24615;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20445;&#30041;&#35889;&#30340;&#25968;&#25454;&#21387;&#32553;&#26469;&#21152;&#36895;&#25903;&#25345;&#21521;&#37327;&#32858;&#31867;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#21407;&#22987;&#25968;&#25454;&#38598;&#21387;&#32553;&#25104;&#23569;&#37327;&#35889;&#34920;&#31034;&#30340;&#32858;&#21512;&#25968;&#25454;&#28857;&#65292;&#28982;&#21518;&#22312;&#21387;&#32553;&#21518;&#30340;&#25968;&#25454;&#38598;&#19978;&#25191;&#34892;&#26631;&#20934;&#30340;&#25903;&#25345;&#21521;&#37327;&#32858;&#31867;&#65292;&#26368;&#21518;&#23558;&#21387;&#32553;&#25968;&#25454;&#38598;&#30340;&#32858;&#31867;&#32467;&#26524;&#26144;&#23556;&#22238;&#21407;&#22987;&#25968;&#25454;&#38598;&#20197;&#21457;&#29616;&#31751;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#22823;&#37327;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#36739;&#20110;&#26631;&#20934;&#25903;&#25345;&#21521;&#37327;&#32858;&#31867;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22823;&#22823;&#25552;&#39640;&#20102;&#36895;&#24230;&#65292;&#32780;&#19981;&#20250;&#25439;&#22833;&#32858;&#31867;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Support vector clustering is an important clustering method. However, it suffers from a scalability issue due to its computational expensive cluster assignment step. In this paper we accelertate the support vector clustering via spectrum-preserving data compression. Specifically, we first compress the original data set into a small amount of spectrally representative aggregated data points. Then, we perform standard support vector clustering on the compressed data set. Finally, we map the clustering results of the compressed data set back to discover the clusters in the original data set. Our extensive experimental results on real-world data set demonstrate dramatically speedups over standard support vector clustering without sacrificing clustering quality.
&lt;/p&gt;</description></item><item><title>DiFaReli&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26465;&#20214;&#25193;&#25955;&#38544;&#24335;&#27169;&#22411;&#35299;&#30721;&#35299;&#32806;&#30340;&#20809;&#32534;&#30721;&#20197;&#21450;&#20174;&#29616;&#25104;&#30340;&#20272;&#31639;&#22120;&#25512;&#26029;&#20986;&#30340;&#19982;3D&#24418;&#29366;&#21644;&#38754;&#37096;&#36523;&#20221;&#30456;&#20851;&#30340;&#20854;&#20182;&#32534;&#30721;&#65292;&#33021;&#22815;&#22788;&#29702;&#21333;&#35270;&#35282;&#30340;&#37326;&#22806;&#29615;&#22659;&#19979;&#30340;&#20154;&#33080;&#37325;&#29031;&#65292;&#26080;&#38656;&#20809;&#32447;&#33310;&#21488;&#25968;&#25454;&#12289;&#22810;&#35270;&#22270;&#22270;&#20687;&#25110;&#20809;&#29031;&#22522;&#30784;&#20107;&#23454;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#25928;&#26524;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.09479</link><description>&lt;p&gt;
DiFaReli: &#25193;&#25955;&#20154;&#33080;&#37325;&#29031;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
DiFaReli : Diffusion Face Relighting. (arXiv:2304.09479v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09479
&lt;/p&gt;
&lt;p&gt;
DiFaReli&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26465;&#20214;&#25193;&#25955;&#38544;&#24335;&#27169;&#22411;&#35299;&#30721;&#35299;&#32806;&#30340;&#20809;&#32534;&#30721;&#20197;&#21450;&#20174;&#29616;&#25104;&#30340;&#20272;&#31639;&#22120;&#25512;&#26029;&#20986;&#30340;&#19982;3D&#24418;&#29366;&#21644;&#38754;&#37096;&#36523;&#20221;&#30456;&#20851;&#30340;&#20854;&#20182;&#32534;&#30721;&#65292;&#33021;&#22815;&#22788;&#29702;&#21333;&#35270;&#35282;&#30340;&#37326;&#22806;&#29615;&#22659;&#19979;&#30340;&#20154;&#33080;&#37325;&#29031;&#65292;&#26080;&#38656;&#20809;&#32447;&#33310;&#21488;&#25968;&#25454;&#12289;&#22810;&#35270;&#22270;&#22270;&#20687;&#25110;&#20809;&#29031;&#22522;&#30784;&#20107;&#23454;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#25928;&#26524;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#37326;&#22806;&#29615;&#22659;&#19979;&#30340;&#21333;&#35270;&#35282;&#20154;&#33080;&#37325;&#29031;&#12290;&#22788;&#29702;&#20840;&#23616;&#29031;&#26126;&#25110;&#25237;&#24433;&#38452;&#24433;&#31561;&#38750;&#28459;&#21453;&#23556;&#25928;&#24212;&#19968;&#30452;&#26159;&#20154;&#33080;&#37325;&#29031;&#39046;&#22495;&#30340;&#38590;&#28857;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#36890;&#24120;&#20551;&#23450;&#20848;&#20271;&#29305;&#21453;&#23556;&#34920;&#38754;&#65292;&#31616;&#21270;&#20809;&#29031;&#27169;&#22411;&#65292;&#25110;&#32773;&#38656;&#35201;&#20272;&#35745;&#19977;&#32500;&#24418;&#29366;&#12289;&#21453;&#23556;&#29575;&#25110;&#38452;&#24433;&#22270;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20272;&#35745;&#26159;&#23481;&#26131;&#20986;&#38169;&#30340;&#65292;&#38656;&#35201;&#35768;&#22810;&#20855;&#26377;&#20809;&#29031;&#22522;&#30784;&#20107;&#23454;&#30340;&#35757;&#32451;&#26679;&#26412;&#25165;&#33021;&#24456;&#22909;&#22320;&#25512;&#24191;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32469;&#36807;&#20102;&#20934;&#30830;&#20272;&#35745;&#22266;&#26377;&#32452;&#20214;&#30340;&#38656;&#35201;&#65292;&#21487;&#20197;&#20165;&#36890;&#36807;2D&#22270;&#20687;&#35757;&#32451;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#20809;&#32447;&#33310;&#21488;&#25968;&#25454;&#12289;&#22810;&#35270;&#22270;&#22270;&#20687;&#25110;&#20809;&#29031;&#22522;&#30784;&#20107;&#23454;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#21033;&#29992;&#26465;&#20214;&#25193;&#25955;&#38544;&#24335;&#27169;&#22411;&#65288;DDIM&#65289;&#35299;&#30721;&#35299;&#32806;&#30340;&#20809;&#32534;&#30721;&#20197;&#21450;&#20174;&#29616;&#25104;&#30340;&#20272;&#31639;&#22120;&#25512;&#26029;&#20986;&#30340;&#19982;3D&#24418;&#29366;&#21644;&#38754;&#37096;&#36523;&#20221;&#30456;&#20851;&#30340;&#20854;&#20182;&#32534;&#30721;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35843;&#33410;&#25216;&#26415;&#65292;&#36890;&#36807;&#20351;&#29992;&#24402;&#19968;&#21270;&#26041;&#26696;&#65292;&#31616;&#21270;&#20809;&#19982;&#20960;&#20309;&#20043;&#38388;&#22797;&#26434;&#20114;&#21160;&#30340;&#24314;&#27169;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel approach to single-view face relighting in the wild. Handling non-diffuse effects, such as global illumination or cast shadows, has long been a challenge in face relighting. Prior work often assumes Lambertian surfaces, simplified lighting models or involves estimating 3D shape, albedo, or a shadow map. This estimation, however, is error-prone and requires many training examples with lighting ground truth to generalize well. Our work bypasses the need for accurate estimation of intrinsic components and can be trained solely on 2D images without any light stage data, multi-view images, or lighting ground truth. Our key idea is to leverage a conditional diffusion implicit model (DDIM) for decoding a disentangled light encoding along with other encodings related to 3D shape and facial identity inferred from off-the-shelf estimators. We also propose a novel conditioning technique that eases the modeling of the complex interaction between light and geometry by using a ren
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#21644;&#23454;&#26045;&#20102;&#19968;&#20010;&#21517;&#20026;MATURE-HEALTH&#30340;&#20581;&#24247;&#25512;&#33616;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#39044;&#27979;&#30005;&#35299;&#36136;&#19981;&#24179;&#34913;&#24182;&#25512;&#33616;&#33829;&#20859;&#24179;&#34913;&#30340;&#39135;&#29289;&#65292;&#20174;&#32780;&#22686;&#21152;&#26089;&#26399;&#26816;&#27979;&#30142;&#30149;&#30340;&#26426;&#20250;&#24182;&#38450;&#27490;&#20581;&#24247;&#36827;&#19968;&#27493;&#24694;&#21270;&#12290;</title><link>http://arxiv.org/abs/2304.09099</link><description>&lt;p&gt;
MATURE-HEALTH: MAndatory FeaTURE&#36873;&#25321;&#30340;&#20581;&#24247;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
MATURE-HEALTH: HEALTH Recommender System for MAndatory FeaTURE choices. (arXiv:2304.09099v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09099
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#21644;&#23454;&#26045;&#20102;&#19968;&#20010;&#21517;&#20026;MATURE-HEALTH&#30340;&#20581;&#24247;&#25512;&#33616;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#39044;&#27979;&#30005;&#35299;&#36136;&#19981;&#24179;&#34913;&#24182;&#25512;&#33616;&#33829;&#20859;&#24179;&#34913;&#30340;&#39135;&#29289;&#65292;&#20174;&#32780;&#22686;&#21152;&#26089;&#26399;&#26816;&#27979;&#30142;&#30149;&#30340;&#26426;&#20250;&#24182;&#38450;&#27490;&#20581;&#24247;&#36827;&#19968;&#27493;&#24694;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24179;&#34913;&#30005;&#35299;&#36136;&#23545;&#20110;&#20154;&#20307;&#22120;&#23448;&#30340;&#36866;&#24403;&#21151;&#33021;&#33267;&#20851;&#37325;&#35201;&#21644;&#24517;&#19981;&#21487;&#23569;&#65292;&#22240;&#20026;&#30005;&#35299;&#36136;&#22833;&#34913;&#21487;&#33021;&#26159;&#28508;&#22312;&#30149;&#29702;&#29983;&#29702;&#23398;&#21457;&#23637;&#30340;&#25351;&#31034;&#12290;&#39640;&#25928;&#30417;&#27979;&#30005;&#35299;&#36136;&#22833;&#34913;&#19981;&#20165;&#21487;&#20197;&#22686;&#21152;&#30142;&#30149;&#26089;&#26399;&#26816;&#27979;&#30340;&#26426;&#20250;&#65292;&#32780;&#19988;&#21487;&#20197;&#36890;&#36807;&#20005;&#26684;&#36981;&#24490;&#33829;&#20859;&#25511;&#21046;&#39278;&#39135;&#20197;&#24179;&#34913;&#30005;&#35299;&#36136;&#20174;&#32780;&#38450;&#27490;&#20581;&#24247;&#36827;&#19968;&#27493;&#24694;&#21270;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#24182;&#23454;&#26045;&#20102;&#19968;&#20010;&#25512;&#33616;&#31995;&#32479;MATURE Health&#65292;&#35813;&#31995;&#32479;&#39044;&#27979;&#34880;&#28082;&#20013;&#24517;&#38656;&#30005;&#35299;&#36136;&#21644;&#20854;&#20182;&#29289;&#36136;&#30340;&#19981;&#24179;&#34913;&#65292;&#28982;&#21518;&#25512;&#33616;&#21547;&#26377;&#24179;&#34913;&#33829;&#20859;&#30340;&#39135;&#29289;&#65292;&#20197;&#36991;&#20813;&#30005;&#35299;&#36136;&#19981;&#24179;&#34913;&#30340;&#21457;&#29983;&#12290;&#35813;&#27169;&#22411;&#32771;&#34385;&#21040;&#29992;&#25143;&#26368;&#36817;&#30340;&#23454;&#39564;&#23460;&#32467;&#26524;&#21644;&#27599;&#26085;&#39135;&#29289;&#25668;&#20837;&#37327;&#26469;&#39044;&#27979;&#30005;&#35299;&#36136;&#19981;&#24179;&#34913;&#12290;MATURE Health&#20381;&#36182;&#20110;MATURE Food&#31639;&#27861;&#25512;&#33616;&#39135;&#29289;&#65292;&#21518;&#32773;&#20165;&#25512;&#33616;&#37027;&#20123;
&lt;/p&gt;
&lt;p&gt;
Balancing electrolytes is utmost important and essential for appropriate functioning of organs in human body as electrolytes imbalance can be an indication of the development of underlying pathophysiology. Efficient monitoring of electrolytes imbalance not only can increase the chances of early detection of disease, but also prevents the further deterioration of the health by strictly following nutrient controlled diet for balancing the electrolytes post disease detection. In this research, a recommender system MATURE Health is proposed and implemented, which predicts the imbalance of mandatory electrolytes and other substances presented in blood and recommends the food items with the balanced nutrients to avoid occurrence of the electrolytes imbalance. The proposed model takes user most recent laboratory results and daily food intake into account to predict the electrolytes imbalance. MATURE Health relies on MATURE Food algorithm to recommend food items as latter recommends only those
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24191;&#20041;Softmax&#20989;&#25968;r-softmax&#65292;&#21487;&#20197;&#36755;&#20986;&#20855;&#26377;&#21487;&#25511;&#31232;&#30095;&#24230;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#30340;&#26367;&#20195;&#26041;&#26696;&#25928;&#26524;&#26356;&#22909;&#65292;&#22312;&#22810;&#26631;&#31614;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#31361;&#20986;&#65292;&#22312;&#39044;&#35757;&#32451;&#36716;&#25442;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#27880;&#24847;&#27169;&#22359;&#20013;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.05243</link><description>&lt;p&gt;
r-softmax: &#20855;&#26377;&#21487;&#25511;&#31232;&#30095;&#29575;&#30340;&#24191;&#20041;Softmax
&lt;/p&gt;
&lt;p&gt;
r-softmax: Generalized Softmax with Controllable Sparsity Rate. (arXiv:2304.05243v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05243
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24191;&#20041;Softmax&#20989;&#25968;r-softmax&#65292;&#21487;&#20197;&#36755;&#20986;&#20855;&#26377;&#21487;&#25511;&#31232;&#30095;&#24230;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#30340;&#26367;&#20195;&#26041;&#26696;&#25928;&#26524;&#26356;&#22909;&#65292;&#22312;&#22810;&#26631;&#31614;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#31361;&#20986;&#65292;&#22312;&#39044;&#35757;&#32451;&#36716;&#25442;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#27880;&#24847;&#27169;&#22359;&#20013;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#35768;&#22810;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;&#23558;&#27169;&#22411;&#25552;&#20379;&#30340;&#34920;&#31034;&#26144;&#23556;&#21040;&#27010;&#29575;&#20998;&#24067;&#30340;&#20989;&#25968;&#26159;&#28145;&#24230;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#30340;&#19981;&#21487;&#20998;&#21106;&#30340;&#26041;&#38754;&#12290;&#34429;&#28982;softmax&#26159;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#20013;&#36890;&#24120;&#25509;&#21463;&#30340;&#27010;&#29575;&#26144;&#23556;&#20989;&#25968;&#65292;&#20294;&#23427;&#19981;&#33021;&#36820;&#22238;&#31232;&#30095;&#30340;&#36755;&#20986;&#65292;&#24182;&#19988;&#24635;&#26159;&#23558;&#27491;&#27010;&#29575;&#20998;&#25955;&#21040;&#25152;&#26377;&#20301;&#32622;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;r-softmax&#65292;&#36825;&#26159;softmax&#30340;&#19968;&#31181;&#20462;&#25913;&#65292;&#23427;&#36755;&#20986;&#20855;&#26377;&#21487;&#25511;&#31232;&#30095;&#24230;&#30340;&#31232;&#30095;&#27010;&#29575;&#20998;&#24067;&#12290;&#19982;&#29616;&#26377;&#30340;&#31232;&#30095;&#27010;&#29575;&#26144;&#23556;&#20989;&#25968;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#30452;&#35266;&#30340;&#26426;&#21046;&#26469;&#25511;&#21046;&#36755;&#20986;&#31232;&#30095;&#24230;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22810;&#26631;&#31614;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;r-softmax&#20248;&#20110;&#20854;&#20182;&#31232;&#30095;&#30340;softmax&#26367;&#20195;&#26041;&#26696;&#65292;&#24182;&#19988;&#19982;&#21407;&#22987;&#30340;softmax&#30456;&#27604;&#20855;&#26377;&#39640;&#31454;&#20105;&#21147;&#12290;&#25105;&#20204;&#36824;&#23558;r-softmax&#24212;&#29992;&#20110;&#39044;&#35757;&#32451;&#36716;&#25442;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#27880;&#24847;&#27169;&#22359;&#20013;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays artificial neural network models achieve remarkable results in many disciplines. Functions mapping the representation provided by the model to the probability distribution are the inseparable aspect of deep learning solutions. Although softmax is a commonly accepted probability mapping function in the machine learning community, it cannot return sparse outputs and always spreads the positive probability to all positions. In this paper, we propose r-softmax, a modification of the softmax, outputting sparse probability distribution with controllable sparsity rate. In contrast to the existing sparse probability mapping functions, we provide an intuitive mechanism for controlling the output sparsity level. We show on several multi-label datasets that r-softmax outperforms other sparse alternatives to softmax and is highly competitive with the original softmax. We also apply r-softmax to the self-attention module of a pre-trained transformer language model and demonstrate that it l
&lt;/p&gt;</description></item><item><title>TPU v4&#26159;&#19968;&#27454;&#25903;&#25345;&#23884;&#20837;&#24335;&#30828;&#20214;&#30340;&#21487;&#37325;&#26500;&#20809;&#23398;&#36229;&#32423;&#35745;&#31639;&#26426;&#65292;&#37319;&#29992;&#20809;&#23398;&#30005;&#36335;&#20132;&#25442;&#26426;&#37325;&#26032;&#37197;&#32622;&#20114;&#36830;&#25299;&#25169;&#65292;&#25552;&#39640;&#35268;&#27169;&#12289;&#21487;&#29992;&#24615;&#12289;&#21033;&#29992;&#29575;&#12289;&#27169;&#22359;&#21270;&#12289;&#37096;&#32626;&#12289;&#23433;&#20840;&#12289;&#21151;&#29575;&#21644;&#24615;&#33021;&#65292;&#23427;&#36890;&#36807;SparseCores&#21152;&#36895;&#23884;&#20837;&#24335;&#27169;&#22411;&#65292;&#24615;&#33021;&#20248;&#36234;&#65292;&#21151;&#32791;&#20302;&#12290;</title><link>http://arxiv.org/abs/2304.01433</link><description>&lt;p&gt;
TPU v4&#65306;&#19968;&#27454;&#25903;&#25345;&#23884;&#20837;&#24335;&#30828;&#20214;&#30340;&#21487;&#37325;&#26500;&#20809;&#23398;&#36229;&#32423;&#35745;&#31639;&#26426;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
TPU v4: An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings. (arXiv:2304.01433v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01433
&lt;/p&gt;
&lt;p&gt;
TPU v4&#26159;&#19968;&#27454;&#25903;&#25345;&#23884;&#20837;&#24335;&#30828;&#20214;&#30340;&#21487;&#37325;&#26500;&#20809;&#23398;&#36229;&#32423;&#35745;&#31639;&#26426;&#65292;&#37319;&#29992;&#20809;&#23398;&#30005;&#36335;&#20132;&#25442;&#26426;&#37325;&#26032;&#37197;&#32622;&#20114;&#36830;&#25299;&#25169;&#65292;&#25552;&#39640;&#35268;&#27169;&#12289;&#21487;&#29992;&#24615;&#12289;&#21033;&#29992;&#29575;&#12289;&#27169;&#22359;&#21270;&#12289;&#37096;&#32626;&#12289;&#23433;&#20840;&#12289;&#21151;&#29575;&#21644;&#24615;&#33021;&#65292;&#23427;&#36890;&#36807;SparseCores&#21152;&#36895;&#23884;&#20837;&#24335;&#27169;&#22411;&#65292;&#24615;&#33021;&#20248;&#36234;&#65292;&#21151;&#32791;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21019;&#26032;&#65292;&#29983;&#20135;&#24037;&#20316;&#36127;&#36733;&#21457;&#29983;&#20102;&#26681;&#26412;&#24615;&#21644;&#36805;&#36895;&#30340;&#21464;&#21270;&#12290;TPU v4&#26159;&#35895;&#27468;&#30340;&#31532;&#20116;&#20195;&#38754;&#21521;&#29305;&#23450;&#39046;&#22495;&#26550;&#26500;&#65288;DSA&#65289;&#65292;&#26159;&#20854;&#31532;&#19977;&#20010;&#29992;&#20110;&#22788;&#29702;&#27492;&#31867;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36229;&#32423;&#35745;&#31639;&#26426;&#12290;&#20809;&#23398;&#30005;&#36335;&#20132;&#25442;&#26426;&#65288;OCS&#65289;&#21160;&#24577;&#37325;&#26032;&#37197;&#32622;&#20854;&#20114;&#36830;&#25299;&#25169;&#65292;&#20197;&#25552;&#39640;&#35268;&#27169;&#12289;&#21487;&#29992;&#24615;&#12289;&#21033;&#29992;&#29575;&#12289;&#27169;&#22359;&#21270;&#12289;&#37096;&#32626;&#12289;&#23433;&#20840;&#12289;&#21151;&#29575;&#21644;&#24615;&#33021;&#12290;&#37096;&#32626;&#33258;2020&#24180;&#20197;&#26469;&#65292;TPU v4&#36229;&#32423;&#35745;&#31639;&#26426;&#30340;&#34920;&#29616;&#20248;&#20110;TPU v3&#65292;&#21516;&#26102;&#24615;&#33021;/Watt&#25552;&#39640;&#20102;2.7&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
In response to innovations in machine learning (ML) models, production workloads changed radically and rapidly. TPU v4 is the fifth Google domain specific architecture (DSA) and its third supercomputer for such ML models. Optical circuit switches (OCSes) dynamically reconfigure its interconnect topology to improve scale, availability, utilization, modularity, deployment, security, power, and performance; users can pick a twisted 3D torus topology if desired. Much cheaper, lower power, and faster than Infiniband, OCSes and underlying optical components are &lt;5% of system cost and &lt;3% of system power. Each TPU v4 includes SparseCores, dataflow processors that accelerate models that rely on embeddings by 5x-7x yet use only 5% of die area and power. Deployed since 2020, TPU v4 outperforms TPU v3 by 2.1x and improves performance/Watt by 2.7x. The TPU v4 supercomputer is 4x larger at 4096 chips and thus ~10x faster overall, which along with OCS flexibility helps large language models. For sim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#27969;&#24418;&#23398;&#20064;&#20013;&#24212;&#29992;&#26080;&#30896;&#25758;&#36816;&#36755;&#22270;&#30340;&#26041;&#27861;&#65292;&#20854;&#21487;&#20197;&#27604;OT&#22270;&#26356;&#20415;&#23452;&#22320;&#35745;&#31639;&#36317;&#31163;&#65292;&#24182;&#25552;&#20379;&#21333;&#20010;&#27010;&#29575;&#27979;&#24230;&#30340;&#24179;&#31227;&#21644;&#20280;&#32553;&#30340;&#31561;&#36317;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.00199</link><description>&lt;p&gt;
&#26080;&#30896;&#25758;&#36816;&#36755;&#22270;&#22312;&#27969;&#34892;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Applications of No-Collision Transportation Maps in Manifold Learning. (arXiv:2304.00199v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00199
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#27969;&#24418;&#23398;&#20064;&#20013;&#24212;&#29992;&#26080;&#30896;&#25758;&#36816;&#36755;&#22270;&#30340;&#26041;&#27861;&#65292;&#20854;&#21487;&#20197;&#27604;OT&#22270;&#26356;&#20415;&#23452;&#22320;&#35745;&#31639;&#36317;&#31163;&#65292;&#24182;&#25552;&#20379;&#21333;&#20010;&#27010;&#29575;&#27979;&#24230;&#30340;&#24179;&#31227;&#21644;&#20280;&#32553;&#30340;&#31561;&#36317;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24341;&#20837;&#20110;[Nurbekyan et al.&#65292;2020]&#30340;&#26080;&#30896;&#25758;&#36816;&#36755;&#22270;&#22312;&#22270;&#20687;&#25968;&#25454;&#30340;&#27969;&#24418;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#12290;&#36817;&#24180;&#26469;&#65292;&#22312;&#34920;&#31034;&#31867;&#20284;&#36816;&#21160;&#25110;&#21464;&#24418;&#29616;&#35937;&#30340;&#25968;&#25454;&#20013;&#65292;&#24212;&#29992;&#22522;&#20110;&#36816;&#36755;&#30340;&#36317;&#31163;&#21644;&#29305;&#24449;&#30340;&#30740;&#31350;&#22823;&#24133;&#22686;&#21152;&#12290;&#20107;&#23454;&#19978;&#65292;&#22266;&#23450;&#20301;&#32622;&#27604;&#36739;&#24378;&#24230;&#36890;&#24120;&#26080;&#27861;&#26174;&#31034;&#25968;&#25454;&#32467;&#26500;&#12290;&#22312;[Nurbekyan et al.&#65292;2020]&#20013;&#24320;&#21457;&#30340;&#26080;&#30896;&#25758;&#22270;&#21644;&#36317;&#31163;&#31867;&#20284;&#20110;&#26368;&#20248;&#20256;&#36755;(OT)&#22270;&#30340;&#20960;&#20309;&#29305;&#24449;&#20294;&#30001;&#20110;&#26080;&#38656;&#20248;&#21270;&#65292;&#35745;&#31639;&#25104;&#26412;&#35201;&#20415;&#23452;&#24471;&#22810;&#12290;&#26412;&#25991;&#35777;&#26126;&#26080;&#30896;&#25758;&#36317;&#31163;&#25552;&#20379;&#21333;&#20010;&#27010;&#29575;&#27979;&#24230;&#30340;&#24179;&#31227;(&#20998;&#21035;&#26159;&#20280;&#32553;)&#21644;&#35013;&#22791;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#30340;&#24179;&#31227;(&#20998;&#21035;&#26159;&#20280;&#32553;)&#21521;&#37327;&#20043;&#38388;&#30340;&#31561;&#36317;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#26080;&#30896;&#25758;&#36816;&#36755;&#22270;&#20197;&#21450;OT&#21644;&#32447;&#24615;OT&#22270;&#65292;&#19968;&#33324;&#26469;&#35828;&#19981;&#33021;&#20026;&#26059;&#36716;&#25552;&#20379;&#31561;&#36317;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we investigate applications of no-collision transportation maps introduced in [Nurbekyan et. al., 2020] in manifold learning for image data. Recently, there has been a surge in applying transportation-based distances and features for data representing motion-like or deformation-like phenomena. Indeed, comparing intensities at fixed locations often does not reveal the data structure. No-collision maps and distances developed in [Nurbekyan et. al., 2020] are sensitive to geometric features similar to optimal transportation (OT) maps but much cheaper to compute due to the absence of optimization. In this work, we prove that no-collision distances provide an isometry between translations (respectively dilations) of a single probability measure and the translation (respectively dilation) vectors equipped with a Euclidean distance. Furthermore, we prove that no-collision transportation maps, as well as OT and linearized OT maps, do not in general provide an isometry for rotatio
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WAE-MDP&#30340;&#28508;&#22312;&#31354;&#38388;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#20219;&#20309;RL&#31574;&#30053;&#20013;&#25552;&#21462;&#24418;&#24335;&#21487;&#39564;&#35777;&#25511;&#21046;&#22120;&#65292;&#24182;&#19988;&#20855;&#26377;&#24179;&#34913;&#25511;&#21046;&#24615;&#33021;&#21644;&#23433;&#20840;&#20043;&#38388;&#30340;&#25928;&#26524;&#21644;&#35299;&#20915;&#19968;&#20123;&#23398;&#20064;&#32570;&#38519;&#30340;&#22810;&#26041;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2303.12558</link><description>&lt;p&gt;
Wasserstein&#33258;&#32534;&#30721;MDPs&#65306;&#20855;&#26377;&#22810;&#26041;&#20445;&#35777;&#30340;&#39640;&#25928;RL&#31574;&#30053;&#27491;&#24335;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Wasserstein Auto-encoded MDPs: Formal Verification of Efficiently Distilled RL Policies with Many-sided Guarantees. (arXiv:2303.12558v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12558
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WAE-MDP&#30340;&#28508;&#22312;&#31354;&#38388;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#20219;&#20309;RL&#31574;&#30053;&#20013;&#25552;&#21462;&#24418;&#24335;&#21487;&#39564;&#35777;&#25511;&#21046;&#22120;&#65292;&#24182;&#19988;&#20855;&#26377;&#24179;&#34913;&#25511;&#21046;&#24615;&#33021;&#21644;&#23433;&#20840;&#20043;&#38388;&#30340;&#25928;&#26524;&#21644;&#35299;&#20915;&#19968;&#20123;&#23398;&#20064;&#32570;&#38519;&#30340;&#22810;&#26041;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#26377;&#35768;&#22810;&#25104;&#21151;&#26696;&#20363;&#65292;&#20294;&#36890;&#36807;&#36825;&#20123;&#20808;&#36827;&#25216;&#26415;&#23398;&#20064;&#30340;&#20915;&#31574;&#32773;&#22312;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#20013;&#30340;&#22823;&#35268;&#27169;&#37096;&#32626;&#21463;&#21040;&#27491;&#24335;&#20445;&#35777;&#19981;&#36275;&#30340;&#38459;&#30861;&#12290;&#21464;&#20998;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;VAE-MDPs&#65289;&#26159;&#31163;&#25955;&#28508;&#22312;&#31354;&#38388;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#20174;&#20219;&#20309;RL&#31574;&#30053;&#20013;&#25552;&#21462;&#24418;&#24335;&#21487;&#39564;&#35777;&#25511;&#21046;&#22120;&#30340;&#21487;&#38752;&#26694;&#26550;&#12290;&#34429;&#28982;&#30456;&#20851;&#20445;&#35777;&#28085;&#30422;&#20102;&#23454;&#38469;&#38382;&#39064;&#30340;&#28385;&#36275;&#24615;&#21644;&#23433;&#20840;&#24615;&#31561;&#26041;&#38754;&#65292;&#20294;VAE&#26041;&#27861;&#22240;&#32570;&#20047;&#25277;&#35937;&#21644;&#34920;&#31034;&#20445;&#35777;&#20197;&#25903;&#25345;&#28508;&#22312;&#26368;&#20248;&#21270;&#32780;&#36973;&#21463;&#22810;&#31181;&#23398;&#20064;&#32570;&#38519;&#65288;&#21518;&#39564;&#23849;&#22604;&#65292;&#23398;&#20064;&#36895;&#24230;&#24930;&#65292;&#21160;&#21147;&#23398;&#20272;&#35745;&#19981;&#33391;&#65289;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Wasserstein&#33258;&#32534;&#30721;MDP&#65288;WAE-MDP&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#28508;&#22312;&#31354;&#38388;&#27169;&#22411;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#25191;&#34892;&#21407;&#22987;&#31574;&#30053;&#30340;&#26234;&#33021;&#20307;&#34892;&#20026;&#21644;&#25552;&#21462;&#20986;&#30340;&#31574;&#30053;&#20043;&#38388;&#30340;&#26368;&#20248;&#36716;&#36816;&#30340;&#24809;&#32602;&#24418;&#24335;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#21033;&#20110;&#25511;&#21046;&#24615;&#33021;&#21644;&#23433;&#20840;&#20043;&#38388;&#30340;&#24179;&#34913;&#65292;&#21516;&#26102;&#20943;&#36731;&#20102;&#19978;&#36848;&#30340;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#20851;&#20110;&#24615;&#33021;&#21644;&#23433;&#20840;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;RL&#22522;&#20934;&#19978;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although deep reinforcement learning (DRL) has many success stories, the large-scale deployment of policies learned through these advanced techniques in safety-critical scenarios is hindered by their lack of formal guarantees. Variational Markov Decision Processes (VAE-MDPs) are discrete latent space models that provide a reliable framework for distilling formally verifiable controllers from any RL policy. While the related guarantees address relevant practical aspects such as the satisfaction of performance and safety properties, the VAE approach suffers from several learning flaws (posterior collapse, slow learning speed, poor dynamics estimates), primarily due to the absence of abstraction and representation guarantees to support latent optimization. We introduce the Wasserstein auto-encoded MDP (WAE-MDP), a latent space model that fixes those issues by minimizing a penalized form of the optimal transport between the behaviors of the agent executing the original policy and the disti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20844;&#20849;&#28216;&#27891;&#27744;&#25805;&#20316;&#25968;&#25454;&#30340;&#21160;&#24577;&#27169;&#22411;&#23398;&#20064;&#22522;&#20934;&#65292;&#26088;&#22312;&#38477;&#20302;&#33021;&#28304;&#24320;&#25903;&#19988;&#20445;&#25345;&#26381;&#21153;&#36136;&#37327;&#27700;&#24179;&#12290;&#32447;&#24615;&#22810;&#21464;&#37327;&#27169;&#22411;&#21644;&#31070;&#32463;&#21160;&#24577;&#27169;&#22411;&#20004;&#31181;&#26041;&#27861;&#24471;&#20986;&#20102;&#21021;&#27493;&#30340;&#35782;&#21035;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.07195</link><description>&lt;p&gt;
&#20197;&#26576;&#27700;&#19978;&#36816;&#21160;&#20013;&#24515;&#30340;&#36816;&#34892;&#25968;&#25454;&#20026;&#22522;&#20934;&#30340;&#21160;&#24577;&#27169;&#22411;&#23398;&#20064;&#65306;&#22312;8&#23567;&#26102;&#33539;&#22260;&#20869;&#25628;&#32034;&#26377;&#25928;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Operating data of a specific Aquatic Center as a Benchmark for dynamic model learning: search for a valid prediction model over an 8-hour horizon. (arXiv:2303.07195v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07195
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20844;&#20849;&#28216;&#27891;&#27744;&#25805;&#20316;&#25968;&#25454;&#30340;&#21160;&#24577;&#27169;&#22411;&#23398;&#20064;&#22522;&#20934;&#65292;&#26088;&#22312;&#38477;&#20302;&#33021;&#28304;&#24320;&#25903;&#19988;&#20445;&#25345;&#26381;&#21153;&#36136;&#37327;&#27700;&#24179;&#12290;&#32447;&#24615;&#22810;&#21464;&#37327;&#27169;&#22411;&#21644;&#31070;&#32463;&#21160;&#24577;&#27169;&#22411;&#20004;&#31181;&#26041;&#27861;&#24471;&#20986;&#20102;&#21021;&#27493;&#30340;&#35782;&#21035;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20844;&#20849;&#28216;&#27891;&#27744;&#25805;&#20316;&#25968;&#25454;&#30340;&#35782;&#21035;&#22522;&#20934;&#12290; &#36825;&#26679;&#30340;&#31995;&#32479;&#26082;&#26159;&#22797;&#26434;&#30340;&#36807;&#31243;&#65292;&#21448;&#26131;&#20110;&#25152;&#26377;&#20154;&#29702;&#35299;&#12290;&#26368;&#32456;&#30446;&#26631;&#26159;&#22312;&#20445;&#25345;&#26381;&#21153;&#36136;&#37327;&#27700;&#24179;&#30340;&#21516;&#26102;&#38477;&#20302;&#33021;&#28304;&#24320;&#25903;&#12290;&#36825;&#19968;&#30446;&#26631;&#20855;&#26377;&#26222;&#36941;&#24615;&#65292;&#19981;&#20165;&#38480;&#20110;&#20844;&#20849;&#28216;&#27891;&#27744;&#12290;&#32463;&#27982;&#39044;&#27979;&#25511;&#21046;&#21487;&#20197;&#26377;&#25928;&#22320;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#36825;&#31181;&#20808;&#36827;&#30340;&#25511;&#21046;&#22522;&#20110;&#19968;&#20010;&#36807;&#31243;&#27169;&#22411;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#23454;&#38469;&#25805;&#20316;&#25968;&#25454;&#33719;&#24471;&#36825;&#26679;&#30340;&#21160;&#24577;&#27169;&#22411;&#12290;&#20026;&#27492;&#65292;&#25972;&#29702;&#21644;&#20849;&#20139;&#25805;&#20316;&#25968;&#25454;&#65292;&#24182;&#25552;&#20986;&#27169;&#22411;&#36136;&#37327;&#25351;&#26631;&#12290;&#22522;&#20110;&#27492;&#65292;&#31532;&#19968;&#27425;&#35782;&#21035;&#32467;&#26524;&#20998;&#21035;&#36890;&#36807;&#32447;&#24615;&#22810;&#21464;&#37327;&#27169;&#22411;&#21644;&#31070;&#32463;&#21160;&#24577;&#27169;&#22411;&#36827;&#34892;&#20998;&#26512;&#12290;&#35813;&#22522;&#20934;&#38656;&#35201;&#20854;&#20182;&#25511;&#21046;&#26041;&#26696;&#21644;&#32467;&#26524;&#30340;&#25552;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article presents an identification benchmark based on data from a public swimming pool in operation. Such a system is both a complex process and easily understandable by all with regard to the stakes. Ultimately, the objective is to reduce the energy bill while maintaining the level of quality of service. This objective is general in scope and is not limited to public swimming pools. This can be done effectively through what is known as economic predictive control. This type of advanced control is based on a process model. It is the aim of this article and the considered benchmark to show that such a dynamic model can be obtained from operating data. For this, operational data is formatted and shared, and model quality indicators are proposed. On this basis, the first identification results illustrate the results obtained by a linear multivariable model on the one hand, and by a neural dynamic model on the other hand. The benchmark calls for other proposals and results from contro
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20998;&#26512;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#24067;&#23572;&#30005;&#36335;&#22797;&#26434;&#24615;&#21644;&#25551;&#36848;&#24615;&#22797;&#26434;&#24615;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#22810;&#39033;&#24335;&#35268;&#27169;&#26377;&#30028;&#28145;&#24230;&#30340;GNN&#26063;&#26063;&#21487;&#20197;&#35745;&#31639;&#30340;&#22270;&#26597;&#35810;&#27491;&#26159;&#24102;&#35745;&#25968;&#21644;&#20869;&#32622;&#20851;&#31995;&#30340;&#19968;&#38454;&#36923;&#36753;&#21463;&#20445;&#25252;&#30340;&#29255;&#26029;GFO+C&#25152;&#23450;&#20041;&#30340;&#65292;&#36825;&#23558;GNN&#25918;&#22312;&#30005;&#36335;&#22797;&#26434;&#24615;&#31867;TC^0&#20013;&#12290;</title><link>http://arxiv.org/abs/2303.04613</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25551;&#36848;&#24615;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Descriptive Complexity of Graph Neural Networks. (arXiv:2303.04613v2 [cs.LO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04613
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20998;&#26512;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#24067;&#23572;&#30005;&#36335;&#22797;&#26434;&#24615;&#21644;&#25551;&#36848;&#24615;&#22797;&#26434;&#24615;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#22810;&#39033;&#24335;&#35268;&#27169;&#26377;&#30028;&#28145;&#24230;&#30340;GNN&#26063;&#26063;&#21487;&#20197;&#35745;&#31639;&#30340;&#22270;&#26597;&#35810;&#27491;&#26159;&#24102;&#35745;&#25968;&#21644;&#20869;&#32622;&#20851;&#31995;&#30340;&#19968;&#38454;&#36923;&#36753;&#21463;&#20445;&#25252;&#30340;&#29255;&#26029;GFO+C&#25152;&#23450;&#20041;&#30340;&#65292;&#36825;&#23558;GNN&#25918;&#22312;&#30005;&#36335;&#22797;&#26434;&#24615;&#31867;TC^0&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20998;&#26512;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#24067;&#23572;&#30005;&#36335;&#22797;&#26434;&#24615;&#21644;&#25551;&#36848;&#24615;&#22797;&#26434;&#24615;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22810;&#39033;&#24335;&#35268;&#27169;&#26377;&#30028;&#28145;&#24230;&#30340;GNN&#26063;&#26063;&#21487;&#20197;&#35745;&#31639;&#30340;&#22270;&#26597;&#35810;&#27491;&#26159;&#37027;&#20123;&#29992;&#24102;&#35745;&#25968;&#21644;&#20869;&#32622;&#20851;&#31995;&#30340;&#19968;&#38454;&#36923;&#36753;&#21463;&#20445;&#25252;&#30340;&#29255;&#26029;GFO+C&#23450;&#20041;&#30340;&#12290;&#36825;&#23558;GNN&#25918;&#22312;&#30005;&#36335;&#22797;&#26434;&#24615;&#31867;TC^0&#20013;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;GNN&#23478;&#26063;&#21487;&#20197;&#20351;&#29992;&#20219;&#24847;&#23454;&#25968;&#26435;&#20540;&#21644;&#21253;&#25324;&#26631;&#20934;ReLU&#12289;Logistic&#8220;sigmod&#8221;&#21644;&#21452;&#26354;&#27491;&#20999;&#20989;&#25968;&#22312;&#20869;&#30340;&#24191;&#27867;&#28608;&#27963;&#20989;&#25968;&#31867;&#12290;&#22914;&#26524;GNN&#34987;&#20801;&#35768;&#20351;&#29992;&#38543;&#26426;&#21021;&#22987;&#21270;&#21644;&#20840;&#23616;&#35835;&#21462;&#65288;&#36825;&#20123;&#37117;&#26159;GNN&#22312;&#23454;&#36341;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#26631;&#20934;&#21151;&#33021;&#65289;&#65292;&#23427;&#20204;&#21487;&#20197;&#35745;&#31639;&#19982;&#38408;&#38376;&#30340;&#26377;&#30028;&#28145;&#24230;&#24067;&#23572;&#30005;&#36335;&#23436;&#20840;&#30456;&#21516;&#30340;&#26597;&#35810;&#65292;&#21363;&#22312;TC^0&#20013;&#30340;&#26597;&#35810;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#24102;&#20998;&#27573;&#32447;&#24615;&#28608;&#27963;&#21644;&#26377;&#29702;&#26435;&#37325;&#30340;&#21333;&#20010;GNN&#21487;&#20197;&#22312;&#19981;&#24314;&#36896;&#20869;&#37096;&#20851;&#31995;&#30340;&#24773;&#20917;&#19979;&#30001;GFO+C&#23450;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
We analyse the power of graph neural networks (GNNs) in terms of Boolean circuit complexity and descriptive complexity.  We prove that the graph queries that can be computed by a polynomial-size bounded-depth family of GNNs are exactly those definable in the guarded fragment GFO+C of first-order logic with counting and with built-in relations. This puts GNNs in the circuit complexity class TC^0. Remarkably, the GNN families may use arbitrary real weights and a wide class of activation functions that includes the standard ReLU, logistic "sigmod", and hyperbolic tangent functions. If the GNNs are allowed to use random initialisation and global readout (both standard features of GNNs widely used in practice), they can compute exactly the same queries as bounded depth Boolean circuits with threshold gates, that is, exactly the queries in TC^0.  Moreover, we show that queries computable by a single GNN with piecewise linear activations and rational weights are definable in GFO+C without bui
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#26426;&#22120;&#29983;&#25104;&#30340;&#34394;&#20551;&#35780;&#35770;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#39640;&#36136;&#37327;&#39184;&#21381;&#35780;&#35770;&#29983;&#25104;&#34394;&#20551;&#35780;&#35770;&#24182;&#24494;&#35843;GPT&#36755;&#20986;&#26816;&#27979;&#22120;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#39044;&#27979;&#34394;&#20551;&#35780;&#35770;&#30340;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#39044;&#27979;&#38750;&#31934;&#33521;&#35780;&#35770;&#30340;&#27169;&#22411;&#65292;&#24182;&#22312;&#20960;&#20010;&#32500;&#24230;&#19978;&#23545;&#36825;&#20123;&#35780;&#35770;&#36827;&#34892;&#20998;&#26512;&#65292;&#27492;&#31867;&#26426;&#22120;&#29983;&#25104;&#30340;&#34394;&#20551;&#35780;&#35770;&#26159;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#38754;&#20020;&#30340;&#25345;&#32493;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2302.07731</link><description>&lt;p&gt;
AI&#23545;&#25239;AI&#65306;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#25171;&#20987;&#26426;&#22120;&#29983;&#25104;&#30340;&#34394;&#20551;&#39184;&#21381;&#35780;&#35770;
&lt;/p&gt;
&lt;p&gt;
Combat AI With AI: Counteract Machine-Generated Fake Restaurant Reviews on Social Media. (arXiv:2302.07731v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#26426;&#22120;&#29983;&#25104;&#30340;&#34394;&#20551;&#35780;&#35770;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#39640;&#36136;&#37327;&#39184;&#21381;&#35780;&#35770;&#29983;&#25104;&#34394;&#20551;&#35780;&#35770;&#24182;&#24494;&#35843;GPT&#36755;&#20986;&#26816;&#27979;&#22120;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#39044;&#27979;&#34394;&#20551;&#35780;&#35770;&#30340;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#39044;&#27979;&#38750;&#31934;&#33521;&#35780;&#35770;&#30340;&#27169;&#22411;&#65292;&#24182;&#22312;&#20960;&#20010;&#32500;&#24230;&#19978;&#23545;&#36825;&#20123;&#35780;&#35770;&#36827;&#34892;&#20998;&#26512;&#65292;&#27492;&#31867;&#26426;&#22120;&#29983;&#25104;&#30340;&#34394;&#20551;&#35780;&#35770;&#26159;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#38754;&#20020;&#30340;&#25345;&#32493;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#29983;&#25104;&#27169;&#22411;&#65288;&#22914;GPT&#65289;&#30340;&#21457;&#23637;&#20351;&#24471;&#20197;&#26356;&#20302;&#30340;&#25104;&#26412;&#21046;&#36896;&#20986;&#38590;&#20197;&#21306;&#20998;&#30340;&#34394;&#20551;&#39038;&#23458;&#35780;&#35770;&#65292;&#20174;&#32780;&#23545;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#26816;&#27979;&#36825;&#20123;&#26426;&#22120;&#29983;&#25104;&#30340;&#34394;&#20551;&#35780;&#35770;&#36896;&#25104;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;Yelp&#39564;&#35777;&#30340;&#39640;&#36136;&#37327;&#30340;&#31934;&#33521;&#39184;&#21381;&#35780;&#35770;&#26469;&#29983;&#25104;OpenAI GPT&#35780;&#35770;&#29983;&#25104;&#22120;&#30340;&#34394;&#20551;&#35780;&#35770;&#65292;&#24182;&#26368;&#32456;&#24494;&#35843;GPT&#36755;&#20986;&#26816;&#27979;&#22120;&#26469;&#39044;&#27979;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#30340;&#34394;&#20551;&#35780;&#35770;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#27169;&#22411;&#24212;&#29992;&#20110;&#39044;&#27979;&#38750;&#31934;&#33521;&#35780;&#35770;&#65292;&#24182;&#22312;&#20960;&#20010;&#32500;&#24230;&#65288;&#22914;&#35780;&#35770;&#12289;&#29992;&#25143;&#21644;&#39184;&#21381;&#29305;&#24449;&#20197;&#21450;&#20889;&#20316;&#39118;&#26684;&#65289;&#19978;&#35782;&#21035;&#27169;&#24335;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#27491;&#22312;&#19981;&#26029;&#38754;&#20020;&#26426;&#22120;&#29983;&#25104;&#30340;&#34394;&#20551;&#35780;&#35770;&#30340;&#25361;&#25112;&#65292;&#23613;&#31649;&#20182;&#20204;&#21487;&#33021;&#23454;&#26045;&#26816;&#27979;&#31995;&#32479;&#20197;&#36807;&#28388;&#20986;&#21487;&#30097;&#30340;&#35780;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in generative models such as GPT may be used to fabricate indistinguishable fake customer reviews at a much lower cost, thus posing challenges for social media platforms to detect these machine-generated fake reviews. We propose to leverage the high-quality elite restaurant reviews verified by Yelp to generate fake reviews from the OpenAI GPT review creator and ultimately fine-tune a GPT output detector to predict fake reviews that significantly outperform existing solutions. We further apply the model to predict non-elite reviews and identify the patterns across several dimensions, such as review, user and restaurant characteristics, and writing style. We show that social media platforms are continuously challenged by machine-generated fake reviews, although they may implement detection systems to filter out suspicious reviews.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23545;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#21518;&#38376;&#35302;&#21457;&#29305;&#24449;&#36827;&#34892;&#20102;&#31995;&#32479;&#24615;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#27745;&#26579;&#29575;&#21644;2x2&#20687;&#32032;&#22823;&#23567;&#30340;&#35302;&#21457;&#22120;&#26159;&#21019;&#24314;&#26377;&#25928;&#21518;&#38376;&#30340;&#26368;&#20851;&#38190;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2302.01740</link><description>&lt;p&gt;
SoK&#65306;&#22270;&#20687;&#20998;&#31867;&#20013;&#21518;&#38376;&#35302;&#21457;&#29305;&#24449;&#30340;&#31995;&#32479;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
SoK: A Systematic Evaluation of Backdoor Trigger Characteristics in Image Classification. (arXiv:2302.01740v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01740
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23545;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#21518;&#38376;&#35302;&#21457;&#29305;&#24449;&#36827;&#34892;&#20102;&#31995;&#32479;&#24615;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#27745;&#26579;&#29575;&#21644;2x2&#20687;&#32032;&#22823;&#23567;&#30340;&#35302;&#21457;&#22120;&#26159;&#21019;&#24314;&#26377;&#25928;&#21518;&#38376;&#30340;&#26368;&#20851;&#38190;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#37117;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#23427;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#36825;&#31181;&#25915;&#20987;&#36890;&#36807;&#20462;&#25913;&#35757;&#32451;&#38598;&#26469;&#23884;&#20837;&#21463;&#25511;&#21151;&#33021;&#20110;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#20013;&#12290;&#25152;&#20462;&#25913;&#35757;&#32451;&#26679;&#26412;&#20855;&#26377;&#31192;&#23494;&#23646;&#24615;&#65292;&#21363;&#19968;&#20010;&#35302;&#21457;&#22120;&#12290;&#22312;&#25512;&#29702;&#26102;&#65292;&#24403;&#36755;&#20837;&#21253;&#21547;&#35302;&#21457;&#22120;&#26102;&#65292;&#31192;&#23494;&#21151;&#33021;&#34987;&#28608;&#27963;&#65292;&#32780;&#22312;&#20854;&#20182;&#24773;&#20917;&#19979;&#27169;&#22411;&#21017;&#33021;&#27491;&#24120;&#36816;&#20316;&#12290;&#34429;&#28982;&#24050;&#30693;&#26377;&#35768;&#22810;&#21518;&#38376;&#25915;&#20987;&#21644;&#38450;&#24481;&#26041;&#27861;&#65292;&#20294;&#25104;&#21151;&#21019;&#24314;&#38544;&#34109;&#30340;&#21518;&#38376;&#25915;&#20987;&#20173;&#28982;&#24456;&#22256;&#38590;&#12290;&#25104;&#21151;&#22320;&#21019;&#24314;&#21518;&#38376;&#35302;&#21457;&#22120;&#21462;&#20915;&#20110;&#20247;&#22810;&#21442;&#25968;&#65292;&#19981;&#24184;&#30340;&#26159;&#65292;&#23578;&#26410;&#30830;&#23450;&#21738;&#20123;&#21442;&#25968;&#23545;&#25915;&#20987;&#24615;&#33021;&#36129;&#29486;&#26368;&#22823;&#12290;&#22240;&#27492;&#26412;&#25991;&#36890;&#36807;&#31995;&#32479;&#20998;&#26512;&#26368;&#30456;&#20851;&#30340;&#21518;&#38376;&#25915;&#20987;&#21442;&#25968;&#65292;&#21363;&#35302;&#21457;&#22120;&#23610;&#23544;&#65292;&#20301;&#32622;&#65292;&#39068;&#33394;&#21644;&#27745;&#26579;&#29575;&#12290;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#26469;&#35780;&#20272;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65288;ResNet&#65292;VGG&#65292;AlexNet&#65289;&#21644;&#25968;&#25454;&#38598;&#65288;MNIST&#65292;CIFAR-10&#65292;ImageNet&#65289;&#20013;&#30340;&#25915;&#20987;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#27745;&#26579;&#29575;&#26159;&#21019;&#24314;&#26377;&#25928;&#21518;&#38376;&#30340;&#26368;&#20851;&#38190;&#21442;&#25968;&#65307;2x2&#20687;&#32032;&#22823;&#23567;&#30340;&#35302;&#21457;&#22120;&#24050;&#36275;&#22815;&#22823;&#22810;&#25968;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning achieves outstanding results in many machine learning tasks. Nevertheless, it is vulnerable to backdoor attacks that modify the training set to embed a secret functionality in the trained model. The modified training samples have a secret property, i. e., a trigger. At inference time, the secret functionality is activated when the input contains the trigger, while the model functions correctly in other cases. While there are many known backdoor attacks (and defenses), deploying a stealthy attack is still far from trivial. Successfully creating backdoor triggers depends on numerous parameters. Unfortunately, research has not yet determined which parameters contribute most to the attack performance.  This paper systematically analyzes the most relevant parameters for the backdoor attacks, i.e., trigger size, position, color, and poisoning rate. Using transfer learning, which is very common in computer vision, we evaluate the attack on state-of-the-art models (ResNet, VGG, A
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#20851;&#27880;&#26435;&#37325;&#30697;&#38453;&#20869;&#26435;&#37325;&#30340;&#31354;&#38388;&#25490;&#21015;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#27169;&#22411;&#21442;&#25968;&#30340;&#38750;&#38646;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2301.07285</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#39062;&#30340;&#31232;&#30095;&#27491;&#21017;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Novel Sparse Regularizer. (arXiv:2301.07285v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07285
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#20851;&#27880;&#26435;&#37325;&#30697;&#38453;&#20869;&#26435;&#37325;&#30340;&#31354;&#38388;&#25490;&#21015;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#27169;&#22411;&#21442;&#25968;&#30340;&#38750;&#38646;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#19981;&#22522;&#20110; $L_{p}$-norm &#30340;&#26032;&#39062;&#27491;&#21017;&#21270;&#26041;&#27861;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#21482;&#32771;&#34385;&#27169;&#22411;&#20013;&#21508;&#26435;&#37325;&#20540;&#30340;&#24230;&#37327;&#19981;&#21516;&#65292;&#36825;&#31181;&#27491;&#21017;&#21270;&#26041;&#27861;&#20851;&#27880;&#26435;&#37325;&#30697;&#38453;&#20869;&#26435;&#37325;&#30340;&#31354;&#38388;&#25490;&#21015;&#12290;&#35813;&#26041;&#27861;&#30340;&#21152;&#20837;&#39033;&#21487;&#20197;&#29992;&#20110;&#25439;&#22833;&#20989;&#25968;&#20013;&#65292;&#21487;&#24494;&#20998;&#65292;&#31616;&#21333;&#24555;&#36895;&#35745;&#31639;&#65292;&#19982;&#23610;&#24230;&#26080;&#20851;&#65292;&#20165;&#38656;&#35201;&#24494;&#23567;&#30340;&#39069;&#22806;&#20869;&#23384;&#65292;&#23481;&#26131;&#24182;&#34892;&#21270;&#12290;&#32463;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#30456;&#21516;&#31934;&#24230;&#27700;&#24179;&#19979;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20351;&#27169;&#22411;&#21442;&#25968;&#30340;&#38750;&#38646;&#25968;&#37327;&#25552;&#39640;&#19968;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
$L_{p}$-norm regularization schemes such as $L_{0}$, $L_{1}$, and $L_{2}$-norm regularization and $L_{p}$-norm-based regularization techniques such as weight decay and group LASSO compute a quantity which de pends on model weights considered in isolation from one another. This paper describes a novel regularizer which is not based on an $L_{p}$-norm. In contrast with $L_{p}$-norm-based regularization, this regularizer is concerned with the spatial arrangement of weights within a weight matrix. This regularizer is an additive term for the loss function and is differentiable, simple and fast to compute, scale-invariant, requires a trivial amount of additional memory, and can easily be parallelized. Empirically this method yields approximately a one order-of-magnitude improvement in the number of nonzero model parameters at a given level of accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#65306;&#21033;&#29992;&#22810;&#20154;&#33258;&#25105;&#35270;&#21548;&#35266;&#23519;&#20013;&#30340;&#20849;&#20139;&#20449;&#24687;&#65292;&#39640;&#25928;&#26500;&#24314;&#20808;&#21069;&#26410;&#35265;&#30340;3D&#29615;&#22659;&#22320;&#22270;&#12290;&#20026;&#20102;&#39640;&#25928;&#32472;&#21046;&#31354;&#38388;&#22270;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#38899;&#35270;&#39057;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#19982;&#20849;&#20139;&#22330;&#26223;&#23545;&#24212;&#30456;&#37197;&#21512;&#65292;&#36873;&#25321;&#24615;&#22320;&#25171;&#24320;&#30456;&#26426;&#20197;&#26368;&#23567;&#21270;&#20887;&#20313;&#24182;&#20943;&#23569;&#21151;&#32791;&#12290;</title><link>http://arxiv.org/abs/2301.02184</link><description>&lt;p&gt;
Chat2Map&#65306;&#20174;&#22810;&#37325;&#33258;&#25105;&#23545;&#35805;&#20013;&#39640;&#25928;&#26500;&#24314;&#22330;&#26223;&#22320;&#22270;
&lt;/p&gt;
&lt;p&gt;
Chat2Map: Efficient Scene Mapping from Multi-Ego Conversations. (arXiv:2301.02184v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.02184
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#65306;&#21033;&#29992;&#22810;&#20154;&#33258;&#25105;&#35270;&#21548;&#35266;&#23519;&#20013;&#30340;&#20849;&#20139;&#20449;&#24687;&#65292;&#39640;&#25928;&#26500;&#24314;&#20808;&#21069;&#26410;&#35265;&#30340;3D&#29615;&#22659;&#22320;&#22270;&#12290;&#20026;&#20102;&#39640;&#25928;&#32472;&#21046;&#31354;&#38388;&#22270;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#38899;&#35270;&#39057;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#19982;&#20849;&#20139;&#22330;&#26223;&#23545;&#24212;&#30456;&#37197;&#21512;&#65292;&#36873;&#25321;&#24615;&#22320;&#25171;&#24320;&#30456;&#26426;&#20197;&#26368;&#23567;&#21270;&#20887;&#20313;&#24182;&#20943;&#23569;&#21151;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#22238;&#31572;&#19968;&#20010;&#38382;&#39064;&#65306;&#20174;&#22810;&#20010;&#33258;&#25105;&#35270;&#35282;&#25429;&#25417;&#30340;&#23545;&#35805;&#35270;&#39057;&#20013;&#33021;&#21542;&#20197;&#25104;&#26412;&#26377;&#25928;&#30340;&#26041;&#24335;&#25581;&#31034;&#22330;&#26223;&#22320;&#22270;&#65311;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#38382;&#39064;&#65306;&#36890;&#36807;&#21033;&#29992;&#33258;&#28982;&#23545;&#35805;&#21442;&#19982;&#32773;&#30340;&#33258;&#25105;&#35270;&#21548;&#35266;&#23519;&#20013;&#30340;&#20849;&#20139;&#20449;&#24687;&#65292;&#39640;&#25928;&#26500;&#24314;&#20808;&#21069;&#26410;&#35265;&#30340;3D&#29615;&#22659;&#22320;&#22270;&#12290;&#25105;&#20204;&#30340;&#20551;&#35774;&#26159;&#65292;&#22810;&#20010;&#20154;&#65288;&#8220;&#33258;&#25105;&#8221;&#65289;&#22312;&#22330;&#26223;&#20013;&#31227;&#21160;&#24182;&#30456;&#20114;&#20132;&#35848;&#26102;&#65292;&#25509;&#25910;&#21040;&#30340;&#20016;&#23500;&#35270;&#21548;&#32447;&#32034;&#21487;&#20197;&#24110;&#21161;&#25581;&#31034;&#22330;&#26223;&#20013;&#26410;&#30693;&#30340;&#22320;&#21306;&#12290;&#37492;&#20110;&#25345;&#32493;&#22788;&#29702;&#33258;&#25105;&#20013;&#24515;&#35270;&#35273;&#27969;&#30340;&#39640;&#25104;&#26412;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#32034;&#22914;&#20309;&#20027;&#21160;&#21327;&#35843;&#35270;&#35273;&#20449;&#24687;&#30340;&#37319;&#26679;&#65292;&#20197;&#26368;&#23567;&#21270;&#20887;&#20313;&#24182;&#20943;&#23569;&#21151;&#32791;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38899;&#35270;&#39057;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#19982;&#25105;&#20204;&#30340;&#20849;&#20139;&#22330;&#26223;&#23545;&#24212;&#30456;&#37197;&#21512;&#65292;&#36873;&#25321;&#24615;&#22320;&#25171;&#24320;&#30456;&#26426;&#20197;&#39640;&#25928;&#22320;&#32472;&#21046;&#31354;&#38388;&#22270;&#12290;&#25105;&#20204;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#38899;&#35270;&#39057;&#33258;&#25105;&#22330;&#26223;&#20849;&#20139;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#35813;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can conversational videos captured from multiple egocentric viewpoints reveal the map of a scene in a cost-efficient way? We seek to answer this question by proposing a new problem: efficiently building the map of a previously unseen 3D environment by exploiting shared information in the egocentric audio-visual observations of participants in a natural conversation. Our hypothesis is that as multiple people ("egos") move in a scene and talk among themselves, they receive rich audio-visual cues that can help uncover the unseen areas of the scene. Given the high cost of continuously processing egocentric visual streams, we further explore how to actively coordinate the sampling of visual information, so as to minimize redundancy and reduce power use. To that end, we present an audio-visual deep reinforcement learning approach that works with our shared scene mapper to selectively turn on the camera to efficiently chart out the space. We evaluate the approach using a state-of-the-art audi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#26041;&#27861;NeRN&#65292;&#21487;&#20197;&#30452;&#25509;&#29992;&#20110;&#34920;&#31034;&#39044;&#35757;&#32451;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20026;&#32593;&#32476;&#20013;&#30340;&#27599;&#20010;&#21367;&#31215;&#26680;&#20998;&#37197;&#19968;&#20010;&#22352;&#26631;&#65292;&#24182;&#23558;&#20854;&#26144;&#23556;&#21040;&#30456;&#24212;&#30340;&#26435;&#37325;&#26469;&#23454;&#29616;&#37325;&#24314;&#12290;&#27492;&#22806;&#65292;&#35813;&#35770;&#25991;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#31283;&#23450;&#27169;&#22411;&#65292;&#24182;&#21152;&#20837;&#24179;&#28369;&#32422;&#26463;&#26377;&#21161;&#20110;&#23454;&#29616;&#26356;&#22909;&#30340;&#37325;&#24314;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2212.13554</link><description>&lt;p&gt;
NeRN&#8212;&#8212;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#30340;&#31070;&#32463;&#34920;&#31034;&#27861;
&lt;/p&gt;
&lt;p&gt;
NeRN -- Learning Neural Representations for Neural Networks. (arXiv:2212.13554v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.13554
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#26041;&#27861;NeRN&#65292;&#21487;&#20197;&#30452;&#25509;&#29992;&#20110;&#34920;&#31034;&#39044;&#35757;&#32451;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20026;&#32593;&#32476;&#20013;&#30340;&#27599;&#20010;&#21367;&#31215;&#26680;&#20998;&#37197;&#19968;&#20010;&#22352;&#26631;&#65292;&#24182;&#23558;&#20854;&#26144;&#23556;&#21040;&#30456;&#24212;&#30340;&#26435;&#37325;&#26469;&#23454;&#29616;&#37325;&#24314;&#12290;&#27492;&#22806;&#65292;&#35813;&#35770;&#25991;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#31283;&#23450;&#27169;&#22411;&#65292;&#24182;&#21152;&#20837;&#24179;&#28369;&#32422;&#26463;&#26377;&#21161;&#20110;&#23454;&#29616;&#26356;&#22909;&#30340;&#37325;&#24314;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#31070;&#32463;&#34920;&#31034;&#27861;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#26377;&#25928;&#22320;&#37325;&#24314;&#24191;&#27867;&#30340;&#20449;&#21495;&#65292;&#20174;3D&#32593;&#26684;&#21644;&#24418;&#29366;&#21040;&#22270;&#20687;&#21644;&#35270;&#39057;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#31070;&#32463;&#34920;&#31034;&#27861;&#36866;&#24403;&#22320;&#34987;&#35843;&#25972;&#26102;&#65292;&#21487;&#20197;&#29992;&#20110;&#30452;&#25509;&#34920;&#31034;&#39044;&#35757;&#32451;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#65292;&#20174;&#32780;&#24471;&#21040;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#30340;&#31070;&#32463;&#34920;&#31034;&#27861;&#65288;NeRN&#65289;&#12290;&#21463;&#20197;&#21069;&#31070;&#32463;&#34920;&#31034;&#27861;&#26041;&#27861;&#30340;&#22352;&#26631;&#36755;&#20837;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20026;&#32593;&#32476;&#20013;&#30340;&#27599;&#20010;&#21367;&#31215;&#26680;&#20998;&#37197;&#19968;&#20010;&#22352;&#26631;&#65292;&#22522;&#20110;&#20854;&#22312;&#20307;&#31995;&#32467;&#26500;&#20013;&#30340;&#20301;&#32622;&#65292;&#24182;&#20248;&#21270;&#19968;&#20010;&#39044;&#27979;&#22120;&#32593;&#32476;&#26469;&#23558;&#22352;&#26631;&#26144;&#23556;&#21040;&#30456;&#24212;&#30340;&#26435;&#37325;&#12290;&#19982;&#35270;&#35273;&#22330;&#26223;&#30340;&#31354;&#38388;&#24179;&#28369;&#24615;&#31867;&#20284;&#65292;&#25105;&#20204;&#26174;&#31034;&#22312;&#21407;&#22987;&#32593;&#32476;&#30340;&#26435;&#37325;&#19978;&#21152;&#20837;&#24179;&#28369;&#32422;&#26463;&#26377;&#21161;&#20110;NeRN&#23454;&#29616;&#26356;&#22909;&#30340;&#37325;&#24314;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#26435;&#37325;&#30340;&#36731;&#24494;&#25200;&#21160;&#21487;&#33021;&#23548;&#33268;&#30456;&#24403;&#22823;&#30340;&#31934;&#24230;&#25439;&#22833;&#65292;&#25105;&#20204;&#37319;&#29992;&#30693;&#35782;&#33976;&#39311;&#39046;&#22495;&#30340;&#25216;&#26415;&#26469;&#31283;&#23450;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Representations have recently been shown to effectively reconstruct a wide range of signals from 3D meshes and shapes to images and videos. We show that, when adapted correctly, neural representations can be used to directly represent the weights of a pre-trained convolutional neural network, resulting in a Neural Representation for Neural Networks (NeRN). Inspired by coordinate inputs of previous neural representation methods, we assign a coordinate to each convolutional kernel in our network based on its position in the architecture, and optimize a predictor network to map coordinates to their corresponding weights. Similarly to the spatial smoothness of visual scenes, we show that incorporating a smoothness constraint over the original network's weights aids NeRN towards a better reconstruction. In addition, since slight perturbations in pre-trained model weights can result in a considerable accuracy loss, we employ techniques from the field of knowledge distillation to stabi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28216;&#25103;&#30340;&#26694;&#26550;&#65292;&#31995;&#32479;&#21270;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#38544;&#31169;&#25512;&#26029;&#39118;&#38505;&#30693;&#35782;&#20307;&#31995;&#12290;&#24182;&#20026;&#25512;&#26029;&#39118;&#38505;&#30340;&#23450;&#20041;&#25552;&#20379;&#20102;&#32479;&#19968;&#32467;&#26500;&#65292;&#27491;&#24335;&#24314;&#31435;&#20102;&#24050;&#30693;&#23450;&#20041;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#20102;&#27492;&#21069;&#38590;&#20197;&#21457;&#29616;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2212.10986</link><description>&lt;p&gt;
SoK&#65306;&#35753;&#38544;&#31169;&#20445;&#25252;&#28216;&#25103;&#24320;&#22987;&#65281;&#26426;&#22120;&#23398;&#20064;&#20013;&#25968;&#25454;&#25512;&#26029;&#38544;&#31169;&#30340;&#32479;&#19968;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
SoK: Let the Privacy Games Begin! A Unified Treatment of Data Inference Privacy in Machine Learning. (arXiv:2212.10986v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10986
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28216;&#25103;&#30340;&#26694;&#26550;&#65292;&#31995;&#32479;&#21270;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#38544;&#31169;&#25512;&#26029;&#39118;&#38505;&#30693;&#35782;&#20307;&#31995;&#12290;&#24182;&#20026;&#25512;&#26029;&#39118;&#38505;&#30340;&#23450;&#20041;&#25552;&#20379;&#20102;&#32479;&#19968;&#32467;&#26500;&#65292;&#27491;&#24335;&#24314;&#31435;&#20102;&#24050;&#30693;&#23450;&#20041;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#20102;&#27492;&#21069;&#38590;&#20197;&#21457;&#29616;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#20135;&#29615;&#22659;&#20013;&#37096;&#32626;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#33021;&#20250;&#20351;&#23545;&#25163;&#25512;&#26029;&#20986;&#26377;&#20851;&#35757;&#32451;&#25968;&#25454;&#30340;&#25935;&#24863;&#20449;&#24687;&#12290;&#24050;&#26377;&#22823;&#37327;&#25991;&#29486;&#20998;&#26512;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#25512;&#26029;&#39118;&#38505;&#65292;&#20174;&#25104;&#21592;&#25512;&#26029;&#21040;&#37325;&#26500;&#25915;&#20987;&#19981;&#31561;&#12290;&#19968;&#20123;&#20316;&#32773;&#21463;&#21040;&#23494;&#30721;&#23398;&#20013;&#20351;&#29992;&#28216;&#25103;&#65288;&#21363;&#27010;&#29575;&#23454;&#39564;&#65289;&#30740;&#31350;&#23433;&#20840;&#24615;&#36136;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#29992;&#31867;&#20284;&#30340;&#22522;&#20110;&#28216;&#25103;&#30340;&#39118;&#26684;&#25551;&#36848;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#38544;&#31169;&#25512;&#26029;&#39118;&#38505;&#12290;&#28982;&#32780;&#65292;&#23545;&#25163;&#30340;&#33021;&#21147;&#21644;&#30446;&#26631;&#22312;&#19981;&#21516;&#30340;&#21576;&#29616;&#26041;&#24335;&#20013;&#32463;&#24120;&#26377;&#24494;&#22937;&#30340;&#19981;&#21516;&#65292;&#36825;&#20351;&#24471;&#38590;&#20197;&#20851;&#32852;&#21644;&#32452;&#21512;&#32467;&#26524;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28216;&#25103;&#30340;&#26694;&#26550;&#65292;&#31995;&#32479;&#21270;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#38544;&#31169;&#25512;&#26029;&#39118;&#38505;&#30693;&#35782;&#20307;&#31995;&#12290;&#25105;&#20204;&#20351;&#29992;&#27492;&#26694;&#26550;&#26469;&#65288;1&#65289;&#20026;&#25512;&#26029;&#39118;&#38505;&#30340;&#23450;&#20041;&#25552;&#20379;&#32479;&#19968;&#32467;&#26500;&#65292;&#65288;2&#65289;&#27491;&#24335;&#24314;&#31435;&#24050;&#30693;&#23450;&#20041;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#65288;3&#65289;&#21457;&#29616;&#27492;&#21069;&#38590;&#20197;&#21457;&#29616;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deploying machine learning models in production may allow adversaries to infer sensitive information about training data. There is a vast literature analyzing different types of inference risks, ranging from membership inference to reconstruction attacks. Inspired by the success of games (i.e., probabilistic experiments) to study security properties in cryptography, some authors describe privacy inference risks in machine learning using a similar game-based style. However, adversary capabilities and goals are often stated in subtly different ways from one presentation to the other, which makes it hard to relate and compose results. In this paper, we present a game-based framework to systematize the body of knowledge on privacy inference risks in machine learning. We use this framework to (1) provide a unifying structure for definitions of inference risks, (2) formally establish known relations among definitions, and (3) to uncover hitherto unknown relations that would have been difficu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20219;&#21153;&#30456;&#20284;&#24230;&#20803;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#21152;&#36895;&#26641;&#24418;&#32467;&#26500;Parzen&#20272;&#35745;&#20013;&#30340;&#22810;&#30446;&#26631;&#38750;&#20998;&#23618;&#36229;&#21442;&#25968;&#26368;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.06751</link><description>&lt;p&gt;
&#22522;&#20110;&#20219;&#21153;&#30456;&#20284;&#24230;&#20803;&#23398;&#20064;&#21152;&#36895;&#22810;&#30446;&#26631;&#38750;&#20998;&#23618;&#36229;&#21442;&#25968;&#26368;&#20248;&#21270;&#30340;&#26641;&#24418;&#32467;&#26500;Parzen&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Speeding up Multi-objective Non-hierarchical Hyperparameter Optimization by Task Similarity-Based Meta-Learning for the Tree-structured Parzen Estimator. (arXiv:2212.06751v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20219;&#21153;&#30456;&#20284;&#24230;&#20803;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#21152;&#36895;&#26641;&#24418;&#32467;&#26500;Parzen&#20272;&#35745;&#20013;&#30340;&#22810;&#30446;&#26631;&#38750;&#20998;&#23618;&#36229;&#21442;&#25968;&#26368;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#21442;&#25968;&#20248;&#21270;&#26159;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#24615;&#33021;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#23454;&#36341;&#32773;&#36890;&#24120;&#38754;&#20020;&#22810;&#20010;&#26041;&#38754;&#30340;&#26435;&#34913;&#65292;&#22914;&#20934;&#30830;&#24615;&#21644;&#24310;&#36831;&#26102;&#38388;&#12290;&#22312;&#28145;&#24230;&#23398;&#20064;&#30340;&#39640;&#35745;&#31639;&#38656;&#27714;&#21644;&#23545;&#39640;&#25928;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#19981;&#26029;&#22686;&#38271;&#38656;&#27714;&#19979;&#65292;&#21152;&#36895;&#22810;&#30446;&#26631;&#20248;&#21270;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26412;&#25991;&#23558;TPE&#30340;&#25910;&#36141;&#20989;&#25968;&#25193;&#23637;&#21040;&#20803;&#23398;&#20064;&#35774;&#32622;&#20013;&#65292;&#20351;&#29992;&#30001;&#20219;&#21153;&#20043;&#38388;&#39030;&#32423;&#22495;&#20043;&#38388;&#30340;&#37325;&#21472;&#24230;&#23450;&#20041;&#30340;&#20219;&#21153;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#20063;&#20174;&#29702;&#35770;&#19978;&#20998;&#26512;&#24182;&#35299;&#20915;&#20102;&#20219;&#21153;&#30456;&#20284;&#24615;&#30340;&#23616;&#38480;&#24615;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#34920;&#26684;HPO&#22522;&#20934;&#19978;&#21152;&#36895;&#20102;MO-TPE&#65292;&#24182;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#36890;&#36807;&#36194;&#24471;AutoML 2022&#26469;&#24471;&#21040;&#22806;&#37096;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperparameter optimization (HPO) is a vital step in improving performance in deep learning (DL). Practitioners are often faced with the trade-off between multiple criteria, such as accuracy and latency. Given the high computational needs of DL and the growing demand for efficient HPO, the acceleration of multi-objective (MO) optimization becomes ever more important. Despite the significant body of work on meta-learning for HPO, existing methods are inapplicable to MO tree-structured Parzen estimator (MO-TPE), a simple yet powerful MO-HPO algorithm. In this paper, we extend TPE's acquisition function to the meta-learning setting using a task similarity defined by the overlap of top domains between tasks. We also theoretically analyze and address the limitations of our task similarity. In the experiments, we demonstrate that our method speeds up MO-TPE on tabular HPO benchmarks and attains state-of-the-art performance. Our method was also validated externally by winning the AutoML 2022 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#38024;&#23545;&#25193;&#25955;&#27169;&#22411;&#30340;&#21518;&#38376;&#25915;&#20987;&#26694;&#26550; BadDiffusion&#65292;&#20854;&#21487;&#20197;&#22312;&#27169;&#22411;&#35757;&#32451;&#26399;&#38388;&#23454;&#29616;&#26893;&#20837;&#21518;&#38376;&#65292;&#23548;&#33268;&#27169;&#22411;&#22312;&#27491;&#24120;&#25968;&#25454;&#36755;&#20837;&#26102;&#20381;&#28982;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#25509;&#25910;&#21040;&#35302;&#21457;&#20449;&#21495;&#26102;&#20135;&#29983;&#35823;&#23548;&#24615;&#36755;&#20986;&#12290;&#35813;&#25915;&#20987;&#21487;&#33021;&#23545;&#24314;&#31435;&#22312;&#26377;&#38382;&#39064;&#30340;&#27169;&#22411;&#20043;&#19978;&#30340;&#19979;&#28216;&#20219;&#21153;&#21644;&#24212;&#29992;&#36896;&#25104;&#20005;&#37325;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2212.05400</link><description>&lt;p&gt;
&#22914;&#20309;&#21518;&#38376;&#25193;&#25955;&#27169;&#22411;&#65311;
&lt;/p&gt;
&lt;p&gt;
How to Backdoor Diffusion Models?. (arXiv:2212.05400v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.05400
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#38024;&#23545;&#25193;&#25955;&#27169;&#22411;&#30340;&#21518;&#38376;&#25915;&#20987;&#26694;&#26550; BadDiffusion&#65292;&#20854;&#21487;&#20197;&#22312;&#27169;&#22411;&#35757;&#32451;&#26399;&#38388;&#23454;&#29616;&#26893;&#20837;&#21518;&#38376;&#65292;&#23548;&#33268;&#27169;&#22411;&#22312;&#27491;&#24120;&#25968;&#25454;&#36755;&#20837;&#26102;&#20381;&#28982;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#25509;&#25910;&#21040;&#35302;&#21457;&#20449;&#21495;&#26102;&#20135;&#29983;&#35823;&#23548;&#24615;&#36755;&#20986;&#12290;&#35813;&#25915;&#20987;&#21487;&#33021;&#23545;&#24314;&#31435;&#22312;&#26377;&#38382;&#39064;&#30340;&#27169;&#22411;&#20043;&#19978;&#30340;&#19979;&#28216;&#20219;&#21153;&#21644;&#24212;&#29992;&#36896;&#25104;&#20005;&#37325;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20854;&#35757;&#32451;&#21407;&#29702;&#26159;&#36890;&#36807;&#36880;&#27493;&#28155;&#21152;&#22122;&#22768;&#21644;&#21435;&#22122;&#23398;&#20064;&#27491;&#21521;&#21644;&#21453;&#21521;&#30340;&#25193;&#25955;&#36807;&#31243;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#20102;&#35299;&#20854;&#38480;&#21046;&#21644;&#28508;&#22312;&#39118;&#38505;&#65292;&#26412;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;&#25193;&#25955;&#27169;&#22411;&#23545;&#21518;&#38376;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BadDiffusion&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#25915;&#20987;&#26694;&#26550;&#65292;&#23427;&#22312;&#27169;&#22411;&#35757;&#32451;&#26399;&#38388;&#24037;&#31243;&#21270;&#20102;&#21463;&#25439;&#30340;&#25193;&#25955;&#36807;&#31243;&#65292;&#36827;&#34892;&#21518;&#38376;&#26893;&#20837;&#12290;&#22312;&#25512;&#29702;&#38454;&#27573;&#65292;&#21518;&#38376;&#25193;&#25955;&#27169;&#22411;&#23558;&#20687;&#26222;&#36890;&#25968;&#25454;&#36755;&#20837;&#30340;&#26410;&#31713;&#25913;&#29983;&#25104;&#22120;&#19968;&#26679;&#36816;&#34892;&#65292;&#21516;&#26102;&#22312;&#25509;&#25910;&#21040;&#26893;&#20837;&#30340;&#35302;&#21457;&#20449;&#21495;&#21518;&#65292;&#20266;&#36896;&#20986;&#19968;&#20123;&#34987;&#22351;&#28436;&#21592;&#35774;&#35745;&#30340;&#30446;&#26631;&#32467;&#26524;&#12290;&#36825;&#31181;&#37325;&#22823;&#39118;&#38505;&#21487;&#33021;&#23545;&#24314;&#31435;&#22312;&#26377;&#38382;&#39064;&#30340;&#27169;&#22411;&#20043;&#19978;&#30340;&#19979;&#28216;&#20219;&#21153;&#21644;&#24212;&#29992;&#36896;&#25104;&#20005;&#37325;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#21518;&#38376;&#25915;&#20987;&#35774;&#32622;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126; BadDiffusion &#21487;&#20197;&#31283;&#23450;&#22320;&#20266;&#36896;&#29305;&#23450;&#30340;&#30446;&#26631;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models are state-of-the-art deep learning empowered generative models that are trained based on the principle of learning forward and reverse diffusion processes via progressive noise-addition and denoising. To gain a better understanding of the limitations and potential risks, this paper presents the first study on the robustness of diffusion models against backdoor attacks. Specifically, we propose BadDiffusion, a novel attack framework that engineers compromised diffusion processes during model training for backdoor implantation. At the inference stage, the backdoored diffusion model will behave just like an untampered generator for regular data inputs, while falsely generating some targeted outcome designed by the bad actor upon receiving the implanted trigger signal. Such a critical risk can be dreadful for downstream tasks and applications built upon the problematic model. Our extensive experiments on various backdoor attack settings show that BadDiffusion can consisten
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#32422;&#26463;TPE&#65288;c-TPE&#65289;&#26041;&#27861;&#65292;&#26159;&#26641;&#24418;Parzen&#20272;&#35745;&#22120;&#65288;TPE&#65289;&#30340;&#25193;&#23637;&#65292;&#21487;&#26377;&#25928;&#22788;&#29702;&#22312;&#24615;&#33021;&#35201;&#27714;&#20043;&#19978;&#26045;&#21152;&#30340;&#32422;&#26463;&#38480;&#21046;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;81&#20010;&#26114;&#36149;&#30340;HPO&#35774;&#32622;&#20013;&#34920;&#29616;&#20986;&#26368;&#20339;&#24615;&#33021;&#25490;&#21517;&#12290;</title><link>http://arxiv.org/abs/2211.14411</link><description>&lt;p&gt;
c-TPE:&#22522;&#20110;&#26641;&#24418;&#32467;&#26500;&#30340;&#24102;&#19981;&#31561;&#24335;&#32422;&#26463;&#30340;&#24085;&#25463;&#26031;&#29305;&#20272;&#35745;&#22120;&#29992;&#20110;&#26114;&#36149;&#36229;&#21442;&#25968;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
c-TPE: Tree-structured Parzen Estimator with Inequality Constraints for Expensive Hyperparameter Optimization. (arXiv:2211.14411v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14411
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#32422;&#26463;TPE&#65288;c-TPE&#65289;&#26041;&#27861;&#65292;&#26159;&#26641;&#24418;Parzen&#20272;&#35745;&#22120;&#65288;TPE&#65289;&#30340;&#25193;&#23637;&#65292;&#21487;&#26377;&#25928;&#22788;&#29702;&#22312;&#24615;&#33021;&#35201;&#27714;&#20043;&#19978;&#26045;&#21152;&#30340;&#32422;&#26463;&#38480;&#21046;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;81&#20010;&#26114;&#36149;&#30340;HPO&#35774;&#32622;&#20013;&#34920;&#29616;&#20986;&#26368;&#20339;&#24615;&#33021;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#21442;&#25968;&#20248;&#21270;&#65288;HPO&#65289;&#23545;&#20110;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#24378;&#22823;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#23454;&#38469;&#24212;&#29992;&#36890;&#24120;&#20250;&#22312;&#24615;&#33021;&#35201;&#27714;&#20043;&#19978;&#26045;&#21152;&#19968;&#20123;&#38480;&#21046;&#65292;&#20363;&#22914;&#20869;&#23384;&#20351;&#29992;&#25110;&#24310;&#36831;&#31561;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32422;&#26463;TPE&#65288;c-TPE&#65289;&#65292;&#36825;&#26159;&#24191;&#27867;&#20351;&#29992;&#30340;&#22810;&#21151;&#33021;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#8212;&#8212;&#26641;&#24418;Parzen&#20272;&#35745;&#22120;&#65288;TPE&#65289;&#30340;&#25193;&#23637;&#65292;&#20197;&#22788;&#29702;&#36825;&#20123;&#32422;&#26463;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#25193;&#23637;&#19981;&#20165;&#26159;&#31616;&#21333;&#22320;&#23558;&#29616;&#26377;&#25910;&#30410;&#20989;&#25968;&#21644;&#21407;&#22987;TPE&#32452;&#21512;&#36215;&#26469;&#65292;&#32780;&#26159;&#21253;&#25324;&#20462;&#25913;&#26469;&#35299;&#20915;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#20174;&#32463;&#39564;&#21644;&#29702;&#35770;&#19978;&#28145;&#20837;&#20998;&#26512;&#36825;&#20123;&#20462;&#25913;&#65292;&#25552;&#20379;&#20102;&#26377;&#20851;&#23427;&#20204;&#22914;&#20309;&#26377;&#25928;&#22320;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#30340;&#35265;&#35299;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;c-TPE&#22312;81&#20010;&#26114;&#36149;&#30340;HPO&#35774;&#32622;&#20013;&#34920;&#29616;&#20986;&#26368;&#20339;&#30340;&#24179;&#22343;&#25490;&#21517;&#24615;&#33021;&#65292;&#20855;&#26377;&#32479;&#35745;&#26174;&#30528;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperparameter optimization (HPO) is crucial for strong performance of deep learning algorithms and real-world applications often impose some constraints, such as memory usage, or latency on top of the performance requirement. In this work, we propose constrained TPE (c-TPE), an extension of the widely-used versatile Bayesian optimization method, tree-structured Parzen estimator (TPE), to handle these constraints. Our proposed extension goes beyond a simple combination of an existing acquisition function and the original TPE, and instead includes modifications that address issues that cause poor performance. We thoroughly analyze these modifications both empirically and theoretically, providing insights into how they effectively overcome these challenges. In the experiments, we demonstrate that c-TPE exhibits the best average rank performance among existing methods with statistical significance on 81 expensive HPO settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#29983;&#25104;&#24314;&#27169;&#22312;&#39640;&#33021;&#29289;&#29702;&#27169;&#25311;&#35745;&#31639;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;FPD&#21644;KPD&#65292;&#24182;&#22312;&#31616;&#21333;&#39640;&#26031;&#20998;&#24067;&#21644;&#27169;&#25311;&#39640;&#33021;&#21943;&#27969;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#35813;&#30740;&#31350;&#21457;&#29616;FPD&#26159;&#23545;&#25152;&#26377;&#26367;&#20195;&#21943;&#27969;&#20998;&#24067;&#26368;&#25935;&#24863;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2211.10295</link><description>&lt;p&gt;
&#22312;&#39640;&#33021;&#29289;&#29702;&#20013;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Evaluating generative models in high energy physics. (arXiv:2211.10295v2 [hep-ex] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#29983;&#25104;&#24314;&#27169;&#22312;&#39640;&#33021;&#29289;&#29702;&#27169;&#25311;&#35745;&#31639;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;FPD&#21644;KPD&#65292;&#24182;&#22312;&#31616;&#21333;&#39640;&#26031;&#20998;&#24067;&#21644;&#27169;&#25311;&#39640;&#33021;&#21943;&#27969;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#35813;&#30740;&#31350;&#21457;&#29616;FPD&#26159;&#23545;&#25152;&#26377;&#26367;&#20195;&#21943;&#27969;&#20998;&#24067;&#26368;&#25935;&#24863;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#29983;&#25104;&#24314;&#27169;&#22312;&#39640;&#33021;&#29289;&#29702;&#65288;HEP&#65289;&#27169;&#25311;&#35745;&#31639;&#26041;&#38754;&#30340;&#24212;&#29992;&#26377;&#20102;&#31361;&#30772;&#24615;&#36827;&#23637;&#12290;&#20026;&#20102;&#22312;&#23454;&#36341;&#20013;&#20351;&#29992;&#36825;&#26679;&#30340;&#26367;&#20195;&#27169;&#25311;&#22120;&#65292;&#25105;&#20204;&#38656;&#35201;&#26126;&#30830;&#23450;&#20041;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#27604;&#36739;&#19981;&#21516;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#35780;&#20272;&#23427;&#20204;&#19982;&#30495;&#23454;&#20998;&#24067;&#30340;&#24046;&#24322;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#31995;&#32479;&#30340;&#35780;&#20272;&#24230;&#37327;&#26631;&#20934;&#30740;&#31350;&#24182;&#25506;&#35752;&#20102;&#23427;&#20204;&#23545;&#22833;&#25928;&#27169;&#24335;&#30340;&#25935;&#24863;&#24615;&#20197;&#21450;&#23427;&#20204;&#23545;HEP&#30340;&#30456;&#20851;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;&#21463;&#29289;&#29702;&#23398;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#20197;&#21069;&#30340;&#24037;&#20316;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#21363;Fr\'echet&#21644;&#26680;&#29289;&#29702;&#36317;&#31163;&#65288;FPD&#21644;KPD&#65289;&#65292;&#24182;&#36827;&#34892;&#20102;&#21508;&#31181;&#23454;&#39564;&#65292;&#27979;&#37327;&#23427;&#20204;&#22312;&#31616;&#21333;&#39640;&#26031;&#20998;&#24067;&#21644;&#27169;&#25311;&#39640;&#33021;&#21943;&#27969;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#29305;&#21035;&#26159;FPD&#26159;&#23545;&#25152;&#26377;&#26367;&#20195;&#21943;&#27969;&#20998;&#24067;&#26368;&#25935;&#24863;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been a recent explosion in research into machine-learning-based generative modeling to tackle computational challenges for simulations in high energy physics (HEP). In order to use such alternative simulators in practice, we need well-defined metrics to compare different generative models and evaluate their discrepancy from the true distributions. We present the first systematic review and investigation into evaluation metrics and their sensitivity to failure modes of generative models, using the framework of two-sample goodness-of-fit testing, and their relevance and viability for HEP. Inspired by previous work in both physics and computer vision, we propose two new metrics, the Fr\'echet and kernel physics distances (FPD and KPD, respectively), and perform a variety of experiments measuring their performance on simple Gaussian-distributed, and simulated high energy jet datasets. We find FPD, in particular, to be the most sensitive metric to all alternative jet distributions
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#25903;&#25345;3D&#29702;&#35299;&#20219;&#21153;&#30340;&#25193;&#25955;&#27169;&#22411;RenderDiffusion&#65292;&#21482;&#38656;&#20351;&#29992;&#21333;&#30524;2D&#30417;&#30563;&#36827;&#34892;&#35757;&#32451;&#12290;&#23427;&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#20687;&#21435;&#22122;&#26550;&#26500;&#65292;&#29983;&#25104;&#21644;&#28210;&#26579;&#20013;&#38388;&#30340;&#19977;&#32500;&#34920;&#31034;&#65292;&#22312;&#25193;&#25955;&#36807;&#31243;&#20013;&#25552;&#20379;&#24378;&#26377;&#21147;&#30340;3D&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.09869</link><description>&lt;p&gt;
RenderDiffusion: &#29992;&#20110;3D&#37325;&#24314;&#12289;&#20462;&#22797;&#21644;&#29983;&#25104;&#30340;&#22270;&#20687;&#25193;&#25955;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
RenderDiffusion: Image Diffusion for 3D Reconstruction, Inpainting and Generation. (arXiv:2211.09869v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#25903;&#25345;3D&#29702;&#35299;&#20219;&#21153;&#30340;&#25193;&#25955;&#27169;&#22411;RenderDiffusion&#65292;&#21482;&#38656;&#20351;&#29992;&#21333;&#30524;2D&#30417;&#30563;&#36827;&#34892;&#35757;&#32451;&#12290;&#23427;&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#20687;&#21435;&#22122;&#26550;&#26500;&#65292;&#29983;&#25104;&#21644;&#28210;&#26579;&#20013;&#38388;&#30340;&#19977;&#32500;&#34920;&#31034;&#65292;&#22312;&#25193;&#25955;&#36807;&#31243;&#20013;&#25552;&#20379;&#24378;&#26377;&#21147;&#30340;3D&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#26377;&#26465;&#20214;&#21644;&#26080;&#26465;&#20214;&#30340;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#22343;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;&#20294;&#26159;&#65292;&#30446;&#21069;&#30340;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#19981;&#25903;&#25345;&#29992;&#20110;3D&#29702;&#35299;&#25152;&#38656;&#30340;&#20219;&#21153;&#65292;&#20363;&#22914;&#35270;&#35282;&#19968;&#33268;&#30340;3D&#29983;&#25104;&#25110;&#21333;&#35270;&#35282;&#29289;&#20307;&#37325;&#24314;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;RenderDiffusion&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;3D&#29983;&#25104;&#21644;&#25512;&#26029;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#21482;&#38656;&#20351;&#29992;&#21333;&#30524;2D&#30417;&#30563;&#36827;&#34892;&#35757;&#32451;&#12290;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#20687;&#21435;&#22122;&#26550;&#26500;&#65292;&#22312;&#27599;&#20010;&#21435;&#22122;&#27493;&#39588;&#20013;&#29983;&#25104;&#21644;&#28210;&#26579;&#22330;&#26223;&#30340;&#20013;&#38388;&#19977;&#32500;&#34920;&#31034;&#12290;&#36825;&#22312;&#25193;&#25955;&#36807;&#31243;&#20013;&#24378;&#21046;&#23454;&#29616;&#20102;&#19968;&#20010;&#24378;&#30340;&#24402;&#32435;&#32467;&#26500;&#65292;&#25552;&#20379;&#20102;3D&#19968;&#33268;&#30340;&#34920;&#31034;&#65292;&#21516;&#26102;&#21482;&#38656;&#35201;2D&#30417;&#30563;&#12290;&#29983;&#25104;&#30340;3D&#34920;&#31034;&#21487;&#20197;&#20174;&#20219;&#20309;&#35270;&#35282;&#28210;&#26579;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;RenderDiffusion&#22312;FFHQ&#12289;AFHQ&#12289;ShapeNet&#21644;CLEVR&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#65292;&#26174;&#31034;&#20986;&#20102;&#22312;&#29983;&#25104;3D&#22330;&#26223;&#21644;&#20174;2D&#22270;&#20687;&#25512;&#26029;3D&#22330;&#26223;&#26041;&#38754;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models currently achieve state-of-the-art performance for both conditional and unconditional image generation. However, so far, image diffusion models do not support tasks required for 3D understanding, such as view-consistent 3D generation or single-view object reconstruction. In this paper, we present RenderDiffusion, the first diffusion model for 3D generation and inference, trained using only monocular 2D supervision. Central to our method is a novel image denoising architecture that generates and renders an intermediate three-dimensional representation of a scene in each denoising step. This enforces a strong inductive structure within the diffusion process, providing a 3D consistent representation while only requiring 2D supervision. The resulting 3D representation can be rendered from any view. We evaluate RenderDiffusion on FFHQ, AFHQ, ShapeNet and CLEVR datasets, showing competitive performance for generation of 3D scenes and inference of 3D scenes from 2D images. Ad
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;SSET&#30340;&#32463;&#39564;&#22238;&#25918;&#37319;&#26679;&#26041;&#27861;&#65292;&#23558;&#32531;&#20914;&#21306;&#20998;&#20026;&#20107;&#20214;&#34920;&#65292;&#24182;&#37319;&#29992;&#29616;&#26377;&#30340;&#20248;&#20808;&#37319;&#26679;&#31574;&#30053;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#23398;&#20064;&#36895;&#24230;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.00576</link><description>&lt;p&gt;
&#26377;&#25928;&#32463;&#39564;&#22238;&#25918;&#30340;&#20107;&#20214;&#34920;
&lt;/p&gt;
&lt;p&gt;
Event Tables for Efficient Experience Replay. (arXiv:2211.00576v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00576
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;SSET&#30340;&#32463;&#39564;&#22238;&#25918;&#37319;&#26679;&#26041;&#27861;&#65292;&#23558;&#32531;&#20914;&#21306;&#20998;&#20026;&#20107;&#20214;&#34920;&#65292;&#24182;&#37319;&#29992;&#29616;&#26377;&#30340;&#20248;&#20808;&#37319;&#26679;&#31574;&#30053;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#23398;&#20064;&#36895;&#24230;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#39564;&#22238;&#25918;(ER)&#26159;&#35768;&#22810;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(RL)&#31995;&#32479;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#20174;ER&#32531;&#20914;&#21306;&#36827;&#34892;&#32479;&#19968;&#37319;&#26679;&#21487;&#33021;&#23548;&#33268;&#32531;&#24930;&#30340;&#25910;&#25947;&#21644;&#19981;&#31283;&#23450;&#30340;&#28176;&#36817;&#34892;&#20026;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20107;&#20214;&#34920;&#30340;&#20998;&#23618;&#37319;&#26679;&#65288;SSET&#65289;&#65292;&#23558;ER&#32531;&#20914;&#21306;&#20998;&#20026;&#20107;&#20214;&#34920;&#65292;&#27599;&#20010;&#20107;&#20214;&#34920;&#25429;&#33719;&#20102;&#26368;&#20248;&#34892;&#20026;&#30340;&#37325;&#35201;&#23376;&#24207;&#21015;&#12290;&#25105;&#20204;&#35777;&#26126;&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;&#32479;&#19968;&#32531;&#20914;&#21306;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#29702;&#35770;&#20248;&#21183;&#65292;&#24182;&#23558;SSET&#19982;&#29616;&#26377;&#30340;&#20248;&#20808;&#37319;&#26679;&#31574;&#30053;&#32467;&#21512;&#36215;&#26469;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#23398;&#20064;&#36895;&#24230;&#21644;&#31283;&#23450;&#24615;&#12290;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;MiniGrid&#39046;&#22495;&#65292;&#22522;&#20934;RL&#29615;&#22659;&#20197;&#21450;&#39640;&#20445;&#30495;&#24230;&#30340;&#27773;&#36710;&#36187;&#36710;&#27169;&#25311;&#22120;&#20013;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#20102;SSET&#30456;&#23545;&#20110;&#29616;&#26377;ER&#32531;&#20914;&#21306;&#37319;&#26679;&#26041;&#27861;&#30340;&#20248;&#21183;&#21644;&#22810;&#21151;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Experience replay (ER) is a crucial component of many deep reinforcement learning (RL) systems. However, uniform sampling from an ER buffer can lead to slow convergence and unstable asymptotic behaviors. This paper introduces Stratified Sampling from Event Tables (SSET), which partitions an ER buffer into Event Tables, each capturing important subsequences of optimal behavior. We prove a theoretical advantage over the traditional monolithic buffer approach and combine SSET with an existing prioritized sampling strategy to further improve learning speed and stability. Empirical results in challenging MiniGrid domains, benchmark RL environments, and a high-fidelity car racing simulator demonstrate the advantages and versatility of SSET over existing ER buffer sampling approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#38750;&#24179;&#34913;&#21453;&#24212;&#27969;&#21160;&#27169;&#25311;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#20998;&#23618;&#33258;&#36866;&#24212;&#28145;&#24230;&#23398;&#20064;&#31574;&#30053;&#23558;&#38477;&#32500;&#21644;&#31070;&#32463;&#32593;&#32476;&#31639;&#23376;&#32467;&#21512;&#36215;&#26469;&#65292;&#23398;&#20064;&#21270;&#23398;&#21160;&#21147;&#23398;&#39046;&#22495;&#22810;&#23610;&#24230;&#31895;&#31890;&#21270;&#21644;&#25511;&#21046;&#26041;&#31243;&#30340;&#35299;&#12290;&#36825;&#31181;&#20998;&#23618;&#32467;&#26500;&#30340;&#31574;&#30053;&#20351;&#24471;&#35757;&#32451;&#31616;&#21333;&#19988;&#35745;&#31639;&#25928;&#29575;&#39640;&#12290;</title><link>http://arxiv.org/abs/2210.15799</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#27714;&#35299;&#38750;&#24179;&#34913;&#27969;&#20307;&#30340;&#31895;&#31890;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Adaptive physics-informed neural operator for coarse-grained non-equilibrium flows. (arXiv:2210.15799v2 [physics.comp-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15799
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#38750;&#24179;&#34913;&#21453;&#24212;&#27969;&#21160;&#27169;&#25311;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#20998;&#23618;&#33258;&#36866;&#24212;&#28145;&#24230;&#23398;&#20064;&#31574;&#30053;&#23558;&#38477;&#32500;&#21644;&#31070;&#32463;&#32593;&#32476;&#31639;&#23376;&#32467;&#21512;&#36215;&#26469;&#65292;&#23398;&#20064;&#21270;&#23398;&#21160;&#21147;&#23398;&#39046;&#22495;&#22810;&#23610;&#24230;&#31895;&#31890;&#21270;&#21644;&#25511;&#21046;&#26041;&#31243;&#30340;&#35299;&#12290;&#36825;&#31181;&#20998;&#23618;&#32467;&#26500;&#30340;&#31574;&#30053;&#20351;&#24471;&#35757;&#32451;&#31616;&#21333;&#19988;&#35745;&#31639;&#25928;&#29575;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#38750;&#24179;&#34913;&#21453;&#24212;&#27969;&#21160;&#27169;&#25311;&#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#21516;&#26102;&#30830;&#20445;&#31526;&#21512;&#24213;&#23618;&#29289;&#29702;&#35268;&#24459;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#20998;&#23618;&#33258;&#36866;&#24212;&#28145;&#24230;&#23398;&#20064;&#31574;&#30053;&#23558;&#38477;&#32500;&#21644;&#31070;&#32463;&#32593;&#32476;&#31639;&#23376;&#32467;&#21512;&#36215;&#26469;&#65292;&#23398;&#20064;&#21270;&#23398;&#21160;&#21147;&#23398;&#39046;&#22495;&#22810;&#23610;&#24230;&#31895;&#31890;&#21270;&#21644;&#25511;&#21046;&#26041;&#31243;&#30340;&#35299;&#12290;&#25152;&#25552;&#20986;&#30340;&#20195;&#29702;&#30340;&#32467;&#26500;&#34987;&#32452;&#32455;&#25104;&#19968;&#20010;&#26641;&#65292;&#20854;&#20013;&#21494;&#33410;&#28857;&#34920;&#31034;&#21333;&#29420;&#30340;&#31070;&#32463;&#32593;&#32476;&#31639;&#23376;&#22359;&#65292;&#29289;&#29702;&#32422;&#26463;&#20197;&#22810;&#31181;&#36719;&#30828;&#32422;&#26463;&#30340;&#24418;&#24335;&#23884;&#20837;&#20854;&#20013;&#12290;&#20998;&#23618;&#23646;&#24615;&#26377;&#20004;&#20010;&#20248;&#28857;&#65306;i&#65289;&#36890;&#36807;&#20174;&#26368;&#24930;&#30340;&#26102;&#38388;&#23610;&#24230;&#24320;&#22987;&#30340;&#36801;&#31227;&#23398;&#20064;&#65292;&#20351;&#35757;&#32451;&#38454;&#27573;&#21464;&#24471;&#31616;&#21333;&#65307;ii&#65289;&#36890;&#36807;&#22522;&#20110;&#27668;&#20307;&#30340;&#23616;&#37096;&#38750;&#24179;&#34913;&#31243;&#24230;&#20165;&#23545;&#24517;&#35201;&#30340;&#21494;&#33410;&#28857;&#36827;&#34892;&#20195;&#29702;&#35780;&#20272;&#65292;&#20174;&#32780;&#21152;&#36895;&#39044;&#27979;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work proposes a new machine learning (ML)-based paradigm aiming to enhance the computational efficiency of non-equilibrium reacting flow simulations while ensuring compliance with the underlying physics. The framework combines dimensionality reduction and neural operators through a hierarchical and adaptive deep learning strategy to learn the solution of multi-scale coarse-grained governing equations for chemical kinetics. The proposed surrogate's architecture is structured as a tree, with leaf nodes representing separate neural operator blocks where physics is embedded in the form of multiple soft and hard constraints. The hierarchical attribute has two advantages: i) It allows the simplification of the training phase via transfer learning, starting from the slowest temporal scales; ii) It accelerates the prediction step by enabling adaptivity as the surrogate's evaluation is limited to the necessary leaf nodes based on the local degree of non-equilibrium of the gas. The model is
&lt;/p&gt;</description></item><item><title>&#31232;&#30095;&#22270;&#20013;&#30340;&#22823;&#37327;&#19977;&#35282;&#24418;&#21487;&#20351;&#29992;&#26080;&#38480;&#32500;&#24230;&#20869;&#31215;&#27169;&#22411;&#36827;&#34892;&#22797;&#29616;&#65292;&#20854;&#20013;&#33410;&#28857;&#34920;&#31034;&#20301;&#20110;&#20302;&#32500;&#27969;&#24418;&#19978;&#12290;&#34429;&#28982;&#20840;&#23616;&#34920;&#31034;&#26159;&#19981;&#21487;&#33021;&#30340;&#65292;&#20294;&#25105;&#20204;&#21487;&#20197;&#22312;&#26412;&#22320;&#37051;&#22495;&#32553;&#23567;&#35268;&#27169;&#65292;&#20197;&#33719;&#21462;&#36739;&#20302;&#32500;&#24230;&#30340;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2210.15277</link><description>&lt;p&gt;
&#31232;&#30095;&#19982;&#39640;&#19977;&#35282;&#23494;&#24230;&#23545;&#20110;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Implications of sparsity and high triangle density for graph representation learning. (arXiv:2210.15277v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15277
&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#22270;&#20013;&#30340;&#22823;&#37327;&#19977;&#35282;&#24418;&#21487;&#20351;&#29992;&#26080;&#38480;&#32500;&#24230;&#20869;&#31215;&#27169;&#22411;&#36827;&#34892;&#22797;&#29616;&#65292;&#20854;&#20013;&#33410;&#28857;&#34920;&#31034;&#20301;&#20110;&#20302;&#32500;&#27969;&#24418;&#19978;&#12290;&#34429;&#28982;&#20840;&#23616;&#34920;&#31034;&#26159;&#19981;&#21487;&#33021;&#30340;&#65292;&#20294;&#25105;&#20204;&#21487;&#20197;&#22312;&#26412;&#22320;&#37051;&#22495;&#32553;&#23567;&#35268;&#27169;&#65292;&#20197;&#33719;&#21462;&#36739;&#20302;&#32500;&#24230;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#21253;&#21547;&#35768;&#22810;&#19977;&#35282;&#24418;&#30340;&#31232;&#30095;&#22270;&#20013;&#65292;&#26080;&#27861;&#20351;&#29992;&#33410;&#28857;&#30340;&#26377;&#38480;&#32500;&#24230;&#34920;&#31034;&#26469;&#37325;&#29616;&#65292;&#20854;&#20013;&#36830;&#32467;&#27010;&#29575;&#26159;&#20869;&#31215;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#26679;&#30340;&#22270;&#21487;&#20197;&#20351;&#29992;&#26080;&#38480;&#32500;&#24230;&#30340;&#20869;&#31215;&#27169;&#22411;&#26469;&#22797;&#29616;&#65292;&#20854;&#20013;&#33410;&#28857;&#34920;&#31034;&#20301;&#20110;&#20302;&#32500;&#27969;&#24418;&#19978;&#12290;&#22312;&#31232;&#30095;&#30340;&#24773;&#20917;&#19979;&#65292;&#24674;&#22797;&#27969;&#24418;&#30340;&#20840;&#23616;&#34920;&#31034;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21487;&#20197;&#32553;&#23567;&#21040;&#26412;&#22320;&#37051;&#22495;&#65292;&#22312;&#37027;&#37324;&#36739;&#20302;&#32500;&#24230;&#30340;&#34920;&#31034;&#26159;&#21487;&#33021;&#30340;&#12290;&#30001;&#20110;&#25105;&#20204;&#30340;&#26500;&#36896;&#20801;&#35768;&#28857;&#22343;&#21248;&#20998;&#24067;&#22312;&#27969;&#24418;&#19978;&#65292;&#22240;&#27492;&#25105;&#20204;&#21457;&#29616;&#20102;&#21453;&#23545;&#36890;&#24120;&#30340;&#30475;&#27861;&#8212;&#8212;&#19977;&#35282;&#24418;&#24847;&#21619;&#30528;&#31038;&#21306;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has shown that sparse graphs containing many triangles cannot be reproduced using a finite-dimensional representation of the nodes, in which link probabilities are inner products. Here, we show that such graphs can be reproduced using an infinite-dimensional inner product model, where the node representations lie on a low-dimensional manifold. Recovering a global representation of the manifold is impossible in a sparse regime. However, we can zoom in on local neighbourhoods, where a lower-dimensional representation is possible. As our constructions allow the points to be uniformly distributed on the manifold, we find evidence against the common perception that triangles imply community structure.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#20559;&#24046;-&#26041;&#24046;&#20998;&#35299;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#27169;&#22411;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#22823;&#22810;&#25968;&#39044;&#27979;&#20219;&#21153;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#23454;&#38469;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2210.12256</link><description>&lt;p&gt;
&#19968;&#31181;&#36890;&#29992;&#20559;&#24046;-&#26041;&#24046;&#20998;&#35299;&#26041;&#27861;&#29992;&#20110;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Estimates of Predictions via a General Bias-Variance Decomposition. (arXiv:2210.12256v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#20559;&#24046;-&#26041;&#24046;&#20998;&#35299;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#27169;&#22411;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#22823;&#22810;&#25968;&#39044;&#27979;&#20219;&#21153;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#23454;&#38469;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23433;&#20840;&#20851;&#38190;&#30340;&#24212;&#29992;&#20013;&#65292;&#21487;&#38752;&#22320;&#20272;&#35745;&#27169;&#22411;&#29983;&#21629;&#21608;&#26399;&#20869;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#24120;&#29992;&#30340;&#34913;&#37327;&#26041;&#27861;&#26159;&#36890;&#36807;&#39044;&#27979;&#32622;&#20449;&#24230;&#26469;&#34913;&#37327;&#12290;&#34429;&#28982;&#36825;&#31181;&#26041;&#27861;&#22312;&#39046;&#22495;&#20869;&#26679;&#26412;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#39046;&#22495;&#28418;&#31227;&#26102;&#36825;&#20123;&#20272;&#35745;&#26159;&#19981;&#21487;&#38752;&#30340;&#65292;&#24182;&#19988;&#20165;&#38480;&#20110;&#20998;&#31867;&#12290;&#30456;&#21453;&#65292;&#23545;&#20110;&#22823;&#22810;&#25968;&#39044;&#27979;&#20219;&#21153;&#65292;&#21487;&#20197;&#20351;&#29992;&#36866;&#24403;&#30340;&#24471;&#20998;&#65292;&#20294;&#26159;&#24403;&#21069;&#25991;&#29486;&#20013;&#19981;&#23384;&#22312;&#29992;&#20110;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#20559;&#24046;-&#26041;&#24046;&#20998;&#35299;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36866;&#29992;&#20110;&#36866;&#24403;&#24471;&#20998;&#30340;&#36890;&#29992;&#20559;&#24046;-&#26041;&#24046;&#20998;&#35299;&#26041;&#27861;&#65292;&#30001;&#27492;&#24341;&#20986;Bregman&#20449;&#24687;&#20316;&#20026;&#26041;&#24046;&#39033;&#12290;&#25105;&#20204;&#21457;&#29616;&#25351;&#25968;&#26063;&#21644;&#20998;&#31867;&#23545;&#25968;&#20284;&#28982;&#26159;&#29305;&#27530;&#24773;&#20917;&#65292;&#24182;&#25552;&#20379;&#20102;&#26032;&#30340;&#20844;&#24335;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21487;&#20197;&#32431;&#31929;&#22320;&#22312;logit&#31354;&#38388;&#20013;&#34920;&#31034;&#20998;&#31867;&#24773;&#20917;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#20998;&#35299;&#26041;&#27861;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#23454;&#38469;&#30456;&#20851;&#24615;&#65292;&#21253;&#25324;&#27169;&#22411;&#38598;&#25104;&#21644;&#32622;&#20449;&#21306;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reliably estimating the uncertainty of a prediction throughout the model lifecycle is crucial in many safety-critical applications. The most common way to measure this uncertainty is via the predicted confidence. While this tends to work well for in-domain samples, these estimates are unreliable under domain drift and restricted to classification. Alternatively, proper scores can be used for most predictive tasks but a bias-variance decomposition for model uncertainty does not exist in the current literature. In this work we introduce a general bias-variance decomposition for proper scores, giving rise to the Bregman Information as the variance term. We discover how exponential families and the classification log-likelihood are special cases and provide novel formulations. Surprisingly, we can express the classification case purely in the logit space. We showcase the practical relevance of this decomposition on several downstream tasks, including model ensembles and confidence regions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#24341;&#23548;&#37319;&#26679;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#38544;&#31169;&#25104;&#26412;&#30340;&#26032;&#32467;&#26524;&#65292;&#21487;&#29992;&#20110;&#25512;&#26029;&#26679;&#26412;&#20998;&#24067;&#24182;&#26500;&#24314;&#32622;&#20449;&#21306;&#38388;&#65292;&#21516;&#26102;&#25351;&#20986;&#20102;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#35823;&#29992;&#12290;&#38543;&#30528;&#37319;&#26679;&#27425;&#25968;&#36235;&#36817;&#26080;&#38480;&#22823;&#65292;&#27492;&#26041;&#27861;&#36880;&#28176;&#28385;&#36275;&#26356;&#20005;&#26684;&#30340;&#24046;&#20998;&#38544;&#31169;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2210.06140</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#24341;&#23548;&#37319;&#26679;&#65306;&#26032;&#30340;&#38544;&#31169;&#20998;&#26512;&#19982;&#25512;&#26029;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Bootstrap: New Privacy Analysis and Inference Strategies. (arXiv:2210.06140v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#24341;&#23548;&#37319;&#26679;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#38544;&#31169;&#25104;&#26412;&#30340;&#26032;&#32467;&#26524;&#65292;&#21487;&#29992;&#20110;&#25512;&#26029;&#26679;&#26412;&#20998;&#24067;&#24182;&#26500;&#24314;&#32622;&#20449;&#21306;&#38388;&#65292;&#21516;&#26102;&#25351;&#20986;&#20102;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#35823;&#29992;&#12290;&#38543;&#30528;&#37319;&#26679;&#27425;&#25968;&#36235;&#36817;&#26080;&#38480;&#22823;&#65292;&#27492;&#26041;&#27861;&#36880;&#28176;&#28385;&#36275;&#26356;&#20005;&#26684;&#30340;&#24046;&#20998;&#38544;&#31169;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#36890;&#36807;&#24341;&#20837;&#38543;&#26426;&#24615;&#26469;&#20445;&#25252;&#20010;&#20154;&#20449;&#24687;&#65292;&#20294;&#22312;&#24212;&#29992;&#20013;&#65292;&#32479;&#35745;&#25512;&#26029;&#20173;&#28982;&#32570;&#20047;&#36890;&#29992;&#25216;&#26415;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#24046;&#20998;&#38544;&#31169;&#24341;&#23548;&#37319;&#26679;&#26041;&#27861;&#65292;&#36890;&#36807;&#21457;&#24067;&#22810;&#20010;&#31169;&#26377;&#24341;&#23548;&#37319;&#26679;&#20272;&#35745;&#26469;&#25512;&#26029;&#26679;&#26412;&#20998;&#24067;&#24182;&#26500;&#24314;&#32622;&#20449;&#21306;&#38388;&#12290;&#25105;&#20204;&#30340;&#38544;&#31169;&#20998;&#26512;&#25552;&#20379;&#20102;&#21333;&#20010;&#24046;&#20998;&#38544;&#31169;&#24341;&#23548;&#37319;&#26679;&#20272;&#35745;&#30340;&#38544;&#31169;&#25104;&#26412;&#26032;&#32467;&#26524;&#65292;&#36866;&#29992;&#20110;&#20219;&#20309;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#65292;&#24182;&#25351;&#20986;&#20102;&#29616;&#26377;&#25991;&#29486;&#20013;&#24341;&#23548;&#37319;&#26679;&#30340;&#19968;&#20123;&#35823;&#29992;&#12290;&#20351;&#29992;Gaussian-DP&#65288;GDP&#65289;&#26694;&#26550;&#65292;&#25105;&#20204;&#35777;&#26126;&#20174;&#28385;&#36275; $(\mu/\sqrt{(2-2/\mathrm{e})B})$-GDP &#30340;&#26426;&#21046;&#20013;&#37322;&#25918; $B$ &#20010;&#24046;&#20998;&#38544;&#31169;&#24341;&#23548;&#37319;&#26679;&#20272;&#35745;&#65292;&#22312; $B$ &#36235;&#36817;&#26080;&#38480;&#22823;&#26102;&#28176;&#36817;&#22320;&#28385;&#36275; $\mu$-GDP&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#24341;&#23548;&#37319;&#26679;&#20272;&#35745;&#30340;&#21453;&#21367;&#31215;&#23545;&#26679;&#26412;&#20998;&#24067;&#36827;&#34892;&#20934;&#30830;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differentially private (DP) mechanisms protect individual-level information by introducing randomness into the statistical analysis procedure. Despite the availability of numerous DP tools, there remains a lack of general techniques for conducting statistical inference under DP. We examine a DP bootstrap procedure that releases multiple private bootstrap estimates to infer the sampling distribution and construct confidence intervals (CIs). Our privacy analysis presents new results on the privacy cost of a single DP bootstrap estimate, applicable to any DP mechanisms, and identifies some misapplications of the bootstrap in the existing literature. Using the Gaussian-DP (GDP) framework (Dong et al.,2022), we show that the release of $B$ DP bootstrap estimates from mechanisms satisfying $(\mu/\sqrt{(2-2/\mathrm{e})B})$-GDP asymptotically satisfies $\mu$-GDP as $B$ goes to infinity. Moreover, we use deconvolution with the DP bootstrap estimates to accurately infer the sampling distribution
&lt;/p&gt;</description></item><item><title>BLOP&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#20462;&#21098;&#26041;&#27861;&#65292;&#21033;&#29992;&#21452;&#23618;&#20248;&#21270;&#30340;&#26041;&#27861;&#23545;&#20462;&#21098;&#38382;&#39064;&#36827;&#34892;&#20102;&#37325;&#26032;&#35299;&#37322;&#65292;&#21487;&#20197;&#22312;&#22810;&#20010;&#24120;&#29992;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#34920;&#29616;&#30340;&#25552;&#21319;&#65292;&#24182;&#27604;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#26356;&#24555;&#30340;&#35745;&#31639;&#36895;&#24230;&#21644;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2210.04092</link><description>&lt;p&gt;
&#36890;&#36807;&#21452;&#23618;&#20248;&#21270;&#25512;&#36827;&#27169;&#22411;&#20462;&#21098;
&lt;/p&gt;
&lt;p&gt;
Advancing Model Pruning via Bi-level Optimization. (arXiv:2210.04092v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04092
&lt;/p&gt;
&lt;p&gt;
BLOP&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#20462;&#21098;&#26041;&#27861;&#65292;&#21033;&#29992;&#21452;&#23618;&#20248;&#21270;&#30340;&#26041;&#27861;&#23545;&#20462;&#21098;&#38382;&#39064;&#36827;&#34892;&#20102;&#37325;&#26032;&#35299;&#37322;&#65292;&#21487;&#20197;&#22312;&#22810;&#20010;&#24120;&#29992;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#34920;&#29616;&#30340;&#25552;&#21319;&#65292;&#24182;&#27604;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#26356;&#24555;&#30340;&#35745;&#31639;&#36895;&#24230;&#21644;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#37096;&#32626;&#38480;&#21046;&#38656;&#35201;&#20462;&#21098;&#22823;&#35268;&#27169;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21363;&#20419;&#36827;&#23427;&#20204;&#30340;&#26435;&#37325;&#31232;&#30095;&#24615;&#12290;&#20462;&#21098;&#36824;&#26377;&#21487;&#33021;&#25552;&#39640;&#23427;&#20204;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20462;&#21098;&#26041;&#27861;&#65306;&#21452;&#23618;&#20248;&#21270;&#65288;BLOP&#65289;&#65292;&#21033;&#29992;&#21452;&#23618;&#20248;&#21270;&#30340;&#26041;&#27861;&#23545;&#20462;&#21098;&#38382;&#39064;&#36827;&#34892;&#20102;&#37325;&#26032;&#35299;&#37322;&#65292;&#24182;&#22312;&#22810;&#20010;&#24120;&#29992;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#34920;&#29616;&#30340;&#25552;&#21319;&#12290;&#35813;&#26041;&#27861;&#27604;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#26356;&#24555;&#30340;&#35745;&#31639;&#36895;&#24230;&#21644;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The deployment constraints in practical applications necessitate the pruning of large-scale deep learning models, i.e., promoting their weight sparsity. As illustrated by the Lottery Ticket Hypothesis (LTH), pruning also has the potential of improving their generalization ability. At the core of LTH, iterative magnitude pruning (IMP) is the predominant pruning method to successfully find 'winning tickets'. Yet, the computation cost of IMP grows prohibitively as the targeted pruning ratio increases. To reduce the computation overhead, various efficient 'one-shot' pruning methods have been developed, but these schemes are usually unable to find winning tickets as good as IMP. This raises the question of how to close the gap between pruning accuracy and pruning efficiency? To tackle it, we pursue the algorithmic advancement of model pruning. Specifically, we formulate the pruning problem from a fresh and novel viewpoint, bi-level optimization (BLO). We show that the BLO interpretation pro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#20844;&#24179;&#32858;&#31867;&#31639;&#27861;FCMI&#65292;&#22312;&#20114;&#20449;&#24687;&#29702;&#35770;&#30340;&#22522;&#30784;&#19978;&#36827;&#34892;&#35774;&#35745;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#21644;&#26368;&#23567;&#21270;&#20114;&#20449;&#24687;&#23454;&#29616;&#20102;&#25968;&#25454;&#30340;&#32039;&#20945;&#12289;&#24179;&#34913;&#21644;&#20844;&#24179;&#30340;&#32858;&#31867;&#65292;&#20197;&#21450;&#20855;&#26377;&#20449;&#24687;&#37327;&#30340;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20844;&#24179;&#32858;&#31867;&#24230;&#37327;&#26631;&#20934;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#34913;&#37327;&#32858;&#31867;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2209.12396</link><description>&lt;p&gt;
&#36890;&#36807;&#26368;&#22823;&#21270;&#21644;&#26368;&#23567;&#21270;&#20114;&#20449;&#24687;&#36827;&#34892;&#28145;&#24230;&#20844;&#24179;&#32858;&#31867;&#65306;&#29702;&#35770;&#12289;&#31639;&#27861;&#21644;&#24230;&#37327;(arXiv:2209.12396v2 [cs.LG]&#24050;&#26356;&#26032;)
&lt;/p&gt;
&lt;p&gt;
Deep Fair Clustering via Maximizing and Minimizing Mutual Information: Theory, Algorithm and Metric. (arXiv:2209.12396v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.12396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#20844;&#24179;&#32858;&#31867;&#31639;&#27861;FCMI&#65292;&#22312;&#20114;&#20449;&#24687;&#29702;&#35770;&#30340;&#22522;&#30784;&#19978;&#36827;&#34892;&#35774;&#35745;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#21644;&#26368;&#23567;&#21270;&#20114;&#20449;&#24687;&#23454;&#29616;&#20102;&#25968;&#25454;&#30340;&#32039;&#20945;&#12289;&#24179;&#34913;&#21644;&#20844;&#24179;&#30340;&#32858;&#31867;&#65292;&#20197;&#21450;&#20855;&#26377;&#20449;&#24687;&#37327;&#30340;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20844;&#24179;&#32858;&#31867;&#24230;&#37327;&#26631;&#20934;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#34913;&#37327;&#32858;&#31867;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#24179;&#32858;&#31867;&#26088;&#22312;&#23558;&#25968;&#25454;&#21010;&#20998;&#20026;&#19981;&#21516;&#30340;&#31751;&#65292;&#21516;&#26102;&#38450;&#27490;&#25935;&#24863;&#23646;&#24615;&#65288;&#20363;&#22914;&#24615;&#21035;&#12289;&#31181;&#26063;&#12289;RNA&#27979;&#24207;&#25216;&#26415;&#65289;&#22312;&#32858;&#31867;&#20013;&#21344;&#20027;&#23548;&#22320;&#20301;&#12290;&#23613;&#31649;&#26368;&#36817;&#36827;&#34892;&#20102;&#35768;&#22810;&#24037;&#20316;&#24182;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#22823;&#22810;&#25968;&#37117;&#26159;&#21551;&#21457;&#24335;&#30340;&#65292;&#32570;&#20047;&#32479;&#19968;&#30340;&#31639;&#27861;&#35774;&#35745;&#29702;&#35770;&#12290;&#26412;&#25991;&#36890;&#36807;&#21457;&#23637;&#28145;&#24230;&#20844;&#24179;&#32858;&#31867;&#30340;&#20114;&#20449;&#24687;&#29702;&#35770;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;&#31639;&#27861;FCMI&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#12290;&#31616;&#32780;&#35328;&#20043;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#21644;&#26368;&#23567;&#21270;&#20114;&#20449;&#24687;&#65292;FCMI&#26088;&#22312;&#23454;&#29616;&#28145;&#24230;&#20844;&#24179;&#32858;&#31867;&#39640;&#24230;&#26399;&#26395;&#30340;&#22235;&#20010;&#29305;&#24449;&#65292;&#21363;&#32039;&#20945;&#12289;&#24179;&#34913;&#21644;&#20844;&#24179;&#30340;&#32858;&#31867;&#65292;&#20197;&#21450;&#20855;&#26377;&#20449;&#24687;&#37327;&#30340;&#29305;&#24449;&#12290;&#38500;&#20102;&#23545;&#29702;&#35770;&#21644;&#31639;&#27861;&#30340;&#36129;&#29486;&#22806;&#65292;&#26412;&#25991;&#30340;&#21478;&#19968;&#20010;&#36129;&#29486;&#26159;&#25552;&#20986;&#20102;&#19968;&#31181;&#24314;&#31435;&#22312;&#20449;&#24687;&#29702;&#35770;&#22522;&#30784;&#19978;&#30340;&#26032;&#30340;&#20844;&#24179;&#32858;&#31867;&#24230;&#37327;&#26631;&#20934;&#12290;&#19982;&#29616;&#26377;&#30340;&#35780;&#20272;&#25351;&#26631;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#24230;&#37327;&#26631;&#20934;&#34913;&#37327;&#32858;&#31867;&#36136;&#37327;...
&lt;/p&gt;
&lt;p&gt;
Fair clustering aims to divide data into distinct clusters while preventing sensitive attributes (\textit{e.g.}, gender, race, RNA sequencing technique) from dominating the clustering. Although a number of works have been conducted and achieved huge success recently, most of them are heuristical, and there lacks a unified theory for algorithm design. In this work, we fill this blank by developing a mutual information theory for deep fair clustering and accordingly designing a novel algorithm, dubbed FCMI. In brief, through maximizing and minimizing mutual information, FCMI is designed to achieve four characteristics highly expected by deep fair clustering, \textit{i.e.}, compact, balanced, and fair clusters, as well as informative features. Besides the contributions to theory and algorithm, another contribution of this work is proposing a novel fair clustering metric built upon information theory as well. Unlike existing evaluation metrics, our metric measures the clustering quality an
&lt;/p&gt;</description></item><item><title>Scyan&#26159;&#19968;&#31181;&#21333;&#32454;&#32990;&#32454;&#32990;&#23398;&#27880;&#37322;&#32593;&#32476;&#65292;&#20165;&#20351;&#29992;&#26377;&#20851;&#32454;&#32990;&#23398;&#38754;&#26495;&#30340;&#20808;&#21069;&#19987;&#23478;&#30693;&#35782;&#33258;&#21160;&#27880;&#37322;&#32454;&#32990;&#31867;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#20256;&#32479;&#25163;&#21160;&#31579;&#36873;&#32454;&#32990;&#20998;&#31867;&#30340;&#32570;&#20047;&#20877;&#29616;&#24615;&#21644;&#20154;&#24037;&#32791;&#26102;&#30340;&#38382;&#39064;&#12290;&#23427;&#36824;&#21487;&#20197;&#24555;&#36895;&#21435;&#38500;&#25209;&#27425;&#25928;&#24212;&#65292;&#21435;&#26465;&#24418;&#30721;&#21644;&#20154;&#21475;&#21457;&#29616;&#12290;</title><link>http://arxiv.org/abs/2208.05745</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#29983;&#29289;&#23398;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#29992;&#20110;&#32454;&#32990;&#20998;&#31867;&#23398;&#20013;&#30340;&#21333;&#32454;&#32990;&#27880;&#37322;
&lt;/p&gt;
&lt;p&gt;
A biology-driven deep generative model for cell-type annotation in cytometry. (arXiv:2208.05745v2 [q-bio.QM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.05745
&lt;/p&gt;
&lt;p&gt;
Scyan&#26159;&#19968;&#31181;&#21333;&#32454;&#32990;&#32454;&#32990;&#23398;&#27880;&#37322;&#32593;&#32476;&#65292;&#20165;&#20351;&#29992;&#26377;&#20851;&#32454;&#32990;&#23398;&#38754;&#26495;&#30340;&#20808;&#21069;&#19987;&#23478;&#30693;&#35782;&#33258;&#21160;&#27880;&#37322;&#32454;&#32990;&#31867;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#20256;&#32479;&#25163;&#21160;&#31579;&#36873;&#32454;&#32990;&#20998;&#31867;&#30340;&#32570;&#20047;&#20877;&#29616;&#24615;&#21644;&#20154;&#24037;&#32791;&#26102;&#30340;&#38382;&#39064;&#12290;&#23427;&#36824;&#21487;&#20197;&#24555;&#36895;&#21435;&#38500;&#25209;&#27425;&#25928;&#24212;&#65292;&#21435;&#26465;&#24418;&#30721;&#21644;&#20154;&#21475;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32454;&#32990;&#20998;&#31867;&#23398;&#21487;&#23454;&#29616;&#23545;&#26434;&#22810;&#31181;&#32676;&#20307;&#20869;&#21333;&#20010;&#32454;&#32990;&#30340;&#20934;&#30830;&#34920;&#22411;&#20998;&#26512;&#12290;&#20256;&#32479;&#19978;&#36825;&#20123;&#32454;&#32990;&#25353;&#20154;&#24037;&#31579;&#36873;&#26041;&#24335;&#36827;&#34892;&#20998;&#31867;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#32570;&#20047;&#20877;&#29616;&#24615;&#65292;&#19988;&#23545;&#25209;&#27425;&#25928;&#24212;&#25935;&#24863;&#12290;&#26368;&#26032;&#30340;&#20809;&#35889;&#27969;&#24335;&#32454;&#32990;&#26415;&#25110;&#36136;&#35889;&#32454;&#32990;&#26415;&#21019;&#36896;&#20102;&#20016;&#23500;&#21644;&#39640;&#32500;&#25968;&#25454;&#65292;&#32780;&#36890;&#36807;&#20154;&#24037;&#31579;&#36873;&#30340;&#20998;&#26512;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#32791;&#26102;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Scyan&#65288;https://github.com/MICS-Lab/scyan&#65289;&#65292;&#19968;&#31181;&#21333;&#32454;&#32990;&#32454;&#32990;&#23398;&#27880;&#37322;&#32593;&#32476;&#65292;&#20165;&#20351;&#29992;&#26377;&#20851;&#32454;&#32990;&#23398;&#38754;&#26495;&#30340;&#20808;&#21069;&#19987;&#23478;&#30693;&#35782;&#33258;&#21160;&#27880;&#37322;&#32454;&#32990;&#31867;&#22411;&#12290;&#25105;&#20204;&#35777;&#26126;Scyan&#22312;&#22810;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#26174;&#30528;&#20248;&#20110;&#30456;&#20851;&#30340;&#26368;&#26032;&#27169;&#22411;&#65292;&#21516;&#26102;&#36895;&#24230;&#26356;&#24555;&#19988;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#12290;&#27492;&#22806;&#65292;Scyan&#36824;&#21487;&#35299;&#20915;&#20960;&#20010;&#36741;&#21161;&#20219;&#21153;&#65292;&#20363;&#22914;&#25209;&#27425;&#25928;&#24212;&#21435;&#38500;&#12289;&#21435;&#26465;&#24418;&#30721;&#21644;&#20154;&#21475;&#21457;&#29616;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#36825;&#19968;&#27169;&#22411;&#21152;&#36895;&#21644;&#31616;&#21270;&#20102;&#32454;&#32990;&#31181;&#32676;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cytometry enables precise single-cell phenotyping within heterogeneous populations. These cell types are traditionally annotated via manual gating, but this method suffers from a lack of reproducibility and sensitivity to batch-effect. Also, the most recent cytometers - spectral flow or mass cytometers - create rich and high-dimensional data whose analysis via manual gating becomes challenging and time-consuming. To tackle these limitations, we introduce Scyan (https://github.com/MICS-Lab/scyan), a Single-cell Cytometry Annotation Network that automatically annotates cell types using only prior expert knowledge about the cytometry panel. We demonstrate that Scyan significantly outperforms the related state-of-the-art models on multiple public datasets while being faster and interpretable. In addition, Scyan overcomes several complementary tasks such as batch-effect removal, debarcoding, and population discovery. Overall, this model accelerates and eases cell population characterisation
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#26694;&#26550;&#30340;&#20998;&#23376;&#34920;&#24449;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25237;&#24433;&#26469;&#25429;&#25417;&#20960;&#20309;&#20449;&#24687;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#29305;&#27530;&#35774;&#35745;&#26469;&#28385;&#36275;&#23545;&#31216;&#35201;&#27714;&#65292;&#20854;&#30456;&#23545;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#24555;&#30340;&#35745;&#31639;&#36895;&#24230;&#21644;&#26356;&#23481;&#26131;&#30340;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2208.00716</link><description>&lt;p&gt;
&#22522;&#20110;&#23616;&#37096;&#26694;&#26550;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#20998;&#23376;&#21183;&#33021;&#38754;&#24314;&#27169;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Network with Local Frame for Molecular Potential Energy Surface. (arXiv:2208.00716v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.00716
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#26694;&#26550;&#30340;&#20998;&#23376;&#34920;&#24449;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25237;&#24433;&#26469;&#25429;&#25417;&#20960;&#20309;&#20449;&#24687;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#29305;&#27530;&#35774;&#35745;&#26469;&#28385;&#36275;&#23545;&#31216;&#35201;&#27714;&#65292;&#20854;&#30456;&#23545;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#24555;&#30340;&#35745;&#31639;&#36895;&#24230;&#21644;&#26356;&#23481;&#26131;&#30340;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#21183;&#33021;&#38754;&#24314;&#27169;&#26159;&#31185;&#23398;&#30740;&#31350;&#20013;&#38750;&#24120;&#20851;&#38190;&#30340;&#19968;&#37096;&#20998;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#36825;&#20010;&#39046;&#22495;&#20869;&#24050;&#32463;&#23637;&#31034;&#20986;&#20102;&#24456;&#22823;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#30340;&#20449;&#24687;&#20256;&#36882;&#26041;&#26696;&#38656;&#35201;&#29305;&#27530;&#35774;&#35745;&#26469;&#25429;&#25417;&#20960;&#20309;&#20449;&#24687;&#21644;&#28385;&#36275;&#26059;&#36716;&#31561;&#21464;&#24615;&#36825;&#26679;&#30340;&#23545;&#31216;&#35201;&#27714;&#65292;&#23548;&#33268;&#20102;&#22797;&#26434;&#30340;&#32467;&#26500;&#12290;&#20026;&#20102;&#36991;&#20813;&#36825;&#20123;&#35774;&#35745;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#23616;&#37096;&#26694;&#26550;&#26041;&#27861;&#26469;&#36827;&#34892;&#20998;&#23376;&#34920;&#24449;&#23398;&#20064;&#65292;&#24182;&#20998;&#26512;&#20102;&#20854;&#34920;&#36798;&#24615;&#12290;&#36890;&#36807;&#25237;&#24433;&#21040;&#26694;&#26550;&#19978;&#65292;&#31561;&#21464;&#29305;&#24449;&#22914;3D&#22352;&#26631;&#34987;&#36716;&#21270;&#20026;&#19981;&#21464;&#29305;&#24449;&#65292;&#36825;&#26679;&#25105;&#20204;&#23601;&#21487;&#20197;&#36890;&#36807;&#36825;&#20123;&#25237;&#24433;&#26469;&#25429;&#25417;&#20960;&#20309;&#20449;&#24687;&#65292;&#21516;&#26102;&#20174;GNN&#35774;&#35745;&#20013;&#20998;&#31163;&#23545;&#31216;&#35201;&#27714;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#65292;&#32473;&#23450;&#38750;&#36864;&#21270;&#30340;&#26694;&#26550;&#65292;&#21363;&#20351;&#26159;&#26222;&#36890;&#30340;GNN&#20063;&#21487;&#20197;&#23558;&#20998;&#23376;&#32534;&#30721;&#20026;&#21487;&#27880;&#20837;&#30340;&#65292;&#24182;&#19988;&#20351;&#29992;&#22352;&#26631;&#25237;&#24433;&#21644;&#26694;&#26550;&#20043;&#38388;&#30340;&#25237;&#24433;&#21487;&#20197;&#36798;&#21040;&#26368;&#22823;&#34920;&#36798;&#24615;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20351;&#29992;&#20102;&#31616;&#21333;&#30340;&#26222;&#36890;GNN&#26550;&#26500;&#65292;&#20294;&#26159;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#31934;&#24230;&#12290;&#36825;&#31181;&#31616;&#21333;&#30340;&#26550;&#26500;&#20063;&#20351;&#24471;&#35745;&#31639;&#26356;&#24555;&#65292;&#23454;&#29616;&#26356;&#23481;&#26131;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling molecular potential energy surface is of pivotal importance in science. Graph Neural Networks have shown great success in this field. However, their message passing schemes need special designs to capture geometric information and fulfill symmetry requirement like rotation equivariance, leading to complicated architectures. To avoid these designs, we introduce a novel local frame method to molecule representation learning and analyze its expressivity. Projected onto a frame, equivariant features like 3D coordinates are converted to invariant features, so that we can capture geometric information with these projections and decouple the symmetry requirement from GNN design. Theoretically, we prove that given non-degenerate frames, even ordinary GNNs can encode molecules injectively and reach maximum expressivity with coordinate projection and frame-frame projection. In experiments, our model uses a simple ordinary GNN architecture yet achieves state-of-the-art accuracy. The simp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21487;&#38752;&#34920;&#31034;&#20248;&#21270;&#65288;R3&#65289;&#30340;&#26032;&#22411;&#26080;&#30417;&#30563;&#32467;&#26500;&#20248;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#25552;&#39640;GNN&#30340;&#40065;&#26834;&#24615;&#33021;&#65292;&#36890;&#36807;&#23398;&#20064;&#36741;&#21161;&#22270;&#24418;&#24182;&#37325;&#24314;&#21407;&#22987;&#22270;&#24418;&#32467;&#26500;&#26469;&#20248;&#21270;&#33410;&#28857;&#34920;&#31034;&#65292;&#21487;&#26377;&#25928;&#20943;&#23569;&#32467;&#26500;&#21644;&#22522;&#20110;&#29305;&#24449;&#30340;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#24182;&#32988;&#36807;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2207.00012</link><description>&lt;p&gt;
&#21487;&#38752;&#34920;&#31034;&#20351;&#38450;&#24481;&#32773;&#26356;&#24378;&#22823;&#65306;&#40065;&#26834;GNN&#30340;&#26080;&#30417;&#30563;&#32467;&#26500;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Reliable Representations Make A Stronger Defender: Unsupervised Structure Refinement for Robust GNN. (arXiv:2207.00012v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.00012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21487;&#38752;&#34920;&#31034;&#20248;&#21270;&#65288;R3&#65289;&#30340;&#26032;&#22411;&#26080;&#30417;&#30563;&#32467;&#26500;&#20248;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#25552;&#39640;GNN&#30340;&#40065;&#26834;&#24615;&#33021;&#65292;&#36890;&#36807;&#23398;&#20064;&#36741;&#21161;&#22270;&#24418;&#24182;&#37325;&#24314;&#21407;&#22987;&#22270;&#24418;&#32467;&#26500;&#26469;&#20248;&#21270;&#33410;&#28857;&#34920;&#31034;&#65292;&#21487;&#26377;&#25928;&#20943;&#23569;&#32467;&#26500;&#21644;&#22522;&#20110;&#29305;&#24449;&#30340;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#24182;&#32988;&#36807;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#22788;&#29702;&#22270;&#24418;&#25968;&#25454;&#26041;&#38754;&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#24694;&#24847;&#20462;&#25913;&#22270;&#24418;&#32467;&#26500;&#26469;&#28798;&#38590;&#24615;&#22320;&#38477;&#20302;GNN&#30340;&#24615;&#33021;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#30452;&#25509;&#26041;&#27861;&#26159;&#36890;&#36807;&#23398;&#20064;&#20004;&#20010;&#31471;&#33410;&#28857;&#20043;&#38388;&#30340;&#25104;&#23545;&#34920;&#31034;&#30340;&#24230;&#37327;&#20989;&#25968;&#26469;&#24314;&#27169;&#36793;&#32536;&#26435;&#37325;&#65292;&#35797;&#22270;&#20026;&#23545;&#25239;&#24615;&#36793;&#32536;&#20998;&#37197;&#20302;&#26435;&#37325;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#20351;&#29992;&#21407;&#22987;&#29305;&#24449;&#25110;&#30001;&#26377;&#30417;&#30563;GNN&#23398;&#20064;&#30340;&#34920;&#31034;&#26469;&#27169;&#25311;&#36793;&#32536;&#26435;&#37325;&#12290;&#28982;&#32780;&#65292;&#36825;&#20004;&#31181;&#31574;&#30053;&#37117;&#38754;&#20020;&#19968;&#20123;&#30452;&#25509;&#38382;&#39064;&#65306;&#21407;&#22987;&#29305;&#24449;&#19981;&#33021;&#34920;&#31034;&#33410;&#28857;&#30340;&#21508;&#31181;&#23646;&#24615;&#65288;&#20363;&#22914;&#32467;&#26500;&#20449;&#24687;&#65289;&#65292;&#32780;&#30001;&#26377;&#30417;&#30563;GNN&#23398;&#20064;&#30340;&#34920;&#31034;&#21487;&#33021;&#20250;&#21463;&#21040;&#22312;&#27745;&#26579;&#22270;&#19978;&#20998;&#31867;&#22120;&#24615;&#33021;&#24046;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#38656;&#35201;&#34920;&#31034;&#26082;&#25658;&#24102;&#29305;&#24449;&#20449;&#24687;&#65292;&#21448;&#23613;&#21487;&#33021;&#22810;&#22320;&#25658;&#24102;&#27491;&#30830;&#32467;&#26500;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21487;&#38752;&#34920;&#31034;&#20248;&#21270;&#30340;&#26032;&#22411;&#26080;&#30417;&#30563;&#32467;&#26500;&#20248;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#40065;&#26834;GNN&#12290;R3&#39318;&#20808;&#36890;&#36807;&#21024;&#38500;&#23545;&#25239;&#24615;&#36793;&#32536;&#20174;&#36755;&#20837;&#22270;&#24418;&#20013;&#23398;&#20064;&#36741;&#21161;&#22270;&#24418;&#65292;&#28982;&#21518;&#36890;&#36807;&#37325;&#24314;&#21407;&#22987;&#22270;&#24418;&#32467;&#26500;&#26469;&#20248;&#21270;&#35813;&#36741;&#21161;&#22270;&#24418;&#19978;&#30340;&#33410;&#28857;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#39044;&#35757;&#32451;&#31574;&#30053;&#23545;&#23398;&#20064;&#33410;&#28857;&#34920;&#31034;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;&#38024;&#23545;&#22270;&#24418;&#25239;&#24615;&#35780;&#20272;&#30340;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;R3&#21487;&#20197;&#26377;&#25928;&#20943;&#23569;&#32467;&#26500;&#21644;&#22522;&#20110;&#29305;&#24449;&#30340;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#24182;&#32988;&#36807;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Benefiting from the message passing mechanism, Graph Neural Networks (GNNs) have been successful on flourish tasks over graph data. However, recent studies have shown that attackers can catastrophically degrade the performance of GNNs by maliciously modifying the graph structure. A straightforward solution to remedy this issue is to model the edge weights by learning a metric function between pairwise representations of two end nodes, which attempts to assign low weights to adversarial edges. The existing methods use either raw features or representations learned by supervised GNNs to model the edge weights. However, both strategies are faced with some immediate problems: raw features cannot represent various properties of nodes (e.g., structure information), and representations learned by supervised GNN may suffer from the poor performance of the classifier on the poisoned graph. We need representations that carry both feature information and as mush correct structure information as p
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphMLP&#30340;&#22270;&#24418;&#22686;&#24378;&#30340;MLP&#24335;&#26550;&#26500;&#65292;&#23427;&#23558;&#22270;&#24418;&#32467;&#26500;&#32435;&#20837;MLP&#27169;&#22411;&#20013;&#65292;&#20197;&#28385;&#36275;3D&#20154;&#20307;&#23039;&#24577;&#30340;&#39046;&#22495;&#29305;&#23450;&#38656;&#27714;&#65292;&#21516;&#26102;&#20801;&#35768;&#23616;&#37096;&#21644;&#20840;&#23616;&#30340;&#31354;&#38388;&#20132;&#20114;&#20316;&#29992;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#36824;&#23558;GraphMLP&#28789;&#27963;&#39640;&#25928;&#22320;&#25193;&#23637;&#21040;&#35270;&#39057;&#39046;&#22495;&#65292;&#24182;&#25104;&#21151;&#22320;&#36827;&#34892;&#20102;&#26102;&#38388;&#21160;&#21147;&#23398;&#30340;&#24314;&#27169;&#12290;</title><link>http://arxiv.org/abs/2206.06420</link><description>&lt;p&gt;
GraphMLP&#65306;&#19968;&#31181;&#29992;&#20110;3D&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;&#30340;&#22270;&#24418;MLP&#24335;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
GraphMLP: A Graph MLP-Like Architecture for 3D Human Pose Estimation. (arXiv:2206.06420v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.06420
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphMLP&#30340;&#22270;&#24418;&#22686;&#24378;&#30340;MLP&#24335;&#26550;&#26500;&#65292;&#23427;&#23558;&#22270;&#24418;&#32467;&#26500;&#32435;&#20837;MLP&#27169;&#22411;&#20013;&#65292;&#20197;&#28385;&#36275;3D&#20154;&#20307;&#23039;&#24577;&#30340;&#39046;&#22495;&#29305;&#23450;&#38656;&#27714;&#65292;&#21516;&#26102;&#20801;&#35768;&#23616;&#37096;&#21644;&#20840;&#23616;&#30340;&#31354;&#38388;&#20132;&#20114;&#20316;&#29992;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#36824;&#23558;GraphMLP&#28789;&#27963;&#39640;&#25928;&#22320;&#25193;&#23637;&#21040;&#35270;&#39057;&#39046;&#22495;&#65292;&#24182;&#25104;&#21151;&#22320;&#36827;&#34892;&#20102;&#26102;&#38388;&#21160;&#21147;&#23398;&#30340;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#27169;&#22411;&#24050;&#32463;&#23637;&#29616;&#20986;&#22312;&#27809;&#26377;&#33258;&#25105;&#27880;&#24847;&#21147;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#35270;&#35273;&#34920;&#31034;&#26041;&#38754;&#30340;&#31454;&#20105;&#24615;&#32467;&#26524;&#65292;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;MLP&#27169;&#22411;&#24182;&#19981;&#25797;&#38271;&#25429;&#25417;&#23616;&#37096;&#32454;&#33410;&#65292;&#20063;&#32570;&#20047;&#26377;&#20851;&#20154;&#20307;&#26500;&#22411;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#29992;&#20110;&#39592;&#39612;&#34920;&#31034;&#23398;&#20064;&#30340;&#24314;&#27169;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22270;&#24418;&#22686;&#24378;&#30340;MLP&#24335;&#26550;&#26500;&#65292;&#31216;&#20026;GraphMLP&#65292;&#23427;&#32467;&#21512;&#20102;MLP&#21644;&#22270;&#24418;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#22312;&#20840;&#23616;-&#23616;&#37096;-&#22270;&#24418;&#32479;&#19968;&#26550;&#26500;&#20013;&#29992;&#20110;3D&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;&#12290;GraphMLP&#23558;&#20154;&#20307;&#30340;&#22270;&#24418;&#32467;&#26500;&#32435;&#20837;MLP&#27169;&#22411;&#20013;&#65292;&#20197;&#28385;&#36275;3D&#20154;&#20307;&#23039;&#24577;&#30340;&#39046;&#22495;&#29305;&#23450;&#38656;&#27714;&#65292;&#21516;&#26102;&#20801;&#35768;&#23616;&#37096;&#21644;&#20840;&#23616;&#30340;&#31354;&#38388;&#20132;&#20114;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;GraphMLP&#28789;&#27963;&#39640;&#25928;&#22320;&#25193;&#23637;&#21040;&#35270;&#39057;&#39046;&#22495;&#65292;&#24182;&#23637;&#31034;&#20102;&#21487;&#20197;&#20197;&#21487;&#24573;&#30053;&#30340;&#35745;&#31639;&#20195;&#20215;&#26469;&#26377;&#25928;&#22320;&#24314;&#27169;&#22797;&#26434;&#30340;&#26102;&#38388;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern multi-layer perceptron (MLP) models have shown competitive results in learning visual representations without self-attention. However, existing MLP models are not good at capturing local details and lack prior knowledge of human body configurations, which limits their modeling power for skeletal representation learning. To address these issues, we propose a simple yet effective graph-reinforced MLP-Like architecture, named GraphMLP, that combines MLPs and graph convolutional networks (GCNs) in a global-local-graphical unified architecture for 3D human pose estimation. GraphMLP incorporates the graph structure of human bodies into an MLP model to meet the domain-specific demand of the 3D human pose, while allowing for both local and global spatial interactions. Furthermore, we propose to flexibly and efficiently extend the GraphMLP to the video domain and show that complex temporal dynamics can be effectively modeled in a simple way with negligible computational cost gains in the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#26465;&#20214;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#20351;&#24471;&#36229;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#26799;&#24230;&#19979;&#38477;&#30340;&#25910;&#25947;&#36895;&#24230;&#24674;&#22797;&#21040;&#32447;&#24615;&#65292;&#24182;&#22312;&#20445;&#35777;&#20840;&#23616;&#26368;&#20248;&#24615;&#35777;&#26126;&#26377;&#25928;&#30340;&#21516;&#26102;&#20445;&#25345;&#20302;&#24265;&#30340;&#35745;&#31639;&#20195;&#20215;&#65292;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#24378;&#20984;&#30340;&#20195;&#20215;&#20989;&#25968; $\phi$&#12290;</title><link>http://arxiv.org/abs/2206.03345</link><description>&lt;p&gt;
&#38024;&#23545;&#36229;&#21442;&#25968;&#21270;&#30340;&#38750;&#20984;Burer-Monteiro&#20998;&#35299;&#30340;&#39044;&#26465;&#20214;&#26799;&#24230;&#19979;&#38477;&#19982;&#20840;&#23616;&#26368;&#20248;&#24615;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
Preconditioned Gradient Descent for Overparameterized Nonconvex Burer--Monteiro Factorization with Global Optimality Certification. (arXiv:2206.03345v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.03345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#26465;&#20214;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#20351;&#24471;&#36229;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#26799;&#24230;&#19979;&#38477;&#30340;&#25910;&#25947;&#36895;&#24230;&#24674;&#22797;&#21040;&#32447;&#24615;&#65292;&#24182;&#22312;&#20445;&#35777;&#20840;&#23616;&#26368;&#20248;&#24615;&#35777;&#26126;&#26377;&#25928;&#30340;&#21516;&#26102;&#20445;&#25345;&#20302;&#24265;&#30340;&#35745;&#31639;&#20195;&#20215;&#65292;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#24378;&#20984;&#30340;&#20195;&#20215;&#20989;&#25968; $\phi$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#38750;&#20984;&#20989;&#25968;$f(X)=\phi(XX^{T})$&#30340;&#26041;&#27861;&#65292;&#20854;&#20013; $\phi$&#26159;&#19968;&#20010;&#24179;&#28369;&#20984;&#30340;$n\times n$&#30697;&#38453;&#19978;&#19979;&#25991;&#30340;&#20195;&#20215;&#20989;&#25968;&#12290;&#34429;&#28982;&#20165;&#26377;&#20108;&#38454;&#20572;&#30041;&#28857;&#21487;&#20197;&#22312;&#21512;&#29702;&#26102;&#38388;&#20869;&#34987;&#35777;&#26126;&#25214;&#21040;&#65292;&#20294;&#22914;&#26524; $X$ &#30340;&#31209;&#32570;&#22833;&#65292;&#37027;&#20040;&#23427;&#30340;&#31209;&#32570;&#22833;&#23558;&#35777;&#26126;&#23427;&#26159;&#20840;&#23616;&#26368;&#20248;&#30340;&#12290;&#36825;&#31181;&#35748;&#35777;&#20840;&#23616;&#26368;&#20248;&#24615;&#30340;&#26041;&#27861;&#24517;&#28982;&#38656;&#35201;&#24403;&#21069;&#36845;&#20195;$X$&#30340;&#25628;&#32034;&#31209; $r$ &#36229;&#36807;&#20840;&#23616;&#26368;&#23567;&#21270;&#22120;$X^{\star}$ &#30340;&#31209;$r^{\star}$&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36229;&#21442;&#25968;&#21270;&#26174;&#33879;&#20943;&#24930;&#20102;&#26799;&#24230;&#19979;&#38477;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#20174; $r=r^{\star}$ &#26102;&#30340;&#32447;&#24615;&#36895;&#24230;&#38477;&#20026; $r&gt;r^{\star}$ &#26102;&#30340;&#20122;&#32447;&#24615;&#36895;&#24230;&#65292;&#21363;&#20351; $\phi$ &#26159;&#24378;&#20984;&#30340;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24265;&#20215;&#30340;&#39044;&#26465;&#20214;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#23558;&#36229;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#26799;&#24230;&#19979;&#38477;&#30340;&#25910;&#25947;&#36895;&#24230;&#24674;&#22797;&#21040;&#32447;&#24615;&#65292;&#21516;&#26102;&#20445;&#35777;&#20840;&#23616;&#26368;&#20248;&#24615;&#35777;&#26126;&#20381;&#26087;&#26377;&#25928;&#12290;&#36825;&#31181;&#26041;&#27861;&#21482;&#38656;&#35201;&#36827;&#34892;&#31616;&#21333;&#30340;&#30697;&#38453;&#20056;&#27861;&#21644;&#27714;&#36870;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#24378;&#20984;&#30340;$&#966;$&#12290;&#25105;&#20204;&#36890;&#36807;&#20223;&#30495;&#23454;&#39564;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#29616;&#23454;&#24212;&#29992;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider using gradient descent to minimize the nonconvex function $f(X)=\phi(XX^{T})$ over an $n\times r$ factor matrix $X$, in which $\phi$ is an underlying smooth convex cost function defined over $n\times n$ matrices. While only a second-order stationary point $X$ can be provably found in reasonable time, if $X$ is additionally rank deficient, then its rank deficiency certifies it as being globally optimal. This way of certifying global optimality necessarily requires the search rank $r$ of the current iterate $X$ to be overparameterized with respect to the rank $r^{\star}$ of the global minimizer $X^{\star}$. Unfortunately, overparameterization significantly slows down the convergence of gradient descent, from a linear rate with $r=r^{\star}$ to a sublinear rate when $r&gt;r^{\star}$, even when $\phi$ is strongly convex. In this paper, we propose an inexpensive preconditioner that restores the convergence rate of gradient descent back to linear in the overparameterized case, while
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25299;&#25169;&#28145;&#24230;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#21547;&#32452;&#21512;&#22797;&#21512;&#20307;&#36825;&#19968;&#26032;&#22411;&#25299;&#25169;&#22495;&#12290;&#32452;&#21512;&#22797;&#21512;&#20307;&#32467;&#21512;&#20102;&#36229;&#22270;&#21644;&#32990;&#33108;&#22797;&#21512;&#20307;&#30340;&#20248;&#28857;&#65292;&#20801;&#35768;&#26500;&#24314;&#20998;&#23618;&#39640;&#38454;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2206.00606</link><description>&lt;p&gt;
&#25299;&#25169;&#28145;&#24230;&#23398;&#20064;&#65306;&#36229;&#36234;&#22270;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Topological Deep Learning: Going Beyond Graph Data. (arXiv:2206.00606v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.00606
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25299;&#25169;&#28145;&#24230;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#21547;&#32452;&#21512;&#22797;&#21512;&#20307;&#36825;&#19968;&#26032;&#22411;&#25299;&#25169;&#22495;&#12290;&#32452;&#21512;&#22797;&#21512;&#20307;&#32467;&#21512;&#20102;&#36229;&#22270;&#21644;&#32990;&#33108;&#22797;&#21512;&#20307;&#30340;&#20248;&#28857;&#65292;&#20801;&#35768;&#26500;&#24314;&#20998;&#23618;&#39640;&#38454;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25299;&#25169;&#28145;&#24230;&#23398;&#20064;&#26159;&#19968;&#20010;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#65292;&#19982;&#24320;&#21457;&#25903;&#25345;&#20110;&#25299;&#25169;&#22495;&#19978;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26377;&#20851;&#65292;&#20363;&#22914;&#21333;&#32431;&#22797;&#21512;&#20307;&#12289;&#32990;&#33108;&#22797;&#21512;&#20307;&#21644;&#36229;&#22270;&#12290;&#36825;&#20123;&#25299;&#25169;&#22495;&#22312;&#31185;&#23398;&#35745;&#31639;&#20013;&#24191;&#27867;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24314;&#31435;&#22312;&#26356;&#20016;&#23500;&#25968;&#25454;&#32467;&#26500;&#20043;&#19978;&#30340;&#32479;&#19968;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21253;&#25324;&#25299;&#25169;&#22495;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#32452;&#21512;&#22797;&#21512;&#20307;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#25299;&#25169;&#22495;&#12290;&#32452;&#21512;&#22797;&#21512;&#20307;&#21487;&#20197;&#30475;&#20316;&#26159;&#20445;&#25345;&#26576;&#20123;&#29702;&#24819;&#24615;&#36136;&#30340;&#22270;&#30340;&#25512;&#24191;&#12290;&#31867;&#20284;&#20110;&#36229;&#22270;&#65292;&#32452;&#21512;&#22797;&#21512;&#20307;&#23545;&#20851;&#31995;&#38598;&#21512;&#19981;&#26045;&#21152;&#20219;&#20309;&#32422;&#26463;&#12290;&#27492;&#22806;&#65292;&#32452;&#21512;&#22797;&#21512;&#20307;&#20801;&#35768;&#26500;&#24314;&#20998;&#23618;&#39640;&#38454;&#20851;&#31995;&#65292;&#31867;&#20284;&#20110;&#21333;&#32431;&#21644;&#32990;&#33108;&#22797;&#21512;&#20307;&#20013;&#30340;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#32452;&#21512;&#22797;&#21512;&#20307;&#25512;&#24191;&#24182;&#32467;&#21512;&#20102;&#36229;&#22270;&#21644;&#32990;&#33108;&#22797;&#21512;&#20307;&#30340;&#26377;&#29992;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Topological deep learning is a rapidly growing field that pertains to the development of deep learning models for data supported on topological domains such as simplicial complexes, cell complexes, and hypergraphs, which generalize many domains encountered in scientific computations. In this paper, we present a unifying deep learning framework built upon a richer data structure that includes widely adopted topological domains.  Specifically, we first introduce combinatorial complexes, a novel type of topological domain. Combinatorial complexes can be seen as generalizations of graphs that maintain certain desirable properties. Similar to hypergraphs, combinatorial complexes impose no constraints on the set of relations. In addition, combinatorial complexes permit the construction of hierarchical higher-order relations, analogous to those found in simplicial and cell complexes. Thus, combinatorial complexes generalize and combine useful traits of both hypergraphs and cell complexes, whi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Wasserstein&#26799;&#24230;&#27969;&#30340;&#21464;&#20998;&#25512;&#26029;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#39640;&#26031;&#25110;&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;&#24182;&#22312;&#22788;&#29702;&#23545;&#25968;&#20985; $\pi$ &#26102;&#20855;&#26377;&#24378;&#22823;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2205.15902</link><description>&lt;p&gt;
&#22522;&#20110;Wasserstein&#26799;&#24230;&#27969;&#30340;&#21464;&#20998;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Variational inference via Wasserstein gradient flows. (arXiv:2205.15902v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.15902
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Wasserstein&#26799;&#24230;&#27969;&#30340;&#21464;&#20998;&#25512;&#26029;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#39640;&#26031;&#25110;&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;&#24182;&#22312;&#22788;&#29702;&#23545;&#25968;&#20985; $\pi$ &#26102;&#20855;&#26377;&#24378;&#22823;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931; (MCMC) &#26041;&#27861;&#19968;&#36215;&#65292;&#21464;&#20998;&#25512;&#26029;&#65288;VI&#65289;&#24050;&#32463;&#25104;&#20026;&#22823;&#35268;&#27169;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#20013;&#24515;&#35745;&#31639;&#26041;&#27861;&#12290;VI &#19981;&#26159;&#20174;&#30495;&#23454;&#21518;&#39564; $\pi$ &#20013;&#36827;&#34892;&#37319;&#26679;&#65292;&#32780;&#26159;&#26088;&#22312;&#29983;&#25104;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#36817;&#20284; $\hat \pi$&#65292;&#20351;&#24471;&#25688;&#35201;&#32479;&#35745;&#37327;&#26131;&#20110;&#35745;&#31639;&#12290;&#28982;&#32780;&#65292;&#19982;&#24191;&#20026;&#30740;&#31350;&#30340; MCMC &#26041;&#27861;&#19981;&#21516;&#65292;VI &#30340;&#31639;&#27861;&#20445;&#35777;&#20173;&#28982;&#30456;&#23545;&#36739;&#23569;&#34987;&#29702;&#35299;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#39640;&#26031;&#25110;&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;&#30340;&#21464;&#20998;&#25512;&#26029;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#22522;&#20110;&#39640;&#26031;&#27979;&#24230;&#30340;Bures-Wasserstein &#31354;&#38388;&#19978;&#30340;&#26799;&#24230;&#27969;&#29702;&#35770;&#12290;&#24403; $\pi$ &#26159;&#23545;&#25968;&#20985;&#30340;&#26102;&#20505;&#65292;&#19982;MCMC&#31867;&#20284;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#24378;&#22823;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Along with Markov chain Monte Carlo (MCMC) methods, variational inference (VI) has emerged as a central computational approach to large-scale Bayesian inference. Rather than sampling from the true posterior $\pi$, VI aims at producing a simple but effective approximation $\hat \pi$ to $\pi$ for which summary statistics are easy to compute. However, unlike the well-studied MCMC methodology, algorithmic guarantees for VI are still relatively less well-understood. In this work, we propose principled methods for VI, in which $\hat \pi$ is taken to be a Gaussian or a mixture of Gaussians, which rest upon the theory of gradient flows on the Bures--Wasserstein space of Gaussian measures. Akin to MCMC, it comes with strong theoretical guarantees when $\pi$ is log-concave.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#24863;&#30693;&#29305;&#24449;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;MMD&#65288;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#65289;&#26469;&#25552;&#39640;&#24046;&#20998;&#38544;&#31169;&#22270;&#20687;&#29983;&#25104;&#30340;&#24615;&#33021;&#65292;&#24182;&#25104;&#21151;&#22320;&#29983;&#25104;&#20102;CIFAR10&#32423;&#21035;&#30340;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2205.12900</link><description>&lt;p&gt;
&#39044;&#20808;&#35757;&#32451;&#30340;&#24863;&#30693;&#29305;&#24449;&#25552;&#39640;&#24046;&#20998;&#38544;&#31169;&#22270;&#20687;&#29983;&#25104;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Pre-trained Perceptual Features Improve Differentially Private Image Generation. (arXiv:2205.12900v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.12900
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#24863;&#30693;&#29305;&#24449;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;MMD&#65288;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#65289;&#26469;&#25552;&#39640;&#24046;&#20998;&#38544;&#31169;&#22270;&#20687;&#29983;&#25104;&#30340;&#24615;&#33021;&#65292;&#24182;&#25104;&#21151;&#22320;&#29983;&#25104;&#20102;CIFAR10&#32423;&#21035;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;DP-SGD&#65289;&#36827;&#34892;&#20013;&#31561;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#30340;&#35757;&#32451;&#38750;&#24120;&#22256;&#38590;&#65306;&#20026;&#20102;&#20445;&#25345;&#21512;&#29702;&#30340;&#38544;&#31169;&#27700;&#24179;&#25152;&#38656;&#30340;&#22122;&#22768;&#27700;&#24179;&#36807;&#22823;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#24314;&#35758;&#21033;&#29992;&#20449;&#24687;&#20016;&#23500;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#33391;&#22909;&#30456;&#20851;&#34920;&#24449;&#65292;&#28982;&#21518;&#23398;&#20064;&#20351;&#29992;&#35813;&#34920;&#24449;&#27169;&#22411;&#21270;&#31169;&#26377;&#25968;&#25454;&#12290;&#29305;&#21035;&#30340;&#65292;&#25105;&#20204;&#20351;&#29992;&#20174;&#20844;&#20849;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#30340;&#24863;&#30693;&#29305;&#24449;&#30340;&#26680;&#20989;&#25968;&#65292;&#26368;&#23567;&#21270;&#31169;&#26377;&#30446;&#26631;&#25968;&#25454;&#19982;&#29983;&#25104;&#22120;&#20998;&#24067;&#20043;&#38388;&#30340;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#65288;MMD&#65289;&#12290;&#20351;&#29992;MMD&#65292;&#25105;&#20204;&#21487;&#20197;&#19968;&#27425;&#24615;&#23545;&#25968;&#25454;&#30456;&#20851;&#39033;&#36827;&#34892;&#38544;&#31169;&#22788;&#29702;&#65292;&#32780;&#26080;&#38656;&#20687;DP-SGD&#19968;&#26679;&#22312;&#20248;&#21270;&#27599;&#19968;&#27493;&#20013;&#24341;&#20837;&#22122;&#22768;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#29983;&#25104;CIFAR10&#32423;&#21035;&#30340;&#22270;&#20687;&#65292;&#20854; $\epsilon \approx 2$&#65292;&#25429;&#25417;&#20102;&#20998;&#24067;&#20013;&#30340;&#29420;&#29305;&#29305;&#24449;&#65292;&#36828;&#36828;&#36229;&#36807;&#24403;&#21069;&#30340;&#25216;&#26415;&#27700;&#24179;&#65292;&#20027;&#35201;&#38598;&#20013;&#20110;&#25968;&#25454;&#38598;&#65292;&#22914;MNIST&#21644;FashionMNIST &#20197;&#36739;&#22823;&#30340; $\epsilon$&#12290;
&lt;/p&gt;
&lt;p&gt;
Training even moderately-sized generative models with differentially-private stochastic gradient descent (DP-SGD) is difficult: the required level of noise for reasonable levels of privacy is simply too large. We advocate instead building off a good, relevant representation on an informative public dataset, then learning to model the private data with that representation. In particular, we minimize the maximum mean discrepancy (MMD) between private target data and a generator's distribution, using a kernel based on perceptual features learned from a public dataset. With the MMD, we can simply privatize the data-dependent term once and for all, rather than introducing noise at each step of optimization as in DP-SGD. Our algorithm allows us to generate CIFAR10-level images with $\epsilon \approx 2$ which capture distinctive features in the distribution, far surpassing the current state of the art, which mostly focuses on datasets such as MNIST and FashionMNIST at a large $\epsilon \appro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20851;&#27880;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#20844;&#24179;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20808;&#36827;&#30340;&#20844;&#24179;&#23398;&#20064;&#26041;&#27861;&#21487;&#33021;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#35825;&#23548;&#27495;&#35270;&#12290;&#20026;&#20102;&#38450;&#27490;&#31639;&#27861;&#31995;&#32479;&#20013;&#30340;&#27495;&#35270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#20351;&#21463;&#20445;&#25252;&#23646;&#24615;&#23545;&#31995;&#32479;&#36755;&#20986;&#30340;&#24433;&#21709;&#26080;&#25928;&#65292;&#21516;&#26102;&#20445;&#30041;&#20854;&#20182;&#29305;&#24449;&#30340;&#24433;&#21709;&#21147;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#38450;&#27490;&#30452;&#25509;&#27495;&#35270;&#65292;&#24182;&#20943;&#36731;&#21508;&#31181;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2204.02947</link><description>&lt;p&gt;
&#22312;&#30417;&#30563;&#23398;&#20064;&#20013;&#23233;&#25509;&#20844;&#24179;&#24615;&#19982;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Marrying Fairness and Explainability in Supervised Learning. (arXiv:2204.02947v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.02947
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#20844;&#24179;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20808;&#36827;&#30340;&#20844;&#24179;&#23398;&#20064;&#26041;&#27861;&#21487;&#33021;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#35825;&#23548;&#27495;&#35270;&#12290;&#20026;&#20102;&#38450;&#27490;&#31639;&#27861;&#31995;&#32479;&#20013;&#30340;&#27495;&#35270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#20351;&#21463;&#20445;&#25252;&#23646;&#24615;&#23545;&#31995;&#32479;&#36755;&#20986;&#30340;&#24433;&#21709;&#26080;&#25928;&#65292;&#21516;&#26102;&#20445;&#30041;&#20854;&#20182;&#29305;&#24449;&#30340;&#24433;&#21709;&#21147;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#38450;&#27490;&#30452;&#25509;&#27495;&#35270;&#65292;&#24182;&#20943;&#36731;&#21508;&#31181;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36741;&#21161;&#20154;&#31867;&#20915;&#31574;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21487;&#33021;&#20250;&#26080;&#24847;&#20013;&#27495;&#35270;&#26576;&#20123;&#21463;&#20445;&#25252;&#30340;&#32676;&#20307;&#12290;&#25105;&#20204;&#23558;&#30452;&#25509;&#27495;&#35270;&#24418;&#24335;&#21270;&#20026;&#21463;&#20445;&#25252;&#23646;&#24615;&#23545;&#20915;&#31574;&#30340;&#30452;&#25509;&#22240;&#26524;&#24433;&#21709;&#65292;&#32780;&#23558;&#38388;&#25509;&#27495;&#35270;&#24418;&#24335;&#21270;&#20026;&#19982;&#21463;&#20445;&#25252;&#23646;&#24615;&#30456;&#20851;&#30340;&#38750;&#20445;&#25252;&#29305;&#24449;&#22240;&#26524;&#24433;&#21709;&#30340;&#25913;&#21464;&#12290;&#36793;&#38469;&#30452;&#25509;&#20316;&#29992;&#65288;MDE&#65289;&#21644;SHapley Additive exPlanations&#65288;SHAP&#65289;&#30340;&#37327;&#24230;&#26174;&#31034;&#65292;&#26368;&#20808;&#36827;&#30340;&#20844;&#24179;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#30340;&#20851;&#32852;&#25110;&#21453;&#21521;&#27495;&#35270;&#35825;&#23548;&#27495;&#35270;&#12290;&#20026;&#20102;&#25233;&#21046;&#31639;&#27861;&#31995;&#32479;&#20013;&#30340;&#27495;&#35270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#21463;&#20445;&#25252;&#23646;&#24615;&#23545;&#31995;&#32479;&#36755;&#20986;&#30340;&#24433;&#21709;&#26080;&#25928;&#65292;&#21516;&#26102;&#20445;&#30041;&#20854;&#20182;&#29305;&#24449;&#24433;&#21709;&#21147;&#30340;&#21518;&#22788;&#29702;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#24182;&#30740;&#31350;&#20102;&#23454;&#29616;&#36825;&#20123;&#30446;&#26631;&#30340;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#21457;&#29616;&#23427;&#20204;&#20855;&#26377;&#30456;&#23545;&#36739;&#39640;&#30340;&#27169;&#22411;&#20934;&#30830;&#24615;&#65292;&#21487;&#20197;&#38450;&#27490;&#30452;&#25509;&#27495;&#35270;&#65292;&#24182;&#20943;&#36731;&#21508;&#31181;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning algorithms that aid human decision-making may inadvertently discriminate against certain protected groups. We formalize direct discrimination as a direct causal effect of the protected attributes on the decisions, while induced discrimination as a change in the causal influence of non-protected features associated with the protected attributes. The measurements of marginal direct effect (MDE) and SHapley Additive exPlanations (SHAP) reveal that state-of-the-art fair learning methods can induce discrimination via association or reverse discrimination in synthetic and real-world datasets. To inhibit discrimination in algorithmic systems, we propose to nullify the influence of the protected attribute on the output of the system, while preserving the influence of remaining features. We introduce and study post-processing methods achieving such objectives, finding that they yield relatively high model accuracy, prevent direct discrimination, and diminishes various disparity
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22312;&#21307;&#23398;&#22270;&#20687;&#39046;&#22495;&#20013;&#29992;&#34987;&#36974;&#34109;&#30340;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#33258;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;&#35813;&#26041;&#27861;&#26174;&#33879;&#25913;&#36827;&#20102;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#21644;&#20998;&#21106;&#31561;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2203.05573</link><description>&lt;p&gt;
&#29992;&#34987;&#36974;&#34109;&#30340;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#33258;&#39044;&#35757;&#32451;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#21644;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Self Pre-training with Masked Autoencoders for Medical Image Classification and Segmentation. (arXiv:2203.05573v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.05573
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22312;&#21307;&#23398;&#22270;&#20687;&#39046;&#22495;&#20013;&#29992;&#34987;&#36974;&#34109;&#30340;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#33258;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;&#35813;&#26041;&#27861;&#26174;&#33879;&#25913;&#36827;&#20102;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#21644;&#20998;&#21106;&#31561;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30740;&#31350;&#34920;&#26126;&#65292;&#34987;&#36974;&#34109;&#30340;&#33258;&#32534;&#30721;&#22120; (MAE) &#22312;&#33258;&#28982;&#22270;&#20687;&#20998;&#26512;&#20013;&#20026;&#35270;&#35273;&#21464;&#25442;&#22120; (ViT) &#30340;&#39044;&#35757;&#32451;&#26159;&#26377;&#25928;&#30340;&#12290;&#36890;&#36807;&#20174;&#37096;&#20998;&#36974;&#34109;&#30340;&#36755;&#20837;&#20013;&#37325;&#24314;&#23436;&#25972;&#30340;&#22270;&#20687;&#65292;ViT &#32534;&#30721;&#22120;&#32858;&#21512;&#19978;&#19979;&#25991;&#20449;&#24687;&#20197;&#25512;&#26029;&#34987;&#36974;&#34109;&#30340;&#22270;&#20687;&#21306;&#22495;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#31181;&#19978;&#19979;&#25991;&#32858;&#21512;&#33021;&#21147;&#22312;&#21307;&#23398;&#22270;&#20687;&#39046;&#22495;&#23588;&#20854;&#37325;&#35201;&#65292;&#22240;&#20026;&#27599;&#20010;&#35299;&#21078;&#32467;&#26500;&#22312;&#21151;&#33021;&#21644;&#26426;&#26800;&#19978;&#37117;&#19982;&#20854;&#20182;&#32467;&#26500;&#21644;&#21306;&#22495;&#30456;&#36830;&#12290;&#30001;&#20110;&#32570;&#20047; ImageNet &#35268;&#27169;&#30340;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#22240;&#27492;&#25105;&#20204;&#30740;&#31350;&#20102;&#29992; MAE &#36827;&#34892;&#33258;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#20197;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#30446;&#26631;&#25968;&#25454;&#30340;&#35757;&#32451;&#38598;&#19978;&#23545; ViT &#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#32780;&#19981;&#26159;&#20351;&#29992;&#21478;&#19968;&#20010;&#25968;&#25454;&#38598;&#12290;&#22240;&#27492;&#65292;&#33258;&#39044;&#35757;&#32451;&#21487;&#20197;&#20351;&#26356;&#22810;&#38590;&#20197;&#33719;&#21462;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#21463;&#30410;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MAE &#33258;&#39044;&#35757;&#32451;&#26174;&#33879;&#25913;&#36827;&#20102;&#21253;&#25324;&#33016;&#37096; X &#20809;&#30142;&#30149;&#20998;&#31867;&#12289;&#33145;&#37096; CT &#22312;&#20869;&#30340;&#21508;&#31181;&#21307;&#23398;&#22270;&#20687;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Masked Autoencoder (MAE) has recently been shown to be effective in pre-training Vision Transformers (ViT) for natural image analysis. By reconstructing full images from partially masked inputs, a ViT encoder aggregates contextual information to infer masked image regions. We believe that this context aggregation ability is particularly essential to the medical image domain where each anatomical structure is functionally and mechanically connected to other structures and regions. Because there is no ImageNet-scale medical image dataset for pre-training, we investigate a self pre-training paradigm with MAE for medical image analysis tasks. Our method pre-trains a ViT on the training set of the target data instead of another dataset. Thus, self pre-training can benefit more scenarios where pre-training data is hard to acquire. Our experimental results show that MAE self pre-training markedly improves diverse medical image tasks including chest X-ray disease classification, abdominal CT m
&lt;/p&gt;</description></item><item><title>ICSML&#26159;&#19968;&#31181;&#33021;&#22815;&#22312;PLC&#19978;&#26412;&#22320;&#25191;&#34892;ML&#27169;&#22411;&#25512;&#26029;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#28040;&#38500;&#22312;&#22806;&#37096;IT&#30828;&#20214;&#19978;&#36816;&#34892;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2202.10075</link><description>&lt;p&gt;
ICSML: &#29992;&#20110;&#20351;&#29992;IEC 61131-3&#20195;&#30721;&#36827;&#34892;&#26412;&#22320;&#25512;&#26029;&#30340;&#24037;&#19994;&#25511;&#21046;&#31995;&#32479;ML&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
ICSML: Industrial Control Systems ML Framework for native inference using IEC 61131-3 code. (arXiv:2202.10075v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.10075
&lt;/p&gt;
&lt;p&gt;
ICSML&#26159;&#19968;&#31181;&#33021;&#22815;&#22312;PLC&#19978;&#26412;&#22320;&#25191;&#34892;ML&#27169;&#22411;&#25512;&#26029;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#28040;&#38500;&#22312;&#22806;&#37096;IT&#30828;&#20214;&#19978;&#36816;&#34892;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#19994;&#25511;&#21046;&#31995;&#32479;&#65288;ICS&#65289;&#22312;&#20419;&#36827;&#31532;&#22235;&#27425;&#24037;&#19994;&#38761;&#21629;&#26041;&#38754;&#21457;&#25381;&#20102;&#20652;&#21270;&#20316;&#29992;&#12290;&#20687;&#21487;&#32534;&#31243;&#36923;&#36753;&#25511;&#21046;&#22120;&#65288;PLC&#65289;&#36825;&#26679;&#30340;ICS&#35774;&#22791;&#33258;&#21160;&#21270;&#12289;&#30417;&#25511;&#21644;&#25511;&#21046;&#30528;&#24037;&#19994;&#12289;&#33021;&#28304;&#21644;&#21830;&#19994;&#29615;&#22659;&#20013;&#30340;&#20851;&#38190;&#36807;&#31243;&#12290;&#20256;&#32479;&#36816;&#33829;&#25216;&#26415;&#65288;OT&#65289;&#19982;&#20449;&#24687;&#25216;&#26415;&#65288;IT&#65289;&#30340;&#34701;&#21512;&#25171;&#24320;&#20102;&#19968;&#20010;&#26032;&#30340;&#29420;&#29305;&#23041;&#32961;&#29615;&#22659;&#12290;&#36825;&#21551;&#21457;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30340;&#38450;&#24481;&#24615;&#30740;&#31350;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#22806;&#37096;IT&#30828;&#20214;&#19978;&#36816;&#34892;&#65292;&#36825;&#24847;&#21619;&#30528;&#25104;&#26412;&#30340;&#22686;&#21152;&#21644;&#23041;&#32961;&#29615;&#22659;&#30340;&#36827;&#19968;&#27493;&#25193;&#23637;&#12290;&#20026;&#20102;&#28040;&#38500;&#36825;&#31181;&#38656;&#27714;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ICS&#26426;&#22120;&#23398;&#20064;&#25512;&#26029;&#26694;&#26550;&#65288;ICSML&#65289;&#65292;&#23427;&#20351;&#24471;&#33021;&#22815;&#22312;PLC&#19978;&#26412;&#22320;&#25191;&#34892;ML&#27169;&#22411;&#25512;&#26029;&#12290; ICSML&#26159;&#29992;IEC 61131-3&#20195;&#30721;&#23454;&#29616;&#30340;&#65292;&#24182;&#25552;&#20379;&#20102;&#20960;&#31181;&#20248;&#21270;&#26041;&#27861;&#65292;&#20197;&#32469;&#36807;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#30340;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#23427;&#36866;&#29992;&#20110;&#25152;&#26377;PLC&#65292;&#26080;&#38656;&#20379;&#24212;&#21830;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
Industrial Control Systems (ICS) have played a catalytic role in enabling the 4th Industrial Revolution. ICS devices like Programmable Logic Controllers (PLCs), automate, monitor, and control critical processes in industrial, energy, and commercial environments. The convergence of traditional Operational Technology (OT) with Information Technology (IT) has opened a new and unique threat landscape. This has inspired defense research that focuses heavily on Machine Learning (ML) based anomaly detection methods that run on external IT hardware, which means an increase in costs and the further expansion of the threat landscape. To remove this requirement, we introduce the ICS machine learning inference framework (ICSML) which enables executing ML model inference natively on the PLC. ICSML is implemented in IEC 61131-3 code and provides several optimizations to bypass the limitations imposed by the domain-specific languages. Therefore, it works on every PLC without the need for vendor suppo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#24403;&#37327;&#23376;&#27604;&#29305;&#25968;&#37327;&#36739;&#22823;&#26102;&#65292;&#22320;&#29702;&#23616;&#37096;&#21270;&#21442;&#25968;&#21270;&#37327;&#23376;&#30005;&#36335;&#30340;&#35757;&#32451;&#36827;&#20837;&#25042;&#24816;&#38454;&#27573;&#65292;&#38480;&#21046;&#20102;&#21442;&#25968;&#21464;&#21270;&#36895;&#29575;&#24182;&#20445;&#35777;&#20102;&#30456;&#24212;&#37327;&#23376;&#27169;&#22411;&#30340;&#32447;&#24615;&#36924;&#36817;&#30340;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2202.08232</link><description>&lt;p&gt;
&#37327;&#23376;&#25042;&#24816;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Quantum Lazy Training. (arXiv:2202.08232v6 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.08232
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#24403;&#37327;&#23376;&#27604;&#29305;&#25968;&#37327;&#36739;&#22823;&#26102;&#65292;&#22320;&#29702;&#23616;&#37096;&#21270;&#21442;&#25968;&#21270;&#37327;&#23376;&#30005;&#36335;&#30340;&#35757;&#32451;&#36827;&#20837;&#25042;&#24816;&#38454;&#27573;&#65292;&#38480;&#21046;&#20102;&#21442;&#25968;&#21464;&#21270;&#36895;&#29575;&#24182;&#20445;&#35777;&#20102;&#30456;&#24212;&#37327;&#23376;&#27169;&#22411;&#30340;&#32447;&#24615;&#36924;&#36817;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#36229;&#21442;&#25968;&#27169;&#22411;&#20989;&#25968;&#26102;&#65292;&#26377;&#26102;&#21442;&#25968;&#19981;&#20250;&#21457;&#29983;&#26174;&#30528;&#21464;&#21270;&#65292;&#20445;&#25345;&#25509;&#36817;&#20854;&#21021;&#22987;&#20540;&#12290;&#36825;&#31181;&#29616;&#35937;&#31216;&#20026;&#25042;&#24816;&#35757;&#32451;&#65292;&#24182;&#28608;&#21457;&#20102;&#23545;&#27169;&#22411;&#20989;&#25968;&#22312;&#21021;&#22987;&#21442;&#25968;&#21608;&#22260;&#30340;&#32447;&#24615;&#36924;&#36817;&#30340;&#32771;&#34385;&#12290;&#22312;&#25042;&#24816;&#38454;&#27573;&#65292;&#32447;&#24615;&#36924;&#36817;&#27169;&#25311;&#20102;&#21442;&#25968;&#21270;&#20989;&#25968;&#30340;&#34892;&#20026;&#65292;&#20854;&#30456;&#20851;&#20869;&#26680;&#31216;&#20026;&#20999;&#21521;&#20869;&#26680;&#65292;&#25351;&#23450;&#20102;&#27169;&#22411;&#30340;&#35757;&#32451;&#24615;&#33021;&#12290;&#24050;&#30693;&#22823;&#23485;&#24230;&#65288;&#32463;&#20856;&#65289;&#31070;&#32463;&#32593;&#32476;&#30340;&#24773;&#20917;&#19979;&#20250;&#20986;&#29616;&#25042;&#24816;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#23637;&#31034;&#20102;&#22312;&#37327;&#23376;&#27604;&#29305;&#25968;&#37327;&#36739;&#22823;&#26102;&#65292;&#22320;&#29702;&#23616;&#37096;&#21270;&#21442;&#25968;&#21270;&#37327;&#23376;&#30005;&#36335;&#30340;&#35757;&#32451;&#36827;&#20837;&#25042;&#24816;&#38454;&#27573;&#12290;&#26356;&#20934;&#30830;&#22320;&#35828;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#22320;&#29702;&#23616;&#37096;&#21270;&#21442;&#25968;&#21270;&#37327;&#23376;&#30005;&#36335;&#21442;&#25968;&#21464;&#21270;&#36895;&#29575;&#30340;&#38480;&#21046;&#65292;&#20197;&#21450;&#20854;&#30456;&#20851;&#37327;&#23376;&#27169;&#22411;&#30340;&#32447;&#24615;&#36924;&#36817;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the training of over-parameterized model functions via gradient descent, sometimes the parameters do not change significantly and remain close to their initial values. This phenomenon is called lazy training, and motivates consideration of the linear approximation of the model function around the initial parameters. In the lazy regime, this linear approximation imitates the behavior of the parameterized function whose associated kernel, called the tangent kernel, specifies the training performance of the model. Lazy training is known to occur in the case of (classical) neural networks with large widths. In this paper, we show that the training of geometrically local parameterized quantum circuits enters the lazy regime for large numbers of qubits. More precisely, we prove bounds on the rate of changes of the parameters of such a geometrically local parameterized quantum circuit in the training process, and on the precision of the linear approximation of the associated quantum model 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20174;&#33258;&#28982;&#22478;&#24066;&#36947;&#36335;&#20132;&#36890;&#25968;&#25454;&#20013;&#25552;&#21462;&#30456;&#20851;&#22330;&#26223;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#32422;30&#23567;&#26102;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#33258;&#21160;&#35782;&#21035;&#30456;&#20851;&#27979;&#35797;&#22330;&#26223;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2202.06608</link><description>&lt;p&gt;
&#38754;&#21521;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#26080;&#30417;&#30563;&#27979;&#35797;&#22330;&#26223;&#25552;&#21462;&#26041;&#27861;&#8212;&#8212;&#22522;&#20110;&#33258;&#28982;&#22478;&#24066;&#36947;&#36335;&#20132;&#36890;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Toward Unsupervised Test Scenario Extraction for Automated Driving Systems from Urban Naturalistic Road Traffic Data. (arXiv:2202.06608v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.06608
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20174;&#33258;&#28982;&#22478;&#24066;&#36947;&#36335;&#20132;&#36890;&#25968;&#25454;&#20013;&#25552;&#21462;&#30456;&#20851;&#22330;&#26223;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#32422;30&#23567;&#26102;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#33258;&#21160;&#35782;&#21035;&#30456;&#20851;&#27979;&#35797;&#22330;&#26223;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22330;&#26223;&#30340;&#27979;&#35797;&#26159;&#35299;&#20915;&#37197;&#22791;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#36710;&#36742;&#23433;&#20840;&#34892;&#20026;&#39564;&#35777;&#25361;&#25112;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#30001;&#20110;&#22312;&#30495;&#23454;&#36947;&#36335;&#20132;&#36890;&#20013;&#21487;&#20197;&#22312;&#29702;&#35770;&#19978;&#21457;&#29983;&#26080;&#38480;&#25968;&#37327;&#30340;&#20855;&#20307;&#22330;&#26223;&#65292;&#22240;&#27492;&#20174;&#23433;&#20840;&#30456;&#20851;&#34892;&#20026;&#30340;&#35282;&#24230;&#25552;&#21462;&#30456;&#20851;&#22330;&#26223;&#26159;&#25104;&#21151;&#39564;&#35777;&#21644;&#39564;&#35777;&#36825;&#20123;&#31995;&#32479;&#30340;&#20851;&#38190;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#33258;&#28982;&#22478;&#24066;&#36947;&#36335;&#20132;&#36890;&#25968;&#25454;&#20013;&#26080;&#30417;&#30563;&#22320;&#25552;&#21462;&#22810;&#27169;&#24335;&#22478;&#24066;&#20132;&#36890;&#22330;&#26223;&#30340;&#26041;&#27861;&#65292;&#26368;&#23567;&#21270;&#65288;&#21487;&#33021;&#26377;&#20559;&#35265;&#30340;&#65289;&#20808;&#21069;&#19987;&#23478;&#30693;&#35782;&#30340;&#37327;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#37319;&#29992;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#65292;&#32780;&#19981;&#26159;&#36890;&#36807;&#25552;&#21462;&#20855;&#20307;&#22330;&#26223;&#21040;&#39044;&#23450;&#20041;&#21151;&#33021;&#22330;&#26223;&#30340;&#65288;&#31934;&#32454;&#30340;&#65289;&#22522;&#20110;&#35268;&#21017;&#30340;&#20998;&#37197;&#12290;&#35813;&#26041;&#27861;&#20801;&#35768;&#25506;&#32034;&#25968;&#25454;&#30340;&#26410;&#30693;&#24615;&#36136;&#21450;&#20854;&#35299;&#37322;&#20026;&#19987;&#23478;&#26410;&#33021;&#39044;&#26399;&#30340;&#27979;&#35797;&#22330;&#26223;&#12290;&#35813;&#26041;&#27861;&#22312;&#32422;30&#20010;&#23567;&#26102;&#30340;&#33258;&#28982;&#36947;&#36335;&#20132;&#36890;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#26174;&#31034;&#20986;&#33258;&#21160;&#35782;&#21035;&#30456;&#20851;&#27979;&#35797;&#22330;&#26223;&#30340;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scenario-based testing is a promising approach to solve the challenge of proving the safe behavior of vehicles equipped with automated driving systems. Since an infinite number of concrete scenarios can theoretically occur in real-world road traffic, the extraction of scenarios relevant in terms of the safety-related behavior of these systems is a key aspect for their successful verification and validation. Therefore, a method for extracting multimodal urban traffic scenarios from naturalistic road traffic data in an unsupervised manner, minimizing the amount of (potentially biased) prior expert knowledge, is proposed. Rather than an (elaborate) rule-based assignment by extracting concrete scenarios into predefined functional scenarios, the presented method deploys an unsupervised machine learning pipeline. The approach allows exploring the unknown nature of the data and their interpretation as test scenarios that experts could not have anticipated. The method is evaluated for naturali
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#39046;&#22495;&#22270;&#23545;&#39046;&#22495;&#30456;&#37051;&#24615;&#36827;&#34892;&#32534;&#30721;&#65292;&#25918;&#23485;&#20102;&#39046;&#22495;&#36866;&#24212;&#30340;&#32479;&#19968;&#23545;&#40784;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#38750;&#24179;&#20961;&#30340;&#23545;&#40784;&#65292;&#24182;&#25104;&#21151;&#22320;&#34701;&#21512;&#20102;&#39046;&#22495;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2202.03628</link><description>&lt;p&gt;
&#22270;&#20851;&#31995;&#39046;&#22495;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Graph-Relational Domain Adaptation. (arXiv:2202.03628v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.03628
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#39046;&#22495;&#22270;&#23545;&#39046;&#22495;&#30456;&#37051;&#24615;&#36827;&#34892;&#32534;&#30721;&#65292;&#25918;&#23485;&#20102;&#39046;&#22495;&#36866;&#24212;&#30340;&#32479;&#19968;&#23545;&#40784;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#38750;&#24179;&#20961;&#30340;&#23545;&#40784;&#65292;&#24182;&#25104;&#21151;&#22320;&#34701;&#21512;&#20102;&#39046;&#22495;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#24448;&#24448;&#23558;&#27599;&#20010;&#39046;&#22495;&#31561;&#21516;&#23545;&#24453;&#24182;&#23436;&#32654;&#23545;&#40784;&#65292;&#24573;&#30053;&#20102;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#25299;&#25169;&#32467;&#26500;&#65292;&#22240;&#27492;&#23545;&#20110;&#30456;&#37051;&#39046;&#22495;&#21487;&#33021;&#26377;&#21033;&#65292;&#20294;&#23545;&#20110;&#36828;&#31163;&#39046;&#22495;&#21017;&#21487;&#33021;&#26080;&#30410;&#12290;&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#39046;&#22495;&#22270;&#23545;&#39046;&#22495;&#30456;&#37051;&#24615;&#36827;&#34892;&#32534;&#30721;&#65292;&#20363;&#22914;&#20197;&#32654;&#22269;&#19981;&#21516;&#24030;&#20026;&#39046;&#22495;&#21019;&#24314;&#30340;&#29366;&#24577;&#22270;&#65292;&#20351;&#24471;&#39046;&#22495;&#21487;&#20197;&#26681;&#25454;&#22270;&#32467;&#26500;&#28789;&#27963;&#23545;&#40784;&#65292;&#20174;&#32780;&#25918;&#23485;&#20102;&#36825;&#31181;&#32479;&#19968;&#30340;&#23545;&#40784;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#22270;&#21028;&#21035;&#22120;&#23558;&#29616;&#26377;&#30340;&#23545;&#25239;&#23398;&#20064;&#26694;&#26550;&#36827;&#34892;&#20102;&#25512;&#24191;&#65292;&#24182;&#20351;&#29992;&#32534;&#30721;&#26465;&#20214;&#22270;&#23884;&#20837;&#12290;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;&#22343;&#34913;&#29366;&#24577;&#19979;&#65292;&#24403;&#22270;&#26159;&#19968;&#20010;&#22242;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20250;&#24674;&#22797;&#32463;&#20856;&#30340;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#65292;&#24182;&#20026;&#20854;&#20182;&#31867;&#22411;&#30340;&#22270;&#23454;&#29616;&#20102;&#38750;&#24179;&#20961;&#30340;&#23545;&#40784;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25104;&#21151;&#22320;&#25512;&#24191;&#32479;&#19968;&#30340;&#23545;&#40784;&#26041;&#27861;&#65292;&#24182;&#33258;&#28982;&#22320;&#34701;&#21512;&#20102;&#39046;&#22495;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing domain adaptation methods tend to treat every domain equally and align them all perfectly. Such uniform alignment ignores topological structures among different domains; therefore it may be beneficial for nearby domains, but not necessarily for distant domains. In this work, we relax such uniform alignment by using a domain graph to encode domain adjacency, e.g., a graph of states in the US with each state as a domain and each edge indicating adjacency, thereby allowing domains to align flexibly based on the graph structure. We generalize the existing adversarial learning framework with a novel graph discriminator using encoding-conditioned graph embeddings. Theoretical analysis shows that at equilibrium, our method recovers classic domain adaptation when the graph is a clique, and achieves non-trivial alignment for other types of graphs. Empirical results show that our approach successfully generalizes uniform alignment, naturally incorporates domain information represented b
&lt;/p&gt;</description></item><item><title>&#35813;&#31639;&#27861;&#22522;&#20110;&#24179;&#28369;&#30340;&#21487;&#20998;&#31163;&#24615;&#20551;&#35774;&#65292;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#24179;&#28369;&#21487;&#20998;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#65288;SSNMF&#65289;&#31639;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#25269;&#25239;&#22312;&#8216;&#32431;&#20687;&#32032;&#20551;&#35774;&#8217;&#23384;&#22312;&#30340;&#22122;&#22768;&#24178;&#25200;</title><link>http://arxiv.org/abs/2110.05528</link><description>&lt;p&gt;
&#24179;&#28369;&#21487;&#20998;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Smoothed Separable Nonnegative Matrix Factorization. (arXiv:2110.05528v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.05528
&lt;/p&gt;
&lt;p&gt;
&#35813;&#31639;&#27861;&#22522;&#20110;&#24179;&#28369;&#30340;&#21487;&#20998;&#31163;&#24615;&#20551;&#35774;&#65292;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#24179;&#28369;&#21487;&#20998;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#65288;SSNMF&#65289;&#31639;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#25269;&#25239;&#22312;&#8216;&#32431;&#20687;&#32032;&#20551;&#35774;&#8217;&#23384;&#22312;&#30340;&#22122;&#22768;&#24178;&#25200;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;--&#24179;&#28369;&#21487;&#20998;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#65288;SSNMF&#65289;&#65292;&#35813;&#31639;&#27861;&#22522;&#20110;&#19968;&#20010;&#32463;&#36807;&#24179;&#28369;&#30340;&#21487;&#20998;&#31163;&#24615;&#20551;&#35774;&#65292;&#34987;&#21046;&#23450;&#20026;&#19968;&#20010;&#20984;&#20248;&#21270;&#38382;&#39064;&#26469;&#25269;&#25239;&#22312;&#8216;&#32431;&#20687;&#32032;&#20551;&#35774;&#8217;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#22122;&#22768;&#30340;&#24178;&#25200;&#12290;&#35813;&#31639;&#27861;&#30340;&#26377;&#25928;&#23454;&#26045;&#21644;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#23427;&#21487;&#20197;&#20445;&#35777;&#22312;&#29305;&#23450;&#22122;&#22768;&#27700;&#24179;&#20869;&#25910;&#25947;&#21040;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#65292;&#19988;&#24471;&#20986;&#30495;&#23454;&#39030;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a set of data points belonging to the convex hull of a set of vertices, a key problem in linear algebra, signal processing, data analysis and machine learning is to estimate these vertices in the presence of noise. Many algorithms have been developed under the assumption that there is at least one nearby data point to each vertex; two of the most widely used ones are vertex component analysis (VCA) and the successive projection algorithm (SPA). This assumption is known as the pure-pixel assumption in blind hyperspectral unmixing, and as the separability assumption in nonnegative matrix factorization. More recently, Bhattacharyya and Kannan (ACM-SIAM Symposium on Discrete Algorithms, 2020) proposed an algorithm for learning a latent simplex (ALLS) that relies on the assumption that there is more than one nearby data point to each vertex. In that scenario, ALLS is probalistically more robust to noise than algorithms based on the separability assumption. In this paper, inspired by A
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#31995;&#32479;&#23398;&#20064;&#26426;&#21046;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#20351;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#22312;&#36830;&#32493;&#23398;&#20064;&#29615;&#22659;&#20013;&#23398;&#20064;&#26032;&#27010;&#24565;&#65292;&#24182;&#23558;&#20854;&#23398;&#21040;&#30340;&#30693;&#35782;&#25299;&#23637;&#21040;&#26032;&#39046;&#22495;&#12290;&#27492;&#27169;&#22411;&#32467;&#21512;&#22810;&#27169;&#24577;&#20998;&#24067;&#31354;&#38388;&#21644;&#20266;&#25490;&#32451;&#35760;&#24518;&#26426;&#21046;&#65292;&#21487;&#29992;&#20110;&#20811;&#26381;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;</title><link>http://arxiv.org/abs/2110.04662</link><description>&lt;p&gt;
&#22522;&#20110;&#35748;&#30693;&#30340;&#22686;&#37327;&#28418;&#31227;&#27010;&#24565;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cognitively Inspired Learning of Incremental Drifting Concepts. (arXiv:2110.04662v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.04662
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#31995;&#32479;&#23398;&#20064;&#26426;&#21046;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#20351;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#22312;&#36830;&#32493;&#23398;&#20064;&#29615;&#22659;&#20013;&#23398;&#20064;&#26032;&#27010;&#24565;&#65292;&#24182;&#23558;&#20854;&#23398;&#21040;&#30340;&#30693;&#35782;&#25299;&#23637;&#21040;&#26032;&#39046;&#22495;&#12290;&#27492;&#27169;&#22411;&#32467;&#21512;&#22810;&#27169;&#24577;&#20998;&#24067;&#31354;&#38388;&#21644;&#20266;&#25490;&#32451;&#35760;&#24518;&#26426;&#21046;&#65292;&#21487;&#29992;&#20110;&#20811;&#26381;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#19981;&#26029;&#23558;&#33258;&#24049;&#23398;&#21040;&#30340;&#30693;&#35782;&#25299;&#23637;&#21040;&#26032;&#30340;&#39046;&#22495;&#65292;&#24182;&#19988;&#22312;&#23398;&#20064;&#26032;&#30340;&#27010;&#24565;&#26102;&#19981;&#20250;&#23545;&#20197;&#21069;&#23398;&#20064;&#30340;&#32463;&#39564;&#26377;&#20219;&#20309;&#24178;&#25200;&#12290;&#30456;&#21453;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#36830;&#32493;&#30340;&#23398;&#20064;&#29615;&#22659;&#20013;&#34920;&#29616;&#19981;&#20339;&#65292;&#22240;&#20026;&#36755;&#20837;&#25968;&#25454;&#30340;&#20998;&#24067;&#20250;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#32780;&#21464;&#21270;&#12290;&#21463;&#31070;&#32463;&#31995;&#32479;&#23398;&#20064;&#26426;&#21046;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#35745;&#31639;&#27169;&#22411;&#65292;&#20351;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#22312;&#36830;&#32493;&#23398;&#20064;&#29615;&#22659;&#20013;&#23398;&#20064;&#26032;&#27010;&#24565;&#65292;&#24182;&#23558;&#20854;&#23398;&#21040;&#30340;&#30693;&#35782;&#25299;&#23637;&#21040;&#26032;&#39046;&#22495;&#12290;&#25105;&#20204;&#20381;&#38752;&#24182;&#34892;&#20998;&#24067;&#22788;&#29702;&#29702;&#35770;&#65292;&#22312;&#19968;&#20010;&#22810;&#27169;&#24577;&#20998;&#24067;&#31354;&#38388;&#20013;&#65292;&#29992;&#20869;&#37096;&#25968;&#25454;&#34920;&#31034;&#22312;&#38544;&#34255;&#32593;&#32476;&#23618;&#20013;&#24314;&#27169;&#25277;&#35937;&#27010;&#24565;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#21033;&#29992;&#34917;&#20805;&#23398;&#20064;&#31995;&#32479;&#29702;&#35770;&#65292;&#36890;&#36807;&#23454;&#29616;&#20266;&#25490;&#32451;&#26469;&#20026;&#27169;&#22411;&#37197;&#22791;&#35760;&#24518;&#26426;&#21046;&#65292;&#20197;&#20811;&#26381;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#20266;&#25968;&#25454;&#28857;&#36827;&#34892;&#32463;&#39564;&#22238;&#25918;&#21644;&#30693;&#35782;&#31215;&#32047;&#65292;&#36825;&#20123;&#28857;&#23558;&#34987;&#29992;&#20316;&#26032;&#25968;&#25454;&#30340;&#35757;&#32451;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans continually expand their learned knowledge to new domains and learn new concepts without any interference with past learned experiences. In contrast, machine learning models perform poorly in a continual learning setting, where input data distribution changes over time. Inspired by the nervous system learning mechanisms, we develop a computational model that enables a deep neural network to learn new concepts and expand its learned knowledge to new domains incrementally in a continual learning setting. We rely on the Parallel Distributed Processing theory to encode abstract concepts in an embedding space in terms of a multimodal distribution. This embedding space is modeled by internal data representations in a hidden network layer. We also leverage the Complementary Learning Systems theory to equip the model with a memory mechanism to overcome catastrophic forgetting through implementing pseudo-rehearsal. Our model can generate pseudo-data points for experience replay and accum
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#35745;&#31639;&#39640;&#26031;&#20998;&#24067;&#30456;&#23545;&#20110;&#26368;&#20248;&#36755;&#36816;&#24230;&#37327;&#30340;&#37325;&#24515;&#30340;&#31639;&#27861;&#65292;&#22312;Bures-Wasserstein&#27969;&#24418;&#19978;&#35777;&#26126;&#20102;&#26032;&#30340;&#27979;&#22320;&#20984;&#24615;&#32467;&#26524;&#65292;&#25552;&#20379;&#20102;&#26356;&#24378;&#30340;&#36845;&#20195;&#25511;&#21046;&#65292;&#23454;&#29616;&#20102;&#26080;&#32500;&#25910;&#25947;&#29575;&#65292;&#21516;&#26102;&#36824;&#25552;&#20379;&#20102;&#29992;&#20110;&#36825;&#20123;&#38382;&#39064;&#30340;Riemannian GD &#30340;&#31532;&#19968;&#20010;&#25910;&#25947;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2106.08502</link><description>&lt;p&gt;
Bures-Wasserstein&#27969;&#24418;&#19978;&#30340;&#24179;&#22343;&#65306;&#26799;&#24230;&#19979;&#38477;&#30340;&#26080;&#32500;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Averaging on the Bures-Wasserstein manifold: dimension-free convergence of gradient descent. (arXiv:2106.08502v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.08502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#35745;&#31639;&#39640;&#26031;&#20998;&#24067;&#30456;&#23545;&#20110;&#26368;&#20248;&#36755;&#36816;&#24230;&#37327;&#30340;&#37325;&#24515;&#30340;&#31639;&#27861;&#65292;&#22312;Bures-Wasserstein&#27969;&#24418;&#19978;&#35777;&#26126;&#20102;&#26032;&#30340;&#27979;&#22320;&#20984;&#24615;&#32467;&#26524;&#65292;&#25552;&#20379;&#20102;&#26356;&#24378;&#30340;&#36845;&#20195;&#25511;&#21046;&#65292;&#23454;&#29616;&#20102;&#26080;&#32500;&#25910;&#25947;&#29575;&#65292;&#21516;&#26102;&#36824;&#25552;&#20379;&#20102;&#29992;&#20110;&#36825;&#20123;&#38382;&#39064;&#30340;Riemannian GD &#30340;&#31532;&#19968;&#20010;&#25910;&#25947;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#29992;&#20110;&#35745;&#31639;&#39640;&#26031;&#20998;&#24067;&#30456;&#23545;&#20110;&#26368;&#20248;&#36755;&#36816;&#24230;&#37327;&#30340;&#37325;&#24515;&#30340;&#19968;&#38454;&#20248;&#21270;&#31639;&#27861;&#12290;&#23613;&#31649;&#30446;&#26631;&#26159;&#27979;&#22320;&#19981;&#20984;&#30340;&#65292;&#20294;&#27979;&#22320;&#26799;&#24230;&#19979;&#38477;&#22312;&#32463;&#39564;&#19978;&#24555;&#36895;&#25910;&#25947;&#65292;&#23454;&#38469;&#19978;&#27604;&#35832;&#22914;&#27431;&#20960;&#37324;&#24503;&#26799;&#24230;&#19979;&#38477;&#21644;SDP&#27714;&#35299;&#22120;&#31561;&#29616;&#25104;&#26041;&#27861;&#26356;&#24555;&#12290;&#36825;&#19982;Riemannian GD&#30340;&#24050;&#30693;&#30340;&#26368;&#20339;&#29702;&#35770;&#32467;&#26524;&#24418;&#25104;&#20102;&#40092;&#26126;&#23545;&#27604;&#65292;&#21518;&#32773;&#26159;&#20197;&#25351;&#25968;&#26041;&#24335;&#20381;&#36182;&#20110;&#32500;&#24230;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#26032;&#30340;&#27979;&#22320;&#20984;&#24615;&#32467;&#26524;&#65292;&#25552;&#20379;&#20102;&#26356;&#24378;&#30340;&#36845;&#20195;&#25511;&#21046;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#26080;&#32500;&#25910;&#25947;&#29575;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#36824;&#20351;&#24471;&#23545;&#20004;&#31181;&#30456;&#20851;&#30340;&#24179;&#22343;&#27010;&#24565; - &#29109;&#27491;&#21017;&#21270;&#30340;&#37325;&#24515;&#21644;&#20960;&#20309;&#20013;&#20301;&#25968;&#36827;&#34892;&#20998;&#26512;&#65292;&#20026;&#36825;&#20123;&#38382;&#39064;&#30340;Riemannian GD&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#25910;&#25947;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study first-order optimization algorithms for computing the barycenter of Gaussian distributions with respect to the optimal transport metric. Although the objective is geodesically non-convex, Riemannian GD empirically converges rapidly, in fact faster than off-the-shelf methods such as Euclidean GD and SDP solvers. This stands in stark contrast to the best-known theoretical results for Riemannian GD, which depend exponentially on the dimension. In this work, we prove new geodesic convexity results which provide stronger control of the iterates, yielding a dimension-free convergence rate. Our techniques also enable the analysis of two related notions of averaging, the entropically-regularized barycenter and the geometric median, providing the first convergence guarantees for Riemannian GD for these problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#31867;&#21644;&#37327;&#21270;&#21463;&#25439;&#25968;&#25454;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#26469;&#25551;&#36848;&#21463;&#25439;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#30417;&#30563;&#33258;&#32534;&#30721;&#22120;&#30340;&#28508;&#22312;&#31354;&#38388;&#23454;&#29616;&#20998;&#31867;&#65292;&#21487;&#21516;&#26102;&#23545;&#20998;&#31867;&#21644;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#22788;&#29702;&#12290;</title><link>http://arxiv.org/abs/2105.13393</link><description>&lt;p&gt;
&#21033;&#29992;&#21322;&#30417;&#30563;&#33258;&#32534;&#30721;&#22120;&#23545;&#21463;&#25439;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Classification and Uncertainty Quantification of Corrupted Data using Semi-Supervised Autoencoders. (arXiv:2105.13393v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.13393
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#31867;&#21644;&#37327;&#21270;&#21463;&#25439;&#25968;&#25454;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#26469;&#25551;&#36848;&#21463;&#25439;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#30417;&#30563;&#33258;&#32534;&#30721;&#22120;&#30340;&#28508;&#22312;&#31354;&#38388;&#23454;&#29616;&#20998;&#31867;&#65292;&#21487;&#21516;&#26102;&#23545;&#20998;&#31867;&#21644;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#21270;&#21644;&#38750;&#21442;&#25968;&#21270;&#20998;&#31867;&#22120;&#32463;&#24120;&#19981;&#24471;&#19981;&#22788;&#29702;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#25968;&#25454;&#65292;&#20854;&#20013;&#22122;&#22768;&#12289;&#36974;&#25377;&#21644;&#27169;&#31946;&#31561;&#27745;&#26579;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#65292;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#31867;&#24378;&#28872;&#21463;&#25439;&#30340;&#25968;&#25454;&#24182;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#65292;&#23613;&#31649;&#27169;&#22411;&#20165;&#22312;&#26410;&#21463;&#25439;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;&#24213;&#23618;&#26550;&#26500;&#26159;&#19968;&#20010;&#21482;&#22312;&#26410;&#32463;&#27745;&#26579;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#21322;&#30417;&#30563;&#33258;&#32534;&#30721;&#22120;&#12290;&#25105;&#20204;&#20351;&#29992;&#35299;&#30721;&#37096;&#20998;&#20316;&#20026;&#30495;&#23454;&#25968;&#25454;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#21367;&#31215;&#12289;&#25513;&#34109;&#21644;&#21152;&#24615;&#39640;&#26031;&#22122;&#22768;&#26469;&#25551;&#36848;&#20854;&#19981;&#23436;&#32654;&#20043;&#22788;&#12290;&#36825;&#26500;&#25104;&#20102;&#19968;&#20010;&#32479;&#35745;&#25512;&#26029;&#38382;&#39064;&#65292;&#28041;&#21450;&#21040;&#24213;&#23618;&#26410;&#21463;&#27745;&#26579;&#25968;&#25454;&#30340;&#26368;&#20339;&#28508;&#22312;&#31354;&#38388;&#28608;&#27963;&#12290;&#25105;&#20204;&#20351;&#29992;&#24230;&#37327;&#39640;&#26031;&#21464;&#20998;&#25512;&#26029;&#65288;MGVI&#65289;&#26469;&#36817;&#20284;&#35299;&#20915;&#27492;&#38382;&#39064;&#12290;&#33258;&#32534;&#30721;&#22120;&#28508;&#22312;&#31354;&#38388;&#30340;&#30417;&#30563;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#30452;&#25509;&#23545;&#21463;&#25439;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#65292;&#21516;&#26102;&#20351;&#29992;&#32479;&#35745;&#25512;&#26029;&#30340;&#28508;&#22312;&#31354;&#38388;&#28608;&#27963;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parametric and non-parametric classifiers often have to deal with real-world data, where corruptions like noise, occlusions, and blur are unavoidable posing significant challenges. We present a probabilistic approach to classify strongly corrupted data and quantify uncertainty, despite the model only having been trained with uncorrupted data. A semi-supervised autoencoder trained on uncorrupted data is the underlying architecture. We use the decoding part as a generative model for realistic data and extend it by convolutions, masking, and additive Gaussian noise to describe imperfections. This constitutes a statistical inference task in terms of the optimal latent space activations of the underlying uncorrupted datum. We solve this problem approximately with Metric Gaussian Variational Inference (MGVI). The supervision of the autoencoder's latent space allows us to classify corrupted data directly under uncertainty with the statistically inferred latent space activations. Furthermore
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38543;&#26426;&#22312;&#32447;&#20984;&#20248;&#21270;&#26694;&#26550;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#38750;&#24179;&#31283;&#20122;&#39640;&#26031;&#26102;&#38388;&#24207;&#21015;&#30340;&#39044;&#27979;&#65292;&#21462;&#24471;&#20102;&#24555;&#36895;&#36895;&#29575;&#30340;&#38543;&#26426;&#36951;&#25022;&#24230;&#36793;&#30028;&#12290;</title><link>http://arxiv.org/abs/2102.00729</link><description>&lt;p&gt;
&#38543;&#26426;&#22312;&#32447;&#20984;&#20248;&#21270;&#12290;&#24212;&#29992;&#20110;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic Online Convex Optimization. Application to probabilistic time series forecasting. (arXiv:2102.00729v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.00729
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38543;&#26426;&#22312;&#32447;&#20984;&#20248;&#21270;&#26694;&#26550;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#38750;&#24179;&#31283;&#20122;&#39640;&#26031;&#26102;&#38388;&#24207;&#21015;&#30340;&#39044;&#27979;&#65292;&#21462;&#24471;&#20102;&#24555;&#36895;&#36895;&#29575;&#30340;&#38543;&#26426;&#36951;&#25022;&#24230;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#38543;&#26426;&#22312;&#32447;&#20984;&#20248;&#21270;&#26694;&#26550;&#65292;&#20197;&#33719;&#24471;&#24555;&#36895;&#36895;&#29575;&#30340;&#38543;&#26426;&#36951;&#25022;&#24230;&#36793;&#30028;&#12290;&#25105;&#20204;&#35777;&#26126;&#22312;&#32447;&#29275;&#39039;&#27493;&#21644;&#19968;&#20010;&#32553;&#25918;&#33258;&#30001;&#30340;&#20271;&#24681;&#26031;&#22374;&#22312;&#32447;&#32858;&#21512;&#31561;&#31639;&#27861;&#22312;&#26080;&#30028;&#38543;&#26426;&#29615;&#22659;&#19979;&#23454;&#29616;&#20102;&#26368;&#20339;&#24050;&#30693;&#36895;&#29575;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#26657;&#20934;&#38750;&#24179;&#31283;&#20122;&#39640;&#26031;&#26102;&#38388;&#24207;&#21015;&#30340;&#21442;&#25968;&#27010;&#29575;&#39044;&#27979;&#22120;&#12290;&#25105;&#20204;&#30340;&#24555;&#36895;&#29575;&#38543;&#26426;&#36951;&#25022;&#24230;&#36793;&#30028;&#26159;&#20219;&#24847;&#26102;&#38388;&#26377;&#25928;&#30340;&#12290;&#25105;&#20204;&#30340;&#35777;&#26126;&#32467;&#21512;&#20102;&#33258;&#38480;&#23450;&#21644;&#27850;&#26494;&#19981;&#31561;&#24335;&#65292;&#29992;&#20110;&#38646;&#22686;&#38271;&#21644;&#27425;&#39640;&#26031;&#38543;&#26426;&#21464;&#37327;&#65292;&#20998;&#21035;&#22312;&#38543;&#26426;exp-&#20984;&#24615;&#20551;&#35774;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a general framework of stochastic online convex optimization to obtain fast-rate stochastic regret bounds. We prove that algorithms such as online newton steps and a scale-free 10 version of Bernstein online aggregation achieve best-known rates in unbounded stochastic settings. We apply our approach to calibrate parametric probabilistic forecasters of non-stationary sub-gaussian time series. Our fast-rate stochastic regret bounds are any-time valid. Our proofs combine self-bounded and Poissonnian inequalities for martingales and sub-gaussian random variables, respectively, under a stochastic exp-concavity assumption.
&lt;/p&gt;</description></item><item><title>VenoMave&#26159;&#38024;&#23545;&#35821;&#38899;&#35782;&#21035;&#30340;&#31532;&#19968;&#31181;&#35757;&#32451;&#26399;&#27602;&#21270;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#21482;&#33021;&#25805;&#32437;&#19968;&#23567;&#37096;&#20998;&#35757;&#32451;&#25968;&#25454;&#32780;&#19981;&#20250;&#22312;&#36816;&#34892;&#26102;&#25913;&#21464;&#30446;&#26631;&#38899;&#39057;&#27874;&#24418;&#12290;</title><link>http://arxiv.org/abs/2010.10682</link><description>&lt;p&gt;
VenoMave&#65306;&#38754;&#21521;&#35821;&#38899;&#35782;&#21035;&#30340;&#26377;&#38024;&#23545;&#24615;&#27602;&#21270;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
VenoMave: Targeted Poisoning Against Speech Recognition. (arXiv:2010.10682v3 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2010.10682
&lt;/p&gt;
&lt;p&gt;
VenoMave&#26159;&#38024;&#23545;&#35821;&#38899;&#35782;&#21035;&#30340;&#31532;&#19968;&#31181;&#35757;&#32451;&#26399;&#27602;&#21270;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#21482;&#33021;&#25805;&#32437;&#19968;&#23567;&#37096;&#20998;&#35757;&#32451;&#25968;&#25454;&#32780;&#19981;&#20250;&#22312;&#36816;&#34892;&#26102;&#25913;&#21464;&#30446;&#26631;&#38899;&#39057;&#27874;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20294;&#20173;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#24178;&#25200;&#12290;&#19982;&#26631;&#20934;&#30340;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#30456;&#27604;&#65292;&#36825;&#20123;&#25915;&#20987;&#26356;&#21152;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#23588;&#20854;&#26159;&#22240;&#20026;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#30340;&#36755;&#20837;&#26159;&#21253;&#21547;&#35821;&#38899;&#30340;&#22768;&#23398;&#21644;&#35821;&#35328;&#23646;&#24615;&#30340;&#26102;&#38388;&#24207;&#21015;&#12290;&#25552;&#21462;&#25152;&#26377;&#35782;&#21035;&#30456;&#20851;&#20449;&#24687;&#38656;&#35201;&#26356;&#22797;&#26434;&#30340;&#27969;&#31243;&#21644;&#19968;&#32452;&#19987;&#38376;&#30340;&#32452;&#20214;&#12290;&#22240;&#27492;&#65292;&#25915;&#20987;&#32773;&#38656;&#35201;&#32771;&#34385;&#25972;&#20010;&#27969;&#31243;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;VenoMave&#65292;&#36825;&#26159;&#38024;&#23545;&#35821;&#38899;&#35782;&#21035;&#30340;&#31532;&#19968;&#31181;&#35757;&#32451;&#26399;&#27602;&#21270;&#25915;&#20987;&#12290;&#19982;&#20027;&#35201;&#30740;&#31350;&#35268;&#36991;&#25915;&#20987;&#30456;&#20284;&#65292;&#25105;&#20204;&#36861;&#27714;&#30456;&#21516;&#30340;&#30446;&#26631;&#65306;&#23558;&#31995;&#32479;&#24341;&#23548;&#21040;&#30446;&#26631;&#38899;&#39057;&#27874;&#24418;&#30340;&#19981;&#27491;&#30830;&#21644;&#25915;&#20987;&#32773;&#36873;&#25321;&#30340;&#36716;&#24405;&#12290;&#28982;&#32780;&#65292;&#19982;&#35268;&#36991;&#25915;&#20987;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#20551;&#23450;&#25915;&#20987;&#32773;&#21482;&#33021;&#25805;&#32437;&#19968;&#23567;&#37096;&#20998;&#35757;&#32451;&#25968;&#25454;&#65292;&#32780;&#19981;&#20250;&#22312;&#36816;&#34892;&#26102;&#25913;&#21464;&#30446;&#26631;&#38899;&#39057;&#27874;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite remarkable improvements, automatic speech recognition is susceptible to adversarial perturbations. Compared to standard machine learning architectures, these attacks are significantly more challenging, especially since the inputs to a speech recognition system are time series that contain both acoustic and linguistic properties of speech. Extracting all recognition-relevant information requires more complex pipelines and an ensemble of specialized components. Consequently, an attacker needs to consider the entire pipeline. In this paper, we present VENOMAVE, the first training-time poisoning attack against speech recognition. Similar to the predominantly studied evasion attacks, we pursue the same goal: leading the system to an incorrect and attacker-chosen transcription of a target audio waveform. In contrast to evasion attacks, however, we assume that the attacker can only manipulate a small part of the training data without altering the target audio waveform at runtime. We e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32447;&#24615;&#27748;&#26222;&#26862;&#25277;&#26679;&#30340;&#39057;&#29575;&#21518;&#24724;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#21518;&#39564;&#26041;&#24046;&#33192;&#32960;&#26159;&#24517;&#38656;&#30340;&#65292;&#24182;&#30830;&#23450;&#20102;&#39057;&#29575;&#21518;&#24724;&#30340;&#26368;&#20302;&#19979;&#38480;&#20026;$\widetilde{\mathcal{O}}(d\sqrt{dT})$ &#12290;</title><link>http://arxiv.org/abs/2006.06790</link><description>&lt;p&gt;
&#20851;&#20110;&#32447;&#24615;&#27748;&#26222;&#26862;&#25277;&#26679;&#30340;&#39057;&#29575;&#21518;&#24724;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
On Frequentist Regret of Linear Thompson Sampling. (arXiv:2006.06790v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2006.06790
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32447;&#24615;&#27748;&#26222;&#26862;&#25277;&#26679;&#30340;&#39057;&#29575;&#21518;&#24724;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#21518;&#39564;&#26041;&#24046;&#33192;&#32960;&#26159;&#24517;&#38656;&#30340;&#65292;&#24182;&#30830;&#23450;&#20102;&#39057;&#29575;&#21518;&#24724;&#30340;&#26368;&#20302;&#19979;&#38480;&#20026;$\widetilde{\mathcal{O}}(d\sqrt{dT})$ &#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#38543;&#26426;&#32447;&#24615;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#20854;&#20013;&#20915;&#31574;&#32773;&#20174;&#21487;&#33021;&#26102;&#21464;&#30340;$\mathbb{R}^d$&#21521;&#37327;&#38598;&#20013;&#36873;&#25321;&#34892;&#21160;&#24182;&#33719;&#24471;&#22122;&#22768;&#22870;&#21169;&#12290;&#30446;&#26631;&#26159;&#22312;&#19968;&#31995;&#21015;$T$&#20010;&#20915;&#31574;&#20013;&#26368;&#23567;&#21270;&#21518;&#24724;&#65292;&#21363;&#20915;&#31574;&#32773;&#30340;&#32047;&#31215;&#39044;&#26399;&#22870;&#21169;&#19982;&#33021;&#22815;&#35775;&#38382;&#27599;&#20010;&#34892;&#21160;&#39044;&#26399;&#22870;&#21169;&#30340;&#31070;&#35861;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#32447;&#24615;&#27748;&#26222;&#26862;&#25277;&#26679;(LinTS)&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#36125;&#21494;&#26031;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#20854;&#36125;&#21494;&#26031;&#21518;&#24724;&#21463;&#21040;$\widetilde{\mathcal{O}}(d\sqrt{T})$&#30340;&#30028;&#38480;&#32422;&#26463;&#65292;&#36798;&#21040;&#26497;&#23567;&#20540;&#19979;&#38480;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;LinTS&#30340;&#39057;&#29575;&#21518;&#24724;&#30028;&#38480;&#20026;$\widetilde{\mathcal{O}}(d\sqrt{dT})$&#65292;&#38656;&#35201;&#21518;&#39564;&#26041;&#24046;&#33192;&#32960;&#65292;&#24182;&#19988;&#27604;&#26368;&#20339;&#22522;&#20110;&#20048;&#35266;&#20027;&#20041;&#30340;&#31639;&#27861;&#24046;&#19968;&#20010;$\sqrt{d}$&#30340;&#22240;&#23376;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#33192;&#32960;&#26159;&#22522;&#26412;&#30340;&#65292;&#24182;&#19988;&#39057;&#29575;&#30028;&#38480;&#20026;$\widetilde{\mathcal{O}}(d\sqrt{dT})$&#26159;&#26368;&#20339;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the stochastic linear bandit problem, where a decision-maker chooses actions from possibly time-dependent sets of vectors in $\mathbb{R}^d$ and receives noisy rewards. The objective is to minimize regret, the difference between the cumulative expected reward of the decision-maker and that of an oracle with access to the expected reward of each action, over a sequence of $T$ decisions. Linear Thompson Sampling (LinTS) is a popular Bayesian heuristic, supported by theoretical analysis that shows its Bayesian regret is bounded by $\widetilde{\mathcal{O}}(d\sqrt{T})$, matching minimax lower bounds. However, previous studies demonstrate that the frequentist regret bound for LinTS is $\widetilde{\mathcal{O}}(d\sqrt{dT})$, which requires posterior variance inflation and is by a factor of $\sqrt{d}$ worse than the best optimism-based algorithms. We prove that this inflation is fundamental and that the frequentist bound of $\widetilde{\mathcal{O}}(d\sqrt{dT})$ is the best pos
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#29275;&#39039;&#31579;&#36873;&#27861;&#30340;&#26032;&#22411;Broad-Newton&#26041;&#27861;&#65292;&#23427;&#24102;&#26377;&#19968;&#20010;&#20869;&#32622;&#30340;&#36739;&#23567;&#30340;&#24037;&#20316;&#38598;&#65292;&#21487;&#29992;&#20110;&#21152;&#36895;&#35299;&#20915;&#22823;&#35268;&#27169;&#31232;&#30095;&#23398;&#20064;&#38382;&#39064;&#30340;&#19968;&#38454;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2001.10616</link><description>&lt;p&gt;
&#35770;&#29275;&#39039;&#31579;&#36873;&#27861;
&lt;/p&gt;
&lt;p&gt;
On Newton Screening. (arXiv:2001.10616v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2001.10616
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#29275;&#39039;&#31579;&#36873;&#27861;&#30340;&#26032;&#22411;Broad-Newton&#26041;&#27861;&#65292;&#23427;&#24102;&#26377;&#19968;&#20010;&#20869;&#32622;&#30340;&#36739;&#23567;&#30340;&#24037;&#20316;&#38598;&#65292;&#21487;&#29992;&#20110;&#21152;&#36895;&#35299;&#20915;&#22823;&#35268;&#27169;&#31232;&#30095;&#23398;&#20064;&#38382;&#39064;&#30340;&#19968;&#38454;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31579;&#36873;&#21644;&#24037;&#20316;&#38598;&#25216;&#26415;&#26159;&#20943;&#23567;&#20248;&#21270;&#38382;&#39064;&#35268;&#27169;&#30340;&#37325;&#35201;&#26041;&#27861;&#65292;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#21152;&#36895;&#35299;&#20915;&#22823;&#35268;&#27169;&#31232;&#30095;&#23398;&#20064;&#38382;&#39064;&#30340;&#19968;&#38454;&#26041;&#27861;&#20013;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31579;&#36873;&#26041;&#27861;&#65292;&#31216;&#20026;&#29275;&#39039;&#31579;&#36873;&#27861;&#65288;NS&#65289;&#65292;&#23427;&#26159;&#19968;&#31181;&#24102;&#26377;&#20869;&#32622;&#31579;&#36873;&#26426;&#21046;&#30340;&#24191;&#20041;&#29275;&#39039;&#26041;&#27861;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#22522;&#20110;&#31561;&#25928;KKT&#31995;&#32479;&#30340;Lasso&#27169;&#22411;&#65292;&#21033;&#29992;&#24191;&#20041;&#29275;&#39039;&#26041;&#27861;&#26469;&#27714;&#35299;KKT&#26041;&#31243;&#32452;&#12290;&#22522;&#20110;&#36825;&#20010;KKT&#31995;&#32479;&#65292;&#39318;&#20808;&#21033;&#29992;&#19978;&#19968;&#27425;&#36845;&#20195;&#29983;&#25104;&#30340;&#21407;&#22987;&#21644;&#23545;&#20598;&#21464;&#37327;&#20043;&#21644;&#30830;&#23450;&#19968;&#20010;&#20855;&#26377;&#30456;&#23545;&#36739;&#23567;&#22823;&#23567;&#30340;&#20869;&#32622;&#24037;&#20316;&#38598;&#65292;&#28982;&#21518;&#36890;&#36807;&#22312;&#24037;&#20316;&#38598;&#19978;&#27714;&#35299;&#26368;&#23567;&#20108;&#20056;&#38382;&#39064;&#26469;&#26356;&#26032;&#21407;&#22987;&#21464;&#37327;&#65292;&#24182;&#22522;&#20110;&#38381;&#24335;&#34920;&#36798;&#24335;&#26356;&#26032;&#23545;&#20598;&#21464;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#28909;&#21551;&#21160;&#31574;&#30053;&#30340;&#29275;&#39039;&#31579;&#36873;&#27861;&#30340;&#36830;&#32493;&#29256;&#26412;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;NS&#22312;&#26368;&#20248;&#25910;&#25947;&#24615;&#26041;&#38754;&#20855;&#26377;&#20248;&#24322;&#30340;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
Screening and working set techniques are important approaches to reducing the size of an optimization problem. They have been widely used in accelerating first-order methods for solving large-scale sparse learning problems. In this paper, we develop a new screening method called Newton screening (NS) which is a generalized Newton method with a built-in screening mechanism. We derive an equivalent KKT system for the Lasso and utilize a generalized Newton method to solve the KKT equations. Based on this KKT system, a built-in working set with a relatively small size is first determined using the sum of primal and dual variables generated from the previous iteration, then the primal variable is updated by solving a least-squares problem on the working set and the dual variable updated based on a closed-form expression. Moreover, we consider a sequential version of Newton screening (SNS) with a warm-start strategy. We show that NS possesses an optimal convergence property in the sense that
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#21487;&#33021;&#24102;&#26377;&#27495;&#35270;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#19988;&#33021;&#22815;&#22312;&#20844;&#24179;&#30340;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#19988;&#35813;&#26041;&#27861;&#21487;&#22312;&#28040;&#38500;&#27495;&#35270;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#65292;&#24182;&#22312;&#21463;&#20445;&#25252;&#32676;&#20307;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/1912.08189</link><description>&lt;p&gt;
&#20174;&#24102;&#26377;&#27495;&#35270;&#24615;&#36136;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning from Discriminatory Training Data. (arXiv:1912.08189v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1912.08189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#21487;&#33021;&#24102;&#26377;&#27495;&#35270;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#19988;&#33021;&#22815;&#22312;&#20844;&#24179;&#30340;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#19988;&#35813;&#26041;&#27861;&#21487;&#22312;&#28040;&#38500;&#27495;&#35270;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#65292;&#24182;&#22312;&#21463;&#20445;&#25252;&#32676;&#20307;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#30563;&#23398;&#20064;&#31995;&#32479;&#26159;&#36890;&#36807;&#21382;&#21490;&#25968;&#25454;&#35757;&#32451;&#30340;&#65292;&#22914;&#26524;&#36825;&#20123;&#25968;&#25454;&#21463;&#21040;&#27495;&#35270;&#24615;&#36136;&#30340;&#24433;&#21709;&#65292;&#37027;&#20040;&#35813;&#31995;&#32479;&#21487;&#33021;&#20250;&#22312;&#20445;&#25252;&#32452;&#20013;&#20135;&#29983;&#27495;&#35270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20844;&#24179;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21363;&#20351;&#22312;&#28508;&#22312;&#30340;&#27495;&#35270;&#24615;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#65292;&#20063;&#23558;&#22312;&#20844;&#24179;&#30340;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#36716;&#21464;&#20026;&#29305;&#23450;&#20844;&#24179;&#23398;&#20064;&#26041;&#27861;&#30340;&#24212;&#29992;&#26041;&#26696;&#12290;&#20363;&#22914;&#65292;&#28040;&#38500;&#30452;&#25509;&#27495;&#35270;&#21487;&#20197;&#34987;&#34920;&#31034;&#20026;&#29305;&#23450;&#30340;&#25968;&#25454;&#38598;&#36716;&#21464;&#38382;&#39064;&#12290;&#23545;&#20110;&#36825;&#31181;&#24773;&#20917;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#30450;&#30446;&#35757;&#32451;&#21253;&#21547;&#30452;&#25509;&#21152;&#24615;&#27495;&#35270;&#30340;&#25968;&#25454;&#38598;&#30340;&#21516;&#26102;&#65292;&#22312;&#20844;&#24179;&#25968;&#25454;&#38598;&#19978;&#21487;&#20197;&#35777;&#26126;&#26368;&#23567;&#21270;&#27169;&#22411;&#35823;&#24046;&#12290;&#35813;&#26041;&#27861;&#19982;&#29616;&#26377;&#30340;&#27861;&#24459;&#20307;&#31995;&#20860;&#23481;&#65292;&#24182;&#36890;&#36807;&#22312;&#21463;&#20445;&#25252;&#32676;&#20307;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#26469;&#35299;&#20915;&#24191;&#27867;&#35752;&#35770;&#30340;&#21463;&#20445;&#25252;&#32676;&#20307;&#20132;&#21449;&#30340;&#38382;&#39064;&#12290;&#20174;&#25216;&#26415;&#19978;&#35762;&#65292;&#35813;&#26041;&#27861;&#24212;&#29992;&#20102;&#27010;&#29575;&#24178;&#39044;&#65292;&#24182;&#20855;&#26377;&#22240;&#26524;&#21644;&#21453;&#20107;&#23454;&#20844;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised learning systems are trained using historical data and, if the data was tainted by discrimination, they may unintentionally learn to discriminate against protected groups. We propose that fair learning methods, despite training on potentially discriminatory datasets, shall perform well on fair test datasets. Such dataset shifts crystallize application scenarios for specific fair learning methods. For instance, the removal of direct discrimination can be represented as a particular dataset shift problem. For this scenario, we propose a learning method that provably minimizes model error on fair datasets, while blindly training on datasets poisoned with direct additive discrimination. The method is compatible with existing legal systems and provides a solution to the widely discussed issue of protected groups' intersectionality by striking a balance between the protected groups. Technically, the method applies probabilistic interventions, has causal and counterfactual formulat
&lt;/p&gt;</description></item></channel></rss>