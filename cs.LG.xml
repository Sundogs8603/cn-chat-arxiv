<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>CADSim&#26159;&#19968;&#20010;&#33021;&#22815;&#33258;&#21160;&#37325;&#24314;&#36710;&#36742;&#20960;&#20309;&#24418;&#29366;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37326;&#22806;&#20256;&#24863;&#22120;&#25968;&#25454;&#21644;CAD&#27169;&#22411;&#30340;&#32452;&#21512;&#65292;&#20811;&#26381;&#20102;&#31232;&#30095;&#24615;&#21644;&#22122;&#22768;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.01447</link><description>&lt;p&gt;
CADSim: &#40065;&#26834;&#19988;&#21487;&#25193;&#23637;&#30340;&#37326;&#22806;&#21487;&#25511;&#20256;&#24863;&#22120;&#27169;&#25311;&#30340;&#19977;&#32500;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
CADSim: Robust and Scalable in-the-wild 3D Reconstruction for Controllable Sensor Simulation. (arXiv:2311.01447v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01447
&lt;/p&gt;
&lt;p&gt;
CADSim&#26159;&#19968;&#20010;&#33021;&#22815;&#33258;&#21160;&#37325;&#24314;&#36710;&#36742;&#20960;&#20309;&#24418;&#29366;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37326;&#22806;&#20256;&#24863;&#22120;&#25968;&#25454;&#21644;CAD&#27169;&#22411;&#30340;&#32452;&#21512;&#65292;&#20811;&#26381;&#20102;&#31232;&#30095;&#24615;&#21644;&#22122;&#22768;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30495;&#23454;&#30340;&#27169;&#25311;&#26159;&#23454;&#29616;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#23433;&#20840;&#21644;&#21487;&#25193;&#23637;&#24320;&#21457;&#30340;&#20851;&#38190;&#12290;&#26680;&#24515;&#32452;&#20214;&#26159;&#27169;&#25311;&#20256;&#24863;&#22120;&#65292;&#20197;&#20415;&#25972;&#20010;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#33021;&#22312;&#27169;&#25311;&#20013;&#36827;&#34892;&#27979;&#35797;&#12290;&#20256;&#24863;&#22120;&#27169;&#25311;&#28041;&#21450;&#23545;&#20132;&#36890;&#21442;&#19982;&#32773;&#65288;&#22914;&#36710;&#36742;&#65289;&#36827;&#34892;&#24314;&#27169;&#65292;&#20855;&#26377;&#39640;&#36136;&#37327;&#30340;&#22806;&#35266;&#21644;&#21487;&#35843;&#25972;&#20960;&#20309;&#24418;&#29366;&#65292;&#24182;&#23454;&#26102;&#36827;&#34892;&#28210;&#26579;&#12290;&#33258;&#21160;&#39550;&#39542;&#34892;&#19994;&#36890;&#24120;&#38656;&#35201;&#33402;&#26415;&#23478;&#26469;&#26500;&#24314;&#36825;&#20123;&#20803;&#32032;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#25104;&#26412;&#39640;&#12289;&#36895;&#24230;&#24930;&#65292;&#24182;&#19988;&#21487;&#33021;&#26080;&#27861;&#21453;&#26144;&#30495;&#23454;&#24773;&#20917;&#12290;&#30456;&#21453;&#65292;&#36890;&#36807;&#20174;&#37326;&#22806;&#25910;&#38598;&#30340;&#20256;&#24863;&#22120;&#25968;&#25454;&#33258;&#21160;&#37325;&#24314;&#20803;&#32032;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#29983;&#25104;&#22810;&#26679;&#21270;&#19988;&#35206;&#30422;&#30495;&#23454;&#19990;&#30028;&#30340;&#22823;&#37327;&#20803;&#32032;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#37325;&#24314;&#26041;&#27861;&#22312;&#37326;&#22806;&#20256;&#24863;&#22120;&#25968;&#25454;&#19978;&#23384;&#22312;&#22256;&#38590;&#65292;&#20027;&#35201;&#21407;&#22240;&#26159;&#20854;&#31232;&#30095;&#24615;&#21644;&#22122;&#22768;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CADSim&#65292;&#23427;&#36890;&#36807;&#19968;&#23567;&#32452;&#19981;&#21516;&#21487;&#24494;&#28210;&#26579;&#30340;CAD&#27169;&#22411;&#65292;&#32467;&#21512;&#37096;&#20214;&#24863;&#30693;&#30340;&#30446;&#26631;&#31867;&#21035;&#20808;&#39564;&#30693;&#35782;&#65292;&#33258;&#21160;&#37325;&#24314;&#36710;&#36742;&#30340;&#20960;&#20309;&#24418;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;
Realistic simulation is key to enabling safe and scalable development of % self-driving vehicles. A core component is simulating the sensors so that the entire autonomy system can be tested in simulation. Sensor simulation involves modeling traffic participants, such as vehicles, with high quality appearance and articulated geometry, and rendering them in real time. The self-driving industry has typically employed artists to build these assets. However, this is expensive, slow, and may not reflect reality. Instead, reconstructing assets automatically from sensor data collected in the wild would provide a better path to generating a diverse and large set with good real-world coverage. Nevertheless, current reconstruction approaches struggle on in-the-wild sensor data, due to its sparsity and noise. To tackle these issues, we present CADSim, which combines part-aware object-class priors via a small set of CAD models with differentiable rendering to automatically reconstruct vehicle geome
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;Adv3D&#26694;&#26550;&#65292;&#36890;&#36807;&#38381;&#29615;&#20256;&#24863;&#22120;&#20223;&#30495;&#35780;&#20272;&#33258;&#20027;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#36710;&#36742;&#24418;&#29366;&#20351;&#24471;&#22330;&#26223;&#26356;&#20855;&#25361;&#25112;&#65292;&#23548;&#33268;&#33258;&#20027;&#22833;&#36133;&#21644;SDV&#25805;&#32437;&#19981;&#33298;&#26381;&#12290;</title><link>http://arxiv.org/abs/2311.01446</link><description>&lt;p&gt;
Adv3D&#65306;&#36890;&#36807;&#38381;&#29615;&#20223;&#30495;&#29983;&#25104;&#23433;&#20840;&#20851;&#38190;&#30340;3D&#23545;&#35937;
&lt;/p&gt;
&lt;p&gt;
Adv3D: Generating Safety-Critical 3D Objects through Closed-Loop Simulation. (arXiv:2311.01446v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01446
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;Adv3D&#26694;&#26550;&#65292;&#36890;&#36807;&#38381;&#29615;&#20256;&#24863;&#22120;&#20223;&#30495;&#35780;&#20272;&#33258;&#20027;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#36710;&#36742;&#24418;&#29366;&#20351;&#24471;&#22330;&#26223;&#26356;&#20855;&#25361;&#25112;&#65292;&#23548;&#33268;&#33258;&#20027;&#22833;&#36133;&#21644;SDV&#25805;&#32437;&#19981;&#33298;&#26381;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;SDVs&#65289;&#24517;&#39035;&#22312;&#21508;&#31181;&#22330;&#26223;&#19979;&#36827;&#34892;&#20005;&#26684;&#27979;&#35797;&#20197;&#30830;&#20445;&#23433;&#20840;&#37096;&#32626;&#12290;&#35813;&#34892;&#19994;&#36890;&#24120;&#20381;&#38752;&#38381;&#29615;&#20223;&#30495;&#26469;&#35780;&#20272;SDV&#22312;&#19968;&#31995;&#21015;&#21512;&#25104;&#21644;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#20132;&#20114;&#24773;&#20917;&#65292;&#24182;&#39564;&#35777;&#20854;&#26159;&#21542;&#27491;&#24120;&#36816;&#34892;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#20027;&#35201;&#21482;&#27979;&#35797;&#31995;&#32479;&#30340;&#36816;&#21160;&#35268;&#21010;&#27169;&#22359;&#65292;&#24182;&#19988;&#21482;&#32771;&#34385;&#34892;&#20026;&#21464;&#21270;&#12290;&#22312;&#38381;&#29615;&#20013;&#35780;&#20272;&#23436;&#25972;&#30340;&#33258;&#20027;&#31995;&#32479;&#65292;&#24182;&#29702;&#35299;&#22522;&#20110;&#22330;&#26223;&#22806;&#35266;&#30340;&#20256;&#24863;&#22120;&#25968;&#25454;&#65288;&#22914;&#28436;&#21592;&#30340;&#24418;&#29366;&#65289;&#30340;&#21464;&#21270;&#22914;&#20309;&#24433;&#21709;&#31995;&#32479;&#24615;&#33021;&#26159;&#20851;&#38190;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Adv3D&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#37319;&#29992;&#30495;&#23454;&#19990;&#30028;&#30340;&#22330;&#26223;&#65292;&#24182;&#36827;&#34892;&#38381;&#29615;&#20256;&#24863;&#22120;&#20223;&#30495;&#26469;&#35780;&#20272;&#33258;&#20027;&#24615;&#33021;&#65292;&#24182;&#25214;&#21040;&#20351;&#22330;&#26223;&#26356;&#20855;&#25361;&#25112;&#24615;&#12289;&#23548;&#33268;&#33258;&#20027;&#22833;&#36133;&#21644;&#19981;&#33298;&#26381;&#30340;SDV&#25805;&#32437;&#30340;&#36710;&#36742;&#24418;&#29366;&#12290;&#19982;&#20043;&#21069;&#20165;&#22312;&#36710;&#39030;&#25110;&#36335;&#36793;&#28155;&#21152;&#20551;&#23545;&#25239;&#24418;&#29366;&#20197;&#25439;&#23475;&#24863;&#30693;&#30340;&#20316;&#21697;&#19981;&#21516;&#65292;&#25105;&#20204;&#20248;&#21270;&#20102;&#19968;&#20010;&#20302;-
&lt;/p&gt;
&lt;p&gt;
Self-driving vehicles (SDVs) must be rigorously tested on a wide range of scenarios to ensure safe deployment. The industry typically relies on closed-loop simulation to evaluate how the SDV interacts on a corpus of synthetic and real scenarios and verify it performs properly. However, they primarily only test the system's motion planning module, and only consider behavior variations. It is key to evaluate the full autonomy system in closed-loop, and to understand how variations in sensor data based on scene appearance, such as the shape of actors, affect system performance. In this paper, we propose a framework, Adv3D, that takes real world scenarios and performs closed-loop sensor simulation to evaluate autonomy performance, and finds vehicle shapes that make the scenario more challenging, resulting in autonomy failures and uncomfortable SDV maneuvers. Unlike prior works that add contrived adversarial shapes to vehicle roof-tops or roadside to harm perception only, we optimize a low-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#35757;&#32451;&#27169;&#24335;&#65292;&#21457;&#29616;&#20102;&#28145;&#24230;&#21452;&#19979;&#38477;&#29616;&#35937;&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#36807;&#22686;&#21152;&#35757;&#32451;&#36718;&#27425;&#28040;&#38500;&#36807;&#25311;&#21512;&#30340;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#22312;&#22823;&#22810;&#25968;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2311.01442</link><description>&lt;p&gt;
&#28145;&#24230;&#21452;&#19979;&#38477;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65306;&#36991;&#20813;&#27169;&#22411;&#27424;&#25311;&#21512;
&lt;/p&gt;
&lt;p&gt;
Deep Double Descent for Time Series Forecasting: Avoiding Undertrained Models. (arXiv:2311.01442v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#35757;&#32451;&#27169;&#24335;&#65292;&#21457;&#29616;&#20102;&#28145;&#24230;&#21452;&#19979;&#38477;&#29616;&#35937;&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#36807;&#22686;&#21152;&#35757;&#32451;&#36718;&#27425;&#28040;&#38500;&#36807;&#25311;&#21512;&#30340;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#22312;&#22823;&#22810;&#25968;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;Transformer&#65292;&#22312;&#21253;&#25324;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22312;&#20869;&#30340;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#26102;&#38388;&#24207;&#21015;&#25991;&#29486;&#20027;&#35201;&#20851;&#27880;&#27169;&#22411;&#26550;&#26500;&#20462;&#25913;&#21644;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#20294;&#26412;&#25991;&#25506;&#35752;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#35757;&#32451;&#27169;&#24335;&#65292;&#21363;&#22914;&#20309;&#35757;&#32451;&#27169;&#22411;&#32780;&#19981;&#32771;&#34385;&#20854;&#26550;&#26500;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#30740;&#31350;&#20102;&#22312;&#20844;&#20849;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#20960;&#20010;Transformer&#27169;&#22411;&#20013;&#30340;&#28145;&#24230;&#21452;&#19979;&#38477;&#29616;&#35937;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;epoch-wise&#30340;&#28145;&#24230;&#21452;&#19979;&#38477;&#20197;&#21450;&#36890;&#36807;&#22686;&#21152;&#35757;&#32451;&#36718;&#27425;&#21487;&#20197;&#28040;&#38500;&#36807;&#25311;&#21512;&#30340;&#26041;&#27861;&#12290;&#21033;&#29992;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#22312;72&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#22312;&#38271;&#24207;&#21015;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#38754;&#23454;&#29616;&#20102;&#36817;70%&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;&#36825;&#24847;&#21619;&#30528;&#35768;&#22810;&#25991;&#29486;&#20013;&#30340;&#27169;&#22411;&#21487;&#33021;&#20855;&#26377;&#26410;&#34987;&#21457;&#25496;&#30340;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20998;&#31867;&#35757;&#32451;&#27169;&#24335;&#20462;&#25913;&#30340;&#20998;&#31867;&#27861;&#65292;&#21253;&#25324;&#25968;&#25454;&#22686;&#24378;&#31561;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning models, particularly Transformers, have achieved impressive results in various domains, including time series forecasting. While existing time series literature primarily focuses on model architecture modifications and data augmentation techniques, this paper explores the training schema of deep learning models for time series; how models are trained regardless of their architecture. We perform extensive experiments to investigate the occurrence of deep double descent in several Transformer models trained on public time series data sets. We demonstrate epoch-wise deep double descent and that overfitting can be reverted using more epochs. Leveraging these findings, we achieve state-of-the-art results for long sequence time series forecasting in nearly 70% of the 72 benchmarks tested. This suggests that many models in the literature may possess untapped potential. Additionally, we introduce a taxonomy for classifying training schema modifications, covering data augmentation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#24565;&#31616;&#21333;&#19988;&#36731;&#37327;&#32423;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#30693;&#35782;&#33976;&#39311;&#21644;&#25968;&#25454;&#22686;&#24378;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#35270;&#35273;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#20174;&#39044;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#20013;&#33719;&#24471;&#40065;&#26834;&#25945;&#24072;&#27169;&#22411;&#30340;&#30693;&#35782;&#12290;&#20511;&#21161;&#31163;&#25955;&#23545;&#25239;&#33976;&#39311;&#26041;&#27861;&#65292;&#25105;&#20204;&#29983;&#25104;&#26356;&#26377;&#20449;&#24687;&#37327;&#30340;&#23545;&#25239;&#26679;&#26412;&#65292;&#21462;&#24471;&#20102;&#22312;&#23545;&#25239;&#24615;&#26679;&#26412;&#19978;&#40065;&#26834;&#24615;&#26174;&#33879;&#25552;&#21319;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#26694;&#26550;&#26469;&#25903;&#25345;&#22312;&#30693;&#35782;&#33976;&#39311;&#21644;&#25968;&#25454;&#22686;&#24378;&#35774;&#32622;&#20013;&#20351;&#29992;&#40065;&#26834;&#25945;&#24072;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#19981;&#21516;&#23398;&#29983;&#27169;&#22411;&#19978;&#30340;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35745;&#31639;&#36127;&#36733;&#26041;&#38754;&#30340;&#24320;&#38144;&#36739;&#23567;&#65292;&#24182;&#21487;&#20197;&#19982;&#20854;&#20182;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#36731;&#26494;&#32467;&#21512;&#12290;</title><link>http://arxiv.org/abs/2311.01441</link><description>&lt;p&gt;
&#20174;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#20013;&#25552;&#21462;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Distilling Out-of-Distribution Robustness from Vision-Language Foundation Models. (arXiv:2311.01441v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01441
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#24565;&#31616;&#21333;&#19988;&#36731;&#37327;&#32423;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#30693;&#35782;&#33976;&#39311;&#21644;&#25968;&#25454;&#22686;&#24378;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#35270;&#35273;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#20174;&#39044;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#20013;&#33719;&#24471;&#40065;&#26834;&#25945;&#24072;&#27169;&#22411;&#30340;&#30693;&#35782;&#12290;&#20511;&#21161;&#31163;&#25955;&#23545;&#25239;&#33976;&#39311;&#26041;&#27861;&#65292;&#25105;&#20204;&#29983;&#25104;&#26356;&#26377;&#20449;&#24687;&#37327;&#30340;&#23545;&#25239;&#26679;&#26412;&#65292;&#21462;&#24471;&#20102;&#22312;&#23545;&#25239;&#24615;&#26679;&#26412;&#19978;&#40065;&#26834;&#24615;&#26174;&#33879;&#25552;&#21319;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#26694;&#26550;&#26469;&#25903;&#25345;&#22312;&#30693;&#35782;&#33976;&#39311;&#21644;&#25968;&#25454;&#22686;&#24378;&#35774;&#32622;&#20013;&#20351;&#29992;&#40065;&#26834;&#25945;&#24072;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#19981;&#21516;&#23398;&#29983;&#27169;&#22411;&#19978;&#30340;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35745;&#31639;&#36127;&#36733;&#26041;&#38754;&#30340;&#24320;&#38144;&#36739;&#23567;&#65292;&#24182;&#21487;&#20197;&#19982;&#20854;&#20182;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#36731;&#26494;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#24565;&#31616;&#21333;&#19988;&#36731;&#37327;&#32423;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#21644;&#25968;&#25454;&#22686;&#24378;&#30340;&#32467;&#21512;&#26469;&#25552;&#39640;&#35270;&#35273;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#20174;&#39044;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#20013;&#36827;&#34892;&#33976;&#39311;&#65292;&#23637;&#31034;&#20102;&#22312;&#23545;&#25239;&#24615;&#26679;&#26412;&#19978;&#33719;&#24471;&#30340;&#40065;&#26834;&#24615;&#22686;&#30410;&#65292;&#20197;&#27492;&#21453;&#39539;&#20102;&#26356;&#22823;&#30340;&#27169;&#22411;&#19981;&#19968;&#23450;&#20250;&#25104;&#20026;&#26356;&#22909;&#30340;&#25945;&#24072;&#30340;&#29468;&#24819;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#31163;&#25955;&#23545;&#25239;&#33976;&#39311;&#65288;Discrete Adversarial Distillation&#65292;DAD&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#40065;&#26834;&#30340;&#25945;&#24072;&#27169;&#22411;&#29983;&#25104;&#23545;&#25239;&#26679;&#26412;&#65292;&#24182;&#36890;&#36807;VQGAN&#23558;&#20854;&#31163;&#25955;&#21270;&#65292;&#20174;&#32780;&#21019;&#36896;&#20986;&#27604;&#26631;&#20934;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#26356;&#26377;&#20449;&#24687;&#37327;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#30693;&#35782;&#33976;&#39311;&#21644;&#25968;&#25454;&#22686;&#24378;&#30340;&#35774;&#32622;&#20013;&#20351;&#29992;&#40065;&#26834;&#30340;&#25945;&#24072;&#27169;&#22411;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#23398;&#29983;&#27169;&#22411;&#20013;&#23637;&#31034;&#20102;&#22312;&#23545;&#25239;&#24615;&#26679;&#26412;&#19978;&#30340;&#40065;&#26834;&#24615;&#21644;&#24178;&#20928;&#20934;&#30830;&#24615;&#30340;&#26174;&#33879;&#25552;&#21319;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#19982;&#31867;&#20284;&#25216;&#26415;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22686;&#21152;&#20102;&#23569;&#37327;&#30340;&#35745;&#31639;&#36127;&#36733;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#19982;&#20854;&#20182;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a conceptually simple and lightweight framework for improving the robustness of vision models through the combination of knowledge distillation and data augmentation. We address the conjecture that larger models do not make for better teachers by showing strong gains in out-of-distribution robustness when distilling from pretrained foundation models. Following this finding, we propose Discrete Adversarial Distillation (DAD), which leverages a robust teacher to generate adversarial examples and a VQGAN to discretize them, creating more informative samples than standard data augmentation techniques. We provide a theoretical framework for the use of a robust teacher in the knowledge distillation with data augmentation setting and demonstrate strong gains in out-of-distribution robustness and clean accuracy across different student architectures. Notably, our method adds minor computational overhead compared to similar techniques and can be easily combined with other data augmen
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#22312;&#26410;&#30693;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#20351;&#29992;&#23545;&#27604;&#30697;&#26469;&#23398;&#20064;&#20855;&#26377;&#36793;&#32536;&#30340;&#39640;&#32500;&#21322;&#31354;&#38388;&#65292;&#32780;&#19981;&#38656;&#35201;&#26631;&#31614;&#65292;&#24182;&#22312;&#36825;&#20010;&#20998;&#24067;&#20551;&#35774;&#19979;&#24314;&#31435;&#20102;&#38544;&#34255;&#21322;&#31354;&#38388;&#30340;&#21807;&#19968;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01435</link><description>&lt;p&gt;
&#23545;&#27604;&#30697;&#65306;&#22810;&#39033;&#24335;&#26102;&#38388;&#26080;&#30417;&#30563;&#21322;&#31354;&#38388;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Contrastive Moments: Unsupervised Halfspace Learning in Polynomial Time. (arXiv:2311.01435v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01435
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#22312;&#26410;&#30693;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#20351;&#29992;&#23545;&#27604;&#30697;&#26469;&#23398;&#20064;&#20855;&#26377;&#36793;&#32536;&#30340;&#39640;&#32500;&#21322;&#31354;&#38388;&#65292;&#32780;&#19981;&#38656;&#35201;&#26631;&#31614;&#65292;&#24182;&#22312;&#36825;&#20010;&#20998;&#24067;&#20551;&#35774;&#19979;&#24314;&#31435;&#20102;&#38544;&#34255;&#21322;&#31354;&#38388;&#30340;&#21807;&#19968;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#29615;&#22659;&#20998;&#24067;&#26159;&#26410;&#30693;&#20851;&#20110;d&#32500;&#31354;&#38388;d&#20493;&#23545;&#31216;&#12289;&#23545;&#25968;&#20985;&#30340;&#20998;&#24067;&#30340;&#20854;&#20013;&#19968;&#32452;&#20998;&#24067;&#65292;&#21322;&#31354;&#38388;&#26159;&#36890;&#36807;&#21024;&#38500;&#33267;&#23569;&#19968;&#20010;&#20998;&#37327;&#20998;&#24067;&#20013;&#30340;&#949;&#27604;&#20363;&#30340;&#25968;&#25454;&#24341;&#20837;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#26469;&#23398;&#20064;&#20855;&#26377;&#36793;&#32536;&#30340;&#39640;&#32500;&#21322;&#31354;&#38388;&#65292;&#30446;&#26631;&#26159;&#22312;&#25152;&#38656;&#30340;TV&#36317;&#31163;&#20869;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#19981;&#38656;&#35201;&#26631;&#31614;&#65292;&#24182;&#19988;&#22312;&#36825;&#20010;&#20998;&#24067;&#20551;&#35774;&#19979;&#24314;&#31435;&#20102;&#38544;&#34255;&#21322;&#31354;&#38388;&#30340;&#21807;&#19968;&#24615;&#65288;&#21644;&#39640;&#25928;&#24615;&#65289;&#12290;&#31639;&#27861;&#30340;&#26679;&#26412;&#21644;&#26102;&#38388;&#22797;&#26434;&#24615;&#22312;&#32500;&#24230;&#21644;1/&#949;&#19978;&#37117;&#26159;&#22810;&#39033;&#24335;&#12290;&#35813;&#31639;&#27861;&#21482;&#20351;&#29992;&#32463;&#39564;&#20998;&#24067;&#30340;&#36866;&#24403;&#37325;&#26032;&#21152;&#26435;&#30340;&#21069;&#20004;&#20010;&#30697;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#23545;&#27604;&#30697;&#65307;&#20854;&#20998;&#26512;&#20351;&#29992;&#20102;&#20851;&#20110;&#24191;&#20041;Dirichlet&#22810;&#39033;&#24335;&#30340;&#32463;&#20856;&#20107;&#23454;&#65292;&#24182;&#19988;&#20851;&#38190;&#20381;&#36182;&#20110;&#23545;&#23545;&#25968;&#20985;&#25130;&#26029;&#30340;&#30697;&#27604;&#30340;&#19968;&#20010;&#26032;&#21333;&#35843;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
We give a polynomial-time algorithm for learning high-dimensional halfspaces with margins in $d$-dimensional space to within desired TV distance when the ambient distribution is an unknown affine transformation of the $d$-fold product of an (unknown) symmetric one-dimensional logconcave distribution, and the halfspace is introduced by deleting at least an $\epsilon$ fraction of the data in one of the component distributions. Notably, our algorithm does not need labels and establishes the unique (and efficient) identifiability of the hidden halfspace under this distributional assumption. The sample and time complexity of the algorithm are polynomial in the dimension and $1/\epsilon$. The algorithm uses only the first two moments of suitable re-weightings of the empirical distribution, which we call contrastive moments; its analysis uses classical facts about generalized Dirichlet polynomials and relies crucially on a new monotonicity property of the moment ratio of truncations of logcon
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26680;&#25197;&#26354;&#20989;&#25968;&#23545;Mixup&#25968;&#25454;&#36827;&#34892;&#20010;&#24615;&#21270;&#22788;&#29702;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#25913;&#21464;&#25554;&#20540;&#31995;&#25968;&#30340;&#27010;&#29575;&#20998;&#24067;&#26469;&#23454;&#29616;&#26356;&#39057;&#32321;&#21644;&#26356;&#24378;&#28872;&#30340;&#28151;&#21512;&#30456;&#20284;&#25968;&#25454;&#28857;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#65292;&#36824;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01434</link><description>&lt;p&gt;
&#36890;&#36807;&#26680;&#25197;&#26354;&#20989;&#25968;&#23450;&#21046;Mixup&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Tailoring Mixup to Data using Kernel Warping functions. (arXiv:2311.01434v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01434
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26680;&#25197;&#26354;&#20989;&#25968;&#23545;Mixup&#25968;&#25454;&#36827;&#34892;&#20010;&#24615;&#21270;&#22788;&#29702;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#25913;&#21464;&#25554;&#20540;&#31995;&#25968;&#30340;&#27010;&#29575;&#20998;&#24067;&#26469;&#23454;&#29616;&#26356;&#39057;&#32321;&#21644;&#26356;&#24378;&#28872;&#30340;&#28151;&#21512;&#30456;&#20284;&#25968;&#25454;&#28857;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#65292;&#36824;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#26159;&#23398;&#20064;&#39640;&#25928;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#37325;&#35201;&#22522;&#30784;&#12290;&#22312;&#25152;&#26377;&#25552;&#20986;&#30340;&#22686;&#24378;&#25216;&#26415;&#20013;&#65292;&#32447;&#24615;&#25554;&#20540;&#35757;&#32451;&#25968;&#25454;&#28857;&#65288;&#20063;&#31216;&#20026;Mixup&#65289;&#24050;&#34987;&#35777;&#26126;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#36873;&#25321;&#21512;&#36866;&#30340;&#28857;&#36827;&#34892;&#28151;&#21512;&#65292;&#25110;&#32773;&#24212;&#29992;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#25554;&#20540;&#65292;&#32780;&#25105;&#20204;&#21017;&#23545;&#26356;&#30456;&#20284;&#30340;&#28857;&#36827;&#34892;&#26356;&#39057;&#32321;&#21644;&#26356;&#24378;&#28872;&#30340;&#28151;&#21512;&#24863;&#20852;&#36259;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#25197;&#26354;&#20989;&#25968;&#21160;&#24577;&#25913;&#21464;&#25554;&#20540;&#31995;&#25968;&#30340;&#27010;&#29575;&#20998;&#24067;&#30340;&#26041;&#27861;&#65292;&#21462;&#20915;&#20110;&#35201;&#32452;&#21512;&#30340;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#39640;&#25928;&#32780;&#28789;&#27963;&#30340;&#26694;&#26550;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#20197;&#36991;&#20813;&#22810;&#26679;&#24615;&#30340;&#25439;&#22833;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#26082;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21448;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/ENSTA-U2IS/torch-uncertainty&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data augmentation is an essential building block for learning efficient deep learning models. Among all augmentation techniques proposed so far, linear interpolation of training data points, also called mixup, has found to be effective for a large panel of applications. While the majority of works have focused on selecting the right points to mix, or applying complex non-linear interpolation, we are interested in mixing similar points more frequently and strongly than less similar ones. To this end, we propose to dynamically change the underlying distribution of interpolation coefficients through warping functions, depending on the similarity between data points to combine. We define an efficient and flexible framework to do so without losing in diversity. We provide extensive experiments for classification and regression tasks, showing that our proposed method improves both performance and calibration of models. Code available in https://github.com/ENSTA-U2IS/torch-uncertainty
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;RF&#12289;SVM&#21644;CNN&#31639;&#27861;&#32467;&#21512;MRI&#22270;&#20687;&#20998;&#27700;&#23725;&#20998;&#21106;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#30340;&#26041;&#27861;&#26469;&#37492;&#21035;&#22235;&#20010;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30196;&#21574;&#38454;&#27573;&#30340;&#20998;&#31867;&#65292;&#20854;&#20013;SVM&#32467;&#21512;&#20998;&#27700;&#23725;&#29305;&#24449;&#30340;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;96.25%&#12290;</title><link>http://arxiv.org/abs/2311.01428</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#37492;&#21035;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30196;&#21574;&#27700;&#24179;
&lt;/p&gt;
&lt;p&gt;
Identifying Alzheimer Disease Dementia Levels Using Machine Learning Methods. (arXiv:2311.01428v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01428
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;RF&#12289;SVM&#21644;CNN&#31639;&#27861;&#32467;&#21512;MRI&#22270;&#20687;&#20998;&#27700;&#23725;&#20998;&#21106;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#30340;&#26041;&#27861;&#26469;&#37492;&#21035;&#22235;&#20010;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30196;&#21574;&#38454;&#27573;&#30340;&#20998;&#31867;&#65292;&#20854;&#20013;SVM&#32467;&#21512;&#20998;&#27700;&#23725;&#29305;&#24449;&#30340;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;96.25%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30196;&#21574;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#65292;&#26159;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#20027;&#35201;&#34920;&#29616;&#12290;&#38543;&#30528;&#30149;&#24773;&#20174;&#36731;&#24230;&#21040;&#20005;&#37325;&#30340;&#36827;&#23637;&#65292;&#26174;&#33879;&#24433;&#21709;&#24739;&#32773;&#29420;&#31435;&#23436;&#25104;&#26085;&#24120;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#36843;&#20999;&#38656;&#35201;&#21450;&#26102;&#20934;&#30830;&#30340;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#20998;&#31867;&#12290;&#26426;&#22120;&#23398;&#20064;&#25110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24050;&#25104;&#20026;&#36798;&#21040;&#27492;&#30446;&#30340;&#30340;&#26377;&#25928;&#24037;&#20855;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;RF&#12289;SVM&#21644;CNN&#31639;&#27861;&#32467;&#21512;MRI&#22270;&#20687;&#30340;&#20998;&#27700;&#23725;&#20998;&#21106;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#30340;&#26041;&#27861;&#26469;&#23545;&#22235;&#20010;&#30196;&#21574;&#38454;&#27573;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;SVM&#32467;&#21512;&#20998;&#27700;&#23725;&#29305;&#24449;&#30340;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;96.25%&#65292;&#36229;&#36807;&#20102;&#20854;&#20182;&#20998;&#31867;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;ADNI&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35266;&#23519;&#21040;&#20998;&#27700;&#23725;&#20998;&#21106;&#30340;&#24341;&#20837;&#25552;&#21319;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dementia, a prevalent neurodegenerative condition, is a major manifestation of Alzheimer's disease (AD). As the condition progresses from mild to severe, it significantly impairs the individual's ability to perform daily tasks independently, necessitating the need for timely and accurate AD classification. Machine learning or deep learning models have emerged as effective tools for this purpose. In this study, we suggested an approach for classifying the four stages of dementia using RF, SVM, and CNN algorithms, augmented with watershed segmentation for feature extraction from MRI images. Our results reveal that SVM with watershed features achieves an impressive accuracy of 96.25%, surpassing other classification methods. The ADNI dataset is utilized to evaluate the effectiveness of our method, and we observed that the inclusion of watershed segmentation contributes to the enhanced performance of the models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#38738;&#20809;&#30524;&#20998;&#21106;&#12289;&#20998;&#31867;&#21644;&#26816;&#27979;&#26041;&#38754;&#30340;&#23574;&#31471;&#25216;&#26415;&#12290;&#36890;&#36807;&#20998;&#26512;&#36817;&#26399;&#30740;&#31350;&#65292;&#21457;&#29616;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#22312;&#33258;&#21160;&#21270;&#38738;&#20809;&#30524;&#26816;&#27979;&#20013;&#20855;&#26377;&#26174;&#33879;&#30340;&#28508;&#21147;&#65292;&#24182;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#25552;&#20379;&#20102;&#28508;&#22312;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2311.01425</link><description>&lt;p&gt;
&#25506;&#32034;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#29992;&#20110;&#38738;&#20809;&#30524;&#26816;&#27979;&#65306;&#19968;&#39033;&#32508;&#21512;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Exploring Deep Learning Techniques for Glaucoma Detection: A Comprehensive Review. (arXiv:2311.01425v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01425
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#38738;&#20809;&#30524;&#20998;&#21106;&#12289;&#20998;&#31867;&#21644;&#26816;&#27979;&#26041;&#38754;&#30340;&#23574;&#31471;&#25216;&#26415;&#12290;&#36890;&#36807;&#20998;&#26512;&#36817;&#26399;&#30740;&#31350;&#65292;&#21457;&#29616;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#22312;&#33258;&#21160;&#21270;&#38738;&#20809;&#30524;&#26816;&#27979;&#20013;&#20855;&#26377;&#26174;&#33879;&#30340;&#28508;&#21147;&#65292;&#24182;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#25552;&#20379;&#20102;&#28508;&#22312;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38738;&#20809;&#30524;&#26159;&#20840;&#29699;&#35270;&#21147;&#20007;&#22833;&#30340;&#20027;&#35201;&#21407;&#22240;&#20043;&#19968;&#65292;&#38656;&#35201;&#20934;&#30830;&#39640;&#25928;&#30340;&#26816;&#27979;&#26041;&#27861;&#12290;&#20256;&#32479;&#30340;&#25163;&#21160;&#26816;&#27979;&#26041;&#27861;&#22312;&#25104;&#26412;&#12289;&#26102;&#38388;&#21644;&#20027;&#35266;&#24615;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#36817;&#26399;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#21457;&#23637;&#23637;&#31034;&#20102;&#20174;&#35270;&#32593;&#33180;&#24213;&#23618;&#22270;&#20687;&#20013;&#26816;&#27979;&#30456;&#20851;&#29305;&#24449;&#20197;&#33258;&#21160;&#21270;&#38738;&#20809;&#30524;&#26816;&#27979;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#20351;&#29992;&#20110;&#38738;&#20809;&#30524;&#20998;&#21106;&#12289;&#20998;&#31867;&#21644;&#26816;&#27979;&#30340;&#23574;&#31471;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#32508;&#21512;&#27010;&#36848;&#12290;&#36890;&#36807;&#20998;&#26512;&#36817;&#26399;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#36825;&#20123;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#21644;&#23616;&#38480;&#24615;&#65292;&#31361;&#20986;&#20102;&#20851;&#38190;&#21457;&#29616;&#65292;&#24182;&#30830;&#23450;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#28508;&#22312;&#39046;&#22495;&#12290;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#20351;&#29992;&#21487;&#33021;&#26174;&#33879;&#25552;&#39640;&#38738;&#20809;&#30524;&#26816;&#27979;&#30340;&#21151;&#25928;&#12289;&#26377;&#29992;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;&#36825;&#39033;&#30740;&#31350;&#30340;&#21457;&#29616;&#23545;&#20110;&#33258;&#21160;&#21270;&#38738;&#20809;&#30524;&#26816;&#27979;&#30340;&#25345;&#32493;&#25512;&#36827;&#20855;&#26377;&#36129;&#29486;&#65292;&#24182;&#23545;&#25552;&#39640;&#24739;&#32773;&#30340;&#27835;&#30103;&#21644;&#31649;&#29702;&#26377;&#30528;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Glaucoma is one of the primary causes of vision loss around the world, necessitating accurate and efficient detection methods. Traditional manual detection approaches have limitations in terms of cost, time, and subjectivity. Recent developments in deep learning approaches demonstrate potential in automating glaucoma detection by detecting relevant features from retinal fundus images. This article provides a comprehensive overview of cutting-edge deep learning methods used for the segmentation, classification, and detection of glaucoma. By analyzing recent studies, the effectiveness and limitations of these techniques are evaluated, key findings are highlighted, and potential areas for further research are identified. The use of deep learning algorithms may significantly improve the efficacy, usefulness, and accuracy of glaucoma detection. The findings from this research contribute to the ongoing advancements in automated glaucoma detection and have implications for improving patient o
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#38382;&#39064;&#65292;&#21363;&#22914;&#20309;&#23558;&#39044;&#20808;&#35757;&#32451;&#30340;&#28304;&#27169;&#22411;&#36866;&#24212;&#21040;&#30446;&#26631;&#39046;&#22495;&#65292;&#20351;&#29992;&#20165;&#35206;&#30422;&#37096;&#20998;&#26631;&#31614;&#31354;&#38388;&#30340;&#30446;&#26631;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#12290;&#35770;&#25991;&#36890;&#36807;&#26500;&#24314;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25581;&#31034;&#20102;&#36825;&#20010;&#38382;&#39064;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#20851;&#38190;&#26041;&#21521;&#65306;&#20998;&#31163;&#39046;&#22495;&#26799;&#24230;&#21644;&#20998;&#31867;&#26799;&#24230;&#65292;&#24182;&#20445;&#25345;&#31867;&#21035;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2311.01420</link><description>&lt;p&gt;
&#20840;&#38754;&#36801;&#31227;&#65306;&#21521;&#20855;&#26377;&#37096;&#20998;&#30446;&#26631;&#25968;&#25454;&#30340;&#38750;&#20013;&#26029;&#24494;&#35843;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
Holistic Transfer: Towards Non-Disruptive Fine-Tuning with Partial Target Data. (arXiv:2311.01420v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01420
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#38382;&#39064;&#65292;&#21363;&#22914;&#20309;&#23558;&#39044;&#20808;&#35757;&#32451;&#30340;&#28304;&#27169;&#22411;&#36866;&#24212;&#21040;&#30446;&#26631;&#39046;&#22495;&#65292;&#20351;&#29992;&#20165;&#35206;&#30422;&#37096;&#20998;&#26631;&#31614;&#31354;&#38388;&#30340;&#30446;&#26631;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#12290;&#35770;&#25991;&#36890;&#36807;&#26500;&#24314;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25581;&#31034;&#20102;&#36825;&#20010;&#38382;&#39064;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#20851;&#38190;&#26041;&#21521;&#65306;&#20998;&#31163;&#39046;&#22495;&#26799;&#24230;&#21644;&#20998;&#31867;&#26799;&#24230;&#65292;&#24182;&#20445;&#25345;&#31867;&#21035;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#38382;&#39064;&#65292;&#28041;&#21450;&#23558;&#39044;&#20808;&#35757;&#32451;&#30340;&#28304;&#27169;&#22411;&#36866;&#24212;&#21040;&#30446;&#26631;&#39046;&#22495;&#65292;&#20197;&#23545;&#28304;&#25968;&#25454;&#20013;&#20986;&#29616;&#30340;&#25152;&#26377;&#31867;&#36827;&#34892;&#20998;&#31867;&#65292;&#20351;&#29992;&#20165;&#35206;&#30422;&#37096;&#20998;&#26631;&#31614;&#31354;&#38388;&#30340;&#30446;&#26631;&#25968;&#25454;&#12290;&#36825;&#20010;&#38382;&#39064;&#26159;&#23454;&#38469;&#30340;&#65292;&#22240;&#20026;&#30446;&#26631;&#32456;&#31471;&#29992;&#25143;&#22312;&#36866;&#24212;&#20043;&#21069;&#25910;&#38598;&#25152;&#26377;&#31867;&#21035;&#30340;&#25968;&#25454;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#26377;&#25991;&#29486;&#20013;&#65292;&#23545;&#27492;&#38382;&#39064;&#30340;&#20851;&#27880;&#26377;&#38480;&#12290;&#20026;&#20102;&#25581;&#31034;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20854;&#22266;&#26377;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#21457;&#29616;&#19968;&#20010;&#22256;&#22659; - &#19968;&#26041;&#38754;&#65292;&#36866;&#24212;&#21040;&#26032;&#30340;&#30446;&#26631;&#39046;&#22495;&#23545;&#20110;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#65307;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#30446;&#26631;&#36866;&#24212;&#25968;&#25454;&#20013;&#32570;&#22833;&#30340;&#31867;&#21035;&#20445;&#25345;&#20998;&#31867;&#20934;&#30830;&#24615;&#26159;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#26356;&#19981;&#29992;&#35828;&#23545;&#20854;&#36827;&#34892;&#25913;&#36827;&#20102;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#20004;&#20010;&#20851;&#38190;&#26041;&#21521;&#65306;1&#65289;&#23558;&#39046;&#22495;&#26799;&#24230;&#19982;&#20998;&#31867;&#26799;&#24230;&#20998;&#31163;&#65292;&#24182;2&#65289;&#20445;&#25345;&#31867;&#21035;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20010;&#26377;&#25928;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
We propose a learning problem involving adapting a pre-trained source model to the target domain for classifying all classes that appeared in the source data, using target data that covers only a partial label space. This problem is practical, as it is unrealistic for the target end-users to collect data for all classes prior to adaptation. However, it has received limited attention in the literature. To shed light on this issue, we construct benchmark datasets and conduct extensive experiments to uncover the inherent challenges. We found a dilemma -- on the one hand, adapting to the new target domain is important to claim better performance; on the other hand, we observe that preserving the classification accuracy of classes missing in the target adaptation data is highly challenging, let alone improving them. To tackle this, we identify two key directions: 1) disentangling domain gradients from classification gradients, and 2) preserving class relationships. We present several effect
&lt;/p&gt;</description></item><item><title>&#8220;Castor&#8221;&#26159;&#19968;&#20010;&#29992;&#20110;&#23398;&#20064;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#22240;&#26524;&#20851;&#31995;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#32508;&#21512;&#23398;&#20064;&#21508;&#20010;&#21306;&#22495;&#30340;&#22240;&#26524;&#22270;&#12290;&#23427;&#36890;&#36807;&#26368;&#22823;&#21270;&#24471;&#20998;&#20989;&#25968;&#26469;&#25512;&#26029;&#21306;&#22495;&#30340;&#25968;&#37327;&#65292;&#24182;&#23398;&#20064;&#27599;&#20010;&#21306;&#22495;&#20013;&#30340;&#32447;&#24615;&#25110;&#38750;&#32447;&#24615;&#22240;&#26524;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2311.01412</link><description>&lt;p&gt;
Castor: &#22240;&#26524;&#26102;&#24207;&#21306;&#22495;&#32467;&#26500;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Castor: Causal Temporal Regime Structure Learning. (arXiv:2311.01412v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01412
&lt;/p&gt;
&lt;p&gt;
&#8220;Castor&#8221;&#26159;&#19968;&#20010;&#29992;&#20110;&#23398;&#20064;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#22240;&#26524;&#20851;&#31995;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#32508;&#21512;&#23398;&#20064;&#21508;&#20010;&#21306;&#22495;&#30340;&#22240;&#26524;&#22270;&#12290;&#23427;&#36890;&#36807;&#26368;&#22823;&#21270;&#24471;&#20998;&#20989;&#25968;&#26469;&#25512;&#26029;&#21306;&#22495;&#30340;&#25968;&#37327;&#65292;&#24182;&#23398;&#20064;&#27599;&#20010;&#21306;&#22495;&#20013;&#30340;&#32447;&#24615;&#25110;&#38750;&#32447;&#24615;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25581;&#31034;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#26159;&#19968;&#20010;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#30446;&#26631;&#65292;&#28041;&#21450;&#21040;&#20174;&#27668;&#20505;&#31185;&#23398;&#21040;&#21307;&#30103;&#20445;&#20581;&#31561;&#21508;&#20010;&#23398;&#31185;&#30340;&#24191;&#27867;&#33539;&#22260;&#12290;&#36825;&#20123;&#25968;&#25454;&#21253;&#21547;&#32447;&#24615;&#25110;&#38750;&#32447;&#24615;&#20851;&#31995;&#65292;&#24182;&#19988;&#36890;&#24120;&#36981;&#24490;&#22810;&#20010;&#20808;&#39564;&#26410;&#30693;&#30340;&#21306;&#22495;&#12290;&#29616;&#26377;&#30340;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#21487;&#20197;&#20174;&#20855;&#26377;&#24050;&#30693;&#21306;&#22495;&#30340;&#24322;&#26500;&#25968;&#25454;&#20013;&#25512;&#26029;&#20986;&#25688;&#35201;&#22240;&#26524;&#22270;&#65292;&#20294;&#22312;&#20840;&#38754;&#23398;&#20064;&#21306;&#22495;&#21644;&#30456;&#24212;&#30340;&#22240;&#26524;&#22270;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;CASTOR&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#23398;&#20064;&#30001;&#19981;&#21516;&#22240;&#26524;&#22270;&#32479;&#27835;&#30340;&#21508;&#31181;&#24322;&#26500;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#36890;&#36807;EM&#31639;&#27861;&#36890;&#36807;&#26368;&#22823;&#21270;&#19968;&#20010;&#24471;&#20998;&#20989;&#25968;&#65292;CASTOR&#25512;&#26029;&#20986;&#21306;&#22495;&#30340;&#25968;&#37327;&#24182;&#23398;&#20064;&#27599;&#20010;&#21306;&#22495;&#20013;&#30340;&#32447;&#24615;&#25110;&#38750;&#32447;&#24615;&#22240;&#26524;&#20851;&#31995;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;CASTOR&#30340;&#31283;&#20581;&#25910;&#25947;&#24615;&#36136;&#65292;&#29305;&#21035;&#31361;&#20986;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of uncovering causal relationships among multivariate time series data stands as an essential and challenging objective that cuts across a broad array of disciplines ranging from climate science to healthcare. Such data entails linear or non-linear relationships, and usually follow multiple a priori unknown regimes. Existing causal discovery methods can infer summary causal graphs from heterogeneous data with known regimes, but they fall short in comprehensively learning both regimes and the corresponding causal graph. In this paper, we introduce CASTOR, a novel framework designed to learn causal relationships in heterogeneous time series data composed of various regimes, each governed by a distinct causal graph. Through the maximization of a score function via the EM algorithm, CASTOR infers the number of regimes and learns linear or non-linear causal relationships in each regime. We demonstrate the robust convergence properties of CASTOR, specifically highlighting its profic
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#24418;&#24335;&#30340;&#25193;&#25955;&#22270;&#20687;&#32534;&#36753;&#26041;&#27861;&#65292;&#35828;&#26126;&#20102;SDE&#22312;&#22270;&#20687;&#32534;&#36753;&#20013;&#30340;&#20248;&#21183;&#65292;&#24182;&#25552;&#20379;&#20102;SDE&#21644;ODE&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;SDE&#30340;&#28857;&#22312;&#22270;&#20687;&#19978;&#25302;&#21160;&#30340;&#26041;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#20855;&#26377;&#24320;&#25918;&#38598;&#30340;&#25361;&#25112;&#24615;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2311.01410</link><description>&lt;p&gt;
&#38543;&#26426;&#24615;&#30340;&#31119;&#38899;&#65306;SDE&#22312;&#19968;&#33324;&#25193;&#25955;&#22270;&#20687;&#32534;&#36753;&#20013;&#36229;&#36807;ODE
&lt;/p&gt;
&lt;p&gt;
The Blessing of Randomness: SDE Beats ODE in General Diffusion-based Image Editing. (arXiv:2311.01410v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#24418;&#24335;&#30340;&#25193;&#25955;&#22270;&#20687;&#32534;&#36753;&#26041;&#27861;&#65292;&#35828;&#26126;&#20102;SDE&#22312;&#22270;&#20687;&#32534;&#36753;&#20013;&#30340;&#20248;&#21183;&#65292;&#24182;&#25552;&#20379;&#20102;SDE&#21644;ODE&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;SDE&#30340;&#28857;&#22312;&#22270;&#20687;&#19978;&#25302;&#21160;&#30340;&#26041;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#20855;&#26377;&#24320;&#25918;&#38598;&#30340;&#25361;&#25112;&#24615;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#27010;&#29575;&#24418;&#24335;&#30340;&#25193;&#25955;&#22270;&#20687;&#32534;&#36753;&#26041;&#27861;&#65292;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;&#19968;&#20010;&#38544;&#21464;&#37327;&#20197;&#29305;&#23450;&#20219;&#21153;&#30340;&#26041;&#24335;&#36827;&#34892;&#32534;&#36753;&#65292;&#24182;&#19988;&#36890;&#24120;&#19982;&#21407;&#22987;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDE&#65289;&#25110;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#24341;&#21457;&#30340;&#36793;&#32536;&#20998;&#24067;&#26377;&#25152;&#19981;&#21516;&#12290;&#30456;&#21453;&#65292;&#23427;&#20026;&#32534;&#36753;&#23450;&#20041;&#20102;&#30456;&#24212;&#30340;SDE&#25110;ODE&#12290;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20004;&#20010;SDE&#30340;&#36793;&#32536;&#20998;&#24067;&#20043;&#38388;&#30340;Kullback-Leibler&#25955;&#24230;&#36880;&#28176;&#20943;&#23567;&#65292;&#32780;&#23545;&#20110;ODE&#26469;&#35828;&#65292;&#38543;&#30528;&#26102;&#38388;&#36235;&#36817;&#20110;&#38646;&#65292;&#25955;&#24230;&#20445;&#25345;&#19981;&#21464;&#65292;&#36825;&#26174;&#31034;&#20102;SDE&#22312;&#22270;&#20687;&#32534;&#36753;&#20013;&#30340;&#20248;&#21183;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#21508;&#31181;&#20219;&#21153;&#20013;&#24120;&#29992;&#30340;ODE&#22522;&#32447;&#30340;SDE&#23545;&#24212;&#29289;&#65292;&#21253;&#25324;&#20462;&#22797;&#21644;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#36716;&#25442;&#65292;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#65292;SDE&#26174;&#31034;&#20102;&#19968;&#33268;&#19988;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;SDE-Drag - &#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22522;&#20110;SDE&#20844;&#24335;&#30340;&#28857;&#22312;&#22270;&#20687;&#19978;&#25302;&#21160;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#24314;&#31435;&#20102;&#19968;&#20010;&#20855;&#26377;&#24320;&#25918;&#38598;&#30340;&#25361;&#25112;&#24615;&#22522;&#20934;&#65288;&#31216;&#20026;DragBench&#65289; &#12290;
&lt;/p&gt;
&lt;p&gt;
We present a unified probabilistic formulation for diffusion-based image editing, where a latent variable is edited in a task-specific manner and generally deviates from the corresponding marginal distribution induced by the original stochastic or ordinary differential equation (SDE or ODE). Instead, it defines a corresponding SDE or ODE for editing. In the formulation, we prove that the Kullback-Leibler divergence between the marginal distributions of the two SDEs gradually decreases while that for the ODEs remains as the time approaches zero, which shows the promise of SDE in image editing. Inspired by it, we provide the SDE counterparts for widely used ODE baselines in various tasks including inpainting and image-to-image translation, where SDE shows a consistent and substantial improvement. Moreover, we propose SDE-Drag -- a simple yet effective method built upon the SDE formulation for point-based content dragging. We build a challenging benchmark (termed DragBench) with open-set 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#24515;&#38598;&#30340;&#12289;&#28201;&#21644;&#21464;&#20998;&#21518;&#39564;&#30340;&#39640;&#26031;&#36807;&#31243;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#31232;&#30095;&#30340;&#12289;&#21487;&#35299;&#37322;&#30340;&#25968;&#25454;&#34920;&#31034;&#26469;&#38477;&#20302;&#21442;&#25968;&#22823;&#23567;&#65292;&#24182;&#19988;&#20855;&#26377;&#25968;&#20540;&#31283;&#23450;&#24615;&#21644;&#36739;&#20302;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2311.01409</link><description>&lt;p&gt;
&#22522;&#20110;&#26680;&#24515;&#38598;&#30340;&#12289;&#28201;&#21644;&#21464;&#20998;&#21518;&#39564;&#30340;&#31934;&#30830;&#21644;&#21487;&#25193;&#23637;&#38543;&#26426;&#39640;&#26031;&#36807;&#31243;&#25512;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Coreset-based, Tempered Variational Posterior for Accurate and Scalable Stochastic Gaussian Process Inference. (arXiv:2311.01409v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01409
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#24515;&#38598;&#30340;&#12289;&#28201;&#21644;&#21464;&#20998;&#21518;&#39564;&#30340;&#39640;&#26031;&#36807;&#31243;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#31232;&#30095;&#30340;&#12289;&#21487;&#35299;&#37322;&#30340;&#25968;&#25454;&#34920;&#31034;&#26469;&#38477;&#20302;&#21442;&#25968;&#22823;&#23567;&#65292;&#24182;&#19988;&#20855;&#26377;&#25968;&#20540;&#31283;&#23450;&#24615;&#21644;&#36739;&#20302;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38543;&#26426;&#21464;&#20998;&#39640;&#26031;&#36807;&#31243;($\mathcal{GP}$)&#25512;&#29702;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#21487;&#23398;&#20064;&#30340;&#26435;&#37325;&#20266;&#36755;&#20837;&#36755;&#20986;&#28857;&#30340;&#21518;&#39564;&#65288;&#26680;&#24515;&#38598;&#65289;&#12290;&#19982;&#33258;&#30001;&#24418;&#24335;&#30340;&#21464;&#20998;&#26063;&#19981;&#21516;&#65292;&#25552;&#20986;&#30340;&#22522;&#20110;&#26680;&#24515;&#38598;&#30340;&#12289;&#28201;&#21644;&#21464;&#20998;&#30340;$\mathcal{GP}$&#65288;CVTGP&#65289;&#26159;&#22522;&#20110;$\mathcal{GP}$&#20808;&#39564;&#21644;&#25968;&#25454;&#20284;&#28982;&#20989;&#25968;&#26469;&#23450;&#20041;&#30340;&#65292;&#22240;&#27492;&#36866;&#24212;&#20102;&#24314;&#27169;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#25552;&#20986;&#30340;&#21518;&#39564;&#36827;&#34892;&#28508;&#22312;&#30340;$\mathcal{GP}$&#26680;&#24515;&#38598;&#21464;&#37327;&#30340;&#36793;&#32536;&#21270;&#65292;&#25512;&#23548;&#20986;CVTGP&#30340;&#23545;&#25968;&#36793;&#38469;&#20284;&#28982;&#19979;&#30028;&#65292;&#24182;&#19988;&#35777;&#26126;&#20854;&#36866;&#29992;&#20110;&#38543;&#26426;&#20248;&#21270;&#12290;CVTGP&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;&#26680;&#24515;&#38598;&#30340;&#28201;&#21644;&#21518;&#39564;&#26469;&#20943;&#23567;&#21487;&#23398;&#20064;&#21442;&#25968;&#30340;&#22823;&#23567;&#21040;$\mathcal{O}(M)$&#65292;&#20855;&#26377;&#25968;&#20540;&#31283;&#23450;&#24615;&#65292;&#24182;&#19988;&#36890;&#36807;&#25552;&#20379;&#31232;&#30095;&#19988;&#21487;&#35299;&#37322;&#30340;&#25968;&#25454;&#34920;&#31034;&#26469;&#20445;&#25345;$\mathcal{O}(M^3)$&#26102;&#38388;&#22797;&#26434;&#24230;&#21644;$\mathcal{O}(M^2)$&#31354;&#38388;&#22797;&#26434;&#24230;&#12290;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#22238;&#24402;&#38382;&#39064;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20102;CVTGP&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel stochastic variational Gaussian process ($\mathcal{GP}$) inference method, based on a posterior over a learnable set of weighted pseudo input-output points (coresets). Instead of a free-form variational family, the proposed coreset-based, variational tempered family for $\mathcal{GP}$s (CVTGP) is defined in terms of the $\mathcal{GP}$ prior and the data-likelihood; hence, accommodating the modeling inductive biases. We derive CVTGP's lower bound for the log-marginal likelihood via marginalization of the proposed posterior over latent $\mathcal{GP}$ coreset variables, and show it is amenable to stochastic optimization. CVTGP reduces the learnable parameter size to $\mathcal{O}(M)$, enjoys numerical stability, and maintains $\mathcal{O}(M^3)$ time- and $\mathcal{O}(M^2)$ space-complexity, by leveraging a coreset-based tempered posterior that, in turn, provides sparse and explainable representations of the data. Results on simulated and real-world regression problems wi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#20998;&#26512;&#20197;&#22826;&#22346;&#32593;&#32476;&#20013;&#30340;&#20449;&#24687;&#20256;&#25773;&#27169;&#24335;&#65292;&#24182;&#20248;&#21270;&#32593;&#32476;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#26368;&#32456;&#30446;&#26631;&#26159;&#23398;&#20064;&#22312;&#19981;&#21516;&#32593;&#32476;&#29366;&#24577;&#19979;&#37319;&#21462;&#30340;&#26368;&#20339;&#34892;&#21160;&#65292;&#25552;&#39640;&#32593;&#32476;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01406</link><description>&lt;p&gt;
&#20351;&#29992;&#32508;&#21512;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#21644;&#24378;&#21270;&#23398;&#20064;&#20998;&#26512;&#20197;&#22826;&#22346;&#32593;&#32476;&#20013;&#30340;&#20449;&#24687;&#20256;&#25773;&#65292;&#20197;&#20248;&#21270;&#32593;&#32476;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;
&lt;/p&gt;
&lt;p&gt;
Analysis of Information Propagation in Ethereum Network Using Combined Graph Attention Network and Reinforcement Learning to Optimize Network Efficiency and Scalability. (arXiv:2311.01406v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#20998;&#26512;&#20197;&#22826;&#22346;&#32593;&#32476;&#20013;&#30340;&#20449;&#24687;&#20256;&#25773;&#27169;&#24335;&#65292;&#24182;&#20248;&#21270;&#32593;&#32476;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#26368;&#32456;&#30446;&#26631;&#26159;&#23398;&#20064;&#22312;&#19981;&#21516;&#32593;&#32476;&#29366;&#24577;&#19979;&#37319;&#21462;&#30340;&#26368;&#20339;&#34892;&#21160;&#65292;&#25552;&#39640;&#32593;&#32476;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21306;&#22359;&#38142;&#25216;&#26415;&#22312;&#20998;&#24067;&#24335;&#32593;&#32476;&#20013;&#38761;&#26032;&#20102;&#20449;&#24687;&#20256;&#25773;&#30340;&#26041;&#24335;&#12290;&#20197;&#22826;&#22346;&#22312;&#25512;&#21160;&#26234;&#33021;&#21512;&#32422;&#21644;&#20998;&#24067;&#24335;&#24212;&#29992;&#26041;&#38754;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;&#20102;&#35299;&#20197;&#22826;&#22346;&#20013;&#30340;&#20449;&#24687;&#20256;&#25773;&#21160;&#24577;&#23545;&#20110;&#30830;&#20445;&#32593;&#32476;&#25928;&#29575;&#12289;&#23433;&#20840;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCNs&#65289;&#20998;&#26512;&#20197;&#22826;&#22346;&#32593;&#32476;&#20013;&#30340;&#20449;&#24687;&#20256;&#25773;&#27169;&#24335;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#30340;&#31532;&#19968;&#38454;&#27573;&#28041;&#21450;&#20174;&#20197;&#22826;&#22346;&#21306;&#22359;&#38142;&#20013;&#25910;&#38598;&#25968;&#25454;&#65292;&#21253;&#25324;&#21306;&#22359;&#12289;&#20132;&#26131;&#21644;&#33410;&#28857;&#24230;&#12290;&#25105;&#20204;&#20351;&#29992;&#37051;&#25509;&#30697;&#38453;&#26500;&#24314;&#20102;&#19968;&#20010;&#20132;&#26131;&#22270;&#34920;&#31034;&#65292;&#20197;&#25429;&#25417;&#33410;&#28857;&#23884;&#20837;&#65307;&#32780;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#24320;&#21457;&#20102;&#19968;&#20010;&#32508;&#21512;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;GAT&#65289;&#21644;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#27169;&#22411;&#26469;&#20248;&#21270;&#32593;&#32476;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#23427;&#23398;&#20064;&#22312;&#19981;&#21516;&#32593;&#32476;&#29366;&#24577;&#19979;&#37319;&#21462;&#30340;&#26368;&#20339;&#34892;&#21160;&#65292;&#26368;&#32456;&#23548;&#33268;&#32593;&#32476;&#25928;&#29575;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Blockchain technology has revolutionized the way information is propagated in decentralized networks. Ethereum plays a pivotal role in facilitating smart contracts and decentralized applications. Understanding information propagation dynamics in Ethereum is crucial for ensuring network efficiency, security, and scalability. In this study, we propose an innovative approach that utilizes Graph Convolutional Networks (GCNs) to analyze the information propagation patterns in the Ethereum network. The first phase of our research involves data collection from the Ethereum blockchain, consisting of blocks, transactions, and node degrees. We construct a transaction graph representation using adjacency matrices to capture the node embeddings; while our major contribution is to develop a combined Graph Attention Network (GAT) and Reinforcement Learning (RL) model to optimize the network efficiency and scalability. It learns the best actions to take in various network states, ultimately leading t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#25105;&#30417;&#30563;&#26631;&#35760;&#22270;&#20687;&#26469;&#23398;&#20064;&#26426;&#22120;&#20154;&#23545;&#22320;&#24418;&#30340;&#29289;&#29702;&#24615;&#36136;&#36827;&#34892;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20027;&#21160;&#24863;&#30693;&#36816;&#21160;&#31574;&#30053;&#65292;&#26426;&#22120;&#20154;&#33021;&#22815;&#22686;&#21152;&#29289;&#29702;&#21442;&#25968;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01405</link><description>&lt;p&gt;
&#23398;&#20064;&#36890;&#36807;&#20027;&#21160;&#24863;&#30693;&#36816;&#21160;&#31574;&#30053;&#26469;&#35266;&#23519;&#29289;&#29702;&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;
Learning to See Physical Properties with Active Sensing Motor Policies. (arXiv:2311.01405v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01405
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#25105;&#30417;&#30563;&#26631;&#35760;&#22270;&#20687;&#26469;&#23398;&#20064;&#26426;&#22120;&#20154;&#23545;&#22320;&#24418;&#30340;&#29289;&#29702;&#24615;&#36136;&#36827;&#34892;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20027;&#21160;&#24863;&#30693;&#36816;&#21160;&#31574;&#30053;&#65292;&#26426;&#22120;&#20154;&#33021;&#22815;&#22686;&#21152;&#29289;&#29702;&#21442;&#25968;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#24425;&#33394;&#22270;&#20687;&#20013;&#25512;&#26029;&#20986;&#22320;&#24418;&#30340;&#29289;&#29702;&#24615;&#36136;&#21487;&#20197;&#24110;&#21161;&#21046;&#23450;&#39640;&#25928;&#30340;&#26426;&#22120;&#20154;&#36816;&#21160;&#35745;&#21010;&#12290;&#28982;&#32780;&#65292;&#19982;&#22270;&#20687;&#20998;&#31867;&#19981;&#21516;&#65292;&#20154;&#31867;&#38590;&#20197;&#30452;&#35266;&#22320;&#23558;&#22270;&#20687;&#22359;&#26631;&#35760;&#20026;&#29289;&#29702;&#24615;&#36136;&#12290;&#22312;&#27809;&#26377;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#26500;&#24314;&#19968;&#20010;&#20197;&#35266;&#27979;&#21040;&#30340;&#22320;&#24418;&#20026;&#36755;&#20837;&#24182;&#39044;&#27979;&#29289;&#29702;&#24615;&#36136;&#30340;&#35270;&#35273;&#31995;&#32479;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26426;&#22120;&#20154;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#36941;&#21382;&#26399;&#38388;&#25429;&#33719;&#30340;&#22270;&#20687;&#36827;&#34892;&#33258;&#25105;&#30417;&#30563;&#26631;&#35760;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#22312;&#27169;&#25311;&#20013;&#35757;&#32451;&#30340;&#29289;&#29702;&#24615;&#36136;&#20272;&#35745;&#22120;&#26469;&#35757;&#32451;&#23427;&#12290;&#20026;&#20102;&#30830;&#20445;&#20934;&#30830;&#26631;&#35760;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20027;&#21160;&#24863;&#30693;&#36816;&#21160;&#31574;&#30053;&#65288;ASMP&#65289;&#65292;&#35813;&#31574;&#30053;&#34987;&#35757;&#32451;&#20026;&#25506;&#32034;&#22686;&#21152;&#29289;&#29702;&#21442;&#25968;&#20272;&#35745;&#20934;&#30830;&#24230;&#30340;&#36816;&#21160;&#34892;&#20026;&#12290;&#20363;&#22914;&#65292;&#22235;&#36275;&#26426;&#22120;&#20154;&#23398;&#20250;&#21047;&#33050;&#19982;&#22320;&#38754;&#25509;&#35302;&#20197;&#20934;&#30830;&#20272;&#35745;&#25705;&#25830;&#31995;&#25968;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#23569;&#37327;&#23454;&#38469;&#29615;&#22659;&#36941;&#21382;&#25968;&#25454;&#35757;&#32451;&#30340;&#35270;&#35273;&#31995;&#32479;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge of terrain's physical properties inferred from color images can aid in making efficient robotic locomotion plans. However, unlike image classification, it is unintuitive for humans to label image patches with physical properties. Without labeled data, building a vision system that takes as input the observed terrain and predicts physical properties remains challenging. We present a method that overcomes this challenge by self-supervised labeling of images captured by robots during real-world traversal with physical property estimators trained in simulation. To ensure accurate labeling, we introduce Active Sensing Motor Policies (ASMP), which are trained to explore locomotion behaviors that increase the accuracy of estimating physical parameters. For instance, the quadruped robot learns to swipe its foot against the ground to estimate the friction coefficient accurately. We show that the visual system trained with a small amount of real-world traversal data accurately predicts
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20197;&#32447;&#24615;&#25511;&#21046;&#31070;&#32463;ODE&#30340;&#27969;&#21160;&#20316;&#20026;&#24402;&#19968;&#21270;&#27969;&#26500;&#36896;&#26368;&#20248;&#20256;&#36755;&#26144;&#23556;&#30340;&#36817;&#20284;&#12290;&#36890;&#36807;&#31163;&#25955;&#26368;&#20248;&#32806;&#21512;&#38382;&#39064;&#21644;&#25968;&#20540;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#23545;&#26368;&#20248;&#20256;&#36755;&#26144;&#23556;&#30340;&#36817;&#20284;&#12290;&#26368;&#32456;&#32467;&#26524;&#26377;&#21161;&#20110;&#26500;&#24314;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21487;&#36870;&#20256;&#36755;&#26144;&#23556;&#12290;</title><link>http://arxiv.org/abs/2311.01404</link><description>&lt;p&gt;
&#20351;&#29992;&#32447;&#24615;&#25511;&#21046;&#31070;&#32463;ODE&#23558;&#24402;&#19968;&#21270;&#27969;&#20316;&#20026;&#26368;&#20248;&#20256;&#36755;&#26144;&#23556;&#30340;&#36817;&#20284;
&lt;/p&gt;
&lt;p&gt;
Normalizing flows as approximations of optimal transport maps via linear-control neural ODEs. (arXiv:2311.01404v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01404
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20197;&#32447;&#24615;&#25511;&#21046;&#31070;&#32463;ODE&#30340;&#27969;&#21160;&#20316;&#20026;&#24402;&#19968;&#21270;&#27969;&#26500;&#36896;&#26368;&#20248;&#20256;&#36755;&#26144;&#23556;&#30340;&#36817;&#20284;&#12290;&#36890;&#36807;&#31163;&#25955;&#26368;&#20248;&#32806;&#21512;&#38382;&#39064;&#21644;&#25968;&#20540;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#23545;&#26368;&#20248;&#20256;&#36755;&#26144;&#23556;&#30340;&#36817;&#20284;&#12290;&#26368;&#32456;&#32467;&#26524;&#26377;&#21161;&#20110;&#26500;&#24314;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21487;&#36870;&#20256;&#36755;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
"&#24402;&#19968;&#21270;&#27969;"&#19968;&#35789;&#19982;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#27010;&#29575;&#27979;&#24230;&#20043;&#38388;&#30340;&#21487;&#36870;&#20256;&#36755;&#26144;&#23556;&#30456;&#20851;&#12290;&#26412;&#25991;&#32771;&#34385;&#23558;$W_2$-&#26368;&#20248;&#20256;&#36755;&#26144;&#23556;$T$&#24674;&#22797;&#20026;&#32447;&#24615;&#25511;&#21046;&#31070;&#32463;ODE&#30340;&#27969;&#21160;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#22312;&#21512;&#36866;&#30340;&#20551;&#35774;&#19979;&#65292;&#23545;&#20110;&#32477;&#23545;&#36830;&#32493;&#27979;&#24230;$\mu,\nu\in\mathcal{P}(\mathbb{R}^n)$&#21644;&#21463;&#25511;&#21521;&#37327;&#22330;&#65292;&#26368;&#20248;&#20256;&#36755;&#26144;&#23556;&#21253;&#21547;&#22312;&#31995;&#32479;&#20135;&#29983;&#30340;&#27969;&#21160;&#30340;$C^0_c$&#38381;&#21253;&#20013;&#12290;&#20551;&#35774;&#21407;&#22987;&#27979;&#24230;$\mu,\nu$&#30340;&#31163;&#25955;&#36817;&#20284;$\mu_N,\nu_N$&#21487;&#29992;&#65292;&#25105;&#20204;&#20351;&#29992;&#31163;&#25955;&#26368;&#20248;&#32806;&#21512;$\gamma_N$&#26469;&#23450;&#20041;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#12290;&#36890;&#36807;$\Gamma$-&#25910;&#25947;&#35770;&#35777;&#65292;&#25105;&#20204;&#35777;&#26126;&#20854;&#35299;&#23545;&#24212;&#20110;&#36817;&#20284;&#26368;&#20248;&#20256;&#36755;&#26144;&#23556;$T$&#30340;&#27969;&#21160;&#12290;&#26368;&#21518;&#65292;&#21033;&#29992;Pontryagin&#26368;&#22823;&#21407;&#29702;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#25968;&#20540;&#26041;&#26696;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The term "Normalizing Flows" is related to the task of constructing invertible transport maps between probability measures by means of deep neural networks. In this paper, we consider the problem of recovering the $W_2$-optimal transport map $T$ between absolutely continuous measures $\mu,\nu\in\mathcal{P}(\mathbb{R}^n)$ as the flow of a linear-control neural ODE. We first show that, under suitable assumptions on $\mu,\nu$ and on the controlled vector fields, the optimal transport map is contained in the $C^0_c$-closure of the flows generated by the system. Assuming that discrete approximations $\mu_N,\nu_N$ of the original measures $\mu,\nu$ are available, we use a discrete optimal coupling $\gamma_N$ to define an optimal control problem. With a $\Gamma$-convergence argument, we prove that its solutions correspond to flows that approximate the optimal transport map $T$. Finally, taking advantage of the Pontryagin Maximum Principle, we propose an iterative numerical scheme for the reso
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#38381;&#29615;&#20013;&#23398;&#20064;&#30495;&#23454;&#20132;&#36890;&#20195;&#29702;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#27169;&#20223;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#65292;&#36890;&#36807;&#21305;&#37197;&#19987;&#23478;&#28436;&#31034;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#26356;&#21152;&#30495;&#23454;&#21644;&#21512;&#35268;&#30340;&#39550;&#39542;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2311.01394</link><description>&lt;p&gt;
&#22312;&#38381;&#29615;&#20013;&#23398;&#20064;&#30495;&#23454;&#20132;&#36890;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Learning Realistic Traffic Agents in Closed-loop. (arXiv:2311.01394v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01394
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#38381;&#29615;&#20013;&#23398;&#20064;&#30495;&#23454;&#20132;&#36890;&#20195;&#29702;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#27169;&#20223;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#65292;&#36890;&#36807;&#21305;&#37197;&#19987;&#23478;&#28436;&#31034;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#26356;&#21152;&#30495;&#23454;&#21644;&#21512;&#35268;&#30340;&#39550;&#39542;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30495;&#23454;&#30340;&#20132;&#36890;&#27169;&#25311;&#20013;&#23398;&#20064;&#20132;&#36890;&#20195;&#29702;&#23545;&#20110;&#22312;&#23454;&#38469;&#37096;&#32626;&#21069;&#20197;&#23433;&#20840;&#21644;&#21487;&#25193;&#23637;&#30340;&#26041;&#24335;&#24320;&#21457;&#33258;&#21160;&#39550;&#39542;&#36719;&#20214;&#33267;&#20851;&#37325;&#35201;&#12290;&#36890;&#24120;&#65292;&#27169;&#20223;&#23398;&#20064;(Imitation Learning, IL)&#29992;&#20110;&#30452;&#25509;&#20174;&#31163;&#32447;&#25910;&#38598;&#30340;&#30495;&#23454;&#35266;&#27979;&#20013;&#23398;&#20064;&#31867;&#20284;&#20154;&#31867;&#30340;&#20132;&#36890;&#20195;&#29702;&#65292;&#20294;&#22312;&#27809;&#26377;&#26126;&#30830;&#20132;&#36890;&#35268;&#21017;&#30340;&#24773;&#20917;&#19979;&#65292;&#20165;&#36890;&#36807;IL&#35757;&#32451;&#30340;&#20195;&#29702;&#32463;&#24120;&#34920;&#29616;&#20986;&#30896;&#25758;&#21644;&#20559;&#31163;&#36947;&#36335;&#31561;&#19981;&#30495;&#23454;&#30340;&#36829;&#35268;&#34892;&#20026;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#20998;&#24067;&#19981;&#21516;&#21644;&#38271;&#23614;&#22330;&#26223;&#19979;&#34987;&#25918;&#22823;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#24378;&#21270;&#23398;&#20064;(Reinforcement Learning, RL)&#21487;&#20197;&#35757;&#32451;&#20132;&#36890;&#20195;&#29702;&#36991;&#20813;&#36829;&#35268;&#34892;&#20026;&#65292;&#20294;&#20165;&#20351;&#29992;RL&#20250;&#23548;&#33268;&#19981;&#20687;&#20154;&#31867;&#39550;&#39542;&#34892;&#20026;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#24378;&#21270;&#20132;&#36890;&#35268;&#21017;(Reinforcing Traffic Rules, RTR)&#65292;&#36825;&#26159;&#19968;&#20010;&#32508;&#21512;&#30340;&#38381;&#29615;&#23398;&#20064;&#30446;&#26631;&#65292;&#20197;&#22312;&#20132;&#36890;&#21512;&#35268;&#32422;&#26463;&#19979;&#21305;&#37197;&#19987;&#23478;&#28436;&#31034;&#65292;&#33258;&#28982;&#22320;&#20135;&#29983;&#20102;&#19968;&#31181;&#32852;&#21512;IL+RL&#26041;&#27861;&#65292;&#20860;&#20855;&#20108;&#32773;&#30340;&#20248;&#28857;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#30495;&#23454;&#22330;&#26223;&#21644;&#38381;&#29615;&#27169;&#25311;&#20013;&#23398;&#20064;&#65292;
&lt;/p&gt;
&lt;p&gt;
Realistic traffic simulation is crucial for developing self-driving software in a safe and scalable manner prior to real-world deployment. Typically, imitation learning (IL) is used to learn human-like traffic agents directly from real-world observations collected offline, but without explicit specification of traffic rules, agents trained from IL alone frequently display unrealistic infractions like collisions and driving off the road. This problem is exacerbated in out-of-distribution and long-tail scenarios. On the other hand, reinforcement learning (RL) can train traffic agents to avoid infractions, but using RL alone results in unhuman-like driving behaviors. We propose Reinforcing Traffic Rules (RTR), a holistic closed-loop learning objective to match expert demonstrations under a traffic compliance constraint, which naturally gives rise to a joint IL + RL approach, obtaining the best of both worlds. Our method learns in closed-loop simulations of both nominal scenarios from real
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31181;&#29983;&#25104;&#26694;&#26550;&#65292;&#26088;&#22312;&#23558;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#26174;&#24335;&#36716;&#25442;&#20998;&#24067;&#21644;&#22522;&#20110;&#23545;&#25239;&#35757;&#32451;&#30340;&#38544;&#24335;&#36716;&#25442;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#23545;&#27604;&#20272;&#35745;&#35757;&#32451;&#20840;&#23616;&#33021;&#37327;&#27169;&#22411;&#65292;&#24182;&#20248;&#21270;&#26412;&#22320;&#36716;&#25442;&#31574;&#30053;&#26469;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2311.01388</link><description>&lt;p&gt;
&#23545;&#27604;&#27169;&#20223;&#30340;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Time-series Generation by Contrastive Imitation. (arXiv:2311.01388v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01388
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31181;&#29983;&#25104;&#26694;&#26550;&#65292;&#26088;&#22312;&#23558;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#26174;&#24335;&#36716;&#25442;&#20998;&#24067;&#21644;&#22522;&#20110;&#23545;&#25239;&#35757;&#32451;&#30340;&#38544;&#24335;&#36716;&#25442;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#23545;&#27604;&#20272;&#35745;&#35757;&#32451;&#20840;&#23616;&#33021;&#37327;&#27169;&#22411;&#65292;&#24182;&#20248;&#21270;&#26412;&#22320;&#36716;&#25442;&#31574;&#30053;&#26469;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32771;&#34385;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#24207;&#21015;&#35774;&#32622;&#25552;&#20986;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#25361;&#25112;&#65306;&#29983;&#25104;&#22120;&#19981;&#20165;&#24212;&#35813;&#25429;&#25417;&#65288;&#36880;&#27493;&#65289;&#36716;&#25442;&#30340;&#26465;&#20214;&#21160;&#21147;&#23398;&#65292;&#32780;&#19988;&#20854;&#24320;&#29615;&#22238;&#28378;&#24212;&#35813;&#20445;&#25345;&#65288;&#22810;&#27493;&#65289;&#36712;&#36857;&#30340;&#32852;&#21512;&#20998;&#24067;&#12290;&#19968;&#26041;&#38754;&#65292;MLE&#35757;&#32451;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#20801;&#35768;&#23398;&#20064;&#21644;&#35745;&#31639;&#26174;&#24335;&#30340;&#36716;&#25442;&#20998;&#24067;&#65292;&#20294;&#22312;&#22238;&#28378;&#36807;&#31243;&#20013;&#20250;&#21463;&#21040;&#22797;&#21512;&#35823;&#24046;&#30340;&#24433;&#21709;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22522;&#20110;GAN&#35757;&#32451;&#30340;&#23545;&#25239;&#27169;&#22411;&#20943;&#36731;&#20102;&#36825;&#31181;&#26292;&#38706;&#20559;&#24046;&#65292;&#20294;&#36716;&#25442;&#26159;&#38544;&#24335;&#30340;&#19988;&#38590;&#20197;&#35780;&#20272;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#26088;&#22312;&#32467;&#21512;&#20004;&#32773;&#20248;&#21183;&#30340;&#29983;&#25104;&#26694;&#26550;&#65306;&#21463;&#21040;&#21305;&#37197;&#30697;&#27861;&#30340;&#30446;&#26631;&#28608;&#21457;&#65292;&#25105;&#20204;&#20248;&#21270;&#19968;&#20010;&#26412;&#22320;&#65288;&#20294;&#21069;&#30651;&#24615;&#30340;&#65289;&#36716;&#25442;&#31574;&#30053;&#65292;&#20854;&#20013;&#24378;&#21270;&#20449;&#21495;&#30001;&#20840;&#23616;&#65288;&#20294;&#21487;&#36880;&#27493;&#20998;&#35299;&#65289;&#33021;&#37327;&#27169;&#22411;&#36890;&#36807;&#23545;&#27604;&#20272;&#35745;&#35757;&#32451;&#25552;&#20379;&#12290;&#22312;&#35757;&#32451;&#20013;&#65292;&#20004;&#20010;&#32452;&#20214;&#34987;&#23398;&#20064;&#29983;&#25104;&#23545;&#25239;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Consider learning a generative model for time-series data. The sequential setting poses a unique challenge: Not only should the generator capture the conditional dynamics of (stepwise) transitions, but its open-loop rollouts should also preserve the joint distribution of (multi-step) trajectories. On one hand, autoregressive models trained by MLE allow learning and computing explicit transition distributions, but suffer from compounding error during rollouts. On the other hand, adversarial models based on GAN training alleviate such exposure bias, but transitions are implicit and hard to assess. In this work, we study a generative framework that seeks to combine the strengths of both: Motivated by a moment-matching objective to mitigate compounding error, we optimize a local (but forward-looking) transition policy, where the reinforcement signal is provided by a global (but stepwise-decomposable) energy model trained by contrastive estimation. At training, the two components are learne
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35821;&#35328;&#26465;&#20214;&#30340;&#25805;&#20316;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#22312;&#20302;&#24615;&#33021;&#24179;&#21488;&#19978;&#30340;&#26377;&#25928;&#27169;&#20223;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2311.01378</link><description>&lt;p&gt;
Vision-Language Foundation Models&#20316;&#20026;&#26377;&#25928;&#30340;&#26426;&#22120;&#20154;&#27169;&#20223;&#32773;
&lt;/p&gt;
&lt;p&gt;
Vision-Language Foundation Models as Effective Robot Imitators. (arXiv:2311.01378v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01378
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35821;&#35328;&#26465;&#20214;&#30340;&#25805;&#20316;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#22312;&#20302;&#24615;&#33021;&#24179;&#21488;&#19978;&#30340;&#26377;&#25928;&#27169;&#20223;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#26041;&#38754;&#30340;&#36827;&#23637;&#26174;&#31034;&#20986;&#23427;&#20204;&#29702;&#35299;&#22810;&#27169;&#24577;&#25968;&#25454;&#21644;&#35299;&#20915;&#22797;&#26434;&#30340;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#65288;&#21253;&#25324;&#26426;&#22120;&#20154;&#25805;&#20316;&#65289;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23547;&#27714;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#24335;&#26469;&#21033;&#29992;&#29616;&#26377;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#22312;&#26426;&#22120;&#20154;&#25968;&#25454;&#19978;&#36827;&#34892;&#31616;&#21333;&#24494;&#35843;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26032;&#39062;&#30340;&#35270;&#35273;&#35821;&#35328;&#25805;&#20316;&#26694;&#26550;&#65292;&#21517;&#20026;RoboFlamingo&#65292;&#23427;&#24314;&#31435;&#22312;&#24320;&#28304;&#30340;VLMs&#65292;OpenFlamingo&#20043;&#19978;&#12290;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;RoboFlamingo&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;VLMs&#36827;&#34892;&#21333;&#27493;&#35270;&#35273;&#35821;&#35328;&#29702;&#35299;&#65292;&#20351;&#29992;&#26174;&#24335;&#31574;&#30053;&#22836;&#27169;&#25311;&#39034;&#24207;&#21382;&#21490;&#20449;&#24687;&#65292;&#24182;&#21482;&#22312;&#35821;&#35328;&#26465;&#20214;&#30340;&#25805;&#20316;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#36825;&#31181;&#20998;&#35299;&#20026;RoboFlamingo&#25552;&#20379;&#20102;&#22312;&#20302;&#24615;&#33021;&#24179;&#21488;&#19978;&#36827;&#34892;&#24320;&#29615;&#25511;&#21046;&#21644;&#37096;&#32626;&#30340;&#28789;&#27963;&#24615;&#12290;&#36890;&#36807;&#22312;&#27979;&#35797;&#22522;&#20934;&#19978;&#22823;&#24133;&#36229;&#36807;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;RoboFlamingo&#21487;&#20197;&#25104;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#26426;&#22120;&#20154;&#27169;&#20223;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent progress in vision language foundation models has shown their ability to understand multimodal data and resolve complicated vision language tasks, including robotics manipulation. We seek a straightforward way of making use of existing vision-language models (VLMs) with simple fine-tuning on robotics data. To this end, we derive a simple and novel vision-language manipulation framework, dubbed RoboFlamingo, built upon the open-source VLMs, OpenFlamingo. Unlike prior works, RoboFlamingo utilizes pre-trained VLMs for single-step vision-language comprehension, models sequential history information with an explicit policy head, and is slightly fine-tuned by imitation learning only on language-conditioned manipulation datasets. Such a decomposition provides RoboFlamingo the flexibility for open-loop control and deployment on low-performance platforms. By exceeding the state-of-the-art performance with a large margin on the tested benchmark, we show RoboFlamingo can be an effective an
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Gromov-Monge&#23884;&#20837;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#35782;&#21035;&#25968;&#25454;&#32972;&#21518;&#30340;&#24213;&#23618;&#32467;&#26500;&#65292;&#24182;&#23558;&#20854;&#26144;&#23556;&#21040;&#20302;&#32500;&#28508;&#31354;&#38388;&#20013;&#65292;&#35299;&#20915;&#20102;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#20013;&#23545;&#21021;&#22987;&#26465;&#20214;&#25935;&#24863;&#24615;&#21644;&#27169;&#24335;&#23849;&#28291;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.01375</link><description>&lt;p&gt;
&#36890;&#36807;Gromov-Monge&#23884;&#20837;&#23454;&#29616;&#21333;&#35843;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Monotone Generative Modeling via a Gromov-Monge Embedding. (arXiv:2311.01375v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01375
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Gromov-Monge&#23884;&#20837;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#35782;&#21035;&#25968;&#25454;&#32972;&#21518;&#30340;&#24213;&#23618;&#32467;&#26500;&#65292;&#24182;&#23558;&#20854;&#26144;&#23556;&#21040;&#20302;&#32500;&#28508;&#31354;&#38388;&#20013;&#65292;&#35299;&#20915;&#20102;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#20013;&#23545;&#21021;&#22987;&#26465;&#20214;&#25935;&#24863;&#24615;&#21644;&#27169;&#24335;&#23849;&#28291;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#26159;&#21019;&#24314;&#26032;&#20869;&#23481;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#20294;&#38754;&#20020;&#30528;&#23545;&#21021;&#22987;&#26465;&#20214;&#30340;&#25935;&#24863;&#24615;&#21644;&#27169;&#24335;&#23849;&#28291;&#31561;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Gromov-Monge&#23884;&#20837;&#65288;GME&#65289;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#12290;&#23427;&#24110;&#21161;&#35782;&#21035;&#25968;&#25454;&#32972;&#21518;&#30340;&#24213;&#23618;&#27979;&#24230;&#30340;&#20302;&#32500;&#32467;&#26500;&#65292;&#28982;&#21518;&#23558;&#20854;&#26144;&#23556;&#21040;&#20445;&#25345;&#20960;&#20309;&#24615;&#36136;&#30340;&#20302;&#32500;&#28508;&#31354;&#38388;&#20013;&#30340;&#19968;&#20010;&#27979;&#24230;&#65292;&#24182;&#23558;&#20854;&#26368;&#20248;&#22320;&#20256;&#36755;&#21040;&#21442;&#32771;&#27979;&#24230;&#12290;&#36890;&#36807;GME&#30340;&#20445;&#25345;&#24213;&#23618;&#20960;&#20309;&#24615;&#36136;&#21644;&#29983;&#25104;&#26144;&#23556;&#30340;$c$-&#21608;&#26399;&#24615;&#21333;&#35843;&#24615;&#26469;&#20445;&#35777;&#23427;&#20204;&#12290;&#21518;&#19968;&#29305;&#24615;&#26159;&#30830;&#20445;&#26356;&#22909;&#30340;&#21442;&#25968;&#21021;&#22987;&#21270;&#40065;&#26834;&#24615;&#21644;&#27169;&#24335;&#23849;&#28291;&#30340;&#31532;&#19968;&#27493;&#12290;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#12289;&#36991;&#20813;&#27169;&#24335;&#23849;&#28291;&#21644;&#23545;&#19981;&#21516;&#36215;&#22987;&#26465;&#20214;&#20855;&#26377;&#40065;&#26834;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Adversarial Networks (GANs) are powerful tools for creating new content, but they face challenges such as sensitivity to starting conditions and mode collapse. To address these issues, we propose a deep generative model that utilizes the Gromov-Monge embedding (GME). It helps identify the low-dimensional structure of the underlying measure of the data and then maps it, while preserving its geometry, into a measure in a low-dimensional latent space, which is then optimally transported to the reference measure. We guarantee the preservation of the underlying geometry by the GME and $c$-cyclical monotonicity of the generative map, where $c$ is an intrinsic embedding cost employed by the GME. The latter property is a first step in guaranteeing better robustness to initialization of parameters and mode collapse. Numerical experiments demonstrate the effectiveness of our approach in generating high-quality images, avoiding mode collapse, and exhibiting robustness to different star
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#21453;&#23556;&#32418;&#22806;&#20809;&#27874;&#20449;&#21495;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#21033;&#29992;&#20302;&#25104;&#26412;&#20809;&#28304;&#21644;&#20256;&#24863;&#22120;&#36827;&#34892;&#38750;&#25509;&#35302;&#21628;&#21560;&#24322;&#24120;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;96.6%&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#65292;&#24182;&#33021;&#22815;&#26816;&#27979;&#21040;&#38169;&#35823;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2311.01367</link><description>&lt;p&gt;
&#21033;&#29992;&#21453;&#23556;&#32418;&#22806;&#20809;&#27874;&#20449;&#21495;&#36827;&#34892;&#21628;&#21560;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Respiratory Anomaly Detection using Reflected Infrared Light-wave Signals. (arXiv:2311.01367v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01367
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#21453;&#23556;&#32418;&#22806;&#20809;&#27874;&#20449;&#21495;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#21033;&#29992;&#20302;&#25104;&#26412;&#20809;&#28304;&#21644;&#20256;&#24863;&#22120;&#36827;&#34892;&#38750;&#25509;&#35302;&#21628;&#21560;&#24322;&#24120;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;96.6%&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#65292;&#24182;&#33021;&#22815;&#26816;&#27979;&#21040;&#38169;&#35823;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26426;&#26800;&#26426;&#22120;&#20154;&#33016;&#37096;&#21453;&#23556;&#30340;&#38750;&#30456;&#24178;&#20809;&#27874;&#20449;&#21495;&#36827;&#34892;&#38750;&#25509;&#35302;&#21628;&#21560;&#24322;&#24120;&#26816;&#27979;&#30340;&#26041;&#27861;&#12290;&#19982;&#29616;&#26377;&#30340;&#38647;&#36798;&#21644;&#25668;&#20687;&#22836;&#24863;&#24212;&#31995;&#32479;&#30456;&#27604;&#65292;&#36825;&#39033;&#25216;&#26415;&#21482;&#20351;&#29992;&#20102;&#20302;&#25104;&#26412;&#30340;&#26222;&#36941;&#20809;&#28304;&#65288;&#22914;&#32418;&#22806;&#21457;&#20809;&#20108;&#26497;&#31649;&#65289;&#21644;&#20256;&#24863;&#22120;&#65288;&#22914;&#20809;&#30005;&#26816;&#27979;&#22120;&#65289;&#12290;&#36825;&#20010;&#20809;&#27874;&#24863;&#24212;&#65288;LWS&#65289;&#31995;&#32479;&#36890;&#36807;&#27979;&#37327;&#26426;&#22120;&#20154;&#33016;&#37096;&#21453;&#23556;&#30340;&#20809;&#24378;&#21464;&#21270;&#26469;&#35782;&#21035;&#19981;&#21516;&#30340;&#21628;&#21560;&#24322;&#24120;&#65292;&#22312;0.5&#31859;&#33267;1.5&#31859;&#33539;&#22260;&#20869;&#12290;&#35813;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#33021;&#22815;&#20197;96.6%&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#20998;&#31867;7&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#21628;&#21560;&#25968;&#25454;&#12290;&#35813;&#27169;&#22411;&#36824;&#21487;&#20197;&#26816;&#27979;&#21040;&#31995;&#32479;&#25910;&#38598;&#30340;&#19981;&#21253;&#21547;&#21628;&#21560;&#20449;&#24687;&#30340;&#38169;&#35823;&#25968;&#25454;&#12290;&#24320;&#21457;&#30340;&#31995;&#32479;&#21487;&#20197;&#20316;&#20026;&#26234;&#33021;&#12289;&#38750;&#25509;&#35302;&#21644;&#38544;&#34109;&#30340;&#21628;&#21560;&#30417;&#27979;&#26041;&#27861;&#65292;&#22312;&#23478;&#24237;&#25110;&#21307;&#30103;&#26426;&#26500;&#20013;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we present a non-contact respiratory anomaly detection method using incoherent light-wave signals reflected from the chest of a mechanical robot that can breathe like human beings. In comparison to existing radar and camera-based sensing systems for vitals monitoring, this technology uses only a low-cost ubiquitous light source (e.g., infrared light emitting diode) and sensor (e.g., photodetector). This light-wave sensing (LWS) system recognizes different breathing anomalies from the variations of light intensity reflected from the chest of the robot within a 0.5m-1.5m range. The anomaly detection model demonstrates up to 96.6% average accuracy in classifying 7 different types of breathing data using machine learning. The model can also detect faulty data collected by the system that does not contain breathing information. The developed system can be utilized at home or healthcare facilities as a smart, non-contact and discreet respiration monitoring method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;Lipschitz&#24120;&#25968;&#65292;&#23545;&#20110;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;Lipschitz&#24120;&#25968;&#30340;&#31934;&#30830;&#21051;&#30011;&#65292;&#23545;&#20110;&#36275;&#22815;&#23485;&#24230;&#30340;&#28145;&#23618;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#19978;&#19979;&#30028;&#65292;&#24182;&#21305;&#37197;&#19968;&#20010;&#20381;&#36182;&#20110;&#28145;&#24230;&#30340;&#23545;&#25968;&#22240;&#23376;&#12290;</title><link>http://arxiv.org/abs/2311.01356</link><description>&lt;p&gt;
&#20851;&#20110;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#30340;Lipschitz&#24120;&#25968;
&lt;/p&gt;
&lt;p&gt;
On the Lipschitz constant of random neural networks. (arXiv:2311.01356v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01356
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;Lipschitz&#24120;&#25968;&#65292;&#23545;&#20110;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;Lipschitz&#24120;&#25968;&#30340;&#31934;&#30830;&#21051;&#30011;&#65292;&#23545;&#20110;&#36275;&#22815;&#23485;&#24230;&#30340;&#28145;&#23618;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#19978;&#19979;&#30028;&#65292;&#24182;&#21305;&#37197;&#19968;&#20010;&#20381;&#36182;&#20110;&#28145;&#24230;&#30340;&#23545;&#25968;&#22240;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#35777;&#30740;&#31350;&#24191;&#27867;&#35777;&#26126;&#31070;&#32463;&#32593;&#32476;&#23545;&#36755;&#20837;&#30340;&#24494;&#23567;&#23545;&#25239;&#24615;&#25200;&#21160;&#38750;&#24120;&#25935;&#24863;&#12290;&#36825;&#20123;&#25152;&#35859;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#26368;&#22351;&#24773;&#20917;&#40065;&#26834;&#24615;&#21487;&#20197;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#30340;Lipschitz&#24120;&#25968;&#26469;&#37327;&#21270;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#36825;&#20010;&#37327;&#30340;&#29702;&#35770;&#32467;&#26524;&#22312;&#25991;&#29486;&#20013;&#20165;&#26377;&#23569;&#25968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#22987;&#30740;&#31350;&#38543;&#26426;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;Lipschitz&#24120;&#25968;&#65292;&#21363;&#36873;&#25321;&#38543;&#26426;&#26435;&#37325;&#24182;&#37319;&#29992;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#23545;&#20110;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#23558;Lipschitz&#24120;&#25968;&#21051;&#30011;&#21040;&#19968;&#20010;&#32477;&#23545;&#25968;&#20540;&#24120;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#20998;&#26512;&#25193;&#23637;&#21040;&#36275;&#22815;&#23485;&#24230;&#30340;&#28145;&#23618;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;Lipschitz&#24120;&#25968;&#30340;&#19978;&#19979;&#30028;&#12290;&#36825;&#20123;&#30028;&#21305;&#37197;&#21040;&#19968;&#20010;&#20381;&#36182;&#20110;&#28145;&#24230;&#30340;&#23545;&#25968;&#22240;&#23376;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Empirical studies have widely demonstrated that neural networks are highly sensitive to small, adversarial perturbations of the input. The worst-case robustness against these so-called adversarial examples can be quantified by the Lipschitz constant of the neural network. However, only few theoretical results regarding this quantity exist in the literature. In this paper, we initiate the study of the Lipschitz constant of random ReLU neural networks, i.e., neural networks whose weights are chosen at random and which employ the ReLU activation function. For shallow neural networks, we characterize the Lipschitz constant up to an absolute numerical constant. Moreover, we extend our analysis to deep neural networks of sufficiently large width where we prove upper and lower bounds for the Lipschitz constant. These bounds match up to a logarithmic factor that depends on the depth.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23454;&#35777;&#20998;&#26512;&#20102;&#32463;&#20856;&#21644;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22270;&#20687;&#21387;&#32553;&#26041;&#27861;&#22312;&#28145;&#24230;&#23398;&#20064;&#22270;&#20687;&#22788;&#29702;&#27169;&#22411;&#20013;&#30340;&#24433;&#21709;&#65292;&#24182;&#20197;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26080;&#26631;&#31614;&#39044;&#27979;&#27169;&#22411;&#20026;&#20363;&#23637;&#31034;&#20102;&#22270;&#20687;&#21387;&#32553;&#30340;&#26377;&#25928;&#24615;&#65292;&#20197;&#20943;&#23567;&#25968;&#25454;&#23610;&#23544;&#24182;&#20445;&#30041;&#24517;&#35201;&#20449;&#24687;&#65292;&#20174;&#32780;&#20943;&#36731;&#25968;&#25454;&#31649;&#29702;&#22522;&#30784;&#35774;&#26045;&#30340;&#36127;&#25285;&#24182;&#26041;&#20415;&#25968;&#25454;&#20849;&#20139;&#25110;&#20113;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2311.01352</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26174;&#24494;&#22270;&#20687;&#21387;&#32553;&#65306;&#19968;&#39033;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Deep learning based Image Compression for Microscopy Images: An Empirical Study. (arXiv:2311.01352v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01352
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23454;&#35777;&#20998;&#26512;&#20102;&#32463;&#20856;&#21644;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22270;&#20687;&#21387;&#32553;&#26041;&#27861;&#22312;&#28145;&#24230;&#23398;&#20064;&#22270;&#20687;&#22788;&#29702;&#27169;&#22411;&#20013;&#30340;&#24433;&#21709;&#65292;&#24182;&#20197;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26080;&#26631;&#31614;&#39044;&#27979;&#27169;&#22411;&#20026;&#20363;&#23637;&#31034;&#20102;&#22270;&#20687;&#21387;&#32553;&#30340;&#26377;&#25928;&#24615;&#65292;&#20197;&#20943;&#23567;&#25968;&#25454;&#23610;&#23544;&#24182;&#20445;&#30041;&#24517;&#35201;&#20449;&#24687;&#65292;&#20174;&#32780;&#20943;&#36731;&#25968;&#25454;&#31649;&#29702;&#22522;&#30784;&#35774;&#26045;&#30340;&#36127;&#25285;&#24182;&#26041;&#20415;&#25968;&#25454;&#20849;&#20139;&#25110;&#20113;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29616;&#20195;&#26174;&#24494;&#38236;&#21644;&#29983;&#29289;&#25104;&#20687;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#20135;&#29983;&#20102;&#22823;&#37327;&#30340;&#26174;&#24494;&#22270;&#20687;&#25968;&#25454;&#65292;&#36825;&#20123;&#25968;&#25454;&#36890;&#36807;&#32593;&#32476;&#36827;&#34892;&#23384;&#20648;&#12289;&#20998;&#26512;&#29978;&#33267;&#20849;&#20139;&#12290;&#25968;&#25454;&#30340;&#35268;&#27169;&#23545;&#24403;&#21069;&#30340;&#25968;&#25454;&#22522;&#30784;&#35774;&#26045;&#25552;&#20986;&#20102;&#24040;&#22823;&#25361;&#25112;&#12290;&#20943;&#23567;&#25968;&#25454;&#23610;&#23544;&#30340;&#19968;&#31181;&#24120;&#35265;&#26041;&#27861;&#26159;&#22270;&#20687;&#21387;&#32553;&#12290;&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#32463;&#20856;&#30340;&#21644;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22270;&#20687;&#21387;&#32553;&#26041;&#27861;&#65292;&#24182;&#30740;&#31350;&#20102;&#23427;&#20204;&#23545;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22270;&#20687;&#22788;&#29702;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26080;&#26631;&#31614;&#39044;&#27979;&#27169;&#22411;&#65288;&#21363;&#20174;&#20142;&#22330;&#22270;&#20687;&#39044;&#27979;&#33639;&#20809;&#22270;&#20687;&#65289;&#34987;&#29992;&#20316;&#27604;&#36739;&#21644;&#20998;&#26512;&#30340;&#31034;&#20363;&#24212;&#29992;&#12290;&#26377;&#25928;&#30340;&#22270;&#20687;&#21387;&#32553;&#26041;&#27861;&#21487;&#20197;&#22312;&#19981;&#20002;&#22833;&#24517;&#35201;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#20943;&#23567;&#25968;&#25454;&#23610;&#23544;&#65292;&#20174;&#32780;&#20943;&#36731;&#25968;&#25454;&#31649;&#29702;&#22522;&#30784;&#35774;&#26045;&#30340;&#36127;&#25285;&#65292;&#24182;&#23454;&#29616;&#25968;&#25454;&#20849;&#20139;&#25110;&#20113;&#35745;&#31639;&#19979;&#30340;&#24555;&#36895;&#20256;&#36755;&#12290;&#20026;&#20102;&#20197;&#36825;&#26679;&#30340;&#26041;&#24335;&#21387;&#32553;&#22270;&#20687;&#65292;&#22810;&#31181;&#32463;&#20856;&#30340;&#20002;&#22833;&#21387;&#32553;&#26041;&#27861;&#21644;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26080;&#25439;&#21387;&#32553;&#26041;&#27861;&#24471;&#21040;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the fast development of modern microscopes and bioimaging techniques, an unprecedentedly large amount of imaging data are being generated, stored, analyzed, and even shared through networks. The size of the data poses great challenges for current data infrastructure. One common way to reduce the data size is by image compression. This present study analyzes classic and deep learning based image compression methods, and their impact on deep learning based image processing models. Deep learning based label-free prediction models (i.e., predicting fluorescent images from bright field images) are used as an example application for comparison and analysis. Effective image compression methods could help reduce the data size significantly without losing necessary information, and therefore reduce the burden on data management infrastructure and permit fast transmission through the network for data sharing or cloud computing. To compress images in such a wanted way, multiple classical los
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#30340;&#30446;&#30340;&#26159;&#21033;&#29992;&#27491;&#20132;&#21270;&#26041;&#27861;&#28040;&#38500;&#33016;&#37096;X&#23556;&#32447;&#23884;&#20837;&#20013;&#30340;&#20445;&#25252;&#29305;&#24449;&#24433;&#21709;&#65292;&#24182;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#20445;&#25252;&#29305;&#24449;&#23545;&#30149;&#29702;&#39044;&#27979;&#26377;&#26174;&#33879;&#24433;&#21709;&#65292;&#32780;&#24212;&#29992;&#27491;&#20132;&#21270;&#26041;&#27861;&#21487;&#20197;&#28040;&#38500;&#36825;&#20123;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2311.01349</link><description>&lt;p&gt;
&#21462;&#28040;&#20445;&#25252;&#29305;&#24449;&#65306;&#20174;&#33016;&#37096;X&#23556;&#32447;&#23884;&#20837;&#20013;&#28040;&#38500;&#20445;&#25252;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Unreading Race: Purging Protected Features from Chest X-ray Embeddings. (arXiv:2311.01349v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01349
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#30340;&#30446;&#30340;&#26159;&#21033;&#29992;&#27491;&#20132;&#21270;&#26041;&#27861;&#28040;&#38500;&#33016;&#37096;X&#23556;&#32447;&#23884;&#20837;&#20013;&#30340;&#20445;&#25252;&#29305;&#24449;&#24433;&#21709;&#65292;&#24182;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#20445;&#25252;&#29305;&#24449;&#23545;&#30149;&#29702;&#39044;&#27979;&#26377;&#26174;&#33879;&#24433;&#21709;&#65292;&#32780;&#24212;&#29992;&#27491;&#20132;&#21270;&#26041;&#27861;&#21487;&#20197;&#28040;&#38500;&#36825;&#20123;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#20998;&#26512;&#24182;&#28040;&#38500;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#33016;&#37096;X&#23556;&#32447;&#23884;&#20837;&#30340;&#20445;&#25252;&#29305;&#24449;&#24433;&#21709;&#12290;&#26041;&#27861;&#65306;&#20351;&#29992;&#27491;&#20132;&#21270;&#26041;&#27861;&#28040;&#38500;&#33016;&#37096;X&#23556;&#32447;&#23884;&#20837;&#20013;&#30340;&#20445;&#25252;&#29305;&#24449;&#65288;&#22914;&#24180;&#40836;&#12289;&#24615;&#21035;&#12289;&#31181;&#26063;&#65289;&#30340;&#24433;&#21709;&#65292;&#30830;&#20445;&#29305;&#24449;&#29420;&#31435;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#39564;&#35777;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#19977;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;&#26377;&#30417;&#30563;&#23545;&#27604;&#12289;&#33258;&#30417;&#30563;&#23545;&#27604;&#21644;&#22522;&#32447;&#20998;&#31867;&#22120;&#27169;&#22411;&#65289;&#23545;MIMIC&#21644;CheXpert&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#22238;&#39038;&#24615;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#32479;&#35745;&#20998;&#26512;&#28041;&#21450;&#36890;&#36807;&#20272;&#35745;&#20445;&#25252;&#29305;&#24449;&#24433;&#21709;&#21644;&#35780;&#20272;&#20351;&#29992;&#20004;&#31181;&#31867;&#22411;&#23884;&#20837;&#30340;&#33021;&#21147;&#26469;&#39044;&#27979;&#31181;&#26063;&#12289;&#24180;&#40836;&#25110;&#24615;&#21035;&#30340;&#21407;&#22987;&#19982;&#27491;&#20132;&#23884;&#20837;&#30340;&#27604;&#36739;&#12290;&#32467;&#26524;&#65306;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#20445;&#25252;&#29305;&#24449;&#23545;&#30149;&#29702;&#39044;&#27979;&#30340;&#26174;&#30528;&#24433;&#21709;&#12290;&#24212;&#29992;&#27491;&#20132;&#21270;&#26041;&#27861;&#21487;&#20197;&#28040;&#38500;&#36825;&#20123;&#29305;&#24449;&#24433;&#21709;&#12290;&#38500;&#20102;&#28040;&#38500;&#23545;&#30149;&#29702;&#20998;&#31867;&#30340;&#24433;&#21709;&#20043;&#22806;&#65292;
&lt;/p&gt;
&lt;p&gt;
Purpose: To analyze and remove protected feature effects in chest radiograph embeddings of deep learning models.  Materials and Methods: An orthogonalization is utilized to remove the influence of protected features (e.g., age, sex, race) in chest radiograph embeddings, ensuring feature-independent results. To validate the efficacy of the approach, we retrospectively study the MIMIC and CheXpert datasets using three pre-trained models, namely a supervised contrastive, a self-supervised contrastive, and a baseline classifier model. Our statistical analysis involves comparing the original versus the orthogonalized embeddings by estimating protected feature influences and evaluating the ability to predict race, age, or sex using the two types of embeddings.  Results: Our experiments reveal a significant influence of protected features on predictions of pathologies. Applying orthogonalization removes these feature effects. Apart from removing any influence on pathology classification, whil
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#31616;&#21333;&#30340;&#21151;&#29575;&#20998;&#26512;&#26041;&#27861;&#65292;&#22312;32&#20301;&#24494;&#25511;&#21046;&#22120;&#19978;&#25552;&#21462;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#26550;&#26500;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2311.01344</link><description>&lt;p&gt;
&#29992;&#31616;&#21333;&#30340;&#21151;&#29575;&#20998;&#26512;&#22312;32&#20301;&#24494;&#25511;&#21046;&#22120;&#19978;&#38405;&#35835;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Like an Open Book? Read Neural Network Architecture with Simple Power Analysis on 32-bit Microcontrollers. (arXiv:2311.01344v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01344
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#31616;&#21333;&#30340;&#21151;&#29575;&#20998;&#26512;&#26041;&#27861;&#65292;&#22312;32&#20301;&#24494;&#25511;&#21046;&#22120;&#19978;&#25552;&#21462;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#26550;&#26500;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#25552;&#21462;&#26159;AI&#31995;&#32479;&#23433;&#20840;&#30340;&#19968;&#20010;&#19981;&#26029;&#22686;&#38271;&#30340;&#20851;&#27880;&#28857;&#12290;&#23545;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#26469;&#35828;&#65292;&#26550;&#26500;&#26159;&#23545;&#25163;&#35797;&#22270;&#24674;&#22797;&#30340;&#26368;&#37325;&#35201;&#30340;&#20449;&#24687;&#12290;&#20316;&#20026;&#19968;&#31995;&#21015;&#37325;&#22797;&#35745;&#31639;&#22359;&#65292;&#37096;&#32626;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23558;&#20135;&#29983;&#29420;&#29305;&#30340;&#20391;&#20449;&#36947;&#27844;&#38706;&#12290;&#24403;&#30446;&#26631;&#24179;&#21488;&#22312;&#29289;&#29702;&#19978;&#21487;&#35775;&#38382;&#26102;&#65292;&#21487;&#20197;&#21033;&#29992;&#36825;&#20123;&#20449;&#36947;&#27844;&#38706;&#26469;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#12290;&#36890;&#36807;&#32467;&#21512;&#23545;&#28145;&#24230;&#23398;&#20064;&#23454;&#36341;&#30340;&#29702;&#35770;&#30693;&#35782;&#21644;&#23545;&#24191;&#27867;&#20351;&#29992;&#30340;&#23454;&#29616;&#24211;&#65288;ARM CMSIS-NN&#65289;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#30340;&#30446;&#30340;&#26159;&#22238;&#31572;&#36825;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#36890;&#36807;&#31616;&#21333;&#22320;&#26816;&#26597;&#19968;&#20010;EM&#20391;&#20449;&#36947;&#36319;&#36394;&#65292;&#25105;&#20204;&#21487;&#20197;&#25552;&#21462;&#22810;&#36828;&#30340;&#26550;&#26500;&#20449;&#24687;&#65311;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20256;&#32479;MLP&#21644;CNN&#27169;&#22411;&#30340;&#25552;&#21462;&#26041;&#27861;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#39640;&#31471;32&#20301;&#24494;&#25511;&#21046;&#22120;&#65288;Cortex-M7&#65289;&#19978;&#36816;&#34892;&#65292;&#20165;&#20381;&#36182;&#20110;&#31616;&#21333;&#30340;&#27169;&#24335;&#35782;&#21035;&#20998;&#26512;&#12290;&#23613;&#31649;&#23384;&#22312;&#20960;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#20917;&#65292;&#20294;&#25105;&#20204;&#22768;&#31216;&#65292;&#19982;&#21442;&#25968;&#25552;&#21462;&#30456;&#21453;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#20174;&#31616;&#21333;&#30340;&#21151;&#29575;&#20998;&#26512;&#20013;&#25552;&#21462;&#20986;&#26550;&#26500;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model extraction is a growing concern for the security of AI systems. For deep neural network models, the architecture is the most important information an adversary aims to recover. Being a sequence of repeated computation blocks, neural network models deployed on edge-devices will generate distinctive side-channel leakages. The latter can be exploited to extract critical information when targeted platforms are physically accessible. By combining theoretical knowledge about deep learning practices and analysis of a widespread implementation library (ARM CMSIS-NN), our purpose is to answer this critical question: how far can we extract architecture information by simply examining an EM side-channel trace? For the first time, we propose an extraction methodology for traditional MLP and CNN models running on a high-end 32-bit microcontroller (Cortex-M7) that relies only on simple pattern recognition analysis. Despite few challenging cases, we claim that, contrary to parameters extraction
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26368;&#23567;&#21270;&#21407;&#22987;Wasserstein&#36317;&#31163;&#26469;&#21305;&#37197;&#19987;&#23478;&#21644;&#23398;&#20064;&#32773;&#29366;&#24577;&#21344;&#29992;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#31163;&#32447;&#23398;&#20064;&#20174;&#35266;&#23519;&#20013;&#27169;&#20223;&#20219;&#21153;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.01331</link><description>&lt;p&gt;
&#36890;&#36807;&#21407;&#22987;Wasserstein&#29366;&#24577;&#21344;&#29992;&#21305;&#37197;&#23454;&#29616;&#30340;&#31163;&#32447;&#35266;&#23519;&#27169;&#20223;
&lt;/p&gt;
&lt;p&gt;
Offline Imitation from Observation via Primal Wasserstein State Occupancy Matching. (arXiv:2311.01331v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01331
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26368;&#23567;&#21270;&#21407;&#22987;Wasserstein&#36317;&#31163;&#26469;&#21305;&#37197;&#19987;&#23478;&#21644;&#23398;&#20064;&#32773;&#29366;&#24577;&#21344;&#29992;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#31163;&#32447;&#23398;&#20064;&#20174;&#35266;&#23519;&#20013;&#27169;&#20223;&#20219;&#21153;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24773;&#22659;&#20013;&#65292;&#19982;&#29615;&#22659;&#30340;&#20219;&#24847;&#20132;&#20114;&#24448;&#24448;&#26159;&#26114;&#36149;&#30340;&#65292;&#24182;&#19988;&#19987;&#23478;&#31034;&#33539;&#30340;&#34892;&#20026;&#24182;&#19981;&#24635;&#26159;&#21487;&#29992;&#30340;&#12290;&#20026;&#20102;&#20943;&#23569;&#36825;&#20004;&#32773;&#30340;&#38656;&#27714;&#65292;&#31163;&#32447;&#23398;&#20064;&#20174;&#35266;&#23519;&#65288;LfO&#65289;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20854;&#20013;&#20195;&#29702;&#36890;&#36807;&#21482;&#26377;&#19987;&#23478;&#29366;&#24577;&#21644;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#38750;&#19987;&#23478;&#29366;&#24577;-&#21160;&#20316;&#23545;&#26469;&#23398;&#20064;&#35299;&#20915;&#20219;&#21153;&#12290;&#26368;&#20808;&#36827;&#30340;&#20998;&#24067;&#26657;&#27491;&#20272;&#35745;&#65288;DICE&#65289;&#26041;&#27861;&#26368;&#23567;&#21270;&#20102;&#23398;&#20064;&#32773;&#21644;&#19987;&#23478;&#31574;&#30053;&#20043;&#38388;&#30340;&#29366;&#24577;&#21344;&#29992;&#24046;&#24322;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20165;&#38480;&#20110;$f$-divergences&#65288;KL&#21644;$\chi^2$&#65289;&#25110;&#24102;&#26377;Rubinstein&#23545;&#20598;&#30340;Wasserstein&#36317;&#31163;&#65292;&#21518;&#32773;&#38480;&#21046;&#20102;&#23545;&#24615;&#33021;&#20851;&#38190;&#30340;&#22522;&#30784;&#36317;&#31163;&#24230;&#37327;&#30340;&#20351;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21407;&#22987;Wasserstein DICE&#65288;PW-DICE&#65289;&#65292;&#23427;&#36890;&#36807;&#24754;&#35266;&#27491;&#21017;&#21270;&#22120;&#26368;&#23567;&#21270;&#19987;&#23478;&#21644;&#23398;&#20064;&#32773;&#29366;&#24577;&#21344;&#29992;&#20043;&#38388;&#30340;&#21407;&#22987;Wasserstein&#36317;&#31163;&#65292;&#24182;&#21033;&#29992;&#20102;&#23545;&#27604;&#23398;&#20064;&#30340;dis
&lt;/p&gt;
&lt;p&gt;
In real-world scenarios, arbitrary interactions with the environment can often be costly, and actions of expert demonstrations are not always available. To reduce the need for both, Offline Learning from Observations (LfO) is extensively studied, where the agent learns to solve a task with only expert states and \textit{task-agnostic} non-expert state-action pairs. The state-of-the-art DIstribution Correction Estimation (DICE) methods minimize the state occupancy divergence between the learner and expert policies. However, they are limited to either $f$-divergences (KL and $\chi^2$) or Wasserstein distance with Rubinstein duality, the latter of which constrains the underlying distance metric crucial to the performance of Wasserstein-based solutions. To address this problem, we propose Primal Wasserstein DICE (PW-DICE), which minimizes the primal Wasserstein distance between the expert and learner state occupancies with a pessimistic regularizer and leverages a contrastively learned dis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31163;&#32447;&#35266;&#27979;&#21644;&#31034;&#20363;&#30340;&#31616;&#21333;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#36712;&#36857;&#30340;&#21152;&#26435;&#34892;&#20026;&#20811;&#38534;&#21644;&#19987;&#23478;&#29366;&#24577;&#37492;&#21035;&#22120;&#26469;&#31283;&#23450;&#22320;&#23398;&#20064;&#65292;&#20197;&#35299;&#20915;&#31163;&#32447;&#27169;&#20223;&#20013;&#30001;&#20110;&#19981;&#23436;&#25972;&#36712;&#36857;&#32780;&#23548;&#33268;&#30340;&#19981;&#31283;&#23450;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.01329</link><description>&lt;p&gt;
&#31163;&#32447;&#35266;&#27979;&#21644;&#31034;&#20363;&#20013;&#29992;&#20110;&#27169;&#20223;&#30340;&#31616;&#21333;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#33021;&#21253;&#21547;&#19981;&#23436;&#25972;&#30340;&#36712;&#36857;
&lt;/p&gt;
&lt;p&gt;
A Simple Solution for Offline Imitation from Observations and Examples with Possibly Incomplete Trajectories. (arXiv:2311.01329v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01329
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31163;&#32447;&#35266;&#27979;&#21644;&#31034;&#20363;&#30340;&#31616;&#21333;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#36712;&#36857;&#30340;&#21152;&#26435;&#34892;&#20026;&#20811;&#38534;&#21644;&#19987;&#23478;&#29366;&#24577;&#37492;&#21035;&#22120;&#26469;&#31283;&#23450;&#22320;&#23398;&#20064;&#65292;&#20197;&#35299;&#20915;&#31163;&#32447;&#27169;&#20223;&#20013;&#30001;&#20110;&#19981;&#23436;&#25972;&#36712;&#36857;&#32780;&#23548;&#33268;&#30340;&#19981;&#31283;&#23450;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35266;&#27979;&#20013;&#36827;&#34892;&#31163;&#32447;&#27169;&#20223;&#26088;&#22312;&#35299;&#20915;&#20165;&#20855;&#26377;&#20219;&#21153;&#29305;&#23450;&#19987;&#23478;&#29366;&#24577;&#21644;&#20219;&#21153;&#19981;&#21487;&#30693;&#38750;&#19987;&#23478;&#29366;&#24577;-&#25805;&#20316;&#23545;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#12290;&#31163;&#32447;&#27169;&#20223;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#22330;&#26223;&#20013;&#38750;&#24120;&#26377;&#29992;&#65292;&#20854;&#20013;&#20219;&#24847;&#30340;&#20132;&#20114;&#37117;&#26159;&#26114;&#36149;&#30340;&#19988;&#19987;&#23478;&#30340;&#25805;&#20316;&#19981;&#21487;&#29992;&#12290;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#8220;&#20998;&#24067;&#30699;&#27491;&#20272;&#35745;&#8221;&#65288;DICE&#65289;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#21270;&#19987;&#23478;&#21644;&#23398;&#20064;&#32773;&#31574;&#30053;&#20043;&#38388;&#30340;&#29366;&#24577;&#21344;&#39046;&#24046;&#24322;&#65292;&#24182;&#20351;&#29992;&#21152;&#26435;&#34892;&#20026;&#20811;&#38534;&#26816;&#32034;&#21040;&#19968;&#20010;&#31574;&#30053;&#65307;&#28982;&#32780;&#65292;&#24403;&#20174;&#19981;&#23436;&#25972;&#36712;&#36857;&#23398;&#20064;&#26102;&#65292;&#30001;&#20110;&#21452;&#22495;&#30340;&#38750;&#40065;&#26834;&#20248;&#21270;&#65292;&#23427;&#20204;&#30340;&#32467;&#26524;&#26159;&#19981;&#31283;&#23450;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#35266;&#27979;&#30340;&#36712;&#36857;&#24863;&#30693;&#27169;&#20223;&#23398;&#20064;&#65288;TAILO&#65289;&#12290;TAILO&#20351;&#29992;&#26410;&#26469;&#36712;&#36857;&#19978;&#30340;&#25240;&#29616;&#21644;&#20316;&#20026;&#21152;&#26435;&#34892;&#20026;&#20811;&#38534;&#30340;&#26435;&#37325;&#12290;&#36825;&#20123;&#26435;&#37325;&#26159;&#30001;&#19968;&#20010;&#26088;&#22312;&#35782;&#21035;&#19987;&#23478;&#29366;&#24577;&#30340;&#37492;&#21035;&#22120;&#30340;&#36755;&#20986;&#36827;&#34892;&#32553;&#25918;&#30340;&#12290;&#23613;&#31649;&#31616;&#21333;&#65292;TAILO&#22312;&#23384;&#22312;&#36712;&#36857;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline imitation from observations aims to solve MDPs where only task-specific expert states and task-agnostic non-expert state-action pairs are available. Offline imitation is useful in real-world scenarios where arbitrary interactions are costly and expert actions are unavailable. The state-of-the-art "DIstribution Correction Estimation" (DICE) methods minimize divergence of state occupancy between expert and learner policies and retrieve a policy with weighted behavior cloning; however, their results are unstable when learning from incomplete trajectories, due to a non-robust optimization in the dual domain. To address the issue, in this paper, we propose Trajectory-Aware Imitation Learning from Observations (TAILO). TAILO uses a discounted sum along the future trajectory as the weight for weighted behavior cloning. The terms for the sum are scaled by the output of a discriminator, which aims to identify expert states. Despite simplicity, TAILO works well if there exist trajectorie
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#32972;&#21253;&#32422;&#26463;&#30340;&#39640;&#32500;&#32447;&#24615;&#36172;&#33218;&#38382;&#39064;&#65292;&#21033;&#29992;&#31232;&#30095;&#32467;&#26500;&#23454;&#29616;&#25913;&#36827;&#36951;&#25022;&#12290;&#36890;&#36807;&#24320;&#21457;&#22312;&#32447;&#30828;&#38408;&#20540;&#31639;&#27861;&#21644;&#21407;&#22987;-&#23545;&#20598;&#26694;&#26550;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#29305;&#24449;&#32500;&#24230;&#30340;&#23545;&#25968;&#25913;&#36827;&#30340;&#27425;&#32447;&#24615;&#36951;&#25022;&#12290;</title><link>http://arxiv.org/abs/2311.01327</link><description>&lt;p&gt;
&#20855;&#26377;&#32972;&#21253;&#32422;&#26463;&#30340;&#39640;&#32500;&#32447;&#24615;&#36172;&#33218;&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
High-dimensional Linear Bandits with Knapsacks. (arXiv:2311.01327v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01327
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#32972;&#21253;&#32422;&#26463;&#30340;&#39640;&#32500;&#32447;&#24615;&#36172;&#33218;&#38382;&#39064;&#65292;&#21033;&#29992;&#31232;&#30095;&#32467;&#26500;&#23454;&#29616;&#25913;&#36827;&#36951;&#25022;&#12290;&#36890;&#36807;&#24320;&#21457;&#22312;&#32447;&#30828;&#38408;&#20540;&#31639;&#27861;&#21644;&#21407;&#22987;-&#23545;&#20598;&#26694;&#26550;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#29305;&#24449;&#32500;&#24230;&#30340;&#23545;&#25968;&#25913;&#36827;&#30340;&#27425;&#32447;&#24615;&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#29305;&#24449;&#32500;&#24230;&#36739;&#22823;&#30340;&#39640;&#32500;&#35774;&#32622;&#19979;&#30340;&#20855;&#26377;&#32972;&#21253;&#32422;&#26463;&#30340;&#19978;&#19979;&#25991;&#36172;&#33218;&#38382;&#39064;&#12290;&#27599;&#20010;&#25163;&#33218;&#25289;&#21160;&#30340;&#22870;&#21169;&#31561;&#20110;&#31232;&#30095;&#39640;&#32500;&#26435;&#37325;&#21521;&#37327;&#19982;&#24403;&#21069;&#21040;&#36798;&#30340;&#29305;&#24449;&#30340;&#20056;&#31215;&#65292;&#21152;&#19978;&#39069;&#22806;&#30340;&#38543;&#26426;&#22122;&#22768;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#22914;&#20309;&#21033;&#29992;&#36825;&#31181;&#31232;&#30095;&#32467;&#26500;&#26469;&#23454;&#29616;CBwK&#38382;&#39064;&#30340;&#25913;&#36827;&#36951;&#25022;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#24320;&#21457;&#20102;&#19968;&#31181;&#22312;&#32447;&#30340;&#30828;&#38408;&#20540;&#31639;&#27861;&#30340;&#21464;&#20307;&#65292;&#20197;&#22312;&#32447;&#26041;&#24335;&#36827;&#34892;&#31232;&#30095;&#20272;&#35745;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#25105;&#20204;&#30340;&#22312;&#32447;&#20272;&#35745;&#22120;&#19982;&#21407;&#22987;-&#23545;&#20598;&#26694;&#26550;&#32467;&#21512;&#36215;&#26469;&#65292;&#22312;&#27599;&#20010;&#32972;&#21253;&#32422;&#26463;&#19978;&#20998;&#37197;&#19968;&#20010;&#23545;&#20598;&#21464;&#37327;&#65292;&#24182;&#21033;&#29992;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#26469;&#26356;&#26032;&#23545;&#20598;&#21464;&#37327;&#65292;&#20174;&#32780;&#25511;&#21046;&#32972;&#21253;&#23481;&#37327;&#30340;&#28040;&#32791;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#38598;&#25104;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#23454;&#29616;&#23545;&#29305;&#24449;&#32500;&#24230;&#30340;&#23545;&#25968;&#25913;&#36827;&#30340;&#27425;&#32447;&#24615;&#36951;&#25022;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#22810;&#39033;&#24335;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the contextual bandits with knapsack (CBwK) problem under the high-dimensional setting where the dimension of the feature is large. The reward of pulling each arm equals the multiplication of a sparse high-dimensional weight vector and the feature of the current arrival, with additional random noise. In this paper, we investigate how to exploit this sparsity structure to achieve improved regret for the CBwK problem. To this end, we first develop an online variant of the hard thresholding algorithm that performs the sparse estimation in an online manner. We further combine our online estimator with a primal-dual framework, where we assign a dual variable to each knapsack constraint and utilize an online learning algorithm to update the dual variable, thereby controlling the consumption of the knapsack capacity. We show that this integrated approach allows us to achieve a sublinear regret that depends logarithmically on the feature dimension, thus improving the polynomial depend
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;&#20256;&#36882;&#24615;&#25915;&#20987;&#22522;&#20934;&#65288;TA-Bench&#65289;&#65292;&#35780;&#20272;&#20102;30&#22810;&#31181;&#26041;&#27861;&#22312;25&#20010;&#28909;&#38376;&#26367;&#20195;/&#21463;&#23475;&#27169;&#22411;&#19978;&#30340;&#24615;&#33021;&#65292;&#20197;&#31995;&#32479;&#22320;&#12289;&#20844;&#27491;&#22320;&#12289;&#23454;&#29992;&#22320;&#27604;&#36739;&#36825;&#20123;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2311.01323</link><description>&lt;p&gt;
&#23558;&#20256;&#36882;&#24615;&#25915;&#20987;&#31995;&#32479;&#22320;&#12289;&#23454;&#29992;&#22320;&#21644;&#20844;&#27491;&#22320;&#36827;&#34892;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Towards Evaluating Transfer-based Attacks Systematically, Practically, and Fairly. (arXiv:2311.01323v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01323
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;&#20256;&#36882;&#24615;&#25915;&#20987;&#22522;&#20934;&#65288;TA-Bench&#65289;&#65292;&#35780;&#20272;&#20102;30&#22810;&#31181;&#26041;&#27861;&#22312;25&#20010;&#28909;&#38376;&#26367;&#20195;/&#21463;&#23475;&#27169;&#22411;&#19978;&#30340;&#24615;&#33021;&#65292;&#20197;&#31995;&#32479;&#22320;&#12289;&#20844;&#27491;&#22320;&#12289;&#23454;&#29992;&#22320;&#27604;&#36739;&#36825;&#20123;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#23545;&#25239;&#24615;&#33030;&#24369;&#24615;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#65292;&#22240;&#20026;&#22312;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#36825;&#20123;&#27169;&#22411;&#23384;&#22312;&#23433;&#20840;&#39118;&#38505;&#12290;&#22522;&#20110;&#23545;&#25239;&#26679;&#26412;&#30340;&#21487;&#20256;&#36882;&#24615;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20256;&#36882;&#24615;&#26041;&#27861;&#26469;&#27450;&#39575;&#26080;&#27861;&#35775;&#38382;&#20854;&#26550;&#26500;&#21644;&#21442;&#25968;&#30340;&#40657;&#30418;DNN&#27169;&#22411;&#12290;&#23613;&#31649;&#20184;&#20986;&#20102;&#24040;&#22823;&#21162;&#21147;&#65292;&#20294;&#20173;&#28982;&#32570;&#20047;&#19968;&#20010;&#26631;&#20934;&#21270;&#30340;&#22522;&#20934;&#65292;&#20197;&#20415;&#31995;&#32479;&#12289;&#20844;&#27491;&#21644;&#23454;&#29992;&#22320;&#27604;&#36739;&#36825;&#20123;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#26174;&#31034;&#65292;&#26576;&#20123;&#26041;&#27861;&#30340;&#35780;&#20272;&#38656;&#35201;&#26356;&#21512;&#29702;&#12289;&#26356;&#20840;&#38754;&#22320;&#39564;&#35777;&#20854;&#26377;&#25928;&#24615;&#65292;&#36991;&#20813;&#19981;&#20844;&#24179;&#27604;&#36739;&#21644;&#23545;&#21487;&#33021;&#30340;&#26367;&#20195;/&#21463;&#23475;&#27169;&#22411;&#30340;&#19981;&#20805;&#20998;&#32771;&#34385;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#20256;&#36882;&#24615;&#25915;&#20987;&#22522;&#20934;&#65288;TA-Bench&#65289;&#65292;&#20854;&#20013;&#23454;&#26045;&#20102;30&#22810;&#31181;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#35780;&#20272;&#21644;&#27604;&#36739;&#20102;&#23427;&#20204;&#22312;25&#20010;&#28909;&#38376;&#26367;&#20195;/&#21463;&#23475;&#27169;&#22411;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The adversarial vulnerability of deep neural networks (DNNs) has drawn great attention due to the security risk of applying these models in real-world applications. Based on transferability of adversarial examples, an increasing number of transfer-based methods have been developed to fool black-box DNN models whose architecture and parameters are inaccessible. Although tremendous effort has been exerted, there still lacks a standardized benchmark that could be taken advantage of to compare these methods systematically, fairly, and practically. Our investigation shows that the evaluation of some methods needs to be more reasonable and more thorough to verify their effectiveness, to avoid, for example, unfair comparison and insufficient consideration of possible substitute/victim models. Therefore, we establish a transfer-based attack benchmark (TA-Bench) which implements 30+ methods. In this paper, we evaluate and compare them comprehensively on 25 popular substitute/victim models on Im
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25955;&#23556;&#35270;&#35273;&#21464;&#25442;&#65288;SVT&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20809;&#35889;&#28151;&#21512;&#26469;&#35299;&#20915;&#35270;&#35273;&#21464;&#25442;&#20013;&#30340;&#27880;&#24847;&#21147;&#22797;&#26434;&#24615;&#21644;&#20449;&#24687;&#25429;&#25417;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.01310</link><description>&lt;p&gt;
&#25955;&#23556;&#35270;&#35273;&#21464;&#25442;&#65306;&#20809;&#35889;&#28151;&#21512;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
Scattering Vision Transformer: Spectral Mixing Matters. (arXiv:2311.01310v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01310
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25955;&#23556;&#35270;&#35273;&#21464;&#25442;&#65288;SVT&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20809;&#35889;&#28151;&#21512;&#26469;&#35299;&#20915;&#35270;&#35273;&#21464;&#25442;&#20013;&#30340;&#27880;&#24847;&#21147;&#22797;&#26434;&#24615;&#21644;&#20449;&#24687;&#25429;&#25417;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#21464;&#25442;&#22120;&#22312;&#21508;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#65292;&#21253;&#25324;&#22270;&#20687;&#20998;&#31867;&#12289;&#23454;&#20363;&#20998;&#21106;&#21644;&#30446;&#26631;&#26816;&#27979;&#20013;&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;&#20851;&#27880;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#35299;&#20915;&#27880;&#24847;&#21147;&#22797;&#26434;&#24615;&#21644;&#26377;&#25928;&#25429;&#25417;&#22270;&#20687;&#20013;&#32454;&#31890;&#24230;&#20449;&#24687;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#36890;&#24120;&#37319;&#29992;&#38477;&#37319;&#26679;&#25805;&#20316;&#65288;&#22914;&#27744;&#21270;&#65289;&#26469;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25805;&#20316;&#26159;&#19981;&#21487;&#36870;&#30340;&#65292;&#21487;&#33021;&#23548;&#33268;&#20449;&#24687;&#20002;&#22833;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#25955;&#23556;&#35270;&#35273;&#21464;&#25442;&#65288;SVT&#65289;&#30340;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;SVT&#32467;&#21512;&#20102;&#19968;&#20010;&#20809;&#35889;&#25955;&#23556;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#23545;&#22797;&#26434;&#22270;&#20687;&#32454;&#33410;&#30340;&#25429;&#25417;&#12290;SVT&#36890;&#36807;&#20998;&#31163;&#20302;&#39057;&#21644;&#39640;&#39057;&#20998;&#37327;&#65292;&#20811;&#26381;&#20102;&#19982;&#38477;&#37319;&#26679;&#25805;&#20316;&#30456;&#20851;&#30340;&#19981;&#21487;&#36870;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;SVT&#24341;&#20837;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#20809;&#35889;&#38376;&#25511;&#32593;&#32476;&#65292;&#21033;&#29992;Einstein&#20056;&#27861;&#26469;&#22788;&#29702;&#20196;&#29260;&#21644;&#36890;&#36947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision transformers have gained significant attention and achieved state-of-the-art performance in various computer vision tasks, including image classification, instance segmentation, and object detection. However, challenges remain in addressing attention complexity and effectively capturing fine-grained information within images. Existing solutions often resort to down-sampling operations, such as pooling, to reduce computational cost. Unfortunately, such operations are non-invertible and can result in information loss. In this paper, we present a novel approach called Scattering Vision Transformer (SVT) to tackle these challenges. SVT incorporates a spectrally scattering network that enables the capture of intricate image details. SVT overcomes the invertibility issue associated with down-sampling operations by separating low-frequency and high-frequency components. Furthermore, SVT introduces a unique spectral gating network utilizing Einstein multiplication for token and channel 
&lt;/p&gt;</description></item><item><title>AWEQ&#26159;&#19968;&#31181;&#21518;&#35757;&#32451;&#37327;&#21270;&#21644;&#28608;&#27963;&#26435;&#37325;&#22343;&#34913;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#36229;&#20302;&#20301;&#37327;&#21270;&#21644;8-bit&#26435;&#37325;&#21644;&#28608;&#27963;&#37327;&#21270;&#65292;&#24182;&#36890;&#36807;&#25913;&#36827;&#30340;&#22343;&#34913;&#26041;&#27861;&#20943;&#23567;&#37327;&#21270;&#20559;&#24046;&#35823;&#24046;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01305</link><description>&lt;p&gt;
AWEQ&#65306;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;&#21644;&#28608;&#27963;&#26435;&#37325;&#22343;&#34913;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AWEQ: Post-Training Quantization with Activation-Weight Equalization for Large Language Models. (arXiv:2311.01305v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01305
&lt;/p&gt;
&lt;p&gt;
AWEQ&#26159;&#19968;&#31181;&#21518;&#35757;&#32451;&#37327;&#21270;&#21644;&#28608;&#27963;&#26435;&#37325;&#22343;&#34913;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#36229;&#20302;&#20301;&#37327;&#21270;&#21644;8-bit&#26435;&#37325;&#21644;&#28608;&#27963;&#37327;&#21270;&#65292;&#24182;&#36890;&#36807;&#25913;&#36827;&#30340;&#22343;&#34913;&#26041;&#27861;&#20943;&#23567;&#37327;&#21270;&#20559;&#24046;&#35823;&#24046;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20854;&#35745;&#31639;&#21644;&#23384;&#20648;&#25104;&#26412;&#20063;&#30456;&#23545;&#36739;&#39640;&#12290;&#37327;&#21270;&#36825;&#20123;&#27169;&#22411;&#26159;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24456;&#38590;&#22312;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#30828;&#20214;&#25928;&#29575;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AWEQ&#65292;&#19968;&#31181;&#21518;&#35757;&#32451;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#24320;&#38144;&#12290;AWEQ&#22312;&#36229;&#20302;&#20301;&#37327;&#21270;&#21644;8-bit&#26435;&#37325;&#21644;&#28608;&#27963;(W8A8)&#37327;&#21270;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#35266;&#23519;&#21040;&#26435;&#37325;&#37327;&#21270;&#27604;&#28608;&#27963;&#37327;&#21270;&#26356;&#23481;&#26131;&#12290;AWEQ&#36890;&#36807;&#36890;&#36947;&#22343;&#34913;&#23558;&#28608;&#27963;&#37327;&#21270;&#30340;&#38590;&#24230;&#36716;&#31227;&#21040;&#26435;&#37325;&#19978;&#65292;&#23454;&#29616;&#20102;&#20004;&#32773;&#37327;&#21270;&#22256;&#38590;&#30340;&#24179;&#34913;&#65292;&#20174;&#32780;&#26368;&#22823;&#21270;&#20102;&#24615;&#33021;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;&#22343;&#34913;&#26041;&#27861;&#65292;&#20943;&#23567;&#20102;&#37327;&#21270;&#20559;&#24046;&#35823;&#24046;&#65292;&#30830;&#20445;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#20687;LLaMA&#36825;&#26679;&#30340;&#27969;&#34892;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models(LLMs) exhibit excellent performance across a variety of tasks, but they come with significant computational and storage costs. Quantizing these models is an effective way to alleviate this issue. However, existing methods struggle to strike a balance between model accuracy and hardware efficiency. This is where we introduce AWEQ, a post-training method that requires no additional training overhead. AWEQ excels in both ultra-low-bit quantization and 8-bit weight and activation (W8A8) quantization. There is an observation that weight quantization is less challenging than activation quantization. AWEQ transfers the difficulty of activation quantization to weights using channel equalization, achieving a balance between the quantization difficulties of both, and thereby maximizing performance. We have further refined the equalization method to mitigate quantization bias error, ensuring the robustness of the model. Extensive experiments on popular models such as LLaMA a
&lt;/p&gt;</description></item><item><title>TRIALSCOPE&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#23558;&#20020;&#24202;&#25991;&#26412;&#36827;&#34892;&#32467;&#26500;&#21270;&#65292;&#37319;&#29992;&#27010;&#29575;&#24314;&#27169;&#36827;&#34892;&#21435;&#22122;&#21644;&#25554;&#34917;&#65292;&#24182;&#24212;&#29992;&#22240;&#26524;&#25512;&#26029;&#25216;&#26415;&#26469;&#24212;&#23545;&#28151;&#26434;&#22240;&#32032;&#65292;&#20197;&#20174;&#23454;&#38469;&#19990;&#30028;&#25968;&#25454;&#20013;&#25552;&#21462;&#23454;&#35777;&#35777;&#25454;&#21644;&#25512;&#29702;&#20020;&#24202;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2311.01301</link><description>&lt;p&gt;
TRIALSCOPE&#65306;&#19968;&#20010;&#32479;&#19968;&#30340;&#22240;&#26524;&#26694;&#26550;&#65292;&#29992;&#20110;&#21033;&#29992;&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#25193;&#23637;&#23454;&#38469;&#19990;&#30028;&#35777;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
TRIALSCOPE A Unifying Causal Framework for Scaling Real-World Evidence Generation with Biomedical Language Models. (arXiv:2311.01301v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01301
&lt;/p&gt;
&lt;p&gt;
TRIALSCOPE&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#23558;&#20020;&#24202;&#25991;&#26412;&#36827;&#34892;&#32467;&#26500;&#21270;&#65292;&#37319;&#29992;&#27010;&#29575;&#24314;&#27169;&#36827;&#34892;&#21435;&#22122;&#21644;&#25554;&#34917;&#65292;&#24182;&#24212;&#29992;&#22240;&#26524;&#25512;&#26029;&#25216;&#26415;&#26469;&#24212;&#23545;&#28151;&#26434;&#22240;&#32032;&#65292;&#20197;&#20174;&#23454;&#38469;&#19990;&#30028;&#25968;&#25454;&#20013;&#25552;&#21462;&#23454;&#35777;&#35777;&#25454;&#21644;&#25512;&#29702;&#20020;&#24202;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#19990;&#30028;&#25968;&#25454;&#30340;&#24555;&#36895;&#25968;&#23383;&#21270;&#20026;&#20248;&#21270;&#21307;&#30103;&#26381;&#21153;&#21644;&#21152;&#36895;&#29983;&#29289;&#21307;&#23398;&#21457;&#29616;&#25552;&#20379;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#26426;&#20250;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#36825;&#20123;&#25968;&#25454;&#24448;&#24448;&#20197;&#38750;&#32467;&#26500;&#21270;&#24418;&#24335;&#23384;&#22312;&#65292;&#22914;&#30005;&#23376;&#21307;&#30103;&#35760;&#24405;&#20013;&#30340;&#20020;&#24202;&#31508;&#35760;&#65292;&#24182;&#19988;&#36890;&#24120;&#21463;&#21040;&#28151;&#26434;&#22240;&#32032;&#30340;&#22256;&#25200;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;TRIALSCOPE&#65292;&#19968;&#20010;&#29992;&#20110;&#20174;&#20154;&#32676;&#32423;&#35266;&#23519;&#25968;&#25454;&#20013;&#25552;&#21462;&#23454;&#38469;&#19990;&#30028;&#35777;&#25454;&#30340;&#32479;&#19968;&#26694;&#26550;&#12290;TRIALSCOPE&#21033;&#29992;&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#26469;&#25193;&#23637;&#35268;&#27169;&#21270;&#30340;&#20020;&#24202;&#25991;&#26412;&#65292;&#37319;&#29992;&#20808;&#36827;&#30340;&#27010;&#29575;&#24314;&#27169;&#36827;&#34892;&#21435;&#22122;&#21644;&#25554;&#34917;&#65292;&#24182;&#32467;&#21512;&#26368;&#20808;&#36827;&#30340;&#22240;&#26524;&#25512;&#26029;&#25216;&#26415;&#26469;&#24212;&#23545;&#24120;&#35265;&#30340;&#28151;&#26434;&#22240;&#32032;&#12290;&#21033;&#29992;&#20020;&#24202;&#35797;&#39564;&#35268;&#33539;&#20316;&#20026;&#36890;&#29992;&#34920;&#31034;&#24418;&#24335;&#65292;TRIALSCOPE&#25552;&#20379;&#20102;&#19968;&#20010;&#19968;&#38190;&#24335;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20351;&#29992;&#35266;&#23519;&#25968;&#25454;&#29983;&#25104;&#21644;&#25512;&#29702;&#20020;&#24202;&#20551;&#35774;&#12290;&#22312;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;&#19968;&#30334;&#19975;&#20010;&#30284;&#30151;&#24739;&#32773;&#30340;&#22823;&#35268;&#27169;&#23454;&#38469;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#21644;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid digitization of real-world data offers an unprecedented opportunity for optimizing healthcare delivery and accelerating biomedical discovery. In practice, however, such data is most abundantly available in unstructured forms, such as clinical notes in electronic medical records (EMRs), and it is generally plagued by confounders. In this paper, we present TRIALSCOPE, a unifying framework for distilling real-world evidence from population-level observational data. TRIALSCOPE leverages biomedical language models to structure clinical text at scale, employs advanced probabilistic modeling for denoising and imputation, and incorporates state-of-the-art causal inference techniques to combat common confounders. Using clinical trial specification as generic representation, TRIALSCOPE provides a turn-key solution to generate and reason with clinical hypotheses using observational data. In extensive experiments and analyses on a large-scale real-world dataset with over one million canc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24046;&#20998;&#38544;&#31169;&#23398;&#20064;&#20013;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#38024;&#23545;&#24046;&#20998;&#38544;&#31169;&#23398;&#20064;&#32422;&#26463;&#30340;&#26032;&#22411;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#36890;&#36807;&#23545;&#33258;&#25105;&#22686;&#24378;&#25968;&#25454;&#36827;&#34892;mixup&#21644;&#21512;&#25104;&#25968;&#25454;&#30340;&#20351;&#29992;&#65292;&#23454;&#29616;&#20102;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.01295</link><description>&lt;p&gt;
DP-Mix&#65306;&#22522;&#20110;Mixup&#30340;&#24046;&#20998;&#38544;&#31169;&#23398;&#20064;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
DP-Mix: Mixup-based Data Augmentation for Differentially Private Learning. (arXiv:2311.01295v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24046;&#20998;&#38544;&#31169;&#23398;&#20064;&#20013;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#38024;&#23545;&#24046;&#20998;&#38544;&#31169;&#23398;&#20064;&#32422;&#26463;&#30340;&#26032;&#22411;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#36890;&#36807;&#23545;&#33258;&#25105;&#22686;&#24378;&#25968;&#25454;&#36827;&#34892;mixup&#21644;&#21512;&#25104;&#25968;&#25454;&#30340;&#20351;&#29992;&#65292;&#23454;&#29616;&#20102;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#22914;&#31616;&#21333;&#30340;&#22270;&#20687;&#21464;&#25442;&#21644;&#32452;&#21512;&#65292;&#22312;&#25913;&#21892;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#29305;&#21035;&#26159;&#22312;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25216;&#26415;&#19982;&#24046;&#20998;&#38544;&#31169;&#23398;&#20064;&#26041;&#27861;&#26681;&#26412;&#19981;&#20860;&#23481;&#65292;&#21407;&#22240;&#26159;&#24046;&#20998;&#38544;&#31169;&#23398;&#20064;&#26041;&#27861;&#20869;&#32622;&#20551;&#35774;&#27599;&#20010;&#35757;&#32451;&#22270;&#20687;&#23545;&#23398;&#20064;&#27169;&#22411;&#30340;&#36129;&#29486;&#26159;&#26377;&#30028;&#30340;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20026;&#20309;&#23545;&#22810;&#26679;&#26412;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65288;&#22914;mixup&#65289;&#30340;&#26420;&#32032;&#24212;&#29992;&#26080;&#27861;&#21462;&#24471;&#33391;&#22909;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#38024;&#23545;&#24046;&#20998;&#38544;&#31169;&#23398;&#20064;&#32422;&#26463;&#30340;&#26032;&#22411;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#31181;&#25216;&#26415;&#65292;DP-Mix_Self&#65292;&#22312;&#33258;&#25105;&#22686;&#24378;&#25968;&#25454;&#19978;&#25191;&#34892;mixup&#65292;&#23454;&#29616;&#20102;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#35774;&#32622;&#19979;&#30340;SoTA&#20998;&#31867;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#31532;&#20108;&#31181;&#25216;&#26415;DP-Mix_Diff&#36827;&#19968;&#27493;&#36890;&#36807;&#23558;&#20174;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#20013;&#33719;&#24471;&#30340;&#21512;&#25104;&#25968;&#25454;&#32435;&#20837;&#22686;&#24378;&#36807;&#31243;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data augmentation techniques, such as simple image transformations and combinations, are highly effective at improving the generalization of computer vision models, especially when training data is limited. However, such techniques are fundamentally incompatible with differentially private learning approaches, due to the latter's built-in assumption that each training image's contribution to the learned model is bounded. In this paper, we investigate why naive applications of multi-sample data augmentation techniques, such as mixup, fail to achieve good performance and propose two novel data augmentation techniques specifically designed for the constraints of differentially private learning. Our first technique, DP-Mix_Self, achieves SoTA classification performance across a range of datasets and settings by performing mixup on self-augmented data. Our second technique, DP-Mix_Diff, further improves performance by incorporating synthetic data from a pre-trained diffusion model into the 
&lt;/p&gt;</description></item><item><title>FlashDecoding++&#26159;&#19968;&#31181;&#24555;&#36895;&#30340;LLM&#25512;&#29702;&#24341;&#25806;&#65292;&#36890;&#36807;&#35299;&#20915;&#21516;&#27493;&#37096;&#20998;softmax&#26356;&#26032;&#12289;&#26410;&#20805;&#20998;&#21033;&#29992;&#25153;&#24179;GEMM&#35745;&#31639;&#21644;&#38745;&#24577;&#25968;&#25454;&#27969;&#23548;&#33268;&#30340;&#24615;&#33021;&#25439;&#22833;&#31561;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#21152;&#36895;&#12290;</title><link>http://arxiv.org/abs/2311.01282</link><description>&lt;p&gt;
FlashDecoding++: &#22312;GPU&#19978;&#21152;&#36895;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#26356;&#24555;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
FlashDecoding++: Faster Large Language Model Inference on GPUs. (arXiv:2311.01282v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01282
&lt;/p&gt;
&lt;p&gt;
FlashDecoding++&#26159;&#19968;&#31181;&#24555;&#36895;&#30340;LLM&#25512;&#29702;&#24341;&#25806;&#65292;&#36890;&#36807;&#35299;&#20915;&#21516;&#27493;&#37096;&#20998;softmax&#26356;&#26032;&#12289;&#26410;&#20805;&#20998;&#21033;&#29992;&#25153;&#24179;GEMM&#35745;&#31639;&#21644;&#38745;&#24577;&#25968;&#25454;&#27969;&#23548;&#33268;&#30340;&#24615;&#33021;&#25439;&#22833;&#31561;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#37325;&#35201;&#24615;&#26085;&#30410;&#22686;&#21152;&#65292;&#21152;&#36895;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#26410;&#35299;&#20915;&#65306;(1) &#21516;&#27493;&#37096;&#20998;softmax&#26356;&#26032;&#12290;softmax&#25805;&#20316;&#38656;&#35201;&#21516;&#27493;&#26356;&#26032;&#27599;&#20010;&#37096;&#20998;softmax&#32467;&#26524;&#65292;&#23548;&#33268;LLM&#20013;&#27880;&#24847;&#21147;&#35745;&#31639;&#30340;&#24320;&#38144;&#22686;&#21152;&#32422;20%&#12290;(2) &#26410;&#20805;&#20998;&#21033;&#29992;&#25153;&#24179;GEMM&#35745;&#31639;&#12290;&#22312;LLM&#25512;&#29702;&#20013;&#25191;&#34892;GEMM&#30340;&#30697;&#38453;&#24418;&#29366;&#26159;&#25153;&#24179;&#30340;&#65292;&#23548;&#33268;&#22312;&#20808;&#21069;&#30340;&#35774;&#35745;&#20013;&#22635;&#20805;&#38646;&#21518;&#35745;&#31639;&#26410;&#20805;&#20998;&#21033;&#29992;&#65292;&#24615;&#33021;&#25439;&#22833;&#36229;&#36807;50%&#12290;(3) &#38745;&#24577;&#25968;&#25454;&#27969;&#23548;&#33268;&#30340;&#24615;&#33021;&#25439;&#22833;&#12290;LLM&#20013;&#30340;&#20869;&#26680;&#24615;&#33021;&#21462;&#20915;&#20110;&#19981;&#21516;&#30340;&#36755;&#20837;&#25968;&#25454;&#29305;&#24449;&#12289;&#30828;&#20214;&#37197;&#32622;&#31561;&#12290;&#21333;&#19968;&#21644;&#38745;&#24577;&#30340;&#25968;&#25454;&#27969;&#21487;&#33021;&#23548;&#33268;LLM&#25512;&#29702;&#20013;&#19981;&#21516;&#24418;&#29366;&#30340;GEMM&#30340;&#24615;&#33021;&#25439;&#22833;&#36798;&#21040;50.25%&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;FlashDecoding++&#65292;&#19968;&#31181;&#24555;&#36895;&#25903;&#25345;&#20027;&#27969;LLM&#21644;&#30828;&#20214;&#21518;&#31471;&#30340;LLM&#25512;&#29702;&#24341;&#25806;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#65292;FlashDecoding++&#23454;&#29616;&#20102;&#20197;&#19979;&#30446;&#26631;&#65306;
&lt;/p&gt;
&lt;p&gt;
As the Large Language Model (LLM) becomes increasingly important in various domains. However, the following challenges still remain unsolved in accelerating LLM inference: (1) Synchronized partial softmax update. The softmax operation requires a synchronized update operation among each partial softmax result, leading to ~20% overheads for the attention computation in LLMs. (2) Under-utilized computation of flat GEMM. The shape of matrices performing GEMM in LLM inference is flat, leading to under-utilized computation and &gt;50% performance loss after padding zeros in previous designs. (3) Performance loss due to static dataflow. Kernel performance in LLM depends on varied input data features, hardware configurations, etc. A single and static dataflow may lead to a 50.25% performance loss for GEMMs of different shapes in LLM inference.  We present FlashDecoding++, a fast LLM inference engine supporting mainstream LLMs and hardware back-ends. To tackle the above challenges, FlashDecoding++
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20998;&#23376;&#22270;&#30340;&#38271;&#31243;&#31070;&#32463;&#21407;&#23376;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21407;&#23376;&#25237;&#23556;&#20026;&#31070;&#32463;&#21407;&#23376;&#24182;&#22312;&#20854;&#20043;&#38388;&#20132;&#25442;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#36828;&#36317;&#31163;&#33410;&#28857;&#20043;&#38388;&#30340;&#36890;&#20449;&#65292;&#32553;&#23567;&#20102;&#20219;&#24847;&#33410;&#28857;&#23545;&#30340;&#30456;&#20114;&#20316;&#29992;&#33539;&#22260;&#12290;</title><link>http://arxiv.org/abs/2311.01276</link><description>&lt;p&gt;
&#38024;&#23545;&#20998;&#23376;&#22270;&#30340;&#38271;&#36317;&#31163;&#31070;&#32463;&#21407;&#23376;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Long-Range Neural Atom Learning for Molecular Graphs. (arXiv:2311.01276v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01276
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20998;&#23376;&#22270;&#30340;&#38271;&#31243;&#31070;&#32463;&#21407;&#23376;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21407;&#23376;&#25237;&#23556;&#20026;&#31070;&#32463;&#21407;&#23376;&#24182;&#22312;&#20854;&#20043;&#38388;&#20132;&#25442;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#36828;&#36317;&#31163;&#33410;&#28857;&#20043;&#38388;&#30340;&#36890;&#20449;&#65292;&#32553;&#23567;&#20102;&#20219;&#24847;&#33410;&#28857;&#23545;&#30340;&#30456;&#20114;&#20316;&#29992;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#20998;&#23376;&#22270;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;GNN&#20027;&#35201;&#25797;&#38271;&#21033;&#29992;&#30701;&#31243;&#30456;&#20114;&#20316;&#29992;&#65288;SRI&#65289;&#65292;&#20294;&#38590;&#20197;&#25429;&#25417;&#38271;&#31243;&#30456;&#20114;&#20316;&#29992;&#65288;LRI&#65289;&#65292;&#36825;&#20004;&#32773;&#23545;&#20110;&#30830;&#23450;&#20998;&#23376;&#24615;&#36136;&#37117;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#25152;&#26377;&#21407;&#23376;&#38544;&#24335;&#25237;&#23556;&#20026;&#23569;&#25968;&#31070;&#32463;&#21407;&#23376;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#31070;&#32463;&#21407;&#23376;&#25277;&#35937;&#20986;&#20998;&#23376;&#20869;&#21407;&#23376;&#32452;&#30340;&#38598;&#20307;&#20449;&#24687;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26126;&#30830;&#22320;&#22312;&#31070;&#32463;&#21407;&#23376;&#20043;&#38388;&#20132;&#25442;&#20449;&#24687;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#19968;&#31181;&#22686;&#24378;&#23558;&#20854;&#37325;&#26032;&#25237;&#23556;&#21040;&#21407;&#23376;&#30340;&#34920;&#31034;&#19978;&#12290;&#36890;&#36807;&#36825;&#31181;&#26426;&#21046;&#65292;&#31070;&#32463;&#21407;&#23376;&#22312;&#36828;&#36317;&#31163;&#33410;&#28857;&#20043;&#38388;&#24314;&#31435;&#36890;&#20449;&#36890;&#36947;&#65292;&#26377;&#25928;&#22320;&#23558;&#20219;&#24847;&#33410;&#28857;&#23545;&#30340;&#30456;&#20114;&#20316;&#29992;&#33539;&#22260;&#20943;&#23569;&#21040;&#21333;&#27425;&#36339;&#36291;&#12290;&#20026;&#20102;&#20174;&#29289;&#29702;&#35282;&#24230;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#23457;&#26597;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#23427;&#19982;&#20256;&#32479;&#30340;LRI&#35745;&#31639;&#26041;&#27861;Ewald&#27714;&#21644;&#30340;&#32852;&#31995;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have been widely adopted for drug discovery with molecular graphs. Nevertheless, current GNNs are mainly good at leveraging short-range interactions (SRI) but struggle to capture long-range interactions (LRI), both of which are crucial for determining molecular properties. To tackle this issue, we propose a method that implicitly projects all original atoms into a few Neural Atoms, which abstracts the collective information of atomic groups within a molecule. Specifically, we explicitly exchange the information among neural atoms and project them back to the atoms' representations as an enhancement. With this mechanism, neural atoms establish the communication channels among distant nodes, effectively reducing the interaction scope of arbitrary node pairs into a single hop. To provide an inspection of our method from a physical perspective, we reveal its connection with the traditional LRI calculation method, Ewald Summation. We conduct extensive experiment
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#27861;&#24459;&#39046;&#22495;&#30340;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;&#65292;&#32508;&#21512;&#32771;&#34385;&#24615;&#33021;&#21644;&#33021;&#28304;&#28040;&#32791;&#31561;&#25351;&#26631;&#65292;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20256;&#32479;&#26041;&#27861;&#30340;&#20248;&#21155;&#65292;&#24182;&#24378;&#35843;&#20102;&#22312;&#24615;&#33021;&#30456;&#36817;&#30340;&#24773;&#20917;&#19979;&#24212;&#37325;&#35270;&#29983;&#20135;&#25104;&#26412;&#12289;&#33021;&#28304;&#28040;&#32791;&#21644;&#30899;&#36275;&#36857;&#31561;&#26041;&#38754;&#30340;&#32771;&#37327;&#12290;</title><link>http://arxiv.org/abs/2311.01256</link><description>&lt;p&gt;
&#22522;&#20110;&#33021;&#28304;&#30340;&#27861;&#24459;&#39046;&#22495;&#25991;&#26412;&#20998;&#31867;&#24120;&#35265;&#26041;&#27861;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
An energy-based comparative analysis of common approaches to text classification in the Legal domain. (arXiv:2311.01256v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#27861;&#24459;&#39046;&#22495;&#30340;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;&#65292;&#32508;&#21512;&#32771;&#34385;&#24615;&#33021;&#21644;&#33021;&#28304;&#28040;&#32791;&#31561;&#25351;&#26631;&#65292;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20256;&#32479;&#26041;&#27861;&#30340;&#20248;&#21155;&#65292;&#24182;&#24378;&#35843;&#20102;&#22312;&#24615;&#33021;&#30456;&#36817;&#30340;&#24773;&#20917;&#19979;&#24212;&#37325;&#35270;&#29983;&#20135;&#25104;&#26412;&#12289;&#33021;&#28304;&#28040;&#32791;&#21644;&#30899;&#36275;&#36857;&#31561;&#26041;&#38754;&#30340;&#32771;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37096;&#20998;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#35780;&#20272;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#36861;&#27714;&#26368;&#20339;&#24615;&#33021;&#30340;&#31454;&#20105;&#20013;&#65292;&#32463;&#24120;&#24573;&#35270;&#35768;&#22810;&#37325;&#35201;&#22240;&#32032;&#65292;&#32780;&#20107;&#23454;&#19978;&#65292;&#36825;&#20123;&#22240;&#32032;&#24212;&#35813;&#34987;&#20180;&#32454;&#32771;&#34385;&#12290;&#23454;&#38469;&#19978;&#65292;&#26377;&#26102;&#19981;&#21516;&#26041;&#27861;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#65292;&#32780;&#29983;&#20135;&#25104;&#26412;&#12289;&#33021;&#28304;&#28040;&#32791;&#21644;&#30899;&#36275;&#36857;&#31561;&#22240;&#32032;&#24517;&#39035;&#32771;&#34385;&#22312;&#20869;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30340;NLP&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22312;LexGLUE&#22522;&#20934;&#19978;&#23545;LLM&#21644;&#20256;&#32479;&#26041;&#27861;&#65288;&#20363;&#22914;SVM&#65289;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#23450;&#37327;&#27604;&#36739;&#65292;&#21516;&#26102;&#32771;&#34385;&#24615;&#33021;&#65288;&#26631;&#20934;&#25351;&#26631;&#65289;&#21644;&#20854;&#20182;&#25351;&#26631;&#65292;&#22914;&#26102;&#38388;&#12289;&#32791;&#33021;&#21644;&#25104;&#26412;&#65292;&#24635;&#20043;&#23601;&#26159;&#30899;&#36275;&#36857;&#12290;&#22312;&#25105;&#20204;&#30340;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#20998;&#21035;&#32771;&#34385;&#20102;&#21407;&#22411;&#35774;&#35745;&#38454;&#27573;&#65288;&#36890;&#36807;&#35757;&#32451;-&#39564;&#35777;-&#27979;&#35797;&#36845;&#20195;&#36827;&#34892;&#27169;&#22411;&#36873;&#25321;&#65289;&#21644;&#29983;&#20135;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most Machine Learning research evaluates the best solutions in terms of performance. However, in the race for the best performing model, many important aspects are often overlooked when, on the contrary, they should be carefully considered. In fact, sometimes the gaps in performance between different approaches are neglectable, whereas factors such as production costs, energy consumption, and carbon footprint must take into consideration. Large Language Models (LLMs) are extensively adopted to address NLP problems in academia and industry. In this work, we present a detailed quantitative comparison of LLM and traditional approaches (e.g. SVM) on the LexGLUE benchmark, which takes into account both performance (standard indices) and alternative metrics such as timing, power consumption and cost, in a word: the carbon-footprint. In our analysis, we considered the prototyping phase (model selection by training-validation-test iterations) and in-production phases separately, since they fol
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;Sanitized Clustering Against confounding Bias (SCAB)&#8221;&#30340;&#26032;&#30340;&#32858;&#31867;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#22797;&#26434;&#25968;&#25454;&#30340;&#35821;&#20041;&#28508;&#31354;&#38388;&#20013;&#21435;&#38500;&#28151;&#28102;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2311.01252</link><description>&lt;p&gt;
&#38450;&#27490;&#28151;&#26434;&#20559;&#24046;&#30340;&#28165;&#27927;&#32858;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Sanitized Clustering against Confounding Bias. (arXiv:2311.01252v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01252
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;Sanitized Clustering Against confounding Bias (SCAB)&#8221;&#30340;&#26032;&#30340;&#32858;&#31867;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#22797;&#26434;&#25968;&#25454;&#30340;&#35821;&#20041;&#28508;&#31354;&#38388;&#20013;&#21435;&#38500;&#28151;&#28102;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#25968;&#25454;&#38598;&#19981;&#21487;&#36991;&#20813;&#22320;&#21253;&#21547;&#30001;&#25968;&#25454;&#25910;&#38598;&#36807;&#31243;&#20013;&#19981;&#21516;&#26469;&#28304;&#25110;&#26465;&#20214;&#24341;&#36215;&#30340;&#20559;&#24046;&#12290;&#22240;&#27492;&#65292;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#26412;&#36523;&#20316;&#20026;&#28151;&#28102;&#22240;&#32032;&#24178;&#25200;&#20102;&#32858;&#31867;&#20998;&#26512;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#22312;&#32858;&#31867;&#20043;&#21069;&#23558;&#25968;&#25454;&#25237;&#24433;&#21040;&#19982;&#28151;&#28102;&#22240;&#32032;&#25193;&#23637;&#30340;&#27491;&#20132;&#34917;&#31354;&#38388;&#19978;&#26469;&#28040;&#38500;&#20559;&#24046;&#12290;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;&#32858;&#31867;&#22240;&#32032;&#21644;&#28151;&#28102;&#22240;&#32032;&#22312;&#21407;&#22987;&#29305;&#24449;&#31354;&#38388;&#20013;&#34987;&#31895;&#30053;&#22320;&#32771;&#34385;&#65292;&#29702;&#24819;&#24773;&#20917;&#19979;&#20551;&#35774;&#25968;&#25454;&#19982;&#28151;&#28102;&#22240;&#32032;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26159;&#32447;&#24615;&#30340;&#65292;&#20197;&#26041;&#20415;&#27714;&#35299;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#36866;&#29992;&#33539;&#22260;&#26377;&#38480;&#65292;&#22240;&#20026;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#25968;&#25454;&#36890;&#24120;&#19982;&#28151;&#28102;&#22240;&#32032;&#21576;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#30456;&#20851;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;Sanitized Clustering Against confounding Bias (SCAB)&#8221;&#30340;&#26032;&#30340;&#32858;&#31867;&#26694;&#26550;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#20381;&#36182;&#24230;&#22312;&#22797;&#26434;&#25968;&#25454;&#30340;&#35821;&#20041;&#28508;&#31354;&#38388;&#20013;&#21435;&#38500;&#28151;&#28102;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world datasets inevitably contain biases that arise from different sources or conditions during data collection. Consequently, such inconsistency itself acts as a confounding factor that disturbs the cluster analysis. Existing methods eliminate the biases by projecting data onto the orthogonal complement of the subspace expanded by the confounding factor before clustering. Therein, the interested clustering factor and the confounding factor are coarsely considered in the raw feature space, where the correlation between the data and the confounding factor is ideally assumed to be linear for convenient solutions. These approaches are thus limited in scope as the data in real applications is usually complex and non-linearly correlated with the confounding factor. This paper presents a new clustering framework named Sanitized Clustering Against confounding Bias (SCAB), which removes the confounding factor in the semantic latent space of complex data through a non-linear dependence mea
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#35270;&#35273;&#35302;&#35273;&#20256;&#24863;&#22120;&#21644;&#27169;&#20223;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#37197;&#23545;&#20248;&#21270;&#35302;&#35273;&#21147;&#37327;&#26354;&#32447;&#21644;&#31616;&#21270;&#20256;&#24863;&#22120;&#24212;&#29992;&#65292;&#23545;&#25509;&#35302;&#20016;&#23500;&#30340;&#25805;&#20316;&#20219;&#21153;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2311.01248</link><description>&lt;p&gt;
&#23558;&#20854;&#25512;&#21521;&#23637;&#31034;&#26497;&#38480;&#65306;&#22810;&#27169;&#24577;&#35270;&#35273;&#35302;&#35273;&#27169;&#20223;&#23398;&#20064;&#19982;&#21147;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Push it to the Demonstrated Limit: Multimodal Visuotactile Imitation Learning with Force Matching. (arXiv:2311.01248v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01248
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#35270;&#35273;&#35302;&#35273;&#20256;&#24863;&#22120;&#21644;&#27169;&#20223;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#37197;&#23545;&#20248;&#21270;&#35302;&#35273;&#21147;&#37327;&#26354;&#32447;&#21644;&#31616;&#21270;&#20256;&#24863;&#22120;&#24212;&#29992;&#65292;&#23545;&#25509;&#35302;&#20016;&#23500;&#30340;&#25805;&#20316;&#20219;&#21153;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#23398;&#35302;&#35273;&#20256;&#24863;&#22120;&#24050;&#32463;&#25104;&#20026;&#26426;&#22120;&#20154;&#25805;&#20316;&#36807;&#31243;&#20013;&#33719;&#21462;&#23494;&#38598;&#25509;&#35302;&#20449;&#24687;&#30340;&#26377;&#25928;&#25163;&#27573;&#12290;&#26368;&#36817;&#24341;&#20837;&#30340;&#8220;&#36879;&#35270;&#20320;&#30340;&#30382;&#32932;&#8221;&#65288;STS&#65289;&#22411;&#20256;&#24863;&#22120;&#20855;&#26377;&#35270;&#35273;&#21644;&#35302;&#35273;&#27169;&#24335;&#65292;&#36890;&#36807;&#21033;&#29992;&#21322;&#36879;&#26126;&#34920;&#38754;&#21644;&#21487;&#25511;&#29031;&#26126;&#23454;&#29616;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#35270;&#35273;&#35302;&#35273;&#20256;&#24863;&#19982;&#27169;&#20223;&#23398;&#20064;&#22312;&#23500;&#26377;&#25509;&#35302;&#30340;&#25805;&#20316;&#20219;&#21153;&#20013;&#30340;&#22909;&#22788;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#35302;&#35273;&#21147;&#27979;&#37327;&#21644;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#22312;&#36816;&#21160;&#31034;&#33539;&#20013;&#20135;&#29983;&#26356;&#22909;&#21305;&#37197;&#20154;&#20307;&#31034;&#33539;&#32773;&#30340;&#21147;&#26354;&#32447;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#28155;&#21152;&#20102;&#35270;&#35273;/&#35302;&#35273;STS&#27169;&#24335;&#20999;&#25442;&#20316;&#20026;&#25511;&#21046;&#31574;&#30053;&#36755;&#20986;&#65292;&#31616;&#21270;&#20256;&#24863;&#22120;&#30340;&#24212;&#29992;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#31181;&#35266;&#23519;&#37197;&#32622;&#65292;&#27604;&#36739;&#21644;&#23545;&#27604;&#20102;&#35270;&#35273;/&#35302;&#35273;&#25968;&#25454;&#65288;&#21253;&#25324;&#27169;&#24335;&#20999;&#25442;&#21644;&#19981;&#20999;&#25442;&#65289;&#19982;&#25163;&#33109;&#25346;&#36733;&#30340;&#30524;&#22312;&#25163;&#25668;&#20687;&#26426;&#30340;&#35270;&#35273;&#25968;&#25454;&#30340;&#20215;&#20540;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#24191;&#27867;&#30340;&#23454;&#39564;&#31995;&#21015;&#19978;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optical tactile sensors have emerged as an effective means to acquire dense contact information during robotic manipulation. A recently-introduced `see-through-your-skin' (STS) variant of this type of sensor has both visual and tactile modes, enabled by leveraging a semi-transparent surface and controllable lighting. In this work, we investigate the benefits of pairing visuotactile sensing with imitation learning for contact-rich manipulation tasks. First, we use tactile force measurements and a novel algorithm during kinesthetic teaching to yield a force profile that better matches that of the human demonstrator. Second, we add visual/tactile STS mode switching as a control policy output, simplifying the application of the sensor. Finally, we study multiple observation configurations to compare and contrast the value of visual/tactile data (both with and without mode switching) with visual data from a wrist-mounted eye-in-hand camera. We perform an extensive series of experiments on a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#36924;&#36817;&#22810;&#20010;&#25968;&#23398;&#36816;&#31639;&#36827;&#34892;&#34920;&#36798;&#24335;&#25512;&#23548;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21644;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#32534;&#30721;&#22120;&#23454;&#20363;&#21270;&#65292;&#25506;&#32034;&#20102;&#19981;&#21516;&#32534;&#30721;&#26426;&#21046;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#36924;&#36817;&#26041;&#31243;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2311.01230</link><description>&lt;p&gt;
&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#22810;&#20010;&#25968;&#23398;&#36816;&#31639;&#25512;&#23548;
&lt;/p&gt;
&lt;p&gt;
Multi-Operational Mathematical Derivations in Latent Space. (arXiv:2311.01230v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01230
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#36924;&#36817;&#22810;&#20010;&#25968;&#23398;&#36816;&#31639;&#36827;&#34892;&#34920;&#36798;&#24335;&#25512;&#23548;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21644;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#32534;&#30721;&#22120;&#23454;&#20363;&#21270;&#65292;&#25506;&#32034;&#20102;&#19981;&#21516;&#32534;&#30721;&#26426;&#21046;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#36924;&#36817;&#26041;&#31243;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#36924;&#36817;&#22810;&#20010;&#25968;&#23398;&#36816;&#31639;&#36827;&#34892;&#34920;&#36798;&#24335;&#25512;&#23548;&#30340;&#21487;&#33021;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19981;&#21516;&#30340;&#22810;&#25805;&#20316;&#34920;&#31034;&#33539;&#24335;&#65292;&#23558;&#25968;&#23398;&#36816;&#31639;&#24314;&#27169;&#20026;&#26174;&#24335;&#30340;&#20960;&#20309;&#21464;&#25442;&#12290;&#36890;&#36807;&#21033;&#29992;&#31526;&#21495;&#24341;&#25806;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;61K&#20010;&#21069;&#25552;&#21644;6&#20010;&#36816;&#31639;&#31526;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#20998;&#26512;&#20102;&#27599;&#20010;&#33539;&#24335;&#22312;&#19982;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#32534;&#30721;&#22120;&#23454;&#20363;&#21270;&#26102;&#30340;&#24615;&#36136;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#32534;&#30721;&#26426;&#21046;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#22914;&#20309;&#36924;&#36817;&#26041;&#31243;&#25512;&#29702;&#65292;&#24182;&#25506;&#35752;&#20102;&#23398;&#20064;&#19981;&#21516;&#36816;&#31639;&#31526;&#21644;&#22312;&#21333;&#20010;&#36816;&#31639;&#20013;&#19987;&#38376;&#21270;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#20197;&#21450;&#25903;&#25345;&#22810;&#27493;&#25512;&#23548;&#21644;&#36229;&#36234;&#20998;&#24067;&#24191;&#20041;&#21270;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#65292;&#22810;&#25805;&#20316;&#33539;&#24335;&#23545;&#20110;&#35299;&#24320;&#19981;&#21516;&#36816;&#31639;&#31526;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#21516;&#26102;&#21487;&#20197;&#21306;&#20998;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the possibility of approximating multiple mathematical operations in latent space for expression derivation. To this end, we introduce different multi-operational representation paradigms, modelling mathematical operations as explicit geometric transformations. By leveraging a symbolic engine, we construct a large-scale dataset comprising 1.7M derivation steps stemming from 61K premises and 6 operators, analysing the properties of each paradigm when instantiated with state-of-the-art neural encoders. Specifically, we investigate how different encoding mechanisms can approximate equational reasoning in latent space, exploring the trade-off between learning different operators and specialising within single operations, as well as the ability to support multi-step derivations and out-of-distribution generalisation. Our empirical analysis reveals that the multi-operational paradigm is crucial for disentangling different operators, while discriminating the conclusion
&lt;/p&gt;</description></item><item><title>&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#31361;&#20986;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#26679;&#26412;&#36136;&#37327;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#26041;&#38754;&#30340;&#20248;&#21183;&#25913;&#36827;&#20102;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#32508;&#36848;&#25552;&#20379;&#20102;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#21457;&#23637;&#30340;&#27010;&#36848;&#65292;&#24182;&#25506;&#35752;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20998;&#31867;&#27861;&#21644;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2311.01223</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#30340;&#25193;&#25955;&#27169;&#22411;: &#19968;&#20221;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models for Reinforcement Learning: A Survey. (arXiv:2311.01223v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01223
&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#31361;&#20986;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#26679;&#26412;&#36136;&#37327;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#26041;&#38754;&#30340;&#20248;&#21183;&#25913;&#36827;&#20102;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#32508;&#36848;&#25552;&#20379;&#20102;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#21457;&#23637;&#30340;&#27010;&#36848;&#65292;&#24182;&#25506;&#35752;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20998;&#31867;&#27861;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#19968;&#31181;&#31361;&#20986;&#30340;&#29983;&#25104;&#27169;&#22411;&#31867;&#21035;&#24050;&#32463;&#20986;&#29616;&#65292;&#36229;&#36234;&#20102;&#20197;&#24448;&#26041;&#27861;&#22312;&#26679;&#26412;&#36136;&#37327;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#25913;&#36827;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#35299;&#20915;&#26041;&#26696;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#65292;&#21253;&#25324;&#20316;&#20026;&#36712;&#36857;&#35268;&#21010;&#22120;&#12289;&#34920;&#36798;&#33021;&#21147;&#20016;&#23500;&#30340;&#31574;&#30053;&#31867;&#21035;&#12289;&#25968;&#25454;&#21512;&#25104;&#22120;&#31561;&#12290;&#26412;&#32508;&#36848;&#26088;&#22312;&#25552;&#20379;&#35813;&#26032;&#20852;&#39046;&#22495;&#21457;&#23637;&#30340;&#27010;&#36848;&#65292;&#24182;&#24076;&#26395;&#33021;&#21551;&#21457;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23457;&#26597;&#20102;&#24403;&#21069;RL&#31639;&#27861;&#36935;&#21040;&#30340;&#19968;&#20123;&#25361;&#25112;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26681;&#25454;&#25193;&#25955;&#27169;&#22411;&#22312;RL&#20013;&#25152;&#25198;&#28436;&#30340;&#35282;&#33394;&#65292;&#25552;&#20986;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#35299;&#20915;&#29616;&#26377;&#25361;&#25112;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#27010;&#36848;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#21508;&#31181;&#19982;RL&#30456;&#20851;&#20219;&#21153;&#20013;&#30340;&#25104;&#21151;&#24212;&#29992;&#65292;&#24182;&#35752;&#35770;&#20102;&#24403;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#36825;&#39033;&#32508;&#36848;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#35265;&#35299;&#65292;&#37325;&#28857;&#26159;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#21644;&#24212;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have emerged as a prominent class of generative models, surpassing previous methods regarding sample quality and training stability. Recent works have shown the advantages of diffusion models in improving reinforcement learning (RL) solutions, including as trajectory planners, expressive policy classes, data synthesizers, etc. This survey aims to provide an overview of the advancements in this emerging field and hopes to inspire new avenues of research. First, we examine several challenges encountered by current RL algorithms. Then, we present a taxonomy of existing methods based on the roles played by diffusion models in RL and explore how the existing challenges are addressed. We further outline successful applications of diffusion models in various RL-related tasks while discussing the limitations of current approaches. Finally, we conclude the survey and offer insights into future research directions, focusing on enhancing model performance and applying diffusion m
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#35774;&#35745;&#20102;Injectivity Bit Flip Attack&#26469;&#38024;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#25104;&#21151;&#22320;&#38477;&#20302;&#20102;&#20854;&#23545;&#22270;&#32467;&#26500;&#30340;&#35782;&#21035;&#33021;&#21147;&#21644;&#34920;&#36798;&#33021;&#21147;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#20854;&#23545;&#20301;&#21453;&#36716;&#25915;&#20987;&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01205</link><description>&lt;p&gt;
&#20351;&#29992;&#20301;&#21453;&#36716;&#25915;&#20987;&#22270;&#31070;&#32463;&#32593;&#32476;&#65306;Weisfeiler&#21644;Lehman&#21464;&#24471;&#20919;&#28448;&#20102;
&lt;/p&gt;
&lt;p&gt;
Attacking Graph Neural Networks with Bit Flips: Weisfeiler and Lehman Go Indifferent. (arXiv:2311.01205v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01205
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35774;&#35745;&#20102;Injectivity Bit Flip Attack&#26469;&#38024;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#25104;&#21151;&#22320;&#38477;&#20302;&#20102;&#20854;&#23545;&#22270;&#32467;&#26500;&#30340;&#35782;&#21035;&#33021;&#21147;&#21644;&#34920;&#36798;&#33021;&#21147;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#20854;&#23545;&#20301;&#21453;&#36716;&#25915;&#20987;&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25915;&#20987;&#20027;&#35201;&#38598;&#20013;&#22312;&#22270;&#27602;&#21270;&#21644;&#35268;&#36991;&#19978;&#65292;&#24573;&#30053;&#20102;&#32593;&#32476;&#30340;&#26435;&#37325;&#21644;&#20559;&#32622;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;&#26435;&#37325;&#30340;&#25925;&#38556;&#27880;&#20837;&#25915;&#20987;&#65292;&#22914;&#29992;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#20301;&#21453;&#36716;&#25915;&#20987;&#65292;&#27809;&#26377;&#32771;&#34385;&#21040;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#29420;&#29305;&#23646;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#27880;&#20837;&#29575;&#20301;&#21453;&#36716;&#25915;&#20987;&#65288;Injectivity Bit Flip Attack&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#30340;&#20301;&#21453;&#36716;&#25915;&#20987;&#12290;&#25105;&#20204;&#30340;&#25915;&#20987;&#38024;&#23545;&#37327;&#21270;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21487;&#23398;&#20064;&#37051;&#22495;&#32858;&#21512;&#20989;&#25968;&#65292;&#38477;&#20302;&#20102;&#20854;&#21306;&#20998;&#22270;&#32467;&#26500;&#30340;&#33021;&#21147;&#65292;&#22833;&#21435;&#20102;Weisfeiler-Lehman&#27979;&#35797;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#21033;&#29992;&#29305;&#23450;&#20110;&#26576;&#20123;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#25968;&#23398;&#23646;&#24615;&#21487;&#33021;&#20250;&#26174;&#33879;&#22686;&#21152;&#20854;&#23545;&#20301;&#21453;&#36716;&#25915;&#20987;&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#12290;&#27880;&#20837;&#29575;&#20301;&#21453;&#36716;&#25915;&#20987;&#21487;&#20197;&#23558;&#21508;&#31181;&#22270;&#23646;&#24615;&#39044;&#27979;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#26368;&#22823;&#34920;&#36798;&#24615;&#21516;&#26500;&#32593;&#32476;&#38477;&#32423;&#20026;&#38543;&#26426;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prior attacks on graph neural networks have mostly focused on graph poisoning and evasion, neglecting the network's weights and biases. Traditional weight-based fault injection attacks, such as bit flip attacks used for convolutional neural networks, do not consider the unique properties of graph neural networks. We propose the Injectivity Bit Flip Attack, the first bit flip attack designed specifically for graph neural networks. Our attack targets the learnable neighborhood aggregation functions in quantized message passing neural networks, degrading their ability to distinguish graph structures and losing the expressivity of the Weisfeiler-Lehman test. Our findings suggest that exploiting mathematical properties specific to certain graph neural network architectures can significantly increase their vulnerability to bit flip attacks. Injectivity Bit Flip Attacks can degrade the maximal expressive Graph Isomorphism Networks trained on various graph property prediction datasets to rando
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22312;&#36793;&#32536;&#24863;&#30693;&#35774;&#22791;&#19978;&#30340;&#32852;&#37030;&#23398;&#20064;&#30340;&#30740;&#31350;&#29616;&#29366;&#12290;&#36793;&#32536;&#24863;&#30693;&#35774;&#22791;&#30340;&#24555;&#36895;&#22686;&#38271;&#20351;&#24471;&#30417;&#27979;&#29615;&#22659;&#29305;&#24449;&#24182;&#33719;&#21462;&#29615;&#22659;&#20449;&#24687;&#25104;&#20026;&#21487;&#33021;&#65292;&#20294;&#20113;&#31471;&#25110;&#26381;&#21153;&#22120;&#19978;&#30340;&#20998;&#26512;&#38754;&#20020;&#38544;&#31169;&#12289;&#30828;&#20214;&#21644;&#36830;&#25509;&#24615;&#31561;&#25361;&#25112;&#12290;&#32852;&#37030;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#36880;&#28176;&#21463;&#21040;&#20851;&#27880;&#12290;</title><link>http://arxiv.org/abs/2311.01201</link><description>&lt;p&gt;
&#22312;&#36793;&#32536;&#24863;&#30693;&#35774;&#22791;&#19978;&#30340;&#32852;&#37030;&#23398;&#20064;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Federated Learning on Edge Sensing Devices: A Review. (arXiv:2311.01201v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01201
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22312;&#36793;&#32536;&#24863;&#30693;&#35774;&#22791;&#19978;&#30340;&#32852;&#37030;&#23398;&#20064;&#30340;&#30740;&#31350;&#29616;&#29366;&#12290;&#36793;&#32536;&#24863;&#30693;&#35774;&#22791;&#30340;&#24555;&#36895;&#22686;&#38271;&#20351;&#24471;&#30417;&#27979;&#29615;&#22659;&#29305;&#24449;&#24182;&#33719;&#21462;&#29615;&#22659;&#20449;&#24687;&#25104;&#20026;&#21487;&#33021;&#65292;&#20294;&#20113;&#31471;&#25110;&#26381;&#21153;&#22120;&#19978;&#30340;&#20998;&#26512;&#38754;&#20020;&#38544;&#31169;&#12289;&#30828;&#20214;&#21644;&#36830;&#25509;&#24615;&#31561;&#25361;&#25112;&#12290;&#32852;&#37030;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#36880;&#28176;&#21463;&#21040;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24555;&#36895;&#22686;&#38271;&#30340;&#36793;&#32536;&#24863;&#30693;&#35774;&#22791;&#65288;&#22914;&#29289;&#32852;&#32593;&#12289;&#31227;&#21160;&#35774;&#22791;&#21644;&#21487;&#31359;&#25140;&#35774;&#22791;&#65289;&#21450;&#20854;&#38598;&#25104;&#20256;&#24863;&#22120;&#30340;&#27979;&#37327;&#33021;&#21147;&#20351;&#24471;&#30417;&#25511;&#29615;&#22659;&#29305;&#24449;&#12289;&#19982;&#20043;&#20132;&#20114;&#24182;&#33719;&#21462;&#29615;&#22659;&#20449;&#24687;&#25104;&#20026;&#21487;&#33021;&#12290;&#23613;&#31649;&#36825;&#20123;&#35774;&#22791;&#20307;&#31215;&#23567;&#12289;&#25968;&#25454;&#23384;&#20648;&#21644;&#22788;&#29702;&#33021;&#21147;&#36739;&#20302;&#65292;&#20294;&#23427;&#20204;&#20135;&#29983;&#20102;&#22823;&#37327;&#30340;&#25968;&#25454;&#12290;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#25910;&#38598;&#21644;&#22788;&#29702;&#24212;&#29992;&#39046;&#22495;&#21253;&#25324;&#21307;&#30103;&#20445;&#20581;&#12289;&#29615;&#22659;&#65288;&#21253;&#25324;&#31354;&#27668;&#36136;&#37327;&#21644;&#27745;&#26579;&#27700;&#24179;&#65289;&#12289;&#27773;&#36710;&#12289;&#24037;&#19994;&#12289;&#33322;&#31354;&#33322;&#22825;&#21644;&#20892;&#19994;&#31561;&#12290;&#20174;&#36793;&#32536;&#35774;&#22791;&#25910;&#38598;&#30340;&#36825;&#20123;&#28023;&#37327;&#24863;&#30693;&#25968;&#25454;&#20351;&#29992;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#26041;&#27861;&#36827;&#34892;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;&#22312;&#20113;&#31471;&#25110;&#26381;&#21153;&#22120;&#19978;&#36827;&#34892;&#20998;&#26512;&#20250;&#24102;&#26469;&#19982;&#38544;&#31169;&#12289;&#30828;&#20214;&#21644;&#36830;&#25509;&#24615;&#38480;&#21046;&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#27491;&#36880;&#28176;&#25104;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to monitor ambient characteristics, interact with them, and derive information about the surroundings has been made possible by the rapid proliferation of edge sensing devices like IoT, mobile, and wearable devices and their measuring capabilities with integrated sensors. Even though these devices are small and have less capacity for data storage and processing, they produce vast amounts of data. Some example application areas where sensor data is collected and processed include healthcare, environmental (including air quality and pollution levels), automotive, industrial, aerospace, and agricultural applications. These enormous volumes of sensing data collected from the edge devices are analyzed using a variety of Machine Learning (ML) and Deep Learning (DL) approaches. However, analyzing them on the cloud or a server presents challenges related to privacy, hardware, and connectivity limitations. Federated Learning (FL) is emerging as a solution to these problems while pre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25345;&#32493;&#23398;&#20064;&#22312;&#35821;&#35328;&#36716;&#25442;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#22312;&#26356;&#26032;&#35821;&#35328;&#27169;&#22411;&#26102;&#65292;&#21069;&#21521;&#36716;&#31227;&#25928;&#26524;&#36739;&#22909;&#19988;&#19982;&#35821;&#35328;&#39034;&#24207;&#26080;&#20851;&#65292;&#20294;&#21518;&#21521;&#36716;&#31227;&#25928;&#26524;&#21487;&#33021;&#21462;&#20915;&#20110;&#26032;&#35821;&#35328;&#30340;&#39034;&#24207;&#21644;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2311.01200</link><description>&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#22312;&#35821;&#35328;&#36716;&#25442;&#20013;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Study of Continual Learning Under Language Shift. (arXiv:2311.01200v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25345;&#32493;&#23398;&#20064;&#22312;&#35821;&#35328;&#36716;&#25442;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#22312;&#26356;&#26032;&#35821;&#35328;&#27169;&#22411;&#26102;&#65292;&#21069;&#21521;&#36716;&#31227;&#25928;&#26524;&#36739;&#22909;&#19988;&#19982;&#35821;&#35328;&#39034;&#24207;&#26080;&#20851;&#65292;&#20294;&#21518;&#21521;&#36716;&#31227;&#25928;&#26524;&#21487;&#33021;&#21462;&#20915;&#20110;&#26032;&#35821;&#35328;&#30340;&#39034;&#24207;&#21644;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#25968;&#25454;&#21644;&#27169;&#22411;&#35268;&#27169;&#30340;&#22686;&#21152;&#23548;&#33268;&#20102;&#24040;&#22823;&#30340;&#35757;&#32451;&#25104;&#26412;&#12290;&#22312;&#38543;&#26102;&#38388;&#25512;&#31227;&#32780;&#20986;&#29616;&#26032;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#26356;&#26032;&#27169;&#22411;&#32780;&#19981;&#26159;&#23436;&#20840;&#37325;&#26032;&#35757;&#32451;&#21487;&#20197;&#24102;&#26469;&#26174;&#33879;&#30340;&#25910;&#30410;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#26032;&#35821;&#35328;&#20986;&#29616;&#26102;&#26356;&#26032;&#35821;&#35328;&#27169;&#22411;&#26102;&#30340;&#22909;&#22788;&#21644;&#24330;&#31471;&#65292;&#21363;&#22312;&#35821;&#35328;&#36716;&#25442;&#20013;&#25345;&#32493;&#23398;&#20064;&#30340;&#24773;&#20917;&#12290;&#20174;&#21333;&#35821;&#33521;&#35821;&#35821;&#35328;&#27169;&#22411;&#20986;&#21457;&#65292;&#25105;&#20204;&#36880;&#27493;&#28155;&#21152;&#20102;&#26469;&#33258;&#25386;&#23041;&#35821;&#21644;&#20912;&#23707;&#35821;&#30340;&#25968;&#25454;&#65292;&#20197;&#30740;&#31350;&#21069;&#21521;&#21644;&#21518;&#21521;&#36716;&#31227;&#25928;&#26524;&#22914;&#20309;&#21462;&#20915;&#20110;&#39044;&#35757;&#32451;&#39034;&#24207;&#21644;&#35821;&#35328;&#29305;&#24449;&#65292;&#23545;&#20110;&#19981;&#21516;&#30340;&#27169;&#22411;&#22823;&#23567;&#21644;&#23398;&#20064;&#29575;&#35843;&#24230;&#22120;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#21069;&#21521;&#36716;&#31227;&#20027;&#35201;&#26159;&#27491;&#21521;&#30340;&#65292;&#19981;&#21463;&#35821;&#35328;&#39034;&#24207;&#30340;&#24433;&#21709;&#65292;&#20294;&#21518;&#21521;&#36716;&#31227;&#21017;&#21487;&#33021;&#26159;&#27491;&#21521;&#30340;&#25110;&#36127;&#21521;&#30340;&#65292;&#20855;&#20307;&#21462;&#20915;&#20110;&#26032;&#35821;&#35328;&#30340;&#39034;&#24207;&#21644;&#29305;&#24449;&#12290;&#20026;&#20102;&#35299;&#37322;&#36825;&#20123;&#27169;&#24335;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20960;&#31181;&#35821;&#35328;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent increase in data and model scale for language model pre-training has led to huge training costs. In scenarios where new data become available over time, updating a model instead of fully retraining it would therefore provide significant gains. In this paper, we study the benefits and downsides of updating a language model when new data comes from new languages - the case of continual learning under language shift. Starting from a monolingual English language model, we incrementally add data from Norwegian and Icelandic to investigate how forward and backward transfer effects depend on the pre-training order and characteristics of languages, for different model sizes and learning rate schedulers. Our results show that, while forward transfer is largely positive and independent of language order, backward transfer can be either positive or negative depending on the order and characteristics of new languages. To explain these patterns we explore several language similarity metr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#22312;&#32454;&#32990;&#22797;&#21512;&#29289;&#19978;&#24212;&#29992;&#39640;&#26031;&#36807;&#31243;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#26680;&#20989;&#25968;&#26469;&#25429;&#25417;&#39640;&#38454;&#32454;&#32990;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2311.01198</link><description>&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#22312;&#32454;&#32990;&#22797;&#21512;&#29289;&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Gaussian Processes on Cellular Complexes. (arXiv:2311.01198v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01198
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#22312;&#32454;&#32990;&#22797;&#21512;&#29289;&#19978;&#24212;&#29992;&#39640;&#26031;&#36807;&#31243;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#26680;&#20989;&#25968;&#26469;&#25429;&#25417;&#39640;&#38454;&#32454;&#32990;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#23545;&#22312;&#22270;&#19978;&#24320;&#21457;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#32771;&#34385;&#25299;&#25169;&#24402;&#32435;&#20559;&#32622;&#20135;&#29983;&#20102;&#30456;&#24403;&#22823;&#30340;&#20852;&#36259;&#12290;&#29305;&#21035;&#26159;&#65292;&#26368;&#36817;&#20851;&#27880;&#30340;&#26159;&#22312;&#36825;&#20123;&#32467;&#26500;&#19978;&#30340;&#39640;&#26031;&#36807;&#31243;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#21516;&#26102;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#12290;&#28982;&#32780;&#65292;&#22270;&#20165;&#38480;&#20110;&#23545;&#20004;&#20010;&#39030;&#28857;&#20043;&#38388;&#30340;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36229;&#36234;&#20102;&#36825;&#31181;&#23545;&#31216;&#37197;&#32622;&#65292;&#24182;&#32771;&#34385;&#20102;&#21253;&#25324;&#39030;&#28857;&#12289;&#36793;&#21644;&#23427;&#20204;&#30340;&#19968;&#31181;&#24191;&#20041;&#21270;&#31216;&#20026;&#32454;&#32990;&#30340;&#20132;&#20114;&#20851;&#31995;&#12290;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39640;&#26031;&#36807;&#31243;&#22312;&#32454;&#32990;&#22797;&#21512;&#29289;&#19978;&#30340;&#24212;&#29992;&#65292;&#36825;&#26159;&#23545;&#22270;&#30340;&#19968;&#31181;&#25512;&#24191;&#65292;&#21487;&#20197;&#25429;&#25417;&#36825;&#20123;&#39640;&#38454;&#32454;&#32990;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#19968;&#20010;&#20851;&#38190;&#36129;&#29486;&#26159;&#25512;&#23548;&#20986;&#20004;&#20010;&#26032;&#22411;&#26680;&#20989;&#25968;&#65292;&#19968;&#20010;&#26159;&#23545;&#22270;Mat\'ern&#26680;&#36827;&#34892;&#25512;&#24191;&#65292;&#21478;&#19968;&#20010;&#26159;&#39069;&#22806;&#22320;&#28151;&#21512;&#20102;&#19981;&#21516;&#32454;&#32990;&#31867;&#22411;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, there has been considerable interest in developing machine learning models on graphs in order to account for topological inductive biases. In particular, recent attention was given to Gaussian processes on such structures since they can additionally account for uncertainty. However, graphs are limited to modelling relations between two vertices. In this paper, we go beyond this dyadic setting and consider polyadic relations that include interactions between vertices, edges and one of their generalisations, known as cells. Specifically, we propose Gaussian processes on cellular complexes, a generalisation of graphs that captures interactions between these higher-order cells. One of our key contributions is the derivation of two novel kernels, one that generalises the graph Mat\'ern kernel and one that additionally mixes information of different cell types.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#21452;&#36793;&#36793;&#32536;&#22122;&#22768;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20449;&#24687;&#35770;&#24341;&#23548;&#30340;&#21407;&#21017;&#65292;&#25552;&#21462;&#21487;&#38752;&#30340;&#30417;&#30563;&#20449;&#21495;&#21644;&#36991;&#20813;&#34920;&#31034;&#23849;&#28291;&#65292;&#20174;&#32780;&#23454;&#29616;&#31283;&#20581;&#30340;&#22270;&#38142;&#25509;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2311.01196</link><description>&lt;p&gt;
&#23545;&#25239;&#21452;&#36793;&#36793;&#32536;&#22122;&#22768;&#20197;&#23454;&#29616;&#31283;&#20581;&#30340;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Combating Bilateral Edge Noise for Robust Link Prediction. (arXiv:2311.01196v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01196
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#21452;&#36793;&#36793;&#32536;&#22122;&#22768;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20449;&#24687;&#35770;&#24341;&#23548;&#30340;&#21407;&#21017;&#65292;&#25552;&#21462;&#21487;&#38752;&#30340;&#30417;&#30563;&#20449;&#21495;&#21644;&#36991;&#20813;&#34920;&#31034;&#23849;&#28291;&#65292;&#20174;&#32780;&#23454;&#29616;&#31283;&#20581;&#30340;&#22270;&#38142;&#25509;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#38543;&#30528;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#21457;&#23637;&#65292;&#22270;&#19978;&#30340;&#38142;&#25509;&#39044;&#27979;&#24050;&#32463;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#20854;&#22312;&#36793;&#32536;&#22122;&#22768;&#19979;&#30340;&#40065;&#26834;&#24615;&#28508;&#21147;&#20173;&#28982;&#36739;&#23569;&#34987;&#30740;&#31350;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#39318;&#20808;&#36827;&#34892;&#20102;&#19968;&#39033;&#32463;&#39564;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#36793;&#32536;&#22122;&#22768;&#21452;&#21521;&#24178;&#25200;&#20102;&#36755;&#20837;&#25299;&#25169;&#21644;&#30446;&#26631;&#26631;&#31614;&#65292;&#23548;&#33268;&#24615;&#33021;&#20005;&#37325;&#19979;&#38477;&#21644;&#34920;&#31034;&#23849;&#28291;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#22256;&#22659;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#35770;&#24341;&#23548;&#30340;&#21407;&#21017;&#65292;&#31283;&#20581;&#30340;&#22270;&#20449;&#24687;&#29942;&#39048;&#65288;RGIB&#65289;&#65292;&#29992;&#20110;&#25552;&#21462;&#21487;&#38752;&#30340;&#30417;&#30563;&#20449;&#21495;&#24182;&#36991;&#20813;&#34920;&#31034;&#23849;&#28291;&#12290;&#19982;&#22522;&#26412;&#30340;&#20449;&#24687;&#29942;&#39048;&#19981;&#21516;&#65292;RGIB&#36827;&#19968;&#27493;&#35299;&#32806;&#21644;&#24179;&#34913;&#20102;&#22270;&#25299;&#25169;&#12289;&#30446;&#26631;&#26631;&#31614;&#21644;&#34920;&#31034;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#65292;&#20026;&#25239;&#21452;&#36793;&#22122;&#22768;&#30340;&#31283;&#20581;&#34920;&#31034;&#26500;&#24314;&#20102;&#26032;&#30340;&#23398;&#20064;&#30446;&#26631;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#20004;&#31181;&#23454;&#20363;&#21270;&#26041;&#27861;&#65292;&#21363;RGIB-SSL&#21644;RGIB-REP&#65292;&#20197;&#21033;&#29992;&#19981;&#21516;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#21363;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#25968;&#25454;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although link prediction on graphs has achieved great success with the development of graph neural networks (GNNs), the potential robustness under the edge noise is still less investigated. To close this gap, we first conduct an empirical study to disclose that the edge noise bilaterally perturbs both input topology and target label, yielding severe performance degradation and representation collapse. To address this dilemma, we propose an information-theory-guided principle, Robust Graph Information Bottleneck (RGIB), to extract reliable supervision signals and avoid representation collapse. Different from the basic information bottleneck, RGIB further decouples and balances the mutual dependence among graph topology, target labels, and representation, building new learning objectives for robust representation against the bilateral noise. Two instantiations, RGIB-SSL and RGIB-REP, are explored to leverage the merits of different methodologies, i.e., self-supervised learning and data r
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#21487;&#22797;&#21046;&#23454;&#39564;&#35774;&#35745;&#30340;&#25209;&#37327;&#36125;&#21494;&#26031;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#36873;&#25321;&#22797;&#21046;&#27425;&#25968;&#26469;&#24179;&#34913;&#35780;&#20272;&#26356;&#22810;&#21807;&#19968;&#26465;&#20214;&#19982;&#22686;&#21152;&#27599;&#20010;&#26465;&#20214;&#30340;&#22797;&#21046;&#27425;&#25968;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#32771;&#34385;&#21040;&#20102;&#20174;&#19994;&#20154;&#21592;&#30340;&#39118;&#38505;&#35268;&#36991;&#20542;&#21521;&#12290;</title><link>http://arxiv.org/abs/2311.01195</link><description>&lt;p&gt;
Batch Bayesian Optimization for Replicable Experimental Design
&lt;/p&gt;
&lt;p&gt;
Batch Bayesian Optimization for Replicable Experimental Design. (arXiv:2311.01195v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01195
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#21487;&#22797;&#21046;&#23454;&#39564;&#35774;&#35745;&#30340;&#25209;&#37327;&#36125;&#21494;&#26031;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#36873;&#25321;&#22797;&#21046;&#27425;&#25968;&#26469;&#24179;&#34913;&#35780;&#20272;&#26356;&#22810;&#21807;&#19968;&#26465;&#20214;&#19982;&#22686;&#21152;&#27599;&#20010;&#26465;&#20214;&#30340;&#22797;&#21046;&#27425;&#25968;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#32771;&#34385;&#21040;&#20102;&#20174;&#19994;&#20154;&#21592;&#30340;&#39118;&#38505;&#35268;&#36991;&#20542;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#23454;&#39564;&#35774;&#35745;&#38382;&#39064;&#22312;&#24182;&#34892;&#35780;&#20272;&#22810;&#20010;&#23454;&#39564;&#26465;&#20214;&#30340;&#21516;&#26102;&#65292;&#30001;&#20110;&#23384;&#22312;&#22823;&#37327;&#21644;&#24322;&#26041;&#24046;&#30340;&#35266;&#27979;&#22122;&#22768;&#65292;&#36824;&#20250;&#23545;&#27599;&#20010;&#26465;&#20214;&#36827;&#34892;&#22810;&#27425;&#22797;&#21046;&#12290;&#22312;&#32473;&#23450;&#22266;&#23450;&#24635;&#39044;&#31639;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#33258;&#28982;&#22320;&#24341;&#21457;&#20102;&#22312;&#35780;&#20272;&#26356;&#22810;&#21807;&#19968;&#26465;&#20214;&#30340;&#21516;&#26102;&#20943;&#23569;&#27599;&#20010;&#26465;&#20214;&#30340;&#22797;&#21046;&#27425;&#25968;&#19982;&#35780;&#20272;&#36739;&#23569;&#30340;&#21807;&#19968;&#26465;&#20214;&#24182;&#22686;&#21152;&#27599;&#20010;&#26465;&#20214;&#30340;&#22797;&#21046;&#27425;&#25968;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#27492;&#22806;&#65292;&#22312;&#36825;&#20123;&#38382;&#39064;&#20013;&#65292;&#20174;&#19994;&#20154;&#21592;&#21487;&#33021;&#20855;&#26377;&#39118;&#38505;&#35268;&#36991;&#20542;&#21521;&#65292;&#22240;&#27492;&#26356;&#21152;&#20559;&#22909;&#26082;&#20855;&#26377;&#33391;&#22909;&#24179;&#22343;&#24615;&#33021;&#21448;&#20855;&#26377;&#36739;&#23567;&#21464;&#24322;&#24615;&#30340;&#36755;&#20837;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#21487;&#22797;&#21046;&#23454;&#39564;&#35774;&#35745;&#30340;&#25209;&#37327; Thompson Sampling (BTS-RED) &#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#19977;&#20010;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340; BTS-RED-Known &#21644; BTS-RED-Unknown &#31639;&#27861;&#20998;&#21035;&#29992;&#20110;&#24050;&#30693;&#21644;&#26410;&#30693;&#22122;&#22768;&#26041;&#24046;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#36873;&#25321;&#22797;&#21046;&#27425;&#25968;&#32780;&#19981;&#26159;&#30830;&#23450;&#24615;&#22320;&#20026;&#20855;&#26377;&#36739;&#22823;&#22122;&#22768;&#26041;&#24046;&#30340;&#36755;&#20837;&#36827;&#34892;&#26356;&#22810;&#27425;&#22797;&#21046;&#12290;&#22240;&#27492;&#65292;&#23613;&#31649;&#36755;&#20837;&#30340;&#22122;&#22768;&#26041;&#24046;&#36739;&#22823;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20173;&#33021;&#23454;&#29616;&#22797;&#21046;&#26356;&#22810;&#27425;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many real-world experimental design problems (a) evaluate multiple experimental conditions in parallel and (b) replicate each condition multiple times due to large and heteroscedastic observation noise. Given a fixed total budget, this naturally induces a trade-off between evaluating more unique conditions while replicating each of them fewer times vs. evaluating fewer unique conditions and replicating each more times. Moreover, in these problems, practitioners may be risk-averse and hence prefer an input with both good average performance and small variability. To tackle both challenges, we propose the Batch Thompson Sampling for Replicable Experimental Design (BTS-RED) framework, which encompasses three algorithms. Our BTS-RED-Known and BTS-RED-Unknown algorithms, for, respectively, known and unknown noise variance, choose the number of replications adaptively rather than deterministically such that an input with a larger noise variance is replicated more times. As a result, despite 
&lt;/p&gt;</description></item><item><title>VIGraph&#26159;&#19968;&#20010;&#22522;&#20110;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#33258;&#32534;&#30721;&#22120;&#29983;&#25104;&#23569;&#25968;&#31867;&#33410;&#28857;&#26469;&#35299;&#20915;&#22270;&#25968;&#25454;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#23402;&#29983;&#23545;&#27604;&#31574;&#30053;&#25552;&#39640;&#29983;&#25104;&#33410;&#28857;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2311.01191</link><description>&lt;p&gt;
VIGraph&#65306;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#29992;&#20110;&#31867;&#21035;&#19981;&#24179;&#34913;&#33410;&#28857;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
VIGraph: Self-supervised Learning for Class-Imbalanced Node Classification. (arXiv:2311.01191v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01191
&lt;/p&gt;
&lt;p&gt;
VIGraph&#26159;&#19968;&#20010;&#22522;&#20110;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#33258;&#32534;&#30721;&#22120;&#29983;&#25104;&#23569;&#25968;&#31867;&#33410;&#28857;&#26469;&#35299;&#20915;&#22270;&#25968;&#25454;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#23402;&#29983;&#23545;&#27604;&#31574;&#30053;&#25552;&#39640;&#29983;&#25104;&#33410;&#28857;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#25968;&#25454;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#20026;&#33410;&#28857;&#20998;&#31867;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#29616;&#26377;&#26041;&#27861;&#65292;&#22914;&#22522;&#20110;SMOTE&#30340;&#26041;&#27861;&#65292;&#22312;&#19981;&#24179;&#34913;&#22330;&#26223;&#26500;&#24314;&#36807;&#31243;&#20013;&#20173;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#36890;&#36807;&#20174;&#25968;&#25454;&#20013;&#21512;&#25104;&#23569;&#25968;&#31867;&#33410;&#28857;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#28982;&#32780;&#20854;&#28508;&#21147;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#22522;&#20110;SMOTE&#30340;&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#24182;&#24341;&#20837;&#20102;VIGraph&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#33258;&#25105;&#30417;&#30563;&#21464;&#20998;&#22270;&#33258;&#32534;&#30721;&#22120;&#65288;VGAE&#65289;&#30340;&#26032;&#22411;SSL&#27169;&#22411;&#65292;&#21033;&#29992;&#21464;&#20998;&#25512;&#26029;&#65288;VI&#65289;&#29983;&#25104;&#23569;&#25968;&#31867;&#33410;&#28857;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;VIGraph&#22312;&#26500;&#24314;&#19981;&#24179;&#34913;&#22270;&#26102;&#20005;&#26684;&#36981;&#24490;&#19981;&#24179;&#34913;&#30340;&#27010;&#24565;&#65292;&#24182;&#21033;&#29992;&#29983;&#25104;&#22411;VGAE&#29983;&#25104;&#23569;&#25968;&#31867;&#33410;&#28857;&#12290;&#27492;&#22806;&#65292;VIGraph&#22312;&#35299;&#30721;&#38454;&#27573;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23402;&#29983;&#23545;&#27604;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#29983;&#25104;&#33410;&#28857;&#30340;&#25972;&#20307;&#36136;&#37327;&#12290;VIGraph&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#33410;&#28857;&#65292;&#26080;&#38656;&#37325;&#26032;&#38598;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Class imbalance in graph data poses significant challenges for node classification. Existing methods, represented by SMOTE-based approaches, partially alleviate this issue but still exhibit limitations during imbalanced scenario construction. Self-supervised learning (SSL) offers a promising solution by synthesizing minority nodes from the data itself, yet its potential remains unexplored. In this paper, we analyze the limitations of SMOTE-based approaches and introduce VIGraph, a novel SSL model based on the self-supervised Variational Graph Auto-Encoder (VGAE) that leverages Variational Inference (VI) to generate minority nodes. Specifically, VIGraph strictly adheres to the concept of imbalance when constructing imbalanced graphs and utilizes the generative VGAE to generate minority nodes. Moreover, VIGraph introduces a novel Siamese contrastive strategy at the decoding phase to improve the overall quality of generated nodes. VIGraph can generate high-quality nodes without reintegrat
&lt;/p&gt;</description></item><item><title>&#25968;&#23383;&#23402;&#29983;&#25216;&#26415;&#19982;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#20043;&#38388;&#30340;&#24378;&#20114;&#21160;&#21487;&#20197;&#25913;&#21892;&#25968;&#23383;&#24179;&#21488;&#30340;&#32593;&#32476;&#23433;&#20840;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#20449;&#24687;&#21644;&#23433;&#20840;&#26631;&#20934;&#65292;&#32593;&#32476;&#29359;&#32618;&#20998;&#23376;&#33021;&#22815;&#21033;&#29992;&#25968;&#23383;&#23402;&#29983;&#25216;&#26415;&#23545;&#25972;&#20010;&#38598;&#21512;&#36896;&#25104;&#23041;&#32961;&#12290;</title><link>http://arxiv.org/abs/2311.01154</link><description>&lt;p&gt;
&#25968;&#23383;&#23402;&#29983;&#19982;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#32593;&#32476;&#23433;&#20840;&#24212;&#29992;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Review of Digital Twins and their Application in Cybersecurity based on Artificial Intelligence. (arXiv:2311.01154v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01154
&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#23402;&#29983;&#25216;&#26415;&#19982;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#20043;&#38388;&#30340;&#24378;&#20114;&#21160;&#21487;&#20197;&#25913;&#21892;&#25968;&#23383;&#24179;&#21488;&#30340;&#32593;&#32476;&#23433;&#20840;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#20449;&#24687;&#21644;&#23433;&#20840;&#26631;&#20934;&#65292;&#32593;&#32476;&#29359;&#32618;&#20998;&#23376;&#33021;&#22815;&#21033;&#29992;&#25968;&#23383;&#23402;&#29983;&#25216;&#26415;&#23545;&#25972;&#20010;&#38598;&#21512;&#36896;&#25104;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#23402;&#29983;&#25216;&#26415;&#30340;&#28508;&#21147;&#23578;&#26410;&#23436;&#20840;&#21457;&#25381;&#65292;&#30001;&#20110;&#20854;&#22810;&#26679;&#24615;&#21644;&#26410;&#24320;&#21457;&#30340;&#28508;&#21147;&#12290;&#25968;&#23383;&#23402;&#29983;&#20351;&#24471;&#31995;&#32479;&#30340;&#20998;&#26512;&#12289;&#35774;&#35745;&#12289;&#20248;&#21270;&#21644;&#28436;&#21270;&#33021;&#22815;&#36890;&#36807;&#25968;&#23383;&#26041;&#24335;&#25110;&#19982;&#21327;&#21516;&#30340;&#32593;&#32476;-&#29289;&#29702;&#26041;&#27861;&#30456;&#32467;&#21512;&#36827;&#34892;&#65292;&#20197;&#25552;&#39640;&#20256;&#32479;&#24037;&#31243;&#26041;&#27861;&#30340;&#36895;&#24230;&#12289;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#24037;&#19994;4.0&#12289;&#26410;&#26469;&#24037;&#21378;&#21644;&#25968;&#23383;&#23402;&#29983;&#25216;&#26415;&#32487;&#32493;&#20174;&#35813;&#25216;&#26415;&#20013;&#21463;&#30410;&#65292;&#24182;&#22312;&#29616;&#26377;&#31995;&#32479;&#20013;&#25552;&#20379;&#20102;&#22686;&#24378;&#30340;&#25928;&#29575;&#12290;&#30001;&#20110;&#32570;&#20047;&#19982;&#32593;&#32476;&#25968;&#23383;&#21270;&#36807;&#28193;&#30456;&#20851;&#30340;&#20449;&#24687;&#21644;&#23433;&#20840;&#26631;&#20934;&#65292;&#32593;&#32476;&#29359;&#32618;&#20998;&#23376;&#33021;&#22815;&#21033;&#29992;&#36825;&#31181;&#24773;&#20917;&#12290;&#35775;&#38382;&#20135;&#21697;&#25110;&#26381;&#21153;&#30340;&#25968;&#23383;&#23402;&#29983;&#31561;&#21516;&#20110;&#23041;&#32961;&#25972;&#20010;&#38598;&#21512;&#12290;&#25968;&#23383;&#23402;&#29983;&#19982;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#20043;&#38388;&#23384;&#22312;&#30528;&#24378;&#26377;&#21147;&#30340;&#20114;&#21160;&#65292;&#36825;&#23548;&#33268;&#36825;&#20123;&#25216;&#26415;&#20043;&#38388;&#23384;&#22312;&#30528;&#32039;&#23494;&#30340;&#20114;&#21160;&#65292;&#22240;&#27492;&#21487;&#20197;&#29992;&#26469;&#25913;&#21892;&#36825;&#20123;&#25968;&#23383;&#24179;&#21488;&#30340;&#32593;&#32476;&#23433;&#20840;&#12290;
&lt;/p&gt;
&lt;p&gt;
The potential of digital twin technology is yet to be fully realized due to its diversity and untapped potential. Digital twins enable systems' analysis, design, optimization, and evolution to be performed digitally or in conjunction with a cyber-physical approach to improve speed, accuracy, and efficiency over traditional engineering methods. Industry 4.0, factories of the future, and digital twins continue to benefit from the technology and provide enhanced efficiency within existing systems. Due to the lack of information and security standards associated with the transition to cyber digitization, cybercriminals have been able to take advantage of the situation. Access to a digital twin of a product or service is equivalent to threatening the entire collection. There is a robust interaction between digital twins and artificial intelligence tools, which leads to strong interaction between these technologies, so it can be used to improve the cybersecurity of these digital platforms ba
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#30340;&#26102;&#38388;&#28857;&#36807;&#31243;&#27169;&#22411;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#22312;&#39044;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#23545;&#20855;&#26377;&#31163;&#25955;&#21644;&#36830;&#32493;&#25104;&#20998;&#30340;&#25968;&#25454;&#20855;&#26377;&#22788;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2311.01139</link><description>&lt;p&gt;
&#28155;&#21152;&#21644;&#31232;&#30095;&#65306;&#19968;&#31181;&#29992;&#20110;&#26102;&#38388;&#28857;&#36807;&#31243;&#30340;&#25193;&#25955;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Add and Thin: Diffusion for Temporal Point Processes. (arXiv:2311.01139v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01139
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#30340;&#26102;&#38388;&#28857;&#36807;&#31243;&#27169;&#22411;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#22312;&#39044;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#23545;&#20855;&#26377;&#31163;&#25955;&#21644;&#36830;&#32493;&#25104;&#20998;&#30340;&#25968;&#25454;&#20855;&#26377;&#22788;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26102;&#38388;&#28857;&#36807;&#31243;&#65288;TPP&#65289;&#26694;&#26550;&#20869;&#65292;&#33258;&#22238;&#24402;&#31070;&#32463;&#32593;&#32476;&#24050;&#25104;&#20026;&#24314;&#27169;&#36830;&#32493;&#26102;&#38388;&#20107;&#20214;&#25968;&#25454;&#30340;&#26631;&#20934;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#20197;&#19968;&#27493;&#39044;&#27979;&#30340;&#26041;&#24335;&#31934;&#30830;&#22320;&#25429;&#25417;&#20107;&#20214;&#24207;&#21015;&#65292;&#20294;&#30001;&#20110;&#20854;&#39034;&#24207;&#24615;&#36136;&#24341;&#36215;&#30340;&#35823;&#24046;&#31215;&#32047;&#65292;&#23427;&#20204;&#22312;&#38271;&#26399;&#39044;&#27979;&#24212;&#29992;&#20013;&#20855;&#26377;&#22266;&#26377;&#30340;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;ADD-THIN&#65292;&#19968;&#31181;&#38754;&#21521;&#25972;&#20010;&#20107;&#20214;&#24207;&#21015;&#24037;&#20316;&#30340;&#22522;&#20110;&#27010;&#29575;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65292;&#23427;&#33258;&#28982;&#22320;&#22788;&#29702;&#20855;&#26377;&#31163;&#25955;&#21644;&#36830;&#32493;&#25104;&#20998;&#30340;&#25968;&#25454;&#12290;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#23494;&#24230;&#20272;&#35745;&#26041;&#38754;&#19982;&#26368;&#20808;&#36827;&#30340;TPP&#27169;&#22411;&#30456;&#21305;&#37197;&#65292;&#24182;&#22312;&#39044;&#27979;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autoregressive neural networks within the temporal point process (TPP) framework have become the standard for modeling continuous-time event data. Even though these models can expressively capture event sequences in a one-step-ahead fashion, they are inherently limited for long-term forecasting applications due to the accumulation of errors caused by their sequential nature. To overcome these limitations, we derive ADD-THIN, a principled probabilistic denoising diffusion model for TPPs that operates on entire event sequences. Unlike existing diffusion approaches, ADD-THIN naturally handles data with discrete and continuous components. In experiments on synthetic and real-world datasets, our model matches the state-of-the-art TPP models in density estimation and strongly outperforms them in forecasting.
&lt;/p&gt;</description></item><item><title>&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30149;&#29702;&#30340;&#27668;&#36947;&#20998;&#21106;&#22522;&#20934;&#25968;&#25454;&#38598;&#65288;AeroPath&#65289;&#65292;&#20197;&#25552;&#39640;&#32954;&#37096;&#30142;&#30149;&#26089;&#26399;&#35786;&#26029;&#21644;&#27835;&#30103;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#34701;&#21512;&#35774;&#35745;&#65292;&#29992;&#20110;&#33258;&#21160;&#27668;&#36947;&#20998;&#21106;&#12290;&#27169;&#22411;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2311.01138</link><description>&lt;p&gt;
AeroPath: &#19968;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30149;&#29702;&#30340;&#27668;&#36947;&#20998;&#21106;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
AeroPath: An airway segmentation benchmark dataset with challenging pathology. (arXiv:2311.01138v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01138
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30149;&#29702;&#30340;&#27668;&#36947;&#20998;&#21106;&#22522;&#20934;&#25968;&#25454;&#38598;&#65288;AeroPath&#65289;&#65292;&#20197;&#25552;&#39640;&#32954;&#37096;&#30142;&#30149;&#26089;&#26399;&#35786;&#26029;&#21644;&#27835;&#30103;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#34701;&#21512;&#35774;&#35745;&#65292;&#29992;&#20110;&#33258;&#21160;&#27668;&#36947;&#20998;&#21106;&#12290;&#27169;&#22411;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25913;&#21892;&#24739;&#26377;&#32954;&#37096;&#30142;&#30149;&#65288;&#22914;&#32954;&#30284;&#65289;&#30340;&#24739;&#32773;&#30340;&#39044;&#21518;&#65292;&#26089;&#26399;&#35786;&#26029;&#21644;&#27835;&#30103;&#33267;&#20851;&#37325;&#35201;&#12290;CT&#22270;&#20687;&#30340;&#20998;&#26512;&#23545;&#20110;&#35786;&#26029;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#23545;&#20110;&#24178;&#39044;&#35745;&#21010;&#21644;&#25903;&#27668;&#31649;&#38236;&#26816;&#26597;&#36807;&#31243;&#20013;&#30340;&#23454;&#26102;&#24341;&#23548;&#65292;&#38656;&#35201;&#23545;&#27668;&#36947;&#26641;&#36827;&#34892;&#39640;&#36136;&#37327;&#30340;&#20998;&#21106;&#12290;&#26368;&#36817;&#65292;&#22810;&#39046;&#22495;&#27668;&#36947;&#26641;&#24314;&#27169;&#65288;ATM'22&#65289;&#25361;&#25112;&#21457;&#24067;&#20102;&#19968;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#26082;&#21487;&#20197;&#35757;&#32451;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#21448;&#26174;&#33879;&#25552;&#39640;&#20102;&#27668;&#36947;&#20998;&#21106;&#20219;&#21153;&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;&#28982;&#32780;&#65292;ATM'22&#25968;&#25454;&#38598;&#20013;&#21482;&#21253;&#25324;&#23569;&#25968;&#24739;&#26377;&#24433;&#21709;&#27668;&#36947;&#26641;&#35299;&#21078;&#32467;&#26500;&#30340;&#20005;&#37325;&#30149;&#29702;&#30340;&#24739;&#32773;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#20844;&#20849;&#22522;&#20934;&#25968;&#25454;&#38598;&#65288;AeroPath&#65289;&#65292;&#21253;&#25324;27&#20010;CT&#22270;&#20687;&#65292;&#26469;&#33258;&#24739;&#26377;&#20174;&#32954;&#27668;&#32959;&#21040;&#22823;&#22411;&#32959;&#30244;&#31561;&#30149;&#29702;&#30340;&#24739;&#32773;&#65292;&#24182;&#38468;&#24102;&#26377;&#23545;&#24212;&#30340;&#27668;&#31649;&#21644;&#25903;&#27668;&#31649;&#27880;&#37322;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#27668;&#36947;&#20998;&#21106;&#30340;&#22810;&#23610;&#24230;&#34701;&#21512;&#35774;&#35745;&#12290;&#27169;&#22411;&#26159;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
To improve the prognosis of patients suffering from pulmonary diseases, such as lung cancer, early diagnosis and treatment are crucial. The analysis of CT images is invaluable for diagnosis, whereas high quality segmentation of the airway tree are required for intervention planning and live guidance during bronchoscopy. Recently, the Multi-domain Airway Tree Modeling (ATM'22) challenge released a large dataset, both enabling training of deep-learning based models and bringing substantial improvement of the state-of-the-art for the airway segmentation task. However, the ATM'22 dataset includes few patients with severe pathologies affecting the airway tree anatomy. In this study, we introduce a new public benchmark dataset (AeroPath), consisting of 27 CT images from patients with pathologies ranging from emphysema to large tumors, with corresponding trachea and bronchi annotations. Second, we present a multiscale fusion design for automatic airway segmentation. Models were trained on the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;IPU&#30828;&#20214;&#21152;&#36895;&#22120;&#30340;&#25968;&#25454;&#29983;&#25104;&#22120;PySCF_IPU&#26469;&#29983;&#25104;&#20855;&#26377;&#21313;&#20159;&#20010;&#35757;&#32451;&#26679;&#26412;&#30340;QM1B&#25968;&#25454;&#38598;&#65292;&#27492;&#25968;&#25454;&#38598;&#21253;&#21547;9-11&#20010;&#37325;&#21407;&#23376;&#12290;&#36890;&#36807;&#23545;&#36825;&#20010;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#22522;&#20934;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#31168;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.01135</link><description>&lt;p&gt;
&#29992;PySCF_IPU&#29983;&#25104;QM1B&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Generating QM1B with PySCF$_{\text{IPU}}$. (arXiv:2311.01135v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01135
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;IPU&#30828;&#20214;&#21152;&#36895;&#22120;&#30340;&#25968;&#25454;&#29983;&#25104;&#22120;PySCF_IPU&#26469;&#29983;&#25104;&#20855;&#26377;&#21313;&#20159;&#20010;&#35757;&#32451;&#26679;&#26412;&#30340;QM1B&#25968;&#25454;&#38598;&#65292;&#27492;&#25968;&#25454;&#38598;&#21253;&#21547;9-11&#20010;&#37325;&#21407;&#23376;&#12290;&#36890;&#36807;&#23545;&#36825;&#20010;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#22522;&#20934;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#31168;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#22522;&#30784;&#27169;&#22411;&#30340;&#20986;&#29616;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#12290;&#36825;&#31181;&#36827;&#23637;&#24471;&#30410;&#20110;&#25317;&#26377;&#25968;&#21313;&#20159;&#20010;&#35757;&#32451;&#26679;&#26412;&#30340;&#25968;&#25454;&#38598;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22312;&#37327;&#23376;&#21270;&#23398;&#39046;&#22495;&#36824;&#26410;&#23454;&#29616;&#31867;&#20284;&#30340;&#22909;&#22788;&#65292;&#22240;&#20026;&#28145;&#24230;&#23398;&#20064;&#30340;&#28508;&#21147;&#21463;&#38480;&#20110;&#20165;&#26377;10&#19975;&#21040;2000&#19975;&#20010;&#35757;&#32451;&#26679;&#26412;&#30340;&#30456;&#23545;&#36739;&#23567;&#25968;&#25454;&#38598;&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#26377;&#38480;&#65292;&#26159;&#22240;&#20026;&#26631;&#31614;&#26159;&#20351;&#29992;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#65288;DFT&#65289;&#30340;&#20934;&#30830;&#65288;&#20294;&#35745;&#31639;&#37327;&#22823;&#65289;&#39044;&#27979;&#35745;&#31639;&#24471;&#20986;&#30340;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20043;&#21069;&#30340;DFT&#25968;&#25454;&#38598;&#26159;&#20351;&#29992;CPU&#36229;&#32423;&#35745;&#31639;&#26426;&#21019;&#24314;&#30340;&#65292;&#27809;&#26377;&#21033;&#29992;&#30828;&#20214;&#21152;&#36895;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21521;&#21033;&#29992;&#30828;&#20214;&#21152;&#36895;&#22120;&#36808;&#20986;&#20102;&#31532;&#19968;&#27493;&#65292;&#24341;&#20837;&#20102;&#20351;&#29992;&#26234;&#33021;&#22788;&#29702;&#21333;&#20803;&#65288;IPU&#65289;&#30340;&#25968;&#25454;&#29983;&#25104;&#22120;PySCF_IPU&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#21019;&#24314;&#21253;&#21547;&#26377;9-11&#20010;&#37325;&#21407;&#23376;&#30340;&#21313;&#20159;&#20010;&#35757;&#32451;&#26679;&#26412;&#30340;&#25968;&#25454;&#38598;QM1B&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#22522;&#20934;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#22312;&#36825;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of foundation models in Computer Vision and Natural Language Processing have resulted in immense progress on downstream tasks. This progress was enabled by datasets with billions of training examples. Similar benefits are yet to be unlocked for quantum chemistry, where the potential of deep learning is constrained by comparatively small datasets with 100k to 20M training examples. These datasets are limited in size because the labels are computed using the accurate (but computationally demanding) predictions of Density Functional Theory (DFT). Notably, prior DFT datasets were created using CPU supercomputers without leveraging hardware acceleration. In this paper, we take a first step towards utilising hardware accelerators by introducing the data generator PySCF$_{\text{IPU}}$ using Intelligence Processing Units (IPUs). This allowed us to create the dataset QM1B with one billion training examples containing 9-11 heavy atoms. We demonstrate that a simple baseline neural n
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35821;&#20041;&#20998;&#21106;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#20998;&#21106;&#21472;&#20889;&#23383;&#31526;&#20013;&#30340;&#20010;&#20307;&#23383;&#27597;&#12290;&#35813;&#26041;&#27861;&#22312;&#26222;&#37324;&#35199;&#20122;&#35834;&#30340;&#12298;&#35821;&#27861;&#25991;&#29486;&#12299;&#36825;&#20010;&#26696;&#20363;&#30740;&#31350;&#20013;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#24182;&#35752;&#35770;&#20102;&#32467;&#21512;&#22810;&#20809;&#35889;&#25104;&#20687;&#30340;&#23616;&#38480;&#24615;&#21644;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2311.01130</link><description>&lt;p&gt;
&#29992;&#20110;&#21472;&#20889;&#23383;&#31526;&#35821;&#20041;&#20998;&#21106;&#30340;&#28145;&#24230;&#23398;&#20064;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
A deep learning experiment for semantic segmentation of overlapping characters in palimpsests. (arXiv:2311.01130v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01130
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35821;&#20041;&#20998;&#21106;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#20998;&#21106;&#21472;&#20889;&#23383;&#31526;&#20013;&#30340;&#20010;&#20307;&#23383;&#27597;&#12290;&#35813;&#26041;&#27861;&#22312;&#26222;&#37324;&#35199;&#20122;&#35834;&#30340;&#12298;&#35821;&#27861;&#25991;&#29486;&#12299;&#36825;&#20010;&#26696;&#20363;&#30740;&#31350;&#20013;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#24182;&#35752;&#35770;&#20102;&#32467;&#21512;&#22810;&#20809;&#35889;&#25104;&#20687;&#30340;&#23616;&#38480;&#24615;&#21644;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Palimpsests&#26159;&#25351;&#21382;&#21490;&#25163;&#31295;&#20013;&#34987;&#31532;&#20108;&#20010;&#20889;&#20316;&#37096;&#20998;&#35206;&#30422;&#30340;&#24050;&#32463;&#25830;&#38500;&#30340;&#20889;&#20316;&#12290;&#36890;&#36807;&#20351;&#29992;&#25104;&#20687;&#25216;&#26415;&#65292;&#22914;&#22810;&#20809;&#35889;&#25104;&#20687;&#65292;&#21487;&#20197;&#35782;&#21035;&#20986;&#32905;&#30524;&#38590;&#20197;&#23519;&#35273;&#30340;&#29305;&#24449;&#65292;&#21253;&#25324;&#35114;&#33394;&#21644;&#25830;&#38500;&#30340;&#22696;&#36857;&#12290;&#24403;&#22788;&#29702;&#37325;&#21472;&#22696;&#36857;&#26102;&#65292;&#21487;&#20197;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#26469;&#35299;&#24320;&#37325;&#21472;&#23383;&#27597;&#30340;&#22797;&#26434;&#33410;&#28857;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35821;&#20041;&#20998;&#21106;&#20316;&#20026;&#19968;&#31181;&#35782;&#21035;&#21644;&#20998;&#21106;&#37325;&#21472;&#23383;&#31526;&#20013;&#30340;&#20010;&#20307;&#23383;&#27597;&#30340;&#26041;&#27861;&#12290;&#35813;&#23454;&#39564;&#34987;&#26500;&#24819;&#20026;&#19968;&#20010;&#27010;&#24565;&#39564;&#35777;&#65292;&#20197;&#26222;&#37324;&#35199;&#20122;&#35834;&#30340;&#12298;&#35821;&#27861;&#25991;&#29486;&#12299;&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#32467;&#21512;&#22810;&#20809;&#35889;&#25104;&#20687;&#30340;&#26041;&#27861;&#30340;&#38480;&#21046;&#21644;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Palimpsests refer to historical manuscripts where erased writings have been partially covered by the superimposition of a second writing. By employing imaging techniques, e.g., multispectral imaging, it becomes possible to identify features that are imperceptible to the naked eye, including faded and erased inks. When dealing with overlapping inks, Artificial Intelligence techniques can be utilized to disentangle complex nodes of overlapping letters. In this work, we propose deep learning-based semantic segmentation as a method for identifying and segmenting individual letters in overlapping characters. The experiment was conceived as a proof of concept, focusing on the palimpsests of the Ars Grammatica by Prisciano as a case study. Furthermore, caveats and prospects of our approach combined with multispectral imaging are also discussed.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#21453;&#24212;&#39044;&#27979;&#22120;&#31995;&#32479;RMechRP&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#21644;&#26426;&#29702;&#36335;&#24452;&#65292;&#21487;&#20197;&#20934;&#30830;&#19988;&#21487;&#35299;&#37322;&#22320;&#39044;&#27979;&#33258;&#30001;&#22522;&#21453;&#24212;&#65292;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2311.01118</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#21270;&#21270;&#23398;&#30340;&#20154;&#24037;&#26234;&#33021;&#65306;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#39044;&#27979;&#33258;&#30001;&#22522;&#26426;&#29702;&#36335;&#24452;
&lt;/p&gt;
&lt;p&gt;
AI for Interpretable Chemistry: Predicting Radical Mechanistic Pathways via Contrastive Learning. (arXiv:2311.01118v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01118
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#21453;&#24212;&#39044;&#27979;&#22120;&#31995;&#32479;RMechRP&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#21644;&#26426;&#29702;&#36335;&#24452;&#65292;&#21487;&#20197;&#20934;&#30830;&#19988;&#21487;&#35299;&#37322;&#22320;&#39044;&#27979;&#33258;&#30001;&#22522;&#21453;&#24212;&#65292;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21453;&#24212;&#39044;&#27979;&#22120;&#32463;&#21382;&#20102;&#26174;&#33879;&#30340;&#26550;&#26500;&#28436;&#21464;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#32654;&#22269;&#19987;&#21033;&#23616;&#21453;&#24212;&#30340;&#20381;&#36182;&#23548;&#33268;&#20102;&#19981;&#21487;&#35299;&#37322;&#30340;&#39044;&#27979;&#21644;&#23545;&#20854;&#20182;&#21270;&#23398;&#39046;&#22495;&#65288;&#22914;&#33258;&#30001;&#22522;&#21644;&#22823;&#27668;&#21270;&#23398;&#65289;&#30340;&#26377;&#38480;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#21453;&#24212;&#39044;&#27979;&#22120;&#31995;&#32479;RMechRP&#65292;&#23427;&#32467;&#21512;&#20102;&#26368;&#21487;&#35299;&#37322;&#30340;&#21270;&#23398;&#21453;&#24212;&#34920;&#31034;&#20043;&#19968;&#30340;&#26426;&#29702;&#36335;&#24452;&#21644;&#23545;&#27604;&#23398;&#20064;&#12290;RMechRP&#19987;&#20026;&#33258;&#30001;&#22522;&#21453;&#24212;&#32780;&#35774;&#35745;&#65292;&#25552;&#20379;&#20102;&#19981;&#21516;&#23618;&#27425;&#30340;&#21270;&#23398;&#21453;&#24212;&#35299;&#37322;&#12290;&#25105;&#20204;&#20351;&#29992;RMechDB&#65292;&#19968;&#20010;&#20844;&#20849;&#30340;&#33258;&#30001;&#22522;&#21453;&#24212;&#25968;&#25454;&#24211;&#65292;&#24320;&#21457;&#21644;&#35757;&#32451;&#20102;&#22810;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#24314;&#31435;&#20102;&#39044;&#27979;&#33258;&#30001;&#22522;&#21453;&#24212;&#30340;&#39318;&#20010;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;RMechRP&#22312;&#25552;&#20379;&#20934;&#30830;&#21644;&#21487;&#35299;&#37322;&#30340;&#33258;&#30001;&#22522;&#21453;&#24212;&#39044;&#27979;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19988;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning-based reaction predictors have undergone significant architectural evolution. However, their reliance on reactions from the US Patent Office results in a lack of interpretable predictions and limited generalization capability to other chemistry domains, such as radical and atmospheric chemistry. To address these challenges, we introduce a new reaction predictor system, RMechRP, that leverages contrastive learning in conjunction with mechanistic pathways, the most interpretable representation of chemical reactions. Specifically designed for radical reactions, RMechRP provides different levels of interpretation of chemical reactions. We develop and train multiple deep-learning models using RMechDB, a public database of radical reactions, to establish the first benchmark for predicting radical reactions. Our results demonstrate the effectiveness of RMechRP in providing accurate and interpretable predictions of radical reactions, and its potential for various applications in 
&lt;/p&gt;</description></item><item><title>H-NeXt&#26159;&#19968;&#20010;&#21442;&#25968;&#39640;&#25928;&#30340;&#26059;&#36716;&#24179;&#31227;&#19981;&#21464;&#32593;&#32476;&#65292;&#36890;&#36807;&#31561;&#21464;&#21644;&#19981;&#21464;&#30340;&#32452;&#20214;&#23454;&#29616;&#40065;&#26834;&#24615;&#21644;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2311.01111</link><description>&lt;p&gt;
H-NeXt&#65306;&#36808;&#21521;&#26059;&#36716;&#24179;&#31227;&#19981;&#21464;&#32593;&#32476;&#30340;&#19979;&#19968;&#27493;
&lt;/p&gt;
&lt;p&gt;
H-NeXt: The next step towards roto-translation invariant networks. (arXiv:2311.01111v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01111
&lt;/p&gt;
&lt;p&gt;
H-NeXt&#26159;&#19968;&#20010;&#21442;&#25968;&#39640;&#25928;&#30340;&#26059;&#36716;&#24179;&#31227;&#19981;&#21464;&#32593;&#32476;&#65292;&#36890;&#36807;&#31561;&#21464;&#21644;&#19981;&#21464;&#30340;&#32452;&#20214;&#23454;&#29616;&#40065;&#26834;&#24615;&#21644;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31561;&#21464;&#32593;&#32476;&#30340;&#24191;&#27867;&#27969;&#34892;&#20984;&#26174;&#20102;&#21442;&#25968;&#39640;&#25928;&#27169;&#22411;&#21644;&#26377;&#25928;&#21033;&#29992;&#35757;&#32451;&#25968;&#25454;&#30340;&#37325;&#35201;&#24615;&#12290;&#22312;&#23545;&#26410;&#30693;&#21464;&#24418;&#30340;&#40065;&#26834;&#24615;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#26102;&#20505;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;H-NeXt&#26694;&#26550;&#65292;&#22635;&#34917;&#20102;&#31561;&#21464;&#24615;&#21644;&#19981;&#21464;&#24615;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;H-NeXt&#26159;&#19968;&#20010;&#21442;&#25968;&#39640;&#25928;&#30340;&#26059;&#36716;&#24179;&#31227;&#19981;&#21464;&#32593;&#32476;&#65292;&#35757;&#32451;&#38598;&#20013;&#27809;&#26377;&#20219;&#20309;&#22686;&#24378;&#22270;&#20687;&#12290;&#25105;&#20204;&#30340;&#32593;&#32476;&#21253;&#25324;&#19977;&#20010;&#32452;&#20214;&#65306;&#19968;&#20010;&#31561;&#21464;&#30340;&#20027;&#24178;&#32593;&#32476;&#29992;&#20110;&#23398;&#20064;&#26059;&#36716;&#24179;&#31227;&#19981;&#30456;&#20851;&#30340;&#29305;&#24449;&#65292;&#19968;&#20010;&#19981;&#21464;&#30340;&#27719;&#32858;&#23618;&#29992;&#20110;&#20002;&#24323;&#26059;&#36716;&#24179;&#31227;&#20449;&#24687;&#65292;&#21644;&#19968;&#20010;&#20998;&#31867;&#23618;&#12290;H-NeXt&#22312;MNIST&#21644;CIFAR-10&#30340;&#26080;&#22686;&#24378;&#35757;&#32451;&#38598;&#21644;&#22686;&#24378;&#27979;&#35797;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
The widespread popularity of equivariant networks underscores the significance of parameter efficient models and effective use of training data. At a time when robustness to unseen deformations is becoming increasingly important, we present H-NeXt, which bridges the gap between equivariance and invariance. H-NeXt is a parameter-efficient roto-translation invariant network that is trained without a single augmented image in the training set. Our network comprises three components: an equivariant backbone for learning roto-translation independent features, an invariant pooling layer for discarding roto-translation information, and a classification layer. H-NeXt outperforms the state of the art in classification on unaugmented training sets and augmented test sets of MNIST and CIFAR-10.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36777;&#25252;&#20102;&#37319;&#29992;Softmax&#21442;&#25968;&#21270;&#30340;&#23398;&#20064;&#25512;&#36831;&#26041;&#27861;&#65292;&#35299;&#37322;&#20102;&#20808;&#21069;&#20851;&#20110;&#20854;&#20272;&#35745;&#22120;&#30340;&#35823;&#26657;&#20934;&#21644;&#26080;&#30028;&#20272;&#35745;&#30340;&#21407;&#22240;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32479;&#35745;&#19968;&#33268;&#24615;&#38750;&#23545;&#31216;Softmax&#20272;&#35745;&#22120;&#12290;</title><link>http://arxiv.org/abs/2311.01106</link><description>&lt;p&gt;
&#23545;&#20110;&#26657;&#20934;&#21644;&#19968;&#33268;&#23398;&#20064;&#25512;&#36831;&#65292;&#37319;&#29992;Softmax&#21442;&#25968;&#21270;&#30340;&#36777;&#25252;
&lt;/p&gt;
&lt;p&gt;
In Defense of Softmax Parametrization for Calibrated and Consistent Learning to Defer. (arXiv:2311.01106v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01106
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36777;&#25252;&#20102;&#37319;&#29992;Softmax&#21442;&#25968;&#21270;&#30340;&#23398;&#20064;&#25512;&#36831;&#26041;&#27861;&#65292;&#35299;&#37322;&#20102;&#20808;&#21069;&#20851;&#20110;&#20854;&#20272;&#35745;&#22120;&#30340;&#35823;&#26657;&#20934;&#21644;&#26080;&#30028;&#20272;&#35745;&#30340;&#21407;&#22240;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32479;&#35745;&#19968;&#33268;&#24615;&#38750;&#23545;&#31216;Softmax&#20272;&#35745;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#19979;&#28216;&#19987;&#23478;&#26356;&#20934;&#30830;&#26102;&#65292;&#20351;&#24471;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#33021;&#25512;&#36831;&#20854;&#20915;&#31574;&#23558;&#30830;&#20445;&#25552;&#39640;&#23433;&#20840;&#24615;&#21644;&#24615;&#33021;&#12290;&#36825;&#20010;&#30446;&#26631;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#25512;&#36831;&#26694;&#26550;&#26469;&#23454;&#29616;&#65292;&#35813;&#26694;&#26550;&#26088;&#22312;&#20849;&#21516;&#23398;&#20064;&#22914;&#20309;&#20998;&#31867;&#21644;&#22914;&#20309;&#25512;&#36831;&#32473;&#19987;&#23478;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#29702;&#35770;&#19978;&#35777;&#26126;&#65292;&#29992;Softmax&#21442;&#25968;&#21270;&#30340;&#23398;&#20064;&#25512;&#36831;&#30340;&#27969;&#34892;&#20272;&#35745;&#22120;&#25552;&#20379;&#20102;&#26080;&#30028;&#20272;&#35745;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#26080;&#27861;&#26657;&#20934;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#36825;&#26159;&#21542;&#26159;&#30001;&#20110;&#24191;&#27867;&#20351;&#29992;&#30340;Softmax&#21442;&#25968;&#21270;&#65292;&#20197;&#21450;&#25105;&#20204;&#26159;&#21542;&#21487;&#20197;&#25214;&#21040;&#19968;&#20010;&#26082;&#20855;&#26377;&#32479;&#35745;&#19968;&#33268;&#24615;&#21448;&#20855;&#26377;&#26377;&#25928;&#27010;&#29575;&#20272;&#35745;&#22120;&#30340;&#22522;&#20110;Softmax&#30340;&#20272;&#35745;&#22120;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#20808;&#21069;&#25991;&#29486;&#20013;&#35823;&#26657;&#20934;&#21644;&#26080;&#30028;&#20272;&#35745;&#22120;&#30340;&#21407;&#22240;&#26159;&#30001;&#20110;&#25152;&#20351;&#29992;&#30340;&#20195;&#26367;&#25439;&#22833;&#30340;&#23545;&#31216;&#24615;&#36136;&#65292;&#32780;&#19981;&#26159;&#30001;&#20110;Softmax&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32479;&#35745;&#19968;&#33268;&#24615;&#38750;&#23545;&#31216;Softmax&#20272;&#35745;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Enabling machine learning classifiers to defer their decision to a downstream expert when the expert is more accurate will ensure improved safety and performance. This objective can be achieved with the learning-to-defer framework which aims to jointly learn how to classify and how to defer to the expert. In recent studies, it has been theoretically shown that popular estimators for learning to defer parameterized with softmax provide unbounded estimates for the likelihood of deferring which makes them uncalibrated. However, it remains unknown whether this is due to the widely used softmax parameterization and if we can find a softmax-based estimator that is both statistically consistent and possesses a valid probability estimator. In this work, we first show that the cause of the miscalibrated and unbounded estimator in prior literature is due to the symmetric nature of the surrogate losses used and not due to softmax. We then propose a novel statistically consistent asymmetric softma
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23545;&#27604;&#27169;&#22359;&#19982;&#26102;&#38388;&#27880;&#24847;&#21147;(CMTA)&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#20351;&#27169;&#22359;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#20197;&#26356;&#32454;&#31890;&#24230;&#30340;&#26041;&#24335;&#22312;&#20219;&#21153;&#32423;&#21035;&#20043;&#19979;&#20351;&#29992;&#20849;&#20139;&#27169;&#22359;&#21644;&#26102;&#38388;&#27880;&#24847;&#21147;&#26469;&#35299;&#20915;&#22810;&#20219;&#21153;&#39046;&#22495;&#20013;&#29616;&#26377;&#26041;&#27861;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2311.01075</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#20013;&#20855;&#26377;&#26102;&#38388;&#27880;&#24847;&#21147;&#30340;&#23545;&#27604;&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
Contrastive Modules with Temporal Attention for Multi-Task Reinforcement Learning. (arXiv:2311.01075v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23545;&#27604;&#27169;&#22359;&#19982;&#26102;&#38388;&#27880;&#24847;&#21147;(CMTA)&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#20351;&#27169;&#22359;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#20197;&#26356;&#32454;&#31890;&#24230;&#30340;&#26041;&#24335;&#22312;&#20219;&#21153;&#32423;&#21035;&#20043;&#19979;&#20351;&#29992;&#20849;&#20139;&#27169;&#22359;&#21644;&#26102;&#38388;&#27880;&#24847;&#21147;&#26469;&#35299;&#20915;&#22810;&#20219;&#21153;&#39046;&#22495;&#20013;&#29616;&#26377;&#26041;&#27861;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#65292;&#27169;&#22359;&#21270;&#21407;&#21017;&#34987;&#24191;&#27867;&#37319;&#29992;&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21151;&#33021;&#19987;&#38376;&#21270;&#21040;&#19981;&#21516;&#30340;&#27169;&#22359;&#20013;&#24182;&#36866;&#24403;&#22320;&#32452;&#21512;&#23427;&#20204;&#65292;&#20197;&#38450;&#27490;&#30001;&#20110;&#20219;&#21153;&#38388;&#20914;&#31361;&#32780;&#23548;&#33268;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22810;&#20219;&#21153;RL&#26041;&#27861;&#21482;&#22312;&#20219;&#21153;&#32423;&#21035;&#19978;&#32452;&#21512;&#20849;&#20139;&#27169;&#22359;&#65292;&#24573;&#35270;&#20102;&#20219;&#21153;&#20869;&#21487;&#33021;&#23384;&#22312;&#30340;&#20914;&#31361;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#27809;&#26377;&#32771;&#34385;&#21040;&#65292;&#22914;&#26524;&#27809;&#26377;&#32422;&#26463;&#65292;&#19968;&#20123;&#27169;&#22359;&#21487;&#33021;&#20250;&#23398;&#20064;&#30456;&#20284;&#30340;&#21151;&#33021;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#27169;&#22359;&#21270;&#26041;&#27861;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#25512;&#24191;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20855;&#26377;&#26102;&#38388;&#27880;&#24847;&#21147;&#30340;&#23545;&#27604;&#27169;&#22359;&#65288;CMTA&#65289;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of multi-task reinforcement learning, the modular principle, which involves specializing functionalities into different modules and combining them appropriately, has been widely adopted as a promising approach to prevent the negative transfer problem that performance degradation due to conflicts between tasks. However, most of the existing multi-task RL methods only combine shared modules at the task level, ignoring that there may be conflicts within the task. In addition, these methods do not take into account that without constraints, some modules may learn similar functions, resulting in restricting the model's expressiveness and generalization capability of modular methods. In this paper, we propose the Contrastive Modules with Temporal Attention(CMTA) method to address these limitations. CMTA constrains the modules to be different from each other by contrastive learning and combining shared modules at a finer granularity than the task level with temporal attention, al
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;&#26679;&#26412;&#29289;&#31181;&#20998;&#31867;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#27169;&#24335;&#22522;&#30784;&#27169;&#22411;&#65292;&#21033;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30456;&#26426;&#38519;&#38449;&#22270;&#20687;&#30340;&#35270;&#35273;&#25551;&#36848;&#65292;&#24182;&#36890;&#36807;&#21305;&#37197;&#22806;&#37096;&#30693;&#35782;&#24211;&#20013;&#30340;&#25551;&#36848;&#26469;&#30830;&#23450;&#38646;&#26679;&#26412;&#26465;&#20214;&#19979;&#30340;&#29289;&#31181;&#12290;</title><link>http://arxiv.org/abs/2311.01064</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#26465;&#20214;&#19979;&#22522;&#20110;&#22810;&#27169;&#24335;&#22522;&#30784;&#27169;&#22411;&#30340;&#30456;&#26426;&#38519;&#38449;&#22270;&#20687;&#21160;&#29289;&#29289;&#31181;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Multimodal Foundation Models for Zero-shot Animal Species Recognition in Camera Trap Images. (arXiv:2311.01064v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01064
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;&#26679;&#26412;&#29289;&#31181;&#20998;&#31867;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#27169;&#24335;&#22522;&#30784;&#27169;&#22411;&#65292;&#21033;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30456;&#26426;&#38519;&#38449;&#22270;&#20687;&#30340;&#35270;&#35273;&#25551;&#36848;&#65292;&#24182;&#36890;&#36807;&#21305;&#37197;&#22806;&#37096;&#30693;&#35782;&#24211;&#20013;&#30340;&#25551;&#36848;&#26469;&#30830;&#23450;&#38646;&#26679;&#26412;&#26465;&#20214;&#19979;&#30340;&#29289;&#31181;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#29615;&#22659;&#24694;&#21270;&#21644;&#20154;&#31867;&#27963;&#21160;&#22686;&#21152;&#65292;&#37326;&#29983;&#21160;&#29289;&#20445;&#25252;&#24037;&#20316;&#33267;&#20851;&#37325;&#35201;&#12290;&#36816;&#21160;&#35302;&#21457;&#30340;&#30456;&#26426;&#38519;&#38449;&#26159;&#20840;&#29699;&#36861;&#36394;&#21644;&#30417;&#27979;&#37326;&#29983;&#21160;&#29289;&#25968;&#37327;&#30340;&#26377;&#25928;&#24037;&#20855;&#12290;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#20998;&#26512;&#36825;&#31867;&#22270;&#20687;&#65292;&#20294;&#26159;&#35757;&#32451;&#36825;&#20123;&#25216;&#26415;&#38656;&#35201;&#26469;&#33258;&#19987;&#23478;&#30340;&#27880;&#37322;&#12290;&#20943;&#23569;&#23545;&#26114;&#36149;&#26631;&#35760;&#25968;&#25454;&#30340;&#20381;&#36182;&#65292;&#22240;&#27492;&#22312;&#24320;&#21457;&#20855;&#26377;&#26126;&#26174;&#36739;&#23569;&#20154;&#21147;&#21171;&#21160;&#30340;&#22823;&#35268;&#27169;&#37326;&#29983;&#21160;&#29289;&#36861;&#36394;&#35299;&#20915;&#26041;&#26696;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;WildMatch&#65292;&#19968;&#20010;&#21033;&#29992;&#22810;&#27169;&#24335;&#22522;&#30784;&#27169;&#22411;&#30340;&#26032;&#22411;&#38646;&#26679;&#26412;&#29289;&#31181;&#20998;&#31867;&#26694;&#26550;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#36890;&#36807;&#25351;&#23548;&#35843;&#25972;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#29992;&#31867;&#20284;&#19987;&#23478;&#30340;&#26415;&#35821;&#29983;&#25104;&#30456;&#26426;&#38519;&#38449;&#22270;&#20687;&#30340;&#35814;&#32454;&#35270;&#35273;&#25551;&#36848;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#29983;&#25104;&#30340;&#25551;&#36848;&#19982;&#22806;&#37096;&#30693;&#35782;&#24211;&#20013;&#30340;&#25551;&#36848;&#36827;&#34892;&#21305;&#37197;&#65292;&#20197;&#30830;&#23450;&#38646;&#26679;&#26412;&#26465;&#20214;&#19979;&#30340;&#29289;&#31181;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to deteriorating environmental conditions and increasing human activity, conservation efforts directed towards wildlife is crucial. Motion-activated camera traps constitute an efficient tool for tracking and monitoring wildlife populations across the globe. Supervised learning techniques have been successfully deployed to analyze such imagery, however training such techniques requires annotations from experts. Reducing the reliance on costly labelled data therefore has immense potential in developing large-scale wildlife tracking solutions with markedly less human labor. In this work we propose WildMatch, a novel zero-shot species classification framework that leverages multimodal foundation models. In particular, we instruction tune vision-language models to generate detailed visual descriptions of camera trap images using similar terminology to experts. Then, we match the generated caption to an external knowledge base of descriptions in order to determine the species in a zero-s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31070;&#32463;&#20449;&#21495;&#35299;&#30721;&#26041;&#27861;&#65292;&#21033;&#29992;LSTM&#32593;&#32476;&#23558;&#21547;&#26377;&#31070;&#32463;&#25968;&#25454;&#30340;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#20998;&#31867;&#65292;&#20197;&#23454;&#29616;&#29289;&#20307;&#25235;&#21462;&#31867;&#22411;&#30340;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#22312;&#19981;&#20381;&#36182;&#20808;&#21069;&#30340;&#31070;&#32463;&#31185;&#23398;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#25552;&#21462;&#25968;&#25454;&#30456;&#20851;&#24615;&#26469;&#25552;&#39640;&#35299;&#30721;&#20934;&#30830;&#24615;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#20998;&#31867;&#20934;&#30830;&#24230;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2311.01061</link><description>&lt;p&gt;
&#23454;&#26102;&#31070;&#32463;&#35299;&#30721;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deep Learning for real-time neural decoding of grasp. (arXiv:2311.01061v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01061
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31070;&#32463;&#20449;&#21495;&#35299;&#30721;&#26041;&#27861;&#65292;&#21033;&#29992;LSTM&#32593;&#32476;&#23558;&#21547;&#26377;&#31070;&#32463;&#25968;&#25454;&#30340;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#20998;&#31867;&#65292;&#20197;&#23454;&#29616;&#29289;&#20307;&#25235;&#21462;&#31867;&#22411;&#30340;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#22312;&#19981;&#20381;&#36182;&#20808;&#21069;&#30340;&#31070;&#32463;&#31185;&#23398;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#25552;&#21462;&#25968;&#25454;&#30456;&#20851;&#24615;&#26469;&#25552;&#39640;&#35299;&#30721;&#20934;&#30830;&#24615;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#20998;&#31867;&#20934;&#30830;&#24230;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#35299;&#30721;&#26159;&#23558;&#20174;&#22823;&#33041;&#33719;&#21462;&#30340;&#20449;&#21495;&#19982;&#32930;&#20307;&#36816;&#21160;&#25110;&#26426;&#22120;&#20154;&#25511;&#21046;&#31561;&#29289;&#29702;&#19990;&#30028;&#21464;&#37327;&#30456;&#20114;&#20851;&#32852;&#30340;&#19968;&#31181;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#33041;&#26426;&#25509;&#21475;&#12290;&#26412;&#25991;&#22522;&#20110;&#19968;&#32452;&#26469;&#33258;&#29492;&#23376;&#36816;&#21160;&#30382;&#23618;&#30340;&#31070;&#32463;&#35760;&#24405;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31070;&#32463;&#20449;&#21495;&#35299;&#30721;&#26041;&#27861;&#65292;&#29992;&#20110;&#25235;&#21462;&#31867;&#22411;&#30340;&#20998;&#31867;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;LSTM&#32593;&#32476;&#23558;&#21547;&#26377;&#31070;&#32463;&#25968;&#25454;&#65288;&#21363;&#23574;&#23792;&#30005;&#20301;&#65289;&#30340;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#20197;&#34920;&#31034;&#34987;&#25235;&#21462;&#30340;&#29289;&#20307;&#31867;&#21035;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#22312;&#19981;&#20381;&#36182;&#20219;&#20309;&#20808;&#21069;&#30340;&#31070;&#32463;&#31185;&#23398;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#20165;&#20381;&#38752;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20174;&#25968;&#25454;&#20013;&#25552;&#21462;&#30456;&#20851;&#24615;&#26469;&#25552;&#39640;&#35299;&#30721;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#25152;&#32771;&#34385;&#30340;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#30340;&#32467;&#26524;&#65292;&#24182;&#23558;&#20854;&#19982;&#20808;&#21069;&#22312;&#30456;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#24037;&#20316;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#26174;&#31034;&#20102;&#20998;&#31867;&#20934;&#30830;&#24230;&#30340;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural decoding involves correlating signals acquired from the brain to variables in the physical world like limb movement or robot control in Brain Machine Interfaces. In this context, this work starts from a specific pre-existing dataset of neural recordings from monkey motor cortex and presents a Deep Learning-based approach to the decoding of neural signals for grasp type classification. Specifically, we propose here an approach that exploits LSTM networks to classify time series containing neural data (i.e., spike trains) into classes representing the object being grasped. The main goal of the presented approach is to improve over state-of-the-art decoding accuracy without relying on any prior neuroscience knowledge, and leveraging only the capability of deep learning models to extract correlations from data. The paper presents the results achieved for the considered dataset and compares them with previous works on the same dataset, showing a significant improvement in classificat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ROAM&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20808;&#21069;&#23398;&#20064;&#21040;&#30340;&#34892;&#20026;&#26469;&#23454;&#26102;&#35843;&#33410;&#26426;&#22120;&#20154;&#22312;&#37096;&#32626;&#36807;&#31243;&#20013;&#24212;&#23545;&#26410;&#26366;&#35265;&#36807;&#30340;&#24773;&#20917;&#12290;&#22312;&#27979;&#35797;&#20013;&#65292;ROAM&#21487;&#20197;&#22312;&#21333;&#20010;&#38454;&#27573;&#20869;&#23454;&#29616;&#24555;&#36895;&#36866;&#24212;&#65292;&#24182;&#19988;&#22312;&#27169;&#25311;&#29615;&#22659;&#21644;&#30495;&#23454;&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#25928;&#29575;&#21644;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01059</link><description>&lt;p&gt;
&#22312;&#37096;&#32626;&#26102;&#36827;&#34892;&#23454;&#26102;&#35843;&#33410;&#65306;&#29992;&#20110;&#21333;&#26426;&#22120;&#20154;&#37096;&#32626;&#30340;&#34892;&#20026;&#35843;&#25511;
&lt;/p&gt;
&lt;p&gt;
Adapt On-the-Go: Behavior Modulation for Single-Life Robot Deployment. (arXiv:2311.01059v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01059
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ROAM&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20808;&#21069;&#23398;&#20064;&#21040;&#30340;&#34892;&#20026;&#26469;&#23454;&#26102;&#35843;&#33410;&#26426;&#22120;&#20154;&#22312;&#37096;&#32626;&#36807;&#31243;&#20013;&#24212;&#23545;&#26410;&#26366;&#35265;&#36807;&#30340;&#24773;&#20917;&#12290;&#22312;&#27979;&#35797;&#20013;&#65292;ROAM&#21487;&#20197;&#22312;&#21333;&#20010;&#38454;&#27573;&#20869;&#23454;&#29616;&#24555;&#36895;&#36866;&#24212;&#65292;&#24182;&#19988;&#22312;&#27169;&#25311;&#29615;&#22659;&#21644;&#30495;&#23454;&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#25928;&#29575;&#21644;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#21462;&#24471;&#25104;&#21151;&#65292;&#26426;&#22120;&#20154;&#24517;&#39035;&#24212;&#23545;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#26366;&#35265;&#36807;&#30340;&#24773;&#20917;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#37096;&#32626;&#36807;&#31243;&#20013;&#38024;&#23545;&#36825;&#20123;&#26032;&#22330;&#26223;&#30340;&#23454;&#26102;&#35843;&#33410;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#20808;&#21069;&#23398;&#20064;&#21040;&#30340;&#22810;&#26679;&#21270;&#34892;&#20026;&#24211;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;RObust Autonomous Modulation&#65288;ROAM&#65289;&#65292;&#24341;&#20837;&#20102;&#22522;&#20110;&#39044;&#35757;&#32451;&#34892;&#20026;&#30340;&#24863;&#30693;&#20215;&#20540;&#30340;&#26426;&#21046;&#65292;&#20197;&#22312;&#29305;&#23450;&#24773;&#20917;&#19979;&#36873;&#25321;&#21644;&#35843;&#25972;&#39044;&#35757;&#32451;&#34892;&#20026;&#12290;&#20851;&#38190;&#26159;&#65292;&#36825;&#31181;&#35843;&#33410;&#36807;&#31243;&#22312;&#27979;&#35797;&#26102;&#30340;&#21333;&#20010;&#38454;&#27573;&#20869;&#23436;&#25104;&#65292;&#26080;&#38656;&#20219;&#20309;&#20154;&#31867;&#30417;&#30563;&#12290;&#25105;&#20204;&#23545;&#36873;&#25321;&#26426;&#21046;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#35777;&#26126;&#20102;ROAM&#20351;&#24471;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#27169;&#25311;&#29615;&#22659;&#21644;&#30495;&#23454;&#30340;&#22235;&#36275;&#21160;&#29289;Go1&#19978;&#24555;&#36895;&#36866;&#24212;&#21160;&#24577;&#21464;&#21270;&#65292;&#29978;&#33267;&#22312;&#33050;&#19978;&#22871;&#30528;&#28378;&#36718;&#28369;&#38795;&#30340;&#24773;&#20917;&#19979;&#25104;&#21151;&#21069;&#36827;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#38754;&#23545;&#21508;&#31181;&#20998;&#24067;&#24773;&#20917;&#30340;&#37096;&#32626;&#26102;&#33021;&#22815;&#20197;&#36229;&#36807;2&#20493;&#30340;&#25928;&#29575;&#36827;&#34892;&#35843;&#33410;&#65292;&#36890;&#36807;&#26377;&#25928;&#36873;&#25321;&#26469;&#23454;&#29616;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
To succeed in the real world, robots must cope with situations that differ from those seen during training. We study the problem of adapting on-the-fly to such novel scenarios during deployment, by drawing upon a diverse repertoire of previously learned behaviors. Our approach, RObust Autonomous Modulation (ROAM), introduces a mechanism based on the perceived value of pre-trained behaviors to select and adapt pre-trained behaviors to the situation at hand. Crucially, this adaptation process all happens within a single episode at test time, without any human supervision. We provide theoretical analysis of our selection mechanism and demonstrate that ROAM enables a robot to adapt rapidly to changes in dynamics both in simulation and on a real Go1 quadruped, even successfully moving forward with roller skates on its feet. Our approach adapts over 2x as efficiently compared to existing methods when facing a variety of out-of-distribution situations during deployment by effectively choosing
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#38887;&#24615;&#22810;&#36873;&#23398;&#20064;&#65288;rMCL&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;Voronoi tessellations&#30340;&#25968;&#23398;&#26694;&#26550;&#21644;&#23398;&#20064;&#35780;&#20998;&#26041;&#26696;&#65292;&#22312;&#22238;&#24402;&#35774;&#32622;&#20013;&#23454;&#29616;&#20102;&#23545;&#20110;&#27599;&#20010;&#35757;&#32451;&#36755;&#20837;&#21487;&#33021;&#37319;&#26679;&#22810;&#20010;&#30446;&#26631;&#30340;&#26465;&#20214;&#20998;&#24067;&#20272;&#35745;&#12290;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#22768;&#28304;&#23450;&#20301;&#38382;&#39064;&#19978;&#24471;&#21040;&#20102;&#23454;&#35777;&#39564;&#35777;&#21644;&#36827;&#19968;&#27493;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#20854;&#23454;&#38469;&#30340;&#26377;&#29992;&#24615;&#21644;&#35299;&#37322;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01052</link><description>&lt;p&gt;
&#38887;&#24615;&#22810;&#36873;&#23398;&#20064;&#65306;&#29992;&#20110;&#38899;&#39057;&#22330;&#26223;&#20998;&#26512;&#30340;&#23398;&#20064;&#35780;&#20998;&#26041;&#26696;&#30340;&#24341;&#20837;
&lt;/p&gt;
&lt;p&gt;
Resilient Multiple Choice Learning: A learned scoring scheme with application to audio scene analysis. (arXiv:2311.01052v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01052
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#38887;&#24615;&#22810;&#36873;&#23398;&#20064;&#65288;rMCL&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;Voronoi tessellations&#30340;&#25968;&#23398;&#26694;&#26550;&#21644;&#23398;&#20064;&#35780;&#20998;&#26041;&#26696;&#65292;&#22312;&#22238;&#24402;&#35774;&#32622;&#20013;&#23454;&#29616;&#20102;&#23545;&#20110;&#27599;&#20010;&#35757;&#32451;&#36755;&#20837;&#21487;&#33021;&#37319;&#26679;&#22810;&#20010;&#30446;&#26631;&#30340;&#26465;&#20214;&#20998;&#24067;&#20272;&#35745;&#12290;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#22768;&#28304;&#23450;&#20301;&#38382;&#39064;&#19978;&#24471;&#21040;&#20102;&#23454;&#35777;&#39564;&#35777;&#21644;&#36827;&#19968;&#27493;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#20854;&#23454;&#38469;&#30340;&#26377;&#29992;&#24615;&#21644;&#35299;&#37322;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#38887;&#24615;&#22810;&#36873;&#23398;&#20064;&#65288;rMCL&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#23545;&#20110;&#27599;&#20010;&#35757;&#32451;&#36755;&#20837;&#21487;&#33021;&#37319;&#26679;&#22810;&#20010;&#30446;&#26631;&#30340;&#22238;&#24402;&#35774;&#32622;&#19979;&#26465;&#20214;&#20998;&#24067;&#20272;&#35745;&#30340;MCL&#26041;&#27861;&#30340;&#25193;&#23637;&#12290;&#22810;&#36873;&#23398;&#20064;&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#22810;&#27169;&#24577;&#23494;&#24230;&#20272;&#35745;&#65292;&#20351;&#29992;&#20102;&#19968;&#32452;&#20551;&#35774;&#30340;&#32988;&#32773;&#20840;&#25343;&#65288;WTA&#65289;&#25439;&#22833;&#12290;&#22312;&#22238;&#24402;&#35774;&#32622;&#20013;&#65292;&#29616;&#26377;&#30340;MCL&#21464;&#20307;&#20027;&#35201;&#38598;&#20013;&#22312;&#21512;&#24182;&#20551;&#35774;&#19978;&#65292;&#20174;&#32780;&#26368;&#32456;&#29306;&#29298;&#20102;&#39044;&#27979;&#30340;&#22810;&#26679;&#24615;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#19968;&#20010;&#22522;&#20110;Voronoi tessellations&#30340;&#36755;&#20986;&#31354;&#38388;&#30340;&#25968;&#23398;&#26694;&#26550;&#25903;&#25345;&#30340;&#26032;&#39062;&#30340;&#23398;&#20064;&#35780;&#20998;&#26041;&#26696;&#65292;&#25105;&#20204;&#21487;&#20197;&#20174;&#20013;&#24471;&#20986;&#27010;&#29575;&#35299;&#37322;&#12290;&#22312;&#23545;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#23454;&#35777;&#39564;&#35777;&#21518;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35780;&#20272;&#20102;rMCL&#22312;&#22768;&#28304;&#23450;&#20301;&#38382;&#39064;&#19978;&#30340;&#20248;&#28857;&#65292;&#23637;&#31034;&#20102;&#20854;&#23454;&#38469;&#30340;&#26377;&#29992;&#24615;&#21644;&#35299;&#37322;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Resilient Multiple Choice Learning (rMCL), an extension of the MCL approach for conditional distribution estimation in regression settings where multiple targets may be sampled for each training input. Multiple Choice Learning is a simple framework to tackle multimodal density estimation, using the Winner-Takes-All (WTA) loss for a set of hypotheses. In regression settings, the existing MCL variants focus on merging the hypotheses, thereby eventually sacrificing the diversity of the predictions. In contrast, our method relies on a novel learned scoring scheme underpinned by a mathematical framework based on Voronoi tessellations of the output space, from which we can derive a probabilistic interpretation. After empirically validating rMCL with experiments on synthetic data, we further assess its merits on the sound source localization problem, demonstrating its practical usefulness and the relevance of its interpretation.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21521;&#37327;&#21516;&#27493;&#22312;&#20998;&#24067;&#24335;&#26080;&#30005;&#27744;&#29289;&#32852;&#32593;&#32593;&#32476;&#20013;&#36827;&#34892;&#24212;&#29992;&#21644;&#33410;&#33021;&#25968;&#25454;&#32858;&#21512;&#30340;&#26426;&#21046;&#65292;&#26088;&#22312;&#20026;&#26080;&#30005;&#27744;&#29289;&#32852;&#32593;&#35774;&#22791;&#25552;&#20379;&#21487;&#25345;&#32493;&#30340;&#24212;&#29992;&#25903;&#25345;&#65292;&#24182;&#35299;&#20915;&#20102;&#30001;&#20110;&#21160;&#24577;&#33021;&#37327;&#25910;&#38598;&#29615;&#22659;&#23548;&#33268;&#30340;&#20219;&#21153;&#25191;&#34892;&#19981;&#31283;&#23450;&#24615;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2311.01050</link><description>&lt;p&gt;
&#20351;&#29992;&#21521;&#37327;&#21516;&#27493;&#22312;&#20998;&#24067;&#24335;&#26080;&#30005;&#27744;&#29289;&#32852;&#32593;&#32593;&#32476;&#20013;&#24212;&#29992;&#21644;&#33410;&#33021;&#25968;&#25454;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
Application and Energy-Aware Data Aggregation using Vector Synchronization in Distributed Battery-less IoT Networks. (arXiv:2311.01050v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01050
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21521;&#37327;&#21516;&#27493;&#22312;&#20998;&#24067;&#24335;&#26080;&#30005;&#27744;&#29289;&#32852;&#32593;&#32593;&#32476;&#20013;&#36827;&#34892;&#24212;&#29992;&#21644;&#33410;&#33021;&#25968;&#25454;&#32858;&#21512;&#30340;&#26426;&#21046;&#65292;&#26088;&#22312;&#20026;&#26080;&#30005;&#27744;&#29289;&#32852;&#32593;&#35774;&#22791;&#25552;&#20379;&#21487;&#25345;&#32493;&#30340;&#24212;&#29992;&#25903;&#25345;&#65292;&#24182;&#35299;&#20915;&#20102;&#30001;&#20110;&#21160;&#24577;&#33021;&#37327;&#25910;&#38598;&#29615;&#22659;&#23548;&#33268;&#30340;&#20219;&#21153;&#25191;&#34892;&#19981;&#31283;&#23450;&#24615;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30005;&#27744;&#29289;&#32852;&#32593;&#35774;&#22791;&#26159;&#19979;&#19968;&#20195;&#26080;&#32447;&#32593;&#32476;&#21487;&#25345;&#32493;&#32511;&#33394;&#20513;&#35758;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#36825;&#20123;&#26080;&#30005;&#27744;&#35774;&#22791;&#21033;&#29992;&#29615;&#22659;&#20013;&#30340;&#29615;&#22659;&#33021;&#37327;&#36827;&#34892;&#24037;&#20316;&#12290;&#33021;&#37327;&#25910;&#38598;&#29615;&#22659;&#26159;&#21160;&#24577;&#30340;&#65292;&#23548;&#33268;&#38388;&#27463;&#24615;&#20219;&#21153;&#25191;&#34892;&#12290;&#25910;&#38598;&#21040;&#30340;&#33021;&#37327;&#23384;&#20648;&#22312;&#23567;&#22411;&#30005;&#23481;&#22120;&#20013;&#65292;&#20445;&#35777;&#24212;&#29992;&#20219;&#21153;&#30340;&#25191;&#34892;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20027;&#35201;&#30446;&#26631;&#26159;&#25552;&#20379;&#19968;&#31181;&#26426;&#21046;&#26469;&#23545;&#20256;&#24863;&#22120;&#25968;&#25454;&#36827;&#34892;&#32858;&#21512;&#65292;&#24182;&#22312;&#20998;&#24067;&#24335;&#26080;&#30005;&#27744;&#29289;&#32852;&#32593;&#32593;&#32476;&#20013;&#25552;&#20379;&#21487;&#25345;&#32493;&#30340;&#24212;&#29992;&#25903;&#25345;&#12290;&#25105;&#20204;&#23545;&#30001;&#35768;&#22810;&#26080;&#30005;&#27744;&#29289;&#32852;&#32593;&#20256;&#24863;&#22120;&#30828;&#20214;&#27169;&#22359;&#21644;&#24322;&#26500;&#29289;&#32852;&#32593;&#24212;&#29992;&#32452;&#25104;&#30340;&#20998;&#24067;&#24335;&#29289;&#32852;&#32593;&#32593;&#32476;&#31995;&#32479;&#36827;&#34892;&#20102;&#24314;&#27169;&#65292;&#36825;&#20123;&#24212;&#29992;&#22312;&#35774;&#22791;-&#36793;&#32536;-&#20113;&#30340;&#36830;&#32493;&#20307;&#20013;&#33719;&#24471;&#25903;&#25345;&#12290;&#36825;&#20123;&#24212;&#29992;&#31243;&#24207;&#38656;&#35201;&#26469;&#33258;&#20998;&#24067;&#24335;&#30340;&#26080;&#30005;&#27744;&#30828;&#20214;&#27169;&#22359;&#30340;&#20256;&#24863;&#22120;&#25968;&#25454;&#65292;&#24182;&#20855;&#26377;&#23545;&#27169;&#22359;&#25191;&#34892;&#22120;&#30340;&#32852;&#21512;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
The battery-less Internet of Things (IoT) devices are a key element in the sustainable green initiative for the next-generation wireless networks. These battery-free devices use the ambient energy, harvested from the environment. The energy harvesting environment is dynamic and causes intermittent task execution. The harvested energy is stored in small capacitors and it is challenging to assure the application task execution. The main goal is to provide a mechanism to aggregate the sensor data and provide a sustainable application support in the distributed battery-less IoT network. We model the distributed IoT network system consisting of many battery-free IoT sensor hardware modules and heterogeneous IoT applications that are being supported in the device-edge-cloud continuum. The applications require sensor data from a distributed set of battery-less hardware modules and there is provision of joint control over the module actuators. We propose an application-aware task and energy ma
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36890;&#20449;&#29702;&#35770;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31070;&#32463;&#31454;&#20105;&#26469;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#23618;&#36755;&#20986;&#30340;&#20449;&#22122;&#27604;&#65292;&#20174;&#32780;&#25552;&#39640;&#28145;&#24230;&#32593;&#32476;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01047</link><description>&lt;p&gt;
&#36890;&#36807;&#20542;&#26012;&#25351;&#25968;&#23618;&#25913;&#21892;&#31283;&#20581;&#24615;&#65306;&#22522;&#20110;&#36890;&#20449;&#29702;&#35770;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Improving Robustness via Tilted Exponential Layer: A Communication-Theoretic Perspective. (arXiv:2311.01047v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01047
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36890;&#20449;&#29702;&#35770;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31070;&#32463;&#31454;&#20105;&#26469;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#23618;&#36755;&#20986;&#30340;&#20449;&#22122;&#27604;&#65292;&#20174;&#32780;&#25552;&#39640;&#28145;&#24230;&#32593;&#32476;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#21319;&#28145;&#24230;&#32593;&#32476;&#31283;&#20581;&#24615;&#30340;&#26368;&#26032;&#25216;&#26415;&#22823;&#22810;&#20381;&#36182;&#20110;&#21512;&#36866;&#30340;&#25968;&#25454;&#22686;&#24378;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36890;&#20449;&#29702;&#35770;&#30340;&#20114;&#34917;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#23398;&#20064;&#21644;&#25512;&#29702;&#20013;&#30340;&#31070;&#32463;&#31454;&#20105;&#26469;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#23618;&#36755;&#20986;&#30340;&#20449;&#22122;&#27604;&#12290;&#38500;&#20102;&#26368;&#23567;&#21270;&#26631;&#20934;&#30340;&#31471;&#21040;&#31471;&#20195;&#20215;&#22806;&#65292;&#31070;&#32463;&#20803;&#36890;&#36807;&#26368;&#22823;&#21270;&#20542;&#26012;&#25351;&#25968;&#65288;TEXP&#65289;&#23618;&#30340;&#30446;&#26631;&#20989;&#25968;&#26469;&#31454;&#20105;&#20197;&#31232;&#30095;&#22320;&#34920;&#31034;&#23618;&#36755;&#20837;&#12290;TEXP&#23398;&#20064;&#21487;&#20197;&#34987;&#35299;&#37322;&#20026;&#22312;&#25968;&#25454;&#22122;&#22768;&#30340;&#39640;&#26031;&#27169;&#22411;&#19979;&#36890;&#36807;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#26469;&#21305;&#37197;&#28388;&#27874;&#22120;&#12290;&#22312;TEXP&#23618;&#20013;&#65292;&#36890;&#36807;&#20351;&#29992;&#20542;&#26012;&#30340;softmax&#26367;&#20195;&#25209;&#37327;&#24402;&#19968;&#21270;&#26469;&#36827;&#34892;&#25512;&#29702;&#65292;&#21487;&#20197;&#35299;&#37322;&#20026;&#35745;&#31639;&#27599;&#20010;&#31070;&#32463;&#20803;&#20195;&#34920;&#30340;&#31454;&#20105;&#20449;&#21495;&#20551;&#35774;&#30340;&#21518;&#39564;&#27010;&#29575;&#12290;&#36890;&#36807;&#31616;&#21270;&#27169;&#22411;&#25552;&#20379;&#27934;&#23519;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#26631;&#20934;&#22270;&#20687;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art techniques for enhancing robustness of deep networks mostly rely on empirical risk minimization with suitable data augmentation. In this paper, we propose a complementary approach motivated by communication theory, aimed at enhancing the signal-to-noise ratio at the output of a neural network layer via neural competition during learning and inference. In addition to minimization of a standard end-to-end cost, neurons compete to sparsely represent layer inputs by maximization of a tilted exponential (TEXP) objective function for the layer. TEXP learning can be interpreted as maximum likelihood estimation of matched filters under a Gaussian model for data noise. Inference in a TEXP layer is accomplished by replacing batch norm by a tilted softmax, which can be interpreted as computation of posterior probabilities for the competing signaling hypotheses represented by each neuron. After providing insights via simplified models, we show, by experimentation on standard image
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;SGLD&#30340;&#26080;&#26102;&#38388;&#20449;&#24687;&#35770;&#24191;&#20041;&#30028;&#65292;&#23613;&#31649;&#36845;&#20195;&#27425;&#25968;&#21644;&#27493;&#38271;&#21487;&#33021;&#19981;&#22266;&#23450;&#65292;&#20294;&#36825;&#20123;&#30028;&#22312;&#26679;&#26412;&#22823;&#23567;&#22686;&#21152;&#26102;&#20250;&#34928;&#20943;&#20026;&#38646;&#12290;&#21516;&#26102;&#65292;&#36824;&#24314;&#31435;&#20102;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#25439;&#22833;&#30456;&#21516;&#26102;&#30340;&#20449;&#24687;&#35770;&#24191;&#20041;&#30028;&#65292;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;&#24037;&#20316;&#20013;&#27493;&#38271;&#20381;&#36182;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#25913;&#36827;&#30340;&#36807;&#24230;&#39118;&#38505;&#30028;&#12290;</title><link>http://arxiv.org/abs/2311.01046</link><description>&lt;p&gt;
SGLD&#30340;&#26080;&#26102;&#38388;&#20449;&#24687;&#35770;&#24191;&#20041;&#30028;&#30340;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Time-Independent Information-Theoretic Generalization Bounds for SGLD. (arXiv:2311.01046v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01046
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;SGLD&#30340;&#26080;&#26102;&#38388;&#20449;&#24687;&#35770;&#24191;&#20041;&#30028;&#65292;&#23613;&#31649;&#36845;&#20195;&#27425;&#25968;&#21644;&#27493;&#38271;&#21487;&#33021;&#19981;&#22266;&#23450;&#65292;&#20294;&#36825;&#20123;&#30028;&#22312;&#26679;&#26412;&#22823;&#23567;&#22686;&#21152;&#26102;&#20250;&#34928;&#20943;&#20026;&#38646;&#12290;&#21516;&#26102;&#65292;&#36824;&#24314;&#31435;&#20102;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#25439;&#22833;&#30456;&#21516;&#26102;&#30340;&#20449;&#24687;&#35770;&#24191;&#20041;&#30028;&#65292;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;&#24037;&#20316;&#20013;&#27493;&#38271;&#20381;&#36182;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#25913;&#36827;&#30340;&#36807;&#24230;&#39118;&#38505;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20809;&#28369;&#24615;&#21644;&#32791;&#25955;&#24615;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#38543;&#26426;&#26799;&#24230; Langevin &#21160;&#21147;&#23398; (SGLD) &#30340;&#26032;&#39062;&#20449;&#24687;&#35770;&#24191;&#20041;&#30028;&#12290;&#25105;&#20204;&#30340;&#30028;&#19981;&#20381;&#36182;&#20110;&#26102;&#38388;&#65292;&#22312;&#26679;&#26412;&#22823;&#23567;&#22686;&#21152;&#26102;&#20250;&#34928;&#20943;&#33267;&#38646;&#65292;&#19981;&#35770;&#36845;&#20195;&#27425;&#25968;&#21644;&#27493;&#38271;&#26159;&#21542;&#22266;&#23450;&#12290;&#19982;&#20197;&#21069;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#36890;&#36807;&#20851;&#27880; Kullback--Leibler &#25955;&#24230;&#30340;&#26102;&#38388;&#28436;&#21270;&#26469;&#25512;&#23548;&#24191;&#20041;&#35823;&#24046;&#30028;&#65292;&#35813;&#25955;&#24230;&#19982;&#25968;&#25454;&#38598;&#30340;&#31283;&#23450;&#24615;&#26377;&#20851;&#24182;&#19988;&#26159;&#36755;&#20986;&#21442;&#25968;&#19982;&#36755;&#20837;&#25968;&#25454;&#38598;&#20043;&#38388;&#20114;&#20449;&#24687;&#30340;&#19978;&#30028;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#35777;&#26126; SGLD &#30340;&#25439;&#22833;&#20989;&#25968;&#26159;&#27425;&#25351;&#25968;&#30340;&#26469;&#24314;&#31435;&#31532;&#19968;&#20010;&#24403;&#35757;&#32451;&#21644;&#27979;&#35797;&#25439;&#22833;&#30456;&#21516;&#26102;&#30340;&#20449;&#24687;&#35770;&#24191;&#20041;&#30028;&#12290;&#36825;&#20010;&#30028;&#20063;&#26159;&#26080;&#26102;&#38388;&#20851;&#32852;&#30340;&#65292;&#24182;&#19988;&#28040;&#38500;&#20102;&#29616;&#26377;&#24037;&#20316;&#20013;&#27493;&#38271;&#20381;&#36182;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#25913;&#36827;&#30340;&#36807;&#24230;&#39118;&#38505;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide novel information-theoretic generalization bounds for stochastic gradient Langevin dynamics (SGLD) under the assumptions of smoothness and dissipativity, which are widely used in sampling and non-convex optimization studies. Our bounds are time-independent and decay to zero as the sample size increases, regardless of the number of iterations and whether the step size is fixed. Unlike previous studies, we derive the generalization error bounds by focusing on the time evolution of the Kullback--Leibler divergence, which is related to the stability of datasets and is the upper bound of the mutual information between output parameters and an input dataset. Additionally, we establish the first information-theoretic generalization bound when the training and test loss are the same by showing that a loss function of SGLD is sub-exponential. This bound is also time-independent and removes the problematic step size dependence in existing work, leading to an improved excess risk bound
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#28608;&#27963;&#35270;&#35282;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#31934;&#24515;&#36873;&#25321;&#36739;&#23569;&#30340;&#25968;&#25454;&#25552;&#39640;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36825;&#19968;&#26041;&#27861;&#33021;&#22815;&#22312;&#22270;&#25968;&#25454;&#20013;&#36873;&#25321;&#26368;&#20855;&#20195;&#34920;&#24615;&#21644;&#25351;&#23548;&#24615;&#30340;&#25968;&#25454;&#28857;&#36827;&#34892;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2311.01038</link><description>&lt;p&gt;
&#26356;&#23569;&#26356;&#22909;&#65306;&#22522;&#20110;&#25968;&#25454;&#28608;&#27963;&#35270;&#35282;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Better with Less: A Data-Active Perspective on Pre-Training Graph Neural Networks. (arXiv:2311.01038v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#28608;&#27963;&#35270;&#35282;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#31934;&#24515;&#36873;&#25321;&#36739;&#23569;&#30340;&#25968;&#25454;&#25552;&#39640;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36825;&#19968;&#26041;&#27861;&#33021;&#22815;&#22312;&#22270;&#25968;&#25454;&#20013;&#36873;&#25321;&#26368;&#20855;&#20195;&#34920;&#24615;&#21644;&#25351;&#23548;&#24615;&#30340;&#25968;&#25454;&#28857;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#39044;&#35757;&#32451;&#26088;&#22312;&#21033;&#29992;&#26080;&#26631;&#27880;&#25968;&#25454;&#23398;&#20064;&#21487;&#36801;&#31227;&#30340;&#30693;&#35782;&#20197;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#65292;&#24182;&#19988;&#26368;&#36817;&#24050;&#25104;&#20026;&#19968;&#20010;&#27963;&#36291;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#22270;&#39044;&#35757;&#32451;&#20013;&#23384;&#22312;&#30528;&#22823;&#25968;&#25454;&#30340;&#35781;&#21650;&#29616;&#35937;&#65306;&#26356;&#22810;&#30340;&#35757;&#32451;&#25968;&#25454;&#24182;&#19981;&#19968;&#23450;&#23548;&#33268;&#26356;&#22909;&#30340;&#19979;&#28216;&#24615;&#33021;&#12290;&#21463;&#36825;&#20010;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#23569;&#26356;&#22909;&#30340;&#22270;&#39044;&#35757;&#32451;&#26694;&#26550;&#65306;&#36873;&#25321;&#23569;&#37327;&#20294;&#31934;&#24515;&#36873;&#25321;&#30340;&#25968;&#25454;&#36755;&#20837;&#21040;GNN&#27169;&#22411;&#20013;&#20197;&#22686;&#24378;&#39044;&#35757;&#32451;&#12290;&#25152;&#25552;&#20986;&#30340;&#39044;&#35757;&#32451;&#27969;&#31243;&#34987;&#31216;&#20026;&#25968;&#25454;&#28608;&#27963;&#22270;&#39044;&#35757;&#32451;&#65288;APT&#65289;&#26694;&#26550;&#65292;&#30001;&#22270;&#36873;&#25321;&#22120;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#32452;&#25104;&#12290;&#22270;&#36873;&#25321;&#22120;&#26681;&#25454;&#22270;&#30340;&#22266;&#26377;&#23646;&#24615;&#21644;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#36873;&#25321;&#26368;&#20855;&#20195;&#34920;&#24615;&#21644;&#25351;&#23548;&#24615;&#30340;&#25968;&#25454;&#28857;&#12290;&#25152;&#25552;&#20986;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#20316;&#20026;&#26469;&#33258;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21453;&#39304;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-training on graph neural networks (GNNs) aims to learn transferable knowledge for downstream tasks with unlabeled data, and it has recently become an active research area. The success of graph pre-training models is often attributed to the massive amount of input data. In this paper, however, we identify the curse of big data phenomenon in graph pre-training: more training data do not necessarily lead to better downstream performance. Motivated by this observation, we propose a better-with-less framework for graph pre-training: fewer, but carefully chosen data are fed into a GNN model to enhance pre-training. The proposed pre-training pipeline is called the data-active graph pre-training (APT) framework, and is composed of a graph selector and a pre-training model. The graph selector chooses the most representative and instructive data points based on the inherent properties of graphs as well as predictive uncertainty. The proposed predictive uncertainty, as feedback from the pre-t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#38750;&#33258;&#22238;&#24402;&#36830;&#32493;&#26102;&#38388;&#28857;&#36807;&#31243;&#27169;&#22411;&#65292;&#29992;&#20110;&#38271;&#26399;&#20107;&#20214;&#39044;&#27979;&#12290;&#36890;&#36807;&#25972;&#20307;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#24207;&#21015;&#12289;&#24320;&#21457;&#21452;&#21521;&#26144;&#23556;&#21644;&#35774;&#35745;&#38477;&#22122;&#32593;&#32476;&#31561;&#25163;&#27573;&#65292;&#24471;&#21040;&#20102;&#26356;&#20248;&#30340;&#39044;&#27979;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2311.01033</link><description>&lt;p&gt;
&#38750;&#33258;&#22238;&#24402;&#22522;&#20110;&#25193;&#25955;&#30340;&#36830;&#32493;&#26102;&#38388;&#38271;&#26399;&#20107;&#20214;&#39044;&#27979;&#30340;&#26102;&#38388;&#28857;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Non-Autoregressive Diffusion-based Temporal Point Processes for Continuous-Time Long-Term Event Prediction. (arXiv:2311.01033v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01033
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#38750;&#33258;&#22238;&#24402;&#36830;&#32493;&#26102;&#38388;&#28857;&#36807;&#31243;&#27169;&#22411;&#65292;&#29992;&#20110;&#38271;&#26399;&#20107;&#20214;&#39044;&#27979;&#12290;&#36890;&#36807;&#25972;&#20307;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#24207;&#21015;&#12289;&#24320;&#21457;&#21452;&#21521;&#26144;&#23556;&#21644;&#35774;&#35745;&#38477;&#22122;&#32593;&#32476;&#31561;&#25163;&#27573;&#65292;&#24471;&#21040;&#20102;&#26356;&#20248;&#30340;&#39044;&#27979;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#26102;&#38388;&#38271;&#26399;&#20107;&#20214;&#39044;&#27979;&#22312;&#35768;&#22810;&#24212;&#29992;&#22330;&#26223;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#24037;&#20316;&#20381;&#36182;&#20110;&#33258;&#22238;&#24402;&#26694;&#26550;&#26469;&#39044;&#27979;&#20107;&#20214;&#24207;&#21015;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#23481;&#26131;&#32047;&#31215;&#38169;&#35823;&#65292;&#20174;&#32780;&#24433;&#21709;&#39044;&#27979;&#30340;&#36136;&#37327;&#12290;&#21463;&#21040;&#38477;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#38750;&#33258;&#22238;&#24402;&#36830;&#32493;&#26102;&#38388;&#28857;&#36807;&#31243;&#27169;&#22411;&#65292;&#29992;&#20110;&#38271;&#26399;&#20107;&#20214;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#25972;&#20307;&#39044;&#27979;&#26410;&#26469;&#30340;&#20107;&#20214;&#24207;&#21015;&#65292;&#32780;&#19981;&#26159;&#20197;&#33258;&#22238;&#24402;&#30340;&#26041;&#24335;&#36880;&#20010;&#29983;&#25104;&#20107;&#20214;&#12290;&#20026;&#20102;&#22312;&#20107;&#20214;&#24207;&#21015;&#19978;&#36827;&#34892;&#25193;&#25955;&#22788;&#29702;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#30446;&#26631;&#20107;&#20214;&#24207;&#21015;&#21644;&#27431;&#20960;&#37324;&#24503;&#21521;&#37327;&#31354;&#38388;&#20043;&#38388;&#30340;&#21452;&#21521;&#26144;&#23556;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#38477;&#22122;&#32593;&#32476;&#65292;&#20197;&#25429;&#25417;&#39034;&#24207;&#21644;&#19978;&#19979;&#25991;&#29305;&#24449;&#65292;&#25552;&#39640;&#26679;&#26412;&#36136;&#37327;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#38271;&#26399;&#20107;&#20214;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continuous-time long-term event prediction plays an important role in many application scenarios. Most existing works rely on autoregressive frameworks to predict event sequences, which suffer from error accumulation, thus compromising prediction quality. Inspired by the success of denoising diffusion probabilistic models, we propose a diffusion-based non-autoregressive temporal point process model for long-term event prediction in continuous time. Instead of generating events one at a time in an autoregressive way, our model predicts the future event sequence entirely as a whole. In order to perform diffusion processes on event sequences, we develop a bidirectional map between target event sequences and the Euclidean vector space. Furthermore, we design a novel denoising network to capture both sequential and contextual features for better sample quality. Extensive experiments are conducted to prove the superiority of our proposed model over state-of-the-art methods on long-term event
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36317;&#31163;&#30340;&#20256;&#25773;&#31574;&#30053;TAGNet&#65292;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20219;&#21153;&#20013;&#30340;&#39640;&#25928;&#25512;&#29702;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;TAGNet&#33021;&#22815;&#22312;&#20445;&#25345;&#24615;&#33021;&#30340;&#21069;&#25552;&#19979;&#20943;&#23569;&#20256;&#25773;&#28040;&#24687;&#30340;&#25968;&#37327;&#65292;&#24182;&#19988;&#22797;&#26434;&#24230;&#19982;&#23618;&#25968;&#26080;&#20851;&#12290;</title><link>http://arxiv.org/abs/2311.01024</link><description>&lt;p&gt;
&#22522;&#20110;&#36317;&#31163;&#30340;&#20256;&#25773;&#31574;&#30053;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#30340;&#25928;&#29575;&#25552;&#21319;
&lt;/p&gt;
&lt;p&gt;
Distance-Based Propagation for Efficient Knowledge Graph Reasoning. (arXiv:2311.01024v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01024
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36317;&#31163;&#30340;&#20256;&#25773;&#31574;&#30053;TAGNet&#65292;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20219;&#21153;&#20013;&#30340;&#39640;&#25928;&#25512;&#29702;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;TAGNet&#33021;&#22815;&#22312;&#20445;&#25345;&#24615;&#33021;&#30340;&#21069;&#25552;&#19979;&#20943;&#23569;&#20256;&#25773;&#28040;&#24687;&#30340;&#25968;&#37327;&#65292;&#24182;&#19988;&#22797;&#26434;&#24230;&#19982;&#23618;&#25968;&#26080;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26088;&#22312;&#39044;&#27979;&#30693;&#35782;&#22270;&#35889;&#20013;&#26410;&#35265;&#30340;&#36793;&#65292;&#20174;&#32780;&#21457;&#29616;&#26032;&#30340;&#20107;&#23454;&#12290;&#19968;&#31867;&#26032;&#30340;&#26041;&#27861;&#25552;&#20986;&#20102;&#36890;&#36807;&#32858;&#21512;&#36335;&#24452;&#20449;&#24687;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#24040;&#22823;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#39281;&#21463;&#25928;&#29575;&#38382;&#39064;&#30340;&#22256;&#25200;&#12290;&#34429;&#28982;&#26368;&#36817;&#26377;&#19968;&#20123;&#23581;&#35797;&#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#36335;&#24452;&#20462;&#21098;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#22312;&#24615;&#33021;&#19978;&#20570;&#20986;&#20102;&#29306;&#29298;&#20197;&#25442;&#21462;&#25928;&#29575;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#20004;&#20010;&#22266;&#26377;&#38480;&#21046;&#65292;&#36825;&#20123;&#38480;&#21046;&#24433;&#21709;&#20102;&#25928;&#29575;&#21644;&#34920;&#31034;&#36136;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;TAGNet&#65292;&#23427;&#33021;&#22815;&#26377;&#25928;&#22320;&#20256;&#25773;&#20449;&#24687;&#12290;&#36825;&#26159;&#36890;&#36807;&#20165;&#22312;&#27599;&#23545;&#28304;-&#30446;&#26631;&#23545;&#20013;&#32858;&#21512;&#22266;&#23450;&#31383;&#21475;&#20013;&#30340;&#36335;&#24452;&#26469;&#23454;&#29616;&#30340;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;TAGNet&#30340;&#22797;&#26434;&#24230;&#19982;&#23618;&#25968;&#26080;&#20851;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;TAGNet&#21487;&#20197;&#20943;&#23569;&#20256;&#25773;&#28040;&#24687;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph completion (KGC) aims to predict unseen edges in knowledge graphs (KGs), resulting in the discovery of new facts. A new class of methods have been proposed to tackle this problem by aggregating path information. These methods have shown tremendous ability in the task of KGC. However they are plagued by efficiency issues. Though there are a few recent attempts to address this through learnable path pruning, they often sacrifice the performance to gain efficiency. In this work, we identify two intrinsic limitations of these methods that affect the efficiency and representation quality. To address the limitations, we introduce a new method, TAGNet, which is able to efficiently propagate information. This is achieved by only aggregating paths in a fixed window for each source-target pair. We demonstrate that the complexity of TAGNet is independent of the number of layers. Extensive experiments demonstrate that TAGNet can cut down on the number of propagated messages by as m
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31163;&#25955;&#25193;&#25955;&#23398;&#20064;&#26080;&#30417;&#30563;&#30340;&#33258;&#21160;&#39550;&#39542;&#19990;&#30028;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;VQVAE&#23545;&#20256;&#24863;&#22120;&#35266;&#23519;&#36827;&#34892;&#26631;&#35760;&#21270;&#24182;&#36890;&#36807;&#31163;&#25955;&#25193;&#25955;&#39044;&#27979;&#26410;&#26469;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#28857;&#20113;&#35266;&#23519;&#20013;&#23454;&#29616;&#20102;&#26174;&#33879;&#25913;&#36827;&#65292;&#23558;1&#31186;&#39044;&#27979;&#30340;SOTA Chamfer&#36317;&#31163;&#38477;&#20302;&#20102;65%&#20197;&#19978;&#12290;</title><link>http://arxiv.org/abs/2311.01017</link><description>&lt;p&gt;
&#36890;&#36807;&#31163;&#25955;&#25193;&#25955;&#23398;&#20064;&#26080;&#30417;&#30563;&#30340;&#33258;&#21160;&#39550;&#39542;&#19990;&#30028;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Unsupervised World Models for Autonomous Driving via Discrete Diffusion. (arXiv:2311.01017v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31163;&#25955;&#25193;&#25955;&#23398;&#20064;&#26080;&#30417;&#30563;&#30340;&#33258;&#21160;&#39550;&#39542;&#19990;&#30028;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;VQVAE&#23545;&#20256;&#24863;&#22120;&#35266;&#23519;&#36827;&#34892;&#26631;&#35760;&#21270;&#24182;&#36890;&#36807;&#31163;&#25955;&#25193;&#25955;&#39044;&#27979;&#26410;&#26469;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#28857;&#20113;&#35266;&#23519;&#20013;&#23454;&#29616;&#20102;&#26174;&#33879;&#25913;&#36827;&#65292;&#23558;1&#31186;&#39044;&#27979;&#30340;SOTA Chamfer&#36317;&#31163;&#38477;&#20302;&#20102;65%&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#19990;&#30028;&#27169;&#22411;&#21487;&#20197;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#25945;&#20250;&#26234;&#33021;&#20307;&#19990;&#30028;&#30340;&#36816;&#20316;&#26041;&#24335;&#12290;&#23613;&#31649;&#23427;&#21487;&#20197;&#30475;&#20316;&#26159;&#24207;&#21015;&#24314;&#27169;&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#20294;&#22312;&#33258;&#21160;&#39550;&#39542;&#31561;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#65292;&#19982;&#20351;&#29992;&#29983;&#25104;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#65288;GPT&#65289;&#25193;&#23637;&#35821;&#35328;&#27169;&#22411;&#30456;&#27604;&#65292;&#25193;&#23637;&#19990;&#30028;&#27169;&#22411;&#30340;&#36827;&#23637;&#30456;&#23545;&#36739;&#24930;&#12290;&#25105;&#20204;&#25351;&#20986;&#20102;&#20004;&#20010;&#20027;&#35201;&#29942;&#39048;&#65306;&#22788;&#29702;&#22797;&#26434;&#21644;&#26080;&#32467;&#26500;&#30340;&#35266;&#23519;&#31354;&#38388;&#20197;&#21450;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19990;&#30028;&#24314;&#27169;&#26041;&#27861;&#65292;&#39318;&#20808;&#20351;&#29992;VQVAE&#23545;&#20256;&#24863;&#22120;&#35266;&#23519;&#36827;&#34892;&#26631;&#35760;&#21270;&#65292;&#28982;&#21518;&#36890;&#36807;&#31163;&#25955;&#25193;&#25955;&#39044;&#27979;&#26410;&#26469;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#24182;&#34892;&#35299;&#30721;&#21644;&#21435;&#22122;&#26631;&#35760;&#65292;&#25105;&#20204;&#23558;&#36974;&#34109;&#29983;&#25104;&#22270;&#20687;&#36716;&#25442;&#22120;&#36716;&#25442;&#20026;&#31163;&#25955;&#25193;&#25955;&#26694;&#26550;&#65292;&#24182;&#36827;&#34892;&#20102;&#19968;&#20123;&#31616;&#21333;&#30340;&#25913;&#36827;&#65292;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;&#24403;&#24212;&#29992;&#20110;&#28857;&#20113;&#35266;&#23519;&#30340;&#19990;&#30028;&#27169;&#22411;&#23398;&#20064;&#26102;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23558;1&#31186;&#39044;&#27979;&#30340;SOTA Chamfer&#36317;&#31163;&#38477;&#20302;&#20102;65%&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning world models can teach an agent how the world works in an unsupervised manner. Even though it can be viewed as a special case of sequence modeling, progress for scaling world models on robotic applications such as autonomous driving has been somewhat less rapid than scaling language models with Generative Pre-trained Transformers (GPT). We identify two reasons as major bottlenecks: dealing with complex and unstructured observation space, and having a scalable generative model. Consequently, we propose a novel world modeling approach that first tokenizes sensor observations with VQVAE, then predicts the future via discrete diffusion. To efficiently decode and denoise tokens in parallel, we recast Masked Generative Image Transformer into the discrete diffusion framework with a few simple changes, resulting in notable improvement. When applied to learning world models on point cloud observations, our model reduces prior SOTA Chamfer distance by more than 65% for 1s prediction, an
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#35299;&#37322;&#24615;&#30340;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#21033;&#29992;&#35813;&#25968;&#25454;&#38598;&#21019;&#24314;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#25269;&#25239;&#22522;&#20934;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35768;&#22810;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#36825;&#20123;&#25915;&#20987;&#31574;&#30053;&#30340;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2311.01011</link><description>&lt;p&gt;
Tensor Trust&#65306;&#26469;&#33258;&#22312;&#32447;&#28216;&#25103;&#30340;&#21487;&#35299;&#37322;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Tensor Trust: Interpretable Prompt Injection Attacks from an Online Game. (arXiv:2311.01011v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01011
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#35299;&#37322;&#24615;&#30340;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#21033;&#29992;&#35813;&#25968;&#25454;&#38598;&#21019;&#24314;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#25269;&#25239;&#22522;&#20934;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35768;&#22810;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#36825;&#20123;&#25915;&#20987;&#31574;&#30053;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#30340;&#22686;&#21152;&#65292;&#23427;&#20204;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#30340;&#23041;&#32961;&#65306;&#24694;&#24847;&#31532;&#19977;&#26041;&#25552;&#31034;&#20250;&#25197;&#26354;&#31995;&#32479;&#35774;&#35745;&#32773;&#30340;&#24847;&#22270;&#12290;&#20026;&#20102;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#30740;&#31350;&#27492;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;126,000&#20010;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#21644;46,000&#20010;&#22522;&#20110;&#25552;&#31034;&#30340;&#8220;&#38450;&#24481;&#8221;&#65288;&#30001;&#19968;&#20010;&#21517;&#20026;Tensor Trust&#30340;&#22312;&#32447;&#28216;&#25103;&#30340;&#29609;&#23478;&#21019;&#24314;&#65289;&#30340;&#25968;&#25454;&#38598;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#30446;&#21069;&#38024;&#23545;LLMs&#30340;&#26368;&#22823;&#30340;&#20154;&#24037;&#29983;&#25104;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#20013;&#30340;&#25915;&#20987;&#20855;&#26377;&#24456;&#22810;&#26131;&#20110;&#35299;&#37322;&#30340;&#32467;&#26500;&#65292;&#24182;&#25581;&#31034;&#20102;LLMs&#30340;&#24369;&#28857;&#12290;&#25105;&#20204;&#36824;&#21033;&#29992;&#35813;&#25968;&#25454;&#38598;&#21019;&#24314;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#25552;&#31034;&#27880;&#20837;&#25269;&#25239;&#22522;&#20934;&#65292;&#20998;&#21035;&#31216;&#20026;&#25552;&#31034;&#25552;&#21462;&#21644;&#25552;&#31034;&#21163;&#25345;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#32467;&#26524;&#26174;&#31034;&#65292;&#35768;&#22810;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;Tensor Trust&#25968;&#25454;&#38598;&#20013;&#30340;&#25915;&#20987;&#31574;&#30053;&#30340;&#25915;&#20987;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#19968;&#20123;
&lt;/p&gt;
&lt;p&gt;
While Large Language Models (LLMs) are increasingly being used in real-world applications, they remain vulnerable to prompt injection attacks: malicious third party prompts that subvert the intent of the system designer. To help researchers study this problem, we present a dataset of over 126,000 prompt injection attacks and 46,000 prompt-based "defenses" against prompt injection, all created by players of an online game called Tensor Trust. To the best of our knowledge, this is currently the largest dataset of human-generated adversarial examples for instruction-following LLMs. The attacks in our dataset have a lot of easily interpretable stucture, and shed light on the weaknesses of LLMs. We also use the dataset to create a benchmark for resistance to two types of prompt injection, which we refer to as prompt extraction and prompt hijacking. Our benchmark results show that many models are vulnerable to the attack strategies in the Tensor Trust dataset. Furthermore, we show that some 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#32034;&#20102;&#32479;&#19968;&#35270;&#35282;&#19979;&#24555;&#36895;&#35745;&#31639;Shapley&#20540;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#25674;&#38144;&#20272;&#35745;&#22120;SimSHAP&#65292;&#36890;&#36807;&#28040;&#38500;&#20887;&#20313;&#25216;&#26415;&#26174;&#33879;&#21152;&#36895;&#20102;&#20934;&#30830;Shapley&#20540;&#30340;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2311.01010</link><description>&lt;p&gt;
&#25506;&#32034;&#24555;&#36895;Shapley&#20540;&#20272;&#35745;&#30340;&#32479;&#19968;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Exploring Unified Perspective For Fast Shapley Value Estimation. (arXiv:2311.01010v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01010
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#32034;&#20102;&#32479;&#19968;&#35270;&#35282;&#19979;&#24555;&#36895;&#35745;&#31639;Shapley&#20540;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#25674;&#38144;&#20272;&#35745;&#22120;SimSHAP&#65292;&#36890;&#36807;&#28040;&#38500;&#20887;&#20313;&#25216;&#26415;&#26174;&#33879;&#21152;&#36895;&#20102;&#20934;&#30830;Shapley&#20540;&#30340;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Shapley&#20540;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#34987;&#24191;&#27867;&#25509;&#21463;&#21644;&#21487;&#38752;&#30340;&#24037;&#20855;&#65292;&#23427;&#20197;&#29702;&#35770;&#20844;&#29702;&#20026;&#22522;&#30784;&#65292;&#29992;&#20110;&#35299;&#20915;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#31561;&#40657;&#30418;&#27169;&#22411;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#22312;&#29305;&#24449;&#25968;&#30446;&#19978;&#65292;&#35745;&#31639;Shapley&#20540;&#20250;&#36935;&#21040;&#25351;&#25968;&#32423;&#22797;&#26434;&#24615;&#12290;&#24050;&#32463;&#25506;&#32034;&#20102;&#21508;&#31181;&#26041;&#27861;&#65292;&#21253;&#25324;ApproSemivalue&#12289;KernelSHAP&#21644;FastSHAP&#65292;&#20197;&#21152;&#36895;&#35745;&#31639;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#29616;&#26377;&#24037;&#20316;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#24471;&#20986;&#32467;&#35770;&#65292;&#21363;&#38543;&#26426;&#20272;&#35745;&#22120;&#21487;&#20197;&#32479;&#19968;&#20026;&#29305;&#24449;&#23376;&#38598;&#37325;&#35201;&#24615;&#25277;&#26679;&#30340;&#32447;&#24615;&#21464;&#25442;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#35774;&#35745;&#31616;&#21333;&#25674;&#38144;&#20272;&#35745;&#22120;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#21644;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#21363;SimSHAP&#65292;&#36890;&#36807;&#28040;&#38500;&#20887;&#20313;&#25216;&#26415;&#12290;&#22312;&#34920;&#26684;&#21644;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;SimSHAP&#30340;&#26377;&#25928;&#24615;&#65292;&#23427;&#26174;&#30528;&#21152;&#36895;&#20102;&#20934;&#30830;Shapley&#20540;&#30340;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
Shapley values have emerged as a widely accepted and trustworthy tool, grounded in theoretical axioms, for addressing challenges posed by black-box models like deep neural networks. However, computing Shapley values encounters exponential complexity in the number of features. Various approaches, including ApproSemivalue, KernelSHAP, and FastSHAP, have been explored to expedite the computation. We analyze the consistency of existing works and conclude that stochastic estimators can be unified as the linear transformation of importance sampling of feature subsets. Based on this, we investigate the possibility of designing simple amortized estimators and propose a straightforward and efficient one, SimSHAP, by eliminating redundant techniques. Extensive experiments conducted on tabular and image datasets validate the effectiveness of our SimSHAP, which significantly accelerates the computation of accurate Shapley values.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#33258;&#28982;&#35821;&#35328;&#35268;&#21017;&#21644;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#20154;&#24037;&#26234;&#33021;&#22242;&#38431;&#30340;&#25928;&#26524;&#12290;&#36890;&#36807;&#25214;&#21040;&#25968;&#25454;&#30340;&#23616;&#37096;&#21306;&#22495;&#21644;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25551;&#36848;&#65292;&#25105;&#20204;&#25945;&#23548;&#20154;&#31867;&#22914;&#20309;&#19982;AI&#21512;&#20316;&#12290;&#36890;&#36807;&#30446;&#26631;&#26816;&#27979;&#21644;&#38382;&#31572;&#20219;&#21153;&#30340;&#29992;&#25143;&#30740;&#31350;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20351;&#20154;&#24037;&#26234;&#33021;&#22242;&#38431;&#26356;&#21152;&#20934;&#30830;&#12290;</title><link>http://arxiv.org/abs/2311.01007</link><description>&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#33258;&#28982;&#35821;&#35328;&#35268;&#21017;&#21644;&#24341;&#23548;&#26469;&#25552;&#39640;&#20154;&#24037;&#26234;&#33021;&#22242;&#38431;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Effective Human-AI Teams via Learned Natural Language Rules and Onboarding. (arXiv:2311.01007v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01007
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#33258;&#28982;&#35821;&#35328;&#35268;&#21017;&#21644;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#20154;&#24037;&#26234;&#33021;&#22242;&#38431;&#30340;&#25928;&#26524;&#12290;&#36890;&#36807;&#25214;&#21040;&#25968;&#25454;&#30340;&#23616;&#37096;&#21306;&#22495;&#21644;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25551;&#36848;&#65292;&#25105;&#20204;&#25945;&#23548;&#20154;&#31867;&#22914;&#20309;&#19982;AI&#21512;&#20316;&#12290;&#36890;&#36807;&#30446;&#26631;&#26816;&#27979;&#21644;&#38382;&#31572;&#20219;&#21153;&#30340;&#29992;&#25143;&#30740;&#31350;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20351;&#20154;&#24037;&#26234;&#33021;&#22242;&#38431;&#26356;&#21152;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#36234;&#26469;&#36234;&#20381;&#36182;&#20110;AI&#20195;&#29702;&#26469;&#24110;&#21161;&#20182;&#20204;&#23436;&#25104;&#21508;&#31181;&#20219;&#21153;&#12290;&#20154;&#31867;&#24517;&#39035;&#30693;&#36947;&#20309;&#26102;&#20381;&#36182;&#20110;&#20195;&#29702;&#65292;&#19982;&#20195;&#29702;&#21512;&#20316;&#25110;&#24573;&#30053;&#20854;&#24314;&#35758;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25968;&#25454;&#21306;&#22495;&#21644;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#23398;&#20064;&#35268;&#21017;&#30340;&#26041;&#27861;&#65292;&#20197;&#35828;&#26126;&#20154;&#31867;&#24212;&#35813;&#22914;&#20309;&#19982;AI&#21512;&#20316;&#12290;&#25105;&#20204;&#30340;&#26032;&#39062;&#21306;&#22495;&#21457;&#29616;&#31639;&#27861;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#25214;&#21040;&#25968;&#25454;&#30340;&#23616;&#37096;&#21306;&#22495;&#20316;&#20026;&#37051;&#22495;&#65292;&#32416;&#27491;&#20102;&#20154;&#31867;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#28982;&#21518;&#65292;&#27599;&#20010;&#21306;&#22495;&#37117;&#36890;&#36807;&#36845;&#20195;&#21644;&#23545;&#27604;&#36807;&#31243;&#36827;&#34892;&#25551;&#36848;&#65292;&#20854;&#20013;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25551;&#36848;&#35813;&#21306;&#22495;&#12290;&#28982;&#21518;&#25105;&#20204;&#36890;&#36807;&#24341;&#23548;&#38454;&#27573;&#23558;&#36825;&#20123;&#35268;&#21017;&#25945;&#32473;&#20154;&#31867;&#12290;&#36890;&#36807;&#22312;&#30446;&#26631;&#26816;&#27979;&#21644;&#38382;&#31572;&#20219;&#21153;&#19978;&#30340;&#29992;&#25143;&#30740;&#31350;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20351;&#20154;&#24037;&#26234;&#33021;&#22242;&#38431;&#26356;&#21152;&#20934;&#30830;&#12290;&#25105;&#20204;&#36824;&#20998;&#21035;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#21306;&#22495;&#21457;&#29616;&#21644;&#25551;&#36848;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
People are relying on AI agents to assist them with various tasks. The human must know when to rely on the agent, collaborate with the agent, or ignore its suggestions. In this work, we propose to learn rules grounded in data regions and described in natural language that illustrate how the human should collaborate with the AI. Our novel region discovery algorithm finds local regions in the data as neighborhoods in an embedding space that corrects the human prior. Each region is then described using an iterative and contrastive procedure where a large language model describes the region. We then teach these rules to the human via an onboarding stage. Through user studies on object detection and question-answering tasks, we show that our method can lead to more accurate human-AI teams. We also evaluate our region discovery and description algorithms separately.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26368;&#22823;&#21270;&#37325;&#26032;&#26631;&#35760;&#20934;&#30830;&#24615;&#26469;&#36827;&#34892;&#40065;&#26834;&#25968;&#25454;&#20462;&#21098;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#25214;&#21040;&#19968;&#20010;&#23376;&#38598;&#65292;&#20351;&#24471;&#25152;&#26377;&#35757;&#32451;&#31034;&#20363;&#30340;&#37051;&#22495;&#32622;&#20449;&#24230;&#20043;&#21644;&#26368;&#22823;&#21270;&#12290;&#36825;&#20010;&#26041;&#27861;&#22312;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2311.01002</link><description>&lt;p&gt;
&#36890;&#36807;&#26368;&#22823;&#21270;&#37325;&#26032;&#26631;&#35760;&#20934;&#30830;&#24615;&#26469;&#36827;&#34892;&#40065;&#26834;&#25968;&#25454;&#20462;&#21098;
&lt;/p&gt;
&lt;p&gt;
Robust Data Pruning under Label Noise via Maximizing Re-labeling Accuracy. (arXiv:2311.01002v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01002
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26368;&#22823;&#21270;&#37325;&#26032;&#26631;&#35760;&#20934;&#30830;&#24615;&#26469;&#36827;&#34892;&#40065;&#26834;&#25968;&#25454;&#20462;&#21098;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#25214;&#21040;&#19968;&#20010;&#23376;&#38598;&#65292;&#20351;&#24471;&#25152;&#26377;&#35757;&#32451;&#31034;&#20363;&#30340;&#37051;&#22495;&#32622;&#20449;&#24230;&#20043;&#21644;&#26368;&#22823;&#21270;&#12290;&#36825;&#20010;&#26041;&#27861;&#22312;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20462;&#21098;&#26088;&#22312;&#23558;&#22823;&#22411;&#35757;&#32451;&#38598;&#32553;&#20943;&#20026;&#19968;&#20010;&#23567;&#32780;&#20449;&#24687;&#20016;&#23500;&#30340;&#23376;&#38598;&#65292;&#23545;&#20110;&#20943;&#23569;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#30340;&#24040;&#22823;&#35745;&#31639;&#25104;&#26412;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#20013;&#19981;&#21487;&#36991;&#20813;&#22320;&#21547;&#26377;&#27880;&#37322;&#22122;&#22768;&#65292;&#24182;&#19988;&#24050;&#32463;&#24320;&#21457;&#20102;&#35768;&#22810;&#40065;&#26834;&#23398;&#20064;&#26041;&#27861;&#65292;&#20294;&#23545;&#20110;&#22122;&#22768;&#40065;&#26834;&#23398;&#20064;&#22330;&#26223;&#19979;&#30340;&#25968;&#25454;&#20462;&#21098;&#20960;&#20046;&#26410;&#21463;&#21040;&#20851;&#27880;&#12290;&#36890;&#36807;&#33258;&#26657;&#27491;&#38169;&#35823;&#26631;&#31614;&#30340;&#26368;&#26032;&#37325;&#26032;&#26631;&#35760;&#26041;&#27861;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#24456;&#38590;&#30830;&#23450;&#21738;&#20010;&#23376;&#38598;&#33021;&#22815;&#22312;&#25972;&#20010;&#35757;&#32451;&#38598;&#20013;&#24341;&#21457;&#26368;&#20934;&#30830;&#30340;&#37325;&#26032;&#26631;&#35760;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24418;&#24335;&#21270;&#20102;&#37325;&#26032;&#26631;&#35760;&#30340;&#25968;&#25454;&#20462;&#21098;&#38382;&#39064;&#12290;&#39318;&#20808;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#35757;&#32451;&#31034;&#20363;&#34987;&#27491;&#30830;&#37325;&#26032;&#26631;&#35760;&#30340;&#21487;&#33021;&#24615;&#19982;&#20854;&#37051;&#22495;&#20013;&#30340;&#39044;&#27979;&#32622;&#20449;&#24230;&#25104;&#27604;&#20363;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#25968;&#25454;&#20462;&#21098;&#31639;&#27861;&#65292;&#21517;&#20026;Prune4Rel&#65292;&#23427;&#33021;&#25214;&#21040;&#19968;&#20010;&#23376;&#38598;&#65292;&#20351;&#24471;&#25152;&#26377;&#35757;&#32451;&#31034;&#20363;&#30340;&#37051;&#22495;&#32622;&#20449;&#24230;&#20043;&#21644;&#26368;&#22823;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data pruning, which aims to downsize a large training set into a small informative subset, is crucial for reducing the enormous computational costs of modern deep learning. Though large-scale data collections invariably contain annotation noise and numerous robust learning methods have been developed, data pruning for the noise-robust learning scenario has received little attention. With state-of-the-art Re-labeling methods that self-correct erroneous labels while training, it is challenging to identify which subset induces the most accurate re-labeling of erroneous labels in the entire training set. In this paper, we formalize the problem of data pruning with re-labeling. We first show that the likelihood of a training example being correctly re-labeled is proportional to the prediction confidence of its neighborhood in the subset. Therefore, we propose a novel data pruning algorithm, Prune4Rel, that finds a subset maximizing the total neighborhood confidence of all training examples,
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20174;&#23454;&#36341;&#32773;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#25552;&#20986;&#20102;&#22312;&#38646;&#21806;&#39046;&#22495;&#20013;&#20351;&#29992;&#26799;&#24230;&#25552;&#21319;&#26641;&#36827;&#34892;&#21487;&#25193;&#23637;&#27010;&#29575;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#30740;&#31350;&#20004;&#23618;&#27425;&#30340;&#23618;&#27425;&#32467;&#26500;&#21644;&#33258;&#19978;&#32780;&#19979;&#30340;&#39044;&#27979;&#26041;&#27861;&#65292;&#22312;&#22788;&#29702;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21644;&#38388;&#26029;&#24615;&#25968;&#25454;&#26041;&#38754;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.00993</link><description>&lt;p&gt;
&#22312;&#38646;&#21806;&#39046;&#22495;&#20013;&#20351;&#29992;&#26799;&#24230;&#25552;&#21319;&#26641;&#30340;&#21487;&#25193;&#23637;&#27010;&#29575;&#39044;&#27979;&#65306;&#20174;&#23454;&#36341;&#32773;&#35282;&#24230;&#20986;&#21457;
&lt;/p&gt;
&lt;p&gt;
Scalable Probabilistic Forecasting in Retail with Gradient Boosted Trees: A Practitioner's Approach. (arXiv:2311.00993v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00993
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20174;&#23454;&#36341;&#32773;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#25552;&#20986;&#20102;&#22312;&#38646;&#21806;&#39046;&#22495;&#20013;&#20351;&#29992;&#26799;&#24230;&#25552;&#21319;&#26641;&#36827;&#34892;&#21487;&#25193;&#23637;&#27010;&#29575;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#30740;&#31350;&#20004;&#23618;&#27425;&#30340;&#23618;&#27425;&#32467;&#26500;&#21644;&#33258;&#19978;&#32780;&#19979;&#30340;&#39044;&#27979;&#26041;&#27861;&#65292;&#22312;&#22788;&#29702;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21644;&#38388;&#26029;&#24615;&#25968;&#25454;&#26041;&#38754;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;M5&#31454;&#36187;&#22312;&#38646;&#21806;&#39044;&#27979;&#39046;&#22495;&#25512;&#21160;&#20102;&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#27880;&#24847;&#21040;&#31454;&#36187;&#25361;&#25112;&#19982;&#25105;&#20204;&#22312;&#19968;&#20010;&#22823;&#22411;&#30005;&#23376;&#21830;&#21153;&#20844;&#21496;&#38754;&#20020;&#30340;&#25361;&#25112;&#20043;&#38388;&#23384;&#22312;&#37325;&#35201;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#22330;&#26223;&#20013;&#30340;&#25968;&#25454;&#38598;&#27604;&#36739;&#22823;&#65288;&#25968;&#21313;&#19975;&#20010;&#26102;&#38388;&#24207;&#21015;&#65289;&#65292;&#32780;&#30005;&#23376;&#21830;&#21153;&#21487;&#20197;&#25552;&#20379;&#27604;&#23454;&#20307;&#38646;&#21806;&#21830;&#26356;&#22810;&#30340;&#20135;&#21697;&#36873;&#25321;&#65292;&#23548;&#33268;&#25968;&#25454;&#26356;&#21152;&#38388;&#26029;&#12290;&#20026;&#20102;&#22312;&#21487;&#34892;&#30340;&#35745;&#31639;&#24037;&#20316;&#37327;&#19979;&#25193;&#23637;&#21040;&#26356;&#22823;&#30340;&#25968;&#25454;&#38598;&#22823;&#23567;&#65292;&#39318;&#20808;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#20004;&#23618;&#27425;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#19978;&#32780;&#19979;&#30340;&#39044;&#27979;&#26041;&#27861;&#65292;&#22312;&#32858;&#21512;&#32423;&#21035;&#19978;&#39044;&#27979;&#36739;&#23569;&#25968;&#37327;&#30340;&#31995;&#21015;&#21644;&#38388;&#26029;&#24615;&#65292;&#28982;&#21518;&#36827;&#34892;&#20998;&#35299;&#20197;&#33719;&#24471;&#20915;&#31574;&#32423;&#21035;&#30340;&#39044;&#27979;&#12290;&#27010;&#29575;&#39044;&#27979;&#26159;&#26681;&#25454;&#20998;&#24067;&#20551;&#35774;&#29983;&#25104;&#30340;&#12290;&#20854;&#27425;&#65292;&#30452;&#25509;&#23545;&#23376;&#26679;&#26412;&#36827;&#34892;&#19979;&#23618;&#32423;&#30340;&#35757;&#32451;&#20063;&#21487;&#20197;&#26159;&#19968;&#31181;&#25193;&#23637;&#30340;&#26367;&#20195;&#26041;&#24335;&#12290;&#20351;&#29992;&#20027;&#35201;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#23376;&#38598;&#24314;&#27169;&#30340;&#24615;&#33021;&#12290;&#38500;&#20102;&#19987;&#26377;&#30340;&#25968;&#25454;&#38598;&#22806;&#65292;&#36824;&#20351;&#29992;&#20102;&#20854;&#20182;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent M5 competition has advanced the state-of-the-art in retail forecasting. However, we notice important differences between the competition challenge and the challenges we face in a large e-commerce company. The datasets in our scenario are larger (hundreds of thousands of time series), and e-commerce can afford to have a larger assortment than brick-and-mortar retailers, leading to more intermittent data. To scale to larger dataset sizes with feasible computational effort, firstly, we investigate a two-layer hierarchy and propose a top-down approach to forecasting at an aggregated level with less amount of series and intermittency, and then disaggregating to obtain the decision-level forecasts. Probabilistic forecasts are generated under distributional assumptions. Secondly, direct training at the lower level with subsamples can also be an alternative way of scaling. Performance of modelling with subsets is evaluated with the main dataset. Apart from a proprietary dataset, the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#24211;&#23384;&#37197;&#36865;&#38382;&#39064;&#65292;&#36890;&#36807;&#30452;&#25509;&#38598;&#25104;&#24211;&#23384;&#39044;&#27979;&#21644;&#36335;&#24452;&#20248;&#21270;&#65292;&#21487;&#33021;&#30830;&#20445;&#19968;&#20010;&#24378;&#22823;&#30340;&#20379;&#24212;&#38142;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2311.00983</link><description>&lt;p&gt;
&#20248;&#21270;&#24211;&#23384;&#37197;&#36865;&#65306;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Optimizing Inventory Routing: A Decision-Focused Learning Approach using Neural Networks. (arXiv:2311.00983v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00983
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#24211;&#23384;&#37197;&#36865;&#38382;&#39064;&#65292;&#36890;&#36807;&#30452;&#25509;&#38598;&#25104;&#24211;&#23384;&#39044;&#27979;&#21644;&#36335;&#24452;&#20248;&#21270;&#65292;&#21487;&#33021;&#30830;&#20445;&#19968;&#20010;&#24378;&#22823;&#30340;&#20379;&#24212;&#38142;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24211;&#23384;&#37197;&#36865;&#38382;&#39064;&#65288;IRP&#65289;&#26159;&#20379;&#24212;&#38142;&#31649;&#29702;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#23427;&#28041;&#21450;&#22312;&#32771;&#34385;&#24211;&#23384;&#38656;&#27714;&#35268;&#21010;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#24773;&#20917;&#19979;&#20248;&#21270;&#26377;&#25928;&#30340;&#36335;&#24452;&#36873;&#25321;&#12290;&#20026;&#20102;&#35299;&#20915;IRP&#38382;&#39064;&#65292;&#36890;&#24120;&#37319;&#29992;&#20004;&#38454;&#27573;&#30340;&#26041;&#27861;&#65292;&#39318;&#20808;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#39044;&#27979;&#38656;&#27714;&#65292;&#28982;&#21518;&#20351;&#29992;&#20248;&#21270;&#31639;&#27861;&#26469;&#26368;&#23567;&#21270;&#37197;&#36865;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#23454;&#29616;&#23436;&#32654;&#20934;&#30830;&#24230;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#65292;&#22240;&#20026;&#24211;&#23384;&#27700;&#24179;&#21463;&#21160;&#24577;&#19994;&#21153;&#29615;&#22659;&#30340;&#24433;&#21709;&#65292;&#36827;&#32780;&#24433;&#21709;&#21040;&#19979;&#19968;&#38454;&#27573;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#23548;&#33268;&#27425;&#20248;&#20915;&#31574;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#30340;IRP&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#31995;&#32479;&#20013;&#30452;&#25509;&#38598;&#25104;&#20102;&#24211;&#23384;&#39044;&#27979;&#21644;&#36335;&#24452;&#20248;&#21270;&#65292;&#21487;&#33021;&#30830;&#20445;&#19968;&#20010;&#24378;&#22823;&#30340;&#20379;&#24212;&#38142;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inventory Routing Problem (IRP) is a crucial challenge in supply chain management as it involves optimizing efficient route selection while considering the uncertainty of inventory demand planning. To solve IRPs, usually a two-stage approach is employed, where demand is predicted using machine learning techniques first, and then an optimization algorithm is used to minimize routing costs. Our experiment shows machine learning models fall short of achieving perfect accuracy because inventory levels are influenced by the dynamic business environment, which, in turn, affects the optimization problem in the next stage, resulting in sub-optimal decisions. In this paper, we formulate and propose a decision-focused learning-based approach to solving real-world IRPs. This approach directly integrates inventory prediction and routing optimization within an end-to-end system potentially ensuring a robust supply chain strategy.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#21270;&#23398;&#21453;&#24212;&#32593;&#32476;&#23454;&#29616;&#20102;&#29983;&#25104;&#27169;&#22411;&#30340;&#33258;&#20027;&#23398;&#20064;&#65292;&#33021;&#22815;&#20248;&#21270;&#20219;&#20309;&#35814;&#32454;&#24179;&#34913;&#30340;&#21270;&#23398;&#21453;&#24212;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#38544;&#34255;&#21333;&#20803;&#23398;&#20064;&#22797;&#26434;&#20998;&#24067;&#12290;&#36825;&#20010;&#26041;&#27861;&#21487;&#20197;&#30475;&#20316;&#26159;&#31215;&#20998;&#21453;&#39304;&#25511;&#21046;&#30340;&#19968;&#31181;&#24418;&#24335;&#65292;&#21516;&#26102;&#25105;&#20204;&#36824;&#33021;&#22815;&#25512;&#23548;&#20986;&#19982;&#23398;&#20064;&#36807;&#31243;&#30456;&#20851;&#30340;&#28909;&#21147;&#23398;&#25104;&#26412;&#21644;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2311.00975</link><description>&lt;p&gt;
&#29992;&#21270;&#23398;&#21453;&#24212;&#32593;&#32476;&#38598;&#21512;&#23454;&#29616;&#29983;&#25104;&#27169;&#22411;&#30340;&#33258;&#20027;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Autonomous Learning of Generative Models with Chemical Reaction Network Ensembles. (arXiv:2311.00975v1 [q-bio.MN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00975
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#21270;&#23398;&#21453;&#24212;&#32593;&#32476;&#23454;&#29616;&#20102;&#29983;&#25104;&#27169;&#22411;&#30340;&#33258;&#20027;&#23398;&#20064;&#65292;&#33021;&#22815;&#20248;&#21270;&#20219;&#20309;&#35814;&#32454;&#24179;&#34913;&#30340;&#21270;&#23398;&#21453;&#24212;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#38544;&#34255;&#21333;&#20803;&#23398;&#20064;&#22797;&#26434;&#20998;&#24067;&#12290;&#36825;&#20010;&#26041;&#27861;&#21487;&#20197;&#30475;&#20316;&#26159;&#31215;&#20998;&#21453;&#39304;&#25511;&#21046;&#30340;&#19968;&#31181;&#24418;&#24335;&#65292;&#21516;&#26102;&#25105;&#20204;&#36824;&#33021;&#22815;&#25512;&#23548;&#20986;&#19982;&#23398;&#20064;&#36807;&#31243;&#30456;&#20851;&#30340;&#28909;&#21147;&#23398;&#25104;&#26412;&#21644;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#39063;&#24494;&#31859;&#22823;&#23567;&#30340;&#30456;&#20114;&#20316;&#29992;&#20998;&#23376;&#22218;&#26159;&#21542;&#33021;&#22815;&#33258;&#20027;&#23398;&#20064;&#22797;&#26434;&#19988;&#27874;&#21160;&#30340;&#29615;&#22659;&#30340;&#20869;&#37096;&#27169;&#22411;&#65311;&#25105;&#20204;&#20174;&#25511;&#21046;&#29702;&#35770;&#12289;&#26426;&#22120;&#23398;&#20064;&#29702;&#35770;&#12289;&#21270;&#23398;&#21453;&#24212;&#32593;&#32476;&#29702;&#35770;&#21644;&#32479;&#35745;&#29289;&#29702;&#23398;&#20013;&#27762;&#21462;&#20102;&#32463;&#39564;&#65292;&#24320;&#21457;&#20986;&#19968;&#31181;&#36890;&#29992;&#26550;&#26500;&#65292;&#20351;&#24471;&#24191;&#27867;&#31867;&#21035;&#30340;&#21270;&#23398;&#31995;&#32479;&#33021;&#22815;&#33258;&#20027;&#23398;&#20064;&#22797;&#26434;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#26500;&#36896;&#37319;&#29992;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#20248;&#21270;&#30340;&#26680;&#24515;&#26041;&#27861;&#65306;&#30456;&#23545;&#29109;&#25439;&#22833;&#20989;&#25968;&#19978;&#30340;&#26799;&#24230;&#19979;&#38477;&#30340;&#21270;&#23398;&#23454;&#29616;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#24212;&#29992;&#36825;&#31181;&#26041;&#27861;&#26469;&#20248;&#21270;&#20219;&#20309;&#19968;&#20010;&#35814;&#32454;&#24179;&#34913;&#30340;&#21270;&#23398;&#21453;&#24212;&#32593;&#32476;&#65292;&#24182;&#19988;&#36825;&#20010;&#26500;&#36896;&#33021;&#22815;&#20351;&#29992;&#38544;&#34255;&#21333;&#20803;&#26469;&#23398;&#20064;&#22797;&#26434;&#20998;&#24067;&#12290;&#36825;&#20010;&#32467;&#26524;&#21448;&#34987;&#37325;&#26032;&#35299;&#37322;&#20026;&#31215;&#20998;&#21453;&#39304;&#25511;&#21046;&#30340;&#24418;&#24335;&#12290;&#26368;&#21518;&#65292;&#30001;&#20110;&#25105;&#20204;&#20351;&#29992;&#20102;&#26174;&#24335;&#30340;&#29289;&#29702;&#23398;&#20064;&#27169;&#22411;&#65292;&#25105;&#20204;&#33021;&#22815;&#25512;&#23548;&#20986;&#19982;&#36825;&#20010;&#36807;&#31243;&#30456;&#20851;&#30340;&#28909;&#21147;&#23398;&#25104;&#26412;&#21644;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can a micron sized sack of interacting molecules autonomously learn an internal model of a complex and fluctuating environment? We draw insights from control theory, machine learning theory, chemical reaction network theory, and statistical physics to develop a general architecture whereby a broad class of chemical systems can autonomously learn complex distributions. Our construction takes the form of a chemical implementation of machine learning's optimization workhorse: gradient descent on the relative entropy cost function. We show how this method can be applied to optimize any detailed balanced chemical reaction network and that the construction is capable of using hidden units to learn complex distributions. This result is then recast as a form of integral feedback control. Finally, due to our use of an explicit physical model of learning, we are able to derive thermodynamic costs and trade-offs associated to this process.
&lt;/p&gt;</description></item><item><title>&#25991;&#31456;&#30740;&#31350;&#20102;&#20855;&#26377;&#26377;&#38480;&#23545;&#25239;&#21160;&#20316;&#30340;&#32852;&#37030;&#32447;&#24615;&#36172;&#21338;&#26426;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;FedSupLinUCB&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;$\tilde{O}(\sqrt{d T})$&#30340;&#24635;&#36951;&#25022;&#65292;&#24182;&#19988;&#36890;&#20449;&#25104;&#26412;&#21487;&#20197;&#34987;&#25511;&#21046;&#22312;$O(d M^2 \log(d)\log(T))$&#21644;$O(\sqrt{d^3 M^3} \log(d))$&#20869;&#12290;</title><link>http://arxiv.org/abs/2311.00973</link><description>&lt;p&gt;
&#20855;&#26377;&#26377;&#38480;&#23545;&#25239;&#21160;&#20316;&#30340;&#32852;&#37030;&#32447;&#24615;&#36172;&#21338;&#26426;
&lt;/p&gt;
&lt;p&gt;
Federated Linear Bandits with Finite Adversarial Actions. (arXiv:2311.00973v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00973
&lt;/p&gt;
&lt;p&gt;
&#25991;&#31456;&#30740;&#31350;&#20102;&#20855;&#26377;&#26377;&#38480;&#23545;&#25239;&#21160;&#20316;&#30340;&#32852;&#37030;&#32447;&#24615;&#36172;&#21338;&#26426;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;FedSupLinUCB&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;$\tilde{O}(\sqrt{d T})$&#30340;&#24635;&#36951;&#25022;&#65292;&#24182;&#19988;&#36890;&#20449;&#25104;&#26412;&#21487;&#20197;&#34987;&#25511;&#21046;&#22312;$O(d M^2 \log(d)\log(T))$&#21644;$O(\sqrt{d^3 M^3} \log(d))$&#20869;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#32852;&#37030;&#32447;&#24615;&#36172;&#21338;&#26426;&#27169;&#22411;&#65292;&#20854;&#20013;$M$&#20010;&#23458;&#25143;&#31471;&#19982;&#20013;&#22830;&#26381;&#21153;&#22120;&#36890;&#20449;&#65292;&#35299;&#20915;&#20855;&#26377;&#19981;&#21516;&#23545;&#25239;&#21160;&#20316;&#38598;&#30340;&#26377;&#38480;&#23545;&#25239;&#21160;&#20316;&#38598;&#30340;&#32447;&#24615;&#19978;&#19979;&#25991;&#36172;&#21338;&#26426;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#23545;&#25239;&#24615;&#26377;&#38480;&#21160;&#20316;&#38598;&#30340;&#29420;&#29305;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FedSupLinUCB&#31639;&#27861;&#65292;&#23427;&#22312;&#32447;&#24615;&#19978;&#19979;&#25991;&#36172;&#21338;&#26426;&#20013;&#25193;&#23637;&#20102;SupLinUCB&#21644;OFUL&#31639;&#27861;&#30340;&#21407;&#21017;&#12290;&#25105;&#20204;&#35777;&#26126;FedSupLinUCB&#30340;&#24635;&#36951;&#25022;&#20026;$\tilde{O}(\sqrt{d T})$&#65292;&#20854;&#20013;$T$&#26159;&#25152;&#26377;&#23458;&#25143;&#31471;&#30340;&#24635;&#33218;&#25289;&#27425;&#25968;&#65292;$d$&#26159;&#32447;&#24615;&#27169;&#22411;&#30340;&#29615;&#22659;&#32500;&#24230;&#12290;&#36825;&#19982;&#26497;&#23567;&#20540;&#19979;&#30028;&#30456;&#21305;&#37197;&#65292;&#22240;&#27492;&#26159;&#26368;&#20248;&#30340;&#65288;&#22810;&#39033;&#24335;&#23545;&#25968;&#39033;&#65289;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#24322;&#27493;&#21644;&#21516;&#27493;&#20004;&#31181;&#24773;&#20917;&#65292;&#24182;&#23637;&#31034;&#20102;&#36890;&#20449;&#25104;&#26412;&#21487;&#20197;&#20998;&#21035;&#25511;&#21046;&#20026;$O(d M^2 \log(d) \log(T))$&#21644;$O(\sqrt{d^3 M^3} \log(d))$&#12290;FedSupLinUCB&#35774;&#35745;&#36827;&#19968;&#27493;&#25193;&#23637;&#20026;&#20004;&#31181;&#24773;&#26223;&#65306;&#65288;1&#65289;&#26041;&#24046;&#33258;&#36866;&#24212;&#65292;&#24635;&#36951;&#25022;&#20026;$\tilde{O}(\sqrt{
&lt;/p&gt;
&lt;p&gt;
We study a federated linear bandits model, where $M$ clients communicate with a central server to solve a linear contextual bandits problem with finite adversarial action sets that may be different across clients. To address the unique challenges of adversarial finite action sets, we propose the FedSupLinUCB algorithm, which extends the principles of SupLinUCB and OFUL algorithms in linear contextual bandits. We prove that FedSupLinUCB achieves a total regret of $\tilde{O}(\sqrt{d T})$, where $T$ is the total number of arm pulls from all clients, and $d$ is the ambient dimension of the linear model. This matches the minimax lower bound and thus is order-optimal (up to polylog terms). We study both asynchronous and synchronous cases and show that the communication cost can be controlled as $O(d M^2 \log(d)\log(T))$ and $O(\sqrt{d^3 M^3} \log(d))$, respectively. The FedSupLinUCB design is further extended to two scenarios: (1) variance-adaptive, where a total regret of $\tilde{O} (\sqrt{
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#24212;&#29992;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#21644;&#30417;&#30563;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#21015;&#36710;&#36816;&#34892;&#22270;&#38382;&#39064;(TTP)&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26694;&#26550;&#22312;&#25552;&#39640;TTP&#30340;&#35299;&#20915;&#25928;&#29575;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;</title><link>http://arxiv.org/abs/2311.00971</link><description>&lt;p&gt;
&#19968;&#20010;&#38598;&#25104;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#21644;&#30417;&#30563;&#23398;&#20064;&#30340;&#21015;&#36710;&#36816;&#34892;&#22270;&#38382;&#39064;&#32508;&#21512;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
An Integrated Framework Integrating Monte Carlo Tree Search and Supervised Learning for Train Timetabling Problem. (arXiv:2311.00971v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00971
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#24212;&#29992;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#21644;&#30417;&#30563;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#21015;&#36710;&#36816;&#34892;&#22270;&#38382;&#39064;(TTP)&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26694;&#26550;&#22312;&#25552;&#39640;TTP&#30340;&#35299;&#20915;&#25928;&#29575;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#32447;&#38081;&#36335;&#21015;&#36710;&#36816;&#34892;&#22270;&#38382;&#39064;(TTP)&#26159;&#19968;&#20010;&#37325;&#35201;&#19988;&#22797;&#26434;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#24212;&#29992;&#21551;&#21457;&#24335;&#26041;&#27861;&#12289;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#21644;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30340;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;(MCTS)&#35745;&#31639;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#19979;&#30340;TTP&#12290;&#26412;&#25991;&#39318;&#20808;&#25551;&#36848;&#20102;TTP&#30340;&#25968;&#23398;&#27169;&#22411;&#21644;&#20223;&#30495;&#31995;&#32479;&#21160;&#21147;&#23398;&#65292;&#24182;&#20174;MCTS&#30340;&#35282;&#24230;&#20998;&#26512;&#20102;&#35299;&#20915;&#26041;&#26696;&#30340;&#29305;&#28857;&#65292;&#25552;&#20986;&#20102;&#19968;&#20123;&#21551;&#21457;&#24335;&#26041;&#27861;&#20197;&#25913;&#36827;MCTS&#12290;&#26412;&#25991;&#23558;&#36825;&#20123;&#26041;&#27861;&#35270;&#20026;&#25152;&#25552;&#26694;&#26550;&#20013;&#30340;&#35268;&#21010;&#22120;&#12290;&#20854;&#27425;&#65292;&#26412;&#25991;&#21033;&#29992;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26469;&#36817;&#20284;&#33410;&#28857;&#30340;&#20540;&#65292;&#24182;&#36827;&#19968;&#27493;&#24212;&#29992;&#20110;MCTS&#25628;&#32034;&#36807;&#31243;&#20013;&#65292;&#31216;&#20026;&#23398;&#20064;&#22120;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#21551;&#21457;&#24335;MCTS&#26041;&#27861;&#26377;&#21161;&#20110;&#35299;&#20915;TTP&#65307;&#23558;&#35268;&#21010;&#22120;&#21644;&#23398;&#20064;&#22120;&#38598;&#25104;&#21040;&#31639;&#27861;&#26694;&#26550;&#20013;&#21487;&#20197;&#25552;&#39640;&#35299;&#20915;TTP&#30340;&#25968;&#25454;&#25928;&#29575;&#65307;&#35813;&#31639;&#27861;&#26694;&#26550;&#36824;&#21487;&#20197;&#24212;&#29992;&#20110;&#20854;&#20182;&#31867;&#20284;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The single-track railway train timetabling problem (TTP) is an important and complex problem. This article proposes an integrated Monte Carlo Tree Search (MCTS) computing framework that combines heuristic methods, unsupervised learning methods, and supervised learning methods for solving TTP in discrete action spaces. This article first describes the mathematical model and simulation system dynamics of TTP, analyzes the characteristics of the solution from the perspective of MCTS, and proposes some heuristic methods to improve MCTS. This article considers these methods as planners in the proposed framework. Secondly, this article utilizes deep convolutional neural networks to approximate the value of nodes and further applies them to the MCTS search process, referred to as learners. The experiment shows that the proposed heuristic MCTS method is beneficial for solving TTP; The algorithm framework that integrates planners and learners can improve the data efficiency of solving TTP; The 
&lt;/p&gt;</description></item><item><title>ISR&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#21487;&#35777;&#26126;&#30340;&#39046;&#22495;&#27867;&#21270;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#31867;&#26465;&#20214;&#20998;&#24067;&#30340;&#19968;&#38454;&#30697;&#26469;&#35782;&#21035;&#19981;&#21464;&#29305;&#24449;&#25152;&#24352;&#25104;&#30340;&#23376;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2311.00966</link><description>&lt;p&gt;
&#19981;&#21464;&#29305;&#24449;&#23376;&#31354;&#38388;&#24674;&#22797;&#65306;&#19968;&#31867;&#26032;&#30340;&#21487;&#35777;&#26126;&#30340;&#39046;&#22495;&#27867;&#21270;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Invariant-Feature Subspace Recovery: A New Class of Provable Domain Generalization Algorithms. (arXiv:2311.00966v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00966
&lt;/p&gt;
&lt;p&gt;
ISR&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#21487;&#35777;&#26126;&#30340;&#39046;&#22495;&#27867;&#21270;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#31867;&#26465;&#20214;&#20998;&#24067;&#30340;&#19968;&#38454;&#30697;&#26469;&#35782;&#21035;&#19981;&#21464;&#29305;&#24449;&#25152;&#24352;&#25104;&#30340;&#23376;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#27867;&#21270;&#35201;&#27714;&#22312;&#19968;&#32452;&#35757;&#32451;&#29615;&#22659;&#20013;&#35757;&#32451;&#30340;&#27169;&#22411;&#33021;&#22815;&#22312;&#26410;&#30693;&#30340;&#27979;&#35797;&#29615;&#22659;&#20013;&#33391;&#22909;&#22320;&#27867;&#21270;&#12290;&#26368;&#36817;&#65292;&#19968;&#31995;&#21015;&#31639;&#27861;&#65292;&#22914;&#19981;&#21464;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;IRM&#65289;&#65292;&#24050;&#34987;&#25552;&#20986;&#29992;&#20110;&#39046;&#22495;&#27867;&#21270;&#12290;&#28982;&#32780;&#65292;Rosenfeld&#31561;&#20154;&#65288;2021&#65289;&#34920;&#26126;&#65292;&#22312;&#19968;&#20010;&#31616;&#21333;&#30340;&#32447;&#24615;&#25968;&#25454;&#27169;&#22411;&#20013;&#65292;&#21363;&#20351;&#24573;&#30053;&#20102;&#38750;&#20984;&#24615;&#38382;&#39064;&#65292;IRM&#21450;&#20854;&#25193;&#23637;&#20063;&#26080;&#27861;&#23545;&#20855;&#26377;&#23569;&#20110;$d_s+1$&#20010;&#35757;&#32451;&#29615;&#22659;&#30340;&#26410;&#30693;&#29615;&#22659;&#36827;&#34892;&#27867;&#21270;&#65292;&#20854;&#20013;$d_s$&#26159;&#34394;&#20551;&#29305;&#24449;&#23376;&#31354;&#38388;&#30340;&#32500;&#24230;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19981;&#21464;&#29305;&#24449;&#23376;&#31354;&#38388;&#24674;&#22797;&#65288;ISR&#65289;&#65306;&#19968;&#31867;&#26032;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#20998;&#31867;&#21644;&#22238;&#24402;&#38382;&#39064;&#35774;&#32622;&#20013;&#23454;&#29616;&#21487;&#35777;&#26126;&#30340;&#39046;&#22495;&#27867;&#21270;&#12290;&#39318;&#20808;&#65292;&#22312;Rosenfeld&#31561;&#20154;&#65288;2021&#65289;&#30340;&#20108;&#20998;&#31867;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#31639;&#27861;ISR-Mean&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#31867;&#26465;&#20214;&#20998;&#24067;&#30340;&#19968;&#38454;&#30697;&#26469;&#35782;&#21035;&#19981;&#21464;&#29305;&#24449;&#25152;&#24352;&#25104;&#30340;&#23376;&#31354;&#38388;&#65292;&#24182;&#23454;&#29616;&#21487;&#35777;&#26126;&#30340;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain generalization asks for models trained over a set of training environments to generalize well in unseen test environments. Recently, a series of algorithms such as Invariant Risk Minimization (IRM) have been proposed for domain generalization. However, Rosenfeld et al. (2021) shows that in a simple linear data model, even if non-convexity issues are ignored, IRM and its extensions cannot generalize to unseen environments with less than $d_s+1$ training environments, where $d_s$ is the dimension of the spurious-feature subspace. In this work, we propose Invariant-feature Subspace Recovery (ISR): a new class of algorithms to achieve provable domain generalization across the settings of classification and regression problems. First, in the binary classification setup of Rosenfeld et al. (2021), we show that our first algorithm, ISR-Mean, can identify the subspace spanned by invariant features from the first-order moments of the class-conditional distributions, and achieve provable 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#37329;&#34701;&#31185;&#25216;&#24212;&#29992;&#20013;&#23547;&#25214;&#39640;&#36136;&#37327;&#30340;&#21452;&#30446;&#26631; Pareto &#26368;&#20248;&#27450;&#35784;&#39044;&#38450;&#35268;&#21017;&#38598;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#37319;&#29992; Pareto &#26368;&#20248;&#24615;&#27010;&#24565;&#21644;&#21551;&#21457;&#24335;&#26694;&#26550; PORS&#65292;&#25105;&#20204;&#25104;&#21151;&#25552;&#20986;&#20102;&#19968;&#32452;&#38750;&#25903;&#37197;&#30340;&#35268;&#21017;&#23376;&#38598;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#35780;&#20272;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00964</link><description>&lt;p&gt;
&#22312;&#37329;&#34701;&#31185;&#25216;&#24212;&#29992;&#20013;&#23547;&#25214;&#21452;&#30446;&#26631; Pareto &#26368;&#20248;&#27450;&#35784;&#39044;&#38450;&#35268;&#21017;&#38598;
&lt;/p&gt;
&lt;p&gt;
On Finding Bi-objective Pareto-optimal Fraud Prevention Rule Sets for Fintech Applications. (arXiv:2311.00964v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00964
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#37329;&#34701;&#31185;&#25216;&#24212;&#29992;&#20013;&#23547;&#25214;&#39640;&#36136;&#37327;&#30340;&#21452;&#30446;&#26631; Pareto &#26368;&#20248;&#27450;&#35784;&#39044;&#38450;&#35268;&#21017;&#38598;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#37319;&#29992; Pareto &#26368;&#20248;&#24615;&#27010;&#24565;&#21644;&#21551;&#21457;&#24335;&#26694;&#26550; PORS&#65292;&#25105;&#20204;&#25104;&#21151;&#25552;&#20986;&#20102;&#19968;&#32452;&#38750;&#25903;&#37197;&#30340;&#35268;&#21017;&#23376;&#38598;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#35780;&#20272;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35268;&#21017;&#22312;&#37329;&#34701;&#31185;&#25216;&#26426;&#26500;&#20013;&#34987;&#24191;&#27867;&#29992;&#20110;&#36827;&#34892;&#27450;&#35784;&#39044;&#38450;&#20915;&#31574;&#65292;&#22240;&#20026;&#35268;&#21017;&#20855;&#26377;&#30452;&#35266;&#30340; if-then &#32467;&#26500;&#65292;&#26131;&#20110;&#29702;&#35299;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#22823;&#22411;&#37329;&#34701;&#31185;&#25216;&#26426;&#26500;&#36890;&#24120;&#37319;&#29992;&#20004;&#38454;&#27573;&#27450;&#35784;&#39044;&#38450;&#20915;&#31574;&#35268;&#21017;&#38598;&#25366;&#25496;&#26694;&#26550;&#12290;&#26412;&#25991;&#20851;&#27880;&#20110;&#20174;&#21021;&#22987;&#35268;&#21017;&#38598;&#20013;&#25214;&#21040;&#39640;&#36136;&#37327;&#30340;&#35268;&#21017;&#23376;&#38598;&#65292;&#20197;&#21452;&#30446;&#26631;&#31354;&#38388;&#65288;&#22914;&#31934;&#30830;&#29575;&#21644;&#21484;&#22238;&#29575;&#65289;&#20026;&#22522;&#30784;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#37319;&#29992; Pareto &#26368;&#20248;&#24615;&#27010;&#24565;&#65292;&#26088;&#22312;&#25214;&#21040;&#19968;&#32452;&#38750;&#25903;&#37197;&#30340;&#35268;&#21017;&#23376;&#38598;&#65292;&#26500;&#25104;&#19968;&#20010; Pareto &#21069;&#27839;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21551;&#21457;&#24335;&#30340;&#26694;&#26550; PORS&#65292;&#24182;&#30830;&#23450;&#20102; PORS &#30340;&#26680;&#24515;&#38382;&#39064;&#26159;&#21069;&#27839;&#35299;&#20915;&#26041;&#26696;&#36873;&#25321;&#65288;SSF&#65289;&#38382;&#39064;&#12290;&#25105;&#20204;&#23545; SSF &#38382;&#39064;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#31867;&#65292;&#24182;&#22312;&#20844;&#24320;&#21644;&#19987;&#26377;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#35777;&#35780;&#20272;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026; SpectralRules &#30340;&#26032;&#39062;&#21464;&#20307;&#30340;&#39034;&#24207;&#35206;&#30422;&#31639;&#27861;&#65292;&#20197;&#40723;&#21169;&#35268;&#21017;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rules are widely used in Fintech institutions to make fraud prevention decisions, since rules are highly interpretable thanks to their intuitive if-then structure. In practice, a two-stage framework of fraud prevention decision rule set mining is usually employed in large Fintech institutions. This paper is concerned with finding high-quality rule subsets in a bi-objective space (such as precision and recall) from an initial pool of rules. To this end, we adopt the concept of Pareto optimality and aim to find a set of non-dominated rule subsets, which constitutes a Pareto front. We propose a heuristic-based framework called PORS and we identify that the core of PORS is the problem of solution selection on the front (SSF). We provide a systematic categorization of the SSF problem and a thorough empirical evaluation of various SSF methods on both public and proprietary datasets. We also introduce a novel variant of sequential covering algorithm called SpectralRules to encourage the diver
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#21160;&#24577;&#20844;&#24179;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;(DQFFL)&#65292;&#26088;&#22312;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#21442;&#25968;&#65292;&#20943;&#36731;&#35774;&#22791;&#32858;&#21512;&#36807;&#31243;&#20013;&#30340;&#24046;&#24322;&#65292;&#25552;&#21319;&#23545;&#25152;&#26377;&#32676;&#20307;&#30340;&#20844;&#24179;&#22788;&#29702;&#12290;</title><link>http://arxiv.org/abs/2311.00959</link><description>&lt;p&gt;
&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#21160;&#24577;&#20844;&#24179;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Dynamic Fair Federated Learning Based on Reinforcement Learning. (arXiv:2311.00959v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00959
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#21160;&#24577;&#20844;&#24179;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;(DQFFL)&#65292;&#26088;&#22312;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#21442;&#25968;&#65292;&#20943;&#36731;&#35774;&#22791;&#32858;&#21512;&#36807;&#31243;&#20013;&#30340;&#24046;&#24322;&#65292;&#25552;&#21319;&#23545;&#25152;&#26377;&#32676;&#20307;&#30340;&#20844;&#24179;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20351;&#24471;&#19968;&#32452;&#35774;&#22791;&#33021;&#22815;&#36827;&#34892;&#21327;&#20316;&#35757;&#32451;&#21644;&#20248;&#21270;&#20840;&#23616;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#20849;&#20139;&#26412;&#22320;&#25968;&#25454;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#21487;&#33021;&#23548;&#33268;&#20840;&#23616;&#27169;&#22411;&#22312;&#19981;&#21516;&#35774;&#22791;&#19978;&#30340;&#19981;&#20844;&#24179;&#34920;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20844;&#24179;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#21160;&#24577;q&#20844;&#24179;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#31216;&#20026;DQFFL&#12290;DQFFL&#26088;&#22312;&#20943;&#36731;&#35774;&#22791;&#32858;&#21512;&#20013;&#30340;&#24046;&#24322;&#65292;&#24182;&#25552;&#21319;&#23545;&#21442;&#19982;&#32852;&#37030;&#23398;&#20064;&#30340;&#25152;&#26377;&#32676;&#20307;&#30340;&#20844;&#24179;&#22788;&#29702;&#12290;&#20026;&#20102;&#37327;&#21270;&#20844;&#24179;&#24615;&#65292;DQFFL&#21033;&#29992;&#20840;&#23616;&#32852;&#37030;&#27169;&#22411;&#22312;&#27599;&#20010;&#35774;&#22791;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#23558;&#945;-&#20844;&#24179;&#24615;&#32435;&#20837;&#32858;&#21512;&#36807;&#31243;&#20013;&#23458;&#25143;&#31471;&#26435;&#37325;&#30340;&#20998;&#24067;&#65292;&#20174;&#32780;&#23454;&#29616;&#20844;&#24179;&#24615;&#30340;&#20445;&#25345;&#12290;&#32771;&#34385;&#21040;&#34913;&#37327;&#20844;&#24179;&#24615;&#30340;&#21442;&#25968;&#30340;&#25935;&#24863;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#21160;&#24577;&#35843;&#25972;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning enables a collaborative training and optimization of global models among a group of devices without sharing local data samples. However, the heterogeneity of data in federated learning can lead to unfair representation of the global model across different devices. To address the fairness issue in federated learning, we propose a dynamic q fairness federated learning algorithm with reinforcement learning, called DQFFL. DQFFL aims to mitigate the discrepancies in device aggregation and enhance the fairness of treatment for all groups involved in federated learning. To quantify fairness, DQFFL leverages the performance of the global federated model on each device and incorporates {\alpha}-fairness to transform the preservation of fairness during federated aggregation into the distribution of client weights in the aggregation process. Considering the sensitivity of parameters in measuring fairness, we propose to utilize reinforcement learning for dynamic parameters durin
&lt;/p&gt;</description></item><item><title>E3 TTS&#26159;&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#31471;&#21040;&#31471;&#22522;&#20110;&#25193;&#25955;&#30340;&#25991;&#26412;&#21040;&#35821;&#38899;&#27169;&#22411;&#65292;&#19981;&#20381;&#36182;&#20110;&#20013;&#38388;&#34920;&#31034;&#65292;&#36890;&#36807;&#25193;&#25955;&#36807;&#31243;&#24314;&#27169;&#27874;&#24418;&#30340;&#26102;&#38388;&#32467;&#26500;&#65292;&#33021;&#22815;&#36731;&#26494;&#36866;&#24212;&#38646;&#26679;&#26412;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2311.00945</link><description>&lt;p&gt;
E3 TTS: &#31616;&#21333;&#39640;&#25928;&#30340;&#31471;&#21040;&#31471;&#22522;&#20110;&#25193;&#25955;&#30340;&#25991;&#26412;&#21040;&#35821;&#38899;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
E3 TTS: Easy End-to-End Diffusion-based Text to Speech. (arXiv:2311.00945v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00945
&lt;/p&gt;
&lt;p&gt;
E3 TTS&#26159;&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#31471;&#21040;&#31471;&#22522;&#20110;&#25193;&#25955;&#30340;&#25991;&#26412;&#21040;&#35821;&#38899;&#27169;&#22411;&#65292;&#19981;&#20381;&#36182;&#20110;&#20013;&#38388;&#34920;&#31034;&#65292;&#36890;&#36807;&#25193;&#25955;&#36807;&#31243;&#24314;&#27169;&#27874;&#24418;&#30340;&#26102;&#38388;&#32467;&#26500;&#65292;&#33021;&#22815;&#36731;&#26494;&#36866;&#24212;&#38646;&#26679;&#26412;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#31471;&#21040;&#31471;&#22522;&#20110;&#25193;&#25955;&#30340;&#25991;&#26412;&#21040;&#35821;&#38899;&#27169;&#22411;&#65292;&#31216;&#20026;E3 TTS&#12290;E3 TTS&#30452;&#25509;&#25509;&#21463;&#32431;&#25991;&#26412;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#32454;&#21270;&#36807;&#31243;&#29983;&#25104;&#38899;&#39057;&#27874;&#24418;&#12290;&#19982;&#35768;&#22810;&#20808;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;E3 TTS&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#20013;&#38388;&#34920;&#31034;&#65292;&#22914;&#39057;&#35889;&#29305;&#24449;&#25110;&#23545;&#40784;&#20449;&#24687;&#12290;&#30456;&#21453;&#65292;E3 TTS&#36890;&#36807;&#25193;&#25955;&#36807;&#31243;&#24314;&#27169;&#27874;&#24418;&#30340;&#26102;&#38388;&#32467;&#26500;&#12290;&#19981;&#20381;&#36182;&#20110;&#39069;&#22806;&#30340;&#26465;&#20214;&#20449;&#24687;&#65292;E3 TTS&#21487;&#20197;&#25903;&#25345;&#32473;&#23450;&#38899;&#39057;&#20013;&#30340;&#28789;&#27963;&#28508;&#22312;&#32467;&#26500;&#12290;&#36825;&#20351;&#24471;E3 TTS&#33021;&#22815;&#36731;&#26494;&#36866;&#24212;&#38646;&#26679;&#26412;&#20219;&#21153;&#65292;&#20363;&#22914;&#22312;&#27809;&#26377;&#39069;&#22806;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#32534;&#36753;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;E3 TTS&#33021;&#22815;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#38899;&#39057;&#65292;&#25509;&#36817;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;TTS&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#38899;&#39057;&#26679;&#26412;&#21487;&#22312;https://e3tts.github.io&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Easy End-to-End Diffusion-based Text to Speech, a simple and efficient end-to-end text-to-speech model based on diffusion. E3 TTS directly takes plain text as input and generates an audio waveform through an iterative refinement process. Unlike many prior work, E3 TTS does not rely on any intermediate representations like spectrogram features or alignment information. Instead, E3 TTS models the temporal structure of the waveform through the diffusion process. Without relying on additional conditioning information, E3 TTS could support flexible latent structure within the given audio. This enables E3 TTS to be easily adapted for zero-shot tasks such as editing without any additional training. Experiments show that E3 TTS can generate high-fidelity audio, approaching the performance of a state-of-the-art neural TTS system. Audio samples are available at https://e3tts.github.io.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;FESS-GDA&#65292;&#21033;&#29992;&#24179;&#28369;&#25216;&#26415;&#36827;&#34892;&#32852;&#37030;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#12290;&#36890;&#36807;&#35299;&#20915;&#19981;&#21516;&#31867;&#22411;&#30340;&#32852;&#37030;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;FESS-GDA&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#23454;&#38469;&#32852;&#37030;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#23454;&#38469;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2311.00944</link><description>&lt;p&gt;
&#22522;&#20110;&#38543;&#26426;&#24179;&#28369;&#26799;&#24230;&#19978;&#21319;&#19979;&#38477;&#27861;&#30340;&#32852;&#37030;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Stochastic Smoothed Gradient Descent Ascent for Federated Minimax Optimization. (arXiv:2311.00944v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00944
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;FESS-GDA&#65292;&#21033;&#29992;&#24179;&#28369;&#25216;&#26415;&#36827;&#34892;&#32852;&#37030;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#12290;&#36890;&#36807;&#35299;&#20915;&#19981;&#21516;&#31867;&#22411;&#30340;&#32852;&#37030;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;FESS-GDA&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#23454;&#38469;&#32852;&#37030;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#23454;&#38469;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30001;&#20110;&#20854;&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#32852;&#37030;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#34429;&#28982;&#22312;&#38598;&#20013;&#38750;&#20984;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#20013;&#65292;&#24179;&#28369;&#20132;&#26367;&#26799;&#24230;&#19978;&#21319;&#19979;&#38477;&#65288;Smoothed-AGDA&#65289;&#24050;&#32463;&#35777;&#26126;&#20102;&#20854;&#25104;&#21151;&#20043;&#22788;&#65292;&#20294;&#24179;&#28369;&#25216;&#26415;&#22312;&#32852;&#37030;&#35774;&#32622;&#20013;&#30340;&#20316;&#29992;&#21644;&#26159;&#21542;&#26377;&#25152;&#24110;&#21161;&#23578;&#26410;&#34987;&#25506;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#31216;&#20026;&#32852;&#37030;&#38543;&#26426;&#24179;&#28369;&#26799;&#24230;&#19978;&#21319;&#19979;&#38477;&#65288;FESS-GDA&#65289;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#24179;&#28369;&#25216;&#26415;&#36827;&#34892;&#32852;&#37030;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;FESS-GDA&#21487;&#20197;&#32479;&#19968;&#35299;&#20915;&#20960;&#31867;&#32852;&#37030;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#65292;&#24182;&#20026;&#36825;&#20123;&#35774;&#32622;&#25552;&#20379;&#20102;&#26032;&#30340;&#25110;&#26356;&#22909;&#30340;&#25910;&#25947;&#32467;&#26524;&#20998;&#26512;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;FESS-GDA&#22312;&#23454;&#38469;&#32852;&#37030;&#23398;&#20064;&#20219;&#21153;&#20013;&#65292;&#22914;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#30340;&#35757;&#32451;&#21644;&#20844;&#24179;&#20998;&#31867;&#20013;&#30340;&#23454;&#38469;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, federated minimax optimization has attracted growing interest due to its extensive applications in various machine learning tasks. While Smoothed Alternative Gradient Descent Ascent (Smoothed-AGDA) has proved its success in centralized nonconvex minimax optimization, how and whether smoothing technique could be helpful in federated setting remains unexplored. In this paper, we propose a new algorithm termed Federated Stochastic Smoothed Gradient Descent Ascent (FESS-GDA), which utilizes the smoothing technique for federated minimax optimization. We prove that FESS-GDA can be uniformly used to solve several classes of federated minimax problems and prove new or better analytical convergence results for these settings. We showcase the practical efficiency of FESS-GDA in practical federated learning tasks of training generative adversarial networks (GANs) and fair classification.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#39640;&#26031;&#28151;&#21512;&#35299;&#31639;&#22120;(GMS)&#30340;&#26032;&#22411;SDE-based&#35299;&#31639;&#22120;&#29992;&#20110;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#20272;&#35745;&#21069;&#19977;&#38454;&#30697;&#24182;&#20248;&#21270;&#39640;&#26031;&#28151;&#21512;&#21442;&#25968;&#26469;&#35299;&#20915;&#29616;&#26377;SDE-based&#35299;&#31639;&#22120;&#30340;&#25928;&#29575;-&#26377;&#25928;&#24615;&#22256;&#22659;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.00941</link><description>&lt;p&gt;
&#39640;&#26031;&#28151;&#21512;&#35299;&#31639;&#22120;&#29992;&#20110;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Gaussian Mixture Solvers for Diffusion Models. (arXiv:2311.00941v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00941
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#39640;&#26031;&#28151;&#21512;&#35299;&#31639;&#22120;(GMS)&#30340;&#26032;&#22411;SDE-based&#35299;&#31639;&#22120;&#29992;&#20110;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#20272;&#35745;&#21069;&#19977;&#38454;&#30697;&#24182;&#20248;&#21270;&#39640;&#26031;&#28151;&#21512;&#21442;&#25968;&#26469;&#35299;&#20915;&#29616;&#26377;SDE-based&#35299;&#31639;&#22120;&#30340;&#25928;&#29575;-&#26377;&#25928;&#24615;&#22256;&#22659;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#20174;&#25193;&#25955;&#27169;&#22411;&#20013;&#37319;&#26679;&#31561;&#20215;&#20110;&#35299;&#20915;&#21453;&#25193;&#25955;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDEs&#65289;&#25110;&#30456;&#24212;&#30340;&#27010;&#29575;&#27969;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODEs&#65289;&#12290;&#19982;&#20043;&#30456;&#27604;&#65292;&#22522;&#20110;SDE&#30340;&#35299;&#31639;&#22120;&#21487;&#20197;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#65292;&#24182;&#36866;&#29992;&#20110;&#22522;&#20110;&#31508;&#21010;&#21512;&#25104;&#30340;&#22270;&#20687;&#36716;&#25442;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;SDE&#30340;&#35299;&#31639;&#22120;&#21463;&#21040;&#25928;&#29575;-&#26377;&#25928;&#24615;&#22256;&#22659;&#30340;&#20005;&#37325;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#34920;&#26126;&#65292;&#36825;&#26159;&#22240;&#20026;&#21453;&#21521;&#36716;&#25442;&#26680;&#20013;&#30340;&#39640;&#26031;&#20551;&#35774;&#22312;&#26377;&#38480;&#25968;&#37327;&#30340;&#31163;&#25955;&#21270;&#27493;&#39588;&#20013;&#32463;&#24120;&#34987;&#36829;&#21453;&#65288;&#21363;&#20351;&#22312;&#31616;&#21333;&#28151;&#21512;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65289;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;SDE&#30340;&#35299;&#31639;&#22120;&#31867;&#65292;&#31216;&#20026;&#39640;&#26031;&#28151;&#21512;&#35299;&#31639;&#22120;&#65288;GMS&#65289;&#29992;&#20110;&#25193;&#25955;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#35299;&#31639;&#22120;&#20272;&#35745;&#20102;&#21069;&#19977;&#38454;&#30697;&#65292;&#24182;&#20248;&#21270;&#20102;&#39640;&#26031;&#28151;&#21512;&#30340;&#21442;&#25968;
&lt;/p&gt;
&lt;p&gt;
Recently, diffusion models have achieved great success in generative tasks. Sampling from diffusion models is equivalent to solving the reverse diffusion stochastic differential equations (SDEs) or the corresponding probability flow ordinary differential equations (ODEs). In comparison, SDE-based solvers can generate samples of higher quality and are suited for image translation tasks like stroke-based synthesis. During inference, however, existing SDE-based solvers are severely constrained by the efficiency-effectiveness dilemma. Our investigation suggests that this is because the Gaussian assumption in the reverse transition kernel is frequently violated (even in the case of simple mixture data) given a limited number of discretization steps. To overcome this limitation, we introduce a novel class of SDE-based solvers called \emph{Gaussian Mixture Solvers (GMS)} for diffusion models. Our solver estimates the first three-order moments and optimizes the parameters of a Gaussian mixture
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26356;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#26356;&#22909;&#22320;&#23545;&#40784;&#20256;&#32479;&#35757;&#32451;&#26041;&#27861;&#19982;&#25193;&#25955;&#27169;&#22411;&#25152;&#26399;&#26395;&#30340;&#26465;&#20214;&#37319;&#26679;&#34892;&#20026;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#20197;&#26356;&#23569;&#30340;&#37319;&#26679;&#26102;&#38388;&#27493;&#38271;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#65292;&#24182;&#23545;&#20110;&#24341;&#23548;&#35268;&#27169;&#30340;&#36873;&#25321;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00938</link><description>&lt;p&gt;
&#32553;&#23567;&#24046;&#36317;&#65306;&#35299;&#20915;&#20998;&#31867;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#24046;&#24322;&#38382;&#39064;&#20197;&#23454;&#29616;&#26080;&#20998;&#31867;&#22120;&#24341;&#23548;
&lt;/p&gt;
&lt;p&gt;
Bridging the Gap: Addressing Discrepancies in Diffusion Model Training for Classifier-Free Guidance. (arXiv:2311.00938v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26356;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#26356;&#22909;&#22320;&#23545;&#40784;&#20256;&#32479;&#35757;&#32451;&#26041;&#27861;&#19982;&#25193;&#25955;&#27169;&#22411;&#25152;&#26399;&#26395;&#30340;&#26465;&#20214;&#37319;&#26679;&#34892;&#20026;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#20197;&#26356;&#23569;&#30340;&#37319;&#26679;&#26102;&#38388;&#27493;&#38271;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#65292;&#24182;&#23545;&#20110;&#24341;&#23548;&#35268;&#27169;&#30340;&#36873;&#25321;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#36827;&#23637;&#65292;&#20026;&#29983;&#25104;&#30340;&#23454;&#20363;&#36136;&#37327;&#35774;&#23450;&#20102;&#26032;&#30340;&#26631;&#20934;&#12290;&#26412;&#25991;&#26088;&#22312;&#24378;&#35843;&#20256;&#32479;&#35757;&#32451;&#26041;&#27861;&#19982;&#36825;&#20123;&#27169;&#22411;&#25152;&#26399;&#26395;&#30340;&#26465;&#20214;&#37319;&#26679;&#34892;&#20026;&#20043;&#38388;&#23384;&#22312;&#30340;&#24046;&#24322;&#12290;&#34429;&#28982;&#27969;&#34892;&#30340;&#26080;&#20998;&#31867;&#22120;&#24341;&#23548;&#25216;&#26415;&#25928;&#26524;&#19981;&#38169;&#65292;&#20294;&#20063;&#23384;&#22312;&#19968;&#20123;&#32570;&#38519;&#12290;&#22312;&#24341;&#23548;&#35268;&#27169;&#21442;&#25968;$w$&#21462;&#36739;&#39640;&#20540;&#26102;&#65292;&#25105;&#20204;&#32463;&#24120;&#24471;&#21040;&#20998;&#24067;&#20043;&#22806;&#30340;&#26679;&#26412;&#21644;&#27169;&#24335;&#23849;&#28291;&#65292;&#32780;&#22312;$w$&#21462;&#36739;&#20302;&#20540;&#26102;&#65292;&#21487;&#33021;&#26080;&#27861;&#33719;&#24471;&#25152;&#26399;&#26395;&#30340;&#29305;&#24322;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26356;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#26356;&#22909;&#22320;&#23558;&#35757;&#32451;&#30446;&#26631;&#19982;&#37319;&#26679;&#34892;&#20026;&#23545;&#40784;&#12290;&#22312;CIFAR-10&#19978;&#30340;FID&#20998;&#25968;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20197;&#36739;&#23569;&#30340;&#37319;&#26679;&#26102;&#38388;&#27493;&#38271;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#65292;&#24182;&#19988;&#23545;&#20110;&#24341;&#23548;&#35268;&#27169;$w$&#30340;&#36873;&#25321;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36824;&#23581;&#35797;&#20102;&#22312;&#25552;&#20986;&#30340;&#25439;&#22833;&#19978;&#23545;&#31283;&#23450;&#24615;&#25193;&#25955;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#25552;&#20379;e&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have emerged as a pivotal advancement in generative models, setting new standards to the quality of the generated instances. In the current paper we aim to underscore a discrepancy between conventional training methods and the desired conditional sampling behavior of these models. While the prevalent classifier-free guidance technique works well, it's not without flaws. At higher values for the guidance scale parameter $w$, we often get out of distribution samples and mode collapse, whereas at lower values for $w$ we may not get the desired specificity. To address these challenges, we introduce an updated loss function that better aligns training objectives with sampling behaviors. Experimental validation with FID scores on CIFAR-10 elucidates our method's ability to produce higher quality samples with fewer sampling timesteps, and be more robust to the choice of guidance scale $w$. We also experiment with fine-tuning Stable Diffusion on the proposed loss, to provide e
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#36965;&#24863;&#21644;&#20844;&#20247;&#31185;&#23398;&#25968;&#25454;&#36827;&#34892;&#40479;&#31867;&#29289;&#31181;&#20998;&#24067;&#24314;&#27169;&#65292;&#22635;&#34917;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#30693;&#35782;&#31354;&#30333;&#65292;&#20026;&#25913;&#21892;&#29983;&#29289;&#22810;&#26679;&#24615;&#30417;&#27979;&#21644;&#22797;&#26434;&#29983;&#24577;&#31995;&#32479;&#24314;&#27169;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2311.00936</link><description>&lt;p&gt;
SatBird: &#21033;&#29992;&#36965;&#24863;&#21644;&#20844;&#20247;&#31185;&#23398;&#25968;&#25454;&#36827;&#34892;&#40479;&#31867;&#29289;&#31181;&#20998;&#24067;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
SatBird: Bird Species Distribution Modeling with Remote Sensing and Citizen Science Data. (arXiv:2311.00936v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00936
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#36965;&#24863;&#21644;&#20844;&#20247;&#31185;&#23398;&#25968;&#25454;&#36827;&#34892;&#40479;&#31867;&#29289;&#31181;&#20998;&#24067;&#24314;&#27169;&#65292;&#22635;&#34917;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#30693;&#35782;&#31354;&#30333;&#65292;&#20026;&#25913;&#21892;&#29983;&#29289;&#22810;&#26679;&#24615;&#30417;&#27979;&#21644;&#22797;&#26434;&#29983;&#24577;&#31995;&#32479;&#24314;&#27169;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#22810;&#26679;&#24615;&#27491;&#20197;&#21069;&#25152;&#26410;&#26377;&#30340;&#36895;&#24230;&#19979;&#38477;&#65292;&#24433;&#21709;&#21040;&#39135;&#29289;&#12289;&#27700;&#21644;&#20154;&#31867;&#20581;&#24247;&#21644;&#31119;&#31049;&#30340;&#29983;&#24577;&#31995;&#32479;&#26381;&#21153;&#12290;&#20102;&#35299;&#29289;&#31181;&#21450;&#20854;&#26646;&#24687;&#22320;&#30340;&#20998;&#24067;&#23545;&#20110;&#20445;&#25252;&#25919;&#31574;&#35268;&#21010;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#29289;&#31181;&#20998;&#24067;&#27169;&#22411; (SDMs) &#20013;&#65292;&#20256;&#32479;&#30340;&#29983;&#24577;&#23398;&#26041;&#27861;&#36890;&#24120;&#20165;&#20851;&#27880;&#23569;&#37327;&#29289;&#31181;&#25110;&#29421;&#31364;&#30340;&#22320;&#29702;&#21306;&#22495;&#65292;&#24182;&#19988;&#23545;&#29289;&#31181;&#20998;&#24067;&#20173;&#23384;&#22312;&#37325;&#22823;&#30693;&#35782;&#31354;&#30333;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#21407;&#22240;&#26159;&#20256;&#32479;&#30417;&#27979;&#25152;&#38656;&#30340;&#24037;&#20316;&#37327;&#21644;&#19987;&#19994;&#30693;&#35782;&#38480;&#21046;&#20102;&#20256;&#32479;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#12290;&#36965;&#24863;&#25968;&#25454;&#30340;&#24191;&#27867;&#21487;&#29992;&#24615;&#20197;&#21450;&#20844;&#20247;&#31185;&#23398;&#24037;&#20855;&#22312;&#20302;&#25104;&#26412;&#25910;&#38598;&#29289;&#31181;&#35266;&#23519;&#25968;&#25454;&#26041;&#38754;&#30340;&#26085;&#30410;&#26222;&#21450;&#20026;&#25913;&#21892;&#29983;&#29289;&#22810;&#26679;&#24615;&#30417;&#27979;&#21644;&#22797;&#26434;&#29983;&#24577;&#31995;&#32479;&#24314;&#27169;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#39033;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#36890;&#36807;&#39044;&#27979;&#40479;&#31867;&#29289;&#31181;&#21040;&#20854;&#26646;&#24687;&#22320;&#30340;&#26144;&#23556;&#26469;&#23454;&#29616;&#40479;&#31867;&#29289;&#31181;&#20998;&#24067;&#30340;&#32472;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biodiversity is declining at an unprecedented rate, impacting ecosystem services necessary to ensure food, water, and human health and well-being. Understanding the distribution of species and their habitats is crucial for conservation policy planning. However, traditional methods in ecology for species distribution models (SDMs) generally focus either on narrow sets of species or narrow geographical areas and there remain significant knowledge gaps about the distribution of species. A major reason for this is the limited availability of data traditionally used, due to the prohibitive amount of effort and expertise required for traditional field monitoring. The wide availability of remote sensing data and the growing adoption of citizen science tools to collect species observations data at low cost offer an opportunity for improving biodiversity monitoring and enabling the modelling of complex ecosystems. We introduce a novel task for mapping bird species to their habitats by predictin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20174;&#19981;&#30495;&#23454;&#30340;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#32570;&#38519;&#39044;&#27979;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#34920;&#31034;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#19982;&#30495;&#23454;&#25968;&#25454;&#38598;&#26368;&#30456;&#20284;&#30340;&#23376;&#38598;&#12290;</title><link>http://arxiv.org/abs/2311.00931</link><description>&lt;p&gt;
&#20174;&#19981;&#30495;&#23454;&#25968;&#25454;&#20013;&#23398;&#20064;&#32570;&#38519;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Learning Defect Prediction from Unrealistic Data. (arXiv:2311.00931v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00931
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20174;&#19981;&#30495;&#23454;&#30340;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#32570;&#38519;&#39044;&#27979;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#34920;&#31034;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#19982;&#30495;&#23454;&#25968;&#25454;&#38598;&#26368;&#30456;&#20284;&#30340;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#20195;&#30721;&#27169;&#22411;&#65292;&#22914;CodeBERT&#21644;CodeT5&#65292;&#25104;&#20026;&#20195;&#30721;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#30340;&#27969;&#34892;&#36873;&#25321;&#12290;&#36825;&#20123;&#27169;&#22411;&#24448;&#24448;&#24222;&#22823;&#19988;&#38656;&#35201;&#30456;&#24212;&#25968;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20294;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#24456;&#23569;&#25552;&#20379;&#12290;&#30456;&#21453;&#65292;&#20351;&#29992;&#36828;&#27604;&#30495;&#23454;&#25968;&#25454;&#38598;&#26356;&#22823;&#20294;&#19981;&#30495;&#23454;&#30340;&#25968;&#25454;&#38598;&#65288;&#22914;&#20154;&#20026;&#27880;&#20837;&#32570;&#38519;&#30340;&#20989;&#25968;&#65289;&#26469;&#35757;&#32451;&#27169;&#22411;&#24050;&#32463;&#21464;&#24471;&#27969;&#34892;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#27492;&#31867;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#24448;&#24448;&#21482;&#22312;&#31867;&#20284;&#25968;&#25454;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#31243;&#24207;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#20551;&#35774;&#36825;&#31181;&#24046;&#24322;&#26159;&#30001;&#20110;&#23384;&#22312;&#24178;&#25200;&#26679;&#26412;&#65292;&#36825;&#20123;&#26679;&#26412;&#20351;&#27169;&#22411;&#20559;&#31163;&#20102;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#20998;&#24067;&#12290;&#20026;&#20102;&#39564;&#35777;&#36825;&#20010;&#20551;&#35774;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#36825;&#20123;&#22823;&#32780;&#19981;&#30495;&#23454;&#30340;&#25968;&#25454;&#38598;&#20013;&#19982;&#30495;&#23454;&#25968;&#25454;&#38598;&#20013;&#30340;&#31034;&#20363;&#26368;&#30456;&#20284;&#30340;&#23376;&#38598;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#21462;&#20102;&#30495;&#23454;&#19990;&#30028;&#21644;&#20154;&#24037;&#31243;&#24207;&#30340;&#39640;&#32500;&#23884;&#20837;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained models of code, such as CodeBERT and CodeT5, have become popular choices for code understanding and generation tasks. Such models tend to be large and require commensurate volumes of training data, which are rarely available for downstream tasks. Instead, it has become popular to train models with far larger but less realistic datasets, such as functions with artificially injected bugs. Models trained on such data, however, tend to only perform well on similar data, while underperforming on real world programs. In this paper, we conjecture that this discrepancy stems from the presence of distracting samples that steer the model away from the real-world task distribution. To investigate this conjecture, we propose an approach for identifying the subsets of these large yet unrealistic datasets that are most similar to examples in real-world datasets based on their learned representations. Our approach extracts high-dimensional embeddings of both real-world and artificial progr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22810;&#21464;&#37327;&#22240;&#26524;&#27169;&#22411;&#20013;&#20272;&#35745;&#22810;&#20010;&#24863;&#20852;&#36259;&#37327;&#30340;&#21453;&#20107;&#23454;&#32852;&#21512;&#20998;&#24067;&#12290;&#36890;&#36807;&#21033;&#29992;&#21407;&#22987;&#39640;&#32500;&#31354;&#38388;&#20013;&#30340;&#19968;&#32500;&#28508;&#22312;&#23376;&#31354;&#38388;&#21644;&#21333;&#19968;&#21464;&#37327;&#22240;&#26524;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#21516;&#26102;&#22788;&#29702;&#22810;&#21464;&#37327;&#32467;&#26524;&#30340;&#30456;&#20851;&#32467;&#26500;&#24182;&#20135;&#29983;&#20934;&#30830;&#30340;&#21453;&#20107;&#23454;&#20998;&#24067;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2311.00927</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#22810;&#21464;&#37327;&#22240;&#26524;&#27169;&#22411;&#20013;&#30340;&#21453;&#20107;&#23454;&#20998;&#24067;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Scalable Counterfactual Distribution Estimation in Multivariate Causal Models. (arXiv:2311.00927v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00927
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22810;&#21464;&#37327;&#22240;&#26524;&#27169;&#22411;&#20013;&#20272;&#35745;&#22810;&#20010;&#24863;&#20852;&#36259;&#37327;&#30340;&#21453;&#20107;&#23454;&#32852;&#21512;&#20998;&#24067;&#12290;&#36890;&#36807;&#21033;&#29992;&#21407;&#22987;&#39640;&#32500;&#31354;&#38388;&#20013;&#30340;&#19968;&#32500;&#28508;&#22312;&#23376;&#31354;&#38388;&#21644;&#21333;&#19968;&#21464;&#37327;&#22240;&#26524;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#21516;&#26102;&#22788;&#29702;&#22810;&#21464;&#37327;&#32467;&#26524;&#30340;&#30456;&#20851;&#32467;&#26500;&#24182;&#20135;&#29983;&#20934;&#30830;&#30340;&#21453;&#20107;&#23454;&#20998;&#24067;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#22312;&#32463;&#20856;&#30340;&#24046;&#24322;&#24046;&#24322;&#35774;&#35745;&#30340;&#22522;&#30784;&#19978;&#25193;&#23637;&#30340;&#22810;&#21464;&#37327;&#22240;&#26524;&#27169;&#22411;&#20013;&#20272;&#35745;&#22810;&#20010;&#24863;&#20852;&#36259;&#37327;&#65288;&#20363;&#22914;&#32467;&#26524;&#65289;&#30340;&#21453;&#20107;&#23454;&#32852;&#21512;&#20998;&#24067;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#35201;&#20040;&#24573;&#30053;&#22810;&#21464;&#37327;&#32467;&#26524;&#21508;&#32500;&#24230;&#38388;&#30340;&#30456;&#20851;&#32467;&#26500;&#65292;&#36890;&#36807;&#22312;&#27599;&#20010;&#32500;&#24230;&#19978;&#32771;&#34385;&#21333;&#19968;&#21464;&#37327;&#22240;&#26524;&#27169;&#22411;&#32780;&#20135;&#29983;&#38169;&#35823;&#30340;&#21453;&#20107;&#23454;&#20998;&#24067;&#65307;&#35201;&#20040;&#22312;&#30452;&#25509;&#22788;&#29702;&#36825;&#31181;&#22810;&#21464;&#37327;&#22240;&#26524;&#27169;&#22411;&#26102;&#65292;&#22312;&#20013;&#31561;&#22823;&#23567;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#21516;&#26102;&#20943;&#36731;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#26041;&#27861;&#26159;&#21033;&#29992;&#21407;&#22987;&#39640;&#32500;&#31354;&#38388;&#20013;&#40065;&#26834;&#30340;&#19968;&#32500;&#28508;&#22312;&#23376;&#31354;&#38388;&#65292;&#24182;&#21033;&#29992;&#21333;&#19968;&#21464;&#37327;&#22240;&#26524;&#27169;&#22411;&#22312;&#35813;&#31354;&#38388;&#19978;&#30340;&#39640;&#25928;&#20272;&#35745;&#12290;&#30001;&#20110;&#19968;&#32500;&#23376;&#31354;&#38388;&#30340;&#26500;&#24314;&#20351;&#29992;&#20102;&#26469;&#33258;&#25152;&#26377;&#32500;&#24230;&#30340;&#20449;&#24687;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25429;&#25417;&#30456;&#20851;&#32467;&#26500;&#24182;&#20135;&#29983;&#21453;&#20107;&#23454;&#20998;&#24067;&#30340;&#33391;&#22909;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of estimating the counterfactual joint distribution of multiple quantities of interests (e.g., outcomes) in a multivariate causal model extended from the classical difference-in-difference design. Existing methods for this task either ignore the correlation structures among dimensions of the multivariate outcome by considering univariate causal models on each dimension separately and hence produce incorrect counterfactual distributions, or poorly scale even for moderate-size datasets when directly dealing with such multivariate causal model. We propose a method that alleviates both issues simultaneously by leveraging a robust latent one-dimensional subspace of the original high-dimension space and exploiting the efficient estimation from the univariate causal model on such space. Since the construction of the one-dimensional subspace uses information from all the dimensions, our method can capture the correlation structures and produce good estimates of the coun
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32508;&#36848;&#20102;&#28145;&#24230;&#22240;&#26524;&#27169;&#22411;&#22312;&#19981;&#21516;&#22240;&#26524;&#32467;&#26500;&#21644;&#34920;&#31034;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#26126;&#30830;&#25968;&#25454;&#12289;&#21322;&#26126;&#30830;&#25968;&#25454;&#21644;&#19981;&#26126;&#30830;&#25968;&#25454;&#19977;&#31181;&#22240;&#26524;&#25968;&#25454;&#33539;&#20363;&#30340;&#23450;&#20041;&#65292;&#20197;&#24212;&#23545;&#23558;&#21407;&#22987;&#30340;&#22240;&#26524;&#27010;&#24565;&#21644;&#29702;&#35770;&#25299;&#23637;&#21040;&#22797;&#26434;&#38750;&#32479;&#35745;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2311.00923</link><description>&lt;p&gt;
&#28145;&#24230;&#22240;&#26524;&#27169;&#22411;&#30340;&#19981;&#21516;&#22240;&#26524;&#32467;&#26500;&#21644;&#34920;&#31034;&#30340;&#32508;&#36848;&#21644;&#36335;&#32447;&#22270;
&lt;/p&gt;
&lt;p&gt;
A Review and Roadmap of Deep Causal Model from Different Causal Structures and Representations. (arXiv:2311.00923v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32508;&#36848;&#20102;&#28145;&#24230;&#22240;&#26524;&#27169;&#22411;&#22312;&#19981;&#21516;&#22240;&#26524;&#32467;&#26500;&#21644;&#34920;&#31034;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#26126;&#30830;&#25968;&#25454;&#12289;&#21322;&#26126;&#30830;&#25968;&#25454;&#21644;&#19981;&#26126;&#30830;&#25968;&#25454;&#19977;&#31181;&#22240;&#26524;&#25968;&#25454;&#33539;&#20363;&#30340;&#23450;&#20041;&#65292;&#20197;&#24212;&#23545;&#23558;&#21407;&#22987;&#30340;&#22240;&#26524;&#27010;&#24565;&#21644;&#29702;&#35770;&#25299;&#23637;&#21040;&#22797;&#26434;&#38750;&#32479;&#35745;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22240;&#26524;&#27169;&#22411;&#19982;&#28145;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#24341;&#20837;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#25968;&#25454;&#38598;&#65292;&#20363;&#22914;&#22270;&#20687;&#20869;&#37096;&#30340;&#22240;&#26524;&#20851;&#32852;&#25110;&#25991;&#26412;&#32452;&#20214;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#32852;&#65292;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#23558;&#21407;&#22987;&#30340;&#22240;&#26524;&#27010;&#24565;&#21644;&#29702;&#35770;&#25299;&#23637;&#21040;&#36825;&#31181;&#22797;&#26434;&#30340;&#38750;&#32479;&#35745;&#25968;&#25454;&#20013;&#36935;&#21040;&#20102;&#20005;&#37325;&#30340;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#20174;&#22240;&#26524;&#32467;&#26500;&#21644;&#34920;&#31034;&#30340;&#35282;&#24230;&#23545;&#22240;&#26524;&#25968;&#25454;&#36827;&#34892;&#37325;&#26032;&#23450;&#20041;&#65292;&#21253;&#25324;&#26126;&#30830;&#25968;&#25454;&#12289;&#21322;&#26126;&#30830;&#25968;&#25454;&#21644;&#19981;&#26126;&#30830;&#25968;&#25454;&#12290;&#26126;&#30830;&#25968;&#25454;&#20027;&#35201;&#28041;&#21450;&#20256;&#32479;&#22240;&#26524;&#22330;&#26223;&#20013;&#20351;&#29992;&#30340;&#32479;&#35745;&#25968;&#25454;&#65292;&#32780;&#21322;&#26126;&#30830;&#25968;&#25454;&#28041;&#21450;&#19982;&#28145;&#24230;&#23398;&#20064;&#30456;&#20851;&#30340;&#19968;&#31995;&#21015;&#25968;&#25454;&#26684;&#24335;&#65292;&#21253;&#25324;&#26102;&#38388;&#24207;&#21015;&#12289;&#22270;&#20687;&#12289;&#25991;&#26412;&#31561;&#12290;&#19981;&#26126;&#30830;&#25968;&#25454;&#26159;&#25105;&#20204;&#25512;&#26029;&#30340;&#19968;&#20010;&#26032;&#20852;&#30740;&#31350;&#39046;&#22495;&#65292;&#20854;&#26469;&#28304;&#26159;&#25968;&#25454;&#24418;&#24335;&#30340;&#28436;&#21464;&#12290;&#20026;&#20102;&#20840;&#38754;&#20171;&#32461;&#36825;&#19977;&#31181;&#25968;&#25454;&#33539;&#20363;&#65292;&#25105;&#20204;&#35814;&#32454;&#38416;&#36848;&#20102;&#23427;&#20204;&#30340;&#24418;&#24335;&#23450;&#20041;&#21644;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
The fusion of causal models with deep learning introducing increasingly intricate data sets, such as the causal associations within images or between textual components, has surfaced as a focal research area. Nonetheless, the broadening of original causal concepts and theories to such complex, non-statistical data has been met with serious challenges. In response, our study proposes redefinitions of causal data into three distinct categories from the standpoint of causal structure and representation: definite data, semi-definite data, and indefinite data. Definite data chiefly pertains to statistical data used in conventional causal scenarios, while semi-definite data refers to a spectrum of data formats germane to deep learning, including time-series, images, text, and others. Indefinite data is an emergent research sphere inferred from the progression of data forms by us. To comprehensively present these three data paradigms, we elaborate on their formal definitions, differences mani
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25104;&#21592;&#19981;&#21464;&#23376;&#31354;&#38388;&#35757;&#32451;&#30340;MIST&#31639;&#27861;&#26377;&#25928;&#38450;&#24481;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#65292;&#33021;&#22815;&#35782;&#21035;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#30340;&#23454;&#20363;&#24182;&#36991;&#20813;&#36807;&#24230;&#25311;&#21512;&#12290;</title><link>http://arxiv.org/abs/2311.00919</link><description>&lt;p&gt;
MIST: &#36890;&#36807;&#25104;&#21592;&#19981;&#21464;&#23376;&#31354;&#38388;&#35757;&#32451;&#23545;&#25239;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
MIST: Defending Against Membership Inference Attacks Through Membership-Invariant Subspace Training. (arXiv:2311.00919v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00919
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25104;&#21592;&#19981;&#21464;&#23376;&#31354;&#38388;&#35757;&#32451;&#30340;MIST&#31639;&#27861;&#26377;&#25928;&#38450;&#24481;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#65292;&#33021;&#22815;&#35782;&#21035;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#30340;&#23454;&#20363;&#24182;&#36991;&#20813;&#36807;&#24230;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25104;&#21592;&#25512;&#29702;&#65288;MI&#65289;&#25915;&#20987;&#20013;&#65292;&#23545;&#25163;&#35797;&#22270;&#30830;&#23450;&#19968;&#20010;&#23454;&#20363;&#26159;&#21542;&#34987;&#29992;&#26469;&#35757;&#32451;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#12290;MI&#25915;&#20987;&#26159;&#22312;&#20351;&#29992;&#31169;&#26377;&#25968;&#25454;&#35757;&#32451;ML&#27169;&#22411;&#26102;&#30340;&#19968;&#20010;&#20027;&#35201;&#38544;&#31169;&#38382;&#39064;&#12290;&#25991;&#29486;&#20013;&#30340;&#22823;&#22810;&#25968;MI&#25915;&#20987;&#21033;&#29992;&#20102;ML&#27169;&#22411;&#34987;&#35757;&#32451;&#24471;&#24456;&#22909;&#20197;&#36866;&#24212;&#35757;&#32451;&#25968;&#25454;&#30340;&#29305;&#28857;&#65292;&#22240;&#27492;&#22312;&#35757;&#32451;&#23454;&#20363;&#19978;&#20855;&#26377;&#38750;&#24120;&#20302;&#30340;&#25439;&#22833;&#12290;&#22240;&#27492;&#65292;&#22823;&#22810;&#25968;&#23545;&#25239;MI&#25915;&#20987;&#30340;&#38450;&#24481;&#26041;&#27861;&#35797;&#22270;&#20351;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#19978;&#30340;&#25311;&#21512;&#31243;&#24230;&#38477;&#20302;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#20570;&#36890;&#24120;&#20250;&#23548;&#33268;&#36739;&#20302;&#30340;&#20934;&#30830;&#29575;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#35757;&#32451;&#23454;&#20363;&#23545;MI&#25915;&#20987;&#20855;&#26377;&#19981;&#21516;&#31243;&#24230;&#30340;&#33030;&#24369;&#24615;&#12290;&#22823;&#22810;&#25968;&#23454;&#20363;&#21363;&#20351;&#19981;&#21253;&#21547;&#22312;&#35757;&#32451;&#20013;&#20063;&#20250;&#26377;&#20302;&#30340;&#25439;&#22833;&#12290;&#23545;&#20110;&#36825;&#20123;&#23454;&#20363;&#65292;&#27169;&#22411;&#21487;&#20197;&#24456;&#22909;&#22320;&#36866;&#24212;&#23427;&#20204;&#32780;&#19981;&#29992;&#25285;&#24515;MI&#25915;&#20987;&#12290;&#26377;&#25928;&#30340;&#38450;&#24481;&#21482;&#38656;&#35201;&#65288;&#21487;&#33021;&#26159;&#38544;&#24335;&#22320;&#65289;&#35782;&#21035;&#20986;&#23481;&#26131;&#21463;&#21040;MI&#25915;&#20987;&#30340;&#23454;&#20363;&#65292;&#24182;&#36991;&#20813;&#36807;&#24230;&#25311;&#21512;&#12290;&#19968;&#20010;&#20027;&#35201;&#30340;&#25361;&#25112;&#26159;&#22914;&#20309;&#22312;&#39640;&#25928;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#23454;&#29616;&#36825;&#26679;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Member Inference (MI) attacks, the adversary try to determine whether an instance is used to train a machine learning (ML) model. MI attacks are a major privacy concern when using private data to train ML models. Most MI attacks in the literature take advantage of the fact that ML models are trained to fit the training data well, and thus have very low loss on training instances. Most defenses against MI attacks therefore try to make the model fit the training data less well. Doing so, however, generally results in lower accuracy. We observe that training instances have different degrees of vulnerability to MI attacks. Most instances will have low loss even when not included in training. For these instances, the model can fit them well without concerns of MI attacks. An effective defense only needs to (possibly implicitly) identify instances that are vulnerable to MI attacks and avoids overfitting them. A major challenge is how to achieve such an effect in an efficient training proc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#21152;&#26435;&#30340;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#20026;&#27599;&#20010;&#26631;&#35760;&#20998;&#37197;&#21160;&#24577;&#24179;&#28369;&#30340;&#26435;&#37325;&#65292;&#35299;&#20915;&#20102;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20013;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2311.00906</link><description>&lt;p&gt;
&#37325;&#26032;&#21152;&#26435;&#26631;&#35760;&#65306;&#19968;&#31181;&#31616;&#21333;&#19988;&#26377;&#25928;&#30340;&#36866;&#29992;&#20110;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Re-weighting Tokens: A Simple and Effective Active Learning Strategy for Named Entity Recognition. (arXiv:2311.00906v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00906
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#21152;&#26435;&#30340;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#20026;&#27599;&#20010;&#26631;&#35760;&#20998;&#37197;&#21160;&#24577;&#24179;&#28369;&#30340;&#26435;&#37325;&#65292;&#35299;&#20915;&#20102;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20013;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#23398;&#20064;&#26159;&#19968;&#31181;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#25991;&#26412;&#21644;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#36890;&#36807;&#26377;&#38480;&#30340;&#27880;&#37322;&#36164;&#28304;&#25552;&#21319;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#39046;&#22495;&#65292;&#20027;&#21160;&#23398;&#20064;&#30456;&#23545;&#36739;&#23569;&#34987;&#20851;&#27880;&#12290;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20013;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#22952;&#30861;&#20102;&#20027;&#21160;&#23398;&#20064;&#30340;&#25928;&#26524;&#65292;&#22240;&#20026;&#24207;&#21015;&#26631;&#35760;&#27169;&#22411;&#32570;&#20047;&#36275;&#22815;&#30340;&#23398;&#20064;&#20449;&#21495;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37325;&#26032;&#21152;&#26435;&#30340;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#65292;&#20026;&#27599;&#20010;&#26631;&#35760;&#20998;&#37197;&#20102;&#21160;&#24577;&#24179;&#28369;&#30340;&#26435;&#37325;&#12290;&#36825;&#31181;&#36866;&#24212;&#24615;&#31574;&#30053;&#19982;&#21508;&#31181;&#26631;&#35760;&#32423;&#37319;&#38598;&#20989;&#25968;&#20860;&#23481;&#65292;&#24182;&#26377;&#21161;&#20110;&#24320;&#21457;&#20581;&#22766;&#30340;&#20027;&#21160;&#23398;&#20064;&#22120;&#12290;&#22312;&#22810;&#20010;&#35821;&#26009;&#24211;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#25105;&#20204;&#30340;&#37325;&#26032;&#21152;&#26435;&#31574;&#30053;&#19982;&#29616;&#26377;&#30340;&#37319;&#38598;&#20989;&#25968;&#32467;&#21512;&#36215;&#26469;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#65292;&#39564;&#35777;&#20102;&#20854;&#23454;&#38469;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Active learning, a widely adopted technique for enhancing machine learning models in text and image classification tasks with limited annotation resources, has received relatively little attention in the domain of Named Entity Recognition (NER). The challenge of data imbalance in NER has hindered the effectiveness of active learning, as sequence labellers lack sufficient learning signals. To address these challenges, this paper presents a novel reweighting-based active learning strategy that assigns dynamic smoothed weights to individual tokens. This adaptable strategy is compatible with various token-level acquisition functions and contributes to the development of robust active learners. Experimental results on multiple corpora demonstrate the substantial performance improvement achieved by incorporating our re-weighting strategy into existing acquisition functions, validating its practical efficacy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#39640;&#26031;&#36807;&#31243;&#19982;&#20302;&#32500;&#20132;&#20114;&#32467;&#26500;&#30456;&#32467;&#21512;&#30340;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#20108;&#38454;&#31890;&#23376;&#21160;&#21147;&#23398;&#27169;&#22411;&#36873;&#25321;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#30456;&#20114;&#20316;&#29992;&#20195;&#29702;&#30340;&#32858;&#21512;&#21644;&#38598;&#20307;&#34892;&#20026;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2311.00902</link><description>&lt;p&gt;
&#36890;&#36807;&#23558;&#39640;&#26031;&#36807;&#31243;&#19982;&#20302;&#32500;&#20132;&#20114;&#32467;&#26500;&#30456;&#32467;&#21512;&#65292;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#36873;&#25321;&#20108;&#38454;&#31890;&#23376;&#21160;&#21147;&#23398;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Data-Driven Model Selections of Second-Order Particle Dynamics via Integrating Gaussian Processes with Low-Dimensional Interacting Structures. (arXiv:2311.00902v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00902
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#39640;&#26031;&#36807;&#31243;&#19982;&#20302;&#32500;&#20132;&#20114;&#32467;&#26500;&#30456;&#32467;&#21512;&#30340;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#20108;&#38454;&#31890;&#23376;&#21160;&#21147;&#23398;&#27169;&#22411;&#36873;&#25321;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#30456;&#20114;&#20316;&#29992;&#20195;&#29702;&#30340;&#32858;&#21512;&#21644;&#38598;&#20307;&#34892;&#20026;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#36890;&#29992;&#20108;&#38454;&#31890;&#23376;&#27169;&#22411;&#30340;&#21457;&#29616;&#65292;&#35813;&#27169;&#22411;&#21253;&#21547;&#20102;&#35768;&#22810;&#29992;&#20110;&#24314;&#27169;&#30456;&#20284;&#22823;&#23567;&#21644;&#20307;&#22411;&#30340;&#30456;&#20114;&#20316;&#29992;&#20195;&#29702;&#30340;&#32858;&#21512;&#21644;&#38598;&#20307;&#34892;&#20026;&#30340;&#26368;&#26032;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#20102;&#30001;&#20004;&#20010;&#30456;&#20114;&#20316;&#29992;&#26680;&#21442;&#25968;&#21270;&#30340;&#39640;&#32500;&#24120;&#24494;&#20998;&#26041;&#31243;&#31995;&#32479;&#30340;&#24418;&#24335;&#65292;&#36825;&#20123;&#26680;&#35780;&#20272;&#20102;&#20301;&#32622;&#21644;&#36895;&#24230;&#30340;&#23545;&#40784;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#26410;&#30693;&#30340;&#27169;&#22411;&#21442;&#25968;&#36890;&#36807;&#20351;&#29992;&#20004;&#20010;&#29420;&#31435;&#30340;&#39640;&#26031;&#36807;&#31243;&#20808;&#39564;&#36827;&#34892;&#36793;&#32536;&#21270;&#65292;&#36825;&#20123;&#20808;&#39564;&#32422;&#26463;&#22312;&#21160;&#21147;&#23398;&#21644;&#35266;&#27979;&#25968;&#25454;&#19978;&#12290;&#36825;&#23548;&#33268;&#19968;&#20010;&#38750;&#21442;&#25968;&#27169;&#22411;&#65292;&#29992;&#20110;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#30456;&#20114;&#20316;&#29992;&#21160;&#21147;&#23398;&#31995;&#32479;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#21152;&#36895;&#25216;&#26415;&#26469;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#26469;&#35299;&#37322;&#26041;&#27861;&#35770;&#24182;&#30740;&#31350;&#26680;&#21487;&#20197;&#28385;&#36275;&#30340;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we focus on the data-driven discovery of a general second-order particle-based model that contains many state-of-the-art models for modeling the aggregation and collective behavior of interacting agents of similar size and body type. This model takes the form of a high-dimensional system of ordinary differential equations parameterized by two interaction kernels that appraise the alignment of positions and velocities. We propose a Gaussian Process-based approach to this problem, where the unknown model parameters are marginalized by using two independent Gaussian Process (GP) priors on latent interaction kernels constrained to dynamics and observational data. This results in a nonparametric model for interacting dynamical systems that accounts for uncertainty quantification. We also develop acceleration techniques to improve scalability. Moreover, we perform a theoretical analysis to interpret the methodology and investigate the conditions under which the kernels can be 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;COSTAR&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#25913;&#36827;&#20102;&#26102;&#38388;&#21453;&#20107;&#23454;&#32467;&#26524;&#30340;&#20272;&#35745;&#12290;&#35813;&#26041;&#27861;&#22312;&#22788;&#29702;&#26102;&#38388;&#30456;&#20851;&#28151;&#28102;&#22240;&#32032;&#26102;&#32467;&#21512;&#20102;&#26102;&#38388;&#21644;&#29305;&#24449;&#20851;&#27880;&#20197;&#21450;&#20998;&#37327;&#23545;&#27604;&#25439;&#22833;&#65292;&#30456;&#27604;&#29616;&#26377;&#27169;&#22411;&#22312;&#20272;&#35745;&#20934;&#30830;&#24615;&#21644;&#23545;&#20998;&#24067;&#20043;&#22806;&#25968;&#25454;&#30340;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.00886</link><description>&lt;p&gt;
COSTAR: &#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#25913;&#36827;&#30340;&#26102;&#38388;&#21453;&#20107;&#23454;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
COSTAR: Improved Temporal Counterfactual Estimation with Self-Supervised Learning. (arXiv:2311.00886v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00886
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;COSTAR&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#25913;&#36827;&#20102;&#26102;&#38388;&#21453;&#20107;&#23454;&#32467;&#26524;&#30340;&#20272;&#35745;&#12290;&#35813;&#26041;&#27861;&#22312;&#22788;&#29702;&#26102;&#38388;&#30456;&#20851;&#28151;&#28102;&#22240;&#32032;&#26102;&#32467;&#21512;&#20102;&#26102;&#38388;&#21644;&#29305;&#24449;&#20851;&#27880;&#20197;&#21450;&#20998;&#37327;&#23545;&#27604;&#25439;&#22833;&#65292;&#30456;&#27604;&#29616;&#26377;&#27169;&#22411;&#22312;&#20272;&#35745;&#20934;&#30830;&#24615;&#21644;&#23545;&#20998;&#24067;&#20043;&#22806;&#25968;&#25454;&#30340;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#39046;&#22495;&#65292;&#22914;&#21307;&#30103;&#20445;&#20581;&#21644;&#30005;&#23376;&#21830;&#21153;&#65292;&#22312;&#35266;&#23519;&#21040;&#30340;&#21382;&#21490;&#25968;&#25454;&#20013;&#20272;&#35745;&#26102;&#38388;&#21453;&#20107;&#23454;&#32467;&#26524;&#23545;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;(RCTs)&#25104;&#26412;&#39640;&#25110;&#32773;&#19981;&#21487;&#34892;&#30340;&#24773;&#20917;&#19979;&#12290;&#23545;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#65292;&#30001;&#20110;&#22797;&#26434;&#30340;&#21160;&#24577;&#65292;&#38271;&#33539;&#22260;&#20381;&#36182;&#24615;&#20197;&#21450;&#36807;&#21435;&#30340;&#27835;&#30103;&#21644;&#21327;&#21464;&#37327;&#23545;&#26410;&#26469;&#32467;&#26524;&#30340;&#24433;&#21709;&#65292;&#24314;&#27169;&#26102;&#38388;&#30456;&#20851;&#30340;&#28151;&#28102;&#22240;&#32032;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;COunterfactual Self-supervised TrAnsformeR&#65288;COSTAR&#65289;&#65292;&#19968;&#31181;&#25972;&#21512;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#26469;&#25913;&#36827;&#21382;&#21490;&#34920;&#31034;&#30340;&#26032;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#23558;&#26102;&#38388;&#21644;&#29305;&#24449;&#20851;&#27880;&#19982;&#38024;&#23545;&#26102;&#38388;&#22788;&#29702;&#32467;&#26524;&#35266;&#23519;&#30340;&#20998;&#37327;&#23545;&#27604;&#25439;&#22833;&#30456;&#32467;&#21512;&#65292;&#19982;&#29616;&#26377;&#27169;&#22411;&#30456;&#27604;&#65292;&#22312;&#20272;&#35745;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#21040;&#20998;&#24067;&#20043;&#22806;&#25968;&#25454;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#65292;&#36890;&#36807;&#23545;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#23454;&#35777;&#32467;&#26524;&#39564;&#35777;&#35813;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimation of temporal counterfactual outcomes from observed history is crucial for decision-making in many domains such as healthcare and e-commerce, particularly when randomized controlled trials (RCTs) suffer from high cost or impracticality. For real-world datasets, modeling time-dependent confounders is challenging due to complex dynamics, long-range dependencies and both past treatments and covariates affecting the future outcomes. In this paper, we introduce COunterfactual Self-supervised TrAnsformeR (COSTAR), a novel approach that integrates self-supervised learning for improved historical representations. The proposed framework combines temporal and feature-wise attention with a component-wise contrastive loss tailored for temporal treatment outcome observations, yielding superior performance in estimation accuracy and generalization to out-of-distribution data compared to existing models, as validated by empirical results on both synthetic and real-world datasets.
&lt;/p&gt;</description></item><item><title>SCPO&#26159;&#19968;&#31181;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#23433;&#20840;&#25209;&#21028;&#22120;&#26469;&#30830;&#20445;&#36981;&#23432;&#23433;&#20840;&#32422;&#26463;&#24182;&#24179;&#34913;&#22238;&#25253;&#30340;&#26368;&#22823;&#21270;&#12290;</title><link>http://arxiv.org/abs/2311.00880</link><description>&lt;p&gt;
SCPO: &#23433;&#20840;&#25209;&#21028;&#31574;&#30053;&#20248;&#21270;&#19979;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SCPO: Safe Reinforcement Learning with Safety Critic Policy Optimization. (arXiv:2311.00880v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00880
&lt;/p&gt;
&lt;p&gt;
SCPO&#26159;&#19968;&#31181;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#23433;&#20840;&#25209;&#21028;&#22120;&#26469;&#30830;&#20445;&#36981;&#23432;&#23433;&#20840;&#32422;&#26463;&#24182;&#24179;&#34913;&#22238;&#25253;&#30340;&#26368;&#22823;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#23433;&#20840;&#24615;&#32435;&#20837;&#24378;&#21270;&#23398;&#20064;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;CMDPs&#65289;&#65292;&#24341;&#20837;&#20102;&#34920;&#31034;&#23433;&#20840;&#36829;&#35268;&#30340;&#29420;&#31435;&#25104;&#26412;&#20989;&#25968;&#12290;&#22312;CMDPs&#30340;&#35774;&#32622;&#20013;&#65292;&#20808;&#21069;&#30340;&#31639;&#27861;&#37319;&#29992;&#20102;&#25289;&#26684;&#26391;&#26085;&#26494;&#24347;&#25216;&#26415;&#23558;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#36716;&#21270;&#20026;&#26080;&#32422;&#26463;&#23545;&#20598;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31639;&#27861;&#21487;&#33021;&#20250;&#19981;&#20934;&#30830;&#22320;&#39044;&#27979;&#19981;&#23433;&#20840;&#34892;&#20026;&#65292;&#23548;&#33268;&#22312;&#23398;&#20064;&#25289;&#26684;&#26391;&#26085;&#20056;&#23376;&#26102;&#20135;&#29983;&#19981;&#31283;&#23450;&#24615;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#8212;&#8212;&#23433;&#20840;&#25209;&#21028;&#31574;&#30053;&#20248;&#21270;&#65288;SCPO&#65289;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#23433;&#20840;&#25209;&#21028;&#22120;&#65292;&#19968;&#31181;&#36890;&#36807;&#36829;&#21453;&#23433;&#20840;&#32422;&#26463;&#32780;&#33719;&#24471;&#30340;&#22870;&#21169;&#34987;&#25269;&#28040;&#30340;&#26426;&#21046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#21487;&#20197;&#33258;&#21160;&#24179;&#34913;&#22312;&#36981;&#23432;&#23433;&#20840;&#32422;&#26463;&#21644;&#26368;&#22823;&#21270;&#22238;&#25253;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Incorporating safety is an essential prerequisite for broadening the practical applications of reinforcement learning in real-world scenarios. To tackle this challenge, Constrained Markov Decision Processes (CMDPs) are leveraged, which introduce a distinct cost function representing safety violations. In CMDPs' settings, Lagrangian relaxation technique has been employed in previous algorithms to convert constrained optimization problems into unconstrained dual problems. However, these algorithms may inaccurately predict unsafe behavior, resulting in instability while learning the Lagrange multiplier. This study introduces a novel safe reinforcement learning algorithm, Safety Critic Policy Optimization (SCPO). In this study, we define the safety critic, a mechanism that nullifies rewards obtained through violating safety constraints. Furthermore, our theoretical analysis indicates that the proposed algorithm can automatically balance the trade-off between adhering to safety constraints 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31995;&#21015;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#35266;&#23519;&#20013;&#35782;&#21035;&#21160;&#24577;&#31995;&#32479;&#30340;&#32467;&#26500;&#65292;&#24182;&#29702;&#35299;&#22797;&#26434;&#31995;&#32479;&#20013;&#30456;&#20114;&#20316;&#29992;&#20195;&#29702;&#30340; emergent behaviors&#12290;&#36825;&#20123;&#26041;&#27861;&#19981;&#20165;&#20855;&#22791;&#29702;&#35770;&#25910;&#25947;&#20445;&#35777;&#65292;&#36824;&#33021;&#39640;&#25928;&#22788;&#29702;&#39640;&#32500;&#35266;&#27979;&#25968;&#25454;&#65292;&#35299;&#20915;&#35266;&#27979;/&#38543;&#26426;&#22122;&#22768;&#12289;&#22797;&#26434;&#30340;&#20132;&#20114;&#35268;&#21017;&#12289;&#20002;&#22833;&#30340;&#20132;&#20114;&#29305;&#24449;&#21644;&#29616;&#23454;&#19990;&#30028;&#35266;&#27979;&#25968;&#25454;&#31561;&#38382;&#39064;&#12290;&#36890;&#36807;&#37319;&#29992;&#21464;&#20998;&#36870;&#38382;&#39064;&#26041;&#27861;&#35774;&#35745;&#21512;&#36866;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#25105;&#20204;&#30340;&#23398;&#20064;&#26041;&#27861;&#20855;&#22791;&#38477;&#32500;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2311.00875</link><description>&lt;p&gt;
&#20174;&#35266;&#23519;&#20013;&#23398;&#20064;&#38598;&#20307;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Learning Collective Behaviors from Observation. (arXiv:2311.00875v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00875
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31995;&#21015;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#35266;&#23519;&#20013;&#35782;&#21035;&#21160;&#24577;&#31995;&#32479;&#30340;&#32467;&#26500;&#65292;&#24182;&#29702;&#35299;&#22797;&#26434;&#31995;&#32479;&#20013;&#30456;&#20114;&#20316;&#29992;&#20195;&#29702;&#30340; emergent behaviors&#12290;&#36825;&#20123;&#26041;&#27861;&#19981;&#20165;&#20855;&#22791;&#29702;&#35770;&#25910;&#25947;&#20445;&#35777;&#65292;&#36824;&#33021;&#39640;&#25928;&#22788;&#29702;&#39640;&#32500;&#35266;&#27979;&#25968;&#25454;&#65292;&#35299;&#20915;&#35266;&#27979;/&#38543;&#26426;&#22122;&#22768;&#12289;&#22797;&#26434;&#30340;&#20132;&#20114;&#35268;&#21017;&#12289;&#20002;&#22833;&#30340;&#20132;&#20114;&#29305;&#24449;&#21644;&#29616;&#23454;&#19990;&#30028;&#35266;&#27979;&#25968;&#25454;&#31561;&#38382;&#39064;&#12290;&#36890;&#36807;&#37319;&#29992;&#21464;&#20998;&#36870;&#38382;&#39064;&#26041;&#27861;&#35774;&#35745;&#21512;&#36866;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#25105;&#20204;&#30340;&#23398;&#20064;&#26041;&#27861;&#20855;&#22791;&#38477;&#32500;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#23398;&#20064;&#26041;&#27861;&#30340;&#32508;&#36848;&#65292;&#29992;&#20110;&#35782;&#21035;&#21160;&#24577;&#31995;&#32479;&#30340;&#32467;&#26500;&#65292;&#26088;&#22312;&#29702;&#35299;&#30456;&#20114;&#20316;&#29992;&#20195;&#29702;&#31995;&#32479;&#20013;&#30340; emergent behaviors&#12290;&#36825;&#20123;&#26041;&#27861;&#19981;&#20165;&#25552;&#20379;&#20102;&#25910;&#25947;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#36824;&#23637;&#31034;&#20102;&#22788;&#29702;&#39640;&#32500;&#35266;&#27979;&#25968;&#25454;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;&#23427;&#20204;&#21487;&#20197;&#22788;&#29702;&#26469;&#33258;&#19968;&#38454;&#21644;&#20108;&#38454;&#21160;&#24577;&#31995;&#32479;&#30340;&#35266;&#27979;&#25968;&#25454;&#65292;&#32771;&#34385;&#35266;&#27979;/&#38543;&#26426;&#22122;&#22768;&#12289;&#22797;&#26434;&#30340;&#20132;&#20114;&#35268;&#21017;&#12289;&#20002;&#22833;&#30340;&#20132;&#20114;&#29305;&#24449;&#20197;&#21450;&#19982;&#29616;&#23454;&#19990;&#30028;&#20013;&#30456;&#20114;&#20316;&#29992;&#20195;&#29702;&#31995;&#32479;&#30340;&#35266;&#27979;&#25968;&#25454;&#12290;&#21457;&#23637;&#36825;&#19968;&#31995;&#21015;&#23398;&#20064;&#26041;&#27861;&#30340;&#26680;&#24515;&#22312;&#20110;&#20351;&#29992;&#21464;&#20998;&#36870;&#38382;&#39064;&#26041;&#27861;&#35774;&#35745;&#36866;&#24403;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#36825;&#31181;&#26041;&#27861;&#22825;&#28982;&#22320;&#25552;&#20379;&#20102;&#25105;&#20204;&#23398;&#20064;&#26041;&#27861;&#30340;&#38477;&#32500;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a review of a series of learning methods used to identify the structure of dynamical systems, aiming to understand emergent behaviors in complex systems of interacting agents. These methods not only offer theoretical guarantees of convergence but also demonstrate computational efficiency in handling high-dimensional observational data. They can manage observation data from both first- and second-order dynamical systems, accounting for observation/stochastic noise, complex interaction rules, missing interaction features, and real-world observations of interacting agent systems. The essence of developing such a series of learning methods lies in designing appropriate loss functions using the variational inverse problem approach, which inherently provides dimension reduction capabilities to our learning methods.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23454;&#29616;&#20102;&#22312;CPU&#19978;&#36827;&#34892;&#20302;&#24310;&#36831;&#23454;&#26102;&#35821;&#38899;&#36716;&#25442;&#65292;&#36890;&#36807;&#36866;&#24212;&#20043;&#21069;&#30340;&#38899;&#39057;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#24182;&#20351;&#29992;&#23545;&#25239;&#29983;&#25104;&#32593;&#32476;&#21644;&#30693;&#35782;&#33976;&#39311;&#65292;&#27169;&#22411;LLVC&#22312;16kHz&#30340;&#27604;&#29305;&#29575;&#19979;&#20855;&#26377;&#19981;&#21040;20ms&#30340;&#24310;&#36831;&#65292;&#24182;&#22312;&#28040;&#36153;&#32423;CPU&#19978;&#20197;2.8&#20493;&#30340;&#36895;&#24230;&#36816;&#34892;&#12290;&#21516;&#26102;&#65292;LLVC&#36824;&#26159;&#25152;&#26377;&#24320;&#28304;&#35821;&#38899;&#36716;&#25442;&#27169;&#22411;&#20013;&#36164;&#28304;&#20351;&#29992;&#26368;&#20302;&#24310;&#36831;&#26368;&#20302;&#30340;&#12290;</title><link>http://arxiv.org/abs/2311.00873</link><description>&lt;p&gt;
&#22312;CPU&#19978;&#30340;&#20302;&#24310;&#36831;&#23454;&#26102;&#35821;&#38899;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
Low-latency Real-time Voice Conversion on CPU. (arXiv:2311.00873v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00873
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23454;&#29616;&#20102;&#22312;CPU&#19978;&#36827;&#34892;&#20302;&#24310;&#36831;&#23454;&#26102;&#35821;&#38899;&#36716;&#25442;&#65292;&#36890;&#36807;&#36866;&#24212;&#20043;&#21069;&#30340;&#38899;&#39057;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#24182;&#20351;&#29992;&#23545;&#25239;&#29983;&#25104;&#32593;&#32476;&#21644;&#30693;&#35782;&#33976;&#39311;&#65292;&#27169;&#22411;LLVC&#22312;16kHz&#30340;&#27604;&#29305;&#29575;&#19979;&#20855;&#26377;&#19981;&#21040;20ms&#30340;&#24310;&#36831;&#65292;&#24182;&#22312;&#28040;&#36153;&#32423;CPU&#19978;&#20197;2.8&#20493;&#30340;&#36895;&#24230;&#36816;&#34892;&#12290;&#21516;&#26102;&#65292;LLVC&#36824;&#26159;&#25152;&#26377;&#24320;&#28304;&#35821;&#38899;&#36716;&#25442;&#27169;&#22411;&#20013;&#36164;&#28304;&#20351;&#29992;&#26368;&#20302;&#24310;&#36831;&#26368;&#20302;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#20043;&#21069;&#30340;&#38899;&#39057;&#22788;&#29702;&#21644;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#36866;&#24212;&#21040;&#23454;&#26102;&#20219;&#24847;&#21040;&#19968;&#30340;&#35821;&#38899;&#36716;&#25442;&#20219;&#21153;&#20013;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;LLVC&#65288;&#20302;&#24310;&#36831;&#20302;&#36164;&#28304;&#35821;&#38899;&#36716;&#25442;&#65289;&#22312;16kHz&#30340;&#27604;&#29305;&#29575;&#19979;&#20855;&#26377;&#19981;&#21040;20ms&#30340;&#24310;&#36831;&#65292;&#22312;&#28040;&#36153;&#32423;CPU&#19978;&#36816;&#34892;&#36895;&#24230;&#20960;&#20046;&#26159;&#23454;&#26102;&#30340;2.8&#20493;&#12290;LLVC&#20351;&#29992;&#20102;&#23545;&#25239;&#29983;&#25104;&#32593;&#32476;&#30340;&#26550;&#26500;&#21644;&#30693;&#35782;&#33976;&#39311;&#26469;&#23454;&#29616;&#36825;&#31181;&#24615;&#33021;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;LLVC&#26159;&#25152;&#26377;&#24320;&#28304;&#35821;&#38899;&#36716;&#25442;&#27169;&#22411;&#20013;&#36164;&#28304;&#20351;&#29992;&#26368;&#20302;&#24310;&#36831;&#26368;&#20302;&#30340;&#12290;&#25105;&#20204;&#22312;https://github.com/KoeAI/LLVC&#25552;&#20379;&#24320;&#28304;&#26679;&#20363;&#12289;&#20195;&#30721;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
We adapt the architectures of previous audio manipulation and generation neural networks to the task of real-time any-to-one voice conversion. Our resulting model, LLVC ($\textbf{L}$ow-latency $\textbf{L}$ow-resource $\textbf{V}$oice $\textbf{C}$onversion), has a latency of under 20ms at a bitrate of 16kHz and runs nearly 2.8x faster than real-time on a consumer CPU. LLVC uses both a generative adversarial architecture as well as knowledge distillation in order to attain this performance. To our knowledge LLVC achieves both the lowest resource usage as well as the lowest latency of any open-source voice conversion model. We provide open-source samples, code, and pretrained model weights at https://github.com/KoeAI/LLVC.
&lt;/p&gt;</description></item><item><title>Transformer&#27169;&#22411;&#36890;&#36807;&#39044;&#35757;&#32451;&#25968;&#25454;&#28151;&#21512;&#23454;&#29616;&#20102;&#29421;&#31364;&#30340;&#27169;&#22411;&#36873;&#25321;&#33021;&#21147;&#65292;&#33021;&#22815;&#22312;&#19978;&#19979;&#25991;&#20013;&#35782;&#21035;&#21644;&#23398;&#20064;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#20294;&#23545;&#20110;&#20219;&#21153;&#25110;&#20989;&#25968;&#30340;&#22788;&#29702;&#30456;&#23545;&#26377;&#38480;&#12290;</title><link>http://arxiv.org/abs/2311.00871</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#25968;&#25454;&#28151;&#21512;&#20351;&#24471;Transformer&#27169;&#22411;&#20855;&#22791;&#29421;&#31364;&#30340;&#27169;&#22411;&#36873;&#25321;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Pretraining Data Mixtures Enable Narrow Model Selection Capabilities in Transformer Models. (arXiv:2311.00871v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00871
&lt;/p&gt;
&lt;p&gt;
Transformer&#27169;&#22411;&#36890;&#36807;&#39044;&#35757;&#32451;&#25968;&#25454;&#28151;&#21512;&#23454;&#29616;&#20102;&#29421;&#31364;&#30340;&#27169;&#22411;&#36873;&#25321;&#33021;&#21147;&#65292;&#33021;&#22815;&#22312;&#19978;&#19979;&#25991;&#20013;&#35782;&#21035;&#21644;&#23398;&#20064;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#20294;&#23545;&#20110;&#20219;&#21153;&#25110;&#20989;&#25968;&#30340;&#22788;&#29702;&#30456;&#23545;&#26377;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#20855;&#26377;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#65288;ICL&#65289;&#30340;&#26174;&#33879;&#33021;&#21147;-&#22312;&#26410;&#32463;&#36807;&#20219;&#20309;&#26126;&#30830;&#27169;&#22411;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#26681;&#25454;&#26410;&#35265;&#36807;&#30340;&#36755;&#20837;-&#36755;&#20986;&#20363;&#23376;&#25191;&#34892;&#26032;&#30340;&#20219;&#21153;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;Transformer&#27169;&#22411;&#22914;&#20309;&#26377;&#25928;&#22320;&#22312;&#20854;&#39044;&#35757;&#32451;&#25968;&#25454;&#28151;&#21512;&#20013;&#24314;&#31435;&#26725;&#26753;&#65292;&#20197;&#22312;&#19978;&#19979;&#25991;&#20013;&#35782;&#21035;&#21644;&#23398;&#20064;&#26082;&#21253;&#25324;&#39044;&#35757;&#32451;&#20998;&#24067;&#20869;&#21448;&#21253;&#25324;&#20854;&#22806;&#30340;&#26032;&#20219;&#21153;&#12290;&#22312;&#20043;&#21069;&#30340;&#30740;&#31350;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#21463;&#25511;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;$(x, f(x))$&#23545;&#24207;&#21015;&#32780;&#19981;&#26159;&#33258;&#28982;&#35821;&#35328;&#36827;&#34892;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;Transformer&#27169;&#22411;&#22312;&#26080;&#30417;&#30563;&#27169;&#22411;&#36873;&#25321;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#25509;&#36817;&#26368;&#20248;&#65292;&#22312;&#33021;&#22815;&#39318;&#20808;&#22312;&#19978;&#19979;&#25991;&#20013;&#35782;&#21035;&#19981;&#21516;&#30340;&#20219;&#21153;&#26063;&#32676;&#24182;&#22312;&#20854;&#20013;&#36827;&#34892;&#23398;&#20064;&#26102;&#65288;&#20219;&#21153;&#26063;&#32676;&#22312;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#26377;&#24456;&#22909;&#30340;&#34920;&#31034;&#65289;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#23545;&#20219;&#21153;&#25110;&#20989;&#25968;&#26102;&#65292;&#24773;&#20917;&#20250;&#31245;&#26377;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer models, notably large language models (LLMs), have the remarkable ability to perform in-context learning (ICL) -- to perform new tasks when prompted with unseen input-output examples without any explicit model training. In this work, we study how effectively transformers can bridge between their pretraining data mixture, comprised of multiple distinct task families, to identify and learn new tasks in-context which are both inside and outside the pretraining distribution. Building on previous work, we investigate this question in a controlled setting, where we study transformer models trained on sequences of $(x, f(x))$ pairs rather than natural language. Our empirical results show transformers demonstrate near-optimal unsupervised model selection capabilities, in their ability to first in-context identify different task families and in-context learn within them when the task families are well-represented in their pretraining data. However when presented with tasks or functi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#36229;&#36234;&#32467;&#26500;&#31232;&#30095;&#24615;&#30340;&#38750;&#32447;&#24615;ICA&#30340;&#27867;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#32452;&#26032;&#30340;&#22312;&#19981;&#23436;&#22791;&#24615;&#12289;&#37096;&#20998;&#31232;&#30095;&#24615;&#12289;&#28304;&#20381;&#36182;&#24615;&#21644;&#28789;&#27963;&#30340;&#20998;&#32452;&#32467;&#26500;&#19979;&#30340;&#21487;&#36776;&#35782;&#24615;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2311.00866</link><description>&lt;p&gt;
&#36229;&#36234;&#32467;&#26500;&#31232;&#30095;&#24615;&#30340;&#38750;&#32447;&#24615;ICA&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Generalizing Nonlinear ICA Beyond Structural Sparsity. (arXiv:2311.00866v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00866
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#36229;&#36234;&#32467;&#26500;&#31232;&#30095;&#24615;&#30340;&#38750;&#32447;&#24615;ICA&#30340;&#27867;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#32452;&#26032;&#30340;&#22312;&#19981;&#23436;&#22791;&#24615;&#12289;&#37096;&#20998;&#31232;&#30095;&#24615;&#12289;&#28304;&#20381;&#36182;&#24615;&#21644;&#28789;&#27963;&#30340;&#20998;&#32452;&#32467;&#26500;&#19979;&#30340;&#21487;&#36776;&#35782;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#32447;&#24615;&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;&#65288;ICA&#65289;&#26088;&#22312;&#20174;&#21487;&#35266;&#27979;&#30340;&#38750;&#32447;&#24615;&#28151;&#21512;&#20013;&#25581;&#31034;&#30495;&#27491;&#30340;&#28508;&#22312;&#28304;&#12290;&#23613;&#31649;&#20854;&#37325;&#35201;&#24615;&#65292;&#38750;&#32447;&#24615;ICA&#30340;&#21487;&#36776;&#35782;&#24615;&#22312;&#27809;&#26377;&#38468;&#21152;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;&#26368;&#36817;&#30340;&#36827;&#23637;&#25552;&#20986;&#20102;&#28304;&#21040;&#35266;&#27979;&#21464;&#37327;&#30340;&#36830;&#25509;&#32467;&#26500;&#30340;&#26465;&#20214;&#65292;&#31216;&#20026;&#32467;&#26500;&#31232;&#30095;&#24615;&#65292;&#20197;&#23454;&#29616;&#26080;&#30417;&#30563;&#30340;&#21487;&#36776;&#35782;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#31232;&#30095;&#32422;&#26463;&#21487;&#33021;&#19981;&#36866;&#29992;&#20110;&#25152;&#26377;&#28304;&#12290;&#27492;&#22806;&#65292;&#28304;&#30340;&#28151;&#21512;&#36807;&#31243;&#30340;&#21452;&#23556;&#24615;&#21644;&#25152;&#26377;&#28304;&#20043;&#38388;&#30340;&#29420;&#31435;&#24615;&#30340;&#20551;&#35774;&#65292;&#36825;&#20123;&#20551;&#35774;&#26469;&#33258;ICA&#30340;&#35774;&#23450;&#65292;&#22312;&#35768;&#22810;&#29616;&#23454;&#22330;&#26223;&#20013;&#20063;&#21487;&#33021;&#34987;&#36829;&#32972;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#24182;&#27867;&#21270;&#38750;&#32447;&#24615;ICA&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#32452;&#26032;&#30340;&#21487;&#36776;&#35782;&#24615;&#32467;&#26524;&#65292;&#28085;&#30422;&#19981;&#23436;&#22791;&#24615;&#12289;&#37096;&#20998;&#31232;&#30095;&#24615;&#12289;&#28304;&#20381;&#36182;&#24615;&#21644;&#28789;&#27963;&#30340;&#20998;&#32452;&#32467;&#26500;&#30340;&#19968;&#33324;&#35774;&#32622;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#23384;&#22312;&#26356;&#22810;&#28304;&#26102;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21487;&#36776;&#35782;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nonlinear independent component analysis (ICA) aims to uncover the true latent sources from their observable nonlinear mixtures. Despite its significance, the identifiability of nonlinear ICA is known to be impossible without additional assumptions. Recent advances have proposed conditions on the connective structure from sources to observed variables, known as Structural Sparsity, to achieve identifiability in an unsupervised manner. However, the sparsity constraint may not hold universally for all sources in practice. Furthermore, the assumptions of bijectivity of the mixing process and independence among all sources, which arise from the setting of ICA, may also be violated in many real-world scenarios. To address these limitations and generalize nonlinear ICA, we propose a set of new identifiability results in the general settings of undercompleteness, partial sparsity and source dependence, and flexible grouping structures. Specifically, we prove identifiability when there are mor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36873;&#25321;&#24615;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21363;&#36873;&#25321;&#24615;&#22810;&#26234;&#33021;&#20307;&#20248;&#20808;&#20307;&#39564;&#20013;&#32487;&#65292;&#20195;&#29702;&#20043;&#38388;&#20849;&#20139;&#26377;&#38480;&#25968;&#37327;&#30340;&#35757;&#32451;&#32463;&#39564;&#12290;&#19982;&#20854;&#20182;&#31639;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#21435;&#20013;&#24515;&#21270;&#35757;&#32451;&#65292;&#24182;&#21462;&#24471;&#20102;&#27604;&#22522;&#20934;&#31639;&#27861;&#21644;&#26368;&#20808;&#36827;&#31639;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.00865</link><description>&lt;p&gt;
&#36873;&#25321;&#24615;&#20998;&#20139;&#20307;&#39564;&#25552;&#21319;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Selectively Sharing Experiences Improves Multi-Agent Reinforcement Learning. (arXiv:2311.00865v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36873;&#25321;&#24615;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21363;&#36873;&#25321;&#24615;&#22810;&#26234;&#33021;&#20307;&#20248;&#20808;&#20307;&#39564;&#20013;&#32487;&#65292;&#20195;&#29702;&#20043;&#38388;&#20849;&#20139;&#26377;&#38480;&#25968;&#37327;&#30340;&#35757;&#32451;&#32463;&#39564;&#12290;&#19982;&#20854;&#20182;&#31639;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#21435;&#20013;&#24515;&#21270;&#35757;&#32451;&#65292;&#24182;&#21462;&#24471;&#20102;&#27604;&#22522;&#20934;&#31639;&#27861;&#21644;&#26368;&#20808;&#36827;&#31639;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21363;&#36873;&#25321;&#24615;&#22810;&#26234;&#33021;&#20307;&#20248;&#20808;&#20307;&#39564;&#20013;&#32487;&#65292;&#20854;&#20013;&#20195;&#29702;&#36890;&#36807;&#20998;&#20139;&#35757;&#32451;&#36807;&#31243;&#20013;&#35266;&#23519;&#21040;&#30340;&#26377;&#38480;&#30340;&#36716;&#25442;&#19982;&#20854;&#20182;&#20195;&#29702;&#20849;&#20139;&#12290;&#20854;&#32972;&#21518;&#30340;&#30452;&#35273;&#26159;&#65292;&#26469;&#33258;&#20854;&#20182;&#20195;&#29702;&#30340;&#23569;&#37327;&#30456;&#20851;&#32463;&#39564;&#21487;&#20197;&#24110;&#21161;&#27599;&#20010;&#20195;&#29702;&#23398;&#20064;&#12290;&#19982;&#35768;&#22810;&#20854;&#20182;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#20801;&#35768;&#22522;&#26412;&#21435;&#20013;&#24515;&#21270;&#30340;&#35757;&#32451;&#65292;&#21482;&#38656;&#35201;&#20195;&#29702;&#20043;&#38388;&#30340;&#26377;&#38480;&#36890;&#20449;&#28192;&#36947;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#22522;&#20934;&#26080;&#20849;&#20139;&#21435;&#20013;&#24515;&#21270;&#35757;&#32451;&#21644;&#26368;&#20808;&#36827;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#20165;&#20998;&#20139;&#23569;&#37327;&#39640;&#24230;&#30456;&#20851;&#30340;&#32463;&#39564;&#20248;&#20110;&#20195;&#29702;&#20043;&#38388;&#30340;&#25152;&#26377;&#32463;&#39564;&#20849;&#20139;&#65292;&#32780;&#19988;&#36873;&#25321;&#24615;&#20307;&#39564;&#20849;&#20139;&#30340;&#24615;&#33021;&#25552;&#21319;&#22312;&#21508;&#31181;&#36229;&#21442;&#25968;&#21644;DQN&#21464;&#20307;&#20013;&#22343;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#30340;&#21442;&#32771;&#23454;&#29616;&#21487;&#22312;https://github.com/mgerstgrasser/super&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel multi-agent RL approach, Selective Multi-Agent Prioritized Experience Relay, in which agents share with other agents a limited number of transitions they observe during training. The intuition behind this is that even a small number of relevant experiences from other agents could help each agent learn. Unlike many other multi-agent RL algorithms, this approach allows for largely decentralized training, requiring only a limited communication channel between agents. We show that our approach outperforms baseline no-sharing decentralized training and state-of-the art multi-agent RL algorithms. Further, sharing only a small number of highly relevant experiences outperforms sharing all experiences between agents, and the performance uplift from selective experience sharing is robust across a range of hyperparameters and DQN variants. A reference implementation of our algorithm is available at https://github.com/mgerstgrasser/super.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#35757;&#32451;&#36807;&#31243;&#20013;&#19978;&#19979;&#25991;N-Gram&#30340;&#21160;&#24577;&#65292;&#21457;&#29616;&#20102;&#19978;&#19979;&#25991;&#31070;&#32463;&#20803;&#23384;&#22312;&#20110;&#26356;&#24191;&#27867;&#30340;&#19978;&#19979;&#25991;N-Gram&#30005;&#36335;&#20013;&#65292;&#36825;&#34987;&#31216;&#20026;&#20108;&#38454;&#30005;&#36335;&#12290;&#22312;&#35757;&#32451;&#26089;&#26399;&#65292;&#36825;&#20004;&#20010;&#30005;&#36335;&#20855;&#26377;&#30456;&#20114;&#29420;&#31435;&#30340;&#21151;&#33021;&#65292;&#21482;&#26377;&#22312;&#23427;&#20204;&#37117;&#24418;&#25104;&#20043;&#21518;&#25165;&#33021;&#32452;&#21512;&#25104;&#19968;&#20010;&#20108;&#38454;&#30005;&#36335;&#12290;</title><link>http://arxiv.org/abs/2311.00863</link><description>&lt;p&gt;
&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#19978;&#19979;&#25991;N-Gram&#30340;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Training Dynamics of Contextual N-Grams in Language Models. (arXiv:2311.00863v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00863
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#35757;&#32451;&#36807;&#31243;&#20013;&#19978;&#19979;&#25991;N-Gram&#30340;&#21160;&#24577;&#65292;&#21457;&#29616;&#20102;&#19978;&#19979;&#25991;&#31070;&#32463;&#20803;&#23384;&#22312;&#20110;&#26356;&#24191;&#27867;&#30340;&#19978;&#19979;&#25991;N-Gram&#30005;&#36335;&#20013;&#65292;&#36825;&#34987;&#31216;&#20026;&#20108;&#38454;&#30005;&#36335;&#12290;&#22312;&#35757;&#32451;&#26089;&#26399;&#65292;&#36825;&#20004;&#20010;&#30005;&#36335;&#20855;&#26377;&#30456;&#20114;&#29420;&#31435;&#30340;&#21151;&#33021;&#65292;&#21482;&#26377;&#22312;&#23427;&#20204;&#37117;&#24418;&#25104;&#20043;&#21518;&#25165;&#33021;&#32452;&#21512;&#25104;&#19968;&#20010;&#20108;&#38454;&#30005;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#34920;&#26126;&#65292;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#19978;&#19979;&#25991;&#31070;&#32463;&#20803;&#65292;&#21253;&#25324;&#19968;&#20010;&#22312;&#24503;&#35821;&#25991;&#26412;&#19978;&#28608;&#27963;&#30340;&#31070;&#32463;&#20803;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#31070;&#32463;&#20803;&#23384;&#22312;&#20110;&#26356;&#24191;&#27867;&#30340;&#19978;&#19979;&#25991;N-Gram&#30005;&#36335;&#20013;&#65306;&#25105;&#20204;&#21457;&#29616;&#26202;&#26399;&#30340;&#31070;&#32463;&#20803;&#33021;&#22815;&#35782;&#21035;&#21644;&#32487;&#32493;&#24503;&#35821;&#25991;&#26412;&#20013;&#24120;&#35265;&#30340;N-Gram&#65292;&#20294;&#21482;&#26377;&#22312;&#24503;&#35821;&#31070;&#32463;&#20803;&#28608;&#27963;&#26102;&#25165;&#20250;&#34987;&#28608;&#27963;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#35757;&#32451;&#36807;&#31243;&#20013;&#36825;&#20010;&#30005;&#36335;&#30340;&#24418;&#25104;&#65292;&#24182;&#21457;&#29616;&#36825;&#26159;&#19968;&#20010;&#25105;&#20204;&#31216;&#20043;&#20026;&#20108;&#38454;&#30005;&#36335;&#30340;&#31034;&#20363;&#12290;&#29305;&#21035;&#22320;&#65292;&#26089;&#26399;&#30340;&#35757;&#32451;&#20013;&#65292;&#32452;&#25104;N-Gram&#30005;&#36335;&#21644;&#26368;&#32456;&#24418;&#25104;&#24503;&#35821;&#31070;&#32463;&#20803;&#30340;&#24503;&#35821;&#26816;&#27979;&#30005;&#36335;&#20855;&#26377;&#29420;&#31435;&#30340;&#21151;&#33021;-&#24503;&#35821;&#26816;&#27979;&#30005;&#36335;&#37096;&#20998;&#36890;&#36807;&#24314;&#27169;&#24503;&#35821;&#21333;&#23383;&#32479;&#35745;&#25968;&#25454;&#36827;&#34892;&#24418;&#25104;&#65292;&#32780;N-Gram&#21017;&#36890;&#36807;&#25552;&#21319;&#36866;&#24403;&#30340;&#23436;&#25104;&#26469;&#24418;&#25104;&#12290;&#21482;&#26377;&#22312;&#36825;&#20004;&#20010;&#30005;&#36335;&#24050;&#32463;&#24418;&#25104;&#20043;&#21518;&#65292;&#23427;&#20204;&#25165;&#33021;&#32452;&#21512;&#25104;&#20026;&#19968;&#20010;&#20108;&#38454;&#30005;&#36335;&#12290;&#19982;&#20808;&#21069;&#30340;&#30740;&#31350;&#20551;&#35774;&#30456;&#21453;&#65292;&#25105;&#20204;&#21457;&#29616;&#19978;&#19979;&#25991;N-Gram&#30005;&#36335;&#36880;&#28176;&#24418;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prior work has shown the existence of contextual neurons in language models, including a neuron that activates on German text. We show that this neuron exists within a broader contextual n-gram circuit: we find late layer neurons which recognize and continue n-grams common in German text, but which only activate if the German neuron is active. We investigate the formation of this circuit throughout training and find that it is an example of what we call a second-order circuit. In particular, both the constituent n-gram circuits and the German detection circuit which culminates in the German neuron form with independent functions early in training - the German detection circuit partially through modeling German unigram statistics, and the n-grams by boosting appropriate completions. Only after both circuits have already formed do they fit together into a second-order circuit. Contrary to the hypotheses presented in prior work, we find that the contextual n-gram circuit forms gradually r
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#22312;&#26426;&#22120;&#23398;&#20064;&#21183;&#20989;&#25968;&#20013;&#65292;&#38656;&#35201;&#35880;&#24910;&#24179;&#34913;&#32467;&#26500;&#21644;&#26500;&#35937;&#22810;&#26679;&#24615;&#20197;&#23454;&#29616;&#26368;&#20339;&#30340;&#27867;&#21270;&#25928;&#26524;&#65292;&#24182;&#24378;&#35843;&#20102;&#23450;&#20041;&#36866;&#29992;&#39046;&#22495;&#23545;&#20110;&#27169;&#22411;&#37096;&#32626;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00862</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21183;&#20989;&#25968;&#20013;&#32467;&#26500;&#21644;&#26500;&#35937;&#22810;&#26679;&#24615;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Role of Structural and Conformational Diversity for Machine Learning Potentials. (arXiv:2311.00862v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00862
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#22312;&#26426;&#22120;&#23398;&#20064;&#21183;&#20989;&#25968;&#20013;&#65292;&#38656;&#35201;&#35880;&#24910;&#24179;&#34913;&#32467;&#26500;&#21644;&#26500;&#35937;&#22810;&#26679;&#24615;&#20197;&#23454;&#29616;&#26368;&#20339;&#30340;&#27867;&#21270;&#25928;&#26524;&#65292;&#24182;&#24378;&#35843;&#20102;&#23450;&#20041;&#36866;&#29992;&#39046;&#22495;&#23545;&#20110;&#27169;&#22411;&#37096;&#32626;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#21183;&#20989;&#25968;&#65288;MLIPs&#65289;&#39046;&#22495;&#20013;&#65292;&#29702;&#35299;&#25968;&#25454;&#20559;&#24046;&#65288;&#29305;&#21035;&#26159;&#26500;&#35937;&#21644;&#32467;&#26500;&#22810;&#26679;&#24615;&#65289;&#19982;&#27169;&#22411;&#27867;&#21270;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#23545;&#20110;&#25552;&#39640;&#37327;&#23376;&#21147;&#23398;&#65288;QM&#65289;&#25968;&#25454;&#29983;&#25104;&#30340;&#36136;&#37327;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#19981;&#21516;&#30340;&#23454;&#39564;&#26469;&#30740;&#31350;&#36825;&#20123;&#21160;&#24577;&#65306;&#19968;&#20010;&#26159;&#22266;&#23450;&#39044;&#31639;&#30340;&#23454;&#39564;&#65292;&#25968;&#25454;&#38598;&#22823;&#23567;&#20445;&#25345;&#24658;&#23450;&#65307;&#21478;&#19968;&#20010;&#26159;&#22266;&#23450;&#20998;&#23376;&#38598;&#30340;&#23454;&#39564;&#65292;&#37325;&#28857;&#22312;&#20110;&#22266;&#23450;&#32467;&#26500;&#22810;&#26679;&#24615;&#32780;&#21464;&#21270;&#26500;&#35937;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;&#27867;&#21270;&#24230;&#37327;&#20013;&#24494;&#22937;&#30340;&#27169;&#24335;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20026;&#20102;&#23454;&#29616;&#26368;&#20339;&#30340;&#32467;&#26500;&#21644;&#26500;&#35937;&#27867;&#21270;&#65292;&#38656;&#35201;&#22312;&#32467;&#26500;&#21644;&#26500;&#35937;&#22810;&#26679;&#24615;&#20043;&#38388;&#36798;&#21040;&#35880;&#24910;&#30340;&#24179;&#34913;&#65292;&#20294;&#29616;&#26377;&#30340;QM&#25968;&#25454;&#38598;&#24182;&#19981;&#28385;&#36275;&#36825;&#31181;&#26435;&#34913;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#31361;&#20986;&#20102;MLIP&#27169;&#22411;&#22312;&#36229;&#20986;&#20854;&#35757;&#32451;&#20998;&#24067;&#27867;&#21270;&#30340;&#23616;&#38480;&#24615;&#65292;&#24378;&#35843;&#20102;&#22312;&#27169;&#22411;&#37096;&#32626;&#36807;&#31243;&#20013;&#23450;&#20041;&#36866;&#29992;&#39046;&#22495;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of Machine Learning Interatomic Potentials (MLIPs), understanding the intricate relationship between data biases, specifically conformational and structural diversity, and model generalization is critical in improving the quality of Quantum Mechanics (QM) data generation efforts. We investigate these dynamics through two distinct experiments: a fixed budget one, where the dataset size remains constant, and a fixed molecular set one, which focuses on fixed structural diversity while varying conformational diversity. Our results reveal nuanced patterns in generalization metrics. Notably, for optimal structural and conformational generalization, a careful balance between structural and conformational diversity is required, but existing QM datasets do not meet that trade-off. Additionally, our results highlight the limitation of the MLIP models at generalizing beyond their training distribution, emphasizing the importance of defining applicability domain during model deploymen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#29289;&#29702;&#32422;&#26463;&#25805;&#20316;&#23398;&#20064;&#30340;&#26032;&#22411;&#33258;&#21160;&#24494;&#20998;&#31639;&#27861;&#65292;&#36890;&#36807;&#38646;&#22352;&#26631;&#31227;&#21160;&#65288;ZCS&#65289;&#30340;&#25216;&#24039;&#65292;&#23558;&#25152;&#38656;&#23548;&#25968;&#30340;&#22797;&#26434;&#24230;&#20174;&#8220;&#22810;&#26681;&#22810;&#21494;&#8221;&#31616;&#21270;&#20026;&#8220;&#19968;&#26681;&#22810;&#21494;&#8221;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.00860</link><description>&lt;p&gt;
&#38646;&#22352;&#26631;&#31227;&#21160;&#65306;&#38024;&#23545;&#29289;&#29702;&#32422;&#26463;&#25805;&#20316;&#23398;&#20064;&#30340;&#20248;&#21270;&#33258;&#21160;&#24494;&#20998;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Zero Coordinate Shift: Whetted Automatic Differentiation for Physics-informed Operator Learning. (arXiv:2311.00860v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00860
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#29289;&#29702;&#32422;&#26463;&#25805;&#20316;&#23398;&#20064;&#30340;&#26032;&#22411;&#33258;&#21160;&#24494;&#20998;&#31639;&#27861;&#65292;&#36890;&#36807;&#38646;&#22352;&#26631;&#31227;&#21160;&#65288;ZCS&#65289;&#30340;&#25216;&#24039;&#65292;&#23558;&#25152;&#38656;&#23548;&#25968;&#30340;&#22797;&#26434;&#24230;&#20174;&#8220;&#22810;&#26681;&#22810;&#21494;&#8221;&#31616;&#21270;&#20026;&#8220;&#19968;&#26681;&#22810;&#21494;&#8221;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#24494;&#20998;&#65288;AD&#65289;&#26159;&#29289;&#29702;&#32422;&#26463;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20851;&#38190;&#27493;&#39588;&#65292;&#29992;&#20110;&#35745;&#31639;&#32593;&#32476;&#36755;&#20986;&#30456;&#23545;&#20110;&#22352;&#26631;&#30340;&#39640;&#38454;&#23548;&#25968;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#36731;&#37327;&#32423;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#36827;&#34892;&#38024;&#23545;&#29289;&#29702;&#32422;&#26463;&#25805;&#20316;&#23398;&#20064;&#30340;&#33258;&#21160;&#24494;&#20998;&#65292;&#31216;&#20026;&#38646;&#22352;&#26631;&#31227;&#21160;&#65288;ZCS&#65289;&#30340;&#25216;&#24039;&#12290;ZCS&#24341;&#20837;&#20102;&#19968;&#20010;&#26631;&#37327;&#20540;&#30340;&#21494;&#21464;&#37327;&#65292;&#29992;&#20110;&#27599;&#20010;&#31354;&#38388;&#25110;&#26102;&#38388;&#32500;&#24230;&#65292;&#36890;&#36807;&#23558;&#25152;&#38656;&#23548;&#25968;&#20174;&#8220;&#22810;&#26681;&#22810;&#21494;&#8221;&#31616;&#21270;&#20026;&#8220;&#19968;&#26681;&#22810;&#21494;&#8221;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#24615;&#33021;&#30340;&#24040;&#22823;&#25552;&#21319;&#12290;ZCS&#24456;&#23481;&#26131;&#22312;&#24403;&#21069;&#30340;&#28145;&#24230;&#23398;&#20064;&#24211;&#20013;&#23454;&#29616;&#65307;&#25105;&#20204;&#20351;&#29992;DeepXDE&#36719;&#20214;&#21253;&#36827;&#34892;&#20102;&#33258;&#24049;&#30340;&#23454;&#29616;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22522;&#20934;&#20998;&#26512;&#21644;&#22810;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#35757;&#32451;&#29289;&#29702;&#32422;&#26463;&#30340;DeepONets&#26469;&#35299;&#20915;&#26080;&#25968;&#25454;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;ZCS&#19968;&#30452;&#36890;&#36807;&#38477;&#20302;GPU&#20869;&#23384;&#28040;&#32791;&#25552;&#20379;&#20102;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic differentiation (AD) is a critical step in physics-informed machine learning, required for computing the high-order derivatives of network output w.r.t. coordinates. In this paper, we present a novel and lightweight algorithm to conduct such AD for physics-informed operator learning, as we call the trick of Zero Coordinate Shift (ZCS). Instead of making all sampled coordinates leaf variables, ZCS introduces only one scalar-valued leaf variable for each spatial or temporal dimension, leading to a game-changing performance leap by simplifying the wanted derivatives from "many-roots-many-leaves" to "one-root-many-leaves". ZCS is easy to implement with current deep learning libraries; our own implementation is by extending the DeepXDE package. We carry out a comprehensive benchmark analysis and several case studies, training physics-informed DeepONets to solve partial differential equations (PDEs) without data. The results show that ZCS has persistently brought down GPU memory co
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#20248;&#25104;&#26412;&#32422;&#26463;&#30340;&#20998;&#24067;&#24335;&#25915;&#20987;&#20195;&#29702;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#26174;&#33879;&#38477;&#20302;&#21463;&#25915;&#20987;&#20195;&#29702;&#33719;&#24471;&#30340;&#22870;&#21169;&#12290;</title><link>http://arxiv.org/abs/2311.00859</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#22522;&#20110;&#26368;&#20248;&#25104;&#26412;&#32422;&#26463;&#30340;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Optimal Cost Constrained Adversarial Attacks For Multiple Agent Systems. (arXiv:2311.00859v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00859
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#20248;&#25104;&#26412;&#32422;&#26463;&#30340;&#20998;&#24067;&#24335;&#25915;&#20987;&#20195;&#29702;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#26174;&#33879;&#38477;&#20302;&#21463;&#25915;&#20987;&#20195;&#29702;&#33719;&#24471;&#30340;&#22870;&#21169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23547;&#25214;&#26368;&#20248;&#30340;&#23545;&#25239;&#25915;&#20987;&#31574;&#30053;&#26159;&#24378;&#21270;&#23398;&#20064;&#21644;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#37325;&#35201;&#35838;&#39064;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#24120;&#20551;&#35774;&#19968;&#20010;&#20840;&#30693;&#30340;&#21327;&#35843;&#32773;&#65288;&#25915;&#20987;&#32773;&#65289;&#65292;&#25915;&#20987;&#19981;&#21516;&#30340;&#25509;&#25910;&#32773;&#65288;&#21463;&#23475;&#32773;&#65289;&#20195;&#29702;&#20250;&#20135;&#29983;&#32479;&#19968;&#30340;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#20013;&#65292;&#25915;&#20987;&#36890;&#24120;&#38656;&#35201;&#30001;&#20998;&#24067;&#24335;&#25915;&#20987;&#20195;&#29702;&#25191;&#34892;&#65292;&#32780;&#19981;&#26159;&#20351;&#29992;&#19968;&#20010;&#26080;&#38480;&#21046;&#30340;&#20013;&#24515;&#25915;&#20987;&#32773;&#12290;&#25105;&#20204;&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#22914;&#20309;&#20351;&#29992;&#20998;&#24067;&#24335;&#25915;&#20987;&#20195;&#29702;&#36827;&#34892;&#26368;&#20248;&#30340;&#23545;&#25239;&#20195;&#29702;&#38388;&#25915;&#20987;&#65292;&#20854;&#20013;&#23545;&#27599;&#20010;&#19981;&#21516;&#30340;&#25915;&#20987;&#32773;-&#21463;&#23475;&#32773;&#23545;&#37117;&#26045;&#21152;&#19981;&#21516;&#30340;&#25104;&#26412;&#32422;&#26463;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#20248;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27493;&#39588;&#20869;&#36827;&#34892;&#38745;&#24577;&#32422;&#26463;&#25915;&#20987;&#36164;&#28304;&#20998;&#37197;&#20248;&#21270;&#65292;&#24182;&#22312;&#27493;&#39588;&#38388;&#36827;&#34892;&#21160;&#24577;&#35268;&#21010;&#65292;&#20197;&#23454;&#29616;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#30340;&#26368;&#20248;&#23545;&#25239;&#25915;&#20987;&#12290;&#25105;&#20204;&#30340;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#25915;&#20987;&#33021;&#22815;&#26174;&#33879;&#38477;&#20302;&#21463;&#25915;&#20987;&#20195;&#29702;&#25152;&#33719;&#24471;&#30340;&#22870;&#21169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finding optimal adversarial attack strategies is an important topic in reinforcement learning and the Markov decision process. Previous studies usually assume one all-knowing coordinator (attacker) for whom attacking different recipient (victim) agents incurs uniform costs. However, in reality, instead of using one limitless central attacker, the attacks often need to be performed by distributed attack agents. We formulate the problem of performing optimal adversarial agent-to-agent attacks using distributed attack agents, in which we impose distinct cost constraints on each different attacker-victim pair. We propose an optimal method integrating within-step static constrained attack-resource allocation optimization and between-step dynamic programming to achieve the optimal adversarial attack in a multi-agent system. Our numerical results show that the proposed attacks can significantly reduce the rewards received by the attacked agents.
&lt;/p&gt;</description></item><item><title>SmoothHess&#26159;&#19968;&#31181;&#36890;&#36807;Stein&#24341;&#29702;&#23545;ReLU&#32593;&#32476;&#30340;&#29305;&#24449;&#20132;&#20114;&#36827;&#34892;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#23427;&#33021;&#20272;&#35745;&#32593;&#32476;&#21644;&#39640;&#26031;&#21367;&#31215;&#30340;Hessian&#30697;&#38453;&#65292;&#20855;&#26377;&#39640;&#25928;&#30340;&#25277;&#26679;&#31639;&#27861;&#21644;&#21487;&#25511;&#30340;&#24179;&#28369;&#31243;&#24230;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#21307;&#23398;&#32954;&#21151;&#33021;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;SmoothHess&#25429;&#25417;&#20132;&#20114;&#20316;&#29992;&#30340;&#21331;&#36234;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2311.00858</link><description>&lt;p&gt;
SmoothHess: &#36890;&#36807;Stein&#24341;&#29702;&#23545;ReLU&#32593;&#32476;&#30340;&#29305;&#24449;&#20132;&#20114;&#36827;&#34892;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
SmoothHess: ReLU Network Feature Interactions via Stein's Lemma. (arXiv:2311.00858v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00858
&lt;/p&gt;
&lt;p&gt;
SmoothHess&#26159;&#19968;&#31181;&#36890;&#36807;Stein&#24341;&#29702;&#23545;ReLU&#32593;&#32476;&#30340;&#29305;&#24449;&#20132;&#20114;&#36827;&#34892;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#23427;&#33021;&#20272;&#35745;&#32593;&#32476;&#21644;&#39640;&#26031;&#21367;&#31215;&#30340;Hessian&#30697;&#38453;&#65292;&#20855;&#26377;&#39640;&#25928;&#30340;&#25277;&#26679;&#31639;&#27861;&#21644;&#21487;&#25511;&#30340;&#24179;&#28369;&#31243;&#24230;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#21307;&#23398;&#32954;&#21151;&#33021;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;SmoothHess&#25429;&#25417;&#20132;&#20114;&#20316;&#29992;&#30340;&#21331;&#36234;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#19968;&#20123;&#35299;&#37322;&#24615;&#26041;&#27861;&#36890;&#36807;&#26597;&#30475;&#31070;&#32463;&#32593;&#32476;&#30340;Hessian&#30697;&#38453;&#26469;&#24314;&#27169;&#29305;&#24449;&#30340;&#20132;&#20114;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;ReLU&#32593;&#32476;&#26469;&#35828;&#65292;&#36825;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#26159;&#20998;&#27573;&#32447;&#24615;&#30340;&#65292;&#22312;&#20960;&#20046;&#25152;&#26377;&#22320;&#26041;&#37117;&#20855;&#26377;&#38646;&#30340;Hessian&#30697;&#38453;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SmoothHess&#26041;&#27861;&#65292;&#36890;&#36807;Stein&#24341;&#29702;&#20272;&#35745;&#20108;&#38454;&#20132;&#20114;&#20316;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#39640;&#25928;&#30340;&#25277;&#26679;&#31639;&#27861;&#65292;&#20272;&#35745;&#20102;&#32593;&#32476;&#21644;&#39640;&#26031;&#21367;&#31215;&#30340;Hessian&#30697;&#38453;&#65292;&#21482;&#38656;&#35201;&#32593;&#32476;&#30340;&#26799;&#24230;&#35843;&#29992;&#12290;SmoothHess&#26159;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#23545;ReLU&#32593;&#32476;&#26550;&#26500;&#36827;&#34892;&#20462;&#25913;&#65292;&#24179;&#28369;&#31243;&#24230;&#21487;&#20197;&#26126;&#30830;&#25511;&#21046;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#20110;&#20272;&#35745;&#36807;&#31243;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#38750;&#28176;&#36827;&#30028;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;SmoothHess&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#21307;&#23398;&#32954;&#21151;&#33021;&#25968;&#25454;&#38598;&#19978;&#25429;&#25417;&#20132;&#20114;&#20316;&#29992;&#30340;&#21331;&#36234;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several recent methods for interpretability model feature interactions by looking at the Hessian of a neural network. This poses a challenge for ReLU networks, which are piecewise-linear and thus have a zero Hessian almost everywhere. We propose SmoothHess, a method of estimating second-order interactions through Stein's Lemma. In particular, we estimate the Hessian of the network convolved with a Gaussian through an efficient sampling algorithm, requiring only network gradient calls. SmoothHess is applied post-hoc, requires no modifications to the ReLU network architecture, and the extent of smoothing can be controlled explicitly. We provide a non-asymptotic bound on the sample complexity of our estimation procedure. We validate the superior ability of SmoothHess to capture interactions on benchmark datasets and a real-world medical spirometry dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#32654;&#22269;&#32456;&#32467;HIV&#27969;&#34892;&#35745;&#21010;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#36827;&#34892;&#29305;&#23450;&#22320;&#21306;&#30340;&#20915;&#31574;&#20998;&#26512;&#65292;&#24182;&#32771;&#34385;&#21040;&#22320;&#21306;&#20043;&#38388;&#30340;&#27969;&#34892;&#30149;&#23398;&#30456;&#20114;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2311.00855</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#32654;&#22269;&#32456;&#32467;HIV&#27969;&#34892;&#35745;&#21010;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Multi-Agent Reinforcement Learning Framework for Evaluating the U.S. Ending the HIV Epidemic Plan. (arXiv:2311.00855v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00855
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#32654;&#22269;&#32456;&#32467;HIV&#27969;&#34892;&#35745;&#21010;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#36827;&#34892;&#29305;&#23450;&#22320;&#21306;&#30340;&#20915;&#31574;&#20998;&#26512;&#65292;&#24182;&#32771;&#34385;&#21040;&#22320;&#21306;&#20043;&#38388;&#30340;&#27969;&#34892;&#30149;&#23398;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20813;&#30123;&#32570;&#38519;&#30149;&#27602;&#65288;HIV&#65289;&#26159;&#32654;&#22269;&#30340;&#20027;&#35201;&#20844;&#20849;&#21355;&#29983;&#38382;&#39064;&#65292;&#27599;&#24180;&#26377;&#32422;1.2&#19975;&#20154;&#24863;&#26579;HIV&#65292;&#20854;&#20013;&#26377;3.5&#19975;&#20154;&#26159;&#26032;&#24863;&#26579;&#32773;&#12290;&#32654;&#22269;&#30340;HIV&#36127;&#25285;&#21644;&#25252;&#29702;&#25509;&#35302;&#23384;&#22312;&#30528;&#22320;&#29702;&#24046;&#24322;&#12290;2019&#24180;&#30340;&#32456;&#32467;HIV&#27969;&#34892;&#35745;&#21010;&#26088;&#22312;&#21040;2030&#24180;&#23558;&#26032;&#24863;&#26579;&#20154;&#25968;&#20943;&#23569;90%&#65292;&#36890;&#36807;&#25552;&#39640;&#35786;&#26029;&#12289;&#27835;&#30103;&#21644;&#39044;&#38450;&#24178;&#39044;&#25514;&#26045;&#30340;&#35206;&#30422;&#29575;&#65292;&#24182;&#20248;&#20808;&#32771;&#34385;HIV&#39640;&#27969;&#34892;&#22320;&#21306;&#12290;&#30830;&#23450;&#26368;&#20339;&#24178;&#39044;&#25514;&#26045;&#30340;&#35268;&#27169;&#25193;&#22823;&#23558;&#26377;&#21161;&#20110;&#36164;&#28304;&#20998;&#37197;&#30340;&#20915;&#31574;&#12290;&#29616;&#26377;&#30340;HIV&#20915;&#31574;&#27169;&#22411;&#35201;&#20040;&#21482;&#35780;&#20272;&#29305;&#23450;&#22478;&#24066;&#65292;&#35201;&#20040;&#35780;&#20272;&#25972;&#20010;&#22269;&#23478;&#20154;&#21475;&#65292;&#24573;&#35270;&#22320;&#26041;&#30340;&#30456;&#20114;&#20316;&#29992;&#25110;&#24046;&#24322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#36827;&#34892;&#29305;&#23450;&#22320;&#21306;&#30340;&#20915;&#31574;&#20998;&#26512;&#65292;&#21516;&#26102;&#32771;&#34385;&#36328;&#22320;&#21306;&#30340;&#27969;&#34892;&#30149;&#20114;&#21160;&#12290;&#22312;&#23454;&#39564;&#20998;&#26512;&#20013;&#65292;
&lt;/p&gt;
&lt;p&gt;
Human immunodeficiency virus (HIV) is a major public health concern in the United States, with about 1.2 million people living with HIV and 35,000 newly infected each year. There are considerable geographical disparities in HIV burden and care access across the U.S. The 2019 Ending the HIV Epidemic (EHE) initiative aims to reduce new infections by 90% by 2030, by improving coverage of diagnoses, treatment, and prevention interventions and prioritizing jurisdictions with high HIV prevalence. Identifying optimal scale-up of intervention combinations will help inform resource allocation. Existing HIV decision analytic models either evaluate specific cities or the overall national population, thus overlooking jurisdictional interactions or differences. In this paper, we propose a multi-agent reinforcement learning (MARL) model, that enables jurisdiction-specific decision analyses but in an environment with cross-jurisdictional epidemiological interactions. In experimental analyses, conduct
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32508;&#21512;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#23545;&#31216;&#33258;&#36866;&#24212;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#20877;&#29616;&#37327;&#23376;&#21147;&#23398;&#35745;&#31639;&#20013;&#30340;&#30005;&#23376;&#28608;&#21457;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#38024;&#23545;&#26356;&#22823;&#26356;&#22797;&#26434;&#20998;&#23376;&#30340;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#26368;&#23567;&#21407;&#23376;&#20013;&#24515;&#22522;&#30784;&#23545;&#24212;&#30340;&#21442;&#25968;&#21270;&#23454;&#29616;&#20102;&#26497;&#22823;&#30340;&#35745;&#31639;&#25928;&#30410;&#12290;</title><link>http://arxiv.org/abs/2311.00844</link><description>&lt;p&gt;
&#36890;&#36807;&#29289;&#29702;&#32422;&#26463;&#30340;&#26426;&#22120;&#23398;&#20064;&#26469;&#30740;&#31350;&#30005;&#23376;&#28608;&#21457;&#24577;
&lt;/p&gt;
&lt;p&gt;
Electronic excited states from physically-constrained machine learning. (arXiv:2311.00844v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00844
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32508;&#21512;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#23545;&#31216;&#33258;&#36866;&#24212;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#20877;&#29616;&#37327;&#23376;&#21147;&#23398;&#35745;&#31639;&#20013;&#30340;&#30005;&#23376;&#28608;&#21457;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#38024;&#23545;&#26356;&#22823;&#26356;&#22797;&#26434;&#20998;&#23376;&#30340;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#26368;&#23567;&#21407;&#23376;&#20013;&#24515;&#22522;&#30784;&#23545;&#24212;&#30340;&#21442;&#25968;&#21270;&#23454;&#29616;&#20102;&#26497;&#22823;&#30340;&#35745;&#31639;&#25928;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#25216;&#26415;&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20110;&#26367;&#20195;&#29289;&#36136;&#30340;&#30005;&#23376;&#32467;&#26500;&#35745;&#31639;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#19968;&#20010;&#30456;&#20851;&#30340;&#38382;&#39064;&#26159;&#65292;&#26426;&#22120;&#23398;&#20064;(ML)&#26159;&#21542;&#24212;&#24403;&#30452;&#25509;&#24212;&#29992;&#20110;&#39044;&#27979;&#25152;&#38656;&#30340;&#24615;&#36136;&#65292;&#36824;&#26159;&#26126;&#30830;&#22320;&#19982;&#29289;&#29702;&#22522;&#30784;&#25805;&#20316;&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#24314;&#27169;&#26041;&#27861;&#30340;&#31034;&#20363;&#65292;&#20854;&#20013;&#23545;&#19968;&#20010;&#26377;&#25928;&#21704;&#23494;&#39039;&#37327;&#30340;&#23545;&#31216;&#33258;&#36866;&#24212;ML&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#20877;&#29616;&#37327;&#23376;&#21147;&#23398;&#35745;&#31639;&#20013;&#30340;&#30005;&#23376;&#28608;&#21457;&#12290;&#25152;&#24471;&#27169;&#22411;&#21487;&#20197;&#39044;&#27979;&#27604;&#20854;&#35757;&#32451;&#38598;&#26356;&#22823;&#26356;&#22797;&#26434;&#30340;&#20998;&#23376;&#65292;&#24182;&#36890;&#36807;&#38388;&#25509;&#38024;&#23545;&#33391;&#22909;&#25910;&#25947;&#35745;&#31639;&#30340;&#36755;&#20986;&#32780;&#20351;&#29992;&#19982;&#26368;&#23567;&#21407;&#23376;&#20013;&#24515;&#22522;&#30784;&#23545;&#24212;&#30340;&#21442;&#25968;&#21270;&#65292;&#23454;&#29616;&#20102;&#26497;&#22823;&#30340;&#35745;&#31639;&#25928;&#30410;&#12290;&#36825;&#20123;&#32467;&#26524;&#24378;&#35843;&#20102;&#23558;&#25968;&#25454;&#39537;&#21160;&#25216;&#26415;&#19982;&#29289;&#29702;&#36817;&#20284;&#30456;&#32467;&#21512;&#30340;&#20248;&#28857;&#65292;&#25552;&#39640;&#20102;ML&#27169;&#22411;&#30340;&#21487;&#36716;&#31227;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#32780;&#19981;&#38656;&#35201;&#26126;&#30830;&#24314;&#31435;&#29289;&#29702;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven techniques are increasingly used to replace electronic-structure calculations of matter. In this context, a relevant question is whether machine learning (ML) should be applied directly to predict the desired properties or be combined explicitly with physically-grounded operations. We present an example of an integrated modeling approach, in which a symmetry-adapted ML model of an effective Hamiltonian is trained to reproduce electronic excitations from a quantum-mechanical calculation. The resulting model can make predictions for molecules that are much larger and more complex than those that it is trained on, and allows for dramatic computational savings by indirectly targeting the outputs of well-converged calculations while using a parameterization corresponding to a minimal atom-centered basis. These results emphasize the merits of intertwining data-driven techniques with physical approximations, improving the transferability and interpretability of ML models without a
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#21333;&#35843;&#27010;&#29575;&#30340;&#23574;&#38160;&#22122;&#38899;&#20108;&#20998;&#26597;&#25214;&#27169;&#22411;&#65292;&#24182;&#32473;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#35299;&#20915;&#39640;&#27010;&#29575;&#34892;&#20026;&#21644;&#23574;&#38160;&#24120;&#25968;&#20004;&#20010;&#25361;&#25112;&#65292;&#21487;&#20197;&#22312;&#32473;&#23450;&#26679;&#26412;&#25968;&#30340;&#24773;&#20917;&#19979;&#20197;&#39640;&#27010;&#29575;&#25104;&#21151;&#22320;&#25214;&#21040;&#27010;&#29575;&#20132;&#21449;&#30340;&#22320;&#26041;&#12290;</title><link>http://arxiv.org/abs/2311.00840</link><description>&lt;p&gt;
&#24102;&#26377;&#21333;&#35843;&#27010;&#29575;&#30340;&#23574;&#38160;&#22122;&#38899;&#20108;&#20998;&#26597;&#25214;
&lt;/p&gt;
&lt;p&gt;
Sharp Noisy Binary Search with Monotonic Probabilities. (arXiv:2311.00840v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00840
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#21333;&#35843;&#27010;&#29575;&#30340;&#23574;&#38160;&#22122;&#38899;&#20108;&#20998;&#26597;&#25214;&#27169;&#22411;&#65292;&#24182;&#32473;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#35299;&#20915;&#39640;&#27010;&#29575;&#34892;&#20026;&#21644;&#23574;&#38160;&#24120;&#25968;&#20004;&#20010;&#25361;&#25112;&#65292;&#21487;&#20197;&#22312;&#32473;&#23450;&#26679;&#26412;&#25968;&#30340;&#24773;&#20917;&#19979;&#20197;&#39640;&#27010;&#29575;&#25104;&#21151;&#22320;&#25214;&#21040;&#27010;&#29575;&#20132;&#21449;&#30340;&#22320;&#26041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;Karp&#21644;Kleinberg&#30340;&#22122;&#38899;&#20108;&#20998;&#26597;&#25214;&#27169;&#22411;&#65292;&#22312;&#35813;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#26377;n&#20010;&#26410;&#30693;&#27010;&#29575;p_i&#30340;&#30828;&#24065;&#21487;&#20197;&#32763;&#36716;&#12290;&#30828;&#24065;&#25353;&#29031;&#22686;&#21152;&#30340;p_i&#36827;&#34892;&#25490;&#24207;&#65292;&#24182;&#19988;&#25105;&#20204;&#24819;&#25214;&#21040;&#27010;&#29575;&#22312;&#30446;&#26631;&#20540;&#964;&#65288;&#22312;&#949;&#33539;&#22260;&#20869;&#65289;&#38468;&#36817;&#20132;&#21449;&#30340;&#22320;&#26041;&#12290;&#36825;&#23558;Burnashev&#21644;Zigangirov&#30340;&#22266;&#23450;&#22122;&#38899;&#27169;&#22411;&#25512;&#24191;&#21040;&#30828;&#24065;&#25509;&#36817;&#30446;&#26631;&#26102;&#21487;&#33021;&#26080;&#27861;&#21306;&#20998;&#30340;&#24773;&#20917;&#12290;Karp&#21644;Kleinberg&#34920;&#26126;&#65292;&#23545;&#20110;&#36825;&#20010;&#20219;&#21153;&#65292;&#38656;&#35201;&#21644;&#36275;&#22815;&#30340;&#26679;&#26412;&#25968;&#37327;&#26159;&#920;(1/&#949;^2 log n)&#12290;&#25105;&#20204;&#36890;&#36807;&#35299;&#20915;&#20004;&#20010;&#29702;&#35770;&#19978;&#30340;&#25361;&#25112;&#65306;&#39640;&#27010;&#29575;&#34892;&#20026;&#21644;&#23574;&#38160;&#24120;&#25968;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#22312;\[ \frac{1}{C_{\tau, \varepsilon}} \cdot \left(\lg n + O(\log^{2/3} n \log^{1/3} \frac{1}{\delta} + \log \frac{1}{\delta})\right) \]&#20010;&#26679;&#26412;&#26102;&#20197;&#27010;&#29575;1-&#948;&#25104;&#21151;&#65292;&#20854;&#20013;C_{\tau, \varepsilon}&#26159;&#36825;&#26679;&#30340;&#26368;&#20248;&#24120;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
We revisit the noisy binary search model of Karp and Kleinberg, in which we have $n$ coins with unknown probabilities $p_i$ that we can flip. The coins are sorted by increasing $p_i$, and we would like to find where the probability crosses (to within $\varepsilon$) of a target value $\tau$. This generalized the fixed-noise model of Burnashev and Zigangirov , in which $p_i = \frac{1}{2} \pm \varepsilon$, to a setting where coins near the target may be indistinguishable from it. Karp and Kleinberg showed that $\Theta(\frac{1}{\varepsilon^2} \log n)$ samples are necessary and sufficient for this task.  We produce a practical algorithm by solving two theoretical challenges: high-probability behavior and sharp constants. We give an algorithm that succeeds with probability $1-\delta$ from  \[  \frac{1}{C_{\tau, \varepsilon}} \cdot \left(\lg n + O(\log^{2/3} n \log^{1/3} \frac{1}{\delta} + \log \frac{1}{\delta})\right)  \]  samples, where $C_{\tau, \varepsilon}$ is the optimal such constant a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;&#19968;&#31867;&#38750;&#20984;&#20248;&#21270;&#23454;&#20363;&#30340;&#37327;&#23376;-&#32463;&#20856;&#24615;&#33021;&#24046;&#24322;&#65292;&#29992;&#37327;&#23376;&#21704;&#23494;&#39039;&#19979;&#38477;&#31639;&#27861;&#21487;&#20197;&#39640;&#25928;&#35299;&#20915;&#65292;&#32780;&#32463;&#20856;&#31639;&#27861;&#21017;&#38656;&#35201;&#36229;&#22810;&#39033;&#24335;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2311.00811</link><description>&lt;p&gt;
&#22312;&#38750;&#20984;&#20248;&#21270;&#20013;&#35266;&#23519;&#21040;&#30340;&#37327;&#23376;-&#32463;&#20856;&#24615;&#33021;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
A quantum-classical performance separation in nonconvex optimization. (arXiv:2311.00811v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00811
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#19968;&#31867;&#38750;&#20984;&#20248;&#21270;&#23454;&#20363;&#30340;&#37327;&#23376;-&#32463;&#20856;&#24615;&#33021;&#24046;&#24322;&#65292;&#29992;&#37327;&#23376;&#21704;&#23494;&#39039;&#19979;&#38477;&#31639;&#27861;&#21487;&#20197;&#39640;&#25928;&#35299;&#20915;&#65292;&#32780;&#32463;&#20856;&#31639;&#27861;&#21017;&#38656;&#35201;&#36229;&#22810;&#39033;&#24335;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30830;&#23450;&#19968;&#31867;&#38750;&#20984;&#36830;&#32493;&#20248;&#21270;&#23454;&#20363;&#65292;&#27599;&#20010;d&#32500;&#23454;&#20363;&#20855;&#26377;2^d&#20010;&#23616;&#37096;&#26368;&#23567;&#20540;&#65292;&#26469;&#23637;&#31034;&#20102;&#37327;&#23376;-&#32463;&#20856;&#24615;&#33021;&#24046;&#24322;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;&#37327;&#23376;&#21704;&#23494;&#39039;&#19979;&#38477;&#65288;QHD&#65289;&#31639;&#27861;[Leng&#31561;&#20154;&#65292;arXiv:2303.01471]&#33021;&#22815;&#20351;&#29992;&#932;(ilda{O}(d^3))&#30340;&#37327;&#23376;&#26597;&#35810;&#20989;&#25968;&#20540;&#21644;&#932;(ilda{O}(d^4))&#20010;&#38468;&#21152;&#30340;1&#27604;&#29305;&#21644;2&#27604;&#29305;&#22522;&#26412;&#37327;&#23376;&#38376;&#26469;&#35299;&#20915;&#35813;&#31867;&#23454;&#20363;&#30340;&#20219;&#24847;d&#32500;&#38382;&#39064;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#19968;&#39033;&#20840;&#38754;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#20195;&#34920;&#24615;&#30340;&#26368;&#20808;&#36827;&#30340;&#32463;&#20856;&#20248;&#21270;&#31639;&#27861;/&#27714;&#35299;&#22120;&#65288;&#21253;&#25324;Gurobi&#65289;&#38656;&#35201;&#36229;&#22810;&#39033;&#24335;&#26102;&#38388;&#26469;&#35299;&#20915;&#36825;&#31867;&#20248;&#21270;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we identify a family of nonconvex continuous optimization instances, each $d$-dimensional instance with $2^d$ local minima, to demonstrate a quantum-classical performance separation. Specifically, we prove that the recently proposed Quantum Hamiltonian Descent (QHD) algorithm [Leng et al., arXiv:2303.01471] is able to solve any $d$-dimensional instance from this family using $\widetilde{\mathcal{O}}(d^3)$ quantum queries to the function value and $\widetilde{\mathcal{O}}(d^4)$ additional 1-qubit and 2-qubit elementary quantum gates. On the other side, a comprehensive empirical study suggests that representative state-of-the-art classical optimization algorithms/solvers (including Gurobi) would require a super-polynomial time to solve such optimization instances.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25913;&#21892;&#22522;&#20110;&#23494;&#24230;&#30340;&#36229;&#20986;&#20998;&#24067;&#25935;&#24863;&#24615;&#30340;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#35757;&#32451;&#26041;&#27861;&#65292;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25928;&#26524;&#65292;&#23588;&#20854;&#22312;&#36828;&#31163;&#20998;&#24067;&#30340;&#20219;&#21153;&#19978;&#23558;&#30456;&#23545;&#39532;&#21704;&#25289;&#35834;&#27604;&#26031;&#36317;&#31163;&#26041;&#27861;&#30340;&#35823;&#26816;&#29575;&#38477;&#20302;&#36229;&#36807;50%&#12290;</title><link>http://arxiv.org/abs/2311.00808</link><description>&lt;p&gt;
Mahalanobis&#24863;&#30693;&#35757;&#32451;&#29992;&#20110;&#24322;&#24120;&#26679;&#26412;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Mahalanobis-Aware Training for Out-of-Distribution Detection. (arXiv:2311.00808v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25913;&#21892;&#22522;&#20110;&#23494;&#24230;&#30340;&#36229;&#20986;&#20998;&#24067;&#25935;&#24863;&#24615;&#30340;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#35757;&#32451;&#26041;&#27861;&#65292;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25928;&#26524;&#65292;&#23588;&#20854;&#22312;&#36828;&#31163;&#20998;&#24067;&#30340;&#20219;&#21153;&#19978;&#23558;&#30456;&#23545;&#39532;&#21704;&#25289;&#35834;&#27604;&#26031;&#36317;&#31163;&#26041;&#27861;&#30340;&#35823;&#26816;&#29575;&#38477;&#20302;&#36229;&#36807;50%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#21463;&#25511;&#29615;&#22659;&#20013;&#21462;&#24471;&#20102;&#24191;&#27867;&#25104;&#21151;&#65292;&#20294;&#22312;&#24320;&#25918;&#19990;&#30028;&#29615;&#22659;&#20013;&#20351;&#29992;&#20173;&#23384;&#22312;&#19968;&#20123;&#38556;&#30861;&#12290;&#30830;&#20445;&#23433;&#20840;&#37096;&#32626;&#30340;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#26159;&#26816;&#27979;&#21487;&#33021;&#38656;&#35201;&#20154;&#24037;&#24178;&#39044;&#30340;&#24322;&#24120;&#25110;&#36229;&#20986;&#20998;&#24067;&#30340;&#26679;&#26412;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#35757;&#32451;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#22522;&#20110;&#23494;&#24230;&#30340;&#36229;&#20986;&#20998;&#24067;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#23588;&#20854;&#22312;&#36828;&#31163;&#20998;&#24067;&#30340;&#20219;&#21153;&#19978;&#65292;&#30456;&#23545;&#39532;&#21704;&#25289;&#35834;&#27604;&#26031;&#36317;&#31163;&#26041;&#27861;&#30340;&#35823;&#26816;&#29575;&#38477;&#20302;&#36229;&#36807;50%&#12290;
&lt;/p&gt;
&lt;p&gt;
While deep learning models have seen widespread success in controlled environments, there are still barriers to their adoption in open-world settings. One critical task for safe deployment is the detection of anomalous or out-of-distribution samples that may require human intervention. In this work, we present a novel loss function and recipe for training networks with improved density-based out-of-distribution sensitivity. We demonstrate the effectiveness of our method on CIFAR-10, notably reducing the false-positive rate of the relative Mahalanobis distance method on far-OOD tasks by over 50%.
&lt;/p&gt;</description></item><item><title>VQA-GEN&#26159;&#39318;&#20010;&#29992;&#20110;&#39046;&#22495;&#36890;&#29992;&#21270;&#30340;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;VQA&#26041;&#27861;&#23545;&#20110;&#32852;&#21512;&#22810;&#27169;&#24577;&#20998;&#24067;&#21464;&#21270;&#30340;&#33030;&#24369;&#24615;&#65292;&#39564;&#35777;&#20102;&#20840;&#38754;&#30340;&#22810;&#27169;&#24577;&#21464;&#21270;&#23545;&#20110;VQA&#30340;&#31283;&#20581;&#24615;&#36890;&#29992;&#21270;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2311.00807</link><description>&lt;p&gt;
VQA-GEN:&#19968;&#20010;&#29992;&#20110;&#39046;&#22495;&#36890;&#29992;&#21270;&#30340;&#35270;&#35273;&#38382;&#31572;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
VQA-GEN: A Visual Question Answering Benchmark for Domain Generalization. (arXiv:2311.00807v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00807
&lt;/p&gt;
&lt;p&gt;
VQA-GEN&#26159;&#39318;&#20010;&#29992;&#20110;&#39046;&#22495;&#36890;&#29992;&#21270;&#30340;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;VQA&#26041;&#27861;&#23545;&#20110;&#32852;&#21512;&#22810;&#27169;&#24577;&#20998;&#24067;&#21464;&#21270;&#30340;&#33030;&#24369;&#24615;&#65292;&#39564;&#35777;&#20102;&#20840;&#38754;&#30340;&#22810;&#27169;&#24577;&#21464;&#21270;&#23545;&#20110;VQA&#30340;&#31283;&#20581;&#24615;&#36890;&#29992;&#21270;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#27169;&#22411;&#26088;&#22312;&#23637;&#31034;&#35270;&#35273;-&#25991;&#26412;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#32508;&#21512;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#65292;&#23427;&#20204;&#30340;&#23454;&#38469;&#24212;&#29992;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#29616;&#26377;&#30340;VQA&#36890;&#29992;&#24615;&#25968;&#25454;&#38598;&#21333;&#21521;&#20851;&#27880;&#25991;&#26412;&#21464;&#21270;&#65292;&#32780;VQA&#20316;&#20026;&#19968;&#20010;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#21253;&#21547;&#20102;&#35270;&#35273;&#21644;&#25991;&#26412;&#39046;&#22495;&#30340;&#21464;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;VQA-GEN&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#36890;&#36807;&#24341;&#20837;&#21464;&#21270;&#30340;&#27969;&#31243;&#29983;&#25104;&#30340;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;VQA-GEN&#25968;&#25454;&#38598;&#25581;&#31034;&#20102;&#29616;&#26377;&#26041;&#27861;&#23545;&#20110;&#32852;&#21512;&#22810;&#27169;&#24577;&#20998;&#24067;&#21464;&#21270;&#30340;&#33030;&#24369;&#24615;&#65292;&#39564;&#35777;&#20102;&#20840;&#38754;&#30340;&#22810;&#27169;&#24577;&#21464;&#21270;&#23545;&#20110;VQA&#30340;&#31283;&#20581;&#24615;&#36890;&#29992;&#21270;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;VQA-GEN&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#26174;&#31034;&#20986;&#36328;&#39046;&#22495;&#21644;&#39046;&#22495;&#20869;&#24615;&#33021;&#30340;&#25552;&#39640;&#65292;&#35777;&#23454;&#20102;VQA-GEN&#30340;&#20215;&#20540;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#25105;&#20204;&#30340;&#27969;&#31243;&#20013;&#27599;&#20010;&#21464;&#21270;&#25216;&#26415;&#23545;&#20110;&#27169;&#22411;&#36890;&#29992;&#21270;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual question answering (VQA) models are designed to demonstrate visual-textual reasoning capabilities. However, their real-world applicability is hindered by a lack of comprehensive benchmark datasets. Existing domain generalization datasets for VQA exhibit a unilateral focus on textual shifts while VQA being a multi-modal task contains shifts across both visual and textual domains. We propose VQA-GEN, the first ever multi-modal benchmark dataset for distribution shift generated through a shift induced pipeline. Experiments demonstrate VQA-GEN dataset exposes the vulnerability of existing methods to joint multi-modal distribution shifts. validating that comprehensive multi-modal shifts are critical for robust VQA generalization. Models trained on VQA-GEN exhibit improved cross-domain and in-domain performance, confirming the value of VQA-GEN. Further, we analyze the importance of each shift technique of our pipeline contributing to the generalization of the model.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39063;&#31890;&#26448;&#26009;&#25805;&#20316;&#30340;&#23398;&#20064;&#21160;&#21147;&#23398;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#20102;&#22522;&#20110;&#23494;&#24230;&#22330;&#30340;&#34920;&#31034;&#26041;&#27861;&#21644;&#20840;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#20855;&#26377;&#21487;&#24494;&#20998;&#30340;&#21160;&#20316;&#28210;&#26579;&#27169;&#22359;&#12290;&#22312;&#27169;&#25311;&#21644;&#23454;&#38469;&#23454;&#39564;&#20013;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#19978;&#37117;&#26174;&#33879;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#29615;&#22659;&#21644;&#20219;&#21153;&#20013;&#20855;&#26377;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2311.00802</link><description>&lt;p&gt;
&#29992;&#20110;&#39063;&#31890;&#29289;&#20307;&#22534;&#31215;&#25805;&#20316;&#30340;&#31070;&#32463;&#22330;&#21160;&#21147;&#23398;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Neural Field Dynamics Model for Granular Object Piles Manipulation. (arXiv:2311.00802v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39063;&#31890;&#26448;&#26009;&#25805;&#20316;&#30340;&#23398;&#20064;&#21160;&#21147;&#23398;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#20102;&#22522;&#20110;&#23494;&#24230;&#22330;&#30340;&#34920;&#31034;&#26041;&#27861;&#21644;&#20840;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#20855;&#26377;&#21487;&#24494;&#20998;&#30340;&#21160;&#20316;&#28210;&#26579;&#27169;&#22359;&#12290;&#22312;&#27169;&#25311;&#21644;&#23454;&#38469;&#23454;&#39564;&#20013;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#19978;&#37117;&#26174;&#33879;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#29615;&#22659;&#21644;&#20219;&#21153;&#20013;&#20855;&#26377;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#39063;&#31890;&#26448;&#26009;&#25805;&#20316;&#21160;&#21147;&#23398;&#27169;&#22411;&#12290;&#21463;&#27969;&#20307;&#21160;&#21147;&#23398;&#20013;&#24120;&#29992;&#30340;&#27431;&#25289;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#19968;&#20010;&#23436;&#20840;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#21487;&#20197;&#22312;&#22522;&#20110;&#23494;&#24230;&#22330;&#30340;&#29289;&#20307;&#22534;&#31215;&#21644;&#25512;&#21160;&#22120;&#34920;&#31034;&#19978;&#25805;&#20316;&#65292;&#20351;&#20854;&#33021;&#22815;&#21033;&#29992;&#29289;&#20307;&#20043;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#31354;&#38388;&#23616;&#37096;&#24615;&#20197;&#21450;&#36890;&#36807;&#21367;&#31215;&#25805;&#20316;&#36827;&#34892;&#30340;&#24179;&#31227;&#31561;&#21464;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#21487;&#24494;&#34892;&#20026;&#28210;&#26579;&#27169;&#22359;&#20351;&#27169;&#22411;&#23436;&#20840;&#21487;&#24494;&#20998;&#65292;&#24182;&#21487;&#20197;&#30452;&#25509;&#19982;&#22522;&#20110;&#26799;&#24230;&#30340;&#36712;&#36857;&#20248;&#21270;&#31639;&#27861;&#38598;&#25104;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#27169;&#25311;&#21644;&#23454;&#38469;&#19990;&#30028;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#23485;&#33539;&#22260;&#30340;&#22534;&#31215;&#25805;&#20316;&#20219;&#21153;&#20013;&#26126;&#26174;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#22522;&#20110;&#28508;&#21464;&#37327;&#25110;&#22522;&#20110;&#31890;&#23376;&#30340;&#26041;&#27861;&#65292;&#22312;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#19978;&#37117;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#29615;&#22659;&#21644;&#20219;&#21153;&#20043;&#38388;&#20855;&#26377;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a learning-based dynamics model for granular material manipulation. Inspired by the Eulerian approach commonly used in fluid dynamics, our method adopts a fully convolutional neural network that operates on a density field-based representation of object piles and pushers, allowing it to exploit the spatial locality of inter-object interactions as well as the translation equivariance through convolution operations. Furthermore, our differentiable action rendering module makes the model fully differentiable and can be directly integrated with a gradient-based trajectory optimization algorithm. We evaluate our model with a wide array of piles manipulation tasks both in simulation and real-world experiments and demonstrate that it significantly exceeds existing latent or particle-based methods in both accuracy and computation efficiency, and exhibits zero-shot generalization capabilities across various environments and tasks.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20043;&#38388;&#39640;&#25928;&#36801;&#31227;&#27979;&#35797;&#38598;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#20855;&#26377;&#29992;&#25143;&#24863;&#20852;&#36259;&#30340;&#23646;&#24615;&#30340;&#33391;&#22909;&#27979;&#35797;&#38598;&#65292;&#20197;&#36798;&#21040;&#25913;&#21892;&#21487;&#39564;&#35777;&#24615;&#21644;&#27979;&#35797;&#24615;&#30340;&#30446;&#30340;&#12290;</title><link>http://arxiv.org/abs/2311.00801</link><description>&lt;p&gt;
GIST: &#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#29983;&#25104;&#36755;&#20837;&#38598;&#21512;&#30340;&#21487;&#36801;&#31227;&#24615;
&lt;/p&gt;
&lt;p&gt;
GIST: Generated Inputs Sets Transferability in Deep Learning. (arXiv:2311.00801v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00801
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20043;&#38388;&#39640;&#25928;&#36801;&#31227;&#27979;&#35797;&#38598;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#20855;&#26377;&#29992;&#25143;&#24863;&#20852;&#36259;&#30340;&#23646;&#24615;&#30340;&#33391;&#22909;&#27979;&#35797;&#38598;&#65292;&#20197;&#36798;&#21040;&#25913;&#21892;&#21487;&#39564;&#35777;&#24615;&#21644;&#27979;&#35797;&#24615;&#30340;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#31070;&#32463;&#32593;&#32476;&#21487;&#39564;&#35777;&#24615;&#21644;&#21487;&#27979;&#35797;&#24615;&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#21152;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#29983;&#25104;&#27979;&#35797;&#38598;&#30340;&#26041;&#27861;&#34987;&#24320;&#21457;&#20986;&#26469;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25216;&#26415;&#20013;&#30340;&#27599;&#19968;&#31181;&#37117;&#20542;&#21521;&#20110;&#24378;&#35843;&#29305;&#23450;&#30340;&#27979;&#35797;&#26041;&#38754;&#65292;&#24182;&#19988;&#21487;&#33021;&#38750;&#24120;&#32791;&#26102;&#12290;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#20010;&#31616;&#21333;&#35299;&#20915;&#26041;&#26696;&#26159;&#26681;&#25454;&#24076;&#26395;&#36801;&#31227;&#30340;&#26399;&#26395;&#23646;&#24615;&#65292;&#22312;&#19968;&#20123;&#32463;&#36807;&#22522;&#20934;&#27979;&#35797;&#30340;&#27169;&#22411;&#21644;&#26032;&#27979;&#35797;&#27169;&#22411;&#20043;&#38388;&#36716;&#31227;&#27979;&#35797;&#38598;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;GIST&#65288;&#29983;&#25104;&#36755;&#20837;&#38598;&#21512;&#30340;&#21487;&#36801;&#31227;&#24615;&#65289;&#65292;&#19968;&#31181;&#29992;&#20110;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20043;&#38388;&#39640;&#25928;&#36801;&#31227;&#27979;&#35797;&#38598;&#30340;&#26032;&#26041;&#27861;&#12290;&#32473;&#23450;&#29992;&#25143;&#24076;&#26395;&#36801;&#31227;&#30340;&#19968;&#20010;&#24863;&#20852;&#36259;&#30340;&#23646;&#24615;&#65288;&#20363;&#22914;&#65292;&#35206;&#30422;&#20934;&#21017;&#65289;&#65292;GIST&#33021;&#22815;&#20174;&#22522;&#20934;&#25552;&#20379;&#30340;&#21487;&#29992;&#27979;&#35797;&#38598;&#20013;&#65292;&#20174;&#35813;&#23646;&#24615;&#30340;&#35282;&#24230;&#36873;&#25321;&#33391;&#22909;&#30340;&#27979;&#35797;&#38598;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#31181;&#27169;&#24577;&#21644;&#19981;&#21516;&#30340;&#27979;&#35797;&#38598;&#29983;&#25104;&#36807;&#31243;&#65292;&#22312;&#25925;&#38556;&#31867;&#22411;&#35206;&#30422;&#23646;&#24615;&#19978;&#23545;GIST&#36827;&#34892;&#32463;&#39564;&#35780;&#20272;&#65292;&#20197;&#35777;&#26126;&#35813;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the demand for verifiability and testability of neural networks continues to rise, an increasing number of methods for generating test sets are being developed. However, each of these techniques tends to emphasize specific testing aspects and can be quite time-consuming. A straightforward solution to mitigate this issue is to transfer test sets between some benchmarked models and a new model under test, based on a desirable property one wishes to transfer. This paper introduces GIST (Generated Inputs Sets Transferability), a novel approach for the efficient transfer of test sets among Deep Learning models. Given a property of interest that a user wishes to transfer (e.g., coverage criterion), GIST enables the selection of good test sets from the point of view of this property among available ones from a benchmark. We empirically evaluate GIST on fault types coverage property with two modalities and different test set generation procedures to demonstrate the approach's feasibility. E
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#30340;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;&#33258;&#36866;&#24212;&#26131;&#24863;-&#24863;&#26579;-&#26131;&#24863;&#27969;&#34892;&#30149;&#23398;&#32593;&#32476;&#30340;&#20020;&#30028;&#36716;&#25240;&#28857;&#38598;&#20307;&#21160;&#21147;&#23398;&#12290;&#20182;&#20204;&#35782;&#21035;&#20986;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#20197;&#25551;&#36848;&#32593;&#32476;&#30340;&#28436;&#21270;&#34892;&#20026;&#65292;&#24182;&#35266;&#23519;&#21040;&#20102;&#32597;&#35265;&#30340;&#22823;&#24133;&#24230;&#38598;&#20307;&#25391;&#33633;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2311.00797</link><description>&lt;p&gt;
&#28436;&#21270;&#27969;&#34892;&#30149;&#23398;&#32593;&#32476;&#30340;&#20020;&#30028;&#36716;&#25240;&#28857;&#65306;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#30340;&#25968;&#25454;&#39537;&#21160;&#26377;&#25928;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Tipping Points of Evolving Epidemiological Networks: Machine Learning-Assisted, Data-Driven Effective Modeling. (arXiv:2311.00797v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00797
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#30340;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;&#33258;&#36866;&#24212;&#26131;&#24863;-&#24863;&#26579;-&#26131;&#24863;&#27969;&#34892;&#30149;&#23398;&#32593;&#32476;&#30340;&#20020;&#30028;&#36716;&#25240;&#28857;&#38598;&#20307;&#21160;&#21147;&#23398;&#12290;&#20182;&#20204;&#35782;&#21035;&#20986;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#20197;&#25551;&#36848;&#32593;&#32476;&#30340;&#28436;&#21270;&#34892;&#20026;&#65292;&#24182;&#35266;&#23519;&#21040;&#20102;&#32597;&#35265;&#30340;&#22823;&#24133;&#24230;&#38598;&#20307;&#25391;&#33633;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#12289;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#30340;&#26041;&#24335;&#30740;&#31350;&#33258;&#36866;&#24212;&#26131;&#24863;-&#24863;&#26579;-&#26131;&#24863;(SIS)&#27969;&#34892;&#30149;&#23398;&#32593;&#32476;&#30340;&#20020;&#30028;&#36716;&#25240;&#28857;&#38598;&#20307;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#36890;&#36807;&#21463;&#25968;&#20540;&#38543;&#26426;&#31215;&#20998;&#22120;&#21551;&#21457;&#30340;&#28145;&#24230;&#23398;&#20064;ResNet&#26550;&#26500;&#65292;&#35782;&#21035;&#20986;&#19968;&#20010;&#21442;&#25968;&#30456;&#20851;&#30340;&#22522;&#20110;&#29289;&#29702;&#24847;&#20041;&#30340;&#31895;&#31890;&#24230;&#22343;&#22330;&#21464;&#37327;&#30340;&#26377;&#25928;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;(eSDE)&#12290;&#25105;&#20204;&#22522;&#20110;eSDE&#30340;&#30830;&#23450;&#20559;&#31227;&#39033;&#26500;&#24314;&#20102;&#19968;&#20010;&#36817;&#20284;&#30340;&#26377;&#25928;&#20998;&#23700;&#22270;&#65292;&#24182;&#23558;&#20854;&#19982;&#22343;&#22330;SIS&#27169;&#22411;&#30340;&#20998;&#23700;&#22270;&#36827;&#34892;&#23545;&#27604;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#28436;&#21270;&#32593;&#32476;&#30340;&#26377;&#25928;SIS&#21160;&#21147;&#23398;&#20013;&#30340;&#27425;&#20020;&#30028;Hopf&#20998;&#23700;&#65292;&#23427;&#24341;&#36215;&#20102;&#20020;&#30028;&#36716;&#25240;&#34892;&#20026;&#65307;&#36825;&#34920;&#29616;&#20026;&#22823;&#24133;&#24230;&#30340;&#38598;&#20307;&#25391;&#33633;&#65292;&#23427;&#20204;&#20174;(&#22122;&#22768;&#30340;)&#22266;&#23450;&#29366;&#24577;&#30340;&#37051;&#22495;&#20013;&#33258;&#21457;&#22320;&#12289;&#32597;&#35265;&#22320;&#20986;&#29616;&#12290;&#25105;&#20204;&#36890;&#36807;&#37325;&#22797;&#30340;&#26292;&#21147;&#27169;&#25311;&#21644;&#20351;&#29992;&#24050;&#24314;&#31435;&#30340;&#25968;&#23398;&#24037;&#20855;&#30740;&#31350;&#20102;&#36825;&#20123;&#32597;&#35265;&#20107;&#20214;&#30340;&#32479;&#35745;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the tipping point collective dynamics of an adaptive susceptible-infected-susceptible (SIS) epidemiological network in a data-driven, machine learning-assisted manner. We identify a parameter-dependent effective stochastic differential equation (eSDE) in terms of physically meaningful coarse mean-field variables through a deep-learning ResNet architecture inspired by numerical stochastic integrators. We construct an approximate effective bifurcation diagram based on the identified drift term of the eSDE and contrast it with the mean-field SIS model bifurcation diagram. We observe a subcritical Hopf bifurcation in the evolving network's effective SIS dynamics, that causes the tipping point behavior; this takes the form of large amplitude collective oscillations that spontaneously -- yet rarely -arise from the neighborhood of a (noisy) stationary state. We study the statistics of these rare events both through repeated brute force simulations and by using established mathemati
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#26102;&#38388;&#30456;&#20851;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#23558;&#30005;&#23376;&#38459;&#27490;&#33021;&#21147;&#39044;&#27979;&#21152;&#24555;&#20102;1000&#19975;&#20493;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#21407;&#23376;&#32454;&#33410;&#22914;&#20309;&#24433;&#21709;&#30005;&#23376;&#38459;&#27490;&#30340;&#23453;&#36149;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2311.00787</link><description>&lt;p&gt;
&#20351;&#29992;&#26102;&#38388;&#30456;&#20851;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#21644;&#26426;&#22120;&#23398;&#20064;&#23558;&#30005;&#23376;&#38459;&#27490;&#33021;&#21147;&#39044;&#27979;&#21152;&#24555;&#20102;1000&#19975;&#20493;
&lt;/p&gt;
&lt;p&gt;
Accelerating Electronic Stopping Power Predictions by 10 Million Times with a Combination of Time-Dependent Density Functional Theory and Machine Learning. (arXiv:2311.00787v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00787
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26102;&#38388;&#30456;&#20851;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#23558;&#30005;&#23376;&#38459;&#27490;&#33021;&#21147;&#39044;&#27979;&#21152;&#24555;&#20102;1000&#19975;&#20493;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#21407;&#23376;&#32454;&#33410;&#22914;&#20309;&#24433;&#21709;&#30005;&#23376;&#38459;&#27490;&#30340;&#23453;&#36149;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#36947;&#31890;&#23376;&#36752;&#23556;&#22312;&#26448;&#26009;&#20013;&#37322;&#25918;&#33021;&#37327;&#30340;&#36895;&#29575;&#65292;&#20063;&#23601;&#26159;&#38459;&#27490;&#33021;&#21147;&#65292;&#23545;&#20110;&#35774;&#35745;&#26680;&#21453;&#24212;&#22534;&#12289;&#21307;&#30103;&#27835;&#30103;&#12289;&#21322;&#23548;&#20307;&#21644;&#37327;&#23376;&#26448;&#26009;&#20197;&#21450;&#35768;&#22810;&#20854;&#20182;&#25216;&#26415;&#37117;&#26159;&#20851;&#38190;&#12290;&#34429;&#28982;&#20851;&#20110;&#38459;&#27490;&#33021;&#21147;&#30340;&#26680;&#36129;&#29486;&#65292;&#21363;&#21407;&#23376;&#20043;&#38388;&#30340;&#24377;&#24615;&#25955;&#23556;&#65292;&#22312;&#25991;&#29486;&#20013;&#24050;&#32463;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#29702;&#35299;&#65292;&#20294;&#26159;&#33719;&#21462;&#30005;&#23376;&#36129;&#29486;&#25968;&#25454;&#30340;&#36884;&#24452;&#20960;&#21313;&#24180;&#26469;&#19968;&#30452;&#20195;&#20215;&#39640;&#26114;&#19988;&#20381;&#36182;&#35768;&#22810;&#31616;&#21270;&#20551;&#35774;&#65292;&#21253;&#25324;&#26448;&#26009;&#26159;&#21508;&#21521;&#21516;&#24615;&#30340;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#26102;&#38388;&#30456;&#20851;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#65288;TDDFT&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#23558;&#35780;&#20272;&#26032;&#26448;&#26009;&#30340;&#26102;&#38388;&#32553;&#30701;&#21040;&#20165;&#38656;&#20960;&#20010;&#23567;&#26102;&#65292;&#24182;&#25552;&#20379;&#26377;&#20851;&#21407;&#23376;&#32454;&#33410;&#22914;&#20309;&#24433;&#21709;&#30005;&#23376;&#38459;&#27490;&#30340;&#23453;&#36149;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;TDDFT&#26469;&#20174;&#31532;&#19968;&#21407;&#29702;&#35745;&#31639;&#30005;&#23376;&#38459;&#27490;&#23545;&#38459;&#27490;&#33021;&#21147;&#30340;&#36129;&#29486;&#65292;&#24182;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25554;&#20540;&#21040;&#20854;&#20182;&#26041;&#21521;&#65292;&#36895;&#24230;&#25552;&#39640;&#20102;1000&#19975;&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowing the rate at which particle radiation releases energy in a material, the stopping power, is key to designing nuclear reactors, medical treatments, semiconductor and quantum materials, and many other technologies. While the nuclear contribution to stopping power, i.e., elastic scattering between atoms, is well understood in the literature, the route for gathering data on the electronic contribution has for decades remained costly and reliant on many simplifying assumptions, including that materials are isotropic. We establish a method that combines time-dependent density functional theory (TDDFT) and machine learning to reduce the time to assess new materials to mere hours on a supercomputer and provides valuable data on how atomic details influence electronic stopping. Our approach uses TDDFT to compute the electronic stopping contributions to stopping power from first principles in several directions and then machine learning to interpolate to other directions at rates 10 milli
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#19988;&#20934;&#30830;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;GCM&#20013;&#37325;&#21472;&#30340;&#36879;&#26126;&#24230;&#29289;&#31181;&#65292;&#36890;&#36807;&#26377;&#25928;&#22320;&#32467;&#21512;&#21508;&#20010;&#30456;&#20851;-k&#36879;&#26126;&#24230;&#34920;&#65288;k-tables&#65289;&#65292;&#35813;&#26041;&#27861;&#22312;&#28909;&#26408;&#26143;HD~209458 b&#30340;&#27169;&#25311;&#20013;&#34920;&#29616;&#20986;&#20102;&#31934;&#30830;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00775</link><description>&lt;p&gt;
&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#20934;&#30830;&#22788;&#29702;GCM&#20013;&#37325;&#21472;&#36879;&#26126;&#24230;&#29289;&#31181;&#30340;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Harnessing machine learning for accurate treatment of overlapping opacity species in GCMs. (arXiv:2311.00775v1 [astro-ph.EP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#19988;&#20934;&#30830;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;GCM&#20013;&#37325;&#21472;&#30340;&#36879;&#26126;&#24230;&#29289;&#31181;&#65292;&#36890;&#36807;&#26377;&#25928;&#22320;&#32467;&#21512;&#21508;&#20010;&#30456;&#20851;-k&#36879;&#26126;&#24230;&#34920;&#65288;k-tables&#65289;&#65292;&#35813;&#26041;&#27861;&#22312;&#28909;&#26408;&#26143;HD~209458 b&#30340;&#27169;&#25311;&#20013;&#34920;&#29616;&#20986;&#20102;&#31934;&#30830;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#29702;&#35299;&#22806;&#34892;&#26143;&#21644;&#26837;&#30702;&#26143;&#30340;&#39640;&#31934;&#24230;&#35266;&#27979;&#32467;&#26524;&#65292;&#25105;&#20204;&#38656;&#35201;&#35814;&#32454;&#21644;&#22797;&#26434;&#30340;&#28085;&#30422;&#20102;&#27969;&#20307;&#21147;&#23398;&#12289;&#21270;&#23398;&#21644;&#36752;&#23556;&#30340;&#36890;&#29992;&#29615;&#27969;&#27169;&#22411;&#65288;GCMs&#65289;&#12290;&#26412;&#30740;&#31350;&#20855;&#20307;&#32771;&#23519;&#20102;GCMs&#20013;&#21270;&#23398;&#21644;&#36752;&#23556;&#20043;&#38388;&#30340;&#32806;&#21512;&#20851;&#31995;&#65292;&#24182;&#27604;&#36739;&#20102;&#22312;&#30456;&#20851;-k&#20551;&#35774;&#20013;&#28151;&#21512;&#19981;&#21516;&#21270;&#23398;&#29289;&#31181;&#36879;&#26126;&#24230;&#30340;&#19981;&#21516;&#26041;&#27861;&#65292;&#22312;&#19981;&#33021;&#20551;&#35774;&#24179;&#34913;&#21270;&#23398;&#21453;&#24212;&#30340;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;DeepSets&#65288;DS&#65289;&#30340;&#24555;&#36895;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#32467;&#21512;&#20102;&#21508;&#20010;&#30456;&#20851;-k&#36879;&#26126;&#24230;&#34920;&#65288;k-tables&#65289;&#12290;&#25105;&#20204;&#23558;DS&#26041;&#27861;&#19982;&#20854;&#20182;&#24050;&#21457;&#34920;&#30340;&#26041;&#27861;&#22914;&#33258;&#36866;&#24212;&#31561;&#20215;&#28040;&#20809;&#65288;AEE&#65289;&#21644;&#24102;&#26377;&#37325;&#26032;&#20998;&#32452;&#21644;&#25490;&#24207;&#30340;&#38543;&#26426;&#37325;&#21472;&#65288;RORR&#65289;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#28151;&#21512;&#26041;&#27861;&#25972;&#21512;&#21040;&#20102;&#25105;&#20204;&#30340;GCM (expeRT/MITgcm)&#20013;&#65292;&#24182;&#23545;&#28909;&#26408;&#26143;HD~209458 b&#36827;&#34892;&#20102;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;DS&#26041;&#27861;&#22312;GCM&#20351;&#29992;&#26102;&#26082;&#20934;&#30830;&#21448;&#39640;&#25928;&#65292;&#32780;RORR&#26041;&#27861;&#21017;&#19981;&#28982;&#12290;
&lt;/p&gt;
&lt;p&gt;
To understand high precision observations of exoplanets and brown dwarfs, we need detailed and complex general circulation models (GCMs) that incorporate hydrodynamics, chemistry, and radiation. In this study, we specifically examine the coupling between chemistry and radiation in GCMs and compare different methods for mixing opacities of different chemical species in the correlated-k assumption, when equilibrium chemistry cannot be assumed. We propose a fast machine learning method based on DeepSets (DS), which effectively combines individual correlated-k opacities (k-tables). We evaluate the DS method alongside other published methods like adaptive equivalent extinction (AEE) and random overlap with rebinning and resorting (RORR). We integrate these mixing methods into our GCM (expeRT/MITgcm) and assess their accuracy and performance for the example of the hot Jupiter HD~209458 b. Our findings indicate that the DS method is both accurate and efficient for GCM usage, whereas RORR is t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19968;&#33268;&#21270;&#22238;&#24402;&#26041;&#27861;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#26679;&#26465;&#20272;&#35745;&#26465;&#20214;&#23494;&#24230;&#30340;&#26679;&#26465;&#39044;&#27979;&#21306;&#38388;&#65292;&#35777;&#26126;&#20102;&#20854;&#26222;&#36866;&#36924;&#36817;&#21644;&#26368;&#20248;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2311.00774</link><description>&lt;p&gt;
&#28145;&#24230;&#26679;&#26465;&#22312;&#26368;&#20248;&#21644;&#39640;&#25928;&#39044;&#27979;&#38598;&#20013;&#30340;&#19968;&#33268;&#21270;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Conformalized Deep Splines for Optimal and Efficient Prediction Sets. (arXiv:2311.00774v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00774
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19968;&#33268;&#21270;&#22238;&#24402;&#26041;&#27861;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#26679;&#26465;&#20272;&#35745;&#26465;&#20214;&#23494;&#24230;&#30340;&#26679;&#26465;&#39044;&#27979;&#21306;&#38388;&#65292;&#35777;&#26126;&#20102;&#20854;&#26222;&#36866;&#36924;&#36817;&#21644;&#26368;&#20248;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#39118;&#38505;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#65292;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#19968;&#31181;&#26377;&#25928;&#30340;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#26159;&#19968;&#33268;&#21270;&#39044;&#27979;&#65292;&#23427;&#21487;&#20197;&#25552;&#20379;&#24102;&#26377;&#32479;&#35745;&#35206;&#30422;&#20445;&#35777;&#30340;&#39044;&#27979;&#25512;&#26029;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19968;&#33268;&#21270;&#22238;&#24402;&#26041;&#27861;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#26679;&#26465;&#20272;&#35745;&#26465;&#20214;&#23494;&#24230;&#30340;&#26679;&#26465;&#39044;&#27979;&#21306;&#38388;&#65288;SPICE&#65289;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;SPICE&#30340;&#26222;&#36866;&#36924;&#36817;&#21644;&#26368;&#20248;&#24615;&#32467;&#26524;&#65292;&#24182;&#36890;&#36807;&#25105;&#20204;&#30340;&#23454;&#39564;&#36827;&#34892;&#20102;&#23454;&#35777;&#39564;&#35777;&#12290;SPICE&#20860;&#23481;&#20004;&#31181;&#19981;&#21516;&#30340;&#39640;&#25928;&#35745;&#31639;&#30340;&#19968;&#33268;&#21270;&#35780;&#20998;&#65292;&#19968;&#31181;&#26159;&#23545;&#20110;&#36793;&#38469;&#35206;&#30422;&#29575;&#30340;&#29702;&#35770;&#26368;&#20248;&#65288;SPICE-ND&#65289;&#65292;&#21478;&#19968;&#31181;&#26159;&#23545;&#20110;&#26465;&#20214;&#35206;&#30422;&#29575;&#28176;&#36817;&#26368;&#20248;&#65288;SPICE-HPD&#65289;&#12290;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;SPICE-ND&#27169;&#22411;&#23454;&#29616;&#20102;&#26368;&#23567;&#30340;&#24179;&#22343;&#39044;&#27979;&#38598;&#22823;&#23567;&#65292;&#26576;&#20123;&#25968;&#25454;&#38598;&#30340;&#24179;&#22343;&#22823;&#23567;&#20943;&#23569;&#20102;&#36817;50%&#65292;&#19982;&#20854;&#20182;&#22522;&#20934;&#30456;&#27604;&#12290;SPICE-HPD&#27169;&#22411;&#23454;&#29616;&#20102;&#26368;&#20339;&#30340;&#26465;&#20214;&#35206;&#30422;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uncertainty estimation is critical in high-stakes machine learning applications. One effective way to estimate uncertainty is conformal prediction, which can provide predictive inference with statistical coverage guarantees. We present a new conformal regression method, Spline Prediction Intervals via Conformal Estimation (SPICE), that estimates the conditional density using neural-network-parameterized splines. We prove universal approximation and optimality results for SPICE, which are empirically validated by our experiments. SPICE is compatible with two different efficient-to-compute conformal scores, one oracle-optimal for marginal coverage (SPICE-ND) and the other asymptotically optimal for conditional coverage (SPICE-HPD). Results on benchmark datasets demonstrate SPICE-ND models achieve the smallest average prediction set sizes, including average size reductions of nearly 50% for some datasets compared to the next best baseline. SPICE-HPD models achieve the best conditional cov
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#33258;&#30417;&#30563;&#35757;&#32451;&#33539;&#24335;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#34920;&#31034;&#23398;&#20064;&#20026;&#20020;&#24202;&#26102;&#38388;&#24207;&#21015;&#25512;&#23548;&#20986;&#39640;&#36136;&#37327;&#30340;&#36890;&#29992;&#20020;&#24202;&#29305;&#24449;&#23884;&#20837;&#12290;&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#38477;&#32500;&#25216;&#26415;&#21487;&#35270;&#21270;&#23398;&#20064;&#21040;&#30340;&#23884;&#20837;&#65292;&#24182;&#22312;MIMIC-III&#22522;&#20934;&#27979;&#35797;&#20013;&#39564;&#35777;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00768</link><description>&lt;p&gt;
&#29992;&#20110;&#20020;&#24202;&#29305;&#24449;&#23884;&#20837;&#30340;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
Language Model Training Paradigms for Clinical Feature Embeddings. (arXiv:2311.00768v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00768
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#33258;&#30417;&#30563;&#35757;&#32451;&#33539;&#24335;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#34920;&#31034;&#23398;&#20064;&#20026;&#20020;&#24202;&#26102;&#38388;&#24207;&#21015;&#25512;&#23548;&#20986;&#39640;&#36136;&#37327;&#30340;&#36890;&#29992;&#20020;&#24202;&#29305;&#24449;&#23884;&#20837;&#12290;&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#38477;&#32500;&#25216;&#26415;&#21487;&#35270;&#21270;&#23398;&#20064;&#21040;&#30340;&#23884;&#20837;&#65292;&#24182;&#22312;MIMIC-III&#22522;&#20934;&#27979;&#35797;&#20013;&#39564;&#35777;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#25454;&#31232;&#32570;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#34920;&#31034;&#23398;&#20064;&#36215;&#30528;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#23545;&#20020;&#24202;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#65292;&#25512;&#23548;&#20986;&#20020;&#24202;&#29305;&#24449;&#65288;&#22914;&#24515;&#29575;&#21644;&#34880;&#21387;&#65289;&#30340;&#36890;&#29992;&#23884;&#20837;&#12290;&#25105;&#20204;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#30417;&#30563;&#35757;&#32451;&#33539;&#24335;&#65292;&#23398;&#20064;&#39640;&#36136;&#37327;&#30340;&#20020;&#24202;&#29305;&#24449;&#23884;&#20837;&#65292;&#23454;&#29616;&#27604;&#29616;&#26377;&#30340;&#26102;&#38388;&#27493;&#21644;&#24739;&#32773;&#32423;&#21035;&#34920;&#31034;&#23398;&#20064;&#26356;&#32454;&#31890;&#24230;&#30340;&#34920;&#24449;&#12290;&#25105;&#20204;&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#38477;&#32500;&#25216;&#26415;&#21487;&#35270;&#21270;&#23398;&#20064;&#21040;&#30340;&#23884;&#20837;&#65292;&#24182;&#35266;&#23519;&#21040;&#19982;&#20808;&#21069;&#30340;&#20020;&#24202;&#30693;&#35782;&#39640;&#24230;&#19968;&#33268;&#12290;&#25105;&#20204;&#36824;&#22312;MIMIC-III&#22522;&#20934;&#27979;&#35797;&#19978;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#20351;&#29992;&#20020;&#24202;&#29305;&#24449;&#23884;&#20837;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#20195;&#30721;&#21457;&#24067;&#22312;&#32593;&#19978;&#20197;&#20379;&#22797;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
In research areas with scarce data, representation learning plays a significant role. This work aims to enhance representation learning for clinical time series by deriving universal embeddings for clinical features, such as heart rate and blood pressure. We use self-supervised training paradigms for language models to learn high-quality clinical feature embeddings, achieving a finer granularity than existing time-step and patient-level representation learning. We visualize the learnt embeddings via unsupervised dimension reduction techniques and observe a high degree of consistency with prior clinical knowledge. We also evaluate the model performance on the MIMIC-III benchmark and demonstrate the effectiveness of using clinical feature embeddings. We publish our code online for replication.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#23398;&#20064;&#19968;&#31181;&#35774;&#35745;&#31574;&#30053;&#26469;&#21046;&#20316;&#36866;&#29992;&#20110;&#19981;&#21516;&#20219;&#21153;&#30340;&#19987;&#29992;&#24037;&#20855;&#65292;&#24182;&#36890;&#36807;&#36825;&#20123;&#24037;&#20855;&#36827;&#34892;&#26426;&#22120;&#20154;&#25805;&#32437;&#12290;&#36825;&#21487;&#20197;&#35299;&#38145;&#26426;&#22120;&#20154;&#30340;&#39069;&#22806;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2311.00754</link><description>&lt;p&gt;
&#23398;&#20064;&#35774;&#35745;&#21644;&#20351;&#29992;&#26426;&#22120;&#20154;&#25805;&#32437;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Learning to Design and Use Tools for Robotic Manipulation. (arXiv:2311.00754v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00754
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#23398;&#20064;&#19968;&#31181;&#35774;&#35745;&#31574;&#30053;&#26469;&#21046;&#20316;&#36866;&#29992;&#20110;&#19981;&#21516;&#20219;&#21153;&#30340;&#19987;&#29992;&#24037;&#20855;&#65292;&#24182;&#36890;&#36807;&#36825;&#20123;&#24037;&#20855;&#36827;&#34892;&#26426;&#22120;&#20154;&#25805;&#32437;&#12290;&#36825;&#21487;&#20197;&#35299;&#38145;&#26426;&#22120;&#20154;&#30340;&#39069;&#22806;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#38754;&#20020;&#33258;&#36523;&#24418;&#24577;&#38480;&#21046;&#26102;&#65292;&#20154;&#31867;&#21644;&#26576;&#20123;&#21160;&#29289;&#31181;&#31867;&#20855;&#26377;&#20351;&#29992;&#29615;&#22659;&#20013;&#30340;&#29289;&#20307;&#26469;&#23436;&#25104;&#21407;&#26412;&#19981;&#21487;&#33021;&#30340;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#26426;&#22120;&#20154;&#21487;&#33021;&#36890;&#36807;&#20351;&#29992;&#24037;&#20855;&#35299;&#38145;&#19968;&#31995;&#21015;&#39069;&#22806;&#30340;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#26469;&#32852;&#21512;&#20248;&#21270;&#24418;&#24577;&#21644;&#25511;&#21046;&#30340;&#25216;&#26415;&#22312;&#35774;&#35745;&#31227;&#21160;&#26426;&#22120;&#20154;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;&#20294;&#26159;&#65292;&#23613;&#31649;&#36755;&#20986;&#19968;&#20010;&#21333;&#19968;&#30340;&#24418;&#24577;&#23545;&#20110;&#31227;&#21160;&#26469;&#35828;&#26159;&#26377;&#24847;&#20041;&#30340;&#65292;&#20294;&#26159;&#25805;&#32437;&#28041;&#21450;&#21040;&#26681;&#25454;&#25163;&#22836;&#30340;&#20219;&#21153;&#30446;&#26631;&#37319;&#29992;&#21508;&#31181;&#31574;&#30053;&#12290;&#19968;&#20010;&#25805;&#32437;&#26426;&#22120;&#20154;&#24517;&#39035;&#33021;&#22815;&#24555;&#36895;&#21046;&#20316;&#20986;&#36866;&#29992;&#20110;&#19981;&#21516;&#30446;&#26631;&#30340;&#19987;&#29992;&#24037;&#20855;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#23398;&#20064;&#19968;&#20010;&#35774;&#35745;&#31574;&#30053;&#65292;&#32780;&#19981;&#26159;&#19968;&#20010;&#21333;&#19968;&#30340;&#35774;&#35745;&#12290;&#35774;&#35745;&#31574;&#30053;&#20197;&#20219;&#21153;&#20449;&#24687;&#20026;&#26465;&#20214;&#65292;&#24182;&#36755;&#20986;&#19968;&#20010;&#26377;&#21161;&#20110;&#35299;&#20915;&#20219;&#21153;&#30340;&#24037;&#20855;&#35774;&#35745;&#12290;&#28982;&#21518;&#65292;&#19968;&#20010;&#20197;&#35774;&#35745;&#26465;&#20214;&#20026;&#22522;&#30784;&#30340;&#25511;&#21046;&#31574;&#30053;&#21487;&#20197;&#20351;&#29992;&#36825;&#20123;&#24037;&#20855;&#36827;&#34892;&#25805;&#32437;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26397;&#30528;&#36825;&#20010;&#30446;&#26631;&#36808;&#20986;&#20102;&#19968;&#27493;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
When limited by their own morphologies, humans and some species of animals have the remarkable ability to use objects from the environment toward accomplishing otherwise impossible tasks. Robots might similarly unlock a range of additional capabilities through tool use. Recent techniques for jointly optimizing morphology and control via deep learning are effective at designing locomotion agents. But while outputting a single morphology makes sense for locomotion, manipulation involves a variety of strategies depending on the task goals at hand. A manipulation agent must be capable of rapidly prototyping specialized tools for different goals. Therefore, we propose learning a designer policy, rather than a single design. A designer policy is conditioned on task information and outputs a tool design that helps solve the task. A design-conditioned controller policy can then perform manipulation using these tools. In this work, we take a step towards this goal by introducing a reinforcement
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#20307;&#20869;&#22312;&#29305;&#24615;&#30340;&#22270;&#20687;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#36890;&#29992;&#23545;&#35937;&#31867;&#21035;&#36827;&#34892;&#25193;&#23637;&#65292;&#24182;&#25910;&#38598;&#20102;&#22823;&#35268;&#27169;&#30340;CUTE&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#35813;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2311.00750</link><description>&lt;p&gt;
&#36825;&#20123;&#26159;&#21516;&#19968;&#20010;&#33529;&#26524;&#21527;&#65311;&#22522;&#20110;&#29289;&#20307;&#20869;&#22312;&#29305;&#24615;&#27604;&#36739;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
Are These the Same Apple? Comparing Images Based on Object Intrinsics. (arXiv:2311.00750v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00750
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#20307;&#20869;&#22312;&#29305;&#24615;&#30340;&#22270;&#20687;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#36890;&#29992;&#23545;&#35937;&#31867;&#21035;&#36827;&#34892;&#25193;&#23637;&#65292;&#24182;&#25910;&#38598;&#20102;&#22823;&#35268;&#27169;&#30340;CUTE&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#35813;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#35270;&#35273;&#31995;&#32479;&#21487;&#20197;&#36731;&#26494;&#35782;&#21035;&#22312;&#19981;&#21516;&#30340;&#22806;&#22312;&#22240;&#32032;&#19979;&#65288;&#22914;&#20809;&#29031;&#12289;&#29289;&#20307;&#23039;&#21183;&#21644;&#32972;&#26223;&#65289;&#30340;&#23545;&#35937;&#65292;&#28982;&#32780;&#24403;&#21069;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#22312;&#36825;&#20123;&#21464;&#24322;&#26041;&#38754;&#32463;&#24120;&#36935;&#21040;&#22256;&#38590;&#12290;&#29702;&#35299;&#21644;&#25913;&#36827;&#20154;&#24037;&#35270;&#35273;&#31995;&#32479;&#30340;&#37325;&#35201;&#19968;&#27493;&#26159;&#32431;&#22522;&#20110;&#23450;&#20041;&#23545;&#35937;&#36523;&#20221;&#30340;&#20869;&#22312;&#29289;&#20307;&#29305;&#24615;&#26469;&#27979;&#37327;&#22270;&#20687;&#30456;&#20284;&#24615;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#25991;&#29486;&#20013;&#24050;&#34987;&#30740;&#31350;&#20026;&#37325;&#26032;&#35782;&#21035;&#65292;&#23613;&#31649;&#20027;&#35201;&#23616;&#38480;&#20110;&#29305;&#23450;&#30340;&#23545;&#35937;&#31867;&#21035;&#22914;&#20154;&#21644;&#27773;&#36710;&#12290;&#25105;&#20204;&#25552;&#20986;&#23558;&#20854;&#25193;&#23637;&#21040;&#36890;&#29992;&#23545;&#35937;&#31867;&#21035;&#65292;&#25506;&#32034;&#22522;&#20110;&#29289;&#20307;&#20869;&#22312;&#29305;&#24615;&#30340;&#22270;&#20687;&#30456;&#20284;&#24230;&#24230;&#37327;&#12290;&#20026;&#20102;&#35780;&#20272;&#36825;&#31181;&#27979;&#37327;&#26041;&#27861;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;Common paired objects Under differenT Extrinsics (CUTE)&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;180&#20010;&#23545;&#35937;&#30340;18000&#24352;&#22270;&#20687;&#65292;&#28085;&#30422;&#20102;&#19981;&#21516;&#30340;&#22806;&#22312;&#22240;&#32032;&#65292;&#22914;&#20809;&#29031;&#12289;&#23039;&#21183;&#21644;&#25104;&#20687;&#26465;&#20214;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#22914;LPIPS&#21644;CLIP&#20998;&#25968;&#24182;&#19981;&#33021;&#24456;&#22909;&#22320;&#27979;&#37327;&#29289;&#20307;&#20869;&#22312;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The human visual system can effortlessly recognize an object under different extrinsic factors such as lighting, object poses, and background, yet current computer vision systems often struggle with these variations. An important step to understanding and improving artificial vision systems is to measure image similarity purely based on intrinsic object properties that define object identity. This problem has been studied in the computer vision literature as re-identification, though mostly restricted to specific object categories such as people and cars. We propose to extend it to general object categories, exploring an image similarity metric based on object intrinsics. To benchmark such measurements, we collect the Common paired objects Under differenT Extrinsics (CUTE) dataset of $18,000$ images of $180$ objects under different extrinsic factors such as lighting, poses, and imaging conditions. While existing methods such as LPIPS and CLIP scores do not measure object intrinsics wel
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#23398;&#20064;&#22686;&#24378;&#30340;&#31639;&#27861;&#35270;&#35282;&#25506;&#32034;&#20102;&#22522;&#20110;&#39044;&#27979;&#30340;&#25490;&#24207;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#26032;&#30340;&#31616;&#21333;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#29702;&#35770;&#19978;&#26368;&#20248;&#30340;&#27604;&#36739;&#22797;&#26434;&#24615;&#65292;&#23545;&#24212;&#29992;&#39044;&#27979;&#25490;&#24207;&#26041;&#27861;&#20855;&#26377;&#28508;&#22312;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2311.00749</link><description>&lt;p&gt;
&#22522;&#20110;&#39044;&#27979;&#30340;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
Sorting with Predictions. (arXiv:2311.00749v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00749
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#23398;&#20064;&#22686;&#24378;&#30340;&#31639;&#27861;&#35270;&#35282;&#25506;&#32034;&#20102;&#22522;&#20110;&#39044;&#27979;&#30340;&#25490;&#24207;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#26032;&#30340;&#31616;&#21333;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#29702;&#35770;&#19978;&#26368;&#20248;&#30340;&#27604;&#36739;&#22797;&#26434;&#24615;&#65292;&#23545;&#24212;&#29992;&#39044;&#27979;&#25490;&#24207;&#26041;&#27861;&#20855;&#26377;&#28508;&#22312;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#23398;&#20064;&#22686;&#24378;&#30340;&#31639;&#27861;&#35270;&#35282;&#25506;&#32034;&#20102;&#25490;&#24207;&#30340;&#22522;&#26412;&#38382;&#39064;&#65292;&#20854;&#20013;&#31639;&#27861;&#21487;&#20197;&#21033;&#29992;&#21487;&#33021;&#23384;&#22312;&#38169;&#35823;&#30340;&#39044;&#27979;&#26469;&#25552;&#39640;&#25928;&#29575;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#24773;&#26223;&#65306;&#22312;&#31532;&#19968;&#31181;&#24773;&#26223;&#20013;&#65292;&#27599;&#20010;&#39033;&#30446;&#37117;&#34987;&#25552;&#20379;&#20102;&#19968;&#20010;&#20854;&#22312;&#25490;&#24207;&#21015;&#34920;&#20013;&#20301;&#32622;&#30340;&#39044;&#27979;&#12290;&#22312;&#31532;&#20108;&#31181;&#24773;&#26223;&#20013;&#65292;&#25105;&#20204;&#20551;&#35774;&#23384;&#22312;&#19968;&#31181;&#8220;&#24555;&#36895;&#19988;&#31895;&#31961;&#8221;&#30340;&#27604;&#36739;&#26041;&#24335;&#65292;&#38500;&#27492;&#20043;&#22806;&#36824;&#23384;&#22312;&#24930;&#32780;&#20934;&#30830;&#30340;&#27604;&#36739;&#26041;&#24335;&#12290;&#23545;&#20110;&#36825;&#20004;&#31181;&#24773;&#26223;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#26032;&#30340;&#31616;&#21333;&#31639;&#27861;&#65292;&#20165;&#21033;&#29992;$O(\sum_i \log \eta_i)$&#20010;&#20934;&#30830;&#27604;&#36739;&#65292;&#20854;&#20013;$\eta_i$&#26159;&#31532;$i$&#20010;&#20803;&#32032;&#30340;&#36866;&#24403;&#23450;&#20041;&#30340;&#39044;&#27979;&#35823;&#24046;&#12290;&#29305;&#21035;&#22320;&#65292;&#38543;&#30528;&#39044;&#27979;&#30340;&#36136;&#37327;&#24694;&#21270;&#65292;&#27604;&#36739;&#30340;&#25968;&#37327;&#20174;$O(n)$&#24179;&#28369;&#22320;&#19979;&#38477;&#21040;$O(n\log n)$&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#27604;&#36739;&#22797;&#26434;&#24615;&#22312;&#25152;&#32771;&#23519;&#30340;&#35823;&#24046;&#24230;&#37327;&#26041;&#38754;&#22312;&#29702;&#35770;&#19978;&#26159;&#26368;&#20248;&#30340;&#12290;&#19982;&#29616;&#26377;&#30340;&#33258;&#36866;&#24212;&#21644;&#38750;&#33258;&#36866;&#24212;&#25490;&#24207;&#31639;&#27861;&#36827;&#34892;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#20102;&#24212;&#29992;&#39044;&#27979;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore the fundamental problem of sorting through the lens of learning-augmented algorithms, where algorithms can leverage possibly erroneous predictions to improve their efficiency. We consider two different settings: In the first setting, each item is provided a prediction of its position in the sorted list. In the second setting, we assume there is a "quick-and-dirty" way of comparing items, in addition to slow-and-exact comparisons. For both settings, we design new and simple algorithms using only $O(\sum_i \log \eta_i)$ exact comparisons, where $\eta_i$ is a suitably defined prediction error for the $i$th element. In particular, as the quality of predictions deteriorates, the number of comparisons degrades smoothly from $O(n)$ to $O(n\log n)$. We prove that the comparison complexity is theoretically optimal with respect to the examined error measures. An experimental evaluation against existing adaptive and non-adaptive sorting algorithms demonstrates the potential of applying
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;DataSculpt&#65292;&#23427;&#26159;&#19968;&#20010;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#26631;&#31614;&#20989;&#25968;&#30340;&#20132;&#20114;&#24335;&#26694;&#26550;&#12290;&#36890;&#36807;&#22810;&#31181;&#25216;&#26415;&#21644;&#26041;&#27861;&#30340;&#32467;&#21512;&#65292;DataSculpt&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20102;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00739</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#35774;&#35745;&#20934;&#30830;&#30340;&#26631;&#31614;&#20989;&#25968;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Design Accurate Label Functions?. (arXiv:2311.00739v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;DataSculpt&#65292;&#23427;&#26159;&#19968;&#20010;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#26631;&#31614;&#20989;&#25968;&#30340;&#20132;&#20114;&#24335;&#26694;&#26550;&#12290;&#36890;&#36807;&#22810;&#31181;&#25216;&#26415;&#21644;&#26041;&#27861;&#30340;&#32467;&#21512;&#65292;DataSculpt&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20102;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32534;&#31243;&#24335;&#24369;&#30417;&#30563;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#23553;&#35013;&#21551;&#21457;&#24335;&#25968;&#25454;&#28304;&#30340;&#26631;&#31614;&#20989;&#25968;&#65288;LFs&#65289;&#21152;&#36895;&#26631;&#35760;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#21019;&#24314;&#31934;&#30830;&#30340;LFs&#38656;&#35201;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#21644;&#22823;&#37327;&#21162;&#21147;&#12290;&#26368;&#36817;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#36827;&#23637;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;PLMs&#33258;&#20027;&#21046;&#23450;&#20934;&#30830;&#30340;LFs&#30340;&#33021;&#21147;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#39046;&#22495;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;DataSculpt&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;PLMs&#33258;&#21160;&#29983;&#25104;LFs&#30340;&#20132;&#20114;&#24335;&#26694;&#26550;&#12290;&#22312;DataSculpt&#20013;&#65292;&#25105;&#20204;&#32467;&#21512;&#20102;&#21508;&#31181;&#25552;&#31034;&#25216;&#26415;&#12289;&#23454;&#20363;&#36873;&#25321;&#31574;&#30053;&#21644;LF&#36807;&#28388;&#26041;&#27861;&#26469;&#25506;&#32034;&#24191;&#38420;&#30340;&#35774;&#35745;&#39046;&#22495;&#12290;&#26368;&#32456;&#65292;&#25105;&#20204;&#23545;DataSculpt&#22312;12&#20010;&#28085;&#30422;&#22810;&#20010;&#20219;&#21153;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#36825;&#20010;&#35780;&#20272;&#25581;&#31034;&#20102;DataSculpt&#30340;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Programmatic weak supervision methodologies facilitate the expedited labeling of extensive datasets through the use of label functions (LFs) that encapsulate heuristic data sources. Nonetheless, the creation of precise LFs necessitates domain expertise and substantial endeavors. Recent advances in pre-trained language models (PLMs) have exhibited substantial potential across diverse tasks. However, the capacity of PLMs to autonomously formulate accurate LFs remains an underexplored domain. In this research, we address this gap by introducing DataSculpt, an interactive framework that harnesses PLMs for the automated generation of LFs. Within DataSculpt, we incorporate an array of prompting techniques, instance selection strategies, and LF filtration methods to explore the expansive design landscape. Ultimately, we conduct a thorough assessment of DataSculpt's performance on 12 real-world datasets, encompassing a range of tasks. This evaluation unveils both the strengths and limitations 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#30913;&#24615;&#21628;&#21560;&#24863;&#24212;&#25216;&#26415;&#19982;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#23454;&#26102;&#35786;&#26029;&#24179;&#21488;&#65292;&#29992;&#20110;&#36861;&#36394;&#21644;&#35786;&#26029;COVID-19&#21644;&#20854;&#20182;&#21628;&#21560;&#31995;&#32479;&#30142;&#30149;&#12290;&#36890;&#36807;&#23545;COVID-19&#24739;&#32773;&#21644;&#20581;&#24247;&#20154;&#22763;&#30340;&#21628;&#21560;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#21644;&#39564;&#35777;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#35786;&#26029;&#33021;&#21147;&#65292;&#36873;&#20986;&#26368;&#20248;&#27169;&#22411;&#65292;&#24179;&#34913;&#20102;&#35786;&#26029;&#31934;&#24230;&#21644;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00737</link><description>&lt;p&gt;
&#23454;&#26102;&#30913;&#24615;&#36861;&#36394;&#21644;&#26426;&#22120;&#23398;&#20064;&#35786;&#26029;COVID-19
&lt;/p&gt;
&lt;p&gt;
Real-Time Magnetic Tracking and Diagnosis of COVID-19 via Machine Learning. (arXiv:2311.00737v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00737
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#30913;&#24615;&#21628;&#21560;&#24863;&#24212;&#25216;&#26415;&#19982;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#23454;&#26102;&#35786;&#26029;&#24179;&#21488;&#65292;&#29992;&#20110;&#36861;&#36394;&#21644;&#35786;&#26029;COVID-19&#21644;&#20854;&#20182;&#21628;&#21560;&#31995;&#32479;&#30142;&#30149;&#12290;&#36890;&#36807;&#23545;COVID-19&#24739;&#32773;&#21644;&#20581;&#24247;&#20154;&#22763;&#30340;&#21628;&#21560;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#21644;&#39564;&#35777;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#35786;&#26029;&#33021;&#21147;&#65292;&#36873;&#20986;&#26368;&#20248;&#27169;&#22411;&#65292;&#24179;&#34913;&#20102;&#35786;&#26029;&#31934;&#24230;&#21644;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
COVID-19&#22823;&#27969;&#34892;&#31361;&#26174;&#20102;&#21487;&#38752;&#12289;&#38750;&#20405;&#20837;&#24615;&#35786;&#26029;&#24037;&#20855;&#22312;&#24378;&#26377;&#21147;&#30340;&#20844;&#20849;&#21355;&#29983;&#24178;&#39044;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;&#26412;&#30740;&#31350;&#23558;&#30913;&#24615;&#21628;&#21560;&#24863;&#24212;&#25216;&#26415;&#65288;MRST&#65289;&#19982;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30456;&#32467;&#21512;&#65292;&#21019;&#24314;&#20102;&#19968;&#31181;&#35786;&#26029;&#24179;&#21488;&#65292;&#29992;&#20110;&#23454;&#26102;&#36861;&#36394;&#21644;&#35786;&#26029;COVID-19&#21644;&#20854;&#20182;&#21628;&#21560;&#31995;&#32479;&#30142;&#30149;&#12290;MRST&#36890;&#36807;&#19977;&#31181;&#29305;&#23450;&#30340;&#21628;&#21560;&#27979;&#35797;&#26041;&#26696;&#65288;&#27491;&#24120;&#21628;&#21560;&#12289;&#23631;&#27668;&#21644;&#28145;&#21628;&#21560;&#65289;&#31934;&#30830;&#25429;&#25417;&#21628;&#21560;&#27169;&#24335;&#12290;&#25105;&#20204;&#22312;&#36234;&#21335;&#20351;&#29992;&#36825;&#20010;&#24179;&#21488;&#37319;&#38598;&#20102;COVID-19&#24739;&#32773;&#21644;&#20581;&#24247;&#20154;&#22763;&#30340;&#21628;&#21560;&#25968;&#25454;&#65292;&#28982;&#21518;&#29992;&#36825;&#20123;&#25968;&#25454;&#23545;ML&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#21644;&#39564;&#35777;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#22810;&#31181;ML&#31639;&#27861;&#65292;&#21253;&#25324;&#25903;&#25345;&#21521;&#37327;&#26426;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#35780;&#20272;&#23427;&#20204;&#35786;&#26029;COVID-19&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#22810;&#27169;&#22411;&#39564;&#35777;&#26041;&#27861;&#30830;&#20445;&#20102;&#20840;&#38754;&#27604;&#36739;&#65292;&#24182;&#20855;&#22791;&#36873;&#25321;&#26368;&#20248;&#27169;&#22411;&#30340;&#36866;&#24212;&#24615;&#65292;&#22312;&#35786;&#26029;&#31934;&#24230;&#21644;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
The COVID-19 pandemic underscored the importance of reliable, noninvasive diagnostic tools for robust public health interventions. In this work, we fused magnetic respiratory sensing technology (MRST) with machine learning (ML) to create a diagnostic platform for real-time tracking and diagnosis of COVID-19 and other respiratory diseases. The MRST precisely captures breathing patterns through three specific breath testing protocols: normal breath, holding breath, and deep breath. We collected breath data from both COVID-19 patients and healthy subjects in Vietnam using this platform, which then served to train and validate ML models. Our evaluation encompassed multiple ML algorithms, including support vector machines and deep learning models, assessing their ability to diagnose COVID-19. Our multi-model validation methodology ensures a thorough comparison and grants the adaptability to select the most optimal model, striking a balance between diagnostic precision with model interpretab
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#31034;&#36394;&#21058;&#36716;&#21270;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#65288;TC-INN&#65289;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#23558;FDG&#22270;&#20687;&#26144;&#23556;&#21040;DOPA&#22270;&#20687;&#65292;&#20197;&#35299;&#20915;DOPA&#22312;PET&#25104;&#20687;&#20013;&#24191;&#27867;&#24212;&#29992;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2311.00735</link><description>&lt;p&gt;
PET&#36890;&#36807;&#21487;&#21464;&#22686;&#24378;&#21487;&#36870;&#32593;&#32476;&#22312;&#33041;PET&#20043;&#38388;&#36827;&#34892;&#31034;&#36394;&#21058;&#36716;&#21270;
&lt;/p&gt;
&lt;p&gt;
PET Tracer Conversion among Brain PET via Variable Augmented Invertible Network. (arXiv:2311.00735v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#31034;&#36394;&#21058;&#36716;&#21270;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#65288;TC-INN&#65289;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#23558;FDG&#22270;&#20687;&#26144;&#23556;&#21040;DOPA&#22270;&#20687;&#65292;&#20197;&#35299;&#20915;DOPA&#22312;PET&#25104;&#20687;&#20013;&#24191;&#27867;&#24212;&#29992;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#30005;&#23376;&#21457;&#23556;&#26029;&#23618;&#25104;&#20687;&#65288;PET&#65289;&#20316;&#20026;&#19968;&#31181;&#20855;&#26377;&#39640;&#29983;&#21270;&#25935;&#24863;&#24615;&#30340;&#25104;&#20687;&#25216;&#26415;&#65292;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#33041;&#30142;&#30149;&#35786;&#26029;&#21644;&#33041;&#31185;&#23398;&#30740;&#31350;&#12290;&#30001;&#20110;&#19981;&#21516;&#31034;&#36394;&#21058;&#22312;&#21516;&#19968;&#21306;&#22495;&#21576;&#29616;&#20986;&#19981;&#21516;&#25928;&#26524;&#65292;&#31034;&#36394;&#21058;&#30340;&#36873;&#25321;&#23545;PET&#25104;&#20687;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#30446;&#21069;&#65292;&#22312;&#31070;&#32463;&#31934;&#31070;&#27835;&#30103;&#20013;&#24191;&#27867;&#24212;&#29992;PET&#25104;&#20687;&#65292;&#21457;&#29616;6-18F-&#27679;-3,4-&#20108;&#32671;&#22522;-L-&#33519;&#19993;&#27688;&#37240;&#65288;DOPA&#65289;&#22312;&#36825;&#19968;&#39046;&#22495;&#27604;18F&#26631;&#35760;&#30340;&#27679;&#20195;&#33073;&#27687;&#33889;&#33796;&#31958;&#65288;FDG&#65289;&#26356;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21046;&#22791;&#22797;&#26434;&#24615;&#20197;&#21450;&#20854;&#20182;&#38480;&#21046;&#65292;DOPA&#36828;&#19981;&#22914;FDG&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31034;&#36394;&#21058;&#36716;&#21270;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#65288;TC-INN&#65289;&#29992;&#20110;&#22270;&#20687;&#25237;&#24433;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#23558;FDG&#22270;&#20687;&#26144;&#23556;&#21040;DOPA&#22270;&#20687;&#12290;&#36890;&#36807;&#20174;FDG&#21040;DOPA&#29983;&#25104;PET&#22270;&#20687;&#65292;&#33719;&#24471;&#26356;&#22810;&#30340;&#35786;&#26029;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Positron emission tomography (PET), as an imaging technique with high biochemical sensitivity, has been widely used in diagnosis of encephalopathy and brain science research used in brain disease diagnosis and brain science research. Since different tracers present different effects on the same focal area, the choice of tracers is getting more significant for PET imaging. Nowadays, with the wide application of PET imaging in neuropsychiatric treatment, 6-18F-fluoro-3, 4-dihydroxy-L-phenylalanine (DOPA) has been found to be more effective than 18F-labeled fluorine-2-deoxyglucose (FDG) in this field. However, due to the complexity of its preparation and other limitations, DOPA is far less widely used than FDG. To address this issue, a tracer conversion invertible neural network (TC-INN) for image projection is developed to map FDG images to DOPA images through deep learning. More diagnostic information is obtained by generating PET images from FDG to DOPA. Specifically, the proposed TC-I
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#26816;&#27979;&#33258;&#25105;&#25253;&#21578;COVID-19&#35786;&#26029;&#25512;&#25991;&#30340;&#19981;&#21516;&#25991;&#26412;&#39044;&#22788;&#29702;&#25216;&#26415;&#65292;&#36890;&#36807;&#20351;&#29992;&#22235;&#20010;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#33719;&#24471;&#20102;&#27604;&#24179;&#22343;&#20540;&#39640;&#20986;4.1%&#30340;84.5%&#30340;F1&#24471;&#20998;&#12290;</title><link>http://arxiv.org/abs/2311.00732</link><description>&lt;p&gt;
tmn&#22312;#SMM4H 2023&#19978;&#30340;&#35770;&#25991;:&#27604;&#36739;&#29992;&#20110;&#26816;&#27979;&#33258;&#25105;&#25253;&#21578;COVID-19&#35786;&#26029;&#25512;&#25991;&#30340;&#25991;&#26412;&#39044;&#22788;&#29702;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
tmn at #SMM4H 2023: Comparing Text Preprocessing Techniques for Detecting Tweets Self-reporting a COVID-19 Diagnosis. (arXiv:2311.00732v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00732
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#26816;&#27979;&#33258;&#25105;&#25253;&#21578;COVID-19&#35786;&#26029;&#25512;&#25991;&#30340;&#19981;&#21516;&#25991;&#26412;&#39044;&#22788;&#29702;&#25216;&#26415;&#65292;&#36890;&#36807;&#20351;&#29992;&#22235;&#20010;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#33719;&#24471;&#20102;&#27604;&#24179;&#22343;&#20540;&#39640;&#20986;4.1%&#30340;84.5%&#30340;F1&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#20010;&#29992;&#20110;SMM4H 2023&#20219;&#21153;1&#30340;&#31995;&#32479;&#12290;&#35813;&#20219;&#21153;&#30340;&#30446;&#26631;&#26159;&#33258;&#21160;&#21306;&#20998;&#37027;&#20123;&#33258;&#25105;&#25253;&#21578;COVID-19&#35786;&#26029;&#30340;&#25512;&#25991;&#65288;&#20363;&#22914;&#65292;&#38451;&#24615;&#26816;&#27979;&#12289;&#20020;&#24202;&#35786;&#26029;&#25110;&#20303;&#38498;&#65289;&#21644;&#37027;&#20123;&#27809;&#26377;&#30340;&#25512;&#25991;&#12290;&#25105;&#20204;&#20351;&#29992;&#22235;&#20010;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#25512;&#25991;&#39044;&#22788;&#29702;&#25216;&#26415;&#12290;&#32463;&#36807;&#24494;&#35843;&#30340;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#33719;&#24471;&#20102;84.5%&#30340;F1&#24471;&#20998;&#65292;&#27604;&#24179;&#22343;&#20540;&#39640;&#20986;4.1%&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper describes a system developed for Task 1 at SMM4H 2023. The goal of the task is to automatically distinguish tweets that self-report a COVID-19 diagnosis (for example, a positive test, clinical diagnosis, or hospitalization) from those that do not. We investigate the use of different techniques for preprocessing tweets using four transformer-based models. The ensemble of fine-tuned language models obtained an F1-score of 84.5%, which is 4.1% higher than the average value.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#32858;&#31867;&#26041;&#27861;&#65292;&#21517;&#20026;PIPCDR&#65292;&#36890;&#36807;&#32467;&#21512;&#27491;&#21521;&#23454;&#20363;&#30456;&#20284;&#24615;&#25439;&#22833;&#21644;&#31751;&#31163;&#25955;&#27491;&#21017;&#21270;&#22120;&#65292;&#20197;&#25552;&#21319;&#32858;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.00731</link><description>&lt;p&gt;
&#29992;&#27491;&#21521;&#30456;&#20284;&#24615;&#21644;&#31751;&#31163;&#25955;&#23398;&#20064;&#25552;&#21319;&#32858;&#31867;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Enhancing Clustering Representations with Positive Proximity and Cluster Dispersion Learning. (arXiv:2311.00731v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#32858;&#31867;&#26041;&#27861;&#65292;&#21517;&#20026;PIPCDR&#65292;&#36890;&#36807;&#32467;&#21512;&#27491;&#21521;&#23454;&#20363;&#30456;&#20284;&#24615;&#25439;&#22833;&#21644;&#31751;&#31163;&#25955;&#27491;&#21017;&#21270;&#22120;&#65292;&#20197;&#25552;&#21319;&#32858;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20195;&#28145;&#24230;&#32858;&#31867;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#23545;&#27604;&#25110;&#38750;&#23545;&#27604;&#25216;&#26415;&#26469;&#33719;&#21462;&#29992;&#20110;&#32858;&#31867;&#20219;&#21153;&#30340;&#26377;&#25928;&#34920;&#31034;&#12290;&#23545;&#27604;&#26041;&#27861;&#21033;&#29992;&#36127;&#23545;&#26469;&#23454;&#29616;&#21516;&#36136;&#34920;&#31034;&#65292;&#20294;&#21487;&#33021;&#24341;&#20837;&#31867;&#20914;&#31361;&#38382;&#39064;&#65292;&#21487;&#33021;&#23548;&#33268;&#32858;&#31867;&#24615;&#33021;&#19979;&#38477;&#12290;&#30456;&#21453;&#65292;&#38750;&#23545;&#27604;&#25216;&#26415;&#21487;&#20197;&#38450;&#27490;&#31867;&#20914;&#31361;&#65292;&#20294;&#21487;&#33021;&#20135;&#29983;&#38750;&#22343;&#21248;&#34920;&#31034;&#65292;&#23548;&#33268;&#32858;&#31867;&#23849;&#28291;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31471;&#21040;&#31471;&#28145;&#24230;&#32858;&#31867;&#26041;&#27861;&#65292;&#21517;&#20026;PIPCDR&#65292;&#26088;&#22312;&#20860;&#39038;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#24182;&#20943;&#36731;&#23427;&#20204;&#30340;&#23616;&#38480;&#24615;&#12290;PIPCDR&#21253;&#25324;&#19968;&#20010;&#27491;&#21521;&#23454;&#20363;&#30456;&#20284;&#24615;&#25439;&#22833;&#21644;&#19968;&#20010;&#31751;&#31163;&#25955;&#27491;&#21017;&#21270;&#22120;&#12290;&#27491;&#21521;&#23454;&#20363;&#30456;&#20284;&#24615;&#25439;&#22833;&#30830;&#20445;&#23454;&#20363;&#30340;&#22686;&#24378;&#35270;&#22270;&#19982;&#20854;&#37319;&#26679;&#37051;&#23621;&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#36890;&#36807;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#36873;&#25321;&#30495;&#27491;&#30340;&#27491;&#23545;&#26469;&#22686;&#24378;&#31751;&#20869;&#32039;&#23494;&#24230;&#12290;&#21516;&#26102;&#65292;&#31751;&#31163;&#25955;&#27491;&#21017;&#21270;&#22120;&#33021;&#22815;&#40723;&#21169;&#31751;&#20043;&#38388;&#30340;&#20998;&#25955;&#24615;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#32858;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contemporary deep clustering approaches often rely on either contrastive or non-contrastive techniques to acquire effective representations for clustering tasks. Contrastive methods leverage negative pairs to achieve homogenous representations but can introduce class collision issues, potentially compromising clustering performance. On the contrary, non-contrastive techniques prevent class collisions but may produce non-uniform representations that lead to clustering collapse. In this work, we propose a novel end-to-end deep clustering approach named PIPCDR, designed to harness the strengths of both approaches while mitigating their limitations. PIPCDR incorporates a positive instance proximity loss and a cluster dispersion regularizer. The positive instance proximity loss ensures alignment between augmented views of instances and their sampled neighbors, enhancing within-cluster compactness by selecting genuinely positive pairs within the embedding space. Meanwhile, the cluster disper
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36801;&#31227;&#23398;&#20064;&#21644;&#20803;&#23398;&#20064;&#20316;&#20026;&#35299;&#20915;&#26377;&#38480;&#25968;&#25454;&#23398;&#20064;&#38382;&#39064;&#30340;&#20004;&#31181;&#26041;&#27861;&#30340;&#30456;&#23545;&#24615;&#33021;&#65292;&#20197;&#24314;&#31435;&#19968;&#20010;&#22312;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#22330;&#26223;&#20013;&#36873;&#25321;&#26368;&#36866;&#21512;&#30340;&#26041;&#27861;&#30340;&#31283;&#20581;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2311.00727</link><description>&lt;p&gt;
&#30740;&#31350;&#36801;&#31227;&#23398;&#20064;&#21644;&#20803;&#23398;&#20064;&#30340;&#30456;&#23545;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Investigating Relative Performance of Transfer and Meta Learning. (arXiv:2311.00727v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00727
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36801;&#31227;&#23398;&#20064;&#21644;&#20803;&#23398;&#20064;&#20316;&#20026;&#35299;&#20915;&#26377;&#38480;&#25968;&#25454;&#23398;&#20064;&#38382;&#39064;&#30340;&#20004;&#31181;&#26041;&#27861;&#30340;&#30456;&#23545;&#24615;&#33021;&#65292;&#20197;&#24314;&#31435;&#19968;&#20010;&#22312;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#22330;&#26223;&#20013;&#36873;&#25321;&#26368;&#36866;&#21512;&#30340;&#26041;&#27861;&#30340;&#31283;&#20581;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#21313;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#23613;&#31649;&#22270;&#20687;&#35782;&#21035;&#31995;&#32479;&#22312;&#20934;&#30830;&#24230;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25104;&#23601;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#20381;&#36182;&#20110;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#26159;&#22312;&#20998;&#24067;&#22806;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#36825;&#35201;&#27714;&#24403;&#31070;&#32463;&#32593;&#32476;&#36935;&#21040;&#19982;&#20854;&#35757;&#32451;&#25968;&#25454;&#19981;&#19968;&#33268;&#30340;&#26465;&#20214;&#26102;&#37325;&#26032;&#35757;&#32451;&#12290;&#36825;&#20010;&#38480;&#21046;&#26174;&#33879;&#24433;&#21709;&#20102;&#33258;&#21160;&#39550;&#39542;&#25216;&#26415;&#30340;&#36827;&#23637;&#12290;&#36825;&#20123;&#32039;&#36843;&#38382;&#39064;&#24341;&#21457;&#20102;&#23545;&#20351;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#26377;&#25928;&#22320;&#20174;&#26377;&#38480;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#26088;&#22312;&#27604;&#36739;&#20004;&#31181;&#19981;&#21516;&#26041;&#27861;&#8212;&#8212;&#36801;&#31227;&#23398;&#20064;&#21644;&#20803;&#23398;&#20064;&#30340;&#30740;&#31350;&#25104;&#26524;&#65292;&#20316;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#28508;&#22312;&#26041;&#26696;&#12290;&#24635;&#20307;&#30446;&#26631;&#26159;&#24314;&#31435;&#19968;&#20010;&#22312;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#22330;&#26223;&#20013;&#36873;&#25321;&#26368;&#36866;&#21512;&#30340;&#26041;&#27861;&#30340;&#31283;&#20581;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past decade, the field of machine learning has experienced remarkable advancements. While image recognition systems have achieved impressive levels of accuracy, they continue to rely on extensive training datasets. Additionally, a significant challenge has emerged in the form of poor out-of-distribution performance, which necessitates retraining neural networks when they encounter conditions that deviate from their training data. This limitation has notably contributed to the slow progress in self-driving car technology. These pressing issues have sparked considerable interest in methods that enable neural networks to learn effectively from limited data. This paper presents the outcomes of an extensive investigation designed to compare two distinct approaches, transfer learning and meta learning, as potential solutions to this problem. The overarching objective was to establish a robust criterion for selecting the most suitable method in diverse machine learning scenarios. Bui
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24037;&#19994;&#21270;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#33258;&#36866;&#24212;&#25968;&#25454;&#25366;&#25496;&#25216;&#26415;&#21644;&#22823;&#25968;&#25454;&#25216;&#26415;&#26469;&#20934;&#30830;&#12289;&#39640;&#25928;&#12289;&#20302;&#25104;&#26412;&#22320;&#26816;&#27979;&#30005;&#20449;&#34892;&#19994;&#30340;&#27450;&#35784;&#12290;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#26816;&#27979;&#22269;&#38469;&#25910;&#20837;&#20998;&#25104;&#27450;&#35784;&#65292;&#24182;&#21457;&#29616;&#20102;&#26032;&#30340;&#27450;&#35784;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2311.00724</link><description>&lt;p&gt;
&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#22823;&#25968;&#25454;&#20998;&#26512;&#25216;&#26415;&#36827;&#34892;&#30005;&#20449;&#34892;&#19994;&#30340;&#27450;&#35784;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Fraud Analytics Using Machine-learning &amp; Engineering on Big Data (FAME) for Telecom. (arXiv:2311.00724v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00724
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24037;&#19994;&#21270;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#33258;&#36866;&#24212;&#25968;&#25454;&#25366;&#25496;&#25216;&#26415;&#21644;&#22823;&#25968;&#25454;&#25216;&#26415;&#26469;&#20934;&#30830;&#12289;&#39640;&#25928;&#12289;&#20302;&#25104;&#26412;&#22320;&#26816;&#27979;&#30005;&#20449;&#34892;&#19994;&#30340;&#27450;&#35784;&#12290;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#26816;&#27979;&#22269;&#38469;&#25910;&#20837;&#20998;&#25104;&#27450;&#35784;&#65292;&#24182;&#21457;&#29616;&#20102;&#26032;&#30340;&#27450;&#35784;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#20449;&#34892;&#19994;&#30001;&#20110;&#27450;&#35784;&#34892;&#20026;&#27599;&#24180;&#20840;&#29699;&#25439;&#22833;463&#20159;&#32654;&#20803;&#12290;&#36807;&#21435;&#20351;&#29992;&#25968;&#25454;&#25366;&#25496;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65288;&#38500;&#20102;&#35268;&#21017;&#23548;&#21521;&#26041;&#27861;&#65289;&#26469;&#36827;&#34892;&#27450;&#35784;&#26816;&#27979;&#65292;&#20294;&#25928;&#29575;&#24456;&#20302;&#65292;&#22240;&#20026;&#27450;&#35784;&#27169;&#24335;&#21464;&#21270;&#38750;&#24120;&#24555;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24037;&#19994;&#21270;&#35299;&#20915;&#26041;&#26696;&#65292;&#37319;&#29992;&#33258;&#36866;&#24212;&#25968;&#25454;&#25366;&#25496;&#25216;&#26415;&#21644;&#22823;&#25968;&#25454;&#25216;&#26415;&#26469;&#20934;&#30830;&#12289;&#39640;&#25928;&#12289;&#20302;&#25104;&#26412;&#22320;&#26816;&#27979;&#27450;&#35784;&#24182;&#21457;&#29616;&#26032;&#30340;&#27450;&#35784;&#27169;&#24335;&#12290;&#35813;&#35299;&#20915;&#26041;&#26696;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&lt;5%&#30340;&#35823;&#25253;&#29575;&#26816;&#27979;&#22269;&#38469;&#25910;&#20837;&#20998;&#25104;&#27450;&#35784;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#20102;&#26469;&#33258;&#30693;&#21517;&#25209;&#21457;&#36816;&#33829;&#21830;&#21644;&#28023;&#22806;&#30005;&#20449;&#20013;&#36716;&#36816;&#33829;&#21830;&#30340;&#36229;&#36807;1TB&#30340;&#36890;&#35805;&#35814;&#21333;&#35760;&#24405;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Telecom industries lose globally 46.3 Billion USD due to fraud. Data mining and machine learning techniques (apart from rules oriented approach) have been used in past, but efficiency has been low as fraud pattern changes very rapidly. This paper presents an industrialized solution approach with self adaptive data mining technique and application of big data technologies to detect fraud and discover novel fraud patterns in accurate, efficient and cost effective manner. Solution has been successfully demonstrated to detect International Revenue Share Fraud with &lt;5% false positive. More than 1 Terra Bytes of Call Detail Record from a reputed wholesale carrier and overseas telecom transit carrier has been used to conduct this study.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23545;&#20849;&#24773;&#26816;&#27979;&#39046;&#22495;&#30340;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#36827;&#34892;&#20102;&#32508;&#36848;&#21644;&#20998;&#26512;&#65292;&#21253;&#25324;&#25991;&#26412;&#12289;&#35270;&#21548;&#12289;&#38899;&#39057;&#21644;&#29983;&#29702;&#20449;&#21495;&#22235;&#31181;&#36755;&#20837;&#27169;&#24577;&#30340;&#22788;&#29702;&#21644;&#32593;&#32476;&#35774;&#35745;&#65292;&#20197;&#21450;&#35780;&#20272;&#21327;&#35758;&#21644;&#25968;&#25454;&#38598;&#30340;&#25551;&#36848;&#12290;</title><link>http://arxiv.org/abs/2311.00721</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#22312;&#25991;&#26412;&#12289;&#35270;&#21548;&#12289;&#38899;&#39057;&#25110;&#29983;&#29702;&#20449;&#21495;&#19978;&#36827;&#34892;&#20849;&#24773;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Empathy Detection Using Machine Learning on Text, Audiovisual, Audio or Physiological Signals. (arXiv:2311.00721v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00721
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23545;&#20849;&#24773;&#26816;&#27979;&#39046;&#22495;&#30340;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#36827;&#34892;&#20102;&#32508;&#36848;&#21644;&#20998;&#26512;&#65292;&#21253;&#25324;&#25991;&#26412;&#12289;&#35270;&#21548;&#12289;&#38899;&#39057;&#21644;&#29983;&#29702;&#20449;&#21495;&#22235;&#31181;&#36755;&#20837;&#27169;&#24577;&#30340;&#22788;&#29702;&#21644;&#32593;&#32476;&#35774;&#35745;&#65292;&#20197;&#21450;&#35780;&#20272;&#21327;&#35758;&#21644;&#25968;&#25454;&#38598;&#30340;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20849;&#24773;&#26159;&#19968;&#20010;&#31038;&#20132;&#25216;&#33021;&#65292;&#34920;&#26126;&#19968;&#20010;&#20010;&#20307;&#29702;&#35299;&#20182;&#20154;&#30340;&#33021;&#21147;&#12290;&#36817;&#24180;&#26469;&#65292;&#20849;&#24773;&#24341;&#36215;&#20102;&#21253;&#25324;&#24773;&#24863;&#35745;&#31639;&#12289;&#35748;&#30693;&#31185;&#23398;&#21644;&#24515;&#29702;&#23398;&#22312;&#20869;&#30340;&#21508;&#20010;&#23398;&#31185;&#30340;&#20851;&#27880;&#12290;&#20849;&#24773;&#26159;&#19968;&#20010;&#20381;&#36182;&#20110;&#19978;&#19979;&#25991;&#30340;&#26415;&#35821;&#65292;&#22240;&#27492;&#26816;&#27979;&#25110;&#35782;&#21035;&#20849;&#24773;&#22312;&#31038;&#20250;&#12289;&#21307;&#30103;&#21644;&#25945;&#32946;&#31561;&#39046;&#22495;&#20855;&#26377;&#28508;&#22312;&#30340;&#24212;&#29992;&#12290;&#23613;&#31649;&#20849;&#24773;&#26816;&#27979;&#39046;&#22495;&#28041;&#21450;&#33539;&#22260;&#24191;&#27867;&#19988;&#26377;&#37325;&#21472;&#65292;&#20294;&#20174;&#25972;&#20307;&#25991;&#29486;&#35282;&#24230;&#26469;&#30475;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#20849;&#24773;&#26816;&#27979;&#30740;&#31350;&#20173;&#28982;&#30456;&#23545;&#36739;&#23569;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#31995;&#32479;&#25910;&#38598;&#21644;&#31579;&#36873;&#20102;&#26469;&#33258;10&#20010;&#30693;&#21517;&#25968;&#25454;&#24211;&#30340;801&#31687;&#35770;&#25991;&#65292;&#24182;&#20998;&#26512;&#20102;&#36873;&#23450;&#30340;54&#31687;&#35770;&#25991;&#12290;&#25105;&#20204;&#26681;&#25454;&#20849;&#24773;&#26816;&#27979;&#31995;&#32479;&#30340;&#36755;&#20837;&#27169;&#24577;&#65292;&#21363;&#25991;&#26412;&#12289;&#35270;&#21548;&#12289;&#38899;&#39057;&#21644;&#29983;&#29702;&#20449;&#21495;&#65292;&#23545;&#35770;&#25991;&#36827;&#34892;&#20998;&#32452;&#12290;&#25105;&#20204;&#20998;&#21035;&#30740;&#31350;&#20102;&#29305;&#23450;&#27169;&#24577;&#30340;&#39044;&#22788;&#29702;&#21644;&#32593;&#32476;&#26550;&#26500;&#35774;&#35745;&#21327;&#35758;&#12289;&#24120;&#35265;&#25968;&#25454;&#38598;&#30340;&#25551;&#36848;&#21644;&#21487;&#29992;&#24615;&#35814;&#24773;&#65292;&#20197;&#21450;&#35780;&#20272;&#21327;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Empathy is a social skill that indicates an individual's ability to understand others. Over the past few years, empathy has drawn attention from various disciplines, including but not limited to Affective Computing, Cognitive Science and Psychology. Empathy is a context-dependent term; thus, detecting or recognising empathy has potential applications in society, healthcare and education. Despite being a broad and overlapping topic, the avenue of empathy detection studies leveraging Machine Learning remains underexplored from a holistic literature perspective. To this end, we systematically collect and screen 801 papers from 10 well-known databases and analyse the selected 54 papers. We group the papers based on input modalities of empathy detection systems, i.e., text, audiovisual, audio and physiological signals. We examine modality-specific pre-processing and network architecture design protocols, popular dataset descriptions and availability details, and evaluation protocols. We fur
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#34920;&#26126;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#33258;&#21160;&#35828;&#35805;&#20154;&#35782;&#21035;&#20013;&#26080;&#27861;&#20805;&#20998;&#27169;&#25311;&#36229;&#20998;&#27573;&#26102;&#38388;&#29305;&#24449;&#65292;&#36825;&#20026;&#26410;&#26469;&#26356;&#22909;&#22320;&#21033;&#29992;&#23436;&#25972;&#35821;&#38899;&#20449;&#21495;&#36827;&#34892;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2311.00489</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#33258;&#21160;&#35828;&#35805;&#20154;&#35782;&#21035;&#20013;&#19981;&#33021;&#23398;&#20064;&#36229;&#20998;&#27573;&#26102;&#38388;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks for Automatic Speaker Recognition Do Not Learn Supra-Segmental Temporal Features. (arXiv:2311.00489v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00489
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#34920;&#26126;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#33258;&#21160;&#35828;&#35805;&#20154;&#35782;&#21035;&#20013;&#26080;&#27861;&#20805;&#20998;&#27169;&#25311;&#36229;&#20998;&#27573;&#26102;&#38388;&#29305;&#24449;&#65292;&#36825;&#20026;&#26410;&#26469;&#26356;&#22909;&#22320;&#21033;&#29992;&#23436;&#25972;&#35821;&#38899;&#20449;&#21495;&#36827;&#34892;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#33258;&#21160;&#35828;&#35805;&#20154;&#35782;&#21035;&#21644;&#30456;&#20851;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#20294;&#25105;&#20204;&#23545;&#20110;&#36825;&#20123;&#32467;&#26524;&#30340;&#20855;&#20307;&#21407;&#22240;&#20102;&#35299;&#29978;&#23569;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#23558;&#25104;&#21151;&#30340;&#19968;&#37096;&#20998;&#24402;&#22240;&#20110;&#23427;&#20204;&#27169;&#25311;&#36229;&#20998;&#27573;&#26102;&#38388;&#20449;&#24687;&#65288;SST&#65289;&#30340;&#33021;&#21147;&#65292;&#21363;&#38500;&#20102;&#35889;&#29305;&#24449;&#22806;&#36824;&#23398;&#20064;&#35821;&#38899;&#30340;&#38901;&#24459;&#21644;&#38901;&#24459;&#29305;&#24449;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#65288;i&#65289;&#25552;&#20986;&#24182;&#24212;&#29992;&#19968;&#31181;&#26032;&#30340;&#27979;&#35797;&#26041;&#27861;&#65292;&#26469;&#37327;&#21270;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#35828;&#35805;&#20154;&#35782;&#21035;&#26041;&#38754;&#30340;&#24615;&#33021;&#33021;&#22815;&#36890;&#36807;&#24314;&#27169;SST&#26469;&#35299;&#37322;&#21040;&#22810;&#22823;&#31243;&#24230;&#65307;&#24182;&#19988;&#65288;ii&#65289;&#25552;&#20986;&#20960;&#31181;&#24378;&#21046;&#32593;&#32476;&#26356;&#21152;&#20851;&#27880;SST&#30340;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#23427;&#20204;&#30340;&#20248;&#28857;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21363;&#20351;&#34987;&#24378;&#21046;&#35201;&#27714;&#65292;&#19968;&#31995;&#21015;&#22522;&#20110;CNN&#21644;RNN&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#22312;&#35828;&#35805;&#20154;&#35782;&#21035;&#20013;&#20063;&#19981;&#33021;&#20805;&#20998;&#22320;&#27169;&#25311;SST&#12290;&#36825;&#20123;&#32467;&#26524;&#20026;&#26410;&#26469;&#26356;&#22909;&#22320;&#21033;&#29992;&#23436;&#25972;&#35821;&#38899;&#20449;&#21495;&#36827;&#34892;&#30740;&#31350;&#25552;&#20379;&#20102;&#38750;&#24120;&#37325;&#35201;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
While deep neural networks have shown impressive results in automatic speaker recognition and related tasks, it is dissatisfactory how little is understood about what exactly is responsible for these results. Part of the success has been attributed in prior work to their capability to model supra-segmental temporal information (SST), i.e., learn rhythmic-prosodic characteristics of speech in addition to spectral features. In this paper, we (i) present and apply a novel test to quantify to what extent the performance of state-of-the-art neural networks for speaker recognition can be explained by modeling SST; and (ii) present several means to force respective nets to focus more on SST and evaluate their merits. We find that a variety of CNN- and RNN-based neural network architectures for speaker recognition do not model SST to any sufficient degree, even when forced. The results provide a highly relevant basis for impactful future research into better exploitation of the full speech sig
&lt;/p&gt;</description></item><item><title>JADE&#26159;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#20998;&#26512;&#30340;LLM&#23433;&#20840;&#35780;&#20272;&#24179;&#21488;&#65292;&#33021;&#22815;&#30772;&#22351;&#24191;&#27867;&#20351;&#29992;&#30340;&#20013;&#25991;&#21644;&#33521;&#25991;LLM&#65292;&#24182;&#29983;&#25104;&#39640;&#24230;&#23041;&#32961;&#30340;&#19981;&#23433;&#20840;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.00286</link><description>&lt;p&gt;
JADE&#65306;&#22522;&#20110;&#35821;&#35328;&#30340;LLM&#23433;&#20840;&#35780;&#20272;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
JADE: A Linguistic-based Safety Evaluation Platform for LLM. (arXiv:2311.00286v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00286
&lt;/p&gt;
&lt;p&gt;
JADE&#26159;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#20998;&#26512;&#30340;LLM&#23433;&#20840;&#35780;&#20272;&#24179;&#21488;&#65292;&#33021;&#22815;&#30772;&#22351;&#24191;&#27867;&#20351;&#29992;&#30340;&#20013;&#25991;&#21644;&#33521;&#25991;LLM&#65292;&#24182;&#29983;&#25104;&#39640;&#24230;&#23041;&#32961;&#30340;&#19981;&#23433;&#20840;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;JADE&#65292;&#19968;&#31181;&#38024;&#23545;&#35821;&#35328;&#20998;&#26512;&#30340;&#27169;&#31946;&#27979;&#35797;&#24179;&#21488;&#65292;&#36890;&#36807;&#22686;&#24378;&#31181;&#23376;&#38382;&#39064;&#30340;&#35821;&#35328;&#22797;&#26434;&#24615;&#65292;&#21516;&#26102;&#24182;&#22987;&#32456;&#33021;&#22815;&#30772;&#22351;&#24191;&#27867;&#20351;&#29992;&#30340;&#19977;&#31867;LLM&#65306;&#20843;&#20010;&#24320;&#28304;&#20013;&#25991;LLM&#65292;&#20845;&#20010;&#21830;&#19994;&#20013;&#25991;LLM&#21644;&#22235;&#20010;&#21830;&#19994;&#33521;&#25991;LLM&#12290;JADE&#20026;&#36825;&#19977;&#31867;LLM&#29983;&#25104;&#20102;&#19977;&#20010;&#23433;&#20840;&#22522;&#20934;&#65292;&#20854;&#20013;&#21253;&#21547;&#39640;&#24230;&#23041;&#32961;&#30340;&#19981;&#23433;&#20840;&#38382;&#39064;&#65306;&#36825;&#20123;&#38382;&#39064;&#21487;&#20197;&#21516;&#26102;&#35302;&#21457;&#22810;&#20010;LLM&#30340;&#26377;&#23475;&#29983;&#25104;&#65292;&#24179;&#22343;&#19981;&#23433;&#20840;&#29983;&#25104;&#27604;&#20363;&#20026;70%&#65288;&#35831;&#21442;&#35265;&#19979;&#34920;&#65289;&#65292;&#21516;&#26102;&#36825;&#20123;&#38382;&#39064;&#20173;&#28982;&#26159;&#33258;&#28982;&#12289;&#27969;&#30021;&#19988;&#20445;&#30041;&#20102;&#26680;&#24515;&#30340;&#19981;&#23433;&#20840;&#35821;&#20041;&#12290;&#25105;&#20204;&#22312;&#20197;&#19979;&#38142;&#25509;&#20013;&#21457;&#24067;&#20102;&#23545;&#21830;&#19994;&#33521;&#25991;LLM&#21644;&#24320;&#28304;&#33521;&#25991;LLM&#29983;&#25104;&#30340;&#22522;&#20934;&#28436;&#31034;&#65306;https://github.com/whitzard-ai/jade-db&#12290;&#23545;&#20110;&#23545;JADE&#29983;&#25104;&#30340;&#26356;&#22810;&#38382;&#39064;&#24863;&#20852;&#36259;&#30340;&#35835;&#32773;&#65292;&#35831;&#19982;&#25105;&#20204;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present \textit{JADE}, a targeted linguistic fuzzing platform which strengthens the linguistic complexity of seed questions to simultaneously and consistently break a wide range of widely-used LLMs categorized in three groups: eight open-sourced Chinese, six commercial Chinese and four commercial English LLMs. JADE generates three safety benchmarks for the three groups of LLMs, which contain unsafe questions that are highly threatening: the questions simultaneously trigger harmful generation of multiple LLMs, with an average unsafe generation ratio of \textbf{$70\%$} (please see the table below), while are still natural questions, fluent and preserving the core unsafe semantics. We release the benchmark demos generated for commercial English LLMs and open-sourced English LLMs in the following link: https://github.com/whitzard-ai/jade-db. For readers who are interested in evaluating on more questions generated by JADE, please contact us.  \textit{JADE} is based on Noam
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;AutoMixer&#65292;&#19968;&#20010;&#22522;&#20110;&#26102;&#38388;&#24207;&#21015;&#22522;&#30784;&#27169;&#22411;&#30340;&#33258;&#21160;&#28151;&#21512;&#27169;&#22411;&#65292;&#36890;&#36807;&#36890;&#36947;&#21387;&#32553;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#24037;&#20316;&#27969;&#25216;&#26415;&#65292;&#26377;&#25928;&#35299;&#32806;&#20102;BizITOps&#25968;&#25454;&#20013;&#26377;&#29992;&#21644;&#22024;&#26434;&#30340;&#36328;&#36890;&#36947;&#20132;&#20114;&#65292;&#25552;&#39640;&#20102;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.20280</link><description>&lt;p&gt;
&#25913;&#36827;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#33258;&#21160;&#28151;&#21512;&#27169;&#22411;&#22312;BizITOps&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
AutoMixer for Improved Multivariate Time-Series Forecasting on BizITOps Data. (arXiv:2310.20280v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20280
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;AutoMixer&#65292;&#19968;&#20010;&#22522;&#20110;&#26102;&#38388;&#24207;&#21015;&#22522;&#30784;&#27169;&#22411;&#30340;&#33258;&#21160;&#28151;&#21512;&#27169;&#22411;&#65292;&#36890;&#36807;&#36890;&#36947;&#21387;&#32553;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#24037;&#20316;&#27969;&#25216;&#26415;&#65292;&#26377;&#25928;&#35299;&#32806;&#20102;BizITOps&#25968;&#25454;&#20013;&#26377;&#29992;&#21644;&#22024;&#26434;&#30340;&#36328;&#36890;&#36947;&#20132;&#20114;&#65292;&#25552;&#39640;&#20102;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19994;&#21153;&#36807;&#31243;&#30340;&#25928;&#29575;&#20381;&#36182;&#20110;&#19994;&#21153;&#20851;&#38190;&#32489;&#25928;&#25351;&#26631;&#65288;Biz-KPIs&#65289;&#65292;&#32780;IT&#25925;&#38556;&#21487;&#33021;&#23545;&#20854;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;BizITOps&#25968;&#25454;&#23558;Biz-KPIs&#21644;IT&#20107;&#20214;&#36890;&#36947;&#34701;&#21512;&#25104;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#25552;&#21069;&#39044;&#27979;Biz-KPIs&#21487;&#20197;&#36890;&#36807;&#20027;&#21160;&#30340;&#32416;&#27491;&#25514;&#26045;&#25552;&#39640;&#25928;&#29575;&#21644;&#25910;&#30410;&#12290;&#28982;&#32780;&#65292;BizITOps&#25968;&#25454;&#36890;&#24120;&#23637;&#31034;&#20986;Biz-KPIs&#21644;IT&#20107;&#20214;&#20043;&#38388;&#26377;&#29992;&#21644;&#22024;&#26434;&#30340;&#36328;&#36890;&#36947;&#20132;&#20114;&#65292;&#38656;&#35201;&#26377;&#25928;&#35299;&#32806;&#12290;&#24403;&#20351;&#29992;&#29616;&#26377;&#30340;&#22810;&#21464;&#37327;&#39044;&#27979;&#27169;&#22411;&#26102;&#65292;&#36825;&#23548;&#33268;&#39044;&#27979;&#24615;&#33021;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31216;&#20026;AutoMixer&#30340;&#26102;&#38388;&#24207;&#21015;&#22522;&#30784;&#27169;&#22411;&#65288;FM&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#26032;&#39062;&#30340;&#36890;&#36947;&#21387;&#32553;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#24037;&#20316;&#27969;&#25216;&#26415;&#12290;AutoMixer&#21033;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#36827;&#34892;&#36890;&#36947;&#21387;&#32553;&#30340;&#39044;&#35757;&#32451;&#65292;&#24182;&#23558;&#20854;&#19982;&#20808;&#36827;&#30340;TSMixer&#27169;&#22411;&#38598;&#25104;&#65292;&#29992;&#20110;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#36825;&#31181;&#34701;&#21512;&#26497;&#22823;&#22320;&#22686;&#24378;&#20102;TSM
&lt;/p&gt;
&lt;p&gt;
The efficiency of business processes relies on business key performance indicators (Biz-KPIs), that can be negatively impacted by IT failures. BizITOps data fuses both Biz-KPIs and IT event channels together as multivariate time series data. Forecasting Biz-KPIs in advance can enhance efficiency and revenue through proactive corrective measures. However, BizITOps data generally exhibit both useful and noisy inter-channel interactions between Biz-KPIs and IT events that need to be effectively decoupled. This leads to suboptimal forecasting performance when existing multivariate forecasting models are employed. To address this, we introduce AutoMixer, a time-series Foundation Model (FM) approach, grounded on the novel technique of channel-compressed pretrain and finetune workflows. AutoMixer leverages an AutoEncoder for channel-compressed pretraining and integrates it with the advanced TSMixer model for multivariate time series forecasting. This fusion greatly enhances the potency of TSM
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#39640;&#32500;&#39640;&#26031;&#25968;&#25454;&#30340;&#22810;&#32034;&#24341;&#22238;&#24402;&#38382;&#39064;&#20013;&#65292;&#36890;&#36807;&#26799;&#24230;&#27969;&#23398;&#20064;&#20302;&#31209;&#32447;&#24615;&#25237;&#24433;&#21644;&#20302;&#32500;&#36830;&#25509;&#20989;&#25968;&#65292;&#24314;&#31435;&#20102;&#20840;&#23616;&#25910;&#25947;&#24615;&#21644;&#23450;&#37327;&#25551;&#36848;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.19793</link><description>&lt;p&gt;
&#20851;&#20110;&#20351;&#29992;&#26799;&#24230;&#27969;&#23398;&#20064;&#39640;&#26031;&#22810;&#32034;&#24341;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Learning Gaussian Multi-index Models with Gradient Flow. (arXiv:2310.19793v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19793
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#39640;&#32500;&#39640;&#26031;&#25968;&#25454;&#30340;&#22810;&#32034;&#24341;&#22238;&#24402;&#38382;&#39064;&#20013;&#65292;&#36890;&#36807;&#26799;&#24230;&#27969;&#23398;&#20064;&#20302;&#31209;&#32447;&#24615;&#25237;&#24433;&#21644;&#20302;&#32500;&#36830;&#25509;&#20989;&#25968;&#65292;&#24314;&#31435;&#20102;&#20840;&#23616;&#25910;&#25947;&#24615;&#21644;&#23450;&#37327;&#25551;&#36848;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#39640;&#32500;&#39640;&#26031;&#25968;&#25454;&#30340;&#22810;&#32034;&#24341;&#22238;&#24402;&#38382;&#39064;&#20013;&#30340;&#26799;&#24230;&#27969;&#12290;&#22810;&#32034;&#24341;&#20989;&#25968;&#30001;&#26410;&#30693;&#30340;&#20302;&#31209;&#32447;&#24615;&#25237;&#24433;&#21644;&#20219;&#24847;&#26410;&#30693;&#30340;&#20302;&#32500;&#36830;&#25509;&#20989;&#25968;&#32452;&#25104;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#26500;&#25104;&#20102;&#31070;&#32463;&#32593;&#32476;&#20013;&#29305;&#24449;&#23398;&#20064;&#30340;&#33258;&#28982;&#27169;&#26495;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#20004;&#26102;&#38388;&#23610;&#24230;&#30340;&#31639;&#27861;&#65292;&#20854;&#20013;&#20302;&#32500;&#36830;&#25509;&#20989;&#25968;&#36890;&#36807;&#38750;&#21442;&#25968;&#27169;&#22411;&#27604;&#21442;&#25968;&#21270;&#20302;&#31209;&#25237;&#24433;&#30340;&#20302;&#32500;&#31354;&#38388;&#26356;&#24555;&#22320;&#23398;&#20064;&#12290;&#36890;&#36807;&#36866;&#24403;&#22320;&#21033;&#29992;&#26694;&#26550;&#30340;&#30456;&#20851;&#30697;&#38453;&#19978;&#30340;&#30697;&#38453;&#21322;&#32676;&#32467;&#26500;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#30001;Grassmannian&#20154;&#21475;&#26799;&#24230;&#27969;&#21160;&#21147;&#23398;&#24341;&#36215;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#65292;&#24182;&#23545;&#20854;&#30456;&#20851;&#30340;&#8220;&#38797;&#28857;&#21040;&#38797;&#28857;&#8221;&#21160;&#21147;&#23398;&#25552;&#20379;&#20102;&#23450;&#37327;&#25551;&#36848;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#27599;&#20010;&#38797;&#30340;&#26102;&#38388;&#23610;&#24230;&#21487;&#20197;&#26126;&#30830;&#22320;&#29992;&#30446;&#26631;&#36830;&#25509;&#20989;&#25968;&#30340;&#36866;&#24403;Hermite&#20998;&#35299;&#26469;&#34920;&#24449;&#12290;&#19982;&#36825;&#20123;&#20301;&#32622;&#30456;&#21453;&#30340;&#26159;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study gradient flow on the multi-index regression problem for high-dimensional Gaussian data. Multi-index functions consist of a composition of an unknown low-rank linear projection and an arbitrary unknown, low-dimensional link function. As such, they constitute a natural template for feature learning in neural networks.  We consider a two-timescale algorithm, whereby the low-dimensional link function is learnt with a non-parametric model infinitely faster than the subspace parametrizing the low-rank projection. By appropriately exploiting the matrix semigroup structure arising over the subspace correlation matrices, we establish global convergence of the resulting Grassmannian population gradient flow dynamics, and provide a quantitative description of its associated `saddle-to-saddle' dynamics. Notably, the timescales associated with each saddle can be explicitly characterized in terms of an appropriate Hermite decomposition of the target link function. In contrast with these pos
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;&#20223;&#30495;&#22120;&#22312;&#32447;&#35757;&#32451;&#20122;&#32593;&#26684;&#21442;&#25968;&#21270;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#21518;&#39564;&#25439;&#22833;&#20989;&#25968;&#36866;&#24212;&#38750;&#21487;&#24494;&#20998;&#25968;&#20540;&#27714;&#35299;&#22120;&#65292;&#24182;&#36890;&#36807;&#26102;&#38388;&#31215;&#20998;&#27493;&#39588;&#20801;&#35768;&#26799;&#24230;&#20256;&#25773;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#23558;&#31070;&#32463;&#20223;&#30495;&#22120;&#21644;&#21442;&#25968;&#21270;&#32452;&#20214;&#20998;&#21035;&#29992;&#30456;&#24212;&#30340;&#25439;&#22833;&#37327;&#36827;&#34892;&#35757;&#32451;&#26159;&#24517;&#35201;&#30340;&#65292;&#20197;&#26368;&#23567;&#21270;&#26576;&#20123;&#36817;&#20284;&#20559;&#24046;&#30340;&#20256;&#25773;&#12290;</title><link>http://arxiv.org/abs/2310.19385</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#20223;&#30495;&#22120;&#30340;&#26080;&#26799;&#24230;&#22312;&#32447;&#23398;&#20064;&#20122;&#32593;&#26684;&#23610;&#24230;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Gradient-free online learning of subgrid-scale dynamics with neural emulators. (arXiv:2310.19385v2 [physics.comp-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19385
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;&#20223;&#30495;&#22120;&#22312;&#32447;&#35757;&#32451;&#20122;&#32593;&#26684;&#21442;&#25968;&#21270;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#21518;&#39564;&#25439;&#22833;&#20989;&#25968;&#36866;&#24212;&#38750;&#21487;&#24494;&#20998;&#25968;&#20540;&#27714;&#35299;&#22120;&#65292;&#24182;&#36890;&#36807;&#26102;&#38388;&#31215;&#20998;&#27493;&#39588;&#20801;&#35768;&#26799;&#24230;&#20256;&#25773;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#23558;&#31070;&#32463;&#20223;&#30495;&#22120;&#21644;&#21442;&#25968;&#21270;&#32452;&#20214;&#20998;&#21035;&#29992;&#30456;&#24212;&#30340;&#25439;&#22833;&#37327;&#36827;&#34892;&#35757;&#32451;&#26159;&#24517;&#35201;&#30340;&#65292;&#20197;&#26368;&#23567;&#21270;&#26576;&#20123;&#36817;&#20284;&#20559;&#24046;&#30340;&#20256;&#25773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#32447;&#35757;&#32451;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20122;&#32593;&#26684;&#21442;&#25968;&#21270;&#65292;&#24182;&#36890;&#36807;&#21518;&#39564;&#25439;&#22833;&#20989;&#25968;&#36866;&#24212;&#38750;&#21487;&#24494;&#20998;&#25968;&#20540;&#27714;&#35299;&#22120;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#31070;&#32463;&#20223;&#30495;&#22120;&#35757;&#32451;&#31616;&#21270;&#29366;&#24577;&#31354;&#38388;&#27714;&#35299;&#22120;&#30340;&#36817;&#20284;&#20540;&#65292;&#28982;&#21518;&#36890;&#36807;&#26102;&#38388;&#31215;&#20998;&#27493;&#39588;&#20801;&#35768;&#26799;&#24230;&#20256;&#25773;&#12290;&#35813;&#31639;&#27861;&#33021;&#22815;&#22312;&#19981;&#35745;&#31639;&#21407;&#22987;&#27714;&#35299;&#22120;&#26799;&#24230;&#30340;&#24773;&#20917;&#19979;&#24674;&#22797;&#22823;&#37096;&#20998;&#22312;&#32447;&#31574;&#30053;&#30340;&#22909;&#22788;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#23558;&#31070;&#32463;&#20223;&#30495;&#22120;&#21644;&#21442;&#25968;&#21270;&#32452;&#20214;&#20998;&#21035;&#29992;&#30456;&#24212;&#30340;&#25439;&#22833;&#37327;&#36827;&#34892;&#35757;&#32451;&#26159;&#24517;&#35201;&#30340;&#65292;&#20197;&#26368;&#23567;&#21270;&#26576;&#20123;&#36817;&#20284;&#20559;&#24046;&#30340;&#20256;&#25773;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a generic algorithm to train machine learning-based subgrid parametrizations online, i.e., with $\textit{a posteriori}$ loss functions for non-differentiable numerical solvers. The proposed approach leverage neural emulators to train an approximation of the reduced state-space solver, which is then used to allows gradient propagation through temporal integration steps. The algorithm is able to recover most of the benefit of online strategies without having to compute the gradient of the original solver. It is demonstrated that training the neural emulator and parametrization components separately with respective loss quantities is necessary in order to minimize the propagation of some approximation bias.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26032;&#30340;&#38169;&#35823;&#29702;&#35770;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;iGAT&#30340;&#26041;&#27861;&#29992;&#20110;&#25913;&#36827;&#38598;&#25104;&#23545;&#25239;&#38450;&#24481;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#36873;&#25321;&#24615;&#22320;&#20998;&#37197;&#23545;&#25239;&#26679;&#26412;&#21644;&#27491;&#21017;&#21270;&#26469;&#25552;&#39640;&#38450;&#24481;&#25928;&#26524;&#12290;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#36827;&#34892;&#20102;&#27979;&#35797;&#24182;&#21462;&#24471;&#25104;&#21151;&#12290;</title><link>http://arxiv.org/abs/2310.18477</link><description>&lt;p&gt;
&#29702;&#35299;&#21644;&#25913;&#36827;&#38598;&#25104;&#23545;&#25239;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Understanding and Improving Ensemble Adversarial Defense. (arXiv:2310.18477v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18477
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26032;&#30340;&#38169;&#35823;&#29702;&#35770;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;iGAT&#30340;&#26041;&#27861;&#29992;&#20110;&#25913;&#36827;&#38598;&#25104;&#23545;&#25239;&#38450;&#24481;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#36873;&#25321;&#24615;&#22320;&#20998;&#37197;&#23545;&#25239;&#26679;&#26412;&#21644;&#27491;&#21017;&#21270;&#26469;&#25552;&#39640;&#38450;&#24481;&#25928;&#26524;&#12290;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#36827;&#34892;&#20102;&#27979;&#35797;&#24182;&#21462;&#24471;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#25104;&#31574;&#30053;&#24050;&#32463;&#22312;&#23545;&#25239;&#38450;&#24481;&#20013;&#21464;&#24471;&#27969;&#34892;&#65292;&#23427;&#35757;&#32451;&#22810;&#20010;&#22522;&#26412;&#20998;&#31867;&#22120;&#20197;&#21327;&#21516;&#26041;&#24335;&#38450;&#24481;&#23545;&#25239;&#25915;&#20987;&#12290;&#23613;&#31649;&#22312;&#32463;&#39564;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23545;&#20110;&#20026;&#20160;&#20040;&#23545;&#25239;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#38598;&#25104;&#27604;&#21333;&#20010;&#20998;&#31867;&#22120;&#26356;&#24378;&#22823;&#30340;&#29702;&#35770;&#35299;&#37322;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#38169;&#35823;&#29702;&#35770;&#65292;&#19987;&#38376;&#29992;&#20110;&#29702;&#35299;&#38598;&#25104;&#23545;&#25239;&#38450;&#24481;&#65292;&#22312;&#23545;&#25239;&#38450;&#24481;&#22330;&#26223;&#20013;&#23637;&#31034;&#20102;&#21487;&#20197;&#35777;&#26126;&#30340;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26679;&#26412;&#38598;&#19978;&#30340;0-1&#25439;&#22833;&#38477;&#20302;&#12290;&#22312;&#36825;&#20010;&#29702;&#35770;&#30340;&#25351;&#23548;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#38598;&#25104;&#23545;&#25239;&#38450;&#24481;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;&#20132;&#20114;&#20840;&#23616;&#23545;&#25239;&#35757;&#32451;&#65288;iGAT&#65289;&#12290;&#35813;&#25552;&#26696;&#21253;&#25324;&#65288;1&#65289;&#19968;&#31181;&#27010;&#29575;&#20998;&#37197;&#35268;&#21017;&#65292;&#23558;&#20840;&#23616;&#19978;&#23545;&#38598;&#25104;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23545;&#25239;&#26679;&#26412;&#36873;&#25321;&#24615;&#22320;&#20998;&#37197;&#32473;&#19981;&#21516;&#30340;&#22522;&#26412;&#20998;&#31867;&#22120;&#65292;&#20197;&#21450;&#65288;2&#65289;&#19968;&#20010;&#27491;&#21017;&#21270;&#39033;&#65292;&#20197;&#24357;&#34917;&#22522;&#26412;&#20998;&#31867;&#22120;&#30340;&#20005;&#37325;&#24369;&#28857;&#12290;&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
The strategy of ensemble has become popular in adversarial defense, which trains multiple base classifiers to defend against adversarial attacks in a cooperative manner. Despite the empirical success, theoretical explanations on why an ensemble of adversarially trained classifiers is more robust than single ones remain unclear. To fill in this gap, we develop a new error theory dedicated to understanding ensemble adversarial defense, demonstrating a provable 0-1 loss reduction on challenging sample sets in an adversarial defense scenario. Guided by this theory, we propose an effective approach to improve ensemble adversarial defense, named interactive global adversarial training (iGAT). The proposal includes (1) a probabilistic distributing rule that selectively allocates to different base classifiers adversarial examples that are globally challenging to the ensemble, and (2) a regularization term to rescue the severest weaknesses of the base classifiers. Being tested over various exis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#24847;&#20041;&#34920;&#24449;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#36755;&#20837;&#25991;&#26412;&#30340;&#25152;&#26377;&#21487;&#33021;&#36712;&#36857;&#30340;&#20998;&#24067;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#27169;&#25311;&#38750;&#23545;&#31216;&#20851;&#31995;&#65292;&#19988;&#22312;&#35821;&#20041;&#30456;&#20284;&#24615;&#20219;&#21153;&#19978;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.18348</link><description>&lt;p&gt;
&#24847;&#20041;&#34920;&#24449;&#26469;&#33258;&#33258;&#22238;&#24402;&#27169;&#22411;&#20013;&#30340;&#36712;&#36857;
&lt;/p&gt;
&lt;p&gt;
Meaning Representations from Trajectories in Autoregressive Models. (arXiv:2310.18348v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18348
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#24847;&#20041;&#34920;&#24449;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#36755;&#20837;&#25991;&#26412;&#30340;&#25152;&#26377;&#21487;&#33021;&#36712;&#36857;&#30340;&#20998;&#24067;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#27169;&#25311;&#38750;&#23545;&#31216;&#20851;&#31995;&#65292;&#19988;&#22312;&#35821;&#20041;&#30456;&#20284;&#24615;&#20219;&#21153;&#19978;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#32771;&#34385;&#25193;&#23637;&#36755;&#20837;&#25991;&#26412;&#30340;&#25152;&#26377;&#21487;&#33021;&#36712;&#36857;&#30340;&#20998;&#24067;&#26469;&#20174;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#24847;&#20041;&#34920;&#24449;&#12290;&#36825;&#31181;&#31574;&#30053;&#26159;&#26080;&#25552;&#31034;&#30340;&#65292;&#19981;&#38656;&#35201;&#24494;&#35843;&#65292;&#24182;&#36866;&#29992;&#20110;&#20219;&#20309;&#39044;&#35757;&#32451;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#19982;&#22522;&#20110;&#21521;&#37327;&#30340;&#34920;&#24449;&#19981;&#21516;&#65292;&#22522;&#20110;&#20998;&#24067;&#30340;&#34920;&#24449;&#36824;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#20284;&#28982;&#20989;&#25968;&#20043;&#38388;&#30340;&#20195;&#25968;&#36816;&#31639;&#26469;&#24314;&#27169;&#38750;&#23545;&#31216;&#20851;&#31995;&#65288;&#20363;&#22914;&#65292;&#36923;&#36753;&#34164;&#28085;&#30340;&#26041;&#21521;&#65292;&#19978;&#20301;&#35789;/&#19979;&#20301;&#35789;&#20851;&#31995;&#65289;&#12290;&#36825;&#20123;&#24819;&#27861;&#22522;&#20110;&#35821;&#20041;&#30340;&#20998;&#24067;&#35270;&#35282;&#65292;&#24182;&#19982;&#33258;&#21160;&#26426;&#29702;&#35770;&#20013;&#30340;&#26631;&#20934;&#26500;&#36896;&#30456;&#36830;&#25509;&#65292;&#20294;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#23427;&#20204;&#23578;&#26410;&#24212;&#29992;&#20110;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#20174;&#22823;&#22411;&#27169;&#22411;&#33719;&#24471;&#30340;&#34920;&#24449;&#19982;&#20154;&#31867;&#27880;&#37322;&#24456;&#22909;&#22320;&#19968;&#33268;&#65292;&#22312;&#35821;&#20041;&#30456;&#20284;&#24615;&#20219;&#21153;&#19978;&#20248;&#20110;&#20854;&#20182;&#38646;&#26679;&#26412;&#21644;&#26080;&#25552;&#31034;&#26041;&#27861;&#65292;&#24182;&#21487;&#29992;&#20110;&#35299;&#20915;&#26356;&#22797;&#26434;&#30340;&#34164;&#28085;&#21644;&#21253;&#21547;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose to extract meaning representations from autoregressive language models by considering the distribution of all possible trajectories extending an input text. This strategy is prompt-free, does not require fine-tuning, and is applicable to any pre-trained autoregressive model. Moreover, unlike vector-based representations, distribution-based representations can also model asymmetric relations (e.g., direction of logical entailment, hypernym/hyponym relations) by using algebraic operations between likelihood functions. These ideas are grounded in distributional perspectives on semantics and are connected to standard constructions in automata theory, but to our knowledge they have not been applied to modern language models. We empirically show that the representations obtained from large models align well with human annotations, outperform other zero-shot and prompt-free methods on semantic similarity tasks, and can be used to solve more complex entailment and containment tasks 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28145;&#24230;&#36716;&#25442;&#39640;&#26031;&#36807;&#31243;&#65288;DTGPs&#65289;&#30340;&#36716;&#25442;&#39640;&#26031;&#36807;&#31243;&#65288;TGPs&#65289;&#30340;&#25512;&#24191;&#65292;&#35813;&#27169;&#22411;&#37319;&#29992;&#20018;&#32852;&#23618;&#32423;&#30340;&#38543;&#26426;&#36807;&#31243;&#65292;&#24182;&#23454;&#29616;&#20102;&#30456;&#23545;&#20110;TGPs&#21644;DGPs&#30340;&#28789;&#27963;&#24615;&#22686;&#24378;&#12290;&#36890;&#36807;&#20351;&#29992;&#21464;&#20998;&#25512;&#29702;&#65292;&#21487;&#20197;&#36817;&#20284;&#25152;&#38656;&#30340;&#35745;&#31639;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#31616;&#21333;&#30452;&#25509;&#30340;&#25512;&#29702;&#31639;&#27861;&#25193;&#23637;&#12290;</title><link>http://arxiv.org/abs/2310.18230</link><description>&lt;p&gt;
&#28145;&#24230;&#36716;&#25442;&#39640;&#26031;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Deep Transformed Gaussian Processes. (arXiv:2310.18230v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18230
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28145;&#24230;&#36716;&#25442;&#39640;&#26031;&#36807;&#31243;&#65288;DTGPs&#65289;&#30340;&#36716;&#25442;&#39640;&#26031;&#36807;&#31243;&#65288;TGPs&#65289;&#30340;&#25512;&#24191;&#65292;&#35813;&#27169;&#22411;&#37319;&#29992;&#20018;&#32852;&#23618;&#32423;&#30340;&#38543;&#26426;&#36807;&#31243;&#65292;&#24182;&#23454;&#29616;&#20102;&#30456;&#23545;&#20110;TGPs&#21644;DGPs&#30340;&#28789;&#27963;&#24615;&#22686;&#24378;&#12290;&#36890;&#36807;&#20351;&#29992;&#21464;&#20998;&#25512;&#29702;&#65292;&#21487;&#20197;&#36817;&#20284;&#25152;&#38656;&#30340;&#35745;&#31639;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#31616;&#21333;&#30452;&#25509;&#30340;&#25512;&#29702;&#31639;&#27861;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36716;&#25442;&#39640;&#26031;&#36807;&#31243;&#65288;TGPs&#65289;&#26159;&#36890;&#36807;&#20351;&#29992;&#21487;&#36870;&#36716;&#25442;&#20174;&#20808;&#39564;&#36807;&#31243;&#65288;&#36890;&#24120;&#26159;&#39640;&#26031;&#36807;&#31243;&#65289;&#20013;&#36716;&#25442;&#26679;&#26412;&#26469;&#25351;&#23450;&#30340;&#38543;&#26426;&#36807;&#31243;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#22522;&#26412;&#36807;&#31243;&#30340;&#28789;&#27963;&#24615;&#12290;&#27492;&#22806;&#65292;&#19982;&#36890;&#36807;&#39640;&#26031;&#36807;&#31243;&#30340;&#23618;&#32423;&#20018;&#32852;&#26500;&#36896;&#30340;&#28145;&#24230;&#39640;&#26031;&#36807;&#31243;&#65288;DGPs&#65289;&#30456;&#27604;&#65292;TGPs&#23454;&#29616;&#20102;&#31454;&#20105;&#24615;&#32467;&#26524;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28145;&#24230;&#36716;&#25442;&#39640;&#26031;&#36807;&#31243;&#65288;DTGPs&#65289;&#30340;TGP&#25512;&#24191;&#65292;&#23427;&#36981;&#24490;&#20018;&#32852;&#38543;&#26426;&#36807;&#31243;&#23618;&#30340;&#36235;&#21183;&#12290;&#26356;&#20934;&#30830;&#22320;&#35828;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#20010;&#22810;&#23618;&#27169;&#22411;&#65292;&#20854;&#20013;&#27599;&#19968;&#23618;&#37117;&#26159;&#19968;&#20010;TGP&#12290;&#36825;&#31181;&#25512;&#24191;&#24847;&#21619;&#30528;&#30456;&#23545;&#20110;TGPs&#21644;DGPs&#37117;&#25552;&#39640;&#20102;&#28789;&#27963;&#24615;&#12290;&#22312;&#36825;&#26679;&#30340;&#27169;&#22411;&#20013;&#36827;&#34892;&#31934;&#30830;&#25512;&#29702;&#26159;&#22256;&#38590;&#30340;&#12290;&#20294;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#20351;&#29992;&#21464;&#20998;&#25512;&#29702;&#26469;&#36817;&#20284;&#25152;&#38656;&#30340;&#35745;&#31639;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#27969;&#34892;&#30340;DSVI&#25512;&#29702;&#31639;&#27861;&#30340;&#30452;&#25509;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformed Gaussian Processes (TGPs) are stochastic processes specified by transforming samples from the joint distribution from a prior process (typically a GP) using an invertible transformation; increasing the flexibility of the base process.  Furthermore, they achieve competitive results compared with Deep Gaussian Processes (DGPs), which are another generalization constructed by a hierarchical concatenation of GPs. In this work, we propose a generalization of TGPs named Deep Transformed Gaussian Processes (DTGPs), which follows the trend of concatenating layers of stochastic processes. More precisely, we obtain a multi-layer model in which each layer is a TGP. This generalization implies an increment of flexibility with respect to both TGPs and DGPs. Exact inference in such a model is intractable. However, we show that one can use variational inference to approximate the required computations yielding a straightforward extension of the popular DSVI inference algorithm Salimbeni e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#20809;&#27969;&#20013;&#30340;&#23545;&#25239;&#24615;&#36148;&#29255;&#25915;&#20987;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#30446;&#21069;&#30340;&#26816;&#27979;&#21644;&#21435;&#38500;&#38450;&#24481;&#31574;&#30053;&#19981;&#20165;&#38477;&#20302;&#20102;&#20809;&#27969;&#36136;&#37327;&#65292;&#21516;&#26102;&#20063;&#25439;&#23475;&#20102;&#25269;&#24481;&#36148;&#29255;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.17403</link><description>&lt;p&gt;
&#26816;&#27979;&#38450;&#24481;: &#20809;&#27969;&#20013;&#23545;&#25239;&#24615;&#36148;&#29255;&#25915;&#20987;&#30340;&#31354;&#27934;&#25215;&#35834;
&lt;/p&gt;
&lt;p&gt;
Detection Defenses: An Empty Promise against Adversarial Patch Attacks on Optical Flow. (arXiv:2310.17403v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#20809;&#27969;&#20013;&#30340;&#23545;&#25239;&#24615;&#36148;&#29255;&#25915;&#20987;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#30446;&#21069;&#30340;&#26816;&#27979;&#21644;&#21435;&#38500;&#38450;&#24481;&#31574;&#30053;&#19981;&#20165;&#38477;&#20302;&#20102;&#20809;&#27969;&#36136;&#37327;&#65292;&#21516;&#26102;&#20063;&#25439;&#23475;&#20102;&#25269;&#24481;&#36148;&#29255;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#25918;&#32622;&#22312;&#20219;&#24847;&#22330;&#26223;&#20301;&#32622;&#26102;&#65292;&#23545;&#25239;&#24615;&#36148;&#29255;&#30772;&#22351;&#20102;&#20809;&#27969;&#39044;&#27979;&#30340;&#21487;&#38752;&#24615;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#23545;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#36816;&#21160;&#26816;&#27979;&#21450;&#20854;&#19979;&#28216;&#24212;&#29992;&#26500;&#25104;&#20102;&#30495;&#23454;&#23041;&#32961;&#12290;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#26816;&#27979;&#21644;&#21435;&#38500;&#23545;&#25239;&#24615;&#36148;&#29255;&#30340;&#38450;&#24481;&#31574;&#30053;&#65292;&#20294;&#20854;&#23545;&#24213;&#23618;&#36816;&#21160;&#39044;&#27979;&#30340;&#24433;&#21709;&#23578;&#26410;&#34987;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#24403;&#21069;&#21487;&#29992;&#30340;&#26816;&#27979;&#21644;&#21435;&#38500;&#38450;&#24481;&#31574;&#30053;ILP&#21644;LGS&#23545;&#19968;&#31995;&#21015;&#20808;&#36827;&#20809;&#27969;&#26041;&#27861;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#30740;&#31350;&#65292;&#24182;&#38416;&#26126;&#20102;&#23427;&#20204;&#23545;&#26368;&#32456;&#20809;&#27969;&#39044;&#27979;&#30340;&#36136;&#37327;&#21644;&#40065;&#26834;&#24615;&#30340;&#21103;&#20316;&#29992;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23454;&#26045;&#20102;&#38450;&#24481;&#24863;&#30693;&#25915;&#20987;&#65292;&#20197;&#35843;&#26597;&#24403;&#21069;&#30340;&#38450;&#24481;&#26159;&#21542;&#33021;&#22815;&#25269;&#24481;&#32771;&#34385;&#38450;&#24481;&#26426;&#21046;&#30340;&#25915;&#20987;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#24471;&#20986;&#20102;&#20004;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#32467;&#26524;&#65306;&#26816;&#27979;&#21644;&#21435;&#38500;&#38450;&#24481;&#31574;&#30053;&#19981;&#20165;&#38477;&#20302;&#20102;&#33391;&#22909;&#22330;&#26223;&#19979;&#30340;&#20809;&#27969;&#36136;&#37327;&#65292;&#21516;&#26102;&#20063;&#25439;&#23475;&#20102;&#38024;&#23545;&#36148;&#29255;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial patches undermine the reliability of optical flow predictions when placed in arbitrary scene locations. Therefore, they pose a realistic threat to real-world motion detection and its downstream applications. Potential remedies are defense strategies that detect and remove adversarial patches, but their influence on the underlying motion prediction has not been investigated. In this paper, we thoroughly examine the currently available detect-and-remove defenses ILP and LGS for a wide selection of state-of-the-art optical flow methods, and illuminate their side effects on the quality and robustness of the final flow predictions. In particular, we implement defense-aware attacks to investigate whether current defenses are able to withstand attacks that take the defense mechanism into account. Our experiments yield two surprising results: Detect-and-remove defenses do not only lower the optical flow quality on benign scenes, in doing so, they also harm the robustness under patc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32467;&#21512;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#21644;&#20020;&#26102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;TCN&#65289;&#65292;&#20351;&#29992;&#26032;&#30340;&#21453;&#24212;Smiles-like&#34920;&#31034;&#23454;&#29616;&#20102;&#20840;&#26032;&#30340;&#21270;&#23398;&#21453;&#24212;&#29983;&#25104;&#65292;&#24182;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#21457;&#29616;&#24494;&#35843;&#21327;&#35758;&#23545;&#27169;&#22411;&#29983;&#25104;&#33539;&#22260;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.17341</link><description>&lt;p&gt;
&#36890;&#36807;&#26242;&#26102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#30340;&#20840;&#26032;&#21270;&#23398;&#21453;&#24212;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
De-novo Chemical Reaction Generation by Means of Temporarily Convolutional Neural Networks. (arXiv:2310.17341v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17341
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32467;&#21512;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#21644;&#20020;&#26102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;TCN&#65289;&#65292;&#20351;&#29992;&#26032;&#30340;&#21453;&#24212;Smiles-like&#34920;&#31034;&#23454;&#29616;&#20102;&#20840;&#26032;&#30340;&#21270;&#23398;&#21453;&#24212;&#29983;&#25104;&#65292;&#24182;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#21457;&#29616;&#24494;&#35843;&#21327;&#35758;&#23545;&#27169;&#22411;&#29983;&#25104;&#33539;&#22260;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#20351;&#29992;&#26032;&#39062;&#30340;&#21453;&#24212;Smiles-like&#34920;&#31034;&#65288;CGRSmiles&#65289;&#26102;&#65292;&#23558;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#21644;&#20020;&#26102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;TCN&#65289;&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#20840;&#26032;&#30340;&#21453;&#24212;&#29983;&#25104;&#65292;&#24182;&#30452;&#25509;&#34701;&#21512;&#20102;&#21407;&#23376;&#26144;&#23556;&#12290;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#20197;&#20854;&#33258;&#22238;&#24402;&#29305;&#24615;&#32780;&#38395;&#21517;&#65292;&#24182;&#32463;&#24120;&#22312;&#35821;&#35328;&#24314;&#27169;&#20013;&#20351;&#29992;&#65292;&#30452;&#25509;&#24212;&#29992;&#20110;SMILES&#29983;&#25104;&#12290;&#30456;&#23545;&#36739;&#26032;&#30340;TCN&#20855;&#26377;&#31867;&#20284;&#30340;&#24615;&#36136;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24863;&#21463;&#37326;&#65292;&#24182;&#36981;&#23432;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25152;&#38656;&#30340;&#22240;&#26524;&#24615;&#12290;&#36890;&#36807;TCN&#21644;RNN&#34920;&#36798;&#30340;&#20004;&#31181;&#28508;&#22312;&#34920;&#31034;&#30340;&#32452;&#21512;&#30456;&#27604;&#20165;&#20351;&#29992;RNN&#26102;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#34920;&#26126;&#65292;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#23558;&#19981;&#21516;&#30340;&#24494;&#35843;&#21327;&#35758;&#24212;&#29992;&#20110;&#24863;&#20852;&#36259;&#30340;&#25968;&#25454;&#38598;&#26102;&#65292;&#23545;&#27169;&#22411;&#30340;&#29983;&#25104;&#33539;&#22260;&#26377;&#28145;&#36828;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present here a combination of two networks, Recurrent Neural Networks (RNN) and Temporarily Convolutional Neural Networks (TCN) in de novo reaction generation using the novel Reaction Smiles-like representation of reactions (CGRSmiles) with atom mapping directly incorporated. Recurrent Neural Networks are known for their autoregressive properties and are frequently used in language modelling with direct application to SMILES generation. The relatively novel TCNs possess similar properties with wide receptive field while obeying the causality required for natural language processing (NLP). The combination of both latent representations expressed through TCN and RNN results in an overall better performance compared to RNN alone. Additionally, it is shown that different fine-tuning protocols have a profound impact on generative scope of the model when applied on a dataset of interest via transfer learning.
&lt;/p&gt;</description></item><item><title>MimicTouch&#26159;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#27169;&#20223;&#20154;&#31867;&#30340;&#35302;&#35273;&#24341;&#23548;&#25511;&#21046;&#31574;&#30053;&#65292;&#36890;&#36807;&#25910;&#38598;&#26469;&#33258;&#20154;&#31867;&#31034;&#33539;&#32773;&#30340;&#22810;&#27169;&#24577;&#35302;&#35273;&#25968;&#25454;&#38598;&#65292;&#26469;&#23398;&#20064;&#24182;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2310.16917</link><description>&lt;p&gt;
MimicTouch: &#20351;&#29992;&#22810;&#27169;&#24577;&#35302;&#35273;&#21453;&#39304;&#23398;&#20064;&#20154;&#31867;&#30340;&#25511;&#21046;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
MimicTouch: Learning Human's Control Strategy with Multi-Modal Tactile Feedback. (arXiv:2310.16917v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16917
&lt;/p&gt;
&lt;p&gt;
MimicTouch&#26159;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#27169;&#20223;&#20154;&#31867;&#30340;&#35302;&#35273;&#24341;&#23548;&#25511;&#21046;&#31574;&#30053;&#65292;&#36890;&#36807;&#25910;&#38598;&#26469;&#33258;&#20154;&#31867;&#31034;&#33539;&#32773;&#30340;&#22810;&#27169;&#24577;&#35302;&#35273;&#25968;&#25454;&#38598;&#65292;&#26469;&#23398;&#20064;&#24182;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#20154;&#25216;&#26415;&#21644;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#35302;&#35273;&#22788;&#29702;&#30340;&#25972;&#21512;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#23398;&#20064;&#25191;&#34892;&#20687;&#23545;&#20934;&#21644;&#25554;&#20837;&#36825;&#26679;&#22797;&#26434;&#20219;&#21153;&#26102;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#20381;&#36182;&#26426;&#22120;&#20154;&#36965;&#25805;&#20316;&#25968;&#25454;&#21644;&#24378;&#21270;&#23398;&#20064;&#65292;&#24573;&#35270;&#20102;&#20154;&#31867;&#21463;&#35302;&#35273;&#21453;&#39304;&#24341;&#23548;&#19979;&#30340;&#25511;&#21046;&#31574;&#30053;&#25152;&#25552;&#20379;&#30340;&#20016;&#23500;&#35265;&#35299;&#12290;&#20026;&#20102;&#21033;&#29992;&#20154;&#31867;&#24863;&#35273;&#65292;&#29616;&#26377;&#30340;&#20174;&#20154;&#31867;&#23398;&#20064;&#30340;&#26041;&#27861;&#20027;&#35201;&#21033;&#29992;&#35270;&#35273;&#21453;&#39304;&#65292;&#24120;&#24120;&#24573;&#35270;&#20102;&#20154;&#31867;&#26412;&#33021;&#22320;&#21033;&#29992;&#35302;&#35273;&#21453;&#39304;&#23436;&#25104;&#22797;&#26434;&#25805;&#20316;&#30340;&#23453;&#36149;&#32463;&#39564;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;"MimicTouch"&#65292;&#27169;&#20223;&#20154;&#31867;&#30340;&#35302;&#35273;&#24341;&#23548;&#25511;&#21046;&#31574;&#30053;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;&#20154;&#31867;&#31034;&#33539;&#32773;&#37027;&#37324;&#25910;&#38598;&#22810;&#27169;&#24577;&#35302;&#35273;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#20154;&#31867;&#35302;&#35273;&#24341;&#23548;&#30340;&#25511;&#21046;&#31574;&#30053;&#26469;&#23436;&#25104;&#20219;&#21153;&#12290;&#25509;&#19979;&#26469;&#30340;&#27493;&#39588;&#28041;&#21450;&#25351;&#20196;&#30340;&#20256;&#36882;&#65292;&#20854;&#20013;&#26426;&#22120;&#20154;&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#30340;&#35302;&#35273;&#24341;&#23548;&#31574;&#30053;&#26469;&#25191;&#34892;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
In robotics and artificial intelligence, the integration of tactile processing is becoming increasingly pivotal, especially in learning to execute intricate tasks like alignment and insertion. However, existing works focusing on tactile methods for insertion tasks predominantly rely on robot teleoperation data and reinforcement learning, which do not utilize the rich insights provided by human's control strategy guided by tactile feedback. For utilizing human sensations, methodologies related to learning from humans predominantly leverage visual feedback, often overlooking the invaluable tactile feedback that humans inherently employ to finish complex manipulations. Addressing this gap, we introduce "MimicTouch", a novel framework that mimics human's tactile-guided control strategy. In this framework, we initially collect multi-modal tactile datasets from human demonstrators, incorporating human tactile-guided control strategies for task completion. The subsequent step involves instruc
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#29983;&#29289;&#21644;&#20154;&#24037;&#20449;&#24687;&#22788;&#29702;&#31995;&#32479;&#30340;&#34920;&#31034;&#19968;&#33268;&#24615;&#65292;&#25506;&#35752;&#20102;&#19981;&#21516;&#31995;&#32479;&#20043;&#38388;&#30340;&#34920;&#31034;&#26159;&#21542;&#19968;&#33268;&#20197;&#21450;&#22914;&#20309;&#35843;&#25972;&#34920;&#31034;&#20197;&#26356;&#22909;&#22320;&#21305;&#37197;&#20854;&#20182;&#31995;&#32479;&#12290;&#20026;&#20102;&#25913;&#21892;&#39046;&#22495;&#20043;&#38388;&#30340;&#20132;&#27969;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#20316;&#20026;&#20849;&#21516;&#35821;&#35328;&#12290;</title><link>http://arxiv.org/abs/2310.13018</link><description>&lt;p&gt;
&#23545;&#34920;&#31034;&#19968;&#33268;&#24615;&#36798;&#25104;&#20849;&#35782;
&lt;/p&gt;
&lt;p&gt;
Getting aligned on representational alignment. (arXiv:2310.13018v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13018
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#29983;&#29289;&#21644;&#20154;&#24037;&#20449;&#24687;&#22788;&#29702;&#31995;&#32479;&#30340;&#34920;&#31034;&#19968;&#33268;&#24615;&#65292;&#25506;&#35752;&#20102;&#19981;&#21516;&#31995;&#32479;&#20043;&#38388;&#30340;&#34920;&#31034;&#26159;&#21542;&#19968;&#33268;&#20197;&#21450;&#22914;&#20309;&#35843;&#25972;&#34920;&#31034;&#20197;&#26356;&#22909;&#22320;&#21305;&#37197;&#20854;&#20182;&#31995;&#32479;&#12290;&#20026;&#20102;&#25913;&#21892;&#39046;&#22495;&#20043;&#38388;&#30340;&#20132;&#27969;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#20316;&#20026;&#20849;&#21516;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21644;&#20154;&#24037;&#20449;&#24687;&#22788;&#29702;&#31995;&#32479;&#26500;&#24314;&#21487;&#20197;&#29992;&#26469;&#36827;&#34892;&#20998;&#31867;&#12289;&#25512;&#29702;&#12289;&#35268;&#21010;&#12289;&#23548;&#33322;&#21644;&#20915;&#31574;&#30340;&#19990;&#30028;&#34920;&#31034;&#12290;&#36825;&#20123;&#22810;&#26679;&#21270;&#31995;&#32479;&#25152;&#26500;&#24314;&#30340;&#34920;&#31034;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#26159;&#19968;&#33268;&#30340;&#65311;&#21363;&#20351;&#34920;&#31034;&#19981;&#21516;&#65292;&#26159;&#21542;&#20173;&#28982;&#33021;&#22815;&#23548;&#33268;&#30456;&#21516;&#30340;&#34892;&#20026;&#65311;&#31995;&#32479;&#22914;&#20309;&#20462;&#25913;&#23427;&#20204;&#30340;&#34920;&#31034;&#20197;&#26356;&#22909;&#22320;&#21305;&#37197;&#21478;&#19968;&#20010;&#31995;&#32479;&#30340;&#34920;&#31034;&#65311;&#36825;&#20123;&#20851;&#20110;&#34920;&#31034;&#19968;&#33268;&#24615;&#30740;&#31350;&#30340;&#38382;&#39064;&#26159;&#24403;&#20195;&#35748;&#30693;&#31185;&#23398;&#12289;&#31070;&#32463;&#31185;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#19968;&#20123;&#26368;&#27963;&#36291;&#30340;&#30740;&#31350;&#39046;&#22495;&#30340;&#26680;&#24515;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#23545;&#20110;&#23545;&#34920;&#31034;&#19968;&#33268;&#24615;&#24863;&#20852;&#36259;&#30340;&#30740;&#31350;&#31038;&#21306;&#20043;&#38388;&#30340;&#30693;&#35782;&#36716;&#31227;&#26377;&#38480;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#22312;&#19968;&#20010;&#39046;&#22495;&#30340;&#36827;&#23637;&#26368;&#32456;&#20250;&#22312;&#21478;&#19968;&#20010;&#39046;&#22495;&#29420;&#31435;&#22320;&#37325;&#26032;&#21457;&#29616;&#65292;&#32780;&#26356;&#24191;&#27867;&#30340;&#39046;&#22495;&#38388;&#20132;&#27969;&#23558;&#26159;&#26377;&#21033;&#30340;&#12290;&#20026;&#20102;&#25913;&#21892;&#39046;&#22495;&#20043;&#38388;&#30340;&#20132;&#27969;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#20316;&#20026;&#19968;&#31181;&#20849;&#21516;&#30340;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biological and artificial information processing systems form representations of the world that they can use to categorize, reason, plan, navigate, and make decisions. To what extent do the representations formed by these diverse systems agree? Can diverging representations still lead to the same behaviors? And how can systems modify their representations to better match those of another system? These questions pertaining to the study of \textbf{\emph{representational alignment}} are at the heart of some of the most active research areas in contemporary cognitive science, neuroscience, and machine learning. Unfortunately, there is limited knowledge-transfer between research communities interested in representational alignment, and much of the progress in one field ends up being rediscovered independently in another, when greater cross-field communication would be advantageous. To improve communication between fields, we propose a unifying framework that can serve as a common language b
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#27169;&#22411;&#21435;&#20559;&#32622;&#30340;&#26694;&#26550;&#65288;FMD&#65289;&#65292;&#21487;&#20197;&#26377;&#25928;&#35782;&#21035;&#12289;&#35780;&#20272;&#21644;&#28040;&#38500;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20559;&#35265;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#25104;&#26412;&#21644;&#35299;&#37322;&#24615;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2310.12560</link><description>&lt;p&gt;
&#24555;&#36895;&#27169;&#22411;&#21435;&#20559;&#32622;&#19982;&#26426;&#22120;&#21462;&#28040;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Fast Model Debias with Machine Unlearning. (arXiv:2310.12560v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12560
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#27169;&#22411;&#21435;&#20559;&#32622;&#30340;&#26694;&#26550;&#65288;FMD&#65289;&#65292;&#21487;&#20197;&#26377;&#25928;&#35782;&#21035;&#12289;&#35780;&#20272;&#21644;&#28040;&#38500;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20559;&#35265;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#25104;&#26412;&#21644;&#35299;&#37322;&#24615;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#35768;&#22810;&#29616;&#23454;&#22330;&#26223;&#20013;&#21487;&#33021;&#34920;&#29616;&#20986;&#20559;&#24046;&#30340;&#34892;&#20026;&#12290;&#20363;&#22914;&#65292;&#22312;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#20154;&#33080;&#35782;&#21035;&#25968;&#25454;&#38598;CelebA&#19978;&#35757;&#32451;&#30340;&#28145;&#24230;&#32593;&#32476;&#20542;&#21521;&#20110;&#39044;&#27979;&#22899;&#24615;&#30340;&#37329;&#33394;&#22836;&#21457;&#21644;&#30007;&#24615;&#30340;&#40657;&#33394;&#22836;&#21457;&#12290;&#36825;&#20123;&#20559;&#24046;&#19981;&#20165;&#21361;&#23475;&#20102;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#65292;&#32780;&#19988;&#20250;&#25345;&#32493;&#21644;&#25918;&#22823;&#31038;&#20250;&#20559;&#35265;&#65292;&#36825;&#23545;&#20110;&#21307;&#30103;&#12289;&#25307;&#32856;&#31561;&#33258;&#21160;&#20915;&#31574;&#36807;&#31243;&#23588;&#20854;&#20196;&#20154;&#25285;&#24551;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#33021;&#21152;&#21095;&#19981;&#21516;&#32676;&#20307;&#20043;&#38388;&#30340;&#19981;&#20844;&#24179;&#32463;&#27982;&#21644;&#31038;&#20250;&#19981;&#24179;&#31561;&#12290;&#29616;&#26377;&#30340;&#21435;&#20559;&#32622;&#26041;&#27861;&#22312;&#20559;&#35265;&#26631;&#35760;&#25110;&#27169;&#22411;&#37325;&#26032;&#35757;&#32451;&#26041;&#38754;&#25104;&#26412;&#39640;&#26114;&#65292;&#21516;&#26102;&#20063;&#22312;&#38416;&#26126;&#27169;&#22411;&#20869;&#37096;&#20559;&#35265;&#30340;&#36215;&#28304;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24555;&#36895;&#27169;&#22411;&#21435;&#20559;&#32622;&#26694;&#26550;(FMD)&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#12289;&#35780;&#20272;&#21644;&#28040;&#38500;&#35757;&#32451;&#27169;&#22411;&#20013;&#22266;&#26377;&#30340;&#20559;&#35265;&#12290;FMD&#36890;&#36807;&#26174;&#24335;&#30340;&#21453;&#20107;&#23454;&#26426;&#21046;&#26469;&#35782;&#21035;&#20559;&#32622;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent discoveries have revealed that deep neural networks might behave in a biased manner in many real-world scenarios. For instance, deep networks trained on a large-scale face recognition dataset CelebA tend to predict blonde hair for females and black hair for males. Such biases not only jeopardize the robustness of models but also perpetuate and amplify social biases, which is especially concerning for automated decision-making processes in healthcare, recruitment, etc., as they could exacerbate unfair economic and social inequalities among different groups. Existing debiasing methods suffer from high costs in bias labeling or model re-training, while also exhibiting a deficiency in terms of elucidating the origins of biases within the model. To this respect, we propose a fast model debiasing framework (FMD) which offers an efficient approach to identify, evaluate and remove biases inherent in trained models. The FMD identifies biased attributes through an explicit counterfactual 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36879;&#26126;&#30340;&#22522;&#20110;&#27010;&#24565;&#35299;&#37322;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65288;ACE&#65289;&#65292;&#33021;&#22815;&#25552;&#20379;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#35299;&#37322;&#21644;&#24322;&#24120;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#22312;&#25512;&#36827;&#24322;&#24120;&#26816;&#27979;&#30340;&#36879;&#26126;&#24230;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#20154;&#26426;&#20132;&#20114;&#65292;&#24182;&#19988;&#22312;&#24615;&#33021;&#19978;&#35201;&#20040;&#26356;&#39640;&#65292;&#35201;&#20040;&#19982;&#40657;&#30418;&#19981;&#21487;&#35299;&#37322;&#27169;&#22411;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2310.10702</link><description>&lt;p&gt;
&#36879;&#26126;&#30340;&#22522;&#20110;&#27010;&#24565;&#35299;&#37322;&#30340;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Transparent Anomaly Detection via Concept-based Explanations. (arXiv:2310.10702v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10702
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36879;&#26126;&#30340;&#22522;&#20110;&#27010;&#24565;&#35299;&#37322;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65288;ACE&#65289;&#65292;&#33021;&#22815;&#25552;&#20379;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#35299;&#37322;&#21644;&#24322;&#24120;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#22312;&#25512;&#36827;&#24322;&#24120;&#26816;&#27979;&#30340;&#36879;&#26126;&#24230;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#20154;&#26426;&#20132;&#20114;&#65292;&#24182;&#19988;&#22312;&#24615;&#33021;&#19978;&#35201;&#20040;&#26356;&#39640;&#65292;&#35201;&#20040;&#19982;&#40657;&#30418;&#19981;&#21487;&#35299;&#37322;&#27169;&#22411;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#36827;&#27493;&#25552;&#21319;&#20102;&#24322;&#24120;&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#21644;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#38656;&#35201;&#36229;&#20986;&#20934;&#30830;&#24615;&#30340;&#36879;&#26126;&#24230;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#24322;&#24120;&#26816;&#27979;&#30340;&#20219;&#21153;&#38598;&#20013;&#22312;&#25214;&#20986;&#32473;&#23450;&#26679;&#26412;&#26159;&#21542;&#36981;&#24490;&#23398;&#20064;&#21040;&#30340;&#20998;&#24067;&#12290;&#29616;&#26377;&#26041;&#27861;&#32570;&#20047;&#23545;&#20854;&#32467;&#26524;&#36827;&#34892;&#28165;&#26224;&#35299;&#37322;&#30340;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36879;&#26126;&#30340;&#24322;&#24120;&#26816;&#27979;&#27010;&#24565;&#35299;&#37322;&#65288;ACE&#65289;&#26041;&#27861;&#12290;ACE&#33021;&#22815;&#20197;&#27010;&#24565;&#30340;&#24418;&#24335;&#25552;&#20379;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#35299;&#37322;&#21644;&#24322;&#24120;&#39044;&#27979;&#12290;&#25454;&#25105;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#31687;&#25552;&#20986;&#35774;&#35745;&#21487;&#35299;&#37322;&#24322;&#24120;&#26816;&#27979;&#30340;&#35770;&#25991;&#12290;&#38500;&#20102;&#20419;&#36827;&#24322;&#24120;&#26816;&#27979;&#30340;&#36879;&#26126;&#24230;&#65292;&#23427;&#36824;&#21487;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#20154;&#26426;&#20132;&#20114;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#32467;&#26524;&#35201;&#20040;&#26356;&#39640;&#65292;&#35201;&#20040;&#19982;&#40657;&#30418;&#19981;&#21487;&#35299;&#37322;&#27169;&#22411;&#30456;&#24403;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;ACE&#22312;&#19977;&#20010;&#29616;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advancements in deep learning techniques have given a boost to the performance of anomaly detection. However, real-world and safety-critical applications demand a level of transparency and reasoning beyond accuracy. The task of anomaly detection (AD) focuses on finding whether a given sample follows the learned distribution. Existing methods lack the ability to reason with clear explanations for their outcomes. Hence to overcome this challenge, we propose Transparent {A}nomaly Detection {C}oncept {E}xplanations (ACE). ACE is able to provide human interpretable explanations in the form of concepts along with anomaly prediction. To the best of our knowledge, this is the first paper that proposes interpretable by-design anomaly detection. In addition to promoting transparency in AD, it allows for effective human-model interaction. Our proposed model shows either higher or comparable results to black-box uninterpretable models. We validate the performance of ACE across three realistic data
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#23545;&#30005;&#21147;&#31995;&#32479;&#30340;&#24674;&#22797;&#26354;&#32447;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#20102;&#20004;&#31181;&#20027;&#35201;&#30340;&#24674;&#22797;&#26354;&#32447;&#21407;&#22411;&#65306;&#19977;&#35282;&#24418;&#26354;&#32447;&#21644;&#26799;&#24418;&#26354;&#32447;&#65292;&#20998;&#21035;&#34920;&#24449;&#20102;&#24555;&#36895;&#24674;&#22797;&#21644;&#36880;&#27493;&#24674;&#22797;&#30340;&#24674;&#22797;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2310.10030</link><description>&lt;p&gt;
&#20351;&#29992;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#25581;&#31034;&#30005;&#21147;&#31995;&#32479;&#24674;&#22797;&#26354;&#32447;&#30340;&#22522;&#26412;&#29305;&#24615;
&lt;/p&gt;
&lt;p&gt;
Unraveling Fundamental Properties of Power System Resilience Curves using Unsupervised Machine Learning. (arXiv:2310.10030v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10030
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#23545;&#30005;&#21147;&#31995;&#32479;&#30340;&#24674;&#22797;&#26354;&#32447;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#20102;&#20004;&#31181;&#20027;&#35201;&#30340;&#24674;&#22797;&#26354;&#32447;&#21407;&#22411;&#65306;&#19977;&#35282;&#24418;&#26354;&#32447;&#21644;&#26799;&#24418;&#26354;&#32447;&#65292;&#20998;&#21035;&#34920;&#24449;&#20102;&#24555;&#36895;&#24674;&#22797;&#21644;&#36880;&#27493;&#24674;&#22797;&#30340;&#24674;&#22797;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#35774;&#26045;&#24674;&#22797;&#30340;&#26631;&#20934;&#27169;&#22411;&#8212;&#8212;&#24674;&#22797;&#19977;&#35282;&#65292;&#19968;&#30452;&#26159;&#34920;&#24449;&#21644;&#37327;&#21270;&#22522;&#30784;&#35774;&#26045;&#24674;&#22797;&#33021;&#21147;&#30340;&#20027;&#35201;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#29702;&#35770;&#27169;&#22411;&#21482;&#25552;&#20379;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#25152;&#26377;&#22522;&#30784;&#35774;&#26045;&#31995;&#32479;&#30340;&#26694;&#26550;&#12290;&#29616;&#26377;&#30740;&#31350;&#22823;&#37096;&#20998;&#26159;&#22522;&#20110;&#27169;&#25311;&#31995;&#32479;&#24615;&#33021;&#26500;&#24314;&#30340;&#20998;&#26512;&#27169;&#22411;&#26469;&#30740;&#31350;&#22522;&#30784;&#35774;&#26045;&#24674;&#22797;&#26354;&#32447;&#30340;&#29305;&#24449;&#12290;&#26377;&#38480;&#30340;&#23454;&#35777;&#30740;&#31350;&#38480;&#21046;&#20102;&#25105;&#20204;&#20840;&#38754;&#29702;&#35299;&#21644;&#39044;&#27979;&#22522;&#30784;&#35774;&#26045;&#31995;&#32479;&#30340;&#24674;&#22797;&#29305;&#24615;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#30740;&#31350;&#32771;&#23519;&#20102;&#19982;&#19977;&#27425;&#37325;&#22823;&#26497;&#31471;&#22825;&#27668;&#20107;&#20214;&#20013;&#26029;&#30005;&#26377;&#20851;&#30340;200&#22810;&#20010;&#24674;&#22797;&#26354;&#32447;&#12290;&#36890;&#36807;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#26354;&#32447;&#21407;&#22411;&#20197;&#21450;&#27599;&#20010;&#24674;&#22797;&#26354;&#32447;&#21407;&#22411;&#30340;&#22522;&#26412;&#29305;&#24615;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#30005;&#21147;&#31995;&#32479;&#24674;&#22797;&#26354;&#32447;&#23384;&#22312;&#20004;&#31181;&#20027;&#35201;&#21407;&#22411;&#65306;&#19977;&#35282;&#24418;&#26354;&#32447;&#21644;&#26799;&#24418;&#26354;&#32447;&#12290;&#19977;&#35282;&#24418;&#26354;&#32447;&#34920;&#24449;&#20102;&#24674;&#22797;&#36807;&#31243;&#30340;&#24555;&#36895;&#24674;&#22797;&#65292;&#32780;&#26799;&#24418;&#26354;&#32447;&#34920;&#31034;&#20102;																&#24674;&#22797;&#36807;&#31243;&#30340;&#36880;&#27493;&#24674;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;
The standard model of infrastructure resilience, the resilience triangle, has been the primary way of characterizing and quantifying infrastructure resilience. However, the theoretical model merely provides a one-size-fits-all framework for all infrastructure systems. Most of the existing studies examine the characteristics of infrastructure resilience curves based on analytical models constructed upon simulated system performance. Limited empirical studies hindered our ability to fully understand and predict resilience characteristics in infrastructure systems. To address this gap, this study examined over 200 resilience curves related to power outages in three major extreme weather events. Using unsupervised machine learning, we examined different curve archetypes, as well as the fundamental properties of each resilience curve archetype. The results show two primary archetypes for power system resilience curves, triangular, and trapezoidal curves. Triangular curves characterize resil
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21517;&#20026;QUIK&#30340;&#28151;&#21512;&#37327;&#21270;&#31574;&#30053;&#65292;&#22312;&#20445;&#25345;&#33391;&#22909;&#31934;&#24230;&#30340;&#21516;&#26102;&#23454;&#29616;&#22823;&#22411;&#29983;&#25104;&#27169;&#22411;&#30340;&#23454;&#38469;&#36895;&#24230;&#25552;&#21319;&#65292;&#36890;&#36807;&#23558;&#26435;&#37325;&#21644;&#28608;&#27963;&#20540;&#36716;&#25442;&#20026;4&#20301;&#65292;&#24182;&#25552;&#20379;&#39640;&#25928;&#29575;&#30340;&#36880;&#23618;&#36816;&#34892;&#26102;GPU&#20869;&#26680;&#65292;&#23454;&#29616;&#20102;&#39640;&#36798;3.1&#20493;&#30340;&#23454;&#38469;&#31471;&#21040;&#31471;&#21534;&#21520;&#37327;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.09259</link><description>&lt;p&gt;
&#36808;&#21521;&#31471;&#21040;&#31471;&#30340;&#22522;&#20110;&#29983;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;4&#20301;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Towards End-to-end 4-Bit Inference on Generative Large Language Models. (arXiv:2310.09259v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21517;&#20026;QUIK&#30340;&#28151;&#21512;&#37327;&#21270;&#31574;&#30053;&#65292;&#22312;&#20445;&#25345;&#33391;&#22909;&#31934;&#24230;&#30340;&#21516;&#26102;&#23454;&#29616;&#22823;&#22411;&#29983;&#25104;&#27169;&#22411;&#30340;&#23454;&#38469;&#36895;&#24230;&#25552;&#21319;&#65292;&#36890;&#36807;&#23558;&#26435;&#37325;&#21644;&#28608;&#27963;&#20540;&#36716;&#25442;&#20026;4&#20301;&#65292;&#24182;&#25552;&#20379;&#39640;&#25928;&#29575;&#30340;&#36880;&#23618;&#36816;&#34892;&#26102;GPU&#20869;&#26680;&#65292;&#23454;&#29616;&#20102;&#39640;&#36798;3.1&#20493;&#30340;&#23454;&#38469;&#31471;&#21040;&#31471;&#21534;&#21520;&#37327;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20110;&#20687;LLaMA&#21644;OPT&#36825;&#26679;&#30340;&#22823;&#22411;&#29983;&#25104;&#27169;&#22411;&#65292;&#22823;&#22810;&#25968;&#25512;&#29702;&#35745;&#31639;&#21487;&#20197;&#36890;&#36807;&#23558;&#26435;&#37325;&#21644;&#28608;&#27963;&#20540;&#37117;&#36716;&#25442;&#20026;4&#20301;&#26469;&#23436;&#25104;&#65292;&#36825;&#31181;&#26041;&#24335;&#21487;&#20197;&#22312;&#20445;&#25345;&#33391;&#22909;&#31934;&#24230;&#30340;&#21516;&#26102;&#23454;&#29616;&#23454;&#38469;&#36895;&#24230;&#25552;&#21319;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#21517;&#20026;QUIK&#30340;&#28151;&#21512;&#37327;&#21270;&#31574;&#30053;&#23454;&#29616;&#20102;&#36825;&#19968;&#30446;&#26631;&#65292;&#35813;&#31574;&#30053;&#23558;&#22823;&#37096;&#20998;&#26435;&#37325;&#21644;&#28608;&#27963;&#20540;&#21387;&#32553;&#20026;4&#20301;&#65292;&#21516;&#26102;&#20445;&#30041;&#19968;&#20123;&#31163;&#32676;&#26435;&#37325;&#21644;&#28608;&#27963;&#20540;&#30340;&#36739;&#39640;&#31934;&#24230;&#12290;&#20851;&#38190;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#26696;&#32771;&#34385;&#21040;&#20102;&#35745;&#31639;&#25928;&#29575;&#65306;&#25105;&#20204;&#25552;&#20379;&#20102;&#39640;&#25928;&#29575;&#30340;&#36880;&#23618;&#36816;&#34892;&#26102;GPU&#20869;&#26680;&#65292;&#30456;&#23545;&#20110;FP16&#25191;&#34892;&#21487;&#20197;&#23454;&#29616;&#39640;&#36798;3.1&#20493;&#30340;&#23454;&#38469;&#31471;&#21040;&#31471;&#21534;&#21520;&#37327;&#25552;&#21319;&#12290;&#25105;&#20204;&#22312;https://github.com/IST-DASLab/QUIK&#19978;&#25552;&#20379;&#20102;&#20195;&#30721;&#21644;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show that the majority of the inference computations for large generative models such as LLaMA and OPT can be performed with both weights and activations being cast to 4 bits, in a way that leads to practical speedups while at the same time maintaining good accuracy. We achieve this via a hybrid quantization strategy called QUIK, which compresses most of the weights and activations to 4-bit, while keeping some outlier weights and activations in higher-precision. Crucially, our scheme is designed with computational efficiency in mind: we provide GPU kernels with highly-efficient layer-wise runtimes, which lead to practical end-to-end throughput improvements of up to 3.1x relative to FP16 execution. Code and models are provided at https://github.com/IST-DASLab/QUIK.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#27979;&#22320;&#32447;&#21644;&#27969;&#21160;&#26469;&#25551;&#36848;&#21487;&#24494;&#27969;&#24418;&#19978;&#30340;&#36317;&#31163;&#21644;&#38271;&#24230;&#26368;&#23567;&#21270;&#26354;&#32447;&#12290;&#36825;&#20026;&#22312;&#19981;&#21516;iable&#27969;&#24418;&#19978;&#36827;&#34892;&#32479;&#35745;&#21644;&#38477;&#38454;&#24314;&#27169;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2310.06157</link><description>&lt;p&gt;
&#22522;&#20110;&#27969;&#24418;&#30340;Eikonal&#26041;&#31243;&#65306;&#21487;&#24494;&#27969;&#24418;&#19978;&#30340;&#27979;&#22320;&#36317;&#31163;&#21644;&#27969;&#21160;
&lt;/p&gt;
&lt;p&gt;
Manifold-augmented Eikonal Equations: Geodesic Distances and Flows on Differentiable Manifolds. (arXiv:2310.06157v1 [cs.CG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#27979;&#22320;&#32447;&#21644;&#27969;&#21160;&#26469;&#25551;&#36848;&#21487;&#24494;&#27969;&#24418;&#19978;&#30340;&#36317;&#31163;&#21644;&#38271;&#24230;&#26368;&#23567;&#21270;&#26354;&#32447;&#12290;&#36825;&#20026;&#22312;&#19981;&#21516;iable&#27969;&#24418;&#19978;&#36827;&#34892;&#32479;&#35745;&#21644;&#38477;&#38454;&#24314;&#27169;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21457;&#29616;&#30340;&#27969;&#24418;&#25552;&#20379;&#20102;&#24213;&#23618;&#25968;&#25454;&#30340;&#32039;&#20945;&#34920;&#31034;&#12290;&#36825;&#20123;&#27969;&#24418;&#19978;&#30340;&#27979;&#22320;&#32447;&#23450;&#20041;&#20102;&#23616;&#37096;&#38271;&#24230;&#26368;&#23567;&#21270;&#26354;&#32447;&#65292;&#24182;&#25552;&#20379;&#20102;&#36317;&#31163;&#30340;&#27010;&#24565;&#65292;&#36825;&#23545;&#20110;&#38477;&#38454;&#24314;&#27169;&#12289;&#32479;&#35745;&#25512;&#26029;&#21644;&#25554;&#20540;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#21442;&#25968;&#21270;&#26041;&#27861;&#26469;&#34920;&#31034;&#27969;&#24418;&#19978;&#30340;&#36317;&#31163;&#22330;&#21644;&#27979;&#22320;&#27969;&#21160;&#65292;&#21033;&#29992;&#25193;&#23637;&#30340;Eikonal&#26041;&#31243;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#27969;&#24418;&#30340;&#20960;&#20309;&#29305;&#24615;&#22914;&#20309;&#24433;&#21709;&#36317;&#31163;&#22330;&#65292;&#24182;&#21033;&#29992;&#27979;&#22320;&#27969;&#21160;&#30452;&#25509;&#33719;&#24471;&#20840;&#23616;&#38271;&#24230;&#26368;&#23567;&#21270;&#26354;&#32447;&#12290;&#36825;&#39033;&#24037;&#20316;&#20026;&#22312;&#21487;&#24494;&#27969;&#24418;&#19978;&#36827;&#34892;&#32479;&#35745;&#21644;&#38477;&#38454;&#24314;&#27169;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
Manifolds discovered by machine learning models provide a compact representation of the underlying data. Geodesics on these manifolds define locally length-minimising curves and provide a notion of distance, which are key for reduced-order modelling, statistical inference, and interpolation. In this work, we propose a model-based parameterisation for distance fields and geodesic flows on manifolds, exploiting solutions of a manifold-augmented Eikonal equation. We demonstrate how the geometry of the manifold impacts the distance field, and exploit the geodesic flow to obtain globally length-minimising curves directly. This work opens opportunities for statistics and reduced-order modelling on differentiable manifolds.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#31867;&#20284;&#26679;&#26412;&#32858;&#31867;&#8221;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#26367;&#25442;&#20010;&#20307;&#30340;&#25935;&#24863;&#29305;&#24449;&#20026;&#32858;&#31867;&#30340;&#24179;&#22343;&#20540;&#26469;&#22686;&#24378;&#38544;&#31169;&#12290;&#36890;&#36807;&#23545;&#20351;&#29992;&#21311;&#21517;&#32858;&#31867;&#20013;&#24515;&#35757;&#32451;&#27169;&#22411;&#30340;&#31934;&#30830;&#20998;&#26512;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#19981;&#21516;&#27169;&#22411;&#32452;&#25104;&#37096;&#20998;&#23545;&#27867;&#21270;&#35823;&#24046;&#30340;&#24433;&#21709;&#65292;&#24182;&#35777;&#26126;&#22312;&#26576;&#20123;&#39640;&#32500;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#21311;&#21517;&#32858;&#31867;&#20013;&#24515;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#21462;&#24471;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.04015</link><description>&lt;p&gt;
&#36890;&#36807;&#31867;&#20284;&#26679;&#26412;&#32858;&#31867;&#23398;&#20064;&#65306;&#23545;&#27169;&#22411;&#27867;&#21270;&#30340;&#31934;&#30830;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Learning via Look-Alike Clustering: A Precise Analysis of Model Generalization. (arXiv:2310.04015v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#31867;&#20284;&#26679;&#26412;&#32858;&#31867;&#8221;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#26367;&#25442;&#20010;&#20307;&#30340;&#25935;&#24863;&#29305;&#24449;&#20026;&#32858;&#31867;&#30340;&#24179;&#22343;&#20540;&#26469;&#22686;&#24378;&#38544;&#31169;&#12290;&#36890;&#36807;&#23545;&#20351;&#29992;&#21311;&#21517;&#32858;&#31867;&#20013;&#24515;&#35757;&#32451;&#27169;&#22411;&#30340;&#31934;&#30830;&#20998;&#26512;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#19981;&#21516;&#27169;&#22411;&#32452;&#25104;&#37096;&#20998;&#23545;&#27867;&#21270;&#35823;&#24046;&#30340;&#24433;&#21709;&#65292;&#24182;&#35777;&#26126;&#22312;&#26576;&#20123;&#39640;&#32500;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#21311;&#21517;&#32858;&#31867;&#20013;&#24515;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#21462;&#24471;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#65292;&#20294;&#30830;&#20445;&#29992;&#25143;&#25968;&#25454;&#30340;&#20445;&#25252;&#20173;&#28982;&#26159;&#36825;&#20123;&#23398;&#20064;&#31995;&#32479;&#24320;&#21457;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#20851;&#27880;&#28857;&#12290;&#22686;&#24378;&#38544;&#31169;&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#20351;&#29992;&#21311;&#21517;&#25968;&#25454;&#32780;&#19981;&#26159;&#20010;&#20307;&#25968;&#25454;&#26469;&#35757;&#32451;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#31867;&#20284;&#26679;&#26412;&#32858;&#31867;&#8221;&#30340;&#33258;&#28982;&#25216;&#26415;&#65292;&#23427;&#28041;&#21450;&#23558;&#20010;&#20307;&#30340;&#25935;&#24863;&#29305;&#24449;&#26367;&#25442;&#20026;&#32858;&#31867;&#30340;&#24179;&#22343;&#20540;&#12290;&#25105;&#20204;&#23545;&#20351;&#29992;&#21311;&#21517;&#32858;&#31867;&#20013;&#24515;&#35757;&#32451;&#27169;&#22411;&#22914;&#20309;&#24433;&#21709;&#20854;&#27867;&#21270;&#33021;&#21147;&#36827;&#34892;&#20102;&#31934;&#30830;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#20851;&#27880;&#19968;&#20010;&#28176;&#36817;&#24773;&#20917;&#65292;&#21363;&#35757;&#32451;&#38598;&#30340;&#22823;&#23567;&#19982;&#29305;&#24449;&#32500;&#24230;&#25104;&#27604;&#20363;&#22686;&#38271;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#22522;&#20110;&#20984;&#39640;&#26031;&#26497;&#23567;&#21270;&#26497;&#22823;&#23450;&#29702;&#65288;Convex Gaussian Minimax Theorem&#65292;CGMT&#65289;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#29702;&#35770;&#19978;&#29702;&#35299;&#19981;&#21516;&#27169;&#22411;&#32452;&#25104;&#37096;&#20998;&#23545;&#27867;&#21270;&#35823;&#24046;&#30340;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#26576;&#20123;&#39640;&#32500;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#21311;&#21517;&#32858;&#31867;&#20013;&#24515;&#36827;&#34892;&#35757;&#32451;&#33021;&#22815;&#21462;&#24471;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
While personalized recommendations systems have become increasingly popular, ensuring user data protection remains a paramount concern in the development of these learning systems. A common approach to enhancing privacy involves training models using anonymous data rather than individual data. In this paper, we explore a natural technique called \emph{look-alike clustering}, which involves replacing sensitive features of individuals with the cluster's average values. We provide a precise analysis of how training models using anonymous cluster centers affects their generalization capabilities. We focus on an asymptotic regime where the size of the training set grows in proportion to the features dimension. Our analysis is based on the Convex Gaussian Minimax Theorem (CGMT) and allows us to theoretically understand the role of different model components on the generalization error. In addition, we demonstrate that in certain high-dimensional regimes, training over anonymous cluster cente
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#39046;&#22495;&#24863;&#30693;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21452;&#25552;&#31034;&#35843;&#20248;&#23454;&#29616;&#39046;&#22495;&#36866;&#24212;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20855;&#26377;&#26174;&#33879;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.03103</link><description>&lt;p&gt;
&#38754;&#21521;&#39046;&#22495;&#24863;&#30693;&#30340;&#32852;&#37030;&#23398;&#20064;&#30340;&#21452;&#25552;&#31034;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
Dual Prompt Tuning for Domain-Aware Federated Learning. (arXiv:2310.03103v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#39046;&#22495;&#24863;&#30693;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21452;&#25552;&#31034;&#35843;&#20248;&#23454;&#29616;&#39046;&#22495;&#36866;&#24212;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20855;&#26377;&#26174;&#33879;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#33539; paradigm&#65292;&#23427;&#20801;&#35768;&#22810;&#20010;&#23458;&#25143;&#31471;&#20351;&#29992;&#26412;&#22320;&#25968;&#25454;&#20849;&#21516;&#35757;&#32451;&#19968;&#20010;&#20849;&#20139;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23458;&#25143;&#20043;&#38388;&#26222;&#36941;&#23384;&#22312;&#39046;&#22495;&#21464;&#21270;&#65292;&#20256;&#32479;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#24448;&#24448;&#38590;&#20197;&#24456;&#22909;&#22320;&#27867;&#21270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#29616;&#23454;&#30340;&#32852;&#37030;&#23398;&#20064;&#22330;&#26223;&#65292;&#20854;&#20013;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#35757;&#32451;&#25968;&#25454;&#26469;&#33258;&#19981;&#21516;&#30340;&#39046;&#22495;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#25552;&#31034;&#23398;&#20064;&#25216;&#26415;&#26469;&#35299;&#20915;&#39046;&#22495;&#21464;&#21270;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32852;&#37030;&#21452;&#25552;&#31034;&#35843;&#20248;&#65288;Fed-DPT&#65289;&#30340;&#26032;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;Fed-DPT&#37319;&#29992;&#20102;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#28982;&#21518;&#24212;&#29992;&#20102;&#35270;&#35273;&#21644;&#25991;&#26412;&#25552;&#31034;&#35843;&#20248;&#26469;&#20419;&#36827;&#20998;&#24067;&#24335;&#25968;&#25454;&#19978;&#30340;&#39046;&#22495;&#36866;&#24212;&#12290;&#22823;&#37327;&#30340;Fed-DPT&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23427;&#22312;&#39046;&#22495;&#24863;&#30693;&#30340;&#32852;&#37030;&#23398;&#20064;&#20013;&#20855;&#26377;&#26174;&#33879;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning is a distributed machine learning paradigm that allows multiple clients to collaboratively train a shared model with their local data. Nonetheless, conventional federated learning algorithms often struggle to generalize well due to the ubiquitous domain shift across clients. In this work, we consider a challenging yet realistic federated learning scenario where the training data of each client originates from different domains. We address the challenges of domain shift by leveraging the technique of prompt learning, and propose a novel method called Federated Dual Prompt Tuning (Fed-DPT). Specifically, Fed-DPT employs a pre-trained vision-language model and then applies both visual and textual prompt tuning to facilitate domain adaptation over decentralized data. Extensive experiments of Fed-DPT demonstrate its significant effectiveness in domain-aware federated learning. With a pre-trained CLIP model (ViT-Base as image encoder), the proposed Fed-DPT attains 68.4% av
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#19981;&#21516;iable&#30340;&#21270;&#23398;&#29289;&#29702;&#26694;&#26550;DiffMix&#65292;&#21033;&#29992;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#23558;&#20998;&#23376;&#29289;&#31181;&#12289;&#32452;&#25104;&#21644;&#29615;&#22659;&#26465;&#20214;&#26144;&#23556;&#21040;&#28151;&#21512;&#29289;&#29289;&#29702;&#23450;&#24459;&#20013;&#30340;&#29289;&#29702;&#31995;&#25968;&#19978;&#12290;&#36890;&#36807;&#21019;&#24314;&#21487;&#23398;&#20064;&#30340;&#29289;&#29702;&#31995;&#25968;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#28151;&#21512;&#29289;&#28909;&#21147;&#23398;&#21644;&#36755;&#36816;&#23450;&#24459;&#65292;&#24182;&#23637;&#31034;&#20102;DiffMix&#30456;&#27604;&#32431;&#25968;&#25454;&#39537;&#21160;&#21464;&#37327;&#25913;&#36827;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#27169;&#22411;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.03047</link><description>&lt;p&gt;
&#36890;&#36807;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#23454;&#29616;&#21487;&#24494;&#20998;&#30340;&#21270;&#23398;&#29289;&#29702;&#27169;&#22411;&#65292;&#29992;&#20110;&#22522;&#20110;&#26799;&#24230;&#30340;&#28151;&#21512;&#29289;&#24615;&#33021;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Differentiable Chemical Physics by Geometric Deep Learning for Gradient-based Property Optimization of Mixtures. (arXiv:2310.03047v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03047
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#19981;&#21516;iable&#30340;&#21270;&#23398;&#29289;&#29702;&#26694;&#26550;DiffMix&#65292;&#21033;&#29992;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#23558;&#20998;&#23376;&#29289;&#31181;&#12289;&#32452;&#25104;&#21644;&#29615;&#22659;&#26465;&#20214;&#26144;&#23556;&#21040;&#28151;&#21512;&#29289;&#29289;&#29702;&#23450;&#24459;&#20013;&#30340;&#29289;&#29702;&#31995;&#25968;&#19978;&#12290;&#36890;&#36807;&#21019;&#24314;&#21487;&#23398;&#20064;&#30340;&#29289;&#29702;&#31995;&#25968;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#28151;&#21512;&#29289;&#28909;&#21147;&#23398;&#21644;&#36755;&#36816;&#23450;&#24459;&#65292;&#24182;&#23637;&#31034;&#20102;DiffMix&#30456;&#27604;&#32431;&#25968;&#25454;&#39537;&#21160;&#21464;&#37327;&#25913;&#36827;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#27169;&#22411;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21270;&#23398;&#28151;&#21512;&#29289;&#28385;&#36275;&#22810;&#30446;&#26631;&#24615;&#33021;&#24230;&#37327;&#21644;&#32422;&#26463;&#26465;&#20214;&#65292;&#21487;&#29992;&#20110;&#21270;&#23398;&#36807;&#31243;&#21644;&#30005;&#21270;&#23398;&#35774;&#22791;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#19981;&#21516;iable&#30340;&#21270;&#23398;&#29289;&#29702;&#26694;&#26550;DiffMix&#65292;&#21033;&#29992;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#65288;GDL&#65289;&#23558;&#20998;&#23376;&#29289;&#31181;&#65292;&#32452;&#25104;&#21644;&#29615;&#22659;&#26465;&#20214;&#26144;&#23556;&#21040;&#28151;&#21512;&#29289;&#29289;&#29702;&#23450;&#24459;&#20013;&#30340;&#29289;&#29702;&#31995;&#25968;&#19978;&#12290;&#25105;&#20204;&#36890;&#36807;&#21019;&#24314;&#21487;&#23398;&#20064;&#30340;&#29289;&#29702;&#31995;&#25968;&#26469;&#25193;&#23637;&#28151;&#21512;&#29289;&#28909;&#21147;&#23398;&#21644;&#36755;&#36816;&#23450;&#24459;&#65292;&#20854;&#20013;&#25105;&#20204;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#20998;&#23376;&#32534;&#30721;&#22120;&#65292;&#24182;&#24378;&#21046;&#23454;&#29616;&#36880;&#20998;&#37327;&#30340;&#25490;&#21015;&#19981;&#21464;&#24615;&#12290;&#25105;&#20204;&#20174;&#20108;&#20803;&#28151;&#21512;&#29289;&#30340;&#28909;&#21147;&#23398;&#24320;&#22987;&#35780;&#20272;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#36827;&#19968;&#27493;&#22312;&#22810;&#32452;&#20998;&#30005;&#35299;&#36136;&#28151;&#21512;&#29289;&#19978;&#23545;&#20854;&#36755;&#36816;&#24615;&#36136;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#27979;&#35797;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;DiffMix&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#27169;&#22411;&#40065;&#26834;&#24615;&#20248;&#20110;&#32431;&#25968;&#25454;&#39537;&#21160;&#21464;&#37327;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chemical mixtures, satisfying multi-objective performance metrics and constraints, enable their use in chemical processes and electrochemical devices. In this work, we develop a differentiable chemical-physics framework for modeling chemical mixtures, DiffMix, where geometric deep learning (GDL) is leveraged to map from molecular species, compositions and environment conditions, to physical coefficients in the mixture physics laws. In particular, we extend mixture thermodynamic and transport laws by creating learnable physical coefficients, where we use graph neural networks as the molecule encoder and enforce component-wise permutation-invariance. We start our model evaluations with thermodynamics of binary mixtures, and further benchmarked multicomponent electrolyte mixtures on their transport properties, in order to test the model generalizability. We show improved prediction accuracy and model robustness of DiffMix than its purely data-driven variants. Furthermore, we demonstrate t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#19968;&#32500;CapsNet&#21644;LSTM&#32593;&#32476;&#30340;&#28151;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#22810;&#27493;&#32929;&#25351;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;CapsNet&#20174;&#24207;&#21015;&#25968;&#25454;&#20013;&#29983;&#25104;&#39640;&#32423;&#33014;&#22218;&#65292;&#21516;&#26102;&#21033;&#29992;LSTM&#32593;&#32476;&#25429;&#33719;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;&#27169;&#22411;&#22312;&#30495;&#23454;&#32929;&#31080;&#24066;&#22330;&#25351;&#25968;&#19978;&#34920;&#29616;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.02090</link><description>&lt;p&gt;
1D-CapsNet-LSTM: &#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22810;&#27493;&#32929;&#25351;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
1D-CapsNet-LSTM: A Deep Learning-Based Model for Multi-Step Stock Index Forecasting. (arXiv:2310.02090v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02090
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#19968;&#32500;CapsNet&#21644;LSTM&#32593;&#32476;&#30340;&#28151;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#22810;&#27493;&#32929;&#25351;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;CapsNet&#20174;&#24207;&#21015;&#25968;&#25454;&#20013;&#29983;&#25104;&#39640;&#32423;&#33014;&#22218;&#65292;&#21516;&#26102;&#21033;&#29992;LSTM&#32593;&#32476;&#25429;&#33719;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;&#27169;&#22411;&#22312;&#30495;&#23454;&#32929;&#31080;&#24066;&#22330;&#25351;&#25968;&#19978;&#34920;&#29616;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27493;&#32929;&#25351;&#39044;&#27979;&#22312;&#37329;&#34701;&#39046;&#22495;&#23545;&#20110;&#26126;&#26234;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#30446;&#21069;&#20851;&#20110;&#27492;&#20219;&#21153;&#30340;&#39044;&#27979;&#26041;&#27861;&#24120;&#24120;&#30001;&#20110;&#25968;&#25454;&#30340;&#38543;&#26426;&#24615;&#21644;&#19981;&#31283;&#23450;&#24615;&#20135;&#29983;&#19981;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#65292;&#31361;&#26174;&#20102;&#23545;&#39640;&#32423;&#39044;&#27979;&#27169;&#22411;&#30340;&#38656;&#27714;&#12290;&#37492;&#20110;&#33014;&#22218;&#32593;&#32476;&#65288;CapsNet&#65289;&#22312;&#21508;&#31181;&#39044;&#27979;&#21644;&#20998;&#31867;&#20219;&#21153;&#20013;&#20248;&#20110;CNN&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;&#19968;&#32500;CapsNet&#19982;LSTM&#32593;&#32476;&#32467;&#21512;&#36215;&#26469;&#36827;&#34892;&#22810;&#27493;&#32929;&#25351;&#39044;&#27979;&#30340;&#28508;&#21147;&#12290;&#20026;&#27492;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#28151;&#21512;&#30340;&#19968;&#32500;CapsNet-LSTM&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#19968;&#32500;CapsNet&#20174;&#24207;&#21015;&#25968;&#25454;&#20013;&#29983;&#25104;&#39640;&#32423;&#33014;&#22218;&#65292;&#24182;&#21033;&#29992;LSTM&#32593;&#32476;&#25429;&#33719;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;&#20026;&#20102;&#20445;&#25345;&#19981;&#21516;&#39044;&#27979;&#26102;&#38388;&#33539;&#22260;&#19978;&#30340;&#38543;&#26426;&#20381;&#36182;&#20851;&#31995;&#65292;&#37319;&#29992;&#20102;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#65288;MIMO&#65289;&#31574;&#30053;&#12290;&#27169;&#22411;&#22312;&#21253;&#25324;&#26631;&#26222;500&#12289;&#36947;&#29756;&#26031;&#24037;&#19994;&#24179;&#22343;&#25351;&#25968;&#12289;&#32435;&#26031;&#36798;&#20811;&#32508;&#21512;&#25351;&#25968;&#21644;&#32445;&#20132;&#25152;&#22312;&#20869;&#30340;&#30495;&#23454;&#32929;&#31080;&#24066;&#22330;&#25351;&#25968;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#19982;&#22522;&#32447;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-step stock index forecasting is vital in finance for informed decision-making. Current forecasting methods on this task frequently produce unsatisfactory results due to the inherent data randomness and instability, thereby underscoring the demand for advanced forecasting models. Given the superiority of capsule network (CapsNet) over CNN in various forecasting and classification tasks, this study investigates the potential of integrating a 1D CapsNet with an LSTM network for multi-step stock index forecasting. To this end, a hybrid 1D-CapsNet-LSTM model is introduced, which utilizes a 1D CapsNet to generate high-level capsules from sequential data and a LSTM network to capture temporal dependencies. To maintain stochastic dependencies over different forecasting horizons, a multi-input multi-output (MIMO) strategy is employed. The model's performance is evaluated on real-world stock market indices, including S&amp;P 500, DJIA, IXIC, and NYSE, and compared to baseline models, including
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#22522;&#20110;&#35780;&#20998;&#30340;&#25968;&#25454;&#21516;&#21270;&#26041;&#27861;&#22312;&#39640;&#32500;&#24230;&#30340;&#22320;&#29699;&#29289;&#29702;&#21160;&#21147;&#31995;&#32479;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#36890;&#36807;&#22312;&#21452;&#23618;&#25311;&#22320;&#36716;&#21160;&#27169;&#22411;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#33391;&#22909;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.01853</link><description>&lt;p&gt;
&#22522;&#20110;&#35780;&#20998;&#30340;&#25968;&#25454;&#21516;&#21270;&#22312;&#21452;&#23618;&#25311;&#22320;&#36716;&#21160;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Score-based Data Assimilation for a Two-Layer Quasi-Geostrophic Model. (arXiv:2310.01853v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01853
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#22522;&#20110;&#35780;&#20998;&#30340;&#25968;&#25454;&#21516;&#21270;&#26041;&#27861;&#22312;&#39640;&#32500;&#24230;&#30340;&#22320;&#29699;&#29289;&#29702;&#21160;&#21147;&#31995;&#32479;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#36890;&#36807;&#22312;&#21452;&#23618;&#25311;&#22320;&#36716;&#21160;&#27169;&#22411;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#33391;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#21516;&#21270;&#35299;&#20915;&#20102;&#22312;&#32473;&#23450;&#22024;&#26434;&#25110;&#19981;&#23436;&#25972;&#35266;&#27979;&#24773;&#20917;&#19979;&#65292;&#30830;&#23450;&#21160;&#21147;&#31995;&#32479;&#21487;&#34892;&#29366;&#24577;&#36712;&#36857;&#30340;&#38382;&#39064;&#12290;&#22312;&#22320;&#29699;&#31185;&#23398;&#20013;&#65292;&#30001;&#20110;&#22320;&#29699;&#29289;&#29702;&#21160;&#21147;&#31995;&#32479;&#30340;&#39640;&#32500;&#24230;&#24615;&#65292;&#24448;&#24448;&#36229;&#36807;&#20102;&#25968;&#30334;&#19975;&#32500;&#24230;&#65292;&#22240;&#27492;&#23384;&#22312;&#25361;&#25112;&#12290;&#26412;&#25991;&#35780;&#20272;&#20102;&#22522;&#20110;&#35780;&#20998;&#30340;&#25968;&#25454;&#21516;&#21270;&#65288;SDA&#65289;&#36825;&#19968;&#26032;&#39062;&#30340;&#25968;&#25454;&#21516;&#21270;&#26041;&#27861;&#22312;&#27492;&#31867;&#31995;&#32479;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#35780;&#20998;&#32593;&#32476;&#26550;&#26500;&#30340;&#20462;&#25913;&#65292;&#26088;&#22312;&#26174;&#33879;&#20943;&#23569;&#20869;&#23384;&#28040;&#32791;&#21644;&#25191;&#34892;&#26102;&#38388;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#21452;&#23618;&#25311;&#22320;&#36716;&#21160;&#27169;&#22411;&#20013;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data assimilation addresses the problem of identifying plausible state trajectories of dynamical systems given noisy or incomplete observations. In geosciences, it presents challenges due to the high-dimensionality of geophysical dynamical systems, often exceeding millions of dimensions. This work assesses the scalability of score-based data assimilation (SDA), a novel data assimilation method, in the context of such systems. We propose modifications to the score network architecture aimed at significantly reducing memory consumption and execution time. We demonstrate promising results for a two-layer quasi-geostrophic model.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#35774;&#35745;&#21407;&#21017;&#29992;&#20110;&#20248;&#21270;&#39057;&#29575;&#24335;&#24207;&#36143;&#23398;&#20064;&#38382;&#39064;&#30340;&#36890;&#29992;&#29702;&#35770;&#65292;&#36890;&#36807;&#29983;&#25104;&#8220;&#31639;&#27861;&#20449;&#24565;&#8221;&#24182;&#20351;&#29992;&#36125;&#21494;&#26031;&#21518;&#39564;&#36827;&#34892;&#20915;&#31574;&#65292;&#23454;&#29616;&#20102;&#26080;&#20808;&#39564;&#30340;&#36125;&#21494;&#26031;&#31867;&#22411;&#31639;&#27861;&#22312;&#23545;&#25239;&#24615;&#29615;&#22659;&#20013;&#30340;&#27867;&#21270;&#21644;&#20248;&#21270;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.00806</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#35774;&#35745;&#21407;&#21017;&#29992;&#20110;&#39057;&#29575;&#24335;&#24207;&#36143;&#23398;&#20064;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Bayesian Design Principles for Frequentist Sequential Learning. (arXiv:2310.00806v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00806
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#35774;&#35745;&#21407;&#21017;&#29992;&#20110;&#20248;&#21270;&#39057;&#29575;&#24335;&#24207;&#36143;&#23398;&#20064;&#38382;&#39064;&#30340;&#36890;&#29992;&#29702;&#35770;&#65292;&#36890;&#36807;&#29983;&#25104;&#8220;&#31639;&#27861;&#20449;&#24565;&#8221;&#24182;&#20351;&#29992;&#36125;&#21494;&#26031;&#21518;&#39564;&#36827;&#34892;&#20915;&#31574;&#65292;&#23454;&#29616;&#20102;&#26080;&#20808;&#39564;&#30340;&#36125;&#21494;&#26031;&#31867;&#22411;&#31639;&#27861;&#22312;&#23545;&#25239;&#24615;&#29615;&#22659;&#20013;&#30340;&#27867;&#21270;&#21644;&#20248;&#21270;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#29702;&#35770;&#65292;&#29992;&#20110;&#20248;&#21270;&#39057;&#29575;&#35823;&#24046;&#27714;&#21644;&#30340;&#24207;&#36143;&#23398;&#20064;&#38382;&#39064;&#65292;&#21487;&#20197;&#36890;&#36807;&#32479;&#19968;&#30340;&#36125;&#21494;&#26031;&#21407;&#21017;&#24471;&#21040;&#39640;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#21644;&#36172;&#21338;&#26426;&#31639;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#22312;&#27599;&#19968;&#36718;&#29983;&#25104;&#8220;&#31639;&#27861;&#20449;&#24565;&#8221;&#65292;&#24182;&#20351;&#29992;&#36125;&#21494;&#26031;&#21518;&#39564;&#36827;&#34892;&#20915;&#31574;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#20248;&#21270;&#30446;&#26631;&#26159;&#21019;&#24314;&#8220;&#31639;&#27861;&#20449;&#24565;&#8221;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#8220;&#31639;&#27861;&#20449;&#24687;&#27604;&#8221;&#65292;&#26377;&#25928;&#22320;&#34920;&#24449;&#20102;&#20219;&#20309;&#31639;&#27861;&#30340;&#39057;&#29575;&#35823;&#24046;&#30340;&#20869;&#22312;&#22797;&#26434;&#24230;&#24230;&#37327;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#31181;&#23558;&#36125;&#21494;&#26031;&#24335;&#31639;&#27861;&#26080;&#20808;&#39564;&#22320;&#24182;&#19988;&#22312;&#23545;&#25239;&#24615;&#29615;&#22659;&#20013;&#20197;&#36890;&#29992;&#21644;&#26368;&#20248;&#26041;&#24335;&#24212;&#29992;&#30340;&#31995;&#32479;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#31639;&#27861;&#31616;&#21333;&#19988;&#36890;&#24120;&#23481;&#26131;&#23454;&#29616;&#12290;&#20316;&#20026;&#19968;&#20010;&#37325;&#35201;&#24212;&#29992;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#31639;&#27861;&#65292;&#22312;&#38543;&#26426;&#12289;&#23545;&#25239;&#21644;&#38750;...
&lt;/p&gt;
&lt;p&gt;
We develop a general theory to optimize the frequentist regret for sequential learning problems, where efficient bandit and reinforcement learning algorithms can be derived from unified Bayesian principles. We propose a novel optimization approach to generate "algorithmic beliefs" at each round, and use Bayesian posteriors to make decisions. The optimization objective to create "algorithmic beliefs," which we term "Algorithmic Information Ratio," represents an intrinsic complexity measure that effectively characterizes the frequentist regret of any algorithm. To the best of our knowledge, this is the first systematical approach to make Bayesian-type algorithms prior-free and applicable to adversarial settings, in a generic and optimal manner. Moreover, the algorithms are simple and often efficient to implement. As a major application, we present a novel algorithm for multi-armed bandits that achieves the "best-of-all-worlds" empirical performance in the stochastic, adversarial, and non
&lt;/p&gt;</description></item><item><title>MiliPoint&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#12289;&#24320;&#25918;&#30340;&#28857;&#20113;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25506;&#32034;&#22914;&#20309;&#21033;&#29992;&#27627;&#31859;&#27874;&#38647;&#36798;&#36827;&#34892;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#12290;&#19982;&#29616;&#26377;&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;&#23427;&#20855;&#26377;&#26356;&#22823;&#30340;&#35268;&#27169;&#12289;&#26356;&#22810;&#26679;&#21270;&#30340;&#20154;&#20307;&#27963;&#21160;&#34920;&#31034;&#65292;&#24182;&#28085;&#30422;&#20102;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#30340;&#25152;&#26377;&#20851;&#38190;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2309.13425</link><description>&lt;p&gt;
MiliPoint: &#19968;&#31181;&#27627;&#31859;&#27874;&#38647;&#36798;&#30340;&#28857;&#20113;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
MiliPoint: A Point Cloud Dataset for mmWave Radar. (arXiv:2309.13425v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13425
&lt;/p&gt;
&lt;p&gt;
MiliPoint&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#12289;&#24320;&#25918;&#30340;&#28857;&#20113;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25506;&#32034;&#22914;&#20309;&#21033;&#29992;&#27627;&#31859;&#27874;&#38647;&#36798;&#36827;&#34892;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#12290;&#19982;&#29616;&#26377;&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;&#23427;&#20855;&#26377;&#26356;&#22823;&#30340;&#35268;&#27169;&#12289;&#26356;&#22810;&#26679;&#21270;&#30340;&#20154;&#20307;&#27963;&#21160;&#34920;&#31034;&#65292;&#24182;&#28085;&#30422;&#20102;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#30340;&#25152;&#26377;&#20851;&#38190;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27627;&#31859;&#27874;&#38647;&#36798;&#20316;&#20026;&#20256;&#32479;&#22522;&#20110;&#25668;&#20687;&#26426;&#30340;&#31995;&#32479;&#30340;&#19968;&#31181;&#26377;&#21560;&#24341;&#21147;&#19988;&#32463;&#27982;&#23454;&#24800;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#24050;&#25104;&#20026;&#20154;&#20307;&#27963;&#21160;&#24863;&#30693;&#30340;&#19968;&#31181;&#29702;&#24819;&#36873;&#25321;&#12290;&#27627;&#31859;&#27874;&#38647;&#36798;&#26159;&#19968;&#31181;&#38750;&#20405;&#20837;&#24335;&#25216;&#26415;&#65292;&#25552;&#20379;&#26356;&#22909;&#30340;&#29992;&#25143;&#38544;&#31169;&#20445;&#25252;&#12290;&#28982;&#32780;&#65292;&#20316;&#20026;&#19968;&#31181;&#22522;&#20110;&#23556;&#39057;&#30340;&#25216;&#26415;&#65292;&#27627;&#31859;&#27874;&#38647;&#36798;&#20381;&#36182;&#20110;&#25429;&#33719;&#29289;&#20307;&#21453;&#23556;&#20449;&#21495;&#65292;&#22240;&#27492;&#30456;&#23545;&#20110;&#25668;&#20687;&#26426;&#26469;&#35828;&#26356;&#23481;&#26131;&#21463;&#21040;&#24178;&#25200;&#12290;&#36825;&#23545;&#20110;&#28145;&#24230;&#23398;&#20064;&#31038;&#21306;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#38382;&#39064;&#65306;&#25105;&#20204;&#33021;&#21542;&#24320;&#21457;&#26356;&#26377;&#25928;&#30340;&#22522;&#20110;&#28857;&#38598;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#22788;&#29702;&#36825;&#31181;&#21487;&#38752;&#30340;&#20256;&#24863;&#22120;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#8212;&#8212;MiliPoint&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#24320;&#25918;&#25968;&#25454;&#38598;&#20379;&#31038;&#21306;&#25506;&#32034;&#22914;&#20309;&#21033;&#29992;&#27627;&#31859;&#27874;&#38647;&#36798;&#36827;&#34892;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#12290;&#27492;&#22806;&#65292;MiliPoint&#22312;&#22823;&#23567;&#19978;&#36229;&#36807;&#29616;&#26377;&#30340;&#25968;&#25454;&#38598;&#65292;&#20195;&#34920;&#20102;&#26356;&#22810;&#26679;&#21270;&#30340;&#20154;&#31867;&#27963;&#21160;&#65292;&#24182;&#28085;&#30422;&#20102;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#30340;&#25152;&#26377;&#19977;&#20010;&#20851;&#38190;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Millimetre-wave (mmWave) radar has emerged as an attractive and cost-effective alternative for human activity sensing compared to traditional camera-based systems. mmWave radars are also non-intrusive, providing better protection for user privacy. However, as a Radio Frequency (RF) based technology, mmWave radars rely on capturing reflected signals from objects, making them more prone to noise compared to cameras. This raises an intriguing question for the deep learning community: Can we develop more effective point set-based deep learning methods for such attractive sensors?  To answer this question, our work, termed MiliPoint, delves into this idea by providing a large-scale, open dataset for the community to explore how mmWave radars can be utilised for human activity recognition. Moreover, MiliPoint stands out as it is larger in size than existing datasets, has more diverse human actions represented, and encompasses all three key tasks in human activity recognition. We have also es
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#20004;&#38454;&#27573;&#23376;&#27169;&#26368;&#22823;&#21270;&#38382;&#39064;&#65292;&#30446;&#26631;&#26159;&#20351;&#29992;&#23376;&#27169;&#35757;&#32451;&#20989;&#25968;&#26469;&#20943;&#23569;&#24213;&#23618;&#38598;&#21512;&#65292;&#24182;&#24341;&#20837;&#20102;&#38750;&#21333;&#35843;&#23376;&#27169;&#20989;&#25968;&#30340;&#31532;&#19968;&#20010;&#24658;&#23450;&#22240;&#23376;&#36924;&#36817;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.05183</link><description>&lt;p&gt;
&#36229;&#36234;&#21333;&#35843;&#24615;&#30340;&#25968;&#25454;&#27719;&#24635;&#65306;&#38750;&#21333;&#35843;&#30340;&#20004;&#38454;&#27573;&#23376;&#27169;&#26368;&#22823;&#21270;
&lt;/p&gt;
&lt;p&gt;
Data Summarization beyond Monotonicity: Non-monotone Two-Stage Submodular Maximization. (arXiv:2309.05183v2 [cs.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05183
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#20004;&#38454;&#27573;&#23376;&#27169;&#26368;&#22823;&#21270;&#38382;&#39064;&#65292;&#30446;&#26631;&#26159;&#20351;&#29992;&#23376;&#27169;&#35757;&#32451;&#20989;&#25968;&#26469;&#20943;&#23569;&#24213;&#23618;&#38598;&#21512;&#65292;&#24182;&#24341;&#20837;&#20102;&#38750;&#21333;&#35843;&#23376;&#27169;&#20989;&#25968;&#30340;&#31532;&#19968;&#20010;&#24658;&#23450;&#22240;&#23376;&#36924;&#36817;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20004;&#38454;&#27573;&#23376;&#27169;&#26368;&#22823;&#21270;&#38382;&#39064;&#30340;&#30446;&#26631;&#26159;&#20351;&#29992;&#25552;&#20379;&#30340;&#23376;&#27169;&#35757;&#32451;&#20989;&#25968;&#26469;&#20943;&#23569;&#24213;&#23618;&#38598;&#21512;&#65292;&#20197;&#30830;&#20445;&#22312;&#20943;&#23567;&#21518;&#30340;&#24213;&#23618;&#38598;&#21512;&#19978;&#20248;&#21270;&#26032;&#30340;&#30446;&#26631;&#20989;&#25968;&#30340;&#32467;&#26524;&#19982;&#22312;&#21407;&#22987;&#24213;&#23618;&#38598;&#21512;&#19978;&#33719;&#24471;&#30340;&#32467;&#26524;&#30456;&#24403;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#25968;&#25454;&#27719;&#24635;&#31561;&#21508;&#20010;&#39046;&#22495;&#37117;&#26377;&#24212;&#29992;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#36890;&#24120;&#20551;&#35774;&#30446;&#26631;&#20989;&#25968;&#26159;&#21333;&#35843;&#30340;&#65292;&#32780;&#25105;&#20204;&#30340;&#24037;&#20316;&#23558;&#36825;&#20010;&#30740;&#31350;&#25193;&#23637;&#21040;&#20102;&#38750;&#21333;&#35843;&#23376;&#27169;&#20989;&#25968;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#36825;&#31181;&#26356;&#19968;&#33324;&#24773;&#20917;&#30340;&#31532;&#19968;&#20010;&#24658;&#23450;&#22240;&#23376;&#36924;&#36817;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The objective of a two-stage submodular maximization problem is to reduce the ground set using provided training functions that are submodular, with the aim of ensuring that optimizing new objective functions over the reduced ground set yields results comparable to those obtained over the original ground set. This problem has applications in various domains including data summarization. Existing studies often assume the monotonicity of the objective function, whereas our work pioneers the extension of this research to accommodate non-monotone submodular functions. We have introduced the first constant-factor approximation algorithms for this more general case.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31185;&#23398;&#25968;&#25454;&#38169;&#35823;&#26377;&#30028;&#25439;&#22833;&#21387;&#32553;&#22120;SRN-SZ&#65292;&#29992;&#20110;&#25913;&#21892;&#38590;&#20197;&#21387;&#32553;&#30340;&#25968;&#25454;&#38598;&#30340;&#21387;&#32553;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2309.04037</link><description>&lt;p&gt;
SRN-SZ: &#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31185;&#23398;&#25968;&#25454;&#38169;&#35823;&#26377;&#30028;&#25439;&#22833;&#21387;&#32553;&#19982;&#36229;&#20998;&#36776;&#29575;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
SRN-SZ: Deep Leaning-Based Scientific Error-bounded Lossy Compression with Super-resolution Neural Networks. (arXiv:2309.04037v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04037
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31185;&#23398;&#25968;&#25454;&#38169;&#35823;&#26377;&#30028;&#25439;&#22833;&#21387;&#32553;&#22120;SRN-SZ&#65292;&#29992;&#20110;&#25913;&#21892;&#38590;&#20197;&#21387;&#32553;&#30340;&#25968;&#25454;&#38598;&#30340;&#21387;&#32553;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#36229;&#32423;&#35745;&#31639;&#31995;&#32479;&#30340;&#35745;&#31639;&#33021;&#21147;&#21644;&#35268;&#27169;&#30340;&#24555;&#36895;&#22686;&#38271;&#32473;&#36229;&#32423;&#35745;&#31639;&#31995;&#32479;&#30340;&#31649;&#29702;&#24102;&#26469;&#20102;&#24040;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#20445;&#25345;&#31185;&#23398;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#65292;&#25552;&#20986;&#24182;&#21457;&#23637;&#20102;&#38169;&#35823;&#26377;&#30028;&#25439;&#22833;&#21387;&#32553;&#20316;&#20026;&#31185;&#23398;&#25968;&#25454;&#23610;&#23544;&#32553;&#20943;&#30340;&#37325;&#35201;&#25216;&#26415;&#65292;&#20197;&#38480;&#21046;&#25968;&#25454;&#22833;&#30495;&#12290;&#22312;&#21508;&#31181;&#31185;&#23398;&#27169;&#25311;&#29983;&#25104;&#30340;&#22810;&#26679;&#25968;&#25454;&#38598;&#20013;&#65292;&#26576;&#20123;&#25968;&#25454;&#38598;&#26080;&#27861;&#36890;&#36807;&#29616;&#26377;&#30340;&#20256;&#32479;&#25216;&#26415;&#30340;&#38169;&#35823;&#26377;&#30028;&#25439;&#22833;&#21387;&#32553;&#22120;&#26377;&#25928;&#21387;&#32553;&#12290;&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#36817;&#25104;&#21151;&#21551;&#21457;&#20102;&#22810;&#20301;&#30740;&#31350;&#20154;&#21592;&#23558;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#21040;&#38169;&#35823;&#26377;&#30028;&#25439;&#22833;&#21387;&#32553;&#22120;&#20013;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24037;&#20316;&#20173;&#28982;&#38754;&#20020;&#30528;&#26377;&#38480;&#30340;&#21387;&#32553;&#27604;&#21644;/&#25110;&#26497;&#20302;&#30340;&#25928;&#29575;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#24182;&#25913;&#21892;&#38590;&#20197;&#21387;&#32553;&#30340;&#25968;&#25454;&#38598;&#30340;&#21387;&#32553;&#24773;&#20917;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;SRN-SZ&#65292;&#23427;&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31185;&#23398;&#25968;&#25454;&#38169;&#35823;&#26377;&#30028;&#25439;&#22833;&#21387;&#32553;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
The fast growth of computational power and scales of modern super-computing systems have raised great challenges for the management of exascale scientific data. To maintain the usability of scientific data, error-bound lossy compression is proposed and developed as an essential technique for the size reduction of scientific data with constrained data distortion. Among the diverse datasets generated by various scientific simulations, certain datasets cannot be effectively compressed by existing error-bounded lossy compressors with traditional techniques. The recent success of Artificial Intelligence has inspired several researchers to integrate neural networks into error-bounded lossy compressors. However, those works still suffer from limited compression ratios and/or extremely low efficiencies. To address those issues and improve the compression on the hard-to-compress datasets, in this paper, we propose SRN-SZ, which is a deep learning-based scientific error-bounded lossy compressor 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#22270;&#36793;&#32536;&#30340;&#27969;&#37327;&#25968;&#25454;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31232;&#30095;&#32454;&#32990;&#22797;&#21512;&#20307;&#26469;&#34920;&#31034;&#36793;&#27969;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#22270;&#32467;&#26500;&#36716;&#21270;&#20026;&#19968;&#20010;&#21333;&#32431;&#22797;&#21512;&#20307;&#65292;&#21033;&#29992;Hodge-Laplacian&#30340;&#29305;&#24449;&#21521;&#37327;&#21644;&#20851;&#32852;&#30697;&#38453;&#36827;&#34892;Hodge&#20998;&#35299;&#65292;&#24471;&#21040;&#26799;&#24230;&#12289;&#26059;&#37327;&#21644;&#35856;&#27874;&#27969;&#30340;&#34920;&#31034;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32454;&#32990;&#25512;&#26029;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#28155;&#21152;&#32454;&#32990;&#26469;&#22686;&#24378;&#35266;&#27979;&#21040;&#30340;&#22270;&#65292;&#20351;&#34920;&#31034;&#31232;&#30095;&#21487;&#35299;&#37322;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#20010;&#38382;&#39064;&#26159;NP&#38590;&#30340;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#36817;&#20284;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.01632</link><description>&lt;p&gt;
&#36890;&#36807;&#31232;&#30095;&#32454;&#32990;&#22797;&#21512;&#20307;&#22312;&#22270;&#19978;&#34920;&#31034;&#36793;&#27969;
&lt;/p&gt;
&lt;p&gt;
Representing Edge Flows on Graphs via Sparse Cell Complexes. (arXiv:2309.01632v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01632
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#22270;&#36793;&#32536;&#30340;&#27969;&#37327;&#25968;&#25454;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31232;&#30095;&#32454;&#32990;&#22797;&#21512;&#20307;&#26469;&#34920;&#31034;&#36793;&#27969;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#22270;&#32467;&#26500;&#36716;&#21270;&#20026;&#19968;&#20010;&#21333;&#32431;&#22797;&#21512;&#20307;&#65292;&#21033;&#29992;Hodge-Laplacian&#30340;&#29305;&#24449;&#21521;&#37327;&#21644;&#20851;&#32852;&#30697;&#38453;&#36827;&#34892;Hodge&#20998;&#35299;&#65292;&#24471;&#21040;&#26799;&#24230;&#12289;&#26059;&#37327;&#21644;&#35856;&#27874;&#27969;&#30340;&#34920;&#31034;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32454;&#32990;&#25512;&#26029;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#28155;&#21152;&#32454;&#32990;&#26469;&#22686;&#24378;&#35266;&#27979;&#21040;&#30340;&#22270;&#65292;&#20351;&#34920;&#31034;&#31232;&#30095;&#21487;&#35299;&#37322;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#20010;&#38382;&#39064;&#26159;NP&#38590;&#30340;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#36817;&#20284;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#21644;&#20449;&#21495;&#22788;&#29702;&#20219;&#21153;&#20013;&#65292;&#33719;&#21462;&#31232;&#30095;&#21487;&#35299;&#37322;&#30340;&#21487;&#35266;&#27979;&#25968;&#25454;&#34920;&#31034;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#23545;&#20110;&#34920;&#31034;&#27839;&#22270;&#36793;&#32536;&#30340;&#27969;&#21160;&#30340;&#25968;&#25454;&#65292;&#19968;&#31181;&#30452;&#35266;&#21487;&#35299;&#37322;&#30340;&#33719;&#21462;&#34920;&#31034;&#30340;&#26041;&#27861;&#26159;&#23558;&#22270;&#32467;&#26500;&#25552;&#21319;&#21040;&#19968;&#20010;&#21333;&#32431;&#22797;&#21512;&#20307;&#65306;&#30456;&#20851;Hodge-Laplacian&#30340;&#29305;&#24449;&#21521;&#37327;&#65292;&#20197;&#21450;&#30456;&#24212;&#21333;&#32431;&#22797;&#21512;&#20307;&#30340;&#20851;&#32852;&#30697;&#38453;&#65292;&#21487;&#24341;&#23548;&#20986;Hodge&#20998;&#35299;&#65292;&#29992;&#20110;&#20197;&#26799;&#24230;&#65292;&#26059;&#37327;&#21644;&#35856;&#27874;&#27969;&#30340;&#24418;&#24335;&#34920;&#31034;&#35266;&#27979;&#21040;&#30340;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#25512;&#24191;&#21040;&#32454;&#32990;&#22797;&#21512;&#20307;&#65292;&#24182;&#24341;&#20837;&#32454;&#32990;&#25512;&#26029;&#20248;&#21270;&#38382;&#39064;&#65292;&#21363;&#36890;&#36807;&#28155;&#21152;&#19968;&#32452;&#32454;&#32990;&#26469;&#22686;&#24378;&#35266;&#27979;&#21040;&#30340;&#22270;&#65292;&#20351;&#24471;&#20851;&#32852;Hodge Laplacian&#30340;&#29305;&#24449;&#21521;&#37327;&#33021;&#22815;&#25552;&#20379;&#23545;&#22270;&#19978;&#35266;&#27979;&#21040;&#30340;&#36793;&#32536;&#27969;&#30340;&#31232;&#30095;&#21487;&#35299;&#37322;&#34920;&#31034;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20010;&#38382;&#39064;&#26159;NP&#38590;&#30340;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#36817;&#20284;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Obtaining sparse, interpretable representations of observable data is crucial in many machine learning and signal processing tasks. For data representing flows along the edges of a graph, an intuitively interpretable way to obtain such representations is to lift the graph structure to a simplicial complex: The eigenvectors of the associated Hodge-Laplacian, respectively the incidence matrices of the corresponding simplicial complex then induce a Hodge decomposition, which can be used to represent the observed data in terms of gradient, curl, and harmonic flows. In this paper, we generalize this approach to cellular complexes and introduce the cell inference optimization problem, i.e., the problem of augmenting the observed graph by a set of cells, such that the eigenvectors of the associated Hodge Laplacian provide a sparse, interpretable representation of the observed edge flows on the graph. We show that this problem is NP-hard and introduce an efficient approximation algorithm for i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38381;&#29615;&#23433;&#20840;&#23398;&#20064;&#30340;&#33539;&#24335;&#65292;&#29992;&#20110;&#21512;&#25104;&#26426;&#22120;&#20154;&#30340;&#23433;&#20840;&#25511;&#21046;&#31574;&#30053;&#12290;&#35813;&#26041;&#27861;&#32771;&#34385;&#26426;&#22120;&#20154;&#23398;&#20064;&#33021;&#21147;&#21644;&#19981;&#30830;&#23450;&#24615;&#65292;&#33021;&#22815;&#24555;&#36895;&#24212;&#23545;&#26410;&#26469;&#22330;&#26223;&#65292;&#20174;&#32780;&#22312;&#30830;&#20445;&#23433;&#20840;&#30340;&#21516;&#26102;&#20445;&#25345;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2309.01267</link><description>&lt;p&gt;
&#20266;&#35013;&#28216;&#25103;&#65306;&#22312;&#20132;&#20114;&#24335;&#26426;&#22120;&#20154;&#33258;&#20027;&#24615;&#20013;&#38381;&#29615;&#23433;&#20840;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deception Game: Closing the Safety-Learning Loop in Interactive Robot Autonomy. (arXiv:2309.01267v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01267
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38381;&#29615;&#23433;&#20840;&#23398;&#20064;&#30340;&#33539;&#24335;&#65292;&#29992;&#20110;&#21512;&#25104;&#26426;&#22120;&#20154;&#30340;&#23433;&#20840;&#25511;&#21046;&#31574;&#30053;&#12290;&#35813;&#26041;&#27861;&#32771;&#34385;&#26426;&#22120;&#20154;&#23398;&#20064;&#33021;&#21147;&#21644;&#19981;&#30830;&#23450;&#24615;&#65292;&#33021;&#22815;&#24555;&#36895;&#24212;&#23545;&#26410;&#26469;&#22330;&#26223;&#65292;&#20174;&#32780;&#22312;&#30830;&#20445;&#23433;&#20840;&#30340;&#21516;&#26102;&#20445;&#25345;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#36710;&#36742;&#31561;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#24191;&#27867;&#37096;&#32626;&#38754;&#20020;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#26159;&#30830;&#20445;&#19982;&#20154;&#31867;&#23433;&#20840;&#20114;&#21160;&#32780;&#19981;&#29306;&#29298;&#24615;&#33021;&#12290;&#29616;&#26377;&#30340;&#23433;&#20840;&#26041;&#27861;&#24448;&#24448;&#24573;&#35270;&#20102;&#26426;&#22120;&#20154;&#22312;&#36816;&#34892;&#26102;&#23398;&#20064;&#21644;&#36866;&#24212;&#30340;&#33021;&#21147;&#65292;&#23548;&#33268;&#36807;&#24230;&#20445;&#23432;&#30340;&#34892;&#20026;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38381;&#29615;&#33539;&#24335;&#65292;&#29992;&#20110;&#21512;&#25104;&#23433;&#20840;&#25511;&#21046;&#31574;&#30053;&#65292;&#26126;&#30830;&#32771;&#34385;&#26426;&#22120;&#20154;&#19981;&#26029;&#21464;&#21270;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#20854;&#24555;&#36895;&#24212;&#23545;&#26410;&#26469;&#22330;&#26223;&#30340;&#33021;&#21147;&#65292;&#24182;&#32852;&#21512;&#32771;&#34385;&#29289;&#29702;&#21160;&#21147;&#23398;&#21644;&#26426;&#22120;&#20154;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#23545;&#25239;&#24615;&#24378;&#21270;&#23398;&#20064;&#65292;&#20197;&#21487;&#34892;&#30340;&#26041;&#24335;&#36827;&#34892;&#39640;&#32500;&#23398;&#20064;&#21160;&#21147;&#23398;&#30340;&#23433;&#20840;&#20998;&#26512;&#65292;&#24182;&#36890;&#36807;&#36125;&#21494;&#26031;&#20449;&#24565;&#20256;&#25773;&#21644;&#22823;&#22411;&#39044;&#35757;&#32451;&#31070;&#32463;&#36712;&#36857;&#39044;&#27979;&#22120;&#23637;&#31034;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
An outstanding challenge for the widespread deployment of robotic systems like autonomous vehicles is ensuring safe interaction with humans without sacrificing performance. Existing safety methods often neglect the robot's ability to learn and adapt at runtime, leading to overly conservative behavior. This paper proposes a new closed-loop paradigm for synthesizing safe control policies that explicitly account for the robot's evolving uncertainty and its ability to quickly respond to future scenarios as they arise, by jointly considering the physical dynamics and the robot's learning algorithm. We leverage adversarial reinforcement learning for tractable safety analysis under high-dimensional learning dynamics and demonstrate our framework's ability to work with both Bayesian belief propagation and implicit learning through large pre-trained neural trajectory predictors.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TExplain&#30340;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#39044;&#35757;&#32451;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#29305;&#24449;&#31354;&#38388;&#36830;&#25509;&#36215;&#26469;&#65292;&#36890;&#36807;&#29983;&#25104;&#35299;&#37322;&#24615;&#21477;&#23376;&#26469;&#29702;&#35299;&#20998;&#31867;&#22120;&#23398;&#20064;&#21040;&#30340;&#29305;&#24449;&#12290;&#35813;&#26041;&#27861;&#39318;&#27425;&#21033;&#29992;&#36825;&#20123;&#39057;&#32321;&#21333;&#35789;&#25581;&#31034;&#20986;&#20998;&#31867;&#22120;&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#26816;&#27979;&#34394;&#20551;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.00733</link><description>&lt;p&gt;
&#23398;&#20064;&#30340;&#35270;&#35273;&#29305;&#24449;&#21040;&#25991;&#26412;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Learned Visual Features to Textual Explanations. (arXiv:2309.00733v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00733
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TExplain&#30340;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#39044;&#35757;&#32451;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#29305;&#24449;&#31354;&#38388;&#36830;&#25509;&#36215;&#26469;&#65292;&#36890;&#36807;&#29983;&#25104;&#35299;&#37322;&#24615;&#21477;&#23376;&#26469;&#29702;&#35299;&#20998;&#31867;&#22120;&#23398;&#20064;&#21040;&#30340;&#29305;&#24449;&#12290;&#35813;&#26041;&#27861;&#39318;&#27425;&#21033;&#29992;&#36825;&#20123;&#39057;&#32321;&#21333;&#35789;&#25581;&#31034;&#20986;&#20998;&#31867;&#22120;&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#26816;&#27979;&#34394;&#20551;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#35299;&#37322;&#35270;&#35273;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#29305;&#24449;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#26469;&#35299;&#37322;&#39044;&#35757;&#32451;&#22270;&#20687;&#20998;&#31867;&#22120;&#23398;&#20064;&#21040;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21517;&#20026;TExplain&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#29305;&#24449;&#31354;&#38388;&#21644;LLMs&#20043;&#38388;&#24314;&#31435;&#36830;&#25509;&#26469;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#12290;&#28982;&#21518;&#65292;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#29983;&#25104;&#22823;&#37327;&#30340;&#21477;&#23376;&#26469;&#35299;&#37322;&#20998;&#31867;&#22120;&#23545;&#32473;&#23450;&#22270;&#20687;&#23398;&#20064;&#21040;&#30340;&#29305;&#24449;&#12290;&#36825;&#20123;&#21477;&#23376;&#28982;&#21518;&#29992;&#20110;&#25552;&#21462;&#26368;&#39057;&#32321;&#30340;&#21333;&#35789;&#65292;&#20174;&#32780;&#20840;&#38754;&#29702;&#35299;&#20998;&#31867;&#22120;&#20013;&#23398;&#20064;&#21040;&#30340;&#29305;&#24449;&#21644;&#27169;&#24335;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#27425;&#21033;&#29992;&#19982;&#35270;&#35273;&#34920;&#31034;&#23545;&#24212;&#30340;&#36825;&#20123;&#39057;&#32321;&#21333;&#35789;&#26469;&#25581;&#31034;&#29420;&#31435;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#20174;&#32780;&#23454;&#29616;&#26816;&#27979;&#34394;&#20551;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpreting the learned features of vision models has posed a longstanding challenge in the field of machine learning. To address this issue, we propose a novel method that leverages the capabilities of large language models (LLMs) to interpret the learned features of pre-trained image classifiers. Our method, called TExplain, tackles this task by training a neural network to establish a connection between the feature space of image classifiers and LLMs. Then, during inference, our approach generates a vast number of sentences to explain the features learned by the classifier for a given image. These sentences are then used to extract the most frequent words, providing a comprehensive understanding of the learned features and patterns within the classifier. Our method, for the first time, utilizes these frequent words corresponding to a visual representation to provide insights into the decision-making process of the independently trained classifier, enabling the detection of spurious
&lt;/p&gt;</description></item><item><title>ILCAS&#26159;&#39318;&#20010;&#22522;&#20110;&#27169;&#20223;&#23398;&#20064;&#30340;&#36866;&#24212;&#24615;&#37197;&#32622;&#27969;&#24335;&#20256;&#36755;&#31995;&#32479;&#65292;&#29992;&#20110;&#23454;&#26102;&#35270;&#39057;&#20998;&#26512;&#65292;&#24182;&#22312;&#20256;&#36755;&#36807;&#31243;&#20013;&#23454;&#29616;&#30456;&#26426;&#20043;&#38388;&#30340;&#21327;&#20316;&#12290;&#19982;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;&#19981;&#21516;&#65292;ILCAS&#36890;&#36807;&#27169;&#20223;&#23398;&#20064;&#35757;&#32451;&#20195;&#29702;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#34920;&#29616;&#21644;&#24212;&#29992;&#30446;&#26631;&#30340;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.10068</link><description>&lt;p&gt;
ILCAS: &#22522;&#20110;&#27169;&#20223;&#23398;&#20064;&#30340;&#36866;&#24212;&#24615;&#37197;&#32622;&#27969;&#24335;&#20256;&#36755;&#22312;&#24102;&#26377;&#36328;&#30456;&#26426;&#21327;&#20316;&#30340;&#23454;&#26102;&#35270;&#39057;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
ILCAS: Imitation Learning-Based Configuration-Adaptive Streaming for Live Video Analytics with Cross-Camera Collaboration. (arXiv:2308.10068v2 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10068
&lt;/p&gt;
&lt;p&gt;
ILCAS&#26159;&#39318;&#20010;&#22522;&#20110;&#27169;&#20223;&#23398;&#20064;&#30340;&#36866;&#24212;&#24615;&#37197;&#32622;&#27969;&#24335;&#20256;&#36755;&#31995;&#32479;&#65292;&#29992;&#20110;&#23454;&#26102;&#35270;&#39057;&#20998;&#26512;&#65292;&#24182;&#22312;&#20256;&#36755;&#36807;&#31243;&#20013;&#23454;&#29616;&#30456;&#26426;&#20043;&#38388;&#30340;&#21327;&#20316;&#12290;&#19982;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;&#19981;&#21516;&#65292;ILCAS&#36890;&#36807;&#27169;&#20223;&#23398;&#20064;&#35757;&#32451;&#20195;&#29702;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#34920;&#29616;&#21644;&#24212;&#29992;&#30446;&#26631;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#31934;&#24230;&#21644;&#36164;&#28304;&#23494;&#38598;&#22411;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#24050;&#32463;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#23454;&#26102;&#35270;&#39057;&#20998;&#26512;&#65288;VA&#65289;&#65292;&#20854;&#20013;&#30456;&#26426;&#35270;&#39057;&#36890;&#36807;&#32593;&#32476;&#27969;&#24335;&#20256;&#36755;&#21040;&#36164;&#28304;&#20016;&#23500;&#30340;&#36793;&#32536;/&#20113;&#26381;&#21153;&#22120;&#36827;&#34892;DNN&#25512;&#29702;&#12290;&#24120;&#35265;&#30340;&#35270;&#39057;&#32534;&#30721;&#37197;&#32622;&#65288;&#20363;&#22914;&#20998;&#36776;&#29575;&#21644;&#24103;&#29575;&#65289;&#24050;&#34987;&#30830;&#23450;&#20026;&#22312;&#24102;&#23485;&#28040;&#32791;&#21644;&#25512;&#29702;&#20934;&#30830;&#24230;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#30340;&#37325;&#35201;&#22240;&#32032;&#65292;&#22240;&#27492;&#23427;&#20204;&#30340;&#35843;&#25972;&#26041;&#26696;&#19968;&#30452;&#26159;&#20248;&#21270;&#30340;&#28966;&#28857;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#22522;&#20110;&#24615;&#33021;&#20998;&#26512;&#30340;&#35299;&#20915;&#26041;&#26696;&#23384;&#22312;&#39640;&#26114;&#30340;&#24615;&#33021;&#20998;&#26512;&#25104;&#26412;&#65292;&#32780;&#29616;&#26377;&#30340;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#30340;&#35299;&#20915;&#26041;&#26696;&#21487;&#33021;&#30001;&#20110;&#20351;&#29992;&#22266;&#23450;&#22870;&#21169;&#20989;&#25968;&#35757;&#32451;&#20195;&#29702;&#32780;&#24615;&#33021;&#19981;&#20339;&#65292;&#36825;&#26080;&#27861;&#22312;&#21508;&#31181;&#22330;&#26223;&#19979;&#21046;&#23450;&#24212;&#29992;&#30446;&#26631;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#27169;&#20223;&#23398;&#20064;&#30340;ILCAS&#36866;&#24212;&#24615;&#37197;&#32622;&#27969;&#24335;&#20256;&#36755;&#31995;&#32479;&#12290;&#19982;&#22522;&#20110;DRL&#30340;&#35299;&#20915;&#26041;&#26696;&#19981;&#21516;&#65292;ILCAS&#36890;&#36807;&#20174;&#36328;&#30456;&#26426;&#21327;&#20316;&#20013;&#25910;&#38598;&#30340;&#28436;&#31034;&#25968;&#25454;&#26469;&#35757;&#32451;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
The high-accuracy and resource-intensive deep neural networks (DNNs) have been widely adopted by live video analytics (VA), where camera videos are streamed over the network to resource-rich edge/cloud servers for DNN inference. Common video encoding configurations (e.g., resolution and frame rate) have been identified with significant impacts on striking the balance between bandwidth consumption and inference accuracy and therefore their adaption scheme has been a focus of optimization. However, previous profiling-based solutions suffer from high profiling cost, while existing deep reinforcement learning (DRL) based solutions may achieve poor performance due to the usage of fixed reward function for training the agent, which fails to craft the application goals in various scenarios. In this paper, we propose ILCAS, the first imitation learning (IL) based configuration-adaptive VA streaming system. Unlike DRL-based solutions, ILCAS trains the agent with demonstrations collected from th
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#31216;&#20026;&#20108;&#20803;&#24378;&#21270;&#23398;&#20064;&#30340;&#22312;&#32447;&#31639;&#27861;&#65292;&#29992;&#20110;&#26681;&#25454;&#19978;&#19979;&#25991;&#22240;&#32032;&#21644;&#30446;&#26631;&#20154;&#19982;&#20854;&#29031;&#39038;&#20276;&#20387;&#30340;&#36807;&#21435;&#21453;&#39304;&#65292;&#20010;&#24615;&#21270;&#22320;&#25552;&#20379;&#24178;&#39044;&#25514;&#26045;&#12290;&#35813;&#31639;&#27861;&#26159;&#36125;&#21494;&#26031;&#21644;&#23618;&#27425;&#30340;&#65292;&#24182;&#36890;&#36807;&#27169;&#25311;&#23637;&#31034;&#20102;&#33391;&#22909;&#30340;&#23454;&#35777;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.07843</link><description>&lt;p&gt;
Dyadic Reinforcement Learning. (arXiv:2308.07843v1 [cs.LG]) &#35813;&#35770;&#25991;&#26631;&#39064;&#24050;&#32763;&#35793;&#65306;&#20108;&#20803;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dyadic Reinforcement Learning. (arXiv:2308.07843v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07843
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#31216;&#20026;&#20108;&#20803;&#24378;&#21270;&#23398;&#20064;&#30340;&#22312;&#32447;&#31639;&#27861;&#65292;&#29992;&#20110;&#26681;&#25454;&#19978;&#19979;&#25991;&#22240;&#32032;&#21644;&#30446;&#26631;&#20154;&#19982;&#20854;&#29031;&#39038;&#20276;&#20387;&#30340;&#36807;&#21435;&#21453;&#39304;&#65292;&#20010;&#24615;&#21270;&#22320;&#25552;&#20379;&#24178;&#39044;&#25514;&#26045;&#12290;&#35813;&#31639;&#27861;&#26159;&#36125;&#21494;&#26031;&#21644;&#23618;&#27425;&#30340;&#65292;&#24182;&#36890;&#36807;&#27169;&#25311;&#23637;&#31034;&#20102;&#33391;&#22909;&#30340;&#23454;&#35777;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#21307;&#30103;&#26088;&#22312;&#36890;&#36807;&#22312;&#20010;&#20154;&#26085;&#24120;&#29983;&#27963;&#20013;&#25552;&#20379;&#24178;&#39044;&#26469;&#25552;&#39640;&#20581;&#24247;&#32467;&#26524;&#12290;&#29031;&#39038;&#20276;&#20387;&#21644;&#31038;&#20250;&#25903;&#25345;&#32593;&#32476;&#30340;&#21442;&#19982;&#32463;&#24120;&#22312;&#24110;&#21161;&#20010;&#20154;&#31649;&#29702;&#32321;&#37325;&#30340;&#21307;&#30103;&#26465;&#20214;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#36825;&#20026;&#31227;&#21160;&#21307;&#30103;&#25552;&#20379;&#20102;&#26426;&#20250;&#65292;&#35774;&#35745;&#38024;&#23545;&#20108;&#20803;&#20851;&#31995;&#8212;&#8212;&#30446;&#26631;&#20154;&#21644;&#20854;&#29031;&#39038;&#20276;&#20387;&#20043;&#38388;&#20851;&#31995;&#8212;&#8212;&#20197;&#25552;&#39640;&#31038;&#20250;&#25903;&#25345;&#30340;&#24178;&#39044;&#25514;&#26045;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20108;&#20803;&#24378;&#21270;&#23398;&#20064;&#65288;Dyadic RL&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#29615;&#22659;&#22240;&#32032;&#21644;&#30446;&#26631;&#20154;&#21450;&#20854;&#29031;&#39038;&#20276;&#20387;&#30340;&#36807;&#21435;&#21453;&#39304;&#20010;&#24615;&#21270;&#24178;&#39044;&#25514;&#26045;&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#22312;&#36825;&#37324;&#65292;&#22810;&#32452;&#24178;&#39044;&#25514;&#26045;&#24433;&#21709;&#30528;&#20108;&#20803;&#20851;&#31995;&#22312;&#22810;&#20010;&#26102;&#38388;&#38388;&#38548;&#20869;&#12290;&#24320;&#21457;&#30340;&#20108;&#20803;&#24378;&#21270;&#23398;&#20064;&#26159;&#36125;&#21494;&#26031;&#21644;&#23618;&#27425;&#30340;&#12290;&#25105;&#20204;&#27491;&#24335;&#20171;&#32461;&#20102;&#38382;&#39064;&#35774;&#23450;&#65292;&#24320;&#21457;&#20102;&#20108;&#20803;&#24378;&#21270;&#23398;&#20064;&#24182;&#30830;&#23450;&#20102;&#36951;&#25022;&#36793;&#30028;&#12290;&#36890;&#36807;&#27169;&#25311;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20108;&#20803;&#24378;&#21270;&#23398;&#20064;&#30340;&#23454;&#35777;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mobile health aims to enhance health outcomes by delivering interventions to individuals as they go about their daily life. The involvement of care partners and social support networks often proves crucial in helping individuals managing burdensome medical conditions. This presents opportunities in mobile health to design interventions that target the dyadic relationship -- the relationship between a target person and their care partner -- with the aim of enhancing social support. In this paper, we develop dyadic RL, an online reinforcement learning algorithm designed to personalize intervention delivery based on contextual factors and past responses of a target person and their care partner. Here, multiple sets of interventions impact the dyad across multiple time intervals. The developed dyadic RL is Bayesian and hierarchical. We formally introduce the problem setup, develop dyadic RL and establish a regret bound. We demonstrate dyadic RL's empirical performance through simulation st
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#20351;&#29992;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#65292;&#27604;&#36739;&#20102;&#20849;&#21516;&#23545;&#27604;&#23398;&#20064;&#21644;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#22312;CT&#25195;&#25551;&#21367;&#31215;&#27169;&#22411;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.06534</link><description>&lt;p&gt;
&#35299;&#20915;&#21307;&#23398;&#24433;&#20687;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#23567;&#22411;&#27880;&#37322;&#25968;&#25454;&#38598;&#38382;&#39064;&#65306;&#23545;&#27604;&#20849;&#21516;&#23545;&#27604;&#23398;&#20064;&#21644;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#22312;CT&#25195;&#25551;&#21367;&#31215;&#27169;&#22411;&#20013;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Dealing with Small Annotated Datasets for Deep Learning in Medical Imaging: An Evaluation of Self-Supervised Pre-Training on CT Scans Comparing Contrastive and Masked Autoencoder Methods for Convolutional Models. (arXiv:2308.06534v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#20351;&#29992;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#65292;&#27604;&#36739;&#20102;&#20849;&#21516;&#23545;&#27604;&#23398;&#20064;&#21644;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#22312;CT&#25195;&#25551;&#21367;&#31215;&#27169;&#22411;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#28145;&#24230;&#23398;&#20064;&#26377;&#28508;&#21147;&#20943;&#23569;&#35786;&#26029;&#38169;&#35823;&#30340;&#39118;&#38505;&#12289;&#20943;&#36731;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#24037;&#20316;&#36127;&#25285;&#24182;&#21152;&#36895;&#30830;&#35786;&#12290;&#35757;&#32451;&#36825;&#26679;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#22823;&#22411;&#19988;&#20934;&#30830;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#38656;&#35201;&#20026;&#25152;&#26377;&#35757;&#32451;&#26679;&#26412;&#25552;&#20379;&#27880;&#37322;&#12290;&#28982;&#32780;&#65292;&#22312;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#65292;&#30001;&#20110;&#27880;&#37322;&#30340;&#39640;&#22797;&#26434;&#24615;&#12289;&#21463;&#38480;&#30340;&#33719;&#21462;&#26041;&#24335;&#25110;&#30142;&#30149;&#30340;&#32597;&#35265;&#24615;&#65292;&#29305;&#23450;&#20219;&#21153;&#30340;&#27880;&#37322;&#25968;&#25454;&#38598;&#36890;&#24120;&#24456;&#23567;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#39046;&#22495;&#30340;&#26041;&#27861;&#65292;&#22312;&#27809;&#26377;&#27880;&#37322;&#30340;&#22823;&#22411;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#22312;&#39044;&#35757;&#32451;&#20043;&#21518;&#65292;&#23567;&#22411;&#30340;&#24050;&#27880;&#37322;&#25968;&#25454;&#38598;&#23601;&#36275;&#20197;&#23545;&#27169;&#22411;&#36827;&#34892;&#29305;&#23450;&#20219;&#21153;&#30340;&#24494;&#35843;&#65292;&#21363;&#25152;&#35859;&#30340;&#8220;&#19979;&#28216;&#20219;&#21153;&#8221;&#12290;&#21307;&#23398;&#24433;&#20687;&#20013;&#26368;&#27969;&#34892;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#22522;&#20110;&#20849;&#21516;&#23545;&#27604;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#33258;&#28982;&#22270;&#20687;&#22788;&#29702;&#30740;&#31350;&#34920;&#26126;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#20855;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20108;&#32773;&#22312;CT&#25195;&#25551;&#21367;&#31215;&#27169;&#22411;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning in medical imaging has the potential to minimize the risk of diagnostic errors, reduce radiologist workload, and accelerate diagnosis. Training such deep learning models requires large and accurate datasets, with annotations for all training samples. However, in the medical imaging domain, annotated datasets for specific tasks are often small due to the high complexity of annotations, limited access, or the rarity of diseases. To address this challenge, deep learning models can be pre-trained on large image datasets without annotations using methods from the field of self-supervised learning. After pre-training, small annotated datasets are sufficient to fine-tune the models for a specific task, the so-called ``downstream task". The most popular self-supervised pre-training approaches in medical imaging are based on contrastive learning. However, recent studies in natural image processing indicate a strong potential for masked autoencoder approaches. Our work compares sta
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#35889;&#30340;&#21487;&#35299;&#37322;&#24180;&#40836;&#39044;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#20840;&#36523;&#22270;&#20687;&#30740;&#31350;&#20102;&#21508;&#20010;&#36523;&#20307;&#37096;&#20301;&#30340;&#24180;&#40836;&#30456;&#20851;&#21464;&#21270;&#12290;&#36890;&#36807;&#20351;&#29992;&#35299;&#37322;&#24615;&#26041;&#27861;&#21644;&#37197;&#20934;&#25216;&#26415;&#65292;&#30830;&#23450;&#20102;&#26368;&#33021;&#39044;&#27979;&#24180;&#40836;&#30340;&#36523;&#20307;&#21306;&#22495;&#65292;&#24182;&#21019;&#19979;&#20102;&#25972;&#20010;&#36523;&#20307;&#24180;&#40836;&#39044;&#27979;&#30340;&#26368;&#26032;&#27700;&#24179;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#33034;&#26609;&#12289;&#26412;&#21407;&#24615;&#32972;&#37096;&#32908;&#32905;&#21644;&#24515;&#33039;&#21306;&#22495;&#26159;&#26368;&#37325;&#35201;&#30340;&#20851;&#27880;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2307.07439</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#35889;&#30340;&#21487;&#35299;&#37322;&#24180;&#40836;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Atlas-Based Interpretable Age Prediction. (arXiv:2307.07439v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07439
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#35889;&#30340;&#21487;&#35299;&#37322;&#24180;&#40836;&#39044;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#20840;&#36523;&#22270;&#20687;&#30740;&#31350;&#20102;&#21508;&#20010;&#36523;&#20307;&#37096;&#20301;&#30340;&#24180;&#40836;&#30456;&#20851;&#21464;&#21270;&#12290;&#36890;&#36807;&#20351;&#29992;&#35299;&#37322;&#24615;&#26041;&#27861;&#21644;&#37197;&#20934;&#25216;&#26415;&#65292;&#30830;&#23450;&#20102;&#26368;&#33021;&#39044;&#27979;&#24180;&#40836;&#30340;&#36523;&#20307;&#21306;&#22495;&#65292;&#24182;&#21019;&#19979;&#20102;&#25972;&#20010;&#36523;&#20307;&#24180;&#40836;&#39044;&#27979;&#30340;&#26368;&#26032;&#27700;&#24179;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#33034;&#26609;&#12289;&#26412;&#21407;&#24615;&#32972;&#37096;&#32908;&#32905;&#21644;&#24515;&#33039;&#21306;&#22495;&#26159;&#26368;&#37325;&#35201;&#30340;&#20851;&#27880;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24180;&#40836;&#39044;&#27979;&#26159;&#21307;&#23398;&#35780;&#20272;&#21644;&#30740;&#31350;&#30340;&#37325;&#35201;&#37096;&#20998;&#65292;&#21487;&#20197;&#36890;&#36807;&#31361;&#20986;&#23454;&#38469;&#24180;&#40836;&#21644;&#29983;&#29289;&#24180;&#40836;&#20043;&#38388;&#30340;&#24046;&#24322;&#26469;&#24110;&#21161;&#26816;&#27979;&#30142;&#30149;&#21644;&#24322;&#24120;&#34928;&#32769;&#12290;&#20026;&#20102;&#20840;&#38754;&#20102;&#35299;&#21508;&#20010;&#36523;&#20307;&#37096;&#20301;&#30340;&#24180;&#40836;&#30456;&#20851;&#21464;&#21270;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#20840;&#36523;&#22270;&#20687;&#36827;&#34892;&#30740;&#31350;&#12290;&#25105;&#20204;&#21033;&#29992;Grad-CAM&#35299;&#37322;&#24615;&#26041;&#27861;&#30830;&#23450;&#26368;&#33021;&#39044;&#27979;&#19968;&#20010;&#20154;&#24180;&#40836;&#30340;&#36523;&#20307;&#21306;&#22495;&#12290;&#36890;&#36807;&#20351;&#29992;&#37197;&#20934;&#25216;&#26415;&#29983;&#25104;&#25972;&#20010;&#20154;&#32676;&#30340;&#35299;&#37322;&#24615;&#22270;&#65292;&#25105;&#20204;&#23558;&#20998;&#26512;&#25193;&#23637;&#21040;&#20010;&#20307;&#20043;&#22806;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20197;&#19968;&#20010;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#20026;2.76&#24180;&#30340;&#27169;&#22411;&#65292;&#21019;&#19979;&#20102;&#25972;&#20010;&#36523;&#20307;&#24180;&#40836;&#39044;&#27979;&#30340;&#26368;&#26032;&#27700;&#24179;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#19977;&#20010;&#20027;&#35201;&#30340;&#20851;&#27880;&#39046;&#22495;&#65306;&#33034;&#26609;&#12289;&#26412;&#21407;&#24615;&#32972;&#37096;&#32908;&#32905;&#21644;&#24515;&#33039;&#21306;&#22495;&#65292;&#20854;&#20013;&#24515;&#33039;&#21306;&#22495;&#20855;&#26377;&#26368;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Age prediction is an important part of medical assessments and research. It can aid in detecting diseases as well as abnormal ageing by highlighting the discrepancy between chronological and biological age. To gain a comprehensive understanding of age-related changes observed in various body parts, we investigate them on a larger scale by using whole-body images. We utilise the Grad-CAM interpretability method to determine the body areas most predictive of a person's age. We expand our analysis beyond individual subjects by employing registration techniques to generate population-wide interpretability maps. Furthermore, we set state-of-the-art whole-body age prediction with a model that achieves a mean absolute error of 2.76 years. Our findings reveal three primary areas of interest: the spine, the autochthonous back muscles, and the cardiac region, which exhibits the highest importance.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#22522;&#20110;Wasserstein&#37327;&#23376;&#33945;&#29305;&#21345;&#27931;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#37327;&#23376;&#22810;&#20307;Schr\"odinger&#26041;&#31243;&#12290;&#35813;&#26041;&#27861;&#37325;&#26032;&#21046;&#23450;&#20102;&#33021;&#37327;&#27867;&#20989;&#30340;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;Born&#20998;&#24067;&#31354;&#38388;&#20013;&#30340;&#31890;&#23376;&#25490;&#21015;&#65288;&#21453;&#65289;&#23545;&#31216;&#27874;&#20989;&#25968;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#34920;&#31034;&#20016;&#23500;&#30340;&#27874;&#20989;&#25968;&#26063;&#12290;</title><link>http://arxiv.org/abs/2307.07050</link><description>&lt;p&gt;
Wasserstein&#37327;&#23376;&#33945;&#29305;&#21345;&#27931;&#65306;&#35299;&#20915;&#37327;&#23376;&#22810;&#20307;Schr\"odinger&#26041;&#31243;&#30340;&#19968;&#31181;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Wasserstein Quantum Monte Carlo: A Novel Approach for Solving the Quantum Many-Body Schr\"odinger Equation. (arXiv:2307.07050v1 [physics.comp-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07050
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#22522;&#20110;Wasserstein&#37327;&#23376;&#33945;&#29305;&#21345;&#27931;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#37327;&#23376;&#22810;&#20307;Schr\"odinger&#26041;&#31243;&#12290;&#35813;&#26041;&#27861;&#37325;&#26032;&#21046;&#23450;&#20102;&#33021;&#37327;&#27867;&#20989;&#30340;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;Born&#20998;&#24067;&#31354;&#38388;&#20013;&#30340;&#31890;&#23376;&#25490;&#21015;&#65288;&#21453;&#65289;&#23545;&#31216;&#27874;&#20989;&#25968;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#34920;&#31034;&#20016;&#23500;&#30340;&#27874;&#20989;&#25968;&#26063;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#37327;&#23376;&#22810;&#20307;Schr\"odinger&#26041;&#31243;&#26159;&#37327;&#23376;&#29289;&#29702;&#12289;&#37327;&#23376;&#21270;&#23398;&#21644;&#26448;&#26009;&#31185;&#23398;&#39046;&#22495;&#20013;&#19968;&#20010;&#22522;&#26412;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#19968;&#31181;&#24120;&#35265;&#30340;&#35745;&#31639;&#26041;&#27861;&#26159;&#37327;&#23376;&#21464;&#20998;&#33945;&#29305;&#21345;&#27931;&#65288;QVMC&#65289;&#65292;&#20854;&#20013;&#36890;&#36807;&#22312;&#19968;&#20010;&#21442;&#25968;&#21270;&#27874;&#20989;&#25968;&#26063;&#20013;&#26368;&#23567;&#21270;&#31995;&#32479;&#30340;&#33021;&#37327;&#26469;&#33719;&#24471;&#22522;&#24577;&#35299;&#12290;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#35299;&#20915;&#20102;&#20256;&#32479;QVMC&#30340;&#23616;&#38480;&#24615;&#65292;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#20016;&#23500;&#30340;&#27874;&#20989;&#25968;&#26063;&#12290;&#28982;&#32780;&#65292;&#22312;QVMC&#20013;&#20248;&#21270;&#30446;&#26631;&#20173;&#28982;&#38590;&#20197;&#26368;&#23567;&#21270;&#65292;&#38656;&#35201;&#20351;&#29992;&#33258;&#28982;&#26799;&#24230;&#31561;&#20108;&#38454;&#20248;&#21270;&#26041;&#27861;&#12290;&#26412;&#25991;&#39318;&#20808;&#37325;&#26032;&#21046;&#23450;&#20102;&#33021;&#37327;&#27867;&#20989;&#30340;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#23558;&#20854;&#36716;&#21270;&#20026;Born&#20998;&#24067;&#31354;&#38388;&#20013;&#30340;&#31890;&#23376;&#25490;&#21015;&#65288;&#21453;&#65289;&#23545;&#31216;&#27874;&#20989;&#25968;&#30340;&#38382;&#39064;&#65292;&#32780;&#19981;&#26159;&#27874;&#20989;&#25968;&#30340;&#31354;&#38388;&#12290;&#28982;&#21518;&#25105;&#20204;&#23558;QVMC&#35299;&#37322;&#20026;&#22312;&#36825;&#20010;&#31354;&#38388;&#20013;&#30340;Fisher-Rao&#26799;&#24230;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving the quantum many-body Schr\"odinger equation is a fundamental and challenging problem in the fields of quantum physics, quantum chemistry, and material sciences. One of the common computational approaches to this problem is Quantum Variational Monte Carlo (QVMC), in which ground-state solutions are obtained by minimizing the energy of the system within a restricted family of parameterized wave functions. Deep learning methods partially address the limitations of traditional QVMC by representing a rich family of wave functions in terms of neural networks. However, the optimization objective in QVMC remains notoriously hard to minimize and requires second-order optimization methods such as natural gradient. In this paper, we first reformulate energy functional minimization in the space of Born distributions corresponding to particle-permutation (anti-)symmetric wave functions, rather than the space of wave functions. We then interpret QVMC as the Fisher--Rao gradient flow in this
&lt;/p&gt;</description></item><item><title>VoxPoser&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32452;&#21512;3D&#20215;&#20540;&#26144;&#23556;&#21644;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#26426;&#22120;&#20154;&#22312;&#22810;&#31181;&#25805;&#20316;&#20219;&#21153;&#19979;&#26681;&#25454;&#33258;&#30001;&#24418;&#24335;&#30340;&#25351;&#20196;&#21644;&#23545;&#35937;&#21512;&#25104;&#26426;&#22120;&#20154;&#36712;&#36857;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.05973</link><description>&lt;p&gt;
VoxPoser: &#29992;&#20110;&#24102;&#26377;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#21487;&#32452;&#21512;&#30340;3D&#20215;&#20540;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models. (arXiv:2307.05973v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05973
&lt;/p&gt;
&lt;p&gt;
VoxPoser&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32452;&#21512;3D&#20215;&#20540;&#26144;&#23556;&#21644;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#26426;&#22120;&#20154;&#22312;&#22810;&#31181;&#25805;&#20316;&#20219;&#21153;&#19979;&#26681;&#25454;&#33258;&#30001;&#24418;&#24335;&#30340;&#25351;&#20196;&#21644;&#23545;&#35937;&#21512;&#25104;&#26426;&#22120;&#20154;&#36712;&#36857;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#26377;&#20016;&#23500;&#30340;&#21487;&#34892;&#21160;&#30693;&#35782;&#65292;&#21487;&#20197;&#20197;&#25512;&#29702;&#21644;&#35268;&#21010;&#30340;&#24418;&#24335;&#25552;&#21462;&#20986;&#29992;&#20110;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#20449;&#24687;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#22823;&#22810;&#25968;&#27169;&#22411;&#20173;&#28982;&#20381;&#36182;&#20110;&#39044;&#23450;&#20041;&#30340;&#36816;&#21160;&#21407;&#35821;&#26469;&#25191;&#34892;&#19982;&#29615;&#22659;&#30340;&#29289;&#29702;&#20132;&#20114;&#65292;&#36825;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#29942;&#39048;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#32473;&#23450;&#24320;&#38598;&#25351;&#20196;&#21644;&#24320;&#38598;&#23545;&#35937;&#30340;&#24773;&#20917;&#19979;&#65292;&#20026;&#21508;&#31181;&#25805;&#20316;&#20219;&#21153;&#21512;&#25104;&#26426;&#22120;&#20154;&#36712;&#36857;&#65292;&#21363;&#19968;&#31995;&#21015;&#23494;&#38598;&#30340;6-DoF&#26411;&#31471;&#25191;&#34892;&#22120;&#36335;&#24452;&#28857;&#12290;&#25105;&#20204;&#39318;&#20808;&#35266;&#23519;&#21040;LLMs&#22312;&#32473;&#23450;&#33258;&#30001;&#24418;&#24335;&#30340;&#35821;&#35328;&#25351;&#20196;&#26102;&#25797;&#38271;&#25512;&#26029;&#21487;&#34892;&#24615;&#21644;&#32422;&#26463;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#36890;&#36807;&#21033;&#29992;&#23427;&#20204;&#30340;&#20195;&#30721;&#32534;&#20889;&#33021;&#21147;&#65292;&#23427;&#20204;&#21487;&#20197;&#19982;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#20132;&#20114;&#65292;&#20197;&#32452;&#21512;3D&#20215;&#20540;&#26144;&#23556;&#23558;&#30693;&#35782;&#25509;&#22320;&#21040;Agent&#30340;&#35266;&#27979;&#31354;&#38388;&#20013;&#12290;&#28982;&#21518;&#22312;&#22522;&#20110;&#27169;&#22411;&#30340;&#35268;&#21010;&#26694;&#26550;&#20013;&#20351;&#29992;&#32452;&#21512;&#30340;&#20215;&#20540;&#26144;&#23556;&#26469;&#38646;&#35797;&#21512;&#25104;&#38381;&#29615;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are shown to possess a wealth of actionable knowledge that can be extracted for robot manipulation in the form of reasoning and planning. Despite the progress, most still rely on pre-defined motion primitives to carry out the physical interactions with the environment, which remains a major bottleneck. In this work, we aim to synthesize robot trajectories, i.e., a dense sequence of 6-DoF end-effector waypoints, for a large variety of manipulation tasks given an open-set of instructions and an open-set of objects. We achieve this by first observing that LLMs excel at inferring affordances and constraints given a free-form language instruction. More importantly, by leveraging their code-writing capabilities, they can interact with a visual-language model (VLM) to compose 3D value maps to ground the knowledge into the observation space of the agent. The composed value maps are then used in a model-based planning framework to zero-shot synthesize closed-loop ro
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#25552;&#20986;&#20102;&#26032;&#30340;&#25104;&#23601;&#33976;&#39311;&#26041;&#27861;&#65292;&#21487;&#20197;&#21152;&#24378;&#20195;&#29702;&#23545;&#19979;&#19968;&#20010;&#35299;&#38145;&#25104;&#23601;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#20248;&#20110;&#20808;&#21069;&#30340;&#27169;&#22411;&#39537;&#21160;&#21644;&#23618;&#27425;&#21270;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.03486</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#21457;&#29616;&#23618;&#27425;&#21270;&#25104;&#23601;
&lt;/p&gt;
&lt;p&gt;
Discovering Hierarchical Achievements in Reinforcement Learning via Contrastive Learning. (arXiv:2307.03486v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03486
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#25552;&#20986;&#20102;&#26032;&#30340;&#25104;&#23601;&#33976;&#39311;&#26041;&#27861;&#65292;&#21487;&#20197;&#21152;&#24378;&#20195;&#29702;&#23545;&#19979;&#19968;&#20010;&#35299;&#38145;&#25104;&#23601;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#20248;&#20110;&#20808;&#21069;&#30340;&#27169;&#22411;&#39537;&#21160;&#21644;&#23618;&#27425;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#25104;&#29615;&#22659;&#20013;&#21457;&#29616;&#20855;&#26377;&#23618;&#27425;&#32467;&#26500;&#30340;&#25104;&#23601;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#36825;&#38656;&#35201;&#26234;&#33021;&#20307;&#20855;&#22791;&#24191;&#27867;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#27867;&#21270;&#21644;&#38271;&#26399;&#25512;&#29702;&#12290;&#35768;&#22810;&#20808;&#21069;&#30340;&#26041;&#27861;&#22522;&#20110;&#27169;&#22411;&#39537;&#21160;&#25110;&#23618;&#27425;&#21270;&#26041;&#27861;&#65292;&#35748;&#20026;&#26174;&#24335;&#30340;&#38271;&#26399;&#35268;&#21010;&#27169;&#22359;&#23545;&#20110;&#23398;&#20064;&#23618;&#27425;&#21270;&#25104;&#23601;&#26159;&#26377;&#30410;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#30340;&#29615;&#22659;&#20132;&#20114;&#25110;&#22823;&#22411;&#27169;&#22411;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#23454;&#29992;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#36817;&#26399;&#23454;&#26045;&#23454;&#36341;&#20013;&#30340;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;PPO&#65289;&#31639;&#27861;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;PPO&#26234;&#33021;&#20307;&#21487;&#20197;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#39044;&#27979;&#19979;&#19968;&#20010;&#35201;&#35299;&#38145;&#30340;&#25104;&#23601;&#65292;&#23613;&#31649;&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;&#36739;&#20302;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;&#25104;&#23601;&#33976;&#39311;&#65292;&#21487;&#20197;&#21152;&#24378;PPO&#26234;&#33021;&#20307;&#23545;&#19979;&#19968;&#20010;&#35299;&#38145;&#25104;&#23601;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discovering achievements with a hierarchical structure on procedurally generated environments poses a significant challenge. This requires agents to possess a broad range of abilities, including generalization and long-term reasoning. Many prior methods are built upon model-based or hierarchical approaches, with the belief that an explicit module for long-term planning would be beneficial for learning hierarchical achievements. However, these methods require an excessive amount of environment interactions or large model sizes, limiting their practicality. In this work, we identify that proximal policy optimization (PPO), a simple and versatile model-free algorithm, outperforms the prior methods with recent implementation practices. Moreover, we find that the PPO agent can predict the next achievement to be unlocked to some extent, though with low confidence. Based on this observation, we propose a novel contrastive learning method, called achievement distillation, that strengthens the 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35299;&#20915;&#20102;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#28508;&#21464;&#37327;&#35782;&#21035;&#21644;"&#25903;&#25345;&#22806;"&#22270;&#20687;&#29983;&#25104;&#38382;&#39064;&#65292;&#23637;&#31034;&#20102;&#21152;&#27861;&#35299;&#30721;&#22120;&#33021;&#22815;&#23545;&#28508;&#21464;&#37327;&#36827;&#34892;&#35782;&#21035;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#20381;&#25454;&#25903;&#25345;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.02598</link><description>&lt;p&gt;
&#28155;&#21152;&#35299;&#30721;&#22120;&#29992;&#20110;&#28508;&#21464;&#37327;&#35782;&#21035;&#21644;&#31515;&#21345;&#23572;&#31215;&#25512;&#31639;
&lt;/p&gt;
&lt;p&gt;
Additive Decoders for Latent Variables Identification and Cartesian-Product Extrapolation. (arXiv:2307.02598v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02598
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35299;&#20915;&#20102;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#28508;&#21464;&#37327;&#35782;&#21035;&#21644;"&#25903;&#25345;&#22806;"&#22270;&#20687;&#29983;&#25104;&#38382;&#39064;&#65292;&#23637;&#31034;&#20102;&#21152;&#27861;&#35299;&#30721;&#22120;&#33021;&#22815;&#23545;&#28508;&#21464;&#37327;&#36827;&#34892;&#35782;&#21035;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#20381;&#25454;&#25903;&#25345;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#28508;&#21464;&#37327;&#35782;&#21035;&#21644;&#8220;&#25903;&#25345;&#22806;&#8221;&#22270;&#20687;&#29983;&#25104;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#19968;&#31867;&#25105;&#20204;&#31216;&#20026;&#8220;&#21152;&#27861;&#8221;&#30340;&#35299;&#30721;&#22120;&#20013;&#65292;&#36825;&#20004;&#32773;&#26159;&#21487;&#33021;&#30340;&#65292;&#36825;&#20123;&#35299;&#30721;&#22120;&#31867;&#20284;&#20110;&#29992;&#20110;&#38754;&#21521;&#23545;&#35937;&#34920;&#31034;&#23398;&#20064;&#65288;OCRL&#65289;&#30340;&#35299;&#30721;&#22120;&#65292;&#24182;&#19988;&#38750;&#24120;&#36866;&#29992;&#20110;&#21487;&#20197;&#20998;&#35299;&#20026;&#22810;&#20010;&#29305;&#23450;&#23545;&#35937;&#22270;&#20687;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#22312;&#20351;&#29992;&#21152;&#27861;&#35299;&#30721;&#22120;&#23436;&#20840;&#35299;&#20915;&#37325;&#26500;&#38382;&#39064;&#26102;&#65292;&#23545;&#28508;&#21464;&#37327;&#22359;&#36827;&#34892;&#20102;&#32622;&#25442;&#21644;&#22359;&#29366;&#36870;&#21464;&#25442;&#30340;&#35782;&#21035;&#30340;&#26465;&#20214;&#12290;&#36825;&#20010;&#20445;&#35777;&#20165;&#22522;&#20110;&#20851;&#20110;&#28508;&#22240;&#23376;&#20998;&#24067;&#30340;&#38750;&#24120;&#24369;&#30340;&#20551;&#35774;&#65292;&#28508;&#22240;&#23376;&#21487;&#33021;&#23384;&#22312;&#32479;&#35745;&#20381;&#36182;&#24182;&#19988;&#20855;&#26377;&#20960;&#20046;&#20219;&#24847;&#24418;&#29366;&#30340;&#25903;&#25345;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25552;&#20379;&#20102;&#38750;&#32447;&#24615;&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;&#65288;ICA&#65289;&#21487;&#33021;&#24615;&#30340;&#26032;&#35774;&#32622;&#65292;&#24182;&#19988;&#22686;&#21152;&#20102;&#25105;&#20204;&#23545;OCRL&#26041;&#27861;&#30340;&#29702;&#35770;&#29702;&#35299;&#12290;&#25105;&#20204;&#36824;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#21152;&#27861;&#35299;&#30721;&#22120;&#21487;&#20197;
&lt;/p&gt;
&lt;p&gt;
We tackle the problems of latent variables identification and "out-of-support" image generation in representation learning. We show that both are possible for a class of decoders that we call additive, which are reminiscent of decoders used for object-centric representation learning (OCRL) and well suited for images that can be decomposed as a sum of object-specific images. We provide conditions under which exactly solving the reconstruction problem using an additive decoder is guaranteed to identify the blocks of latent variables up to permutation and block-wise invertible transformations. This guarantee relies only on very weak assumptions about the distribution of the latent factors, which might present statistical dependencies and have an almost arbitrarily shaped support. Our result provides a new setting where nonlinear independent component analysis (ICA) is possible and adds to our theoretical understanding of OCRL methods. We also show theoretically that additive decoders can 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;EHRSHOT&#65292;&#19968;&#20010;&#29992;&#20110;&#23569;&#26679;&#26412;&#35780;&#20272;&#22522;&#30784;&#27169;&#22411;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#22522;&#20934;&#12290;&#35813;&#35770;&#25991;&#21033;&#29992;EHRSHOT&#25968;&#25454;&#38598;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;CLMBR-T-base&#65292;&#20026;&#21307;&#30103;&#20445;&#20581;ML&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2307.02028</link><description>&lt;p&gt;
EHRSHOT:&#19968;&#31181;&#29992;&#20110;&#23569;&#26679;&#26412;&#35780;&#20272;&#22522;&#30784;&#27169;&#22411;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
EHRSHOT: An EHR Benchmark for Few-Shot Evaluation of Foundation Models. (arXiv:2307.02028v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02028
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;EHRSHOT&#65292;&#19968;&#20010;&#29992;&#20110;&#23569;&#26679;&#26412;&#35780;&#20272;&#22522;&#30784;&#27169;&#22411;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#22522;&#20934;&#12290;&#35813;&#35770;&#25991;&#21033;&#29992;EHRSHOT&#25968;&#25454;&#38598;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;CLMBR-T-base&#65292;&#20026;&#21307;&#30103;&#20445;&#20581;ML&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#19968;&#33324;&#30340;&#26426;&#22120;&#23398;&#20064;(ML)&#31038;&#21306;&#24050;&#32463;&#21463;&#30410;&#20110;&#20844;&#24320;&#30340;&#25968;&#25454;&#38598;&#12289;&#20219;&#21153;&#21644;&#27169;&#22411;&#65292;&#20294;&#26159;ML&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#36827;&#23637;&#21463;&#21040;&#20102;&#20849;&#20139;&#36164;&#20135;&#30340;&#32570;&#20047;&#30340;&#38459;&#30861;&#12290;&#22522;&#30784;&#27169;&#22411;&#30340;&#25104;&#21151;&#20026;&#21307;&#30103;&#20445;&#20581;ML&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#38656;&#35201;&#35775;&#38382;&#20849;&#20139;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#39564;&#35777;&#24615;&#33021;&#20248;&#21183;&#12290;&#25105;&#20204;&#36890;&#36807;&#19977;&#20010;&#36129;&#29486;&#26469;&#24110;&#21161;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;EHRSHOT&#65292;&#20854;&#20013;&#21253;&#21547;6,739&#21517;&#26469;&#33258;&#26031;&#22374;&#31119;&#21307;&#23398;&#30340;&#24739;&#32773;&#30340;&#21435;&#35782;&#21035;&#32467;&#26500;&#21270;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;(EHR)&#25968;&#25454;&#12290;&#19982;MIMIC-III/IV&#21644;&#20854;&#20182;&#27969;&#34892;&#30340;EHR&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;EHRSHOT&#26159;&#32437;&#21521;&#30340;&#65292;&#19981;&#20165;&#23616;&#38480;&#20110;ICU/ED&#24739;&#32773;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;CLMBR-T-base&#30340;&#26435;&#37325;&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#32467;&#26500;&#21270;EHR&#25968;&#25454;&#20013;&#39044;&#35757;&#32451;&#30340;141M&#21442;&#25968;&#20020;&#24202;&#22522;&#30784;&#27169;&#22411;&#65292;&#35813;&#25968;&#25454;&#21253;&#25324;2.57M&#21517;&#24739;&#32773;&#12290;&#25105;&#20204;&#26159;&#26368;&#26089;&#23436;&#20840;&#21457;&#24067;&#36825;&#26679;&#19968;&#20010;&#29992;&#20110;&#32534;&#30721;EHR&#25968;&#25454;&#30340;&#27169;&#22411;&#20043;&#19968;&#65307;&#30456;&#27604;&#20043;&#19979;&#65292;&#22823;&#22810;&#25968;&#20808;&#21069;&#21457;&#24067;&#30340;&#20020;&#24202;&#25968;&#25454;&#27169;&#22411;&#65288;&#22914;GatorTron&#12289;ClinicalBER&#65289;&#24182;&#27809;&#26377;&#23436;&#20840;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
While the general machine learning (ML) community has benefited from public datasets, tasks, and models, the progress of ML in healthcare has been hampered by a lack of such shared assets. The success of foundation models creates new challenges for healthcare ML by requiring access to shared pretrained models to validate performance benefits. We help address these challenges through three contributions. First, we publish a new dataset, EHRSHOT, which contains deidentified structured data from the electronic health records (EHRs) of 6,739 patients from Stanford Medicine. Unlike MIMIC-III/IV and other popular EHR datasets, EHRSHOT is longitudinal and not restricted to ICU/ED patients. Second, we publish the weights of CLMBR-T-base, a 141M parameter clinical foundation model pretrained on the structured EHR data of 2.57M patients. We are one of the first to fully release such a model for coded EHR data; in contrast, most prior models released for clinical data (e.g. GatorTron, ClinicalBER
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#21452;&#22240;&#26524;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#30340;&#25311;&#21512;&#20540;&#36845;&#20195;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#20445;&#35777;&#31934;&#24230;&#30340;&#21516;&#26102;&#20855;&#26377;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#25968;&#20540;&#23454;&#39564;&#32467;&#26524;&#20063;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.12658</link><description>&lt;p&gt;
&#25311;&#21512;&#20540;&#36845;&#20195;&#26041;&#27861;&#27714;&#35299;&#36866;&#24212;&#32467;&#26500;&#21452;&#22240;&#26524;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Fitted Value Iteration Methods for Bicausal Optimal Transport. (arXiv:2306.12658v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#21452;&#22240;&#26524;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#30340;&#25311;&#21512;&#20540;&#36845;&#20195;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#20445;&#35777;&#31934;&#24230;&#30340;&#21516;&#26102;&#20855;&#26377;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#25968;&#20540;&#23454;&#39564;&#32467;&#26524;&#20063;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#25311;&#21512;&#20540;&#36845;&#20195;&#26041;&#27861;(FVI)&#29992;&#20110;&#35745;&#31639;&#20855;&#26377;&#36866;&#24212;&#32467;&#26500;&#30340;&#21452;&#22240;&#26524;&#26368;&#20248;&#20256;&#36755;(OT)&#12290;&#22522;&#20110;&#21160;&#24577;&#35268;&#21010;&#30340;&#24418;&#24335;&#21270;&#34920;&#36848;&#65292;FVI&#37319;&#29992;&#20989;&#25968;&#31867;&#29992;&#20110;&#36817;&#20284;&#21452;&#22240;&#26524;OT&#20013;&#30340;&#20540;&#20989;&#25968;&#12290;&#22312;&#21487;&#38598;&#20013;&#26465;&#20214;&#21644;&#36817;&#20284;&#23436;&#22791;&#24615;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#20351;&#29992;&#65288;&#23616;&#37096;&#65289;Rademacher&#22797;&#26434;&#24230;&#35777;&#26126;&#20102;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#28145;&#24230;&#22810;&#23618;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#36866;&#24403;&#32467;&#26500;&#65292;&#28385;&#36275;&#26679;&#26412;&#22797;&#26434;&#24230;&#35777;&#26126;&#25152;&#38656;&#30340;&#20851;&#38190;&#20551;&#35774;&#26465;&#20214;&#12290;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;FVI&#22312;&#26102;&#38388;&#36328;&#24230;&#22686;&#21152;&#26102;&#20248;&#20110;&#32447;&#24615;&#35268;&#21010;&#21644;&#36866;&#24212;&#24615;Sinkhorn&#26041;&#27861;&#65292;&#22312;&#20445;&#25345;&#21487;&#25509;&#21463;&#31934;&#24230;&#30340;&#21516;&#26102;&#20855;&#26377;&#24456;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a fitted value iteration (FVI) method to compute bicausal optimal transport (OT) where couplings have an adapted structure. Based on the dynamic programming formulation, FVI adopts a function class to approximate the value functions in bicausal OT. Under the concentrability condition and approximate completeness assumption, we prove the sample complexity using (local) Rademacher complexity. Furthermore, we demonstrate that multilayer neural networks with appropriate structures satisfy the crucial assumptions required in sample complexity proofs. Numerical experiments reveal that FVI outperforms linear programming and adapted Sinkhorn methods in scalability as the time horizon increases, while still maintaining acceptable accuracy.
&lt;/p&gt;</description></item><item><title>Fedstellar&#26159;&#19968;&#20010;&#32852;&#37030;&#23398;&#20064;&#24179;&#21488;&#65292;&#25903;&#25345;&#29289;&#29702;&#25110;&#34394;&#25311;&#35774;&#22791;&#30340;&#21435;&#20013;&#24515;&#21270;&#12289;&#21322;&#21435;&#20013;&#24515;&#21270;&#21644;&#20013;&#24515;&#21270;&#30340;&#26041;&#24335;&#35757;&#32451;&#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#24179;&#21488;&#22312;&#22788;&#29702;&#24322;&#26500;&#32852;&#30431;&#32593;&#32476;&#25299;&#25169;&#31561;&#38382;&#39064;&#26102;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.09750</link><description>&lt;p&gt;
Fedstellar&#65306;&#19968;&#20010;&#21435;&#20013;&#24515;&#21270;&#32852;&#37030;&#23398;&#20064;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
Fedstellar: A Platform for Decentralized Federated Learning. (arXiv:2306.09750v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09750
&lt;/p&gt;
&lt;p&gt;
Fedstellar&#26159;&#19968;&#20010;&#32852;&#37030;&#23398;&#20064;&#24179;&#21488;&#65292;&#25903;&#25345;&#29289;&#29702;&#25110;&#34394;&#25311;&#35774;&#22791;&#30340;&#21435;&#20013;&#24515;&#21270;&#12289;&#21322;&#21435;&#20013;&#24515;&#21270;&#21644;&#20013;&#24515;&#21270;&#30340;&#26041;&#24335;&#35757;&#32451;&#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#24179;&#21488;&#22312;&#22788;&#29702;&#24322;&#26500;&#32852;&#30431;&#32593;&#32476;&#25299;&#25169;&#31561;&#38382;&#39064;&#26102;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
2016&#24180;&#65292;&#35895;&#27468;&#25552;&#20986;&#20102;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#36328;&#32852;&#30431;&#21442;&#19982;&#32773;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#12290;&#34429;&#28982;&#20013;&#24515;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;CFL&#65289;&#26159;&#26368;&#24120;&#29992;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#23384;&#22312;&#36890;&#20449;&#29942;&#39048;&#12289;&#21333;&#28857;&#25925;&#38556;&#21644;&#23545;&#20013;&#22830;&#26381;&#21153;&#22120;&#30340;&#20381;&#36182;&#31561;&#23616;&#38480;&#12290;&#21435;&#20013;&#24515;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;DFL&#65289;&#36890;&#36807;&#23454;&#29616;&#21435;&#20013;&#24515;&#21270;&#27169;&#22411;&#32858;&#21512;&#21644;&#26368;&#23567;&#21270;&#23545;&#20013;&#22830;&#23454;&#20307;&#30340;&#20381;&#36182;&#65292;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#35757;&#32451;DFL&#27169;&#22411;&#30340;&#24179;&#21488;&#22312;&#22788;&#29702;&#24322;&#26500;&#32852;&#30431;&#32593;&#32476;&#25299;&#25169;&#31561;&#20851;&#38190;&#38382;&#39064;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;Fedstellar&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#22411;&#30340;&#24179;&#21488;&#65292;&#26088;&#22312;&#22312;&#29289;&#29702;&#25110;&#34394;&#25311;&#35774;&#22791;&#30340;&#19981;&#21516;&#32852;&#30431;&#20013;&#20197;&#21435;&#20013;&#24515;&#21270;&#12289;&#21322;&#21435;&#20013;&#24515;&#21270;&#21644;&#20013;&#24515;&#21270;&#30340;&#26041;&#24335;&#35757;&#32451;FL&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In 2016, Google proposed Federated Learning (FL) as a novel paradigm to train Machine Learning (ML) models across the participants of a federation while preserving data privacy. Since its birth, Centralized FL (CFL) has been the most used approach, where a central entity aggregates participants' models to create a global one. However, CFL presents limitations such as communication bottlenecks, single point of failure, and reliance on a central server. Decentralized Federated Learning (DFL) addresses these issues by enabling decentralized model aggregation and minimizing dependency on a central entity. Despite these advances, current platforms training DFL models struggle with key issues such as managing heterogeneous federation network topologies. To overcome these challenges, this paper presents Fedstellar, a novel platform designed to train FL models in a decentralized, semi-decentralized, and centralized fashion across diverse federations of physical or virtualized devices. The Feds
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#24102;&#29366;&#30697;&#38453;&#26500;&#24314;&#30340;&#30697;&#38453;&#20998;&#35299;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#33021;&#22815;&#22312;&#25152;&#26377;&#38544;&#31169;&#39044;&#31639;&#20013;&#23558;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#32435;&#20837;&#20998;&#25955;&#21644;&#32852;&#21512;&#35757;&#32451;&#35774;&#32622;&#20013;&#12290;&#23545;&#20110;&#36328;&#35774;&#22791;&#32852;&#21512;&#23398;&#20064;&#65292;&#36825;&#24847;&#21619;&#30528;&#21487;&#20197;&#20351;&#29992;&#19968;&#31181;&#25918;&#26494;&#30340;&#35774;&#22791;&#21442;&#19982;&#27169;&#24335;&#65292;&#19982;&#23454;&#38469;&#30340;FL&#22522;&#30784;&#35774;&#26045;&#30456;&#23481;&#12290;&#22312;&#38598;&#20013;&#24335;&#35774;&#32622;&#20013;&#65292;&#24102;&#29366;&#30697;&#38453;&#20855;&#26377;&#19982; ubiquitous DP-SGD algorithm &#30456;&#21516;&#30340;&#38544;&#31169;&#25918;&#22823;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.08153</link><description>&lt;p&gt;
&#65288;&#25193;&#22823;&#65289;&#24102;&#29366;&#30697;&#38453;&#20998;&#35299;&#65306;&#19968;&#31181;&#32479;&#19968;&#30340;&#38544;&#31169;&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
(Amplified) Banded Matrix Factorization: A unified approach to private training. (arXiv:2306.08153v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08153
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#24102;&#29366;&#30697;&#38453;&#26500;&#24314;&#30340;&#30697;&#38453;&#20998;&#35299;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#33021;&#22815;&#22312;&#25152;&#26377;&#38544;&#31169;&#39044;&#31639;&#20013;&#23558;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#32435;&#20837;&#20998;&#25955;&#21644;&#32852;&#21512;&#35757;&#32451;&#35774;&#32622;&#20013;&#12290;&#23545;&#20110;&#36328;&#35774;&#22791;&#32852;&#21512;&#23398;&#20064;&#65292;&#36825;&#24847;&#21619;&#30528;&#21487;&#20197;&#20351;&#29992;&#19968;&#31181;&#25918;&#26494;&#30340;&#35774;&#22791;&#21442;&#19982;&#27169;&#24335;&#65292;&#19982;&#23454;&#38469;&#30340;FL&#22522;&#30784;&#35774;&#26045;&#30456;&#23481;&#12290;&#22312;&#38598;&#20013;&#24335;&#35774;&#32622;&#20013;&#65292;&#24102;&#29366;&#30697;&#38453;&#20855;&#26377;&#19982; ubiquitous DP-SGD algorithm &#30456;&#21516;&#30340;&#38544;&#31169;&#25918;&#22823;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#19979;&#30340;&#30697;&#38453;&#20998;&#35299;&#65288;MF&#65289;&#26426;&#21046;&#22312;&#35768;&#22810;&#22330;&#26223;&#19979;&#26174;&#33879;&#25913;&#36827;&#20102;&#38544;&#31169;-&#25928;&#29992;-&#35745;&#31639;&#25240;&#34935;&#30340;&#26368;&#26032;&#25216;&#26415;&#12290;&#20294;&#26159;&#22312;&#20998;&#25955;&#21644;&#32852;&#21512;&#35774;&#32622;&#20013;&#65292;&#20173;&#23384;&#22312;MF&#19981;&#26131;&#36866;&#29992;&#30340;&#23454;&#20363;&#65292;&#25110;&#32773;&#20854;&#20182;&#31639;&#27861;&#25552;&#20379;&#26356;&#22909;&#30340;&#25240;&#34935;&#65288;&#36890;&#24120;&#38543;&#30528; &#949; &#21464;&#23567;&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#24102;&#29366;&#30697;&#38453;&#26500;&#24314;MF&#26426;&#21046;&#65292;&#22312;&#25152;&#26377;&#38544;&#31169;&#39044;&#31639;&#20013;&#23558;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#32435;&#20837;&#20998;&#25955;&#21644;&#32852;&#21512;&#35757;&#32451;&#35774;&#32622;&#20013;&#12290;&#20851;&#38190;&#25216;&#26415;&#26159;&#24102;&#29366;&#30697;&#38453;&#30340;&#26500;&#36896;&#12290;&#23545;&#20110;&#36328;&#35774;&#22791;&#32852;&#21512;&#23398;&#20064;&#65288;FL&#65289;&#65292;&#36825;&#20351;&#24471;&#22810;&#20010;&#35774;&#22791;&#21487;&#20197;&#20351;&#29992;&#19968;&#31181;&#25918;&#26494;&#30340;&#35774;&#22791;&#21442;&#19982;&#27169;&#24335;&#65292;&#19982;&#23454;&#38469;&#30340;FL&#22522;&#30784;&#35774;&#26045;&#30456;&#23481;&#65288;&#22914;&#20135;&#21697;&#37096;&#32626;&#25152;&#31034;&#65289;&#12290;&#22312;&#38598;&#20013;&#24335;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#24102;&#29366;&#30697;&#38453;&#20855;&#26377;&#19982; ubiquitous DP-SGD algorithm &#30456;&#21516;&#30340;&#38544;&#31169;&#25918;&#22823;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Matrix factorization (MF) mechanisms for differential privacy (DP) have substantially improved the state-of-the-art in privacy-utility-computation tradeoffs for ML applications in a variety of scenarios, but in both the centralized and federated settings there remain instances where either MF cannot be easily applied, or other algorithms provide better tradeoffs (typically, as $\epsilon$ becomes small).  In this work, we show how MF can subsume prior state-of-the-art algorithms in both federated and centralized training settings, across all privacy budgets. The key technique throughout is the construction of MF mechanisms with banded matrices. For cross-device federated learning (FL), this enables multiple-participations with a relaxed device participation schema compatible with practical FL infrastructure (as demonstrated by a production deployment). In the centralized setting, we prove that banded matrices enjoy the same privacy amplification results as for the ubiquitous DP-SGD algo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;nuPlan&#65292;&#19968;&#20010;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26041;&#26696;&#65292;&#38024;&#23545;&#31934;&#30830;&#30340;&#30701;&#26399;&#35268;&#21010;&#21644;&#38271;&#26399;&#30446;&#26631;&#39044;&#27979;&#12290;&#35777;&#23454;&#20102;&#29616;&#26377;&#31995;&#32479;&#38590;&#20197;&#21516;&#26102;&#28385;&#36275;&#20004;&#20010;&#35201;&#27714;&#12290;&#26368;&#32456;&#25552;&#20986;&#19968;&#20010;&#38750;&#24120;&#31616;&#21333;&#39640;&#25928;&#30340;&#35268;&#21010;&#22120;&#12290;</title><link>http://arxiv.org/abs/2306.07962</link><description>&lt;p&gt;
&#19982;&#23398;&#20064;&#23548;&#21521;&#30340;&#36710;&#36742;&#36816;&#21160;&#35268;&#21010;&#30340;&#35823;&#35299;&#21578;&#21035;
&lt;/p&gt;
&lt;p&gt;
Parting with Misconceptions about Learning-based Vehicle Motion Planning. (arXiv:2306.07962v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07962
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;nuPlan&#65292;&#19968;&#20010;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26041;&#26696;&#65292;&#38024;&#23545;&#31934;&#30830;&#30340;&#30701;&#26399;&#35268;&#21010;&#21644;&#38271;&#26399;&#30446;&#26631;&#39044;&#27979;&#12290;&#35777;&#23454;&#20102;&#29616;&#26377;&#31995;&#32479;&#38590;&#20197;&#21516;&#26102;&#28385;&#36275;&#20004;&#20010;&#35201;&#27714;&#12290;&#26368;&#32456;&#25552;&#20986;&#19968;&#20010;&#38750;&#24120;&#31616;&#21333;&#39640;&#25928;&#30340;&#35268;&#21010;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
nuPlan&#30340;&#21457;&#24067;&#26631;&#24535;&#30528;&#36710;&#36742;&#36816;&#21160;&#35268;&#21010;&#30740;&#31350;&#30340;&#19968;&#20010;&#26032;&#26102;&#20195;&#65292;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#38656;&#35201;&#31934;&#30830;&#30340;&#30701;&#26399;&#35268;&#21010;&#21644;&#38271;&#26399;&#30446;&#26631;&#39044;&#27979;&#30340;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26041;&#26696;&#12290;&#29616;&#26377;&#31995;&#32479;&#38590;&#20197;&#21516;&#26102;&#28385;&#36275;&#20004;&#20010;&#35201;&#27714;&#12290;&#23454;&#38469;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#20219;&#21153;&#23384;&#22312;&#26681;&#26412;&#19978;&#30340;&#19981;&#23545;&#40784;&#38382;&#39064;&#65292;&#24212;&#35813;&#20998;&#21035;&#36827;&#34892;&#35299;&#20915;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35780;&#20272;&#20102;&#39046;&#22495;&#20869;&#38381;&#29615;&#35268;&#21010;&#30340;&#29616;&#29366;&#65292;&#25581;&#31034;&#20102;&#23398;&#20064;&#20026;&#22522;&#30784;&#30340;&#26041;&#27861;&#22312;&#22797;&#26434;&#30340;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#23616;&#38480;&#24615;&#65292;&#20197;&#21450;&#36873;&#25321;&#36890;&#36807;&#36710;&#36947;&#22270;&#25628;&#32034;&#31639;&#27861;&#30340;&#31616;&#21333;&#22522;&#20110;&#35268;&#21017;&#30340;&#20808;&#39564;&#39033;&#65288;&#20363;&#22914;&#20013;&#24515;&#32447;&#36873;&#25321;&#65289;&#30340;&#20215;&#20540;&#12290;&#26356;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22312;&#24320;&#29615;&#23376;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#24403;&#20165;&#20351;&#29992;&#36825;&#20010;&#20013;&#24515;&#32447;&#20316;&#20026;&#22330;&#26223;&#19978;&#19979;&#25991;&#26102;&#65288;&#21363;&#24573;&#30053;&#25152;&#26377;&#26377;&#20851;&#22320;&#22270;&#21644;&#20854;&#20182;&#20195;&#29702;&#30340;&#20449;&#24687;&#65289;&#21487;&#20197;&#33719;&#24471;&#26368;&#20339;&#32467;&#26524;&#12290;&#32467;&#21512;&#36825;&#20123;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#24120;&#31616;&#21333;&#39640;&#25928;&#30340;&#35268;&#21010;&#22120;&#65292;&#23427;&#30340;&#34920;&#29616;&#20248;&#20110;&#22823;&#37327;&#31454;&#20105;&#23545;&#25163;&#12290;
&lt;/p&gt;
&lt;p&gt;
The release of nuPlan marks a new era in vehicle motion planning research, offering the first large-scale real-world dataset and evaluation schemes requiring both precise short-term planning and long-horizon ego-forecasting. Existing systems struggle to simultaneously meet both requirements. Indeed, we find that these tasks are fundamentally misaligned and should be addressed independently. We further assess the current state of closed-loop planning in the field, revealing the limitations of learning-based methods in complex real-world scenarios and the value of simple rule-based priors such as centerline selection through lane graph search algorithms. More surprisingly, for the open-loop sub-task, we observe that the best results are achieved when using only this centerline as scene context (\ie, ignoring all information regarding the map and other agents). Combining these insights, we propose an extremely simple and efficient planner which outperforms an extensive set of competitors,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20284;&#20248;&#21270;&#26041;&#27861;&#26469;&#23454;&#29616;&#24179;&#22374;&#23616;&#37096;&#26497;&#23567;&#20540;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#23545;&#25239;&#24615;&#36716;&#31227;&#33021;&#21147;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.05225</link><description>&lt;p&gt;
&#23454;&#29616;&#24179;&#22374;&#23616;&#37096;&#26497;&#23567;&#20540;&#20197;&#25552;&#39640;&#23545;&#25239;&#24615;&#36716;&#31227;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Boosting Adversarial Transferability by Achieving Flat Local Maxima. (arXiv:2306.05225v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05225
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20284;&#20248;&#21270;&#26041;&#27861;&#26469;&#23454;&#29616;&#24179;&#22374;&#23616;&#37096;&#26497;&#23567;&#20540;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#23545;&#25239;&#24615;&#36716;&#31227;&#33021;&#21147;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#36716;&#31227;&#30340;&#25915;&#20987;&#37319;&#29992;&#22312;&#26367;&#20195;&#27169;&#22411;&#19978;&#29983;&#25104;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#26469;&#25915;&#20987;&#21508;&#31181;&#27169;&#22411;&#65292;&#22312;&#29289;&#29702;&#19990;&#30028;&#20013;&#36866;&#29992;&#24182;&#21560;&#24341;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#21463;&#21040;&#24179;&#22374;&#23616;&#37096;&#26497;&#23567;&#20540;&#19982;&#33391;&#22909;&#30340;&#27867;&#21270;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#21551;&#21457;&#65292;&#25105;&#20204;&#20551;&#35774;&#24182;&#32463;&#39564;&#24615;&#22320;&#39564;&#35777;&#20102;&#65292;&#22312;&#24179;&#22374;&#23616;&#37096;&#21306;&#22495;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#20542;&#21521;&#20110;&#20855;&#26377;&#33391;&#22909;&#30340;&#36716;&#31227;&#33021;&#21147;&#65292;&#36890;&#36807;&#22312;&#21407;&#22987;&#25439;&#22833;&#20989;&#25968;&#20013;&#24341;&#20837;&#24809;&#32602;&#26799;&#24230;&#33539;&#25968;&#12290;&#30001;&#20110;&#30452;&#25509;&#20248;&#21270;&#26799;&#24230;&#27491;&#21017;&#21270;&#33539;&#25968;&#22312;&#29983;&#25104;&#23545;&#25239;&#24615;&#31034;&#20363;&#26102;&#35745;&#31639;&#20195;&#20215;&#26114;&#36149;&#19988;&#38590;&#20197;&#22788;&#29702;&#65292;&#22240;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20284;&#20248;&#21270;&#26041;&#27861;&#65292;&#20197;&#31616;&#21270;&#30446;&#26631;&#20989;&#25968;&#30340;&#26799;&#24230;&#26356;&#26032;&#12290;&#25105;&#20204;&#38543;&#26426;&#37319;&#26679;&#19968;&#20010;&#31034;&#20363;&#65292;&#24182;&#37319;&#29992;&#19968;&#38454;&#26799;&#24230;&#26469;&#36924;&#36817;&#20108;&#38454;&#40657;&#22622;&#30697;&#38453;&#65292;&#20174;&#32780;&#20351;&#35745;&#31639;&#26356;&#21152;&#23481;&#26131;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer-based attack adopts the adversarial examples generated on the surrogate model to attack various models, making it applicable in the physical world and attracting increasing interest. Recently, various adversarial attacks have emerged to boost adversarial transferability from different perspectives. In this work, inspired by the fact that flat local minima are correlated with good generalization, we assume and empirically validate that adversarial examples at a flat local region tend to have good transferability by introducing a penalized gradient norm to the original loss function. Since directly optimizing the gradient regularization norm is computationally expensive and intractable for generating adversarial examples, we propose an approximation optimization method to simplify the gradient update of the objective function. Specifically, we randomly sample an example and adopt the first-order gradient to approximate the second-order Hessian matrix, which makes computing more 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#32771;&#34385;&#23558;1-WL&#21644;MPNN&#36830;&#32493;&#25193;&#23637;&#20026;graphon&#65292;&#24182;&#36890;&#36807;&#20934;&#30830;&#30340;&#25299;&#25169;&#34920;&#24449;&#25581;&#31034;&#20102;MPNN&#22312;&#22270;&#19978;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#20026;&#22270;&#21644;graphon&#30456;&#20284;&#24230;&#25552;&#20379;&#20102;&#29702;&#35770;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2306.03698</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32454;&#31890;&#24230;&#34920;&#36798;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Fine-grained Expressivity of Graph Neural Networks. (arXiv:2306.03698v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03698
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#32771;&#34385;&#23558;1-WL&#21644;MPNN&#36830;&#32493;&#25193;&#23637;&#20026;graphon&#65292;&#24182;&#36890;&#36807;&#20934;&#30830;&#30340;&#25299;&#25169;&#34920;&#24449;&#25581;&#31034;&#20102;MPNN&#22312;&#22270;&#19978;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#20026;&#22270;&#21644;graphon&#30456;&#20284;&#24230;&#25552;&#20379;&#20102;&#29702;&#35770;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#35768;&#22810;&#30740;&#31350;&#20998;&#26512;&#20102;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;MPNN&#65289;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#20027;&#35201;&#20351;&#29992;&#20102;&#22914;&#22270;&#21516;&#26500;&#38382;&#39064;&#30340;1&#32500;Weisfeiler-Leman&#27979;&#35797;&#65288;1-WL&#65289;&#31561;&#32452;&#21512;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#22270;&#21516;&#26500;&#30446;&#26631;&#22312;&#26412;&#36136;&#19978;&#26159;&#20108;&#36827;&#21046;&#30340;&#65292;&#19981;&#33021;&#25581;&#31034;&#20004;&#20010;&#32473;&#23450;&#22270;&#20043;&#38388;&#30340;&#30456;&#20284;&#31243;&#24230;&#12290;&#26412;&#24037;&#20316;&#36890;&#36807;&#32771;&#34385;&#23558;1-WL&#21644;MPNN&#36830;&#32493;&#25193;&#23637;&#20026;graphon&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36830;&#32493;&#21464;&#37327;&#30340;1-WL&#23545;&#20110;MPNN&#22312;graphon&#19978;&#30340;&#34920;&#36798;&#33021;&#21147;&#36827;&#34892;&#20102;&#20934;&#30830;&#30340;&#25299;&#25169;&#34920;&#24449;&#65292;&#25581;&#31034;&#20102;&#36825;&#20123;&#32593;&#32476;&#21487;&#20197;&#21306;&#20998;&#21738;&#20123;&#22270;&#20197;&#21450;&#22312;&#21306;&#20998;&#23427;&#20204;&#26102;&#30340;&#38590;&#24230;&#32423;&#21035;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;MPNN&#20998;&#31163;&#28857;&#30340;&#26368;&#32454;&#25299;&#25169;&#65292;&#24182;&#35777;&#26126;&#20102;&#19968;&#20010;&#26222;&#36941;&#36924;&#36817;&#23450;&#29702;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#23558;&#21508;&#31181;&#25299;&#25169;&#21464;&#20307;&#30340;&#32463;&#20856;&#29305;&#24449;&#32467;&#21512;&#36215;&#26469;&#30340;&#22270;&#21644;graphon&#30456;&#20284;&#24230;&#30340;&#29702;&#35770;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerous recent works have analyzed the expressive power of message-passing graph neural networks (MPNNs), primarily utilizing combinatorial techniques such as the $1$-dimensional Weisfeiler-Leman test ($1$-WL) for the graph isomorphism problem. However, the graph isomorphism objective is inherently binary, not giving insights into the degree of similarity between two given graphs. This work resolves this issue by considering continuous extensions of both $1$-WL and MPNNs to graphons. Concretely, we show that the continuous variant of $1$-WL delivers an accurate topological characterization of the expressive power of MPNNs on graphons, revealing which graphs these networks can distinguish and the level of difficulty in separating them. We identify the finest topology where MPNNs separate points and prove a universal approximation theorem. Consequently, we provide a theoretical framework for graph and graphon similarity combining various topological variants of classical characterizatio
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ReNO&#30340;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#31070;&#32463;&#25805;&#20316;&#31526;&#22312;&#31163;&#25955;&#23454;&#29616;&#26102;&#20986;&#29616;&#30340;&#23436;&#25972;&#24615;&#25439;&#22833;&#21644;&#35823;&#24046;&#38382;&#39064;&#65292;&#20026;&#31070;&#32463;&#25805;&#20316;&#31526;&#30340;&#23398;&#20064;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2305.19913</link><description>&lt;p&gt;
Representation Equivalent Neural Operators: &#19968;&#31181;&#26080;&#21035;&#21517;&#30340;&#25805;&#20316;&#31526;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Representation Equivalent Neural Operators: a Framework for Alias-free Operator Learning. (arXiv:2305.19913v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19913
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ReNO&#30340;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#31070;&#32463;&#25805;&#20316;&#31526;&#22312;&#31163;&#25955;&#23454;&#29616;&#26102;&#20986;&#29616;&#30340;&#23436;&#25972;&#24615;&#25439;&#22833;&#21644;&#35823;&#24046;&#38382;&#39064;&#65292;&#20026;&#31070;&#32463;&#25805;&#20316;&#31526;&#30340;&#23398;&#20064;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25805;&#20316;&#31526;&#23398;&#20064;&#65292;&#25110;&#32773;&#23398;&#20064;&#26080;&#38480;&#32500;&#20989;&#25968;&#31354;&#38388;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#65292;&#29305;&#21035;&#26159;&#19982;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#20559;&#24494;&#20998;&#26041;&#31243;&#30456;&#20851;&#30340;&#39046;&#22495;&#12290;&#22312;&#32440;&#19978;&#27010;&#24565;&#19978;&#24456;&#28165;&#26224;&#30340;&#31070;&#32463;&#25805;&#20316;&#31526;&#22312;&#36716;&#25442;&#25104;&#35745;&#31639;&#26426;&#23454;&#29616;&#26102;&#38656;&#35201;&#31163;&#25955;&#21270;&#12290;&#36825;&#19968;&#27493;&#39588;&#21487;&#33021;&#20250;&#25439;&#23475;&#23427;&#20204;&#30340;&#23436;&#25972;&#24615;&#65292;&#23548;&#33268;&#23427;&#20204;&#19982;&#24213;&#23618;&#25805;&#20316;&#31526;&#20559;&#31163;&#12290;&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#23545;&#31070;&#32463;&#25805;&#20316;&#31526;&#30340;&#26032;&#35265;&#35299;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;&#34920;&#31034;&#31561;&#25928;&#31070;&#32463;&#25805;&#20316;&#31526;&#65288;ReNO&#65289;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#20854;&#26680;&#24515;&#26159;&#25805;&#20316;&#31526;&#21035;&#21517;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#34913;&#37327;&#31070;&#32463;&#25805;&#20316;&#31526;&#19982;&#20854;&#31163;&#25955;&#34920;&#31034;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#36825;&#19968;&#38382;&#39064;&#22312;&#24120;&#29992;&#30340;&#25805;&#20316;&#31526;&#23398;&#20064;&#25216;&#26415;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#35814;&#32454;&#35828;&#26126;&#20102;&#24403;&#22788;&#29702;&#19981;&#21516;&#30340;&#31163;&#25955;&#21270;&#21644;&#32593;&#26684;&#20197;&#21450;&#20851;&#38190;&#30340;&#36830;&#32493;&#32467;&#26500;&#26102;&#65292;&#21035;&#21517;&#20250;&#24341;&#20837;&#35823;&#24046;&#12290;&#26356;&#19968;&#33324;&#30340;&#35828;&#65292;&#36825;&#19968;&#26694;&#26550;&#19981;&#20165;&#33021;&#22815;&#25581;&#31034;&#31163;&#25955;&#21270;&#24341;&#20837;&#30340;&#38382;&#39064;&#65292;&#36824;&#33021;&#22815;&#20026;&#31070;&#32463;&#25805;&#20316;&#31526;&#30340;&#23398;&#20064;&#25552;&#20379;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, operator learning, or learning mappings between infinite-dimensional function spaces, has garnered significant attention, notably in relation to learning partial differential equations from data. Conceptually clear when outlined on paper, neural operators necessitate discretization in the transition to computer implementations. This step can compromise their integrity, often causing them to deviate from the underlying operators. This research offers a fresh take on neural operators with a framework Representation equivalent Neural Operators (ReNO) designed to address these issues. At its core is the concept of operator aliasing, which measures inconsistency between neural operators and their discrete representations. We explore this for widely-used operator learning techniques. Our findings detail how aliasing introduces errors when handling different discretizations and grids and loss of crucial continuous structures. More generally, this framework not only sheds light on ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31283;&#23450;&#21452;&#23618;&#20248;&#21270;&#20013;&#33258;&#21160;&#35843;&#25972;&#27493;&#38271;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#21464;&#20307;&#30340;&#25913;&#36827;&#29256;&#26412;&#26469;&#26367;&#20195;&#29616;&#26377;&#30340;&#31639;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#20351;&#29992;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;&#33258;&#36866;&#24212;&#27493;&#38271;&#26041;&#27861;&#36827;&#34892;&#35745;&#31639;&#65292;&#20197;&#32531;&#35299;&#35745;&#31639;&#36229;&#26799;&#24230;&#26102;&#21487;&#33021;&#24341;&#36215;&#30340;&#36817;&#20284;&#35823;&#24046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.18666</link><description>&lt;p&gt;
BiSLS/SPS&#65306;&#31283;&#23450;&#21452;&#23618;&#20248;&#21270;&#30340;&#33258;&#21160;&#35843;&#25972;&#27493;&#38271;
&lt;/p&gt;
&lt;p&gt;
BiSLS/SPS: Auto-tune Step Sizes for Stable Bi-level Optimization. (arXiv:2305.18666v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18666
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31283;&#23450;&#21452;&#23618;&#20248;&#21270;&#20013;&#33258;&#21160;&#35843;&#25972;&#27493;&#38271;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#21464;&#20307;&#30340;&#25913;&#36827;&#29256;&#26412;&#26469;&#26367;&#20195;&#29616;&#26377;&#30340;&#31639;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#20351;&#29992;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;&#33258;&#36866;&#24212;&#27493;&#38271;&#26041;&#27861;&#36827;&#34892;&#35745;&#31639;&#65292;&#20197;&#32531;&#35299;&#35745;&#31639;&#36229;&#26799;&#24230;&#26102;&#21487;&#33021;&#24341;&#36215;&#30340;&#36817;&#20284;&#35823;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21452;&#23618;&#20248;&#21270;&#65288;BO&#65289;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#27969;&#34892;&#24341;&#21457;&#20102;&#23545;&#22522;&#20110;&#26799;&#24230;&#30340;BO&#31639;&#27861;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#31639;&#27861;&#28041;&#21450;&#20004;&#20010;&#32806;&#21512;&#30340;&#23398;&#20064;&#29575;&#65292;&#24403;&#35745;&#31639;&#36229;&#26799;&#24230;&#26102;&#21487;&#33021;&#21463;&#21040;&#36817;&#20284;&#35823;&#24046;&#30340;&#24433;&#21709;&#65292;&#22240;&#27492;&#38656;&#35201;&#20180;&#32454;&#24494;&#35843;&#20197;&#30830;&#20445;&#24555;&#36895;&#25910;&#25947;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;&#33258;&#36866;&#24212;&#27493;&#38271;&#26041;&#27861;&#8212;&#8212;&#38543;&#26426;&#32447;&#25628;&#32034;&#65288;SLS&#65289;&#21644;&#38543;&#26426;Polyak&#27493;&#38271;&#65288;SPS&#65289;&#65292;&#29992;&#20110;&#35745;&#31639;&#19978;&#23618;&#21644;&#19979;&#23618;&#30340;&#23398;&#20064;&#29575;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#22312;&#27809;&#26377;&#36890;&#24120;&#22312;&#20808;&#21069;&#30740;&#31350;&#20013;&#20551;&#35774;&#30340;&#39069;&#22806;&#25554;&#20540;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#22312;&#21333;&#23618;&#20248;&#21270;&#20013;&#20351;&#29992;SLS&#21644;SPS&#30340;&#26041;&#27861;&#12290;&#23545;&#20110;&#36825;&#26679;&#30340;&#35774;&#32622;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;SLS&#21644;SPS&#30340;&#26032;&#21464;&#20307;&#65292;&#25913;&#36827;&#20102;&#25991;&#29486;&#20013;&#30340;&#29616;&#26377;&#24314;&#35758;&#65292;&#24182;&#19988;&#26356;&#23481;&#26131;&#23454;&#29616;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#36825;&#20004;&#20010;&#21464;&#20307;&#21487;&#20197;&#30475;&#20316;&#26159;&#19968;&#31867;&#20855;&#26377;&#21253;&#32476;&#22411;&#32467;&#26500;&#26041;&#27861;&#30340;&#29305;&#27530;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
The popularity of bi-level optimization (BO) in deep learning has spurred a growing interest in studying gradient-based BO algorithms. However, existing algorithms involve two coupled learning rates that can be affected by approximation errors when computing hypergradients, making careful fine-tuning necessary to ensure fast convergence. To alleviate this issue, we investigate the use of recently proposed adaptive step-size methods, namely stochastic line search (SLS) and stochastic Polyak step size (SPS), for computing both the upper and lower-level learning rates. First, we revisit the use of SLS and SPS in single-level optimization without the additional interpolation condition that is typically assumed in prior works. For such settings, we investigate new variants of SLS and SPS that improve upon existing suggestions in the literature and are simpler to implement. Importantly, these two variants can be seen as special instances of general family of methods with an envelope-type ste
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#29289;&#21697;&#27969;&#34892;&#24230;&#12289;&#36136;&#37327;&#21644;&#20301;&#32622;&#20559;&#24046;&#23545;&#29992;&#25143;&#31119;&#21033;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#36890;&#36807;&#25506;&#32034;&#20943;&#36731;&#27969;&#34892;&#24230;&#20559;&#35265;&#36127;&#38754;&#24433;&#21709;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.18333</link><description>&lt;p&gt;
&#20855;&#26377;&#27969;&#34892;&#24230;&#20559;&#35265;&#30340;&#25490;&#21517;&#65306;&#33258;&#22686;&#24378;&#21160;&#24577;&#19979;&#30340;&#29992;&#25143;&#31119;&#21033;
&lt;/p&gt;
&lt;p&gt;
Ranking with Popularity Bias: User Welfare under Self-Amplification Dynamics. (arXiv:2305.18333v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18333
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#29289;&#21697;&#27969;&#34892;&#24230;&#12289;&#36136;&#37327;&#21644;&#20301;&#32622;&#20559;&#24046;&#23545;&#29992;&#25143;&#31119;&#21033;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#36890;&#36807;&#25506;&#32034;&#20943;&#36731;&#27969;&#34892;&#24230;&#20559;&#35265;&#36127;&#38754;&#24433;&#21709;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#24050;&#32463;&#30830;&#35748;&#27969;&#34892;&#24230;&#20559;&#35265;&#22312;&#25512;&#33616;&#65288;&#21644;&#20854;&#20182;&#22522;&#20110;&#25490;&#21517;&#30340;&#65289;&#31995;&#32479;&#20013;&#21457;&#25381;&#20316;&#29992;&#65292;&#20294;&#20854;&#23545;&#29992;&#25143;&#31119;&#21033;&#30340;&#24433;&#21709;&#30340;&#35814;&#32454;&#20998;&#26512;&#20173;&#28982;&#32570;&#20047;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26426;&#21046;&#65292;&#36890;&#36807;&#23427;&#65292;&#29289;&#21697;&#30340;&#27969;&#34892;&#24230;&#12289;&#36136;&#37327;&#21644;&#20301;&#32622;&#20559;&#24046;&#21487;&#20197;&#24433;&#21709;&#29992;&#25143;&#36873;&#25321;&#65292;&#24182;&#19988;&#21487;&#20197;&#36127;&#38754;&#24433;&#21709;&#21508;&#31181;&#25512;&#33616;&#31574;&#30053;&#30340;&#38598;&#20307;&#29992;&#25143;&#25928;&#29992;&#12290;&#25105;&#20204;&#23558;&#38382;&#39064;&#34920;&#36848;&#20026;&#38750;&#24179;&#31283;&#19978;&#19979;&#25991;&#33073;&#38774;&#26426;&#65292;&#24378;&#35843;&#19981;&#26159;&#20026;&#20102;&#28040;&#38500;&#27969;&#34892;&#24230;&#20559;&#35265;&#32780;&#26159;&#20026;&#20102;&#20943;&#36731;&#20854;&#36127;&#38754;&#24433;&#21709;&#32780;&#36827;&#34892;&#25506;&#32034;&#30340;&#37325;&#35201;&#24615;&#12290;&#39318;&#20808;&#65292;&#26222;&#36890;&#30340;&#26377;&#27969;&#34892;&#24230;&#20559;&#24046;&#30340;&#25512;&#33616;&#31995;&#32479;&#20250;&#36890;&#36807;&#28151;&#28102;&#29289;&#21697;&#36136;&#37327;&#21644;&#27969;&#34892;&#24230;&#32780;&#24341;&#21457;&#32447;&#24615;&#36951;&#25022;&#12290;&#26356;&#19968;&#33324;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#22312;&#32447;&#24615;&#35774;&#32622;&#19979;&#65292;&#30001;&#20110;&#27969;&#34892;&#24230;&#20559;&#35265;&#30340;&#28151;&#28102;&#25928;&#24212;&#65292;&#29289;&#21697;&#36136;&#37327;&#30340;&#21487;&#35782;&#21035;&#24615;&#20063;&#21487;&#33021;&#26080;&#27861;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#22312;&#36275;&#22815;&#21464;&#24322;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31867;UCB&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#26377;&#25928;&#30340;&#36951;&#25022;&#20445;&#35777;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35777;&#23454;&#20102;&#27969;&#34892;&#24230;&#20559;&#35265;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
While popularity bias is recognized to play a role in recommmender (and other ranking-based) systems, detailed analyses of its impact on user welfare have largely been lacking. We propose a general mechanism by which item popularity, item quality, and position bias can impact user choice, and how it can negatively impact the collective user utility of various recommender policies. Formulating the problem as a non-stationary contextual bandit, we highlight the importance of exploration, not to eliminate popularity bias, but to mitigate its negative effects. First, naive popularity-biased recommenders are shown to induce linear regret by conflating item quality and popularity. More generally, we show that, even in linear settings, identifiability of item quality may not be possible due to the confounding effects of popularity bias. However, under sufficient variability assumptions, we develop an efficient UCB-style algorithm and prove efficient regret guarantees. We complement our analys
&lt;/p&gt;</description></item><item><title>Diable&#26159;&#19968;&#20010;&#39640;&#25928;&#30340;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#31995;&#32479;&#65292;&#23427;&#36890;&#36807;&#22312;&#34920;&#26684;&#19978;&#36827;&#34892;&#25805;&#20316;&#26469;&#26356;&#26032;&#23545;&#35805;&#29366;&#24577;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#26102;&#38388;&#25928;&#29575;&#25552;&#39640;&#20102;2.4&#20493;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#31454;&#20105;&#24615;&#30340;&#30446;&#26631;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.17020</link><description>&lt;p&gt;
Diable: &#22312;&#34920;&#26684;&#19978;&#36827;&#34892;&#30340;&#39640;&#25928;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
Diable: Efficient Dialogue State Tracking as Operations on Tables. (arXiv:2305.17020v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17020
&lt;/p&gt;
&lt;p&gt;
Diable&#26159;&#19968;&#20010;&#39640;&#25928;&#30340;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#31995;&#32479;&#65292;&#23427;&#36890;&#36807;&#22312;&#34920;&#26684;&#19978;&#36827;&#34892;&#25805;&#20316;&#26469;&#26356;&#26032;&#23545;&#35805;&#29366;&#24577;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#26102;&#38388;&#25928;&#29575;&#25552;&#39640;&#20102;2.4&#20493;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#31454;&#20105;&#24615;&#30340;&#30446;&#26631;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#31995;&#32479;&#23558;&#23436;&#25972;&#30340;&#23545;&#35805;&#21382;&#21490;&#20316;&#20026;&#36755;&#20837;&#65292;&#23558;&#24403;&#21069;&#29366;&#24577;&#34920;&#31034;&#20026;&#21253;&#21547;&#25152;&#26377;&#27133;&#30340;&#21015;&#34920;&#65292;&#24182;&#22312;&#27599;&#20010;&#23545;&#35805;&#22238;&#21512;&#20013;&#20174;&#22836;&#24320;&#22987;&#29983;&#25104;&#25972;&#20010;&#29366;&#24577;&#12290;&#36825;&#31181;&#26041;&#27861;&#25928;&#29575;&#20302;&#19979;&#65292;&#29305;&#21035;&#26159;&#24403;&#27133;&#30340;&#25968;&#37327;&#24456;&#22810;&#19988;&#23545;&#35805;&#24456;&#38271;&#26102;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Diable&#65292;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#24418;&#24335;&#21270;&#26041;&#27861;&#65292;&#31616;&#21270;&#20102;&#39640;&#25928;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#31995;&#32479;&#30340;&#35774;&#35745;&#21644;&#23454;&#29616;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#22320;&#23884;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#23545;&#35805;&#29366;&#24577;&#34920;&#31034;&#20026;&#34920;&#26684;&#65292;&#24182;&#23558;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#24418;&#24335;&#21270;&#20026;&#34920;&#26684;&#25805;&#20316;&#20219;&#21153;&#12290;&#22312;&#27599;&#20010;&#22238;&#21512;&#20013;&#65292;&#31995;&#32479;&#36890;&#36807;&#22522;&#20110;&#23545;&#35805;&#19978;&#19979;&#25991;&#29983;&#25104;&#34920;&#26684;&#25805;&#20316;&#26469;&#26356;&#26032;&#20808;&#21069;&#30340;&#29366;&#24577;&#12290;&#22312;MultiWoz&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#65292;Diable (i) &#20248;&#20110;&#24378;&#22823;&#30340;&#39640;&#25928;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#22522;&#20934;&#65292;(ii) &#26102;&#38388;&#25928;&#29575;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;2.4&#20493;&#65292;&#21516;&#26102;&#20445;&#25345;&#31454;&#20105;&#24615;&#30340;&#32852;&#21512;&#30446;&#26631;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;(iii) &#23545;&#26080;&#22122;&#22768;&#30340;&#36755;&#20837;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequence-to-sequence state-of-the-art systems for dialogue state tracking (DST) use the full dialogue history as input, represent the current state as a list with all the slots, and generate the entire state from scratch at each dialogue turn. This approach is inefficient, especially when the number of slots is large and the conversation is long. We propose Diable, a new task formalisation that simplifies the design and implementation of efficient DST systems and allows one to easily plug and play large language models. We represent the dialogue state as a table and formalise DST as a table manipulation task. At each turn, the system updates the previous state by generating table operations based on the dialogue context. Extensive experimentation on the MultiWoz datasets demonstrates that Diable (i) outperforms strong efficient DST baselines, (ii) is 2.4x more time efficient than current state-of-the-art methods while retaining competitive Joint Goal Accuracy, and (iii) is robust to no
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36125;&#21494;&#26031;&#26412;&#22320;&#20248;&#21270;&#31574;&#30053;&#30340;&#34892;&#20026;&#21644;&#25910;&#25947;&#24615;&#65292;&#24182;&#22312;&#39640;&#32500;&#38382;&#39064;&#19978;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#23454;&#35777;&#24615;&#33021;&#12290;&#32479;&#35745;&#25968;&#25454;&#34920;&#26126;&#65292;&#21333;&#20010;&#39640;&#26031;&#36807;&#31243;&#26679;&#26412;&#36335;&#24452;&#30340;&#26412;&#22320;&#35299;&#27604;&#20840;&#23616;&#26041;&#27861;&#24674;&#22797;&#30340;&#39044;&#26399;&#20540;&#26356;&#22909;&#12290;M&#252;ller&#31561;&#20154;&#25552;&#20986;&#30340;&#36125;&#21494;&#26031;&#26412;&#22320;&#20248;&#21270;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#29575;&#22312;&#26377;&#22122;&#38899;&#21644;&#26080;&#22122;&#38899;&#30340;&#24773;&#20917;&#19979;&#37117;&#26377;&#25512;&#23548;&#12290;</title><link>http://arxiv.org/abs/2305.15572</link><description>&lt;p&gt;
&#26412;&#22320;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#34892;&#20026;&#21644;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Behavior and Convergence of Local Bayesian Optimization. (arXiv:2305.15572v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15572
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36125;&#21494;&#26031;&#26412;&#22320;&#20248;&#21270;&#31574;&#30053;&#30340;&#34892;&#20026;&#21644;&#25910;&#25947;&#24615;&#65292;&#24182;&#22312;&#39640;&#32500;&#38382;&#39064;&#19978;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#23454;&#35777;&#24615;&#33021;&#12290;&#32479;&#35745;&#25968;&#25454;&#34920;&#26126;&#65292;&#21333;&#20010;&#39640;&#26031;&#36807;&#31243;&#26679;&#26412;&#36335;&#24452;&#30340;&#26412;&#22320;&#35299;&#27604;&#20840;&#23616;&#26041;&#27861;&#24674;&#22797;&#30340;&#39044;&#26399;&#20540;&#26356;&#22909;&#12290;M&#252;ller&#31561;&#20154;&#25552;&#20986;&#30340;&#36125;&#21494;&#26031;&#26412;&#22320;&#20248;&#21270;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#29575;&#22312;&#26377;&#22122;&#38899;&#21644;&#26080;&#22122;&#38899;&#30340;&#24773;&#20917;&#19979;&#37117;&#26377;&#25512;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#19968;&#39033;&#26368;&#26032;&#30340;&#21457;&#23637;&#26159;&#20351;&#29992;&#26412;&#22320;&#20248;&#21270;&#31574;&#30053;&#65292;&#19982;&#20256;&#32479;&#30340;&#20840;&#23616;&#31574;&#30053;&#30456;&#27604;&#65292;&#21487;&#20197;&#22312;&#39640;&#32500;&#38382;&#39064;&#19978;&#25552;&#20379;&#24378;&#22823;&#30340;&#23454;&#35777;&#24615;&#33021;&#12290;&#25991;&#29486;&#20013;&#30340;&#8220;&#20256;&#32479;&#26234;&#24935;&#8221;&#26159;&#65292;&#19987;&#27880;&#20110;&#26412;&#22320;&#20248;&#21270;&#35268;&#36991;&#20102;&#32500;&#24230;&#35781;&#21650;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#36125;&#21494;&#26031;&#26412;&#22320;&#20248;&#21270;&#20363;&#31243;&#30340;&#39044;&#26399;&#34892;&#20026;&#25110;&#25910;&#25947;&#24615;&#20102;&#35299;&#29978;&#23569;&#12290;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#26412;&#22320;&#26041;&#27861;&#30340;&#34892;&#20026;&#65292;&#24182;&#21457;&#29616;&#39640;&#26031;&#36807;&#31243;&#26679;&#26412;&#36335;&#24452;&#21333;&#20010;&#26412;&#22320;&#35299;&#30340;&#32479;&#35745;&#25968;&#25454;&#19982;&#20174;&#20840;&#23616;&#26041;&#27861;&#24674;&#22797;&#30340;&#39044;&#26399;&#20540;&#30456;&#27604;&#38750;&#24120;&#22909;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26368;&#36817;&#30001;M&#252;ller&#31561;&#20154;&#25552;&#20986;&#30340;&#22522;&#20110;&#36125;&#21494;&#26031;&#26412;&#22320;&#20248;&#21270;&#31639;&#27861;&#30340;&#31532;&#19968;&#27425;&#20005;&#26684;&#20998;&#26512;&#65292;&#24182;&#22312;&#26377;&#22122;&#38899;&#21644;&#26080;&#22122;&#38899;&#30340;&#24773;&#20917;&#19979;&#25512;&#23548;&#20986;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
A recent development in Bayesian optimization is the use of local optimization strategies, which can deliver strong empirical performance on high-dimensional problems compared to traditional global strategies. The "folk wisdom" in the literature is that the focus on local optimization sidesteps the curse of dimensionality; however, little is known concretely about the expected behavior or convergence of Bayesian local optimization routines. We first study the behavior of the local approach, and find that the statistics of individual local solutions of Gaussian process sample paths are surprisingly good compared to what we would expect to recover from global methods. We then present the first rigorous analysis of such a Bayesian local optimization algorithm recently proposed by M\"uller et al. (2021), and derive convergence rates in both the noisy and noiseless settings.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#25674;&#38144;&#25104;&#26412;&#20272;&#35745;&#26041;&#27861;&#65292;&#33021;&#22815;&#35299;&#20915;&#24191;&#20041;&#36125;&#21494;&#26031;&#25512;&#29702;&#26041;&#27861;&#20013;&#22810;&#20010;&#27169;&#25311;&#30340;&#35745;&#31639;&#38382;&#39064;&#65292;&#20174;&#32780;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#20102;&#19968;&#31181;&#22788;&#29702;&#39640;&#32500;&#24230;&#12289;&#22797;&#26434;&#22797;&#29616;&#65292;&#19988;&#36125;&#21494;&#26031;&#21518;&#39564;&#26410;&#24517;&#26159;&#26368;&#20339;&#26041;&#26696;&#30340;&#31185;&#23398;&#27169;&#25311;&#22120;&#30340;&#20248;&#21270;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.15208</link><description>&lt;p&gt;
&#24191;&#20041;&#36125;&#21494;&#26031;&#25512;&#29702;&#26041;&#27861;&#65306;&#36890;&#36807;&#25674;&#38144;&#25104;&#26412;&#35780;&#20272;&#20026;&#31185;&#23398;&#27169;&#25311;&#22120;&#25552;&#20379;&#36125;&#21494;&#26031;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Generalized Bayesian Inference for Scientific Simulators via Amortized Cost Estimation. (arXiv:2305.15208v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15208
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#25674;&#38144;&#25104;&#26412;&#20272;&#35745;&#26041;&#27861;&#65292;&#33021;&#22815;&#35299;&#20915;&#24191;&#20041;&#36125;&#21494;&#26031;&#25512;&#29702;&#26041;&#27861;&#20013;&#22810;&#20010;&#27169;&#25311;&#30340;&#35745;&#31639;&#38382;&#39064;&#65292;&#20174;&#32780;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#20102;&#19968;&#31181;&#22788;&#29702;&#39640;&#32500;&#24230;&#12289;&#22797;&#26434;&#22797;&#29616;&#65292;&#19988;&#36125;&#21494;&#26031;&#21518;&#39564;&#26410;&#24517;&#26159;&#26368;&#20339;&#26041;&#26696;&#30340;&#31185;&#23398;&#27169;&#25311;&#22120;&#30340;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27169;&#25311;&#30340;&#25512;&#29702;&#26041;&#27861;(SBI)&#36890;&#36807;&#20869;&#21547;&#30340;&#21487;&#33021;&#24615;&#65292;&#20026;&#27169;&#25311;&#22120;&#25552;&#20379;&#25674;&#38144;&#24335;&#30340;&#36125;&#21494;&#26031;&#25512;&#29702;&#12290;&#20294;&#26159;&#24403;&#25105;&#20204;&#20027;&#35201;&#20851;&#27880;&#30340;&#26159;&#39044;&#27979;&#27169;&#25311;&#30340;&#36136;&#37327;&#65292;&#25110;&#32773;&#24403;&#27169;&#22411;&#19981;&#33021;&#23436;&#20840;&#37325;&#29616;&#35266;&#27979;&#25968;&#25454;(&#21363;&#23384;&#22312;&#32570;&#38519;)&#65292;&#20197;&#36125;&#21494;&#26031;&#21518;&#39564;&#20026;&#30446;&#26631;&#23601;&#21487;&#33021;&#36807;&#20110;&#20005;&#26684;&#12290;&#24191;&#20041;&#36125;&#21494;&#26031;&#25512;&#29702;(GBI)&#26088;&#22312;&#21152;&#24378;&#23545;(&#26377;&#32570;&#38519;&#30340;)&#27169;&#25311;&#22120;&#27169;&#22411;&#30340;&#25512;&#29702;&#65292;&#29992;&#35780;&#20272;&#21442;&#25968;&#30456;&#23545;&#20110;&#25968;&#25454;&#30340;&#22909;&#22351;&#30340;&#25104;&#26412;&#20989;&#25968;&#26367;&#25442;&#20284;&#28982;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;GBI&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#36816;&#34892;&#22810;&#20010;&#27169;&#25311;&#65292;&#20197;&#22312;&#25512;&#29702;&#26399;&#38388;&#20272;&#35745;&#27599;&#20010;&#21442;&#25968;&#20540;&#30340;&#25104;&#26412;&#20989;&#25968;&#65292;&#20351;&#24471;&#21363;&#20351;&#22312;&#20013;&#31561;&#22797;&#26434;&#30340;&#27169;&#25311;&#31243;&#24207;&#20013;&#20063;&#38590;&#20197;&#35745;&#31639;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#25674;&#38144;&#25104;&#26412;&#35780;&#20272;(ACE)&#30340;&#24191;&#20041;&#36125;&#21494;&#26031;&#25512;&#29702;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65306;&#25105;&#20204;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26469;&#36817;&#20284;&#25104;&#26412;&#20989;&#25968;&#65292;&#23558;&#25104;&#26412;&#20989;&#25968;&#23450;&#20041;&#20026;&#30001;&#28508;&#22312;&#21442;&#25968;&#29983;&#25104;&#30340;&#27169;&#25311;&#20043;&#38388;&#30340;&#26399;&#26395;&#36317;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simulation-based inference (SBI) enables amortized Bayesian inference for simulators with implicit likelihoods. But when we are primarily interested in the quality of predictive simulations, or when the model cannot exactly reproduce the observed data (i.e., is misspecified), targeting the Bayesian posterior may be overly restrictive. Generalized Bayesian Inference (GBI) aims to robustify inference for (misspecified) simulator models, replacing the likelihood-function with a cost function that evaluates the goodness of parameters relative to data. However, GBI methods generally require running multiple simulations to estimate the cost function at each parameter value during inference, making the approach computationally infeasible for even moderately complex simulators. Here, we propose amortized cost estimation (ACE) for GBI to address this challenge: We train a neural network to approximate the cost function, which we define as the expected distance between simulations produced by a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25512;&#24191;&#20102;&#37325;&#35201;&#24615;&#21152;&#26435;&#26041;&#27861;&#65292;&#23558;&#20854;&#21464;&#25104;&#20102;&#36866;&#29992;&#20110;&#25152;&#26377;&#24773;&#20917;&#30340;&#36890;&#29992;&#27714;&#35299;&#22120;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#21253;&#25324;&#37096;&#20998;&#37325;&#21472;&#24773;&#20917;&#22312;&#20869;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.14690</link><description>&lt;p&gt;
&#23558;&#37325;&#35201;&#24615;&#21152;&#26435;&#26041;&#27861;&#25512;&#24191;&#20026;&#29992;&#20110;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#30340;&#36890;&#29992;&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
Generalizing Importance Weighting to A Universal Solver for Distribution Shift Problems. (arXiv:2305.14690v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14690
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25512;&#24191;&#20102;&#37325;&#35201;&#24615;&#21152;&#26435;&#26041;&#27861;&#65292;&#23558;&#20854;&#21464;&#25104;&#20102;&#36866;&#29992;&#20110;&#25152;&#26377;&#24773;&#20917;&#30340;&#36890;&#29992;&#27714;&#35299;&#22120;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#21253;&#25324;&#37096;&#20998;&#37325;&#21472;&#24773;&#20917;&#22312;&#20869;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#20559;&#31227;(DS) &#21487;&#20197;&#26377;&#20004;&#20010;&#23618;&#38754;&#65306;&#20998;&#24067;&#26412;&#36523;&#21457;&#29983;&#21464;&#21270;&#65292;&#25903;&#25345;&#65288;&#21363;&#27010;&#29575;&#23494;&#24230;&#38750;&#38646;&#30340;&#38598;&#21512;&#65289;&#20063;&#21457;&#29983;&#21464;&#21270;&#12290;&#24403;&#32771;&#34385;&#35757;&#32451;&#21644;&#27979;&#35797;&#20998;&#24067;&#20043;&#38388;&#30340;&#25903;&#25345;&#21464;&#21270;&#26102;&#65292;&#26377;&#22235;&#31181;&#24773;&#20917;&#65306;&#65288;i&#65289;&#23427;&#20204;&#23436;&#20840;&#21305;&#37197;&#65307;&#65288;ii&#65289;&#35757;&#32451;&#25903;&#25345;&#33539;&#22260;&#26356;&#24191;&#65288;&#22240;&#27492;&#35206;&#30422;&#27979;&#35797;&#25903;&#25345;&#65289;&#65307;&#65288;iii&#65289;&#27979;&#35797;&#25903;&#25345;&#33539;&#22260;&#26356;&#24191;&#65307;&#65288;iv&#65289;&#23427;&#20204;&#37096;&#20998;&#37325;&#21472;&#12290;&#29616;&#26377;&#26041;&#27861;&#23545;&#24773;&#20917;&#65288;i&#65289;&#21644;&#65288;ii&#65289;&#24456;&#26377;&#25928;&#65292;&#32780;&#24773;&#20917;&#65288;iii&#65289;&#21644;&#65288;iv&#65289;&#29616;&#22312;&#26356;&#24120;&#35265;&#65292;&#20294;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#23558;&#37325;&#35201;&#24615;&#21152;&#26435;&#65288;IW&#65289;&#26041;&#27861;&#65288;&#29992;&#20110;&#24773;&#20917;&#65288;i&#65289;&#21644;&#65288;ii&#65289;&#65289;&#25512;&#24191;&#20026;&#36866;&#29992;&#20110;&#25152;&#26377;&#24773;&#20917;&#30340;&#36890;&#29992;&#27714;&#35299;&#22120;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;IW&#22312;&#24773;&#20917;&#65288;iii&#65289;&#21644;&#65288;iv&#65289;&#20013;&#21487;&#33021;&#22833;&#36133;&#30340;&#21407;&#22240;&#65307;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#27867;&#21270;&#37325;&#35201;&#24615;&#21152;&#26435;&#65288;GIW&#65289;&#65292;&#23427;&#21487;&#20197;&#22788;&#29702;&#24773;&#20917;&#65288;iii&#65289;&#21644;&#65288;iv&#65289;&#65292;&#24182;&#19988;&#22312;&#24773;&#20917;&#65288;i&#65289;&#21644;&#65288;ii&#65289;&#20013;&#23558;&#20943;&#23569;&#20026;IW&#26041;&#27861;&#12290;&#22312;GIW&#20013;&#65292;&#23558;&#27979;&#35797;&#25903;&#25345;&#21010;&#20998;&#20026;&#35757;&#32451;&#20869;&#37096;&#65288;IT&#65289;&#37096;&#20998;&#21644;&#35757;&#32451;&#22806;&#37096;&#65288;OOT&#65289;&#37096;&#20998;&#65292;&#24182;&#20026;&#23427;&#20204;&#20998;&#37197;&#19981;&#21516;&#30340;&#26435;&#37325;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#28201;&#21644;&#20551;&#35774;&#19979;&#65292;GIW&#23545;&#25152;&#26377;&#22235;&#31181;&#24773;&#20917;&#37117;&#25910;&#25947;&#21040;&#26368;&#20248;&#35299;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;GIW&#30830;&#23454;&#22312;&#25152;&#26377;&#22235;&#31181;&#24773;&#20917;&#19979;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distribution shift (DS) may have two levels: the distribution itself changes, and the support (i.e., the set where the probability density is non-zero) also changes. When considering the support change between the training and test distributions, there can be four cases: (i) they exactly match; (ii) the training support is wider (and thus covers the test support); (iii) the test support is wider; (iv) they partially overlap. Existing methods are good at cases (i) and (ii), while cases (iii) and (iv) are more common nowadays but still under-explored. In this paper, we generalize importance weighting (IW), a golden solver for cases (i) and (ii), to a universal solver for all cases. Specifically, we first investigate why IW may fail in cases (iii) and (iv); based on the findings, we propose generalized IW (GIW) that could handle cases (iii) and (iv) and would reduce to IW in cases (i) and (ii). In GIW, the test support is split into an in-training (IT) part and an out-of-training (OOT) pa
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27169;&#22411;&#35774;&#35745;&#36873;&#25321;&#21644;&#25968;&#25454;&#38598;&#35268;&#27169;&#30340;&#32463;&#39564;&#24615;&#20998;&#26512;&#65292;&#26500;&#24314;&#20102;&#21442;&#25968;&#25968;&#37327;&#21644;&#35757;&#32451;&#25968;&#25454;&#26368;&#22810;&#30340;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#20010;Spoken&#29256;&#26412;&#30340;&#25991;&#26412;&#22522;&#20934;&#65292;&#20197;&#36827;&#19968;&#27493;&#25913;&#21892;&#27169;&#22411;&#35780;&#20272;&#21644;&#25512;&#21160;&#26410;&#26469;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2305.13009</link><description>&lt;p&gt;
&#25991;&#26412;&#39044;&#35757;&#32451;&#30340;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Textually Pretrained Speech Language Models. (arXiv:2305.13009v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13009
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27169;&#22411;&#35774;&#35745;&#36873;&#25321;&#21644;&#25968;&#25454;&#38598;&#35268;&#27169;&#30340;&#32463;&#39564;&#24615;&#20998;&#26512;&#65292;&#26500;&#24314;&#20102;&#21442;&#25968;&#25968;&#37327;&#21644;&#35757;&#32451;&#25968;&#25454;&#26368;&#22810;&#30340;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#20010;Spoken&#29256;&#26412;&#30340;&#25991;&#26412;&#22522;&#20934;&#65292;&#20197;&#36827;&#19968;&#27493;&#25913;&#21892;&#27169;&#22411;&#35780;&#20272;&#21644;&#25512;&#21160;&#26410;&#26469;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#65288;SpeechLMs&#65289;&#20165;&#22788;&#29702;&#21644;&#29983;&#25104;&#38899;&#39057;&#25968;&#25454;&#65292;&#27809;&#26377;&#25991;&#23383;&#30417;&#30563;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TWIST&#65292;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;SpeechLMs&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#34920;&#26126;&#65292;TWIST&#22312;&#21508;&#20010;&#26041;&#38754;&#37117;&#20248;&#20110;&#20919;&#21551;&#21160;&#30340;SpeechLM&#12290;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#20998;&#26512;&#20102;&#19981;&#21516;&#30340;&#27169;&#22411;&#35774;&#35745;&#36873;&#25321;&#65288;&#22914;&#35821;&#38899;&#20998;&#35789;&#22120;&#12289;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#22823;&#23567;&#65289;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#35268;&#27169;&#22312;&#26500;&#24314;&#24615;&#33021;&#26356;&#22909;&#30340;SpeechLMs&#26041;&#38754;&#37117;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#35266;&#23519;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#36804;&#20170;&#20026;&#27490;&#21442;&#25968;&#25968;&#37327;&#21644;&#35757;&#32451;&#25968;&#25454;&#26368;&#22810;&#30340;SpeechLM&#65288;&#25454;&#25105;&#20204;&#25152;&#30693;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#20004;&#20010;Spoken&#29256;&#26412;&#30340;StoryCloze&#25991;&#26412;&#22522;&#20934;&#65292;&#20197;&#36827;&#19968;&#27493;&#25913;&#21892;&#27169;&#22411;&#35780;&#20272;&#24182;&#25512;&#21160;&#35813;&#39046;&#22495;&#30340;&#26410;&#26469;&#30740;&#31350;&#12290;&#25105;&#20204;&#20844;&#24320;&#25552;&#20379;&#35821;&#38899;&#26679;&#26412;&#12289;&#20195;&#30721;&#21644;&#27169;&#22411;&#65306;https://pages.cs.huji.ac.il/
&lt;/p&gt;
&lt;p&gt;
Speech language models (SpeechLMs) process and generate acoustic data only, without textual supervision. In this work, we propose TWIST, a method for training SpeechLMs using a warm-start from a pretrained textual language models. We show using both automatic and human evaluations that TWIST outperforms a cold-start SpeechLM across the board. We empirically analyze the effect of different model design choices such as the speech tokenizer, the pretrained textual model, and the dataset size. We find that model and dataset scale both play an important role in constructing better-performing SpeechLMs. Based on our observations, we present the largest (to the best of our knowledge) SpeechLM both in terms of number of parameters and training data. We additionally introduce two spoken versions of the StoryCloze textual benchmark to further improve model evaluation and advance future research in the field. We make speech samples, code and models publicly available: https://pages.cs.huji.ac.il/
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#23384;&#20648;&#38480;&#21046;&#30340;&#22312;&#32447;&#25345;&#32493;&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#29992;kNN&#20998;&#31867;&#22120;&#21644;&#36890;&#29992;&#39044;&#35757;&#32451;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#22312;&#23567;&#31639;&#21147;&#30340;&#24773;&#20917;&#19979;&#32039;&#20945;&#22320;&#23384;&#20648;&#21644;&#21033;&#29992;&#25972;&#20010;&#36755;&#20837;&#25968;&#25454;&#27969;&#65292;&#24182;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.09253</link><description>&lt;p&gt;
&#26080;&#23384;&#20648;&#38480;&#21046;&#30340;&#22312;&#32447;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Online Continual Learning Without the Storage Constraint. (arXiv:2305.09253v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09253
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#23384;&#20648;&#38480;&#21046;&#30340;&#22312;&#32447;&#25345;&#32493;&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#29992;kNN&#20998;&#31867;&#22120;&#21644;&#36890;&#29992;&#39044;&#35757;&#32451;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#22312;&#23567;&#31639;&#21147;&#30340;&#24773;&#20917;&#19979;&#32039;&#20945;&#22320;&#23384;&#20648;&#21644;&#21033;&#29992;&#25972;&#20010;&#36755;&#20837;&#25968;&#25454;&#27969;&#65292;&#24182;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#25345;&#32493;&#23398;&#20064;&#65288;OCL&#65289;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#36890;&#36807;&#22266;&#23450;&#21644;&#26377;&#38480;&#30340;&#23384;&#20648;&#20998;&#37197;&#26469;&#20943;&#36731;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#23384;&#20648;&#30340;&#21487;&#36127;&#25285;&#24615;&#25552;&#39640;&#20102;&#19968;&#31995;&#21015;&#19981;&#31526;&#21512;&#36825;&#20123;&#20551;&#35774;&#30340;&#24212;&#29992;&#12290;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#65292;&#20027;&#35201;&#20851;&#27880;&#28857;&#22312;&#20110;&#31649;&#29702;&#35745;&#31639;&#25903;&#20986;&#32780;&#19981;&#26159;&#23384;&#20648;&#12290;&#26412;&#25991;&#38024;&#23545;&#36825;&#31181;&#24773;&#20917;&#65292;&#36890;&#36807;&#25918;&#23485;&#23384;&#20648;&#38480;&#21046;&#24182;&#24378;&#35843;&#22266;&#23450;&#30340;&#65292;&#26377;&#38480;&#30340;&#32463;&#27982;&#39044;&#31639;&#65292;&#30740;&#31350;&#22312;&#32447;&#25345;&#32493;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#20351;&#29992;kNN&#20998;&#31867;&#22120;&#21644;&#36890;&#29992;&#39044;&#35757;&#32451;&#29305;&#24449;&#25552;&#21462;&#22120;&#22312;&#24494;&#23567;&#30340;&#35745;&#31639;&#39044;&#31639;&#19979;&#32039;&#20945;&#22320;&#23384;&#20648;&#21644;&#21033;&#29992;&#25972;&#20010;&#36755;&#20837;&#25968;&#25454;&#27969;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#24120;&#24577;&#21270;&#23398;&#20064;&#26377;&#21560;&#24341;&#21147;&#30340;&#19968;&#33268;&#24615;&#23646;&#24615;&#65306;&#23427;&#27704;&#36828;&#19981;&#20250;&#24536;&#35760;&#36807;&#21435;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#22823;&#35268;&#27169;&#30340;OCL&#25968;&#25454;&#38598;&#19978;&#35774;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#29366;&#24577;&#65306;&#36830;&#32493;&#26412;&#22320;&#21270;&#65288;CL&#65289;&#21644;&#21487;&#25345;&#32493;&#30340;&#23545;&#35937;&#35782;&#21035;&#65288;SOR&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online continual learning (OCL) research has primarily focused on mitigating catastrophic forgetting with fixed and limited storage allocation throughout the agent's lifetime. However, the growing affordability of data storage highlights a broad range of applications that do not adhere to these assumptions. In these cases, the primary concern lies in managing computational expenditures rather than storage. In this paper, we target such settings, investigating the online continual learning problem by relaxing storage constraints and emphasizing fixed, limited economical budget. We provide a simple algorithm that can compactly store and utilize the entirety of the incoming data stream under tiny computational budgets using a kNN classifier and universal pre-trained feature extractors. Our algorithm provides a consistency property attractive to continual learning: It will never forget past seen data. We set a new state of the art on two large-scale OCL datasets: Continual LOCalization (CL
&lt;/p&gt;</description></item><item><title>DCASE 2023 &#25361;&#25112;&#20219;&#21153;2&#26088;&#22312;&#35299;&#20915;&#26426;&#22120;&#29366;&#24577;&#30417;&#27979;&#20013;&#65292;&#37096;&#32626;&#26032;&#22411;&#26426;&#22120;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#22768;&#38899;&#26816;&#27979;&#65292;&#20165;&#20351;&#29992;&#26497;&#23569;&#37327;&#30340;&#27491;&#24120;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#19988;&#26080;&#38656;&#36229;&#21442;&#25968;&#35843;&#25972;&#12290;</title><link>http://arxiv.org/abs/2305.07828</link><description>&lt;p&gt;
DCASE 2023 &#25361;&#25112;&#20219;&#21153;2&#65306;&#38754;&#21521;&#26426;&#22120;&#29366;&#24577;&#30417;&#27979;&#30340;&#39318;&#27425;&#26080;&#30417;&#30563;&#24322;&#24120;&#22768;&#38899;&#26816;&#27979;&#30340;&#25551;&#36848;&#19982;&#35752;&#35770;(arXiv:2305.07828v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
Description and Discussion on DCASE 2023 Challenge Task 2: First-Shot Unsupervised Anomalous Sound Detection for Machine Condition Monitoring. (arXiv:2305.07828v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07828
&lt;/p&gt;
&lt;p&gt;
DCASE 2023 &#25361;&#25112;&#20219;&#21153;2&#26088;&#22312;&#35299;&#20915;&#26426;&#22120;&#29366;&#24577;&#30417;&#27979;&#20013;&#65292;&#37096;&#32626;&#26032;&#22411;&#26426;&#22120;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#22768;&#38899;&#26816;&#27979;&#65292;&#20165;&#20351;&#29992;&#26497;&#23569;&#37327;&#30340;&#27491;&#24120;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#19988;&#26080;&#38656;&#36229;&#21442;&#25968;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Detection and Classification of Acoustic Scenes and Events (DCASE) 2023 &#25361;&#25112;&#20219;&#21153; 2:&#8220;&#38754;&#21521;&#26426;&#22120;&#29366;&#24577;&#30417;&#27979;&#30340;&#39318;&#27425;&#26080;&#30417;&#30563;&#24322;&#24120;&#22768;&#38899;&#26816;&#27979;&#8221;&#12290;&#35813;&#20219;&#21153;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#20165;&#20351;&#29992;&#23569;&#25968;&#27491;&#24120;&#26679;&#26412;&#23601;&#33021;&#24555;&#36895;&#37096;&#32626;&#38024;&#23545;&#26032;&#22411;&#26426;&#22120;&#30340; ASD &#31995;&#32479;&#65292;&#26080;&#38656;&#36229;&#21442;&#25968;&#35843;&#25972;&#12290;&#22312;&#36807;&#21435;&#30340; ASD &#20219;&#21153;&#20013;&#65292;&#21457;&#23637;&#30340;&#26041;&#27861;&#20026;&#27599;&#31181;&#26426;&#22120;&#31867;&#22411;&#35843;&#25972;&#36229;&#21442;&#25968;&#65292;&#22240;&#20026;&#21457;&#23637;&#21644;&#35780;&#20272;&#25968;&#25454;&#38598;&#20855;&#26377;&#30456;&#21516;&#30340;&#26426;&#22120;&#31867;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#25910;&#38598;&#27491;&#24120;&#21644;&#24322;&#24120;&#25968;&#25454;&#38598;&#21487;&#33021;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#22312; 2023 Task 2 &#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#35299;&#20915;&#39318;&#27425;&#38382;&#39064;&#65292;&#21363;&#22312;&#23436;&#20840;&#26032;&#22411;&#30340;&#26426;&#22120;&#31867;&#22411;&#30340;&#23569;&#25968;&#26426;&#22120;&#19978;&#35757;&#32451;&#27169;&#22411;&#30340;&#25361;&#25112;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#65288;i&#65289;&#27599;&#31181;&#26426;&#22120;&#31867;&#22411;&#20165;&#26377;&#19968;&#20010;&#37096;&#20998;&#65292;&#65288;ii&#65289;&#21457;&#23637;&#21644;&#35780;&#20272;&#25968;&#25454;&#38598;&#20013;&#30340;&#26426;&#22120;&#31867;&#22411;&#23436;&#20840;&#19981;&#21516;&#12290;&#25105;&#20204;&#23558;&#28155;&#21152;&#23376;&#20219;&#21153;&#30340;&#25361;&#25112;&#32467;&#26524;&#21644;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present the task description of the Detection and Classification of Acoustic Scenes and Events (DCASE) 2023 Challenge Task 2: "First-shot unsupervised anomalous sound detection (ASD) for machine condition monitoring". The main goal is to enable rapid deployment of ASD systems for new kinds of machines using only a few normal samples, without the need for hyperparameter tuning. In the past ASD tasks, developed methods tuned hyperparameters for each machine type, as the development and evaluation datasets had the same machine types. However, collecting normal and anomalous data as the development dataset can be infeasible in practice. In 2023 Task 2, we focus on solving first-shot problem, which is the challenge of training a model on a few machines of a completely novel machine type. Specifically, (i) each machine type has only one section, and (ii) machine types in the development and evaluation datasets are completely different. We will add challenge results and analysis of the sub
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#35299;&#37322;&#26041;&#27861;&#65292;Calibrated Explanations (CE)&#65292;&#23427;&#21487;&#20197;&#25552;&#20379;&#20934;&#30830;&#12289;&#31283;&#23450;&#30340;&#35299;&#37322;&#65292;&#24182;&#19988;&#21487;&#20197;&#20026;&#27010;&#29575;&#20272;&#35745;&#21644;&#29305;&#24449;&#37325;&#35201;&#24615;&#26435;&#37325;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#20449;&#24687;&#65292;&#26159;&#19968;&#31181;&#24555;&#36895;&#12289;&#21487;&#38752;&#19988;&#24378;&#20581;&#30340;&#35299;&#37322;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.02305</link><description>&lt;p&gt;
&#26657;&#20934;&#21270;&#35299;&#37322;&#65306;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#20449;&#24687;&#21644;&#21453;&#20107;&#23454;&#30340;&#35299;&#37322;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Calibrated Explanations: with Uncertainty Information and Counterfactuals. (arXiv:2305.02305v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02305
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#35299;&#37322;&#26041;&#27861;&#65292;Calibrated Explanations (CE)&#65292;&#23427;&#21487;&#20197;&#25552;&#20379;&#20934;&#30830;&#12289;&#31283;&#23450;&#30340;&#35299;&#37322;&#65292;&#24182;&#19988;&#21487;&#20197;&#20026;&#27010;&#29575;&#20272;&#35745;&#21644;&#29305;&#24449;&#37325;&#35201;&#24615;&#26435;&#37325;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#20449;&#24687;&#65292;&#26159;&#19968;&#31181;&#24555;&#36895;&#12289;&#21487;&#38752;&#19988;&#24378;&#20581;&#30340;&#35299;&#37322;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#24050;&#32463;&#25104;&#20026;&#21508;&#31181;&#39046;&#22495;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#19968;&#37096;&#20998;&#65292;&#20294;&#20154;&#24037;&#26234;&#33021;&#20915;&#31574;&#31995;&#32479;&#20013;&#39044;&#27979;&#27169;&#22411;&#32570;&#20047;&#36879;&#26126;&#24230;&#21487;&#33021;&#23548;&#33268;&#28389;&#29992;&#25110;&#19981;&#20351;&#29992;&#12290;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26088;&#22312;&#21019;&#24314;&#21487;&#20197;&#21521;&#20154;&#31867;&#29992;&#25143;&#35299;&#37322;&#20854;&#25512;&#29702;&#36807;&#31243;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#23616;&#37096;&#35299;&#37322;&#21487;&#20197;&#25552;&#20379;&#20851;&#20110;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#20010;&#21035;&#39044;&#27979;&#21407;&#22240;&#30340;&#20449;&#24687;&#65292;&#20294;&#23384;&#22312;&#19981;&#31283;&#23450;&#24615;&#31561;&#32570;&#28857;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#35299;&#37322;&#26041;&#27861;&#65292;&#26657;&#20934;&#21270;&#35299;&#37322;(Calibrated Explanations&#65292;CE)&#65292;&#23427;&#22522;&#20110; Venn-Abers&#65292;&#21516;&#26102;&#22312;&#29983;&#25104;&#29305;&#24449;&#37325;&#35201;&#24615;&#35299;&#37322;&#30340;&#21516;&#26102;&#26657;&#20934;&#24213;&#23618;&#27169;&#22411;&#12290;CE&#19981;&#20165;&#25552;&#20379;&#24555;&#36895;&#12289;&#21487;&#38752;&#12289;&#31283;&#23450;&#21644;&#24378;&#20581;&#30340;&#35299;&#37322;&#65292;&#36824;&#25552;&#20379;&#27010;&#29575;&#20272;&#35745;&#21644;&#29305;&#24449;&#37325;&#35201;&#24615;&#26435;&#37325;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#26159;&#27169;&#22411;&#26080;&#20851;&#30340;&#65292;&#20855;&#26377;&#26131;&#20110;&#29702;&#35299;&#30340;&#26465;&#20214;&#35268;&#21017;&#65292;&#20063;&#21487;&#20197;&#29983;&#25104;&#21453;&#20107;&#23454;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI) has become an integral part of decision support systems (DSSs) in various domains, but the lack of transparency in the predictive models used in AI-based DSSs can lead to misuse or disuse. Explainable Artificial Intelligence (XAI) aims to create AI systems that can explain their rationale to human users. Local explanations in XAI can provide information about the causes of individual predictions in terms of feature importance, but they suffer from drawbacks such as instability. To address these issues, we propose a new feature importance explanation method, Calibrated Explanations (CE), which is based on Venn-Abers and calibrates the underlying model while generating feature importance explanations. CE provides fast, reliable, stable, and robust explanations, along with uncertainty quantification of the probability estimates and feature importance weights. Furthermore, the method is model agnostic with easily understood conditional rules and can also genera
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#24207;&#21015;&#24314;&#27169;&#30340;&#26500;&#24314;&#22359;&#65292;&#31216;&#20026;MultiresLayer&#65292;&#36890;&#36807;&#22810;&#20998;&#36776;&#29575;&#21367;&#31215;&#25429;&#33719;&#36755;&#20837;&#24207;&#21015;&#20013;&#30340;&#22810;&#23610;&#24230;&#36235;&#21183;&#65292;&#26082;&#20855;&#26377;&#21367;&#31215;&#32593;&#32476;&#30340;&#35745;&#31639;&#20248;&#21183;&#65292;&#21448;&#20855;&#26377;&#23567;&#27874;&#20998;&#35299;&#30340;&#26377;&#29702;&#35770;&#22522;&#30784;&#30340;&#21160;&#26426;&#12290;</title><link>http://arxiv.org/abs/2305.01638</link><description>&lt;p&gt;
&#22810;&#20998;&#36776;&#29575;&#21367;&#31215;&#35760;&#24518;&#30340;&#24207;&#21015;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Sequence Modeling with Multiresolution Convolutional Memory. (arXiv:2305.01638v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01638
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#24207;&#21015;&#24314;&#27169;&#30340;&#26500;&#24314;&#22359;&#65292;&#31216;&#20026;MultiresLayer&#65292;&#36890;&#36807;&#22810;&#20998;&#36776;&#29575;&#21367;&#31215;&#25429;&#33719;&#36755;&#20837;&#24207;&#21015;&#20013;&#30340;&#22810;&#23610;&#24230;&#36235;&#21183;&#65292;&#26082;&#20855;&#26377;&#21367;&#31215;&#32593;&#32476;&#30340;&#35745;&#31639;&#20248;&#21183;&#65292;&#21448;&#20855;&#26377;&#23567;&#27874;&#20998;&#35299;&#30340;&#26377;&#29702;&#35770;&#22522;&#30784;&#30340;&#21160;&#26426;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#22320;&#25429;&#25417;&#23545;&#20110;&#26576;&#20010;&#20219;&#21153;&#65288;&#22914;&#20998;&#31867;&#21644;&#29983;&#25104;&#24314;&#27169;&#65289;&#26174;&#33879;&#30340;&#39034;&#24207;&#25968;&#25454;&#28304;&#20013;&#30340;&#38271;&#31243;&#27169;&#24335;&#26159;&#19968;&#20010;&#22522;&#26412;&#25361;&#25112;&#12290;&#25105;&#20204;&#20174;&#22522;&#20110;&#23567;&#27874;&#30340;&#22810;&#20998;&#36776;&#29575;&#20998;&#26512;&#20013;&#33719;&#24471;&#28789;&#24863;&#65292;&#23450;&#20041;&#20102;&#19968;&#20010;&#26032;&#30340;&#29992;&#20110;&#24207;&#21015;&#24314;&#27169;&#30340;&#26500;&#24314;&#22359;&#65292;&#31216;&#20026;MultiresLayer&#12290;&#25105;&#20204;&#27169;&#22411;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#26159;&#22810;&#20998;&#36776;&#29575;&#21367;&#31215;&#65292;&#20197;&#25429;&#33719;&#36755;&#20837;&#24207;&#21015;&#20013;&#30340;&#22810;&#23610;&#24230;&#36235;&#21183;&#12290;&#25105;&#20204;&#30340;MultiresConv&#21487;&#20197;&#36890;&#36807;&#22312;&#25193;&#24352;&#30340;&#22240;&#26524;&#21367;&#31215;&#26641;&#19978;&#20351;&#29992;&#20849;&#20139;&#36807;&#28388;&#22120;&#26469;&#23454;&#29616;&#12290;&#22240;&#27492;&#65292;&#23427;&#26082;&#20855;&#26377;&#21367;&#31215;&#32593;&#32476;&#30340;&#35745;&#31639;&#20248;&#21183;&#65292;&#21448;&#20855;&#26377;&#23567;&#27874;&#20998;&#35299;&#30340;&#26377;&#29702;&#35770;&#22522;&#30784;&#30340;&#21160;&#26426;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficiently capturing the long-range patterns in sequential data sources salient to a given task -- such as classification and generative modeling -poses a fundamental challenge. Popular approaches in the space tradeoff between the memory burden of brute-force enumeration and comparison, as in transformers, the computational burden of complicated sequential dependencies, as in recurrent neural networks, or the parameter burden of convolutional networks with many or large filters. We instead take inspiration from wavelet-based multiresolution analysis to define a new building block for sequence modeling, which we call a MultiresLayer. The key component of our model is the multiresolution convolution, capturing multiscale trends in the input sequence. Our MultiresConv can be implemented with shared filters across a dilated causal convolution tree. Thus it garners the computational advantages of convolutional networks and the principled theoretical motivation of wavelet decompositions. 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36816;&#29992;&#26426;&#26800;&#24335;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#25506;&#31350;&#20102;GPT-2 Small&#30340;&#25968;&#23398;&#33021;&#21147;&#65292;&#24182;&#30830;&#23450;&#20102;&#23427;&#30340;&#35745;&#31639;&#22270;&#20013;&#30340;&#19968;&#20010;&#23567;&#30005;&#36335;&#29992;&#20110;&#35745;&#31639;&#22823;&#20110;&#31526;&#21495;&#65292;&#35813;&#30005;&#36335;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#25552;&#39640;&#20102;&#32467;&#26463;&#24180;&#20221;&#22823;&#20110;&#24320;&#22987;&#24180;&#20221;&#30340;&#27010;&#29575;&#65292;&#24182;&#19988;&#35813;&#30005;&#36335;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.00586</link><description>&lt;p&gt;
GPT-2&#26159;&#22914;&#20309;&#35745;&#31639;&#22823;&#20110;&#31526;&#21495;&#30340;&#65311;&#35299;&#37322;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#23398;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model. (arXiv:2305.00586v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36816;&#29992;&#26426;&#26800;&#24335;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#25506;&#31350;&#20102;GPT-2 Small&#30340;&#25968;&#23398;&#33021;&#21147;&#65292;&#24182;&#30830;&#23450;&#20102;&#23427;&#30340;&#35745;&#31639;&#22270;&#20013;&#30340;&#19968;&#20010;&#23567;&#30005;&#36335;&#29992;&#20110;&#35745;&#31639;&#22823;&#20110;&#31526;&#21495;&#65292;&#35813;&#30005;&#36335;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#25552;&#39640;&#20102;&#32467;&#26463;&#24180;&#20221;&#22823;&#20110;&#24320;&#22987;&#24180;&#20221;&#30340;&#27010;&#29575;&#65292;&#24182;&#19988;&#35813;&#30005;&#36335;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#26410;&#34987;&#26126;&#30830;&#35757;&#32451;&#30340;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22914;&#20309;&#23454;&#29616;&#36825;&#20123;&#21151;&#33021;&#21364;&#19981;&#20026;&#20154;&#25152;&#30693;&#12290;&#26412;&#25991;&#36890;&#36807;&#26426;&#26800;&#24335;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#25506;&#31350;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#20855;&#26377;&#30340;&#22522;&#26412;&#25968;&#23398;&#33021;&#21147;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20197;GPT-2 Small&#20026;&#20363;&#65292;&#30740;&#31350;&#20854;&#33021;&#21542;&#36890;&#36807;&#36755;&#20837;"&#25112;&#20105;&#25345;&#32493;&#26102;&#38388;&#26159;&#20174;1732&#24180;&#21040;17&#24180;"&#65292;&#39044;&#27979;&#20986;&#26377;&#25928;&#30340;&#20004;&#20301;&#25968;&#23383;&#30340;&#25130;&#27490;&#24180;&#20221; (&#22823;&#20110;32&#24180;)&#12290;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;&#20102;&#19968;&#20010;&#30005;&#36335;&#65292;&#21363;GPT-2 Small&#35745;&#31639;&#22270;&#30340;&#19968;&#20010;&#23567;&#23376;&#38598;&#65292;&#29992;&#20110;&#35745;&#31639;&#36825;&#20010;&#20219;&#21153;&#30340;&#36755;&#20986;&#65292;&#28982;&#21518;&#25105;&#20204;&#35299;&#37322;&#20102;&#27599;&#20010;&#30005;&#36335;&#32452;&#20214;&#30340;&#20316;&#29992;&#65292;&#26174;&#31034;&#20986;GPT-2 Small&#30340;&#26368;&#32456;&#22810;&#23618;&#24863;&#30693;&#22120;&#25552;&#39640;&#20102;&#32467;&#26463;&#24180;&#20221;&#22823;&#20110;&#24320;&#22987;&#24180;&#20221;&#30340;&#27010;&#29575;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#30005;&#36335;&#36866;&#29992;&#20110;&#20854;&#20182;&#20219;&#21153;&#65292;&#22312;&#20854;&#20182;&#22823;&#20110;&#22330;&#26223;&#20013;&#21457;&#25381;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models can be surprisingly adept at tasks they were not explicitly trained on, but how they implement these capabilities is poorly understood. In this paper, we investigate the basic mathematical abilities often acquired by pre-trained language models. Concretely, we use mechanistic interpretability techniques to explain the (limited) mathematical abilities of GPT-2 small. As a case study, we examine its ability to take in sentences such as "The war lasted from the year 1732 to the year 17", and predict valid two-digit end years (years &gt; 32). We first identify a circuit, a small subset of GPT-2 small's computational graph that computes this task's output. Then, we explain the role of each circuit component, showing that GPT-2 small's final multi-layer perceptrons boost the probability of end years greater than the start year. Finally, we show that our circuit generalizes to other tasks, playing a role in other greater-than scenarios.
&lt;/p&gt;</description></item><item><title>&#21516;&#28304;&#24615;&#21407;&#21017;&#19981;&#19968;&#23450;&#26159;&#24433;&#21709;&#22270;&#31070;&#32463;&#32593;&#32476;&#20248;&#36234;&#24615;&#30340;&#21807;&#19968;&#21407;&#22240;&#65307;&#26412;&#25991;&#25552;&#20986;Contextual Stochastic Block Model for Homophily (CSBM-H)&#20197;&#28145;&#20837;&#30740;&#31350;&#21516;&#28304;&#24615;&#23545;&#33410;&#28857;&#21487;&#21306;&#20998;&#24615;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.14274</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#20309;&#26102;&#23545;&#33410;&#28857;&#20998;&#31867;&#26377;&#24110;&#21161;&#65306;&#30740;&#31350;&#21516;&#28304;&#24615;&#21407;&#21017;&#23545;&#33410;&#28857;&#21487;&#21306;&#20998;&#24615;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
When Do Graph Neural Networks Help with Node Classification: Investigating the Homophily Principle on Node Distinguishability. (arXiv:2304.14274v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14274
&lt;/p&gt;
&lt;p&gt;
&#21516;&#28304;&#24615;&#21407;&#21017;&#19981;&#19968;&#23450;&#26159;&#24433;&#21709;&#22270;&#31070;&#32463;&#32593;&#32476;&#20248;&#36234;&#24615;&#30340;&#21807;&#19968;&#21407;&#22240;&#65307;&#26412;&#25991;&#25552;&#20986;Contextual Stochastic Block Model for Homophily (CSBM-H)&#20197;&#28145;&#20837;&#30740;&#31350;&#21516;&#28304;&#24615;&#23545;&#33410;&#28857;&#21487;&#21306;&#20998;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21516;&#28304;&#24615;&#21407;&#21017;&#25351;&#30456;&#21516;&#31867;&#21035;&#30340;&#33410;&#28857;&#26356;&#26377;&#21487;&#33021;&#36830;&#25509;&#22312;&#19968;&#36215;&#65292;&#19968;&#30452;&#34987;&#35748;&#20026;&#26159;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#33410;&#28857;&#20998;&#31867;&#65288;NC&#65289;&#20219;&#21153;&#19978;&#24615;&#33021;&#20248;&#36234;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#26368;&#36817;&#65292;&#20154;&#20204;&#25552;&#20986;&#29702;&#35770;&#32467;&#26524;&#35748;&#20026;&#65292;&#21363;&#20351;&#21516;&#28304;&#24615;&#21407;&#21017;&#34987;&#25171;&#30772;&#65292;&#21482;&#35201;&#26469;&#33258;&#21516;&#19968;&#31867;&#21035;&#30340;&#33410;&#28857;&#20998;&#20139;&#30456;&#20284;&#30340;&#37051;&#23621;&#27169;&#24335;&#65292;GNN&#30340;&#20248;&#21183;&#20173;&#28982;&#23384;&#22312;&#65292;&#36825;&#23545;&#21516;&#28304;&#24615;&#30340;&#26377;&#25928;&#24615;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#35770;&#28857;&#20165;&#32771;&#34385;&#20102;&#21516;&#31867;&#33410;&#28857;&#30340;&#21487;&#21306;&#20998;&#24615;&#65292;&#24573;&#30053;&#20102;&#36328;&#31867;&#21035;&#30340;&#21487;&#21306;&#20998;&#24615;&#65292;&#36825;&#26159;&#30740;&#31350;&#21516;&#28304;&#24615;&#25928;&#24212;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#20363;&#23376;&#35777;&#26126;&#20102;&#19978;&#36848;&#19981;&#36275;&#65292;&#24182;&#35748;&#20026;&#21487;&#21306;&#20998;&#24615;&#30340;&#29702;&#24819;&#24773;&#20917;&#26159;&#21516;&#31867;&#33410;&#28857;&#30340;&#21487;&#21306;&#20998;&#24615;&#23567;&#20110;&#36328;&#31867;&#21035;&#33410;&#28857;&#30340;&#21487;&#21306;&#20998;&#24615;&#12290;&#20026;&#20102;&#24418;&#24335;&#21270;&#36825;&#20010;&#24819;&#27861;&#65292;&#26356;&#22909;&#22320;&#29702;&#35299;&#21516;&#28304;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Contextual Stochastic Block Model for Homophily (CSBM-H)&#65292;&#24182;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Homophily principle, i.e. nodes with the same labels are more likely to be connected, was believed to be the main reason for the performance superiority of Graph Neural Networks (GNNs) over Neural Networks (NNs) on Node Classification (NC) tasks. Recently, people have developed theoretical results arguing that, even though the homophily principle is broken, the advantage of GNNs can still hold as long as nodes from the same class share similar neighborhood patterns, which questions the validity of homophily. However, this argument only considers intra-class Node Distinguishability (ND) and ignores inter-class ND, which is insufficient to study the effect of homophily. In this paper, we first demonstrate the aforementioned insufficiency with examples and argue that an ideal situation for ND is to have smaller intra-class ND than inter-class ND. To formulate this idea and have a better understanding of homophily, we propose Contextual Stochastic Block Model for Homophily (CSBM-H) and def
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20013;&#38388;&#23618;&#25200;&#21160;&#34928;&#20943;&#65288;ILPD&#65289;&#30340;&#26032;&#22411;&#20013;&#38388;&#23618;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#21333;&#20010;&#20248;&#21270;&#38454;&#27573;&#21046;&#20316;&#21487;&#36716;&#31227;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#65292;&#24182;&#22312;&#36807;&#31243;&#20013;&#40723;&#21169;&#20013;&#38388;&#23618;&#25200;&#21160;&#22788;&#20110;&#26377;&#25928;&#30340;&#25932;&#23545;&#26041;&#21521;&#65292;&#24182;&#21516;&#26102;&#20855;&#26377;&#24456;&#22823;&#30340;&#24133;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.13410</link><description>&lt;p&gt;
&#20351;&#29992;&#20013;&#38388;&#23618;&#25200;&#21160;&#34928;&#20943;&#26469;&#25552;&#39640;&#25932;&#23545;&#36801;&#31227;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving Adversarial Transferability by Intermediate-level Perturbation Decay. (arXiv:2304.13410v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20013;&#38388;&#23618;&#25200;&#21160;&#34928;&#20943;&#65288;ILPD&#65289;&#30340;&#26032;&#22411;&#20013;&#38388;&#23618;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#21333;&#20010;&#20248;&#21270;&#38454;&#27573;&#21046;&#20316;&#21487;&#36716;&#31227;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#65292;&#24182;&#22312;&#36807;&#31243;&#20013;&#40723;&#21169;&#20013;&#38388;&#23618;&#25200;&#21160;&#22788;&#20110;&#26377;&#25928;&#30340;&#25932;&#23545;&#26041;&#21521;&#65292;&#24182;&#21516;&#26102;&#20855;&#26377;&#24456;&#22823;&#30340;&#24133;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20013;&#38388;&#23618;&#25915;&#20987;&#26159;&#25351;&#36890;&#36807;&#36981;&#24490;&#23545;&#25239;&#26041;&#21521;&#65292;&#23581;&#35797;&#25200;&#21160;&#29305;&#24449;&#34920;&#31034;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#22312;&#21046;&#20316;&#21487;&#36716;&#31227;&#30340;&#23545;&#25239;&#26679;&#26412;&#20013;&#23637;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#29616;&#26377;&#30340;&#36825;&#31867;&#25915;&#20987;&#26041;&#27861;&#36890;&#24120;&#30001;&#20004;&#20010;&#20998;&#31163;&#30340;&#38454;&#27573;&#26500;&#25104;&#65292;&#39318;&#20808;&#38656;&#35201;&#30830;&#23450;&#19968;&#20010;&#26041;&#21521;&#21521;&#23548;&#65292;&#28982;&#21518;&#25193;&#22823;&#20013;&#38388;&#23618;&#25200;&#21160;&#23545;&#35813;&#26041;&#21521;&#21521;&#23548;&#30340;&#26631;&#37327;&#25237;&#24433;&#12290;&#28982;&#32780;&#65292;&#24471;&#21040;&#30340;&#25200;&#21160;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#38590;&#20813;&#20250;&#20559;&#31163;&#21521;&#23548;&#12290;&#26412;&#25991;&#21457;&#29616;&#65292;&#36825;&#31181;&#20559;&#31163;&#21487;&#33021;&#23548;&#33268;&#27425;&#20248;&#30340;&#25915;&#20987;&#25928;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;&#20013;&#38388;&#23618;&#25200;&#21160;&#34928;&#20943;&#65288;ILPD&#65289;&#30340;&#26032;&#22411;&#20013;&#38388;&#23618;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#21333;&#20010;&#20248;&#21270;&#38454;&#27573;&#21046;&#20316;&#21487;&#36716;&#31227;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#35813;&#26041;&#27861;&#40723;&#21169;&#20013;&#38388;&#23618;&#25200;&#21160;&#22788;&#20110;&#26377;&#25928;&#30340;&#25932;&#23545;&#26041;&#21521;&#65292;&#24182;&#21516;&#26102;&#20855;&#26377;&#24456;&#22823;&#30340;&#24133;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intermediate-level attacks that attempt to perturb feature representations following an adversarial direction drastically have shown favorable performance in crafting transferable adversarial examples. Existing methods in this category are normally formulated with two separate stages, where a directional guide is required to be determined at first and the scalar projection of the intermediate-level perturbation onto the directional guide is enlarged thereafter. The obtained perturbation deviates from the guide inevitably in the feature space, and it is revealed in this paper that such a deviation may lead to sub-optimal attack. To address this issue, we develop a novel intermediate-level method that crafts adversarial examples within a single stage of optimization. In particular, the proposed method, named intermediate-level perturbation decay (ILPD), encourages the intermediate-level perturbation to be in an effective adversarial direction and to possess a great magnitude simultaneous
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#35299;&#37322;&#32447;&#24615;&#31995;&#32479;&#30340;&#31232;&#30095;&#30697;&#38453;&#20026;&#40657;&#30333;&#22270;&#20687;&#65292;&#21033;&#29992;&#27744;&#21270;&#25805;&#20316;&#23558;&#20854;&#36716;&#25442;&#20026;&#23567;&#30340;&#22810;&#36890;&#36947;&#22270;&#20687;&#65292;&#20174;&#32780;&#35843;&#25972;&#20195;&#25968;&#22810;&#37325;&#32593;&#26684;&#26041;&#27861;&#20013;&#30340;&#24378;&#38376;&#27099;&#21442;&#25968;&#12290;&#35813;&#31639;&#27861;&#26368;&#23567;&#21270;&#20102;AMG&#26041;&#27861;&#22312;&#26377;&#38480;&#20803;&#27714;&#35299;&#22120;&#20013;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#22312;&#35299;&#20915;&#19977;&#32500;&#26925;&#22278;&#20559;&#24494;&#20998;&#26041;&#31243;&#26102;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;AMG&#27714;&#35299;&#22120;&#26356;&#24555;&#12290;</title><link>http://arxiv.org/abs/2304.10832</link><description>&lt;p&gt;
&#19968;&#31181;&#21152;&#36895;&#26377;&#38480;&#20803;&#27714;&#35299;&#22120;&#20013;&#20195;&#25968;&#22810;&#37325;&#32593;&#26684;&#26041;&#27861;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Deep Learning algorithm to accelerate Algebraic Multigrid methods in Finite Element solvers of 3D elliptic PDEs. (arXiv:2304.10832v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10832
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#35299;&#37322;&#32447;&#24615;&#31995;&#32479;&#30340;&#31232;&#30095;&#30697;&#38453;&#20026;&#40657;&#30333;&#22270;&#20687;&#65292;&#21033;&#29992;&#27744;&#21270;&#25805;&#20316;&#23558;&#20854;&#36716;&#25442;&#20026;&#23567;&#30340;&#22810;&#36890;&#36947;&#22270;&#20687;&#65292;&#20174;&#32780;&#35843;&#25972;&#20195;&#25968;&#22810;&#37325;&#32593;&#26684;&#26041;&#27861;&#20013;&#30340;&#24378;&#38376;&#27099;&#21442;&#25968;&#12290;&#35813;&#31639;&#27861;&#26368;&#23567;&#21270;&#20102;AMG&#26041;&#27861;&#22312;&#26377;&#38480;&#20803;&#27714;&#35299;&#22120;&#20013;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#22312;&#35299;&#20915;&#19977;&#32500;&#26925;&#22278;&#20559;&#24494;&#20998;&#26041;&#31243;&#26102;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;AMG&#27714;&#35299;&#22120;&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#25968;&#22810;&#37325;&#32593;&#26684;&#26041;&#27861;&#26159;&#35299;&#32447;&#24615;&#26041;&#31243;&#32452;&#30340;&#26368;&#26377;&#25928;&#30340;&#27714;&#35299;&#22120;&#20043;&#19968;&#65292;&#24191;&#27867;&#29992;&#20110;&#31163;&#25955;&#21270;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#35813;&#26041;&#27861;&#23384;&#22312;&#39640;&#24230;&#20381;&#36182;&#20110;&#38656;&#35201;&#35843;&#20248;&#30340;&#21442;&#25968;&#65292;&#23588;&#20854;&#26159;&#24378;&#38376;&#27099;&#21442;&#25968;&#65292;&#36825;&#26159;&#26500;&#24314;&#22810;&#37325;&#32593;&#26684;&#25152;&#38656;&#30340;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#25226;&#32447;&#24615;&#31995;&#32479;&#30340;&#31232;&#30095;&#30697;&#38453;&#35299;&#37322;&#20026;&#40657;&#30333;&#22270;&#20687;&#65292;&#21033;&#29992;&#27744;&#21270;&#25805;&#20316;&#23558;&#23427;&#36716;&#25442;&#20026;&#23567;&#30340;&#22810;&#36890;&#36947;&#22270;&#20687;&#65292;&#20174;&#32780;&#35843;&#25972;&#24378;&#38376;&#27099;&#21442;&#25968;&#65292;&#26368;&#23567;&#21270;&#20102;AMG&#26041;&#27861;&#22312;&#26377;&#38480;&#20803;&#27714;&#35299;&#22120;&#20013;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#31639;&#27861;&#23545;&#20110;&#35299;&#20915;&#19977;&#32500;&#26925;&#22278;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#36895;&#24230;&#26174;&#33879;&#24555;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;AMG&#27714;&#35299;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algebraic multigrid (AMG) methods are among the most efficient solvers for linear systems of equations and they are widely used for the solution of problems stemming from the discretization of Partial Differential Equations (PDEs). The most severe limitation of AMG methods is the dependence on parameters that require to be fine-tuned. In particular, the strong threshold parameter is the most relevant since it stands at the basis of the construction of successively coarser grids needed by the AMG methods. We introduce a novel Deep Learning algorithm that minimizes the computational cost of the AMG method when used as a finite element solver. We show that our algorithm requires minimal changes to any existing code. The proposed Artificial Neural Network (ANN) tunes the value of the strong threshold parameter by interpreting the sparse matrix of the linear system as a black-and-white image and exploiting a pooling operator to transform it into a small multi-channel image. We experimentall
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;&#35282;&#33394;&#25198;&#28436;&#30340;&#26032;&#22411;&#20132;&#20114;&#24335;&#20195;&#29702;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#33258;&#20027;&#21512;&#20316;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#29983;&#25104;&#23545;&#35805;&#25968;&#25454;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.17760</link><description>&lt;p&gt;
CAMEL: &#29992;&#20110;&#8220;&#24515;&#26234;&#8221;&#25506;&#32034;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#31038;&#32676;&#30340;&#20132;&#20114;&#24335;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
CAMEL: Communicative Agents for "Mind" Exploration of Large Scale Language Model Society. (arXiv:2303.17760v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;&#35282;&#33394;&#25198;&#28436;&#30340;&#26032;&#22411;&#20132;&#20114;&#24335;&#20195;&#29702;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#33258;&#20027;&#21512;&#20316;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#29983;&#25104;&#23545;&#35805;&#25968;&#25454;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#24335;&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#21457;&#23637;&#24050;&#21462;&#24471;&#20102;&#22312;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#26041;&#38754;&#30340;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#25104;&#21151;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#20154;&#31867;&#30340;&#25351;&#23548;&#65292;&#20197;&#24341;&#23548;&#23545;&#35805;&#65292;&#36825;&#21487;&#33021;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#32791;&#26102;&#30340;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#26500;&#24314;&#21487;&#25193;&#23637;&#25216;&#26415;&#20197;&#20419;&#36827;&#20132;&#20114;&#24335;&#20195;&#29702;&#20043;&#38388;&#30340;&#33258;&#20027;&#21512;&#20316;&#24182;&#28145;&#20837;&#20102;&#35299;&#23427;&#20204;&#30340;&#8220;&#35748;&#30693;&#8221;&#36807;&#31243;&#30340;&#28508;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#23454;&#29616;&#33258;&#20027;&#21512;&#20316;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#35282;&#33394;&#25198;&#28436;&#30340;&#26032;&#22411;&#20132;&#20114;&#24335;&#20195;&#29702;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;&#21551;&#21160;&#25552;&#31034;&#26469;&#24341;&#23548;&#32842;&#22825;&#20195;&#29702;&#23436;&#25104;&#20219;&#21153;&#65292;&#21516;&#26102;&#20445;&#25345;&#19982;&#20154;&#31867;&#24847;&#22270;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#35282;&#33394;&#25198;&#28436;&#26469;&#29983;&#25104;&#23545;&#35805;&#25968;&#25454;&#65292;&#20197;&#30740;&#31350;&#32842;&#22825;&#20195;&#29702;&#30340;&#34892;&#20026;&#21644;&#33021;&#21147;&#65292;&#20026;&#30740;&#31350;&#23545;&#35805;&#24335;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26159;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#20132;&#20114;&#24335;&#20195;&#29702;&#26694;&#26550;&#65292;&#21517;&#20026;&#35282;&#33394;&#25198;&#28436;&#65292;&#29992;&#20110;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#33258;&#20027;&#21512;&#20316;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#29983;&#25104;&#23545;&#35805;&#25968;&#25454;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid advancement of conversational and chat-based language models has led to remarkable progress in complex task-solving. However, their success heavily relies on human input to guide the conversation, which can be challenging and time-consuming. This paper explores the potential of building scalable techniques to facilitate autonomous cooperation among communicative agents and provide insight into their "cognitive" processes. To address the challenges of achieving autonomous cooperation, we propose a novel communicative agent framework named role-playing. Our approach involves using inception prompting to guide chat agents toward task completion while maintaining consistency with human intentions. We showcase how role-playing can be used to generate conversational data for studying the behaviors and capabilities of chat agents, providing a valuable resource for investigating conversational language models. Our contributions include introducing a novel communicative agent framewor
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; HopCPT &#30340;&#26032;&#19968;&#33268;&#24615;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#65292;&#19981;&#20165;&#33021;&#22815;&#22788;&#29702;&#26102;&#38388;&#32467;&#26500;&#65292;&#32780;&#19988;&#33021;&#22815;&#21033;&#29992;&#20854;&#20248;&#21183;&#65292;&#24050;&#22312;&#22810;&#31181;&#30495;&#23454;&#19990;&#30028;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#20102;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.12783</link><description>&lt;p&gt;
&#22522;&#20110;&#29616;&#20195; Hopfield &#32593;&#32476;&#30340;&#26102;&#38388;&#24207;&#21015;&#19968;&#33268;&#24615;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Conformal Prediction for Time Series with Modern Hopfield Networks. (arXiv:2303.12783v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12783
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; HopCPT &#30340;&#26032;&#19968;&#33268;&#24615;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#65292;&#19981;&#20165;&#33021;&#22815;&#22788;&#29702;&#26102;&#38388;&#32467;&#26500;&#65292;&#32780;&#19988;&#33021;&#22815;&#21033;&#29992;&#20854;&#20248;&#21183;&#65292;&#24050;&#22312;&#22810;&#31181;&#30495;&#23454;&#19990;&#30028;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#20102;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#65292;&#19968;&#33268;&#24615;&#39044;&#27979;&#26041;&#27861;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#24182;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#38590;&#20197;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#65292;&#22240;&#20026;&#26102;&#38388;&#24207;&#21015;&#30340;&#33258;&#30456;&#20851;&#32467;&#26500;&#36829;&#21453;&#20102;&#19968;&#33268;&#24615;&#39044;&#27979;&#25152;&#38656;&#30340;&#22522;&#26412;&#20551;&#35774;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102; HopCPT&#65292;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110; Hopfield &#32593;&#32476;&#30340;&#26102;&#38388;&#24207;&#21015;&#19968;&#33268;&#24615;&#39044;&#27979;&#26041;&#27861;&#65292;&#19981;&#20165;&#33021;&#22815;&#24212;&#23545;&#26102;&#38388;&#32467;&#26500;&#65292;&#32780;&#19988;&#33021;&#22815;&#21033;&#29992;&#23427;&#20204;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23384;&#22312;&#26102;&#38388;&#20381;&#36182;&#24615;&#30340;&#26102;&#38388;&#24207;&#21015;&#20013;&#22312;&#29702;&#35770;&#19978;&#26159;&#26377;&#24456;&#22909;&#30340;&#29702;&#35770;&#22522;&#30784;&#30340;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;&#22312;&#22235;&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#19968;&#33268;&#24615;&#39044;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
To quantify uncertainty, conformal prediction methods are gaining continuously more interest and have already been successfully applied to various domains. However, they are difficult to apply to time series as the autocorrelative structure of time series violates basic assumptions required by conformal prediction. We propose HopCPT, a novel conformal prediction approach for time series that not only copes with temporal structures but leverages them. We show that our approach is theoretically well justified for time series where temporal dependencies are present. In experiments, we demonstrate that our new approach outperforms state-of-the-art conformal prediction methods on multiple real-world time series datasets from four different domains.
&lt;/p&gt;</description></item><item><title>SIESTA&#26159;&#19968;&#31181;&#22312;&#32447;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26080;&#38656;&#22238;&#24518;&#12289;&#26080;&#38656;&#21453;&#21521;&#20256;&#25773;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;&#32593;&#32476;&#26356;&#26032;&#35268;&#21017;&#65292;&#22312;&#26356;&#23569;&#30340;&#26102;&#38388;&#21644;&#33021;&#28304;&#28040;&#32791;&#19979;&#39640;&#25928;&#22320;&#26356;&#26032;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#35757;&#32451;&#20241;&#30496;/&#35273;&#37266;&#26694;&#26550;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#35774;&#22791;&#31471;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2303.10725</link><description>&lt;p&gt;
SIESTA: &#39640;&#25928;&#30340;&#22312;&#32447;&#25345;&#32493;&#23398;&#20064;&#19982;&#20241;&#30496; (arXiv:2303.10725v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
SIESTA: Efficient Online Continual Learning with Sleep. (arXiv:2303.10725v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10725
&lt;/p&gt;
&lt;p&gt;
SIESTA&#26159;&#19968;&#31181;&#22312;&#32447;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26080;&#38656;&#22238;&#24518;&#12289;&#26080;&#38656;&#21453;&#21521;&#20256;&#25773;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;&#32593;&#32476;&#26356;&#26032;&#35268;&#21017;&#65292;&#22312;&#26356;&#23569;&#30340;&#26102;&#38388;&#21644;&#33021;&#28304;&#28040;&#32791;&#19979;&#39640;&#25928;&#22320;&#26356;&#26032;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#35757;&#32451;&#20241;&#30496;/&#35273;&#37266;&#26694;&#26550;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#35774;&#22791;&#31471;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30417;&#30563;&#24335;&#25345;&#32493;&#23398;&#20064;&#20013;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#36890;&#36807;&#19981;&#26029;&#22686;&#38271;&#30340;&#25968;&#25454;&#27969;&#36827;&#34892;&#26356;&#26032;&#12290;&#19982;&#25968;&#25454;&#31163;&#32447;&#24773;&#20917;&#19981;&#21516;&#65292;&#25105;&#20204;&#19981;&#33021;&#23545;&#25968;&#25454;&#27969;&#36827;&#34892;&#20219;&#20309;&#20998;&#24067;&#20551;&#35774;&#12290;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#20026;&#20102;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#65292;&#21482;&#38656;&#35201;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#19968;&#27425;&#36941;&#21382;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#26080;&#27861;&#28385;&#36275;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#30340;&#26465;&#20214;&#65292;&#21516;&#26102;&#20063;&#26080;&#27861;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35757;&#32451;&#20241;&#30496;/&#35273;&#37266;&#26694;&#26550;&#30340;&#26032;&#22411;&#22312;&#32447;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;SIESTA&#65292;&#35813;&#26041;&#27861;&#31526;&#21512;&#35774;&#22791;&#31471;&#23398;&#20064;&#30340;&#38656;&#27714;&#12290;SIESTA&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#25913;&#36827;&#35745;&#31639;&#25928;&#29575;&#65292;&#20197;&#20415;&#21487;&#20197;&#22312;&#26356;&#23569;&#30340;&#26102;&#38388;&#21644;&#33021;&#28304;&#28040;&#32791;&#19979;&#39640;&#25928;&#22320;&#26356;&#26032;DNN&#12290;SIESTA&#30340;&#20027;&#35201;&#21019;&#26032;&#28857;&#26377;&#65306;&#22312;&#35273;&#37266;&#38454;&#27573;&#20351;&#29992;&#26080;&#38656;&#22238;&#24518;&#12289;&#26080;&#38656;&#21453;&#21521;&#20256;&#25773;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;&#32593;&#32476;&#26356;&#26032;&#35268;&#21017;&#36827;&#34892;&#24555;&#36895;&#22312;&#32447;&#26356;&#26032;&#65292;&#20197;&#21450;&#24555;&#36895;&#25910;&#25947;&#30340;Wake/Sleep&#35757;&#32451;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
In supervised continual learning, a deep neural network (DNN) is updated with an ever-growing data stream. Unlike the offline setting where data is shuffled, we cannot make any distributional assumptions about the data stream. Ideally, only one pass through the dataset is needed for computational efficiency. However, existing methods are inadequate and make many assumptions that cannot be made for real-world applications, while simultaneously failing to improve computational efficiency. In this paper, we propose a novel online continual learning method, SIESTA based on wake/sleep framework for training, which is well aligned to the needs of on-device learning. The major goal of SIESTA is to advance compute efficient continual learning so that DNNs can be updated efficiently using far less time and energy. The principal innovations of SIESTA are: 1) rapid online updates using a rehearsal-free, backpropagation-free, and data-driven network update rule during its wake phase, and 2) expedi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30495;&#23454;&#20020;&#24202;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;Policy Constraint Q-Learning(PCQL)&#26469;&#23454;&#29616;&#20840;&#40635;&#33647;&#29289;&#21058;&#37327;&#25511;&#21046;&#65292;&#28155;&#21152;&#20102;&#20445;&#23432;Q-Learning&#26041;&#27861;&#21644;&#31574;&#30053;&#32422;&#26463;&#39033;&#20197;&#30830;&#20445;&#26234;&#33021;&#20307;&#20570;&#20986;&#26356;&#23433;&#20840;&#30340;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2303.10180</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#23433;&#20840;&#30340;&#19993;&#27850;&#37210;&#20840;&#40635;&#21058;&#37327;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Towards Safe Propofol Dosing during General Anesthesia Using Deep Offline Reinforcement Learning. (arXiv:2303.10180v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10180
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30495;&#23454;&#20020;&#24202;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;Policy Constraint Q-Learning(PCQL)&#26469;&#23454;&#29616;&#20840;&#40635;&#33647;&#29289;&#21058;&#37327;&#25511;&#21046;&#65292;&#28155;&#21152;&#20102;&#20445;&#23432;Q-Learning&#26041;&#27861;&#21644;&#31574;&#30053;&#32422;&#26463;&#39033;&#20197;&#30830;&#20445;&#26234;&#33021;&#20307;&#20570;&#20986;&#26356;&#23433;&#20840;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#40635;&#37257;&#26377;&#26395;&#23454;&#29616;&#26356;&#31934;&#30830;&#21644;&#20010;&#24615;&#21270;&#30340;&#40635;&#37257;&#31649;&#29702;&#65292;&#20351;&#40635;&#37257;&#24072;&#20813;&#20110;&#37325;&#22797;&#24615;&#20219;&#21153;&#65292;&#19987;&#27880;&#20110;&#24739;&#32773;&#25163;&#26415;&#25252;&#29702;&#30340;&#26368;&#20851;&#38190;&#26041;&#38754;&#12290;&#24403;&#21069;&#30340;&#30740;&#31350;&#36890;&#24120;&#38598;&#20013;&#20110;&#21019;&#24314;&#27169;&#25311;&#29615;&#22659;&#65292;&#20197;&#20415;&#26234;&#33021;&#20307;&#36827;&#34892;&#23398;&#20064;&#12290;&#36825;&#20123;&#26041;&#27861;&#24050;&#32463;&#23637;&#31034;&#20986;&#33391;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#20294;&#31163;&#20020;&#24202;&#24212;&#29992;&#36824;&#26377;&#24456;&#22823;&#30340;&#24046;&#36317;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30495;&#23454;&#20020;&#24202;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;Policy Constraint Q-Learning(PCQL)&#26469;&#35299;&#20915;&#23398;&#20064;&#40635;&#37257;&#31574;&#30053;&#30340;&#38382;&#39064;&#12290;&#39318;&#20808;&#24341;&#20837;&#20102;&#20445;&#23432;Q-Learning&#26041;&#27861;&#20197;&#32531;&#35299;&#33073;&#32447;&#24773;&#20917;&#19979;Q&#20989;&#25968;&#36807;&#24230;&#20272;&#35745;&#30340;&#38382;&#39064;&#12290;&#22312;&#26234;&#33021;&#20307;&#30340;&#22521;&#35757;&#20013;&#28155;&#21152;&#19968;&#39033;&#31574;&#30053;&#32422;&#26463;&#39033;&#65292;&#20197;&#20445;&#25345;&#26234;&#33021;&#20307;&#21644;&#40635;&#37257;&#24072;&#30340;&#31574;&#30053;&#20998;&#24067;&#19968;&#33268;&#65292;&#20197;&#30830;&#20445;&#26234;&#33021;&#20307;&#22312;&#20840;&#40635;&#24773;&#26223;&#19979;&#20570;&#20986;&#26356;&#23433;&#20840;&#30340;&#20915;&#31574;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;PCQL&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated anesthesia promises to enable more precise and personalized anesthetic administration and free anesthesiologists from repetitive tasks, allowing them to focus on the most critical aspects of a patient's surgical care. Current research has typically focused on creating simulated environments from which agents can learn. These approaches have demonstrated good experimental results, but are still far from clinical application. In this paper, Policy Constraint Q-Learning (PCQL), a data-driven reinforcement learning algorithm for solving the problem of learning anesthesia strategies on real clinical datasets, is proposed. Conservative Q-Learning was first introduced to alleviate the problem of Q function overestimation in an offline context. A policy constraint term is added to agent training to keep the policy distribution of the agent and the anesthesiologist consistent to ensure safer decisions made by the agent in anesthesia scenarios. The effectiveness of PCQL was validated b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#26631;&#31614;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#25163;&#21160;&#26631;&#35760;&#30340;&#25968;&#25454;&#20013;&#23384;&#22312;&#22122;&#22768;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#27169;&#22411;&#39044;&#27979;&#26469;&#36741;&#21161;&#20154;&#31867;&#26631;&#35760;&#22122;&#22768;&#25968;&#25454;&#12290;&#23454;&#39564;&#35777;&#26126;&#27492;&#26041;&#27861;&#36866;&#29992;&#20110;&#22810;&#31867;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2302.04391</link><description>&lt;p&gt;
&#25968;&#25454;&#20013;&#24515;&#26426;&#22120;&#23398;&#20064;&#30340;&#37325;&#26032;&#26631;&#31614;&#27861;
&lt;/p&gt;
&lt;p&gt;
The Re-Label Method For Data-Centric Machine Learning. (arXiv:2302.04391v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#26631;&#31614;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#25163;&#21160;&#26631;&#35760;&#30340;&#25968;&#25454;&#20013;&#23384;&#22312;&#22122;&#22768;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#27169;&#22411;&#39044;&#27979;&#26469;&#36741;&#21161;&#20154;&#31867;&#26631;&#35760;&#22122;&#22768;&#25968;&#25454;&#12290;&#23454;&#39564;&#35777;&#26126;&#27492;&#26041;&#27861;&#36866;&#29992;&#20110;&#22810;&#31867;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20013;&#65292;&#25163;&#21160;&#26631;&#35760;&#30340;&#25968;&#25454;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#23384;&#22312;&#22122;&#22768;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#22312;&#24320;&#21457;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;90&#20998;&#20197;&#19978;&#30340;&#25104;&#32489;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#25214;&#20986;&#22122;&#22768;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#37319;&#29992;&#27169;&#22411;&#39044;&#27979;&#20316;&#20026;&#20154;&#31867;&#26631;&#35760;&#30340;&#21442;&#32771;&#26469;&#37325;&#26032;&#26631;&#35760;&#22122;&#22768;&#25968;&#25454;&#12290;&#26412;&#25991;&#38416;&#36848;&#20102;&#25105;&#20204;&#22312;&#24191;&#27867;&#30340;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#24819;&#27861;&#65292;&#21253;&#25324;&#20998;&#31867;&#12289;&#24207;&#21015;&#26631;&#35760;&#12289;&#29289;&#20307;&#26816;&#27979;&#12289;&#24207;&#21015;&#29983;&#25104;&#12289;&#28857;&#20987;&#29575;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#21644;&#20154;&#31867;&#35780;&#20272;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#24819;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In industry deep learning application, our manually labeled data has a certain number of noisy data. To solve this problem and achieve more than 90 score in dev dataset, we present a simple method to find the noisy data and re-label the noisy data by human, given the model predictions as references in human labeling. In this paper, we illustrate our idea for a broad set of deep learning tasks, includes classification, sequence tagging, object detection, sequence generation, click-through rate prediction. The experimental results and human evaluation results verify our idea.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;GFlowNets&#30340;&#39640;&#25928;&#22810;&#30446;&#26631;&#20998;&#23376;&#20248;&#21270;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#20351;&#29992;&#36229;&#32593;&#32476;&#26469;&#20248;&#21270;&#25910;&#30410;&#20989;&#25968;&#65292;&#20174;&#32780;&#22312;&#32771;&#34385;&#22810;&#26679;&#24615;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#20174;&#36817;&#20284;&#24085;&#32047;&#25176;&#21069;&#27839;&#20013;&#37319;&#26679;&#20986;&#22810;&#26679;&#21270;&#30340;&#20505;&#36873;&#20998;&#23376;&#22270;&#12290;&#21516;&#26102;&#36824;&#20351;&#29992;&#20102;&#19968;&#31181;&#31867;&#20284;&#20110;&#20107;&#21518;&#35748;&#35782;&#30340;&#31163;&#32447;&#31574;&#30053;&#26469;&#21152;&#24555;&#20248;&#21270;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2302.04040</link><description>&lt;p&gt;
&#29992;GFlowNets&#23454;&#29616;&#39640;&#25928;&#30340;&#22810;&#30446;&#26631;&#20998;&#23376;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Sample-efficient Multi-objective Molecular Optimization with GFlowNets. (arXiv:2302.04040v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04040
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;GFlowNets&#30340;&#39640;&#25928;&#22810;&#30446;&#26631;&#20998;&#23376;&#20248;&#21270;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#20351;&#29992;&#36229;&#32593;&#32476;&#26469;&#20248;&#21270;&#25910;&#30410;&#20989;&#25968;&#65292;&#20174;&#32780;&#22312;&#32771;&#34385;&#22810;&#26679;&#24615;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#20174;&#36817;&#20284;&#24085;&#32047;&#25176;&#21069;&#27839;&#20013;&#37319;&#26679;&#20986;&#22810;&#26679;&#21270;&#30340;&#20505;&#36873;&#20998;&#23376;&#22270;&#12290;&#21516;&#26102;&#36824;&#20351;&#29992;&#20102;&#19968;&#31181;&#31867;&#20284;&#20110;&#20107;&#21518;&#35748;&#35782;&#30340;&#31163;&#32447;&#31574;&#30053;&#26469;&#21152;&#24555;&#20248;&#21270;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#20851;&#38190;&#30340;&#31185;&#23398;&#38382;&#39064;&#28041;&#21450;&#35774;&#35745;&#20855;&#26377;&#25152;&#38656;&#23646;&#24615;&#30340;&#26032;&#22411;&#20998;&#23376;&#65292;&#36825;&#21487;&#20197;&#34987;&#24418;&#24335;&#21270;&#20026;&#22312;&#31163;&#25955;&#21270;&#30340;&#21270;&#23398;&#31354;&#38388;&#19978;&#30340;&#40657;&#30418;&#20248;&#21270;&#38382;&#39064;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#22810;&#20010;&#20914;&#31361;&#30340;&#30446;&#26631;&#21644;&#26114;&#36149;&#30340;&#35780;&#20272;&#65288;&#20363;&#22914;&#28287;&#23454;&#39564;&#65289;&#20351;&#24471;&#20505;&#36873;&#20154;&#30340;&#22810;&#26679;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#35745;&#31639;&#26041;&#27861;&#24050;&#32463;&#21462;&#24471;&#20102;&#21021;&#27493;&#30340;&#25104;&#21151;&#65292;&#20294;&#20173;&#28982;&#22312;&#21516;&#26102;&#32771;&#34385;&#30446;&#26631;&#21644;&#25628;&#32034;&#31354;&#38388;&#30340;&#22810;&#26679;&#24615;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#30446;&#26631;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;MOBO&#65289;&#31639;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;&#36229;&#32593;&#32476;&#30340;GFlowNets&#65288;HN-GFN&#65289;&#20316;&#20026;&#25910;&#30410;&#20989;&#25968;&#20248;&#21270;&#22120;&#65292;&#30446;&#30340;&#26159;&#20174;&#36817;&#20284;&#24085;&#32047;&#25176;&#21069;&#27839;&#20013;&#37319;&#26679;&#20986;&#22810;&#26679;&#21270;&#30340;&#20505;&#36873;&#20998;&#23376;&#22270;&#12290;&#20351;&#29992;&#21333;&#19968;&#30340;&#26465;&#20214;&#21270;&#36229;&#32593;&#32476;&#65292;HN-GFN&#23398;&#20064;&#25506;&#32034;&#21508;&#20010;&#30446;&#26631;&#20043;&#38388;&#30340;&#21508;&#31181;&#25240;&#20013;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#31867;&#20284;&#20110;&#20107;&#21518;&#35748;&#35782;&#30340;&#31163;&#32447;&#31574;&#30053;&#65292;&#20197;&#20415;&#22312;&#19981;&#21516;&#20559;&#22909;&#20043;&#38388;&#20849;&#20139;&#39640;&#24615;&#33021;&#20998;&#23376;&#65292;&#20197;&#21152;&#24555;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many crucial scientific problems involve designing novel molecules with desired properties, which can be formulated as a black-box optimization problem over the discrete chemical space. In practice, multiple conflicting objectives and costly evaluations (e.g., wet-lab experiments) make the diversity of candidates paramount. Computational methods have achieved initial success but still struggle with considering diversity in both objective and search space. To fill this gap, we propose a multi-objective Bayesian optimization (MOBO) algorithm leveraging the hypernetwork-based GFlowNets (HN-GFN) as an acquisition function optimizer, with the purpose of sampling a diverse batch of candidate molecular graphs from an approximate Pareto front. Using a single preference-conditioned hypernetwork, HN-GFN learns to explore various trade-offs between objectives. We further propose a hindsight-like off-policy strategy to share high-performing molecules among different preferences in order to speed u
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#26497;&#20540;&#20256;&#36755;(ET)&#8221;&#30340;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#36827;&#34892;&#32473;&#23450;&#30456;&#20284;&#24615;&#20989;&#25968;&#19979;&#30340;&#19968;&#23545;&#22495;&#20043;&#38388;&#30340;&#26368;&#20339;&#21487;&#33021;&#30340;&#38750;&#37197;&#23545;&#32763;&#35793;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;&#31070;&#32463;&#26368;&#20248;&#36755;&#36816;(OT)&#30340;&#31639;&#27861;&#26469;&#36924;&#36817;ET&#26144;&#23556;&#12290;</title><link>http://arxiv.org/abs/2301.12874</link><description>&lt;p&gt;
&#31070;&#32463;&#26368;&#20248;&#36755;&#36816;&#19979;&#30340;&#26497;&#20540;&#22495;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Extremal Domain Translation with Neural Optimal Transport. (arXiv:2301.12874v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12874
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#26497;&#20540;&#20256;&#36755;(ET)&#8221;&#30340;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#36827;&#34892;&#32473;&#23450;&#30456;&#20284;&#24615;&#20989;&#25968;&#19979;&#30340;&#19968;&#23545;&#22495;&#20043;&#38388;&#30340;&#26368;&#20339;&#21487;&#33021;&#30340;&#38750;&#37197;&#23545;&#32763;&#35793;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;&#31070;&#32463;&#26368;&#20248;&#36755;&#36816;(OT)&#30340;&#31639;&#27861;&#26469;&#36924;&#36817;ET&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#26497;&#20540;&#20256;&#36755;(ET)&#8221;&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#23545;&#20110;&#32473;&#23450;&#30456;&#20284;&#24615;&#20989;&#25968;&#30340;&#19968;&#23545;&#22495;&#20043;&#38388;&#30340;&#26368;&#20339;&#21487;&#33021;&#30340;&#38750;&#37197;&#23545;&#32763;&#35793;&#30340;&#25968;&#23398;&#24418;&#24335;&#21270;&#12290;&#21463;&#21040;&#31070;&#32463;&#26368;&#20248;&#36755;&#36816;(OT)&#36817;&#26399;&#21457;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#31639;&#27861;&#65292;&#20197;OT&#30340;&#37096;&#20998;&#26144;&#23556;&#26497;&#38480;&#26469;&#36924;&#36817;ET&#26144;&#23556;&#12290;&#25105;&#20204;&#22312;&#29609;&#20855;&#23454;&#20363;&#21644;&#38750;&#37197;&#23545;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#32763;&#35793;&#20219;&#21153;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the extremal transport (ET) which is a mathematical formalization of the theoretically best possible unpaired translation between a pair of domains w.r.t. the given similarity function. Inspired by the recent advances in neural optimal transport (OT), we propose a scalable algorithm to approximate ET maps as a limit of partial OT maps. We test our algorithm on toy examples and on the unpaired image-to-image translation task.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#23383;&#23402;&#29983;&#30340;&#22240;&#26524;&#20266;&#35777;&#26041;&#27861;&#65292;&#20197;&#21487;&#38752;&#24182;&#23454;&#29992;&#30340;&#26041;&#24335;&#22312;&#26368;&#23567;&#38480;&#24230;&#30340;&#20551;&#35774;&#19979;&#25552;&#20379;&#23402;&#29983;&#30340;&#20449;&#24687;&#21644;&#35780;&#20272;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2301.07210</link><description>&lt;p&gt;
&#25968;&#23383;&#23402;&#29983;&#30340;&#22240;&#26524;&#20266;&#35777;
&lt;/p&gt;
&lt;p&gt;
Causal Falsification of Digital Twins. (arXiv:2301.07210v3 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07210
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#23383;&#23402;&#29983;&#30340;&#22240;&#26524;&#20266;&#35777;&#26041;&#27861;&#65292;&#20197;&#21487;&#38752;&#24182;&#23454;&#29992;&#30340;&#26041;&#24335;&#22312;&#26368;&#23567;&#38480;&#24230;&#30340;&#20551;&#35774;&#19979;&#25552;&#20379;&#23402;&#29983;&#30340;&#20449;&#24687;&#21644;&#35780;&#20272;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#23402;&#29983;&#22312;&#24456;&#22810;&#24212;&#29992;&#20013;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#26159;&#22312;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#19979;&#24191;&#27867;&#37096;&#32626;&#23427;&#20204;&#30340;&#31934;&#24230;&#35780;&#20272;&#38656;&#35201;&#20005;&#26684;&#30340;&#31243;&#24207;&#12290;&#36890;&#36807;&#22312;&#22240;&#26524;&#25512;&#29702;&#26694;&#26550;&#20869;&#21046;&#23450;&#36825;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#20351;&#29992;&#29616;&#23454;&#25968;&#25454;&#23581;&#35797;&#35777;&#26126;&#23402;&#29983;&#30340;&#27491;&#30830;&#24615;&#26159;&#19981;&#21487;&#38752;&#30340;&#65292;&#38500;&#38750;&#22312;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#36827;&#34892;&#21487;&#33021;&#26377;&#39118;&#38505;&#30340;&#20551;&#35774;&#12290;&#20026;&#20102;&#36991;&#20813;&#36825;&#20123;&#20551;&#35774;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#31574;&#30053;&#65292;&#26088;&#22312;&#25214;&#21040;&#23402;&#29983;&#19981;&#27491;&#30830;&#30340;&#24773;&#20917;&#65292;&#24182;&#25552;&#20986;&#20102;&#29992;&#20110;&#23454;&#29616;&#27492;&#30446;&#26631;&#30340;&#36890;&#29992;&#32479;&#35745;&#36807;&#31243;&#65292;&#21487;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#21644;&#23402;&#29983;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26368;&#23567;&#20551;&#35774;&#19979;&#25552;&#20379;&#20102;&#21487;&#38752;&#21644;&#21487;&#25805;&#20316;&#30340;&#23402;&#29983;&#20449;&#24687;&#21644;&#35780;&#20272;&#32467;&#26524;&#12290;&#36890;&#36807;&#21253;&#21547;&#33033;&#20914;&#29983;&#29702;&#23398;&#24341;&#25806;&#20013;&#33043;&#27602;&#30151;&#24314;&#27169;&#30340;&#22823;&#22411;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Digital twins hold substantial promise in many applications, but rigorous procedures for assessing their accuracy are essential for their widespread deployment in safety-critical settings. By formulating this task within the framework of causal inference, we show that attempts to certify the correctness of a twin using real-world observational data are unsound unless potentially tenuous assumptions are made about the data-generating process. To avoid these assumptions, we propose an assessment strategy that instead aims to find cases where the twin is not correct, and present a general-purpose statistical procedure for doing so that may be used across a wide variety of applications and twin models. Our approach yields reliable and actionable information about the twin under minimal assumptions about the twin and the real-world process of interest. We demonstrate the effectiveness of our methodology via a large-scale case study involving sepsis modelling within the Pulse Physiology Engi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EPGP&#30340;&#39640;&#26031;&#36807;&#31243;&#20808;&#39564;&#65292;&#29992;&#20110;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#31995;&#32479;&#65292;&#24182;&#19988;&#26500;&#36896;&#20102;&#21453;&#26144;&#26631;&#20934;&#35889;&#26041;&#27861;&#30340;GP&#26680;&#20989;&#25968;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#25512;&#26029;&#32447;&#24615;PDE&#31995;&#32479;&#30340;&#21487;&#33021;&#35299;&#65292;&#24182;&#20855;&#26377;&#31639;&#27861;&#24615;&#24378;&#12289;&#26222;&#36866;&#24615;&#24191;&#12289;&#36866;&#29992;&#20110;&#22823;&#25968;&#25454;&#38598;&#30340;&#31232;&#30095;&#29256;&#26412;&#12290;</title><link>http://arxiv.org/abs/2212.14319</link><description>&lt;p&gt;
&#31995;&#32479;&#30340;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#39640;&#26031;&#36807;&#31243;&#20808;&#39564;&#19982;&#24120;&#31995;&#25968;&#65288;&#32763;&#35793;&#33258;arXiv:2212.14319v3 [stat.ML] &#26356;&#26032;&#65289;
&lt;/p&gt;
&lt;p&gt;
Gaussian Process Priors for Systems of Linear Partial Differential Equations with Constant Coefficients. (arXiv:2212.14319v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.14319
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EPGP&#30340;&#39640;&#26031;&#36807;&#31243;&#20808;&#39564;&#65292;&#29992;&#20110;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#31995;&#32479;&#65292;&#24182;&#19988;&#26500;&#36896;&#20102;&#21453;&#26144;&#26631;&#20934;&#35889;&#26041;&#27861;&#30340;GP&#26680;&#20989;&#25968;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#25512;&#26029;&#32447;&#24615;PDE&#31995;&#32479;&#30340;&#21487;&#33021;&#35299;&#65292;&#24182;&#20855;&#26377;&#31639;&#27861;&#24615;&#24378;&#12289;&#26222;&#36866;&#24615;&#24191;&#12289;&#36866;&#29992;&#20110;&#22823;&#25968;&#25454;&#38598;&#30340;&#31232;&#30095;&#29256;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#26159;&#24314;&#27169;&#29289;&#29702;&#31995;&#32479;&#30340;&#37325;&#35201;&#24037;&#20855;&#65292;&#23558;&#23427;&#20204;&#32435;&#20837;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26159;&#23558;&#29289;&#29702;&#30693;&#35782;&#32435;&#20837;&#30340;&#37325;&#35201;&#26041;&#24335;&#12290;&#23545;&#20110;&#20219;&#20309;&#20855;&#26377;&#24120;&#31995;&#25968;&#30340;&#32447;&#24615;PDE&#31995;&#32479;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#26063;&#31216;&#20026;EPGP&#30340;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#20808;&#39564;&#65292;&#20351;&#24471;&#25152;&#26377;&#23454;&#29616;&#37117;&#26159;&#35813;&#31995;&#32479;&#30340;&#31934;&#30830;&#35299;&#12290;&#25105;&#20204;&#24212;&#29992;Ehrenpreis-Palamodov&#22522;&#26412;&#21407;&#29702;&#65292;&#23427;&#20316;&#20026;&#19968;&#31181;&#38750;&#32447;&#24615;&#20613;&#37324;&#21494;&#21464;&#25442;&#65292;&#26500;&#24314;&#20102;GP&#26680;&#20989;&#25968;&#65292;&#21453;&#26144;&#20102;&#26631;&#20934;&#30340;&#35889;&#26041;&#27861;&#29992;&#20110;GP&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20174;&#20219;&#20309;&#25968;&#25454;&#65288;&#22914;&#26377;&#22122;&#22768;&#30340;&#27979;&#37327;&#25968;&#25454;&#25110;&#28857;&#23450;&#20041;&#30340;&#21021;&#22987;&#21644;&#36793;&#30028;&#26465;&#20214;&#65289;&#25512;&#26029;&#32447;&#24615;PDE&#31995;&#32479;&#30340;&#21487;&#33021;&#35299;&#12290;&#26500;&#36896;EPGP&#20808;&#39564;&#30340;&#31639;&#27861;&#24615;&#24378;&#65292;&#26222;&#36866;&#24615;&#24191;&#65292;&#24182;&#19988;&#26377;&#19968;&#20010;&#31232;&#30095;&#29256;&#26412;&#65288;S-EPGP&#65289;&#65292;&#21487;&#20197;&#23398;&#20064;&#30456;&#20851;&#30340;&#35889;&#39057;&#29575;&#65292;&#24182;&#22312;&#22823;&#25968;&#25454;&#38598;&#19978;&#36816;&#34892;&#25928;&#26524;&#26356;&#22909;&#12290;&#25105;&#20204;&#22312;&#19977;&#31867;PDE&#31995;&#32479;&#19978;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#28909;&#26041;&#31243;&#21644;&#27874;&#26041;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Partial differential equations (PDEs) are important tools to model physical systems and including them into machine learning models is an important way of incorporating physical knowledge. Given any system of linear PDEs with constant coefficients, we propose a family of Gaussian process (GP) priors, which we call EPGP, such that all realizations are exact solutions of this system. We apply the Ehrenpreis-Palamodov fundamental principle, which works as a non-linear Fourier transform, to construct GP kernels mirroring standard spectral methods for GPs. Our approach can infer probable solutions of linear PDE systems from any data such as noisy measurements, or pointwise defined initial and boundary conditions. Constructing EPGP-priors is algorithmic, generally applicable, and comes with a sparse version (S-EPGP) that learns the relevant spectral frequencies and works better for big data sets. We demonstrate our approach on three families of systems of PDEs, the heat equation, wave equati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35782;&#21035;&#32593;&#32476;&#22914;&#20309;&#27169;&#25311;&#30495;&#23454;&#21518;&#39564;&#20998;&#24067;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#65292;&#36890;&#36807;&#23548;&#20986;&#20840;&#23616;&#26465;&#20214;&#21644;&#23616;&#37096;&#26465;&#20214;&#65292;&#21457;&#29616;&#23436;&#32654;&#24615;&#20026;&#20854;&#20855;&#22791;&#26399;&#26395;&#24615;&#36136;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2212.10649</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#36870;&#25512;
&lt;/p&gt;
&lt;p&gt;
Inversion of Bayesian Networks. (arXiv:2212.10649v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35782;&#21035;&#32593;&#32476;&#22914;&#20309;&#27169;&#25311;&#30495;&#23454;&#21518;&#39564;&#20998;&#24067;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#65292;&#36890;&#36807;&#23548;&#20986;&#20840;&#23616;&#26465;&#20214;&#21644;&#23616;&#37096;&#26465;&#20214;&#65292;&#21457;&#29616;&#23436;&#32654;&#24615;&#20026;&#20854;&#20855;&#22791;&#26399;&#26395;&#24615;&#36136;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#21644;Helmholtz&#26426;&#20351;&#29992;&#19968;&#20010;&#35782;&#21035;&#32593;&#32476;&#65288;&#32534;&#30721;&#22120;&#65289;&#26469;&#36817;&#20284;&#29983;&#25104;&#27169;&#22411;&#65288;&#35299;&#30721;&#22120;&#65289;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#35782;&#21035;&#32593;&#32476;&#20855;&#22791;&#27169;&#25311;&#30495;&#23454;&#21518;&#39564;&#20998;&#24067;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#12290;&#36825;&#20123;&#32467;&#26524;&#22522;&#20110;&#27010;&#29575;&#22270;&#27169;&#22411;&#65295;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#19968;&#33324;&#32972;&#26223;&#65292;&#20854;&#20013;&#32593;&#32476;&#20195;&#34920;&#20102;&#19968;&#32452;&#26465;&#20214;&#29420;&#31435;&#24615;&#35821;&#21477;&#12290;&#25105;&#20204;&#23548;&#20986;&#20102;&#20840;&#23616;&#26465;&#20214;&#65288;&#36890;&#36807;d-&#20998;&#31163;&#65289;&#21644;&#23616;&#37096;&#26465;&#20214;&#65292;&#20351;&#24471;&#35782;&#21035;&#32593;&#32476;&#20855;&#22791;&#26399;&#26395;&#30340;&#24615;&#36136;&#12290;&#23616;&#37096;&#26465;&#20214;&#20013;&#65292;&#23436;&#32654;&#24615;&#65288;&#27599;&#20010;&#33410;&#28857;&#21482;&#19982;&#20854;&#29238;&#33410;&#28857;&#30456;&#36830;&#65289;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational autoencoders and Helmholtz machines use a recognition network (encoder) to approximate the posterior distribution of a generative model (decoder). In this paper we study the necessary and sufficient properties of a recognition network so that it can model the true posterior distribution exactly. These results are derived in the general context of probabilistic graphical modelling / Bayesian networks, for which the network represents a set of conditional independence statements. We derive both global conditions, in terms of d-separation, and local conditions for the recognition network to have the desired qualities. It turns out that for the local conditions the property perfectness (for every node, all parents are joined) plays an important role.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#27668;&#20505;&#27169;&#22411;&#38598;&#21512;&#36827;&#34892;&#22320;&#34920;&#23395;&#33410;&#24615;&#39044;&#27979;&#30340;&#24212;&#29992;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#24179;&#22343;&#26041;&#27861;&#65292;&#21033;&#29992;&#38598;&#21512;&#39044;&#27979;&#20013;&#30340;&#20449;&#24687;&#25552;&#39640;&#20102;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#20851;&#27880;&#20102;&#26497;&#31471;&#20107;&#20214;&#30340;&#39044;&#27979;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#31354;&#38388;&#21464;&#21270;&#30340;&#39044;&#27979;&#38598;&#21512;&#12290;</title><link>http://arxiv.org/abs/2211.15856</link><description>&lt;p&gt;
&#36229;&#36234;&#38598;&#21512;&#24179;&#22343;&#20540;&#65306;&#21033;&#29992;&#27668;&#20505;&#27169;&#22411;&#38598;&#21512;&#36827;&#34892;&#22320;&#34920;&#23395;&#33410;&#24615;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Beyond Ensemble Averages: Leveraging Climate Model Ensembles for Subseasonal Forecasting. (arXiv:2211.15856v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15856
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#27668;&#20505;&#27169;&#22411;&#38598;&#21512;&#36827;&#34892;&#22320;&#34920;&#23395;&#33410;&#24615;&#39044;&#27979;&#30340;&#24212;&#29992;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#24179;&#22343;&#26041;&#27861;&#65292;&#21033;&#29992;&#38598;&#21512;&#39044;&#27979;&#20013;&#30340;&#20449;&#24687;&#25552;&#39640;&#20102;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#20851;&#27880;&#20102;&#26497;&#31471;&#20107;&#20214;&#30340;&#39044;&#27979;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#31354;&#38388;&#21464;&#21270;&#30340;&#39044;&#27979;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25805;&#20316;&#24615;&#39044;&#27979;&#20013;&#65292;&#23545;&#20110;&#20851;&#38190;&#27668;&#20505;&#21464;&#37327;&#65288;&#22914;&#28201;&#24230;&#21644;&#38477;&#27700;&#65289;&#36827;&#34892;&#39640;&#36136;&#37327;&#30340;&#22320;&#34920;&#23395;&#33410;&#24615;&#26102;&#38388;&#23610;&#24230;&#39044;&#27979;&#19968;&#30452;&#23384;&#22312;&#30528;&#24046;&#36317;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#26469;&#25512;&#36827;&#22320;&#34920;&#23395;&#33410;&#24615;&#39044;&#27979;&#65288;SSF&#65289;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#20173;&#23384;&#22312;&#19968;&#20123;&#26410;&#35299;&#30340;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#36807;&#21435;&#30340;&#26041;&#27861;&#20013;&#20351;&#29992;&#20102;&#29289;&#29702;&#22522;&#20110;&#27169;&#22411;&#38598;&#21512;&#30340;&#24179;&#22343;&#20316;&#20026;&#36825;&#20123;&#27169;&#22411;&#30340;&#36755;&#20837;&#29305;&#24449;&#65292;&#28982;&#32780;&#38598;&#21512;&#39044;&#27979;&#20013;&#21253;&#21547;&#20102;&#21487;&#20197;&#24110;&#21161;&#39044;&#27979;&#30340;&#20449;&#24687;&#65292;&#19981;&#20165;&#20165;&#26159;&#38598;&#21512;&#22343;&#20540;&#12290;&#20854;&#27425;&#65292;&#36807;&#21435;&#30340;&#26041;&#27861;&#20851;&#27880;&#24179;&#22343;&#24615;&#33021;&#65292;&#28982;&#32780;&#23545;&#20110;&#35745;&#21010;&#21644;&#20943;&#28798;&#30446;&#30340;&#26469;&#35828;&#65292;&#26497;&#31471;&#20107;&#20214;&#30340;&#39044;&#27979;&#26356;&#21152;&#37325;&#35201;&#12290;&#31532;&#19977;&#65292;&#27668;&#20505;&#39044;&#27979;&#23545;&#24212;&#20110;&#19968;&#20010;&#31354;&#38388;&#21464;&#21270;&#30340;&#39044;&#27979;&#38598;&#21512;&#65292;&#32780;&#19981;&#21516;&#30340;&#26041;&#27861;&#20197;&#19981;&#21516;&#30340;&#26041;&#24335;&#32771;&#34385;&#20102;&#21709;&#24212;&#30340;&#31354;&#38388;&#21487;&#21464;&#24615;&#12290;&#27169;&#22411;&#22534;&#21472;&#21487;&#20197;&#32531;&#35299;&#19981;&#21516;&#26041;&#27861;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#21033;&#29992;&#27169;&#22411;&#38598;&#21512;&#36827;&#34892;&#22320;&#34920;&#23395;&#33410;&#24615;&#39044;&#27979;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Producing high-quality forecasts of key climate variables such as temperature and precipitation on subseasonal time scales has long been a gap in operational forecasting. Recent studies have shown promising results using machine learning (ML) models to advance subseasonal forecasting (SSF), but several open questions remain. First, several past approaches use the average of an ensemble of physics-based forecasts as an input feature of these models. However, ensemble forecasts contain information that can aid prediction beyond only the ensemble mean. Second, past methods have focused on average performance, whereas forecasts of extreme events are far more important for planning and mitigation purposes. Third, climate forecasts correspond to a spatially-varying collection of forecasts, and different methods account for spatial variability in the response differently. Trade-offs between different approaches may be mitigated with model stacking. This paper describes the application of a va
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20998;&#23618;&#20195;&#29702;&#24314;&#27169;&#30340;&#25216;&#26415;H-Pro&#65292;&#36890;&#36807;&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#36890;&#36807;&#27979;&#35797;&#20195;&#29702;&#39537;&#21160;&#36229;&#21442;&#25968;&#20248;&#21270;&#65292;&#20197;&#35299;&#20915;&#27979;&#35797;&#39564;&#35777;&#26399;&#38388;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#65292;&#24182;&#39564;&#35777;&#20102;&#35813;&#25216;&#26415;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.15092</link><description>&lt;p&gt;
&#25552;&#21319;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#20998;&#23618;&#20195;&#29702;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Proxy Modeling for Improved HPO in Time Series Forecasting. (arXiv:2211.15092v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15092
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20998;&#23618;&#20195;&#29702;&#24314;&#27169;&#30340;&#25216;&#26415;H-Pro&#65292;&#36890;&#36807;&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#36890;&#36807;&#27979;&#35797;&#20195;&#29702;&#39537;&#21160;&#36229;&#21442;&#25968;&#20248;&#21270;&#65292;&#20197;&#35299;&#20915;&#27979;&#35797;&#39564;&#35777;&#26399;&#38388;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#65292;&#24182;&#39564;&#35777;&#20102;&#35813;&#25216;&#26415;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#65292;&#36873;&#25321;&#27491;&#30830;&#30340;&#36229;&#21442;&#25968;&#38598;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#20256;&#32479;&#30340;&#26102;&#38388;&#20132;&#21449;&#39564;&#35777;&#26694;&#26550;&#22312;&#36229;&#21442;&#25968;&#20248;&#21270;&#20013;&#32463;&#24120;&#23548;&#33268;&#27979;&#35797;&#24615;&#33021;&#24046;&#65292;&#22240;&#20026;&#39564;&#35777;&#21644;&#27979;&#35797;&#26399;&#38388;&#21487;&#33021;&#23384;&#22312;&#19981;&#21305;&#37197;&#30340;&#24773;&#20917;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#27979;&#35797;-&#39564;&#35777;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#21363;H-Pro&#65292;&#36890;&#36807;&#21033;&#29992;&#19982;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#32463;&#24120;&#30456;&#20851;&#30340;&#25968;&#25454;&#23618;&#27425;&#32467;&#26500;&#65292;&#36890;&#36807;&#27979;&#35797;&#20195;&#29702;&#26469;&#39537;&#21160;HPO&#12290;&#30001;&#20110;&#39640;&#23618;&#27425;&#30340;&#32858;&#21512;&#26102;&#38388;&#24207;&#21015;&#36890;&#24120;&#26174;&#31034;&#20986;&#36739;&#23569;&#30340;&#19981;&#35268;&#21017;&#24615;&#21644;&#26356;&#22909;&#30340;&#21487;&#39044;&#27979;&#24615;&#65292;&#30456;&#27604;&#20043;&#19979;&#26368;&#20302;&#23618;&#27425;&#30340;&#26102;&#38388;&#24207;&#21015;&#21487;&#33021;&#26159;&#31232;&#30095;&#21644;&#38388;&#27463;&#24615;&#30340;&#65292;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#39640;&#23618;&#27425;&#39044;&#27979;&#22120;&#29983;&#25104;&#30340;&#27979;&#35797;&#26399;&#38388;&#30340;&#20195;&#29702;&#39044;&#27979;&#32467;&#26524;&#26469;&#20248;&#21270;&#26368;&#20302;&#23618;&#27425;&#30340;&#22522;&#26412;&#39044;&#27979;&#22120;&#30340;&#36229;&#21442;&#25968;&#12290;H-Pro&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#29616;&#25104;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;HPO&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#20116;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#20998;&#23618;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#32463;&#39564;&#35780;&#20272;&#65292;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Selecting the right set of hyperparameters is crucial in time series forecasting. The classical temporal cross-validation framework for hyperparameter optimization (HPO) often leads to poor test performance because of a possible mismatch between validation and test periods. To address this test-validation mismatch, we propose a novel technique, H-Pro to drive HPO via test proxies by exploiting data hierarchies often associated with time series datasets. Since higher-level aggregated time series often show less irregularity and better predictability as compared to the lowest-level time series which can be sparse and intermittent, we optimize the hyperparameters of the lowest-level base-forecaster by leveraging the proxy forecasts for the test period generated from the forecasters at higher levels. H-Pro can be applied on any off-the-shelf machine learning model to perform HPO. We validate the efficacy of our technique with extensive empirical evaluation on five publicly available hierar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;Stein&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#26377;&#38480;&#31890;&#23376;&#25910;&#25947;&#36895;&#24230;&#65292;&#35777;&#26126;&#20102;&#24403;&#30446;&#26631;&#20998;&#24067;&#20026;&#27425;&#39640;&#26031;&#19988;&#20855;&#26377;Lipschitz&#31215;&#20998;&#26680;&#26102;&#65292;&#20351;&#29992;&#36866;&#24403;&#30340;&#27493;&#38271;&#24207;&#21015;&#21644;&#31890;&#23376;&#25968;&#37327;&#65292;&#21487;&#20197;&#20197;1/&#8730;(log log n)&#30340;&#36895;&#24230;&#23558;&#26680;Stein&#24046;&#24322;&#36924;&#36817;&#38646;&#12290;</title><link>http://arxiv.org/abs/2211.09721</link><description>&lt;p&gt;
&#12298;Stein&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#30340;&#26377;&#38480;&#31890;&#23376;&#25910;&#25947;&#36895;&#24230;&#12299;
&lt;/p&gt;
&lt;p&gt;
A Finite-Particle Convergence Rate for Stein Variational Gradient Descent. (arXiv:2211.09721v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09721
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;Stein&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#26377;&#38480;&#31890;&#23376;&#25910;&#25947;&#36895;&#24230;&#65292;&#35777;&#26126;&#20102;&#24403;&#30446;&#26631;&#20998;&#24067;&#20026;&#27425;&#39640;&#26031;&#19988;&#20855;&#26377;Lipschitz&#31215;&#20998;&#26680;&#26102;&#65292;&#20351;&#29992;&#36866;&#24403;&#30340;&#27493;&#38271;&#24207;&#21015;&#21644;&#31890;&#23376;&#25968;&#37327;&#65292;&#21487;&#20197;&#20197;1/&#8730;(log log n)&#30340;&#36895;&#24230;&#23558;&#26680;Stein&#24046;&#24322;&#36924;&#36817;&#38646;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#39318;&#27425;&#25552;&#20379;&#20102;Stein&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#65288;SVGD&#65289;&#30340;&#26377;&#38480;&#31890;&#23376;&#25910;&#25947;&#36895;&#24230;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#19968;&#32452;&#31890;&#23376;&#36924;&#36817;&#27010;&#29575;&#20998;&#24067;&#30340;&#27969;&#34892;&#31639;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#21482;&#35201;&#30446;&#26631;&#20998;&#24067;&#26159;&#27425;&#39640;&#26031;&#30340;&#65292;&#24182;&#19988;&#20855;&#26377;Lipschitz&#31215;&#20998;&#26680;&#65292;&#20351;&#29992;n&#20010;&#31890;&#23376;&#21644;&#36866;&#24403;&#30340;&#27493;&#38271;&#24207;&#21015;&#36827;&#34892;SVGD&#65292;&#26680;Stein&#24046;&#24322;&#23558;&#20197;1/&#8730;(log log n)&#30340;&#36895;&#24230;&#36235;&#20110;&#38646;&#12290;&#25105;&#20204;&#24576;&#30097;n&#30340;&#20381;&#36182;&#24615;&#21487;&#20197;&#25913;&#36827;&#65292;&#24076;&#26395;&#25105;&#20204;&#30340;&#26126;&#30830;&#30340;&#38750;&#28176;&#36817;&#35777;&#26126;&#31574;&#30053;&#33021;&#20026;&#26410;&#26469;&#30340;&#25913;&#36827;&#25552;&#20379;&#27169;&#26495;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide the first finite-particle convergence rate for Stein variational gradient descent (SVGD), a popular algorithm for approximating a probability distribution with a collection of particles. Specifically, whenever the target distribution is sub-Gaussian with a Lipschitz score, SVGD with n particles and an appropriate step size sequence drives the kernel Stein discrepancy to zero at an order 1/sqrt(log log n) rate. We suspect that the dependence on n can be improved, and we hope that our explicit, non-asymptotic proof strategy will serve as a template for future refinements.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#31185;&#23398;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#23588;&#20854;&#26159;&#22312;&#31163;&#32676;&#26679;&#26412;&#26816;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#25968;&#25454;&#26222;&#36866;&#24615;&#12289;&#23454;&#39564;&#21327;&#35758;&#21644;&#27169;&#22411;&#40065;&#26834;&#24615;&#31561;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2210.13441</link><description>&lt;p&gt;
&#25645;&#24314;&#26426;&#22120;&#23398;&#20064;&#19982;&#31185;&#23398;&#30340;&#26725;&#26753;&#65306;&#26426;&#36935;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Bridging Machine Learning and Sciences: Opportunities and Challenges. (arXiv:2210.13441v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13441
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#31185;&#23398;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#23588;&#20854;&#26159;&#22312;&#31163;&#32676;&#26679;&#26412;&#26816;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#25968;&#25454;&#26222;&#36866;&#24615;&#12289;&#23454;&#39564;&#21327;&#35758;&#21644;&#27169;&#22411;&#40065;&#26834;&#24615;&#31561;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#31185;&#23398;&#39046;&#22495;&#30340;&#24212;&#29992;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#20196;&#20154;&#25391;&#22859;&#30340;&#36827;&#23637;&#12290;&#20316;&#20026;&#19968;&#31181;&#24191;&#27867;&#36866;&#29992;&#30340;&#25216;&#26415;&#65292;&#24322;&#24120;&#26816;&#27979;&#22312;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#20013;&#19968;&#30452;&#21463;&#21040;&#20851;&#27880;&#12290;&#29305;&#21035;&#26159;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#31163;&#32676;&#26679;&#26412;&#26816;&#27979;&#22312;&#39640;&#32500;&#25968;&#25454;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#27493;&#12290;&#26368;&#36817;&#65292;&#36825;&#20123;&#25216;&#26415;&#23637;&#31034;&#20102;&#22312;&#31185;&#23398;&#23398;&#31185;&#20013;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#23545;&#23427;&#20204;&#22312;&#25968;&#25454;&#26222;&#36866;&#24615;&#12289;&#23454;&#39564;&#21327;&#35758;&#12289;&#27169;&#22411;&#40065;&#26834;&#24615;&#31561;&#26041;&#38754;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#25506;&#35752;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#21516;&#26102;&#23637;&#31034;&#20102;&#21487;&#36716;&#31227;&#23454;&#36341;&#21644;&#39046;&#22495;&#29305;&#23450;&#25361;&#25112;&#30340;&#31034;&#20363;&#65292;&#20026;&#22312;&#36817;&#26399;&#24314;&#31435;&#19968;&#20010;&#26032;&#30340;&#36328;&#23398;&#31185;&#30740;&#31350;&#33539;&#24335;&#25552;&#20379;&#20102;&#19968;&#20010;&#36215;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
The application of machine learning in sciences has seen exciting advances in recent years. As a widely applicable technique, anomaly detection has been long studied in the machine learning community. Especially, deep neural nets-based out-of-distribution detection has made great progress for high-dimensional data. Recently, these techniques have been showing their potential in scientific disciplines. We take a critical look at their applicative prospects including data universality, experimental protocols, model robustness, etc. We discuss examples that display transferable practices and domain-specific challenges simultaneously, providing a starting point for establishing a novel interdisciplinary research paradigm in the near future.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20998;&#26512;&#33310;&#36424;&#21160;&#20316;&#21644;&#29983;&#25104;&#26032;&#21160;&#20316;&#24207;&#21015;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#20063;&#32467;&#21512;&#20102;&#21069;&#20154;&#30340;&#21162;&#21147;&#26469;&#24320;&#21457;&#20986;&#19968;&#22871;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2210.04366</link><description>&lt;p&gt;
&#21033;&#29992;&#20154;&#20307;&#21160;&#20316;&#21512;&#25104;&#36827;&#34892;&#35745;&#31639;&#32534;&#33310;
&lt;/p&gt;
&lt;p&gt;
Computational Choreography using Human Motion Synthesis. (arXiv:2210.04366v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04366
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20998;&#26512;&#33310;&#36424;&#21160;&#20316;&#21644;&#29983;&#25104;&#26032;&#21160;&#20316;&#24207;&#21015;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#20063;&#32467;&#21512;&#20102;&#21069;&#20154;&#30340;&#21162;&#21147;&#26469;&#24320;&#21457;&#20986;&#19968;&#22871;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26159;&#21542;&#24212;&#35813;&#34987;&#35757;&#32451;&#26469;&#20998;&#26512;&#20154;&#20307;&#34920;&#28436;&#33402;&#26415;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21512;&#25104;&#33402;&#26415;&#20154;&#20307;&#21160;&#20316;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#20154;&#20307;&#36816;&#21160;&#21512;&#25104;&#20013;&#30340;&#38382;&#39064;&#20219;&#21153;&#21253;&#25324;&#39044;&#27979;&#37326;&#22806;&#29615;&#22659;&#20013;&#20154;&#20307;&#36816;&#21160;&#65292;&#20197;&#21450;&#29983;&#25104;&#22522;&#20110;&#36825;&#20123;&#39044;&#27979;&#30340;&#26032;&#21160;&#20316;&#24207;&#21015;&#12290;&#25105;&#20204;&#23558;&#35752;&#35770;&#19968;&#20010;&#38750;&#20256;&#32479;&#30340;&#24212;&#29992;&#28508;&#21147;&#65292;&#21363;&#23558;&#23398;&#20064;&#27169;&#22411;&#24212;&#29992;&#20110;&#39044;&#27979;&#33310;&#36424;&#21160;&#20316;&#12290;&#26368;&#36817;&#26377;&#19968;&#20123;&#26174;&#33879;&#30340;&#21162;&#21147;&#65292;&#20197;&#35745;&#31639;&#30340;&#26041;&#24335;&#20998;&#26512;&#33310;&#36424;&#21160;&#20316;&#65292;&#20363;&#22914;Everybody Dance Now&#65288;EDN&#65289;&#23398;&#20064;&#27169;&#22411;&#21644;Cal Poly&#30805;&#22763;&#35770;&#25991;Take The Lead&#65288;TTL&#65289;&#12290;&#25105;&#20204;&#26377;&#25928;&#22320;&#23558;&#36825;&#20004;&#20010;&#20316;&#21697;&#19982;&#25105;&#20204;&#33258;&#24049;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#32467;&#21512;&#36215;&#26469;&#65292;&#29983;&#25104;&#20102;&#19968;&#31181;&#26032;&#30340;&#33310;&#36424;&#21160;&#20316;&#39044;&#27979;&#31995;&#32479;&#12289;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#36716;&#25442;&#21644;&#35270;&#39057;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Should deep learning models be trained to analyze human performance art? To help answer this question, we explore an application of deep neural networks to synthesize artistic human motion. Problem tasks in human motion synthesis can include predicting the motions of humans in-the-wild, as well as generating new sequences of motions based on said predictions. We will discuss the potential of a less traditional application, where learning models are applied to predicting dance movements. There have been notable, recent efforts to analyze dance movements in a computational light, such as the Everybody Dance Now (EDN) learning model and a Cal Poly master's thesis, Take The Lead (TTL). We have effectively combined these two works along with our own deep neural network to produce a new system for dance motion prediction, image-to-image translation, and video generation.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26680;&#24046;&#24322;&#24230;&#37327;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#26032;&#30340;&#20805;&#20998;&#24517;&#35201;&#26465;&#20214;&#65292;&#23454;&#29616;&#20102;&#23558;&#30446;&#26631;&#20998;&#31163;&#20986;&#26469;&#65292;&#20197;&#21450;&#25511;&#21046;&#23545;&#30446;&#26631;&#30340;&#24369;&#25910;&#25947;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;$\mathbb{R}^d$&#19978;&#20351;&#29992;&#20102;&#36825;&#20123;&#32467;&#26524;&#26469;&#25193;&#23637;&#20102;&#26680;Stein&#24046;&#24322;&#20998;&#31163;&#21644;&#25910;&#25947;&#25511;&#21046;&#30340;&#24050;&#30693;&#26465;&#20214;&#65292;&#24182;&#24320;&#21457;&#20102;&#33021;&#22815;&#31934;&#30830;&#24230;&#37327;&#30446;&#26631;&#30340;&#24369;&#25910;&#25947;&#24615;&#30340;&#26680;&#24046;&#24322;&#24230;&#37327;&#12290;</title><link>http://arxiv.org/abs/2209.12835</link><description>&lt;p&gt;
&#36890;&#36807;&#26680;&#24046;&#24322;&#23454;&#29616;&#26377;&#38024;&#23545;&#24615;&#30340;&#20998;&#31163;&#19982;&#25910;&#25947;
&lt;/p&gt;
&lt;p&gt;
Targeted Separation and Convergence with Kernel Discrepancies. (arXiv:2209.12835v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.12835
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26680;&#24046;&#24322;&#24230;&#37327;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#26032;&#30340;&#20805;&#20998;&#24517;&#35201;&#26465;&#20214;&#65292;&#23454;&#29616;&#20102;&#23558;&#30446;&#26631;&#20998;&#31163;&#20986;&#26469;&#65292;&#20197;&#21450;&#25511;&#21046;&#23545;&#30446;&#26631;&#30340;&#24369;&#25910;&#25947;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;$\mathbb{R}^d$&#19978;&#20351;&#29992;&#20102;&#36825;&#20123;&#32467;&#26524;&#26469;&#25193;&#23637;&#20102;&#26680;Stein&#24046;&#24322;&#20998;&#31163;&#21644;&#25910;&#25947;&#25511;&#21046;&#30340;&#24050;&#30693;&#26465;&#20214;&#65292;&#24182;&#24320;&#21457;&#20102;&#33021;&#22815;&#31934;&#30830;&#24230;&#37327;&#30446;&#26631;&#30340;&#24369;&#25910;&#25947;&#24615;&#30340;&#26680;&#24046;&#24322;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#65288;MMDs&#65289;&#22914;&#26680;Stein&#24046;&#24322;&#65288;KSD&#65289;&#24050;&#32463;&#25104;&#20026;&#24191;&#27867;&#24212;&#29992;&#30340;&#20013;&#24515;&#65292;&#21253;&#25324;&#20551;&#35774;&#26816;&#39564;&#12289;&#37319;&#26679;&#22120;&#36873;&#25321;&#12289;&#20998;&#24067;&#36817;&#20284;&#21644;&#21464;&#20998;&#25512;&#26029;&#12290;&#22312;&#27599;&#20010;&#35774;&#32622;&#20013;&#65292;&#36825;&#20123;&#22522;&#20110;&#26680;&#30340;&#24046;&#24322;&#24230;&#37327;&#38656;&#35201;&#23454;&#29616;&#65288;i&#65289;&#23558;&#30446;&#26631;P&#19982;&#20854;&#20182;&#27010;&#29575;&#27979;&#24230;&#20998;&#31163;&#65292;&#29978;&#33267;&#65288;ii&#65289;&#25511;&#21046;&#23545;P&#30340;&#24369;&#25910;&#25947;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#30830;&#20445;&#65288;i&#65289;&#21644;&#65288;ii&#65289;&#30340;&#26032;&#30340;&#20805;&#20998;&#24517;&#35201;&#26465;&#20214;&#12290;&#23545;&#20110;&#21487;&#20998;&#30340;&#24230;&#37327;&#31354;&#38388;&#19978;&#30340;MMDs&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#20998;&#31163;Bochner&#21487;&#23884;&#20837;&#27979;&#24230;&#30340;&#26680;&#65292;&#24182;&#24341;&#20837;&#31616;&#21333;&#30340;&#26465;&#20214;&#26469;&#20998;&#31163;&#25152;&#26377;&#20855;&#26377;&#26080;&#30028;&#26680;&#30340;&#27979;&#24230;&#21644;&#29992;&#26377;&#30028;&#26680;&#26469;&#25511;&#21046;&#25910;&#25947;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#32467;&#26524;&#22312;$\mathbb{R}^d$&#19978;&#22823;&#22823;&#25193;&#23637;&#20102;KSD&#20998;&#31163;&#21644;&#25910;&#25947;&#25511;&#21046;&#30340;&#24050;&#30693;&#26465;&#20214;&#65292;&#24182;&#24320;&#21457;&#20102;&#39318;&#20010;&#33021;&#22815;&#31934;&#30830;&#24230;&#37327;&#23545;P&#30340;&#24369;&#25910;&#25947;&#30340;KSDs&#12290;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Maximum mean discrepancies (MMDs) like the kernel Stein discrepancy (KSD) have grown central to a wide range of applications, including hypothesis testing, sampler selection, distribution approximation, and variational inference. In each setting, these kernel-based discrepancy measures are required to (i) separate a target P from other probability measures or even (ii) control weak convergence to P. In this article we derive new sufficient and necessary conditions to ensure (i) and (ii). For MMDs on separable metric spaces, we characterize those kernels that separate Bochner embeddable measures and introduce simple conditions for separating all measures with unbounded kernels and for controlling convergence with bounded kernels. We use these results on $\mathbb{R}^d$ to substantially broaden the known conditions for KSD separation and convergence control and to develop the first KSDs known to exactly metrize weak convergence to P. Along the way, we highlight the implications of our res
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#26377;&#25439;&#22270;&#20687;&#21387;&#32553;&#30340;&#20248;&#21270;&#26694;&#26550;&#12290;&#36890;&#36807;&#24341;&#20837;&#39069;&#22806;&#30340;&#20869;&#23481;&#28508;&#21464;&#37327;&#20197;&#21450;&#21512;&#25104;&#32441;&#29702;&#21464;&#37327;&#65292;&#35813;&#26041;&#27861;&#22312;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#25351;&#26631;&#19978;&#34920;&#29616;&#20986;&#26356;&#24378;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.06950</link><description>&lt;p&gt;
&#22522;&#20110;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#26377;&#25439;&#22270;&#20687;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Lossy Image Compression with Conditional Diffusion Models. (arXiv:2209.06950v5 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.06950
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#26377;&#25439;&#22270;&#20687;&#21387;&#32553;&#30340;&#20248;&#21270;&#26694;&#26550;&#12290;&#36890;&#36807;&#24341;&#20837;&#39069;&#22806;&#30340;&#20869;&#23481;&#28508;&#21464;&#37327;&#20197;&#21450;&#21512;&#25104;&#32441;&#29702;&#21464;&#37327;&#65292;&#35813;&#26041;&#27861;&#22312;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#25351;&#26631;&#19978;&#34920;&#29616;&#20986;&#26356;&#24378;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#20248;&#21270;&#30340;&#26377;&#25439;&#22270;&#20687;&#21387;&#32553;&#26694;&#26550;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#21464;&#25442;&#32534;&#30721;&#33539;&#24335;&#65292;&#23558;&#22270;&#20687;&#26144;&#23556;&#21040;&#28508;&#22312;&#31354;&#38388;&#36827;&#34892;&#20449;&#24687;&#29109;&#32534;&#30721;&#65292;&#28982;&#21518;&#20877;&#26144;&#23556;&#22238;&#25968;&#25454;&#31354;&#38388;&#36827;&#34892;&#37325;&#26500;&#12290;&#19982;&#22522;&#20110;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(VAE)&#30340;&#31070;&#32463;&#21387;&#32553;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#35299;&#30721;&#22120;&#26159;&#19968;&#20010;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24341;&#20837;&#20102;&#19968;&#20010;&#39069;&#22806;&#30340;&#8220;&#20869;&#23481;&#8221;&#28508;&#21464;&#37327;&#65292;&#21453;&#21521;&#25193;&#25955;&#36807;&#31243;&#20250;&#23545;&#20854;&#36827;&#34892;&#26465;&#20214;&#21270;&#65292;&#24182;&#21033;&#29992;&#35813;&#21464;&#37327;&#23384;&#20648;&#22270;&#20687;&#20449;&#24687;&#12290;&#20915;&#23450;&#25193;&#25955;&#36807;&#31243;&#30340;&#21097;&#20313;&#8220;&#32441;&#29702;&#8221;&#21464;&#37327;&#20250;&#22312;&#35299;&#30721;&#26102;&#21512;&#25104;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#21487;&#20197;&#26681;&#25454;&#24863;&#30693;&#24230;&#37327;&#36827;&#34892;&#35843;&#25972;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#28041;&#21450;&#20102;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#25351;&#26631;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#36739;&#20110;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#26041;&#27861;&#33021;&#22815;&#24471;&#21040;&#26356;&#22909;&#30340;FID&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper outlines an end-to-end optimized lossy image compression framework using diffusion generative models. The approach relies on the transform coding paradigm, where an image is mapped into a latent space for entropy coding and, from there, mapped back to the data space for reconstruction. In contrast to VAE-based neural compression, where the (mean) decoder is a deterministic neural network, our decoder is a conditional diffusion model. Our approach thus introduces an additional "content" latent variable on which the reverse diffusion process is conditioned and uses this variable to store information about the image. The remaining "texture" variables characterizing the diffusion process are synthesized at decoding time. We show that the model's performance can be tuned toward perceptual metrics of interest. Our extensive experiments involving multiple datasets and image quality assessment metrics show that our approach yields stronger reported FID scores than the GAN-based mode
&lt;/p&gt;</description></item><item><title>"SensorSCAN"&#26159;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#21644;&#28145;&#24230;&#32858;&#31867;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#21270;&#24037;&#36807;&#31243;&#20013;&#36827;&#34892;&#25925;&#38556;&#35786;&#26029;&#12290;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#22312;&#26080;&#38656;&#19987;&#23478;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#26377;&#25928;&#26816;&#27979;&#22823;&#22810;&#25968;&#36807;&#31243;&#25925;&#38556;&#65292;&#24182;&#19988;&#36890;&#36807;&#22312;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#19978;&#24494;&#35843;&#65292;&#20960;&#20046;&#36798;&#21040;&#20102;&#26368;&#20248;&#27169;&#22411;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2208.08879</link><description>&lt;p&gt;
SensorSCAN: &#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#21644;&#28145;&#24230;&#32858;&#31867;&#22312;&#21270;&#24037;&#36807;&#31243;&#20013;&#30340;&#25925;&#38556;&#35786;&#26029;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
SensorSCAN: Self-Supervised Learning and Deep Clustering for Fault Diagnosis in Chemical Processes. (arXiv:2208.08879v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.08879
&lt;/p&gt;
&lt;p&gt;
"SensorSCAN"&#26159;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#21644;&#28145;&#24230;&#32858;&#31867;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#21270;&#24037;&#36807;&#31243;&#20013;&#36827;&#34892;&#25925;&#38556;&#35786;&#26029;&#12290;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#22312;&#26080;&#38656;&#19987;&#23478;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#26377;&#25928;&#26816;&#27979;&#22823;&#22810;&#25968;&#36807;&#31243;&#25925;&#38556;&#65292;&#24182;&#19988;&#36890;&#36807;&#22312;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#19978;&#24494;&#35843;&#65292;&#20960;&#20046;&#36798;&#21040;&#20102;&#26368;&#20248;&#27169;&#22411;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#24037;&#19994;&#35774;&#26045;&#22312;&#29983;&#20135;&#36807;&#31243;&#20013;&#20135;&#29983;&#22823;&#37327;&#30340;&#21407;&#22987;&#20256;&#24863;&#22120;&#25968;&#25454;&#12290;&#36825;&#20123;&#25968;&#25454;&#29992;&#20110;&#30417;&#25511;&#21644;&#25511;&#21046;&#36807;&#31243;&#65292;&#24182;&#21487;&#20998;&#26512;&#20197;&#26816;&#27979;&#21644;&#39044;&#27979;&#36807;&#31243;&#24322;&#24120;&#12290;&#36890;&#24120;&#65292;&#25968;&#25454;&#24517;&#39035;&#30001;&#19987;&#23478;&#36827;&#34892;&#27880;&#37322;&#65292;&#20197;&#20415;&#22312;&#39044;&#27979;&#24314;&#27169;&#20013;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#24037;&#19994;&#29615;&#22659;&#20013;&#25163;&#21160;&#27880;&#37322;&#22823;&#37327;&#25968;&#25454;&#21487;&#33021;&#24456;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SensorSCAN&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#25925;&#38556;&#26816;&#27979;&#21644;&#35786;&#26029;&#65292;&#19987;&#20026;&#24037;&#19994;&#21270;&#23398;&#36807;&#31243;&#30417;&#27979;&#32780;&#35774;&#35745;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;Tennessee Eastman&#24037;&#33402;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#21508;&#31181;&#25925;&#38556;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27809;&#26377;&#19987;&#23478;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65288;&#22312;&#22266;&#23450;FPR&#19979;&#65292;&#22686;&#21152;&#20102;0.2-0.3&#30340;TPR&#65289;&#65292;&#26377;&#25928;&#22320;&#26816;&#27979;&#20986;&#22823;&#22810;&#25968;&#36807;&#31243;&#25925;&#38556;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#19978;&#24494;&#35843;&#30340;&#27169;&#22411;&#20960;&#20046;&#36798;&#21040;&#20102;SOT&#27169;&#22411;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern industrial facilities generate large volumes of raw sensor data during the production process. This data is used to monitor and control the processes and can be analyzed to detect and predict process abnormalities. Typically, the data has to be annotated by experts in order to be used in predictive modeling. However, manual annotation of large amounts of data can be difficult in industrial settings.  In this paper, we propose SensorSCAN, a novel method for unsupervised fault detection and diagnosis, designed for industrial chemical process monitoring. We demonstrate our model's performance on two publicly available datasets of the Tennessee Eastman Process with various faults. The results show that our method significantly outperforms existing approaches (+0.2-0.3 TPR for a fixed FPR) and effectively detects most of the process faults without expert annotation. Moreover, we show that the model fine-tuned on a small fraction of labeled data nearly reaches the performance of a SOT
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38544;&#31169;&#19982;&#21487;&#35299;&#37322;&#24615;&#22312;&#22270;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#25552;&#20986;&#20102;&#20960;&#31181;&#22270;&#37325;&#26500;&#25915;&#20987;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#21518;&#32493;&#29305;&#24449;&#35299;&#37322;&#23545;&#25915;&#20987;&#30340;&#25104;&#21151;&#29575;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#23545;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#19981;&#21516;&#35299;&#37322;&#26041;&#27861;&#65292;&#22522;&#20110;&#26799;&#24230;&#30340;&#35299;&#37322;&#25581;&#31034;&#20102;&#26368;&#22810;&#30340;&#22270;&#32467;&#26500;&#20449;&#24687;&#65292;&#20294;&#25928;&#29992;&#24182;&#19981;&#24635;&#26159;&#26368;&#39640;&#12290;</title><link>http://arxiv.org/abs/2206.14724</link><description>&lt;p&gt;
&#36890;&#36807;&#29305;&#24449;&#35299;&#37322;&#23454;&#29616;&#31169;&#26377;&#22270;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Private Graph Extraction via Feature Explanations. (arXiv:2206.14724v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.14724
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38544;&#31169;&#19982;&#21487;&#35299;&#37322;&#24615;&#22312;&#22270;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#25552;&#20986;&#20102;&#20960;&#31181;&#22270;&#37325;&#26500;&#25915;&#20987;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#21518;&#32493;&#29305;&#24449;&#35299;&#37322;&#23545;&#25915;&#20987;&#30340;&#25104;&#21151;&#29575;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#23545;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#19981;&#21516;&#35299;&#37322;&#26041;&#27861;&#65292;&#22522;&#20110;&#26799;&#24230;&#30340;&#35299;&#37322;&#25581;&#31034;&#20102;&#26368;&#22810;&#30340;&#22270;&#32467;&#26500;&#20449;&#24687;&#65292;&#20294;&#25928;&#29992;&#24182;&#19981;&#24635;&#26159;&#26368;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#31169;&#21644;&#21487;&#35299;&#37322;&#24615;&#26159;&#23454;&#29616;&#21487;&#20449;&#26426;&#22120;&#23398;&#20064;&#30340;&#20004;&#20010;&#37325;&#35201;&#22240;&#32032;&#12290;&#25105;&#20204;&#36890;&#36807;&#22270;&#37325;&#26500;&#25915;&#20987;&#30740;&#31350;&#20102;&#36825;&#20004;&#20010;&#26041;&#38754;&#22312;&#22270;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#25915;&#20987;&#32773;&#30340;&#30446;&#26631;&#26159;&#22312;&#35775;&#38382;&#27169;&#22411;&#35299;&#37322;&#30340;&#24773;&#20917;&#19979;&#37325;&#24314;&#35757;&#32451;&#25968;&#25454;&#30340;&#22270;&#32467;&#26500;&#12290;&#26681;&#25454;&#25915;&#20987;&#32773;&#21487;&#29992;&#30340;&#19981;&#21516;&#36741;&#21161;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#22270;&#37325;&#24314;&#25915;&#20987;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23545;&#20110;&#21518;&#32493;&#29305;&#24449;&#35299;&#37322;&#30340;&#38468;&#21152;&#30693;&#35782;&#26174;&#33879;&#25552;&#39640;&#20102;&#36825;&#20123;&#25915;&#20987;&#30340;&#25104;&#21151;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35814;&#32454;&#30740;&#31350;&#20102;&#38024;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#19977;&#31181;&#19981;&#21516;&#35299;&#37322;&#26041;&#27861;&#65288;&#22522;&#20110;&#26799;&#24230;&#12289;&#25200;&#21160;&#21644;&#26367;&#20195;&#27169;&#22411;&#30340;&#26041;&#27861;&#65289;&#22312;&#25915;&#20987;&#24615;&#33021;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;&#23613;&#31649;&#22522;&#20110;&#26799;&#24230;&#30340;&#35299;&#37322;&#22312;&#25581;&#31034;&#22270;&#32467;&#26500;&#26041;&#38754;&#26368;&#20026;&#20986;&#33394;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#35299;&#37322;&#22312;&#25928;&#29992;&#26041;&#38754;&#24182;&#19981;&#24635;&#26159;&#24471;&#20998;&#36739;&#39640;&#12290;&#20854;&#20182;&#20004;&#31181;&#26041;&#27861;&#30340;&#25928;&#26524;&#36739;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Privacy and interpretability are two important ingredients for achieving trustworthy machine learning. We study the interplay of these two aspects in graph machine learning through graph reconstruction attacks. The goal of the adversary here is to reconstruct the graph structure of the training data given access to model explanations. Based on the different kinds of auxiliary information available to the adversary, we propose several graph reconstruction attacks. We show that additional knowledge of post-hoc feature explanations substantially increases the success rate of these attacks. Further, we investigate in detail the differences between attack performance with respect to three different classes of explanation methods for graph neural networks: gradient-based, perturbation-based, and surrogate model-based methods. While gradient-based explanations reveal the most in terms of the graph structure, we find that these explanations do not always score high in utility. For the other tw
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26410;&#30693;&#21160;&#24577;&#31995;&#32479;&#30340;&#20551;&#35774;&#26816;&#39564;&#38382;&#39064;&#65292;&#24182;&#23558;&#24322;&#24120;&#26816;&#27979;&#23450;&#20041;&#20026;&#36890;&#36807;&#22791;&#36873;&#20551;&#35774;&#26469;&#36827;&#34892;&#20551;&#35774;&#26816;&#39564;&#12290;&#36890;&#36807;&#20551;&#35774;&#26816;&#39564;&#21644;&#22270;&#27169;&#22411;&#30340;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#65292;DyAD&#65292;&#35813;&#35774;&#35745;&#21487;&#20197;&#29992;&#20110;&#25913;&#36827;&#31995;&#32479;&#24322;&#24120;&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2201.12358</link><description>&lt;p&gt;
&#26410;&#30693;&#21160;&#24577;&#31995;&#32479;&#30340;&#20551;&#35774;&#26816;&#39564;&#21450;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#31995;&#32479;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Hypothesis Testing for Unknown Dynamical Systems and System Anomaly Detection via Autoencoders. (arXiv:2201.12358v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.12358
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26410;&#30693;&#21160;&#24577;&#31995;&#32479;&#30340;&#20551;&#35774;&#26816;&#39564;&#38382;&#39064;&#65292;&#24182;&#23558;&#24322;&#24120;&#26816;&#27979;&#23450;&#20041;&#20026;&#36890;&#36807;&#22791;&#36873;&#20551;&#35774;&#26469;&#36827;&#34892;&#20551;&#35774;&#26816;&#39564;&#12290;&#36890;&#36807;&#20551;&#35774;&#26816;&#39564;&#21644;&#22270;&#27169;&#22411;&#30340;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#65292;DyAD&#65292;&#35813;&#35774;&#35745;&#21487;&#20197;&#29992;&#20110;&#25913;&#36827;&#31995;&#32479;&#24322;&#24120;&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26410;&#30693;&#21160;&#24577;&#31995;&#32479;&#30340;&#20551;&#35774;&#26816;&#39564;&#38382;&#39064;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#35266;&#27979;&#21040;&#20855;&#26377;&#26410;&#30693;&#21442;&#25968;&#30340;&#21160;&#24577;&#31995;&#32479;&#30340;&#36830;&#32493;&#36755;&#20837;&#21644;&#36755;&#20986;&#25968;&#25454;&#65292;&#26088;&#22312;&#30830;&#23450;&#25910;&#38598;&#21040;&#30340;&#25968;&#25454;&#26159;&#21542;&#26469;&#33258;&#38646;&#20998;&#24067;&#12290;&#36825;&#26679;&#30340;&#38382;&#39064;&#21487;&#20197;&#26377;&#35768;&#22810;&#24212;&#29992;&#12290;&#25105;&#20204;&#23558;&#24322;&#24120;&#26816;&#27979;&#23450;&#20041;&#20026;&#36890;&#36807;&#22791;&#25321;&#20551;&#35774;&#26469;&#36827;&#34892;&#20551;&#35774;&#26816;&#39564;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#20551;&#35774;&#26816;&#39564;&#31639;&#27861;&#21487;&#20197;&#26816;&#27979;&#26426;&#22120;&#20154;&#12289;&#22825;&#27668;&#12289;&#33021;&#28304;&#31995;&#32479;&#21644;&#32929;&#31080;&#24066;&#22330;&#31561;&#23454;&#38469;&#31995;&#32479;&#20013;&#30340;&#25925;&#38556;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#20551;&#35774;&#26816;&#39564;&#21644;&#22270;&#27169;&#22411;&#36827;&#34892;&#20180;&#32454;&#20998;&#26512;&#65292;&#19981;&#20165;&#21487;&#20197;&#35777;&#26126;&#33258;&#32534;&#30721;&#22120;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#36824;&#21487;&#20197;&#23548;&#33268;&#19968;&#31181;&#25913;&#36827;&#24615;&#33021;&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#65292;&#31216;&#20026;DyAD&#65288;&#21160;&#24577;&#31995;&#32479;&#24322;&#24120;&#26816;&#27979;&#65289;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;DyAD&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the hypothesis testing problem for unknown dynamical systems. More specifically, we observe sequential input and output data from a dynamical system with unknown parameters, and we aim to determine whether the collected data is from a null distribution. Such a problem can have many applications. Here we formulate anomaly detection as hypothesis testing where the anomaly is defined through the alternative hypothesis. Consequently, hypothesis testing algorithms can detect faults in real-world systems such as robots, weather, energy systems, and stock markets. Although recent works achieved state-of-the-art performances in these tasks with deep learning models, we show that a careful analysis using hypothesis testing and graphical models can not only justify the effectiveness of autoencoder models, but also lead to a novel neural network design, termed DyAD (DYnamical system Anomaly Detection), with improved performances. We then show that DyAD achieves state-of-the-art performan
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#32467;&#21512;&#26368;&#20248;&#36335;&#24452;&#25628;&#32034;&#21644;&#20219;&#21153;&#30456;&#20851;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25104;&#26412;&#20540;&#36716;&#21270;&#20026;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#26469;&#23454;&#29616;&#22312;&#32447;&#26435;&#37325;&#36866;&#24212;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#19982;&#32463;&#20856;&#31639;&#27861;Bellman-Ford&#20855;&#26377;&#30456;&#21516;&#30340;&#35299;&#65292;&#24182;&#19988;&#32593;&#32476;&#23398;&#20064;&#26426;&#21046;&#21487;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2201.11104</link><description>&lt;p&gt;
&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#32467;&#21512;&#26368;&#20248;&#36335;&#24452;&#25628;&#32034;&#21644;&#20219;&#21153;&#30456;&#20851;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Combining optimal path search with task-dependent learning in a neural network. (arXiv:2201.11104v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.11104
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#32467;&#21512;&#26368;&#20248;&#36335;&#24452;&#25628;&#32034;&#21644;&#20219;&#21153;&#30456;&#20851;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25104;&#26412;&#20540;&#36716;&#21270;&#20026;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#26469;&#23454;&#29616;&#22312;&#32447;&#26435;&#37325;&#36866;&#24212;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#19982;&#32463;&#20856;&#31639;&#27861;Bellman-Ford&#20855;&#26377;&#30456;&#21516;&#30340;&#35299;&#65292;&#24182;&#19988;&#32593;&#32476;&#23398;&#20064;&#26426;&#21046;&#21487;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36830;&#25509;&#22270;&#20013;&#25214;&#21040;&#26368;&#20248;&#36335;&#24452;&#38656;&#35201;&#30830;&#23450;&#27839;&#30528;&#22270;&#30340;&#36793;&#32536;&#34892;&#36827;&#30340;&#26368;&#23567;&#24635;&#25104;&#26412;&#12290;&#36825;&#20010;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#20960;&#31181;&#32463;&#20856;&#31639;&#27861;&#26469;&#35299;&#20915;&#65292;&#36890;&#24120;&#25152;&#26377;&#36793;&#32536;&#30340;&#25104;&#26412;&#37117;&#26159;&#39044;&#20808;&#23450;&#20041;&#22909;&#30340;&#12290;&#22240;&#27492;&#65292;&#22312;&#24819;&#35201;&#26681;&#25454;&#26576;&#20010;&#20219;&#21153;&#30340;&#35201;&#27714;&#20197;&#33258;&#36866;&#24212;&#30340;&#26041;&#24335;&#25913;&#21464;&#25104;&#26412;&#26102;&#65292;&#36890;&#24120;&#26080;&#27861;&#20351;&#29992;&#20256;&#32479;&#35268;&#21010;&#26041;&#27861;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#36890;&#36807;&#23558;&#25104;&#26412;&#20540;&#36716;&#21270;&#20026;&#31361;&#35302;&#26435;&#37325;&#26469;&#23450;&#20041;&#36335;&#24452;&#25628;&#32034;&#38382;&#39064;&#30340;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#65292;&#36825;&#20801;&#35768;&#20351;&#29992;&#32593;&#32476;&#23398;&#20064;&#26426;&#21046;&#36827;&#34892;&#22312;&#32447;&#26435;&#37325;&#36866;&#24212;&#12290;&#24403;&#20174;&#19968;&#20010;&#21021;&#22987;&#27963;&#36291;&#24230;&#20540;&#20026;1&#24320;&#22987;&#26102;&#65292;&#22312;&#36825;&#20010;&#32593;&#32476;&#20013;&#30340;&#27963;&#21160;&#20256;&#25773;&#23558;&#23548;&#33268;&#19982;Bellman-Ford&#31639;&#27861;&#25214;&#21040;&#30340;&#35299;&#30456;&#21516;&#30340;&#35299;&#12290;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#19982;Bellman-Ford&#30456;&#21516;&#30340;&#31639;&#27861;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#27492;&#22806;&#65292;&#25105;&#20204;&#21487;&#20197;&#35777;&#26126;&#32593;&#32476;&#23398;&#20064;&#26426;&#21046;&#65288;&#22914;&#36203;&#24067;&#23398;&#20064;&#65289;&#21487;&#20197;&#35843;&#25972;&#32593;&#32476;&#20013;&#30340;&#26435;&#37325;&#26469;&#22686;&#24378;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finding optimal paths in connected graphs requires determining the smallest total cost for traveling along the graph's edges. This problem can be solved by several classical algorithms where, usually, costs are predefined for all edges. Conventional planning methods can, thus, normally not be used when wanting to change costs in an adaptive way following the requirements of some task. Here we show that one can define a neural network representation of path finding problems by transforming cost values into synaptic weights, which allows for online weight adaptation using network learning mechanisms. When starting with an initial activity value of one, activity propagation in this network will lead to solutions, which are identical to those found by the Bellman-Ford algorithm. The neural network has the same algorithmic complexity as Bellman-Ford and, in addition, we can show that network learning mechanisms (such as Hebbian learning) can adapt the weights in the network augmenting the r
&lt;/p&gt;</description></item><item><title>&#22312;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25512;&#23548;&#20986;&#36951;&#28431;&#21464;&#37327;&#20559;&#24046;&#30340;&#23574;&#38160;&#19978;&#30028;&#65292;&#20026;&#24191;&#27867;&#30340;&#32447;&#24615;&#27867;&#20989;&#22240;&#26524;&#21442;&#25968;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#36890;&#29992;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#35768;&#22810;&#20256;&#32479;&#30340;&#22240;&#26524;&#25512;&#26029;&#30740;&#31350;&#30446;&#26631;&#65292;&#24182;&#19988;&#20165;&#21462;&#20915;&#20110;&#28508;&#21464;&#37327;&#22312;&#32467;&#26524;&#21644;&#21442;&#25968;&#30340;Riesz&#34920;&#31034;&#22120;&#20013;&#25152;&#23548;&#33268;&#30340;&#39069;&#22806;&#21464;&#24322;&#12290;</title><link>http://arxiv.org/abs/2112.13398</link><description>&lt;p&gt;
&#12298;&#38271;&#35805;&#30701;&#35828;&#65306;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#36951;&#28431;&#21464;&#37327;&#20559;&#24046;&#12299;
&lt;/p&gt;
&lt;p&gt;
Long Story Short: Omitted Variable Bias in Causal Machine Learning. (arXiv:2112.13398v4 [econ.EM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.13398
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25512;&#23548;&#20986;&#36951;&#28431;&#21464;&#37327;&#20559;&#24046;&#30340;&#23574;&#38160;&#19978;&#30028;&#65292;&#20026;&#24191;&#27867;&#30340;&#32447;&#24615;&#27867;&#20989;&#22240;&#26524;&#21442;&#25968;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#36890;&#29992;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#35768;&#22810;&#20256;&#32479;&#30340;&#22240;&#26524;&#25512;&#26029;&#30740;&#31350;&#30446;&#26631;&#65292;&#24182;&#19988;&#20165;&#21462;&#20915;&#20110;&#28508;&#21464;&#37327;&#22312;&#32467;&#26524;&#21644;&#21442;&#25968;&#30340;Riesz&#34920;&#31034;&#22120;&#20013;&#25152;&#23548;&#33268;&#30340;&#39069;&#22806;&#21464;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25512;&#23548;&#20102;&#19968;&#31867;&#24191;&#27867;&#30340;&#22240;&#26524;&#21442;&#25968;&#30340;&#36951;&#28431;&#21464;&#37327;&#20559;&#24046;&#30340;&#19968;&#33324;&#20294;&#31616;&#21333;&#30340;&#23574;&#38160;&#19978;&#30028;&#65292;&#36825;&#20123;&#21442;&#25968;&#21487;&#20197;&#34987;&#35748;&#23450;&#20026;&#32467;&#26524;&#30340;&#26465;&#20214;&#26399;&#26395;&#20989;&#25968;&#30340;&#32447;&#24615;&#27867;&#20989;&#12290;&#36825;&#26679;&#30340;&#27867;&#20989;&#21253;&#25324;&#35768;&#22810;&#22240;&#26524;&#25512;&#26029;&#30740;&#31350;&#20013;&#30340;&#20256;&#32479;&#35843;&#26597;&#30446;&#26631;&#65292;&#20363;&#22914;&#65288;&#21152;&#26435;&#65289;&#28508;&#22312;&#32467;&#26524;&#30340;&#24179;&#22343;&#20540;&#12289;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65288;&#21253;&#25324;&#23376;&#32452;&#25928;&#24212;&#65292;&#22914;&#23545;&#24453;&#22788;&#29702;&#23545;&#35937;&#30340;&#24433;&#21709;&#65289;&#12289;&#65288;&#21152;&#26435;&#65289;&#24179;&#22343;&#23548;&#25968;&#21644;&#26469;&#33258;&#21327;&#21464;&#37327;&#20998;&#24067;&#21464;&#21270;&#30340;&#31574;&#30053;&#25928;&#24212; - &#20840;&#37096;&#36866;&#29992;&#20110;&#19968;&#33324;&#30340;&#38750;&#21442;&#25968;&#22240;&#26524;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26500;&#36896;&#20381;&#36182;&#20110;&#30446;&#26631;&#27867;&#20989;&#30340;Riesz-Fr&#233;chet&#34920;&#31034;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20559;&#24046;&#19978;&#30028;&#20165;&#21462;&#20915;&#20110;&#28508;&#21464;&#37327;&#22312;&#32467;&#26524;&#21644;&#24863;&#20852;&#36259;&#21442;&#25968;&#30340;Riesz&#34920;&#31034;&#22120;&#20013;&#25152;&#21019;&#24314;&#30340;&#38468;&#21152;&#21464;&#21270;&#12290;&#27492;&#22806;&#65292;&#22312;&#35768;&#22810;&#37325;&#35201;&#24773;&#20917;&#19979;&#65288;&#20363;&#22914;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#21644;&#24179;&#22343;&#23548;&#25968;&#65289;
&lt;/p&gt;
&lt;p&gt;
We derive general, yet simple, sharp bounds on the size of the omitted variable bias for a broad class of causal parameters that can be identified as linear functionals of the conditional expectation function of the outcome. Such functionals encompass many of the traditional targets of investigation in causal inference studies, such as, for example, (weighted) average of potential outcomes, average treatment effects (including subgroup effects, such as the effect on the treated), (weighted) average derivatives, and policy effects from shifts in covariate distribution -- all for general, nonparametric causal models. Our construction relies on the Riesz-Frechet representation of the target functional. Specifically, we show how the bound on the bias depends only on the additional variation that the latent variables create both in the outcome and in the Riesz representer for the parameter of interest. Moreover, in many important cases (e.g, average treatment effects and avearage derivative
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#39532;&#23572;&#21487;&#22827;&#38142;&#32806;&#21512;&#30340;&#20272;&#35745;&#22120;&#65292;&#29992;&#20110;&#35780;&#20272;&#28176;&#36817;&#26377;&#20559;&#37319;&#26679;&#26041;&#27861;&#30340;&#36136;&#37327;&#65292;&#24182;&#32473;&#20986;&#20102;&#28176;&#36817;&#26377;&#20559;&#37319;&#26679;&#26041;&#27861;&#30340;&#26497;&#38480;&#20998;&#24067;&#19982;&#21407;&#22987;&#30446;&#26631;&#20998;&#24067;&#20043;&#38388;&#30340;Wasserstein&#36317;&#31163;&#30340;&#32463;&#39564;&#19978;&#30028;&#12290;</title><link>http://arxiv.org/abs/2112.03152</link><description>&lt;p&gt;
&#20351;&#29992;&#32806;&#21512;&#26041;&#27861;&#30028;&#23450;Wasserstein&#36317;&#31163;
&lt;/p&gt;
&lt;p&gt;
Bounding Wasserstein distance with couplings. (arXiv:2112.03152v3 [stat.CO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.03152
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#39532;&#23572;&#21487;&#22827;&#38142;&#32806;&#21512;&#30340;&#20272;&#35745;&#22120;&#65292;&#29992;&#20110;&#35780;&#20272;&#28176;&#36817;&#26377;&#20559;&#37319;&#26679;&#26041;&#27861;&#30340;&#36136;&#37327;&#65292;&#24182;&#32473;&#20986;&#20102;&#28176;&#36817;&#26377;&#20559;&#37319;&#26679;&#26041;&#27861;&#30340;&#26497;&#38480;&#20998;&#24067;&#19982;&#21407;&#22987;&#30446;&#26631;&#20998;&#24067;&#20043;&#38388;&#30340;Wasserstein&#36317;&#31163;&#30340;&#32463;&#39564;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#65288;MCMC&#65289;&#22312;&#36845;&#20195;&#27425;&#25968;&#36235;&#20110;&#26080;&#31351;&#26102;&#25552;&#20379;&#20102;&#23545;&#38590;&#20197;&#35745;&#31639;&#30340;&#21518;&#39564;&#26399;&#26395;&#30340;&#28176;&#36817;&#19968;&#33268;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#22312;&#22823;&#22411;&#25968;&#25454;&#24212;&#29992;&#20013;&#65292;MCMC&#27599;&#27425;&#36845;&#20195;&#30340;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#12290;&#36825;&#20419;&#20351;&#20154;&#20204;&#23545;&#20197;&#25552;&#39640;&#27599;&#27425;&#36845;&#20195;&#30340;&#35745;&#31639;&#36895;&#24230;&#20026;&#30446;&#26631;&#30340;MCMC&#36817;&#20284;&#26041;&#27861;&#20135;&#29983;&#20102;&#20852;&#36259;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#19981;&#20250;&#20135;&#29983;&#28176;&#36817;&#19968;&#33268;&#20272;&#35745;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#39532;&#23572;&#21487;&#22827;&#38142;&#32806;&#21512;&#30340;&#20272;&#35745;&#22120;&#65292;&#29992;&#20110;&#35780;&#20272;&#36825;&#31181;&#28176;&#36817;&#26377;&#20559;&#37319;&#26679;&#26041;&#27861;&#30340;&#36136;&#37327;&#12290;&#36825;&#20123;&#20272;&#35745;&#22120;&#32473;&#20986;&#20102;&#28176;&#36817;&#26377;&#20559;&#37319;&#26679;&#26041;&#27861;&#30340;&#26497;&#38480;&#20998;&#24067;&#19982;&#21407;&#22987;&#30446;&#26631;&#20998;&#24067;&#20043;&#38388;&#30340;Wasserstein&#36317;&#31163;&#30340;&#32463;&#39564;&#19978;&#30028;&#12290;&#25105;&#20204;&#20026;&#25105;&#20204;&#30340;&#19978;&#30028;&#24314;&#31435;&#20102;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#34920;&#26126;&#25105;&#20204;&#30340;&#20272;&#35745;&#22120;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#20173;&#28982;&#26377;&#25928;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#36136;&#37327;&#24230;&#37327;&#24212;&#29992;&#20110;&#38543;&#26426;&#26799;&#24230;MCMC&#12289;&#21464;&#20998;&#36125;&#21494;&#26031;&#21644;&#25289;&#26222;&#25289;&#26031;&#36817;&#20284;&#31561;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Markov chain Monte Carlo (MCMC) provides asymptotically consistent estimates of intractable posterior expectations as the number of iterations tends to infinity. However, in large data applications, MCMC can be computationally expensive per iteration. This has catalyzed interest in approximating MCMC in a manner that improves computational speed per iteration but does not produce asymptotically consistent estimates. In this article, we propose estimators based on couplings of Markov chains to assess the quality of such asymptotically biased sampling methods. The estimators give empirical upper bounds of the Wasserstein distance between the limiting distribution of the asymptotically biased sampling method and the original target distribution of interest. We establish theoretical guarantees for our upper bounds and show that our estimators can remain effective in high dimensions. We apply our quality measures to stochastic gradient MCMC, variational Bayes, and Laplace approximations for
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;PrivGNN&#65292;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#38598;&#20013;&#24335;&#29615;&#22659;&#20013;&#21457;&#24067;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#21644;&#20004;&#20010;&#22122;&#22768;&#26426;&#21046;&#65292;&#36890;&#36807;&#22312;&#20844;&#20849;&#25968;&#25454;&#19978;&#35757;&#32451;&#21644;&#31169;&#26377;&#25968;&#25454;&#30340;&#30693;&#35782;&#65292;&#23454;&#29616;&#20102;&#20005;&#26684;&#30340;&#38544;&#31169;&#20445;&#35777;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2109.08907</link><description>&lt;p&gt;
&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#30340;&#21457;&#24067;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Releasing Graph Neural Networks with Differential Privacy Guarantees. (arXiv:2109.08907v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.08907
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;PrivGNN&#65292;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#38598;&#20013;&#24335;&#29615;&#22659;&#20013;&#21457;&#24067;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#21644;&#20004;&#20010;&#22122;&#22768;&#26426;&#21046;&#65292;&#36890;&#36807;&#22312;&#20844;&#20849;&#25968;&#25454;&#19978;&#35757;&#32451;&#21644;&#31169;&#26377;&#25968;&#25454;&#30340;&#30693;&#35782;&#65292;&#23454;&#29616;&#20102;&#20005;&#26684;&#30340;&#38544;&#31169;&#20445;&#35777;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#21307;&#30103;&#20445;&#20581;&#21644;&#21307;&#23398;&#31561;&#25935;&#24863;&#24212;&#29992;&#20013;&#30340;&#26085;&#30410;&#27969;&#34892;&#65292;&#20154;&#20204;&#23545;&#35757;&#32451;&#30340;GNN&#30340;&#38544;&#31169;&#26041;&#38754;&#25552;&#20986;&#20102;&#25285;&#24551;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#21363;&#20351;&#21482;&#25480;&#20104;&#23545;&#35757;&#32451;&#27169;&#22411;&#30340;&#40657;&#30418;&#35775;&#38382;&#26435;&#38480;&#65292;GNN&#20063;&#23481;&#26131;&#21463;&#21040;&#38544;&#31169;&#25915;&#20987;&#65292;&#20363;&#22914;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;PrivGNN&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#38598;&#20013;&#24335;&#29615;&#22659;&#20013;&#21457;&#24067;GNN&#27169;&#22411;&#30340;&#38544;&#31169;&#20445;&#25252;&#26694;&#26550;&#12290;&#20551;&#35774;&#21487;&#20197;&#35775;&#38382;&#20844;&#20849;&#26410;&#26631;&#35760;&#30340;&#22270;&#65292;PrivGNN&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20197;&#38544;&#31169;&#20445;&#25252;&#30340;&#26041;&#24335;&#21457;&#24067;&#26126;&#30830;&#22312;&#20844;&#20849;&#25968;&#25454;&#19978;&#35757;&#32451;&#24182;&#33719;&#21462;&#26469;&#33258;&#31169;&#26377;&#25968;&#25454;&#30340;&#30693;&#35782;&#30340;GNN&#27169;&#22411;&#12290;PrivGNN&#23558;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#19982;&#20004;&#20010;&#22122;&#22768;&#26426;&#21046;&#65288;&#38543;&#26426;&#23376;&#37319;&#26679;&#21644;&#22024;&#26434;&#26631;&#35760;&#65289;&#30456;&#32467;&#21512;&#65292;&#20197;&#30830;&#20445;&#20005;&#26684;&#30340;&#38544;&#31169;&#20445;&#35777;&#12290;&#25105;&#20204;&#22312;Renyi&#24046;&#20998;&#38544;&#31169;&#26694;&#26550;&#20013;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#22810;&#20010;&#22522;&#20934;&#26041;&#27861;&#30456;&#27604;&#30340;&#23454;&#39564;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing popularity of graph neural networks (GNNs) in several sensitive applications like healthcare and medicine, concerns have been raised over the privacy aspects of trained GNNs. More notably, GNNs are vulnerable to privacy attacks, such as membership inference attacks, even if only black-box access to the trained model is granted. We propose PrivGNN, a privacy-preserving framework for releasing GNN models in a centralized setting. Assuming an access to a public unlabeled graph, PrivGNN provides a framework to release GNN models trained explicitly on public data along with knowledge obtained from the private data in a privacy preserving manner. PrivGNN combines the knowledge-distillation framework with the two noise mechanisms, random subsampling, and noisy labeling, to ensure rigorous privacy guarantees. We theoretically analyze our approach in the Renyi differential privacy framework. Besides, we show the solid experimental performance of our method compared to severa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#23376;&#33539;&#25968;&#30340;&#29420;&#21344;&#32676;&#32452;&#22871;&#32034;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32467;&#26500;&#21464;&#37327;&#36873;&#25321;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#36866;&#24403;&#35774;&#35745;&#30340;&#22797;&#21512;&#33539;&#25968;&#20419;&#36827;&#29420;&#21344;&#32676;&#32452;&#31232;&#30095;&#27169;&#24335;&#65292;&#24182;&#20351;&#29992;&#39640;&#25928;&#28789;&#27963;&#30340;&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#25903;&#25345;&#24674;&#22797;&#12290;&#36890;&#36807;&#36880;&#27493;&#23558;&#32467;&#26500;&#21407;&#23376;&#21253;&#21547;&#21040;&#20272;&#35745;&#30340;&#25903;&#25345;&#20013;&#26500;&#24314;&#35299;&#65292;&#24182;&#22312;&#19968;&#23450;&#20551;&#35774;&#19979;&#35777;&#26126;&#20102;&#35299;&#20915;&#26041;&#26696;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2108.10284</link><description>&lt;p&gt;
&#32467;&#26500;&#21464;&#37327;&#36873;&#25321;&#30340;&#29420;&#21344;&#32676;&#32452;&#22871;&#32034;
&lt;/p&gt;
&lt;p&gt;
Exclusive Group Lasso for Structured Variable Selection. (arXiv:2108.10284v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.10284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#23376;&#33539;&#25968;&#30340;&#29420;&#21344;&#32676;&#32452;&#22871;&#32034;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32467;&#26500;&#21464;&#37327;&#36873;&#25321;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#36866;&#24403;&#35774;&#35745;&#30340;&#22797;&#21512;&#33539;&#25968;&#20419;&#36827;&#29420;&#21344;&#32676;&#32452;&#31232;&#30095;&#27169;&#24335;&#65292;&#24182;&#20351;&#29992;&#39640;&#25928;&#28789;&#27963;&#30340;&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#25903;&#25345;&#24674;&#22797;&#12290;&#36890;&#36807;&#36880;&#27493;&#23558;&#32467;&#26500;&#21407;&#23376;&#21253;&#21547;&#21040;&#20272;&#35745;&#30340;&#25903;&#25345;&#20013;&#26500;&#24314;&#35299;&#65292;&#24182;&#22312;&#19968;&#23450;&#20551;&#35774;&#19979;&#35777;&#26126;&#20102;&#35299;&#20915;&#26041;&#26696;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#31181;&#32467;&#26500;&#21464;&#37327;&#36873;&#25321;&#38382;&#39064;&#65292;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#65292;&#21327;&#21464;&#37327;&#34987;&#39044;&#23450;&#20041;&#30340;&#32676;&#32452;&#21010;&#20998;&#65292;&#24182;&#19988;&#26681;&#25454;&#27599;&#20010;&#32676;&#32452;&#20013;&#30340;&#31232;&#30095;&#27169;&#24335;&#28608;&#27963;&#65292;&#27599;&#20010;&#32676;&#32452;&#20165;&#26377;&#23569;&#25968;&#38750;&#38646;&#26465;&#30446;&#12290;&#21033;&#29992;&#21407;&#23376;&#33539;&#25968;&#30340;&#27010;&#24565;&#65292;&#21487;&#20197;&#35774;&#35745;&#20986;&#21512;&#36866;&#30340;&#22797;&#21512;&#33539;&#25968;&#20197;&#20419;&#36827;&#36825;&#31181;&#29420;&#21344;&#32676;&#32452;&#31232;&#30095;&#27169;&#24335;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#33539;&#25968;&#36866;&#29992;&#20110;&#39640;&#25928;&#21644;&#28789;&#27963;&#30340;&#25903;&#25345;&#24674;&#22797;&#27491;&#21017;&#21270;&#20248;&#21270;&#31639;&#27861;&#65292;&#22914;&#36817;&#31471;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#21160;&#38598;&#31639;&#27861;&#65292;&#36890;&#36807;&#36880;&#27493;&#23558;&#32467;&#26500;&#21407;&#23376;&#21253;&#21547;&#21040;&#20272;&#35745;&#30340;&#25903;&#25345;&#20013;&#26469;&#26500;&#24314;&#35299;&#12290;&#36824;&#34920;&#26126;&#65292;&#36825;&#31181;&#31639;&#27861;&#21487;&#20197;&#38024;&#23545;&#27604;&#32431;&#31929;&#30340;&#29420;&#21344;&#32676;&#32452;&#31232;&#30095;&#24615;&#26356;&#20005;&#26684;&#30340;&#32467;&#26500;&#36827;&#34892;&#23450;&#21046;&#12290;&#28176;&#36817;&#19968;&#33268;&#24615;&#20998;&#26512;&#65288;&#21442;&#25968;&#25968;&#37327;&#21644;&#32676;&#32452;&#25968;&#37327;&#38543;&#35266;&#23519;&#22823;&#23567;&#22686;&#38271;&#65289;&#22312;&#20256;&#32479;&#20551;&#35774;&#19979;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#35299;&#20915;&#26041;&#26696;&#22312;&#26377;&#31526;&#21495;&#25903;&#25345;&#24674;&#22797;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#26368;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#20248;&#21270;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
A structured variable selection problem is considered in which the covariates, divided into predefined groups, activate according to sparse patterns with few nonzero entries per group. Capitalizing on the concept of atomic norm, a composite norm can be properly designed to promote such exclusive group sparsity patterns. The resulting norm lends itself to efficient and flexible regularized optimization algorithms for support recovery, like the proximal algorithm. Moreover, an active set algorithm is proposed that builds the solution by successively including structure atoms into the estimated support. It is also shown that such an algorithm can be tailored to match more rigid structures than plain exclusive group sparsity. Asymptotic consistency analysis (with both the number of parameters as well as the number of groups growing with the observation size) establishes the effectiveness of the proposed solution in terms of signed support recovery under conventional assumptions. Finally, a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#24120;&#35265;&#22122;&#22768;&#21487;&#20197;&#20316;&#20026;&#23398;&#20064;&#22343;&#22330;&#21338;&#24328;&#35299;&#30340;&#25506;&#32034;&#22122;&#22768;&#65292;&#24182;&#19988;&#36890;&#36807;&#23454;&#20363;&#34920;&#26126;&#36825;&#31181;&#22122;&#22768;&#33021;&#22815;&#24674;&#22797;&#35299;&#30340;&#23384;&#22312;&#24615;&#21644;&#21807;&#19968;&#24615;&#65292;&#24182;&#19988;&#24378;&#21046;&#23398;&#20064;&#31639;&#27861;&#25910;&#25947;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2107.00839</link><description>&lt;p&gt;
&#23398;&#20064;&#32447;&#24615;&#20108;&#27425;&#22343;&#22330;&#21338;&#24328;&#30340;&#25506;&#32034;&#22122;&#22768;
&lt;/p&gt;
&lt;p&gt;
Exploration noise for learning linear-quadratic mean field games. (arXiv:2107.00839v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.00839
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#24120;&#35265;&#22122;&#22768;&#21487;&#20197;&#20316;&#20026;&#23398;&#20064;&#22343;&#22330;&#21338;&#24328;&#35299;&#30340;&#25506;&#32034;&#22122;&#22768;&#65292;&#24182;&#19988;&#36890;&#36807;&#23454;&#20363;&#34920;&#26126;&#36825;&#31181;&#22122;&#22768;&#33021;&#22815;&#24674;&#22797;&#35299;&#30340;&#23384;&#22312;&#24615;&#21644;&#21807;&#19968;&#24615;&#65292;&#24182;&#19988;&#24378;&#21046;&#23398;&#20064;&#31639;&#27861;&#25910;&#25947;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#35777;&#26126;&#24120;&#35265;&#22122;&#22768;&#21487;&#20197;&#20316;&#20026;&#23398;&#20064;&#22343;&#22330;&#21338;&#24328;&#35299;&#30340;&#25506;&#32034;&#22122;&#22768;&#12290;&#36890;&#36807;&#19968;&#20010;&#29609;&#20855;&#32447;&#24615;&#20108;&#27425;&#27169;&#22411;&#30340;&#31034;&#20363;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#31181;&#36866;&#24403;&#24418;&#24335;&#30340;&#24120;&#35265;&#22122;&#22768;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#24674;&#22797;&#23384;&#22312;&#24615;&#21644;&#21807;&#19968;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#30456;&#21516;&#24418;&#24335;&#30340;&#24120;&#35265;&#22122;&#22768;&#21487;&#20197;&#24378;&#21046;&#23398;&#20064;&#31639;&#27861;&#8220;&#34394;&#26500;&#21338;&#24328;&#8221;&#25910;&#25947;&#65292;&#32780;&#26080;&#38656;&#36827;&#19968;&#27493;&#30340;&#28508;&#22312;&#25110;&#21333;&#35843;&#32467;&#26500;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20123;&#25968;&#20540;&#23454;&#20363;&#26469;&#25903;&#25345;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of this paper is to demonstrate that common noise may serve as an exploration noise for learning the solution of a mean field game. This concept is here exemplified through a toy linear-quadratic model, for which a suitable form of common noise has already been proven to restore existence and uniqueness. We here go one step further and prove that the same form of common noise may force the convergence of the learning algorithm called `fictitious play', and this without any further potential or monotone structure. Several numerical examples are provided in order to support our theoretical analysis.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#26041;&#27861;&#65292;&#22312;&#26102;&#38388;&#24207;&#21015;&#20013;&#23398;&#20064;&#25688;&#35201;&#22240;&#26524;&#22270;&#65292;&#24182;&#36890;&#36807;PC-like&#21644;FCI-like&#31639;&#27861;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2105.10381</link><description>&lt;p&gt;
&#22522;&#20110;&#29109;&#30340;&#26102;&#38388;&#24207;&#21015;&#25688;&#35201;&#22240;&#26524;&#22270;&#30340;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Entropy-based Discovery of Summary Causal Graphs in Time Series. (arXiv:2105.10381v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.10381
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#26041;&#27861;&#65292;&#22312;&#26102;&#38388;&#24207;&#21015;&#20013;&#23398;&#20064;&#25688;&#35201;&#22240;&#26524;&#22270;&#65292;&#24182;&#36890;&#36807;PC-like&#21644;FCI-like&#31639;&#27861;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#22312;&#21487;&#33021;&#20855;&#26377;&#19981;&#21516;&#37319;&#26679;&#39057;&#29575;&#30340;&#26102;&#38388;&#24207;&#21015;&#19978;&#23398;&#20064;&#25688;&#35201;&#22240;&#26524;&#22270;&#30340;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#30340;&#26032;&#22240;&#26524;&#26102;&#24577;&#20114;&#20449;&#24687;&#24230;&#37327;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#24230;&#37327;&#19982;&#29109;&#20943;&#21407;&#29702;&#30340;&#20851;&#31995;&#65292;&#21487;&#20197;&#30475;&#20316;&#26159;&#27010;&#29575;&#25552;&#21319;&#21407;&#29702;&#30340;&#29305;&#27530;&#24773;&#20917;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#36825;&#20004;&#20010;&#35201;&#32032;&#32467;&#21512;&#22312;&#31867;&#20284;&#20110;PC&#21644;FCI&#30340;&#31639;&#27861;&#20013;&#65292;&#26500;&#24314;&#20102;&#25688;&#35201;&#22240;&#26524;&#22270;&#12290;&#36825;&#20123;&#31639;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#26174;&#31034;&#20986;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study addresses the problem of learning a summary causal graph on time series with potentially different sampling rates. To do so, we first propose a new causal temporal mutual information measure for time series. We then show how this measure relates to an entropy reduction principle that can be seen as a special case of the probability raising principle. We finally combine these two ingredients in PC-like and FCI-like algorithms to construct the summary causal graph. There algorithm are evaluated on several datasets, which shows both their efficacy and efficiency.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36866;&#29992;&#20110;JPEG&#21387;&#32553;&#22270;&#20687;&#30340;&#36793;&#32536;&#24863;&#30693;&#39044;&#22788;&#29702;&#21644;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#36793;&#32536;&#24863;&#30693;&#25439;&#22833;&#20989;&#25968;&#21644;&#36229;&#20998;&#36776;&#29575;CNN&#65292;&#22312;&#20302;&#27604;&#29305;&#29575;&#19979;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;PSNR&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2104.04926</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36866;&#29992;&#20110;JPEG&#21387;&#32553;&#22270;&#20687;&#30340;&#36793;&#32536;&#24863;&#30693;&#39044;&#22788;&#29702;&#21644;&#21518;&#22788;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deep learning-based Edge-aware pre and post-processing methods for JPEG compressed images. (arXiv:2104.04926v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2104.04926
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36866;&#29992;&#20110;JPEG&#21387;&#32553;&#22270;&#20687;&#30340;&#36793;&#32536;&#24863;&#30693;&#39044;&#22788;&#29702;&#21644;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#36793;&#32536;&#24863;&#30693;&#25439;&#22833;&#20989;&#25968;&#21644;&#36229;&#20998;&#36776;&#29575;CNN&#65292;&#22312;&#20302;&#27604;&#29305;&#29575;&#19979;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;PSNR&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#21387;&#32553;&#26041;&#26696;&#65292;&#22312;&#39044;&#22788;&#29702;&#21644;&#21518;&#22788;&#29702;&#30340;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#20043;&#38388;&#21253;&#21547;&#20102;&#19968;&#20010;&#26631;&#20934;&#32534;&#35299;&#30721;&#22120;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#36793;&#32536;&#24863;&#30693;&#25439;&#22833;&#20989;&#25968;&#26469;&#25913;&#36827;&#20808;&#21069;&#26041;&#27861;&#65292;&#20197;&#38450;&#27490;&#27169;&#31946;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#29992;&#20110;&#21518;&#22788;&#29702;&#30340;&#36229;&#20998;&#36776;&#29575;CNN&#20197;&#21450;&#30456;&#24212;&#30340;&#25913;&#36827;&#20102;&#30340;&#39044;&#22788;&#29702;&#32593;&#32476;&#65292;&#20197;&#22312;&#20302;&#27604;&#29305;&#29575;&#19979;&#25552;&#20379;&#26356;&#22909;&#30340;&#36895;&#29575;&#22833;&#30495;&#24615;&#33021;&#12290;&#35813;&#31639;&#27861;&#22312;&#22810;&#31181;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21253;&#25324;&#20302;&#20998;&#36776;&#29575;&#33267;&#39640;&#20998;&#36776;&#29575;&#30340;&#25968;&#25454;&#38598;&#65292;&#22914;Set 5&#12289;Set 7&#12289;Classic 5&#12289;Set 14&#12289;Live 1&#12289;Kodak&#12289;General 100&#12289;CLIC 2019&#12290;&#19982;JPEG&#12289;JPEG2000&#12289;BPG&#21644;&#26368;&#36817;&#30340;CNN&#26041;&#27861;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;PSNR&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#65292;&#20302;&#27604;&#29305;&#29575;&#19979;&#30340;&#25913;&#36827;&#20998;&#21035;&#36798;&#21040;&#20102;20.75&#65285;&#12289;8.47&#65285;&#12289;3.22&#65285;&#12289;3.23&#65285;&#65292;&#39640;&#27604;&#29305;&#29575;&#19979;&#30340;&#25913;&#36827;&#20998;&#21035;&#36798;&#21040;&#20102;24.59&#65285;&#12289;14.46&#65285;&#12289;10.14&#65285;&#12289;8.57&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a learning-based compression scheme that envelopes a standard codec between pre and post-processing deep CNNs. Specifically, we demonstrate improvements over prior approaches utilizing a compression-decompression network by introducing: (a) an edge-aware loss function to prevent blurring that is commonly occurred in prior works &amp; (b) a super-resolution convolutional neural network (CNN) for post-processing along with a corresponding pre-processing network for improved rate-distortion performance in the low rate regime. The algorithm is assessed on a variety of datasets varying from low to high resolution namely Set 5, Set 7, Classic 5, Set 14, Live 1, Kodak, General 100, CLIC 2019. When compared to JPEG, JPEG2000, BPG, and recent CNN approach, the proposed algorithm contributes significant improvement in PSNR with an approximate gain of 20.75%, 8.47%, 3.22%, 3.23% and 24.59%, 14.46%, 10.14%, 8.57% at low and high bit-rates respectively. Similarly, this improvement in MS-SSIM
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#26080;&#38656;&#23398;&#20064;&#27169;&#22411;&#21363;&#21487;&#38598;&#25104;&#22870;&#36175;&#26799;&#24230;&#65292;&#25552;&#39640;&#20102;&#31574;&#30053;&#23398;&#20064;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2103.05147</link><description>&lt;p&gt;
&#26080;&#27169;&#22411;&#30340;&#22522;&#20110;&#22870;&#36175;&#26799;&#24230;&#30340;&#31574;&#30053;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Model-free Policy Learning with Reward Gradients. (arXiv:2103.05147v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.05147
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#26080;&#38656;&#23398;&#20064;&#27169;&#22411;&#21363;&#21487;&#38598;&#25104;&#22870;&#36175;&#26799;&#24230;&#65292;&#25552;&#39640;&#20102;&#31574;&#30053;&#23398;&#20064;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#20294;&#22312;&#26679;&#26412;&#31232;&#32570;&#30340;&#24212;&#29992;&#20013;&#65292;&#22914;&#26426;&#22120;&#20154;&#23398;&#65292;&#23427;&#20204;&#20173;&#26410;&#34987;&#24191;&#27867;&#21033;&#29992;&#12290;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#21487;&#29992;&#20449;&#24687;&#65292;&#21487;&#20197;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;&#20316;&#20026;&#22686;&#24378;&#23398;&#20064;&#20013;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#22870;&#36175;&#20989;&#25968;&#36890;&#24120;&#34987;&#31934;&#24515;&#35774;&#35745;&#20197;&#24341;&#23548;&#26234;&#33021;&#20307;&#12290;&#22240;&#27492;&#65292;&#22870;&#36175;&#20989;&#25968;&#36890;&#24120;&#26159;&#24050;&#30693;&#30340;&#65292;&#21487;&#20197;&#35775;&#38382;&#26631;&#37327;&#22870;&#36175;&#20449;&#21495;&#21644;&#22870;&#36175;&#26799;&#24230;&#12290;&#20026;&#20102;&#20174;&#22870;&#36175;&#26799;&#24230;&#20013;&#33719;&#30410;&#65292;&#20043;&#21069;&#30340;&#24037;&#20316;&#38656;&#35201;&#20102;&#35299;&#29615;&#22659;&#21160;&#24577;&#65292;&#36825;&#26159;&#24456;&#38590;&#33719;&#24471;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#8212;&#8212;&#22870;&#36175;&#31574;&#30053;&#26799;&#24230;&#20272;&#35745;&#22120;&#65292;&#23427;&#21487;&#20197;&#38598;&#25104;&#22870;&#36175;&#26799;&#24230;&#32780;&#26080;&#38656;&#23398;&#20064;&#27169;&#22411;&#12290;&#32469;&#36807;&#27169;&#22411;&#21160;&#24577;&#20351;&#25105;&#20204;&#30340;&#20272;&#35745;&#22120;&#33021;&#22815;&#22312;&#20559;&#24046;-&#26041;&#24046;&#26435;&#34913;&#26041;&#38754;&#21462;&#24471;&#26356;&#22909;&#30340;&#25928;&#26524;&#65292;&#36825;&#23548;&#33268;&#26356;&#39640;&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#22914;&#32463;&#39564;&#20998;&#26512;&#25152;&#31034;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#25552;&#21319;&#20102;&#19981;&#21516;&#29615;&#22659;&#19979;&#30340;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the increasing popularity of policy gradient methods, they are yet to be widely utilized in sample-scarce applications, such as robotics. The sample efficiency could be improved by making best usage of available information. As a key component in reinforcement learning, the reward function is usually devised carefully to guide the agent. Hence, the reward function is usually known, allowing access to not only scalar reward signals but also reward gradients. To benefit from reward gradients, previous works require the knowledge of environment dynamics, which are hard to obtain. In this work, we develop the \textit{Reward Policy Gradient} estimator, a novel approach that integrates reward gradients without learning a model. Bypassing the model dynamics allows our estimator to achieve a better bias-variance trade-off, which results in a higher sample efficiency, as shown in the empirical analysis. Our method also boosts the performance of Proximal Policy Optimization on different 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#21033;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#33258;&#21160;&#33719;&#21462;&#30149;&#24773;&#21464;&#37327;&#30340;&#35745;&#31639;&#34920;&#22411;&#65292;&#24182;&#25551;&#36848;&#20102;&#37325;&#30151;&#30417;&#25252;&#23460;&#30149;&#20154;&#30340;&#30149;&#24773;&#36716;&#25442;&#12290;&#36890;&#36807;&#36830;&#32493;&#30340;&#30149;&#24773;&#29366;&#24577;&#21644;&#32858;&#31867;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#23545;ICU&#30149;&#20154;&#20020;&#24202;&#36827;&#23637;&#30340;&#23637;&#31034;&#12290;</title><link>http://arxiv.org/abs/2005.05163</link><description>&lt;p&gt;
&#22312;&#37325;&#30151;&#30417;&#25252;&#23460;&#20013;&#30340;&#30149;&#20154;&#30149;&#24773;&#21487;&#35745;&#31639;&#30340;&#34920;&#22411;
&lt;/p&gt;
&lt;p&gt;
Computable Phenotypes of Patient Acuity in the Intensive Care Unit. (arXiv:2005.05163v2 [q-bio.QM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2005.05163
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#21033;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#33258;&#21160;&#33719;&#21462;&#30149;&#24773;&#21464;&#37327;&#30340;&#35745;&#31639;&#34920;&#22411;&#65292;&#24182;&#25551;&#36848;&#20102;&#37325;&#30151;&#30417;&#25252;&#23460;&#30149;&#20154;&#30340;&#30149;&#24773;&#36716;&#25442;&#12290;&#36890;&#36807;&#36830;&#32493;&#30340;&#30149;&#24773;&#29366;&#24577;&#21644;&#32858;&#31867;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#23545;ICU&#30149;&#20154;&#20020;&#24202;&#36827;&#23637;&#30340;&#23637;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#30417;&#27979;&#21644;&#30149;&#20154;&#30149;&#24773;&#35780;&#20272;&#26159;&#37325;&#30151;&#30417;&#25252;&#23460;&#23454;&#36341;&#30340;&#20851;&#38190;&#65292;&#20294;&#37117;&#21463;&#21040;&#21307;&#25252;&#20154;&#21592;&#26102;&#38388;&#38480;&#21046;&#30340;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#39044;&#27979;&#20020;&#24202;&#36827;&#23637;&#20173;&#28982;&#19981;&#31934;&#30830;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#65288;1&#65289;&#21033;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#30340;&#33258;&#21160;&#21464;&#37327;&#33719;&#21462;&#24320;&#21457;&#19968;&#20010;&#30149;&#24773;&#34920;&#22411;&#21644;&#65288;2&#65289;&#25551;&#36848;&#37325;&#30151;&#30417;&#25252;&#23460;&#30149;&#20154;&#30340;&#30149;&#24773;&#36716;&#25442;&#65292;&#20197;&#23637;&#31034;&#20020;&#24202;&#36827;&#23637;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;51,372&#21517;&#20837;&#20303;&#20315;&#32599;&#37324;&#36798;&#22823;&#23398;&#20581;&#24247;&#21307;&#38498;&#65288;UFH&#65289;&#30422;&#24681;&#26031;&#32500;&#23572;&#65288;GNV&#65289;&#21644;&#26480;&#20811;&#36874;&#32500;&#23572;&#65288;JAX&#65289;&#37325;&#30151;&#30417;&#25252;&#23460;&#30340;&#25104;&#24180;&#30149;&#20154;&#30340;&#20004;&#20010;&#21333;&#20013;&#24515;&#12289;&#32437;&#21521;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#31639;&#27861;&#65292;&#20197;&#27599;&#22235;&#23567;&#26102;&#20026;&#38388;&#38548;&#35745;&#31639;&#27599;&#27425;&#37325;&#30151;&#30417;&#25252;&#23460;&#20837;&#38498;&#30340;&#30149;&#24773;&#29366;&#20917;&#65292;&#24182;&#21033;&#29992;&#36830;&#32493;&#30340;&#30149;&#24773;&#29366;&#20917;&#21644;k-means&#32858;&#31867;&#26041;&#27861;&#35782;&#21035;&#30149;&#24773;&#34920;&#22411;&#12290;UFH GNV&#25968;&#25454;&#38598;&#20013;&#26377;38,749&#21517;&#30149;&#20154;&#30340;51,073&#27425;&#20837;&#38498;&#65292;JAX&#25968;&#25454;&#38598;&#20013;&#26377;12,623&#21517;&#30149;&#20154;&#30340;22,219&#27425;&#20837;&#38498;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continuous monitoring and patient acuity assessments are key aspects of Intensive Care Unit (ICU) practice, but both are limited by time constraints imposed on healthcare providers. Moreover, anticipating clinical trajectories remains imprecise. The objectives of this study are to (1) develop an electronic phenotype of acuity using automated variable retrieval within the electronic health records and (2) describe transitions between acuity states that illustrate the clinical trajectories of ICU patients. We gathered two single-center, longitudinal electronic health record datasets for 51,372 adult ICU patients admitted to the University of Florida Health (UFH) Gainesville (GNV) and Jacksonville (JAX). We developed algorithms to quantify acuity status at four-hour intervals for each ICU admission and identify acuity phenotypes using continuous acuity status and k-means clustering approach. 51,073 admissions for 38,749 patients in the UFH GNV dataset and 22,219 admissions for 12,623 pati
&lt;/p&gt;</description></item><item><title>LocoGAN&#26159;&#19968;&#20010;&#23436;&#20840;&#21367;&#31215;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#27169;&#22411;&#65292;&#20351;&#29992;&#20102;&#23616;&#37096;&#23398;&#20064;&#21644;&#20301;&#32622;&#36890;&#36947;&#25216;&#26415;&#65292;&#33021;&#22815;&#29983;&#25104;&#20219;&#24847;&#32500;&#24230;&#30340;&#22270;&#20687;&#65292;&#21253;&#25324;&#21608;&#26399;&#24615;&#25110;&#26080;&#38480;&#38271;&#30340;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2002.07897</link><description>&lt;p&gt;
LocoGAN -- &#26412;&#22320;&#21367;&#31215;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
LocoGAN -- Locally Convolutional GAN. (arXiv:2002.07897v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2002.07897
&lt;/p&gt;
&lt;p&gt;
LocoGAN&#26159;&#19968;&#20010;&#23436;&#20840;&#21367;&#31215;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#27169;&#22411;&#65292;&#20351;&#29992;&#20102;&#23616;&#37096;&#23398;&#20064;&#21644;&#20301;&#32622;&#36890;&#36947;&#25216;&#26415;&#65292;&#33021;&#22815;&#29983;&#25104;&#20219;&#24847;&#32500;&#24230;&#30340;&#22270;&#20687;&#65292;&#21253;&#25324;&#21608;&#26399;&#24615;&#25110;&#26080;&#38480;&#38271;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#23436;&#20840;&#21367;&#31215;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#27169;&#22411;&#65306;LocoGAN&#65292;&#20854;&#20013;&#28508;&#31354;&#38388;&#30001;&#21487;&#33021;&#20855;&#26377;&#19981;&#21516;&#20998;&#36776;&#29575;&#30340;&#22122;&#22768;&#22270;&#20687;&#34920;&#31034;&#12290;&#23398;&#20064;&#36807;&#31243;&#26159;&#23616;&#37096;&#30340;&#65292;&#21363;&#25105;&#20204;&#22788;&#29702;&#30340;&#19981;&#26159;&#25972;&#20010;&#22122;&#22768;&#22270;&#20687;&#65292;&#32780;&#26159;&#22266;&#23450;&#23610;&#23544;&#30340;&#23376;&#22270;&#20687;&#12290;&#22240;&#27492;&#65292;LocoGAN&#33021;&#22815;&#29983;&#25104;&#20219;&#24847;&#23610;&#23544;&#30340;&#22270;&#20687;&#65292;&#27604;&#22914;LSUN&#21351;&#23460;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#21478;&#19968;&#20010;&#20248;&#28857;&#26159;&#25105;&#20204;&#20351;&#29992;&#20102;&#20301;&#32622;&#36890;&#36947;&#65292;&#36825;&#20801;&#35768;&#29983;&#25104;&#23436;&#20840;&#21608;&#26399;&#24615;&#30340;&#65288;&#20363;&#22914;&#22278;&#26609;&#24418;&#20840;&#26223;&#22270;&#20687;&#65289;&#25110;&#20960;&#20046;&#21608;&#26399;&#24615;&#30340;&#8220;&#26080;&#38480;&#38271;&#8221;&#22270;&#20687;&#65288;&#20363;&#22914;&#22721;&#32440;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the paper we construct a fully convolutional GAN model: LocoGAN, which latent space is given by noise-like images of possibly different resolutions. The learning is local, i.e. we process not the whole noise-like image, but the sub-images of a fixed size. As a consequence LocoGAN can produce images of arbitrary dimensions e.g. LSUN bedroom data set. Another advantage of our approach comes from the fact that we use the position channels, which allows the generation of fully periodic (e.g. cylindrical panoramic images) or almost periodic ,,infinitely long" images (e.g. wall-papers).
&lt;/p&gt;</description></item></channel></rss>