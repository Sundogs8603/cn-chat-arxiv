<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;C-LoRA&#65292;&#29992;&#20110;&#25345;&#32493;&#33258;&#23450;&#21046;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#36991;&#20813;&#20102;&#26032;&#27010;&#24565;&#21152;&#20837;&#21518;&#36807;&#21435;&#30456;&#20284;&#27010;&#24565;&#30340;&#22270;&#20687;&#36136;&#37327;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.06027</link><description>&lt;p&gt;
&#25345;&#32493;&#25193;&#25955;&#65306;&#20351;&#29992;C-LoRA&#36827;&#34892;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#30340;&#25345;&#32493;&#23450;&#21046;
&lt;/p&gt;
&lt;p&gt;
Continual Diffusion: Continual Customization of Text-to-Image Diffusion with C-LoRA. (arXiv:2304.06027v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06027
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;C-LoRA&#65292;&#29992;&#20110;&#25345;&#32493;&#33258;&#23450;&#21046;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#36991;&#20813;&#20102;&#26032;&#27010;&#24565;&#21152;&#20837;&#21518;&#36807;&#21435;&#30456;&#20284;&#27010;&#24565;&#30340;&#22270;&#20687;&#36136;&#37327;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#21482;&#25552;&#20379;&#23569;&#37327;&#31034;&#20363;&#22270;&#20687;&#30340;&#24773;&#20917;&#19979;&#65292;&#33258;&#23450;&#20041;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20855;&#26377;&#26174;&#30528;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24403;&#20351;&#29992;&#22810;&#20010;&#32454;&#31890;&#24230;&#27010;&#24565;&#20197;&#36830;&#32493;&#26041;&#24335;&#65288;&#21363;&#25345;&#32493;&#24615;&#22320;&#65289;&#33258;&#23450;&#20041;&#36825;&#26679;&#30340;&#27169;&#22411;&#26102;&#65292;&#26368;&#36817;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#23450;&#21046;&#25216;&#26415;&#20250;&#36973;&#21463;&#28798;&#38590;&#24615;&#30340;&#36951;&#24536;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;C-LoRA&#65292;&#37319;&#29992;&#27969;&#34892;&#30340;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#36328;&#27880;&#24847;&#21147;&#23618;&#20013;&#30340;&#36830;&#32493;&#33258;&#25105;&#27491;&#21017;&#21270;&#20302;&#31209;&#36866;&#24212;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#19981;&#21253;&#25324;&#33258;&#23450;&#20041;&#23545;&#35937;&#30340;&#21333;&#35789;&#65288;&#21363;&#8220;&#20154;&#8221;&#29992;&#20110;&#20154;&#33080;&#25968;&#25454;&#38598;&#65289;&#24182;&#21021;&#22987;&#21270;&#20026;&#23436;&#20840;&#38543;&#26426;&#23884;&#20837;&#30340;&#23450;&#21046;&#25552;&#31034;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21482;&#24341;&#20837;&#20102;&#24494;&#23567;&#30340;&#39069;&#22806;&#21442;&#25968;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works demonstrate a remarkable ability to customize text-to-image diffusion models while only providing a few example images. What happens if you try to customize such models using multiple, fine-grained concepts in a sequential (i.e., continual) manner? In our work, we show that recent state-of-the-art customization of text-to-image models suffer from catastrophic forgetting when new concepts arrive sequentially. Specifically, when adding a new concept, the ability to generate high quality images of past, similar concepts degrade. To circumvent this forgetting, we propose a new method, C-LoRA, composed of a continually self-regularized low-rank adaptation in cross attention layers of the popular Stable Diffusion model. Furthermore, we use customization prompts which do not include the word of the customized object (i.e., "person" for a human face dataset) and are initialized as completely random embeddings. Importantly, our method induces only marginal additional parameter cost
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31232;&#30095;&#26631;&#27880;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#20154;&#32676;&#35745;&#25968;&#65292;&#20854;&#21487;&#20943;&#23569;&#26631;&#27880;&#24037;&#20316;&#24182;&#33719;&#21462;&#26356;&#22810;&#22810;&#26679;&#21270;&#20449;&#24687;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28857;&#30340;&#28176;&#36827;&#28857;&#21305;&#37197;&#32593;&#32476;&#65288;PPM&#65289;&#65292;&#22312;&#30456;&#21516;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#65292;PPM&#21487;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.06021</link><description>&lt;p&gt;
&#31232;&#30095;&#26631;&#27880;&#19979;&#30340;&#20154;&#32676;&#35745;&#25968;
&lt;/p&gt;
&lt;p&gt;
Crowd Counting with Sparse Annotation. (arXiv:2304.06021v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06021
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31232;&#30095;&#26631;&#27880;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#20154;&#32676;&#35745;&#25968;&#65292;&#20854;&#21487;&#20943;&#23569;&#26631;&#27880;&#24037;&#20316;&#24182;&#33719;&#21462;&#26356;&#22810;&#22810;&#26679;&#21270;&#20449;&#24687;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28857;&#30340;&#28176;&#36827;&#28857;&#21305;&#37197;&#32593;&#32476;&#65288;PPM&#65289;&#65292;&#22312;&#30456;&#21516;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#65292;PPM&#21487;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#31232;&#30095;&#26631;&#27880;&#65288;SA&#65289;&#30340;&#26032;&#26631;&#27880;&#26041;&#27861;&#65292;&#29992;&#20110;&#20154;&#32676;&#35745;&#25968;&#65292;&#36890;&#36807;&#23545;&#22270;&#20687;&#20013;&#20010;&#20307;&#36827;&#34892;&#31232;&#30095;&#26631;&#27880;&#26469;&#20943;&#23569;&#20154;&#24037;&#26631;&#27880;&#24037;&#20316;&#12290;&#25105;&#20204;&#35748;&#20026;&#31232;&#30095;&#26631;&#27880;&#21487;&#20197;&#20943;&#23569;&#23436;&#20840;&#26631;&#27880;&#30340;&#20887;&#20313;&#65292;&#24182;&#20174;&#26410;&#34987;&#23436;&#20840;&#25429;&#25417;&#30340;&#36828;&#22788;&#20010;&#20307;&#20013;&#25429;&#33719;&#26356;&#22810;&#22810;&#26679;&#21270;&#30340;&#20449;&#24687;&#65292;&#32780;&#36825;&#26159;&#37096;&#20998;&#26631;&#27880;&#26041;&#27861;&#26080;&#27861;&#36798;&#21040;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28857;&#30340;&#28176;&#36827;&#28857;&#21305;&#37197;&#32593;&#32476;&#65288;PPM&#65289;&#65292;&#20197;&#26356;&#22909;&#22320;&#20174;&#25972;&#20010;&#22270;&#20687;&#20013;&#25506;&#32034;&#31232;&#30095;&#26631;&#27880;&#30340;&#20154;&#32676;&#65292;&#20854;&#20013;&#21253;&#25324;&#25552;&#35758;&#21305;&#37197;&#32593;&#32476;&#65288;PMN&#65289;&#21644;&#24615;&#33021;&#24674;&#22797;&#32593;&#32476;&#65288;PRN&#65289;&#12290;PMN&#20351;&#29992;&#22522;&#26412;&#30340;&#28857;&#20998;&#31867;&#22120;&#29983;&#25104;&#20266;&#28857;&#26679;&#26412;&#65292;&#32780;PRN&#21017;&#20351;&#29992;&#20266;&#28857;&#23545;&#28857;&#20998;&#31867;&#22120;&#36827;&#34892;&#32454;&#21270;&#20197;&#26368;&#22823;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#30456;&#21516;&#25968;&#37327;&#30340;&#27880;&#37322;&#19979;&#65292;PPM&#27604;&#20808;&#21069;&#30340;&#21322;&#30417;&#30563;&#20154;&#32676;&#35745;&#25968;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#33719;&#24471;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#20840;&#30417;&#30563;&#26041;&#27861;&#30456;&#31454;&#20105;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a new annotation method called Sparse Annotation (SA) for crowd counting, which reduces human labeling efforts by sparsely labeling individuals in an image. We argue that sparse labeling can reduce the redundancy of full annotation and capture more diverse information from distant individuals that is not fully captured by Partial Annotation methods. Besides, we propose a point-based Progressive Point Matching network (PPM) to better explore the crowd from the whole image with sparse annotation, which includes a Proposal Matching Network (PMN) and a Performance Restoration Network (PRN). The PMN generates pseudo-point samples using a basic point classifier, while the PRN refines the point classifier with the pseudo points to maximize performance. Our experimental results show that PPM outperforms previous semi-supervised crowd counting methods with the same amount of annotation by a large margin and achieves competitive performance with state-of-the-art fully-supervi
&lt;/p&gt;</description></item><item><title>PD-ADSV&#26159;&#19968;&#31181;&#20351;&#29992;&#35821;&#38899;&#20449;&#21495;&#33258;&#21160;&#35786;&#26029;&#24085;&#37329;&#26862;&#30149;&#30340;&#31995;&#32479;&#65292;&#23427;&#37319;&#29992;&#22235;&#20010;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#21644;&#30828;&#25237;&#31080;&#38598;&#25104;&#26041;&#27861;&#36798;&#21040;&#26368;&#39640;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.06016</link><description>&lt;p&gt;
PD-ADSV&#65306;&#19968;&#31181;&#20351;&#29992;&#35821;&#38899;&#20449;&#21495;&#21644;&#30828;&#25237;&#31080;&#38598;&#25104;&#26041;&#27861;&#36827;&#34892;&#24085;&#37329;&#26862;&#30149;&#33258;&#21160;&#35786;&#26029;&#30340;&#31995;&#32479;(arXiv:2304.06016v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
PD-ADSV: An Automated Diagnosing System Using Voice Signals and Hard Voting Ensemble Method for Parkinson's Disease. (arXiv:2304.06016v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06016
&lt;/p&gt;
&lt;p&gt;
PD-ADSV&#26159;&#19968;&#31181;&#20351;&#29992;&#35821;&#38899;&#20449;&#21495;&#33258;&#21160;&#35786;&#26029;&#24085;&#37329;&#26862;&#30149;&#30340;&#31995;&#32479;&#65292;&#23427;&#37319;&#29992;&#22235;&#20010;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#21644;&#30828;&#25237;&#31080;&#38598;&#25104;&#26041;&#27861;&#36798;&#21040;&#26368;&#39640;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24085;&#37329;&#26862;&#30149;&#65288;PD&#65289;&#26159;&#26368;&#24120;&#35265;&#30340;&#36816;&#21160;&#38556;&#30861;&#21644;&#20165;&#27425;&#20110;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#31532;&#20108;&#24120;&#35265;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#12290;&#36816;&#21160;&#30151;&#29366;&#21644;&#25104;&#20687;&#25216;&#26415;&#26159;&#35786;&#26029;&#35813;&#30142;&#30149;&#26368;&#24120;&#29992;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#19981;&#20934;&#30830;&#21644;&#24555;&#36895;&#65292;&#24182;&#19988;&#21487;&#33021;&#20165;&#23545;&#23569;&#25968;&#20154;&#21487;&#29992;&#12290;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#38899;&#20449;&#21495;&#36827;&#34892;PD&#35786;&#26029;&#30340;&#33258;&#20027;&#31995;&#32479;PD-ADSV&#65292;&#35813;&#31995;&#32479;&#20351;&#29992;&#20102;&#22235;&#20010;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#21644;&#30828;&#25237;&#31080;&#38598;&#25104;&#26041;&#27861;&#20197;&#33719;&#24471;&#26368;&#39640;&#31934;&#24230;&#12290;PD-ADSV&#20351;&#29992;Python&#21644;Gradio web&#26694;&#26550;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parkinson's disease (PD) is the most widespread movement condition and the second most common neurodegenerative disorder, following Alzheimer's. Movement symptoms and imaging techniques are the most popular ways to diagnose this disease. However, they are not accurate and fast and may only be accessible to a few people. This study provides an autonomous system, i.e., PD-ADSV, for diagnosing PD based on voice signals, which uses four machine learning classifiers and the hard voting ensemble method to achieve the highest accuracy. PD-ADSV is developed using Python and the Gradio web framework.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#24515;&#33039;&#30149;&#25968;&#25454;&#38598;&#30340;&#24515;&#33039;&#30149;&#35786;&#26029;&#31995;&#32479;&#65292;&#24182;&#37319;&#29992;&#22534;&#21472;&#38598;&#25104;&#25216;&#26415;&#65292;&#36890;&#36807;&#32452;&#21512;&#22810;&#20010;&#22522;&#27169;&#22411;&#25552;&#39640;&#20102;&#39044;&#27979;&#20934;&#30830;&#29575;&#65292;&#36798;&#21040;&#20102;91.8%&#30340;&#39640;&#39044;&#27979;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.06015</link><description>&lt;p&gt;
&#19968;&#31181;&#20351;&#29992;&#22534;&#21472;&#38598;&#25104;&#26041;&#27861;&#25913;&#36827;&#24515;&#33039;&#30142;&#30149;&#39044;&#27979;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Improved Heart Disease Prediction Using Stacked Ensemble Method. (arXiv:2304.06015v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#24515;&#33039;&#30149;&#25968;&#25454;&#38598;&#30340;&#24515;&#33039;&#30149;&#35786;&#26029;&#31995;&#32479;&#65292;&#24182;&#37319;&#29992;&#22534;&#21472;&#38598;&#25104;&#25216;&#26415;&#65292;&#36890;&#36807;&#32452;&#21512;&#22810;&#20010;&#22522;&#27169;&#22411;&#25552;&#39640;&#20102;&#39044;&#27979;&#20934;&#30830;&#29575;&#65292;&#36798;&#21040;&#20102;91.8%&#30340;&#39640;&#39044;&#27979;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#33039;&#30142;&#30149;&#24050;&#32463;&#21462;&#20195;&#30284;&#30151;&#25104;&#20026;&#20840;&#29699;&#26368;&#22823;&#30340;&#27515;&#22240;&#12290;&#21450;&#26089;&#21457;&#29616;&#21644;&#27835;&#30103;&#21487;&#20197;&#38477;&#20302;&#22810;&#31181;&#24515;&#33039;&#30149;&#30456;&#20851;&#30340;&#24182;&#21457;&#30151;&#12289;&#27515;&#20129;&#29575;&#21644;&#35786;&#26029;&#25104;&#26412;&#12290;&#21307;&#30103;&#34892;&#19994;&#25910;&#38598;&#20102;&#22823;&#37327;&#21307;&#30103;&#25968;&#25454;&#65292;&#20294;&#23578;&#26410;&#24456;&#22909;&#22320;&#24320;&#21457;&#12290;&#22312;&#36825;&#20123;&#25968;&#25454;&#20013;&#21457;&#29616;&#20043;&#21069;&#26410;&#30693;&#30340;&#27169;&#24335;&#21644;&#32852;&#31995;&#21487;&#20197;&#24110;&#21161;&#26356;&#22909;&#22320;&#39044;&#27979;&#24515;&#33039;&#30142;&#30149;&#30340;&#39118;&#38505;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#24515;&#33039;&#30142;&#30149;&#25968;&#25454;&#38598;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#24515;&#33039;&#30149;&#35786;&#26029;&#31995;&#32479;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#35768;&#22810;&#25968;&#25454;&#39044;&#22788;&#29702;&#25216;&#26415;&#65292;&#22914;&#24322;&#24120;&#20540;&#26816;&#27979;&#21644;&#21435;&#38500;&#12289;&#32570;&#22833;&#20540;&#26816;&#27979;&#21644;&#21435;&#38500;&#12289;&#29305;&#24449;&#35268;&#33539;&#21270;&#12289;&#20132;&#21449;&#39564;&#35777;&#31561;&#65292;&#24182;&#20351;&#29992;9&#31181;&#20998;&#31867;&#31639;&#27861;&#21644;8&#31181;&#20998;&#31867;&#22120;&#24615;&#33021;&#24230;&#37327;&#25351;&#26631;&#12290;&#20026;&#20102;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#29575;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#22534;&#21472;&#38598;&#25104;&#25216;&#26415;&#65292;&#23558;&#22810;&#20010;&#22522;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#36827;&#34892;&#32452;&#21512;&#65292;&#36825;&#20123;&#22522;&#27169;&#22411;&#20351;&#29992;&#19981;&#21516;&#30340;&#31639;&#27861;&#31867;&#22411;&#21644;&#21442;&#25968;&#35774;&#32622;&#36827;&#34892;&#35757;&#32451;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#38598;&#25104;&#27169;&#22411;&#32988;&#36807;&#20102;&#21333;&#20010;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;Matthews&#30456;&#20851;&#31995;&#25968;&#20316;&#20026;&#24615;&#33021;&#24230;&#37327;&#25351;&#26631;&#65292;&#23454;&#29616;&#20102;91.8%&#30340;&#39640;&#39044;&#27979;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Heart disorder has just overtaken cancer as the world's biggest cause of mortality. Several cardiac failures, heart disease mortality, and diagnostic costs can all be reduced with early identification and treatment. Medical data is collected in large quantities by the healthcare industry, but it is not well mined. The discovery of previously unknown patterns and connections in this information can help with an improved decision when it comes to forecasting heart disorder risk. In the proposed study, we constructed an ML-based diagnostic system for heart illness forecasting, using a heart disorder dataset. We used data preprocessing techniques like outlier detection and removal, checking and removing missing entries, feature normalization, cross-validation, nine classification algorithms like RF, MLP, KNN, ETC, XGB, SVC, ADB, DT, and GBM, and eight classifier measuring performance metrics like ramification accuracy, precision, F1 score, specificity, ROC, sensitivity, log-loss, and Matth
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;MARL&#31639;&#27861;&#65292;BiLL&#65292;&#23427;&#23398;&#20064;&#19968;&#20010;&#21452;&#23618;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;&#35813;&#27169;&#22411;&#22312;&#39030;&#23618;&#23398;&#20064;&#20840;&#23616;&#29366;&#24577;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#24213;&#23618;&#23398;&#20064;&#27599;&#20010;&#26234;&#33021;&#20307;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#24182;&#29983;&#25104;&#28508;&#22312;&#36712;&#36857;&#29992;&#20110;&#31574;&#30053;&#23398;&#20064;&#12290;&#22312;SMAC&#21644;Flatland&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#26679;&#26412;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26080;&#27169;&#22411;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#22522;&#20934;&#31639;&#27861;&#65292;&#21253;&#25324;&#22312;Super Hard SMAC&#22320;&#22270;&#19978;&#12290;</title><link>http://arxiv.org/abs/2304.06011</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#21452;&#23618;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Bi-level Latent Variable Model for Sample-Efficient Multi-Agent Reinforcement Learning. (arXiv:2304.06011v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06011
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;MARL&#31639;&#27861;&#65292;BiLL&#65292;&#23427;&#23398;&#20064;&#19968;&#20010;&#21452;&#23618;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;&#35813;&#27169;&#22411;&#22312;&#39030;&#23618;&#23398;&#20064;&#20840;&#23616;&#29366;&#24577;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#24213;&#23618;&#23398;&#20064;&#27599;&#20010;&#26234;&#33021;&#20307;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#24182;&#29983;&#25104;&#28508;&#22312;&#36712;&#36857;&#29992;&#20110;&#31574;&#30053;&#23398;&#20064;&#12290;&#22312;SMAC&#21644;Flatland&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#26679;&#26412;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26080;&#27169;&#22411;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#22522;&#20934;&#31639;&#27861;&#65292;&#21253;&#25324;&#22312;Super Hard SMAC&#22320;&#22270;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;(MARL)&#31639;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20855;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#36890;&#24120;&#20855;&#26377;&#36739;&#39640;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;MARL&#31639;&#27861;&#65306;BiLL(&#21452;&#23618;&#28508;&#21464;&#37327;&#27169;&#22411;&#23398;&#20064;)&#65292;&#35813;&#31639;&#27861;&#20174;&#39640;&#32500;&#24230;&#36755;&#20837;&#20013;&#23398;&#20064;&#21452;&#23618;&#28508;&#22312;&#21464;&#37327;&#27169;&#22411;&#12290;&#22312;&#39030;&#23618;&#65292;&#35813;&#27169;&#22411;&#23398;&#20064;&#20840;&#23616;&#29366;&#24577;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#35813;&#34920;&#31034;&#32534;&#30721;&#19982;&#34892;&#20026;&#23398;&#20064;&#30456;&#20851;&#30340;&#20840;&#23616;&#20449;&#24687;&#12290;&#22312;&#24213;&#23618;&#65292;&#27169;&#22411;&#23398;&#20064;&#27599;&#20010;&#26234;&#33021;&#20307;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#32473;&#23450;&#26469;&#33258;&#39030;&#23618;&#30340;&#20840;&#23616;&#28508;&#22312;&#34920;&#31034;&#12290;&#27169;&#22411;&#29983;&#25104;&#28508;&#22312;&#36712;&#36857;&#20197;&#29992;&#20110;&#31574;&#30053;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;SMAC&#21644;Flatland&#29615;&#22659;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#26679;&#26412;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26080;&#27169;&#22411;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#22522;&#20934;&#31639;&#27861;&#65292;&#21253;&#25324;&#22312;&#20004;&#20010;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;Super Hard SMAC&#22320;&#22270;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite their potential in real-world applications, multi-agent reinforcement learning (MARL) algorithms often suffer from high sample complexity. To address this issue, we present a novel model-based MARL algorithm, BiLL (Bi-Level Latent Variable Model-based Learning), that learns a bi-level latent variable model from high-dimensional inputs. At the top level, the model learns latent representations of the global state, which encode global information relevant to behavior learning. At the bottom level, it learns latent representations for each agent, given the global latent representations from the top level. The model generates latent trajectories to use for policy learning. We evaluate our algorithm on complex multi-agent tasks in the challenging SMAC and Flatland environments. Our algorithm outperforms state-of-the-art model-free and model-based baselines in sample efficiency, including on two extremely challenging Super Hard SMAC maps.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36827;&#34892;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#22312;&#29289;&#27969;&#21644;&#20179;&#20648;&#20013;&#24212;&#29992;&#39046;&#22495;&#30340;&#25991;&#29486;&#32508;&#36848;&#65292;&#23558;&#25991;&#29486;&#20998;&#20026;&#30417;&#35270;&#21644;&#25805;&#20316;&#20004;&#20010;&#39046;&#22495;&#65292;&#24182;&#25351;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#26368;&#26032;&#21457;&#23637;&#12290;&#30740;&#31350;&#32467;&#26524;&#23545;&#20110;&#29289;&#27969;&#20174;&#19994;&#32773;&#20063;&#26377;&#21442;&#32771;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2304.06009</link><description>&lt;p&gt;
&#25991;&#29486;&#32508;&#36848;&#65306;&#35745;&#31639;&#26426;&#35270;&#35273;&#22312;&#29289;&#27969;&#21644;&#20179;&#20648;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Literature Review: Computer Vision Applications in Transportation Logistics and Warehousing. (arXiv:2304.06009v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06009
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36827;&#34892;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#22312;&#29289;&#27969;&#21644;&#20179;&#20648;&#20013;&#24212;&#29992;&#39046;&#22495;&#30340;&#25991;&#29486;&#32508;&#36848;&#65292;&#23558;&#25991;&#29486;&#20998;&#20026;&#30417;&#35270;&#21644;&#25805;&#20316;&#20004;&#20010;&#39046;&#22495;&#65292;&#24182;&#25351;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#26368;&#26032;&#21457;&#23637;&#12290;&#30740;&#31350;&#32467;&#26524;&#23545;&#20110;&#29289;&#27969;&#20174;&#19994;&#32773;&#20063;&#26377;&#21442;&#32771;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#35270;&#35273;&#22312;&#29289;&#27969;&#21644;&#20179;&#20648;&#20013;&#30340;&#24212;&#29992;&#20855;&#26377;&#24040;&#22823;&#30340;&#33258;&#21160;&#21270;&#28508;&#21147;&#12290;&#20026;&#20102;&#21033;&#29992;&#36825;&#20010;&#28508;&#21147;&#65292;&#25105;&#20204;&#23545;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#36827;&#34892;&#20102;&#32467;&#26500;&#21270;&#30340;&#25991;&#29486;&#32508;&#36848;&#12290;&#25152;&#26377;&#25991;&#29486;&#37117;&#34987;&#20998;&#31867;&#20026;&#24212;&#29992;&#21644;&#29992;&#20110;&#35299;&#20915;&#20219;&#21153;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#12290;&#20851;&#20110;&#24212;&#29992;&#65292;&#25105;&#20204;&#23558;&#25991;&#29486;&#20998;&#20026;&#20004;&#20010;&#39046;&#22495;&#65306;&#30417;&#35270;&#21644;&#25805;&#20316;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25351;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#65292;&#24182;&#38142;&#25509;&#21040;&#36866;&#29992;&#20110;&#29289;&#27969;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#26368;&#26032;&#21457;&#23637;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#29616;&#26377;&#25968;&#25454;&#38598;&#21644;&#24037;&#19994;&#35299;&#20915;&#26041;&#26696;&#30340;&#27010;&#36848;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#34429;&#28982;&#24050;&#32463;&#30740;&#31350;&#20102;&#35768;&#22810;&#30740;&#31350;&#39046;&#22495;&#65292;&#20294;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#21147;&#20173;&#28982;&#24040;&#22823;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#32467;&#26524;&#23545;&#29289;&#27969;&#20174;&#19994;&#32773;&#20063;&#24456;&#26377;&#24110;&#21161;&#65292;&#22240;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#24320;&#21457;&#21644;&#27979;&#35797;&#30340;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#30340;&#27010;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computer vision applications in transportation logistics and warehousing have a huge potential for process automation. We present a structured literature review on research in the field to help leverage this potential. All literature is categorized w.r.t. the application, i.e. the task it tackles and w.r.t. the computer vision techniques that are used. Regarding applications, we subdivide the literature in two areas: Monitoring, i.e. observing and retrieving relevant information from the environment, and manipulation, where approaches are used to analyze and interact with the environment. In addition to that, we point out directions for future research and link to recent developments in computer vision that are suitable for application in logistics. Finally, we present an overview of existing datasets and industrial solutions. We conclude that while already many research areas have been investigated, there is still huge potential for future research. The results of our analysis are als
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22312;PINNs&#20013;&#20351;&#29992;MLE&#30340;&#26041;&#27861;&#65292;&#28040;&#38500;&#20102;&#36229;&#21442;&#25968;&#35843;&#25972;&#12290;&#36890;&#36807;ODE&#32806;&#21512;&#30697;&#38453;&#30340;SVD&#20998;&#35299;&#38477;&#32500;&#65292;&#22686;&#21152;&#20102;PINNs&#39044;&#27979;&#30340;&#31283;&#23450;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.05991</link><description>&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22120;&#29992;&#20110;&#39640;&#32500;&#21453;&#38382;&#39064;&#27714;&#35299;
&lt;/p&gt;
&lt;p&gt;
Maximum-likelihood Estimators in Physics-Informed Neural Networks for High-dimensional Inverse Problems. (arXiv:2304.05991v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05991
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22312;PINNs&#20013;&#20351;&#29992;MLE&#30340;&#26041;&#27861;&#65292;&#28040;&#38500;&#20102;&#36229;&#21442;&#25968;&#35843;&#25972;&#12290;&#36890;&#36807;ODE&#32806;&#21512;&#30697;&#38453;&#30340;SVD&#20998;&#35299;&#38477;&#32500;&#65292;&#22686;&#21152;&#20102;PINNs&#39044;&#27979;&#30340;&#31283;&#23450;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(PINNs)&#24050;&#34987;&#35777;&#26126;&#26159;&#35299;&#20915;&#21453;&#24120;(ODE)&#21644;&#20559;&#24494;&#20998;&#26041;&#31243;(PDE)&#30340;&#21512;&#36866;&#25968;&#23398;&#26694;&#26550;&#12290;&#20856;&#22411;&#30340;&#21453;&#21521;PINNs&#34987;&#21046;&#23450;&#20026;&#24102;&#26377;&#20960;&#20010;&#36229;&#21442;&#25968;&#30340;&#36719;&#32422;&#26463;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#21453;&#21521;PINNs&#21487;&#20197;&#29992;&#26497;&#22823;&#20284;&#28982;&#20272;&#35745;&#22120;(MLE)&#30340;&#24418;&#24335;&#26469;&#34920;&#36798;&#65292;&#36890;&#36807;Taylor&#23637;&#24320;&#65292;&#23558;&#25554;&#20540;&#35823;&#24046;&#26126;&#30830;&#22320;&#20256;&#25773;&#21040;&#29289;&#29702;&#27169;&#22411;&#31354;&#38388;&#20013;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#36229;&#21442;&#25968;&#35843;&#25972;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#20854;&#24212;&#29992;&#20110;&#39640;&#32500;&#32806;&#21512;ODEs&#30340;&#24773;&#20917;&#65292;&#36825;&#20123;ODEs&#21463;&#21040;&#22312;&#30636;&#24577;&#21270;&#23398;&#21644;&#29983;&#29289;&#21160;&#21147;&#23398;&#20013;&#24120;&#35265;&#30340;&#24494;&#20998;&#20195;&#25968;&#26041;&#31243;&#30340;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;ODE&#32806;&#21512;&#30697;&#38453;(&#21453;&#24212;&#21270;&#23398;&#35745;&#37327;&#30697;&#38453;)&#30340;&#22855;&#24322;&#20540;&#20998;&#35299;(SVD)&#25552;&#20379;&#20102;&#20943;&#23569;&#30340;&#19981;&#30456;&#20851;&#23376;&#31354;&#38388;&#65292;&#22312;&#20854;&#20013;&#21487;&#20197;&#34920;&#31034;PINNs&#35299;&#65292;&#24182;&#21487;&#20197;&#23545;&#27531;&#24046;&#36827;&#34892;&#25237;&#24433;&#12290;&#26368;&#21518;&#65292;SVD&#22522;&#20989;&#25968;&#20316;&#20026;&#20808;&#39564;&#32422;&#26463;&#22686;&#24378;&#20102;&#39044;&#27979;&#30340;&#31283;&#23450;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics-informed neural networks (PINNs) have proven a suitable mathematical scaffold for solving inverse ordinary (ODE) and partial differential equations (PDE). Typical inverse PINNs are formulated as soft-constrained multi-objective optimization problems with several hyperparameters. In this work, we demonstrate that inverse PINNs can be framed in terms of maximum-likelihood estimators (MLE) to allow explicit error propagation from interpolation to the physical model space through Taylor expansion, without the need of hyperparameter tuning. We explore its application to high-dimensional coupled ODEs constrained by differential algebraic equations that are common in transient chemical and biological kinetics. Furthermore, we show that singular-value decomposition (SVD) of the ODE coupling matrices (reaction stoichiometry matrix) provides reduced uncorrelated subspaces in which PINNs solutions can be represented and over which residuals can be projected. Finally, SVD bases serve as pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#26080;&#30417;&#30563;&#23398;&#20064;&#30456;&#20284;&#29289;&#20307;&#20132;&#20114;&#65292;&#35825;&#23548;&#29289;&#20307;&#33021;&#21147;&#31751;&#65292;&#20351;&#29992;&#26032;&#39062;&#30340;&#28145;&#24230;&#24863;&#30693;&#23450;&#24615;&#31354;&#38388;&#34920;&#31034;&#27861;&#26500;&#24314;&#27963;&#21160;&#22270;&#24182;&#36827;&#34892;&#32858;&#31867;&#65292;&#20174;&#32780;&#23454;&#29616;&#20855;&#26377;&#19981;&#30830;&#23450;&#20132;&#20114;&#24320;&#25918;&#38598;&#21512;&#30340;&#31867;&#21035;&#26080;&#20851;&#29289;&#20307;&#30340;&#33021;&#21147;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2304.05989</link><description>&lt;p&gt;
&#36890;&#36807;&#26080;&#30417;&#30563;&#23398;&#20064;&#22270;&#23884;&#20837;&#36827;&#34892;&#29289;&#20307;&#26080;&#20851;&#33021;&#21147;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Object-agnostic Affordance Categorization via Unsupervised Learning of Graph Embeddings. (arXiv:2304.05989v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05989
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#26080;&#30417;&#30563;&#23398;&#20064;&#30456;&#20284;&#29289;&#20307;&#20132;&#20114;&#65292;&#35825;&#23548;&#29289;&#20307;&#33021;&#21147;&#31751;&#65292;&#20351;&#29992;&#26032;&#39062;&#30340;&#28145;&#24230;&#24863;&#30693;&#23450;&#24615;&#31354;&#38388;&#34920;&#31034;&#27861;&#26500;&#24314;&#27963;&#21160;&#22270;&#24182;&#36827;&#34892;&#32858;&#31867;&#65292;&#20174;&#32780;&#23454;&#29616;&#20855;&#26377;&#19981;&#30830;&#23450;&#20132;&#20114;&#24320;&#25918;&#38598;&#21512;&#30340;&#31867;&#21035;&#26080;&#20851;&#29289;&#20307;&#30340;&#33021;&#21147;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33719;&#21462;&#20851;&#20110;&#29289;&#20307;&#20132;&#20114;&#21644;&#33021;&#21147;&#30340;&#30693;&#35782;&#21487;&#20197;&#20419;&#36827;&#22330;&#26223;&#29702;&#35299;&#21644;&#20154;&#26426;&#21327;&#20316;&#20219;&#21153;&#12290;&#22312;&#26085;&#24120;&#29983;&#27963;&#22330;&#26223;&#20013;&#65292;&#20154;&#20204;&#20542;&#21521;&#20110;&#26681;&#25454;&#22330;&#26223;&#21644;&#29289;&#20307;&#30340;&#21487;&#29992;&#24615;&#20197;&#22810;&#31181;&#19981;&#21516;&#30340;&#26041;&#24335;&#20351;&#29992;&#29289;&#20307;&#12290;&#38024;&#23545;&#23384;&#22312;&#24320;&#25918;&#20114;&#21160;&#21644;&#29289;&#20307;&#30340;&#31867;&#21035;&#19981;&#30830;&#23450;&#30340;&#24773;&#20917;&#19979;&#65292;&#23398;&#20064;&#29289;&#20307;&#30340;&#33021;&#21147;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#23454;&#29616;&#20855;&#26377;&#19981;&#30830;&#23450;&#20132;&#20114;&#24320;&#25918;&#38598;&#21512;&#30340;&#31867;&#21035;&#26080;&#20851;&#29289;&#20307;&#30340;&#33021;&#21147;&#20998;&#31867;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#23398;&#20064;&#29289;&#20307;&#20132;&#20114;&#20043;&#38388;&#30340;&#30456;&#20284;&#20043;&#22788;&#20174;&#32780;&#35825;&#23548;&#29289;&#20307;&#33021;&#21147;&#31751;&#12290;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#24863;&#30693;&#23450;&#24615;&#31354;&#38388;&#34920;&#31034;&#27861;&#26469;&#26500;&#24314;&#27963;&#21160;&#22270;&#65288;AGs&#65289;&#65292;&#36825;&#20123;&#22270;&#20174;RGB-D&#35270;&#39057;&#20013;&#25277;&#35937;&#20986;&#26102;&#31354;&#20132;&#20114;&#30340;&#36830;&#32493;&#34920;&#31034;&#12290;&#28982;&#21518;&#23545;&#36825;&#20123;AGs&#36827;&#34892;&#32858;&#31867;&#65292;&#20197;&#33719;&#21462;&#20855;&#26377;&#31867;&#20284;&#33021;&#21147;&#30340;&#19968;&#32452;&#29289;&#20307;&#12290;&#25105;&#20204;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;
&lt;/p&gt;
&lt;p&gt;
Acquiring knowledge about object interactions and affordances can facilitate scene understanding and human-robot collaboration tasks. As humans tend to use objects in many different ways depending on the scene and the objects' availability, learning object affordances in everyday-life scenarios is a challenging task, particularly in the presence of an open set of interactions and objects. We address the problem of affordance categorization for class-agnostic objects with an open set of interactions; we achieve this by learning similarities between object interactions in an unsupervised way and thus inducing clusters of object affordances. A novel depth-informed qualitative spatial representation is proposed for the construction of Activity Graphs (AGs), which abstract from the continuous representation of spatio-temporal interactions in RGB-D videos. These AGs are clustered to obtain groups of objects with similar affordances. Our experiments in a real-world scenario demonstrate that o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35299;&#20915;&#20102;30&#22825;&#20869;&#20877;&#20837;&#38498;&#30340;&#38382;&#39064;&#65292;&#24182;&#23545;&#22522;&#20110;&#25935;&#24863;&#23646;&#24615;&#30340;&#23376;&#32452;&#36827;&#34892;&#20102;&#20844;&#24179;&#23457;&#35745;&#65292;&#32467;&#26524;&#21457;&#29616;&#35813;&#27169;&#22411;&#22312;&#19981;&#21516;&#32452;&#21035;&#20013;&#34920;&#29616;&#20986;&#24046;&#24322;&#65292;&#24378;&#35843;&#20102;&#24314;&#31435;&#26356;&#22909;&#30340;&#20844;&#27491;&#21644;&#20559;&#24046;&#32531;&#35299;&#31574;&#30053;&#30340;&#24517;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.05986</link><description>&lt;p&gt;
&#22312;&#20020;&#24202;&#25968;&#25454;&#24211;&#20013;&#23457;&#35745;ICU&#20877;&#20837;&#38498;&#29575;&#65306;&#39118;&#38505;&#22240;&#32032;&#21644;&#20020;&#24202;&#32467;&#26524;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Auditing ICU Readmission Rates in an Clinical Database: An Analysis of Risk Factors and Clinical Outcomes. (arXiv:2304.05986v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05986
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35299;&#20915;&#20102;30&#22825;&#20869;&#20877;&#20837;&#38498;&#30340;&#38382;&#39064;&#65292;&#24182;&#23545;&#22522;&#20110;&#25935;&#24863;&#23646;&#24615;&#30340;&#23376;&#32452;&#36827;&#34892;&#20102;&#20844;&#24179;&#23457;&#35745;&#65292;&#32467;&#26524;&#21457;&#29616;&#35813;&#27169;&#22411;&#22312;&#19981;&#21516;&#32452;&#21035;&#20013;&#34920;&#29616;&#20986;&#24046;&#24322;&#65292;&#24378;&#35843;&#20102;&#24314;&#31435;&#26356;&#22909;&#30340;&#20844;&#27491;&#21644;&#20559;&#24046;&#32531;&#35299;&#31574;&#30053;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27969;&#31243;&#29992;&#20110;&#20020;&#24202;&#25968;&#25454;&#20998;&#31867;&#65292;&#30446;&#30340;&#26159;&#35299;&#20915;30&#22825;&#20869;&#20877;&#20837;&#38498;&#30340;&#38382;&#39064;&#65292;&#24182;&#23545;&#22522;&#20110;&#25935;&#24863;&#23646;&#24615;&#30340;&#23376;&#32452;&#36827;&#34892;&#20102;&#20844;&#24179;&#23457;&#35745;&#12290;&#35813;&#20998;&#31867;&#38382;&#39064;&#20351;&#29992;&#20102;&#19968;&#31995;&#21015;ML&#27169;&#22411;&#65292;&#24182;&#22312;&#27169;&#22411;&#39044;&#27979;&#19978;&#36827;&#34892;&#20102;&#20844;&#24179;&#23457;&#35745;&#12290;&#38024;&#23545;MIMIC III&#25968;&#25454;&#24211;&#20013;&#30340;&#24615;&#21035;&#12289;&#31181;&#26063;&#12289;&#35821;&#35328;&#21644;&#20445;&#38505;&#32452;&#31561;&#23646;&#24615;&#65292;&#20844;&#24179;&#23457;&#35745;&#21457;&#29616;&#20102;&#26426;&#20250;&#24179;&#31561;&#12289;&#39044;&#27979;&#24179;&#31561;&#12289;&#35823;&#25253;&#29575;&#24179;&#31561;&#21644;&#28431;&#25253;&#29575;&#24179;&#31561;&#31561;&#26631;&#20934;&#23384;&#22312;&#24046;&#24322;&#12290;&#32467;&#26524;&#26174;&#31034;&#20102;&#35813;&#27169;&#22411;&#22312;&#19981;&#21516;&#32452;&#21035;&#20013;&#34920;&#29616;&#30340;&#24046;&#24322;&#65292;&#24182;&#24378;&#35843;&#20102;&#24314;&#31435;&#26356;&#22909;&#30340;&#20844;&#27491;&#21644;&#20559;&#24046;&#32531;&#35299;&#31574;&#30053;&#30340;&#24517;&#35201;&#24615;&#12290;&#26412;&#30740;&#31350;&#24314;&#35758;&#30740;&#31350;&#20154;&#21592;&#12289;&#25919;&#31574;&#21046;&#23450;&#32773;&#21644;&#23454;&#36341;&#32773;&#36827;&#34892;&#21512;&#20316;&#65292;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#30340;&#20559;&#24046;&#21644;&#20844;&#27491;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents a machine learning (ML) pipeline for clinical data classification in the context of a 30-day readmission problem, along with a fairness audit on subgroups based on sensitive attributes. A range of ML models are used for classification and the fairness audit is conducted on the model predictions. The fairness audit uncovers disparities in equal opportunity, predictive parity, false positive rate parity, and false negative rate parity criteria on the MIMIC III dataset based on attributes such as gender, ethnicity, language, and insurance group. The results identify disparities in the model's performance across different groups and highlights the need for better fairness and bias mitigation strategies. The study suggests the need for collaborative efforts among researchers, policymakers, and practitioners to address bias and fairness in artificial intelligence (AI) systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;&#31070;&#32463;&#27880;&#24847;&#26862;&#26519;&#30340;&#26032;&#26041;&#27861;&#65292;&#23558;&#27880;&#24847;&#21147;&#26426;&#21046;&#24341;&#20837;&#21040;&#38543;&#26426;&#26862;&#26519;&#20013;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#35745;&#31639;&#33719;&#24471;&#27880;&#24847;&#26435;&#37325;&#65292;&#36827;&#32780;&#35299;&#20915;&#22238;&#24402;&#21644;&#20998;&#31867;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2304.05980</link><description>&lt;p&gt;
&#31070;&#32463;&#27880;&#24847;&#26862;&#26519;&#65306;&#22522;&#20110;Transformer&#30340;&#26862;&#26519;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Neural Attention Forests: Transformer-Based Forest Improvement. (arXiv:2304.05980v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05980
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;&#31070;&#32463;&#27880;&#24847;&#26862;&#26519;&#30340;&#26032;&#26041;&#27861;&#65292;&#23558;&#27880;&#24847;&#21147;&#26426;&#21046;&#24341;&#20837;&#21040;&#38543;&#26426;&#26862;&#26519;&#20013;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#35745;&#31639;&#33719;&#24471;&#27880;&#24847;&#26435;&#37325;&#65292;&#36827;&#32780;&#35299;&#20915;&#22238;&#24402;&#21644;&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#31070;&#32463;&#27880;&#24847;&#26862;&#26519;&#65288;NAF&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#34920;&#26684;&#24418;&#24335;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#22238;&#24402;&#21644;&#20998;&#31867;&#20219;&#21153;&#12290;&#35813;&#27169;&#22411;&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#23558;&#27880;&#24847;&#21147;&#26426;&#21046;&#24341;&#20837;&#21040;&#38543;&#26426;&#26862;&#26519;&#20013;&#65292;&#22312; Nadaraya-Watson &#26680;&#22238;&#24402;&#26694;&#26550;&#19979;&#65292;&#36890;&#36807;&#23558;&#29305;&#23450;&#24418;&#24335;&#30340;&#31070;&#32463;&#32593;&#32476;&#35745;&#31639;&#24471;&#21040;&#30340;&#27880;&#24847;&#26435;&#37325;&#20998;&#37197;&#21040;&#20915;&#31574;&#26641;&#20013;&#30340;&#21494;&#23376;&#25968;&#25454;&#21644;&#38543;&#26426;&#26862;&#26519;&#26412;&#36523;&#20013;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#38543;&#26426;&#26862;&#26519;&#30340;&#27169;&#22411;&#19981;&#21516;&#65292;&#27880;&#24847;&#26435;&#37325;&#21644; Nadaraya-Watson &#22238;&#24402;&#37319;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#24418;&#24335;&#34920;&#31034;&#65292;&#20854;&#26435;&#37325;&#21487;&#20197;&#35270;&#20026;&#21487;&#35757;&#32451;&#21442;&#25968;&#12290;&#20849;&#20139;&#26435;&#37325;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#31532;&#19968;&#37096;&#20998;&#29992;&#20110;&#25152;&#26377;&#20915;&#31574;&#26641;&#30340;&#35757;&#32451;&#65292;&#24182;&#35745;&#31639;&#21494;&#23376;&#25968;&#25454;&#30340;&#27880;&#24847;&#21147;&#26435;&#37325;&#12290;&#31532;&#20108;&#37096;&#20998;&#32858;&#21512;&#26641;&#32593;&#32476;&#30340;&#36755;&#20986;&#65292;&#24182;&#26088;&#22312;&#26368;&#23567;&#21270;&#38543;&#26426;&#26862;&#26519;&#39044;&#27979;&#19982;&#35757;&#32451;&#38598;&#30446;&#26631;&#30495;&#20540;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
A new approach called NAF (the Neural Attention Forest) for solving regression and classification tasks under tabular training data is proposed. The main idea behind the proposed NAF model is to introduce the attention mechanism into the random forest by assigning attention weights calculated by neural networks of a specific form to data in leaves of decision trees and to the random forest itself in the framework of the Nadaraya-Watson kernel regression. In contrast to the available models like the attention-based random forest, the attention weights and the Nadaraya-Watson regression are represented in the form of neural networks whose weights can be regarded as trainable parameters. The first part of neural networks with shared weights is trained for all trees and computes attention weights of data in leaves. The second part aggregates outputs of the tree networks and aims to minimize the difference between the random forest prediction and the truth target value from a training set. 
&lt;/p&gt;</description></item><item><title>ImageReward&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#20154;&#31867;&#21916;&#22909;&#22870;&#21169;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#25910;&#38598;&#19987;&#23478;&#30340;&#27604;&#36739;&#25968;&#25454;&#38598;&#26469;&#35299;&#20915;&#29983;&#25104;&#27169;&#22411;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#20154;&#31867;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#26377;&#26395;&#25104;&#20026;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#21644;&#25913;&#36827;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#30340;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2304.05977</link><description>&lt;p&gt;
ImageReward&#65306;&#23398;&#20064;&#21644;&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#20154;&#31867;&#21916;&#22909;
&lt;/p&gt;
&lt;p&gt;
ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation. (arXiv:2304.05977v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05977
&lt;/p&gt;
&lt;p&gt;
ImageReward&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#20154;&#31867;&#21916;&#22909;&#22870;&#21169;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#25910;&#38598;&#19987;&#23478;&#30340;&#27604;&#36739;&#25968;&#25454;&#38598;&#26469;&#35299;&#20915;&#29983;&#25104;&#27169;&#22411;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#20154;&#31867;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#26377;&#26395;&#25104;&#20026;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#21644;&#25913;&#36827;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#30340;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#36890;&#29992;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20154;&#31867;&#21916;&#22909;&#22870;&#21169;&#27169;&#22411;ImageReward&#65292;&#26088;&#22312;&#35299;&#20915;&#29983;&#25104;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#21508;&#31181;&#38382;&#39064;&#65292;&#24182;&#20351;&#20854;&#19982;&#20154;&#31867;&#20215;&#20540;&#21644;&#20559;&#22909;&#20445;&#25345;&#19968;&#33268;&#12290;&#35813;&#22870;&#21169;&#27169;&#22411;&#30340;&#35757;&#32451;&#22522;&#20110;&#25105;&#20204;&#30340;&#31995;&#32479;&#27880;&#37322;&#27969;&#31243;&#65292;&#20854;&#20013;&#21253;&#25324;&#35780;&#20998;&#21644;&#25490;&#21517;&#32452;&#20214;&#65292;&#36804;&#20170;&#24050;&#25910;&#38598;&#20102;137k&#30340;&#19987;&#23478;&#27604;&#36739;&#25968;&#25454;&#38598;&#12290;&#22312;&#20154;&#31867;&#35780;&#20272;&#20013;&#65292;ImageReward&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#35780;&#20998;&#26041;&#27861;&#65288;&#20363;&#22914;&#27604;CLIP&#39640;38.6\%&#65289;&#65292;&#22240;&#27492;&#23427;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#29992;&#20110;&#35780;&#20272;&#21644;&#25913;&#36827;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#30340;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#12290;&#35813;&#22870;&#21169;&#27169;&#22411;&#36890;&#36807;\texttt {image-reward}&#31243;&#24207;&#21253;&#20844;&#24320;&#25552;&#20379;&#65292;&#32593;&#22336;&#20026;\url{https://github.com/THUDM/ImageReward}&#12290;
&lt;/p&gt;
&lt;p&gt;
We present ImageReward -- the first general-purpose text-to-image human preference reward model -- to address various prevalent issues in generative models and align them with human values and preferences. Its training is based on our systematic annotation pipeline that covers both the rating and ranking components, collecting a dataset of 137k expert comparisons to date. In human evaluation, ImageReward outperforms existing scoring methods (e.g., CLIP by 38.6\%), making it a promising automatic metric for evaluating and improving text-to-image synthesis. The reward model is publicly available via the \texttt{image-reward} package at \url{https://github.com/THUDM/ImageReward}.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#25552;&#31034;&#38598;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#29992;&#23567;&#22411;&#25968;&#25454;&#38598;&#26500;&#24314;&#38598;&#25104;&#25552;&#31034;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20248;&#20110;&#21333;&#25552;&#31034;&#36755;&#20986;&#31354;&#38388;&#38598;&#25104;&#21644;&#34955;&#35013;&#25552;&#31034;&#31354;&#38388;&#38598;&#25104;&#12290;</title><link>http://arxiv.org/abs/2304.05970</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22686;&#24378;&#25552;&#31034;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
Boosted Prompt Ensembles for Large Language Models. (arXiv:2304.05970v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05970
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#25552;&#31034;&#38598;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#29992;&#23567;&#22411;&#25968;&#25454;&#38598;&#26500;&#24314;&#38598;&#25104;&#25552;&#31034;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20248;&#20110;&#21333;&#25552;&#31034;&#36755;&#20986;&#31354;&#38388;&#38598;&#25104;&#21644;&#34955;&#35013;&#25552;&#31034;&#31354;&#38388;&#38598;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38142;&#24335;&#24605;&#32500;&#25552;&#31034;&#21644;&#33258;&#19968;&#33268;&#24615;&#31561;&#26041;&#27861;&#24050;&#32463;&#25512;&#21160;&#20102;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#24615;&#33021;&#30340;&#21069;&#27839;&#65292;&#32780;&#19988;&#27809;&#26377;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#65292;&#25105;&#20204;&#24314;&#35758;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#19968;&#31181;&#25552;&#31034;&#38598;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#23567;&#22411;&#25968;&#25454;&#38598;&#26469;&#26500;&#24314;&#19968;&#32452;&#23569;&#37327;&#30340;&#25552;&#31034;&#65292;&#36825;&#20123;&#25552;&#31034;&#20849;&#21516;&#26500;&#25104;&#20102;&#19968;&#20010;&#8220;&#22686;&#24378;&#30340;&#25552;&#31034;&#38598;&#25104;&#8221;&#12290;&#27599;&#20010;&#25552;&#31034;&#30340;&#23569;&#25968;&#26679;&#20363;&#26159;&#36890;&#36807;&#28176;&#36827;&#24335;&#26041;&#24335;&#36873;&#25321;&#30340;&#65292;&#20197;&#20415;&#22312;&#19978;&#19968;&#20010;&#27493;&#39588;&#30340;&#38598;&#25104;&#32467;&#26524;&#19981;&#30830;&#23450;&#26102;&#25104;&#20026;&#8220;&#22256;&#38590;&#8221;&#26679;&#20363;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;GSM8k&#21644;AQuA&#25968;&#25454;&#38598;&#31561;&#26041;&#38754;&#20248;&#20110;&#21333;&#25552;&#31034;&#36755;&#20986;&#31354;&#38388;&#38598;&#25104;&#21644;&#34955;&#35013;&#25552;&#31034;&#31354;&#38388;&#38598;&#25104;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#35757;&#32451;&#26102;&#38388;&#21644;&#27979;&#35797;&#26102;&#38388;&#29256;&#26412;&#30340;&#22686;&#24378;&#25552;&#31034;&#65292;&#24182;&#20351;&#29992;&#19981;&#21516;&#32423;&#21035;&#30340;&#21487;&#29992;&#27880;&#37322;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Methods such as chain-of-thought prompting and self-consistency have pushed the frontier of language model reasoning performance with no additional training. To further improve performance, we propose a prompt ensembling method for large language models, which uses a small dataset to construct a set of few shot prompts that together comprise a ``boosted prompt ensemble''. The few shot examples for each prompt are chosen in a stepwise fashion to be ``hard'' examples on which the previous step's ensemble is uncertain. We show that this outperforms single-prompt output-space ensembles and bagged prompt-space ensembles on the GSM8k and AQuA datasets, among others. We propose both train-time and test-time versions of boosted prompting that use different levels of available annotation and conduct a detailed empirical study of our algorithm.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#8212;&#8212;&#36335;&#24452;&#20462;&#34917;&#65292;&#29992;&#20110;&#34920;&#36798;&#21644;&#23450;&#37327;&#27979;&#35797;&#34920;&#26126;&#34892;&#20026;&#34987;&#23450;&#20301;&#21040;&#19968;&#32452;&#36335;&#24452;&#30340;&#19968;&#31867;&#33258;&#28982;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2304.05969</link><description>&lt;p&gt;
&#22522;&#20110;&#36335;&#24452;&#20462;&#34917;&#30340;&#27169;&#22411;&#34892;&#20026;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Localizing Model Behavior with Path Patching. (arXiv:2304.05969v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05969
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#8212;&#8212;&#36335;&#24452;&#20462;&#34917;&#65292;&#29992;&#20110;&#34920;&#36798;&#21644;&#23450;&#37327;&#27979;&#35797;&#34920;&#26126;&#34892;&#20026;&#34987;&#23450;&#20301;&#21040;&#19968;&#32452;&#36335;&#24452;&#30340;&#19968;&#31867;&#33258;&#28982;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#31070;&#32463;&#32593;&#32476;&#30340;&#34892;&#20026;&#23450;&#20301;&#21040;&#32593;&#32476;&#32452;&#20214;&#30340;&#26576;&#20010;&#23376;&#38598;&#25110;&#32452;&#20214;&#20043;&#38388;&#30340;&#26576;&#20010;&#20132;&#20114;&#30340;&#23376;&#38598;&#26159;&#20998;&#26512;&#32593;&#32476;&#26426;&#21046;&#21644;&#21487;&#33021;&#22833;&#25928;&#27169;&#24335;&#30340;&#33258;&#28982;&#31532;&#19968;&#27493;&#12290;&#29616;&#26377;&#24037;&#20316;&#24120;&#24120;&#26159;&#23450;&#24615;&#19988;&#20020;&#26102;&#30340;&#65292;&#23545;&#20110;&#35780;&#20272;&#23450;&#20301;&#22768;&#26126;&#30340;&#36866;&#24403;&#26041;&#24335;&#27809;&#26377;&#20849;&#35782;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#36335;&#24452;&#20462;&#34917;&#25216;&#26415;&#65292;&#29992;&#20110;&#34920;&#36798;&#21644;&#23450;&#37327;&#27979;&#35797;&#34920;&#26126;&#34892;&#20026;&#34987;&#23450;&#20301;&#21040;&#19968;&#32452;&#36335;&#24452;&#30340;&#19968;&#31867;&#33258;&#28982;&#20551;&#35774;&#12290;&#25105;&#20204;&#25913;&#36827;&#20102;&#24863;&#24212;&#22836;&#30340;&#35299;&#37322;&#65292;&#34920;&#24449;&#20102;GPT-2&#30340;&#34892;&#20026;&#65292;&#24182;&#24320;&#28304;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20197;&#20415;&#39640;&#25928;&#22320;&#36816;&#34892;&#31867;&#20284;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Localizing behaviors of neural networks to a subset of the network's components or a subset of interactions between components is a natural first step towards analyzing network mechanisms and possible failure modes. Existing work is often qualitative and ad-hoc, and there is no consensus on the appropriate way to evaluate localization claims. We introduce path patching, a technique for expressing and quantitatively testing a natural class of hypotheses expressing that behaviors are localized to a set of paths. We refine an explanation of induction heads, characterize a behavior of GPT-2, and open source a framework for efficiently running similar experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#22522;&#20110;&#38543;&#26426;&#30913;&#38567;&#36947;&#32467;&#65288;sMTJ&#65289;&#30340;&#27010;&#29575;&#27604;&#29305;&#65288;p&#20301;&#65289;&#19982;&#22810;&#21151;&#33021;&#21487;&#32534;&#31243;&#38376;&#38453;&#21015;&#65288;FPGA&#65289;&#30456;&#32467;&#21512;&#65292;&#35774;&#35745;&#20986;&#19968;&#31181;&#33021;&#28304;&#39640;&#25928;&#30340;&#24322;&#26500;CMOS + X&#65288;X = sMTJ&#65289;&#21407;&#22411;&#65292;&#20854;&#25104;&#21151;&#22320;&#25191;&#34892;&#20102;&#27010;&#29575;&#25512;&#29702;&#21644;&#24322;&#27493;Boltzmann&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2304.05949</link><description>&lt;p&gt;
CMOS + &#38543;&#26426;&#32435;&#31859;&#30913;&#20307;&#65306;&#27010;&#29575;&#25512;&#29702;&#19982;&#23398;&#20064;&#24322;&#26500;&#35745;&#31639;&#26426;
&lt;/p&gt;
&lt;p&gt;
CMOS + stochastic nanomagnets: heterogeneous computers for probabilistic inference and learning. (arXiv:2304.05949v1 [cond-mat.mes-hall])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#22522;&#20110;&#38543;&#26426;&#30913;&#38567;&#36947;&#32467;&#65288;sMTJ&#65289;&#30340;&#27010;&#29575;&#27604;&#29305;&#65288;p&#20301;&#65289;&#19982;&#22810;&#21151;&#33021;&#21487;&#32534;&#31243;&#38376;&#38453;&#21015;&#65288;FPGA&#65289;&#30456;&#32467;&#21512;&#65292;&#35774;&#35745;&#20986;&#19968;&#31181;&#33021;&#28304;&#39640;&#25928;&#30340;&#24322;&#26500;CMOS + X&#65288;X = sMTJ&#65289;&#21407;&#22411;&#65292;&#20854;&#25104;&#21151;&#22320;&#25191;&#34892;&#20102;&#27010;&#29575;&#25512;&#29702;&#21644;&#24322;&#27493;Boltzmann&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25705;&#23572;&#23450;&#24459;&#30340;&#25918;&#32531;&#65292;&#21033;&#29992;&#26032;&#20852;&#30340;&#32435;&#31859;&#25216;&#26415;&#65288;X&#65289;&#22686;&#24378;&#20114;&#34917;&#37329;&#23646;&#27687;&#21270;&#29289;&#21322;&#23548;&#20307;&#65288;CMOS&#65289;&#26230;&#20307;&#31649;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#22522;&#20110;&#38543;&#26426;&#30913;&#38567;&#36947;&#32467;&#65288;sMTJ&#65289;&#30340;&#27010;&#29575;&#27604;&#29305;&#65288;p&#20301;&#65289;&#19982;&#22810;&#21151;&#33021;&#21487;&#32534;&#31243;&#38376;&#38453;&#21015;&#65288;FPGA&#65289;&#30456;&#32467;&#21512;&#65292;&#35774;&#35745;&#20986;&#19968;&#31181;&#33021;&#28304;&#39640;&#25928;&#30340;&#24322;&#26500;CMOS + X&#65288;X = sMTJ&#65289;&#21407;&#22411;&#12290;&#23613;&#31649;sMTJs&#35774;&#22791;&#38388;&#23384;&#22312;&#24046;&#24322;&#65292;&#25105;&#20204;&#30340;&#24322;&#26500;&#35745;&#31639;&#26426;&#25104;&#21151;&#22320;&#25191;&#34892;&#20102;&#27010;&#29575;&#25512;&#29702;&#21644;&#24322;&#27493;Boltzmann&#23398;&#20064;&#12290;&#20351;&#29992;CMOS&#39044;&#27979;&#27969;&#31243;&#35774;&#35745;&#22871;&#20214;&#65288;PDK&#65289;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#65292;&#25968;&#23383;CMOS-based p-bits&#27169;&#25311;&#39640;&#36136;&#37327;&#38543;&#26426;&#24615;&#38656;&#35201;&#36229;&#36807;10,000&#20010;&#26230;&#20307;&#31649;&#65292;&#27599;&#29983;&#25104;&#19968;&#20010;&#38543;&#26426;&#25968;&#30340;&#33021;&#37327;&#27604;&#20351;&#29992;&#21482;&#28040;&#32791;2fJ&#30340;sMTJ-based p-bits&#39640;&#32422;&#20004;&#20010;&#25968;&#37327;&#32423;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#32553;&#25918;&#21644;&#38598;&#25104;&#29256;&#26412;&#21487;&#20197;&#26174;&#30528;&#25512;&#36827;&#27010;&#29575;&#24615;&#30340;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the slowing down of Moore's law, augmenting complementary-metal-oxide semiconductor (CMOS) transistors with emerging nanotechnologies (X) is becoming increasingly important. In this paper, we demonstrate how stochastic magnetic tunnel junction (sMTJ)-based probabilistic bits, or p-bits, can be combined with versatile Field Programmable Gate Arrays (FPGA) to design an energy-efficient, heterogeneous CMOS + X (X = sMTJ) prototype. Our heterogeneous computer successfully performs probabilistic inference and asynchronous Boltzmann learning despite device-to-device variations in sMTJs. A comprehensive comparison using a CMOS predictive process design kit (PDK) reveals that digital CMOS-based p-bits emulating high-quality randomness use over 10,000 transistors with the energy per generated random number being roughly two orders of magnitude greater than the sMTJ-based p-bits that dissipate only 2 fJ. Scaled and integrated versions of our approach can significantly advance probabilistic 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#37325;&#26500;&#39033;&#20844;&#24335;&#65292;&#23427;&#29305;&#21035;&#38024;&#23545;&#29983;&#25104;&#27169;&#31946;&#22270;&#20687;&#36827;&#34892;&#24809;&#32602;&#65292;&#21516;&#26102;&#20173;&#28982;&#26368;&#22823;&#21270;&#24314;&#27169;&#20998;&#24067;&#19979;&#30340;ELBO&#12290; &#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;VAE&#30340;&#20960;&#20010;&#26368;&#36817;&#25552;&#20986;&#30340;&#37325;&#26500;&#25439;&#22833;&#12290;</title><link>http://arxiv.org/abs/2304.05939</link><description>&lt;p&gt;
&#26174;&#24335;&#26368;&#23567;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#27169;&#31946;&#35823;&#24046;
&lt;/p&gt;
&lt;p&gt;
Explicitly Minimizing the Blur Error of Variational Autoencoders. (arXiv:2304.05939v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05939
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#37325;&#26500;&#39033;&#20844;&#24335;&#65292;&#23427;&#29305;&#21035;&#38024;&#23545;&#29983;&#25104;&#27169;&#31946;&#22270;&#20687;&#36827;&#34892;&#24809;&#32602;&#65292;&#21516;&#26102;&#20173;&#28982;&#26368;&#22823;&#21270;&#24314;&#27169;&#20998;&#24067;&#19979;&#30340;ELBO&#12290; &#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;VAE&#30340;&#20960;&#20010;&#26368;&#36817;&#25552;&#20986;&#30340;&#37325;&#26500;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#26159;&#24378;&#22823;&#30340;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#65292;&#20294;&#19982;&#23427;&#20204;&#25152;&#35757;&#32451;&#30340;&#22270;&#20687;&#30456;&#27604;&#65292;&#23427;&#20204;&#20250;&#20135;&#29983;&#27169;&#31946;&#30340;&#29983;&#25104;&#26679;&#26412;&#21644;&#37325;&#26500;&#22270;&#20687;&#12290;&#24050;&#32463;&#25237;&#20837;&#20102;&#22823;&#37327;&#30340;&#30740;&#31350;&#21162;&#21147;&#26469;&#21019;&#24314;&#26356;&#28789;&#27963;&#30340;&#27169;&#22411;&#20197;&#22686;&#21152;&#29983;&#25104;&#33021;&#21147;&#65292;&#20294;&#36890;&#24120;&#28789;&#27963;&#24615;&#30340;&#20195;&#20215;&#26159;&#26356;&#39640;&#30340;&#22797;&#26434;&#24615;&#21644;&#35745;&#31639;&#25104;&#26412;&#12290;&#20960;&#39033;&#24037;&#20316;&#38598;&#20013;&#22312;&#25913;&#21464;&#35777;&#25454;&#19979;&#30028;&#65288;ELBO&#65289;&#30340;&#37325;&#26500;&#39033;&#19978;&#65292;&#20294;&#24448;&#24448;&#26159;&#20197;&#25439;&#22833;&#23558;&#26679;&#26412;&#26368;&#22823;&#20284;&#28982;&#30340;&#25968;&#23398;&#32852;&#31995;&#20026;&#20195;&#20215;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#38024;&#23545;VAE&#30340;&#37325;&#26500;&#39033;&#30340;&#26032;&#20844;&#24335;&#65292;&#29305;&#21035;&#24809;&#32602;&#29983;&#25104;&#27169;&#31946;&#22270;&#20687;&#65292;&#21516;&#26102;&#20173;&#28982;&#22312;&#24314;&#27169;&#20998;&#24067;&#19979;&#26368;&#22823;&#21270;ELBO&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#25439;&#22833;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#28508;&#21147;&#65292;&#20854;&#20013;&#23427;&#20248;&#20110;VAE&#30340;&#20960;&#20010;&#26368;&#36817;&#25552;&#20986;&#30340;&#37325;&#26500;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational autoencoders (VAEs) are powerful generative modelling methods, however they suffer from blurry generated samples and reconstructions compared to the images they have been trained on. Significant research effort has been spent to increase the generative capabilities by creating more flexible models but often flexibility comes at the cost of higher complexity and computational cost. Several works have focused on altering the reconstruction term of the evidence lower bound (ELBO), however, often at the expense of losing the mathematical link to maximizing the likelihood of the samples under the modeled distribution. Here we propose a new formulation of the reconstruction term for the VAE that specifically penalizes the generation of blurry images while at the same time still maximizing the ELBO under the modeled distribution. We show the potential of the proposed loss on three different data sets, where it outperforms several recently proposed reconstruction losses for VAEs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21809;&#27468;&#36716;&#24405;&#20013;&#26356;&#20934;&#30830;&#22320;&#25214;&#21040;&#38899;&#31526;&#36215;&#28857;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#20102;&#26757;&#23572;&#23610;&#24230;&#35889;&#22270;&#21644;&#38899;&#32032;&#21518;&#39564;&#22270;&#20316;&#20026;&#36755;&#20837;&#65292;&#21518;&#32773;&#26159;&#30001;&#39044;&#20808;&#35757;&#32451;&#30340;&#32593;&#32476;&#29983;&#25104;&#30340;&#65292;&#24182;&#35777;&#26126;&#35821;&#35328;&#29305;&#24449;&#23545;&#36215;&#22987;&#26816;&#27979;&#26377;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.05917</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#38899;&#32032;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#26469;&#36827;&#34892;&#38899;&#31526;&#32423;&#27468;&#21809;&#36716;&#24405;
&lt;/p&gt;
&lt;p&gt;
A Phoneme-Informed Neural Network Model for Note-Level Singing Transcription. (arXiv:2304.05917v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05917
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21809;&#27468;&#36716;&#24405;&#20013;&#26356;&#20934;&#30830;&#22320;&#25214;&#21040;&#38899;&#31526;&#36215;&#28857;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#20102;&#26757;&#23572;&#23610;&#24230;&#35889;&#22270;&#21644;&#38899;&#32032;&#21518;&#39564;&#22270;&#20316;&#20026;&#36755;&#20837;&#65292;&#21518;&#32773;&#26159;&#30001;&#39044;&#20808;&#35757;&#32451;&#30340;&#32593;&#32476;&#29983;&#25104;&#30340;&#65292;&#24182;&#35777;&#26126;&#35821;&#35328;&#29305;&#24449;&#23545;&#36215;&#22987;&#26816;&#27979;&#26377;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#31526;&#32423;&#21035;&#30340;&#33258;&#21160;&#38899;&#20048;&#36716;&#24405;&#26159;&#26368;&#20855;&#20195;&#34920;&#24615;&#30340;&#38899;&#20048;&#20449;&#24687;&#26816;&#32034;&#65288;MIR&#65289;&#20219;&#21153;&#20043;&#19968;&#65292;&#24050;&#32463;&#30740;&#31350;&#20102;&#21508;&#31181;&#20048;&#22120;&#26469;&#29702;&#35299;&#38899;&#20048;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#39640;&#36136;&#37327;&#30340;&#26631;&#27880;&#25968;&#25454;&#65292;&#35768;&#22810;&#20048;&#22120;&#30340;&#36716;&#24405;&#20173;&#28982;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#29305;&#21035;&#26159;&#23545;&#20110;&#21809;&#27468;&#65292;&#30001;&#20110;&#20854;&#22312;&#38899;&#39640;&#12289;&#38899;&#33394;&#21644;&#21160;&#24577;&#26041;&#38754;&#30340;&#34920;&#29616;&#21147;&#65292;&#24456;&#38590;&#25214;&#21040;&#20934;&#30830;&#30340;&#38899;&#31526;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20165;&#22312;&#21809;&#27468;&#20013;&#21487;&#35265;&#30340;&#35821;&#35328;&#29305;&#24449;&#65292;&#26356;&#20934;&#30830;&#22320;&#25214;&#21040;&#21809;&#27468;&#22768;&#38899;&#30340;&#38899;&#31526;&#36215;&#28857;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#20351;&#29992;&#20102;&#26757;&#23572;&#23610;&#24230;&#35889;&#22270;&#21644;&#38899;&#32032;&#21518;&#39564;&#22270;&#65288;PPG&#65289;&#65292;&#21363;&#38899;&#32032;&#30340;&#24103;&#32423;&#20284;&#28982;&#65292;&#20316;&#20026;&#36215;&#22987;&#26816;&#27979;&#32593;&#32476;&#30340;&#36755;&#20837;&#65292;&#32780;PPG&#26159;&#36890;&#36807;&#20351;&#29992;&#21809;&#27468;&#21644;&#35821;&#38899;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#32593;&#32476;&#29983;&#25104;&#30340;&#12290;&#20026;&#20102;&#39564;&#35777;&#35821;&#35328;&#29305;&#24449;&#22914;&#20309;&#24433;&#21709;&#36215;&#22987;&#26816;&#27979;&#65292;&#25105;&#20204;&#36890;&#36807;&#20855;&#26377;&#19981;&#21516;&#35821;&#35328;&#30340;&#25968;&#25454;&#38598;&#27604;&#36739;&#20102;&#35780;&#20272;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Note-level automatic music transcription is one of the most representative music information retrieval (MIR) tasks and has been studied for various instruments to understand music. However, due to the lack of high-quality labeled data, transcription of many instruments is still a challenging task. In particular, in the case of singing, it is difficult to find accurate notes due to its expressiveness in pitch, timbre, and dynamics. In this paper, we propose a method of finding note onsets of singing voice more accurately by leveraging the linguistic characteristics of singing, which are not seen in other instruments. The proposed model uses mel-scaled spectrogram and phonetic posteriorgram (PPG), a frame-wise likelihood of phoneme, as an input of the onset detection network while PPG is generated by the pre-trained network with singing and speech data. To verify how linguistic features affect onset detection, we compare the evaluation results through the dataset with different languages
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#29992;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#19981;&#21516;&#22122;&#22768;&#20998;&#24067;&#65292;&#35777;&#26126;&#20102;&#39640;&#26031;&#20998;&#24067;&#22312;&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#20013;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2304.05907</link><description>&lt;p&gt;
&#24102;&#26377;&#23450;&#20301;-&#23610;&#24230;&#22122;&#22768;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Diffusion models with location-scale noise. (arXiv:2304.05907v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05907
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#29992;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#19981;&#21516;&#22122;&#22768;&#20998;&#24067;&#65292;&#35777;&#26126;&#20102;&#39640;&#26031;&#20998;&#24067;&#22312;&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#20013;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;(DMs)&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#28155;&#21152;&#39640;&#26031;&#22122;&#22768;&#21040;&#25968;&#25454;&#20013;&#65292;&#24182;&#23398;&#20250;&#21435;&#38500;&#22122;&#22768;&#12290;&#25105;&#20204;&#24819;&#30830;&#23450;&#21738;&#31181;&#22122;&#22768;&#20998;&#24067;&#65288;&#39640;&#26031;&#25110;&#38750;&#39640;&#26031;&#65289;&#22312;DMs&#20013;&#23548;&#33268;&#26356;&#22909;&#30340;&#29983;&#25104;&#25968;&#25454;&#12290;&#30001;&#20110;DMs&#30340;&#35774;&#35745;&#19981;&#36866;&#29992;&#20110;&#38750;&#39640;&#26031;&#22122;&#22768;&#65292;&#22240;&#27492;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20801;&#35768;&#20351;&#29992;&#38750;&#39640;&#26031;&#23450;&#20301;-&#23610;&#24230;&#22122;&#22768;&#26469;&#36870;&#36716;&#25193;&#25955;&#36807;&#31243;&#12290;&#25105;&#20204;&#20351;&#29992;&#35813;&#26694;&#26550;&#23637;&#31034;&#39640;&#26031;&#20998;&#24067;&#22312;&#21508;&#31181;&#20854;&#23427;&#20998;&#24067;&#65288;&#25289;&#26222;&#25289;&#26031;&#12289;&#22343;&#21248;&#12289;t&#12289;&#24191;&#20041;&#39640;&#26031;&#65289;&#20013;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models (DMs) are powerful generative models that add Gaussian noise to the data and learn to remove it. We wanted to determine which noise distribution (Gaussian or non-Gaussian) led to better generated data in DMs. Since DMs do not work by design with non-Gaussian noise, we built a framework that allows reversing a diffusion process with non-Gaussian location-scale noise. We use that framework to show that the Gaussian distribution performs the best over a wide range of other distributions (Laplace, Uniform, t, Generalized-Gaussian).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#34920;&#38754;&#32908;&#30005;&#22270;&#26696;&#35782;&#21035;&#20013;&#35780;&#20272;&#20998;&#31867;&#22120;&#20998;&#31867;&#30340;&#32622;&#20449;&#24230;&#65292;&#24182;&#21457;&#29616;&#22522;&#20110;&#27604;&#20363;&#28151;&#21512;&#27169;&#22411;&#30340;&#20998;&#31867;&#22120;&#20135;&#29983;&#20102;&#26356;&#20934;&#30830;&#30340;&#32622;&#20449;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.05898</link><description>&lt;p&gt;
&#35780;&#20272;&#34920;&#38754;&#32908;&#30005;&#22270;&#26696;&#35782;&#21035;&#20998;&#31867;&#22120;&#32622;&#20449;&#24230;
&lt;/p&gt;
&lt;p&gt;
Evaluating Classifier Confidence for Surface EMG Pattern Recognition. (arXiv:2304.05898v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05898
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#34920;&#38754;&#32908;&#30005;&#22270;&#26696;&#35782;&#21035;&#20013;&#35780;&#20272;&#20998;&#31867;&#22120;&#20998;&#31867;&#30340;&#32622;&#20449;&#24230;&#65292;&#24182;&#21457;&#29616;&#22522;&#20110;&#27604;&#20363;&#28151;&#21512;&#27169;&#22411;&#30340;&#20998;&#31867;&#22120;&#20135;&#29983;&#20102;&#26356;&#20934;&#30830;&#30340;&#32622;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#38754;&#32908;&#30005;&#22270;&#65288;EMG&#65289;&#21487;&#20197;&#36890;&#36807;&#27169;&#24335;&#35782;&#21035;&#24212;&#29992;&#20026;&#21508;&#31181;&#35774;&#22791;&#21644;&#36719;&#20214;&#30340;&#25509;&#21475;&#20449;&#21495;&#12290;&#22312;&#22522;&#20110;EMG&#30340;&#27169;&#24335;&#35782;&#21035;&#20013;&#65292;&#20998;&#31867;&#22120;&#19981;&#20165;&#24212;&#20934;&#30830;&#65292;&#36824;&#24212;&#36755;&#20986;&#36866;&#24403;&#30340;&#32622;&#20449;&#24230;&#65288;&#21363;&#27491;&#30830;&#24615;&#30340;&#27010;&#29575;&#65289;&#36827;&#34892;&#39044;&#27979;&#12290;&#22914;&#26524;&#32622;&#20449;&#24230;&#33021;&#22815;&#31934;&#30830;&#22320;&#21453;&#26144;&#30495;&#27491;&#27491;&#30830;&#24615;&#30340;&#21487;&#33021;&#24615;&#65292;&#37027;&#20040;&#23427;&#23558;&#22312;&#21508;&#31181;&#24212;&#29992;&#20219;&#21153;&#20013;&#27966;&#19978;&#29992;&#22330;&#65292;&#20363;&#22914;&#36816;&#21160;&#25298;&#32477;&#21644;&#22312;&#32447;&#36866;&#24212;&#12290;&#26412;&#25991;&#26088;&#22312;&#30830;&#23450;&#21738;&#31181;&#31867;&#22411;&#30340;&#20998;&#31867;&#22120;&#22312;EMG&#27169;&#24335;&#35782;&#21035;&#20013;&#25552;&#20379;&#26356;&#39640;&#30340;&#20934;&#30830;&#24230;&#21644;&#26356;&#22909;&#30340;&#32622;&#20449;&#24230;&#12290;&#25105;&#20204;&#23450;&#37327;&#21644;&#23450;&#24615;&#22320;&#35780;&#20272;&#20102;&#22235;&#20010;EMG&#25968;&#25454;&#38598;&#19978;&#21508;&#31181;&#21306;&#20998;&#24615;&#21644;&#29983;&#25104;&#24335;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;&#20998;&#26512;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21306;&#20998;&#24615;&#20998;&#31867;&#22120;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#20854;&#36755;&#20986;&#30340;&#32622;&#20449;&#24230;&#19982;&#30495;&#23454;&#27010;&#29575;&#19981;&#21516;&#12290;&#30456;&#21453;&#65292;&#22522;&#20110;&#27604;&#20363;&#28151;&#21512;&#27169;&#22411;&#30340;&#20998;&#31867;&#22120;&#65292;&#23427;&#26159;
&lt;/p&gt;
&lt;p&gt;
Surface electromyogram (EMG) can be employed as an interface signal for various devices and software via pattern recognition. In EMG-based pattern recognition, the classifier should not only be accurate, but also output an appropriate confidence (i.e., probability of correctness) for its prediction. If the confidence accurately reflects the likelihood of true correctness, then it will be useful in various application tasks, such as motion rejection and online adaptation. The aim of this paper is to identify the types of classifiers that provide higher accuracy and better confidence in EMG pattern recognition. We evaluate the performance of various discriminative and generative classifiers on four EMG datasets, both visually and quantitatively. The analysis results show that while a discriminative classifier based on a deep neural network exhibits high accuracy, it outputs a confidence that differs from true probabilities. By contrast, a scale mixture model-based classifier, which is a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#25968;&#25454;&#22686;&#24378;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#20013;&#30340;&#20316;&#29992;&#26426;&#29702;&#20570;&#20102;&#20840;&#38754;&#25506;&#31350;&#65292;&#32467;&#26524;&#34920;&#26126;&#22686;&#24378;&#20102;&#20998;&#31867;&#22120;&#21457;&#29616;&#21644;&#34917;&#20607;&#19981;&#24179;&#34913;&#25968;&#25454;&#20013;&#31867;&#21035;&#29305;&#23450;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.05895</link><description>&lt;p&gt;
&#25506;&#32034;&#25968;&#25454;&#22686;&#24378;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#20013;&#30340;&#20316;&#29992;&#26426;&#29702;
&lt;/p&gt;
&lt;p&gt;
Towards Understanding How Data Augmentation Works with Imbalanced Data. (arXiv:2304.05895v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05895
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#25968;&#25454;&#22686;&#24378;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#20013;&#30340;&#20316;&#29992;&#26426;&#29702;&#20570;&#20102;&#20840;&#38754;&#25506;&#31350;&#65292;&#32467;&#26524;&#34920;&#26126;&#22686;&#24378;&#20102;&#20998;&#31867;&#22120;&#21457;&#29616;&#21644;&#34917;&#20607;&#19981;&#24179;&#34913;&#25968;&#25454;&#20013;&#31867;&#21035;&#29305;&#23450;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#26159;&#35768;&#22810;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#27969;&#31243;&#30340;&#22522;&#30707;&#65292;&#28982;&#32780;&#23427;&#30340;&#20316;&#29992;&#26426;&#29702;&#23578;&#19981;&#28165;&#26970;&#12290;&#26412;&#30740;&#31350;&#23545;&#19977;&#31181;&#19981;&#21516;&#30340;&#20998;&#31867;&#22120;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12289;&#25903;&#25345;&#21521;&#37327;&#26426;&#21644;&#36923;&#36753;&#22238;&#24402;&#27169;&#22411;&#22312;&#22270;&#20687;&#21644;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#24212;&#29992;&#20110;&#19981;&#24179;&#34913;&#25968;&#25454;&#26102;&#65292;&#25968;&#25454;&#22686;&#24378;&#20250;&#20135;&#29983;&#27169;&#22411;&#26435;&#37325;&#12289;&#25903;&#25345;&#21521;&#37327;&#21644;&#29305;&#24449;&#36873;&#25321;&#31561;&#26041;&#38754;&#30340;&#26174;&#33879;&#25913;&#21464;&#12290;&#35813;&#30740;&#31350;&#35748;&#20026;&#25968;&#25454;&#22686;&#24378;&#19981;&#20165;&#20250;&#29983;&#25104;&#26032;&#26679;&#26412;&#65292;&#26356;&#37325;&#35201;&#30340;&#26159;&#22686;&#24378;&#20102;&#20998;&#31867;&#22120;&#21457;&#29616;&#21644;&#34917;&#20607;&#19981;&#24179;&#34913;&#25968;&#25454;&#20013;&#30340;&#31867;&#21035;&#29305;&#23450;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data augmentation forms the cornerstone of many modern machine learning training pipelines; yet, the mechanisms by which it works are not clearly understood. Much of the research on data augmentation (DA) has focused on improving existing techniques, examining its regularization effects in the context of neural network over-fitting, or investigating its impact on features. Here, we undertake a holistic examination of the effect of DA on three different classifiers, convolutional neural networks, support vector machines, and logistic regression models, which are commonly used in supervised classification of imbalanced data. We support our examination with testing on three image and five tabular datasets. Our research indicates that DA, when applied to imbalanced data, produces substantial changes in model weights, support vectors and feature selection; even though it may only yield relatively modest changes to global metrics, such as balanced accuracy or F1 measure. We hypothesize that 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#28151;&#21512;&#25104;&#21592;&#38543;&#26426;&#22359;&#27169;&#22411;&#26469;&#25512;&#26029;&#21160;&#24577;&#26631;&#31614;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#40065;&#26834;&#24615;&#21644;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#30456;&#23545;&#20110;&#38745;&#24577;&#26631;&#31614;&#32593;&#32476;&#65292;&#23545;&#25968;&#25454;&#30340;&#35757;&#32451;&#38656;&#27714;&#36739;&#23569;&#12290;</title><link>http://arxiv.org/abs/2304.05894</link><description>&lt;p&gt;
&#24102;&#26435;&#26631;&#31614;&#32593;&#32476;&#30340;&#21160;&#24577;&#28151;&#21512;&#25104;&#21592;&#38543;&#26426;&#22359;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Dynamic Mixed Membership Stochastic Block Model for Weighted Labeled Networks. (arXiv:2304.05894v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05894
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#28151;&#21512;&#25104;&#21592;&#38543;&#26426;&#22359;&#27169;&#22411;&#26469;&#25512;&#26029;&#21160;&#24577;&#26631;&#31614;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#40065;&#26834;&#24615;&#21644;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#30456;&#23545;&#20110;&#38745;&#24577;&#26631;&#31614;&#32593;&#32476;&#65292;&#23545;&#25968;&#25454;&#30340;&#35757;&#32451;&#38656;&#27714;&#36739;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29616;&#23454;&#20013;&#30340;&#32593;&#32476;&#37117;&#26159;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#12290;&#29616;&#26377;&#30340;&#21160;&#24577;&#32593;&#32476;&#27169;&#22411;&#35201;&#20040;&#27809;&#26377;&#26631;&#31614;&#65292;&#35201;&#20040;&#20551;&#23450;&#21482;&#26377;&#19968;&#20010;&#25104;&#21592;&#32467;&#26500;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#25104;&#21592;&#38543;&#26426;&#22359;&#27169;&#22411;&#65288;MMSBM&#65289;&#23478;&#26063;&#20801;&#35768;&#22312;&#28151;&#21512;&#25104;&#21592;&#32858;&#31867;&#30340;&#20551;&#35774;&#19979;&#27169;&#25311;&#38745;&#24577;&#26631;&#31614;&#32593;&#32476;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#36825;&#31181;&#27169;&#22411;&#25193;&#23637;&#21040;&#22312;&#28151;&#21512;&#25104;&#21592;&#20551;&#35774;&#19979;&#25512;&#26029;&#21160;&#24577;&#26631;&#31614;&#32593;&#32476;&#30340;&#27169;&#22411;&#31867;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#27169;&#22411;&#21442;&#25968;&#30340;&#26102;&#38388;&#20808;&#39564;&#24418;&#24335;&#65292;&#24182;&#20381;&#36182;&#20110;&#21160;&#21147;&#23398;&#19981;&#26159;&#31361;&#28982;&#30340;&#21333;&#19968;&#20551;&#35774;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#29616;&#26377;&#26041;&#27861;&#26174;&#33879;&#19981;&#21516;&#65292;&#24182;&#19988;&#21487;&#20197;&#27169;&#25311;&#26356;&#22797;&#26434;&#30340;&#31995;&#32479;&#8212;&#8212;&#21160;&#24577;&#26631;&#35760;&#32593;&#32476;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#29616;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20960;&#20010;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#20248;&#21183;&#26159;&#65292;&#23427;&#21482;&#38656;&#35201;&#24456;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#23601;&#33021;&#20135;&#29983;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#19982;&#38745;&#24577;&#26631;&#31614;&#32593;&#32476;&#30456;&#27604;&#65292;&#25105;&#20204;&#26041;&#27861;&#22312;&#21160;&#24577;&#26631;&#31614;&#32593;&#32476;&#19979;&#30340;&#24615;&#33021;&#25552;&#21319;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most real-world networks evolve over time. Existing literature proposes models for dynamic networks that are either unlabeled or assumed to have a single membership structure. On the other hand, a new family of Mixed Membership Stochastic Block Models (MMSBM) allows to model static labeled networks under the assumption of mixed-membership clustering. In this work, we propose to extend this later class of models to infer dynamic labeled networks under a mixed membership assumption. Our approach takes the form of a temporal prior on the model's parameters. It relies on the single assumption that dynamics are not abrupt. We show that our method significantly differs from existing approaches, and allows to model more complex systems --dynamic labeled networks. We demonstrate the robustness of our method with several experiments on both synthetic and real-world datasets. A key interest of our approach is that it needs very few training data to yield good results. The performance gain under 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MusIK&#30340;&#26032;&#31639;&#27861;&#65292;&#21033;&#29992;&#22810;&#27493;&#36870;&#36816;&#21160;&#23398;&#23454;&#29616;&#34920;&#31034;&#23398;&#20064;&#21644;&#31995;&#32479;&#25506;&#32034;&#30456;&#32467;&#21512;&#65292;&#36798;&#21040;&#35745;&#31639;&#39640;&#25928;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#26368;&#20248;&#30340;&#25928;&#26524;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#23500;&#35266;&#27979;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#12290;</title><link>http://arxiv.org/abs/2304.05889</link><description>&lt;p&gt;
&#22810;&#27493;&#36870;&#36816;&#21160;&#23398;&#34920;&#31034;&#23398;&#20064;&#65306;&#23500;&#35266;&#27979;&#24378;&#21270;&#23398;&#20064;&#30340;&#39640;&#25928;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Representation Learning with Multi-Step Inverse Kinematics: An Efficient and Optimal Approach to Rich-Observation RL. (arXiv:2304.05889v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05889
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MusIK&#30340;&#26032;&#31639;&#27861;&#65292;&#21033;&#29992;&#22810;&#27493;&#36870;&#36816;&#21160;&#23398;&#23454;&#29616;&#34920;&#31034;&#23398;&#20064;&#21644;&#31995;&#32479;&#25506;&#32034;&#30456;&#32467;&#21512;&#65292;&#36798;&#21040;&#35745;&#31639;&#39640;&#25928;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#26368;&#20248;&#30340;&#25928;&#26524;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#23500;&#35266;&#27979;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#22359;MDP&#38382;&#39064;&#19979;&#65292;&#38024;&#23545;&#23500;&#26377;&#39640;&#32500;&#24230;&#35266;&#27979;&#32780;&#35774;&#35745;&#30340;&#26679;&#26412;&#39640;&#25928;&#31639;&#27861;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#31639;&#27861;&#23384;&#22312;1&#65289;&#35745;&#31639;&#22797;&#26434;&#24230;&#36807;&#39640;&#65292;2&#65289;&#20855;&#26377;&#19981;&#24517;&#35201;&#30340;&#24378;&#32479;&#35745;&#20551;&#35774;&#65292;&#25110;3&#65289;&#20855;&#26377;&#27425;&#20248;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#31532;&#19968;&#20010;&#22312;&#35745;&#31639;&#19978;&#39640;&#25928;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#22312;&#26368;&#23567;&#32479;&#35745;&#20551;&#35774;&#30340;&#21069;&#25552;&#19979;&#23454;&#29616;&#20102;&#19982;&#25152;&#38656;&#31934;&#24230;&#27700;&#24179;&#30456;&#23545;&#24212;&#30340;&#36895;&#29575;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;MusIK&#23558;&#31995;&#32479;&#25506;&#32034;&#19982;&#22522;&#20110;&#22810;&#27493;&#36870;&#36816;&#21160;&#23398;&#30340;&#34920;&#31034;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#36825;&#26159;&#19968;&#31181;&#23398;&#20064;&#30446;&#26631;&#65292;&#21363;&#20174;&#24403;&#21069;&#35266;&#23519;&#21644;&#65288;&#21487;&#33021;&#36965;&#36828;&#30340;&#65289;&#26410;&#26469;&#35266;&#23519;&#20013;&#39044;&#27979;&#23398;&#20064;&#32773;&#33258;&#24049;&#30340;&#34892;&#21160;&#12290;MusIK&#31616;&#21333;&#32780;&#28789;&#27963;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#21033;&#29992;&#36890;&#29992;&#20989;&#25968;&#36924;&#36817;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#21033;&#29992;&#20102;&#20960;&#31181;&#26032;&#25216;&#26415;&#65292;&#21253;&#25324;&#19968;&#31181;&#26032;&#30340;&#38142;&#26465;&#35770;&#35777;&#26041;&#27861;&#65292;&#29992;&#20110;&#38480;&#21046;&#26102;&#38388;&#21644;&#34920;&#31034;&#31354;&#38388;&#20013;&#30340;&#35823;&#24046;&#20256;&#25773;&#65292;&#20197;&#21450;&#19968;&#31181;&#26032;&#30340;&#25506;&#32034;&#20805;&#20998;&#26465;&#20214;&#65292;&#29992;&#20110;&#25429;&#25417;&#20302;&#32500;&#20960;&#20309;&#32467;&#26500;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#31034;&#20102;MusIK&#30340;&#20248;&#21183;&#65292;&#26080;&#35770;&#26159;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#36824;&#26159;&#22681;&#38047;&#26102;&#38388;&#26041;&#38754;&#37117;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the design of sample-efficient algorithms for reinforcement learning in the presence of rich, high-dimensional observations, formalized via the Block MDP problem. Existing algorithms suffer from either 1) computational intractability, 2) strong statistical assumptions that are not necessarily satisfied in practice, or 3) suboptimal sample complexity. We address these issues by providing the first computationally efficient algorithm that attains rate-optimal sample complexity with respect to the desired accuracy level, with minimal statistical assumptions. Our algorithm, MusIK, combines systematic exploration with representation learning based on multi-step inverse kinematics, a learning objective in which the aim is to predict the learner's own action from the current observation and observations in the (potentially distant) future. MusIK is simple and flexible, and can efficiently take advantage of general-purpose function approximation. Our analysis leverages several new tec
&lt;/p&gt;</description></item><item><title>FetMRQC &#26159;&#19968;&#31181;&#38024;&#23545;&#32974;&#20799;&#33041; MRI &#30340;&#33258;&#21160;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#21462;&#19968;&#31995;&#21015;&#36136;&#37327;&#25351;&#26631;&#21487;&#20197;&#39044;&#27979;&#19987;&#23478;&#35780;&#20998;&#65292;&#24182;&#33021;&#22815;&#22312;&#22823;&#37096;&#20998;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20934;&#30830;&#35780;&#20272;&#65292;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2304.05879</link><description>&lt;p&gt;
FetMRQC: &#33258;&#21160;&#21270;&#32974;&#20799;&#33041; MRI &#36136;&#37327;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
FetMRQC: Automated Quality Control for fetal brain MRI. (arXiv:2304.05879v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05879
&lt;/p&gt;
&lt;p&gt;
FetMRQC &#26159;&#19968;&#31181;&#38024;&#23545;&#32974;&#20799;&#33041; MRI &#30340;&#33258;&#21160;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#21462;&#19968;&#31995;&#21015;&#36136;&#37327;&#25351;&#26631;&#21487;&#20197;&#39044;&#27979;&#19987;&#23478;&#35780;&#20998;&#65292;&#24182;&#33021;&#22815;&#22312;&#22823;&#37096;&#20998;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20934;&#30830;&#35780;&#20272;&#65292;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36136;&#37327;&#25511;&#21046;&#19968;&#30452;&#34987;&#35748;&#20026;&#26159;&#30830;&#20445;&#31070;&#32463;&#24433;&#20687;&#23398;&#30740;&#31350;&#21487;&#38752;&#24615;&#30340;&#37325;&#35201;&#22240;&#32032;&#12290;&#23545;&#20110;&#32974;&#20799;&#33041; MRI &#23588;&#20026;&#37325;&#35201;&#65292;&#22240;&#20026;&#32974;&#21160;&#39057;&#32321;&#19988;&#19981;&#21487;&#39044;&#27979;&#65292;&#20250;&#23548;&#33268;&#22270;&#20687;&#20013;&#20135;&#29983;&#20005;&#37325;&#20266;&#24433;&#12290;&#29616;&#26377;&#30340;&#32974;&#20799;&#33041;&#36136;&#37327;&#35780;&#20272;&#26041;&#27861;&#20165;&#20174; \textit{&#23618;&#38754;} &#32771;&#34385;&#65292;&#26080;&#27861;&#20840;&#38754;&#20102;&#35299;&#22270;&#20687;&#36136;&#37327;&#65292;&#32780;&#35780;&#20272;&#25972;&#20010;&#33041;&#23481;&#31215;&#25165;&#33021;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102; FetMRQC&#65292;&#19968;&#31181;&#38024;&#23545;&#32974;&#20799;&#33041; MRI &#30340;&#33258;&#21160;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#25552;&#21462;&#19968;&#31995;&#21015;&#36136;&#37327;&#25351;&#26631;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#25351;&#26631;&#39044;&#27979;&#19987;&#23478;&#35780;&#20998;&#12290;&#22522;&#20110;&#22312;&#20004;&#20010;&#19981;&#21516;&#26426;&#26500;&#25910;&#38598;&#30340;&#36229;&#36807; 1000 &#20010;&#20302;&#20998;&#36776;&#29575;&#32974;&#20799;&#33041; MRI &#26679;&#26412;&#30340;&#25163;&#21160;&#35780;&#20998;&#65292;&#25105;&#20204;&#21457;&#29616; FetMRQC &#33021;&#22815;&#19968;&#33324;&#21270;&#21040;&#20854;&#20182;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#26159;&#21487;&#35299;&#37322;&#30340;&#21644;&#39640;&#25928;&#30340;&#12290;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#39640;&#36136;&#37327;&#21644;&#20302;&#36136;&#37327;&#30340;&#32974;&#20799;&#33041; MRI &#23481;&#31215;&#21644;&#19987;&#23478;&#35780;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quality control (QC) has long been considered essential to guarantee the reliability of neuroimaging studies. It is particularly important for fetal brain MRI, where large and unpredictable fetal motion can lead to substantial artifacts in the acquired images. Existing methods for fetal brain quality assessment operate at the \textit{slice} level, and fail to get a comprehensive picture of the quality of an image, that can only be achieved by looking at the \textit{entire} brain volume. In this work, we propose FetMRQC, a machine learning framework for automated image quality assessment tailored to fetal brain MRI, which extracts an ensemble of quality metrics that are then used to predict experts' ratings. Based on the manual ratings of more than 1000 low-resolution stacks acquired across two different institutions, we show that, compared with existing quality metrics, FetMRQC is able to generalize out-of-domain, while being interpretable and data efficient. We also release a novel ma
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#38376;&#25511;&#22270;&#21367;&#31215;&#32593;&#32476;(AGGCN)&#65292;&#35813;&#32593;&#32476;&#32467;&#21512;&#21367;&#31215;&#33410;&#28857;&#29305;&#24449;&#22686;&#24378;&#21644;&#21151;&#33021;&#36830;&#25509;&#24230;&#37327;&#33258;&#36866;&#24212;&#23398;&#20064;&#22270;&#32467;&#26500;&#65292;&#23454;&#29616;&#20102;&#39640;&#31934;&#24230;&#30340;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#35786;&#26029;&#65292;&#24182;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#33041;&#21306;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2304.05874</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#38376;&#25511;&#22270;&#21367;&#31215;&#32593;&#32476;&#29992;&#20110;&#22522;&#20110;EEG&#25968;&#25454;&#30340;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#21487;&#35299;&#37322;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Adaptive Gated Graph Convolutional Network for Explainable Diagnosis of Alzheimer's Disease using EEG Data. (arXiv:2304.05874v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05874
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#38376;&#25511;&#22270;&#21367;&#31215;&#32593;&#32476;(AGGCN)&#65292;&#35813;&#32593;&#32476;&#32467;&#21512;&#21367;&#31215;&#33410;&#28857;&#29305;&#24449;&#22686;&#24378;&#21644;&#21151;&#33021;&#36830;&#25509;&#24230;&#37327;&#33258;&#36866;&#24212;&#23398;&#20064;&#22270;&#32467;&#26500;&#65292;&#23454;&#29616;&#20102;&#39640;&#31934;&#24230;&#30340;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#35786;&#26029;&#65292;&#24182;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#33041;&#21306;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26469;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#27169;&#22411;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#29992;&#20110;&#20998;&#31867;&#33041;&#30005;&#22270;(EEG)&#25968;&#25454;&#65292;&#28982;&#32780;&#65292;&#22522;&#20110;GNN&#30340;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#65292;&#22914;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;(AD)&#30340;&#35786;&#26029;&#20173;&#28982;&#26159;&#30456;&#23545;&#26410;&#24320;&#21457;&#30340;&#39046;&#22495;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#38376;&#25511;&#22270;&#21367;&#31215;&#32593;&#32476;(AGGCN)&#65292;&#35813;&#32593;&#32476;&#21487;&#20197;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;AGGCN&#36890;&#36807;&#23558;&#22522;&#20110;&#21367;&#31215;&#30340;&#33410;&#28857;&#29305;&#24449;&#22686;&#24378;&#19982;&#22522;&#20110;&#21151;&#33021;&#36830;&#25509;&#24615;&#30340;&#33879;&#21517;&#30456;&#20851;&#24230;&#37327;&#30456;&#32467;&#21512;&#26469;&#33258;&#36866;&#24212;&#23398;&#20064;&#22270;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#38376;&#25511;&#22270;&#21367;&#31215;&#21487;&#20197;&#21160;&#24577;&#22320;&#21152;&#26435;&#32771;&#34385;&#21508;&#31181;&#31354;&#38388;&#23610;&#24230;&#30340;&#36129;&#29486;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#38381;&#30524;&#21644;&#30529;&#30524;&#29366;&#24577;&#19979;&#22343;&#33021;&#21462;&#24471;&#36739;&#39640;&#30340;&#31934;&#24230;&#65292;&#34920;&#26126;&#23398;&#20064;&#21040;&#30340;&#34920;&#24449;&#32467;&#26524;&#30340;&#31283;&#23450;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;AGGCN&#27169;&#22411;&#21487;&#20197;&#25552;&#20379;&#26377;&#20851;AD&#26368;&#21463;&#24433;&#21709;&#30340;&#33041;&#21306;&#30340;&#37325;&#35201;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural network (GNN) models are increasingly being used for the classification of electroencephalography (EEG) data. However, GNN-based diagnosis of neurological disorders, such as Alzheimer's disease (AD), remains a relatively unexplored area of research. Previous studies have relied on functional connectivity methods to infer brain graph structures and used simple GNN architectures for the diagnosis of AD. In this work, we propose a novel adaptive gated graph convolutional network (AGGCN) that can provide explainable predictions. AGGCN adaptively learns graph structures by combining convolution-based node feature enhancement with a well-known correlation-based measure of functional connectivity. Furthermore, the gated graph convolution can dynamically weigh the contribution of various spatial scales. The proposed model achieves high accuracy in both eyes-closed and eyes-open conditions, indicating the stability of learned representations. Finally, we demonstrate that the propos
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#12289;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#20132;&#20114;&#24335;&#28023;&#27915;&#24223;&#24323;&#29289;&#28165;&#29702;&#30340;&#36890;&#20449;&#26426;&#21046;&#65292;&#20351;&#24471;&#19981;&#21516;&#20195;&#29702;&#20043;&#38388;&#21487;&#20197;&#21327;&#20316;&#31454;&#20105;&#24182;&#23454;&#29616;&#25910;&#38598;&#24223;&#24323;&#29289;&#30340;&#26368;&#22823;&#21270;&#12290;</title><link>http://arxiv.org/abs/2304.05872</link><description>&lt;p&gt;
&#22312;&#31454;&#20105;&#24615;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#23398;&#20064;&#27807;&#36890;&#21644;&#21327;&#20316;&#20197;&#28165;&#29702;&#28023;&#27915;&#24223;&#24323;&#22609;&#26009;
&lt;/p&gt;
&lt;p&gt;
Learning to Communicate and Collaborate in a Competitive Multi-Agent Setup to Clean the Ocean from Macroplastics. (arXiv:2304.05872v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05872
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#12289;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#20132;&#20114;&#24335;&#28023;&#27915;&#24223;&#24323;&#29289;&#28165;&#29702;&#30340;&#36890;&#20449;&#26426;&#21046;&#65292;&#20351;&#24471;&#19981;&#21516;&#20195;&#29702;&#20043;&#38388;&#21487;&#20197;&#21327;&#20316;&#31454;&#20105;&#24182;&#23454;&#29616;&#25910;&#38598;&#24223;&#24323;&#29289;&#30340;&#26368;&#22823;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#21327;&#20316;&#19982;&#31454;&#20105;&#20043;&#38388;&#30340;&#24179;&#34913;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#24314;&#31435;&#22312;&#19968;&#20010;&#39640;&#24433;&#21709;&#38382;&#39064;&#19978;&#65292;&#36890;&#36807;&#23545;&#28023;&#27915;&#24223;&#24323;&#22609;&#26009;&#30340;&#25910;&#38598;&#23454;&#29616;&#20102;&#21327;&#20316;&#19982;&#31454;&#20105;&#30340;&#24179;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#36890;&#20449;&#26426;&#21046;&#65292;&#23427;&#22686;&#21152;&#20102;&#20195;&#29702;&#30340;&#35266;&#23519;&#31354;&#38388;&#12290;&#22312;&#25105;&#20204;&#33258;&#23450;&#20041;&#30340;&#29615;&#22659;&#20013;&#65292;&#20195;&#29702;&#25511;&#21046;&#30528;&#25910;&#38598;&#22609;&#26009;&#30340;&#33337;&#21482;&#12290;&#36825;&#31181;&#36890;&#20449;&#26426;&#21046;&#20351;&#20195;&#29702;&#33021;&#22815;&#20351;&#29992;&#20108;&#36827;&#21046;&#20449;&#21495;&#26469;&#24320;&#21457;&#36890;&#20449;&#21327;&#35758;&#12290;&#34429;&#28982;&#20195;&#29702;&#30340;&#38598;&#20307;&#30446;&#26631;&#26159;&#23613;&#21487;&#33021;&#22320;&#28165;&#29702;&#28023;&#27915;&#24223;&#24323;&#22609;&#26009;&#65292;&#20294;&#20195;&#29702;&#20250;&#22240;&#20010;&#20154;&#25910;&#38598;&#21040;&#30340;&#24223;&#24323;&#22609;&#26009;&#25968;&#37327;&#32780;&#33719;&#24471;&#22870;&#21169;&#12290;&#22240;&#27492;&#65292;&#20195;&#29702;&#24517;&#39035;&#23398;&#20250;&#26377;&#25928;&#22320;&#27807;&#36890;&#24182;&#20445;&#25345;&#31454;&#20105;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finding a balance between collaboration and competition is crucial for artificial agents in many real-world applications. We investigate this using a Multi-Agent Reinforcement Learning (MARL) setup on the back of a high-impact problem. The accumulation and yearly growth of plastic in the ocean cause irreparable damage to many aspects of oceanic health and the marina system. To prevent further damage, we need to find ways to reduce macroplastics from known plastic patches in the ocean. Here we propose a Graph Neural Network (GNN) based communication mechanism that increases the agents' observation space. In our custom environment, agents control a plastic collecting vessel. The communication mechanism enables agents to develop a communication protocol using a binary signal. While the goal of the agent collective is to clean up as much as possible, agents are rewarded for the individual amount of macroplastics collected. Hence agents have to learn to communicate effectively while maintai
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36793;&#32536;&#20113;&#21327;&#20316;&#30693;&#35782;&#36716;&#31227;&#26694;&#26550;&#65288;ECCT&#65289;&#65292;&#20351;&#24471;&#20849;&#20139;&#29305;&#24449;&#23884;&#20837;&#21644;&#39044;&#27979;&#26085;&#24535;&#30340;&#21452;&#21521;&#30693;&#35782;&#20256;&#36755;&#25104;&#20026;&#21487;&#33021;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#20010;&#24615;&#21270;&#22686;&#24378;&#12289;&#27169;&#22411;&#30340;&#24322;&#26500;&#24615;&#12289;&#23481;&#24525;&#35757;&#32451;&#30340;&#24322;&#27493;&#24615;&#21644;&#32531;&#35299;&#36890;&#20449;&#36127;&#25285;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.05871</link><description>&lt;p&gt;
&#24102;&#26377;&#32852;&#37030;&#21644;&#38598;&#20013;&#29305;&#24449;&#30340;&#36793;&#32536;&#20113;&#21327;&#20316;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Edge-cloud Collaborative Learning with Federated and Centralized Features. (arXiv:2304.05871v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05871
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36793;&#32536;&#20113;&#21327;&#20316;&#30693;&#35782;&#36716;&#31227;&#26694;&#26550;&#65288;ECCT&#65289;&#65292;&#20351;&#24471;&#20849;&#20139;&#29305;&#24449;&#23884;&#20837;&#21644;&#39044;&#27979;&#26085;&#24535;&#30340;&#21452;&#21521;&#30693;&#35782;&#20256;&#36755;&#25104;&#20026;&#21487;&#33021;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#20010;&#24615;&#21270;&#22686;&#24378;&#12289;&#27169;&#22411;&#30340;&#24322;&#26500;&#24615;&#12289;&#23481;&#24525;&#35757;&#32451;&#30340;&#24322;&#27493;&#24615;&#21644;&#32531;&#35299;&#36890;&#20449;&#36127;&#25285;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#21463;&#27426;&#36814;&#30340;&#36793;&#32536;&#35745;&#31639;&#26041;&#24335;&#65292;&#19981;&#20250;&#21361;&#21450;&#29992;&#25143;&#30340;&#38544;&#31169;&#12290;&#30446;&#21069;&#30340;FL&#33539;&#20363;&#20551;&#23450;&#25968;&#25454;&#20165;&#39547;&#30041;&#22312;&#36793;&#32536;&#65292;&#32780;&#20113;&#26381;&#21153;&#22120;&#20165;&#25191;&#34892;&#27169;&#22411;&#24179;&#22343;&#12290;&#20294;&#26159;&#65292;&#22312;&#35832;&#22914;&#25512;&#33616;&#31995;&#32479;&#20043;&#31867;&#30340;&#23454;&#38469;&#24773;&#20917;&#19979;&#65292;&#20113;&#26381;&#21153;&#22120;&#20855;&#26377;&#23384;&#20648;&#21382;&#21490;&#21644;&#20132;&#20114;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#30340;Edge-Cloud Collaborative Knowledge Transfer Framework&#65288;ECCT&#65289;&#24357;&#21512;&#20102;&#36793;&#32536;&#21644;&#20113;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#20004;&#32773;&#20043;&#38388;&#36827;&#34892;&#21452;&#21521;&#30693;&#35782;&#20256;&#36755;&#65292;&#20849;&#20139;&#29305;&#24449;&#23884;&#20837;&#21644;&#39044;&#27979;&#26085;&#24535;&#12290; ECCT&#24041;&#22266;&#20102;&#21508;&#31181;&#22909;&#22788;&#65292;&#21253;&#25324;&#22686;&#24378;&#20010;&#24615;&#21270;&#65292;&#23454;&#29616;&#27169;&#22411;&#24322;&#26500;&#24615;&#65292;&#23481;&#24525;&#22521;&#35757;&#24322;&#27493;&#24615;&#21644;&#32531;&#35299;&#36890;&#20449;&#36127;&#25285;&#12290;&#23545;&#20844;&#20849;&#21644;&#24037;&#19994;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;ECCT&#30340;&#26377;&#25928;&#24615;&#21644;&#23398;&#26415;&#21644;&#24037;&#19994;&#20351;&#29992;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a popular way of edge computing that doesn't compromise users' privacy. Current FL paradigms assume that data only resides on the edge, while cloud servers only perform model averaging. However, in real-life situations such as recommender systems, the cloud server has the ability to store historical and interactive features. In this paper, our proposed Edge-Cloud Collaborative Knowledge Transfer Framework (ECCT) bridges the gap between the edge and cloud, enabling bi-directional knowledge transfer between both, sharing feature embeddings and prediction logits. ECCT consolidates various benefits, including enhancing personalization, enabling model heterogeneity, tolerating training asynchronization, and relieving communication burdens. Extensive experiments on public and industrial datasets demonstrate ECCT's effectiveness and potential for use in academia and industry.
&lt;/p&gt;</description></item><item><title>LMR&#26159;&#19968;&#31181;&#22522;&#20110;&#36710;&#36947;&#36317;&#31163;&#30340;&#24230;&#37327;&#65292;&#36866;&#29992;&#20110;&#36712;&#36857;&#39044;&#27979;&#20013;&#30340;&#32467;&#26500;&#21270;&#29615;&#22659;&#65292;&#30456;&#27604;&#20256;&#32479;&#30340;&#27431;&#27663;&#36317;&#31163;&#24230;&#37327;&#26356;&#20934;&#30830;&#12290;</title><link>http://arxiv.org/abs/2304.05869</link><description>&lt;p&gt;
LMR: &#22522;&#20110;&#36710;&#36947;&#36317;&#31163;&#30340;&#36712;&#36857;&#39044;&#27979;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
LMR: Lane Distance-Based Metric for Trajectory Prediction. (arXiv:2304.05869v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05869
&lt;/p&gt;
&lt;p&gt;
LMR&#26159;&#19968;&#31181;&#22522;&#20110;&#36710;&#36947;&#36317;&#31163;&#30340;&#24230;&#37327;&#65292;&#36866;&#29992;&#20110;&#36712;&#36857;&#39044;&#27979;&#20013;&#30340;&#32467;&#26500;&#21270;&#29615;&#22659;&#65292;&#30456;&#27604;&#20256;&#32479;&#30340;&#27431;&#27663;&#36317;&#31163;&#24230;&#37327;&#26356;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#30340;&#24320;&#21457;&#38656;&#35201;&#24230;&#37327;&#26469;&#39564;&#35777;&#21644;&#27604;&#36739;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#30446;&#21069;&#24050;&#32463;&#30830;&#23450;&#30340;&#24230;&#37327;&#22522;&#20110;&#27431;&#27663;&#36317;&#31163;&#65292;&#36825;&#24847;&#21619;&#30528;&#22312;&#25152;&#26377;&#26041;&#21521;&#19978;&#37117;&#32473;&#20986;&#20102;&#30456;&#21516;&#30340;&#35823;&#24046;&#26435;&#37325;&#12290;&#27431;&#20960;&#37324;&#24471;&#24230;&#37327;&#23545;&#20110;&#20687;&#36947;&#36335;&#36825;&#26679;&#30340;&#32467;&#26500;&#21270;&#29615;&#22659;&#26159;&#19981;&#36275;&#22815;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#27809;&#26377;&#22949;&#21892;&#25429;&#25417;&#21040;&#19982;&#24213;&#23618;&#36710;&#36947;&#30456;&#20851;&#30340;&#25805;&#20316;&#21592;&#24847;&#22270;&#12290;&#20026;&#20102;&#38024;&#23545;&#19979;&#28216;&#35268;&#21010;&#20219;&#21153;&#21512;&#29702;&#35780;&#20272;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#65292;&#21363;&#22522;&#20110;&#36710;&#36947;&#36317;&#31163;&#30340;&#36710;&#36947;&#38169;&#36807;&#29575;&#65288;LMR&#65289;&#12290;&#23545;&#20110;LMR&#30340;&#35745;&#31639;&#65292;&#23558;&#22320;&#38754;&#23454;&#27979;&#21644;&#39044;&#27979;&#31471;&#28857;&#20998;&#37197;&#32473;&#36710;&#36947;&#32447;&#27573;&#65292;&#26356;&#30830;&#20999;&#22320;&#35828;&#26159;&#23427;&#20204;&#30340;&#20013;&#24515;&#32447;&#12290;&#36890;&#36807;&#27839;&#36710;&#36947;&#32447;&#27573;&#30340;&#36317;&#31163;&#27979;&#37327;&#65292;&#39044;&#27979;&#19982;&#23454;&#27979;&#20043;&#38388;&#30340;&#36317;&#31163;&#22312;&#19968;&#23450;&#38408;&#20540;&#33539;&#22260;&#20869;&#30340;&#39044;&#27979;&#34987;&#31216;&#20026;&#21629;&#20013;&#65292;&#21542;&#21017;&#31216;&#20026;&#38169;&#36807;&#12290;LMR&#21017;&#23450;&#20041;&#20026;&#20135;&#29983;&#38169;&#36807;&#30340;&#24207;&#21015;&#30340;&#27604;&#29575;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;&#27431;&#27663;&#36317;&#31163;&#30340;&#24230;&#37327;&#65292;LMR&#26159;&#36866;&#29992;&#20110;&#31867;&#20284;&#36710;&#36947;&#36825;&#26679;&#30340;&#32467;&#26500;&#21270;&#29615;&#22659;&#30340;&#36712;&#36857;&#39044;&#27979;&#26356;&#20026;&#21512;&#36866;&#30340;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of approaches for trajectory prediction requires metrics to validate and compare their performance. Currently established metrics are based on Euclidean distance, which means that errors are weighted equally in all directions. Euclidean metrics are insufficient for structured environments like roads, since they do not properly capture the agent's intent relative to the underlying lane. In order to provide a reasonable assessment of trajectory prediction approaches with regard to the downstream planning task, we propose a new metric that is lane distance-based: Lane Miss Rate (LMR). For the calculation of LMR, the ground-truth and predicted endpoints are assigned to lane segments, more precisely their centerlines. Measured by the distance along the lane segments, predictions that are within a certain threshold distance to the ground-truth count as hits, otherwise they count as misses. LMR is then defined as the ratio of sequences that yield a miss. Our results on three s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NoisyTwins&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#22686;&#24378;&#31867;&#21035;&#23884;&#20837;&#24182;&#20351;&#29992;&#33258;&#25105;&#30417;&#30563;&#22312;W&#31354;&#38388;&#20013;&#21435;&#30456;&#20851;&#21270;&#28508;&#31354;&#38388;&#26469;&#25913;&#21892;&#22823;&#35268;&#27169;&#38271;&#23614;&#25968;&#25454;&#38598;&#19978;StyleGANs&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#20174;&#32780;&#22312;&#22270;&#20687;&#29983;&#25104;&#20013;&#20445;&#30041;&#20102;&#31867;&#20869;&#22810;&#26679;&#24615;&#21644;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.05866</link><description>&lt;p&gt;
NoisyTwins: &#36890;&#36807;StyleGANs&#23454;&#29616;&#19968;&#33268;&#31867;&#21035;&#21644;&#22810;&#26679;&#21270;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
NoisyTwins: Class-Consistent and Diverse Image Generation through StyleGANs. (arXiv:2304.05866v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05866
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NoisyTwins&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#22686;&#24378;&#31867;&#21035;&#23884;&#20837;&#24182;&#20351;&#29992;&#33258;&#25105;&#30417;&#30563;&#22312;W&#31354;&#38388;&#20013;&#21435;&#30456;&#20851;&#21270;&#28508;&#31354;&#38388;&#26469;&#25913;&#21892;&#22823;&#35268;&#27169;&#38271;&#23614;&#25968;&#25454;&#38598;&#19978;StyleGANs&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#20174;&#32780;&#22312;&#22270;&#20687;&#29983;&#25104;&#20013;&#20445;&#30041;&#20102;&#31867;&#20869;&#22810;&#26679;&#24615;&#21644;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
StyleGANs&#26159;&#21487;&#25511;&#22270;&#29255;&#29983;&#25104;&#30340;&#21069;&#27839;&#25216;&#26415;&#65292;&#20854;&#20135;&#29983;&#20102;&#19968;&#20010;&#35821;&#20041;&#35299;&#32806;&#30340;&#28508;&#31354;&#38388;&#65292;&#36866;&#29992;&#20110;&#22270;&#20687;&#32534;&#36753;&#21644;&#25805;&#20316;&#12290;&#20294;&#26159;&#65292;&#24403;&#36890;&#36807;&#31867;&#21035;&#26465;&#20214;&#22312;&#22823;&#35268;&#27169;&#38271;&#23614;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;StyleGANs&#30340;&#24615;&#33021;&#20005;&#37325;&#19979;&#38477;&#12290;&#25105;&#20204;&#21457;&#29616;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#19968;&#20010;&#21407;&#22240;&#26159;&#22312;W&#28508;&#31354;&#38388;&#20013;&#27599;&#20010;&#31867;&#21035;&#30340;&#28508;&#31354;&#38388;&#22349;&#22604;&#12290;&#20351;&#29992;NoisyTwins&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#31181;&#26377;&#25928;&#19988;&#20302;&#25104;&#26412;&#30340;&#31867;&#21035;&#23884;&#20837;&#22686;&#24378;&#31574;&#30053;&#65292;&#28982;&#21518;&#22522;&#20110;W&#31354;&#38388;&#30340;&#33258;&#25105;&#30417;&#30563;&#26469;&#21435;&#30456;&#20851;&#21270;&#28508;&#31354;&#38388;&#12290;&#36825;&#31181;&#21435;&#30456;&#20851;&#21270;&#32531;&#35299;&#20102;&#22349;&#22604;&#65292;&#30830;&#20445;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22270;&#20687;&#29983;&#25104;&#20013;&#20445;&#30041;&#20102;&#31867;&#20869;&#24046;&#24322;&#21644;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;ImageNet-LT&#21644;iNaturalist 2019&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#38271;&#23614;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;FID&#19978;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#32422;19&#65285;&#65292;&#24314;&#31435;&#20102;&#26032;&#30340;&#26368;&#39640;&#32426;&#24405;&#12290;
&lt;/p&gt;
&lt;p&gt;
StyleGANs are at the forefront of controllable image generation as they produce a latent space that is semantically disentangled, making it suitable for image editing and manipulation. However, the performance of StyleGANs severely degrades when trained via class-conditioning on large-scale long-tailed datasets. We find that one reason for degradation is the collapse of latents for each class in the $\mathcal{W}$ latent space. With NoisyTwins, we first introduce an effective and inexpensive augmentation strategy for class embeddings, which then decorrelates the latents based on self-supervision in the $\mathcal{W}$ space. This decorrelation mitigates collapse, ensuring that our method preserves intra-class diversity with class-consistency in image generation. We show the effectiveness of our approach on large-scale real-world long-tailed datasets of ImageNet-LT and iNaturalist 2019, where our method outperforms other methods by $\sim 19\%$ on FID, establishing a new state-of-the-art.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#19977;&#32500;&#25968;&#25454;&#30340;&#23610;&#24230;&#31561;&#21464;&#21367;&#31215;&#32593;&#32476;&#23618;&#65292;&#21487;&#22312;&#22788;&#29702;&#19981;&#21516;&#23610;&#24230;&#30340;&#23545;&#35937;&#21644;&#23545;&#35937;&#37096;&#20998;&#26102;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#21644;&#20135;&#29983;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.05864</link><description>&lt;p&gt;
&#19977;&#32500;&#25968;&#25454;&#30340;&#23610;&#24230;&#31561;&#21464;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Scale-Equivariant Deep Learning for 3D Data. (arXiv:2304.05864v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05864
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#19977;&#32500;&#25968;&#25454;&#30340;&#23610;&#24230;&#31561;&#21464;&#21367;&#31215;&#32593;&#32476;&#23618;&#65292;&#21487;&#22312;&#22788;&#29702;&#19981;&#21516;&#23610;&#24230;&#30340;&#23545;&#35937;&#21644;&#23545;&#35937;&#37096;&#20998;&#26102;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#21644;&#20135;&#29983;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#24179;&#31227;&#31561;&#21464;&#24615;&#26159;&#23427;&#35782;&#21035;&#22270;&#20687;&#20013;&#20301;&#32622;&#19981;&#21464;&#30340;&#29289;&#20307;&#30340;&#33021;&#21147;&#65292;&#32780;&#32676;&#31561;&#21464;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23558;&#36825;&#31181;&#31561;&#21464;&#24615;&#36716;&#25442;&#20026;&#36755;&#20837;&#30340;&#20854;&#20182;&#21464;&#25442;&#12290;&#24688;&#24403;&#22788;&#29702;&#19981;&#21516;&#23610;&#24230;&#30340;&#23545;&#35937;&#21644;&#23545;&#35937;&#37096;&#20998;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#32780;&#23610;&#24230;&#21487;&#20197;&#22240;&#22810;&#31181;&#21407;&#22240;&#32780;&#21464;&#21270;&#65292;&#20363;&#22914;&#22522;&#30784;&#23545;&#35937;&#30340;&#22823;&#23567;&#25110;&#25104;&#20687;&#27169;&#24577;&#30340;&#20998;&#36776;&#29575;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#19977;&#32500;&#25968;&#25454;&#30340;&#23610;&#24230;&#31561;&#21464;&#21367;&#31215;&#32593;&#32476;&#23618;&#65292;&#20445;&#35777;&#19977;&#32500;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#23610;&#24230;&#31561;&#21464;&#24615;&#12290;&#23610;&#24230;&#31561;&#21464;&#24615;&#20943;&#36731;&#20102;&#20998;&#21035;&#23398;&#20064;&#27599;&#31181;&#21487;&#33021;&#23610;&#24230;&#30340;&#36127;&#25285;&#65292;&#20351;&#31070;&#32463;&#32593;&#32476;&#19987;&#27880;&#20110;&#26356;&#39640;&#23618;&#27425;&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#20174;&#32780;&#20135;&#29983;&#26356;&#22909;&#30340;&#32467;&#26524;&#21644;&#26356;&#22909;&#30340;&#25968;&#25454;&#25928;&#29575;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#26377;&#20851;&#23610;&#24230;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#22312;&#20108;&#32500;&#39046;&#22495;&#30340;&#29702;&#35770;&#22522;&#30784;&#21644;&#31185;&#23398;&#24037;&#20316;&#30340;&#27010;&#36848;&#65292;&#28982;&#21518;&#23558;&#20854;&#36716;&#31227;&#21040;&#19977;&#32500;&#39046;&#22495;&#65292;&#24182;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability of convolutional neural networks (CNNs) to recognize objects regardless of their position in the image is due to the translation-equivariance of the convolutional operation. Group-equivariant CNNs transfer this equivariance to other transformations of the input. Dealing appropriately with objects and object parts of different scale is challenging, and scale can vary for multiple reasons such as the underlying object size or the resolution of the imaging modality. In this paper, we propose a scale-equivariant convolutional network layer for three-dimensional data that guarantees scale-equivariance in 3D CNNs. Scale-equivariance lifts the burden of having to learn each possible scale separately, allowing the neural network to focus on higher-level learning goals, which leads to better results and better data-efficiency. We provide an overview of the theoretical foundations and scientific work on scale-equivariant neural networks in the two-dimensional domain. We then transfer
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#26465;&#20214;&#39550;&#39542;&#34892;&#20026;&#39044;&#27979;&#30340;&#22522;&#20110;&#38598;&#21512;&#27861;&#30340;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;RESET&#65292;&#21487;&#20197;&#39044;&#27979;&#28789;&#27963;&#25968;&#37327;&#30340;&#36712;&#36857;&#32780;&#19981;&#24433;&#21709;&#36816;&#34892;&#26102;&#38388;&#25110;&#20540;&#22495;&#12290;</title><link>http://arxiv.org/abs/2304.05856</link><description>&lt;p&gt;
&#37325;&#35775;&#36712;&#36857;&#38598;&#21512;&#65292;&#29992;&#20110;&#26465;&#20214;&#39550;&#39542;&#34892;&#20026;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
RESET: Revisiting Trajectory Sets for Conditional Behavior Prediction. (arXiv:2304.05856v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05856
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#26465;&#20214;&#39550;&#39542;&#34892;&#20026;&#39044;&#27979;&#30340;&#22522;&#20110;&#38598;&#21512;&#27861;&#30340;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;RESET&#65292;&#21487;&#20197;&#39044;&#27979;&#28789;&#27963;&#25968;&#37327;&#30340;&#36712;&#36857;&#32780;&#19981;&#24433;&#21709;&#36816;&#34892;&#26102;&#38388;&#25110;&#20540;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#19981;&#21516;&#36712;&#36857;&#26465;&#20214;&#19979;&#20132;&#36890;&#21442;&#19982;&#32773;&#30340;&#34892;&#20026;&#26159;&#24517;&#35201;&#30340;&#12290;&#36825;&#21487;&#20197;&#20351;&#24471;&#19979;&#28216;&#30340;&#35268;&#21010;&#22120;&#26356;&#20934;&#30830;&#22320;&#35780;&#20272;&#20854;&#20915;&#31574;&#30340;&#24433;&#21709;&#12290;&#26368;&#36817;&#30340;&#26465;&#20214;&#34892;&#20026;&#39044;&#27979;&#26041;&#27861;&#20381;&#36182;&#20110;&#22238;&#24402;&#35299;&#30721;&#22120;&#65292;&#36825;&#24847;&#21619;&#30528;&#22352;&#26631;&#25110;&#22810;&#39033;&#24335;&#31995;&#25968;&#20250;&#34987;&#22238;&#24402;&#12290;&#26412;&#25991;&#37325;&#26032;&#32771;&#34385;&#22522;&#20110;&#38598;&#21512;&#30340;&#36712;&#36857;&#39044;&#27979;&#65292;&#20854;&#20013;&#39044;&#23450;&#20041;&#36712;&#36857;&#38598;&#21512;&#20013;&#27599;&#31181;&#36712;&#36857;&#30340;&#27010;&#29575;&#30001;&#20998;&#31867;&#27169;&#22411;&#20915;&#23450;&#65292;&#24182;&#39318;&#27425;&#23558;&#20854;&#24212;&#29992;&#20110;&#26465;&#20214;&#39550;&#39542;&#34892;&#20026;&#39044;&#27979;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;RESET&#65292;&#23427;&#32467;&#21512;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#39537;&#21160;&#30340;&#36712;&#36857;&#38598;&#21512;&#29983;&#25104;&#31639;&#27861;&#21644;&#22522;&#20110;&#22270;&#24418;&#30340;&#32534;&#30721;&#22120;&#12290;&#23545;&#20110;&#26080;&#26465;&#20214;&#39044;&#27979;&#65292;RESET&#30340;&#34920;&#29616;&#19982;&#22522;&#20110;&#22238;&#24402;&#30340;&#26041;&#27861;&#30456;&#24403;&#12290;&#30001;&#20110;&#22522;&#20110;&#38598;&#21512;&#27861;&#30340;&#29305;&#24615;&#65292;&#23427;&#20855;&#26377;&#21487;&#39044;&#27979;&#28789;&#27963;&#25968;&#37327;&#36712;&#36857;&#30340;&#20248;&#21183;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#36816;&#34892;&#26102;&#38388;&#25110;&#20540;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is desirable to predict the behavior of traffic participants conditioned on different planned trajectories of the autonomous vehicle. This allows the downstream planner to estimate the impact of its decisions. Recent approaches for conditional behavior prediction rely on a regression decoder, meaning that coordinates or polynomial coefficients are regressed. In this work we revisit set-based trajectory prediction, where the probability of each trajectory in a predefined trajectory set is determined by a classification model, and first-time employ it to the task of conditional behavior prediction. We propose RESET, which combines a new metric-driven algorithm for trajectory set generation with a graph-based encoder. For unconditional prediction, RESET achieves comparable performance to a regression-based approach. Due to the nature of set-based approaches, it has the advantageous property of being able to predict a flexible number of trajectories without influencing runtime or comple
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25506;&#32034;&#20915;&#31574;&#26641;&#31354;&#38388;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#26469;&#23398;&#20064;&#32039;&#20945;&#30340;&#20915;&#31574;&#26641;&#24182;&#26368;&#20339;&#21270;&#21487;&#35299;&#37322;&#24615;&#19982;&#24615;&#33021;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2304.05839</link><description>&lt;p&gt;
&#20351;&#29992;&#40657;&#30418;&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#31867;&#26641;&#30340;&#26368;&#20339;&#21487;&#35299;&#37322;&#24615; - &#24615;&#33021;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Optimal Interpretability-Performance Trade-off of Classification Trees with Black-Box Reinforcement Learning. (arXiv:2304.05839v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05839
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25506;&#32034;&#20915;&#31574;&#26641;&#31354;&#38388;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#26469;&#23398;&#20064;&#32039;&#20945;&#30340;&#20915;&#31574;&#26641;&#24182;&#26368;&#20339;&#21270;&#21487;&#35299;&#37322;&#24615;&#19982;&#24615;&#33021;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21487;&#20197;&#24314;&#31435;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#20449;&#20219;&#65292;&#20174;&#32780;&#20801;&#35768;&#36827;&#34892;&#29992;&#25143;&#23433;&#20840;&#26816;&#26597;&#12290;&#20915;&#31574;&#26641;&#65288;DT&#65289;&#29305;&#21035;&#25552;&#20379;&#20102;&#20851;&#20110;&#23398;&#20064;&#27169;&#22411;&#30340;&#20840;&#23616;&#35270;&#22270;&#65292;&#24182;&#28165;&#26224;&#22320;&#27010;&#36848;&#20102;&#23545;&#20110;&#20998;&#31867;&#32473;&#23450;&#25968;&#25454;&#33267;&#20851;&#37325;&#35201;&#30340;&#29305;&#24449;&#30340;&#35282;&#33394;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;DT&#22826;&#22823;&#65292;&#21017;&#36825;&#31181;&#21487;&#35299;&#37322;&#24615;&#20250;&#21463;&#21040;&#38459;&#30861;&#12290;&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26694;&#26550;&#26469;&#25506;&#32034;DT&#31354;&#38388;&#20197;&#23398;&#20064;&#32039;&#20945;&#30340;&#26641;&#12290;&#19968;&#20010;&#32473;&#23450;&#30340;&#30417;&#30563;&#20998;&#31867;&#20219;&#21153;&#34987;&#24314;&#27169;&#20026;&#19968;&#20010;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#38382;&#39064;&#65288;MDP&#65289;&#65292;&#28982;&#21518;&#28155;&#21152;&#20102;&#25910;&#38598;&#20851;&#20110;&#29305;&#24449;&#20449;&#24687;&#30340;&#39069;&#22806;&#21160;&#20316;&#65292;&#30456;&#24403;&#20110;&#26500;&#24314;DT&#12290;&#36890;&#36807;&#36866;&#24403;&#22320;&#24809;&#32602;&#36825;&#20123;&#25805;&#20316;&#65292;RL&#20195;&#29702;&#23398;&#20064;&#26368;&#20339;&#26435;&#34913;DT&#30340;&#22823;&#23567;&#21644;&#24615;&#33021;&#12290;&#20294;&#26159;&#65292;&#35201;&#20570;&#21040;&#36825;&#19968;&#28857;&#65292;&#36825;&#20010;RL&#20195;&#29702;&#38656;&#35201;&#35299;&#20915;&#19968;&#20010;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;MDP&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#35777;&#26126;&#65292;&#35299;&#20915;&#19968;&#20010;&#23436;&#20840;&#21487;&#35266;&#23519;&#30340;&#38382;&#39064;&#23601;&#36275;&#20197;&#23398;&#20064;&#19968;&#20010;&#20248;&#21270;&#21487;&#35299;&#37322;&#24615; - &#24615;&#33021;&#26435;&#34913;&#30340;DT&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretability of AI models allows for user safety checks to build trust in these models. In particular, decision trees (DTs) provide a global view on the learned model and clearly outlines the role of the features that are critical to classify a given data. However, interpretability is hindered if the DT is too large. To learn compact trees, a Reinforcement Learning (RL) framework has been recently proposed to explore the space of DTs. A given supervised classification task is modeled as a Markov decision problem (MDP) and then augmented with additional actions that gather information about the features, equivalent to building a DT. By appropriately penalizing these actions, the RL agent learns to optimally trade-off size and performance of a DT. However, to do so, this RL agent has to solve a partially observable MDP. The main contribution of this paper is to prove that it is sufficient to solve a fully observable problem to learn a DT optimizing the interpretability-performance tr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992; DARTS &#26041;&#27861;&#23545;&#26631;&#20934; RNN &#21333;&#20803;&#36827;&#34892;&#25913;&#36827;&#65292;&#25552;&#20986;&#20102;&#29992;&#20110; ReNet &#26550;&#26500;&#30340;&#26032;&#30340; RNN &#21333;&#20803;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#22312; CIFAR-10 &#21644; SVHN &#25968;&#25454;&#38598;&#19978;&#30340;&#20998;&#31867;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.05838</link><description>&lt;p&gt;
DartsReNet&#65306;&#22312;ReNet&#26550;&#26500;&#20013;&#25506;&#32034;&#26032;&#30340;RNN&#21333;&#20803;
&lt;/p&gt;
&lt;p&gt;
DartsReNet: Exploring new RNN cells in ReNet architectures. (arXiv:2304.05838v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992; DARTS &#26041;&#27861;&#23545;&#26631;&#20934; RNN &#21333;&#20803;&#36827;&#34892;&#25913;&#36827;&#65292;&#25552;&#20986;&#20102;&#29992;&#20110; ReNet &#26550;&#26500;&#30340;&#26032;&#30340; RNN &#21333;&#20803;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#22312; CIFAR-10 &#21644; SVHN &#25968;&#25454;&#38598;&#19978;&#30340;&#20998;&#31867;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#26041;&#27861; DARTS&#65292;&#20026;&#22270;&#20687;&#20998;&#31867;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#21333;&#20803;&#65292;&#35813;&#21333;&#20803;&#29992;&#20110; ReNet &#26550;&#26500;&#12290;&#25105;&#20204;&#23545; ReNet &#26550;&#26500;&#24863;&#20852;&#36259;&#65292;&#23427;&#26159;&#19968;&#31181;&#22522;&#20110; RNN &#30340;&#26041;&#27861;&#65292;&#20316;&#20026;&#21367;&#31215;&#21644;&#27744;&#21270;&#27493;&#39588;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#25105;&#20204;&#20351;&#29992; DARTS &#26469;&#21457;&#29616;&#26032;&#30340;&#21333;&#20803;&#35774;&#35745;&#20197;&#20811;&#26381;&#26631;&#20934; RNN &#21333;&#20803;&#38024;&#23545;&#19968;&#32500;&#24207;&#21015;&#25968;&#25454;&#32780;&#38750;&#22270;&#20687;&#20998;&#31867;&#36825;&#31181;&#20108;&#32500;&#25968;&#25454;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#23558;&#32467;&#26524;&#19982;&#20351;&#29992; GRU &#21644; LSTM &#21333;&#20803;&#30340; ReNet &#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#21457;&#29616;&#30340;&#26032;&#21333;&#20803;&#22312; CIFAR-10 &#21644; SVHN &#19978;&#20248;&#20110;&#26631;&#20934; RNN &#21333;&#20803;&#65292;&#32780;&#23545; SVHN &#32467;&#26524;&#30340;&#25913;&#36827;&#34920;&#26126;&#20854;&#20855;&#26377;&#25512;&#24191;&#24615;&#65292;&#22240;&#20026;&#25105;&#20204;&#26159;&#20174; CIFAR-10 &#25512;&#23548;&#20986; RNN &#21333;&#20803;&#35774;&#35745;&#30340;&#65292;&#32780;&#27809;&#26377;&#38024;&#23545; SVHN &#36827;&#34892;&#26032;&#30340;&#21333;&#20803;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present new Recurrent Neural Network (RNN) cells for image classification using a Neural Architecture Search (NAS) approach called DARTS. We are interested in the ReNet architecture, which is a RNN based approach presented as an alternative for convolutional and pooling steps. ReNet can be defined using any standard RNN cells, such as LSTM and GRU. One limitation is that standard RNN cells were designed for one dimensional sequential data and not for two dimensions like it is the case for image classification. We overcome this limitation by using DARTS to find new cell designs. We compare our results with ReNet that uses GRU and LSTM cells. Our found cells outperform the standard RNN cells on CIFAR-10 and SVHN. The improvements on SVHN indicate generalizability, as we derived the RNN cell designs from CIFAR-10 without performing a new cell search for SVHN.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#32852;&#37030;&#23398;&#20064;&#23433;&#20840;&#21338;&#24328;&#65288;FLSG&#65289;&#30340;&#21338;&#24328;&#35770;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21516;&#26102;&#32771;&#34385;&#21040;&#32852;&#37030;&#23398;&#20064;&#30340;&#20445;&#25252;&#32773;&#21644;&#25915;&#20987;&#32773;&#30340;&#25910;&#30410;&#65292;&#21253;&#25324;&#35745;&#31639;&#25104;&#26412;&#12289;FL&#27169;&#22411;&#25928;&#29992;&#21644;&#38544;&#31169;&#27844;&#28431;&#39118;&#38505;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#29992;&#31639;&#27861;&#26469;&#36817;&#20284;oracle&#24182;&#20445;&#25345;&#38544;&#31169;&#12290;&#30740;&#31350;&#34920;&#26126;&#35813;&#31639;&#27861;&#23545;&#20110;&#39044;&#38450;&#21644;&#26816;&#27979;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#32852;&#37030;&#23398;&#20064;&#25915;&#20987;&#20855;&#26377;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.05836</link><description>&lt;p&gt;
&#19968;&#31181;&#32852;&#37030;&#23398;&#20064;&#30340;&#21338;&#24328;&#35770;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Game-theoretic Framework for Federated Learning. (arXiv:2304.05836v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#32852;&#37030;&#23398;&#20064;&#23433;&#20840;&#21338;&#24328;&#65288;FLSG&#65289;&#30340;&#21338;&#24328;&#35770;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21516;&#26102;&#32771;&#34385;&#21040;&#32852;&#37030;&#23398;&#20064;&#30340;&#20445;&#25252;&#32773;&#21644;&#25915;&#20987;&#32773;&#30340;&#25910;&#30410;&#65292;&#21253;&#25324;&#35745;&#31639;&#25104;&#26412;&#12289;FL&#27169;&#22411;&#25928;&#29992;&#21644;&#38544;&#31169;&#27844;&#28431;&#39118;&#38505;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#29992;&#31639;&#27861;&#26469;&#36817;&#20284;oracle&#24182;&#20445;&#25345;&#38544;&#31169;&#12290;&#30740;&#31350;&#34920;&#26126;&#35813;&#31639;&#27861;&#23545;&#20110;&#39044;&#38450;&#21644;&#26816;&#27979;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#32852;&#37030;&#23398;&#20064;&#25915;&#20987;&#20855;&#26377;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#33391;&#24615;&#21442;&#19982;&#32773;&#26088;&#22312;&#21327;&#21516;&#20248;&#21270;&#20840;&#23616;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#23384;&#22312;&#21322;&#35802;&#23454;&#30340;&#23545;&#25163;&#26102;&#65292;\textit{&#38544;&#31169;&#27844;&#28431;}&#30340;&#39118;&#38505;&#26159;&#19981;&#21487;&#24573;&#35270;&#30340;&#12290;&#29616;&#26377;&#30740;&#31350;&#35201;&#20040;&#19987;&#27880;&#20110;&#35774;&#35745;&#20445;&#25252;&#26426;&#21046;&#65292;&#35201;&#20040;&#19987;&#27880;&#20110;&#21457;&#26126;&#25915;&#20987;&#26426;&#21046;&#12290;&#34429;&#28982;&#20445;&#25252;&#32773;&#19982;&#25915;&#20987;&#32773;&#20043;&#38388;&#30340;&#26007;&#20105;&#20284;&#20046;&#27704;&#26080;&#27490;&#22659;&#65292;&#20294;&#25105;&#20204;&#20851;&#24515;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#26159;&#21542;&#21487;&#33021;&#20107;&#20808;&#39044;&#38450;&#28508;&#22312;&#30340;&#25915;&#20987;&#65311;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21338;&#24328;&#35770;&#26694;&#26550;&#65292;&#21516;&#26102;&#32771;&#34385;FL&#20445;&#25252;&#32773;&#21644;&#25915;&#20987;&#32773;&#30340;&#30456;&#24212;&#25910;&#30410;&#65292;&#20854;&#20013;&#21253;&#25324;&#35745;&#31639;&#25104;&#26412;&#12289;FL&#27169;&#22411;&#25928;&#29992;&#21644;&#38544;&#31169;&#27844;&#28431;&#39118;&#38505;&#12290;&#25105;&#20204;&#23558;&#27492;&#28216;&#25103;&#31216;&#20026;&#32852;&#37030;&#23398;&#20064;&#23433;&#20840;&#21338;&#24328;&#65288;FLSG&#65289;&#65292;&#22312;&#20854;&#20013;&#20445;&#25252;&#32773;&#21644;&#25915;&#20987;&#32773;&#37117;&#19981;&#30693;&#36947;&#25152;&#26377;&#21442;&#19982;&#32773;&#30340;&#25910;&#30410;&#12290;&#20026;&#20102;&#22788;&#29702;&#36825;&#31181;&#24773;&#20917;&#22266;&#26377;&#30340;\textit{&#19981;&#23436;&#20840;&#20449;&#24687;}&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;FLSG&#19982;&#19968;&#20010;\textit{oracle}&#30456;&#20851;&#32852;&#65292;&#35813;oracle&#20855;&#26377;&#25152;&#26377;&#21442;&#19982;&#32773;&#30340;&#25910;&#30410;&#30693;&#35782;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#22312;&#21508;&#31181;&#25928;&#29992;&#20989;&#25968;&#21644;&#25915;&#20987;&#27169;&#22411;&#32452;&#21512;&#19979;FLSG&#30340;&#32435;&#20160;&#22343;&#34913;&#23384;&#22312;&#24615;&#21644;&#21807;&#19968;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#29992;&#31639;&#27861;&#26469;&#36817;&#20284;oracle&#24182;&#20445;&#25345;&#38544;&#31169;&#12290;&#23454;&#39564;&#32467;&#26524;&#35828;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#39044;&#38450;&#21644;&#26816;&#27979;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;FL&#22330;&#26223;&#20013;&#30340;&#25915;&#20987;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In federated learning, benign participants aim to optimize a global model collaboratively. However, the risk of \textit{privacy leakage} cannot be ignored in the presence of \textit{semi-honest} adversaries. Existing research has focused either on designing protection mechanisms or on inventing attacking mechanisms. While the battle between defenders and attackers seems never-ending, we are concerned with one critical question: is it possible to prevent potential attacks in advance? To address this, we propose the first game-theoretic framework that considers both FL defenders and attackers in terms of their respective payoffs, which include computational costs, FL model utilities, and privacy leakage risks. We name this game the Federated Learning Security Game (FLSG), in which neither defenders nor attackers are aware of all participants' payoffs.  To handle the \textit{incomplete information} inherent in this situation, we propose associating the FLSG with an \textit{oracle} that ha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20013;&#20171;&#32461;&#20102;DiscoGen&#65292;&#23427;&#26159;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;-based GRN&#21457;&#29616;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#22024;&#26434;&#30340;&#22522;&#22240;&#34920;&#36798;&#27979;&#37327;&#25968;&#25454;&#24182;&#22788;&#29702;&#24178;&#39044;&#25968;&#25454;&#65292;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#35782;&#21035;GRN&#12290;</title><link>http://arxiv.org/abs/2304.05823</link><description>&lt;p&gt;
DiscoGen: &#23398;&#20064;&#21457;&#29616;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
DiscoGen: Learning to Discover Gene Regulatory Networks. (arXiv:2304.05823v1 [q-bio.MN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#20171;&#32461;&#20102;DiscoGen&#65292;&#23427;&#26159;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;-based GRN&#21457;&#29616;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#22024;&#26434;&#30340;&#22522;&#22240;&#34920;&#36798;&#27979;&#37327;&#25968;&#25454;&#24182;&#22788;&#29702;&#24178;&#39044;&#25968;&#25454;&#65292;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#35782;&#21035;GRN&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#22320;&#25512;&#26029;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#65288;GRN&#65289;&#26159;&#29983;&#29289;&#23398;&#20013;&#20851;&#38190;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;GRN&#27169;&#22411;&#25551;&#36848;&#20102;&#22522;&#22240;&#20043;&#38388;&#30340;&#20419;&#36827;&#21644;&#25233;&#21046;&#20316;&#29992;&#65292;&#24182;&#22825;&#28982;&#22320;&#20855;&#26377;&#22240;&#26524;&#20851;&#31995;&#12290;&#20026;&#20102;&#20934;&#30830;&#22320;&#35782;&#21035;GRN&#65292;&#38656;&#35201;&#36827;&#34892;&#24178;&#39044;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;GRN&#21457;&#29616;&#26041;&#27861;&#21482;&#26159;&#22522;&#20110;&#35266;&#23519;&#25968;&#25454;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#30340;&#36827;&#27493;&#26174;&#33879;&#25913;&#21892;&#20102;&#22240;&#26524;&#21457;&#29616;&#30340;&#20934;&#30830;&#24615;&#65292;&#21253;&#25324;&#22788;&#29702;&#24178;&#39044;&#25968;&#25454;&#12289;&#25913;&#21892;&#24615;&#33021;&#21644;&#21487;&#20280;&#32553;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#29983;&#29289;&#23398;&#20013;&#24212;&#29992;&#26368;&#20808;&#36827;&#65288;SOTA&#65289;&#30340;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#23384;&#22312;&#30528;&#25361;&#25112;&#65292;&#20363;&#22914;&#22024;&#26434;&#30340;&#25968;&#25454;&#21644;&#22823;&#37327;&#30340;&#26679;&#26412;&#12290;&#22240;&#27492;&#65292;&#24517;&#39035;&#25913;&#36827;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#26469;&#22788;&#29702;&#36825;&#20123;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DiscoGen&#65292;&#23427;&#26159;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;GRN&#21457;&#29616;&#26041;&#27861;&#65292;&#21487;&#20197;&#21435;&#22122;&#22522;&#22240;&#34920;&#36798;&#27979;&#37327;&#25968;&#25454;&#24182;&#22788;&#29702;&#24178;&#39044;&#25968;&#25454;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurately inferring Gene Regulatory Networks (GRNs) is a critical and challenging task in biology. GRNs model the activatory and inhibitory interactions between genes and are inherently causal in nature. To accurately identify GRNs, perturbational data is required. However, most GRN discovery methods only operate on observational data. Recent advances in neural network-based causal discovery methods have significantly improved causal discovery, including handling interventional data, improvements in performance and scalability. However, applying state-of-the-art (SOTA) causal discovery methods in biology poses challenges, such as noisy data and a large number of samples. Thus, adapting the causal discovery methods is necessary to handle these challenges. In this paper, we introduce DiscoGen, a neural network-based GRN discovery method that can denoise gene expression measurements and handle interventional data. We demonstrate that our model outperforms SOTA neural network-based causal
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27604;&#36739;&#22235;&#31181;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21644;&#21160;&#24577;&#22240;&#23376;&#27169;&#22411;&#23545;&#32654;&#22269;GDP&#23395;&#24230;&#22686;&#38271;&#30340;&#39044;&#27979;&#34920;&#29616;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#24179;&#34913;&#32463;&#27982;&#22686;&#38271;&#26399;&#38388;&#65292;&#26356;&#38271;&#30340;&#36755;&#20837;&#24207;&#21015;&#33021;&#22815;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#20294;&#26159;&#36825;&#31181;&#25928;&#26524;&#20250;&#22312;&#19981;&#21040;&#20004;&#24180;&#30340;&#26102;&#38388;&#20869;&#28040;&#22833;&#12290;&#22312;&#32463;&#27982;&#21160;&#33633;&#26102;&#26399;&#65292;&#38271;&#26399;&#35760;&#24518;&#30340;&#25928;&#26524;&#21464;&#24471;&#26126;&#26174;&#12290;</title><link>http://arxiv.org/abs/2304.05805</link><description>&lt;p&gt;
&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#22269;&#20869;&#29983;&#20135;&#24635;&#20540;&#65306;&#38271;&#26399;&#35760;&#24518;&#26377;&#22810;&#22823;&#30340;&#20316;&#29992;&#65311;
&lt;/p&gt;
&lt;p&gt;
GDP nowcasting with artificial neural networks: How much does long-term memory matter?. (arXiv:2304.05805v1 [econ.EM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05805
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27604;&#36739;&#22235;&#31181;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21644;&#21160;&#24577;&#22240;&#23376;&#27169;&#22411;&#23545;&#32654;&#22269;GDP&#23395;&#24230;&#22686;&#38271;&#30340;&#39044;&#27979;&#34920;&#29616;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#24179;&#34913;&#32463;&#27982;&#22686;&#38271;&#26399;&#38388;&#65292;&#26356;&#38271;&#30340;&#36755;&#20837;&#24207;&#21015;&#33021;&#22815;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#20294;&#26159;&#36825;&#31181;&#25928;&#26524;&#20250;&#22312;&#19981;&#21040;&#20004;&#24180;&#30340;&#26102;&#38388;&#20869;&#28040;&#22833;&#12290;&#22312;&#32463;&#27982;&#21160;&#33633;&#26102;&#26399;&#65292;&#38271;&#26399;&#35760;&#24518;&#30340;&#25928;&#26524;&#21464;&#24471;&#26126;&#26174;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;&#19981;&#21516;&#30340;&#32479;&#35745;&#27169;&#22411;&#24212;&#29992;&#20110;&#32654;&#22269;&#32463;&#27982;&#23395;&#24230;&#22269;&#20869;&#29983;&#20135;&#24635;&#20540;&#65288;GDP&#65289;&#22686;&#38271;&#39044;&#27979;&#12290;&#20351;&#29992;&#27599;&#26376;&#30340;FRED-MD&#25968;&#25454;&#24211;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#21160;&#24577;&#22240;&#23376;&#27169;&#22411;&#65288;DFM&#65289;&#21644;&#22235;&#20010;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#30340;&#39044;&#27979;&#34920;&#29616;&#65306;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#12289;&#19968;&#32500;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;1D CNN&#65289;&#12289;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65288;LSTM&#65289;&#21644;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#65288;GRU&#65289;&#12290;&#23454;&#35777;&#20998;&#26512;&#21576;&#29616;&#20102;&#20004;&#20010;&#19981;&#21516;&#35780;&#20272;&#21608;&#26399;&#30340;&#32467;&#26524;&#12290;&#31532;&#19968;&#20010;&#21608;&#26399;&#65288;2010&#24180;&#31532;1&#23395;&#24230;&#33267;2019&#24180;&#31532;4&#23395;&#24230;&#65289;&#20855;&#26377;&#24179;&#34913;&#30340;&#32463;&#27982;&#22686;&#38271;&#65292;&#32780;&#31532;&#20108;&#20010;&#21608;&#26399;&#65288;2010&#24180;&#31532;1&#23395;&#24230;&#33267;2022&#24180;&#31532;3&#23395;&#24230;&#65289;&#36824;&#21253;&#25324;COVID-19&#34928;&#36864;&#26399;&#38388;&#30340;&#26102;&#38388;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#32467;&#26524;&#65292;&#26356;&#38271;&#30340;&#36755;&#20837;&#24207;&#21015;&#22312;&#24179;&#34913;&#32463;&#27982;&#22686;&#38271;&#26399;&#38388;&#33021;&#22815;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#22312;&#19968;&#20010;&#30456;&#23545;&#36739;&#20302;&#30340;&#38408;&#20540;&#20540;&#65288;&#32422;&#20845;&#20010;&#23395;&#24230;&#25110;&#21313;&#20843;&#20010;&#26376;&#65289;&#20197;&#21518;&#65292;&#36825;&#31181;&#25928;&#24212;&#20250;&#28040;&#22833;&#12290;&#22312;&#32463;&#27982;&#21160;&#33633;&#26399;&#65288;&#22914;COVID-19&#34928;&#36864;&#26399;&#38388;&#65289;&#65292;&#38271;&#26399;&#35760;&#24518;&#30340;&#25928;&#26524;&#20250;&#21464;&#24471;&#36739;&#20026;&#26126;&#26174;&#12290;
&lt;/p&gt;
&lt;p&gt;
In our study, we apply different statistical models to nowcast quarterly GDP growth for the US economy. Using the monthly FRED-MD database, we compare the nowcasting performance of the dynamic factor model (DFM) and four artificial neural networks (ANNs): the multilayer perceptron (MLP), the one-dimensional convolutional neural network (1D CNN), the long short-term memory network (LSTM), and the gated recurrent unit (GRU). The empirical analysis presents the results from two distinctively different evaluation periods. The first (2010:Q1 -- 2019:Q4) is characterized by balanced economic growth, while the second (2010:Q1 -- 2022:Q3) also includes periods of the COVID-19 recession. According to our results, longer input sequences result in more accurate nowcasts in periods of balanced economic growth. However, this effect ceases above a relatively low threshold value of around six quarters (eighteen months). During periods of economic turbulence (e.g., during the COVID-19 recession), long
&lt;/p&gt;</description></item><item><title>Proximity Forest 2.0&#26159;&#19968;&#31181;&#26032;&#30340;&#26377;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#22120;&#65292;&#20248;&#20110;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#20998;&#31867;&#22120;&#20197;&#21450;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#20869;&#26680;&#12289;&#31070;&#32463;&#32593;&#32476;&#21644;&#28151;&#21512;&#26041;&#27861;&#22312;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.05800</link><description>&lt;p&gt;
Proximity Forest 2.0&#65306;&#19968;&#31181;&#26032;&#30340;&#26377;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Proximity Forest 2.0: A new effective and scalable similarity-based classifier for time series. (arXiv:2304.05800v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05800
&lt;/p&gt;
&lt;p&gt;
Proximity Forest 2.0&#26159;&#19968;&#31181;&#26032;&#30340;&#26377;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#22120;&#65292;&#20248;&#20110;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#20998;&#31867;&#22120;&#20197;&#21450;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#20869;&#26680;&#12289;&#31070;&#32463;&#32593;&#32476;&#21644;&#28151;&#21512;&#26041;&#27861;&#22312;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#65288;TSC&#65289;&#30001;&#20110;&#21487;&#33021;&#19982;&#19981;&#21516;&#20998;&#31867;&#20219;&#21153;&#30456;&#20851;&#30340;&#29305;&#24449;&#31867;&#22411;&#30340;&#22810;&#26679;&#24615;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#21253;&#25324;&#36235;&#21183;&#12289;&#26041;&#24046;&#12289;&#39057;&#29575;&#12289;&#24133;&#24230;&#21644;&#21508;&#31181;&#27169;&#24335;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#20960;&#31181;&#26367;&#20195;&#26041;&#27861;&#31867;&#21035;&#65292;&#21253;&#25324;&#22522;&#20110;&#30456;&#20284;&#24615;&#12289;&#29305;&#24449;&#21644;&#38388;&#38548;&#12289;&#24418;&#29366;&#12289;&#23383;&#20856;&#12289;&#20869;&#26680;&#12289;&#31070;&#32463;&#32593;&#32476;&#21644;&#28151;&#21512;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#20998;&#31867;&#22120;Proximity Forest&#29256;&#26412;2.0&#65288;PF 2.0&#65289;&#65292;&#23427;&#22312;UCR&#22522;&#20934;&#27979;&#35797;&#20013;&#20248;&#20110;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#20998;&#31867;&#22120;&#65292;&#24182;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#20869;&#26680;&#12289;&#31070;&#32463;&#32593;&#32476;&#21644;&#28151;&#21512;&#26041;&#27861;&#30340;&#29305;&#23450;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#26368;&#36866;&#21512;&#20351;&#29992;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#26041;&#27861;&#12290;PF 2.0 &#21512;&#24182;&#20102;&#26102;&#38388;&#24207;&#21015;&#30456;&#20284;&#24615;&#26368;&#36817;&#30340;&#19977;&#20010;&#36827;&#23637;&#8230;&#8230;
&lt;/p&gt;
&lt;p&gt;
Time series classification (TSC) is a challenging task due to the diversity of types of feature that may be relevant for different classification tasks, including trends, variance, frequency, magnitude, and various patterns. To address this challenge, several alternative classes of approach have been developed, including similarity-based, features and intervals, shapelets, dictionary, kernel, neural network, and hybrid approaches. While kernel, neural network, and hybrid approaches perform well overall, some specialized approaches are better suited for specific tasks. In this paper, we propose a new similarity-based classifier, Proximity Forest version 2.0 (PF 2.0), which outperforms previous state-of-the-art similarity-based classifiers across the UCR benchmark and outperforms state-of-the-art kernel, neural network, and hybrid methods on specific datasets in the benchmark that are best addressed by similarity-base methods. PF 2.0 incorporates three recent advances in time series simi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#29616;&#20102;&#19968;&#31181;&#39640;&#32500;&#36830;&#32493;&#20989;&#25968;&#65292;&#21487;&#20197;&#29992;DNN&#36924;&#36817;&#32780;&#26080;&#38656;&#25285;&#24515;&#8220;&#32500;&#24230;&#35781;&#21650;&#8221;&#65292;&#26041;&#27861;&#26159;&#23558;&#35768;&#22810;&#29305;&#27530;&#20989;&#25968;&#32452;&#21512;&#22312;&#19968;&#36215;&#12290;</title><link>http://arxiv.org/abs/2304.05790</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#26080;&#8220;&#32500;&#24230;&#35781;&#21650;&#8221;&#24773;&#20917;&#19979;&#36924;&#36817;&#22797;&#21512;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Deep neural network approximation of composite functions without the curse of dimensionality. (arXiv:2304.05790v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05790
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#29616;&#20102;&#19968;&#31181;&#39640;&#32500;&#36830;&#32493;&#20989;&#25968;&#65292;&#21487;&#20197;&#29992;DNN&#36924;&#36817;&#32780;&#26080;&#38656;&#25285;&#24515;&#8220;&#32500;&#24230;&#35781;&#21650;&#8221;&#65292;&#26041;&#27861;&#26159;&#23558;&#35768;&#22810;&#29305;&#27530;&#20989;&#25968;&#32452;&#21512;&#22312;&#19968;&#36215;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37492;&#23450;&#20102;&#19968;&#31867;&#39640;&#32500;&#36830;&#32493;&#20989;&#25968;&#65292;&#21487;&#20197;&#29992;&#24102;&#26377;&#20462;&#27491;&#32447;&#24615;&#21333;&#20803;&#65288;ReLU&#65289;&#28608;&#27963;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#36924;&#36817;&#65292;&#26080;&#38656;&#25285;&#24515;&#8220;&#32500;&#24230;&#35781;&#21650;&#8221;&#12290;&#25442;&#35328;&#20043;&#65292;DNN&#21442;&#25968;&#30340;&#25968;&#37327;&#26368;&#22810;&#22312;&#36755;&#20837;&#32500;&#24230;&#21644;&#36924;&#36817;&#35823;&#24046;&#20013;&#20197;&#22810;&#39033;&#24335;&#24418;&#24335;&#22686;&#38271;&#12290;&#25105;&#20204;&#30340;&#31867;&#21035;&#21253;&#25324;&#19968;&#20123;&#29305;&#27530;&#20989;&#25968;&#30340;&#26080;&#38480;&#32452;&#21512;&#65292;&#21253;&#25324;&#20056;&#31215;&#12289;&#26497;&#22823;&#20540;&#21644;&#19968;&#23450;&#33539;&#22260;&#20869;&#30340;Lipschitz&#36830;&#32493;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this article we identify a general class of high-dimensional continuous functions that can be approximated by deep neural networks (DNNs) with the rectified linear unit (ReLU) activation without the curse of dimensionality. In other words, the number of DNN parameters grows at most polynomially in the input dimension and the approximation error. The functions in our class can be expressed as a potentially unbounded number of compositions of special functions which include products, maxima, and certain parallelized Lipschitz continuous functions.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#20855;&#26377;&#19968;&#32500;&#21367;&#31215;&#23618;&#30340;&#32447;&#24615;&#32593;&#32476;&#30340;&#20989;&#25968;&#31354;&#38388;&#65292;&#20998;&#26512;&#20102;&#32593;&#32476;&#26550;&#26500;&#23545;&#20989;&#25968;&#31354;&#38388;&#30340;&#24433;&#21709;&#24182;&#35777;&#26126;&#20102;&#23545;&#20110;&#27493;&#24133;&#22823;&#20110;&#19968;&#19988;&#25968;&#25454;&#19968;&#33324;&#30340;&#26550;&#26500;&#65292;&#35813;&#20248;&#21270;&#38382;&#39064;&#30340;&#38750;&#38646;&#20020;&#30028;&#28857;&#26159;&#20989;&#25968;&#31354;&#38388;&#30340;&#24179;&#28369;&#20869;&#37096;&#28857;&#12290;</title><link>http://arxiv.org/abs/2304.05752</link><description>&lt;p&gt;
&#32447;&#24615;&#21367;&#31215;&#32593;&#32476;&#30340;&#20989;&#25968;&#31354;&#38388;&#21644;&#20020;&#30028;&#28857;
&lt;/p&gt;
&lt;p&gt;
Function Space and Critical Points of Linear Convolutional Networks. (arXiv:2304.05752v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05752
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#20855;&#26377;&#19968;&#32500;&#21367;&#31215;&#23618;&#30340;&#32447;&#24615;&#32593;&#32476;&#30340;&#20989;&#25968;&#31354;&#38388;&#65292;&#20998;&#26512;&#20102;&#32593;&#32476;&#26550;&#26500;&#23545;&#20989;&#25968;&#31354;&#38388;&#30340;&#24433;&#21709;&#24182;&#35777;&#26126;&#20102;&#23545;&#20110;&#27493;&#24133;&#22823;&#20110;&#19968;&#19988;&#25968;&#25454;&#19968;&#33324;&#30340;&#26550;&#26500;&#65292;&#35813;&#20248;&#21270;&#38382;&#39064;&#30340;&#38750;&#38646;&#20020;&#30028;&#28857;&#26159;&#20989;&#25968;&#31354;&#38388;&#30340;&#24179;&#28369;&#20869;&#37096;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#19968;&#32500;&#21367;&#31215;&#23618;&#30340;&#32447;&#24615;&#32593;&#32476;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;&#36825;&#20123;&#32593;&#32476;&#30340;&#20989;&#25968;&#31354;&#38388;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#20855;&#26377;&#31232;&#30095;&#22240;&#23376;&#20998;&#35299;&#30340;&#21322;&#20195;&#25968;&#22810;&#39033;&#24335;&#26063;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#32593;&#32476;&#26550;&#26500;&#23545;&#20989;&#25968;&#31354;&#38388;&#30340;&#32500;&#24230;&#12289;&#36793;&#30028;&#21644;&#22855;&#24322;&#28857;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36824;&#25551;&#36848;&#20102;&#32593;&#32476;&#21442;&#25968;&#21270;&#26144;&#23556;&#30340;&#20020;&#30028;&#28857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#24179;&#26041;&#35823;&#24046;&#25439;&#22833;&#35757;&#32451;&#32593;&#32476;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#25152;&#26377;&#27493;&#24133;&#22823;&#20110;&#19968;&#19988;&#25968;&#25454;&#19968;&#33324;&#30340;&#26550;&#26500;&#65292;&#35813;&#20248;&#21270;&#38382;&#39064;&#30340;&#38750;&#38646;&#20020;&#30028;&#28857;&#26159;&#20989;&#25968;&#31354;&#38388;&#30340;&#24179;&#28369;&#20869;&#37096;&#28857;&#12290;&#23545;&#20110;&#31264;&#23494;&#30340;&#32447;&#24615;&#32593;&#32476;&#21644;&#27493;&#24133;&#20026;&#19968;&#30340;&#32447;&#24615;&#21367;&#31215;&#32593;&#32476;&#65292;&#36825;&#31181;&#29305;&#24615;&#34987;&#35748;&#20026;&#26159;&#38169;&#35823;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the geometry of linear networks with one-dimensional convolutional layers. The function spaces of these networks can be identified with semi-algebraic families of polynomials admitting sparse factorizations. We analyze the impact of the network's architecture on the function space's dimension, boundary, and singular points. We also describe the critical points of the network's parameterization map. Furthermore, we study the optimization problem of training a network with the squared error loss. We prove that for architectures where all strides are larger than one and generic data, the non-zero critical points of that optimization problem are smooth interior points of the function space. This property is known to be false for dense linear networks and linear convolutional networks with stride one.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25554;&#20837;&#24335;&#27169;&#22359;&#8212;&#8212;&#19981;&#30830;&#23450;&#24615;&#25513;&#34109;&#28151;&#21512;&#65288;UmmU&#65289;&#65292;&#23427;&#33021;&#22815;&#22312;&#20013;&#38388;&#23618;&#23884;&#20837;&#20013;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#36827;&#32780;&#22686;&#24378;&#23884;&#20837;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20351;&#20854;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#36830;&#32493;&#26102;&#38388;&#21160;&#24577;&#22270;&#32593;&#32476;&#30340;&#38271;&#26399;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.05749</link><description>&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#25552;&#39640;&#36830;&#32493;&#26102;&#38388;&#21160;&#24577;&#22270;&#32593;&#32476;&#38271;&#26399;&#39044;&#27979;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Boosting long-term forecasting performance for continuous-time dynamic graph networks via data augmentation. (arXiv:2304.05749v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05749
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25554;&#20837;&#24335;&#27169;&#22359;&#8212;&#8212;&#19981;&#30830;&#23450;&#24615;&#25513;&#34109;&#28151;&#21512;&#65288;UmmU&#65289;&#65292;&#23427;&#33021;&#22815;&#22312;&#20013;&#38388;&#23618;&#23884;&#20837;&#20013;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#36827;&#32780;&#22686;&#24378;&#23884;&#20837;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20351;&#20854;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#36830;&#32493;&#26102;&#38388;&#21160;&#24577;&#22270;&#32593;&#32476;&#30340;&#38271;&#26399;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20391;&#37325;&#20110;&#23545;&#36830;&#32493;&#26102;&#38388;&#21160;&#24577;&#22270;&#32593;&#32476;&#65288;CTDGNs&#65289;&#36827;&#34892;&#38271;&#26399;&#39044;&#27979;&#65288;LTF&#65289;&#65292;&#36825;&#23545;&#20110;&#29616;&#23454;&#19990;&#30028;&#24314;&#27169;&#38750;&#24120;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;CTDGN&#23545;&#20110;&#24314;&#27169;&#26102;&#38388;&#22270;&#25968;&#25454;&#38750;&#24120;&#26377;&#25928;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#25429;&#25417;&#22797;&#26434;&#30340;&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#20294;&#30001;&#20110;&#23545;&#21382;&#21490;&#25968;&#25454;&#30340;&#23454;&#36136;&#35201;&#27714;&#65292;&#23427;&#20204;&#22312;LTF&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#20063;&#19981;&#20999;&#23454;&#38469;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26368;&#30452;&#35266;&#30340;&#26041;&#27861;&#26159;&#25968;&#25454;&#22686;&#24378;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25554;&#20837;&#24335;&#27169;&#22359;&#8212;&#8212;&#19981;&#30830;&#23450;&#24615;&#25513;&#34109;&#28151;&#21512;&#65288;UmmU&#65289;&#65292;&#23427;&#33021;&#22815;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#23558;&#19981;&#30830;&#23450;&#24615;&#24341;&#20837;CTDGN&#30340;&#20013;&#38388;&#23618;&#23884;&#20837;&#20013;&#65292;&#24182;&#36827;&#34892;&#25513;&#34109;&#28151;&#21512;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#23884;&#20837;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20351;&#20854;&#33021;&#22815;&#36866;&#29992;&#20110;&#26356;&#22810;&#24773;&#20917;&#65292;&#32780;&#19988;&#21487;&#20197;&#36731;&#26494;&#22320;&#25554;&#20837;&#21040;&#20219;&#24847;CTDGN&#20013;&#65292;&#32780;&#19981;&#22686;&#21152;&#21442;&#25968;&#25968;&#37327;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#21160;&#24577;&#22270;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#24182;&#34920;&#26126;UmmU&#22312;LTF&#20219;&#21153;&#19978;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study focuses on long-term forecasting (LTF) on continuous-time dynamic graph networks (CTDGNs), which is important for real-world modeling. Existing CTDGNs are effective for modeling temporal graph data due to their ability to capture complex temporal dependencies but perform poorly on LTF due to the substantial requirement for historical data, which is not practical in most cases. To relieve this problem, a most intuitive way is data augmentation. In this study, we propose \textbf{\underline{U}ncertainty \underline{M}asked \underline{M}ix\underline{U}p (UmmU)}: a plug-and-play module that conducts uncertainty estimation to introduce uncertainty into the embedding of intermediate layer of CTDGNs, and perform masked mixup to further enhance the uncertainty of the embedding to make it generalize to more situations. UmmU can be easily inserted into arbitrary CTDGNs without increasing the number of parameters. We conduct comprehensive experiments on three real-world dynamic graph dat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#22495;&#22686;&#24378;&#32422;&#26463;&#21644;&#36328;&#22495;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#36328;&#39046;&#22495;&#23569;&#26679;&#26412;&#22686;&#37327;&#23398;&#20064;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#22312;MedMNIST&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.05734</link><description>&lt;p&gt;
&#36328;&#39046;&#22495;&#30142;&#30149;&#20998;&#31867;&#30340;&#23567;&#26679;&#26412;&#22686;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Few-shot Class-incremental Learning for Cross-domain Disease Classification. (arXiv:2304.05734v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05734
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#22495;&#22686;&#24378;&#32422;&#26463;&#21644;&#36328;&#22495;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#36328;&#39046;&#22495;&#23569;&#26679;&#26412;&#22686;&#37327;&#23398;&#20064;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#22312;MedMNIST&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#26377;&#38480;&#30340;&#26679;&#26412;&#20013;&#22686;&#37327;&#23398;&#20064;&#26032;&#31867;&#21035;&#30340;&#33021;&#21147;&#23545;&#20110;&#24320;&#21457;&#29992;&#20110;&#23454;&#38469;&#20020;&#24202;&#24212;&#29992;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#29616;&#26377;&#22686;&#37327;&#23398;&#20064;&#25216;&#26415;&#24050;&#32463;&#23581;&#35797;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#26159;&#24403;&#26679;&#26412;&#26469;&#33258;&#19981;&#21516;&#30340;&#39046;&#22495;&#24182;&#19988;&#26679;&#26412;&#26631;&#35760;&#36739;&#23569;&#26102;&#65292;&#23427;&#20204;&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#36328;&#22495;&#23569;&#26679;&#26412;&#22686;&#37327;&#23398;&#20064;&#38382;&#39064;&#12290;&#36328;&#22495;&#23569;&#26679;&#26412;&#22686;&#37327;&#23398;&#20064;&#38656;&#35201;&#27169;&#22411;&#20174;&#26497;&#23569;&#37327;&#26631;&#35760;&#26679;&#26412;&#20013;&#22686;&#37327;&#23398;&#20064;&#26032;&#30340;&#31867;&#21035;&#65292;&#24182;&#19988;&#36825;&#20123;&#26032;&#30340;&#31867;&#21035;&#21487;&#33021;&#19982;&#30446;&#26631;&#31354;&#38388;&#22823;&#19981;&#30456;&#21516;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38590;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#22495;&#22686;&#24378;&#32422;&#26463;&#21644;&#36328;&#22495;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#12290;&#22312;MedMNIST&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#30340;&#20998;&#31867;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#31867;&#20284;&#30340;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to incrementally learn new classes from limited samples is crucial to the development of artificial intelligence systems for real clinical application. Although existing incremental learning techniques have attempted to address this issue, they still struggle with only few labeled data, particularly when the samples are from varied domains. In this paper, we explore the cross-domain few-shot incremental learning (CDFSCIL) problem. CDFSCIL requires models to learn new classes from very few labeled samples incrementally, and the new classes may be vastly different from the target space. To counteract this difficulty, we propose a cross-domain enhancement constraint and cross-domain data augmentation method. Experiments on MedMNIST show that the classification performance of this method is better than other similar incremental learning methods.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#25991;&#31456;&#24635;&#32467;&#20102;&#36817;&#24180;&#26469;&#21160;&#24577;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#21457;&#23637;&#65292;&#20171;&#32461;&#20102;&#21160;&#24577;&#22270;&#30340;&#37325;&#35201;&#24615;&#21644;&#29366;&#24577;&#65292;&#24182;&#22238;&#39038;&#20102;&#21508;&#31181;&#21160;&#24577;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21253;&#25324;&#30417;&#30563;&#12289;&#26080;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#12290;&#25991;&#31456;&#36824;&#25506;&#35752;&#20102;&#21160;&#24577;&#22270;&#23398;&#20064;&#30340;&#24403;&#21069;&#38382;&#39064;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2304.05729</link><description>&lt;p&gt;
&#21160;&#24577;&#22270;&#34920;&#31034;&#23398;&#20064;&#32508;&#36848;&#65306;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Dynamic Graph Representation Learning with Neural Networks: A Survey. (arXiv:2304.05729v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05729
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#25991;&#31456;&#24635;&#32467;&#20102;&#36817;&#24180;&#26469;&#21160;&#24577;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#21457;&#23637;&#65292;&#20171;&#32461;&#20102;&#21160;&#24577;&#22270;&#30340;&#37325;&#35201;&#24615;&#21644;&#29366;&#24577;&#65292;&#24182;&#22238;&#39038;&#20102;&#21508;&#31181;&#21160;&#24577;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21253;&#25324;&#30417;&#30563;&#12289;&#26080;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#12290;&#25991;&#31456;&#36824;&#25506;&#35752;&#20102;&#21160;&#24577;&#22270;&#23398;&#20064;&#30340;&#24403;&#21069;&#38382;&#39064;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21160;&#24577;&#22270;&#34920;&#24449;&#30001;&#20110;&#20854;&#33021;&#22815;&#23558;&#25299;&#25169;&#20449;&#24687;&#21644;&#26102;&#38388;&#20449;&#24687;&#38598;&#25104;&#20026;&#19968;&#20010;&#32039;&#20945;&#30340;&#34920;&#31034;&#65292;&#22240;&#27492;&#22312;&#24314;&#27169;&#21160;&#24577;&#31995;&#32479;&#26041;&#38754;&#24471;&#21040;&#20102;&#36234;&#26469;&#36234;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#21160;&#24577;&#22270;&#30340;&#20986;&#29616;&#20351;&#24471;&#21487;&#20197;&#39640;&#25928;&#22320;&#22788;&#29702;&#35832;&#22914;&#31038;&#20132;&#32593;&#32476;&#39044;&#27979;&#12289;&#25512;&#33616;&#31995;&#32479;&#12289;&#20132;&#36890;&#39044;&#27979;&#25110;&#33041;&#30005;&#22270;&#20998;&#26512;&#31561;&#26080;&#27861;&#36890;&#36807;&#26631;&#20934;&#25968;&#23383;&#34920;&#31034;&#35299;&#20915;&#30340;&#24212;&#29992;&#12290;&#38543;&#30528;&#21160;&#24577;&#22270;&#34920;&#31034;&#30340;&#20986;&#29616;&#65292;&#21160;&#24577;&#22270;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#65292;&#32467;&#21512;&#20102;&#39034;&#24207;/&#26102;&#38388;&#25968;&#25454;&#22788;&#29702;&#21644;&#38745;&#24577;&#22270;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#65292;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#26159;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#36817;&#24180;&#26469;&#25552;&#20986;&#20102;&#22823;&#37327;&#30340;&#27169;&#22411;&#12290;&#26412;&#25991;&#26088;&#22312;&#32508;&#36848;&#19982;&#21160;&#24577;&#22270;&#23398;&#20064;&#30456;&#20851;&#30340;&#38382;&#39064;&#21644;&#27169;&#22411;&#65292;&#24182;&#22238;&#39038;&#21508;&#31181;&#21160;&#24577;&#22270;&#30417;&#30563;&#12289;&#26080;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#24403;&#21069;&#30340;&#24320;&#25918;&#38382;&#39064;&#21644;&#22522;&#20110;DGNN&#30340;&#21160;&#24577;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, Dynamic Graph (DG) representations have been increasingly used for modeling dynamic systems due to their ability to integrate both topological and temporal information in a compact representation. Dynamic graphs allow to efficiently handle applications such as social network prediction, recommender systems, traffic forecasting or electroencephalography analysis, that can not be adressed using standard numeric representations. As a direct consequence of the emergence of dynamic graph representations, dynamic graph learning has emerged as a new machine learning problem, combining challenges from both sequential/temporal data processing and static graph learning. In this research area, Dynamic Graph Neural Network (DGNN) has became the state of the art approach and plethora of models have been proposed in the very recent years. This paper aims at providing a review of problems and models related to dynamic graph learning. The various dynamic graph supervised learning sett
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;Explanation-Guided Exposure Minimization (EGEM)&#65292;&#35813;&#26041;&#27861;&#39044;&#38450;&#24615;&#22320;&#20462;&#21098;&#20102;ML&#27169;&#22411;&#20013;&#26410;&#21463;&#21040;&#31215;&#26497;&#35299;&#37322;&#21453;&#39304;&#30340;&#21464;&#21270;&#65292;&#20174;&#32780;&#22823;&#22823;&#20943;&#23569;&#20102;&#23545;&#38544;&#34255;Clever Hans&#31574;&#30053;&#30340;&#20381;&#36182;&#65292;&#24182;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.05727</link><description>&lt;p&gt;
&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#39044;&#38450;&#24615;&#20462;&#21098;Clever Hans&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Preemptively Pruning Clever-Hans Strategies in Deep Neural Networks. (arXiv:2304.05727v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05727
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;Explanation-Guided Exposure Minimization (EGEM)&#65292;&#35813;&#26041;&#27861;&#39044;&#38450;&#24615;&#22320;&#20462;&#21098;&#20102;ML&#27169;&#22411;&#20013;&#26410;&#21463;&#21040;&#31215;&#26497;&#35299;&#37322;&#21453;&#39304;&#30340;&#21464;&#21270;&#65292;&#20174;&#32780;&#22823;&#22823;&#20943;&#23569;&#20102;&#23545;&#38544;&#34255;Clever Hans&#31574;&#30053;&#30340;&#20381;&#36182;&#65292;&#24182;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;AI&#24050;&#25104;&#20026;&#39564;&#35777;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#27969;&#34892;&#24037;&#20855;&#12290;&#35299;&#37322;&#27169;&#22411;&#30340;&#20915;&#31574;&#31574;&#30053;&#19982;&#29992;&#25143;&#30340;&#39046;&#22495;&#30693;&#35782;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#65288;&#20363;&#22914;Clever Hans&#25928;&#24212;&#65289;&#20063;&#34987;&#35748;&#20026;&#26159;&#25913;&#36827;&#38169;&#35823;&#27169;&#22411;&#30340;&#36215;&#28857;&#12290;&#28982;&#32780;&#65292;&#24403;&#29992;&#25143;&#21644;&#35299;&#37322;&#36798;&#25104;&#19968;&#33268;&#26102;&#65292;&#35201;&#24590;&#20040;&#20570;&#23601;&#19981;&#37027;&#20040;&#28165;&#26970;&#20102;&#12290;&#26412;&#25991;&#36890;&#36807;&#23637;&#31034;&#29992;&#25143;&#25509;&#21463;&#35299;&#37322;&#24182;&#19981;&#20445;&#35777;ML&#27169;&#22411;&#30340;&#33391;&#22909;&#21151;&#33021;&#65292;&#29305;&#21035;&#26159;&#19968;&#20123;&#38544;&#34255;&#30340;Clever Hans&#25928;&#24212;&#21487;&#33021;&#20173;&#28982;&#26410;&#34987;&#21457;&#29616;&#65292;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#36890;&#36807;&#36129;&#29486;&#19968;&#20010;&#26032;&#26041;&#27861;Explanation-Guided Exposure Minimization (EGEM)&#65292;&#35813;&#26041;&#27861;&#39044;&#38450;&#24615;&#22320;&#20462;&#21098;&#20102;ML&#27169;&#22411;&#20013;&#26410;&#21463;&#21040;&#31215;&#26497;&#35299;&#37322;&#21453;&#39304;&#30340;&#21464;&#21270;&#12290;&#33258;&#28982;&#30011;&#20687;&#25968;&#25454;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23548;&#33268;&#27169;&#22411;&#22823;&#22823;&#20943;&#23569;&#20102;&#23545;&#38544;&#34255;&#30340;Clever Hans&#31574;&#30053;&#30340;&#20381;&#36182;&#65292;&#24182;&#22240;&#27492;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable AI has become a popular tool for validating machine learning models. Mismatches between the explained model's decision strategy and the user's domain knowledge (e.g. Clever Hans effects) have also been recognized as a starting point for improving faulty models. However, it is less clear what to do when the user and the explanation agree. In this paper, we demonstrate that acceptance of explanations by the user is not a guarantee for a ML model to function well, in particular, some Clever Hans effects may remain undetected. Such hidden flaws of the model can nevertheless be mitigated, and we demonstrate this by contributing a new method, Explanation-Guided Exposure Minimization (EGEM), that premptively prunes variations in the ML model that have not been the subject of positive explanation feedback. Experiments on natural image data demonstrate that our approach leads to models that strongly reduce their reliance on hidden Clever Hans strategies, and consequently achieve hig
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#27491;&#21017;&#21270;&#21644;&#22810;&#35270;&#35282;&#25903;&#25345;&#21521;&#37327;&#26426;&#23398;&#20064;&#38382;&#39064;&#30340;&#26412;&#22320;&#21270;&#29256;&#26412;&#65292;&#35777;&#26126;&#20102;&#19968;&#20123;&#34920;&#31034;&#23450;&#29702;&#65292;&#30740;&#31350;&#20102;&#19982;&#25439;&#22833;&#20989;&#25968;&#21644;&#36755;&#20837;&#31354;&#38388;&#32500;&#24230;&#30456;&#20851;&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#29305;&#21035;&#26159;&#25439;&#22833;&#20989;&#25968;&#20026; G&#226;teaux &#21487;&#24494;&#20989;&#25968;&#26102;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2304.05655</link><description>&lt;p&gt;
&#27491;&#21017;&#21270;&#21644;&#22810;&#35270;&#35282;&#25903;&#25345;&#21521;&#37327;&#26426;&#23398;&#20064;&#30340;&#26412;&#22320;&#21270;
&lt;/p&gt;
&lt;p&gt;
Localisation of Regularised and Multiview Support Vector Machine Learning. (arXiv:2304.05655v1 [math.FA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05655
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#27491;&#21017;&#21270;&#21644;&#22810;&#35270;&#35282;&#25903;&#25345;&#21521;&#37327;&#26426;&#23398;&#20064;&#38382;&#39064;&#30340;&#26412;&#22320;&#21270;&#29256;&#26412;&#65292;&#35777;&#26126;&#20102;&#19968;&#20123;&#34920;&#31034;&#23450;&#29702;&#65292;&#30740;&#31350;&#20102;&#19982;&#25439;&#22833;&#20989;&#25968;&#21644;&#36755;&#20837;&#31354;&#38388;&#32500;&#24230;&#30456;&#20851;&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#29305;&#21035;&#26159;&#25439;&#22833;&#20989;&#25968;&#20026; G&#226;teaux &#21487;&#24494;&#20989;&#25968;&#26102;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102; H.Q. Minh&#12289;L. Bazzani &#21644; V. Murino &#22312;&#12298;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#12299;&#65288;Journal of Machine Learning Research&#65289;&#20013;&#20171;&#32461;&#30340;&#19968;&#31181;&#28041;&#21450;&#31639;&#23376;&#20540;&#27491;&#23450;&#26680;&#21450;&#20854;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#30340;&#27491;&#21017;&#21270;&#21644;&#22810;&#35270;&#35282;&#25903;&#25345;&#21521;&#37327;&#26426;&#23398;&#20064;&#38382;&#39064;&#30340;&#26412;&#22320;&#21270;&#29256;&#26412;&#30340;&#19968;&#20123;&#34920;&#31034;&#23450;&#29702;&#12290;&#32467;&#26524;&#28041;&#21450;&#21040;&#32771;&#34385;&#20984;&#25110;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#20197;&#21450;&#26377;&#38480;&#25110;&#26080;&#38480;&#32500;&#36755;&#20837;&#31354;&#38388;&#30340;&#19968;&#33324;&#24773;&#20917;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#19968;&#33324;&#26694;&#26550;&#20801;&#35768;&#19968;&#20123;&#29305;&#27530;&#24773;&#20917;&#19979;&#30340;&#26080;&#38480;&#32500;&#36755;&#20837;&#31354;&#38388;&#21644;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#65292;&#29305;&#21035;&#26159;&#24403;&#25439;&#22833;&#20989;&#25968;&#20026; G&#226;teaux &#21487;&#24494;&#20989;&#25968;&#26102;&#12290;&#23545;&#23548;&#33268;&#37096;&#20998;&#38750;&#32447;&#24615;&#38382;&#39064;&#30340;&#25351;&#25968;&#26368;&#23567;&#20108;&#20056;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#20102;&#35814;&#32454;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
We prove a few representer theorems for a localised version of the regularised and multiview support vector machine learning problem introduced by H.Q.~Minh, L.~Bazzani, and V.~Murino, \textit{Journal of Machine Learning Research}, \textbf{17}(2016) 1--72, that involves operator valued positive semidefinite kernels and their reproducing kernel Hilbert spaces. The results concern general cases when convex or nonconvex loss functions and finite or infinite dimensional input spaces are considered. We show that the general framework allows infinite dimensional input spaces and nonconvex loss functions for some special cases, in particular in case the loss functions are G\^ateaux differentiable. Detailed calculations are provided for the exponential least squares loss functions that leads to partially nonlinear problems.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36951;&#20256;&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#25105;&#20248;&#21270;&#22797;&#26434;&#36807;&#31243;&#30340;&#31995;&#32479;&#36923;&#36753;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#35299;&#26512;&#36951;&#20256;&#32467;&#26524;&#65292;&#29983;&#25104;&#31243;&#24207;&#20195;&#30721;&#23454;&#29616;&#31995;&#32479;&#65292;&#23454;&#29616;&#20102;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.05638</link><description>&lt;p&gt;
PLC&#22522;&#25511;&#21046;&#36807;&#31243;&#20013;&#30340;&#33258;&#25105;&#20248;&#21270;&#21644;&#33258;&#21160;&#20195;&#30721;&#29983;&#25104;&#30340;&#36827;&#21270;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Self Optimisation and Automatic Code Generation by Evolutionary Algorithms in PLC based Controlling Processes. (arXiv:2304.05638v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05638
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36951;&#20256;&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#25105;&#20248;&#21270;&#22797;&#26434;&#36807;&#31243;&#30340;&#31995;&#32479;&#36923;&#36753;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#35299;&#26512;&#36951;&#20256;&#32467;&#26524;&#65292;&#29983;&#25104;&#31243;&#24207;&#20195;&#30721;&#23454;&#29616;&#31995;&#32479;&#65292;&#23454;&#29616;&#20102;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#30340;&#25968;&#23383;&#21270;&#36716;&#22411;&#23545;&#24037;&#19994;&#36807;&#31243;&#20013;&#30340;&#25968;&#25454;&#37319;&#38598;&#21644;&#22788;&#29702;&#25552;&#20986;&#20102;&#26032;&#30340;&#35201;&#27714;&#12290;&#33719;&#24471;&#30340;&#25968;&#25454;&#19982;&#21608;&#26399;&#24615;&#30340;&#36807;&#31243;&#24207;&#21015;&#20043;&#38388;&#30340;&#36923;&#36753;&#20851;&#31995;&#24517;&#39035;&#24471;&#21040;&#27491;&#30830;&#30340;&#35299;&#37322;&#21644;&#35780;&#20272;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36827;&#21270;&#31639;&#27861;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#25105;&#20248;&#21270;&#22797;&#26434;&#36807;&#31243;&#30340;&#31995;&#32479;&#36923;&#36753;&#12290;&#22522;&#20110;&#36951;&#20256;&#32467;&#26524;&#65292;&#36890;&#36807;&#35299;&#30721;&#35299;&#20915;&#26041;&#26696;&#26469;&#24471;&#21040;&#31995;&#32479;&#23454;&#29616;&#30340;&#31243;&#24207;&#20195;&#30721;&#12290;&#36825;&#26159;&#36890;&#36807;&#19968;&#20010;&#20855;&#26377;&#19978;&#28216;&#12289;&#20013;&#28216;&#21644;&#19979;&#28216;&#21333;&#20803;&#30340;&#28789;&#27963;&#31995;&#32479;&#32467;&#26500;&#23454;&#29616;&#30340;&#12290;&#22312;&#20013;&#38388;&#21333;&#20803;&#20013;&#65292;&#19968;&#20010;&#26377;&#21521;&#23398;&#20064;&#36807;&#31243;&#19982;&#31995;&#32479;&#21103;&#26412;&#21644;&#35780;&#20272;&#20989;&#25968;&#22312;&#38381;&#29615;&#20013;&#20132;&#20114;&#12290;&#20195;&#30721;&#29983;&#25104;&#31574;&#30053;&#30001;&#20887;&#20313;&#21644;&#20248;&#20808;&#32423;&#12289;&#25490;&#24207;&#21644;&#24615;&#33021;&#27966;&#29983;&#34920;&#31034;&#12290;&#35813;&#26041;&#27861;&#22312;&#19968;&#20010;&#24037;&#19994;&#28082;&#20307;&#31449;&#36807;&#31243;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#35813;&#36807;&#31243;&#38754;&#20020;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The digital transformation of automation places new demands on data acquisition and processing in industrial processes. Logical relationships between acquired data and cyclic process sequences must be correctly interpreted and evaluated. To solve this problem, a novel approach based on evolutionary algorithms is proposed to self optimise the system logic of complex processes. Based on the genetic results, a programme code for the system implementation is derived by decoding the solution. This is achieved by a flexible system structure with an upstream, intermediate and downstream unit. In the intermediate unit, a directed learning process interacts with a system replica and an evaluation function in a closed loop. The code generation strategy is represented by redundancy and priority, sequencing and performance derivation. The presented approach is evaluated on an industrial liquid station process subject to a multi-objective optimisation problem.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#26234;&#33021;&#20307;&#31574;&#30053;&#20114;&#24800;&#65288;PR&#65289;&#26694;&#26550;&#65292;&#33021;&#22815;&#36890;&#36807;&#23450;&#20041;&#37051;&#25509;&#31354;&#38388;&#21644;&#35774;&#35745;&#21363;&#25554;&#21363;&#29992;&#27169;&#22359;&#22312;&#19981;&#21305;&#37197;&#30340;&#29366;&#24577;&#19979;&#20805;&#20998;&#21033;&#29992;&#20132;&#21449;&#26234;&#33021;&#20307;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#21516;&#26102;&#20855;&#26377;&#29702;&#35770;&#20445;&#38556;&#65292;&#33021;&#22312;&#20010;&#20307;&#24863;&#30693;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#31283;&#23450;&#25910;&#25947;&#20110;&#26368;&#20248;&#20540;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2304.05632</link><description>&lt;p&gt;
&#20855;&#26377;&#29702;&#35770;&#20445;&#38556;&#30340;&#22810;&#26234;&#33021;&#20307;&#31574;&#30053;&#20114;&#24800;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multi-agent Policy Reciprocity with Theoretical Guarantee. (arXiv:2304.05632v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05632
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#26234;&#33021;&#20307;&#31574;&#30053;&#20114;&#24800;&#65288;PR&#65289;&#26694;&#26550;&#65292;&#33021;&#22815;&#36890;&#36807;&#23450;&#20041;&#37051;&#25509;&#31354;&#38388;&#21644;&#35774;&#35745;&#21363;&#25554;&#21363;&#29992;&#27169;&#22359;&#22312;&#19981;&#21305;&#37197;&#30340;&#29366;&#24577;&#19979;&#20805;&#20998;&#21033;&#29992;&#20132;&#21449;&#26234;&#33021;&#20307;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#21516;&#26102;&#20855;&#26377;&#29702;&#35770;&#20445;&#38556;&#65292;&#33021;&#22312;&#20010;&#20307;&#24863;&#30693;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#31283;&#23450;&#25910;&#25947;&#20110;&#26368;&#20248;&#20540;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20855;&#26377;&#35299;&#20915;&#21508;&#31181;&#23454;&#38469;&#38382;&#39064;&#30340;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#26410;&#33021;&#20805;&#20998;&#21033;&#29992;&#20132;&#21449;&#26234;&#33021;&#20307;&#30693;&#35782;&#26469;&#38477;&#20302;&#26679;&#26412;&#22797;&#26434;&#24230;&#21644;&#25552;&#39640;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#26234;&#33021;&#20307;&#31574;&#30053;&#20114;&#24800;&#65288;PR&#65289;&#26694;&#26550;&#65292;&#27599;&#20010;&#26234;&#33021;&#20307;&#21487;&#20197;&#20805;&#20998;&#21033;&#29992;&#20132;&#21449;&#26234;&#33021;&#20307;&#31574;&#30053;&#65292;&#21363;&#20351;&#22312;&#19981;&#21305;&#37197;&#30340;&#29366;&#24577;&#19979;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#31181;&#19981;&#21305;&#37197;&#29366;&#24577;&#30340;&#37051;&#25509;&#31354;&#38388;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#29992;&#20110;&#20215;&#20540;&#36845;&#20195;&#30340;&#21363;&#25554;&#21363;&#29992;&#27169;&#22359;&#65292;&#20351;&#26234;&#33021;&#20307;&#21487;&#20197;&#25512;&#26029;&#26356;&#31934;&#30830;&#30340;&#22238;&#25253;&#12290;&#20026;&#20102;&#25552;&#39640;PR&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#30340;&#28145;&#23618;PR&#31639;&#27861;&#12290;&#21478;&#22806;&#65292;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;&#20010;&#20307;&#24863;&#30693;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#65292;&#26234;&#33021;&#20307;&#21487;&#20197;&#28176;&#36827;&#22320;&#36798;&#25104;&#20849;&#35782;&#24182;&#25910;&#25947;&#20110;&#26368;&#20248;&#20540;&#20989;&#25968;&#65292;&#36825;&#20998;&#21035;&#24847;&#21619;&#30528;PR&#30340;&#31283;&#23450;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;PR&#22312;&#22810;&#20010;&#22522;&#20934;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern multi-agent reinforcement learning (RL) algorithms hold great potential for solving a variety of real-world problems. However, they do not fully exploit cross-agent knowledge to reduce sample complexity and improve performance. Although transfer RL supports knowledge sharing, it is hyperparameter sensitive and complex. To solve this problem, we propose a novel multi-agent policy reciprocity (PR) framework, where each agent can fully exploit cross-agent policies even in mismatched states. We then define an adjacency space for mismatched states and design a plug-and-play module for value iteration, which enables agents to infer more precise returns. To improve the scalability of PR, deep PR is proposed for continuous control tasks. Moreover, theoretical analysis shows that agents can asymptotically reach consensus through individual perceived rewards and converge to an optimal value function, which implies the stability and effectiveness of PR, respectively. Experimental results o
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;Segment Any Medical Model (SAMM)&#65292;&#23427;&#26159;&#29992;&#20110;3D Slicer&#30340;SAM&#30340;&#25193;&#23637;&#12290;SAMM&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#22312;&#23454;&#26102;&#24615;&#21644;&#36890;&#29992;&#24615;&#26041;&#38754;&#37117;&#26377;&#24456;&#22909;&#30340;&#24615;&#33021;&#65292;&#21487;&#20197;&#25512;&#26029;&#20986;&#25513;&#27169;&#12290;</title><link>http://arxiv.org/abs/2304.05622</link><description>&lt;p&gt;
SAMM&#65288;Segment Any Medical Model&#65289;&#65306;&#29992;&#20110;SAM&#30340;3D Slicer&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
SAMM (Segment Any Medical Model): A 3D Slicer Integration to SAM. (arXiv:2304.05622v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05622
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;Segment Any Medical Model (SAMM)&#65292;&#23427;&#26159;&#29992;&#20110;3D Slicer&#30340;SAM&#30340;&#25193;&#23637;&#12290;SAMM&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#22312;&#23454;&#26102;&#24615;&#21644;&#36890;&#29992;&#24615;&#26041;&#38754;&#37117;&#26377;&#24456;&#22909;&#30340;&#24615;&#33021;&#65292;&#21487;&#20197;&#25512;&#26029;&#20986;&#25513;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Segment Anything Model&#65288;SAM&#65289;&#26159;&#19968;&#20010;&#26032;&#30340;&#22270;&#20687;&#20998;&#21106;&#24037;&#20855;&#65292;&#20351;&#29992;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#20998;&#21106;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#35813;&#27169;&#22411;&#34920;&#26126;&#23427;&#21487;&#20197;&#21019;&#24314;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#20998;&#21106;&#25513;&#27169;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#23454;&#26102;&#24615;&#21644;&#36890;&#29992;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#21307;&#23398;&#22270;&#20687;&#19978;&#30340;&#24615;&#33021;&#38656;&#35201;&#36827;&#19968;&#27493;&#39564;&#35777;&#12290;&#20026;&#20102;&#21327;&#21161;&#22312;&#21307;&#23398;&#22270;&#20687;&#19978;&#24320;&#21457;&#65292;&#35780;&#20272;&#21644;&#21033;&#29992;SAM&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Segment Any Medical Model&#65288;SAMM&#65289;&#65292;&#23427;&#26159;SAM&#22312;3D Slicer&#19978;&#30340;&#25193;&#23637;&#12290;3D Slicer&#26159;&#19968;&#20010;&#24191;&#27867;&#20351;&#29992;&#20110;&#21307;&#23398;&#24433;&#20687;&#22788;&#29702;&#21644;&#21487;&#35270;&#21270;&#36719;&#20214;&#30340;&#24320;&#28304;&#36719;&#20214;&#12290;&#36825;&#20010;&#24320;&#28304;&#25193;&#23637;&#31243;&#24207;&#21450;&#20854;&#28436;&#31034;&#24050;&#21457;&#24067;&#22312;GitHub&#19978;&#65288;https://github.com/bingogome/samm&#65289;&#12290;SAMM&#22312;&#23436;&#25972;&#21608;&#26399;&#20013;&#23454;&#29616;&#20102;0.6&#31186;&#30340;&#24310;&#36831;&#65292;&#24182;&#21487;&#20197;&#23454;&#26102;&#25512;&#26029;&#20986;&#22270;&#20687;&#25513;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Segment Anything Model (SAM) is a new image segmentation tool trained with the largest segmentation dataset at this time. The model has demonstrated that it can create high-quality masks for image segmentation with good promptability and generalizability. However, the performance of the model on medical images requires further validation. To assist with the development, assessment, and utilization of SAM on medical images, we introduce Segment Any Medical Model (SAMM), an extension of SAM on 3D Slicer, a widely-used open-source image processing and visualization software that has been extensively used in the medical imaging community. This open-source extension to 3D Slicer and its demonstrations are posted on GitHub (https://github.com/bingogome/samm). SAMM achieves 0.6-second latency of a complete cycle and can infer image masks in nearly real-time.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#22686;&#21152;&#37197;&#38899;&#26469;&#22686;&#24378;&#36328;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#38899;&#39057;&#21644;&#35270;&#21548;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.05600</link><description>&lt;p&gt;
&#30456;&#35980;&#30456;&#20284;&#65292;&#22768;&#38899;&#19981;&#21516;&#65306;&#21033;&#29992;&#21453;&#20107;&#23454;&#36328;&#27169;&#24577;&#23545;&#23398;&#20064;&#38899;&#35270;&#39057;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Looking Similar, Sounding Different: Leveraging Counterfactual Cross-Modal Pairs for Audiovisual Representation Learning. (arXiv:2304.05600v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05600
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#22686;&#21152;&#37197;&#38899;&#26469;&#22686;&#24378;&#36328;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#38899;&#39057;&#21644;&#35270;&#21548;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#35270;&#39057;&#34920;&#31034;&#23398;&#20064;&#36890;&#24120;&#20381;&#36182;&#20110;&#35270;&#21548;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#22312;&#19968;&#20010;&#35270;&#35273;&#22330;&#26223;&#20013;&#21487;&#33021;&#23384;&#22312;&#22810;&#20010;&#22768;&#38899;&#36712;&#36947;&#19982;&#20043;&#23545;&#24212;&#12290;&#20363;&#22914;&#65292;&#22312;&#21516;&#19968;&#25317;&#25380;&#30340;&#34903;&#36947;&#19978;&#26377;&#19981;&#21516;&#30340;&#20132;&#35848;&#22768;&#12290;&#36825;&#20123;&#21453;&#20107;&#23454;&#23545;&#20110;&#38899;&#35270;&#39057;&#34920;&#31034;&#23398;&#20064;&#30340;&#24433;&#21709;&#23578;&#26410;&#30740;&#31350;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#30005;&#24433;&#30340;&#37197;&#38899;&#29256;&#26412;&#26469;&#22686;&#21152;&#36328;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23398;&#20064;&#34920;&#31034;&#31867;&#20284;&#20110;&#30456;&#21516;&#35270;&#39057;&#30340;&#20165;&#22312;&#35821;&#38899;&#20869;&#23481;&#19978;&#19981;&#21516;&#30340;&#26367;&#20195;&#38899;&#36712;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22686;&#21152;&#37197;&#38899;&#30340;&#35757;&#32451;&#22312;&#19968;&#31995;&#21015;&#21548;&#35273;&#21644;&#35270;&#21548;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#32780;&#23545;&#35821;&#35328;&#20219;&#21153;&#30340;&#25972;&#20307;&#34920;&#29616;&#24433;&#21709;&#19981;&#22823;&#12290;&#25105;&#20204;&#36824;&#23558;&#36825;&#31181;&#26041;&#27861;&#19982;&#22312;&#39044;&#35757;&#32451;&#20043;&#21069;&#21435;&#38500;&#35821;&#38899;&#30340;&#24378;&#22823;&#22522;&#32447;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#21457;&#29616;&#22686;&#21152;&#37197;&#38899;&#30340;&#35757;&#32451;&#26356;&#26377;&#25928;&#65292;&#21253;&#25324;&#35821;&#38899;&#22806;&#35821;&#21644;&#35270;&#21548;&#20219;&#21153;&#65292;&#20854;&#20013;&#35821;&#38899;&#24674;&#22797;&#20219;&#21153;&#24615;&#33021;&#25552;&#39640;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Audiovisual representation learning typically relies on the correspondence between sight and sound. However, there are often multiple audio tracks that can correspond with a visual scene. Consider, for example, different conversations on the same crowded street. The effect of such counterfactual pairs on audiovisual representation learning has not been previously explored. To investigate this, we use dubbed versions of movies to augment cross-modal contrastive learning. Our approach learns to represent alternate audio tracks, differing only in speech content, similarly to the same video. Our results show that dub-augmented training improves performance on a range of auditory and audiovisual tasks, without significantly affecting linguistic task performance overall. We additionally compare this approach to a strong baseline where we remove speech before pretraining, and find that dub-augmented training is more effective, including for paralinguistic and audiovisual tasks where speech re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SLIM&#30340;&#24320;&#28304;&#36719;&#20214;&#26694;&#26550;&#65292;&#23558;&#21487;&#24494;&#32534;&#31243;&#21644;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#22810;&#29289;&#29702;&#21453;&#28436;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20280;&#32553;&#30340;&#21407;&#22411;&#29992;&#20110;&#20174;&#26102;&#38388;&#36830;&#32493;&#30340;&#36328;&#20117;&#22320;&#38663;&#25968;&#25454;&#20013;&#20272;&#35745;&#28183;&#36879;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.05592</link><description>&lt;p&gt;
&#21033;&#29992;&#21487;&#24494;&#32534;&#31243;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#22810;&#29289;&#29702;&#21453;&#28436;&#25216;&#26415;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learned multiphysics inversion with differentiable programming and machine learning. (arXiv:2304.05592v1 [cs.MS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SLIM&#30340;&#24320;&#28304;&#36719;&#20214;&#26694;&#26550;&#65292;&#23558;&#21487;&#24494;&#32534;&#31243;&#21644;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#22810;&#29289;&#29702;&#21453;&#28436;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20280;&#32553;&#30340;&#21407;&#22411;&#29992;&#20110;&#20174;&#26102;&#38388;&#36830;&#32493;&#30340;&#36328;&#20117;&#22320;&#38663;&#25968;&#25454;&#20013;&#20272;&#35745;&#28183;&#36879;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Seismic Laboratory for Imaging and Modeling/Monitoring (SLIM)&#30340;&#24320;&#28304;&#36719;&#20214;&#26694;&#26550;&#65292;&#29992;&#20110;&#35745;&#31639;&#22320;&#29699;&#29289;&#29702;&#23398;&#21644;&#26356;&#19968;&#33324;&#30340;&#21253;&#25324;&#27874;&#21160;&#26041;&#31243;&#36870;&#38382;&#39064;(&#20363;&#22914;&#22320;&#38663;&#21644;&#21307;&#23398;&#36229;&#22768;)&#12289;&#23398;&#20064;&#20808;&#39564;&#27491;&#21017;&#21270;&#21644;&#23398;&#20064;&#31070;&#32463;&#20195;&#29702;&#22810;&#30456;&#27969;&#27169;&#25311;&#12290;&#36890;&#36807;&#25972;&#21512;&#22810;&#31181;&#25277;&#35937;&#23618;&#27425;&#65292;&#25105;&#20204;&#30340;&#36719;&#20214;&#26088;&#22312;&#26082;&#21487;&#35835;&#24615;&#21448;&#21487;&#20280;&#32553;&#24615;&#12290;&#36825;&#20351;&#24471;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#36731;&#26494;&#22320;&#20197;&#25277;&#35937;&#30340;&#26041;&#24335;&#26500;&#24605;&#24182;&#21033;&#29992;&#26368;&#26032;&#30340;&#39640;&#24615;&#33021;&#35745;&#31639;&#25216;&#26415;&#12290;&#25105;&#20204;&#36890;&#36807;&#24314;&#31435;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#21407;&#22411;&#26469;&#20174;&#26102;&#38388;&#36830;&#32493;&#30340;&#36328;&#20117;&#22320;&#38663;&#25968;&#25454;&#20013;&#20272;&#35745;&#28183;&#36879;&#29575;&#26469;&#35828;&#26126;&#21644;&#35777;&#26126;&#25105;&#20204;&#30340;&#35774;&#35745;&#21407;&#21017;&#21450;&#20854;&#30410;&#22788;&#65292;&#35813;&#21407;&#22411;&#28041;&#21450;&#27874;&#21160;&#29289;&#29702;&#21644;&#22810;&#30456;&#27969;&#30340;&#32806;&#21512;&#20197;&#21450;&#26426;&#22120;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present the Seismic Laboratory for Imaging and Modeling/Monitoring (SLIM) open-source software framework for computational geophysics and, more generally, inverse problems involving the wave-equation (e.g., seismic and medical ultrasound), regularization with learned priors, and learned neural surrogates for multiphase flow simulations. By integrating multiple layers of abstraction, our software is designed to be both readable and scalable. This allows researchers to easily formulate their problems in an abstract fashion while exploiting the latest developments in high-performance computing. We illustrate and demonstrate our design principles and their benefits by means of building a scalable prototype for permeability inversion from time-lapse crosswell seismic data, which aside from coupling of wave physics and multiphase flow, involves machine learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#34920;&#26126;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26497;&#22823;&#22320;&#22686;&#24378;&#20256;&#32479;&#30340;&#35821;&#20041;&#29305;&#24449;&#35268;&#33539;&#39564;&#35777;&#26041;&#27861;&#65292;&#20351;&#20043;&#33021;&#22815;&#25429;&#25417;&#36229;&#36807;&#20154;&#31867;&#35268;&#33539;&#33539;&#30068;&#30340;&#20449;&#24687;&#65292;&#23545;&#20110;&#25105;&#20204;&#29702;&#35299;&#20154;&#31867;&#21644;&#26426;&#22120;&#30340;&#27010;&#24565;&#34920;&#31034;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2304.05591</link><description>&lt;p&gt;
FLAN-T5&#20013;&#35821;&#20041;&#29305;&#24449;&#39564;&#35777;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Semantic Feature Verification in FLAN-T5. (arXiv:2304.05591v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05591
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#34920;&#26126;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26497;&#22823;&#22320;&#22686;&#24378;&#20256;&#32479;&#30340;&#35821;&#20041;&#29305;&#24449;&#35268;&#33539;&#39564;&#35777;&#26041;&#27861;&#65292;&#20351;&#20043;&#33021;&#22815;&#25429;&#25417;&#36229;&#36807;&#20154;&#31867;&#35268;&#33539;&#33539;&#30068;&#30340;&#20449;&#24687;&#65292;&#23545;&#20110;&#25105;&#20204;&#29702;&#35299;&#20154;&#31867;&#21644;&#26426;&#22120;&#30340;&#27010;&#24565;&#34920;&#31034;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#35821;&#20041;&#29305;&#24449;&#35268;&#33539;&#26041;&#38754;&#30340;&#28508;&#21147;&#8212;&#8212;&#36825;&#26159;&#35780;&#20215;&#35748;&#30693;&#31185;&#23398;&#20013;&#27010;&#24565;&#32467;&#26500;&#30340;&#20851;&#38190;&#24037;&#20855;&#12290;&#25105;&#20204;&#22522;&#20110;&#29616;&#26377;&#30340;&#20154;&#24037;&#29983;&#25104;&#25968;&#25454;&#38598;&#36827;&#34892;&#26500;&#24314;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#26426;&#22120;&#39564;&#35777;&#30340;&#35268;&#33539;&#25429;&#25417;&#20102;&#27010;&#24565;&#32467;&#26500;&#30340;&#26576;&#20123;&#26041;&#38754;&#65292;&#36229;&#36234;&#20102;&#20165;&#32771;&#34385;&#20154;&#31867;&#35268;&#33539;&#25152;&#33021;&#28085;&#30422;&#30340;&#33539;&#22260;&#65292;&#24182;&#19988;&#26356;&#22909;&#22320;&#35299;&#37322;&#20102;&#22312;&#36828;&#36317;&#31163;&#30456;&#20851;&#30340;&#39033;&#30446;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24230;&#20154;&#31867;&#21028;&#26029;&#12290;&#35813;&#32467;&#26524;&#34920;&#26126;LLM&#21487;&#20197;&#26497;&#22823;&#22320;&#22686;&#24378;&#20256;&#32479;&#30340;&#35821;&#20041;&#29305;&#24449;&#35268;&#33539;&#39564;&#35777;&#26041;&#27861;&#65292;&#36825;&#23545;&#20110;&#25105;&#20204;&#29702;&#35299;&#20154;&#31867;&#21644;&#26426;&#22120;&#30340;&#27010;&#24565;&#34920;&#31034;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study evaluates the potential of a large language model for aiding in generation of semantic feature norms - a critical tool for evaluating conceptual structure in cognitive science. Building from an existing human-generated dataset, we show that machine-verified norms capture aspects of conceptual structure beyond what is expressed in human norms alone, and better explain human judgments of semantic similarity amongst items that are distally related. The results suggest that LLMs can greatly enhance traditional methods of semantic feature norm verification, with implications for our understanding of conceptual representation in humans and machines.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#30340;&#25945;&#32946;&#23545;&#35805;&#34892;&#20026;&#20998;&#31867;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#36873;&#25321;&#20449;&#24687;&#26679;&#26412;&#65292;&#24182;&#19988;&#33021;&#22815;&#20248;&#20110;&#38543;&#26426;&#25277;&#26679;&#26041;&#27861;&#21644;&#20854;&#20182;AL&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.05578</link><description>&lt;p&gt;
&#20449;&#24687;&#37327;&#26159;&#21542;&#37325;&#35201;&#65311;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#25945;&#32946;&#23545;&#35805;&#34892;&#20026;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Does Informativeness Matter? Active Learning for Educational Dialogue Act Classification. (arXiv:2304.05578v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05578
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#30340;&#25945;&#32946;&#23545;&#35805;&#34892;&#20026;&#20998;&#31867;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#36873;&#25321;&#20449;&#24687;&#26679;&#26412;&#65292;&#24182;&#19988;&#33021;&#22815;&#20248;&#20110;&#38543;&#26426;&#25277;&#26679;&#26041;&#27861;&#21644;&#20854;&#20182;AL&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#23545;&#35805;&#34892;&#20026;&#65288;DA&#65289;&#65292;&#21487;&#20197;&#35299;&#37322;&#19987;&#23478;&#23548;&#24072;&#22312;&#36741;&#23548;&#36807;&#31243;&#20013;&#20570;&#20102;&#20160;&#20040;&#20197;&#21450;&#23398;&#29983;&#30693;&#36947;&#20160;&#20040;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23454;&#35777;&#30740;&#31350;&#22810;&#37319;&#29992;&#38543;&#26426;&#25277;&#26679;&#27861;&#26469;&#33719;&#24471;&#25163;&#21160;&#27880;&#37322;DA&#30340;&#21477;&#23376;&#26679;&#26412;&#65292;&#28982;&#21518;&#29992;&#20110;&#22521;&#35757;DA&#20998;&#31867;&#22120;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#65288;AL&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#36873;&#25321;&#25945;&#32946;&#23545;&#35805;&#34892;&#20026;&#20998;&#31867;&#30340;&#20449;&#24687;&#26679;&#26412;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20182;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#38543;&#26426;&#25277;&#26679;&#26041;&#27861;&#21644;&#20854;&#20182;&#26368;&#26032;&#30340;AL&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dialogue Acts (DAs) can be used to explain what expert tutors do and what students know during the tutoring process. Most empirical studies adopt the random sampling method to obtain sentence samples for manual annotation of DAs, which are then used to train DA classifiers. However, these studies have paid little attention to sample informativeness, which can reflect the information quantity of the selected samples and inform the extent to which a classifier can learn patterns. Notably, the informativeness level may vary among the samples and the classifier might only need a small amount of low informative samples to learn the patterns. Random sampling may overlook sample informativeness, which consumes human labelling costs and contributes less to training the classifiers. As an alternative, researchers suggest employing statistical sampling methods of Active Learning (AL) to identify the informative samples for training the classifiers. However, the use of AL methods in educational D
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;&#20998;&#31867;&#25968;&#25454;&#25366;&#25496;&#25216;&#26415;&#21644;&#20915;&#31574;&#26641;&#31639;&#27861;&#65292;&#25552;&#20986;&#19968;&#20010;&#39044;&#27979;&#27169;&#22411;&#65292;&#21487;&#20197;&#39044;&#27979;&#23398;&#29983;&#22312;&#23398;&#26399;&#21021;&#38454;&#27573;&#36890;&#36807;&#25152;&#36873;&#35838;&#31243;&#30340;&#27010;&#29575;&#65292;&#35813;&#27169;&#22411;&#21487;&#38752;&#12289;&#20934;&#30830;&#12289;&#21487;&#25512;&#33616;&#12290;</title><link>http://arxiv.org/abs/2304.05565</link><description>&lt;p&gt;
&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#39044;&#27979;&#27169;&#22411;&#26469;&#30830;&#23450;&#23398;&#29983;&#36890;&#36807;&#23398;&#26399;&#35838;&#31243;&#30340;&#27010;&#29575;
&lt;/p&gt;
&lt;p&gt;
A Predictive Model using Machine Learning Algorithm in Identifying Students Probability on Passing Semestral Course. (arXiv:2304.05565v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05565
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;&#20998;&#31867;&#25968;&#25454;&#25366;&#25496;&#25216;&#26415;&#21644;&#20915;&#31574;&#26641;&#31639;&#27861;&#65292;&#25552;&#20986;&#19968;&#20010;&#39044;&#27979;&#27169;&#22411;&#65292;&#21487;&#20197;&#39044;&#27979;&#23398;&#29983;&#22312;&#23398;&#26399;&#21021;&#38454;&#27573;&#36890;&#36807;&#25152;&#36873;&#35838;&#31243;&#30340;&#27010;&#29575;&#65292;&#35813;&#27169;&#22411;&#21487;&#38752;&#12289;&#20934;&#30830;&#12289;&#21487;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#30830;&#23450;&#19968;&#20010;&#39044;&#27979;&#27169;&#22411;&#65292;&#20197;&#22312;&#23398;&#26399;&#21021;&#38454;&#27573;&#23398;&#29983;&#36890;&#36807;&#25152;&#36873;&#35838;&#31243;&#30340;&#27010;&#29575;&#12290;&#35813;&#30740;&#31350;&#37319;&#29992;&#20998;&#31867;&#25968;&#25454;&#25366;&#25496;&#25216;&#26415;&#21644;&#20915;&#31574;&#26641;&#31639;&#27861;&#12290;&#21033;&#29992;&#26032;&#21457;&#29616;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#23398;&#29983;&#36890;&#36807;&#24403;&#21069;&#35838;&#31243;&#30340;&#39044;&#27979;&#20934;&#30830;&#29575;&#20026;0.7619&#65292;&#31934;&#24230;&#20026;0.8333&#65292;&#21484;&#22238;&#29575;&#20026;0.8823&#65292;f1&#20998;&#25968;&#20026;0.8571&#65292;&#34920;&#26126;&#39044;&#27979;&#27169;&#22411;&#26159;&#21487;&#38752;&#12289;&#20934;&#30830;&#21644;&#21487;&#25512;&#33616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study aims to determine a predictive model to learn students probability to pass their courses taken at the earliest stage of the semester. To successfully discover a good predictive model with high acceptability, accurate, and precision rate which delivers a useful outcome for decision making in education systems, in improving the processes of conveying knowledge and uplifting students academic performance, the proponent applies and strictly followed the CRISP-DM (Cross-Industry Standard Process for Data Mining) methodology. This study employs classification for data mining techniques, and decision tree for algorithm. With the utilization of the newly discovered predictive model, the prediction of students probabilities to pass the current courses they take gives 0.7619 accuracy, 0.8333 precision, 0.8823 recall, and 0.8571 f1 score, which shows that the model used in the prediction is reliable, accurate, and recommendable. Considering the indicators and the results, it can be not
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#23545;&#25239;&#24615;&#32972;&#26223;&#19979;&#65292;&#26080;&#27861;&#30452;&#25509;&#35775;&#38382;&#21407;&#22987;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#30340;&#29983;&#29289;&#29305;&#24449;&#30340;&#21453;&#28436;&#65292;&#35777;&#26126;&#22312;&#19968;&#23450;&#31243;&#24230;&#30340;&#25915;&#20987;&#32972;&#26223;&#30693;&#35782;&#19979;&#65292;&#40657;&#30418;&#27169;&#22411;&#29983;&#29289;&#29305;&#24449;&#34920;&#31034;&#20173;&#28982;&#24456;&#23481;&#26131;&#34987;&#21453;&#28436;&#12290;</title><link>http://arxiv.org/abs/2304.05561</link><description>&lt;p&gt;
&#20851;&#20110;&#28145;&#24230;&#29983;&#29289;&#29305;&#24449;&#34920;&#31034;&#30340;&#23545;&#25239;&#24615;&#21453;&#28436;
&lt;/p&gt;
&lt;p&gt;
On the Adversarial Inversion of Deep Biometric Representations. (arXiv:2304.05561v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05561
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#23545;&#25239;&#24615;&#32972;&#26223;&#19979;&#65292;&#26080;&#27861;&#30452;&#25509;&#35775;&#38382;&#21407;&#22987;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#30340;&#29983;&#29289;&#29305;&#24449;&#30340;&#21453;&#28436;&#65292;&#35777;&#26126;&#22312;&#19968;&#23450;&#31243;&#24230;&#30340;&#25915;&#20987;&#32972;&#26223;&#30693;&#35782;&#19979;&#65292;&#40657;&#30418;&#27169;&#22411;&#29983;&#29289;&#29305;&#24449;&#34920;&#31034;&#20173;&#28982;&#24456;&#23481;&#26131;&#34987;&#21453;&#28436;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#35748;&#35777;&#26381;&#21153;&#25552;&#20379;&#21830;&#36890;&#24120;&#22768;&#31216;&#65292;&#26080;&#27861;&#20174;&#20854;&#25968;&#23398;&#65288;&#29305;&#24449;&#31354;&#38388;&#65289;&#34920;&#31034;&#25552;&#21462;&#20986;&#29992;&#25143;&#30340;&#21407;&#22987;&#29983;&#29289;&#29305;&#24449;&#26679;&#26412;&#65292;&#20363;&#22914;&#25351;&#32441;&#25110;&#38754;&#37096;&#22270;&#20687;&#12290;&#26412;&#25991;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#23884;&#20837;&#30340;&#29305;&#23450;&#31034;&#20363;&#19978;&#30740;&#31350;&#20102;&#36825;&#19968;&#20027;&#24352;&#12290;&#29616;&#26377;&#30740;&#31350;&#21033;&#29992;&#20102;&#21407;&#22987;&#27169;&#22411;&#30340;&#25152;&#26377;&#23618;&#20197;&#21450;&#25152;&#26377;&#21487;&#33021;&#30340;&#25968;&#25454;&#38598;&#20449;&#24687;&#65292;&#30740;&#31350;&#21453;&#28436;DNN&#23884;&#20837;&#20197;&#35299;&#37322;&#28145;&#24230;&#22270;&#20687;&#34920;&#31034;&#25110;&#21512;&#25104;&#26631;&#20934;&#21270;&#22270;&#20687;&#12290;&#23545;&#20110;&#29983;&#29289;&#35748;&#35777;&#29992;&#20363;&#65292;&#25105;&#20204;&#38656;&#35201;&#22312;&#23545;&#25239;&#24615;&#35774;&#32622;&#19979;&#36827;&#34892;&#30740;&#31350;&#65292;&#20854;&#20013;&#25915;&#20987;&#32773;&#21487;&#20197;&#35775;&#38382;&#29305;&#24449;&#31354;&#38388;&#34920;&#31034;&#65292;&#20294;&#26080;&#27861;&#30452;&#25509;&#35775;&#38382;&#30830;&#20999;&#30340;&#21407;&#22987;&#25968;&#25454;&#38598;&#25110;&#21407;&#22987;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biometric authentication service providers often claim that it is not possible to reverse-engineer a user's raw biometric sample, such as a fingerprint or a face image, from its mathematical (feature-space) representation. In this paper, we investigate this claim on the specific example of deep neural network (DNN) embeddings. Inversion of DNN embeddings has been investigated for explaining deep image representations or synthesizing normalized images. Existing studies leverage full access to all layers of the original model, as well as all possible information on the original dataset. For the biometric authentication use case, we need to investigate this under adversarial settings where an attacker has access to a feature-space representation but no direct access to the exact original dataset nor the original learned model. Instead, we assume varying degree of attacker's background knowledge about the distribution of the dataset as well as the original learned model (architecture and t
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20998;&#31867;&#23398;&#31867;&#21035;&#26641;&#30340;&#23398;&#20064;&#20219;&#21153;&#35774;&#32622;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;&#20998;&#31867;&#23398;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#65288;TCIL&#65289;&#65292;&#20351;&#29992;&#21442;&#25968;&#32487;&#25215;&#26041;&#26696;&#23454;&#29616;&#23545;&#31062;&#20808;&#31867;&#21035;&#30693;&#35782;&#23545;&#21518;&#20195;&#31867;&#21035;&#30340;&#22686;&#37327;&#36716;&#31227;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;SOTA&#26041;&#27861;&#65292;&#22312;CIFAR-100&#21644;ImageNet-100&#19978;&#65292;&#23454;&#29616;&#20102;2&#65285;&#21644;3&#65285;&#30340;&#20934;&#30830;&#29575;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2304.05547</link><description>&lt;p&gt;
&#20998;&#31867;&#23398;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Taxonomic Class Incremental Learning. (arXiv:2304.05547v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05547
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20998;&#31867;&#23398;&#31867;&#21035;&#26641;&#30340;&#23398;&#20064;&#20219;&#21153;&#35774;&#32622;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;&#20998;&#31867;&#23398;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#65288;TCIL&#65289;&#65292;&#20351;&#29992;&#21442;&#25968;&#32487;&#25215;&#26041;&#26696;&#23454;&#29616;&#23545;&#31062;&#20808;&#31867;&#21035;&#30693;&#35782;&#23545;&#21518;&#20195;&#31867;&#21035;&#30340;&#22686;&#37327;&#36716;&#31227;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;SOTA&#26041;&#27861;&#65292;&#22312;CIFAR-100&#21644;ImageNet-100&#19978;&#65292;&#23454;&#29616;&#20102;2&#65285;&#21644;3&#65285;&#30340;&#20934;&#30830;&#29575;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36830;&#32493;&#23398;&#20064;&#38382;&#39064;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#20154;&#36136;&#30097;&#22522;&#20110;&#38543;&#26426;&#31867;&#20219;&#21153;&#35838;&#31243;&#30340;&#24120;&#29992;&#23398;&#20064;&#35774;&#32622;&#12290;&#36825;&#19982;&#20154;&#31867;&#36830;&#32493;&#23398;&#20064;&#26377;&#24456;&#22823;&#21306;&#21035;&#65292;&#21518;&#32773;&#26159;&#30001;&#20998;&#31867;&#23398;&#35838;&#31243;&#24341;&#23548;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20998;&#31867;&#23398;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#65288;TCIL&#65289;&#38382;&#39064;&#12290;&#22312;TCIL&#20013;&#65292;&#20219;&#21153;&#24207;&#21015;&#22522;&#20110;&#20998;&#31867;&#23398;&#31867;&#21035;&#26641;&#36827;&#34892;&#32452;&#32455;&#12290;&#25105;&#20204;&#23558;&#29616;&#26377;&#30340;CIL&#21644;&#20998;&#31867;&#23398;&#20064;&#26041;&#27861;&#32479;&#19968;&#20026;&#21442;&#25968;&#32487;&#25215;&#26041;&#26696;&#65292;&#24182;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;TCIL&#23398;&#20064;&#26041;&#26696;&#12290;&#36825;&#20351;&#24471;&#20174;&#31867;&#21035;&#20998;&#31867;&#23398;&#30340;&#31062;&#20808;&#36716;&#31227;&#21040;&#23376;&#23385;&#31867;&#21035;&#30340;&#30693;&#35782;&#30340;&#22686;&#37327;&#36716;&#31227;&#25104;&#20026;&#21487;&#33021;&#12290;&#22312;CIFAR-100&#21644;ImageNet-100&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;TCIL&#26041;&#27861;&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#65292;&#20854;&#26368;&#32456;&#20934;&#30830;&#29575;&#22312;CIFAR-100&#19978;&#27604;&#29616;&#26377;SOTA&#26041;&#27861;&#39640;&#20986;2&#65285;&#65292;&#22312;ImageNet-100&#19978;&#27604;&#29616;&#26377;SOTA&#26041;&#27861;&#39640;&#20986;3&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of continual learning has attracted rising attention in recent years. However, few works have questioned the commonly used learning setup, based on a task curriculum of random class. This differs significantly from human continual learning, which is guided by taxonomic curricula. In this work, we propose the Taxonomic Class Incremental Learning (TCIL) problem. In TCIL, the task sequence is organized based on a taxonomic class tree. We unify existing approaches to CIL and taxonomic learning as parameter inheritance schemes and introduce a new such scheme for the TCIL learning. This enables the incremental transfer of knowledge from ancestor to descendant class of a class taxonomy through parameter inheritance. Experiments on CIFAR-100 and ImageNet-100 show the effectiveness of the proposed TCIL method, which outperforms existing SOTA methods by 2% in terms of final accuracy on CIFAR-100 and 3% on ImageNet-100.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;MEMA&#26694;&#26550;&#65292;&#21487;&#20197;&#24555;&#36895;&#19988;&#23481;&#26131;&#22320;&#25512;&#23548;&#20986;&#22312;&#24494;&#25511;&#21046;&#22120;&#19978;&#23454;&#29616;TinyML&#26102;&#26368;&#23567;&#21270;&#22806;&#37096;&#20869;&#23384;&#35775;&#38382;&#30340;&#26377;&#25928;&#36816;&#34892;&#26102;&#65292;&#36825;&#21487;&#20197;&#20351;&#31070;&#32463;&#32593;&#32476;&#22522;&#20934;&#27979;&#35797;&#22312;ARM Cortex-M4&#19978;&#21152;&#36895;&#39640;&#36798;1.8&#20493;&#21644;&#20943;&#23569;44&#65285;&#30340;&#33021;&#37327;&#28040;&#32791;&#12290;</title><link>http://arxiv.org/abs/2304.05544</link><description>&lt;p&gt;
MEMA&#36816;&#34892;&#26102;&#26694;&#26550;&#65306;&#22312;&#24494;&#25511;&#21046;&#22120;&#19978;&#23454;&#29616;TinyML&#26102;&#26368;&#23567;&#21270;&#22806;&#37096;&#20869;&#23384;&#35775;&#38382;
&lt;/p&gt;
&lt;p&gt;
MEMA Runtime Framework: Minimizing External Memory Accesses for TinyML on Microcontrollers. (arXiv:2304.05544v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05544
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;MEMA&#26694;&#26550;&#65292;&#21487;&#20197;&#24555;&#36895;&#19988;&#23481;&#26131;&#22320;&#25512;&#23548;&#20986;&#22312;&#24494;&#25511;&#21046;&#22120;&#19978;&#23454;&#29616;TinyML&#26102;&#26368;&#23567;&#21270;&#22806;&#37096;&#20869;&#23384;&#35775;&#38382;&#30340;&#26377;&#25928;&#36816;&#34892;&#26102;&#65292;&#36825;&#21487;&#20197;&#20351;&#31070;&#32463;&#32593;&#32476;&#22522;&#20934;&#27979;&#35797;&#22312;ARM Cortex-M4&#19978;&#21152;&#36895;&#39640;&#36798;1.8&#20493;&#21644;&#20943;&#23569;44&#65285;&#30340;&#33021;&#37327;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;MEMA&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;TinyML&#31995;&#32479;&#19978;&#30697;&#38453;&#20056;&#27861;&#30340;&#25512;&#26029;&#36816;&#34892;&#26102;&#24555;&#36895;&#19988;&#23481;&#26131;&#22320;&#25512;&#23548;&#20986;&#26368;&#23567;&#21270;&#22806;&#37096;&#20869;&#23384;&#35775;&#38382;&#30340;&#26377;&#25928;&#36816;&#34892;&#26102;&#12290;&#35813;&#26694;&#26550;&#32771;&#34385;&#30828;&#20214;&#36164;&#28304;&#38480;&#21046;&#21644;&#38382;&#39064;&#22823;&#23567;&#65292;&#36890;&#36807;&#20998;&#26512;&#30830;&#23450;&#26368;&#20248;&#21270;&#30340;&#35843;&#24230;&#21644;&#20869;&#26680;&#65292;&#20197;&#26368;&#23567;&#21270;&#20869;&#23384;&#35775;&#38382;&#12290;MEMA&#25552;&#20379;&#20102;&#24403;&#21069;&#23454;&#36341;&#20013;&#24050;&#30693;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#20165;&#36890;&#36807;&#32791;&#26102;&#19988;&#21551;&#21457;&#24335;&#30340;&#25628;&#32034;&#22823;&#37327;&#35843;&#24230;&#31354;&#38388;&#25165;&#33021;&#25214;&#21040;&#26368;&#20248;&#35843;&#24230;&#12290;&#25105;&#20204;&#23558;MEMA&#25512;&#23548;&#20986;&#30340;&#36816;&#34892;&#26102;&#24615;&#33021;&#19982;&#22522;&#20110;ARM&#30340;TinyML&#31995;&#32479;&#19978;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#24211;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#20363;&#22914;&#22312;ARM Cortex-M4&#19978;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#27604;CMSIS-NN&#26356;&#39640;&#36798;1.8&#20493;&#30340;&#21152;&#36895;&#21644;44&#65285;&#30340;&#33021;&#37327;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present the MEMA framework for the easy and quick derivation of efficient inference runtimes that minimize external memory accesses for matrix multiplication on TinyML systems. The framework accounts for hardware resource constraints and problem sizes in analytically determining optimized schedules and kernels that minimize memory accesses. MEMA provides a solution to a well-known problem in the current practice, that is, optimal schedules tend to be found only through a time consuming and heuristic search of a large scheduling space. We compare the performance of runtimes derived from MEMA to existing state-of-the-art libraries on ARM-based TinyML systems. For example, for neural network benchmarks on the ARM Cortex-M4, we achieve up to a 1.8x speedup and 44% energy reduction over CMSIS-NN.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#21644;&#33258;&#27880;&#24847;&#21147;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;CLCLSA&#65292;&#21487;&#29992;&#20110;&#19981;&#23436;&#25972;&#22810;&#32452;&#23398;&#25968;&#25454;&#30340;&#19968;&#20307;&#21270;&#65292;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.05542</link><description>&lt;p&gt;
CLCLSA&#65306;&#20132;&#21449;&#32452;&#23398;&#38142;&#25509;&#30340;&#23545;&#27604;&#23398;&#20064;&#21644;&#33258;&#27880;&#24847;&#21147;&#22810;&#32452;&#23398;&#25968;&#25454;&#19981;&#23436;&#25972;&#24773;&#20917;&#19979;&#19968;&#20307;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CLCLSA: Cross-omics Linked embedding with Contrastive Learning and Self Attention for multi-omics integration with incomplete multi-omics data. (arXiv:2304.05542v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05542
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#21644;&#33258;&#27880;&#24847;&#21147;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;CLCLSA&#65292;&#21487;&#29992;&#20110;&#19981;&#23436;&#25972;&#22810;&#32452;&#23398;&#25968;&#25454;&#30340;&#19968;&#20307;&#21270;&#65292;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#32500;&#24322;&#36136;&#22810;&#32452;&#23398;&#25968;&#25454;&#30340;&#25972;&#21512;&#23545;&#20110;&#29702;&#35299;&#36951;&#20256;&#25968;&#25454;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22810;&#32452;&#23398;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#26410;&#37197;&#23545;&#30340;&#25968;&#25454;&#65292;&#36825;&#26159;&#30001;&#20110;&#20202;&#22120;&#28789;&#25935;&#24230;&#21644;&#25104;&#26412;&#25152;&#23548;&#33268;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#21644;&#33258;&#27880;&#24847;&#21147;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;CLCLSA&#65292;&#21487;&#29992;&#20110;&#19981;&#23436;&#25972;&#22810;&#32452;&#23398;&#25968;&#25454;&#30340;&#19968;&#20307;&#21270;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#23436;&#25972;&#25968;&#25454;&#20316;&#20026;&#30417;&#30563;&#65292;&#37319;&#29992;&#20132;&#21449;&#32452;&#23398;&#33258;&#32534;&#30721;&#22120;&#36328;&#36234;&#19981;&#21516;&#31867;&#22411;&#30340;&#29983;&#29289;&#25968;&#25454;&#23398;&#20064;&#29305;&#24449;&#34920;&#31034;&#65292;&#28982;&#21518;&#24341;&#20837;&#22810;&#32452;&#23398;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#65292;&#32852;&#21512;&#23398;&#20064;&#24050;&#37197;&#23545;&#21644;&#26410;&#37197;&#23545;&#25968;&#25454;&#30340;&#20844;&#20849;&#34920;&#31034;&#65292;&#26377;&#25928;&#22320;&#38598;&#25104;&#19981;&#23436;&#25972;&#25968;&#25454;&#12290;CLCLSA&#22312;&#22810;&#20010;&#22522;&#20934;&#22810;&#32452;&#23398;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Integration of heterogeneous and high-dimensional multi-omics data is becoming increasingly important in understanding genetic data. Each omics technique only provides a limited view of the underlying biological process and integrating heterogeneous omics layers simultaneously would lead to a more comprehensive and detailed understanding of diseases and phenotypes. However, one obstacle faced when performing multi-omics data integration is the existence of unpaired multi-omics data due to instrument sensitivity and cost. Studies may fail if certain aspects of the subjects are missing or incomplete. In this paper, we propose a deep learning method for multi-omics integration with incomplete data by Cross-omics Linked unified embedding with Contrastive Learning and Self Attention (CLCLSA). Utilizing complete multi-omics data as supervision, the model employs cross-omics autoencoders to learn the feature representation across different types of biological data. The multi-omics contrastive
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#8220;&#30830;&#23450;&#24615;ADVI&#8221;&#65288;DADVI&#65289;&#65292;&#23427;&#29992;&#19968;&#31181;&#22266;&#23450;&#30340;&#33945;&#29305;&#21345;&#32599;&#36817;&#20284;&#26367;&#25442;&#20102;&#22343;&#20540;&#22330;&#21464;&#20998;&#36125;&#21494;&#26031;&#65288;MFVB&#65289;&#30340;&#19981;&#21487;&#35299;&#30446;&#26631;&#65292;&#21487;&#20197;&#20351;&#29992;&#29616;&#25104;&#30340;&#20108;&#38454;&#20248;&#21270;&#65292;&#36866;&#29992;&#20110;&#26356;&#20934;&#30830;&#30340;&#21518;&#39564;&#32447;&#24615;&#21709;&#24212;&#65288;LR&#65289;&#21327;&#26041;&#24046;&#20272;&#35745;&#65292;&#22312;&#26576;&#20123;&#24120;&#35265;&#30340;&#32479;&#35745;&#38382;&#39064;&#31867;&#21035;&#19978;&#25928;&#26524;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2304.05527</link><description>&lt;p&gt;
&#19968;&#31181;&#20351;&#29992;&#30830;&#23450;&#24615;&#30446;&#26631;&#30340;&#40657;&#21283;&#23376;&#21464;&#20998;&#25512;&#26029;&#65306;&#26356;&#24555;&#65292;&#26356;&#31934;&#30830;&#65292;&#26356;&#40657;&#12290;
&lt;/p&gt;
&lt;p&gt;
Black Box Variational Inference with a Deterministic Objective: Faster, More Accurate, and Even More Black Box. (arXiv:2304.05527v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05527
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#8220;&#30830;&#23450;&#24615;ADVI&#8221;&#65288;DADVI&#65289;&#65292;&#23427;&#29992;&#19968;&#31181;&#22266;&#23450;&#30340;&#33945;&#29305;&#21345;&#32599;&#36817;&#20284;&#26367;&#25442;&#20102;&#22343;&#20540;&#22330;&#21464;&#20998;&#36125;&#21494;&#26031;&#65288;MFVB&#65289;&#30340;&#19981;&#21487;&#35299;&#30446;&#26631;&#65292;&#21487;&#20197;&#20351;&#29992;&#29616;&#25104;&#30340;&#20108;&#38454;&#20248;&#21270;&#65292;&#36866;&#29992;&#20110;&#26356;&#20934;&#30830;&#30340;&#21518;&#39564;&#32447;&#24615;&#21709;&#24212;&#65288;LR&#65289;&#21327;&#26041;&#24046;&#20272;&#35745;&#65292;&#22312;&#26576;&#20123;&#24120;&#35265;&#30340;&#32479;&#35745;&#38382;&#39064;&#31867;&#21035;&#19978;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#24494;&#20998;&#21464;&#20998;&#25512;&#26029;&#65288;ADVI&#65289;&#25552;&#20379;&#20102;&#22810;&#31181;&#29616;&#20195;&#27010;&#29575;&#32534;&#31243;&#35821;&#35328;&#20013;&#24555;&#36895;&#26131;&#29992;&#30340;&#21518;&#39564;&#36817;&#20284;&#26041;&#27861;&#12290;&#28982;&#32780;&#23427;&#30340;&#38543;&#26426;&#20248;&#21270;&#22120;&#32570;&#20047;&#26126;&#30830;&#30340;&#25910;&#25947;&#26631;&#20934;&#65292;&#24182;&#19988;&#38656;&#35201;&#35843;&#25972;&#21442;&#25968;&#12290;&#27492;&#22806;&#65292;ADVI&#32487;&#25215;&#20102;&#22343;&#20540;&#22330;&#21464;&#20998;&#36125;&#21494;&#26031;&#65288;MFVB&#65289;&#30340;&#36739;&#24046;&#21518;&#39564;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;&#30830;&#23450;&#24615;ADVI&#8221;&#65288;DADVI&#65289;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;DADVI&#29992;&#22266;&#23450;&#30340;&#33945;&#29305;&#21345;&#32599;&#36817;&#20284;&#26367;&#25442;&#20102;MFVB&#30340;&#19981;&#21487;&#35299;&#30446;&#26631;&#65292;&#36825;&#19968;&#25216;&#26415;&#22312;&#38543;&#26426;&#20248;&#21270;&#25991;&#29486;&#20013;&#34987;&#31216;&#20026;&#8220;&#26679;&#26412;&#24179;&#22343;&#36817;&#20284;&#8221;&#65288;SAA&#65289;&#12290;&#36890;&#36807;&#20248;&#21270;&#36817;&#20284;&#20294;&#30830;&#23450;&#30340;&#30446;&#26631;&#65292;DADVI&#21487;&#20197;&#20351;&#29992;&#29616;&#25104;&#30340;&#20108;&#38454;&#20248;&#21270;&#65292;&#32780;&#19988;&#19982;&#26631;&#20934;&#22343;&#20540;&#22330;ADVI&#19981;&#21516;&#30340;&#26159;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#26356;&#20934;&#30830;&#30340;&#21518;&#39564;&#32447;&#24615;&#21709;&#24212;&#65288;LR&#65289;&#21327;&#26041;&#24046;&#20272;&#35745;&#12290;&#19982;&#29616;&#26377;&#30340;&#26368;&#22351;&#24773;&#20917;&#29702;&#35770;&#30456;&#21453;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#26576;&#20123;&#24120;&#35265;&#30340;&#32479;&#35745;&#38382;&#39064;&#31867;&#21035;&#19978;&#65292;DADVI&#21644;SAA&#21487;&#20197;&#34920;&#29616;&#24471;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic differentiation variational inference (ADVI) offers fast and easy-to-use posterior approximation in multiple modern probabilistic programming languages. However, its stochastic optimizer lacks clear convergence criteria and requires tuning parameters. Moreover, ADVI inherits the poor posterior uncertainty estimates of mean-field variational Bayes (MFVB). We introduce ``deterministic ADVI'' (DADVI) to address these issues. DADVI replaces the intractable MFVB objective with a fixed Monte Carlo approximation, a technique known in the stochastic optimization literature as the ``sample average approximation'' (SAA). By optimizing an approximate but deterministic objective, DADVI can use off-the-shelf second-order optimization, and, unlike standard mean-field ADVI, is amenable to more accurate posterior linear response (LR) covariance estimates. In contrast to existing worst-case theory, we show that, on certain classes of common statistical problems, DADVI and the SAA can perform 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22238;&#31572;&#22240;&#26524;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#23427;&#20204;&#21487;&#20197;&#22238;&#31572;&#22522;&#20110;&#24050;&#26377;&#22240;&#26524;&#30693;&#35782;&#30340;&#38382;&#39064;&#65292;&#20294;&#23545;&#20110;&#21457;&#29616;&#26032;&#30693;&#35782;&#25110;&#39640;&#39118;&#38505;&#20915;&#31574;&#20219;&#21153;&#30340;&#39640;&#31934;&#24230;&#35201;&#27714;&#19981;&#36275;&#12290;&#26410;&#26469;&#21487;&#20197;&#36890;&#36807;&#21551;&#29992;&#26174;&#24335;&#21644;&#38544;&#24335;&#30340;&#22240;&#26524;&#27169;&#22359;&#20197;&#21450;&#28145;&#24230;&#22240;&#26524;&#24863;&#30693;&#30340;LLMs&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.05524</link><description>&lt;p&gt;
&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#22240;&#26524;&#20851;&#31995;: &#21487;&#34892;&#24615;&#21644;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
Understanding Causality with Large Language Models: Feasibility and Opportunities. (arXiv:2304.05524v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05524
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22238;&#31572;&#22240;&#26524;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#23427;&#20204;&#21487;&#20197;&#22238;&#31572;&#22522;&#20110;&#24050;&#26377;&#22240;&#26524;&#30693;&#35782;&#30340;&#38382;&#39064;&#65292;&#20294;&#23545;&#20110;&#21457;&#29616;&#26032;&#30693;&#35782;&#25110;&#39640;&#39118;&#38505;&#20915;&#31574;&#20219;&#21153;&#30340;&#39640;&#31934;&#24230;&#35201;&#27714;&#19981;&#36275;&#12290;&#26410;&#26469;&#21487;&#20197;&#36890;&#36807;&#21551;&#29992;&#26174;&#24335;&#21644;&#38544;&#24335;&#30340;&#22240;&#26524;&#27169;&#22359;&#20197;&#21450;&#28145;&#24230;&#22240;&#26524;&#24863;&#30693;&#30340;LLMs&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#22238;&#31572;&#19977;&#31181;&#31867;&#22411;&#22240;&#26524;&#38382;&#39064;&#26102;&#30340;&#20248;&#32570;&#28857;&#65292;&#35780;&#20272;&#20102;&#23427;&#20204;&#22238;&#31572;&#22240;&#26524;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#24403;&#21069;&#30340;LLMs&#21487;&#20197;&#20687;&#39046;&#22495;&#19987;&#23478;&#19968;&#26679;&#22238;&#31572;&#22522;&#20110;&#24050;&#26377;&#22240;&#26524;&#30693;&#35782;&#30340;&#22240;&#26524;&#38382;&#39064;&#12290;&#20294;&#26159;&#65292;&#23427;&#20204;&#36824;&#19981;&#33021;&#20026;&#21457;&#29616;&#26032;&#30693;&#35782;&#25110;&#39640;&#31934;&#24230;&#39640;&#39118;&#38505;&#20915;&#31574;&#20219;&#21153;&#25552;&#20379;&#20196;&#20154;&#28385;&#24847;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#26410;&#26469;&#21487;&#33021;&#30340;&#26041;&#21521;&#21644;&#26426;&#36935;&#65292;&#20363;&#22914;&#21551;&#29992;&#26174;&#24335;&#21644;&#38544;&#24335;&#30340;&#22240;&#26524;&#27169;&#22359;&#20197;&#21450;&#28145;&#24230;&#22240;&#26524;&#24863;&#30693;&#30340;LLMs&#12290;&#36825;&#20123;&#19981;&#20165;&#21487;&#20197;&#20351;LLMs&#22238;&#31572;&#26356;&#22810;&#31867;&#22411;&#30340;&#22240;&#26524;&#38382;&#39064;&#20197;&#23454;&#29616;&#26356;&#22823;&#30340;&#24433;&#21709;&#21147;&#65292;&#36824;&#21487;&#20197;&#20351;LLMs&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#26356;&#21152;&#20540;&#24471;&#20449;&#20219;&#21644;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
We assess the ability of large language models (LLMs) to answer causal questions by analyzing their strengths and weaknesses against three types of causal question. We believe that current LLMs can answer causal questions with existing causal knowledge as combined domain experts. However, they are not yet able to provide satisfactory answers for discovering new knowledge or for high-stakes decision-making tasks with high precision. We discuss possible future directions and opportunities, such as enabling explicit and implicit causal modules as well as deep causal-aware LLMs. These will not only enable LLMs to answer many different types of causal questions for greater impact but also enable LLMs to be more trustworthy and efficient in general.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20010;&#24615;&#21270;&#38544;&#31169;&#20445;&#25252;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#22312;&#20445;&#35777;&#26412;&#22320;&#38544;&#31169;&#30340;&#21516;&#26102;&#23454;&#29616;&#24378;&#20013;&#24515;&#38544;&#31169;&#20445;&#35777;&#65292;&#24182;&#21033;&#29992;Shuffle&#27169;&#22411;&#36827;&#34892;&#38544;&#31169;&#25193;&#22686;&#12290;</title><link>http://arxiv.org/abs/2304.05516</link><description>&lt;p&gt;
&#37051;&#23621;&#30340;&#22238;&#21709;&#65306;&#22522;&#20110;Shuffle&#27169;&#22411;&#30340;&#20010;&#24615;&#21270;&#38544;&#31169;&#20445;&#25252;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#38544;&#31169;&#25193;&#22686;
&lt;/p&gt;
&lt;p&gt;
Echo of Neighbors: Privacy Amplification for Personalized Private Federated Learning with Shuffle Model. (arXiv:2304.05516v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20010;&#24615;&#21270;&#38544;&#31169;&#20445;&#25252;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#22312;&#20445;&#35777;&#26412;&#22320;&#38544;&#31169;&#30340;&#21516;&#26102;&#23454;&#29616;&#24378;&#20013;&#24515;&#38544;&#31169;&#20445;&#35777;&#65292;&#24182;&#21033;&#29992;Shuffle&#27169;&#22411;&#36827;&#34892;&#38544;&#31169;&#25193;&#22686;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#21327;&#21516;&#35757;&#32451;&#33539;&#20363;&#65292;&#20294;&#20250;&#21463;&#21040;&#38544;&#31169;&#25915;&#20987;&#12290;&#20026;&#20102;&#28385;&#36275;&#29992;&#25143;&#23545;&#20110;&#19981;&#21516;&#38544;&#31169;&#38656;&#27714;&#30340;&#26412;&#22320;&#38656;&#27714;&#65292;&#38656;&#35201;&#20445;&#30041;&#20010;&#24615;&#21270;&#30340;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#65292;&#21516;&#26102;&#36824;&#38656;&#35201;&#20026;&#20840;&#23616;&#27169;&#22411;&#25552;&#20379;&#20005;&#26684;&#30340;&#38544;&#31169;&#20445;&#35777;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65288;APES&#65289;&#26469;&#21152;&#24378;&#20010;&#24615;&#21270;&#26412;&#22320;&#38544;&#31169;&#20445;&#25252;&#26465;&#20214;&#19979;&#30340;&#27169;&#22411;&#38544;&#31169;&#65292;&#21033;&#29992;Shuffle&#27169;&#22411;&#30340;&#38544;&#31169;&#25193;&#22686;&#25928;&#26524;&#12290;&#20026;&#20102;&#22686;&#24378;&#38544;&#31169;&#20445;&#35777;&#65292;&#25105;&#20204;&#37327;&#21270;&#27599;&#20010;&#29992;&#25143;&#23545;&#20013;&#24515;&#38544;&#31169;&#30340;&#24322;&#26500;&#36129;&#29486;&#65292;&#24182;&#36890;&#36807;&#25200;&#21160;&#8220;&#22238;&#22768;&#8221;&#26469;&#25551;&#36848;&#29992;&#25143;&#30340;&#29305;&#24449;&#12290;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning, as a popular paradigm for collaborative training, is vulnerable against privacy attacks. Different privacy levels regarding users' attitudes need to be satisfied locally, while a strict privacy guarantee for the global model is also required centrally. Personalized Local Differential Privacy (PLDP) is suitable for preserving users' varying local privacy, yet only provides a central privacy guarantee equivalent to the worst-case local privacy level. Thus, achieving strong central privacy as well as personalized local privacy with a utility-promising model is a challenging problem. In this work, a general framework (APES) is built up to strengthen model privacy under personalized local privacy by leveraging the privacy amplification effect of the shuffle model. To tighten the privacy bound, we quantify the heterogeneous contributions to the central privacy user by user. The contributions are characterized by the ability of generating "echos" from the perturbation of e
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31232;&#30095;&#24615;&#21644;&#25968;&#25454;&#27969;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#19981;&#20165;&#21487;&#20197;&#26377;&#25928;&#21033;&#29992;&#35745;&#31639;&#36164;&#28304;&#65292;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#65292;&#36824;&#33021;&#36798;&#21040;&#19982;&#31264;&#23494;&#27169;&#22411;&#31867;&#20284;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.05511</link><description>&lt;p&gt;
&#21033;&#29992;&#31232;&#30095;&#24615;&#21644;&#25968;&#25454;&#27969;&#39640;&#25928;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Training Large Language Models Efficiently with Sparsity and Dataflow. (arXiv:2304.05511v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05511
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31232;&#30095;&#24615;&#21644;&#25968;&#25454;&#27969;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#19981;&#20165;&#21487;&#20197;&#26377;&#25928;&#21033;&#29992;&#35745;&#31639;&#36164;&#28304;&#65292;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#65292;&#36824;&#33021;&#36798;&#21040;&#19982;&#31264;&#23494;&#27169;&#22411;&#31867;&#20284;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#22823;&#22411;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#21644;&#26426;&#22120;&#23398;&#20064;&#19987;&#23478;&#30340;&#19987;&#19994;&#30693;&#35782;, &#31232;&#30095;&#24615;&#25216;&#26415;&#21487;&#20197;&#32531;&#35299;&#35757;&#32451;&#20013;&#30340;&#35745;&#31639;&#35201;&#27714;&#65292;&#20294;&#31232;&#30095;&#24615;&#20250;&#24341;&#20837;&#26032;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31232;&#30095;&#24615;&#21644;&#25968;&#25454;&#27969;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#26041;&#27861;&#65292;&#22312;&#26377;&#25928;&#21033;&#29992;&#35745;&#31639;&#36164;&#28304;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;13&#20159;GPT&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#65292;&#19988;&#31232;&#30095;&#24615;&#25216;&#26415;&#21644;&#31264;&#23494;&#27169;&#22411;&#23545;&#27604;&#20855;&#26377;&#30456;&#24403;&#30340;&#32467;&#26524;&#21644;&#35745;&#31639;&#36164;&#28304;&#21644;&#35760;&#24518;&#35201;&#27714;&#30340;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large foundation language models have shown their versatility in being able to be adapted to perform a wide variety of downstream tasks, such as text generation, sentiment analysis, semantic search etc. However, training such large foundational models is a non-trivial exercise that requires a significant amount of compute power and expertise from machine learning and systems experts. As models get larger, these demands are only increasing. Sparsity is a promising technique to relieve the compute requirements for training. However, sparsity introduces new challenges in training the sparse model to the same quality as the dense counterparts. Furthermore, sparsity drops the operation intensity and introduces irregular memory access patterns that makes it challenging to efficiently utilize compute resources. This paper demonstrates an end-to-end training flow on a large language model - 13 billion GPT - using sparsity and dataflow. The dataflow execution model and architecture enables effi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#25511;&#21046;&#19981;&#21464;&#38598;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25511;&#21046;&#19981;&#21464;&#38598;&#30340;&#24212;&#29992;&#25552;&#39640;&#31283;&#23450;&#24615;&#20445;&#35777;&#21644;&#37319;&#26679;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.05509</link><description>&lt;p&gt;
&#25511;&#21046;&#19981;&#21464;&#38598;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#22312;&#36807;&#31243;&#25511;&#21046;&#26041;&#38754;&#30340;&#24212;&#29992;&#65306;&#25552;&#39640;&#37319;&#26679;&#25928;&#29575;&#24182;&#20445;&#35777;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Control invariant set enhanced reinforcement learning for process control: improved sampling efficiency and guaranteed stability. (arXiv:2304.05509v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05509
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#25511;&#21046;&#19981;&#21464;&#38598;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25511;&#21046;&#19981;&#21464;&#38598;&#30340;&#24212;&#29992;&#25552;&#39640;&#31283;&#23450;&#24615;&#20445;&#35777;&#21644;&#37319;&#26679;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#20010;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#23588;&#20854;&#26159;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#22240;&#20854;&#22788;&#29702;&#23454;&#38469;&#24212;&#29992;&#20013;&#20851;&#38190;&#30340;&#23433;&#20840;&#24615;&#32422;&#26463;&#30340;&#33021;&#21147;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#26041;&#27861;&#65292;&#31216;&#20026;&#25511;&#21046;&#19981;&#21464;&#38598;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#12290;&#23427;&#21033;&#29992;&#25511;&#21046;&#19981;&#21464;&#38598;&#30340;&#20248;&#28857;&#26469;&#25552;&#39640;&#31283;&#23450;&#24615;&#20445;&#35777;&#21644;&#37319;&#26679;&#25928;&#29575;&#12290;&#35813;&#26041;&#27861;&#20998;&#20026;&#31163;&#32447;&#38454;&#27573;&#21644;&#22312;&#32447;&#38454;&#27573;&#12290;&#31163;&#32447;&#38454;&#27573;&#23558;&#25511;&#21046;&#19981;&#21464;&#38598;&#32435;&#20837;&#22870;&#21169;&#35774;&#35745;&#12289;&#21021;&#22987;&#29366;&#24577;&#37319;&#26679;&#21644;&#29366;&#24577;&#37325;&#32622;&#31243;&#24207;&#20013;&#12290;&#22312;&#22312;&#32447;&#38454;&#27573;&#65292;&#24403;&#29366;&#24577;&#22312;&#25511;&#21046;&#19981;&#21464;&#38598;&#20043;&#22806;&#26102;&#65292;&#37325;&#26032;&#35757;&#32451;&#24378;&#21270;&#23398;&#20064;&#20197;&#28385;&#36275;&#31283;&#23450;&#24615;&#26631;&#20934;&#12290;&#21033;&#29992;&#25511;&#21046;&#19981;&#21464;&#38598;&#30340;&#26174;&#24335;&#24418;&#24335;&#24471;&#21040;&#20102;&#19968;&#20010;&#22791;&#20221;&#34920;&#26469;&#20445;&#35777;&#22312;&#32447;&#31283;&#23450;&#24615;&#12290;&#20026;&#20102;&#35780;&#20272;&#35813;&#26041;&#27861;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#27169;&#25311;&#21270;&#23398;&#21453;&#24212;&#22120;&#20013;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#31163;&#32447;&#35757;&#32451;&#20013;&#37319;&#26679;&#25928;&#29575;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) is an area of significant research interest, and safe RL in particular is attracting attention due to its ability to handle safety-driven constraints that are crucial for real-world applications of RL algorithms. This work proposes a novel approach to RL training, called control invariant set (CIS) enhanced RL, which leverages the benefits of CIS to improve stability guarantees and sampling efficiency. The approach consists of two learning stages: offline and online. In the offline stage, CIS is incorporated into the reward design, initial state sampling, and state reset procedures. In the online stage, RL is retrained whenever the state is outside of CIS, which serves as a stability criterion. A backup table that utilizes the explicit form of CIS is obtained to ensure the online stability. To evaluate the proposed approach, we apply it to a simulated chemical reactor. The results show a significant improvement in sampling efficiency during offline training 
&lt;/p&gt;</description></item><item><title>DistHD&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#30340;&#23398;&#20064;&#24863;&#30693;&#21160;&#24577;&#32534;&#30721;&#25216;&#26415;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#21644;&#37325;&#26500;&#24433;&#21709;&#23398;&#20064;&#36136;&#37327;&#30340;&#32500;&#24230;&#65292;&#20197;&#26174;&#33879;&#36739;&#20302;&#30340;&#32500;&#24230;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#21152;&#36895;&#23398;&#20064;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2304.05503</link><description>&lt;p&gt;
DistHD: &#19968;&#31181;&#36866;&#29992;&#20110;&#36229;&#39640;&#32500;&#20998;&#31867;&#30340;&#23398;&#20064;&#24863;&#30693;&#21160;&#24577;&#32534;&#30721;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DistHD: A Learner-Aware Dynamic Encoding Method for Hyperdimensional Classification. (arXiv:2304.05503v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05503
&lt;/p&gt;
&lt;p&gt;
DistHD&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#30340;&#23398;&#20064;&#24863;&#30693;&#21160;&#24577;&#32534;&#30721;&#25216;&#26415;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#21644;&#37325;&#26500;&#24433;&#21709;&#23398;&#20064;&#36136;&#37327;&#30340;&#32500;&#24230;&#65292;&#20197;&#26174;&#33879;&#36739;&#20302;&#30340;&#32500;&#24230;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#21152;&#36895;&#23398;&#20064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21551;&#21457;&#20110;&#20154;&#33041;&#30340;&#36229;&#39640;&#32500;&#35745;&#31639;&#25216;&#26415;(HDC)&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#30340;&#26377;&#21069;&#36884;&#30340;&#23398;&#20064;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#20351;&#29992;&#38745;&#24577;&#32534;&#30721;&#22120;&#65292;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#20174;&#19981;&#26356;&#26032;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#38750;&#24120;&#39640;&#30340;&#32500;&#24230;&#25165;&#33021;&#33719;&#24471;&#36275;&#22815;&#30340;&#20934;&#30830;&#24615;&#65292;&#20005;&#37325;&#38477;&#20302;&#32534;&#30721;&#21644;&#35757;&#32451;&#25928;&#29575;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;DistHD&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#21160;&#24577;&#32534;&#30721;&#25216;&#26415;&#65292;&#29992;&#20110;HDC&#33258;&#36866;&#24212;&#23398;&#20064;&#65292;&#33021;&#26377;&#25928;&#22320;&#35782;&#21035;&#21644;&#37325;&#26500;&#23545;&#20998;&#31867;&#20135;&#29983;&#35823;&#23548;&#24182;&#24433;&#21709;&#23398;&#20064;&#36136;&#37327;&#30340;&#32500;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;DistHD&#25104;&#21151;&#21152;&#36895;&#20102;&#23398;&#20064;&#36807;&#31243;&#65292;&#24182;&#20197;&#26174;&#33879;&#36739;&#20302;&#30340;&#32500;&#24230;&#36798;&#21040;&#20102;&#25152;&#26399;&#26395;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Brain-inspired hyperdimensional computing (HDC) has been recently considered a promising learning approach for resource-constrained devices. However, existing approaches use static encoders that are never updated during the learning process. Consequently, it requires a very high dimensionality to achieve adequate accuracy, severely lowering the encoding and training efficiency. In this paper, we propose DistHD, a novel dynamic encoding technique for HDC adaptive learning that effectively identifies and regenerates dimensions that mislead the classification and compromise the learning quality. Our proposed algorithm DistHD successfully accelerates the learning process and achieves the desired accuracy with considerably lower dimensionality.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#32780;&#27835;&#20043;&#26041;&#27861;&#23454;&#29616;&#32447;&#24615;&#35268;&#27169;&#35745;&#31639;&#65292;&#21033;&#29992;&#23616;&#37096;&#24615;&#20551;&#35774;&#23558;&#31995;&#32479;&#21010;&#20998;&#20026;&#21487;&#20197;&#20998;&#21035;&#35299;&#20915;&#30340;&#23376;&#22495;&#65292;&#20174;&#32780;&#29992;&#20110;&#39044;&#27979;&#22810;&#20307;&#31995;&#32479;&#30340;&#23494;&#38598;&#24615;&#36136;&#21644;&#20998;&#31867;&#30456;&#12290;&#27169;&#22411;&#36866;&#29992;&#24615;&#30001;&#22359;&#22823;&#23567;&#21644;&#22312;&#20020;&#30028;&#28857;&#38468;&#36817;&#35757;&#32451;ML&#27169;&#22411;&#30340;&#22359;&#25968;&#20915;&#23450;&#12290;</title><link>http://arxiv.org/abs/2304.05502</link><description>&lt;p&gt;
&#29992;&#20110;&#32467;&#26500;-&#24615;&#36136;&#20851;&#31995;&#30340;&#26426;&#22120;&#23398;&#20064;&#65306;&#21487;&#25193;&#23637;&#24615;&#21644;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Machine learning for structure-property relationships: Scalability and limitations. (arXiv:2304.05502v1 [cond-mat.stat-mech])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05502
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#32780;&#27835;&#20043;&#26041;&#27861;&#23454;&#29616;&#32447;&#24615;&#35268;&#27169;&#35745;&#31639;&#65292;&#21033;&#29992;&#23616;&#37096;&#24615;&#20551;&#35774;&#23558;&#31995;&#32479;&#21010;&#20998;&#20026;&#21487;&#20197;&#20998;&#21035;&#35299;&#20915;&#30340;&#23376;&#22495;&#65292;&#20174;&#32780;&#29992;&#20110;&#39044;&#27979;&#22810;&#20307;&#31995;&#32479;&#30340;&#23494;&#38598;&#24615;&#36136;&#21644;&#20998;&#31867;&#30456;&#12290;&#27169;&#22411;&#36866;&#29992;&#24615;&#30001;&#22359;&#22823;&#23567;&#21644;&#22312;&#20020;&#30028;&#28857;&#38468;&#36817;&#35757;&#32451;ML&#27169;&#22411;&#30340;&#22359;&#25968;&#20915;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#39044;&#27979;&#22810;&#20307;&#31995;&#32479;&#30340;&#23494;&#38598;&#24615;&#36136;&#65292;&#29305;&#21035;&#26159;&#20998;&#31867;&#30456;&#12290;&#21487;&#25193;&#23637;&#24615;&#21644;&#21487;&#36716;&#31227;&#24615;&#26159;ML&#26041;&#27861;&#26080;&#21069;&#20363;&#30340;&#35745;&#31639;&#25928;&#29575;&#30340;&#26680;&#24515;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;&#36890;&#36807;&#20998;&#32780;&#27835;&#20043;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#32447;&#24615;&#35268;&#27169;&#30340;&#35745;&#31639;&#65292;&#29289;&#29702;&#24615;&#36136;&#30340;&#23616;&#37096;&#24615;&#26159;&#23558;&#31995;&#32479;&#21010;&#20998;&#20026;&#21487;&#20197;&#20998;&#21035;&#35299;&#20915;&#30340;&#23376;&#22495;&#30340;&#20851;&#38190;&#12290;&#22522;&#20110;&#23616;&#37096;&#24615;&#20551;&#35774;&#65292;&#24320;&#21457;&#20102;ML&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#26377;&#38480;&#23610;&#23544;&#22359;&#30340;&#23494;&#38598;&#24615;&#36136;&#12290;&#21487;&#20197;&#36890;&#36807;&#23545;&#31995;&#32479;&#38543;&#26426;&#37319;&#26679;&#22359;&#30340;ML&#27169;&#22411;&#32467;&#26524;&#36827;&#34892;&#24179;&#22343;&#24471;&#20986;&#22823;&#35268;&#27169;&#31995;&#32479;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#21462;&#20915;&#20110;ML&#27169;&#22411;&#30340;&#22359;&#22823;&#23567;&#26159;&#21542;&#22823;&#20110;&#31995;&#32479;&#30340;&#29305;&#24449;&#38271;&#24230;&#23610;&#24230;&#12290;&#29305;&#21035;&#26159;&#22312;&#36328;&#36234;&#20020;&#30028;&#28857;&#30340;&#30456;&#20301;&#35782;&#21035;&#30340;&#24773;&#20917;&#19979;&#65292;ML&#39044;&#27979;&#30340;&#31934;&#24230;&#21462;&#20915;&#20110;&#22359;&#22823;&#23567;&#20197;&#21450;&#22312;&#20020;&#30028;&#28857;&#38468;&#36817;&#35757;&#32451;ML&#27169;&#22411;&#30340;&#22359;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a scalable machine learning (ML) framework for predicting intensive properties and particularly classifying phases of many-body systems. Scalability and transferability are central to the unprecedented computational efficiency of ML methods. In general, linear-scaling computation can be achieved through the divide and conquer approach, and the locality of physical properties is key to partitioning the system into sub-domains that can be solved separately. Based on the locality assumption, ML model is developed for the prediction of intensive properties of a finite-size block. Predictions of large-scale systems can then be obtained by averaging results of the ML model from randomly sampled blocks of the system. We show that the applicability of this approach depends on whether the block-size of the ML model is greater than the characteristic length scale of the system. In particular, in the case of phase identification across a critical point, the accuracy of the ML predictio
&lt;/p&gt;</description></item><item><title>GraphGANFed&#26159;&#19968;&#20010;&#25972;&#21512;&#20102;GCN&#12289;GAN&#21644;&#32852;&#37030;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#21487;&#22312;&#19981;&#20849;&#20139;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#35757;&#32451;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#21161;&#21147;&#22522;&#20110;&#22270;&#30340;&#20998;&#23376;&#30340;&#33647;&#29289;&#21457;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.05498</link><description>&lt;p&gt;
GraphGANFed: &#38024;&#23545;&#22522;&#20110;&#22270;&#30340;&#20998;&#23376;&#30340;&#32852;&#21512;&#29983;&#25104;&#26694;&#26550;&#65292;&#21161;&#21147;&#39640;&#25928;&#33647;&#29289;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
GraphGANFed: A Federated Generative Framework for Graph-Structured Molecules Towards Efficient Drug Discovery. (arXiv:2304.05498v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05498
&lt;/p&gt;
&lt;p&gt;
GraphGANFed&#26159;&#19968;&#20010;&#25972;&#21512;&#20102;GCN&#12289;GAN&#21644;&#32852;&#37030;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#21487;&#22312;&#19981;&#20849;&#20139;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#35757;&#32451;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#21161;&#21147;&#22522;&#20110;&#22270;&#30340;&#20998;&#23376;&#30340;&#33647;&#29289;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#21152;&#36895;&#20102;&#23427;&#22312;&#21508;&#20010;&#39046;&#22495;&#24212;&#29992;&#65292;&#22914;&#32454;&#32990;&#22270;&#20687;&#20998;&#26512;&#21644;&#20998;&#23376;&#21457;&#29616;&#12290;&#22312;&#20998;&#23376;&#21457;&#29616;&#20013;&#65292;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#22240;&#20854;&#33021;&#22815;&#39640;&#25928;&#22320;&#20174;&#22823;&#22411;&#20998;&#23376;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#24182;&#29983;&#25104;&#20445;&#30041;&#30456;&#20284;&#24615;&#36136;&#30340;&#26032;&#20998;&#23376;&#30340;&#33021;&#21147;&#32780;&#25104;&#20026;&#39318;&#36873;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#30340;&#21046;&#33647;&#20844;&#21496;&#21487;&#33021;&#19981;&#24895;&#24847;&#25110;&#26080;&#27861;&#20849;&#20139;&#20854;&#26412;&#22320;&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#30001;&#20110;&#20998;&#23376;&#25968;&#25454;&#38598;&#30340;&#22320;&#29702;&#20998;&#24067;&#21644;&#25935;&#24863;&#24615;&#36136;&#65292;&#36825;&#20351;&#24471;&#22312;&#38598;&#20013;&#24335;&#26041;&#24335;&#20013;&#35757;&#32451;GAN&#21464;&#24471;&#19981;&#21487;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;GCN&#65289;&#12289;GAN&#21644;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20316;&#20026;&#25972;&#20307;&#31995;&#32479;&#30340;GraphGANFed&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#20998;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in deep learning have accelerated its use in various applications, such as cellular image analysis and molecular discovery. In molecular discovery, a generative adversarial network (GAN), which comprises a discriminator to distinguish generated molecules from existing molecules and a generator to generate new molecules, is one of the premier technologies due to its ability to learn from a large molecular data set efficiently and generate novel molecules that preserve similar properties. However, different pharmaceutical companies may be unwilling or unable to share their local data sets due to the geo-distributed and sensitive nature of molecular data sets, making it impossible to train GANs in a centralized manner. In this paper, we propose a Graph convolutional network in Generative Adversarial Networks via Federated learning (GraphGANFed) framework, which integrates graph convolutional neural Network (GCN), GAN, and federated learning (FL) as a whole system to genera
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#21333;&#38376;&#38480;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;(MoE)&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21152;&#23454;&#29992;&#30340;&#35757;&#32451;&#26041;&#24335;&#65292;&#19988;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#30340;&#25928;&#29575;-&#20934;&#30830;&#24615;&#26435;&#34913;&#20855;&#22791;&#31454;&#20105;&#21147;&#24182;&#20248;&#20110;&#38750;&#28151;&#21512;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2304.05497</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#21333;&#38376;&#38480;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Revisiting Single-gated Mixtures of Experts. (arXiv:2304.05497v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05497
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#21333;&#38376;&#38480;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;(MoE)&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21152;&#23454;&#29992;&#30340;&#35757;&#32451;&#26041;&#24335;&#65292;&#19988;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#30340;&#25928;&#29575;-&#20934;&#30830;&#24615;&#26435;&#34913;&#20855;&#22791;&#31454;&#20105;&#21147;&#24182;&#20248;&#20110;&#38750;&#28151;&#21512;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#65288;MoE&#65289;&#30001;&#20110;&#20854;&#33021;&#22815;&#22312;&#35757;&#32451;&#26497;&#20854;&#22823;&#35268;&#27169;&#27169;&#22411;&#30340;&#21516;&#26102;&#65292;&#20801;&#35768;&#21512;&#29702;&#30340;&#25512;&#29702;&#35745;&#31639;&#25104;&#26412;&#65292;&#22240;&#32780;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#20294;&#26159;&#65292;&#26368;&#36817;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#36890;&#24120;&#20551;&#35774;&#26377;&#24456;&#22810;&#19987;&#23478;&#65292;&#24182;&#35201;&#27714;&#32852;&#21512;&#35757;&#32451;&#25152;&#26377;&#19987;&#23478;&#65292;&#36825;&#32463;&#24120;&#23548;&#33268;&#36335;&#30001;&#22120;&#22349;&#22604;&#31561;&#35757;&#32451;&#19981;&#31283;&#23450;&#38382;&#39064;&#12290;&#30456;&#21453;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24314;&#35758;&#37325;&#26032;&#23457;&#35270;&#31616;&#21333;&#30340;&#21333;&#38376;&#38480;MoE&#65292;&#36825;&#20351;&#24471;&#20854;&#35757;&#32451;&#26356;&#20026;&#23454;&#29992;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#30340;&#20027;&#35201;&#20248;&#21183;&#22312;&#20110;&#65306;&#65288;i&#65289;&#22522;&#30784;&#27169;&#22411;&#20998;&#20026;&#26089;&#26399;&#36864;&#20986;&#21644;&#38598;&#25104;&#27491;&#21017;&#21270;&#26041;&#26696;&#65292;&#65288;ii&#65289;&#19968;&#20010;&#31616;&#21333;&#19988;&#39640;&#25928;&#30340;&#24322;&#27493;&#35757;&#32451;&#31649;&#36947;&#65292;&#26080;&#36335;&#30001;&#22120;&#23849;&#28291;&#38382;&#39064;&#65292;&#20197;&#21450;&#65288;iii&#65289;&#27599;&#20010;&#26679;&#26412;&#22522;&#20110;&#32858;&#31867;&#30340;&#21021;&#22987;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#33719;&#24471;&#20102;&#19982;&#20854;&#20182;&#26356;&#22797;&#26434;&#30340;MoE&#30456;&#24403;&#30340;&#25928;&#29575;-&#20934;&#30830;&#24615;&#26435;&#34913;&#65292;&#24182;&#19988;&#20248;&#20110;&#38750;&#28151;&#21512;&#22522;&#32447;&#12290;&#36825;&#23637;&#31034;&#20102;&#21363;&#20351;&#26159;&#31616;&#21333;&#30340;&#21333;&#38376;&#38480;MoE&#30340;&#20248;&#28857;&#65292;&#24182;&#28608;&#21169;&#36827;&#19968;&#27493;&#30340;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mixture of Experts (MoE) are rising in popularity as a means to train extremely large-scale models, yet allowing for a reasonable computational cost at inference time. Recent state-of-the-art approaches usually assume a large number of experts, and require training all experts jointly, which often lead to training instabilities such as the router collapsing In contrast, in this work, we propose to revisit the simple single-gate MoE, which allows for more practical training. Key to our work are (i) a base model branch acting both as an early-exit and an ensembling regularization scheme, (ii) a simple and efficient asynchronous training pipeline without router collapse issues, and finally (iii) a per-sample clustering-based initialization. We show experimentally that the proposed model obtains efficiency-to-accuracy trade-offs comparable with other more complex MoE, and outperforms non-mixture baselines. This showcases the merits of even a simple single-gate MoE, and motivates further ex
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#32423;&#32852;&#25351;&#23548;&#19979;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65292;&#22686;&#24378;&#20102;&#20018;&#32852;&#25512;&#33616;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#65292;&#21462;&#24471;&#20102;&#27604;&#24050;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.05492</link><description>&lt;p&gt;
&#25913;&#36827;&#20018;&#32852;&#25512;&#33616;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;: &#20276;&#38543;&#32423;&#32852;&#25351;&#23548;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards More Robust and Accurate Sequential Recommendation with Cascade-guided Adversarial Training. (arXiv:2304.05492v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05492
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#32423;&#32852;&#25351;&#23548;&#19979;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65292;&#22686;&#24378;&#20102;&#20018;&#32852;&#25512;&#33616;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#65292;&#21462;&#24471;&#20102;&#27604;&#24050;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20018;&#32852;&#25512;&#33616;&#27169;&#22411;&#26159;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#29992;&#25143;&#19982;&#29289;&#21697;&#38388;&#30340;&#26102;&#38388;&#39034;&#24207;&#20114;&#21160;&#26469;&#36827;&#34892;&#25512;&#33616;&#30340;&#27169;&#22411;&#65292;&#20854;&#24050;&#32463;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#23637;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#36817;&#26399;&#20018;&#32852;&#25512;&#33616;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#22791;&#21463;&#36136;&#30097;&#12290;&#36825;&#31181;&#27169;&#22411;&#30340;&#20004;&#20010;&#29305;&#24615;&#20351;&#20854;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987; - &#22312;&#35757;&#32451;&#20013;&#20250;&#20135;&#29983;&#32423;&#32852;&#25928;&#24212;&#65292;&#22312;&#27169;&#22411;&#36807;&#24230;&#20381;&#36182;&#26102;&#38388;&#20449;&#24687;&#30340;&#21516;&#26102;&#20250;&#24573;&#30053;&#20854;&#20182;&#29305;&#24449;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20018;&#32852;&#25512;&#33616;&#27169;&#22411;&#30340;&#32423;&#32852;&#25351;&#23548;&#19979;&#30340;&#23545;&#25239;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20018;&#32852;&#24314;&#27169;&#20013;&#30340;&#20869;&#22312;&#32423;&#32852;&#25928;&#24212;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20135;&#29983;&#25112;&#30053;&#24615;&#30340;&#23545;&#25239;&#24615;&#25200;&#21160;&#26469;&#24433;&#21709;&#29289;&#21697;&#23884;&#20837;&#12290;&#22312;&#20351;&#29992;&#19981;&#21516;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#35757;&#32451;&#22235;&#31181;&#26368;&#20808;&#36827;&#30340;&#20018;&#32852;&#27169;&#22411;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#35757;&#32451;&#26041;&#27861;&#20135;&#29983;&#20102;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#39640;&#30340;&#27169;&#22411;&#40065;&#26834;&#24615;&#65292;&#24182;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential recommendation models, models that learn from chronological user-item interactions, outperform traditional recommendation models in many settings. Despite the success of sequential recommendation models, their robustness has recently come into question. Two properties unique to the nature of sequential recommendation models may impair their robustness - the cascade effects induced during training and the model's tendency to rely too heavily on temporal information. To address these vulnerabilities, we propose Cascade-guided Adversarial training, a new adversarial training procedure that is specifically designed for sequential recommendation models. Our approach harnesses the intrinsic cascade effects present in sequential modeling to produce strategic adversarial perturbations to item embeddings during training. Experiments on training state-of-the-art sequential models on four public datasets from different domains show that our training approach produces superior model ran
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#23548;&#21521;&#21644;&#36136;&#37327;&#35780;&#20272;&#31995;&#32479;&#65292;&#20351;&#29992;&#25913;&#36827;&#30340; Faster R-CNN &#31639;&#27861;&#26469;&#24314;&#35758;&#33040;&#21160;&#33033;&#22810;&#26222;&#21202;&#27969;&#38376;&#30340;&#20301;&#32622;&#65292;&#24182;&#35780;&#20272;&#22810;&#26222;&#21202;&#27874;&#24418;&#36136;&#37327;&#65292;&#26377;&#25928;&#22320;&#22635;&#34917;&#20102;&#32463;&#39564;&#19981;&#36275;&#30340;&#36229;&#22768;&#21307;&#29983;&#30340;&#32570;&#38519;&#12290;</title><link>http://arxiv.org/abs/2304.05463</link><description>&lt;p&gt;
&#33258;&#21160;&#23548;&#21521;&#21644;&#36136;&#37327;&#35780;&#20272;&#31995;&#32479;&#29992;&#20110;&#33040;&#21160;&#33033;&#22810;&#26222;&#21202;&#25104;&#20687;
&lt;/p&gt;
&lt;p&gt;
An Automatic Guidance and Quality Assessment System for Doppler Imaging of Umbilical Artery. (arXiv:2304.05463v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05463
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#23548;&#21521;&#21644;&#36136;&#37327;&#35780;&#20272;&#31995;&#32479;&#65292;&#20351;&#29992;&#25913;&#36827;&#30340; Faster R-CNN &#31639;&#27861;&#26469;&#24314;&#35758;&#33040;&#21160;&#33033;&#22810;&#26222;&#21202;&#27969;&#38376;&#30340;&#20301;&#32622;&#65292;&#24182;&#35780;&#20272;&#22810;&#26222;&#21202;&#27874;&#24418;&#36136;&#37327;&#65292;&#26377;&#25928;&#22320;&#22635;&#34917;&#20102;&#32463;&#39564;&#19981;&#36275;&#30340;&#36229;&#22768;&#21307;&#29983;&#30340;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32974;&#20799;&#36229;&#22768;&#31579;&#26597;&#20013;&#65292;&#36890;&#36807;&#33040;&#24102;&#36827;&#34892;&#30417;&#27979;&#30340;&#33040;&#21160;&#33033;&#22810;&#26222;&#21202;&#22270;&#20687;&#23545;&#20110;&#30417;&#27979;&#32974;&#20799;&#30340;&#34880;&#28082;&#20379;&#24212;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#25429;&#25417;&#33040;&#21160;&#33033;&#22810;&#26222;&#21202;&#22270;&#20687;&#65292;&#38656;&#35201;&#27491;&#30830;&#22320;&#25191;&#34892;&#22810;&#20010;&#27493;&#39588;&#65306;&#22312;&#36229;&#22768;&#22270;&#20687;&#20013;&#25918;&#32622;&#38376;&#65292;&#20197;&#33719;&#21462;&#34880;&#27969;&#27874;&#24418;&#65292;&#24182;&#21028;&#26029;&#22810;&#26222;&#21202;&#27874;&#24418;&#36136;&#37327;&#12290;&#36825;&#20123;&#27493;&#39588;&#37117;&#20381;&#36182;&#20110;&#25805;&#20316;&#32773;&#30340;&#32463;&#39564;&#12290;&#32463;&#39564;&#19981;&#36275;&#30340;&#36229;&#22768;&#21307;&#29983;&#30340;&#30701;&#32570;&#22240;&#27492;&#20135;&#29983;&#20102;&#26426;&#22120;&#36741;&#21161;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#31995;&#32479;&#26469;&#22635;&#34917;&#36825;&#20010;&#32570;&#21475;&#12290;&#20351;&#29992;&#25913;&#36827;&#30340; Faster R-CNN &#26041;&#27861;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#23427;&#24314;&#35758;&#22810;&#26222;&#21202;&#27969;&#38376;&#30340;&#20301;&#32622;&#12290;&#25105;&#20204;&#38543;&#21518;&#35780;&#20272;&#22810;&#26222;&#21202;&#27874;&#24418;&#36136;&#37327;&#12290;&#25105;&#20204;&#22312;&#22269;&#23478;&#36229;&#22768;&#31579;&#26597;&#25968;&#25454;&#24211;&#19978;&#23545;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#28085;&#30422;&#20102;657&#20010;&#25195;&#25551;&#32467;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#25351;&#23548;&#25805;&#20316;&#32773;&#25429;&#25417;&#33040;&#21160;&#33033;&#22810;&#26222;&#21202;&#22270;&#20687;&#21644;&#35780;&#20272;&#22270;&#20687;&#36136;&#37327;&#26041;&#38754;&#26159;&#26377;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In fetal ultrasound screening, Doppler images on the umbilical artery (UA) are important for monitoring blood supply through the umbilical cord. However, to capture UA Doppler images, a number of steps need to be done correctly: placing the gate at a proper location in the ultrasound image to obtain blood flow waveforms, and judging the Doppler waveform quality. Both of these rely on the operator's experience. The shortage of experienced sonographers thus creates a demand for machine assistance. We propose an automatic system to fill this gap. Using a modified Faster R-CNN we obtain an algorithm that suggests Doppler flow gate locations. We subsequently assess the Doppler waveform quality. We validate the proposed system on 657 scans from a national ultrasound screening database. The experimental results demonstrate that our system is useful in guiding operators for UA Doppler image capture and quality assessment.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36229;&#32593;&#32476;&#31574;&#30053;&#65292;&#21487;&#20197;&#26681;&#25454;&#32553;&#25918;&#22240;&#23376;&#24555;&#36895;&#29983;&#25104; Pareto &#21069;&#27839;&#65292;&#26080;&#38656;&#35757;&#32451;&#22810;&#20010;&#32593;&#32476;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#20351;&#29992;&#26356;&#23569;&#30340;&#21442;&#25968;&#21644;&#35745;&#31639;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.05448</link><description>&lt;p&gt;
&#21160;&#24577;&#29305;&#24449;&#32553;&#25918;&#30340;&#20998;&#27573;&#22270;&#20687;&#26041;&#27861;&#30340;&#23454;&#29616;&#19982;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Amortized Learning of Dynamic Feature Scaling for Image Segmentation. (arXiv:2304.05448v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05448
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36229;&#32593;&#32476;&#31574;&#30053;&#65292;&#21487;&#20197;&#26681;&#25454;&#32553;&#25918;&#22240;&#23376;&#24555;&#36895;&#29983;&#25104; Pareto &#21069;&#27839;&#65292;&#26080;&#38656;&#35757;&#32451;&#22810;&#20010;&#32593;&#32476;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#20351;&#29992;&#26356;&#23569;&#30340;&#21442;&#25968;&#21644;&#35745;&#31639;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#24050;&#25104;&#20026;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#20013;&#21331;&#36234;&#30340;&#27169;&#22411;&#12290;&#22823;&#22810;&#25968;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20998;&#21106;&#26550;&#26500;&#36890;&#36807;&#22266;&#23450;&#30340;&#22240;&#23376;&#23558;&#31354;&#38388;&#32500;&#24230;&#35843;&#25972;&#20026;&#20108;&#26469;&#32858;&#21512;&#31354;&#38388;&#19978;&#19979;&#25991;&#12290;&#20026;&#20102;&#25552;&#39640;&#29305;&#23450;&#24212;&#29992;&#31243;&#24207;&#30340;&#27169;&#22411;&#20934;&#30830;&#24615;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#20854;&#20182;&#35843;&#25972;&#22240;&#23376;&#12290;&#28982;&#32780;&#65292;&#25214;&#21040;&#21512;&#36866;&#30340;&#35843;&#25972;&#22240;&#23376;&#36890;&#24120;&#38656;&#35201;&#20026;&#35768;&#22810;&#19981;&#21516;&#30340;&#22240;&#23376;&#35757;&#32451;&#21333;&#29420;&#30340;&#32593;&#32476;&#65292;&#24182;&#27604;&#36739;&#27599;&#20010;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#35745;&#31639;&#36127;&#33655;&#24847;&#21619;&#30528;&#22312;&#23454;&#36341;&#20013;&#24456;&#23569;&#36825;&#26679;&#20570;&#65292;&#32780;&#19988;&#21482;&#32771;&#34385;&#20102;&#20960;&#20010;&#19981;&#21516;&#30340;&#32553;&#25918;&#22240;&#23376;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#32593;&#32476;&#31574;&#30053;&#65292;&#21487;&#20197;&#29992;&#26469;&#36731;&#26494;&#24555;&#36895;&#22320;&#29983;&#25104;&#22312;&#35843;&#25972;&#22240;&#23376;&#21464;&#21270;&#26102;&#65292;&#22312;&#20934;&#30830;&#24230;&#21644;&#25928;&#29575;&#20043;&#38388;&#30340; Pareto &#21069;&#27839;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#35757;&#32451;&#19968;&#20010;&#21333;&#29420;&#30340;&#36229;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#29983;&#25104;&#26465;&#20214;&#20110;&#35843;&#25972;&#22240;&#23376;&#30340; CNN &#21442;&#25968;&#12290;&#36825;&#20351;&#24471;&#29992;&#25143;&#21487;&#20197;&#24555;&#36895;&#36873;&#25321;&#20182;&#20204;&#30340;&#29305;&#23450;&#24212;&#29992;&#31243;&#24207;&#30340;&#32553;&#25918;&#22240;&#23376;&#65292;&#32780;&#26080;&#38656;&#35757;&#32451;&#22810;&#20010;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#27604;&#29616;&#26377;&#26041;&#27861;&#20351;&#29992;&#26356;&#23569;&#30340;&#21442;&#25968;&#21644;&#35745;&#31639;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutional neural networks (CNN) have become the predominant model for image segmentation tasks. Most CNN segmentation architectures resize spatial dimensions by a fixed factor of two to aggregate spatial context. Recent work has explored using other resizing factors to improve model accuracy for specific applications. However, finding the appropriate rescaling factor most often involves training a separate network for many different factors and comparing the performance of each model. The computational burden of these models means that in practice it is rarely done, and when done only a few different scaling factors are considered.  In this work, we present a hypernetwork strategy that can be used to easily and rapidly generate the Pareto frontier for the trade-off between accuracy and efficiency as the rescaling factor varies. We show how to train a single hypernetwork that generates CNN parameters conditioned on a rescaling factor. This enables a user to quickly choose a rescalin
&lt;/p&gt;</description></item><item><title>Co-ML&#26159;&#19968;&#20010;&#22522;&#20110;&#24179;&#26495;&#30005;&#33041;&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#29992;&#20110;&#21327;&#21516;&#26500;&#24314;ML&#22270;&#20687;&#20998;&#31867;&#22120;&#65292;&#21487;&#20197;&#24110;&#21161;&#23398;&#20064;&#32773;&#22312;&#21512;&#20316;&#20013;&#21457;&#25496;&#26032;&#30340;&#24819;&#27861;&#21644;&#26041;&#27861;&#65292;&#35299;&#20915;&#25968;&#25454;&#34920;&#29616;&#21644;&#22810;&#26679;&#24615;&#31561;&#20851;&#38190;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.05444</link><description>&lt;p&gt;
&#20351;&#29992;Co-ML&#21327;&#21516;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26500;&#24314;&#23478;&#24237;&#30340;&#21512;&#20316;&#27169;&#22411;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
Collaborative Machine Learning Model Building with Families Using Co-ML. (arXiv:2304.05444v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05444
&lt;/p&gt;
&lt;p&gt;
Co-ML&#26159;&#19968;&#20010;&#22522;&#20110;&#24179;&#26495;&#30005;&#33041;&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#29992;&#20110;&#21327;&#21516;&#26500;&#24314;ML&#22270;&#20687;&#20998;&#31867;&#22120;&#65292;&#21487;&#20197;&#24110;&#21161;&#23398;&#20064;&#32773;&#22312;&#21512;&#20316;&#20013;&#21457;&#25496;&#26032;&#30340;&#24819;&#27861;&#21644;&#26041;&#27861;&#65292;&#35299;&#20915;&#25968;&#25454;&#34920;&#29616;&#21644;&#22810;&#26679;&#24615;&#31561;&#20851;&#38190;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#38024;&#23545;&#26032;&#25163;&#21451;&#22909;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24314;&#27169;&#24037;&#20855;&#65292;&#20391;&#37325;&#20110;&#21333;&#19968;&#29992;&#25143;&#20307;&#39564;&#65292;&#19968;&#20010;&#21333;&#19968;&#29992;&#25143;&#20165;&#25910;&#38598;&#33258;&#24049;&#30340;&#25968;&#25454;&#26469;&#26500;&#24314;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#21333;&#29420;&#24314;&#27169;&#32463;&#21382;&#38480;&#21046;&#20102;&#23398;&#20064;&#32773;&#20849;&#21516;&#24037;&#20316;&#26102;&#20250;&#36935;&#21040;&#30340;&#20132;&#26367;&#24819;&#27861;&#21644;&#26041;&#27861;&#30340;&#23453;&#36149;&#26426;&#20250;&#12290;&#22240;&#27492;&#65292;&#24403;&#19981;&#21516;&#30340;&#35266;&#28857;&#20307;&#29616;&#22312;&#32676;&#20307;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#20013;&#26102;&#65292;&#24448;&#24448;&#25490;&#38500;&#20102;ML&#22260;&#32469;&#25968;&#25454;&#34920;&#29616;&#21644;&#22810;&#26679;&#24615;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;Co-ML&#8212;&#8212;&#19968;&#20010;&#38754;&#21521;&#23398;&#20064;&#32773;&#30340;&#22522;&#20110;&#24179;&#26495;&#30005;&#33041;&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#36890;&#36807;&#31471;&#23545;&#31471;&#30340;&#36845;&#20195;&#27169;&#22411;&#26500;&#24314;&#27969;&#31243;&#65292;&#21327;&#21516;&#26500;&#24314;ML&#22270;&#20687;&#20998;&#31867;&#22120;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20171;&#32461;&#19968;&#20010;&#23478;&#24237;&#65288;&#30001;&#20004;&#20010;11&#21644;14&#23681;&#30340;&#23401;&#23376;&#19982;&#29238;&#27597;&#19968;&#36215;&#24037;&#20316;&#65289;&#22312;&#23478;&#20013;&#20351;&#29992;Co-ML&#36827;&#34892;&#24341;&#23548;&#24615;&#20171;&#32461;ML&#27963;&#21160;&#30340;&#28145;&#20837;&#26696;&#20363;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#21327;&#20316;&#24314;&#27169;&#30340;&#21487;&#34892;&#24615;&#21644;&#28508;&#22312;&#20016;&#23500;&#24615;&#12290;&#25105;&#20204;&#20998;&#20139;&#20102;Co-ML&#31995;&#32479;&#30340;d&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing novice-friendly machine learning (ML) modeling tools center around a solo user experience, where a single user collects only their own data to build a model. However, solo modeling experiences limit valuable opportunities for encountering alternative ideas and approaches that can arise when learners work together; consequently, it often precludes encountering critical issues in ML around data representation and diversity that can surface when different perspectives are manifested in a group-constructed data set. To address this issue, we created Co-ML -- a tablet-based app for learners to collaboratively build ML image classifiers through an end-to-end, iterative model-building process. In this paper, we illustrate the feasibility and potential richness of collaborative modeling by presenting an in-depth case study of a family (two children 11 and 14-years-old working with their parents) using Co-ML in a facilitated introductory ML activity at home. We share the Co-ML system d
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#24322;&#26500;&#29305;&#24449;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#26032;&#30340;&#30446;&#26631;&#30828;&#20214;&#19978;&#36890;&#36807;&#23398;&#20064;&#32852;&#21512;&#31070;&#32463;&#32593;&#32476;&#21644;&#30828;&#20214;&#29305;&#24449;&#65292;&#35299;&#20915;&#20102;&#24352;&#37327;&#31243;&#24207;&#29983;&#25104;&#30340;&#33258;&#21160;&#35843;&#25972;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.05430</link><description>&lt;p&gt;
&#36866;&#29992;&#20110;&#24322;&#26500;&#29305;&#24449;&#30340;&#36801;&#31227;&#23398;&#20064;&#65292;&#23454;&#29616;&#39640;&#25928;&#30340;&#24352;&#37327;&#31243;&#24207;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Transfer Learning Across Heterogeneous Features For Efficient Tensor Program Generation. (arXiv:2304.05430v1 [cs.PL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05430
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#24322;&#26500;&#29305;&#24449;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#26032;&#30340;&#30446;&#26631;&#30828;&#20214;&#19978;&#36890;&#36807;&#23398;&#20064;&#32852;&#21512;&#31070;&#32463;&#32593;&#32476;&#21644;&#30828;&#20214;&#29305;&#24449;&#65292;&#35299;&#20915;&#20102;&#24352;&#37327;&#31243;&#24207;&#29983;&#25104;&#30340;&#33258;&#21160;&#35843;&#25972;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25913;&#36827;&#24352;&#37327;&#31243;&#24207;&#29983;&#25104;&#38656;&#35201;&#22312;&#30446;&#26631;&#30828;&#20214;&#19978;&#20026;&#32473;&#23450;&#31243;&#24207;&#25628;&#32034;&#21508;&#31181;&#21487;&#33021;&#30340;&#31243;&#24207;&#36716;&#25442;&#32452;&#21512;&#65292;&#20197;&#20248;&#21270;&#24352;&#37327;&#31243;&#24207;&#30340;&#25191;&#34892;&#12290;&#30001;&#20110;&#24222;&#22823;&#30340;&#25628;&#32034;&#31354;&#38388;&#21644;&#25351;&#25968;&#32423;&#21035;&#30340;&#21464;&#25442;&#32452;&#21512;&#65292;&#33258;&#21160;&#35843;&#25972;&#24352;&#37327;&#31243;&#24207;&#30340;&#29983;&#25104;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#65292;&#23588;&#20854;&#26159;&#24403;&#38656;&#35201;&#38754;&#23545;&#24322;&#26500;&#30340;&#30446;&#26631;&#26102;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#23398;&#20064;&#32852;&#21512;&#31070;&#32463;&#32593;&#32476;&#21644;&#30828;&#20214;&#29305;&#24449;&#65292;&#24182;&#23558;&#23427;&#20204;&#36716;&#31227;&#21040;&#26032;&#30340;&#30446;&#26631;&#30828;&#20214;&#19978;&#65292;&#20174;&#32780;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#24191;&#27867;&#30740;&#31350;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#25968;&#25454;&#38598;TenSet&#65292;&#22312;&#27979;&#35797;&#38598;&#20998;&#21106;&#31574;&#30053;&#19978;&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20248;&#21270;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#37319;&#29992;&#27880;&#24847;&#21147;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20026;&#35843;&#25972;&#24352;&#37327;&#31243;&#24207;&#25552;&#20379;&#25903;&#25345;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#34701;&#20837;&#31070;&#32463;&#32593;&#32476;&#21644;&#30828;&#20214;&#29305;&#23450;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#23558;&#25968;&#25454;&#38598;&#30340;&#22522;&#32447;&#31934;&#31616;&#39640;&#36798;45&#65285;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;Pairwise Comparis&#12290;
&lt;/p&gt;
&lt;p&gt;
Tuning tensor program generation involves searching for various possible program transformation combinations for a given program on target hardware to optimize the tensor program execution. It is already a complex process because of the massive search space and exponential combinations of transformations make auto-tuning tensor program generation more challenging, especially when we have a heterogeneous target. In this research, we attempt to address these problems by learning the joint neural network and hardware features and transferring them to the new target hardware. We extensively study the existing state-of-the-art dataset, TenSet, perform comparative analysis on the test split strategies and propose methodologies to prune the dataset. We adopt an attention-inspired approach for tuning the tensor programs enabling them to embed neural network and hardware-specific features. Our approach could prune the dataset up to 45\% of the baseline without compromising the Pairwise Comparis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#24418;&#34920;&#31034;&#32467;&#26500;&#21644;&#23646;&#24615;&#30340;&#26230;&#26684;&#26448;&#26009;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#20351;&#29992;&#21487;&#24494;&#20998;&#20256;&#36882;&#31639;&#27861;&#35745;&#31639;&#26426;&#26800;&#23646;&#24615;&#20197;&#23454;&#29616;&#21453;&#21521;&#35774;&#35745;&#65292;&#36827;&#32780;&#23454;&#29616;&#20102;&#21487;&#25193;&#23637;&#30340;&#12289;&#20855;&#26377;&#21069;&#25152;&#26410;&#26377;&#30340;&#32467;&#26500;&#21644;&#21151;&#33021;&#22810;&#26679;&#24615;&#30340;&#26230;&#26684;&#26448;&#26009;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2304.05422</link><description>&lt;p&gt;
&#21487;&#21306;&#20998;&#30340;&#22270;&#32467;&#26500;&#27169;&#22411;&#29992;&#20110;&#26230;&#26684;&#26448;&#26009;&#21453;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Differentiable graph-structured models for inverse design of lattice materials. (arXiv:2304.05422v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05422
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#24418;&#34920;&#31034;&#32467;&#26500;&#21644;&#23646;&#24615;&#30340;&#26230;&#26684;&#26448;&#26009;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#20351;&#29992;&#21487;&#24494;&#20998;&#20256;&#36882;&#31639;&#27861;&#35745;&#31639;&#26426;&#26800;&#23646;&#24615;&#20197;&#23454;&#29616;&#21453;&#21521;&#35774;&#35745;&#65292;&#36827;&#32780;&#23454;&#29616;&#20102;&#21487;&#25193;&#23637;&#30340;&#12289;&#20855;&#26377;&#21069;&#25152;&#26410;&#26377;&#30340;&#32467;&#26500;&#21644;&#21151;&#33021;&#22810;&#26679;&#24615;&#30340;&#26230;&#26684;&#26448;&#26009;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#20110;&#28145;&#31354;&#24694;&#21155;&#29615;&#22659;&#20013;&#33021;&#22815;&#26681;&#25454;&#38656;&#35201;&#33258;&#36866;&#24212;&#30340;&#29289;&#29702;&#21270;&#23398;&#24615;&#36136;&#30340;&#26448;&#26009;&#23558;&#22312;&#23450;&#20041;&#26410;&#26469;&#30340;&#31354;&#38388;&#25506;&#32034;&#26041;&#38754;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#33258;&#28982;&#30028;&#20013;&#65292;&#24494;&#22937;&#30340;&#24494;&#35266;&#32467;&#26500;&#21644;&#26684;&#23376;&#20960;&#20309;&#24418;&#29366;&#26159;&#35774;&#35745;&#36866;&#24212;&#20110;&#29305;&#23450;&#29615;&#22659;&#26448;&#26009;&#30340;&#20196;&#20154;&#20852;&#22859;&#30340;&#28789;&#24863;&#26469;&#28304;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#31181;&#19981;&#35268;&#21017;&#25299;&#25169;&#35206;&#30422;&#30340;&#24040;&#22823;&#35774;&#35745;&#31354;&#38388;&#65292;&#22312;&#20998;&#26512;&#19978;&#36827;&#34892;&#25506;&#32034;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22240;&#27492;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#22823;&#22810;&#25968;&#21512;&#25104;&#26230;&#26684;&#26448;&#26009;&#37117;&#26159;&#22522;&#20110;&#21608;&#26399;&#24615;&#32467;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#26041;&#27861;&#65292;&#20351;&#29992;&#22270;&#24418;&#34920;&#31034;&#23545;&#35268;&#21017;&#21644;&#19981;&#35268;&#21017;&#26230;&#26684;&#26448;&#26009;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#21487;&#24494;&#20998;&#20256;&#36882;&#31639;&#27861;&#35745;&#31639;&#21147;&#23398;&#24615;&#36136;&#65292;&#22240;&#27492;&#21487;&#20197;&#20351;&#29992;&#33258;&#21160;&#24494;&#20998;&#26469;&#35843;&#25972;&#21333;&#20010;&#26230;&#26684;&#20803;&#32032;&#30340;&#20960;&#20309;&#32467;&#26500;&#21644;&#23646;&#24615;&#65292;&#20174;&#32780;&#35774;&#35745;&#20855;&#26377;&#25152;&#38656;&#23646;&#24615;&#30340;&#26448;&#26009;&#12290;&#24341;&#20837;&#23545;&#26230;&#26684;&#32467;&#26500;&#21644;&#26448;&#26009;&#23646;&#24615;&#30340;&#38544;&#24335;&#21487;&#23398;&#20064;&#20960;&#20309;&#34920;&#31034;&#65292;&#32467;&#21512;&#21453;&#35774;&#35745;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#21487;&#25193;&#23637;&#30340;&#12289;&#20855;&#26377;&#21069;&#25152;&#26410;&#26377;&#30340;&#32467;&#26500;&#21644;&#21151;&#33021;&#22810;&#26679;&#24615;&#30340;&#26230;&#26684;&#26448;&#26009;&#35774;&#35745;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Materials possessing flexible physico-chemical properties that adapt on-demand to the hostile environmental conditions of deep space will become essential in defining the future of space exploration. A promising venue for inspiration towards the design of environment-specific materials is in the intricate micro-architectures and lattice geometry found throughout nature. However, the immense design space covered by such irregular topologies is challenging to probe analytically. For this reason, most synthetic lattice materials have to date been based on periodic architectures instead. Here, we propose a computational approach using a graph representation for both regular and irregular lattice materials. Our method uses differentiable message passing algorithms to calculate mechanical properties, and therefore allows using automatic differentiation to adjust both the geometric structure and attributes of individual lattice elements to design materials with desired properties. The introdu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#26368;&#36817;&#22312;&#19981;&#21516;iable&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#20013;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#25361;&#25112;&#30340;&#20998;&#31867;&#27861;&#65292;&#23545;DARTS&#26041;&#27861;&#30340;&#36129;&#29486;&#21644;&#24433;&#21709;&#36827;&#34892;&#20102;&#35752;&#35770;&#65292;&#24182;&#25506;&#35752;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2304.05405</link><description>&lt;p&gt;
&#19981;&#21516;iable&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#20013;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#30340;&#39640;&#25928;&#33258;&#21160;&#21270;:&#19968;&#39033;&#27010;&#36848;&#30740;&#31350;(arXiv: 2304.05405v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
Efficient Automation of Neural Network Design: A Survey on Differentiable Neural Architecture Search. (arXiv:2304.05405v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05405
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#26368;&#36817;&#22312;&#19981;&#21516;iable&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#20013;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#25361;&#25112;&#30340;&#20998;&#31867;&#27861;&#65292;&#23545;DARTS&#26041;&#27861;&#30340;&#36129;&#29486;&#21644;&#24433;&#21709;&#36827;&#34892;&#20102;&#35752;&#35770;&#65292;&#24182;&#25506;&#35752;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#20013;&#65292;&#19981;&#21516;iable&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;DNAS&#65289;&#36805;&#36895;&#25104;&#20026;&#33258;&#21160;&#21457;&#29616;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30340;&#27969;&#34892;&#26041;&#27861;&#12290; &#36825;&#31181;&#23835;&#36215;&#20027;&#35201;&#24402;&#21151;&#20110;DARTS&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#37325;&#35201;&#30340;DNAS&#26041;&#27861;&#20043;&#19968;&#12290; &#19982;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#25110;&#36827;&#21270;&#31639;&#27861;&#30340;&#20197;&#21069;&#30340;&#20316;&#21697;&#30456;&#27604;&#65292;DNAS&#36895;&#24230;&#24555;&#20102;&#25968;&#20010;&#25968;&#37327;&#32423;&#65292;&#24182;&#19988;&#20351;&#29992;&#30340;&#35745;&#31639;&#36164;&#28304;&#26356;&#23569;&#12290; &#22312;&#36825;&#31687;&#20840;&#38754;&#30340;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#19987;&#38376;&#20851;&#27880;DNAS&#24182;&#23457;&#26597;&#20102;&#35813;&#39046;&#22495;&#30340;&#26368;&#26032;&#26041;&#27861;&#12290; &#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25361;&#25112;&#30340;&#20998;&#31867;&#27861;&#26469;&#20998;&#31867;DNAS&#26041;&#27861;&#12290; &#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#36807;&#21435;&#20960;&#24180;&#23545;DNAS&#24102;&#26469;&#30340;&#36129;&#29486;&#20197;&#21450;&#20854;&#23545;&#20840;&#29699;NAS&#39046;&#22495;&#30340;&#24433;&#21709;&#12290; &#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#19968;&#20123;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#35265;&#35299;&#26469;&#20570;&#20986;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the past few years, Differentiable Neural Architecture Search (DNAS) rapidly imposed itself as the trending approach to automate the discovery of deep neural network architectures. This rise is mainly due to the popularity of DARTS, one of the first major DNAS methods. In contrast with previous works based on Reinforcement Learning or Evolutionary Algorithms, DNAS is faster by several orders of magnitude and uses fewer computational resources. In this comprehensive survey, we focus specifically on DNAS and review recent approaches in this field. Furthermore, we propose a novel challenge-based taxonomy to classify DNAS methods. We also discuss the contributions brought to DNAS in the past few years and its impact on the global NAS field. Finally, we conclude by giving some insights into future research directions for the DNAS field.
&lt;/p&gt;</description></item><item><title>VRAP&#26159;&#19968;&#31181;&#22810;&#20219;&#21153;&#30340;&#23545;&#25239;&#36148;&#29255;&#29983;&#25104;&#26041;&#27861;&#65292;&#21033;&#29992;&#22330;&#26223;&#22270;&#23558;&#29289;&#20307;&#35782;&#21035;&#20026;&#22522;&#30784;&#30340;&#23545;&#25239;&#36148;&#29255;&#32452;&#21512;&#25104;&#26356;&#22823;&#26356;&#22797;&#26434;&#30340;&#23545;&#25239;&#36148;&#29255;&#65292;&#20174;&#32780;&#25552;&#39640;&#23545;&#25239;&#36148;&#29255;&#22312;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#30340;&#36716;&#31227;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.05402</link><description>&lt;p&gt;
&#22522;&#20110;&#35270;&#35273;&#20851;&#31995;&#30340;&#36328;&#20219;&#21153;&#23545;&#25239;&#36148;&#29255;&#30340;&#36716;&#31227;&#22686;&#24378;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Boosting Cross-task Transferability of Adversarial Patches with Visual Relations. (arXiv:2304.05402v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05402
&lt;/p&gt;
&lt;p&gt;
VRAP&#26159;&#19968;&#31181;&#22810;&#20219;&#21153;&#30340;&#23545;&#25239;&#36148;&#29255;&#29983;&#25104;&#26041;&#27861;&#65292;&#21033;&#29992;&#22330;&#26223;&#22270;&#23558;&#29289;&#20307;&#35782;&#21035;&#20026;&#22522;&#30784;&#30340;&#23545;&#25239;&#36148;&#29255;&#32452;&#21512;&#25104;&#26356;&#22823;&#26356;&#22797;&#26434;&#30340;&#23545;&#25239;&#36148;&#29255;&#65292;&#20174;&#32780;&#25552;&#39640;&#23545;&#25239;&#36148;&#29255;&#22312;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#30340;&#36716;&#31227;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#21487;&#36716;&#31227;&#24615;&#26159;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#40065;&#26834;&#24615;&#30340;&#20851;&#38190;&#26041;&#38754;&#65292;&#23588;&#20854;&#26159;&#22312;&#40657;&#30418;&#22330;&#26223;&#20013;&#65292;&#30446;&#21069;&#34429;&#28982;&#25552;&#20986;&#20102;&#22810;&#31181;&#26041;&#27861;&#26469;&#22686;&#24378;&#36328;&#27169;&#22411;&#30340;&#21487;&#36716;&#31227;&#24615;&#65292;&#20294;&#24456;&#23569;&#20851;&#27880;&#23545;&#25239;&#26679;&#26412;&#22312;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#30340;&#21487;&#36716;&#31227;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#20851;&#31995;&#30340;&#36328;&#20219;&#21153;&#23545;&#25239;&#36148;&#29255;&#29983;&#25104;&#26041;&#27861;VRAP&#65292;&#26088;&#22312;&#35780;&#20272;&#21508;&#31181;&#35270;&#35273;&#20219;&#21153;&#30340;&#40065;&#26834;&#24615;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#28041;&#21450;&#35270;&#35273;&#25512;&#29702;&#30340;&#20219;&#21153;&#65292;&#20363;&#22914;&#35270;&#35273;&#38382;&#31572;&#21644;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#12290;VRAP&#21033;&#29992;&#22330;&#26223;&#22270;&#23558;&#29289;&#20307;&#35782;&#21035;&#20026;&#22522;&#30784;&#30340;&#23545;&#25239;&#36148;&#29255;&#32452;&#21512;&#25104;&#26356;&#22823;&#26356;&#22797;&#26434;&#30340;&#23545;&#25239;&#36148;&#29255;&#65292;&#20174;&#32780;&#25200;&#20081;&#30446;&#26631;&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#21333;&#19968;&#20219;&#21153;&#36148;&#29255;&#30456;&#27604;&#65292;VRAP&#29983;&#25104;&#30340;&#23545;&#25239;&#36148;&#29255;&#22312;&#20219;&#21153;&#38388;&#20855;&#26377;&#26356;&#39640;&#30340;&#21487;&#36716;&#31227;&#24615;&#65292;&#21363;&#20351;&#36825;&#20123;&#20219;&#21153;&#28041;&#21450;&#19981;&#21516;&#30340;&#25512;&#29702;&#20989;&#25968;&#25110;&#36755;&#20837;&#26684;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
The transferability of adversarial examples is a crucial aspect of evaluating the robustness of deep learning systems, particularly in black-box scenarios. Although several methods have been proposed to enhance cross-model transferability, little attention has been paid to the transferability of adversarial examples across different tasks. This issue has become increasingly relevant with the emergence of foundational multi-task AI systems such as Visual ChatGPT, rendering the utility of adversarial samples generated by a single task relatively limited. Furthermore, these systems often entail inferential functions beyond mere recognition-like tasks. To address this gap, we propose a novel Visual Relation-based cross-task Adversarial Patch generation method called VRAP, which aims to evaluate the robustness of various visual tasks, especially those involving visual reasoning, such as Visual Question Answering and Image Captioning. VRAP employs scene graphs to combine object recognition-b
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;Bures-Wasserstein&#31354;&#38388;&#65292;&#24320;&#21457;&#20102;(&#38543;&#26426;)&#27491;&#21453;&#39640;&#26031;&#21464;&#20998;&#25512;&#26029;&#31639;&#27861;&#65292;&#21516;&#26102;&#20998;&#26512;&#20102;&#31639;&#27861;&#22312;&#19981;&#21516;&#20998;&#24067;&#19979;&#30340;&#25910;&#25947;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2304.05398</link><description>&lt;p&gt;
&#36890;&#36807;&#22312;Bures-Wasserstein&#31354;&#38388;&#20013;&#20351;&#29992;JKO&#36827;&#34892;&#27491;&#21453;&#39640;&#26031;&#21464;&#20998;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Forward-backward Gaussian variational inference via JKO in the Bures-Wasserstein Space. (arXiv:2304.05398v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05398
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;Bures-Wasserstein&#31354;&#38388;&#65292;&#24320;&#21457;&#20102;(&#38543;&#26426;)&#27491;&#21453;&#39640;&#26031;&#21464;&#20998;&#25512;&#26029;&#31639;&#27861;&#65292;&#21516;&#26102;&#20998;&#26512;&#20102;&#31639;&#27861;&#22312;&#19981;&#21516;&#20998;&#24067;&#19979;&#30340;&#25910;&#25947;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#25512;&#26029;&#26088;&#22312;&#36890;&#36807;&#21487;&#36861;&#28335;&#30340;&#20998;&#24067;&#26063;&#20013;&#30340;&#20803;&#32032;&#36924;&#36817;&#30446;&#26631;&#20998;&#24067;$\pi$&#12290;&#32479;&#35745;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20027;&#35201;&#20851;&#27880;&#28857;&#26159;&#39640;&#26031;&#21464;&#20998;&#25512;&#26029;&#65292;&#23427;&#36890;&#36807;&#22312;&#39640;&#26031;&#31354;&#38388;&#19978;&#26368;&#23567;&#21270;Kullback-Leibler&#25955;&#24230;&#21040;$\pi$&#26469;&#36924;&#36817;$\pi$&#12290;&#26412;&#25991;&#24320;&#21457;&#20102;(&#38543;&#26426;)&#27491;&#21453;&#39640;&#26031;&#21464;&#20998;&#25512;&#26029;(FB-GVI)&#31639;&#27861;&#26469;&#35299;&#20915;&#39640;&#26031;&#21464;&#20998;&#25512;&#26029;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;KL&#25955;&#24230;&#30340;&#22797;&#21512;&#32467;&#26500;&#65292;KL&#25955;&#24230;&#21487;&#34987;&#20889;&#20316;&#39640;&#26031;&#22312;Bures-Wasserstein(BW)&#31354;&#38388;&#19978;&#30340;&#21183;&#20989;&#25968;&#19982;&#29109;&#20989;&#25968;&#30340;&#21644;&#12290;&#23545;&#20110;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#65292;&#24403;$\pi$&#26159;log-smooth&#21644;log-concave&#26102;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#25910;&#25947;&#24615;&#20445;&#35777;&#65292;&#21516;&#26102;&#22312;$\pi$&#21482;&#26159;log-smooth&#26102;&#65292;&#25105;&#20204;&#39318;&#27425;&#33719;&#24471;&#20102;&#21040;&#19968;&#38454;&#31283;&#23450;&#35299;&#30340;&#25910;&#25947;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational inference (VI) seeks to approximate a target distribution $\pi$ by an element of a tractable family of distributions. Of key interest in statistics and machine learning is Gaussian VI, which approximates $\pi$ by minimizing the Kullback-Leibler (KL) divergence to $\pi$ over the space of Gaussians. In this work, we develop the (Stochastic) Forward-Backward Gaussian Variational Inference (FB-GVI) algorithm to solve Gaussian VI. Our approach exploits the composite structure of the KL divergence, which can be written as the sum of a smooth term (the potential) and a non-smooth term (the entropy) over the Bures-Wasserstein (BW) space of Gaussians endowed with the Wasserstein distance. For our proposed algorithm, we obtain state-of-the-art convergence guarantees when $\pi$ is log-smooth and log-concave, as well as the first convergence guarantees to first-order stationary solutions when $\pi$ is only log-smooth.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#37096;&#20998;&#21442;&#19982;&#29615;&#22659;&#19979;&#30340;&#28151;&#21512;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#21253;&#25324;&#33258;&#36866;&#24212;&#37319;&#26679;&#26041;&#27861;&#21644;&#37096;&#20998;&#32858;&#21512;&#26041;&#27861;&#65292;&#26377;&#21161;&#20110;&#21152;&#24555;&#27169;&#22411;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#20174;&#32780;&#20943;&#23569;&#36890;&#20449;&#25104;&#26412;&#19988;&#19981;&#24433;&#21709;&#27169;&#22411;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.05397</link><description>&lt;p&gt;
&#37096;&#20998;&#21442;&#19982;&#19979;&#21152;&#36895;&#28151;&#21512;&#32852;&#37030;&#23398;&#20064;&#30340;&#25910;&#25947;
&lt;/p&gt;
&lt;p&gt;
Accelerating Hybrid Federated Learning Convergence under Partial Participation. (arXiv:2304.05397v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#37096;&#20998;&#21442;&#19982;&#29615;&#22659;&#19979;&#30340;&#28151;&#21512;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#21253;&#25324;&#33258;&#36866;&#24212;&#37319;&#26679;&#26041;&#27861;&#21644;&#37096;&#20998;&#32858;&#21512;&#26041;&#27861;&#65292;&#26377;&#21161;&#20110;&#21152;&#24555;&#27169;&#22411;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#20174;&#32780;&#20943;&#23569;&#36890;&#20449;&#25104;&#26412;&#19988;&#19981;&#24433;&#21709;&#27169;&#22411;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#32852;&#37030;&#23398;&#20064;(Federated Learning&#65292;FL)&#24050;&#25104;&#20026;&#19968;&#31181;&#24120;&#35265;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#12290;FL&#28041;&#21450;&#19968;&#32452;&#26377;&#30528;&#20998;&#25955;&#25968;&#25454;&#30340;&#23458;&#25143;&#31471;&#65292;&#36890;&#36807;&#38598;&#20013;&#26381;&#21153;&#22120;&#30340;&#21327;&#35843;&#21512;&#20316;&#23398;&#20064;&#19968;&#20010;&#20844;&#20849;&#27169;&#22411;&#65292;&#20854;&#30446;&#30340;&#26159;&#36890;&#36807;&#30830;&#20445;&#26412;&#22320;&#25968;&#25454;&#38598;&#27704;&#36828;&#19981;&#20250;&#31163;&#24320;&#23458;&#25143;&#31471;&#65292;&#21482;&#26377;&#26381;&#21153;&#22120;&#36827;&#34892;&#27169;&#22411;&#32858;&#21512;&#26469;&#20445;&#25252;&#23458;&#25143;&#31471;&#30340;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#26381;&#21153;&#22120;&#21487;&#33021;&#33021;&#22815;&#25910;&#38598;&#23569;&#37327;&#25968;&#25454;&#20197;&#36817;&#20284;&#24635;&#20307;&#20998;&#24067;&#65292;&#24182;&#20855;&#26377;&#26356;&#24378;&#30340;&#35745;&#31639;&#33021;&#21147;&#26469;&#25191;&#34892;&#23398;&#20064;&#36807;&#31243;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#32858;&#28966;&#20110;&#28151;&#21512;FL&#26694;&#26550;&#12290;&#22312;&#20808;&#21069;&#28151;&#21512;FL&#24037;&#20316;&#20013;&#65292;&#24050;&#32463;&#35777;&#26126;&#20102;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#30340;&#20132;&#26367;&#35757;&#32451;&#21487;&#20197;&#22686;&#21152;&#25910;&#25947;&#36895;&#24230;&#65292;&#20294;&#26159;&#23427;&#20165;&#20851;&#27880;&#23458;&#25143;&#31471;&#23436;&#20840;&#21442;&#19982;&#30340;&#24773;&#20917;&#65292;&#32780;&#24573;&#30053;&#20102;&#37096;&#20998;&#21442;&#19982;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#37319;&#26679;&#26041;&#27861;&#21644;&#37096;&#20998;&#32858;&#21512;&#26041;&#27861;&#26469;&#25913;&#36827;&#28151;&#21512;FL&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20943;&#23569;&#36890;&#20449;&#25104;&#26412;&#30340;&#21516;&#26102;&#19981;&#20250;&#36896;&#25104;&#22826;&#22823;&#31934;&#24230;&#25439;&#22833;&#65292;&#19982;&#20256;&#32479;FL&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#22312;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past few years, Federated Learning (FL) has become a popular distributed machine learning paradigm. FL involves a group of clients with decentralized data who collaborate to learn a common model under the coordination of a centralized server, with the goal of protecting clients' privacy by ensuring that local datasets never leave the clients and that the server only performs model aggregation. However, in realistic scenarios, the server may be able to collect a small amount of data that approximately mimics the population distribution and has stronger computational ability to perform the learning process. To address this, we focus on the hybrid FL framework in this paper. While previous hybrid FL work has shown that the alternative training of clients and server can increase convergence speed, it has focused on the scenario where clients fully participate and ignores the negative effect of partial participation. In this paper, we provide theoretical analysis of hybrid FL under
&lt;/p&gt;</description></item><item><title>SAM.MD&#26159;Segment Anything Model&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#29256;&#26412;&#65292;&#21487;&#20197;&#36827;&#34892;&#38646;&#26679;&#26412;&#20998;&#21106;&#65292;&#34429;&#28982;&#24615;&#33021;&#19981;&#26159;&#26368;&#20248;&#65292;&#20294;&#26159;&#21487;&#20197;&#20026;&#21307;&#23398;&#39046;&#22495;&#30340;&#21322;&#33258;&#21160;&#20998;&#21106;&#24037;&#20855;&#25552;&#20379;&#28508;&#22312;&#20652;&#21270;&#21058;&#12290;</title><link>http://arxiv.org/abs/2304.05396</link><description>&lt;p&gt;
SAM.MD&#65306;Segment Anything Model&#30340;&#38646;&#26679;&#26412;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
SAM.MD: Zero-shot medical image segmentation capabilities of the Segment Anything Model. (arXiv:2304.05396v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05396
&lt;/p&gt;
&lt;p&gt;
SAM.MD&#26159;Segment Anything Model&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#29256;&#26412;&#65292;&#21487;&#20197;&#36827;&#34892;&#38646;&#26679;&#26412;&#20998;&#21106;&#65292;&#34429;&#28982;&#24615;&#33021;&#19981;&#26159;&#26368;&#20248;&#65292;&#20294;&#26159;&#21487;&#20197;&#20026;&#21307;&#23398;&#39046;&#22495;&#30340;&#21322;&#33258;&#21160;&#20998;&#21106;&#24037;&#20855;&#25552;&#20379;&#28508;&#22312;&#20652;&#21270;&#21058;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#30001;&#20110;&#25552;&#31034;&#30340;&#28789;&#27963;&#24615;&#24050;&#32463;&#21344;&#25454;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22270;&#20687;&#29983;&#25104;&#39046;&#22495;&#12290;&#38543;&#30528;Segment Anything Model&#65288;SAM&#65289;&#30340;&#26368;&#36817;&#25512;&#20986;&#65292;&#36825;&#31181;&#25552;&#31034;&#39537;&#21160;&#30340;&#33539;&#20363;&#24050;&#32463;&#36827;&#20837;&#20102;&#20855;&#26377;&#22823;&#37327;&#26410;&#24320;&#21457;&#33021;&#21147;&#30340;&#22270;&#20687;&#20998;&#21106;&#39046;&#22495;&#12290;&#26412;&#25991;&#30340;&#30446;&#30340;&#26159;&#36890;&#36807;&#35780;&#20272;SAM&#22312;&#33145;&#37096;CT&#22120;&#23448;&#20998;&#21106;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#36890;&#36807;&#28857;&#25110;&#36793;&#30028;&#26694;&#25552;&#31034;&#65292;&#23545;SAM&#30340;&#24320;&#31665;&#21363;&#29992;&#30340;&#38646;&#26679;&#26412;&#33021;&#21147;&#36827;&#34892;&#21021;&#27493;&#35780;&#20272;&#65292;&#23637;&#31034;SAM&#22312;CT&#25968;&#25454;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#33391;&#22909;&#65292;&#25104;&#20026;&#20419;&#36827;&#20020;&#24202;&#21307;&#29983;&#21322;&#33258;&#21160;&#20998;&#21106;&#24037;&#20855;&#36827;&#27493;&#30340;&#28508;&#22312;&#20652;&#21270;&#21058;&#12290;&#34429;&#28982;SAM&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#26410;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#20998;&#21106;&#24615;&#33021;&#65292;&#20294;&#25105;&#20204;&#30456;&#20449;&#36825;&#31181;&#22522;&#30784;&#27169;&#22411;&#21487;&#20197;&#20316;&#20026;&#21307;&#23398;&#39046;&#22495;&#27492;&#31867;&#27169;&#22411;&#36827;&#19968;&#27493;&#36866;&#24212;&#22797;&#26434;&#24615;&#30340;&#24378;&#22823;&#36215;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation models have taken over natural language processing and image generation domains due to the flexibility of prompting. With the recent introduction of the Segment Anything Model (SAM), this prompt-driven paradigm has entered image segmentation with a hitherto unexplored abundance of capabilities. The purpose of this paper is to conduct an initial evaluation of the out-of-the-box zero-shot capabilities of SAM for medical image segmentation, by evaluating its performance on an abdominal CT organ segmentation task, via point or bounding box based prompting. We show that SAM generalizes well to CT data, making it a potential catalyst for the advancement of semi-automatic segmentation tools for clinicians. We believe that this foundation model, while not reaching state-of-the-art segmentation performance in our investigations, can serve as a highly potent starting point for further adaptations of such models to the intricacies of the medical domain. Keywords: medical image segmenta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#25968;&#25454;&#22240;&#26524;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#21516;&#26102;&#22788;&#29702;&#19968;&#32452;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#29983;&#25104;&#19968;&#20010;&#21333;&#19968;&#30340;&#22240;&#26524;&#39537;&#21160;&#38598;&#65292;&#24182;&#19988;&#21487;&#20197;&#36807;&#28388;&#25481;&#22240;&#26524;&#34394;&#20551;&#38142;&#25509;&#65292;&#26368;&#32456;&#36755;&#20837;&#21040;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#39044;&#27979;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2304.05294</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#25968;&#25454;&#22240;&#26524;&#25512;&#26029;&#36873;&#25321;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;&#24378;&#20581;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Selecting Robust Features for Machine Learning Applications using Multidata Causal Discovery. (arXiv:2304.05294v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05294
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#25968;&#25454;&#22240;&#26524;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#21516;&#26102;&#22788;&#29702;&#19968;&#32452;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#29983;&#25104;&#19968;&#20010;&#21333;&#19968;&#30340;&#22240;&#26524;&#39537;&#21160;&#38598;&#65292;&#24182;&#19988;&#21487;&#20197;&#36807;&#28388;&#25481;&#22240;&#26524;&#34394;&#20551;&#38142;&#25509;&#65292;&#26368;&#32456;&#36755;&#20837;&#21040;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#39044;&#27979;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#20581;&#30340;&#29305;&#24449;&#36873;&#25321;&#23545;&#20110;&#21019;&#24314;&#21487;&#38752;&#21644;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#39046;&#22495;&#30693;&#35782;&#26377;&#38480;&#12289;&#28508;&#22312;&#20132;&#20114;&#26410;&#30693;&#30340;&#24773;&#20917;&#19979;&#35774;&#35745;&#32479;&#35745;&#39044;&#27979;&#27169;&#22411;&#26102;&#65292;&#36873;&#25321;&#26368;&#20248;&#29305;&#24449;&#38598;&#36890;&#24120;&#24456;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22810;&#25968;&#25454;&#65288;M&#65289;&#22240;&#26524;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#23427;&#21516;&#26102;&#22788;&#29702;&#19968;&#32452;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#24182;&#29983;&#25104;&#19968;&#20010;&#21333;&#19968;&#30340;&#22240;&#26524;&#39537;&#21160;&#38598;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;Tigramite Python&#21253;&#20013;&#23454;&#29616;&#30340;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;PC1&#25110;PCMCI&#12290;&#36825;&#20123;&#31639;&#27861;&#21033;&#29992;&#26465;&#20214;&#29420;&#31435;&#24615;&#27979;&#35797;&#25512;&#26029;&#22240;&#26524;&#22270;&#30340;&#37096;&#20998;&#12290;&#25105;&#20204;&#30340;&#22240;&#26524;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#22312;&#23558;&#21097;&#20313;&#22240;&#26524;&#29305;&#24449;&#20316;&#20026;&#36755;&#20837;&#20256;&#36882;&#32473;ML&#27169;&#22411;&#65288;&#22810;&#20803;&#32447;&#24615;&#22238;&#24402;&#65292;&#38543;&#26426;&#26862;&#26519;&#65289;&#39044;&#27979;&#30446;&#26631;&#20043;&#21069;&#65292;&#36807;&#28388;&#25481;&#22240;&#26524;&#34394;&#20551;&#38142;&#25509;&#12290;&#25105;&#20204;&#23558;&#35813;&#26694;&#26550;&#24212;&#29992;&#20110;&#39044;&#27979;&#35199;&#22826;&#24179;&#27915;&#28909;&#24102;&#22320;&#21306;&#30340;&#22320;&#38663;&#24378;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robust feature selection is vital for creating reliable and interpretable Machine Learning (ML) models. When designing statistical prediction models in cases where domain knowledge is limited and underlying interactions are unknown, choosing the optimal set of features is often difficult. To mitigate this issue, we introduce a Multidata (M) causal feature selection approach that simultaneously processes an ensemble of time series datasets and produces a single set of causal drivers. This approach uses the causal discovery algorithms PC1 or PCMCI that are implemented in the Tigramite Python package. These algorithms utilize conditional independence tests to infer parts of the causal graph. Our causal feature selection approach filters out causally-spurious links before passing the remaining causal features as inputs to ML models (Multiple linear regression, Random Forest) that predict the targets. We apply our framework to the statistical intensity prediction of Western Pacific Tropical
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;COTI&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#24341;&#20837;&#29702;&#35770;&#25351;&#23548;&#30340;&#25439;&#22833;&#30446;&#26631;&#21644;&#20840;&#38754;&#30340;&#21152;&#26435;&#35780;&#20998;&#26426;&#21046;&#65292;&#24182;&#32467;&#21512;&#20027;&#21160;&#23398;&#20064;&#33539;&#24335;&#26469;&#35299;&#20915;&#25991;&#26412;&#21453;&#36716;&#26102;&#30340;&#22256;&#38590;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#65292;&#25968;&#25454;&#25928;&#29575;&#39640;&#65292;&#26131;&#20110;&#20351;&#29992;&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2304.05265</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#21487;&#25511;&#25991;&#26412;&#21453;&#36716;
&lt;/p&gt;
&lt;p&gt;
Controllable Textual Inversion for Personalized Text-to-Image Generation. (arXiv:2304.05265v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05265
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;COTI&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#24341;&#20837;&#29702;&#35770;&#25351;&#23548;&#30340;&#25439;&#22833;&#30446;&#26631;&#21644;&#20840;&#38754;&#30340;&#21152;&#26435;&#35780;&#20998;&#26426;&#21046;&#65292;&#24182;&#32467;&#21512;&#20027;&#21160;&#23398;&#20064;&#33539;&#24335;&#26469;&#35299;&#20915;&#25991;&#26412;&#21453;&#36716;&#26102;&#30340;&#22256;&#38590;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#65292;&#25968;&#25454;&#25928;&#29575;&#39640;&#65292;&#26131;&#20110;&#20351;&#29992;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#22312;&#20197;&#25991;&#26412;&#20026;&#24341;&#23548;&#30340;&#39640;&#20445;&#30495;&#22270;&#20687;&#30340;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#24615;&#33021;&#12290;&#24403;&#24341;&#23548;&#20449;&#24687;&#21253;&#21547;&#29992;&#25143;&#23450;&#20041;&#30340;&#12289;&#26410;&#35265;&#36807;&#30340;&#25110;&#38271;&#23614;&#27010;&#24565;&#26631;&#35760;&#26102;&#65292;&#25991;&#26412;&#21453;&#36716;&#25104;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#20010;&#24615;&#21270;&#29983;&#25104;&#25216;&#26415;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#25105;&#20204;&#21457;&#29616;&#24182;&#23637;&#31034;&#20102;&#25991;&#26412;&#21453;&#36716;&#30340;&#37096;&#32626;&#20173;&#20805;&#28385;&#20102;&#8220;&#40657;&#39764;&#27861;&#8221;&#65292;&#20363;&#22914;&#39069;&#22806;&#25968;&#25454;&#38598;&#30340;&#20005;&#33499;&#35201;&#27714;&#65292;&#22312;&#24490;&#29615;&#20013;&#38656;&#35201;&#33392;&#33510;&#30340;&#20154;&#21147;&#25104;&#26412;&#21644;&#32570;&#20047;&#40065;&#26834;&#24615;&#31561;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21487;&#25511;&#25991;&#26412;&#21453;&#36716;&#30340;&#22823;&#22823;&#22686;&#24378;&#29256;&#21453;&#36716;&#65292;&#35299;&#20915;&#20102;&#25152;&#26377;&#19978;&#36848;&#38382;&#39064;&#65292;&#24182;&#21453;&#36807;&#26469;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#65292;&#25968;&#25454;&#25928;&#29575;&#39640;&#65292;&#26131;&#20110;&#20351;&#29992;&#30340;&#26694;&#26550;&#12290;COTI&#30340;&#26680;&#24515;&#26159;&#22522;&#20110;&#29702;&#35770;&#30340;&#25439;&#22833;&#30446;&#26631;&#65292;&#20855;&#26377;&#20840;&#38754;&#21644;&#26032;&#39062;&#30340;&#21152;&#26435;&#35780;&#20998;&#26426;&#21046;&#65292;&#24182;&#30001;&#20027;&#21160;&#23398;&#20064;&#33539;&#24335;&#25152;&#25552;&#21462;&#12290;&#24191;&#27867;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;COTI&#30340;&#24615;&#33021;&#27604;&#20043;&#21069;&#25216;&#26415;&#26377;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#65292;&#23588;&#20854;&#26159;&#22312;&#25968;&#25454;&#23569;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent large-scale generative modeling has attained unprecedented performance especially in producing high-fidelity images driven by text prompts. Text inversion (TI), alongside the text-to-image model backbones, is proposed as an effective technique in personalizing the generation when the prompts contain user-defined, unseen or long-tail concept tokens. Despite that, we find and show that the deployment of TI remains full of "dark-magics" -- to name a few, the harsh requirement of additional datasets, arduous human efforts in the loop and lack of robustness. In this work, we propose a much-enhanced version of TI, dubbed Controllable Textual Inversion (COTI), in resolving all the aforementioned problems and in turn delivering a robust, data-efficient and easy-to-use framework. The core to COTI is a theoretically-guided loss objective instantiated with a comprehensive and novel weighted scoring mechanism, encapsulated by an active-learning paradigm. The extensive results show that 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24191;&#20041;Softmax&#20989;&#25968;r-softmax&#65292;&#21487;&#20197;&#36755;&#20986;&#20855;&#26377;&#21487;&#25511;&#31232;&#30095;&#24230;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#30340;&#26367;&#20195;&#26041;&#26696;&#25928;&#26524;&#26356;&#22909;&#65292;&#22312;&#22810;&#26631;&#31614;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#31361;&#20986;&#65292;&#22312;&#39044;&#35757;&#32451;&#36716;&#25442;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#27880;&#24847;&#27169;&#22359;&#20013;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.05243</link><description>&lt;p&gt;
r-softmax: &#20855;&#26377;&#21487;&#25511;&#31232;&#30095;&#29575;&#30340;&#24191;&#20041;Softmax
&lt;/p&gt;
&lt;p&gt;
r-softmax: Generalized Softmax with Controllable Sparsity Rate. (arXiv:2304.05243v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05243
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24191;&#20041;Softmax&#20989;&#25968;r-softmax&#65292;&#21487;&#20197;&#36755;&#20986;&#20855;&#26377;&#21487;&#25511;&#31232;&#30095;&#24230;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#30340;&#26367;&#20195;&#26041;&#26696;&#25928;&#26524;&#26356;&#22909;&#65292;&#22312;&#22810;&#26631;&#31614;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#31361;&#20986;&#65292;&#22312;&#39044;&#35757;&#32451;&#36716;&#25442;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#27880;&#24847;&#27169;&#22359;&#20013;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#35768;&#22810;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;&#23558;&#27169;&#22411;&#25552;&#20379;&#30340;&#34920;&#31034;&#26144;&#23556;&#21040;&#27010;&#29575;&#20998;&#24067;&#30340;&#20989;&#25968;&#26159;&#28145;&#24230;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#30340;&#19981;&#21487;&#20998;&#21106;&#30340;&#26041;&#38754;&#12290;&#34429;&#28982;softmax&#26159;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#20013;&#36890;&#24120;&#25509;&#21463;&#30340;&#27010;&#29575;&#26144;&#23556;&#20989;&#25968;&#65292;&#20294;&#23427;&#19981;&#33021;&#36820;&#22238;&#31232;&#30095;&#30340;&#36755;&#20986;&#65292;&#24182;&#19988;&#24635;&#26159;&#23558;&#27491;&#27010;&#29575;&#20998;&#25955;&#21040;&#25152;&#26377;&#20301;&#32622;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;r-softmax&#65292;&#36825;&#26159;softmax&#30340;&#19968;&#31181;&#20462;&#25913;&#65292;&#23427;&#36755;&#20986;&#20855;&#26377;&#21487;&#25511;&#31232;&#30095;&#24230;&#30340;&#31232;&#30095;&#27010;&#29575;&#20998;&#24067;&#12290;&#19982;&#29616;&#26377;&#30340;&#31232;&#30095;&#27010;&#29575;&#26144;&#23556;&#20989;&#25968;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#30452;&#35266;&#30340;&#26426;&#21046;&#26469;&#25511;&#21046;&#36755;&#20986;&#31232;&#30095;&#24230;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22810;&#26631;&#31614;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;r-softmax&#20248;&#20110;&#20854;&#20182;&#31232;&#30095;&#30340;softmax&#26367;&#20195;&#26041;&#26696;&#65292;&#24182;&#19988;&#19982;&#21407;&#22987;&#30340;softmax&#30456;&#27604;&#20855;&#26377;&#39640;&#31454;&#20105;&#21147;&#12290;&#25105;&#20204;&#36824;&#23558;r-softmax&#24212;&#29992;&#20110;&#39044;&#35757;&#32451;&#36716;&#25442;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#27880;&#24847;&#27169;&#22359;&#20013;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays artificial neural network models achieve remarkable results in many disciplines. Functions mapping the representation provided by the model to the probability distribution are the inseparable aspect of deep learning solutions. Although softmax is a commonly accepted probability mapping function in the machine learning community, it cannot return sparse outputs and always spreads the positive probability to all positions. In this paper, we propose r-softmax, a modification of the softmax, outputting sparse probability distribution with controllable sparsity rate. In contrast to the existing sparse probability mapping functions, we provide an intuitive mechanism for controlling the output sparsity level. We show on several multi-label datasets that r-softmax outperforms other sparse alternatives to softmax and is highly competitive with the original softmax. We also apply r-softmax to the self-attention module of a pre-trained transformer language model and demonstrate that it l
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20043;&#21069;&#21387;&#32553;&#21367;&#31215;&#23618;&#30340;&#24352;&#37327;&#26684;&#24335;&#26041;&#27861;&#65292;&#36890;&#36807;&#26367;&#25442;&#22810;&#32500;&#26680;&#20026;&#19968;&#32500;&#28388;&#27874;&#22120;&#26469;&#20943;&#23569;CNN&#27169;&#22411;&#30340;&#22823;&#23567;&#21644;&#20943;&#23569;&#36807;&#24230;&#25311;&#21512;&#65292;&#20174;&#32780;&#25552;&#39640;&#23454;&#26102;&#20934;&#30830;&#39044;&#27979;&#30340;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.04964</link><description>&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#27874;&#24418;&#27169;&#25311;&#22120;&#20013;&#30340;&#20808;&#39564;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
A priori compression of convolutional neural networks for wave simulators. (arXiv:2304.04964v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04964
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20043;&#21069;&#21387;&#32553;&#21367;&#31215;&#23618;&#30340;&#24352;&#37327;&#26684;&#24335;&#26041;&#27861;&#65292;&#36890;&#36807;&#26367;&#25442;&#22810;&#32500;&#26680;&#20026;&#19968;&#32500;&#28388;&#27874;&#22120;&#26469;&#20943;&#23569;CNN&#27169;&#22411;&#30340;&#22823;&#23567;&#21644;&#20943;&#23569;&#36807;&#24230;&#25311;&#21512;&#65292;&#20174;&#32780;&#25552;&#39640;&#23454;&#26102;&#20934;&#30830;&#39044;&#27979;&#30340;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#29616;&#22312;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#65292;&#21253;&#25324;&#22270;&#20687;&#20998;&#31867;&#12289;&#38754;&#37096;&#21644;&#29289;&#20307;&#35782;&#21035;&#12289;&#21307;&#23398;&#25104;&#20687;&#20998;&#26512;&#31561;&#12290;&#27492;&#22806;&#65292;&#20687;&#29289;&#29702;&#20449;&#24687;&#27169;&#25311;&#22120;&#36825;&#26679;&#30340;&#24212;&#29992;&#38656;&#35201;&#23454;&#26102;&#20934;&#30830;&#39044;&#27979;&#65292;&#20294;&#29616;&#26377;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21253;&#21547;&#25968;&#20197;&#30334;&#19975;&#35745;&#30340;&#21442;&#25968;&#65292;&#36825;&#20351;&#24471;&#22312;&#20855;&#26377;&#26377;&#38480;&#20869;&#23384;&#30340;&#35774;&#22791;&#19978;&#23433;&#35013;&#27492;&#31867;&#27169;&#22411;&#21464;&#24471;&#22256;&#38590;&#12290;&#21387;&#32553;&#25216;&#26415;&#21487;&#33021;&#36890;&#36807;&#20943;&#23569;&#36129;&#29486;&#21040;&#27169;&#22411;&#22797;&#26434;&#24615;&#30340;&#21442;&#25968;&#25968;&#37327;&#26469;&#20943;&#23567;CNN&#27169;&#22411;&#30340;&#22823;&#23567;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21387;&#32553;&#30340;&#21367;&#31215;&#23618;&#24352;&#37327;&#26684;&#24335;&#65292;&#20808;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#36827;&#34892;&#12290;&#21367;&#31215;&#23618;&#20013;&#30340;&#19977;&#32500;&#25110;&#20108;&#32500;&#26680;&#34987;&#19968;&#32500;&#36807;&#28388;&#22120;&#26367;&#25442;&#12290;&#36807;&#24230;&#25311;&#21512;&#29616;&#35937;&#20063;&#23558;&#20943;&#23569;&#12290;&#39044;&#27979;&#25152;&#38656;&#30340;&#26102;&#38388;&#25110;&#35745;&#31639;&#26426;&#20223;&#30495;&#21644;&#39044;&#27979;&#25152;&#38656;&#30340;&#26102;&#38388;&#20063;&#23558;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutional neural networks are now seeing widespread use in a variety of fields, including image classification, facial and object recognition, medical imaging analysis, and many more. In addition, there are applications such as physics-informed simulators in which accurate forecasts in real time with a minimal lag are required. The present neural network designs include millions of parameters, which makes it difficult to install such complex models on devices that have limited memory. Compression techniques might be able to resolve these issues by decreasing the size of CNN models that are created by reducing the number of parameters that contribute to the complexity of the models. We propose a compressed tensor format of convolutional layer, a priori, before the training of the neural network. 3-way kernels or 2-way kernels in convolutional layers are replaced by one-way fiters. The overfitting phenomena will be reduced also. The time needed to make predictions or time required fo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;COhort Representation lEarning&#65288;CORE&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;EHR&#34920;&#31034;&#23398;&#20064;&#65292;&#25903;&#25345;&#38024;&#23545;&#19981;&#21516;&#38431;&#21015;&#30340;&#29305;&#24449;&#36827;&#34892;&#21487;&#35299;&#37322;&#24615;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2304.04468</link><description>&lt;p&gt;
&#23454;&#29616;&#38431;&#21015;&#26234;&#33021;&#21270;&#65306;&#19968;&#31181;&#38024;&#23545;&#30005;&#23376;&#30149;&#21382;&#20998;&#26512;&#30340;&#36890;&#29992;&#32676;&#20307;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Toward Cohort Intelligence: A Universal Cohort Representation Learning Framework for Electronic Health Record Analysis. (arXiv:2304.04468v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04468
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;COhort Representation lEarning&#65288;CORE&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;EHR&#34920;&#31034;&#23398;&#20064;&#65292;&#25903;&#25345;&#38024;&#23545;&#19981;&#21516;&#38431;&#21015;&#30340;&#29305;&#24449;&#36827;&#34892;&#21487;&#35299;&#37322;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#30149;&#21382;&#65288;EHR&#65289;&#26159;&#20174;&#20020;&#24202;&#24120;&#35268;&#25252;&#29702;&#20013;&#29983;&#25104;&#30340;&#65292;&#35760;&#24405;&#20102;&#24191;&#27867;&#30340;&#30149;&#20154;&#20154;&#32676;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#65292;&#20026;&#25913;&#21892;&#20020;&#24202;&#23454;&#36341;&#20013;&#30340;&#30149;&#20154;&#31649;&#29702;&#21644;&#24178;&#39044;&#31574;&#30053;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#26426;&#20250;&#12290;&#20026;&#20102;&#21033;&#29992;EHR&#25968;&#25454;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#26426;&#22120;&#23398;&#20064;&#20013;&#27969;&#34892;&#30340;EHR&#25968;&#25454;&#20998;&#26512;&#33539;&#24335;&#26159;EHR&#34920;&#31034;&#23398;&#20064;&#65292;&#23427;&#39318;&#20808;&#21033;&#29992;&#21333;&#20010;&#30149;&#20154;&#30340;EHR&#25968;&#25454;&#36890;&#36807;&#19968;&#20010;&#20027;&#24178;&#23398;&#20064;&#20449;&#24687;&#20016;&#23500;&#30340;&#34920;&#31034;&#65292;&#24182;&#25903;&#25345;&#24314;&#31435;&#22312;&#36825;&#20123;&#34920;&#31034;&#30340;&#22810;&#26679;&#21270;&#30340;&#21307;&#30103;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#33539;&#24335;&#26080;&#27861;&#28145;&#20837;&#20998;&#26512;&#30149;&#20154;&#30340;&#30456;&#20851;&#24615;&#65292;&#36890;&#24120;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#34987;&#31216;&#20026;&#38431;&#21015;&#30740;&#31350;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#21516;&#19968;&#38431;&#21015;&#20013;&#30340;&#30149;&#20154;&#20542;&#21521;&#20110;&#20855;&#26377;&#30456;&#20284;&#30340;&#29305;&#24449;&#65292;&#34920;&#26126;&#20182;&#20204;&#22312;&#21307;&#30103;&#26465;&#20214;&#65288;&#22914;&#30151;&#29366;&#25110;&#30142;&#30149;&#65289;&#26041;&#38754;&#20855;&#26377;&#30456;&#20284;&#20043;&#22788;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;COhort Representation lEarning (CORE)&#26694;&#26550;&#26469;&#22686;&#24378;EHR&#34920;&#31034;&#23398;&#20064;&#65292;&#36890;&#36807;&#20351;&#29992;&#38431;&#21015;&#34920;&#31034;&#23398;&#20064;&#31639;&#27861;&#65292;&#23545;&#32676;&#20307;&#20449;&#24687;&#36827;&#34892;&#24314;&#27169;&#24182;&#25903;&#25345;&#38024;&#23545;&#19981;&#21516;&#38431;&#21015;&#30340;&#29305;&#24449;&#36827;&#34892;&#21487;&#35299;&#37322;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electronic Health Records (EHR) are generated from clinical routine care recording valuable information of broad patient populations, which provide plentiful opportunities for improving patient management and intervention strategies in clinical practice. To exploit the enormous potential of EHR data, a popular EHR data analysis paradigm in machine learning is EHR representation learning, which first leverages the individual patient's EHR data to learn informative representations by a backbone, and supports diverse health-care downstream tasks grounded on the representations. Unfortunately, such a paradigm fails to access the in-depth analysis of patients' relevance, which is generally known as cohort studies in clinical practice. Specifically, patients in the same cohort tend to share similar characteristics, implying their resemblance in medical conditions such as symptoms or diseases. In this paper, we propose a universal COhort Representation lEarning (CORE) framework to augment EHR
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Sat-NeRF&#27169;&#22411;&#65292;&#33021;&#22815;&#20174;&#23569;&#37327;&#30340;&#21355;&#26143;&#22270;&#20687;&#38598;&#21512;&#20013;&#21512;&#25104;&#26032;&#30340;&#35270;&#35282;&#65292;&#24182;&#20934;&#30830;&#22320;&#20272;&#35745;&#22330;&#26223;&#34920;&#38754;&#30340;&#39640;&#31243;&#12290;</title><link>http://arxiv.org/abs/2304.04133</link><description>&lt;p&gt;
&#22522;&#20110;NeRF&#25216;&#26415;&#30340;&#21355;&#26143;&#22270;&#20687;&#34920;&#38754;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
NeRF applied to satellite imagery for surface reconstruction. (arXiv:2304.04133v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04133
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Sat-NeRF&#27169;&#22411;&#65292;&#33021;&#22815;&#20174;&#23569;&#37327;&#30340;&#21355;&#26143;&#22270;&#20687;&#38598;&#21512;&#20013;&#21512;&#25104;&#26032;&#30340;&#35270;&#35282;&#65292;&#24182;&#20934;&#30830;&#22320;&#20272;&#35745;&#22330;&#26223;&#34920;&#38754;&#30340;&#39640;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Sat-NeRF&#27169;&#22411;&#65292;&#26159;&#23545;&#26368;&#36817;&#24341;&#20837;&#30340;S-NeRF&#27169;&#22411;&#30340;&#20462;&#25913;&#23454;&#29616;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#20174;&#31232;&#30095;&#30340;&#21355;&#26143;&#22270;&#20687;&#38598;&#21512;&#20013;&#21512;&#25104;&#26032;&#30340;&#35270;&#35282;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#22270;&#29255;&#20013;&#30340;&#20809;&#29031;&#21464;&#21270;&#12290;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#36824;&#33021;&#22815;&#31934;&#30830;&#22320;&#20272;&#35745;&#22330;&#26223;&#34920;&#38754;&#30340;&#39640;&#31243;&#65292;&#36825;&#23545;&#21355;&#26143;&#35266;&#27979;&#24212;&#29992;&#38750;&#24120;&#26377;&#24110;&#21161;&#12290;S-NeRF&#26041;&#27861;&#25913;&#36827;&#20102;&#26631;&#20934;&#30340;NeRF&#26041;&#27861;&#65292;&#23558;&#36752;&#23556;&#24378;&#24230;&#32771;&#34385;&#20026;&#39640;&#21453;&#23556;&#29575;&#21644;&#20837;&#23556;&#36752;&#29031;&#24230;&#30340;&#20989;&#25968;&#12290;&#36825;&#20004;&#20010;&#37327;&#37117;&#26159;&#27169;&#22411;&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#26525;&#26465;&#30340;&#36755;&#20986;&#65292;&#32780;&#21518;&#32773;&#21017;&#34987;&#35270;&#20026;&#26469;&#33258;&#22826;&#38451;&#30340;&#30452;&#25509;&#20809;&#32447;&#21644;&#26469;&#33258;&#22825;&#31354;&#30340;&#28459;&#21453;&#23556;&#39068;&#33394;&#20989;&#25968;&#12290;&#35813;&#23454;&#29616;&#22522;&#20110;&#29992;&#32553;&#25918;-&#35009;&#21098;&#25216;&#26415;&#22686;&#24378;&#30340;&#21355;&#26143;&#22270;&#20687;&#25968;&#25454;&#38598;&#12290;&#23545;NeRF&#36827;&#34892;&#20102;&#36229;&#21442;&#25968;&#30740;&#31350;&#65292;&#24471;&#20986;&#20102;&#19968;&#20123;&#26377;&#36259;&#30340;&#35266;&#23519;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Sat-NeRF, a modified implementation of the recently introduced Shadow Neural Radiance Field (S-NeRF) model. This method is able to synthesize novel views from a sparse set of satellite images of a scene, while accounting for the variation in lighting present in the pictures. The trained model can also be used to accurately estimate the surface elevation of the scene, which is often a desirable quantity for satellite observation applications. S-NeRF improves on the standard Neural Radiance Field (NeRF) method by considering the radiance as a function of the albedo and the irradiance. Both these quantities are output by fully connected neural network branches of the model, and the latter is considered as a function of the direct light from the sun and the diffuse color from the sky. The implementations were run on a dataset of satellite images, augmented using a zoom-and-crop technique. A hyperparameter study for NeRF was carried out, leading to intriguing observations on the 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#21487;&#22797;&#21046;&#24615;&#21644;&#20840;&#23616;&#31283;&#23450;&#24615;&#65292;&#24182;&#35777;&#26126;&#35768;&#22810;&#23398;&#20064;&#20219;&#21153;&#21482;&#33021;&#24369;&#21270;&#22320;&#23454;&#29616;&#20840;&#23616;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.03757</link><description>&lt;p&gt;
&#23398;&#20064;&#20013;&#30340;&#21487;&#22797;&#21046;&#24615;&#21644;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Replicability and stability in learning. (arXiv:2304.03757v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03757
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#21487;&#22797;&#21046;&#24615;&#21644;&#20840;&#23616;&#31283;&#23450;&#24615;&#65292;&#24182;&#35777;&#26126;&#35768;&#22810;&#23398;&#20064;&#20219;&#21153;&#21482;&#33021;&#24369;&#21270;&#22320;&#23454;&#29616;&#20840;&#23616;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#22797;&#21046;&#24615;&#26159;&#31185;&#23398;&#20013;&#30340;&#20851;&#38190;&#65292;&#22240;&#20026;&#23427;&#20351;&#25105;&#20204;&#33021;&#22815;&#39564;&#35777;&#21644;&#39564;&#35777;&#30740;&#31350;&#32467;&#26524;&#12290;Impagliazzo&#12289;Lei&#12289;Pitassi&#21644;Sorrell&#65288;'22&#65289;&#26368;&#36817;&#24320;&#22987;&#30740;&#31350;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#21487;&#22797;&#21046;&#24615;&#12290;&#22914;&#26524;&#21516;&#19968;&#31639;&#27861;&#22312;&#20004;&#20010;&#29420;&#31435;&#21516;&#20998;&#24067;&#36755;&#20837;&#19978;&#20351;&#29992;&#30456;&#21516;&#30340;&#20869;&#37096;&#38543;&#26426;&#24615;&#26102;&#36890;&#24120;&#20135;&#29983;&#30456;&#21516;&#30340;&#36755;&#20986;&#65292;&#21017;&#23398;&#20064;&#31639;&#27861;&#26159;&#21487;&#22797;&#21046;&#30340;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#19981;&#28041;&#21450;&#22266;&#23450;&#38543;&#26426;&#24615;&#30340;&#21487;&#22797;&#21046;&#24615;&#21464;&#20307;&#12290;&#22914;&#26524;&#19968;&#20010;&#31639;&#27861;&#22312;&#20004;&#20010;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#36755;&#20837;&#19978;&#65288;&#19981;&#22266;&#23450;&#20869;&#37096;&#38543;&#26426;&#24615;&#65289;&#24212;&#29992;&#26102;&#36890;&#24120;&#20135;&#29983;&#30456;&#21516;&#30340;&#36755;&#20986;&#65292;&#21017;&#31639;&#27861;&#28385;&#36275;&#36825;&#31181;&#24418;&#24335;&#30340;&#21487;&#22797;&#21046;&#24615;&#12290;&#36825;&#20010;&#21464;&#31181;&#34987;&#31216;&#20026;&#20840;&#23616;&#31283;&#23450;&#24615;&#65292;&#24182;&#22312;&#24046;&#20998;&#38544;&#31169;&#30340;&#19978;&#19979;&#25991;&#20013;&#30001;Bun&#12289;Livni&#21644;Moran&#65288;'20&#65289;&#20171;&#32461;&#12290; Impagliazzo&#31561;&#20154;&#23637;&#31034;&#20102;&#22914;&#20309;&#25552;&#39640;&#20219;&#20309;&#21487;&#22797;&#21046;&#31639;&#27861;&#30340;&#25928;&#26524;&#65292;&#20197;&#20351;&#20854;&#20135;&#29983;&#30340;&#36755;&#20986;&#27010;&#29575;&#26080;&#38480;&#25509;&#36817;&#20110;1&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#35768;&#22810;&#23398;&#20064;&#20219;&#21153;&#65292;&#21482;&#33021;&#24369;&#21270;&#22320;&#23454;&#29616;&#20840;&#23616;&#31283;&#23450;&#24615;&#65292;&#36825;&#37324;&#36755;&#20986;&#21482;&#26377;&#30456;&#21516;&#30340;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Replicability is essential in science as it allows us to validate and verify research findings. Impagliazzo, Lei, Pitassi and Sorrell (`22) recently initiated the study of replicability in machine learning. A learning algorithm is replicable if it typically produces the same output when applied on two i.i.d. inputs using the same internal randomness. We study a variant of replicability that does not involve fixing the randomness. An algorithm satisfies this form of replicability if it typically produces the same output when applied on two i.i.d. inputs (without fixing the internal randomness). This variant is called global stability and was introduced by Bun, Livni and Moran (`20) in the context of differential privacy.  Impagliazzo et al. showed how to boost any replicable algorithm so that it produces the same output with probability arbitrarily close to 1. In contrast, we demonstrate that for numerous learning tasks, global stability can only be accomplished weakly, where the same o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#38750;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#30340;&#33258;&#36866;&#24212;&#23398;&#29983;t&#20998;&#24067;&#26041;&#27861;&#65292;&#22522;&#20110;&#26041;&#27861;&#30340;&#19968;&#33324;&#33258;&#36866;&#24212;&#30697;&#21487;&#20197;&#20351;&#29992;&#24265;&#20215;&#30340;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#20540;&#65288;EMA&#65289;&#26469;&#20272;&#35745;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2304.03069</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#23398;&#29983;t&#20998;&#24067;&#19982;&#26041;&#27861;&#30697;&#31227;&#21160;&#20272;&#35745;&#22120;&#29992;&#20110;&#38750;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
Adaptive Student's t-distribution with method of moments moving estimator for nonstationary time series. (arXiv:2304.03069v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03069
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#38750;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#30340;&#33258;&#36866;&#24212;&#23398;&#29983;t&#20998;&#24067;&#26041;&#27861;&#65292;&#22522;&#20110;&#26041;&#27861;&#30340;&#19968;&#33324;&#33258;&#36866;&#24212;&#30697;&#21487;&#20197;&#20351;&#29992;&#24265;&#20215;&#30340;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#20540;&#65288;EMA&#65289;&#26469;&#20272;&#35745;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30495;&#23454;&#30340;&#26102;&#38388;&#24207;&#21015;&#36890;&#24120;&#26159;&#38750;&#24179;&#31283;&#30340;&#65292;&#36825;&#24102;&#26469;&#20102;&#27169;&#22411;&#36866;&#24212;&#30340;&#38590;&#39064;&#12290;&#20256;&#32479;&#26041;&#27861;&#22914;GARCH&#20551;&#23450;&#20219;&#24847;&#31867;&#22411;&#30340;&#20381;&#36182;&#24615;&#12290;&#20026;&#20102;&#36991;&#20813;&#36825;&#31181;&#20559;&#24046;&#65292;&#25105;&#20204;&#23558;&#30528;&#30524;&#20110;&#26368;&#36817;&#25552;&#20986;&#30340;&#19981;&#21487;&#30693;&#30340;&#31227;&#21160;&#20272;&#35745;&#22120;&#21746;&#23398;&#65306;&#22312;&#26102;&#38388;$t$&#25214;&#21040;&#20248;&#21270;$F_t=\sum_{\tau&lt;t} (1-\eta)^{t-\tau} \ln(\rho_\theta (x_\tau))$&#31227;&#21160;&#23545;&#25968;&#20284;&#28982;&#30340;&#21442;&#25968;&#65292;&#38543;&#26102;&#38388;&#28436;&#21270;&#12290;&#20363;&#22914;&#65292;&#23427;&#20801;&#35768;&#20351;&#29992;&#24265;&#20215;&#30340;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#20540;&#65288;EMA&#65289;&#26469;&#20272;&#35745;&#21442;&#25968;&#65292;&#20363;&#22914;&#32477;&#23545;&#20013;&#24515;&#30697;$E[|x-\mu|^p]$&#38543;$p\in\mathbb{R}^+$&#30340;&#21464;&#21270;&#32780;&#28436;&#21270;$m_{p,t+1} = m_{p,t} + \eta (|x_t-\mu_t|^p-m_{p,t})$&#12290;&#36825;&#31181;&#22522;&#20110;&#26041;&#27861;&#30340;&#19968;&#33324;&#33258;&#36866;&#24212;&#30697;&#30340;&#24212;&#29992;&#23558;&#21576;&#29616;&#22312;&#23398;&#29983;t&#20998;&#24067;&#19978;&#65292;&#23588;&#20854;&#26159;&#22312;&#32463;&#27982;&#24212;&#29992;&#20013;&#27969;&#34892;&#65292;&#36825;&#37324;&#24212;&#29992;&#20110;DJIA&#20844;&#21496;&#30340;&#23545;&#25968;&#25910;&#30410;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The real life time series are usually nonstationary, bringing a difficult question of model adaptation. Classical approaches like GARCH assume arbitrary type of dependence. To prevent such bias, we will focus on recently proposed agnostic philosophy of moving estimator: in time $t$ finding parameters optimizing e.g. $F_t=\sum_{\tau&lt;t} (1-\eta)^{t-\tau} \ln(\rho_\theta (x_\tau))$ moving log-likelihood, evolving in time. It allows for example to estimate parameters using inexpensive exponential moving averages (EMA), like absolute central moments $E[|x-\mu|^p]$ evolving with $m_{p,t+1} = m_{p,t} + \eta (|x_t-\mu_t|^p-m_{p,t})$ for one or multiple powers $p\in\mathbb{R}^+$. Application of such general adaptive methods of moments will be presented on Student's t-distribution, popular especially in economical applications, here applied to log-returns of DJIA companies.
&lt;/p&gt;</description></item><item><title>HATELEXICON&#26159;&#19968;&#20010;&#21253;&#21547;&#24052;&#35199;&#65292;&#24503;&#22269;&#65292;&#21360;&#24230;&#21644;&#32943;&#23612;&#20122;&#20167;&#24680;&#35328;&#35770;&#30340;&#35789;&#27719;&#34920;&#65292;&#21033;&#29992;&#20854;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#22312;&#35757;&#32451;&#20013;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.01890</link><description>&lt;p&gt;
&#31038;&#20250;&#25991;&#21270;&#30693;&#35782;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#20219;&#21153;&#20013;&#23545;&#36873;&#39033;&#30340;&#36873;&#25321;&#26159;&#24517;&#35201;&#30340;
&lt;/p&gt;
&lt;p&gt;
Sociocultural knowledge is needed for selection of shots in hate speech detection tasks. (arXiv:2304.01890v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01890
&lt;/p&gt;
&lt;p&gt;
HATELEXICON&#26159;&#19968;&#20010;&#21253;&#21547;&#24052;&#35199;&#65292;&#24503;&#22269;&#65292;&#21360;&#24230;&#21644;&#32943;&#23612;&#20122;&#20167;&#24680;&#35328;&#35770;&#30340;&#35789;&#27719;&#34920;&#65292;&#21033;&#29992;&#20854;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#22312;&#35757;&#32451;&#20013;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;HATELEXICON&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#24052;&#35199;&#65292;&#24503;&#22269;&#65292;&#21360;&#24230;&#21644;&#32943;&#23612;&#20122;&#30340;&#34065;&#31216;&#21644;&#20167;&#24680;&#35328;&#35770;&#30446;&#26631;&#30340;&#35789;&#27719;&#34920;&#65292;&#20197;&#24110;&#21161;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#35789;&#27719;&#34920;&#22914;&#20309;&#29992;&#20110;&#35299;&#37322;&#27169;&#22411;&#39044;&#27979;&#65292;&#34920;&#26126;&#21457;&#23637;&#29992;&#20110;&#20998;&#31867;&#26497;&#31471;&#35328;&#35770;&#30340;&#27169;&#22411;&#65292;&#22312;&#36827;&#34892;&#39044;&#27979;&#26102;&#20005;&#37325;&#20381;&#36182;&#30446;&#26631;&#35789;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;HATELEXICON&#26469;&#36741;&#21161;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#35757;&#32451;&#36873;&#39033;&#30340;&#26041;&#27861;&#65292;&#36873;&#39033;&#36873;&#25321;&#22312;&#23567;&#26679;&#26412;&#23398;&#20064;&#20013;&#23588;&#20026;&#37325;&#35201;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;HASOC&#25968;&#25454;&#23545;&#24503;&#35821;&#21644;&#21360;&#22320;&#35821;&#36827;&#34892;&#20102;&#20960;&#20010;&#31034;&#33539;&#23398;&#20064;&#65292;&#24182;&#23558;Multilingual HateCheck&#65288;MHC&#65289;&#20316;&#20026;&#22522;&#20934;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26681;&#25454;&#25105;&#20204;&#30340;&#35789;&#27719;&#34920;&#36873;&#25321;&#26679;&#26412;&#65292;&#30456;&#23545;&#20110;&#38543;&#26426;&#37319;&#26679;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#22312;MHC&#19978;&#34920;&#29616;&#12290;&#22240;&#27492;&#65292;&#24403;&#20165;&#26377;&#23569;&#37327;&#30340;&#35757;&#32451;&#26679;&#26412;&#26102;&#65292;&#20351;&#29992;&#25105;&#20204;&#30340;&#35789;&#27719;&#34920;&#26469;&#36873;&#25321;&#21253;&#21547;&#26356;&#22810;&#31038;&#20250;&#25991;&#21270;&#20449;&#24687;&#30340;&#26679;&#26412;&#33021;&#22815;&#26356;&#22909;&#22320;&#25552;&#39640;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce HATELEXICON, a lexicon of slurs and targets of hate speech for the countries of Brazil, Germany, India and Kenya, to aid training and interpretability of models. We demonstrate how our lexicon can be used to interpret model predictions, showing that models developed to classify extreme speech rely heavily on target words when making predictions. Further, we propose a method to aid shot selection for training in low-resource settings via HATELEXICON. In few-shot learning, the selection of shots is of paramount importance to model performance. In our work, we simulate a few-shot setting for German and Hindi, using HASOC data for training and the Multilingual HateCheck (MHC) as a benchmark. We show that selecting shots based on our lexicon leads to models performing better on MHC than models trained on shots sampled randomly. Thus, when given only a few training examples, using our lexicon to select shots containing more sociocultural information leads to better few-shot perf
&lt;/p&gt;</description></item><item><title>FedIN&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#25903;&#25345;&#24322;&#26500;&#27169;&#22411;&#65292;&#26080;&#38656;&#20844;&#20849;&#25968;&#25454;&#38598;&#12290;&#22312;FedIN&#20013;&#65292;&#25552;&#21462;&#22120;&#21644;&#20998;&#31867;&#22120;&#30340;&#27169;&#22411;&#32467;&#26500;&#22312;&#25152;&#26377;&#35774;&#22791;&#20013;&#37117;&#30456;&#21516;&#65292;&#32780;&#20013;&#38388;&#23618;&#30340;&#26550;&#26500;&#21487;&#20197;&#26681;&#25454;&#24322;&#26500;&#35774;&#22791;&#30340;&#36164;&#28304;&#23481;&#37327;&#32780;&#21464;&#21270;&#12290;IN&#35757;&#32451;&#21487;&#29992;&#20110;&#21033;&#29992;&#29305;&#24449;&#30693;&#35782;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.00759</link><description>&lt;p&gt;
FedIN&#65306;&#29992;&#20110;&#27169;&#22411;&#24322;&#26500;&#30340;&#32852;&#37030;&#20013;&#38388;&#23618;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedIN: Federated Intermediate Layers Learning for Model Heterogeneity. (arXiv:2304.00759v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00759
&lt;/p&gt;
&lt;p&gt;
FedIN&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#25903;&#25345;&#24322;&#26500;&#27169;&#22411;&#65292;&#26080;&#38656;&#20844;&#20849;&#25968;&#25454;&#38598;&#12290;&#22312;FedIN&#20013;&#65292;&#25552;&#21462;&#22120;&#21644;&#20998;&#31867;&#22120;&#30340;&#27169;&#22411;&#32467;&#26500;&#22312;&#25152;&#26377;&#35774;&#22791;&#20013;&#37117;&#30456;&#21516;&#65292;&#32780;&#20013;&#38388;&#23618;&#30340;&#26550;&#26500;&#21487;&#20197;&#26681;&#25454;&#24322;&#26500;&#35774;&#22791;&#30340;&#36164;&#28304;&#23481;&#37327;&#32780;&#21464;&#21270;&#12290;IN&#35757;&#32451;&#21487;&#29992;&#20110;&#21033;&#29992;&#29305;&#24449;&#30693;&#35782;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20351;&#24471;&#36793;&#32536;&#35774;&#22791;&#33021;&#22815;&#21512;&#20316;&#35757;&#32451;&#20840;&#23616;&#20849;&#20139;&#27169;&#22411;&#65292;&#21516;&#26102;&#22312;&#26412;&#22320;&#21644;&#31169;&#23494;&#22320;&#20445;&#30041;&#35757;&#32451;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#22312;FL&#20013;&#19968;&#20010;&#26222;&#36941;&#20294;&#19981;&#20999;&#23454;&#38469;&#30340;&#20551;&#35774;&#26159;&#21442;&#19982;&#36793;&#32536;&#35774;&#22791;&#25317;&#26377;&#30456;&#21516;&#30340;&#24517;&#38656;&#36164;&#28304;&#24182;&#20849;&#20139;&#30456;&#21516;&#30340;&#20840;&#23616;&#27169;&#22411;&#26550;&#26500;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Federated Intermediate Layers Learning&#65288;FedIN&#65289;&#30340;&#26032;&#22411;FL&#26041;&#27861;&#65292;&#25903;&#25345;&#24322;&#26500;&#27169;&#22411;&#32780;&#19981;&#20351;&#29992;&#20219;&#20309;&#20844;&#20849;&#25968;&#25454;&#38598;&#12290;FedIN&#20013;&#30340;&#35757;&#32451;&#27169;&#22411;&#20998;&#20026;&#19977;&#37096;&#20998;&#65292;&#21253;&#25324;&#25552;&#21462;&#22120;&#12289;&#20013;&#38388;&#23618;&#21644;&#20998;&#31867;&#22120;&#12290;&#25552;&#21462;&#22120;&#21644;&#20998;&#31867;&#22120;&#30340;&#27169;&#22411;&#32467;&#26500;&#22312;&#25152;&#26377;&#35774;&#22791;&#20013;&#37117;&#30456;&#21516;&#65292;&#20197;&#20445;&#25345;&#20013;&#38388;&#23618;&#29305;&#24449;&#30340;&#19968;&#33268;&#24615;&#65292;&#32780;&#20013;&#38388;&#23618;&#30340;&#26550;&#26500;&#21487;&#20197;&#26681;&#25454;&#24322;&#26500;&#35774;&#22791;&#30340;&#36164;&#28304;&#23481;&#37327;&#32780;&#21464;&#21270;&#12290;&#20026;&#20102;&#21033;&#29992;&#29305;&#24449;&#30693;&#35782;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;IN&#35757;&#32451;&#65292;&#20197;IN&#26631;&#20934;&#21270;&#20026;&#22522;&#30784;&#35757;&#32451;&#20013;&#38388;&#23618;&#12290;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#31934;&#24230;&#21644;&#25928;&#29575;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) facilitates edge devices to cooperatively train a global shared model while maintaining the training data locally and privately. However, a common but impractical assumption in FL is that the participating edge devices possess the same required resources and share identical global model architecture. In this study, we propose a novel FL method called Federated Intermediate Layers Learning (FedIN), supporting heterogeneous models without utilizing any public dataset. The training models in FedIN are divided into three parts, including an extractor, the intermediate layers, and a classifier. The model architectures of the extractor and classifier are the same in all devices to maintain the consistency of the intermediate layer features, while the architectures of the intermediate layers can vary for heterogeneous devices according to their resource capacities. To exploit the knowledge from features, we propose IN training, training the intermediate layers in line 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#37327;&#21270;&#19981;&#21516;&#21306;&#22495;&#23545;&#30446;&#26631;&#35782;&#21035;&#30340;&#36129;&#29486;&#21644;&#35299;&#37322;&#25968;&#25454;&#20559;&#24046;&#21644;&#27169;&#22411;&#20559;&#24046;&#23545;&#38750;&#22240;&#26524;&#24615;&#30340;&#24433;&#21709;&#65292;&#25552;&#20379;&#20102;&#25913;&#21892;&#28145;&#24230;&#23398;&#20064;&#22312;SAR ATR&#20013;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#30340;&#37325;&#35201;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2304.00668</link><description>&lt;p&gt;
&#22312;SAR ATR&#20013;&#21457;&#29616;&#21644;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#30340;&#38750;&#22240;&#26524;&#24615;
&lt;/p&gt;
&lt;p&gt;
Discovering and Explaining the Non-Causality of Deep Learning in SAR ATR. (arXiv:2304.00668v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00668
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#37327;&#21270;&#19981;&#21516;&#21306;&#22495;&#23545;&#30446;&#26631;&#35782;&#21035;&#30340;&#36129;&#29486;&#21644;&#35299;&#37322;&#25968;&#25454;&#20559;&#24046;&#21644;&#27169;&#22411;&#20559;&#24046;&#23545;&#38750;&#22240;&#26524;&#24615;&#30340;&#24433;&#21709;&#65292;&#25552;&#20379;&#20102;&#25913;&#21892;&#28145;&#24230;&#23398;&#20064;&#22312;SAR ATR&#20013;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#30340;&#37325;&#35201;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;SAR ATR&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#24182;&#22312;MSTAR&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21463;&#38480;&#30340;&#25104;&#20687;&#26465;&#20214;&#65292;MSTAR&#23384;&#22312;&#32972;&#26223;&#30456;&#20851;&#31561;&#25968;&#25454;&#20559;&#35265;&#65292;&#21363;&#32972;&#26223;&#26434;&#27874;&#29305;&#24615;&#19982;&#30446;&#26631;&#31867;&#21035;&#23384;&#22312;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;&#28145;&#24230;&#23398;&#20064;&#21487;&#20197;&#36807;&#24230;&#25311;&#21512;&#26434;&#27874;&#20197;&#20943;&#23569;&#35757;&#32451;&#35823;&#24046;&#12290;&#22240;&#27492;&#65292;&#26434;&#27874;&#30340;&#36807;&#24230;&#25311;&#21512;&#31243;&#24230;&#21453;&#26144;&#20102;SAR ATR&#20013;&#28145;&#24230;&#23398;&#20064;&#30340;&#38750;&#22240;&#26524;&#24615;&#12290;&#29616;&#26377;&#26041;&#27861;&#20165;&#23450;&#24615;&#20998;&#26512;&#27492;&#29616;&#35937;&#12290;&#26412;&#25991;&#22522;&#20110;Shapley&#20540;&#37327;&#21270;&#19981;&#21516;&#21306;&#22495;&#23545;&#30446;&#26631;&#35782;&#21035;&#30340;&#36129;&#29486;&#12290;&#26434;&#27874;&#30340;Shapley&#20540;&#21487;&#20197;&#34913;&#37327;&#20854;&#36807;&#24230;&#25311;&#21512;&#31243;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;&#25968;&#25454;&#20559;&#24046;&#21644;&#27169;&#22411;&#20559;&#24046;&#23545;&#38750;&#22240;&#26524;&#24615;&#30340;&#24433;&#21709;&#12290;&#31616;&#35328;&#20043;&#65292;&#25968;&#25454;&#20559;&#24046;&#23548;&#33268;&#35757;&#32451;&#38598;&#21644;&#27979;&#35797;&#38598;&#20013;&#30340;&#20449;&#26434;&#27604;&#21644;&#26434;&#27874;&#32441;&#29702;&#21487;&#27604;&#65292;&#32780;&#21508;&#31181;&#27169;&#22411;&#32467;&#26500;&#23545;&#36825;&#20123;&#20559;&#24046;&#30340;&#36807;&#25311;&#21512;&#31243;&#24230;&#19981;&#21516;&#12290;&#38750;&#22240;&#26524;&#24615;&#30340;&#35299;&#37322;&#20026;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#22312;SAR ATR&#20013;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, deep learning has been widely used in SAR ATR and achieved excellent performance on the MSTAR dataset. However, due to constrained imaging conditions, MSTAR has data biases such as background correlation, i.e., background clutter properties have a spurious correlation with target classes. Deep learning can overfit clutter to reduce training errors. Therefore, the degree of overfitting for clutter reflects the non-causality of deep learning in SAR ATR. Existing methods only qualitatively analyze this phenomenon. In this paper, we quantify the contributions of different regions to target recognition based on the Shapley value. The Shapley value of clutter measures the degree of overfitting. Moreover, we explain how data bias and model bias contribute to non-causality. Concisely, data bias leads to comparable signal-to-clutter ratios and clutter textures in training and test sets. And various model structures have different degrees of overfitting for these biases. The exp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#25193;&#25955;&#27169;&#22411;&#30340;&#21442;&#25968;&#25928;&#29575;&#24494;&#35843;&#65292;&#36890;&#36807;&#25554;&#20837;&#23567;&#22411;&#21487;&#23398;&#20064;&#27169;&#22359;&#26469;&#23454;&#29616;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#36866;&#37197;&#22120;&#30340;&#36755;&#20837;&#20301;&#32622;&#26159;&#24433;&#21709;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#32780;&#23558;&#36755;&#20837;&#20301;&#32622;&#25918;&#22312;&#20132;&#21449;&#27880;&#24847;&#21147;&#22359;&#20043;&#21518;&#21487;&#33719;&#24471;&#26368;&#20339;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.18181</link><description>&lt;p&gt;
&#20851;&#20110;&#25193;&#25955;&#27169;&#22411;&#21442;&#25968;&#25928;&#29575;&#24494;&#35843;&#30340;&#36827;&#19968;&#27493;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Closer Look at Parameter-Efficient Tuning in Diffusion Models. (arXiv:2303.18181v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#25193;&#25955;&#27169;&#22411;&#30340;&#21442;&#25968;&#25928;&#29575;&#24494;&#35843;&#65292;&#36890;&#36807;&#25554;&#20837;&#23567;&#22411;&#21487;&#23398;&#20064;&#27169;&#22359;&#26469;&#23454;&#29616;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#36866;&#37197;&#22120;&#30340;&#36755;&#20837;&#20301;&#32622;&#26159;&#24433;&#21709;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#32780;&#23558;&#36755;&#20837;&#20301;&#32622;&#25918;&#22312;&#20132;&#21449;&#27880;&#24847;&#21147;&#22359;&#20043;&#21518;&#21487;&#33719;&#24471;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#25193;&#25955;&#27169;&#22411;&#22914;Stable Diffusion&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#37117;&#34920;&#29616;&#20986;&#36275;&#22815;&#30340;&#24378;&#22823;&#65292;&#28982;&#32780;&#22312;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#20010;&#24615;&#21270;&#24494;&#35843;&#26102;&#21364;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#20869;&#23384;&#21644;&#26102;&#38388;&#12290;&#21463;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#36817;&#26399;&#30340;&#36827;&#23637;&#21551;&#21457;&#65292;&#25105;&#20204;&#36890;&#36807;&#25554;&#20837;&#23567;&#22411;&#21487;&#23398;&#20064;&#27169;&#22359;(&#31216;&#20316;&#36866;&#37197;&#22120;)&#26469;&#30740;&#31350;&#22823;&#35268;&#27169;&#25193;&#25955;&#27169;&#22411;&#30340;&#21442;&#25968;&#25928;&#29575;&#24494;&#35843;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#36866;&#37197;&#22120;&#30340;&#35774;&#35745;&#31354;&#38388;&#20998;&#35299;&#20026;&#27491;&#20132;&#22240;&#23376;&#8212;&#8212;&#36755;&#20837;&#20301;&#32622;&#12289;&#36755;&#20986;&#20301;&#32622;&#20197;&#21450;&#20989;&#25968;&#24418;&#24335;&#65292;&#24182;&#36827;&#34892;ANOVA&#20998;&#26512;&#65292;&#19968;&#31181;&#20998;&#26512;&#31163;&#25955;(&#35774;&#35745;&#36873;&#39033;)&#19982;&#36830;&#32493;&#21464;&#37327;(&#35780;&#20272;&#25351;&#26631;)&#20043;&#38388;&#30456;&#20851;&#24615;&#30340;&#32463;&#20856;&#32479;&#35745;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#36866;&#37197;&#22120;&#30340;&#36755;&#20837;&#20301;&#32622;&#26159;&#24433;&#21709;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#20180;&#32454;&#30740;&#31350;&#20102;&#36755;&#20837;&#20301;&#32622;&#30340;&#36873;&#25321;&#65292;&#21457;&#29616;&#23558;&#36755;&#20837;&#20301;&#32622;&#25918;&#22312;&#20132;&#21449;&#27880;&#24847;&#21147;&#22359;&#21518;&#21487;&#20197;&#20351;&#24615;&#33021;&#36798;&#21040;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale diffusion models like Stable Diffusion are powerful and find various real-world applications while customizing such models by fine-tuning is both memory and time inefficient. Motivated by the recent progress in natural language processing, we investigate parameter-efficient tuning in large diffusion models by inserting small learnable modules (termed adapters). In particular, we decompose the design space of adapters into orthogonal factors -- the input position, the output position as well as the function form, and perform Analysis of Variance (ANOVA), a classical statistical approach for analyzing the correlation between discrete (design options) and continuous variables (evaluation metrics). Our analysis suggests that the input position of adapters is the critical factor influencing the performance of downstream tasks. Then, we carefully study the choice of the input position, and we find that putting the input position after the cross-attention block can lead to the bes
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#39537;&#21160;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#20026;silent&#35270;&#39057;&#21098;&#36753;&#21512;&#25104;&#39640;&#20445;&#30495;&#30340;&#20914;&#20987;&#22768;&#65292;&#24182;&#20351;&#29992;&#39069;&#22806;&#30340;&#29289;&#29702;&#20808;&#39564;&#30693;&#35782;&#26469;&#25351;&#23548;&#20914;&#20987;&#22768;&#21512;&#25104;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2303.16897</link><description>&lt;p&gt;
&#29289;&#29702;&#39537;&#21160;&#30340;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#20174;&#35270;&#39057;&#20013;&#21512;&#25104;&#20914;&#20987;&#22768;
&lt;/p&gt;
&lt;p&gt;
Physics-Driven Diffusion Models for Impact Sound Synthesis from Videos. (arXiv:2303.16897v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16897
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#39537;&#21160;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#20026;silent&#35270;&#39057;&#21098;&#36753;&#21512;&#25104;&#39640;&#20445;&#30495;&#30340;&#20914;&#20987;&#22768;&#65292;&#24182;&#20351;&#29992;&#39069;&#22806;&#30340;&#29289;&#29702;&#20808;&#39564;&#30693;&#35782;&#26469;&#25351;&#23548;&#20914;&#20987;&#22768;&#21512;&#25104;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#29289;&#20307;&#30456;&#20114;&#20316;&#29992;&#21457;&#20986;&#30340;&#22768;&#38899;&#36827;&#34892;&#24314;&#27169;&#23545;&#20110;&#23454;&#38469;&#19990;&#30028;&#21644;&#34394;&#25311;&#19990;&#30028;&#20013;&#30340;&#27785;&#28024;&#24335;&#24863;&#23448;&#20307;&#39564;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;&#20914;&#20987;&#22768;&#21512;&#25104;&#26041;&#27861;&#20351;&#29992;&#29289;&#29702;&#27169;&#25311;&#26469;&#33719;&#24471;&#19968;&#32452;&#33021;&#22815;&#34920;&#31034;&#21644;&#21512;&#25104;&#22768;&#38899;&#30340;&#29289;&#29702;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#38656;&#35201;&#29289;&#20307;&#30340;&#32454;&#33410;&#21644;&#20914;&#20987;&#20301;&#32622;&#65292;&#36825;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#24456;&#23569;&#21487;&#29992;&#65292;&#24182;&#19988;&#26080;&#27861;&#24212;&#29992;&#20110;&#20174;&#26222;&#36890;&#35270;&#39057;&#20013;&#21512;&#25104;&#20914;&#20987;&#22768;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#29616;&#26377;&#30340;&#35270;&#39057;&#39537;&#21160;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#21482;&#33021;&#25429;&#25417;&#21040;&#35270;&#35273;&#20869;&#23481;&#21644;&#20914;&#20987;&#22768;&#20043;&#38388;&#30340;&#24369;&#23545;&#24212;&#20851;&#31995;&#65292;&#22240;&#20026;&#23427;&#20204;&#32570;&#20047;&#29289;&#29702;&#30693;&#35782;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#39537;&#21160;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#20026;&#38745;&#24577;&#35270;&#39057;&#21098;&#36753;&#21512;&#25104;&#39640;&#20445;&#30495;&#30340;&#20914;&#20987;&#22768;&#12290;&#38500;&#20102;&#35270;&#39057;&#20869;&#23481;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20351;&#29992;&#39069;&#22806;&#30340;&#29289;&#29702;&#20808;&#39564;&#30693;&#35782;&#26469;&#25351;&#23548;&#20914;&#20987;&#22768;&#21512;&#25104;&#36807;&#31243;&#65292;&#36825;&#20123;&#20808;&#39564;&#21253;&#25324;&#26082;&#21487;&#25511;&#21046;&#29289;&#29702;&#21442;&#25968;&#65292;&#21516;&#26102;&#20063;&#33021;&#20445;&#35777;&#38899;&#25928;&#36136;&#37327;&#30340;&#22122;&#22768;&#25200;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling sounds emitted from physical object interactions is critical for immersive perceptual experiences in real and virtual worlds. Traditional methods of impact sound synthesis use physics simulation to obtain a set of physics parameters that could represent and synthesize the sound. However, they require fine details of both the object geometries and impact locations, which are rarely available in the real world and can not be applied to synthesize impact sounds from common videos. On the other hand, existing video-driven deep learning-based approaches could only capture the weak correspondence between visual content and impact sounds since they lack of physics knowledge. In this work, we propose a physics-driven diffusion model that can synthesize high-fidelity impact sound for a silent video clip. In addition to the video content, we propose to use additional physics priors to guide the impact sound synthesis procedure. The physics priors include both physics parameters that are
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#25968;&#25454;&#39537;&#21160;&#30340;&#20195;&#29702;&#24314;&#27169;&#22312;&#39640;&#32500;&#29289;&#29702;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#30340;&#27010;&#29575;&#24314;&#27169;&#26041;&#27861;&#65292;&#20197;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#21644;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.16869</link><description>&lt;p&gt;
&#27010;&#29575;&#24314;&#27169;&#19982;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#22312;&#39640;&#32500;&#24212;&#21147;&#22330;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Application of probabilistic modeling and automated machine learning framework for high-dimensional stress field. (arXiv:2303.16869v1 [cs.CE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#25968;&#25454;&#39537;&#21160;&#30340;&#20195;&#29702;&#24314;&#27169;&#22312;&#39640;&#32500;&#29289;&#29702;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#30340;&#27010;&#29575;&#24314;&#27169;&#26041;&#27861;&#65292;&#20197;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#21644;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#35745;&#31639;&#26041;&#27861;&#37319;&#29992;&#39640;&#24230;&#22797;&#26434;&#30340;&#25968;&#23398;&#20844;&#24335;&#65292;&#20351;&#24471;&#24314;&#27169;&#22797;&#26434;&#30340;&#29289;&#29702;&#29616;&#35937;&#12289;&#39044;&#27979;&#20851;&#38190;&#24615;&#33021;&#21644;&#35774;&#35745;&#20248;&#21270;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35745;&#31639;&#27169;&#22411;&#30340;&#39640;&#20445;&#30495;&#24230;&#20351;&#24471;&#26597;&#35810;&#25104;&#26412;&#26497;&#39640;&#65292;&#36890;&#24120;&#20250;&#37319;&#29992;&#31616;&#21270;&#27169;&#22411;&#20197;&#25442;&#21462;&#39044;&#27979;&#31934;&#24230;&#21644;&#31934;&#30830;&#24230;&#30340;&#20195;&#20215;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25968;&#25454;&#39537;&#21160;&#30340;&#20195;&#29702;&#24314;&#27169;&#26041;&#27861;&#22312;&#20223;&#30495;&#26114;&#36149;&#30340;&#35745;&#31639;&#27169;&#22411;&#34892;&#20026;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#24456;&#22823;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#19968;&#20010;&#20027;&#35201;&#29942;&#39048;&#26159;&#26080;&#27861;&#22788;&#29702;&#39640;&#32500;&#24230;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#37327;&#65292;&#38656;&#35201;&#30456;&#23545;&#36739;&#22823;&#30340;&#25968;&#25454;&#38598;&#12290;&#23545;&#20110;&#27492;&#31867;&#38382;&#39064;&#65292;&#24120;&#29992;&#30340;&#20195;&#29702;&#24314;&#27169;&#26041;&#27861;&#20250;&#35201;&#27714;&#22823;&#37327;&#30340;&#35745;&#31639;&#35780;&#20272;&#65292;&#20351;&#24471;&#20854;&#20182;&#25968;&#20540;&#22788;&#29702;&#21464;&#24471;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern computational methods, involving highly sophisticated mathematical formulations, enable several tasks like modeling complex physical phenomenon, predicting key properties and design optimization. The higher fidelity in these computer models makes it computationally intensive to query them hundreds of times for optimization and one usually relies on a simplified model albeit at the cost of losing predictive accuracy and precision. Towards this, data-driven surrogate modeling methods have shown a lot of promise in emulating the behavior of the expensive computer models. However, a major bottleneck in such methods is the inability to deal with high input dimensionality and the need for relatively large datasets. With such problems, the input and output quantity of interest are tensors of high dimensionality. Commonly used surrogate modeling methods for such problems, suffer from requirements like high number of computational evaluations that precludes one from performing other nume
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#21452;&#26354;&#21644;&#29699;&#24418;&#27169;&#22411;&#31354;&#38388;&#30340;&#31435;&#20307;&#25237;&#24433;&#20197;&#21450;Riemannian&#38544;&#31354;&#38388;&#30340;&#20056;&#31215;&#24212;&#29992;&#20110;&#28508;&#22312;&#22270;&#25512;&#26029;&#65292;&#23454;&#29616;&#20102;&#19982;&#38750;&#25237;&#24433;&#31354;&#38388;&#30456;&#24403;&#30340;&#24615;&#33021;&#24182;&#25552;&#20379;&#29702;&#35770;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2303.11754</link><description>&lt;p&gt;
&#28508;&#22312;&#22270;&#25512;&#26029;&#20013;&#30340;&#27169;&#22411;&#31354;&#38388;&#25237;&#24433;&#12290;
&lt;/p&gt;
&lt;p&gt;
Projections of Model Spaces for Latent Graph Inference. (arXiv:2303.11754v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11754
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#21452;&#26354;&#21644;&#29699;&#24418;&#27169;&#22411;&#31354;&#38388;&#30340;&#31435;&#20307;&#25237;&#24433;&#20197;&#21450;Riemannian&#38544;&#31354;&#38388;&#30340;&#20056;&#31215;&#24212;&#29992;&#20110;&#28508;&#22312;&#22270;&#25512;&#26029;&#65292;&#23454;&#29616;&#20102;&#19982;&#38750;&#25237;&#24433;&#31354;&#38388;&#30456;&#24403;&#30340;&#24615;&#33021;&#24182;&#25552;&#20379;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#21033;&#29992;&#22270;&#30340;&#36830;&#25509;&#32467;&#26500;&#20316;&#20026;&#24402;&#32435;&#20559;&#24046;&#12290;&#28508;&#22312;&#22270;&#25512;&#26029;&#20851;&#27880;&#20110;&#23398;&#20064;&#19968;&#20010;&#21512;&#36866;&#30340;&#22270;&#32467;&#26500;&#26469;&#25193;&#25955;&#20449;&#24687;&#24182;&#25552;&#39640;&#27169;&#22411;&#30340;&#19979;&#28216;&#24615;&#33021;&#12290;&#26412;&#25991;&#21033;&#29992;&#21452;&#26354;&#21644;&#29699;&#24418;&#27169;&#22411;&#31354;&#38388;&#30340;&#31435;&#20307;&#25237;&#24433;&#65292;&#20197;&#21450;Riemannian&#38544;&#31354;&#38388;&#30340;&#20056;&#31215;&#65292;&#29992;&#20110;&#28508;&#22312;&#22270;&#25512;&#26029;&#12290;&#22312;&#36991;&#20813;&#26354;&#29575;&#36235;&#20110;&#38646;&#26102;&#31354;&#38388;&#21457;&#25955;&#30340;&#29702;&#35770;&#20445;&#35777;&#19979;&#65292;&#31435;&#20307;&#25237;&#24433;&#27169;&#22411;&#31354;&#38388;&#33021;&#23454;&#29616;&#19982;&#38750;&#25237;&#24433;&#23545;&#24212;&#27169;&#22411;&#31354;&#38388;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#21516;&#26500;&#21644;&#24322;&#26500;&#22270;&#19978;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks leverage the connectivity structure of graphs as an inductive bias. Latent graph inference focuses on learning an adequate graph structure to diffuse information on and improve the downstream performance of the model. In this work we employ stereographic projections of the hyperbolic and spherical model spaces, as well as products of Riemannian manifolds, for the purpose of latent graph inference. Stereographically projected model spaces achieve comparable performance to their non-projected counterparts, while providing theoretical guarantees that avoid divergence of the spaces when the curvature tends to zero. We perform experiments on both homophilic and heterophilic graphs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#25991;&#26412;&#24341;&#23548;&#22270;&#20687;&#39118;&#26684;&#36801;&#31227;&#20013;&#30340;&#38646;&#26679;&#26412;&#23545;&#27604;&#25439;&#22833;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#39069;&#22806;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#20855;&#26377;&#30456;&#21516;&#35821;&#20041;&#20869;&#23481;&#30340;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2303.08622</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#23545;&#27604;&#25439;&#22833;&#29992;&#20110;&#25991;&#26412;&#24341;&#23548;&#25193;&#25955;&#22270;&#20687;&#39118;&#26684;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Contrastive Loss for Text-Guided Diffusion Image Style Transfer. (arXiv:2303.08622v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08622
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#25991;&#26412;&#24341;&#23548;&#22270;&#20687;&#39118;&#26684;&#36801;&#31227;&#20013;&#30340;&#38646;&#26679;&#26412;&#23545;&#27604;&#25439;&#22833;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#39069;&#22806;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#20855;&#26377;&#30456;&#21516;&#35821;&#20041;&#20869;&#23481;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#25991;&#26412;&#24341;&#23548;&#22270;&#20687;&#39118;&#26684;&#36801;&#31227;&#20013;&#34920;&#29616;&#20986;&#26497;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#30001;&#20110;&#20854;&#38543;&#26426;&#24615;&#32780;&#23384;&#22312;&#39118;&#26684;&#36716;&#25442;&#21644;&#20869;&#23481;&#20445;&#25252;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#35745;&#31639;&#23494;&#38598;&#30340;&#25193;&#25955;&#27169;&#22411;&#24494;&#35843;&#25110;&#38468;&#21152;&#31070;&#32463;&#32593;&#32476;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#25193;&#25955;&#27169;&#22411;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#23545;&#27604;&#25439;&#22833;&#65292;&#23427;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#24494;&#35843;&#25110;&#36741;&#21161;&#32593;&#32476;&#12290;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#20013;&#29983;&#25104;&#26679;&#26412;&#21644;&#21407;&#22987;&#22270;&#20687;&#23884;&#20837;&#20043;&#38388;&#30340;&#22270;&#22359;&#23545;&#27604;&#25439;&#22833;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20197;&#38646;&#26679;&#26412;&#30340;&#26041;&#24335;&#29983;&#25104;&#20855;&#26377;&#19982;&#28304;&#22270;&#20687;&#30456;&#21516;&#35821;&#20041;&#20869;&#23481;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20445;&#30041;&#20869;&#23481;&#19988;&#19981;&#38656;&#35201;&#39069;&#22806;&#35757;&#32451;&#30340;&#21516;&#26102;&#65292;&#22312;&#22270;&#20687;&#39118;&#26684;&#36801;&#31227;&#12289;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#36716;&#25442;&#21644;&#25805;&#20316;&#20013;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have shown great promise in text-guided image style transfer, but there is a trade-off between style transformation and content preservation due to their stochastic nature. Existing methods require computationally expensive fine-tuning of diffusion models or additional neural network. To address this, here we propose a zero-shot contrastive loss for diffusion models that doesn't require additional fine-tuning or auxiliary networks. By leveraging patch-wise contrastive loss between generated samples and original image embeddings in the pre-trained diffusion model, our method can generate images with the same semantic content as the source image in a zero-shot manner. Our approach outperforms existing methods while preserving content and requiring no additional training, not only for image style transfer but also for image-to-image translation and manipulation. Our experimental results validate the effectiveness of our proposed method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;SPSA&#31639;&#27861;&#27714;&#35299;n&#27493;&#26102;&#24207;&#24046;&#20998;&#23398;&#20064;&#20013;&#30340;&#26368;&#20248;n&#20540;&#30340;&#31639;&#27861;SDPSA&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.07068</link><description>&lt;p&gt;
&#12298;&#20855;&#26377;&#26368;&#20248;n&#30340;n&#27493;&#26102;&#24207;&#24046;&#20998;&#23398;&#20064;&#12299;
&lt;/p&gt;
&lt;p&gt;
n-Step Temporal Difference Learning with Optimal n. (arXiv:2303.07068v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;SPSA&#31639;&#27861;&#27714;&#35299;n&#27493;&#26102;&#24207;&#24046;&#20998;&#23398;&#20064;&#20013;&#30340;&#26368;&#20248;n&#20540;&#30340;&#31639;&#27861;SDPSA&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#22312;n&#27493;&#26102;&#24207;&#24046;&#20998;&#31639;&#27861;&#20013;&#25214;&#21040;&#26368;&#20248;n&#20540;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#27169;&#22411;&#33258;&#30001;&#20248;&#21270;&#25216;&#26415;&#65292;&#21363;&#21516;&#26102;&#25200;&#21160;&#38543;&#26426;&#36924;&#36817;&#65288;SPSA&#65289;&#26041;&#27861;&#26469;&#23547;&#25214;&#26368;&#20248;n&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#27169;&#25311;SPSA&#31243;&#24207;&#65292;&#23558;&#20854;&#21407;&#22987;&#36830;&#32493;&#20248;&#21270;&#26694;&#26550;&#24341;&#20837;&#31163;&#25955;&#20248;&#21270;&#26694;&#26550;&#65292;&#20294;&#24182;&#32467;&#21512;&#20102;&#24490;&#29615;&#25200;&#21160;&#24207;&#21015;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;SDPSA&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#34920;&#26126;&#23427;&#21487;&#20197;&#22312;&#20219;&#24847;&#21021;&#22987;&#20540;&#30340;&#24773;&#20917;&#19979;&#25214;&#21040;n&#27493;TD&#20013;&#30340;&#26368;&#20248;n&#20540;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;SDPSA&#33021;&#22815;&#23454;&#29616;&#26368;&#20248;n&#20540;&#30340;&#27714;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of finding the optimal value of n in the n-step temporal difference (TD) algorithm. We find the optimal n by resorting to the model-free optimization technique of simultaneous perturbation stochastic approximation (SPSA). We adopt a one-simulation SPSA procedure that is originally for continuous optimization to the discrete optimization framework but incorporates a cyclic perturbation sequence. We prove the convergence of our proposed algorithm, SDPSA, and show that it finds the optimal value of n in n-step TD. Through experiments, we show that the optimal value of n is achieved with SDPSA for any arbitrary initial value of the same.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PromptAttack&#30340;&#32452;&#21512;&#27979;&#35797;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#34987;&#20302;&#39057;&#29575;&#35206;&#30422;&#30340;&#23376;&#32676;&#20307;&#20013;&#25628;&#32034;&#25214;&#21040;&#30446;&#26631;&#27169;&#22411;&#22312;&#36825;&#20123;&#26465;&#20214;&#19979;&#34920;&#29616;&#19981;&#20339;&#30340;&#23376;&#32676;&#20307;&#65292;&#20197;&#27492;&#35782;&#21035;&#20986;&#20998;&#31867;&#22120;&#21487;&#33021;&#23384;&#22312;&#30340;&#31995;&#32479;&#35823;&#24046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.05072</link><description>&lt;p&gt;
&#21457;&#29616;&#22270;&#20687;&#20998;&#31867;&#22120;&#22312;&#32597;&#35265;&#23376;&#32676;&#20307;&#19978;&#30340;&#31995;&#32479;&#24615;&#38169;&#35823;
&lt;/p&gt;
&lt;p&gt;
Identification of Systematic Errors of Image Classifiers on Rare Subgroups. (arXiv:2303.05072v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05072
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PromptAttack&#30340;&#32452;&#21512;&#27979;&#35797;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#34987;&#20302;&#39057;&#29575;&#35206;&#30422;&#30340;&#23376;&#32676;&#20307;&#20013;&#25628;&#32034;&#25214;&#21040;&#30446;&#26631;&#27169;&#22411;&#22312;&#36825;&#20123;&#26465;&#20214;&#19979;&#34920;&#29616;&#19981;&#20339;&#30340;&#23376;&#32676;&#20307;&#65292;&#20197;&#27492;&#35782;&#21035;&#20986;&#20998;&#31867;&#22120;&#21487;&#33021;&#23384;&#22312;&#30340;&#31995;&#32479;&#35823;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35768;&#22810;&#22270;&#20687;&#20998;&#31867;&#22120;&#20855;&#26377;&#20986;&#33394;&#30340;&#24179;&#22343;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#26410;&#34987;&#20805;&#20998;&#35206;&#30422;&#30340;&#35821;&#20041;&#36830;&#36143;&#30340;&#23376;&#32676;&#20307;&#19978;&#30340;&#24615;&#33021;&#21487;&#33021;&#20250;&#26174;&#33879;&#38477;&#20302;&#12290;&#36825;&#20123;&#31995;&#32479;&#35823;&#24046;&#21487;&#33021;&#20250;&#24433;&#21709;&#27665;&#26063;&#23569;&#25968;&#32676;&#20307;&#30340;&#20844;&#24179;&#24615;&#65292;&#20197;&#21450;&#22312;&#39046;&#22495;&#36716;&#31227;&#26102;&#30340;&#40065;&#26834;&#24615;&#21644;&#23433;&#20840;&#24615;&#12290;&#25105;&#20204;&#21033;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#65292;&#25628;&#32034;&#25968;&#25454;&#20013;&#34987;&#20302;&#39057;&#29575;&#35206;&#30422;&#30340;&#23376;&#32676;&#20307;&#65288;&#8220;&#25552;&#31034;&#8221;&#65289;&#65292;&#25214;&#21040;&#30446;&#26631;&#27169;&#22411;&#22312;&#36825;&#20123;&#26465;&#20214;&#19979;&#34920;&#29616;&#19981;&#20339;&#30340;&#23376;&#32676;&#20307;&#65292;&#20197;&#27492;&#35782;&#21035;&#20986;&#20998;&#31867;&#22120;&#21487;&#33021;&#23384;&#22312;&#30340;&#31995;&#32479;&#35823;&#24046;&#38382;&#39064;&#12290;&#25105;&#20204;&#37319;&#29992;&#32452;&#21512;&#27979;&#35797;&#35299;&#20915;&#20102;&#23376;&#32676;&#20307;&#25968;&#37327;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#30340;&#25361;&#25112;&#65292;&#24182;&#23558;&#36825;&#20010;&#36807;&#31243;&#31216;&#20026;PromptAttack&#12290;&#25105;&#20204;&#22312;&#21463;&#25511;&#30340;&#23454;&#39564;&#29615;&#22659;&#19979;&#30740;&#31350;&#20102;PromptAttack&#30340;&#23376;&#32676;&#20307;&#28085;&#30422;&#29575;&#21644;&#21487;&#35782;&#21035;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite excellent average-case performance of many image classifiers, their performance can substantially deteriorate on semantically coherent subgroups of the data that were under-represented in the training data. These systematic errors can impact both fairness for demographic minority groups as well as robustness and safety under domain shift. A major challenge is to identify such subgroups with subpar performance when the subgroups are not annotated and their occurrence is very rare. We leverage recent advances in text-to-image models and search in the space of textual descriptions of subgroups ("prompts") for subgroups where the target model has low performance on the prompt-conditioned synthesized data. To tackle the exponentially growing number of subgroups, we employ combinatorial testing. We denote this procedure as PromptAttack as it can be interpreted as an adversarial attack in a prompt space. We study subgroup coverage and identifiability with PromptAttack in a controlled 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#25991;&#31456;&#31995;&#32479;&#22320;&#24635;&#32467;&#20102;&#26368;&#26032;&#30340;&#36827;&#21270;&#35745;&#31639;&#26041;&#27861;&#22312;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#26041;&#38754;&#25152;&#21462;&#24471;&#30340;&#33391;&#22909;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.04150</link><description>&lt;p&gt;
&#36827;&#21270;&#24378;&#21270;&#23398;&#20064;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Evolutionary Reinforcement Learning: A Survey. (arXiv:2303.04150v3 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04150
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#25991;&#31456;&#31995;&#32479;&#22320;&#24635;&#32467;&#20102;&#26368;&#26032;&#30340;&#36827;&#21270;&#35745;&#31639;&#26041;&#27861;&#22312;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#26041;&#38754;&#25152;&#21462;&#24471;&#30340;&#33391;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#36890;&#36807;&#19982;&#29615;&#22659;&#20132;&#20114;&#35757;&#32451;&#26234;&#33021;&#20307;&#26368;&#22823;&#21270;&#32047;&#31215;&#22870;&#21169;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#26368;&#36817;&#23558;&#24378;&#21270;&#23398;&#20064;&#19982;&#28145;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#22312;&#26827;&#30424;&#28216;&#25103;&#12289;&#34903;&#26426;&#28216;&#25103;&#21644;&#26426;&#22120;&#20154;&#25511;&#21046;&#31561;&#21508;&#31181;&#25361;&#25112;&#24615;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#25104;&#23601;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#20173;&#23384;&#22312;&#19968;&#20123;&#20851;&#38190;&#25361;&#25112;&#65292;&#21253;&#25324;&#30001;&#25935;&#24863;&#36229;&#21442;&#25968;&#23548;&#33268;&#30340;&#33030;&#24369;&#25910;&#25947;&#29305;&#24615;&#65292;&#38271;&#26102;&#38388;&#36328;&#24230;&#21644;&#31232;&#30095;&#22870;&#21169;&#30340;&#26102;&#38388;&#20998;&#37197;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#22312;&#36830;&#32493;&#25628;&#32034;&#31354;&#38388;&#22330;&#26223;&#20013;&#30340;&#22810;&#26679;&#24615;&#25506;&#32034;&#19981;&#36275;&#65292;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20449;&#29992;&#20998;&#37197;&#22256;&#38590;&#20197;&#21450;&#22870;&#21169;&#20914;&#31361;&#30446;&#26631;&#12290;&#36827;&#21270;&#35745;&#31639;&#32500;&#25252;&#30528;&#19968;&#32676;&#23398;&#20064;&#26234;&#33021;&#20307;&#65292;&#24050;&#23637;&#29616;&#20986;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#30340;&#33391;&#22909;&#24615;&#33021;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#38598;&#25104;&#36827;&#21270;&#35745;&#31639;&#30340;&#26368;&#26032;&#26041;&#27861;&#30340;&#20840;&#38754;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) is a machine learning approach that trains agents to maximize cumulative rewards through interactions with environments. The integration of RL with deep learning has recently resulted in impressive achievements in a wide range of challenging tasks, including board games, arcade games, and robot control. Despite these successes, there remain several crucial challenges, including brittle convergence properties caused by sensitive hyperparameters, difficulties in temporal credit assignment with long time horizons and sparse rewards, a lack of diverse exploration, especially in continuous search space scenarios, difficulties in credit assignment in multi-agent reinforcement learning, and conflicting objectives for rewards. Evolutionary computation (EC), which maintains a population of learning agents, has demonstrated promising performance in addressing these limitations. This article presents a comprehensive survey of state-of-the-art methods for integrating EC
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861; One-4-All (O4A)&#65292;&#21033;&#29992;&#33258;&#25105;&#30417;&#30563;&#21644;&#27969;&#24418;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#19968;&#31181;&#26080;&#22270;&#24418;&#12289;&#31471;&#21040;&#31471;&#30340;&#23548;&#33322;&#31243;&#24207;&#12290;&#20854;&#36890;&#36807;&#36138;&#23146;&#26368;&#23567;&#21270;&#28508;&#22312;&#31354;&#38388;&#20869;&#30340;&#28508;&#21147;&#20989;&#25968;&#26469;&#36827;&#34892;&#23548;&#33322;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#23548;&#33322;&#20013;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2303.04011</link><description>&lt;p&gt;
&#19968;&#25307;&#40092;&#65306;&#31070;&#32463;&#28508;&#21147;&#22330;&#29992;&#20110;&#23454;&#20307;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
One-4-All: Neural Potential Fields for Embodied Navigation. (arXiv:2303.04011v2 [cs.RO] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861; One-4-All (O4A)&#65292;&#21033;&#29992;&#33258;&#25105;&#30417;&#30563;&#21644;&#27969;&#24418;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#19968;&#31181;&#26080;&#22270;&#24418;&#12289;&#31471;&#21040;&#31471;&#30340;&#23548;&#33322;&#31243;&#24207;&#12290;&#20854;&#36890;&#36807;&#36138;&#23146;&#26368;&#23567;&#21270;&#28508;&#22312;&#31354;&#38388;&#20869;&#30340;&#28508;&#21147;&#20989;&#25968;&#26469;&#36827;&#34892;&#23548;&#33322;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#23548;&#33322;&#20013;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#30340;&#22522;&#26412;&#20219;&#21153;&#20043;&#19968;&#26159;&#22312;&#20004;&#20010;&#20301;&#32622;&#20043;&#38388;&#23548;&#33322;&#12290;&#29305;&#21035;&#22320;&#65292;&#30495;&#23454;&#19990;&#30028;&#30340;&#23548;&#33322;&#21487;&#33021;&#38656;&#35201;&#20351;&#29992;&#39640;&#32500;RGB&#22270;&#20687;&#36827;&#34892;&#38271;&#26399;&#35268;&#21010;&#65292;&#36825;&#23545;&#20110;&#31471;&#21040;&#31471;&#30340;&#23398;&#20064;&#26041;&#27861;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#30446;&#21069;&#30340;&#21322;&#21442;&#25968;&#26041;&#27861;&#36890;&#36807;&#23558;&#23398;&#20064;&#27169;&#22359;&#19982;&#29615;&#22659;&#30340;&#25299;&#25169;&#35760;&#24518;&#30456;&#32467;&#21512;&#26469;&#23454;&#29616;&#38271;&#26399;&#35268;&#21010;&#65292;&#36890;&#24120;&#20351;&#29992;&#20197;&#20808;&#21069;&#25910;&#38598;&#30340;&#22270;&#20687;&#20026;&#22522;&#30784;&#30340;&#22270;&#24418;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#19978;&#20351;&#29992;&#36825;&#20123;&#22270;&#34920;&#36890;&#24120;&#38656;&#35201;&#35843;&#25972;&#22823;&#37327;&#30340;&#20462;&#21098;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20197;&#36991;&#20813;&#34394;&#20551;&#30340;&#36793;&#32536;&#65292;&#38480;&#21046;&#36816;&#34892;&#26102;&#20869;&#23384;&#20351;&#29992;&#24182;&#20801;&#35768;&#30456;&#24403;&#24555;&#36895;&#30340;&#22270;&#24418;&#26597;&#35810;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;One-4-All&#65288;O4A&#65289;&#65292;&#19968;&#31181;&#21033;&#29992;&#33258;&#25105;&#30417;&#30563;&#21644;&#27969;&#24418;&#23398;&#20064;&#26469;&#33719;&#24471;&#26080;&#22270;&#24418;&#12289;&#31471;&#21040;&#31471;&#23548;&#33322;&#31649;&#36947;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#30446;&#26631;&#34987;&#25351;&#23450;&#20026;&#19968;&#24133;&#22270;&#20687;&#12290;&#23548;&#33322;&#36890;&#36807;&#36138;&#23146;&#22320;&#26368;&#23567;&#21270;&#36830;&#32493;&#23450;&#20041;&#22312;O4A&#28508;&#22312;&#31354;&#38388;&#19978;&#30340;&#28508;&#21147;&#20989;&#25968;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#26159;&#31163;&#32447;&#35757;&#32451;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
A fundamental task in robotics is to navigate between two locations. In particular, real-world navigation can require long-horizon planning using high-dimensional RGB images, which poses a substantial challenge for end-to-end learning-based approaches. Current semi-parametric methods instead achieve long-horizon navigation by combining learned modules with a topological memory of the environment, often represented as a graph over previously collected images. However, using these graphs in practice typically involves tuning a number of pruning heuristics to avoid spurious edges, limit runtime memory usage and allow reasonably fast graph queries. In this work, we present One-4-All (O4A), a method leveraging self-supervised and manifold learning to obtain a graph-free, end-to-end navigation pipeline in which the goal is specified as an image. Navigation is achieved by greedily minimizing a potential function defined continuously over the O4A latent space. Our system is trained offline on 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32622;&#20449;&#24230;&#23376;&#22270;&#21305;&#37197;&#30340;&#21452;&#38454;&#27573;&#31639;&#27861;&#65292;&#36890;&#36807;&#30830;&#23450;&#36793;&#26435;&#31995;&#25968;&#21644;&#25913;&#21892;&#26631;&#31614;&#20256;&#25773;&#26426;&#21046;&#26469;&#35299;&#20915;&#20102;&#24322;&#36136;&#32467;&#26500;&#37051;&#23621;&#35782;&#21035;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.09755</link><description>&lt;p&gt;
&#22522;&#20110;&#32622;&#20449;&#24230;&#23376;&#22270;&#21305;&#37197;&#30340;&#24322;&#36136;&#32467;&#26500;&#37051;&#23621;&#21457;&#29616;&#31639;&#27861;&#29992;&#20110;&#21322;&#30417;&#30563;&#33410;&#28857;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Finding Heterophilic Neighbors via Confidence-based Subgraph Matching for Semi-supervised Node Classification. (arXiv:2302.09755v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32622;&#20449;&#24230;&#23376;&#22270;&#21305;&#37197;&#30340;&#21452;&#38454;&#27573;&#31639;&#27861;&#65292;&#36890;&#36807;&#30830;&#23450;&#36793;&#26435;&#31995;&#25968;&#21644;&#25913;&#21892;&#26631;&#31614;&#20256;&#25773;&#26426;&#21046;&#26469;&#35299;&#20915;&#20102;&#24322;&#36136;&#32467;&#26500;&#37051;&#23621;&#35782;&#21035;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#35768;&#22810;&#22522;&#20110;&#22270;&#30340;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#24322;&#36136;&#32467;&#26500;&#35774;&#32622;&#19979;&#65288;&#21363;&#37051;&#23621;&#33410;&#28857;&#20855;&#26377;&#19981;&#21516;&#30340;&#26631;&#31614;&#65289;&#65292;&#23427;&#20204;&#30340;&#27867;&#21270;&#33021;&#21147;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#38454;&#27573;&#31639;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#34917;&#20805;&#27169;&#22359;&#36890;&#36807;&#23376;&#22270;&#21305;&#37197;&#26469;&#30830;&#23450;&#36793;&#32536;&#31995;&#25968;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#20855;&#26377;&#20462;&#25913;&#26631;&#31614;&#20256;&#25773;&#26426;&#21046;&#30340;GNNs&#26469;&#26377;&#25928;&#21033;&#29992;&#36793;&#32536;&#31995;&#25968;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#34917;&#20805;&#27169;&#22359;&#26681;&#25454;&#32473;&#23450;&#30340;&#32622;&#20449;&#24230;&#27604;&#20363;&#30830;&#23450;&#26576;&#20123;&#20219;&#21153;&#19981;&#30456;&#20851;&#30340;&#36793;&#32536;&#12290;&#20351;&#29992;&#21097;&#20313;&#30340;&#36793;&#32536;&#65292;&#25105;&#20204;&#37319;&#29992;&#24191;&#27867;&#20351;&#29992;&#30340;&#26368;&#20248;&#36816;&#36755;&#26469;&#35745;&#31639;&#20004;&#20010;&#33410;&#28857;&#21450;&#20854;&#23376;&#22270;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#12290;&#26368;&#21518;&#65292;&#21033;&#29992;&#31995;&#25968;&#20316;&#20026;GNNs&#30340;&#34917;&#20805;&#20449;&#24687;&#65292;&#25105;&#20204;&#25913;&#21892;&#20102;&#26631;&#31614;&#20256;&#25773;&#26426;&#21046;&#65292;&#21487;&#20197;&#38450;&#27490;&#20004;&#20010;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have proven to be powerful in many graph-based applications. However, they fail to generalize well under heterophilic setups, where neighbor nodes have different labels. To address this challenge, we employ a confidence ratio as a hyper-parameter, assuming that some of the edges are disassortative (heterophilic). Here, we propose a two-phased algorithm. Firstly, we determine edge coefficients through subgraph matching using a supplementary module. Then, we apply GNNs with a modified label propagation mechanism to utilize the edge coefficients effectively. Specifically, our supplementary module identifies a certain proportion of task-irrelevant edges based on a given confidence ratio. Using the remaining edges, we employ the widely used optimal transport to measure the similarity between two nodes with their subgraphs. Finally, using the coefficients as supplementary information on GNNs, we improve the label propagation mechanism which can prevent two nodes 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#26426;&#22120;&#29983;&#25104;&#30340;&#34394;&#20551;&#35780;&#35770;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#39640;&#36136;&#37327;&#39184;&#21381;&#35780;&#35770;&#29983;&#25104;&#34394;&#20551;&#35780;&#35770;&#24182;&#24494;&#35843;GPT&#36755;&#20986;&#26816;&#27979;&#22120;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#39044;&#27979;&#34394;&#20551;&#35780;&#35770;&#30340;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#39044;&#27979;&#38750;&#31934;&#33521;&#35780;&#35770;&#30340;&#27169;&#22411;&#65292;&#24182;&#22312;&#20960;&#20010;&#32500;&#24230;&#19978;&#23545;&#36825;&#20123;&#35780;&#35770;&#36827;&#34892;&#20998;&#26512;&#65292;&#27492;&#31867;&#26426;&#22120;&#29983;&#25104;&#30340;&#34394;&#20551;&#35780;&#35770;&#26159;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#38754;&#20020;&#30340;&#25345;&#32493;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2302.07731</link><description>&lt;p&gt;
AI&#23545;&#25239;AI&#65306;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#25171;&#20987;&#26426;&#22120;&#29983;&#25104;&#30340;&#34394;&#20551;&#39184;&#21381;&#35780;&#35770;
&lt;/p&gt;
&lt;p&gt;
Combat AI With AI: Counteract Machine-Generated Fake Restaurant Reviews on Social Media. (arXiv:2302.07731v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#26426;&#22120;&#29983;&#25104;&#30340;&#34394;&#20551;&#35780;&#35770;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#39640;&#36136;&#37327;&#39184;&#21381;&#35780;&#35770;&#29983;&#25104;&#34394;&#20551;&#35780;&#35770;&#24182;&#24494;&#35843;GPT&#36755;&#20986;&#26816;&#27979;&#22120;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#39044;&#27979;&#34394;&#20551;&#35780;&#35770;&#30340;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#39044;&#27979;&#38750;&#31934;&#33521;&#35780;&#35770;&#30340;&#27169;&#22411;&#65292;&#24182;&#22312;&#20960;&#20010;&#32500;&#24230;&#19978;&#23545;&#36825;&#20123;&#35780;&#35770;&#36827;&#34892;&#20998;&#26512;&#65292;&#27492;&#31867;&#26426;&#22120;&#29983;&#25104;&#30340;&#34394;&#20551;&#35780;&#35770;&#26159;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#38754;&#20020;&#30340;&#25345;&#32493;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#29983;&#25104;&#27169;&#22411;&#65288;&#22914;GPT&#65289;&#30340;&#21457;&#23637;&#20351;&#24471;&#20197;&#26356;&#20302;&#30340;&#25104;&#26412;&#21046;&#36896;&#20986;&#38590;&#20197;&#21306;&#20998;&#30340;&#34394;&#20551;&#39038;&#23458;&#35780;&#35770;&#65292;&#20174;&#32780;&#23545;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#26816;&#27979;&#36825;&#20123;&#26426;&#22120;&#29983;&#25104;&#30340;&#34394;&#20551;&#35780;&#35770;&#36896;&#25104;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;Yelp&#39564;&#35777;&#30340;&#39640;&#36136;&#37327;&#30340;&#31934;&#33521;&#39184;&#21381;&#35780;&#35770;&#26469;&#29983;&#25104;OpenAI GPT&#35780;&#35770;&#29983;&#25104;&#22120;&#30340;&#34394;&#20551;&#35780;&#35770;&#65292;&#24182;&#26368;&#32456;&#24494;&#35843;GPT&#36755;&#20986;&#26816;&#27979;&#22120;&#26469;&#39044;&#27979;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#30340;&#34394;&#20551;&#35780;&#35770;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#27169;&#22411;&#24212;&#29992;&#20110;&#39044;&#27979;&#38750;&#31934;&#33521;&#35780;&#35770;&#65292;&#24182;&#22312;&#20960;&#20010;&#32500;&#24230;&#65288;&#22914;&#35780;&#35770;&#12289;&#29992;&#25143;&#21644;&#39184;&#21381;&#29305;&#24449;&#20197;&#21450;&#20889;&#20316;&#39118;&#26684;&#65289;&#19978;&#35782;&#21035;&#27169;&#24335;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#27491;&#22312;&#19981;&#26029;&#38754;&#20020;&#26426;&#22120;&#29983;&#25104;&#30340;&#34394;&#20551;&#35780;&#35770;&#30340;&#25361;&#25112;&#65292;&#23613;&#31649;&#20182;&#20204;&#21487;&#33021;&#23454;&#26045;&#26816;&#27979;&#31995;&#32479;&#20197;&#36807;&#28388;&#20986;&#21487;&#30097;&#30340;&#35780;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in generative models such as GPT may be used to fabricate indistinguishable fake customer reviews at a much lower cost, thus posing challenges for social media platforms to detect these machine-generated fake reviews. We propose to leverage the high-quality elite restaurant reviews verified by Yelp to generate fake reviews from the OpenAI GPT review creator and ultimately fine-tune a GPT output detector to predict fake reviews that significantly outperform existing solutions. We further apply the model to predict non-elite reviews and identify the patterns across several dimensions, such as review, user and restaurant characteristics, and writing style. We show that social media platforms are continuously challenged by machine-generated fake reviews, although they may implement detection systems to filter out suspicious reviews.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25345;&#32493;&#39046;&#22495;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#19968;&#31995;&#21015;&#26410;&#26631;&#35760;&#30340;&#39046;&#22495;&#35821;&#26009;&#24211;&#26469;&#36880;&#27493;&#36866;&#24212;&#39046;&#22495;&#20197;&#25552;&#39640;LM&#22312;&#39046;&#22495;&#20869;&#30340;&#32456;&#31471;&#20219;&#21153;&#34920;&#29616;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#19968;&#20010;&#36719;&#25513;&#34109;&#26426;&#21046;&#26469;&#30452;&#25509;&#25511;&#21046;LM&#30340;&#26356;&#26032;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#20195;&#29702;&#26469;&#20445;&#30041;LM&#20013;&#30340;&#25972;&#20307;&#30693;&#35782;&#65292;&#21516;&#26102;&#23545;&#27604;&#24050;&#23398;&#20064;&#39046;&#22495;&#30693;&#35782;&#21644;&#24403;&#21069;&#20840;&#32593;&#32476;&#30340;&#30693;&#35782;&#26469;&#23454;&#29616;&#30693;&#35782;&#25972;&#21512;&#12290;</title><link>http://arxiv.org/abs/2302.03241</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Continual Pre-training of Language Models. (arXiv:2302.03241v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25345;&#32493;&#39046;&#22495;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#19968;&#31995;&#21015;&#26410;&#26631;&#35760;&#30340;&#39046;&#22495;&#35821;&#26009;&#24211;&#26469;&#36880;&#27493;&#36866;&#24212;&#39046;&#22495;&#20197;&#25552;&#39640;LM&#22312;&#39046;&#22495;&#20869;&#30340;&#32456;&#31471;&#20219;&#21153;&#34920;&#29616;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#19968;&#20010;&#36719;&#25513;&#34109;&#26426;&#21046;&#26469;&#30452;&#25509;&#25511;&#21046;LM&#30340;&#26356;&#26032;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#20195;&#29702;&#26469;&#20445;&#30041;LM&#20013;&#30340;&#25972;&#20307;&#30693;&#35782;&#65292;&#21516;&#26102;&#23545;&#27604;&#24050;&#23398;&#20064;&#39046;&#22495;&#30693;&#35782;&#21644;&#24403;&#21069;&#20840;&#32593;&#32476;&#30340;&#30693;&#35782;&#26469;&#23454;&#29616;&#30693;&#35782;&#25972;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24555;&#36895;&#21457;&#23637;&#30340;&#20851;&#38190;&#12290;&#26412;&#25991;&#30740;&#31350;LMs&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#65292;&#29305;&#21035;&#26159;&#25345;&#32493;&#39046;&#22495;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#65288;&#25110;&#25345;&#32493;DAP&#35757;&#32451;&#65289;&#12290;&#29616;&#26377;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#39046;&#22495;&#35821;&#26009;&#24211;&#36827;&#19968;&#27493;&#39044;&#35757;&#32451;LMs&#20197;&#20351;&#20854;&#36866;&#24212;&#20110;&#39046;&#22495;&#65292;&#21487;&#20197;&#25552;&#39640;&#39046;&#22495;&#20869;&#30340;&#26368;&#32456;&#20219;&#21153;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#19968;&#31995;&#21015;&#26410;&#26631;&#35760;&#30340;&#39046;&#22495;&#35821;&#26009;&#24211;&#26469;&#25345;&#32493;DAP&#35757;&#32451;LMs&#65292;&#20197;&#20351;&#20854;&#36866;&#24212;&#20110;&#36825;&#20123;&#39046;&#22495;&#65292;&#20174;&#32780;&#25552;&#39640;&#23427;&#20204;&#30340;&#32456;&#31471;&#20219;&#21153;&#24615;&#33021;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#21019;&#26032;&#22312;&#20110;&#19968;&#31181;&#36719;&#25513;&#34109;&#26426;&#21046;&#65292;&#21487;&#30452;&#25509;&#25511;&#21046;LMs&#30340;&#26356;&#26032;&#12290;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20195;&#29702;&#26469;&#20445;&#30041;&#21407;&#22987;LMs&#20013;&#30340;&#26222;&#36890;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#23427;&#23545;&#20808;&#21069;&#23398;&#20064;&#30340;&#39046;&#22495;&#30693;&#35782;&#65288;&#21253;&#25324;&#39044;&#20808;&#35757;&#32451;&#30340;LMs&#20013;&#30340;&#26222;&#36890;&#30693;&#35782;&#65289;&#21644;&#26469;&#33258;&#24403;&#21069;&#20840;&#32593;&#32476;&#30340;&#30693;&#35782;&#36827;&#34892;&#23545;&#27604;&#65292;&#20197;&#23454;&#29616;&#30693;&#35782;&#25972;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) have been instrumental for the rapid advance of natural language processing. This paper studies continual pre-training of LMs, in particular, continual domain-adaptive pre-training (or continual DAP-training). Existing research has shown that further pre-training an LM using a domain corpus to adapt the LM to the domain can improve the end-task performance in the domain. This paper proposes a novel method to continually DAP-train an LM with a sequence of unlabeled domain corpora to adapt the LM to these domains to improve their end-task performances. The key novelty of our method is a soft-masking mechanism that directly controls the update to the LM. A novel proxy is also proposed to preserve the general knowledge in the original LM. Additionally, it contrasts the representations of the previously learned domain knowledge (including the general knowledge in the pre-trained LM) and the knowledge from the current full network to achieve knowledge integration. The m
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#20284;&#28982;&#27604;&#26816;&#39564;&#20026;&#22522;&#20934;&#27979;&#35797;TSC&#31639;&#27861;&#22312;&#21306;&#20998;&#25193;&#25955;&#36807;&#31243;&#20013;&#30340;&#26368;&#20248;&#24615;&#12290;&#38543;&#26426;&#26862;&#26519;&#12289;ResNet&#21644;ROCKET&#31639;&#27861;&#22312;&#21333;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#21644;&#22810;&#20803;&#39640;&#26031;&#36807;&#31243;&#20013;&#21487;&#20197;&#23454;&#29616;LRT&#26368;&#20248;&#24615;&#65292;&#20294;&#22312;&#20998;&#31867;&#39640;&#32500;&#38750;&#32447;&#24615;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#26102;&#26159;&#27425;&#20248;&#30340;&#12290;</title><link>http://arxiv.org/abs/2301.13112</link><description>&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#26041;&#27861;&#22312;&#20998;&#36776;&#25193;&#25955;&#36807;&#31243;&#20013;&#30340;&#26368;&#20248;&#24615;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking optimality of time series classification methods in distinguishing diffusions. (arXiv:2301.13112v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#20284;&#28982;&#27604;&#26816;&#39564;&#20026;&#22522;&#20934;&#27979;&#35797;TSC&#31639;&#27861;&#22312;&#21306;&#20998;&#25193;&#25955;&#36807;&#31243;&#20013;&#30340;&#26368;&#20248;&#24615;&#12290;&#38543;&#26426;&#26862;&#26519;&#12289;ResNet&#21644;ROCKET&#31639;&#27861;&#22312;&#21333;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#21644;&#22810;&#20803;&#39640;&#26031;&#36807;&#31243;&#20013;&#21487;&#20197;&#23454;&#29616;LRT&#26368;&#20248;&#24615;&#65292;&#20294;&#22312;&#20998;&#31867;&#39640;&#32500;&#38750;&#32447;&#24615;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#26102;&#26159;&#27425;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32479;&#35745;&#26368;&#20248;&#24615;&#22522;&#20934;&#27979;&#35797;&#23545;&#20110;&#20998;&#26512;&#21644;&#35774;&#35745;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#65288;TSC&#65289;&#31639;&#27861;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#20284;&#28982;&#27604;&#26816;&#39564;&#65288;LRT&#65289;&#22522;&#20934;&#27979;&#35797;TSC&#31639;&#27861;&#22312;&#21306;&#20998;&#25193;&#25955;&#36807;&#31243;&#20013;&#30340;&#26368;&#20248;&#24615;&#12290;LRT&#26159;&#26681;&#25454;Neyman-Pearson&#24341;&#29702;&#24471;&#20986;&#30340;&#26368;&#20248;&#20998;&#31867;&#22120;&#12290;LRT&#22522;&#20934;&#27979;&#35797;&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#22240;&#20026;LRT&#19981;&#38656;&#35201;&#35757;&#32451;&#65292;&#24182;&#19988;&#25193;&#25955;&#36807;&#31243;&#21487;&#20197;&#36827;&#34892;&#39640;&#25928;&#27169;&#25311;&#65292;&#21487;&#20197;&#28789;&#27963;&#21453;&#26144;&#20986;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#30340;&#29305;&#23450;&#29305;&#24449;&#12290;&#25105;&#20204;&#20351;&#29992;&#19977;&#31181;&#24120;&#29992;&#30340;TSC&#31639;&#27861;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65306;&#38543;&#26426;&#26862;&#26519;&#12289;ResNet&#21644;ROCKET&#12290;&#36825;&#20123;&#31639;&#27861;&#21487;&#20197;&#23454;&#29616;&#21333;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#21644;&#22810;&#20803;&#39640;&#26031;&#36807;&#31243;&#30340;LRT&#26368;&#20248;&#24615;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#27169;&#22411;&#26080;&#20851;&#30340;&#31639;&#27861;&#22312;&#20998;&#31867;&#39640;&#32500;&#38750;&#32447;&#24615;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#26102;&#26159;&#27425;&#20248;&#30340;&#12290;&#27492;&#22806;&#65292;LRT&#22522;&#20934;&#27979;&#35797;&#25552;&#20379;&#20102;&#24037;&#20855;&#26469;&#20998;&#26512;&#20998;&#31867;&#20934;&#30830;&#24615;&#19982;&#26102;&#38388;&#20381;&#36182;&#24615;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Statistical optimality benchmarking is crucial for analyzing and designing time series classification (TSC) algorithms. This study proposes to benchmark the optimality of TSC algorithms in distinguishing diffusion processes by the likelihood ratio test (LRT). The LRT is an optimal classifier by the Neyman-Pearson lemma. The LRT benchmarks are computationally efficient because the LRT does not need training, and the diffusion processes can be efficiently simulated and are flexible to reflect the specific features of real-world applications. We demonstrate the benchmarking with three widely-used TSC algorithms: random forest, ResNet, and ROCKET. These algorithms can achieve the LRT optimality for univariate time series and multivariate Gaussian processes. However, these model-agnostic algorithms are suboptimal in classifying high-dimensional nonlinear multivariate time series. Additionally, the LRT benchmark provides tools to analyze the dependence of classification accuracy on the time 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#23398;&#20064;&#22312;&#20004;&#20010;&#29609;&#23478;&#30340;&#20998;&#25955;&#21512;&#20316;Stackelberg&#21338;&#24328;&#20013;&#30340;&#24212;&#29992;&#65292;&#20551;&#35774;&#36861;&#38543;&#32773;&#20855;&#26377;&#20840;&#30693;&#65292;&#32467;&#26524;&#34920;&#26126;&#20854;&#23384;&#22312;&#21487;&#20197;&#20174;&#24120;&#25968;&#21040;&#25351;&#25968;&#32423;&#22320;&#25913;&#21464;&#36951;&#25022;&#26368;&#23567;&#21270;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#23384;&#22312;&#29420;&#29305;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2301.11518</link><description>&lt;p&gt;
&#20855;&#26377;&#20840;&#30693;&#36861;&#38543;&#32773;&#30340;Stackelberg Games&#20013;&#30340;&#22312;&#32447;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Online Learning in Stackelberg Games with an Omniscient Follower. (arXiv:2301.11518v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#23398;&#20064;&#22312;&#20004;&#20010;&#29609;&#23478;&#30340;&#20998;&#25955;&#21512;&#20316;Stackelberg&#21338;&#24328;&#20013;&#30340;&#24212;&#29992;&#65292;&#20551;&#35774;&#36861;&#38543;&#32773;&#20855;&#26377;&#20840;&#30693;&#65292;&#32467;&#26524;&#34920;&#26126;&#20854;&#23384;&#22312;&#21487;&#20197;&#20174;&#24120;&#25968;&#21040;&#25351;&#25968;&#32423;&#22320;&#25913;&#21464;&#36951;&#25022;&#26368;&#23567;&#21270;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#23384;&#22312;&#29420;&#29305;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#20010;&#29609;&#23478;&#30340;&#20998;&#25955;&#21512;&#20316;Stackelberg&#21338;&#24328;&#20013;&#30340;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#12290;&#22312;&#27599;&#19968;&#36718;&#20013;&#65292;&#39046;&#23548;&#32773;&#39318;&#20808;&#37319;&#21462;&#34892;&#21160;&#65292;&#20043;&#21518;&#36861;&#38543;&#32773;&#22312;&#35266;&#23519;&#39046;&#23548;&#32773;&#30340;&#34892;&#21160;&#21518;&#37319;&#21462;&#20182;&#20204;&#30340;&#34892;&#21160;&#12290;&#39046;&#23548;&#32773;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#20114;&#21160;&#21382;&#21490;&#26469;&#23398;&#20064;&#22914;&#20309;&#26368;&#23567;&#21270;&#32047;&#31215;&#36951;&#25022;&#12290;&#19981;&#21516;&#20110;&#20256;&#32479;&#30340;&#37325;&#22797;Stackelberg&#21338;&#24328;&#30340;&#34920;&#36848;&#65292;&#25105;&#20204;&#20551;&#35774;&#36861;&#38543;&#32773;&#26159;&#20840;&#30693;&#30340;&#65292;&#20855;&#26377;&#30495;&#23454;&#22870;&#21169;&#30340;&#20840;&#37096;&#30693;&#35782;&#65292;&#20182;&#20204;&#24635;&#26159;&#26368;&#20339;&#21709;&#24212;&#39046;&#23548;&#32773;&#30340;&#34892;&#21160;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#35813;&#37325;&#22797;Stackelberg&#21338;&#24328;&#20013;&#36951;&#25022;&#26368;&#23567;&#21270;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#26681;&#25454;&#22870;&#21169;&#32467;&#26500;&#65292;&#20840;&#30693;&#36861;&#38543;&#32773;&#30340;&#23384;&#22312;&#21487;&#33021;&#20250;&#20351;&#26679;&#26412;&#22797;&#26434;&#24230;&#20174;&#24120;&#25968;&#21040;&#25351;&#25968;&#32423;&#21457;&#29983;&#24040;&#22823;&#21464;&#21270;&#65292;&#21363;&#20351;&#26159;&#23545;&#20110;&#32447;&#24615;&#21512;&#20316;Stackelberg&#21338;&#24328;&#20063;&#26159;&#22914;&#27492;&#12290;&#36825;&#20026;&#39046;&#23548;&#32773;&#30340;&#23398;&#20064;&#36807;&#31243;&#21644;&#38543;&#21518;&#30340;&#36951;&#25022;&#20998;&#26512;&#24102;&#26469;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of online learning in a two-player decentralized cooperative Stackelberg game. In each round, the leader first takes an action, followed by the follower who takes their action after observing the leader's move. The goal of the leader is to learn to minimize the cumulative regret based on the history of interactions. Differing from the traditional formulation of repeated Stackelberg games, we assume the follower is omniscient, with full knowledge of the true reward, and that they always best-respond to the leader's actions. We analyze the sample complexity of regret minimization in this repeated Stackelberg game. We show that depending on the reward structure, the existence of the omniscient follower may change the sample complexity drastically, from constant to exponential, even for linear cooperative Stackelberg games. This poses unique challenges for the learning process of the leader and the subsequent regret analysis.
&lt;/p&gt;</description></item><item><title>LDMIC&#26159;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#20998;&#24067;&#24335;&#22810;&#35270;&#22270;&#22270;&#20687;&#32534;&#30721;&#26694;&#26550;&#65292;&#36890;&#36807;&#29420;&#31435;&#32534;&#30721;&#22120;&#21644;&#32852;&#21512;&#19978;&#19979;&#25991;&#20256;&#36755;&#27169;&#22359;&#23454;&#29616;&#20102;&#20840;&#23616;&#35270;&#22270;&#38388;&#30340;&#30456;&#20851;&#24615;&#25429;&#25417;&#65292;&#23545;&#20960;&#20309;&#20851;&#31995;&#19981;&#25935;&#24863;&#12290;</title><link>http://arxiv.org/abs/2301.09799</link><description>&lt;p&gt;
LDMIC&#65306;&#22522;&#20110;&#23398;&#20064;&#30340;&#20998;&#24067;&#24335;&#22810;&#35270;&#22270;&#22270;&#20687;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
LDMIC: Learning-based Distributed Multi-view Image Coding. (arXiv:2301.09799v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09799
&lt;/p&gt;
&lt;p&gt;
LDMIC&#26159;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#20998;&#24067;&#24335;&#22810;&#35270;&#22270;&#22270;&#20687;&#32534;&#30721;&#26694;&#26550;&#65292;&#36890;&#36807;&#29420;&#31435;&#32534;&#30721;&#22120;&#21644;&#32852;&#21512;&#19978;&#19979;&#25991;&#20256;&#36755;&#27169;&#22359;&#23454;&#29616;&#20102;&#20840;&#23616;&#35270;&#22270;&#38388;&#30340;&#30456;&#20851;&#24615;&#25429;&#25417;&#65292;&#23545;&#20960;&#20309;&#20851;&#31995;&#19981;&#25935;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;
LDMIC is a learning-based distributed multi-view image coding framework that captures global inter-view correlations through independent encoders and a joint context transfer module based on the cross-attention mechanism, which is insensitive to geometric relations.
&lt;/p&gt;
&lt;p&gt;
&#22810;&#35270;&#22270;&#22270;&#20687;&#21387;&#32553;&#22312;3D&#30456;&#20851;&#24212;&#29992;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#29616;&#26377;&#26041;&#27861;&#37319;&#29992;&#39044;&#27979;&#32534;&#30721;&#26550;&#26500;&#65292;&#38656;&#35201;&#32852;&#21512;&#32534;&#30721;&#21387;&#32553;&#30456;&#24212;&#30340;&#35270;&#24046;&#21644;&#27531;&#24046;&#20449;&#24687;&#12290;&#36825;&#35201;&#27714;&#30456;&#26426;&#20043;&#38388;&#36827;&#34892;&#21327;&#20316;&#65292;&#24182;&#24378;&#21046;&#25191;&#34892;&#19981;&#21516;&#35270;&#22270;&#20043;&#38388;&#30340;&#26497;&#32447;&#20960;&#20309;&#32422;&#26463;&#65292;&#36825;&#20351;&#24471;&#22312;&#20855;&#26377;&#38543;&#26426;&#37325;&#21472;&#35270;&#37326;&#30340;&#20998;&#24067;&#24335;&#30456;&#26426;&#31995;&#32479;&#20013;&#37096;&#32626;&#36825;&#20123;&#26041;&#27861;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#21516;&#26102;&#65292;&#20998;&#24067;&#24335;&#28304;&#32534;&#30721;&#29702;&#35770;&#34920;&#26126;&#65292;&#21487;&#20197;&#36890;&#36807;&#29420;&#31435;&#32534;&#30721;&#21644;&#32852;&#21512;&#35299;&#30721;&#23454;&#29616;&#30456;&#20851;&#28304;&#30340;&#39640;&#25928;&#25968;&#25454;&#21387;&#32553;&#65292;&#36825;&#28608;&#21457;&#20102;&#25105;&#20204;&#35774;&#35745;&#22522;&#20110;&#23398;&#20064;&#30340;&#20998;&#24067;&#24335;&#22810;&#35270;&#22270;&#22270;&#20687;&#32534;&#30721;&#65288;LDMIC&#65289;&#26694;&#26550;&#30340;&#21160;&#26426;&#12290;&#36890;&#36807;&#29420;&#31435;&#32534;&#30721;&#22120;&#65292;LDMIC&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22522;&#20110;&#20132;&#21449;&#27880;&#24847;&#26426;&#21046;&#30340;&#32852;&#21512;&#19978;&#19979;&#25991;&#20256;&#36755;&#27169;&#22359;&#65292;&#20197;&#26377;&#25928;&#25429;&#25417;&#20840;&#23616;&#35270;&#22270;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#23545;&#20960;&#20309;&#20851;&#31995;&#19981;&#25935;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-view image compression plays a critical role in 3D-related applications. Existing methods adopt a predictive coding architecture, which requires joint encoding to compress the corresponding disparity as well as residual information. This demands collaboration among cameras and enforces the epipolar geometric constraint between different views, which makes it challenging to deploy these methods in distributed camera systems with randomly overlapping fields of view. Meanwhile, distributed source coding theory indicates that efficient data compression of correlated sources can be achieved by independent encoding and joint decoding, which motivates us to design a learning-based distributed multi-view image coding (LDMIC) framework. With independent encoders, LDMIC introduces a simple yet effective joint context transfer module based on the cross-attention mechanism at the decoder to effectively capture the global inter-view correlations, which is insensitive to the geometric relation
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25193;&#23637;&#20102;&#20449;&#24687;&#22330;&#29702;&#35770;(IFT)&#21040;&#29289;&#29702;&#20449;&#24687;&#22330;&#29702;&#35770;(PIFT)&#65292;&#23558;&#25551;&#36848;&#22330;&#30340;&#29289;&#29702;&#23450;&#24459;&#30340;&#20449;&#24687;&#32534;&#30721;&#20026;&#20989;&#25968;&#20808;&#39564;&#12290;&#20174;&#36825;&#20010;PIFT&#24471;&#20986;&#30340;&#21518;&#39564;&#19982;&#20219;&#20309;&#25968;&#20540;&#26041;&#26696;&#26080;&#20851;&#65292;&#24182;&#19988;&#21487;&#20197;&#25429;&#25417;&#22810;&#31181;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2301.07609</link><description>&lt;p&gt;
&#29289;&#29702;&#23398;&#30693;&#35782;&#20316;&#20026;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#27169;&#22411;&#30340;&#20449;&#24687;&#22330;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Physics-informed Information Field Theory for Modeling Physical Systems with Uncertainty Quantification. (arXiv:2301.07609v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07609
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25193;&#23637;&#20102;&#20449;&#24687;&#22330;&#29702;&#35770;(IFT)&#21040;&#29289;&#29702;&#20449;&#24687;&#22330;&#29702;&#35770;(PIFT)&#65292;&#23558;&#25551;&#36848;&#22330;&#30340;&#29289;&#29702;&#23450;&#24459;&#30340;&#20449;&#24687;&#32534;&#30721;&#20026;&#20989;&#25968;&#20808;&#39564;&#12290;&#20174;&#36825;&#20010;PIFT&#24471;&#20986;&#30340;&#21518;&#39564;&#19982;&#20219;&#20309;&#25968;&#20540;&#26041;&#26696;&#26080;&#20851;&#65292;&#24182;&#19988;&#21487;&#20197;&#25429;&#25417;&#22810;&#31181;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#32467;&#21512;&#29289;&#29702;&#23398;&#30693;&#35782;&#26159;&#24314;&#27169;&#31995;&#32479;&#30340;&#24378;&#26377;&#21147;&#25216;&#26415;&#12290;&#27492;&#31867;&#27169;&#22411;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#23558;&#27979;&#37327;&#32467;&#26524;&#19982;&#24050;&#30693;&#29289;&#29702;&#23450;&#24459;&#30456;&#32467;&#21512;&#65292;&#39640;&#25928;&#22320;&#27714;&#35299;&#22522;&#26412;&#22330;&#12290;&#30001;&#20110;&#35768;&#22810;&#31995;&#32479;&#21253;&#21547;&#26410;&#30693;&#20803;&#32032;&#65292;&#22914;&#32570;&#22833;&#21442;&#25968;&#12289;&#22024;&#26434;&#25968;&#25454;&#25110;&#19981;&#23436;&#25972;&#30340;&#29289;&#29702;&#23450;&#24459;&#65292;&#22240;&#27492;&#36825;&#36890;&#24120;&#34987;&#35270;&#20026;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#38382;&#39064;&#12290;&#22788;&#29702;&#25152;&#26377;&#21464;&#37327;&#30340;&#24120;&#35265;&#25216;&#26415;&#36890;&#24120;&#21462;&#20915;&#20110;&#29992;&#20110;&#36817;&#20284;&#21518;&#39564;&#30340;&#25968;&#20540;&#26041;&#26696;&#65292;&#24182;&#19988;&#24076;&#26395;&#26377;&#19968;&#31181;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#31163;&#25955;&#21270;&#30340;&#26041;&#27861;&#12290;&#20449;&#24687;&#22330;&#29702;&#35770;&#65288;IFT&#65289;&#25552;&#20379;&#20102;&#23545;&#19981;&#19968;&#23450;&#26159;&#39640;&#26031;&#22330;&#30340;&#22330;&#36827;&#34892;&#32479;&#35745;&#23398;&#30340;&#24037;&#20855;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#25551;&#36848;&#22330;&#30340;&#29289;&#29702;&#23450;&#24459;&#30340;&#20449;&#24687;&#32534;&#30721;&#20026;&#20989;&#25968;&#20808;&#39564;&#26469;&#25193;&#23637;IFT&#21040;&#29289;&#29702;&#20449;&#24687;&#22330;&#29702;&#35770;&#65288;PIFT&#65289;&#12290;&#20174;&#36825;&#20010;PIFT&#24471;&#20986;&#30340;&#21518;&#39564;&#19982;&#20219;&#20309;&#25968;&#20540;&#26041;&#26696;&#26080;&#20851;&#65292;&#24182;&#19988;&#21487;&#20197;&#25429;&#25417;&#22810;&#31181;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven approaches coupled with physical knowledge are powerful techniques to model systems. The goal of such models is to efficiently solve for the underlying field by combining measurements with known physical laws. As many systems contain unknown elements, such as missing parameters, noisy data, or incomplete physical laws, this is widely approached as an uncertainty quantification problem. The common techniques to handle all the variables typically depend on the numerical scheme used to approximate the posterior, and it is desirable to have a method which is independent of any such discretization. Information field theory (IFT) provides the tools necessary to perform statistics over fields that are not necessarily Gaussian. We extend IFT to physics-informed IFT (PIFT) by encoding the functional priors with information about the physical laws which describe the field. The posteriors derived from this PIFT remain independent of any numerical scheme and can capture multiple modes,
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#20851;&#27880;&#26435;&#37325;&#30697;&#38453;&#20869;&#26435;&#37325;&#30340;&#31354;&#38388;&#25490;&#21015;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#27169;&#22411;&#21442;&#25968;&#30340;&#38750;&#38646;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2301.07285</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#39062;&#30340;&#31232;&#30095;&#27491;&#21017;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Novel Sparse Regularizer. (arXiv:2301.07285v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07285
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#20851;&#27880;&#26435;&#37325;&#30697;&#38453;&#20869;&#26435;&#37325;&#30340;&#31354;&#38388;&#25490;&#21015;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#27169;&#22411;&#21442;&#25968;&#30340;&#38750;&#38646;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#19981;&#22522;&#20110; $L_{p}$-norm &#30340;&#26032;&#39062;&#27491;&#21017;&#21270;&#26041;&#27861;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#21482;&#32771;&#34385;&#27169;&#22411;&#20013;&#21508;&#26435;&#37325;&#20540;&#30340;&#24230;&#37327;&#19981;&#21516;&#65292;&#36825;&#31181;&#27491;&#21017;&#21270;&#26041;&#27861;&#20851;&#27880;&#26435;&#37325;&#30697;&#38453;&#20869;&#26435;&#37325;&#30340;&#31354;&#38388;&#25490;&#21015;&#12290;&#35813;&#26041;&#27861;&#30340;&#21152;&#20837;&#39033;&#21487;&#20197;&#29992;&#20110;&#25439;&#22833;&#20989;&#25968;&#20013;&#65292;&#21487;&#24494;&#20998;&#65292;&#31616;&#21333;&#24555;&#36895;&#35745;&#31639;&#65292;&#19982;&#23610;&#24230;&#26080;&#20851;&#65292;&#20165;&#38656;&#35201;&#24494;&#23567;&#30340;&#39069;&#22806;&#20869;&#23384;&#65292;&#23481;&#26131;&#24182;&#34892;&#21270;&#12290;&#32463;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#30456;&#21516;&#31934;&#24230;&#27700;&#24179;&#19979;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20351;&#27169;&#22411;&#21442;&#25968;&#30340;&#38750;&#38646;&#25968;&#37327;&#25552;&#39640;&#19968;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
$L_{p}$-norm regularization schemes such as $L_{0}$, $L_{1}$, and $L_{2}$-norm regularization and $L_{p}$-norm-based regularization techniques such as weight decay and group LASSO compute a quantity which de pends on model weights considered in isolation from one another. This paper describes a novel regularizer which is not based on an $L_{p}$-norm. In contrast with $L_{p}$-norm-based regularization, this regularizer is concerned with the spatial arrangement of weights within a weight matrix. This regularizer is an additive term for the loss function and is differentiable, simple and fast to compute, scale-invariant, requires a trivial amount of additional memory, and can easily be parallelized. Empirically this method yields approximately a one order-of-magnitude improvement in the number of nonzero model parameters at a given level of accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20123;&#26497;&#31471;&#30340;&#22270;&#20687;&#21464;&#25442;&#65292;&#21457;&#29616;&#26426;&#22120;&#21644;&#20154;&#31867;&#22312;&#36825;&#20123;&#21464;&#25442;&#19979;&#30340;&#34920;&#29616;&#24046;&#24322;&#36739;&#22823;&#65292;&#26426;&#22120;&#22312;&#26576;&#20123;&#21464;&#25442;&#19979;&#27604;&#20154;&#31867;&#34920;&#29616;&#26356;&#22909;&#65292;&#20294;&#22312;&#20154;&#31867;&#23481;&#26131;&#22788;&#29702;&#30340;&#21464;&#25442;&#19979;&#34920;&#29616;&#19981;&#22914;&#20154;&#31867;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#26469;&#25913;&#21892;&#26426;&#22120;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.13967</link><description>&lt;p&gt;
&#26497;&#31471;&#22270;&#20687;&#21464;&#25442;&#22312;&#20154;&#31867;&#21644;&#26426;&#22120;&#19978;&#20135;&#29983;&#19981;&#21516;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Extreme Image Transformations Affect Humans and Machines Differently. (arXiv:2212.13967v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.13967
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20123;&#26497;&#31471;&#30340;&#22270;&#20687;&#21464;&#25442;&#65292;&#21457;&#29616;&#26426;&#22120;&#21644;&#20154;&#31867;&#22312;&#36825;&#20123;&#21464;&#25442;&#19979;&#30340;&#34920;&#29616;&#24046;&#24322;&#36739;&#22823;&#65292;&#26426;&#22120;&#22312;&#26576;&#20123;&#21464;&#25442;&#19979;&#27604;&#20154;&#31867;&#34920;&#29616;&#26356;&#22909;&#65292;&#20294;&#22312;&#20154;&#31867;&#23481;&#26131;&#22788;&#29702;&#30340;&#21464;&#25442;&#19979;&#34920;&#29616;&#19981;&#22914;&#20154;&#31867;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#26469;&#25913;&#21892;&#26426;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20123;&#26368;&#36817;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;(ANN)&#22768;&#31216;&#27169;&#25311;&#28789;&#38271;&#31867;&#21160;&#29289;&#31070;&#32463;&#21644;&#20154;&#31867;&#30340;&#34920;&#29616;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#23545;&#35937;&#35782;&#21035;&#26041;&#38754;&#30340;&#25104;&#21151;&#20381;&#36182;&#20110;&#20197;&#20154;&#31867;&#19981;&#24120;&#37319;&#29992;&#30340;&#26041;&#24335;&#21033;&#29992;&#20302;&#32423;&#29305;&#24449;&#26469;&#35299;&#20915;&#35270;&#35273;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;ANN&#26469;&#35828;&#65292;&#36229;&#20986;&#20998;&#24067;&#25110;&#23545;&#25239;&#24615;&#36755;&#20837;&#32463;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#30456;&#21453;&#65292;&#20154;&#31867;&#23398;&#20064;&#25277;&#35937;&#30340;&#27169;&#24335;&#65292;&#24182;&#19988;&#23545;&#35768;&#22810;&#26497;&#31471;&#22270;&#20687;&#30072;&#21464;&#30340;&#24433;&#21709;&#24456;&#23567;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#32452;&#26032;&#30340;&#22270;&#20687;&#21464;&#25442;&#65292;&#21463;&#31070;&#32463;&#29983;&#29702;&#23398;&#21457;&#29616;&#30340;&#21551;&#21457;&#65292;&#24182;&#22312;&#23545;&#35937;&#35782;&#21035;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#20154;&#31867;&#21644;ANN&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#23545;&#20110;&#26576;&#20123;&#21464;&#25442;&#65292;&#26426;&#22120;&#30340;&#34920;&#29616;&#27604;&#20154;&#31867;&#26356;&#22909;&#65292;&#32780;&#23545;&#20110;&#26576;&#20123;&#23545;&#20154;&#31867;&#31616;&#21333;&#30340;&#21464;&#25442;&#65292;&#26426;&#22120;&#38590;&#20197;&#19982;&#20154;&#31867;&#34920;&#29616;&#30456;&#24403;&#12290;&#25105;&#20204;&#37327;&#21270;&#20102;&#20154;&#31867;&#21644;&#26426;&#22120;&#30340;&#20934;&#30830;&#24615;&#24046;&#24322;&#65292;&#24182;&#20026;&#25105;&#20204;&#30340;&#21464;&#25442;&#25968;&#25454;&#25214;&#21040;&#20102;&#19968;&#20010;&#38590;&#24230;&#25490;&#21517;&#12290;&#25105;&#20204;&#36824;&#24314;&#35758;&#22914;&#20309;&#25913;&#21464;&#20154;&#31867;&#35270;&#35273;&#22788;&#29702;&#30340;&#26576;&#20123;&#29305;&#24449;&#20197;&#25552;&#39640;ANN&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Some recent artificial neural networks (ANNs) claim to model aspects of primate neural and human performance data. Their success in object recognition is, however, dependent on exploiting low-level features for solving visual tasks in a way that humans do not. As a result, out-of-distribution or adversarial input is often challenging for ANNs. Humans instead learn abstract patterns and are mostly unaffected by many extreme image distortions. We introduce a set of novel image transforms inspired by neurophysiological findings and evaluate humans and ANNs on an object recognition task. We show that machines perform better than humans for certain transforms and struggle to perform at par with humans on others that are easy for humans. We quantify the differences in accuracy for humans and machines and find a ranking of difficulty for our transforms for human data. We also suggest how certain characteristics of human visual processing can be adapted to improve the performance of ANNs for o
&lt;/p&gt;</description></item><item><title>BaCO&#26159;&#19968;&#20010;&#24555;&#36895;&#21487;&#31227;&#26893;&#30340;&#33258;&#21160;&#21270;&#32534;&#35793;&#22120;&#20248;&#21270;&#26694;&#26550;&#65292;&#20351;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#65292;&#33021;&#22815;&#20197;&#23567;&#25628;&#32034;&#39044;&#31639;&#25552;&#20379;1.36x-1.56x&#26356;&#24555;&#30340;&#20195;&#30721;&#24182;&#20197;2.9x-3.9x&#30340;&#36895;&#24230;&#36798;&#21040;&#19987;&#23478;&#32423;&#21035;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.11142</link><description>&lt;p&gt;
BaCO: &#19968;&#20010;&#24555;&#36895;&#21487;&#31227;&#26893;&#30340;&#36125;&#21494;&#26031;&#32534;&#35793;&#22120;&#20248;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
BaCO: A Fast and Portable Bayesian Compiler Optimization Framework. (arXiv:2212.11142v2 [cs.PL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.11142
&lt;/p&gt;
&lt;p&gt;
BaCO&#26159;&#19968;&#20010;&#24555;&#36895;&#21487;&#31227;&#26893;&#30340;&#33258;&#21160;&#21270;&#32534;&#35793;&#22120;&#20248;&#21270;&#26694;&#26550;&#65292;&#20351;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#65292;&#33021;&#22815;&#20197;&#23567;&#25628;&#32034;&#39044;&#31639;&#25552;&#20379;1.36x-1.56x&#26356;&#24555;&#30340;&#20195;&#30721;&#24182;&#20197;2.9x-3.9x&#30340;&#36895;&#24230;&#36798;&#21040;&#19987;&#23478;&#32423;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#36125;&#21494;&#26031;&#32534;&#35793;&#22120;&#20248;&#21270;&#26694;&#26550;&#65288;BaCO&#65289;&#65292;&#23427;&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#33258;&#21160;&#35843;&#35856;&#22120;&#65292;&#36866;&#29992;&#20110;&#38754;&#21521;CPU&#12289;GPU&#21644;FPGA&#30340;&#29616;&#20195;&#32534;&#35793;&#22120;&#12290;BaCO&#25552;&#20379;&#20102;&#22788;&#29702;&#29616;&#20195;&#33258;&#21160;&#35843;&#35856;&#20219;&#21153;&#25152;&#38656;&#30340;&#28789;&#27963;&#24615;&#12290;&#29305;&#21035;&#22320;&#65292;&#23427;&#22788;&#29702;&#25490;&#21015;&#12289;&#26377;&#24207;&#21644;&#36830;&#32493;&#21442;&#25968;&#31867;&#22411;&#20197;&#21450;&#24050;&#30693;&#21644;&#26410;&#30693;&#21442;&#25968;&#32422;&#26463;&#12290;&#20026;&#20102;&#25512;&#29702;&#36825;&#20123;&#21442;&#25968;&#31867;&#22411;&#24182;&#39640;&#25928;&#22320;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20195;&#30721;&#65292;BaCO&#20351;&#29992;&#38024;&#23545;&#33258;&#21160;&#35843;&#35856;&#39046;&#22495;&#19987;&#38376;&#20248;&#21270;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#29616;&#20195;&#32534;&#35793;&#22120;&#31995;&#32479;&#65306;TACO&#12289;RISE&#65286;ELEVATE&#21644;CPU&#12289;GPU&#21644;FPGA&#30340;HPVM2FPGA&#19978;&#23637;&#31034;&#20102;BaCO&#30340;&#26377;&#25928;&#24615;&#12290;&#23545;&#20110;&#36825;&#20123;&#39046;&#22495;&#65292;BaCO&#36890;&#36807;&#20351;&#29992;&#24456;&#23567;&#30340;&#25628;&#32034;&#39044;&#31639;&#65292;&#24179;&#22343;&#25552;&#20379;1.36x-1.56x&#26356;&#24555;&#30340;&#20195;&#30721;&#65292;&#24182;&#19988;BaCO&#33021;&#22815;&#27604;&#19987;&#23478;&#27700;&#24179;&#30340;&#24615;&#33021;&#26356;&#24555;&#22320;&#23454;&#29616;2.9x-3.9x&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the Bayesian Compiler Optimization framework (BaCO), a general purpose autotuner for modern compilers targeting CPUs, GPUs, and FPGAs. BaCO provides the flexibility needed to handle the requirements of modern autotuning tasks. Particularly, it deals with permutation, ordered, and continuous parameter types along with both known and unknown parameter constraints. To reason about these parameter types and efficiently deliver high-quality code, BaCO uses Bayesian optimiza tion algorithms specialized towards the autotuning domain. We demonstrate BaCO's effectiveness on three modern compiler systems: TACO, RISE &amp; ELEVATE, and HPVM2FPGA for CPUs, GPUs, and FPGAs respectively. For these domains, BaCO outperforms current state-of-the-art autotuners by delivering on average 1.36x-1.56x faster code with a tiny search budget, and BaCO is able to reach expert-level performance 2.9x-3.9x faster.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#31574;&#30053;&#28151;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#19981;&#24179;&#34913;&#30340;&#26368;&#20248;&#20256;&#36755;&#23454;&#29616;&#31574;&#30053;&#28151;&#21512;&#65292;&#24041;&#22266;&#24213;&#23618;&#40654;&#26364;&#36816;&#21160;&#31574;&#30053;&#30340;&#27604;&#20363;&#65292;&#26377;&#25928;&#35843;&#25972;&#40654;&#26364;&#30697;&#38453;&#65292;&#20915;&#23450;&#19987;&#23478;&#21644;&#20195;&#29702;&#20154;&#20043;&#38388;&#30340;&#20248;&#20808;&#32423;&#65292;&#20445;&#35777;&#23433;&#20840;&#21644;&#20219;&#21153;&#25104;&#21151;&#65292;&#24182;&#22312;&#19968;&#31995;&#21015;&#26426;&#22120;&#20154;&#25511;&#21046;&#30340;&#24212;&#29992;&#22330;&#26223;&#20013;&#36229;&#36807;&#29616;&#26377;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2212.01938</link><description>&lt;p&gt;
&#20998;&#32423;&#31574;&#30053;&#28151;&#21512;&#20316;&#20026;&#26368;&#20248;&#36755;&#36816;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Policy Blending As Optimal Transport. (arXiv:2212.01938v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01938
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#31574;&#30053;&#28151;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#19981;&#24179;&#34913;&#30340;&#26368;&#20248;&#20256;&#36755;&#23454;&#29616;&#31574;&#30053;&#28151;&#21512;&#65292;&#24041;&#22266;&#24213;&#23618;&#40654;&#26364;&#36816;&#21160;&#31574;&#30053;&#30340;&#27604;&#20363;&#65292;&#26377;&#25928;&#35843;&#25972;&#40654;&#26364;&#30697;&#38453;&#65292;&#20915;&#23450;&#19987;&#23478;&#21644;&#20195;&#29702;&#20154;&#20043;&#38388;&#30340;&#20248;&#20808;&#32423;&#65292;&#20445;&#35777;&#23433;&#20840;&#21644;&#20219;&#21153;&#25104;&#21151;&#65292;&#24182;&#22312;&#19968;&#31995;&#21015;&#26426;&#22120;&#20154;&#25511;&#21046;&#30340;&#24212;&#29992;&#22330;&#26223;&#20013;&#36229;&#36807;&#29616;&#26377;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#20998;&#32423;&#31574;&#30053;&#28151;&#21512;&#20316;&#20026;&#26368;&#20248;&#36755;&#36816;&#65288;HiPBOT&#65289;&#12290;&#36825;&#31181;&#20998;&#23618;&#26694;&#26550;&#36890;&#36807;&#23545;&#19987;&#23478;&#31574;&#30053;&#30340;&#20302;&#32423;&#21453;&#24212;&#36866;&#24212;&#26435;&#37325;&#65292;&#22312;&#19987;&#23478;&#31574;&#30053;&#21644;&#20195;&#29702;&#20154;&#30340;&#21442;&#25968;&#31354;&#38388;&#19978;&#28155;&#21152;&#20102;&#19968;&#20010;&#21069;&#30651;&#35268;&#21010;&#23618;&#12290;&#25105;&#20204;&#30340;&#39640;&#23618;&#35268;&#21010;&#32773;&#36890;&#36807;&#19981;&#24179;&#34913;&#30340;&#26368;&#20248;&#20256;&#36755;&#23454;&#29616;&#31574;&#30053;&#28151;&#21512;&#65292;&#24041;&#22266;&#24213;&#23618;&#40654;&#26364;&#36816;&#21160;&#31574;&#30053;&#30340;&#27604;&#20363;&#65292;&#26377;&#25928;&#35843;&#25972;&#23427;&#20204;&#30340;&#40654;&#26364;&#30697;&#38453;&#65292;&#24182;&#22312;&#19987;&#23478;&#21644;&#20195;&#29702;&#20154;&#20043;&#38388;&#20915;&#23450;&#20248;&#20808;&#32423;&#65292;&#20445;&#35777;&#23433;&#20840;&#21644;&#20219;&#21153;&#25104;&#21151;&#12290;&#25105;&#20204;&#22312;&#20174;&#20302;&#32500;&#23548;&#33322;&#21040;&#39640;&#32500;&#20840;&#36523;&#25511;&#21046;&#30340;&#19968;&#31995;&#21015;&#24212;&#29992;&#22330;&#26223;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;HiPBOT&#30340;&#21151;&#25928;&#21644;&#25928;&#29575;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#25191;&#34892;&#27010;&#29575;&#25512;&#26029;&#25110;&#23450;&#20041;&#19987;&#23478;&#26641;&#32467;&#26500;&#30340;&#29616;&#26377;&#22522;&#32447;&#65292;&#20026;&#26368;&#20248;&#36755;&#36816;&#22312;&#26426;&#22120;&#20154;&#25511;&#21046;&#20013;&#30340;&#26032;&#24212;&#29992;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present hierarchical policy blending as optimal transport (HiPBOT). This hierarchical framework adapts the weights of low-level reactive expert policies, adding a look-ahead planning layer on the parameter space of a product of expert policies and agents. Our high-level planner realizes a policy blending via unbalanced optimal transport, consolidating the scaling of underlying Riemannian motion policies, effectively adjusting their Riemannian matrix, and deciding over the priorities between experts and agents, guaranteeing safety and task success. Our experimental results in a range of application scenarios from low-dimensional navigation to high-dimensional whole-body control showcase the efficacy and efficiency of HiPBOT, which outperforms state-of-the-art baselines that either perform probabilistic inference or define a tree structure of experts, paving the way for new applications of optimal transport to robot control. More material at https://sites.google.com/view/hipobot
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#26597;&#35810;&#30340;&#22810;&#27169;&#24577;&#36335;&#24452;&#34701;&#21512;&#31639;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#23545;&#30693;&#35782;&#24211;&#36827;&#34892;&#34917;&#20840;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#19988;&#20351;&#29992;&#20102;&#22522;&#20110;&#26597;&#35810;&#30340;&#25216;&#26415;&#26469;&#25552;&#39640;&#31995;&#32479;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2212.01923</link><description>&lt;p&gt;
&#22522;&#20110;&#26597;&#35810;&#30340;&#22810;&#27169;&#24577;&#36335;&#24452;&#34701;&#21512;&#30693;&#35782;&#24211;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Query-Driven Knowledge Base Completion using Multimodal Path Fusion over Multimodal Knowledge Graph. (arXiv:2212.01923v2 [cs.DB] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01923
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26597;&#35810;&#30340;&#22810;&#27169;&#24577;&#36335;&#24452;&#34701;&#21512;&#31639;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#23545;&#30693;&#35782;&#24211;&#36827;&#34892;&#34917;&#20840;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#19988;&#20351;&#29992;&#20102;&#22522;&#20110;&#26597;&#35810;&#30340;&#25216;&#26415;&#26469;&#25552;&#39640;&#31995;&#32479;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#20013;&#65292;&#22823;&#22411;&#30693;&#35782;&#24211;&#24050;&#32463;&#34987;&#26500;&#24314;&#26469;&#23384;&#20648;&#22823;&#37327;&#30340;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30693;&#35782;&#24211;&#39640;&#24230;&#19981;&#23436;&#25972;&#65292;&#20363;&#22914;Freebase&#20013;&#26377;70%&#30340;&#20154;&#27809;&#26377;&#20986;&#29983;&#22320;&#28857;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#20449;&#24687;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#30340;&#12289;&#22522;&#20110;&#26597;&#35810;&#39537;&#21160;&#30340;&#30693;&#35782;&#24211;&#34917;&#20840;&#31995;&#32479;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#34701;&#21512;&#26469;&#33258;Web&#30340;&#38750;&#32467;&#26500;&#21270;&#20449;&#24687;&#21644;&#30693;&#35782;&#24211;&#20013;&#30340;&#32467;&#26500;&#21270;&#20449;&#24687;&#20197;&#23454;&#29616;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#22522;&#20110;&#38382;&#31572;&#21644;&#35268;&#21017;&#25512;&#29702;&#26500;&#24314;&#20102;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#36335;&#24452;&#34701;&#21512;&#31639;&#27861;&#65292;&#22312;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#20013;&#22522;&#20110;&#19981;&#21516;&#30340;&#36335;&#24452;&#23545;&#20505;&#36873;&#31572;&#26696;&#36827;&#34892;&#25490;&#21517;&#65292;&#21462;&#24471;&#20102;&#27604;&#38382;&#31572;&#12289;&#35268;&#21017;&#25512;&#29702;&#21644;&#22522;&#32447;&#34701;&#21512;&#31639;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#25552;&#39640;&#31995;&#32479;&#25928;&#29575;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#22522;&#20110;&#26597;&#35810;&#30340;&#25216;&#26415;&#26469;&#20943;&#23569;&#31995;&#32479;&#30340;&#36816;&#34892;&#26102;&#38388;&#65292;&#20026;&#29992;&#25143;&#26597;&#35810;&#25552;&#20379;&#24555;&#36895;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past few years, large knowledge bases have been constructed to store massive amounts of knowledge. However, these knowledge bases are highly incomplete, for example, over 70% of people in Freebase have no known place of birth. To solve this problem, we propose a query-driven knowledge base completion system with multimodal fusion of unstructured and structured information. To effectively fuse unstructured information from the Web and structured information in knowledge bases to achieve good performance, our system builds multimodal knowledge graphs based on question answering and rule inference. We propose a multimodal path fusion algorithm to rank candidate answers based on different paths in the multimodal knowledge graphs, achieving much better performance than question answering, rule inference and a baseline fusion algorithm. To improve system efficiency, query-driven techniques are utilized to reduce the runtime of our system, providing fast responses to user queries. Ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#33258;&#25105;&#38598;&#25104;&#20445;&#25252;&#25216;&#26415;&#65292;&#36890;&#36807;&#23545;&#35757;&#32451;&#25968;&#25454;&#28155;&#21152;&#19981;&#21487;&#24863;&#30693;&#30340;&#25200;&#21160;&#65292;&#36890;&#36807;&#27169;&#22411;&#26816;&#26597;&#28857;&#30340;&#26799;&#24230;&#21457;&#29616;&#36825;&#20123;&#26679;&#26412;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#38450;&#27490;&#31454;&#20105;&#23545;&#25163;&#22312;&#25968;&#25454;&#19978;&#35757;&#32451;&#39640;&#24615;&#33021;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2211.12005</link><description>&lt;p&gt;
&#33258;&#25105;&#38598;&#25104;&#20445;&#25252;&#65306;&#35757;&#32451;&#26816;&#26597;&#28857;&#26159;&#33391;&#22909;&#30340;&#25968;&#25454;&#20445;&#25252;&#32773;
&lt;/p&gt;
&lt;p&gt;
Self-Ensemble Protection: Training Checkpoints Are Good Data Protectors. (arXiv:2211.12005v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.12005
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#33258;&#25105;&#38598;&#25104;&#20445;&#25252;&#25216;&#26415;&#65292;&#36890;&#36807;&#23545;&#35757;&#32451;&#25968;&#25454;&#28155;&#21152;&#19981;&#21487;&#24863;&#30693;&#30340;&#25200;&#21160;&#65292;&#36890;&#36807;&#27169;&#22411;&#26816;&#26597;&#28857;&#30340;&#26799;&#24230;&#21457;&#29616;&#36825;&#20123;&#26679;&#26412;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#38450;&#27490;&#31454;&#20105;&#23545;&#25163;&#22312;&#25968;&#25454;&#19978;&#35757;&#32451;&#39640;&#24615;&#33021;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25968;&#25454;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#20844;&#21496;&#22312;&#21457;&#24067;&#25968;&#25454;&#26102;&#36890;&#24120;&#20250;&#38750;&#24120;&#35880;&#24910;&#65292;&#22240;&#20026;&#31454;&#20105;&#23545;&#25163;&#21487;&#20197;&#20351;&#29992;&#23427;&#26469;&#35757;&#32451;&#39640;&#24615;&#33021;&#27169;&#22411;&#65292;&#20174;&#32780;&#23545;&#20844;&#21496;&#30340;&#21830;&#19994;&#31454;&#20105;&#21147;&#36896;&#25104;&#24040;&#22823;&#23041;&#32961;&#12290;&#20026;&#20102;&#38450;&#27490;&#22312;&#25968;&#25454;&#19978;&#35757;&#32451;&#33391;&#22909;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#21487;&#20197;&#23545;&#20854;&#28155;&#21152;&#19981;&#21487;&#24863;&#30693;&#30340;&#25200;&#21160;&#12290;&#30001;&#20110;&#36825;&#26679;&#30340;&#25200;&#21160;&#26088;&#22312;&#20260;&#23475;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#65292;&#22240;&#27492;&#23427;&#20204;&#24212;&#35813;&#21453;&#26144;DNN&#35757;&#32451;&#30340;&#33030;&#24369;&#24615;&#65292;&#32780;&#19981;&#26159;&#21333;&#20010;&#27169;&#22411;&#30340;&#33030;&#24369;&#24615;&#12290;&#22522;&#20110;&#36825;&#20010;&#26032;&#24819;&#27861;&#65292;&#25105;&#20204;&#23547;&#25214;&#22312;&#35757;&#32451;&#20013;&#22987;&#32456;&#26080;&#27861;&#35782;&#21035;&#65288;&#20174;&#26410;&#34987;&#27491;&#30830;&#20998;&#31867;&#65289;&#30340;&#25200;&#21160;&#26679;&#26412;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#27169;&#22411;&#26816;&#26597;&#28857;&#30340;&#26799;&#24230;&#21457;&#29616;&#36825;&#20123;&#26679;&#26412;&#65292;&#24418;&#25104;&#25152;&#25552;&#20986;&#30340;&#33258;&#25105;&#38598;&#25104;&#20445;&#25252;&#65288;SEP&#65289;&#12290;&#35813;&#26041;&#27861;&#38750;&#24120;&#26377;&#25928;&#65292;&#22240;&#20026;&#65288;1&#65289;&#22312;&#27491;&#24120;&#35757;&#32451;&#36807;&#31243;&#20013;&#24573;&#30053;&#30340;&#31034;&#20363;&#19978;&#36827;&#34892;&#23398;&#20064;&#24448;&#24448;&#20250;&#20135;&#29983;&#19981;&#24573;&#30053;&#27491;&#24120;&#31034;&#20363;&#30340;DNN&#65307;&#65288;2&#65289;&#26816;&#26597;&#28857;&#20043;&#38388;&#36328;&#27169;&#22411;&#30340;&#26799;&#24230;&#19982;&#27491;&#20132;&#25509;&#36817;&#65292;&#34920;&#31034;&#23427;&#20204;&#19982;&#20855;&#26377;&#19981;&#21516;&#20307;&#31995;&#32467;&#26500;&#30340;DNN&#19968;&#26679;&#22810;&#26679;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
As data becomes increasingly vital, a company would be very cautious about releasing data, because the competitors could use it to train high-performance models, thereby posing a tremendous threat to the company's commercial competence. To prevent training good models on the data, we could add imperceptible perturbations to it. Since such perturbations aim at hurting the entire training process, they should reflect the vulnerability of DNN training, rather than that of a single model. Based on this new idea, we seek perturbed examples that are always unrecognized (never correctly classified) in training. In this paper, we uncover them by model checkpoints' gradients, forming the proposed self-ensemble protection (SEP), which is very effective because (1) learning on examples ignored during normal training tends to yield DNNs ignoring normal examples; (2) checkpoints' cross-model gradients are close to orthogonal, meaning that they are as diverse as DNNs with different architectures. Th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;HypAD&#65292;&#23427;&#37319;&#29992;&#36229;&#20960;&#20309;&#19981;&#30830;&#23450;&#24615;&#26469;&#35780;&#20272;&#24322;&#24120;&#65292;&#21033;&#29992;&#26080;&#30417;&#30563;&#23398;&#20064;&#26469;&#37325;&#26032;&#24314;&#31435;&#36755;&#20837;&#20449;&#21495;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.09224</link><description>&lt;p&gt;
&#8220;&#25105;&#20204;&#26159;&#21542;&#30830;&#20449;&#36825;&#26159;&#24322;&#24120;&#65311;&#65288;arXiv:2211.09224v3 [cs.LG] UPDATED&#65289;&#8221;
&lt;/p&gt;
&lt;p&gt;
Are we certain it's anomalous?. (arXiv:2211.09224v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;HypAD&#65292;&#23427;&#37319;&#29992;&#36229;&#20960;&#20309;&#19981;&#30830;&#23450;&#24615;&#26469;&#35780;&#20272;&#24322;&#24120;&#65292;&#21033;&#29992;&#26080;&#30417;&#30563;&#23398;&#20064;&#26469;&#37325;&#26032;&#24314;&#31435;&#36755;&#20837;&#20449;&#21495;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#20110;&#26102;&#38388;&#24207;&#21015;&#21644;&#32467;&#26500;&#21270;&#25968;&#25454;&#24207;&#21015;&#30340;&#24314;&#27169;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#36825;&#22823;&#22823;&#25512;&#36827;&#20102;&#24322;&#24120;&#26816;&#27979;&#39046;&#22495;&#30340;&#30740;&#31350;&#12290;&#35813;&#20219;&#21153;&#29992;&#20110;&#35782;&#21035;&#37329;&#34701;&#24207;&#21015;&#12289;IT&#31995;&#32479;&#12289;&#33322;&#22825;&#27979;&#37327;&#20197;&#21450;&#21307;&#30103;&#39046;&#22495;&#20013;&#30340;&#24322;&#24120;&#34892;&#20026;&#65292;&#20854;&#20013;&#24322;&#24120;&#26816;&#27979;&#21487;&#20197;&#24110;&#21161;&#38548;&#31163;&#20986;&#25233;&#37057;&#30151;&#21644;&#32769;&#24180;&#20154;&#30340;&#29305;&#27530;&#30149;&#20363;&#12290;&#30001;&#20110;&#38750;&#24120;&#19981;&#30830;&#23450;&#24615;&#30340;&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#20197;&#21450;&#24322;&#24120;&#30340;&#23450;&#20041;&#26377;&#26102;&#26159;&#20027;&#35266;&#30340;&#65292;&#25152;&#20197;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#26159;&#19968;&#39033;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#20351;&#29992;&#8220;&#36229;&#20960;&#20309;&#19981;&#30830;&#23450;&#24615;&#8221;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#65288;HypAD&#65289;&#12290;HypAD&#36890;&#36807;&#26080;&#30417;&#30563;&#23398;&#20064;&#26469;&#37325;&#24314;&#36755;&#20837;&#20449;&#21495;&#12290;&#25105;&#20204;&#37319;&#29992;&#29616;&#26377;&#25216;&#26415;&#30340;&#26368;&#20339;&#23454;&#36341;&#65292;&#36890;&#36807;LSTM&#24314;&#31435;&#24207;&#21015;&#65292;&#21516;&#26102;&#23398;&#20064;&#35299;&#30721;&#22120;&#26469;&#37325;&#24314;&#20449;&#21495;&#65292;&#24182;&#20511;&#21161;GAN&#35780;&#35770;&#23478;&#30340;&#24110;&#21161;&#12290;&#21033;&#29992;&#36229;&#20960;&#20309;&#31070;&#32463;&#32593;&#32476;&#26469;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#36890;&#36807;&#20351;&#29992;&#19981;&#30830;&#23450;&#24615;&#65292;HypAD&#21487;&#20197;&#35780;&#20272;&#26159;&#21542;&#23384;&#22312;&#24322;&#24120;&#12290;
&lt;/p&gt;
&lt;p&gt;
The progress in modelling time series and, more generally, sequences of structured data has recently revamped research in anomaly detection. The task stands for identifying abnormal behaviors in financial series, IT systems, aerospace measurements, and the medical domain, where anomaly detection may aid in isolating cases of depression and attend the elderly. Anomaly detection in time series is a complex task since anomalies are rare due to highly non-linear temporal correlations and since the definition of anomalous is sometimes subjective. Here we propose the novel use of Hyperbolic uncertainty for Anomaly Detection (HypAD). HypAD learns self-supervisedly to reconstruct the input signal. We adopt best practices from the state-of-the-art to encode the sequence by an LSTM, jointly learned with a decoder to reconstruct the signal, with the aid of GAN critics. Uncertainty is estimated end-to-end by means of a hyperbolic neural network. By using uncertainty, HypAD may assess whether it is
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#32593;&#32476;&#38382;&#31572;&#21644;&#22810;&#27169;&#24577;&#34701;&#21512;&#30340;&#26041;&#27861;&#22635;&#34917;&#30693;&#35782;&#24211;&#20013;&#30340;&#32570;&#22833;&#20449;&#24687;&#12290;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#22810;&#27169;&#24577;&#29305;&#24449;&#21644;&#38382;&#39064;&#27169;&#26495;&#30340;&#22522;&#20110;&#32593;&#32476;&#38382;&#31572;&#30340;&#31995;&#32479;&#26469;&#36798;&#21040;&#26356;&#39640;&#25928;&#30340;&#30693;&#35782;&#24211;&#34917;&#20840;&#65292;&#21516;&#26102;&#32467;&#21512;&#20102;&#30693;&#35782;&#24211;&#20013;&#30340;&#32467;&#26500;&#21270;&#20449;&#24687;&#26469;&#25552;&#39640;&#25277;&#21462;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2211.07098</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#32593;&#32476;&#38382;&#31572;&#21644;&#22810;&#27169;&#24577;&#34701;&#21512;&#23436;&#25104;&#30693;&#35782;&#24211;
&lt;/p&gt;
&lt;p&gt;
Knowledge Base Completion using Web-Based Question Answering and Multimodal Fusion. (arXiv:2211.07098v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07098
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#32593;&#32476;&#38382;&#31572;&#21644;&#22810;&#27169;&#24577;&#34701;&#21512;&#30340;&#26041;&#27861;&#22635;&#34917;&#30693;&#35782;&#24211;&#20013;&#30340;&#32570;&#22833;&#20449;&#24687;&#12290;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#22810;&#27169;&#24577;&#29305;&#24449;&#21644;&#38382;&#39064;&#27169;&#26495;&#30340;&#22522;&#20110;&#32593;&#32476;&#38382;&#31572;&#30340;&#31995;&#32479;&#26469;&#36798;&#21040;&#26356;&#39640;&#25928;&#30340;&#30693;&#35782;&#24211;&#34917;&#20840;&#65292;&#21516;&#26102;&#32467;&#21512;&#20102;&#30693;&#35782;&#24211;&#20013;&#30340;&#32467;&#26500;&#21270;&#20449;&#24687;&#26469;&#25552;&#39640;&#25277;&#21462;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#30693;&#35782;&#24211;&#24050;&#32463;&#24314;&#31435;&#26469;&#23384;&#20648;&#22823;&#37327;&#30693;&#35782;&#65292;&#28982;&#32780;&#36825;&#20123;&#30693;&#35782;&#24211;&#38750;&#24120;&#19981;&#23436;&#25972;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#32593;&#32476;&#38382;&#31572;&#21644;&#22810;&#27169;&#24577;&#34701;&#21512;&#30340;&#26041;&#27861;&#22635;&#34917;&#30693;&#35782;&#24211;&#20013;&#30340;&#32570;&#22833;&#20449;&#24687;&#12290;&#20026;&#20102;&#21033;&#29992;&#32593;&#32476;&#19978;&#30340;&#38750;&#32467;&#26500;&#21270;&#20449;&#24687;&#23436;&#25104;&#30693;&#35782;&#24211;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#32593;&#32476;&#38382;&#31572;&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#22810;&#27169;&#24577;&#29305;&#24449;&#21644;&#38382;&#39064;&#27169;&#26495;&#26469;&#25552;&#21462;&#32570;&#22833;&#30340;&#20107;&#23454;&#65292;&#20165;&#20165;&#36890;&#36807;&#38750;&#24120;&#23569;&#30340;&#38382;&#39064;&#23601;&#21487;&#20197;&#36798;&#21040;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#35813;&#38382;&#31572;&#31995;&#32479;&#36824;&#20351;&#29992;&#30693;&#35782;&#24211;&#20013;&#30340;&#32467;&#26500;&#21270;&#20449;&#24687;&#65292;&#27604;&#22914;&#23454;&#20307;&#31867;&#22411;&#21644;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#32852;&#24615;&#65292;&#20197;&#24110;&#21161;&#25552;&#39640;&#25277;&#21462;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past few years, large knowledge bases have been constructed to store massive amounts of knowledge. However, these knowledge bases are highly incomplete. To solve this problem, we propose a web-based question answering system system with multimodal fusion of unstructured and structured information, to fill in missing information for knowledge bases. To utilize unstructured information from the Web for knowledge base completion, we design a web-based question answering system using multimodal features and question templates to extract missing facts, which can achieve good performance with very few questions. To help improve extraction quality, the question answering system employs structured information from knowledge bases, such as entity types and entity-to-entity relatedness.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#39044;&#27979;ICU&#33041;&#30005;&#30417;&#27979;&#20013;&#24120;&#35265;&#30340;6&#31181;&#33041;&#27874;&#22270;&#26696;&#30340;&#23384;&#22312;&#65292;&#24182;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#35299;&#37322;&#21644;&#19977;&#31181;&#35299;&#37322;&#26041;&#27861;&#65292;&#36825;&#23545;&#20110;&#24314;&#31435;AI&#30340;&#20449;&#20219;&#21644;&#20020;&#24202;&#37319;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2211.05207</link><description>&lt;p&gt;
&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#26469;&#35782;&#21035;&#30315;&#30187;-&#38388;&#38553;-&#25439;&#20260;&#36830;&#32493;&#29366;&#24577;&#19979;&#30340;&#33041;&#30005;&#22270;&#22270;&#26696;
&lt;/p&gt;
&lt;p&gt;
An Interpretable Machine Learning System to Identify EEG Patterns on the Ictal-Interictal-Injury Continuum. (arXiv:2211.05207v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05207
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#39044;&#27979;ICU&#33041;&#30005;&#30417;&#27979;&#20013;&#24120;&#35265;&#30340;6&#31181;&#33041;&#27874;&#22270;&#26696;&#30340;&#23384;&#22312;&#65292;&#24182;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#35299;&#37322;&#21644;&#19977;&#31181;&#35299;&#37322;&#26041;&#27861;&#65292;&#36825;&#23545;&#20110;&#24314;&#31435;AI&#30340;&#20449;&#20219;&#21644;&#20020;&#24202;&#37319;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#21307;&#23398;&#39046;&#22495;&#65292;&#20154;&#20204;&#21628;&#21505;&#22312;&#29992;&#20110;&#20020;&#24202;&#24037;&#20316;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20013;&#22686;&#21152;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;ICU&#33041;&#30005;&#30417;&#27979;&#20013;&#24120;&#35265;&#30340;6&#31181;&#33041;&#27874;&#22270;&#26696;&#65288;&#30315;&#30187;&#12289;LPD&#12289;GPD&#12289;LRDA&#12289;GRDA&#12289;&#20854;&#20182;&#65289;&#30340;&#23384;&#22312;&#12290;&#27599;&#20010;&#39044;&#27979;&#37117;&#37197;&#26377;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#35299;&#37322;&#65292;&#20511;&#21161;&#20110;&#19987;&#38376;&#30340;&#29992;&#25143;&#30028;&#38754;&#25552;&#20379;&#25903;&#25345;&#12290;&#27492;&#26032;&#22411;&#27169;&#22411;&#26550;&#26500;&#23398;&#20064;&#20102;&#19968;&#32452;&#21407;&#22411;&#31034;&#20363;&#65288;&#8220;&#21407;&#22411;&#8221;&#65289;&#65292;&#24182;&#36890;&#36807;&#23558;&#26032;&#30340;EEG&#29255;&#27573;&#19982;&#36825;&#20123;&#21407;&#22411;&#36827;&#34892;&#27604;&#36739;&#26469;&#20570;&#20986;&#20915;&#31574;&#12290;&#36825;&#20123;&#21407;&#22411;&#21487;&#20197;&#26159;&#21333;&#31867;&#65288;&#20165;&#19982;&#19968;&#20010;&#31867;&#30456;&#20851;&#65289;&#25110;&#21452;&#31867;&#65288;&#19982;&#20004;&#20010;&#31867;&#30456;&#20851;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#20027;&#35201;&#30340;&#27169;&#22411;&#35299;&#37322;&#26041;&#27861;&#65306;1&#65289;&#20351;&#29992;&#20840;&#23616;&#32467;&#26500;&#20445;&#25345;&#26041;&#27861;&#65292;&#23558;1275&#32500;cEEG&#28508;&#22312;&#29305;&#24449;&#26144;&#23556;&#21040;&#20108;&#32500;&#31354;&#38388;&#20013;&#65292;&#21487;&#35270;&#21270;&#30315;&#30187;-&#38388;&#38553;-&#25439;&#20260;&#36830;&#32493;&#29366;&#24577;&#65292;&#20174;&#32780;&#28145;&#20837;&#20102;&#35299;&#20854;&#39640;&#32500;&#32467;&#26500;&#12290;2&#65289;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#35299;&#37322;&#26041;&#27861;&#65292;&#20351;&#20154;&#31867;&#19987;&#23478;&#33021;&#22815;&#26597;&#35810;&#27169;&#22411;&#39044;&#27979;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#24182;&#20197;&#33258;&#28982;&#35821;&#35328;&#25509;&#25910;&#32463;&#36807;&#19987;&#23478;&#39564;&#35777;&#30340;&#35299;&#37322;&#12290;3&#65289;&#25105;&#20204;&#21487;&#35270;&#21270;&#20102;&#23548;&#33268;&#27169;&#22411;&#20570;&#20986;&#26576;&#20010;&#20915;&#31574;&#30340;&#36755;&#20837;&#30340;&#26368;&#37325;&#35201;&#29305;&#24449;&#65292;&#20801;&#35768;&#35814;&#32454;&#26816;&#26597;&#36755;&#20837;&#21644;&#36755;&#20986;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35299;&#37322;&#24615;&#27169;&#22411;&#20998;&#31867;EEG&#22270;&#26696;&#21644;&#25552;&#20379;&#19987;&#23478;&#21451;&#22909;&#30340;&#35299;&#37322;&#30340;&#23454;&#29992;&#24615;&#65292;&#36825;&#20004;&#20010;&#26041;&#38754;&#23545;&#20110;&#24314;&#31435;AI&#30340;&#20449;&#20219;&#21644;&#20020;&#24202;&#37319;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many medical subfields, there is a call for greater interpretability in the machine learning systems used for clinical work. In this paper, we design an interpretable deep learning model to predict the presence of 6 types of brainwave patterns (Seizure, LPD, GPD, LRDA, GRDA, other) commonly encountered in ICU EEG monitoring. Each prediction is accompanied by a high-quality explanation delivered with the assistance of a specialized user interface. This novel model architecture learns a set of prototypical examples (``prototypes'') and makes decisions by comparing a new EEG segment to these prototypes. These prototypes are either single-class (affiliated with only one class) or dual-class (affiliated with two classes).  We present three main ways of interpreting the model: 1) Using global-structure preserving methods, we map the 1275-dimensional cEEG latent features to a 2D space to visualize the ictal-interictal-injury continuum and gain insight into its high-dimensional structure. 2
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#31354;&#38388;&#31232;&#30095;&#25512;&#29702;&#65288;SSI&#65289;&#30340;&#36890;&#29992;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#36873;&#25321;&#24615;&#22320;&#20026;&#32534;&#36753;&#21306;&#22495;&#25191;&#34892;&#35745;&#31639;&#24182;&#21152;&#36895;&#21508;&#31181;&#29983;&#25104;&#27169;&#22411;&#65292;&#21253;&#25324;&#26465;&#20214;GAN&#21644;&#25193;&#25955;&#27169;&#22411;&#12290;&#36890;&#36807;&#32531;&#23384;&#21644;&#37325;&#22797;&#20351;&#29992;&#21407;&#22987;&#22270;&#20687;&#30340;&#29305;&#24449;&#22270;&#65292;&#25105;&#20204;&#23558;&#21367;&#31215;&#28388;&#27874;&#22120;&#31232;&#30095;&#22320;&#24212;&#29992;&#20110;&#32534;&#36753;&#21306;&#22495;&#65292;&#24182;&#22312;&#26410;&#32534;&#36753;&#30340;&#21306;&#22495;&#20013;&#37325;&#22797;&#20351;&#29992;&#32531;&#23384;&#29305;&#24449;&#65292;&#20174;&#32780;&#36890;&#36807;&#32422;$1\%$&#30340;&#21306;&#22495;&#32534;&#36753;&#26469;&#20943;&#23569;&#35745;&#31639;&#36164;&#28304;&#30340;&#28010;&#36153;&#12290;</title><link>http://arxiv.org/abs/2211.02048</link><description>&lt;p&gt;
&#26377;&#25928;&#31232;&#30095;&#25512;&#29702;&#29992;&#20110;&#26465;&#20214;GAN&#21644;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Efficient Spatially Sparse Inference for Conditional GANs and Diffusion Models. (arXiv:2211.02048v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02048
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#31354;&#38388;&#31232;&#30095;&#25512;&#29702;&#65288;SSI&#65289;&#30340;&#36890;&#29992;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#36873;&#25321;&#24615;&#22320;&#20026;&#32534;&#36753;&#21306;&#22495;&#25191;&#34892;&#35745;&#31639;&#24182;&#21152;&#36895;&#21508;&#31181;&#29983;&#25104;&#27169;&#22411;&#65292;&#21253;&#25324;&#26465;&#20214;GAN&#21644;&#25193;&#25955;&#27169;&#22411;&#12290;&#36890;&#36807;&#32531;&#23384;&#21644;&#37325;&#22797;&#20351;&#29992;&#21407;&#22987;&#22270;&#20687;&#30340;&#29305;&#24449;&#22270;&#65292;&#25105;&#20204;&#23558;&#21367;&#31215;&#28388;&#27874;&#22120;&#31232;&#30095;&#22320;&#24212;&#29992;&#20110;&#32534;&#36753;&#21306;&#22495;&#65292;&#24182;&#22312;&#26410;&#32534;&#36753;&#30340;&#21306;&#22495;&#20013;&#37325;&#22797;&#20351;&#29992;&#32531;&#23384;&#29305;&#24449;&#65292;&#20174;&#32780;&#36890;&#36807;&#32422;$1\%$&#30340;&#21306;&#22495;&#32534;&#36753;&#26469;&#20943;&#23569;&#35745;&#31639;&#36164;&#28304;&#30340;&#28010;&#36153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22270;&#20687;&#32534;&#36753;&#20013;&#65292;&#29616;&#26377;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#24448;&#24448;&#20250;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#21512;&#25104;&#25972;&#20010;&#36755;&#20986;&#65292;&#21253;&#25324;&#26410;&#32534;&#36753;&#30340;&#21306;&#22495;&#12290;&#36825;&#23548;&#33268;&#35745;&#31639;&#36164;&#28304;&#30340;&#28010;&#36153;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#36739;&#23567;&#30340;&#32534;&#36753;&#25805;&#20316;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31354;&#38388;&#31232;&#30095;&#25512;&#29702;&#65288;SSI&#65289;&#30340;&#36890;&#29992;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#36873;&#25321;&#24615;&#22320;&#20026;&#32534;&#36753;&#21306;&#22495;&#25191;&#34892;&#35745;&#31639;&#24182;&#21152;&#36895;&#21508;&#31181;&#29983;&#25104;&#27169;&#22411;&#65292;&#21253;&#25324;&#26465;&#20214;GAN&#21644;&#25193;&#25955;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35266;&#23519;&#26159;&#29992;&#25143;&#20542;&#21521;&#20110;&#36880;&#28176;&#25913;&#21464;&#36755;&#20837;&#22270;&#20687;&#65292;&#36825;&#28608;&#21457;&#20102;&#25105;&#20204;&#32531;&#23384;&#21644;&#37325;&#22797;&#20351;&#29992;&#21407;&#22987;&#22270;&#20687;&#30340;&#29305;&#24449;&#22270;&#30340;&#24819;&#27861;&#12290;&#32473;&#23450;&#32534;&#36753;&#36807;&#30340;&#22270;&#20687;&#65292;&#25105;&#20204;&#31232;&#30095;&#22320;&#23558;&#21367;&#31215;&#28388;&#27874;&#22120;&#24212;&#29992;&#20110;&#32534;&#36753;&#21306;&#22495;&#65292;&#21516;&#26102;&#37325;&#22797;&#20351;&#29992;&#26410;&#32534;&#36753;&#21306;&#22495;&#30340;&#32531;&#23384;&#29305;&#24449;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#31639;&#27861;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#31232;&#30095;&#28176;&#36827;&#24335;&#29983;&#25104;&#24341;&#25806;&#65288;SIGE&#65289;&#26469;&#23558;&#35745;&#31639;&#20943;&#23569;&#36716;&#21270;&#20026;&#22312;&#29616;&#25104;&#30828;&#20214;&#19978;&#30340;&#24310;&#36831;&#20943;&#23569;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#32422;$1\%$&#30340;&#21306;&#22495;&#32534;&#36753;&#65292;&#20943;&#23569;&#20102;&#35745;&#31639;&#36164;&#28304;&#30340;&#28010;&#36153;&#12290;
&lt;/p&gt;
&lt;p&gt;
During image editing, existing deep generative models tend to re-synthesize the entire output from scratch, including the unedited regions. This leads to a significant waste of computation, especially for minor editing operations. In this work, we present Spatially Sparse Inference (SSI), a general-purpose technique that selectively performs computation for edited regions and accelerates various generative models, including both conditional GANs and diffusion models. Our key observation is that users tend to gradually change the input image. This motivates us to cache and reuse the feature maps of the original image. Given an edited image, we sparsely apply the convolutional filters to the edited regions while reusing the cached features for the unedited areas. Based on our algorithm, we further propose Sparse Incremental Generative Engine (SIGE) to convert the computation reduction to latency reduction on off-the-shelf hardware. With about $1\%$-area edits, our method reduces the comp
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#24322;&#26500;&#22270;&#39044;&#35757;&#32451;&#26041;&#27861;SHGP&#65292;&#23427;&#36890;&#36807;&#32467;&#26500;&#32858;&#31867;&#20135;&#29983;&#20266;&#26631;&#31614;&#26469;&#25351;&#23548;&#27169;&#22411;&#23398;&#20064;&#23545;&#35937;&#23884;&#20837;&#21644;&#27880;&#24847;&#21147;&#31995;&#25968;&#65292;&#19981;&#38656;&#35201;&#29983;&#25104;&#20219;&#20309;&#27491;&#20363;&#25110;&#36127;&#20363;&#65292;&#20855;&#26377;&#21069;&#26223;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2210.10462</link><description>&lt;p&gt;
&#22522;&#20110;&#32467;&#26500;&#32858;&#31867;&#30340;&#33258;&#30417;&#30563;&#24322;&#26500;&#22270;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Self-supervised Heterogeneous Graph Pre-training Based on Structural Clustering. (arXiv:2210.10462v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10462
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#24322;&#26500;&#22270;&#39044;&#35757;&#32451;&#26041;&#27861;SHGP&#65292;&#23427;&#36890;&#36807;&#32467;&#26500;&#32858;&#31867;&#20135;&#29983;&#20266;&#26631;&#31614;&#26469;&#25351;&#23548;&#27169;&#22411;&#23398;&#20064;&#23545;&#35937;&#23884;&#20837;&#21644;&#27880;&#24847;&#21147;&#31995;&#25968;&#65292;&#19981;&#38656;&#35201;&#29983;&#25104;&#20219;&#20309;&#27491;&#20363;&#25110;&#36127;&#20363;&#65292;&#20855;&#26377;&#21069;&#26223;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#33258;&#30417;&#30563;&#30340;HIN&#39044;&#35757;&#32451;&#26041;&#27861;&#22312;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#19978;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#31454;&#20105;&#21147;&#65292;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;&#21322;&#30417;&#30563;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#21508;&#31181;&#31574;&#30053;&#30340;&#31934;&#32454;&#23450;&#21046;&#65292;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#27491;&#20363;&#21644;&#36127;&#20363;&#65292;&#36825;&#26126;&#26174;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#28789;&#27963;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#24322;&#26500;&#22270;&#39044;&#35757;&#32451;&#26041;&#27861;SHGP&#65292;&#23427;&#19981;&#38656;&#35201;&#29983;&#25104;&#20219;&#20309;&#27491;&#20363;&#25110;&#36127;&#20363;&#12290;&#23427;&#30001;&#20004;&#20010;&#27169;&#22359;&#32452;&#25104;&#65292;&#20849;&#20139;&#30456;&#21516;&#30340;&#27880;&#24847;&#21147;&#32858;&#21512;&#26426;&#21046;&#12290;&#22312;&#27599;&#19968;&#27425;&#36845;&#20195;&#20013;&#65292;Att-LPA&#27169;&#22359;&#36890;&#36807;&#32467;&#26500;&#32858;&#31867;&#20135;&#29983;&#20266;&#26631;&#31614;&#65292;&#20316;&#20026;&#33258;&#30417;&#30563;&#20449;&#21495;&#26469;&#25351;&#23548;Att-HGNN&#27169;&#22359;&#23398;&#20064;&#23545;&#35937;&#23884;&#20837;&#21644;&#27880;&#24847;&#21147;&#31995;&#25968;&#12290;&#20004;&#20010;&#27169;&#22359;&#21487;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#21644;&#22686;&#24378;&#24444;&#27492;&#65292;&#20419;&#36827;&#27169;&#22411;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent self-supervised pre-training methods on Heterogeneous Information Networks (HINs) have shown promising competitiveness over traditional semi-supervised Heterogeneous Graph Neural Networks (HGNNs). Unfortunately, their performance heavily depends on careful customization of various strategies for generating high-quality positive examples and negative examples, which notably limits their flexibility and generalization ability. In this work, we present SHGP, a novel Self-supervised Heterogeneous Graph Pre-training approach, which does not need to generate any positive examples or negative examples. It consists of two modules that share the same attention-aggregation scheme. In each iteration, the Att-LPA module produces pseudo-labels through structural clustering, which serve as the self-supervision signals to guide the Att-HGNN module to learn object embeddings and attention coefficients. The two modules can effectively utilize and enhance each other, promoting the model to learn 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#32467;&#26524;&#35777;&#26126;&#65292;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20013;&#65292;&#39069;&#22806;&#30340;&#36890;&#36947;&#32500;&#24230;&#36828;&#38750;&#24494;&#19981;&#36275;&#36947;&#65292;&#26368;&#20339;&#30340;&#32467;&#26524;&#24182;&#19981;&#26159;&#36890;&#36807;&#21333;&#29420;&#23545;&#27599;&#20010;&#36890;&#36947;&#36827;&#34892;&#32553;&#25918;&#65292;&#32780;&#26159;&#36890;&#36807;&#21033;&#29992;&#36890;&#36947;&#20043;&#38388;&#30340;&#20132;&#20114;&#20449;&#24687;&#36827;&#34892;&#32553;&#25918;&#26469;&#23454;&#29616;&#30340;&#12290;</title><link>http://arxiv.org/abs/2210.07713</link><description>&lt;p&gt;
&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#21450;&#20854;&#22312;&#19981;&#21516;&#32500;&#24230;&#19979;&#30340;&#29305;&#28857;&#30340;&#23454;&#35777;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
An Empirical Evaluation of Multivariate Time Series Classification with Input Transformation across Different Dimensions. (arXiv:2210.07713v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#32467;&#26524;&#35777;&#26126;&#65292;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20013;&#65292;&#39069;&#22806;&#30340;&#36890;&#36947;&#32500;&#24230;&#36828;&#38750;&#24494;&#19981;&#36275;&#36947;&#65292;&#26368;&#20339;&#30340;&#32467;&#26524;&#24182;&#19981;&#26159;&#36890;&#36807;&#21333;&#29420;&#23545;&#27599;&#20010;&#36890;&#36947;&#36827;&#34892;&#32553;&#25918;&#65292;&#32780;&#26159;&#36890;&#36807;&#21033;&#29992;&#36890;&#36947;&#20043;&#38388;&#30340;&#20132;&#20114;&#20449;&#24687;&#36827;&#34892;&#32553;&#25918;&#26469;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#21069;&#30740;&#31350;&#20013;&#65292;&#38024;&#23545;&#26102;&#38388;&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#65292;&#20174;&#21333;&#36890;&#36947;&#25968;&#25454;&#38598;&#65288;&#21333;&#20803;&#65289;&#21521;&#26377;&#22810;&#20010;&#20449;&#24687;&#36890;&#36947;&#30340;&#38382;&#39064;&#65288;&#22810;&#20803;&#65289;&#36716;&#31227;&#12290;&#20854;&#20013;&#22823;&#37096;&#20998;&#30740;&#31350;&#30528;&#37325;&#26041;&#27861;&#30340;&#21019;&#26032;&#21644;&#26550;&#26500;&#65292;&#36755;&#20837;&#25968;&#25454;&#30340;&#26684;&#24335;&#24120;&#24120;&#34987;&#38544;&#21547;&#22320;&#23545;&#24453;&#12290;&#29305;&#21035;&#22320;&#65292;&#22810;&#20803;&#25968;&#25454;&#38598;&#24120;&#24120;&#34987;&#24403;&#20316;&#19968;&#31995;&#21015;&#21333;&#20803;&#26102;&#38388;&#24207;&#21015;&#30340;&#22534;&#26632;&#26469;&#36827;&#34892;&#36755;&#20837;&#39044;&#22788;&#29702;&#65292;&#37319;&#29992;&#21333;&#29420;&#23545;&#27599;&#20010;&#36890;&#36947;&#37319;&#29992;&#32553;&#25918;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#35777;&#26126;&#39069;&#22806;&#30340;&#36890;&#36947;&#32500;&#24230;&#36828;&#38750;&#24494;&#19981;&#36275;&#36947;&#65292;&#32780;&#19981;&#21516;&#30340;&#32553;&#25918;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#24433;&#21709;&#35299;&#20915;&#26041;&#26696;&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22312;&#22235;&#20010;&#19981;&#21516;&#30340;&#26102;&#38388;&#32500;&#24230;&#19978;&#27979;&#35797;&#20102;&#19971;&#31181;&#19981;&#21516;&#30340;&#25968;&#25454;&#36716;&#25442;&#26041;&#27861;&#65292;&#24182;&#30740;&#31350;&#20102;&#23427;&#20204;&#23545;&#20116;&#31181;&#26368;&#36817;&#26041;&#27861;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23545;&#20110;&#22823;&#22810;&#25968;&#27979;&#35797;&#25968;&#25454;&#38598;&#65292;&#26368;&#20339;&#30340;&#32467;&#26524;&#19981;&#26159;&#36890;&#36807;&#21333;&#29420;&#23545;&#27599;&#20010;&#36890;&#36947;&#36827;&#34892;&#32553;&#25918;&#65292;&#32780;&#26159;&#36890;&#36807;&#21033;&#29992;&#36890;&#36947;&#20043;&#38388;&#30340;&#20132;&#20114;&#20449;&#24687;&#36827;&#34892;&#32553;&#25918;&#26469;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In current research, machine and deep learning solutions for the classification of temporal data are shifting from single-channel datasets (univariate) to problems with multiple channels of information (multivariate). The majority of these works are focused on the method novelty and architecture, and the format of the input data is often treated implicitly. Particularly, multivariate datasets are often treated as a stack of univariate time series in terms of input preprocessing, with scaling methods applied across each channel separately. In this evaluation, we aim to demonstrate that the additional channel dimension is far from trivial and different approaches to scaling can lead to significantly different results in the accuracy of a solution. To that end, we test seven different data transformation methods on four different temporal dimensions and study their effect on the classification accuracy of five recent methods. We show that, for the large majority of tested datasets, the be
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20840;&#23616;&#36923;&#36753;GNN&#35299;&#37322;&#22120;GLGExplainer&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#20197;&#20219;&#24847;&#24067;&#23572;&#32452;&#21512;&#30340;&#24418;&#24335;&#29983;&#25104;GNN&#23398;&#20064;&#30340;&#22270;&#24418;&#27010;&#24565;&#30340;&#35299;&#37322;&#22120;&#12290;GLGExplainer&#25552;&#20379;&#20102;&#20934;&#30830;&#21487;&#35299;&#37322;&#30340;&#20840;&#23616;&#35299;&#37322;&#65292;&#19982;GNN&#30340;&#32452;&#21512;&#24615;&#36136;&#30456;&#19968;&#33268;&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#20219;&#20309;&#22270;&#20998;&#31867;&#25110;&#22238;&#24402;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2210.07147</link><description>&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#27010;&#24565;&#30340;&#36923;&#36753;&#32452;&#21512;&#65292;&#20840;&#23616;&#35299;&#37322;GNNs
&lt;/p&gt;
&lt;p&gt;
Global Explainability of GNNs via Logic Combination of Learned Concepts. (arXiv:2210.07147v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07147
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20840;&#23616;&#36923;&#36753;GNN&#35299;&#37322;&#22120;GLGExplainer&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#20197;&#20219;&#24847;&#24067;&#23572;&#32452;&#21512;&#30340;&#24418;&#24335;&#29983;&#25104;GNN&#23398;&#20064;&#30340;&#22270;&#24418;&#27010;&#24565;&#30340;&#35299;&#37322;&#22120;&#12290;GLGExplainer&#25552;&#20379;&#20102;&#20934;&#30830;&#21487;&#35299;&#37322;&#30340;&#20840;&#23616;&#35299;&#37322;&#65292;&#19982;GNN&#30340;&#32452;&#21512;&#24615;&#36136;&#30456;&#19968;&#33268;&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#20219;&#20309;&#22270;&#20998;&#31867;&#25110;&#22238;&#24402;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;GNN&#30340;&#23454;&#20363;&#32423;&#35299;&#37322;&#26159;&#19968;&#20010;&#34987;&#24191;&#27867;&#30740;&#31350;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#24050;&#32463;&#24320;&#21457;&#20102;&#24456;&#22810;&#26041;&#27861;&#65292;&#20294;&#26159;&#25552;&#20379;GNN&#34892;&#20026;&#30340;&#20840;&#23616;&#35299;&#37322;&#21364;&#24456;&#23569;&#34987;&#25506;&#35752;&#65292;&#23613;&#31649;&#23427;&#22312;&#21487;&#35299;&#37322;&#24615;&#21644;&#35843;&#35797;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#35201;&#20040;&#20165;&#21015;&#20986;&#32473;&#23450;&#31867;&#21035;&#30340;&#23616;&#37096;&#35299;&#37322;&#65292;&#35201;&#20040;&#29983;&#25104;&#19968;&#20010;&#20855;&#26377;&#32473;&#23450;&#31867;&#21035;&#26368;&#22823;&#20998;&#25968;&#30340;&#21512;&#25104;&#21407;&#22411;&#22270;&#65292;&#23436;&#20840;&#24573;&#30053;&#20102;GNN&#21487;&#33021;&#24050;&#32463;&#23398;&#20064;&#30340;&#20219;&#20309;&#32452;&#21512;&#24615;&#26041;&#38754;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GLGExplainer&#65288;&#22522;&#20110;&#20840;&#23616;&#36923;&#36753;&#30340;GNN&#35299;&#37322;&#22120;&#65289;&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#29983;&#25104;&#23398;&#20064;&#36807;&#30340;&#22270;&#24418;&#27010;&#24565;&#30340;&#20219;&#24847;&#24067;&#23572;&#32452;&#21512;&#30340;&#35299;&#37322;&#30340;&#20840;&#23616;&#35299;&#37322;&#22120;&#12290;GLGExplainer&#26159;&#19968;&#20010;&#20840;&#21487;&#24494;&#26550;&#26500;&#65292;&#23427;&#20197;&#23616;&#37096;&#35299;&#37322;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#23558;&#23427;&#20204;&#32452;&#21512;&#25104;&#22522;&#20110;&#22270;&#24418;&#27010;&#24565;&#30340;&#36923;&#36753;&#20844;&#24335;&#65292;&#34920;&#31034;&#20026;&#23616;&#37096;&#35299;&#37322;&#30340;&#38598;&#32676;&#12290;&#19982;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#30456;&#21453;&#65292;GLGExplainer&#25552;&#20379;&#20102;&#20934;&#30830;&#21644;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#20840;&#23616;&#35299;&#37322;&#65292;&#19982;GNN&#30340;&#32452;&#21512;&#24615;&#36136;&#30456;&#19968;&#33268;&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#20219;&#20309;&#22270;&#20998;&#31867;&#25110;&#22238;&#24402;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;GLGExplainer&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#20854;&#20013;&#20960;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
While instance-level explanation of GNN is a well-studied problem with plenty of approaches being developed, providing a global explanation for the behaviour of a GNN is much less explored, despite its potential in interpretability and debugging. Existing solutions either simply list local explanations for a given class, or generate a synthetic prototypical graph with maximal score for a given class, completely missing any combinatorial aspect that the GNN could have learned. In this work, we propose GLGExplainer (Global Logic-based GNN Explainer), the first Global Explainer capable of generating explanations as arbitrary Boolean combinations of learned graphical concepts. GLGExplainer is a fully differentiable architecture that takes local explanations as inputs and combines them into a logic formula over graphical concepts, represented as clusters of local explanations. Contrary to existing solutions, GLGExplainer provides accurate and human-interpretable global explanations that are
&lt;/p&gt;</description></item><item><title>TimesNet&#20351;&#29992;&#26102;&#38388;&#20108;&#32500;&#21464;&#21270;&#24314;&#27169;&#65292;&#23558;&#19968;&#32500;&#26102;&#38388;&#24207;&#21015;&#36716;&#25442;&#20026;&#22522;&#20110;&#22810;&#20010;&#21608;&#26399;&#30340;&#19968;&#32452;&#20108;&#32500;&#24352;&#37327;&#65292;&#20174;&#32780;&#20351;&#20108;&#32500;&#21464;&#21270;&#21487;&#20197;&#36731;&#26494;&#22320;&#36890;&#36807;&#20108;&#32500;&#21367;&#31215;&#26680;&#36827;&#34892;&#24314;&#27169;&#65292;&#21487;&#20197;&#29992;&#20110;&#24191;&#27867;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2210.02186</link><description>&lt;p&gt;
TimesNet: &#36890;&#29992;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#26102;&#38388;&#20108;&#32500;&#21464;&#21270;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
TimesNet: Temporal 2D-Variation Modeling for General Time Series Analysis. (arXiv:2210.02186v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.02186
&lt;/p&gt;
&lt;p&gt;
TimesNet&#20351;&#29992;&#26102;&#38388;&#20108;&#32500;&#21464;&#21270;&#24314;&#27169;&#65292;&#23558;&#19968;&#32500;&#26102;&#38388;&#24207;&#21015;&#36716;&#25442;&#20026;&#22522;&#20110;&#22810;&#20010;&#21608;&#26399;&#30340;&#19968;&#32452;&#20108;&#32500;&#24352;&#37327;&#65292;&#20174;&#32780;&#20351;&#20108;&#32500;&#21464;&#21270;&#21487;&#20197;&#36731;&#26494;&#22320;&#36890;&#36807;&#20108;&#32500;&#21367;&#31215;&#26680;&#36827;&#34892;&#24314;&#27169;&#65292;&#21487;&#20197;&#29992;&#20110;&#24191;&#27867;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#22312;&#22825;&#27668;&#39044;&#25253;&#12289;&#24322;&#24120;&#26816;&#27979;&#21644;&#21160;&#20316;&#35782;&#21035;&#31561;&#24191;&#27867;&#24212;&#29992;&#20013;&#20855;&#26377;&#24040;&#22823;&#37325;&#35201;&#24615;&#12290;&#26412;&#25991;&#20851;&#27880;&#20110;&#26102;&#38388;&#21464;&#21270;&#24314;&#27169;&#65292;&#36825;&#26159;&#24191;&#27867;&#20998;&#26512;&#20219;&#21153;&#30340;&#20849;&#21516;&#20851;&#38190;&#38382;&#39064;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#23581;&#35797;&#30452;&#25509;&#20174;&#19968;&#32500;&#26102;&#38388;&#24207;&#21015;&#20013;&#23436;&#25104;&#27492;&#20219;&#21153;&#65292;&#20294;&#30001;&#20110;&#22797;&#26434;&#30340;&#26102;&#38388;&#27169;&#24335;&#65292;&#36825;&#26159;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#12290;&#22522;&#20110;&#23545;&#26102;&#38388;&#24207;&#21015;&#30340;&#22810;&#21608;&#26399;&#24615;&#35266;&#23519;&#65292;&#25105;&#20204;&#23558;&#22797;&#26434;&#30340;&#26102;&#38388;&#21464;&#21270;&#20998;&#35299;&#20026;&#22810;&#20010;&#21608;&#26399;&#20869;&#21644;&#21608;&#26399;&#38388;&#30340;&#21464;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#19968;&#32500;&#26102;&#38388;&#24207;&#21015;&#22312;&#34920;&#31034;&#33021;&#21147;&#26041;&#38754;&#30340;&#38480;&#21046;&#65292;&#25105;&#20204;&#23558;&#26102;&#38388;&#21464;&#21270;&#20998;&#26512;&#25193;&#23637;&#21040;&#20102;&#20108;&#32500;&#31354;&#38388;&#20013;&#65292;&#23558;&#19968;&#32500;&#26102;&#38388;&#24207;&#21015;&#36716;&#25442;&#25104;&#22522;&#20110;&#22810;&#20010;&#21608;&#26399;&#30340;&#19968;&#32452;&#20108;&#32500;&#24352;&#37327;&#12290;&#36825;&#31181;&#36716;&#25442;&#21487;&#20197;&#23558;&#21608;&#26399;&#20869;&#21644;&#21608;&#26399;&#38388;&#30340;&#21464;&#21270;&#23884;&#20837;&#21040;&#20108;&#32500;&#24352;&#37327;&#30340;&#21015;&#21644;&#34892;&#20013;&#65292;&#20351;&#24471;&#20108;&#32500;&#21464;&#21270;&#21487;&#20197;&#36731;&#26494;&#22320;&#36890;&#36807;&#20108;&#32500;&#21367;&#31215;&#26680;&#36827;&#34892;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series analysis is of immense importance in extensive applications, such as weather forecasting, anomaly detection, and action recognition. This paper focuses on temporal variation modeling, which is the common key problem of extensive analysis tasks. Previous methods attempt to accomplish this directly from the 1D time series, which is extremely challenging due to the intricate temporal patterns. Based on the observation of multi-periodicity in time series, we ravel out the complex temporal variations into the multiple intraperiod- and interperiod-variations. To tackle the limitations of 1D time series in representation capability, we extend the analysis of temporal variations into the 2D space by transforming the 1D time series into a set of 2D tensors based on multiple periods. This transformation can embed the intraperiod- and interperiod-variations into the columns and rows of the 2D tensors respectively, making the 2D-variations to be easily modeled by 2D kernels. Technicall
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#29983;&#28079;&#36866;&#24212;&#24615;&#36870;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#20174;&#23398;&#20064;&#30340;&#31574;&#30053;&#20013;&#26500;&#24314;&#22810;&#26679;&#31574;&#30053;&#30340;&#32452;&#21512;&#23454;&#29616;&#20102;&#23545;&#26032;&#30340;&#28436;&#31034;&#30340;&#24555;&#36895;&#36866;&#24212;&#65292;&#21516;&#26102;&#25972;&#21512;&#28436;&#31034;&#20013;&#30340;&#20849;&#24615;&#30693;&#35782;&#65292;&#23454;&#29616;&#20934;&#30830;&#30340;&#20219;&#21153;&#25512;&#26029;&#65292;&#36824;&#33021;&#22815;&#22312;&#22823;&#35268;&#27169;&#37096;&#32626;&#20013;&#36890;&#36807;&#32500;&#25252;&#19968;&#20010;&#31934;&#31616;&#30340;&#21407;&#22411;&#31574;&#30053;&#38598;&#21512;&#24182;&#36890;&#36807;&#31574;&#30053;&#32452;&#21512;&#26469;&#36924;&#36817;&#25152;&#26377;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2209.11908</link><description>&lt;p&gt;
&#26469;&#33258;&#28436;&#31034;&#30340;&#24555;&#36895;&#29983;&#28079;&#36866;&#24212;&#24615;&#36870;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Fast Lifelong Adaptive Inverse Reinforcement Learning from Demonstrations. (arXiv:2209.11908v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.11908
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#29983;&#28079;&#36866;&#24212;&#24615;&#36870;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#20174;&#23398;&#20064;&#30340;&#31574;&#30053;&#20013;&#26500;&#24314;&#22810;&#26679;&#31574;&#30053;&#30340;&#32452;&#21512;&#23454;&#29616;&#20102;&#23545;&#26032;&#30340;&#28436;&#31034;&#30340;&#24555;&#36895;&#36866;&#24212;&#65292;&#21516;&#26102;&#25972;&#21512;&#28436;&#31034;&#20013;&#30340;&#20849;&#24615;&#30693;&#35782;&#65292;&#23454;&#29616;&#20934;&#30830;&#30340;&#20219;&#21153;&#25512;&#26029;&#65292;&#36824;&#33021;&#22815;&#22312;&#22823;&#35268;&#27169;&#37096;&#32626;&#20013;&#36890;&#36807;&#32500;&#25252;&#19968;&#20010;&#31934;&#31616;&#30340;&#21407;&#22411;&#31574;&#30053;&#38598;&#21512;&#24182;&#36890;&#36807;&#31574;&#30053;&#32452;&#21512;&#26469;&#36924;&#36817;&#25152;&#26377;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28436;&#31034;&#23398;&#20064;&#65288;LfD&#65289;&#26041;&#27861;&#20351;&#32456;&#31471;&#29992;&#25143;&#36890;&#36807;&#25152;&#38656;&#34892;&#20026;&#30340;&#28436;&#31034;&#26469;&#25945;&#25480;&#26426;&#22120;&#20154;&#26032;&#20219;&#21153;&#65292;&#20174;&#32780;&#20351;&#26426;&#22120;&#20154;&#25216;&#26415;&#30340;&#20351;&#29992;&#38754;&#26356;&#24191;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;LfD&#26694;&#26550;&#26080;&#27861;&#24555;&#36895;&#36866;&#24212;&#24322;&#26500;&#30340;&#20154;&#31867;&#28436;&#31034;&#65292;&#20063;&#19981;&#33021;&#22312;&#26222;&#36866;&#30340;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#36827;&#34892;&#22823;&#35268;&#27169;&#37096;&#32626;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;LfD&#26694;&#26550;&#8212;&#8212;&#24555;&#36895;&#29983;&#28079;&#36866;&#24212;&#24615;&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;FLAIR&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;(1)&#21033;&#29992;&#23398;&#20064;&#31574;&#30053;&#26500;&#24314;&#22810;&#26679;&#31574;&#30053;&#30340;&#32452;&#21512;&#65292;&#20174;&#32780;&#24555;&#36895;&#36866;&#24212;&#26032;&#30340;&#28436;&#31034;&#65292;&#20801;&#35768;&#24555;&#36895;&#30340;&#32456;&#31471;&#29992;&#25143;&#20010;&#24615;&#21270;&#65292;(2)&#25972;&#21512;&#28436;&#31034;&#20013;&#30340;&#20849;&#24615;&#30693;&#35782;&#65292;&#23454;&#29616;&#20934;&#30830;&#30340;&#20219;&#21153;&#25512;&#26029;&#65307;(3)&#22312;&#32456;&#36523;&#37096;&#32626;&#20013;&#21482;&#22312;&#38656;&#35201;&#26102;&#25193;&#23637;&#20854;&#27169;&#22411;&#65292;&#36890;&#36807;&#31574;&#30053;&#32452;&#21512;&#32500;&#25252;&#19968;&#20010;&#31934;&#31616;&#30340;&#21407;&#22411;&#31574;&#30053;&#38598;&#21512;&#65292;&#24182;&#33021;&#22815;&#36924;&#36817;&#25152;&#26377;&#34892;&#20026;&#12290;&#25105;&#20204;&#22312;&#23454;&#39564;&#35777;&#26126;&#65292;FLAIR&#23454;&#29616;&#20102;&#36866;&#24212;&#24615;&#65288;&#21363;&#26426;&#22120;&#20154;&#36866;&#24212;&#20102;&#24322;&#26500;&#30340;&#12289;&#29305;&#23450;&#20110;&#29992;&#25143;&#30340;&#20219;&#21153;&#65289;&#24182;&#19988;&#22312;&#22823;&#35268;&#27169;&#37096;&#32626;&#20013;&#33410;&#30465;&#20102;&#27169;&#22411;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from Demonstration (LfD) approaches empower end-users to teach robots novel tasks via demonstrations of the desired behaviors, democratizing access to robotics. However, current LfD frameworks are not capable of fast adaptation to heterogeneous human demonstrations nor the large-scale deployment in ubiquitous robotics applications. In this paper, we propose a novel LfD framework, Fast Lifelong Adaptive Inverse Reinforcement learning (FLAIR). Our approach (1) leverages learned strategies to construct policy mixtures for fast adaptation to new demonstrations, allowing for quick end-user personalization, (2) distills common knowledge across demonstrations, achieving accurate task inference; and (3) expands its model only when needed in lifelong deployments, maintaining a concise set of prototypical strategies that can approximate all behaviors via policy mixtures. We empirically validate that FLAIR achieves adaptability (i.e., the robot adapts to heterogeneous, user-specific task
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#22797;&#26434;&#32593;&#32476;&#29702;&#35770;&#26469;&#35782;&#21035;&#21487;&#20877;&#29983;&#30005;&#21147;&#20998;&#24067;&#31995;&#32479;&#24377;&#24615;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#35782;&#21035;&#31995;&#32479;&#20013;&#22826;&#38451;&#33021;&#30005;&#27744;&#26495;&#30340;&#25176;&#31649;&#33021;&#21147;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#25552;&#39640;&#31995;&#32479;&#30340;&#38887;&#24615;&#12290;</title><link>http://arxiv.org/abs/2208.11543</link><description>&lt;p&gt;
&#19968;&#31181;&#21033;&#29992;&#22797;&#26434;&#32593;&#32476;&#35782;&#21035;&#21487;&#20877;&#29983;&#30005;&#21147;&#20998;&#24067;&#31995;&#32479;&#24377;&#24615;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A methodology for identifying resiliency in renewable electrical distribution system using complex network. (arXiv:2208.11543v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.11543
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#22797;&#26434;&#32593;&#32476;&#29702;&#35770;&#26469;&#35782;&#21035;&#21487;&#20877;&#29983;&#30005;&#21147;&#20998;&#24067;&#31995;&#32479;&#24377;&#24615;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#35782;&#21035;&#31995;&#32479;&#20013;&#22826;&#38451;&#33021;&#30005;&#27744;&#26495;&#30340;&#25176;&#31649;&#33021;&#21147;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#25552;&#39640;&#31995;&#32479;&#30340;&#38887;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30005;&#21147;&#37197;&#30005;&#31995;&#32479;&#24191;&#27867;&#37319;&#29992;&#20998;&#24067;&#24335;&#33021;&#28304;&#36164;&#28304;&#65288;DER&#65289;&#20197;&#28385;&#36275;&#33021;&#28304;&#38656;&#27714;&#65292;&#26222;&#36941;&#35748;&#20026;&#36825;&#21487;&#20197;&#25552;&#39640;&#31995;&#32479;&#30340;&#24377;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21508;&#31181;&#22240;&#32032;&#65288;&#22914;&#38388;&#27463;&#24615;&#21487;&#29992;&#24615;&#12289;&#22825;&#27668;&#26465;&#20214;&#30340;&#21160;&#24577;&#21464;&#21270;&#12289;&#38750;&#32447;&#24615;&#31561;&#65289;&#21487;&#33021;&#23545;&#30005;&#32593;&#36816;&#33829;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22797;&#26434;&#32593;&#32476;&#29702;&#35770;&#26469;&#35782;&#21035;&#24102;&#26377;&#22826;&#38451;&#33021;&#20809;&#20239;&#21457;&#30005;&#30340;&#37197;&#30005;&#31995;&#32479;&#24377;&#24615;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#26681;&#25454;&#19981;&#21516;&#26465;&#20214;&#33719;&#24471;&#20102;&#19981;&#21516;&#26465;&#20214;&#19979;&#30340;&#22797;&#26434;&#30456;&#20851;&#32593;&#32476;&#65292;&#24182;&#35745;&#31639;&#20102;&#21508;&#31181;&#32593;&#32476;&#21442;&#25968;&#65292;&#20197;&#35782;&#21035;&#32593;&#32476;&#30340;&#24377;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#35782;&#21035;&#31995;&#32479;&#20013;&#22826;&#38451;&#33021;&#30005;&#27744;&#26495;&#30340;&#25176;&#31649;&#33021;&#21147;&#65292;&#24182;&#22312;&#19981;&#21516;&#19981;&#33391;&#26465;&#20214;&#19979;&#20445;&#25345;&#31995;&#32479;&#30340;&#24377;&#24615;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#25552;&#39640;&#31995;&#32479;&#30340;&#38887;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Electrical Distribution Systems are extensively penetrated with the Distributed Energy Resources (DERs) to cater the energy demands with general perception that it enhances the system resiliency. However, it may be adverse for the grid operation due to various factors like its intermittent availability, dynamics in weather condition, introduction of nonlinearity, complexity etc. This needs a detailed understanding of system resiliency that our method proposes here. We introduce a methodology using complex network theory to identify the resiliency of distribution system when incorporated with Solar PV generation under various undesirable configurations. Complex correlated networks for different conditions were obtained and various network parameters were computed for identifying the resiliency of those networks. The proposed methodology identifies the hosting capacity of solar panels in the system while maintaining the resiliency under different unwanted conditions hence helps
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36827;&#34892;&#20102;&#20851;&#20110;&#22826;&#38451;&#33021;&#39044;&#27979;&#30340;&#20803;&#20998;&#26512;&#65292;&#21457;&#29616;&#39044;&#27979;&#26102;&#38388;&#27573;&#26159;&#24433;&#21709;&#25216;&#33021;&#24471;&#20998;&#26368;&#37325;&#35201;&#30340;&#22240;&#32032;&#65292;&#34920;&#26126;&#24212;&#35813;&#20998;&#21035;&#38024;&#23545;&#19981;&#21516;&#30340;&#26102;&#38388;&#27573;&#36827;&#34892;&#39044;&#27979;&#20998;&#26512;&#12290;&#27668;&#20505;&#21306;&#21464;&#37327;&#19982;&#25216;&#33021;&#24471;&#20998;&#23384;&#22312;&#26174;&#33879;&#30456;&#20851;&#24615;&#12290;&#21382;&#21490;&#25968;&#25454;&#21644;&#26102;&#31354;&#20449;&#24687;&#22312;&#22826;&#38451;&#33021;&#39044;&#27979;&#20013;&#38750;&#24120;&#26377;&#24110;&#21161;&#65292;&#22825;&#31354;&#21644;&#21355;&#26143;&#22270;&#20687;&#21017;&#22312;&#26085;&#20869;&#39044;&#27979;&#20013;&#26368;&#37325;&#35201;&#65292;&#32780;&#25968;&#20540;&#27668;&#35937;&#39044;&#27979;&#21644;&#23454;&#27979;&#20540;&#22312;&#26085;&#21069;&#39044;&#27979;&#20013;&#26368;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2208.10536</link><description>&lt;p&gt;
&#22522;&#20110;&#25216;&#33021;&#24471;&#20998;&#30340;&#22826;&#38451;&#33021;&#39044;&#27979;&#30340;&#20803;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Meta-Analysis of Solar Forecasting Based on Skill Score. (arXiv:2208.10536v2 [stat.AP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.10536
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36827;&#34892;&#20102;&#20851;&#20110;&#22826;&#38451;&#33021;&#39044;&#27979;&#30340;&#20803;&#20998;&#26512;&#65292;&#21457;&#29616;&#39044;&#27979;&#26102;&#38388;&#27573;&#26159;&#24433;&#21709;&#25216;&#33021;&#24471;&#20998;&#26368;&#37325;&#35201;&#30340;&#22240;&#32032;&#65292;&#34920;&#26126;&#24212;&#35813;&#20998;&#21035;&#38024;&#23545;&#19981;&#21516;&#30340;&#26102;&#38388;&#27573;&#36827;&#34892;&#39044;&#27979;&#20998;&#26512;&#12290;&#27668;&#20505;&#21306;&#21464;&#37327;&#19982;&#25216;&#33021;&#24471;&#20998;&#23384;&#22312;&#26174;&#33879;&#30456;&#20851;&#24615;&#12290;&#21382;&#21490;&#25968;&#25454;&#21644;&#26102;&#31354;&#20449;&#24687;&#22312;&#22826;&#38451;&#33021;&#39044;&#27979;&#20013;&#38750;&#24120;&#26377;&#24110;&#21161;&#65292;&#22825;&#31354;&#21644;&#21355;&#26143;&#22270;&#20687;&#21017;&#22312;&#26085;&#20869;&#39044;&#27979;&#20013;&#26368;&#37325;&#35201;&#65292;&#32780;&#25968;&#20540;&#27668;&#35937;&#39044;&#27979;&#21644;&#23454;&#27979;&#20540;&#22312;&#26085;&#21069;&#39044;&#27979;&#20013;&#26368;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36827;&#34892;&#20102;&#22522;&#20110;&#25216;&#33021;&#24471;&#20998;&#30340;&#30830;&#23450;&#24615;&#22826;&#38451;&#33021;&#39044;&#27979;&#30340;&#31532;&#19968;&#39033;&#32508;&#21512;&#24615;&#20803;&#20998;&#26512;&#65292;&#31579;&#36873;&#20102;&#35895;&#27468;&#23398;&#26415;&#19978;&#30340;1,447&#31687;&#35770;&#25991;&#65292;&#24182;&#23545;320&#31687;&#35770;&#25991;&#36827;&#34892;&#20840;&#25991;&#23457;&#38405;&#20197;&#36827;&#34892;&#25968;&#25454;&#25552;&#21462;&#12290;&#24314;&#31435;&#20102;&#19968;&#20010;&#21253;&#21547;4,687&#20010;&#25968;&#25454;&#28857;&#30340;&#25968;&#25454;&#24211;&#65292;&#24182;&#36816;&#29992;&#22810;&#20803;&#33258;&#36866;&#24212;&#22238;&#24402;&#26679;&#26465;&#24314;&#27169;&#12289;&#37096;&#20998;&#20381;&#36182;&#22270;&#21644;&#32447;&#24615;&#22238;&#24402;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#23450;&#37327;&#27979;&#37327;&#20102;&#21313;&#20010;&#22240;&#32032;&#23545;&#25216;&#33021;&#24471;&#20998;&#30340;&#36793;&#38469;&#24433;&#21709;&#12290;&#20998;&#26512;&#32467;&#26524;&#26174;&#31034;&#20102;&#25968;&#25454;&#24211;&#20013;&#21464;&#37327;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#21644;&#22797;&#26434;&#20132;&#20114;&#20316;&#29992;&#12290;&#39044;&#27979;&#26102;&#38388;&#27573;&#23545;&#25216;&#33021;&#24471;&#20998;&#26377;&#20013;&#24515;&#24433;&#21709;&#24182;&#25903;&#37197;&#20102;&#20854;&#20182;&#22240;&#32032;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#22826;&#38451;&#33021;&#39044;&#27979;&#30340;&#20998;&#26512;&#24212;&#35813;&#38024;&#23545;&#27599;&#20010;&#26102;&#38388;&#27573;&#21333;&#29420;&#36827;&#34892;&#12290;&#27668;&#20505;&#21306;&#21464;&#37327;&#19982;&#25216;&#33021;&#24471;&#20998;&#20855;&#26377;&#26174;&#33879;&#30340;&#30456;&#20851;&#24615;&#12290;&#22312;&#36755;&#20837;&#26041;&#38754;&#65292;&#21382;&#21490;&#25968;&#25454;&#21644;&#26102;&#31354;&#20449;&#24687;&#38750;&#24120;&#26377;&#24110;&#21161;&#12290;&#23545;&#20110;&#26085;&#20869;&#39044;&#27979;&#65292;&#22825;&#31354;&#21644;&#21355;&#26143;&#22270;&#20687;&#26368;&#37325;&#35201;&#12290;&#23545;&#20110;&#26085;&#21069;&#39044;&#27979;&#65292;&#21017;&#26159;&#25968;&#20540;&#27668;&#35937;&#39044;&#27979;&#21644;&#26412;&#22320;&#23454;&#27979;&#20540;&#26368;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
We conduct the first comprehensive meta-analysis of deterministic solar forecasting based on skill score, screening 1,447 papers from Google Scholar and reviewing the full texts of 320 papers for data extraction. A database of 4,687 points was built and analyzed with multivariate adaptive regression spline modelling, partial dependence plots, and linear regression. The marginal impacts on skill score of ten factors were quantified. The analysis shows the non-linearity and complex interaction between variables in the database. Forecast horizon has a central impact and dominates other factors' impacts. Therefore, the analysis of solar forecasts should be done separately for each horizon. Climate zone variables have statistically significant correlation with skill score. Regarding inputs, historical data and spatial temporal information are highly helpful. For intra-day, sky and satellite images show the most importance. For day-ahead, numerical weather predictions and locally measured me
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Medical X-VL&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#29992;&#33258;&#30417;&#30563;&#21333;&#27169;&#22411;&#21644;&#34701;&#21512;&#32534;&#30721;&#22120;&#65292;&#20197;&#23454;&#29616;&#25918;&#23556;&#23398;&#20013;&#30340;&#38646;&#26679;&#26412;&#30417;&#30563;&#20154;&#24037;&#26234;&#33021;&#12290;</title><link>http://arxiv.org/abs/2208.05140</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#30340;&#22810;&#27169;&#24577;&#35757;&#32451;&#65292;&#21033;&#29992;&#26410;&#32463;&#25972;&#29702;&#30340;&#22270;&#20687;&#21644;&#25253;&#21578;&#23454;&#29616;&#25918;&#23556;&#23398;&#38646;&#26679;&#26412;&#30417;&#30563;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Self-supervised Multi-modal Training from Uncurated Image and Reports Enables Zero-shot Oversight Artificial Intelligence in Radiology. (arXiv:2208.05140v4 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.05140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Medical X-VL&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#29992;&#33258;&#30417;&#30563;&#21333;&#27169;&#22411;&#21644;&#34701;&#21512;&#32534;&#30721;&#22120;&#65292;&#20197;&#23454;&#29616;&#25918;&#23556;&#23398;&#20013;&#30340;&#38646;&#26679;&#26412;&#30417;&#30563;&#20154;&#24037;&#26234;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#31649;&#22411;AI&#26159;&#25918;&#23556;&#23398;&#20013;&#30340;&#26032;&#20852;&#27010;&#24565;&#65292;&#20854;&#20013;AI&#36890;&#36807;&#19981;&#26029;&#25903;&#25345;&#25918;&#23556;&#23398;&#23478;&#30340;&#20915;&#31574;&#65292;&#24418;&#25104;&#19982;&#25918;&#23556;&#23398;&#23478;&#30340;&#20849;&#29983;&#20851;&#31995;&#12290;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#25581;&#31034;&#20102;&#30417;&#31649;&#24615;AI&#30340;&#38271;&#26399;&#38382;&#39064;&#65292;&#21363;&#23427;&#20204;&#29702;&#35299;&#35270;&#35273;&#21644;&#25991;&#26412;&#27010;&#24565;&#21450;&#20854;&#35821;&#20041;&#23545;&#24212;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#23558;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#21307;&#23398;&#39046;&#22495;&#30340;&#25104;&#21151;&#26696;&#20363;&#36824;&#24456;&#26377;&#38480;&#65292;&#22240;&#20026;&#30446;&#21069;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#21644;&#23398;&#20064;&#31574;&#30053;&#38656;&#35201;&#22270;&#20687;&#21644;&#25991;&#26412;&#23545;&#30340;&#32593;&#32476;&#35268;&#27169;&#25968;&#25454;&#35821;&#26009;&#24211;&#65292;&#36825;&#22312;&#21307;&#23398;&#39046;&#22495;&#36890;&#24120;&#38590;&#20197;&#23454;&#29616;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#34987;&#31216;&#20026;&#21307;&#23398;&#20132;&#21449;&#20851;&#27880;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;Medical X-VL&#65289;&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#36866;&#29992;&#20110;&#21307;&#23398;&#39046;&#22495;&#30340;&#20851;&#38190;&#32452;&#20214;&#12290;&#25105;&#20204;&#30340;&#21307;&#23398;X-VL&#27169;&#22411;&#22522;&#20110;&#20197;&#19979;&#32452;&#20214;&#65306;&#21307;&#23398;&#39046;&#22495;&#30340;&#33258;&#30417;&#30563;&#21333;&#27169;&#22411;&#21644;&#34701;&#21512;&#32534;&#30721;&#22120;&#65292;&#20197;&#26500;&#24314;&#22810;&#27169;&#24577;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Oversight AI is an emerging concept in radiology where the AI forms a symbiosis with radiologists by continuously supporting radiologists in their decision-making. Recent advances in vision-language models sheds a light on the long-standing problems of the oversight AI by the understanding both visual and textual concepts and their semantic correspondences. However, there have been limited successes in the application of vision-language models in the medical domain, as the current vision-language models and learning strategies for photographic images and captions call for the web-scale data corpus of image and text pairs which was not often feasible in the medical domain. To address this, here we present a model dubbed Medical Cross-attention Vision-Language model (Medical X-VL), leveraging the key components to be tailored for the medical domain. Our medical X-VL model is based on the following components: self-supervised uni-modal models in medical domain and fusion encoder to bridge
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#22238;&#39038;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#22522;&#20110;&#25490;&#21517;&#21487;&#20998;&#35299;&#25439;&#22833;&#20989;&#25968;&#30340;&#30740;&#31350;&#24182;&#25552;&#20379;&#20102;&#20998;&#31867;&#27861;&#12290;&#23427;&#35752;&#35770;&#20102;&#21487;&#20998;&#35299;&#24615;&#19982;&#20248;&#21270;&#31639;&#27861;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#21015;&#20030;&#20102;&#25490;&#21517;&#25439;&#22833;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#26368;&#26032;&#24212;&#29992;&#21644;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2207.08768</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#22522;&#20110;&#25490;&#21517;&#21487;&#20998;&#35299;&#25439;&#22833;&#20989;&#25968;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Rank-based Decomposable Losses in Machine Learning: A Survey. (arXiv:2207.08768v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.08768
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#22238;&#39038;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#22522;&#20110;&#25490;&#21517;&#21487;&#20998;&#35299;&#25439;&#22833;&#20989;&#25968;&#30340;&#30740;&#31350;&#24182;&#25552;&#20379;&#20102;&#20998;&#31867;&#27861;&#12290;&#23427;&#35752;&#35770;&#20102;&#21487;&#20998;&#35299;&#24615;&#19982;&#20248;&#21270;&#31639;&#27861;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#21015;&#20030;&#20102;&#25490;&#21517;&#25439;&#22833;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#26368;&#26032;&#24212;&#29992;&#21644;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#25439;&#22833;&#20989;&#25968;&#35774;&#35745;&#20013;&#21306;&#20998;&#20010;&#20307;&#25439;&#22833;&#19982;&#32858;&#21512;&#25439;&#22833;&#30340;&#37325;&#35201;&#33539;&#24335;&#12290;&#20010;&#20307;&#25439;&#22833;&#35780;&#20272;&#27169;&#22411;&#22312;&#26679;&#26412;&#19978;&#30340;&#36136;&#37327;&#65292;&#32780;&#32858;&#21512;&#25439;&#22833;&#23558;&#27599;&#20010;&#35757;&#32451;&#26679;&#26412;&#30340;&#20010;&#20307;&#25439;&#22833;/&#20998;&#25968;&#32452;&#21512;&#36215;&#26469;&#12290;&#20004;&#32773;&#37117;&#26377;&#19968;&#20010;&#20849;&#21516;&#30340;&#36807;&#31243;&#65292;&#23558;&#19968;&#32452;&#20010;&#20307;&#20540;&#32858;&#21512;&#25104;&#21333;&#20010;&#25968;&#23383;&#20540;&#12290;&#25490;&#21517;&#39034;&#24207;&#21453;&#26144;&#20102;&#35774;&#35745;&#25439;&#22833;&#20013;&#20010;&#20307;&#20540;&#20043;&#38388;&#26368;&#22522;&#26412;&#30340;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#21487;&#20998;&#35299;&#24615;&#65292;&#20854;&#20013;&#25439;&#22833;&#21487;&#20197;&#20998;&#35299;&#20026;&#19968;&#32452;&#20010;&#20307;&#39033;&#30340;&#38598;&#21512;&#65292;&#25104;&#20026;&#32452;&#32455;&#25439;&#22833;/&#20998;&#25968;&#30340;&#37325;&#35201;&#23646;&#24615;&#12290;&#26412;&#32508;&#36848;&#31995;&#32479;&#20840;&#38754;&#22320;&#22238;&#39038;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#22522;&#20110;&#25490;&#21517;&#21487;&#20998;&#35299;&#25439;&#22833;&#20989;&#25968;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#25353;&#32858;&#21512;&#25439;&#22833;&#21644;&#20010;&#20307;&#25439;&#22833;&#35270;&#35282;&#30340;&#26032;&#25439;&#22833;&#20989;&#25968;&#20998;&#31867;&#27861;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#24418;&#25104;&#36825;&#31181;&#25439;&#22833;&#30340;&#32858;&#21512;&#22120;&#65292;&#23427;&#20204;&#26159;&#38598;&#21512;&#20989;&#25968;&#30340;&#31034;&#20363;&#12290;&#25105;&#20204;&#32452;&#32455;&#20102;&#26377;&#30417;&#30563;&#23398;&#20064;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#22522;&#20110;&#25490;&#21517;&#30340;&#21487;&#20998;&#35299;&#25439;&#22833;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#25439;&#22833;&#30340;&#21487;&#20998;&#35299;&#24615;&#19982;&#20248;&#21270;&#31639;&#27861;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#25490;&#21517;&#25439;&#22833;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#26368;&#26032;&#24212;&#29992;&#21644;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have revealed an essential paradigm in designing loss functions that differentiate individual losses vs. aggregate losses. The individual loss measures the quality of the model on a sample, while the aggregate loss combines individual losses/scores over each training sample. Both have a common procedure that aggregates a set of individual values to a single numerical value. The ranking order reflects the most fundamental relation among individual values in designing losses. In addition, decomposability, in which a loss can be decomposed into an ensemble of individual terms, becomes a significant property of organizing losses/scores. This survey provides a systematic and comprehensive review of rank-based decomposable losses in machine learning. Specifically, we provide a new taxonomy of loss functions that follows the perspectives of aggregate loss and individual loss. We identify the aggregator to form such losses, which are examples of set functions. We organize the rank
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#23545;&#25239;&#24615;&#20195;&#29702;&#39118;&#38505;&#30340;&#23384;&#22312;&#24615;&#12289;&#27491;&#21017;&#24615;&#21644;&#26497;&#23567;&#21270;&#23450;&#29702;&#65292;&#36825;&#19968;&#32467;&#26524;&#20026;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#29702;&#35770;&#25552;&#20379;&#20102;&#25903;&#25345;&#65292;&#24182;&#19988;&#21487;&#20197;&#25351;&#23548;&#31639;&#27861;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2206.09098</link><description>&lt;p&gt;
&#23545;&#25239;&#24615;&#20195;&#29702;&#39118;&#38505;&#22312;&#20108;&#20803;&#20998;&#31867;&#20013;&#30340;&#23384;&#22312;&#24615;&#21644;&#26497;&#23567;&#21270;&#23450;&#29702;
&lt;/p&gt;
&lt;p&gt;
Existence and Minimax Theorems for Adversarial Surrogate Risks in Binary Classification. (arXiv:2206.09098v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.09098
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#23545;&#25239;&#24615;&#20195;&#29702;&#39118;&#38505;&#30340;&#23384;&#22312;&#24615;&#12289;&#27491;&#21017;&#24615;&#21644;&#26497;&#23567;&#21270;&#23450;&#29702;&#65292;&#36825;&#19968;&#32467;&#26524;&#20026;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#29702;&#35770;&#25552;&#20379;&#20102;&#25903;&#25345;&#65292;&#24182;&#19988;&#21487;&#20197;&#25351;&#23548;&#31639;&#27861;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#35757;&#32451;&#26159;&#35757;&#32451;&#40065;&#26834;&#24615;&#24378;&#30340;&#26041;&#27861;&#20043;&#19968;&#65292;&#28982;&#32780;&#65292;&#23427;&#20174;&#29702;&#35770;&#35282;&#24230;&#24182;&#19981;&#20026;&#20154;&#20204;&#25152;&#29087;&#30693;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#23545;&#25239;&#24615;&#20195;&#29702;&#39118;&#38505;&#30340;&#23384;&#22312;&#24615;&#12289;&#27491;&#21017;&#24615;&#21644;&#26497;&#23567;&#21270;&#23450;&#29702;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#35299;&#37322;&#20102;&#20043;&#21069;&#30740;&#31350;&#20013;&#26377;&#20851;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#19968;&#20123;&#32463;&#39564;&#35266;&#23519;&#65292;&#24182;&#25552;&#20986;&#20102;&#31639;&#27861;&#21457;&#23637;&#30340;&#26032;&#26041;&#21521;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#23558;&#20043;&#21069;&#24050;&#30693;&#30340;&#23545;&#25239;&#20998;&#31867;&#39118;&#38505;&#30340;&#23384;&#22312;&#21644;&#26497;&#23567;&#21270;&#23450;&#29702;&#25193;&#23637;&#21040;&#20102;&#20195;&#29702;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial training is one of the most popular methods for training methods robust to adversarial attacks, however, it is not well-understood from a theoretical perspective. We prove and existence, regularity, and minimax theorems for adversarial surrogate risks. Our results explain some empirical observations on adversarial robustness from prior work and suggest new directions in algorithm development. Furthermore, our results extend previously known existence and minimax theorems for the adversarial classification risk to surrogate risks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26816;&#27979;&#21644;&#38450;&#24481;&#25200;&#21160;&#22522;&#30784;&#35299;&#37322;&#22120;&#30340;&#24694;&#24847;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#23545;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#21644;&#20449;&#20219;&#24615;&#30340;&#20445;&#38556;&#12290;</title><link>http://arxiv.org/abs/2205.14772</link><description>&lt;p&gt;
&#26080;&#27450;&#39575;&#24615;&#22522;&#20110;&#25200;&#21160;&#30340;&#20107;&#21518;&#35299;&#37322;&#22120;
&lt;/p&gt;
&lt;p&gt;
Unfooling Perturbation-Based Post Hoc Explainers. (arXiv:2205.14772v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.14772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26816;&#27979;&#21644;&#38450;&#24481;&#25200;&#21160;&#22522;&#30784;&#35299;&#37322;&#22120;&#30340;&#24694;&#24847;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#23545;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#21644;&#20449;&#20219;&#24615;&#30340;&#20445;&#38556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#24040;&#22823;&#36827;&#27493;&#21560;&#24341;&#20102;&#21307;&#29983;&#12289;&#36151;&#27454;&#20154;&#12289;&#27861;&#23448;&#21644;&#20854;&#20182;&#19987;&#19994;&#20154;&#21592;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29087;&#24713;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#20154;&#23545;&#20854;&#20915;&#31574;&#36807;&#31243;&#32570;&#20047;&#36879;&#26126;&#24230;&#34920;&#31034;&#25285;&#24551;&#12290;&#25200;&#21160;&#22522;&#30784;&#20107;&#21518;&#35299;&#37322;&#22120;&#25552;&#20379;&#20102;&#19968;&#31181;&#23545;&#36825;&#20123;&#31995;&#32479;&#36827;&#34892;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#21482;&#38656;&#35201;&#26597;&#35810;&#32423;&#21035;&#35775;&#38382;&#26435;&#38480;&#21363;&#21487;&#35299;&#37322;&#20219;&#20309;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#20123;&#35299;&#37322;&#22120;&#21487;&#20197;&#34987;&#24694;&#24847;&#25915;&#20987;&#12290;&#36825;&#19968;&#21457;&#29616;&#23545;&#23457;&#35745;&#21592;&#12289;&#30417;&#31649;&#26426;&#26500;&#21644;&#20854;&#20182;&#30417;&#30563;&#32773;&#20135;&#29983;&#20102;&#20005;&#37325;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#20960;&#20010;&#33258;&#28982;&#38382;&#39064;&#26174;&#32780;&#26131;&#35265;&#8212;&#8212;&#25105;&#20204;&#22914;&#20309;&#23457;&#35745;&#36825;&#20123;&#40657;&#21283;&#23376;&#31995;&#32479;&#65311;&#25105;&#20204;&#22914;&#20309;&#30830;&#23450;&#23457;&#35745;&#23545;&#35937;&#26159;&#21542;&#26159;&#30495;&#35802;&#37197;&#21512;&#23457;&#35745;&#30340;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20005;&#26684;&#35268;&#33539;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#25269;&#24481;&#25200;&#21160;&#22522;&#30784;&#35299;&#37322;&#22120;&#30340;&#24694;&#24847;&#25915;&#20987;&#30340;&#38450;&#24481;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#26816;&#27979;&#65288;CAD-Detect&#65289;&#21644;&#38450;&#24481;&#65288;CAD-Defense&#65289;&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#39640;&#31934;&#24230;&#22320;&#26816;&#27979;&#20986;&#24694;&#24847;&#25915;&#20987;&#65292;&#32780;&#19988;&#36824;&#21487;&#20197;&#26377;&#25928;&#22320;&#25269;&#24481;&#23427;&#20204;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#21644;&#20449;&#20219;&#24615;&#25552;&#20379;&#20102;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Monumental advancements in artificial intelligence (AI) have lured the interest of doctors, lenders, judges, and other professionals. While these high-stakes decision-makers are optimistic about the technology, those familiar with AI systems are wary about the lack of transparency of its decision-making processes. Perturbation-based post hoc explainers offer a model agnostic means of interpreting these systems while only requiring query-level access. However, recent work demonstrates that these explainers can be fooled adversarially. This discovery has adverse implications for auditors, regulators, and other sentinels. With this in mind, several natural questions arise - how can we audit these black box systems? And how can we ascertain that the auditee is complying with the audit in good faith? In this work, we rigorously formalize this problem and devise a defense against adversarial attacks on perturbation-based explainers. We propose algorithms for the detection (CAD-Detect) and de
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#26631;&#31614;&#35268;&#33539;&#21270;&#25216;&#26415;&#65292;&#21457;&#29616;&#26631;&#31614;&#35268;&#33539;&#21270;&#36890;&#24120;&#20250;&#25552;&#39640;&#24494;&#35843;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#24403;&#20351;&#29992;&#36739;&#23567;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26102;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#25945;&#24072;&#32593;&#32476;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20854;&#25928;&#26524;&#19982;&#30693;&#35782;&#33976;&#39311;&#30456;&#24403;&#65292;&#20294;&#21462;&#20915;&#20110;&#25945;&#24072;&#32593;&#32476;&#30340;&#22823;&#23567;&#21644;&#19979;&#28216;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2205.12428</link><description>&lt;p&gt;
&#25105;&#20204;&#38656;&#35201;&#26631;&#31614;&#35268;&#33539;&#21270;&#26469;&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do we need Label Regularization to Fine-tune Pre-trained Language Models?. (arXiv:2205.12428v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.12428
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#26631;&#31614;&#35268;&#33539;&#21270;&#25216;&#26415;&#65292;&#21457;&#29616;&#26631;&#31614;&#35268;&#33539;&#21270;&#36890;&#24120;&#20250;&#25552;&#39640;&#24494;&#35843;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#24403;&#20351;&#29992;&#36739;&#23567;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26102;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#25945;&#24072;&#32593;&#32476;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20854;&#25928;&#26524;&#19982;&#30693;&#35782;&#33976;&#39311;&#30456;&#24403;&#65292;&#20294;&#21462;&#20915;&#20110;&#25945;&#24072;&#32593;&#32476;&#30340;&#22823;&#23567;&#21644;&#19979;&#28216;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#31070;&#32463;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#65292;&#23427;&#20005;&#37325;&#20381;&#36182;&#20110;&#25945;&#24072;&#32593;&#32476;&#30340;&#39044;&#27979;&#26469;&#25351;&#23548;&#23398;&#29983;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#26029;&#22686;&#22823;&#65292;&#30693;&#35782;&#33976;&#39311;&#32463;&#24120;&#34987;&#37319;&#29992;&#22312;&#35768;&#22810;&#28041;&#21450;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#21508;&#31181;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;&#26631;&#31614;&#35268;&#33539;&#21270;&#36890;&#24120;&#20250;&#25552;&#39640;&#24494;&#35843;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21363;&#20351;&#20351;&#29992;&#36739;&#23567;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#35813;&#26041;&#27861;&#30340;&#22909;&#22788;&#21462;&#20915;&#20110;&#25945;&#24072;&#32593;&#32476;&#30340;&#22823;&#23567;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#25945;&#24072;&#32593;&#32476;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;&#32852;&#21512;&#24494;&#35843;&#8221;&#65292;&#23427;&#20351;&#29992;&#23398;&#29983;&#27169;&#22411;&#30340;&#38598;&#21512;&#26469;&#20195;&#26367;&#25945;&#24072;&#32593;&#32476;&#65292;&#21462;&#24471;&#20102;&#19982;&#30693;&#35782;&#33976;&#39311;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge Distillation (KD) is a prominent neural model compression technique that heavily relies on teacher network predictions to guide the training of a student model. Considering the ever-growing size of pre-trained language models (PLMs), KD is often adopted in many NLP tasks involving PLMs. However, it is evident that in KD, deploying the teacher network during training adds to the memory and computational requirements of training. In the computer vision literature, the necessity of the teacher network is put under scrutiny by showing that KD is a label regularization technique that can be replaced with lighter teacher-free variants such as the label-smoothing technique. However, to the best of our knowledge, this issue is not investigated in NLP. Therefore, this work concerns studying different label regularization techniques and whether we actually need them to improve the fine-tuning of smaller PLM networks on downstream tasks. In this regard, we did a comprehensive set of exp
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35299;&#37322;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#20998;&#31867;&#19978;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#35748;&#20026;CNN&#22312;&#23398;&#20064;&#22270;&#20687;&#22359;&#21518;&#36991;&#20813;&#20102;&#32500;&#25968;&#28798;&#38590;&#65292;&#24182;&#25552;&#20379;&#20102;&#25903;&#25345;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#35777;&#25454;&#12290;</title><link>http://arxiv.org/abs/2205.10760</link><description>&lt;p&gt;
CNN&#22312;&#23398;&#20064;&#22270;&#20687;&#22359;&#21518;&#36991;&#20813;&#20102;&#32500;&#25968;&#28798;&#38590;
&lt;/p&gt;
&lt;p&gt;
CNNs Avoid Curse of Dimensionality by Learning on Patches. (arXiv:2205.10760v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.10760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35299;&#37322;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#20998;&#31867;&#19978;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#35748;&#20026;CNN&#22312;&#23398;&#20064;&#22270;&#20687;&#22359;&#21518;&#36991;&#20813;&#20102;&#32500;&#25968;&#28798;&#38590;&#65292;&#24182;&#25552;&#20379;&#20102;&#25903;&#25345;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#22312;&#35768;&#22810;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#24182;&#20855;&#26377;&#38750;&#20961;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#20294;&#30446;&#21069;&#20026;&#27490;&#65292;&#26377;&#20851;&#39044;&#27979;CNN&#27867;&#21270;&#38169;&#35823;&#30340;&#23581;&#35797;&#21482;&#38480;&#20110;&#20107;&#21518;&#20998;&#26512;&#12290;&#20107;&#20808;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#24615;&#33021;&#30340;&#29702;&#35770;&#22823;&#22810;&#24573;&#30053;&#20102;&#21367;&#31215;&#24615;&#26041;&#38754;&#65292;&#24182;&#19988;&#19981;&#25351;&#26126;CNN&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#65288;&#22914;&#22270;&#20687;&#20998;&#31867;&#65292;&#20854;&#20013;&#22270;&#20687;&#32500;&#25968;&#20026;&#25968;&#21315;&#65289;&#20013;&#20026;&#20309;&#33021;&#22815;&#20284;&#20046;&#20811;&#26381;&#32500;&#25968;&#28798;&#38590;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#35797;&#22270;&#22312;&#20551;&#35774;CNN&#22312;&#22270;&#20687;&#22359;&#30340;&#22495;&#19978;&#36816;&#20316;&#30340;&#24773;&#20917;&#19979;&#35299;&#37322;CNN&#22312;&#22270;&#20687;&#20998;&#31867;&#19978;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26159;&#25105;&#20204;&#25152;&#30693;&#36947;&#30340;&#31532;&#19968;&#20010;&#20026;CNN&#30340;&#27867;&#21270;&#35823;&#24046;&#25512;&#23548;&#20808;&#39564;&#35823;&#24046;&#30028;&#30340;&#24037;&#20316;&#65292;&#24182;&#25105;&#20204;&#25552;&#20379;&#20102;&#23450;&#37327;&#21644;&#23450;&#24615;&#35777;&#25454;&#25903;&#25345;&#25105;&#20204;&#30340;&#29702;&#35770;&#12290;&#25105;&#20204;&#30340;&#22522;&#20110;&#22359;&#30340;&#29702;&#35770;&#36824;&#20026;&#20160;&#20040;&#25968;&#25454;&#22686;&#24378;&#34920;&#29616;&#20986;&#21487;&#38752;&#24615;&#25552;&#20379;&#20102;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the success of convolutional neural networks (CNNs) in numerous computer vision tasks and their extraordinary generalization performances, several attempts to predict the generalization errors of CNNs have only been limited to a posteriori analyses thus far. A priori theories explaining the generalization performances of deep neural networks have mostly ignored the convolutionality aspect and do not specify why CNNs are able to seemingly overcome curse of dimensionality on computer vision tasks like image classification where the image dimensions are in thousands. Our work attempts to explain the generalization performance of CNNs on image classification under the hypothesis that CNNs operate on the domain of image patches. Ours is the first work we are aware of to derive an a priori error bound for the generalization error of CNNs and we present both quantitative and qualitative evidences in the support of our theory. Our patch-based theory also offers explanation for why data
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#20845;&#31181;AutoML&#26694;&#26550;&#22312;100&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#27604;&#36739;&#65292;&#21457;&#29616;&#19981;&#21516;&#26694;&#26550;&#22312;&#19981;&#21516;&#24615;&#33021;&#25351;&#26631;&#26041;&#38754;&#21508;&#26377;&#20248;&#21183;&#21644;&#21155;&#21183;&#65292;&#26694;&#26550;&#30340;&#36873;&#25321;&#24212;&#26681;&#25454;&#29305;&#23450;&#20219;&#21153;&#35201;&#27714;&#26469;&#36827;&#34892;&#12290;</title><link>http://arxiv.org/abs/2204.08358</link><description>&lt;p&gt;
AutoMLBench&#65306;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#30340;&#20840;&#38754;&#23454;&#39564;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
AutoMLBench: A Comprehensive Experimental Evaluation of Automated Machine Learning Frameworks. (arXiv:2204.08358v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.08358
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#20845;&#31181;AutoML&#26694;&#26550;&#22312;100&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#27604;&#36739;&#65292;&#21457;&#29616;&#19981;&#21516;&#26694;&#26550;&#22312;&#19981;&#21516;&#24615;&#33021;&#25351;&#26631;&#26041;&#38754;&#21508;&#26377;&#20248;&#21183;&#21644;&#21155;&#21183;&#65292;&#26694;&#26550;&#30340;&#36873;&#25321;&#24212;&#26681;&#25454;&#29305;&#23450;&#20219;&#21153;&#35201;&#27714;&#26469;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#38656;&#27714;&#30340;&#36805;&#36895;&#22686;&#38271;&#65292;&#24050;&#32463;&#24847;&#35782;&#21040;&#26377;&#36275;&#22815;&#30693;&#35782;&#30340;&#25968;&#25454;&#31185;&#23398;&#23478;&#30340;&#25968;&#37327;&#26080;&#27861;&#36319;&#38543;&#25968;&#23383;&#19990;&#30028;&#20013;&#19981;&#26029;&#22686;&#38271;&#30340;&#25968;&#25454;&#37327;&#21644;&#24212;&#29992;&#38656;&#27714;&#32780;&#25193;&#23637;&#12290;&#20026;&#28385;&#36275;&#36825;&#19968;&#38656;&#27714;&#65292;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#20960;&#31181;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#65288;AutoML&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#26500;&#24314;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#26469;&#22635;&#34917;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#30340;&#24046;&#36317;&#12290;&#27599;&#20010;&#26694;&#26550;&#37117;&#24102;&#26377;&#19981;&#21516;&#30340;&#22522;&#20110;&#21551;&#21457;&#24335;&#30340;&#35774;&#35745;&#20915;&#31574;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#20845;&#31181;&#27969;&#34892;&#30340;AutoML&#26694;&#26550;&#65288;AutoWeka&#12289;AutoSKlearn&#12289;TPOT&#12289;Recipe&#12289;ATM&#21644;SmartML&#65289;&#22312;&#26469;&#33258;&#24050;&#24314;&#31435;&#30340;AutoML&#22522;&#20934;&#22871;&#20214;&#30340;100&#20010;&#25968;&#25454;&#38598;&#19978;&#25191;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#21644;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35780;&#20272;&#32771;&#34385;&#20102;&#19981;&#21516;&#26041;&#38754;&#30340;&#27604;&#36739;&#65292;&#21253;&#25324;&#22810;&#20010;&#35774;&#35745;&#20915;&#31574;&#30340;&#24615;&#33021;&#24433;&#21709;&#65292;&#22914;&#26102;&#38388;&#39044;&#31639;&#12289;&#25628;&#32034;&#31354;&#38388;&#30340;&#22823;&#23567;&#12289;&#20803;&#23398;&#20064;&#21644;&#38598;&#25104;&#26500;&#24314;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#27599;&#20010;AutoML&#26694;&#26550;&#22312;&#19981;&#21516;&#24615;&#33021;&#25351;&#26631;&#26041;&#38754;&#37117;&#26377;&#20854;&#20248;&#28857;&#21644;&#32570;&#28857;&#65292;&#26694;&#26550;&#30340;&#36873;&#25321;&#21487;&#33021;&#21462;&#20915;&#20110;&#25163;&#22836;&#20219;&#21153;&#30340;&#29305;&#23450;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the booming demand for machine learning applications, it has been recognized that the number of knowledgeable data scientists can not scale with the growing data volumes and application needs in our digital world. In response to this demand, several automated machine learning (AutoML) frameworks have been developed to fill the gap of human expertise by automating the process of building machine learning pipelines. Each framework comes with different heuristics-based design decisions. In this study, we present a comprehensive evaluation and comparison of the performance characteristics of six popular AutoML frameworks, namely, AutoWeka, AutoSKlearn, TPOT, Recipe, ATM, and SmartML, across 100 data sets from established AutoML benchmark suites. Our experimental evaluation considers different aspects for its comparison, including the performance impact of several design decisions, including time budget, size of search space, meta-learning, and ensemble construction. The results of our
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#19968;&#33268;&#24615;&#26041;&#27861;&#26469;&#35299;&#20915;&#30697;&#38453;&#21644;&#24352;&#37327;&#34917;&#20840;&#38382;&#39064;&#65292;&#22312;&#25512;&#33616;&#31995;&#32479;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36890;&#36807;&#20445;&#30041;&#21333;&#20301;&#27604;&#20363;&#21644;&#19968;&#33268;&#24615;&#20004;&#20010;&#32422;&#26463;&#26465;&#20214;&#21487;&#20197;&#23454;&#29616;&#35299;&#30340;&#23384;&#22312;&#24615;&#19982;&#21807;&#19968;&#24615;&#12290;</title><link>http://arxiv.org/abs/2204.01815</link><description>&lt;p&gt;
&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#19968;&#33268;&#24615;&#21644;&#20844;&#24179;&#20445;&#35777;&#30340;&#25512;&#33616;&#31995;&#32479;&#24352;&#37327;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Tensor Completion with Provable Consistency and Fairness Guarantees for Recommender Systems. (arXiv:2204.01815v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.01815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#19968;&#33268;&#24615;&#26041;&#27861;&#26469;&#35299;&#20915;&#30697;&#38453;&#21644;&#24352;&#37327;&#34917;&#20840;&#38382;&#39064;&#65292;&#22312;&#25512;&#33616;&#31995;&#32479;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36890;&#36807;&#20445;&#30041;&#21333;&#20301;&#27604;&#20363;&#21644;&#19968;&#33268;&#24615;&#20004;&#20010;&#32422;&#26463;&#26465;&#20214;&#21487;&#20197;&#23454;&#29616;&#35299;&#30340;&#23384;&#22312;&#24615;&#19982;&#21807;&#19968;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#19968;&#33268;&#24615;&#30340;&#26041;&#27861;&#26469;&#23450;&#20041;&#21644;&#35299;&#20915;&#38750;&#36127;/&#27491;&#30697;&#38453;&#21644;&#24352;&#37327;&#34917;&#20840;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#30340;&#26032;&#39062;&#20043;&#22788;&#22312;&#20110;&#65292;&#25105;&#20204;&#19981;&#26159;&#20154;&#20026;&#22320;&#23558;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#20219;&#24847;&#20248;&#21270;&#38382;&#39064;&#65292;&#20363;&#22914;&#65292;&#26368;&#23567;&#21270;&#19968;&#20010;&#32467;&#26500;&#37327;&#65292;&#22914;&#31209;&#25110;&#33539;&#25968;&#65292;&#32780;&#26159;&#23637;&#31034;&#20102;&#19968;&#20010;&#21333;&#19968;&#30340;&#23646;&#24615;/&#32422;&#26463;&#65306;&#20445;&#30041;&#21333;&#20301;&#27604;&#20363;&#19968;&#33268;&#24615;&#65292;&#20445;&#35777;&#20102;&#35299;&#30340;&#23384;&#22312;&#65292;&#24182;&#22312;&#30456;&#23545;&#36739;&#24369;&#30340;&#25903;&#25345;&#20551;&#35774;&#19979;&#20445;&#35777;&#20102;&#35299;&#30340;&#21807;&#19968;&#24615;&#12290;&#35813;&#26694;&#26550;&#21644;&#35299;&#31639;&#27861;&#20063;&#30452;&#25509;&#25512;&#24191;&#21040;&#20219;&#24847;&#32500;&#24230;&#30340;&#24352;&#37327;&#20013;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#22266;&#23450;&#32500;&#24230; d &#30340;&#38382;&#39064;&#35268;&#27169;&#30340;&#32447;&#24615;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#22312;&#25512;&#33616;&#31995;&#32479;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20004;&#20010;&#21512;&#29702;&#30340;&#24615;&#36136;&#65292;&#36825;&#20123;&#24615;&#36136;&#24212;&#35813;&#36866;&#29992;&#20110;&#20219;&#20309; RS &#38382;&#39064;&#30340;&#35299;&#65292;&#36275;&#20197;&#20801;&#35768;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20869;&#24314;&#31435;&#21807;&#19968;&#24615;&#20445;&#35777;&#12290;&#20851;&#38190;&#29702;&#35770;&#36129;&#29486;&#26159;&#23637;&#31034;&#20102;&#36825;&#20123;&#32422;&#26463;&#19979;&#35299;&#30340;&#23384;&#22312;&#24615;&#19982;&#21807;&#19968;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new consistency-based approach for defining and solving nonnegative/positive matrix and tensor completion problems. The novelty of the framework is that instead of artificially making the problem well-posed in the form of an application-arbitrary optimization problem, e.g., minimizing a bulk structural measure such as rank or norm, we show that a single property/constraint: preserving unit-scale consistency, guarantees the existence of both a solution and, under relatively weak support assumptions, uniqueness. The framework and solution algorithms also generalize directly to tensors of arbitrary dimensions while maintaining computational complexity that is linear in problem size for fixed dimension d. In the context of recommender system (RS) applications, we prove that two reasonable properties that should be expected to hold for any solution to the RS problem are sufficient to permit uniqueness guarantees to be established within our framework. Key theoretical contribu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#35757;&#32451;&#35823;&#24046;&#30340;&#26799;&#24230;&#19979;&#38477;&#21160;&#24577;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#20998;&#26512;&#20844;&#24335;&#65292;&#21487;&#20197;&#25429;&#25417;&#21040;&#23485;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#25439;&#22833;&#20989;&#25968;&#30340;&#24179;&#22343;&#34892;&#20026;&#12290;&#25105;&#20204;&#39044;&#27979;&#24182;&#34920;&#24449;&#20102;&#38543;&#26426;&#37327;&#23376;&#30005;&#36335;&#27531;&#20313;&#35757;&#32451;&#35823;&#24046;&#20316;&#20026;&#31995;&#32479;&#21442;&#25968;&#30340;&#25351;&#25968;&#34928;&#20943;&#12290;</title><link>http://arxiv.org/abs/2203.16711</link><description>&lt;p&gt;
&#23485;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#21160;&#21147;&#23398;&#30340;&#20998;&#26512;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Analytic theory for the dynamics of wide quantum neural networks. (arXiv:2203.16711v3 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.16711
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#35757;&#32451;&#35823;&#24046;&#30340;&#26799;&#24230;&#19979;&#38477;&#21160;&#24577;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#20998;&#26512;&#20844;&#24335;&#65292;&#21487;&#20197;&#25429;&#25417;&#21040;&#23485;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#25439;&#22833;&#20989;&#25968;&#30340;&#24179;&#22343;&#34892;&#20026;&#12290;&#25105;&#20204;&#39044;&#27979;&#24182;&#34920;&#24449;&#20102;&#38543;&#26426;&#37327;&#23376;&#30005;&#36335;&#27531;&#20313;&#35757;&#32451;&#35823;&#24046;&#20316;&#20026;&#31995;&#32479;&#21442;&#25968;&#30340;&#25351;&#25968;&#34928;&#20943;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#37327;&#23376;&#30005;&#36335;&#21487;&#29992;&#20316;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#26377;&#28508;&#21147;&#22312;&#35299;&#20915;&#23398;&#20064;&#38382;&#39064;&#26102;&#20248;&#20110;&#23427;&#20204;&#30340;&#32463;&#20856;&#23545;&#24212;&#29289;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#22823;&#37096;&#20998;&#20851;&#20110;&#23427;&#20204;&#22312;&#23454;&#38469;&#38382;&#39064;&#19978;&#34920;&#29616;&#30340;&#32467;&#26524;&#26159;&#21551;&#21457;&#24335;&#30340;&#12290;&#29305;&#21035;&#26159;&#65292;&#23545;&#20110;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#25910;&#25947;&#29575;&#36824;&#27809;&#26377;&#23436;&#20840;&#29702;&#35299;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20998;&#26512;&#26799;&#24230;&#19979;&#38477;&#30340;&#21160;&#24577;&#65292;&#30740;&#31350;&#19968;&#31867;&#21487;&#21464;&#37327;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#35823;&#24046;&#12290;&#25105;&#20204;&#23558;&#23485;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#23450;&#20041;&#20026;&#24102;&#26377;&#22823;&#37327;&#37327;&#23376;&#20301;&#21644;&#21487;&#21464;&#21442;&#25968;&#30340;&#21442;&#25968;&#21270;&#37327;&#23376;&#30005;&#36335;&#26497;&#38480;&#12290;&#25105;&#20204;&#28982;&#21518;&#21457;&#29616;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#20998;&#26512;&#20844;&#24335;&#65292;&#21487;&#20197;&#25429;&#25417;&#21040;&#23427;&#20204;&#25439;&#22833;&#20989;&#25968;&#30340;&#24179;&#22343;&#34892;&#20026;&#65292;&#24182;&#35752;&#35770;&#20102;&#25105;&#20204;&#30740;&#31350;&#30340;&#32467;&#26524;&#30340;&#21518;&#26524;&#12290;&#20363;&#22914;&#65292;&#23545;&#20110;&#38543;&#26426;&#37327;&#23376;&#30005;&#36335;&#65292;&#25105;&#20204;&#39044;&#27979;&#24182;&#34920;&#24449;&#20102;&#27531;&#20313;&#35757;&#32451;&#35823;&#24046;&#20316;&#20026;&#31995;&#32479;&#21442;&#25968;&#30340;&#25351;&#25968;&#34928;&#20943;&#12290;&#25105;&#20204;&#26368;&#32456;&#36890;&#36807;&#21508;&#31181;&#37327;&#23376;&#30005;&#36335;&#30340;&#25968;&#20540;&#27169;&#25311;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#20998;&#26512;&#29702;&#35770;&#65292;&#24182;&#23637;&#31034;&#20102;&#19982;&#25105;&#20204;&#30340;&#39044;&#27979;&#19968;&#33268;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameterized quantum circuits can be used as quantum neural networks and have the potential to outperform their classical counterparts when trained for addressing learning problems. To date, much of the results on their performance on practical problems are heuristic in nature. In particular, the convergence rate for the training of quantum neural networks is not fully understood. Here, we analyze the dynamics of gradient descent for the training error of a class of variational quantum machine learning models. We define wide quantum neural networks as parameterized quantum circuits in the limit of a large number of qubits and variational parameters. We then find a simple analytic formula that captures the average behavior of their loss function and discuss the consequences of our findings. For example, for random quantum circuits, we predict and characterize an exponential decay of the residual training error as a function of the parameters of the system. We finally validate our analy
&lt;/p&gt;</description></item><item><title>COWERAGE&#31639;&#27861;&#25552;&#20986;&#65292;&#36890;&#36807;&#35757;&#32451;&#38169;&#35823;&#29575;&#20445;&#35777;&#26679;&#26412;&#35206;&#30422;&#24230;&#65292;&#23454;&#29616;&#22312;&#33258;&#30417;&#30563;&#35821;&#38899;&#35782;&#21035;&#20013;&#39640;&#25928;&#24494;&#35843;&#12290;</title><link>http://arxiv.org/abs/2203.09829</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#35821;&#38899;&#35782;&#21035;&#20013;&#39640;&#25928;&#24494;&#35843;&#30340;&#20195;&#34920;&#24615;&#23376;&#38598;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Representative Subset Selection for Efficient Fine-Tuning in Self-Supervised Speech Recognition. (arXiv:2203.09829v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.09829
&lt;/p&gt;
&lt;p&gt;
COWERAGE&#31639;&#27861;&#25552;&#20986;&#65292;&#36890;&#36807;&#35757;&#32451;&#38169;&#35823;&#29575;&#20445;&#35777;&#26679;&#26412;&#35206;&#30422;&#24230;&#65292;&#23454;&#29616;&#22312;&#33258;&#30417;&#30563;&#35821;&#38899;&#35782;&#21035;&#20013;&#39640;&#25928;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#26631;&#27880;&#25968;&#25454;&#25165;&#33021;&#23398;&#20064;&#39640;&#20445;&#30495;&#30340;&#35821;&#38899;&#35782;&#21035;&#34920;&#24449;&#65292;&#36825;&#38656;&#35201;&#24456;&#22823;&#30340;&#35745;&#31639;&#37327;&#21644;&#26102;&#38388;&#25104;&#26412;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#20013;&#35782;&#21035;&#26368;&#20339;&#25968;&#25454;&#23376;&#38598;&#20197;&#23454;&#29616;&#39640;&#25928;&#24494;&#35843;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#35270;&#35273;&#20219;&#21153;&#20013;&#29992;&#20110;&#25277;&#26679;&#26368;&#20855;&#20449;&#24687;&#30340;&#20363;&#23376;&#30340;&#25968;&#25454;&#38598;&#20462;&#21098;&#31574;&#30053;&#19981;&#22914;&#38543;&#26426;&#23376;&#38598;&#36873;&#25321;&#22312;&#33258;&#30417;&#30563;&#35821;&#38899;&#35782;&#21035;&#20013;&#25928;&#26524;&#22909;&#12290;&#25105;&#20204;&#25552;&#20986;COWERAGE&#31639;&#27861;&#65292;&#20197;&#22312;&#33258;&#30417;&#30563;&#35821;&#38899;&#35782;&#21035;&#20013;&#36873;&#25321;&#20195;&#34920;&#24615;&#23376;&#38598;&#12290;COWERAGE&#22522;&#20110;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#22312;&#26089;&#26399;&#30340;&#35757;&#32451;&#36718;&#25968;&#20013;&#22522;&#20110;&#35757;&#32451;&#23383;&#38169;&#35823;&#29575;(WER)&#20445;&#35777;&#20363;&#23376;&#35206;&#30422;&#24230;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#27867;&#21270;&#34920;&#29616;&#12290;&#23545;TIMIT&#12289;Librispeech&#21644;LJSpeech&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;wav2vec 2.0&#21644;HuBERT&#27169;&#22411;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#20102;COWERAGE&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#20256;&#36882;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised speech recognition models require considerable labeled training data for learning high-fidelity representations for Automatic Speech Recognition (ASR) which is computationally demanding and time-consuming. We consider the task of identifying an optimal subset of data for efficient fine-tuning in self-supervised speech models for ASR. We discover that the dataset pruning strategies used in vision tasks for sampling the most informative examples do not perform better than random subset selection on fine-tuning self-supervised ASR. We then present the COWERAGE algorithm for representative subset selection in self-supervised ASR. COWERAGE is based on our finding that ensuring the coverage of examples based on training Word Error Rate (WER) in the early training epochs leads to better generalization performance. Extensive experiments with the wav2vec 2.0 and HuBERT model on TIMIT, Librispeech, and LJSpeech datasets show the effectiveness of COWERAGE and its transferability a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;FL&#20013;TEE&#30340;&#28431;&#27934;&#65292;&#24182;&#22312;TEE&#20013;&#24341;&#20837;Oblivious Memory Access&#65288;OMA&#65289;&#20197;&#20445;&#25252;&#20813;&#21463;&#31232;&#30095;&#21270;&#39118;&#38505;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;OLIVE&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#36890;&#20449;&#25928;&#29575;&#21644;&#27169;&#22411;&#31934;&#24230;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#23433;&#20840;&#32858;&#21512;&#21644;&#24046;&#20998;&#38544;&#31169;FL&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2202.07165</link><description>&lt;p&gt;
OLIVE: &#22522;&#20110;&#21487;&#20449;&#25191;&#34892;&#29615;&#22659;&#30340;&#38544;&#31169;&#20445;&#25252;&#32852;&#37030;&#23398;&#20064;&#38450;&#33539;&#31232;&#30095;&#24615;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
OLIVE: Oblivious Federated Learning on Trusted Execution Environment against the risk of sparsification. (arXiv:2202.07165v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.07165
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;FL&#20013;TEE&#30340;&#28431;&#27934;&#65292;&#24182;&#22312;TEE&#20013;&#24341;&#20837;Oblivious Memory Access&#65288;OMA&#65289;&#20197;&#20445;&#25252;&#20813;&#21463;&#31232;&#30095;&#21270;&#39118;&#38505;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;OLIVE&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#36890;&#20449;&#25928;&#29575;&#21644;&#27169;&#22411;&#31934;&#24230;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#23433;&#20840;&#32858;&#21512;&#21644;&#24046;&#20998;&#38544;&#31169;FL&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#21512;&#21487;&#20449;&#25191;&#34892;&#29615;&#22659;(TEE)&#30340;&#32852;&#37030;&#23398;&#20064;(FL)&#26159;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;FL&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#23398;&#26415;&#20851;&#27880;&#12290;&#22312;&#26381;&#21153;&#22120;&#31471;&#23454;&#29616;TEE&#21487;&#20197;&#20351;&#27599;&#36718;FL&#22312;&#19981;&#23558;&#23458;&#25143;&#31471;&#26799;&#24230;&#20449;&#24687;&#26292;&#38706;&#32473;&#19981;&#21487;&#20449;&#30340;&#26381;&#21153;&#22120;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#12290;&#36825;&#35299;&#20915;&#20102;&#29616;&#26377;&#23433;&#20840;&#32858;&#21512;&#26041;&#26696;&#20013;&#23384;&#22312;&#30340;&#21487;&#29992;&#24615;&#24046;&#36317;&#20197;&#21450;&#24046;&#20998;&#38544;&#31169;FL&#20013;&#30340;&#25928;&#29992;&#24046;&#36317;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#35299;&#20915;&#20351;&#29992;TEE&#30340;&#38382;&#39064;&#65292;&#38656;&#35201;&#32771;&#34385;&#26381;&#21153;&#22120;&#31471;TEE&#30340;&#28431;&#27934;&#65292;&#36825;&#22312;FL&#30340;&#32972;&#26223;&#19979;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30340;&#30740;&#31350;&#12290;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#25216;&#26415;&#36129;&#29486;&#26159;&#20998;&#26512;FL&#20013;TEE&#30340;&#28431;&#27934;&#21644;&#38450;&#24481;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#20869;&#23384;&#35775;&#38382;&#27169;&#24335;&#30340;&#27844;&#28431;&#65292;&#25581;&#31034;&#20102;&#31232;&#30095;&#26799;&#24230;&#30340;&#39118;&#38505;&#65292;&#31232;&#30095;&#26799;&#24230;&#36890;&#24120;&#29992;&#20110;&#22686;&#24378;&#36890;&#20449;&#25928;&#29575;&#21644;&#27169;&#22411;&#31934;&#24230;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#25512;&#29702;&#25915;&#20987;&#20197;&#20445;&#25252;&#20813;&#21463;&#31232;&#30095;&#21270;&#39118;&#38505;&#30340;&#24433;&#21709;&#65292;&#35813;&#25915;&#20987;&#20351;&#29992;TEE&#20013;&#30340;&#28151;&#28102;RAM&#24341;&#20837;&#20102;Oblivious Memory Access(OMA)&#12290;&#25105;&#20204;&#23545;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;OLIVE&#22312;&#36890;&#20449;&#25928;&#29575;&#21644;&#27169;&#22411;&#31934;&#24230;&#26041;&#38754;&#37117;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#23433;&#20840;&#32858;&#21512;&#21644;&#24046;&#20998;&#38544;&#31169;FL&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Combining Federated Learning (FL) with a Trusted Execution Environment (TEE) is a promising approach for realizing privacy-preserving FL, which has garnered significant academic attention in recent years. Implementing the TEE on the server side enables each round of FL to proceed without exposing the client's gradient information to untrusted servers. This addresses usability gaps in existing secure aggregation schemes as well as utility gaps in differentially private FL. However, to address the issue using a TEE, the vulnerabilities of server-side TEEs need to be considered -- this has not been sufficiently investigated in the context of FL. The main technical contribution of this study is the analysis of the vulnerabilities of TEE in FL and the defense. First, we theoretically analyze the leakage of memory access patterns, revealing the risk of sparsified gradients, which are commonly used in FL to enhance communication efficiency and model accuracy. Second, we devise an inference at
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#19987;&#23478;&#22686;&#24378;&#8221;&#30340;&#28151;&#21512;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#21487;&#20197;&#23558;&#20854;&#32435;&#20837;&#28151;&#21512;&#31995;&#32479;&#20197;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#20811;&#26381;&#28151;&#21512;&#27169;&#22411;&#24615;&#33021;&#20165;&#38480;&#20110;&#35757;&#32451;&#20998;&#24067;&#30340;&#38480;&#21046;&#12290;&#20316;&#32773;&#22312;&#19977;&#20010;&#25511;&#21046;&#23454;&#39564;&#20013;&#20174;&#24120;&#24494;&#20998;&#26041;&#31243;&#21644;&#20559;&#24494;&#20998;&#26041;&#31243;&#24314;&#27169;&#21160;&#24577;&#31995;&#32479;&#65292;&#24182;&#22312;&#30495;&#23454;&#21452;&#25670;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#35813;&#26041;&#27861;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2202.03881</link><description>&lt;p&gt;
&#24378;&#38887;&#30340;&#28151;&#21512;&#23398;&#20064;&#65306;&#19987;&#23478;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Robust Hybrid Learning With Expert Augmentation. (arXiv:2202.03881v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.03881
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#19987;&#23478;&#22686;&#24378;&#8221;&#30340;&#28151;&#21512;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#21487;&#20197;&#23558;&#20854;&#32435;&#20837;&#28151;&#21512;&#31995;&#32479;&#20197;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#20811;&#26381;&#28151;&#21512;&#27169;&#22411;&#24615;&#33021;&#20165;&#38480;&#20110;&#35757;&#32451;&#20998;&#24067;&#30340;&#38480;&#21046;&#12290;&#20316;&#32773;&#22312;&#19977;&#20010;&#25511;&#21046;&#23454;&#39564;&#20013;&#20174;&#24120;&#24494;&#20998;&#26041;&#31243;&#21644;&#20559;&#24494;&#20998;&#26041;&#31243;&#24314;&#27169;&#21160;&#24577;&#31995;&#32479;&#65292;&#24182;&#22312;&#30495;&#23454;&#21452;&#25670;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#35813;&#26041;&#27861;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#24314;&#27169;&#36890;&#36807;&#23558;&#19987;&#23478;&#27169;&#22411;&#19982;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#32452;&#20214;&#30456;&#32467;&#21512;&#65292;&#20943;&#23569;&#20102;&#19987;&#23478;&#27169;&#22411;&#30340;&#38169;&#35823;&#24314;&#27169;&#12290;&#19982;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#31867;&#20284;&#65292;&#28151;&#21512;&#27169;&#22411;&#30340;&#24615;&#33021;&#20445;&#35777;&#20165;&#38480;&#20110;&#35757;&#32451;&#20998;&#24067;&#12290;&#21033;&#29992;&#19987;&#23478;&#27169;&#22411;&#36890;&#24120;&#22312;&#35757;&#32451;&#22495;&#22806;&#20063;&#36866;&#29992;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#21517;&#20026;&#19987;&#23478;&#22686;&#24378;&#30340;&#28151;&#21512;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#20811;&#26381;&#20102;&#36825;&#20010;&#38480;&#21046;&#12290;&#22522;&#20110;&#28151;&#21512;&#24314;&#27169;&#30340;&#27010;&#29575;&#24418;&#24335;&#21270;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21487;&#20197;&#23558;&#19987;&#23478;&#22686;&#24378;&#32435;&#20837;&#29616;&#26377;&#30340;&#28151;&#21512;&#31995;&#32479;&#20197;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#22312;&#19977;&#20010;&#25511;&#21046;&#23454;&#39564;&#20013;&#20174;&#24120;&#24494;&#20998;&#26041;&#31243;&#21644;&#20559;&#24494;&#20998;&#26041;&#31243;&#24314;&#27169;&#21160;&#24577;&#31995;&#32479;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#19987;&#23478;&#22686;&#24378;&#22312;&#30495;&#23454;&#21452;&#25670;&#25968;&#25454;&#38598;&#19978;&#30340;&#28508;&#22312;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hybrid modelling reduces the misspecification of expert models by combining them with machine learning (ML) components learned from data. Similarly to many ML algorithms, hybrid model performance guarantees are limited to the training distribution. Leveraging the insight that the expert model is usually valid even outside the training domain, we overcome this limitation by introducing a hybrid data augmentation strategy termed \textit{expert augmentation}. Based on a probabilistic formalization of hybrid modelling, we demonstrate that expert augmentation, which can be incorporated into existing hybrid systems, improves generalization. We empirically validate the expert augmentation on three controlled experiments modelling dynamical systems with ordinary and partial differential equations. Finally, we assess the potential real-world applicability of expert augmentation on a dataset of a real double pendulum.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;&#29289;&#29702;&#19982;&#26426;&#22120;&#23398;&#20064;&#30340;&#23439;&#35266;&#20132;&#36890;&#29366;&#24577;&#20272;&#35745;&#26041;&#27861;&#65292;&#32467;&#21512;&#20132;&#36890;&#29289;&#29702;&#27169;&#22411;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292; &#20351;&#29992;&#26469;&#33258;&#20132;&#36890;&#20256;&#24863;&#22120;&#30340;&#20449;&#24687;&#20316;&#20026;&#36755;&#20837;&#20197;&#26500;&#24314;&#20934;&#30830;&#19988;&#23436;&#25972;&#30340;&#39640;&#36895;&#20844;&#36335;&#31995;&#32479;&#20132;&#36890;&#29366;&#24577;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2202.01888</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;&#29289;&#29702;&#19982;&#26426;&#22120;&#23398;&#20064;&#30340;&#23439;&#35266;&#20132;&#36890;&#29366;&#24577;&#20272;&#35745;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Hybrid Physics Machine Learning Approach for Macroscopic Traffic State Estimation. (arXiv:2202.01888v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.01888
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;&#29289;&#29702;&#19982;&#26426;&#22120;&#23398;&#20064;&#30340;&#23439;&#35266;&#20132;&#36890;&#29366;&#24577;&#20272;&#35745;&#26041;&#27861;&#65292;&#32467;&#21512;&#20132;&#36890;&#29289;&#29702;&#27169;&#22411;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292; &#20351;&#29992;&#26469;&#33258;&#20132;&#36890;&#20256;&#24863;&#22120;&#30340;&#20449;&#24687;&#20316;&#20026;&#36755;&#20837;&#20197;&#26500;&#24314;&#20934;&#30830;&#19988;&#23436;&#25972;&#30340;&#39640;&#36895;&#20844;&#36335;&#31995;&#32479;&#20132;&#36890;&#29366;&#24577;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23436;&#25972;&#30340;&#20132;&#36890;&#29366;&#24577;&#20449;&#24687;&#65288;&#20363;&#22914;&#27969;&#37327;&#12289;&#36895;&#24230;&#21644;&#23494;&#24230;&#65289;&#23545;&#20110;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#65288;ITS&#65289;&#30340;&#27491;&#24120;&#36816;&#34892;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22823;&#22810;&#25968;&#21306;&#22495;&#23433;&#35013;&#30340;&#20132;&#36890;&#26816;&#27979;&#22120;&#19981;&#36275;&#65292;&#22240;&#27492;&#21482;&#33021;&#37319;&#38598;&#21040;&#19981;&#23436;&#25972;&#30340;&#20132;&#36890;&#20449;&#24687;&#65292;&#36825;&#26159;&#26222;&#21450;ITS&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20132;&#36890;&#29366;&#24577;&#20272;&#35745;&#65288;TSE&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#20132;&#36890;&#29289;&#29702;&#27169;&#22411;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65288;&#22914;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#12289;&#38543;&#26426;&#26862;&#26519;&#65288;RF&#65289;&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#65289;&#24182;&#32467;&#21512;&#26469;&#33258;&#20132;&#36890;&#20256;&#24863;&#22120;&#30340;&#25968;&#37327;&#26377;&#38480;&#30340;&#20449;&#24687;&#20316;&#20026;&#36755;&#20837;&#65292;&#26500;&#24314;&#20934;&#30830;&#19988;&#23436;&#25972;&#30340;&#39640;&#36895;&#20844;&#36335;&#31995;&#32479;&#20132;&#36890;&#29366;&#24577;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Full-field traffic state information (i.e., flow, speed, and density) is critical for the successful operation of Intelligent Transportation Systems (ITS) on freeways. However, incomplete traffic information tends to be directly collected from traffic detectors that are insufficiently installed in most areas, which is a major obstacle to the popularization of ITS. To tackle this issue, this paper introduces an innovative traffic state estimation (TSE) framework that hybrid regression machine learning techniques (e.g., artificial neural network (ANN), random forest (RF), and support vector machine (SVM)) with a traffic physics model (e.g., second-order macroscopic traffic flow model) using limited information from traffic sensors as inputs to construct accurate and full-field estimated traffic state for freeway systems. To examine the effectiveness of the proposed TSE framework, this paper conducted empirical studies on a real-world data set collected from a stretch of I-15 freeway in S
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;&#27169;&#22411;&#30340;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#28151;&#21512;&#38382;&#39064;&#21644;&#24674;&#22797;&#27010;&#29575;&#20998;&#24067;&#65292;&#21487;&#20197;&#30830;&#23450;&#21407;&#26412;&#26080;&#27861;&#30830;&#23450;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2112.11602</link><description>&lt;p&gt;
&#36890;&#36807;&#28151;&#21512;&#27169;&#22411;&#36827;&#34892;&#26377;&#38480;&#20840;&#23616;&#28151;&#28102;&#30340;&#22240;&#26524;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Causal Inference Despite Limited Global Confounding via Mixture Models. (arXiv:2112.11602v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.11602
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;&#27169;&#22411;&#30340;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#28151;&#21512;&#38382;&#39064;&#21644;&#24674;&#22797;&#27010;&#29575;&#20998;&#24067;&#65292;&#21487;&#20197;&#30830;&#23450;&#21407;&#26412;&#26080;&#27861;&#30830;&#23450;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#32593;&#32476;&#26159;&#19968;&#32452;$n$&#20010;&#38543;&#26426;&#21464;&#37327;&#65288;&#22270;&#30340;&#39030;&#28857;&#65289;&#19978;&#30340;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAG&#65289;; &#36125;&#21494;&#26031;&#32593;&#32476;&#20998;&#24067;&#65288;BND&#65289;&#26159;&#22312;&#22270;&#19978;&#39532;&#23572;&#21487;&#22827;&#30340;&#38543;&#26426;&#21464;&#37327;&#30340;&#27010;&#29575;&#20998;&#24067;&#12290;&#36825;&#31181;&#27169;&#22411;&#30340;&#26377;&#38480;$k$-&#28151;&#21512;&#30001;&#19968;&#20010;&#26356;&#22823;&#30340;&#22270;&#24418;&#24335;&#21270;&#34920;&#31034;&#65292;&#35813;&#22270;&#20855;&#26377;&#19968;&#20010;&#39069;&#22806;&#30340;&#8220;&#38544;&#34255;&#8221;&#65288;&#25110;&#8220;&#28508;&#22312;&#8221;&#65289;&#38543;&#26426;&#21464;&#37327;$U$&#65292;&#20854;&#33539;&#22260;&#20026;$\{1,\ldots,k\}$&#65292;&#24182;&#19988;$U$&#21040;&#27599;&#20010;&#20854;&#20182;&#39030;&#28857;&#37117;&#26377;&#19968;&#20010;&#26377;&#21521;&#36793;&#12290;&#36825;&#31181;&#31867;&#22411;&#30340;&#27169;&#22411;&#22312;&#22240;&#26524;&#25512;&#26029;&#20013;&#26159;&#22522;&#26412;&#30340;&#65292;&#20854;&#20013;$U$&#27169;&#25311;&#20102;&#22810;&#20010;&#32676;&#20307;&#30340;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#25928;&#24212;&#65292;&#20351;&#24471;&#21487;&#35266;&#23519;&#30340;DAG&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#21464;&#24471;&#27169;&#31946;&#19981;&#28165;&#12290;&#36890;&#36807;&#35299;&#20915;&#28151;&#21512;&#38382;&#39064;&#24182;&#24674;&#22797;$U$&#19978;&#30340;&#32852;&#21512;&#27010;&#29575;&#20998;&#24067;&#65292;&#20256;&#32479;&#19978;&#26080;&#27861;&#30830;&#23450;&#30340;&#22240;&#26524;&#20851;&#31995;&#21464;&#24471;&#21487;&#30830;&#23450;&#12290;&#36890;&#36807;&#23558;&#20854;&#32422;&#21270;&#20026;&#26356;&#20026;&#30740;&#31350;&#30340;&#8220;&#31354;&#8221;&#22270;&#20013;&#30340;&#8220;&#20056;&#31215;&#8221;&#24773;&#20917;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#23398;&#20064;&#38750;&#31354;DAG&#30340;&#28151;&#21512;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Bayesian Network is a directed acyclic graph (DAG) on a set of $n$ random variables (the vertices); a Bayesian Network Distribution (BND) is a probability distribution on the random variables that is Markovian on the graph. A finite $k$-mixture of such models is graphically represented by a larger graph which has an additional "hidden" (or "latent") random variable $U$, ranging in $\{1,\ldots,k\}$, and a directed edge from $U$ to every other vertex. Models of this type are fundamental to causal inference, where $U$ models an unobserved confounding effect of multiple populations, obscuring the causal relationships in the observable DAG. By solving the mixture problem and recovering the joint probability distribution on $U$, traditionally unidentifiable causal relationships become identifiable. Using a reduction to the more well-studied "product" case on empty graphs, we give the first algorithm to learn mixtures of non-empty DAGs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#36890;&#36807;&#19987;&#23478;&#24341;&#23548;&#30340;&#23545;&#31216;&#26816;&#27979;&#31639;&#27861;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#38024;&#23545;&#38750;&#30830;&#23450;&#24615;MDP&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#26816;&#27979;&#31639;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#19968;&#32452;&#22522;&#20934;&#20219;&#21153;&#19978;&#26126;&#26174;&#25552;&#39640;&#20102;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2112.09943</link><description>&lt;p&gt;
&#36890;&#36807;&#19987;&#23478;&#24341;&#23548;&#30340;&#23545;&#31216;&#26816;&#27979;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#20197;&#25552;&#39640;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation through Expert-guided Symmetry Detection to Improve Performance in Offline Reinforcement Learning. (arXiv:2112.09943v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.09943
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#36890;&#36807;&#19987;&#23478;&#24341;&#23548;&#30340;&#23545;&#31216;&#26816;&#27979;&#31639;&#27861;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#38024;&#23545;&#38750;&#30830;&#23450;&#24615;MDP&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#26816;&#27979;&#31639;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#19968;&#32452;&#22522;&#20934;&#20219;&#21153;&#19978;&#26126;&#26174;&#25552;&#39640;&#20102;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#30830;&#23450;&#24615;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#30340;&#31163;&#32447;&#21160;&#24577;&#27169;&#22411;&#20272;&#35745;&#26159;&#19968;&#39033;&#38750;&#24120;&#22256;&#38590;&#30340;&#20219;&#21153;&#65292;&#23427;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#23398;&#20064;&#38454;&#27573;&#21487;&#29992;&#30340;&#25968;&#25454;&#12290;&#26377;&#26102;&#65292;&#27169;&#22411;&#30340;&#21160;&#24577;&#29305;&#24615;&#19982;&#24403;&#21069;&#29366;&#24577;&#21644;&#21160;&#20316;&#30340;&#26576;&#20123;&#21464;&#25442;&#26159;&#19981;&#21464;&#30340;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#19968;&#31181;&#22522;&#20110;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#30340;&#19987;&#23478;&#24341;&#23548;&#27969;&#27700;&#32447;&#65292;&#22914;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24402;&#19968;&#21270;&#27969;&#65292;&#21487;&#26377;&#25928;&#26816;&#27979;&#30830;&#23450;&#24615;&#29615;&#22659;&#20013;&#30340;&#36825;&#31181;&#32467;&#26500;&#65292;&#21253;&#25324;&#31867;&#21035;&#21644;&#36830;&#32493;&#20540;&#12290;&#25152;&#33719;&#24471;&#30340;&#30693;&#35782;&#21487;&#20197;&#34987;&#21033;&#29992;&#26469;&#22686;&#24378;&#21407;&#22987;&#25968;&#25454;&#38598;&#65292;&#26368;&#32456;&#23548;&#33268;&#30495;&#23454;&#27169;&#22411;&#21644;&#23398;&#20064;&#27169;&#22411;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#20943;&#23569;&#12290;&#36825;&#31181;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#21021;&#27493;&#36807;&#31243;&#65292;&#22312;&#37319;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26550;&#26500;&#20043;&#21069;&#25191;&#34892;&#65292;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#31181;&#33539;&#20363;&#25193;&#23637;&#21040;&#35299;&#20915;&#38750;&#30830;&#23450;&#24615;MDP&#65292;&#29305;&#21035;&#26159;1&#65289;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#26816;&#27979;&#31639;&#27861;&#26469;&#35782;&#21035;&#21644;&#21033;&#29992;&#23545;&#31216;&#24615;&#65292;2&#65289;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#19968;&#32452;&#22522;&#20934;&#20219;&#21153;&#19978;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline estimation of the dynamical model of a Markov Decision Process (MDP) is a non-trivial task that greatly depends on the data available in the learning phase. Sometimes the dynamics of the model is invariant with respect to some transformations of the current state and action. Recent works showed that an expert-guided pipeline relying on Density Estimation methods as Deep Neural Network based Normalizing Flows effectively detects this structure in deterministic environments, both categorical and continuous-valued. The acquired knowledge can be exploited to augment the original data set, leading eventually to a reduction in the distributional shift between the true and the learned model. Such data augmentation technique can be exploited as a preliminary process to be executed before adopting an Offline Reinforcement Learning architecture, increasing its performance. In this work we extend the paradigm to also tackle non-deterministic MDPs, in particular, 1) we propose a detection 
&lt;/p&gt;</description></item><item><title>MDPFuzz&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#27979;&#35797;&#35299;&#20915;MDP&#30340;&#27169;&#22411;&#30340;&#40657;&#30418;&#27169;&#31946;&#27979;&#35797;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#26816;&#26597;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#36827;&#20837;&#24322;&#24120;&#21361;&#38505;&#29366;&#24577;&#26469;&#29983;&#25104;&#27979;&#35797;&#39044;&#35328;&#65292;&#24182;&#20351;&#29992;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#21450;&#21160;&#24577;&#26399;&#26395;&#26368;&#22823;&#21270;&#30340;&#26041;&#27861;&#26469;&#37327;&#21270;&#29366;&#24577;&#24207;&#21015;&#30340;&#8220;&#26032;&#40092;&#24230;&#8221;&#20197;&#20915;&#23450;&#20445;&#30041;&#21738;&#20010;&#31361;&#21464;&#29366;&#24577;&#12290;</title><link>http://arxiv.org/abs/2112.02807</link><description>&lt;p&gt;
MDPFuzz: &#29992;&#20110;&#27979;&#35797;&#35299;&#20915;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#27169;&#22411;&#30340;&#40657;&#30418;&#27169;&#31946;&#27979;&#35797;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
MDPFuzz: Testing Models Solving Markov Decision Processes. (arXiv:2112.02807v4 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.02807
&lt;/p&gt;
&lt;p&gt;
MDPFuzz&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#27979;&#35797;&#35299;&#20915;MDP&#30340;&#27169;&#22411;&#30340;&#40657;&#30418;&#27169;&#31946;&#27979;&#35797;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#26816;&#26597;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#36827;&#20837;&#24322;&#24120;&#21361;&#38505;&#29366;&#24577;&#26469;&#29983;&#25104;&#27979;&#35797;&#39044;&#35328;&#65292;&#24182;&#20351;&#29992;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#21450;&#21160;&#24577;&#26399;&#26395;&#26368;&#22823;&#21270;&#30340;&#26041;&#27861;&#26469;&#37327;&#21270;&#29366;&#24577;&#24207;&#21015;&#30340;&#8220;&#26032;&#40092;&#24230;&#8221;&#20197;&#20915;&#23450;&#20445;&#30041;&#21738;&#20010;&#31361;&#21464;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#25968;&#23398;&#26694;&#26550;&#65292;&#29992;&#20110;&#24314;&#27169;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#65292;&#20854;&#20013;&#35768;&#22810;&#38382;&#39064;&#23545;&#20110;&#23433;&#20840;&#21644;&#21487;&#38752;&#24615;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20363;&#22914;&#33258;&#21160;&#39550;&#39542;&#21644;&#26426;&#22120;&#20154;&#25511;&#21046;&#12290;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#24555;&#36895;&#21457;&#23637;&#24050;&#32463;&#21019;&#36896;&#20986;&#20102;&#35299;&#20915;MDP&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#20363;&#22914;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#12289;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#21644;&#27169;&#20223;&#23398;&#20064;&#65288;IL&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#21463;&#27426;&#36814;&#30340;&#35299;&#20915;MDP&#30340;&#27169;&#22411;&#26082;&#27809;&#26377;&#24471;&#21040;&#24443;&#24213;&#30340;&#27979;&#35797;&#65292;&#20063;&#19981;&#26159;&#20005;&#26684;&#21487;&#38752;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;MDPFuzz&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#27169;&#22411;&#35299;&#20915;MDP&#30340;&#40657;&#30418;&#27169;&#31946;&#27979;&#35797;&#26694;&#26550;&#12290;MDPFuzz&#36890;&#36807;&#26816;&#26597;&#30446;&#26631;&#27169;&#22411;&#26159;&#21542;&#36827;&#20837;&#24322;&#24120;&#21644;&#21361;&#38505;&#29366;&#24577;&#26469;&#24418;&#25104;&#27979;&#35797;&#39044;&#35328;&#12290;&#22312;&#27169;&#31946;&#21270;&#36807;&#31243;&#20013;&#65292;MDPFuzz&#36890;&#36807;&#27979;&#37327;&#26159;&#21542;&#21487;&#20197;&#20943;&#23569;&#32047;&#31215;&#22870;&#21169;&#25110;&#24418;&#25104;&#26032;&#30340;&#29366;&#24577;&#24207;&#21015;&#26469;&#20915;&#23450;&#20445;&#30041;&#21738;&#20010;&#31361;&#21464;&#29366;&#24577;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#26377;&#25928;&#30340;&#25216;&#26415;&#26469;&#20351;&#29992;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65288;GMMs&#65289;&#21644;&#21160;&#24577;&#26399;&#26395;&#26368;&#22823;&#21270;&#37327;&#21270;&#29366;&#24577;&#24207;&#21015;&#30340;&#8220;&#26032;&#40092;&#24230;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Markov decision process (MDP) provides a mathematical framework for modeling sequential decision-making problems, many of which are crucial to security and safety, such as autonomous driving and robot control. The rapid development of artificial intelligence research has created efficient methods for solving MDPs, such as deep neural networks (DNNs), reinforcement learning (RL), and imitation learning (IL). However, these popular models solving MDPs are neither thoroughly tested nor rigorously reliable.  We present MDPFuzz, the first blackbox fuzz testing framework for models solving MDPs. MDPFuzz forms testing oracles by checking whether the target model enters abnormal and dangerous states. During fuzzing, MDPFuzz decides which mutated state to retain by measuring if it can reduce cumulative rewards or form a new state sequence. We design efficient techniques to quantify the "freshness" of a state sequence using Gaussian mixture models (GMMs) and dynamic expectation-maximization 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; BASIC &#30340;&#32508;&#21512;&#32553;&#25918;&#26041;&#27861;&#65292;&#22312;&#38646;&#26679;&#26412;&#36801;&#31227;&#23398;&#20064;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;85.7%&#30340;&#39640;&#20934;&#30830;&#29575;&#65292;&#24182;&#22312;&#31283;&#20581;&#24615;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20123;&#32467;&#26524;&#65292;&#35813;&#26041;&#27861;&#22312;&#25968;&#25454;&#22823;&#23567;&#12289;&#27169;&#22411;&#22823;&#23567;&#21644;&#25209;&#37327;&#22823;&#23567;&#19977;&#20010;&#32500;&#24230;&#19978;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#36827;&#34892;&#20102;&#25918;&#22823;&#12290;</title><link>http://arxiv.org/abs/2111.10050</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#36801;&#31227;&#23398;&#20064;&#20013;&#30340;&#32508;&#21512;&#32553;&#25918;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Combined Scaling for Zero-shot Transfer Learning. (arXiv:2111.10050v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.10050
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; BASIC &#30340;&#32508;&#21512;&#32553;&#25918;&#26041;&#27861;&#65292;&#22312;&#38646;&#26679;&#26412;&#36801;&#31227;&#23398;&#20064;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;85.7%&#30340;&#39640;&#20934;&#30830;&#29575;&#65292;&#24182;&#22312;&#31283;&#20581;&#24615;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20123;&#32467;&#26524;&#65292;&#35813;&#26041;&#27861;&#22312;&#25968;&#25454;&#22823;&#23567;&#12289;&#27169;&#22411;&#22823;&#23567;&#21644;&#25209;&#37327;&#22823;&#23567;&#19977;&#20010;&#32500;&#24230;&#19978;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#36827;&#34892;&#20102;&#25918;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#32553;&#25918;&#26041;&#27861;&#65288;&#31216;&#20026; BASIC&#65289;&#65292;&#22312;&#19981;&#23398;&#20064;&#20219;&#20309;&#26631;&#35760;&#30340; ImageNet &#31034;&#20363;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#26041;&#27861;&#22312; ImageNet ILSVRC-2012 &#39564;&#35777;&#38598;&#19978;&#23454;&#29616;&#20102;85.7%&#30340; top-1 &#20934;&#30830;&#29575;&#12290;&#35813;&#20934;&#30830;&#29575;&#36229;&#36807;&#20102;&#26368;&#20339;&#21457;&#24067;&#30340;&#31867;&#20284;&#27169;&#22411;&#65288;CLIP &#21644; ALIGN&#65289;9.3%&#12290;&#25105;&#20204;&#30340; BASIC &#27169;&#22411;&#36824;&#22312;&#31283;&#20581;&#24615;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#20363;&#22914;&#65292;&#22312; 5 &#20010;&#20855;&#26377;&#33258;&#28982;&#20998;&#24067;&#20559;&#31227;&#30340;&#27979;&#35797;&#38598;&#65288;&#20363;&#22914; ImageNet-{A,R,V2, Sketch} &#21644; ObjectNet&#65289;&#19978;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23454;&#29616;&#20102;84.3% &#30340; top-1 &#24179;&#22343;&#20934;&#30830;&#29575;&#65292;&#21482;&#26377;&#19968;&#20010;&#23567;&#23567;&#30340;&#36300;&#33853;&#65292;&#19982;&#20854;&#21407;&#22987;&#30340; ImageNet &#20934;&#30830;&#29575;&#30456;&#27604;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20123;&#32467;&#26524;&#65292;&#25105;&#20204;&#22312;&#19977;&#20010;&#32500;&#24230;&#19978;&#25918;&#22823;&#20102; CLIP &#21644; ALIGN &#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65306;&#25968;&#25454;&#22823;&#23567;&#65292;&#27169;&#22411;&#22823;&#23567;&#21644;&#25209;&#37327;&#22823;&#23567;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#25317;&#26377; 66 &#20159;&#20010;&#24102;&#26377;&#22122;&#22768;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#65292;&#27604; ALIGN &#22823; 4 &#20493;&#65292;&#27604; CLIP &#22823; 16 &#20493;&#12290;&#25105;&#20204;&#26368;&#22823;&#30340;&#27169;&#22411;&#26377; 30 &#20159;&#20010;&#26435;&#37325;&#65292;&#21442;&#25968;&#27604; ALIGN &#21644; CLIP &#22810;&#20986; 3.75 &#20493;&#65292;FLOPs &#22810;&#20986; 8 &#20493;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30340;&#25209;&#22788;&#29702;&#22823;&#23567;&#20026; 65536&#65292;&#27604; CLIP &#22810; 2 &#20493;&#65292;&#27604; ALIGN &#22810; 4 &#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a combined scaling method - named BASIC - that achieves 85.7% top-1 accuracy on the ImageNet ILSVRC-2012 validation set without learning from any labeled ImageNet example. This accuracy surpasses best published similar models - CLIP and ALIGN - by 9.3%. Our BASIC model also shows significant improvements in robustness benchmarks. For instance, on 5 test sets with natural distribution shifts such as ImageNet-{A,R,V2,Sketch} and ObjectNet, our model achieves 84.3% top-1 average accuracy, only a small drop from its original ImageNet accuracy. To achieve these results, we scale up the contrastive learning framework of CLIP and ALIGN in three dimensions: data size, model size, and batch size. Our dataset has 6.6B noisy image-text pairs, which is 4x larger than ALIGN, and 16x larger than CLIP. Our largest model has 3B weights, which is 3.75x larger in parameters and 8x larger in FLOPs than ALIGN and CLIP. Finally, our batch size is 65536 which is 2x more than CLIP and 4x more than
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;&#29366;&#24577;&#35775;&#38382;&#27169;&#24335;&#23545;&#29366;&#24577;&#36827;&#34892;&#32534;&#30721;&#30340;&#32487;&#25215;&#34920;&#31034;&#65288;SR&#65289;&#21487;&#20197;&#34987;&#30475;&#20316;&#21457;&#29616;&#21644;&#20351;&#29992;&#26102;&#38388;&#25277;&#35937;&#30340;&#33258;&#28982;&#22522;&#36136;&#65292;&#24182;&#19988;&#21487;&#20197;&#29992;&#20110;&#21457;&#29616;&#26377;&#21161;&#20110;&#36827;&#34892;&#26242;&#26102;&#25193;&#23637;&#25506;&#32034;&#25110;&#35268;&#21010;&#30340;&#36873;&#39033;&#12290;</title><link>http://arxiv.org/abs/2110.05740</link><description>&lt;p&gt;
&#22522;&#20110;&#32487;&#25215;&#34920;&#31034;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26102;&#38388;&#25277;&#35937;
&lt;/p&gt;
&lt;p&gt;
Temporal Abstraction in Reinforcement Learning with the Successor Representation. (arXiv:2110.05740v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.05740
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;&#29366;&#24577;&#35775;&#38382;&#27169;&#24335;&#23545;&#29366;&#24577;&#36827;&#34892;&#32534;&#30721;&#30340;&#32487;&#25215;&#34920;&#31034;&#65288;SR&#65289;&#21487;&#20197;&#34987;&#30475;&#20316;&#21457;&#29616;&#21644;&#20351;&#29992;&#26102;&#38388;&#25277;&#35937;&#30340;&#33258;&#28982;&#22522;&#36136;&#65292;&#24182;&#19988;&#21487;&#20197;&#29992;&#20110;&#21457;&#29616;&#26377;&#21161;&#20110;&#36827;&#34892;&#26242;&#26102;&#25193;&#23637;&#25506;&#32034;&#25110;&#35268;&#21010;&#30340;&#36873;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#23618;&#27425;&#30340;&#26102;&#38388;&#25277;&#35937;&#26159;&#26234;&#33021;&#30340;&#19968;&#20010;&#37325;&#35201;&#29305;&#24449;&#12290;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#36825;&#36890;&#24120;&#36890;&#36807;&#34987;&#31216;&#20026;&#36873;&#39033;&#30340;&#26242;&#26102;&#24615;&#34892;&#21160;&#26469;&#24314;&#27169;&#12290;&#36873;&#39033;&#20801;&#35768;&#26234;&#33021;&#20307;&#22312;&#29615;&#22659;&#20013;&#36827;&#34892;&#39044;&#27979;&#24182;&#22312;&#19981;&#21516;&#30340;&#25277;&#35937;&#23618;&#27425;&#19978;&#25805;&#20316;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#36873;&#39033;&#26694;&#26550;&#30340;&#26041;&#27861;&#36890;&#24120;&#20174;&#24050;&#30693;&#30340;&#21512;&#29702;&#36873;&#39033;&#38598;&#24320;&#22987;&#20551;&#35774;&#12290;&#24403;&#27809;&#26377;&#36825;&#31181;&#24773;&#20917;&#26102;&#65292;&#23545;&#20110;&#24212;&#35813;&#32771;&#34385;&#21738;&#20123;&#36873;&#39033;&#65292;&#27809;&#26377;&#26126;&#30830;&#30340;&#31572;&#26696;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#22522;&#20110;&#29366;&#24577;&#35775;&#38382;&#27169;&#24335;&#23545;&#29366;&#24577;&#36827;&#34892;&#32534;&#30721;&#30340;&#32487;&#25215;&#34920;&#31034;&#65288;SR&#65289;&#21487;&#20197;&#34987;&#35270;&#20026;&#21457;&#29616;&#21644;&#20351;&#29992;&#26102;&#38388;&#25277;&#35937;&#30340;&#33258;&#28982;&#22522;&#36136;&#12290;&#20026;&#20102;&#25903;&#25345;&#25105;&#20204;&#30340;&#35266;&#28857;&#65292;&#25105;&#20204;&#20174;&#36817;&#26399;&#30740;&#31350;&#30340;&#20840;&#23616;&#35270;&#35282;&#20986;&#21457;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;SR&#21457;&#29616;&#26377;&#21161;&#20110;&#36827;&#34892;&#26242;&#26102;&#25193;&#23637;&#25506;&#32034;&#25110;&#35268;&#21010;&#30340;&#36873;&#39033;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#32467;&#26524;&#32467;&#21512;&#21040;&#19968;&#20010;&#23436;&#25972;&#30340;&#23454;&#39564;&#20013;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#29615;&#22659;&#19979;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reasoning at multiple levels of temporal abstraction is one of the key attributes of intelligence. In reinforcement learning, this is often modeled through temporally extended courses of actions called options. Options allow agents to make predictions and to operate at different levels of abstraction within an environment. Nevertheless, approaches based on the options framework often start with the assumption that a reasonable set of options is known beforehand. When this is not the case, there are no definitive answers for which options one should consider. In this paper, we argue that the successor representation (SR), which encodes states based on the pattern of state visitation that follows them, can be seen as a natural substrate for the discovery and use of temporal abstractions. To support our claim, we take a big picture view of recent results, showing how the SR can be used to discover options that facilitate either temporally-extended exploration or planning. We cast these re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#26354;&#29575;&#24863;&#30693;&#30340;&#26080;&#23548;&#25968;&#20248;&#21270;&#31639;&#27861;&#8212;&#8212;&#26354;&#29575;&#24863;&#30693;&#38543;&#26426;&#25628;&#32034;&#65288;CARS&#65289;&#65292;&#35813;&#31639;&#27861;&#20351;&#29992;&#19968;&#38454;&#21644;&#20108;&#38454;&#26377;&#38480;&#24046;&#20998;&#36924;&#36817;&#26469;&#35745;&#31639;&#20505;&#36873;&#27493;&#38271;&#65292;&#35777;&#26126;&#20102;&#20854;&#23545;&#20110;&#24378;&#20984;&#30446;&#26631;&#20989;&#25968;&#30340;&#32447;&#24615;&#25910;&#25947;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#26080;&#38656;&#21464;&#37327;&#36716;&#25442;&#30340;&#31435;&#26041;&#27491;&#21017;&#21270;&#21464;&#20307;CARS-CR&#65292;&#21487;&#20197;&#20197;O(k^-1)&#30340;&#36895;&#29575;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2109.13391</link><description>&lt;p&gt;
&#26354;&#29575;&#24863;&#30693;&#30340;&#26080;&#23548;&#25968;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Curvature-Aware Derivative-Free Optimization. (arXiv:2109.13391v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.13391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#26354;&#29575;&#24863;&#30693;&#30340;&#26080;&#23548;&#25968;&#20248;&#21270;&#31639;&#27861;&#8212;&#8212;&#26354;&#29575;&#24863;&#30693;&#38543;&#26426;&#25628;&#32034;&#65288;CARS&#65289;&#65292;&#35813;&#31639;&#27861;&#20351;&#29992;&#19968;&#38454;&#21644;&#20108;&#38454;&#26377;&#38480;&#24046;&#20998;&#36924;&#36817;&#26469;&#35745;&#31639;&#20505;&#36873;&#27493;&#38271;&#65292;&#35777;&#26126;&#20102;&#20854;&#23545;&#20110;&#24378;&#20984;&#30446;&#26631;&#20989;&#25968;&#30340;&#32447;&#24615;&#25910;&#25947;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#26080;&#38656;&#21464;&#37327;&#36716;&#25442;&#30340;&#31435;&#26041;&#27491;&#21017;&#21270;&#21464;&#20307;CARS-CR&#65292;&#21487;&#20197;&#20197;O(k^-1)&#30340;&#36895;&#29575;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#26080;&#23548;&#25968;&#20248;&#21270;&#38382;&#39064;&#65292;&#21363;&#20165;&#36890;&#36807;&#20989;&#25968;&#35780;&#20215;&#32780;&#38750;&#26799;&#24230;&#25110;&#26041;&#21521;&#23548;&#25968;&#26469;&#26368;&#23567;&#21270;&#19968;&#20010;&#20989;&#25968;&#12290;&#32463;&#20856;&#30340;&#26080;&#23548;&#25968;&#20248;&#21270;&#31639;&#27861;&#65292;&#22914;Nelder-Mead&#21644;&#30452;&#25509;&#25628;&#32034;&#31867;&#20284;&#20110;&#26799;&#24230;&#19979;&#38477;&#65292;&#23545;&#20110;&#39640;&#32500;&#38382;&#39064;&#30340;&#21487;&#25193;&#23637;&#24615;&#26377;&#38480;&#12290;&#38543;&#30528;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;&#38656;&#27714;&#65292;&#38646;&#38454;&#26041;&#27861;&#36234;&#26469;&#36234;&#21463;&#21040;&#38738;&#30544;&#65292;&#26412;&#25991;&#37325;&#28857;&#35752;&#35770;&#36825;&#20123;&#26041;&#27861;&#20013;&#27493;&#38271;&#945;k&#30340;&#36873;&#25321;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21517;&#20026;&#26354;&#29575;&#24863;&#30693;&#38543;&#26426;&#25628;&#32034;&#65288;CARS&#65289;&#65292;&#20351;&#29992;&#19968;&#38454;&#21644;&#20108;&#38454;&#26377;&#38480;&#24046;&#20998;&#36924;&#36817;&#26469;&#35745;&#31639;&#20505;&#36873;&#27493;&#38271;&#945;+&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#23545;&#20110;&#24378;&#20984;&#30446;&#26631;&#20989;&#25968;&#65292;&#21482;&#35201;&#25628;&#32034;&#26041;&#21521;&#26469;&#33258;&#28385;&#36275;&#38750;&#24120;&#28201;&#21644;&#26465;&#20214;&#30340;&#20998;&#24067;&#65292;CARS&#23601;&#21487;&#20197;&#32447;&#24615;&#25910;&#25947;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;CARS-CR&#65292;&#23427;&#26159;CARS&#30340;&#31435;&#26041;&#27491;&#21017;&#21270;&#21464;&#20307;&#65292;&#26080;&#38656;&#36827;&#34892;&#21464;&#37327;&#36716;&#25442;&#21363;&#21487;&#20197;O(k^-1)&#30340;&#36895;&#29575;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper discusses derivative-free optimization (DFO), which involves minimizing a function without access to gradients or directional derivatives, only function evaluations. Classical DFO methods, which mimic gradient-based methods, such as Nelder-Mead and direct search have limited scalability for high-dimensional problems. Zeroth-order methods have been gaining popularity due to the demands of large-scale machine learning applications, and the paper focuses on the selection of the step size $\alpha_k$ in these methods. The proposed approach, called Curvature-Aware Random Search (CARS), uses first- and second-order finite difference approximations to compute a candidate $\alpha_{+}$. We prove that for strongly convex objective functions, CARS converges linearly provided that the search direction is drawn from a distribution satisfying very mild conditions. We also present a Cubic Regularized variant of CARS, named CARS-CR, which converges in a rate of $\mathcal{O}(k^{-1})$ without t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31227;&#21160;&#35774;&#22791;&#21644;&#20998;&#24067;&#24335;&#32593;&#32476;&#20013;&#23454;&#29616;&#20998;&#24067;&#24335;&#35757;&#32451;&#21644;&#25512;&#26029;&#30340;&#31639;&#27861;&#21644;&#26550;&#26500;&#65292;&#24182;&#23454;&#29616;&#20102;&#25512;&#26029;&#30340;&#20256;&#25773;&#21644;&#34701;&#21512;&#12290;&#19982;&#29616;&#26377;&#25216;&#26415;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2107.03433</link><description>&lt;p&gt;
&#32593;&#32476;&#20869;&#23398;&#20064;&#65306;&#20998;&#24067;&#24335;&#35757;&#32451;&#21644;&#32593;&#32476;&#20013;&#30340;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
In-Network Learning: Distributed Training and Inference in Networks. (arXiv:2107.03433v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.03433
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31227;&#21160;&#35774;&#22791;&#21644;&#20998;&#24067;&#24335;&#32593;&#32476;&#20013;&#23454;&#29616;&#20998;&#24067;&#24335;&#35757;&#32451;&#21644;&#25512;&#26029;&#30340;&#31639;&#27861;&#21644;&#26550;&#26500;&#65292;&#24182;&#23454;&#29616;&#20102;&#25512;&#26029;&#30340;&#20256;&#25773;&#21644;&#34701;&#21512;&#12290;&#19982;&#29616;&#26377;&#25216;&#26415;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#25104;&#21151;&#20351;&#24471;&#31227;&#21160;&#35774;&#22791;&#21644;&#26080;&#32447;&#32593;&#32476;&#33021;&#22815;&#23454;&#29616;&#37325;&#35201;&#30340;&#26032;&#26381;&#21153;&#65292;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#21644;&#22788;&#29702;&#33021;&#21147;&#22312;&#26080;&#32447;&#32593;&#32476;&#20013;&#39640;&#24230;&#20998;&#24067;&#65292;&#36825;&#20063;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#31639;&#27861;&#21644;&#26550;&#26500;&#65292;&#21033;&#29992;&#22810;&#20010;&#25968;&#25454;&#27969;&#21644;&#22788;&#29702;&#21333;&#20803;&#65292;&#19981;&#20165;&#22312;&#35757;&#32451;&#38454;&#27573;&#32780;&#19988;&#22312;&#25512;&#26029;&#38454;&#27573;&#36827;&#34892;&#25512;&#26029;&#20256;&#25773;&#21644;&#34701;&#21512;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25552;&#20986;&#26041;&#27861;&#30340;&#35774;&#35745;&#20934;&#21017;&#21450;&#20854;&#23545;&#24102;&#23485;&#30340;&#35201;&#27714;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#22312;&#20856;&#22411;&#30340;&#26080;&#32447;&#30005;&#25509;&#20837;&#20013;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#23454;&#29616;&#26041;&#38754;&#65292;&#24182;&#25552;&#20379;&#20102;&#23454;&#39564;&#35777;&#26126;&#20102;&#26412;&#26041;&#27861;&#30456;&#27604;&#29616;&#26377;&#25216;&#26415;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is widely perceived that leveraging the success of modern machine learning techniques to mobile devices and wireless networks has the potential of enabling important new services. This, however, poses significant challenges, essentially due to that both data and processing power are highly distributed in a wireless network. In this paper, we develop a learning algorithm and an architecture that make use of multiple data streams and processing units, not only during the training phase but also during the inference phase. In particular, the analysis reveals how inference propagates and fuses across a network. We study the design criterion of our proposed method and its bandwidth requirements. Also, we discuss implementation aspects using neural networks in typical wireless radio access; and provide experiments that illustrate benefits over state-of-the-art techniques.
&lt;/p&gt;</description></item><item><title>GitTables&#26159;&#19968;&#20010;&#20174;GitHub&#20013;&#25552;&#21462;&#30340;&#22823;&#22411;&#35821;&#26009;&#24211;&#65292;&#21253;&#21547;1M&#20010;&#20851;&#31995;&#34920;&#26684;&#65292;&#21487;&#20197;&#26377;&#25928;&#35757;&#32451;&#21644;&#35780;&#20272;&#39640;&#23481;&#37327;&#27169;&#22411;&#65292;&#20197;&#25913;&#36827;&#20851;&#31995;&#34920;&#26684;&#20219;&#21153;</title><link>http://arxiv.org/abs/2106.07258</link><description>&lt;p&gt;
GitTables&#65306;&#19968;&#20010;&#22823;&#22411;&#20851;&#31995;&#34920;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
GitTables: A Large-Scale Corpus of Relational Tables. (arXiv:2106.07258v5 [cs.DB] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.07258
&lt;/p&gt;
&lt;p&gt;
GitTables&#26159;&#19968;&#20010;&#20174;GitHub&#20013;&#25552;&#21462;&#30340;&#22823;&#22411;&#35821;&#26009;&#24211;&#65292;&#21253;&#21547;1M&#20010;&#20851;&#31995;&#34920;&#26684;&#65292;&#21487;&#20197;&#26377;&#25928;&#35757;&#32451;&#21644;&#35780;&#20272;&#39640;&#23481;&#37327;&#27169;&#22411;&#65292;&#20197;&#25913;&#36827;&#20851;&#31995;&#34920;&#26684;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#25104;&#21151;&#24341;&#21457;&#20102;&#23545;&#20351;&#29992;&#35757;&#32451;&#20110;&#22823;&#22411;&#34920;&#26684;&#35821;&#26009;&#24211;&#30340;&#34920;&#26684;&#34920;&#31034;&#27169;&#22411;&#25913;&#36827;&#20851;&#31995;&#34920;&#26684;&#20219;&#21153;&#65288;&#22914;&#25968;&#25454;&#20934;&#22791;&#21644;&#25628;&#32034;&#65289;&#30340;&#20852;&#36259;&#12290;&#29616;&#26377;&#30340;&#34920;&#26684;&#35821;&#26009;&#24211;&#20027;&#35201;&#21253;&#21547;&#20174;HTML&#39029;&#38754;&#20013;&#25552;&#21462;&#30340;&#34920;&#26684;&#65292;&#38480;&#21046;&#20102;&#34920;&#31034;&#31163;&#32447;&#25968;&#25454;&#24211;&#34920;&#26684;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35757;&#32451;&#21644;&#35780;&#20272;&#36229;&#20986;Web&#24212;&#29992;&#30340;&#39640;&#23481;&#37327;&#27169;&#22411;&#65292;&#25105;&#20204;&#38656;&#35201;&#25317;&#26377;&#31867;&#20284;&#20110;&#20851;&#31995;&#25968;&#25454;&#24211;&#34920;&#26684;&#30340;&#34920;&#26684;&#36164;&#28304;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;GitTables&#65292;&#36825;&#26159;&#19968;&#20010;&#20174;GitHub&#20013;&#25552;&#21462;&#30340;1M&#20010;&#20851;&#31995;&#34920;&#26684;&#30340;&#35821;&#26009;&#24211;&#12290;&#25105;&#20204;&#25345;&#32493;&#26356;&#26032;&#26088;&#22312;&#23558;&#27492;&#35821;&#26009;&#24211;&#25193;&#23637;&#33267;&#33267;&#23569;10M&#20010;&#34920;&#26684;&#12290;&#23545;GitTables&#30340;&#20998;&#26512;&#26174;&#31034;&#20854;&#32467;&#26500;&#12289;&#20869;&#23481;&#21644;&#20027;&#39064;&#35206;&#30422;&#33539;&#22260;&#19982;&#29616;&#26377;&#34920;&#26684;&#35821;&#26009;&#24211;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#25105;&#20204;&#36890;&#36807;Schema.org&#21644;DBpedia&#23545;GitTables&#20013;&#30340;&#34920;&#26684;&#21015;&#36827;&#34892;&#35821;&#20041;&#31867;&#22411;&#12289;&#23618;&#27425;&#20851;&#31995;&#21644;&#25551;&#36848;&#27880;&#37322;&#12290;&#25105;&#20204;&#22312;T2Dv2&#22522;&#20934;&#35780;&#20272;&#19978;&#30340;&#27880;&#37322;&#27969;&#31243;&#39564;&#35777;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of deep learning has sparked interest in improving relational table tasks, like data preparation and search, with table representation models trained on large table corpora. Existing table corpora primarily contain tables extracted from HTML pages, limiting the capability to represent offline database tables. To train and evaluate high-capacity models for applications beyond the Web, we need resources with tables that resemble relational database tables. Here we introduce GitTables, a corpus of 1M relational tables extracted from GitHub. Our continuing curation aims at growing the corpus to at least 10M tables. Analyses of GitTables show that its structure, content, and topical coverage differ significantly from existing table corpora. We annotate table columns in GitTables with semantic types, hierarchical relations and descriptions from Schema.org and DBpedia. The evaluation of our annotation pipeline on the T2Dv2 benchmark illustrates that our approach provides results o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25991;&#26412;&#34920;&#31034;&#32858;&#31867;&#26041;&#27861;Vec2GC&#65292;&#23558;&#32858;&#31867;&#31639;&#27861;&#19982;&#22522;&#20110;&#25991;&#26412;&#34920;&#31034;&#23398;&#20064;&#21019;&#24314;&#30340;&#26415;&#35821;&#25110;&#25991;&#26723;&#21152;&#26435;&#22270;&#30340;&#31038;&#21306;&#26816;&#27979;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#29992;&#20110;&#26080;&#30417;&#30563;&#30340;&#25991;&#26723;&#22788;&#29702;&#12290;</title><link>http://arxiv.org/abs/2104.09439</link><description>&lt;p&gt;
Vec2GC -- &#22522;&#20110;&#22270;&#30340;&#25991;&#26412;&#34920;&#31034;&#32858;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Vec2GC -- A Graph Based Clustering Method for Text Representations. (arXiv:2104.09439v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2104.09439
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25991;&#26412;&#34920;&#31034;&#32858;&#31867;&#26041;&#27861;Vec2GC&#65292;&#23558;&#32858;&#31867;&#31639;&#27861;&#19982;&#22522;&#20110;&#25991;&#26412;&#34920;&#31034;&#23398;&#20064;&#21019;&#24314;&#30340;&#26415;&#35821;&#25110;&#25991;&#26723;&#21152;&#26435;&#22270;&#30340;&#31038;&#21306;&#26816;&#27979;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#29992;&#20110;&#26080;&#30417;&#30563;&#30340;&#25991;&#26723;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26377;&#38480;&#25110;&#27809;&#26377;&#26631;&#31614;&#25968;&#25454;&#30340;NLP&#27969;&#27700;&#32447;&#20013;&#65292;&#38656;&#35201;&#20381;&#36182;&#26080;&#30417;&#30563;&#26041;&#27861;&#36827;&#34892;&#25991;&#26723;&#22788;&#29702;&#12290;&#26080;&#30417;&#30563;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#26415;&#35821;&#25110;&#25991;&#26723;&#30340;&#32858;&#31867;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#32858;&#31867;&#31639;&#27861;&#65292;Vec2GC (Vector to Graph Communities)&#65292;&#23427;&#26159;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#27969;&#27700;&#32447;&#65292;&#21487;&#20197;&#38024;&#23545;&#20219;&#20309;&#32473;&#23450;&#30340;&#25991;&#26412;&#35821;&#26009;&#24211;&#32858;&#31867;&#26415;&#35821;&#25110;&#25991;&#26723;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;&#25991;&#26412;&#34920;&#31034;&#23398;&#20064;&#21019;&#24314;&#30340;&#26415;&#35821;&#25110;&#25991;&#26723;&#21152;&#26435;&#22270;&#30340;&#31038;&#21306;&#26816;&#27979;&#12290;Vec2GC&#32858;&#31867;&#31639;&#27861;&#26159;&#19968;&#31181;&#22522;&#20110;&#23494;&#24230;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#25903;&#25345;&#23618;&#27425;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
NLP pipelines with limited or no labeled data, rely on unsupervised methods for document processing. Unsupervised approaches typically depend on clustering of terms or documents. In this paper, we introduce a novel clustering algorithm, Vec2GC (Vector to Graph Communities), an end-to-end pipeline to cluster terms or documents for any given text corpus. Our method uses community detection on a weighted graph of the terms or documents, created using text representation learning. Vec2GC clustering algorithm is a density based approach, that supports hierarchical clustering as well.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#21518;&#39564;&#20849;&#35782;&#20998;&#24067;&#30340;&#25554;&#34917;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#25511;&#21046;&#26041;&#24046;&#21644;&#21069;&#30651;&#24615;&#20559;&#24046;&#26435;&#34913;&#30340;&#21516;&#26102;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#25554;&#34917;&#65292;&#36866;&#29992;&#20110;&#37329;&#34701;&#31561;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2102.12736</link><description>&lt;p&gt;
&#29992;Wasserstein&#25554;&#20540;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#25554;&#34917;&#20197;&#36798;&#21040;&#26368;&#20339;&#36828;&#26399;&#20559;&#24046;&#21644;&#26041;&#24046;&#24179;&#34913;
&lt;/p&gt;
&lt;p&gt;
Time-Series Imputation with Wasserstein Interpolation for Optimal Look-Ahead-Bias and Variance Tradeoff. (arXiv:2102.12736v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.12736
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#21518;&#39564;&#20849;&#35782;&#20998;&#24067;&#30340;&#25554;&#34917;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#25511;&#21046;&#26041;&#24046;&#21644;&#21069;&#30651;&#24615;&#20559;&#24046;&#26435;&#34913;&#30340;&#21516;&#26102;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#25554;&#34917;&#65292;&#36866;&#29992;&#20110;&#37329;&#34701;&#31561;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32570;&#22833;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26159;&#19968;&#20010;&#26222;&#36941;&#23384;&#22312;&#30340;&#23454;&#38469;&#38382;&#39064;&#12290;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#25554;&#34917;&#26041;&#27861;&#36890;&#24120;&#34987;&#24212;&#29992;&#20110;&#23436;&#25972;&#30340;&#38754;&#26495;&#25968;&#25454;&#65292;&#30446;&#30340;&#26159;&#20026;&#20102;&#35757;&#32451;&#19968;&#20010;&#29992;&#20110;&#19979;&#28216;&#26679;&#26412;&#22806;&#20219;&#21153;&#30340;&#27169;&#22411;&#12290;&#20363;&#22914;&#65292;&#22312;&#37329;&#34701;&#20013;&#65292;&#32570;&#22833;&#25910;&#30410;&#30340;&#25554;&#34917;&#21487;&#33021;&#34987;&#24212;&#29992;&#20110;&#22312;&#35757;&#32451;&#32452;&#21512;&#20248;&#21270;&#27169;&#22411;&#20043;&#21069;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20570;&#27861;&#21487;&#33021;&#20250;&#23548;&#33268;&#26410;&#26469;&#24615;&#33021;&#19978;&#30340;&#21069;&#30651;&#24615;&#20559;&#24046;&#12290;&#20351;&#29992;&#23436;&#25972;&#25968;&#25454;&#38598;&#36827;&#34892;&#25554;&#34917;&#23384;&#22312;&#20351;&#29992;&#21482;&#35757;&#32451;&#25968;&#25454;&#30340;&#25554;&#34917;&#20013;&#36739;&#22823;&#30340;&#26041;&#24046;&#20043;&#38388;&#30340;&#22266;&#26377;&#26435;&#34913;&#12290;&#36890;&#36807;&#36830;&#25509;&#26102;&#38388;&#20013;&#26174;&#31034;&#30340;&#20449;&#24687;&#23618;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36125;&#21494;&#26031;&#21518;&#39564;&#20849;&#35782;&#20998;&#24067;&#65292;&#23427;&#22312;&#25554;&#34917;&#20013;&#26368;&#20248;&#22320;&#25511;&#21046;&#20102;&#26041;&#24046;&#21644;&#21069;&#30651;&#24615;&#20559;&#24046;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#37329;&#34701;&#25968;&#25454;&#20013;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Missing time-series data is a prevalent practical problem. Imputation methods in time-series data often are applied to the full panel data with the purpose of training a model for a downstream out-of-sample task. For example, in finance, imputation of missing returns may be applied prior to training a portfolio optimization model. Unfortunately, this practice may result in a look-ahead-bias in the future performance on the downstream task. There is an inherent trade-off between the look-ahead-bias of using the full data set for imputation and the larger variance in the imputation from using only the training data. By connecting layers of information revealed in time, we propose a Bayesian posterior consensus distribution which optimally controls the variance and look-ahead-bias trade-off in the imputation. We demonstrate the benefit of our methodology both in synthetic and real financial data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35748;&#35777;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#35770;&#25991;&#24635;&#32467;&#20102;&#20851;&#20110;&#20984;&#26494;&#24347;&#12289;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#21644;&#38543;&#26426;&#24179;&#28369;&#31561;&#26041;&#27861;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#21644;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2009.04131</link><description>&lt;p&gt;
SoK: &#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35748;&#35777;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
SoK: Certified Robustness for Deep Neural Networks. (arXiv:2009.04131v9 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2009.04131
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35748;&#35777;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#35770;&#25991;&#24635;&#32467;&#20102;&#20851;&#20110;&#20984;&#26494;&#24347;&#12289;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#21644;&#38543;&#26426;&#24179;&#28369;&#31561;&#26041;&#27861;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#65292;&#36825;&#22312;&#23558;&#36825;&#20123;&#27169;&#22411;&#37096;&#32626;&#21040;&#33258;&#21160;&#39550;&#39542;&#31561;&#23433;&#20840;&#20851;&#38190;&#22411;&#24212;&#29992;&#26102;&#24341;&#36215;&#20102;&#37325;&#22823;&#20851;&#27880;&#12290;&#19981;&#21516;&#30340;&#38450;&#24481;&#26041;&#27861;&#24050;&#34987;&#25552;&#20986;&#26469;&#23545;&#25239;&#23545;&#25239;&#25915;&#20987;&#65292;&#21253;&#25324;&#32463;&#39564;&#24615;&#38450;&#24481;&#21644;&#35748;&#35777;&#40065;&#26834;&#24615;&#38450;&#24481;&#12290;&#26412;&#25991;&#31995;&#32479;&#21270;&#30740;&#31350;&#20102;&#35748;&#35777;&#40065;&#26834;&#24615;&#38450;&#24481;&#26041;&#27861;&#21450;&#30456;&#20851;&#30340;&#23454;&#38469;&#21644;&#29702;&#35770;&#24847;&#20041;&#21644;&#21457;&#29616;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#39318;&#27425;&#23545;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#29616;&#26377;&#40065;&#26834;&#24615;&#35748;&#35777;&#21644;&#35757;&#32451;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#20851;&#27880;&#22522;&#20110;&#20984;&#26494;&#24347;&#12289;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#21644;&#38543;&#26426;&#24179;&#28369;&#30340;&#35748;&#35777;&#40065;&#26834;&#24615;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#23545;&#26356;&#20808;&#36827;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#22914;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;GCNN&#65289;&#21644;&#29983;&#25104;&#27169;&#22411;&#30340;&#35748;&#35777;&#40065;&#26834;&#24615;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#21644;&#35748;&#35777;&#40065;&#26834;&#24615;DNN&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Great advances in deep neural networks (DNNs) have led to state-of-the-art performance on a wide range of tasks. However, recent studies have shown that DNNs are vulnerable to adversarial attacks, which have brought great concerns when deploying these models to safety-critical applications such as autonomous driving. Different defense approaches have been proposed against adversarial attacks, including: a) empirical defenses, which can usually be adaptively attacked again without providing robustness certification; and b) certifiably robust approaches, which consist of robustness verification providing the lower bound of robust accuracy against any attacks under certain conditions and corresponding robust training approaches. In this paper, we systematize certifiably robust approaches and related practical and theoretical implications and findings. We also provide the first comprehensive benchmark on existing robustness verification and training approaches on different datasets. In par
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#38454;&#20056;&#24130;&#20316;&#20026;&#26032;&#30340;&#21442;&#25968;&#35774;&#32622;&#24037;&#20855;&#65292;&#21487;&#20197;&#31616;&#21270;&#25110;&#25552;&#39640;&#21160;&#37327;&#27861;&#21644;&#38543;&#26426;&#20248;&#21270;&#26041;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2006.01244</link><description>&lt;p&gt;
&#38454;&#20056;&#24130;&#30340;&#23041;&#21147;: (&#38543;&#26426;) &#20248;&#21270;&#30340;&#26032;&#21442;&#25968;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Power of Factorial Powers: New Parameter settings for (Stochastic) Optimization. (arXiv:2006.01244v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2006.01244
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#38454;&#20056;&#24130;&#20316;&#20026;&#26032;&#30340;&#21442;&#25968;&#35774;&#32622;&#24037;&#20855;&#65292;&#21487;&#20197;&#31616;&#21270;&#25110;&#25552;&#39640;&#21160;&#37327;&#27861;&#21644;&#38543;&#26426;&#20248;&#21270;&#26041;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20984;&#20248;&#21270;&#21644;&#38750;&#20984;&#20248;&#21270;&#26041;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#21462;&#20915;&#20110;&#35768;&#22810;&#24120;&#25968;&#30340;&#36873;&#25321;&#65292;&#21253;&#25324;&#27493;&#38271;&#12289;Lyapunov&#20989;&#25968;&#24120;&#25968;&#21644;&#21160;&#37327;&#24120;&#25968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#38454;&#20056;&#24130;&#20316;&#20026;&#23450;&#20041;&#20986;&#29616;&#22312;&#25910;&#25947;&#35777;&#26126;&#20013;&#30340;&#24120;&#25968;&#30340;&#28789;&#27963;&#24037;&#20855;&#12290;&#25105;&#20204;&#21015;&#20030;&#20102;&#36825;&#20123;&#25968;&#21015;&#20855;&#26377;&#30340;&#19968;&#20123;&#26174;&#33879;&#24615;&#36136;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;&#21152;&#36895;&#26799;&#24230;&#26041;&#27861;&#12289;&#21160;&#37327;&#27861;&#21644;&#38543;&#26426;&#26041;&#24046;&#32553;&#20943;&#26041;&#27861;&#65288;SVRG&#65289;&#30340;&#25910;&#25947;&#35777;&#26126;&#20013;&#65292;&#20197;&#31616;&#21270;&#25110;&#25913;&#36827;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The convergence rates for convex and non-convex optimization methods depend on the choice of a host of constants, including step sizes, Lyapunov function constants and momentum constants. In this work we propose the use of factorial powers as a flexible tool for defining constants that appear in convergence proofs. We list a number of remarkable properties that these sequences enjoy, and show how they can be applied to convergence proofs to simplify or improve the convergence rates of the momentum method, accelerated gradient and the stochastic variance reduced method (SVRG).
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#32479;&#35745; ML &#27169;&#22411;&#19982;&#36923;&#36753;&#25512;&#29702;&#32452;&#20214;&#38598;&#25104;&#65292;&#25552;&#20986;&#20102;&#26041;&#27861;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640; ML &#27169;&#22411;&#30340;&#35748;&#35777;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#32473;&#20986;&#20102;&#39318;&#20010;&#36866;&#29992;&#20110;&#39532;&#23572;&#21487;&#22827;&#36923;&#36753;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2003.00120</link><description>&lt;p&gt;
&#36890;&#36807;&#36923;&#36753;&#25512;&#29702;&#19982;&#32479;&#35745;&#23398;&#20064;&#25552;&#39640;&#35748;&#35777;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Certified Robustness via Statistical Learning with Logical Reasoning. (arXiv:2003.00120v9 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2003.00120
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#32479;&#35745; ML &#27169;&#22411;&#19982;&#36923;&#36753;&#25512;&#29702;&#32452;&#20214;&#38598;&#25104;&#65292;&#25552;&#20986;&#20102;&#26041;&#27861;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640; ML &#27169;&#22411;&#30340;&#35748;&#35777;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#32473;&#20986;&#20102;&#39318;&#20010;&#36866;&#29992;&#20110;&#39532;&#23572;&#21487;&#22827;&#36923;&#36753;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#38024;&#23545;&#22797;&#26434; ML &#27169;&#22411;&#30340;&#35748;&#35777;&#40065;&#26834;&#24615;&#24555;&#36895;&#25552;&#39640;&#38656;&#35201;&#36827;&#34892;&#23494;&#38598;&#30340;&#31639;&#27861;&#24037;&#20316;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#40065;&#26834;&#24615;&#35748;&#35777;&#26041;&#27861;&#21482;&#33021;&#22312;&#26377;&#38480;&#30340;&#25200;&#21160;&#21322;&#24452;&#20869;&#36827;&#34892;&#35748;&#35777;&#12290;&#32771;&#34385;&#21040;&#29616;&#26377;&#30340;&#32431;&#25968;&#25454;&#39537;&#21160;&#30340;&#32479;&#35745;&#26041;&#27861;&#24050;&#32463;&#36798;&#21040;&#29942;&#39048;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#36923;&#36753;&#32593;&#32476;&#65288;MLN&#65289;&#23558;&#32479;&#35745; ML &#27169;&#22411;&#19982;&#30693;&#35782;&#65288;&#36890;&#36807;&#36923;&#36753;&#35268;&#21017;&#34920;&#31034;&#65289;&#20316;&#20026;&#25512;&#29702;&#32452;&#20214;&#36827;&#34892;&#38598;&#25104;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#25972;&#20307;&#30340;&#35748;&#35777;&#40065;&#26834;&#24615;&#12290;&#36825;&#24341;&#21457;&#20102;&#20851;&#20110;&#35748;&#35777;&#36825;&#31181;&#33539;&#24335;&#65288;&#29305;&#21035;&#26159;&#25512;&#29702;&#32452;&#20214;&#65292;&#22914; MLN&#65289;&#40065;&#26834;&#24615;&#30340;&#26032;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#20316;&#20026;&#29702;&#35299;&#36825;&#20123;&#38382;&#39064;&#30340;&#31532;&#19968;&#27493;&#65292;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#35777;&#26126; MLN &#40065;&#26834;&#24615;&#35745;&#31639;&#22797;&#26434;&#24230;&#26159; #P-&#38590;&#30340;&#12290;&#22312;&#36825;&#20010;&#38590;&#24230;&#32467;&#26524;&#30340;&#25351;&#23548;&#19979;&#65292;&#25105;&#20204;&#36890;&#36807;&#20180;&#32454;&#20998;&#26512;&#19981;&#21516;&#30340;&#27169;&#22411;&#35268;&#21017;&#65292;&#25512;&#23548;&#20986;&#20102; MLN &#30340;&#31532;&#19968;&#20010;&#35748;&#35777;&#30340;&#40065;&#26834;&#24615;&#30028;&#38480;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#20004;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intensive algorithmic efforts have been made to enable the rapid improvements of certificated robustness for complex ML models recently. However, current robustness certification methods are only able to certify under a limited perturbation radius. Given that existing pure data-driven statistical approaches have reached a bottleneck, in this paper, we propose to integrate statistical ML models with knowledge (expressed as logical rules) as a reasoning component using Markov logic networks (MLN, so as to further improve the overall certified robustness. This opens new research questions about certifying the robustness of such a paradigm, especially the reasoning component (e.g., MLN). As the first step towards understanding these questions, we first prove that the computational complexity of certifying the robustness of MLN is #P-hard. Guided by this hardness result, we then derive the first certified robustness bound for MLN by carefully analyzing different model regimes. Finally, we c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20272;&#35745;&#22320;&#38663;&#26029;&#23618;&#19981;&#30830;&#23450;&#24615;&#65292;&#36890;&#36807;&#35299;&#20915;&#25968;&#25454;&#19981;&#36275;&#38382;&#39064;&#21644;&#30830;&#23450;&#23548;&#33268;&#26029;&#23618;&#30340;&#21442;&#25968;&#32452;&#21512;&#65292;&#24182;&#20351;&#29992;&#20004;&#21315;&#27425;&#30340;&#26029;&#23618;&#27169;&#25311;&#26469;&#35757;&#32451;&#21644;&#27979;&#35797;&#27169;&#22411;&#65292;&#26368;&#32456;&#24471;&#20998;0.83&#12290;</title><link>http://arxiv.org/abs/1911.09660</link><description>&lt;p&gt;
&#20351;&#29992;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20272;&#35745;&#22320;&#38663;&#26029;&#23618;&#30340;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Estimating uncertainty of earthquake rupture using Bayesian neural network. (arXiv:1911.09660v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1911.09660
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20272;&#35745;&#22320;&#38663;&#26029;&#23618;&#19981;&#30830;&#23450;&#24615;&#65292;&#36890;&#36807;&#35299;&#20915;&#25968;&#25454;&#19981;&#36275;&#38382;&#39064;&#21644;&#30830;&#23450;&#23548;&#33268;&#26029;&#23618;&#30340;&#21442;&#25968;&#32452;&#21512;&#65292;&#24182;&#20351;&#29992;&#20004;&#21315;&#27425;&#30340;&#26029;&#23618;&#27169;&#25311;&#26469;&#35757;&#32451;&#21644;&#27979;&#35797;&#27169;&#22411;&#65292;&#26368;&#32456;&#24471;&#20998;0.83&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BNN&#65289;&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#21644;&#38543;&#26426;&#36807;&#31243;&#20248;&#21183;&#30340;&#27010;&#29575;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;BNN&#21487;&#20197;&#35299;&#20915;&#36807;&#24230;&#25311;&#21512;&#38382;&#39064;&#65292;&#24182;&#22312;&#25968;&#25454;&#26377;&#38480;&#30340;&#24212;&#29992;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;&#22320;&#38663;&#26029;&#23618;&#30740;&#31350;&#23601;&#26159;&#36825;&#26679;&#19968;&#20010;&#25968;&#25454;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#31185;&#23398;&#23478;&#24517;&#39035;&#20381;&#38752;&#35768;&#22810;&#35797;&#39564;&#21644;&#38169;&#35823;&#30340;&#25968;&#20540;&#25110;&#29289;&#29702;&#27169;&#22411;&#26469;&#36827;&#34892;&#30740;&#31350;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#20351;&#29992;BNN&#65292;&#65288;1&#65289;&#35299;&#20915;&#25968;&#25454;&#38382;&#39064;&#65292;&#65288;2&#65289;&#25214;&#20986;&#23548;&#33268;&#22320;&#38663;&#26029;&#35010;&#30340;&#21442;&#25968;&#32452;&#21512;&#65292;&#65288;3&#65289;&#20272;&#35745;&#22320;&#38663;&#26029;&#23618;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20351;&#29992;&#20102;&#20004;&#21315;&#27425;&#30340;&#26029;&#23618;&#27169;&#25311;&#26469;&#35757;&#32451;&#21644;&#27979;&#35797;&#27169;&#22411;&#65292;&#22312;&#27599;&#20010;&#27169;&#25311;&#20013;&#65292;&#19968;&#20010;&#31616;&#21333;&#30340;2D&#26029;&#23618;&#32467;&#26500;&#34987;&#32771;&#34385;&#65292;&#22312;&#20013;&#24515;&#22788;&#20855;&#26377;&#39640;&#26031;&#20960;&#20309;&#24322;&#36136;&#24615;&#65292;8&#20010;&#21442;&#25968;&#22312;&#27599;&#27425;&#27169;&#25311;&#20013;&#21464;&#21270;&#12290;BNN&#30340;&#27979;&#35797;F1&#24471;&#20998;&#20026;0.83&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian neural networks (BNN) are the probabilistic model that combines the strengths of both neural network (NN) and stochastic processes. As a result, BNN can combat overfitting and perform well in applications where data is limited. Earthquake rupture study is such a problem where data is insufficient, and scientists have to rely on many trial and error numerical or physical models. Lack of resources and computational expenses, often, it becomes hard to determine the reasons behind the earthquake rupture. In this work, a BNN has been used (1) to combat the small data problem and (2) to find out the parameter combinations responsible for earthquake rupture and (3) to estimate the uncertainty associated with earthquake rupture. Two thousand rupture simulations are used to train and test the model. A simple 2D rupture geometry is considered where the fault has a Gaussian geometric heterogeneity at the center, and eight parameters vary in each simulation. The test F1-score of BNN (0.83
&lt;/p&gt;</description></item></channel></rss>