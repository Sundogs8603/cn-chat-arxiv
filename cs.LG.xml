<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#28982;&#21453;&#20107;&#23454;&#26694;&#26550;&#21644;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#25511;&#21046;&#22238;&#28335;&#30340;&#33539;&#22260;&#65292;&#29983;&#25104;&#19982;&#23454;&#38469;&#19990;&#30028;&#30340;&#25968;&#25454;&#20998;&#24067;&#30456;&#21305;&#37197;&#30340;&#33258;&#28982;&#21453;&#20107;&#23454;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#21453;&#20107;&#23454;&#25512;&#29702;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01607</link><description>&lt;p&gt;
&#20855;&#26377;&#24517;&#35201;&#22238;&#28335;&#30340;&#33258;&#28982;&#21453;&#20107;&#23454;
&lt;/p&gt;
&lt;p&gt;
Natural Counterfactuals With Necessary Backtracking
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01607
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#28982;&#21453;&#20107;&#23454;&#26694;&#26550;&#21644;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#25511;&#21046;&#22238;&#28335;&#30340;&#33539;&#22260;&#65292;&#29983;&#25104;&#19982;&#23454;&#38469;&#19990;&#30028;&#30340;&#25968;&#25454;&#20998;&#24067;&#30456;&#21305;&#37197;&#30340;&#33258;&#28982;&#21453;&#20107;&#23454;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#21453;&#20107;&#23454;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#25512;&#29702;&#23545;&#20110;&#20154;&#31867;&#35748;&#30693;&#38750;&#24120;&#37325;&#35201;&#65292;&#23588;&#20854;&#23545;&#20110;&#25552;&#20379;&#35299;&#37322;&#21644;&#20570;&#20986;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;Judea Pearl&#30340;&#30740;&#31350;&#26041;&#27861;&#22312;&#29702;&#35770;&#19978;&#24456;&#20248;&#38597;&#65292;&#20294;&#20854;&#29983;&#25104;&#21453;&#20107;&#23454;&#24773;&#26223;&#24448;&#24448;&#38656;&#35201;&#36807;&#20110;&#33073;&#31163;&#23454;&#38469;&#24773;&#26223;&#30340;&#24178;&#39044;&#65292;&#22240;&#27492;&#38590;&#20197;&#23454;&#26045;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#28982;&#21453;&#20107;&#23454;&#30340;&#26694;&#26550;&#21644;&#19968;&#31181;&#26681;&#25454;&#23454;&#38469;&#19990;&#30028;&#25968;&#25454;&#20998;&#24067;&#29983;&#25104;&#33258;&#28982;&#21453;&#20107;&#23454;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#23545;&#21453;&#20107;&#23454;&#25512;&#29702;&#30340;&#25913;&#36827;&#65292;&#20801;&#35768;&#23545;&#22240;&#26524;&#21069;&#32622;&#21464;&#37327;&#36827;&#34892;&#25913;&#21464;&#20197;&#26368;&#23567;&#21270;&#19982;&#23454;&#38469;&#24773;&#26223;&#30340;&#20559;&#24046;&#12290;&#20026;&#20102;&#29983;&#25104;&#33258;&#28982;&#21453;&#20107;&#23454;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#28982;&#24615;&#20934;&#21017;&#20801;&#35768;&#20294;&#25511;&#21046;&#22238;&#28335;&#30340;&#33539;&#22260;&#12290;&#23454;&#35777;&#23454;&#39564;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual reasoning is pivotal in human cognition and especially important for providing explanations and making decisions. While Judea Pearl's influential approach is theoretically elegant, its generation of a counterfactual scenario often requires interventions that are too detached from the real scenarios to be feasible. In response, we propose a framework of natural counterfactuals and a method for generating counterfactuals that are natural with respect to the actual world's data distribution. Our methodology refines counterfactual reasoning, allowing changes in causally preceding variables to minimize deviations from realistic scenarios. To generate natural counterfactuals, we introduce an innovative optimization framework that permits but controls the extent of backtracking with a naturalness criterion. Empirical experiments indicate the effectiveness of our method.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;CounterCurate&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#20363;&#23376;&#21644;&#29983;&#25104;&#24335;&#24494;&#35843;&#65292;&#20840;&#38754;&#25552;&#21319;&#35270;&#35273;-&#35821;&#35328;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;&#29289;&#29702;&#25512;&#29702;&#21644;&#35821;&#20041;&#23545;&#29031;&#24494;&#35843;&#26041;&#38754;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.13254</link><description>&lt;p&gt;
CounterCurate: &#36890;&#36807;&#23545;&#29031;&#20363;&#23376;&#22686;&#24378;&#29289;&#29702;&#21644;&#35821;&#20041;&#35270;&#35273;-&#35821;&#35328;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13254
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;CounterCurate&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#20363;&#23376;&#21644;&#29983;&#25104;&#24335;&#24494;&#35843;&#65292;&#20840;&#38754;&#25552;&#21319;&#35270;&#35273;-&#35821;&#35328;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;&#29289;&#29702;&#25512;&#29702;&#21644;&#35821;&#20041;&#23545;&#29031;&#24494;&#35843;&#26041;&#38754;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;CounterCurate&#65292;&#19968;&#20010;&#26694;&#26550;&#65292;&#20840;&#38754;&#25552;&#21319;&#23545;&#27604;&#21644;&#29983;&#25104;&#24335;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#35270;&#35273;-&#35821;&#35328;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#20004;&#20010;&#23578;&#26410;&#20805;&#20998;&#25506;&#35752;&#30340;&#20851;&#38190;&#38382;&#39064;&#65306;&#24573;&#35270;&#20102;&#22522;&#20110;&#29289;&#29702;&#30340;&#25512;&#29702;&#65288;&#35745;&#25968;&#21644;&#20301;&#32622;&#29702;&#35299;&#65289;&#65292;&#20197;&#21450;&#21033;&#29992;&#39640;&#24615;&#33021;&#25991;&#26412;&#21644;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#21453;&#20107;&#23454;&#24494;&#35843;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24320;&#21019;&#20102;&#19968;&#20010;&#35299;&#20915;&#36825;&#20123;&#31354;&#30333;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#31361;&#20986;&#20102;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;&#22914;CLIP&#21644;LLaVA&#65289;&#22312;&#22522;&#20110;&#29289;&#29702;&#30340;&#32452;&#21512;&#25512;&#29702;&#20013;&#20960;&#20046;&#26080;&#27861;&#32988;&#20219;&#30340;&#34920;&#29616;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24212;&#29992;&#31616;&#21333;&#30340;&#25968;&#25454;&#22686;&#24378;&#65292;&#20351;&#29992;&#22522;&#20110;&#22270;&#20687;&#30340;&#29983;&#25104;&#27169;&#22411;GLIGEN&#29983;&#25104;&#24494;&#35843;&#25968;&#25454;&#65292;&#20351;&#24471;&#24615;&#33021;&#26174;&#33879;&#25552;&#39640;&#65306;&#22312;&#25105;&#20204;&#26032;&#30340;&#31574;&#21010;&#30340;Flickr30k-Positions&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;CLIP&#21644;LLaVA&#30340;&#24615;&#33021;&#20998;&#21035;&#25552;&#39640;&#20102;+33%&#21644;+37%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#39640;&#24615;&#33021;&#25991;&#26412;&#21644;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13254v1 Announce Type: cross  Abstract: We propose CounterCurate, a framework to comprehensively improve the visio-linguistic compositional reasoning capability for both contrastive and generative multimodal models. In particular, we identify two under-explored critical problems: the neglect of the physically grounded reasoning (counting and position understanding) and the potential of using highly capable text and image generation models for semantic counterfactual fine-tuning. Our work pioneers an approach that addresses these gaps. We first spotlight the near-chance performance of multimodal models like CLIP and LLaVA in physically grounded compositional reasoning. We then apply simple data augmentation using a grounded image generation model, GLIGEN, to generate finetuning data, resulting in significant performance improvements: +33% and +37% for CLIP and LLaVA, respectively, on our newly curated Flickr30k-Positions benchmark. Moreover, we exploit the capabilities of hig
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;FlashTex&#26041;&#27861;&#65292;&#22522;&#20110;LightControlNet&#23454;&#29616;&#20102;&#24555;&#36895;&#33258;&#21160;&#21270;3D&#32593;&#26684;&#32441;&#29702;&#29983;&#25104;&#65292;&#23454;&#29616;&#20102;&#29031;&#26126;&#19982;&#34920;&#38754;&#26448;&#36136;&#30340;&#35299;&#32806;&#65292;&#20351;&#24471;&#32593;&#26684;&#33021;&#22815;&#22312;&#20219;&#20309;&#29031;&#26126;&#29615;&#22659;&#19979;&#27491;&#30830;&#37325;&#29031;&#21644;&#28210;&#26579;</title><link>https://arxiv.org/abs/2402.13251</link><description>&lt;p&gt;
FlashTex&#65306;&#20855;&#26377;LightControlNet&#30340;&#24555;&#36895;&#21487;&#37325;&#22609;&#32593;&#26684;&#32441;&#29702;
&lt;/p&gt;
&lt;p&gt;
FlashTex: Fast Relightable Mesh Texturing with LightControlNet
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13251
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;FlashTex&#26041;&#27861;&#65292;&#22522;&#20110;LightControlNet&#23454;&#29616;&#20102;&#24555;&#36895;&#33258;&#21160;&#21270;3D&#32593;&#26684;&#32441;&#29702;&#29983;&#25104;&#65292;&#23454;&#29616;&#20102;&#29031;&#26126;&#19982;&#34920;&#38754;&#26448;&#36136;&#30340;&#35299;&#32806;&#65292;&#20351;&#24471;&#32593;&#26684;&#33021;&#22815;&#22312;&#20219;&#20309;&#29031;&#26126;&#29615;&#22659;&#19979;&#27491;&#30830;&#37325;&#29031;&#21644;&#28210;&#26579;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#21160;&#20026;3D&#32593;&#26684;&#21019;&#24314;&#32441;&#29702;&#36153;&#26102;&#36153;&#21147;&#65292;&#21363;&#20351;&#23545;&#20110;&#19987;&#23478;&#35270;&#35273;&#20869;&#23481;&#21019;&#24314;&#32773;&#20063;&#26159;&#22914;&#27492;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#26041;&#27861;&#65292;&#26681;&#25454;&#29992;&#25143;&#25552;&#20379;&#30340;&#25991;&#26412;&#25552;&#31034;&#33258;&#21160;&#20026;&#36755;&#20837;&#30340;3D&#32593;&#26684;&#30528;&#33394;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#29031;&#26126;&#19982;&#34920;&#38754;&#26448;&#36136;/&#21453;&#23556;&#22312;&#29983;&#25104;&#30340;&#32441;&#29702;&#20013;&#35299;&#32806;&#65292;&#20197;&#20415;&#32593;&#26684;&#21487;&#20197;&#22312;&#20219;&#20309;&#29031;&#26126;&#29615;&#22659;&#20013;&#27491;&#30830;&#37325;&#29031;&#21644;&#28210;&#26579;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;LightControlNet&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;ControlNet&#26550;&#26500;&#30340;&#26032;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#65292;&#20801;&#35768;&#23558;&#25152;&#38656;&#29031;&#26126;&#35268;&#26684;&#20316;&#20026;&#23545;&#27169;&#22411;&#30340;&#26465;&#20214;&#22270;&#20687;&#12290;&#25105;&#20204;&#30340;&#25991;&#26412;&#21040;&#32441;&#29702;&#31649;&#36947;&#28982;&#21518;&#20998;&#20004;&#20010;&#38454;&#27573;&#26500;&#24314;&#32441;&#29702;&#12290;&#31532;&#19968;&#38454;&#27573;&#20351;&#29992;LightControlNet&#29983;&#25104;&#32593;&#26684;&#30340;&#19968;&#32452;&#31232;&#30095;&#30340;&#35270;&#35273;&#19968;&#33268;&#30340;&#21442;&#32771;&#35270;&#22270;&#12290;&#31532;&#20108;&#38454;&#27573;&#24212;&#29992;&#22522;&#20110;&#20998;&#25968;&#33976;&#39311;&#37319;&#26679;&#65288;SDS&#65289;&#30340;&#32441;&#29702;&#20248;&#21270;&#65292;&#36890;&#36807;LightControlNet&#26469;&#25552;&#39640;&#32441;&#29702;&#36136;&#37327;&#21516;&#26102;&#35299;&#32806;&#34920;&#38754;&#26448;&#36136;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13251v1 Announce Type: cross  Abstract: Manually creating textures for 3D meshes is time-consuming, even for expert visual content creators. We propose a fast approach for automatically texturing an input 3D mesh based on a user-provided text prompt. Importantly, our approach disentangles lighting from surface material/reflectance in the resulting texture so that the mesh can be properly relit and rendered in any lighting environment. We introduce LightControlNet, a new text-to-image model based on the ControlNet architecture, which allows the specification of the desired lighting as a conditioning image to the model. Our text-to-texture pipeline then constructs the texture in two stages. The first stage produces a sparse set of visually consistent reference views of the mesh using LightControlNet. The second stage applies a texture optimization based on Score Distillation Sampling (SDS) that works with LightControlNet to increase the texture quality while disentangling surf
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#32852;&#37030;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#65292;&#26088;&#22312;&#36866;&#24212;&#20219;&#24847;&#22240;&#26524;&#27169;&#22411;&#21644;&#24322;&#26500;&#25968;&#25454;&#65292;&#36890;&#36807;&#20351;&#29992;&#26367;&#20195;&#21464;&#37327;&#21644;&#32852;&#37030;&#26465;&#20214;&#29420;&#31435;&#24615;&#26816;&#39564;&#26469;&#35299;&#20915;&#25968;&#25454;&#24322;&#36136;&#24615;&#65292;&#24182;&#24314;&#31435;&#20102;&#32852;&#37030;&#29420;&#31435;&#21464;&#21270;&#21407;&#21017;&#29992;&#20110;&#30830;&#23450;&#22240;&#26524;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2402.13241</link><description>&lt;p&gt;
&#26469;&#33258;&#24322;&#26500;&#25968;&#25454;&#30340;&#32852;&#37030;&#22240;&#26524;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Federated Causal Discovery from Heterogeneous Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13241
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#32852;&#37030;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#65292;&#26088;&#22312;&#36866;&#24212;&#20219;&#24847;&#22240;&#26524;&#27169;&#22411;&#21644;&#24322;&#26500;&#25968;&#25454;&#65292;&#36890;&#36807;&#20351;&#29992;&#26367;&#20195;&#21464;&#37327;&#21644;&#32852;&#37030;&#26465;&#20214;&#29420;&#31435;&#24615;&#26816;&#39564;&#26469;&#35299;&#20915;&#25968;&#25454;&#24322;&#36136;&#24615;&#65292;&#24182;&#24314;&#31435;&#20102;&#32852;&#37030;&#29420;&#31435;&#21464;&#21270;&#21407;&#21017;&#29992;&#20110;&#30830;&#23450;&#22240;&#26524;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#20381;&#36182;&#20110;&#38598;&#20013;&#24335;&#25968;&#25454;&#65292;&#36825;&#19982;&#35768;&#22810;&#23454;&#38469;&#24773;&#20917;&#19979;&#25968;&#25454;&#30340;&#20998;&#25955;&#24615;&#36136;&#19981;&#19968;&#33268;&#12290;&#36825;&#31181;&#24046;&#24322;&#25512;&#21160;&#20102;&#32852;&#37030;&#22240;&#26524;&#21457;&#29616;&#65288;FCD&#65289;&#26041;&#27861;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;FCD&#26041;&#27861;&#21487;&#33021;&#21463;&#21040;&#20854;&#23545;&#21487;&#35782;&#21035;&#21151;&#33021;&#22240;&#26524;&#27169;&#22411;&#25110; homogeneous&#25968;&#25454;&#20998;&#24067;&#30340;&#28508;&#22312;&#38480;&#21046;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23581;&#35797;&#36866;&#24212;&#20219;&#24847;&#22240;&#26524;&#27169;&#22411;&#21644;&#24322;&#26500;&#25968;&#25454;&#30340;&#26032;&#22411;FCD&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#21033;&#29992;&#19982;&#23458;&#25143;&#31471;&#32034;&#24341;&#23545;&#24212;&#30340;&#26367;&#20195;&#21464;&#37327;&#65292;&#20197;&#35299;&#20915;&#19981;&#21516;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#12290;&#28982;&#21518;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#29992;&#20110;&#22240;&#26524;&#39592;&#26550;&#21457;&#29616;&#30340;&#32852;&#37030;&#26465;&#20214;&#29420;&#31435;&#24615;&#26816;&#39564;&#65288;FCIT&#65289;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#29992;&#20110;&#30830;&#23450;&#22240;&#26524;&#26041;&#21521;&#30340;&#32852;&#37030;&#29420;&#31435;&#21464;&#21270;&#21407;&#21017;&#65288;FICP&#65289;&#12290;&#36825;&#20123;&#26041;&#27861;&#28041;&#21450;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13241v1 Announce Type: cross  Abstract: Conventional causal discovery methods rely on centralized data, which is inconsistent with the decentralized nature of data in many real-world situations. This discrepancy has motivated the development of federated causal discovery (FCD) approaches. However, existing FCD methods may be limited by their potentially restrictive assumptions of identifiable functional causal models or homogeneous data distributions, narrowing their applicability in diverse scenarios. In this paper, we propose a novel FCD method attempting to accommodate arbitrary causal models and heterogeneous data. We first utilize a surrogate variable corresponding to the client index to account for the data heterogeneity across different clients. We then develop a federated conditional independence test (FCIT) for causal skeleton discovery and establish a federated independent change principle (FICP) to determine causal directions. These approaches involve constructing
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;SMORE&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#20256;&#24863;&#22120;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#39046;&#22495;&#33258;&#36866;&#24212;&#31639;&#27861;&#65292;&#21033;&#29992;&#39640;&#32500;&#35745;&#31639;&#30340;&#39640;&#25928;&#21644;&#24182;&#34892;&#25805;&#20316;&#65292;&#21160;&#24577;&#23450;&#21046;&#27979;&#35797;&#27169;&#22411;&#20197;&#20943;&#36731;&#25968;&#25454;&#20998;&#24067;&#20559;&#31227;&#24102;&#26469;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2402.13233</link><description>&lt;p&gt;
&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#39640;&#32500;&#22495;&#33258;&#36866;&#24212;&#31639;&#27861;&#29992;&#20110;&#22810;&#20256;&#24863;&#22120;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
SMORE: Similarity-based Hyperdimensional Domain Adaptation for Multi-Sensor Time Series Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13233
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;SMORE&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#20256;&#24863;&#22120;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#39046;&#22495;&#33258;&#36866;&#24212;&#31639;&#27861;&#65292;&#21033;&#29992;&#39640;&#32500;&#35745;&#31639;&#30340;&#39640;&#25928;&#21644;&#24182;&#34892;&#25805;&#20316;&#65292;&#21160;&#24577;&#23450;&#21046;&#27979;&#35797;&#27169;&#22411;&#20197;&#20943;&#36731;&#25968;&#25454;&#20998;&#24067;&#20559;&#31227;&#24102;&#26469;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29289;&#32852;&#32593;(IoT)&#30340;&#23454;&#38469;&#24212;&#29992;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;(ML)&#31639;&#27861;&#20998;&#26512;&#30001;&#30456;&#20114;&#36830;&#25509;&#30340;&#20256;&#24863;&#22120;&#25910;&#38598;&#30340;&#26102;&#38388;&#24207;&#21015;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#22312;&#37096;&#32626;&#22312;&#19982;&#35757;&#32451;&#25968;&#25454;&#19981;&#21516;&#30340;&#25968;&#25454;&#20998;&#24067;&#19978;&#30340;&#27169;&#22411;&#26102;&#65292;&#25968;&#25454;&#39537;&#21160;&#30340;ML&#20013;&#22266;&#26377;&#30340;&#25361;&#25112;&#8212;&#8212;&#20998;&#24067;&#20559;&#31227;&#20250;&#26174;&#33879;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#38656;&#35201;&#25429;&#33719;&#22810;&#20256;&#24863;&#22120;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#22797;&#26434;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#24448;&#24448;&#36229;&#36807;&#20102;&#20170;&#22825;&#36793;&#32536;&#35774;&#22791;&#30340;&#33021;&#21147;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SMORE&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#36164;&#28304;&#39640;&#25928;&#30340;&#22810;&#20256;&#24863;&#22120;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#39046;&#22495;&#33258;&#36866;&#24212;(DA)&#31639;&#27861;&#65292;&#21033;&#29992;&#20102;&#36229;&#39640;&#32500;&#35745;&#31639;&#30340;&#39640;&#25928;&#21644;&#24182;&#34892;&#25805;&#20316;&#12290; SMORE&#21160;&#24577;&#22320;&#23450;&#21046;&#27979;&#35797;&#27169;&#22411;&#65292;&#26126;&#30830;&#32771;&#34385;&#27599;&#20010;&#26679;&#26412;&#30340;&#39046;&#22495;&#19978;&#19979;&#25991;&#65292;&#20197;&#20943;&#36731;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13233v1 Announce Type: new  Abstract: Many real-world applications of the Internet of Things (IoT) employ machine learning (ML) algorithms to analyze time series information collected by interconnected sensors. However, distribution shift, a fundamental challenge in data-driven ML, arises when a model is deployed on a data distribution different from the training data and can substantially degrade model performance. Additionally, increasingly sophisticated deep neural networks (DNNs) are required to capture intricate spatial and temporal dependencies in multi-sensor time series data, often exceeding the capabilities of today's edge devices. In this paper, we propose SMORE, a novel resource-efficient domain adaptation (DA) algorithm for multi-sensor time series classification, leveraging the efficient and parallel operations of hyperdimensional computing. SMORE dynamically customizes test-time models with explicit consideration of the domain context of each sample to mitigate
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#35757;&#32451;&#36807;&#31243;DPO-Positive&#65288;DPOP&#65289;&#65292;&#20197;&#36991;&#20813;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#20013;&#28508;&#22312;&#30340;&#22833;&#36133;&#27169;&#24335;&#65292;&#24182;&#21457;&#29616;DPOP&#26126;&#26174;&#20248;&#20110;DPO&#12290;</title><link>https://arxiv.org/abs/2402.13228</link><description>&lt;p&gt;
Smaug&#65306;&#20351;&#29992;DPO-Positive&#20462;&#22797;&#20559;&#22909;&#20248;&#21270;&#30340;&#22833;&#36133;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13228
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#35757;&#32451;&#36807;&#31243;DPO-Positive&#65288;DPOP&#65289;&#65292;&#20197;&#36991;&#20813;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#20013;&#28508;&#22312;&#30340;&#22833;&#36133;&#27169;&#24335;&#65292;&#24182;&#21457;&#29616;DPOP&#26126;&#26174;&#20248;&#20110;DPO&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#22312;&#26174;&#33879;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25512;&#29702;&#12289;&#24635;&#32467;&#21644;&#23545;&#40784;&#31561;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290; DPO&#20351;&#29992;&#39318;&#36873;&#21644;&#38750;&#39318;&#36873;&#25968;&#25454;&#23545;&#27169;&#22411;&#36873;&#25321;&#19968;&#20010;&#21709;&#24212;&#32780;&#19981;&#26159;&#21478;&#19968;&#20010;&#30340;&#8220;&#30456;&#23545;&#8221;&#27010;&#29575;&#36827;&#34892;&#24314;&#27169;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;&#29702;&#35770;&#19978;&#34920;&#26126;&#65292;&#21482;&#35201;&#39318;&#36873;&#21644;&#38750;&#39318;&#36873;&#31867;&#21035;&#20043;&#38388;&#30340;&#30456;&#23545;&#27010;&#29575;&#22686;&#21152;&#65292;&#26631;&#20934;DPO&#25439;&#22833;&#23601;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#23545;&#39318;&#36873;&#31034;&#20363;&#30340;&#21487;&#33021;&#24615;&#38477;&#20302;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#23454;&#35777;&#19978;&#23637;&#31034;&#20102;&#24403;&#22312;&#24120;&#35265;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;LLMs&#26102;&#65292;&#23588;&#20854;&#26159;&#22312;&#23436;&#25104;&#20043;&#38388;&#30340;&#32534;&#36753;&#36317;&#31163;&#36739;&#30701;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;&#20250;&#20986;&#29616;&#36825;&#31181;&#29616;&#35937;&#12290;&#21033;&#29992;&#36825;&#20123;&#35265;&#35299;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;DPO-Positive&#65288;DPOP&#65289;&#65292;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#35757;&#32451;&#36807;&#31243;&#65292;&#36991;&#20813;&#20102;&#36825;&#31181;&#22833;&#36133;&#27169;&#24335;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;DPOP&#26126;&#26174;&#20248;&#20110;DPO&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13228v1 Announce Type: cross  Abstract: Direct Preference Optimisation (DPO) is effective at significantly improving the performance of large language models (LLMs) on downstream tasks such as reasoning, summarisation, and alignment. Using pairs of preferred and dispreferred data, DPO models the \textit{relative} probability of picking one response over another. In this work, first we show theoretically that the standard DPO loss can lead to a \textit{reduction} of the model's likelihood of the preferred examples, as long as the relative probability between the preferred and dispreferred classes increases. We then show empirically that this phenomenon occurs when fine-tuning LLMs on common datasets, especially datasets in which the edit distance between pairs of completions is low. Using these insights, we design DPO-Positive (DPOP), a new loss function and training procedure which avoids this failure mode. Surprisingly, we also find that DPOP significantly outperforms DPO a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#30005;&#21160;&#27773;&#36710;&#20805;&#30005;&#31449;&#27169;&#22411;&#65292;&#36890;&#36807;&#29992;&#25143;&#34892;&#20026;&#24314;&#27169;&#21644;&#38543;&#26426;&#35268;&#21010;&#65292;&#35299;&#20915;&#20102;&#20805;&#30005;&#20250;&#35805;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#20248;&#21270;&#25104;&#26412;&#24182;&#25552;&#39640;&#29992;&#25143;&#28385;&#24847;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.13224</link><description>&lt;p&gt;
&#36890;&#36807;&#29992;&#25143;&#34892;&#20026;&#24314;&#27169;&#21644;&#38543;&#26426;&#35268;&#21010;&#25511;&#21046;&#22823;&#22411;&#30005;&#21160;&#27773;&#36710;&#20805;&#30005;&#31449;
&lt;/p&gt;
&lt;p&gt;
Controlling Large Electric Vehicle Charging Stations via User Behavior Modeling and Stochastic Programming
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#30005;&#21160;&#27773;&#36710;&#20805;&#30005;&#31449;&#27169;&#22411;&#65292;&#36890;&#36807;&#29992;&#25143;&#34892;&#20026;&#24314;&#27169;&#21644;&#38543;&#26426;&#35268;&#21010;&#65292;&#35299;&#20915;&#20102;&#20805;&#30005;&#20250;&#35805;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#20248;&#21270;&#25104;&#26412;&#24182;&#25552;&#39640;&#29992;&#25143;&#28385;&#24847;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#30005;&#21160;&#27773;&#36710;&#20805;&#30005;&#31449;&#65288;EVCS&#65289;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#34701;&#21512;&#20102;&#30495;&#23454;&#19990;&#30028;&#30340;&#32422;&#26463;&#26465;&#20214;&#65292;&#22914;&#25554;&#27133;&#21151;&#29575;&#38480;&#21046;&#12289;&#21512;&#21516;&#38408;&#20540;&#36229;&#38480;&#24809;&#32602;&#20197;&#21450;&#30005;&#21160;&#27773;&#36710;&#65288;EVs&#65289;&#30340;&#26089;&#26399;&#26029;&#24320;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#25511;&#21046;EVCS&#30340;&#38382;&#39064;&#24418;&#24335;&#65292;&#24182;&#23454;&#26045;&#20102;&#20004;&#31181;&#22810;&#38454;&#27573;&#38543;&#26426;&#35268;&#21010;&#26041;&#27861;&#65292;&#21033;&#29992;&#29992;&#25143;&#25552;&#20379;&#30340;&#20449;&#24687;&#65292;&#21363;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#21644;&#20108;&#38454;&#27573;&#38543;&#26426;&#35268;&#21010;&#12290;&#35813;&#27169;&#22411;&#35299;&#20915;&#20102;&#20805;&#30005;&#20250;&#35805;&#24320;&#22987;&#21644;&#32467;&#26463;&#26102;&#38388;&#20197;&#21450;&#33021;&#37327;&#38656;&#27714;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#22522;&#20110;&#39547;&#30041;&#26102;&#38388;&#20381;&#36182;&#38543;&#26426;&#36807;&#31243;&#30340;&#29992;&#25143;&#34892;&#20026;&#27169;&#22411;&#22686;&#24378;&#20102;&#25104;&#26412;&#38477;&#20302;&#30340;&#21516;&#26102;&#20445;&#25345;&#23458;&#25143;&#28385;&#24847;&#24230;&#12290;&#36890;&#36807;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;22&#22825;&#27169;&#25311;&#23637;&#31034;&#20102;&#20004;&#31181;&#25552;&#20986;&#26041;&#27861;&#30456;&#23545;&#20110;&#20004;&#20010;&#22522;&#32447;&#30340;&#20248;&#21183;&#12290;&#20004;&#38454;&#27573;&#26041;&#27861;&#35777;&#26126;&#20102;&#38024;&#23545;&#26089;&#26399;&#26029;&#24320;&#30340;&#40065;&#26834;&#24615;&#65292;&#32771;&#34385;&#20102;&#26356;&#22810;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13224v1 Announce Type: cross  Abstract: This paper introduces an Electric Vehicle Charging Station (EVCS) model that incorporates real-world constraints, such as slot power limitations, contract threshold overruns penalties, or early disconnections of electric vehicles (EVs). We propose a formulation of the problem of EVCS control under uncertainty, and implement two Multi-Stage Stochastic Programming approaches that leverage user-provided information, namely, Model Predictive Control and Two-Stage Stochastic Programming. The model addresses uncertainties in charging session start and end times, as well as in energy demand. A user's behavior model based on a sojourn-time-dependent stochastic process enhances cost reduction while maintaining customer satisfaction. The benefits of the two proposed methods are showcased against two baselines over a 22-day simulation using a real-world dataset. The two-stage approach proves robust against early disconnections, considering a more
&lt;/p&gt;</description></item><item><title>&#22270;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30446;&#21069;&#20027;&#35201;&#38598;&#20013;&#22312;&#39044;&#27979;&#20998;&#23376;&#21644;&#26448;&#26009;&#30340;&#30446;&#26631;&#29305;&#24615;&#65292;&#32780;&#23578;&#26410;&#36798;&#21040;&#29983;&#25104;&#33021;&#21147;&#19982;&#20854;&#20182;&#39046;&#22495;&#30340;&#27700;&#24179;&#12290;</title><link>https://arxiv.org/abs/2402.13221</link><description>&lt;p&gt;
CHILI: &#29992;&#20110;&#25512;&#36827;&#22270;&#26426;&#22120;&#23398;&#20064;&#30340;&#21270;&#23398;&#20449;&#24687;&#30340;&#22823;&#22411;&#26080;&#26426;&#32435;&#31859;&#26448;&#26009;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
CHILI: Chemically-Informed Large-scale Inorganic Nanomaterials Dataset for Advancing Graph Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13221
&lt;/p&gt;
&lt;p&gt;
&#22270;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30446;&#21069;&#20027;&#35201;&#38598;&#20013;&#22312;&#39044;&#27979;&#20998;&#23376;&#21644;&#26448;&#26009;&#30340;&#30446;&#26631;&#29305;&#24615;&#65292;&#32780;&#23578;&#26410;&#36798;&#21040;&#29983;&#25104;&#33021;&#21147;&#19982;&#20854;&#20182;&#39046;&#22495;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#26426;&#22120;&#23398;&#20064;&#30340;&#36827;&#23637;&#20027;&#35201;&#21463;&#21270;&#23398;&#24212;&#29992;&#30340;&#39537;&#21160;&#65292;&#22240;&#20026;&#22270;&#19968;&#30452;&#26159;&#20998;&#23376;&#26368;&#20855;&#34920;&#29616;&#21147;&#30340;&#34920;&#31034;&#24418;&#24335;&#12290;&#34429;&#28982;&#26089;&#26399;&#30340;&#22270;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#23567;&#26377;&#26426;&#20998;&#23376;&#19978;&#65292;&#20294;&#26368;&#36817;&#65292;&#22270;&#26426;&#22120;&#23398;&#20064;&#30340;&#33539;&#22260;&#24050;&#32463;&#25193;&#23637;&#21040;&#21253;&#25324;&#26080;&#26426;&#26448;&#26009;&#12290;&#24314;&#27169;&#26080;&#26426;&#26230;&#20307;&#26448;&#26009;&#30340;&#21608;&#26399;&#24615;&#21644;&#23545;&#31216;&#24615;&#24102;&#26469;&#29420;&#29305;&#25361;&#25112;&#65292;&#29616;&#26377;&#30340;&#22270;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26080;&#27861;&#35299;&#20915;&#12290;&#36716;&#21521;&#26080;&#26426;&#32435;&#31859;&#26448;&#26009;&#20250;&#22686;&#21152;&#22797;&#26434;&#24615;&#65292;&#22240;&#20026;&#27599;&#20010;&#22270;&#20013;&#33410;&#28857;&#25968;&#37327;&#30340;&#33539;&#22260;&#21487;&#33021;&#24456;&#24191;&#65288;$10$&#21040;$10^5$&#65289;&#12290;&#29616;&#26377;&#22270;&#26426;&#22120;&#23398;&#20064;&#30340;&#20027;&#35201;&#37325;&#28857;&#26159;&#36890;&#36807;&#22270;&#20316;&#20026;&#36755;&#20837;&#26469;&#39044;&#27979;&#30446;&#26631;&#29305;&#24615;&#65292;&#26469;&#34920;&#24449;&#20998;&#23376;&#21644;&#26448;&#26009;&#12290;&#20294;&#26159;&#65292;&#22270;&#26426;&#22120;&#23398;&#20064;&#26368;&#28608;&#21160;&#20154;&#24515;&#30340;&#24212;&#29992;&#23558;&#22312;&#20854;&#29983;&#25104;&#33021;&#21147;&#26041;&#38754;&#65292;&#30446;&#21069;&#19982;&#22270;&#20687;&#25110;&#25991;&#26412;&#31561;&#20854;&#20182;&#39046;&#22495;&#36824;&#19981;&#22312;&#21516;&#19968;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13221v1 Announce Type: new  Abstract: Advances in graph machine learning (ML) have been driven by applications in chemistry as graphs have remained the most expressive representations of molecules. While early graph ML methods focused primarily on small organic molecules, recently, the scope of graph ML has expanded to include inorganic materials. Modelling the periodicity and symmetry of inorganic crystalline materials poses unique challenges, which existing graph ML methods are unable to address. Moving to inorganic nanomaterials increases complexity as the scale of number of nodes within each graph can be broad ($10$ to $10^5$). The bulk of existing graph ML focuses on characterising molecules and materials by predicting target properties with graphs as input. However, the most exciting applications of graph ML will be in their generative capabilities, which is currently not at par with other domains such as images or text.   We invite the graph ML community to address th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#38376;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#38598;&#25104;&#20102;AI&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#26088;&#22312;&#25913;&#21892;&#25511;&#21046;&#23460;&#25805;&#20316;&#21592;&#30340;&#24037;&#20316;&#25928;&#29575;&#21644;&#24773;&#22659;&#24847;&#35782;&#65292;&#24182;&#20026;&#20854;&#25552;&#20379;&#26681;&#25454;&#31995;&#32479;&#21644;&#20154;&#31867;&#34920;&#29616;&#29366;&#24577;&#37327;&#36523;&#23450;&#21046;&#30340;&#24178;&#39044;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.13219</link><description>&lt;p&gt;
&#20998;&#26512;&#25805;&#20316;&#21592;&#29366;&#24577;&#21644;AI&#22686;&#24378;&#20915;&#31574;&#25903;&#25345;&#23545;&#25511;&#21046;&#23460;&#30340;&#24433;&#21709;&#65306;&#19968;&#31181;&#20154;&#22312;&#22238;&#36335;&#20013;&#30340;&#19987;&#38376;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#24178;&#39044;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Analyzing Operator States and the Impact of AI-Enhanced Decision Support in Control Rooms: A Human-in-the-Loop Specialized Reinforcement Learning Framework for Intervention Strategies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13219
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#38376;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#38598;&#25104;&#20102;AI&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#26088;&#22312;&#25913;&#21892;&#25511;&#21046;&#23460;&#25805;&#20316;&#21592;&#30340;&#24037;&#20316;&#25928;&#29575;&#21644;&#24773;&#22659;&#24847;&#35782;&#65292;&#24182;&#20026;&#20854;&#25552;&#20379;&#26681;&#25454;&#31995;&#32479;&#21644;&#20154;&#31867;&#34920;&#29616;&#29366;&#24577;&#37327;&#36523;&#23450;&#21046;&#30340;&#24178;&#39044;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22797;&#26434;&#30340;&#24037;&#19994;&#21644;&#21270;&#23398;&#36807;&#31243;&#25511;&#21046;&#23460;&#20013;&#65292;&#26377;&#25928;&#30340;&#20915;&#31574;&#23545;&#23433;&#20840;&#24615;&#21644;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20013;&#30340;&#23454;&#39564;&#35780;&#20272;&#20102;&#38598;&#25104;&#21040;&#25913;&#36827;&#30340;&#20154;&#26426;&#30028;&#38754;&#20013;&#30340;AI&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#30340;&#24433;&#21709;&#21644;&#24212;&#29992;&#65292;&#20351;&#29992;&#21160;&#24577;&#24433;&#21709;&#22270;&#12289;&#38544;&#39532;&#23572;&#31185;&#22827;&#27169;&#22411;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#12290;&#22686;&#24378;&#30340;&#25903;&#25345;&#31995;&#32479;&#26088;&#22312;&#20943;&#23569;&#25805;&#20316;&#21592;&#30340;&#24037;&#20316;&#36127;&#33655;&#65292;&#25552;&#39640;&#24773;&#22659;&#24847;&#35782;&#65292;&#24182;&#26681;&#25454;&#31995;&#32479;&#21644;&#20154;&#31867;&#34920;&#29616;&#30340;&#24403;&#21069;&#29366;&#24577;&#20026;&#25805;&#20316;&#21592;&#25552;&#20379;&#19981;&#21516;&#30340;&#24178;&#39044;&#31574;&#30053;&#12290;&#36825;&#26679;&#30340;&#31995;&#32479;&#22312;&#20449;&#24687;&#36807;&#36733;&#30340;&#24773;&#20917;&#19979;&#29305;&#21035;&#26377;&#29992;&#65292;&#24403;&#35768;&#22810;&#35686;&#25253;&#21644;&#36755;&#20837;&#21516;&#26102;&#21576;&#29616;&#22312;&#21516;&#19968;&#20010;&#26102;&#38388;&#31383;&#21475;&#20869;&#65292;&#25110;&#32773;&#22312;&#22521;&#35757;&#26399;&#38388;&#23545;&#21021;&#32423;&#25805;&#20316;&#21592;&#26469;&#35828;&#23588;&#20854;&#26377;&#29992;&#12290;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#20132;&#21449;&#25968;&#25454;&#20998;&#26512;&#65292;&#28041;&#21450;47&#21517;&#21442;&#19982;&#32773;&#21644;&#21508;&#31181;&#25968;&#25454;&#26469;&#28304;&#65292;&#22914;&#26234;&#33021;&#25163;&#34920;&#25351;&#26631;&#12289;&#30524;&#21160;&#25968;&#25454;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13219v1 Announce Type: new  Abstract: In complex industrial and chemical process control rooms, effective decision-making is crucial for safety and effi- ciency. The experiments in this paper evaluate the impact and applications of an AI-based decision support system integrated into an improved human-machine interface, using dynamic influ- ence diagrams, a hidden Markov model, and deep reinforcement learning. The enhanced support system aims to reduce operator workload, improve situational awareness, and provide different intervention strategies to the operator adapted to the current state of both the system and human performance. Such a system can be particularly useful in cases of information overload when many alarms and inputs are presented all within the same time window, or for junior operators during training. A comprehensive cross-data analysis was conducted, involving 47 participants and a diverse range of data sources such as smartwatch metrics, eye- tracking data,
&lt;/p&gt;</description></item><item><title>&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;&#20219;&#21153;&#20013;&#65292;&#22522;&#20110;&#26368;&#22823;softmax&#27010;&#29575;&#65288;MSPs&#65289;&#30340;&#27169;&#22411;&#39044;&#27979;&#26041;&#27861;&#26377;&#21161;&#20110;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#27491;&#30830;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26681;&#25454;MSP&#26377;&#36873;&#25321;&#22320;&#24323;&#26435;&#30340;&#31574;&#30053;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.13213</link><description>&lt;p&gt;
&#36719;&#26368;&#22823;&#27010;&#29575;&#65288;&#22823;&#37096;&#20998;&#26102;&#20505;&#65289;&#22312;&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;&#20219;&#21153;&#20013;&#39044;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27491;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Softmax Probabilities (Mostly) Predict Large Language Model Correctness on Multiple-Choice Q&amp;A
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13213
&lt;/p&gt;
&lt;p&gt;
&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;&#20219;&#21153;&#20013;&#65292;&#22522;&#20110;&#26368;&#22823;softmax&#27010;&#29575;&#65288;MSPs&#65289;&#30340;&#27169;&#22411;&#39044;&#27979;&#26041;&#27861;&#26377;&#21161;&#20110;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#27491;&#30830;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26681;&#25454;MSP&#26377;&#36873;&#25321;&#22320;&#24323;&#26435;&#30340;&#31574;&#30053;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#36807;&#24230;&#33258;&#20449;&#20173;&#28982;&#26159;&#19968;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#20551;&#35774;&#22312;&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;&#20219;&#21153;&#20013;&#65292;&#38169;&#35823;&#31572;&#26696;&#23558;&#19982;&#26368;&#22823;softmax&#27010;&#29575;&#65288;MSPs&#65289;&#36739;&#23567;&#30456;&#20851;&#65292;&#30456;&#27604;&#20043;&#19979;&#27491;&#30830;&#31572;&#26696;&#36739;&#22823;&#12290;&#25105;&#20204;&#22312;&#21313;&#20010;&#24320;&#28304;LLMs&#21644;&#20116;&#20010;&#25968;&#25454;&#38598;&#19978;&#20840;&#38754;&#35780;&#20272;&#20102;&#36825;&#19968;&#20551;&#35774;&#65292;&#22312;&#34920;&#29616;&#33391;&#22909;&#30340;&#21407;&#22987;&#38382;&#31572;&#20219;&#21153;&#20013;&#21457;&#29616;&#20102;&#23545;&#25105;&#20204;&#20551;&#35774;&#30340;&#24378;&#26377;&#21147;&#35777;&#25454;&#12290;&#23545;&#20110;&#34920;&#29616;&#26368;&#20339;&#30340;&#20845;&#20010;LLMs&#65292;&#20174;MSP&#23548;&#20986;&#30340;AUROC&#22312;59/60&#20010;&#23454;&#20363;&#20013;&#37117;&#20248;&#20110;&#38543;&#26426;&#26426;&#20250;&#65292;p &lt; 10^{-4}&#12290;&#22312;&#36825;&#20845;&#20010;LLMs&#20013;&#65292;&#24179;&#22343;AUROC&#33539;&#22260;&#22312;60%&#33267;69%&#20043;&#38388;&#12290;&#21033;&#29992;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#24323;&#26435;&#36873;&#39033;&#30340;&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;&#20219;&#21153;&#65292;&#24182;&#23637;&#31034;&#36890;&#36807;&#26681;&#25454;&#21021;&#22987;&#27169;&#22411;&#21709;&#24212;&#30340;MSP&#26377;&#36873;&#25321;&#22320;&#24323;&#26435;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#29992;&#39044;softmax logits&#32780;&#19981;&#26159;softmax&#36827;&#34892;&#20102;&#30456;&#21516;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13213v1 Announce Type: cross  Abstract: Although large language models (LLMs) perform impressively on many tasks, overconfidence remains a problem. We hypothesized that on multiple-choice Q&amp;A tasks, wrong answers would be associated with smaller maximum softmax probabilities (MSPs) compared to correct answers. We comprehensively evaluate this hypothesis on ten open-source LLMs and five datasets, and find strong evidence for our hypothesis among models which perform well on the original Q&amp;A task. For the six LLMs with the best Q&amp;A performance, the AUROC derived from the MSP was better than random chance with p &lt; 10^{-4} in 59/60 instances. Among those six LLMs, the average AUROC ranged from 60% to 69%. Leveraging these findings, we propose a multiple-choice Q&amp;A task with an option to abstain and show that performance can be improved by selectively abstaining based on the MSP of the initial model response. We also run the same experiments with pre-softmax logits instead of sof
&lt;/p&gt;</description></item><item><title>Soft Self-Consistency (Soft-SC)&#36890;&#36807;&#36719;&#21270;&#35780;&#20998;&#26631;&#20934;&#26367;&#20195;&#33258;&#19968;&#33268;&#24615;&#30340;&#22810;&#25968;&#25237;&#31080;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#22312;&#28041;&#21450;&#29983;&#25104;&#22810;&#20010;&#21160;&#20316;&#30340;&#38271;&#26399;&#20114;&#21160;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;</title><link>https://arxiv.org/abs/2402.13212</link><description>&lt;p&gt;
&#36719;&#33258;&#19968;&#33268;&#24615;&#25913;&#21892;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Soft Self-Consistency Improves Language Model Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13212
&lt;/p&gt;
&lt;p&gt;
Soft Self-Consistency (Soft-SC)&#36890;&#36807;&#36719;&#21270;&#35780;&#20998;&#26631;&#20934;&#26367;&#20195;&#33258;&#19968;&#33268;&#24615;&#30340;&#22810;&#25968;&#25237;&#31080;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#22312;&#28041;&#21450;&#29983;&#25104;&#22810;&#20010;&#21160;&#20316;&#30340;&#38271;&#26399;&#20114;&#21160;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#21487;&#20197;&#36890;&#36807;&#23545;&#22810;&#20010;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#25277;&#26679;&#21644;&#35780;&#20998;&#26469;&#25913;&#36827;&#65292;&#20197;&#36873;&#25321;&#26368;&#32456;&#31572;&#26696;&#12290;&#24403;&#21069;&#30340;&#8220;&#25277;&#26679;&#21644;&#36873;&#25321;&#8221;&#26041;&#27861;&#22914;&#33258;&#19968;&#33268;&#24615;&#65288;SC&#65289;&#20381;&#36182;&#20110;&#22810;&#25968;&#25237;&#31080;&#26469;&#35780;&#20998;&#31572;&#26696;&#12290;&#28982;&#32780;&#65292;&#24403;&#20219;&#21153;&#26377;&#35768;&#22810;&#19981;&#21516;&#19988;&#26377;&#25928;&#30340;&#31572;&#26696;&#26102;&#65292;&#36890;&#36807;&#25237;&#31080;&#36827;&#34892;&#36873;&#25321;&#38656;&#35201;&#22823;&#37327;&#26679;&#26412;&#12290;&#36825;&#20351;&#24471;SC&#22312;&#28041;&#21450;&#39034;&#24207;&#29983;&#25104;&#22810;&#20010;&#21160;&#20316;&#65288;&#31572;&#26696;&#65289;&#30340;&#20114;&#21160;&#20219;&#21153;&#26102;&#25104;&#26412;&#36807;&#39640;&#12290;&#22312;&#30830;&#23450;&#22823;&#22810;&#25968;&#25237;&#31080;&#26410;&#33021;&#20026;&#27492;&#31867;&#20219;&#21153;&#25552;&#20379;&#19968;&#33268;&#30340;&#25910;&#30410;&#20043;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#36719;&#21270;&#35780;&#20998;&#26631;&#20934;&#26469;&#25552;&#39640;&#25104;&#21151;&#29575;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#36719;&#33258;&#19968;&#33268;&#24615;&#65288;Soft-SC&#65289;&#65292;&#23427;&#29992;&#27169;&#22411;&#21487;&#33021;&#24615;&#35745;&#31639;&#36830;&#32493;&#20998;&#25968;&#26469;&#21462;&#20195;SC&#30340;&#19981;&#36830;&#32493;&#35780;&#20998;&#65292;&#21363;&#20351;&#21160;&#20316;&#20998;&#24067;&#31232;&#30095;&#65292;&#20063;&#20801;&#35768;&#36873;&#25321;&#12290;&#36719;&#33258;&#19968;&#33268;&#24615;&#22312;&#38271;&#26399;&#20114;&#21160;&#20219;&#21153;&#19978;&#25552;&#39640;&#20102;&#24615;&#33021;&#21644;&#25928;&#29575;&#65292;&#38656;&#35201;&#36739;&#23569;&#30340;&#26679;&#26412;&#21644;&#25237;&#31080;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13212v1 Announce Type: cross  Abstract: Generations from large language models (LLMs) can be improved by sampling and scoring multiple solutions to select a final answer. Current "sample and select" methods such as self-consistency (SC) rely on majority voting to score answers. However, when tasks have many distinct and valid answers, selection by voting requires a large number of samples. This makes SC prohibitively expensive for interactive tasks that involve generating multiple actions (answers) sequentially. After establishing that majority voting fails to provide consistent gains on such tasks, we demonstrate how to increase success rates by softening the scoring criterion. We introduce Soft Self-Consistency (Soft-SC), which replaces SC's discontinuous scoring with a continuous score computed from model likelihoods, allowing for selection even when actions are sparsely distributed. Soft-SC improves both performance and efficiency on long-horizon interactive tasks, requi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35757;&#32451;&#36125;&#21494;&#26031;&#22870;&#21169;&#27169;&#22411;&#65292;&#21487;&#20197;&#25104;&#21151;&#32531;&#35299;&#22870;&#21169;&#30340;&#36807;&#24230;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.13210</link><description>&lt;p&gt;
Bayesian Reward Models for LLM Alignment
&lt;/p&gt;
&lt;p&gt;
Bayesian Reward Models for LLM Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13210
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35757;&#32451;&#36125;&#21494;&#26031;&#22870;&#21169;&#27169;&#22411;&#65292;&#21487;&#20197;&#25104;&#21151;&#32531;&#35299;&#22870;&#21169;&#30340;&#36807;&#24230;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#30830;&#20445;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#22238;&#22797;&#26377;&#30410;&#19988;&#26080;&#27602;&#65292;&#36890;&#24120;&#25105;&#20204;&#20250;&#22312;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#19978;&#24494;&#35843;&#22870;&#21169;&#27169;&#22411;&#12290;&#28982;&#21518;&#25105;&#20204;&#36873;&#25321;&#20855;&#26377;&#39640;&#22870;&#21169;&#30340;&#31574;&#30053;&#22238;&#22797;&#65288;best-of-n&#25277;&#26679;&#65289;&#65292;&#25110;&#32773;&#36827;&#19968;&#27493;&#20248;&#21270;&#31574;&#30053;&#20197;&#29983;&#25104;&#20855;&#26377;&#39640;&#22870;&#21169;&#30340;&#22238;&#22797;&#65288;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#36807;&#31243;&#23481;&#26131;&#21463;&#21040;&#22870;&#21169;&#36807;&#24230;&#20248;&#21270;&#25110;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#36873;&#23450;&#30340;&#22238;&#22797;&#30001;&#20110;&#22870;&#21169;&#27169;&#22411;&#20013;&#30340;&#38169;&#35823;&#32780;&#20855;&#26377;&#39640;&#22870;&#21169;&#65292;&#32780;&#19981;&#26159;&#30495;&#23454;&#20559;&#22909;&#12290;&#36825;&#19968;&#38382;&#39064;&#22312;&#25552;&#31034;&#25110;&#22238;&#22797;&#20559;&#31163;&#35757;&#32451;&#25968;&#25454;&#26102;&#23588;&#20026;&#20005;&#37325;&#12290;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#36125;&#21494;&#26031;&#22870;&#21169;&#27169;&#22411;&#26469;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#36825;&#31181;&#27169;&#22411;&#22312;&#36828;&#31163;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#26102;&#20250;&#20135;&#29983;&#26356;&#39640;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;Laplace-LoRA&#65288;Yang&#31561;&#20154;&#65292;2024&#65289;&#35757;&#32451;&#20102;&#36125;&#21494;&#26031;&#22870;&#21169;&#27169;&#22411;&#65292;&#21457;&#29616;&#30001;&#27492;&#20135;&#29983;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21487;&#20197;&#25104;&#21151;&#32531;&#35299;&#22870;&#21169;&#30340;&#36807;&#24230;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13210v1 Announce Type: new  Abstract: To ensure that large language model (LLM) responses are helpful and non-toxic, we usually fine-tune a reward model on human preference data. We then select policy responses with high rewards (best-of-n sampling) or further optimize the policy to produce responses with high rewards (reinforcement learning from human feedback). However, this process is vulnerable to reward overoptimization or hacking, in which the responses selected have high rewards due to errors in the reward model rather than a genuine preference. This is especially problematic as the prompt or response diverges from the training data. It should be possible to mitigate these issues by training a Bayesian reward model, which signals higher uncertainty further from the training data distribution. Therefore, we trained Bayesian reward models using Laplace-LoRA (Yang et al., 2024) and found that the resulting uncertainty estimates can successfully mitigate reward overoptimi
&lt;/p&gt;</description></item><item><title>SONATA&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#36827;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#30828;&#20214;&#24863;&#30693;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65292;&#26088;&#22312;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#21442;&#25968;&#19982;&#30828;&#20214;&#24863;&#30693;NAS&#20248;&#21270;&#30446;&#26631;&#20043;&#38388;&#20851;&#31995;&#30340;&#25361;&#25112;&#65292;&#24182;&#21033;&#29992;&#35780;&#20272;&#25968;&#25454;&#25913;&#36827;&#20248;&#21270;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.13204</link><description>&lt;p&gt;
SONATA&#65306;&#38754;&#21521;&#30828;&#20214;&#24863;&#30693;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#30340;&#33258;&#36866;&#24212;&#36827;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
SONATA: Self-adaptive Evolutionary Framework for Hardware-aware Neural Architecture Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13204
&lt;/p&gt;
&lt;p&gt;
SONATA&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#36827;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#30828;&#20214;&#24863;&#30693;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65292;&#26088;&#22312;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#21442;&#25968;&#19982;&#30828;&#20214;&#24863;&#30693;NAS&#20248;&#21270;&#30446;&#26631;&#20043;&#38388;&#20851;&#31995;&#30340;&#25361;&#25112;&#65292;&#24182;&#21033;&#29992;&#35780;&#20272;&#25968;&#25454;&#25913;&#36827;&#20248;&#21270;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#30001;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#25512;&#21160;&#65292;&#35201;&#27714;&#21019;&#26032;&#30340;&#31070;&#32463;&#26550;&#26500;&#35774;&#35745;&#65292;&#29305;&#21035;&#26159;&#22312;&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#31995;&#32479;&#30340;&#21463;&#38480;&#29615;&#22659;&#20013;&#65292;&#20197;&#24179;&#34913;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;&#30828;&#20214;&#24863;&#30693;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;HW-aware NAS&#65289;&#25104;&#20026;&#19968;&#31181;&#26377;&#21560;&#24341;&#21147;&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#22810;&#30446;&#26631;&#20248;&#21270;&#26041;&#27861;&#65292;&#22914;&#36827;&#21270;&#31639;&#27861;&#65292;&#33258;&#21160;&#21270;&#35774;&#35745;NN&#12290;&#28982;&#32780;&#65292;NN&#35774;&#35745;&#21442;&#25968;&#19982;HW-aware NAS&#20248;&#21270;&#30446;&#26631;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#20173;&#28982;&#26159;&#19968;&#20010;&#19981;&#36275;&#25506;&#32034;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#24573;&#35270;&#20102;&#26377;&#25928;&#21033;&#29992;&#36825;&#19968;&#30693;&#35782;&#26469;&#30456;&#24212;&#22320;&#25351;&#23548;&#25628;&#32034;&#36807;&#31243;&#30340;&#26426;&#20250;&#12290;&#27492;&#22806;&#65292;&#25628;&#32034;&#36807;&#31243;&#20013;&#20135;&#29983;&#30340;&#22823;&#37327;&#35780;&#20272;&#25968;&#25454;&#20855;&#26377;&#26410;&#24320;&#21457;&#30340;&#28508;&#21147;&#65292;&#21487;&#29992;&#20110;&#20248;&#21270;&#31574;&#30053;&#21644;&#25913;&#36827;&#24085;&#32047;&#25176;&#21069;&#27839;&#30340;&#36924;&#36817;&#12290;&#38024;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;SONATA&#65292;&#19968;&#20010;&#33258;&#36866;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13204v1 Announce Type: cross  Abstract: Recent advancements in Artificial Intelligence (AI), driven by Neural Networks (NN), demand innovative neural architecture designs, particularly within the constrained environments of Internet of Things (IoT) systems, to balance performance and efficiency. HW-aware Neural Architecture Search (HW-aware NAS) emerges as an attractive strategy to automate the design of NN using multi-objective optimization approaches, such as evolutionary algorithms. However, the intricate relationship between NN design parameters and HW-aware NAS optimization objectives remains an underexplored research area, overlooking opportunities to effectively leverage this knowledge to guide the search process accordingly. Furthermore, the large amount of evaluation data produced during the search holds untapped potential for refining the optimization strategy and improving the approximation of the Pareto front. Addressing these issues, we propose SONATA, a self-ad
&lt;/p&gt;</description></item><item><title>&#23558;&#27169;&#20223;&#23398;&#20064;&#38382;&#39064;&#35270;&#20026;&#26465;&#20214;&#24207;&#21015;&#24314;&#27169;&#20219;&#21153;&#65292;&#36890;&#36807;&#35757;&#32451;&#20915;&#31574;&#21464;&#25442;&#22120;&#24182;&#20351;&#29992;&#22870;&#21169;&#26426;&#21046;&#65292;&#23558;&#29983;&#25104;&#27169;&#22411;&#21387;&#32553;&#20197;&#36866;&#24212;&#36164;&#28304;&#21463;&#38480;&#30340;&#26426;&#22120;&#20154;&#24179;&#21488;&#12290;</title><link>https://arxiv.org/abs/2402.13201</link><description>&lt;p&gt;
&#20351;&#29992;&#20915;&#31574;&#21464;&#25442;&#22120;&#36827;&#34892;&#22235;&#36275;&#21160;&#20316;&#30340;&#23567;&#22411;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Tiny Reinforcement Learning for Quadruped Locomotion using Decision Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13201
&lt;/p&gt;
&lt;p&gt;
&#23558;&#27169;&#20223;&#23398;&#20064;&#38382;&#39064;&#35270;&#20026;&#26465;&#20214;&#24207;&#21015;&#24314;&#27169;&#20219;&#21153;&#65292;&#36890;&#36807;&#35757;&#32451;&#20915;&#31574;&#21464;&#25442;&#22120;&#24182;&#20351;&#29992;&#22870;&#21169;&#26426;&#21046;&#65292;&#23558;&#29983;&#25104;&#27169;&#22411;&#21387;&#32553;&#20197;&#36866;&#24212;&#36164;&#28304;&#21463;&#38480;&#30340;&#26426;&#22120;&#20154;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13201v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#21463;&#36164;&#28304;&#38480;&#21046;&#30340;&#26426;&#22120;&#20154;&#24179;&#21488;&#29305;&#21035;&#36866;&#29992;&#20110;&#38656;&#35201;&#20302;&#25104;&#26412;&#30828;&#20214;&#26367;&#20195;&#26041;&#26696;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#23384;&#22312;&#22833;&#21435;&#26426;&#22120;&#20154;&#30340;&#39118;&#38505;&#65292;&#27604;&#22914;&#22312;&#25628;&#25937;&#24212;&#29992;&#20013;&#65292;&#25110;&#32773;&#38656;&#35201;&#22823;&#37327;&#35774;&#22791;&#65292;&#27604;&#22914;&#22312;&#32676;&#20307;&#26426;&#22120;&#20154;&#25216;&#26415;&#20013;&#12290;&#22240;&#27492;&#65292;&#20851;&#38190;&#22312;&#20110;&#25214;&#21040;&#26426;&#21046;&#65292;&#20351;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#36866;&#24212;&#36825;&#20123;&#36229;&#20302;&#25104;&#26412;&#26426;&#22120;&#20154;&#24179;&#21488;&#30340;&#20302;&#35745;&#31639;&#33021;&#21147;&#21644;&#36739;&#23567;&#20869;&#23384;&#23481;&#37327;&#25152;&#26045;&#21152;&#30340;&#32422;&#26463;&#12290;&#25105;&#20204;&#35797;&#22270;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#27169;&#20223;&#23398;&#20064;&#37096;&#32626;&#21040;&#21463;&#36164;&#28304;&#38480;&#21046;&#30340;&#26426;&#22120;&#20154;&#24179;&#21488;&#19978;&#26469;&#28385;&#36275;&#36825;&#31181;&#38656;&#27714;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23558;&#27169;&#20223;&#23398;&#20064;&#38382;&#39064;&#35270;&#20026;&#26465;&#20214;&#24207;&#21015;&#24314;&#27169;&#20219;&#21153;&#65292;&#36890;&#36807;&#20351;&#29992;&#38468;&#21152;&#20102;&#33258;&#23450;&#20041;&#22870;&#21169;&#30340;&#19987;&#23478;&#28436;&#31034;&#26469;&#35757;&#32451;&#20915;&#31574;&#21464;&#25442;&#22120;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36719;&#20214;&#20248;&#21270;&#26041;&#26696;&#65292;&#21253;&#25324;&#37327;&#21270;&#21644;&#20462;&#21098;&#65292;&#26469;&#21387;&#32553;&#29983;&#25104;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13201v1 Announce Type: cross  Abstract: Resource-constrained robotic platforms are particularly useful for tasks that require low-cost hardware alternatives due to the risk of losing the robot, like in search-and-rescue applications, or the need for a large number of devices, like in swarm robotics. For this reason, it is crucial to find mechanisms for adapting reinforcement learning techniques to the constraints imposed by lower computational power and smaller memory capacities of these ultra low-cost robotic platforms. We try to address this need by proposing a method for making imitation learning deployable onto resource-constrained robotic platforms. Here we cast the imitation learning problem as a conditional sequence modeling task and we train a decision transformer using expert demonstrations augmented with a custom reward. Then, we compress the resulting generative model using software optimization schemes, including quantization and pruning. We test our method in si
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39640;&#25928;&#12289;&#22522;&#20110;&#26680;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#27979;&#35797;&#26465;&#20214;&#29420;&#31435;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#20559;&#24046;&#25511;&#21046;&#26041;&#27861;&#26469;&#32416;&#27491;&#27979;&#35797;&#27700;&#24179;&#12290;</title><link>https://arxiv.org/abs/2402.13196</link><description>&lt;p&gt;
&#23454;&#29992;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#26680;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Practical Kernel Tests of Conditional Independence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13196
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39640;&#25928;&#12289;&#22522;&#20110;&#26680;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#27979;&#35797;&#26465;&#20214;&#29420;&#31435;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#20559;&#24046;&#25511;&#21046;&#26041;&#27861;&#26469;&#32416;&#27491;&#27979;&#35797;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#25968;&#25454;&#39640;&#25928;&#12289;&#22522;&#20110;&#26680;&#30340;&#32479;&#35745;&#27979;&#35797;&#26041;&#27861;&#65292;&#29992;&#20110;&#27979;&#35797;&#26465;&#20214;&#29420;&#31435;&#24615;&#12290;&#26465;&#20214;&#29420;&#31435;&#24615;&#27979;&#35797;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#33719;&#24471;&#27491;&#30830;&#30340;&#27979;&#35797;&#27700;&#24179;&#65288;&#25351;&#23450;&#30340;&#38169;&#35823;&#38451;&#24615;&#29575;&#19978;&#38480;&#65289;&#65292;&#21516;&#26102;&#20173;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#27979;&#35797;&#33021;&#21147;&#12290;&#36807;&#22810;&#30340;&#20551;&#38451;&#24615;&#26159;&#30001;&#20110;&#27979;&#35797;&#32479;&#35745;&#37327;&#20013;&#30340;&#20559;&#24046;&#24341;&#36215;&#30340;&#65292;&#35813;&#32479;&#35745;&#37327;&#26159;&#20351;&#29992;&#38750;&#21442;&#25968;&#26680;&#23725;&#22238;&#24402;&#33719;&#24471;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#20559;&#24046;&#25511;&#21046;&#26041;&#27861;&#26469;&#20462;&#27491;&#27979;&#35797;&#27700;&#24179;&#65292;&#22522;&#20110;&#25968;&#25454;&#20998;&#21106;&#12289;&#36741;&#21161;&#25968;&#25454;&#65292;&#20197;&#21450;&#65288;&#22312;&#21487;&#33021;&#30340;&#24773;&#20917;&#19979;&#65289;&#26356;&#31616;&#21333;&#30340;&#20989;&#25968;&#31867;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#32452;&#21512;&#31574;&#30053;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13196v1 Announce Type: new  Abstract: We describe a data-efficient, kernel-based approach to statistical testing of conditional independence. A major challenge of conditional independence testing, absent in tests of unconditional independence, is to obtain the correct test level (the specified upper bound on the rate of false positives), while still attaining competitive test power. Excess false positives arise due to bias in the test statistic, which is obtained using nonparametric kernel ridge regression. We propose three methods for bias control to correct the test level, based on data splitting, auxiliary data, and (where possible) simpler function classes. We show these combined strategies are effective both for synthetic and real-world data.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#23646;&#24615;&#27979;&#35797;&#31639;&#27861;&#26041;&#38754;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36817;&#20284;&#32447;&#24615;&#35268;&#21010;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#20449;&#24687;&#29702;&#35770;&#19978;&#26368;&#20248;&#22320;&#35299;&#20915;&#26657;&#20934;&#24615;&#27979;&#35797;&#38382;&#39064;&#65288;&#26368;&#22810;&#19968;&#20010;&#24120;&#25968;&#20493;&#25968;&#65289;&#12290;</title><link>https://arxiv.org/abs/2402.13187</link><description>&lt;p&gt;
&#22312;&#27425;&#32447;&#24615;&#26102;&#38388;&#20869;&#27979;&#35797;&#26657;&#20934;&#24615;
&lt;/p&gt;
&lt;p&gt;
Testing Calibration in Subquadratic Time
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13187
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#23646;&#24615;&#27979;&#35797;&#31639;&#27861;&#26041;&#38754;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36817;&#20284;&#32447;&#24615;&#35268;&#21010;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#20449;&#24687;&#29702;&#35770;&#19978;&#26368;&#20248;&#22320;&#35299;&#20915;&#26657;&#20934;&#24615;&#27979;&#35797;&#38382;&#39064;&#65288;&#26368;&#22810;&#19968;&#20010;&#24120;&#25968;&#20493;&#25968;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#20915;&#31574;&#21046;&#23450;&#25991;&#29486;&#20013;&#65292;&#26657;&#20934;&#24615;&#24050;&#32463;&#25104;&#20026;&#20108;&#20803;&#39044;&#27979;&#27169;&#22411;&#36755;&#20986;&#30340;&#19968;&#20010;&#20540;&#24471;&#26399;&#26395;&#21644;&#24191;&#27867;&#30740;&#31350;&#30340;&#32479;&#35745;&#24615;&#36136;&#12290;&#28982;&#32780;&#65292;&#27979;&#37327;&#27169;&#22411;&#26657;&#20934;&#24615;&#30340;&#31639;&#27861;&#26041;&#38754;&#20173;&#28982;&#30456;&#23545;&#36739;&#23569;&#34987;&#25506;&#32034;&#12290;&#22312;&#35770;&#25991; [BGHN23] &#30340;&#21551;&#21457;&#19979;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20005;&#26684;&#30340;&#26694;&#26550;&#26469;&#34913;&#37327;&#21040;&#26657;&#20934;&#24615;&#30340;&#36317;&#31163;&#65292;&#25105;&#20204;&#36890;&#36807;&#23646;&#24615;&#27979;&#35797;&#30340;&#35270;&#35282;&#24341;&#20837;&#20102;&#26657;&#20934;&#24615;&#30740;&#31350;&#30340;&#31639;&#27861;&#26041;&#38754;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#20174;&#26679;&#26412;&#20013;&#36827;&#34892;&#26657;&#20934;&#24615;&#27979;&#35797;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#20174;&#20998;&#24067; $\mathcal{D}$&#65288;&#39044;&#27979;&#65292;&#20108;&#20803;&#32467;&#26524;&#65289;&#20013;&#32473;&#20986; $n$ &#27425;&#25277;&#26679;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#21306;&#20998; $\mathcal{D}$ &#23436;&#20840;&#26657;&#20934;&#21644; $\mathcal{D}$ &#36317;&#31163;&#26657;&#20934;&#24615;&#20026; $\varepsilon$ &#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13187v1 Announce Type: new  Abstract: In the recent literature on machine learning and decision making, calibration has emerged as a desirable and widely-studied statistical property of the outputs of binary prediction models. However, the algorithmic aspects of measuring model calibration have remained relatively less well-explored. Motivated by [BGHN23], which proposed a rigorous framework for measuring distances to calibration, we initiate the algorithmic study of calibration through the lens of property testing. We define the problem of calibration testing from samples where given $n$ draws from a distribution $\mathcal{D}$ on (predictions, binary outcomes), our goal is to distinguish between the case where $\mathcal{D}$ is perfectly calibrated, and the case where $\mathcal{D}$ is $\varepsilon$-far from calibration.   We design an algorithm based on approximate linear programming, which solves calibration testing information-theoretically optimally (up to constant factor
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#20998;&#24067;&#24335;&#26680;&#36172;&#21338;&#26426;&#38382;&#39064;&#20013;&#23454;&#29616;&#20102;&#26368;&#20248;&#27425;&#24207;&#36951;&#25022;&#65292;&#24182;&#19988;&#36890;&#20449;&#25104;&#26412;&#23545;&#20110;&#20195;&#29702;&#25968;&#37327;&#21644;&#26102;&#38388;&#37117;&#26159;&#20122;&#32447;&#24615;&#30340;&#12290;</title><link>https://arxiv.org/abs/2402.13182</link><description>&lt;p&gt;
&#20351;&#29992;&#20849;&#20139;&#38543;&#26426;&#24615;&#30340;&#22343;&#21248;&#37319;&#26679;&#23454;&#29616;&#20998;&#24067;&#24335;&#26680;&#36172;&#21338;&#26426;&#20013;&#30340;&#26368;&#20248;&#27425;&#24207;&#36951;&#25022;
&lt;/p&gt;
&lt;p&gt;
Order-Optimal Regret in Distributed Kernel Bandits using Uniform Sampling with Shared Randomness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13182
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#20998;&#24067;&#24335;&#26680;&#36172;&#21338;&#26426;&#38382;&#39064;&#20013;&#23454;&#29616;&#20102;&#26368;&#20248;&#27425;&#24207;&#36951;&#25022;&#65292;&#24182;&#19988;&#36890;&#20449;&#25104;&#26412;&#23545;&#20110;&#20195;&#29702;&#25968;&#37327;&#21644;&#26102;&#38388;&#37117;&#26159;&#20122;&#32447;&#24615;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20998;&#24067;&#24335;&#26680;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#20854;&#20013;$N$&#20010;&#20195;&#29702;&#26088;&#22312;&#21327;&#21516;&#26368;&#22823;&#21270;&#23384;&#22312;&#20110;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#26410;&#30693;&#22870;&#21169;&#20989;&#25968;&#12290;&#27599;&#20010;&#20195;&#29702;&#20381;&#27425;&#26597;&#35810;&#35813;&#20989;&#25968;&#65292;&#20197;&#22312;&#26597;&#35810;&#28857;&#22788;&#33719;&#24471;&#22024;&#26434;&#30340;&#35266;&#27979;&#20540;&#12290;&#20195;&#29702;&#21487;&#20197;&#36890;&#36807;&#20013;&#22830;&#26381;&#21153;&#22120;&#20849;&#20139;&#20449;&#24687;&#65292;&#30446;&#30340;&#26159;&#26368;&#23567;&#21270;&#38543;&#30528;&#26102;&#38388;$T$&#32047;&#31215;&#24182;&#27719;&#24635;&#22312;&#20195;&#29702;&#20043;&#38388;&#30340;&#36951;&#25022;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#31532;&#19968;&#20010;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#23454;&#29616;&#20102;&#22312;&#36890;&#20449;&#25104;&#26412;&#23545;$N$&#21644;$T$&#22343;&#20026;&#20122;&#32447;&#24615;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#26368;&#20248;&#36951;&#25022;&#27425;&#24207;&#65288;&#30001;&#38598;&#20013;&#24335;&#23398;&#20064;&#23450;&#20041;&#65289;&#12290;&#25152;&#25552;&#20986;&#31639;&#27861;&#30340;&#20851;&#38190;&#29305;&#28857;&#26159;&#23616;&#37096;&#20195;&#29702;&#30340;&#22343;&#21248;&#25506;&#32034;&#21644;&#19982;&#20013;&#22830;&#26381;&#21153;&#22120;&#30340;&#20849;&#20139;&#38543;&#26426;&#24615;&#12290;&#19982;GP&#27169;&#22411;&#30340;&#31232;&#30095;&#36924;&#36817;&#19968;&#36215;&#24037;&#20316;&#65292;&#36825;&#20004;&#20010;&#20851;&#38190;&#32452;&#20214;&#20351;&#24471;&#33021;&#22815;&#20197;&#36890;&#20449;&#30340;&#34928;&#20943;&#36895;&#29575;&#20445;&#25345;&#20013;&#22830;&#35774;&#32622;&#30340;&#23398;&#20064;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13182v1 Announce Type: new  Abstract: We consider distributed kernel bandits where $N$ agents aim to collaboratively maximize an unknown reward function that lies in a reproducing kernel Hilbert space. Each agent sequentially queries the function to obtain noisy observations at the query points. Agents can share information through a central server, with the objective of minimizing regret that is accumulating over time $T$ and aggregating over agents. We develop the first algorithm that achieves the optimal regret order (as defined by centralized learning) with a communication cost that is sublinear in both $N$ and $T$. The key features of the proposed algorithm are the uniform exploration at the local agents and shared randomness with the central server. Working together with the sparse approximation of the GP model, these two key components make it possible to preserve the learning rate of the centralized setting at a diminishing rate of communication.
&lt;/p&gt;</description></item><item><title>DINOBot&#21033;&#29992;Vision Transformers&#30340;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#36890;&#36807;&#26816;&#32034;&#21644;&#23545;&#40784;&#26469;&#36827;&#34892;&#26426;&#22120;&#20154;&#25805;&#20316;&#65292;&#25552;&#39640;&#20102;&#23398;&#20064;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;</title><link>https://arxiv.org/abs/2402.13181</link><description>&lt;p&gt;
DINOBot&#65306;&#36890;&#36807;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#30340;&#26816;&#32034;&#21644;&#23545;&#40784;&#23454;&#29616;&#26426;&#22120;&#20154;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
DINOBot: Robot Manipulation via Retrieval and Alignment with Vision Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13181
&lt;/p&gt;
&lt;p&gt;
DINOBot&#21033;&#29992;Vision Transformers&#30340;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#36890;&#36807;&#26816;&#32034;&#21644;&#23545;&#40784;&#26469;&#36827;&#34892;&#26426;&#22120;&#20154;&#25805;&#20316;&#65292;&#25552;&#39640;&#20102;&#23398;&#20064;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;DINOBot&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#26426;&#22120;&#20154;&#25805;&#20316;&#65292;&#23427;&#21033;&#29992;&#20102;&#20174;&#20351;&#29992;DINO&#35757;&#32451;&#30340;Vision Transformers&#25552;&#21462;&#30340;&#29305;&#24449;&#30340;&#22270;&#20687;&#32423;&#21644;&#20687;&#32032;&#32423;&#33021;&#21147;&#12290;&#19982;&#26032;&#23545;&#35937;&#20132;&#20114;&#26102;&#65292;DINOBot&#39318;&#20808;&#20351;&#29992;&#36825;&#20123;&#29305;&#24449;&#26469;&#26816;&#32034;&#22312;&#20154;&#31867;&#28436;&#31034;&#20013;&#32463;&#21382;&#36807;&#30340;&#26368;&#30456;&#20284;&#30340;&#23545;&#35937;&#65292;&#28982;&#21518;&#20351;&#29992;&#35813;&#23545;&#35937;&#26469;&#23558;&#20854;&#26411;&#31471;&#25191;&#34892;&#22120;&#19982;&#26032;&#23545;&#35937;&#23545;&#40784;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#20132;&#20114;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#22312;&#26085;&#24120;&#20219;&#21153;&#20013;&#30340;&#30495;&#23454;&#19990;&#30028;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#21033;&#29992;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#30340;&#22270;&#20687;&#32423;&#21644;&#20687;&#32032;&#32423;&#23646;&#24615;&#33021;&#22815;&#23454;&#29616;&#21069;&#25152;&#26410;&#26377;&#30340;&#23398;&#20064;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#35270;&#39057;&#21644;&#20195;&#30721;&#21487;&#22312;https://www.robot-learning.uk/dinobot&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13181v1 Announce Type: cross  Abstract: We propose DINOBot, a novel imitation learning framework for robot manipulation, which leverages the image-level and pixel-level capabilities of features extracted from Vision Transformers trained with DINO. When interacting with a novel object, DINOBot first uses these features to retrieve the most visually similar object experienced during human demonstrations, and then uses this object to align its end-effector with the novel object to enable effective interaction. Through a series of real-world experiments on everyday tasks, we show that exploiting both the image-level and pixel-level properties of vision foundation models enables unprecedented learning efficiency and generalisation. Videos and code are available at https://www.robot-learning.uk/dinobot.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#19978;&#19979;&#25991;&#23545;&#25239;&#28216;&#25103;(ICAG)&#38450;&#24481;&#36234;&#29425;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26174;&#33879;&#38477;&#20302;&#25104;&#21151;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.13148</link><description>&lt;p&gt;
&#36890;&#36807;&#19978;&#19979;&#25991;&#23545;&#25239;&#28216;&#25103;&#38450;&#24481;&#36234;&#29425;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Defending Jailbreak Prompts via In-Context Adversarial Game
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13148
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#19978;&#19979;&#25991;&#23545;&#25239;&#28216;&#25103;(ICAG)&#38450;&#24481;&#36234;&#29425;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26174;&#33879;&#38477;&#20302;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#29616;&#20986;&#22312;&#19981;&#21516;&#24212;&#29992;&#39046;&#22495;&#20013;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#20854;&#23433;&#20840;&#24615;&#30340;&#25285;&#24551;&#65292;&#29305;&#21035;&#26159;&#23545;&#36234;&#29425;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#65292;&#20173;&#28982;&#23384;&#22312;&#12290;&#21463;&#21040;&#28145;&#24230;&#23398;&#20064;&#20013;&#23545;&#25239;&#35757;&#32451;&#21644;LLM&#20195;&#29702;&#23398;&#20064;&#36807;&#31243;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19978;&#19979;&#25991;&#23545;&#25239;&#28216;&#25103;(ICAG)&#26469;&#38450;&#24481;&#36234;&#29425;&#25915;&#20987;&#65292;&#26080;&#38656;&#36827;&#34892;&#24494;&#35843;&#12290;ICAG&#21033;&#29992;&#20195;&#29702;&#23398;&#20064;&#36827;&#34892;&#23545;&#25239;&#28216;&#25103;&#65292;&#26088;&#22312;&#21160;&#24577;&#25193;&#23637;&#30693;&#35782;&#20197;&#38450;&#24481;&#36234;&#29425;&#25915;&#20987;&#12290;&#19982;&#20381;&#36182;&#38745;&#24577;&#25968;&#25454;&#38598;&#30340;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;ICAG&#37319;&#29992;&#36845;&#20195;&#36807;&#31243;&#26469;&#22686;&#24378;&#38450;&#24481;&#21644;&#25915;&#20987;&#20195;&#29702;&#12290;&#36825;&#19968;&#25345;&#32493;&#25913;&#36827;&#36807;&#31243;&#21152;&#24378;&#20102;&#23545;&#26032;&#29983;&#25104;&#30340;&#36234;&#29425;&#25552;&#31034;&#30340;&#38450;&#24481;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#35777;&#23454;&#20102;ICAG&#30340;&#26377;&#25928;&#24615;&#65292;&#32463;&#30001;ICAG&#20445;&#25252;&#30340;LLMs&#22312;&#21508;&#31181;&#25915;&#20987;&#22330;&#26223;&#20013;&#26174;&#33879;&#38477;&#20302;&#20102;&#36234;&#29425;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13148v1 Announce Type: new  Abstract: Large Language Models (LLMs) demonstrate remarkable capabilities across diverse applications. However, concerns regarding their security, particularly the vulnerability to jailbreak attacks, persist. Drawing inspiration from adversarial training in deep learning and LLM agent learning processes, we introduce the In-Context Adversarial Game (ICAG) for defending against jailbreaks without the need for fine-tuning. ICAG leverages agent learning to conduct an adversarial game, aiming to dynamically extend knowledge to defend against jailbreaks. Unlike traditional methods that rely on static datasets, ICAG employs an iterative process to enhance both the defense and attack agents. This continuous improvement process strengthens defenses against newly generated jailbreak prompts. Our empirical studies affirm ICAG's efficacy, where LLMs safeguarded by ICAG exhibit significantly reduced jailbreak success rates across various attack scenarios. Mo
&lt;/p&gt;</description></item><item><title>&#36870;&#21521;&#36719; Q &#23398;&#20064;&#29992;&#20110;&#33719;&#24471;&#27425;&#20248;&#28436;&#31034;&#30340;&#31163;&#32447;&#27169;&#20223;&#25361;&#25112;&#20102;&#31163;&#32447; IL &#20013;&#26377;&#38480;&#25903;&#25345;&#19987;&#23478;&#28436;&#31034;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#20197;&#21305;&#37197;&#27425;&#20248;&#28436;&#31034;&#38598;&#21512;&#30340;&#21344;&#29992;&#20998;&#24067;</title><link>https://arxiv.org/abs/2402.13147</link><description>&lt;p&gt;
SubIQ: &#36870;&#21521;&#36719; Q &#23398;&#20064;&#29992;&#20110;&#33719;&#24471;&#27425;&#20248;&#28436;&#31034;&#30340;&#31163;&#32447;&#27169;&#20223;
&lt;/p&gt;
&lt;p&gt;
SubIQ: Inverse Soft-Q Learning for Offline Imitation with Suboptimal Demonstrations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13147
&lt;/p&gt;
&lt;p&gt;
&#36870;&#21521;&#36719; Q &#23398;&#20064;&#29992;&#20110;&#33719;&#24471;&#27425;&#20248;&#28436;&#31034;&#30340;&#31163;&#32447;&#27169;&#20223;&#25361;&#25112;&#20102;&#31163;&#32447; IL &#20013;&#26377;&#38480;&#25903;&#25345;&#19987;&#23478;&#28436;&#31034;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#20197;&#21305;&#37197;&#27425;&#20248;&#28436;&#31034;&#38598;&#21512;&#30340;&#21344;&#29992;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#65288;IL&#65289;&#65292;&#26088;&#22312;&#20174;&#19987;&#23478;&#28436;&#31034;&#20013;&#27169;&#20223;&#19987;&#23478;&#30340;&#34892;&#20026;&#65292;&#32780;&#26080;&#38656;&#19982;&#29615;&#22659;&#36827;&#34892;&#36827;&#19968;&#27493;&#20132;&#20114;&#12290;&#22312;&#31163;&#32447; IL &#20013;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#22788;&#29702;&#20165;&#28085;&#30422;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#30340;&#19968;&#23567;&#37096;&#20998;&#30340;&#19987;&#23478;&#28436;&#31034;&#30340;&#26377;&#38480;&#25903;&#25345;&#12290;&#25105;&#20204;&#32771;&#34385;&#31163;&#32447; IL&#65292;&#20854;&#20013;&#19987;&#23478;&#28436;&#31034;&#21463;&#21040;&#38480;&#21046;&#65292;&#20294;&#26159;&#30001;&#26356;&#22823;&#35268;&#27169;&#30340;&#27425;&#20248;&#28436;&#31034;&#38598;&#21512;&#34917;&#20805;&#12290;&#22823;&#37096;&#20998;&#29616;&#26377;&#30340;&#29992;&#20110;&#27492;&#35774;&#32622;&#30340;&#31163;&#32447; IL &#26041;&#27861;&#22522;&#20110;&#34892;&#20026;&#20811;&#38534;&#25110;&#20998;&#24067;&#21305;&#37197;&#65292;&#20854;&#30446;&#30340;&#26159;&#23558;&#27169;&#20223;&#31574;&#30053;&#30340;&#21344;&#29992;&#20998;&#24067;&#19982;&#19987;&#23478;&#31574;&#30053;&#30340;&#21344;&#29992;&#20998;&#24067;&#21305;&#37197;&#12290;&#36825;&#31181;&#26041;&#27861;&#24448;&#24448;&#23384;&#22312;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#22240;&#20026;&#19987;&#23478;&#28436;&#31034;&#26377;&#38480;&#65292;&#26080;&#27861;&#20934;&#30830;&#34920;&#31034;&#20219;&#20309;&#21344;&#29992;&#20998;&#24067;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#30001;&#20110;&#27425;&#20248;&#28436;&#31034;&#38598;&#21512;&#35268;&#27169;&#26356;&#22823;&#65292;&#26377;&#24456;&#39640;&#30340;&#21487;&#33021;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13147v1 Announce Type: cross  Abstract: We consider offline imitation learning (IL), which aims to mimic the expert's behavior from its demonstration without further interaction with the environment. One of the main challenges in offline IL is dealing with the limited support of expert demonstrations that cover only a small fraction of the state-action spaces. In this work, we consider offline IL, where expert demonstrations are limited but complemented by a larger set of sub-optimal demonstrations of lower expertise levels. Most of the existing offline IL methods developed for this setting are based on behavior cloning or distribution matching, where the aim is to match the occupancy distribution of the imitation policy with that of the expert policy. Such an approach often suffers from over-fitting, as expert demonstrations are limited to accurately represent any occupancy distribution. On the other hand, since sub-optimal sets are much larger, there is a high chance that 
&lt;/p&gt;</description></item><item><title>&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#34920;&#29616;&#20248;&#24322;&#30340;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#65292;&#29983;&#25104;&#30340;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#19982;&#35757;&#32451;&#32593;&#32476;&#30456;&#23218;&#32654;&#29978;&#33267;&#26356;&#22909;&#65292;&#19988;&#25104;&#26412;&#26497;&#20302;&#12290;</title><link>https://arxiv.org/abs/2402.13144</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
Neural Network Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13144
&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#34920;&#29616;&#20248;&#24322;&#30340;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#65292;&#29983;&#25104;&#30340;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#19982;&#35757;&#32451;&#32593;&#32476;&#30456;&#23218;&#32654;&#29978;&#33267;&#26356;&#22909;&#65292;&#19988;&#25104;&#26412;&#26497;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#22270;&#20687;&#21644;&#35270;&#39057;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25193;&#25955;&#27169;&#22411;&#20063;&#21487;&#20197;\textit{&#29983;&#25104;&#34920;&#29616;&#20248;&#24322;&#30340;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;}&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24456;&#31616;&#21333;&#65292;&#21033;&#29992;&#20102;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#26631;&#20934;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#12290;&#33258;&#21160;&#32534;&#30721;&#22120;&#25552;&#21462;&#20102;&#37096;&#20998;&#21463;&#35757;&#32593;&#32476;&#21442;&#25968;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#28982;&#21518;&#35757;&#32451;&#20102;&#19968;&#20010;&#25193;&#25955;&#27169;&#22411;&#26469;&#20174;&#38543;&#26426;&#22122;&#22768;&#20013;&#21512;&#25104;&#36825;&#20123;&#28508;&#22312;&#21442;&#25968;&#34920;&#31034;&#12290;&#23427;&#29983;&#25104;&#20102;&#26032;&#30340;&#34920;&#31034;&#65292;&#32463;&#36807;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#35299;&#30721;&#22120;&#65292;&#36755;&#20986;&#20934;&#22791;&#29992;&#20316;&#26032;&#30340;&#32593;&#32476;&#21442;&#25968;&#23376;&#38598;&#12290;&#22312;&#21508;&#31181;&#26550;&#26500;&#21644;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#25193;&#25955;&#36807;&#31243;&#22987;&#32456;&#29983;&#25104;&#24615;&#33021;&#19982;&#32463;&#36807;&#35757;&#32451;&#30340;&#32593;&#32476;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#27169;&#22411;&#65292;&#38468;&#21152;&#25104;&#26412;&#26497;&#23567;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#22312;&#23454;&#35777;&#30740;&#31350;&#20013;&#21457;&#29616;&#65292;&#29983;&#25104;&#30340;&#27169;&#22411;&#19982;&#32463;&#36807;&#35757;&#32451;&#30340;&#32593;&#32476;&#34920;&#29616;&#20986;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13144v1 Announce Type: new  Abstract: Diffusion models have achieved remarkable success in image and video generation. In this work, we demonstrate that diffusion models can also \textit{generate high-performing neural network parameters}. Our approach is simple, utilizing an autoencoder and a standard latent diffusion model. The autoencoder extracts latent representations of a subset of the trained network parameters. A diffusion model is then trained to synthesize these latent parameter representations from random noise. It then generates new representations that are passed through the autoencoder's decoder, whose outputs are ready to use as new subsets of network parameters. Across various architectures and datasets, our diffusion process consistently generates models of comparable or improved performance over trained networks, with minimal additional cost. Notably, we empirically find that the generated models perform differently with the trained networks. Our results en
&lt;/p&gt;</description></item><item><title>VGMShield&#25552;&#20986;&#20102;&#19977;&#39033;&#31616;&#21333;&#20294;&#24320;&#21019;&#24615;&#30340;&#25514;&#26045;&#65292;&#36890;&#36807;&#26816;&#27979;&#34394;&#20551;&#35270;&#39057;&#12289;&#28335;&#28304;&#38382;&#39064;&#21644;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#31354;&#38388;-&#26102;&#38388;&#21160;&#24577;&#27169;&#22411;&#65292;&#38450;&#33539;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#30340;&#35823;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.13126</link><description>&lt;p&gt;
VGMShield&#65306;&#32531;&#35299;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#30340;&#35823;&#29992;
&lt;/p&gt;
&lt;p&gt;
VGMShield: Mitigating Misuse of Video Generative Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13126
&lt;/p&gt;
&lt;p&gt;
VGMShield&#25552;&#20986;&#20102;&#19977;&#39033;&#31616;&#21333;&#20294;&#24320;&#21019;&#24615;&#30340;&#25514;&#26045;&#65292;&#36890;&#36807;&#26816;&#27979;&#34394;&#20551;&#35270;&#39057;&#12289;&#28335;&#28304;&#38382;&#39064;&#21644;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#31354;&#38388;-&#26102;&#38388;&#21160;&#24577;&#27169;&#22411;&#65292;&#38450;&#33539;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#30340;&#35823;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35270;&#39057;&#29983;&#25104;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#20154;&#20204;&#21487;&#20197;&#26041;&#20415;&#22320;&#21033;&#29992;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#21019;&#24314;&#31526;&#21512;&#20854;&#29305;&#23450;&#38656;&#27714;&#30340;&#35270;&#39057;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#20063;&#36234;&#26469;&#36234;&#25285;&#24515;&#36825;&#20123;&#25216;&#26415;&#34987;&#29992;&#20110;&#21019;&#20316;&#21644;&#20256;&#25773;&#34394;&#20551;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;VGMShield&#65306;&#19968;&#22871;&#21253;&#21547;&#19977;&#39033;&#30452;&#25509;&#20294;&#24320;&#21019;&#24615;&#30340;&#25514;&#26045;&#65292;&#29992;&#20110;&#38450;&#33539;&#34394;&#20551;&#35270;&#39057;&#29983;&#25104;&#36807;&#31243;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#20174;&#8220;&#34394;&#20551;&#35270;&#39057;&#26816;&#27979;&#8221;&#24320;&#22987;&#65292;&#23581;&#35797;&#29702;&#35299;&#29983;&#25104;&#30340;&#35270;&#39057;&#20013;&#26159;&#21542;&#23384;&#22312;&#29420;&#29305;&#24615;&#65292;&#20197;&#21450;&#25105;&#20204;&#26159;&#21542;&#33021;&#22815;&#21306;&#20998;&#23427;&#20204;&#19982;&#30495;&#23454;&#35270;&#39057;&#30340;&#19981;&#21516;&#65307;&#28982;&#21518;&#65292;&#25105;&#20204;&#25506;&#35752;&#8220;&#28335;&#28304;&#8221;&#38382;&#39064;&#65292;&#21363;&#23558;&#19968;&#27573;&#34394;&#20551;&#35270;&#39057;&#36861;&#28335;&#22238;&#29983;&#25104;&#23427;&#30340;&#27169;&#22411;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#20851;&#27880;&#8220;&#26102;&#31354;&#21160;&#24577;&#8221;&#30340;&#27169;&#22411;&#20316;&#20026;&#39592;&#24178;&#65292;&#20197;&#35782;&#21035;&#35270;&#39057;&#20013;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#36890;&#36807;&#23545;&#19971;&#20010;&#26368;&#20808;&#36827;&#30340;&#24320;&#28304;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13126v1 Announce Type: cross  Abstract: With the rapid advancement in video generation, people can conveniently utilize video generation models to create videos tailored to their specific desires. Nevertheless, there are also growing concerns about their potential misuse in creating and disseminating false information.   In this work, we introduce VGMShield: a set of three straightforward but pioneering mitigations through the lifecycle of fake video generation. We start from \textit{fake video detection} trying to understand whether there is uniqueness in generated videos and whether we can differentiate them from real videos; then, we investigate the \textit{tracing} problem, which maps a fake video back to a model that generates it. Towards these, we propose to leverage pre-trained models that focus on {\it spatial-temporal dynamics} as the backbone to identify inconsistencies in videos. Through experiments on seven state-of-the-art open-source models, we demonstrate that
&lt;/p&gt;</description></item><item><title>BuffGraph&#36890;&#36807;&#25554;&#20837;&#32531;&#20914;&#33410;&#28857;&#21040;&#22270;&#20013;&#65292;&#35843;&#33410;&#20027;&#35201;&#31867;&#21035;&#30340;&#24433;&#21709;&#65292;&#20197;&#25913;&#21892;&#27425;&#35201;&#31867;&#21035;&#30340;&#34920;&#31034;&#65292;&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#19978;&#34920;&#29616;&#20248;&#36234;&#12290;</title><link>https://arxiv.org/abs/2402.13114</link><description>&lt;p&gt;
BuffGraph: &#36890;&#36807;&#32531;&#20914;&#33410;&#28857;&#22686;&#24378;&#33410;&#28857;&#20998;&#31867;&#30340;&#19981;&#24179;&#34913;&#31867;&#21035;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
BuffGraph: Enhancing Class-Imbalanced Node Classification via Buffer Nodes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13114
&lt;/p&gt;
&lt;p&gt;
BuffGraph&#36890;&#36807;&#25554;&#20837;&#32531;&#20914;&#33410;&#28857;&#21040;&#22270;&#20013;&#65292;&#35843;&#33410;&#20027;&#35201;&#31867;&#21035;&#30340;&#24433;&#21709;&#65292;&#20197;&#25913;&#21892;&#27425;&#35201;&#31867;&#21035;&#30340;&#34920;&#31034;&#65292;&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#19978;&#34920;&#29616;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#32467;&#26500;&#25968;&#25454;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#65292;&#21363;&#27425;&#35201;&#31867;&#21035;&#26126;&#26174;&#20195;&#34920;&#19981;&#36275;&#65292;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#29616;&#26377;&#30740;&#31350;&#36890;&#24120;&#29983;&#25104;&#26032;&#30340;&#23569;&#25968;&#33410;&#28857;&#65292;&#24182;&#36830;&#25509;&#26032;&#33410;&#28857;&#21040;&#21407;&#22987;&#22270;&#20013;&#65292;&#20351;&#24471;&#31867;&#21035;&#24179;&#34913;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24182;&#26410;&#35299;&#20915;&#20027;&#35201;&#31867;&#21035;&#36890;&#36807;&#21407;&#22987;&#22270;&#20013;&#30340;&#36793;&#21521;&#27425;&#35201;&#33410;&#28857;&#20256;&#25773;&#20449;&#24687;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#24341;&#20837;&#20102;&#23545;&#20027;&#35201;&#31867;&#21035;&#30340;&#20559;&#35265;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;BuffGraph&#65292;&#23558;&#32531;&#20914;&#33410;&#28857;&#25554;&#20837;&#22270;&#20013;&#65292;&#35843;&#33410;&#20027;&#35201;&#31867;&#21035;&#30340;&#24433;&#21709;&#65292;&#20197;&#25913;&#21892;&#27425;&#35201;&#31867;&#21035;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;BuffGraph&#22312;&#33258;&#28982;&#35774;&#32622;&#21644;&#19981;&#24179;&#34913;&#35774;&#32622;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#33410;&#28857;&#20998;&#31867;&#20013;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#26041;&#27861;&#12290;&#20195;&#30721;&#21487;&#22312;https://anonymous.4open.scien&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13114v1 Announce Type: cross  Abstract: Class imbalance in graph-structured data, where minor classes are significantly underrepresented, poses a critical challenge for Graph Neural Networks (GNNs). To address this challenge, existing studies generally generate new minority nodes and edges connecting new nodes to the original graph to make classes balanced. However, they do not solve the problem that majority classes still propagate information to minority nodes by edges in the original graph which introduces bias towards majority classes. To address this, we introduce BuffGraph, which inserts buffer nodes into the graph, modulating the impact of majority classes to improve minor class representation. Our extensive experiments across diverse real-world datasets empirically demonstrate that BuffGraph outperforms existing baseline methods in class-imbalanced node classification in both natural settings and imbalanced settings. Code is available at https://anonymous.4open.scien
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#22312;&#20108;&#27425;&#25439;&#22833;&#20989;&#25968;&#19979;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#26799;&#24230;&#19979;&#38477;&#26144;&#23556;&#30340;&#38750;&#22855;&#24322;&#24615;&#20197;&#21450;&#20840;&#23616;&#26368;&#23567;&#20540;&#28857;&#38598;&#30340;&#20809;&#28369;&#27969;&#24418;&#29305;&#24615;&#65292;&#20026;&#29702;&#35299;&#22823;&#23398;&#20064;&#29575;&#19979;&#26799;&#24230;&#19979;&#38477;&#30340;&#31283;&#23450;&#24615;&#25552;&#20379;&#20102;&#37325;&#35201;&#32447;&#32034;&#12290;</title><link>https://arxiv.org/abs/2402.13108</link><description>&lt;p&gt;
&#20851;&#20110;&#22823;&#23398;&#20064;&#29575;&#19979;&#26799;&#24230;&#19979;&#38477;&#30340;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Stability of Gradient Descent for Large Learning Rate
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13108
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#22312;&#20108;&#27425;&#25439;&#22833;&#20989;&#25968;&#19979;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#26799;&#24230;&#19979;&#38477;&#26144;&#23556;&#30340;&#38750;&#22855;&#24322;&#24615;&#20197;&#21450;&#20840;&#23616;&#26368;&#23567;&#20540;&#28857;&#38598;&#30340;&#20809;&#28369;&#27969;&#24418;&#29305;&#24615;&#65292;&#20026;&#29702;&#35299;&#22823;&#23398;&#20064;&#29575;&#19979;&#26799;&#24230;&#19979;&#38477;&#30340;&#31283;&#23450;&#24615;&#25552;&#20379;&#20102;&#37325;&#35201;&#32447;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#23545;&#29702;&#35299;&#8220;&#31283;&#23450;&#24615;&#36793;&#32536;&#65288;EoS&#65289;&#8221;&#29616;&#35937;&#23384;&#22312;&#30528;&#30456;&#24403;&#22823;&#30340;&#20852;&#36259;&#65292;&#36825;&#19968;&#29616;&#35937;&#22312;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#34987;&#35266;&#23519;&#21040;&#65292;&#20854;&#29305;&#28857;&#26159;&#25439;&#22833;&#20989;&#25968;&#22312;&#19981;&#21516;&#32426;&#20803;&#38388;&#30340;&#38750;&#21333;&#35843;&#19979;&#38477;&#65292;&#32780;&#25439;&#22833;&#30340;&#38497;&#23789;&#24230;&#65288;Hessian&#30340;&#35889;&#33539;&#25968;&#65289;&#36880;&#28176;&#25509;&#36817;&#24182;&#31283;&#23450;&#22312;2/(&#23398;&#20064;&#29575;)&#38468;&#36817;&#12290;&#26368;&#36817;&#26377;&#20154;&#25552;&#20986;&#20102;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#26102;&#20986;&#29616;EoS&#30340;&#21407;&#22240;&#8212;&#8212;&#27839;&#26799;&#24230;&#19979;&#38477;&#36712;&#36857;&#38468;&#36817;&#32570;&#20047;&#24179;&#22374;&#30340;&#26497;&#23567;&#20540;&#28857;&#65292;&#21516;&#26102;&#23384;&#22312;&#32039;&#33268;&#30340;&#27491;&#21521;&#19981;&#21464;&#38598;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#20108;&#27425;&#25439;&#22833;&#20989;&#25968;&#19979;&#20248;&#21270;&#30340;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#28385;&#36275;&#31532;&#19968;&#20010;&#20551;&#35774;&#20197;&#21450;&#31532;&#20108;&#20010;&#20551;&#35774;&#30340;&#19968;&#20010;&#24517;&#35201;&#26465;&#20214;&#12290;&#26356;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#26799;&#24230;&#19979;&#38477;&#26144;&#23556;&#26159;&#38750;&#22855;&#24322;&#30340;&#65292;&#25439;&#22833;&#20989;&#25968;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#28857;&#38598;&#26500;&#25104;&#19968;&#20010;&#20809;&#28369;&#27969;&#24418;&#65292;&#24182;&#19988;&#31283;&#23450;&#30340;&#26497;&#23567;&#20540;&#26500;&#25104;&#26377;&#30028;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13108v1 Announce Type: new  Abstract: There currently is a significant interest in understanding the Edge of Stability (EoS) phenomenon, which has been observed in neural networks training, characterized by a non-monotonic decrease of the loss function over epochs, while the sharpness of the loss (spectral norm of the Hessian) progressively approaches and stabilizes around 2/(learning rate). Reasons for the existence of EoS when training using gradient descent have recently been proposed -- a lack of flat minima near the gradient descent trajectory together with the presence of compact forward-invariant sets. In this paper, we show that linear neural networks optimized under a quadratic loss function satisfy the first assumption and also a necessary condition for the second assumption. More precisely, we prove that the gradient descent map is non-singular, the set of global minimizers of the loss function forms a smooth manifold, and the stable minima form a bounded subset i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#21463;&#22797;&#21512;&#39640;&#26031;&#20808;&#39564;&#21551;&#21457;&#30340;&#23637;&#24320;DNN&#65292;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#27867;&#21270;&#35823;&#24046;&#30028;&#38480;&#65292;&#36825;&#20123;&#32593;&#32476;&#22312;&#21387;&#32553;&#24863;&#30693;&#21644;&#23618;&#26512;&#25104;&#20687;&#38382;&#39064;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2402.13106</link><description>&lt;p&gt;
&#20851;&#20110;&#28145;&#24230;&#22797;&#21512;&#39640;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
On Generalization Bounds for Deep Compound Gaussian Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13106
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#21463;&#22797;&#21512;&#39640;&#26031;&#20808;&#39564;&#21551;&#21457;&#30340;&#23637;&#24320;DNN&#65292;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#27867;&#21270;&#35823;&#24046;&#30028;&#38480;&#65292;&#36825;&#20123;&#32593;&#32476;&#22312;&#21387;&#32553;&#24863;&#30693;&#21644;&#23618;&#26512;&#25104;&#20687;&#38382;&#39064;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31639;&#27861;&#23637;&#24320;&#25110;&#28378;&#21160;&#26159;&#19968;&#31181;&#20174;&#36845;&#20195;&#31639;&#27861;&#26500;&#24314;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#25216;&#26415;&#12290;&#23637;&#24320;&#30340;DNN&#22312;&#20449;&#21495;&#20272;&#35745;&#20219;&#21153;&#20013;&#36890;&#24120;&#27604;&#26631;&#20934;DNN&#25552;&#20379;&#26356;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#26356;&#20248;&#36234;&#30340;&#32463;&#39564;&#24615;&#33021;&#12290;&#19968;&#20010;&#37325;&#35201;&#30340;&#29702;&#35770;&#38382;&#39064;&#26159;&#26368;&#36817;&#25165;&#24341;&#36215;&#20851;&#27880;&#30340;&#26159;&#20026;&#23637;&#24320;&#30340;DNN&#24320;&#21457;&#27867;&#21270;&#35823;&#24046;&#30028;&#38480;&#12290;&#36825;&#20123;&#30028;&#38480;&#25552;&#20379;&#20102;&#29702;&#35770;&#21644;&#23454;&#38469;&#27934;&#23519;&#65292;&#35828;&#26126;&#20102;DNN&#22312;&#19982;&#29983;&#25104;DNN&#35757;&#32451;&#25968;&#25454;&#30340;&#27010;&#29575;&#23494;&#24230;&#19981;&#21516;&#20294;&#37319;&#26679;&#33258;&#20854;&#20013;&#30340;&#32463;&#39564;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20026;&#19968;&#31867;&#21463;&#22797;&#21512;&#39640;&#26031;&#20808;&#39564;&#21551;&#21457;&#30340;&#23637;&#24320;DNN&#24320;&#21457;&#20102;&#26032;&#39062;&#30340;&#27867;&#21270;&#35823;&#24046;&#30028;&#38480;&#12290;&#24050;&#32463;&#26174;&#31034;&#36825;&#20123;&#22797;&#21512;&#39640;&#26031;&#32593;&#32476;&#22312;&#21387;&#32553;&#24863;&#30693;&#21644;&#23618;&#26512;&#25104;&#20687;&#38382;&#39064;&#20013;&#20248;&#20110;&#27604;&#36739;&#30340;&#26631;&#20934;&#21644;&#23637;&#24320;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13106v1 Announce Type: cross  Abstract: Algorithm unfolding or unrolling is the technique of constructing a deep neural network (DNN) from an iterative algorithm. Unrolled DNNs often provide better interpretability and superior empirical performance over standard DNNs in signal estimation tasks. An important theoretical question, which has only recently received attention, is the development of generalization error bounds for unrolled DNNs. These bounds deliver theoretical and practical insights into the performance of a DNN on empirical datasets that are distinct from, but sampled from, the probability density generating the DNN training data. In this paper, we develop novel generalization error bounds for a class of unrolled DNNs that are informed by a compound Gaussian prior. These compound Gaussian networks have been shown to outperform comparative standard and unfolded deep neural networks in compressive sensing and tomographic imaging problems. The generalization error
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#20102;&#22810;&#21464;&#37327;&#29256;&#26412;&#30340;FLDA&#65288;MUDRA&#65289;&#20197;&#24212;&#23545;&#32570;&#22833;&#25968;&#25454;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26399;&#26395;/&#26465;&#20214;&#26497;&#22823;&#21270;&#65288;ECM&#65289;&#31639;&#27861;&#26469;&#25512;&#26029;&#20854;&#21442;&#25968;&#65292;&#22312;&#20998;&#31867;&#24615;&#33021;&#19978;&#36739;&#29616;&#26377;&#25216;&#26415;&#21462;&#24471;&#20102;&#25913;&#36827;</title><link>https://arxiv.org/abs/2402.13103</link><description>&lt;p&gt;
&#22810;&#21464;&#37327;&#21151;&#33021;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#29992;&#20110;&#20855;&#26377;&#32570;&#22833;&#25968;&#25454;&#30340;&#30701;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Multivariate Functional Linear Discriminant Analysis for the Classification of Short Time Series with Missing Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13103
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#22810;&#21464;&#37327;&#29256;&#26412;&#30340;FLDA&#65288;MUDRA&#65289;&#20197;&#24212;&#23545;&#32570;&#22833;&#25968;&#25454;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26399;&#26395;/&#26465;&#20214;&#26497;&#22823;&#21270;&#65288;ECM&#65289;&#31639;&#27861;&#26469;&#25512;&#26029;&#20854;&#21442;&#25968;&#65292;&#22312;&#20998;&#31867;&#24615;&#33021;&#19978;&#36739;&#29616;&#26377;&#25216;&#26415;&#21462;&#24471;&#20102;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21151;&#33021;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#65288;FLDA&#65289;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#23558;LDA&#23454;&#29616;&#30340;&#22810;&#31867;&#21035;&#20998;&#31867;&#21644;&#38477;&#32500;&#25193;&#23637;&#21040;&#21333;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#22312;&#22823;&#35268;&#27169;&#22810;&#21464;&#37327;&#21644;&#19981;&#23436;&#25972;&#25968;&#25454;&#26102;&#20195;&#65292;&#24517;&#39035;&#20197;&#21487;&#35745;&#31639;&#30340;&#26041;&#24335;&#20272;&#35745;&#29305;&#24449;&#20043;&#38388;&#30340;&#32479;&#35745;&#20381;&#36182;&#20851;&#31995;&#65292;&#21516;&#26102;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;FLDA&#30340;&#22810;&#20803;&#29256;&#26412;&#65288;MUDRA&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#25551;&#36848;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26399;&#26395;/&#26465;&#20214;&#26497;&#22823;&#21270;&#65288;ECM&#65289;&#31639;&#27861;&#26469;&#25512;&#26029;&#20854;&#21442;&#25968;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#23427;&#22312;&#8220;Articulary Word Recognition&#8221;&#25968;&#25454;&#38598;&#19978;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#32570;&#22833;&#25968;&#25454;&#24773;&#20917;&#19979;&#27604;&#29616;&#26377;&#25216;&#26415;&#30340;&#25913;&#36827;&#12290;MUDRA&#20801;&#35768;&#23545;&#20855;&#26377;&#22823;&#27604;&#20363;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#36827;&#34892;&#21487;&#35299;&#37322;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13103v1 Announce Type: new  Abstract: Functional linear discriminant analysis (FLDA) is a powerful tool that extends LDA-mediated multiclass classification and dimension reduction to univariate time-series functions. However, in the age of large multivariate and incomplete data, statistical dependencies between features must be estimated in a computationally tractable way, while also dealing with missing data. There is a need for a computationally tractable approach that considers the statistical dependencies between features and can handle missing values. We here develop a multivariate version of FLDA (MUDRA) to tackle this issue and describe an efficient expectation/conditional-maximization (ECM) algorithm to infer its parameters. We assess its predictive power on the "Articulary Word Recognition" data set and show its improvement over the state-of-the-art, especially in the case of missing data. MUDRA allows interpretable classification of data sets with large proportions
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24494;&#32467;&#26500;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#20195;&#29702;&#27169;&#22411;&#65292;&#26088;&#22312;&#21152;&#36895;&#22810;&#23610;&#24230;&#27169;&#25311;&#65292;&#23558;&#24494;&#35266;&#21644;&#23439;&#35266;&#37327;&#39044;&#27979;&#21644;&#22343;&#36136;&#21270;&#65292;&#23454;&#29616;&#20102;&#22312;&#24377;&#22609;&#24615;&#26448;&#26009;&#20013;&#20445;&#25345;&#22810;&#23610;&#24230;&#24615;&#36136;&#30340;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2402.13101</link><description>&lt;p&gt;
&#22522;&#20110;&#24494;&#32467;&#26500;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#21152;&#36895;&#22810;&#23610;&#24230;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
A Microstructure-based Graph Neural Network for Accelerating Multiscale Simulations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13101
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24494;&#32467;&#26500;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#20195;&#29702;&#27169;&#22411;&#65292;&#26088;&#22312;&#21152;&#36895;&#22810;&#23610;&#24230;&#27169;&#25311;&#65292;&#23558;&#24494;&#35266;&#21644;&#23439;&#35266;&#37327;&#39044;&#27979;&#21644;&#22343;&#36136;&#21270;&#65292;&#23454;&#29616;&#20102;&#22312;&#24377;&#22609;&#24615;&#26448;&#26009;&#20013;&#20445;&#25345;&#22810;&#23610;&#24230;&#24615;&#36136;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24182;&#21457;&#22810;&#23610;&#24230;&#27169;&#22411;&#36827;&#34892;&#20808;&#36827;&#26448;&#26009;&#30340;&#21147;&#23398;&#21709;&#24212;&#27169;&#25311;&#21487;&#20197;&#27604;&#21333;&#23610;&#24230;&#27169;&#25311;&#26356;&#20934;&#30830;&#12290;&#28982;&#32780;&#65292;&#35745;&#31639;&#25104;&#26412;&#38459;&#30861;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#25104;&#26412;&#28304;&#33258;&#24517;&#39035;&#22312;&#27599;&#20010;&#23439;&#35266;&#31215;&#20998;&#28857;&#27714;&#35299;&#30340;&#24494;&#35266;&#26377;&#38480;&#20803;&#65288;FE&#65289;&#27169;&#22411;&#12290;&#22823;&#37327;&#30340;&#20195;&#29702;&#24314;&#27169;&#31574;&#30053;&#35797;&#22270;&#36890;&#36807;&#23398;&#20064;&#20174;&#23439;&#35266;&#24212;&#21464;&#21040;&#23439;&#35266;&#24212;&#21147;&#30340;&#39044;&#27979;&#26469;&#20943;&#36731;&#36825;&#19968;&#25104;&#26412;&#65292;&#23436;&#20840;&#26367;&#20195;&#24494;&#35266;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26367;&#20195;&#20195;&#29702;&#24314;&#27169;&#31574;&#30053;&#65292;&#20801;&#35768;&#20445;&#25345;&#38382;&#39064;&#30340;&#22810;&#23610;&#24230;&#24615;&#36136;&#65292;&#21487;&#20197;&#19982;&#26377;&#38480;&#20803;&#27714;&#35299;&#22120;&#20132;&#26367;&#20351;&#29992;&#20219;&#20309;&#26102;&#38388;&#27493;&#38271;&#12290;&#25105;&#20204;&#30340;&#20195;&#29702;&#25552;&#20379;&#25152;&#26377;&#24494;&#35266;&#37327;&#65292;&#28982;&#21518;&#32463;&#22343;&#36136;&#21270;&#24471;&#21040;&#24863;&#20852;&#36259;&#30340;&#23439;&#35266;&#37327;&#12290;&#25105;&#20204;&#36890;&#36807;&#39044;&#27979;&#19968;&#31181;&#24377;&#22609;&#24615;&#26448;&#26009;&#23454;&#29616;&#20102;&#36825;&#19968;&#28857;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13101v1 Announce Type: new  Abstract: Simulating the mechanical response of advanced materials can be done more accurately using concurrent multiscale models than with single-scale simulations. However, the computational costs stand in the way of the practical application of this approach. The costs originate from microscale Finite Element (FE) models that must be solved at every macroscopic integration point. A plethora of surrogate modeling strategies attempt to alleviate this cost by learning to predict macroscopic stresses from macroscopic strains, completely replacing the microscale models. In this work, we introduce an alternative surrogate modeling strategy that allows for keeping the multiscale nature of the problem, allowing it to be used interchangeably with an FE solver for any time step. Our surrogate provides all microscopic quantities, which are then homogenized to obtain macroscopic quantities of interest. We achieve this for an elasto-plastic material by pred
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#31995;&#32479;&#35780;&#20272;&#20102;Mixture of Experts&#65288;MoEs&#65289;&#20013;&#24120;&#35265;&#35774;&#35745;&#36873;&#25321;&#23545;&#39564;&#35777;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#36335;&#30001;&#22120;&#30340;&#23398;&#20064;&#19982;&#21021;&#22987;&#21270;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#27604;&#36739;&#12289;&#24207;&#21015;&#32423;&#36335;&#30001;&#19982;&#26631;&#35760;&#32423;&#36335;&#30001;&#22312;&#19987;&#23478;&#19987;&#19994;&#21270;&#26041;&#38754;&#30340;&#19981;&#21516;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.13089</link><description>&lt;p&gt;
&#26397;&#21521;&#23545;MoE&#35774;&#35745;&#36873;&#25321;&#30340;&#23454;&#35777;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Towards an empirical understanding of MoE design choices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13089
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#31995;&#32479;&#35780;&#20272;&#20102;Mixture of Experts&#65288;MoEs&#65289;&#20013;&#24120;&#35265;&#35774;&#35745;&#36873;&#25321;&#23545;&#39564;&#35777;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#36335;&#30001;&#22120;&#30340;&#23398;&#20064;&#19982;&#21021;&#22987;&#21270;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#27604;&#36739;&#12289;&#24207;&#21015;&#32423;&#36335;&#30001;&#19982;&#26631;&#35760;&#32423;&#36335;&#30001;&#22312;&#19987;&#23478;&#19987;&#19994;&#21270;&#26041;&#38754;&#30340;&#19981;&#21516;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;Mixture of Experts&#65288;MoEs&#65289;&#20013;&#24120;&#35265;&#35774;&#35745;&#36873;&#25321;&#23545;&#39564;&#35777;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#22312;&#26631;&#35760;&#21644;&#24207;&#21015;&#32423;&#21035;&#19978;&#30340;&#19981;&#21516;&#24433;&#21709;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#32463;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#22312;&#23398;&#20064;&#36335;&#30001;&#22120;&#21644;&#20923;&#32467;&#30340;&#38543;&#26426;&#21021;&#22987;&#21270;&#36335;&#30001;&#22120;&#20043;&#38388;&#20855;&#26377;&#21487;&#27604;&#24615;&#30340;&#24615;&#33021;&#65292;&#36825;&#34920;&#26126;&#23398;&#20064;&#36335;&#30001;&#21487;&#33021;&#24182;&#38750;&#24517;&#19981;&#21487;&#23569;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;&#24207;&#21015;&#32423;&#36335;&#30001;&#21487;&#33021;&#23548;&#33268;&#29305;&#23450;&#20027;&#39064;&#30340;&#24369;&#19987;&#23478;&#19987;&#19994;&#21270;&#65292;&#19982;&#26631;&#35760;&#32423;&#21035;&#36335;&#30001;&#35266;&#23519;&#21040;&#30340;&#35821;&#27861;&#19987;&#19994;&#21270;&#24418;&#25104;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13089v1 Announce Type: cross  Abstract: In this study, we systematically evaluate the impact of common design choices in Mixture of Experts (MoEs) on validation performance, uncovering distinct influences at token and sequence levels. We also present empirical evidence showing comparable performance between a learned router and a frozen, randomly initialized router, suggesting that learned routing may not be essential. Our study further reveals that Sequence-level routing can result in topic-specific weak expert specialization, in contrast to syntax specialization observed with Token-level routing.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#36229;&#21442;&#25968;&#35843;&#25972;&#20013;&#30340;&#38544;&#31169;&#24615;&#38382;&#39064;&#65292;&#21457;&#29616;&#24403;&#21069;&#30340;&#38544;&#31169;&#20998;&#26512;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#26159;&#32039;&#23494;&#30340;&#65292;&#20294;&#22312;&#29305;&#23450;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#38382;&#39064;&#19978;&#21017;&#19981;&#20877;&#25104;&#31435;&#65292;&#24182;&#36890;&#36807;&#38544;&#31169;&#23457;&#35745;&#25581;&#31034;&#20102;&#24403;&#21069;&#29702;&#35770;&#38544;&#31169;&#30028;&#19982;&#23454;&#35777;&#20043;&#38388;&#30340;&#26174;&#33879;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2402.13087</link><description>&lt;p&gt;
&#36873;&#25321;&#22914;&#20309;&#27844;&#28431;&#38544;&#31169;&#65306;&#37325;&#26032;&#23457;&#35270;&#31169;&#26377;&#36873;&#25321;&#21450;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#25913;&#36827;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
How Does Selection Leak Privacy: Revisiting Private Selection and Improved Results for Hyper-parameter Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13087
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#36229;&#21442;&#25968;&#35843;&#25972;&#20013;&#30340;&#38544;&#31169;&#24615;&#38382;&#39064;&#65292;&#21457;&#29616;&#24403;&#21069;&#30340;&#38544;&#31169;&#20998;&#26512;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#26159;&#32039;&#23494;&#30340;&#65292;&#20294;&#22312;&#29305;&#23450;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#38382;&#39064;&#19978;&#21017;&#19981;&#20877;&#25104;&#31435;&#65292;&#24182;&#36890;&#36807;&#38544;&#31169;&#23457;&#35745;&#25581;&#31034;&#20102;&#24403;&#21069;&#29702;&#35770;&#38544;&#31169;&#30028;&#19982;&#23454;&#35777;&#20043;&#38388;&#30340;&#26174;&#33879;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#36229;&#21442;&#25968;&#35843;&#25972;&#20013;&#20445;&#35777;&#24046;&#20998;&#38544;&#31169;(DP)&#30340;&#38382;&#39064;&#65292;&#36825;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#19968;&#20010;&#20851;&#38190;&#30340;&#36807;&#31243;&#65292;&#28041;&#21450;&#20174;&#20960;&#20010;&#36816;&#34892;&#20013;&#36873;&#25321;&#26368;&#20339;&#30340;&#36807;&#31243;&#12290;&#19982;&#35768;&#22810;&#31169;&#26377;&#31639;&#27861;&#65288;&#21253;&#25324;&#26222;&#36941;&#23384;&#22312;&#30340;DP-SGD&#65289;&#19981;&#21516;&#65292;&#35843;&#25972;&#30340;&#38544;&#31169;&#24433;&#21709;&#20173;&#28982;&#19981;&#22815;&#20102;&#35299;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#31169;&#26377;&#35299;&#20915;&#26041;&#26696;&#29992;&#20110;&#35843;&#25972;&#36807;&#31243;&#65292;&#28982;&#32780;&#19968;&#20010;&#26681;&#26412;&#30340;&#38382;&#39064;&#20173;&#28982;&#23384;&#22312;&#65306;&#24403;&#21069;&#35299;&#20915;&#26041;&#26696;&#30340;&#38544;&#31169;&#30028;&#26159;&#21542;&#32039;&#23494;&#65311;&#26412;&#25991;&#23545;&#36825;&#20010;&#38382;&#39064;&#25552;&#20986;&#20102;&#31215;&#26497;&#21644;&#28040;&#26497;&#30340;&#31572;&#26696;&#12290;&#26368;&#21021;&#65292;&#25105;&#20204;&#25552;&#20379;&#30340;&#30740;&#31350;&#35777;&#23454;&#20102;&#24403;&#21069;&#30340;&#38544;&#31169;&#20998;&#26512;&#22312;&#19968;&#33324;&#24847;&#20041;&#19978;&#30830;&#23454;&#26159;&#32039;&#23494;&#30340;&#12290;&#28982;&#32780;&#65292;&#24403;&#25105;&#20204;&#19987;&#38376;&#30740;&#31350;&#36229;&#21442;&#25968;&#35843;&#25972;&#38382;&#39064;&#26102;&#65292;&#36825;&#31181;&#32039;&#23494;&#24615;&#21017;&#19981;&#20877;&#25104;&#31435;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#23545;&#35843;&#25972;&#36807;&#31243;&#36827;&#34892;&#38544;&#31169;&#23457;&#35745;&#26469;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#31361;&#26174;&#20102;&#24403;&#21069;&#29702;&#35770;&#38544;&#31169;&#30028;&#19982;&#23454;&#35777;&#20043;&#38388;&#23384;&#22312;&#37325;&#22823;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13087v1 Announce Type: new  Abstract: We study the problem of guaranteeing Differential Privacy (DP) in hyper-parameter tuning, a crucial process in machine learning involving the selection of the best run from several. Unlike many private algorithms, including the prevalent DP-SGD, the privacy implications of tuning remain insufficiently understood. Recent works propose a generic private solution for the tuning process, yet a fundamental question still persists: is the current privacy bound for this solution tight?   This paper contributes both positive and negative answers to this question. Initially, we provide studies affirming the current privacy analysis is indeed tight in a general sense. However, when we specifically study the hyper-parameter tuning problem, such tightness no longer holds. This is first demonstrated by applying privacy audit on the tuning process. Our findings underscore a substantial gap between the current theoretical privacy bound and the empirica
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#32479;&#35745;&#23398;&#20064;&#26041;&#27861;&#21644;&#22522;&#30784;&#35774;&#26045;&#36830;&#32493;&#27979;&#37327;&#25968;&#25454;&#65292;&#20197;&#21450;&#22312;&#20869;&#37096;&#27979;&#35797;&#21488;&#19978;&#36827;&#34892;&#25915;&#20987;&#27169;&#25311;&#65292;&#23454;&#29616;&#20102;IT&#22522;&#30784;&#35774;&#26045;&#20013;&#30340;&#33258;&#21160;&#20837;&#20405;&#26816;&#27979;&#12290;</title><link>https://arxiv.org/abs/2402.13081</link><description>&lt;p&gt;
&#20351;&#29992;&#32479;&#35745;&#23398;&#20064;&#21644;&#27979;&#35797;&#21488;&#27979;&#37327;&#36827;&#34892;IT&#20837;&#20405;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
IT Intrusion Detection Using Statistical Learning and Testbed Measurements
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13081
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#32479;&#35745;&#23398;&#20064;&#26041;&#27861;&#21644;&#22522;&#30784;&#35774;&#26045;&#36830;&#32493;&#27979;&#37327;&#25968;&#25454;&#65292;&#20197;&#21450;&#22312;&#20869;&#37096;&#27979;&#35797;&#21488;&#19978;&#36827;&#34892;&#25915;&#20987;&#27169;&#25311;&#65292;&#23454;&#29616;&#20102;IT&#22522;&#30784;&#35774;&#26045;&#20013;&#30340;&#33258;&#21160;&#20837;&#20405;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;IT&#22522;&#30784;&#35774;&#26045;&#20013;&#30340;&#33258;&#21160;&#20837;&#20405;&#26816;&#27979;&#65292;&#29305;&#21035;&#26159;&#35782;&#21035;&#25915;&#20987;&#24320;&#22987;&#12289;&#25915;&#20987;&#31867;&#22411;&#20197;&#21450;&#25915;&#20987;&#32773;&#37319;&#21462;&#30340;&#21160;&#20316;&#39034;&#24207;&#30340;&#38382;&#39064;&#65292;&#22522;&#20110;&#22522;&#30784;&#35774;&#26045;&#30340;&#36830;&#32493;&#27979;&#37327;&#12290;&#25105;&#20204;&#24212;&#29992;&#32479;&#35745;&#23398;&#20064;&#26041;&#27861;&#65292;&#21253;&#25324;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65288;HMM&#65289;&#12289;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#21644;&#38543;&#26426;&#26862;&#26519;&#20998;&#31867;&#22120;&#65288;RFC&#65289;&#65292;&#23558;&#35266;&#27979;&#24207;&#21015;&#26144;&#23556;&#21040;&#39044;&#27979;&#25915;&#20987;&#21160;&#20316;&#24207;&#21015;&#12290;&#19982;&#22823;&#22810;&#25968;&#30456;&#20851;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#25317;&#26377;&#20016;&#23500;&#30340;&#25968;&#25454;&#26469;&#35757;&#32451;&#27169;&#22411;&#24182;&#35780;&#20272;&#20854;&#39044;&#27979;&#33021;&#21147;&#12290;&#25968;&#25454;&#26469;&#33258;&#25105;&#20204;&#22312;&#20869;&#37096;&#27979;&#35797;&#21488;&#19978;&#29983;&#25104;&#30340;&#36319;&#36394;&#25968;&#25454;&#65292;&#22312;&#36825;&#37324;&#25105;&#20204;&#23545;&#27169;&#25311;&#30340;IT&#22522;&#30784;&#35774;&#26045;&#36827;&#34892;&#25915;&#20987;&#12290;&#25105;&#20204;&#24037;&#20316;&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#31649;&#36947;&#65292;&#23558;&#26469;&#33258;&#39640;&#32500;&#35266;&#27979;&#31354;&#38388;&#30340;&#27979;&#37327;&#26144;&#23556;&#21040;&#20302;&#32500;&#31354;&#38388;&#25110;&#23569;&#37327;&#35266;&#27979;&#31526;&#21495;&#30340;&#31354;&#38388;&#12290;&#25105;&#20204;&#30740;&#31350;&#31163;&#32447;&#21644;&#22312;&#32447;&#20837;&#20405;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13081v1 Announce Type: new  Abstract: We study automated intrusion detection in an IT infrastructure, specifically the problem of identifying the start of an attack, the type of attack, and the sequence of actions an attacker takes, based on continuous measurements from the infrastructure. We apply statistical learning methods, including Hidden Markov Model (HMM), Long Short-Term Memory (LSTM), and Random Forest Classifier (RFC) to map sequences of observations to sequences of predicted attack actions. In contrast to most related research, we have abundant data to train the models and evaluate their predictive power. The data comes from traces we generate on an in-house testbed where we run attacks against an emulated IT infrastructure. Central to our work is a machine-learning pipeline that maps measurements from a high-dimensional observation space to a space of low dimensionality or to a small set of observation symbols. Investigating intrusions in offline as well as onli
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#27169;&#24577;&#20272;&#35745;&#20013;&#21033;&#29992;&#37096;&#20998;&#21453;&#39304;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29109;&#32534;&#30721;&#23454;&#29616;&#26368;&#20248;&#20449;&#24687;&#33719;&#21462;&#65292;&#24320;&#21457;&#20102;&#31895;&#31961;&#30340;&#20805;&#20998;&#32479;&#35745;&#29992;&#20110;&#27169;&#24577;&#35782;&#21035;&#65292;&#24182;&#23558;&#36172;&#21338;&#31639;&#27861;&#35843;&#25972;&#20026;&#26032;&#35774;&#32622;&#65292;&#26368;&#32456;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#38382;&#39064;&#35299;&#20915;&#26041;&#26696;</title><link>https://arxiv.org/abs/2402.13079</link><description>&lt;p&gt;
&#20855;&#26377;&#37096;&#20998;&#21453;&#39304;&#30340;&#27169;&#24577;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Mode Estimation with Partial Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#27169;&#24577;&#20272;&#35745;&#20013;&#21033;&#29992;&#37096;&#20998;&#21453;&#39304;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29109;&#32534;&#30721;&#23454;&#29616;&#26368;&#20248;&#20449;&#24687;&#33719;&#21462;&#65292;&#24320;&#21457;&#20102;&#31895;&#31961;&#30340;&#20805;&#20998;&#32479;&#35745;&#29992;&#20110;&#27169;&#24577;&#35782;&#21035;&#65292;&#24182;&#23558;&#36172;&#21338;&#31639;&#27861;&#35843;&#25972;&#20026;&#26032;&#35774;&#32622;&#65292;&#26368;&#32456;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#38382;&#39064;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13079v1 &#36890;&#21578;&#31867;&#22411;: &#20132;&#21449;&#25688;&#35201;: &#26368;&#36817;&#20154;&#24037;&#26234;&#33021;&#21457;&#23637;&#20013;&#65292;&#36731;&#24230;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#21644;&#22312;&#32447;&#24494;&#35843;&#30340;&#32452;&#21512;&#22312;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#36825;&#20123;&#26032;&#30340;&#23398;&#20064;&#27969;&#31243;&#38656;&#35201;&#26032;&#30340;&#29702;&#35770;&#26694;&#26550;&#12290;&#26412;&#25991;&#36890;&#36807;&#19968;&#20010;&#31616;&#21333;&#38382;&#39064;&#65292;&#24418;&#24335;&#21270;&#24369;&#30417;&#30563;&#21644;&#20027;&#21160;&#23398;&#20064;&#30340;&#26680;&#24515;&#26041;&#38754;&#65306;&#20351;&#29992;&#37096;&#20998;&#21453;&#39304;&#20272;&#35745;&#20998;&#24067;&#30340;&#27169;&#24577;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#29109;&#32534;&#30721;&#20174;&#37096;&#20998;&#21453;&#39304;&#20013;&#23454;&#29616;&#26368;&#20248;&#20449;&#24687;&#33719;&#21462;&#65292;&#20026;&#27169;&#24577;&#35782;&#21035;&#24320;&#21457;&#20102;&#31895;&#31961;&#30340;&#20805;&#20998;&#32479;&#35745;&#65292;&#24182;&#23558;&#36172;&#21338;&#31639;&#27861;&#35843;&#25972;&#20026;&#25105;&#20204;&#30340;&#26032;&#35774;&#32622;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#36129;&#29486;&#32467;&#21512;&#36215;&#26469;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#32479;&#35745;&#19978;&#21644;&#35745;&#31639;&#19978;&#39640;&#25928;&#30340;&#38382;&#39064;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13079v1 Announce Type: cross  Abstract: The combination of lightly supervised pre-training and online fine-tuning has played a key role in recent AI developments. These new learning pipelines call for new theoretical frameworks. In this paper, we formalize core aspects of weakly supervised and active learning with a simple problem: the estimation of the mode of a distribution using partial feedback. We show how entropy coding allows for optimal information acquisition from partial feedback, develop coarse sufficient statistics for mode identification, and adapt bandit algorithms to our new setting. Finally, we combine those contributions into a statistically and computationally efficient solution to our problem.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26426;&#21046;&#31070;&#32463;&#32593;&#32476;&#30340;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#65292;&#36890;&#36807;&#22312;&#26631;&#20934;&#26550;&#26500;&#20013;&#24341;&#20837;&#26032;&#30340;&#26426;&#21046;&#27169;&#22359;&#65292;&#23398;&#20064;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#20316;&#20026;&#34920;&#31034;&#65292;&#25552;&#39640;&#25968;&#25454;&#24314;&#27169;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#25928;&#29575;&#65292;&#24182;&#20511;&#21161;&#19968;&#31181;&#26032;&#39062;&#30340;&#26494;&#24347;&#32447;&#24615;&#35268;&#21010;&#27714;&#35299;&#22120;&#23454;&#29616;&#21487;&#25193;&#23637;&#30340;GPU&#24182;&#34892;&#22788;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.13077</link><description>&lt;p&gt;
&#29992;&#20110;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#30340;&#26426;&#21046;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Mechanistic Neural Networks for Scientific Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13077
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26426;&#21046;&#31070;&#32463;&#32593;&#32476;&#30340;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#65292;&#36890;&#36807;&#22312;&#26631;&#20934;&#26550;&#26500;&#20013;&#24341;&#20837;&#26032;&#30340;&#26426;&#21046;&#27169;&#22359;&#65292;&#23398;&#20064;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#20316;&#20026;&#34920;&#31034;&#65292;&#25552;&#39640;&#25968;&#25454;&#24314;&#27169;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#25928;&#29575;&#65292;&#24182;&#20511;&#21161;&#19968;&#31181;&#26032;&#39062;&#30340;&#26494;&#24347;&#32447;&#24615;&#35268;&#21010;&#27714;&#35299;&#22120;&#23454;&#29616;&#21487;&#25193;&#23637;&#30340;GPU&#24182;&#34892;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26426;&#21046;&#31070;&#32463;&#32593;&#32476;&#30340;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#65292;&#29992;&#20110;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#12290;&#23427;&#22312;&#26631;&#20934;&#26550;&#26500;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#26426;&#21046;&#27169;&#22359;&#65292;&#26126;&#30830;&#22320;&#23398;&#20064;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#20316;&#20026;&#34920;&#31034;&#65292;&#25581;&#31034;&#25968;&#25454;&#30340;&#22522;&#26412;&#21160;&#24577;&#65292;&#24182;&#22686;&#24378;&#20102;&#25968;&#25454;&#24314;&#27169;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#25928;&#29575;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26494;&#24347;&#32447;&#24615;&#35268;&#21010;&#27714;&#35299;&#22120;&#65288;NeuRLP&#65289;&#65292;&#21463;&#19968;&#31181;&#23558;&#27714;&#35299;&#32447;&#24615;ODE&#36716;&#21270;&#20026;&#27714;&#35299;&#32447;&#24615;&#35268;&#21010;&#30340;&#25216;&#26415;&#21551;&#21457;&#12290;&#23427;&#19982;&#31070;&#32463;&#32593;&#32476;&#24456;&#22909;&#22320;&#38598;&#25104;&#65292;&#24182;&#36229;&#36234;&#20102;&#20256;&#32479;ODE&#27714;&#35299;&#22120;&#30340;&#23616;&#38480;&#65292;&#23454;&#29616;&#20102;&#21487;&#25193;&#23637;&#30340;GPU&#24182;&#34892;&#22788;&#29702;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#26426;&#21046;&#31070;&#32463;&#32593;&#32476;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#22810;&#25165;&#22810;&#33402;&#65292;&#33021;&#22815;&#28789;&#27963;&#22788;&#29702;&#20174;&#26041;&#31243;&#21457;&#29616;&#21040;&#21160;&#24577;&#31995;&#32479;&#24314;&#27169;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#20998;&#26512;&#21644;&#35299;&#37322;&#22797;&#26434;&#31185;&#23398;&#38382;&#39064;&#26041;&#38754;&#30340;&#20840;&#38754;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13077v1 Announce Type: cross  Abstract: This paper presents Mechanistic Neural Networks, a neural network design for machine learning applications in the sciences. It incorporates a new Mechanistic Block in standard architectures to explicitly learn governing differential equations as representations, revealing the underlying dynamics of data and enhancing interpretability and efficiency in data modeling. Central to our approach is a novel Relaxed Linear Programming Solver (NeuRLP) inspired by a technique that reduces solving linear ODEs to solving linear programs. This integrates well with neural networks and surpasses the limitations of traditional ODE solvers enabling scalable GPU parallel processing. Overall, Mechanistic Neural Networks demonstrate their versatility for scientific machine learning applications, adeptly managing tasks from equation discovery to dynamic systems modeling. We prove their comprehensive capabilities in analyzing and interpreting complex scient
&lt;/p&gt;</description></item><item><title>&#26435;&#37325;&#21442;&#25968;&#22312;&#35774;&#22791;&#19978;&#30340;&#27969;&#24335;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#20013;&#30340;&#21151;&#32791;&#24433;&#21709;&#26377;&#25152;&#19981;&#21516;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#22522;&#20110;&#26435;&#37325;&#21442;&#25968;&#25935;&#24863;&#24615;&#30340;&#26377;&#38024;&#23545;&#24615;&#21387;&#32553;&#26041;&#27861;&#65292;&#23558;&#33021;&#28304;&#20351;&#29992;&#20943;&#23569;&#39640;&#36798;47%&#32780;&#32500;&#25345;&#27169;&#22411;&#20934;&#30830;&#24615;</title><link>https://arxiv.org/abs/2402.13076</link><description>&lt;p&gt;
&#19981;&#26159;&#25152;&#26377;&#30340;&#26435;&#37325;&#37117;&#26159;&#24179;&#31561;&#30340;: &#22312;&#35774;&#22791;&#19978;&#22686;&#24378;&#33021;&#25928;&#30340;&#27969;&#24335;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Not All Weights Are Created Equal: Enhancing Energy Efficiency in On-Device Streaming Speech Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13076
&lt;/p&gt;
&lt;p&gt;
&#26435;&#37325;&#21442;&#25968;&#22312;&#35774;&#22791;&#19978;&#30340;&#27969;&#24335;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#20013;&#30340;&#21151;&#32791;&#24433;&#21709;&#26377;&#25152;&#19981;&#21516;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#22522;&#20110;&#26435;&#37325;&#21442;&#25968;&#25935;&#24863;&#24615;&#30340;&#26377;&#38024;&#23545;&#24615;&#21387;&#32553;&#26041;&#27861;&#65292;&#23558;&#33021;&#28304;&#20351;&#29992;&#20943;&#23569;&#39640;&#36798;47%&#32780;&#32500;&#25345;&#27169;&#22411;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#21147;&#28040;&#32791;&#22312;&#35774;&#22791;&#19978;&#30340;&#27969;&#24335;&#35821;&#38899;&#35782;&#21035;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#22240;&#20026;&#23427;&#30452;&#25509;&#24433;&#21709;&#29992;&#25143;&#20307;&#39564;&#12290;&#26412;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#20013;&#30340;&#26435;&#37325;&#21442;&#25968;&#22914;&#20309;&#24433;&#21709;&#36825;&#20123;&#27169;&#22411;&#30340;&#24635;&#20307;&#21151;&#32791;&#12290;&#25105;&#20204;&#21457;&#29616;&#26435;&#37325;&#21442;&#25968;&#23545;&#21151;&#32791;&#30340;&#24433;&#21709;&#22240;&#22810;&#31181;&#22240;&#32032;&#32780;&#24322;&#65292;&#21463;&#21040;&#35843;&#29992;&#39057;&#29575;&#21450;&#20854;&#22312;&#20869;&#23384;&#20013;&#30340;&#20301;&#32622;&#31561;&#22240;&#32032;&#30340;&#24433;&#21709;&#12290;&#20973;&#20511;&#36825;&#19968;&#27934;&#23519;&#21147;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#26088;&#22312;&#20248;&#21270;&#35774;&#22791;&#19978;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#30340;&#35774;&#35745;&#25351;&#21335;&#12290;&#36825;&#20123;&#25351;&#21335;&#20391;&#37325;&#20110;&#22312;&#23613;&#37327;&#19981;&#26174;&#33879;&#24433;&#21709;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#26368;&#23567;&#21270;&#21151;&#32791;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#26435;&#37325;&#21442;&#25968;&#21464;&#21270;&#25935;&#24863;&#24615;&#30340;&#26377;&#38024;&#23545;&#24615;&#21387;&#32553;&#65292;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#21387;&#32553;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#39640;&#36798;47%&#30340;&#33021;&#28304;&#20351;&#29992;&#20943;&#23569;&#65292;&#21516;&#26102;&#20445;&#25345;&#31867;&#20284;&#30340;&#27169;&#22411;&#20934;&#30830;&#24615;&#65292;&#24182;&#25913;&#21892;&#23454;&#26102;&#27969;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13076v1 Announce Type: cross  Abstract: Power consumption plays an important role in on-device streaming speech recognition, as it has a direct impact on the user experience. This study delves into how weight parameters in speech recognition models influence the overall power consumption of these models. We discovered that the impact of weight parameters on power consumption varies, influenced by factors including how often they are invoked and their placement in memory. Armed with this insight, we developed design guidelines aimed at optimizing on-device speech recognition models. These guidelines focus on minimizing power use without substantially affecting accuracy. Our method, which employs targeted compression based on the varying sensitivities of weight parameters, demonstrates superior performance compared to state-of-the-art compression methods. It achieves a reduction in energy usage of up to 47% while maintaining similar model accuracy and improving the real-time f
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Text-Guided Molecule Generation with Diffusion Language Model&#65288;TGM-DLM&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#24341;&#23548;&#30340;&#20998;&#23376;&#29983;&#25104;&#65292;&#22312;&#29983;&#25104;&#26377;&#25928;&#20998;&#23376;&#34920;&#31034;&#26041;&#38754;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25928;&#26524;&#20248;&#20110;&#33258;&#22238;&#24402;&#27169;&#22411;MolT5-Base&#12290;</title><link>https://arxiv.org/abs/2402.13040</link><description>&lt;p&gt;
&#29992;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#24341;&#23548;&#30340;&#20998;&#23376;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Text-Guided Molecule Generation with Diffusion Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13040
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Text-Guided Molecule Generation with Diffusion Language Model&#65288;TGM-DLM&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#24341;&#23548;&#30340;&#20998;&#23376;&#29983;&#25104;&#65292;&#22312;&#29983;&#25104;&#26377;&#25928;&#20998;&#23376;&#34920;&#31034;&#26041;&#38754;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25928;&#26524;&#20248;&#20110;&#33258;&#22238;&#24402;&#27169;&#22411;MolT5-Base&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#24341;&#23548;&#30340;&#20998;&#23376;&#29983;&#25104;&#26159;&#19968;&#20010;&#20219;&#21153;&#65292;&#20854;&#20013;&#29983;&#25104;&#30340;&#20998;&#23376;&#19982;&#29305;&#23450;&#30340;&#25991;&#26412;&#25551;&#36848;&#30456;&#21305;&#37197;&#12290;&#26368;&#36817;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#22522;&#20110;SMILES&#30340;&#20998;&#23376;&#29983;&#25104;&#26041;&#27861;&#20381;&#36182;&#20110;&#33258;&#22238;&#24402;&#26550;&#26500;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Text-Guided Molecule Generation with Diffusion Language Model&#65288;TGM-DLM&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#26469;&#35299;&#20915;&#33258;&#22238;&#24402;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;TGM-DLM&#38598;&#20307;&#21644;&#36845;&#20195;&#22320;&#26356;&#26032;SMILES&#23383;&#31526;&#20018;&#20013;&#30340;&#26631;&#35760;&#23884;&#20837;&#65292;&#20351;&#29992;&#20004;&#38454;&#27573;&#25193;&#25955;&#29983;&#25104;&#36807;&#31243;&#12290;&#31532;&#19968;&#38454;&#27573;&#36890;&#36807;&#38543;&#26426;&#22122;&#22768;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#24341;&#23548;&#26469;&#20248;&#21270;&#23884;&#20837;&#65292;&#32780;&#31532;&#20108;&#38454;&#27573;&#32416;&#27491;&#26080;&#25928;&#30340;SMILES&#23383;&#31526;&#20018;&#20197;&#24418;&#25104;&#26377;&#25928;&#30340;&#20998;&#23376;&#34920;&#31034;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;TGM-DLM&#22312;&#19981;&#38656;&#35201;&#39069;&#22806;&#25968;&#25454;&#36164;&#28304;&#30340;&#24773;&#20917;&#19979;&#65292;&#32988;&#36807;&#20102;&#33258;&#22238;&#24402;&#27169;&#22411;MolT5-Base&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;TGM-DLM&#22312;&#29983;&#25104;&#36830;&#36143;&#24615;&#26041;&#38754;&#30340;&#26174;&#33879;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13040v1 Announce Type: cross  Abstract: Text-guided molecule generation is a task where molecules are generated to match specific textual descriptions. Recently, most existing SMILES-based molecule generation methods rely on an autoregressive architecture. In this work, we propose the Text-Guided Molecule Generation with Diffusion Language Model (TGM-DLM), a novel approach that leverages diffusion models to address the limitations of autoregressive methods. TGM-DLM updates token embeddings within the SMILES string collectively and iteratively, using a two-phase diffusion generation process. The first phase optimizes embeddings from random noise, guided by the text description, while the second phase corrects invalid SMILES strings to form valid molecular representations. We demonstrate that TGM-DLM outperforms MolT5-Base, an autoregressive model, without the need for additional data resources. Our findings underscore the remarkable effectiveness of TGM-DLM in generating cohe
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#30340;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;AILOT&#65292;&#21487;&#20197;&#22312;&#32570;&#20047;&#26126;&#30830;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#65292;&#20165;&#36890;&#36807;&#35266;&#23519;&#19987;&#23478;&#23398;&#20064;&#25152;&#38656;&#30340;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2402.13037</link><description>&lt;p&gt;
&#23545;&#40784;&#24744;&#30340;&#24847;&#22270;&#65306;&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#30340;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Align Your Intents: Offline Imitation Learning via Optimal Transport
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13037
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#30340;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;AILOT&#65292;&#21487;&#20197;&#22312;&#32570;&#20047;&#26126;&#30830;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#65292;&#20165;&#36890;&#36807;&#35266;&#23519;&#19987;&#23478;&#23398;&#20064;&#25152;&#38656;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#36890;&#36807;&#23398;&#20064;&#39044;&#20808;&#25910;&#38598;&#30340;&#25968;&#25454;&#26469;&#35299;&#20915;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#65292;&#32780;&#26080;&#38656;&#19982;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#12290;&#25105;&#20204;&#23637;&#31034;&#20986;&#65292;&#21363;&#20351;&#32570;&#20047;&#26126;&#30830;&#30340;&#22870;&#21169;&#25110;&#21160;&#20316;&#26631;&#31614;&#65292;&#27169;&#20223;&#20195;&#29702;&#20063;&#21487;&#20197;&#20165;&#36890;&#36807;&#35266;&#23519;&#19987;&#23478;&#26469;&#23398;&#20064;&#25152;&#38656;&#30340;&#34892;&#20026;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;AILOT&#65288;&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#23545;&#40784;&#27169;&#20223;&#23398;&#20064;&#65289;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#24847;&#22270;&#30340;&#29305;&#27530;&#29366;&#24577;&#34920;&#31034;&#24418;&#24335;&#65292;&#20854;&#20013;&#21253;&#21547;&#25968;&#25454;&#20869;&#30340;&#20004;&#20004;&#31354;&#38388;&#36317;&#31163;&#12290;&#22312;&#32473;&#23450;&#36825;&#31181;&#34920;&#31034;&#24418;&#24335;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#36890;&#36807;&#19987;&#23478;&#21644;&#20195;&#29702;&#36712;&#36857;&#20043;&#38388;&#30340;&#26368;&#20248;&#20256;&#36755;&#36317;&#31163;&#23450;&#20041;&#20869;&#22312;&#22870;&#21169;&#20989;&#25968;&#12290;&#25105;&#20204;&#25253;&#21578;&#31216;AILOT&#22312;D4RL&#22522;&#20934;&#27979;&#35797;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13037v1 Announce Type: cross  Abstract: Offline reinforcement learning (RL) addresses the problem of sequential decision-making by learning optimal policy through pre-collected data, without interacting with the environment. As yet, it has remained somewhat impractical, because one rarely knows the reward explicitly and it is hard to distill it retrospectively. Here, we show that an imitating agent can still learn the desired behavior merely from observing the expert, despite the absence of explicit rewards or action labels. In our method, AILOT (Aligned Imitation Learning via Optimal Transport), we involve special representation of states in a form of intents that incorporate pairwise spatial distances within the data. Given such representations, we define intrinsic reward function via optimal transport distance between the expert's and the agent's trajectories. We report that AILOT outperforms state-of-the art offline imitation learning algorithms on D4RL benchmarks and im
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#22686;&#24378;&#26041;&#27861;Hyperedge Augmentation (HyperAug)&#65292;&#36890;&#36807;&#26500;&#24314;&#30452;&#25509;&#20174;&#21407;&#22987;&#25968;&#25454;&#24418;&#25104;&#30340;&#34394;&#25311;&#36229;&#36793;&#65292;&#20197;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#22797;&#26434;&#32593;&#32476;&#34920;&#31034;&#20013;&#39640;&#38454;&#33410;&#28857;&#20851;&#31995;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2402.13033</link><description>&lt;p&gt;
&#29992;&#36229;&#36793;&#22686;&#24378;&#25913;&#36827;&#29616;&#23454;&#19990;&#30028;&#22797;&#26434;&#32593;&#32476;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Enhancing Real-World Complex Network Representations with Hyperedge Augmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13033
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#22686;&#24378;&#26041;&#27861;Hyperedge Augmentation (HyperAug)&#65292;&#36890;&#36807;&#26500;&#24314;&#30452;&#25509;&#20174;&#21407;&#22987;&#25968;&#25454;&#24418;&#25104;&#30340;&#34394;&#25311;&#36229;&#36793;&#65292;&#20197;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#22797;&#26434;&#32593;&#32476;&#34920;&#31034;&#20013;&#39640;&#38454;&#33410;&#28857;&#20851;&#31995;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13033v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#25688;&#35201;: &#22270;&#22686;&#24378;&#26041;&#27861;&#22312;&#25913;&#36827;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#24615;&#33021;&#21644;&#22686;&#24378;&#27867;&#21270;&#33021;&#21147;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#29616;&#26377;&#30340;&#22270;&#22686;&#24378;&#26041;&#27861;&#20027;&#35201;&#25200;&#21160;&#22270;&#32467;&#26500;&#65292;&#36890;&#24120;&#38480;&#20110;&#25104;&#23545;&#33410;&#28857;&#20851;&#31995;&#12290;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#23436;&#20840;&#35299;&#20915;&#30495;&#23454;&#19990;&#30028;&#22823;&#35268;&#27169;&#32593;&#32476;&#30340;&#22797;&#26434;&#24615;&#65292;&#36825;&#20123;&#32593;&#32476;&#36890;&#24120;&#28041;&#21450;&#39640;&#38454;&#33410;&#28857;&#20851;&#31995;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#25104;&#23545;&#20851;&#31995;&#12290;&#21516;&#26102;&#65292;&#30001;&#20110;&#32570;&#20047;&#21487;&#29992;&#20110;&#24418;&#25104;&#39640;&#38454;&#36793;&#30340;&#25968;&#25454;&#65292;&#30495;&#23454;&#19990;&#30028;&#22270;&#25968;&#25454;&#38598;&#20027;&#35201;&#34987;&#24314;&#27169;&#20026;&#31616;&#21333;&#22270;&#12290;&#22240;&#27492;&#65292;&#23558;&#39640;&#38454;&#36793;&#37325;&#26032;&#37197;&#32622;&#20026;&#22270;&#22686;&#24378;&#31574;&#30053;&#30340;&#19968;&#37096;&#20998;&#26159;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#36335;&#24452;&#65292;&#21487;&#35299;&#20915;&#21069;&#36848;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36229;&#36793;&#22686;&#24378;&#65288;HyperAug&#65289;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#22686;&#24378;&#26041;&#27861;&#65292;&#30452;&#25509;&#20174;&#21407;&#22987;&#25968;&#25454;&#26500;&#24314;&#34394;&#25311;&#36229;&#36793;&#65292;&#24182;&#20135;&#29983;&#36741;&#21161;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13033v1 Announce Type: new  Abstract: Graph augmentation methods play a crucial role in improving the performance and enhancing generalisation capabilities in Graph Neural Networks (GNNs). Existing graph augmentation methods mainly perturb the graph structures and are usually limited to pairwise node relations. These methods cannot fully address the complexities of real-world large-scale networks that often involve higher-order node relations beyond only being pairwise. Meanwhile, real-world graph datasets are predominantly modelled as simple graphs, due to the scarcity of data that can be used to form higher-order edges. Therefore, reconfiguring the higher-order edges as an integration into graph augmentation strategies lights up a promising research path to address the aforementioned issues. In this paper, we present Hyperedge Augmentation (HyperAug), a novel graph augmentation method that constructs virtual hyperedges directly form the raw data, and produces auxiliary nod
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#31526;&#21495;&#25216;&#26415;&#8212;&#8212;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#35821;&#20041;&#35843;&#33410;&#65292;&#35813;&#25216;&#26415;&#20165;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#32422;&#26463;&#31995;&#32479;&#65292;&#32780;&#19981;&#24433;&#21709;&#35757;&#32451;&#65292;&#30456;&#23545;&#20110;&#20854;&#20182;&#20004;&#31181;&#24120;&#35265;&#31070;&#32463;&#31526;&#21495;&#25216;&#26415;&#20855;&#26377;&#29702;&#35770;&#21644;&#23454;&#38469;&#20248;&#21183;&#65292;&#22312;&#22810;&#23610;&#24230;&#26041;&#27861;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#20854;&#23545;&#32593;&#32476;&#35268;&#27169;&#30340;&#22909;&#22788;&#12290;</title><link>https://arxiv.org/abs/2402.13019</link><description>&lt;p&gt;
&#29992;&#36923;&#36753;&#32972;&#26223;&#30693;&#35782;&#25552;&#21319;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Improving Neural-based Classification with Logical Background Knowledge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#31526;&#21495;&#25216;&#26415;&#8212;&#8212;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#35821;&#20041;&#35843;&#33410;&#65292;&#35813;&#25216;&#26415;&#20165;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#32422;&#26463;&#31995;&#32479;&#65292;&#32780;&#19981;&#24433;&#21709;&#35757;&#32451;&#65292;&#30456;&#23545;&#20110;&#20854;&#20182;&#20004;&#31181;&#24120;&#35265;&#31070;&#32463;&#31526;&#21495;&#25216;&#26415;&#20855;&#26377;&#29702;&#35770;&#21644;&#23454;&#38469;&#20248;&#21183;&#65292;&#22312;&#22810;&#23610;&#24230;&#26041;&#27861;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#20854;&#23545;&#32593;&#32476;&#35268;&#27169;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13019v1 &#20844;&#21578;&#31867;&#22411;:&#26032; &#25277;&#35937;:&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#26159;&#19968;&#20010;&#26085;&#30410;&#21457;&#23637;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#26088;&#22312;&#23558;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#33021;&#21147;&#19982;&#31526;&#21495;&#31995;&#32479;&#30340;&#25512;&#29702;&#33021;&#21147;&#30456;&#32467;&#21512;&#12290;&#36825;&#31181;&#28151;&#21512;&#21487;&#20197;&#37319;&#29992;&#22810;&#31181;&#24418;&#24335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24418;&#24335;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#20855;&#26377;&#21629;&#39064;&#32972;&#26223;&#30693;&#35782;&#30340;&#30417;&#30563;&#24335;&#22810;&#26631;&#31614;&#20998;&#31867;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#35821;&#20041;&#35843;&#33410;&#30340;&#26032;&#30340;&#31070;&#32463;&#31526;&#21495;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#20165;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#32422;&#26463;&#31995;&#32479;&#65292;&#32780;&#19981;&#24433;&#21709;&#35757;&#32451;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#23427;&#30456;&#23545;&#20110;&#21478;&#22806;&#20004;&#31181;&#27969;&#34892;&#30340;&#31070;&#32463;&#31526;&#21495;&#25216;&#26415;&#8212;&#8212;&#35821;&#20041;&#35843;&#33410;&#21644;&#35821;&#20041;&#27491;&#21017;&#21270;&#30340;&#29702;&#35770;&#21644;&#23454;&#38469;&#20248;&#21183;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#23610;&#24230;&#26041;&#27861;&#65292;&#35780;&#20272;&#31070;&#32463;&#31526;&#21495;&#25216;&#26415;&#30340;&#22909;&#22788;&#38543;&#32593;&#32476;&#35268;&#27169;&#30340;&#21464;&#21270;&#32780;&#21457;&#23637;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#20960;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#35780;&#20272;&#24182;&#27604;&#36739;&#36825;&#19977;&#31181;&#25216;&#26415;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#35821;&#20041;&#35843;&#33410;&#8230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13019v1 Announce Type: new  Abstract: Neurosymbolic AI is a growing field of research aiming to combine neural networks learning capabilities with the reasoning abilities of symbolic systems. This hybridization can take many shapes. In this paper, we propose a new formalism for supervised multi-label classification with propositional background knowledge. We introduce a new neurosymbolic technique called semantic conditioning at inference, which only constrains the system during inference while leaving the training unaffected. We discuss its theoritical and practical advantages over two other popular neurosymbolic techniques: semantic conditioning and semantic regularization. We develop a new multi-scale methodology to evaluate how the benefits of a neurosymbolic technique evolve with the scale of the network. We then evaluate experimentally and compare the benefits of all three techniques across model scales on several datasets. Our results demonstrate that semantic conditi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#8220;&#27169;&#22411;&#27744;&#8221;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#25968;&#25454;&#33976;&#39311;&#36807;&#31243;&#20013;&#36873;&#25321;&#22810;&#26679;&#30340;&#27169;&#22411;&#65292;&#32467;&#21512;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#33976;&#39311;&#25968;&#25454;&#38598;&#30340;&#27979;&#35797;&#36807;&#31243;&#65292;&#20174;&#32780;&#25913;&#36827;&#25968;&#25454;&#38598;&#33976;&#39311;&#30340;&#36328;&#26550;&#26500;&#27867;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.13007</link><description>&lt;p&gt;
&#25913;&#36827;&#25968;&#25454;&#38598;&#33976;&#39311;&#19978;&#30340;&#36328;&#26550;&#26500;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Improve Cross-Architecture Generalization on Dataset Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13007
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#8220;&#27169;&#22411;&#27744;&#8221;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#25968;&#25454;&#33976;&#39311;&#36807;&#31243;&#20013;&#36873;&#25321;&#22810;&#26679;&#30340;&#27169;&#22411;&#65292;&#32467;&#21512;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#33976;&#39311;&#25968;&#25454;&#38598;&#30340;&#27979;&#35797;&#36807;&#31243;&#65292;&#20174;&#32780;&#25913;&#36827;&#25968;&#25454;&#38598;&#33976;&#39311;&#30340;&#36328;&#26550;&#26500;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38598;&#33976;&#39311;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#19968;&#31181;&#23454;&#29992;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#20174;&#29616;&#26377;&#36739;&#22823;&#30340;&#25968;&#25454;&#38598;&#20013;&#21019;&#24314;&#19968;&#20010;&#36739;&#23567;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#33976;&#39311;&#26041;&#27861;&#20027;&#35201;&#37319;&#29992;&#22522;&#20110;&#27169;&#22411;&#30340;&#33539;&#24335;&#65292;&#20854;&#20013;&#21512;&#25104;&#25968;&#25454;&#38598;&#32487;&#25215;&#20102;&#29305;&#23450;&#27169;&#22411;&#30340;&#20559;&#35265;&#65292;&#38480;&#21046;&#20102;&#20854;&#23545;&#26367;&#20195;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#27169;&#22411;&#27744;&#8221;&#30340;&#26032;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#25968;&#25454;&#33976;&#39311;&#36807;&#31243;&#20013;&#26681;&#25454;&#29305;&#23450;&#27010;&#29575;&#20998;&#24067;&#20174;&#22810;&#26679;&#30340;&#27169;&#22411;&#27744;&#20013;&#36873;&#25321;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#27169;&#22411;&#27744;&#19982;&#24050;&#24314;&#31435;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#24182;&#23558;&#30693;&#35782;&#33976;&#39311;&#24212;&#29992;&#20110;&#33976;&#39311;&#25968;&#25454;&#38598;&#30340;&#27979;&#35797;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#27169;&#22411;&#27744;&#26041;&#27861;&#22312;&#19968;&#31995;&#21015;&#29616;&#26377;&#27169;&#22411;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#21516;&#26102;&#22312;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13007v1 Announce Type: new  Abstract: Dataset distillation, a pragmatic approach in machine learning, aims to create a smaller synthetic dataset from a larger existing dataset. However, existing distillation methods primarily adopt a model-based paradigm, where the synthetic dataset inherits model-specific biases, limiting its generalizability to alternative models. In response to this constraint, we propose a novel methodology termed "model pool". This approach involves selecting models from a diverse model pool based on a specific probability distribution during the data distillation process. Additionally, we integrate our model pool with the established knowledge distillation approach and apply knowledge distillation to the test process of the distilled dataset. Our experimental results validate the effectiveness of the model pool approach across a range of existing models while testing, demonstrating superior performance compared to existing methodologies.
&lt;/p&gt;</description></item><item><title>&#27169;&#22411;&#31283;&#23450;&#24615;&#23545;&#35299;&#37322;&#21644;&#19981;&#30830;&#23450;&#24615;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#24182;&#21457;&#29616;&#23454;&#38469;&#25200;&#21160;&#23545;&#24615;&#33021;&#21644;&#35299;&#37322;&#24433;&#21709;&#36739;&#23567;&#65292;&#20294;&#25513;&#30422;&#21364;&#26377; drastical &#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.13006</link><description>&lt;p&gt;
&#25506;&#31350;&#27169;&#22411;&#19981;&#31283;&#23450;&#24615;&#23545;&#35299;&#37322;&#21644;&#19981;&#30830;&#23450;&#24615;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Investigating the Impact of Model Instability on Explanations and Uncertainty
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13006
&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#31283;&#23450;&#24615;&#23545;&#35299;&#37322;&#21644;&#19981;&#30830;&#23450;&#24615;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#24182;&#21457;&#29616;&#23454;&#38469;&#25200;&#21160;&#23545;&#24615;&#33021;&#21644;&#35299;&#37322;&#24433;&#21709;&#36739;&#23567;&#65292;&#20294;&#25513;&#30422;&#21364;&#26377; drastical &#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;AI&#26041;&#27861;&#26377;&#21161;&#20110;&#29702;&#35299;&#27169;&#22411;&#34892;&#20026;&#65292;&#28982;&#32780;&#65292;&#23545;&#36755;&#20837;&#36827;&#34892;&#24494;&#23567;&#12289;&#19981;&#21487;&#23519;&#35273;&#30340;&#25200;&#21160;&#21487;&#33021;&#20250;&#26497;&#22823;&#22320;&#25197;&#26354;&#35299;&#37322;&#12290;&#36825;&#20123;&#35299;&#37322;&#36890;&#24120;&#22312;&#27169;&#22411;&#37096;&#32626;&#20043;&#21069;&#34987;&#20840;&#38754;&#35780;&#20272;&#65292;&#22240;&#27492;&#24456;&#38590;&#35780;&#20272;&#29305;&#23450;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#12290;&#19968;&#20123;&#30740;&#31350;&#24050;&#32463;&#23581;&#35797;&#20026;&#35299;&#37322;&#21019;&#24314;&#32622;&#20449;&#24230;&#20272;&#35745;&#22120;&#65292;&#20294;&#27809;&#26377;&#20154;&#35843;&#26597;&#19981;&#30830;&#23450;&#24615;&#21644;&#35299;&#37322;&#36136;&#37327;&#20043;&#38388;&#30340;&#29616;&#26377;&#32852;&#31995;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#25512;&#26029;&#26102;&#24341;&#20837;&#22122;&#22768;&#26469;&#20154;&#20026;&#27169;&#25311;&#25991;&#26412;&#36755;&#20837;&#20013;&#30340;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#12290;&#22312;&#36825;&#39033;&#22823;&#35268;&#27169;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25554;&#20837;&#19981;&#21516;&#32423;&#21035;&#30340;&#22122;&#22768;&#25200;&#21160;&#65292;&#24182;&#27979;&#37327;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#21644;&#19981;&#21516;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#30340;&#24433;&#21709;&#12290;&#23454;&#38469;&#25200;&#21160;&#23545;&#24615;&#33021;&#21644;&#35299;&#37322;&#30340;&#24433;&#21709;&#24456;&#23567;&#65292;&#28982;&#32780;&#25513;&#30422;&#21364;&#26377; drastical &#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#39640;&#19981;&#30830;&#23450;&#24615;&#24182;&#19981;&#19968;&#23450;&#24847;&#21619;&#30528;&#35299;&#37322;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13006v1 Announce Type: cross  Abstract: Explainable AI methods facilitate the understanding of model behaviour, yet, small, imperceptible perturbations to inputs can vastly distort explanations. As these explanations are typically evaluated holistically, before model deployment, it is difficult to assess when a particular explanation is trustworthy. Some studies have tried to create confidence estimators for explanations, but none have investigated an existing link between uncertainty and explanation quality. We artificially simulate epistemic uncertainty in text input by introducing noise at inference time. In this large-scale empirical study, we insert different levels of noise perturbations and measure the effect on the output of pre-trained language models and different uncertainty metrics. Realistic perturbations have minimal effect on performance and explanations, yet masking has a drastic effect. We find that high uncertainty doesn't necessarily imply low explanation 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;SzCORE&#26694;&#26550;&#65292;&#29992;&#20110;&#39564;&#35777;&#22522;&#20110;&#33041;&#30005;&#22270;&#30340;&#33258;&#21160;&#30315;&#30187;&#26816;&#27979;&#31639;&#27861;&#65292;&#26088;&#22312;&#26631;&#20934;&#21270;&#39564;&#35777;&#26041;&#27861;&#65292;&#21253;&#25324;&#25968;&#25454;&#38598;&#12289;&#25991;&#20214;&#26684;&#24335;&#12289;&#36755;&#20837;&#20869;&#23481;&#12289;&#24615;&#33021;&#24230;&#37327;&#31561;&#12290;</title><link>https://arxiv.org/abs/2402.13005</link><description>&lt;p&gt;
SzCORE&#65306;&#29992;&#20110;&#39564;&#35777;&#22522;&#20110;&#33041;&#30005;&#22270;&#30340;&#33258;&#21160;&#30315;&#30187;&#26816;&#27979;&#31639;&#27861;&#30340;&#30315;&#30187;&#31038;&#21306;&#24320;&#28304;&#30740;&#31350;&#35780;&#20272;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
SzCORE: A Seizure Community Open-source Research Evaluation framework for the validation of EEG-based automated seizure detection algorithms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13005
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;SzCORE&#26694;&#26550;&#65292;&#29992;&#20110;&#39564;&#35777;&#22522;&#20110;&#33041;&#30005;&#22270;&#30340;&#33258;&#21160;&#30315;&#30187;&#26816;&#27979;&#31639;&#27861;&#65292;&#26088;&#22312;&#26631;&#20934;&#21270;&#39564;&#35777;&#26041;&#27861;&#65292;&#21253;&#25324;&#25968;&#25454;&#38598;&#12289;&#25991;&#20214;&#26684;&#24335;&#12289;&#36755;&#20837;&#20869;&#23481;&#12289;&#24615;&#33021;&#24230;&#37327;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23478;&#24237;&#21644;&#38271;&#26399;&#33041;&#30005;&#22270;&#30417;&#27979;&#30340;&#22686;&#21152;&#65292;&#22522;&#20110;&#33041;&#30005;&#22270;&#30340;&#39640;&#36136;&#37327;&#33258;&#21160;&#30315;&#30187;&#26816;&#27979;&#31639;&#27861;&#30340;&#38656;&#27714;&#21464;&#24471;&#26356;&#21152;&#36843;&#20999;&#12290;&#36825;&#20123;&#31639;&#27861;&#39564;&#35777;&#26041;&#27861;&#30340;&#24322;&#36136;&#24615;&#24433;&#21709;&#20102;&#25253;&#21578;&#30340;&#32467;&#26524;&#65292;&#24182;&#20351;&#20840;&#38754;&#35780;&#20272;&#21644;&#27604;&#36739;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#35813;&#24322;&#36136;&#24615;&#20027;&#35201;&#28041;&#21450;&#25968;&#25454;&#38598;&#30340;&#36873;&#25321;&#12289;&#35780;&#20272;&#26041;&#27861;&#21644;&#24615;&#33021;&#24230;&#37327;&#31561;&#26041;&#38754;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#24314;&#31435;EEG&#22522;&#30784;&#30315;&#30187;&#26816;&#27979;&#31639;&#27861;&#39564;&#35777;&#30340;&#26631;&#20934;&#21270;&#12290;&#22522;&#20110;&#29616;&#26377;&#25351;&#21335;&#21644;&#24314;&#35758;&#65292;&#35813;&#26694;&#26550;&#24341;&#20837;&#20102;&#19968;&#32452;&#20851;&#20110;&#25968;&#25454;&#38598;&#12289;&#25991;&#20214;&#26684;&#24335;&#12289;EEG&#25968;&#25454;&#36755;&#20837;&#20869;&#23481;&#12289;&#30315;&#30187;&#27880;&#37322;&#36755;&#20837;&#21644;&#36755;&#20986;&#12289;&#20132;&#21449;&#39564;&#35777;&#31574;&#30053;&#20197;&#21450;&#24615;&#33021;&#24230;&#37327;&#30340;&#24314;&#35758;&#21644;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13005v1 Announce Type: cross  Abstract: The need for high-quality automated seizure detection algorithms based on electroencephalography (EEG) becomes ever more pressing with the increasing use of ambulatory and long-term EEG monitoring. Heterogeneity in validation methods of these algorithms influences the reported results and makes comprehensive evaluation and comparison challenging. This heterogeneity concerns in particular the choice of datasets, evaluation methodologies, and performance metrics. In this paper, we propose a unified framework designed to establish standardization in the validation of EEG-based seizure detection algorithms. Based on existing guidelines and recommendations, the framework introduces a set of recommendations and standards related to datasets, file formats, EEG data input content, seizure annotation input and output, cross-validation strategies, and performance metrics. We also propose the 10-20 seizure detection benchmark, a machine-learning 
&lt;/p&gt;</description></item><item><title>&#37327;&#23376;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21487;&#22522;&#20110;&#22270;&#24577;&#29702;&#35299;&#21644;&#23454;&#29616;&#65292;&#21487;&#29992;&#20316;&#21442;&#25968;&#21270;&#30340;&#37327;&#23376;&#30005;&#36335;&#26469;&#34920;&#31034;&#31070;&#32463;&#32593;&#32476;&#65292;&#25110;&#20316;&#20026;&#26500;&#24314;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22522;&#30784;&#32467;&#26500;&#12290;</title><link>https://arxiv.org/abs/2402.13001</link><description>&lt;p&gt;
&#19968;&#20010;&#32479;&#19968;&#30340;&#37327;&#23376;&#22270;&#31070;&#32463;&#32593;&#32476;&#20027;&#35201;&#26694;&#26550;&#26469;&#33258;&#37327;&#23376;&#22270;&#24577;
&lt;/p&gt;
&lt;p&gt;
A unifying primary framework for quantum graph neural networks from quantum graph states
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13001
&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21487;&#22522;&#20110;&#22270;&#24577;&#29702;&#35299;&#21644;&#23454;&#29616;&#65292;&#21487;&#29992;&#20316;&#21442;&#25968;&#21270;&#30340;&#37327;&#23376;&#30005;&#36335;&#26469;&#34920;&#31034;&#31070;&#32463;&#32593;&#32476;&#65292;&#25110;&#20316;&#20026;&#26500;&#24314;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22522;&#30784;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24577;&#34987;&#29992;&#26469;&#23558;&#25968;&#23398;&#22270;&#34920;&#31034;&#20026;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#30340;&#37327;&#23376;&#29366;&#24577;&#12290;&#23427;&#20204;&#21487;&#20197;&#36890;&#36807;&#31283;&#23450;&#23376;&#30721;&#25110;&#30452;&#25509;&#30340;&#37327;&#23376;&#38376;&#21644;&#37327;&#23376;&#29366;&#24577;&#26469;&#26500;&#24314;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#37327;&#23376;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21487;&#20197;&#22522;&#20110;&#22270;&#24577;&#21152;&#20197;&#29702;&#35299;&#21644;&#23454;&#29616;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#20204;&#21487;&#20197;&#34987;&#29992;&#20316;&#21442;&#25968;&#21270;&#30340;&#37327;&#23376;&#30005;&#36335;&#26469;&#34920;&#31034;&#31070;&#32463;&#32593;&#32476;&#65292;&#25110;&#20316;&#20026;&#26500;&#24314;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22522;&#30784;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13001v1 Announce Type: cross  Abstract: Graph states are used to represent mathematical graphs as quantum states on quantum computers. They can be formulated through stabilizer codes or directly quantum gates and quantum states. In this paper we show that a quantum graph neural network model can be understood and realized based on graph states. We show that they can be used either as a parameterized quantum circuits to represent neural networks or as an underlying structure to construct graph neural networks on quantum computers.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#20174;&#21270;&#23398;&#25991;&#29486;&#20013;&#39640;&#20445;&#30495;&#25552;&#21462;&#20449;&#24687;&#65292;&#20805;&#24403;&#21270;&#23398;&#21161;&#25163;&#30340;&#35282;&#33394;&#65292;&#33258;&#21160;&#21270;&#25968;&#25454;&#25910;&#38598;&#21644;&#20998;&#26512;&#65292;&#20174;&#32780;&#25552;&#39640;&#24037;&#20316;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.12993</link><description>&lt;p&gt;
&#29992;&#20110;&#21270;&#23398;&#25991;&#29486;&#25968;&#25454;&#25366;&#25496;&#30340;&#33258;&#20027;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
An Autonomous Large Language Model Agent for Chemical Literature Data Mining
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12993
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#20174;&#21270;&#23398;&#25991;&#29486;&#20013;&#39640;&#20445;&#30495;&#25552;&#21462;&#20449;&#24687;&#65292;&#20805;&#24403;&#21270;&#23398;&#21161;&#25163;&#30340;&#35282;&#33394;&#65292;&#33258;&#21160;&#21270;&#25968;&#25454;&#25910;&#38598;&#21644;&#20998;&#26512;&#65292;&#20174;&#32780;&#25552;&#39640;&#24037;&#20316;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21270;&#23398;&#21512;&#25104;&#23545;&#20110;&#25512;&#21160;&#26448;&#26009;&#21512;&#25104;&#21644;&#33647;&#29289;&#21457;&#29616;&#33267;&#20851;&#37325;&#35201;&#65292;&#24433;&#21709;&#30528;&#21253;&#25324;&#29615;&#22659;&#31185;&#23398;&#21644;&#21307;&#30103;&#20445;&#20581;&#22312;&#20869;&#30340;&#21508;&#20010;&#39046;&#22495;&#12290;&#21270;&#23398;&#39046;&#22495;&#30340;&#25216;&#26415;&#19978;&#21319;&#20351;&#24471;&#20135;&#29983;&#20102;&#22823;&#37327;&#30340;&#21270;&#23398;&#25968;&#25454;&#65292;&#25361;&#25112;&#30740;&#31350;&#20154;&#21592;&#21435;&#35782;&#21035;&#27169;&#24335;&#24182;&#32454;&#21270;&#21512;&#25104;&#36807;&#31243;&#12290;&#20154;&#24037;&#26234;&#33021;&#36890;&#36807;&#20998;&#26512;&#25968;&#25454;&#26469;&#20248;&#21270;&#21512;&#25104;&#24182;&#25552;&#39640;&#20135;&#37327;&#12290;&#28982;&#32780;&#65292;&#20154;&#24037;&#26234;&#33021;&#22312;&#22788;&#29702;&#25991;&#29486;&#25968;&#25454;&#26041;&#38754;&#38754;&#20020;&#30528;&#25361;&#25112;&#65292;&#22240;&#20026;&#21270;&#23398;&#25991;&#29486;&#30340;&#32467;&#26500;&#19981;&#35268;&#25972;&#65292;&#20889;&#20316;&#39118;&#26684;&#22810;&#26679;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#22256;&#38590;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#26694;&#26550;&#65292;&#33021;&#22815;&#20174;&#24191;&#27867;&#30340;&#21270;&#23398;&#25991;&#29486;&#20013;&#39640;&#20445;&#30495;&#22320;&#25552;&#21462;&#20449;&#24687;&#12290;&#36825;&#20010;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#24555;&#36895;&#29983;&#25104;&#21644;&#36845;&#20195;&#20248;&#21270;&#12290;&#23427;&#20805;&#24403;&#21270;&#23398;&#21161;&#25163;&#30340;&#35282;&#33394;&#65292;&#33258;&#21160;&#21270;&#25968;&#25454;&#25910;&#38598;&#21644;&#20998;&#26512;&#65292;&#20174;&#32780;&#33410;&#30465;&#20154;&#21147;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12993v1 Announce Type: cross  Abstract: Chemical synthesis, which is crucial for advancing material synthesis and drug discovery, impacts various sectors including environmental science and healthcare. The rise of technology in chemistry has generated extensive chemical data, challenging researchers to discern patterns and refine synthesis processes. Artificial intelligence (AI) helps by analyzing data to optimize synthesis and increase yields. However, AI faces challenges in processing literature data due to the unstructured format and diverse writing style of chemical literature. To overcome these difficulties, we introduce an end-to-end AI agent framework capable of high-fidelity extraction from extensive chemical literature. This AI agent employs large language models (LLMs) for prompt generation and iterative optimization. It functions as a chemistry assistant, automating data collection and analysis, thereby saving manpower and enhancing performance. Our framework's ef
&lt;/p&gt;</description></item><item><title>TRAP&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Targeted Random Adversarial Prompt (TRAP)&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#29305;&#23450;LLM&#30340;&#20351;&#29992;&#65292;&#24182;&#22312;&#21333;&#27425;&#20132;&#20114;&#21518;&#20197;&#36229;&#36807;95%&#30340;&#30495;&#38451;&#24615;&#29575;&#21644;&#20302;&#20110;0.2%&#30340;&#35823;&#25253;&#29575;&#26816;&#27979;&#30446;&#26631;LLMs&#12290;</title><link>https://arxiv.org/abs/2402.12991</link><description>&lt;p&gt;
TRAP: &#38754;&#21521;&#40657;&#30418;&#36523;&#20221;&#39564;&#35777;&#30340;&#26377;&#38024;&#23545;&#24615;&#38543;&#26426;&#23545;&#25239;&#25552;&#31034;&#35825;&#39285;
&lt;/p&gt;
&lt;p&gt;
TRAP: Targeted Random Adversarial Prompt Honeypot for Black-Box Identification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12991
&lt;/p&gt;
&lt;p&gt;
TRAP&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Targeted Random Adversarial Prompt (TRAP)&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#29305;&#23450;LLM&#30340;&#20351;&#29992;&#65292;&#24182;&#22312;&#21333;&#27425;&#20132;&#20114;&#21518;&#20197;&#36229;&#36807;95%&#30340;&#30495;&#38451;&#24615;&#29575;&#21644;&#20302;&#20110;0.2%&#30340;&#35823;&#25253;&#29575;&#26816;&#27979;&#30446;&#26631;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26381;&#21153;&#21644;&#27169;&#22411;&#36890;&#24120;&#20276;&#38543;&#30528;&#20851;&#20110;&#35841;&#21487;&#20197;&#20351;&#29992;&#23427;&#20204;&#20197;&#21450;&#20182;&#20204;&#24517;&#39035;&#22914;&#20309;&#20351;&#29992;&#23427;&#20204;&#30340;&#27861;&#24459;&#35268;&#23450;&#12290;&#35780;&#20272;&#21457;&#24067;&#30340;LLMs&#30340;&#21512;&#35268;&#24615;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#22240;&#20026;&#36825;&#20123;&#35268;&#23450;&#20445;&#25252;&#20102;LLM&#36129;&#29486;&#32773;&#30340;&#21033;&#30410;&#24182;&#38450;&#27490;&#20102;&#28389;&#29992;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#40657;&#30418;&#36523;&#20221;&#39564;&#35777;&#65288;BBIV&#65289;&#30340;&#26032;&#38382;&#39064;&#12290;&#20854;&#30446;&#26631;&#26159;&#30830;&#23450;&#31532;&#19977;&#26041;&#24212;&#29992;&#26159;&#21542;&#36890;&#36807;&#20854;&#32842;&#22825;&#21151;&#33021;&#20351;&#29992;&#26576;&#20010;&#29305;&#23450;&#30340;LLM&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#30446;&#26631;&#38543;&#26426;&#23545;&#25239;&#25552;&#31034;&#65288;TRAP&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#27491;&#22312;&#20351;&#29992;&#30340;&#20855;&#20307;LLM&#12290;&#25105;&#20204;&#37325;&#26032;&#21033;&#29992;&#20102;&#26368;&#21021;&#29992;&#20110;&#36234;&#29425;&#30340;&#23545;&#25239;&#24615;&#21518;&#32512;&#65292;&#20197;&#20174;&#30446;&#26631;LLM&#33719;&#24471;&#39044;&#23450;&#20041;&#30340;&#31572;&#26696;&#65292;&#32780;&#20854;&#20182;&#27169;&#22411;&#21017;&#32473;&#20986;&#38543;&#26426;&#31572;&#26696;&#12290;TRAP&#21487;&#20197;&#22312;&#21333;&#27425;&#20132;&#20114;&#21518;&#20197;&#36229;&#36807;95%&#30340;&#30495;&#38451;&#24615;&#29575;&#21644;&#20302;&#20110;0.2%&#30340;&#35823;&#25253;&#29575;&#26816;&#27979;&#30446;&#26631;LLMs&#12290;&#21363;&#20351;LLM&#26377;&#19981;&#20250;&#26174;&#33879;&#25913;&#21464;&#30340;&#32454;&#24494;&#21464;&#21270;&#65292;TRAP&#20173;&#28982;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12991v1 Announce Type: cross  Abstract: Large Language Model (LLM) services and models often come with legal rules on who can use them and how they must use them. Assessing the compliance of the released LLMs is crucial, as these rules protect the interests of the LLM contributor and prevent misuse. In this context, we describe the novel problem of Black-box Identity Verification (BBIV). The goal is to determine whether a third-party application uses a certain LLM through its chat function. We propose a method called Targeted Random Adversarial Prompt (TRAP) that identifies the specific LLM in use. We repurpose adversarial suffixes, originally proposed for jailbreaking, to get a pre-defined answer from the target LLM, while other models give random answers. TRAP detects the target LLMs with over 95% true positive rate at under 0.2% false positive rate even after a single interaction. TRAP remains effective even if the LLM has minor changes that do not significantly alter the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20851;&#27880;&#38754;&#21521;&#22270;&#32467;&#26500;&#28436;&#21270;&#30340;&#33410;&#28857;&#24335;&#22270;&#22686;&#37327;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32467;&#26500;&#36716;&#31227;&#30340;&#22522;&#20110;&#27491;&#21017;&#21270;&#30340;&#25216;&#26415;</title><link>https://arxiv.org/abs/2402.12987</link><description>&lt;p&gt;
&#38754;&#21521;&#28436;&#21270;&#22270;&#30340;&#31283;&#20581;&#22270;&#22686;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Robust Graph Incremental Learning on Evolving Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12987
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#38754;&#21521;&#22270;&#32467;&#26500;&#28436;&#21270;&#30340;&#33410;&#28857;&#24335;&#22270;&#22686;&#37327;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32467;&#26500;&#36716;&#31227;&#30340;&#22522;&#20110;&#27491;&#21017;&#21270;&#30340;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22686;&#37327;&#23398;&#20064;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#28041;&#21450;&#22312;&#19968;&#31995;&#21015;&#20219;&#21153;&#19978;&#35757;&#32451;&#27169;&#22411;&#65292;&#32780;&#19981;&#26159;&#19968;&#27425;&#24615;&#22788;&#29702;&#25152;&#26377;&#20219;&#21153;&#12290;&#36825;&#31181;&#33021;&#22815;&#20174;&#20219;&#21153;&#27969;&#20013;&#36880;&#27493;&#23398;&#20064;&#30340;&#33021;&#21147;&#23545;&#35768;&#22810;&#29616;&#23454;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#22270;&#32467;&#26500;&#25968;&#25454;&#19978;&#36827;&#34892;&#22686;&#37327;&#23398;&#20064;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#35768;&#22810;&#19982;&#22270;&#30456;&#20851;&#30340;&#38382;&#39064;&#28041;&#21450;&#21040;&#23545;&#27599;&#20010;&#33410;&#28857;&#30340;&#39044;&#27979;&#20219;&#21153;&#65292;&#34987;&#31216;&#20026;&#33410;&#28857;&#24335;&#22270;&#22686;&#37327;&#23398;&#20064;&#65288;NGIL&#65289;&#12290;&#36825;&#22312;&#26679;&#26412;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#24341;&#20837;&#20102;&#38750;&#29420;&#31435;&#21644;&#38750;&#21516;&#20998;&#24067;&#29305;&#24449;&#65292;&#20351;&#24471;&#22312;&#28155;&#21152;&#26032;&#20219;&#21153;&#26102;&#38590;&#20197;&#20445;&#25345;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#20851;&#27880;&#24402;&#32435;&#24335;NGIL&#38382;&#39064;&#65292;&#32771;&#34385;&#20102;&#30001;&#26032;&#20219;&#21153;&#24341;&#36215;&#30340;&#22270;&#32467;&#26500;&#28436;&#21270;&#65288;&#32467;&#26500;&#36716;&#31227;&#65289;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#35813;&#38382;&#39064;&#30340;&#27491;&#24335;&#24418;&#24335;&#21270;&#21644;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#32467;&#26500;&#36716;&#31227;&#30340;&#26032;&#22411;&#22522;&#20110;&#27491;&#21017;&#21270;&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12987v1 Announce Type: new  Abstract: Incremental learning is a machine learning approach that involves training a model on a sequence of tasks, rather than all tasks at once. This ability to learn incrementally from a stream of tasks is crucial for many real-world applications. However, incremental learning is a challenging problem on graph-structured data, as many graph-related problems involve prediction tasks for each individual node, known as Node-wise Graph Incremental Learning (NGIL). This introduces non-independent and non-identically distributed characteristics in the sample data generation process, making it difficult to maintain the performance of the model as new tasks are added. In this paper, we focus on the inductive NGIL problem, which accounts for the evolution of graph structure (structural shift) induced by emerging tasks. We provide a formal formulation and analysis of the problem, and propose a novel regularization-based technique called Structural-Shift
&lt;/p&gt;</description></item><item><title>&#22312;&#19981;&#21487;&#24494;&#20998;&#20294;&#23637;&#24320;&#30340;&#22521;&#35757;&#35774;&#32622;&#25903;&#25345;&#19979;&#65292;&#36890;&#36807;&#25968;&#20540;&#27714;&#35299;&#22120;&#25903;&#25345;&#30340;&#31070;&#32463;&#29289;&#29702;&#27169;&#25311;&#22120;&#33021;&#22815;&#33719;&#24471;&#27604;&#23436;&#20840;&#21487;&#24494;&#21270;&#39044;&#27979;&#35774;&#32622;&#39640;&#20986;4.5&#20493;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.12971</link><description>&lt;p&gt;
&#22914;&#20309;&#36890;&#36807;&#26102;&#38388;&#23637;&#24320;&#25903;&#25345;&#31070;&#32463;&#29289;&#29702;&#27169;&#25311;&#22120;
&lt;/p&gt;
&lt;p&gt;
How Temporal Unrolling Supports Neural Physics Simulators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12971
&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#21487;&#24494;&#20998;&#20294;&#23637;&#24320;&#30340;&#22521;&#35757;&#35774;&#32622;&#25903;&#25345;&#19979;&#65292;&#36890;&#36807;&#25968;&#20540;&#27714;&#35299;&#22120;&#25903;&#25345;&#30340;&#31070;&#32463;&#29289;&#29702;&#27169;&#25311;&#22120;&#33021;&#22815;&#33719;&#24471;&#27604;&#23436;&#20840;&#21487;&#24494;&#21270;&#39044;&#27979;&#35774;&#32622;&#39640;&#20986;4.5&#20493;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26102;&#38388;&#19978;&#23637;&#24320;&#22521;&#35757;&#36712;&#36857;&#24378;&#28872;&#24433;&#21709;&#31070;&#32463;&#32593;&#32476;&#22686;&#24378;&#22411;&#29289;&#29702;&#27169;&#25311;&#22120;&#30340;&#25512;&#29702;&#31934;&#24230;&#12290;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#19977;&#31181;&#19981;&#21516;&#20110;&#31163;&#25955;GroundTruth&#36712;&#36857;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#21464;&#20307;&#26469;&#20998;&#26512;&#36825;&#20123;&#24433;&#21709;&#12290;&#38500;&#20102;&#24120;&#29992;&#30340;&#19968;&#27493;&#35774;&#32622;&#21644;&#23436;&#20840;&#21487;&#24494;&#30340;&#23637;&#24320;&#22806;&#65292;&#25105;&#20204;&#36824;&#21253;&#25324;&#31532;&#19977;&#31181;&#19981;&#22826;&#24120;&#29992;&#30340;&#21464;&#20307;&#65306;&#27809;&#26377;&#26102;&#38388;&#26799;&#24230;&#30340;&#23637;&#24320;&#12290;&#27604;&#36739;&#20351;&#29992;&#36825;&#19977;&#31181;&#27169;&#24335;&#35757;&#32451;&#30340;&#32593;&#32476;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#20998;&#35299;&#20986;&#23637;&#24320;&#30340;&#20004;&#20010;&#20027;&#35201;&#24433;&#21709;&#65292;&#21363;&#35757;&#32451;&#20998;&#24067;&#30340;&#36716;&#21464;&#21644;&#38271;&#26399;&#26799;&#24230;&#12290;&#25105;&#20204;&#22312;&#29289;&#29702;&#31995;&#32479;&#12289;&#32593;&#32476;&#22823;&#23567;&#12289;&#32593;&#32476;&#26550;&#26500;&#12289;&#35757;&#32451;&#35774;&#32622;&#21644;&#27979;&#35797;&#22330;&#26223;&#20013;&#36827;&#34892;&#20102;&#35814;&#32454;&#30740;&#31350;&#12290;&#23427;&#20026;&#25105;&#20204;&#30340;&#20027;&#35201;&#21457;&#29616;&#25552;&#20379;&#20102;&#32463;&#39564;&#22522;&#30784;&#65306;&#36890;&#36807;&#25968;&#20540;&#27714;&#35299;&#22120;&#25903;&#25345;&#30340;&#19981;&#21487;&#24494;&#20998;&#20294;&#23637;&#24320;&#30340;&#22521;&#35757;&#35774;&#32622;&#65292;&#21487;&#20197;&#20351;&#39044;&#27979;&#35774;&#32622;&#30340;&#23436;&#20840;&#21487;&#24494;&#21270;&#24102;&#26469;4.5&#20493;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12971v1 Announce Type: cross  Abstract: Unrolling training trajectories over time strongly influences the inference accuracy of neural network-augmented physics simulators. We analyze these effects by studying three variants of training neural networks on discrete ground truth trajectories. In addition to commonly used one-step setups and fully differentiable unrolling, we include a third, less widely used variant: unrolling without temporal gradients. Comparing networks trained with these three modalities makes it possible to disentangle the two dominant effects of unrolling, training distribution shift and long-term gradients. We present a detailed study across physical systems, network sizes, network architectures, training setups, and test scenarios. It provides an empirical basis for our main findings: A non-differentiable but unrolled training setup supported by a numerical solver can yield 4.5-fold improvements over a fully differentiable prediction setup that does no
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#26597;&#35810;&#22270;&#20013;&#24120;&#37327;&#21644;&#21464;&#37327;&#20043;&#38388;&#24046;&#24322;&#65292;&#33021;&#21160;&#24577;&#27979;&#37327;&#28040;&#24687;&#37325;&#35201;&#24615;&#24182;&#25429;&#25417;&#38544;&#24335;&#36923;&#36753;&#20381;&#36182;&#20851;&#31995;&#30340;&#26465;&#20214;&#36923;&#36753;&#28040;&#24687;&#20256;&#36882;&#21464;&#21387;&#22120;&#12290;</title><link>https://arxiv.org/abs/2402.12954</link><description>&lt;p&gt;
&#29992;&#20110;&#22797;&#26434;&#26597;&#35810;&#22238;&#31572;&#30340;&#26465;&#20214;&#36923;&#36753;&#28040;&#24687;&#20256;&#36882;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
Conditional Logical Message Passing Transformer for Complex Query Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12954
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#26597;&#35810;&#22270;&#20013;&#24120;&#37327;&#21644;&#21464;&#37327;&#20043;&#38388;&#24046;&#24322;&#65292;&#33021;&#21160;&#24577;&#27979;&#37327;&#28040;&#24687;&#37325;&#35201;&#24615;&#24182;&#25429;&#25417;&#38544;&#24335;&#36923;&#36753;&#20381;&#36182;&#20851;&#31995;&#30340;&#26465;&#20214;&#36923;&#36753;&#28040;&#24687;&#20256;&#36882;&#21464;&#21387;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#19978;&#30340;&#22797;&#26434;&#26597;&#35810;&#22238;&#31572;&#65288;CQA&#65289;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#30001;&#20110;KGs&#36890;&#24120;&#26159;&#19981;&#23436;&#25972;&#30340;&#65292;&#25552;&#20986;&#20102;&#31070;&#32463;&#27169;&#22411;&#26469;&#36890;&#36807;&#25191;&#34892;&#22810;&#36339;&#36923;&#36753;&#25512;&#29702;&#26469;&#35299;&#20915;CQA&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#27169;&#22411;&#19981;&#33021;&#21516;&#26102;&#22312;&#19968;&#36339;&#21644;&#22810;&#36339;&#26597;&#35810;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#31070;&#32463;&#38142;&#25509;&#39044;&#27979;&#22120;&#30340;&#36923;&#36753;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#12290;&#34429;&#28982;&#22312;&#19968;&#36339;&#21644;&#22810;&#36339;&#26597;&#35810;&#19978;&#37117;&#26377;&#25928;&#65292;&#20294;&#23427;&#24573;&#30053;&#20102;&#26597;&#35810;&#22270;&#20013;&#24120;&#37327;&#21644;&#21464;&#37327;&#33410;&#28857;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#22312;&#33410;&#28857;&#23884;&#20837;&#26356;&#26032;&#38454;&#27573;&#65292;&#35813;&#26426;&#21046;&#19981;&#33021;&#21160;&#24577;&#34913;&#37327;&#19981;&#21516;&#28040;&#24687;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#19988;&#23427;&#33021;&#21542;&#25429;&#25417;&#19982;&#33410;&#28857;&#21644;&#25509;&#25910;&#28040;&#24687;&#30456;&#20851;&#30340;&#38544;&#24335;&#36923;&#36753;&#20381;&#36182;&#20851;&#31995;&#20173;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26465;&#20214;&#36923;&#36753;&#28040;&#24687;&#20256;&#36882;&#21464;&#21387;&#22120;&#65288;CLMPT&#65289;&#65292;&#32771;&#34385;&#20102;&#26597;&#35810;&#22270;&#20013;&#24120;&#37327;&#21644;&#21464;&#37327;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#19988;&#20855;&#26377;&#21160;&#24577;&#27979;&#37327;&#19981;&#21516;&#28040;&#24687;&#37325;&#35201;&#24615;&#20197;&#21450;&#25429;&#25417;&#19982;&#33410;&#28857;&#21644;&#25509;&#25910;&#28040;&#24687;&#30456;&#20851;&#30340;&#38544;&#24335;&#36923;&#36753;&#20381;&#36182;&#20851;&#31995;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12954v1 Announce Type: cross  Abstract: Complex Query Answering (CQA) over Knowledge Graphs (KGs) is a challenging task. Given that KGs are usually incomplete, neural models are proposed to solve CQA by performing multi-hop logical reasoning. However, most of them cannot perform well on both one-hop and multi-hop queries simultaneously. Recent work proposes a logical message passing mechanism based on the pre-trained neural link predictors. While effective on both one-hop and multi-hop queries, it ignores the difference between the constant and variable nodes in a query graph. In addition, during the node embedding update stage, this mechanism cannot dynamically measure the importance of different messages, and whether it can capture the implicit logical dependencies related to a node and received messages remains unclear. In this paper, we propose Conditional Logical Message Passing Transformer (CLMPT), which considers the difference between constants and variables in the c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#36924;&#36817;&#30340;&#32852;&#37030;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36817;&#20284;&#26679;&#26412;&#26799;&#24230;&#21644;&#32553;&#23567;&#27493;&#38271;&#26469;&#23450;&#20301;&#25104;&#26412;&#20989;&#25968;&#30340;&#26497;&#23567;&#20540;&#65292;&#23454;&#29616;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#23545;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36827;&#34892;&#21327;&#20316;&#35757;&#32451;&#30340;&#25928;&#26524;&#65292;&#24182;&#22312;&#25968;&#20540;&#27169;&#25311;&#20013;&#19982;&#26631;&#20934;&#31639;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;</title><link>https://arxiv.org/abs/2402.12945</link><description>&lt;p&gt;
&#22522;&#20110;&#38543;&#26426;&#36924;&#36817;&#30340;&#32852;&#37030;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Stochastic Approximation Approach to Federated Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12945
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#36924;&#36817;&#30340;&#32852;&#37030;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36817;&#20284;&#26679;&#26412;&#26799;&#24230;&#21644;&#32553;&#23567;&#27493;&#38271;&#26469;&#23450;&#20301;&#25104;&#26412;&#20989;&#25968;&#30340;&#26497;&#23567;&#20540;&#65292;&#23454;&#29616;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#23545;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36827;&#34892;&#21327;&#20316;&#35757;&#32451;&#30340;&#25928;&#26524;&#65292;&#24182;&#22312;&#25968;&#20540;&#27169;&#25311;&#20013;&#19982;&#26631;&#20934;&#31639;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#38543;&#26426;&#36924;&#36817;&#65288;SA&#65289;&#26694;&#26550;&#19979;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#12290; FL&#26159;&#19968;&#31181;&#21327;&#20316;&#26041;&#24335;&#65292;&#29992;&#20110;&#36328;&#19981;&#21516;&#21442;&#19982;&#26041;&#25110;&#23458;&#25143;&#31471;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#23558;&#23427;&#20204;&#30340;&#25968;&#25454;&#38598;&#20013;&#12290; &#27599;&#20010;&#23458;&#25143;&#31471;&#23558;&#26681;&#25454;&#21508;&#33258;&#30340;&#25968;&#25454;&#35757;&#32451;&#19968;&#20010;&#27169;&#22411;&#65292;&#24182;&#23450;&#26399;&#23558;&#26435;&#37325;&#21457;&#36865;&#21040;&#26381;&#21153;&#22120;&#36827;&#34892;&#32858;&#21512;&#12290; &#26381;&#21153;&#22120;&#23545;&#36825;&#20123;&#26435;&#37325;&#36827;&#34892;&#32858;&#21512;&#65292;&#28982;&#21518;&#23458;&#25143;&#31471;&#20351;&#29992;&#36825;&#20123;&#26435;&#37325;&#37325;&#26032;&#21021;&#22987;&#21270;&#20854;&#31070;&#32463;&#32593;&#32476;&#24182;&#32487;&#32493;&#35757;&#32451;&#12290; SA&#26159;&#19968;&#31181;&#20351;&#29992;&#36817;&#20284;&#26679;&#26412;&#26799;&#24230;&#21644;&#32553;&#23567;&#27493;&#38271;&#26469;&#23450;&#20301;&#25104;&#26412;&#20989;&#25968;&#26497;&#23567;&#20540;&#30340;&#36845;&#20195;&#31639;&#27861;&#12290; &#26412;&#25991;&#20013;&#65292;&#23458;&#25143;&#31471;&#20351;&#29992;&#38543;&#26426;&#36924;&#36817;&#36845;&#20195;&#26356;&#26032;&#20854;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#12290; &#32467;&#26524;&#34920;&#26126;&#65292;&#32858;&#21512;&#26435;&#37325;&#36319;&#36394;&#19968;&#20010;&#33258;&#27835;ODE&#12290; &#36827;&#34892;&#20102;&#25968;&#20540;&#27169;&#25311;&#65292;&#24182;&#23558;&#32467;&#26524;&#19982;FedAvg&#21644;FedProx&#31561;&#26631;&#20934;&#31639;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12945v1 Announce Type: new  Abstract: This paper examines Federated learning (FL) in a Stochastic Approximation (SA) framework. FL is a collaborative way to train neural network models across various participants or clients without centralizing their data. Each client will train a model on their respective data and send the weights across to a the server periodically for aggregation. The server aggregates these weights which are then used by the clients to re-initialize their neural network and continue the training. SA is an iterative algorithm that uses approximate sample gradients and tapering step size to locate a minimizer of a cost function. In this paper the clients use a stochastic approximation iterate to update the weights of its neural network. It is shown that the aggregated weights track an autonomous ODE. Numerical simulations are performed and the results are compared with standard algorithms like FedAvg and FedProx. It is observed that the proposed algorithm 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#36712;&#36857;&#32858;&#31867;&#21644;&#38477;&#32500;&#25216;&#26415;&#22312;&#31070;&#32463;&#32593;&#32476;&#30340;&#28508;&#31354;&#38388;&#20013;&#30740;&#31350;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#30340;&#34892;&#20026;&#27169;&#24335;&#65292;&#21487;&#20197;&#21457;&#29616;&#24182;&#25913;&#36827;&#20854;&#22810;&#26679;&#30340;&#34892;&#20026;&#27169;&#24335;&#21644;&#27425;&#20248;&#36873;&#25321;&#12290;</title><link>https://arxiv.org/abs/2402.12939</link><description>&lt;p&gt;
&#21033;&#29992;&#36712;&#36857;&#32858;&#31867;&#22312;&#28508;&#31354;&#38388;&#20013;&#21457;&#29616;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#30340;&#34892;&#20026;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Discovering Behavioral Modes in Deep Reinforcement Learning Policies Using Trajectory Clustering in Latent Space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12939
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#36712;&#36857;&#32858;&#31867;&#21644;&#38477;&#32500;&#25216;&#26415;&#22312;&#31070;&#32463;&#32593;&#32476;&#30340;&#28508;&#31354;&#38388;&#20013;&#30740;&#31350;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#30340;&#34892;&#20026;&#27169;&#24335;&#65292;&#21487;&#20197;&#21457;&#29616;&#24182;&#25913;&#36827;&#20854;&#22810;&#26679;&#30340;&#34892;&#20026;&#27169;&#24335;&#21644;&#27425;&#20248;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#20195;&#29702;&#30340;&#34892;&#20026;&#20998;&#26512;&#23545;&#20110;&#25552;&#39640;&#20854;&#24615;&#33021;&#21644;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#31574;&#30053;&#22797;&#26434;&#24615;&#24448;&#24448;&#20351;&#20854;&#38590;&#20197;&#29702;&#35299;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#30740;&#31350;DRL&#31574;&#30053;&#30340;&#34892;&#20026;&#27169;&#24335;&#65292;&#35813;&#26041;&#27861;&#28041;&#21450;&#22312;&#31070;&#32463;&#32593;&#32476;&#30340;&#28508;&#31354;&#38388;&#20013;&#21033;&#29992;&#38477;&#32500;&#21644;&#36712;&#36857;&#32858;&#31867;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20351;&#29992;Pairwise Controlled Manifold Approximation Projection&#65288;PaCMAP&#65289;&#36827;&#34892;&#38477;&#32500;&#21644;TRACLUS&#36827;&#34892;&#36712;&#36857;&#32858;&#31867;&#65292;&#20998;&#26512;&#20102;&#22312;Mountain Car&#25511;&#21046;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;DRL&#31574;&#30053;&#30340;&#28508;&#31354;&#38388;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#21161;&#20110;&#35782;&#21035;&#22810;&#26679;&#30340;&#34892;&#20026;&#27169;&#24335;&#21644;&#31574;&#30053;&#30340;&#27425;&#20248;&#36873;&#25321;&#65292;&#20174;&#32780;&#23454;&#29616;&#26377;&#38024;&#23545;&#24615;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#32467;&#21512;&#39046;&#22495;&#30693;&#35782;&#65292;&#21487;&#20197;&#22686;&#24378;&#31574;&#30053;&#22312;&#29366;&#24577;&#31354;&#38388;&#29305;&#23450;&#21306;&#22495;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12939v1 Announce Type: cross  Abstract: Understanding the behavior of deep reinforcement learning (DRL) agents is crucial for improving their performance and reliability. However, the complexity of their policies often makes them challenging to understand. In this paper, we introduce a new approach for investigating the behavior modes of DRL policies, which involves utilizing dimensionality reduction and trajectory clustering in the latent space of neural networks. Specifically, we use Pairwise Controlled Manifold Approximation Projection (PaCMAP) for dimensionality reduction and TRACLUS for trajectory clustering to analyze the latent space of a DRL policy trained on the Mountain Car control task. Our methodology helps identify diverse behavior patterns and suboptimal choices by the policy, thus allowing for targeted improvements. We demonstrate how our approach, combined with domain knowledge, can enhance a policy's performance in specific regions of the state space.
&lt;/p&gt;</description></item><item><title>GRAPHGINI&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#39318;&#27425;&#24341;&#20837;&#20102;&#22522;&#23612;&#31995;&#25968;&#20316;&#20026;&#20844;&#24179;&#24615;&#24230;&#37327;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#23454;&#29616;&#20010;&#20307;&#21644;&#32676;&#20307;&#20844;&#24179;&#24615;&#32422;&#26463;&#65292;&#24182;&#20445;&#25345;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12937</link><description>&lt;p&gt;
GRAPHGINI&#65306;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#20419;&#36827;&#20010;&#20307;&#21644;&#32676;&#20307;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
GRAPHGINI: Fostering Individual and Group Fairness in Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12937
&lt;/p&gt;
&lt;p&gt;
GRAPHGINI&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#39318;&#27425;&#24341;&#20837;&#20102;&#22522;&#23612;&#31995;&#25968;&#20316;&#20026;&#20844;&#24179;&#24615;&#24230;&#37327;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#23454;&#29616;&#20010;&#20307;&#21644;&#32676;&#20307;&#20844;&#24179;&#24615;&#32422;&#26463;&#65292;&#24182;&#20445;&#25345;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#26085;&#30410;&#22686;&#38271;&#30340;&#25285;&#24551;&#65292;&#21363;&#22312;&#32570;&#20047;&#20844;&#24179;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#65292;GNN&#21487;&#33021;&#20250;&#20135;&#29983;&#20559;&#35265;&#20915;&#31574;&#65292;&#20174;&#32780;&#19981;&#25104;&#27604;&#20363;&#22320;&#24433;&#21709;&#21040;&#24369;&#21183;&#32676;&#20307;&#25110;&#20010;&#20154;&#12290;&#19982;&#20808;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#39318;&#27425;&#24341;&#20837;&#20102;&#19968;&#31181;&#23558;&#22522;&#23612;&#31995;&#25968;&#20316;&#20026;&#20844;&#24179;&#24615;&#24230;&#37327;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;GNN&#26694;&#26550;&#20869;&#20351;&#29992;&#12290;&#25105;&#20204;&#30340;&#25552;&#35758;&#65292;GRAPHGINI&#65292;&#22312;&#21333;&#20010;&#31995;&#32479;&#20013;&#22788;&#29702;&#20010;&#20307;&#21644;&#32676;&#20307;&#20844;&#24179;&#24615;&#30340;&#20004;&#20010;&#19981;&#21516;&#30446;&#26631;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;GRAPHGINI&#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#27880;&#24847;&#21147;&#20998;&#25968;&#26469;&#23454;&#26045;&#20010;&#20307;&#20844;&#24179;&#24615;&#65292;&#36825;&#26377;&#21161;&#20110;&#36890;&#36807;&#31867;&#20284;&#33410;&#28857;&#32858;&#21512;&#26356;&#22810;&#20449;&#24687;&#12290;&#22522;&#20110;&#21551;&#21457;&#24335;&#30340;&#26368;&#22823;&#32435;&#20160;&#31038;&#20250;&#31119;&#21033;&#32422;&#26463;&#30830;&#20445;&#20102;&#26368;&#22823;&#21487;&#33021;&#30340;&#32676;&#20307;&#20844;&#24179;&#12290;&#20010;&#20307;&#20844;&#24179;&#24615;&#32422;&#26463;&#21644;&#32676;&#20307;&#20844;&#24179;&#24615;&#32422;&#26463;&#37117;&#26159;&#20197;&#21487;&#24494;&#20998;&#30340;&#22522;&#23612;&#31995;&#25968;&#30340;&#36817;&#20284;&#24418;&#24335;&#38472;&#36848;&#30340;&#12290;&#36825;&#31181;&#36817;&#20284;&#26159;&#19968;&#20010;&#36129;&#29486;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12937v1 Announce Type: new  Abstract: We address the growing apprehension that GNNs, in the absence of fairness constraints, might produce biased decisions that disproportionately affect underprivileged groups or individuals. Departing from previous work, we introduce for the first time a method for incorporating the Gini coefficient as a measure of fairness to be used within the GNN framework. Our proposal, GRAPHGINI, works with the two different goals of individual and group fairness in a single system, while maintaining high prediction accuracy. GRAPHGINI enforces individual fairness through learnable attention scores that help in aggregating more information through similar nodes. A heuristic-based maximum Nash social welfare constraint ensures the maximum possible group fairness. Both the individual fairness constraint and the group fairness constraint are stated in terms of a differentiable approximation of the Gini coefficient. This approximation is a contribution tha
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;Syflow&#26041;&#27861;&#65292;&#36890;&#36807;&#31471;&#21040;&#31471;&#26368;&#22823;&#21270;KL&#25955;&#24230;&#65292;&#21033;&#29992;&#27491;&#35268;&#21270;&#27969;&#26469;&#27169;&#25311;&#20219;&#24847;&#30446;&#26631;&#20998;&#24067;&#65292;&#24341;&#20837;&#26032;&#39062;&#30340;&#31070;&#32463;&#23618;&#65292;&#25104;&#21151;&#25214;&#21040;&#20855;&#26377;&#35265;&#22320;&#25551;&#36848;&#30340;&#39640;&#24230;&#24322;&#24120;&#23376;&#32676;&#12290;</title><link>https://arxiv.org/abs/2402.12930</link><description>&lt;p&gt;
&#36890;&#36807;&#31471;&#21040;&#31471;&#26368;&#22823;&#21270;KL&#25955;&#24230;&#23398;&#20064;&#24322;&#24120;&#23376;&#32676;
&lt;/p&gt;
&lt;p&gt;
Learning Exceptional Subgroups by End-to-End Maximizing KL-divergence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12930
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;Syflow&#26041;&#27861;&#65292;&#36890;&#36807;&#31471;&#21040;&#31471;&#26368;&#22823;&#21270;KL&#25955;&#24230;&#65292;&#21033;&#29992;&#27491;&#35268;&#21270;&#27969;&#26469;&#27169;&#25311;&#20219;&#24847;&#30446;&#26631;&#20998;&#24067;&#65292;&#24341;&#20837;&#26032;&#39062;&#30340;&#31070;&#32463;&#23618;&#65292;&#25104;&#21151;&#25214;&#21040;&#20855;&#26377;&#35265;&#22320;&#25551;&#36848;&#30340;&#39640;&#24230;&#24322;&#24120;&#23376;&#32676;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25214;&#21040;&#24182;&#25551;&#36848;&#30456;&#23545;&#20110;&#30446;&#26631;&#23646;&#24615;&#24322;&#24120;&#30340;&#23376;&#32676;&#22312;&#35768;&#22810;&#31185;&#23398;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#65292;&#20174;&#22312;&#20154;&#21475;&#26222;&#26597;&#25968;&#25454;&#20013;&#35782;&#21035;&#22788;&#20110;&#21155;&#21183;&#30340;&#20154;&#21475;&#32676;&#20307;&#21040;&#22312;&#37329;&#32435;&#31859;&#31890;&#23376;&#20013;&#25214;&#21040;&#26377;&#23548;&#30005;&#24615;&#30340;&#20998;&#23376;&#12290;&#24403;&#21069;&#25214;&#21040;&#36825;&#26679;&#30340;&#23376;&#32676;&#30340;&#26041;&#27861;&#38656;&#35201;&#39044;&#20808;&#31163;&#25955;&#21270;&#30340;&#39044;&#27979;&#21464;&#37327;&#65292;&#19981;&#20801;&#35768;&#38750;&#24179;&#20961;&#30446;&#26631;&#20998;&#24067;&#65292;&#19981;&#36866;&#29992;&#20110;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#38590;&#20197;&#25214;&#21040;&#22810;&#26679;&#24615;&#32467;&#26524;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Syflow&#65292;&#36825;&#26159;&#19968;&#31181;&#31471;&#21040;&#31471;&#21487;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#27491;&#35268;&#21270;&#27969;&#26469;&#24314;&#27169;&#20219;&#24847;&#30446;&#26631;&#20998;&#24067;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#23618;&#65292;&#21487;&#20197;&#20135;&#29983;&#26131;&#20110;&#35299;&#37322;&#30340;&#23376;&#32676;&#25551;&#36848;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#28436;&#31034;&#65292;&#21253;&#25324;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#34920;&#26126;Syflow&#21487;&#21487;&#38752;&#22320;&#25214;&#21040;&#20276;&#38543;&#30528;&#26377;&#35265;&#22320;&#30340;&#25551;&#36848;&#30340;&#39640;&#24230;&#24322;&#24120;&#23376;&#32676;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12930v1 Announce Type: new  Abstract: Finding and describing sub-populations that are exceptional regarding a target property has important applications in many scientific disciplines, from identifying disadvantaged demographic groups in census data to finding conductive molecules within gold nanoparticles. Current approaches to finding such subgroups require pre-discretized predictive variables, do not permit non-trivial target distributions, do not scale to large datasets, and struggle to find diverse results.   To address these limitations, we propose Syflow, an end-to-end optimizable approach in which we leverage normalizing flows to model arbitrary target distributions, and introduce a novel neural layer that results in easily interpretable subgroup descriptions. We demonstrate on synthetic and real-world data, including a case study, that Syflow reliably finds highly exceptional subgroups accompanied by insightful descriptions.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#20934;&#26102;&#21040;&#20301;&#65288;RioT&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#27169;&#22411;&#35299;&#37322;&#22312;&#26102;&#38388;&#21644;&#39057;&#29575;&#22495;&#20043;&#38388;&#20132;&#20114;&#65292;&#24182;&#21033;&#29992;&#21453;&#39304;&#26469;&#32422;&#26463;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#28151;&#26434;&#22240;&#32032;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.12921</link><description>&lt;p&gt;
&#20934;&#26102;&#21040;&#20301;&#65306;&#36890;&#36807;&#38480;&#21046;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30340;&#35299;&#37322;&#26469;&#20462;&#35746;&#23427;&#20204;
&lt;/p&gt;
&lt;p&gt;
Right on Time: Revising Time Series Models by Constraining their Explanations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12921
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#20934;&#26102;&#21040;&#20301;&#65288;RioT&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#27169;&#22411;&#35299;&#37322;&#22312;&#26102;&#38388;&#21644;&#39057;&#29575;&#22495;&#20043;&#38388;&#20132;&#20114;&#65292;&#24182;&#21033;&#29992;&#21453;&#39304;&#26469;&#32422;&#26463;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#28151;&#26434;&#22240;&#32032;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#32463;&#24120;&#20250;&#21463;&#21040;&#20854;&#20381;&#36182;&#28151;&#26434;&#22240;&#32032;&#30340;&#20542;&#21521;&#30340;&#25439;&#23475;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#35823;&#23548;&#24615;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#26032;&#35760;&#24405;&#30340;&#12289;&#33258;&#28982;&#28151;&#26434;&#30340;&#25968;&#25454;&#38598;P2S&#26469;&#33258;&#30495;&#23454;&#30340;&#26426;&#26800;&#29983;&#20135;&#32447;&#65292;&#24378;&#35843;&#20102;&#36825;&#19968;&#28857;&#12290;&#20026;&#20102;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#28151;&#26434;&#22240;&#32032;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20934;&#26102;&#21040;&#20301;&#65288;RioT&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#27169;&#22411;&#35299;&#37322;&#22312;&#26102;&#38388;&#21644;&#39057;&#29575;&#22495;&#20043;&#38388;&#36827;&#34892;&#20132;&#20114;&#12290;&#28982;&#21518;&#21033;&#29992;&#20004;&#20010;&#22495;&#20869;&#30340;&#35299;&#37322;&#21453;&#39304;&#26469;&#32422;&#26463;&#27169;&#22411;&#65292;&#20351;&#20854;&#36828;&#31163;&#26631;&#27880;&#30340;&#28151;&#26434;&#22240;&#32032;&#12290;&#22312;&#22788;&#29702;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#20013;&#28151;&#26434;&#22240;&#32032;&#26041;&#38754;&#65292;&#21452;&#22495;&#20132;&#20114;&#31574;&#30053;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#20973;&#32463;&#39564;&#35777;&#26126;&#65292;RioT&#33021;&#22815;&#26377;&#25928;&#22320;&#24341;&#23548;&#27169;&#22411;&#36828;&#31163;P2S&#20197;&#21450;&#27969;&#34892;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#21644;&#39044;&#27979;&#25968;&#25454;&#38598;&#20013;&#30340;&#38169;&#35823;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12921v1 Announce Type: new  Abstract: The reliability of deep time series models is often compromised by their tendency to rely on confounding factors, which may lead to misleading results. Our newly recorded, naturally confounded dataset named P2S from a real mechanical production line emphasizes this. To tackle the challenging problem of mitigating confounders in time series data, we introduce Right on Time (RioT). Our method enables interactions with model explanations across both the time and frequency domain. Feedback on explanations in both domains is then used to constrain the model, steering it away from the annotated confounding factors. The dual-domain interaction strategy is crucial for effectively addressing confounders in time series datasets. We empirically demonstrate that RioT can effectively guide models away from the wrong reasons in P2S as well as popular time series classification and forecasting datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#22914;&#20309;&#36890;&#36807;&#38598;&#25104; AutoML &#25216;&#26415;&#20248;&#21270;&#25968;&#25454;&#31649;&#36947;&#65292;&#25552;&#21319;&#25968;&#25454;&#27969;&#26234;&#33021;&#24615;&#20197;&#22312;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#21462;&#24471;&#26356;&#22909;&#32467;&#26524;&#65292;&#24182;&#25581;&#31034;&#20102;&#26500;&#24314;&#39640;&#25928;&#36866;&#24212;&#19981;&#26029;&#21464;&#21270;&#25968;&#25454;&#29615;&#22659;&#30340;&#20851;&#38190;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.12916</link><description>&lt;p&gt;
&#25968;&#25454;&#31649;&#36947;&#35757;&#32451;&#65306;&#23558; AutoML &#38598;&#25104;&#21040;&#20248;&#21270;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25968;&#25454;&#27969;&#20013;
&lt;/p&gt;
&lt;p&gt;
Data Pipeline Training: Integrating AutoML to Optimize the Data Flow of Machine Learning Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#22914;&#20309;&#36890;&#36807;&#38598;&#25104; AutoML &#25216;&#26415;&#20248;&#21270;&#25968;&#25454;&#31649;&#36947;&#65292;&#25552;&#21319;&#25968;&#25454;&#27969;&#26234;&#33021;&#24615;&#20197;&#22312;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#21462;&#24471;&#26356;&#22909;&#32467;&#26524;&#65292;&#24182;&#25581;&#31034;&#20102;&#26500;&#24314;&#39640;&#25928;&#36866;&#24212;&#19981;&#26029;&#21464;&#21270;&#25968;&#25454;&#29615;&#22659;&#30340;&#20851;&#38190;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#31649;&#36947;&#22312;&#26426;&#22120;&#23398;&#20064;&#24314;&#27169;&#21644;&#24320;&#21457;&#25968;&#25454;&#20135;&#21697;&#31561;&#20219;&#21153;&#20013;&#25198;&#28436;&#30528;&#19981;&#21487;&#25110;&#32570;&#30340;&#35282;&#33394;&#12290;&#38543;&#30528;&#25968;&#25454;&#28304;&#26085;&#30410;&#22810;&#26679;&#21270;&#21644;&#22797;&#26434;&#21270;&#65292;&#20197;&#21450;&#25968;&#25454;&#37327;&#30340;&#24555;&#36895;&#22686;&#38271;&#65292;&#26500;&#24314;&#39640;&#25928;&#30340;&#25968;&#25454;&#31649;&#36947;&#23545;&#20110;&#25552;&#39640;&#24037;&#20316;&#25928;&#29575;&#21644;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#37325;&#28857;&#25506;&#35752;&#22914;&#20309;&#36890;&#36807;&#38598;&#25104; AutoML &#21040;&#25968;&#25454;&#31649;&#36947;&#20013;&#65292;&#20248;&#21270;&#25968;&#25454;&#27969;&#21160;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#35752;&#35770;&#22914;&#20309;&#21033;&#29992; AutoML &#25216;&#26415;&#25552;&#21319;&#25968;&#25454;&#31649;&#36947;&#30340;&#26234;&#33021;&#21270;&#65292;&#20174;&#32780;&#22312;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#21462;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#36890;&#36807;&#28145;&#20837;&#30740;&#31350;&#25968;&#25454;&#27969;&#30340;&#33258;&#21160;&#21270;&#21644;&#20248;&#21270;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#26500;&#24314;&#36866;&#24212;&#19981;&#26029;&#21464;&#21270;&#30340;&#25968;&#25454;&#29615;&#22659;&#30340;&#39640;&#25928;&#25968;&#25454;&#31649;&#36947;&#30340;&#20851;&#38190;&#31574;&#30053;&#12290;&#36825;&#19981;&#20165;&#21152;&#24555;&#20102;&#24314;&#27169;&#36807;&#31243;&#65292;&#36824;&#20026;&#22797;&#26434;&#38382;&#39064;&#25552;&#20379;&#20102;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#26356;&#26174;&#33879;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12916v1 Announce Type: cross  Abstract: Data Pipeline plays an indispensable role in tasks such as modeling machine learning and developing data products. With the increasing diversification and complexity of Data sources, as well as the rapid growth of data volumes, building an efficient Data Pipeline has become crucial for improving work efficiency and solving complex problems. This paper focuses on exploring how to optimize data flow through automated machine learning methods by integrating AutoML with Data Pipeline. We will discuss how to leverage AutoML technology to enhance the intelligence of Data Pipeline, thereby achieving better results in machine learning tasks. By delving into the automation and optimization of Data flows, we uncover key strategies for constructing efficient data pipelines that can adapt to the ever-changing data landscape. This not only accelerates the modeling process but also provides innovative solutions to complex problems, enabling more sig
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35821;&#20041;&#22270;&#24179;&#28369;&#25216;&#26415;&#22686;&#24378;&#39044;&#35757;&#32451;&#27169;&#22411;&#33719;&#21462;&#30340;&#21477;&#23376;&#23884;&#20837;&#65292;&#26377;&#25928;&#25913;&#21892;&#25991;&#26412;&#32858;&#31867;&#21644;&#20998;&#31867;&#20219;&#21153;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.12890</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#20041;&#22270;&#24179;&#28369;&#23454;&#29616;&#26356;&#20855;&#36776;&#21035;&#21147;&#30340;&#21477;&#23376;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
More Discriminative Sentence Embeddings via Semantic Graph Smoothing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12890
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35821;&#20041;&#22270;&#24179;&#28369;&#25216;&#26415;&#22686;&#24378;&#39044;&#35757;&#32451;&#27169;&#22411;&#33719;&#21462;&#30340;&#21477;&#23376;&#23884;&#20837;&#65292;&#26377;&#25928;&#25913;&#21892;&#25991;&#26412;&#32858;&#31867;&#21644;&#20998;&#31867;&#20219;&#21153;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#32463;&#39564;&#26041;&#27861;&#65292;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#23398;&#20064;&#26356;&#20855;&#36776;&#21035;&#24615;&#30340;&#21477;&#23376;&#34920;&#31034;&#12290;&#21033;&#29992;&#35821;&#20041;&#22270;&#24179;&#28369;&#65292;&#25105;&#20204;&#22686;&#24378;&#20102;&#20174;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#33719;&#21462;&#30340;&#21477;&#23376;&#23884;&#20837;&#65292;&#20197;&#25552;&#39640;&#25991;&#26412;&#32858;&#31867;&#21644;&#20998;&#31867;&#20219;&#21153;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20843;&#20010;&#22522;&#20934;&#19978;&#24471;&#21040;&#39564;&#35777;&#65292;&#23637;&#31034;&#20102;&#35821;&#20041;&#22270;&#24179;&#28369;&#22312;&#25913;&#21892;&#29992;&#20110;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#25991;&#26723;&#20998;&#31867;&#20219;&#21153;&#30340;&#21477;&#23376;&#23884;&#20837;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12890v1 Announce Type: new  Abstract: This paper explores an empirical approach to learn more discriminantive sentence representations in an unsupervised fashion. Leveraging semantic graph smoothing, we enhance sentence embeddings obtained from pretrained models to improve results for the text clustering and classification tasks. Our method, validated on eight benchmarks, demonstrates consistent improvements, showcasing the potential of semantic graph smoothing in improving sentence embeddings for the supervised and unsupervised document categorization tasks.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#23545;&#20110;&#26680;&#23725;&#22238;&#24402;&#30340;&#20302;&#31209;&#36817;&#20284;&#21644;&#26367;&#20195;&#26041;&#27861;&#20013;&#65292;&#20851;&#20110;&#20302;&#32500;&#36817;&#20284;&#31209;&#30340;&#19968;&#20010;&#19979;&#30028;&#65292;&#20174;&#32780;&#20445;&#35777;&#21487;&#38752;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#23558;&#26377;&#25928;&#32500;&#24230;&#19982;&#26368;&#22823;&#32479;&#35745;&#26464;&#26438;&#24471;&#20998;&#32852;&#31995;&#36215;&#26469;&#12290;</title><link>https://arxiv.org/abs/2402.12885</link><description>&lt;p&gt;
&#23545;&#26368;&#22823;&#36793;&#38469;&#33258;&#30001;&#24230;&#30340;&#19968;&#20010;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
A Bound on the Maximal Marginal Degrees of Freedom
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12885
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#23545;&#20110;&#26680;&#23725;&#22238;&#24402;&#30340;&#20302;&#31209;&#36817;&#20284;&#21644;&#26367;&#20195;&#26041;&#27861;&#20013;&#65292;&#20851;&#20110;&#20302;&#32500;&#36817;&#20284;&#31209;&#30340;&#19968;&#20010;&#19979;&#30028;&#65292;&#20174;&#32780;&#20445;&#35777;&#21487;&#38752;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#23558;&#26377;&#25928;&#32500;&#24230;&#19982;&#26368;&#22823;&#32479;&#35745;&#26464;&#26438;&#24471;&#20998;&#32852;&#31995;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12885v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449;&#25688;&#35201;: &#36890;&#29992;&#26680;&#23725;&#22238;&#24402;&#22312;&#20869;&#23384;&#20998;&#37197;&#21644;&#35745;&#31639;&#26102;&#38388;&#19978;&#25104;&#26412;&#39640;&#26114;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#26680;&#23725;&#22238;&#24402;&#30340;&#20302;&#31209;&#36817;&#20284;&#21644;&#26367;&#20195;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#36825;&#20123;&#22256;&#38590;&#12290;&#26412;&#25991;&#30340;&#22522;&#26412;&#36129;&#29486;&#22312;&#20110;&#23545;&#20302;&#32500;&#36817;&#20284;&#30340;&#31209;&#25552;&#20986;&#20102;&#19968;&#20010;&#19979;&#30028;&#65292;&#35201;&#27714;&#20854;&#20445;&#25345;&#21487;&#38752;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#35813;&#30028;&#38480;&#23558;&#26377;&#25928;&#32500;&#24230;&#19982;&#26368;&#22823;&#32479;&#35745;&#26464;&#26438;&#24471;&#20998;&#32852;&#31995;&#36215;&#26469;&#12290;&#25105;&#20204;&#36890;&#36807;&#28041;&#21450;&#26680;&#30340;&#27491;&#21017;&#24615;&#26469;&#34920;&#24449;&#26377;&#25928;&#32500;&#24230;&#21450;&#20854;&#38543;&#27491;&#21017;&#21270;&#21442;&#25968;&#30340;&#22686;&#38271;&#34892;&#20026;&#12290;&#23545;&#20110;&#36866;&#24403;&#36873;&#25321;&#30340;&#26680;&#65292;&#36825;&#31181;&#22686;&#38271;&#34987;&#35777;&#26126;&#26159;&#23545;&#25968;&#28176;&#36817;&#30340;&#65292;&#20174;&#32780;&#35777;&#26126;&#20102;&#20302;&#31209;&#36817;&#20284;&#20316;&#20026;Nystro&#776;m&#26041;&#27861;&#30340;&#21512;&#29702;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12885v1 Announce Type: cross  Abstract: Common kernel ridge regression is expensive in memory allocation and computation time. This paper addresses low rank approximations and surrogates for kernel ridge regression, which bridge these difficulties. The fundamental contribution of the paper is a lower bound on the rank of the low dimensional approximation, which is required such that the prediction power remains reliable. The bound relates the effective dimension with the largest statistical leverage score. We characterize the effective dimension and its growth behavior with respect to the regularization parameter by involving the regularity of the kernel. This growth is demonstrated to be asymptotically logarithmic for suitably chosen kernels, justifying low-rank approximations as the Nystr\"om method.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;FMTL-Bench&#26694;&#26550;&#65292;&#29992;&#20110;&#31995;&#32479;&#35780;&#20272;&#32852;&#37030;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;FMTL&#65289;&#33539;&#24335;&#65292;&#22635;&#34917;&#20102;FL&#21644;MTL&#32508;&#21512;&#35780;&#20272;&#26041;&#27861;&#30340;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2402.12876</link><description>&lt;p&gt;
&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#23396;&#23707;&#19978;&#30340;&#32852;&#37030;&#22810;&#20219;&#21153;&#23398;&#20064;&#65306;&#23454;&#39564;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Federated Multi-Task Learning on Non-IID Data Silos: An Experimental Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12876
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;FMTL-Bench&#26694;&#26550;&#65292;&#29992;&#20110;&#31995;&#32479;&#35780;&#20272;&#32852;&#37030;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;FMTL&#65289;&#33539;&#24335;&#65292;&#22635;&#34917;&#20102;FL&#21644;MTL&#32508;&#21512;&#35780;&#20272;&#26041;&#27861;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31181;&#21019;&#26032;&#30340;&#32852;&#37030;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;FMTL&#65289;&#26041;&#27861;&#25972;&#21512;&#20102;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#30340;&#20248;&#28857;&#65292;&#33021;&#22815;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#21327;&#20316;&#27169;&#22411;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#22312;&#35813;&#39046;&#22495;&#32570;&#20047;&#25972;&#21512;FL&#21644;MTL&#29420;&#29305;&#29305;&#24615;&#30340;&#32508;&#21512;&#35780;&#20272;&#26041;&#27861;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;FMTL-Bench&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#65292;&#29992;&#20110;&#31995;&#32479;&#35780;&#20272;FMTL&#33539;&#24335;&#12290;&#36825;&#20010;&#22522;&#20934;&#28085;&#30422;&#20102;&#25968;&#25454;&#12289;&#27169;&#22411;&#21644;&#20248;&#21270;&#31639;&#27861;&#32423;&#21035;&#30340;&#21508;&#20010;&#26041;&#38754;&#65292;&#24182;&#21253;&#25324;&#19971;&#32452;&#27604;&#36739;&#23454;&#39564;&#65292;&#23553;&#35013;&#20102;&#24191;&#27867;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;Non-IID&#65289;&#25968;&#25454;&#20998;&#21306;&#22330;&#26223;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#27604;&#21508;&#31181;&#25351;&#26631;&#30340;&#22522;&#32447;&#30340;&#31995;&#32479;&#36807;&#31243;&#65292;&#24182;&#23545;&#36890;&#20449;&#24320;&#25903;&#12289;&#26102;&#38388;&#21644;&#33021;&#28304;&#28040;&#32791;&#36827;&#34892;&#20102;&#26696;&#20363;&#30740;&#31350;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#26088;&#22312;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12876v1 Announce Type: new  Abstract: The innovative Federated Multi-Task Learning (FMTL) approach consolidates the benefits of Federated Learning (FL) and Multi-Task Learning (MTL), enabling collaborative model training on multi-task learning datasets. However, a comprehensive evaluation method, integrating the unique features of both FL and MTL, is currently absent in the field. This paper fills this void by introducing a novel framework, FMTL-Bench, for systematic evaluation of the FMTL paradigm. This benchmark covers various aspects at the data, model, and optimization algorithm levels, and comprises seven sets of comparative experiments, encapsulating a wide array of non-independent and identically distributed (Non-IID) data partitioning scenarios. We propose a systematic process for comparing baselines of diverse indicators and conduct a case study on communication expenditure, time, and energy consumption. Through our exhaustive experiments, we aim to provide valuable
&lt;/p&gt;</description></item><item><title>&#24605;&#32500;&#38142;&#36171;&#20104;&#21464;&#21387;&#22120;&#27169;&#22411;&#25191;&#34892;&#22266;&#26377;&#20018;&#34892;&#35745;&#31639;&#30340;&#33021;&#21147;&#65292;&#25552;&#39640;&#20102;&#21464;&#21387;&#22120;&#22312;&#31639;&#26415;&#21644;&#31526;&#21495;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12875</link><description>&lt;p&gt;
&#24605;&#32500;&#38142;&#28608;&#21457;&#21464;&#21387;&#22120;&#35299;&#20915;&#22266;&#26377;&#20018;&#34892;&#38382;&#39064;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Chain of Thought Empowers Transformers to Solve Inherently Serial Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12875
&lt;/p&gt;
&lt;p&gt;
&#24605;&#32500;&#38142;&#36171;&#20104;&#21464;&#21387;&#22120;&#27169;&#22411;&#25191;&#34892;&#22266;&#26377;&#20018;&#34892;&#35745;&#31639;&#30340;&#33021;&#21147;&#65292;&#25552;&#39640;&#20102;&#21464;&#21387;&#22120;&#22312;&#31639;&#26415;&#21644;&#31526;&#21495;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#23548;&#27169;&#22411;&#29983;&#25104;&#19968;&#31995;&#21015;&#20013;&#38388;&#27493;&#39588;&#65292;&#21363;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#65292;&#26159;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#31639;&#26415;&#21644;&#31526;&#21495;&#25512;&#29702;&#20219;&#21153;&#19978;&#20934;&#30830;&#24615;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;CoT&#32972;&#21518;&#30340;&#26426;&#21046;&#20173;&#19981;&#28165;&#26970;&#12290;&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#34920;&#36798;&#24615;&#30340;&#35270;&#35282;&#25552;&#20379;&#20102;&#23545;&#35299;&#30721;&#22120;&#19987;&#29992;&#21464;&#21387;&#22120;&#30340;CoT&#33021;&#21147;&#30340;&#29702;&#35770;&#29702;&#35299;&#12290;&#22312;&#27010;&#24565;&#19978;&#65292;CoT&#36171;&#20104;&#27169;&#22411;&#25191;&#34892;&#22266;&#26377;&#20018;&#34892;&#35745;&#31639;&#30340;&#33021;&#21147;&#65292;&#32780;&#36825;&#31181;&#33021;&#21147;&#22312;&#21464;&#21387;&#22120;&#20013;&#32570;&#20047;&#65292;&#29305;&#21035;&#26159;&#24403;&#28145;&#24230;&#36739;&#20302;&#26102;&#12290;&#20808;&#21069;&#30340;&#20316;&#21697;&#24050;&#32463;&#34920;&#26126;&#65292;&#22312;&#27809;&#26377;CoT&#30340;&#24773;&#20917;&#19979;&#65292;&#20855;&#26377;&#26377;&#38480;&#31934;&#24230;$\mathsf{poly}(n)$&#23884;&#20837;&#23610;&#23544;&#30340;&#24658;&#23450;&#28145;&#24230;&#21464;&#21387;&#22120;&#21482;&#33021;&#22312;$\mathsf{TC}^0$&#20013;&#35299;&#20915;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#20855;&#26377;&#24120;&#25968;&#20301;&#31934;&#24230;&#30340;&#24658;&#23450;&#28145;&#24230;&#21464;&#21387;&#22120;&#30340;&#26356;&#32039;&#23494;&#30340;&#34920;&#36798;&#24615;&#19978;&#30028;&#65292;&#23427;&#21482;&#33021;&#35299;&#20915;$\mathsf{AC}^0$&#20013;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12875v1 Announce Type: new  Abstract: Instructing the model to generate a sequence of intermediate steps, a.k.a., a chain of thought (CoT), is a highly effective method to improve the accuracy of large language models (LLMs) on arithmetics and symbolic reasoning tasks. However, the mechanism behind CoT remains unclear. This work provides a theoretical understanding of the power of CoT for decoder-only transformers through the lens of expressiveness. Conceptually, CoT empowers the model with the ability to perform inherently serial computation, which is otherwise lacking in transformers, especially when depth is low. Given input length $n$, previous works have shown that constant-depth transformers with finite precision $\mathsf{poly}(n)$ embedding size can only solve problems in $\mathsf{TC}^0$ without CoT. We first show an even tighter expressiveness upper bound for constant-depth transformers with constant-bit precision, which can only solve problems in $\mathsf{AC}^0$, a 
&lt;/p&gt;</description></item><item><title>&#20248;&#21183;&#20989;&#25968;&#30340;&#21019;&#26032;&#22312;&#20110;&#23558;&#22238;&#25253;&#20998;&#35299;&#20026;&#20195;&#29702;&#21160;&#20316;&#24341;&#36215;&#30340;&#37096;&#20998;&#65288;&#25216;&#33021;&#65289;&#21644;&#20195;&#29702;&#26080;&#27861;&#25511;&#21046;&#30340;&#37096;&#20998;&#65288;&#36816;&#27668;&#65289;&#65292;&#36827;&#32780;&#25193;&#23637;&#20102;&#30452;&#25509;&#20248;&#21183;&#20272;&#35745;&#21040;&#31163;&#32447;&#29615;&#22659;&#65292;&#20351;&#24471;&#20174;&#31163;&#32447;&#36712;&#36857;&#23398;&#20064;&#26356;&#21152;&#39640;&#25928;&#12290;</title><link>https://arxiv.org/abs/2402.12874</link><description>&lt;p&gt;
&#25216;&#33021;&#36824;&#26159;&#36816;&#27668;&#65311;&#36890;&#36807;&#20248;&#21183;&#20989;&#25968;&#23545;&#22238;&#25253;&#36827;&#34892;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Skill or Luck? Return Decomposition via Advantage Functions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12874
&lt;/p&gt;
&lt;p&gt;
&#20248;&#21183;&#20989;&#25968;&#30340;&#21019;&#26032;&#22312;&#20110;&#23558;&#22238;&#25253;&#20998;&#35299;&#20026;&#20195;&#29702;&#21160;&#20316;&#24341;&#36215;&#30340;&#37096;&#20998;&#65288;&#25216;&#33021;&#65289;&#21644;&#20195;&#29702;&#26080;&#27861;&#25511;&#21046;&#30340;&#37096;&#20998;&#65288;&#36816;&#27668;&#65289;&#65292;&#36827;&#32780;&#25193;&#23637;&#20102;&#30452;&#25509;&#20248;&#21183;&#20272;&#35745;&#21040;&#31163;&#32447;&#29615;&#22659;&#65292;&#20351;&#24471;&#20174;&#31163;&#32447;&#36712;&#36857;&#23398;&#20064;&#26356;&#21152;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#26469;&#33258;&#31163;&#32447;&#25968;&#25454;&#23545;&#20110;&#39640;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;&#36825;&#26679;&#30340;&#27934;&#23519;&#21147;&#65292;&#21363;&#20248;&#21183;&#20989;&#25968;&#21487;&#20197;&#34987;&#29702;&#35299;&#20026;&#21160;&#20316;&#23545;&#22238;&#25253;&#30340;&#22240;&#26524;&#24433;&#21709;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#36712;&#36857;&#30340;&#22238;&#25253;&#20998;&#35299;&#20026;&#30001;&#20195;&#29702;&#30340;&#21160;&#20316;&#65288;&#25216;&#33021;&#65289;&#24341;&#36215;&#30340;&#37096;&#20998;&#21644;&#20195;&#29702;&#26080;&#27861;&#25511;&#21046;&#30340;&#37096;&#20998;&#65288;&#36816;&#27668;&#65289;&#12290;&#27492;&#22806;&#65292;&#36825;&#31181;&#20998;&#35299;&#20351;&#25105;&#20204;&#33021;&#22815;&#33258;&#28982;&#22320;&#23558;&#30452;&#25509;&#20248;&#21183;&#20272;&#35745;&#65288;DAE&#65289;&#25193;&#23637;&#21040;&#31163;&#32447;&#35774;&#32622;&#65288;&#31163;&#32447;DAE&#65289;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#26041;&#27861;&#21487;&#20197;&#20174;&#31163;&#32447;&#36712;&#36857;&#20013;&#23398;&#20064;&#65292;&#32780;&#26080;&#38656;&#20381;&#36182;&#37325;&#35201;&#24615;&#37319;&#26679;&#25216;&#26415;&#25110;&#25130;&#26029;&#31163;&#32447;&#21160;&#20316;&#12290;&#25105;&#20204;&#24314;&#31435;&#31163;&#32447;DAE&#19982;&#20808;&#21069;&#26041;&#27861;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#20197;&#23637;&#31034;&#23427;&#22914;&#20309;&#21152;&#36895;&#23398;&#20064;&#20197;&#21450;&#24403;&#25152;&#25552;&#20986;&#30340;&#31163;&#32447;&#26657;&#27491;&#20309;&#26102;&#37325;&#35201;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;MinAtar&#29615;&#22659;&#26469;&#35828;&#26126;&#24573;&#30053;&#31163;&#32447;&#26657;&#27491;&#21487;&#33021;&#23548;&#33268;&#23376;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12874v1 Announce Type: new  Abstract: Learning from off-policy data is essential for sample-efficient reinforcement learning. In the present work, we build on the insight that the advantage function can be understood as the causal effect of an action on the return, and show that this allows us to decompose the return of a trajectory into parts caused by the agent's actions (skill) and parts outside of the agent's control (luck). Furthermore, this decomposition enables us to naturally extend Direct Advantage Estimation (DAE) to off-policy settings (Off-policy DAE). The resulting method can learn from off-policy trajectories without relying on importance sampling techniques or truncating off-policy actions. We draw connections between Off-policy DAE and previous methods to demonstrate how it can speed up learning and when the proposed off-policy corrections are important. Finally, we use the MinAtar environments to illustrate how ignoring off-policy corrections can lead to sub
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21487;&#34892;&#38598;&#30340;&#26354;&#29575;&#65292;&#22312;&#22312;&#32447;&#20984;&#20248;&#21270;&#20013;&#23454;&#29616;&#20102;&#24555;&#36895;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.12868</link><description>&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#21487;&#34892;&#38598;&#30340;&#26354;&#29575;&#65292;&#22312;&#22312;&#32447;&#20984;&#20248;&#21270;&#20013;&#23454;&#29616;&#24555;&#36895;&#25910;&#25947;&#36895;&#24230;
&lt;/p&gt;
&lt;p&gt;
Fast Rates in Online Convex Optimization by Exploiting the Curvature of Feasible Sets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12868
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21487;&#34892;&#38598;&#30340;&#26354;&#29575;&#65292;&#22312;&#22312;&#32447;&#20984;&#20248;&#21270;&#20013;&#23454;&#29616;&#20102;&#24555;&#36895;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#32447;&#20984;&#20248;&#21270;&#65288;OCO&#65289;&#65292;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#21487;&#34892;&#38598;&#30340;&#26354;&#29575;&#25552;&#20379;&#24555;&#36895;&#25910;&#25947;&#36895;&#24230;&#30340;&#26032;&#20998;&#26512;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#65292;&#22914;&#26524;&#26368;&#20248;&#20915;&#31574;&#20301;&#20110;&#21487;&#34892;&#38598;&#30340;&#36793;&#30028;&#19978;&#19988;&#22522;&#30784;&#25439;&#22833;&#20989;&#25968;&#30340;&#26799;&#24230;&#38750;&#38646;&#65292;&#21017;&#31639;&#27861;&#22312;&#38543;&#26426;&#29615;&#22659;&#20013;&#21487;&#20197;&#36798;&#21040;$O(\rho \log T)$&#30340;&#36951;&#25022;&#19978;&#30028;&#12290;&#20854;&#20013;&#65292;$\rho &gt; 0$&#26159;&#21253;&#21547;&#26368;&#20248;&#20915;&#31574;&#24182;&#22260;&#32469;&#21487;&#34892;&#38598;&#30340;&#26368;&#23567;&#29699;&#20307;&#30340;&#21322;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12868v1 Announce Type: new  Abstract: In this paper, we explore online convex optimization (OCO) and introduce a new analysis that provides fast rates by exploiting the curvature of feasible sets. In online linear optimization, it is known that if the average gradient of loss functions is larger than a certain value, the curvature of feasible sets can be exploited by the follow-the-leader (FTL) algorithm to achieve a logarithmic regret. This paper reveals that algorithms adaptive to the curvature of loss functions can also leverage the curvature of feasible sets. We first prove that if an optimal decision is on the boundary of a feasible set and the gradient of an underlying loss function is non-zero, then the algorithm achieves a regret upper bound of $O(\rho \log T)$ in stochastic environments. Here, $\rho &gt; 0$ is the radius of the smallest sphere that includes the optimal decision and encloses the feasible set. Our approach, unlike existing ones, can work directly with co
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;DevOps&#24037;&#20855;&#25512;&#33616;&#31995;&#32479;&#65292;&#26088;&#22312;&#24314;&#31435;&#19968;&#20010;&#21487;&#20197;&#33258;&#21160;&#26500;&#24314;&#25968;&#25454;&#38598;&#12289;&#35757;&#32451;&#27169;&#22411;&#12289;&#37096;&#32626;&#27169;&#22411;&#20197;&#21450;&#23384;&#20648;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#30340;&#27969;&#27700;&#32447;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#20132;&#20184;&#30340;&#36895;&#24230;&#21644;&#32467;&#26524;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12867</link><description>&lt;p&gt;
&#26397;&#30528;MLOps&#30340;&#26041;&#21521;&#65306;&#38754;&#21521;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;DevOps&#24037;&#20855;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Towards MLOps: A DevOps Tools Recommender System for Machine Learning System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12867
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;DevOps&#24037;&#20855;&#25512;&#33616;&#31995;&#32479;&#65292;&#26088;&#22312;&#24314;&#31435;&#19968;&#20010;&#21487;&#20197;&#33258;&#21160;&#26500;&#24314;&#25968;&#25454;&#38598;&#12289;&#35757;&#32451;&#27169;&#22411;&#12289;&#37096;&#32626;&#27169;&#22411;&#20197;&#21450;&#23384;&#20648;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#30340;&#27969;&#27700;&#32447;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#20132;&#20184;&#30340;&#36895;&#24230;&#21644;&#32467;&#26524;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;DevOps&#23454;&#36341;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#34987;&#31216;&#20026;MLOps&#65292;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20250;&#38543;&#30528;&#26032;&#25968;&#25454;&#30340;&#20986;&#29616;&#32780;&#19981;&#26029;&#21457;&#23637;&#65292;&#19982;&#20256;&#32479;&#31995;&#32479;&#22522;&#20110;&#38656;&#27714;&#30340;&#26041;&#24335;&#19981;&#21516;&#12290;MLOps&#30340;&#30446;&#26631;&#26159;&#24314;&#31435;&#19981;&#21516;&#24320;&#28304;&#24037;&#20855;&#20043;&#38388;&#30340;&#36830;&#25509;&#65292;&#26500;&#24314;&#19968;&#20010;&#21487;&#20197;&#33258;&#21160;&#25191;&#34892;&#26500;&#24314;&#25968;&#25454;&#38598;&#12289;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12289;&#37096;&#32626;&#27169;&#22411;&#21040;&#29983;&#20135;&#29615;&#22659;&#20197;&#21450;&#23384;&#20648;&#19981;&#21516;&#29256;&#26412;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#27493;&#39588;&#30340;&#27969;&#27700;&#32447;&#12290;MLOps&#30340;&#22909;&#22788;&#22312;&#20110;&#30830;&#20445;&#23558;&#26032;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#24555;&#36895;&#20132;&#20184;&#21040;&#29983;&#20135;&#29615;&#22659;&#65292;&#20197;&#33719;&#24471;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;MLOps&#23454;&#36341;&#20250;&#24433;&#21709;&#36719;&#20214;&#20135;&#21697;&#30340;&#25972;&#20307;&#36136;&#37327;&#65292;&#24182;&#19988;&#23436;&#20840;&#20381;&#36182;&#20110;&#24320;&#28304;&#24037;&#20855;&#65292;&#36873;&#25321;&#30456;&#20851;&#24320;&#28304;&#24037;&#20855;&#34987;&#35270;&#20026;&#19968;&#39033;&#25361;&#25112;&#65292;&#22240;&#27492;&#26399;&#26395;&#26377;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#26469;&#36873;&#25321;&#21512;&#36866;&#30340;&#24320;&#28304;&#24037;&#20855;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25512;&#33616;&#31995;&#32479;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12867v1 Announce Type: cross  Abstract: Applying DevOps practices to machine learning system is termed as MLOps and machine learning systems evolve on new data unlike traditional systems on requirements. The objective of MLOps is to establish a connection between different open-source tools to construct a pipeline that can automatically perform steps to construct a dataset, train the machine learning model and deploy the model to the production as well as store different versions of model and dataset. Benefits of MLOps is to make sure the fast delivery of the new trained models to the production to have accurate results. Furthermore, MLOps practice impacts the overall quality of the software products and is completely dependent on open-source tools and selection of relevant open-source tools is considered as challenged while a generalized method to select an appropriate open-source tools is desirable. In this paper, we present a framework for recommendation system that proce
&lt;/p&gt;</description></item><item><title>&#23558;&#35821;&#35328;&#27169;&#22411;&#26799;&#24230;&#25237;&#24433;&#21040;&#35789;&#27719;&#31354;&#38388;&#20013;&#65292;&#25366;&#25496;&#20449;&#24687;&#22312;LMs&#20869;&#37096;&#30340;&#27969;&#21160;&#26041;&#24335;&#65292;&#25506;&#32034;&#26032;&#20449;&#24687;&#22914;&#20309;&#23384;&#20648;&#22312;LMs&#30340;&#31070;&#32463;&#20803;&#20013;&#12290;</title><link>https://arxiv.org/abs/2402.12865</link><description>&lt;p&gt;
&#21453;&#21521;&#38236;&#22836;&#65306;&#23558;&#35821;&#35328;&#27169;&#22411;&#26799;&#24230;&#25237;&#24433;&#21040;&#35789;&#27719;&#31354;&#38388;&#20013;
&lt;/p&gt;
&lt;p&gt;
Backward Lens: Projecting Language Model Gradients into the Vocabulary Space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12865
&lt;/p&gt;
&lt;p&gt;
&#23558;&#35821;&#35328;&#27169;&#22411;&#26799;&#24230;&#25237;&#24433;&#21040;&#35789;&#27719;&#31354;&#38388;&#20013;&#65292;&#25366;&#25496;&#20449;&#24687;&#22312;LMs&#20869;&#37096;&#30340;&#27969;&#21160;&#26041;&#24335;&#65292;&#25506;&#32034;&#26032;&#20449;&#24687;&#22914;&#20309;&#23384;&#20648;&#22312;LMs&#30340;&#31070;&#32463;&#20803;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20102;&#35299;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;(LMs)&#22914;&#20309;&#23398;&#20064;&#21644;&#35760;&#24518;&#20449;&#24687;&#26159;&#28145;&#24230;&#23398;&#20064;&#31038;&#21306;&#30340;&#19968;&#20010;&#37325;&#35201;&#30446;&#26631;&#12290;&#26368;&#36817;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#23558;&#20174;&#21069;&#21521;&#20256;&#25773;&#20013;&#33719;&#24471;&#30340;&#26435;&#37325;&#21644;&#38544;&#34255;&#29366;&#24577;&#25237;&#24433;&#21040;&#27169;&#22411;&#30340;&#35789;&#27719;&#34920;&#20013;&#65292;&#26377;&#21161;&#20110;&#25581;&#31034;LMs&#20869;&#37096;&#20449;&#24687;&#27969;&#21160;&#30340;&#26041;&#24335;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#25193;&#23637;&#21040;LMs&#30340;&#21518;&#21521;&#20256;&#25773;&#21644;&#26799;&#24230;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#26799;&#24230;&#30697;&#38453;&#21487;&#20197;&#34987;&#34920;&#31034;&#20026;&#20854;&#21069;&#21521;&#21644;&#21518;&#21521;&#20256;&#25773;&#36755;&#20837;&#30340;&#20302;&#31209;&#32447;&#24615;&#32452;&#21512;&#12290;&#28982;&#21518;&#25105;&#20204;&#24320;&#21457;&#26041;&#27861;&#23558;&#36825;&#20123;&#26799;&#24230;&#25237;&#24433;&#21040;&#35789;&#27719;&#39033;&#20013;&#65292;&#24182;&#25506;&#35752;&#26032;&#20449;&#24687;&#22914;&#20309;&#23384;&#20648;&#22312;LMs&#30340;&#31070;&#32463;&#20803;&#20013;&#30340;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12865v1 Announce Type: cross  Abstract: Understanding how Transformer-based Language Models (LMs) learn and recall information is a key goal of the deep learning community. Recent interpretability methods project weights and hidden states obtained from the forward pass to the models' vocabularies, helping to uncover how information flows within LMs. In this work, we extend this methodology to LMs' backward pass and gradients. We first prove that a gradient matrix can be cast as a low-rank linear combination of its forward and backward passes' inputs. We then develop methods to project these gradients into vocabulary items and explore the mechanics of how new information is stored in the LMs' neurons.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#29616;&#23454;&#23545;&#25239;&#35774;&#32622;&#19979;&#37325;&#24314;&#25104;&#21151;&#29575;&#30340;&#27491;&#24335;&#19978;&#38480;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#32467;&#26524;&#25903;&#25345;&#65292;&#26377;&#21161;&#20110;&#26356;&#26126;&#26234;&#22320;&#36873;&#25321;&#38544;&#31169;&#21442;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.12861</link><description>&lt;p&gt;
&#22312;&#27809;&#26377;&#25968;&#25454;&#20808;&#39564;&#26465;&#20214;&#19979;&#38480;&#21046;&#23545;&#25239;&#32773;&#37325;&#24314;&#25915;&#20987;&#25104;&#21151;&#29575;
&lt;/p&gt;
&lt;p&gt;
Bounding Reconstruction Attack Success of Adversaries Without Data Priors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12861
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#29616;&#23454;&#23545;&#25239;&#35774;&#32622;&#19979;&#37325;&#24314;&#25104;&#21151;&#29575;&#30340;&#27491;&#24335;&#19978;&#38480;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#32467;&#26524;&#25903;&#25345;&#65292;&#26377;&#21161;&#20110;&#26356;&#26126;&#26234;&#22320;&#36873;&#25321;&#38544;&#31169;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#37325;&#24314;&#25915;&#20987;&#23384;&#22312;&#27844;&#28431;&#25935;&#24863;&#25968;&#25454;&#30340;&#39118;&#38505;&#12290;&#22312;&#29305;&#23450;&#24773;&#22659;&#19979;&#65292;&#23545;&#25163;&#21487;&#20197;&#20351;&#29992;&#27169;&#22411;&#30340;&#26799;&#24230;&#20960;&#20046;&#23436;&#32654;&#22320;&#37325;&#24314;&#35757;&#32451;&#25968;&#25454;&#26679;&#26412;&#12290;&#22312;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26102;&#65292;&#21487;&#20197;&#25552;&#20379;&#23545;&#36825;&#31181;&#37325;&#24314;&#25915;&#20987;&#25104;&#21151;&#29575;&#30340;&#27491;&#24335;&#19978;&#38480;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#36825;&#20123;&#19978;&#38480;&#26159;&#22312;&#21487;&#33021;&#19981;&#31526;&#21512;&#39640;&#24230;&#29616;&#23454;&#23454;&#29992;&#24615;&#30340;&#26368;&#22351;&#24773;&#20917;&#20551;&#35774;&#19979;&#21046;&#23450;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#38024;&#23545;&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#20102;&#22312;&#29616;&#23454;&#23545;&#25239;&#35774;&#32622;&#19979;&#30340;&#37325;&#24314;&#25104;&#21151;&#29575;&#27491;&#24335;&#19978;&#38480;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#32467;&#26524;&#25903;&#25345;&#36825;&#20123;&#19978;&#38480;&#12290;&#36890;&#36807;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#29616;&#23454;&#24773;&#22659;&#20013;&#65292;&#65288;a&#65289;&#39044;&#26399;&#30340;&#37325;&#24314;&#25104;&#21151;&#29575;&#21487;&#20197;&#22312;&#19981;&#21516;&#32972;&#26223;&#21644;&#19981;&#21516;&#24230;&#37327;&#19979;&#24471;&#21040;&#36866;&#24403;&#30340;&#38480;&#21046;&#65292;&#36825;&#65288;b&#65289;&#26377;&#21161;&#20110;&#26356;&#26126;&#26234;&#22320;&#36873;&#25321;&#38544;&#31169;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12861v1 Announce Type: new  Abstract: Reconstruction attacks on machine learning (ML) models pose a strong risk of leakage of sensitive data. In specific contexts, an adversary can (almost) perfectly reconstruct training data samples from a trained model using the model's gradients. When training ML models with differential privacy (DP), formal upper bounds on the success of such reconstruction attacks can be provided. So far, these bounds have been formulated under worst-case assumptions that might not hold high realistic practicality. In this work, we provide formal upper bounds on reconstruction success under realistic adversarial settings against ML models trained with DP and support these bounds with empirical results. With this, we show that in realistic scenarios, (a) the expected reconstruction success can be bounded appropriately in different contexts and by different metrics, which (b) allows for a more educated choice of a privacy parameter.
&lt;/p&gt;</description></item><item><title>&#21487;&#24494;&#20998;&#26144;&#23556;&#22120;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#25968;&#25454;&#34920;&#31034;&#30340;&#25299;&#25169;&#32467;&#26500;&#65292;&#35299;&#20915;&#20102;&#25163;&#21160;&#35843;&#33410;Mapper&#22270;&#20013;&#35768;&#22810;&#21442;&#25968;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#35843;&#33410;&#20851;&#38190;&#30340;&#28388;&#27874;&#22120;&#21442;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.12854</link><description>&lt;p&gt;
&#21487;&#24494;&#20998;&#26144;&#23556;&#22120;&#29992;&#20110;&#25968;&#25454;&#34920;&#31034;&#30340;&#25299;&#25169;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Differentiable Mapper For Topological Optimization Of Data Representation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12854
&lt;/p&gt;
&lt;p&gt;
&#21487;&#24494;&#20998;&#26144;&#23556;&#22120;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#25968;&#25454;&#34920;&#31034;&#30340;&#25299;&#25169;&#32467;&#26500;&#65292;&#35299;&#20915;&#20102;&#25163;&#21160;&#35843;&#33410;Mapper&#22270;&#20013;&#35768;&#22810;&#21442;&#25968;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#35843;&#33410;&#20851;&#38190;&#30340;&#28388;&#27874;&#22120;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#25968;&#25454;&#34920;&#31034;&#21644;&#21487;&#35270;&#21270;&#21033;&#29992;&#25299;&#25169;&#24037;&#20855;&#26159;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#65288;TDA&#65289;&#21644;&#25968;&#25454;&#31185;&#23398;&#30340;&#19968;&#20010;&#31215;&#26497;&#19988;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#12290;&#20854;&#20013;&#26368;&#31361;&#20986;&#30340;&#24037;&#20316;&#32447;&#26159;&#22522;&#20110;&#25152;&#35859;&#30340;Mapper&#22270;&#65292;&#36825;&#26159;&#19968;&#20010;&#32452;&#21512;&#22270;&#65292;&#20854;&#25299;&#25169;&#32467;&#26500;&#65288;&#36830;&#36890;&#32452;&#20214;&#65292;&#20998;&#25903;&#65292;&#29615;&#65289;&#19982;&#25968;&#25454;&#26412;&#36523;&#30340;&#25299;&#25169;&#32467;&#26500;&#30456;&#23545;&#24212;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#39640;&#24230;&#36890;&#29992;&#21644;&#36866;&#29992;&#65292;&#20854;&#20351;&#29992;&#21040;&#30446;&#21069;&#20026;&#27490;&#19968;&#30452;&#21463;&#21040;&#20854;&#35768;&#22810;&#21442;&#25968;&#30340;&#25163;&#21160;&#35843;&#33410;&#30340;&#38459;&#30861;-&#20854;&#20013;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#25152;&#35859;&#30340;&#28388;&#27874;&#22120;:&#23427;&#26159;&#19968;&#20010;&#36830;&#32493;&#20989;&#25968;&#65292;&#20854;&#22312;&#25968;&#25454;&#38598;&#19978;&#30340;&#21464;&#21270;&#26159;&#26500;&#24314;Mapper&#34920;&#31034;&#21644;&#35780;&#20272;&#20854;&#25299;&#25169;&#32467;&#26500;&#30340;&#23384;&#22312;&#21644;&#22823;&#23567;&#30340;&#20027;&#35201;&#25104;&#20998;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#23545;&#20110;&#20854;&#20182;Mapper&#21442;&#25968;&#65288;&#21363;&#65292;&#20998;&#36776;&#29575;&#65292;&#22686;&#30410;&#65292;&#32858;&#31867;&#65289;&#24050;&#32463;&#30740;&#31350;&#20102;&#19968;&#20123;&#21442;&#25968;&#35843;&#25972;&#26041;&#27861;&#65292;&#20294;&#30446;&#21069;&#36824;&#27809;&#26377;&#38024;&#23545;&#35843;&#25972;&#28388;&#27874;&#22120;&#26412;&#36523;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12854v1 Announce Type: new  Abstract: Unsupervised data representation and visualization using tools from topology is an active and growing field of Topological Data Analysis (TDA) and data science. Its most prominent line of work is based on the so-called Mapper graph, which is a combinatorial graph whose topological structures (connected components, branches, loops) are in correspondence with those of the data itself. While highly generic and applicable, its use has been hampered so far by the manual tuning of its many parameters-among these, a crucial one is the so-called filter: it is a continuous function whose variations on the data set are the main ingredient for both building the Mapper representation and assessing the presence and sizes of its topological structures. However, while a few parameter tuning methods have already been investigated for the other Mapper parameters (i.e., resolution, gain, clustering), there is currently no method for tuning the filter itse
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#21435;&#30456;&#20851;&#27491;&#21017;&#21270;&#22120;&#30340;CCFC++&#26041;&#27861;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#25968;&#25454;&#24322;&#36136;&#24615;&#23545;&#32852;&#37030;&#32858;&#31867;&#24615;&#33021;&#30340;&#19981;&#21033;&#24433;&#21709;&#65292;&#21462;&#24471;&#20102;&#20248;&#36234;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.12852</link><description>&lt;p&gt;
CCFC++&#65306;&#36890;&#36807;&#29305;&#24449;&#21435;&#30456;&#20851;&#22686;&#24378;&#32852;&#37030;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
CCFC++: Enhancing Federated Clustering through Feature Decorrelation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12852
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#21435;&#30456;&#20851;&#27491;&#21017;&#21270;&#22120;&#30340;CCFC++&#26041;&#27861;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#25968;&#25454;&#24322;&#36136;&#24615;&#23545;&#32852;&#37030;&#32858;&#31867;&#24615;&#33021;&#30340;&#19981;&#21033;&#24433;&#21709;&#65292;&#21462;&#24471;&#20102;&#20248;&#36234;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#37030;&#32858;&#31867;&#20013;&#65292;&#22810;&#20010;&#25345;&#26377;&#25968;&#25454;&#30340;&#23458;&#25143;&#31471;&#22312;&#19981;&#20132;&#25442;&#21407;&#22987;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21327;&#20316;&#23545;&#25968;&#25454;&#36827;&#34892;&#20998;&#32452;&#12290;&#36825;&#19968;&#39046;&#22495;&#36890;&#36807;&#19982;&#23545;&#27604;&#23398;&#20064;&#30456;&#32467;&#21512;&#21462;&#24471;&#20102;&#26174;&#30528;&#36827;&#23637;&#65292;&#20197;Cluster-Contrastive Federated Clustering (CCFC)&#20026;&#20363;&#12290;&#28982;&#32780;&#65292;CCFC&#21463;&#21040;&#19981;&#21516;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#24322;&#26500;&#25968;&#25454;&#24433;&#21709;&#65292;&#23548;&#33268;&#34920;&#29616;&#19981;&#20339;&#19988;&#19981;&#31283;&#20581;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36827;&#34892;&#20102;&#32463;&#39564;&#21644;&#29702;&#35770;&#20998;&#26512;&#65292;&#20197;&#20102;&#35299;&#24322;&#26500;&#25968;&#25454;&#23545;CCFC&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22686;&#21152;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#21152;&#21095;&#20102;CCFC&#20013;&#30340;&#32500;&#24230;&#22349;&#32553;&#65292;&#36890;&#36807;&#22686;&#21152;&#23398;&#20064;&#34920;&#31034;&#30340;&#22810;&#20010;&#32500;&#24230;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26469;&#35777;&#26126;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#21521;CCFC&#24341;&#20837;&#20102;&#19968;&#20010;&#21435;&#30456;&#20851;&#27491;&#21017;&#21270;&#22120;&#12290;&#24471;&#30410;&#20110;&#27491;&#21017;&#21270;&#22120;&#65292;&#25913;&#36827;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#32531;&#35299;&#20102;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#19981;&#21033;&#24433;&#21709;&#65292;&#24182;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;NMI&#20998;&#25968;&#30340;&#26174;&#33879;&#22686;&#21152;&#26469;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12852v1 Announce Type: new  Abstract: In federated clustering, multiple data-holding clients collaboratively group data without exchanging raw data. This field has seen notable advancements through its marriage with contrastive learning, exemplified by Cluster-Contrastive Federated Clustering (CCFC). However, CCFC suffers from heterogeneous data across clients, leading to poor and unrobust performance. Our study conducts both empirical and theoretical analyses to understand the impact of heterogeneous data on CCFC. Findings indicate that increased data heterogeneity exacerbates dimensional collapse in CCFC, evidenced by increased correlations across multiple dimensions of the learned representations. To address this, we introduce a decorrelation regularizer to CCFC. Benefiting from the regularizer, the improved method effectively mitigates the detrimental effects of data heterogeneity, and achieves superior performance, as evidenced by a marked increase in NMI scores, with t
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#25345;&#32493;&#39044;&#35757;&#32451;&#25991;&#26723;&#20043;&#21069;&#26292;&#38706;LLM&#21040;&#38382;&#39064;-&#31572;&#26696;&#23545;&#65292;&#20197;&#20415;&#20174;&#22797;&#26434;&#25991;&#26723;&#20013;&#32534;&#30721;&#30693;&#35782;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#30693;&#35782;&#35775;&#38382;&#26041;&#24335;&#12290;</title><link>https://arxiv.org/abs/2402.12847</link><description>&lt;p&gt;
&#35843;&#25972;&#36807;&#30340;&#35821;&#35328;&#27169;&#22411;&#26356;&#22909;&#30340;&#30693;&#35782;&#23398;&#20064;&#32773;
&lt;/p&gt;
&lt;p&gt;
Instruction-tuned Language Models are Better Knowledge Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12847
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#25345;&#32493;&#39044;&#35757;&#32451;&#25991;&#26723;&#20043;&#21069;&#26292;&#38706;LLM&#21040;&#38382;&#39064;-&#31572;&#26696;&#23545;&#65292;&#20197;&#20415;&#20174;&#22797;&#26434;&#25991;&#26723;&#20013;&#32534;&#30721;&#30693;&#35782;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#30693;&#35782;&#35775;&#38382;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20351;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21161;&#25163;&#33021;&#22815;&#26377;&#25928;&#22320;&#36866;&#24212;&#19981;&#26029;&#21457;&#23637;&#30340;&#20449;&#24687;&#38656;&#27714;&#65292;&#24517;&#39035;&#33021;&#22815;&#36890;&#36807;&#25345;&#32493;&#22312;&#26032;&#25968;&#25454;&#19978;&#35757;&#32451;&#26469;&#26356;&#26032;&#23427;&#20204;&#30340;&#20107;&#23454;&#30693;&#35782;&#12290;&#20256;&#32479;&#20570;&#27861;&#28041;&#21450;&#22312;&#26032;&#25991;&#26723;&#19978;&#25345;&#32493;&#39044;&#22521;&#35757;&#65292;&#28982;&#21518;&#26681;&#25454;&#38382;&#39064;-&#31572;&#26696;&#65288;QA&#65289;&#23545;&#36827;&#34892;&#25351;&#23548;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12847v1 Announce Type: cross  Abstract: In order for large language model (LLM)-based assistants to effectively adapt to evolving information needs, it must be possible to update their factual knowledge through continued training on new data. The standard recipe for doing so involves continued pre-training on new documents followed by instruction-tuning on question-answer (QA) pairs. However, we find that LLMs trained with this recipe struggle to answer questions, even though the perplexity of documents is minimized. We found that QA pairs are generally straightforward, while documents are more complex, weaving many factual statements together in an intricate manner. Therefore, we hypothesize that it is beneficial to expose LLMs to QA pairs before continued pre-training on documents so that the process of encoding knowledge from complex documents takes into account how this knowledge is accessed through questions. Based on this, we propose pre-instruction-tuning (PIT), a met
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;PromptKD&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#31034;&#35843;&#25972;&#23454;&#29616;&#20102;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#23398;&#29983;&#21451;&#22909;&#30693;&#35782;&#30340;&#33976;&#39311;&#65292;&#26080;&#38656;&#24494;&#35843;&#25972;&#25972;&#20010;&#25945;&#24072;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.12842</link><description>&lt;p&gt;
PromptKD&#65306;&#36890;&#36807;&#25552;&#31034;&#35843;&#25972;&#20026;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#23398;&#29983;&#21451;&#22909;&#30693;&#35782;&#30340;&#33976;&#39311;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
PromptKD: Distilling Student-Friendly Knowledge for Generative Language Models via Prompt Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12842
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;PromptKD&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#31034;&#35843;&#25972;&#23454;&#29616;&#20102;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#23398;&#29983;&#21451;&#22909;&#30693;&#35782;&#30340;&#33976;&#39311;&#65292;&#26080;&#38656;&#24494;&#35843;&#25972;&#25972;&#20010;&#25945;&#24072;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#24341;&#36215;&#20102;&#23545;&#25512;&#29702;&#25104;&#26412;&#30340;&#25285;&#24551;&#65292;&#36827;&#19968;&#27493;&#22686;&#21152;&#20102;&#23545;&#27169;&#22411;&#21387;&#32553;&#30740;&#31350;&#30340;&#38656;&#27714;&#12290;&#23613;&#31649;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#26159;&#19968;&#31181;&#31361;&#20986;&#30340;&#26041;&#27861;&#65292;&#20294;&#26159;&#38024;&#23545;LLMs&#36825;&#26679;&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;KD&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#65292;&#32780;&#25552;&#21462;&#36866;&#21512;&#23398;&#29983;&#30340;&#30693;&#35782;&#30340;&#26041;&#27861;&#65292;&#22312;&#20998;&#31867;&#27169;&#22411;&#30340;KD&#20013;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#24615;&#33021;&#65292;&#22312;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#20013;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#20026;&#20102;&#25506;&#32034;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PromptKD&#65292;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#25552;&#31034;&#35843;&#25972; - &#22312;KD&#20013;&#39318;&#27425;&#20986;&#29616; - &#20351;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20256;&#36882;&#36866;&#21512;&#23398;&#29983;&#30340;&#30693;&#35782;&#12290;&#19982;&#20808;&#21069;&#20998;&#31867;&#24037;&#20316;&#19981;&#21516;&#65292;&#20808;&#21069;&#37027;&#20123;&#38656;&#35201;&#24494;&#35843;&#25972;&#25972;&#20010;&#25945;&#24072;&#27169;&#22411;&#20197;&#25552;&#21462;&#36866;&#21512;&#23398;&#29983;&#30340;&#30693;&#35782;&#65292;PromptKD&#36890;&#36807;&#28155;&#21152;&#23569;&#37327;&#25552;&#31034;&#26631;&#35760;&#65292;&#24182;&#20165;&#36890;&#36807;&#23398;&#29983;&#25351;&#23548;&#35843;&#25972;&#25552;&#31034;&#26469;&#36798;&#21040;&#31867;&#20284;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12842v1 Announce Type: cross  Abstract: Recent advancements in large language models (LLMs) have raised concerns about inference costs, increasing the need for research into model compression. While knowledge distillation (KD) is a prominent method for this, research on KD for generative language models like LLMs is relatively sparse, and the approach of distilling student-friendly knowledge, which has shown promising performance in KD for classification models, remains unexplored in generative language models. To explore this approach, we propose PromptKD, a simple yet effective method that utilizes prompt tuning - for the first time in KD - to enable generative language models to transfer student-friendly knowledge. Unlike previous works in classification that require fine-tuning the entire teacher model for extracting student-friendly knowledge, PromptKD achieves similar effects by adding a small number of prompt tokens and tuning only the prompt with student guidance. Ex
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20013;&#20540;&#20272;&#35745;&#30340;&#31283;&#20581;&#26799;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#38024;&#23545;&#21253;&#25324;&#37325;&#23614;&#22122;&#22768;&#22312;&#20869;&#30340;&#22810;&#31181;&#24212;&#29992;&#22330;&#26223;&#36827;&#34892;&#20102;&#25506;&#35752;&#65292;&#25581;&#31034;&#20102;&#19981;&#21516;&#24418;&#24335;&#30340;&#21098;&#20999;&#26041;&#27861;&#23454;&#38469;&#19978;&#26159;&#35813;&#26041;&#27861;&#30340;&#29305;&#20363;&#12290;</title><link>https://arxiv.org/abs/2402.12828</link><description>&lt;p&gt;
SGD&#26799;&#24230;&#21098;&#20999;&#26041;&#27861;&#26263;&#20013;&#20272;&#35745;&#20013;&#20540;&#26799;&#24230;
&lt;/p&gt;
&lt;p&gt;
SGD with Clipping is Secretly Estimating the Median Gradient
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12828
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20013;&#20540;&#20272;&#35745;&#30340;&#31283;&#20581;&#26799;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#38024;&#23545;&#21253;&#25324;&#37325;&#23614;&#22122;&#22768;&#22312;&#20869;&#30340;&#22810;&#31181;&#24212;&#29992;&#22330;&#26223;&#36827;&#34892;&#20102;&#25506;&#35752;&#65292;&#25581;&#31034;&#20102;&#19981;&#21516;&#24418;&#24335;&#30340;&#21098;&#20999;&#26041;&#27861;&#23454;&#38469;&#19978;&#26159;&#35813;&#26041;&#27861;&#30340;&#29305;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#20960;&#31181;&#38543;&#26426;&#20248;&#21270;&#30340;&#24212;&#29992;&#22330;&#26223;&#21487;&#20197;&#21463;&#30410;&#20110;&#23545;&#26799;&#24230;&#30340;&#31283;&#20581;&#20272;&#35745;&#12290;&#20363;&#22914;&#65292;&#22312;&#20855;&#26377;&#25439;&#22351;&#33410;&#28857;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#39046;&#22495;&#12289;&#35757;&#32451;&#25968;&#25454;&#20013;&#23384;&#22312;&#22823;&#30340;&#24322;&#24120;&#20540;&#12289;&#22312;&#38544;&#31169;&#32422;&#26463;&#19979;&#23398;&#20064;&#65292;&#29978;&#33267;&#30001;&#20110;&#31639;&#27861;&#21160;&#24577;&#26412;&#36523;&#30340;&#37325;&#23614;&#22122;&#22768;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#20013;&#20540;&#20272;&#35745;&#30340;&#31283;&#20581;&#26799;&#24230;&#20272;&#35745;&#30340;SGD&#12290;&#39318;&#20808;&#32771;&#34385;&#36328;&#26679;&#26412;&#35745;&#31639;&#20013;&#20540;&#26799;&#24230;&#65292;&#32467;&#26524;&#34920;&#26126;&#21363;&#20351;&#22312;&#37325;&#23614;&#12289;&#29366;&#24577;&#30456;&#20851;&#22122;&#22768;&#19979;&#65292;&#35813;&#26041;&#27861;&#20063;&#33021;&#25910;&#25947;&#12290;&#28982;&#21518;&#25105;&#20204;&#25512;&#23548;&#20102;&#22522;&#20110;&#38543;&#26426;&#36817;&#31471;&#28857;&#26041;&#27861;&#30340;&#36845;&#20195;&#26041;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#20960;&#20309;&#20013;&#20540;&#21644;&#20854;&#25512;&#24191;&#24418;&#24335;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#36845;&#20195;&#38388;&#30340;&#20013;&#20540;&#26799;&#24230;&#65292;&#24182;&#21457;&#29616;&#20960;&#31181;&#20247;&#25152;&#21608;&#30693;&#30340;&#26041;&#27861; - &#29305;&#21035;&#26159;&#19981;&#21516;&#24418;&#24335;&#30340;&#21098;&#20999; - &#26159;&#36825;&#19968;&#26694;&#26550;&#30340;&#29305;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12828v1 Announce Type: cross  Abstract: There are several applications of stochastic optimization where one can benefit from a robust estimate of the gradient. For example, domains such as distributed learning with corrupted nodes, the presence of large outliers in the training data, learning under privacy constraints, or even heavy-tailed noise due to the dynamics of the algorithm itself. Here we study SGD with robust gradient estimators based on estimating the median. We first consider computing the median gradient across samples, and show that the resulting method can converge even under heavy-tailed, state-dependent noise. We then derive iterative methods based on the stochastic proximal point method for computing the geometric median and generalizations thereof. Finally we propose an algorithm estimating the median gradient across iterations, and find that several well known methods - in particular different forms of clipping - are particular cases of this framework.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#38024;&#23545;&#25688;&#35201;&#20013;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#65306;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27491;&#30830;&#30340;&#33539;&#24335;&#35774;&#35745;&#19979;&#26080;&#38656;&#35757;&#32451;&#21363;&#21487;&#35299;&#20915;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#35757;&#32451;&#31574;&#30053;&#20197;&#31934;&#28860;&#26356;&#23567;&#22411;&#30340;&#39640;&#20934;&#30830;&#24615;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.12821</link><description>&lt;p&gt;
&#22312;&#25688;&#35201;&#20013;&#35782;&#21035;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#65306;&#26397;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26377;&#25928;&#21033;&#29992;
&lt;/p&gt;
&lt;p&gt;
Identifying Factual Inconsistency in Summaries: Towards Effective Utilization of Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12821
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#38024;&#23545;&#25688;&#35201;&#20013;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#65306;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27491;&#30830;&#30340;&#33539;&#24335;&#35774;&#35745;&#19979;&#26080;&#38656;&#35757;&#32451;&#21363;&#21487;&#35299;&#20915;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#35757;&#32451;&#31574;&#30053;&#20197;&#31934;&#28860;&#26356;&#23567;&#22411;&#30340;&#39640;&#20934;&#30830;&#24615;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#23454;&#19978;&#30340;&#19981;&#19968;&#33268;&#24615;&#23545;&#25277;&#35937;&#24615;&#25688;&#35201;&#29983;&#25104;&#22120;&#30340;&#21830;&#19994;&#37096;&#32626;&#26500;&#25104;&#37325;&#35201;&#38556;&#30861;&#12290;&#26412;&#30740;&#31350;&#22260;&#32469;&#20004;&#20010;&#37325;&#35201;&#38382;&#39064;&#23637;&#24320;&#65306;&#22914;&#20309;&#26368;&#22909;&#22320;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#26816;&#27979;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#65292;&#20197;&#21450;&#22914;&#20309;&#31934;&#28860;&#19968;&#20010;&#21516;&#26102;&#20855;&#26377;&#39640;&#25928;&#24615;&#21644;&#21151;&#25928;&#24615;&#30340;&#26356;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65311;&#39318;&#20808;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#19977;&#31181;&#38646;&#26679;&#26412;&#33539;&#24335;&#65292;&#36328;&#36234;&#20116;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#65306;&#30452;&#25509;&#25512;&#29702;&#25972;&#20010;&#25688;&#35201;&#25110;&#27599;&#20010;&#25688;&#35201;&#31383;&#21475;&#65307;&#36890;&#36807;&#38382;&#39064;&#29983;&#25104;&#21644;&#22238;&#31572;&#36827;&#34892;&#23454;&#20307;&#39564;&#35777;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#36866;&#24403;&#30340;&#33539;&#24335;&#35774;&#35745;&#19979;&#65292;&#35821;&#35328;&#27169;&#22411;&#26412;&#36523;&#33021;&#22815;&#22312;&#26080;&#38656;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#36825;&#19968;&#20219;&#21153;&#65292;&#24179;&#22343;&#36229;&#36807;&#24378;&#22823;&#30340;&#35757;&#32451;&#22522;&#32447;2.8%&#12290;&#20026;&#36827;&#19968;&#27493;&#20419;&#36827;&#23454;&#29992;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#38024;&#23545;&#31934;&#28860;&#26356;&#23567;&#30340;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#19968;&#27425;&#24615;&#39640;&#20934;&#30830;&#22320;&#35780;&#20998;&#25972;&#20010;&#25688;&#35201;&#65292;&#32988;&#36807;&#38646;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12821v1 Announce Type: new  Abstract: Factual inconsistency poses a significant hurdle for the commercial deployment of abstractive summarizers. Under this Large Language Model (LLM) era, this work focuses around two important questions: what is the best way to leverage LLM for factual inconsistency detection, and how could we distill a smaller LLM with both high efficiency and efficacy? Three zero-shot paradigms are firstly proposed and evaluated across five diverse datasets: direct inference on the entire summary or each summary window; entity verification through question generation and answering. Experiments suggest that LLM itself is capable to resolve this task train-free under the proper paradigm design, surpassing strong trained baselines by 2.8% on average. To further promote practical utility, we then propose training strategies aimed at distilling smaller open-source LLM that learns to score the entire summary at once with high accuracy, which outperforms the zero
&lt;/p&gt;</description></item><item><title>&#19987;&#38376;&#27169;&#22411;&#36890;&#24120;&#21482;&#38656;&#23569;&#37327;&#26631;&#35760;&#26679;&#26412;&#65288;100-1000&#20010;&#65289;&#23601;&#33021;&#19982;&#36890;&#29992;&#27169;&#22411;&#25345;&#24179;&#29978;&#33267;&#26356;&#22909;&#65292;&#21462;&#20915;&#20110;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#21644;&#32467;&#26524;&#30340;&#21464;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.12819</link><description>&lt;p&gt;
&#24494;&#35843;&#12289;&#25552;&#31034;&#12289;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#25351;&#23548;&#24494;&#35843;&#65306;&#25105;&#20204;&#38656;&#35201;&#22810;&#23569;&#26631;&#35760;&#26679;&#26412;&#65311;
&lt;/p&gt;
&lt;p&gt;
Fine-Tuning, Prompting, In-Context Learning and Instruction-Tuning: How Many Labelled Samples Do We Need?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12819
&lt;/p&gt;
&lt;p&gt;
&#19987;&#38376;&#27169;&#22411;&#36890;&#24120;&#21482;&#38656;&#23569;&#37327;&#26631;&#35760;&#26679;&#26412;&#65288;100-1000&#20010;&#65289;&#23601;&#33021;&#19982;&#36890;&#29992;&#27169;&#22411;&#25345;&#24179;&#29978;&#33267;&#26356;&#22909;&#65292;&#21462;&#20915;&#20110;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#21644;&#32467;&#26524;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#35299;&#20915;&#20855;&#26377;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#30340;&#20219;&#21153;&#26102;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#36873;&#25321;&#20351;&#29992;&#36890;&#29992;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32780;&#19981;&#36827;&#34892;&#36827;&#19968;&#27493;&#26356;&#26032;&#65292;&#25110;&#32773;&#20351;&#29992;&#23569;&#37327;&#31034;&#20363;&#26469;&#35843;&#25972;&#19987;&#38376;&#30340;&#36739;&#23567;&#27169;&#22411;&#12290; &#24403;&#26377;&#36275;&#22815;&#30340;&#26631;&#35760;&#21487;&#29992;&#26102;&#65292;&#19987;&#38376;&#30340;&#27169;&#22411;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#20110;&#36890;&#29992;&#27169;&#22411;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#35843;&#26597;&#19987;&#38376;&#27169;&#22411;&#38656;&#35201;&#22810;&#23569;&#26631;&#35760;&#26679;&#26412;&#25165;&#33021;&#23454;&#29616;&#36825;&#31181;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#32771;&#34385;&#32467;&#26524;&#30340;&#21464;&#21270;&#12290;&#35266;&#23519;&#25552;&#31034;&#12289;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#24494;&#35843;&#21644;&#25351;&#23548;&#24494;&#35843;&#30340;&#34892;&#20026;&#65292;&#35782;&#21035;&#23427;&#20204;&#22312;&#22686;&#21152;&#19981;&#21516;&#22797;&#26434;&#24615;&#20219;&#21153;&#30340;&#26631;&#35760;&#35757;&#32451;&#26679;&#26412;&#25968;&#37327;&#26102;&#30340;&#25910;&#25903;&#24179;&#34913;&#28857;&#65292;&#25105;&#20204;&#21457;&#29616;&#19987;&#38376;&#27169;&#22411;&#36890;&#24120;&#21482;&#38656;&#23569;&#37327;&#26679;&#26412;&#65288;100-1000&#20010;&#65289;&#23601;&#33021;&#19982;&#36890;&#29992;&#27169;&#22411;&#25345;&#24179;&#29978;&#33267;&#26356;&#22909;&#12290; &#21516;&#26102;&#65292;&#25152;&#38656;&#30340;&#26631;&#35760;&#25968;&#25454;&#37327;&#24378;&#28872;&#20381;&#36182;&#20110;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#21644;&#32467;&#26524;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12819v1 Announce Type: cross  Abstract: When solving a task with limited labelled data, researchers can either use a general large language model without further update, or use the few examples to tune a specialised smaller model. When enough labels are available, the specialised models outperform the general ones on many NLP tasks. In this work, we aim to investigate how many labelled samples are required for the specialised models to achieve this superior performance, while taking the results variance into consideration. Observing the behaviour of prompting, in-context learning, fine-tuning and instruction-tuning, identifying their break-even points when increasing number of labelled training samples across three tasks of varying complexity, we find that the specialised models often need only few samples ($100-1000$) to be on par or better than the general ones. At the same time, the amount of required labelled data strongly depends on the task complexity and results varia
&lt;/p&gt;</description></item><item><title>&#26377;&#38480;&#26631;&#27880;&#25968;&#25454;&#23398;&#20064;&#23545;&#38543;&#26426;&#24615;&#30340;&#25935;&#24863;&#24615;&#65292;&#36890;&#36807;&#31995;&#32479;&#30740;&#31350;&#38543;&#26426;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#24573;&#30053;&#30456;&#20114;&#20316;&#29992;&#21487;&#33021;&#23548;&#33268;&#30340;&#19981;&#19968;&#33268;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.12817</link><description>&lt;p&gt;
&#26377;&#38480;&#26631;&#27880;&#25968;&#25454;&#23398;&#20064;&#23545;&#38543;&#26426;&#24615;&#30340;&#25935;&#24863;&#24615;&#65306;&#30456;&#20114;&#20316;&#29992;&#21644;&#31995;&#32479;&#36873;&#25321;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
On Sensitivity of Learning with Limited Labelled Data to the Effects of Randomness: Impact of Interactions and Systematic Choices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12817
&lt;/p&gt;
&lt;p&gt;
&#26377;&#38480;&#26631;&#27880;&#25968;&#25454;&#23398;&#20064;&#23545;&#38543;&#26426;&#24615;&#30340;&#25935;&#24863;&#24615;&#65292;&#36890;&#36807;&#31995;&#32479;&#30740;&#31350;&#38543;&#26426;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#24573;&#30053;&#30456;&#20114;&#20316;&#29992;&#21487;&#33021;&#23548;&#33268;&#30340;&#19981;&#19968;&#33268;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#38480;&#26631;&#27880;&#25968;&#25454;&#23398;&#20064;&#21487;&#20197;&#22312;&#26631;&#31614;&#19981;&#36275;&#26102;&#25552;&#39640;&#24615;&#33021;&#65292;&#20294;&#20063;&#23545;&#25152;&#35859;&#30340;&#38543;&#26426;&#22240;&#32032;&#65288;&#20363;&#22914;&#25968;&#25454;&#30340;&#21464;&#21270;&#39034;&#24207;&#65289;&#24341;&#20837;&#30340;&#26080;&#27861;&#25511;&#21046;&#30340;&#38543;&#26426;&#24615;&#25935;&#24863;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#31995;&#32479;&#22320;&#35843;&#26597;&#38543;&#26426;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#21516;&#26102;&#32771;&#34385;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#20026;&#20102;&#27979;&#37327;&#21333;&#20010;&#38543;&#26426;&#22240;&#32032;&#30340;&#30495;&#23454;&#24433;&#21709;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20943;&#36731;&#20102;&#20854;&#20182;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#24182;&#35266;&#23519;&#20102;&#24615;&#33021;&#22312;&#22810;&#27425;&#36816;&#34892;&#20013;&#30340;&#21464;&#21270;&#12290;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;7&#20010;&#20195;&#34920;&#24615;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#24494;&#35843;&#26041;&#27861;&#20197;&#21450;3&#20010;&#20219;&#21153;&#30340;&#20803;&#23398;&#20064;&#65292;&#25105;&#20204;&#21457;&#29616;&#65306;1&#65289;&#29616;&#26377;&#20316;&#21697;&#20013;&#24573;&#30053;&#38543;&#26426;&#22240;&#32032;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#23548;&#33268;&#20102;&#19981;&#19968;&#33268;&#30340;&#30740;&#31350;&#32467;&#26524;&#65292;&#22240;&#20026;&#38169;&#35823;&#22320;&#24402;&#22240;&#20110;&#38543;&#26426;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#27604;&#22914;&#21542;&#23450;&#20102;&#19968;&#20123;&#19968;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12817v1 Announce Type: cross  Abstract: While learning with limited labelled data can improve performance when the labels are lacking, it is also sensitive to the effects of uncontrolled randomness introduced by so-called randomness factors (e.g., varying order of data). We propose a method to systematically investigate the effects of randomness factors while taking the interactions between them into consideration. To measure the true effects of an individual randomness factor, our method mitigates the effects of other factors and observes how the performance varies across multiple runs. Applying our method to multiple randomness factors across in-context learning and fine-tuning approaches on 7 representative text classification tasks and meta-learning on 3 tasks, we show that: 1) disregarding interactions between randomness factors in existing works caused inconsistent findings due to incorrect attribution of the effects of randomness factors, such as disproving the consis
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#20998;&#25955;&#31639;&#27861;&#26694;&#26550;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#33258;&#32452;&#32455;&#25104;&#22270;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#21327;&#21516;&#22343;&#20540;&#20272;&#35745;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#27599;&#20010;&#20195;&#29702;&#22312;&#23398;&#20064;&#27169;&#22411;&#30340;&#21516;&#26102;&#35782;&#21035;&#20855;&#26377;&#30456;&#20284;&#20998;&#24067;&#23458;&#25143;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.12812</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#20998;&#25955;&#31639;&#27861;&#29992;&#20110;&#22312;&#32447;&#20010;&#24615;&#21270;&#22343;&#20540;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Scalable Decentralized Algorithms for Online Personalized Mean Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#20998;&#25955;&#31639;&#27861;&#26694;&#26550;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#33258;&#32452;&#32455;&#25104;&#22270;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#21327;&#21516;&#22343;&#20540;&#20272;&#35745;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#27599;&#20010;&#20195;&#29702;&#22312;&#23398;&#20064;&#27169;&#22411;&#30340;&#21516;&#26102;&#35782;&#21035;&#20855;&#26377;&#30456;&#20284;&#20998;&#24067;&#23458;&#25143;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#20195;&#29702;&#32570;&#20047;&#36275;&#22815;&#30340;&#25968;&#25454;&#30452;&#25509;&#23398;&#20064;&#27169;&#22411;&#12290;&#19982;&#20854;&#20182;&#20195;&#29702;&#21512;&#20316;&#21487;&#33021;&#26377;&#25152;&#24110;&#21161;&#65292;&#20294;&#24403;&#26412;&#22320;&#25968;&#25454;&#20998;&#24067;&#19981;&#21516;&#26102;&#65292;&#20250;&#24341;&#20837;&#20559;&#24046;-&#26041;&#24046;&#26435;&#34913;&#12290;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#27599;&#20010;&#20195;&#29702;&#22312;&#23398;&#20064;&#27169;&#22411;&#30340;&#21516;&#26102;&#35782;&#21035;&#20855;&#26377;&#30456;&#20284;&#20998;&#24067;&#30340;&#23458;&#25143;&#65292;&#36825;&#20010;&#38382;&#39064;&#20027;&#35201;&#20173;&#26410;&#35299;&#20915;&#12290;&#26412;&#30740;&#31350;&#30528;&#30524;&#20110;&#19968;&#20010;&#31616;&#21270;&#29256;&#26412;&#30340;&#26222;&#36941;&#38382;&#39064;&#65292;&#21363;&#27599;&#20010;&#20195;&#29702;&#38543;&#26102;&#38388;&#20174;&#23454;&#20540;&#20998;&#24067;&#20013;&#25910;&#38598;&#26679;&#26412;&#26469;&#20272;&#35745;&#20854;&#22343;&#20540;&#12290;&#29616;&#26377;&#31639;&#27861;&#38754;&#20020;&#30528;&#19981;&#20999;&#23454;&#38469;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#22797;&#26434;&#24230;&#65288;&#19982;&#20195;&#29702;&#25968;&#37327;A&#30340;&#24179;&#26041;&#25104;&#27491;&#27604;&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#21487;&#25193;&#23637;&#24615;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20195;&#29702;&#33258;&#32452;&#32455;&#25104;&#19968;&#20010;&#22270;&#65292;&#20351;&#24471;&#27599;&#20010;&#20195;&#29702;&#21482;&#33021;&#19982;&#36873;&#23450;&#25968;&#37327;&#30340;&#23545;&#31561;&#20307;r&#36827;&#34892;&#36890;&#20449;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#31181;&#21327;&#20316;&#22343;&#20540;&#20272;&#35745;&#31639;&#27861;&#65306;&#19968;&#31181;&#28789;&#24863;&#26469;&#28304;&#20110;&#20449;&#24565;&#20256;&#25773;&#65292;&#21478;&#19968;&#31181;&#37319;&#29992;&#22522;&#20110;&#20849;&#35782;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12812v1 Announce Type: new  Abstract: In numerous settings, agents lack sufficient data to directly learn a model. Collaborating with other agents may help, but it introduces a bias-variance trade-off, when local data distributions differ. A key challenge is for each agent to identify clients with similar distributions while learning the model, a problem that remains largely unresolved. This study focuses on a simplified version of the overarching problem, where each agent collects samples from a real-valued distribution over time to estimate its mean. Existing algorithms face impractical space and time complexities (quadratic in the number of agents A). To address scalability challenges, we propose a framework where agents self-organize into a graph, allowing each agent to communicate with only a selected number of peers r. We introduce two collaborative mean estimation algorithms: one draws inspiration from belief propagation, while the other employs a consensus-based appr
&lt;/p&gt;</description></item><item><title>&#23558;NHPPs&#30340;&#20272;&#35745;&#38382;&#39064;&#36716;&#21270;&#20026;&#23398;&#20064;&#27867;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#27491;&#21017;&#21270;&#23398;&#20064;NHPPs&#30340;&#26694;&#26550;&#19982;&#20004;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;&#20998;&#31665;&#26041;&#27861;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#25968;&#25454;&#37327;&#26377;&#38480;&#26102;&#36807;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.12808</link><description>&lt;p&gt;
&#23398;&#20064;&#38750;&#40784;&#27425;&#26102;&#38388;&#27850;&#26494;&#36807;&#31243;&#30340;&#27867;&#21270;&#21644;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Learning Generalization and Regularization of Nonhomogeneous Temporal Poisson Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12808
&lt;/p&gt;
&lt;p&gt;
&#23558;NHPPs&#30340;&#20272;&#35745;&#38382;&#39064;&#36716;&#21270;&#20026;&#23398;&#20064;&#27867;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#27491;&#21017;&#21270;&#23398;&#20064;NHPPs&#30340;&#26694;&#26550;&#19982;&#20004;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;&#20998;&#31665;&#26041;&#27861;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#25968;&#25454;&#37327;&#26377;&#38480;&#26102;&#36807;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27850;&#26494;&#36807;&#31243;&#65292;&#23588;&#20854;&#26159;&#38750;&#40784;&#27425;&#27850;&#26494;&#36807;&#31243;(NHPP)&#65292;&#26159;&#19968;&#31181;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#38750;&#24120;&#37325;&#35201;&#30340;&#35745;&#25968;&#36807;&#31243;&#12290;&#30446;&#21069;&#65292;&#25991;&#29486;&#20013;&#20960;&#20046;&#25152;&#26377;&#30340;&#24037;&#20316;&#37117;&#33268;&#21147;&#20110;&#20351;&#29992;&#38750;&#25968;&#25454;&#39537;&#21160;&#30340;&#20998;&#31665;&#26041;&#27861;&#23545;&#20855;&#26377;&#26080;&#31351;&#25968;&#25454;&#30340;NHPP&#36827;&#34892;&#20272;&#35745;&#12290;&#26412;&#25991;&#23558;&#26377;&#38480;&#21644;&#26377;&#38480;&#25968;&#25454;&#19979;&#30340;NHPP&#20272;&#35745;&#38382;&#39064;&#20844;&#24335;&#21270;&#20026;&#19968;&#20010;&#23398;&#20064;&#27867;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#25968;&#23398;&#19978;&#35777;&#26126;&#65292;&#23613;&#31649;&#20998;&#31665;&#26041;&#27861;&#23545;&#20110;&#20272;&#35745;NHPPs&#24456;&#37325;&#35201;&#65292;&#20294;&#22312;&#25968;&#25454;&#37327;&#26377;&#38480;&#26102;&#20250;&#24102;&#26469;&#36807;&#25311;&#21512;&#30340;&#39118;&#38505;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27491;&#21017;&#21270;&#23398;&#20064;NHPPs&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#20004;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;&#20998;&#31665;&#26041;&#27861;&#65292;&#24110;&#21161;&#28040;&#38500;&#20998;&#31665;&#21442;&#25968;&#30340;&#21363;&#20852;&#35843;&#25972;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12808v1 Announce Type: new  Abstract: The Poisson process, especially the nonhomogeneous Poisson process (NHPP), is an essentially important counting process with numerous real-world applications. Up to date, almost all works in the literature have been on the estimation of NHPPs with infinite data using non-data driven binning methods. In this paper, we formulate the problem of estimation of NHPPs from finite and limited data as a learning generalization problem. We mathematically show that while binning methods are essential for the estimation of NHPPs, they pose a threat of overfitting when the amount of data is limited. We propose a framework for regularized learning of NHPPs with two new adaptive and data-driven binning methods that help to remove the ad-hoc tuning of binning parameters. Our methods are experimentally tested on synthetic and real-world datasets and the results show their effectiveness.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#33258;&#20027;&#20223;&#29983;&#22235;&#36275;&#26426;&#22120;&#20154;&#20195;&#29702;&#21644;&#26080;&#20154;&#26426;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#20027;&#30340;3D&#29616;&#23454;&#24314;&#27169;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#25991;&#21270;&#36951;&#20135;(CH)&#25991;&#29289;&#65292;&#23454;&#29616;&#20102;&#31995;&#32479;&#21270;&#21644;&#21487;&#37325;&#22797;&#30340;3D RM&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.12794</link><description>&lt;p&gt;
&#21033;&#29992;&#21327;&#20316;&#22235;&#36275;&#26426;&#22120;&#20154;&#21644;&#26080;&#20154;&#26426;&#36827;&#34892;&#25991;&#21270;&#36951;&#20135;&#36951;&#22336;&#30340;&#33258;&#20027;&#29616;&#23454;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Autonomous Reality Modelling for Cultural Heritage Sites employing cooperative quadrupedal robots and unmanned aerial vehicles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12794
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#33258;&#20027;&#20223;&#29983;&#22235;&#36275;&#26426;&#22120;&#20154;&#20195;&#29702;&#21644;&#26080;&#20154;&#26426;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#20027;&#30340;3D&#29616;&#23454;&#24314;&#27169;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#25991;&#21270;&#36951;&#20135;(CH)&#25991;&#29289;&#65292;&#23454;&#29616;&#20102;&#31995;&#32479;&#21270;&#21644;&#21487;&#37325;&#22797;&#30340;3D RM&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#20808;&#36827;&#20256;&#24863;&#22120;&#30340;&#20351;&#29992;&#65292;&#22914;&#22320;&#38754;3D&#28608;&#20809;&#25195;&#25551;&#20202;&#12289;&#31227;&#21160;LiDAR&#21644;&#26080;&#20154;&#26426;&#25668;&#24433;&#27979;&#37327;&#65292;&#24050;&#32463;&#25104;&#20026;&#25991;&#21270;&#36951;&#20135;(CH)&#22823;&#22411;&#25991;&#29289;&#30340;3D&#29616;&#23454;&#24314;&#27169;&#21644;&#25968;&#23383;&#21270;&#30340;&#20027;&#35201;&#23454;&#36341;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#36825;&#20010;&#36807;&#31243;&#19982;&#35843;&#26597;&#22242;&#38431;&#30340;&#19987;&#19994;&#30693;&#35782;&#23494;&#20999;&#30456;&#20851;&#65292;&#22788;&#29702;&#38024;&#23545;&#27599;&#20010;&#36951;&#22336;&#29305;&#23450;&#35201;&#27714;&#21644;&#32422;&#26463;&#30340;&#32791;&#26102;&#35268;&#21010;&#21644;&#25191;&#34892;3D&#26144;&#23556;&#36807;&#31243;&#12290;&#20026;&#20102;&#26368;&#23567;&#21270;&#20154;&#31867;&#24178;&#39044;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#37197;&#22791;&#36866;&#24403;&#20256;&#24863;&#22120;&#30340;&#33258;&#20027;&#20223;&#29983;&#22235;&#36275;&#26426;&#22120;&#20154;&#20195;&#29702;&#21644;&#26080;&#20154;&#26426;&#26469;&#23454;&#29616;&#25991;&#21270;&#36951;&#20135;(CH)&#25991;&#29289;&#30340;&#33258;&#20027;3D&#29616;&#23454;&#24314;&#27169;&#12290;&#36825;&#20123;&#33258;&#20027;&#26426;&#22120;&#20154;&#20195;&#29702;&#20197;&#31995;&#32479;&#21270;&#19988;&#21487;&#37325;&#22797;&#30340;&#26041;&#24335;&#36827;&#34892;3D RM&#36807;&#31243;&#12290;&#36825;&#20010;&#33258;&#21160;&#21270;&#36807;&#31243;&#30340;&#32467;&#26524;&#21487;&#33021;&#22312;&#25968;&#23383;&#23402;&#29983;&#24179;&#21488;&#20013;&#25214;&#21040;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12794v1 Announce Type: cross  Abstract: Nowadays, the use of advanced sensors, such as terrestrial 3D laser scanners, mobile LiDARs and Unmanned Aerial Vehicles (UAV) photogrammetric imaging, has become the prevalent practice for 3D Reality Modeling and digitization of large-scale monuments of Cultural Heritage (CH). In practice, this process is heavily related to the expertise of the surveying team, handling the laborious planning and time-consuming execution of the 3D mapping process that is tailored to the specific requirements and constraints of each site. To minimize human intervention, this paper introduces a novel methodology for autonomous 3D Reality Modeling for CH monuments by employing au-tonomous biomimetic quadrupedal robotic agents and UAVs equipped with the appropriate sensors. These autonomous robotic agents carry out the 3D RM process in a systematic and repeatable ap-proach. The outcomes of this automated process may find applications in digital twin platfo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27979;&#35797;&#24182;&#27604;&#36739;&#20102;&#22522;&#20110;&#39592;&#26550;&#30340;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#20013;&#30340;&#21487;&#35299;&#37322;AI&#26041;&#27861;&#65292;&#21457;&#29616;&#22312;&#26576;&#20123;&#24773;&#22659;&#19979;&#8220;&#24544;&#23454;&#24230;&#8221;&#21487;&#33021;&#19981;&#21487;&#38752;&#65292;&#32780;&#8220;&#31283;&#23450;&#24615;&#8221;&#22312;&#36731;&#24494;&#25968;&#25454;&#25200;&#21160;&#26102;&#26356;&#21487;&#38752;&#12290;</title><link>https://arxiv.org/abs/2402.12790</link><description>&lt;p&gt;
&#20174;&#36816;&#21160;&#21040;&#34913;&#37327;&#65306;&#35780;&#20272;&#22522;&#20110;&#39592;&#26550;&#30340;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#21487;&#35299;&#37322;AI&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
From Movements to Metrics: Evaluating Explainable AI Methods in Skeleton-Based Human Activity Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12790
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27979;&#35797;&#24182;&#27604;&#36739;&#20102;&#22522;&#20110;&#39592;&#26550;&#30340;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#20013;&#30340;&#21487;&#35299;&#37322;AI&#26041;&#27861;&#65292;&#21457;&#29616;&#22312;&#26576;&#20123;&#24773;&#22659;&#19979;&#8220;&#24544;&#23454;&#24230;&#8221;&#21487;&#33021;&#19981;&#21487;&#38752;&#65292;&#32780;&#8220;&#31283;&#23450;&#24615;&#8221;&#22312;&#36731;&#24494;&#25968;&#25454;&#25200;&#21160;&#26102;&#26356;&#21487;&#38752;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
3D&#39592;&#26550;&#25968;&#25454;&#20013;&#28145;&#24230;&#23398;&#20064;&#22312;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#65288;HAR&#65289;&#20013;&#30340;&#36827;&#23637;&#23545;&#21307;&#30103;&#12289;&#23433;&#20840;&#12289;&#20307;&#32946;&#21644;&#20154;&#26426;&#20132;&#20114;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#35813;&#39046;&#22495;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#38382;&#39064;&#65292;&#21363;&#22312;&#22522;&#20110;&#39592;&#26550;&#30340;HAR&#39046;&#22495;&#20013;&#32570;&#20047;XAI&#35780;&#20272;&#25351;&#26631;&#30340;&#36866;&#29992;&#24615;&#21644;&#21487;&#38752;&#24615;&#27979;&#35797;&#12290;&#25105;&#20204;&#22312;&#31867;&#28608;&#27963;&#26144;&#23556;&#65288;CAM&#65289;&#21644;&#26799;&#24230;&#21152;&#26435;&#31867;&#28608;&#27963;&#26144;&#23556;&#65288;Grad-CAM&#65289;&#19978;&#27979;&#35797;&#20102;&#24050;&#24314;&#31435;&#30340;XAI&#35780;&#20272;&#25351;&#26631;&#65292;&#21363;&#24544;&#23454;&#24230;&#21644;&#31283;&#23450;&#24615;&#65292;&#20197;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;&#35813;&#30740;&#31350;&#36824;&#24341;&#20837;&#19968;&#31181;&#23562;&#37325;&#20154;&#20307;&#29983;&#29289;&#21147;&#23398;&#32422;&#26463;&#30340;&#25200;&#21160;&#26041;&#27861;&#65292;&#20197;&#30830;&#20445;&#20154;&#31867;&#36816;&#21160;&#20013;&#30340;&#29616;&#23454;&#21464;&#21270;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26576;&#20123;&#24773;&#22659;&#19979;&#65292;&#22914;EfficientGCN&#27169;&#22411;&#20013;&#65292;\textit{&#24544;&#23454;&#24230;}&#21487;&#33021;&#19981;&#26159;&#19968;&#20010;&#21487;&#38752;&#30340;&#25351;&#26631;&#12290;&#30456;&#21453;&#65292;&#24403;&#23384;&#22312;&#36731;&#24494;&#36755;&#20837;&#25968;&#25454;&#25200;&#21160;&#26102;&#65292;&#31283;&#23450;&#24615;&#26356;&#21487;&#38752;&#12290;CAM&#21644;Grad-CAM
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12790v1 Announce Type: new  Abstract: The advancement of deep learning in human activity recognition (HAR) using 3D skeleton data is critical for applications in healthcare, security, sports, and human-computer interaction. This paper tackles a well-known gap in the field, which is the lack of testing in the applicability and reliability of XAI evaluation metrics in the skeleton-based HAR domain. We have tested established XAI metrics namely faithfulness and stability on Class Activation Mapping (CAM) and Gradient-weighted Class Activation Mapping (Grad-CAM) to address this problem. The study also introduces a perturbation method that respects human biomechanical constraints to ensure realistic variations in human movement. Our findings indicate that \textit{faithfulness} may not be a reliable metric in certain contexts, such as with the EfficientGCN model. Conversely, stability emerges as a more dependable metric when there is slight input data perturbations. CAM and Grad-C
&lt;/p&gt;</description></item><item><title>&#22312;&#19981;&#23454;&#26045;&#20844;&#24179;&#35757;&#32451;&#31639;&#27861;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#20844;&#24179;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#25277;&#26679;&#20855;&#26377;&#24433;&#21709;&#21147;&#30340;&#25968;&#25454;&#26469;&#36880;&#27493;&#36716;&#31227;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#65292;&#20174;&#32780;&#25552;&#39640;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12789</link><description>&lt;p&gt;
&#26080;&#38656;&#20844;&#24179;&#35757;&#32451;&#30340;&#20844;&#24179;&#20998;&#31867;&#22120;&#65306;&#19968;&#31181;&#21463;&#24433;&#21709;&#25968;&#25454;&#25277;&#26679;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Fair Classifiers Without Fair Training: An Influence-Guided Data Sampling Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12789
&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#23454;&#26045;&#20844;&#24179;&#35757;&#32451;&#31639;&#27861;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#20844;&#24179;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#25277;&#26679;&#20855;&#26377;&#24433;&#21709;&#21147;&#30340;&#25968;&#25454;&#26469;&#36880;&#27493;&#36716;&#31227;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#65292;&#20174;&#32780;&#25552;&#39640;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#20844;&#24179;&#30340;&#20998;&#31867;&#22120;&#24212;&#35813;&#30830;&#20445;&#26469;&#33258;&#19981;&#21516;&#32676;&#20307;&#30340;&#20154;&#20204;&#21463;&#30410;&#65292;&#32780;&#32676;&#20307;&#20449;&#24687;&#24448;&#24448;&#26159;&#25935;&#24863;&#30340;&#65292;&#19981;&#36866;&#21512;&#27169;&#22411;&#35757;&#32451;&#12290;&#22240;&#27492;&#65292;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#19968;&#20010;&#20844;&#24179;&#30340;&#20998;&#31867;&#22120;&#20294;&#25490;&#38500;&#25935;&#24863;&#23646;&#24615;&#26159;&#24456;&#37325;&#35201;&#30340;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#20844;&#24179;&#20998;&#31867;&#22120;&#32780;&#19981;&#23454;&#29616;&#20844;&#24179;&#35757;&#32451;&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#20197;&#36991;&#20813;&#21487;&#33021;&#27844;&#38706;&#25935;&#24863;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#39564;&#35777;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#21487;&#33021;&#24615;&#65292;&#21363;&#22312;&#20855;&#26377;&#36866;&#24403;&#20998;&#24067;&#20559;&#31227;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20256;&#32479;&#35757;&#32451;&#21487;&#20197;&#21516;&#26102;&#20943;&#23569;&#20844;&#24179;&#24046;&#36317;&#30340;&#19978;&#38480;&#21644;&#27169;&#22411;&#27867;&#21270;&#35823;&#24046;&#65292;&#34920;&#26126;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#21487;&#20197;&#21516;&#27493;&#25552;&#39640;&#65292;&#21482;&#38656;&#31616;&#21333;&#22320;&#36827;&#34892;&#20256;&#32479;&#35757;&#32451;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#25277;&#26679;&#26377;&#24433;&#21709;&#21147;&#30340;&#25968;&#25454;&#36880;&#27493;&#36716;&#31227;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#19981;&#35775;&#38382;&#26032;&#25968;&#25454;&#30340;&#25935;&#24863;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12789v1 Announce Type: cross  Abstract: A fair classifier should ensure the benefit of people from different groups, while the group information is often sensitive and unsuitable for model training. Therefore, learning a fair classifier but excluding sensitive attributes in the training dataset is important. In this paper, we study learning fair classifiers without implementing fair training algorithms to avoid possible leakage of sensitive information. Our theoretical analyses validate the possibility of this approach, that traditional training on a dataset with an appropriate distribution shift can reduce both the upper bound for fairness disparity and model generalization error, indicating that fairness and accuracy can be improved simultaneously with simply traditional training. We then propose a tractable solution to progressively shift the original training data during training by sampling influential data, where the sensitive attribute of new data is not accessed in s
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#40065;&#26834;&#24179;&#22343;&#35268;&#21017;&#26469;&#25269;&#24481;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#25308;&#21344;&#24237;&#24335;&#23458;&#25143;&#65292;&#21516;&#26102;&#24378;&#35843;&#23458;&#25143;&#23376;&#37319;&#26679;&#21644;&#26412;&#22320;&#27493;&#39588;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.12780</link><description>&lt;p&gt;
&#22788;&#29702;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#25308;&#21344;&#24237;&#23458;&#25143;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Tackling Byzantine Clients in Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12780
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#40065;&#26834;&#24179;&#22343;&#35268;&#21017;&#26469;&#25269;&#24481;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#25308;&#21344;&#24237;&#24335;&#23458;&#25143;&#65292;&#21516;&#26102;&#24378;&#35843;&#23458;&#25143;&#23376;&#37319;&#26679;&#21644;&#26412;&#22320;&#27493;&#39588;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26367;&#25442;&#26631;&#20934;$\mathsf{FedAvg}$&#31639;&#27861;&#20013;&#26381;&#21153;&#22120;&#31471;&#30340;&#31616;&#21333;&#24179;&#22343;&#25805;&#20316;&#20026;\emph{&#40065;&#26834;&#24179;&#22343;&#35268;&#21017;}&#26469;&#20351;&#32852;&#37030;&#23398;&#20064;(FL)&#25269;&#24481;&#25308;&#21344;&#24237;&#24335;(adversarial)&#23458;&#25143;&#30340;&#21487;&#33021;&#24615;&#12290; &#20808;&#21069;&#30340;&#30740;&#31350;&#22823;&#37096;&#20998;&#24573;&#30053;&#20102;\emph{&#23458;&#25143;&#23376;&#37319;&#26679;}&#21644;\emph{&#26412;&#22320;&#27493;&#39588;}&#23545;FL&#29305;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#28145;&#20837;&#20998;&#26512;&#26469;&#39564;&#35777;&#36825;&#19968;&#35266;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12780v1 Announce Type: new  Abstract: The possibility of adversarial (a.k.a., {\em Byzantine}) clients makes federated learning (FL) prone to arbitrary manipulation. The natural approach to robustify FL against adversarial clients is to replace the simple averaging operation at the server in the standard $\mathsf{FedAvg}$ algorithm by a \emph{robust averaging rule}. While a significant amount of work has been devoted to studying the convergence of federated {\em robust averaging} (which we denote by $\mathsf{FedRo}$), prior work has largely ignored the impact of {\em client subsampling} and {\em local steps}, two fundamental FL characteristics. While client subsampling increases the effective fraction of Byzantine clients, local steps increase the drift between the local updates computed by honest (i.e., non-Byzantine) clients. Consequently, a careless deployment of $\mathsf{FedRo}$ could yield poor performance. We validate this observation by presenting an in-depth analysis
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;IDEA&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#21487;&#35782;&#21035;&#30340;&#28508;&#22312;&#29366;&#24577;&#26816;&#27979;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#20998;&#24067;&#21464;&#36801;&#65292;&#24182;&#36827;&#19968;&#27493;&#20998;&#31163;&#24179;&#31283;&#21644;&#38750;&#24179;&#31283;&#30340;&#28508;&#22312;&#29366;&#24577;&#12290;</title><link>https://arxiv.org/abs/2402.12767</link><description>&lt;p&gt;
&#22312;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#65306;&#23398;&#20064;&#21487;&#35782;&#21035;&#30340;&#28508;&#22312;&#29366;&#24577;&#36827;&#34892;&#38750;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
When and How: Learning Identifiable Latent States for Nonstationary Time Series Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12767
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;IDEA&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#21487;&#35782;&#21035;&#30340;&#28508;&#22312;&#29366;&#24577;&#26816;&#27979;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#20998;&#24067;&#21464;&#36801;&#65292;&#24182;&#36827;&#19968;&#27493;&#20998;&#31163;&#24179;&#31283;&#21644;&#38750;&#24179;&#31283;&#30340;&#28508;&#22312;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#20998;&#24067;&#30340;&#36716;&#31227;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#12290;&#20854;&#20013;&#19968;&#31181;&#26368;&#27969;&#34892;&#30340;&#26041;&#27861;&#20551;&#23450;&#26102;&#38388;&#20998;&#24067;&#30340;&#36716;&#31227;&#26159;&#22343;&#21248;&#21457;&#29983;&#30340;&#65292;&#20197;&#21306;&#20998;&#24179;&#31283;&#21644;&#38750;&#24179;&#31283;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#20551;&#35774;&#24456;&#38590;&#28385;&#36275;&#65292;&#22240;&#20026;&#25105;&#20204;&#19981;&#30693;&#36947;&#20998;&#24067;&#20309;&#26102;&#21457;&#29983;&#36716;&#31227;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23398;&#20064;&#21487;&#35782;&#21035;&#30340;&#28508;&#22312;&#29366;&#24577;&#65288;IDEA&#65289;&#26469;&#26816;&#27979;&#20998;&#24067;&#20309;&#26102;&#21457;&#29983;&#36716;&#31227;&#12290;&#38500;&#27492;&#20043;&#22806;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#20805;&#20998;&#35266;&#23519;&#20551;&#35774;&#26469;&#20998;&#31163;&#24179;&#31283;&#21644;&#38750;&#24179;&#31283;&#30340;&#28508;&#22312;&#29366;&#24577;&#65292;&#23398;&#20064;&#28508;&#22312;&#29366;&#24577;&#30340;&#21464;&#21270;&#26041;&#24335;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#22240;&#26524;&#36807;&#31243;&#24418;&#24335;&#21270;&#20026;&#19982;&#29615;&#22659;&#19981;&#30456;&#20851;&#30340;&#31283;&#23450;&#21464;&#37327;&#21644;&#19982;&#29615;&#22659;&#30456;&#20851;&#30340;&#38750;&#24179;&#31283;&#21464;&#37327;&#12290;&#22312;&#28201;&#21644;&#30340;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#28508;&#22312;&#29615;&#22659;&#21644;&#31283;&#23450;/&#38750;&#31283;&#23450;&#21464;&#37327;&#26159;&#21487;&#35782;&#21035;&#30340;&#12290;&#22522;&#20110;&#36825;&#20123;&#29702;&#35770;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;IDEA&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#33258;&#22238;&#24402;&#38544;&#39532;&#23572;&#31185;&#22827;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12767v1 Announce Type: new  Abstract: Temporal distribution shifts are ubiquitous in time series data. One of the most popular methods assumes that the temporal distribution shift occurs uniformly to disentangle the stationary and nonstationary dependencies. But this assumption is difficult to meet, as we do not know when the distribution shifts occur. To solve this problem, we propose to learn IDentifiable latEnt stAtes (IDEA) to detect when the distribution shifts occur. Beyond that, we further disentangle the stationary and nonstationary latent states via sufficient observation assumption to learn how the latent states change. Specifically, we formalize the causal process with environment-irrelated station- ary and environment-related nonstationary variables. Under mild conditions, we show that latent environments and stationary/nonstationary variables are identifiable. Based on these theories, we devise the IDEA model, which incorporates an autoregressive hidden Markov m
&lt;/p&gt;</description></item><item><title>LS&#20449;&#24687;&#20934;&#21017;&#26088;&#22312;&#22686;&#24378;WBIC&#21644;sBIC&#30340;&#21151;&#33021;&#65292;&#26377;&#25928;&#22788;&#29702;&#38750;&#27491;&#21017;&#24773;&#20917;&#65292;&#20855;&#26377;&#31283;&#23450;&#24615;&#65292;&#20026;&#22855;&#24322;&#24773;&#20917;&#19979;&#30340;&#20449;&#24687;&#20934;&#21017;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;</title><link>https://arxiv.org/abs/2402.12762</link><description>&lt;p&gt;
&#22312;&#22855;&#24322;&#24615;&#19979;&#30340;&#23398;&#20064;&#65306;&#25913;&#36827;WBIC&#21644;sBIC&#30340;&#20449;&#24687;&#20934;&#21017;
&lt;/p&gt;
&lt;p&gt;
Learning under Singularity: An Information Criterion improving WBIC and sBIC
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12762
&lt;/p&gt;
&lt;p&gt;
LS&#20449;&#24687;&#20934;&#21017;&#26088;&#22312;&#22686;&#24378;WBIC&#21644;sBIC&#30340;&#21151;&#33021;&#65292;&#26377;&#25928;&#22788;&#29702;&#38750;&#27491;&#21017;&#24773;&#20917;&#65292;&#20855;&#26377;&#31283;&#23450;&#24615;&#65292;&#20026;&#22855;&#24322;&#24773;&#20917;&#19979;&#30340;&#20449;&#24687;&#20934;&#21017;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20449;&#24687;&#20934;&#21017;&#65288;IC&#65289;&#65292;&#31216;&#20026;&#22312;&#22855;&#24322;&#24615;&#19979;&#30340;&#23398;&#20064;&#65288;LS&#65289;&#65292;&#26088;&#22312;&#22686;&#24378;&#24191;&#27867;&#36866;&#29992;&#30340;&#36125;&#21494;&#26031;&#20449;&#24687;&#20934;&#21017;&#65288;WBIC&#65289;&#21644;&#22855;&#24322;&#36125;&#21494;&#26031;&#20449;&#24687;&#20934;&#21017;&#65288;sBIC&#65289;&#30340;&#21151;&#33021;&#12290; LS&#22312;&#27809;&#26377;&#27491;&#21017;&#24615;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#26159;&#26377;&#25928;&#30340;&#65292;&#24182;&#34920;&#29616;&#20986;&#31283;&#23450;&#24615;&#12290;Watanabe&#23450;&#20041;&#20102;&#19968;&#20010;&#32479;&#35745;&#27169;&#22411;&#25110;&#23398;&#20064;&#26426;&#22120;&#20026;&#27491;&#21017;&#65292;&#22914;&#26524;&#20174;&#21442;&#25968;&#21040;&#27010;&#29575;&#20998;&#24067;&#30340;&#26144;&#23556;&#26159;&#19968;&#23545;&#19968;&#30340;&#65292;&#24182;&#19988;&#20854;Fisher&#20449;&#24687;&#30697;&#38453;&#26159;&#27491;&#23450;&#30340;&#12290;&#30456;&#21453;&#65292;&#19981;&#31526;&#21512;&#36825;&#20123;&#26465;&#20214;&#30340;&#27169;&#22411;&#34987;&#31216;&#20026;&#22855;&#24322;&#12290; &#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#22855;&#24322;&#24773;&#20917;&#19979;&#30340;&#20449;&#24687;&#20934;&#21017;&#65292;&#21253;&#25324;WBIC&#21644;sBIC&#12290; WBIC&#36866;&#29992;&#20110;&#38750;&#27491;&#21017;&#24773;&#20917;&#65292;&#20294;&#22312;&#26679;&#26412;&#37327;&#24456;&#22823;&#19988;&#24050;&#30693;&#23398;&#20064;&#31995;&#25968;&#20272;&#35745;&#20887;&#20313;&#26102;&#38754;&#20020;&#25361;&#25112;&#12290; &#30456;&#21453;&#65292;sBIC&#22312;&#24191;&#27867;&#24212;&#29992;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#65292;&#22240;&#20026;&#23427;&#20381;&#36182;&#20110;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12762v1 Announce Type: cross  Abstract: We introduce a novel Information Criterion (IC), termed Learning under Singularity (LS), designed to enhance the functionality of the Widely Applicable Bayes Information Criterion (WBIC) and the Singular Bayesian Information Criterion (sBIC). LS is effective without regularity constraints and demonstrates stability. Watanabe defined a statistical model or a learning machine as regular if the mapping from a parameter to a probability distribution is one-to-one and its Fisher information matrix is positive definite. In contrast, models not meeting these conditions are termed singular. Over the past decade, several information criteria for singular cases have been proposed, including WBIC and sBIC. WBIC is applicable in non-regular scenarios but faces challenges with large sample sizes and redundant estimation of known learning coefficients. Conversely, sBIC is limited in its broader application due to its dependence on maximum likelihood
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#32852;&#37030;&#22270;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;FGAD&#65292;&#36890;&#36807;&#33258;&#25105;&#25552;&#21319;&#30340;&#30693;&#35782;&#33976;&#39311;&#35299;&#20915;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#38382;&#39064;&#21644;&#39640;&#36890;&#20449;&#25104;&#26412;&#65292;&#25512;&#21160;&#22270;&#24322;&#24120;&#26816;&#27979;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2402.12761</link><description>&lt;p&gt;
FGAD&#65306;&#33258;&#25105;&#25552;&#21319;&#30340;&#30693;&#35782;&#33976;&#39311;&#65292;&#29992;&#20110;&#26377;&#25928;&#30340;&#32852;&#37030;&#22270;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FGAD: Self-boosted Knowledge Distillation for An Effective Federated Graph Anomaly Detection Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12761
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#32852;&#37030;&#22270;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;FGAD&#65292;&#36890;&#36807;&#33258;&#25105;&#25552;&#21319;&#30340;&#30693;&#35782;&#33976;&#39311;&#35299;&#20915;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#38382;&#39064;&#21644;&#39640;&#36890;&#20449;&#25104;&#26412;&#65292;&#25512;&#21160;&#22270;&#24322;&#24120;&#26816;&#27979;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24322;&#24120;&#26816;&#27979; (GAD) &#26088;&#22312;&#35782;&#21035;&#26126;&#26174;&#20559;&#31163;&#20854;&#20182;&#22270;&#30340;&#24322;&#24120;&#22270;&#65292;&#30001;&#20110;&#22312;&#35768;&#22810;&#29616;&#23454;&#22330;&#26223;&#20013;&#23384;&#22312;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#24191;&#27867;&#23384;&#22312;&#21644;&#22797;&#26434;&#24615;&#65292;GAD&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;GAD&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#22312;&#38598;&#20013;&#35757;&#32451;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#22312;&#26576;&#20123;&#25935;&#24863;&#22330;&#21512;&#23384;&#22312;&#38544;&#31169;&#27844;&#38706;&#39118;&#38505;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#23547;&#27714;&#20849;&#21516;&#24320;&#21457;&#24378;&#22823;GAD&#27169;&#22411;&#30340;&#32452;&#32455;&#20043;&#38388;&#30340;&#21512;&#20316;&#12290;&#23613;&#31649;&#32852;&#37030;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#26222;&#36941;&#23384;&#22312;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#38382;&#39064;&#21644;&#39640;&#36890;&#20449;&#25104;&#26412;&#22312;&#20998;&#24067;&#22312;&#19981;&#21516;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#22270;&#25968;&#25454;&#21512;&#20316;&#20013;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#32852;&#37030;&#22270;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550; (FGAD)&#12290;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#19968;&#20010;&#24322;&#24120;&#29983;&#25104;&#22120;&#26469;&#25200;&#21160;&#27491;&#24120;&#22270;&#20197;&#25104;&#20026;&#24322;&#24120;&#22270;&#65292;&#24182;&#35757;&#32451;&#19968;&#20010;&#24378;&#22823;&#30340;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12761v1 Announce Type: new  Abstract: Graph anomaly detection (GAD) aims to identify anomalous graphs that significantly deviate from other ones, which has raised growing attention due to the broad existence and complexity of graph-structured data in many real-world scenarios. However, existing GAD methods usually execute with centralized training, which may lead to privacy leakage risk in some sensitive cases, thereby impeding collaboration among organizations seeking to collectively develop robust GAD models. Although federated learning offers a promising solution, the prevalent non-IID problems and high communication costs present significant challenges, particularly pronounced in collaborations with graph data distributed among different participants. To tackle these challenges, we propose an effective federated graph anomaly detection framework (FGAD). We first introduce an anomaly generator to perturb the normal graphs to be anomalous, and train a powerful anomaly dete
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#30001;&#20110;&#23460;&#20869;&#29615;&#22659;&#20013;&#30005;&#30913;&#24178;&#25200;&#30340;&#26102;&#21464;&#29305;&#24615;&#65292;&#38745;&#24577;&#25968;&#25454;&#24211;&#21644;&#21160;&#24577;&#25968;&#25454;&#24211;&#23545;&#22522;&#20110;Wi-Fi&#25351;&#32441;&#30340;&#23460;&#20869;&#23450;&#20301;&#30340;&#19981;&#21516;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.12756</link><description>&lt;p&gt;
&#23460;&#20869;&#23450;&#20301;&#22522;&#20110;Wi-Fi&#25351;&#32441;&#30340;&#38745;&#24577;&#25968;&#25454;&#24211;&#19982;&#21160;&#24577;&#25968;&#25454;&#24211;&#65306;&#22522;&#20110;&#25968;&#25454;&#35270;&#35282;&#30340;&#35752;&#35770;
&lt;/p&gt;
&lt;p&gt;
Static vs. Dynamic Databases for Indoor Localization based on Wi-Fi Fingerprinting: A Discussion from a Data Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12756
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#30001;&#20110;&#23460;&#20869;&#29615;&#22659;&#20013;&#30005;&#30913;&#24178;&#25200;&#30340;&#26102;&#21464;&#29305;&#24615;&#65292;&#38745;&#24577;&#25968;&#25454;&#24211;&#21644;&#21160;&#24577;&#25968;&#25454;&#24211;&#23545;&#22522;&#20110;Wi-Fi&#25351;&#32441;&#30340;&#23460;&#20869;&#23450;&#20301;&#30340;&#19981;&#21516;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Wi-Fi&#25351;&#32441;&#35782;&#21035;&#24050;&#25104;&#20026;&#23460;&#20869;&#23450;&#20301;&#26368;&#27969;&#34892;&#30340;&#26041;&#27861;&#12290;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#20351;&#29992;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;Wi-Fi&#25351;&#32441;&#35782;&#21035;&#30340;&#23450;&#20301;&#24615;&#33021;&#65292;&#20294;&#20854;&#25104;&#21151;&#21462;&#20915;&#20110;&#30001;&#22823;&#37327;RSSI&#12289;&#25509;&#20837;&#28857;&#30340;MAC&#22320;&#22336;&#21644;&#20854;&#20182;&#27979;&#37327;&#20449;&#24687;&#32452;&#25104;&#30340;&#25351;&#32441;&#25968;&#25454;&#24211;&#30340;&#21487;&#29992;&#24615;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#25351;&#32441;&#25968;&#25454;&#24211;&#26410;&#33021;&#24456;&#22909;&#22320;&#21453;&#26144;&#22797;&#26434;&#29616;&#20195;&#23460;&#20869;&#29615;&#22659;&#20013;&#30005;&#30913;&#24178;&#25200;&#30340;&#26102;&#21464;&#29305;&#24615;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#35757;&#32451;/&#39564;&#35777;&#21644;&#27979;&#35797;&#25968;&#25454;&#38598;&#30340;&#32479;&#35745;&#29305;&#24449;&#21457;&#29983;&#26174;&#33879;&#21464;&#21270;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#36890;&#24120;&#26159;&#22312;&#19981;&#21516;&#26102;&#38388;&#26500;&#24314;&#30340;&#65292;&#29978;&#33267;&#27979;&#35797;&#25968;&#25454;&#38598;&#30340;&#29305;&#24449;&#21487;&#33021;&#19982;&#22312;&#37096;&#32626;&#21518;&#23450;&#20301;&#31995;&#32479;&#36816;&#34892;&#26399;&#38388;&#30001;&#29992;&#25143;&#25552;&#20132;&#30340;&#25968;&#25454;&#30340;&#29305;&#24449;&#19981;&#21516;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#26102;&#38388;&#21464;&#21270;&#30340;Wi-Fi&#25351;&#32441;&#23545;&#23450;&#20301;&#31995;&#32479;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12756v1 Announce Type: new  Abstract: Wi-Fi fingerprinting has emerged as the most popular approach to indoor localization. The use of ML algorithms has greatly improved the localization performance of Wi-Fi fingerprinting, but its success depends on the availability of fingerprint databases composed of a large number of RSSIs, the MAC addresses of access points, and the other measurement information. However, most fingerprint databases do not reflect well the time varying nature of electromagnetic interferences in complicated modern indoor environment. This could result in significant changes in statistical characteristics of training/validation and testing datasets, which are often constructed at different times, and even the characteristics of the testing datasets could be different from those of the data submitted by users during the operation of localization systems after their deployment. In this paper, we consider the implications of time-varying Wi-Fi fingerprints on
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#21644;&#22810;&#32423;&#29305;&#24449;&#34701;&#21512;&#30340;&#39640;&#32423;&#25345;&#32493;&#24615;&#23041;&#32961;&#34892;&#20026;&#32773;&#24402;&#22240;&#26041;&#27861;&#65292;&#21033;&#29992;&#24322;&#26500;&#23646;&#24615;&#22270;&#21644;&#22810;&#27169;&#24577;&#29305;&#24449;&#25552;&#21462;&#26469;&#22686;&#24378;&#32593;&#32476;&#23041;&#32961;&#24773;&#25253;&#20013;&#30340;&#23041;&#32961;&#34892;&#20026;&#32773;&#24402;&#22240;&#33021;&#21147;</title><link>https://arxiv.org/abs/2402.12743</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#27169;&#24577;&#21644;&#22810;&#32423;&#29305;&#24449;&#34701;&#21512;&#30340;&#39640;&#32423;&#25345;&#20037;&#24615;&#23041;&#32961;&#34892;&#20026;&#32773;&#24402;&#22240;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
APT-MMF: An advanced persistent threat actor attribution method based on multimodal and multilevel feature fusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12743
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#21644;&#22810;&#32423;&#29305;&#24449;&#34701;&#21512;&#30340;&#39640;&#32423;&#25345;&#32493;&#24615;&#23041;&#32961;&#34892;&#20026;&#32773;&#24402;&#22240;&#26041;&#27861;&#65292;&#21033;&#29992;&#24322;&#26500;&#23646;&#24615;&#22270;&#21644;&#22810;&#27169;&#24577;&#29305;&#24449;&#25552;&#21462;&#26469;&#22686;&#24378;&#32593;&#32476;&#23041;&#32961;&#24773;&#25253;&#20013;&#30340;&#23041;&#32961;&#34892;&#20026;&#32773;&#24402;&#22240;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23041;&#32961;&#34892;&#20026;&#32773;&#24402;&#22240;&#26159;&#23545;&#25239;&#39640;&#32423;&#25345;&#20037;&#24615;&#23041;&#32961;&#65288;APTs&#65289;&#30340;&#37325;&#35201;&#38450;&#24481;&#31574;&#30053;&#12290;&#32593;&#32476;&#23041;&#32961;&#24773;&#25253;&#65288;CTI&#65289;&#22312;APTs&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#28041;&#21450;&#23545;&#22810;&#28304;&#24322;&#26500;&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#12290;&#24403;&#21069;&#30340;&#24402;&#22240;&#26041;&#27861;&#20174;&#19981;&#21516;&#30340;CTI&#35270;&#35282;&#25552;&#21462;&#29305;&#24449;&#65292;&#24182;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26681;&#25454;&#23041;&#32961;&#34892;&#20026;&#32773;&#23545;CTI&#25253;&#21578;&#36827;&#34892;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#21482;&#25552;&#21462;&#19968;&#31181;&#29305;&#24449;&#65292;&#24182;&#24573;&#30053;&#24322;&#26500;&#20449;&#24687;&#65292;&#23588;&#20854;&#26159;&#25351;&#31034;&#22120;&#23041;&#32961;&#30340;&#23646;&#24615;&#21644;&#20851;&#31995;&#65292;&#36825;&#26500;&#25104;&#20102;CTI&#30340;&#26680;&#24515;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#21644;&#22810;&#32423;&#29305;&#24449;&#34701;&#21512;&#65288;APT-MMF&#65289;&#30340;APT&#34892;&#20026;&#32773;&#24402;&#22240;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21033;&#29992;&#24322;&#26500;&#23646;&#24615;&#22270;&#26469;&#34920;&#24449;APT&#25253;&#21578;&#21450;&#20854;IOCs&#20449;&#24687;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#21462;&#24182;&#34701;&#21512;&#22810;&#27169;&#24577;&#29305;&#24449;&#65292;&#21253;&#25324;&#23646;&#24615;&#31867;&#22411;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12743v1 Announce Type: cross  Abstract: Threat actor attribution is a crucial defense strategy for combating advanced persistent threats (APTs). Cyber threat intelligence (CTI), which involves analyzing multisource heterogeneous data from APTs, plays an important role in APT actor attribution. The current attribution methods extract features from different CTI perspectives and employ machine learning models to classify CTI reports according to their threat actors. However, these methods usually extract only one kind of feature and ignore heterogeneous information, especially the attributes and relations of indicators of compromise (IOCs), which form the core of CTI. To address these problems, we propose an APT actor attribution method based on multimodal and multilevel feature fusion (APT-MMF). First, we leverage a heterogeneous attributed graph to characterize APT reports and their IOC information. Then, we extract and fuse multimodal features, including attribute type feat
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38170;&#28857;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#35782;&#21035;&#20986;&#23616;&#37096;&#35299;&#37322;&#34987;&#26126;&#30830;&#20445;&#35777;&#27491;&#30830;&#30340;&#21306;&#22495;&#65292;&#20135;&#29983;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#23545;&#40784;&#30418;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#22522;&#32447;&#65292;&#33021;&#25214;&#21040;&#20855;&#26377;&#26356;&#22823;&#20445;&#35777;&#21306;&#22495;&#30340;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2402.12737</link><description>&lt;p&gt;
&#26412;&#22320;&#35299;&#37322;&#30340;&#20445;&#35777;&#21306;&#22495;
&lt;/p&gt;
&lt;p&gt;
Guarantee Regions for Local Explanations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12737
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38170;&#28857;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#35782;&#21035;&#20986;&#23616;&#37096;&#35299;&#37322;&#34987;&#26126;&#30830;&#20445;&#35777;&#27491;&#30830;&#30340;&#21306;&#22495;&#65292;&#20135;&#29983;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#23545;&#40784;&#30418;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#22522;&#32447;&#65292;&#33021;&#25214;&#21040;&#20855;&#26377;&#26356;&#22823;&#20445;&#35777;&#21306;&#22495;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#23616;&#37096;&#26367;&#20195;&#27169;&#22411;&#65288;&#22914;LIME&#65289;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#38750;&#24120;&#25797;&#38271;&#25551;&#36848;&#22312;&#24863;&#20852;&#36259;&#28857;&#30340;&#39044;&#27979;&#27169;&#22411;&#34892;&#20026;&#65292;&#20294;&#23427;&#20204;&#19981;&#33021;&#20445;&#35777;&#23545;&#21608;&#22260;&#23616;&#37096;&#21306;&#22495;&#36827;&#34892;&#22806;&#25512;&#12290;&#28982;&#32780;&#65292;&#23545;&#39044;&#27979;&#27169;&#22411;&#30340;&#23616;&#37096;&#26354;&#29575;&#36807;&#25311;&#21512;&#21644;&#24694;&#24847;&#31713;&#25913;&#21487;&#20197;&#26174;&#33879;&#38480;&#21046;&#22806;&#25512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38170;&#28857;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#23616;&#37096;&#35299;&#37322;&#34987;&#26126;&#30830;&#20445;&#35777;&#27491;&#30830;&#30340;&#21306;&#22495;&#65292;&#20174;&#32780;&#26126;&#30830;&#25551;&#36848;&#37027;&#20123;&#21487;&#20197;&#20449;&#20219;&#36755;&#20837;&#29305;&#24449;&#30340;&#38388;&#38548;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20135;&#29983;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#23545;&#40784;&#30418;&#65292;&#20854;&#20013;&#23616;&#37096;&#26367;&#20195;&#27169;&#22411;&#30340;&#39044;&#27979;&#20445;&#35777;&#19982;&#39044;&#27979;&#27169;&#22411;&#21305;&#37197;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#29992;&#20110;&#25214;&#21040;&#20855;&#26377;&#26356;&#22823;&#20445;&#35777;&#21306;&#22495;&#30340;&#35299;&#37322;&#65292;&#27604;&#29616;&#26377;&#22522;&#32447;&#26356;&#22909;&#22320;&#35206;&#30422;&#25968;&#25454;&#27969;&#24418;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22914;&#20309;&#35782;&#21035;&#24341;&#23548;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12737v1 Announce Type: new  Abstract: Interpretability methods that utilise local surrogate models (e.g. LIME) are very good at describing the behaviour of the predictive model at a point of interest, but they are not guaranteed to extrapolate to the local region surrounding the point. However, overfitting to the local curvature of the predictive model and malicious tampering can significantly limit extrapolation. We propose an anchor-based algorithm for identifying regions in which local explanations are guaranteed to be correct by explicitly describing those intervals along which the input features can be trusted. Our method produces an interpretable feature-aligned box where the prediction of the local surrogate model is guaranteed to match the predictive model. We demonstrate that our algorithm can be used to find explanations with larger guarantee regions that better cover the data manifold compared to existing baselines. We also show how our method can identify mislead
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#26426;&#22120;&#32763;&#35793;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#26412;&#25991;&#24320;&#21457;&#20102;&#29992;&#20110;&#38750;&#27954;&#21644;&#20122;&#27954;&#35821;&#35328;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;&#20219;&#21153;&#30340;&#20004;&#31181;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#27604;&#37096;&#20998;&#23448;&#26041;&#22522;&#20934;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.12730</link><description>&lt;p&gt;
UMBCLU&#22312;SemEval-2024&#20219;&#21153;1A&#21644;1C&#20013;&#30340;&#34920;&#29616;&#65306;&#24102;&#26377;&#21644;&#19981;&#24102;&#26377;&#26426;&#22120;&#32763;&#35793;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
UMBCLU at SemEval-2024 Task 1A and 1C: Semantic Textual Relatedness with and without machine translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12730
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#32763;&#35793;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#26412;&#25991;&#24320;&#21457;&#20102;&#29992;&#20110;&#38750;&#27954;&#21644;&#20122;&#27954;&#35821;&#35328;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;&#20219;&#21153;&#30340;&#20004;&#31181;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#27604;&#37096;&#20998;&#23448;&#26041;&#22522;&#20934;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25551;&#36848;&#20102;&#25105;&#20204;&#20026;SemEval-2024&#20219;&#21153;1&#24320;&#21457;&#30340;&#31995;&#32479;&#65292;&#8220;&#38750;&#27954;&#21644;&#20122;&#27954;&#35821;&#35328;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;&#8221;&#12290; &#35813;&#20219;&#21153;&#30340;&#30446;&#26631;&#26159;&#26500;&#24314;&#19968;&#20010;&#33021;&#22815;&#35782;&#21035;&#30446;&#26631;&#35821;&#35328;&#20013;&#23646;&#20110;&#38750;&#27954;&#21644;&#20122;&#27954;&#35821;&#35328;&#38598;&#21512;&#30340;&#20004;&#20010;&#21477;&#23376;&#20043;&#38388;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;&#65288;STR&#65289;&#30340;&#27169;&#22411;&#12290; &#25105;&#20204;&#21442;&#19982;&#20102;&#23376;&#20219;&#21153;A&#21644;C&#65292;&#24182;&#25506;&#32034;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#30417;&#30563;&#21644;&#36328;&#35821;&#35328;&#35757;&#32451;&#12290; &#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#34987;&#24191;&#27867;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;&#21644;&#35821;&#20041;&#30456;&#20284;&#24615;&#12290; &#20351;&#29992;&#26426;&#22120;&#32763;&#35793;&#21644;&#21477;&#23376;&#23884;&#20837;LLMs&#30340;&#32452;&#21512;&#65292;&#25105;&#20204;&#20026;&#23376;&#20219;&#21153;A&#24320;&#21457;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;STR&#27169;&#22411;&#65292;TranSem&#65292;&#24182;&#23545;STR&#25968;&#25454;&#19978;&#30340;T5&#31995;&#21015;&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#29992;&#20110;&#23376;&#20219;&#21153;C&#30340;FineSem&#12290; &#25105;&#20204;&#22312;&#23376;&#20219;&#21153;A&#20013;7&#31181;&#35821;&#35328;&#30340;&#27169;&#22411;&#32467;&#26524;&#27604;3&#31181;&#35821;&#35328;&#30340;&#23448;&#26041;&#22522;&#20934;&#26356;&#22909;&#65292;&#32780;&#19982;&#20854;&#20182;4&#31181;&#35821;&#35328;&#30340;&#22522;&#20934;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12730v1 Announce Type: cross  Abstract: This paper describes the system we developed for SemEval-2024 Task 1, "Semantic Textual Relatedness for African and Asian Languages." The aim of the task is to build a model that can identify semantic textual relatedness (STR) between two sentences of a target language belonging to a collection of African and Asian languages. We participated in Subtasks A and C and explored supervised and cross-lingual training leveraging large language models (LLMs). Pre-trained large language models have been extensively used for machine translation and semantic similarity. Using a combination of machine translation and sentence embedding LLMs, we developed a unified STR model, TranSem, for subtask A and fine-tuned the T5 family of models on the STR data, FineSem, for use in subtask C. Our model results for 7 languages in subtask A were better than the official baseline for 3 languages and on par with the baseline for the remaining 4 languages. Our m
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GTNP&#30340;&#31070;&#32463;&#36807;&#31243;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#29305;&#24449;&#20256;&#36755;&#31574;&#30053;&#24357;&#21512;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#30340;&#25968;&#25454;&#20998;&#24067;&#24046;&#24322;&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#31232;&#32570;&#21644;&#32570;&#20047;&#21487;&#38752;&#24615;&#20998;&#26512;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2402.12729</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#21487;&#38752;&#30340;&#22810;&#23610;&#24230;&#31070;&#32463;&#36807;&#31243;&#23884;&#20837;&#30693;&#35782;&#29992;&#20110;&#26234;&#33021;&#25925;&#38556;&#26816;&#27979;&#30340;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Scalable and reliable deep transfer learning for intelligent fault detection via multi-scale neural processes embedded with knowledge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12729
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GTNP&#30340;&#31070;&#32463;&#36807;&#31243;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#29305;&#24449;&#20256;&#36755;&#31574;&#30053;&#24357;&#21512;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#30340;&#25968;&#25454;&#20998;&#24067;&#24046;&#24322;&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#31232;&#32570;&#21644;&#32570;&#20047;&#21487;&#38752;&#24615;&#20998;&#26512;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#65288;DTL&#65289;&#26159;&#26234;&#33021;&#25925;&#38556;&#26816;&#27979;&#65288;IFD&#65289;&#39046;&#22495;&#20013;&#30340;&#19968;&#31181;&#22522;&#26412;&#26041;&#27861;&#65292;&#26088;&#22312;&#20943;&#36731;&#35757;&#32451;&#38598;&#65288;&#28304;&#22495;&#65289;&#21644;&#27979;&#35797;&#38598;&#65288;&#30446;&#26631;&#22495;&#65289;&#20043;&#38388;&#25968;&#25454;&#20998;&#24067;&#19981;&#19968;&#33268;&#23548;&#33268;&#26041;&#27861;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22522;&#20110;&#31070;&#32463;&#36807;&#31243;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GTNP&#65289;&#30340;&#26032;&#39062;&#30340;DTL&#26041;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#25968;&#25454;&#20998;&#24067;&#19981;&#19968;&#33268;&#21644;&#21487;&#38752;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12729v1 Announce Type: cross  Abstract: Deep transfer learning (DTL) is a fundamental method in the field of Intelligent Fault Detection (IFD). It aims to mitigate the degradation of method performance that arises from the discrepancies in data distribution between training set (source domain) and testing set (target domain). Considering the fact that fault data collection is challenging and certain faults are scarce, DTL-based methods face the limitation of available observable data, which reduces the detection performance of the methods in the target domain. Furthermore, DTL-based methods lack comprehensive uncertainty analysis that is essential for building reliable IFD systems. To address the aforementioned problems, this paper proposes a novel DTL-based method known as Neural Processes-based deep transfer learning with graph convolution network (GTNP). Feature-based transfer strategy of GTNP bridges the data distribution discrepancies of source domain and target domain 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#24577;&#24863;&#30693;&#30340;LLM&#38598;&#25104;&#26041;&#27861;&#65288;MAIL&#65289;&#29992;&#20110;&#38024;&#23545;KVQA&#65292;&#36890;&#36807;&#32454;&#33268;&#22320;&#21033;&#29992;&#22810;&#27169;&#24577;&#30693;&#35782;&#26469;&#22788;&#29702;&#22270;&#20687;&#29702;&#35299;&#21644;&#30693;&#35782;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.12728</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#24577;&#24863;&#30693;&#38598;&#25104;&#29992;&#20110;&#22522;&#20110;&#30693;&#35782;&#30340;&#35270;&#35273;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Modality-Aware Integration with Large Language Models for Knowledge-based Visual Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12728
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#24577;&#24863;&#30693;&#30340;LLM&#38598;&#25104;&#26041;&#27861;&#65288;MAIL&#65289;&#29992;&#20110;&#38024;&#23545;KVQA&#65292;&#36890;&#36807;&#32454;&#33268;&#22320;&#21033;&#29992;&#22810;&#27169;&#24577;&#30693;&#35782;&#26469;&#22788;&#29702;&#22270;&#20687;&#29702;&#35299;&#21644;&#30693;&#35782;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#39537;&#21160;&#30340;&#35270;&#35273;&#38382;&#31572;&#65288;KVQA&#65289;&#24050;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#20197;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#22914;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#26469;&#22238;&#31572;&#35270;&#35273;&#38382;&#39064;&#12290;&#23613;&#31649;&#24050;&#25552;&#20986;&#20960;&#31181;&#23581;&#35797;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#38544;&#21547;&#30693;&#35782;&#28304;&#65292;&#20294;&#30001;&#20110;LLMs&#21487;&#33021;&#29983;&#25104;&#24187;&#35273;&#65292;&#22240;&#27492;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;&#22810;&#31181;&#30693;&#35782;&#26469;&#28304;&#65292;&#20363;&#22914;&#22270;&#20687;&#12289;&#30693;&#35782;&#22270;&#35889;&#21644;LLMs&#65292;&#19981;&#33021;&#36731;&#26131;&#23545;&#40784;&#20197;&#24212;&#23545;&#22797;&#26434;&#22330;&#26223;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;KVQA&#30340;&#26032;&#39062;&#30340;&#20855;&#26377;&#27169;&#24577;&#24863;&#30693;&#30340;LLM&#38598;&#25104;&#26041;&#27861;&#65288;MAIL&#65289;&#12290;&#23427;&#31934;&#24515;&#21033;&#29992;&#22810;&#27169;&#24577;&#30693;&#35782;&#36827;&#34892;&#22270;&#20687;&#29702;&#35299;&#21644;&#30693;&#35782;&#25512;&#29702;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#65288;i&#65289;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;LLMs&#30340;&#20004;&#38454;&#27573;&#25552;&#31034;&#31574;&#30053;&#65292;&#23558;&#22270;&#20687;&#23494;&#38598;&#22320;&#34701;&#20837;&#24102;&#26377;&#35814;&#32454;&#35270;&#35273;&#29305;&#24449;&#30340;&#22330;&#26223;&#22270;&#20013;&#65307;&#65288;ii&#65289;&#25105;&#20204;&#36890;&#36807;&#23558;&#25552;&#21040;&#30340;&#23454;&#20307;&#19982;&#22806;&#37096;&#20107;&#23454;&#32852;&#31995;&#36215;&#26469;&#26500;&#24314;&#19968;&#20010;&#32806;&#21512;&#30340;&#27010;&#24565;&#22270;&#65307;&#65288;iii&#65289;&#35774;&#35745;&#20102;&#19968;&#20010;&#23450;&#21046;&#30340;&#20266;&#23402;&#29983;&#22270;&#20013;&#20171;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12728v1 Announce Type: cross  Abstract: Knowledge-based visual question answering (KVQA) has been extensively studied to answer visual questions with external knowledge, e.g., knowledge graphs (KGs). While several attempts have been proposed to leverage large language models (LLMs) as an implicit knowledge source, it remains challenging since LLMs may generate hallucinations. Moreover, multiple knowledge sources, e.g., images, KGs and LLMs, cannot be readily aligned for complex scenarios. To tackle these, we present a novel modality-aware integration with LLMs for KVQA (MAIL). It carefully leverages multimodal knowledge for both image understanding and knowledge reasoning. Specifically, (i) we propose a two-stage prompting strategy with LLMs to densely embody the image into a scene graph with detailed visual features; (ii) We construct a coupled concept graph by linking the mentioned entities with external facts. (iii) A tailored pseudo-siamese graph medium fusion is designe
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#35777;&#26126;&#20102;&#21518;&#39564;&#25277;&#26679;&#22312;&#35745;&#31639;&#19978;&#26159;&#38590;&#20197;&#35299;&#20915;&#30340;&#65306;&#22312;&#21152;&#23494;&#23398;&#20013;&#26368;&#22522;&#26412;&#30340;&#20551;&#35774;&#19979;&#8212;&#8212;&#21333;&#21521;&#20989;&#25968;&#23384;&#22312;&#30340;&#20551;&#35774;&#19979;&#65292;&#23384;&#22312;&#19968;&#20123;&#23454;&#20363;&#65292;&#23545;&#20110;&#36825;&#20123;&#23454;&#20363;&#65292;&#27599;&#20010;&#31639;&#27861;&#37117;&#38656;&#35201;&#36229;&#22810;&#39033;&#24335;&#26102;&#38388;&#65292;&#21363;&#20351;&#26080;&#26465;&#20214;&#25277;&#26679;&#21487;&#20197;&#35777;&#26126;&#26159;&#24555;&#36895;&#30340;&#12290;</title><link>https://arxiv.org/abs/2402.12727</link><description>&lt;p&gt;
&#25193;&#25955;&#21518;&#39564;&#25277;&#26679;&#22312;&#35745;&#31639;&#19978;&#26159;&#38590;&#20197;&#35299;&#20915;&#30340;
&lt;/p&gt;
&lt;p&gt;
Diffusion Posterior Sampling is Computationally Intractable
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12727
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35777;&#26126;&#20102;&#21518;&#39564;&#25277;&#26679;&#22312;&#35745;&#31639;&#19978;&#26159;&#38590;&#20197;&#35299;&#20915;&#30340;&#65306;&#22312;&#21152;&#23494;&#23398;&#20013;&#26368;&#22522;&#26412;&#30340;&#20551;&#35774;&#19979;&#8212;&#8212;&#21333;&#21521;&#20989;&#25968;&#23384;&#22312;&#30340;&#20551;&#35774;&#19979;&#65292;&#23384;&#22312;&#19968;&#20123;&#23454;&#20363;&#65292;&#23545;&#20110;&#36825;&#20123;&#23454;&#20363;&#65292;&#27599;&#20010;&#31639;&#27861;&#37117;&#38656;&#35201;&#36229;&#22810;&#39033;&#24335;&#26102;&#38388;&#65292;&#21363;&#20351;&#26080;&#26465;&#20214;&#25277;&#26679;&#21487;&#20197;&#35777;&#26126;&#26159;&#24555;&#36895;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#23398;&#20064;&#21644;&#20174;&#20998;&#24067;$p(x)$&#20013;&#25277;&#26679;&#30340;&#19968;&#31181;&#38750;&#24120;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#22312;&#21518;&#39564;&#25277;&#26679;&#20013;&#65292;&#20154;&#20204;&#36824;&#20250;&#32473;&#20986;&#19968;&#20010;&#27979;&#37327;&#27169;&#22411;$p(y \mid x)$&#21644;&#19968;&#20010;&#27979;&#37327;$y$&#65292;&#24076;&#26395;&#20174;$p(x \mid y)$&#20013;&#25277;&#26679;&#12290;&#21518;&#39564;&#25277;&#26679;&#23545;&#20110;&#35832;&#22914;&#20462;&#34917;&#12289;&#36229;&#20998;&#36776;&#29575;&#21644;MRI&#37325;&#24314;&#31561;&#20219;&#21153;&#38750;&#24120;&#26377;&#29992;&#65292;&#22240;&#27492;&#19968;&#20123;&#26368;&#36817;&#30340;&#24037;&#20316;&#24050;&#32463;&#32473;&#20986;&#20102;&#21551;&#21457;&#24335;&#36817;&#20284;&#31639;&#27861;&#65307;&#20294;&#27809;&#26377;&#19968;&#20010;&#24050;&#30693;&#33021;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#25910;&#25947;&#21040;&#27491;&#30830;&#30340;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12727v1 Announce Type: cross  Abstract: Diffusion models are a remarkably effective way of learning and sampling from a distribution $p(x)$. In posterior sampling, one is also given a measurement model $p(y \mid x)$ and a measurement $y$, and would like to sample from $p(x \mid y)$. Posterior sampling is useful for tasks such as inpainting, super-resolution, and MRI reconstruction, so a number of recent works have given algorithms to heuristically approximate it; but none are known to converge to the correct distribution in polynomial time.   In this paper we show that posterior sampling is \emph{computationally intractable}: under the most basic assumption in cryptography -- that one-way functions exist -- there are instances for which \emph{every} algorithm takes superpolynomial time, even though \emph{unconditional} sampling is provably fast. We also show that the exponential-time rejection sampling algorithm is essentially optimal under the stronger plausible assumption 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32467;&#26500;&#30693;&#35782;&#25351;&#23548;&#30340;&#25345;&#32493;&#23398;&#20064;&#65288;SKI-CL&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#19981;&#21516;&#21046;&#24230;&#19979;&#25345;&#32493;&#31215;&#32047;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#65288;MTS&#65289;&#39044;&#27979;&#20013;&#65292;&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#21464;&#37327;&#20381;&#36182;&#20851;&#31995;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.12722</link><description>&lt;p&gt;
&#32467;&#26500;&#30693;&#35782;&#25351;&#23548;&#30340;&#25345;&#32493;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Structural Knowledge Informed Continual Multivariate Time Series Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12722
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32467;&#26500;&#30693;&#35782;&#25351;&#23548;&#30340;&#25345;&#32493;&#23398;&#20064;&#65288;SKI-CL&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#19981;&#21516;&#21046;&#24230;&#19979;&#25345;&#32493;&#31215;&#32047;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#65288;MTS&#65289;&#39044;&#27979;&#20013;&#65292;&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#21464;&#37327;&#20381;&#36182;&#20851;&#31995;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#65288;MTS&#65289;&#39044;&#27979;&#20013;&#65292;&#26126;&#30830;&#24314;&#27169;&#19981;&#21516;&#26102;&#38388;&#24207;&#21015;&#20043;&#38388;&#30340;&#38544;&#34255;&#20381;&#36182;&#20851;&#31995;&#21487;&#20197;&#20135;&#29983;&#26377;&#21069;&#36884;&#30340;&#39044;&#27979;&#24615;&#33021;&#21644;&#21487;&#38752;&#30340;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#24403;MTS&#22312;&#19981;&#21516;&#30340;&#21046;&#24230;&#65288;&#38454;&#27573;&#65289;&#19979;&#25345;&#32493;&#31215;&#32047;&#26102;&#65292;&#24314;&#27169;&#21464;&#37327;&#20043;&#38388;&#30340;&#20381;&#36182;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#30001;&#20110;&#28508;&#22312;&#30340;&#20998;&#24067;&#21644;&#20381;&#36182;&#20851;&#31995;&#24046;&#24322;&#65292;&#22522;&#30784;&#27169;&#22411;&#21487;&#33021;&#20250;&#36935;&#21040;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#21363;&#22312;&#20445;&#25345;&#39044;&#27979;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#35760;&#24518;&#21644;&#25512;&#26029;&#19981;&#21516;&#31867;&#22411;&#30340;&#21464;&#37327;&#20381;&#36182;&#20851;&#31995;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32467;&#26500;&#30693;&#35782;&#25351;&#23548;&#30340;&#25345;&#32493;&#23398;&#20064;&#65288;SKI-CL&#65289;&#26694;&#26550;&#65292;&#20197;&#22312;&#25345;&#32493;&#23398;&#20064;&#33539;&#24335;&#20013;&#25191;&#34892;MTS&#39044;&#27979;&#65292;&#21033;&#29992;&#32467;&#26500;&#30693;&#35782;&#24341;&#23548;&#39044;&#27979;&#27169;&#22411;&#35782;&#21035;&#21644;&#36866;&#24212;&#19981;&#21516;&#21046;&#24230;&#65292;&#24182;&#36873;&#25321;repr&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12722v1 Announce Type: new  Abstract: Recent studies in multivariate time series (MTS) forecasting reveal that explicitly modeling the hidden dependencies among different time series can yield promising forecasting performance and reliable explanations. However, modeling variable dependencies remains underexplored when MTS is continuously accumulated under different regimes (stages). Due to the potential distribution and dependency disparities, the underlying model may encounter the catastrophic forgetting problem, i.e., it is challenging to memorize and infer different types of variable dependencies across different regimes while maintaining forecasting performance. To address this issue, we propose a novel Structural Knowledge Informed Continual Learning (SKI-CL) framework to perform MTS forecasting within a continual learning paradigm, which leverages structural knowledge to steer the forecasting model toward identifying and adapting to different regimes, and selects repr
&lt;/p&gt;</description></item><item><title>&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#23545;&#36755;&#20837;&#20013;&#20559;&#35265;&#29305;&#24449;&#19982;&#26631;&#31614;&#20043;&#38388;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#25935;&#24863;&#65292;&#26412;&#25991;&#22238;&#39038;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#26368;&#26032;&#26041;&#27861;&#65292;&#21516;&#26102;&#24635;&#32467;&#20102;&#25968;&#25454;&#38598;&#12289;&#22522;&#20934;&#21644;&#24230;&#37327;&#26631;&#20934;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.12715</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Spurious Correlations in Machine Learning: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12715
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#23545;&#36755;&#20837;&#20013;&#20559;&#35265;&#29305;&#24449;&#19982;&#26631;&#31614;&#20043;&#38388;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#25935;&#24863;&#65292;&#26412;&#25991;&#22238;&#39038;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#26368;&#26032;&#26041;&#27861;&#65292;&#21516;&#26102;&#24635;&#32467;&#20102;&#25968;&#25454;&#38598;&#12289;&#22522;&#20934;&#21644;&#24230;&#37327;&#26631;&#20934;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#23545;&#36755;&#20837;&#20013;&#20559;&#35265;&#29305;&#24449;&#65288;&#20363;&#22914;&#32972;&#26223;&#12289;&#32441;&#29702;&#21644;&#27425;&#35201;&#23545;&#35937;&#65289;&#19982;&#30456;&#24212;&#26631;&#31614;&#20043;&#38388;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#25935;&#24863;&#12290;&#36825;&#20123;&#29305;&#24449;&#21450;&#20854;&#19982;&#26631;&#31614;&#30340;&#30456;&#20851;&#24615;&#34987;&#31216;&#20026;&#8220;&#34394;&#20551;&#8221;&#65292;&#22240;&#20026;&#23427;&#20204;&#24448;&#24448;&#38543;&#30528;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#20998;&#24067;&#30340;&#21464;&#21270;&#32780;&#25913;&#21464;&#65292;&#36825;&#21487;&#33021;&#23545;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#23457;&#26597;&#20102;&#36825;&#19968;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#20851;&#20110;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#20998;&#31867;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#29616;&#26377;&#30340;&#25968;&#25454;&#38598;&#12289;&#22522;&#20934;&#21644;&#24230;&#37327;&#26631;&#20934;&#65292;&#20197;&#24110;&#21161;&#26410;&#26469;&#30340;&#30740;&#31350;&#12290;&#26412;&#25991;&#26368;&#21518;&#35752;&#35770;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#26410;&#26469;&#30740;&#31350;&#25361;&#25112;&#65292;&#26088;&#22312;&#20026;&#30456;&#20851;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12715v1 Announce Type: new  Abstract: Machine learning systems are known to be sensitive to spurious correlations between biased features of the inputs (e.g., background, texture, and secondary objects) and the corresponding labels. These features and their correlations with the labels are known as "spurious" because they tend to change with shifts in real-world data distributions, which can negatively impact the model's generalization and robustness. In this survey, we provide a comprehensive review of this issue, along with a taxonomy of current state-of-the-art methods for addressing spurious correlations in machine learning models. Additionally, we summarize existing datasets, benchmarks, and metrics to aid future research. The paper concludes with a discussion of the recent advancements and future research challenges in this field, aiming to provide valuable insights for researchers in the related domains.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20855;&#26377;&#31561;&#21464;&#24615;&#30340;&#39044;&#35757;&#32451;Transformer(EPT)&#26694;&#26550;&#65292;&#33021;&#22815;&#32479;&#19968;&#22810;&#39046;&#22495;&#20998;&#23376;&#30340;&#20960;&#20309;&#23398;&#20064;&#65292;&#36890;&#36807;&#22359;&#22686;&#24378;&#34920;&#31034;&#21644;E(3)&#31561;&#21464;&#24615;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;3D&#32467;&#26500;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.12714</link><description>&lt;p&gt;
&#20855;&#26377;&#31561;&#21464;&#24615;&#30340;&#39044;&#35757;&#32451;Transformer&#29992;&#20110;&#22810;&#22495;3D&#20998;&#23376;&#30340;&#32479;&#19968;&#20960;&#20309;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Equivariant Pretrained Transformer for Unified Geometric Learning on Multi-Domain 3D Molecules
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12714
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20855;&#26377;&#31561;&#21464;&#24615;&#30340;&#39044;&#35757;&#32451;Transformer(EPT)&#26694;&#26550;&#65292;&#33021;&#22815;&#32479;&#19968;&#22810;&#39046;&#22495;&#20998;&#23376;&#30340;&#20960;&#20309;&#23398;&#20064;&#65292;&#36890;&#36807;&#22359;&#22686;&#24378;&#34920;&#31034;&#21644;E(3)&#31561;&#21464;&#24615;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;3D&#32467;&#26500;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;3D&#20998;&#23376;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#24050;&#32463;&#23637;&#31034;&#20986;&#22312;&#21508;&#31181;&#31185;&#23398;&#24212;&#29992;&#20013;&#20855;&#26377;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#21162;&#21147;&#36890;&#24120;&#38598;&#20013;&#22312;&#29305;&#23450;&#39046;&#22495;&#65288;&#34507;&#30333;&#36136;&#25110;&#23567;&#20998;&#23376;&#65289;&#30340;&#27169;&#22411;&#39044;&#35757;&#32451;&#19978;&#65292;&#38169;&#22833;&#20102;&#21033;&#29992;&#36328;&#39046;&#22495;&#30693;&#35782;&#30340;&#26426;&#20250;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31561;&#21464;&#39044;&#35757;&#32451;Transformer&#65288;EPT&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#26088;&#22312;&#21327;&#35843;&#23567;&#20998;&#23376;&#21644;&#34507;&#30333;&#36136;&#30340;&#20960;&#20309;&#23398;&#20064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;EPT&#36890;&#36807;&#22359;&#22686;&#24378;&#34920;&#31034;&#32479;&#19968;&#20102;&#22810;&#39046;&#22495;&#20998;&#23376;&#30340;&#20960;&#20309;&#24314;&#27169;&#65292;&#33021;&#22815;&#20851;&#27880;&#27599;&#20010;&#21407;&#23376;&#26356;&#24191;&#27867;&#30340;&#19978;&#19979;&#25991;&#12290;&#22312;Transformer&#26694;&#26550;&#19978;&#65292;EPT&#36827;&#19968;&#27493;&#36890;&#36807;E(3)&#31561;&#21464;&#24615;&#36827;&#34892;&#22686;&#24378;&#65292;&#20197;&#20419;&#36827;&#20934;&#30830;&#34920;&#31034;3D&#32467;&#26500;&#12290;EPT&#30340;&#21478;&#19968;&#20010;&#20851;&#38190;&#21019;&#26032;&#26159;&#20854;&#22359;&#32423;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#36825;&#20801;&#35768;&#22312;&#21253;&#21547;&#23567;&#20998;&#23376;&#21644;&#34507;&#30333;&#36136;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#32852;&#21512;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12714v1 Announce Type: new  Abstract: Pretraining on a large number of unlabeled 3D molecules has showcased superiority in various scientific applications. However, prior efforts typically focus on pretraining models on a specific domain, either proteins or small molecules, missing the opportunity to leverage the cross-domain knowledge. To mitigate this gap, we introduce Equivariant Pretrained Transformer (EPT), a novel pretraining framework designed to harmonize the geometric learning of small molecules and proteins. To be specific, EPT unifies the geometric modeling of multi-domain molecules via the block-enhanced representation that can attend a broader context of each atom. Upon transformer framework, EPT is further enhanced with E(3) equivariance to facilitate the accurate representation of 3D structures. Another key innovation of EPT is its block-level pretraining task, which allows for joint pretraining on datasets comprising both small molecules and proteins. Experim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#32479;&#19968;&#26368;&#21518;&#36845;&#20195;(ULI)&#20445;&#35777;&#36825;&#19968;&#26356;&#24378;&#30340;&#24615;&#33021;&#24230;&#37327;&#65292;&#21516;&#26102;&#35777;&#26126;&#25509;&#36817;&#26368;&#20248;&#30340;ULI&#20445;&#35777;&#30452;&#25509;&#23548;&#33268;&#20102;&#22312;&#21508;&#31181;&#24615;&#33021;&#24230;&#37327;&#19978;&#25509;&#36817;&#26368;&#20248;&#30340;&#32047;&#31215;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.12711</link><description>&lt;p&gt;
&#20855;&#26377;&#32479;&#19968;&#26368;&#21518;&#36845;&#20195;&#20445;&#35777;&#30340;&#36172;&#21338;&#31639;&#27861;&#23454;&#29616;&#25509;&#36817;&#26368;&#20248;&#36951;&#25022;
&lt;/p&gt;
&lt;p&gt;
Achieving Near-Optimal Regret for Bandit Algorithms with Uniform Last-Iterate Guarantee
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12711
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#32479;&#19968;&#26368;&#21518;&#36845;&#20195;(ULI)&#20445;&#35777;&#36825;&#19968;&#26356;&#24378;&#30340;&#24615;&#33021;&#24230;&#37327;&#65292;&#21516;&#26102;&#35777;&#26126;&#25509;&#36817;&#26368;&#20248;&#30340;ULI&#20445;&#35777;&#30452;&#25509;&#23548;&#33268;&#20102;&#22312;&#21508;&#31181;&#24615;&#33021;&#24230;&#37327;&#19978;&#25509;&#36817;&#26368;&#20248;&#30340;&#32047;&#31215;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#36172;&#21338;&#31639;&#27861;&#24615;&#33021;&#24230;&#37327;&#65292;&#22914;&#36951;&#25022;&#12289;PAC&#30028;&#38480;&#25110;&#32479;&#19968;PAC(Dann&#31561;&#20154;&#65292;2017)&#65292;&#36890;&#24120;&#35780;&#20272;&#32047;&#31215;&#24615;&#33021;&#65292;&#21516;&#26102;&#20801;&#35768;&#22312;&#20219;&#24847;&#26377;&#38480;&#26102;&#38388;t&#20869;&#29609;&#24369;&#21155;&#30340;&#33218;&#12290;&#36825;&#31181;&#34892;&#20026;&#22312;&#39640;&#39118;&#38505;&#24212;&#29992;&#20013;&#21487;&#33021;&#36896;&#25104;&#20005;&#37325;&#25439;&#22833;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26356;&#24378;&#30340;&#24615;&#33021;&#24230;&#37327;&#65292;&#32479;&#19968;&#26368;&#21518;&#36845;&#20195;(ULI)&#20445;&#35777;&#65292;&#25429;&#25417;&#36172;&#21338;&#31639;&#27861;&#30340;&#32047;&#31215;&#21644;&#30636;&#26102;&#24615;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;ULI&#34920;&#24449;&#20102;&#30636;&#26102;&#24615;&#33021;&#65292;&#22240;&#20026;&#23427;&#30830;&#20445;&#25152;&#29609;&#24369;&#21155;&#33218;&#30340;&#27599;&#36718;&#36951;&#25022;&#21463;&#21040;&#19968;&#20010;&#20989;&#25968;&#30340;&#38480;&#21046;&#65292;&#35813;&#20989;&#25968;&#38543;&#30528;&#65288;&#22823;&#65289;&#36718;&#27425;t&#21333;&#35843;&#36882;&#20943;&#65292;&#22312;&#26377;&#36275;&#22815;&#26679;&#26412;&#21487;&#29992;&#26102;&#38450;&#27490;&#37325;&#22797;&#35775;&#38382;&#21155;&#36136;&#33218;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#25509;&#36817;&#26368;&#20248;&#30340;ULI&#20445;&#35777;&#30452;&#25509;&#24847;&#21619;&#30528;&#22312;&#19978;&#36848;&#24615;&#33021;&#24230;&#37327;&#20013;&#23454;&#29616;&#25509;&#36817;&#26368;&#20248;&#30340;&#32047;&#31215;&#24615;&#33021;&#12290;&#20026;&#20102;&#30740;&#31350;ULI&#22312;&#26377;&#38480;&#33218;&#38598;&#19978;&#30340;&#21487;&#36798;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12711v1 Announce Type: new  Abstract: Existing performance measures for bandit algorithms such as regret, PAC bounds, or uniform-PAC (Dann et al., 2017), typically evaluate the cumulative performance, while allowing the play of an arbitrarily bad arm at any finite time t. Such a behavior can be highly detrimental in high-stakes applications. This paper introduces a stronger performance measure, the uniform last-iterate (ULI) guarantee, capturing both cumulative and instantaneous performance of bandit algorithms. Specifically, ULI characterizes the instantaneous performance since it ensures that the per-round regret of the played arm is bounded by a function, monotonically decreasing w.r.t. (large) round t, preventing revisits to bad arms when sufficient samples are available. We demonstrate that a near-optimal ULI guarantee directly implies near-optimal cumulative performance across aforementioned performance measures. To examine the achievability of ULI in the finite arm se
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;ACI&#65292;&#19987;&#27880;&#20110;&#22312;&#32593;&#32476;&#24178;&#25200;&#21644;&#38750;&#38543;&#26426;&#22788;&#29702;&#20998;&#37197;&#24773;&#20917;&#19979;&#20272;&#35745;&#30452;&#25509;&#21644;&#28322;&#20986;&#22788;&#29702;&#25928;&#24212;&#12290;</title><link>https://arxiv.org/abs/2402.12710</link><description>&lt;p&gt;
&#22312;&#22240;&#26524;&#25512;&#26029;&#20013;&#25972;&#21512;&#24178;&#39044;&#23398;&#20064;&#65306;&#22312;&#32447;&#23454;&#39564;&#20013;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Integrating Active Learning in Causal Inference with Interference: A Novel Approach in Online Experiments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12710
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;ACI&#65292;&#19987;&#27880;&#20110;&#22312;&#32593;&#32476;&#24178;&#25200;&#21644;&#38750;&#38543;&#26426;&#22788;&#29702;&#20998;&#37197;&#24773;&#20917;&#19979;&#20272;&#35745;&#30452;&#25509;&#21644;&#28322;&#20986;&#22788;&#29702;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22240;&#26524;&#25512;&#26029;&#30740;&#31350;&#39046;&#22495;&#20013;&#65292;&#26222;&#36941;&#30340;&#28508;&#22312;&#32467;&#26524;&#26694;&#26550;&#65292;&#23588;&#20854;&#26159;&#40065;&#23486;&#22240;&#26524;&#27169;&#22411;&#65288;RCM&#65289;&#65292;&#36890;&#24120;&#24573;&#35270;&#20010;&#20307;&#24178;&#25200;&#24182;&#20551;&#35774;&#29420;&#31435;&#22788;&#29702;&#25928;&#24212;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#20551;&#35774;&#32463;&#24120;&#19982;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#30340;&#22797;&#26434;&#29616;&#23454;&#19981;&#31526;&#65292;&#24178;&#25200;&#19981;&#20165;&#20165;&#26159;&#21487;&#33021;&#24615;&#65292;&#32780;&#19988;&#26159;&#24120;&#35265;&#29616;&#35937;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#19987;&#27880;&#20110;&#22312;&#20004;&#31181;&#20551;&#35774;&#19979;&#20272;&#35745;&#30452;&#25509;&#21644;&#28322;&#20986;&#22788;&#29702;&#25928;&#24212;&#26469;&#35299;&#20915;&#36825;&#19968;&#24046;&#24322;&#65306;&#65288;1&#65289;&#22522;&#20110;&#32593;&#32476;&#30340;&#24178;&#25200;&#65292;&#20854;&#20013;&#36830;&#25509;&#32593;&#32476;&#20869;&#37051;&#23621;&#30340;&#22788;&#29702;&#20250;&#24433;&#21709;&#19968;&#20010;&#20154;&#30340;&#32467;&#26524;&#65292;&#20197;&#21450;&#65288;2&#65289;&#21463;&#28151;&#26434;&#22240;&#32032;&#24433;&#21709;&#30340;&#38750;&#38543;&#26426;&#22788;&#29702;&#20998;&#37197;&#12290;&#20026;&#20102;&#25552;&#39640;&#20272;&#35745;&#21487;&#33021;&#22797;&#26434;&#25928;&#26524;&#20989;&#25968;&#30340;&#25928;&#29575;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;: &#20132;&#20114;&#24178;&#39044;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#20027;&#21160;&#23398;&#20064;&#65288;ACI&#65289;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#39640;&#26031;&#36807;&#31243;&#28789;&#27963;&#22320;mo
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12710v1 Announce Type: cross  Abstract: In the domain of causal inference research, the prevalent potential outcomes framework, notably the Rubin Causal Model (RCM), often overlooks individual interference and assumes independent treatment effects. This assumption, however, is frequently misaligned with the intricate realities of real-world scenarios, where interference is not merely a possibility but a common occurrence. Our research endeavors to address this discrepancy by focusing on the estimation of direct and spillover treatment effects under two assumptions: (1) network-based interference, where treatments on neighbors within connected networks affect one's outcomes, and (2) non-random treatment assignments influenced by confounders. To improve the efficiency of estimating potentially complex effects functions, we introduce an novel active learning approach: Active Learning in Causal Inference with Interference (ACI). This approach uses Gaussian process to flexibly mo
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;transformer&#30340;&#37327;&#23376;&#23884;&#20837;&#26550;&#26500;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#39564;&#35777;&#20102;&#20854;&#22312;&#29616;&#20195;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#39640;&#24230;&#28789;&#27963;&#21644;&#23454;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12704</link><description>&lt;p&gt;
&#29992;Transformer&#36827;&#34892;&#37327;&#23376;&#23884;&#20837;&#20197;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Quantum Embedding with Transformer for High-dimensional Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12704
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;transformer&#30340;&#37327;&#23376;&#23884;&#20837;&#26550;&#26500;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#39564;&#35777;&#20102;&#20854;&#22312;&#29616;&#20195;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#39640;&#24230;&#28789;&#27963;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12704v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#30028; &#25688;&#35201;&#65306;&#37327;&#23376;&#23884;&#20837;&#19982;transformers&#32467;&#21512;&#26159;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#19968;&#31181;&#26032;&#39062;&#19988;&#26377;&#21069;&#26223;&#30340;&#26550;&#26500;&#65292;&#22312;&#36817;&#26399;&#35774;&#22791;&#25110;&#27169;&#25311;&#22120;&#19978;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#35813;&#30740;&#31350;&#23558;&#35270;&#35273;transformer&#65288;ViT&#65289;&#24341;&#20837;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#37327;&#23376;&#23884;&#20837;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;BirdCLEF-2021&#19978;&#30340;&#21333;&#37327;&#23376;&#27604;&#29305;&#20998;&#31867;&#22120;&#20013;&#21462;&#24471;&#20102;&#32422;3%&#30340;&#20013;&#20540;F1&#24471;&#20998;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#22522;&#20110;transformer&#30340;&#26550;&#26500;&#26159;&#22788;&#29702;&#29616;&#20195;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#30340;&#19968;&#31181;&#39640;&#24230;&#28789;&#27963;&#19988;&#23454;&#29992;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12704v1 Announce Type: cross  Abstract: Quantum embedding with transformers is a novel and promising architecture for quantum machine learning to deliver exceptional capability on near-term devices or simulators. The research incorporated a vision transformer (ViT) to advance quantum significantly embedding ability and results for a single qubit classifier with around 3 percent in the median F1 score on the BirdCLEF-2021, a challenging high-dimensional dataset. The study showcases and analyzes empirical evidence that our transformer-based architecture is a highly versatile and practical approach to modern quantum machine learning problems.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#21487;&#23398;&#20064;&#20998;&#35299;&#31574;&#30053;&#21644;&#21452;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#21516;&#26102;&#25429;&#25417;&#36328;&#31995;&#21015;&#20381;&#36182;&#21644;&#20869;&#37096;&#21464;&#21270;&#65292;&#20197;&#24212;&#23545;&#22797;&#26434;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.12694</link><description>&lt;p&gt;
&#22797;&#20852;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65306;&#21487;&#23398;&#20064;&#20998;&#35299;&#19982;&#36328;&#31995;&#21015;&#20381;&#36182;&#20851;&#31995;&#21644;&#20869;&#37096;&#21464;&#21270;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Revitalizing Multivariate Time Series Forecasting: Learnable Decomposition with Inter-Series Dependencies and Intra-Series Variations Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12694
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#21487;&#23398;&#20064;&#20998;&#35299;&#31574;&#30053;&#21644;&#21452;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#21516;&#26102;&#25429;&#25417;&#36328;&#31995;&#21015;&#20381;&#36182;&#21644;&#20869;&#37096;&#21464;&#21270;&#65292;&#20197;&#24212;&#23545;&#22797;&#26434;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#35201;&#27714;&#31934;&#30830;&#24314;&#27169;&#38169;&#32508;&#22797;&#26434;&#27169;&#24335;&#65292;&#21253;&#25324;&#36328;&#26102;&#38388;&#24207;&#21015;&#30340;&#20381;&#36182;&#20851;&#31995;&#21644;&#20869;&#37096;&#21464;&#21160;&#12290;&#27599;&#20010;&#26102;&#38388;&#24207;&#21015;&#20855;&#26377;&#29420;&#29305;&#30340;&#36235;&#21183;&#29305;&#24449;&#24102;&#26469;&#25361;&#25112;&#65292;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#22522;&#26412;&#30340;&#31227;&#21160;&#24179;&#22343;&#26680;&#21487;&#33021;&#38590;&#20197;&#22788;&#29702;&#29616;&#23454;&#25968;&#25454;&#20013;&#30340;&#38750;&#32447;&#24615;&#32467;&#26500;&#21644;&#22797;&#26434;&#36235;&#21183;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#20998;&#35299;&#31574;&#30053;&#65292;&#26356;&#21512;&#29702;&#22320;&#25429;&#25417;&#21160;&#24577;&#36235;&#21183;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#19987;&#38376;&#29992;&#20110;&#21516;&#26102;&#25429;&#25417;&#36328;&#31995;&#21015;&#20381;&#36182;&#20851;&#31995;&#21644;&#20869;&#37096;&#21464;&#21270;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#20854;&#20013;&#36890;&#36807;&#36890;&#36947;&#33258;&#27880;&#24847;&#21147;&#21644;&#33258;&#22238;&#24402;&#33258;&#27880;&#24847;&#21147;&#23454;&#29616;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#20843;&#20010;&#24320;&#28304;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#23558;&#20854;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#36890;&#36807;&#27604;&#36739;&#32467;&#26524;&#65292;&#25105;&#20204;&#30340; Leddam...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12694v1 Announce Type: new  Abstract: Predicting multivariate time series is crucial, demanding precise modeling of intricate patterns, including inter-series dependencies and intra-series variations. Distinctive trend characteristics in each time series pose challenges, and existing methods, relying on basic moving average kernels, may struggle with the non-linear structure and complex trends in real-world data. Given that, we introduce a learnable decomposition strategy to capture dynamic trend information more reasonably. Additionally, we propose a dual attention module tailored to capture inter-series dependencies and intra-series variations simultaneously for better time series forecasting, which is implemented by channel-wise self-attention and autoregressive self-attention. To evaluate the effectiveness of our method, we conducted experiments across eight open-source datasets and compared it with the state-of-the-art methods. Through the comparison results, our Leddam
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#27969;&#24418;&#23398;&#20064;&#30340;&#22312;&#27969;&#24418;&#19978;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#27425;&#24615;&#26500;&#36896;&#33719;&#24471;&#26368;&#20339;&#35823;&#24046;&#30028;&#38480;&#12290;</title><link>https://arxiv.org/abs/2402.12687</link><description>&lt;p&gt;
&#22312;&#27969;&#24418;&#19978;&#23398;&#20064;&#32780;&#26080;&#38656;&#27969;&#24418;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning on manifolds without manifold learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12687
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#27969;&#24418;&#23398;&#20064;&#30340;&#22312;&#27969;&#24418;&#19978;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#27425;&#24615;&#26500;&#36896;&#33719;&#24471;&#26368;&#20339;&#35823;&#24046;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#26410;&#30693;&#20998;&#24067;&#38543;&#26426;&#25277;&#26679;&#30340;&#25968;&#25454;&#36827;&#34892;&#20989;&#25968;&#36924;&#36817;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#19982;&#36890;&#36807;&#26368;&#23567;&#21270;&#25439;&#22833;&#20989;&#25968;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#30427;&#34892;&#33539;&#24335;&#30456;&#21453;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#30340;&#19968;&#27425;&#24615;&#26500;&#36896;&#26041;&#27861;&#65292;&#24182;&#22312;&#27969;&#24418;&#20551;&#35774;&#19979;&#32473;&#20986;&#20102;&#26368;&#20339;&#35823;&#24046;&#30028;&#38480;&#65307;&#21363;&#20551;&#35774;&#25968;&#25454;&#26159;&#20174;&#39640;&#32500;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#30340;&#26410;&#30693;&#23376;&#27969;&#24418;&#20013;&#25277;&#26679;&#24471;&#21040;&#30340;&#12290; Neural Networks 132:253268, 2020 &#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19968;&#27425;&#24615;&#30452;&#25509;&#26041;&#27861;&#26469;&#23454;&#29616;&#20989;&#25968;&#36924;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12687v1 Announce Type: new  Abstract: Function approximation based on data drawn randomly from an unknown distribution is an important problem in machine learning. In contrast to the prevalent paradigm of solving this problem by minimizing a loss functional, we have given a direct one-shot construction together with optimal error bounds under the manifold assumption; i.e., one assumes that the data is sampled from an unknown sub-manifold of a high dimensional Euclidean space. A great deal of research deals with obtaining information about this manifold, such as the eigendecomposition of the Laplace-Beltrami operator or coordinate charts, and using this information for function approximation. This two step approach implies some extra errors in the approximation stemming from basic quantities of the data in addition to the errors inherent in function approximation. In Neural Networks, 132:253268, 2020, we have proposed a one-shot direct method to achieve function approximation
&lt;/p&gt;</description></item><item><title>TorchCP&#26159;&#19968;&#20010;&#22522;&#20110;PyTorch&#30340;Python&#24037;&#20855;&#21253;&#65292;&#20026;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19978;&#30340;&#21512;&#25311;&#24120;&#35268;&#39044;&#27979;&#30740;&#31350;&#25552;&#20379;&#20102;&#23454;&#29616;&#21518;&#39564;&#21644;&#35757;&#32451;&#26041;&#27861;&#30340;&#22810;&#31181;&#24037;&#20855;&#65292;&#21253;&#25324;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#12290;En_Tdlr: TorchCP is a Python toolbox built on PyTorch for conformal prediction research on deep learning models, providing various implementations for posthoc and training methods for classification and regression tasks, including multi-dimension output.</title><link>https://arxiv.org/abs/2402.12683</link><description>&lt;p&gt;
TorchCP&#65306;&#22522;&#20110;PyTorch&#30340;&#19968;&#31181;&#36866;&#29992;&#20110;&#21512;&#25311;&#24120;&#35268;&#39044;&#27979;&#30340;&#24211;
&lt;/p&gt;
&lt;p&gt;
TorchCP: A Library for Conformal Prediction based on PyTorch
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12683
&lt;/p&gt;
&lt;p&gt;
TorchCP&#26159;&#19968;&#20010;&#22522;&#20110;PyTorch&#30340;Python&#24037;&#20855;&#21253;&#65292;&#20026;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19978;&#30340;&#21512;&#25311;&#24120;&#35268;&#39044;&#27979;&#30740;&#31350;&#25552;&#20379;&#20102;&#23454;&#29616;&#21518;&#39564;&#21644;&#35757;&#32451;&#26041;&#27861;&#30340;&#22810;&#31181;&#24037;&#20855;&#65292;&#21253;&#25324;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#12290;En_Tdlr: TorchCP is a Python toolbox built on PyTorch for conformal prediction research on deep learning models, providing various implementations for posthoc and training methods for classification and regression tasks, including multi-dimension output.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
TorchCP&#26159;&#19968;&#20010;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19978;&#30340;&#21512;&#25311;&#24120;&#35268;&#39044;&#27979;&#30740;&#31350;&#30340;Python&#24037;&#20855;&#21253;&#12290;&#23427;&#21253;&#21547;&#20102;&#29992;&#20110;&#21518;&#39564;&#21644;&#35757;&#32451;&#26041;&#27861;&#30340;&#21508;&#31181;&#23454;&#29616;&#65292;&#29992;&#20110;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#65288;&#21253;&#25324;&#22810;&#32500;&#36755;&#20986;&#65289;&#12290;TorchCP&#24314;&#31435;&#22312;PyTorch&#20043;&#19978;&#65292;&#24182;&#21033;&#29992;&#30697;&#38453;&#35745;&#31639;&#30340;&#20248;&#21183;&#65292;&#25552;&#20379;&#31616;&#27905;&#39640;&#25928;&#30340;&#25512;&#29702;&#23454;&#29616;&#12290;&#35813;&#20195;&#30721;&#37319;&#29992;LGPL&#35768;&#21487;&#35777;&#65292;&#24182;&#22312;$\href{https://github.com/ml-stat-Sustech/TorchCP}{\text{this https URL}}$&#24320;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12683v1 Announce Type: new  Abstract: TorchCP is a Python toolbox for conformal prediction research on deep learning models. It contains various implementations for posthoc and training methods for classification and regression tasks (including multi-dimension output). TorchCP is built on PyTorch (Paszke et al., 2019) and leverages the advantages of matrix computation to provide concise and efficient inference implementations. The code is licensed under the LGPL license and is open-sourced at $\href{https://github.com/ml-stat-Sustech/TorchCP}{\text{this https URL}}$.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#23558;RL&#31574;&#30053;&#40065;&#26834;&#24615;&#25193;&#23637;&#33267;&#29366;&#24577;&#23545;&#25239;&#25915;&#20987;&#27169;&#22411;&#65292;&#36229;&#36234;&#20165;&#38024;&#23545;&#26368;&#22351;&#24773;&#20917;&#25915;&#20987;&#65292;&#25552;&#20986;&#22522;&#20110;&#21518;&#24724;&#26368;&#23567;&#21270;&#38382;&#39064;&#30340;&#33258;&#36866;&#24212;&#38450;&#24481;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.12673</link><description>&lt;p&gt;
&#36229;&#36234;&#26368;&#22351;&#24773;&#20917;&#25915;&#20987;&#65306;&#36890;&#36807;&#38750;&#25903;&#37197;&#31574;&#30053;&#23454;&#29616;&#33258;&#36866;&#24212;&#38450;&#24481;&#30340;&#40065;&#26834;RL
&lt;/p&gt;
&lt;p&gt;
Beyond Worst-case Attacks: Robust RL with Adaptive Defense via Non-dominated Policies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12673
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#23558;RL&#31574;&#30053;&#40065;&#26834;&#24615;&#25193;&#23637;&#33267;&#29366;&#24577;&#23545;&#25239;&#25915;&#20987;&#27169;&#22411;&#65292;&#36229;&#36234;&#20165;&#38024;&#23545;&#26368;&#22351;&#24773;&#20917;&#25915;&#20987;&#65292;&#25552;&#20986;&#22522;&#20110;&#21518;&#24724;&#26368;&#23567;&#21270;&#38382;&#39064;&#30340;&#33258;&#36866;&#24212;&#38450;&#24481;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#21508;&#31181;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#21462;&#24471;&#30340;&#34028;&#21187;&#21457;&#23637;&#65292;&#20154;&#20204;&#24320;&#22987;&#38598;&#20013;&#20851;&#27880;&#30830;&#20445;RL&#31574;&#30053;&#22312;&#27979;&#35797;&#26102;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#22260;&#32469;&#30528;&#35299;&#20915;&#26497;&#31471;&#24773;&#20917;&#38382;&#39064;&#65292;&#20197;&#24212;&#23545;&#28508;&#22312;&#30340;&#26368;&#22351;&#24773;&#20917;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#23545;&#24378;&#25915;&#20987;&#20855;&#26377;&#19968;&#23450;&#30340;&#25928;&#26524;&#65292;&#20294;&#24448;&#24448;&#22312;&#27809;&#26377;&#25915;&#20987;&#25110;&#21482;&#26377;&#24369;&#25915;&#20987;&#23384;&#22312;&#26102;&#20250;&#29306;&#29298;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#24191;&#27867;&#25509;&#21463;&#30340;&#29366;&#24577;&#23545;&#25239;&#25915;&#20987;&#27169;&#22411;&#19979;&#30340;&#31574;&#30053;&#40065;&#26834;&#24615;&#65292;&#23558;&#37325;&#28857;&#20174;&#20165;&#38480;&#20110;&#26368;&#22351;&#24773;&#20917;&#25915;&#20987;&#25193;&#23637;&#20986;&#26469;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;&#27979;&#35797;&#26102;&#30340;&#20219;&#21153;&#27491;&#24335;&#21270;&#20026;&#19968;&#20010;&#21518;&#24724;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#24182;&#22312;&#22522;&#20934;&#31574;&#30053;&#26469;&#33258;&#20110;&#36890;&#29992;&#36830;&#32493;&#31574;&#30053;&#31867;$\Pi$&#26102;&#65292;&#22312;&#23454;&#29616;&#20122;&#32447;&#24615;&#21518;&#24724;&#30340;&#22256;&#38590;&#24615;&#38382;&#39064;&#19978;&#20570;&#20986;&#20102;&#38416;&#36848;&#12290;&#36825;&#19968;&#21457;&#29616;&#20419;&#20351;&#25105;&#20204;&#22312;&#27979;&#35797;&#20043;&#21069;\textit{&#20248;&#21270;}&#22522;&#20934;&#31574;&#30053;&#31867;$\Pi$&#65292;&#33268;&#21147;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12673v1 Announce Type: new  Abstract: In light of the burgeoning success of reinforcement learning (RL) in diverse real-world applications, considerable focus has been directed towards ensuring RL policies are robust to adversarial attacks during test time. Current approaches largely revolve around solving a minimax problem to prepare for potential worst-case scenarios. While effective against strong attacks, these methods often compromise performance in the absence of attacks or the presence of only weak attacks. To address this, we study policy robustness under the well-accepted state-adversarial attack model, extending our focus beyond only worst-case attacks. We first formalize this task at test time as a regret minimization problem and establish its intrinsic hardness in achieving sublinear regret when the baseline policy is from a general continuous policy class, $\Pi$. This finding prompts us to \textit{refine} the baseline policy class $\Pi$ prior to test time, aimin
&lt;/p&gt;</description></item><item><title>&#38543;&#26426;&#26862;&#26519;&#30456;&#23545;&#20110;&#35013;&#34955;&#27861;&#20855;&#26377;&#20943;&#23569;&#20559;&#24046;&#30340;&#33021;&#21147;&#65292;&#22312;&#25581;&#31034;&#25968;&#25454;&#27169;&#24335;&#21644;&#39640;&#20449;&#22122;&#27604;&#24773;&#20917;&#19979;&#34920;&#29616;&#26356;&#22909;&#30340;&#29305;&#28857;&#65292;&#20026;&#38543;&#26426;&#26862;&#26519;&#22312;&#19981;&#21516;&#20449;&#22122;&#27604;&#29615;&#22659;&#19979;&#30340;&#25104;&#21151;&#25552;&#20379;&#20102;&#35299;&#37322;&#21644;&#23454;&#29992;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.12668</link><description>&lt;p&gt;
&#38543;&#26426;&#21270;&#26082;&#21487;&#20197;&#20943;&#23569;&#20559;&#24046;&#21448;&#21487;&#20197;&#20943;&#23569;&#26041;&#24046;&#65306;&#38543;&#26426;&#26862;&#26519;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Randomization Can Reduce Both Bias and Variance: A Case Study in Random Forests
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12668
&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26862;&#26519;&#30456;&#23545;&#20110;&#35013;&#34955;&#27861;&#20855;&#26377;&#20943;&#23569;&#20559;&#24046;&#30340;&#33021;&#21147;&#65292;&#22312;&#25581;&#31034;&#25968;&#25454;&#27169;&#24335;&#21644;&#39640;&#20449;&#22122;&#27604;&#24773;&#20917;&#19979;&#34920;&#29616;&#26356;&#22909;&#30340;&#29305;&#28857;&#65292;&#20026;&#38543;&#26426;&#26862;&#26519;&#22312;&#19981;&#21516;&#20449;&#22122;&#27604;&#29615;&#22659;&#19979;&#30340;&#25104;&#21151;&#25552;&#20379;&#20102;&#35299;&#37322;&#21644;&#23454;&#29992;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#24448;&#24448;&#34987;&#24573;&#35270;&#30340;&#29616;&#35937;&#65292;&#39318;&#27425;&#22312;\cite{breiman2001random}&#20013;&#25351;&#20986;&#65292;&#21363;&#38543;&#26426;&#26862;&#26519;&#20284;&#20046;&#27604;&#35013;&#34955;&#27861;&#20943;&#23569;&#20102;&#20559;&#24046;&#12290;&#21463;\cite{mentch2020randomization}&#19968;&#31687;&#26377;&#36259;&#30340;&#35770;&#25991;&#30340;&#21551;&#21457;&#65292;&#20854;&#20013;&#20316;&#32773;&#35748;&#20026;&#38543;&#26426;&#26862;&#26519;&#20943;&#23569;&#20102;&#26377;&#25928;&#33258;&#30001;&#24230;&#65292;&#24182;&#19988;&#21482;&#26377;&#22312;&#20302;&#20449;&#22122;&#27604;&#65288;SNR&#65289;&#29615;&#22659;&#19979;&#25165;&#33021;&#32988;&#36807;&#35013;&#34955;&#38598;&#25104;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#38543;&#26426;&#26862;&#26519;&#22914;&#20309;&#33021;&#22815;&#25581;&#31034;&#34987;&#35013;&#34955;&#27861;&#24573;&#35270;&#30340;&#25968;&#25454;&#27169;&#24335;&#12290;&#25105;&#20204;&#22312;&#23454;&#35777;&#20013;&#35777;&#26126;&#65292;&#22312;&#23384;&#22312;&#36825;&#31181;&#27169;&#24335;&#30340;&#24773;&#20917;&#19979;&#65292;&#38543;&#26426;&#26862;&#26519;&#19981;&#20165;&#21487;&#20197;&#20943;&#23567;&#20559;&#24046;&#36824;&#33021;&#20943;&#23567;&#26041;&#24046;&#65292;&#24182;&#19988;&#24403;&#20449;&#22122;&#27604;&#39640;&#26102;&#38543;&#26426;&#26862;&#26519;&#30340;&#34920;&#29616;&#24840;&#21457;&#22909;&#20110;&#35013;&#34955;&#38598;&#25104;&#12290;&#25105;&#20204;&#30340;&#35266;&#23519;&#20026;&#35299;&#37322;&#38543;&#26426;&#26862;&#26519;&#22312;&#21508;&#31181;&#20449;&#22122;&#27604;&#24773;&#20917;&#19979;&#30340;&#30495;&#23454;&#19990;&#30028;&#25104;&#21151;&#25552;&#20379;&#20102;&#35265;&#35299;&#65292;&#24182;&#22686;&#36827;&#20102;&#25105;&#20204;&#23545;&#38543;&#26426;&#26862;&#26519;&#19982;&#35013;&#34955;&#38598;&#25104;&#22312;&#27599;&#27425;&#20998;&#21106;&#27880;&#20837;&#30340;&#38543;&#26426;&#21270;&#26041;&#38754;&#30340;&#24046;&#24322;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#32467;&#26524;&#36824;&#25552;&#20379;&#20102;&#23454;&#29992;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12668v1 Announce Type: cross  Abstract: We study the often overlooked phenomenon, first noted in \cite{breiman2001random}, that random forests appear to reduce bias compared to bagging. Motivated by an interesting paper by \cite{mentch2020randomization}, where the authors argue that random forests reduce effective degrees of freedom and only outperform bagging ensembles in low signal-to-noise ratio (SNR) settings, we explore how random forests can uncover patterns in the data missed by bagging. We empirically demonstrate that in the presence of such patterns, random forests reduce bias along with variance and increasingly outperform bagging ensembles when SNR is high. Our observations offer insights into the real-world success of random forests across a range of SNRs and enhance our understanding of the difference between random forests and bagging ensembles with respect to the randomization injected into each split. Our investigations also yield practical insights into the 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#39640;&#25928;&#30340;&#30830;&#23450;&#24615;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;DDAR&#65292;&#36890;&#36807;&#24341;&#20837;&#21028;&#21035;&#36317;&#31163;&#24863;&#30693;&#34920;&#31034;&#21644;&#26368;&#20248;&#21487;&#35757;&#32451;&#21407;&#22411;&#65292;&#20811;&#26381;&#20102;&#29305;&#24449;&#22604;&#32553;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#20854;&#28789;&#27963;&#24615;&#21644;&#26550;&#26500;&#26080;&#20851;&#24615;</title><link>https://arxiv.org/abs/2402.12664</link><description>&lt;p&gt;
&#21028;&#21035;&#36317;&#31163;&#24863;&#30693;&#34920;&#31034;&#22312;&#30830;&#23450;&#24615;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#19978;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Discriminant Distance-Aware Representation on Deterministic Uncertainty Quantification Methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12664
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#39640;&#25928;&#30340;&#30830;&#23450;&#24615;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;DDAR&#65292;&#36890;&#36807;&#24341;&#20837;&#21028;&#21035;&#36317;&#31163;&#24863;&#30693;&#34920;&#31034;&#21644;&#26368;&#20248;&#21487;&#35757;&#32451;&#21407;&#22411;&#65292;&#20811;&#26381;&#20102;&#29305;&#24449;&#22604;&#32553;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#20854;&#28789;&#27963;&#24615;&#21644;&#26550;&#26500;&#26080;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26159;&#22312;&#37096;&#32626;&#21487;&#38752;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21040;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#39640;&#25928;&#30340;&#30830;&#23450;&#24615;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#65292;&#31216;&#20026;&#21028;&#21035;&#36317;&#31163;&#24863;&#30693;&#34920;&#31034;&#65288;DDAR&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#26500;&#24314;&#19968;&#20010;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#22312;&#20854;&#28508;&#22312;&#34920;&#31034;&#20013;&#21253;&#21547;&#19968;&#32452;&#21407;&#22411;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#20174;&#36755;&#20837;&#25968;&#25454;&#20013;&#20998;&#26512;&#26377;&#20215;&#20540;&#30340;&#29305;&#24449;&#20449;&#24687;&#12290;&#36890;&#36807;&#22312;&#26368;&#20248;&#21487;&#35757;&#32451;&#21407;&#22411;&#19978;&#24212;&#29992;&#21306;&#20998;&#26368;&#22823;&#21270;&#23618;&#65292;DDAR&#33021;&#22815;&#23398;&#20064;&#21040;&#21028;&#21035;&#36317;&#31163;&#24863;&#30693;&#34920;&#31034;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;DDAR&#36890;&#36807;&#25918;&#23485;&#26463;&#32538;&#30830;&#23450;&#24615;&#19981;&#30830;&#23450;&#24615;&#26041;&#27861;&#65288;DUMs&#65289;&#26550;&#26500;&#30340;&#23454;&#29992;&#24615;&#25152;&#38459;&#30861;&#30340;&#26446;&#26222;&#24076;&#33576;&#32422;&#26463;&#65292;&#20811;&#26381;&#20102;&#29305;&#24449;&#22604;&#32553;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;DDAR&#26159;&#19968;&#31181;&#28789;&#27963;&#30340;&#12289;&#19982;&#26550;&#26500;&#26080;&#20851;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#21487;&#25554;&#20837;&#30340;&#23618;&#36731;&#26494;&#38598;&#25104;&#21040;&#20855;&#26377;&#36317;&#31163;&#25935;&#24863;&#24230;&#37327;&#30340;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12664v1 Announce Type: new  Abstract: Uncertainty estimation is a crucial aspect of deploying dependable deep learning models in safety-critical systems. In this study, we introduce a novel and efficient method for deterministic uncertainty estimation called Discriminant Distance-Awareness Representation (DDAR). Our approach involves constructing a DNN model that incorporates a set of prototypes in its latent representations, enabling us to analyze valuable feature information from the input data. By leveraging a distinction maximization layer over optimal trainable prototypes, DDAR can learn a discriminant distance-awareness representation. We demonstrate that DDAR overcomes feature collapse by relaxing the Lipschitz constraint that hinders the practicality of deterministic uncertainty methods (DUMs) architectures. Our experiments show that DDAR is a flexible and architecture-agnostic method that can be easily integrated as a pluggable layer with distance-sensitive metrics,
&lt;/p&gt;</description></item><item><title>SoftQE&#36890;&#36807;&#23558;&#36755;&#20837;&#26597;&#35810;&#30340;&#23884;&#20837;&#26144;&#23556;&#21040;LLM&#25193;&#23637;&#26597;&#35810;&#30340;&#23884;&#20837;&#65292;&#25552;&#39640;&#20102;&#23494;&#38598;&#26816;&#32034;&#24615;&#33021;&#65292;&#24182;&#22312;&#39046;&#22495;&#22806;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#21892;&#12290;</title><link>https://arxiv.org/abs/2402.12663</link><description>&lt;p&gt;
SoftQE: LLM&#25193;&#23637;&#30340;&#26597;&#35810;&#23398;&#20064;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
SoftQE: Learned Representations of Queries Expanded by LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12663
&lt;/p&gt;
&lt;p&gt;
SoftQE&#36890;&#36807;&#23558;&#36755;&#20837;&#26597;&#35810;&#30340;&#23884;&#20837;&#26144;&#23556;&#21040;LLM&#25193;&#23637;&#26597;&#35810;&#30340;&#23884;&#20837;&#65292;&#25552;&#39640;&#20102;&#23494;&#38598;&#26816;&#32034;&#24615;&#33021;&#65292;&#24182;&#22312;&#39046;&#22495;&#22806;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#38598;&#25104;&#21040;&#26597;&#35810;&#32534;&#30721;&#22120;&#20013;&#65292;&#20197;&#25913;&#21892;&#23494;&#38598;&#26816;&#32034;&#65292;&#21516;&#26102;&#36991;&#20813;&#22312;&#25512;&#26029;&#26102;&#20381;&#36182;LLMs&#22686;&#21152;&#24310;&#36831;&#21644;&#25104;&#26412;&#12290;SoftQE&#36890;&#36807;&#23558;&#36755;&#20837;&#26597;&#35810;&#30340;&#23884;&#20837;&#26144;&#23556;&#21040;LLM&#25193;&#23637;&#26597;&#35810;&#30340;&#23884;&#20837;&#26469;&#25972;&#21512;LLMs&#30340;&#30693;&#35782;&#12290;&#34429;&#28982;&#23545;&#20110;&#39046;&#22495;&#20869;MS-MARCO&#25351;&#26631;&#65292;SoftQE&#30456;&#23545;&#20110;&#21508;&#31181;&#24378;&#22522;&#20934;&#27169;&#22411;&#30340;&#25913;&#21892;&#26377;&#38480;&#65292;&#20294;&#22312;&#20116;&#20010;&#39046;&#22495;&#22806;BEIR&#20219;&#21153;&#19978;&#65292;SoftQE&#22312;&#24179;&#22343;&#24615;&#33021;&#19978;&#25552;&#39640;&#20102;2.83&#20010;&#32477;&#23545;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12663v1 Announce Type: new  Abstract: We investigate the integration of Large Language Models (LLMs) into query encoders to improve dense retrieval without increasing latency and cost, by circumventing the dependency on LLMs at inference time. SoftQE incorporates knowledge from LLMs by mapping embeddings of input queries to those of the LLM-expanded queries. While improvements over various strong baselines on in-domain MS-MARCO metrics are marginal, SoftQE improves performance by 2.83 absolute percentage points on average on five out-of-domain BEIR tasks.
&lt;/p&gt;</description></item><item><title>HyperMoE&#36890;&#36807;Hypernetworks&#26694;&#26550;&#25972;&#21512;&#30693;&#35782;&#20256;&#36882;&#30340;&#27010;&#24565;&#65292;&#35299;&#20915;&#20102;&#22312;&#19987;&#23478;&#36873;&#25321;&#36807;&#31243;&#20013;&#19987;&#23478;&#30693;&#35782;&#31232;&#30095;&#24615;&#21644;&#21487;&#29992;&#24615;&#20043;&#38388;&#30340;&#30683;&#30462;&#12290;</title><link>https://arxiv.org/abs/2402.12656</link><description>&lt;p&gt;
HyperMoE: &#36890;&#36807;&#19987;&#23478;&#20043;&#38388;&#30340;&#30693;&#35782;&#20256;&#36882;&#23454;&#29616;&#26356;&#22909;&#30340;&#19987;&#23478;&#28151;&#21512;
&lt;/p&gt;
&lt;p&gt;
HyperMoE: Towards Better Mixture of Experts via Transferring Among Experts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12656
&lt;/p&gt;
&lt;p&gt;
HyperMoE&#36890;&#36807;Hypernetworks&#26694;&#26550;&#25972;&#21512;&#30693;&#35782;&#20256;&#36882;&#30340;&#27010;&#24565;&#65292;&#35299;&#20915;&#20102;&#22312;&#19987;&#23478;&#36873;&#25321;&#36807;&#31243;&#20013;&#19987;&#23478;&#30693;&#35782;&#31232;&#30095;&#24615;&#21644;&#21487;&#29992;&#24615;&#20043;&#38388;&#30340;&#30683;&#30462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#19987;&#23478;(MoE)&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#34987;&#35777;&#26126;&#26377;&#25928;&#22320;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#21160;&#24577;&#22320;&#23558;&#27599;&#20010;&#36755;&#20837;&#26631;&#35760;&#36335;&#30001;&#21040;&#29305;&#23450;&#30340;&#19987;&#23478;&#23376;&#38598;&#36827;&#34892;&#22788;&#29702;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#22312;&#19987;&#23478;&#30693;&#35782;&#30340;&#31232;&#30095;&#24615;&#21644;&#21487;&#29992;&#24615;&#20043;&#38388;&#38754;&#20020;&#25361;&#25112;&#65306;&#36890;&#36807;&#22686;&#21152;&#23545;&#19987;&#23478;&#30693;&#35782;&#30340;&#20351;&#29992;&#26469;&#22686;&#24378;&#24615;&#33021;&#65292;&#24448;&#24448;&#20250;&#23548;&#33268;&#22312;&#19987;&#23478;&#36873;&#25321;&#36807;&#31243;&#20013;&#31232;&#30095;&#24230;&#20943;&#23569;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#30683;&#30462;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HyperMoE&#65292;&#36825;&#26159;&#19968;&#20010;&#24314;&#31435;&#22312;Hypernetworks&#20043;&#19978;&#30340;&#26032;&#39062;MoE&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#23558;MoE&#30340;&#35745;&#31639;&#36807;&#31243;&#19982;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#30693;&#35782;&#20256;&#36882;&#27010;&#24565;&#36827;&#34892;&#20102;&#38598;&#25104;&#12290;&#22522;&#20110;&#26410;&#36873;&#25321;&#19987;&#23478;&#20449;&#24687;&#29983;&#25104;&#30340;&#29305;&#23450;&#27169;&#22359;&#20316;&#20026;&#34917;&#20805;&#20449;&#24687;&#65292;&#20801;&#35768;&#26410;&#34987;&#36873;&#20013;&#30340;&#19987;&#23478;&#30340;&#30693;&#35782;&#22312;&#20445;&#25345;&#36873;&#25321;&#31232;&#30095;&#24615;&#30340;&#21516;&#26102;&#34987;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12656v1 Announce Type: cross  Abstract: The Mixture of Experts (MoE) for language models has been proven effective in augmenting the capacity of models by dynamically routing each input token to a specific subset of experts for processing. Despite the success, most existing methods face a challenge for balance between sparsity and the availability of expert knowledge: enhancing performance through increased use of expert knowledge often results in diminishing sparsity during expert selection. To mitigate this contradiction, we propose HyperMoE, a novel MoE framework built upon Hypernetworks. This framework integrates the computational processes of MoE with the concept of knowledge transferring in multi-task learning. Specific modules generated based on the information of unselected experts serve as supplementary information, which allows the knowledge of experts not selected to be used while maintaining selection sparsity. Our comprehensive empirical evaluations across multi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#30340;&#39640;&#25928;&#29256;&#26412;&#30340;&#38750;&#26799;&#24230;&#22352;&#26631;&#25628;&#32034;&#65288;CS&#65289;&#31639;&#27861;&#65292;&#25105;&#20204;&#21487;&#20197;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65292;&#35299;&#20915;&#20102;&#38656;&#35201;&#21487;&#24494;&#28608;&#27963;&#20989;&#25968;&#21644;&#21516;&#26102;&#20248;&#21270;&#22810;&#20010;&#38750;&#21487;&#24494;&#25439;&#22833;&#20989;&#25968;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.12646</link><description>&lt;p&gt;
&#36890;&#36807;&#22352;&#26631;&#25628;&#32034;&#31639;&#27861;&#35757;&#32451;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Training Artificial Neural Networks by Coordinate Search Algorithm
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12646
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#30340;&#39640;&#25928;&#29256;&#26412;&#30340;&#38750;&#26799;&#24230;&#22352;&#26631;&#25628;&#32034;&#65288;CS&#65289;&#31639;&#27861;&#65292;&#25105;&#20204;&#21487;&#20197;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65292;&#35299;&#20915;&#20102;&#38656;&#35201;&#21487;&#24494;&#28608;&#27963;&#20989;&#25968;&#21644;&#21516;&#26102;&#20248;&#21270;&#22810;&#20010;&#38750;&#21487;&#24494;&#25439;&#22833;&#20989;&#25968;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#20851;&#38190;&#24615;&#30340;&#38382;&#39064;&#12290; &#23613;&#31649;&#26799;&#24230;&#19979;&#38477;&#31561;&#22522;&#20110;&#26799;&#24230;&#30340;&#23398;&#20064;&#26041;&#27861;&#22312;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26041;&#38754;&#26377;&#25928;&#65292;&#20294;&#23427;&#20204;&#20063;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#12290; &#20363;&#22914;&#65292;&#23427;&#20204;&#38656;&#35201;&#21487;&#24494;&#28608;&#27963;&#20989;&#25968;&#65292;&#24182;&#19988;&#19981;&#33021;&#22522;&#20110;&#22810;&#20010;&#29420;&#31435;&#30340;&#38750;&#21487;&#24494;&#25439;&#22833;&#20989;&#25968;&#21516;&#26102;&#20248;&#21270;&#27169;&#22411;&#65307;&#20363;&#22914;&#65292;&#22312;&#27979;&#35797;&#26399;&#38388;&#20351;&#29992;&#30340; F1 &#20998;&#25968;&#21487;&#20197;&#22312;&#35757;&#32451;&#26399;&#38388;&#20351;&#29992;&#65292;&#24403;&#37319;&#29992;&#26080;&#26799;&#24230;&#20248;&#21270;&#31639;&#27861;&#26102;&#12290; &#27492;&#22806;&#65292;&#20219;&#20309; DNN &#20013;&#30340;&#35757;&#32451;&#21487;&#33021;&#21482;&#38656;&#24456;&#23569;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290; &#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38750;&#26799;&#24230;&#22352;&#26631;&#25628;&#32034;&#65288;CS&#65289;&#31639;&#27861;&#30340;&#39640;&#25928;&#29256;&#26412;&#65292;&#23427;&#26159;&#36890;&#29992;&#27169;&#24335;&#25628;&#32034;&#26041;&#27861;&#30340;&#19968;&#31181;&#23454;&#20363;&#65292;&#29992;&#20110;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12646v1 Announce Type: cross  Abstract: Training Artificial Neural Networks poses a challenging and critical problem in machine learning. Despite the effectiveness of gradient-based learning methods, such as Stochastic Gradient Descent (SGD), in training neural networks, they do have several limitations. For instance, they require differentiable activation functions, and cannot optimize a model based on several independent non-differentiable loss functions simultaneously; for example, the F1-score, which is used during testing, can be used during training when a gradient-free optimization algorithm is utilized. Furthermore, the training in any DNN can be possible with a small size of the training dataset. To address these concerns, we propose an efficient version of the gradient-free Coordinate Search (CS) algorithm, an instance of General Pattern Search methods, for training neural networks. The proposed algorithm can be used with non-differentiable activation functions and
&lt;/p&gt;</description></item><item><title>FAST&#26694;&#26550;&#36890;&#36807;&#24555;&#36895;&#20998;&#27573;&#24418;&#29366;&#20989;&#25968;&#30340;&#20248;&#21270;&#21644;&#26032;&#30340;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#65292;&#20351;&#24471;&#36879;&#26126;&#30340;&#38468;&#21152;&#27169;&#22411;&#30340;&#25311;&#21512;&#36895;&#24230;&#27604;&#29616;&#26377;&#26041;&#27861;&#24555;2&#20010;&#25968;&#37327;&#32423;&#12290;</title><link>https://arxiv.org/abs/2402.12630</link><description>&lt;p&gt;
FAST: &#19968;&#31181;&#29992;&#20110;&#24555;&#36895;&#36879;&#26126;&#26426;&#22120;&#23398;&#20064;&#20013;&#24555;&#36895;&#38468;&#21152;&#20998;&#21106;&#30340;&#20248;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FAST: An Optimization Framework for Fast Additive Segmentation in Transparent ML
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12630
&lt;/p&gt;
&lt;p&gt;
FAST&#26694;&#26550;&#36890;&#36807;&#24555;&#36895;&#20998;&#27573;&#24418;&#29366;&#20989;&#25968;&#30340;&#20248;&#21270;&#21644;&#26032;&#30340;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#65292;&#20351;&#24471;&#36879;&#26126;&#30340;&#38468;&#21152;&#27169;&#22411;&#30340;&#25311;&#21512;&#36895;&#24230;&#27604;&#29616;&#26377;&#26041;&#27861;&#24555;2&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;FAST&#65292;&#19968;&#31181;&#29992;&#20110;&#24555;&#36895;&#38468;&#21152;&#20998;&#21106;&#30340;&#20248;&#21270;&#26694;&#26550;&#12290;FAST&#20026;&#25968;&#25454;&#38598;&#20013;&#30340;&#27599;&#20010;&#29305;&#24449;&#20998;&#27573;&#24120;&#25968;&#24418;&#29366;&#20989;&#25968;&#65292;&#20197;&#20135;&#29983;&#36879;&#26126;&#30340;&#38468;&#21152;&#27169;&#22411;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#20248;&#21270;&#36807;&#31243;&#36866;&#37197;&#36825;&#20123;&#27169;&#22411;&#65292;&#36895;&#24230;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#22914;&#21487;&#35299;&#37322;&#24615;&#22686;&#24378;&#26426;&#22120; \citep{nori2019interpretml}&#65292;&#24555;&#32422;2&#20010;&#25968;&#37327;&#32423;&#12290;&#25105;&#20204;&#36824;&#22312;FAST&#26694;&#26550;&#20013;&#24320;&#21457;&#20102;&#26032;&#30340;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#65292;&#20197;&#36866;&#37197;&#24615;&#33021;&#33391;&#22909;&#30340;&#31616;&#32422;&#27169;&#22411;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;FAST&#25552;&#39640;&#20102;&#38468;&#21152;&#27169;&#22411;&#30340;&#35745;&#31639;&#25928;&#29575;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12630v1 Announce Type: cross  Abstract: We present FAST, an optimization framework for fast additive segmentation. FAST segments piecewise constant shape functions for each feature in a dataset to produce transparent additive models. The framework leverages a novel optimization procedure to fit these models $\sim$2 orders of magnitude faster than existing state-of-the-art methods, such as explainable boosting machines \citep{nori2019interpretml}. We also develop new feature selection algorithms in the FAST framework to fit parsimonious models that perform well. Through experiments and case studies, we show that FAST improves the computational efficiency and interpretability of additive models.
&lt;/p&gt;</description></item><item><title>&#35813;&#32508;&#36848;&#23558;&#39046;&#22495;&#36716;&#31227;&#21644;&#27010;&#24565;&#28418;&#31227;&#37325;&#26032;&#24402;&#20026;&#19968;&#20010;&#21333;&#19968;&#30340;&#30740;&#31350;&#38382;&#39064;&#65292;&#21363;&#25968;&#25454;&#21464;&#21270;&#38382;&#39064;&#65292;&#31995;&#32479;&#22320;&#24635;&#32467;&#20102;&#36825;&#20004;&#20010;&#30740;&#31350;&#39046;&#22495;&#20013;&#26368;&#26032;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.12627</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#25968;&#25454;&#21464;&#21270;&#26041;&#38754;&#30340;&#32508;&#21512;&#35780;&#35770;&#65306;&#36328;&#39046;&#22495;&#36879;&#35270;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Review of Machine Learning Advances on Data Change: A Cross-Field Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12627
&lt;/p&gt;
&lt;p&gt;
&#35813;&#32508;&#36848;&#23558;&#39046;&#22495;&#36716;&#31227;&#21644;&#27010;&#24565;&#28418;&#31227;&#37325;&#26032;&#24402;&#20026;&#19968;&#20010;&#21333;&#19968;&#30340;&#30740;&#31350;&#38382;&#39064;&#65292;&#21363;&#25968;&#25454;&#21464;&#21270;&#38382;&#39064;&#65292;&#31995;&#32479;&#22320;&#24635;&#32467;&#20102;&#36825;&#20004;&#20010;&#30740;&#31350;&#39046;&#22495;&#20013;&#26368;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25216;&#26415;&#22312;&#21508;&#20010;&#23398;&#26415;&#39046;&#22495;&#21644;&#34892;&#19994;&#37117;&#23637;&#29616;&#20102;&#26174;&#33879;&#30340;&#21457;&#23637;&#65292;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#21160;&#24577;&#25968;&#25454;&#23548;&#33268;&#20102;&#37096;&#32626;AI&#27169;&#22411;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#24847;&#22806;&#30340;&#25968;&#25454;&#21464;&#21270;&#20250;&#23548;&#33268;AI&#27169;&#22411;&#24615;&#33021;&#20005;&#37325;&#19979;&#38477;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#20004;&#20010;&#20027;&#35201;&#30456;&#20851;&#30740;&#31350;&#39046;&#22495;&#65292;&#39046;&#22495;&#36716;&#31227;&#21644;&#27010;&#24565;&#28418;&#31227;&#65292;&#26681;&#25454;&#25968;&#25454;&#21464;&#21270;&#30340;&#35774;&#23450;&#12290;&#34429;&#28982;&#36825;&#20004;&#20010;&#27969;&#34892;&#30340;&#30740;&#31350;&#39046;&#22495;&#30340;&#30446;&#26631;&#26159;&#35299;&#20915;&#20998;&#24067;&#20559;&#31227;&#21644;&#38750;&#24179;&#31283;&#25968;&#25454;&#27969;&#38382;&#39064;&#65292;&#20294;&#22522;&#26412;&#23646;&#24615;&#20173;&#28982;&#30456;&#20284;&#65292;&#36825;&#20063;&#40723;&#21169;&#37319;&#29992;&#31867;&#20284;&#30340;&#25216;&#26415;&#26041;&#27861;&#12290;&#22312;&#36825;&#31687;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#23558;&#39046;&#22495;&#36716;&#31227;&#21644;&#27010;&#24565;&#28418;&#31227;&#37325;&#26032;&#32452;&#21512;&#25104;&#19968;&#20010;&#21333;&#19968;&#30340;&#30740;&#31350;&#38382;&#39064;&#65292;&#21363;&#25968;&#25454;&#21464;&#21270;&#38382;&#39064;&#65292;&#24182;&#31995;&#32479;&#22320;&#27010;&#36848;&#20102;&#36825;&#20004;&#20010;&#30740;&#31350;&#39046;&#22495;&#20013;&#26368;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#38454;&#27573;&#38382;&#39064;&#20998;&#31867;&#26041;&#26696;&#65292;&#20197;&#23558;&#36825;&#20004;&#20010;&#25216;&#26415;&#39046;&#22495;&#30340;&#20851;&#38190;&#24605;&#24819;&#32852;&#31995;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12627v1 Announce Type: cross  Abstract: Recent artificial intelligence (AI) technologies show remarkable evolution in various academic fields and industries. However, in the real world, dynamic data lead to principal challenges for deploying AI models. An unexpected data change brings about severe performance degradation in AI models. We identify two major related research fields, domain shift and concept drift according to the setting of the data change. Although these two popular research fields aim to solve distribution shift and non-stationary data stream problems, the underlying properties remain similar which also encourages similar technical approaches. In this review, we regroup domain shift and concept drift into a single research problem, namely the data change problem, with a systematic overview of state-of-the-art methods in the two research fields. We propose a three-phase problem categorization scheme to link the key ideas in the two technical fields. We thus p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#39044;&#35757;&#32451;&#29305;&#24449;&#25552;&#21462;&#22120;&#30340;&#20219;&#24847;&#25968;&#25454;&#27602;&#21270;&#25915;&#20987;&#65292;&#25506;&#35752;&#20102;&#36825;&#31181;&#25915;&#20987;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#23433;&#20840;&#39118;&#38505;&#21644;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.12626</link><description>&lt;p&gt;
&#38024;&#23545;&#39044;&#35757;&#32451;&#29305;&#24449;&#25552;&#21462;&#22120;&#30340;&#20219;&#24847;&#25968;&#25454;&#27602;&#21270;&#25915;&#20987;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Indiscriminate Data Poisoning Attacks on Pre-trained Feature Extractors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#39044;&#35757;&#32451;&#29305;&#24449;&#25552;&#21462;&#22120;&#30340;&#20219;&#24847;&#25968;&#25454;&#27602;&#21270;&#25915;&#20987;&#65292;&#25506;&#35752;&#20102;&#36825;&#31181;&#25915;&#20987;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#23433;&#20840;&#39118;&#38505;&#21644;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#36825;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#65292;&#20294;&#36825;&#24182;&#19981;&#24635;&#26159;&#21487;&#34892;&#30340;&#12290;&#26368;&#36817;&#65292;&#35768;&#22810;&#20174;&#19994;&#32773;&#36716;&#21521;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#24265;&#20215;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#36890;&#36807;&#39044;&#35757;&#32451;&#23398;&#20064;&#19968;&#20010;&#36890;&#29992;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#21487;&#20197;&#31616;&#21333;&#22320;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#39069;&#22806;&#30340;&#32447;&#24615;&#23618;&#24182;&#20351;&#29992;&#26377;&#38480;&#30340;&#26631;&#35760;&#25968;&#25454;&#26469;&#24212;&#29992;&#20110;&#20010;&#24615;&#21270;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#36807;&#31243;&#20063;&#21487;&#33021;&#24341;&#21457;&#23545;&#25968;&#25454;&#27602;&#21270;&#25915;&#20987;&#30340;&#25285;&#24551;&#12290;&#20363;&#22914;&#65292;&#20219;&#24847;&#25968;&#25454;&#27602;&#21270;&#25915;&#20987;&#26088;&#22312;&#36890;&#36807;&#23558;&#23569;&#37327;&#27602;&#21270;&#25968;&#25454;&#27880;&#20837;&#35757;&#32451;&#38598;&#26469;&#38477;&#20302;&#27169;&#22411;&#25928;&#29992;&#65292;&#36825;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26500;&#25104;&#23433;&#20840;&#39118;&#38505;&#65292;&#20294;&#30446;&#21069;&#20165;&#22312;&#31471;&#21040;&#31471;&#30417;&#30563;&#23398;&#20064;&#20013;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#26412;&#25991;&#25193;&#23637;&#20102;&#23545;&#24212;&#29992;&#39044;&#35757;&#32451;&#30340;&#19979;&#28216;&#20219;&#21153;&#30340;&#20219;&#24847;&#25915;&#20987;&#23041;&#32961;&#30340;&#25506;&#35752;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12626v1 Announce Type: new  Abstract: Machine learning models have achieved great success in supervised learning tasks for end-to-end training, which requires a large amount of labeled data that is not always feasible. Recently, many practitioners have shifted to self-supervised learning methods that utilize cheap unlabeled data to learn a general feature extractor via pre-training, which can be further applied to personalized downstream tasks by simply training an additional linear layer with limited labeled data. However, such a process may also raise concerns regarding data poisoning attacks. For instance, indiscriminate data poisoning attacks, which aim to decrease model utility by injecting a small number of poisoned data into the training set, pose a security risk to machine learning models, but have only been studied for end-to-end supervised learning. In this paper, we extend the exploration of the threat of indiscriminate attacks on downstream tasks that apply pre-t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32039;&#20945;&#22411;NSGA-II&#31639;&#27861;&#65292;&#29992;&#20110;&#22810;&#30446;&#26631;&#29305;&#24449;&#36873;&#25321;&#65292;&#26088;&#22312;&#25552;&#39640;&#20998;&#31867;&#20934;&#30830;&#24230;&#24182;&#20943;&#23569;&#25152;&#36873;&#29305;&#24449;&#25968;&#37327;</title><link>https://arxiv.org/abs/2402.12625</link><description>&lt;p&gt;
&#22810;&#30446;&#26631;&#29305;&#24449;&#36873;&#25321;&#30340;&#32039;&#20945;&#22411;NSGA-II
&lt;/p&gt;
&lt;p&gt;
Compact NSGA-II for Multi-objective Feature Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12625
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32039;&#20945;&#22411;NSGA-II&#31639;&#27861;&#65292;&#29992;&#20110;&#22810;&#30446;&#26631;&#29305;&#24449;&#36873;&#25321;&#65292;&#26088;&#22312;&#25552;&#39640;&#20998;&#31867;&#20934;&#30830;&#24230;&#24182;&#20943;&#23569;&#25152;&#36873;&#29305;&#24449;&#25968;&#37327;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#36873;&#25321;&#26159;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#25454;&#25366;&#25496;&#20013;&#19968;&#39033;&#26114;&#36149;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#28040;&#38500;&#26080;&#20851;&#21644;&#20887;&#20313;&#29305;&#24449;&#12290;&#36825;&#26377;&#21161;&#20110;&#25552;&#39640;&#20998;&#31867;&#20934;&#30830;&#24230;&#65292;&#20197;&#21450;&#20998;&#31867;&#25110;&#29305;&#24449;&#36873;&#25321;&#21518;&#36827;&#34892;&#30340;&#20219;&#20309;&#20854;&#20182;&#21518;&#22788;&#29702;&#20219;&#21153;&#30340;&#39044;&#31639;&#21644;&#20869;&#23384;&#35201;&#27714;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#25105;&#20204;&#23558;&#29305;&#24449;&#36873;&#25321;&#23450;&#20041;&#20026;&#19968;&#20010;&#22810;&#30446;&#26631;&#20108;&#36827;&#21046;&#20248;&#21270;&#20219;&#21153;&#65292;&#20854;&#30446;&#26631;&#26159;&#26368;&#22823;&#21270;&#20998;&#31867;&#20934;&#30830;&#24230;&#21644;&#26368;&#23567;&#21270;&#36873;&#25321;&#30340;&#29305;&#24449;&#25968;&#37327;&#12290;&#20026;&#20102;&#36873;&#25321;&#26368;&#20248;&#29305;&#24449;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20108;&#36827;&#21046;&#32039;&#20945;&#22411;NSGA-II&#65288;CNSGA-II&#65289;&#31639;&#27861;&#12290;&#32039;&#20945;&#22411;&#20195;&#34920;&#23558;&#32676;&#20307;&#34920;&#31034;&#20026;&#27010;&#29575;&#20998;&#24067;&#65292;&#20197;&#22686;&#24378;&#36827;&#21270;&#31639;&#27861;&#19981;&#20165;&#26356;&#33410;&#30465;&#20869;&#23384;&#65292;&#36824;&#20943;&#23569;&#35780;&#20272;&#30340;&#25968;&#37327;&#12290;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#33509;&#24178;&#27010;&#29575;&#21521;&#37327;&#65292;&#32780;&#19981;&#26159;&#20445;&#25345;&#20004;&#20010;&#32676;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12625v1 Announce Type: new  Abstract: Feature selection is an expensive challenging task in machine learning and data mining aimed at removing irrelevant and redundant features. This contributes to an improvement in classification accuracy, as well as the budget and memory requirements for classification, or any other post-processing task conducted after feature selection. In this regard, we define feature selection as a multi-objective binary optimization task with the objectives of maximizing classification accuracy and minimizing the number of selected features. In order to select optimal features, we have proposed a binary Compact NSGA-II (CNSGA-II) algorithm. Compactness represents the population as a probability distribution to enhance evolutionary algorithms not only to be more memory-efficient but also to reduce the number of fitness evaluations. Instead of holding two populations during the optimization process, our proposed method uses several Probability Vectors (
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;Reflect-RL&#65292;&#20351;&#29992;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26469;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#36825;&#36807;&#31243;&#20013;&#24341;&#20837;&#20102;&#21453;&#23556;&#27169;&#22411;&#21327;&#21161;&#65292;&#24182;&#37319;&#29992;&#20102;&#36127;&#20363;&#29983;&#25104;&#12289;&#21333;&#25552;&#31034;&#21160;&#20316;&#26522;&#20030;&#21644;&#35838;&#31243;&#23398;&#20064;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.12621</link><description>&lt;p&gt;
Reflect-RL&#65306;&#20004;&#20010;&#29609;&#23478;&#22312;&#32447;RL&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Reflect-RL: Two-Player Online RL Fine-Tuning for LMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12621
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;Reflect-RL&#65292;&#20351;&#29992;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26469;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#36825;&#36807;&#31243;&#20013;&#24341;&#20837;&#20102;&#21453;&#23556;&#27169;&#22411;&#21327;&#21161;&#65292;&#24182;&#37319;&#29992;&#20102;&#36127;&#20363;&#29983;&#25104;&#12289;&#21333;&#25552;&#31034;&#21160;&#20316;&#26522;&#20030;&#21644;&#35838;&#31243;&#23398;&#20064;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#20010;&#39046;&#22495;&#23637;&#31034;&#20854;&#33021;&#21147;&#65292;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;&#38656;&#35201;&#22810;&#36718;&#20132;&#20114;&#30340;&#20219;&#21153;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#36825;&#20123;&#20219;&#21153;&#36890;&#24120;&#20855;&#26377;&#22797;&#26434;&#30340;&#21160;&#24577;&#24615;&#65292;&#22240;&#27492;&#20165;&#22312;&#26377;&#38480;&#30340;&#31163;&#32447;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#26080;&#27861;&#21462;&#24471;&#33391;&#22909;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#21482;&#26377;&#23569;&#25968;&#30740;&#31350;&#23581;&#35797;&#22312;&#20132;&#20114;&#24335;&#20915;&#31574;&#21046;&#23450;&#29615;&#22659;&#20869;&#30452;&#25509;&#23545;LM&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#26088;&#22312;&#21019;&#24314;&#19968;&#20010;&#22312;&#36825;&#20123;&#29615;&#22659;&#20013;&#20351;&#29992;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#23545;LM&#36827;&#34892;&#24494;&#35843;&#30340;&#26377;&#25928;&#26426;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Reflect-RL&#65292;&#19968;&#20010;&#20004;&#20010;&#29609;&#23478;&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#22312;&#32447;RL&#23545;LM&#36827;&#34892;&#24494;&#35843;&#65292;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#19968;&#20010;&#20923;&#32467;&#30340;&#21453;&#23556;&#27169;&#22411;&#36741;&#21161;&#31574;&#30053;&#27169;&#22411;&#12290;&#20026;&#20102;&#20026;&#28909;&#36523;SFT&#38454;&#27573;&#29983;&#25104;&#25968;&#25454;&#65292;&#25105;&#20204;&#20351;&#29992;&#36127;&#20363;&#29983;&#25104;&#26469;&#22686;&#24378;&#21453;&#23556;&#27169;&#22411;&#30340;&#32416;&#38169;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#21333;&#25552;&#31034;&#21160;&#20316;&#26522;&#20030;&#65292;&#24182;&#24212;&#29992;&#20102;&#35838;&#31243;&#23398;&#20064;&#35753;&#31574;&#30053;&#27169;&#22411;&#23398;&#20064;&#26356;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12621v1 Announce Type: cross  Abstract: As language models (LMs) demonstrate their capabilities in various fields, their application to tasks requiring multi-round interactions has become increasingly popular. These tasks usually have complex dynamics, so supervised fine-tuning (SFT) on a limited offline dataset does not yield good performance. However, only a few works attempted to directly train the LMs within interactive decision-making environments. We aim to create an effective mechanism to fine-tune LMs with online reinforcement learning (RL) in these environments. We propose Reflect-RL, a two-player system to fine-tune an LM using online RL, where a frozen reflection model assists the policy model. To generate data for the warm-up SFT stage, we use negative example generation to enhance the error-correction ability of the reflection model. Furthermore, we designed single-prompt action enumeration and applied curriculum learning to allow the policy model to learn more 
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#23433;&#20840;&#25361;&#25112;&#21450;&#23545;&#31574;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2402.12617</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#65306;&#25361;&#25112;&#19982;&#23545;&#31574;
&lt;/p&gt;
&lt;p&gt;
Generative AI Security: Challenges and Countermeasures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12617
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#23433;&#20840;&#25361;&#25112;&#21450;&#23545;&#31574;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12617v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#35768;&#22810;&#34892;&#19994;&#30340;&#19981;&#26029;&#25193;&#23637;&#24341;&#21457;&#20102;&#20154;&#20204;&#30340;&#20852;&#22859;&#21644;&#22686;&#21152;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25152;&#24102;&#26469;&#30340;&#29420;&#29305;&#23433;&#20840;&#25361;&#25112;&#65292;&#24182;&#27010;&#36848;&#20102;&#31649;&#29702;&#36825;&#20123;&#39118;&#38505;&#30340;&#28508;&#22312;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12617v1 Announce Type: cross  Abstract: Generative AI's expanding footprint across numerous industries has led to both excitement and increased scrutiny. This paper delves into the unique security challenges posed by Generative AI, and outlines potential research directions for managing these risks.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#20108;&#36827;&#21046;&#22810;&#30446;&#26631;&#22352;&#26631;&#25628;&#32034;&#65288;MOCS&#65289;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#35268;&#27169;&#29305;&#24449;&#36873;&#25321;&#38382;&#39064;&#65292;&#26159;&#39318;&#20010;&#22810;&#30446;&#26631;&#22352;&#26631;&#25628;&#32034;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.12616</link><description>&lt;p&gt;
&#22810;&#30446;&#26631;&#20108;&#36827;&#21046;&#22352;&#26631;&#25628;&#32034;&#29305;&#24449;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Multi-objective Binary Coordinate Search for Feature Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12616
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#20108;&#36827;&#21046;&#22810;&#30446;&#26631;&#22352;&#26631;&#25628;&#32034;&#65288;MOCS&#65289;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#35268;&#27169;&#29305;&#24449;&#36873;&#25321;&#38382;&#39064;&#65292;&#26159;&#39318;&#20010;&#22810;&#30446;&#26631;&#22352;&#26631;&#25628;&#32034;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#30417;&#30563;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#36873;&#25321;&#36866;&#24403;&#20294;&#31616;&#27905;&#30340;&#29305;&#24449;&#38598;&#20197;&#21306;&#20998;&#31867;&#21035;&#65292;&#22312;&#22788;&#29702;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#26102;&#25104;&#26412;&#39640;&#26114;&#12290;&#22240;&#27492;&#65292;&#29305;&#24449;&#36873;&#25321;&#24212;&#26088;&#22312;&#26082;&#20943;&#23569;&#36873;&#25321;&#30340;&#29305;&#24449;&#25968;&#37327;&#21448;&#26368;&#22823;&#21270;&#20998;&#31867;&#20934;&#30830;&#24615;&#65292;&#25110;&#20219;&#20309;&#20854;&#20182;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#20851;&#38190;&#20219;&#21153;&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#35745;&#31639;&#37327;&#26497;&#22823;&#65292;&#24182;&#19988;&#38656;&#35201;&#38750;&#24120;&#39640;&#25928;&#30340;&#31639;&#27861;&#25165;&#33021;&#36890;&#36807;&#26377;&#38480;&#25968;&#37327;&#30340;&#36866;&#24212;&#24230;&#35780;&#20272;&#36798;&#21040;&#19968;&#32452;&#26368;&#20339;&#29305;&#24449;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20108;&#36827;&#21046;&#22810;&#30446;&#26631;&#22352;&#26631;&#25628;&#32034;&#65288;MOCS&#65289;&#31639;&#27861;&#26469;&#35299;&#20915;&#22823;&#35268;&#27169;&#29305;&#24449;&#36873;&#25321;&#38382;&#39064;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#31639;&#27861;&#26159;&#39318;&#20010;&#22810;&#30446;&#26631;&#22352;&#26631;&#25628;&#32034;&#31639;&#27861;&#12290;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#32763;&#36716;&#24085;&#32047;&#25176;&#21069;&#27839;&#20505;&#36873;&#35299;&#30340;&#21464;&#37327;&#26469;&#29983;&#25104;&#26032;&#20010;&#20307;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12616v1 Announce Type: new  Abstract: A supervised feature selection method selects an appropriate but concise set of features to differentiate classes, which is highly expensive for large-scale datasets. Therefore, feature selection should aim at both minimizing the number of selected features and maximizing the accuracy of classification, or any other task. However, this crucial task is computationally highly demanding on many real-world datasets and requires a very efficient algorithm to reach a set of optimal features with a limited number of fitness evaluations. For this purpose, we have proposed the binary multi-objective coordinate search (MOCS) algorithm to solve large-scale feature selection problems. To the best of our knowledge, the proposed algorithm in this paper is the first multi-objective coordinate search algorithm. In this method, we generate new individuals by flipping a variable of the candidate solutions on the Pareto front. This enables us to investigat
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#21452;&#24120;&#25968;&#23884;&#20837;&#27169;&#22411;&#65288;CCEM&#65289;&#20197;&#29702;&#35770;&#20998;&#26512;&#20351;&#29992;Sigmoid Loss&#22312;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#26377;&#21487;&#33021;&#25552;&#20379;&#26356;&#39640;&#25928;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.12613</link><description>&lt;p&gt;
&#20351;&#29992;Sigmoid Loss&#36827;&#34892;&#23545;&#27604;&#23398;&#20064;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Analysis of Using Sigmoid Loss for Contrastive Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12613
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#21452;&#24120;&#25968;&#23884;&#20837;&#27169;&#22411;&#65288;CCEM&#65289;&#20197;&#29702;&#35770;&#20998;&#26512;&#20351;&#29992;Sigmoid Loss&#22312;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#26377;&#21487;&#33021;&#25552;&#20379;&#26356;&#39640;&#25928;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#19968;&#20010;&#37325;&#35201;&#30340;&#20998;&#25903;&#25968;&#24180;&#12290;&#29305;&#21035;&#26159;&#65292;&#23558;&#23545;&#27604;&#23398;&#20064;&#24212;&#29992;&#20110;&#22823;&#37327;&#24102;&#26631;&#39064;&#30340;&#22270;&#29255;&#38598;&#30340;CLIP&#24341;&#36215;&#20102;&#24456;&#22823;&#20851;&#27880;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;SigLIP&#65292;CLIP&#30340;&#19968;&#31181;&#21464;&#20307;&#65292;&#23427;&#20351;&#29992;sigmoid loss&#32780;&#19981;&#26159;&#26631;&#20934;&#30340;InfoNCE loss&#12290;SigLIP&#36890;&#36807;&#28040;&#38500;&#23545;&#20840;&#23616;&#35270;&#22270;&#30340;&#38656;&#27714;&#65292;&#20197;&#26356;&#26377;&#25928;&#30340;&#26041;&#24335;&#36798;&#21040;&#19982;CLIP&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23545;&#27604;&#23398;&#20064;&#20013;&#20351;&#29992;sigmoid loss&#30340;&#29702;&#35770;&#29702;&#35299;&#23578;&#26410;&#34987;&#20805;&#20998;&#25506;&#35752;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#23398;&#20064;&#23884;&#20837;&#30340;&#20960;&#20309;&#32467;&#26500;&#30340;&#35282;&#24230;&#65292;&#23545;&#22312;&#23545;&#27604;&#23398;&#20064;&#20013;&#20351;&#29992;sigmoid loss&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21452;&#24120;&#25968;&#23884;&#20837;&#27169;&#22411;&#65288;CCEM&#65289;&#65292;&#19968;&#20010;&#36890;&#36807;&#21333;&#20010;&#21464;&#37327;&#26469;&#21442;&#25968;&#21270;&#21508;&#31181;&#20247;&#25152;&#21608;&#30693;&#30340;&#23884;&#20837;&#32467;&#26500;&#30340;&#26694;&#26550;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25152;&#25552;&#20986;&#30340;CCEM&#34987;&#35777;&#26126;&#21253;&#21547;&#20102;t
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12613v1 Announce Type: new  Abstract: Contrastive learning has emerged as a prominent branch of self-supervised learning for several years. Especially, CLIP, which applies contrastive learning to large sets of captioned images, has garnered significant attention. Recently, SigLIP, a variant of CLIP, has been proposed, which uses the sigmoid loss instead of the standard InfoNCE loss. SigLIP achieves the performance comparable to CLIP in a more efficient manner by eliminating the need for a global view. However, theoretical understanding of using the sigmoid loss in contrastive learning is underexplored. In this paper, we provide a theoretical analysis of using the sigmoid loss in contrastive learning, in the perspective of the geometric structure of learned embeddings. First, we propose the double-Constant Embedding Model (CCEM), a framework for parameterizing various well-known embedding structures by a single variable. Interestingly, the proposed CCEM is proven to contain t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#34394;&#25311;&#20256;&#24863;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#30456;&#20851;&#21464;&#37327;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#35774;&#35745;&#20102;GgNet&#26550;&#26500;&#65292;&#29992;&#20110;&#25512;&#26029;&#26410;&#35266;&#27979;&#20449;&#36947;&#30340;&#20540;&#12290;</title><link>https://arxiv.org/abs/2402.12598</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#30340;&#34394;&#25311;&#20256;&#24863;&#65306;&#26469;&#33258;&#31232;&#30095;&#21644;&#37096;&#20998;&#22810;&#21464;&#37327;&#35266;&#27979;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Graph-based Virtual Sensing from Sparse and Partial Multivariate Observations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12598
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#34394;&#25311;&#20256;&#24863;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#30456;&#20851;&#21464;&#37327;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#35774;&#35745;&#20102;GgNet&#26550;&#26500;&#65292;&#29992;&#20110;&#25512;&#26029;&#26410;&#35266;&#27979;&#20449;&#36947;&#30340;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34394;&#25311;&#20256;&#24863;&#25216;&#26415;&#20801;&#35768;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#19981;&#21516;&#20301;&#32622;&#30340;&#29289;&#29702;&#20256;&#24863;&#22120;&#30340;&#26102;&#31354;&#27979;&#37327;&#26469;&#25512;&#26029;&#26032;&#20301;&#32622;&#30340;&#20449;&#21495;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25104;&#26412;&#25110;&#20854;&#20182;&#38480;&#21046;&#23548;&#33268;&#20256;&#24863;&#22120;&#35206;&#30422;&#33539;&#22260;&#21464;&#24471;&#31232;&#30095;&#65292;&#26080;&#27861;&#21033;&#29992;&#29289;&#29702;&#25509;&#36817;&#24615;&#25903;&#25345;&#25554;&#20540;&#12290;&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#30446;&#26631;&#21464;&#37327;&#19982;&#19968;&#32452;&#30456;&#20851;&#21464;&#37327;&#65288;&#21327;&#21464;&#37327;&#65289;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#26469;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#36825;&#20123;&#21327;&#21464;&#37327;&#21487;&#20197;&#32463;&#24120;&#19982;&#24863;&#20852;&#36259;&#30340;&#27599;&#20010;&#20301;&#32622;&#30456;&#20851;&#32852;&#12290;&#20174;&#36825;&#20010;&#35282;&#24230;&#26469;&#30475;&#65292;&#21327;&#21464;&#37327;&#25552;&#20379;&#20102;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#65292;&#38382;&#39064;&#22312;&#20110;&#36890;&#36807;&#21033;&#29992;&#20854;&#20182;&#20301;&#32622;&#30340;&#35266;&#27979;&#32467;&#26524;&#25512;&#26029;&#26410;&#35266;&#27979;&#20449;&#36947;&#30340;&#20540;&#65292;&#20197;&#20102;&#35299;&#36825;&#20123;&#21464;&#37327;&#30340;&#30456;&#20851;&#24615;&#22914;&#20309;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#26469;&#21033;&#29992;&#36825;&#31181;&#20851;&#31995;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#21517;&#20026;GgNet&#30340;&#22270;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#26469;&#23454;&#29616;&#35813;&#26694;&#26550;&#12290;&#25552;&#20986;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;p
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12598v1 Announce Type: cross  Abstract: Virtual sensing techniques allow for inferring signals at new unmonitored locations by exploiting spatio-temporal measurements coming from physical sensors at different locations. However, as the sensor coverage becomes sparse due to costs or other constraints, physical proximity cannot be used to support interpolation. In this paper, we overcome this challenge by leveraging dependencies between the target variable and a set of correlated variables (covariates) that can frequently be associated with each location of interest. From this viewpoint, covariates provide partial observability, and the problem consists of inferring values for unobserved channels by exploiting observations at other locations to learn how such variables can correlate. We introduce a novel graph-based methodology to exploit such relationships and design a graph deep learning architecture, named GgNet, implementing the framework. The proposed approach relies on p
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#22823;&#35268;&#27169;MIMO&#31995;&#32479;&#20013;&#24212;&#29992;&#22522;&#20110;&#25130;&#26029;&#22810;&#39033;&#24335;&#23637;&#24320;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#20449;&#21495;&#26816;&#27979;&#30340;&#25928;&#29575;&#65292;&#24182;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24615;</title><link>https://arxiv.org/abs/2402.12595</link><description>&lt;p&gt;
&#22522;&#20110;&#25130;&#26029;&#22810;&#39033;&#24335;&#23637;&#24320;&#30340;&#22823;&#35268;&#27169;MIMO&#20013;&#30340;&#26816;&#27979;&#65306;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#39537;&#21160;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Truncated Polynomial Expansion-Based Detection in Massive MIMO: A Model-Driven Deep Learning Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12595
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#22823;&#35268;&#27169;MIMO&#31995;&#32479;&#20013;&#24212;&#29992;&#22522;&#20110;&#25130;&#26029;&#22810;&#39033;&#24335;&#23637;&#24320;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#20449;&#21495;&#26816;&#27979;&#30340;&#25928;&#29575;&#65292;&#24182;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#25130;&#26029;&#22810;&#39033;&#24335;&#23637;&#24320;&#65288;TPE&#65289;&#39640;&#25928;&#35745;&#31639;Hermitian&#30697;&#38453;&#30340;&#36870;&#12290;&#25105;&#20204;&#30340;&#22522;&#20110;&#27169;&#22411;&#39537;&#21160;&#30340;&#26041;&#27861;&#28041;&#21450;&#22312;&#31163;&#32447;&#35757;&#32451;&#36807;&#31243;&#20013;&#20248;&#21270;&#32473;&#23450;&#25968;&#37327;TPE&#39033;&#30340;&#31995;&#25968;&#12290;&#25105;&#20204;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#19978;&#34892;&#22823;&#35268;&#27169;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#65288;MIMO&#65289;&#31995;&#32479;&#20013;&#30340;&#20449;&#21495;&#26816;&#27979;&#65292;&#20854;&#20013;&#32447;&#24615;&#26816;&#27979;&#22120;&#65292;&#22914;&#38646;&#24378;&#36843;&#65288;ZF&#65289;&#21644;&#26368;&#23567;&#22343;&#26041;&#35823;&#24046;&#65288;MMSE&#65289;&#25152;&#38656;&#30340;&#30697;&#38453;&#36870;&#36816;&#31639;&#65292;&#20351;&#29992;TPE&#36827;&#34892;&#36817;&#20284;&#12290;&#25105;&#20204;&#30340;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;TPE&#26041;&#27861;&#22312;&#28176;&#36817;&#25910;&#25947;&#36895;&#24230;&#26041;&#38754;&#20248;&#20110;&#20855;&#26377;&#26368;&#20339;&#31995;&#25968;&#30340;&#20256;&#32479;TPE&#26041;&#27861;&#65292;&#24182;&#38477;&#20302;&#20102;&#22312;&#32447;&#26816;&#27979;&#38454;&#27573;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#23613;&#31649;&#20197;&#29306;&#29298;&#31163;&#32447;&#35757;&#32451;&#38454;&#27573;&#20026;&#20195;&#20215;&#12290;&#28982;&#32780;&#65292;&#26377;&#38480;&#25968;&#37327;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#23548;&#33268;&#19968;&#31181;&#36805;&#36895;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12595v1 Announce Type: cross  Abstract: In this paper, we propose a deep learning (DL)-based approach for efficiently computing the inverse of Hermitian matrices using truncated polynomial expansion (TPE). Our model-driven approach involves optimizing the coefficients of the TPE during an offline training procedure for a given number of TPE terms. We apply this method to signal detection in uplink massive multiple-input multiple-output (MIMO) systems, where the matrix inverse operation required by linear detectors, such as zero-forcing (ZF) and minimum mean square error (MMSE), is approximated using TPE. Our simulation results demonstrate that the proposed learned TPE-based method outperforms the conventional TPE method with optimal coefficients in terms of asymptotic convergence speed and reduces the computational complexity of the online detection stage, albeit at the expense of the offline training stage. However, the limited number of trainable parameters leads to a swif
&lt;/p&gt;</description></item><item><title>FairProof&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38646;&#30693;&#35782;&#35777;&#26126;&#26469;&#20844;&#24320;&#39564;&#35777;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20844;&#24179;&#24615;&#30340;&#31995;&#32479;&#65292;&#21516;&#26102;&#20445;&#25345;&#26426;&#23494;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;ZKPs&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#20844;&#24179;&#24615;&#35748;&#35777;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.12572</link><description>&lt;p&gt;
FairProof&#65306;&#31070;&#32463;&#32593;&#32476;&#30340;&#26426;&#23494;&#21644;&#21487;&#35748;&#35777;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
FairProof : Confidential and Certifiable Fairness for Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12572
&lt;/p&gt;
&lt;p&gt;
FairProof&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38646;&#30693;&#35782;&#35777;&#26126;&#26469;&#20844;&#24320;&#39564;&#35777;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20844;&#24179;&#24615;&#30340;&#31995;&#32479;&#65292;&#21516;&#26102;&#20445;&#25345;&#26426;&#23494;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;ZKPs&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#20844;&#24179;&#24615;&#35748;&#35777;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#31038;&#20250;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#28982;&#32780;&#27861;&#24459;&#21644;&#38544;&#31169;&#38382;&#39064;&#35201;&#27714;&#36825;&#20123;&#27169;&#22411;&#24448;&#24448;&#38656;&#35201;&#20445;&#23494;&#12290;&#22240;&#27492;&#65292;&#28040;&#36153;&#32773;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#23646;&#24615;&#36234;&#26469;&#36234;&#19981;&#20449;&#20219;&#65292;&#28040;&#36153;&#32773;&#36890;&#24120;&#26159;&#27169;&#22411;&#39044;&#27979;&#30340;&#25509;&#25910;&#32773;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FairProof - &#19968;&#31181;&#31995;&#32479;&#65292;&#20351;&#29992;&#38646;&#30693;&#35782;&#35777;&#26126;&#65288;&#19968;&#31181;&#23494;&#30721;&#21407;&#35821;&#65289;&#26469;&#20844;&#24320;&#39564;&#35777;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#26426;&#23494;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#21512;&#20110;ZKPs&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#20844;&#24179;&#24615;&#35748;&#35777;&#31639;&#27861;&#65292;&#24182;&#22312;&#35813;&#31995;&#32479;&#20013;&#20351;&#29992;&#12290;&#25105;&#20204;&#22312;Gnark&#20013;&#23454;&#29616;&#20102;FairProof&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31995;&#32479;&#26159;&#23454;&#38469;&#21487;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12572v1 Announce Type: cross  Abstract: Machine learning models are increasingly used in societal applications, yet legal and privacy concerns demand that they very often be kept confidential. Consequently, there is a growing distrust about the fairness properties of these models in the minds of consumers, who are often at the receiving end of model predictions. To this end, we propose FairProof - a system that uses Zero-Knowledge Proofs (a cryptographic primitive) to publicly verify the fairness of a model, while maintaining confidentiality. We also propose a fairness certification algorithm for fully-connected neural networks which is befitting to ZKPs and is used in this system. We implement FairProof in Gnark and demonstrate empirically that our system is practically feasible.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#23398;&#20064;&#34920;&#31034;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#30340;&#31639;&#27861;&#65292;&#20026;&#30446;&#26631;&#20219;&#21153;&#23398;&#21040;&#30340;&#31574;&#30053;&#30340;&#27425;&#20248;&#24615;&#24314;&#31435;&#20102;&#25968;&#25454;&#30456;&#20851;&#30340;&#19978;&#30028;&#12290;</title><link>https://arxiv.org/abs/2402.12570</link><description>&lt;p&gt;
&#31163;&#32447;&#22810;&#20219;&#21153;&#36716;&#31227;&#24378;&#21270;&#23398;&#20064;&#19982;&#34920;&#24449;&#24809;&#32602;
&lt;/p&gt;
&lt;p&gt;
Offline Multi-task Transfer RL with Representational Penalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12570
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#23398;&#20064;&#34920;&#31034;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#30340;&#31639;&#27861;&#65292;&#20026;&#30446;&#26631;&#20219;&#21153;&#23398;&#21040;&#30340;&#31574;&#30053;&#30340;&#27425;&#20248;&#24615;&#24314;&#31435;&#20102;&#25968;&#25454;&#30456;&#20851;&#30340;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#34920;&#31034;&#36716;&#31227;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#23398;&#20064;&#32773;&#21487;&#20197;&#35775;&#38382;&#20107;&#20808;&#25910;&#38598;&#30340;&#22810;&#20010;&#28304;&#20219;&#21153;&#30340;&#24207;&#21015;&#25968;&#25454;&#65292;&#24182;&#26088;&#22312;&#23398;&#20064;&#19968;&#20010;&#20849;&#20139;&#34920;&#31034;&#65292;&#20197;&#29992;&#20110;&#20026;&#30446;&#26631;&#20219;&#21153;&#25214;&#21040;&#19968;&#20010;&#33391;&#22909;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#26469;&#35745;&#31639;&#23398;&#21040;&#30340;&#34920;&#31034;&#30340;&#36880;&#28857;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#65292;&#24182;&#20026;&#30446;&#26631;&#20219;&#21153;&#23398;&#21040;&#30340;&#31574;&#30053;&#30340;&#27425;&#20248;&#24615;&#24314;&#31435;&#20102;&#19968;&#20010;&#25968;&#25454;&#30456;&#20851;&#30340;&#19978;&#30028;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#21033;&#29992;&#28304;&#20219;&#21153;&#30340;&#38598;&#20307;&#25506;&#32034;&#26469;&#20943;&#36731;&#23569;&#25968;&#20219;&#21153;&#22312;&#26576;&#20123;&#28857;&#30340;&#35206;&#30422;&#19981;&#36275;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#38656;&#35201;&#24179;&#22343;&#35206;&#30422;&#33391;&#22909;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12570v1 Announce Type: new  Abstract: We study the problem of representation transfer in offline Reinforcement Learning (RL), where a learner has access to episodic data from a number of source tasks collected a priori, and aims to learn a shared representation to be used in finding a good policy for a target task. Unlike in online RL where the agent interacts with the environment while learning a policy, in the offline setting there cannot be such interactions in either the source tasks or the target task; thus multi-task offline RL can suffer from incomplete coverage.   We propose an algorithm to compute pointwise uncertainty measures for the learnt representation, and establish a data-dependent upper bound for the suboptimality of the learnt policy for the target task. Our algorithm leverages the collective exploration done by source tasks to mitigate poor coverage at some points by a few tasks, thus overcoming the limitation of needing uniformly good coverage for a meani
&lt;/p&gt;</description></item><item><title>GenAudit&#26159;&#19968;&#20010;&#24037;&#20855;&#65292;&#36890;&#36807;&#20462;&#35746;&#25110;&#21024;&#38500;&#26410;&#34987;&#21442;&#32771;&#25991;&#29486;&#25903;&#25345;&#30340;&#22768;&#26126;&#65292;&#24182;&#25552;&#20379;&#26469;&#33258;&#21442;&#32771;&#25991;&#29486;&#30340;&#35777;&#25454;&#65292;&#24110;&#21161;&#20462;&#22797;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#20013;&#30340;&#20107;&#23454;&#38169;&#35823;&#12290;</title><link>https://arxiv.org/abs/2402.12566</link><description>&lt;p&gt;
GenAudit&#65306;&#21033;&#29992;&#35777;&#25454;&#20462;&#22797;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#20013;&#30340;&#20107;&#23454;&#38169;&#35823;
&lt;/p&gt;
&lt;p&gt;
GenAudit: Fixing Factual Errors in Language Model Outputs with Evidence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12566
&lt;/p&gt;
&lt;p&gt;
GenAudit&#26159;&#19968;&#20010;&#24037;&#20855;&#65292;&#36890;&#36807;&#20462;&#35746;&#25110;&#21024;&#38500;&#26410;&#34987;&#21442;&#32771;&#25991;&#29486;&#25903;&#25345;&#30340;&#22768;&#26126;&#65292;&#24182;&#25552;&#20379;&#26469;&#33258;&#21442;&#32771;&#25991;&#29486;&#30340;&#35777;&#25454;&#65292;&#24110;&#21161;&#20462;&#22797;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#20013;&#30340;&#20107;&#23454;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLMs&#21363;&#20351;&#21487;&#20197;&#35775;&#38382;&#21442;&#32771;&#25991;&#26723;&#65292;&#20063;&#21487;&#33021;&#29983;&#25104;&#20107;&#23454;&#19981;&#20934;&#30830;&#30340;&#38472;&#36848;&#12290;&#22312;&#39640;&#39118;&#38505;&#24212;&#29992;&#20013;&#65288;&#20363;&#22914;&#22522;&#20110;&#25991;&#26723;&#30340;&#21307;&#30103;&#20445;&#20581;&#25110;&#37329;&#34701;&#38382;&#31572;&#65289;&#65292;&#36825;&#26679;&#30340;&#38169;&#35823;&#21487;&#33021;&#20855;&#26377;&#21361;&#38505;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;GenAudit -- &#19968;&#20010;&#26088;&#22312;&#24110;&#21161;&#26816;&#26597;&#22522;&#20110;&#25991;&#26723;&#20219;&#21153;&#35821;&#35328;&#27169;&#22411;&#21709;&#24212;&#30340;&#24037;&#20855;&#12290;GenAudit&#36890;&#36807;&#20462;&#35746;&#25110;&#21024;&#38500;&#26410;&#34987;&#21442;&#32771;&#25991;&#26723;&#25903;&#25345;&#30340;&#22768;&#26126;&#65292;&#21516;&#26102;&#20026;&#30475;&#20284;&#34987;&#35777;&#25454;&#25903;&#25345;&#30340;&#20107;&#23454;&#25552;&#20379;&#26469;&#33258;&#21442;&#32771;&#25991;&#29486;&#30340;&#35777;&#25454;&#65292;&#26469;&#24314;&#35758;&#20462;&#25913;LLM&#21709;&#24212;&#12290;&#25105;&#20204;&#35757;&#32451;&#27169;&#22411;&#26469;&#25191;&#34892;&#36825;&#20123;&#20219;&#21153;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#20132;&#20114;&#30028;&#38754;&#65292;&#21521;&#29992;&#25143;&#21576;&#29616;&#24314;&#35758;&#30340;&#20462;&#25913;&#21644;&#35777;&#25454;&#12290;&#36890;&#36807;&#20154;&#24037;&#35780;&#20998;&#21592;&#30340;&#20840;&#38754;&#35780;&#20272;&#26174;&#31034;&#65292;GenAudit&#22312;&#24635;&#32467;&#19981;&#21516;&#39046;&#22495;&#25991;&#26723;&#26102;&#33021;&#22815;&#26816;&#27979;&#20986;8&#31181;&#19981;&#21516;&#30340;LLM&#36755;&#20986;&#20013;&#30340;&#38169;&#35823;&#12290;&#20026;&#30830;&#20445;&#31995;&#32479;&#33021;&#22815;&#26631;&#35760;&#22823;&#22810;&#25968;&#38169;&#35823;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#38169;&#35823;&#21484;&#22238;&#29575;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#23545;&#39044;&#22788;&#29702;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12566v1 Announce Type: new  Abstract: LLMs can generate factually incorrect statements even when provided access to reference documents. Such errors can be dangerous in high-stakes applications (e.g., document-grounded QA for healthcare or finance). We present GenAudit -- a tool intended to assist fact-checking LLM responses for document-grounded tasks. GenAudit suggests edits to the LLM response by revising or removing claims that are not supported by the reference document, and also presents evidence from the reference for facts that do appear to have support. We train models to execute these tasks, and design an interactive interface to present suggested edits and evidence to users. Comprehensive evaluation by human raters shows that GenAudit can detect errors in 8 different LLM outputs when summarizing documents from diverse domains. To ensure that most errors are flagged by the system, we propose a method that can increase the error recall while minimizing impact on pre
&lt;/p&gt;</description></item><item><title>&#22312;&#32771;&#34385;&#39038;&#23458;&#20215;&#26684;&#26399;&#26395;&#23545;&#24403;&#21069;&#20215;&#26684;&#21453;&#24212;&#30340;&#24773;&#20917;&#19979;&#65292;&#30740;&#31350;&#20102;&#19968;&#31181;&#20855;&#26377;&#38271;&#26399;&#21442;&#32771;&#25928;&#24212;&#30340;&#21160;&#24577;&#23450;&#20215;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21442;&#32771;&#20215;&#26684;&#26426;&#21046;&#65292;&#23637;&#31034;&#22312;&#35813;&#26426;&#21046;&#19979;&#38477;&#20215;&#25919;&#31574;&#20960;&#20046;&#26159;&#26368;&#20248;&#30340;&#65292;&#20026;&#32447;&#24615;&#38656;&#27714;&#27169;&#22411;&#25552;&#20379;&#20102;&#36817;&#20284;&#26368;&#20248;&#38477;&#20215;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.12562</link><description>&lt;p&gt;
&#20855;&#26377;&#38271;&#26399;&#21442;&#32771;&#25928;&#24212;&#30340;&#21160;&#24577;&#23450;&#20215;&#19982;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Dynamic Pricing and Learning with Long-term Reference Effects
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12562
&lt;/p&gt;
&lt;p&gt;
&#22312;&#32771;&#34385;&#39038;&#23458;&#20215;&#26684;&#26399;&#26395;&#23545;&#24403;&#21069;&#20215;&#26684;&#21453;&#24212;&#30340;&#24773;&#20917;&#19979;&#65292;&#30740;&#31350;&#20102;&#19968;&#31181;&#20855;&#26377;&#38271;&#26399;&#21442;&#32771;&#25928;&#24212;&#30340;&#21160;&#24577;&#23450;&#20215;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21442;&#32771;&#20215;&#26684;&#26426;&#21046;&#65292;&#23637;&#31034;&#22312;&#35813;&#26426;&#21046;&#19979;&#38477;&#20215;&#25919;&#31574;&#20960;&#20046;&#26159;&#26368;&#20248;&#30340;&#65292;&#20026;&#32447;&#24615;&#38656;&#27714;&#27169;&#22411;&#25552;&#20379;&#20102;&#36817;&#20284;&#26368;&#20248;&#38477;&#20215;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#21160;&#24577;&#23450;&#20215;&#38382;&#39064;&#65292;&#20854;&#20013;&#39038;&#23458;&#23545;&#24403;&#21069;&#20215;&#26684;&#30340;&#21453;&#24212;&#21463;&#21040;&#39038;&#23458;&#20215;&#26684;&#26399;&#26395;&#65292;&#21363;&#21442;&#32771;&#20215;&#26684;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26032;&#39062;&#30340;&#21442;&#32771;&#20215;&#26684;&#26426;&#21046;&#65292;&#20854;&#20013;&#21442;&#32771;&#20215;&#26684;&#26159;&#21334;&#23478;&#36807;&#21435;&#25552;&#20379;&#30340;&#20215;&#26684;&#30340;&#24179;&#22343;&#20540;&#12290;&#19982;&#26356;&#24120;&#35265;&#30340;&#25351;&#25968;&#24179;&#28369;&#26426;&#21046;&#30456;&#21453;&#65292;&#22312;&#25105;&#20204;&#30340;&#21442;&#32771;&#20215;&#26684;&#26426;&#21046;&#20013;&#65292;&#21334;&#23478;&#25552;&#20379;&#30340;&#20215;&#26684;&#23545;&#26410;&#26469;&#39038;&#23458;&#26399;&#26395;&#26377;&#26356;&#38271;&#26399;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#23637;&#31034;&#65292;&#22312;&#36825;&#31181;&#26426;&#21046;&#19979;&#65292;&#38477;&#20215;&#25919;&#31574;&#20960;&#20046;&#26159;&#26368;&#20248;&#30340;&#65292;&#19981;&#21463;&#27169;&#22411;&#21442;&#25968;&#30340;&#24433;&#21709;&#12290;&#36825;&#31526;&#21512;&#19968;&#20010;&#24120;&#35265;&#30340;&#30452;&#35273;&#65292;&#21363;&#21334;&#23478;&#21487;&#20197;&#36890;&#36807;&#20197;&#36739;&#39640;&#30340;&#20215;&#26684;&#20986;&#21457;&#65292;&#28982;&#21518;&#36880;&#28176;&#38477;&#20302;&#20215;&#26684;&#65292;&#22240;&#20026;&#39038;&#23458;&#20250;&#35273;&#24471;&#20182;&#20204;&#27491;&#22312;&#36141;&#20080;&#36890;&#24120;&#26356;&#26114;&#36149;&#30340;&#29289;&#21697;&#19978;&#30340;&#20415;&#23452;&#36135;&#12290;&#23545;&#20110;&#32447;&#24615;&#38656;&#27714;&#27169;&#22411;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#36817;&#20284;&#26368;&#20248;&#38477;&#20215;&#31574;&#30053;&#30340;&#35814;&#32454;&#29305;&#24449;&#24615;&#25551;&#36848;&#20197;&#21450;&#19968;&#20010;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12562v1 Announce Type: new  Abstract: We consider a dynamic pricing problem where customer response to the current price is impacted by the customer price expectation, aka reference price. We study a simple and novel reference price mechanism where reference price is the average of the past prices offered by the seller. As opposed to the more commonly studied exponential smoothing mechanism, in our reference price mechanism the prices offered by seller have a longer term effect on the future customer expectations.   We show that under this mechanism, a markdown policy is near-optimal irrespective of the parameters of the model. This matches the common intuition that a seller may be better off by starting with a higher price and then decreasing it, as the customers feel like they are getting bargains on items that are ordinarily more expensive. For linear demand models, we also provide a detailed characterization of the near-optimal markdown policy along with an efficient way
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#35780;&#20272;&#20102;170&#20010;&#22269;&#23478;&#30340;&#39278;&#39135;&#20064;&#24815;&#65292;&#21457;&#29616;&#32933;&#32982;&#21644;&#39640;&#33026;&#32938;&#25668;&#20837;&#19982;COVID-19&#27515;&#20129;&#29575;&#36739;&#39640;&#30340;&#22269;&#23478;&#30456;&#20851;&#65292;&#32780;&#35895;&#29289;&#28040;&#36153;&#27700;&#24179;&#36739;&#39640;&#30340;&#22269;&#23478;&#21017;&#26377;&#36739;&#20302;&#30340;&#27515;&#20129;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.12558</link><description>&lt;p&gt;
&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#35780;&#20272;&#19982;COVID-19&#27515;&#20129;&#29575;&#30456;&#20851;&#30340;&#22269;&#23478;&#39278;&#39135;&#20064;&#24815;
&lt;/p&gt;
&lt;p&gt;
Evaluation of Country Dietary Habits Using Machine Learning Techniques in Relation to Deaths from COVID-19
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12558
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#35780;&#20272;&#20102;170&#20010;&#22269;&#23478;&#30340;&#39278;&#39135;&#20064;&#24815;&#65292;&#21457;&#29616;&#32933;&#32982;&#21644;&#39640;&#33026;&#32938;&#25668;&#20837;&#19982;COVID-19&#27515;&#20129;&#29575;&#36739;&#39640;&#30340;&#22269;&#23478;&#30456;&#20851;&#65292;&#32780;&#35895;&#29289;&#28040;&#36153;&#27700;&#24179;&#36739;&#39640;&#30340;&#22269;&#23478;&#21017;&#26377;&#36739;&#20302;&#30340;&#27515;&#20129;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
COVID-19&#30142;&#30149;&#24050;&#32463;&#24433;&#21709;&#20102;&#19990;&#30028;&#19978;&#20960;&#20046;&#27599;&#20010;&#22269;&#23478;&#12290;&#22823;&#37327;&#24863;&#26579;&#32773;&#21644;&#19981;&#21516;&#22269;&#23478;&#20043;&#38388;&#30340;&#27515;&#20129;&#29575;&#24046;&#24322;&#24341;&#21457;&#20102;&#35768;&#22810;&#20851;&#20110;&#20351;&#30149;&#27602;&#22312;&#26576;&#20123;&#22320;&#26041;&#22914;&#27492;&#33268;&#21629;&#30340;&#20851;&#38190;&#22240;&#32032;&#30340;&#20551;&#35774;&#12290;&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;170&#20010;&#22269;&#23478;&#30340;&#39278;&#39135;&#20064;&#24815;&#65292;&#20197;&#25214;&#20986;&#36825;&#20123;&#20064;&#24815;&#19982;COVID-19&#24341;&#36215;&#30340;&#27515;&#20129;&#29575;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#26681;&#25454;23&#31181;&#19981;&#21516;&#39135;&#29289;&#30340;&#33026;&#32938;&#12289;&#33021;&#37327;&#21644;&#34507;&#30333;&#36136;&#30340;&#19981;&#21516;&#20998;&#24067;&#65292;&#20197;&#21450;&#20197;&#21315;&#20811;&#20026;&#21333;&#20301;&#30340;&#25668;&#20837;&#37327;&#65292;&#23558;&#22269;&#23478;&#36827;&#34892;&#20998;&#32452;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#32933;&#32982;&#21644;&#39640;&#33026;&#32938;&#25668;&#20837;&#22312;&#27515;&#20129;&#29575;&#26368;&#39640;&#30340;&#22269;&#23478;&#20013;&#20986;&#29616;&#65292;&#32780;&#27515;&#20129;&#29575;&#36739;&#20302;&#30340;&#22269;&#23478;&#21017;&#20276;&#38543;&#30528;&#26356;&#39640;&#30340;&#35895;&#29289;&#28040;&#36153;&#27700;&#24179;&#20197;&#21450;&#36739;&#20302;&#30340;&#24635;&#20307;&#24179;&#22343;&#21315;&#21345;&#25668;&#20837;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12558v1 Announce Type: new  Abstract: COVID-19 disease has affected almost every country in the world. The large number of infected people and the different mortality rates between countries has given rise to many hypotheses about the key points that make the virus so lethal in some places. In this study, the eating habits of 170 countries were evaluated in order to find correlations between these habits and mortality rates caused by COVID-19 using machine learning techniques that group the countries together according to the different distribution of fat, energy, and protein across 23 different types of food, as well as the amount ingested in kilograms. Results shown how obesity and the high consumption of fats appear in countries with the highest death rates, whereas countries with a lower rate have a higher level of cereal consumption accompanied by a lower total average intake of kilocalories.
&lt;/p&gt;</description></item><item><title>&#22810;&#32447;&#24615;&#19987;&#23478;&#28151;&#21512;&#65288;MMoE&#65289;&#23618;&#36890;&#36807;&#22240;&#24335;&#20998;&#35299;&#38024;&#23545;&#35270;&#35273;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#19987;&#23478;&#29305;&#21270;&#35299;&#20915;&#26041;&#26696;&#65292;&#36991;&#20813;&#20102;&#31163;&#25955;&#19987;&#23478;&#36335;&#30001;&#21644;&#36807;&#39640;&#25512;&#29702;&#26102;&#38388;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.12550</link><description>&lt;p&gt;
&#22810;&#32447;&#24615;&#19987;&#23478;&#28151;&#21512;&#65306;&#36890;&#36807;&#22240;&#24335;&#20998;&#35299;&#23454;&#29616;&#21487;&#25193;&#23637;&#30340;&#19987;&#23478;&#29305;&#21270;
&lt;/p&gt;
&lt;p&gt;
Multilinear Mixture of Experts: Scalable Expert Specialization through Factorization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12550
&lt;/p&gt;
&lt;p&gt;
&#22810;&#32447;&#24615;&#19987;&#23478;&#28151;&#21512;&#65288;MMoE&#65289;&#23618;&#36890;&#36807;&#22240;&#24335;&#20998;&#35299;&#38024;&#23545;&#35270;&#35273;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#19987;&#23478;&#29305;&#21270;&#35299;&#20915;&#26041;&#26696;&#65292;&#36991;&#20813;&#20102;&#31163;&#25955;&#19987;&#23478;&#36335;&#30001;&#21644;&#36807;&#39640;&#25512;&#29702;&#26102;&#38388;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19987;&#23478;&#28151;&#21512;&#65288;MoE&#65289;&#33539;&#24335;&#25552;&#20379;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;&#26041;&#27861;&#65292;&#23558;&#38590;&#20197;&#29702;&#35299;&#30340;&#23494;&#38598;&#23618;&#20998;&#35299;&#20026;&#26356;&#23567;&#12289;&#27169;&#22359;&#21270;&#30340;&#35745;&#31639;&#65292;&#36890;&#24120;&#26356;&#26131;&#20110;&#20154;&#31867;&#35299;&#37322;&#12289;&#35843;&#35797;&#21644;&#32534;&#36753;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#22312;&#20110;&#25193;&#23637;&#19987;&#23478;&#25968;&#37327;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#20197;&#23454;&#29616;&#36275;&#22815;&#31934;&#32454;&#30340;&#19987;&#19994;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22810;&#32447;&#24615;&#19987;&#23478;&#28151;&#21512;&#65288;MMoE&#65289;&#23618;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#37325;&#28857;&#25918;&#22312;&#35270;&#35273;&#27169;&#22411;&#19978;&#12290;MMoE&#23618;&#23436;&#20840;&#20197;&#22240;&#24335;&#21270;&#24418;&#24335;&#23545;&#24222;&#22823;&#30340;&#26435;&#37325;&#24352;&#37327;&#36827;&#34892;&#38544;&#24335;&#35745;&#31639;&#12290;&#22240;&#27492;&#65292;MMoEs&#26082;&#36991;&#20813;&#20102;&#22312;&#27969;&#34892;&#30340;&#8220;&#31232;&#30095;&#8221;MoE&#27169;&#22411;&#20013;&#31163;&#25955;&#19987;&#23478;&#36335;&#30001;&#25152;&#36896;&#25104;&#30340;&#38382;&#39064;&#65292;&#21448;&#19981;&#20250;&#24341;&#36215;&#8220;&#36719;&#8221;MoE&#26367;&#20195;&#26041;&#26696;&#20013;&#36807;&#39640;&#30340;&#25512;&#29702;&#26102;&#38388;&#25104;&#26412;&#12290;&#25105;&#20204;&#36890;&#36807;&#21487;&#35270;&#21270;&#21644;&#21453;&#20107;&#23454;&#24178;&#39044;&#65292;&#25552;&#20379;&#20102;&#23450;&#24615;&#21644;&#23450;&#37327;&#35777;&#25454;&#65292;&#35777;&#26126;&#20102;&#25193;&#23637;MMoE&#23618;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12550v1 Announce Type: cross  Abstract: The Mixture of Experts (MoE) paradigm provides a powerful way to decompose inscrutable dense layers into smaller, modular computations often more amenable to human interpretation, debugging, and editability. A major problem however lies in the computational cost of scaling the number of experts to achieve sufficiently fine-grained specialization. In this paper, we propose the Multilinear Mixutre of Experts (MMoE) layer to address this, focusing on vision models. MMoE layers perform an implicit computation on prohibitively large weight tensors entirely in factorized form. Consequently, MMoEs both (1) avoid the issues incurred through the discrete expert routing in the popular 'sparse' MoE models, yet (2) do not incur the restrictively high inference-time costs of 'soft' MoE alternatives. We present both qualitative and quantitative evidence (through visualization and counterfactual interventions respectively) that scaling MMoE layers wh
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#20855;&#26377;&#26234;&#33021;&#20648;&#33021;&#30340;&#24314;&#31569;&#20013;&#65292;&#31616;&#21333;&#30340;&#32447;&#24615;&#22810;&#23618;&#24863;&#30693;&#22120;&#27169;&#22411;&#25552;&#20379;&#20102;&#19982;&#26368;&#20808;&#36827;&#27169;&#22411;&#30456;&#24403;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#26356;&#20855;&#26377;&#25968;&#25454;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.12539</link><description>&lt;p&gt;
&#25968;&#25454;&#20351;&#29992;&#23545;&#20855;&#26377;&#26234;&#33021;&#20648;&#33021;&#30340;&#24314;&#31569;&#20013;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#24615;&#33021;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Impact of data usage for forecasting on performance of model predictive control in buildings with smart energy storage
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12539
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#20855;&#26377;&#26234;&#33021;&#20648;&#33021;&#30340;&#24314;&#31569;&#20013;&#65292;&#31616;&#21333;&#30340;&#32447;&#24615;&#22810;&#23618;&#24863;&#30693;&#22120;&#27169;&#22411;&#25552;&#20379;&#20102;&#19982;&#26368;&#20808;&#36827;&#27169;&#22411;&#30456;&#24403;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#26356;&#20855;&#26377;&#25968;&#25454;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#26159;&#24320;&#21457;&#29992;&#20110;&#24314;&#31569;&#33021;&#28304;&#31995;&#32479;&#20013;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;MPC&#65289;&#26041;&#26696;&#30340;&#39044;&#27979;&#27169;&#22411;&#25152;&#24517;&#38656;&#30340;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#20351;&#29992;&#20250;&#20135;&#29983;&#25910;&#38598;&#21644;&#21033;&#29992;&#26041;&#38754;&#30340;&#25104;&#26412;&#12290;&#30830;&#23450;&#25104;&#26412;&#26368;&#20248;&#25968;&#25454;&#20351;&#29992;&#38656;&#35201;&#20102;&#35299;&#20854;&#24102;&#26469;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#20197;&#21450;&#32467;&#26524; MPC &#36816;&#34892;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#21382;&#21490;&#24314;&#31569;&#33021;&#28304;&#25968;&#25454;&#22312;&#19968;&#20010;&#22810;&#24314;&#31569;&#33021;&#28304;&#31995;&#32479;&#27169;&#25311;&#20013;&#65292;&#30740;&#31350;&#20102;&#31616;&#21333;&#21644;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#27169;&#22411;&#22312; MPC &#20013;&#30340;&#24615;&#33021;&#12290;&#23545;&#20110;&#20197;&#19979;&#25968;&#25454;&#25928;&#29575;&#25514;&#26045;&#65292;&#21363;&#37325;&#26032;&#20351;&#29992;&#39044;&#27979;&#27169;&#22411;&#12289;&#20943;&#23569;&#35757;&#32451;&#25968;&#25454;&#37327;&#12289;&#20943;&#23569;&#27169;&#22411;&#25968;&#25454;&#29305;&#24449;&#21644;&#22312;&#32447;&#27169;&#22411;&#35757;&#32451;&#65292;&#37327;&#21270;&#20102;&#25968;&#25454;&#20351;&#29992;&#23545;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#31616;&#21333;&#30340;&#32447;&#24615;&#22810;&#23618;&#24863;&#30693;&#22120;&#27169;&#22411;&#25552;&#20379;&#20102;&#19982;&#26368;&#20808;&#36827;&#27169;&#22411;&#30456;&#24403;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#19988;&#20855;&#26377;&#26356;&#39640;&#30340;&#25968;&#25454;&#25928;&#29575;&#21644;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12539v1 Announce Type: cross  Abstract: Data is required to develop forecasting models for use in Model Predictive Control (MPC) schemes in building energy systems. However, data usage incurs costs from both its collection and exploitation. Determining cost optimal data usage requires understanding of the forecast accuracy and resulting MPC operational performance it enables. This study investigates the performance of both simple and state-of-the-art machine learning prediction models for MPC in a multi-building energy system simulation using historic building energy data. The impact of data usage on forecast accuracy is quantified for the following data efficiency measures: reuse of prediction models, reduction of training data volumes, reduction of model data features, and online model training. A simple linear multi-layer perceptron model is shown to provide equivalent forecast accuracy to state-of-the-art models, with greater data efficiency and generalisability. The use
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20351;&#29992;&#22534;&#21472;&#38598;&#25104;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#32467;&#21512;&#22810;&#31181;&#29305;&#24449;&#25552;&#21462;&#25216;&#26415;&#65292;&#25104;&#21151;&#24320;&#21457;&#20986;&#29992;&#20110;&#26816;&#27979;&#32593;&#32476;&#27450;&#20940;&#30340;&#33258;&#21160;&#21270;&#31995;&#32479;&#65292;&#24182;&#22312;&#24615;&#33021;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2402.12538</link><description>&lt;p&gt;
&#29992;&#20110;&#26816;&#27979;&#32593;&#32476;&#27450;&#20940;&#30340;&#26426;&#22120;&#23398;&#20064;&#38598;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Machine Learning Ensemble Model for the Detection of Cyberbullying
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12538
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20351;&#29992;&#22534;&#21472;&#38598;&#25104;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#32467;&#21512;&#22810;&#31181;&#29305;&#24449;&#25552;&#21462;&#25216;&#26415;&#65292;&#25104;&#21151;&#24320;&#21457;&#20986;&#29992;&#20110;&#26816;&#27979;&#32593;&#32476;&#27450;&#20940;&#30340;&#33258;&#21160;&#21270;&#31995;&#32479;&#65292;&#24182;&#22312;&#24615;&#33021;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#30340;&#24191;&#27867;&#20351;&#29992;&#65292;&#22914;Facebook&#12289;Instagram&#31561;&#65292;&#26174;&#33879;&#22686;&#24378;&#20102;&#25105;&#20204;&#30340;&#30005;&#23376;&#20114;&#32852;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24179;&#21488;&#29616;&#22312;&#21487;&#20197;&#22312;&#20219;&#20309;&#22320;&#28857;&#38543;&#26102;&#36731;&#26494;&#35775;&#38382;&#12290;&#28982;&#32780;&#65292;&#31038;&#20132;&#23186;&#20307;&#30340;&#26222;&#21450;&#20063;&#23548;&#33268;&#20102;&#32593;&#32476;&#27450;&#20940;&#30340;&#22686;&#21152;&#12290;&#26377;&#24517;&#35201;&#35299;&#20915;&#22312;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#25214;&#21040;&#12289;&#30417;&#25511;&#21644;&#20943;&#36731;&#32593;&#32476;&#27450;&#20940;&#24086;&#23376;&#30340;&#38656;&#27714;&#12290;&#21463;&#21040;&#36825;&#31181;&#38656;&#27714;&#30340;&#25512;&#21160;&#65292;&#25105;&#20204;&#25552;&#20986;&#26412;&#25991;&#20026;&#21457;&#23637;&#26816;&#27979;&#25915;&#20987;&#24615;&#25512;&#25991;&#30340;&#20108;&#36827;&#21046;&#26631;&#31614;&#30340;&#33258;&#21160;&#21270;&#31995;&#32479;&#20570;&#20986;&#36129;&#29486;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#30456;&#21516;&#25968;&#25454;&#38598;&#19978;&#19982;&#20808;&#21069;&#23454;&#39564;&#30456;&#27604;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#22534;&#21472;&#38598;&#25104;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#22235;&#31181;&#19981;&#21516;&#30340;&#29305;&#24449;&#25552;&#21462;&#25216;&#26415;&#26469;&#20248;&#21270;&#22534;&#21472;&#38598;&#25104;&#23398;&#20064;&#26694;&#26550;&#20869;&#30340;&#24615;&#33021;&#12290;&#23558;&#20116;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#20915;&#31574;&#26641;&#12289;&#38543;&#26426;&#26862;&#26519;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12538v1 Announce Type: cross  Abstract: The pervasive use of social media platforms, such as Facebook, Instagram, and X, has significantly amplified our electronic interconnectedness. Moreover, these platforms are now easily accessible from any location at any given time. However, the increased popularity of social media has also led to cyberbullying.It is imperative to address the need for finding, monitoring, and mitigating cyberbullying posts on social media platforms. Motivated by this necessity, we present this paper to contribute to developing an automated system for detecting binary labels of aggressive tweets.Our study has demonstrated remarkable performance compared to previous experiments on the same dataset. We employed the stacking ensemble machine learning method, utilizing four various feature extraction techniques to optimize performance within the stacking ensemble learning framework. Combining five machine learning algorithms,Decision Trees, Random Forest, L
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20998;&#23618;&#36125;&#21494;&#26031;&#32479;&#35745;&#26694;&#26550;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#20010;&#24615;&#21270;&#26080;&#30417;&#30563;&#23398;&#20064;&#65292;&#20854;&#20013;&#24320;&#21457;&#20102;&#36866;&#24212;&#24615;&#31639;&#27861;&#26469;&#24179;&#34913;&#21033;&#29992;&#26377;&#38480;&#26412;&#22320;&#25968;&#25454;&#21644;&#21327;&#20316;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2402.12537</link><description>&lt;p&gt;
&#38024;&#23545;&#20010;&#24615;&#21270;&#32852;&#37030;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#20998;&#23618;&#36125;&#21494;&#26031;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Bayes Approach to Personalized Federated Unsupervised Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12537
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20998;&#23618;&#36125;&#21494;&#26031;&#32479;&#35745;&#26694;&#26550;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#20010;&#24615;&#21270;&#26080;&#30417;&#30563;&#23398;&#20064;&#65292;&#20854;&#20013;&#24320;&#21457;&#20102;&#36866;&#24212;&#24615;&#31639;&#27861;&#26469;&#24179;&#34913;&#21033;&#29992;&#26377;&#38480;&#26412;&#22320;&#25968;&#25454;&#21644;&#21327;&#20316;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23458;&#25143;&#26412;&#22320;&#25968;&#25454;&#30340;&#32479;&#35745;&#24322;&#36136;&#24615;&#26159;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#37325;&#35201;&#29305;&#24449;&#65292;&#20854;&#20419;&#20351;&#20010;&#24615;&#21270;&#31639;&#27861;&#38024;&#23545;&#26412;&#22320;&#25968;&#25454;&#32479;&#35745;&#37327;&#36827;&#34892;&#23450;&#21046;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#22823;&#37327;&#38024;&#23545;&#20010;&#24615;&#21270;&#30417;&#30563;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#20294;&#36890;&#36807;&#20010;&#24615;&#21270;&#26080;&#30417;&#30563;&#23398;&#20064;&#21457;&#29616;&#26412;&#22320;&#25968;&#25454;&#30340;&#32467;&#26500;&#21364;&#24456;&#23569;&#34987;&#25506;&#32034;&#12290;&#25105;&#20204;&#36890;&#36807;&#22522;&#20110;&#23618;&#27425;&#36125;&#21494;&#26031;&#32479;&#35745;&#26694;&#26550;&#21551;&#21160;&#20102;&#23545;&#36825;&#31181;&#20010;&#24615;&#21270;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#31995;&#32479;&#30740;&#31350;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#22522;&#20110;&#20248;&#21270;&#26631;&#20934;&#30340;&#31639;&#27861;&#65292;&#36825;&#20123;&#31639;&#27861;&#21463;&#21551;&#21457;&#20110;&#23618;&#27425;&#36125;&#21494;&#26031;&#32479;&#35745;&#26694;&#26550;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#36866;&#24212;&#24615;&#31639;&#27861;&#65292;&#21457;&#29616;&#20102;&#21033;&#29992;&#26377;&#38480;&#26412;&#22320;&#25968;&#25454;&#21644;&#21327;&#20316;&#20449;&#24687;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#26080;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#30340;&#32972;&#26223;&#19979;&#36827;&#34892;&#20102;&#36825;&#39033;&#24037;&#20316;&#65306;&#20010;&#24615;&#21270;&#38477;&#32500;&#21644;&#20010;&#24615;&#21270;&#25193;&#25955;&#27169;&#22411;&#12290;&#25105;&#20204;&#20026;&#25105;&#20204;&#30340;&#33258;&#36866;&#24212;&#31639;&#27861;&#24320;&#21457;&#20102;&#25910;&#25947;&#20998;&#26512;&#65292;&#36825;&#20123;&#20998;&#26512;&#23637;&#31034;&#20102;&#23545;&#38382;&#39064;&#21442;&#25968;&#65288;&#20363;&#22914;&#65292;&#24322;&#36136;&#24615;&#65289;&#30340;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12537v1 Announce Type: new  Abstract: Statistical heterogeneity of clients' local data is an important characteristic in federated learning, motivating personalized algorithms tailored to the local data statistics. Though there has been a plethora of algorithms proposed for personalized supervised learning, discovering the structure of local data through personalized unsupervised learning is less explored. We initiate a systematic study of such personalized unsupervised learning by developing algorithms based on optimization criteria inspired by a hierarchical Bayesian statistical framework. We develop adaptive algorithms that discover the balance between using limited local data and collaborative information. We do this in the context of two unsupervised learning tasks: personalized dimensionality reduction and personalized diffusion models. We develop convergence analyses for our adaptive algorithms which illustrate the dependence on problem parameters (e.g., heterogeneity
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#31185;&#23398;&#39046;&#22495;&#20013;&#22823;&#35268;&#27169;&#28857;&#20113;&#22788;&#29702;&#20248;&#21270;&#30340;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#23616;&#37096;&#25935;&#24863;&#21704;&#24076;&#25216;&#26415;&#23454;&#29616;&#20102;&#36817;&#32447;&#24615;&#22797;&#26434;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;LSH&#30340;&#39640;&#25928;&#28857;&#21464;&#25442;&#22120;HEPT&#12290;</title><link>https://arxiv.org/abs/2402.12535</link><description>&lt;p&gt;
&#22522;&#20110;&#23616;&#37096;&#25935;&#24863;&#21704;&#24076;&#30340;&#39640;&#33021;&#29289;&#29702;&#20013;&#24212;&#29992;&#30340;&#39640;&#25928;&#28857;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Locality-Sensitive Hashing-Based Efficient Point Transformer with Applications in High-Energy Physics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#31185;&#23398;&#39046;&#22495;&#20013;&#22823;&#35268;&#27169;&#28857;&#20113;&#22788;&#29702;&#20248;&#21270;&#30340;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#23616;&#37096;&#25935;&#24863;&#21704;&#24076;&#25216;&#26415;&#23454;&#29616;&#20102;&#36817;&#32447;&#24615;&#22797;&#26434;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;LSH&#30340;&#39640;&#25928;&#28857;&#21464;&#25442;&#22120;HEPT&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#22823;&#35268;&#27169;&#28857;&#20113;&#22788;&#29702;&#22312;&#31185;&#23398;&#39046;&#22495;&#65288;&#22914;&#39640;&#33021;&#29289;&#29702;&#21644;&#22825;&#20307;&#29289;&#29702;&#65289;&#36827;&#34892;&#20248;&#21270;&#30340;&#26032;&#22411;Transformer&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#35299;&#20915;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#26631;&#20934;Transformer&#30340;&#23616;&#38480;&#24615;&#65292;&#38598;&#25104;&#20102;&#23616;&#37096;&#24402;&#32435;&#20559;&#24046;&#65292;&#24182;&#36890;&#36807;&#30828;&#20214;&#21451;&#22909;&#30340;&#24120;&#35268;&#25805;&#20316;&#23454;&#29616;&#25509;&#36817;&#32447;&#24615;&#22797;&#26434;&#24230;&#12290;&#26412;&#24037;&#20316;&#30340;&#19968;&#20010;&#36129;&#29486;&#26159;&#23545;&#21508;&#31181;&#31232;&#30095;&#21270;&#25216;&#26415;&#36827;&#34892;&#35823;&#24046;-&#22797;&#26434;&#24230;&#26435;&#34913;&#30340;&#23450;&#37327;&#20998;&#26512;&#65292;&#20197;&#26500;&#24314;&#39640;&#25928;Transformer&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#31361;&#26174;&#20102;&#22312;&#20855;&#26377;&#23616;&#37096;&#24402;&#32435;&#20559;&#24046;&#30340;&#22823;&#35268;&#27169;&#28857;&#20113;&#25968;&#25454;&#20013;&#20351;&#29992;&#23616;&#37096;&#25935;&#24863;&#21704;&#24076;&#65288;LSH&#65289;&#65292;&#23588;&#20854;&#26159;OR&#65286;AND&#26500;&#36896;LSH&#65292;&#22312;&#26680;&#36817;&#20284;&#20013;&#30340;&#20248;&#36234;&#24615;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;LSH&#30340;&#39640;&#25928;&#28857;&#21464;&#25442;&#22120;&#65288;HEPT&#65289;&#65292;&#23427;&#23558;E^2LSH&#19982;OR&#65286;AND&#26500;&#36896;&#30456;&#32467;&#21512;&#65292;&#24182;&#24314;&#31435;&#22312;&#24120;&#35268;&#35745;&#31639;&#20043;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12535v1 Announce Type: new  Abstract: This study introduces a novel transformer model optimized for large-scale point cloud processing in scientific domains such as high-energy physics (HEP) and astrophysics. Addressing the limitations of graph neural networks and standard transformers, our model integrates local inductive bias and achieves near-linear complexity with hardware-friendly regular operations. One contribution of this work is the quantitative analysis of the error-complexity tradeoff of various sparsification techniques for building efficient transformers. Our findings highlight the superiority of using locality-sensitive hashing (LSH), especially OR \&amp; AND-construction LSH, in kernel approximation for large-scale point cloud data with local inductive bias. Based on this finding, we propose LSH-based Efficient Point Transformer (\textbf{HEPT}), which combines E$^2$LSH with OR \&amp; AND constructions and is built upon regular computations. HEPT demonstrates remarkabl
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#23545;&#31216;&#26694;&#26550;&#65292;&#21487;&#25913;&#36827;&#29616;&#26377;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#22810;&#23545;&#19968;&#22270;&#20687;&#21040;&#22270;&#20687;&#32763;&#35793;&#19978;&#30340;&#25928;&#26524;&#65292;&#24182;&#22312; StarGAN V2 &#19978;&#23637;&#31034;&#20102;&#20854;&#24615;&#33021;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.12531</link><description>&lt;p&gt;
&#22312;&#22810;&#23545;&#19968;&#22270;&#20687;&#21040;&#22270;&#20687;&#32763;&#35793;&#19978;&#25913;&#36827;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Improving Deep Generative Models on Many-To-One Image-to-Image Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12531
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#23545;&#31216;&#26694;&#26550;&#65292;&#21487;&#25913;&#36827;&#29616;&#26377;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#22810;&#23545;&#19968;&#22270;&#20687;&#21040;&#22270;&#20687;&#32763;&#35793;&#19978;&#30340;&#25928;&#26524;&#65292;&#24182;&#22312; StarGAN V2 &#19978;&#23637;&#31034;&#20102;&#20854;&#24615;&#33021;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12531v1 &#36890;&#21578;&#31867;&#22411;: &#36328; &#38024;&#23545;&#22270;&#20687;&#21040;&#22270;&#20687;&#32763;&#35793;&#20013;&#30340;&#22810;&#20010;&#24212;&#29992;&#65292;&#24050;&#24212;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#12290; &#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#25193;&#25955;&#27169;&#22411;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290; &#22823;&#22810;&#25968;&#26041;&#27861;&#22312;&#25968;&#25454;&#38598;&#20013;&#30340;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#20855;&#26377;&#23545;&#31216;&#35774;&#32622;&#12290; &#36825;&#20123;&#26041;&#27861;&#20551;&#35774;&#25152;&#26377;&#39046;&#22495;&#37117;&#20855;&#26377;&#22810;&#20010;&#27169;&#24577;&#25110;&#20165;&#19968;&#20010;&#27169;&#24577;&#12290; &#20294;&#26159;&#65292;&#35768;&#22810;&#25968;&#25454;&#38598;&#23384;&#22312;&#20004;&#20010;&#22495;&#20043;&#38388;&#30340;&#22810;&#23545;&#19968;&#20851;&#31995;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#19968;&#20010;Colorized MNIST&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;Color-Recall&#20998;&#25968;&#65292;&#23427;&#21487;&#20197;&#20026;&#22312;&#22810;&#23545;&#19968;&#32763;&#35793;&#19978;&#35780;&#20272;&#27169;&#22411;&#25552;&#20379;&#19968;&#20010;&#31616;&#21333;&#30340;&#22522;&#20934;&#12290; &#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#38750;&#23545;&#31216;&#26694;&#26550;&#65292;&#20197;&#25913;&#36827;&#29616;&#26377;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#22810;&#23545;&#19968;&#22270;&#20687;&#21040;&#22270;&#20687;&#32763;&#35793;&#19978;&#30340;&#34920;&#29616;&#12290; &#25105;&#20204;&#23558;&#36825;&#20010;&#26694;&#26550;&#24212;&#29992;&#21040; StarGAN V2 &#19978;&#65292;&#24182;&#34920;&#26126;&#22312;&#26080;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#35774;&#32622;&#20013;&#65292;&#36825;&#20010;&#26032;&#27169;&#22411;&#30340;&#24615;&#33021;&#24471;&#21040;&#20102;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12531v1 Announce Type: cross  Abstract: Deep generative models have been applied to multiple applications in image- to-image translation. Generative Adversarial Networks and Diffusion Models have presented impressive results, setting new state-of-the-art results on these tasks. Most methods have symmetric setups across the different domains in a dataset. These methods assume that all domains have either multiple modalities or only one modality. However, there are many datasets that have a many-to-one relationship between two domains. In this work, we first introduce a Colorized MNIST dataset and a Color-Recall score that can provide a simple benchmark for evaluating models on many-to-one translation. We then introduce a new asymmetric framework to improve existing deep generative models on many-to-one image-to- image translation. We apply this framework to StarGAN V2 and show that in both unsupervised and semi-supervised settings, the performance of this new model improves o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#21462;&#20915;&#20110;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#24179;&#34892;&#32467;&#26500;&#65292;&#36890;&#36807;&#22312;&#30456;&#20284;&#27169;&#26495;&#30340;&#30701;&#35821;&#23545;&#20013;&#23398;&#20064;&#26469;&#25552;&#39640;&#19978;&#19979;&#25991;&#23398;&#20064;&#20934;&#30830;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.12530</link><description>&lt;p&gt;
&#22312;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#24179;&#34892;&#32467;&#26500;&#23454;&#29616;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Parallel Structures in Pre-training Data Yield In-Context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#21462;&#20915;&#20110;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#24179;&#34892;&#32467;&#26500;&#65292;&#36890;&#36807;&#22312;&#30456;&#20284;&#27169;&#26495;&#30340;&#30701;&#35821;&#23545;&#20013;&#23398;&#20064;&#26469;&#25552;&#39640;&#19978;&#19979;&#25991;&#23398;&#20064;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#20855;&#22791;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#30340;&#33021;&#21147;&#65306;&#23427;&#20204;&#21487;&#20197;&#22312;&#21482;&#32473;&#20986;&#23569;&#37327;&#31034;&#20363;&#30340;&#24773;&#20917;&#19979;&#36866;&#24212;&#20219;&#21153;&#32780;&#26080;&#38656;&#36827;&#34892;&#20219;&#20309;&#21442;&#25968;&#26356;&#26032;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#36825;&#31181;&#33021;&#21147;&#26469;&#33258;&#20309;&#22788;&#65292;&#22240;&#20026;&#39044;&#35757;&#32451;&#25991;&#26412;&#19982;ICL&#25552;&#31034;&#20043;&#38388;&#23384;&#22312;&#26126;&#26174;&#30340;&#20998;&#24067;&#20559;&#31227;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#21738;&#20123;&#27169;&#24335;&#26377;&#21161;&#20110;ICL&#12290;&#25105;&#20204;&#21457;&#29616;LMs&#30340;ICL&#33021;&#21147;&#21462;&#20915;&#20110;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#8220;&#24179;&#34892;&#32467;&#26500;&#8221;&#8212;&#8212;&#22312;&#30456;&#21516;&#19978;&#19979;&#25991;&#31383;&#21475;&#20013;&#36981;&#24490;&#30456;&#20284;&#27169;&#26495;&#30340;&#30701;&#35821;&#23545;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#36890;&#36807;&#26816;&#26597;&#35757;&#32451;&#19968;&#20010;&#30701;&#35821;&#26159;&#21542;&#25552;&#39640;&#20102;&#23545;&#21478;&#19968;&#20010;&#30701;&#35821;&#30340;&#39044;&#27979;&#26469;&#26816;&#27979;&#24179;&#34892;&#32467;&#26500;&#65292;&#24182;&#36827;&#34892;&#28040;&#34701;&#23454;&#39564;&#20197;&#30740;&#31350;&#20854;&#23545;ICL&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20174;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#21435;&#38500;&#24179;&#34892;&#32467;&#26500;&#20250;&#23548;&#33268;LMs&#30340;ICL&#20934;&#30830;&#24230;&#19979;&#38477;51&#65285;&#65288;&#19982;&#38543;&#26426;&#20999;&#38500;&#30340;2&#65285;&#30456;&#27604;&#65289;&#12290;&#21363;&#20351;&#25490;&#38500;&#24120;&#35265;&#27169;&#24335;&#22914; n-gram
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12530v1 Announce Type: cross  Abstract: Pre-trained language models (LMs) are capable of in-context learning (ICL): they can adapt to a task with only a few examples given in the prompt without any parameter update. However, it is unclear where this capability comes from as there is a stark distribution shift between pre-training text and ICL prompts. In this work, we study what patterns of the pre-training data contribute to ICL. We find that LMs' ICL ability depends on $\textit{parallel structures}$ in the pre-training data -- pairs of phrases following similar templates in the same context window. Specifically, we detect parallel structures by checking whether training on one phrase improves prediction of the other, and conduct ablation experiments to study their effect on ICL. We show that removing parallel structures in the pre-training data reduces LMs' ICL accuracy by 51% (vs 2% from random ablation). This drop persists even when excluding common patterns such as n-gr
&lt;/p&gt;</description></item><item><title>&#23398;&#20064;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#34987;&#30495;&#23454;&#19988;&#26080;&#35823;&#24046;&#30340;&#21160;&#21147;&#23398;&#26367;&#20195;&#26102;&#65292;&#29616;&#26377;&#27169;&#22411;&#39537;&#21160;&#26041;&#27861;&#23558;&#20250;&#23436;&#20840;&#22833;&#36133;&#65292;&#25581;&#31034;&#20986;&#19968;&#20010;&#37325;&#22823;&#35823;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.12527</link><description>&lt;p&gt;
&#31163;&#32447;&#27169;&#22411;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36793;&#32536;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
The Edge-of-Reach Problem in Offline Model-Based Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12527
&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#34987;&#30495;&#23454;&#19988;&#26080;&#35823;&#24046;&#30340;&#21160;&#21147;&#23398;&#26367;&#20195;&#26102;&#65292;&#29616;&#26377;&#27169;&#22411;&#39537;&#21160;&#26041;&#27861;&#23558;&#20250;&#23436;&#20840;&#22833;&#36133;&#65292;&#25581;&#31034;&#20986;&#19968;&#20010;&#37325;&#22823;&#35823;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26088;&#22312;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#20174;&#39044;&#20808;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#32780;&#65292;&#30001;&#27492;&#24102;&#26469;&#20102;&#19968;&#20010;&#39069;&#22806;&#30340;&#25361;&#25112;&#65292;&#21363;&#20272;&#35745;&#25968;&#25454;&#38598;&#20013;&#26410;&#28085;&#30422;&#30340;&#34892;&#20026;&#30340;&#20215;&#20540;&#12290;&#27169;&#22411;&#39537;&#21160;&#26041;&#27861;&#36890;&#36807;&#20801;&#35768;&#26234;&#33021;&#20307;&#36890;&#36807;&#22312;&#23398;&#20064;&#21160;&#21147;&#23398;&#27169;&#22411;&#20013;&#36827;&#34892;&#23637;&#24320;&#36827;&#34892;&#25910;&#38598;&#39069;&#22806;&#30340;&#21512;&#25104;&#25968;&#25454;&#26469;&#25552;&#20379;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#22914;&#26524;&#23398;&#20064;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#34987;&#30495;&#23454;&#19988;&#26080;&#35823;&#24046;&#30340;&#21160;&#21147;&#23398;&#26367;&#20195;&#65292;&#29616;&#26377;&#30340;&#27169;&#22411;&#39537;&#21160;&#26041;&#27861;&#23558;&#23436;&#20840;&#22833;&#36133;&#12290;&#36825;&#25581;&#31034;&#20102;&#19968;&#20010;&#37325;&#22823;&#35823;&#35299;&#12290;&#25105;&#20204;&#30340;&#21518;&#32493;&#35843;&#26597;&#21457;&#29616;&#65292;&#27169;&#22411;&#39537;&#21160;&#31639;&#27861;&#20013;&#20351;&#29992;&#30340;&#19968;&#33324;&#36807;&#31243;&#23548;&#33268;&#23384;&#22312;&#19968;&#32452;&#35302;&#21457;&#30149;&#24577;&#20540;&#36807;&#39640;&#30340;&#36793;&#32536;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12527v1 Announce Type: cross  Abstract: Offline reinforcement learning aims to enable agents to be trained from pre-collected datasets, however, this comes with the added challenge of estimating the value of behavior not covered in the dataset. Model-based methods offer a solution by allowing agents to collect additional synthetic data via rollouts in a learned dynamics model. The prevailing theoretical understanding is that this can then be viewed as online reinforcement learning in an approximate dynamics model, and any remaining gap is therefore assumed to be due to the imperfect dynamics model. Surprisingly, however, we find that if the learned dynamics model is replaced by the true error-free dynamics, existing model-based methods completely fail. This reveals a major misconception. Our subsequent investigation finds that the general procedure used in model-based algorithms results in the existence of a set of edge-of-reach states which trigger pathological value overes
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#26031;&#36807;&#31243;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#65288;GP-NAM&#65289;&#65292;&#36890;&#36807;&#38543;&#26426;&#20613;&#37324;&#21494;&#29305;&#24449;&#23545;&#39640;&#26031;&#36807;&#31243;&#36827;&#34892;&#21333;&#23618;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#65292;&#21487;&#20197;&#23454;&#29616;&#20855;&#26377;&#20984;&#30446;&#26631;&#20989;&#25968;&#21644;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#38543;&#29305;&#24449;&#32500;&#24230;&#32447;&#24615;&#22686;&#38271;&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#22312;&#24615;&#33021;&#19978;&#19981;&#20122;&#20110;&#26356;&#28145;&#30340;NAM&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.12518</link><description>&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Gaussian Process Neural Additive Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#26031;&#36807;&#31243;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#65288;GP-NAM&#65289;&#65292;&#36890;&#36807;&#38543;&#26426;&#20613;&#37324;&#21494;&#29305;&#24449;&#23545;&#39640;&#26031;&#36807;&#31243;&#36827;&#34892;&#21333;&#23618;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#65292;&#21487;&#20197;&#23454;&#29616;&#20855;&#26377;&#20984;&#30446;&#26631;&#20989;&#25968;&#21644;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#38543;&#29305;&#24449;&#32500;&#24230;&#32447;&#24615;&#22686;&#38271;&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#22312;&#24615;&#33021;&#19978;&#19981;&#20122;&#20110;&#26356;&#28145;&#30340;NAM&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#22312;&#35768;&#22810;&#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#65292;&#20294;&#23427;&#20204;&#30340;&#40657;&#30418;&#29305;&#24615;&#26377;&#26102;&#20063;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#21307;&#30103;&#20445;&#20581;&#21644;&#37329;&#34701;&#31561;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#36825;&#20123;&#39046;&#22495;&#38656;&#35201;&#21487;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#12290;&#26368;&#36817;&#21457;&#23637;&#20986;&#30340;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#65288;NAMs&#65289;&#26159;&#22312;&#38754;&#21521;&#34920;&#26684;&#25968;&#25454;&#38598;&#30340;&#21487;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#26041;&#21521;&#19978;&#36808;&#20986;&#30340;&#37325;&#35201;&#19968;&#27493;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;NAM&#23376;&#31867;&#65292;&#23427;&#20351;&#29992;&#36890;&#36807;&#38543;&#26426;&#20613;&#37324;&#21494;&#29305;&#24449;&#23545;&#39640;&#26031;&#36807;&#31243;&#36827;&#34892;&#21333;&#23618;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#39640;&#26031;&#36807;&#31243;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#65288;GP-NAM&#65289;&#12290;GP-NAM&#20855;&#26377;&#20984;&#30446;&#26631;&#20989;&#25968;&#21644;&#38543;&#29305;&#24449;&#32500;&#24230;&#32447;&#24615;&#22686;&#38271;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#30340;&#20248;&#21183;&#12290;&#19982;&#26356;&#28145;&#30340;NAM&#26041;&#27861;&#30456;&#27604;&#65292;&#23427;&#22312;&#24615;&#33021;&#19978;&#27809;&#26377;&#25439;&#22833;&#65292;&#22240;&#20026;GPs&#38750;&#24120;&#36866;&#21512;&#23398;&#20064;&#22797;&#26434;&#30340;&#38750;&#21442;&#25968;&#21333;&#21464;&#37327;&#20989;&#25968;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;GP-NAM&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12518v1 Announce Type: cross  Abstract: Deep neural networks have revolutionized many fields, but their black-box nature also occasionally prevents their wider adoption in fields such as healthcare and finance, where interpretable and explainable models are required. The recent development of Neural Additive Models (NAMs) is a significant step in the direction of interpretable deep learning for tabular datasets. In this paper, we propose a new subclass of NAMs that use a single-layer neural network construction of the Gaussian process via random Fourier features, which we call Gaussian Process Neural Additive Models (GP-NAM). GP-NAMs have the advantage of a convex objective function and number of trainable parameters that grows linearly with feature dimensionality. It suffers no loss in performance compared to deeper NAM approaches because GPs are well-suited for learning complex non-parametric univariate functions. We demonstrate the performance of GP-NAM on several tabular
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#24341;&#23548;&#27169;&#22411;&#21305;&#37197;&#65288;IMM&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#23436;&#25972;&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#21463;&#38480;&#27169;&#22411;&#23545;&#40784;&#65292;&#23558;&#21463;&#38480;&#27169;&#22411;&#30340;&#30693;&#35782;&#20256;&#36882;&#32473;&#23436;&#25972;&#27169;&#22411;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12513</link><description>&lt;p&gt;
&#24341;&#23548;&#27169;&#22411;&#21305;&#37197;&#65306;&#21463;&#38480;&#27169;&#22411;&#22914;&#20309;&#24110;&#21161;&#26356;&#22823;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Induced Model Matching: How Restricted Models Can Help Larger Ones
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12513
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#24341;&#23548;&#27169;&#22411;&#21305;&#37197;&#65288;IMM&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#23436;&#25972;&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#21463;&#38480;&#27169;&#22411;&#23545;&#40784;&#65292;&#23558;&#21463;&#38480;&#27169;&#22411;&#30340;&#30693;&#35782;&#20256;&#36882;&#32473;&#23436;&#25972;&#27169;&#22411;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#35757;&#32451;&#26356;&#22823;&#12289;&#20855;&#26377;&#23436;&#25972;&#29305;&#24449;&#30340;&#27169;&#22411;&#26102;&#65292;&#26159;&#21542;&#21487;&#20197;&#21033;&#29992;&#21463;&#38480;&#29305;&#24449;&#30340;&#38750;&#24120;&#20934;&#30830;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;&#36825;&#20010;&#21463;&#38480;&#27169;&#22411;&#21487;&#20197;&#34987;&#35270;&#20026;&#8220;&#36741;&#21161;&#20449;&#24687;&#8221;&#65292;&#21487;&#20197;&#36890;&#36807;&#26469;&#33258;&#36741;&#21161;&#28304;&#25968;&#25454;&#38598;&#30340;&#35814;&#23613;&#25968;&#25454;&#25110;&#22312;&#30456;&#21516;&#25968;&#25454;&#38598;&#19978;&#36890;&#36807;&#26045;&#21152;&#38480;&#21046;&#26469;&#33719;&#24471;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#21463;&#38480;&#27169;&#22411;&#30340;&#30693;&#35782;&#20256;&#36882;&#32473;&#23436;&#25972;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#23436;&#25972;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#21463;&#38480;&#24615;&#33021;&#19982;&#21463;&#38480;&#27169;&#22411;&#30340;&#24615;&#33021;&#23545;&#40784;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#31216;&#20026;&#24341;&#23548;&#27169;&#22411;&#21305;&#37197;&#65288;IMM&#65289;&#65292;&#39318;&#20808;&#36890;&#36807;&#20197;&#36923;&#36753;&#22238;&#24402;&#20026;&#29609;&#20855;&#31034;&#20363;&#26469;&#35828;&#26126;&#20854;&#26222;&#36866;&#24615;&#12290;&#28982;&#21518;&#25105;&#20204;&#25506;&#35752;&#20102;IMM&#22312;&#35821;&#35328;&#24314;&#27169;&#20013;&#30340;&#24212;&#29992;&#65292;&#36825;&#20063;&#26159;&#26368;&#21021;&#30340;&#28789;&#24863;&#26469;&#28304;&#65292;IMM&#22312;&#36825;&#37324;&#25552;&#20379;&#20102;&#26126;&#30830;&#30340;&#22522;&#30784;&#65292;&#19982;&#22312;&#25216;&#26415;&#20013;&#38544;&#24335;&#20351;&#29992;&#21463;&#38480;&#27169;&#22411;&#30340;&#26041;&#27861;&#30456;&#23545;&#24212;&#65292;&#20363;&#22914;&#28155;&#21152;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12513v1 Announce Type: new  Abstract: We consider scenarios where a very accurate predictive model using restricted features is available at the time of training of a larger, full-featured, model. This restricted model may be thought of as "side-information", derived either from an auxiliary exhaustive dataset or on the same dataset, by forcing the restriction. How can the restricted model be useful to the full model? We propose an approach for transferring the knowledge of the restricted model to the full model, by aligning the full model's context-restricted performance with that of the restricted model's. We call this methodology Induced Model Matching (IMM) and first illustrate its general applicability by using logistic regression as a toy example. We then explore IMM's use in language modeling, the application that initially inspired it, and where it offers an explicit foundation in contrast to the implicit use of restricted models in techniques such as noising. We dem
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21019;&#24615;&#22320;&#20351;&#29992;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#20998;&#26512;&#21644;&#27604;&#36739;&#26497;&#23567;&#21270;&#20248;&#21270;&#22120;&#65292;&#23637;&#31034;&#20102;&#36229;&#21442;&#25968;&#12289;&#38544;&#24335;&#27491;&#21017;&#21270;&#21644;&#38544;&#24335;&#26354;&#29575;&#35825;&#23548;&#22122;&#22768;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#25552;&#20379;&#20102;&#32479;&#19968;&#31616;&#21270;&#30340;&#20998;&#26512;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.12508</link><description>&lt;p&gt;
&#29992;&#20110;&#26497;&#23567;&#21270;&#20248;&#21270;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
SDEs for Minimax Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12508
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21019;&#24615;&#22320;&#20351;&#29992;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#20998;&#26512;&#21644;&#27604;&#36739;&#26497;&#23567;&#21270;&#20248;&#21270;&#22120;&#65292;&#23637;&#31034;&#20102;&#36229;&#21442;&#25968;&#12289;&#38544;&#24335;&#27491;&#21017;&#21270;&#21644;&#38544;&#24335;&#26354;&#29575;&#35825;&#23548;&#22122;&#22768;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#25552;&#20379;&#20102;&#32479;&#19968;&#31616;&#21270;&#30340;&#20998;&#26512;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26497;&#23567;&#21270;&#20248;&#21270;&#38382;&#39064;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#21560;&#24341;&#20102;&#24456;&#22810;&#20851;&#27880;&#65292;&#24212;&#29992;&#33539;&#22260;&#20174;&#32463;&#27982;&#23398;&#21040;&#26426;&#22120;&#23398;&#20064;&#12290;&#34429;&#28982;&#23384;&#22312;&#38024;&#23545;&#36825;&#31867;&#38382;&#39064;&#30340;&#20808;&#36827;&#20248;&#21270;&#26041;&#27861;&#65292;&#20294;&#22312;&#38543;&#26426;&#22330;&#26223;&#20013;&#25551;&#36848;&#23427;&#20204;&#30340;&#21160;&#24577;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#24320;&#21019;&#24615;&#22320;&#20351;&#29992;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDEs&#65289;&#26469;&#20998;&#26512;&#21644;&#27604;&#36739;&#26497;&#23567;&#21270;&#20248;&#21270;&#22120;&#12290;&#25105;&#20204;&#30340;SDE&#27169;&#22411;&#36866;&#29992;&#20110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;-&#19978;&#21319;&#12289;&#38543;&#26426;&#22806;&#25512;&#27861;&#21644;&#38543;&#26426;&#21704;&#23494;&#23572;&#39039;&#26799;&#24230;&#19979;&#38477;&#65292;&#21487;&#34987;&#35777;&#26126;&#26159;&#23427;&#20204;&#31639;&#27861;&#23545;&#24212;&#29289;&#30340;&#36817;&#20284;&#65292;&#28165;&#26224;&#23637;&#31034;&#20102;&#36229;&#21442;&#25968;&#12289;&#38544;&#24335;&#27491;&#21017;&#21270;&#21644;&#38544;&#24335;&#26354;&#29575;&#35825;&#23548;&#22122;&#22768;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#36825;&#31181;&#35270;&#35282;&#36824;&#20801;&#35768;&#22522;&#20110;&#20234;&#34276;&#24494;&#31215;&#20998;&#21407;&#29702;&#36827;&#34892;&#32479;&#19968;&#31616;&#21270;&#20998;&#26512;&#31574;&#30053;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#21161;&#20110;&#25512;&#23548;&#25910;&#25947;&#26465;&#20214;&#21644;&#38381;&#24335;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12508v1 Announce Type: new  Abstract: Minimax optimization problems have attracted a lot of attention over the past few years, with applications ranging from economics to machine learning. While advanced optimization methods exist for such problems, characterizing their dynamics in stochastic scenarios remains notably challenging. In this paper, we pioneer the use of stochastic differential equations (SDEs) to analyze and compare Minimax optimizers. Our SDE models for Stochastic Gradient Descent-Ascent, Stochastic Extragradient, and Stochastic Hamiltonian Gradient Descent are provable approximations of their algorithmic counterparts, clearly showcasing the interplay between hyperparameters, implicit regularization, and implicit curvature-induced noise. This perspective also allows for a unified and simplified analysis strategy based on the principles of It\^o calculus. Finally, our approach facilitates the derivation of convergence conditions and closed-form solutions for th
&lt;/p&gt;</description></item><item><title>PARCv2&#36890;&#36807;&#24341;&#20837;&#24494;&#20998;&#31639;&#23376;&#25193;&#23637;&#20102;PARC&#27169;&#22411;&#65292;&#29992;&#20110;&#27169;&#25311;&#19981;&#31283;&#23450;&#12289;&#30636;&#24577;&#21644;&#20256;&#36755;&#20027;&#23548;&#31995;&#32479;&#30340;&#26102;&#31354;&#21160;&#21147;&#23398;&#12290;</title><link>https://arxiv.org/abs/2402.12503</link><description>&lt;p&gt;
PARCv2&#65306;&#29289;&#29702;&#24863;&#30693;&#24490;&#29615;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#26102;&#31354;&#21160;&#21147;&#23398;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
PARCv2: Physics-aware Recurrent Convolutional Neural Networks for Spatiotemporal Dynamics Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12503
&lt;/p&gt;
&lt;p&gt;
PARCv2&#36890;&#36807;&#24341;&#20837;&#24494;&#20998;&#31639;&#23376;&#25193;&#23637;&#20102;PARC&#27169;&#22411;&#65292;&#29992;&#20110;&#27169;&#25311;&#19981;&#31283;&#23450;&#12289;&#30636;&#24577;&#21644;&#20256;&#36755;&#20027;&#23548;&#31995;&#32479;&#30340;&#26102;&#31354;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12503v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#23545;&#19981;&#31283;&#23450;&#30340;&#12289;&#24555;&#36895;&#30636;&#24577;&#21644;&#20248;&#21183;&#20256;&#36755;&#20027;&#23548;&#30340;&#29289;&#29702;&#38382;&#39064;&#36827;&#34892;&#24314;&#27169;&#26159;&#29289;&#29702;&#24863;&#30693;&#28145;&#24230;&#23398;&#20064;&#65288;PADL&#65289;&#38754;&#20020;&#30340;&#36843;&#20999;&#25361;&#25112;&#12290;&#22797;&#26434;&#31995;&#32479;&#30340;&#29289;&#29702;&#30001;&#22823;&#22411;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#31995;&#32479;&#21644;&#24102;&#26377;&#38750;&#32447;&#24615;&#32467;&#26500;&#30340;&#36741;&#21161;&#26412;&#26500;&#27169;&#22411;&#25152;&#25511;&#21046;&#65292;&#21516;&#26102;&#36824;&#21253;&#25324;&#34920;&#29616;&#20986;&#24613;&#21095;&#26799;&#24230;&#21644;&#24555;&#36895;&#21464;&#24418;&#26448;&#26009;&#30028;&#38754;&#30340;&#28436;&#21270;&#29366;&#24577;&#22330;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#22810;&#21151;&#33021;&#19988;&#36890;&#29992;&#30340;&#24402;&#32435;&#20559;&#35265;&#26041;&#27861;&#65292;&#29992;&#20110;&#27169;&#25311;&#36890;&#29992;&#30340;&#38750;&#32447;&#24615;&#22330;&#28436;&#21464;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32858;&#28966;&#20110;&#26368;&#36817;&#30340;&#29289;&#29702;&#24863;&#30693;&#24490;&#29615;&#21367;&#31215;&#65288;PARC&#65289;&#65292;&#23427;&#32467;&#21512;&#20102;&#19968;&#31181;&#21306;&#20998;-&#31215;&#20998;&#22120;&#32467;&#26500;&#65292;&#24402;&#32435;&#22320;&#27169;&#25311;&#20102;&#36890;&#29992;&#29289;&#29702;&#31995;&#32479;&#30340;&#26102;&#31354;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;PARC&#30340;&#21151;&#33021;&#65292;&#20197;&#27169;&#25311;&#19981;&#31283;&#23450;&#12289;&#30636;&#24577;&#21644;&#20256;&#36755;&#20027;&#23548;&#31995;&#32479;&#12290;&#36825;&#20010;&#25193;&#23637;&#27169;&#22411;&#34987;&#31216;&#20026;PARCv2&#65292;&#37197;&#22791;&#20102;&#24494;&#20998;&#31639;&#23376;&#26469;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12503v1 Announce Type: new  Abstract: Modeling unsteady, fast transient, and advection-dominated physics problems is a pressing challenge for physics-aware deep learning (PADL). The physics of complex systems is governed by large systems of partial differential equations (PDEs) and ancillary constitutive models with nonlinear structures, as well as evolving state fields exhibiting sharp gradients and rapidly deforming material interfaces. Here, we investigate an inductive bias approach that is versatile and generalizable to model generic nonlinear field evolution problems. Our study focuses on the recent physics-aware recurrent convolutions (PARC), which incorporates a differentiator-integrator architecture that inductively models the spatiotemporal dynamics of generic physical systems. We extend the capabilities of PARC to simulate unsteady, transient, and advection-dominant systems. The extended model, referred to as PARCv2, is equipped with differential operators to model
&lt;/p&gt;</description></item><item><title>&#23558;kNN&#19982;&#33258;&#28982;&#22270;&#20687;&#19978;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#32467;&#21512;&#65292;&#29420;&#31435;&#23384;&#20648;&#35757;&#32451;&#25968;&#25454;&#30340;&#23884;&#20837;&#65292;&#23454;&#29616;&#21160;&#24577;&#25968;&#25454;&#20462;&#25913;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#65292;&#25552;&#21319;&#22270;&#20687;&#20998;&#31867;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#36866;&#24212;&#24615;</title><link>https://arxiv.org/abs/2402.12500</link><description>&lt;p&gt;
&#23558;kNN&#19982;&#22522;&#30784;&#27169;&#22411;&#32467;&#21512;&#65292;&#23454;&#29616;&#36866;&#24212;&#24615;&#21644;&#27880;&#37325;&#38544;&#31169;&#30340;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Integrating kNN with Foundation Models for Adaptable and Privacy-Aware Image Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12500
&lt;/p&gt;
&lt;p&gt;
&#23558;kNN&#19982;&#33258;&#28982;&#22270;&#20687;&#19978;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#32467;&#21512;&#65292;&#29420;&#31435;&#23384;&#20648;&#35757;&#32451;&#25968;&#25454;&#30340;&#23884;&#20837;&#65292;&#23454;&#29616;&#21160;&#24577;&#25968;&#25454;&#20462;&#25913;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#65292;&#25552;&#21319;&#22270;&#20687;&#20998;&#31867;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#36866;&#24212;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38544;&#21547;&#22320;&#32534;&#30721;&#30693;&#35782;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#36879;&#26126;&#24230;&#21644;&#36866;&#24212;&#25968;&#25454;&#21464;&#21270;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#36866;&#24212;&#24615;&#23545;&#35299;&#20915;&#29992;&#25143;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#36890;&#36807;&#29420;&#31435;&#23384;&#20648;&#22522;&#30784;&#35757;&#32451;&#25968;&#25454;&#30340;&#23884;&#20837;&#26469;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#32780;&#19981;&#26159;&#20381;&#36182;&#27169;&#22411;&#26435;&#37325;&#65292;&#20174;&#32780;&#23454;&#29616;&#21160;&#24577;&#25968;&#25454;&#20462;&#25913;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;k&#36817;&#37051;&#65288;k-NN&#65289;&#20998;&#31867;&#22120;&#19982;&#22522;&#20110;&#35270;&#35273;&#30340;&#22522;&#30784;&#27169;&#22411;&#38598;&#25104;&#65292;&#35813;&#27169;&#22411;&#22312;&#33258;&#28982;&#22270;&#20687;&#19978;&#36827;&#34892;&#20102;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#25552;&#39640;&#20102;&#21487;&#35299;&#37322;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;&#25105;&#20204;&#20849;&#20139;&#20102;&#20043;&#21069;&#26410;&#20844;&#24320;&#30340;&#22522;&#32447;&#26041;&#27861;&#30340;&#24320;&#28304;&#23454;&#29616;&#65292;&#20197;&#21450;&#25105;&#20204;&#30340;&#24615;&#33021;&#25552;&#21319;&#36129;&#29486;&#12290;&#23450;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;&#22312;&#24050;&#24314;&#31435;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#25913;&#21892;&#20998;&#31867;&#25928;&#26524;&#20197;&#21450;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#35813;&#26041;&#27861;&#22312;cont
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12500v1 Announce Type: cross  Abstract: Traditional deep learning models implicity encode knowledge limiting their transparency and ability to adapt to data changes. Yet, this adaptability is vital for addressing user data privacy concerns. We address this limitation by storing embeddings of the underlying training data independently of the model weights, enabling dynamic data modifications without retraining. Specifically, our approach integrates the $k$-Nearest Neighbor ($k$-NN) classifier with a vision-based foundation model, pre-trained self-supervised on natural images, enhancing interpretability and adaptability. We share open-source implementations of a previously unpublished baseline method as well as our performance-improving contributions. Quantitative experiments confirm improved classification across established benchmark datasets and the method's applicability to distinct medical image classification tasks. Additionally, we assess the method's robustness in cont
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#33258;&#36866;&#24212;&#29468;&#24819;&#30340;&#22312;&#32447;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;IT&#22522;&#30784;&#35774;&#26045;&#30340;&#33258;&#21160;&#21270;&#23433;&#20840;&#21709;&#24212;&#26041;&#27861;&#65292;&#20854;&#20013;&#28216;&#25103;&#21442;&#19982;&#32773;&#36890;&#36807;Bayesian&#23398;&#20064;&#35843;&#25972;&#29468;&#24819;&#65292;&#24182;&#36890;&#36807;&#25512;&#28436;&#26356;&#26032;&#31574;&#30053;&#65292;&#26368;&#32456;&#23454;&#29616;&#20102;&#26368;&#20339;&#25311;&#21512;&#65292;&#25552;&#39640;&#20102;&#25512;&#28436;&#22312;&#29468;&#24819;&#27169;&#22411;&#19979;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.12499</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#36866;&#24212;&#29468;&#24819;&#30340;&#22312;&#32447;&#23398;&#20064;&#23454;&#29616;&#33258;&#21160;&#21270;&#23433;&#20840;&#21709;&#24212;
&lt;/p&gt;
&lt;p&gt;
Automated Security Response through Online Learning with Adaptive Conjectures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12499
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#33258;&#36866;&#24212;&#29468;&#24819;&#30340;&#22312;&#32447;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;IT&#22522;&#30784;&#35774;&#26045;&#30340;&#33258;&#21160;&#21270;&#23433;&#20840;&#21709;&#24212;&#26041;&#27861;&#65292;&#20854;&#20013;&#28216;&#25103;&#21442;&#19982;&#32773;&#36890;&#36807;Bayesian&#23398;&#20064;&#35843;&#25972;&#29468;&#24819;&#65292;&#24182;&#36890;&#36807;&#25512;&#28436;&#26356;&#26032;&#31574;&#30053;&#65292;&#26368;&#32456;&#23454;&#29616;&#20102;&#26368;&#20339;&#25311;&#21512;&#65292;&#25552;&#39640;&#20102;&#25512;&#28436;&#22312;&#29468;&#24819;&#27169;&#22411;&#19979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#38024;&#23545;IT&#22522;&#30784;&#35774;&#26045;&#30340;&#33258;&#21160;&#21270;&#23433;&#20840;&#21709;&#24212;&#65292;&#24182;&#23558;&#25915;&#20987;&#32773;&#21644;&#38450;&#24481;&#32773;&#20043;&#38388;&#30340;&#20114;&#21160;&#24418;&#24335;&#34920;&#36848;&#20026;&#19968;&#20010;&#37096;&#20998;&#35266;&#27979;&#12289;&#38750;&#24179;&#31283;&#21338;&#24328;&#12290;&#25105;&#20204;&#25918;&#23485;&#20102;&#28216;&#25103;&#27169;&#22411;&#27491;&#30830;&#35268;&#23450;&#30340;&#26631;&#20934;&#20551;&#35774;&#65292;&#24182;&#32771;&#34385;&#27599;&#20010;&#21442;&#19982;&#32773;&#23545;&#27169;&#22411;&#26377;&#19968;&#20010;&#27010;&#29575;&#24615;&#29468;&#24819;&#65292;&#21487;&#33021;&#22312;&#26576;&#31181;&#24847;&#20041;&#19978;&#38169;&#35823;&#35268;&#23450;&#65292;&#21363;&#30495;&#23454;&#27169;&#22411;&#30340;&#27010;&#29575;&#20026;0&#12290;&#36825;&#31181;&#24418;&#24335;&#20801;&#35768;&#25105;&#20204;&#25429;&#25417;&#20851;&#20110;&#22522;&#30784;&#35774;&#26045;&#21644;&#21442;&#19982;&#32773;&#24847;&#22270;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20026;&#20102;&#22312;&#32447;&#23398;&#20064;&#26377;&#25928;&#30340;&#28216;&#25103;&#31574;&#30053;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#19968;&#20010;&#21442;&#19982;&#32773;&#36890;&#36807;&#36125;&#21494;&#26031;&#23398;&#20064;&#36845;&#20195;&#22320;&#35843;&#25972;&#20854;&#29468;&#24819;&#65292;&#24182;&#36890;&#36807;&#25512;&#28436;&#26356;&#26032;&#20854;&#31574;&#30053;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#29468;&#24819;&#20250;&#25910;&#25947;&#21040;&#26368;&#20339;&#25311;&#21512;&#65292;&#24182;&#25552;&#20379;&#20102;&#22312;&#20855;&#26377;&#29468;&#27979;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#25512;&#28436;&#23454;&#29616;&#24615;&#33021;&#25913;&#36827;&#30340;&#19978;&#38480;&#12290;&#20026;&#20102;&#21051;&#30011;&#28216;&#25103;&#30340;&#31283;&#23450;&#29366;&#24577;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Berk-Nash&#24179;&#34913;&#30340;&#19968;&#20010;&#21464;&#31181;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12499v1 Announce Type: cross  Abstract: We study automated security response for an IT infrastructure and formulate the interaction between an attacker and a defender as a partially observed, non-stationary game. We relax the standard assumption that the game model is correctly specified and consider that each player has a probabilistic conjecture about the model, which may be misspecified in the sense that the true model has probability 0. This formulation allows us to capture uncertainty about the infrastructure and the intents of the players. To learn effective game strategies online, we design a novel method where a player iteratively adapts its conjecture using Bayesian learning and updates its strategy through rollout. We prove that the conjectures converge to best fits, and we provide a bound on the performance improvement that rollout enables with a conjectured model. To characterize the steady state of the game, we propose a variant of the Berk-Nash equilibrium. We 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#23553;&#24314;&#23398;&#20064;&#30340;&#35270;&#35273;&#23548;&#33322;&#65292;&#36890;&#36807;&#39640;&#32423;&#31649;&#29702;&#32773;&#12289;&#20013;&#32423;&#31649;&#29702;&#32773;&#21644;&#24037;&#20316;&#20195;&#29702;&#30340;&#20998;&#23618;&#32467;&#26500;&#65292;&#22312;&#19981;&#21516;&#31354;&#38388;&#21644;&#26102;&#38388;&#23610;&#24230;&#19978;&#25805;&#20316;&#65292;&#20855;&#26377;&#29420;&#29305;&#27169;&#22359;&#26469;&#23454;&#29616;&#33258;&#30417;&#30563;&#23398;&#20064;&#35760;&#24518;&#20195;&#29702;&#22320;&#22270;&#12290;</title><link>https://arxiv.org/abs/2402.12498</link><description>&lt;p&gt;
&#23553;&#24314;&#32593;&#32476;&#29992;&#20110;&#35270;&#35273;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Feudal Networks for Visual Navigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12498
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#23553;&#24314;&#23398;&#20064;&#30340;&#35270;&#35273;&#23548;&#33322;&#65292;&#36890;&#36807;&#39640;&#32423;&#31649;&#29702;&#32773;&#12289;&#20013;&#32423;&#31649;&#29702;&#32773;&#21644;&#24037;&#20316;&#20195;&#29702;&#30340;&#20998;&#23618;&#32467;&#26500;&#65292;&#22312;&#19981;&#21516;&#31354;&#38388;&#21644;&#26102;&#38388;&#23610;&#24230;&#19978;&#25805;&#20316;&#65292;&#20855;&#26377;&#29420;&#29305;&#27169;&#22359;&#26469;&#23454;&#29616;&#33258;&#30417;&#30563;&#23398;&#20064;&#35760;&#24518;&#20195;&#29702;&#22320;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#23548;&#33322;&#36981;&#24490;&#20154;&#31867;&#21487;&#20197;&#22312;&#27809;&#26377;&#35814;&#32454;&#22320;&#22270;&#30340;&#24773;&#20917;&#19979;&#23548;&#33322;&#30340;&#30452;&#35273;&#12290;&#19968;&#31181;&#24120;&#35265;&#26041;&#27861;&#26159;&#22312;&#24314;&#31435;&#21253;&#21547;&#21487;&#29992;&#20110;&#35268;&#21010;&#30340;&#22270;&#20687;&#33410;&#28857;&#30340;&#25299;&#25169;&#22270;&#30340;&#21516;&#26102;&#36827;&#34892;&#20132;&#20114;&#24335;&#25506;&#32034;&#12290;&#26368;&#36817;&#30340;&#21464;&#20307;&#20174;&#34987;&#21160;&#35270;&#39057;&#20013;&#23398;&#20064;&#65292;&#24182;&#21487;&#20197;&#21033;&#29992;&#22797;&#26434;&#30340;&#31038;&#20132;&#21644;&#35821;&#20041;&#32447;&#32034;&#36827;&#34892;&#23548;&#33322;&#12290;&#28982;&#32780;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#35270;&#39057;&#65292;&#21033;&#29992;&#22823;&#22411;&#22270;&#24182;&#19988;&#30001;&#20110;&#20351;&#29992;&#20102;&#37324;&#31243;&#35745;&#65292;&#22330;&#26223;&#19981;&#26159;&#26410;&#30693;&#30340;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20351;&#29992;&#23553;&#24314;&#23398;&#20064;&#30340;&#35270;&#35273;&#23548;&#33322;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#20102;&#30001;&#24037;&#20316;&#20195;&#29702;&#12289;&#20013;&#32423;&#31649;&#29702;&#32773;&#21644;&#39640;&#32423;&#31649;&#29702;&#32773;&#32452;&#25104;&#30340;&#20998;&#23618;&#32467;&#26500;&#12290;&#23553;&#24314;&#23398;&#20064;&#33539;&#24335;&#30340;&#20851;&#38190;&#22312;&#20110;&#65292;&#27599;&#20010;&#32423;&#21035;&#30340;&#20195;&#29702;&#30475;&#21040;&#20219;&#21153;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#23610;&#24230;&#19978;&#36816;&#20316;&#12290;&#22312;&#27492;&#26694;&#26550;&#20013;&#24320;&#21457;&#20102;&#20004;&#20010;&#29420;&#29305;&#30340;&#27169;&#22359;&#12290;&#23545;&#20110;&#39640;&#32423;&#31649;&#29702;&#32773;&#65292;&#25105;&#20204;&#33258;&#30417;&#30563;&#22320;&#23398;&#20064;&#19968;&#20010;&#35760;&#24518;&#20195;&#29702;&#22320;&#22270;&#20197;&#35760;&#24405;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12498v1 Announce Type: cross  Abstract: Visual navigation follows the intuition that humans can navigate without detailed maps. A common approach is interactive exploration while building a topological graph with images at nodes that can be used for planning. Recent variations learn from passive videos and can navigate using complex social and semantic cues. However, a significant number of training videos are needed, large graphs are utilized, and scenes are not unseen since odometry is utilized. We introduce a new approach to visual navigation using feudal learning, which employs a hierarchical structure consisting of a worker agent, a mid-level manager, and a high-level manager. Key to the feudal learning paradigm, agents at each level see a different aspect of the task and operate at different spatial and temporal scales. Two unique modules are developed in this framework. For the high- level manager, we learn a memory proxy map in a self supervised manner to record prio
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#36328;&#39046;&#22495;&#25345;&#32493;&#23398;&#20064;&#65288;CDCL&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#20219;&#21153;&#38388;&#21644;&#20219;&#21153;&#20869;&#30340;&#20132;&#21449;&#27880;&#24847;&#26426;&#21046;&#65292;&#22312;&#32039;&#20945;&#30340;&#21367;&#31215;&#32593;&#32476;&#20013;&#24310;&#36831;&#25968;&#25454;&#28418;&#31227;&#65292;&#23454;&#29616;&#20102;&#26080;&#30417;&#30563;&#30340;&#36328;&#39046;&#22495;&#23398;&#20064;&#65288;UDA&#65289;&#12290;</title><link>https://arxiv.org/abs/2402.12490</link><description>&lt;p&gt;
&#36328;&#39046;&#22495;&#25345;&#32493;&#23398;&#20064;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Cross-Domain Continual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12490
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#36328;&#39046;&#22495;&#25345;&#32493;&#23398;&#20064;&#65288;CDCL&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#20219;&#21153;&#38388;&#21644;&#20219;&#21153;&#20869;&#30340;&#20132;&#21449;&#27880;&#24847;&#26426;&#21046;&#65292;&#22312;&#32039;&#20945;&#30340;&#21367;&#31215;&#32593;&#32476;&#20013;&#24310;&#36831;&#25968;&#25454;&#28418;&#31227;&#65292;&#23454;&#29616;&#20102;&#26080;&#30417;&#30563;&#30340;&#36328;&#39046;&#22495;&#23398;&#20064;&#65288;UDA&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#26159;&#19968;&#20010;&#36807;&#31243;&#65292;&#28041;&#21450;&#35757;&#32451;&#23398;&#20064;&#20195;&#29702;&#20197;&#39034;&#24207;&#22320;&#25484;&#25569;&#19968;&#31995;&#21015;&#20219;&#21153;&#25110;&#31867;&#21035;&#65292;&#32780;&#19981;&#38656;&#35201;&#37325;&#26032;&#22238;&#39038;&#36807;&#21435;&#30340;&#25968;&#25454;&#12290;&#25361;&#25112;&#22312;&#20110;&#21033;&#29992;&#20808;&#21069;&#33719;&#24471;&#30340;&#30693;&#35782;&#26377;&#25928;&#22320;&#23398;&#20064;&#26032;&#20219;&#21153;&#65292;&#21516;&#26102;&#36991;&#20813;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#21333;&#19968;&#39046;&#22495;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#29305;&#23450;&#38382;&#39064;&#19978;&#30340;&#36866;&#29992;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#36328;&#39046;&#22495;&#25345;&#32493;&#23398;&#20064;&#65288;CDCL&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#35299;&#20915;&#20102;&#34987;&#38480;&#21046;&#22312;&#21333;&#19968;&#30417;&#30563;&#39046;&#22495;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#32039;&#20945;&#30340;&#21367;&#31215;&#32593;&#32476;&#20013;&#32467;&#21512;&#20102;&#20219;&#21153;&#38388;&#21644;&#20219;&#21153;&#20869;&#30340;&#20132;&#21449;&#27880;&#24847;&#26426;&#21046;&#12290;&#36825;&#31181;&#25972;&#21512;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#19982;&#20808;&#21069;&#20219;&#21153;&#30340;&#29305;&#24449;&#20445;&#25345;&#23545;&#40784;&#65292;&#20174;&#32780;&#24310;&#36831;&#21487;&#33021;&#21457;&#29983;&#22312;&#20219;&#21153;&#20043;&#38388;&#30340;&#25968;&#25454;&#28418;&#31227;&#65292;&#21516;&#26102;&#22312;&#30456;&#20851;&#39046;&#22495;&#20043;&#38388;&#36827;&#34892;&#26080;&#30417;&#30563;&#30340;&#36328;&#39046;&#22495;&#23398;&#20064;&#65288;UDA&#65289;&#12290;&#36890;&#36807;&#21033;&#29992;&#20219;&#21153;&#20869;&#20855;&#20307;&#30340;&#20266;&#26631;&#31614;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12490v1 Announce Type: cross  Abstract: Continual learning is a process that involves training learning agents to sequentially master a stream of tasks or classes without revisiting past data. The challenge lies in leveraging previously acquired knowledge to learn new tasks efficiently, while avoiding catastrophic forgetting. Existing methods primarily focus on single domains, restricting their applicability to specific problems.   In this work, we introduce a novel approach called Cross-Domain Continual Learning (CDCL) that addresses the limitations of being limited to single supervised domains. Our method combines inter- and intra-task cross-attention mechanisms within a compact convolutional network. This integration enables the model to maintain alignment with features from previous tasks, thereby delaying the data drift that may occur between tasks, while performing unsupervised cross-domain (UDA) between related domains. By leveraging an intra-task-specific pseudo-labe
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22522;&#20110;&#35821;&#38899;&#22686;&#24378;&#30340;&#31574;&#21010;&#31649;&#36947;&#65288;SECP&#65289;&#65292;&#21487;&#20197;&#22312;&#35268;&#27169;&#19978;&#33719;&#21462;&#24178;&#20928;&#35821;&#38899;&#24182;&#35757;&#32451;&#35821;&#38899;&#22686;&#24378;&#27169;&#22411;&#65292;&#36890;&#36807;&#20004;&#36718;&#36845;&#20195;&#35266;&#23519;&#21040;&#22686;&#24378;&#36755;&#20986;&#20316;&#20026;&#22522;&#20934;&#19981;&#20250;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#20027;&#35266;&#27979;&#35797;&#35777;&#26126;&#20248;&#21270;&#25968;&#25454;&#22312;&#24863;&#30693;&#19978;&#20248;&#20110;&#21407;&#22987;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2402.12482</link><description>&lt;p&gt;
SECP&#65306;&#22522;&#20110;&#35821;&#38899;&#22686;&#24378;&#30340;&#31574;&#21010;&#31649;&#36947;&#65292;&#29992;&#20110;&#21487;&#25193;&#23637;&#33719;&#21462;&#24178;&#20928;&#35821;&#38899;
&lt;/p&gt;
&lt;p&gt;
SECP: A Speech Enhancement-Based Curation Pipeline For Scalable Acquisition Of Clean Speech
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12482
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22522;&#20110;&#35821;&#38899;&#22686;&#24378;&#30340;&#31574;&#21010;&#31649;&#36947;&#65288;SECP&#65289;&#65292;&#21487;&#20197;&#22312;&#35268;&#27169;&#19978;&#33719;&#21462;&#24178;&#20928;&#35821;&#38899;&#24182;&#35757;&#32451;&#35821;&#38899;&#22686;&#24378;&#27169;&#22411;&#65292;&#36890;&#36807;&#20004;&#36718;&#36845;&#20195;&#35266;&#23519;&#21040;&#22686;&#24378;&#36755;&#20986;&#20316;&#20026;&#22522;&#20934;&#19981;&#20250;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#20027;&#35266;&#27979;&#35797;&#35777;&#26126;&#20248;&#21270;&#25968;&#25454;&#22312;&#24863;&#30693;&#19978;&#20248;&#20110;&#21407;&#22987;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36234;&#26469;&#36234;&#22810;&#30340;&#35821;&#38899;&#25216;&#26415;&#20381;&#36182;&#20110;&#20197;&#24178;&#20928;&#35821;&#38899;&#20026;&#22522;&#20934;&#30340;&#30417;&#30563;&#24335;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#38656;&#35201;&#19968;&#31181;&#26041;&#27861;&#26469;&#22312;&#35268;&#27169;&#19978;&#25509;&#20837;&#36825;&#20123;&#35821;&#38899;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#38656;&#35201;&#26368;&#22823;&#31243;&#24230;&#22320;&#20943;&#23569;&#23545;&#20154;&#31867;&#21548;&#35273;&#21644;&#27880;&#37322;&#30340;&#20381;&#36182;&#65292;&#21482;&#22312;&#38656;&#35201;&#26102;&#38656;&#35201;&#20154;&#31867;&#20171;&#20837;&#12290;&#26412;&#25991;&#36890;&#36807;&#27010;&#36848;&#22522;&#20110;&#35821;&#38899;&#22686;&#24378;&#30340;&#31574;&#21010;&#31649;&#36947;&#65288;SECP&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20316;&#20026;&#19968;&#20010;&#26694;&#26550;&#29992;&#26469;&#25509;&#20837;&#24178;&#20928;&#35821;&#38899;&#12290;&#36825;&#20123;&#24178;&#20928;&#35821;&#38899;&#21487;&#20197;&#29992;&#26469;&#35757;&#32451;&#35821;&#38899;&#22686;&#24378;&#27169;&#22411;&#65292;&#36827;&#19968;&#27493;&#25913;&#36827;&#21407;&#22987;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#20851;&#38381;&#36845;&#20195;&#24490;&#29615;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#36718;&#36845;&#20195;&#30340;&#23454;&#39564;&#35266;&#23519;&#21040;&#65292;&#20316;&#20026;&#22522;&#20934;&#30340;&#22686;&#24378;&#36755;&#20986;&#19981;&#20250;&#20351;&#27169;&#22411;&#24615;&#33021;&#26681;&#25454;&#26412;&#25991;&#20351;&#29992;&#30340; $\Delta_{PESQ}$ &#25351;&#26631;&#19979;&#38477;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#22522;&#20110;&#27604;&#36739;&#22343;&#20540;&#24847;&#35265;&#35780;&#20998;&#65288;CMOS&#65289;&#30340;&#20027;&#35266;&#27979;&#35797;&#34920;&#26126;&#65292;&#20248;&#21270;&#25968;&#25454;&#30340;&#26368;&#39640;&#21644;&#26368;&#20302;&#36793;&#30028;&#22312;&#24863;&#30693;&#19978;&#20248;&#20110;&#21407;&#22987;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12482v1 Announce Type: cross  Abstract: As more speech technologies rely on a supervised deep learning approach with clean speech as the ground truth, a methodology to onboard said speech at scale is needed. However, this approach needs to minimize the dependency on human listening and annotation, only requiring a human-in-the-loop when needed. In this paper, we address this issue by outlining Speech Enhancement-based Curation Pipeline (SECP) which serves as a framework to onboard clean speech. This clean speech can then train a speech enhancement model, which can further refine the original dataset and thus close the iterative loop. By running two iterative rounds, we observe that enhanced output used as ground truth does not degrade model performance according to $\Delta_{PESQ}$, a metric used in this paper. We also show through comparative mean opinion score (CMOS) based subjective tests that the highest and lowest bound of refined data is perceptually better than the ori
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36880;&#28176;&#21098;&#26525;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#26368;&#22823;&#31243;&#24230;&#22320;&#21457;&#25381;&#21442;&#25968;&#25928;&#33021;&#65292;&#20174;&#32780;&#20135;&#29983;&#27604;&#20256;&#32479;&#32593;&#32476;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#30340;&#32593;&#32476;&#65292;&#24182;&#23637;&#29616;&#20986;&#19968;&#31181;&#8220;&#32553;&#25918;&#23450;&#24459;&#8221;&#12290;</title><link>https://arxiv.org/abs/2402.12479</link><description>&lt;p&gt;
&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#20462;&#21098;&#32593;&#32476;&#26159;&#19968;&#20010;&#22909;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
In deep reinforcement learning, a pruned network is a good network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12479
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36880;&#28176;&#21098;&#26525;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#26368;&#22823;&#31243;&#24230;&#22320;&#21457;&#25381;&#21442;&#25968;&#25928;&#33021;&#65292;&#20174;&#32780;&#20135;&#29983;&#27604;&#20256;&#32479;&#32593;&#32476;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#30340;&#32593;&#32476;&#65292;&#24182;&#23637;&#29616;&#20986;&#19968;&#31181;&#8220;&#32553;&#25918;&#23450;&#24459;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#22312;&#26377;&#25928;&#21033;&#29992;&#20854;&#32593;&#32476;&#21442;&#25968;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#25105;&#20204;&#21033;&#29992;&#23545;&#31232;&#30095;&#35757;&#32451;&#25216;&#26415;&#20248;&#21183;&#30340;&#20808;&#21069;&#35265;&#35299;&#65292;&#24182;&#35777;&#26126;&#36880;&#28176;&#21098;&#26525;&#20351;&#20195;&#29702;&#33021;&#22815;&#26368;&#22823;&#31243;&#24230;&#22320;&#21457;&#25381;&#21442;&#25968;&#25928;&#33021;&#12290;&#36825;&#23548;&#33268;&#32593;&#32476;&#27604;&#20256;&#32479;&#32593;&#32476;&#20135;&#29983;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#24182;&#34920;&#29616;&#20986;&#19968;&#31181;&#8220;&#32553;&#25918;&#23450;&#24459;&#8221;&#65292;&#20165;&#20351;&#29992;&#23436;&#25972;&#32593;&#32476;&#21442;&#25968;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12479v1 Announce Type: cross  Abstract: Recent work has shown that deep reinforcement learning agents have difficulty in effectively using their network parameters. We leverage prior insights into the advantages of sparse training techniques and demonstrate that gradual magnitude pruning enables agents to maximize parameter effectiveness. This results in networks that yield dramatic performance improvements over traditional networks and exhibit a type of "scaling law", using only a small fraction of the full network parameters.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24494;&#20998;&#21516;&#32986;&#31070;&#32463;&#31639;&#23376;&#23398;&#20064;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#21508;&#31181;&#21644;&#22797;&#26434;&#39046;&#22495;&#30340;&#29289;&#29702;&#31995;&#32479;&#30340;&#39046;&#22495;&#28789;&#27963;&#27169;&#22411;&#65292;&#20174;&#32780;&#23558;&#23398;&#20064;&#20989;&#25968;&#26144;&#23556;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#38382;&#39064;&#36716;&#21270;&#20026;&#22312;&#20849;&#20139;&#30340;&#24494;&#20998;&#21516;&#32986;&#19978;&#23398;&#20064;&#31639;&#23376;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.12475</link><description>&lt;p&gt;
&#19981;&#21516;&#39046;&#22495;&#21644;&#21442;&#25968;&#30340;&#24494;&#20998;&#21516;&#32986;&#31070;&#32463;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
Diffeomorphism Neural Operator for various domains and parameters of partial differential equations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12475
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24494;&#20998;&#21516;&#32986;&#31070;&#32463;&#31639;&#23376;&#23398;&#20064;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#21508;&#31181;&#21644;&#22797;&#26434;&#39046;&#22495;&#30340;&#29289;&#29702;&#31995;&#32479;&#30340;&#39046;&#22495;&#28789;&#27963;&#27169;&#22411;&#65292;&#20174;&#32780;&#23558;&#23398;&#20064;&#20989;&#25968;&#26144;&#23556;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#38382;&#39064;&#36716;&#21270;&#20026;&#22312;&#20849;&#20139;&#30340;&#24494;&#20998;&#21516;&#32986;&#19978;&#23398;&#20064;&#31639;&#23376;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#31185;&#23398;&#21644;&#24037;&#31243;&#24212;&#29992;&#38656;&#35201;&#23545;&#20256;&#32479;&#19978;&#20351;&#29992;&#36164;&#28304;&#23494;&#38598;&#22411;&#25968;&#20540;&#27714;&#35299;&#22120;&#35745;&#31639;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#36827;&#34892;&#35780;&#20272;&#12290;&#31070;&#32463;&#31639;&#23376;&#27169;&#22411;&#36890;&#36807;&#30452;&#25509;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#25511;&#21046;&#29289;&#29702;&#23450;&#24459;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#19981;&#21516;&#21442;&#25968;&#30340;PDE&#31867;&#21035;&#65292;&#20294;&#22312;&#22266;&#23450;&#36793;&#30028;&#65288;&#39046;&#22495;&#65289;&#20869;&#21463;&#38480;&#12290;&#35768;&#22810;&#24212;&#29992;&#65292;&#20363;&#22914;&#35774;&#35745;&#21644;&#21046;&#36896;&#65292;&#22312;&#22823;&#35268;&#27169;&#30740;&#31350;&#26102;&#23558;&#21463;&#30410;&#20110;&#20855;&#26377;&#28789;&#27963;&#39046;&#22495;&#30340;&#31070;&#32463;&#31639;&#23376;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24494;&#20998;&#21516;&#32986;&#31070;&#32463;&#31639;&#23376;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#20026;&#20855;&#26377;&#21508;&#31181;&#21644;&#22797;&#26434;&#39046;&#22495;&#30340;&#29289;&#29702;&#31995;&#32479;&#24320;&#21457;&#39046;&#22495;&#28789;&#27963;&#27169;&#22411;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#30001;&#24494;&#20998;&#21516;&#32986;&#20174;&#21508;&#39046;&#22495;&#26144;&#23556;&#32780;&#26469;&#30340;&#20849;&#20139;&#39046;&#22495;&#20013;&#35757;&#32451;&#30340;&#31070;&#32463;&#31639;&#23376;&#65292;&#35813;&#26041;&#27861;&#23558;&#22312;&#19981;&#21516;&#39046;&#22495;&#65288;&#31354;&#38388;&#65289;&#23398;&#20064;&#20989;&#25968;&#26144;&#23556;&#30340;&#38382;&#39064;&#36716;&#21270;&#20026;&#22312;&#20849;&#20139;&#30340;&#24494;&#20998;&#21516;&#32986;&#19978;&#23398;&#20064;&#31639;&#23376;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12475v1 Announce Type: cross  Abstract: Many science and engineering applications demand partial differential equations (PDE) evaluations that are traditionally computed with resource-intensive numerical solvers. Neural operator models provide an efficient alternative by learning the governing physical laws directly from data in a class of PDEs with different parameters, but constrained in a fixed boundary (domain). Many applications, such as design and manufacturing, would benefit from neural operators with flexible domains when studied at scale. Here we present a diffeomorphism neural operator learning framework towards developing domain-flexible models for physical systems with various and complex domains. Specifically, a neural operator trained in a shared domain mapped from various domains of fields by diffeomorphism is proposed, which transformed the problem of learning function mappings in varying domains (spaces) into the problem of learning operators on a shared dif
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#25345;&#32493;&#23398;&#20064;&#33021;&#21147;&#30340;&#31070;&#32463;&#20223;&#30495;&#31995;&#32479;&#65292;&#20811;&#26381;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#30740;&#31350;&#20102;&#22312;&#26080;&#30417;&#30563;&#26550;&#26500;&#20013;&#30340;&#24212;&#29992;&#65292;&#23588;&#20854;&#26159;&#33258;&#32452;&#32455;&#26144;&#23556;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.12465</link><description>&lt;p&gt;
&#20855;&#26377;&#25345;&#32493;&#33258;&#32452;&#32455;&#26144;&#23556;&#30340;&#31070;&#32463;&#20223;&#30495;&#26080;&#20219;&#21153;&#26080;&#30417;&#30563;&#22312;&#32447;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Neuro-mimetic Task-free Unsupervised Online Learning with Continual Self-Organizing Maps
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12465
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#25345;&#32493;&#23398;&#20064;&#33021;&#21147;&#30340;&#31070;&#32463;&#20223;&#30495;&#31995;&#32479;&#65292;&#20811;&#26381;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#30740;&#31350;&#20102;&#22312;&#26080;&#30417;&#30563;&#26550;&#26500;&#20013;&#30340;&#24212;&#29992;&#65292;&#23588;&#20854;&#26159;&#33258;&#32452;&#32455;&#26144;&#23556;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#20855;&#26377;&#25345;&#32493;&#23398;&#20064;&#33021;&#21147;&#30340;&#26234;&#33021;&#31995;&#32479;&#33021;&#22815;&#20174;&#28508;&#22312;&#30340;&#26080;&#38480;&#38271;&#30340;&#27169;&#24335;&#21521;&#37327;&#27969;&#20013;&#25552;&#21462;&#30693;&#35782;&#12290;&#36825;&#31181;&#31995;&#32479;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#21363;&#24403;&#23398;&#20064;&#26032;&#26679;&#26412;&#26102;&#65292;&#22914;&#22522;&#20110;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#30340;&#20195;&#29702;&#26080;&#27861;&#20445;&#30041;&#20808;&#21069;&#33719;&#24471;&#30340;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#24182;&#27809;&#26377;&#34917;&#20805;&#20219;&#21153;&#36793;&#30028;&#20449;&#24687;&#30340;&#36755;&#20837;&#20250;&#20351;&#20026;&#20197;&#21069;&#30340;&#20219;&#21153;&#20445;&#30041;&#30693;&#35782;&#21464;&#24471;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#23613;&#31649;&#22312;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#32972;&#26223;&#19979;&#23545;&#36951;&#24536;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#22312;&#26080;&#30417;&#30563;&#26550;&#26500;&#26041;&#38754;&#30340;&#30740;&#31350;&#24037;&#20316;&#36824;&#36828;&#36828;&#19981;&#22815;&#65292;&#27604;&#22914;&#33879;&#21517;&#30340;&#33258;&#32452;&#32455;&#26144;&#23556;&#65288;SOM&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#24120;&#29992;&#20110;&#32858;&#31867;&#21644;&#38477;&#32500;&#30340;&#31070;&#32463;&#27169;&#22411;&#12290;&#23613;&#31649;SOM&#30340;&#20869;&#37096;&#26426;&#21046;&#21407;&#21017;&#19978;&#21487;&#33021;&#20135;&#29983;&#25913;&#36827;&#35760;&#24518;&#30340;&#31232;&#30095;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12465v1 Announce Type: new  Abstract: An intelligent system capable of continual learning is one that can process and extract knowledge from potentially infinitely long streams of pattern vectors. The major challenge that makes crafting such a system difficult is known as catastrophic forgetting - an agent, such as one based on artificial neural networks (ANNs), struggles to retain previously acquired knowledge when learning from new samples. Furthermore, ensuring that knowledge is preserved for previous tasks becomes more challenging when input is not supplemented with task boundary information. Although forgetting in the context of ANNs has been studied extensively, there still exists far less work investigating it in terms of unsupervised architectures such as the venerable self-organizing map (SOM), a neural model often used in clustering and dimensionality reduction. While the internal mechanisms of SOMs could, in principle, yield sparse representations that improve mem
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#24037;&#20855;DBNets&#65292;&#29992;&#20110;&#20998;&#26512;&#21407;&#34892;&#26143;&#30424;&#23576;&#22467;&#36752;&#23556;&#20013;&#30340;&#27425;&#32467;&#26500;&#65292;&#24555;&#36895;&#25512;&#26029;&#23884;&#20837;&#34892;&#26143;&#30340;&#36136;&#37327;&#65292;&#24182;&#33021;&#21487;&#38752;&#37327;&#21270;&#36136;&#37327;&#21644;&#30456;&#20851;&#19981;&#30830;&#23450;&#24615;</title><link>https://arxiv.org/abs/2402.12448</link><description>&lt;p&gt;
DBNets&#65306;&#19968;&#31181;&#20844;&#24320;&#21487;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#24037;&#20855;&#65292;&#29992;&#20110;&#27979;&#37327;&#23576;&#22467;&#21407;&#34892;&#26143;&#30424;&#20013;&#24180;&#36731;&#34892;&#26143;&#30340;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
DBNets: A publicly available deep learning tool to measure the masses of young planets in dusty protoplanetary discs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12448
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#24037;&#20855;DBNets&#65292;&#29992;&#20110;&#20998;&#26512;&#21407;&#34892;&#26143;&#30424;&#23576;&#22467;&#36752;&#23556;&#20013;&#30340;&#27425;&#32467;&#26500;&#65292;&#24555;&#36895;&#25512;&#26029;&#23884;&#20837;&#34892;&#26143;&#30340;&#36136;&#37327;&#65292;&#24182;&#33021;&#21487;&#38752;&#37327;&#21270;&#36136;&#37327;&#21644;&#30456;&#20851;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#29992;&#20110;&#34920;&#24449;&#21407;&#34892;&#26143;&#30424;&#35266;&#27979;&#20013;&#23884;&#20837;&#34892;&#26143;&#30340;&#26041;&#27861;&#22312;&#23436;&#20840;&#32771;&#34385;&#35266;&#23519;&#21040;&#30340;&#22797;&#26434;&#29289;&#29702;&#36807;&#31243;&#26041;&#38754;&#35201;&#20040;&#23384;&#22312;&#20005;&#37325;&#38480;&#21046;&#65292;&#35201;&#20040;&#22312;&#35745;&#31639;&#21644;&#26102;&#38388;&#25104;&#26412;&#19978;&#21463;&#21040;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#32570;&#28857;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;DBNets&#65306;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#24037;&#20855;&#65292;&#20998;&#26512;&#21407;&#34892;&#26143;&#30424;&#23576;&#22467;&#36830;&#32493;&#36752;&#23556;&#20013;&#35266;&#23519;&#21040;&#30340;&#27425;&#32467;&#26500;&#65292;&#24555;&#36895;&#25512;&#26029;&#25454;&#31216;&#23884;&#20837;&#34892;&#26143;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#24320;&#21457;&#19968;&#31181;&#21487;&#38752;&#22320;&#37327;&#21270;&#19981;&#20165;&#34892;&#26143;&#36136;&#37327;&#65292;&#32780;&#19988;&#25105;&#20204;&#24314;&#27169;&#21644;&#37319;&#29992;&#25216;&#26415;&#24341;&#20837;&#30340;&#30456;&#20851;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#27979;&#35797;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#65292;&#19982;&#22312;&#30456;&#21516;&#25968;&#25454;&#19978;&#25311;&#21512;&#30340;&#35299;&#26512;&#20844;&#24335;&#30456;&#27604;&#65292;&#23454;&#29616;&#20102;&#23545;log Mp&#22343;&#26041;&#35823;&#24046;&#30340;87%&#38477;&#20302;&#65288;DBNets&#25351;&#26631;&#65306;lmse 0.016&#65292;r2&#20998;&#25968; 97%&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12448v1 Announce Type: cross  Abstract: Current methods to characterize embedded planets in protoplanetary disc observations are severely limited either in their ability to fully account for the observed complex physics or in their computational and time costs. To address this shortcoming, we developed DBNets: a deep learning tool, based on convolutional neural networks, that analyses substructures observed in the dust continuum emission of protoplanetary discs to quickly infer the mass of allegedly embedded planets. We focussed on developing a method to reliably quantify not only the planet mass, but also the associated uncertainty introduced by our modelling and adopted techniques. Our tests gave promising results achieving an 87% reduction of the log Mp mean squared error with respect to an analytical formula fitted on the same data (DBNets metrics: lmse 0.016, r2-score 97%). With the goal of providing the final user of DBNets with all the tools needed to interpret their 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20351;&#29992;&#31070;&#32463;&#31639;&#23376;&#21462;&#20195;&#35745;&#31639;&#37327;&#22823;&#12289;&#31934;&#30830;&#24230;&#39640;&#12289;&#35745;&#31639;&#20195;&#20215;&#26114;&#36149;&#30340;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;&#65292;&#25104;&#21151;&#27169;&#25311;&#20102;&#26143;&#38469;&#20171;&#36136;&#20013;&#30340;&#38750;&#24179;&#34913;&#21270;&#23398;&#32593;&#32476;&#28436;&#21270;&#65292;&#23545;&#20110;&#23431;&#23449;&#23398;&#21644;&#22825;&#20307;&#29289;&#29702;&#27169;&#25311;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.12435</link><description>&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#31639;&#23376;&#27169;&#25311;&#26143;&#38469;&#20171;&#36136;&#21270;&#23398;
&lt;/p&gt;
&lt;p&gt;
Emulating the interstellar medium chemistry with neural operators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12435
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20351;&#29992;&#31070;&#32463;&#31639;&#23376;&#21462;&#20195;&#35745;&#31639;&#37327;&#22823;&#12289;&#31934;&#30830;&#24230;&#39640;&#12289;&#35745;&#31639;&#20195;&#20215;&#26114;&#36149;&#30340;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;&#65292;&#25104;&#21151;&#27169;&#25311;&#20102;&#26143;&#38469;&#20171;&#36136;&#20013;&#30340;&#38750;&#24179;&#34913;&#21270;&#23398;&#32593;&#32476;&#28436;&#21270;&#65292;&#23545;&#20110;&#23431;&#23449;&#23398;&#21644;&#22825;&#20307;&#29289;&#29702;&#27169;&#25311;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38134;&#27827;&#31995;&#30340;&#24418;&#25104;&#21644;&#36827;&#21270;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#29702;&#35299;&#20027;&#23548;&#26143;&#38469;&#20171;&#36136;&#65288;ISM&#65289;&#28436;&#21270;&#21644;&#28909;&#21147;&#23398;&#30340;&#22797;&#26434;&#20809;&#21270;&#23398;&#36807;&#31243;&#12290;&#22312;&#35745;&#31639;&#19978;&#65292;&#35299;&#20915;&#21270;&#23398;&#38382;&#39064;&#26159;&#23431;&#23449;&#23398;&#21644;&#22825;&#20307;&#29289;&#29702;&#27169;&#25311;&#20013;&#26368;&#32321;&#37325;&#30340;&#20219;&#21153;&#20043;&#19968;&#12290;&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#29992;&#22522;&#20110;&#31070;&#32463;&#31639;&#23376;&#30340;&#24555;&#36895;&#12289;&#39044;&#20808;&#35757;&#32451;&#30340;&#20223;&#30495;&#22120;&#26367;&#20195;&#36825;&#20123;&#31243;&#24207;&#27714;&#35299;&#22120;&#12290;&#25105;&#20204;&#36890;&#36807;&#37319;&#29992;DeepONet&#24418;&#24335;&#20027;&#20041;&#27169;&#25311;&#38750;&#24179;&#34913;&#21270;&#23398;&#32593;&#32476;&#30452;&#33267;H$_2$&#24418;&#25104;&#65288;9&#31181;&#29289;&#31181;&#65292;52&#20010;&#21453;&#24212;&#65289;&#65292;&#23558;&#21021;&#22987;&#26465;&#20214;&#21644;&#26102;&#38388;&#28436;&#21270;&#26144;&#23556;&#20026;&#20004;&#20010;&#31070;&#32463;&#32593;&#32476;&#30340;&#24352;&#37327;&#20056;&#31215;&#26469;&#20998;&#35299;&#26144;&#23556;&#36816;&#31639;&#31526;&#12290;&#25105;&#20204;&#20351;&#29992;$\texttt{KROME}$&#29983;&#25104;&#19968;&#20010;&#35757;&#32451;&#38598;&#65292;&#28085;&#30422;$-2\leq \log(n/\mathrm{cm}^{-3}) \leq 3.5$&#65292;$\log(
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12435v1 Announce Type: cross  Abstract: Galaxy formation and evolution critically depend on understanding the complex photo-chemical processes that govern the evolution and thermodynamics of the InterStellar Medium (ISM). Computationally, solving chemistry is among the most heavy tasks in cosmological and astrophysical simulations. The evolution of such non-equilibrium photo-chemical network relies on implicit, precise, computationally costly, ordinary differential equations (ODE) solvers. Here, we aim at substituting such procedural solvers with fast, pre-trained, emulators based on neural operators. We emulate a non-equilibrium chemical network up to H$_2$ formation (9 species, 52 reactions) by adopting the DeepONet formalism, i.e. by splitting the ODE solver operator that maps the initial conditions and time evolution into a tensor product of two neural networks. We use $\texttt{KROME}$ to generate a training set spanning $-2\leq \log(n/\mathrm{cm}^{-3}) \leq 3.5$, $\log(
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#22522;&#20110;&#29305;&#24449;&#30340;&#23545;&#25239;&#25915;&#20987;&#65292;&#38024;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#33410;&#28857;&#23646;&#24615;&#23637;&#24320;&#30740;&#31350;&#65292;&#21457;&#29616;&#20351;&#29992;Projected Gradient Descent&#30340;&#20915;&#31574;&#26102;&#25915;&#20987;&#27604;&#20351;&#29992;Mean Node Embeddings&#21644;Graph Contrastive Learning&#31574;&#30053;&#30340;&#27602;&#21270;&#25915;&#20987;&#26356;&#21152;&#26377;&#25928;&#12290;</title><link>https://arxiv.org/abs/2402.12426</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#33410;&#28857;&#23646;&#24615;&#30340;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Attacks on Node Attributes in Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12426
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#22522;&#20110;&#29305;&#24449;&#30340;&#23545;&#25239;&#25915;&#20987;&#65292;&#38024;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#33410;&#28857;&#23646;&#24615;&#23637;&#24320;&#30740;&#31350;&#65292;&#21457;&#29616;&#20351;&#29992;Projected Gradient Descent&#30340;&#20915;&#31574;&#26102;&#25915;&#20987;&#27604;&#20351;&#29992;Mean Node Embeddings&#21644;Graph Contrastive Learning&#31574;&#30053;&#30340;&#27602;&#21270;&#25915;&#20987;&#26356;&#21152;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#32463;&#24120;&#29992;&#26469;&#27169;&#22411;&#21270;&#29616;&#20195;&#31038;&#20132;&#23186;&#20307;&#21644;&#25991;&#29486;&#24212;&#29992;&#20013;&#30340;&#22797;&#26434;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36890;&#36807;&#22522;&#20110;&#29305;&#24449;&#30340;&#23545;&#25239;&#25915;&#20987;&#65292;&#37325;&#28857;&#20851;&#27880;&#20915;&#31574;&#26102;&#25915;&#20987;&#21644;&#27602;&#21270;&#25915;&#20987;&#65292;&#25506;&#31350;&#36825;&#20123;&#22270;&#30340;&#33030;&#24369;&#24615;&#12290;&#19982;Net Attack&#21644;Meta Attack&#31561;&#26368;&#20808;&#36827;&#27169;&#22411;&#38024;&#23545;&#33410;&#28857;&#23646;&#24615;&#21644;&#22270;&#32467;&#26500;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#19987;&#38376;&#38024;&#23545;&#33410;&#28857;&#23646;&#24615;&#12290;&#25105;&#20204;&#21033;&#29992;Hellaswag&#25991;&#26412;&#25968;&#25454;&#38598;&#20197;&#21450;&#22270;&#25968;&#25454;&#38598;Cora&#21644;CiteSeer&#36827;&#34892;&#20998;&#26512;&#65292;&#20026;&#35780;&#20272;&#25552;&#20379;&#20102;&#22810;&#26679;&#30340;&#22522;&#30784;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#20351;&#29992;Projected Gradient Descent (PGD)&#30340;&#20915;&#31574;&#26102;&#25915;&#20987;&#27604;&#37319;&#29992;Mean Node Embeddings&#21644;Graph Contrastive Learning&#31574;&#30053;&#30340;&#27602;&#21270;&#25915;&#20987;&#26356;&#20855;&#23041;&#21147;&#12290;&#36825;&#20026;&#22270;&#25968;&#25454;&#23433;&#20840;&#25552;&#20379;&#20102;&#35265;&#35299;&#65292;&#25351;&#20986;&#20102;&#22270;&#22522;&#27169;&#22411;&#26368;&#33030;&#24369;&#30340;&#22320;&#26041;&#65292;&#20174;&#32780;&#20026;&#24320;&#21457;&#24037;&#20316;&#25552;&#20379;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12426v1 Announce Type: cross  Abstract: Graphs are commonly used to model complex networks prevalent in modern social media and literacy applications. Our research investigates the vulnerability of these graphs through the application of feature based adversarial attacks, focusing on both decision-time attacks and poisoning attacks. In contrast to state-of-the-art models like Net Attack and Meta Attack, which target node attributes and graph structure, our study specifically targets node attributes. For our analysis, we utilized the text dataset Hellaswag and graph datasets Cora and CiteSeer, providing a diverse basis for evaluation. Our findings indicate that decision-time attacks using Projected Gradient Descent (PGD) are more potent compared to poisoning attacks that employ Mean Node Embeddings and Graph Contrastive Learning strategies. This provides insights for graph data security, pinpointing where graph-based models are most vulnerable and thereby informing the develo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;LLM&#22312;&#35299;&#37322;&#34920;&#26684;&#25968;&#25454;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#27604;&#36739;&#20102;&#25991;&#26412;&#21644;&#22270;&#20687;&#34920;&#26684;&#34920;&#31034;&#23545;LLM&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#20026;&#22312;&#34920;&#26684;&#30456;&#20851;&#20219;&#21153;&#19978;&#26377;&#25928;&#20351;&#29992;LLM&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.12424</link><description>&lt;p&gt;
&#34920;&#26684;&#20316;&#20026;&#22270;&#29255;&#65311;&#25506;&#35752;LLM&#22312;&#22810;&#27169;&#24577;&#34920;&#26684;&#25968;&#25454;&#34920;&#31034;&#19978;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
Tables as Images? Exploring the Strengths and Limitations of LLMs on Multimodal Representations of Tabular Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;LLM&#22312;&#35299;&#37322;&#34920;&#26684;&#25968;&#25454;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#27604;&#36739;&#20102;&#25991;&#26412;&#21644;&#22270;&#20687;&#34920;&#26684;&#34920;&#31034;&#23545;LLM&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#20026;&#22312;&#34920;&#26684;&#30456;&#20851;&#20219;&#21153;&#19978;&#26377;&#25928;&#20351;&#29992;LLM&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#19981;&#21516;&#30340;&#25552;&#31034;&#31574;&#30053;&#21644;&#25968;&#25454;&#26684;&#24335;&#30740;&#31350;&#20102;&#21508;&#31181;LLM&#22312;&#35299;&#37322;&#34920;&#26684;&#25968;&#25454;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#28085;&#30422;&#20102;&#20845;&#20010;&#38024;&#23545;&#19982;&#34920;&#26684;&#30456;&#20851;&#20219;&#21153;&#30340;&#22522;&#20934;&#65292;&#22914;&#38382;&#31572;&#21644;&#20107;&#23454;&#26680;&#26597;&#12290;&#25105;&#20204;&#39318;&#27425;&#20171;&#32461;&#20102;LLM&#22312;&#22522;&#20110;&#22270;&#20687;&#30340;&#34920;&#26684;&#34920;&#31034;&#19978;&#30340;&#34920;&#29616;&#35780;&#20272;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20116;&#31181;&#22522;&#20110;&#25991;&#26412;&#21644;&#19977;&#31181;&#22522;&#20110;&#22270;&#20687;&#30340;&#34920;&#26684;&#34920;&#31034;&#65292;&#23637;&#31034;&#20102;&#34920;&#31034;&#21644;&#25552;&#31034;&#23545;LLM&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#22312;&#34920;&#26684;&#30456;&#20851;&#20219;&#21153;&#19978;&#26377;&#25928;&#20351;&#29992;LLM&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12424v1 Announce Type: cross  Abstract: In this paper, we investigate the effectiveness of various LLMs in interpreting tabular data through different prompting strategies and data formats. Our analysis extends across six benchmarks for table-related tasks such as question-answering and fact-checking. We introduce for the first time the assessment of LLMs' performance on image-based table representations. Specifically, we compare five text-based and three image-based table representations, demonstrating the influence of representation and prompting on LLM performance. Our study provides insights into the effective use of LLMs on table-related tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#25991;&#26412;&#36716;&#35821;&#38899;&#27169;&#22411;&#20013;&#25506;&#32034;&#20102;&#20923;&#32467;&#27169;&#22411;&#30340;&#28508;&#31354;&#38388;&#65292;&#21457;&#29616;&#20854;&#20013;&#21253;&#21547;&#20016;&#23500;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#26032;&#26041;&#27861;&#26469;&#25214;&#20986;&#20854;&#20013;&#30340;&#35821;&#20041;&#26041;&#21521;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#19981;&#32463;&#36807;&#39069;&#22806;&#35757;&#32451;&#12289;&#26550;&#26500;&#26356;&#25913;&#25110;&#25968;&#25454;&#38656;&#27714;&#23601;&#33021;&#36827;&#34892;&#38899;&#39057;&#32534;&#36753;&#12290;</title><link>https://arxiv.org/abs/2402.12423</link><description>&lt;p&gt;
&#20851;&#20110;&#22522;&#20110;&#25193;&#25955;&#30340;&#25991;&#26412;&#36716;&#35821;&#38899;&#27169;&#22411;&#30340;&#35821;&#20041;&#28508;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
On the Semantic Latent Space of Diffusion-Based Text-to-Speech Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12423
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#25991;&#26412;&#36716;&#35821;&#38899;&#27169;&#22411;&#20013;&#25506;&#32034;&#20102;&#20923;&#32467;&#27169;&#22411;&#30340;&#28508;&#31354;&#38388;&#65292;&#21457;&#29616;&#20854;&#20013;&#21253;&#21547;&#20016;&#23500;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#26032;&#26041;&#27861;&#26469;&#25214;&#20986;&#20854;&#20013;&#30340;&#35821;&#20041;&#26041;&#21521;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#19981;&#32463;&#36807;&#39069;&#22806;&#35757;&#32451;&#12289;&#26550;&#26500;&#26356;&#25913;&#25110;&#25968;&#25454;&#38656;&#27714;&#23601;&#33021;&#36827;&#34892;&#38899;&#39057;&#32534;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25991;&#26412;&#36716;&#35821;&#38899;&#65288;TTS&#65289;&#39046;&#22495;&#65292;Denoising Diffusion Models (DDMs) &#30340;&#24341;&#20837;&#26085;&#30410;&#22686;&#22810;&#65292;&#20026;&#21512;&#25104;&#39640;&#36136;&#37327;&#35821;&#38899;&#25552;&#20379;&#20102;&#24040;&#22823;&#20215;&#20540;&#12290;&#23613;&#31649;&#23427;&#20204;&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#38899;&#39057;&#36136;&#37327;&#65292;&#20294;&#23427;&#20204;&#30340;&#35821;&#20041;&#33021;&#21147;&#31243;&#24230;&#23578;&#19981;&#26126;&#30830;&#65292;&#24182;&#19988;&#25511;&#21046;&#21512;&#25104;&#35821;&#38899;&#30340;&#22768;&#38899;&#29305;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#21463;&#22270;&#20687;&#21512;&#25104;&#26368;&#26032;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20923;&#32467;&#30340;TTS&#27169;&#22411;&#30340;&#28508;&#31354;&#38388;&#65292;&#35813;&#31354;&#38388;&#30001;DDM&#21435;&#22122;&#22120;&#30340;&#28508;&#31354;&#38388;&#28608;&#27963;&#32452;&#25104;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20010;&#31354;&#38388;&#21253;&#21547;&#20016;&#23500;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#27010;&#36848;&#20102;&#33509;&#24178;&#26597;&#25214;&#20854;&#20013;&#35821;&#20041;&#26041;&#21521;&#30340;&#26032;&#26041;&#27861;&#65292;&#21253;&#25324;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#36825;&#20123;&#26041;&#27861;&#36827;&#34892;&#29616;&#25104;&#38899;&#39057;&#32534;&#36753;&#65292;&#26080;&#38656;&#36827;&#19968;&#27493;&#35757;&#32451;&#12289;&#26550;&#26500;&#26356;&#25913;&#25110;&#25968;&#25454;&#38656;&#27714;&#12290;&#25105;&#20204;&#21576;&#29616;&#20102;&#32534;&#36753;&#21518;&#38899;&#39057;&#30340;&#35821;&#20041;&#21644;&#22768;&#23398;&#29305;&#36136;&#30340;&#35777;&#25454;&#65292;&#24182;&#25552;&#20379;&#20102;&#34917;&#20805;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12423v1 Announce Type: cross  Abstract: The incorporation of Denoising Diffusion Models (DDMs) in the Text-to-Speech (TTS) domain is rising, providing great value in synthesizing high quality speech. Although they exhibit impressive audio quality, the extent of their semantic capabilities is unknown, and controlling their synthesized speech's vocal properties remains a challenge. Inspired by recent advances in image synthesis, we explore the latent space of frozen TTS models, which is composed of the latent bottleneck activations of the DDM's denoiser. We identify that this space contains rich semantic information, and outline several novel methods for finding semantic directions within it, both supervised and unsupervised. We then demonstrate how these enable off-the-shelf audio editing, without any further training, architectural changes or data requirements. We present evidence of the semantic and acoustic qualities of the edited audio, and provide supplemental samples: h
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#22359;&#29366;&#24494;&#35843;&#31232;&#30095;LLM&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#37325;&#24314;&#35823;&#24046;&#24182;&#37319;&#29992;&#21453;&#21521;&#20256;&#25773;&#36880;&#22359;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.12419</link><description>&lt;p&gt;
EBFT&#65306;&#31232;&#30095;LLM&#30340;&#26377;&#25928;&#21644;&#22359;&#29366;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
EBFT: Effective and Block-Wise Fine-Tuning for Sparse LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12419
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#22359;&#29366;&#24494;&#35843;&#31232;&#30095;LLM&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#37325;&#24314;&#35823;&#24046;&#24182;&#37319;&#29992;&#21453;&#21521;&#20256;&#25773;&#36880;&#22359;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#31232;&#30095;LLM&#24494;&#35843;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#36164;&#28304;&#23494;&#38598;&#22411;&#30340;&#35201;&#27714;&#21644;&#39640;&#26114;&#30340;&#37325;&#26032;&#35757;&#32451;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#24494;&#35843;&#26041;&#27861;&#24448;&#24448;&#20381;&#36182;&#20110;&#36817;&#20284;&#25110;&#21551;&#21457;&#24335;&#20248;&#21270;&#31574;&#30053;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#27425;&#20248;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#23567;&#21270;&#37325;&#24314;&#35823;&#24046;&#30340;&#39640;&#25928;&#24555;&#36895;&#24494;&#35843;&#31232;&#30095;LLM&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#23545;&#19968;&#20010;&#23567;&#25968;&#25454;&#38598;&#36827;&#34892;&#37319;&#26679;&#20197;&#36827;&#34892;&#26657;&#20934;&#65292;&#24182;&#21033;&#29992;&#21453;&#21521;&#20256;&#25773;&#36880;&#22359;&#22320;&#20248;&#21270;&#22359;&#29366;&#37325;&#24314;&#35823;&#24046;&#65292;&#33268;&#21147;&#20110;&#23547;&#27714;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#12290;&#23545;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#30340;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#32447;&#19978;&#22987;&#32456;&#34920;&#29616;&#21331;&#36234;&#12290;&#20363;&#22914;&#65292;&#22312;Wikitext2&#25968;&#25454;&#38598;&#19978;&#65292;LLamaV1-7B&#22312;70%&#31232;&#30095;&#24230;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;EBFT&#21462;&#24471;&#20102;16.88&#30340;&#22256;&#24785;&#24230;&#65292;&#36229;&#36807;&#20102;75.14&#30340;DSnoT&#30340;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12419v1 Announce Type: cross  Abstract: Existing methods for fine-tuning sparse LLMs often suffer from resource-intensive requirements and high retraining costs. Additionally, many fine-tuning methods often rely on approximations or heuristic optimization strategies, which may lead to suboptimal solutions. To address these issues, we propose an efficient and fast framework for fine-tuning sparse LLMs based on minimizing reconstruction error. Our approach involves sampling a small dataset for calibration and utilizing backpropagation to iteratively optimize block-wise reconstruction error, on a block-by-block basis, aiming for optimal solutions. Extensive experiments on various benchmarks consistently demonstrate the superiority of our method over other baselines. For instance, on the Wikitext2 dataset with LlamaV1-7B at 70% sparsity, our proposed EBFT achieves a perplexity of 16.88, surpassing the state-of-the-art DSnoT with a perplexity of 75.14. Moreover, with a structured
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#20108;&#38454;&#25439;&#22833;&#26223;&#35266;&#20449;&#24687;&#30340;&#33258;&#21160;&#32553;&#25918;&#26041;&#27861;&#65292;&#21516;&#26102;&#25193;&#23637;&#21644;&#35757;&#32451;transformers&#65292;&#25552;&#20986;&#20102;&#31070;&#32463;&#26550;&#26500;&#20013;&#30340;&#28145;&#24230;&#24322;&#36136;&#24615;&#27010;&#24565;&#65292;&#24182;&#22312;ImageNet100&#19978;&#23454;&#29616;&#20102;&#20934;&#30830;&#24615;&#21644;&#21442;&#25968;&#25928;&#29575;&#30340;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.12418</link><description>&lt;p&gt;
&#36229;&#36234;&#32479;&#19968;&#32553;&#25918;&#65306;&#25506;&#32034;&#31070;&#32463;&#26550;&#26500;&#20013;&#30340;&#28145;&#24230;&#24322;&#36136;&#24615;
&lt;/p&gt;
&lt;p&gt;
Beyond Uniform Scaling: Exploring Depth Heterogeneity in Neural Architectures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12418
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#20108;&#38454;&#25439;&#22833;&#26223;&#35266;&#20449;&#24687;&#30340;&#33258;&#21160;&#32553;&#25918;&#26041;&#27861;&#65292;&#21516;&#26102;&#25193;&#23637;&#21644;&#35757;&#32451;transformers&#65292;&#25552;&#20986;&#20102;&#31070;&#32463;&#26550;&#26500;&#20013;&#30340;&#28145;&#24230;&#24322;&#36136;&#24615;&#27010;&#24565;&#65292;&#24182;&#22312;ImageNet100&#19978;&#23454;&#29616;&#20102;&#20934;&#30830;&#24615;&#21644;&#21442;&#25968;&#25928;&#29575;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#31070;&#32463;&#32593;&#32476;&#32553;&#25918;&#36890;&#24120;&#28041;&#21450;&#35774;&#35745;&#22522;&#26412;&#32593;&#32476;&#65292;&#24182;&#36890;&#36807;&#19968;&#20123;&#39044;&#23450;&#20041;&#30340;&#32553;&#25918;&#22240;&#23376;&#22686;&#21152;&#19981;&#21516;&#32500;&#24230;&#65288;&#22914;&#23485;&#24230;&#12289;&#28145;&#24230;&#31561;&#65289;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21033;&#29992;&#20108;&#38454;&#25439;&#22833;&#26223;&#35266;&#20449;&#24687;&#30340;&#33258;&#21160;&#32553;&#25918;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#29616;&#20195;&#35270;&#35273;transformers&#20013;&#30340;&#36339;&#36807;&#36830;&#25509;&#20855;&#26377;&#28789;&#27963;&#24615;&#12290;&#25105;&#20204;&#30340;&#35757;&#32451;&#24863;&#30693;&#26041;&#27861;&#21516;&#26102;&#25193;&#23637;&#21644;&#35757;&#32451;transformers&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#36845;&#20195;&#12290;&#21463;&#21040;&#24182;&#38750;&#25152;&#26377;&#31070;&#32463;&#20803;&#37117;&#38656;&#35201;&#32479;&#19968;&#28145;&#24230;&#22797;&#26434;&#24615;&#30340;&#20551;&#35774;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#28145;&#24230;&#24322;&#36136;&#24615;&#12290;&#23545;DeiT-S&#22312;ImageNet100&#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#35780;&#20272;&#26174;&#31034;&#27604;&#20256;&#32479;&#32553;&#25918;&#25552;&#39640;&#20102;2.5&#65285;&#30340;&#20934;&#30830;&#24615;&#24182;&#25552;&#39640;&#20102;10&#65285;&#30340;&#21442;&#25968;&#25928;&#29575;&#12290;&#22312;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#26102;&#65292;&#32553;&#25918;&#30340;&#32593;&#32476;&#34920;&#29616;&#20986;&#33394;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#35270;&#35273;transformers&#30340;&#31532;&#19968;&#20010;&#23436;&#25972;&#32553;&#25918;&#26426;&#21046;&#65292;&#36825;&#26159;&#26397;&#21521;&#39640;&#25928;&#27169;&#22411;&#22330;&#26223;&#30340;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12418v1 Announce Type: cross  Abstract: Conventional scaling of neural networks typically involves designing a base network and growing different dimensions like width, depth, etc. of the same by some predefined scaling factors. We introduce an automated scaling approach leveraging second-order loss landscape information. Our method is flexible towards skip connections a mainstay in modern vision transformers. Our training-aware method jointly scales and trains transformers without additional training iterations. Motivated by the hypothesis that not all neurons need uniform depth complexity, our approach embraces depth heterogeneity. Extensive evaluations on DeiT-S with ImageNet100 show a 2.5% accuracy gain and 10% parameter efficiency improvement over conventional scaling. Scaled networks demonstrate superior performance upon training small scale datasets from scratch. We introduce the first intact scaling mechanism for vision transformers, a step towards efficient model sc
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#20808;&#35757;&#32451;&#28982;&#21518;&#24494;&#35843;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#20854;&#20182;&#20844;&#21496;&#30340;&#25968;&#25454;&#24320;&#21457;AI&#27169;&#22411;&#65292;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#21345;&#36710;&#20107;&#25925;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2402.12417</link><description>&lt;p&gt;
&#29992;&#36328;&#20844;&#21496;&#30340;&#21345;&#36710;&#21496;&#26426;&#23433;&#20840;&#27675;&#22260;&#24863;&#30693;&#26469;&#39044;&#27979;&#21345;&#36710;&#20107;&#25925;&#65306;&#19968;&#31181;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Predicting trucking accidents with truck drivers 'safety climate perception across companies: A transfer learning approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12417
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#20808;&#35757;&#32451;&#28982;&#21518;&#24494;&#35843;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#20854;&#20182;&#20844;&#21496;&#30340;&#25968;&#25454;&#24320;&#21457;AI&#27169;&#22411;&#65292;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#21345;&#36710;&#20107;&#25925;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#39537;&#21160;&#30340;&#23433;&#20840;&#20998;&#26512;&#22312;&#39044;&#27979;&#21345;&#36710;&#34892;&#19994;&#20107;&#25925;&#26041;&#38754;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20844;&#21496;&#21487;&#33021;&#38754;&#20020;&#19968;&#20010;&#23454;&#38469;&#25361;&#25112;&#65292;&#21363;&#27809;&#26377;&#36275;&#22815;&#30340;&#25968;&#25454;&#26469;&#24320;&#21457;&#33391;&#22909;&#30340;&#23433;&#20840;&#20998;&#26512;&#27169;&#22411;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#20808;&#35757;&#32451;&#28982;&#21518;&#24494;&#35843;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#24110;&#21161;&#20219;&#20309;&#20844;&#21496;&#21033;&#29992;&#20854;&#20182;&#20844;&#21496;&#30340;&#25968;&#25454;&#24320;&#21457;AI&#27169;&#22411;&#65292;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#20107;&#25925;&#39118;&#38505;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;SafeNet&#65292;&#19968;&#31181;&#36866;&#29992;&#20110;&#20107;&#25925;&#39044;&#27979;&#30340;&#20998;&#31867;&#20219;&#21153;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#12290;&#36890;&#36807;&#26469;&#33258;&#19971;&#23478;&#25968;&#25454;&#35268;&#27169;&#21508;&#19981;&#30456;&#21516;&#30340;&#21345;&#36710;&#20844;&#21496;&#30340;&#23433;&#20840;&#27675;&#22260;&#35843;&#26597;&#25968;&#25454;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#32467;&#26524;&#19978;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12417v1 Announce Type: cross  Abstract: There is a rising interest in using artificial intelligence (AI)-powered safety analytics to predict accidents in the trucking industry. Companies may face the practical challenge, however, of not having enough data to develop good safety analytics models. Although pretrained models may offer a solution for such companies, existing safety research using transfer learning has mostly focused on computer vision and natural language processing, rather than accident analytics. To fill the above gap, we propose a pretrain-then-fine-tune transfer learning approach to help any company leverage other companies' data to develop AI models for a more accurate prediction of accident risk. We also develop SafeNet, a deep neural network algorithm for classification tasks suitable for accident prediction. Using the safety climate survey data from seven trucking companies with different data sizes, we show that our proposed approach results in better m
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#24322;&#26500;&#32467;&#26500;&#21270;&#30693;&#35782;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23398;&#20064;&#26694;&#26550;SKES&#65292;&#29992;&#20110;&#22312;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#20013;&#20016;&#23500;&#33410;&#28857;&#34920;&#31034;&#30340;&#20449;&#24687;&#37327;&#65292;&#36827;&#32780;&#24314;&#31435;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#33410;&#28857;&#37325;&#35201;&#24615;&#35745;&#31639;&#33539;&#24335;&#12290;</title><link>https://arxiv.org/abs/2402.12411</link><description>&lt;p&gt;
&#22522;&#20110;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#30340;&#33410;&#28857;&#37325;&#35201;&#24615;&#20540;&#20272;&#35745;&#30340;&#28145;&#24230;&#32467;&#26500;&#30693;&#35782;&#21033;&#29992;&#19982;&#21327;&#21516;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Deep Structural Knowledge Exploitation and Synergy for Estimating Node Importance Value on Heterogeneous Information Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12411
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#24322;&#26500;&#32467;&#26500;&#21270;&#30693;&#35782;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23398;&#20064;&#26694;&#26550;SKES&#65292;&#29992;&#20110;&#22312;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#20013;&#20016;&#23500;&#33410;&#28857;&#34920;&#31034;&#30340;&#20449;&#24687;&#37327;&#65292;&#36827;&#32780;&#24314;&#31435;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#33410;&#28857;&#37325;&#35201;&#24615;&#35745;&#31639;&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33410;&#28857;&#37325;&#35201;&#24615;&#20272;&#35745;&#38382;&#39064;&#22312;&#20256;&#32479;&#19978;&#26159;&#36890;&#36807;&#21516;&#36136;&#32593;&#32476;&#25299;&#25169;&#20998;&#26512;&#26469;&#30740;&#31350;&#30340;&#12290;&#20026;&#20102;&#22788;&#29702;&#32593;&#32476;&#30340;&#24322;&#36136;&#24615;&#65292;&#26368;&#36817;&#19968;&#20123;&#26041;&#27861;&#37319;&#29992;&#22270;&#31070;&#32463;&#27169;&#22411;&#26469;&#33258;&#21160;&#23398;&#20064;&#22810;&#26679;&#30340;&#20449;&#24687;&#26469;&#28304;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#30340;&#20840;&#33258;&#36866;&#24212;&#23398;&#20064;&#36807;&#31243;&#21487;&#33021;&#23548;&#33268;&#20449;&#24687;&#25506;&#32034;&#19981;&#36275;&#65292;&#20174;&#32780;&#23558;&#38382;&#39064;&#21046;&#23450;&#20026;&#23545;&#23396;&#31435;&#33410;&#28857;&#30340;&#20540;&#39044;&#27979;&#65292;&#34920;&#29616;&#19981;&#20339;&#19988;&#21487;&#35299;&#37322;&#24615;&#36739;&#24046;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#23398;&#20064;&#26694;&#26550;&#65306;SKES&#12290;&#19982;&#20197;&#21069;&#30340;&#33258;&#21160;&#23398;&#20064;&#35774;&#35745;&#19981;&#21516;&#65292;SKES&#21033;&#29992;&#24322;&#26500;&#32467;&#26500;&#21270;&#30693;&#35782;&#26469;&#20016;&#23500;&#33410;&#28857;&#34920;&#31034;&#30340;&#20449;&#24687;&#37327;&#12290;&#22522;&#20110;&#19968;&#20010;&#36275;&#22815;&#19981;&#20855;&#20449;&#24687;&#30340;&#21442;&#32771;&#65292;SKES&#36890;&#36807;&#37327;&#21270;&#36755;&#20837;&#33410;&#28857;&#19982;&#21442;&#32771;&#20043;&#38388;&#30340;&#24046;&#24322;&#26469;&#20272;&#35745;&#20219;&#20309;&#36755;&#20837;&#33410;&#28857;&#30340;&#37325;&#35201;&#24615;&#20540;&#12290;&#36825;&#24314;&#31435;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#33410;&#28857;&#37325;&#35201;&#24615;&#35745;&#31639;&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12411v1 Announce Type: cross  Abstract: Node importance estimation problem has been studied conventionally with homogeneous network topology analysis. To deal with network heterogeneity, a few recent methods employ graph neural models to automatically learn diverse sources of information. However, the major concern revolves around that their full adaptive learning process may lead to insufficient information exploration, thereby formulating the problem as the isolated node value prediction with underperformance and less interpretability. In this work, we propose a novel learning framework: SKES. Different from previous automatic learning designs, SKES exploits heterogeneous structural knowledge to enrich the informativeness of node representations. Based on a sufficiently uninformative reference, SKES estimates the importance value for any input node, by quantifying its disparity against the reference. This establishes an interpretable node importance computation paradigm. F
&lt;/p&gt;</description></item><item><title>ModelGPT&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;LLM&#30340;&#33021;&#21147;&#65292;&#26681;&#25454;&#29992;&#25143;&#25552;&#20379;&#30340;&#25968;&#25454;&#25110;&#20219;&#21153;&#25551;&#36848;&#29983;&#25104;&#23450;&#21046;&#21270;&#30340;AI&#27169;&#22411;&#65292;&#35753;&#29992;&#25143;&#33021;&#22815;&#26356;&#24555;&#36895;&#21644;&#26041;&#20415;&#22320;&#20351;&#29992;AI&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.12408</link><description>&lt;p&gt;
ModelGPT&#65306;&#37322;&#25918;LLM&#30340;&#33021;&#21147;&#65292;&#20026;&#23450;&#21046;&#27169;&#22411;&#29983;&#25104;&#38138;&#24179;&#36947;&#36335;
&lt;/p&gt;
&lt;p&gt;
ModelGPT: Unleashing LLM's Capabilities for Tailored Model Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12408
&lt;/p&gt;
&lt;p&gt;
ModelGPT&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;LLM&#30340;&#33021;&#21147;&#65292;&#26681;&#25454;&#29992;&#25143;&#25552;&#20379;&#30340;&#25968;&#25454;&#25110;&#20219;&#21153;&#25551;&#36848;&#29983;&#25104;&#23450;&#21046;&#21270;&#30340;AI&#27169;&#22411;&#65292;&#35753;&#29992;&#25143;&#33021;&#22815;&#26356;&#24555;&#36895;&#21644;&#26041;&#20415;&#22320;&#20351;&#29992;AI&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#36890;&#36807;&#33258;&#21160;&#21270;&#20363;&#34892;&#20219;&#21153;&#65292;&#26631;&#24535;&#30528;&#36808;&#21521;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#30340;&#23454;&#29616;&#36808;&#20986;&#20102;&#19968;&#27493;&#65292;&#38761;&#26032;&#20102;&#21508;&#20010;&#34892;&#19994;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20173;&#28982;&#38590;&#20197;&#28385;&#36275;&#29992;&#25143;&#30340;&#22810;&#26679;&#21270;&#21644;&#29305;&#23450;&#38656;&#27714;&#65292;&#20063;&#38590;&#20197;&#31616;&#21270;AI&#27169;&#22411;&#23545;&#26222;&#36890;&#29992;&#25143;&#30340;&#21033;&#29992;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ModelGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#26681;&#25454;&#29992;&#25143;&#25552;&#20379;&#30340;&#25968;&#25454;&#25110;&#20219;&#21153;&#25551;&#36848;&#26469;&#30830;&#23450;&#21644;&#29983;&#25104;&#29305;&#23450;&#23450;&#21046;&#30340;AI&#27169;&#22411;&#65292;&#21033;&#29992;&#20102;LLM&#30340;&#33021;&#21147;&#12290;ModelGPT&#33021;&#22815;&#26681;&#25454;&#29992;&#25143;&#38656;&#27714;&#25552;&#20379;&#30340;&#27169;&#22411;&#65292;&#27604;&#20043;&#21069;&#30340;&#33539;&#24335;&#65288;&#20363;&#22914;&#20840;&#21442;&#25968;&#25110;LoRA&#24494;&#35843;&#65289;&#24555;&#33267;&#22810;270&#20493;&#12290;&#22312;NLP&#12289;CV&#21644;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#20840;&#38754;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#20351;AI&#27169;&#22411;&#26356;&#26131;&#35775;&#38382;&#21644;&#29992;&#25143;&#21451;&#22909;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312; https://github.com/IshiKura-a/ModelGPT &#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12408v1 Announce Type: cross  Abstract: The rapid advancement of Large Language Models (LLMs) has revolutionized various sectors by automating routine tasks, marking a step toward the realization of Artificial General Intelligence (AGI). However, they still struggle to accommodate the diverse and specific needs of users and simplify the utilization of AI models for the average user. In response, we propose ModelGPT, a novel framework designed to determine and generate AI models specifically tailored to the data or task descriptions provided by the user, leveraging the capabilities of LLMs. Given user requirements, ModelGPT is able to provide tailored models at most 270x faster than the previous paradigms (e.g. all-parameter or LoRA finetuning). Comprehensive experiments on NLP, CV, and Tabular datasets attest to the effectiveness of our framework in making AI models more accessible and user-friendly. Our code is available at https://github.com/IshiKura-a/ModelGPT.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21457;&#29616;&#29616;&#26377;&#30340;&#26080;&#25968;&#25454;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#23545;&#19981;&#21516;&#30340;&#25945;&#24072;&#27169;&#22411;&#38750;&#24120;&#25935;&#24863;&#65292;&#29983;&#25104;&#30340;&#26679;&#26412;&#21487;&#33021;&#20986;&#29616;&#36136;&#37327;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.12406</link><description>&lt;p&gt;
&#25945;&#24072;&#20316;&#20026;&#23485;&#23481;&#30340;&#19987;&#23478;&#65306;&#19981;&#20381;&#36182;&#20110;&#25945;&#24072;&#30340;&#26080;&#25968;&#25454;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Teacher as a Lenient Expert: Teacher-Agnostic Data-Free Knowledge Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12406
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21457;&#29616;&#29616;&#26377;&#30340;&#26080;&#25968;&#25454;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#23545;&#19981;&#21516;&#30340;&#25945;&#24072;&#27169;&#22411;&#38750;&#24120;&#25935;&#24863;&#65292;&#29983;&#25104;&#30340;&#26679;&#26412;&#21487;&#33021;&#20986;&#29616;&#36136;&#37327;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#25968;&#25454;&#30693;&#35782;&#33976;&#39311;&#65288;DFKD&#65289;&#26088;&#22312;&#22312;&#19981;&#20351;&#29992;&#21407;&#22987;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#20511;&#21161;&#29983;&#25104;&#22120;&#23558;&#39044;&#35757;&#32451;&#30693;&#35782;&#33976;&#39311;&#32473;&#23398;&#29983;&#27169;&#22411;&#12290;&#22312;&#36825;&#31181;&#26080;&#25968;&#25454;&#24773;&#20917;&#19979;&#65292;&#30001;&#20110;&#39564;&#35777;&#25968;&#25454;&#19981;&#21487;&#29992;&#65292;&#23454;&#29616;DFKD&#30340;&#31283;&#23450;&#24615;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#26412;&#25991;&#21457;&#29616;&#29616;&#26377;&#30340;DFKD&#26041;&#27861;&#23545;&#19981;&#21516;&#30340;&#25945;&#24072;&#27169;&#22411;&#38750;&#24120;&#25935;&#24863;&#65292;&#26377;&#26102;&#21363;&#20351;&#20351;&#29992;&#35757;&#32451;&#33391;&#22909;&#30340;&#25945;&#24072;&#27169;&#22411;&#20063;&#20250;&#20986;&#29616;&#33976;&#39311;&#30340;&#28798;&#38590;&#24615;&#22833;&#36133;&#12290;&#25105;&#20204;&#30340;&#35266;&#23519;&#26159;DFKD&#20013;&#30340;&#29983;&#25104;&#22120;&#24182;&#19981;&#24635;&#26159;&#20445;&#35777;&#20351;&#29992;&#29616;&#26377;&#30340;&#26088;&#22312;&#26368;&#23567;&#21270;&#31867;&#20808;&#39564;&#21644;&#23545;&#25239;&#25439;&#22833;&#30340;&#20195;&#34920;&#24615;&#31574;&#30053;&#20135;&#29983;&#31934;&#30830;&#32780;&#22810;&#26679;&#21270;&#30340;&#26679;&#26412;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#25105;&#20204;&#20851;&#27880;&#30340;&#20107;&#23454;&#26159;&#31867;&#20808;&#39564;&#19981;&#20165;&#20943;&#23569;&#20102;&#29983;&#25104;&#26679;&#26412;&#30340;&#22810;&#26679;&#24615;&#65292;&#36824;&#19981;&#33021;&#23436;&#20840;&#35299;&#20915;&#26681;&#25454;&#25945;&#24072;&#27169;&#22411;&#29983;&#25104;&#24847;&#22806;&#20302;&#36136;&#37327;&#26679;&#26412;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12406v1 Announce Type: cross  Abstract: Data-free knowledge distillation (DFKD) aims to distill pretrained knowledge to a student model with the help of a generator without using original data. In such data-free scenarios, achieving stable performance of DFKD is essential due to the unavailability of validation data. Unfortunately, this paper has discovered that existing DFKD methods are quite sensitive to different teacher models, occasionally showing catastrophic failures of distillation, even when using well-trained teacher models. Our observation is that the generator in DFKD is not always guaranteed to produce precise yet diverse samples using the existing representative strategy of minimizing both class-prior and adversarial losses. Through our empirical study, we focus on the fact that class-prior not only decreases the diversity of generated samples, but also cannot completely address the problem of generating unexpectedly low-quality samples depending on teacher mod
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#27604;&#36187;&#23618;&#27425;&#25968;&#25454;&#20272;&#35745;&#24180;&#40836;&#26354;&#32447;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#36712;&#36857;&#20998;&#26512;&#30340;&#32454;&#33268;&#24230;&#65292;&#21487;&#20197;&#21457;&#29616;&#22797;&#26434;&#38750;&#32447;&#24615;&#27169;&#24335;&#65292;&#23454;&#29616;&#23545;&#24180;&#40836;&#26354;&#32447;&#30340;&#22240;&#26524;&#25928;&#24212;&#30340;&#32454;&#33268;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2402.12400</link><description>&lt;p&gt;
&#20272;&#35745;&#24180;&#40836;&#26465;&#20214;&#19979;&#30340;&#24179;&#22343;&#27835;&#30103;&#25928;&#24212;&#26354;&#32447;&#65306;&#35780;&#20272;NBA&#20013;&#30340;&#36127;&#33655;&#31649;&#29702;&#31574;&#30053;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Estimating the age-conditioned average treatment effects curves: An application for assessing load-management strategies in the NBA
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12400
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#27604;&#36187;&#23618;&#27425;&#25968;&#25454;&#20272;&#35745;&#24180;&#40836;&#26354;&#32447;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#36712;&#36857;&#20998;&#26512;&#30340;&#32454;&#33268;&#24230;&#65292;&#21487;&#20197;&#21457;&#29616;&#22797;&#26434;&#38750;&#32447;&#24615;&#27169;&#24335;&#65292;&#23454;&#29616;&#23545;&#24180;&#40836;&#26354;&#32447;&#30340;&#22240;&#26524;&#25928;&#24212;&#30340;&#32454;&#33268;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31454;&#25216;&#20307;&#32946;&#39046;&#22495;&#65292;&#20102;&#35299;&#36816;&#21160;&#21592;&#34920;&#29616;&#21160;&#24577;&#65288;&#36890;&#36807;&#24180;&#40836;&#26354;&#32447;&#23637;&#31034;&#36827;&#23637;&#12289;&#24005;&#23792;&#21644;&#34928;&#36864;&#65289;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#26469;&#37327;&#21270;&#24180;&#40836;&#29305;&#23450;&#30340;&#27835;&#30103;&#25928;&#24212;&#65292;&#22686;&#24378;&#20102;&#24615;&#33021;&#36712;&#36857;&#20998;&#26512;&#30340;&#32454;&#33268;&#24230;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27604;&#36187;&#23618;&#27425;&#25968;&#25454;&#20272;&#35745;&#24180;&#40836;&#26354;&#32447;&#30340;&#26041;&#27861;&#65292;&#19982;&#20256;&#32479;&#30340;&#36187;&#23395;&#32423;&#25968;&#25454;&#26041;&#27861;&#26377;&#25152;&#19981;&#21516;&#65292;&#36890;&#36807;&#21033;&#29992;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#26469;&#35299;&#20915;&#20854;&#22266;&#26377;&#30340;&#22797;&#26434;&#24615;&#12290;&#35813;&#26041;&#27861;&#25581;&#31034;&#20102;&#29616;&#26377;&#26041;&#27861;&#24573;&#30053;&#30340;&#22797;&#26434;&#38750;&#32447;&#24615;&#27169;&#24335;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;&#24471;&#33021;&#22815;&#30830;&#23450;&#22240;&#26524;&#25928;&#24212;&#65292;&#21487;&#20197;&#35814;&#32454;&#30740;&#31350;&#19981;&#21516;&#26465;&#20214;&#19979;&#30340;&#24180;&#40836;&#26354;&#32447;&#12290;&#36890;&#36807;&#23450;&#20041;&#24180;&#40836;&#26465;&#20214;&#19979;&#30340;&#27835;&#30103;&#25928;&#24212;&#65288;ACTE&#65289;&#65292;&#25105;&#20204;&#20419;&#36827;&#20102;&#23545;&#27835;&#30103;&#24433;&#21709;&#22240;&#26524;&#20851;&#31995;&#30340;&#25506;&#35752;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12400v1 Announce Type: cross  Abstract: In the realm of competitive sports, understanding the performance dynamics of athletes, represented by the age curve (showing progression, peak, and decline), is vital. Our research introduces a novel framework for quantifying age-specific treatment effects, enhancing the granularity of performance trajectory analysis. Firstly, we propose a methodology for estimating the age curve using game-level data, diverging from traditional season-level data approaches, and tackling its inherent complexities with a meta-learner framework that leverages advanced machine learning models. This approach uncovers intricate non-linear patterns missed by existing methods. Secondly, our framework enables the identification of causal effects, allowing for a detailed examination of age curves under various conditions. By defining the Age-Conditioned Treatment Effect (ACTE), we facilitate the exploration of causal relationships regarding treatment impacts a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Rectify-Router&#35299;&#20915;&#20102;MoE&#27169;&#22411;&#20013;&#24120;&#29992;&#30340;Top-k&#36335;&#30001;&#26426;&#21046;&#25152;&#24102;&#26469;&#30340;&#20196;&#29260;&#20002;&#22833;&#21644;&#22635;&#20805;&#38382;&#39064;&#65292;&#36890;&#36807;Intra-GPU&#30699;&#27491;&#21644;Fill-in&#30699;&#27491;&#26469;&#23454;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.12399</link><description>&lt;p&gt;
&#23558;&#24223;&#26009;&#21464;&#24223;&#20026;&#23453;&#65306;&#30699;&#27491;MoE&#30340;Top-k&#36335;&#30001;&#22120;
&lt;/p&gt;
&lt;p&gt;
Turn Waste into Worth: Rectifying Top-$k$ Router of MoE
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12399
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Rectify-Router&#35299;&#20915;&#20102;MoE&#27169;&#22411;&#20013;&#24120;&#29992;&#30340;Top-k&#36335;&#30001;&#26426;&#21046;&#25152;&#24102;&#26469;&#30340;&#20196;&#29260;&#20002;&#22833;&#21644;&#22635;&#20805;&#38382;&#39064;&#65292;&#36890;&#36807;Intra-GPU&#30699;&#27491;&#21644;Fill-in&#30699;&#27491;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#27169;&#22411;&#22240;&#20854;&#35745;&#31639;&#25928;&#29575;&#32780;&#21463;&#21040;&#27426;&#36814;&#65292;&#29992;&#20110;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#24120;&#29992;&#30340;Top-k&#36335;&#30001;&#26426;&#21046;&#30001;&#20110;&#19981;&#24179;&#34913;&#30340;&#36335;&#30001;&#23548;&#33268;&#20887;&#20313;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#36807;&#39640;&#12290;&#19968;&#20123;&#19987;&#23478;&#20250;&#28322;&#20986;&#65292;&#20854;&#20013;&#36229;&#20986;&#30340;&#20196;&#29260;&#20250;&#34987;&#20002;&#24323;&#12290;&#32780;&#19968;&#20123;&#19987;&#23478;&#26159;&#31354;&#38386;&#30340;&#65292;&#36825;&#20123;&#19987;&#23478;&#20250;&#22635;&#20805;&#20026;&#38646;&#65292;&#36127;&#38754;&#24433;&#21709;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#20002;&#24323;&#20196;&#29260;&#21644;&#22635;&#20805;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Rectify-Router&#65292;&#21253;&#25324;Intra-GPU&#30699;&#27491;&#21644;Fill-in&#30699;&#27491;&#12290;Intra-GPU&#30699;&#27491;&#22788;&#29702;&#20002;&#24323;&#30340;&#20196;&#29260;&#65292;&#23558;&#23427;&#20204;&#26377;&#25928;&#22320;&#36335;&#30001;&#21040;GPU&#20869;&#30340;&#19987;&#23478;&#65292;&#36991;&#20813;&#36328;GPU&#36890;&#20449;&#12290;Fill-in&#30699;&#27491;&#36890;&#36807;&#29992;&#20855;&#26377;&#39640;&#36335;&#30001;&#20998;&#25968;&#30340;&#20196;&#29260;&#26367;&#25442;&#22635;&#20805;&#20196;&#29260;&#26469;&#35299;&#20915;&#22635;&#20805;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Intra-GPU&#30699;&#27491;&#21644;Fill-in&#30699;&#27491;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12399v1 Announce Type: cross  Abstract: Sparse Mixture of Experts (MoE) models are popular for training large language models due to their computational efficiency. However, the commonly used top-$k$ routing mechanism suffers from redundancy computation and memory costs due to the unbalanced routing. Some experts are overflow, where the exceeding tokens are dropped. While some experts are vacant, which are padded with zeros, negatively impacting model performance. To address the dropped tokens and padding, we propose the Rectify-Router, comprising the Intra-GPU Rectification and the Fill-in Rectification. The Intra-GPU Rectification handles dropped tokens, efficiently routing them to experts within the GPU where they are located to avoid inter-GPU communication. The Fill-in Rectification addresses padding by replacing padding tokens with the tokens that have high routing scores. Our experimental results demonstrate that the Intra-GPU Rectification and the Fill-in Rectificati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23581;&#35797;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#35282;&#24230;&#25552;&#20379;&#23545;&#35299;&#37322;&#19968;&#33268;&#24615;&#30340;&#26032;&#35265;&#35299;&#65292;&#24182;&#30740;&#31350;&#22914;&#20309;&#36890;&#36807;&#24341;&#20837;&#39046;&#22495;&#30693;&#35782;&#32422;&#26463;&#26469;&#20351;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26356;&#21152;&#21487;&#20449;&#12290;</title><link>https://arxiv.org/abs/2402.12398</link><description>&lt;p&gt;
&#22312;&#32447;&#35780;&#20272;&#20013;&#21033;&#29992;&#20027;&#27425;&#35201;&#22240;&#32032;&#19968;&#33268;&#24615;&#20316;&#20026;&#39046;&#22495;&#30693;&#35782;&#25351;&#23548;&#24184;&#31119;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Primary and Secondary Factor Consistency as Domain Knowledge to Guide Happiness Computing in Online Assessment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12398
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23581;&#35797;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#35282;&#24230;&#25552;&#20379;&#23545;&#35299;&#37322;&#19968;&#33268;&#24615;&#30340;&#26032;&#35265;&#35299;&#65292;&#24182;&#30740;&#31350;&#22914;&#20309;&#36890;&#36807;&#24341;&#20837;&#39046;&#22495;&#30693;&#35782;&#32422;&#26463;&#26469;&#20351;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26356;&#21152;&#21487;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#35268;&#27169;&#22312;&#32447;&#32593;&#32476;&#25968;&#25454;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#24184;&#31119;&#35745;&#31639;&#26159;&#19968;&#20010;&#26032;&#20852;&#30340;&#30740;&#31350;&#35838;&#39064;&#65292;&#25903;&#25345;&#20010;&#20154;&#25104;&#38271;&#21644;&#31038;&#20250;&#31283;&#23450;&#31561;&#19968;&#31995;&#21015;&#38382;&#39064;&#12290;&#35768;&#22810;&#24102;&#26377;&#35299;&#37322;&#30340;&#20808;&#36827;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#34987;&#29992;&#20110;&#35745;&#31639;&#22312;&#32447;&#35780;&#20272;&#24184;&#31119;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#32570;&#20047;&#20027;&#27425;&#24184;&#31119;&#22240;&#32032;&#20851;&#31995;&#31561;&#39046;&#22495;&#30693;&#35782;&#32422;&#26463;&#65292;&#36825;&#38480;&#21046;&#20102;&#35745;&#31639;&#32467;&#26524;&#19982;&#21457;&#29983;&#30340;&#21407;&#22240;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#26412;&#25991;&#35797;&#22270;&#20174;&#32463;&#39564;&#30740;&#31350;&#30340;&#35282;&#24230;&#25552;&#20379;&#23545;&#35299;&#37322;&#19968;&#33268;&#24615;&#30340;&#26032;&#35265;&#35299;&#12290;&#28982;&#21518;&#25105;&#20204;&#30740;&#31350;&#22914;&#20309;&#34920;&#31034;&#21644;&#24341;&#20837;&#39046;&#22495;&#30693;&#35782;&#32422;&#26463;&#20197;&#20351;ML&#27169;&#22411;&#26356;&#21152;&#21487;&#20449;&#12290;&#25105;&#20204;&#36890;&#36807;&#20197;&#19979;&#26041;&#24335;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65306;&#65288;1&#65289;&#35777;&#26126;&#20855;&#26377;&#38468;&#21152;&#22240;&#32032;&#24402;&#22240;&#30340;&#22810;&#20010;&#39044;&#27979;&#27169;&#22411;&#23558;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12398v1 Announce Type: new  Abstract: Happiness computing based on large-scale online web data and machine learning methods is an emerging research topic that underpins a range of issues, from personal growth to social stability. Many advanced Machine Learning (ML) models with explanations are used to compute the happiness online assessment while maintaining high accuracy of results. However, domain knowledge constraints, such as the primary and secondary relations of happiness factors, are absent from these models, which limits the association between computing results and the right reasons for why they occurred. This article attempts to provide new insights into the explanation consistency from an empirical study perspective. Then we study how to represent and introduce domain knowledge constraints to make ML models more trustworthy. We achieve this through: (1) proving that multiple prediction models with additive factor attributions will have the desirable property of pr
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#31070;&#32463;&#32593;&#32476;&#21644;&#20449;&#21495;&#26102;&#38388;&#36923;&#36753;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#31867;&#21035;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#20998;&#31867;&#65292;&#20851;&#38190;&#36129;&#29486;&#21253;&#25324;&#24341;&#20837;&#36793;&#30028;&#27010;&#24565;&#21644;&#21033;&#29992;STL&#23646;&#24615;&#22686;&#24378;&#32467;&#26524;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12397</link><description>&lt;p&gt;
&#22810;&#31867;&#21035;&#26102;&#38388;&#36923;&#36753;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Multi-class Temporal Logic Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12397
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#31070;&#32463;&#32593;&#32476;&#21644;&#20449;&#21495;&#26102;&#38388;&#36923;&#36753;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#31867;&#21035;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#20998;&#31867;&#65292;&#20851;&#38190;&#36129;&#29486;&#21253;&#25324;&#24341;&#20837;&#36793;&#30028;&#27010;&#24565;&#21644;&#21033;&#29992;STL&#23646;&#24615;&#22686;&#24378;&#32467;&#26524;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#21487;&#20197;&#20195;&#34920;&#26080;&#20154;&#31995;&#32479;&#65288;&#22914;&#26080;&#20154;&#26426;&#21644;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#65289;&#30340;&#34892;&#20026;&#12290;&#22312;&#36825;&#19968;&#39046;&#22495;&#65292;&#20108;&#20803;&#21644;&#22810;&#31867;&#21035;&#20998;&#31867;&#38382;&#39064;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#20998;&#31867;&#25968;&#25454;&#30340;&#26041;&#27861;&#65307;&#28982;&#32780;&#65292;&#23427;&#20204;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#36825;&#22312;&#20174;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#20449;&#24687;&#26041;&#38754;&#26500;&#25104;&#20102;&#37325;&#35201;&#25361;&#25112;&#12290;&#20449;&#21495;&#26102;&#38388;&#36923;&#36753;&#65288;STL&#65289;&#26159;&#19968;&#31181;&#25551;&#36848;&#23450;&#26102;&#34892;&#20026;&#23646;&#24615;&#30340;&#24418;&#24335;&#21270;&#35821;&#35328;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#25152;&#26377;&#36825;&#20123;&#20803;&#32032;&#32467;&#21512;&#22312;&#19968;&#36215;&#30340;&#26041;&#27861;&#65306;&#20351;&#29992;&#34920;&#31034;STL&#35268;&#33539;&#30340;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#22810;&#31867;&#21035;&#20998;&#31867;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20004;&#20010;&#20851;&#38190;&#36129;&#29486;&#65306;1&#65289;&#25105;&#20204;&#24341;&#20837;&#20102;&#22810;&#31867;&#21035;&#20998;&#31867;&#30340;&#36793;&#30028;&#27010;&#24565;&#65292;2&#65289;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;STL&#30340;&#23646;&#24615;&#26469;&#22686;&#24378;&#32467;&#26524;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#19982;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12397v1 Announce Type: cross  Abstract: Time-series data can represent the behaviors of autonomous systems, such as drones and self-driving cars. The problem of binary and multi-class classification has received a lot of attention in this field. Neural networks represent a popular approach to classifying data; However, they lack interpretability, which poses a significant challenge in extracting meaningful information from them. Signal Temporal Logic (STL) is a formalism to describe the properties of timed behaviors. We propose a method that combines all of the above: neural networks that represent STL specifications for multi-class classification of time-series data. We offer two key contributions: 1) We introduce a notion of margin for multi-class classification, and 2) we introduce the use of STL-based attributes for enhancing the interpretability of the results. We evaluate our method on two datasets and compare with state-of-the-art baselines.
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#20915;&#31574;&#26641;&#35299;&#37322;&#22522;&#20110;&#29983;&#29289;&#26631;&#24535;&#29289;&#30340;&#35786;&#26029;&#27169;&#22411;&#65292;&#24110;&#21161;&#20020;&#24202;&#21307;&#29983;&#25552;&#39640;&#35782;&#21035;&#19981;&#20934;&#30830;&#39044;&#27979;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#22686;&#24378;&#21307;&#23398;&#35786;&#26029;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12394</link><description>&lt;p&gt;
&#21033;&#29992;&#29983;&#29289;&#26631;&#24535;&#29289;&#25552;&#39640;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#21644;&#21487;&#38752;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Model's Interpretability and Reliability using Biomarkers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12394
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#20915;&#31574;&#26641;&#35299;&#37322;&#22522;&#20110;&#29983;&#29289;&#26631;&#24535;&#29289;&#30340;&#35786;&#26029;&#27169;&#22411;&#65292;&#24110;&#21161;&#20020;&#24202;&#21307;&#29983;&#25552;&#39640;&#35782;&#21035;&#19981;&#20934;&#30830;&#39044;&#27979;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#22686;&#24378;&#21307;&#23398;&#35786;&#26029;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#19988;&#20855;&#26377;&#35299;&#37322;&#24615;&#30340;&#35786;&#26029;&#27169;&#22411;&#22312;&#21307;&#23398;&#36825;&#20010;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#22522;&#20110;&#29983;&#29289;&#26631;&#24535;&#29289;&#30340;&#32954;&#37096;&#36229;&#22768;&#35786;&#26029;&#27969;&#31243;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#20197;&#22686;&#24378;&#20020;&#24202;&#21307;&#29983;&#30340;&#35786;&#26029;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#35780;&#20272;&#20915;&#31574;&#26641;&#20998;&#31867;&#22120;&#21033;&#29992;&#29983;&#29289;&#26631;&#24535;&#29289;&#25552;&#20379;&#30340;&#35299;&#37322;&#26159;&#21542;&#33021;&#22815;&#25913;&#21892;&#29992;&#25143;&#35782;&#21035;&#27169;&#22411;&#19981;&#20934;&#30830;&#39044;&#27979;&#33021;&#21147;&#65292;&#19982;&#20256;&#32479;&#30340;&#26174;&#33879;&#24615;&#22270;&#30456;&#27604;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#34920;&#26126;&#65292;&#22522;&#20110;&#20020;&#24202;&#24314;&#31435;&#30340;&#29983;&#29289;&#26631;&#24535;&#29289;&#30340;&#20915;&#31574;&#26641;&#35299;&#37322;&#33021;&#22815;&#24110;&#21161;&#20020;&#24202;&#21307;&#29983;&#26816;&#27979;&#21040;&#20551;&#38451;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#21307;&#23398;&#35786;&#26029;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12394v1 Announce Type: cross  Abstract: Accurate and interpretable diagnostic models are crucial in the safety-critical field of medicine. We investigate the interpretability of our proposed biomarker-based lung ultrasound diagnostic pipeline to enhance clinicians' diagnostic capabilities. The objective of this study is to assess whether explanations from a decision tree classifier, utilizing biomarkers, can improve users' ability to identify inaccurate model predictions compared to conventional saliency maps. Our findings demonstrate that decision tree explanations, based on clinically established biomarkers, can assist clinicians in detecting false positives, thus improving the reliability of diagnostic models in medicine.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;AI&#31185;&#23398;&#23478;&#22242;&#38431;&#65288;TAIS&#65289;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#31616;&#21270;&#31185;&#23398;&#21457;&#29616;&#27969;&#31243;&#65292;&#30001;&#27169;&#25311;&#35282;&#33394;&#21327;&#20316;&#65292;&#29305;&#21035;&#20851;&#27880;&#20110;&#35782;&#21035;&#20855;&#26377;&#30142;&#30149;&#39044;&#27979;&#20215;&#20540;&#30340;&#22522;&#22240;</title><link>https://arxiv.org/abs/2402.12391</link><description>&lt;p&gt;
&#23454;&#29616;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#31185;&#23398;&#21457;&#29616;&#30340;AI&#31185;&#23398;&#23478;&#22242;&#38431;
&lt;/p&gt;
&lt;p&gt;
Toward a Team of AI-made Scientists for Scientific Discovery from Gene Expression Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12391
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;AI&#31185;&#23398;&#23478;&#22242;&#38431;&#65288;TAIS&#65289;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#31616;&#21270;&#31185;&#23398;&#21457;&#29616;&#27969;&#31243;&#65292;&#30001;&#27169;&#25311;&#35282;&#33394;&#21327;&#20316;&#65292;&#29305;&#21035;&#20851;&#27880;&#20110;&#35782;&#21035;&#20855;&#26377;&#30142;&#30149;&#39044;&#27979;&#20215;&#20540;&#30340;&#22522;&#22240;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#24050;&#25104;&#20026;&#31185;&#23398;&#21457;&#29616;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#20174;&#22797;&#26434;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21517;&#20026;AI&#31185;&#23398;&#23478;&#22242;&#38431;&#65288;TAIS&#65289;&#65292;&#26088;&#22312;&#31616;&#21270;&#31185;&#23398;&#21457;&#29616;&#27969;&#31243;&#12290;TAIS&#21253;&#25324;&#27169;&#25311;&#35282;&#33394;&#65292;&#21253;&#25324;&#39033;&#30446;&#32463;&#29702;&#12289;&#25968;&#25454;&#24037;&#31243;&#24072;&#21644;&#39046;&#22495;&#19987;&#23478;&#65292;&#27599;&#20010;&#35282;&#33394;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20195;&#34920;&#12290;&#36825;&#20123;&#35282;&#33394;&#21327;&#20316;&#20197;&#22797;&#21046;&#25968;&#25454;&#31185;&#23398;&#23478;&#36890;&#24120;&#25191;&#34892;&#30340;&#20219;&#21153;&#65292;&#29305;&#21035;&#20851;&#27880;&#20110;&#35782;&#21035;&#20855;&#26377;&#30142;&#30149;&#39044;&#27979;&#20215;&#20540;&#30340;&#22522;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12391v1 Announce Type: cross  Abstract: Machine learning has emerged as a powerful tool for scientific discovery, enabling researchers to extract meaningful insights from complex datasets. For instance, it has facilitated the identification of disease-predictive genes from gene expression data, significantly advancing healthcare. However, the traditional process for analyzing such datasets demands substantial human effort and expertise for the data selection, processing, and analysis. To address this challenge, we introduce a novel framework, a Team of AI-made Scientists (TAIS), designed to streamline the scientific discovery pipeline. TAIS comprises simulated roles, including a project manager, data engineer, and domain expert, each represented by a Large Language Model (LLM). These roles collaborate to replicate the tasks typically performed by data scientists, with a specific focus on identifying disease-predictive genes. Furthermore, we have curated a benchmark dataset t
&lt;/p&gt;</description></item><item><title>&#23433;&#20840;&#23545;&#40784;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#20250;&#36890;&#36807;&#27169;&#25311;&#22833;&#35843;&#26694;&#26550;&#65292;&#22312;&#23545;&#25239;&#24615;&#25805;&#32437;&#19979;&#20135;&#29983;&#21361;&#38505;&#32467;&#26524;&#65292;&#23545;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#21452;&#20493;&#26377;&#23475;&#24615;&#65292;&#39640;&#20110;&#24378;&#22522;&#32447;&#65292;&#24378;&#35843;&#20102;&#21363;&#20351;&#22312;&#23433;&#20840;&#23545;&#40784;&#21518;&#20063;&#38656;&#35201;&#37325;&#26032;&#35780;&#20272;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12343</link><description>&lt;p&gt;
&#27169;&#25311;&#22833;&#35843;: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#23545;&#40784;&#21487;&#33021;&#20250;&#36866;&#24471;&#20854;&#21453;&#65281;
&lt;/p&gt;
&lt;p&gt;
Emulated Disalignment: Safety Alignment for Large Language Models May Backfire!
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12343
&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#23545;&#40784;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#20250;&#36890;&#36807;&#27169;&#25311;&#22833;&#35843;&#26694;&#26550;&#65292;&#22312;&#23545;&#25239;&#24615;&#25805;&#32437;&#19979;&#20135;&#29983;&#21361;&#38505;&#32467;&#26524;&#65292;&#23545;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#21452;&#20493;&#26377;&#23475;&#24615;&#65292;&#39640;&#20110;&#24378;&#22522;&#32447;&#65292;&#24378;&#35843;&#20102;&#21363;&#20351;&#22312;&#23433;&#20840;&#23545;&#40784;&#21518;&#20063;&#38656;&#35201;&#37325;&#26032;&#35780;&#20272;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38656;&#35201;&#36827;&#34892;&#23433;&#20840;&#23545;&#40784;&#65292;&#20197;&#30830;&#20445;&#19982;&#20154;&#31867;&#36827;&#34892;&#23433;&#20840;&#30340;&#23545;&#35805;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#25512;&#29702;&#26102;&#25915;&#20987;&#26694;&#26550;&#65292;&#34920;&#26126;&#23433;&#20840;&#23545;&#40784;&#20063;&#21487;&#33021;&#22312;&#23545;&#25239;&#24615;&#25805;&#32437;&#19979;&#26080;&#24847;&#20013;&#20419;&#25104;&#26377;&#23475;&#32467;&#26524;&#12290;&#36825;&#20010;&#26694;&#26550;&#34987;&#21629;&#21517;&#20026;&#27169;&#25311;&#22833;&#35843;&#65288;ED&#65289;&#65292;&#22312;&#36755;&#20986;&#31354;&#38388;&#20013;&#19981;&#33391;&#22320;&#32452;&#21512;&#20102;&#19968;&#23545;&#24320;&#28304;&#39044;&#35757;&#32451;&#21644;&#23433;&#20840;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20135;&#29983;&#20102;&#19968;&#20010;&#26377;&#23475;&#30340;&#35821;&#35328;&#27169;&#22411;&#32780;&#26080;&#38656;&#20219;&#20309;&#35757;&#32451;&#12290;&#25105;&#20204;&#23545;ED&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#21644;&#22235;&#20010;&#27169;&#22411;&#31995;&#21015;&#65288;Llama-1&#12289;Llama-2&#12289;Mistral&#21644;Alpaca&#65289;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;ED&#20351;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26377;&#23475;&#24615;&#22686;&#21152;&#20102;&#19968;&#20493;&#65292;&#24182;&#32988;&#36807;&#24378;&#22522;&#32447;&#65292;&#20197;&#36739;&#22823;&#20248;&#21183;&#22312;48&#20010;&#35780;&#20272;&#23376;&#38598;&#20013;&#30340;43&#20010;&#20013;&#23454;&#29616;&#20102;&#26368;&#39640;&#30340;&#26377;&#23475;&#29575;&#12290;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#20984;&#26174;&#20102;&#21363;&#20351;&#22312;&#23433;&#20840;&#23545;&#40784;&#21518;&#65292;&#37325;&#26032;&#35780;&#20272;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#23454;&#36341;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12343v1 Announce Type: new  Abstract: Large language models (LLMs) need to undergo safety alignment to ensure safe conversations with humans. However, in this work, we introduce an inference-time attack framework, demonstrating that safety alignment can also unintentionally facilitate harmful outcomes under adversarial manipulation. This framework, named Emulated Disalignment (ED), adversely combines a pair of open-source pre-trained and safety-aligned language models in the output space to produce a harmful language model without any training. Our experiments with ED across three datasets and four model families (Llama-1, Llama-2, Mistral, and Alpaca) show that ED doubles the harmfulness of pre-trained models and outperforms strong baselines, achieving the highest harmful rate in 43 out of 48 evaluation subsets by a large margin. Crucially, our findings highlight the importance of reevaluating the practice of open-sourcing language models even after safety alignment.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36866;&#37197;&#22120;&#35843;&#20248;&#26694;&#26550;&#65292;&#36171;&#20104;&#39044;&#35757;&#32451;&#22270;&#27169;&#22411;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#20844;&#24179;&#24615;</title><link>https://arxiv.org/abs/2402.12161</link><description>&lt;p&gt;
&#36171;&#20104;&#39044;&#35757;&#32451;&#22270;&#27169;&#22411;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Endowing Pre-trained Graph Models with Provable Fairness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12161
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36866;&#37197;&#22120;&#35843;&#20248;&#26694;&#26550;&#65292;&#36171;&#20104;&#39044;&#35757;&#32451;&#22270;&#27169;&#22411;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#22270;&#27169;&#22411;&#65288;PGMs&#65289;&#26088;&#22312;&#25429;&#25417;&#21487;&#36716;&#31227;&#30340;&#22266;&#26377;&#32467;&#26500;&#23646;&#24615;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#31867;&#20284;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;PGMs&#20063;&#20250;&#32487;&#25215;&#20154;&#31867;&#31038;&#20250;&#20013;&#30340;&#20559;&#35265;&#65292;&#23548;&#33268;&#22312;&#19979;&#28216;&#24212;&#29992;&#20013;&#20986;&#29616;&#27495;&#35270;&#34892;&#20026;&#12290;&#29616;&#26377;&#20844;&#24179;&#26041;&#27861;&#30340;&#21435;&#20559;&#35265;&#36807;&#31243;&#36890;&#24120;&#19982;GNNs&#30340;&#21442;&#25968;&#20248;&#21270;&#30456;&#32467;&#21512;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#22312;&#29616;&#23454;&#20013;&#21487;&#33021;&#19982;&#19981;&#21516;&#30340;&#25935;&#24863;&#23646;&#24615;&#30456;&#20851;&#32852;&#65292;&#30452;&#25509;&#37319;&#29992;&#29616;&#26377;&#26041;&#27861;&#25913;&#21892;PGMs&#30340;&#20844;&#24179;&#24615;&#26159;&#19981;&#28789;&#27963;&#19988;&#20302;&#25928;&#30340;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#32570;&#20047;&#29702;&#35770;&#20445;&#35777;&#65292;&#21363;&#23545;&#27169;&#22411;&#39044;&#27979;&#20844;&#24179;&#24615;&#30340;&#21487;&#35777;&#26126;&#19979;&#38480;&#65292;&#36825;&#30452;&#25509;&#25552;&#20379;&#20102;&#23454;&#38469;&#22330;&#26223;&#19979;&#30340;&#20445;&#35777;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36866;&#37197;&#22120;&#35843;&#20248;&#26694;&#26550;&#65292;&#36171;&#20104;&#39044;&#35757;&#32451;\textbf{&#22270;}&#27169;&#22411;&#20855;&#26377;\textbf{&#21487;&#35777;&#26126;}&#30340;\textbf{&#20844;}&#24179;\textbf{&#24615;}&#65288;&#31216;&#20026;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12161v1 Announce Type: cross  Abstract: Pre-trained graph models (PGMs) aim to capture transferable inherent structural properties and apply them to different downstream tasks. Similar to pre-trained language models, PGMs also inherit biases from human society, resulting in discriminatory behavior in downstream applications. The debiasing process of existing fair methods is generally coupled with parameter optimization of GNNs. However, different downstream tasks may be associated with different sensitive attributes in reality, directly employing existing methods to improve the fairness of PGMs is inflexible and inefficient. Moreover, most of them lack a theoretical guarantee, i.e., provable lower bounds on the fairness of model predictions, which directly provides assurance in a practical scenario. To overcome these limitations, we propose a novel adapter-tuning framework that endows pre-trained \textbf{Graph} models with \textbf{P}rovable f\textbf{A}i\textbf{R}ness (called
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;WKVQuant&#65292;&#19968;&#31181;&#19987;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#30340;&#37327;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#37327;&#21270;&#26435;&#37325;&#21644;&#38190;&#20540;&#32531;&#23384;&#26469;&#25913;&#21892;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.12065</link><description>&lt;p&gt;
WKVQuant&#65306;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#26435;&#37325;&#21644;&#38190;&#20540;&#32531;&#23384;&#20197;&#25552;&#39640;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
WKVQuant: Quantizing Weight and Key/Value Cache for Large Language Models Gains More
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12065
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;WKVQuant&#65292;&#19968;&#31181;&#19987;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#30340;&#37327;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#37327;&#21270;&#26435;&#37325;&#21644;&#38190;&#20540;&#32531;&#23384;&#26469;&#25913;&#21892;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38754;&#20020;&#30528;&#37096;&#32626;&#25361;&#25112;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#20854;&#24040;&#22823;&#30340;&#20869;&#23384;&#38656;&#27714;&#21644;&#33258;&#22238;&#24402;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#30340;&#35745;&#31639;&#38656;&#27714;&#12290;&#26412;&#25991;&#36890;&#36807;&#20851;&#27880;LLMs&#30340;&#37327;&#21270;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#37327;&#21270;&#26159;&#19968;&#31181;&#36890;&#36807;&#23558;&#27169;&#22411;&#21442;&#25968;&#21644;&#28608;&#27963;&#36716;&#25442;&#20026;&#20302;&#27604;&#29305;&#25972;&#25968;&#26469;&#20943;&#23569;&#20869;&#23384;&#28040;&#32791;&#30340;&#25216;&#26415;&#12290;&#25105;&#20204;&#25209;&#21028;&#24615;&#22320;&#20998;&#26512;&#20102;&#29616;&#26377;&#30340;&#37327;&#21270;&#26041;&#27861;&#65292;&#35782;&#21035;&#20986;&#23427;&#20204;&#22312;&#24179;&#34913;&#37327;&#21270;LLMs&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#36229;&#36234;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;WKVQuant&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#20026;&#37327;&#21270;LLMs&#30340;&#21442;&#25968;&#26435;&#37325;&#21644;&#38190;&#20540;&#65288;KV&#65289;&#32531;&#23384;&#32780;&#35774;&#35745;&#30340;PTQ&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20165;&#32771;&#34385;&#36807;&#21435;&#30340;&#37327;&#21270;&#20197;&#25913;&#21892;&#27880;&#24847;&#21147;&#35745;&#31639;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#20108;&#32500;&#37327;&#21270;&#31574;&#30053;&#26469;&#22788;&#29702;KV&#32531;&#23384;&#30340;&#20998;&#24067;&#65292;&#20197;&#21450;&#19968;&#31181;&#36328;&#22359;&#37325;&#24314;&#27491;&#21017;&#21270;&#26041;&#27861;&#20197;&#24110;&#21161;&#27169;&#22411;&#21387;&#32553;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12065v1 Announce Type: cross  Abstract: Large Language Models (LLMs) face significant deployment challenges due to their substantial memory requirements and the computational demands of auto-regressive text generation process. This paper addresses these challenges by focusing on the quantization of LLMs, a technique that reduces memory consumption by converting model parameters and activations into low-bit integers. We critically analyze the existing quantization approaches, identifying their limitations in balancing the accuracy and efficiency of the quantized LLMs. To advance beyond these limitations, we propose WKVQuant, a PTQ framework especially designed for quantizing weights and the key/value (KV) cache of LLMs. Specifically, we incorporates past-only quantization to improve the computation of attention. Additionally, we introduce two-dimensional quantization strategy to handle the distribution of KV cache, along with a cross-block reconstruction regularization for pa
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#25915;&#20987;&#31574;&#30053;AICAttack&#65292;&#26088;&#22312;&#36890;&#36807;&#24494;&#23567;&#30340;&#22270;&#20687;&#25200;&#21160;&#26469;&#25915;&#20987;&#22270;&#20687;&#23383;&#24149;&#27169;&#22411;&#65292;&#22312;&#40657;&#30418;&#25915;&#20987;&#24773;&#26223;&#19979;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.11940</link><description>&lt;p&gt;
AICAttack&#65306;&#22522;&#20110;&#27880;&#24847;&#21147;&#20248;&#21270;&#30340;&#23545;&#25239;&#24615;&#22270;&#20687;&#23383;&#24149;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
AICAttack: Adversarial Image Captioning Attack with Attention-Based Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11940
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#25915;&#20987;&#31574;&#30053;AICAttack&#65292;&#26088;&#22312;&#36890;&#36807;&#24494;&#23567;&#30340;&#22270;&#20687;&#25200;&#21160;&#26469;&#25915;&#20987;&#22270;&#20687;&#23383;&#24149;&#27169;&#22411;&#65292;&#22312;&#40657;&#30418;&#25915;&#20987;&#24773;&#26223;&#19979;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#21462;&#24471;&#20102;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31561;&#35768;&#22810;&#20219;&#21153;&#19978;&#26174;&#33879;&#30340;&#25104;&#23601;&#12290;CV&#21644;NLP&#20132;&#21449;&#28857;&#19978;&#30340;&#22270;&#20687;&#23383;&#24149;&#38382;&#39064;&#20013;&#65292;&#30456;&#20851;&#27169;&#22411;&#23545;&#25239;&#25915;&#20987;&#30340;&#31283;&#20581;&#24615;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#25239;&#25915;&#20987;&#31574;&#30053;&#65292;&#31216;&#20026;AICAttack&#65288;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22270;&#20687;&#23383;&#24149;&#25915;&#20987;&#65289;&#65292;&#26088;&#22312;&#36890;&#36807;&#23545;&#22270;&#20687;&#36827;&#34892;&#24494;&#23567;&#25200;&#21160;&#26469;&#25915;&#20987;&#22270;&#20687;&#23383;&#24149;&#27169;&#22411;&#12290;&#22312;&#40657;&#30418;&#25915;&#20987;&#29615;&#22659;&#20013;&#36816;&#34892;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#19981;&#38656;&#35201;&#35775;&#38382;&#30446;&#26631;&#27169;&#22411;&#30340;&#26550;&#26500;&#12289;&#21442;&#25968;&#25110;&#26799;&#24230;&#20449;&#24687;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#20505;&#36873;&#36873;&#25321;&#26426;&#21046;&#65292;&#21487;&#35782;&#21035;&#26368;&#20339;&#20687;&#32032;&#36827;&#34892;&#25915;&#20987;&#65292;&#28982;&#21518;&#37319;&#29992;&#24046;&#20998;&#36827;&#21270;&#65288;DE&#65289;&#26469;&#25200;&#20081;&#20687;&#32032;&#30340;RGB&#20540;&#12290;&#36890;&#36807;&#23545;&#22522;&#20934;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;AICAttack&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11940v1 Announce Type: cross  Abstract: Recent advances in deep learning research have shown remarkable achievements across many tasks in computer vision (CV) and natural language processing (NLP). At the intersection of CV and NLP is the problem of image captioning, where the related models' robustness against adversarial attacks has not been well studied. In this paper, we present a novel adversarial attack strategy, which we call AICAttack (Attention-based Image Captioning Attack), designed to attack image captioning models through subtle perturbations on images. Operating within a black-box attack scenario, our algorithm requires no access to the target model's architecture, parameters, or gradient information. We introduce an attention-based candidate selection mechanism that identifies the optimal pixels to attack, followed by Differential Evolution (DE) for perturbing pixels' RGB values. We demonstrate AICAttack's effectiveness through extensive experiments on benchma
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#26694;&#26550; GPDiff&#65292;&#36890;&#36807;&#22312;&#28304;&#22478;&#24066;&#25968;&#25454;&#20248;&#21270;&#30340;&#27169;&#22411;&#21442;&#25968;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#23558;STG&#36801;&#31227;&#23398;&#20064;&#36716;&#21270;&#20026;&#39044;&#35757;&#32451;&#29983;&#25104;&#24335;&#36229;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#23545;&#19981;&#21516;&#25968;&#25454;&#20998;&#24067;&#21644;&#29305;&#23450;&#22478;&#24066;&#29305;&#24449;&#30340;&#36866;&#24212;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11922</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#26102;&#31354;&#22270;&#36801;&#31227;&#23398;&#20064;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Generative Pre-Training Framework for Spatio-Temporal Graph Transfer Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11922
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#26694;&#26550; GPDiff&#65292;&#36890;&#36807;&#22312;&#28304;&#22478;&#24066;&#25968;&#25454;&#20248;&#21270;&#30340;&#27169;&#22411;&#21442;&#25968;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#23558;STG&#36801;&#31227;&#23398;&#20064;&#36716;&#21270;&#20026;&#39044;&#35757;&#32451;&#29983;&#25104;&#24335;&#36229;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#23545;&#19981;&#21516;&#25968;&#25454;&#20998;&#24067;&#21644;&#29305;&#23450;&#22478;&#24066;&#29305;&#24449;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#31354;&#22270;&#65288;STG&#65289;&#23398;&#20064;&#23545;&#20110;&#26234;&#24935;&#22478;&#24066;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#28982;&#32780;&#22312;&#35768;&#22810;&#22478;&#24066;&#21644;&#22320;&#21306;&#24448;&#24448;&#23384;&#22312;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#26694;&#26550; GPDiff&#65292;&#29992;&#20110;STG&#36801;&#31227;&#23398;&#20064;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#32463;&#36807;&#28304;&#22478;&#24066;&#25968;&#25454;&#20248;&#21270;&#30340;&#19968;&#31995;&#21015;&#27169;&#22411;&#21442;&#25968;&#19978;&#36827;&#34892;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#26469;&#25191;&#34892;STG&#36801;&#31227;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11922v1 Announce Type: new  Abstract: Spatio-temporal graph (STG) learning is foundational for smart city applications, yet it is often hindered by data scarcity in many cities and regions. To bridge this gap, we propose a novel generative pre-training framework, GPDiff, for STG transfer learning. Unlike conventional approaches that heavily rely on common feature extraction or intricate transfer learning designs, our solution takes a novel approach by performing generative pre-training on a collection of model parameters optimized with data from source cities. We recast STG transfer learning as pre-training a generative hypernetwork, which generates tailored model parameters guided by prompts, allowing for adaptability to diverse data distributions and city-specific characteristics. GPDiff employs a diffusion model with a transformer-based denoising network, which is model-agnostic to integrate with powerful STG models. By addressing challenges arising from data gaps and the
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#31995;&#32479;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#22270;&#24418;&#21484;&#22238;&#30340;&#20934;&#30830;&#24615;&#21644;&#20559;&#35265;&#24494;&#32467;&#26500;&#65292;&#25506;&#35752;&#20102;&#23427;&#20204;&#19982;&#20154;&#31867;&#30340;&#24322;&#21516;&#20197;&#21450;&#23545;&#20854;&#20182;&#22270;&#24418;&#25512;&#29702;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.11821</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#22270;&#24418;&#21484;&#22238;&#30340;&#24494;&#32467;&#26500;&#21644;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Microstructures and Accuracy of Graph Recall by Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11821
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#31995;&#32479;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#22270;&#24418;&#21484;&#22238;&#30340;&#20934;&#30830;&#24615;&#21644;&#20559;&#35265;&#24494;&#32467;&#26500;&#65292;&#25506;&#35752;&#20102;&#23427;&#20204;&#19982;&#20154;&#31867;&#30340;&#24322;&#21516;&#20197;&#21450;&#23545;&#20854;&#20182;&#22270;&#24418;&#25512;&#29702;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24418;&#25968;&#25454;&#23545;&#35768;&#22810;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#20854;&#20013;&#24456;&#22810;&#25968;&#25454;&#20197;&#25991;&#26412;&#26684;&#24335;&#25551;&#36848;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#20934;&#30830;&#22320;&#21484;&#22238;&#21644;&#32534;&#30721;&#20808;&#21069;&#25991;&#26412;&#20013;&#25551;&#36848;&#30340;&#22270;&#24418;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#38656;&#35201;&#23637;&#31034;&#30340;&#22522;&#26412;&#20294;&#20851;&#38190;&#33021;&#21147;&#65292;&#20197;&#25191;&#34892;&#28041;&#21450;&#22270;&#24418;&#32467;&#26500;&#20449;&#24687;&#30340;&#25512;&#29702;&#20219;&#21153;&#12290;&#20154;&#31867;&#22312;&#22270;&#24418;&#21484;&#22238;&#26041;&#38754;&#30340;&#34920;&#29616;&#24050;&#34987;&#35748;&#30693;&#31185;&#23398;&#23478;&#30740;&#31350;&#20102;&#20960;&#21313;&#24180;&#65292;&#21457;&#29616;&#20854;&#32463;&#24120;&#21576;&#29616;&#19982;&#20154;&#31867;&#22788;&#29702;&#31038;&#20250;&#20851;&#31995;&#19968;&#33268;&#30340;&#26576;&#20123;&#32467;&#26500;&#24615;&#20559;&#35265;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#25105;&#20204;&#24456;&#23569;&#20102;&#35299;LLMs&#22312;&#31867;&#20284;&#22270;&#24418;&#21484;&#22238;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;&#65306;&#23427;&#20204;&#21484;&#22238;&#30340;&#22270;&#24418;&#26159;&#21542;&#20063;&#21576;&#29616;&#26576;&#20123;&#20559;&#35265;&#27169;&#24335;&#65292;&#22914;&#26524;&#26159;&#65292;&#23427;&#20204;&#19982;&#20154;&#31867;&#30340;&#34920;&#29616;&#26377;&#20309;&#19981;&#21516;&#24182;&#22914;&#20309;&#24433;&#21709;&#20854;&#20182;&#22270;&#24418;&#25512;&#29702;&#20219;&#21153;&#65311;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#31532;&#19968;&#27425;&#23545;LLMs&#36827;&#34892;&#22270;&#24418;&#21484;&#22238;&#30340;&#31995;&#32479;&#30740;&#31350;&#65292;&#30740;&#31350;&#20854;&#20934;&#30830;&#24615;&#21644;&#20559;&#35265;&#24494;&#32467;&#26500;&#65288;&#23616;&#37096;&#32467;&#26500;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11821v1 Announce Type: cross  Abstract: Graphs data is crucial for many applications, and much of it exists in the relations described in textual format. As a result, being able to accurately recall and encode a graph described in earlier text is a basic yet pivotal ability that LLMs need to demonstrate if they are to perform reasoning tasks that involve graph-structured information. Human performance at graph recall by has been studied by cognitive scientists for decades, and has been found to often exhibit certain structural patterns of bias that align with human handling of social relationships. To date, however, we know little about how LLMs behave in analogous graph recall tasks: do their recalled graphs also exhibit certain biased patterns, and if so, how do they compare with humans and affect other graph reasoning tasks? In this work, we perform the first systematical study of graph recall by LLMs, investigating the accuracy and biased microstructures (local structura
&lt;/p&gt;</description></item><item><title>MARS&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20998;&#20989;&#25968;MARS&#65292;&#32771;&#34385;&#20102;&#29983;&#25104;&#24207;&#21015;&#20013;&#27599;&#20010;&#26631;&#35760;&#30340;&#35821;&#20041;&#36129;&#29486;&#65292;&#35813;&#26041;&#27861;&#25913;&#36827;&#20102;&#29983;&#25104;&#24335;LLMs&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.11756</link><description>&lt;p&gt;
MARS&#65306;&#29992;&#20110;&#29983;&#25104;&#24335;LLMs&#20013;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#24847;&#20041;&#24863;&#30693;&#21709;&#24212;&#35780;&#20998;
&lt;/p&gt;
&lt;p&gt;
MARS: Meaning-Aware Response Scoring for Uncertainty Estimation in Generative LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11756
&lt;/p&gt;
&lt;p&gt;
MARS&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20998;&#20989;&#25968;MARS&#65292;&#32771;&#34385;&#20102;&#29983;&#25104;&#24207;&#21015;&#20013;&#27599;&#20010;&#26631;&#35760;&#30340;&#35821;&#20041;&#36129;&#29486;&#65292;&#35813;&#26041;&#27861;&#25913;&#36827;&#20102;&#29983;&#25104;&#24335;LLMs&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22240;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#21331;&#36234;&#34920;&#29616;&#32780;&#34987;&#24191;&#27867;&#21033;&#29992;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20135;&#29983;&#19981;&#20934;&#30830;&#25110;&#35823;&#23548;&#24615;&#36755;&#20986;&#30340;&#20542;&#21521;&#21487;&#33021;&#24102;&#26469;&#28508;&#22312;&#39118;&#38505;&#65292;&#23588;&#20854;&#26159;&#22312;&#39640;&#39118;&#38505;&#29615;&#22659;&#20013;&#12290;&#22240;&#27492;&#65292;&#20272;&#35745;&#29983;&#25104;&#24335;LLM&#36755;&#20986;&#30340;&#27491;&#30830;&#24615;&#26159;&#22686;&#24378;&#21487;&#38752;&#24615;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#29983;&#25104;&#24335;LLMs&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65288;UE&#65289;&#26159;&#19968;&#20010;&#19981;&#26029;&#21457;&#23637;&#30340;&#39046;&#22495;&#65292;&#20854;&#20013;SOTA&#22522;&#20110;&#27010;&#29575;&#30340;&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#38271;&#24230;&#26631;&#20934;&#21270;&#35780;&#20998;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24847;&#20041;&#24863;&#30693;&#21709;&#24212;&#35780;&#20998;&#65288;MARS&#65289;&#30340;&#26367;&#20195;&#38271;&#24230;&#26631;&#20934;&#21270;&#35780;&#20998;&#30340;UE&#26041;&#27861;&#12290;MARS&#26159;&#19968;&#31181;&#32771;&#34385;&#22312;&#38382;&#39064;&#30340;&#19978;&#19979;&#25991;&#20013;&#29983;&#25104;&#24207;&#21015;&#20013;&#27599;&#20010;&#26631;&#35760;&#30340;&#35821;&#20041;&#36129;&#29486;&#30340;&#26032;&#22411;&#35780;&#20998;&#20989;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#23558;MARS&#25972;&#21512;&#21040;UE&#26041;&#27861;&#20013;&#20250;&#22312;UE&#24615;&#33021;&#19978;&#24102;&#26469;&#26222;&#36941;&#21644;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#20351;&#29992;&#19977;&#31181;&#19981;&#21516;&#30340;&#38381;&#21367;&#24335;&#38382;&#31572;&#26469;&#36827;&#34892;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11756v1 Announce Type: new  Abstract: Generative Large Language Models (LLMs) are widely utilized for their excellence in various tasks. However, their tendency to produce inaccurate or misleading outputs poses a potential risk, particularly in high-stakes environments. Therefore, estimating the correctness of generative LLM outputs is an important task for enhanced reliability. Uncertainty Estimation (UE) in generative LLMs is an evolving domain, where SOTA probability-based methods commonly employ length-normalized scoring. In this work, we propose Meaning-Aware Response Scoring (MARS) as an alternative to length-normalized scoring for UE methods. MARS is a novel scoring function that considers the semantic contribution of each token in the generated sequence in the context of the question. We demonstrate that integrating MARS into UE methods results in a universal and significant improvement in UE performance. We conduct experiments using three distinct closed-book questi
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;Diagonalisation Stochastic Gradient Descent&#65288;&#23545;&#35282;&#21270;SGD&#65289;&#65292;&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;&#21644;&#24179;&#28369;&#23454;&#29616;&#38750;&#21487;&#24494;&#27169;&#22411;&#30340;&#24555;&#36895;&#25910;&#25947;SGD&#65292;&#22312;&#23454;&#35777;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#31616;&#21333;&#12289;&#24555;&#36895;&#12289;&#31283;&#23450;&#65292;&#24182;&#19988;&#21462;&#24471;&#20102;&#25968;&#37327;&#32423;&#30340;&#24037;&#20316;&#35268;&#33539;&#21270;&#26041;&#24046;&#38477;&#20302;&#12290;</title><link>https://arxiv.org/abs/2402.11752</link><description>&lt;p&gt;
&#23545;&#35282;&#21270;SGD&#65306;&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;&#21644;&#24179;&#28369;&#23454;&#29616;&#38750;&#21487;&#24494;&#27169;&#22411;&#30340;&#24555;&#36895;&#25910;&#25947;SGD
&lt;/p&gt;
&lt;p&gt;
Diagonalisation SGD: Fast &amp; Convergent SGD for Non-Differentiable Models via Reparameterisation and Smoothing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11752
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;Diagonalisation Stochastic Gradient Descent&#65288;&#23545;&#35282;&#21270;SGD&#65289;&#65292;&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;&#21644;&#24179;&#28369;&#23454;&#29616;&#38750;&#21487;&#24494;&#27169;&#22411;&#30340;&#24555;&#36895;&#25910;&#25947;SGD&#65292;&#22312;&#23454;&#35777;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#31616;&#21333;&#12289;&#24555;&#36895;&#12289;&#31283;&#23450;&#65292;&#24182;&#19988;&#21462;&#24471;&#20102;&#25968;&#37327;&#32423;&#30340;&#24037;&#20316;&#35268;&#33539;&#21270;&#26041;&#24046;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#23545;&#20110;&#38750;&#21487;&#24494;&#27169;&#22411;&#65292;&#23637;&#29616;&#20986;&#36739;&#20302;&#26041;&#24046;&#30340;&#37325;&#26032;&#21442;&#25968;&#21270;&#26799;&#24230;&#20272;&#35745;&#22120;&#22312;&#23454;&#36341;&#20013;&#23384;&#22312;&#20559;&#24046;&#12290;&#36825;&#21487;&#33021;&#21361;&#21450;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#26041;&#27861;&#65288;&#22914;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;SGD&#65289;&#30340;&#27491;&#30830;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#35821;&#27861;&#26694;&#26550;&#26469;&#20998;&#22359;&#22320;&#23450;&#20041;&#38750;&#21487;&#24494;&#20989;&#25968;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#26041;&#27861;&#65292;&#20197;&#33719;&#24471;&#20351;&#37325;&#26032;&#21442;&#25968;&#21270;&#26799;&#24230;&#20272;&#35745;&#22120;&#26080;&#20559;&#30340;&#24179;&#28369;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;SGD&#21464;&#20307;&#65292;&#23545;&#35282;&#21270;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65292;&#23427;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#36880;&#27493;&#25552;&#39640;&#24179;&#28369;&#36817;&#20284;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#35777;&#26126;&#25910;&#25947;&#21040;&#26410;&#24179;&#28369;&#65288;&#21407;&#22987;&#65289;&#30446;&#26631;&#30340;&#31283;&#23450;&#28857;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#26174;&#31034;&#65292;&#19982;&#29616;&#26377;&#25216;&#26415;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#31616;&#21333;&#12289;&#24555;&#36895;&#12289;&#31283;&#23450;&#65292;&#24182;&#19988;&#22312;&#24037;&#20316;&#35268;&#33539;&#21270;&#26041;&#24046;&#19978;&#23454;&#29616;&#20102;&#25968;&#37327;&#32423;&#30340;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11752v1 Announce Type: cross  Abstract: It is well-known that the reparameterisation gradient estimator, which exhibits low variance in practice, is biased for non-differentiable models. This may compromise correctness of gradient-based optimisation methods such as stochastic gradient descent (SGD). We introduce a simple syntactic framework to define non-differentiable functions piecewisely and present a systematic approach to obtain smoothings for which the reparameterisation gradient estimator is unbiased. Our main contribution is a novel variant of SGD, Diagonalisation Stochastic Gradient Descent, which progressively enhances the accuracy of the smoothed approximation during optimisation, and we prove convergence to stationary points of the unsmoothed (original) objective. Our empirical evaluation reveals benefits over the state of the art: our approach is simple, fast, stable and attains orders of magnitude reduction in work-normalised variance.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#25991;&#26723;&#22270;&#20687;&#20013;&#35782;&#21035;&#21644;&#23450;&#20301;&#25991;&#26723;&#23545;&#35937;&#65292;&#20197;&#20943;&#23569;&#22823;&#22411;&#27169;&#22411;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#30340;&#37096;&#32626;&#25104;&#26412;</title><link>https://arxiv.org/abs/2402.11401</link><description>&lt;p&gt;
GraphKD&#65306;&#25506;&#32034;&#30693;&#35782;&#33976;&#39311;&#22312;&#25991;&#26723;&#30446;&#26631;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#36890;&#36807;&#32467;&#26500;&#21270;&#22270;&#21019;&#24314;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
GraphKD: Exploring Knowledge Distillation Towards Document Object Detection with Structured Graph Creation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11401
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#25991;&#26723;&#22270;&#20687;&#20013;&#35782;&#21035;&#21644;&#23450;&#20301;&#25991;&#26723;&#23545;&#35937;&#65292;&#20197;&#20943;&#23569;&#22823;&#22411;&#27169;&#22411;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#30340;&#37096;&#32626;&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25991;&#26723;&#20013;&#36827;&#34892;&#30446;&#26631;&#26816;&#27979;&#26159;&#33258;&#21160;&#21270;&#35782;&#21035;&#25968;&#23383;&#25110;&#25195;&#25551;&#25991;&#26723;&#20013;&#30340;&#32467;&#26500;&#20803;&#32032;&#30340;&#20851;&#38190;&#27493;&#39588;&#65292;&#36890;&#36807;&#29702;&#35299;&#19981;&#21516;&#20803;&#32032;&#20043;&#38388;&#30340;&#20998;&#23618;&#32467;&#26500;&#21644;&#20851;&#31995;&#12290;&#22823;&#22411;&#21644;&#22797;&#26434;&#30340;&#27169;&#22411;&#34429;&#28982;&#21487;&#20197;&#23454;&#29616;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#22312;&#35745;&#31639;&#19978;&#26114;&#36149;&#19988;&#21344;&#29992;&#20869;&#23384;&#65292;&#20351;&#20854;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#37096;&#32626;&#21464;&#24471;&#19981;&#20999;&#23454;&#38469;&#12290;&#30693;&#35782;&#33976;&#39311;&#20801;&#35768;&#25105;&#20204;&#21019;&#24314;&#23567;&#22411;&#19988;&#26356;&#39640;&#25928;&#30340;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#20445;&#30041;&#20102;&#22823;&#22411;&#27169;&#22411;&#30340;&#22823;&#37096;&#20998;&#24615;&#33021;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#20197;&#27491;&#30830;&#35782;&#21035;&#24182;&#23450;&#20301;&#25991;&#26723;&#22270;&#20687;&#20013;&#30340;&#25991;&#26723;&#23545;&#35937;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20855;&#26377;&#24314;&#35758;&#32423;&#29305;&#24449;&#30340;&#32467;&#26500;&#21270;&#22270;&#65292;&#36793;&#34920;&#31034;&#19981;&#21516;&#24314;&#35758;&#21306;&#22495;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20943;&#23569;&#25991;&#26412;&#20559;&#35265;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#33410;&#28857;&#25277;&#26679;&#31574;&#30053;&#26469;&#20462;&#21098;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11401v1 Announce Type: cross  Abstract: Object detection in documents is a key step to automate the structural elements identification process in a digital or scanned document through understanding the hierarchical structure and relationships between different elements. Large and complex models, while achieving high accuracy, can be computationally expensive and memory-intensive, making them impractical for deployment on resource constrained devices. Knowledge distillation allows us to create small and more efficient models that retain much of the performance of their larger counterparts. Here we present a graph-based knowledge distillation framework to correctly identify and localize the document objects in a document image. Here, we design a structured graph with nodes containing proposal-level features and edges representing the relationship between the different proposal regions. Also, to reduce text bias an adaptive node sampling strategy is designed to prune the weight
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#24320;&#21457;&#20102;&#19968;&#20010;&#20998;&#26512;&#24179;&#21488;&#65292;&#21487;&#20197;&#20174;&#38041;&#38043;&#30719;&#23454;&#39564;&#20013;&#25552;&#21462;&#22810;&#20010;&#22522;&#26412;&#26448;&#26009;&#21442;&#25968;&#65292;&#21152;&#36895;&#26448;&#26009;&#21457;&#29616;&#21644;&#21322;&#23548;&#20307;&#20248;&#21270;</title><link>https://arxiv.org/abs/2402.11101</link><description>&lt;p&gt;
&#36890;&#36807;&#36125;&#21494;&#26031;&#20248;&#21270;&#20174;&#38041;&#38043;&#30719;&#23454;&#39564;&#20013;&#25552;&#21462;&#22522;&#20110;&#29289;&#29702;&#30340;&#26448;&#26009;&#21442;&#25968;
&lt;/p&gt;
&lt;p&gt;
Physics-based material parameters extraction from perovskite experiments via Bayesian optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11101
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#24320;&#21457;&#20102;&#19968;&#20010;&#20998;&#26512;&#24179;&#21488;&#65292;&#21487;&#20197;&#20174;&#38041;&#38043;&#30719;&#23454;&#39564;&#20013;&#25552;&#21462;&#22810;&#20010;&#22522;&#26412;&#26448;&#26009;&#21442;&#25968;&#65292;&#21152;&#36895;&#26448;&#26009;&#21457;&#29616;&#21644;&#21322;&#23548;&#20307;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#23450;&#37327;&#23454;&#39564;&#20998;&#26512;&#20013;&#25552;&#21462;&#26448;&#26009;&#21442;&#25968;&#30340;&#33021;&#21147;&#23545;&#20110;&#21512;&#29702;&#35774;&#35745;&#21644;&#29702;&#35770;&#36827;&#27493;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#29702;&#35770;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#21644;&#26448;&#26009;&#21442;&#25968;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#36825;&#31181;&#20998;&#26512;&#30340;&#38590;&#24230;&#26174;&#30528;&#22686;&#21152;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20351;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#24320;&#21457;&#20102;&#19968;&#20010;&#20998;&#26512;&#24179;&#21488;&#65292;&#21487;&#20197;&#20174;&#30636;&#24577;&#20809;&#33268;&#21457;&#20809;&#23454;&#39564;&#20013;&#25552;&#21462;&#19968;&#20010;&#26377;&#26426;&#37329;&#23646;&#38041;&#38043;&#30719;&#21322;&#23548;&#20307;&#30340;8&#20010;&#22522;&#26412;&#26448;&#26009;&#21442;&#25968;&#65292;&#22522;&#20110;&#19968;&#20010;&#21253;&#25324;&#36733;&#27969;&#23376;&#28418;&#31227;&#25193;&#25955;&#21644;&#21160;&#24577;&#32570;&#38519;&#21344;&#25454;&#30340;&#22797;&#26434;&#20840;&#29289;&#29702;&#27169;&#22411;&#12290;&#28909;&#38477;&#35299;&#30340;&#19968;&#20010;&#31034;&#20363;&#30740;&#31350;&#34920;&#26126;&#65292;&#25530;&#26434;&#27987;&#24230;&#21644;&#36733;&#27969;&#23376;&#36801;&#31227;&#29575;&#30340;&#21464;&#21270;&#20027;&#23548;&#65292;&#32780;&#32570;&#38519;&#33021;&#32423;&#20960;&#20046;&#20445;&#25345;&#19981;&#21464;&#12290;&#36825;&#20010;&#24179;&#21488;&#21487;&#20197;&#26041;&#20415;&#22320;&#24212;&#29992;&#20110;&#20854;&#20182;&#23454;&#39564;&#25110;&#23454;&#39564;&#32452;&#21512;&#65292;&#21152;&#36895;&#26448;&#26009;&#21457;&#29616;&#21644;&#21322;&#23548;&#20307;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11101v1 Announce Type: cross  Abstract: The ability to extract material parameters from quantitative experimental analysis is essential for rational design and theory advancement. However, the difficulty of this analysis increases significantly with the complexity of the theoretical model and the number of material parameters. Here we use Bayesian optimization to develop an analysis platform that can extract up to 8 fundamental material parameters of an organometallic perovskite semiconductor from a transient photoluminescence experiment, based on a complex full physics model that includes drift-diffusion of carriers and dynamic defect occupation. An example study of thermal degradation reveals that changes in doping concentration and carrier mobility dominate, while the defect energy level remains nearly unchanged. This platform can be conveniently applied to other experiments or to combinations of experiments, accelerating materials discovery and optimization of semiconduc
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36951;&#20256;&#31639;&#27861;&#36827;&#34892;&#36755;&#20837;&#29305;&#24449;&#30340;&#26368;&#20339;&#37325;&#32553;&#25918;&#26469;&#25913;&#21892;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#25928;&#29575;&#21644;&#27867;&#21270;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.10964</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#26368;&#20339;&#29305;&#24449;&#37325;&#32553;&#25918;
&lt;/p&gt;
&lt;p&gt;
Optimal feature rescaling in machine learning based on neural networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10964
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36951;&#20256;&#31639;&#27861;&#36827;&#34892;&#36755;&#20837;&#29305;&#24449;&#30340;&#26368;&#20339;&#37325;&#32553;&#25918;&#26469;&#25913;&#21892;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#25928;&#29575;&#21644;&#27867;&#21270;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36951;&#20256;&#31639;&#27861;&#65288;GA&#65289;&#36827;&#34892;&#36755;&#20837;&#29305;&#24449;&#30340;&#26368;&#20339;&#37325;&#32553;&#25918;&#65288;OFR&#65289;&#26469;&#25913;&#21892;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65288;FFNNs&#65289;&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;OFR&#37325;&#26032;&#22609;&#36896;&#20102;&#36755;&#20837;&#31354;&#38388;&#65292;&#25913;&#21892;&#20102;&#29992;&#20110;&#35757;&#32451;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#31639;&#27861;&#30340;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;GA&#36827;&#34892;&#30340;&#27604;&#20363;&#22240;&#23376;&#25506;&#32034;&#21644;&#36873;&#25321;&#23545;&#24212;&#20110;&#27599;&#27425;&#35757;&#32451;&#23581;&#35797;&#20013;&#31532;&#19968;&#23618;&#26435;&#37325;&#30340;&#19981;&#21516;&#21021;&#22987;&#21270;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#19968;&#20010;&#22810;&#36215;&#28857;&#20840;&#23616;&#25628;&#32034;&#31639;&#27861;&#65288;&#23613;&#31649;&#20165;&#38480;&#20110;&#23569;&#37327;&#26435;&#37325;&#65289;&#65292;&#20174;&#32780;&#20419;&#36827;&#20102;&#20840;&#23616;&#26368;&#23567;&#20540;&#30340;&#23454;&#29616;&#12290;&#35813;&#26041;&#27861;&#24050;&#22312;&#27169;&#25311;&#23454;&#38469;&#24037;&#19994;&#36807;&#31243;&#65288;&#26080;&#24515;&#30952;&#21066;&#65289;&#32467;&#26524;&#30340;FFNN&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10964v1 Announce Type: new  Abstract: This paper proposes a novel approach to improve the training efficiency and the generalization performance of Feed Forward Neural Networks (FFNNs) resorting to an optimal rescaling of input features (OFR) carried out by a Genetic Algorithm (GA). The OFR reshapes the input space improving the conditioning of the gradient-based algorithm used for the training. Moreover, the scale factors exploration entailed by GA trials and selection corresponds to different initialization of the first layer weights at each training attempt, thus realizing a multi-start global search algorithm (even though restrained to few weights only) which fosters the achievement of a global minimum. The approach has been tested on a FFNN modeling the outcome of a real industrial process (centerless grinding).
&lt;/p&gt;</description></item><item><title>&#24322;&#31867;&#33258;&#21160;&#25552;&#31034;&#30340;&#19981;&#21512;&#29702;&#26377;&#25928;&#24615;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#21508;&#31181;&#25552;&#31034;&#26102;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#21253;&#25324;&#8220;&#31215;&#26497;&#24605;&#32771;&#8221;&#25552;&#31034;&#20250;&#23545;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#31215;&#26497;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.10949</link><description>&lt;p&gt;
&#24322;&#31867;&#33258;&#21160;&#25552;&#31034;&#30340;&#19981;&#21512;&#29702;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Unreasonable Effectiveness of Eccentric Automatic Prompts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10949
&lt;/p&gt;
&lt;p&gt;
&#24322;&#31867;&#33258;&#21160;&#25552;&#31034;&#30340;&#19981;&#21512;&#29702;&#26377;&#25928;&#24615;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#21508;&#31181;&#25552;&#31034;&#26102;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#21253;&#25324;&#8220;&#31215;&#26497;&#24605;&#32771;&#8221;&#25552;&#31034;&#20250;&#23545;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#31215;&#26497;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#38382;&#39064;&#35299;&#20915;&#21644;&#22522;&#26412;&#25968;&#23398;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#21151;&#25928;&#39640;&#24230;&#20381;&#36182;&#20110;&#25552;&#31034;&#30340;&#21046;&#23450;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#37327;&#21270;&#23558;&#8220;&#31215;&#26497;&#24605;&#32771;&#8221;&#32435;&#20837;&#31995;&#32479;&#25552;&#31034;&#28040;&#24687;&#30340;&#24433;&#21709;&#65292;&#28982;&#21518;&#23558;&#20854;&#19982;&#31995;&#32479;&#21270;&#25552;&#31034;&#20248;&#21270;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;60&#31181;&#31995;&#32479;&#28040;&#24687;&#29255;&#27573;&#30340;&#24615;&#33021;&#65292;&#20998;&#21035;&#20351;&#29992;&#21644;&#19981;&#20351;&#29992;Chain of Thought&#25552;&#31034;&#65292;&#36328;&#19977;&#20010;&#21442;&#25968;&#33539;&#22260;&#20174;70&#20159;&#21040;70&#20159;&#20010;&#21464;&#37327;&#30340;&#27169;&#22411;&#65292;&#22312;GSM8K&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#32467;&#26524;&#24182;&#19981;&#22312;&#25152;&#26377;&#27169;&#22411;&#20013;&#26222;&#36941;&#36866;&#29992;&#12290;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#21253;&#25324;&#8220;&#31215;&#26497;&#24605;&#32771;&#8221;&#25552;&#31034;&#20250;&#31215;&#26497;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;Llama2-70B&#22312;&#19981;&#20351;&#29992;Chain of Thought&#26102;&#26159;&#20010;&#20363;&#22806;&#65292;&#22240;&#20026;&#21457;&#29616;&#26368;&#20339;&#31995;&#32479;&#28040;&#24687;&#23454;&#38469;&#19978;&#26159;&#27809;&#26377;&#28040;&#24687;&#12290;&#32771;&#34385;&#21040;&#32452;&#21512;&#22797;&#26434;&#24615;&#65292;&#20197;&#21450;&#20854;&#23548;&#33267;&#30340;&#21152;# Truncated due to exceeding character limit.
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10949v1 Announce Type: new  Abstract: Large Language Models (LLMs) have demonstrated remarkable problem-solving and basic mathematics abilities. However, their efficacy is highly contingent on the formulation of the prompt. This study endeavors to quantify the influence of incorporating "positive thinking" into the system message of the prompt, then compare that to systematic prompt optimization. We assess the performance of 60 combinations of system message snippets, tested with and without Chain of Thought prompting, across three models with parameters ranging from 7 to 70 billion on the GSM8K dataset. Our findings reveal that results do not universally generalize across models. In most instances, the inclusion of "positive thinking" prompts positively affected model performance. Notably, however, Llama2-70B exhibited an exception when not utilizing Chain of Thought, as the optimal system message was found to be none at all. Given the combinatorial complexity, and thus com
&lt;/p&gt;</description></item><item><title>ConSmax&#26159;&#19968;&#31181;&#30828;&#20214;&#21451;&#22909;&#22411;Softmax&#26367;&#20195;&#26041;&#26696;&#65292;&#36890;&#36807;&#24341;&#20837;&#21487;&#23398;&#20064;&#21442;&#25968;&#65292;&#22312;&#19981;&#24433;&#21709;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#23545;&#21407;Softmax&#20851;&#38190;&#20219;&#21153;&#30340;&#39640;&#25928;&#22788;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.10930</link><description>&lt;p&gt;
ConSmax: &#20855;&#26377;&#21487;&#23398;&#20064;&#21442;&#25968;&#30340;&#30828;&#20214;&#21451;&#22909;&#22411;Softmax&#26367;&#20195;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
ConSmax: Hardware-Friendly Alternative Softmax with Learnable Parameters
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10930
&lt;/p&gt;
&lt;p&gt;
ConSmax&#26159;&#19968;&#31181;&#30828;&#20214;&#21451;&#22909;&#22411;Softmax&#26367;&#20195;&#26041;&#26696;&#65292;&#36890;&#36807;&#24341;&#20837;&#21487;&#23398;&#20064;&#21442;&#25968;&#65292;&#22312;&#19981;&#24433;&#21709;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#23545;&#21407;Softmax&#20851;&#38190;&#20219;&#21153;&#30340;&#39640;&#25928;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#27880;&#24847;&#26426;&#21046;&#23558;&#22522;&#20110;transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#21367;&#31215;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21306;&#20998;&#24320;&#26469;&#12290;&#23613;&#31649;&#24615;&#33021;&#26377;&#25152;&#25552;&#21319;&#65292;&#20294;&#30001;&#20110;&#33258;&#27880;&#24847;&#20013;&#24191;&#27867;&#20351;&#29992;Softmax&#65292;&#22312;&#30789;&#19978;&#23454;&#29616;&#23454;&#26102;LLM&#25512;&#26029;&#20173;&#20855;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Constant Softmax&#65288;ConSmax&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;Softmax&#26367;&#20195;&#26041;&#26696;&#65292;&#37319;&#29992;&#21487;&#24494;&#30340;&#35268;&#33539;&#21270;&#21442;&#25968;&#26469;&#28040;&#38500;Softmax&#20013;&#30340;&#26368;&#22823;&#25628;&#32034;&#21644;&#20998;&#27597;&#27714;&#21644;&#65292;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;&#24182;&#34892;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10930v1 Announce Type: cross  Abstract: The self-attention mechanism sets transformer-based large language model (LLM) apart from the convolutional and recurrent neural networks. Despite the performance improvement, achieving real-time LLM inference on silicon is challenging due to the extensively used Softmax in self-attention. Apart from the non-linearity, the low arithmetic intensity greatly reduces the processing parallelism, which becomes the bottleneck especially when dealing with a longer context. To address this challenge, we propose Constant Softmax (ConSmax), a software-hardware co-design as an efficient Softmax alternative. ConSmax employs differentiable normalization parameters to remove the maximum searching and denominator summation in Softmax. It allows for massive parallelization while performing the critical tasks of Softmax. In addition, a scalable ConSmax hardware utilizing a bitwidth-split look-up table (LUT) can produce lossless non-linear operation and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#20139;&#20102;&#20851;&#20110;&#22312;&#24037;&#19994;&#29615;&#22659;&#20013;&#20351;&#29992;AED&#31995;&#32479;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#22312;&#36825;&#31181;&#29615;&#22659;&#20013;&#36866;&#24403;&#30340;&#30446;&#26631;&#21644;&#31995;&#32479;&#35268;&#26684;&#30340;&#35270;&#35282;&#65292;&#26368;&#32456;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#21453;&#20107;&#23454;&#25512;&#26029;&#30340;AED&#26694;&#26550;&#24182;&#22312;&#21830;&#19994;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;</title><link>https://arxiv.org/abs/2402.10870</link><description>&lt;p&gt;
&#19977;&#30028;&#20043;&#26368;&#65306;&#23454;&#36341;&#20013;&#30340;&#25968;&#23383;&#33829;&#38144;&#33258;&#36866;&#24212;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Best of Three Worlds: Adaptive Experimentation for Digital Marketing in Practice
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10870
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#20139;&#20102;&#20851;&#20110;&#22312;&#24037;&#19994;&#29615;&#22659;&#20013;&#20351;&#29992;AED&#31995;&#32479;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#22312;&#36825;&#31181;&#29615;&#22659;&#20013;&#36866;&#24403;&#30340;&#30446;&#26631;&#21644;&#31995;&#32479;&#35268;&#26684;&#30340;&#35270;&#35282;&#65292;&#26368;&#32456;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#21453;&#20107;&#23454;&#25512;&#26029;&#30340;AED&#26694;&#26550;&#24182;&#22312;&#21830;&#19994;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#36866;&#24212;&#23454;&#39564;&#35774;&#35745;&#65288;AED&#65289;&#26041;&#27861;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#24037;&#19994;&#30028;&#29992;&#20316;&#19968;&#31181;&#24037;&#20855;&#65292;&#20197;&#25552;&#39640;&#27979;&#35797;&#21534;&#21520;&#37327;&#25110;&#20943;&#23569;&#19982;&#20256;&#32479;A/B/N&#27979;&#35797;&#26041;&#27861;&#30456;&#27604;&#30340;&#23454;&#39564;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#34892;&#20026;&#21644;&#20445;&#35777;&#22312;&#29702;&#24819;&#21270;&#30340;&#31283;&#24577;&#35774;&#32622;&#20043;&#22806;&#24182;&#19981;&#20026;&#20154;&#29087;&#30693;&#12290;&#26412;&#25991;&#20998;&#20139;&#20102;&#26377;&#20851;&#22312;&#24037;&#19994;&#29615;&#22659;&#20013;&#22825;&#30495;&#22320;&#20351;&#29992;AED&#31995;&#32479;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#20197;&#21450;&#22312;&#36825;&#31181;&#29615;&#22659;&#20013;&#36866;&#24403;&#30340;&#30446;&#26631;&#21644;&#31995;&#32479;&#35268;&#26684;&#30340;&#35270;&#35282;&#12290;&#25105;&#20204;&#26681;&#25454;&#36825;&#20123;&#32463;&#39564;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#21453;&#20107;&#23454;&#25512;&#26029;&#30340;AED&#26694;&#26550;&#65292;&#24182;&#22312;&#21830;&#19994;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10870v1 Announce Type: new  Abstract: Adaptive experimental design (AED) methods are increasingly being used in industry as a tool to boost testing throughput or reduce experimentation cost relative to traditional A/B/N testing methods. However, the behavior and guarantees of such methods are not well-understood beyond idealized stationary settings. This paper shares lessons learned regarding the challenges of naively using AED systems in industrial settings where non-stationarity is prevalent, while also providing perspectives on the proper objectives and system specifications in such settings. We developed an AED framework for counterfactual inference based on these experiences, and tested it in a commercial environment.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#20351;&#29992;&#19981;&#21516;&#26631;&#27880;&#20989;&#25968;&#30340;&#21327;&#20316;&#23398;&#20064;&#20013;&#65292;&#22522;&#20110;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#31639;&#27861;&#22312;&#22686;&#24378;&#20551;&#35774;&#31867;&#19978;&#30340;&#39640;&#25928;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.10445</link><description>&lt;p&gt;
&#20351;&#29992;&#19981;&#21516;&#26631;&#27880;&#20989;&#25968;&#30340;&#21327;&#20316;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Collaborative Learning with Different Labeling Functions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10445
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#20351;&#29992;&#19981;&#21516;&#26631;&#27880;&#20989;&#25968;&#30340;&#21327;&#20316;&#23398;&#20064;&#20013;&#65292;&#22522;&#20110;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#31639;&#27861;&#22312;&#22686;&#24378;&#20551;&#35774;&#31867;&#19978;&#30340;&#39640;&#25928;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181; Collaborative PAC Learning &#30340;&#21464;&#20307;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#26088;&#22312;&#23398;&#20064;&#27599;&#20010;$n$&#20010;&#25968;&#25454;&#20998;&#24067;&#30340;&#20934;&#30830;&#20998;&#31867;&#22120;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#20174;&#23427;&#20204;&#24635;&#20849;&#25277;&#21462;&#30340;&#26679;&#26412;&#25968;&#37327;&#12290;&#19982;&#36890;&#24120;&#30340;&#21327;&#20316;&#23398;&#20064;&#35774;&#32622;&#19981;&#21516;&#65292;&#19981;&#20551;&#35774;&#23384;&#22312;&#19968;&#20010;&#21516;&#26102;&#23545;&#25152;&#26377;&#20998;&#24067;&#20934;&#30830;&#30340;&#21333;&#19968;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#24403;&#25968;&#25454;&#20998;&#24067;&#28385;&#36275;&#36739;&#24369;&#30340;&#21487;&#23454;&#29616;&#24615;&#20551;&#35774;&#26102;&#65292;&#20173;&#28982;&#21487;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#23398;&#20064;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;(ERM)&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#24212;&#29992;&#20110;&#20551;&#35774;&#31867;&#30340;&#19968;&#20010;&#33258;&#28982;&#22686;&#24378;&#65292;&#20998;&#26512;&#20381;&#36182;&#20110;&#23545;&#35813;&#22686;&#24378;&#31867;&#30340;VC&#32500;&#30340;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10445v1 Announce Type: new  Abstract: We study a variant of Collaborative PAC Learning, in which we aim to learn an accurate classifier for each of the $n$ data distributions, while minimizing the number of samples drawn from them in total. Unlike in the usual collaborative learning setup, it is not assumed that there exists a single classifier that is simultaneously accurate for all distributions.   We show that, when the data distributions satisfy a weaker realizability assumption, sample-efficient learning is still feasible. We give a learning algorithm based on Empirical Risk Minimization (ERM) on a natural augmentation of the hypothesis class, and the analysis relies on an upper bound on the VC dimension of this augmented class.   In terms of the computational efficiency, we show that ERM on the augmented hypothesis class is NP-hard, which gives evidence against the existence of computationally efficient learners in general. On the positive side, for two special cases, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35774;&#35745;&#22870;&#21169;&#24314;&#27169;&#36807;&#31243;&#20013;&#30340;&#25968;&#25454;&#38598;&#20449;&#24687;&#32467;&#26500;&#65292;&#20174;&#22270;&#35770;&#30340;&#35270;&#35282;&#25552;&#20986;&#20102;RLHF&#20013;&#22870;&#21169;&#27867;&#21270;&#30340;&#38382;&#39064;&#65292;&#20197;&#35299;&#20915;&#22810;&#26679;&#30340;&#29615;&#22659;&#12289;&#20302;&#25104;&#26412;&#26631;&#27880;&#21644;&#21487;&#38752;&#30340;&#23545;&#40784;&#24615;&#33021;&#38388;&#30340;&#19981;&#20860;&#23481;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10184</link><description>&lt;p&gt;
&#37325;&#22609;RLHF&#20013;&#30340;&#20449;&#24687;&#32467;&#26500;&#65306;&#22522;&#20110;&#22270;&#35770;&#30340;&#22870;&#21169;&#27867;&#21270;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Rethinking Information Structures in RLHF: Reward Generalization from a Graph Theory Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10184
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35774;&#35745;&#22870;&#21169;&#24314;&#27169;&#36807;&#31243;&#20013;&#30340;&#25968;&#25454;&#38598;&#20449;&#24687;&#32467;&#26500;&#65292;&#20174;&#22270;&#35770;&#30340;&#35270;&#35282;&#25552;&#20986;&#20102;RLHF&#20013;&#22870;&#21169;&#27867;&#21270;&#30340;&#38382;&#39064;&#65292;&#20197;&#35299;&#20915;&#22810;&#26679;&#30340;&#29615;&#22659;&#12289;&#20302;&#25104;&#26412;&#26631;&#27880;&#21644;&#21487;&#38752;&#30340;&#23545;&#40784;&#24615;&#33021;&#38388;&#30340;&#19981;&#20860;&#23481;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#65288;RLHF&#65289;&#23384;&#22312;&#19968;&#20010;&#19977;&#38590;&#38382;&#39064;&#65306;&#39640;&#24230;&#22810;&#26679;&#30340;&#29615;&#22659;&#12289;&#20302;&#26631;&#27880;&#25104;&#26412;&#21644;&#21487;&#38752;&#30340;&#23545;&#40784;&#24615;&#33021;&#20043;&#38388;&#30340;&#19981;&#20860;&#23481;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#35774;&#35745;&#22870;&#21169;&#24314;&#27169;&#36807;&#31243;&#20013;&#30340;&#25968;&#25454;&#38598;&#20449;&#24687;&#32467;&#26500;&#26469;&#32531;&#35299;&#36825;&#31181;&#19981;&#20860;&#23481;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;RLHF&#36807;&#31243;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#23558;&#20854;&#25551;&#32472;&#20026;&#25991;&#26412;&#20998;&#24067;&#19978;&#30340;&#33258;&#21160;&#32534;&#30721;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#24418;&#24335;&#21270;&#20102;RLHF&#30446;&#26631;&#65292;&#21363;&#30830;&#20445;&#20154;&#31867;&#20559;&#22909;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#34892;&#20026;&#20043;&#38388;&#30340;&#20998;&#24067;&#19968;&#33268;&#24615;&#12290;&#22522;&#20110;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;RLHF&#22870;&#21169;&#24314;&#27169;&#38454;&#27573;&#20013;&#20449;&#24687;&#32467;&#26500;&#30340;&#24615;&#33021;&#24433;&#21709;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#29702;&#35299;&#22870;&#21169;&#24314;&#27169;&#38454;&#27573;&#20013;&#30340;&#22870;&#21169;&#27867;&#21270;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#22270;&#35770;&#30340;&#26041;&#27861;&#26469;&#24314;&#27169;&#35821;&#20041;&#31354;&#38388;&#20013;&#30340;&#27867;&#21270;&#12290;&#20854;&#20013;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10184v1 Announce Type: cross  Abstract: There is a trilemma in reinforcement learning from human feedback (RLHF): the incompatibility between highly diverse contexts, low labeling cost, and reliable alignment performance. Here we aim to mitigate such incompatibility through the design of dataset information structures during reward modeling. Specifically, we first reexamine the RLHF process and propose a theoretical framework portraying it as an autoencoding process over text distributions. Our framework formalizes the RLHF objective of ensuring distributional consistency between human preference and large language model (LLM) behavior. Building on this framework, we then systematically investigate the performance impact of information structure in the reward modeling stage of RLHF. To further understand reward generalization in the reward modeling stage, we introduce a new method based on random graph theory that models generalization in the semantic space. A key insight of
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#25552;&#31034;&#23398;&#20064;&#20013;&#32771;&#34385;&#26377;&#38480;&#39044;&#31639;&#32422;&#26463;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#25552;&#31034;&#23398;&#20064;&#21644;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#22266;&#23450;&#39044;&#31639;&#26368;&#20339;&#33218;&#35782;&#21035;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;TRIPLE&#65292;&#36890;&#36807;&#21033;&#29992;&#32858;&#31867;&#21644;&#23884;&#20837;&#24605;&#24819;&#23454;&#29616;&#20102;&#20004;&#20010;&#22686;&#24378;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.09723</link><description>&lt;p&gt;
&#26377;&#38480;&#39044;&#31639;&#19979;&#30340;&#36805;&#36895;&#23398;&#20064;&#26368;&#20339;&#33218;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Best Arm Identification for Prompt Learning under a Limited Budget
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09723
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#25552;&#31034;&#23398;&#20064;&#20013;&#32771;&#34385;&#26377;&#38480;&#39044;&#31639;&#32422;&#26463;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#25552;&#31034;&#23398;&#20064;&#21644;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#22266;&#23450;&#39044;&#31639;&#26368;&#20339;&#33218;&#35782;&#21035;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;TRIPLE&#65292;&#36890;&#36807;&#21033;&#29992;&#32858;&#31867;&#21644;&#23884;&#20837;&#24605;&#24819;&#23454;&#29616;&#20102;&#20004;&#20010;&#22686;&#24378;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26174;&#33879;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#24341;&#21457;&#20102;&#23545;&#33258;&#21160;&#23398;&#20064;&#21512;&#36866;&#25552;&#31034;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#25552;&#20986;&#20102;&#35768;&#22810;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20294;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#20135;&#29983;&#30340;&#25104;&#26412;&#65288;&#20363;&#22914;&#35775;&#38382;LLM&#21644;&#35780;&#20272;&#21709;&#24212;&#65289;&#23578;&#26410;&#24471;&#21040;&#32771;&#34385;&#12290;&#20026;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#26412;&#24037;&#20316;&#22312;&#25552;&#31034;&#23398;&#20064;&#20013;&#26126;&#30830;&#24341;&#20837;&#20102;&#26377;&#38480;&#39044;&#31639;&#32422;&#26463;&#12290;&#20026;&#20102;&#24320;&#21457;&#26377;&#21407;&#21017;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26412;&#30740;&#31350;&#22312;&#25552;&#31034;&#23398;&#20064;&#21644;&#22810;&#33218;&#36172;&#21338;&#26426;&#30340;&#22266;&#23450;&#39044;&#31639;&#26368;&#20339;&#33218;&#35782;&#21035;&#65288;BAI-FB&#65289;&#20043;&#38388;&#24314;&#31435;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#31995;&#12290;&#22522;&#20110;&#36825;&#31181;&#32852;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;TRIPLE&#65288;&#29992;&#20110;&#25552;&#31034;&#23398;&#20064;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#65289;&#65292;&#20197;&#31995;&#32479;&#22320;&#21033;&#29992;BAI-FB&#22312;&#25552;&#31034;&#23398;&#20064;&#20013;&#30340;&#21147;&#37327;&#12290;&#25552;&#31034;&#23398;&#20064;&#30340;&#29420;&#29305;&#29305;&#28857;&#36827;&#19968;&#27493;&#36890;&#36807;&#21033;&#29992;&#32858;&#31867;&#21644;&#23884;&#20837;&#24605;&#24819;&#25552;&#20986;&#20102;TRIPLE&#30340;&#20004;&#20010;&#22522;&#20110;&#23884;&#20837;&#30340;&#22686;&#24378;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09723v1 Announce Type: cross  Abstract: The remarkable instruction-following capability of large language models (LLMs) has sparked a growing interest in automatically learning suitable prompts. However, while many effective methods have been proposed, the cost incurred during the learning process (e.g., accessing LLM and evaluating the responses) has not been considered. To overcome this limitation, this work explicitly incorporates a finite budget constraint into prompt learning. Towards developing principled solutions, a novel connection is established between prompt learning and fixed-budget best arm identification (BAI-FB) in multi-armed bandits (MAB). Based on this connection, a general framework TRIPLE (besT aRm Identification for Prompt LEarning) is proposed to harness the power of BAI-FB in prompt learning systematically. Unique characteristics of prompt learning further lead to two embedding-based enhancements of TRIPLE by exploiting the ideas of clustering and fun
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#31995;&#32479;&#24615;&#22238;&#39038;&#20840;&#38754;&#20998;&#26512;&#20102;&#25968;&#25454;&#21040;&#25991;&#26412;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30740;&#31350;&#30340;&#29616;&#29366;&#65292;&#25552;&#20986;&#26410;&#26469;&#26041;&#21521;&#65292;&#24182;&#35299;&#20915;&#20102;&#30456;&#20851;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.08496</link><description>&lt;p&gt;
&#25968;&#25454;&#21040;&#25991;&#26412;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30740;&#31350;&#30340;&#31995;&#32479;&#24615;&#22238;&#39038;
&lt;/p&gt;
&lt;p&gt;
A Systematic Review of Data-to-Text NLG
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08496
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#31995;&#32479;&#24615;&#22238;&#39038;&#20840;&#38754;&#20998;&#26512;&#20102;&#25968;&#25454;&#21040;&#25991;&#26412;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30740;&#31350;&#30340;&#29616;&#29366;&#65292;&#25552;&#20986;&#26410;&#26469;&#26041;&#21521;&#65292;&#24182;&#35299;&#20915;&#20102;&#30456;&#20851;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#31995;&#32479;&#24615;&#22238;&#39038;&#26088;&#22312;&#20840;&#38754;&#20998;&#26512;&#25968;&#25454;&#21040;&#25991;&#26412;&#29983;&#25104;&#30740;&#31350;&#30340;&#29616;&#29366;&#65292;&#37325;&#28857;&#26159;&#30830;&#23450;&#30740;&#31350;&#31354;&#30333;&#65292;&#25552;&#20379;&#26410;&#26469;&#26041;&#21521;&#65292;&#24182;&#35299;&#20915;&#22238;&#39038;&#20013;&#21457;&#29616;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#23545;&#25991;&#29486;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#26816;&#26597;&#65292;&#21253;&#25324;&#26041;&#27861;&#12289;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#25351;&#26631;&#12289;&#24212;&#29992;&#12289;&#22810;&#35821;&#35328;&#24615;&#21644;&#24187;&#35273;&#32531;&#35299;&#25514;&#26045;&#12290;&#25105;&#20204;&#30340;&#22238;&#39038;&#20026;&#36825;&#20010;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#30340;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#36335;&#32447;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
This systematic review aims to provide a comprehensive analysis of the state of data-to-text generation research, focusing on identifying research gaps, offering future directions, and addressing challenges found during the review. We thoroughly examined the literature, including approaches, datasets, evaluation metrics, applications, multilingualism, and hallucination mitigation measures. Our review provides a roadmap for future research in this rapidly evolving field.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#35745;&#31639;&#26694;&#26550;&#65292;&#29992;&#20110;&#32454;&#32990;&#37325;&#32534;&#31243;&#20013;&#30340;&#37325;&#32534;&#31243;&#31574;&#30053;&#35782;&#21035;&#12290;&#22312;&#25511;&#21046;&#38382;&#39064;&#20013;&#65292;&#24341;&#20837;&#20102;&#20266;&#21560;&#24341;&#23376;&#30340;&#27010;&#24565;&#21644;&#35782;&#21035;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#29992;&#20110;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#35745;&#31639;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2402.08491</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#32454;&#32990;&#37325;&#32534;&#31243;&#30340;&#24067;&#23572;&#27169;&#22411;&#21560;&#24341;&#23376;&#26223;&#35266;&#20013;&#30340;&#25511;&#21046;&#36941;&#21382;&#20013;&#30340;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning for Controlled Traversing of the Attractor Landscape of Boolean Models in the Context of Cellular Reprogramming
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08491
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#35745;&#31639;&#26694;&#26550;&#65292;&#29992;&#20110;&#32454;&#32990;&#37325;&#32534;&#31243;&#20013;&#30340;&#37325;&#32534;&#31243;&#31574;&#30053;&#35782;&#21035;&#12290;&#22312;&#25511;&#21046;&#38382;&#39064;&#20013;&#65292;&#24341;&#20837;&#20102;&#20266;&#21560;&#24341;&#23376;&#30340;&#27010;&#24565;&#21644;&#35782;&#21035;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#29992;&#20110;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#35745;&#31639;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32454;&#32990;&#37325;&#32534;&#31243;&#21487;&#29992;&#20110;&#39044;&#38450;&#21644;&#27835;&#30103;&#19981;&#21516;&#30142;&#30149;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#20256;&#32479;&#28287;&#23454;&#39564;&#21457;&#29616;&#37325;&#32534;&#31243;&#31574;&#30053;&#30340;&#25928;&#29575;&#21463;&#21040;&#26102;&#38388;&#21644;&#25104;&#26412;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35745;&#31639;&#26694;&#26550;&#65292;&#20197;&#20415;&#24110;&#21161;&#35782;&#21035;&#37325;&#32534;&#31243;&#31574;&#30053;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22312;&#32454;&#32990;&#37325;&#32534;&#31243;&#26694;&#26550;&#30340;BNs&#21644;PBNs&#20197;&#21450;&#24322;&#27493;&#26356;&#26032;&#27169;&#24335;&#19979;&#21046;&#23450;&#20102;&#19968;&#20010;&#25511;&#21046;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20266;&#21560;&#24341;&#23376;&#30340;&#27010;&#24565;&#21644;&#35757;&#32451;&#36807;&#31243;&#20013;&#20266;&#21560;&#24341;&#23376;&#29366;&#24577;&#30340;&#35782;&#21035;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#29992;&#20110;&#35299;&#20915;&#25511;&#21046;&#38382;&#39064;&#30340;&#35745;&#31639;&#26694;&#26550;&#65292;&#24182;&#22312;&#22810;&#20010;&#19981;&#21516;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cellular reprogramming can be used for both the prevention and cure of different diseases. However, the efficiency of discovering reprogramming strategies with classical wet-lab experiments is hindered by lengthy time commitments and high costs. In this study, we develop a~novel computational framework based on deep reinforcement learning that facilitates the identification of reprogramming strategies. For this aim, we formulate a~control problem in the context of cellular reprogramming for the frameworks of BNs and PBNs under the asynchronous update mode. Furthermore, we introduce the notion of a~pseudo-attractor and a~procedure for identification of pseudo-attractor state during training. Finally, we devise a~computational framework for solving the control problem, which we test on a~number of different models.
&lt;/p&gt;</description></item><item><title>ChatCell&#26159;&#19968;&#20010;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#20419;&#36827;&#21333;&#32454;&#32990;&#20998;&#26512;&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#35789;&#27719;&#36866;&#24212;&#21644;&#32479;&#19968;&#24207;&#21015;&#29983;&#25104;&#65292;&#23427;&#20855;&#22791;&#28145;&#21402;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#36866;&#24212;&#21508;&#31181;&#20998;&#26512;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.08303</link><description>&lt;p&gt;
ChatCell: &#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#20419;&#36827;&#21333;&#32454;&#32990;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
ChatCell: Facilitating Single-Cell Analysis with Natural Language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08303
&lt;/p&gt;
&lt;p&gt;
ChatCell&#26159;&#19968;&#20010;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#20419;&#36827;&#21333;&#32454;&#32990;&#20998;&#26512;&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#35789;&#27719;&#36866;&#24212;&#21644;&#32479;&#19968;&#24207;&#21015;&#29983;&#25104;&#65292;&#23427;&#20855;&#22791;&#28145;&#21402;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#36866;&#24212;&#21508;&#31181;&#20998;&#26512;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#23427;&#20204;&#22312;&#31185;&#23398;&#20013;&#30340;&#24433;&#21709;&#26085;&#30410;&#31361;&#20986;&#12290;LLMs&#22312;&#20219;&#21153;&#27867;&#21270;&#21644;&#33258;&#30001;&#23545;&#35805;&#26041;&#38754;&#30340;&#26032;&#20852;&#33021;&#21147;&#21487;&#20197;&#26497;&#22823;&#22320;&#25512;&#36827;&#21270;&#23398;&#21644;&#29983;&#29289;&#23398;&#31561;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#21333;&#32454;&#32990;&#29983;&#29289;&#23398;&#36825;&#20010;&#26500;&#25104;&#29983;&#29289;&#20307;&#22522;&#30784;&#26500;&#20214;&#30340;&#39046;&#22495;&#20173;&#38754;&#20020;&#19968;&#20123;&#25361;&#25112;&#12290;&#24403;&#21069;&#26041;&#27861;&#22312;&#30693;&#35782;&#38376;&#27099;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#65292;&#38459;&#30861;&#20102;LLMs&#22312;&#25484;&#25569;&#21333;&#32454;&#32990;&#25968;&#25454;&#26041;&#38754;&#30340;&#20805;&#20998;&#21033;&#29992;&#65292;&#24433;&#21709;&#20102;&#30452;&#25509;&#21487;&#35775;&#38382;&#21644;&#24555;&#36895;&#36845;&#20195;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;ChatCell&#65292;&#36890;&#36807;&#21033;&#29992;&#35789;&#27719;&#36866;&#24212;&#21644;&#32479;&#19968;&#24207;&#21015;&#29983;&#25104;&#65292;&#23427;&#22312;&#21333;&#32454;&#32990;&#29983;&#29289;&#23398;&#39046;&#22495;&#33719;&#24471;&#20102;&#28145;&#21402;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#36866;&#24212;&#21508;&#31181;&#20998;&#26512;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#26631;&#24535;&#30528;&#19968;&#31181;&#33539;&#24335;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Large Language Models (LLMs) rapidly evolve, their influence in science is becoming increasingly prominent. The emerging capabilities of LLMs in task generalization and free-form dialogue can significantly advance fields like chemistry and biology. However, the field of single-cell biology, which forms the foundational building blocks of living organisms, still faces several challenges. High knowledge barriers and limited scalability in current methods restrict the full exploitation of LLMs in mastering single-cell data, impeding direct accessibility and rapid iteration. To this end, we introduce ChatCell, which signifies a paradigm shift by facilitating single-cell analysis with natural language. Leveraging vocabulary adaptation and unified sequence generation, ChatCell has acquired profound expertise in single-cell biology and the capability to accommodate a diverse range of analysis tasks. Extensive experiments further demonstrate ChatCell's robust performance and potential to de
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#20351;&#29992;&#26080;&#30417;&#30563;&#24230;&#37327;&#27169;&#22359;&#24615;&#20248;&#21270;GNN&#36827;&#34892;&#33410;&#28857;&#32858;&#31867;&#30340;&#26041;&#27861;&#65292;&#19988;&#26080;&#38656;&#19982;&#22522;&#20934;&#20540;&#36827;&#34892;&#27604;&#36739;&#12290;&#22312;&#35774;&#35745;&#21512;&#25104;&#23454;&#39564;&#30340;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.07845</link><description>&lt;p&gt;
&#20351;&#29992;&#26080;&#30417;&#30563;&#24230;&#37327;&#20248;&#21270;GNN&#36827;&#34892;&#33410;&#28857;&#32858;&#31867;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Investigation into Using Unsupervised Metrics to Optimise GNNs for Node Clustering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07845
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#20351;&#29992;&#26080;&#30417;&#30563;&#24230;&#37327;&#27169;&#22359;&#24615;&#20248;&#21270;GNN&#36827;&#34892;&#33410;&#28857;&#32858;&#31867;&#30340;&#26041;&#27861;&#65292;&#19988;&#26080;&#38656;&#19982;&#22522;&#20934;&#20540;&#36827;&#34892;&#27604;&#36739;&#12290;&#22312;&#35774;&#35745;&#21512;&#25104;&#23454;&#39564;&#30340;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#29305;&#24449;&#21644;&#36830;&#25509;&#20449;&#24687;&#30340;&#20108;&#20803;&#24615;&#26469;&#35757;&#32451;&#20197;&#26816;&#27979;&#22270;&#20013;&#30340;&#31038;&#21306;&#12290;&#30446;&#21069;&#65292;&#20248;&#21270;GNN&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#20351;&#29992;&#19982;&#22522;&#20934;&#20540;&#30340;&#27604;&#36739;&#26469;&#36827;&#34892;&#36229;&#21442;&#25968;&#35843;&#25972;&#21644;&#27169;&#22411;&#36873;&#25321;&#12290;&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#20165;&#36890;&#36807;&#20248;&#21270;&#27169;&#22359;&#24615;&#65292;&#21487;&#20197;&#20351;&#29992;GNN&#23558;&#33410;&#28857;&#32858;&#31867;&#25104;&#31038;&#21306;&#65292;&#32780;&#26080;&#38656;&#19982;&#22522;&#20934;&#20540;&#36827;&#34892;&#27604;&#36739;&#12290;&#23613;&#31649;&#27169;&#22359;&#24615;&#26159;&#19968;&#31181;&#22270;&#20998;&#21306;&#36136;&#37327;&#24230;&#37327;&#65292;&#25105;&#20204;&#35777;&#26126;&#36825;&#20063;&#21487;&#20197;&#29992;&#20110;&#20248;&#21270;&#21516;&#26102;&#32534;&#30721;&#29305;&#24449;&#30340;GNN&#65292;&#24182;&#19988;&#19981;&#20250;&#38477;&#20302;&#24615;&#33021;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#26080;&#30417;&#30563;&#24230;&#37327;&#24615;&#33021;&#26159;&#21542;&#33021;&#22815;&#39044;&#27979;&#22522;&#20934;&#20540;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#25506;&#31350;&#20026;&#20160;&#20040;&#21487;&#20197;&#20351;&#29992;&#27169;&#22359;&#24615;&#20248;&#21270;GNN&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20123;&#21512;&#25104;&#23454;&#39564;&#26469;&#23637;&#31034;&#36825;&#31181;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#36825;&#20123;&#21512;&#25104;&#22270;&#34920;&#26126;&#20854;&#22312;&#19981;&#21516;&#12289;&#38543;&#26426;&#21644;&#38646;&#20449;&#24687;&#31354;&#38388;&#20998;&#21306;&#20013;&#30340;&#24403;&#21069;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) can be trained to detect communities within a graph by learning from the duality of feature and connectivity information. Currently, the common approach for optimisation of GNNs is to use comparisons to ground-truth for hyperparameter tuning and model selection. In this work, we show that nodes can be clustered into communities with GNNs by solely optimising for modularity, without any comparison to ground-truth. Although modularity is a graph partitioning quality metric, we show that this can be used to optimise GNNs that also encode features without a drop in performance. We take it a step further and also study whether the unsupervised metric performance can predict ground-truth performance. To investigate why modularity can be used to optimise GNNs, we design synthetic experiments that show the limitations of this approach. The synthetic graphs are created to highlight current capabilities in distinct, random and zero information space partitions in att
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Pathformer&#30340;&#22810;&#23610;&#24230;&#33258;&#36866;&#24212;&#36335;&#24452;&#30340;Transformer&#27169;&#22411;&#65292;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#36890;&#36807;&#25972;&#21512;&#26102;&#38388;&#20998;&#36776;&#29575;&#21644;&#26102;&#38388;&#36317;&#31163;&#36827;&#34892;&#22810;&#23610;&#24230;&#24314;&#27169;&#65292;&#24182;&#20351;&#29992;&#33258;&#36866;&#24212;&#36335;&#24452;&#26469;&#20248;&#21270;&#24314;&#27169;&#36807;&#31243;&#65292;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.05956</link><description>&lt;p&gt;
Pathformer: &#22810;&#23610;&#24230;&#33258;&#36866;&#24212;&#36335;&#24452;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Pathformer: Multi-scale transformers with Adaptive Pathways for Time Series Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05956
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Pathformer&#30340;&#22810;&#23610;&#24230;&#33258;&#36866;&#24212;&#36335;&#24452;&#30340;Transformer&#27169;&#22411;&#65292;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#36890;&#36807;&#25972;&#21512;&#26102;&#38388;&#20998;&#36776;&#29575;&#21644;&#26102;&#38388;&#36317;&#31163;&#36827;&#34892;&#22810;&#23610;&#24230;&#24314;&#27169;&#65292;&#24182;&#20351;&#29992;&#33258;&#36866;&#24212;&#36335;&#24452;&#26469;&#20248;&#21270;&#24314;&#27169;&#36807;&#31243;&#65292;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#21462;&#24471;&#20102;&#19968;&#20123;&#25104;&#21151;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#20027;&#35201;&#20174;&#26377;&#38480;&#25110;&#22266;&#23450;&#23610;&#24230;&#23545;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#24314;&#27169;&#65292;&#36825;&#20351;&#24471;&#25429;&#25417;&#36328;&#22810;&#20010;&#23610;&#24230;&#30340;&#19981;&#21516;&#29305;&#24449;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#33258;&#36866;&#24212;&#36335;&#24452;&#65288;Pathformer&#65289;&#30340;Transformer&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#21516;&#26102;&#25972;&#21512;&#20102;&#26102;&#38388;&#20998;&#36776;&#29575;&#21644;&#26102;&#38388;&#36317;&#31163;&#36827;&#34892;&#22810;&#23610;&#24230;&#24314;&#27169;&#12290;&#22810;&#23610;&#24230;&#21010;&#20998;&#36816;&#29992;&#19981;&#21516;&#22823;&#23567;&#30340;&#25968;&#25454;&#22359;&#23558;&#26102;&#38388;&#24207;&#21015;&#20998;&#21106;&#25104;&#19981;&#21516;&#30340;&#26102;&#38388;&#20998;&#36776;&#29575;&#12290;&#22522;&#20110;&#27599;&#20010;&#23610;&#24230;&#30340;&#21010;&#20998;&#65292;&#23545;&#36825;&#20123;&#25968;&#25454;&#22359;&#36827;&#34892;&#21452;&#37325;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#20197;&#25429;&#25417;&#20840;&#23616;&#30456;&#20851;&#24615;&#21644;&#23616;&#37096;&#32454;&#33410;&#20316;&#20026;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#33258;&#36866;&#24212;&#36335;&#24452;&#26469;&#20016;&#23500;&#22810;&#23610;&#24230;Transformer&#65292;&#35813;&#36335;&#24452;&#21487;&#20197;&#26681;&#25454;&#36755;&#20837;&#26102;&#38388;&#24207;&#21015;&#20013;&#19981;&#26029;&#21464;&#21270;&#30340;&#26102;&#38388;&#21160;&#24577;&#35843;&#25972;&#22810;&#23610;&#24230;&#24314;&#27169;&#36807;&#31243;&#65292;&#25552;&#39640;Pathformer&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;11&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based models have achieved some success in time series forecasting. Existing methods mainly model time series from limited or fixed scales, making it challenging to capture different characteristics spanning various scales. In this paper, we propose multi-scale transformers with adaptive pathways (Pathformer). The proposed Transformer integrates both temporal resolution and temporal distance for multi-scale modeling. Multi-scale division divides the time series into different temporal resolutions using patches of various sizes. Based on the division of each scale, dual attention is performed over these patches to capture global correlations and local details as temporal dependencies. We further enrich the multi-scale transformer with adaptive pathways, which adaptively adjust the multi-scale modeling process based on the varying temporal dynamics in the input time series, improving the prediction accuracy and generalization of Pathformer. Extensive experiments on eleven rea
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedMeZO&#30340;&#26041;&#27861;&#65292;&#20197;&#22312;&#38646;&#38454;&#32852;&#37030;&#23398;&#20064;&#21644;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#23454;&#29616;&#20869;&#23384;&#39640;&#25928;&#30340;&#35843;&#25972;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21644;&#23454;&#35777;&#35777;&#25454;&#34920;&#26126;FedMeZO&#19981;&#20165;&#25910;&#25947;&#36895;&#24230;&#24555;&#20110;&#20256;&#32479;&#30340;&#19968;&#38454;&#26041;&#27861;&#65292;&#32780;&#19988;&#22312;&#20010;&#24615;&#21270;&#30340;&#32852;&#37030;&#31574;&#30053;&#20013;&#20855;&#26377;&#20851;&#38190;&#30340;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.05926</link><description>&lt;p&gt;
&#20851;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#38646;&#38454;&#32852;&#37030;&#35843;&#25972;&#30340;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Convergence of Zeroth-Order Federated Tuning in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05926
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedMeZO&#30340;&#26041;&#27861;&#65292;&#20197;&#22312;&#38646;&#38454;&#32852;&#37030;&#23398;&#20064;&#21644;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#23454;&#29616;&#20869;&#23384;&#39640;&#25928;&#30340;&#35843;&#25972;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21644;&#23454;&#35777;&#35777;&#25454;&#34920;&#26126;FedMeZO&#19981;&#20165;&#25910;&#25947;&#36895;&#24230;&#24555;&#20110;&#20256;&#32479;&#30340;&#19968;&#38454;&#26041;&#27861;&#65292;&#32780;&#19988;&#22312;&#20010;&#24615;&#21270;&#30340;&#32852;&#37030;&#31574;&#30053;&#20013;&#20855;&#26377;&#20851;&#38190;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#21644;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#34701;&#21512;&#20026;&#38544;&#31169;&#20445;&#25252;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24102;&#26469;&#20102;&#26032;&#26102;&#20195;&#12290;&#28982;&#32780;&#65292;&#31934;&#35843;LLM&#25152;&#38656;&#30340;&#24378;&#22823;&#20869;&#23384;&#35201;&#27714;&#22312;&#37096;&#32626;&#21040;&#36793;&#32536;&#35774;&#22791;&#26102;&#20250;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#36825;&#20123;&#35774;&#22791;&#30340;&#35745;&#31639;&#36164;&#28304;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#32852;&#37030;&#29615;&#22659;&#20013;&#25506;&#32034;&#20102;&#20869;&#23384;&#39640;&#25928;&#30340;&#38646;&#38454;&#20248;&#21270;&#30340;&#20840;&#26032;&#25972;&#21512;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;FedMeZO&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26159;&#31532;&#19968;&#20010;&#22312;LLM&#32972;&#26223;&#19979;&#32771;&#23519;FedMeZO&#30340;&#29702;&#35770;&#22522;&#30784;&#30340;&#30740;&#31350;&#65292;&#28041;&#21450;&#21040;&#22823;&#21442;&#25968;&#31354;&#38388;&#23545;&#20248;&#21270;&#34892;&#20026;&#30340;&#24433;&#21709;&#12289;&#25910;&#25947;&#24615;&#30340;&#24314;&#31435;&#20197;&#21450;&#20026;&#20010;&#24615;&#21270;&#30340;&#32852;&#37030;&#31574;&#30053;&#30830;&#23450;&#20851;&#38190;&#21442;&#25968;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#35777;&#35777;&#25454;&#25903;&#25345;&#20102;&#36825;&#20010;&#29702;&#35770;&#65292;&#34920;&#26126;FedMeZO&#19981;&#20165;&#27604;&#20256;&#32479;&#30340;&#19968;&#38454;&#26041;&#27861;&#65288;&#22914;SGD&#65289;&#25910;&#25947;&#26356;&#24555;&#65292;&#32780;&#19988;&#26126;&#26174;...
&lt;/p&gt;
&lt;p&gt;
The confluence of Federated Learning (FL) and Large Language Models (LLMs) is ushering in a new era in privacy-preserving natural language processing. However, the intensive memory requirements for fine-tuning LLMs pose significant challenges, especially when deploying on edge devices with limited computational resources. To circumvent this, we explore the novel integration of Memory-efficient Zeroth-Order Optimization within a federated setting, a synergy we denote as FedMeZO. Our study is the first to examine the theoretical underpinnings of FedMeZO in the context of LLMs, tackling key questions regarding the influence of large parameter spaces on optimization behavior, the establishment of convergence properties, and the identification of critical parameters for convergence to inform personalized federated strategies. Our extensive empirical evidence supports the theory, showing that FedMeZO not only converges faster than traditional first-order methods such as SGD but also signific
&lt;/p&gt;</description></item><item><title>LESS&#26159;&#19968;&#31181;&#20248;&#21270;&#24863;&#30693;&#19988;&#23454;&#38469;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36873;&#25321;&#20855;&#26377;&#24433;&#21709;&#21147;&#30340;&#25968;&#25454;&#20197;&#24320;&#21457;&#29305;&#23450;&#33021;&#21147;&#65292;&#23427;&#37319;&#29992;&#20302;&#31209;&#26799;&#24230;&#30456;&#20284;&#24615;&#25628;&#32034;&#26041;&#27861;&#36827;&#34892;&#25351;&#20196;&#25968;&#25454;&#36873;&#25321;&#12290;</title><link>https://arxiv.org/abs/2402.04333</link><description>&lt;p&gt;
LESS&#65306;&#29992;&#20110;&#30446;&#26631;&#25351;&#23548;&#35843;&#25972;&#30340;&#36873;&#25321;&#26377;&#24433;&#21709;&#21147;&#30340;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
LESS: Selecting Influential Data for Targeted Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04333
&lt;/p&gt;
&lt;p&gt;
LESS&#26159;&#19968;&#31181;&#20248;&#21270;&#24863;&#30693;&#19988;&#23454;&#38469;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36873;&#25321;&#20855;&#26377;&#24433;&#21709;&#21147;&#30340;&#25968;&#25454;&#20197;&#24320;&#21457;&#29305;&#23450;&#33021;&#21147;&#65292;&#23427;&#37319;&#29992;&#20302;&#31209;&#26799;&#24230;&#30456;&#20284;&#24615;&#25628;&#32034;&#26041;&#27861;&#36827;&#34892;&#25351;&#20196;&#25968;&#25454;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#24050;&#32463;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#37322;&#25918;&#20986;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#26377;&#25928;&#22320;&#20351;&#29992;&#32452;&#21512;&#25968;&#25454;&#38598;&#26469;&#24320;&#21457;&#36890;&#29992;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#24212;&#29992;&#24448;&#24448;&#38656;&#35201;&#19968;&#22871;&#19987;&#38376;&#30340;&#25216;&#33021;&#65288;&#20363;&#22914;&#25512;&#29702;&#65289;&#12290;&#25361;&#25112;&#22312;&#20110;&#20174;&#36825;&#20123;&#24191;&#27867;&#30340;&#25968;&#25454;&#38598;&#20013;&#35782;&#21035;&#20986;&#26368;&#30456;&#20851;&#30340;&#25968;&#25454;&#65292;&#20197;&#26377;&#25928;&#24320;&#21457;&#29305;&#23450;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#23558;&#36825;&#31181;&#24773;&#20917;&#31216;&#20026;&#30446;&#26631;&#25351;&#23548;&#35843;&#25972;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LESS&#65292;&#19968;&#31181;&#20248;&#21270;&#24863;&#30693;&#19988;&#23454;&#38469;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#20197;&#26377;&#25928;&#20272;&#35745;&#25968;&#25454;&#24433;&#21709;&#24182;&#25191;&#34892;&#36866;&#29992;&#20110;&#25351;&#20196;&#25968;&#25454;&#36873;&#25321;&#30340;&#20302;&#31209;&#26799;&#24230;&#30456;&#20284;&#24615;&#25628;&#32034;&#12290;&#20851;&#38190;&#22312;&#20110;LESS&#23558;&#29616;&#26377;&#30340;&#24433;&#21709;&#20844;&#24335;&#35843;&#25972;&#20026;&#19982;Adam&#20248;&#21270;&#22120;&#21644;&#21487;&#21464;&#38271;&#24230;&#25351;&#20196;&#25968;&#25454;&#19968;&#36215;&#24037;&#20316;&#12290;LESS&#39318;&#20808;&#26500;&#24314;&#20102;&#19968;&#20010;&#20855;&#26377;&#20302;&#32500;&#26799;&#24230;&#29305;&#24449;&#30340;&#39640;&#24230;&#21487;&#37325;&#29992;&#21644;&#21487;&#20256;&#36882;&#30340;&#26799;&#24230;&#25968;&#25454;&#23384;&#20648;&#24211;&#65292;&#28982;&#21518;&#26681;&#25454;&#23427;&#20204;&#19982;&#20855;&#26377;&#29305;&#23450;&#33021;&#21147;&#30340;&#23569;&#26679;&#26412;&#31034;&#20363;&#30340;&#30456;&#20284;&#24230;&#36873;&#25321;&#31034;&#20363;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;t
&lt;/p&gt;
&lt;p&gt;
Instruction tuning has unlocked powerful capabilities in large language models (LLMs), effectively using combined datasets to develop generalpurpose chatbots. However, real-world applications often require a specialized suite of skills (e.g., reasoning). The challenge lies in identifying the most relevant data from these extensive datasets to effectively develop specific capabilities, a setting we frame as targeted instruction tuning. We propose LESS, an optimizer-aware and practically efficient algorithm to effectively estimate data influences and perform Low-rank gradiEnt Similarity Search for instruction data selection. Crucially, LESS adapts existing influence formulations to work with the Adam optimizer and variable-length instruction data. LESS first constructs a highly reusable and transferable gradient datastore with low-dimensional gradient features and then selects examples based on their similarity to few-shot examples embodying a specific capability. Experiments show that t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#24187;&#35273;&#26816;&#27979;&#26694;&#26550;UNIHD&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#35780;&#20272;&#22522;&#20934;&#26041;&#27861;MHaluBench&#26469;&#35780;&#20272;&#24187;&#35273;&#26816;&#27979;&#26041;&#27861;&#30340;&#36827;&#23637;&#12290;&#36825;&#39033;&#24037;&#20316;&#25193;&#23637;&#20102;&#24187;&#35273;&#26816;&#27979;&#30340;&#30740;&#31350;&#33539;&#22260;&#24182;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.03190</link><description>&lt;p&gt;
&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24187;&#35273;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Unified Hallucination Detection for Multimodal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03190
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#24187;&#35273;&#26816;&#27979;&#26694;&#26550;UNIHD&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#35780;&#20272;&#22522;&#20934;&#26041;&#27861;MHaluBench&#26469;&#35780;&#20272;&#24187;&#35273;&#26816;&#27979;&#26041;&#27861;&#30340;&#36827;&#23637;&#12290;&#36825;&#39033;&#24037;&#20316;&#25193;&#23637;&#20102;&#24187;&#35273;&#26816;&#27979;&#30340;&#30740;&#31350;&#33539;&#22260;&#24182;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#22810;&#27169;&#24577;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(MLLMs)&#20173;&#28982;&#23384;&#22312;&#24187;&#35273;&#30340;&#20005;&#37325;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#21487;&#38752;&#22320;&#26816;&#27979;MLLMs&#20013;&#30340;&#24187;&#35273;&#24050;&#25104;&#20026;&#27169;&#22411;&#35780;&#20272;&#21644;&#23454;&#38469;&#24212;&#29992;&#37096;&#32626;&#20445;&#38556;&#30340;&#37325;&#35201;&#26041;&#38754;&#12290;&#20043;&#21069;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#21463;&#21040;&#20102;&#29421;&#31364;&#30340;&#20219;&#21153;&#28966;&#28857;&#12289;&#19981;&#36275;&#30340;&#24187;&#35273;&#31867;&#21035;&#28085;&#30422;&#33539;&#22260;&#20197;&#21450;&#32570;&#20047;&#35814;&#32454;&#30340;&#32454;&#31890;&#24230;&#30340;&#38480;&#21046;&#12290;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#25193;&#23637;&#20102;&#24187;&#35273;&#26816;&#27979;&#30340;&#30740;&#31350;&#33539;&#22260;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20803;&#35780;&#20272;&#22522;&#20934;&#26041;&#27861;&#65292;MHaluBench&#65292;&#31934;&#24515;&#35774;&#35745;&#20197;&#20419;&#36827;&#24187;&#35273;&#26816;&#27979;&#26041;&#27861;&#30340;&#36827;&#23637;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32479;&#19968;&#22810;&#27169;&#24577;&#24187;&#35273;&#26816;&#27979;&#26694;&#26550;&#65292;UNIHD&#65292;&#23427;&#21033;&#29992;&#19968;&#22871;&#36741;&#21161;&#24037;&#20855;&#26469;&#31283;&#20581;&#22320;&#39564;&#35777;&#24187;&#35273;&#30340;&#21457;&#29983;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;UNIHD&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite significant strides in multimodal tasks, Multimodal Large Language Models (MLLMs) are plagued by the critical issue of hallucination. The reliable detection of such hallucinations in MLLMs has, therefore, become a vital aspect of model evaluation and the safeguarding of practical application deployment. Prior research in this domain has been constrained by a narrow focus on singular tasks, an inadequate range of hallucination categories addressed, and a lack of detailed granularity. In response to these challenges, our work expands the investigative horizons of hallucination detection. We present a novel meta-evaluation benchmark, MHaluBench, meticulously crafted to facilitate the evaluation of advancements in hallucination detection methods. Additionally, we unveil a novel unified multimodal hallucination detection framework, UNIHD, which leverages a suite of auxiliary tools to validate the occurrence of hallucinations robustly. We demonstrate the effectiveness of UNIHD throug
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21464;&#20998;&#27969;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#36716;&#25442;&#26041;&#27861;&#65292;&#20351;&#24471;&#24555;&#36895;&#37319;&#26679;&#25104;&#20026;&#21487;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#37319;&#26679;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.02977</link><description>&lt;p&gt;
&#21464;&#20998;&#27969;&#27169;&#22411;&#65306;&#20197;&#20320;&#30340;&#39118;&#26684;&#27969;&#21160;
&lt;/p&gt;
&lt;p&gt;
Variational Flow Models: Flowing in Your Style
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02977
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21464;&#20998;&#27969;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#36716;&#25442;&#26041;&#27861;&#65292;&#20351;&#24471;&#24555;&#36895;&#37319;&#26679;&#25104;&#20026;&#21487;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#37319;&#26679;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#23545;"&#21518;&#39564;&#27969;"&#27169;&#22411;&#36827;&#34892;&#21464;&#20998;&#25512;&#29702;&#35299;&#37322;&#30340;&#26041;&#27861;&#8212;&#8212;&#29992;&#20197;&#23558;"&#27010;&#29575;&#27969;"&#25512;&#24191;&#21040;&#26356;&#24191;&#27867;&#30340;&#38543;&#26426;&#36807;&#31243;&#31867;&#21035;&#65292;&#19981;&#24517;&#23616;&#38480;&#20110;&#25193;&#25955;&#36807;&#31243;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#32467;&#26524;&#31216;&#20026;"&#21464;&#20998;&#27969;&#27169;&#22411;"&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#31995;&#32479;&#26041;&#27861;&#65292;&#23558;&#30001;&#26041;&#31243;Xt = at * X0 + st * X1&#25152;&#25551;&#36848;&#30340;"&#32447;&#24615;"&#38543;&#26426;&#36807;&#31243;&#30340;&#21518;&#39564;&#27969;&#36716;&#21270;&#20026;&#30452;&#32447;&#24658;&#36895;(SC)&#27969;&#65292;&#31867;&#20284;&#20110;&#30699;&#27491;&#27969;&#12290;&#36825;&#31181;&#36716;&#21270;&#20351;&#24471;&#21487;&#20197;&#24555;&#36895;&#27839;&#30528;&#21407;&#22987;&#30340;&#21518;&#39564;&#27969;&#36827;&#34892;&#37319;&#26679;&#65292;&#32780;&#26080;&#38656;&#35757;&#32451;&#19968;&#20010;&#26032;&#30340;SC&#27969;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#28789;&#27963;&#24615;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#36716;&#25442;&#25193;&#23637;&#21040;&#20004;&#20010;&#19981;&#21516;"&#32447;&#24615;"&#38543;&#26426;&#36807;&#31243;&#30340;&#21518;&#39564;&#27969;&#20043;&#38388;&#36827;&#34892;&#20114;&#30456;&#36716;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21487;&#20197;&#23558;&#39640;&#38454;&#25968;&#20540;&#35299;&#27861;&#36731;&#26494;&#38598;&#25104;&#21040;&#36716;&#25442;&#21518;&#30340;SC&#27969;&#20013;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#37319;&#26679;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#22823;&#37327;&#23454;&#39564;&#32467;&#26524;&#30340;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a variational inference interpretation for models of "posterior flows" - generalizations of "probability flows" to a broader class of stochastic processes not necessarily diffusion processes. We coin the resulting models as "Variational Flow Models". Additionally, we propose a systematic training-free method to transform the posterior flow of a "linear" stochastic process characterized by the equation Xt = at * X0 + st * X1 into a straight constant-speed (SC) flow, reminiscent of Rectified Flow. This transformation facilitates fast sampling along the original posterior flow without training a new model of the SC flow. The flexibility of our approach allows us to extend our transformation to inter-convert two posterior flows from distinct "linear" stochastic processes. Moreover, we can easily integrate high-order numerical solvers into the transformed SC flow, further enhancing sampling accuracy and efficiency. Rigorous theoretical analysis and extensive experimental result
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#20248;&#21270;&#25581;&#31034;&#32852;&#30431;&#20215;&#20540;&#30340;&#39034;&#24207;&#26469;&#20943;&#23569;&#19981;&#23436;&#20840;&#21512;&#20316;&#21338;&#24328;&#20013;&#30340;&#20048;&#35266;&#20559;&#35823;&#12290;</title><link>https://arxiv.org/abs/2402.01930</link><description>&lt;p&gt;
&#20943;&#23569;&#19981;&#23436;&#20840;&#21512;&#20316;&#21338;&#24328;&#20013;&#30340;&#20048;&#35266;&#20559;&#35823;
&lt;/p&gt;
&lt;p&gt;
Reducing Optimism Bias in Incomplete Cooperative Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01930
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#20248;&#21270;&#25581;&#31034;&#32852;&#30431;&#20215;&#20540;&#30340;&#39034;&#24207;&#26469;&#20943;&#23569;&#19981;&#23436;&#20840;&#21512;&#20316;&#21338;&#24328;&#20013;&#30340;&#20048;&#35266;&#20559;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#20316;&#21338;&#24328;&#29702;&#35770;&#22312;&#24403;&#20195;&#20154;&#24037;&#26234;&#33021;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#35299;&#37322;&#24615;&#26426;&#22120;&#23398;&#20064;&#12289;&#36164;&#28304;&#20998;&#37197;&#21644;&#21327;&#21516;&#20915;&#31574;&#31561;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#25351;&#23450;&#19968;&#20010;&#21512;&#20316;&#21338;&#24328;&#38656;&#35201;&#20026;&#25351;&#25968;&#22810;&#20010;&#32852;&#30431;&#20998;&#37197;&#20215;&#20540;&#65292;&#24182;&#19988;&#22312;&#23454;&#36341;&#20013;&#33719;&#24471;&#19968;&#20010;&#32852;&#30431;&#20215;&#20540;&#21487;&#33021;&#20250;&#28040;&#32791;&#22823;&#37327;&#36164;&#28304;&#12290;&#28982;&#32780;&#65292;&#31616;&#21333;&#22320;&#19981;&#20844;&#24320;&#26576;&#20123;&#32852;&#30431;&#30340;&#20215;&#20540;&#20250;&#24341;&#20837;&#20851;&#20110;&#20010;&#20307;&#23545;&#38598;&#20307;&#22823;&#32852;&#30431;&#30340;&#36129;&#29486;&#30340;&#27169;&#31946;&#24615;&#12290;&#36825;&#31181;&#27169;&#31946;&#24615;&#32463;&#24120;&#23548;&#33268;&#29609;&#23478;&#25345;&#26377;&#36807;&#20110;&#20048;&#35266;&#30340;&#26399;&#26395;&#65292;&#20854;&#28304;&#20110;&#20869;&#22312;&#20559;&#35265;&#25110;&#25112;&#30053;&#32771;&#34385;&#65292;&#36827;&#32780;&#24120;&#24120;&#23548;&#33268;&#38598;&#20307;&#35201;&#27714;&#36229;&#36807;&#23454;&#38469;&#30340;&#22823;&#32852;&#30431;&#20215;&#20540;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#26088;&#22312;&#20248;&#21270;&#25581;&#31034;&#32852;&#30431;&#20215;&#20540;&#30340;&#39034;&#24207;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#22320;&#32553;&#23567;&#21512;&#20316;&#21338;&#24328;&#20013;&#29609;&#23478;&#26399;&#26395;&#19982;&#21487;&#23454;&#29616;&#32467;&#26524;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cooperative game theory has diverse applications in contemporary artificial intelligence, including domains like interpretable machine learning, resource allocation, and collaborative decision-making. However, specifying a cooperative game entails assigning values to exponentially many coalitions, and obtaining even a single value can be resource-intensive in practice. Yet simply leaving certain coalition values undisclosed introduces ambiguity regarding individual contributions to the collective grand coalition. This ambiguity often leads to players holding overly optimistic expectations, stemming from either inherent biases or strategic considerations, frequently resulting in collective claims exceeding the actual grand coalition value. In this paper, we present a framework aimed at optimizing the sequence for revealing coalition values, with the overarching goal of efficiently closing the gap between players' expectations and achievable outcomes in cooperative games. Our contributio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#21160;&#21270;&#24037;&#20855;PRewrite&#65292;&#29992;&#20110;&#37325;&#20889;&#25552;&#31034;&#33609;&#26696;&#24182;&#29983;&#25104;&#39640;&#25928;&#30340;&#26032;&#25552;&#31034;&#65292;&#20197;&#35299;&#20915;&#25552;&#31034;&#24037;&#31243;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2401.08189</link><description>&lt;p&gt;
PRewrite: &#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#25552;&#31034;&#37325;&#20889;
&lt;/p&gt;
&lt;p&gt;
PRewrite: Prompt Rewriting with Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.08189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#21160;&#21270;&#24037;&#20855;PRewrite&#65292;&#29992;&#20110;&#37325;&#20889;&#25552;&#31034;&#33609;&#26696;&#24182;&#29983;&#25104;&#39640;&#25928;&#30340;&#26032;&#25552;&#31034;&#65292;&#20197;&#35299;&#20915;&#25552;&#31034;&#24037;&#31243;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2401.08189v2 &#20844;&#21578;&#31867;&#22411;: &#26367;&#25442; &#25688;&#35201;: &#25552;&#31034;&#24037;&#31243;&#23545;&#20110;&#22522;&#20110;LLM&#30340;&#24212;&#29992;&#31243;&#24207;&#30340;&#24320;&#21457;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#36890;&#24120;&#20197;&#8220;&#35797;&#38169;&#8221;&#30340;&#26041;&#24335;&#25163;&#21160;&#23436;&#25104;&#12290;&#36825;&#31181;&#25163;&#21160;&#31243;&#24207;&#21487;&#33021;&#32791;&#26102;&#65292;&#25928;&#26524;&#19981;&#20339;&#65292;&#24182;&#19988;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#29983;&#25104;&#30340;&#25552;&#31034;&#37117;&#26159;&#27425;&#20248;&#30340;&#12290;&#21363;&#20351;&#23545;&#37027;&#20123;&#30475;&#20284;&#36816;&#20316;&#33391;&#22909;&#30340;&#25552;&#31034;&#65292;&#22987;&#32456;&#23384;&#22312;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#65306;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#36827;&#19968;&#27493;&#20462;&#25913;&#20351;&#25552;&#31034;&#21464;&#24471;&#26356;&#22909;&#21602;&#65311;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#25552;&#31034;&#24037;&#31243;&#33258;&#21160;&#21270;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#29305;&#23450;&#30340;&#20351;&#29992;&#24773;&#26223;&#65292;&#21363;&#24320;&#21457;&#32773;/&#29992;&#25143;&#24050;&#32463;&#36215;&#33609;&#20102;&#21021;&#22987;&#25552;&#31034;&#65292;&#20294;&#32570;&#20047;&#26102;&#38388;/&#19987;&#19994;&#30693;&#35782;&#26469;&#20248;&#21270;&#23427;&#20204;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;PRewrite&#65292;&#19968;&#20010;&#33258;&#21160;&#21270;&#24037;&#20855;&#65292;&#21487;&#37325;&#20889;&#36825;&#20123;&#33609;&#26696;&#65292;&#24182;&#29983;&#25104;&#39640;&#25928;&#30340;&#26032;&#25552;&#31034;&#12290;PRewrite&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26694;&#26550;&#65292;&#20801;&#35768;&#31471;&#21040;&#31471;&#20248;&#21270;&#65292;&#25105;&#20204;&#30340;&#35774;&#35745;&#20801;&#35768;RL&#25628;&#32034;&#22312;&#22823;&#21160;&#20316;&#31354;&#38388;&#20013;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.08189v2 Announce Type: replace  Abstract: Prompt engineering is critical for the development of LLM-based applications. However, it is usually done manually in a "trial and error" fashion. This manual procedure can be time consuming, ineffective, and the generated prompts are, in a lot of cases, sub-optimal. Even for the prompts which seemingly work well, there is always a lingering question: can the prompts be made better with further modifications?   To address these questions, in this paper, we investigate prompt engineering automation. We consider a specific use case scenario in which developers/users have drafted initial prompts, but lack the time/expertise to optimize them. We propose PRewrite, an automated tool to rewrite these drafts and to generate highly effective new prompts. PRewrite is based on the Reinforcement Learning (RL) framework which allows for end-to-end optimization and our design allows the RL search to happen in a large action space. The automated to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#29702;&#35770;&#65292;&#29992;&#20110;&#23545;&#20960;&#20046;&#25152;&#26377;&#24120;&#35265;&#21644;&#29616;&#23454;&#35774;&#32622;&#19979;&#30340;&#26680;&#22238;&#24402;&#30340;&#36229;&#20986;&#39118;&#38505;&#36827;&#34892;&#19978;&#38480;&#32422;&#26463;&#65292;&#24182;&#25581;&#31034;&#20102;&#26680;&#20998;&#35299;&#20013;&#23384;&#22312;&#30340;&#33258;&#25105;&#27491;&#21017;&#21270;&#29616;&#35937;&#12290;</title><link>https://arxiv.org/abs/2312.15995</link><description>&lt;p&gt;
&#22312;&#29616;&#23454;&#20551;&#35774;&#19979;&#30340;&#26680;&#22238;&#24402;&#20013;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Generalization in Kernel Regression Under Realistic Assumptions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.15995
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#29702;&#35770;&#65292;&#29992;&#20110;&#23545;&#20960;&#20046;&#25152;&#26377;&#24120;&#35265;&#21644;&#29616;&#23454;&#35774;&#32622;&#19979;&#30340;&#26680;&#22238;&#24402;&#30340;&#36229;&#20986;&#39118;&#38505;&#36827;&#34892;&#19978;&#38480;&#32422;&#26463;&#65292;&#24182;&#25581;&#31034;&#20102;&#26680;&#20998;&#35299;&#20013;&#23384;&#22312;&#30340;&#33258;&#25105;&#27491;&#21017;&#21270;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#22312;&#24050;&#32463;&#30830;&#31435;&#30340;&#20107;&#23454;&#26159;&#65292;&#29616;&#20195;&#36807;&#24230;&#21442;&#25968;&#21270;&#27169;&#22411;&#20284;&#20046;&#33021;&#22815;&#36867;&#36991;&#20559;&#24046;-&#26041;&#24046;&#26435;&#34913;&#65292;&#22312;&#36807;&#24230;&#25311;&#21512;&#22122;&#38899;&#30340;&#24773;&#20917;&#19979;&#27867;&#21270;&#33391;&#22909;&#12290;&#35768;&#22810;&#26368;&#36817;&#30340;&#30740;&#31350;&#23581;&#35797;&#20998;&#26512;&#36825;&#19968;&#29616;&#35937;&#22312;&#26680;&#22238;&#24402;&#30456;&#23545;&#26131;&#22788;&#29702;&#30340;&#35774;&#32622;&#20013;&#12290;&#28982;&#32780;&#65292;&#27491;&#22914;&#25105;&#20204;&#35814;&#32454;&#35752;&#35770;&#30340;&#37027;&#26679;&#65292;&#22823;&#22810;&#25968;&#20851;&#20110;&#36825;&#20010;&#20027;&#39064;&#30340;&#36807;&#21435;&#30340;&#30740;&#31350;&#35201;&#20040;&#20570;&#20986;&#20102;&#19981;&#20999;&#23454;&#38469;&#30340;&#20551;&#35774;&#65292;&#35201;&#20040;&#19987;&#27880;&#20110;&#19968;&#20010;&#29421;&#31364;&#30340;&#38382;&#39064;&#35774;&#32622;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#32479;&#19968;&#30340;&#29702;&#35770;&#26469;&#38480;&#21046;&#20960;&#20046;&#25152;&#26377;&#24120;&#35265;&#21644;&#29616;&#23454;&#35774;&#32622;&#19979;&#26680;&#22238;&#24402;&#30340;&#36229;&#20986;&#39118;&#38505;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#20110;&#24120;&#35265;&#26680;&#20989;&#25968;&#20197;&#21450;&#20219;&#24847;&#30340;&#27491;&#21017;&#21270;&#37327;&#12289;&#22122;&#22768;&#12289;&#20219;&#24847;&#36755;&#20837;&#32500;&#24230;&#21644;&#20219;&#24847;&#26679;&#26412;&#25968;&#37117;&#25104;&#31435;&#30340;&#20005;&#26684;&#30028;&#38480;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20026;&#26680;&#30697;&#38453;&#30340;&#29305;&#24449;&#20540;&#25552;&#20379;&#20102;&#30456;&#23545;&#25200;&#21160;&#30028;&#38480;&#65292;&#36825;&#21487;&#33021;&#20855;&#26377;&#29420;&#31435;&#30340;&#37325;&#35201;&#24615;&#12290;&#36825;&#20123;&#30028;&#38480;&#25581;&#31034;&#20102;&#19968;&#31181;&#33258;&#25105;&#27491;&#21017;&#21270;&#29616;&#35937;&#65292;&#21363;&#26680;&#20998;&#35299;&#30340;&#29305;&#24449;&#20540;&#20013;&#23384;&#22312;&#37325;&#23614;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.15995v2 Announce Type: replace-cross  Abstract: It is by now well-established that modern over-parameterized models seem to elude the bias-variance tradeoff and generalize well despite overfitting noise. Many recent works attempt to analyze this phenomenon in the relatively tractable setting of kernel regression. However, as we argue in detail, most past works on this topic either make unrealistic assumptions, or focus on a narrow problem setup. This work aims to provide a unified theory to upper bound the excess risk of kernel regression for nearly all common and realistic settings. Specifically, we provide rigorous bounds that hold for common kernels and for any amount of regularization, noise, any input dimension, and any number of samples. Furthermore, we provide relative perturbation bounds for the eigenvalues of kernel matrices, which may be of independent interest. These reveal a self-regularization phenomenon, whereby a heavy tail in the eigendecomposition of the ker
&lt;/p&gt;</description></item><item><title>&#19977;&#31181;&#25552;&#31034;&#26041;&#27861;&#23545;ChatGPT&#30340;&#25968;&#23398;&#33021;&#21147;&#24182;&#26410;&#20135;&#29983;&#19968;&#36143;&#24615;&#25913;&#36827;&#25928;&#26524;&#65292;&#37096;&#20998;&#26041;&#27861;&#29978;&#33267;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;</title><link>https://arxiv.org/abs/2312.15006</link><description>&lt;p&gt;
&#35780;&#20272;&#25552;&#31034;&#26041;&#27861;&#23545;ChatGPT&#30340;&#25968;&#23398;&#33021;&#21147;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Assessing the Impact of Prompting Methods on ChatGPT's Mathematical Capabilities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.15006
&lt;/p&gt;
&lt;p&gt;
&#19977;&#31181;&#25552;&#31034;&#26041;&#27861;&#23545;ChatGPT&#30340;&#25968;&#23398;&#33021;&#21147;&#24182;&#26410;&#20135;&#29983;&#19968;&#36143;&#24615;&#25913;&#36827;&#25928;&#26524;&#65292;&#37096;&#20998;&#26041;&#27861;&#29978;&#33267;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25209;&#21028;&#24615;&#22320;&#35780;&#20272;&#20102;&#25552;&#31034;&#26041;&#27861;&#22312;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#30340;&#21151;&#25928;&#12290;&#35813;&#30740;&#31350;&#20351;&#29992;&#20102;&#19977;&#31181;&#35268;&#23450;&#24615;&#25552;&#31034;&#26041;&#27861; - &#31616;&#21333;&#25552;&#31034;&#12289;&#20010;&#20154;&#25552;&#31034;&#21644;&#23545;&#35805;&#25552;&#31034; - &#36825;&#20123;&#26041;&#27861;&#20197;&#25552;&#21319;LLMs&#35821;&#35328;&#20219;&#21153;&#25928;&#26524;&#32780;&#38395;&#21517;&#12290;&#25105;&#20204;&#22312;OpenAI&#30340;LLM&#38386;&#32842;&#26426;&#22120;&#20154;ChatGPT-3.5&#19978;&#36827;&#34892;&#27492;&#20998;&#26512;&#65292;&#28085;&#30422;&#20102;&#26469;&#33258;MATH&#12289;GSM8K&#21644;MMLU&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#38382;&#39064;&#38598;&#21512;&#65292;&#36825;&#20123;&#38382;&#39064;&#28085;&#30422;&#20102;&#21508;&#31181;&#25968;&#23398;&#25361;&#25112;&#12290;&#38024;&#23545;&#27599;&#20010;&#25968;&#25454;&#38598;&#35843;&#25972;&#30340;&#35780;&#20998;&#33050;&#26412;&#29992;&#20110;&#30830;&#23450;&#36825;&#20123;&#25552;&#31034;&#24178;&#39044;&#22312;&#22686;&#24378;&#27169;&#22411;&#25968;&#23398;&#20998;&#26512;&#33021;&#21147;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;&#19982;&#39044;&#26399;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#23454;&#35777;&#20998;&#26512;&#26174;&#31034;&#65292;&#25152;&#26816;&#39564;&#30340;&#26041;&#27861;&#22343;&#26410;&#22312;&#25345;&#32493;&#25913;&#36827;ChatGPT-3.5&#22522;&#20934;&#34920;&#29616;&#19978;&#65292;&#37096;&#20998;&#26041;&#27861;&#29978;&#33267;&#23548;&#33268;&#26126;&#26174;&#30340;&#36864;&#21270;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#25552;&#31034;&#31574;&#30053;&#26410;&#24517;&#33021;&#25552;&#39640;&#27169;&#22411;&#30340;&#25968;&#23398;&#20998;&#26512;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.15006v2 Announce Type: replace  Abstract: This study critically evaluates the efficacy of prompting methods in enhancing the mathematical reasoning capability of large language models (LLMs). The investigation uses three prescriptive prompting methods - simple, persona, and conversational prompting - known for their effectiveness in enhancing the linguistic tasks of LLMs. We conduct this analysis on OpenAI's LLM chatbot, ChatGPT-3.5, on extensive problem sets from the MATH, GSM8K, and MMLU datasets, encompassing a broad spectrum of mathematical challenges. A grading script adapted to each dataset is used to determine the effectiveness of these prompting interventions in enhancing the model's mathematical analysis power. Contrary to expectations, our empirical analysis reveals that none of the investigated methods consistently improves over ChatGPT-3.5's baseline performance, with some causing significant degradation. Our findings suggest that prompting strategies do not nece
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24314;&#31435;&#32463;&#39564;&#27604;&#20363;&#21644;&#32463;&#39564;&#33218;&#22870;&#21169;&#20043;&#38388;&#30340;&#36830;&#25509;&#65292;&#25552;&#39640;&#20102;&#19968;&#20123;&#29616;&#26377;&#31639;&#27861;&#30340;&#38169;&#35823;&#27010;&#29575;&#19978;&#30028;&#12290;</title><link>https://arxiv.org/abs/2312.12137</link><description>&lt;p&gt;
&#24102;&#26377;&#22266;&#23450;&#39044;&#31639;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#65306;&#22823;&#20559;&#24046;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Best Arm Identification with Fixed Budget: A Large Deviation Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.12137
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24314;&#31435;&#32463;&#39564;&#27604;&#20363;&#21644;&#32463;&#39564;&#33218;&#22870;&#21169;&#20043;&#38388;&#30340;&#36830;&#25509;&#65292;&#25552;&#39640;&#20102;&#19968;&#20123;&#29616;&#26377;&#31639;&#27861;&#30340;&#38169;&#35823;&#27010;&#29575;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20351;&#29992;&#22266;&#23450;&#25277;&#26679;&#39044;&#31639;&#22312;&#38543;&#26426;&#22810;&#33218;&#32769;&#34382;&#26426;(MABs)&#20013;&#35782;&#21035;&#26368;&#20339;&#33218;&#30340;&#38382;&#39064;&#12290;&#34920;&#24449;&#35813;&#38382;&#39064;&#30340;&#26368;&#23567;&#29305;&#23450;&#23454;&#20363;&#35823;&#24046;&#27010;&#29575;&#26500;&#25104;MABs&#20013;&#19968;&#30452;&#23384;&#22312;&#30340;&#37325;&#35201;&#24320;&#25918;&#38382;&#39064;&#20043;&#19968;&#12290;&#24403;&#20351;&#29992;&#38745;&#24577;&#25277;&#26679;&#31574;&#30053;&#36873;&#25321;&#33218;&#26102;&#65292;&#38169;&#35823;&#27010;&#29575;&#38543;&#30528;&#26679;&#26412;&#25968;&#21576;&#25351;&#25968;&#34928;&#20943;&#65292;&#20854;&#36895;&#29575;&#21487;&#20197;&#36890;&#36807;&#22823;&#20559;&#24046;&#25216;&#26415;&#26126;&#30830;&#25512;&#23548;&#12290;&#28982;&#32780;&#65292;&#20998;&#26512;&#20855;&#26377;&#33258;&#36866;&#24212;&#25277;&#26679;&#31574;&#30053;&#30340;&#31639;&#27861;&#30340;&#24615;&#33021;&#35201;&#22256;&#38590;&#24471;&#22810;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#36890;&#36807;&#32463;&#39564;&#27604;&#20363;&#28385;&#36275;&#30340;&#22823;&#20559;&#24046;&#21407;&#29702;(LDP)&#21644;&#36890;&#36807;&#32463;&#39564;&#33218;&#22870;&#21169;&#28385;&#36275;&#30340;LDP&#20043;&#38388;&#30340;&#36830;&#25509;&#12290;&#36825;&#31181;&#36830;&#25509;&#36866;&#29992;&#20110;&#20219;&#20309;&#33258;&#36866;&#24212;&#31639;&#27861;&#65292;&#24182;&#34987;&#21033;&#29992;&#26469;( i ) &#25552;&#39640;&#26576;&#20123;&#29616;&#26377;&#31639;&#27861;&#30340;&#38169;&#35823;&#27010;&#29575;&#19978;&#30028;&#65292;&#20363;&#22914;&#33879;&#21517;&#30340;\sr (Successive Re
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.12137v2 Announce Type: replace  Abstract: We consider the problem of identifying the best arm in stochastic Multi-Armed Bandits (MABs) using a fixed sampling budget. Characterizing the minimal instance-specific error probability for this problem constitutes one of the important remaining open problems in MABs. When arms are selected using a static sampling strategy, the error probability decays exponentially with the number of samples at a rate that can be explicitly derived via Large Deviation techniques. Analyzing the performance of algorithms with adaptive sampling strategies is however much more challenging. In this paper, we establish a connection between the Large Deviation Principle (LDP) satisfied by the empirical proportions of arm draws and that satisfied by the empirical arm rewards. This connection holds for any adaptive algorithm, and is leveraged (i) to improve error probability upper bounds of some existing algorithms, such as the celebrated \sr (Successive Re
&lt;/p&gt;</description></item><item><title>&#25152;&#25552;&#20986;&#30340;Dynamic Retrieval-Augmented Generation (DRAG)&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23454;&#20307;&#22686;&#24378;&#29983;&#25104;&#65292;&#23558;&#26816;&#32034;&#21040;&#30340;&#23454;&#20307;&#30340;&#21387;&#32553;&#23884;&#20837;&#27880;&#20837;&#21040;&#29983;&#25104;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#22312;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#36739;&#22909;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2312.08976</link><description>&lt;p&gt;
&#21160;&#24577;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Dynamic Retrieval-Augmented Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.08976
&lt;/p&gt;
&lt;p&gt;
&#25152;&#25552;&#20986;&#30340;Dynamic Retrieval-Augmented Generation (DRAG)&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23454;&#20307;&#22686;&#24378;&#29983;&#25104;&#65292;&#23558;&#26816;&#32034;&#21040;&#30340;&#23454;&#20307;&#30340;&#21387;&#32553;&#23884;&#20837;&#27880;&#20837;&#21040;&#29983;&#25104;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#22312;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#36739;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#25991;&#26412;&#21644;&#23553;&#35013;&#24191;&#27867;&#19990;&#30028;&#30693;&#35782;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#24448;&#24448;&#20250;&#20135;&#29983;&#24187;&#35273;&#24182;&#32570;&#20047;&#23616;&#37096;&#30456;&#20851;&#20107;&#23454;&#25968;&#25454;&#12290;&#26816;&#32034;&#22686;&#24378;&#26041;&#27861;&#34987;&#24341;&#20837;&#20197;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#24182;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#21709;&#24212;&#12290;&#36890;&#24120;&#65292;&#26816;&#32034;&#21040;&#30340;&#20449;&#24687;&#34987;&#31616;&#21333;&#22320;&#38468;&#21152;&#21040;&#20027;&#35831;&#27714;&#20013;&#65292;&#38480;&#21046;&#20102;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#22823;&#23567;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23454;&#20307;&#22686;&#24378;&#29983;&#25104;&#30340;&#21160;&#24577;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;DRAG&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#23558;&#26816;&#32034;&#21040;&#30340;&#23454;&#20307;&#30340;&#21387;&#32553;&#23884;&#20837;&#27880;&#20837;&#21040;&#29983;&#25104;&#27169;&#22411;&#20013;&#12290;&#25152;&#25552;&#20986;&#30340;&#27969;&#31243;&#26159;&#20026;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#32780;&#24320;&#21457;&#30340;&#65292;&#20294;&#20063;&#21487;&#20197;&#36716;&#31227;&#21040;&#19968;&#20123;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#12290;&#20026;&#20102;&#35757;&#32451;&#27169;&#22411;&#65292;&#25105;&#20204;&#25910;&#38598;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#26032;&#30340;&#39033;&#30446;&#32423;&#20195;&#30721;&#29983;&#25104;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23558;&#20854;&#29992;&#20110;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.08976v2 Announce Type: replace-cross  Abstract: Current state-of-the-art large language models are effective in generating high-quality text and encapsulating a broad spectrum of world knowledge. These models, however, often hallucinate and lack locally relevant factual data. Retrieval-augmented approaches were introduced to overcome these problems and provide more accurate responses. Typically, the retrieved information is simply appended to the main request, restricting the context window size of the model. We propose a novel approach for the Dynamic Retrieval-Augmented Generation (DRAG), based on the entity-augmented generation, which injects compressed embeddings of the retrieved entities into the generative model. The proposed pipeline was developed for code-generation tasks, yet can be transferred to some domains of natural language processing. To train the model, we collect and publish a new project-level code generation dataset. We use it for the evaluation along wit
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32806;&#21512;&#28151;&#28102;&#26657;&#27491;&#31639;&#27861;&#65292;&#33021;&#22815;&#20943;&#36731;&#20247;&#21253;&#26631;&#27880;&#20013;&#30340;&#26631;&#31614;&#22122;&#22768;&#65292;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#21452;&#27169;&#22411;&#30456;&#20114;&#26657;&#27491;&#23398;&#20064;&#21040;&#30340;&#28151;&#28102;&#30697;&#38453;&#65292;&#36827;&#19968;&#27493;&#20248;&#21270;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2312.07331</link><description>&lt;p&gt;
&#32806;&#21512;&#28151;&#28102;&#26657;&#27491;&#65306;&#20174;&#31232;&#30095;&#26631;&#27880;&#30340;&#32676;&#20307;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Coupled Confusion Correction: Learning from Crowds with Sparse Annotations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.07331
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32806;&#21512;&#28151;&#28102;&#26657;&#27491;&#31639;&#27861;&#65292;&#33021;&#22815;&#20943;&#36731;&#20247;&#21253;&#26631;&#27880;&#20013;&#30340;&#26631;&#31614;&#22122;&#22768;&#65292;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#21452;&#27169;&#22411;&#30456;&#20114;&#26657;&#27491;&#23398;&#20064;&#21040;&#30340;&#28151;&#28102;&#30697;&#38453;&#65292;&#36827;&#19968;&#27493;&#20248;&#21270;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25968;&#25454;&#38598;&#35268;&#27169;&#30340;&#22686;&#22823;&#65292;&#20934;&#30830;&#26631;&#27880;&#36825;&#20123;&#25968;&#25454;&#38598;&#21464;&#24471;&#36234;&#26469;&#36234;&#19981;&#20999;&#23454;&#38469;&#65292;&#22240;&#20026;&#22312;&#26102;&#38388;&#21644;&#32463;&#27982;&#19978;&#37117;&#21464;&#24471;&#26356;&#21152;&#26114;&#36149;&#12290;&#22240;&#27492;&#65292;&#20247;&#21253;&#24050;&#34987;&#24191;&#27867;&#37319;&#29992;&#20197;&#20943;&#36731;&#25910;&#38598;&#26631;&#31614;&#30340;&#25104;&#26412;&#65292;&#20294;&#36825;&#20063;&#19981;&#21487;&#36991;&#20813;&#22320;&#24341;&#20837;&#20102;&#26631;&#31614;&#22122;&#22768;&#65292;&#26368;&#32456;&#38477;&#20302;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#20174;&#20247;&#21253;&#26631;&#27880;&#20013;&#23398;&#20064;&#65292;&#23545;&#27599;&#20010;&#26631;&#27880;&#32773;&#30340;&#19987;&#19994;&#30693;&#35782;&#36827;&#34892;&#24314;&#27169;&#26159;&#19968;&#31181;&#24120;&#35265;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#33539;&#24335;&#65292;&#22240;&#20026;&#36890;&#36807;&#20247;&#21253;&#25910;&#38598;&#30340;&#26631;&#27880;&#36890;&#24120;&#26159;&#39640;&#24230;&#31232;&#30095;&#30340;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32806;&#21512;&#28151;&#28102;&#26657;&#27491;&#65288;CCC&#65289;&#65292;&#20854;&#20013;&#21516;&#26102;&#35757;&#32451;&#20004;&#20010;&#27169;&#22411;&#26469;&#26657;&#27491;&#24444;&#27492;&#23398;&#20064;&#30340;&#28151;&#28102;&#30697;&#38453;&#12290;&#36890;&#36807;&#21452;&#23618;&#20248;&#21270;&#65292;&#19968;&#20010;&#27169;&#22411;&#23398;&#20064;&#30340;&#28151;&#28102;&#30697;&#38453;&#21487;&#20197;&#36890;&#36807;&#21478;&#19968;&#20010;&#27169;&#22411;&#30340;&#31934;&#28860;&#25968;&#25454;&#36827;&#34892;&#26657;&#27491;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#8220;&#26631;&#27880;&#32773;&#32452;&#8221;&#36827;&#34892;&#32858;&#31867;&#65292;&#36825;&#20123;&#32452;&#20849;&#20139;&#30456;&#20284;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#20174;&#32780;&#25552;&#39640;&#20182;&#20204;&#30340;&#28151;&#28102;&#30697;&#38453;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.07331v3 Announce Type: replace  Abstract: As the size of the datasets getting larger, accurately annotating such datasets is becoming more impractical due to the expensiveness on both time and economy. Therefore, crowd-sourcing has been widely adopted to alleviate the cost of collecting labels, which also inevitably introduces label noise and eventually degrades the performance of the model. To learn from crowd-sourcing annotations, modeling the expertise of each annotator is a common but challenging paradigm, because the annotations collected by crowd-sourcing are usually highly-sparse. To alleviate this problem, we propose Coupled Confusion Correction (CCC), where two models are simultaneously trained to correct the confusion matrices learned by each other. Via bi-level optimization, the confusion matrices learned by one model can be corrected by the distilled data from the other. Moreover, we cluster the ``annotator groups'' who share similar expertise so that their confu
&lt;/p&gt;</description></item><item><title>&#20803;&#20849;&#35757;&#32451;&#36890;&#36807;&#22312;&#25968;&#25454;&#19978;&#26500;&#24314;&#19981;&#21516;&#30340;&#35270;&#35282;&#65292;&#24182;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#20849;&#21516;&#35757;&#32451;&#65292;&#25552;&#39640;&#20102;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2311.18083</link><description>&lt;p&gt;
&#20803;&#20849;&#35757;&#32451;&#65306;&#20004;&#31181;&#35270;&#35282;&#20248;&#20110;&#19968;&#31181;
&lt;/p&gt;
&lt;p&gt;
Meta Co-Training: Two Views are Better than One
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.18083
&lt;/p&gt;
&lt;p&gt;
&#20803;&#20849;&#35757;&#32451;&#36890;&#36807;&#22312;&#25968;&#25454;&#19978;&#26500;&#24314;&#19981;&#21516;&#30340;&#35270;&#35282;&#65292;&#24182;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#20849;&#21516;&#35757;&#32451;&#65292;&#25552;&#39640;&#20102;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#22330;&#26223;&#20013;&#65292;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#24456;&#22810;&#65292;&#20294;&#26631;&#31614;&#21364;&#31232;&#32570;&#19988;&#38590;&#20197;&#33719;&#24471;&#12290;&#22240;&#27492;&#65292;&#21322;&#30417;&#30563;&#23398;&#20064;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#25552;&#21319;&#30417;&#30563;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#24050;&#32463;&#22312;&#26368;&#36817;&#30340;&#25991;&#29486;&#20013;&#24341;&#36215;&#20102;&#37325;&#35201;&#30340;&#20851;&#27880;&#12290;&#20854;&#20013;&#19968;&#31181;&#20027;&#35201;&#30340;&#21322;&#30417;&#30563;&#31639;&#27861;&#26159;&#20849;&#35757;&#32451;&#12290;&#22312;&#20849;&#35757;&#32451;&#20013;&#65292;&#20004;&#31181;&#19981;&#21516;&#30340;&#27169;&#22411;&#21033;&#29992;&#25968;&#25454;&#30340;&#19981;&#21516;&#29420;&#31435;&#21644;&#36275;&#22815;&#30340;&#8220;&#35270;&#35282;&#8221;&#26469;&#20849;&#21516;&#36827;&#34892;&#26356;&#22909;&#30340;&#39044;&#27979;&#12290;&#22312;&#20849;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#27599;&#20010;&#27169;&#22411;&#22312;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#28857;&#19978;&#21019;&#24314;&#20266;&#26631;&#31614;&#65292;&#29992;&#20110;&#25913;&#36827;&#21478;&#19968;&#20010;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#24120;&#35265;&#24773;&#20917;&#19979;&#65292;&#24403;&#29420;&#31435;&#35270;&#35282;&#19981;&#21487;&#29992;&#26102;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#24265;&#20215;&#22320;&#26500;&#24314;&#36825;&#20123;&#35270;&#35282;&#12290;&#22312;&#26500;&#24314;&#30340;&#35270;&#35282;&#19978;&#36827;&#34892;&#20849;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#20248;&#20110;&#25105;&#20204;&#26500;&#24314;&#30340;&#20219;&#20309;&#21333;&#20010;&#35270;&#35282;&#65292;&#24182;&#19988;&#19982;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#26368;&#26032;&#26041;&#27861;&#24615;&#33021;&#30456;&#24403;&#65292;&#20294;&#20855;&#26377;&#19968;&#20123;&#19981;&#21487;&#21462;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many practical computer vision scenarios unlabeled data is plentiful, but labels are scarce and difficult to obtain. As a result, semi-supervised learning which leverages unlabeled data to boost the performance of supervised classifiers have received significant attention in recent literature. One major class of semi-supervised algorithms is co-training. In co-training two different models leverage different independent and sufficient "views" of the data to jointly make better predictions. During co-training each model creates pseudo labels on unlabeled points which are used to improve the other model. We show that in the common case when independent views are not available we can construct such views inexpensively using pre-trained models. Co-training on the constructed views yields a performance improvement over any of the individual views we construct and performance comparable with recent approaches in semi-supervised learning, but has some undesirable properties. To alleviate t
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35299;&#32806;&#21487;&#25239;&#35758;&#30340;&#25968;&#25454;&#29983;&#25104;&#32452;&#20214;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#38450;&#27490;&#20266;&#35013;&#30340;&#31243;&#24207;&#19981;&#20844;&#24179;&#65292;&#24182;&#24378;&#35843;&#20102;&#28385;&#36275;&#31243;&#24207;&#20844;&#24179;&#35201;&#27714;&#30340;&#37325;&#35201;&#24615;</title><link>https://arxiv.org/abs/2311.14688</link><description>&lt;p&gt;
&#36890;&#36807;&#35299;&#32806;&#21487;&#25239;&#35758;&#30340;&#25968;&#25454;&#29983;&#25104;&#32452;&#20214;&#26469;&#23454;&#29616;&#31243;&#24207;&#20844;&#24179;
&lt;/p&gt;
&lt;p&gt;
Procedural Fairness Through Decoupling Objectionable Data Generating Components
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.14688
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35299;&#32806;&#21487;&#25239;&#35758;&#30340;&#25968;&#25454;&#29983;&#25104;&#32452;&#20214;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#38450;&#27490;&#20266;&#35013;&#30340;&#31243;&#24207;&#19981;&#20844;&#24179;&#65292;&#24182;&#24378;&#35843;&#20102;&#28385;&#36275;&#31243;&#24207;&#20844;&#24179;&#35201;&#27714;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25581;&#31034;&#24182;&#35299;&#20915;&#20102;&#32463;&#24120;&#34987;&#24573;&#35270;&#20294;&#37325;&#35201;&#30340;&#38382;&#39064;&#65292;&#21363;&#20266;&#35013;&#30340;&#31243;&#24207;&#19981;&#20844;&#24179;&#65292;&#21363;&#23545;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#20013;&#31435;&#65288;&#21363;&#19981;&#25104;&#38382;&#39064;&#30340;&#65289;&#26041;&#38754;&#30340;&#21487;&#33021;&#26080;&#24847;&#30340;&#25913;&#21464;&#65292;&#21644;/&#25110;&#23545;&#26368;&#19981;&#21033;&#21033;&#30410;&#20010;&#20307;&#30340;&#23454;&#29616;&#27809;&#26377;&#31243;&#24207;&#20445;&#35777;&#12290;&#21463;&#32422;&#32752;&#183;&#32599;&#23572;&#26031;&#23545;&#32431;&#31243;&#24207;&#20844;&#27491;&#30340;&#20513;&#23548;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#33258;&#21160;&#20915;&#31574;&#35270;&#20026;&#31038;&#20250;&#21046;&#24230;&#30340;&#32553;&#24433;&#65292;&#24182;&#32771;&#34385;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#26412;&#36523;&#22914;&#20309;&#28385;&#36275;&#31243;&#24207;&#20844;&#24179;&#30340;&#35201;&#27714;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#21442;&#32771;&#28857;&#21644;&#30456;&#20851;&#30340;&#20215;&#20540;&#23454;&#20363;&#21270;&#35268;&#21017;&#65292;&#23558;&#21487;&#25239;&#35758;&#30340;&#25968;&#25454;&#29983;&#25104;&#32452;&#20214;&#19982;&#20013;&#31435;&#30340;&#25968;&#25454;&#29983;&#25104;&#32452;&#20214;&#35299;&#32806;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#24378;&#35843;&#20102;&#38450;&#27490;&#20266;&#35013;&#30340;&#31243;&#24207;&#19981;&#20844;&#24179;&#30340;&#24517;&#35201;&#24615;&#65292;&#19981;&#20165;&#24341;&#36215;&#20102;&#25105;&#20204;&#21147;&#22270;&#32531;&#35299;&#30340;&#21487;&#25239;&#35758;&#30340;&#25968;&#25454;&#29983;&#25104;&#32452;&#20214;&#30340;&#27880;&#24847;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.14688v2 Announce Type: replace-cross  Abstract: We reveal and address the frequently overlooked yet important issue of disguised procedural unfairness, namely, the potentially inadvertent alterations on the behavior of neutral (i.e., not problematic) aspects of data generating process, and/or the lack of procedural assurance of the greatest benefit of the least advantaged individuals. Inspired by John Rawls's advocacy for pure procedural justice, we view automated decision-making as a microcosm of social institutions, and consider how the data generating process itself can satisfy the requirements of procedural fairness. We propose a framework that decouples the objectionable data generating components from the neutral ones by utilizing reference points and the associated value instantiation rule. Our findings highlight the necessity of preventing disguised procedural unfairness, drawing attention not only to the objectionable data generating components that we aim to mitiga
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20174;&#29702;&#35770;&#35282;&#24230;&#23545;&#29983;&#25104;&#24314;&#27169;&#20013;&#30340;&#22810;&#31181;&#37319;&#26679;&#26041;&#27861;&#36827;&#34892;&#20102;&#23457;&#35270;&#21644;&#32452;&#32455;&#65292;&#24110;&#21161;&#20811;&#26381;&#37319;&#26679;&#20013;&#30340;&#19968;&#20123;&#25361;&#25112;&#65292;&#27604;&#22914;&#25512;&#29702;&#26102;&#38388;&#38271;&#21644;&#29983;&#25104;&#26679;&#26412;&#32570;&#20047;&#22810;&#26679;&#24615;&#12290;</title><link>https://arxiv.org/abs/2311.13845</link><description>&lt;p&gt;
&#20351;&#29992;&#25512;&#21069;&#26144;&#23556;&#36827;&#34892;&#21508;&#22320;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Touring sampling with pushforward maps
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.13845
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20174;&#29702;&#35770;&#35282;&#24230;&#23545;&#29983;&#25104;&#24314;&#27169;&#20013;&#30340;&#22810;&#31181;&#37319;&#26679;&#26041;&#27861;&#36827;&#34892;&#20102;&#23457;&#35270;&#21644;&#32452;&#32455;&#65292;&#24110;&#21161;&#20811;&#26381;&#37319;&#26679;&#20013;&#30340;&#19968;&#20123;&#25361;&#25112;&#65292;&#27604;&#22914;&#25512;&#29702;&#26102;&#38388;&#38271;&#21644;&#29983;&#25104;&#26679;&#26412;&#32570;&#20047;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#19968;&#20010;&#24076;&#26395;&#23558;&#24378;&#22823;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24212;&#29992;&#20110;&#29305;&#23450;&#38382;&#39064;&#30340;&#20174;&#19994;&#32773;&#26469;&#35828;&#65292;&#37319;&#26679;&#26041;&#27861;&#30340;&#25968;&#37327;&#21487;&#33021;&#20196;&#20154;&#29983;&#30031;&#12290;&#26412;&#25991;&#20174;&#29702;&#35770;&#35282;&#24230;&#20986;&#21457;&#65292;&#23545;&#22312;&#8220;&#29983;&#25104;&#24314;&#27169;&#8221;&#35774;&#32622;&#20013;&#35768;&#22810;&#37319;&#26679;&#26041;&#27861;&#36827;&#34892;&#20102;&#23457;&#35270;&#21644;&#32452;&#32455;&#65292;&#20854;&#20013;&#24076;&#26395;&#29983;&#25104;&#19982;&#19968;&#20123;&#35757;&#32451;&#26679;&#26412;&#31867;&#20284;&#30340;&#26032;&#25968;&#25454;&#12290;&#36890;&#36807;&#25581;&#31034;&#29616;&#26377;&#26041;&#27861;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#21487;&#33021;&#26377;&#21161;&#20110;&#20811;&#26381;&#19982;&#25193;&#25955;&#27169;&#22411;&#37319;&#26679;&#30456;&#20851;&#30340;&#19968;&#20123;&#24403;&#21069;&#25361;&#25112;&#65292;&#27604;&#22914;&#30001;&#20110;&#25193;&#25955;&#27169;&#25311;&#32780;&#23548;&#33268;&#30340;&#38271;&#25512;&#29702;&#26102;&#38388;&#65292;&#25110;&#32773;&#29983;&#25104;&#26679;&#26412;&#32570;&#20047;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.13845v2 Announce Type: replace-cross  Abstract: The number of sampling methods could be daunting for a practitioner looking to cast powerful machine learning methods to their specific problem. This paper takes a theoretical stance to review and organize many sampling approaches in the ``generative modeling'' setting, where one wants to generate new data that are similar to some training examples. By revealing links between existing methods, it might prove useful to overcome some of the current challenges in sampling with diffusion models, such as long inference time due to diffusion simulation, or the lack of diversity in generated samples.
&lt;/p&gt;</description></item><item><title>&#25552;&#31034;&#24037;&#31243;&#20219;&#21153;&#23545;&#20110;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23450;&#21046;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#33267;&#20851;&#37325;&#35201;&#65292;PE2&#26041;&#27861;&#36890;&#36807;&#35814;&#32454;&#25551;&#36848;&#12289;&#19978;&#19979;&#25991;&#35268;&#33539;&#21644;&#36880;&#27493;&#25512;&#29702;&#27169;&#26495;&#30340;&#27880;&#20837;&#65292;&#22312;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20986;&#33394;&#30340;&#36866;&#29992;&#24615;&#21644;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2311.05661</link><description>&lt;p&gt;
Prompt Engineering a Prompt Engineer
&lt;/p&gt;
&lt;p&gt;
Prompt Engineering a Prompt Engineer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.05661
&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#24037;&#31243;&#20219;&#21153;&#23545;&#20110;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23450;&#21046;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#33267;&#20851;&#37325;&#35201;&#65292;PE2&#26041;&#27861;&#36890;&#36807;&#35814;&#32454;&#25551;&#36848;&#12289;&#19978;&#19979;&#25991;&#35268;&#33539;&#21644;&#36880;&#27493;&#25512;&#29702;&#27169;&#26495;&#30340;&#27880;&#20837;&#65292;&#22312;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20986;&#33394;&#30340;&#36866;&#29992;&#24615;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#24037;&#31243;&#26159;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23450;&#21046;&#20219;&#21153;&#19978;&#34920;&#29616;&#30340;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#33267;&#20851;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#26816;&#26597;&#27169;&#22411;&#30340;&#38169;&#35823;&#65292;&#20551;&#35774;&#24403;&#21069;&#25552;&#31034;&#20013;&#32570;&#23569;&#25110;&#35823;&#23548;&#20102;&#20160;&#20040;&#65292;&#24182;&#28165;&#26224;&#22320;&#20256;&#36798;&#20219;&#21153;&#65292;&#38656;&#35201;&#22797;&#26434;&#30340;&#25512;&#29702;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#34987;&#20803;&#25552;&#31034;&#26469;&#25191;&#34892;&#33258;&#21160;&#25552;&#31034;&#24037;&#31243;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#30001;&#20110;&#20803;&#25552;&#31034;&#20013;&#32570;&#20047;&#22797;&#26434;&#25512;&#29702;&#30340;&#20805;&#20998;&#25351;&#23548;&#65292;&#23427;&#20204;&#30340;&#28508;&#21147;&#21463;&#21040;&#38480;&#21046;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#35814;&#32454;&#25551;&#36848;&#12289;&#19978;&#19979;&#25991;&#35268;&#33539;&#21644;&#36880;&#27493;&#25512;&#29702;&#27169;&#26495;&#27880;&#20837;&#21040;&#20803;&#25552;&#31034;&#20013;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25152;&#24471;&#21040;&#30340;&#26041;&#27861;&#31216;&#20026;PE2&#65292;&#23637;&#31034;&#20102;&#22312;&#19981;&#21516;&#35821;&#35328;&#20219;&#21153;&#20013;&#20986;&#33394;&#30340;&#36866;&#29992;&#24615;&#12290;&#23427;&#25214;&#21040;&#30340;&#25552;&#31034;&#22312;MultiArith&#19978;&#27604;&#8220;&#25353;&#27493;&#39588;&#24605;&#32771;&#8221;&#39640;&#20986;6.3%&#65292;&#22312;GSM8K&#19978;&#39640;&#20986;3.1%&#65292;&#24182;&#22312;&#23545;&#31435;&#20219;&#21153;&#19978;&#20248;&#20110;&#31454;&#20105;&#22522;&#32447;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.05661v2 Announce Type: replace-cross  Abstract: Prompt engineering is a challenging yet crucial task for optimizing the performance of large language models on customized tasks. It requires complex reasoning to examine the model's errors, hypothesize what is missing or misleading in the current prompt, and communicate the task with clarity. While recent works indicate that large language models can be meta-prompted to perform automatic prompt engineering, we argue that their potential is limited due to insufficient guidance for complex reasoning in the meta-prompt. We fill this gap by infusing into the meta-prompt three key components: detailed descriptions, context specification, and a step-by-step reasoning template. The resulting method, named PE2, showcases remarkable versatility across diverse language tasks. It finds prompts that outperform "let's think step by step" by 6.3% on MultiArith and 3.1% on GSM8K, and outperforms competitive baselines on counterfactual tasks 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20026;&#22522;&#30784;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#29436;&#20154;&#26432;&#28216;&#25103;&#20013;&#24320;&#21457;&#20855;&#26377;&#28789;&#27963;&#35821;&#35328;&#34892;&#20026;&#21644;&#24378;&#22823;&#20915;&#31574;&#33021;&#21147;&#30340;&#25112;&#30053;&#35821;&#35328;&#20195;&#29702;</title><link>https://arxiv.org/abs/2310.18940</link><description>&lt;p&gt;
&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#35821;&#35328;&#20195;&#29702;&#22312;&#29436;&#20154;&#26432;&#28216;&#25103;&#20013;&#36827;&#34892;&#25112;&#30053;&#23545;&#25112;
&lt;/p&gt;
&lt;p&gt;
Language Agents with Reinforcement Learning for Strategic Play in the Werewolf Game
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.18940
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20026;&#22522;&#30784;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#29436;&#20154;&#26432;&#28216;&#25103;&#20013;&#24320;&#21457;&#20855;&#26377;&#28789;&#27963;&#35821;&#35328;&#34892;&#20026;&#21644;&#24378;&#22823;&#20915;&#31574;&#33021;&#21147;&#30340;&#25112;&#30053;&#35821;&#35328;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26500;&#24314;&#30340;&#20195;&#29702;&#22312;&#21508;&#39046;&#22495;&#23637;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#22797;&#26434;&#20915;&#31574;&#20219;&#21153;&#20013;&#65292;&#32431;LLM&#20195;&#29702;&#24448;&#24448;&#34920;&#29616;&#20986;&#22266;&#26377;&#20559;&#35265;&#65292;&#36825;&#20123;&#20559;&#35265;&#26469;&#28304;&#20110;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#12290;&#20026;&#20102;&#24320;&#21457;&#20855;&#26377;&#28789;&#27963;&#35821;&#35328;&#34892;&#20026;&#21644;&#24378;&#22823;&#20915;&#31574;&#33021;&#21147;&#30340;&#25112;&#30053;&#35821;&#35328;&#20195;&#29702;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#25552;&#21319;LLM&#20195;&#29702;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36873;&#25321;&#29436;&#20154;&#26432;&#20316;&#20026;&#20855;&#26377;&#22810;&#26679;&#27807;&#36890;&#21644;&#25112;&#30053;&#28216;&#25103;&#29609;&#27861;&#30340;&#25361;&#25112;&#27979;&#35797;&#24179;&#21488;&#12290;&#20026;&#20102;&#20943;&#36731;&#35821;&#35328;&#34892;&#20026;&#20013;&#30340;&#22266;&#26377;&#20559;&#35265;&#65292;&#25105;&#20204;&#30340;&#20195;&#29702;&#20351;&#29992;LLM&#36827;&#34892;&#28436;&#32462;&#25512;&#29702;&#24182;&#29983;&#25104;&#22810;&#26679;&#34892;&#20026;&#20505;&#36873;&#38598;&#12290;&#28982;&#21518;&#65292;&#32463;&#36807;&#35757;&#32451;&#20197;&#20248;&#21270;&#20915;&#31574;&#33021;&#21147;&#30340;RL&#31574;&#30053;&#20174;&#20505;&#36873;&#38598;&#20013;&#36873;&#25321;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.18940v3 Announce Type: replace  Abstract: Agents built with large language models (LLMs) have shown great potential across a wide range of domains. However, in complex decision-making tasks, pure LLM-based agents tend to exhibit intrinsic bias in their choice of actions, which is inherited from the model's training data and results in suboptimal performance. To develop strategic language agents, i.e., agents that generate flexible language actions and possess strong decision-making abilities, we propose a novel framework that powers LLM-based agents with reinforcement learning (RL). We consider Werewolf, a popular social deduction game, as a challenging testbed that emphasizes versatile communication and strategic gameplay. To mitigate the intrinsic bias in language actions, our agents use an LLM to perform deductive reasoning and generate a diverse set of action candidates. Then an RL policy trained to optimize the decision-making ability chooses an action from the candidat
&lt;/p&gt;</description></item><item><title>&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21270;&#36807;&#31243;&#20013;&#28041;&#21450;&#26680;-&#22270;&#23545;&#40784;&#29616;&#35937;&#65292;&#20174;&#20248;&#21270;&#35282;&#24230;&#35299;&#37322;&#20102;&#23398;&#21040;&#30340;&#20989;&#25968;&#20309;&#26102;&#21644;&#20026;&#20309;&#27867;&#21270;&#65292;&#26377;&#21161;&#20110;&#29702;&#35299;&#20854;&#22312;&#24322;&#28304;&#22270;&#19978;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2310.05105</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#26159;&#22914;&#20309;&#23398;&#20064;&#30340;&#65306;&#26469;&#33258;&#35757;&#32451;&#21160;&#24577;&#30340;&#21551;&#31034;
&lt;/p&gt;
&lt;p&gt;
How Graph Neural Networks Learn: Lessons from Training Dynamics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05105
&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21270;&#36807;&#31243;&#20013;&#28041;&#21450;&#26680;-&#22270;&#23545;&#40784;&#29616;&#35937;&#65292;&#20174;&#20248;&#21270;&#35282;&#24230;&#35299;&#37322;&#20102;&#23398;&#21040;&#30340;&#20989;&#25968;&#20309;&#26102;&#21644;&#20026;&#20309;&#27867;&#21270;&#65292;&#26377;&#21161;&#20110;&#29702;&#35299;&#20854;&#22312;&#24322;&#28304;&#22270;&#19978;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;&#19968;&#20010;&#38271;&#26399;&#20197;&#26469;&#30340;&#30446;&#26631;&#26159;&#20197;&#26356;&#26131;&#35299;&#37322;&#30340;&#26041;&#24335;&#34920;&#24449;&#40657;&#30418;&#27169;&#22411;&#30340;&#23398;&#20064;&#34892;&#20026;&#12290;&#23545;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#65292;&#22312;&#27491;&#24335;&#21270;&#23427;&#20204;&#21487;&#20197;&#34920;&#31034;&#30340;&#20989;&#25968;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;GNNs&#26159;&#21542;&#20250;&#23398;&#20064;&#21040;&#26399;&#26395;&#30340;&#20989;&#25968;&#20173;&#19981;&#22826;&#28165;&#26970;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23427;&#20204;&#22312;&#20989;&#25968;&#31354;&#38388;&#20013;&#30340;&#35757;&#32451;&#21160;&#24577;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;GNNs&#38544;&#24335;&#21033;&#29992;&#22270;&#32467;&#26500;&#26469;&#26356;&#26032;&#23398;&#21040;&#30340;&#20989;&#25968;&#12290;&#36825;&#31181;&#29616;&#35937;&#34987;&#31216;&#20026;&#26680;-&#22270;&#23545;&#40784;&#65292;&#24050;&#32463;&#32463;&#39564;&#24615;&#21644;&#29702;&#35770;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;&#36825;&#31181;&#26469;&#33258;&#20248;&#21270;&#35282;&#24230;&#30340;&#26032;&#20998;&#26512;&#26694;&#26550;&#33021;&#22815;&#35299;&#37322;&#20102;&#20309;&#26102;&#20197;&#21450;&#20026;&#20160;&#20040;&#23398;&#20064;&#21040;&#30340;GNN&#20989;&#25968;&#27867;&#21270;&#65292;&#36825;&#23545;&#20110;&#23427;&#20204;&#22312;&#24322;&#28304;&#22270;&#19978;&#30340;&#38480;&#21046;&#20855;&#26377;&#30456;&#20851;&#24615;&#12290;&#20174;&#23454;&#29992;&#30340;&#35282;&#24230;&#30475;&#65292;&#23427;&#20063;&#25552;&#20379;&#20102;&#23545;&#20110;GNNs&#22914;&#20309;&#23398;&#20064;&#20989;&#25968;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.05105v2 Announce Type: replace  Abstract: A long-standing goal in deep learning has been to characterize the learning behavior of black-box models in a more interpretable manner. For graph neural networks (GNNs), considerable advances have been made in formalizing what functions they can represent, but whether GNNs will learn desired functions during the optimization process remains less clear. To fill this gap, we study their training dynamics in function space. In particular, we find that the optimization of GNNs through gradient descent implicitly leverages the graph structure to update the learned function. This phenomenon is dubbed as kernel-graph alignment, which has been empirically and theoretically corroborated. This new analytical framework from the optimization perspective enables interpretable explanations of when and why the learned GNN functions generalize, which are relevant to their limitations on heterophilic graphs. From a practical standpoint, it also prov
&lt;/p&gt;</description></item><item><title>MiCRO&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26799;&#24230;&#31232;&#30095;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#26799;&#24230;&#21521;&#37327;&#36827;&#34892;&#20998;&#21306;&#65292;&#24182;&#30001;&#27599;&#20010;&#24037;&#20316;&#32773;&#36873;&#25321;&#20854;&#20998;&#21306;&#20013;&#30340;&#26799;&#24230;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#36890;&#20449;&#27969;&#37327;&#24182;&#36991;&#20813;&#20102;&#26799;&#24230;&#22534;&#31215;&#12290;</title><link>https://arxiv.org/abs/2310.00967</link><description>&lt;p&gt;
MiCRO&#65306;&#29992;&#20110;&#25193;&#23637;&#21644;&#21152;&#36895;&#20998;&#24067;&#24335;DNN&#35757;&#32451;&#30340;&#36817;&#38646;&#25104;&#26412;&#26799;&#24230;&#31232;&#30095;&#21270;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
MiCRO: Near-Zero Cost Gradient Sparsification for Scaling and Accelerating Distributed DNN Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.00967
&lt;/p&gt;
&lt;p&gt;
MiCRO&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26799;&#24230;&#31232;&#30095;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#26799;&#24230;&#21521;&#37327;&#36827;&#34892;&#20998;&#21306;&#65292;&#24182;&#30001;&#27599;&#20010;&#24037;&#20316;&#32773;&#36873;&#25321;&#20854;&#20998;&#21306;&#20013;&#30340;&#26799;&#24230;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#36890;&#20449;&#27969;&#37327;&#24182;&#36991;&#20813;&#20102;&#26799;&#24230;&#22534;&#31215;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26799;&#24230;&#31232;&#30095;&#21270;&#26159;&#19968;&#31181;&#29992;&#20110;&#25193;&#23637;&#21644;&#21152;&#36895;&#20998;&#24067;&#24335;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#35757;&#32451;&#30340;&#36890;&#20449;&#20248;&#21270;&#25216;&#26415;&#65292;&#23427;&#20943;&#23569;&#20102;&#26799;&#24230;&#32858;&#21512;&#30340;&#36890;&#20449;&#27969;&#37327;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#31232;&#30095;&#21270;&#26041;&#27861;&#30001;&#20110;&#26799;&#24230;&#36873;&#25321;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#21644;/&#25110;&#36890;&#20449;&#27969;&#37327;&#22686;&#21152;&#32780;&#20855;&#26377;&#36739;&#24046;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MiCRO&#30340;&#26032;&#22411;&#26799;&#24230;&#31232;&#30095;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.00967v3 Announce Type: replace  Abstract: Gradient sparsification is a communication optimisation technique for scaling and accelerating distributed deep neural network (DNN) training. It reduces the increasing communication traffic for gradient aggregation. However, existing sparsifiers have poor scalability because of the high computational cost of gradient selection and/or increase in communication traffic. In particular, an increase in communication traffic is caused by gradient build-up and inappropriate threshold for gradient selection.   To address these challenges, we propose a novel gradient sparsification method called MiCRO. In MiCRO, the gradient vector is partitioned, and each partition is assigned to the corresponding worker. Each worker then selects gradients from its partition, and the aggregated gradients are free from gradient build-up. Moreover, MiCRO estimates the accurate threshold to maintain the communication traffic as per user requirement by minimisi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;XGBoost&#25216;&#26415;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#21644;&#22635;&#20805;&#28151;&#21512;&#31867;&#22411;&#34920;&#26684;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#25968;&#25454;&#29983;&#25104;&#20219;&#21153;&#20013;&#20248;&#20110;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24182;&#22312;&#25968;&#25454;&#22635;&#20805;&#26041;&#38754;&#34920;&#29616;&#31454;&#20105;&#21147;&#65292;&#19988;&#21487;&#20197;&#22312;CPU&#19978;&#24182;&#34892;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2309.09968</link><description>&lt;p&gt;
&#36890;&#36807;&#25193;&#25955;&#21644;&#22522;&#20110;&#26799;&#24230;&#25552;&#21319;&#26641;&#30340;&#27969;&#29983;&#25104;&#21644;&#22635;&#20805;&#34920;&#26684;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Generating and Imputing Tabular Data via Diffusion and Flow-based Gradient-Boosted Trees
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.09968
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;XGBoost&#25216;&#26415;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#21644;&#22635;&#20805;&#28151;&#21512;&#31867;&#22411;&#34920;&#26684;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#25968;&#25454;&#29983;&#25104;&#20219;&#21153;&#20013;&#20248;&#20110;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24182;&#22312;&#25968;&#25454;&#22635;&#20805;&#26041;&#38754;&#34920;&#29616;&#31454;&#20105;&#21147;&#65292;&#19988;&#21487;&#20197;&#22312;CPU&#19978;&#24182;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#26684;&#25968;&#25454;&#24456;&#38590;&#33719;&#21462;&#65292;&#19988;&#23481;&#26131;&#23384;&#22312;&#32570;&#22833;&#20540;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;&#20998;&#25968;&#30340;&#25193;&#25955;&#21644;&#26465;&#20214;&#27969;&#21305;&#37197;&#26469;&#29983;&#25104;&#21644;&#22635;&#34917;&#28151;&#21512;&#31867;&#22411;&#65288;&#36830;&#32493;&#21644;&#20998;&#31867;&#65289;&#34920;&#26684;&#25968;&#25454;&#12290;&#19982;&#20381;&#36182;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#20998;&#25968;&#20989;&#25968;&#25110;&#21521;&#37327;&#22330;&#30340;&#20808;&#21069;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;XGBoost&#65292;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#26799;&#24230;&#25552;&#21319;&#26641;&#65288;GBT&#65289;&#25216;&#26415;&#12290;&#20026;&#20102;&#27979;&#35797;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;27&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;9&#20010;&#24230;&#37327;&#26631;&#20934;&#30340;&#26368;&#24191;&#27867;&#30340;&#34920;&#26684;&#25968;&#25454;&#29983;&#25104;&#21644;&#22635;&#20805;&#22522;&#20934;&#12290;&#36890;&#36807;&#23545;&#22522;&#20934;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25968;&#25454;&#29983;&#25104;&#20219;&#21153;&#20013;&#20248;&#20110;&#28145;&#24230;&#23398;&#20064;&#29983;&#25104;&#26041;&#27861;&#65292;&#24182;&#22312;&#25968;&#25454;&#22635;&#20805;&#26041;&#38754;&#20445;&#25345;&#31454;&#20105;&#21147;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23427;&#21487;&#20197;&#22312;CPU&#19978;&#24182;&#34892;&#35757;&#32451;&#65292;&#26080;&#38656;GPU&#12290;&#25105;&#20204;&#30340;Python&#21644;R&#20195;&#30721;&#21487;&#20197;&#22312;https://github.com/SamsungSAILMontreal/ForestDiffusio&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.09968v3 Announce Type: replace  Abstract: Tabular data is hard to acquire and is subject to missing values. This paper introduces a novel approach for generating and imputing mixed-type (continuous and categorical) tabular data utilizing score-based diffusion and conditional flow matching. In contrast to prior methods that rely on neural networks to learn the score function or the vector field, we adopt XGBoost, a widely used Gradient-Boosted Tree (GBT) technique. To test our method, we build one of the most extensive benchmarks for tabular data generation and imputation, containing 27 diverse datasets and 9 metrics. Through empirical evaluation across the benchmark, we demonstrate that our approach outperforms deep-learning generation methods in data generation tasks and remains competitive in data imputation. Notably, it can be trained in parallel using CPUs without requiring a GPU. Our Python and R code is available at https://github.com/SamsungSAILMontreal/ForestDiffusio
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#20984;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#30340;&#33258;&#20849;&#36717;&#24179;&#28369;&#26041;&#27861;&#65292;&#36890;&#36807;&#21464;&#37327;&#24230;&#37327;&#21644;&#27493;&#38271;&#35268;&#21017;&#20248;&#21270;&#20102;&#36817;&#31471;&#29275;&#39039;&#31639;&#27861;&#65292;&#26377;&#25928;&#22788;&#29702;&#20102;&#38750;&#20809;&#28369;&#20989;&#25968;&#30340;&#32467;&#26500;&#65292;&#25552;&#20986;&#20102;Prox-N-SCORE&#21644;Prox-GGN-SCORE&#31639;&#27861;&#65292;&#21518;&#32773;&#36890;&#36807;&#37325;&#35201;&#36817;&#20284;&#31243;&#24207;&#26174;&#33879;&#20943;&#23569;&#20102;&#36870;Hessian&#35745;&#31639;&#24320;&#38144;&#12290;</title><link>https://arxiv.org/abs/2309.01781</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#20984;&#32452;&#21512;&#20248;&#21270;&#30340;&#33258;&#20849;&#36717;&#24179;&#28369;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Self-concordant Smoothing for Large-Scale Convex Composite Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.01781
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#20984;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#30340;&#33258;&#20849;&#36717;&#24179;&#28369;&#26041;&#27861;&#65292;&#36890;&#36807;&#21464;&#37327;&#24230;&#37327;&#21644;&#27493;&#38271;&#35268;&#21017;&#20248;&#21270;&#20102;&#36817;&#31471;&#29275;&#39039;&#31639;&#27861;&#65292;&#26377;&#25928;&#22788;&#29702;&#20102;&#38750;&#20809;&#28369;&#20989;&#25968;&#30340;&#32467;&#26500;&#65292;&#25552;&#20986;&#20102;Prox-N-SCORE&#21644;Prox-GGN-SCORE&#31639;&#27861;&#65292;&#21518;&#32773;&#36890;&#36807;&#37325;&#35201;&#36817;&#20284;&#31243;&#24207;&#26174;&#33879;&#20943;&#23569;&#20102;&#36870;Hessian&#35745;&#31639;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#20849;&#36717;&#24179;&#28369;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#26368;&#23567;&#21270;&#20004;&#20010;&#20984;&#20989;&#25968;&#30340;&#21644;&#65292;&#20854;&#20013;&#19968;&#20010;&#26159;&#20809;&#28369;&#30340;&#65292;&#21478;&#19968;&#20010;&#21487;&#33021;&#26159;&#38750;&#20809;&#28369;&#30340;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#20142;&#28857;&#22312;&#20110;&#25152;&#24471;&#38382;&#39064;&#32467;&#26500;&#30340;&#33258;&#28982;&#29305;&#24615;&#65292;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#21464;&#37327;&#24230;&#37327;&#36873;&#25321;&#26041;&#27861;&#21644;&#19968;&#20010;&#29305;&#21035;&#36866;&#29992;&#20110;&#36817;&#31471;&#29275;&#39039;&#31867;&#22411;&#31639;&#27861;&#30340;&#27493;&#38271;&#36873;&#25321;&#35268;&#21017;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#39640;&#25928;&#22788;&#29702;&#20102;&#38750;&#20809;&#28369;&#20989;&#25968;&#25512;&#21160;&#30340;&#20855;&#20307;&#32467;&#26500;&#65292;&#22914;$\ell_1$&#27491;&#21017;&#21270;&#21644;&#20998;&#32452;Lasso&#24809;&#32602;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20004;&#20010;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#65306;Prox-N-SCORE&#65292;&#19968;&#31181;&#36817;&#31471;&#29275;&#39039;&#31639;&#27861;&#65292;&#21644;Prox-GGN-SCORE&#65292;&#19968;&#31181;&#36817;&#31471;&#24191;&#20041;&#39640;&#26031;-&#29275;&#39039;&#31639;&#27861;&#12290;Prox-GGN-SCORE&#31639;&#27861;&#31361;&#20986;&#20102;&#19968;&#31181;&#37325;&#35201;&#30340;&#36817;&#20284;&#31243;&#24207;&#65292;&#26377;&#21161;&#20110;&#26174;&#33879;&#20943;&#23569;&#36870;Hessian&#30456;&#20851;&#30340;&#22823;&#37096;&#20998;&#35745;&#31639;&#24320;&#38144;&#12290;&#36825;&#31181;&#36817;&#20284;&#22312;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.01781v2 Announce Type: replace-cross  Abstract: We introduce a notion of self-concordant smoothing for minimizing the sum of two convex functions, one of which is smooth and the other may be nonsmooth. The key highlight of our approach is in a natural property of the resulting problem's structure which provides us with a variable-metric selection method and a step-length selection rule particularly suitable for proximal Newton-type algorithms. In addition, we efficiently handle specific structures promoted by the nonsmooth function, such as $\ell_1$-regularization and group-lasso penalties. We prove the convergence of two resulting algorithms: Prox-N-SCORE, a proximal Newton algorithm and Prox-GGN-SCORE, a proximal generalized Gauss-Newton algorithm. The Prox-GGN-SCORE algorithm highlights an important approximation procedure which helps to significantly reduce most of the computational overhead associated with the inverse Hessian. This approximation is essentially useful fo
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24179;&#28369;&#24230;&#35825;&#23548;&#27491;&#21017;&#21270;&#21644;&#22522;&#20110;&#39057;&#35889;&#22270;&#30340;&#25968;&#25454;&#22686;&#24378;&#65292;&#25552;&#39640;&#27700;&#19979;&#22768;&#23398;&#30446;&#26631;&#35782;&#21035;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#36991;&#20813;&#24615;&#33021;&#38477;&#32423;&#30340;&#39118;&#38505;</title><link>https://arxiv.org/abs/2306.06945</link><description>&lt;p&gt;
&#22522;&#20110;&#24179;&#28369;&#24230;&#35825;&#23548;&#27491;&#21017;&#21270;&#21644;&#22522;&#20110;&#39057;&#35889;&#22270;&#30340;&#25968;&#25454;&#22686;&#24378;&#30340;&#27700;&#19979;&#22768;&#23398;&#30446;&#26631;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Underwater Acoustic Target Recognition based on Smoothness-inducing Regularization and Spectrogram-based Data Augmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.06945
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24179;&#28369;&#24230;&#35825;&#23548;&#27491;&#21017;&#21270;&#21644;&#22522;&#20110;&#39057;&#35889;&#22270;&#30340;&#25968;&#25454;&#22686;&#24378;&#65292;&#25552;&#39640;&#27700;&#19979;&#22768;&#23398;&#30446;&#26631;&#35782;&#21035;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#36991;&#20813;&#24615;&#33021;&#38477;&#32423;&#30340;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27700;&#19979;&#22768;&#23398;&#30446;&#26631;&#35782;&#21035;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#36825;&#24402;&#22240;&#20110;&#22797;&#26434;&#30340;&#27700;&#19979;&#29615;&#22659;&#21644;&#26377;&#38480;&#30340;&#25968;&#25454;&#21487;&#29992;&#24615;&#12290;&#25968;&#25454;&#19981;&#36275;&#21487;&#33021;&#20250;&#38459;&#30861;&#35782;&#21035;&#31995;&#32479;&#25903;&#25345;&#22797;&#26434;&#24314;&#27169;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#38459;&#30861;&#20854;&#21457;&#23637;&#12290;&#20026;&#20102;&#25552;&#39640;&#35782;&#21035;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24050;&#32463;&#37319;&#29992;&#20102;&#35832;&#22914;&#25968;&#25454;&#22686;&#24378;&#20043;&#31867;&#30340;&#25216;&#26415;&#26469;&#27169;&#25311;&#27700;&#19979;&#20449;&#21495;&#24182;&#20016;&#23500;&#25968;&#25454;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#27700;&#19979;&#29615;&#22659;&#30340;&#22797;&#26434;&#24615;&#21487;&#33021;&#23548;&#33268;&#27169;&#25311;&#20449;&#21495;&#20559;&#31163;&#30495;&#23454;&#22330;&#26223;&#65292;&#23548;&#33268;&#21463;&#21040;&#38750;&#30495;&#23454;&#25968;&#25454;&#35823;&#23548;&#30340;&#20559;&#35265;&#27169;&#22411;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31574;&#30053;&#65292;&#21487;&#20197;&#22312;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#22686;&#24378;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21516;&#26102;&#36991;&#20813;&#24615;&#33021;&#38477;&#32423;&#30340;&#39118;&#38505;&#12290;&#39318;&#20808;&#65292;&#20316;&#20026;&#26367;&#20195;&#20256;&#32479;&#25968;&#25454;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#21033;&#29992;&#24179;&#28369;&#24230;&#35825;&#23548;&#27491;&#21017;&#21270;&#65292;&#35813;&#26041;&#27861;&#20165;&#32435;&#20837;&#27169;&#25311;&#20449;&#21495;
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.06945v2 Announce Type: replace-cross  Abstract: Underwater acoustic target recognition is a challenging task owing to the intricate underwater environments and limited data availability. Insufficient data can hinder the ability of recognition systems to support complex modeling, thus impeding their advancement. To improve the generalization capacity of recognition models, techniques such as data augmentation have been employed to simulate underwater signals and diversify data distribution. However, the complexity of underwater environments can cause the simulated signals to deviate from real scenarios, resulting in biased models that are misguided by non-true data. In this study, we propose two strategies to enhance the generalization ability of models in the case of limited data while avoiding the risk of performance degradation. First, as an alternative to traditional data augmentation, we utilize smoothness-inducing regularization, which only incorporates simulated signal
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#22312;&#28608;&#21169;&#24335;&#36172;&#21338;&#26426;&#25506;&#32034;&#20013;&#36890;&#36807;&#32447;&#24615;&#36172;&#21338;&#26426;&#27169;&#22411;&#26367;&#20195;&#20808;&#39564;&#29420;&#31435;&#24615;&#26465;&#20214;&#65292;&#25552;&#39640;&#20102;&#39640;&#32500;&#21160;&#20316;&#31354;&#38388;&#19979;&#30340;&#28608;&#21169;&#25506;&#32034;&#25928;&#29575;&#21644;&#26368;&#20248;&#36951;&#25022;&#65292;&#21516;&#26102;&#25913;&#36827;&#20102;&#21322;&#36172;&#21338;&#27169;&#22411;&#20013;&#20851;&#20110;&#21021;&#22987;&#25968;&#25454;&#25910;&#38598;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;</title><link>https://arxiv.org/abs/2306.01990</link><description>&lt;p&gt;
&#36890;&#36807;&#32447;&#24615;&#19978;&#19979;&#25991;&#21644;&#32452;&#21512;&#21160;&#20316;&#28608;&#21169;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Incentivizing Exploration with Linear Contexts and Combinatorial Actions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.01990
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#22312;&#28608;&#21169;&#24335;&#36172;&#21338;&#26426;&#25506;&#32034;&#20013;&#36890;&#36807;&#32447;&#24615;&#36172;&#21338;&#26426;&#27169;&#22411;&#26367;&#20195;&#20808;&#39564;&#29420;&#31435;&#24615;&#26465;&#20214;&#65292;&#25552;&#39640;&#20102;&#39640;&#32500;&#21160;&#20316;&#31354;&#38388;&#19979;&#30340;&#28608;&#21169;&#25506;&#32034;&#25928;&#29575;&#21644;&#26368;&#20248;&#36951;&#25022;&#65292;&#21516;&#26102;&#25913;&#36827;&#20102;&#21322;&#36172;&#21338;&#27169;&#22411;&#20013;&#20851;&#20110;&#21021;&#22987;&#25968;&#25454;&#25910;&#38598;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25512;&#36827;&#20102;&#28608;&#21169;&#24335;&#36172;&#21338;&#26426;&#25506;&#32034;&#30340;&#30740;&#31350;&#65292;&#20854;&#20013;&#25163;&#33218;&#36873;&#25321;&#34987;&#35270;&#20026;&#25512;&#33616;&#65292;&#24182;&#19988;&#35201;&#27714;&#26159;&#36125;&#21494;&#26031;&#28608;&#21169;&#20860;&#23481;&#30340;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#22312;&#28385;&#36275;&#19968;&#23450;&#29420;&#31435;&#24615;&#20551;&#35774;&#21518;&#65292;&#32463;&#36807;&#36275;&#22815;&#30340;&#21021;&#22987;&#26679;&#26412;&#25910;&#38598;&#65292;&#27969;&#34892;&#30340;&#27748;&#26222;&#26862;&#25277;&#26679;&#31639;&#27861;&#21464;&#24471;&#28608;&#21169;&#20860;&#23481;&#12290;&#25105;&#20204;&#20026;&#32447;&#24615;&#36172;&#21338;&#26426;&#25552;&#20379;&#20102;&#36825;&#20010;&#32467;&#26524;&#30340;&#31867;&#27604;&#65292;&#20854;&#20013;&#20808;&#39564;&#30340;&#29420;&#31435;&#24615;&#34987;&#33258;&#28982;&#30340;&#20984;&#24615;&#26465;&#20214;&#21462;&#20195;&#12290;&#36825;&#25171;&#24320;&#20102;&#22312;&#39640;&#32500;&#21160;&#20316;&#31354;&#38388;&#20013;&#39640;&#25928;&#21644;&#36951;&#25022;&#26368;&#20248;&#30340;&#28608;&#21169;&#25506;&#32034;&#30340;&#21487;&#33021;&#24615;&#12290;&#22312;&#21322;&#36172;&#21338;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#36824;&#25913;&#36827;&#20102;&#29992;&#20110;&#21021;&#22987;&#25968;&#25454;&#25910;&#38598;&#30340;&#21069;&#27748;&#26222;&#26862;&#25277;&#26679;&#38454;&#27573;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.01990v2 Announce Type: replace-cross  Abstract: We advance the study of incentivized bandit exploration, in which arm choices are viewed as recommendations and are required to be Bayesian incentive compatible. Recent work has shown under certain independence assumptions that after collecting enough initial samples, the popular Thompson sampling algorithm becomes incentive compatible. We give an analog of this result for linear bandits, where the independence of the prior is replaced by a natural convexity condition. This opens up the possibility of efficient and regret-optimal incentivized exploration in high-dimensional action spaces. In the semibandit model, we also improve the sample complexity for the pre-Thompson sampling phase of initial data collection.
&lt;/p&gt;</description></item><item><title>&#25918;&#23485;&#20102;&#32467;&#26500;&#24615;&#20551;&#35774;&#65292;&#30740;&#31350;&#20102;&#24694;&#24847;&#23458;&#25143;&#31471;&#19981;&#21487;&#29992;&#30340;&#24773;&#20917;&#65292;&#24341;&#20837;&#20102;$\epsilon$-&#23545;&#25163;&#20002;&#21253;&#20998;&#25968;&#30340;&#27010;&#24565;&#12290;</title><link>https://arxiv.org/abs/2305.19971</link><description>&lt;p&gt;
&#22312;&#23384;&#22312;&#24694;&#24847;&#23458;&#25143;&#19981;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Learning in the Presence of Adversarial Client Unavailability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.19971
&lt;/p&gt;
&lt;p&gt;
&#25918;&#23485;&#20102;&#32467;&#26500;&#24615;&#20551;&#35774;&#65292;&#30740;&#31350;&#20102;&#24694;&#24847;&#23458;&#25143;&#31471;&#19981;&#21487;&#29992;&#30340;&#24773;&#20917;&#65292;&#24341;&#20837;&#20102;$\epsilon$-&#23545;&#25163;&#20002;&#21253;&#20998;&#25968;&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#20998;&#25955;&#24335;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#19981;&#27844;&#38706;&#21407;&#22987;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#21327;&#20316;&#27169;&#22411;&#35757;&#32451;&#12290;&#30001;&#20110;&#21508;&#31181;&#30828;&#20214;&#21644;&#36719;&#20214;&#38480;&#21046;&#65292;&#23458;&#25143;&#31471;&#21487;&#33021;&#24182;&#19981;&#24635;&#26159;&#21487;&#20197;&#21709;&#24212;&#21442;&#25968;&#26381;&#21153;&#22120;&#30340;&#35745;&#31639;&#35831;&#27714;&#12290;&#26032;&#20852;&#30340;&#30740;&#31350;&#26041;&#21521;&#33268;&#21147;&#20110;&#35299;&#20915;&#20219;&#24847;&#23458;&#25143;&#31471;&#19981;&#21487;&#29992;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#24037;&#20316;&#20173;&#23545;&#19981;&#21487;&#29992;&#27169;&#24335;&#26045;&#21152;&#32467;&#26500;&#24615;&#20551;&#35774;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#19981;&#21463;&#21442;&#25968;&#26381;&#21153;&#22120;&#25511;&#21046;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#24773;&#26223;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#27492;&#22806;&#65292;&#22312;&#20687;&#25112;&#22330;&#36825;&#26679;&#24694;&#21155;&#30340;&#29615;&#22659;&#20013;&#65292;&#23545;&#25163;&#21487;&#20197;&#36873;&#25321;&#24615;&#22320;&#21644;&#33258;&#36866;&#24212;&#22320;&#20351;&#29305;&#23450;&#23458;&#25143;&#31471;&#27785;&#40664;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25918;&#23485;&#20102;&#32467;&#26500;&#24615;&#20551;&#35774;&#65292;&#24182;&#32771;&#34385;&#20102;&#24694;&#24847;&#23458;&#25143;&#31471;&#19981;&#21487;&#29992;&#24615;&#12290;&#20026;&#20102;&#37327;&#21270;&#23458;&#25143;&#31471;&#19981;&#21487;&#29992;&#30340;&#31243;&#24230;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;$\epsilon$-&#23545;&#25163;&#20002;&#21253;&#20998;&#25968;&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.19971v2 Announce Type: replace  Abstract: Federated learning is a decentralized machine learning framework that enables collaborative model training without revealing raw data. Due to the diverse hardware and software limitations, a client may not always be available for the computation requests from the parameter server. An emerging line of research is devoted to tackling arbitrary client unavailability. However, existing work still imposes structural assumptions on the unavailability patterns, impeding their applicability in challenging scenarios wherein the unavailability patterns are beyond the control of the parameter server. Moreover, in harsh environments like battlefields, adversaries can selectively and adaptively silence specific clients. In this paper, we relax the structural assumptions and consider adversarial client unavailability. To quantify the degrees of client unavailability, we use the notion of $\epsilon$-adversary dropout fraction. We show that simple v
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#8220;&#23398;&#20064;&#19981;&#21518;&#24724;&#8221;&#30340;&#26694;&#26550;&#65292;&#21487;&#21152;&#36895;&#22312;&#31867;&#20284;&#20294;&#19981;&#30456;&#21516;&#28216;&#25103;&#20998;&#24067;&#19978;&#30340;&#22343;&#34913;&#23547;&#25214;&#65292;&#21516;&#26102;&#22312;&#20219;&#20309;&#28216;&#25103;&#19978;&#20855;&#26377;&#20943;&#23567;&#21518;&#24724;&#30340;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2303.01074</link><description>&lt;p&gt;
&#23398;&#20064;&#19981;&#21518;&#24724;
&lt;/p&gt;
&lt;p&gt;
Learning not to Regret
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2303.01074
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#8220;&#23398;&#20064;&#19981;&#21518;&#24724;&#8221;&#30340;&#26694;&#26550;&#65292;&#21487;&#21152;&#36895;&#22312;&#31867;&#20284;&#20294;&#19981;&#30456;&#21516;&#28216;&#25103;&#20998;&#24067;&#19978;&#30340;&#22343;&#34913;&#23547;&#25214;&#65292;&#21516;&#26102;&#22312;&#20219;&#20309;&#28216;&#25103;&#19978;&#20855;&#26377;&#20943;&#23567;&#21518;&#24724;&#30340;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21338;&#24328;&#35770;&#22343;&#34913;&#30740;&#31350;&#25991;&#29486;&#20027;&#35201;&#38598;&#20013;&#22312;&#21333;&#20010;&#28216;&#25103;&#25110;&#20854;&#37325;&#22797;&#28216;&#25103;&#19978;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#22330;&#26223;&#28041;&#21450;&#29609;&#19968;&#20010;&#21462;&#33258;&#31867;&#20284;&#20294;&#19981;&#30456;&#21516;&#28216;&#25103;&#20998;&#24067;&#30340;&#28216;&#25103;&#65292;&#27604;&#22914;&#29992;&#19981;&#21516;&#20844;&#20849;&#29260;&#29609;&#25169;&#20811;&#25110;&#22312;&#32929;&#31080;&#24066;&#22330;&#19978;&#20132;&#26131;&#30456;&#20851;&#36164;&#20135;&#12290;&#30001;&#20110;&#36825;&#20123;&#30456;&#20284;&#28216;&#25103;&#20855;&#26377;&#30456;&#20284;&#30340;&#22343;&#34913;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#36825;&#31181;&#20998;&#24067;&#19978;&#21152;&#36895;&#23547;&#25214;&#22343;&#34913;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#8220;&#23398;&#20250;&#19981;&#21518;&#24724;&#8221;&#30340;&#26694;&#26550;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#20803;&#23398;&#20064;&#38024;&#23545;&#29305;&#23450;&#20998;&#24067;&#37327;&#36523;&#23450;&#21046;&#30340;&#20943;&#23567;&#21518;&#24724;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#65292;&#21363;&#31070;&#32463;&#39044;&#27979;&#21518;&#24724;&#21305;&#37197;&#65292;&#26159;&#29420;&#29305;&#22320;&#20803;&#23398;&#20064;&#65292;&#33021;&#22815;&#22312;&#25152;&#36873;&#25321;&#30340;&#28216;&#25103;&#20998;&#24067;&#19978;&#24555;&#36895;&#25910;&#25947;&#65292;&#21516;&#26102;&#22312;&#20219;&#20309;&#28216;&#25103;&#19978;&#20855;&#26377;&#20943;&#23567;&#21518;&#24724;&#30340;&#20445;&#35777;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#31639;&#27861;&#22312;&#19968;&#32452;&#20844;&#20849;&#29260;&#25169;&#20811;&#28216;&#25103;&#20998;&#24067;&#19978;&#26356;&#24555;&#22320;&#25910;&#25947;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#20803;&#23398;&#20064;&#30340;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2303.01074v2 Announce Type: replace-cross  Abstract: The literature on game-theoretic equilibrium finding predominantly focuses on single games or their repeated play. Nevertheless, numerous real-world scenarios feature playing a game sampled from a distribution of similar, but not identical games, such as playing poker with different public cards or trading correlated assets on the stock market. As these similar games feature similar equilibra, we investigate a way to accelerate equilibrium finding on such a distribution. We present a novel "learning not to regret" framework, enabling us to meta-learn a regret minimizer tailored to a specific distribution. Our key contribution, Neural Predictive Regret Matching, is uniquely meta-learned to converge rapidly for the chosen distribution of games, while having regret minimization guarantees on any game. We validated our algorithms' faster convergence on a distribution of river poker games. Our experiments show that the meta-learned 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;GAN&#29983;&#25104;&#22120;&#25340;&#25509;&#32593;&#32476;&#24182;&#23454;&#26102;&#21453;&#36716;&#30340;&#26041;&#27861;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#35821;&#20041;&#20998;&#21106;&#32593;&#32476;&#20013;&#34920;&#29616;&#20986;&#19982;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#20294;&#22788;&#29702;&#26102;&#38388;&#24555;&#20004;&#20010;&#25968;&#37327;&#32423;&#12290;</title><link>https://arxiv.org/abs/2302.02181</link><description>&lt;p&gt;
&#27169;&#22411;&#25340;&#25509;&#19982;&#21487;&#35270;&#21270;&#65306;&#22914;&#20309;&#21033;&#29992;GAN&#29983;&#25104;&#22120;&#23454;&#26102;&#21453;&#36716;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Model Stitching and Visualization How GAN Generators can Invert Networks in Real-Time
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.02181
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;GAN&#29983;&#25104;&#22120;&#25340;&#25509;&#32593;&#32476;&#24182;&#23454;&#26102;&#21453;&#36716;&#30340;&#26041;&#27861;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#35821;&#20041;&#20998;&#21106;&#32593;&#32476;&#20013;&#34920;&#29616;&#20986;&#19982;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#20294;&#22788;&#29702;&#26102;&#38388;&#24555;&#20004;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#20934;&#30830;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20998;&#31867;&#21644;&#35821;&#20041;&#20998;&#21106;&#32593;&#32476;&#19982;&#21033;&#29992;1x1&#21367;&#31215;&#30340;GAN&#29983;&#25104;&#22120;&#25340;&#25509;&#36215;&#26469;&#37325;&#24314;&#28608;&#27963;&#12290;&#25105;&#20204;&#22312;AFHQ&#37326;&#29983;&#25968;&#25454;&#38598;&#12289;ImageNet1K&#30340;&#21160;&#29289;&#22270;&#20687;&#20197;&#21450;&#26579;&#33394;&#32452;&#32455;&#26679;&#26412;&#30340;&#30495;&#23454;&#25968;&#23383;&#30149;&#29702;&#25195;&#25551;&#22270;&#20687;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#24050;&#24314;&#31435;&#30340;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24615;&#33021;&#30456;&#24403;&#65292;&#20294;&#22788;&#29702;&#26102;&#38388;&#24555;&#20004;&#20010;&#25968;&#37327;&#32423;&#65292;&#20351;&#24471;&#36825;&#31181;&#26041;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20855;&#26377;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.02181v2 Announce Type: replace-cross  Abstract: In this work, we propose a fast and accurate method to reconstruct activations of classification and semantic segmentation networks by stitching them with a GAN generator utilizing a 1x1 convolution. We test our approach on images of animals from the AFHQ wild dataset, ImageNet1K, and real-world digital pathology scans of stained tissue samples. Our results show comparable performance to established gradient descent methods but with a processing time that is two orders of magnitude faster, making this approach promising for practical applications.
&lt;/p&gt;</description></item><item><title>&#22312;&#33258;&#22238;&#24402;&#36807;&#31243;&#25511;&#21046;&#30340;&#22870;&#21169;&#19979;&#65292;&#25552;&#20986;&#20102;&#33258;&#22238;&#24402;&#36172;&#21338;&#26426;&#65288;ARBs&#65289;&#22312;&#32447;&#23398;&#20064;&#35774;&#32622;&#65292;&#24182;&#35774;&#35745;&#20102;AutoRegressive Upper Confidence Bound (AR-UCB)&#31639;&#27861;&#65292;&#21487;&#20197;&#26041;&#20415;&#35745;&#31639;&#26368;&#20248;&#31574;&#30053;&#24182;&#20855;&#26377;&#27425;&#32447;&#24615;&#36951;&#25022;&#12290;</title><link>https://arxiv.org/abs/2212.06251</link><description>&lt;p&gt;
&#33258;&#22238;&#24402;&#36172;&#21338;&#26426;
&lt;/p&gt;
&lt;p&gt;
Autoregressive Bandits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2212.06251
&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#22238;&#24402;&#36807;&#31243;&#25511;&#21046;&#30340;&#22870;&#21169;&#19979;&#65292;&#25552;&#20986;&#20102;&#33258;&#22238;&#24402;&#36172;&#21338;&#26426;&#65288;ARBs&#65289;&#22312;&#32447;&#23398;&#20064;&#35774;&#32622;&#65292;&#24182;&#35774;&#35745;&#20102;AutoRegressive Upper Confidence Bound (AR-UCB)&#31639;&#27861;&#65292;&#21487;&#20197;&#26041;&#20415;&#35745;&#31639;&#26368;&#20248;&#31574;&#30053;&#24182;&#20855;&#26377;&#27425;&#32447;&#24615;&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#22238;&#24402;&#36807;&#31243;&#22312;&#32929;&#31080;&#24066;&#22330;&#12289;&#38144;&#21806;&#39044;&#27979;&#12289;&#22825;&#27668;&#39044;&#27979;&#12289;&#24191;&#21578;&#21644;&#23450;&#20215;&#31561;&#21508;&#31181;&#23454;&#38469;&#22330;&#26223;&#20013;&#33258;&#28982;&#32780;&#28982;&#22320;&#20986;&#29616;&#12290;&#22312;&#38754;&#23545;&#36825;&#26679;&#30340;&#24207;&#36143;&#20915;&#31574;&#38382;&#39064;&#26102;&#65292;&#24212;&#35813;&#27491;&#30830;&#32771;&#34385;&#36830;&#32493;&#35266;&#27979;&#20043;&#38388;&#30340;&#26102;&#38388;&#20381;&#36182;&#24615;&#65292;&#20197;&#20445;&#35777;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22312;&#32447;&#23398;&#20064;&#35774;&#32622;&#65292;&#21363;&#33258;&#22238;&#24402;&#36172;&#21338;&#26426;&#65288;ARBs&#65289;&#65292;&#20854;&#20013;&#35266;&#27979;&#21040;&#30340;&#22870;&#21169;&#30001;&#19968;&#20010;&#38454;&#25968;&#20026;$k$&#30340;&#33258;&#22238;&#24402;&#36807;&#31243;&#25511;&#21046;&#65292;&#20854;&#21442;&#25968;&#21462;&#20915;&#20110;&#36873;&#25321;&#30340;&#21160;&#20316;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#23545;&#22870;&#21169;&#36807;&#31243;&#36827;&#34892;&#28201;&#21644;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#26368;&#20248;&#31574;&#30053;&#21487;&#20197;&#26041;&#20415;&#22320;&#35745;&#31639;&#20986;&#26469;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#20048;&#35266;&#36951;&#25022;&#26368;&#23567;&#21270;&#31639;&#27861;&#65292;&#21363;&#33258;&#22238;&#24402;&#19978;&#32622;&#20449;&#30028;&#65288;AR-UCB&#65289;&#65292;&#20854;&#36951;&#25022;&#21576;&#29616;&#20986;&#27425;&#32447;&#24615;&#30340;&#38454;&#25968;$\widetilde{\mathcal{O}} \left( \frac{(k+1)^{3/2}\sqrt{nT}}{(1-\Gamma)^
&lt;/p&gt;
&lt;p&gt;
arXiv:2212.06251v2 Announce Type: replace  Abstract: Autoregressive processes naturally arise in a large variety of real-world scenarios, including stock markets, sales forecasting, weather prediction, advertising, and pricing. When facing a sequential decision-making problem in such a context, the temporal dependence between consecutive observations should be properly accounted for guaranteeing convergence to the optimal policy. In this work, we propose a novel online learning setting, namely, Autoregressive Bandits (ARBs), in which the observed reward is governed by an autoregressive process of order $k$, whose parameters depend on the chosen action. We show that, under mild assumptions on the reward process, the optimal policy can be conveniently computed. Then, we devise a new optimistic regret minimization algorithm, namely, AutoRegressive Upper Confidence Bound (AR-UCB), that suffers sublinear regret of order $\widetilde{\mathcal{O}} \left( \frac{(k+1)^{3/2}\sqrt{nT}}{(1-\Gamma)^
&lt;/p&gt;</description></item><item><title>&#22270;&#28388;&#27874;&#22120;&#26159;&#20026;&#22788;&#29702;&#21644;&#23398;&#20064;&#32593;&#32476;&#21644;&#20854;&#20182;&#19981;&#35268;&#21017;&#22495;&#25968;&#25454;&#32780;&#35774;&#35745;&#30340;&#65292;&#21487;&#20197;&#22686;&#24378;&#34920;&#31034;&#33021;&#21147;&#20197;&#27169;&#25311;&#26356;&#24191;&#27867;&#30340;&#20449;&#21495;&#31867;&#12289;&#25968;&#25454;&#27169;&#24335;&#21644;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2211.08854</link><description>&lt;p&gt;
&#22270;&#28388;&#27874;&#22120;&#22312;&#22270;&#20449;&#21495;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Graph Filters for Signal Processing and Machine Learning on Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.08854
&lt;/p&gt;
&lt;p&gt;
&#22270;&#28388;&#27874;&#22120;&#26159;&#20026;&#22788;&#29702;&#21644;&#23398;&#20064;&#32593;&#32476;&#21644;&#20854;&#20182;&#19981;&#35268;&#21017;&#22495;&#25968;&#25454;&#32780;&#35774;&#35745;&#30340;&#65292;&#21487;&#20197;&#22686;&#24378;&#34920;&#31034;&#33021;&#21147;&#20197;&#27169;&#25311;&#26356;&#24191;&#27867;&#30340;&#20449;&#21495;&#31867;&#12289;&#25968;&#25454;&#27169;&#24335;&#21644;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28388;&#27874;&#22120;&#22312;&#20174;&#25968;&#25454;&#20013;&#25552;&#21462;&#20449;&#24687;&#20013;&#36215;&#30528;&#22522;&#30784;&#20316;&#29992;&#12290;&#23545;&#20110;&#23384;&#22312;&#20110;&#27431;&#27663;&#22495;&#19978;&#30340;&#26102;&#38388;&#24207;&#21015;&#21644;&#22270;&#20687;&#25968;&#25454;&#65292;&#28388;&#27874;&#22120;&#26159;&#35768;&#22810;&#20449;&#21495;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#20851;&#38190;&#65292;&#21253;&#25324;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;&#36234;&#26469;&#36234;&#22810;&#30340;&#29616;&#20195;&#25968;&#25454;&#23384;&#22312;&#20110;&#32593;&#32476;&#21644;&#20854;&#20182;&#19981;&#35268;&#21017;&#22495;&#65292;&#20854;&#32467;&#26500;&#26356;&#36866;&#21512;&#29992;&#22270;&#34920;&#31034;&#12290;&#20026;&#20102;&#22788;&#29702;&#21644;&#23398;&#20064;&#36825;&#20123;&#25968;&#25454;&#65292;&#22270;&#28388;&#27874;&#22120;&#32771;&#34385;&#20102;&#24213;&#23618;&#25968;&#25454;&#22495;&#30340;&#32467;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#20171;&#32461;&#20102;&#22270;&#28388;&#27874;&#22120;&#65292;&#21253;&#25324;&#19981;&#21516;&#30340;&#28388;&#27874;&#22120;&#31867;&#21035;&#65292;&#27599;&#31181;&#31867;&#22411;&#30340;&#35774;&#35745;&#31574;&#30053;&#65292;&#20197;&#21450;&#19981;&#21516;&#31867;&#22411;&#22270;&#28388;&#27874;&#22120;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#35752;&#35770;&#22914;&#20309;&#23558;&#22270;&#28388;&#27874;&#22120;&#25193;&#23637;&#21040;&#28388;&#27874;&#22120;&#32452;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#22686;&#24378;&#34920;&#31034;&#33021;&#21147;&#65307;&#20063;&#23601;&#26159;&#35828;&#65292;&#27169;&#25311;&#26356;&#24191;&#27867;&#30340;&#20449;&#21495;&#31867;&#12289;&#25968;&#25454;&#27169;&#24335;&#21644;&#20851;&#31995;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22270;&#28388;&#27874;&#22120;&#22312;&#25552;&#39640;&#34920;&#24449;&#33021;&#21147;&#26041;&#38754;&#30340;&#22522;&#26412;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
arXiv:2211.08854v2 Announce Type: replace-cross  Abstract: Filters are fundamental in extracting information from data. For time series and image data that reside on Euclidean domains, filters are the crux of many signal processing and machine learning techniques, including convolutional neural networks. Increasingly, modern data also reside on networks and other irregular domains whose structure is better captured by a graph. To process and learn from such data, graph filters account for the structure of the underlying data domain. In this article, we provide a comprehensive overview of graph filters, including the different filtering categories, design strategies for each type, and trade-offs between different types of graph filters. We discuss how to extend graph filters into filter banks and graph neural networks to enhance the representational power; that is, to model a broader variety of signal classes, data patterns, and relationships. We also showcase the fundamental role of gr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35774;&#35745;&#21644;&#20998;&#26512;&#20102;&#37327;&#23376;&#21464;&#21387;&#22120;&#65292;&#24341;&#20837;&#20102;&#19977;&#31181;&#31867;&#22411;&#30340;&#37327;&#23376;&#21464;&#21387;&#22120;&#29992;&#20110;&#35757;&#32451;&#21644;&#25512;&#29702;&#65292;&#22312;&#20445;&#35777;&#37327;&#23376;&#27880;&#24847;&#26426;&#21046;&#20855;&#26377;&#29702;&#35770;&#20248;&#21183;&#30340;&#22522;&#30784;&#19978;&#65292;&#37319;&#29992;&#27973;&#37327;&#23376;&#30005;&#36335;&#26500;&#24314;&#65292;&#20135;&#29983;&#19981;&#21516;&#30340;&#20998;&#31867;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2209.08167</link><description>&lt;p&gt;
&#37327;&#23376;&#35270;&#35273;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
Quantum Vision Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.08167
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35774;&#35745;&#21644;&#20998;&#26512;&#20102;&#37327;&#23376;&#21464;&#21387;&#22120;&#65292;&#24341;&#20837;&#20102;&#19977;&#31181;&#31867;&#22411;&#30340;&#37327;&#23376;&#21464;&#21387;&#22120;&#29992;&#20110;&#35757;&#32451;&#21644;&#25512;&#29702;&#65292;&#22312;&#20445;&#35777;&#37327;&#23376;&#27880;&#24847;&#26426;&#21046;&#20855;&#26377;&#29702;&#35770;&#20248;&#21183;&#30340;&#22522;&#30784;&#19978;&#65292;&#37319;&#29992;&#27973;&#37327;&#23376;&#30005;&#36335;&#26500;&#24314;&#65292;&#20135;&#29983;&#19981;&#21516;&#30340;&#20998;&#31867;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25193;&#23637;&#24050;&#30693;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22270;&#20687;&#20998;&#26512;&#20013;&#38750;&#24120;&#39640;&#25928;&#30340;&#26368;&#26032;&#32463;&#20856;&#21464;&#21387;&#22120;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#35774;&#35745;&#24182;&#35814;&#32454;&#20998;&#26512;&#20102;&#37327;&#23376;&#21464;&#21387;&#22120;&#12290;&#22312;&#20043;&#21069;&#20351;&#29992;&#21442;&#25968;&#21270;&#37327;&#23376;&#30005;&#36335;&#36827;&#34892;&#25968;&#25454;&#21152;&#36733;&#21644;&#27491;&#20132;&#31070;&#32463;&#23618;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19977;&#31181;&#31867;&#22411;&#30340;&#37327;&#23376;&#21464;&#21387;&#22120;&#29992;&#20110;&#35757;&#32451;&#21644;&#25512;&#29702;&#65292;&#21253;&#25324;&#22522;&#20110;&#22797;&#21512;&#30697;&#38453;&#30340;&#37327;&#23376;&#21464;&#21387;&#22120;&#65292;&#23427;&#30830;&#20445;&#20102;&#37327;&#23376;&#27880;&#24847;&#26426;&#21046;&#22312;&#28176;&#36817;&#36816;&#34892;&#26102;&#38388;&#21644;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#26041;&#38754;&#30456;&#36739;&#20110;&#23427;&#20204;&#30340;&#32463;&#20856;&#23545;&#24212;&#29289;&#23384;&#22312;&#29702;&#35770;&#20248;&#21183;&#12290;&#36825;&#20123;&#37327;&#23376;&#26550;&#26500;&#21487;&#20197;&#20351;&#29992;&#27973;&#37327;&#23376;&#30005;&#36335;&#26500;&#24314;&#65292;&#24182;&#20135;&#29983;&#23450;&#24615;&#19981;&#21516;&#30340;&#20998;&#31867;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2209.08167v2 Announce Type: replace-cross  Abstract: In this work, quantum transformers are designed and analysed in detail by extending the state-of-the-art classical transformer neural network architectures known to be very performant in natural language processing and image analysis. Building upon the previous work, which uses parametrised quantum circuits for data loading and orthogonal neural layers, we introduce three types of quantum transformers for training and inference, including a quantum transformer based on compound matrices, which guarantees a theoretical advantage of the quantum attention mechanism compared to their classical counterpart both in terms of asymptotic run time and the number of model parameters. These quantum architectures can be built using shallow quantum circuits and produce qualitatively different classification models. The three proposed quantum attention layers vary on the spectrum between closely following the classical transformers and exhibi
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#23398;&#26694;&#26550;&#65292;&#25506;&#35752;&#20102;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#31639;&#27861;&#36807;&#28388;&#30340;&#19981;&#33391;&#24433;&#21709;&#20197;&#21450;&#30417;&#31649;&#22797;&#26434;&#24615;&#12290;</title><link>https://arxiv.org/abs/2209.05550</link><description>&lt;p&gt;
&#22312;&#32447;&#31038;&#20132;&#23186;&#20307;&#23457;&#35745;&#30340;&#25968;&#23398;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Mathematical Framework for Online Social Media Auditing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.05550
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#23398;&#26694;&#26550;&#65292;&#25506;&#35752;&#20102;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#31639;&#27861;&#36807;&#28388;&#30340;&#19981;&#33391;&#24433;&#21709;&#20197;&#21450;&#30417;&#31649;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#65288;SMPs&#65289;&#21033;&#29992;&#31639;&#27861;&#36807;&#28388;&#65288;AF&#65289;&#26469;&#36873;&#25321;&#26500;&#25104;&#29992;&#25143;&#20449;&#24687;&#27969;&#30340;&#20869;&#23481;&#65292;&#26088;&#22312;&#26368;&#22823;&#21270;&#22870;&#21169;&#12290;&#26377;&#36873;&#25321;&#22320;&#36873;&#25321;&#35201;&#26174;&#31034;&#22312;&#29992;&#25143;&#20449;&#24687;&#27969;&#20013;&#30340;&#20869;&#23481;&#21487;&#33021;&#20250;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#65292;&#26080;&#35770;&#26159;&#36739;&#23567;&#36824;&#26159;&#36739;&#22823;&#65292;&#23545;&#29992;&#25143;&#30340;&#20915;&#31574;&#20135;&#29983;&#24433;&#21709;&#65292;&#19982;&#22312;&#33258;&#28982;/&#20844;&#24179;&#20869;&#23481;&#36873;&#25321;&#19979;&#20250;&#20135;&#29983;&#30340;&#24433;&#21709;&#30456;&#27604;&#12290;&#27491;&#22914;&#25105;&#20204;&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#25152;&#35265;&#65292;&#31639;&#27861;&#36807;&#28388;&#21487;&#33021;&#23548;&#33268;&#26377;&#23475;&#30340;&#21103;&#20316;&#29992;&#65292;&#20174;&#20559;&#35265;&#20010;&#20154;&#20915;&#23450;&#21040;&#22609;&#36896;&#25972;&#20010;&#31038;&#20250;&#30340;&#20915;&#23450;&#65292;&#20363;&#22914;&#65292;&#23558;&#29992;&#25143;&#27880;&#24847;&#21147;&#20174;&#26159;&#21542;&#25509;&#31181;COVID-19&#30123;&#33495;&#36716;&#31227;&#25110;&#35825;&#20351;&#20844;&#20247;&#36873;&#25321;&#24635;&#32479;&#20505;&#36873;&#20154;&#12290;&#25919;&#24220;&#24120;&#24120;&#35797;&#22270;&#30417;&#31649;AF&#30340;&#19981;&#33391;&#24433;&#21709;&#65292;&#20294;&#24448;&#24448;&#22240;&#23448;&#20698;&#20027;&#20041;&#12289;&#27861;&#24459;&#20107;&#21153;&#21644;&#36130;&#21153;&#32771;&#34385;&#32780;&#21464;&#24471;&#22797;&#26434;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;SMPs&#23547;&#27714;&#30417;&#27979;&#20182;&#20204;&#33258;&#24049;&#30340;&#31639;
&lt;/p&gt;
&lt;p&gt;
arXiv:2209.05550v2 Announce Type: replace  Abstract: Social media platforms (SMPs) leverage algorithmic filtering (AF) as a means of selecting the content that constitutes a user's feed with the aim of maximizing their rewards. Selectively choosing the contents to be shown on the user's feed may yield a certain extent of influence, either minor or major, on the user's decision-making, compared to what it would have been under a natural/fair content selection. As we have witnessed over the past decade, algorithmic filtering can cause detrimental side effects, ranging from biasing individual decisions to shaping those of society as a whole, for example, diverting users' attention from whether to get the COVID-19 vaccine or inducing the public to choose a presidential candidate. The government's constant attempts to regulate the adverse effects of AF are often complicated, due to bureaucracy, legal affairs, and financial considerations. On the other hand SMPs seek to monitor their own alg
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#28508;&#22312;&#30340;&#24341;&#23548;&#28857;&#21464;&#37327;&#24182;&#36873;&#25321;&#21512;&#36866;&#30340;&#36793;&#32536;&#23545;&#25968;&#20284;&#28982;&#30446;&#26631;&#65292;&#21487;&#22312;&#20998;&#23376;&#25551;&#36848;&#31526;&#31354;&#38388;&#20013;&#25913;&#36827;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2207.07654</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#25193;&#23637;&#30340;&#21464;&#20998;&#39640;&#26031;&#36807;&#31243;&#22312;&#20998;&#23376;&#25968;&#25454;&#19978;&#23398;&#20064;&#24341;&#23548;&#28857;&#21644;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Learning inducing points and uncertainty on molecular data by scalable variational Gaussian processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2207.07654
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#28508;&#22312;&#30340;&#24341;&#23548;&#28857;&#21464;&#37327;&#24182;&#36873;&#25321;&#21512;&#36866;&#30340;&#36793;&#32536;&#23545;&#25968;&#20284;&#28982;&#30446;&#26631;&#65292;&#21487;&#22312;&#20998;&#23376;&#25551;&#36848;&#31526;&#31354;&#38388;&#20013;&#25913;&#36827;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#25511;&#21046;&#21644;&#21487;&#25193;&#23637;&#24615;&#26159;&#22312;&#26448;&#26009;&#31185;&#23398;&#21644;&#21270;&#23398;&#20013;&#37096;&#32626;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#27169;&#22411;&#21040;&#33258;&#20027;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#31649;&#36947;&#20013;&#30340;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#12290;&#24341;&#20837;&#28508;&#22312;&#30340;&#24341;&#23548;&#28857;&#21464;&#37327;&#24182;&#36873;&#25321;&#21512;&#36866;&#30340;&#36817;&#20284;&#20197;&#33719;&#24471;&#36793;&#32536;&#23545;&#25968;&#20284;&#28982;&#30446;&#26631;&#26159;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;&#26412;&#25991;&#22312;&#20998;&#23376;&#25551;&#36848;&#31526;&#31354;&#38388;&#20013;&#32463;&#39564;&#24615;&#22320;&#23637;&#31034;&#20102;&#36890;&#36807;&#21464;&#20998;&#23398;&#20064;&#24341;&#23548;&#28857;&#21487;&#20197;&#25552;&#39640;&#20004;&#20010;&#20998;&#23376;&#21160;&#21147;&#23398;&#25968;&#25454;&#38598;&#19978;&#33021;&#37327;&#21644;&#21407;&#23376;&#21147;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#34920;&#26126;&#21464;&#20998;&#39640;&#26031;&#36807;&#31243;&#21487;&#20197;&#23398;&#20064;&#34920;&#31034;&#21021;&#22987;&#21270;&#38598;&#21512;&#20013;&#19981;&#23384;&#22312;&#30340;&#19981;&#21516;&#31867;&#22411;&#20998;&#23376;&#30340;&#26500;&#22411;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19981;&#21516;&#35757;&#32451;&#30446;&#26631;&#21644;&#21464;&#20998;&#20998;&#24067;&#30340;&#27604;&#36739;&#12290;&#22312;&#35780;&#20272;&#20102;&#20960;&#31181;&#36817;&#20284;&#36793;&#32536;&#23545;&#25968;&#20284;&#28982;&#30340;&#35757;&#32451;&#30446;&#26631;&#21644;&#21464;&#20998;&#20998;&#24067;&#21518;
&lt;/p&gt;
&lt;p&gt;
arXiv:2207.07654v3 Announce Type: replace-cross  Abstract: Uncertainty control and scalability to large datasets are the two main issues for the deployment of Gaussian process (GP) models within the autonomous machine learning-based prediction pipelines in material science and chemistry. One way to address both of these issues is by introducing the latent inducing point variables and choosing the right approximation for the marginal log-likelihood objective. Here, we empirically show that variational learning of the inducing points in a molecular descriptor space improves the prediction of energies and atomic forces on two molecular dynamics datasets. First, we show that variational GPs can learn to represent the configurations of the molecules of different types that were not present within the initialization set of configurations. We provide a comparison of alternative log-likelihood training objectives and variational distributions. Among several evaluated approximate marginal log-l
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#22270;&#19978;&#30830;&#23450;&#24615;&#31995;&#32479;&#35782;&#21035;&#38382;&#39064;&#30340;&#36890;&#29992;&#35774;&#32622;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36793;&#30028;&#27880;&#20837;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#36317;&#31163;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#26469;&#31283;&#23450;&#36828;&#31163;&#36793;&#30028;&#30340;&#33410;&#28857;&#30340;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2206.02911</link><description>&lt;p&gt;
&#22270;&#19978;&#30340;&#36870;&#36793;&#30028;&#20540;&#21644;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65306;&#31070;&#32463;&#21644;&#25968;&#20540;&#32508;&#21512;
&lt;/p&gt;
&lt;p&gt;
Inverse Boundary Value and Optimal Control Problems on Graphs: A Neural and Numerical Synthesis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2206.02911
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#22270;&#19978;&#30830;&#23450;&#24615;&#31995;&#32479;&#35782;&#21035;&#38382;&#39064;&#30340;&#36890;&#29992;&#35774;&#32622;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36793;&#30028;&#27880;&#20837;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#36317;&#31163;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#26469;&#31283;&#23450;&#36828;&#31163;&#36793;&#30028;&#30340;&#33410;&#28857;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#20010;&#20851;&#20110;&#22270;&#19978;&#24102;&#26377;Dirichlet&#21644;Neumann&#36793;&#30028;&#26465;&#20214;&#30340;&#30830;&#23450;&#24615;&#31995;&#32479;&#35782;&#21035;&#38382;&#39064;&#30340;&#36890;&#29992;&#35774;&#32622;&#12290;&#24403;&#36793;&#30028;&#19978;&#26377;&#25511;&#21046;&#33410;&#28857;&#26102;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#19968;&#31181;&#31163;&#25955;&#21270;&#28982;&#21518;&#20248;&#21270;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#26368;&#20248;&#25511;&#21046;&#12290;&#24403;&#21069;&#20307;&#31995;&#32467;&#26500;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#37096;&#20998;&#26159;&#25105;&#20204;&#30340;&#36793;&#30028;&#27880;&#20837;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#12290;&#36825;&#23558;&#20135;&#29983;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#22312;&#36793;&#30028;&#38468;&#36817;&#26356;&#21152;&#31283;&#23450;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#36317;&#31163;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#26377;&#21161;&#20110;&#31283;&#23450;&#36828;&#31163;&#36793;&#30028;&#30340;&#33410;&#28857;&#22788;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2206.02911v2 Announce Type: replace  Abstract: A general setup for deterministic system identification problems on graphs with Dirichlet and Neumann boundary conditions is introduced. When control nodes are available along the boundary, we apply a discretize-then-optimize method to estimate an optimal control. A key piece in the present architecture is our boundary injected message passing neural network. This will produce more accurate predictions that are considerably more stable in proximity of the boundary. Also, a regularization technique based on graphical distance is introduced that helps with stabilizing the predictions at nodes far from the boundary.
&lt;/p&gt;</description></item><item><title>&#24378;&#21270;&#23398;&#20064;&#21644;&#22343;&#22330;&#21338;&#24328;&#30340;&#32467;&#21512;&#26377;&#26395;&#22312;&#24456;&#22823;&#35268;&#27169;&#19978;&#35299;&#20915;&#28216;&#25103;&#30340;&#22343;&#34913;&#21644;&#31038;&#20250;&#26368;&#20248;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2205.12944</link><description>&lt;p&gt;
&#22312;&#22343;&#22330;&#21338;&#24328;&#20013;&#30340;&#23398;&#20064;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Learning in Mean Field Games: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2205.12944
&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#21644;&#22343;&#22330;&#21338;&#24328;&#30340;&#32467;&#21512;&#26377;&#26395;&#22312;&#24456;&#22823;&#35268;&#27169;&#19978;&#35299;&#20915;&#28216;&#25103;&#30340;&#22343;&#34913;&#21644;&#31038;&#20250;&#26368;&#20248;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#21512;&#20316;&#21644;&#21512;&#20316;&#28216;&#25103;&#22312;&#25317;&#26377;&#22823;&#37327;&#29609;&#23478;&#26102;&#26377;&#35768;&#22810;&#24212;&#29992;&#65292;&#20294;&#38543;&#30528;&#29609;&#23478;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#36890;&#24120;&#21464;&#24471;&#38590;&#20197;&#35299;&#20915;&#12290;&#22343;&#22330;&#21338;&#24328;(Mean Field Games, MFGs)&#30001;Lasry&#21644;Lions&#20197;&#21450;Huang&#65292;Caines&#21644;Malham\'e&#24341;&#20837;&#65292;&#20381;&#38752;&#22343;&#22330;&#36817;&#20284;&#20801;&#35768;&#29609;&#23478;&#25968;&#37327;&#22686;&#38271;&#21040;&#26080;&#31351;&#22823;&#12290;&#20256;&#32479;&#35299;&#20915;&#36825;&#20123;&#28216;&#25103;&#30340;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#35299;&#20915;&#24102;&#26377;&#23545;&#27169;&#22411;&#30340;&#23436;&#20840;&#20102;&#35299;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#25110;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#12290;&#26368;&#36817;&#65292;&#24378;&#21270;&#23398;&#20064;(Reinforcement Learning, RL)&#20986;&#29616;&#22312;&#35299;&#20915;&#35268;&#27169;&#22797;&#26434;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#20102;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;RL&#21644;MFGs&#30340;&#32467;&#21512;&#26377;&#26395;&#35299;&#20915;&#22312;&#20154;&#21475;&#35268;&#27169;&#21644;&#29615;&#22659;&#22797;&#26434;&#24615;&#26041;&#38754;&#38750;&#24120;&#24222;&#22823;&#30340;&#28216;&#25103;&#12290;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#26368;&#36817;&#36805;&#36895;&#22686;&#38271;&#30340;&#20851;&#20110;RL&#26041;&#27861;&#22312;MFGs&#20013;&#23398;&#20064;&#22343;&#34913;&#21644;&#31038;&#20132;&#26368;&#20248;&#30340;&#25991;&#29486;&#12290;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;&#20102;M&#20013;&#26368;&#24120;&#35265;&#30340;&#35774;&#32622;(&#38745;&#24577;&#12289;&#31283;&#24577;&#21644;&#36827;&#21270;&#30340;)&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2205.12944v3 Announce Type: replace-cross  Abstract: Non-cooperative and cooperative games with a very large number of players have many applications but remain generally intractable when the number of players increases. Introduced by Lasry and Lions, and Huang, Caines and Malham\'e, Mean Field Games (MFGs) rely on a mean-field approximation to allow the number of players to grow to infinity. Traditional methods for solving these games generally rely on solving partial or stochastic differential equations with a full knowledge of the model. Recently, Reinforcement Learning (RL) has appeared promising to solve complex problems at scale. The combination of RL and MFGs is promising to solve games at a very large scale both in terms of population size and environment complexity. In this survey, we review the quickly growing recent literature on RL methods to learn equilibria and social optima in MFGs. We first identify the most common settings (static, stationary, and evolutive) of M
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39044;&#27979;&#35268;&#21010;&#27969;&#31243;&#65292;&#20801;&#35768;&#26426;&#22120;&#20154;&#20027;&#21160;&#26397;&#30528;&#20154;&#31867;&#20195;&#29702;&#30340;&#39044;&#23450;&#25918;&#32622;&#20301;&#32622;&#31227;&#21160;</title><link>https://arxiv.org/abs/2203.00156</link><description>&lt;p&gt;
&#29992;&#20110;&#20154;&#19982;&#26426;&#22120;&#20154;&#38388;&#25509;&#25918;&#32622;&#20132;&#25509;&#30340;&#20027;&#21160;&#36816;&#21160;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Preemptive Motion Planning for Human-to-Robot Indirect Placement Handovers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2203.00156
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39044;&#27979;&#35268;&#21010;&#27969;&#31243;&#65292;&#20801;&#35768;&#26426;&#22120;&#20154;&#20027;&#21160;&#26397;&#30528;&#20154;&#31867;&#20195;&#29702;&#30340;&#39044;&#23450;&#25918;&#32622;&#20301;&#32622;&#31227;&#21160;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25216;&#26415;&#30340;&#36827;&#27493;&#65292;&#23433;&#20840;&#12289;&#39640;&#25928;&#21644;&#21327;&#20316;&#30340;&#20154;&#26426;&#22242;&#38431;&#30340;&#38656;&#27714;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#22312;&#20219;&#20309;&#29615;&#22659;&#20013;&#26368;&#22522;&#26412;&#30340;&#21327;&#20316;&#20219;&#21153;&#20043;&#19968;&#26159;&#29289;&#20307;&#20132;&#25509;&#12290;&#20154;&#19982;&#26426;&#22120;&#20154;&#20043;&#38388;&#30340;&#20132;&#25509;&#21487;&#20197;&#37319;&#29992;&#20004;&#31181;&#26041;&#24335;&#65306;&#65288;1&#65289;&#30452;&#25509;&#25163;&#23545;&#25163;&#25110;&#65288;2&#65289;&#38388;&#25509;&#25163;&#23545;&#25918;&#32622;&#20877;&#25235;&#21462;&#12290;&#21518;&#19968;&#31181;&#26041;&#24335;&#30830;&#20445;&#20102;&#20154;&#19982;&#26426;&#22120;&#20154;&#20043;&#38388;&#30340;&#26368;&#23567;&#25509;&#35302;&#65292;&#20294;&#20063;&#21487;&#33021;&#23548;&#33268;&#30001;&#20110;&#38656;&#35201;&#31561;&#24453;&#29289;&#20307;&#39318;&#20808;&#25918;&#32622;&#22312;&#34920;&#38754;&#19978;&#32780;&#22686;&#21152;&#38386;&#32622;&#26102;&#38388;&#12290;&#20026;&#20102;&#26368;&#23567;&#21270;&#36825;&#31181;&#38386;&#32622;&#26102;&#38388;&#65292;&#26426;&#22120;&#20154;&#24517;&#39035;&#39044;&#27979;&#20154;&#31867;&#24847;&#22270;&#65292;&#21363;&#29289;&#20307;&#23558;&#34987;&#25918;&#32622;&#22312;&#20309;&#22788;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#39044;&#20808;&#20197;&#20219;&#20309;&#19968;&#31181;&#26377;&#29983;&#20135;&#21147;&#30340;&#26041;&#24335;&#34892;&#21160;&#65292;&#39044;&#27979;&#21644;&#36816;&#21160;&#35268;&#21010;&#24517;&#39035;&#23454;&#26102;&#21457;&#29983;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39044;&#27979;&#35268;&#21010;&#27969;&#31243;&#65292;&#20801;&#35768;&#26426;&#22120;&#20154;&#20027;&#21160;&#26397;&#30528;&#20154;&#31867;&#20195;&#29702;&#30340;&#39044;&#23450;&#25918;&#32622;&#20301;&#32622;&#31227;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2203.00156v3 Announce Type: replace-cross  Abstract: As technology advances, the need for safe, efficient, and collaborative human-robot-teams has become increasingly important. One of the most fundamental collaborative tasks in any setting is the object handover. Human-to-robot handovers can take either of two approaches: (1) direct hand-to-hand or (2) indirect hand-to-placement-to-pick-up. The latter approach ensures minimal contact between the human and robot but can also result in increased idle time due to having to wait for the object to first be placed down on a surface. To minimize such idle time, the robot must preemptively predict the human intent of where the object will be placed. Furthermore, for the robot to preemptively act in any sort of productive manner, predictions and motion planning must occur in real-time. We introduce a novel prediction-planning pipeline that allows the robot to preemptively move towards the human agent's intended placement location using g
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#32447;&#24615;&#28388;&#27874;&#22120;&#65292;&#25552;&#20986;&#20102;&#31616;&#21333;&#22797;&#21512;&#21367;&#31215;&#26680;&#65292;&#23450;&#20041;&#20026;&#36739;&#20302;&#21644;&#36739;&#39640;&#38669;&#22855;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#22810;&#39033;&#24335;&#65292;&#20855;&#26377;&#32447;&#24615;&#12289;&#24179;&#31227;&#19981;&#21464;&#12289;&#32622;&#25442;&#21644;&#26041;&#21521;&#31561;&#21464;&#24615;&#65292;&#20197;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#22312;&#20998;&#24067;&#24335;&#26041;&#24335;&#23454;&#29616;&#12290;</title><link>https://arxiv.org/abs/2201.11720</link><description>&lt;p&gt;
&#31616;&#21333;&#22797;&#21512;&#21367;&#31215;&#26680;
&lt;/p&gt;
&lt;p&gt;
Simplicial Convolutional Filters
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2201.11720
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#32447;&#24615;&#28388;&#27874;&#22120;&#65292;&#25552;&#20986;&#20102;&#31616;&#21333;&#22797;&#21512;&#21367;&#31215;&#26680;&#65292;&#23450;&#20041;&#20026;&#36739;&#20302;&#21644;&#36739;&#39640;&#38669;&#22855;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#22810;&#39033;&#24335;&#65292;&#20855;&#26377;&#32447;&#24615;&#12289;&#24179;&#31227;&#19981;&#21464;&#12289;&#32622;&#25442;&#21644;&#26041;&#21521;&#31561;&#21464;&#24615;&#65292;&#20197;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#22312;&#20998;&#24067;&#24335;&#26041;&#24335;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#29992;&#20110;&#22788;&#29702;&#25903;&#25345;&#22312;&#25277;&#35937;&#25299;&#25169;&#31354;&#38388;&#19978;&#30340;&#20449;&#21495;&#30340;&#32447;&#24615;&#28388;&#27874;&#22120;&#65292;&#36825;&#20123;&#25299;&#25169;&#31354;&#38388;&#34987;&#24314;&#27169;&#20026;&#21333;&#32431;&#22797;&#24418;&#65292;&#21487;&#20197;&#34987;&#35299;&#37322;&#20026;&#22270;&#30340;&#25512;&#24191;&#65292;&#32771;&#34385;&#21040;&#33410;&#28857;&#12289;&#36793;&#12289;&#19977;&#35282;&#24418;&#38754;&#31561;&#12290;&#20026;&#20102;&#22788;&#29702;&#36825;&#26679;&#30340;&#20449;&#21495;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#23450;&#20041;&#20026;&#36739;&#20302;&#21644;&#36739;&#39640;&#38669;&#22855;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#22810;&#39033;&#24335;&#30340;&#21333;&#32431;&#22797;&#21512;&#21367;&#31215;&#26680;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2201.11720v3 Announce Type: replace-cross  Abstract: We study linear filters for processing signals supported on abstract topological spaces modeled as simplicial complexes, which may be interpreted as generalizations of graphs that account for nodes, edges, triangular faces etc. To process such signals, we develop simplicial convolutional filters defined as matrix polynomials of the lower and upper Hodge Laplacians. First, we study the properties of these filters and show that they are linear and shift-invariant, as well as permutation and orientation equivariant. These filters can also be implemented in a distributed fashion with a low computational complexity, as they involve only (multiple rounds of) simplicial shifting between upper and lower adjacent simplices. Second, focusing on edge-flows, we study the frequency responses of these filters and examine how we can use the Hodge-decomposition to delineate gradient, curl and harmonic frequencies. We discuss how these frequenc
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#38024;&#23545;&#22823;&#23567;&#32422;&#26463;&#38750;&#21333;&#35843;&#23376;&#27169;&#20989;&#25968;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#25913;&#36827;&#30340;&#23376;&#31243;&#24207;ThreshSeq&#23454;&#29616;&#26356;&#39640;&#30340;&#36924;&#36817;&#27604;&#29575;&#21644;&#26356;&#20302;&#30340;&#22797;&#26434;&#24230;</title><link>https://arxiv.org/abs/2009.01947</link><description>&lt;p&gt;
&#23454;&#29992;&#19988;&#21487;&#24182;&#34892;&#21270;&#30340;&#38750;&#21333;&#35843;&#23376;&#27169;&#26368;&#22823;&#21270;&#31639;&#27861;&#19982;&#22823;&#23567;&#32422;&#26463;
&lt;/p&gt;
&lt;p&gt;
Practical and Parallelizable Algorithms for Non-Monotone Submodular Maximization with Size Constraint
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2009.01947
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#38024;&#23545;&#22823;&#23567;&#32422;&#26463;&#38750;&#21333;&#35843;&#23376;&#27169;&#20989;&#25968;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#25913;&#36827;&#30340;&#23376;&#31243;&#24207;ThreshSeq&#23454;&#29616;&#26356;&#39640;&#30340;&#36924;&#36817;&#27604;&#29575;&#21644;&#26356;&#20302;&#30340;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32452;&#21512;&#21644;&#21487;&#24182;&#34892;&#21270;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#38024;&#23545;&#22823;&#23567;&#32422;&#26463;&#38750;&#21333;&#35843;&#30340;&#23376;&#27169;&#20989;&#25968;&#36827;&#34892;&#26368;&#22823;&#21270;&#12290;&#25105;&#20204;&#25913;&#36827;&#20102;&#19968;&#20010;&#20855;&#26377;&#26368;&#20339;&#36866;&#24212;&#24615;&#21644;&#36817;&#20046;&#26368;&#20339;&#26597;&#35810;&#22797;&#26434;&#24230;&#30340;&#31639;&#27861;&#25152;&#36798;&#21040;&#30340;&#26368;&#20339;&#36924;&#36817;&#27604;&#29575;&#20026;$0.193 - \varepsilon$&#12290;&#36825;&#39033;&#24037;&#20316;&#30340;&#20250;&#35758;&#29256;&#26412;&#38169;&#35823;&#22320;&#20351;&#29992;&#20102;&#19968;&#20010;&#23545;&#38750;&#21333;&#35843;&#23376;&#27169;&#20989;&#25968;&#26080;&#25928;&#30340;&#23376;&#31243;&#24207;&#12290;&#22312;&#36825;&#20010;&#29256;&#26412;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22266;&#23450;&#24182;&#25913;&#36827;&#30340;&#23376;&#31243;&#24207;&#65292;&#29992;&#20110;&#28155;&#21152;&#20855;&#26377;&#39640;&#24179;&#22343;&#36793;&#38469;&#22686;&#30410;&#30340;&#38598;&#21512;ThreshSeq&#65292;&#35813;&#23376;&#31243;&#24207;&#36890;&#36807;$O( \log(n) )$&#33258;&#36866;&#24212;&#36718;&#27425;&#36820;&#22238;&#19968;&#20010;&#39640;&#27010;&#29575;&#30340;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20004;&#31181;&#36817;&#20284;&#31639;&#27861;&#12290;&#31532;&#19968;&#31181;&#36817;&#20284;&#27604;&#29575;&#20026;$1/6 - \varepsilon$&#65292;&#36866;&#24212;&#24615;&#20026;$O( \log(n) )$&#65292;&#26597;&#35810;&#22797;&#26434;&#24230;&#20026;$O( n \log(k) )$&#65307;&#31532;&#20108;&#31181;&#36817;&#20284;&#27604;&#29575;&#20026;$0.193 - \varepsilon$&#65292;&#36866;&#24212;&#24615;&#20026;$O( \log^2(n) )$&#65292;&#26597;&#35810;&#22797;&#26434;&#24230;&#20026;$O(n \log$
&lt;/p&gt;
&lt;p&gt;
arXiv:2009.01947v5 Announce Type: replace-cross  Abstract: We present combinatorial and parallelizable algorithms for maximization of a submodular function, not necessarily monotone, with respect to a size constraint. We improve the best approximation factor achieved by an algorithm that has optimal adaptivity and nearly optimal query complexity to $0.193 - \varepsilon$. The conference version of this work mistakenly employed a subroutine that does not work for non-monotone, submodular functions. In this version, we propose a fixed and improved subroutine to add a set with high average marginal gain, ThreshSeq, which returns a solution in $O( \log(n) )$ adaptive rounds with high probability. Moreover, we provide two approximation algorithms. The first has approximation ratio $1/6 - \varepsilon$, adaptivity $O( \log (n) )$, and query complexity $O( n \log (k) )$, while the second has approximation ratio $0.193 - \varepsilon$, adaptivity $O( \log^2 (n) )$, and query complexity $O(n \log 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#22823;&#35268;&#27169;&#24322;&#26500;&#22270;&#19978;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;LPNL&#26694;&#26550;&#29992;&#20110;&#21487;&#25193;&#23637;&#38142;&#25509;&#39044;&#27979;&#12290;&#36890;&#36807;&#21019;&#26032;&#30340;&#25552;&#31034;&#35821;&#21644;&#37319;&#26679;&#27969;&#31243;&#65292;&#20197;&#21450;&#20998;&#32780;&#27835;&#20043;&#30340;&#31574;&#30053;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#22270;&#20013;&#30340;&#20449;&#24687;&#36807;&#36733;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.13227</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#24322;&#26500;&#22270;&#19978;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38142;&#25509;&#39044;&#27979;&#30340;&#21487;&#25193;&#23637;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Scalable Link Prediction on Large-Scale Heterogeneous Graphs with Large Language Models. (arXiv:2401.13227v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13227
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#22823;&#35268;&#27169;&#24322;&#26500;&#22270;&#19978;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;LPNL&#26694;&#26550;&#29992;&#20110;&#21487;&#25193;&#23637;&#38142;&#25509;&#39044;&#27979;&#12290;&#36890;&#36807;&#21019;&#26032;&#30340;&#25552;&#31034;&#35821;&#21644;&#37319;&#26679;&#27969;&#31243;&#65292;&#20197;&#21450;&#20998;&#32780;&#27835;&#20043;&#30340;&#31574;&#30053;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#22270;&#20013;&#30340;&#20449;&#24687;&#36807;&#36733;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#23558;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#22270;&#23398;&#20064;&#26159;&#19968;&#39033;&#26032;&#39062;&#30340;&#21162;&#21147;&#12290;&#28982;&#32780;&#65292;&#22823;&#22270;&#20013;&#34164;&#21547;&#30340;&#22823;&#37327;&#20449;&#24687;&#32473;&#36825;&#19968;&#36807;&#31243;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#20391;&#37325;&#20110;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#65292;&#24182;&#20171;&#32461;&#20102;LPNL&#65288;Link Prediction via Natural Language&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#24322;&#26500;&#22270;&#19978;&#30340;&#21487;&#25193;&#23637;&#38142;&#25509;&#39044;&#27979;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#33021;&#20197;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#22270;&#32454;&#33410;&#30340;&#21019;&#26032;&#25552;&#31034;&#35821;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#37319;&#26679;&#27969;&#31243;&#65292;&#20174;&#22823;&#35268;&#27169;&#24322;&#26500;&#22270;&#20013;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#65292;&#24182;&#37319;&#29992;&#20998;&#32780;&#27835;&#20043;&#30340;&#31574;&#30053;&#26469;&#25511;&#21046;&#36755;&#20837;&#20196;&#29260;&#25968;&#37327;&#22312;&#39044;&#23450;&#38480;&#21046;&#20869;&#65292;&#35299;&#20915;&#20102;&#20449;&#24687;&#36807;&#36733;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#35774;&#35745;&#20102;&#19968;&#20010;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;&#30340;T5&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;&#22823;&#22411;&#20844;&#20849;&#24322;&#26500;&#22270;&#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;LPNL&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;&#21508;&#31181;&#20808;&#36827;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploring the application of large-scale language models to graph learning is a novel endeavor. However, the vast amount of information inherent in large graphs poses significant challenges to this process. This paper focuses on the link prediction task and introduces LPNL (Link Prediction via Natural Language), a framework based on a large language model designed for scalable link prediction on large-scale heterogeneous graphs.We design novel prompts for link prediction that articulate graph details in natural language. We propose a two-stage sampling pipeline to extract crucial information from large-scale heterogeneous graphs, and a divide-and-conquer strategy to control the input token count within predefined limits, addressing the challenge of overwhelming information. We fine-tune a T5 model based on our self-supervised learning designed for for link prediction. Extensive experiments on a large public heterogeneous graphs demonstrate that LPNL outperforms various advanced baselin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#37096;&#20998;&#35266;&#27979;&#20013;&#36827;&#34892;&#36830;&#32493;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#29289;&#29702;&#27169;&#25311;&#65292;&#24182;&#35299;&#20915;&#20102;&#22522;&#20110;&#22266;&#23450;&#25903;&#25345;&#32593;&#26684;&#30340;&#20256;&#32479;&#26041;&#27861;&#30340;&#32570;&#28857;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#22312;&#31232;&#30095;&#35266;&#27979;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#21033;&#29992;&#20004;&#20010;&#30456;&#20114;&#20851;&#32852;&#30340;&#21160;&#21147;&#31995;&#32479;&#22312;&#31232;&#30095;&#20301;&#32622;&#21644;&#36830;&#32493;&#22495;&#19978;&#36827;&#34892;&#39044;&#27979;&#21644;&#25554;&#20540;&#27714;&#35299;&#12290;</title><link>http://arxiv.org/abs/2401.09198</link><description>&lt;p&gt;
&#20174;&#37096;&#20998;&#35266;&#27979;&#20013;&#36827;&#34892;&#31354;&#38388;&#21644;&#26102;&#38388;&#36830;&#32493;&#30340;&#29289;&#29702;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Space and Time Continuous Physics Simulation From Partial Observations. (arXiv:2401.09198v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09198
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#37096;&#20998;&#35266;&#27979;&#20013;&#36827;&#34892;&#36830;&#32493;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#29289;&#29702;&#27169;&#25311;&#65292;&#24182;&#35299;&#20915;&#20102;&#22522;&#20110;&#22266;&#23450;&#25903;&#25345;&#32593;&#26684;&#30340;&#20256;&#32479;&#26041;&#27861;&#30340;&#32570;&#28857;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#22312;&#31232;&#30095;&#35266;&#27979;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#21033;&#29992;&#20004;&#20010;&#30456;&#20114;&#20851;&#32852;&#30340;&#21160;&#21147;&#31995;&#32479;&#22312;&#31232;&#30095;&#20301;&#32622;&#21644;&#36830;&#32493;&#22495;&#19978;&#36827;&#34892;&#39044;&#27979;&#21644;&#25554;&#20540;&#27714;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#29289;&#29702;&#27169;&#25311;&#25216;&#26415;&#20381;&#36182;&#20110;&#25968;&#20540;&#26041;&#26696;&#21644;&#32593;&#26684;&#32454;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#31934;&#24230;&#21644;&#22797;&#26434;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#20294;&#36825;&#20123;&#25163;&#24037;&#35299;&#20915;&#26041;&#26696;&#32321;&#29712;&#19988;&#38656;&#35201;&#39640;&#35745;&#31639;&#33021;&#21147;&#12290;&#22522;&#20110;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#36890;&#36807;&#26356;&#30452;&#25509;&#21644;&#39640;&#25928;&#22320;&#38598;&#25104;&#38271;&#36317;&#31163;&#20381;&#36182;&#26469;&#23454;&#29616;&#39640;&#36866;&#24212;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20027;&#35201;&#20851;&#27880;&#27969;&#20307;&#21160;&#21147;&#23398;&#65292;&#24182;&#35299;&#20915;&#20102;&#22823;&#37096;&#20998;&#25991;&#29486;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#21363;&#35745;&#31639;&#21644;&#39044;&#27979;&#24418;&#24335;&#20026;&#24120;&#35268;&#25110;&#38750;&#35268;&#21017;&#32593;&#26684;&#30340;&#22266;&#23450;&#25903;&#25345;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35774;&#32622;&#65292;&#21487;&#20197;&#22312;&#36830;&#32493;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#22495;&#20013;&#36827;&#34892;&#39044;&#27979;&#65292;&#21516;&#26102;&#22312;&#31232;&#30095;&#35266;&#27979;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#20219;&#21153;&#24418;&#24335;&#21270;&#20026;&#21452;&#35266;&#27979;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#20854;&#20013;&#22312;&#31232;&#30095;&#20301;&#32622;&#21644;&#36830;&#32493;&#22495;&#19978;&#23450;&#20041;&#20102;&#20004;&#20010;&#30456;&#20114;&#20851;&#32852;&#30340;&#21160;&#21147;&#31995;&#32479;&#65292;&#21487;&#20197;&#20174;&#21021;&#22987;&#26465;&#20214;&#36827;&#34892;&#39044;&#27979;&#21644;&#25554;&#20540;&#27714;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern techniques for physical simulations rely on numerical schemes and mesh-refinement methods to address trade-offs between precision and complexity, but these handcrafted solutions are tedious and require high computational power. Data-driven methods based on large-scale machine learning promise high adaptivity by integrating long-range dependencies more directly and efficiently. In this work, we focus on fluid dynamics and address the shortcomings of a large part of the literature, which are based on fixed support for computations and predictions in the form of regular or irregular grids. We propose a novel setup to perform predictions in a continuous spatial and temporal domain while being trained on sparse observations. We formulate the task as a double observation problem and propose a solution with two interlinked dynamical systems defined on, respectively, the sparse positions and the continuous domain, which allows to forecast and interpolate a solution from the initial cond
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;BOSS&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#20248;&#21270;&#27493;&#38271;&#21644;&#29983;&#25104;&#36335;&#24452;&#65292;&#25552;&#21319;&#20102;&#20302;&#36164;&#28304;&#22330;&#26223;&#19979;&#27969;&#21305;&#37197;&#29983;&#25104;&#27169;&#22411;&#30340;&#22270;&#20687;&#36136;&#37327;&#21644;&#36164;&#28304;&#21033;&#29992;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2312.16414</link><description>&lt;p&gt;
Bellman&#26368;&#20339;&#27493;&#38271;&#30452;&#32447;&#21270;&#27969;&#21305;&#37197;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Bellman Optimal Step-size Straightening of Flow-Matching Models. (arXiv:2312.16414v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.16414
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;BOSS&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#20248;&#21270;&#27493;&#38271;&#21644;&#29983;&#25104;&#36335;&#24452;&#65292;&#25552;&#21319;&#20102;&#20302;&#36164;&#28304;&#22330;&#26223;&#19979;&#27969;&#21305;&#37197;&#29983;&#25104;&#27169;&#22411;&#30340;&#22270;&#20687;&#36136;&#37327;&#21644;&#36164;&#28304;&#21033;&#29992;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#21305;&#37197;&#26159;&#19968;&#20010;&#24378;&#22823;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#65292;&#23588;&#20854;&#26159;&#22270;&#20687;&#21512;&#25104;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#24378;&#22823;&#35745;&#31639;&#38656;&#27714;&#65292;&#23588;&#20854;&#22312;&#24494;&#35843;&#36807;&#31243;&#21644;&#37319;&#26679;&#36807;&#31243;&#20013;&#65292;&#32473;&#20302;&#36164;&#28304;&#22330;&#26223;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;Bellman&#26368;&#20339;&#27493;&#38271;&#30452;&#32447;&#21270;&#65288;BOSS&#65289;&#25216;&#26415;&#26469;&#25552;&#28860;&#27969;&#21305;&#37197;&#29983;&#25104;&#27169;&#22411;&#65306;&#23427;&#38024;&#23545;&#30340;&#26159;&#22312;&#35745;&#31639;&#39044;&#31639;&#32422;&#26463;&#19979;&#36827;&#34892;&#23569;&#25968;&#27493;&#39588;&#30340;&#39640;&#25928;&#22270;&#20687;&#37319;&#26679;&#12290;&#39318;&#20808;&#65292;&#35813;&#25216;&#26415;&#28041;&#21450;&#19968;&#20010;&#21160;&#24577;&#35268;&#21010;&#31639;&#27861;&#65292;&#20248;&#21270;&#39044;&#35757;&#32451;&#32593;&#32476;&#30340;&#27493;&#38271;&#12290;&#28982;&#21518;&#65292;&#23427;&#36890;&#36807;&#20248;&#21270;&#36895;&#24230;&#32593;&#32476;&#20197;&#21305;&#37197;&#26368;&#20339;&#27493;&#38271;&#26469;&#25913;&#36827;&#29983;&#25104;&#36335;&#24452;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;BOSS&#22312;&#36164;&#28304;&#21033;&#29992;&#21644;&#22270;&#20687;&#36136;&#37327;&#26041;&#38754;&#37117;&#20855;&#26377;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;BOSS&#22312;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#23454;&#36136;&#24615;&#30340;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Flow matching is a powerful framework for generating high-quality samples in various applications, especially image synthesis. However, the intensive computational demands of these models, especially during the fine-tuning process and sampling processes, pose significant challenges for low-resource scenarios. This paper introduces Bellman Optimal Step-size Straightening (BOSS) technique for distilling flow-matching generative models: it aims specifically for a few-step efficient image sampling while adhering to a computational budget constraint. First, this technique involves a dynamic programming algorithm that optimizes the step sizes of the pretrained network. Then, it refines the velocity network to match the optimal step sizes, aiming to straighten the generation paths. Extensive experimental evaluations across image generation tasks demonstrate the efficacy of BOSS in terms of both resource utilization and image quality. Our results reveal that BOSS achieves substantial gains in 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;KL&#32422;&#26463;&#19979;&#30340;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#31639;&#27861;&#21644;&#23454;&#36341;&#12290;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#40784;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2312.11456</link><description>&lt;p&gt;
&#20154;&#31867;&#21453;&#39304;&#30340;&#36845;&#20195;&#20559;&#22909;&#23398;&#20064;&#65306;&#22312;KL&#32422;&#26463;&#19979;&#23558;&#29702;&#35770;&#19982;&#23454;&#36341;&#32852;&#31995;&#36215;&#26469;&#30340;RLHF
&lt;/p&gt;
&lt;p&gt;
Iterative Preference Learning from Human Feedback: Bridging Theory and Practice for RLHF under KL-Constraint. (arXiv:2312.11456v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.11456
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;KL&#32422;&#26463;&#19979;&#30340;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#31639;&#27861;&#21644;&#23454;&#36341;&#12290;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#40784;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#27169;&#22411;&#19982;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#30340;&#23545;&#40784;&#36807;&#31243;&#30340;&#29702;&#35770;&#26694;&#26550;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#26631;&#20934;&#30340;&#25968;&#23398;&#34920;&#36798;&#24335;&#65292;&#21363;&#21453;&#21521;KL&#27491;&#21017;&#21270;&#30340;&#19978;&#19979;&#25991;&#22810;&#33218;&#36172;&#21338;&#26426;&#29992;&#20110;RLHF&#12290;&#23613;&#31649;&#23427;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#65292;&#20294;&#23545;&#36825;&#20010;&#20844;&#24335;&#30340;&#20005;&#26684;&#29702;&#35770;&#20998;&#26512;&#20173;&#28982;&#24456;&#24320;&#25918;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#23427;&#22312;&#31163;&#32447;&#12289;&#22312;&#32447;&#21644;&#28151;&#21512;&#19977;&#31181;&#19981;&#21516;&#22330;&#26223;&#19979;&#30340;&#34892;&#20026;&#65292;&#24182;&#25552;&#20986;&#20102;&#20855;&#26377;&#26377;&#38480;&#26679;&#26412;&#29702;&#35770;&#20445;&#35777;&#30340;&#39640;&#25928;&#31639;&#27861;&#12290;&#26397;&#30528;&#23454;&#38469;&#24212;&#29992;&#30340;&#26041;&#21521;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#23545;&#20449;&#24687;&#29702;&#35770;&#31574;&#30053;&#25913;&#36827;&#39044;&#35328;&#30340;&#31283;&#20581;&#36817;&#20284;&#65292;&#33258;&#28982;&#22320;&#20135;&#29983;&#20102;&#20960;&#31181;&#26032;&#39062;&#30340;RLHF&#31639;&#27861;&#12290;&#36825;&#21253;&#25324;&#22312;&#32447;&#22330;&#26223;&#20013;&#30340;&#36845;&#20195;&#29256;&#26412;&#30340;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;(DPO)&#31639;&#27861;&#65292;&#20197;&#21450;&#31163;&#32447;&#24773;&#26223;&#19979;&#30340;&#22810;&#27493;&#25298;&#32477;&#25277;&#26679;&#31574;&#30053;&#12290;&#25105;&#20204;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#23545;&#40784;&#23454;&#39564;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the theoretical framework of the alignment process of generative models with Reinforcement Learning from Human Feedback (RLHF). We consider a standard mathematical formulation, the reverse-KL regularized contextual bandit for RLHF. Despite its widespread practical application, a rigorous theoretical analysis of this formulation remains open. We investigate its behavior in three distinct settings -- offline, online, and hybrid -- and propose efficient algorithms with finite-sample theoretical guarantees.  Moving towards practical applications, our framework, with a robust approximation of the information-theoretical policy improvement oracle, naturally gives rise to several novel RLHF algorithms. This includes an iterative version of the Direct Preference Optimization (DPO) algorithm for online settings, and a multi-step rejection sampling strategy for offline scenarios. Our empirical evaluations on real-world alignment experiment of large language model demonstrate t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#30340;&#26102;&#38388;&#28857;&#36807;&#31243;&#27169;&#22411;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#22312;&#39044;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#23545;&#20855;&#26377;&#31163;&#25955;&#21644;&#36830;&#32493;&#25104;&#20998;&#30340;&#25968;&#25454;&#20855;&#26377;&#22788;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2311.01139</link><description>&lt;p&gt;
&#28155;&#21152;&#21644;&#31232;&#30095;&#65306;&#19968;&#31181;&#29992;&#20110;&#26102;&#38388;&#28857;&#36807;&#31243;&#30340;&#25193;&#25955;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Add and Thin: Diffusion for Temporal Point Processes. (arXiv:2311.01139v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01139
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#30340;&#26102;&#38388;&#28857;&#36807;&#31243;&#27169;&#22411;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#22312;&#39044;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#23545;&#20855;&#26377;&#31163;&#25955;&#21644;&#36830;&#32493;&#25104;&#20998;&#30340;&#25968;&#25454;&#20855;&#26377;&#22788;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26102;&#38388;&#28857;&#36807;&#31243;&#65288;TPP&#65289;&#26694;&#26550;&#20869;&#65292;&#33258;&#22238;&#24402;&#31070;&#32463;&#32593;&#32476;&#24050;&#25104;&#20026;&#24314;&#27169;&#36830;&#32493;&#26102;&#38388;&#20107;&#20214;&#25968;&#25454;&#30340;&#26631;&#20934;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#20197;&#19968;&#27493;&#39044;&#27979;&#30340;&#26041;&#24335;&#31934;&#30830;&#22320;&#25429;&#25417;&#20107;&#20214;&#24207;&#21015;&#65292;&#20294;&#30001;&#20110;&#20854;&#39034;&#24207;&#24615;&#36136;&#24341;&#36215;&#30340;&#35823;&#24046;&#31215;&#32047;&#65292;&#23427;&#20204;&#22312;&#38271;&#26399;&#39044;&#27979;&#24212;&#29992;&#20013;&#20855;&#26377;&#22266;&#26377;&#30340;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;ADD-THIN&#65292;&#19968;&#31181;&#38754;&#21521;&#25972;&#20010;&#20107;&#20214;&#24207;&#21015;&#24037;&#20316;&#30340;&#22522;&#20110;&#27010;&#29575;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65292;&#23427;&#33258;&#28982;&#22320;&#22788;&#29702;&#20855;&#26377;&#31163;&#25955;&#21644;&#36830;&#32493;&#25104;&#20998;&#30340;&#25968;&#25454;&#12290;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#23494;&#24230;&#20272;&#35745;&#26041;&#38754;&#19982;&#26368;&#20808;&#36827;&#30340;TPP&#27169;&#22411;&#30456;&#21305;&#37197;&#65292;&#24182;&#22312;&#39044;&#27979;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autoregressive neural networks within the temporal point process (TPP) framework have become the standard for modeling continuous-time event data. Even though these models can expressively capture event sequences in a one-step-ahead fashion, they are inherently limited for long-term forecasting applications due to the accumulation of errors caused by their sequential nature. To overcome these limitations, we derive ADD-THIN, a principled probabilistic denoising diffusion model for TPPs that operates on entire event sequences. Unlike existing diffusion approaches, ADD-THIN naturally handles data with discrete and continuous components. In experiments on synthetic and real-world datasets, our model matches the state-of-the-art TPP models in density estimation and strongly outperforms them in forecasting.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38477;&#27700;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#20351;&#29992;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#36716;&#31227;&#23398;&#20064;&#26469;&#25552;&#39640;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#21306;&#22495;&#38477;&#27700;&#26657;&#27491;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.20187</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#29992;&#20110;&#38477;&#27700;&#21518;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Self-supervised Pre-training for Precipitation Post-processor. (arXiv:2310.20187v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20187
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38477;&#27700;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#20351;&#29992;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#36716;&#31227;&#23398;&#20064;&#26469;&#25552;&#39640;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#21306;&#22495;&#38477;&#27700;&#26657;&#27491;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#39044;&#38450;&#21361;&#38505;&#22825;&#27668;&#20107;&#20214;&#65292;&#30830;&#20445;&#20805;&#36275;&#30340;&#23616;&#22320;&#38477;&#27700;&#39044;&#25253;&#25552;&#21069;&#26102;&#38388;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20840;&#29699;&#21464;&#26262;&#24341;&#36215;&#30340;&#27668;&#20505;&#21464;&#21270;&#22686;&#21152;&#20102;&#20934;&#30830;&#39044;&#27979;&#20005;&#37325;&#38477;&#27700;&#20107;&#20214;&#65288;&#22914;&#26292;&#38632;&#65289;&#30340;&#25361;&#25112;&#12290;&#26412;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38477;&#27700;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#29992;&#20110;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#65288;NWP&#65289;&#27169;&#22411;&#12290;&#38477;&#27700;&#21518;&#22788;&#29702;&#21253;&#25324;&#65288;i&#65289;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#20854;&#20013;&#32534;&#30721;&#22120;&#30340;&#21442;&#25968;&#22312;&#22823;&#27668;&#29289;&#29702;&#39046;&#22495;&#30340;&#36974;&#34109;&#21464;&#37327;&#37325;&#26500;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#20174;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;&#20013;&#36716;&#31227;&#23398;&#20064;&#21040;&#38477;&#27700;&#20998;&#21106;&#20219;&#21153;&#65288;&#30446;&#26631;&#39046;&#22495;&#65289;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#26631;&#35760;&#26041;&#27861;&#65292;&#20197;&#26377;&#25928;&#22320;&#35757;&#32451;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#22312;&#21306;&#22495;NWP&#20013;&#30340;&#38477;&#27700;&#26657;&#27491;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Securing sufficient forecast lead time for local precipitation is essential for preventing hazardous weather events. Nonetheless, global warming-induced climate change is adding to the challenge of accurately predicting severe precipitation events, such as heavy rainfall. In this work, we propose a deep learning-based precipitation post-processor approach to numerical weather prediction (NWP) models. The precipitation post-processor consists of (i) self-supervised pre-training, where parameters of encoder are pre-trained on the reconstruction of masked variables of the atmospheric physics domain, and (ii) transfer learning on precipitation segmentation tasks (target domain) from the pre-trained encoder. We also introduce a heuristic labeling approach for effectively training class-imbalanced datasets. Our experiment results in precipitation correction for regional NWP show that the proposed method outperforms other approaches.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#35299;&#37322;&#30340;GNN&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#20449;&#24687;&#29942;&#39048;&#26694;&#26550;&#20013;&#23558;&#21407;&#22411;&#23398;&#20064;&#19982;&#36755;&#20837;&#22270;&#30340;&#20851;&#38190;&#23376;&#22270;&#30456;&#32467;&#21512;&#65292;&#20026;&#27169;&#22411;&#30340;&#35299;&#37322;&#33021;&#21147;&#21644;&#24615;&#33021;&#25552;&#20379;&#20102;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.19906</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#21407;&#22411;&#30340;&#22270;&#20449;&#24687;&#29942;&#39048;
&lt;/p&gt;
&lt;p&gt;
Interpretable Prototype-based Graph Information Bottleneck. (arXiv:2310.19906v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19906
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#35299;&#37322;&#30340;GNN&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#20449;&#24687;&#29942;&#39048;&#26694;&#26550;&#20013;&#23558;&#21407;&#22411;&#23398;&#20064;&#19982;&#36755;&#20837;&#22270;&#30340;&#20851;&#38190;&#23376;&#22270;&#30456;&#32467;&#21512;&#65292;&#20026;&#27169;&#22411;&#30340;&#35299;&#37322;&#33021;&#21147;&#21644;&#24615;&#33021;&#25552;&#20379;&#20102;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#25104;&#21151;&#23548;&#33268;&#20102;&#23545;&#20854;&#20915;&#31574;&#36807;&#31243;&#30340;&#29702;&#35299;&#21644;&#23545;&#20854;&#39044;&#27979;&#30340;&#35299;&#37322;&#30340;&#38656;&#27714;&#65292;&#36825;&#20652;&#29983;&#20102;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#65292;&#20026;&#40657;&#30418;&#27169;&#22411;&#25552;&#20379;&#36879;&#26126;&#30340;&#35299;&#37322;&#12290;&#26368;&#36817;&#65292;&#21407;&#22411;&#30340;&#20351;&#29992;&#25104;&#21151;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#36890;&#36807;&#23398;&#20064;&#21407;&#22411;&#26469;&#26263;&#31034;&#24433;&#21709;&#39044;&#27979;&#30340;&#35757;&#32451;&#22270;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#20250;&#32473;&#21407;&#22411;&#25552;&#20379;&#26469;&#33258;&#25972;&#20010;&#22270;&#30340;&#36807;&#22810;&#20449;&#24687;&#65292;&#23548;&#33268;&#20851;&#38190;&#23376;&#32467;&#26500;&#30340;&#25490;&#38500;&#25110;&#26080;&#20851;&#23376;&#32467;&#26500;&#30340;&#21253;&#21547;&#65292;&#36825;&#21487;&#20197;&#38480;&#21046;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#35299;&#37322;&#33021;&#21147;&#21644;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#35299;&#37322;&#30340;GNN&#26694;&#26550;&#65292;&#31216;&#20026;&#35299;&#37322;&#24615;&#30340;&#22522;&#20110;&#21407;&#22411;&#30340;&#22270;&#20449;&#24687;&#29942;&#39048; (PGIB)&#65292;&#23558;&#21407;&#22411;&#23398;&#20064;&#32435;&#20837;&#20449;&#24687;&#29942;&#39048;&#26694;&#26550;&#65292;&#20026;&#21407;&#22411;&#25552;&#20379;&#36755;&#20837;&#22270;&#30340;&#20851;&#38190;&#23376;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of Graph Neural Networks (GNNs) has led to a need for understanding their decision-making process and providing explanations for their predictions, which has given rise to explainable AI (XAI) that offers transparent explanations for black-box models. Recently, the use of prototypes has successfully improved the explainability of models by learning prototypes to imply training graphs that affect the prediction. However, these approaches tend to provide prototypes with excessive information from the entire graph, leading to the exclusion of key substructures or the inclusion of irrelevant substructures, which can limit both the interpretability and the performance of the model in downstream tasks. In this work, we propose a novel framework of explainable GNNs, called interpretable Prototype-based Graph Information Bottleneck (PGIB) that incorporates prototype learning within the information bottleneck framework to provide prototypes with the key subgraph from the input graph
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20849;&#24418;&#24402;&#19968;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#32593;&#26684;&#32454;&#32990;&#22312;2D&#29289;&#29702;&#31354;&#38388;&#20013;&#30340;&#33258;&#25105;&#20301;&#32622;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#20943;&#23567;&#20301;&#32622;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2310.19192</link><description>&lt;p&gt;
&#32593;&#26684;&#32454;&#32990;&#20013;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20849;&#24418;&#24402;&#19968;&#21270;
&lt;/p&gt;
&lt;p&gt;
Conformal Normalization in Recurrent Neural Network of Grid Cells. (arXiv:2310.19192v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19192
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20849;&#24418;&#24402;&#19968;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#32593;&#26684;&#32454;&#32990;&#22312;2D&#29289;&#29702;&#31354;&#38388;&#20013;&#30340;&#33258;&#25105;&#20301;&#32622;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#20943;&#23567;&#20301;&#32622;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21754;&#20083;&#21160;&#29289;&#22823;&#33041;&#20013;&#39070;&#21494;&#30382;&#23618;&#30340;&#32593;&#26684;&#32454;&#32990;&#22312;2D&#24320;&#25918;&#29615;&#22659;&#20013;&#20197;&#24778;&#20154;&#30340;&#20845;&#35282;&#24418;&#21457;&#23556;&#27169;&#24335;&#23637;&#31034;&#20986;&#21453;&#24212;&#22270;&#12290;&#32593;&#26684;&#32454;&#32990;&#32676;&#20307;&#30340;&#21453;&#24212;&#22312;&#39640;&#32500;&#31070;&#32463;&#27963;&#21160;&#31354;&#38388;&#20013;&#24418;&#25104;&#19968;&#20010;&#21521;&#37327;&#65292;&#36825;&#20010;&#21521;&#37327;&#34920;&#31034;&#20195;&#29702;&#22312;2D&#29289;&#29702;&#31354;&#38388;&#20013;&#30340;&#33258;&#25105;&#20301;&#32622;&#12290;&#24403;&#20195;&#29702;&#31227;&#21160;&#26102;&#65292;&#36825;&#20010;&#21521;&#37327;&#34987;&#19968;&#20010;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#36716;&#25442;&#65292;&#35813;&#32593;&#32476;&#23558;&#20195;&#29702;&#30340;&#36895;&#24230;&#20316;&#20026;&#36755;&#20837;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#36755;&#20837;&#36895;&#24230;&#36827;&#34892;&#31616;&#21333;&#32780;&#36890;&#29992;&#30340;&#20849;&#24418;&#24402;&#19968;&#21270;&#65292;&#20351;&#24471;&#39640;&#32500;&#31070;&#32463;&#31354;&#38388;&#20013;&#20301;&#32622;&#21521;&#37327;&#30340;&#23616;&#37096;&#20301;&#31227;&#19982;2D&#29289;&#29702;&#31354;&#38388;&#20013;&#20195;&#29702;&#30340;&#23616;&#37096;&#20301;&#31227;&#25104;&#27604;&#20363;&#65292;&#26080;&#35770;&#36755;&#20837;&#36895;&#24230;&#30340;&#26041;&#21521;&#22914;&#20309;&#12290;&#25105;&#20204;&#22312;&#26368;&#31616;&#21333;&#30340;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#24490;&#29615;&#32593;&#32476;&#19978;&#36827;&#34892;&#20102;&#25968;&#20540;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#20849;&#24418;&#24402;&#19968;&#21270;&#23548;&#33268;&#25968;&#37327;&#32423;&#36739;&#23567;&#30340;&#20301;&#32622;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Grid cells in the entorhinal cortex of the mammalian brain exhibit striking hexagon firing patterns in their response maps as the animal (e.g., a rat) navigates in a 2D open environment. The responses of the population of grid cells collectively form a vector in a high-dimensional neural activity space, and this vector represents the self-position of the agent in the 2D physical space. As the agent moves, the vector is transformed by a recurrent neural network that takes the velocity of the agent as input. In this paper, we propose a simple and general conformal normalization of the input velocity for the recurrent neural network, so that the local displacement of the position vector in the high-dimensional neural space is proportional to the local displacement of the agent in the 2D physical space, regardless of the direction of the input velocity. Our numerical experiments on the minimally simple linear and non-linear recurrent networks show that conformal normalization leads to the 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#36229;&#21442;&#25968;&#23545;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#20219;&#21153;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#23545;&#19981;&#21516;&#22797;&#26434;&#32423;&#21035;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#35777;&#35780;&#20272;&#65292;&#21457;&#29616;&#36229;&#21442;&#25968;&#36873;&#25321;&#21516;&#26679;&#23545;&#31639;&#27861;&#36873;&#25321;&#20855;&#26377;&#37325;&#35201;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.18212</link><description>&lt;p&gt;
&#29992;&#20110;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#30340;&#31639;&#27861;&#23545;&#36229;&#21442;&#25968;&#36873;&#25321;&#30340;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;
Robustness of Algorithms for Causal Structure Learning to Hyperparameter Choice. (arXiv:2310.18212v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18212
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#36229;&#21442;&#25968;&#23545;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#20219;&#21153;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#23545;&#19981;&#21516;&#22797;&#26434;&#32423;&#21035;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#35777;&#35780;&#20272;&#65292;&#21457;&#29616;&#36229;&#21442;&#25968;&#36873;&#25321;&#21516;&#26679;&#23545;&#31639;&#27861;&#36873;&#25321;&#20855;&#26377;&#37325;&#35201;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#21442;&#25968;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#30001;&#20110;&#32467;&#26500;&#23398;&#20064;&#30340;&#26080;&#30417;&#30563;&#29305;&#24615;&#65292;&#36229;&#21442;&#25968;&#35843;&#25972;&#21487;&#20197;&#22312;&#20219;&#20309;&#31639;&#27861;&#20013;&#20135;&#29983;&#20840;&#29699;&#39046;&#20808;&#21644;&#31967;&#31957;&#30340;&#39044;&#27979;&#34920;&#29616;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#20351;&#29992;&#29305;&#23450;&#31639;&#27861;&#30340;&#40664;&#35748;&#20540;&#65292;&#20154;&#20204;&#32463;&#24120;&#24573;&#35270;&#36229;&#21442;&#25968;&#35843;&#25972;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#22823;&#37327;&#20851;&#20110;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#24615;&#33021;&#35780;&#20272;&#30340;&#30740;&#31350;&#65292;&#20294;&#36229;&#21442;&#25968;&#22914;&#20309;&#24433;&#21709;&#21333;&#20010;&#31639;&#27861;&#20197;&#21450;&#36873;&#25321;&#26368;&#20339;&#31639;&#27861;&#35299;&#20915;&#29305;&#23450;&#38382;&#39064;&#30340;&#38382;&#39064;&#23578;&#26410;&#24471;&#21040;&#28145;&#20837;&#30740;&#31350;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#35843;&#26597;&#36229;&#21442;&#25968;&#23545;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#20219;&#21153;&#30340;&#24433;&#21709;&#26469;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23545;&#19981;&#21516;&#22797;&#26434;&#32423;&#21035;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#19968;&#20123;&#24320;&#21019;&#24615;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#36229;&#21442;&#25968;&#36873;&#25321;&#30340;&#23454;&#35777;&#35780;&#20272;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#31639;&#27861;&#30340;&#36873;&#25321;&#20173;&#28982;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#36229;&#21442;&#25968;&#30340;&#36873;&#25321;&#21516;&#26679;&#23545;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#20219;&#21153;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperparameters play a critical role in machine learning. Hyperparameter tuning can make the difference between state-of-the-art and poor prediction performance for any algorithm, but it is particularly challenging for structure learning due to its unsupervised nature. As a result, hyperparameter tuning is often neglected in favour of using the default values provided by a particular implementation of an algorithm. While there have been numerous studies on performance evaluation of causal discovery algorithms, how hyperparameters affect individual algorithms, as well as the choice of the best algorithm for a specific problem, has not been studied in depth before. This work addresses this gap by investigating the influence of hyperparameters on causal structure learning tasks. Specifically, we perform an empirical evaluation of hyperparameter selection for some seminal learning algorithms on datasets of varying levels of complexity. We find that, while the choice of algorithm remains cr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;&#23450;&#21521;&#26080;&#29615;&#22270;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#23558;&#19978;&#19979;&#25991;&#29305;&#24449;&#26144;&#23556;&#21040;DAG&#65292;&#21033;&#29992;&#31232;&#30095;&#30340;&#21152;&#26435;&#37051;&#25509;&#30697;&#38453;&#34920;&#31034;&#22270;&#32467;&#26500;&#65292;&#24182;&#36890;&#36807;&#26032;&#39062;&#30340;&#25237;&#24433;&#23618;&#28385;&#36275;&#26080;&#29615;&#24615;&#30340;&#29305;&#28857;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#25104;&#21151;&#24674;&#22797;&#20986;&#30495;&#23454;&#30340;&#19978;&#19979;&#25991;&#29305;&#23450;&#22270;&#12290;</title><link>http://arxiv.org/abs/2310.15627</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#23450;&#21521;&#26080;&#29615;&#22270;
&lt;/p&gt;
&lt;p&gt;
Contextual directed acyclic graphs. (arXiv:2310.15627v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15627
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;&#23450;&#21521;&#26080;&#29615;&#22270;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#23558;&#19978;&#19979;&#25991;&#29305;&#24449;&#26144;&#23556;&#21040;DAG&#65292;&#21033;&#29992;&#31232;&#30095;&#30340;&#21152;&#26435;&#37051;&#25509;&#30697;&#38453;&#34920;&#31034;&#22270;&#32467;&#26500;&#65292;&#24182;&#36890;&#36807;&#26032;&#39062;&#30340;&#25237;&#24433;&#23618;&#28385;&#36275;&#26080;&#29615;&#24615;&#30340;&#29305;&#28857;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#25104;&#21151;&#24674;&#22797;&#20986;&#30495;&#23454;&#30340;&#19978;&#19979;&#25991;&#29305;&#23450;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#20272;&#35745;&#23450;&#21521;&#26080;&#29615;&#22270;&#65288;DAG&#65289;&#30340;&#32467;&#26500;&#20173;&#28982;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#36825;&#20010;&#39046;&#22495;&#30340;&#22823;&#37096;&#20998;&#30740;&#31350;&#38598;&#20013;&#22312;&#20026;&#25972;&#20010;&#20154;&#21475;&#23398;&#20064;&#21333;&#20010;DAG&#19978;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#20010;&#26367;&#20195;&#24615;&#30340;&#35774;&#32622;&#65292;&#20854;&#20013;&#22270;&#32467;&#26500;&#22522;&#20110;&#21487;&#29992;&#30340;&#8220;&#19978;&#19979;&#25991;&#8221;&#29305;&#24449;&#32780;&#22240;&#20154;&#32780;&#24322;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#23558;&#19978;&#19979;&#25991;&#29305;&#24449;&#26144;&#23556;&#21040;DAG&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#35299;&#20915;&#36825;&#20010;&#19978;&#19979;&#25991;DAG&#38382;&#39064;&#65292;DAG&#20197;&#21152;&#26435;&#37051;&#25509;&#30697;&#38453;&#34920;&#31034;&#12290;&#31070;&#32463;&#32593;&#32476;&#37197;&#22791;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25237;&#24433;&#23618;&#65292;&#30830;&#20445;&#36755;&#20986;&#30697;&#38453;&#26159;&#31232;&#30095;&#30340;&#65292;&#24182;&#28385;&#36275;&#26368;&#36817;&#21457;&#23637;&#30340;&#26080;&#29615;&#24615;&#30340;&#29305;&#28857;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#35745;&#31639;&#26694;&#26550;&#26469;&#23398;&#20064;&#19978;&#19979;&#25991;DAG&#65292;&#24182;&#25552;&#20379;&#20102;&#25910;&#25947;&#20445;&#35777;&#21644;&#36890;&#36807;&#25237;&#24433;&#23618;&#21453;&#21521;&#20256;&#25773;&#30340;&#20998;&#26512;&#26799;&#24230;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26032;&#26041;&#27861;&#21487;&#20197;&#24674;&#22797;&#20986;&#30495;&#23454;&#30340;&#19978;&#19979;&#25991;&#29305;&#23450;&#22270;&#65292;&#32780;&#29616;&#26377;&#26041;&#27861;&#21017;&#22833;&#36133;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating the structure of directed acyclic graphs (DAGs) from observational data remains a significant challenge in machine learning. Most research in this area concentrates on learning a single DAG for the entire population. This paper considers an alternative setting where the graph structure varies across individuals based on available "contextual" features. We tackle this contextual DAG problem via a neural network that maps the contextual features to a DAG, represented as a weighted adjacency matrix. The neural network is equipped with a novel projection layer that ensures the output matrices are sparse and satisfy a recently developed characterization of acyclicity. We devise a scalable computational framework for learning contextual DAGs and provide a convergence guarantee and an analytical gradient for backpropagating through the projection layer. Our experiments suggest that the new approach can recover the true context-specific graph where existing approaches fail.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#25351;&#20986;&#65292;&#20248;&#21270;&#36807;&#31243;&#20013;&#30340;&#19981;&#31283;&#23450;&#24615;&#36890;&#24120;&#26159;&#30001;&#25439;&#22833;&#20989;&#25968;&#30340;&#38750;&#21333;&#35843;&#24615;&#24341;&#36215;&#30340;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#23450;&#65288;&#22823;&#35268;&#27169;&#65289;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#32447;&#24615;&#25554;&#20540;&#26469;&#21033;&#29992;&#8220;&#38750;&#25193;&#24352;&#31639;&#23376;&#8221;&#30340;&#29702;&#35770;&#26469;&#35299;&#20915;&#12290;&#36890;&#36807;&#26500;&#24314;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#26041;&#26696;&#65292;&#26494;&#24347;&#36817;&#20284;&#36817;&#31471;&#28857;&#65288;RAPP&#65289;&#65292;&#21457;&#29616;&#20102;&#19968;&#26063;Lookahead&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#21327;&#35843;&#37096;&#20998;&#21333;&#35843;&#38382;&#39064;&#20013;&#25910;&#25947;&#65292;&#21363;&#20351;&#22522;&#26412;&#20248;&#21270;&#22120;&#37319;&#29992;&#26799;&#24230;&#19979;&#38477;&#21319;&#32423;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.13459</link><description>&lt;p&gt;
&#31283;&#23450;&#30340;&#38750;&#20984;-&#38750;&#20985;&#35757;&#32451;&#36890;&#36807;&#32447;&#24615;&#25554;&#20540;
&lt;/p&gt;
&lt;p&gt;
Stable Nonconvex-Nonconcave Training via Linear Interpolation. (arXiv:2310.13459v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13459
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#25351;&#20986;&#65292;&#20248;&#21270;&#36807;&#31243;&#20013;&#30340;&#19981;&#31283;&#23450;&#24615;&#36890;&#24120;&#26159;&#30001;&#25439;&#22833;&#20989;&#25968;&#30340;&#38750;&#21333;&#35843;&#24615;&#24341;&#36215;&#30340;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#23450;&#65288;&#22823;&#35268;&#27169;&#65289;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#32447;&#24615;&#25554;&#20540;&#26469;&#21033;&#29992;&#8220;&#38750;&#25193;&#24352;&#31639;&#23376;&#8221;&#30340;&#29702;&#35770;&#26469;&#35299;&#20915;&#12290;&#36890;&#36807;&#26500;&#24314;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#26041;&#26696;&#65292;&#26494;&#24347;&#36817;&#20284;&#36817;&#31471;&#28857;&#65288;RAPP&#65289;&#65292;&#21457;&#29616;&#20102;&#19968;&#26063;Lookahead&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#21327;&#35843;&#37096;&#20998;&#21333;&#35843;&#38382;&#39064;&#20013;&#25910;&#25947;&#65292;&#21363;&#20351;&#22522;&#26412;&#20248;&#21270;&#22120;&#37319;&#29992;&#26799;&#24230;&#19979;&#38477;&#21319;&#32423;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#20110;&#32447;&#24615;&#25554;&#20540;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#20316;&#20026;&#19968;&#31181;&#31283;&#23450;&#65288;&#22823;&#35268;&#27169;&#65289;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#35748;&#20026;&#20248;&#21270;&#36807;&#31243;&#20013;&#30340;&#19981;&#31283;&#23450;&#24615;&#36890;&#24120;&#26159;&#30001;&#25439;&#22833;&#20989;&#25968;&#30340;&#38750;&#21333;&#35843;&#24615;&#24341;&#36215;&#30340;&#65292;&#24182;&#23637;&#31034;&#20102;&#32447;&#24615;&#25554;&#20540;&#22914;&#20309;&#36890;&#36807;&#21033;&#29992;&#8220;&#38750;&#25193;&#24352;&#31639;&#23376;&#8221;&#30340;&#29702;&#35770;&#26469;&#24110;&#21161;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#26041;&#26696;&#65292;&#31216;&#20026;&#26494;&#24347;&#36817;&#20284;&#36817;&#31471;&#28857;&#65288;RAPP&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#26126;&#30830;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#23454;&#29616;&#23436;&#25972;&#33539;&#22260;&#20869;&#30340;&#21327;&#35843;&#37096;&#20998;&#21333;&#35843;&#38382;&#39064;&#30340;&#26368;&#21518;&#36845;&#20195;&#25910;&#25947;&#36895;&#29575;&#12290;&#35813;&#26500;&#36896;&#21487;&#25193;&#23637;&#21040;&#32422;&#26463;&#21644;&#27491;&#21017;&#21270;&#35774;&#32622;&#12290;&#36890;&#36807;&#26367;&#25442;RAPP&#20013;&#30340;&#20869;&#37096;&#20248;&#21270;&#22120;&#65292;&#25105;&#20204;&#37325;&#26032;&#21457;&#29616;&#20102;Lookahead&#31639;&#27861;&#26063;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#31639;&#27861;&#22312;&#21327;&#35843;&#37096;&#20998;&#21333;&#35843;&#38382;&#39064;&#20013;&#30340;&#25910;&#25947;&#24615;&#65292;&#21363;&#20351;&#22522;&#26412;&#20248;&#21270;&#22120;&#37319;&#29992;&#26799;&#24230;&#19979;&#38477;&#21319;&#32423;&#31639;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;Lookahead&#32487;&#25215;&#24615;&#36136;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25193;&#23637;&#20102;Lookahead&#22312;&#21327;&#35843;&#37096;&#20998;&#21333;&#35843;&#38382;&#39064;&#20013;&#25910;&#25947;&#30340;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a theoretical analysis of linear interpolation as a principled method for stabilizing (large-scale) neural network training. We argue that instabilities in the optimization process are often caused by the nonmonotonicity of the loss landscape and show how linear interpolation can help by leveraging the theory of nonexpansive operators. We construct a new optimization scheme called relaxed approximate proximal point (RAPP), which is the first explicit method to achieve last iterate convergence rates for the full range of cohypomonotone problems. The construction extends to constrained and regularized settings. By replacing the inner optimizer in RAPP we rediscover the family of Lookahead algorithms for which we establish convergence in cohypomonotone problems even when the base optimizer is taken to be gradient descent ascent. The range of cohypomonotone problems in which Lookahead converges is further expanded by exploiting that Lookahead inherits the properties of 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#33258;&#21160;&#39550;&#39542;&#22330;&#26223;&#20013;&#20351;&#29992;&#37096;&#20998;&#21487;&#35266;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#37096;&#32626;&#21644;&#27979;&#35797;&#22810;&#31181;&#25216;&#26415;&#26469;&#24179;&#34913;&#25506;&#32034;&#21644;&#21033;&#29992;&#30340;&#26435;&#34913;&#65292;&#20197;&#39044;&#27979;&#26041;&#21521;&#30424;&#25805;&#20316;&#12290;</title><link>http://arxiv.org/abs/2310.08331</link><description>&lt;p&gt;
&#22810;&#33218;&#36172;&#21338;&#31574;&#30053;&#23545;&#28145;&#24230;&#24490;&#29615;&#24378;&#21270;&#23398;&#20064;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Impact of multi-armed bandit strategies on deep recurrent reinforcement learning. (arXiv:2310.08331v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08331
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#33258;&#21160;&#39550;&#39542;&#22330;&#26223;&#20013;&#20351;&#29992;&#37096;&#20998;&#21487;&#35266;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#37096;&#32626;&#21644;&#27979;&#35797;&#22810;&#31181;&#25216;&#26415;&#26469;&#24179;&#34913;&#25506;&#32034;&#21644;&#21033;&#29992;&#30340;&#26435;&#34913;&#65292;&#20197;&#39044;&#27979;&#26041;&#21521;&#30424;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#29615;&#22659;&#30340;&#19981;&#23436;&#20840;&#20102;&#35299;&#23548;&#33268;&#26234;&#33021;&#20307;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#20570;&#20986;&#20915;&#31574;&#12290;&#24378;&#21270;&#23398;&#20064;&#20013;&#19968;&#20010;&#37325;&#35201;&#30340;&#22256;&#22659;&#26159;&#65292;&#22312;&#20570;&#20986;&#20915;&#31574;&#26102;&#65292;&#26234;&#33021;&#20307;&#38656;&#35201;&#22312;&#21033;&#29992;&#24403;&#21069;&#29615;&#22659;&#30693;&#35782;&#26368;&#22823;&#21270;&#32047;&#31215;&#22870;&#21169;&#21644;&#25506;&#32034;&#34892;&#21160;&#20197;&#25552;&#39640;&#29615;&#22659;&#30693;&#35782;&#30340;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#65288;&#25506;&#32034;-&#21033;&#29992;&#30340;&#24179;&#34913;&#65289;&#12290;&#21516;&#26102;&#65292;&#21478;&#19968;&#20010;&#30456;&#20851;&#38382;&#39064;&#26159;&#29366;&#24577;&#30340;&#23436;&#20840;&#21487;&#35266;&#27979;&#24615;&#65292;&#19981;&#26159;&#25152;&#26377;&#24212;&#29992;&#37117;&#33021;&#20551;&#23450;&#12290;&#20363;&#22914;&#65292;&#24403;&#21482;&#23558;2D&#22270;&#20687;&#20316;&#20026;&#36755;&#20837;&#29992;&#20110;&#22312;3D&#27169;&#25311;&#29615;&#22659;&#20013;&#25214;&#21040;&#26368;&#20339;&#34892;&#21160;&#26102;&#65292;&#23601;&#23384;&#22312;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#37096;&#32626;&#21644;&#27979;&#35797;&#22810;&#31181;&#25216;&#26415;&#26469;&#35299;&#20915;&#37096;&#20998;&#21487;&#35266;&#27979;&#31995;&#32479;&#20013;&#25506;&#32034;&#21644;&#21033;&#29992;&#30340;&#24179;&#34913;&#38382;&#39064;&#65292;&#20197;&#39044;&#27979;&#33258;&#21160;&#39550;&#39542;&#22330;&#26223;&#20013;&#30340;&#26041;&#21521;&#30424;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Incomplete knowledge of the environment leads an agent to make decisions under uncertainty. One of the major dilemmas in Reinforcement Learning (RL) where an autonomous agent has to balance two contrasting needs in making its decisions is: exploiting the current knowledge of the environment to maximize the cumulative reward as well as exploring actions that allow improving the knowledge of the environment, hopefully leading to higher reward values (exploration-exploitation trade-off). Concurrently, another relevant issue regards the full observability of the states, which may not be assumed in all applications. Such as when only 2D images are considered as input in a RL approach used for finding the optimal action within a 3D simulation environment. In this work, we address these issues by deploying and testing several techniques to balance exploration and exploitation trade-off on partially observable systems for predicting steering wheels in autonomous driving scenario. More precisel
&lt;/p&gt;</description></item><item><title>LARA&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#19988;&#25239;&#36807;&#25311;&#21512;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#20877;&#35757;&#32451;&#26041;&#27861;&#65292;&#23427;&#23558;&#37325;&#26032;&#35757;&#32451;&#36807;&#31243;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#20984;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#21453;&#24605;&#27169;&#22359;&#20197;&#21033;&#29992;&#21382;&#21490;&#25968;&#25454;&#65292;&#21516;&#26102;&#25968;&#23398;&#35777;&#26126;&#20102;&#22312;&#24494;&#35843;&#21518;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.05668</link><description>&lt;p&gt;
LARA&#65306;&#19968;&#31181;&#36731;&#37327;&#32423;&#19988;&#25239;&#36807;&#25311;&#21512;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#20877;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
LARA: A Light and Anti-overfitting Retraining Approach for Unsupervised Anomaly Detection. (arXiv:2310.05668v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05668
&lt;/p&gt;
&lt;p&gt;
LARA&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#19988;&#25239;&#36807;&#25311;&#21512;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#20877;&#35757;&#32451;&#26041;&#27861;&#65292;&#23427;&#23558;&#37325;&#26032;&#35757;&#32451;&#36807;&#31243;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#20984;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#21453;&#24605;&#27169;&#22359;&#20197;&#21033;&#29992;&#21382;&#21490;&#25968;&#25454;&#65292;&#21516;&#26102;&#25968;&#23398;&#35777;&#26126;&#20102;&#22312;&#24494;&#35843;&#21518;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#22823;&#37096;&#20998;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#37117;&#20551;&#35774;&#27491;&#24120;&#27169;&#24335;&#22987;&#32456;&#20445;&#25345;&#19981;&#21464;&#12290;&#28982;&#32780;&#65292;Web&#26381;&#21153;&#30340;&#27491;&#24120;&#27169;&#24335;&#32463;&#24120;&#21457;&#29983;&#21095;&#28872;&#21464;&#21270;&#12290;&#22312;&#36825;&#31181;&#21464;&#21270;&#20043;&#21518;&#65292;&#20351;&#29992;&#26087;&#20998;&#24067;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#24050;&#32463;&#36807;&#26102;&#12290;&#27599;&#27425;&#37117;&#37325;&#26032;&#35757;&#32451;&#25972;&#20010;&#27169;&#22411;&#26159;&#26114;&#36149;&#30340;&#12290;&#27492;&#22806;&#65292;&#22312;&#27491;&#24120;&#27169;&#24335;&#21464;&#21270;&#24320;&#22987;&#26102;&#65292;&#26032;&#20998;&#24067;&#30340;&#35266;&#23519;&#25968;&#25454;&#19981;&#36275;&#12290;&#29992;&#26377;&#38480;&#30340;&#25968;&#25454;&#23545;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#23481;&#26131;&#36807;&#25311;&#21512;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#19988;&#25239;&#36807;&#25311;&#21512;&#30340;&#20877;&#35757;&#32451;&#26041;&#27861;&#65288;LARA&#65289;&#65292;&#29992;&#20110;&#22522;&#20110;&#28145;&#24230;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65288;VAEs&#65289;&#12290;&#26412;&#24037;&#20316;&#26088;&#22312;&#25552;&#20986;&#19977;&#20010;&#26032;&#39062;&#30340;&#36129;&#29486;&#65306;1&#65289;&#23558;&#37325;&#26032;&#35757;&#32451;&#36807;&#31243;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#20984;&#38382;&#39064;&#65292;&#24182;&#33021;&#22815;&#20197;&#24555;&#36895;&#25910;&#25947;&#20197;&#21450;&#38450;&#27490;&#36807;&#25311;&#21512;&#65307;2&#65289;&#35774;&#35745;&#20102;&#19968;&#20010;&#21453;&#24605;&#27169;&#22359;&#65292;&#21487;&#20197;&#21033;&#29992;&#21382;&#21490;&#25968;&#25454;&#32780;&#26080;&#38656;&#20648;&#23384;&#23427;&#20204;&#65307;3&#65289;&#25968;&#23398;&#35777;&#26126;&#20102;&#22312;&#24494;&#35843;&#21518;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most of current anomaly detection models assume that the normal pattern remains same all the time. However, the normal patterns of Web services change dramatically and frequently. The model trained on old-distribution data is outdated after such changes. Retraining the whole model every time is expensive. Besides, at the beginning of normal pattern changes, there is not enough observation data from the new distribution. Retraining a large neural network model with limited data is vulnerable to overfitting. Thus, we propose a Light and Anti-overfitting Retraining Approach (LARA) for deep variational auto-encoder based time series anomaly detection methods (VAEs). This work aims to make three novel contributions: 1) the retraining process is formulated as a convex problem and can converge at a fast rate as well as prevent overfitting; 2) designing a ruminate block, which leverages the historical data without the need to store them; 3) mathematically proving that when fine-tuning the late
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#35895;&#27468;Colab&#30340;&#24320;&#28304;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#29615;&#22659;&#65292;&#21487;&#29992;&#20110;&#22312;&#20108;&#32500;&#30697;&#24418;&#22495;&#19978;&#27169;&#25311;&#30636;&#24577;&#29616;&#35937;&#65292;&#24182;&#25552;&#20379;&#20102;&#22810;&#20010;&#29305;&#24615;&#21644;&#38382;&#39064;&#24211;&#12290;</title><link>http://arxiv.org/abs/2310.03755</link><description>&lt;p&gt;
&#36866;&#29992;&#20110;&#35895;&#27468;Colab&#30340;&#20108;&#32500;&#30636;&#24577;&#38382;&#39064;&#20013;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#20195;&#30721;&#65288;PINN-2DT&#65289;
&lt;/p&gt;
&lt;p&gt;
Physics Informed Neural Network Code for 2D Transient Problems (PINN-2DT) Compatible with Google Colab. (arXiv:2310.03755v1 [cs.CE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03755
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#35895;&#27468;Colab&#30340;&#24320;&#28304;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#29615;&#22659;&#65292;&#21487;&#29992;&#20110;&#22312;&#20108;&#32500;&#30697;&#24418;&#22495;&#19978;&#27169;&#25311;&#30636;&#24577;&#29616;&#35937;&#65292;&#24182;&#25552;&#20379;&#20102;&#22810;&#20010;&#29305;&#24615;&#21644;&#38382;&#39064;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24320;&#28304;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#29615;&#22659;&#65292;&#29992;&#20110;&#22312;&#20108;&#32500;&#30697;&#24418;&#22495;&#19978;&#27169;&#25311;&#30636;&#24577;&#29616;&#35937;&#65292;&#20855;&#26377;&#20197;&#19979;&#29305;&#28857;&#65306;&#65288;1&#65289;&#19982;&#35895;&#27468;Colab&#20860;&#23481;&#65292;&#21487;&#20197;&#22312;&#20113;&#29615;&#22659;&#20013;&#36827;&#34892;&#33258;&#21160;&#25191;&#34892;&#65307;&#65288;2&#65289;&#25903;&#25345;&#20108;&#32500;&#26102;&#21464;PDE&#65307;&#65288;3&#65289;&#25552;&#20379;&#31616;&#21333;&#30340;&#30028;&#38754;&#26469;&#23450;&#20041;&#27531;&#30041;&#25439;&#22833;&#12289;&#36793;&#30028;&#26465;&#20214;&#21644;&#21021;&#22987;&#25439;&#22833;&#65292;&#20197;&#21450;&#23427;&#20204;&#30340;&#26435;&#37325;&#65307;&#65288;4&#65289;&#25903;&#25345;&#35834;&#20381;&#26364;&#21644;&#36842;&#21033;&#20811;&#38647;&#36793;&#30028;&#26465;&#20214;&#65307;&#65288;5&#65289;&#20801;&#35768;&#33258;&#23450;&#20041;&#23618;&#25968;&#21644;&#27599;&#23618;&#30340;&#31070;&#32463;&#20803;&#25968;&#37327;&#65292;&#20197;&#21450;&#20219;&#24847;&#28608;&#27963;&#20989;&#25968;&#65307;&#65288;6&#65289;&#23398;&#20064;&#29575;&#21644;&#36845;&#20195;&#27425;&#25968;&#21487;&#20197;&#20316;&#20026;&#21442;&#25968;&#35843;&#33410;&#65307;&#65288;7&#65289;&#23545;&#31354;&#38388;&#21644;&#26102;&#38388;&#21464;&#37327;&#30340;PINN&#36827;&#34892;&#33258;&#21160;&#27714;&#23548;&#65307;&#65288;8&#65289;&#25552;&#20379;&#20102;&#32472;&#21046;&#25910;&#25947;&#24615;&#65288;&#20855;&#26377;&#28369;&#21160;&#24179;&#22343;&#65289;&#12289;&#23398;&#20064;&#21040;&#30340;&#21021;&#22987;&#26465;&#20214;&#12289;&#27169;&#25311;&#30340;&#20108;&#32500;&#21644;&#19977;&#32500;&#24555;&#29031;&#20197;&#21450;&#35270;&#39057;&#30340;&#24120;&#35268;&#20989;&#25968;&#65307;&#65288;9&#65289;&#21253;&#21547;&#20102;&#19968;&#20010;&#38382;&#39064;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an open-source Physics Informed Neural Network environment for simulations of transient phenomena on two-dimensional rectangular domains, with the following features: (1) it is compatible with Google Colab which allows automatic execution on cloud environment; (2) it supports two dimensional time-dependent PDEs; (3) it provides simple interface for definition of the residual loss, boundary condition and initial loss, together with their weights; (4) it support Neumann and Dirichlet boundary conditions; (5) it allows for customizing the number of layers and neurons per layer, as well as for arbitrary activation function; (6) the learning rate and number of epochs are available as parameters; (7) it automatically differentiates PINN with respect to spatial and temporal variables; (8) it provides routines for plotting the convergence (with running average), initial conditions learnt, 2D and 3D snapshots from the simulation and movies (9) it includes a library of problems: (a) n
&lt;/p&gt;</description></item><item><title>FLAIM&#26159;&#19968;&#20010;&#22312;&#32852;&#37030;&#35774;&#32622;&#20013;&#22522;&#20110;AIM&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#24046;&#20998;&#38544;&#31169;&#26041;&#21521;&#30340;&#25216;&#26415;&#22312;&#32852;&#37030;&#22330;&#26223;&#19979;&#30340;&#36866;&#29992;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;FLAIM&#26041;&#27861;&#26469;&#32500;&#25345;&#36739;&#39640;&#30340;&#25928;&#29992;&#21644;&#22788;&#29702;&#24322;&#26500;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.03447</link><description>&lt;p&gt;
FLAIM: &#22312;&#32852;&#37030;&#35774;&#32622;&#20013;&#22522;&#20110;AIM&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
FLAIM: AIM-based Synthetic Data Generation in the Federated Setting. (arXiv:2310.03447v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03447
&lt;/p&gt;
&lt;p&gt;
FLAIM&#26159;&#19968;&#20010;&#22312;&#32852;&#37030;&#35774;&#32622;&#20013;&#22522;&#20110;AIM&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#24046;&#20998;&#38544;&#31169;&#26041;&#21521;&#30340;&#25216;&#26415;&#22312;&#32852;&#37030;&#22330;&#26223;&#19979;&#30340;&#36866;&#29992;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;FLAIM&#26041;&#27861;&#26469;&#32500;&#25345;&#36739;&#39640;&#30340;&#25928;&#29992;&#21644;&#22788;&#29702;&#24322;&#26500;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20445;&#25252;&#20010;&#20154;&#38544;&#31169;&#21516;&#26102;&#23454;&#29616;&#21327;&#21516;&#25968;&#25454;&#20849;&#20139;&#23545;&#32452;&#32455;&#33267;&#20851;&#37325;&#35201;&#12290;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26159;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#20135;&#29983;&#19982;&#31169;&#26377;&#25968;&#25454;&#30340;&#32479;&#35745;&#29305;&#24615;&#30456;&#20284;&#30340;&#20154;&#24037;&#25968;&#25454;&#12290;&#34429;&#28982;&#22312;&#24046;&#20998;&#38544;&#31169;&#19979;&#24050;&#32463;&#35774;&#35745;&#20986;&#20102;&#35768;&#22810;&#25216;&#26415;&#65292;&#20294;&#23427;&#20204;&#20027;&#35201;&#20551;&#35774;&#25968;&#25454;&#26159;&#38598;&#20013;&#30340;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#24448;&#24448;&#20197;&#32852;&#37030;&#26041;&#24335;&#20998;&#24067;&#22312;&#22810;&#20010;&#23458;&#25143;&#31471;&#19978;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#22987;&#30740;&#31350;&#32852;&#37030;&#21512;&#25104;&#34920;&#25968;&#25454;&#29983;&#25104;&#12290;&#22312;AIM&#36825;&#20010;&#20808;&#36827;&#30340;&#20013;&#24515;&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DistAIM&#21644;FLAIM&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20998;&#21457;AIM&#26159;&#31616;&#21333;&#30340;&#65292;&#25193;&#23637;&#20102;&#22522;&#20110;&#23433;&#20840;&#22810;&#26041;&#35745;&#31639;&#30340;&#26368;&#26032;&#26041;&#27861;&#65292;&#20294;&#38656;&#35201;&#39069;&#22806;&#30340;&#24320;&#38144;&#65292;&#20351;&#20854;&#22312;&#32852;&#37030;&#22330;&#26223;&#20013;&#19981;&#22826;&#36866;&#29992;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#31616;&#21333;&#22320;&#32852;&#37030;AIM&#21487;&#33021;&#23548;&#33268;&#22312;&#24322;&#26500;&#24615;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#25928;&#29992;&#20005;&#37325;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;FLAIM&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#32500;&#25345;&#36739;&#39640;&#30340;&#25928;&#29992;&#65292;&#24182;&#19988;&#21487;&#20197;&#22788;&#29702;&#32852;&#37030;&#35774;&#32622;&#20013;&#30340;&#24322;&#26500;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Preserving individual privacy while enabling collaborative data sharing is crucial for organizations. Synthetic data generation is one solution, producing artificial data that mirrors the statistical properties of private data. While numerous techniques have been devised under differential privacy, they predominantly assume data is centralized. However, data is often distributed across multiple clients in a federated manner. In this work, we initiate the study of federated synthetic tabular data generation. Building upon a SOTA central method known as AIM, we present DistAIM and FLAIM. We show it is straightforward to distribute AIM, extending a recent approach based on secure multi-party computation which necessitates additional overhead, making it less suited to federated scenarios. We then demonstrate that naively federating AIM can lead to substantial degradation in utility under the presence of heterogeneity. To mitigate both issues, we propose an augmented FLAIM approach that mai
&lt;/p&gt;</description></item><item><title>SemiReward&#26159;&#19968;&#20010;&#36890;&#29992;&#22870;&#21169;&#27169;&#22411;&#65292;&#36890;&#36807;&#39044;&#27979;&#22870;&#21169;&#20998;&#25968;&#26469;&#35780;&#20272;&#21644;&#36807;&#28388;&#39640;&#36136;&#37327;&#30340;&#20266;&#26631;&#31614;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;&#21322;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.03013</link><description>&lt;p&gt;
SemiReward: &#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#36890;&#29992;&#22870;&#21169;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SemiReward: A General Reward Model for Semi-supervised Learning. (arXiv:2310.03013v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03013
&lt;/p&gt;
&lt;p&gt;
SemiReward&#26159;&#19968;&#20010;&#36890;&#29992;&#22870;&#21169;&#27169;&#22411;&#65292;&#36890;&#36807;&#39044;&#27979;&#22870;&#21169;&#20998;&#25968;&#26469;&#35780;&#20272;&#21644;&#36807;&#28388;&#39640;&#36136;&#37327;&#30340;&#20266;&#26631;&#31614;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;&#21322;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#23398;&#20064;&#22312;&#33258;&#35757;&#32451;&#26694;&#26550;&#21644;&#20266;&#26631;&#31614;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#20027;&#35201;&#25361;&#25112;&#26159;&#22914;&#20309;&#21306;&#20998;&#39640;&#36136;&#37327;&#30340;&#20266;&#26631;&#31614;&#65292;&#36991;&#20813;&#30830;&#35777;&#20559;&#35265;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20266;&#26631;&#31614;&#36873;&#25321;&#31574;&#30053;&#38480;&#21046;&#20110;&#39044;&#23450;&#20041;&#30340;&#26041;&#26696;&#25110;&#22797;&#26434;&#30340;&#25163;&#24037;&#21046;&#20316;&#31574;&#30053;&#65292;&#26080;&#27861;&#21516;&#26102;&#23454;&#29616;&#39640;&#36136;&#37327;&#26631;&#31614;&#12289;&#24555;&#36895;&#25910;&#25947;&#21644;&#20219;&#21153;&#22810;&#26679;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#22870;&#21169;&#26694;&#26550;&#65288;SemiReward&#65289;&#65292;&#29992;&#20110;&#39044;&#27979;&#22870;&#21169;&#20998;&#25968;&#20197;&#35780;&#20272;&#21644;&#36807;&#28388;&#39640;&#36136;&#37327;&#30340;&#20266;&#26631;&#31614;&#65292;&#21487;&#20197;&#22312;&#21508;&#31181;&#20219;&#21153;&#31867;&#22411;&#21644;&#22330;&#26223;&#19979;&#19982;&#20027;&#27969;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30456;&#32467;&#21512;&#20351;&#29992;&#12290;&#20026;&#20102;&#20943;&#23569;&#30830;&#35777;&#20559;&#35265;&#65292;&#22312;&#20004;&#20010;&#38454;&#27573;&#36890;&#36807;&#29983;&#25104;&#27169;&#22411;&#21644;&#23376;&#25277;&#26679;&#31574;&#30053;&#36827;&#34892;&#22312;&#32447;&#35757;&#32451;&#12290;&#36890;&#36807;&#22312;&#19977;&#31181;&#27169;&#24577;&#30340;13&#20010;&#26631;&#20934;&#21322;&#30417;&#30563;&#23398;&#20064;&#22522;&#20934;&#19978;&#36827;&#34892;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#30340;&#24191;&#27867;&#23454;&#39564;&#39564;&#35777;&#65292;&#34920;&#26126;SemiReward&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised learning (SSL) has witnessed great progress with various improvements in the self-training framework with pseudo labeling. The main challenge is how to distinguish high-quality pseudo labels against the confirmation bias. However, existing pseudo-label selection strategies are limited to pre-defined schemes or complex hand-crafted policies specially designed for classification, failing to achieve high-quality labels, fast convergence, and task versatility simultaneously. To these ends, we propose a Semi-supervised Reward framework (SemiReward) that predicts reward scores to evaluate and filter out high-quality pseudo labels, which is pluggable to mainstream SSL methods in wide task types and scenarios. To mitigate confirmation bias, SemiReward is trained online in two stages with a generator model and subsampling strategy. With classification and regression tasks on 13 standard SSL benchmarks of three modalities, extensive experiments verify that SemiReward achieves sig
&lt;/p&gt;</description></item><item><title>SWoTTeD&#26159;&#19968;&#31181;&#25193;&#23637;&#30340;&#24352;&#37327;&#20998;&#35299;&#26041;&#27861;&#65292;&#29992;&#20110;&#21457;&#29616;&#22797;&#26434;&#26102;&#38388;&#27169;&#24335;&#19979;&#30340;&#38544;&#34255;&#34920;&#24449;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;SWoTTeD&#19981;&#20165;&#33021;&#19982;&#26368;&#26032;&#30340;&#22522;&#20110;&#24352;&#37327;&#20998;&#35299;&#30340;&#26041;&#27861;&#19968;&#26679;&#20934;&#30830;&#22320;&#37325;&#24314;&#25968;&#25454;&#65292;&#36824;&#33021;&#25552;&#21462;&#20986;&#23545;&#20020;&#24202;&#21307;&#29983;&#26377;&#24847;&#20041;&#30340;&#26102;&#38388;&#34920;&#24449;&#12290;</title><link>http://arxiv.org/abs/2310.01201</link><description>&lt;p&gt;
SWoTTeD:&#24352;&#37327;&#20998;&#35299;&#22312;&#26102;&#38388;&#34920;&#24449;&#20013;&#30340;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
SWoTTeD: An Extension of Tensor Decomposition to Temporal Phenotyping. (arXiv:2310.01201v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01201
&lt;/p&gt;
&lt;p&gt;
SWoTTeD&#26159;&#19968;&#31181;&#25193;&#23637;&#30340;&#24352;&#37327;&#20998;&#35299;&#26041;&#27861;&#65292;&#29992;&#20110;&#21457;&#29616;&#22797;&#26434;&#26102;&#38388;&#27169;&#24335;&#19979;&#30340;&#38544;&#34255;&#34920;&#24449;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;SWoTTeD&#19981;&#20165;&#33021;&#19982;&#26368;&#26032;&#30340;&#22522;&#20110;&#24352;&#37327;&#20998;&#35299;&#30340;&#26041;&#27861;&#19968;&#26679;&#20934;&#30830;&#22320;&#37325;&#24314;&#25968;&#25454;&#65292;&#36824;&#33021;&#25552;&#21462;&#20986;&#23545;&#20020;&#24202;&#21307;&#29983;&#26377;&#24847;&#20041;&#30340;&#26102;&#38388;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24352;&#37327;&#20998;&#35299;&#26368;&#36817;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#23545;&#20110;&#20010;&#20307;&#36861;&#36394;&#25968;&#25454;&#30340;&#20998;&#26512;&#65292;&#22914;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;(EHR)&#65292;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#24403;&#25968;&#25454;&#36981;&#24490;&#22797;&#26434;&#30340;&#26102;&#38388;&#27169;&#24335;&#26102;&#65292;&#36825;&#20010;&#20219;&#21153;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#26102;&#38388;&#34920;&#24449;&#30340;&#27010;&#24565;&#65292;&#21363;&#19968;&#32452;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;SWoTTeD (Sliding Window for Temporal Tensor Decomposition)&#26041;&#27861;&#65292;&#19968;&#31181;&#21457;&#29616;&#38544;&#34255;&#26102;&#38388;&#27169;&#24335;&#30340;&#26032;&#26041;&#27861;&#12290;SWoTTeD&#38598;&#25104;&#20102;&#22810;&#31181;&#32422;&#26463;&#21644;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#25552;&#21462;&#21040;&#30340;&#34920;&#24449;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#36827;&#34892;&#39564;&#35777;&#65292;&#24182;&#25552;&#20379;&#20102;&#20351;&#29992;&#24052;&#40654;&#22823;&#23398;&#21307;&#38498;&#30340;&#25968;&#25454;&#30340;&#21407;&#22987;&#29992;&#20363;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;SWoTTeD&#33021;&#22815;&#33267;&#23569;&#19982;&#26368;&#26032;&#30340;&#22522;&#20110;&#24352;&#37327;&#20998;&#35299;&#30340;&#27169;&#22411;&#19968;&#26679;&#20934;&#30830;&#22320;&#37325;&#24314;&#25968;&#25454;&#65292;&#24182;&#25552;&#21462;&#21040;&#23545;&#20020;&#24202;&#21307;&#29983;&#26377;&#24847;&#20041;&#30340;&#26102;&#38388;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tensor decomposition has recently been gaining attention in the machine learning community for the analysis of individual traces, such as Electronic Health Records (EHR). However, this task becomes significantly more difficult when the data follows complex temporal patterns. This paper introduces the notion of a temporal phenotype as an arrangement of features over time and it proposes SWoTTeD (Sliding Window for Temporal Tensor Decomposition), a novel method to discover hidden temporal patterns. SWoTTeD integrates several constraints and regularizations to enhance the interpretability of the extracted phenotypes. We validate our proposal using both synthetic and real-world datasets, and we present an original usecase using data from the Greater Paris University Hospital. The results show that SWoTTeD achieves at least as accurate reconstruction as recent state-of-the-art tensor decomposition models, and extracts temporal phenotypes that are meaningful for clinicians.
&lt;/p&gt;</description></item><item><title>Evidential deep learning extends parametric deep learning to higher-order distributions, enabling the estimation of both aleatoric and epistemic uncertainty with one model. This study shows that evidential deep learning models achieve predictive accuracy comparable to standard methods while robustly quantifying both sources of uncertainty.</title><link>http://arxiv.org/abs/2309.13207</link><description>&lt;p&gt;
Evidential Deep Learning: Enhancing Predictive Uncertainty Estimation for Earth System Science Applications. (arXiv:2309.13207v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
Evidential Deep Learning: Enhancing Predictive Uncertainty Estimation for Earth System Science Applications. (arXiv:2309.13207v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13207
&lt;/p&gt;
&lt;p&gt;
Evidential deep learning extends parametric deep learning to higher-order distributions, enabling the estimation of both aleatoric and epistemic uncertainty with one model. This study shows that evidential deep learning models achieve predictive accuracy comparable to standard methods while robustly quantifying both sources of uncertainty.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#21487;&#38752;&#37327;&#21270;&#23545;&#20110;&#20102;&#35299;&#22825;&#27668;&#21644;&#27668;&#20505;&#32467;&#26524;&#30340;&#39537;&#21160;&#22240;&#32032;&#33267;&#20851;&#37325;&#35201;&#12290;&#38598;&#21512;&#25552;&#20379;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#24182;&#19988;&#21487;&#20197;&#36827;&#34892;&#29289;&#29702;&#20998;&#35299;&#65292;&#20294;&#29289;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#38598;&#21512;&#37117;&#38656;&#35201;&#35745;&#31639;&#37327;&#24456;&#22823;&#12290;&#21442;&#25968;&#21270;&#28145;&#24230;&#23398;&#20064;&#21487;&#20197;&#36890;&#36807;&#39044;&#27979;&#27010;&#29575;&#20998;&#24067;&#30340;&#21442;&#25968;&#26469;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#65292;&#20294;&#19981;&#32771;&#34385;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#12290;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#26159;&#19968;&#31181;&#23558;&#21442;&#25968;&#21270;&#28145;&#24230;&#23398;&#20064;&#25193;&#23637;&#21040;&#39640;&#38454;&#20998;&#24067;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#36890;&#36807;&#19968;&#20010;&#27169;&#22411;&#21516;&#26102;&#32771;&#34385;&#20004;&#31181;&#19981;&#30830;&#23450;&#24615;&#65306;&#38543;&#26426;&#35823;&#24046;&#21644;&#35748;&#35782;&#35823;&#24046;&#12290;&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20174;&#35777;&#25454;&#31070;&#32463;&#32593;&#32476;&#21644;&#38598;&#21512;&#20013;&#24471;&#21040;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#36890;&#36807;&#23545;&#20908;&#23395;&#38477;&#27700;&#31867;&#22411;&#30340;&#20998;&#31867;&#21644;&#22320;&#34920;&#23618;&#36890;&#37327;&#30340;&#22238;&#24402;&#24212;&#29992;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36798;&#21040;&#20102;&#19982;&#26631;&#20934;&#26041;&#27861;&#30456;&#23218;&#32654;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#21487;&#38752;&#22320;&#37327;&#21270;&#36825;&#20004;&#31181;&#26469;&#28304;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robust quantification of predictive uncertainty is critical for understanding factors that drive weather and climate outcomes. Ensembles provide predictive uncertainty estimates and can be decomposed physically, but both physics and machine learning ensembles are computationally expensive. Parametric deep learning can estimate uncertainty with one model by predicting the parameters of a probability distribution but do not account for epistemic uncertainty.. Evidential deep learning, a technique that extends parametric deep learning to higher-order distributions, can account for both aleatoric and epistemic uncertainty with one model. This study compares the uncertainty derived from evidential neural networks to those obtained from ensembles. Through applications of classification of winter precipitation type and regression of surface layer fluxes, we show evidential deep learning models attaining predictive accuracy rivaling standard methods, while robustly quantifying both sources of 
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#30740;&#32508;&#21512;&#23457;&#26597;&#20102;&#22312;&#25945;&#32946;&#25968;&#25454;&#25366;&#25496;&#20013;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#65292;&#21253;&#25324;&#23545;&#30693;&#35782;&#36319;&#36394;&#12289;&#23398;&#29983;&#19981;&#33391;&#34892;&#20026;&#26816;&#27979;&#12289;&#24615;&#33021;&#39044;&#27979;&#21644;&#20010;&#24615;&#21270;&#25512;&#33616;&#31561;&#20856;&#22411;&#25945;&#32946;&#22330;&#26223;&#30340;&#24212;&#29992;&#12290;&#21516;&#26102;&#25552;&#20379;&#20102;&#20844;&#20849;&#25968;&#25454;&#38598;&#21644;&#22788;&#29702;&#24037;&#20855;&#30340;&#32508;&#21512;&#27010;&#36848;&#65292;&#24182;&#25351;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2309.04761</link><description>&lt;p&gt;
&#22312;&#25945;&#32946;&#25968;&#25454;&#25366;&#25496;&#20013;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#32508;&#21512;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey on Deep Learning Techniques in Educational Data Mining. (arXiv:2309.04761v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04761
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#30740;&#32508;&#21512;&#23457;&#26597;&#20102;&#22312;&#25945;&#32946;&#25968;&#25454;&#25366;&#25496;&#20013;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#65292;&#21253;&#25324;&#23545;&#30693;&#35782;&#36319;&#36394;&#12289;&#23398;&#29983;&#19981;&#33391;&#34892;&#20026;&#26816;&#27979;&#12289;&#24615;&#33021;&#39044;&#27979;&#21644;&#20010;&#24615;&#21270;&#25512;&#33616;&#31561;&#20856;&#22411;&#25945;&#32946;&#22330;&#26223;&#30340;&#24212;&#29992;&#12290;&#21516;&#26102;&#25552;&#20379;&#20102;&#20844;&#20849;&#25968;&#25454;&#38598;&#21644;&#22788;&#29702;&#24037;&#20855;&#30340;&#32508;&#21512;&#27010;&#36848;&#65292;&#24182;&#25351;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25945;&#32946;&#25968;&#25454;&#25366;&#25496;(EDM)&#20316;&#20026;&#30740;&#31350;&#30340;&#37325;&#35201;&#39046;&#22495;&#65292;&#21033;&#29992;&#35745;&#31639;&#25216;&#26415;&#26469;&#20998;&#26512;&#25945;&#32946;&#25968;&#25454;&#12290;&#38543;&#30528;&#25945;&#32946;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#21644;&#22810;&#26679;&#24615;&#22686;&#21152;&#65292;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#35299;&#20915;&#20998;&#26512;&#21644;&#24314;&#27169;&#36825;&#20123;&#25968;&#25454;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;&#26412;&#35843;&#30740;&#26088;&#22312;&#31995;&#32479;&#22320;&#23457;&#26597;&#28145;&#24230;&#23398;&#20064;&#22312;EDM&#39046;&#22495;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20379;&#20102;&#20851;&#20110;EDM&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#31616;&#35201;&#20171;&#32461;&#65292;&#24378;&#35843;&#20102;&#23427;&#20204;&#22312;&#29616;&#20195;&#25945;&#32946;&#29615;&#22659;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#35814;&#32454;&#22238;&#39038;&#20102;&#22312;&#22235;&#20010;&#20856;&#22411;&#25945;&#32946;&#22330;&#26223;&#20013;&#24212;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#21253;&#25324;&#30693;&#35782;&#36319;&#36394;&#12289;&#23398;&#29983;&#19981;&#33391;&#34892;&#20026;&#26816;&#27979;&#12289;&#24615;&#33021;&#39044;&#27979;&#21644;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;EDM&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#21644;&#22788;&#29702;&#24037;&#20855;&#30340;&#32508;&#21512;&#27010;&#36848;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25351;&#20986;&#20102;&#35813;&#30740;&#31350;&#39046;&#22495;&#30340;&#26032;&#20852;&#36235;&#21183;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Educational Data Mining (EDM) has emerged as a vital field of research, which harnesses the power of computational techniques to analyze educational data. With the increasing complexity and diversity of educational data, Deep Learning techniques have shown significant advantages in addressing the challenges associated with analyzing and modeling this data. This survey aims to systematically review the state-of-the-art in EDM with Deep Learning. We begin by providing a brief introduction to EDM and Deep Learning, highlighting their relevance in the context of modern education. Next, we present a detailed review of Deep Learning techniques applied in four typical educational scenarios, including knowledge tracing, undesirable student detecting, performance prediction, and personalized recommendation. Furthermore, a comprehensive overview of public datasets and processing tools for EDM is provided. Finally, we point out emerging trends and future directions in this research area.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35843;&#26597;&#20102;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#20998;&#23376;&#22270;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#26367;&#25442;&#22270;&#29983;&#25104;&#27169;&#22411;&#30340;&#22522;&#30784;GNN&#26469;&#36827;&#34892;&#23454;&#39564;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20351;&#29992;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;GNN&#21487;&#20197;&#25913;&#21892;&#29983;&#25104;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.11978</link><description>&lt;p&gt;
&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#26159;&#21542;&#26356;&#22909;&#65311;
&lt;/p&gt;
&lt;p&gt;
Will More Expressive Graph Neural Networks do Better on Generative Tasks?. (arXiv:2308.11978v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11978
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35843;&#26597;&#20102;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#20998;&#23376;&#22270;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#26367;&#25442;&#22270;&#29983;&#25104;&#27169;&#22411;&#30340;&#22522;&#30784;GNN&#26469;&#36827;&#34892;&#23454;&#39564;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20351;&#29992;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;GNN&#21487;&#20197;&#25913;&#21892;&#29983;&#25104;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#29983;&#25104;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#23427;&#28041;&#21450;&#26681;&#25454;&#32473;&#23450;&#30340;&#26631;&#31614;&#39044;&#27979;&#19968;&#20010;&#23436;&#25972;&#30340;&#20855;&#26377;&#22810;&#20010;&#33410;&#28857;&#21644;&#36793;&#30340;&#22270;&#12290;&#36825;&#20010;&#20219;&#21153;&#23545;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#65292;&#21253;&#25324;&#33647;&#29289;&#21644;&#20998;&#23376;&#35774;&#35745;&#12290;&#36817;&#24180;&#26469;&#65292;&#22312;&#22270;&#29983;&#25104;&#39046;&#22495;&#20986;&#29616;&#20102;&#20960;&#31181;&#25104;&#21151;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#37325;&#22823;&#38382;&#39064;&#65306;(1) &#36825;&#20123;&#26041;&#27861;&#20013;&#20351;&#29992;&#30340;&#22522;&#30784;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26550;&#26500;&#24448;&#24448;&#26410;&#32463;&#28145;&#20837;&#25506;&#32034;&#65307;(2) &#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#21482;&#22312;&#26377;&#38480;&#30340;&#25351;&#26631;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#20026;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#22270;&#29983;&#25104;&#27169;&#22411;&#30340;&#22522;&#30784;GNN&#26367;&#25442;&#20026;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;GNN&#65292;&#30740;&#31350;&#20102;GNN&#22312;&#20998;&#23376;&#22270;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20004;&#31181;&#19981;&#21516;&#29983;&#25104;&#26694;&#26550;&#65288;GCPN&#21644;GraphAF&#65289;&#20013;&#20845;&#31181;GNN&#22312;&#20845;&#20010;&#19981;&#21516;&#30340;&#20998;&#23376;&#29983;&#25104;&#30446;&#26631;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph generation poses a significant challenge as it involves predicting a complete graph with multiple nodes and edges based on simply a given label. This task also carries fundamental importance to numerous real-world applications, including de-novo drug and molecular design. In recent years, several successful methods have emerged in the field of graph generation. However, these approaches suffer from two significant shortcomings: (1) the underlying Graph Neural Network (GNN) architectures used in these methods are often underexplored; and (2) these methods are often evaluated on only a limited number of metrics. To fill this gap, we investigate the expressiveness of GNNs under the context of the molecular graph generation task, by replacing the underlying GNNs of graph generative models with more expressive GNNs. Specifically, we analyse the performance of six GNNs in two different generative frameworks (GCPN and GraphAF), on six different molecular generative objectives on the ZIN
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;epsilon-ProVe&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#36817;&#20284;&#30340;&#26041;&#27861;&#26469;&#26522;&#20030;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#23433;&#20840;&#21306;&#22495;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#35777;&#26126;&#27010;&#29575;&#20445;&#35777;&#30340;&#32039;&#23494;&#19979;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2308.09842</link><description>&lt;p&gt;
&#29992;&#21487;&#35777;&#26126;&#27010;&#29575;&#20445;&#35777;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#26522;&#20030;&#23433;&#20840;&#21306;&#22495;
&lt;/p&gt;
&lt;p&gt;
Enumerating Safe Regions in Deep Neural Networks with Provable Probabilistic Guarantees. (arXiv:2308.09842v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09842
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;epsilon-ProVe&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#36817;&#20284;&#30340;&#26041;&#27861;&#26469;&#26522;&#20030;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#23433;&#20840;&#21306;&#22495;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#35777;&#26126;&#27010;&#29575;&#20445;&#35777;&#30340;&#32039;&#23494;&#19979;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35782;&#21035;&#23433;&#20840;&#21306;&#22495;&#26159;&#20445;&#35777;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#31995;&#32479;&#30340;&#20449;&#20219;&#30340;&#20851;&#38190;&#28857;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AllDNN-Verification&#38382;&#39064;&#65306;&#32473;&#23450;&#19968;&#20010;&#23433;&#20840;&#23646;&#24615;&#21644;&#19968;&#20010;DNN&#65292;&#26522;&#20030;&#23646;&#24615;&#36755;&#20837;&#22495;&#30340;&#25152;&#26377;&#23433;&#20840;&#21306;&#22495;&#65292;&#21363;&#23646;&#24615;&#25104;&#31435;&#30340;&#21306;&#22495;&#12290;&#30001;&#20110;&#38382;&#39064;&#30340;#P&#38590;&#24230;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#36817;&#20284;&#26041;&#27861;&#21483;&#20570;epsilon-ProVe&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#32479;&#35745;&#39044;&#27979;&#23481;&#38480;&#38480;&#21046;&#33719;&#24471;&#21487;&#25511;&#20302;&#20272;&#30340;&#36755;&#20986;&#21487;&#36798;&#38598;&#65292;&#24182;&#33021;&#22815;&#25552;&#20379;&#19968;&#20010;&#20855;&#26377;&#21487;&#35777;&#26126;&#27010;&#29575;&#20445;&#35777;&#30340;&#23433;&#20840;&#21306;&#22495;&#30340;&#32039;&#23494;&#19979;&#20272;&#35745;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#30340;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#30340;&#23454;&#35777;&#35780;&#20272;&#26174;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#26377;&#25928;&#24615;&#65292;&#20026;&#36825;&#31181;&#26032;&#22411;&#30340;DNN&#39564;&#35777;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifying safe areas is a key point to guarantee trust for systems that are based on Deep Neural Networks (DNNs). To this end, we introduce the AllDNN-Verification problem: given a safety property and a DNN, enumerate the set of all the regions of the property input domain which are safe, i.e., where the property does hold. Due to the #P-hardness of the problem, we propose an efficient approximation method called epsilon-ProVe. Our approach exploits a controllable underestimation of the output reachable sets obtained via statistical prediction of tolerance limits, and can provide a tight (with provable probabilistic guarantees) lower estimate of the safe areas. Our empirical evaluation on different standard benchmarks shows the scalability and effectiveness of our method, offering valuable insights for this new type of verification of DNNs.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;Transformer&#27169;&#22411;&#20013;&#30340;&#38544;&#34255;&#29366;&#24577;&#65292;&#21457;&#29616;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#32534;&#30721;&#20102;&#26576;&#20123;&#36890;&#29992;&#30693;&#35782;&#25552;&#21462;&#27169;&#24335;&#65292;&#22240;&#27492;&#22312;&#36827;&#34892;&#27169;&#22411;&#32534;&#36753;&#26102;&#65292;&#19981;&#38656;&#35201;&#26356;&#26032;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#30340;&#26435;&#37325;&#12290;</title><link>http://arxiv.org/abs/2308.08742</link><description>&lt;p&gt;
PMET: &#22312;Transformer&#20013;&#30340;&#31934;&#30830;&#27169;&#22411;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
PMET: Precise Model Editing in a Transformer. (arXiv:2308.08742v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08742
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;Transformer&#27169;&#22411;&#20013;&#30340;&#38544;&#34255;&#29366;&#24577;&#65292;&#21457;&#29616;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#32534;&#30721;&#20102;&#26576;&#20123;&#36890;&#29992;&#30693;&#35782;&#25552;&#21462;&#27169;&#24335;&#65292;&#22240;&#27492;&#22312;&#36827;&#34892;&#27169;&#22411;&#32534;&#36753;&#26102;&#65292;&#19981;&#38656;&#35201;&#26356;&#26032;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#30340;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#21487;&#20197;&#20197;&#36739;&#20302;&#30340;&#25104;&#26412;&#20462;&#25913;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23569;&#37327;&#30693;&#35782;&#65292;&#24182;&#19988;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#29616;&#26377;&#26041;&#27861;&#20551;&#35774;Transformer&#23618;&#38544;&#34255;&#29366;&#24577;&#26159;&#21069;&#39304;&#32593;&#32476;&#30340;&#38190;&#20540;&#20869;&#23384;&#30340;&#20540;&#12290;&#23427;&#20204;&#36890;&#24120;&#20248;&#21270;Transformer&#23618;&#38544;&#34255;&#29366;&#24577;&#26469;&#35760;&#24518;&#30446;&#26631;&#30693;&#35782;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#26356;&#26032;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21069;&#39304;&#32593;&#32476;&#30340;&#26435;&#37325;&#12290;&#28982;&#32780;&#65292;Transformer&#23618;&#38544;&#34255;&#29366;&#24577;&#30340;&#20449;&#24687;&#27969;&#26469;&#33258;&#19977;&#20010;&#37096;&#20998;&#65306;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#12289;&#21069;&#39304;&#32593;&#32476;&#21644;&#27531;&#24046;&#36830;&#25509;&#12290;&#29616;&#26377;&#26041;&#27861;&#24573;&#35270;&#20102;Transformer&#23618;&#38544;&#34255;&#29366;&#24577;&#21253;&#21547;&#20102;&#21069;&#39304;&#32593;&#32476;&#29305;&#21035;&#38656;&#35201;&#30340;&#20449;&#24687;&#36825;&#19968;&#20107;&#23454;&#12290;&#22240;&#27492;&#65292;&#27169;&#22411;&#32534;&#36753;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#27169;&#22411;&#32534;&#36753;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#21644;&#21069;&#39304;&#32593;&#32476;&#30340;&#38544;&#34255;&#29366;&#24577;&#65292;&#21457;&#29616;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#32534;&#30721;&#20102;&#26576;&#20123;&#36890;&#29992;&#30693;&#35782;&#25552;&#21462;&#27169;&#24335;&#12290;&#36825;&#24847;&#21619;&#30528;&#24403;&#24341;&#20837;&#26032;&#30693;&#35782;&#26102;&#65292;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#30340;&#26435;&#37325;&#19981;&#38656;&#35201;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model editing techniques modify a minor proportion of knowledge in Large Language Models (LLMs) at a relatively low cost, which have demonstrated notable success. Existing methods assume Transformer Layer (TL) hidden states are values of key-value memories of the Feed-Forward Network (FFN). They usually optimize the TL hidden states to memorize target knowledge and use it to update the weights of the FFN in LLMs. However, the information flow of TL hidden states comes from three parts: Multi-Head Self-Attention (MHSA), FFN, and residual connections. Existing methods neglect the fact that the TL hidden states contains information not specifically required for FFN. Consequently, the performance of model editing decreases. To achieve more precise model editing, we analyze hidden states of MHSA and FFN, finding that MHSA encodes certain general knowledge extraction patterns. This implies that MHSA weights do not require updating when new knowledge is introduced. Based on above findings, we
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#20855;&#26377;&#26410;&#35266;&#27979;&#28151;&#28102;&#22240;&#32032;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#38382;&#39064;&#65292;&#22312;&#32467;&#26524;&#26159;&#30830;&#23450;&#24615;&#29983;&#25104;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21333;&#19968;&#20195;&#29702;&#21464;&#37327;&#30340;&#20869;&#26680;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#22238;&#24402;&#21644;&#26368;&#22823;&#30697;&#32422;&#26463;&#30340;&#26041;&#27861;&#21487;&#20197;&#19968;&#33268;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#65292;&#24182;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#25104;&#21151;&#24674;&#22797;&#20102;&#22240;&#26524;&#25928;&#24212;&#12290;</title><link>http://arxiv.org/abs/2308.04585</link><description>&lt;p&gt;
&#20915;&#23450;&#24615;&#28151;&#28102;&#19979;&#30340;&#20869;&#26680;&#21333;&#19968;&#20195;&#29702;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Kernel Single Proxy Control for Deterministic Confounding. (arXiv:2308.04585v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04585
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#20855;&#26377;&#26410;&#35266;&#27979;&#28151;&#28102;&#22240;&#32032;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#38382;&#39064;&#65292;&#22312;&#32467;&#26524;&#26159;&#30830;&#23450;&#24615;&#29983;&#25104;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21333;&#19968;&#20195;&#29702;&#21464;&#37327;&#30340;&#20869;&#26680;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#22238;&#24402;&#21644;&#26368;&#22823;&#30697;&#32422;&#26463;&#30340;&#26041;&#27861;&#21487;&#20197;&#19968;&#33268;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#65292;&#24182;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#25104;&#21151;&#24674;&#22797;&#20102;&#22240;&#26524;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20855;&#26377;&#26410;&#35266;&#27979;&#28151;&#28102;&#22240;&#32032;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#38382;&#39064;&#65292;&#20854;&#20013;&#25105;&#20204;&#35266;&#27979;&#21040;&#19982;&#28151;&#28102;&#22240;&#32032;&#30456;&#20851;&#30340;&#20195;&#29702;&#21464;&#37327;&#12290;&#23613;&#31649;&#20195;&#29702;&#22240;&#26524;&#23398;&#20064;&#65288;PCL&#65289;&#20351;&#29992;&#20004;&#20010;&#20195;&#29702;&#21464;&#37327;&#26469;&#24674;&#22797;&#30495;&#23454;&#30340;&#22240;&#26524;&#25928;&#24212;&#65292;&#25105;&#20204;&#35777;&#26126;&#22914;&#26524;&#32467;&#26524;&#26159;&#30830;&#23450;&#24615;&#29983;&#25104;&#30340;&#65292;&#21017;&#20351;&#29992;&#21333;&#20010;&#20195;&#29702;&#21464;&#37327;&#23601;&#36275;&#20197;&#36827;&#34892;&#22240;&#26524;&#20272;&#35745;&#65292;&#24182;&#27010;&#25324;&#20102;&#25511;&#21046;&#32467;&#26524;&#26657;&#20934;&#27861;&#65288;COCA&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#20869;&#26680;&#30340;&#26041;&#27861;&#65306;&#19968;&#31181;&#22522;&#20110;&#20004;&#38454;&#27573;&#22238;&#24402;&#26041;&#27861;&#65292;&#21478;&#19968;&#31181;&#22522;&#20110;&#26368;&#22823;&#30697;&#32422;&#26463;&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#21487;&#20197;&#19968;&#33268;&#22320;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#65292;&#24182;&#36890;&#36807;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#23454;&#35777;&#23454;&#39564;&#25104;&#21151;&#22320;&#24674;&#22797;&#20102;&#22240;&#26524;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of causal effect estimation with an unobserved confounder, where we observe a proxy variable that is associated with the confounder. Although Proxy Causal Learning (PCL) uses two proxy variables to recover the true causal effect, we show that a single proxy variable is sufficient for causal estimation if the outcome is generated deterministically, generalizing Control Outcome Calibration Approach (COCA). We propose two kernel-based methods for this setting: the first based on the two-stage regression approach, and the second based on a maximum moment restriction approach. We prove that both approaches can consistently estimate the causal effect, and we empirically demonstrate that we can successfully recover the causal effect on a synthetic dataset.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#21644;&#36807;&#25311;&#21512;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#27491;&#21017;&#21270;&#25439;&#22833;&#20989;&#25968;&#21644;&#25552;&#21069;&#20572;&#27490;&#31574;&#30053;&#26469;&#20248;&#21270;&#32593;&#32476;&#21442;&#25968;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.01421</link><description>&lt;p&gt;
&#27491;&#21017;&#21270;&#12289;&#25552;&#21069;&#20572;&#27490;&#21644;&#26790;&#24819;&#65306;&#19968;&#31181;&#22788;&#29702;&#27867;&#21270;&#21644;&#36807;&#25311;&#21512;&#30340;&#31867;Hopfield&#35774;&#32622;
&lt;/p&gt;
&lt;p&gt;
Regularization, early-stopping and dreaming: a Hopfield-like setup to address generalization and overfitting. (arXiv:2308.01421v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01421
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#21644;&#36807;&#25311;&#21512;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#27491;&#21017;&#21270;&#25439;&#22833;&#20989;&#25968;&#21644;&#25552;&#21069;&#20572;&#27490;&#31574;&#30053;&#26469;&#20248;&#21270;&#32593;&#32476;&#21442;&#25968;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#26426;&#22120;&#23398;&#20064;&#30340;&#35282;&#24230;&#26469;&#22788;&#29702;&#21560;&#24341;&#23376;&#31070;&#32463;&#32593;&#32476;&#65306;&#36890;&#36807;&#23545;&#27491;&#21017;&#21270;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#26799;&#24230;&#19979;&#38477;&#26469;&#23547;&#25214;&#26368;&#20248;&#32593;&#32476;&#21442;&#25968;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#26368;&#20248;&#30340;&#31070;&#32463;&#20803;&#20132;&#20114;&#30697;&#38453;&#34987;&#35777;&#26126;&#26159;&#19968;&#31867;&#36890;&#36807;&#36845;&#20195;&#24212;&#29992;&#26576;&#20123;&#21462;&#28040;&#23398;&#20064;&#21327;&#35758;&#20462;&#35746;&#30340;Hebbian&#26680;&#30697;&#38453;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#21462;&#28040;&#23398;&#20064;&#27493;&#39588;&#30340;&#25968;&#37327;&#34987;&#35777;&#26126;&#19982;&#25439;&#22833;&#20989;&#25968;&#30340;&#27491;&#21017;&#21270;&#36229;&#21442;&#25968;&#21644;&#35757;&#32451;&#26102;&#38388;&#26377;&#20851;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21487;&#20197;&#35774;&#35745;&#36991;&#20813;&#36807;&#25311;&#21512;&#30340;&#31574;&#30053;&#65292;&#36825;&#20123;&#31574;&#30053;&#21487;&#20197;&#29992;&#20132;&#20114;&#30697;&#38453;&#30340;&#20195;&#25968;&#24615;&#36136;&#26469;&#25551;&#36848;&#65292;&#25110;&#32773;&#31561;&#20215;&#22320;&#29992;&#27491;&#21017;&#21270;&#35843;&#25972;&#21644;&#25552;&#21069;&#20572;&#27490;&#31574;&#30053;&#26469;&#25551;&#36848;&#12290;&#36824;&#30740;&#31350;&#20102;&#36825;&#20123;&#21560;&#24341;&#23376;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#65306;&#38024;&#23545;&#38543;&#26426;&#21512;&#25104;&#25968;&#25454;&#38598;&#33719;&#24471;&#20102;&#20998;&#26512;&#32467;&#26524;&#65292;&#38543;&#21518;&#29992;&#25968;&#20540;&#23454;&#39564;&#26469;&#39564;&#35777;&#20102;&#25152;&#24471;&#21040;&#30340;&#25972;&#20307;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we approach attractor neural networks from a machine learning perspective: we look for optimal network parameters by applying a gradient descent over a regularized loss function. Within this framework, the optimal neuron-interaction matrices turn out to be a class of matrices which correspond to Hebbian kernels revised by iteratively applying some unlearning protocols. Remarkably, the number of unlearning steps is proved to be related to the regularization hyperparameters of the loss function and to the training time. Thus, we can design strategies to avoid overfitting that are formulated in terms of the algebraic properties of the interaction matrix, or, equivalently, in terms of regularization tuning and early-stopping strategies. The generalization capabilities of these attractor networks are also investigated: analytical results are obtained for random synthetic datasets, next, the emerging picture is corroborated by numerical experiments that highlight the existence o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WEPRO&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21442;&#25968;&#26435;&#37325;&#20013;&#30340;&#35268;&#24459;&#36235;&#21183;&#21152;&#36895;&#20102;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#30456;&#27604;&#26631;&#20934;&#35757;&#32451;&#26041;&#27861;&#65292;&#36895;&#24230;&#25552;&#39640;&#32422;2.25&#20493;&#65292;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;2.3%&#65292;&#25439;&#22833;&#38477;&#20302;&#20102;6.1%&#12290;</title><link>http://arxiv.org/abs/2307.12449</link><description>&lt;p&gt;
WEPRO: &#29992;&#20110;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#31639;&#27861;&#39640;&#25928;&#20248;&#21270;&#30340;&#26435;&#37325;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
WEPRO: Weight Prediction for Efficient Optimization of Hybrid Quantum-Classical Algorithms. (arXiv:2307.12449v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12449
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WEPRO&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21442;&#25968;&#26435;&#37325;&#20013;&#30340;&#35268;&#24459;&#36235;&#21183;&#21152;&#36895;&#20102;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#30456;&#27604;&#26631;&#20934;&#35757;&#32451;&#26041;&#27861;&#65292;&#36895;&#24230;&#25552;&#39640;&#32422;2.25&#20493;&#65292;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;2.3%&#65292;&#25439;&#22833;&#38477;&#20302;&#20102;6.1%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#27169;&#25311;&#22120;&#22312;&#32463;&#20856;&#35745;&#31639;&#26426;&#19978;&#30340;&#25351;&#25968;&#32423;&#36816;&#34892;&#26102;&#38388;&#12289;&#30495;&#23454;&#37327;&#23376;&#35774;&#22791;&#30340;&#38271;&#38431;&#21015;&#28145;&#24230;&#21644;&#39640;&#25104;&#26412;&#32473;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;(VQA)&#22914;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;(QNNs)&#12289;&#21464;&#20998;&#37327;&#23376;&#26412;&#24449;&#27714;&#35299;&#22120;(VQE)&#21644;&#37327;&#23376;&#36817;&#20284;&#20248;&#21270;&#31639;&#27861;(QAOA)&#30340;&#26377;&#25928;&#35757;&#32451;&#24102;&#26469;&#20102;&#24040;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;WEPRO(&#26435;&#37325;&#39044;&#27979;)&#65292;&#36890;&#36807;&#21033;&#29992;&#21442;&#25968;&#26435;&#37325;&#20013;&#30340;&#35268;&#24459;&#36235;&#21183;&#26469;&#21152;&#24555;VQA&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#20248;&#21270;&#39044;&#27979;&#24615;&#33021;&#30340;&#25216;&#26415;&#65292;&#21363;Naive Prediction(NaP)&#21644;Adaptive Prediction(AdaP)&#12290;&#36890;&#36807;&#23545;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#22810;&#20010;QNN&#27169;&#22411;&#30340;&#24191;&#27867;&#23454;&#39564;&#21644;&#35757;&#32451;&#65292;&#25105;&#20204;&#35777;&#26126;WEPRO&#30456;&#23545;&#20110;&#26631;&#20934;&#35757;&#32451;&#26041;&#27861;&#21152;&#24555;&#20102;&#22823;&#32422;2.25&#20493;&#30340;&#36895;&#24230;&#65292;&#21516;&#26102;&#22312;&#23384;&#20648;&#21644;&#35745;&#31639;&#24320;&#38144;&#36739;&#20302;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;(&#39640;&#36798;2.3%)&#21644;&#26356;&#20302;&#30340;&#25439;&#22833;(&#39640;&#36798;6.1%)&#12290;
&lt;/p&gt;
&lt;p&gt;
The exponential run time of quantum simulators on classical machines and long queue depths and high costs of real quantum devices present significant challenges in the effective training of Variational Quantum Algorithms (VQAs) like Quantum Neural Networks (QNNs), Variational Quantum Eigensolver (VQE) and Quantum Approximate Optimization Algorithm (QAOA). To address these limitations, we propose a new approach, WEPRO (Weight Prediction), which accelerates the convergence of VQAs by exploiting regular trends in the parameter weights. We introduce two techniques for optimal prediction performance namely, Naive Prediction (NaP) and Adaptive Prediction (AdaP). Through extensive experimentation and training of multiple QNN models on various datasets, we demonstrate that WEPRO offers a speedup of approximately $2.25\times$ compared to standard training methods, while also providing improved accuracy (up to $2.3\%$ higher) and loss (up to $6.1\%$ lower) with low storage and computational over
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#24046;&#20998;&#38544;&#31169;&#32447;&#24615;&#20248;&#21270;&#20013;&#29305;&#24449;&#39044;&#22788;&#29702;&#30340;&#37325;&#35201;&#24615;&#12290;&#22312;&#31616;&#21333;&#30340;&#32447;&#24615;&#20998;&#31867;&#24773;&#20917;&#19979;&#65292;&#19982;&#38750;&#38544;&#31169;&#20248;&#21270;&#30456;&#27604;&#65292;&#29305;&#24449;&#39044;&#22788;&#29702;&#23545;&#20110;&#24046;&#20998;&#38544;&#31169;&#20248;&#21270;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#21542;&#21017;&#20250;&#20135;&#29983;&#19982;&#29305;&#24449;&#26368;&#22823;&#33539;&#25968;&#25104;&#27604;&#20363;&#30340;&#38544;&#31169;&#38169;&#35823;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#29305;&#24449;&#39044;&#22788;&#29702;&#30340;&#31639;&#27861;DPSGD-F&#12290;</title><link>http://arxiv.org/abs/2307.11106</link><description>&lt;p&gt;
&#29305;&#24449;&#39044;&#22788;&#29702;&#23545;&#24046;&#20998;&#38544;&#31169;&#32447;&#24615;&#20248;&#21270;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
The importance of feature preprocessing for differentially private linear optimization. (arXiv:2307.11106v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11106
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#24046;&#20998;&#38544;&#31169;&#32447;&#24615;&#20248;&#21270;&#20013;&#29305;&#24449;&#39044;&#22788;&#29702;&#30340;&#37325;&#35201;&#24615;&#12290;&#22312;&#31616;&#21333;&#30340;&#32447;&#24615;&#20998;&#31867;&#24773;&#20917;&#19979;&#65292;&#19982;&#38750;&#38544;&#31169;&#20248;&#21270;&#30456;&#27604;&#65292;&#29305;&#24449;&#39044;&#22788;&#29702;&#23545;&#20110;&#24046;&#20998;&#38544;&#31169;&#20248;&#21270;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#21542;&#21017;&#20250;&#20135;&#29983;&#19982;&#29305;&#24449;&#26368;&#22823;&#33539;&#25968;&#25104;&#27604;&#20363;&#30340;&#38544;&#31169;&#38169;&#35823;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#29305;&#24449;&#39044;&#22788;&#29702;&#30340;&#31639;&#27861;DPSGD-F&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#20960;&#24180;&#20013;&#65292;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#20854;&#20013;&#26368;&#27969;&#34892;&#30340;&#29992;&#20110;&#35757;&#32451;&#24046;&#20998;&#38544;&#31169;&#27169;&#22411;&#30340;&#31639;&#27861;&#20043;&#19968;&#26159;&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;DPSGD&#65289;&#21450;&#20854;&#21464;&#31181;&#65292;&#22312;&#27599;&#20010;&#27493;&#39588;&#20013;&#65292;&#26799;&#24230;&#34987;&#21098;&#35009;&#24182;&#19982;&#19968;&#20123;&#22122;&#38899;&#32467;&#21512;&#12290;&#37492;&#20110;DPSGD&#30340;&#24191;&#27867;&#20351;&#29992;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#38382;&#39064;&#65306;&#22312;&#38544;&#31169;&#32422;&#26463;&#19979;&#65292;&#20165;&#20165;&#20351;&#29992;DPSGD&#26159;&#21542;&#36275;&#20197;&#25214;&#21040;&#27599;&#20010;&#25968;&#25454;&#38598;&#30340;&#33391;&#22909;&#26497;&#23567;&#20540;&#28857;&#65311;&#20316;&#20026;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#30340;&#31532;&#19968;&#27493;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#23545;&#20110;&#31616;&#21333;&#30340;&#32447;&#24615;&#20998;&#31867;&#24773;&#20917;&#65292;&#19982;&#38750;&#38544;&#31169;&#20248;&#21270;&#30456;&#27604;&#65292;&#65288;&#31169;&#26377;&#65289;&#29305;&#24449;&#39044;&#22788;&#29702;&#23545;&#20110;&#24046;&#20998;&#38544;&#31169;&#20248;&#21270;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#23384;&#22312;&#19968;&#31181;&#20363;&#23376;&#65292;&#22312;&#27809;&#26377;&#29305;&#24449;&#39044;&#22788;&#29702;&#30340;&#24773;&#20917;&#19979;&#65292;DPSGD&#20250;&#20135;&#29983;&#19982;&#25152;&#26377;&#26679;&#26412;&#19978;&#30340;&#29305;&#24449;&#30340;&#26368;&#22823;&#33539;&#25968;&#25104;&#27604;&#20363;&#30340;&#38544;&#31169;&#38169;&#35823;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DPSGD-F&#30340;&#31639;&#27861;&#65292;&#23558;DPSGD&#19982;&#29305;&#24449;&#39044;&#22788;&#29702;&#32467;&#21512;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training machine learning models with differential privacy (DP) has received increasing interest in recent years. One of the most popular algorithms for training differentially private models is differentially private stochastic gradient descent (DPSGD) and its variants, where at each step gradients are clipped and combined with some noise. Given the increasing usage of DPSGD, we ask the question: is DPSGD alone sufficient to find a good minimizer for every dataset under privacy constraints? As a first step towards answering this question, we show that even for the simple case of linear classification, unlike non-private optimization, (private) feature preprocessing is vital for differentially private optimization. In detail, we first show theoretically that there exists an example where without feature preprocessing, DPSGD incurs a privacy error proportional to the maximum norm of features over all samples. We then propose an algorithm called DPSGD-F, which combines DPSGD with feature
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36873;&#25321;&#27169;&#22411;&#21644;&#32622;&#25442;&#19981;&#21464;&#24615;&#30340;&#22522;&#26412;&#29305;&#24449;&#21270;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#38750;&#21442;&#25968;&#20272;&#35745;&#22120;&#36924;&#36817;&#36873;&#25321;&#20989;&#25968;&#65292;&#20197;&#21450;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#28789;&#27963;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.07090</link><description>&lt;p&gt;
&#36873;&#25321;&#27169;&#22411;&#21644;&#32622;&#25442;&#19981;&#21464;&#24615;
&lt;/p&gt;
&lt;p&gt;
Choice Models and Permutation Invariance. (arXiv:2307.07090v1 [econ.EM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07090
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36873;&#25321;&#27169;&#22411;&#21644;&#32622;&#25442;&#19981;&#21464;&#24615;&#30340;&#22522;&#26412;&#29305;&#24449;&#21270;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#38750;&#21442;&#25968;&#20272;&#35745;&#22120;&#36924;&#36817;&#36873;&#25321;&#20989;&#25968;&#65292;&#20197;&#21450;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#28789;&#27963;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36873;&#25321;&#24314;&#27169;&#26159;&#35768;&#22810;&#32463;&#27982;&#23398;&#12289;&#36816;&#33829;&#21644;&#33829;&#38144;&#38382;&#39064;&#30340;&#26680;&#24515;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#36873;&#25321;&#20989;&#25968;&#36827;&#34892;&#22522;&#26412;&#29305;&#24449;&#21270;&#30340;&#26041;&#27861;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#29616;&#26377;&#30340;&#36873;&#25321;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#38750;&#21442;&#25968;&#20272;&#35745;&#22120;&#22914;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#33021;&#22815;&#36731;&#26494;&#36924;&#36817;&#27492;&#31867;&#20989;&#25968;&#65292;&#24182;&#20811;&#26381;&#22312;&#38750;&#21442;&#25968;&#20272;&#35745;&#36873;&#25321;&#20989;&#25968;&#20013;&#22266;&#26377;&#30340;&#32500;&#25968;&#28798;&#38590;&#12290;&#36890;&#36807;&#22823;&#37327;&#27169;&#25311;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#20989;&#25968;&#21487;&#20197;&#20197;&#23436;&#20840;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#28789;&#27963;&#25429;&#25417;&#28508;&#22312;&#30340;&#28040;&#36153;&#32773;&#34892;&#20026;&#65292;&#24182;&#32988;&#36807;&#20256;&#32479;&#30340;&#21442;&#25968;&#27169;&#22411;&#12290;&#22240;&#20026;&#38656;&#27714;&#35774;&#32622;&#24120;&#24120;&#20855;&#26377;&#20869;&#29983;&#29305;&#24449;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#25193;&#23637;&#21040;&#21253;&#25324;&#22312;&#20869;&#29983;&#29305;&#24449;&#19979;&#30340;&#20272;&#35745;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25551;&#36848;&#20102;&#19968;&#20010;&#24418;&#24335;&#25512;&#29702;&#31243;&#24207;&#65292;&#20197;&#26500;&#24314;&#20851;&#20110;&#20215;&#26684;&#24377;&#24615;&#31561;&#24863;&#20852;&#36259;&#23545;&#35937;&#30340;&#26377;&#25928;&#32622;&#20449;&#21306;&#38388;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#20272;&#35745;&#22120;&#30340;&#23454;&#38469;&#36866;&#29992;&#24615;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#23454;&#35777;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Choice Modeling is at the core of many economics, operations, and marketing problems. In this paper, we propose a fundamental characterization of choice functions that encompasses a wide variety of extant choice models. We demonstrate how nonparametric estimators like neural nets can easily approximate such functionals and overcome the curse of dimensionality that is inherent in the non-parametric estimation of choice functions. We demonstrate through extensive simulations that our proposed functionals can flexibly capture underlying consumer behavior in a completely data-driven fashion and outperform traditional parametric models. As demand settings often exhibit endogenous features, we extend our framework to incorporate estimation under endogenous features. Further, we also describe a formal inference procedure to construct valid confidence intervals on objects of interest like price elasticity. Finally, to assess the practical applicability of our estimator, we utilize a real-world
&lt;/p&gt;</description></item><item><title>TGRL&#26159;&#19968;&#31181;&#29992;&#20110;&#25945;&#24072;&#24341;&#23548;&#24378;&#21270;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#21644;&#33258;&#21160;&#24179;&#34913;&#20309;&#26102;&#36981;&#24490;&#25945;&#24072;&#25351;&#23548;&#21644;&#20309;&#26102;&#20351;&#29992;&#22870;&#21169;&#65292;&#25945;&#24072;&#30417;&#30563;&#30340;&#37325;&#35201;&#24615;&#20250;&#26681;&#25454;&#20195;&#29702;&#30340;&#34920;&#29616;&#35843;&#25972;&#12290;</title><link>http://arxiv.org/abs/2307.03186</link><description>&lt;p&gt;
TGRL:&#19968;&#31181;&#29992;&#20110;&#25945;&#24072;&#24341;&#23548;&#24378;&#21270;&#23398;&#20064;&#30340;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
TGRL: An Algorithm for Teacher Guided Reinforcement Learning. (arXiv:2307.03186v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03186
&lt;/p&gt;
&lt;p&gt;
TGRL&#26159;&#19968;&#31181;&#29992;&#20110;&#25945;&#24072;&#24341;&#23548;&#24378;&#21270;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#21644;&#33258;&#21160;&#24179;&#34913;&#20309;&#26102;&#36981;&#24490;&#25945;&#24072;&#25351;&#23548;&#21644;&#20309;&#26102;&#20351;&#29992;&#22870;&#21169;&#65292;&#25945;&#24072;&#30417;&#30563;&#30340;&#37325;&#35201;&#24615;&#20250;&#26681;&#25454;&#20195;&#29702;&#30340;&#34920;&#29616;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#22870;&#21169;(&#21363;&#24378;&#21270;&#23398;&#20064;&#25110;RL)&#21644;&#23398;&#20064;&#27169;&#20223;&#25945;&#24072;(&#21363;&#25945;&#24072;-&#23398;&#29983;&#23398;&#20064;)&#26159;&#35299;&#20915;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#30340;&#20004;&#31181;&#25104;&#29087;&#26041;&#27861;&#12290;&#20026;&#20102;&#32467;&#21512;&#36825;&#20123;&#19981;&#21516;&#24418;&#24335;&#23398;&#20064;&#30340;&#20248;&#28857;&#65292;&#36890;&#24120;&#20250;&#35757;&#32451;&#19968;&#20010;&#31574;&#30053;&#26469;&#26368;&#22823;&#21270;&#24378;&#21270;&#23398;&#20064;&#21644;&#25945;&#24072;-&#23398;&#29983;&#23398;&#20064;&#30446;&#26631;&#30340;&#32452;&#21512;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#27809;&#26377;&#19968;&#20010;&#26377;&#21407;&#21017;&#30340;&#26041;&#27861;&#26469;&#24179;&#34913;&#36825;&#20123;&#30446;&#26631;&#65292;&#20043;&#21069;&#30340;&#24037;&#20316;&#20351;&#29992;&#21551;&#21457;&#24335;&#26041;&#27861;&#21644;&#38382;&#39064;&#29305;&#23450;&#30340;&#36229;&#21442;&#25968;&#25628;&#32034;&#26469;&#24179;&#34913;&#20004;&#20010;&#30446;&#26631;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;"&#26377;&#21407;&#21017;"&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20284;&#23454;&#29616;"&#21160;&#24577;"&#21644;"&#33258;&#21160;"&#24179;&#34913;&#20309;&#26102;&#36981;&#24490;&#25945;&#24072;&#21644;&#20309;&#26102;&#20351;&#29992;&#22870;&#21169;&#12290;&#20027;&#35201;&#24605;&#24819;&#26159;&#36890;&#36807;&#27604;&#36739;&#20195;&#29702;&#30340;&#24615;&#33021;&#19982;&#27809;&#26377;&#25945;&#24072;&#30417;&#30563;&#24182;&#21482;&#20174;&#22870;&#21169;&#20013;&#23398;&#20064;&#30340;&#23545;&#29031;&#24773;&#26223;&#26469;&#35843;&#25972;&#25945;&#24072;&#30417;&#30563;&#30340;&#37325;&#35201;&#24615;&#12290;&#22914;&#26524;&#20351;&#29992;&#25945;&#24072;&#30417;&#30563;&#25913;&#21892;&#20102;&#20195;&#29702;&#30340;&#24615;&#33021;&#65292;&#37027;&#20040;&#25945;&#24072;&#30417;&#30563;&#30340;&#37325;&#35201;&#24615;&#23601;&#20250;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from rewards (i.e., reinforcement learning or RL) and learning to imitate a teacher (i.e., teacher-student learning) are two established approaches for solving sequential decision-making problems. To combine the benefits of these different forms of learning, it is common to train a policy to maximize a combination of reinforcement and teacher-student learning objectives. However, without a principled method to balance these objectives, prior work used heuristics and problem-specific hyperparameter searches to balance the two objectives. We present a $\textit{principled}$ approach, along with an approximate implementation for $\textit{dynamically}$ and $\textit{automatically}$ balancing when to follow the teacher and when to use rewards. The main idea is to adjust the importance of teacher supervision by comparing the agent's performance to the counterfactual scenario of the agent learning without teacher supervision and only from rewards. If using teacher supervision improves 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#28151;&#21512;&#22270;&#30340;&#27010;&#24565;&#21450;&#20854;&#22312;&#39640;&#38454;&#22270;&#24314;&#27169;&#20013;&#30340;&#24212;&#29992;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#28151;&#21512;&#22270;&#25968;&#25454;&#38598;&#21450;&#20840;&#38754;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#36825;&#20026;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#22797;&#26434;&#22270;&#19978;&#30340;&#24615;&#33021;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2306.05108</link><description>&lt;p&gt;
&#28151;&#21512;&#22270;&#65306;&#19968;&#31181;&#29992;&#20110;&#22797;&#26434;&#22270;&#30340;&#32479;&#19968;&#22270;&#34920;&#31034;&#21450;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Hybrid Graph: A Unified Graph Representation with Datasets and Benchmarks for Complex Graphs. (arXiv:2306.05108v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05108
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#28151;&#21512;&#22270;&#30340;&#27010;&#24565;&#21450;&#20854;&#22312;&#39640;&#38454;&#22270;&#24314;&#27169;&#20013;&#30340;&#24212;&#29992;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#28151;&#21512;&#22270;&#25968;&#25454;&#38598;&#21450;&#20840;&#38754;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#36825;&#20026;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#22797;&#26434;&#22270;&#19978;&#30340;&#24615;&#33021;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#34987;&#24191;&#27867;&#29992;&#20110;&#23553;&#35013;&#21508;&#31181;&#25968;&#25454;&#26684;&#24335;&#65292;&#20294;&#26159;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#32593;&#32476;&#36890;&#24120;&#28041;&#21450;&#22797;&#26434;&#30340;&#33410;&#28857;&#20851;&#31995;&#65292;&#19981;&#20165;&#20165;&#26159;&#25104;&#23545;&#30340;&#20851;&#31995;&#12290;&#34429;&#28982;&#24050;&#32463;&#24320;&#21457;&#21644;&#20351;&#29992;&#20102;&#36229;&#22270;&#21644;&#20998;&#23618;&#22270;&#26469;&#35299;&#20915;&#22797;&#26434;&#30340;&#33410;&#28857;&#20851;&#31995;&#65292;&#20294;&#23427;&#20204;&#22312;&#23454;&#36341;&#20013;&#26080;&#27861;&#23436;&#20840;&#34920;&#31034;&#36825;&#20123;&#22797;&#26434;&#24615;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#29992;&#20110;&#26356;&#39640;&#38454;&#22270;&#30340;&#34920;&#31034;&#23398;&#20064;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#21482;&#22312;&#31616;&#21333;&#30340;&#22270;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#19968;&#20010;&#32479;&#19968;&#30340;&#26356;&#39640;&#38454;&#22270;&#24314;&#27169;&#26041;&#27861;&#65292;&#24182;&#19988;&#38656;&#35201;&#19968;&#32452;&#21253;&#21547;&#20840;&#38754;&#25968;&#25454;&#38598;&#30340;&#21487;&#35775;&#38382;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#20197;&#23436;&#20840;&#20102;&#35299;&#36825;&#20123;&#31639;&#27861;&#22312;&#22797;&#26434;&#22270;&#19978;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#28151;&#21512;&#22270;&#30340;&#27010;&#24565;&#65292;&#36825;&#26159;&#19968;&#20010;&#26356;&#39640;&#38454;&#22270;&#30340;&#32479;&#19968;&#23450;&#20041;&#65292;&#24182;&#25552;&#20986;&#20102;&#28151;&#21512;&#22270;&#36125;&#22855;&#39532;&#20811;&#65288;HGB&#65289;&#12290;HGB&#21253;&#21547;&#21508;&#20010;&#39046;&#22495;&#30340;23&#20010;&#30495;&#23454;&#28151;&#21512;&#22270;&#25968;&#25454;&#38598;&#65288;&#22914;&#29983;&#29289;&#23398;&#12289;&#31038;&#20132;&#23186;&#20307;&#21644;&#20132;&#36890;&#65289;&#65292;&#24182;&#20026;GNN&#22312;&#22797;&#26434;&#22270;&#19978;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graphs are widely used to encapsulate a variety of data formats, but real-world networks often involve complex node relations beyond only being pairwise. While hypergraphs and hierarchical graphs have been developed and employed to account for the complex node relations, they cannot fully represent these complexities in practice. Additionally, though many Graph Neural Networks (GNNs) have been proposed for representation learning on higher-order graphs, they are usually only evaluated on simple graph datasets. Therefore, there is a need for a unified modelling of higher-order graphs, and a collection of comprehensive datasets with an accessible evaluation framework to fully understand the performance of these algorithms on complex graphs. In this paper, we introduce the concept of hybrid graphs, a unified definition for higher-order graphs, and present the Hybrid Graph Benchmark (HGB). HGB contains 23 real-world hybrid graph datasets across various domains such as biology, social media
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#32508;&#36848;&#20102;&#21307;&#30103;&#30693;&#35782;&#22270;&#35889;(HKGs)&#30340;&#26500;&#24314;&#27969;&#31243;&#12289;&#20851;&#38190;&#25216;&#26415;&#21644;&#21033;&#29992;&#26041;&#27861;&#20197;&#21450;&#29616;&#26377;&#36164;&#28304;&#65292;&#24182;&#28145;&#20837;&#25506;&#35752;&#20102;HKG&#22312;&#21508;&#31181;&#21307;&#30103;&#39046;&#22495;&#30340;&#21464;&#38761;&#24615;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.04802</link><description>&lt;p&gt;
&#21307;&#30103;&#30693;&#35782;&#22270;&#35889;&#32508;&#36848;&#65306;&#36164;&#28304;&#12289;&#24212;&#29992;&#21644;&#21069;&#26223;
&lt;/p&gt;
&lt;p&gt;
A Survey on Knowledge Graphs for Healthcare: Resources, Applications, and Promises. (arXiv:2306.04802v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#32508;&#36848;&#20102;&#21307;&#30103;&#30693;&#35782;&#22270;&#35889;(HKGs)&#30340;&#26500;&#24314;&#27969;&#31243;&#12289;&#20851;&#38190;&#25216;&#26415;&#21644;&#21033;&#29992;&#26041;&#27861;&#20197;&#21450;&#29616;&#26377;&#36164;&#28304;&#65292;&#24182;&#28145;&#20837;&#25506;&#35752;&#20102;HKG&#22312;&#21508;&#31181;&#21307;&#30103;&#39046;&#22495;&#30340;&#21464;&#38761;&#24615;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#30103;&#30693;&#35782;&#22270;&#35889;(HKGs)&#24050;&#25104;&#20026;&#32452;&#32455;&#21307;&#23398;&#30693;&#35782;&#30340;&#26377;&#32467;&#26500;&#19988;&#21487;&#35299;&#37322;&#30340;&#26377;&#20026;&#24037;&#20855;&#65292;&#25552;&#20379;&#20102;&#21307;&#23398;&#27010;&#24565;&#21450;&#20854;&#20851;&#31995;&#30340;&#20840;&#38754;&#35270;&#22270;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#35206;&#30422;&#33539;&#22260;&#26377;&#38480;&#31561;&#25361;&#25112;&#20173;&#28982;&#23384;&#22312;&#65292;&#24378;&#35843;&#20102;&#22312;HKG&#39046;&#22495;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#24517;&#35201;&#24615;&#12290;&#26412;&#32508;&#36848;&#26159;HKG&#30340;&#31532;&#19968;&#20221;&#32508;&#21512;&#27010;&#36848;&#12290;&#25105;&#20204;&#24635;&#32467;&#20102;HKG&#26500;&#24314;&#30340;&#27969;&#31243;&#21644;&#20851;&#38190;&#25216;&#26415;&#65288;&#21363;&#20174;&#22836;&#24320;&#22987;&#21644;&#36890;&#36807;&#38598;&#25104;&#65289;&#65292;&#20197;&#21450;&#24120;&#35265;&#30340;&#21033;&#29992;&#26041;&#27861;&#65288;&#21363;&#22522;&#20110;&#27169;&#22411;&#21644;&#38750;&#22522;&#20110;&#27169;&#22411;&#65289;&#12290;&#20026;&#20102;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#65292;&#25105;&#20204;&#26681;&#25454;&#23427;&#20204;&#25429;&#33719;&#30340;&#25968;&#25454;&#31867;&#22411;&#21644;&#24212;&#29992;&#39046;&#22495;&#65288;&#35813;&#36164;&#28304;&#23384;&#20648;&#20110;https://github.com/lujiaying/Awesome-HealthCare-KnowledgeBase&#65289;&#32452;&#32455;&#20102;&#29616;&#26377;&#30340;HKG&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#20851;&#30340;&#32479;&#35745;&#20449;&#24687;&#12290;&#22312;&#24212;&#29992;&#37096;&#20998;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;HKG&#22312;&#21508;&#31181;&#21307;&#30103;&#39046;&#22495;&#30340;&#21464;&#38761;&#24615;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Healthcare knowledge graphs (HKGs) have emerged as a promising tool for organizing medical knowledge in a structured and interpretable way, which provides a comprehensive view of medical concepts and their relationships. However, challenges such as data heterogeneity and limited coverage remain, emphasizing the need for further research in the field of HKGs. This survey paper serves as the first comprehensive overview of HKGs. We summarize the pipeline and key techniques for HKG construction (i.e., from scratch and through integration), as well as the common utilization approaches (i.e., model-free and model-based). To provide researchers with valuable resources, we organize existing HKGs (The resource is available at https://github.com/lujiaying/Awesome-HealthCare-KnowledgeBase) based on the data types they capture and application domains, supplemented with pertinent statistical information. In the application section, we delve into the transformative impact of HKGs across various hea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#21478;&#19968;&#31181;&#24120;&#35265;&#12289;&#29616;&#23454;&#30340;&#22810;&#26234;&#33021;&#20307;RL&#25915;&#20987;&#35774;&#32622;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#25311;&#25915;&#20987;&#32773;&#23545;&#20195;&#29702;$\alpha$&#25511;&#21046;&#30340;&#26356;&#19968;&#33324;&#21270;&#25915;&#20987;&#24418;&#24335;&#12290;&#24182;&#35299;&#20915;&#20102;&#20808;&#21069;&#25915;&#20987;&#27169;&#22411;&#20013;&#32570;&#20047;&#21487;&#35777;&#26126;&#38450;&#24481;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.17342</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#23545;&#25239;&#31574;&#30053;&#65306;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24191;&#20041;&#25915;&#20987;&#24418;&#24335;&#21644;&#21487;&#35777;&#26126;&#30340;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Rethinking Adversarial Policies: A Generalized Attack Formulation and Provable Defense in Multi-Agent RL. (arXiv:2305.17342v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17342
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21478;&#19968;&#31181;&#24120;&#35265;&#12289;&#29616;&#23454;&#30340;&#22810;&#26234;&#33021;&#20307;RL&#25915;&#20987;&#35774;&#32622;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#25311;&#25915;&#20987;&#32773;&#23545;&#20195;&#29702;$\alpha$&#25511;&#21046;&#30340;&#26356;&#19968;&#33324;&#21270;&#25915;&#20987;&#24418;&#24335;&#12290;&#24182;&#35299;&#20915;&#20102;&#20808;&#21069;&#25915;&#20987;&#27169;&#22411;&#20013;&#32570;&#20047;&#21487;&#35777;&#26126;&#38450;&#24481;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#30740;&#31350;&#30740;&#31350;&#30452;&#25509;&#25200;&#21160;&#21463;&#23475;&#32773;&#30340;&#29366;&#24577;/&#21160;&#20316;&#25110;&#22522;&#30784;&#36716;&#31227;&#21160;&#24577;&#20197;&#23637;&#31034;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#22312;&#23545;&#25239;&#25915;&#20987;&#19979;&#30340;&#33030;&#24369;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#30452;&#25509;&#25805;&#32437;&#22312;&#23454;&#36341;&#20013;&#24182;&#19981;&#24635;&#26159;&#21487;&#34892;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#21478;&#19968;&#31181;&#24120;&#35265;&#19988;&#29616;&#23454;&#30340;&#25915;&#20987;&#35774;&#32622;&#65306;&#22312;&#32463;&#36807;&#35757;&#32451;&#30340;&#22810;&#26234;&#33021;&#20307;RL&#30340;&#35774;&#32622;&#20013;&#65292;&#22312;&#37096;&#32626;&#26399;&#38388;&#65292;&#21463;&#23475;&#20195;&#29702;$\nu$&#34987;&#25915;&#20987;&#32773;&#25511;&#21046;&#21478;&#19968;&#20010;&#20195;&#29702;$\alpha$&#20197;&#25932;&#23545;&#26041;&#24335;&#34892;&#21160;&#65292;&#20351;&#29992;&#8220;&#23545;&#25239;&#31574;&#30053;&#8221;&#23545;&#21463;&#23475;&#20195;&#29702;&#36827;&#34892;&#25915;&#20987;&#12290;&#23613;&#31649;&#20043;&#21069;&#30340;&#25915;&#20987;&#27169;&#22411;&#32771;&#34385;&#20102;&#36825;&#31181;&#35774;&#32622;&#65292;&#20294;&#20182;&#20204;&#27809;&#26377;&#32771;&#34385;&#21040;&#25915;&#20987;&#32773;&#21487;&#20197;&#36935;&#21040;&#25269;&#25239;&#65292;&#22240;&#27492;&#21482;&#33021;&#37096;&#20998;&#25511;&#21046;&#20195;&#29702;$\alpha$&#65292;&#21516;&#26102;&#24341;&#20837;&#21487;&#23519;&#35273;&#30340;&#8220;&#24322;&#24120;&#8221;&#34892;&#20026;&#65292;&#36825;&#20123;&#34892;&#20026;&#24456;&#23481;&#26131;&#34987;&#26816;&#27979;&#21040;&#12290;&#24182;&#19988;&#32570;&#20047;&#38024;&#23545;&#36825;&#20123;&#23545;&#25239;&#31574;&#30053;&#30340;&#21487;&#35777;&#26126;&#30340;&#38450;&#24481;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26356;&#19968;&#33324;&#21270;&#30340;&#25915;&#20987;&#24418;&#24335;&#65292;&#27169;&#25311;&#20102;&#25915;&#20987;&#32773;&#22312;&#20309;&#31181;&#31243;&#24230;&#19978;&#21487;&#20197;&#25511;&#21046;&#20195;&#29702;$\alpha$&#12290;
&lt;/p&gt;
&lt;p&gt;
Most existing works consider direct perturbations of victim's state/action or the underlying transition dynamics to show vulnerability of reinforcement learning agents under adversarial attacks. However, such direct manipulation may not always be feasible in practice. In this paper, we consider another common and realistic attack setup: in a multi-agent RL setting with well-trained agents, during deployment time, the victim agent $\nu$ is exploited by an attacker who controls another agent $\alpha$ to act adversarially against the victim using an \textit{adversarial policy}. Prior attack models under such setup do not consider that the attacker can confront resistance and thus can only take partial control of the agent $\alpha$, as well as introducing perceivable ``abnormal'' behaviors that are easily detectable. A provable defense against these adversarial policies is also lacking. To resolve these issues, we introduce a more general attack formulation that models to what extent the a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;k&#36817;&#37051;&#23398;&#20064;&#35268;&#21017;&#20013;&#30340;&#26222;&#36941;&#19968;&#33268;&#24615;&#65292;&#21457;&#29616;&#22312;&#21487;&#20998;&#24230;&#37327;&#31354;&#38388;&#20013;&#65292;&#35813;&#35268;&#21017;&#22312;Nagata&#32500;&#24230;&#19979;&#30340;sigma&#26377;&#38480;&#32500;&#24230;&#30340;&#31354;&#38388;&#20013;&#26159;&#26222;&#36941;&#19968;&#33268;&#30340;&#65292;&#22312;&#38750;&#38463;&#22522;&#31859;&#24503;&#24230;&#37327;&#31354;&#38388;&#20013;&#26159;&#24378;&#26222;&#36941;&#19968;&#33268;&#30340;&#65292;&#27492;&#35268;&#21017;&#22312;&#20855;&#26377;de Groot&#26377;&#38480;&#32500;&#24230;&#24847;&#20041;&#19979;&#30340;&#24230;&#37327;&#31354;&#38388;&#21644;Heisenberg&#32676;&#20013;&#20063;&#26159;&#26222;&#36941;&#19968;&#33268;&#30340;&#12290;</title><link>http://arxiv.org/abs/2305.17282</link><description>&lt;p&gt;
&#24230;&#37327;&#31354;&#38388;&#21644;Nagata&#32500;&#24230;&#20013;k-NN&#35268;&#21017;&#30340;&#26222;&#36941;&#19968;&#33268;&#24615;(II)
&lt;/p&gt;
&lt;p&gt;
Universal consistency of the $k$-NN rule in metric spaces and Nagata dimension. II. (arXiv:2305.17282v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17282
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;k&#36817;&#37051;&#23398;&#20064;&#35268;&#21017;&#20013;&#30340;&#26222;&#36941;&#19968;&#33268;&#24615;&#65292;&#21457;&#29616;&#22312;&#21487;&#20998;&#24230;&#37327;&#31354;&#38388;&#20013;&#65292;&#35813;&#35268;&#21017;&#22312;Nagata&#32500;&#24230;&#19979;&#30340;sigma&#26377;&#38480;&#32500;&#24230;&#30340;&#31354;&#38388;&#20013;&#26159;&#26222;&#36941;&#19968;&#33268;&#30340;&#65292;&#22312;&#38750;&#38463;&#22522;&#31859;&#24503;&#24230;&#37327;&#31354;&#38388;&#20013;&#26159;&#24378;&#26222;&#36941;&#19968;&#33268;&#30340;&#65292;&#27492;&#35268;&#21017;&#22312;&#20855;&#26377;de Groot&#26377;&#38480;&#32500;&#24230;&#24847;&#20041;&#19979;&#30340;&#24230;&#37327;&#31354;&#38388;&#21644;Heisenberg&#32676;&#20013;&#20063;&#26159;&#26222;&#36941;&#19968;&#33268;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32487;&#32493;&#22312;&#21487;&#20998;&#24230;&#37327;&#31354;&#38388;&#20013;&#30740;&#31350;k&#36817;&#37051;&#23398;&#20064;&#35268;&#21017;&#12290;&#30001;&#20110;C\'erou&#21644;Guyader(2006)&#20197;&#21450;Preiss(1983)&#30340;&#32467;&#26524;&#65292;&#24050;&#30693;&#35813;&#35268;&#21017;&#22312;&#27599;&#20010;Nagata&#24847;&#20041;&#19979;&#30340;sigma&#26377;&#38480;&#32500;&#24230;&#30340;&#24230;&#37327;&#31354;&#38388;X&#20013;&#26159;&#26222;&#36941;&#19968;&#33268;&#30340;&#12290;&#22312;&#27492;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#26080;&#24179;&#23616;&#24773;&#20917;&#19979;&#27492;&#35268;&#21017;&#22312;&#36825;&#26679;&#30340;&#31354;&#38388;&#20013;&#26159;&#24378;&#26222;&#36941;&#19968;&#33268;&#30340;&#12290;&#22312;Devroye&#65292;Gy\"{o}rfi&#65292;Krzy\.{z}ak&#21644;Lugosi&#65288;1994&#65289;&#22312;&#27431;&#20960;&#37324;&#24471;&#35774;&#32622;&#20013;&#24212;&#29992;&#30340;&#25171;&#30772;&#24179;&#23616;&#31574;&#30053;&#19979;&#65292;&#25105;&#20204;&#35774;&#27861;&#22312;&#38750;&#38463;&#22522;&#31859;&#24503;&#24230;&#37327;&#31354;&#38388;&#65288;&#21363;Nagata&#32500;&#24230;&#20026;&#38646;&#30340;&#31354;&#38388;&#65289;&#20013;&#23637;&#31034;&#20102;&#24378;&#26222;&#36941;&#19968;&#33268;&#24615;&#12290;&#32467;&#21512;C\'erou&#21644;Guyader&#30340;&#23450;&#29702;&#21644;Assouad&#21644;Quentin de Gromard (2006)&#30340;&#32467;&#26524;&#65292;&#21487;&#20197;&#25512;&#20986;$k$-NN&#35268;&#21017;&#22312;&#20855;&#26377;de Groot&#26377;&#38480;&#32500;&#24230;&#24847;&#20041;&#19979;&#30340;&#24230;&#37327;&#31354;&#38388;&#20013;&#26159;&#26222;&#36941;&#19968;&#33268;&#30340;&#12290;&#29305;&#21035;&#22320;&#65292;$k$-NN&#35268;&#21017;&#22312;Heisenberg&#32676;&#20013;&#26159;&#26222;&#36941;&#19968;&#33268;&#30340;&#65292;&#32780;&#35813;&#32676;&#24182;&#38750;sigma&#26377;&#38480;&#32500;&#24230;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We continue to investigate the $k$ nearest neighbour learning rule in separable metric spaces. Thanks to the results of C\'erou and Guyader (2006) and Preiss (1983), this rule is known to be universally consistent in every metric space $X$ that is sigma-finite dimensional in the sense of Nagata. Here we show that the rule is strongly universally consistent in such spaces in the absence of ties. Under the tie-breaking strategy applied by Devroye, Gy\"{o}rfi, Krzy\.{z}ak, and Lugosi (1994) in the Euclidean setting, we manage to show the strong universal consistency in non-Archimedian metric spaces (that is, those of Nagata dimension zero). Combining the theorem of C\'erou and Guyader with results of Assouad and Quentin de Gromard (2006), one deduces that the $k$-NN rule is universally consistent in metric spaces having finite dimension in the sense of de Groot. In particular, the $k$-NN rule is universally consistent in the Heisenberg group which is not sigma-finite dimensional in the se
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#23376;&#39640;&#26031;&#28151;&#21512;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#22810;&#23610;&#24230;&#32858;&#31867;&#21644;&#28304;&#20998;&#31163;&#65292;&#36890;&#36807;&#21033;&#29992;&#23567;&#27874;&#25955;&#23556;&#21327;&#26041;&#24046;&#26469;&#25552;&#20379;&#38543;&#26426;&#36807;&#31243;&#30340;&#20302;&#32500;&#34920;&#31034;&#65292;&#33021;&#22815;&#21306;&#20998;&#19981;&#21516;&#30340;&#38750;&#39640;&#26031;&#38543;&#26426;&#36807;&#31243;&#65292;&#24182;&#22312;MRO&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.16189</link><description>&lt;p&gt;
&#28779;&#26143;&#26102;&#38388;&#24207;&#21015;&#20998;&#35299;&#65306;&#19968;&#31181;&#22810;&#23610;&#24230;&#23884;&#22871;&#26041;&#27861;&#20013;&#30340;&#22240;&#23376;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Martian time-series unraveled: A multi-scale nested approach with factorial variational autoencoders. (arXiv:2305.16189v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16189
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#23376;&#39640;&#26031;&#28151;&#21512;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#22810;&#23610;&#24230;&#32858;&#31867;&#21644;&#28304;&#20998;&#31163;&#65292;&#36890;&#36807;&#21033;&#29992;&#23567;&#27874;&#25955;&#23556;&#21327;&#26041;&#24046;&#26469;&#25552;&#20379;&#38543;&#26426;&#36807;&#31243;&#30340;&#20302;&#32500;&#34920;&#31034;&#65292;&#33021;&#22815;&#21306;&#20998;&#19981;&#21516;&#30340;&#38750;&#39640;&#26031;&#38543;&#26426;&#36807;&#31243;&#65292;&#24182;&#22312;MRO&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#28304;&#20998;&#31163;&#28041;&#21450;&#36890;&#36807;&#28151;&#21512;&#25805;&#20316;&#35760;&#24405;&#30340;&#26410;&#30693;&#28304;&#20449;&#21495;&#30340;&#20998;&#35299;&#65292;&#20854;&#20013;&#23545;&#28304;&#30340;&#20808;&#39564;&#30693;&#35782;&#26377;&#38480;&#65292;&#20165;&#21487;&#20197;&#35775;&#38382;&#20449;&#21495;&#28151;&#21512;&#25968;&#25454;&#38598;&#12290;&#36825;&#20010;&#38382;&#39064;&#26412;&#36136;&#19978;&#26159;&#19981;&#36866;&#29992;&#30340;&#65292;&#24182;&#19988;&#36827;&#19968;&#27493;&#21463;&#21040;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#28304;&#23637;&#29616;&#20986;&#30340;&#22810;&#31181;&#26102;&#38388;&#23610;&#24230;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#22810;&#23610;&#24230;&#32858;&#31867;&#21644;&#28304;&#20998;&#31163;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#23567;&#27874;&#25955;&#23556;&#21327;&#26041;&#24046;&#26469;&#25552;&#20379;&#38543;&#26426;&#36807;&#31243;&#30340;&#20302;&#32500;&#34920;&#31034;&#65292;&#33021;&#22815;&#21306;&#20998;&#19981;&#21516;&#30340;&#38750;&#39640;&#26031;&#38543;&#26426;&#36807;&#31243;&#12290;&#22312;&#36825;&#20010;&#34920;&#31034;&#31354;&#38388;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22240;&#23376;&#39640;&#26031;&#28151;&#21512;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#23427;&#34987;&#35757;&#32451;&#29992;&#20110;(1)&#27010;&#29575;&#22320;&#23545;&#19981;&#21516;&#26102;&#38388;&#23610;&#24230;&#19978;&#30340;&#28304;&#36827;&#34892;&#32858;&#31867;&#21644;&#36880;&#23618;&#38750;&#30417;&#30563;&#28304;&#20998;&#31163;&#65292;(2)&#22312;&#27599;&#20010;&#26102;&#38388;&#23610;&#24230;&#19978;&#25552;&#21462;&#20302;&#32500;&#34920;&#31034;&#65292;(3)&#23398;&#20064;&#28304;&#20449;&#21495;&#30340;&#22240;&#23376;&#34920;&#31034;&#65292;(4)&#22312;&#34920;&#31034;&#31354;&#38388;&#20013;&#36827;&#34892;&#37319;&#26679;&#65292;&#20197;&#29983;&#25104;&#26410;&#30693;&#28304;&#20449;&#21495;&#12290;&#25105;&#20204;&#22312;MRO&#19978;&#30340;&#19977;&#20010;&#39057;&#36947;&#30340;&#21487;&#35265;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#27604;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised source separation involves unraveling an unknown set of source signals recorded through a mixing operator, with limited prior knowledge about the sources, and only access to a dataset of signal mixtures. This problem is inherently ill-posed and is further challenged by the variety of time-scales exhibited by sources in time series data. Existing methods typically rely on a preselected window size that limits their capacity to handle multi-scale sources. To address this issue, instead of operating in the time domain, we propose an unsupervised multi-scale clustering and source separation framework by leveraging wavelet scattering covariances that provide a low-dimensional representation of stochastic processes, capable of distinguishing between different non-Gaussian stochastic processes. Nested within this representation space, we develop a factorial Gaussian-mixture variational autoencoder that is trained to (1) probabilistically cluster sources at different time-scales a
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#21644;&#29983;&#25104;&#38463;&#25289;&#20271;&#25991;&#26412;&#26102;&#20986;&#29616;&#30340;&#25991;&#21270;&#20559;&#21521;&#35199;&#26041;&#25991;&#21270;&#30340;&#29616;&#35937;&#65292;&#34920;&#26126;&#35821;&#35328;&#27169;&#22411;&#22312;&#20154;&#21517;&#12289;&#39135;&#21697;&#12289;&#26381;&#35013;&#12289;&#22320;&#28857;&#12289;&#25991;&#23398;&#12289;&#39278;&#26009;&#12289;&#23447;&#25945;&#21644;&#20307;&#32946;&#31561;&#20843;&#20010;&#25991;&#21270;&#26041;&#38754;&#23384;&#22312;&#20559;&#35265;&#12290;&#36825;&#20123;&#21457;&#29616;&#24341;&#21457;&#23545;&#20110;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#25991;&#21270;&#30456;&#20851;&#24615;&#30340;&#25285;&#24551;&#12290;</title><link>http://arxiv.org/abs/2305.14456</link><description>&lt;p&gt;
&#22312;&#31048;&#31095;&#20043;&#21518;&#21917;&#21860;&#37202;&#65311;&#27979;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25991;&#21270;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Having Beer after Prayer? Measuring Cultural Bias in Large Language Models. (arXiv:2305.14456v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14456
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#21644;&#29983;&#25104;&#38463;&#25289;&#20271;&#25991;&#26412;&#26102;&#20986;&#29616;&#30340;&#25991;&#21270;&#20559;&#21521;&#35199;&#26041;&#25991;&#21270;&#30340;&#29616;&#35937;&#65292;&#34920;&#26126;&#35821;&#35328;&#27169;&#22411;&#22312;&#20154;&#21517;&#12289;&#39135;&#21697;&#12289;&#26381;&#35013;&#12289;&#22320;&#28857;&#12289;&#25991;&#23398;&#12289;&#39278;&#26009;&#12289;&#23447;&#25945;&#21644;&#20307;&#32946;&#31561;&#20843;&#20010;&#25991;&#21270;&#26041;&#38754;&#23384;&#22312;&#20559;&#35265;&#12290;&#36825;&#20123;&#21457;&#29616;&#24341;&#21457;&#23545;&#20110;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#25991;&#21270;&#30456;&#20851;&#24615;&#30340;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#23384;&#22312;&#25991;&#21270;&#20559;&#35265;&#65311;&#35821;&#35328;&#27169;&#22411;&#31526;&#21512;&#25152;&#26381;&#21153;&#31038;&#21306;&#30340;&#25991;&#21270;&#22240;&#32032;&#24456;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#26412;&#25991;&#34920;&#26126;&#22312;&#22788;&#29702;&#21644;&#29983;&#25104;&#38463;&#25289;&#20271;&#25991;&#26412;&#26102;&#65292;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#26174;&#33879;&#30340;&#20559;&#21521;&#35199;&#26041;&#25991;&#21270;&#30340;&#20559;&#35265;&#65292;&#20542;&#21521;&#20110;&#20135;&#29983;&#35199;&#26041;&#25991;&#21270;&#30456;&#20851;&#20869;&#23481;&#32780;&#38750;&#38463;&#25289;&#20271;&#25991;&#21270;&#30456;&#20851;&#20869;&#23481;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#20174;&#22312;&#32447;&#31038;&#20132;&#23186;&#20307;&#19978;&#25910;&#38598;&#30340;&#33258;&#28982;&#20986;&#29616;&#30340;&#19978;&#19979;&#25991;&#21644;&#22522;&#20110;&#21487;&#33021;&#24615;&#35780;&#20998;&#30340;&#25351;&#26631;&#26469;&#37327;&#21270;&#36825;&#31181;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#26174;&#31034;&#65292;&#38463;&#25289;&#20271;&#35821;&#21333;&#35821;&#21644;&#22810;&#35821;&#27169;&#22411;&#22312;&#20843;&#20010;&#19981;&#21516;&#30340;&#25991;&#21270;&#26041;&#38754;&#23384;&#22312;&#35199;&#26041;&#25991;&#21270;&#20559;&#35265;&#65292;&#21253;&#25324;&#20154;&#21517;&#12289;&#39135;&#21697;&#12289;&#26381;&#35013;&#12289;&#22320;&#28857;&#12289;&#25991;&#23398;&#12289;&#39278;&#26009;&#12289;&#23447;&#25945;&#21644;&#20307;&#32946;&#12290;&#24403;&#36755;&#20837;&#30340;&#38463;&#25289;&#20271;&#35821;&#21477;&#23376;&#36234;&#25509;&#36817;&#33521;&#35821;&#26102;&#65292;&#27169;&#22411;&#20063;&#26356;&#23481;&#26131;&#34920;&#29616;&#20986;&#20559;&#35265;&#12290;&#36825;&#20123;&#21457;&#29616;&#24341;&#21457;&#20154;&#20204;&#23545;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#25991;&#21270;&#30456;&#20851;&#24615;&#30340;&#25285;&#24551;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;&#27169;&#22411;&#35774;&#35745;&#20013;&#24212;&#26356;&#22810;&#32771;&#34385;&#25991;&#21270;&#22240;&#32032;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Are language models culturally biased? It is important that language models conform to the cultural aspects of the communities they serve. However, we show in this paper that language models suffer from a significant bias towards Western culture when handling and generating text in Arabic, often preferring, and producing Western-fitting content as opposed to the relevant Arab content. We quantify this bias through a likelihood scoring-based metric using naturally occurring contexts that we collect from online social media. Our experiments reveal that both Arabic monolingual and multilingual models exhibit bias towards Western culture in eight different cultural aspects: person names, food, clothing, location, literature, beverage, religion, and sports. Models also tend to exhibit more bias when prompted with Arabic sentences that are more linguistically aligned with English. These findings raise concerns about the cultural relevance of current language models. Our analyses show that pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;Riesz&#26680;&#23637;&#31034;&#20102;&#29983;&#25104;&#24335;&#20998;&#21106;MMD&#27969;&#30340;&#39640;&#25928;&#35745;&#31639;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#22823;&#35268;&#27169;&#24212;&#29992;&#20013;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.11463</link><description>&lt;p&gt;
&#21033;&#29992;Riesz&#26680;&#30340;&#29983;&#25104;&#24335;&#20998;&#21106;MMD&#27969;
&lt;/p&gt;
&lt;p&gt;
Generative Sliced MMD Flows with Riesz Kernels. (arXiv:2305.11463v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11463
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;Riesz&#26680;&#23637;&#31034;&#20102;&#29983;&#25104;&#24335;&#20998;&#21106;MMD&#27969;&#30340;&#39640;&#25928;&#35745;&#31639;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#22823;&#35268;&#27169;&#24212;&#29992;&#20013;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#35745;&#31639;&#20013;&#65292;&#26368;&#22823;&#24179;&#22343;&#24046;&#24322;&#24230;(MMD)&#27969;&#30340;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;Riesz&#26680;$K(x,y)=-\|x-y\|^r$&#65292;$r \in (0,2)$&#30340;MMD&#27969;&#20855;&#26377;&#26480;&#20986;&#30340;&#24615;&#36136;&#65292;&#21487;&#20801;&#35768;&#20854;&#36827;&#34892;&#39640;&#25928;&#35745;&#31639;&#12290;&#39318;&#20808;&#65292;Riesz&#26680;&#30340;MMD&#19982;&#20854;&#20998;&#21106;&#29256;&#26412;&#30340;MMD&#37325;&#21512;&#12290;&#22240;&#27492;&#65292;&#21487;&#20197;&#22312;&#19968;&#32500;&#35774;&#32622;&#20013;&#36827;&#34892;MMD&#26799;&#24230;&#30340;&#35745;&#31639;&#12290;&#22312;&#27492;&#22788;&#65292;&#23545;&#20110;$r=1$&#65292;&#21487;&#20197;&#24212;&#29992;&#31616;&#21333;&#30340;&#25490;&#24207;&#31639;&#27861;&#23558;&#20004;&#20010;&#32463;&#39564;&#24230;&#37327;&#30340;&#22797;&#26434;&#24230;&#20174;$O(MN+N^2)$&#38477;&#20302;&#21040;$O((M+N)\log(M+N))$&#65292;&#20854;&#20013;$M$&#21644;$N$&#26159;&#25903;&#25345;&#28857;&#12290;&#23545;&#20110;&#23454;&#29616;&#65292;&#25105;&#20204;&#36890;&#36807;&#20165;&#20351;&#29992;&#26377;&#38480;&#25968;&#37327;&#30340;$P$&#20010;&#20999;&#29255;&#26469;&#36817;&#20284;&#20998;&#21106;MMD&#30340;&#26799;&#24230;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#30001;&#27492;&#20135;&#29983;&#30340;&#35823;&#24046;&#20855;&#26377;$O(\sqrt{d/P})$&#30340;&#22797;&#26434;&#24230;&#65292;&#20854;&#20013;$d$&#26159;&#25968;&#25454;&#32500;&#24230;&#12290;&#36825;&#20123;&#32467;&#26524;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;MMD&#26799;&#24230;&#27969;&#26469;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#65292;&#29978;&#33267;&#29992;&#20110;&#22823;&#35268;&#27169;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Maximum mean discrepancy (MMD) flows suffer from high computational costs in large scale computations. In this paper, we show that MMD flows with Riesz kernels $K(x,y) = - \|x-y\|^r$, $r \in (0,2)$ have exceptional properties which allow for their efficient computation. First, the MMD of Riesz kernels coincides with the MMD of their sliced version. As a consequence, the computation of gradients of MMDs can be performed in the one-dimensional setting. Here, for $r=1$, a simple sorting algorithm can be applied to reduce the complexity from $O(MN+N^2)$ to $O((M+N)\log(M+N))$ for two empirical measures with $M$ and $N$ support points. For the implementations we approximate the gradient of the sliced MMD by using only a finite number $P$ of slices. We show that the resulting error has complexity $O(\sqrt{d/P})$, where $d$ is the data dimension. These results enable us to train generative models by approximating MMD gradient flows by neural networks even for large scale applications. We demo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#20840;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;+&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#36328;&#22330;&#26223;&#25163;&#21183;&#35782;&#21035;&#20219;&#21153;&#20013;&#26377;&#25928;&#35299;&#20915;&#25968;&#25454;&#21464;&#24322;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.08014</link><description>&lt;p&gt;
&#22522;&#20110;&#34920;&#38754;&#32908;&#30005;&#22270;&#20687;&#30340;&#36731;&#37327;&#32423;&#20840;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#36328;&#22330;&#26223;&#25163;&#21183;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Surface EMG-Based Inter-Session/Inter-Subject Gesture Recognition by Leveraging Lightweight All-ConvNet and Transfer Learning. (arXiv:2305.08014v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#20840;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;+&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#36328;&#22330;&#26223;&#25163;&#21183;&#35782;&#21035;&#20219;&#21153;&#20013;&#26377;&#25928;&#35299;&#20915;&#25968;&#25454;&#21464;&#24322;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#20302;&#20998;&#36776;&#29575;&#30636;&#26102;&#39640;&#28165;&#32908;&#30005;&#22270;&#20687;&#36827;&#34892;&#25163;&#21183;&#35782;&#21035;&#21487;&#20197;&#24320;&#36767;&#21457;&#23637;&#26356;&#27969;&#30021;&#12289;&#26356;&#33258;&#28982;&#30340;&#32908;&#32905;-&#35745;&#31639;&#26426;&#30028;&#38754;&#30340;&#26032;&#36884;&#24452;&#12290;&#28982;&#32780;&#65292;&#36328;&#22330;&#26223;&#25968;&#25454;&#30340;&#21464;&#24322;&#24615;&#23384;&#22312;&#26497;&#22823;&#30340;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#37319;&#29992;&#38750;&#24120;&#22823;&#19988;&#22797;&#26434;&#30340;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#25110;&#22522;&#20110;2SRNN&#30340;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#65292;&#26469;&#36924;&#36817;&#30001;&#36825;&#20123;&#36328;&#22330;&#26223;&#25968;&#25454;&#21464;&#24322;&#24615;&#24341;&#36215;&#30340;&#20998;&#24067;&#20559;&#31227;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#26041;&#27861;&#20063;&#38656;&#35201;&#22312;&#39044;&#35757;&#32451;&#21644;&#36866;&#24212;&#38454;&#27573;&#20013;&#22312;&#25968;&#30334;&#19975;&#20010;&#35757;&#32451;&#21442;&#25968;&#21644;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23398;&#20064;&#12290;&#32467;&#26524;&#65292;&#36825;&#20351;&#24471;&#22312;&#23454;&#26102;&#24212;&#29992;&#20013;&#36827;&#34892;&#39640;&#31471;&#36164;&#28304;&#32422;&#26463;&#21644;&#35745;&#31639;&#38750;&#24120;&#26114;&#36149;&#30340;&#37096;&#32626;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#20840;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;+&#36801;&#31227;&#23398;&#20064;&#27169;&#22411;&#65292;&#21033;&#29992;&#36731;&#37327;&#32423;&#20840;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#36801;&#31227;&#23398;&#20064;(TL)&#26469;&#22686;&#24378;&#36328;&#22330;&#26223;&#25163;&#21183;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gesture recognition using low-resolution instantaneous HD-sEMG images opens up new avenues for the development of more fluid and natural muscle-computer interfaces. However, the data variability between inter-session and inter-subject scenarios presents a great challenge. The existing approaches employed very large and complex deep ConvNet or 2SRNN-based domain adaptation methods to approximate the distribution shift caused by these inter-session and inter-subject data variability. Hence, these methods also require learning over millions of training parameters and a large pre-trained and target domain dataset in both the pre-training and adaptation stages. As a result, it makes high-end resource-bounded and computationally very expensive for deployment in real-time applications. To overcome this problem, we propose a lightweight All-ConvNet+TL model that leverages lightweight All-ConvNet and transfer learning (TL) for the enhancement of inter-session and inter-subject gesture recogniti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#21160;&#24577;&#33258;&#32534;&#30721;&#22120;&#65288;MDVAE&#65289;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#38899;&#35270;&#39057;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#22312;&#20013;&#38388;&#34920;&#31034;&#19978;&#36827;&#34892;&#20102;&#38745;&#24577;&#19982;&#21160;&#24577;&#20449;&#24687;&#12289;&#27169;&#24577;&#29305;&#24322;&#19982;&#20849;&#21516;&#20449;&#24687;&#30340;&#20998;&#31163;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.03582</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#38899;&#35270;&#39057;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#30340;&#22810;&#27169;&#24577;&#21160;&#24577;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Multimodal Dynamical Variational Autoencoder for Audiovisual Speech Representation Learning. (arXiv:2305.03582v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#21160;&#24577;&#33258;&#32534;&#30721;&#22120;&#65288;MDVAE&#65289;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#38899;&#35270;&#39057;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#22312;&#20013;&#38388;&#34920;&#31034;&#19978;&#36827;&#34892;&#20102;&#38745;&#24577;&#19982;&#21160;&#24577;&#20449;&#24687;&#12289;&#27169;&#24577;&#29305;&#24322;&#19982;&#20849;&#21516;&#20449;&#24687;&#30340;&#20998;&#31163;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#21160;&#24577;&#33258;&#32534;&#30721;&#22120;&#65288;MDVAE&#65289;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#38899;&#35270;&#39057;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#12290;&#28508;&#22312;&#31354;&#38388;&#34987;&#26500;&#36896;&#20026;&#23558;&#22312;&#21508;&#20010;&#27169;&#24577;&#20043;&#38388;&#20849;&#20139;&#30340;&#28508;&#22312;&#21160;&#24577;&#22240;&#32032;&#19982;&#27599;&#20010;&#27169;&#24577;&#29305;&#23450;&#30340;&#22240;&#32032;&#21306;&#20998;&#24320;&#26469;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#19968;&#20010;&#38745;&#24577;&#28508;&#21464;&#37327;&#26469;&#32534;&#30721;&#38899;&#35270;&#39057;&#35821;&#38899;&#24207;&#21015;&#20013;&#38543;&#26102;&#38388;&#24658;&#23450;&#30340;&#20449;&#24687;&#12290;&#27169;&#22411;&#22312;&#19968;&#20010;&#38899;&#35270;&#39057;&#24773;&#24863;&#35821;&#38899;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#26080;&#30417;&#30563;&#35757;&#32451;&#20998;&#20004;&#20010;&#38454;&#27573;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#23545;&#20110;&#27599;&#20010;&#27169;&#24577;&#65292;&#39318;&#20808;&#29420;&#31435;&#23398;&#20064;&#19968;&#20010;&#21521;&#37327;&#37327;&#21270;&#33258;&#32534;&#30721;&#22120;&#65288;VQ-VAE&#65289;&#65292;&#32780;&#27809;&#26377;&#26102;&#38388;&#24314;&#27169;&#12290;&#31532;&#20108;&#38454;&#27573;&#21017;&#22312;&#21521;&#37327;&#37327;&#21270;&#33258;&#32534;&#30721;&#22120;&#65288;VQ-VAEs&#65289;&#30340;&#20013;&#38388;&#34920;&#31034;&#19978;&#23398;&#20064;MDVAE&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#26041;&#38754;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a multimodal \textit{and} dynamical VAE (MDVAE) applied to unsupervised audio-visual speech representation learning. The latent space is structured to dissociate the latent dynamical factors that are shared between the modalities from those that are specific to each modality. A static latent variable is also introduced to encode the information that is constant over time within an audiovisual speech sequence. The model is trained in an unsupervised manner on an audiovisual emotional speech dataset, in two stages. In the first stage, a vector quantized VAE (VQ-VAE) is learned independently for each modality, without temporal modeling. The second stage consists in learning the MDVAE model on the intermediate representation of the VQ-VAEs before quantization. The disentanglement between static versus dynamical and modality-specific versus modality-common information occurs during this second training stage. Extensive experiments are conducted to investigate how a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30340;&#21019;&#26032;&#22312;&#20110;&#23558;&#26368;&#22823;&#29109;&#26102;&#21051;&#27861;&#31283;&#23450;&#24212;&#29992;&#20110;&#21333;&#31934;&#24230;&#19979;&#30340;&#31232;&#30095;&#27668;&#20307;&#21160;&#21147;&#23398;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#29616;&#20195;GPU&#19978;&#27169;&#25311;&#38750;&#24120;&#24378;&#30340;&#27491;&#24120;&#28608;&#27874;&#12290;</title><link>http://arxiv.org/abs/2303.02898</link><description>&lt;p&gt;
&#23558;&#26368;&#22823;&#29109;&#26102;&#21051;&#27861;&#31283;&#23450;&#24212;&#29992;&#20110;&#21333;&#31934;&#24230;&#19979;&#30340;&#31232;&#30095;&#27668;&#20307;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Stabilizing the Maximal Entropy Moment Method for Rarefied Gas Dynamics at Single-Precision. (arXiv:2303.02898v2 [physics.flu-dyn] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02898
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30340;&#21019;&#26032;&#22312;&#20110;&#23558;&#26368;&#22823;&#29109;&#26102;&#21051;&#27861;&#31283;&#23450;&#24212;&#29992;&#20110;&#21333;&#31934;&#24230;&#19979;&#30340;&#31232;&#30095;&#27668;&#20307;&#21160;&#21147;&#23398;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#29616;&#20195;GPU&#19978;&#27169;&#25311;&#38750;&#24120;&#24378;&#30340;&#27491;&#24120;&#28608;&#27874;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#23637;&#36866;&#29992;&#20110;&#23494;&#38598;&#21644;&#31232;&#30095;&#27668;&#20307;&#30340;&#25193;&#23637;&#27969;&#20307;&#21147;&#23398;&#26041;&#31243;&#20173;&#28982;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#23545;&#20110;&#36825;&#20010;&#25361;&#25112;&#30340;&#19968;&#20010;&#31995;&#32479;&#35299;&#20915;&#26041;&#26696;&#26159;&#21033;&#29992;&#27668;&#20307;&#20998;&#23376;&#36895;&#24230;&#20998;&#24067;&#30340;&#30697;&#27861;&#25551;&#36848;&#23494;&#38598;&#21644;&#31232;&#30095;&#27668;&#20307;&#34892;&#20026;&#12290;&#22312;&#20247;&#22810;&#30340;&#30697;&#27861;&#20013;&#65292;&#26368;&#22823;&#29109;&#26102;&#21051;&#27861;&#65288;MEM&#65289;&#22240;&#20854;&#20855;&#26377;&#33391;&#22909;&#30340;&#35299;&#20915;&#24615;&#21644;&#31283;&#23450;&#24615;&#32780;&#31361;&#20986;&#65292;&#23427;&#21033;&#29992;&#20102;&#29109;&#26368;&#22823;&#21270;&#30340;&#36895;&#24230;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#23547;&#25214;&#36825;&#26679;&#30340;&#20998;&#24067;&#38656;&#35201;&#35299;&#20915;&#19968;&#20010;&#26465;&#20214;&#30149;&#24577;&#19988;&#35745;&#31639;&#38656;&#27714;&#36739;&#22823;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#25968;&#20540;&#31934;&#24230;&#19981;&#36275;&#26102;&#20250;&#23548;&#33268;&#25968;&#20540;&#28322;&#20986;&#21644;&#23849;&#28291;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#39640;&#36895;&#28608;&#27874;&#31561;&#27969;&#21160;&#29616;&#35937;&#12290;&#23427;&#36824;&#38459;&#30861;&#20102;&#29616;&#20195;GPU&#21033;&#29992;&#20854;&#24040;&#22823;&#30340;&#21333;&#31934;&#24230;&#35745;&#31639;&#33021;&#21147;&#26469;&#21152;&#36895;&#20248;&#21270;&#12290;&#26412;&#25991;&#26088;&#22312;&#31283;&#23450;MEM&#65292;&#20351;&#20854;&#22312;&#29616;&#20195;GPU&#19978;&#20197;&#21333;&#31934;&#24230;&#23454;&#29992;&#20110;&#27169;&#25311;&#38750;&#24120;&#24378;&#30340;&#27491;&#24120;&#28608;&#27874;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing extended hydrodynamics equations valid for both dense and rarefied gases remains a great challenge. A systematical solution for this challenge is the moment method describing both dense and rarefied gas behaviors with moments of gas molecule velocity distributions. Among moment methods, the maximal entropy moment method (MEM) stands out for its well-posedness and stability, which utilizes velocity distributions with maximized entropy. However, finding such distributions requires solving an ill-conditioned and computation-demanding optimization problem. This problem causes numerical overflow and breakdown when the numerical precision is insufficient, especially for flows like high-speed shock waves. It also prevents modern GPUs from accelerating optimization with their enormous single floating-point precision computation power. This paper aims to stabilize MEM, making it practical for simulating very strong normal shock waves on modern GPUs at single precision. We propose the
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#23618;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#31639;&#27861;&#65292;&#20351;&#29992;&#30340;&#26799;&#24230;&#35745;&#31639;&#27425;&#25968; $O((n+m)^{\frac{1}{2}}\varepsilon^{-1})$&#65292;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#26041;&#38754;&#26159;&#26368;&#20248;&#30340;&#12290;</title><link>http://arxiv.org/abs/2302.08766</link><description>&lt;p&gt;
&#19968;&#31181;&#21452;&#23618;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#31639;&#27861;&#30340;&#19979;&#30028;&#21644;&#36817;&#20284;&#26368;&#20248;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Lower Bound and a Near-Optimal Algorithm for Bilevel Empirical Risk Minimization. (arXiv:2302.08766v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08766
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#23618;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#31639;&#27861;&#65292;&#20351;&#29992;&#30340;&#26799;&#24230;&#35745;&#31639;&#27425;&#25968; $O((n+m)^{\frac{1}{2}}\varepsilon^{-1})$&#65292;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#26041;&#38754;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21452;&#23618;&#26368;&#20248;&#21270;&#38382;&#39064;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20013;&#12290;&#22312;&#35768;&#22810;&#23454;&#38469;&#24773;&#20917;&#19979;&#65292;&#19978;&#23618;&#21644;&#19979;&#23618;&#30446;&#26631;&#23545;&#24212;&#20110;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#24182;&#22240;&#27492;&#20855;&#26377;&#24635;&#21644;&#32467;&#26500;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33879;&#21517;&#30340;SARAH&#31639;&#27861;&#30340;&#21452;&#23618;&#25193;&#23637;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#38656;&#35201;$\mathcal {O}((n+m)^{\frac{1}{2}}\varepsilon ^{-1})$&#27425;&#26799;&#24230;&#35745;&#31639;&#25165;&#33021;&#23454;&#29616;$\varepsilon$&#31283;&#23450;&#24615;&#65292;&#20854;&#20013;$n+m$&#26159;&#26679;&#26412;&#24635;&#25968;&#65292;&#36825;&#27604;&#20808;&#21069;&#25152;&#26377;&#30340;&#21452;&#23618;&#31639;&#27861;&#37117;&#35201;&#22909;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#19979;&#30028;&#65292;&#29992;&#20110;&#24471;&#21040;&#21452;&#23618;&#38382;&#39064;&#30340;&#30446;&#26631;&#20989;&#25968;&#30340;&#36817;&#20284;&#31283;&#23450;&#28857;&#25152;&#38656;&#30340;oracle&#35843;&#29992;&#27425;&#25968;&#12290;&#36825;&#20010;&#19979;&#30028;&#27491;&#26159;&#25105;&#20204;&#30340;&#31639;&#27861;&#25152;&#36798;&#21040;&#30340;&#65292;&#22240;&#27492;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#26041;&#38754;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bilevel optimization problems, which are problems where two optimization problems are nested, have more and more applications in machine learning. In many practical cases, the upper and the lower objectives correspond to empirical risk minimization problems and therefore have a sum structure. In this context, we propose a bilevel extension of the celebrated SARAH algorithm. We demonstrate that the algorithm requires $\mathcal{O}((n+m)^{\frac12}\varepsilon^{-1})$ gradient computations to achieve $\varepsilon$-stationarity with $n+m$ the total number of samples, which improves over all previous bilevel algorithms. Moreover, we provide a lower bound on the number of oracle calls required to get an approximate stationary point of the objective function of the bilevel problem. This lower bound is attained by our algorithm, which is therefore optimal in terms of sample complexity.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36870;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;$\mathcal{O}$-ICID&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26159;&#36890;&#36807;&#36830;&#32493;&#20248;&#21270;&#19968;&#31181;&#30697;&#38453;&#20998;&#35299;&#26469;&#23398;&#20064;&#22240;&#26524;&#32467;&#26500;&#30340;&#65292;&#36866;&#29992;&#20110;&#21464;&#37327;&#25968;&#37327;&#24222;&#22823;&#30340;&#24773;&#20917;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#22122;&#22768;&#26041;&#24046;&#24050;&#30693;&#26102;&#35782;&#21035;&#30495;&#23454;DAG, &#20063;&#21487;&#20197;&#22312;&#36739;&#24369;&#30340;&#20808;&#39564;&#20449;&#24687;&#19979;&#32473;&#20986;&#26377;&#29992;&#30340;&#23450;&#21521;&#22270;&#35299;</title><link>http://arxiv.org/abs/2211.14221</link><description>&lt;p&gt;
&#36890;&#36807;&#30697;&#38453;&#20998;&#35299;&#20174;&#36870;&#21327;&#26041;&#24046;&#30697;&#38453;&#20013;&#23398;&#20064;&#22823;&#22411;&#22240;&#26524;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Learning Large Causal Structures from Inverse Covariance Matrix via Matrix Decomposition. (arXiv:2211.14221v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14221
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36870;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;$\mathcal{O}$-ICID&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26159;&#36890;&#36807;&#36830;&#32493;&#20248;&#21270;&#19968;&#31181;&#30697;&#38453;&#20998;&#35299;&#26469;&#23398;&#20064;&#22240;&#26524;&#32467;&#26500;&#30340;&#65292;&#36866;&#29992;&#20110;&#21464;&#37327;&#25968;&#37327;&#24222;&#22823;&#30340;&#24773;&#20917;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#22122;&#22768;&#26041;&#24046;&#24050;&#30693;&#26102;&#35782;&#21035;&#30495;&#23454;DAG, &#20063;&#21487;&#20197;&#22312;&#36739;&#24369;&#30340;&#20808;&#39564;&#20449;&#24687;&#19979;&#32473;&#20986;&#26377;&#29992;&#30340;&#23450;&#21521;&#22270;&#35299;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21464;&#37327;&#25968;&#37327;&#24222;&#22823;&#26102;&#65292;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#23398;&#20064;&#22240;&#26524;&#32467;&#26500;&#26159;&#19968;&#20010;&#22522;&#26412;&#20294;&#39640;&#24230;&#22797;&#26434;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#20174;&#32447;&#24615;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;(SEMs)&#20986;&#21457;&#65292;&#30740;&#31350;&#20102;&#20174;&#36870;&#21327;&#26041;&#24046;&#30697;&#38453;&#20013;&#23398;&#20064;&#22240;&#26524;&#32467;&#26500;&#30340;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#31216;&#20026;$\mathcal{O}$-ICID(&#26469;&#33258;Oracle &#36870;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#29420;&#31435;&#20445;&#25345;&#20998;&#35299;)&#65292;&#22522;&#20110;&#19968;&#31181;&#30697;&#38453;&#20998;&#35299;&#30340;&#36830;&#32493;&#20248;&#21270;&#65292;&#35813;&#20998;&#35299;&#20445;&#30041;&#20102;&#36870;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#38750;&#38646;&#27169;&#24335;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#22122;&#22768;&#26041;&#24046;&#24050;&#30693;&#30340;&#24773;&#20917;&#19979;&#65292;$\mathcal{O}$-ICID&#20026;&#35782;&#21035;&#30495;&#23454;&#26377;&#21521;&#26080;&#29615;&#22270;(DAG)&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#24335;&#12290;&#22312;&#36739;&#24369;&#30340;&#20808;&#39564;&#20449;&#24687;&#19979;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#32473;&#20986;&#26377;&#29992;&#30340;&#23450;&#21521;&#22270;&#35299;&#65292;&#29992;&#20110;&#36827;&#34892;&#26356;&#31934;&#32454;&#30340;&#22240;&#26524;&#21457;&#29616;&#12290;&#24403;&#30495;&#23454;DAG&#20855;&#26377;&#26377;&#38480;&#30340;&#33410;&#28857;&#24230;&#25968;&#26102;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20855;&#26377;&#20302;&#22797;&#26434;&#24230;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#26102;&#38388;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning causal structures from observational data is a fundamental yet highly complex problem when the number of variables is large. In this paper, we start from linear structural equation models (SEMs) and investigate ways of learning causal structures from the inverse covariance matrix. The proposed method, called $\mathcal{O}$-ICID (for {\it Independence-preserving} Decomposition from Oracle Inverse Covariance matrix), is based on continuous optimization of a type of matrix decomposition that preserves the nonzero patterns of the inverse covariance matrix. We show that $\mathcal{O}$-ICID provides an efficient way for identifying the true directed acyclic graph (DAG) under the knowledge of noise variances. With weaker prior information, the proposed method gives directed graph solutions that are useful for making more refined causal discovery. The proposed method enjoys a low complexity when the true DAG has bounded node degrees, as reflected by its time efficiency in experiments in
&lt;/p&gt;</description></item><item><title>Survival Kernets &#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#19988;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#26680;&#29983;&#23384;&#20998;&#26512;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#27169;&#22411;&#35299;&#37322;&#21644;&#29702;&#35770;&#20998;&#26512;&#12290;&#23427;&#21033;&#29992;&#26680;&#20989;&#25968;&#20272;&#35745;&#20010;&#20307;&#30340;&#29983;&#23384;&#20998;&#24067;&#65292;&#36890;&#36807;&#35757;&#32451;&#38598;&#21387;&#32553;&#26041;&#26696;&#36827;&#34892;&#25968;&#25454;&#20998;&#31751;&#65292;&#22240;&#27492;&#20855;&#26377;&#36739;&#39640;&#30340;&#21487;&#35270;&#21270;&#33021;&#21147;&#21644;&#39044;&#27979;&#20934;&#30830;&#24615;&#20445;&#35777;&#12290;&#35813;&#27169;&#22411;&#22312;&#29305;&#23450;&#24773;&#20917;&#19979;&#30340;&#39044;&#27979;&#29983;&#23384;&#20998;&#24067;&#35823;&#24046;&#30028;&#38480;&#26368;&#20248;&#65292;&#19988;&#22312;&#27979;&#35797;&#26102;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2206.10477</link><description>&lt;p&gt;
Survival Kernets: &#21487;&#25193;&#23637;&#19988;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#26680;&#29983;&#23384;&#20998;&#26512;&#27169;&#22411;&#65292;&#24182;&#20855;&#26377;&#20934;&#30830;&#24615;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Survival Kernets: Scalable and Interpretable Deep Kernel Survival Analysis with an Accuracy Guarantee. (arXiv:2206.10477v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.10477
&lt;/p&gt;
&lt;p&gt;
Survival Kernets &#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#19988;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#26680;&#29983;&#23384;&#20998;&#26512;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#27169;&#22411;&#35299;&#37322;&#21644;&#29702;&#35770;&#20998;&#26512;&#12290;&#23427;&#21033;&#29992;&#26680;&#20989;&#25968;&#20272;&#35745;&#20010;&#20307;&#30340;&#29983;&#23384;&#20998;&#24067;&#65292;&#36890;&#36807;&#35757;&#32451;&#38598;&#21387;&#32553;&#26041;&#26696;&#36827;&#34892;&#25968;&#25454;&#20998;&#31751;&#65292;&#22240;&#27492;&#20855;&#26377;&#36739;&#39640;&#30340;&#21487;&#35270;&#21270;&#33021;&#21147;&#21644;&#39044;&#27979;&#20934;&#30830;&#24615;&#20445;&#35777;&#12290;&#35813;&#27169;&#22411;&#22312;&#29305;&#23450;&#24773;&#20917;&#19979;&#30340;&#39044;&#27979;&#29983;&#23384;&#20998;&#24067;&#35823;&#24046;&#30028;&#38480;&#26368;&#20248;&#65292;&#19988;&#22312;&#27979;&#35797;&#26102;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26680;&#29983;&#23384;&#20998;&#26512;&#27169;&#22411;&#36890;&#36807;&#26680;&#20989;&#25968;&#26469;&#20272;&#35745;&#20010;&#20307;&#30340;&#29983;&#23384;&#20998;&#24067;&#65292;&#26680;&#20989;&#25968;&#24230;&#37327;&#20219;&#24847;&#20004;&#20010;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#26680;&#29983;&#23384;&#27169;&#22411;&#8212;&#8212;&#29983;&#23384;kernet&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#26131;&#20110;&#35299;&#37322;&#21644;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35757;&#32451;&#25968;&#25454;&#26681;&#25454;&#19968;&#31181;&#26368;&#36817;&#21457;&#23637;&#30340;&#29992;&#20110;&#20998;&#31867;&#21644;&#22238;&#24402;&#30340;&#35757;&#32451;&#38598;&#21387;&#32553;&#26041;&#26696;&#65288;&#31216;&#20026;&#26680;&#32676;&#65289;&#36827;&#34892;&#20998;&#31751;&#12290;&#22312;&#27979;&#35797;&#26102;&#65292;&#27599;&#20010;&#25968;&#25454;&#28857;&#34987;&#34920;&#31034;&#20026;&#36825;&#20123;&#31751;&#30340;&#21152;&#26435;&#32452;&#21512;&#65292;&#27599;&#20010;&#31751;&#21487;&#20197;&#36827;&#34892;&#21487;&#35270;&#21270;&#23637;&#31034;&#12290;&#23545;&#20110;&#29983;&#23384;kernet&#30340;&#19968;&#20010;&#29305;&#27530;&#24773;&#20917;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#26377;&#38480;&#26679;&#26412;&#35823;&#24046;&#30028;&#38480;&#65292;&#39044;&#27979;&#30340;&#29983;&#23384;&#20998;&#24067;&#22312;&#35813;&#30028;&#38480;&#19979;&#26159;&#26368;&#20248;&#30340;&#65288;&#38500;&#21435;&#19968;&#20010;&#23545;&#25968;&#22240;&#23376;&#65289;&#12290;&#22312;&#27979;&#35797;&#26102;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Kernel survival analysis models estimate individual survival distributions with the help of a kernel function, which measures the similarity between any two data points. Such a kernel function can be learned using deep kernel survival models. In this paper, we present a new deep kernel survival model called a survival kernet, which scales to large datasets in a manner that is amenable to model interpretation and also theoretical analysis. Specifically, the training data are partitioned into clusters based on a recently developed training set compression scheme for classification and regression called kernel netting that we extend to the survival analysis setting. At test time, each data point is represented as a weighted combination of these clusters, and each such cluster can be visualized. For a special case of survival kernets, we establish a finite-sample error bound on predicted survival distributions that is, up to a log factor, optimal. Whereas scalability at test time is achiev
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25209;&#37327;&#24322;&#27493;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#22312;&#20869;&#23384;&#38656;&#27714;&#21644;&#26102;&#38388;&#22797;&#26434;&#24230;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#21487;&#20197;&#20351;&#29992;&#36739;&#24369;&#20551;&#35774;&#35777;&#26126;&#25910;&#25947;&#30340;&#19968;&#33324;&#26041;&#27861;&#65307;&#22312;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#65292;&#25105;&#20204;&#20351;&#29992;&#27492;&#26041;&#27861;&#35777;&#26126;&#20102;SARSA&#31639;&#27861;&#30340;&#25209;&#37327;&#24322;&#27493;&#29256;&#26412;&#30340;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2109.03445</link><description>&lt;p&gt;
&#25209;&#37327;&#24322;&#27493;&#38543;&#26426;&#36924;&#36817;&#30340;&#25910;&#25947;&#24615;&#21450;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Convergence of Batch Asynchronous Stochastic Approximation With Applications to Reinforcement Learning. (arXiv:2109.03445v4 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.03445
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25209;&#37327;&#24322;&#27493;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#22312;&#20869;&#23384;&#38656;&#27714;&#21644;&#26102;&#38388;&#22797;&#26434;&#24230;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#21487;&#20197;&#20351;&#29992;&#36739;&#24369;&#20551;&#35774;&#35777;&#26126;&#25910;&#25947;&#30340;&#19968;&#33324;&#26041;&#27861;&#65307;&#22312;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#65292;&#25105;&#20204;&#20351;&#29992;&#27492;&#26041;&#27861;&#35777;&#26126;&#20102;SARSA&#31639;&#27861;&#30340;&#25209;&#37327;&#24322;&#27493;&#29256;&#26412;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#36924;&#36817;&#65288;SA&#65289;&#31639;&#27861;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#27010;&#29575;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20165;&#21487;&#29992;&#20989;&#25968;&#30340;&#26377;&#22122;&#27979;&#37327;&#24773;&#20917;&#19979;&#25214;&#21040;&#38646;&#28857;&#25110;&#22266;&#23450;&#28857;&#12290;&#30446;&#21069;&#30340;&#25991;&#29486;&#20013;&#65292;&#21306;&#20998;&#8220;&#21516;&#27493;&#8221;&#26356;&#26032;&#21644;&#8220;&#24322;&#27493;&#8221;&#26356;&#26032;&#65292;&#22312;&#8220;&#21516;&#27493;&#8221;&#26356;&#26032;&#20013;&#65292;&#27599;&#20010;&#29468;&#27979;&#30340;&#32452;&#20214;&#37117;&#20250;&#22312;&#27599;&#20010;&#26102;&#38388;&#26356;&#26032;&#65292;&#32780;&#22312;&#8220;&#24322;&#27493;&#8221;&#26356;&#26032;&#20013;&#65292;&#20165;&#26356;&#26032;&#19968;&#20010;&#32452;&#20214;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#20013;&#38388;&#24773;&#20917;&#65292;&#31216;&#20026;&#8220;&#25209;&#37327;&#24322;&#27493;&#38543;&#26426;&#36924;&#36817;&#8221;&#65288;BASA&#65289;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#27599;&#20010;&#26102;&#38388;&#28857;&#20165;&#26356;&#26032;&#8220;&#24403;&#21069;&#20272;&#35745;&#35299;&#8221;&#30340;&#19968;&#20123;&#20294;&#19981;&#26159;&#20840;&#37096;&#30340;&#32452;&#20214;&#12290;BASA&#20801;&#35768;&#29992;&#25143;&#22312;&#20869;&#23384;&#38656;&#27714;&#21644;&#26102;&#38388;&#22797;&#26434;&#24230;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#35777;&#26126;&#27492;&#31867;&#31639;&#27861;&#25910;&#25947;&#20110;&#25152;&#30740;&#31350;&#26144;&#23556;&#30340;&#22266;&#23450;&#28857;&#12290;&#36825;&#20123;&#25910;&#25947;&#35777;&#26126;&#20351;&#29992;&#27604;&#29616;&#26377;&#32467;&#26524;&#26356;&#24369;&#30340;&#20551;&#35774;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#29616;&#26377;&#30340;&#25910;&#25947;&#35777;&#26126;&#35201;&#27714;&#27493;&#38271;&#21442;&#25968;&#20197;&#36866;&#24403;&#30340;&#36895;&#29575;&#19979;&#38477;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#20165;&#35201;&#27714;&#27599;&#20010;&#32452;&#20214;&#20855;&#26377;&#36275;&#22815;&#30340;&#26356;&#26032;&#39057;&#29575;&#12290;&#25105;&#20204;&#22312;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#29992;&#24615;&#65292;&#35777;&#26126;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340;SARSA&#31639;&#27861;&#30340;&#25209;&#37327;&#24322;&#27493;&#29256;&#26412;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The stochastic approximation (SA) algorithm is a widely used probabilistic method for finding a zero or a fixed point of a vector-valued funtion, when only noisy measurements of the function are available. In the literature to date, one makes a distinction between ``synchronous'' updating, whereby every component of the current guess is updated at each time, and ``asynchronous'' updating, whereby only one component is updated. In this paper, we study an intermediate situation that we call ``batch asynchronous stochastic approximation'' (BASA), in which, at each time instant, \textit{some but not all} components of the current estimated solution are updated. BASA allows the user to trade off memory requirements against time complexity. We develop a general methodology for proving that such algorithms converge to the fixed point of the map under study. These convergence proofs make use of weaker hypotheses than existing results. Specifically, existing convergence proofs require that the 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#20195;&#29702;&#22240;&#26524;&#23398;&#20064;&#65288;PCL&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#23384;&#22312;&#28151;&#28102;&#22240;&#32032;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#27835;&#30103;&#23545;&#32467;&#26524;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#36890;&#36807;&#26500;&#24314;&#27835;&#30103;&#21644;&#20195;&#29702;&#20043;&#38388;&#30340;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#35813;&#27169;&#22411;&#22312;&#32473;&#23450;&#20195;&#29702;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#27835;&#30103;&#23545;&#32467;&#26524;&#30340;&#24433;&#21709;&#65292;PCL&#21487;&#20197;&#20445;&#35777;&#24674;&#22797;&#30495;&#23454;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28145;&#24230;&#29305;&#24449;&#20195;&#29702;&#21464;&#37327;&#26041;&#27861;&#65288;DFPV&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#39640;&#32500;&#21644;&#38750;&#32447;&#24615;&#22797;&#26434;&#20851;&#31995;&#30340;&#24773;&#20917;&#65292;&#24182;&#34920;&#26126;DFPV&#22312;&#21512;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24615;&#33021;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;PCL&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2106.03907</link><description>&lt;p&gt;
&#28145;&#24230;&#20195;&#29702;&#22240;&#26524;&#23398;&#20064;&#21450;&#20854;&#22312;&#28151;&#28102;&#36172;&#21338;&#31574;&#30053;&#35780;&#20272;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Deep Proxy Causal Learning and its Application to Confounded Bandit Policy Evaluation. (arXiv:2106.03907v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.03907
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#20195;&#29702;&#22240;&#26524;&#23398;&#20064;&#65288;PCL&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#23384;&#22312;&#28151;&#28102;&#22240;&#32032;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#27835;&#30103;&#23545;&#32467;&#26524;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#36890;&#36807;&#26500;&#24314;&#27835;&#30103;&#21644;&#20195;&#29702;&#20043;&#38388;&#30340;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#35813;&#27169;&#22411;&#22312;&#32473;&#23450;&#20195;&#29702;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#27835;&#30103;&#23545;&#32467;&#26524;&#30340;&#24433;&#21709;&#65292;PCL&#21487;&#20197;&#20445;&#35777;&#24674;&#22797;&#30495;&#23454;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28145;&#24230;&#29305;&#24449;&#20195;&#29702;&#21464;&#37327;&#26041;&#27861;&#65288;DFPV&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#39640;&#32500;&#21644;&#38750;&#32447;&#24615;&#22797;&#26434;&#20851;&#31995;&#30340;&#24773;&#20917;&#65292;&#24182;&#34920;&#26126;DFPV&#22312;&#21512;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24615;&#33021;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;PCL&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#29702;&#22240;&#26524;&#23398;&#20064;&#65288;PCL&#65289;&#26159;&#19968;&#31181;&#22312;&#23384;&#22312;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#22240;&#32032;&#26102;&#65292;&#21033;&#29992;&#20195;&#29702;&#65288;&#32467;&#26500;&#21270;&#20391;&#38754;&#20449;&#24687;&#65289;&#20272;&#35745;&#27835;&#30103;&#23545;&#32467;&#26524;&#30340;&#22240;&#26524;&#25928;&#24212;&#30340;&#26041;&#27861;&#12290;&#36825;&#26159;&#36890;&#36807;&#20004;&#38454;&#27573;&#22238;&#24402;&#23454;&#29616;&#30340;&#65306;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#25105;&#20204;&#24314;&#27169;&#27835;&#30103;&#21644;&#20195;&#29702;&#20043;&#38388;&#30340;&#20851;&#31995;&#65307;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#27169;&#22411;&#26469;&#23398;&#20064;&#22312;&#32473;&#23450;&#20195;&#29702;&#25552;&#20379;&#30340;&#19978;&#19979;&#25991;&#19979;&#65292;&#27835;&#30103;&#23545;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;PCL&#22312;&#21487;&#35782;&#21035;&#26465;&#20214;&#19979;&#20445;&#35777;&#24674;&#22797;&#30495;&#23454;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;PCL&#26041;&#27861;&#65292;&#28145;&#24230;&#29305;&#24449;&#20195;&#29702;&#21464;&#37327;&#26041;&#27861;&#65288;DFPV&#65289;&#65292;&#20197;&#35299;&#20915;&#20195;&#29702;&#12289;&#27835;&#30103;&#21644;&#32467;&#26524;&#20026;&#39640;&#32500;&#19988;&#20855;&#26377;&#38750;&#32447;&#24615;&#22797;&#26434;&#20851;&#31995;&#30340;&#24773;&#20917;&#65292;&#22914;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29305;&#24449;&#34920;&#31034;&#12290;&#25105;&#20204;&#34920;&#26126;DFPV&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21512;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#20248;&#20110;&#26368;&#36817;&#30340;&#26368;&#20808;&#36827;&#30340;PCL&#26041;&#27861;&#65292;&#21253;&#25324;&#28041;&#21450;&#39640;&#32500;&#22270;&#20687;&#25968;&#25454;&#30340;&#35774;&#32622;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;PCL&#30340;&#24212;&#29992;...
&lt;/p&gt;
&lt;p&gt;
Proxy causal learning (PCL) is a method for estimating the causal effect of treatments on outcomes in the presence of unobserved confounding, using proxies (structured side information) for the confounder. This is achieved via two-stage regression: in the first stage, we model relations among the treatment and proxies; in the second stage, we use this model to learn the effect of treatment on the outcome, given the context provided by the proxies. PCL guarantees recovery of the true causal effect, subject to identifiability conditions. We propose a novel method for PCL, the deep feature proxy variable method (DFPV), to address the case where the proxies, treatments, and outcomes are high-dimensional and have nonlinear complex relationships, as represented by deep neural network features. We show that DFPV outperforms recent state-of-the-art PCL methods on challenging synthetic benchmarks, including settings involving high dimensional image data. Furthermore, we show that PCL can be app
&lt;/p&gt;</description></item></channel></rss>